diff --git a/docs/es/commercial/cloud.md b/docs/es/commercial/cloud.md
deleted file mode 100644
index bc593a82ad7e..000000000000
--- a/docs/es/commercial/cloud.md
+++ /dev/null
@@ -1,23 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 1
-toc_title: Nube
----
-
-# Proveedores de servicios en la nube de ClickHouse {#clickhouse-cloud-service-providers}
-
-!!! info "INFO"
-    Si ha lanzado una nube pública con el servicio ClickHouse administrado, no dude en [abrir una solicitud de extracción](https://github.com/ClickHouse/ClickHouse/edit/master/docs/en/commercial/cloud.md) añadiéndolo a la siguiente lista.
-
-## Nube de Yandex {#yandex-cloud}
-
-[Servicio administrado de Yandex para ClickHouse](https://cloud.yandex.com/services/managed-clickhouse?utm_source=referrals&utm_medium=clickhouseofficialsite&utm_campaign=link3) proporciona las siguientes características clave:
-
--   Servicio ZooKeeper totalmente gestionado para [Replicación de ClickHouse](../engines/table-engines/mergetree-family/replication.md)
--   Múltiples opciones de tipo de almacenamiento
--   Réplicas en diferentes zonas de disponibilidad
--   Cifrado y aislamiento
--   Mantenimiento automatizado
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/commercial/cloud/) ##}
diff --git a/docs/es/commercial/index.md b/docs/es/commercial/index.md
deleted file mode 100644
index b367631ae1c8..000000000000
--- a/docs/es/commercial/index.md
+++ /dev/null
@@ -1,9 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Comercial
-toc_priority: 70
-toc_title: Comercial
----
-
-
diff --git a/docs/es/commercial/support.md b/docs/es/commercial/support.md
deleted file mode 100644
index a817d90dcb52..000000000000
--- a/docs/es/commercial/support.md
+++ /dev/null
@@ -1,23 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 3
-toc_title: Apoyo
----
-
-# Proveedores de servicios de soporte comercial ClickHouse {#clickhouse-commercial-support-service-providers}
-
-!!! info "INFO"
-    Si ha lanzado un servicio de soporte comercial ClickHouse, no dude en [abrir una solicitud de extracción](https://github.com/ClickHouse/ClickHouse/edit/master/docs/en/commercial/support.md) añadiéndolo a la siguiente lista.
-
-## Altinidad {#altinity}
-
-Altinity ha ofrecido soporte y servicios empresariales ClickHouse desde 2017. Los clientes de Altinity van desde empresas Fortune 100 hasta startups. Visitar [Más información](https://www.altinity.com/) para más información.
-
-## Mafiree {#mafiree}
-
-[Descripción del servicio](http://mafiree.com/clickhouse-analytics-services.php)
-
-## MinervaDB {#minervadb}
-
-[Descripción del servicio](https://minervadb.com/index.php/clickhouse-consulting-and-support-by-minervadb/)
diff --git a/docs/es/development/architecture.md b/docs/es/development/architecture.md
deleted file mode 100644
index 1620a58a3a08..000000000000
--- a/docs/es/development/architecture.md
+++ /dev/null
@@ -1,203 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 62
-toc_title: "Descripci\xF3n general de la arquitectura ClickHouse"
----
-
-# Descripción general de la arquitectura ClickHouse {#overview-of-clickhouse-architecture}
-
-ClickHouse es un verdadero DBMS orientado a columnas. Los datos se almacenan por columnas y durante la ejecución de matrices (vectores o fragmentos de columnas). Siempre que sea posible, las operaciones se envían en matrices, en lugar de en valores individuales. Se llama “vectorized query execution,” y ayuda a reducir el costo del procesamiento de datos real.
-
-> Esta idea no es nada nuevo. Se remonta a la `APL` lenguaje de programación y sus descendientes: `A +`, `J`, `K`, y `Q`. La programación de matrices se utiliza en el procesamiento de datos científicos. Tampoco es esta idea algo nuevo en las bases de datos relacionales: por ejemplo, se usa en el `Vectorwise` sistema.
-
-Existen dos enfoques diferentes para acelerar el procesamiento de consultas: la ejecución de consultas vectorizadas y la generación de código en tiempo de ejecución. Este último elimina toda la indirección y el despacho dinámico. Ninguno de estos enfoques es estrictamente mejor que el otro. La generación de código de tiempo de ejecución puede ser mejor cuando fusiona muchas operaciones, utilizando así las unidades de ejecución de la CPU y la canalización. La ejecución de consultas vectorizadas puede ser menos práctica porque implica vectores temporales que deben escribirse en la memoria caché y leerse. Si los datos temporales no caben en la memoria caché L2, esto se convierte en un problema. Pero la ejecución de consultas vectorizadas utiliza más fácilmente las capacidades SIMD de la CPU. Un [documento de investigación](http://15721.courses.cs.cmu.edu/spring2016/papers/p5-sompolski.pdf) escrito por nuestros amigos muestra que es mejor combinar ambos enfoques. ClickHouse utiliza la ejecución de consultas vectorizadas y tiene un soporte inicial limitado para la generación de código en tiempo de ejecución.
-
-## Columna {#columns}
-
-`IColumn` interfaz se utiliza para representar columnas en la memoria (en realidad, fragmentos de columnas). Esta interfaz proporciona métodos auxiliares para la implementación de varios operadores relacionales. Casi todas las operaciones son inmutables: no modifican la columna original, sino que crean una nueva modificada. Por ejemplo, el `IColumn :: filter` método acepta una máscara de bytes de filtro. Se utiliza para el `WHERE` y `HAVING` operadores relacionales. Ejemplos adicionales: el `IColumn :: permute` para apoyar `ORDER BY`, el `IColumn :: cut` para apoyar `LIMIT`.
-
-Diversos `IColumn` aplicación (`ColumnUInt8`, `ColumnString`, y así sucesivamente) son responsables del diseño de memoria de las columnas. El diseño de memoria suele ser una matriz contigua. Para el tipo entero de columnas, es solo una matriz contigua, como `std :: vector`. Para `String` y `Array` columnas, son dos vectores: uno para todos los elementos de la matriz, colocados contiguamente, y un segundo para los desplazamientos al comienzo de cada matriz. También hay `ColumnConst` que almacena solo un valor en la memoria, pero parece una columna.
-
-## Campo {#field}
-
-Sin embargo, también es posible trabajar con valores individuales. Para representar un valor individual, el `Field` se utiliza. `Field` es sólo una unión discriminada de `UInt64`, `Int64`, `Float64`, `String` y `Array`. `IColumn` tiene el `operator[]` para obtener el valor n-ésimo como un `Field` y el `insert` método para agregar un `Field` al final de una columna. Estos métodos no son muy eficientes, ya que requieren tratar con temporal `Field` objetos que representan un valor individual. Hay métodos más eficientes, tales como `insertFrom`, `insertRangeFrom` y así sucesivamente.
-
-`Field` no tiene suficiente información sobre un tipo de datos específico para una tabla. Por ejemplo, `UInt8`, `UInt16`, `UInt32`, y `UInt64` todos están representados como `UInt64` en una `Field`.
-
-## Abstracciones con fugas {#leaky-abstractions}
-
-`IColumn` tiene métodos para transformaciones relacionales comunes de datos, pero no satisfacen todas las necesidades. Por ejemplo, `ColumnUInt64` no tiene un método para calcular la suma de dos columnas, y `ColumnString` no tiene un método para ejecutar una búsqueda de subcadena. Estas innumerables rutinas se implementan fuera de `IColumn`.
-
-Varias funciones en columnas se pueden implementar de una manera genérica, no eficiente utilizando `IColumn` para extraer `Field` valores, o de una manera especializada utilizando el conocimiento del diseño de la memoria interna de los datos en un `IColumn` aplicación. Se implementa mediante la conversión de funciones a un `IColumn` escriba y trate con la representación interna directamente. Por ejemplo, `ColumnUInt64` tiene el `getData` método que devuelve una referencia a una matriz interna, luego una rutina separada lee o llena esa matriz directamente. Tenemos “leaky abstractions” para permitir especializaciones eficientes de varias rutinas.
-
-## Tipos de datos {#data_types}
-
-`IDataType` es responsable de la serialización y deserialización: para leer y escribir fragmentos de columnas o valores individuales en formato binario o de texto. `IDataType` corresponde directamente a los tipos de datos en las tablas. Por ejemplo, hay `DataTypeUInt32`, `DataTypeDateTime`, `DataTypeString` y así sucesivamente.
-
-`IDataType` y `IColumn` están vagamente relacionados entre sí. Diferentes tipos de datos se pueden representar en la memoria por el mismo `IColumn` aplicación. Por ejemplo, `DataTypeUInt32` y `DataTypeDateTime` están representados por `ColumnUInt32` o `ColumnConstUInt32`. Además, el mismo tipo de datos se puede representar mediante `IColumn` aplicación. Por ejemplo, `DataTypeUInt8` puede ser representado por `ColumnUInt8` o `ColumnConstUInt8`.
-
-`IDataType` sólo almacena metadatos. Por ejemplo, `DataTypeUInt8` no almacena nada en absoluto (excepto vptr) y `DataTypeFixedString` tiendas solo `N` (el tamaño de las cadenas de tamaño fijo).
-
-`IDataType` tiene métodos auxiliares para varios formatos de datos. Los ejemplos son métodos para serializar un valor con posibles citas, para serializar un valor para JSON y para serializar un valor como parte del formato XML. No hay correspondencia directa con los formatos de datos. Por ejemplo, los diferentes formatos de datos `Pretty` y `TabSeparated` puede utilizar el mismo `serializeTextEscaped` método de ayuda de la `IDataType` interfaz.
-
-## Bloque {#block}
-
-A `Block` es un contenedor que representa un subconjunto (porción) de una tabla en la memoria. Es sólo un conjunto de triples: `(IColumn, IDataType, column name)`. Durante la ejecución de la consulta, los datos son procesados por `Block`s. Si tenemos un `Block`, tenemos datos (en el `IColumn` objeto), tenemos información sobre su tipo (en `IDataType`) que nos dice cómo lidiar con esa columna, y tenemos el nombre de la columna. Podría ser el nombre de columna original de la tabla o algún nombre artificial asignado para obtener resultados temporales de los cálculos.
-
-Cuando calculamos alguna función sobre columnas en un bloque, agregamos otra columna con su resultado al bloque, y no tocamos columnas para argumentos de la función porque las operaciones son inmutables. Más tarde, las columnas innecesarias se pueden eliminar del bloque, pero no se pueden modificar. Es conveniente para la eliminación de subexpresiones comunes.
-
-Se crean bloques para cada fragmento de datos procesado. Tenga en cuenta que para el mismo tipo de cálculo, los nombres y tipos de columna siguen siendo los mismos para diferentes bloques y solo cambian los datos de columna. Es mejor dividir los datos del bloque desde el encabezado del bloque porque los tamaños de bloque pequeños tienen una gran sobrecarga de cadenas temporales para copiar shared_ptrs y nombres de columna.
-
-## Bloquear flujos {#block-streams}
-
-Los flujos de bloques son para procesar datos. Usamos flujos de bloques para leer datos de algún lugar, realizar transformaciones de datos o escribir datos en algún lugar. `IBlockInputStream` tiene el `read` método para buscar el siguiente bloque mientras esté disponible. `IBlockOutputStream` tiene el `write` método para empujar el bloque en alguna parte.
-
-Los flujos son responsables de:
-
-1.  Leer o escribir en una mesa. La tabla solo devuelve una secuencia para leer o escribir bloques.
-2.  Implementación de formatos de datos. Por ejemplo, si desea enviar datos a un terminal en `Pretty` formato, crea un flujo de salida de bloque donde presiona bloques y los formatea.
-3.  Realización de transformaciones de datos. Digamos que tienes `IBlockInputStream` y desea crear una secuencia filtrada. Usted crea `FilterBlockInputStream` e inicializarlo con su transmisión. Luego, cuando tiras de un bloque de `FilterBlockInputStream`, extrae un bloque de su flujo, lo filtra y le devuelve el bloque filtrado. Las canalizaciones de ejecución de consultas se representan de esta manera.
-
-Hay transformaciones más sofisticadas. Por ejemplo, cuando tiras de `AggregatingBlockInputStream`, lee todos los datos de su origen, los agrega y, a continuación, devuelve un flujo de datos agregados para usted. Otro ejemplo: `UnionBlockInputStream` acepta muchas fuentes de entrada en el constructor y también una serie de subprocesos. Lanza múltiples hilos y lee de múltiples fuentes en paralelo.
-
-> Las secuencias de bloques usan el “pull” enfoque para controlar el flujo: cuando extrae un bloque de la primera secuencia, en consecuencia extrae los bloques requeridos de las secuencias anidadas, y toda la tubería de ejecución funcionará. Ni “pull” ni “push” es la mejor solución, porque el flujo de control está implícito y eso limita la implementación de varias características, como la ejecución simultánea de múltiples consultas (fusionando muchas tuberías). Esta limitación podría superarse con coroutines o simplemente ejecutando hilos adicionales que se esperan el uno al otro. Podemos tener más posibilidades si hacemos explícito el flujo de control: si localizamos la lógica para pasar datos de una unidad de cálculo a otra fuera de esas unidades de cálculo. Lea esto [artículo](http://journal.stuffwithstuff.com/2013/01/13/iteration-inside-and-out/) para más pensamientos.
-
-Debemos tener en cuenta que la canalización de ejecución de consultas crea datos temporales en cada paso. Tratamos de mantener el tamaño del bloque lo suficientemente pequeño para que los datos temporales se ajusten a la memoria caché de la CPU. Con esa suposición, escribir y leer datos temporales es casi gratis en comparación con otros cálculos. Podríamos considerar una alternativa, que es fusionar muchas operaciones en la tubería. Podría hacer que la tubería sea lo más corta posible y eliminar gran parte de los datos temporales, lo que podría ser una ventaja, pero también tiene inconvenientes. Por ejemplo, una canalización dividida facilita la implementación de almacenamiento en caché de datos intermedios, el robo de datos intermedios de consultas similares que se ejecutan al mismo tiempo y la fusión de canalizaciones para consultas similares.
-
-## Formato {#formats}
-
-Los formatos de datos se implementan con flujos de bloques. Hay “presentational” sólo es adecuado para la salida de datos al cliente, tales como `Pretty` formato, que proporciona sólo `IBlockOutputStream`. Y hay formatos de entrada / salida, como `TabSeparated` o `JSONEachRow`.
-
-También hay secuencias de filas: `IRowInputStream` y `IRowOutputStream`. Permiten pull/push datos por filas individuales, no por bloques. Y solo son necesarios para simplificar la implementación de formatos orientados a filas. Envoltura `BlockInputStreamFromRowInputStream` y `BlockOutputStreamFromRowOutputStream` le permite convertir flujos orientados a filas en flujos regulares orientados a bloques.
-
-## I/O {#io}
-
-Para la entrada / salida orientada a bytes, hay `ReadBuffer` y `WriteBuffer` clases abstractas. Se usan en lugar de C ++ `iostream`s. No se preocupe: cada proyecto maduro de C ++ está usando algo más que `iostream`s por buenas razones.
-
-`ReadBuffer` y `WriteBuffer` son solo un búfer contiguo y un cursor apuntando a la posición en ese búfer. Las implementaciones pueden poseer o no la memoria del búfer. Hay un método virtual para llenar el búfer con los siguientes datos (para `ReadBuffer`) o para vaciar el búfer en algún lugar (para `WriteBuffer`). Los métodos virtuales rara vez se llaman.
-
-Implementaciones de `ReadBuffer`/`WriteBuffer` se utilizan para trabajar con archivos y descriptores de archivos y sockets de red, para implementar la compresión (`CompressedWriteBuffer` is initialized with another WriteBuffer and performs compression before writing data to it), and for other purposes – the names `ConcatReadBuffer`, `LimitReadBuffer`, y `HashingWriteBuffer` hablar por sí mismos.
-
-Read / WriteBuffers solo se ocupan de bytes. Hay funciones de `ReadHelpers` y `WriteHelpers` archivos de encabezado para ayudar con el formato de entrada / salida. Por ejemplo, hay ayudantes para escribir un número en formato decimal.
-
-Veamos qué sucede cuando quieres escribir un conjunto de resultados en `JSON` formato a stdout. Tiene un conjunto de resultados listo para ser recuperado de `IBlockInputStream`. Usted crea `WriteBufferFromFileDescriptor(STDOUT_FILENO)` para escribir bytes en stdout. Usted crea `JSONRowOutputStream`, inicializado con eso `WriteBuffer` para escribir filas en `JSON` a stdout. Usted crea `BlockOutputStreamFromRowOutputStream` encima de él, para representarlo como `IBlockOutputStream`. Entonces usted llama `copyData` para transferir datos desde `IBlockInputStream` a `IBlockOutputStream` y todo funciona. Internamente, `JSONRowOutputStream` escribirá varios delimitadores JSON y llamará al `IDataType::serializeTextJSON` con una referencia a `IColumn` y el número de fila como argumentos. Consecuentemente, `IDataType::serializeTextJSON` llamará a un método de `WriteHelpers.h`: por ejemplo, `writeText` para tipos numéricos y `writeJSONString` para `DataTypeString`.
-
-## Tabla {#tables}
-
-El `IStorage` interfaz representa tablas. Las diferentes implementaciones de esa interfaz son diferentes motores de tabla. Los ejemplos son `StorageMergeTree`, `StorageMemory` y así sucesivamente. Las instancias de estas clases son solo tablas.
-
-Clave `IStorage` son `read` y `write`. También hay `alter`, `rename`, `drop` y así sucesivamente. El `read` método acepta los siguientes argumentos: el conjunto de columnas para leer de una tabla, el `AST` consulta a considerar, y el número deseado de flujos para devolver. Devuelve uno o varios `IBlockInputStream` objetos e información sobre la etapa de procesamiento de datos que se completó dentro de un motor de tablas durante la ejecución de la consulta.
-
-En la mayoría de los casos, el método de lectura solo es responsable de leer las columnas especificadas de una tabla, no de ningún procesamiento de datos adicional. Todo el procesamiento de datos adicional es realizado por el intérprete de consultas y está fuera de la responsabilidad de `IStorage`.
-
-Pero hay excepciones notables:
-
--   La consulta AST se pasa al `read` método, y el motor de tablas puede usarlo para derivar el uso del índice y leer menos datos de una tabla.
--   A veces, el motor de tablas puede procesar los datos a una etapa específica. Por ejemplo, `StorageDistributed` puede enviar una consulta a servidores remotos, pedirles que procesen datos a una etapa donde se puedan fusionar datos de diferentes servidores remotos y devolver esos datos preprocesados. El intérprete de consultas termina de procesar los datos.
-
-Tabla `read` método puede devolver múltiples `IBlockInputStream` objetos para permitir el procesamiento de datos en paralelo. Estos flujos de entrada de bloques múltiples pueden leer de una tabla en paralelo. A continuación, puede ajustar estas secuencias con varias transformaciones (como la evaluación de expresiones o el filtrado) que se pueden calcular de forma independiente y crear un `UnionBlockInputStream` encima de ellos, para leer desde múltiples flujos en paralelo.
-
-También hay `TableFunction`s. Estas son funciones que devuelven un `IStorage` objeto a utilizar en el `FROM` cláusula de una consulta.
-
-Para tener una idea rápida de cómo implementar su motor de tabla, vea algo simple, como `StorageMemory` o `StorageTinyLog`.
-
-> Como resultado de la `read` método, `IStorage` devoluciones `QueryProcessingStage` – information about what parts of the query were already calculated inside storage.
-
-## Analizador {#parsers}
-
-Un analizador de descenso recursivo escrito a mano analiza una consulta. Por ejemplo, `ParserSelectQuery` simplemente llama recursivamente a los analizadores subyacentes para varias partes de la consulta. Los analizadores crean un `AST`. El `AST` está representado por nodos, que son instancias de `IAST`.
-
-> Los generadores de analizadores no se utilizan por razones históricas.
-
-## Interprete {#interpreters}
-
-Los intérpretes son responsables de crear la canalización de ejecución de consultas `AST`. Hay intérpretes simples, como `InterpreterExistsQuery` y `InterpreterDropQuery` o el más sofisticado `InterpreterSelectQuery`. La canalización de ejecución de consultas es una combinación de flujos de entrada o salida de bloques. Por ejemplo, el resultado de interpretar el `SELECT` la consulta es la `IBlockInputStream` para leer el conjunto de resultados; el resultado de la consulta INSERT es el `IBlockOutputStream` para escribir datos para su inserción, y el resultado de interpretar el `INSERT SELECT` la consulta es la `IBlockInputStream` que devuelve un conjunto de resultados vacío en la primera lectura, pero que copia datos de `SELECT` a `INSERT` al mismo tiempo.
-
-`InterpreterSelectQuery` utilizar `ExpressionAnalyzer` y `ExpressionActions` maquinaria para el análisis de consultas y transformaciones. Aquí es donde se realizan la mayoría de las optimizaciones de consultas basadas en reglas. `ExpressionAnalyzer` es bastante complicado y debe reescribirse: se deben extraer varias transformaciones de consultas y optimizaciones para separar clases para permitir transformaciones modulares o consultas.
-
-## Función {#functions}
-
-Hay funciones ordinarias y funciones agregadas. Para las funciones agregadas, consulte la siguiente sección.
-
-Ordinary functions don't change the number of rows – they work as if they are processing each row independently. In fact, functions are not called for individual rows, but for `Block`de datos para implementar la ejecución de consultas vectorizadas.
-
-Hay algunas funciones diversas, como [BlockSize](../sql-reference/functions/other-functions.md#function-blocksize), [rowNumberInBlock](../sql-reference/functions/other-functions.md#function-rownumberinblock), y [runningAccumulate](../sql-reference/functions/other-functions.md#function-runningaccumulate), que explotan el procesamiento de bloques y violan la independencia de las filas.
-
-ClickHouse tiene una tipificación fuerte, por lo que no hay conversión de tipo implícita. Si una función no admite una combinación específica de tipos, produce una excepción. Pero las funciones pueden funcionar (estar sobrecargadas) para muchas combinaciones diferentes de tipos. Por ejemplo, el `plus` función (para implementar el `+` operador) funciona para cualquier combinación de tipos numéricos: `UInt8` + `Float32`, `UInt16` + `Int8` y así sucesivamente. Además, algunas funciones variadas pueden aceptar cualquier número de argumentos, como el `concat` función.
-
-Implementar una función puede ser un poco inconveniente porque una función distribuye explícitamente tipos de datos compatibles y `IColumns`. Por ejemplo, el `plus` La función tiene código generado por la creación de instancias de una plantilla de C ++ para cada combinación de tipos numéricos y argumentos izquierdo y derecho constantes o no constantes.
-
-Es un excelente lugar para implementar la generación de código en tiempo de ejecución para evitar la hinchazón del código de plantilla. Además, permite agregar funciones fusionadas como multiplicar-agregar fusionado o hacer comparaciones múltiples en una iteración de bucle.
-
-Debido a la ejecución de consultas vectorizadas, las funciones no se cortocircuitan. Por ejemplo, si escribe `WHERE f(x) AND g(y)`, ambos lados se calculan, incluso para las filas, cuando `f(x)` es cero (excepto cuando `f(x)` es una expresión constante cero). Pero si la selectividad del `f(x)` la condición es alta, y el cálculo de `f(x)` es mucho más barato que `g(y)`, es mejor implementar el cálculo de paso múltiple. Primero calcularía `f(x)`, a continuación, filtrar columnas por el resultado, y luego calcular `g(y)` solo para trozos de datos más pequeños y filtrados.
-
-## Funciones agregadas {#aggregate-functions}
-
-Las funciones agregadas son funciones con estado. Acumulan valores pasados en algún estado y le permiten obtener resultados de ese estado. Se gestionan con el `IAggregateFunction` interfaz. Los estados pueden ser bastante simples (el estado para `AggregateFunctionCount` es sólo una sola `UInt64` valor) o bastante complejo (el estado de `AggregateFunctionUniqCombined` es una combinación de una matriz lineal, una tabla hash, y un `HyperLogLog` estructura de datos probabilística).
-
-Los Estados están asignados en `Arena` (un grupo de memoria) para tratar con múltiples estados mientras se ejecuta una alta cardinalidad `GROUP BY` consulta. Los estados pueden tener un constructor y destructor no trivial: por ejemplo, los estados de agregación complicados pueden asignar memoria adicional ellos mismos. Requiere cierta atención a la creación y destrucción de estados y a la adecuada aprobación de su orden de propiedad y destrucción.
-
-Los estados de agregación se pueden serializar y deserializar para pasar a través de la red durante la ejecución de consultas distribuidas o para escribirlos en el disco donde no hay suficiente RAM. Incluso se pueden almacenar en una tabla con el `DataTypeAggregateFunction` para permitir la agregación incremental de datos.
-
-> El formato de datos serializados para los estados de función agregados no tiene versiones en este momento. Está bien si los estados agregados solo se almacenan temporalmente. Pero tenemos el `AggregatingMergeTree` motor de tabla para la agregación incremental, y la gente ya lo está utilizando en producción. Es la razón por la que se requiere compatibilidad con versiones anteriores al cambiar el formato serializado para cualquier función agregada en el futuro.
-
-## Servidor {#server}
-
-El servidor implementa varias interfaces diferentes:
-
--   Una interfaz HTTP para cualquier cliente extranjero.
--   Una interfaz TCP para el cliente nativo de ClickHouse y para la comunicación entre servidores durante la ejecución de consultas distribuidas.
--   Una interfaz para transferir datos para la replicación.
-
-Internamente, es solo un servidor multiproceso primitivo sin corutinas ni fibras. Dado que el servidor no está diseñado para procesar una alta tasa de consultas simples, sino para procesar una tasa relativamente baja de consultas complejas, cada uno de ellos puede procesar una gran cantidad de datos para análisis.
-
-El servidor inicializa el `Context` clase con el entorno necesario para la ejecución de consultas: la lista de bases de datos disponibles, usuarios y derechos de acceso, configuración, clústeres, la lista de procesos, el registro de consultas, etc. Los intérpretes utilizan este entorno.
-
-Mantenemos una compatibilidad total con versiones anteriores y posteriores para el protocolo TCP del servidor: los clientes antiguos pueden hablar con servidores nuevos y los nuevos clientes pueden hablar con servidores antiguos. Pero no queremos mantenerlo eternamente, y estamos eliminando el soporte para versiones antiguas después de aproximadamente un año.
-
-!!! note "Nota"
-    Para la mayoría de las aplicaciones externas, recomendamos usar la interfaz HTTP porque es simple y fácil de usar. El protocolo TCP está más estrechamente vinculado a las estructuras de datos internas: utiliza un formato interno para pasar bloques de datos y utiliza marcos personalizados para datos comprimidos. No hemos lanzado una biblioteca C para ese protocolo porque requiere vincular la mayor parte de la base de código ClickHouse, lo cual no es práctico.
-
-## Ejecución de consultas distribuidas {#distributed-query-execution}
-
-Los servidores de una configuración de clúster son en su mayoría independientes. Puede crear un `Distributed` en uno o todos los servidores de un clúster. El `Distributed` table does not store data itself – it only provides a “view” a todas las tablas locales en varios nodos de un clúster. Cuando se SELECCIONA desde un `Distributed` tabla, reescribe esa consulta, elige nodos remotos de acuerdo con la configuración de equilibrio de carga y les envía la consulta. El `Distributed` table solicita a los servidores remotos que procesen una consulta hasta una etapa en la que se pueden fusionar resultados intermedios de diferentes servidores. Luego recibe los resultados intermedios y los fusiona. La tabla distribuida intenta distribuir tanto trabajo como sea posible a servidores remotos y no envía muchos datos intermedios a través de la red.
-
-Las cosas se vuelven más complicadas cuando tiene subconsultas en cláusulas IN o JOIN, y cada una de ellas usa un `Distributed` tabla. Tenemos diferentes estrategias para la ejecución de estas consultas.
-
-No existe un plan de consulta global para la ejecución de consultas distribuidas. Cada nodo tiene su plan de consulta local para su parte del trabajo. Solo tenemos una ejecución simple de consultas distribuidas de un solo paso: enviamos consultas para nodos remotos y luego fusionamos los resultados. Pero esto no es factible para consultas complicadas con alta cardinalidad GROUP BY o con una gran cantidad de datos temporales para JOIN. En tales casos, necesitamos “reshuffle” datos entre servidores, lo que requiere una coordinación adicional. ClickHouse no admite ese tipo de ejecución de consultas, y tenemos que trabajar en ello.
-
-## Árbol de fusión {#merge-tree}
-
-`MergeTree` es una familia de motores de almacenamiento que admite la indexación por clave principal. La clave principal puede ser una tupla arbitraria de columnas o expresiones. Datos en un `MergeTree` se almacena en “parts”. Cada parte almacena los datos en el orden de la clave principal, por lo que la tupla de la clave principal ordena los datos lexicográficamente. Todas las columnas de la tabla se almacenan en `column.bin` archivos en estas partes. Los archivos consisten en bloques comprimidos. Cada bloque suele ser de 64 KB a 1 MB de datos sin comprimir, dependiendo del tamaño del valor promedio. Los bloques constan de valores de columna colocados contiguamente uno tras otro. Los valores de columna están en el mismo orden para cada columna (la clave principal define el orden), por lo que cuando itera por muchas columnas, obtiene valores para las filas correspondientes.
-
-La clave principal en sí es “sparse”. No aborda cada fila, sino solo algunos rangos de datos. Separado `primary.idx` file tiene el valor de la clave principal para cada fila N-ésima, donde se llama N `index_granularity` (generalmente, N = 8192). Además, para cada columna, tenemos `column.mrk` archivos con “marks,” que son desplazamientos a cada fila N-ésima en el archivo de datos. Cada marca es un par: el desplazamiento en el archivo al comienzo del bloque comprimido y el desplazamiento en el bloque descomprimido al comienzo de los datos. Por lo general, los bloques comprimidos están alineados por marcas, y el desplazamiento en el bloque descomprimido es cero. Datos para `primary.idx` siempre reside en la memoria, y los datos para `column.mrk` archivos se almacena en caché.
-
-Cuando vamos a leer algo de una parte en `MergeTree` miramos `primary.idx` datos y localice rangos que podrían contener datos solicitados, luego mire `column.mrk` datos y calcular compensaciones para dónde comenzar a leer esos rangos. Debido a la escasez, el exceso de datos puede ser leído. ClickHouse no es adecuado para una gran carga de consultas de puntos simples, porque todo el rango con `index_granularity` se deben leer filas para cada clave, y todo el bloque comprimido debe descomprimirse para cada columna. Hicimos que el índice sea disperso porque debemos poder mantener billones de filas por único servidor sin un consumo de memoria notable para el índice. Además, debido a que la clave principal es escasa, no es única: no puede verificar la existencia de la clave en la tabla en el momento de INSERTAR. Podría tener muchas filas con la misma clave en una tabla.
-
-Cuando `INSERT` un montón de datos en `MergeTree`, ese grupo está ordenado por orden de clave primaria y forma una nueva parte. Hay subprocesos de fondo que seleccionan periódicamente algunas partes y las fusionan en una sola parte ordenada para mantener el número de partes relativamente bajo. Es por eso que se llama `MergeTree`. Por supuesto, la fusión conduce a “write amplification”. Todas las partes son inmutables: solo se crean y eliminan, pero no se modifican. Cuando se ejecuta SELECT, contiene una instantánea de la tabla (un conjunto de partes). Después de la fusión, también mantenemos las piezas viejas durante algún tiempo para facilitar la recuperación después de la falla, por lo que si vemos que alguna parte fusionada probablemente esté rota, podemos reemplazarla con sus partes de origen.
-
-`MergeTree` no es un árbol de LSM porque no contiene “memtable” y “log”: inserted data is written directly to the filesystem. This makes it suitable only to INSERT data in batches, not by individual row and not very frequently – about once per second is ok, but a thousand times a second is not. We did it this way for simplicity's sake, and because we are already inserting data in batches in our applications.
-
-> Las tablas MergeTree solo pueden tener un índice (primario): no hay índices secundarios. Sería bueno permitir múltiples representaciones físicas bajo una tabla lógica, por ejemplo, para almacenar datos en más de un orden físico o incluso para permitir representaciones con datos preagregados junto con datos originales.
-
-Hay motores MergeTree que están haciendo un trabajo adicional durante las fusiones en segundo plano. Los ejemplos son `CollapsingMergeTree` y `AggregatingMergeTree`. Esto podría tratarse como soporte especial para actualizaciones. Tenga en cuenta que estas no son actualizaciones reales porque los usuarios generalmente no tienen control sobre el tiempo en que se ejecutan las fusiones en segundo plano y los datos en un `MergeTree` casi siempre se almacena en más de una parte, no en forma completamente fusionada.
-
-## Replicación {#replication}
-
-La replicación en ClickHouse se puede configurar por tabla. Podría tener algunas tablas replicadas y otras no replicadas en el mismo servidor. También puede tener tablas replicadas de diferentes maneras, como una tabla con replicación de dos factores y otra con replicación de tres factores.
-
-La replicación se implementa en el `ReplicatedMergeTree` motor de almacenamiento. El camino en `ZooKeeper` se especifica como un parámetro para el motor de almacenamiento. Todas las tablas con la misma ruta en `ZooKeeper` se convierten en réplicas entre sí: sincronizan sus datos y mantienen la coherencia. Las réplicas se pueden agregar y eliminar dinámicamente simplemente creando o soltando una tabla.
-
-La replicación utiliza un esquema multi-maestro asíncrono. Puede insertar datos en cualquier réplica que tenga una sesión con `ZooKeeper`, y los datos se replican en todas las demás réplicas de forma asíncrona. Como ClickHouse no admite UPDATE, la replicación está libre de conflictos. Como no hay reconocimiento de quórum de inserciones, los datos recién insertados pueden perderse si un nodo falla.
-
-Los metadatos para la replicación se almacenan en ZooKeeper. Hay un registro de replicación que enumera las acciones que se deben realizar. Las acciones son: obtener parte; fusionar partes; soltar una partición, etc. Cada réplica copia el registro de replicación en su cola y, a continuación, ejecuta las acciones desde la cola. Por ejemplo, en la inserción, el “get the part” la acción se crea en el registro y cada réplica descarga esa parte. Las fusiones se coordinan entre réplicas para obtener resultados idénticos en bytes. Todas las piezas se combinan de la misma manera en todas las réplicas. Se logra eligiendo una réplica como líder, y esa réplica inicia fusiones y escrituras “merge parts” acciones al registro.
-
-La replicación es física: solo las partes comprimidas se transfieren entre nodos, no consultas. Las fusiones se procesan en cada réplica de forma independiente en la mayoría de los casos para reducir los costos de red al evitar la amplificación de la red. Las piezas combinadas grandes se envían a través de la red solo en casos de retraso de replicación significativo.
-
-Además, cada réplica almacena su estado en ZooKeeper como el conjunto de piezas y sus sumas de comprobación. Cuando el estado en el sistema de archivos local difiere del estado de referencia en ZooKeeper, la réplica restaura su coherencia descargando partes faltantes y rotas de otras réplicas. Cuando hay algunos datos inesperados o rotos en el sistema de archivos local, ClickHouse no los elimina, sino que los mueve a un directorio separado y los olvida.
-
-!!! note "Nota"
-    El clúster ClickHouse consta de fragmentos independientes y cada fragmento consta de réplicas. El clúster es **no elástico**, por lo tanto, después de agregar un nuevo fragmento, los datos no se reequilibran automáticamente entre fragmentos. En su lugar, se supone que la carga del clúster debe ajustarse para que sea desigual. Esta implementación le da más control, y está bien para clústeres relativamente pequeños, como decenas de nodos. Pero para los clústeres con cientos de nodos que estamos utilizando en producción, este enfoque se convierte en un inconveniente significativo. Debemos implementar un motor de tablas que abarque todo el clúster con regiones replicadas dinámicamente que puedan dividirse y equilibrarse entre clústeres automáticamente.
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/development/architecture/) ##}
diff --git a/docs/es/development/browse-code.md b/docs/es/development/browse-code.md
deleted file mode 100644
index ca031ad03f3a..000000000000
--- a/docs/es/development/browse-code.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 63
-toc_title: "Buscar c\xF3digo fuente"
----
-
-# Examinar el código fuente de ClickHouse {#browse-clickhouse-source-code}
-
-Usted puede utilizar **Woboq** navegador de código en línea disponible [aqui](https://clickhouse.tech/codebrowser/html_report/ClickHouse/src/index.html). Proporciona navegación de código y resaltado semántico, búsqueda e indexación. La instantánea de código se actualiza diariamente.
-
-Además, puede navegar por las fuentes en [GitHub](https://github.com/ClickHouse/ClickHouse) como de costumbre.
-
-Si está interesado en qué IDE usar, recomendamos CLion, QT Creator, VS Code y KDevelop (con advertencias). Puedes usar cualquier IDE favorito. Vim y Emacs también cuentan.
diff --git a/docs/es/development/build-cross-arm.md b/docs/es/development/build-cross-arm.md
deleted file mode 100644
index 2758e9a0e948..000000000000
--- a/docs/es/development/build-cross-arm.md
+++ /dev/null
@@ -1,43 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 67
-toc_title: "C\xF3mo construir ClickHouse en Linux para AARCH64 (ARM64)"
----
-
-# Cómo construir ClickHouse en Linux para la arquitectura AARCH64 (ARM64) {#how-to-build-clickhouse-on-linux-for-aarch64-arm64-architecture}
-
-Esto es para el caso cuando tiene una máquina Linux y desea usarla para compilar `clickhouse` binario que se ejecutará en otra máquina Linux con arquitectura de CPU AARCH64. Esto está destinado a las comprobaciones de integración continua que se ejecutan en servidores Linux.
-
-La compilación cruzada para AARCH64 se basa en el [Instrucciones de construcción](build.md), seguirlos primero.
-
-# Instalar Clang-8 {#install-clang-8}
-
-Siga las instrucciones de https://apt.llvm.org/ para la configuración de Ubuntu o Debian.
-Por ejemplo, en Ubuntu Bionic puede usar los siguientes comandos:
-
-``` bash
-echo "deb [trusted=yes] http://apt.llvm.org/bionic/ llvm-toolchain-bionic-8 main" | sudo tee /etc/apt/sources.list.d/llvm.list
-sudo apt-get update
-sudo apt-get install clang-8
-```
-
-# Instalar conjunto de herramientas de compilación cruzada {#install-cross-compilation-toolset}
-
-``` bash
-cd ClickHouse
-mkdir -p build-aarch64/cmake/toolchain/linux-aarch64
-wget 'https://developer.arm.com/-/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz?revision=2e88a73f-d233-4f96-b1f4-d8b36e9bb0b9&la=en' -O gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz
-tar xJf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C build-aarch64/cmake/toolchain/linux-aarch64 --strip-components=1
-```
-
-# Construir ClickHouse {#build-clickhouse}
-
-``` bash
-cd ClickHouse
-mkdir build-arm64
-CC=clang-8 CXX=clang++-8 cmake . -Bbuild-arm64 -DCMAKE_TOOLCHAIN_FILE=cmake/linux/toolchain-aarch64.cmake
-ninja -C build-arm64
-```
-
-El binario resultante se ejecutará solo en Linux con la arquitectura de CPU AARCH64.
diff --git a/docs/es/development/build-cross-osx.md b/docs/es/development/build-cross-osx.md
deleted file mode 100644
index d00e57c5d310..000000000000
--- a/docs/es/development/build-cross-osx.md
+++ /dev/null
@@ -1,64 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 66
-toc_title: "C\xF3mo construir ClickHouse en Linux para Mac OS X"
----
-
-# Cómo construir ClickHouse en Linux para Mac OS X {#how-to-build-clickhouse-on-linux-for-mac-os-x}
-
-Esto es para el caso cuando tiene una máquina Linux y desea usarla para compilar `clickhouse` Esto está destinado a las comprobaciones de integración continuas que se ejecutan en servidores Linux. Si desea crear ClickHouse directamente en Mac OS X, continúe con [otra instrucción](build-osx.md).
-
-La compilación cruzada para Mac OS X se basa en el [Instrucciones de construcción](build.md), seguirlos primero.
-
-# Instalar Clang-8 {#install-clang-8}
-
-Siga las instrucciones de https://apt.llvm.org/ para la configuración de Ubuntu o Debian.
-Por ejemplo, los comandos para Bionic son como:
-
-``` bash
-sudo echo "deb [trusted=yes] http://apt.llvm.org/bionic/ llvm-toolchain-bionic-8 main" >> /etc/apt/sources.list
-sudo apt-get install clang-8
-```
-
-# Instalar conjunto de herramientas de compilación cruzada {#install-cross-compilation-toolset}
-
-Recordemos la ruta donde instalamos `cctools` como ${CCTOOLS}
-
-``` bash
-mkdir ${CCTOOLS}
-
-git clone https://github.com/tpoechtrager/apple-libtapi.git
-cd apple-libtapi
-INSTALLPREFIX=${CCTOOLS} ./build.sh
-./install.sh
-cd ..
-
-git clone https://github.com/tpoechtrager/cctools-port.git
-cd cctools-port/cctools
-./configure --prefix=${CCTOOLS} --with-libtapi=${CCTOOLS} --target=x86_64-apple-darwin
-make install
-```
-
-Además, necesitamos descargar macOS X SDK en el árbol de trabajo.
-
-``` bash
-cd ClickHouse
-wget 'https://github.com/phracker/MacOSX-SDKs/releases/download/10.15/MacOSX10.15.sdk.tar.xz'
-mkdir -p build-darwin/cmake/toolchain/darwin-x86_64
-tar xJf MacOSX10.15.sdk.tar.xz -C build-darwin/cmake/toolchain/darwin-x86_64 --strip-components=1
-```
-
-# Construir ClickHouse {#build-clickhouse}
-
-``` bash
-cd ClickHouse
-mkdir build-osx
-CC=clang-8 CXX=clang++-8 cmake . -Bbuild-osx -DCMAKE_TOOLCHAIN_FILE=cmake/darwin/toolchain-x86_64.cmake \
-    -DCMAKE_AR:FILEPATH=${CCTOOLS}/bin/x86_64-apple-darwin-ar \
-    -DCMAKE_RANLIB:FILEPATH=${CCTOOLS}/bin/x86_64-apple-darwin-ranlib \
-    -DLINKER_NAME=${CCTOOLS}/bin/x86_64-apple-darwin-ld
-ninja -C build-osx
-```
-
-El binario resultante tendrá un formato ejecutable Mach-O y no se puede ejecutar en Linux.
diff --git a/docs/es/development/build-osx.md b/docs/es/development/build-osx.md
deleted file mode 100644
index 39eba3897989..000000000000
--- a/docs/es/development/build-osx.md
+++ /dev/null
@@ -1,93 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 65
-toc_title: "C\xF3mo crear ClickHouse en Mac OS X"
----
-
-# Cómo crear ClickHouse en Mac OS X {#how-to-build-clickhouse-on-mac-os-x}
-
-Build debería funcionar en Mac OS X 10.15 (Catalina)
-
-## Instalar Homebrew {#install-homebrew}
-
-``` bash
-$ /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
-```
-
-## Instalar compiladores, herramientas y bibliotecas necesarios {#install-required-compilers-tools-and-libraries}
-
-``` bash
-$ brew install cmake ninja libtool gettext
-```
-
-## Fuentes de ClickHouse de pago {#checkout-clickhouse-sources}
-
-``` bash
-$ git clone --recursive git@github.com:ClickHouse/ClickHouse.git
-```
-
-o
-
-``` bash
-$ git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-
-$ cd ClickHouse
-```
-
-## Construir ClickHouse {#build-clickhouse}
-
-``` bash
-$ mkdir build
-$ cd build
-$ cmake .. -DCMAKE_CXX_COMPILER=`which clang++` -DCMAKE_C_COMPILER=`which clang`
-$ ninja
-$ cd ..
-```
-
-## Advertencia {#caveats}
-
-Si tiene la intención de ejecutar clickhouse-server, asegúrese de aumentar la variable maxfiles del sistema.
-
-!!! info "Nota"
-    Tendrás que usar sudo.
-
-Para ello, cree el siguiente archivo:
-
-/Library/LaunchDaemons/limit.maxfiles.lista:
-
-``` xml
-<?xml version="1.0" encoding="UTF-8"?>
-<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"
-        "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
-<plist version="1.0">
-  <dict>
-    <key>Label</key>
-    <string>limit.maxfiles</string>
-    <key>ProgramArguments</key>
-    <array>
-      <string>launchctl</string>
-      <string>limit</string>
-      <string>maxfiles</string>
-      <string>524288</string>
-      <string>524288</string>
-    </array>
-    <key>RunAtLoad</key>
-    <true/>
-    <key>ServiceIPC</key>
-    <false/>
-  </dict>
-</plist>
-```
-
-Ejecute el siguiente comando:
-
-``` bash
-$ sudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist
-```
-
-Reiniciar.
-
-Para verificar si está funcionando, puede usar `ulimit -n` comando.
-
-[Artículo Original](https://clickhouse.tech/docs/en/development/build_osx/) <!--hide-->
diff --git a/docs/es/development/build.md b/docs/es/development/build.md
deleted file mode 100644
index 42cd9b5433f0..000000000000
--- a/docs/es/development/build.md
+++ /dev/null
@@ -1,141 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 64
-toc_title: "C\xF3mo crear ClickHouse en Linux"
----
-
-# Cómo construir ClickHouse para el desarrollo {#how-to-build-clickhouse-for-development}
-
-El siguiente tutorial se basa en el sistema Ubuntu Linux.
-Con los cambios apropiados, también debería funcionar en cualquier otra distribución de Linux.
-Plataformas compatibles: x86_64 y AArch64. El soporte para Power9 es experimental.
-
-## Instalar Git, CMake, Python y Ninja {#install-git-cmake-python-and-ninja}
-
-``` bash
-$ sudo apt-get install git cmake python ninja-build
-```
-
-O cmake3 en lugar de cmake en sistemas más antiguos.
-
-## Instalar GCC 10 {#install-gcc-10}
-
-Hay varias formas de hacer esto.
-
-### Instalar desde un paquete PPA {#install-from-a-ppa-package}
-
-``` bash
-$ sudo apt-get install software-properties-common
-$ sudo apt-add-repository ppa:ubuntu-toolchain-r/test
-$ sudo apt-get update
-$ sudo apt-get install gcc-10 g++-10
-```
-
-### Instalar desde fuentes {#install-from-sources}
-
-Mira [Sistema abierto.](https://github.com/ClickHouse/ClickHouse/blob/master/utils/ci/build-gcc-from-sources.sh)
-
-## Usar GCC 10 para compilaciones {#use-gcc-10-for-builds}
-
-``` bash
-$ export CC=gcc-10
-$ export CXX=g++-10
-```
-
-## Fuentes de ClickHouse de pago {#checkout-clickhouse-sources}
-
-``` bash
-$ git clone --recursive git@github.com:ClickHouse/ClickHouse.git
-```
-
-o
-
-``` bash
-$ git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-```
-
-## Construir ClickHouse {#build-clickhouse}
-
-``` bash
-$ cd ClickHouse
-$ mkdir build
-$ cd build
-$ cmake ..
-$ ninja
-$ cd ..
-```
-
-Para crear un ejecutable, ejecute `ninja clickhouse`.
-Esto creará el `programs/clickhouse` ejecutable, que se puede usar con `client` o `server` argumento.
-
-# Cómo construir ClickHouse en cualquier Linux {#how-to-build-clickhouse-on-any-linux}
-
-La compilación requiere los siguientes componentes:
-
--   Git (se usa solo para verificar las fuentes, no es necesario para la compilación)
--   CMake 3.10 o más reciente
--   Ninja (recomendado) o Hacer
--   Compilador de C ++: gcc 10 o clang 8 o más reciente
--   Enlazador: lld u oro (el clásico GNU ld no funcionará)
--   Python (solo se usa dentro de la compilación LLVM y es opcional)
-
-Si todos los componentes están instalados, puede compilar de la misma manera que los pasos anteriores.
-
-Ejemplo para Ubuntu Eoan:
-
-    sudo apt update
-    sudo apt install git cmake ninja-build g++ python
-    git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-    mkdir build && cd build
-    cmake ../ClickHouse
-    ninja
-
-Ejemplo de OpenSUSE Tumbleweed:
-
-    sudo zypper install git cmake ninja gcc-c++ python lld
-    git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-    mkdir build && cd build
-    cmake ../ClickHouse
-    ninja
-
-Ejemplo de Fedora Rawhide:
-
-    sudo yum update
-    yum --nogpg install git cmake make gcc-c++ python3
-    git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-    mkdir build && cd build
-    cmake ../ClickHouse
-    make -j $(nproc)
-
-# No tienes que construir ClickHouse {#you-dont-have-to-build-clickhouse}
-
-ClickHouse está disponible en binarios y paquetes preconstruidos. Los binarios son portátiles y se pueden ejecutar en cualquier tipo de Linux.
-
-Están diseñados para lanzamientos estables, preestablecidos y de prueba, siempre que para cada compromiso con el maestro y para cada solicitud de extracción.
-
-Para encontrar la construcción más fresca de `master`, ir a [se compromete página](https://github.com/ClickHouse/ClickHouse/commits/master), haga clic en la primera marca de verificación verde o cruz roja cerca de confirmar, y haga clic en “Details” enlace justo después “ClickHouse Build Check”.
-
-# Cómo construir el paquete Debian ClickHouse {#how-to-build-clickhouse-debian-package}
-
-## Instalar Git y Pbuilder {#install-git-and-pbuilder}
-
-``` bash
-$ sudo apt-get update
-$ sudo apt-get install git python pbuilder debhelper lsb-release fakeroot sudo debian-archive-keyring debian-keyring
-```
-
-## Fuentes de ClickHouse de pago {#checkout-clickhouse-sources-1}
-
-``` bash
-$ git clone --recursive --branch master https://github.com/ClickHouse/ClickHouse.git
-$ cd ClickHouse
-```
-
-## Ejecutar secuencia de comandos de lanzamiento {#run-release-script}
-
-``` bash
-$ ./release
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/development/build/) <!--hide-->
diff --git a/docs/es/development/contrib.md b/docs/es/development/contrib.md
deleted file mode 100644
index 3f3013570e5f..000000000000
--- a/docs/es/development/contrib.md
+++ /dev/null
@@ -1,41 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 70
-toc_title: Bibliotecas de terceros utilizadas
----
-
-# Bibliotecas de terceros utilizadas {#third-party-libraries-used}
-
-| Biblioteca         | Licencia                                                                                                                                         |
-|--------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
-| base64             | [Licencia BSD de 2 cláusulas](https://github.com/aklomp/base64/blob/a27c565d1b6c676beaf297fe503c4518185666f7/LICENSE)                            |
-| impulsar           | [Licencia de software Boost 1.0](https://github.com/ClickHouse-Extras/boost-extra/blob/6883b40449f378019aec792f9983ce3afc7ff16e/LICENSE_1_0.txt) |
-| Bienvenido         | [MIT](https://github.com/google/brotli/blob/master/LICENSE)                                                                                      |
-| capnproto          | [MIT](https://github.com/capnproto/capnproto/blob/master/LICENSE)                                                                                |
-| Cctz               | [Licencia Apache 2.0](https://github.com/google/cctz/blob/4f9776a310f4952454636363def82c2bf6641d5f/LICENSE.txt)                                  |
-| doble conversión   | [Licencia de 3 cláusulas BSD](https://github.com/google/double-conversion/blob/cf2f0f3d547dc73b4612028a155b80536902ba02/LICENSE)                 |
-| FastMemcpy         | [MIT](https://github.com/ClickHouse/ClickHouse/blob/master/libs/libmemcpy/impl/LICENSE)                                                          |
-| Más información    | [Licencia de 3 cláusulas BSD](https://github.com/google/googletest/blob/master/LICENSE)                                                          |
-| H3                 | [Licencia Apache 2.0](https://github.com/uber/h3/blob/master/LICENSE)                                                                            |
-| hyperscan          | [Licencia de 3 cláusulas BSD](https://github.com/intel/hyperscan/blob/master/LICENSE)                                                            |
-| libcxxabi          | [BSD + MIT](https://github.com/ClickHouse/ClickHouse/blob/master/libs/libglibc-compatibility/libcxxabi/LICENSE.TXT)                              |
-| libdivide          | [Licencia Zlib](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/libdivide/LICENSE.txt)                                              |
-| libgsasl           | [Información adicional](https://github.com/ClickHouse-Extras/libgsasl/blob/3b8948a4042e34fb00b4fb987535dc9e02e39040/LICENSE)                     |
-| libhdfs3           | [Licencia Apache 2.0](https://github.com/ClickHouse-Extras/libhdfs3/blob/bd6505cbb0c130b0db695305b9a38546fa880e5a/LICENSE.txt)                   |
-| libmetrohash       | [Licencia Apache 2.0](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/libmetrohash/LICENSE)                                         |
-| libpcg-al azar     | [Licencia Apache 2.0](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/libpcg-random/LICENSE-APACHE.txt)                             |
-| Libressl           | [Licencia OpenSSL](https://github.com/ClickHouse-Extras/ssl/blob/master/COPYING)                                                                 |
-| Librdkafka         | [Licencia BSD de 2 cláusulas](https://github.com/edenhill/librdkafka/blob/363dcad5a23dc29381cc626620e68ae418b3af19/LICENSE)                      |
-| libwidechar_width | [CC0 1.0 Universal](https://github.com/ClickHouse/ClickHouse/blob/master/libs/libwidechar_width/LICENSE)                                         |
-| llvm               | [Licencia de 3 cláusulas BSD](https://github.com/ClickHouse-Extras/llvm/blob/163def217817c90fb982a6daf384744d8472b92b/llvm/LICENSE.TXT)          |
-| lz4                | [Licencia BSD de 2 cláusulas](https://github.com/lz4/lz4/blob/c10863b98e1503af90616ae99725ecd120265dfb/LICENSE)                                  |
-| mariadb-conector-c | [Información adicional](https://github.com/ClickHouse-Extras/mariadb-connector-c/blob/3.1/COPYING.LIB)                                           |
-| murmurhash         | [Dominio público](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/murmurhash/LICENSE)                                               |
-| pdqsort            | [Licencia Zlib](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/pdqsort/license.txt)                                                |
-| Poco               | [Boost Software License - Versión 1.0](https://github.com/ClickHouse-Extras/poco/blob/fe5505e56c27b6ecb0dcbc40c49dc2caf4e9637f/LICENSE)          |
-| protobuf           | [Licencia de 3 cláusulas BSD](https://github.com/ClickHouse-Extras/protobuf/blob/12735370922a35f03999afff478e1c6d7aa917a4/LICENSE)               |
-| Re2                | [Licencia de 3 cláusulas BSD](https://github.com/google/re2/blob/7cf8b88e8f70f97fd4926b56aa87e7f53b2717e0/LICENSE)                               |
-| UnixODBC           | [Información adicional](https://github.com/ClickHouse-Extras/UnixODBC/tree/b0ad30f7f6289c12b76f04bfb9d466374bb32168)                             |
-| Sistema abierto.   | [Licencia Zlib](https://github.com/ClickHouse-Extras/zlib-ng/blob/develop/LICENSE.md)                                                            |
-| zstd               | [Licencia de 3 cláusulas BSD](https://github.com/facebook/zstd/blob/dev/LICENSE)                                                                 |
diff --git a/docs/es/development/developer-instruction.md b/docs/es/development/developer-instruction.md
deleted file mode 100644
index 0ce5d0b457ac..000000000000
--- a/docs/es/development/developer-instruction.md
+++ /dev/null
@@ -1,287 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: "La instrucci\xF3n para desarrolladores de ClickHouse para principiantes"
----
-
-La construcción de ClickHouse es compatible con Linux, FreeBSD y Mac OS X.
-
-# Si utiliza Windows {#if-you-use-windows}
-
-Si usa Windows, necesita crear una máquina virtual con Ubuntu. Para comenzar a trabajar con una máquina virtual, instale VirtualBox. Puede descargar Ubuntu desde el sitio web: https://www.ubuntu.com/#download. Por favor, cree una máquina virtual a partir de la imagen descargada (debe reservar al menos 4 GB de RAM para ello). Para ejecutar un terminal de línea de comandos en Ubuntu, busque un programa que contenga la palabra “terminal” en su nombre (gnome-terminal, konsole etc.) o simplemente presione Ctrl + Alt + T.
-
-# Si utiliza un sistema de 32 bits {#if-you-use-a-32-bit-system}
-
-ClickHouse no puede funcionar ni construir en un sistema de 32 bits. Debe adquirir acceso a un sistema de 64 bits y puede continuar leyendo.
-
-# Creación de un repositorio en GitHub {#creating-a-repository-on-github}
-
-Para comenzar a trabajar con el repositorio de ClickHouse, necesitará una cuenta de GitHub.
-
-Probablemente ya tenga uno, pero si no lo hace, regístrese en https://github.com . En caso de que no tenga claves SSH, debe generarlas y luego cargarlas en GitHub. Es necesario para enviar a través de sus parches. También es posible usar las mismas claves SSH que usa con cualquier otro servidor SSH, probablemente ya las tenga.
-
-Cree una bifurcación del repositorio ClickHouse. Para hacerlo por favor haga clic en el “fork” botón en la esquina superior derecha en https://github.com/ClickHouse/ClickHouse . Se bifurcará su propia copia de ClickHouse/ClickHouse a su cuenta.
-
-El proceso de desarrollo consiste en comprometer primero los cambios previstos en su bifurcación de ClickHouse y luego crear un “pull request” para que estos cambios sean aceptados en el repositorio principal (ClickHouse / ClickHouse).
-
-Para trabajar con repositorios git, instale `git`.
-
-Para hacer eso en Ubuntu, ejecutaría en la terminal de línea de comandos:
-
-    sudo apt update
-    sudo apt install git
-
-Puede encontrar un breve manual sobre el uso de Git aquí: https://education.github.com/git-cheat-sheet-education.pdf .
-Para obtener un manual detallado sobre Git, consulte https://git-scm.com/book/en/v2 .
-
-# Clonación de un repositorio en su máquina de desarrollo {#cloning-a-repository-to-your-development-machine}
-
-A continuación, debe descargar los archivos fuente en su máquina de trabajo. Esto se llama “to clone a repository” porque crea una copia local del repositorio en su máquina de trabajo.
-
-En el terminal de línea de comandos, ejecute:
-
-    git clone --recursive git@github.com:your_github_username/ClickHouse.git
-    cd ClickHouse
-
-Nota: por favor, sustituye *your_github_username* con lo que es apropiado!
-
-Este comando creará un directorio `ClickHouse` que contiene la copia de trabajo del proyecto.
-
-Es importante que la ruta al directorio de trabajo no contenga espacios en blanco, ya que puede ocasionar problemas con la ejecución del sistema de compilación.
-
-Tenga en cuenta que el repositorio ClickHouse utiliza `submodules`. That is what the references to additional repositories are called (i.e. external libraries on which the project depends). It means that when cloning the repository you need to specify the `--recursive` como en el ejemplo anterior. Si el repositorio se ha clonado sin submódulos, para descargarlos debe ejecutar lo siguiente:
-
-    git submodule init
-    git submodule update
-
-Puede verificar el estado con el comando: `git submodule status`.
-
-Si recibe el siguiente mensaje de error:
-
-    Permission denied (publickey).
-    fatal: Could not read from remote repository.
-
-    Please make sure you have the correct access rights
-    and the repository exists.
-
-Por lo general, significa que faltan las claves SSH para conectarse a GitHub. Estas teclas se encuentran normalmente en `~/.ssh`. Para que las claves SSH sean aceptadas, debe cargarlas en la sección de configuración de la interfaz de usuario de GitHub.
-
-También puede clonar el repositorio a través del protocolo https:
-
-    git clone https://github.com/ClickHouse/ClickHouse.git
-
-Sin embargo, esto no le permitirá enviar los cambios al servidor. Aún puede usarlo temporalmente y agregar las claves SSH más tarde reemplazando la dirección remota del repositorio con `git remote` comando.
-
-También puede agregar la dirección original del repositorio de ClickHouse a su repositorio local para extraer actualizaciones desde allí:
-
-    git remote add upstream git@github.com:ClickHouse/ClickHouse.git
-
-Después de ejecutar con éxito este comando, podrá extraer actualizaciones del repositorio principal de ClickHouse ejecutando `git pull upstream master`.
-
-## Trabajar con submódulos {#working-with-submodules}
-
-Trabajar con submódulos en git podría ser doloroso. Los siguientes comandos ayudarán a administrarlo:
-
-    # ! each command accepts --recursive
-    # Update remote URLs for submodules. Barely rare case
-    git submodule sync
-    # Add new submodules
-    git submodule init
-    # Update existing submodules to the current state
-    git submodule update
-    # Two last commands could be merged together
-    git submodule update --init
-
-Los siguientes comandos le ayudarían a restablecer todos los submódulos al estado inicial (!¡ADVERTENCIA! - cualquier cambio en el interior será eliminado):
-
-    # Synchronizes submodules' remote URL with .gitmodules
-    git submodule sync --recursive
-    # Update the registered submodules with initialize not yet initialized
-    git submodule update --init --recursive
-    # Reset all changes done after HEAD
-    git submodule foreach git reset --hard
-    # Clean files from .gitignore
-    git submodule foreach git clean -xfd
-    # Repeat last 4 commands for all submodule
-    git submodule foreach git submodule sync --recursive
-    git submodule foreach git submodule update --init --recursive
-    git submodule foreach git submodule foreach git reset --hard
-    git submodule foreach git submodule foreach git clean -xfd
-
-# Sistema de construcción {#build-system}
-
-ClickHouse utiliza CMake y Ninja para la construcción.
-
-CMake - un sistema de meta-construcción que puede generar archivos Ninja (tareas de construcción).
-Ninja: un sistema de compilación más pequeño con un enfoque en la velocidad utilizada para ejecutar esas tareas generadas por cmake.
-
-Para instalar en Ubuntu, Debian o Mint run `sudo apt install cmake ninja-build`.
-
-En CentOS, RedHat se ejecuta `sudo yum install cmake ninja-build`.
-
-Si usa Arch o Gentoo, probablemente lo sepa usted mismo cómo instalar CMake.
-
-Para instalar CMake y Ninja en Mac OS X, primero instale Homebrew y luego instale todo lo demás a través de brew:
-
-    /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
-    brew install cmake ninja
-
-A continuación, verifique la versión de CMake: `cmake --version`. Si está por debajo de 3.3, debe instalar una versión más reciente desde el sitio web: https://cmake.org/download/.
-
-# Bibliotecas externas opcionales {#optional-external-libraries}
-
-ClickHouse utiliza varias bibliotecas externas para la construcción. Todos ellos no necesitan ser instalados por separado, ya que se construyen junto con ClickHouse a partir de las fuentes ubicadas en los submódulos. Puede consultar la lista en `contrib`.
-
-# Compilador de C ++ {#c-compiler}
-
-Los compiladores GCC a partir de la versión 10 y Clang versión 8 o superior son compatibles para construir ClickHouse.
-
-Las compilaciones oficiales de Yandex actualmente usan GCC porque genera código de máquina de un rendimiento ligeramente mejor (con una diferencia de hasta varios por ciento según nuestros puntos de referencia). Y Clang es más conveniente para el desarrollo generalmente. Sin embargo, nuestra plataforma de integración continua (CI) ejecuta verificaciones de aproximadamente una docena de combinaciones de compilación.
-
-Para instalar GCC en Ubuntu, ejecute: `sudo apt install gcc g++`
-
-Compruebe la versión de gcc: `gcc --version`. Si está por debajo de 9, siga las instrucciones aquí: https://clickhouse.tech/docs/es/development/build/#install-gcc-10.
-
-La compilación de Mac OS X solo es compatible con Clang. Sólo tiene que ejecutar `brew install llvm`
-
-Si decide utilizar Clang, también puede instalar `libc++` y `lld` si usted sabe lo que es. Utilizar `ccache` también se recomienda.
-
-# El proceso de construcción {#the-building-process}
-
-Ahora que está listo para construir ClickHouse, le recomendamos que cree un directorio separado `build` dentro `ClickHouse` que contendrá todos los de la generación de artefactos:
-
-    mkdir build
-    cd build
-
-Puede tener varios directorios diferentes (build_release, build_debug, etc.) para diferentes tipos de construcción.
-
-Mientras que dentro de la `build` directorio, configure su compilación ejecutando CMake. Antes de la primera ejecución, debe definir variables de entorno que especifiquen el compilador (compilador gcc versión 10 en este ejemplo).
-
-Linux:
-
-    export CC=gcc-10 CXX=g++-10
-    cmake ..
-
-Mac OS X:
-
-    export CC=clang CXX=clang++
-    cmake ..
-
-El `CC` variable especifica el compilador para C (abreviatura de C Compiler), y `CXX` variable indica qué compilador de C ++ se usará para compilar.
-
-Para una construcción más rápida, puede recurrir al `debug` tipo de compilación: una compilación sin optimizaciones. Para ese suministro el siguiente parámetro `-D CMAKE_BUILD_TYPE=Debug`:
-
-    cmake -D CMAKE_BUILD_TYPE=Debug ..
-
-Puede cambiar el tipo de compilación ejecutando este comando en el `build` directorio.
-
-Ejecutar ninja para construir:
-
-    ninja clickhouse-server clickhouse-client
-
-Solo los binarios requeridos se van a construir en este ejemplo.
-
-Si necesita construir todos los binarios (utilidades y pruebas), debe ejecutar ninja sin parámetros:
-
-    ninja
-
-La compilación completa requiere aproximadamente 30 GB de espacio libre en disco o 15 GB para construir los binarios principales.
-
-Cuando hay una gran cantidad de RAM disponible en la máquina de compilación, debe limitar el número de tareas de compilación que se ejecutan en paralelo con `-j` parámetro:
-
-    ninja -j 1 clickhouse-server clickhouse-client
-
-En máquinas con 4GB de RAM, se recomienda especificar 1, para 8GB de RAM `-j 2` se recomienda.
-
-Si recibe el mensaje: `ninja: error: loading 'build.ninja': No such file or directory`, significa que la generación de una configuración de compilación ha fallado y necesita inspeccionar el mensaje anterior.
-
-Cuando se inicie correctamente el proceso de construcción, verá el progreso de la compilación: el número de tareas procesadas y el número total de tareas.
-
-Al crear mensajes sobre archivos protobuf en la biblioteca libhdfs2, como `libprotobuf WARNING` puede aparecer. Afectan a nada y son seguros para ser ignorado.
-
-Tras la compilación exitosa, obtienes un archivo ejecutable `ClickHouse/<build_dir>/programs/clickhouse`:
-
-    ls -l programs/clickhouse
-
-# Ejecución del ejecutable construido de ClickHouse {#running-the-built-executable-of-clickhouse}
-
-Para ejecutar el servidor bajo el usuario actual, debe navegar hasta `ClickHouse/programs/server/` (situado fuera de `build`) y ejecutar:
-
-    ../../build/programs/clickhouse server
-
-En este caso, ClickHouse usará archivos de configuración ubicados en el directorio actual. Puede ejecutar `clickhouse server` desde cualquier directorio que especifique la ruta a un archivo de configuración como un parámetro de línea de comandos `--config-file`.
-
-Para conectarse a ClickHouse con clickhouse-client en otro terminal, vaya a `ClickHouse/build/programs/` y ejecutar `./clickhouse client`.
-
-Si usted consigue `Connection refused` mensaje en Mac OS X o FreeBSD, intente especificar la dirección de host 127.0.0.1:
-
-    clickhouse client --host 127.0.0.1
-
-Puede reemplazar la versión de producción del binario ClickHouse instalado en su sistema con su binario ClickHouse personalizado. Para ello, instale ClickHouse en su máquina siguiendo las instrucciones del sitio web oficial. A continuación, ejecute lo siguiente:
-
-    sudo service clickhouse-server stop
-    sudo cp ClickHouse/build/programs/clickhouse /usr/bin/
-    sudo service clickhouse-server start
-
-Tenga en cuenta que `clickhouse-client`, `clickhouse-server` y otros son enlaces simbólicos a los comúnmente compartidos `clickhouse` binario.
-
-También puede ejecutar su binario ClickHouse personalizado con el archivo de configuración del paquete ClickHouse instalado en su sistema:
-
-    sudo service clickhouse-server stop
-    sudo -u clickhouse ClickHouse/build/programs/clickhouse server --config-file /etc/clickhouse-server/config.xml
-
-# IDE (entorno de desarrollo integrado) {#ide-integrated-development-environment}
-
-Si no sabe qué IDE usar, le recomendamos que use CLion. CLion es un software comercial, pero ofrece un período de prueba gratuito de 30 días. También es gratuito para los estudiantes. CLion se puede usar tanto en Linux como en Mac OS X.
-
-KDevelop y QTCreator son otras excelentes alternativas de un IDE para desarrollar ClickHouse. KDevelop viene como un IDE muy útil aunque inestable. Si KDevelop se bloquea después de un tiempo al abrir el proyecto, debe hacer clic “Stop All” botón tan pronto como se ha abierto la lista de archivos del proyecto. Después de hacerlo, KDevelop debería estar bien para trabajar.
-
-Como editores de código simples, puede usar Sublime Text o Visual Studio Code, o Kate (todos los cuales están disponibles en Linux).
-
-Por si acaso, vale la pena mencionar que CLion crea `build` por sí mismo, también por sí mismo selecciona `debug` para el tipo de compilación, para la configuración usa una versión de CMake que está definida en CLion y no la instalada por usted, y finalmente, CLion usará `make` para ejecutar tareas de compilación en lugar de `ninja`. Este es un comportamiento normal, solo tenlo en cuenta para evitar confusiones.
-
-# Código de escritura {#writing-code}
-
-La descripción de la arquitectura ClickHouse se puede encontrar aquí: https://clickhouse.tech/docs/es/desarrollo/arquitectura/
-
-La Guía de estilo de código: https://clickhouse.tech/docs/en/development/style/
-
-Pruebas de escritura: https://clickhouse.tech/docs/en/development/tests/
-
-Lista de tareas: https://github.com/ClickHouse/ClickHouse/issues?q=is%3Aopen+is%3Aissue+label%3A%22easy+task%22
-
-# Datos de prueba {#test-data}
-
-El desarrollo de ClickHouse a menudo requiere cargar conjuntos de datos realistas. Es particularmente importante para las pruebas de rendimiento. Tenemos un conjunto especialmente preparado de datos anónimos de Yandex.Métrica. Se requiere, además, unos 3 GB de espacio libre en disco. Tenga en cuenta que estos datos no son necesarios para realizar la mayoría de las tareas de desarrollo.
-
-    sudo apt install wget xz-utils
-
-    wget https://datasets.clickhouse.tech/hits/tsv/hits_v1.tsv.xz
-    wget https://datasets.clickhouse.tech/visits/tsv/visits_v1.tsv.xz
-
-    xz -v -d hits_v1.tsv.xz
-    xz -v -d visits_v1.tsv.xz
-
-    clickhouse-client
-
-    CREATE DATABASE IF NOT EXISTS test
-
-    CREATE TABLE test.hits ( WatchID UInt64,  JavaEnable UInt8,  Title String,  GoodEvent Int16,  EventTime DateTime,  EventDate Date,  CounterID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RegionID UInt32,  UserID UInt64,  CounterClass Int8,  OS UInt8,  UserAgent UInt8,  URL String,  Referer String,  URLDomain String,  RefererDomain String,  Refresh UInt8,  IsRobot UInt8,  RefererCategories Array(UInt16),  URLCategories Array(UInt16),  URLRegions Array(UInt32),  RefererRegions Array(UInt32),  ResolutionWidth UInt16,  ResolutionHeight UInt16,  ResolutionDepth UInt8,  FlashMajor UInt8,  FlashMinor UInt8,  FlashMinor2 String,  NetMajor UInt8,  NetMinor UInt8,  UserAgentMajor UInt16,  UserAgentMinor FixedString(2),  CookieEnable UInt8,  JavascriptEnable UInt8,  IsMobile UInt8,  MobilePhone UInt8,  MobilePhoneModel String,  Params String,  IPNetworkID UInt32,  TraficSourceID Int8,  SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  IsArtifical UInt8,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  ClientTimeZone Int16,  ClientEventTime DateTime,  SilverlightVersion1 UInt8,  SilverlightVersion2 UInt8,  SilverlightVersion3 UInt32,  SilverlightVersion4 UInt16,  PageCharset String,  CodeVersion UInt32,  IsLink UInt8,  IsDownload UInt8,  IsNotBounce UInt8,  FUniqID UInt64,  HID UInt32,  IsOldCounter UInt8,  IsEvent UInt8,  IsParameter UInt8,  DontCountHits UInt8,  WithHash UInt8,  HitColor FixedString(1),  UTCEventTime DateTime,  Age UInt8,  Sex UInt8,  Income UInt8,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16),  RemoteIP UInt32,  RemoteIP6 FixedString(16),  WindowName Int32,  OpenerName Int32,  HistoryLength Int16,  BrowserLanguage FixedString(2),  BrowserCountry FixedString(2),  SocialNetwork String,  SocialAction String,  HTTPError UInt16,  SendTiming Int32,  DNSTiming Int32,  ConnectTiming Int32,  ResponseStartTiming Int32,  ResponseEndTiming Int32,  FetchTiming Int32,  RedirectTiming Int32,  DOMInteractiveTiming Int32,  DOMContentLoadedTiming Int32,  DOMCompleteTiming Int32,  LoadEventStartTiming Int32,  LoadEventEndTiming Int32,  NSToDOMContentLoadedTiming Int32,  FirstPaintTiming Int32,  RedirectCount Int8,  SocialSourceNetworkID UInt8,  SocialSourcePage String,  ParamPrice Int64,  ParamOrderID String,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16,  GoalsReached Array(UInt32),  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String,  UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String,  FromTag String,  HasGCLID UInt8,  RefererHash UInt64,  URLHash UInt64,  CLID UInt32,  YCLID UInt64,  ShareService String,  ShareURL String,  ShareTitle String,  `ParsedParams.Key1` Array(String),  `ParsedParams.Key2` Array(String),  `ParsedParams.Key3` Array(String),  `ParsedParams.Key4` Array(String),  `ParsedParams.Key5` Array(String),  `ParsedParams.ValueDouble` Array(Float64),  IslandID FixedString(16),  RequestNum UInt32,  RequestTry UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(EventDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID), EventTime);
-
-    CREATE TABLE test.visits ( CounterID UInt32,  StartDate Date,  Sign Int8,  IsNew UInt8,  VisitID UInt64,  UserID UInt64,  StartTime DateTime,  Duration UInt32,  UTCStartTime DateTime,  PageViews Int32,  Hits Int32,  IsBounce UInt8,  Referer String,  StartURL String,  RefererDomain String,  StartURLDomain String,  EndURL String,  LinkURL String,  IsDownload UInt8,  TraficSourceID Int8,  SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  PlaceID Int32,  RefererCategories Array(UInt16),  URLCategories Array(UInt16),  URLRegions Array(UInt32),  RefererRegions Array(UInt32),  IsYandex UInt8,  GoalReachesDepth Int32,  GoalReachesURL Int32,  GoalReachesAny Int32,  SocialSourceNetworkID UInt8,  SocialSourcePage String,  MobilePhoneModel String,  ClientEventTime DateTime,  RegionID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RemoteIP UInt32,  RemoteIP6 FixedString(16),  IPNetworkID UInt32,  SilverlightVersion3 UInt32,  CodeVersion UInt32,  ResolutionWidth UInt16,  ResolutionHeight UInt16,  UserAgentMajor UInt16,  UserAgentMinor UInt16,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  SilverlightVersion2 UInt8,  SilverlightVersion4 UInt16,  FlashVersion3 UInt16,  FlashVersion4 UInt16,  ClientTimeZone Int16,  OS UInt8,  UserAgent UInt8,  ResolutionDepth UInt8,  FlashMajor UInt8,  FlashMinor UInt8,  NetMajor UInt8,  NetMinor UInt8,  MobilePhone UInt8,  SilverlightVersion1 UInt8,  Age UInt8,  Sex UInt8,  Income UInt8,  JavaEnable UInt8,  CookieEnable UInt8,  JavascriptEnable UInt8,  IsMobile UInt8,  BrowserLanguage UInt16,  BrowserCountry UInt16,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16),  Params Array(String),  `Goals.ID` Array(UInt32),  `Goals.Serial` Array(UInt32),  `Goals.EventTime` Array(DateTime),  `Goals.Price` Array(Int64),  `Goals.OrderID` Array(String),  `Goals.CurrencyID` Array(UInt32),  WatchIDs Array(UInt64),  ParamSumPrice Int64,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16,  ClickLogID UInt64,  ClickEventID Int32,  ClickGoodEvent Int32,  ClickEventTime DateTime,  ClickPriorityID Int32,  ClickPhraseID Int32,  ClickPageID Int32,  ClickPlaceID Int32,  ClickTypeID Int32,  ClickResourceID Int32,  ClickCost UInt32,  ClickClientIP UInt32,  ClickDomainID UInt32,  ClickURL String,  ClickAttempt UInt8,  ClickOrderID UInt32,  ClickBannerID UInt32,  ClickMarketCategoryID UInt32,  ClickMarketPP UInt32,  ClickMarketCategoryName String,  ClickMarketPPName String,  ClickAWAPSCampaignName String,  ClickPageName String,  ClickTargetType UInt16,  ClickTargetPhraseID UInt64,  ClickContextType UInt8,  ClickSelectType Int8,  ClickOptions String,  ClickGroupBannerID Int32,  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String,  UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String,  FromTag String,  HasGCLID UInt8,  FirstVisit DateTime,  PredLastVisit Date,  LastVisit Date,  TotalVisits UInt32,  `TraficSource.ID` Array(Int8),  `TraficSource.SearchEngineID` Array(UInt16),  `TraficSource.AdvEngineID` Array(UInt8),  `TraficSource.PlaceID` Array(UInt16),  `TraficSource.SocialSourceNetworkID` Array(UInt8),  `TraficSource.Domain` Array(String),  `TraficSource.SearchPhrase` Array(String),  `TraficSource.SocialSourcePage` Array(String),  Attendance FixedString(16),  CLID UInt32,  YCLID UInt64,  NormalizedRefererHash UInt64,  SearchPhraseHash UInt64,  RefererDomainHash UInt64,  NormalizedStartURLHash UInt64,  StartURLDomainHash UInt64,  NormalizedEndURLHash UInt64,  TopLevelDomain UInt64,  URLScheme UInt64,  OpenstatServiceNameHash UInt64,  OpenstatCampaignIDHash UInt64,  OpenstatAdIDHash UInt64,  OpenstatSourceIDHash UInt64,  UTMSourceHash UInt64,  UTMMediumHash UInt64,  UTMCampaignHash UInt64,  UTMContentHash UInt64,  UTMTermHash UInt64,  FromHash UInt64,  WebVisorEnabled UInt8,  WebVisorActivity UInt32,  `ParsedParams.Key1` Array(String),  `ParsedParams.Key2` Array(String),  `ParsedParams.Key3` Array(String),  `ParsedParams.Key4` Array(String),  `ParsedParams.Key5` Array(String),  `ParsedParams.ValueDouble` Array(Float64),  `Market.Type` Array(UInt8),  `Market.GoalID` Array(UInt32),  `Market.OrderID` Array(String),  `Market.OrderPrice` Array(Int64),  `Market.PP` Array(UInt32),  `Market.DirectPlaceID` Array(UInt32),  `Market.DirectOrderID` Array(UInt32),  `Market.DirectBannerID` Array(UInt32),  `Market.GoodID` Array(String),  `Market.GoodName` Array(String),  `Market.GoodQuantity` Array(Int32),  `Market.GoodPrice` Array(Int64),  IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID);
-
-    clickhouse-client --max_insert_block_size 100000 --query "INSERT INTO test.hits FORMAT TSV" < hits_v1.tsv
-    clickhouse-client --max_insert_block_size 100000 --query "INSERT INTO test.visits FORMAT TSV" < visits_v1.tsv
-
-# Creación de solicitud de extracción {#creating-pull-request}
-
-Navega a tu repositorio de fork en la interfaz de usuario de GitHub. Si ha estado desarrollando en una sucursal, debe seleccionar esa sucursal. Habrá un “Pull request” botón situado en la pantalla. En esencia, esto significa “create a request for accepting my changes into the main repository”.
-
-Se puede crear una solicitud de extracción incluso si el trabajo aún no se ha completado. En este caso, por favor ponga la palabra “WIP” (trabajo en curso) al comienzo del título, se puede cambiar más tarde. Esto es útil para la revisión cooperativa y la discusión de los cambios, así como para ejecutar todas las pruebas disponibles. Es importante que proporcione una breve descripción de sus cambios, que más tarde se utilizará para generar registros de cambios de lanzamiento.
-
-Las pruebas comenzarán tan pronto como los empleados de Yandex etiqueten su PR con una etiqueta “can be tested”. The results of some first checks (e.g. code style) will come in within several minutes. Build check results will arrive within half an hour. And the main set of tests will report itself within an hour.
-
-El sistema preparará compilaciones binarias ClickHouse para su solicitud de extracción individualmente. Para recuperar estas compilaciones, haga clic en “Details” junto al link “ClickHouse build check” en la lista de cheques. Allí encontrará enlaces directos a la construcción.deb paquetes de ClickHouse que puede implementar incluso en sus servidores de producción (si no tiene miedo).
-
-Lo más probable es que algunas de las compilaciones fallen las primeras veces. Esto se debe al hecho de que verificamos las compilaciones tanto con gcc como con clang, con casi todas las advertencias existentes (siempre con el `-Werror` bandera) habilitado para sonido. En esa misma página, puede encontrar todos los registros de compilación para que no tenga que compilar ClickHouse de todas las formas posibles.
diff --git a/docs/es/development/index.md b/docs/es/development/index.md
deleted file mode 100644
index 6f96f9b3f023..000000000000
--- a/docs/es/development/index.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Desarrollo
-toc_hidden: true
-toc_priority: 58
-toc_title: oculto
----
-
-# Desarrollo de ClickHouse {#clickhouse-development}
-
-[Artículo Original](https://clickhouse.tech/docs/en/development/) <!--hide-->
diff --git a/docs/es/development/style.md b/docs/es/development/style.md
deleted file mode 100644
index ec55516fe2ce..000000000000
--- a/docs/es/development/style.md
+++ /dev/null
@@ -1,841 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 68
-toc_title: "C\xF3mo escribir c\xF3digo C ++"
----
-
-# Cómo escribir código C ++ {#how-to-write-c-code}
-
-## Recomendaciones generales {#general-recommendations}
-
-**1.** Las siguientes son recomendaciones, no requisitos.
-
-**2.** Si está editando código, tiene sentido seguir el formato del código existente.
-
-**3.** El estilo de código es necesario para la coherencia. La consistencia facilita la lectura del código y también facilita la búsqueda del código.
-
-**4.** Muchas de las reglas no tienen razones lógicas; están dictadas por prácticas establecidas.
-
-## Formatear {#formatting}
-
-**1.** La mayor parte del formato se realizará automáticamente por `clang-format`.
-
-**2.** Las sangrías son 4 espacios. Configure el entorno de desarrollo para que una pestaña agregue cuatro espacios.
-
-**3.** Abrir y cerrar llaves deben estar en una línea separada.
-
-``` cpp
-inline void readBoolText(bool & x, ReadBuffer & buf)
-{
-    char tmp = '0';
-    readChar(tmp, buf);
-    x = tmp != '0';
-}
-```
-
-**4.** Si todo el cuerpo de la función es `statement`, se puede colocar en una sola línea. Coloque espacios alrededor de llaves (además del espacio al final de la línea).
-
-``` cpp
-inline size_t mask() const                { return buf_size() - 1; }
-inline size_t place(HashValue x) const    { return x & mask(); }
-```
-
-**5.** Para funciones. No coloque espacios alrededor de los corchetes.
-
-``` cpp
-void reinsert(const Value & x)
-```
-
-``` cpp
-memcpy(&buf[place_value], &x, sizeof(x));
-```
-
-**6.** En `if`, `for`, `while` y otras expresiones, se inserta un espacio delante del corchete de apertura (a diferencia de las llamadas a funciones).
-
-``` cpp
-for (size_t i = 0; i < rows; i += storage.index_granularity)
-```
-
-**7.** Agregar espacios alrededor de los operadores binarios (`+`, `-`, `*`, `/`, `%`, …) and the ternary operator `?:`.
-
-``` cpp
-UInt16 year = (s[0] - '0') * 1000 + (s[1] - '0') * 100 + (s[2] - '0') * 10 + (s[3] - '0');
-UInt8 month = (s[5] - '0') * 10 + (s[6] - '0');
-UInt8 day = (s[8] - '0') * 10 + (s[9] - '0');
-```
-
-**8.** Si se introduce un avance de línea, coloque al operador en una nueva línea y aumente la sangría antes de ella.
-
-``` cpp
-if (elapsed_ns)
-    message << " ("
-        << rows_read_on_server * 1000000000 / elapsed_ns << " rows/s., "
-        << bytes_read_on_server * 1000.0 / elapsed_ns << " MB/s.) ";
-```
-
-**9.** Puede utilizar espacios para la alineación dentro de una línea, si lo desea.
-
-``` cpp
-dst.ClickLogID         = click.LogID;
-dst.ClickEventID       = click.EventID;
-dst.ClickGoodEvent     = click.GoodEvent;
-```
-
-**10.** No use espacios alrededor de los operadores `.`, `->`.
-
-Si es necesario, el operador se puede envolver a la siguiente línea. En este caso, el desplazamiento frente a él aumenta.
-
-**11.** No utilice un espacio para separar los operadores unarios (`--`, `++`, `*`, `&`, …) from the argument.
-
-**12.** Pon un espacio después de una coma, pero no antes. La misma regla se aplica a un punto y coma dentro de un `for` expresion.
-
-**13.** No utilice espacios para separar el `[]` operador.
-
-**14.** En un `template <...>` expresión, use un espacio entre `template` y `<`; sin espacios después de `<` o antes `>`.
-
-``` cpp
-template <typename TKey, typename TValue>
-struct AggregatedStatElement
-{}
-```
-
-**15.** En clases y estructuras, escribe `public`, `private`, y `protected` en el mismo nivel que `class/struct`, y sangrar el resto del código.
-
-``` cpp
-template <typename T>
-class MultiVersion
-{
-public:
-    /// Version of object for usage. shared_ptr manage lifetime of version.
-    using Version = std::shared_ptr<const T>;
-    ...
-}
-```
-
-**16.** Si el mismo `namespace` se usa para todo el archivo, y no hay nada más significativo, no es necesario un desplazamiento dentro `namespace`.
-
-**17.** Si el bloque para un `if`, `for`, `while`, u otra expresión consiste en una sola `statement`, las llaves son opcionales. Coloque el `statement` en una línea separada, en su lugar. Esta regla también es válida para `if`, `for`, `while`, …
-
-Pero si el interior `statement` contiene llaves o `else`, el bloque externo debe escribirse entre llaves.
-
-``` cpp
-/// Finish write.
-for (auto & stream : streams)
-    stream.second->finalize();
-```
-
-**18.** No debería haber espacios al final de las líneas.
-
-**19.** Los archivos de origen están codificados en UTF-8.
-
-**20.** Los caracteres no ASCII se pueden usar en literales de cadena.
-
-``` cpp
-<< ", " << (timer.elapsed() / chunks_stats.hits) << " μsec/hit.";
-```
-
-**21.** No escriba varias expresiones en una sola línea.
-
-**22.** Agrupe secciones de código dentro de las funciones y sepárelas con no más de una línea vacía.
-
-**23.** Separe funciones, clases, etc. con una o dos líneas vacías.
-
-**24.** `A const` (relacionado con un valor) debe escribirse antes del nombre del tipo.
-
-``` cpp
-//correct
-const char * pos
-const std::string & s
-//incorrect
-char const * pos
-```
-
-**25.** Al declarar un puntero o referencia, el `*` y `&` Los símbolos deben estar separados por espacios en ambos lados.
-
-``` cpp
-//correct
-const char * pos
-//incorrect
-const char* pos
-const char *pos
-```
-
-**26.** Cuando utilice tipos de plantilla, alias con el `using` palabra clave (excepto en los casos más simples).
-
-En otras palabras, los parámetros de la plantilla se especifican solo en `using` y no se repiten en el código.
-
-`using` se puede declarar localmente, como dentro de una función.
-
-``` cpp
-//correct
-using FileStreams = std::map<std::string, std::shared_ptr<Stream>>;
-FileStreams streams;
-//incorrect
-std::map<std::string, std::shared_ptr<Stream>> streams;
-```
-
-**27.** No declare varias variables de diferentes tipos en una instrucción.
-
-``` cpp
-//incorrect
-int x, *y;
-```
-
-**28.** No utilice moldes de estilo C.
-
-``` cpp
-//incorrect
-std::cerr << (int)c <<; std::endl;
-//correct
-std::cerr << static_cast<int>(c) << std::endl;
-```
-
-**29.** En clases y estructuras, los miembros del grupo y las funciones por separado dentro de cada ámbito de visibilidad.
-
-**30.** Para clases y estructuras pequeñas, no es necesario separar la declaración del método de la implementación.
-
-Lo mismo es cierto para los métodos pequeños en cualquier clase o estructura.
-
-Para clases y estructuras con plantillas, no separe las declaraciones de métodos de la implementación (porque de lo contrario deben definirse en la misma unidad de traducción).
-
-**31.** Puede ajustar líneas en 140 caracteres, en lugar de 80.
-
-**32.** Utilice siempre los operadores de incremento / decremento de prefijo si no se requiere postfix.
-
-``` cpp
-for (Names::const_iterator it = column_names.begin(); it != column_names.end(); ++it)
-```
-
-## Comentario {#comments}
-
-**1.** Asegúrese de agregar comentarios para todas las partes no triviales del código.
-
-Esto es muy importante. Escribir el comentario puede ayudarte a darte cuenta de que el código no es necesario o que está diseñado incorrectamente.
-
-``` cpp
-/** Part of piece of memory, that can be used.
-  * For example, if internal_buffer is 1MB, and there was only 10 bytes loaded to buffer from file for reading,
-  * then working_buffer will have size of only 10 bytes
-  * (working_buffer.end() will point to position right after those 10 bytes available for read).
-  */
-```
-
-**2.** Los comentarios pueden ser tan detallados como sea necesario.
-
-**3.** Coloque comentarios antes del código que describen. En casos raros, los comentarios pueden aparecer después del código, en la misma línea.
-
-``` cpp
-/** Parses and executes the query.
-*/
-void executeQuery(
-    ReadBuffer & istr, /// Where to read the query from (and data for INSERT, if applicable)
-    WriteBuffer & ostr, /// Where to write the result
-    Context & context, /// DB, tables, data types, engines, functions, aggregate functions...
-    BlockInputStreamPtr & query_plan, /// Here could be written the description on how query was executed
-    QueryProcessingStage::Enum stage = QueryProcessingStage::Complete /// Up to which stage process the SELECT query
-    )
-```
-
-**4.** Los comentarios deben escribirse en inglés solamente.
-
-**5.** Si está escribiendo una biblioteca, incluya comentarios detallados que la expliquen en el archivo de encabezado principal.
-
-**6.** No agregue comentarios que no proporcionen información adicional. En particular, no deje comentarios vacíos como este:
-
-``` cpp
-/*
-* Procedure Name:
-* Original procedure name:
-* Author:
-* Date of creation:
-* Dates of modification:
-* Modification authors:
-* Original file name:
-* Purpose:
-* Intent:
-* Designation:
-* Classes used:
-* Constants:
-* Local variables:
-* Parameters:
-* Date of creation:
-* Purpose:
-*/
-```
-
-El ejemplo se toma prestado del recurso http://home.tamk.fi/~jaalto/course/coding-style/doc/unmaintainable-code/.
-
-**7.** No escriba comentarios de basura (autor, fecha de creación ..) al principio de cada archivo.
-
-**8.** Los comentarios de una sola línea comienzan con tres barras: `///` y los comentarios de varias líneas comienzan con `/**`. Estos comentarios son considerados “documentation”.
-
-Nota: Puede usar Doxygen para generar documentación a partir de estos comentarios. Pero Doxygen no se usa generalmente porque es más conveniente navegar por el código en el IDE.
-
-**9.** Los comentarios de varias líneas no deben tener líneas vacías al principio y al final (excepto la línea que cierra un comentario de varias líneas).
-
-**10.** Para comentar el código, use comentarios básicos, no “documenting” comentario.
-
-**11.** Elimine las partes comentadas del código antes de confirmar.
-
-**12.** No use blasfemias en comentarios o código.
-
-**13.** No use letras mayúsculas. No use puntuación excesiva.
-
-``` cpp
-/// WHAT THE FAIL???
-```
-
-**14.** No use comentarios para hacer delímetros.
-
-``` cpp
-///******************************************************
-```
-
-**15.** No comiencen las discusiones en los comentarios.
-
-``` cpp
-/// Why did you do this stuff?
-```
-
-**16.** No es necesario escribir un comentario al final de un bloque que describa de qué se trataba.
-
-``` cpp
-/// for
-```
-
-## Nombre {#names}
-
-**1.** Use letras minúsculas con guiones bajos en los nombres de variables y miembros de clase.
-
-``` cpp
-size_t max_block_size;
-```
-
-**2.** Para los nombres de las funciones (métodos), use camelCase comenzando con una letra minúscula.
-
-``` cpp
-std::string getName() const override { return "Memory"; }
-```
-
-**3.** Para los nombres de las clases (estructuras), use CamelCase comenzando con una letra mayúscula. Los prefijos distintos de I no se usan para interfaces.
-
-``` cpp
-class StorageMemory : public IStorage
-```
-
-**4.** `using` se nombran de la misma manera que las clases, o con `_t` al final.
-
-**5.** Nombres de argumentos de tipo de plantilla: en casos simples, use `T`; `T`, `U`; `T1`, `T2`.
-
-Para casos más complejos, siga las reglas para los nombres de clase o agregue el prefijo `T`.
-
-``` cpp
-template <typename TKey, typename TValue>
-struct AggregatedStatElement
-```
-
-**6.** Nombres de argumentos constantes de plantilla: siga las reglas para los nombres de variables o use `N` en casos simples.
-
-``` cpp
-template <bool without_www>
-struct ExtractDomain
-```
-
-**7.** Para clases abstractas (interfaces) puede agregar el `I` prefijo.
-
-``` cpp
-class IBlockInputStream
-```
-
-**8.** Si usa una variable localmente, puede usar el nombre corto.
-
-En todos los demás casos, use un nombre que describa el significado.
-
-``` cpp
-bool info_successfully_loaded = false;
-```
-
-**9.** Nombres de `define`s y las constantes globales usan ALL_CAPS con guiones bajos.
-
-``` cpp
-#define MAX_SRC_TABLE_NAMES_TO_STORE 1000
-```
-
-**10.** Los nombres de archivo deben usar el mismo estilo que su contenido.
-
-Si un archivo contiene una sola clase, nombre el archivo de la misma manera que la clase (CamelCase).
-
-Si el archivo contiene una sola función, nombre el archivo de la misma manera que la función (camelCase).
-
-**11.** Si el nombre contiene una abreviatura, :
-
--   Para los nombres de variables, la abreviatura debe usar letras minúsculas `mysql_connection` (ni `mySQL_connection`).
--   Para los nombres de clases y funciones, mantenga las letras mayúsculas en la abreviatura`MySQLConnection` (ni `MySqlConnection`).
-
-**12.** Los argumentos del constructor que se usan solo para inicializar los miembros de la clase deben nombrarse de la misma manera que los miembros de la clase, pero con un guión bajo al final.
-
-``` cpp
-FileQueueProcessor(
-    const std::string & path_,
-    const std::string & prefix_,
-    std::shared_ptr<FileHandler> handler_)
-    : path(path_),
-    prefix(prefix_),
-    handler(handler_),
-    log(&Logger::get("FileQueueProcessor"))
-{
-}
-```
-
-El sufijo de subrayado se puede omitir si el argumento no se usa en el cuerpo del constructor.
-
-**13.** No hay diferencia en los nombres de las variables locales y los miembros de la clase (no se requieren prefijos).
-
-``` cpp
-timer (not m_timer)
-```
-
-**14.** Para las constantes en un `enum`, usar CamelCase con una letra mayúscula. ALL_CAPS también es aceptable. Si el `enum` no es local, utilice un `enum class`.
-
-``` cpp
-enum class CompressionMethod
-{
-    QuickLZ = 0,
-    LZ4     = 1,
-};
-```
-
-**15.** Todos los nombres deben estar en inglés. La transliteración de palabras rusas no está permitida.
-
-    not Stroka
-
-**16.** Las abreviaturas son aceptables si son bien conocidas (cuando puede encontrar fácilmente el significado de la abreviatura en Wikipedia o en un motor de búsqueda).
-
-    `AST`, `SQL`.
-
-    Not `NVDH` (some random letters)
-
-Las palabras incompletas son aceptables si la versión abreviada es de uso común.
-
-También puede usar una abreviatura si el nombre completo se incluye junto a él en los comentarios.
-
-**17.** Los nombres de archivo con código fuente de C++ deben tener `.cpp` ampliación. Los archivos de encabezado deben tener `.h` ampliación.
-
-## Cómo escribir código {#how-to-write-code}
-
-**1.** Gestión de la memoria.
-
-Desasignación de memoria manual (`delete`) solo se puede usar en el código de la biblioteca.
-
-En el código de la biblioteca, el `delete` operador sólo se puede utilizar en destructores.
-
-En el código de la aplicación, la memoria debe ser liberada por el objeto que la posee.
-
-Ejemplos:
-
--   La forma más fácil es colocar un objeto en la pila o convertirlo en miembro de otra clase.
--   Para una gran cantidad de objetos pequeños, use contenedores.
--   Para la desasignación automática de un pequeño número de objetos que residen en el montón, use `shared_ptr/unique_ptr`.
-
-**2.** Gestión de recursos.
-
-Utilizar `RAII` y ver arriba.
-
-**3.** Manejo de errores.
-
-Utilice excepciones. En la mayoría de los casos, solo necesita lanzar una excepción y no necesita atraparla (debido a `RAII`).
-
-En las aplicaciones de procesamiento de datos fuera de línea, a menudo es aceptable no detectar excepciones.
-
-En los servidores que manejan las solicitudes de los usuarios, generalmente es suficiente detectar excepciones en el nivel superior del controlador de conexión.
-
-En las funciones de subproceso, debe capturar y mantener todas las excepciones para volver a lanzarlas en el subproceso principal después `join`.
-
-``` cpp
-/// If there weren't any calculations yet, calculate the first block synchronously
-if (!started)
-{
-    calculate();
-    started = true;
-}
-else /// If calculations are already in progress, wait for the result
-    pool.wait();
-
-if (exception)
-    exception->rethrow();
-```
-
-Nunca oculte excepciones sin manejo. Nunca simplemente ponga ciegamente todas las excepciones para iniciar sesión.
-
-``` cpp
-//Not correct
-catch (...) {}
-```
-
-Si necesita ignorar algunas excepciones, hágalo solo para las específicas y vuelva a lanzar el resto.
-
-``` cpp
-catch (const DB::Exception & e)
-{
-    if (e.code() == ErrorCodes::UNKNOWN_AGGREGATE_FUNCTION)
-        return nullptr;
-    else
-        throw;
-}
-```
-
-Al usar funciones con códigos de respuesta o `errno`, siempre verifique el resultado y arroje una excepción en caso de error.
-
-``` cpp
-if (0 != close(fd))
-    throwFromErrno("Cannot close file " + file_name, ErrorCodes::CANNOT_CLOSE_FILE);
-```
-
-`Do not use assert`.
-
-**4.** Tipos de excepción.
-
-No es necesario utilizar una jerarquía de excepciones compleja en el código de la aplicación. El texto de excepción debe ser comprensible para un administrador del sistema.
-
-**5.** Lanzar excepciones de destructores.
-
-Esto no es recomendable, pero está permitido.
-
-Utilice las siguientes opciones:
-
--   Crear una función (`done()` o `finalize()`) que hará todo el trabajo de antemano que podría conducir a una excepción. Si se llamó a esa función, no debería haber excepciones en el destructor más adelante.
--   Las tareas que son demasiado complejas (como enviar mensajes a través de la red) se pueden poner en un método separado al que el usuario de la clase tendrá que llamar antes de la destrucción.
--   Si hay una excepción en el destructor, es mejor registrarla que ocultarla (si el registrador está disponible).
--   En aplicaciones simples, es aceptable confiar en `std::terminate` (para los casos de `noexcept` de forma predeterminada en C ++ 11) para manejar excepciones.
-
-**6.** Bloques de código anónimos.
-
-Puede crear un bloque de código separado dentro de una sola función para hacer que ciertas variables sean locales, de modo que se llame a los destructores al salir del bloque.
-
-``` cpp
-Block block = data.in->read();
-
-{
-    std::lock_guard<std::mutex> lock(mutex);
-    data.ready = true;
-    data.block = block;
-}
-
-ready_any.set();
-```
-
-**7.** Multithreading.
-
-En programas de procesamiento de datos fuera de línea:
-
--   Trate de obtener el mejor rendimiento posible en un solo núcleo de CPU. A continuación, puede paralelizar su código si es necesario.
-
-En aplicaciones de servidor:
-
--   Utilice el grupo de subprocesos para procesar solicitudes. En este punto, no hemos tenido ninguna tarea que requiera el cambio de contexto de espacio de usuario.
-
-La horquilla no se usa para la paralelización.
-
-**8.** Sincronización de hilos.
-
-A menudo es posible hacer que diferentes hilos usen diferentes celdas de memoria (incluso mejor: diferentes líneas de caché) y no usar ninguna sincronización de hilos (excepto `joinAll`).
-
-Si se requiere sincronización, en la mayoría de los casos, es suficiente usar mutex bajo `lock_guard`.
-
-En otros casos, use primitivas de sincronización del sistema. No utilice la espera ocupada.
-
-Las operaciones atómicas deben usarse solo en los casos más simples.
-
-No intente implementar estructuras de datos sin bloqueo a menos que sea su principal área de especialización.
-
-**9.** Punteros vs referencias.
-
-En la mayoría de los casos, prefiera referencias.
-
-**10.** Construir.
-
-Usar referencias constantes, punteros a constantes, `const_iterator`, y métodos const.
-
-Considerar `const` para ser predeterminado y usar no-`const` sólo cuando sea necesario.
-
-Al pasar variables por valor, usando `const` por lo general no tiene sentido.
-
-**11.** sin firmar.
-
-Utilizar `unsigned` si es necesario.
-
-**12.** Tipos numéricos.
-
-Utilice los tipos `UInt8`, `UInt16`, `UInt32`, `UInt64`, `Int8`, `Int16`, `Int32`, y `Int64`, así como `size_t`, `ssize_t`, y `ptrdiff_t`.
-
-No use estos tipos para números: `signed/unsigned long`, `long long`, `short`, `signed/unsigned char`, `char`.
-
-**13.** Pasando argumentos.
-
-Pasar valores complejos por referencia (incluyendo `std::string`).
-
-Si una función captura la propiedad de un objeto creado en el montón, cree el tipo de argumento `shared_ptr` o `unique_ptr`.
-
-**14.** Valores devueltos.
-
-En la mayoría de los casos, sólo tiene que utilizar `return`. No escribir `return std::move(res)`.
-
-Si la función asigna un objeto en el montón y lo devuelve, use `shared_ptr` o `unique_ptr`.
-
-En casos excepcionales, es posible que deba devolver el valor a través de un argumento. En este caso, el argumento debe ser una referencia.
-
-``` cpp
-using AggregateFunctionPtr = std::shared_ptr<IAggregateFunction>;
-
-/** Allows creating an aggregate function by its name.
-  */
-class AggregateFunctionFactory
-{
-public:
-    AggregateFunctionFactory();
-    AggregateFunctionPtr get(const String & name, const DataTypes & argument_types) const;
-```
-
-**15.** espacio de nombres.
-
-No hay necesidad de usar un `namespace` para el código de aplicación.
-
-Las bibliotecas pequeñas tampoco necesitan esto.
-
-Para bibliotecas medianas a grandes, coloque todo en un `namespace`.
-
-En la biblioteca `.h` archivo, se puede utilizar `namespace detail` para ocultar los detalles de implementación no necesarios para el código de la aplicación.
-
-En un `.cpp` archivo, puede usar un `static` o espacio de nombres anónimo para ocultar símbolos.
-
-Además, un `namespace` puede ser utilizado para un `enum` para evitar que los nombres correspondientes caigan en un `namespace` (pero es mejor usar un `enum class`).
-
-**16.** Inicialización diferida.
-
-Si se requieren argumentos para la inicialización, normalmente no debe escribir un constructor predeterminado.
-
-Si más adelante tendrá que retrasar la inicialización, puede agregar un constructor predeterminado que creará un objeto no válido. O, para un pequeño número de objetos, puede usar `shared_ptr/unique_ptr`.
-
-``` cpp
-Loader(DB::Connection * connection_, const std::string & query, size_t max_block_size_);
-
-/// For deferred initialization
-Loader() {}
-```
-
-**17.** Funciones virtuales.
-
-Si la clase no está destinada para uso polimórfico, no necesita hacer que las funciones sean virtuales. Esto también se aplica al destructor.
-
-**18.** Codificación.
-
-Usa UTF-8 en todas partes. Utilizar `std::string`y`char *`. No use `std::wstring`y`wchar_t`.
-
-**19.** Tala.
-
-Vea los ejemplos en todas partes del código.
-
-Antes de confirmar, elimine todo el registro de depuración y sin sentido, y cualquier otro tipo de salida de depuración.
-
-Se debe evitar el registro en ciclos, incluso en el nivel Trace.
-
-Los registros deben ser legibles en cualquier nivel de registro.
-
-El registro solo debe usarse en el código de la aplicación, en su mayor parte.
-
-Los mensajes de registro deben estar escritos en inglés.
-
-El registro debe ser preferiblemente comprensible para el administrador del sistema.
-
-No use blasfemias en el registro.
-
-Utilice la codificación UTF-8 en el registro. En casos excepcionales, puede usar caracteres que no sean ASCII en el registro.
-
-**20.** Entrada-salida.
-
-No utilice `iostreams` en ciclos internos que son críticos para el rendimiento de la aplicación (y nunca usan `stringstream`).
-
-Utilice el `DB/IO` biblioteca en su lugar.
-
-**21.** Fecha y hora.
-
-Ver el `DateLUT` biblioteca.
-
-**22.** incluir.
-
-Utilice siempre `#pragma once` en lugar de incluir guardias.
-
-**23.** utilizar.
-
-`using namespace` no se utiliza. Usted puede utilizar `using` con algo específico. Pero hazlo local dentro de una clase o función.
-
-**24.** No use `trailing return type` para funciones a menos que sea necesario.
-
-``` cpp
-auto f() -> void
-```
-
-**25.** Declaración e inicialización de variables.
-
-``` cpp
-//right way
-std::string s = "Hello";
-std::string s{"Hello"};
-
-//wrong way
-auto s = std::string{"Hello"};
-```
-
-**26.** Para funciones virtuales, escriba `virtual` en la clase base, pero escribe `override` en lugar de `virtual` en las clases descendientes.
-
-## Características no utilizadas de C ++ {#unused-features-of-c}
-
-**1.** La herencia virtual no se utiliza.
-
-**2.** Los especificadores de excepción de C ++ 03 no se usan.
-
-## Plataforma {#platform}
-
-**1.** Escribimos código para una plataforma específica.
-
-Pero en igualdad de condiciones, se prefiere el código multiplataforma o portátil.
-
-**2.** Idioma: C++20.
-
-**3.** Compilación: `gcc`. En este momento (agosto 2020), el código se compila utilizando la versión 9.3. (También se puede compilar usando `clang 8`.)
-
-Se utiliza la biblioteca estándar (`libc++`).
-
-**4.**OS: Linux Ubuntu, no más viejo que Precise.
-
-**5.**El código está escrito para la arquitectura de CPU x86_64.
-
-El conjunto de instrucciones de CPU es el conjunto mínimo admitido entre nuestros servidores. Actualmente, es SSE 4.2.
-
-**6.** Utilizar `-Wall -Wextra -Werror` flags de compilación.
-
-**7.** Use enlaces estáticos con todas las bibliotecas, excepto aquellas a las que son difíciles de conectar estáticamente (consulte la salida de la `ldd` comando).
-
-**8.** El código se desarrolla y se depura con la configuración de la versión.
-
-## Herramienta {#tools}
-
-**1.** KDevelop es un buen IDE.
-
-**2.** Para la depuración, use `gdb`, `valgrind` (`memcheck`), `strace`, `-fsanitize=...`, o `tcmalloc_minimal_debug`.
-
-**3.** Para crear perfiles, use `Linux Perf`, `valgrind` (`callgrind`), o `strace -cf`.
-
-**4.** Las fuentes están en Git.
-
-**5.** Usos de ensamblaje `CMake`.
-
-**6.** Los programas se lanzan usando `deb` paquete.
-
-**7.** Los compromisos a dominar no deben romper la compilación.
-
-Aunque solo las revisiones seleccionadas se consideran viables.
-
-**8.** Realice confirmaciones tan a menudo como sea posible, incluso si el código está parcialmente listo.
-
-Use ramas para este propósito.
-
-Si su código en el `master` branch todavía no se puede construir, excluirlo de la compilación antes de la `push`. Tendrá que terminarlo o eliminarlo dentro de unos días.
-
-**9.** Para cambios no triviales, use ramas y publíquelas en el servidor.
-
-**10.** El código no utilizado se elimina del repositorio.
-
-## Biblioteca {#libraries}
-
-**1.** Se utiliza la biblioteca estándar de C++20 (se permiten extensiones experimentales), así como `boost` y `Poco` marco.
-
-**2.** Si es necesario, puede usar cualquier biblioteca conocida disponible en el paquete del sistema operativo.
-
-Si ya hay una buena solución disponible, úsela, incluso si eso significa que debe instalar otra biblioteca.
-
-(Pero prepárese para eliminar las bibliotecas incorrectas del código.)
-
-**3.** Puede instalar una biblioteca que no esté en los paquetes, si los paquetes no tienen lo que necesita o tienen una versión obsoleta o el tipo de compilación incorrecto.
-
-**4.** Si la biblioteca es pequeña y no tiene su propio sistema de compilación complejo, coloque los archivos `contrib` carpeta.
-
-**5.** Siempre se da preferencia a las bibliotecas que ya están en uso.
-
-## Recomendaciones generales {#general-recommendations-1}
-
-**1.** Escribe el menor código posible.
-
-**2.** Pruebe la solución más simple.
-
-**3.** No escriba código hasta que sepa cómo va a funcionar y cómo funcionará el bucle interno.
-
-**4.** En los casos más simples, use `using` en lugar de clases o estructuras.
-
-**5.** Si es posible, no escriba constructores de copia, operadores de asignación, destructores (que no sean virtuales, si la clase contiene al menos una función virtual), mueva constructores o mueva operadores de asignación. En otras palabras, las funciones generadas por el compilador deben funcionar correctamente. Usted puede utilizar `default`.
-
-**6.** Se fomenta la simplificación del código. Reduzca el tamaño de su código siempre que sea posible.
-
-## Recomendaciones adicionales {#additional-recommendations}
-
-**1.** Especificar explícitamente `std::` para tipos de `stddef.h`
-
-no se recomienda. En otras palabras, recomendamos escribir `size_t` en su lugar `std::size_t` porque es más corto.
-
-Es aceptable agregar `std::`.
-
-**2.** Especificar explícitamente `std::` para funciones de la biblioteca C estándar
-
-no se recomienda. En otras palabras, escribir `memcpy` en lugar de `std::memcpy`.
-
-La razón es que hay funciones no estándar similares, tales como `memmem`. Utilizamos estas funciones en ocasiones. Estas funciones no existen en `namespace std`.
-
-Si usted escribe `std::memcpy` en lugar de `memcpy` en todas partes, entonces `memmem` sin `std::` se verá extraño.
-
-Sin embargo, todavía puedes usar `std::` si lo prefieres.
-
-**3.** Usar funciones de C cuando las mismas están disponibles en la biblioteca estándar de C ++.
-
-Esto es aceptable si es más eficiente.
-
-Por ejemplo, use `memcpy` en lugar de `std::copy` para copiar grandes trozos de memoria.
-
-**4.** Argumentos de función multilínea.
-
-Se permite cualquiera de los siguientes estilos de ajuste:
-
-``` cpp
-function(
-  T1 x1,
-  T2 x2)
-```
-
-``` cpp
-function(
-  size_t left, size_t right,
-  const & RangesInDataParts ranges,
-  size_t limit)
-```
-
-``` cpp
-function(size_t left, size_t right,
-  const & RangesInDataParts ranges,
-  size_t limit)
-```
-
-``` cpp
-function(size_t left, size_t right,
-      const & RangesInDataParts ranges,
-      size_t limit)
-```
-
-``` cpp
-function(
-      size_t left,
-      size_t right,
-      const & RangesInDataParts ranges,
-      size_t limit)
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/development/style/) <!--hide-->
diff --git a/docs/es/engines/database-engines/atomic.md b/docs/es/engines/database-engines/atomic.md
deleted file mode 100644
index f019b94a00b2..000000000000
--- a/docs/es/engines/database-engines/atomic.md
+++ /dev/null
@@ -1,17 +0,0 @@
----
-toc_priority: 32
-toc_title: Atomic
----
-
-
-# Atomic {#atomic}
-
-It is supports non-blocking `DROP` and `RENAME TABLE` queries and atomic `EXCHANGE TABLES t1 AND t2` queries. Atomic database engine is used by default.
-
-## Creating a Database {#creating-a-database}
-
-```sql
-CREATE DATABASE test ENGINE = Atomic;
-```
-
-[Original article](https://clickhouse.tech/docs/en/engines/database_engines/atomic/) <!--hide-->
diff --git a/docs/es/engines/database-engines/index.md b/docs/es/engines/database-engines/index.md
deleted file mode 100644
index 8784b9bd02b9..000000000000
--- a/docs/es/engines/database-engines/index.md
+++ /dev/null
@@ -1,21 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Motores de base de datos
-toc_priority: 27
-toc_title: "Implantaci\xF3n"
----
-
-# Motores de base de datos {#database-engines}
-
-Los motores de bases de datos le permiten trabajar con tablas.
-
-De forma predeterminada, ClickHouse utiliza su motor de base de datos nativa, que proporciona [motores de mesa](../../engines/table-engines/index.md) y una [Dialecto SQL](../../sql-reference/syntax.md).
-
-También puede utilizar los siguientes motores de base de datos:
-
--   [MySQL](mysql.md)
-
--   [Perezoso](lazy.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/database_engines/) <!--hide-->
diff --git a/docs/es/engines/database-engines/lazy.md b/docs/es/engines/database-engines/lazy.md
deleted file mode 100644
index 0988c4cb3954..000000000000
--- a/docs/es/engines/database-engines/lazy.md
+++ /dev/null
@@ -1,18 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 31
-toc_title: Perezoso
----
-
-# Perezoso {#lazy}
-
-Mantiene las tablas en RAM solamente `expiration_time_in_seconds` segundos después del último acceso. Solo se puede usar con tablas \*Log.
-
-Está optimizado para almacenar muchas tablas pequeñas \* Log, para las cuales hay un largo intervalo de tiempo entre los accesos.
-
-## Creación de una base de datos {#creating-a-database}
-
-    CREATE DATABASE testlazy ENGINE = Lazy(expiration_time_in_seconds);
-
-[Artículo Original](https://clickhouse.tech/docs/en/database_engines/lazy/) <!--hide-->
diff --git a/docs/es/engines/database-engines/mysql.md b/docs/es/engines/database-engines/mysql.md
deleted file mode 100644
index 5f1dec97f358..000000000000
--- a/docs/es/engines/database-engines/mysql.md
+++ /dev/null
@@ -1,135 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 30
-toc_title: MySQL
----
-
-# MySQL {#mysql}
-
-Permite conectarse a bases de datos en un servidor MySQL remoto y realizar `INSERT` y `SELECT` consultas para intercambiar datos entre ClickHouse y MySQL.
-
-El `MySQL` motor de base de datos traducir consultas al servidor MySQL para que pueda realizar operaciones tales como `SHOW TABLES` o `SHOW CREATE TABLE`.
-
-No puede realizar las siguientes consultas:
-
--   `RENAME`
--   `CREATE TABLE`
--   `ALTER`
-
-## Creación de una base de datos {#creating-a-database}
-
-``` sql
-CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster]
-ENGINE = MySQL('host:port', ['database' | database], 'user', 'password')
-```
-
-**Parámetros del motor**
-
--   `host:port` — MySQL server address.
--   `database` — Remote database name.
--   `user` — MySQL user.
--   `password` — User password.
-
-## Soporte de tipos de datos {#data_types-support}
-
-| MySQL                            | Haga clic en Casa                                            |
-|----------------------------------|--------------------------------------------------------------|
-| UNSIGNED TINYINT                 | [UInt8](../../sql-reference/data-types/int-uint.md)          |
-| TINYINT                          | [Int8](../../sql-reference/data-types/int-uint.md)           |
-| UNSIGNED SMALLINT                | [UInt16](../../sql-reference/data-types/int-uint.md)         |
-| SMALLINT                         | [Int16](../../sql-reference/data-types/int-uint.md)          |
-| UNSIGNED INT, UNSIGNED MEDIUMINT | [UInt32](../../sql-reference/data-types/int-uint.md)         |
-| INT, MEDIUMINT                   | [Int32](../../sql-reference/data-types/int-uint.md)          |
-| UNSIGNED BIGINT                  | [UInt64](../../sql-reference/data-types/int-uint.md)         |
-| BIGINT                           | [Int64](../../sql-reference/data-types/int-uint.md)          |
-| FLOAT                            | [Float32](../../sql-reference/data-types/float.md)           |
-| DOUBLE                           | [Float64](../../sql-reference/data-types/float.md)           |
-| DATE                             | [Fecha](../../sql-reference/data-types/date.md)              |
-| DATETIME, TIMESTAMP              | [FechaHora](../../sql-reference/data-types/datetime.md)      |
-| BINARY                           | [Cadena fija](../../sql-reference/data-types/fixedstring.md) |
-
-Todos los demás tipos de datos MySQL se convierten en [Cadena](../../sql-reference/data-types/string.md).
-
-[NULL](../../sql-reference/data-types/nullable.md) se admite.
-
-## Ejemplos de uso {#examples-of-use}
-
-Tabla en MySQL:
-
-``` text
-mysql> USE test;
-Database changed
-
-mysql> CREATE TABLE `mysql_table` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `float` FLOAT NOT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into mysql_table (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from mysql_table;
-+------+-----+
-| int_id | value |
-+------+-----+
-|      1 |     2 |
-+------+-----+
-1 row in set (0,00 sec)
-```
-
-Base de datos en ClickHouse, intercambiando datos con el servidor MySQL:
-
-``` sql
-CREATE DATABASE mysql_db ENGINE = MySQL('localhost:3306', 'test', 'my_user', 'user_password')
-```
-
-``` sql
-SHOW DATABASES
-```
-
-``` text
-┌─name─────┐
-│ default  │
-│ mysql_db │
-│ system   │
-└──────────┘
-```
-
-``` sql
-SHOW TABLES FROM mysql_db
-```
-
-``` text
-┌─name─────────┐
-│  mysql_table │
-└──────────────┘
-```
-
-``` sql
-SELECT * FROM mysql_db.mysql_table
-```
-
-``` text
-┌─int_id─┬─value─┐
-│      1 │     2 │
-└────────┴───────┘
-```
-
-``` sql
-INSERT INTO mysql_db.mysql_table VALUES (3,4)
-```
-
-``` sql
-SELECT * FROM mysql_db.mysql_table
-```
-
-``` text
-┌─int_id─┬─value─┐
-│      1 │     2 │
-│      3 │     4 │
-└────────┴───────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/database_engines/mysql/) <!--hide-->
diff --git a/docs/es/engines/index.md b/docs/es/engines/index.md
deleted file mode 100644
index 03e4426dd8d7..000000000000
--- a/docs/es/engines/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Motor
-toc_priority: 25
----
-
-
diff --git a/docs/es/engines/table-engines/index.md b/docs/es/engines/table-engines/index.md
deleted file mode 100644
index 7be315e3ee30..000000000000
--- a/docs/es/engines/table-engines/index.md
+++ /dev/null
@@ -1,85 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Motores de mesa
-toc_priority: 26
-toc_title: "Implantaci\xF3n"
----
-
-# Motores de mesa {#table_engines}
-
-El motor de tabla (tipo de tabla) determina:
-
--   Cómo y dónde se almacenan los datos, dónde escribirlos y dónde leerlos.
--   Qué consultas son compatibles y cómo.
--   Acceso a datos simultáneos.
--   Uso de índices, si está presente.
--   Si es posible la ejecución de solicitudes multiproceso.
--   Parámetros de replicación de datos.
-
-## Familias de motores {#engine-families}
-
-### Método de codificación de datos: {#mergetree}
-
-Los motores de mesa más universales y funcionales para tareas de alta carga. La propiedad compartida por estos motores es la inserción rápida de datos con el posterior procesamiento de datos en segundo plano. `MergeTree` Los motores familiares admiten la replicación de datos (con [Replicado\*](mergetree-family/replication.md#table_engines-replication) versiones de motores), particionamiento y otras características no admitidas en otros motores.
-
-Motores en la familia:
-
--   [Método de codificación de datos:](mergetree-family/mergetree.md#mergetree)
--   [ReplacingMergeTree](mergetree-family/replacingmergetree.md#replacingmergetree)
--   [SummingMergeTree](mergetree-family/summingmergetree.md#summingmergetree)
--   [AgregaciónMergeTree](mergetree-family/aggregatingmergetree.md#aggregatingmergetree)
--   [ColapsarMergeTree](mergetree-family/collapsingmergetree.md#table_engine-collapsingmergetree)
--   [VersionedCollapsingMergeTree](mergetree-family/versionedcollapsingmergetree.md#versionedcollapsingmergetree)
--   [GraphiteMergeTree](mergetree-family/graphitemergetree.md#graphitemergetree)
-
-### Registro {#log}
-
-Ligero [motor](log-family/index.md) con funcionalidad mínima. Son los más efectivos cuando necesita escribir rápidamente muchas tablas pequeñas (hasta aproximadamente 1 millón de filas) y leerlas más tarde como un todo.
-
-Motores en la familia:
-
--   [TinyLog](log-family/tinylog.md#tinylog)
--   [StripeLog](log-family/stripelog.md#stripelog)
--   [Registro](log-family/log.md#log)
-
-### Motores de integración {#integration-engines}
-
-Motores para comunicarse con otros sistemas de almacenamiento y procesamiento de datos.
-
-Motores en la familia:
-
--   [Kafka](integrations/kafka.md#kafka)
--   [MySQL](integrations/mysql.md#mysql)
--   [ODBC](integrations/odbc.md#table-engine-odbc)
--   [JDBC](integrations/jdbc.md#table-engine-jdbc)
--   [HDFS](integrations/hdfs.md#hdfs)
-
-### Motores especiales {#special-engines}
-
-Motores en la familia:
-
--   [Distribuido](special/distributed.md#distributed)
--   [Método de codificación de datos:](special/materializedview.md#materializedview)
--   [Diccionario](special/dictionary.md#dictionary)
--   \[Fusión\](special/merge.md#merge
--   [File](special/file.md#file)
--   [Nulo](special/null.md#null)
--   [Establecer](special/set.md#set)
--   [Unir](special/join.md#join)
--   [URL](special/url.md#table_engines-url)
--   [Vista](special/view.md#table_engines-view)
--   [Memoria](special/memory.md#memory)
--   [Búfer](special/buffer.md#buffer)
-
-## Virtual Columnas {#table_engines-virtual_columns}
-
-La columna virtual es un atributo de motor de tabla integral que se define en el código fuente del motor.
-
-No debe especificar columnas virtuales en el `CREATE TABLE` consulta y no puedes verlos en `SHOW CREATE TABLE` y `DESCRIBE TABLE` resultados de la consulta. Las columnas virtuales también son de solo lectura, por lo que no puede insertar datos en columnas virtuales.
-
-Para seleccionar datos de una columna virtual, debe especificar su nombre en el `SELECT` consulta. `SELECT *` no devuelve valores de columnas virtuales.
-
-Si crea una tabla con una columna que tiene el mismo nombre que una de las columnas virtuales de la tabla, la columna virtual se vuelve inaccesible. No recomendamos hacer esto. Para ayudar a evitar conflictos, los nombres de columna virtual suelen tener el prefijo de un guión bajo.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/) <!--hide-->
diff --git a/docs/es/engines/table-engines/integrations/hdfs.md b/docs/es/engines/table-engines/integrations/hdfs.md
deleted file mode 100644
index 5e0211660f57..000000000000
--- a/docs/es/engines/table-engines/integrations/hdfs.md
+++ /dev/null
@@ -1,123 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: HDFS
----
-
-# HDFS {#table_engines-hdfs}
-
-Este motor proporciona integración con [Acerca de nosotros](https://en.wikipedia.org/wiki/Apache_Hadoop) permitiendo gestionar datos sobre [HDFS](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)a través de ClickHouse. Este motor es similar
-a la [File](../special/file.md#table_engines-file) y [URL](../special/url.md#table_engines-url) motores, pero proporciona características específicas de Hadoop.
-
-## Uso {#usage}
-
-``` sql
-ENGINE = HDFS(URI, format)
-```
-
-El `URI` El parámetro es el URI del archivo completo en HDFS.
-El `format` parámetro especifica uno de los formatos de archivo disponibles. Realizar
-`SELECT` consultas, el formato debe ser compatible para la entrada, y para realizar
-`INSERT` queries – for output. The available formats are listed in the
-[Formato](../../../interfaces/formats.md#formats) apartado.
-La parte de la ruta de `URI` puede contener globs. En este caso, la tabla sería de solo lectura.
-
-**Ejemplo:**
-
-**1.** Configurar el `hdfs_engine_table` tabla:
-
-``` sql
-CREATE TABLE hdfs_engine_table (name String, value UInt32) ENGINE=HDFS('hdfs://hdfs1:9000/other_storage', 'TSV')
-```
-
-**2.** Llenar archivo:
-
-``` sql
-INSERT INTO hdfs_engine_table VALUES ('one', 1), ('two', 2), ('three', 3)
-```
-
-**3.** Consultar los datos:
-
-``` sql
-SELECT * FROM hdfs_engine_table LIMIT 2
-```
-
-``` text
-┌─name─┬─value─┐
-│ one  │     1 │
-│ two  │     2 │
-└──────┴───────┘
-```
-
-## Detalles de implementación {#implementation-details}
-
--   Las lecturas y escrituras pueden ser paralelas
--   No soportado:
-    -   `ALTER` y `SELECT...SAMPLE` operación.
-    -   Índices.
-    -   Replicación.
-
-**Globs en el camino**
-
-Múltiples componentes de ruta de acceso pueden tener globs. Para ser procesado, el archivo debe existir y coincidir con todo el patrón de ruta. Listado de archivos determina durante `SELECT` (no en `CREATE` momento).
-
--   `*` — Substitutes any number of any characters except `/` incluyendo cadena vacía.
--   `?` — Substitutes any single character.
--   `{some_string,another_string,yet_another_one}` — Substitutes any of strings `'some_string', 'another_string', 'yet_another_one'`.
--   `{N..M}` — Substitutes any number in range from N to M including both borders.
-
-Construcciones con `{}` son similares a la [remoto](../../../sql-reference/table-functions/remote.md) función de la tabla.
-
-**Ejemplo**
-
-1.  Supongamos que tenemos varios archivos en formato TSV con los siguientes URI en HDFS:
-
--   ‘hdfs://hdfs1:9000/some_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_3’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_3’
-
-1.  Hay varias maneras de hacer una tabla que consta de los seis archivos:
-
-<!-- -->
-
-``` sql
-CREATE TABLE table_with_range (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV')
-```
-
-Otra forma:
-
-``` sql
-CREATE TABLE table_with_question_mark (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_?', 'TSV')
-```
-
-La tabla consta de todos los archivos en ambos directorios (todos los archivos deben satisfacer el formato y el esquema descritos en la consulta):
-
-``` sql
-CREATE TABLE table_with_asterisk (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV')
-```
-
-!!! warning "Advertencia"
-    Si la lista de archivos contiene rangos de números con ceros a la izquierda, use la construcción con llaves para cada dígito por separado o use `?`.
-
-**Ejemplo**
-
-Crear tabla con archivos llamados `file000`, `file001`, … , `file999`:
-
-``` sql
-CREARE TABLE big_table (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV')
-```
-
-## Virtual Columnas {#virtual-columns}
-
--   `_path` — Path to the file.
--   `_file` — Name of the file.
-
-**Ver también**
-
--   [Virtual columnas](../index.md#table_engines-virtual_columns)
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/hdfs/) <!--hide-->
diff --git a/docs/es/engines/table-engines/integrations/index.md b/docs/es/engines/table-engines/integrations/index.md
deleted file mode 100644
index e57aaf887444..000000000000
--- a/docs/es/engines/table-engines/integrations/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Integraci\xF3n"
-toc_priority: 30
----
-
-
diff --git a/docs/es/engines/table-engines/integrations/jdbc.md b/docs/es/engines/table-engines/integrations/jdbc.md
deleted file mode 100644
index fd3450cef7c6..000000000000
--- a/docs/es/engines/table-engines/integrations/jdbc.md
+++ /dev/null
@@ -1,90 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: JDBC
----
-
-# JDBC {#table-engine-jdbc}
-
-Permite que ClickHouse se conecte a bases de datos externas a través de [JDBC](https://en.wikipedia.org/wiki/Java_Database_Connectivity).
-
-Para implementar la conexión JDBC, ClickHouse utiliza el programa independiente [Sistema abierto.](https://github.com/alex-krash/clickhouse-jdbc-bridge) que debería ejecutarse como un demonio.
-
-Este motor soporta el [NULL](../../../sql-reference/data-types/nullable.md) tipo de datos.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name
-(
-    columns list...
-)
-ENGINE = JDBC(dbms_uri, external_database, external_table)
-```
-
-**Parámetros del motor**
-
--   `dbms_uri` — URI of an external DBMS.
-
-    Formato: `jdbc:<driver_name>://<host_name>:<port>/?user=<username>&password=<password>`.
-    Ejemplo para MySQL: `jdbc:mysql://localhost:3306/?user=root&password=root`.
-
--   `external_database` — Database in an external DBMS.
-
--   `external_table` — Name of the table in `external_database`.
-
-## Ejemplo de uso {#usage-example}
-
-Crear una tabla en el servidor MySQL conectándose directamente con su cliente de consola:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Creación de una tabla en el servidor ClickHouse y selección de datos de ella:
-
-``` sql
-CREATE TABLE jdbc_table
-(
-    `int_id` Int32,
-    `int_nullable` Nullable(Int32),
-    `float` Float32,
-    `float_nullable` Nullable(Float32)
-)
-ENGINE JDBC('jdbc:mysql://localhost:3306/?user=root&password=root', 'test', 'test')
-```
-
-``` sql
-SELECT *
-FROM jdbc_table
-```
-
-``` text
-┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐
-│      1 │         ᴺᵁᴸᴸ │     2 │           ᴺᵁᴸᴸ │
-└────────┴──────────────┴───────┴────────────────┘
-```
-
-## Ver también {#see-also}
-
--   [Función de la tabla de JDBC](../../../sql-reference/table-functions/jdbc.md).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/jdbc/) <!--hide-->
diff --git a/docs/es/engines/table-engines/integrations/kafka.md b/docs/es/engines/table-engines/integrations/kafka.md
deleted file mode 100644
index 54250aae82a8..000000000000
--- a/docs/es/engines/table-engines/integrations/kafka.md
+++ /dev/null
@@ -1,180 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 32
-toc_title: Kafka
----
-
-# Kafka {#kafka}
-
-Este motor funciona con [Acerca de nosotros](http://kafka.apache.org/).
-
-Kafka te permite:
-
--   Publicar o suscribirse a flujos de datos.
--   Organice el almacenamiento tolerante a fallos.
--   Secuencias de proceso a medida que estén disponibles.
-
-## Creación de una tabla {#table_engine-kafka-creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = Kafka()
-SETTINGS
-    kafka_broker_list = 'host:port',
-    kafka_topic_list = 'topic1,topic2,...',
-    kafka_group_name = 'group_name',
-    kafka_format = 'data_format'[,]
-    [kafka_row_delimiter = 'delimiter_symbol',]
-    [kafka_schema = '',]
-    [kafka_num_consumers = N,]
-    [kafka_max_block_size = 0,]
-    [kafka_skip_broken_messages = N,]
-    [kafka_commit_every_batch = 0]
-```
-
-Parámetros requeridos:
-
--   `kafka_broker_list` – A comma-separated list of brokers (for example, `localhost:9092`).
--   `kafka_topic_list` – A list of Kafka topics.
--   `kafka_group_name` – A group of Kafka consumers. Reading margins are tracked for each group separately. If you don't want messages to be duplicated in the cluster, use the same group name everywhere.
--   `kafka_format` – Message format. Uses the same notation as the SQL `FORMAT` función, tal como `JSONEachRow`. Para obtener más información, consulte [Formato](../../../interfaces/formats.md) apartado.
-
-Parámetros opcionales:
-
--   `kafka_row_delimiter` – Delimiter character, which ends the message.
--   `kafka_schema` – Parameter that must be used if the format requires a schema definition. For example, [Cap'n Proto](https://capnproto.org/) requiere la ruta de acceso al archivo de esquema y el nombre de la raíz `schema.capnp:Message` objeto.
--   `kafka_num_consumers` – The number of consumers per table. Default: `1`. Especifique más consumidores si el rendimiento de un consumidor es insuficiente. El número total de consumidores no debe exceder el número de particiones en el tema, ya que solo se puede asignar un consumidor por partición.
--   `kafka_max_block_size` - El tamaño máximo de lote (en mensajes) para la encuesta (predeterminado: `max_block_size`).
--   `kafka_skip_broken_messages` – Kafka message parser tolerance to schema-incompatible messages per block. Default: `0`. Si `kafka_skip_broken_messages = N` entonces el motor salta *N* Mensajes de Kafka que no se pueden analizar (un mensaje es igual a una fila de datos).
--   `kafka_commit_every_batch` - Confirmar cada lote consumido y manejado en lugar de una única confirmación después de escribir un bloque completo (predeterminado: `0`).
-
-Ejemplos:
-
-``` sql
-  CREATE TABLE queue (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow');
-
-  SELECT * FROM queue LIMIT 5;
-
-  CREATE TABLE queue2 (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka SETTINGS kafka_broker_list = 'localhost:9092',
-                            kafka_topic_list = 'topic',
-                            kafka_group_name = 'group1',
-                            kafka_format = 'JSONEachRow',
-                            kafka_num_consumers = 4;
-
-  CREATE TABLE queue2 (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka('localhost:9092', 'topic', 'group1')
-              SETTINGS kafka_format = 'JSONEachRow',
-                       kafka_num_consumers = 4;
-```
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No utilice este método en nuevos proyectos. Si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-Kafka(kafka_broker_list, kafka_topic_list, kafka_group_name, kafka_format
-      [, kafka_row_delimiter, kafka_schema, kafka_num_consumers, kafka_skip_broken_messages])
-```
-
-</details>
-
-## Descripci {#description}
-
-Los mensajes entregados se realizan un seguimiento automático, por lo que cada mensaje de un grupo solo se cuenta una vez. Si desea obtener los datos dos veces, cree una copia de la tabla con otro nombre de grupo.
-
-Los grupos son flexibles y se sincronizan en el clúster. Por ejemplo, si tiene 10 temas y 5 copias de una tabla en un clúster, cada copia obtiene 2 temas. Si el número de copias cambia, los temas se redistribuyen automáticamente entre las copias. Lea más sobre esto en http://kafka.apache.org/intro .
-
-`SELECT` no es particularmente útil para leer mensajes (excepto para la depuración), ya que cada mensaje se puede leer solo una vez. Es más práctico crear subprocesos en tiempo real utilizando vistas materializadas. Para hacer esto:
-
-1.  Use el motor para crear un consumidor de Kafka y considérelo como un flujo de datos.
-2.  Crea una tabla con la estructura deseada.
-3.  Cree una vista materializada que convierta los datos del motor y los coloque en una tabla creada previamente.
-
-Cuando el `MATERIALIZED VIEW` se une al motor, comienza a recopilar datos en segundo plano. Esto le permite recibir continuamente mensajes de Kafka y convertirlos al formato requerido usando `SELECT`.
-Una tabla kafka puede tener tantas vistas materializadas como desee, no leen datos de la tabla kafka directamente, sino que reciben nuevos registros (en bloques), de esta manera puede escribir en varias tablas con diferentes niveles de detalle (con agrupación - agregación y sin).
-
-Ejemplo:
-
-``` sql
-  CREATE TABLE queue (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow');
-
-  CREATE TABLE daily (
-    day Date,
-    level String,
-    total UInt64
-  ) ENGINE = SummingMergeTree(day, (day, level), 8192);
-
-  CREATE MATERIALIZED VIEW consumer TO daily
-    AS SELECT toDate(toDateTime(timestamp)) AS day, level, count() as total
-    FROM queue GROUP BY day, level;
-
-  SELECT level, sum(total) FROM daily GROUP BY level;
-```
-
-Para mejorar el rendimiento, los mensajes recibidos se agrupan en bloques del tamaño de [Max_insert_block_size](../../../operations/server-configuration-parameters/settings.md#settings-max_insert_block_size). Si el bloque no se formó dentro de [Nombre de la red inalámbrica (SSID):](../../../operations/server-configuration-parameters/settings.md) milisegundos, los datos se vaciarán a la tabla independientemente de la integridad del bloque.
-
-Para detener la recepción de datos de tema o cambiar la lógica de conversión, desconecte la vista materializada:
-
-``` sql
-  DETACH TABLE consumer;
-  ATTACH TABLE consumer;
-```
-
-Si desea cambiar la tabla de destino utilizando `ALTER`, recomendamos deshabilitar la vista de material para evitar discrepancias entre la tabla de destino y los datos de la vista.
-
-## Configuración {#configuration}
-
-Similar a GraphiteMergeTree, el motor Kafka admite una configuración extendida utilizando el archivo de configuración ClickHouse. Hay dos claves de configuración que puede usar: global (`kafka`) y a nivel de tema (`kafka_*`). La configuración global se aplica primero y, a continuación, se aplica la configuración de nivel de tema (si existe).
-
-``` xml
-  <!-- Global configuration options for all tables of Kafka engine type -->
-  <kafka>
-    <debug>cgrp</debug>
-    <auto_offset_reset>smallest</auto_offset_reset>
-  </kafka>
-
-  <!-- Configuration specific for topic "logs" -->
-  <kafka_logs>
-    <retry_backoff_ms>250</retry_backoff_ms>
-    <fetch_min_bytes>100000</fetch_min_bytes>
-  </kafka_logs>
-```
-
-Para obtener una lista de posibles opciones de configuración, consulte [referencia de configuración librdkafka](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md). Usa el guión bajo (`_`) en lugar de un punto en la configuración de ClickHouse. Por ejemplo, `check.crcs=true` será `<check_crcs>true</check_crcs>`.
-
-## Virtual Columnas {#virtual-columns}
-
--   `_topic` — Kafka topic.
--   `_key` — Key of the message.
--   `_offset` — Offset of the message.
--   `_timestamp` — Timestamp of the message.
--   `_partition` — Partition of Kafka topic.
-
-**Ver también**
-
--   [Virtual columnas](../index.md#table_engines-virtual_columns)
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/kafka/) <!--hide-->
diff --git a/docs/es/engines/table-engines/integrations/mysql.md b/docs/es/engines/table-engines/integrations/mysql.md
deleted file mode 100644
index 527991172556..000000000000
--- a/docs/es/engines/table-engines/integrations/mysql.md
+++ /dev/null
@@ -1,105 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: MySQL
----
-
-# Mysql {#mysql}
-
-El motor MySQL le permite realizar `SELECT` consultas sobre datos almacenados en un servidor MySQL remoto.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2],
-    ...
-) ENGINE = MySQL('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']);
-```
-
-Vea una descripción detallada del [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) consulta.
-
-La estructura de la tabla puede diferir de la estructura de la tabla MySQL original:
-
--   Los nombres de columna deben ser los mismos que en la tabla MySQL original, pero puede usar solo algunas de estas columnas y en cualquier orden.
--   Los tipos de columna pueden diferir de los de la tabla MySQL original. ClickHouse intenta [elenco](../../../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) valores a los tipos de datos ClickHouse.
-
-**Parámetros del motor**
-
--   `host:port` — MySQL server address.
-
--   `database` — Remote database name.
-
--   `table` — Remote table name.
-
--   `user` — MySQL user.
-
--   `password` — User password.
-
--   `replace_query` — Flag that converts `INSERT INTO` consultas a `REPLACE INTO`. Si `replace_query=1`, la consulta se sustituye.
-
--   `on_duplicate_clause` — The `ON DUPLICATE KEY on_duplicate_clause` expresión que se añade a la `INSERT` consulta.
-
-    Ejemplo: `INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1`, donde `on_duplicate_clause` ser `UPDATE c2 = c2 + 1`. Ver el [Documentación de MySQL](https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html) para encontrar qué `on_duplicate_clause` se puede utilizar con el `ON DUPLICATE KEY` clausula.
-
-    Especificar `on_duplicate_clause` tienes que pasar `0` a la `replace_query` parámetro. Si pasa simultáneamente `replace_query = 1` y `on_duplicate_clause`, ClickHouse genera una excepción.
-
-Simple `WHERE` cláusulas tales como `=, !=, >, >=, <, <=` se ejecutan en el servidor MySQL.
-
-El resto de las condiciones y el `LIMIT` La restricción de muestreo se ejecuta en ClickHouse solo después de que finalice la consulta a MySQL.
-
-## Ejemplo de uso {#usage-example}
-
-Tabla en MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Tabla en ClickHouse, recuperando datos de la tabla MySQL creada anteriormente:
-
-``` sql
-CREATE TABLE mysql_table
-(
-    `float_nullable` Nullable(Float32),
-    `int_id` Int32
-)
-ENGINE = MySQL('localhost:3306', 'test', 'test', 'bayonet', '123')
-```
-
-``` sql
-SELECT * FROM mysql_table
-```
-
-``` text
-┌─float_nullable─┬─int_id─┐
-│           ᴺᵁᴸᴸ │      1 │
-└────────────────┴────────┘
-```
-
-## Ver también {#see-also}
-
--   [El ‘mysql’ función de la tabla](../../../sql-reference/table-functions/mysql.md)
--   [Uso de MySQL como fuente de diccionario externo](../../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-mysql)
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/mysql/) <!--hide-->
diff --git a/docs/es/engines/table-engines/integrations/odbc.md b/docs/es/engines/table-engines/integrations/odbc.md
deleted file mode 100644
index 75c79484d61f..000000000000
--- a/docs/es/engines/table-engines/integrations/odbc.md
+++ /dev/null
@@ -1,132 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: ODBC
----
-
-# ODBC {#table-engine-odbc}
-
-Permite que ClickHouse se conecte a bases de datos externas a través de [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity).
-
-Para implementar con seguridad conexiones ODBC, ClickHouse usa un programa separado `clickhouse-odbc-bridge`. Si el controlador ODBC se carga directamente desde `clickhouse-server`, problemas de controlador pueden bloquear el servidor ClickHouse. ClickHouse se inicia automáticamente `clickhouse-odbc-bridge` cuando se requiere. El programa de puente ODBC se instala desde el mismo paquete que el `clickhouse-server`.
-
-Este motor soporta el [NULL](../../../sql-reference/data-types/nullable.md) tipo de datos.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1],
-    name2 [type2],
-    ...
-)
-ENGINE = ODBC(connection_settings, external_database, external_table)
-```
-
-Vea una descripción detallada del [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) consulta.
-
-La estructura de la tabla puede diferir de la estructura de la tabla de origen:
-
--   Los nombres de columna deben ser los mismos que en la tabla de origen, pero puede usar solo algunas de estas columnas y en cualquier orden.
--   Los tipos de columna pueden diferir de los de la tabla de origen. ClickHouse intenta [elenco](../../../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) valores a los tipos de datos ClickHouse.
-
-**Parámetros del motor**
-
--   `connection_settings` — Name of the section with connection settings in the `odbc.ini` file.
--   `external_database` — Name of a database in an external DBMS.
--   `external_table` — Name of a table in the `external_database`.
-
-## Ejemplo de uso {#usage-example}
-
-**Recuperación de datos de la instalación local de MySQL a través de ODBC**
-
-Este ejemplo se comprueba para Ubuntu Linux 18.04 y el servidor MySQL 5.7.
-
-Asegúrese de que unixODBC y MySQL Connector están instalados.
-
-De forma predeterminada (si se instala desde paquetes), ClickHouse comienza como usuario `clickhouse`. Por lo tanto, debe crear y configurar este usuario en el servidor MySQL.
-
-``` bash
-$ sudo mysql
-```
-
-``` sql
-mysql> CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse';
-mysql> GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;
-```
-
-A continuación, configure la conexión en `/etc/odbc.ini`.
-
-``` bash
-$ cat /etc/odbc.ini
-[mysqlconn]
-DRIVER = /usr/local/lib/libmyodbc5w.so
-SERVER = 127.0.0.1
-PORT = 3306
-DATABASE = test
-USERNAME = clickhouse
-PASSWORD = clickhouse
-```
-
-Puede verificar la conexión usando el `isql` utilidad desde la instalación de unixODBC.
-
-``` bash
-$ isql -v mysqlconn
-+-------------------------+
-| Connected!                            |
-|                                       |
-...
-```
-
-Tabla en MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Tabla en ClickHouse, recuperando datos de la tabla MySQL:
-
-``` sql
-CREATE TABLE odbc_t
-(
-    `int_id` Int32,
-    `float_nullable` Nullable(Float32)
-)
-ENGINE = ODBC('DSN=mysqlconn', 'test', 'test')
-```
-
-``` sql
-SELECT * FROM odbc_t
-```
-
-``` text
-┌─int_id─┬─float_nullable─┐
-│      1 │           ᴺᵁᴸᴸ │
-└────────┴────────────────┘
-```
-
-## Ver también {#see-also}
-
--   [Diccionarios externos ODBC](../../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-odbc)
--   [Tabla ODBC función](../../../sql-reference/table-functions/odbc.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/odbc/) <!--hide-->
diff --git a/docs/es/engines/table-engines/log-family/index.md b/docs/es/engines/table-engines/log-family/index.md
deleted file mode 100644
index a7a3016f967b..000000000000
--- a/docs/es/engines/table-engines/log-family/index.md
+++ /dev/null
@@ -1,47 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Familia de registro
-toc_priority: 29
-toc_title: "Implantaci\xF3n"
----
-
-# Familia del motor de registro {#log-engine-family}
-
-Estos motores fueron desarrollados para escenarios en los que necesita escribir rápidamente muchas tablas pequeñas (hasta aproximadamente 1 millón de filas) y leerlas más tarde en su conjunto.
-
-Motores de la familia:
-
--   [StripeLog](stripelog.md)
--   [Registro](log.md)
--   [TinyLog](tinylog.md)
-
-## Propiedades comunes {#common-properties}
-
-Motor:
-
--   Almacenar datos en un disco.
-
--   Agregue datos al final del archivo al escribir.
-
--   Bloqueos de soporte para el acceso a datos simultáneos.
-
-    Durante `INSERT` consultas, la tabla está bloqueada y otras consultas para leer y escribir datos esperan a que la tabla se desbloquee. Si no hay consultas de escritura de datos, se puede realizar cualquier número de consultas de lectura de datos simultáneamente.
-
--   No apoyo [mutación](../../../sql-reference/statements/alter.md#alter-mutations) operación.
-
--   No admite índices.
-
-    Esto significa que `SELECT` las consultas para rangos de datos no son eficientes.
-
--   No escriba datos atómicamente.
-
-    Puede obtener una tabla con datos dañados si algo rompe la operación de escritura, por ejemplo, un cierre anormal del servidor.
-
-## Diferencia {#differences}
-
-El `TinyLog` es el más simple de la familia y proporciona la funcionalidad más pobre y la eficiencia más baja. El `TinyLog` el motor no admite la lectura de datos paralelos por varios hilos. Lee datos más lentamente que otros motores de la familia que admiten lectura paralela y utiliza casi tantos descriptores como los `Log` motor porque almacena cada columna en un archivo separado. Úselo en escenarios simples de baja carga.
-
-El `Log` y `StripeLog` Los motores admiten lectura de datos paralela. Al leer datos, ClickHouse usa múltiples hilos. Cada subproceso procesa un bloque de datos separado. El `Log` utiliza un archivo separado para cada columna de la tabla. `StripeLog` almacena todos los datos en un archivo. Como resultado, el `StripeLog` el motor utiliza menos descriptores en el sistema operativo, pero el `Log` proporciona una mayor eficiencia al leer datos.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/log_family/) <!--hide-->
diff --git a/docs/es/engines/table-engines/log-family/log.md b/docs/es/engines/table-engines/log-family/log.md
deleted file mode 100644
index 1db374390e4f..000000000000
--- a/docs/es/engines/table-engines/log-family/log.md
+++ /dev/null
@@ -1,16 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: Registro
----
-
-# Registro {#log}
-
-El motor pertenece a la familia de motores de registro. Consulte las propiedades comunes de los motores de registro y sus diferencias en [Familia del motor de registro](index.md) artículo.
-
-El registro difiere de [TinyLog](tinylog.md) en que un pequeño archivo de “marks” reside con los archivos de columna. Estas marcas se escriben en cada bloque de datos y contienen compensaciones que indican dónde comenzar a leer el archivo para omitir el número especificado de filas. Esto hace posible leer datos de tabla en múltiples hilos.
-Para el acceso a datos simultáneos, las operaciones de lectura se pueden realizar simultáneamente, mientras que las operaciones de escritura bloquean las lecturas entre sí.
-El motor de registro no admite índices. Del mismo modo, si la escritura en una tabla falla, la tabla se rompe y la lectura de ella devuelve un error. El motor de registro es adecuado para datos temporales, tablas de escritura única y para fines de prueba o demostración.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/log/) <!--hide-->
diff --git a/docs/es/engines/table-engines/log-family/stripelog.md b/docs/es/engines/table-engines/log-family/stripelog.md
deleted file mode 100644
index 0965e9a987c6..000000000000
--- a/docs/es/engines/table-engines/log-family/stripelog.md
+++ /dev/null
@@ -1,95 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 32
-toc_title: StripeLog
----
-
-# Lista de Stripelog {#stripelog}
-
-Este motor pertenece a la familia de motores de registro. Consulte las propiedades comunes de los motores de registro y sus diferencias en [Familia del motor de registro](index.md) artículo.
-
-Utilice este motor en escenarios en los que necesite escribir muchas tablas con una pequeña cantidad de datos (menos de 1 millón de filas).
-
-## Creación de una tabla {#table_engines-stripelog-creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    column1_name [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    column2_name [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = StripeLog
-```
-
-Vea la descripción detallada del [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) consulta.
-
-## Escribir los datos {#table_engines-stripelog-writing-the-data}
-
-El `StripeLog` el motor almacena todas las columnas en un archivo. Para cada `INSERT` consulta, ClickHouse agrega el bloque de datos al final de un archivo de tabla, escribiendo columnas una por una.
-
-Para cada tabla, ClickHouse escribe los archivos:
-
--   `data.bin` — Data file.
--   `index.mrk` — File with marks. Marks contain offsets for each column of each data block inserted.
-
-El `StripeLog` el motor no soporta el `ALTER UPDATE` y `ALTER DELETE` operación.
-
-## Lectura de los datos {#table_engines-stripelog-reading-the-data}
-
-El archivo con marcas permite ClickHouse paralelizar la lectura de datos. Esto significa que un `SELECT` query devuelve filas en un orden impredecible. Utilice el `ORDER BY` cláusula para ordenar filas.
-
-## Ejemplo de uso {#table_engines-stripelog-example-of-use}
-
-Creación de una tabla:
-
-``` sql
-CREATE TABLE stripe_log_table
-(
-    timestamp DateTime,
-    message_type String,
-    message String
-)
-ENGINE = StripeLog
-```
-
-Insertar datos:
-
-``` sql
-INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The first regular message')
-INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The second regular message'),(now(),'WARNING','The first warning message')
-```
-
-Se utilizaron dos `INSERT` consultas para crear dos bloques de datos dentro del `data.bin` file.
-
-ClickHouse usa múltiples subprocesos al seleccionar datos. Cada subproceso lee un bloque de datos separado y devuelve las filas resultantes de forma independiente a medida que termina. Como resultado, el orden de los bloques de filas en la salida no coincide con el orden de los mismos bloques en la entrada en la mayoría de los casos. Por ejemplo:
-
-``` sql
-SELECT * FROM stripe_log_table
-```
-
-``` text
-┌───────────timestamp─┬─message_type─┬─message────────────────────┐
-│ 2019-01-18 14:27:32 │ REGULAR      │ The second regular message │
-│ 2019-01-18 14:34:53 │ WARNING      │ The first warning message  │
-└─────────────────────┴──────────────┴────────────────────────────┘
-┌───────────timestamp─┬─message_type─┬─message───────────────────┐
-│ 2019-01-18 14:23:43 │ REGULAR      │ The first regular message │
-└─────────────────────┴──────────────┴───────────────────────────┘
-```
-
-Ordenación de los resultados (orden ascendente por defecto):
-
-``` sql
-SELECT * FROM stripe_log_table ORDER BY timestamp
-```
-
-``` text
-┌───────────timestamp─┬─message_type─┬─message────────────────────┐
-│ 2019-01-18 14:23:43 │ REGULAR      │ The first regular message  │
-│ 2019-01-18 14:27:32 │ REGULAR      │ The second regular message │
-│ 2019-01-18 14:34:53 │ WARNING      │ The first warning message  │
-└─────────────────────┴──────────────┴────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/stripelog/) <!--hide-->
diff --git a/docs/es/engines/table-engines/log-family/tinylog.md b/docs/es/engines/table-engines/log-family/tinylog.md
deleted file mode 100644
index a2cbf7257b68..000000000000
--- a/docs/es/engines/table-engines/log-family/tinylog.md
+++ /dev/null
@@ -1,16 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: TinyLog
----
-
-# TinyLog {#tinylog}
-
-El motor pertenece a la familia de motores de registro. Ver [Familia del motor de registro](index.md) para las propiedades comunes de los motores de registro y sus diferencias.
-
-Este motor de tablas se usa normalmente con el método write-once: escribir datos una vez, luego leerlos tantas veces como sea necesario. Por ejemplo, puede usar `TinyLog`-type tablas para datos intermedios que se procesan en pequeños lotes. Tenga en cuenta que el almacenamiento de datos en un gran número de tablas pequeñas es ineficiente.
-
-Las consultas se ejecutan en una sola secuencia. En otras palabras, este motor está diseñado para tablas relativamente pequeñas (hasta aproximadamente 1,000,000 filas). Tiene sentido usar este motor de tablas si tiene muchas tablas pequeñas, ya que es más simple que el [Registro](log.md) motor (menos archivos necesitan ser abiertos).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/tinylog/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/aggregatingmergetree.md b/docs/es/engines/table-engines/mergetree-family/aggregatingmergetree.md
deleted file mode 100644
index 2aedfbd2317a..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/aggregatingmergetree.md
+++ /dev/null
@@ -1,105 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: "Agregaci\xF3nMergeTree"
----
-
-# Aggregatingmergetree {#aggregatingmergetree}
-
-El motor hereda de [Método de codificación de datos:](mergetree.md#table_engines-mergetree), alterando la lógica para la fusión de partes de datos. ClickHouse reemplaza todas las filas con la misma clave principal (o más exactamente, con la misma [clave de clasificación](mergetree.md)) con una sola fila (dentro de una parte de datos) que almacena una combinación de estados de funciones agregadas.
-
-Usted puede utilizar `AggregatingMergeTree` tablas para la agregación de datos incrementales, incluidas las vistas materializadas agregadas.
-
-El motor procesa todas las columnas con los siguientes tipos:
-
--   [AggregateFunction](../../../sql-reference/data-types/aggregatefunction.md)
--   [SimpleAggregateFunction](../../../sql-reference/data-types/simpleaggregatefunction.md)
-
-Es apropiado usar `AggregatingMergeTree` si reduce el número de filas por pedidos.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = AggregatingMergeTree()
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[TTL expr]
-[SETTINGS name=value, ...]
-```
-
-Para obtener una descripción de los parámetros de solicitud, consulte [descripción de la solicitud](../../../sql-reference/statements/create.md).
-
-**Cláusulas de consulta**
-
-Al crear un `AggregatingMergeTree` mesa de la misma [clausula](mergetree.md) se requieren, como al crear un `MergeTree` tabla.
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No use este método en proyectos nuevos y, si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] AggregatingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity)
-```
-
-Todos los parámetros tienen el mismo significado que en `MergeTree`.
-</details>
-
-## SELECCIONAR e INSERTAR {#select-and-insert}
-
-Para insertar datos, utilice [INSERT SELECT](../../../sql-reference/statements/insert-into.md) consulta con funciones agregadas -State-.
-Al seleccionar datos de `AggregatingMergeTree` mesa, uso `GROUP BY` cláusula y las mismas funciones agregadas que al insertar datos, pero usando `-Merge` sufijo.
-
-En los resultados de `SELECT` consulta, los valores de `AggregateFunction` tipo tiene representación binaria específica de la implementación para todos los formatos de salida de ClickHouse. Si volcar datos en, por ejemplo, `TabSeparated` formato con `SELECT` consulta entonces este volcado se puede cargar de nuevo usando `INSERT` consulta.
-
-## Ejemplo de una vista materializada agregada {#example-of-an-aggregated-materialized-view}
-
-`AggregatingMergeTree` vista materializada que mira el `test.visits` tabla:
-
-``` sql
-CREATE MATERIALIZED VIEW test.basic
-ENGINE = AggregatingMergeTree() PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate)
-AS SELECT
-    CounterID,
-    StartDate,
-    sumState(Sign)    AS Visits,
-    uniqState(UserID) AS Users
-FROM test.visits
-GROUP BY CounterID, StartDate;
-```
-
-Insertar datos en el `test.visits` tabla.
-
-``` sql
-INSERT INTO test.visits ...
-```
-
-Los datos se insertan tanto en la tabla como en la vista `test.basic` que realizará la agregación.
-
-Para obtener los datos agregados, necesitamos ejecutar una consulta como `SELECT ... GROUP BY ...` de la vista `test.basic`:
-
-``` sql
-SELECT
-    StartDate,
-    sumMerge(Visits) AS Visits,
-    uniqMerge(Users) AS Users
-FROM test.basic
-GROUP BY StartDate
-ORDER BY StartDate;
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/aggregatingmergetree/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/collapsingmergetree.md b/docs/es/engines/table-engines/mergetree-family/collapsingmergetree.md
deleted file mode 100644
index 027d5c2adf72..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/collapsingmergetree.md
+++ /dev/null
@@ -1,306 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: ColapsarMergeTree
----
-
-# ColapsarMergeTree {#table_engine-collapsingmergetree}
-
-El motor hereda de [Método de codificación de datos:](mergetree.md) y agrega la lógica de las filas que colapsan al algoritmo de fusión de partes de datos.
-
-`CollapsingMergeTree` elimina de forma asincrónica (colapsa) pares de filas si todos los campos de una clave de ordenación (`ORDER BY`) son equivalentes excepto el campo particular `Sign` que puede tener `1` y `-1` valor. Las filas sin un par se mantienen. Para más detalles, consulte el [Derrumbar](#table_engine-collapsingmergetree-collapsing) sección del documento.
-
-El motor puede reducir significativamente el volumen de almacenamiento y aumentar la eficiencia de `SELECT` consulta como consecuencia.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = CollapsingMergeTree(sign)
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Para obtener una descripción de los parámetros de consulta, consulte [descripción de la consulta](../../../sql-reference/statements/create.md).
-
-**CollapsingMergeTree Parámetros**
-
--   `sign` — Name of the column with the type of row: `1` es una “state” fila, `-1` es una “cancel” fila.
-
-    Column data type — `Int8`.
-
-**Cláusulas de consulta**
-
-Al crear un `CollapsingMergeTree` mesa, la misma [cláusulas de consulta](mergetree.md#table_engine-mergetree-creating-a-table) se requieren, como al crear un `MergeTree` tabla.
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No use este método en proyectos nuevos y, si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] CollapsingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, sign)
-```
-
-Todos los parámetros excepto `sign` el mismo significado que en `MergeTree`.
-
--   `sign` — Name of the column with the type of row: `1` — “state” fila, `-1` — “cancel” fila.
-
-    Column Data Type — `Int8`.
-
-</details>
-
-## Derrumbar {#table_engine-collapsingmergetree-collapsing}
-
-### Datos {#data}
-
-Considere la situación en la que necesita guardar datos que cambian continuamente para algún objeto. Parece lógico tener una fila para un objeto y actualizarla en cualquier cambio, pero la operación de actualización es costosa y lenta para DBMS porque requiere la reescritura de los datos en el almacenamiento. Si necesita escribir datos rápidamente, la actualización no es aceptable, pero puede escribir los cambios de un objeto secuencialmente de la siguiente manera.
-
-Utilice la columna en particular `Sign`. Si `Sign = 1` significa que la fila es un estado de un objeto, llamémoslo “state” fila. Si `Sign = -1` significa la cancelación del estado de un objeto con los mismos atributos, llamémoslo “cancel” fila.
-
-Por ejemplo, queremos calcular cuántas páginas revisaron los usuarios en algún sitio y cuánto tiempo estuvieron allí. En algún momento escribimos la siguiente fila con el estado de la actividad del usuario:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-En algún momento después registramos el cambio de actividad del usuario y lo escribimos con las siguientes dos filas.
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-La primera fila cancela el estado anterior del objeto (usuario). Debe copiar los campos clave de ordenación del estado cancelado exceptuando `Sign`.
-
-La segunda fila contiene el estado actual.
-
-Como solo necesitamos el último estado de actividad del usuario, las filas
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-│ 4324182021466249494 │         5 │      146 │   -1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-se puede eliminar colapsando el estado no válido (antiguo) de un objeto. `CollapsingMergeTree` hace esto mientras se fusionan las partes de datos.
-
-Por qué necesitamos 2 filas para cada cambio leído en el [Algoritmo](#table_engine-collapsingmergetree-collapsing-algorithm) apartado.
-
-**Propiedades peculiares de tal enfoque**
-
-1.  El programa que escribe los datos debe recordar el estado de un objeto para poder cancelarlo. “Cancel” debe contener copias de los campos de clave de ordenación “state” y lo opuesto `Sign`. Aumenta el tamaño inicial de almacenamiento, pero permite escribir los datos rápidamente.
-2.  Las matrices de largo crecimiento en columnas reducen la eficiencia del motor debido a la carga para escribir. Los datos más sencillos, mayor será la eficiencia.
-3.  El `SELECT` Los resultados dependen en gran medida de la consistencia del historial de cambios de objetos. Sea preciso al preparar los datos para insertarlos. Puede obtener resultados impredecibles en datos incoherentes, por ejemplo, valores negativos para métricas no negativas, como la profundidad de la sesión.
-
-### Algoritmo {#table_engine-collapsingmergetree-collapsing-algorithm}
-
-Cuando ClickHouse combina partes de datos, cada grupo de filas consecutivas tiene la misma clave de ordenación (`ORDER BY`) se reduce a no más de dos filas, una con `Sign = 1` (“state” fila) y otro con `Sign = -1` (“cancel” fila). En otras palabras, las entradas colapsan.
-
-Para cada parte de datos resultante, ClickHouse guarda:
-
-1.  El primero “cancel” y el último “state” si el número de “state” y “cancel” y la última fila es una “state” fila.
-2.  El último “state” fila, si hay más “state” filas que “cancel” filas.
-3.  El primero “cancel” fila, si hay más “cancel” filas que “state” filas.
-4.  Ninguna de las filas, en todos los demás casos.
-
-También cuando hay al menos 2 más “state” filas que “cancel” filas, o al menos 2 más “cancel” filas entonces “state” fila, la fusión continúa, pero ClickHouse trata esta situación como un error lógico y la registra en el registro del servidor. Este error puede producirse si se insertan los mismos datos más de una vez.
-
-Por lo tanto, el colapso no debe cambiar los resultados del cálculo de las estadísticas.
-Los cambios colapsaron gradualmente para que al final solo quedara el último estado de casi todos los objetos.
-
-El `Sign` se requiere porque el algoritmo de fusión no garantiza que todas las filas con la misma clave de clasificación estén en la misma parte de datos resultante e incluso en el mismo servidor físico. Proceso de ClickHouse `SELECT` consultas con múltiples hilos, y no puede predecir el orden de las filas en el resultado. La agregación es necesaria si hay una necesidad de obtener completamente “collapsed” datos de `CollapsingMergeTree` tabla.
-
-Para finalizar el colapso, escriba una consulta con `GROUP BY` cláusula y funciones agregadas que representan el signo. Por ejemplo, para calcular la cantidad, use `sum(Sign)` en lugar de `count()`. Para calcular la suma de algo, use `sum(Sign * x)` en lugar de `sum(x)` y así sucesivamente, y también añadir `HAVING sum(Sign) > 0`.
-
-Los agregados `count`, `sum` y `avg` podría calcularse de esta manera. El agregado `uniq` podría calcularse si un objeto tiene al menos un estado no colapsado. Los agregados `min` y `max` no se pudo calcular porque `CollapsingMergeTree` no guarda el historial de valores de los estados colapsados.
-
-Si necesita extraer datos sin agregación (por ejemplo, para comprobar si hay filas presentes cuyos valores más recientes coinciden con ciertas condiciones), puede utilizar el `FINAL` modificador para el `FROM` clausula. Este enfoque es significativamente menos eficiente.
-
-## Ejemplo de uso {#example-of-use}
-
-Datos de ejemplo:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-Creación de la tabla:
-
-``` sql
-CREATE TABLE UAct
-(
-    UserID UInt64,
-    PageViews UInt8,
-    Duration UInt8,
-    Sign Int8
-)
-ENGINE = CollapsingMergeTree(Sign)
-ORDER BY UserID
-```
-
-Inserción de los datos:
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1)
-```
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1),(4324182021466249494, 6, 185, 1)
-```
-
-Usamos dos `INSERT` consultas para crear dos partes de datos diferentes. Si insertamos los datos con una consulta, ClickHouse crea una parte de datos y nunca realizará ninguna fusión.
-
-Obtener los datos:
-
-``` sql
-SELECT * FROM UAct
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-¿Qué vemos y dónde está colapsando?
-
-Con dos `INSERT` consultas, hemos creado 2 partes de datos. El `SELECT` la consulta se realizó en 2 hilos, y obtuvimos un orden aleatorio de filas. No se ha producido un colapso porque todavía no se había fusionado las partes de datos. ClickHouse fusiona parte de datos en un momento desconocido que no podemos predecir.
-
-Por lo tanto, necesitamos agregación:
-
-``` sql
-SELECT
-    UserID,
-    sum(PageViews * Sign) AS PageViews,
-    sum(Duration * Sign) AS Duration
-FROM UAct
-GROUP BY UserID
-HAVING sum(Sign) > 0
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┐
-│ 4324182021466249494 │         6 │      185 │
-└─────────────────────┴───────────┴──────────┘
-```
-
-Si no necesitamos agregación y queremos forzar el colapso, podemos usar `FINAL` modificador para `FROM` clausula.
-
-``` sql
-SELECT * FROM UAct FINAL
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-Esta forma de seleccionar los datos es muy ineficiente. No lo use para mesas grandes.
-
-## Ejemplo de otro enfoque {#example-of-another-approach}
-
-Datos de ejemplo:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-│ 4324182021466249494 │        -5 │     -146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-La idea es que las fusiones tengan en cuenta solo los campos clave. Y en el “Cancel” línea podemos especificar valores negativos que igualan la versión anterior de la fila al sumar sin usar la columna Sign. Para este enfoque, es necesario cambiar el tipo de datos `PageViews`,`Duration` para almacenar valores negativos de UInt8 -\> Int16.
-
-``` sql
-CREATE TABLE UAct
-(
-    UserID UInt64,
-    PageViews Int16,
-    Duration Int16,
-    Sign Int8
-)
-ENGINE = CollapsingMergeTree(Sign)
-ORDER BY UserID
-```
-
-Vamos a probar el enfoque:
-
-``` sql
-insert into UAct values(4324182021466249494,  5,  146,  1);
-insert into UAct values(4324182021466249494, -5, -146, -1);
-insert into UAct values(4324182021466249494,  6,  185,  1);
-
-select * from UAct final; // avoid using final in production (just for a test or small tables)
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-``` sql
-SELECT
-    UserID,
-    sum(PageViews) AS PageViews,
-    sum(Duration) AS Duration
-FROM UAct
-GROUP BY UserID
-```text
-┌──────────────UserID─┬─PageViews─┬─Duration─┐
-│ 4324182021466249494 │         6 │      185 │
-└─────────────────────┴───────────┴──────────┘
-```
-
-``` sqk
-select count() FROM UAct
-```
-
-``` text
-┌─count()─┐
-│       3 │
-└─────────┘
-```
-
-``` sql
-optimize table UAct final;
-
-select * FROM UAct
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/collapsingmergetree/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/custom-partitioning-key.md b/docs/es/engines/table-engines/mergetree-family/custom-partitioning-key.md
deleted file mode 100644
index 6cbc0a9192e9..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/custom-partitioning-key.md
+++ /dev/null
@@ -1,127 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 32
-toc_title: "Clave de partici\xF3n personalizada"
----
-
-# Clave de partición personalizada {#custom-partitioning-key}
-
-La partición está disponible para el [Método de codificación de datos:](mergetree.md) mesas familiares (incluyendo [repetición](replication.md) tabla). [Vistas materializadas](../special/materializedview.md#materializedview) basado en tablas MergeTree soporte de particionamiento, también.
-
-Una partición es una combinación lógica de registros en una tabla por un criterio especificado. Puede establecer una partición por un criterio arbitrario, como por mes, por día o por tipo de evento. Cada partición se almacena por separado para simplificar las manipulaciones de estos datos. Al acceder a los datos, ClickHouse utiliza el subconjunto más pequeño de particiones posible.
-
-La partición se especifica en el `PARTITION BY expr` cláusula cuando [creando una tabla](mergetree.md#table_engine-mergetree-creating-a-table). La clave de partición puede ser cualquier expresión de las columnas de la tabla. Por ejemplo, para especificar la partición por mes, utilice la expresión `toYYYYMM(date_column)`:
-
-``` sql
-CREATE TABLE visits
-(
-    VisitDate Date,
-    Hour UInt8,
-    ClientID UUID
-)
-ENGINE = MergeTree()
-PARTITION BY toYYYYMM(VisitDate)
-ORDER BY Hour;
-```
-
-La clave de partición también puede ser una tupla de expresiones (similar a la [clave primaria](mergetree.md#primary-keys-and-indexes-in-queries)). Por ejemplo:
-
-``` sql
-ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)
-PARTITION BY (toMonday(StartDate), EventType)
-ORDER BY (CounterID, StartDate, intHash32(UserID));
-```
-
-En este ejemplo, establecemos la partición por los tipos de eventos que se produjeron durante la semana actual.
-
-Al insertar datos nuevos en una tabla, estos datos se almacenan como una parte separada (porción) ordenada por la clave principal. En 10-15 minutos después de insertar, las partes de la misma partición se fusionan en toda la parte.
-
-!!! info "INFO"
-    Una combinación solo funciona para partes de datos que tienen el mismo valor para la expresión de partición. Esto significa **no deberías hacer particiones demasiado granulares** (más de un millar de particiones). De lo contrario, el `SELECT` consulta funciona mal debido a un número excesivamente grande de archivos en el sistema de archivos y descriptores de archivos abiertos.
-
-Utilice el [sistema.parte](../../../operations/system-tables.md#system_tables-parts) tabla para ver las partes y particiones de la tabla. Por ejemplo, supongamos que tenemos un `visits` tabla con partición por mes. Vamos a realizar el `SELECT` consulta para el `system.parts` tabla:
-
-``` sql
-SELECT
-    partition,
-    name,
-    active
-FROM system.parts
-WHERE table = 'visits'
-```
-
-``` text
-┌─partition─┬─name───────────┬─active─┐
-│ 201901    │ 201901_1_3_1   │      0 │
-│ 201901    │ 201901_1_9_2   │      1 │
-│ 201901    │ 201901_8_8_0   │      0 │
-│ 201901    │ 201901_9_9_0   │      0 │
-│ 201902    │ 201902_4_6_1   │      1 │
-│ 201902    │ 201902_10_10_0 │      1 │
-│ 201902    │ 201902_11_11_0 │      1 │
-└───────────┴────────────────┴────────┘
-```
-
-El `partition` columna contiene los nombres de las particiones. Hay dos particiones en este ejemplo: `201901` y `201902`. Puede utilizar este valor de columna para especificar el nombre de partición en [ALTER … PARTITION](#alter_manipulations-with-partitions) consulta.
-
-El `name` columna contiene los nombres de las partes de datos de partición. Puede utilizar esta columna para especificar el nombre de la pieza [ALTER ATTACH PART](#alter_attach-partition) consulta.
-
-Vamos a desglosar el nombre de la primera parte: `201901_1_3_1`:
-
--   `201901` es el nombre de la partición.
--   `1` es el número mínimo del bloque de datos.
--   `3` es el número máximo del bloque de datos.
--   `1` es el nivel de fragmento (la profundidad del árbol de fusión del que se forma).
-
-!!! info "INFO"
-    Las partes de las tablas de tipo antiguo tienen el nombre: `20190117_20190123_2_2_0` (fecha mínima - fecha máxima - número de bloque mínimo - número de bloque máximo - nivel).
-
-El `active` columna muestra el estado de la pieza. `1` está activo; `0` está inactivo. Las partes inactivas son, por ejemplo, las partes de origen que quedan después de fusionarse con una parte más grande. Las partes de datos dañadas también se indican como inactivas.
-
-Como puede ver en el ejemplo, hay varias partes separadas de la misma partición (por ejemplo, `201901_1_3_1` y `201901_1_9_2`). Esto significa que estas partes aún no están fusionadas. ClickHouse combina las partes insertadas de datos periódicamente, aproximadamente 15 minutos después de la inserción. Además, puede realizar una fusión no programada utilizando el [OPTIMIZE](../../../sql-reference/statements/misc.md#misc_operations-optimize) consulta. Ejemplo:
-
-``` sql
-OPTIMIZE TABLE visits PARTITION 201902;
-```
-
-``` text
-┌─partition─┬─name───────────┬─active─┐
-│ 201901    │ 201901_1_3_1   │      0 │
-│ 201901    │ 201901_1_9_2   │      1 │
-│ 201901    │ 201901_8_8_0   │      0 │
-│ 201901    │ 201901_9_9_0   │      0 │
-│ 201902    │ 201902_4_6_1   │      0 │
-│ 201902    │ 201902_4_11_2  │      1 │
-│ 201902    │ 201902_10_10_0 │      0 │
-│ 201902    │ 201902_11_11_0 │      0 │
-└───────────┴────────────────┴────────┘
-```
-
-Las partes inactivas se eliminarán aproximadamente 10 minutos después de la fusión.
-
-Otra forma de ver un conjunto de partes y particiones es ir al directorio de la tabla: `/var/lib/clickhouse/data/<database>/<table>/`. Por ejemplo:
-
-``` bash
-/var/lib/clickhouse/data/default/visits$ ls -l
-total 40
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  1 16:48 201901_1_3_1
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:17 201901_1_9_2
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 15:52 201901_8_8_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 15:52 201901_9_9_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:17 201902_10_10_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:17 201902_11_11_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:19 201902_4_11_2
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 12:09 201902_4_6_1
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  1 16:48 detached
-```
-
-Carpeta ‘201901_1_1_0’, ‘201901_1_7_1’ y así sucesivamente son los directorios de las partes. Cada parte se relaciona con una partición correspondiente y contiene datos solo para un mes determinado (la tabla de este ejemplo tiene particiones por mes).
-
-El `detached` el directorio contiene partes que se separaron de la tabla utilizando el [DETACH](../../../sql-reference/statements/alter.md#alter_detach-partition) consulta. Las partes dañadas también se mueven a este directorio, en lugar de eliminarse. El servidor no utiliza las piezas del `detached` directory. You can add, delete, or modify the data in this directory at any time – the server will not know about this until you run the [ATTACH](../../../sql-reference/statements/alter.md#alter_attach-partition) consulta.
-
-Tenga en cuenta que en el servidor operativo, no puede cambiar manualmente el conjunto de piezas o sus datos en el sistema de archivos, ya que el servidor no lo sabrá. Para tablas no replicadas, puede hacer esto cuando se detiene el servidor, pero no se recomienda. Para tablas replicadas, el conjunto de piezas no se puede cambiar en ningún caso.
-
-ClickHouse le permite realizar operaciones con las particiones: eliminarlas, copiar de una tabla a otra o crear una copia de seguridad. Consulte la lista de todas las operaciones en la sección [Manipulaciones con particiones y piezas](../../../sql-reference/statements/alter.md#alter_manipulations-with-partitions).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/custom_partitioning_key/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/graphitemergetree.md b/docs/es/engines/table-engines/mergetree-family/graphitemergetree.md
deleted file mode 100644
index d33ddcebac29..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/graphitemergetree.md
+++ /dev/null
@@ -1,174 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: GraphiteMergeTree
----
-
-# GraphiteMergeTree {#graphitemergetree}
-
-Este motor está diseñado para el adelgazamiento y la agregación / promedio (rollup) [Grafito](http://graphite.readthedocs.io/en/latest/index.html) datos. Puede ser útil para los desarrolladores que desean usar ClickHouse como almacén de datos para Graphite.
-
-Puede usar cualquier motor de tabla ClickHouse para almacenar los datos de Graphite si no necesita un paquete acumulativo, pero si necesita un paquete acumulativo, use `GraphiteMergeTree`. El motor reduce el volumen de almacenamiento y aumenta la eficiencia de las consultas de Graphite.
-
-El motor hereda propiedades de [Método de codificación de datos:](mergetree.md).
-
-## Creación de una tabla {#creating-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    Path String,
-    Time DateTime,
-    Value <Numeric_type>,
-    Version <Numeric_type>
-    ...
-) ENGINE = GraphiteMergeTree(config_section)
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Vea una descripción detallada del [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) consulta.
-
-Una tabla para los datos de grafito debe tener las siguientes columnas para los siguientes datos:
-
--   Nombre métrico (sensor de grafito). Tipo de datos: `String`.
-
--   Tiempo de medición de la métrica. Tipo de datos: `DateTime`.
-
--   Valor de la métrica. Tipo de datos: cualquier numérico.
-
--   Versión de la métrica. Tipo de datos: cualquier numérico.
-
-    ClickHouse guarda las filas con la versión más alta o la última escrita si las versiones son las mismas. Otras filas se eliminan durante la fusión de partes de datos.
-
-Los nombres de estas columnas deben establecerse en la configuración acumulativa.
-
-**GraphiteMergeTree parámetros**
-
--   `config_section` — Name of the section in the configuration file, where are the rules of rollup set.
-
-**Cláusulas de consulta**
-
-Al crear un `GraphiteMergeTree` mesa, la misma [clausula](mergetree.md#table_engine-mergetree-creating-a-table) se requieren, como al crear un `MergeTree` tabla.
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No use este método en proyectos nuevos y, si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    EventDate Date,
-    Path String,
-    Time DateTime,
-    Value <Numeric_type>,
-    Version <Numeric_type>
-    ...
-) ENGINE [=] GraphiteMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, config_section)
-```
-
-Todos los parámetros excepto `config_section` el mismo significado que en `MergeTree`.
-
--   `config_section` — Name of the section in the configuration file, where are the rules of rollup set.
-
-</details>
-
-## Configuración acumulativa {#rollup-configuration}
-
-La configuración del paquete acumulativo está definida por [graphite_rollup](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-graphite) parámetro en la configuración del servidor. El nombre del parámetro podría ser cualquiera. Puede crear varias configuraciones y usarlas para diferentes tablas.
-
-Estructura de configuración Rollup:
-
-      required-columns
-      patterns
-
-### Columnas requeridas {#required-columns}
-
--   `path_column_name` — The name of the column storing the metric name (Graphite sensor). Default value: `Path`.
--   `time_column_name` — The name of the column storing the time of measuring the metric. Default value: `Time`.
--   `value_column_name` — The name of the column storing the value of the metric at the time set in `time_column_name`. Valor predeterminado: `Value`.
--   `version_column_name` — The name of the column storing the version of the metric. Default value: `Timestamp`.
-
-### Patrón {#patterns}
-
-Estructura del `patterns` apartado:
-
-``` text
-pattern
-    regexp
-    function
-pattern
-    regexp
-    age + precision
-    ...
-pattern
-    regexp
-    function
-    age + precision
-    ...
-pattern
-    ...
-default
-    function
-    age + precision
-    ...
-```
-
-!!! warning "Atención"
-    Los patrones deben ser estrictamente ordenados:
-
-      1. Patterns without `function` or `retention`.
-      1. Patterns with both `function` and `retention`.
-      1. Pattern `default`.
-
-Al procesar una fila, ClickHouse comprueba las reglas en el `pattern` apartado. Cada uno de `pattern` (incluir `default` secciones pueden contener `function` parámetro para la agregación, `retention` parámetros o ambos. Si el nombre de la métrica coincide con `regexp`, las reglas de la `pattern` sección (o secciones); de lo contrario, las reglas de la `default` sección se utilizan.
-
-Campos para `pattern` y `default` apartado:
-
--   `regexp`– A pattern for the metric name.
--   `age` – The minimum age of the data in seconds.
--   `precision`– How precisely to define the age of the data in seconds. Should be a divisor for 86400 (seconds in a day).
--   `function` – The name of the aggregating function to apply to data whose age falls within the range `[age, age + precision]`.
-
-### Ejemplo de configuración {#configuration-example}
-
-``` xml
-<graphite_rollup>
-    <version_column_name>Version</version_column_name>
-    <pattern>
-        <regexp>click_cost</regexp>
-        <function>any</function>
-        <retention>
-            <age>0</age>
-            <precision>5</precision>
-        </retention>
-        <retention>
-            <age>86400</age>
-            <precision>60</precision>
-        </retention>
-    </pattern>
-    <default>
-        <function>max</function>
-        <retention>
-            <age>0</age>
-            <precision>60</precision>
-        </retention>
-        <retention>
-            <age>3600</age>
-            <precision>300</precision>
-        </retention>
-        <retention>
-            <age>86400</age>
-            <precision>3600</precision>
-        </retention>
-    </default>
-</graphite_rollup>
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/graphitemergetree/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/index.md b/docs/es/engines/table-engines/mergetree-family/index.md
deleted file mode 100644
index 359d58b2ff12..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Familia MergeTree
-toc_priority: 28
----
-
-
diff --git a/docs/es/engines/table-engines/mergetree-family/mergetree.md b/docs/es/engines/table-engines/mergetree-family/mergetree.md
deleted file mode 100644
index a4bab840b52a..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/mergetree.md
+++ /dev/null
@@ -1,654 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 30
-toc_title: "M\xE9todo de codificaci\xF3n de datos:"
----
-
-# Método de codificación de datos: {#table_engines-mergetree}
-
-El `MergeTree` motor y otros motores de esta familia (`*MergeTree`) son los motores de mesa ClickHouse más robustos.
-
-Motores en el `MergeTree` familia están diseñados para insertar una gran cantidad de datos en una tabla. Los datos se escriben rápidamente en la tabla parte por parte, luego se aplican reglas para fusionar las partes en segundo plano. Este método es mucho más eficiente que reescribir continuamente los datos en almacenamiento durante la inserción.
-
-Principales características:
-
--   Almacena datos ordenados por clave principal.
-
-    Esto le permite crear un pequeño índice disperso que ayuda a encontrar datos más rápido.
-
--   Las particiones se pueden utilizar si [clave de partición](custom-partitioning-key.md) se especifica.
-
-    ClickHouse admite ciertas operaciones con particiones que son más efectivas que las operaciones generales en los mismos datos con el mismo resultado. ClickHouse también corta automáticamente los datos de partición donde se especifica la clave de partición en la consulta. Esto también mejora el rendimiento de las consultas.
-
--   Soporte de replicación de datos.
-
-    La familia de `ReplicatedMergeTree` proporciona la replicación de datos. Para obtener más información, consulte [Replicación de datos](replication.md).
-
--   Soporte de muestreo de datos.
-
-    Si es necesario, puede establecer el método de muestreo de datos en la tabla.
-
-!!! info "INFO"
-    El [Fusionar](../special/merge.md#merge) el motor no pertenece al `*MergeTree` familia.
-
-## Creación de una tabla {#table_engine-mergetree-creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2],
-    ...
-    INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1,
-    INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2
-) ENGINE = MergeTree()
-[PARTITION BY expr]
-[ORDER BY expr]
-[PRIMARY KEY expr]
-[SAMPLE BY expr]
-[TTL expr [DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'], ...]
-[SETTINGS name=value, ...]
-```
-
-Para obtener una descripción de los parámetros, consulte [Descripción de la consulta CREATE](../../../sql-reference/statements/create.md).
-
-!!! note "Nota"
-    `INDEX` es una característica experimental, ver [Índices de saltos de datos](#table_engine-mergetree-data_skipping-indexes).
-
-### Cláusulas de consulta {#mergetree-query-clauses}
-
--   `ENGINE` — Name and parameters of the engine. `ENGINE = MergeTree()`. El `MergeTree` el motor no tiene parámetros.
-
--   `PARTITION BY` — The [clave de partición](custom-partitioning-key.md).
-
-    Para particionar por mes, utilice el `toYYYYMM(date_column)` expresión, donde `date_column` es una columna con una fecha del tipo [Fecha](../../../sql-reference/data-types/date.md). Los nombres de partición aquí tienen el `"YYYYMM"` formato.
-
--   `ORDER BY` — The sorting key.
-
-    Una tupla de columnas o expresiones arbitrarias. Ejemplo: `ORDER BY (CounterID, EventDate)`.
-
--   `PRIMARY KEY` — The primary key if it [difiere de la clave de clasificación](#choosing-a-primary-key-that-differs-from-the-sorting-key).
-
-    De forma predeterminada, la clave principal es la misma que la clave de ordenación (que se especifica `ORDER BY` clausula). Por lo tanto, en la mayoría de los casos no es necesario especificar un `PRIMARY KEY` clausula.
-
--   `SAMPLE BY` — An expression for sampling.
-
-    Si se utiliza una expresión de muestreo, la clave principal debe contenerla. Ejemplo: `SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID))`.
-
--   `TTL` — A list of rules specifying storage duration of rows and defining logic of automatic parts movement [entre discos y volúmenes](#table_engine-mergetree-multiple-volumes).
-
-    La expresión debe tener una `Date` o `DateTime` columna como resultado. Ejemplo:
-    `TTL date + INTERVAL 1 DAY`
-
-    Tipo de regla `DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'` especifica una acción que debe realizarse con la pieza si la expresión está satisfecha (alcanza la hora actual): eliminación de filas caducadas, mover una pieza (si la expresión está satisfecha para todas las filas de una pieza) al disco especificado (`TO DISK 'xxx'`) o al volumen (`TO VOLUME 'xxx'`). El tipo predeterminado de la regla es la eliminación (`DELETE`). Se puede especificar una lista de varias reglas, pero no debe haber más de una `DELETE` regla.
-
-    Para obtener más información, consulte [TTL para columnas y tablas](#table_engine-mergetree-ttl)
-
--   `SETTINGS` — Additional parameters that control the behavior of the `MergeTree`:
-
-    -   `index_granularity` — Maximum number of data rows between the marks of an index. Default value: 8192. See [Almacenamiento de datos](#mergetree-data-storage).
-    -   `index_granularity_bytes` — Maximum size of data granules in bytes. Default value: 10Mb. To restrict the granule size only by number of rows, set to 0 (not recommended). See [Almacenamiento de datos](#mergetree-data-storage).
-    -   `enable_mixed_granularity_parts` — Enables or disables transitioning to control the granule size with the `index_granularity_bytes` configuración. Antes de la versión 19.11, sólo existía el `index_granularity` ajuste para restringir el tamaño del gránulo. El `index_granularity_bytes` mejora el rendimiento de ClickHouse al seleccionar datos de tablas con filas grandes (decenas y cientos de megabytes). Si tiene tablas con filas grandes, puede habilitar esta configuración para que las tablas mejoren la eficiencia de `SELECT` consulta.
-    -   `use_minimalistic_part_header_in_zookeeper` — Storage method of the data parts headers in ZooKeeper. If `use_minimalistic_part_header_in_zookeeper=1`, entonces ZooKeeper almacena menos datos. Para obtener más información, consulte [descripción del ajuste](../../../operations/server-configuration-parameters/settings.md#server-settings-use_minimalistic_part_header_in_zookeeper) en “Server configuration parameters”.
-    -   `min_merge_bytes_to_use_direct_io` — The minimum data volume for merge operation that is required for using direct I/O access to the storage disk. When merging data parts, ClickHouse calculates the total storage volume of all the data to be merged. If the volume exceeds `min_merge_bytes_to_use_direct_io` bytes, ClickHouse lee y escribe los datos en el disco de almacenamiento utilizando la interfaz de E / S directa (`O_DIRECT` opcion). Si `min_merge_bytes_to_use_direct_io = 0`, entonces la E/S directa está deshabilitada. Valor predeterminado: `10 * 1024 * 1024 * 1024` byte.
-        <a name="mergetree_setting-merge_with_ttl_timeout"></a>
-    -   `merge_with_ttl_timeout` — Minimum delay in seconds before repeating a merge with TTL. Default value: 86400 (1 day).
-    -   `write_final_mark` — Enables or disables writing the final index mark at the end of data part (after the last byte). Default value: 1. Don't turn it off.
-    -   `merge_max_block_size` — Maximum number of rows in block for merge operations. Default value: 8192.
-    -   `storage_policy` — Storage policy. See [Uso de varios dispositivos de bloque para el almacenamiento de datos](#table_engine-mergetree-multiple-volumes).
-
-**Ejemplo de configuración de secciones**
-
-``` sql
-ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity=8192
-```
-
-En el ejemplo, configuramos la partición por mes.
-
-También establecemos una expresión para el muestreo como un hash por el ID de usuario. Esto le permite pseudoaleatorizar los datos en la tabla para cada `CounterID` y `EventDate`. Si define un [SAMPLE](../../../sql-reference/statements/select/sample.md#select-sample-clause) cláusula al seleccionar los datos, ClickHouse devolverá una muestra de datos pseudoaleatoria uniforme para un subconjunto de usuarios.
-
-El `index_granularity` se puede omitir porque 8192 es el valor predeterminado.
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No utilice este método en nuevos proyectos. Si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] MergeTree(date-column [, sampling_expression], (primary, key), index_granularity)
-```
-
-**Parámetros MergeTree()**
-
--   `date-column` — The name of a column of the [Fecha](../../../sql-reference/data-types/date.md) tipo. ClickHouse crea automáticamente particiones por mes en función de esta columna. Los nombres de partición están en el `"YYYYMM"` formato.
--   `sampling_expression` — An expression for sampling.
--   `(primary, key)` — Primary key. Type: [Tupla()](../../../sql-reference/data-types/tuple.md)
--   `index_granularity` — The granularity of an index. The number of data rows between the “marks” de un índice. El valor 8192 es apropiado para la mayoría de las tareas.
-
-**Ejemplo**
-
-``` sql
-MergeTree(EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID)), 8192)
-```
-
-El `MergeTree` engine se configura de la misma manera que en el ejemplo anterior para el método de configuración del motor principal.
-</details>
-
-## Almacenamiento de datos {#mergetree-data-storage}
-
-Una tabla consta de partes de datos ordenadas por clave principal.
-
-Cuando se insertan datos en una tabla, se crean partes de datos separadas y cada una de ellas se ordena lexicográficamente por clave principal. Por ejemplo, si la clave principal es `(CounterID, Date)`, los datos en la parte se ordenan por `CounterID`, y dentro de cada `CounterID` es ordenado por `Date`.
-
-Los datos que pertenecen a diferentes particiones se separan en diferentes partes. En el fondo, ClickHouse combina partes de datos para un almacenamiento más eficiente. Las piezas que pertenecen a particiones diferentes no se fusionan. El mecanismo de combinación no garantiza que todas las filas con la misma clave principal estén en la misma parte de datos.
-
-Cada parte de datos se divide lógicamente en gránulos. Un gránulo es el conjunto de datos indivisibles más pequeño que ClickHouse lee al seleccionar datos. ClickHouse no divide filas o valores, por lo que cada gránulo siempre contiene un número entero de filas. La primera fila de un gránulo está marcada con el valor de la clave principal de la fila. Para cada parte de datos, ClickHouse crea un archivo de índice que almacena las marcas. Para cada columna, ya sea en la clave principal o no, ClickHouse también almacena las mismas marcas. Estas marcas le permiten encontrar datos directamente en archivos de columnas.
-
-El tamaño del gránulo es restringido por `index_granularity` y `index_granularity_bytes` configuración del motor de tabla. El número de filas en un gránulo se encuentra en el `[1, index_granularity]` rango, dependiendo del tamaño de las filas. El tamaño de un gránulo puede exceder `index_granularity_bytes` si el tamaño de una sola fila es mayor que el valor de la configuración. En este caso, el tamaño del gránulo es igual al tamaño de la fila.
-
-## Claves e índices principales en consultas {#primary-keys-and-indexes-in-queries}
-
-Tome el `(CounterID, Date)` clave primaria como ejemplo. En este caso, la clasificación y el índice se pueden ilustrar de la siguiente manera:
-
-      Whole data:     [---------------------------------------------]
-      CounterID:      [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]
-      Date:           [1111111222222233331233211111222222333211111112122222223111112223311122333]
-      Marks:           |      |      |      |      |      |      |      |      |      |      |
-                      a,1    a,2    a,3    b,3    e,2    e,3    g,1    h,2    i,1    i,3    l,3
-      Marks numbers:   0      1      2      3      4      5      6      7      8      9      10
-
-Si la consulta de datos especifica:
-
--   `CounterID in ('a', 'h')`, el servidor lee los datos en los rangos de marcas `[0, 3)` y `[6, 8)`.
--   `CounterID IN ('a', 'h') AND Date = 3`, el servidor lee los datos en los rangos de marcas `[1, 3)` y `[7, 8)`.
--   `Date = 3`, el servidor lee los datos en el rango de marcas `[1, 10]`.
-
-Los ejemplos anteriores muestran que siempre es más efectivo usar un índice que un análisis completo.
-
-Un índice disperso permite leer datos adicionales. Al leer un único rango de la clave primaria, hasta `index_granularity * 2` se pueden leer filas adicionales en cada bloque de datos.
-
-Los índices dispersos le permiten trabajar con una gran cantidad de filas de tabla, porque en la mayoría de los casos, dichos índices caben en la RAM de la computadora.
-
-ClickHouse no requiere una clave principal única. Puede insertar varias filas con la misma clave principal.
-
-### Selección de la clave principal {#selecting-the-primary-key}
-
-El número de columnas en la clave principal no está explícitamente limitado. Dependiendo de la estructura de datos, puede incluir más o menos columnas en la clave principal. Esto puede:
-
--   Mejorar el rendimiento de un índice.
-
-    Si la clave principal es `(a, b)`, a continuación, añadir otra columna `c` mejorará el rendimiento si se cumplen las siguientes condiciones:
-
-    -   Hay consultas con una condición en la columna `c`.
-    -   Rangos de datos largos (varias veces más `index_granularity`) con valores idénticos para `(a, b)` son comunes. En otras palabras, al agregar otra columna le permite omitir rangos de datos bastante largos.
-
--   Mejorar la compresión de datos.
-
-    ClickHouse ordena los datos por clave principal, por lo que cuanto mayor sea la consistencia, mejor será la compresión.
-
--   Proporcione una lógica adicional al fusionar partes de datos en el [ColapsarMergeTree](collapsingmergetree.md#table_engine-collapsingmergetree) y [SummingMergeTree](summingmergetree.md) motor.
-
-    En este caso tiene sentido especificar el *clave de clasificación* que es diferente de la clave principal.
-
-Una clave principal larga afectará negativamente al rendimiento de la inserción y al consumo de memoria, pero las columnas adicionales de la clave principal no afectarán al rendimiento de ClickHouse durante `SELECT` consulta.
-
-### Elegir una clave principal que difiere de la clave de ordenación {#choosing-a-primary-key-that-differs-from-the-sorting-key}
-
-Es posible especificar una clave principal (una expresión con valores que se escriben en el archivo de índice para cada marca) que es diferente de la clave de ordenación (una expresión para ordenar las filas en partes de datos). En este caso, la tupla de expresión de clave primaria debe ser un prefijo de la tupla de expresión de clave de ordenación.
-
-Esta característica es útil cuando se [SummingMergeTree](summingmergetree.md) y
-[AgregaciónMergeTree](aggregatingmergetree.md) motores de mesa. En un caso común cuando se utilizan estos motores, la tabla tiene dos tipos de columnas: *cota* y *medida*. Las consultas típicas agregan valores de columnas de medida con `GROUP BY` y filtrado por dimensiones. Debido a que SummingMergeTree y AggregatingMergeTree agregan filas con el mismo valor de la clave de ordenación, es natural agregarle todas las dimensiones. Como resultado, la expresión de clave consta de una larga lista de columnas y esta lista debe actualizarse con frecuencia con las dimensiones recién agregadas.
-
-En este caso, tiene sentido dejar solo unas pocas columnas en la clave principal que proporcionarán análisis de rango eficientes y agregarán las columnas de dimensión restantes a la tupla de clave de clasificación.
-
-[ALTER](../../../sql-reference/statements/alter.md) de la clave de ordenación es una operación ligera porque cuando se agrega una nueva columna simultáneamente a la tabla y a la clave de ordenación, las partes de datos existentes no necesitan ser cambiadas. Dado que la clave de ordenación anterior es un prefijo de la nueva clave de ordenación y no hay datos en la columna recién agregada, los datos se ordenan tanto por las claves de ordenación antiguas como por las nuevas en el momento de la modificación de la tabla.
-
-### Uso de índices y particiones en consultas {#use-of-indexes-and-partitions-in-queries}
-
-Para `SELECT` consultas, ClickHouse analiza si se puede usar un índice. Se puede usar un índice si el `WHERE/PREWHERE` clause tiene una expresión (como uno de los elementos de conjunción, o enteramente) que representa una operación de comparación de igualdad o desigualdad, o si tiene `IN` o `LIKE` con un prefijo fijo en columnas o expresiones que están en la clave principal o clave de partición, o en ciertas funciones parcialmente repetitivas de estas columnas, o relaciones lógicas de estas expresiones.
-
-Por lo tanto, es posible ejecutar rápidamente consultas en uno o varios rangos de la clave principal. En este ejemplo, las consultas serán rápidas cuando se ejecuten para una etiqueta de seguimiento específica, para una etiqueta y un intervalo de fechas específicos, para una etiqueta y una fecha específicas, para varias etiquetas con un intervalo de fechas, etc.
-
-Veamos el motor configurado de la siguiente manera:
-
-      ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate) SETTINGS index_granularity=8192
-
-En este caso, en consultas:
-
-``` sql
-SELECT count() FROM table WHERE EventDate = toDate(now()) AND CounterID = 34
-SELECT count() FROM table WHERE EventDate = toDate(now()) AND (CounterID = 34 OR CounterID = 42)
-SELECT count() FROM table WHERE ((EventDate >= toDate('2014-01-01') AND EventDate <= toDate('2014-01-31')) OR EventDate = toDate('2014-05-01')) AND CounterID IN (101500, 731962, 160656) AND (CounterID = 101500 OR EventDate != toDate('2014-05-01'))
-```
-
-ClickHouse utilizará el índice de clave principal para recortar datos incorrectos y la clave de partición mensual para recortar particiones que están en intervalos de fechas incorrectos.
-
-Las consultas anteriores muestran que el índice se usa incluso para expresiones complejas. La lectura de la tabla está organizada de modo que el uso del índice no puede ser más lento que un escaneo completo.
-
-En el siguiente ejemplo, el índice no se puede usar.
-
-``` sql
-SELECT count() FROM table WHERE CounterID = 34 OR URL LIKE '%upyachka%'
-```
-
-Para comprobar si ClickHouse puede usar el índice al ejecutar una consulta, use la configuración [Fecha de nacimiento](../../../operations/settings/settings.md#settings-force_index_by_date) y [force_primary_key](../../../operations/settings/settings.md).
-
-La clave para particionar por mes permite leer solo aquellos bloques de datos que contienen fechas del rango adecuado. En este caso, el bloque de datos puede contener datos para muchas fechas (hasta un mes). Dentro de un bloque, los datos se ordenan por clave principal, que puede no contener la fecha como la primera columna. Debido a esto, el uso de una consulta con solo una condición de fecha que no especifica el prefijo de clave principal hará que se lean más datos que para una sola fecha.
-
-### Uso del índice para claves primarias parcialmente monótonas {#use-of-index-for-partially-monotonic-primary-keys}
-
-Considere, por ejemplo, los días del mes. Ellos forman un [monótona secuencia](https://en.wikipedia.org/wiki/Monotonic_function) durante un mes, pero no monótono durante períodos más prolongados. Esta es una secuencia parcialmente monotónica. Si un usuario crea la tabla con clave primaria parcialmente monótona, ClickHouse crea un índice disperso como de costumbre. Cuando un usuario selecciona datos de este tipo de tabla, ClickHouse analiza las condiciones de consulta. Si el usuario desea obtener datos entre dos marcas del índice y ambas marcas caen dentro de un mes, ClickHouse puede usar el índice en este caso particular porque puede calcular la distancia entre los parámetros de una consulta y las marcas de índice.
-
-ClickHouse no puede usar un índice si los valores de la clave principal en el rango de parámetros de consulta no representan una secuencia monotónica. En este caso, ClickHouse utiliza el método de análisis completo.
-
-ClickHouse usa esta lógica no solo para secuencias de días del mes, sino para cualquier clave principal que represente una secuencia parcialmente monotónica.
-
-### Índices de saltos de datos (experimental) {#table_engine-mergetree-data_skipping-indexes}
-
-La declaración de índice se encuentra en la sección de columnas del `CREATE` consulta.
-
-``` sql
-INDEX index_name expr TYPE type(...) GRANULARITY granularity_value
-```
-
-Para tablas de la `*MergeTree` familia, se pueden especificar índices de omisión de datos.
-
-Estos índices agregan cierta información sobre la expresión especificada en bloques, que consisten en `granularity_value` gránulos (el tamaño del gránulo se especifica utilizando el `index_granularity` ajuste en el motor de la tabla). Entonces estos agregados se usan en `SELECT` consultas para reducir la cantidad de datos a leer desde el disco omitiendo grandes bloques de datos donde el `where` consulta no puede ser satisfecha.
-
-**Ejemplo**
-
-``` sql
-CREATE TABLE table_name
-(
-    u64 UInt64,
-    i32 Int32,
-    s String,
-    ...
-    INDEX a (u64 * i32, s) TYPE minmax GRANULARITY 3,
-    INDEX b (u64 * length(s)) TYPE set(1000) GRANULARITY 4
-) ENGINE = MergeTree()
-...
-```
-
-ClickHouse puede utilizar los índices del ejemplo para reducir la cantidad de datos que se leen desde el disco en las siguientes consultas:
-
-``` sql
-SELECT count() FROM table WHERE s < 'z'
-SELECT count() FROM table WHERE u64 * i32 == 10 AND u64 * length(s) >= 1234
-```
-
-#### Tipos de índices disponibles {#available-types-of-indices}
-
--   `minmax`
-
-    Almacena los extremos de la expresión especificada (si la expresión `tuple`, entonces almacena extremos para cada elemento de `tuple`), utiliza información almacenada para omitir bloques de datos como la clave principal.
-
--   `set(max_rows)`
-
-    Almacena valores únicos de la expresión especificada (no más de `max_rows` filas, `max_rows=0` medio “no limits”). Utiliza los valores para comprobar si `WHERE` expresión no es satisfactorio en un bloque de datos.
-
--   `ngrambf_v1(n, size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)`
-
-    Tiendas a [Filtro de floración](https://en.wikipedia.org/wiki/Bloom_filter) que contiene todos los ngrams de un bloque de datos. Funciona solo con cadenas. Puede ser utilizado para la optimización de `equals`, `like` y `in` expresiones.
-
-    -   `n` — ngram size,
-    -   `size_of_bloom_filter_in_bytes` — Bloom filter size in bytes (you can use large values here, for example, 256 or 512, because it can be compressed well).
-    -   `number_of_hash_functions` — The number of hash functions used in the Bloom filter.
-    -   `random_seed` — The seed for Bloom filter hash functions.
-
--   `tokenbf_v1(size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)`
-
-    Lo mismo que `ngrambf_v1`, pero almacena tokens en lugar de ngrams. Los tokens son secuencias separadas por caracteres no alfanuméricos.
-
--   `bloom_filter([false_positive])` — Stores a [Filtro de floración](https://en.wikipedia.org/wiki/Bloom_filter) para las columnas especificadas.
-
-    Opcional `false_positive` parámetro es la probabilidad de recibir una respuesta falsa positiva del filtro. Valores posibles: (0, 1). Valor predeterminado: 0.025.
-
-    Tipos de datos admitidos: `Int*`, `UInt*`, `Float*`, `Enum`, `Date`, `DateTime`, `String`, `FixedString`, `Array`, `LowCardinality`, `Nullable`.
-
-    Las siguientes funciones pueden usarlo: [igual](../../../sql-reference/functions/comparison-functions.md), [notEquals](../../../sql-reference/functions/comparison-functions.md), [en](../../../sql-reference/functions/in-functions.md), [noEn](../../../sql-reference/functions/in-functions.md), [tener](../../../sql-reference/functions/array-functions.md).
-
-<!-- -->
-
-``` sql
-INDEX sample_index (u64 * length(s)) TYPE minmax GRANULARITY 4
-INDEX sample_index2 (u64 * length(str), i32 + f64 * 100, date, str) TYPE set(100) GRANULARITY 4
-INDEX sample_index3 (lower(str), str) TYPE ngrambf_v1(3, 256, 2, 0) GRANULARITY 4
-```
-
-#### Funciones de apoyo {#functions-support}
-
-Condiciones en el `WHERE` cláusula contiene llamadas de las funciones que operan con columnas. Si la columna forma parte de un índice, ClickHouse intenta usar este índice al realizar las funciones. ClickHouse admite diferentes subconjuntos de funciones para usar índices.
-
-El `set` index se puede utilizar con todas las funciones. Subconjuntos de funciones para otros índices se muestran en la siguiente tabla.
-
-| Función (operador) / Índice                                                                              | clave primaria | minmax | Descripción | Sistema abierto. | bloom_filter |
-|----------------------------------------------------------------------------------------------------------|----------------|--------|-------------|------------------|---------------|
-| [igual (=, ==)](../../../sql-reference/functions/comparison-functions.md#function-equals)                | ✔              | ✔      | ✔           | ✔                | ✔             |
-| [notEquals(!=, \<\>)](../../../sql-reference/functions/comparison-functions.md#function-notequals)       | ✔              | ✔      | ✔           | ✔                | ✔             |
-| [como](../../../sql-reference/functions/string-search-functions.md#function-like)                        | ✔              | ✔      | ✔           | ✗                | ✗             |
-| [No como](../../../sql-reference/functions/string-search-functions.md#function-notlike)                  | ✔              | ✔      | ✔           | ✗                | ✗             |
-| [Comienza con](../../../sql-reference/functions/string-functions.md#startswith)                          | ✔              | ✔      | ✔           | ✔                | ✗             |
-| [Finaliza con](../../../sql-reference/functions/string-functions.md#endswith)                            | ✗              | ✗      | ✔           | ✔                | ✗             |
-| [multiSearchAny](../../../sql-reference/functions/string-search-functions.md#function-multisearchany)    | ✗              | ✗      | ✔           | ✗                | ✗             |
-| [en](../../../sql-reference/functions/in-functions.md#in-functions)                                      | ✔              | ✔      | ✔           | ✔                | ✔             |
-| [noEn](../../../sql-reference/functions/in-functions.md#in-functions)                                    | ✔              | ✔      | ✔           | ✔                | ✔             |
-| [menos (\<)](../../../sql-reference/functions/comparison-functions.md#function-less)                     | ✔              | ✔      | ✗           | ✗                | ✗             |
-| [mayor (\>)](../../../sql-reference/functions/comparison-functions.md#function-greater)                  | ✔              | ✔      | ✗           | ✗                | ✗             |
-| [menosOrEquals (\<=)](../../../sql-reference/functions/comparison-functions.md#function-lessorequals)    | ✔              | ✔      | ✗           | ✗                | ✗             |
-| [mayorOrEquals (\>=)](../../../sql-reference/functions/comparison-functions.md#function-greaterorequals) | ✔              | ✔      | ✗           | ✗                | ✗             |
-| [vaciar](../../../sql-reference/functions/array-functions.md#function-empty)                             | ✔              | ✔      | ✗           | ✗                | ✗             |
-| [notEmpty](../../../sql-reference/functions/array-functions.md#function-notempty)                        | ✔              | ✔      | ✗           | ✗                | ✗             |
-| hasToken                                                                                                 | ✗              | ✗      | ✗           | ✔                | ✗             |
-
-Las funciones con un argumento constante que es menor que el tamaño de ngram no pueden ser utilizadas por `ngrambf_v1` para la optimización de consultas.
-
-Los filtros Bloom pueden tener coincidencias falsas positivas, por lo que `ngrambf_v1`, `tokenbf_v1`, y `bloom_filter` los índices no se pueden usar para optimizar consultas donde se espera que el resultado de una función sea falso, por ejemplo:
-
--   Puede ser optimizado:
-    -   `s LIKE '%test%'`
-    -   `NOT s NOT LIKE '%test%'`
-    -   `s = 1`
-    -   `NOT s != 1`
-    -   `startsWith(s, 'test')`
--   No se puede optimizar:
-    -   `NOT s LIKE '%test%'`
-    -   `s NOT LIKE '%test%'`
-    -   `NOT s = 1`
-    -   `s != 1`
-    -   `NOT startsWith(s, 'test')`
-
-## Acceso a datos simultáneos {#concurrent-data-access}
-
-Para el acceso simultáneo a tablas, usamos versiones múltiples. En otras palabras, cuando una tabla se lee y actualiza simultáneamente, los datos se leen de un conjunto de partes que está actualizado en el momento de la consulta. No hay cerraduras largas. Las inserciones no se interponen en el camino de las operaciones de lectura.
-
-La lectura de una tabla se paralela automáticamente.
-
-## TTL para columnas y tablas {#table_engine-mergetree-ttl}
-
-Determina la duración de los valores.
-
-El `TTL` se puede establecer para toda la tabla y para cada columna individual. TTL de nivel de tabla también puede especificar la lógica de movimiento automático de datos entre discos y volúmenes.
-
-Las expresiones deben evaluar [Fecha](../../../sql-reference/data-types/date.md) o [FechaHora](../../../sql-reference/data-types/datetime.md) tipo de datos.
-
-Ejemplo:
-
-``` sql
-TTL time_column
-TTL time_column + interval
-```
-
-Definir `interval`, utilizar [intervalo de tiempo](../../../sql-reference/operators/index.md#operators-datetime) operador.
-
-``` sql
-TTL date_time + INTERVAL 1 MONTH
-TTL date_time + INTERVAL 15 HOUR
-```
-
-### Columna TTL {#mergetree-column-ttl}
-
-Cuando los valores de la columna caducan, ClickHouse los reemplaza con los valores predeterminados para el tipo de datos de columna. Si todos los valores de columna en la parte de datos caducan, ClickHouse elimina esta columna de la parte de datos en un sistema de archivos.
-
-El `TTL` cláusula no se puede utilizar para columnas clave.
-
-Ejemplos:
-
-Creación de una tabla con TTL
-
-``` sql
-CREATE TABLE example_table
-(
-    d DateTime,
-    a Int TTL d + INTERVAL 1 MONTH,
-    b Int TTL d + INTERVAL 1 MONTH,
-    c String
-)
-ENGINE = MergeTree
-PARTITION BY toYYYYMM(d)
-ORDER BY d;
-```
-
-Adición de TTL a una columna de una tabla existente
-
-``` sql
-ALTER TABLE example_table
-    MODIFY COLUMN
-    c String TTL d + INTERVAL 1 DAY;
-```
-
-Modificación de TTL de la columna
-
-``` sql
-ALTER TABLE example_table
-    MODIFY COLUMN
-    c String TTL d + INTERVAL 1 MONTH;
-```
-
-### Tabla TTL {#mergetree-table-ttl}
-
-La tabla puede tener una expresión para la eliminación de filas caducadas y varias expresiones para el movimiento automático de partes entre [discos o volúmenes](#table_engine-mergetree-multiple-volumes). Cuando las filas de la tabla caducan, ClickHouse elimina todas las filas correspondientes. Para la entidad de movimiento de piezas, todas las filas de una pieza deben cumplir los criterios de expresión de movimiento.
-
-``` sql
-TTL expr [DELETE|TO DISK 'aaa'|TO VOLUME 'bbb'], ...
-```
-
-El tipo de regla TTL puede seguir cada expresión TTL. Afecta a una acción que debe realizarse una vez que se satisface la expresión (alcanza la hora actual):
-
--   `DELETE` - Eliminar filas caducadas (acción predeterminada);
--   `TO DISK 'aaa'` - mover parte al disco `aaa`;
--   `TO VOLUME 'bbb'` - mover parte al disco `bbb`.
-
-Ejemplos:
-
-Creación de una tabla con TTL
-
-``` sql
-CREATE TABLE example_table
-(
-    d DateTime,
-    a Int
-)
-ENGINE = MergeTree
-PARTITION BY toYYYYMM(d)
-ORDER BY d
-TTL d + INTERVAL 1 MONTH [DELETE],
-    d + INTERVAL 1 WEEK TO VOLUME 'aaa',
-    d + INTERVAL 2 WEEK TO DISK 'bbb';
-```
-
-Modificación de TTL de la tabla
-
-``` sql
-ALTER TABLE example_table
-    MODIFY TTL d + INTERVAL 1 DAY;
-```
-
-**Eliminación de datos**
-
-Los datos con un TTL caducado se eliminan cuando ClickHouse fusiona partes de datos.
-
-Cuando ClickHouse ve que los datos han caducado, realiza una combinación fuera de programación. Para controlar la frecuencia de tales fusiones, puede establecer `merge_with_ttl_timeout`. Si el valor es demasiado bajo, realizará muchas fusiones fuera de horario que pueden consumir muchos recursos.
-
-Si realiza el `SELECT` consulta entre fusiones, puede obtener datos caducados. Para evitarlo, use el [OPTIMIZE](../../../sql-reference/statements/misc.md#misc_operations-optimize) consulta antes `SELECT`.
-
-## Uso de varios dispositivos de bloque para el almacenamiento de datos {#table_engine-mergetree-multiple-volumes}
-
-### Implantación {#introduction}
-
-`MergeTree` Los motores de tablas familiares pueden almacenar datos en múltiples dispositivos de bloque. Por ejemplo, puede ser útil cuando los datos de una determinada tabla se dividen implícitamente en “hot” y “cold”. Los datos más recientes se solicitan regularmente, pero solo requieren una pequeña cantidad de espacio. Por el contrario, los datos históricos de cola gorda se solicitan raramente. Si hay varios discos disponibles, el “hot” los datos pueden estar ubicados en discos rápidos (por ejemplo, SSD NVMe o en memoria), mientras que “cold” datos - en los relativamente lentos (por ejemplo, HDD).
-
-La parte de datos es la unidad móvil mínima para `MergeTree`-mesas de motor. Los datos que pertenecen a una parte se almacenan en un disco. Las partes de datos se pueden mover entre discos en segundo plano (según la configuración del usuario) así como por medio de la [ALTER](../../../sql-reference/statements/alter.md#alter_move-partition) consulta.
-
-### Plazo {#terms}
-
--   Disk — Block device mounted to the filesystem.
--   Default disk — Disk that stores the path specified in the [camino](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-path) configuración del servidor.
--   Volume — Ordered set of equal disks (similar to [JBOD](https://en.wikipedia.org/wiki/Non-RAID_drive_architectures)).
--   Storage policy — Set of volumes and the rules for moving data between them.
-
-Los nombres dados a las entidades descritas se pueden encontrar en las tablas del sistema, [sistema.almacenamiento_policies](../../../operations/system-tables.md#system_tables-storage_policies) y [sistema.disco](../../../operations/system-tables.md#system_tables-disks). Para aplicar una de las directivas de almacenamiento configuradas para una tabla, `storage_policy` establecimiento de `MergeTree`-mesas de la familia del motor.
-
-### Configuración {#table_engine-mergetree-multiple-volumes_configure}
-
-Los discos, los volúmenes y las políticas de almacenamiento deben declararse `<storage_configuration>` etiqueta ya sea en el archivo principal `config.xml` o en un archivo distinto en el `config.d` directorio.
-
-Estructura de configuración:
-
-``` xml
-<storage_configuration>
-    <disks>
-        <disk_name_1> <!-- disk name -->
-            <path>/mnt/fast_ssd/clickhouse/</path>
-        </disk_name_1>
-        <disk_name_2>
-            <path>/mnt/hdd1/clickhouse/</path>
-            <keep_free_space_bytes>10485760</keep_free_space_bytes>
-        </disk_name_2>
-        <disk_name_3>
-            <path>/mnt/hdd2/clickhouse/</path>
-            <keep_free_space_bytes>10485760</keep_free_space_bytes>
-        </disk_name_3>
-
-        ...
-    </disks>
-
-    ...
-</storage_configuration>
-```
-
-Tags:
-
--   `<disk_name_N>` — Disk name. Names must be different for all disks.
--   `path` — path under which a server will store data (`data` y `shadow` carpetas), debe terminarse con ‘/’.
--   `keep_free_space_bytes` — the amount of free disk space to be reserved.
-
-El orden de la definición del disco no es importante.
-
-Marcado de configuración de directivas de almacenamiento:
-
-``` xml
-<storage_configuration>
-    ...
-    <policies>
-        <policy_name_1>
-            <volumes>
-                <volume_name_1>
-                    <disk>disk_name_from_disks_configuration</disk>
-                    <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>
-                </volume_name_1>
-                <volume_name_2>
-                    <!-- configuration -->
-                </volume_name_2>
-                <!-- more volumes -->
-            </volumes>
-            <move_factor>0.2</move_factor>
-        </policy_name_1>
-        <policy_name_2>
-            <!-- configuration -->
-        </policy_name_2>
-
-        <!-- more policies -->
-    </policies>
-    ...
-</storage_configuration>
-```
-
-Tags:
-
--   `policy_name_N` — Policy name. Policy names must be unique.
--   `volume_name_N` — Volume name. Volume names must be unique.
--   `disk` — a disk within a volume.
--   `max_data_part_size_bytes` — the maximum size of a part that can be stored on any of the volume's disks.
--   `move_factor` — when the amount of available space gets lower than this factor, data automatically start to move on the next volume if any (by default, 0.1).
-
-Cofiguration ejemplos:
-
-``` xml
-<storage_configuration>
-    ...
-    <policies>
-        <hdd_in_order> <!-- policy name -->
-            <volumes>
-                <single> <!-- volume name -->
-                    <disk>disk1</disk>
-                    <disk>disk2</disk>
-                </single>
-            </volumes>
-        </hdd_in_order>
-
-        <moving_from_ssd_to_hdd>
-            <volumes>
-                <hot>
-                    <disk>fast_ssd</disk>
-                    <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>
-                </hot>
-                <cold>
-                    <disk>disk1</disk>
-                </cold>
-            </volumes>
-            <move_factor>0.2</move_factor>
-        </moving_from_ssd_to_hdd>
-    </policies>
-    ...
-</storage_configuration>
-```
-
-En un ejemplo dado, el `hdd_in_order` la política implementa el [Ronda-robin](https://en.wikipedia.org/wiki/Round-robin_scheduling) enfoque. Por lo tanto, esta política define solo un volumen (`single`), las partes de datos se almacenan en todos sus discos en orden circular. Dicha política puede ser bastante útil si hay varios discos similares montados en el sistema, pero RAID no está configurado. Tenga en cuenta que cada unidad de disco individual no es confiable y es posible que desee compensarlo con un factor de replicación de 3 o más.
-
-Si hay diferentes tipos de discos disponibles en el sistema, `moving_from_ssd_to_hdd` política se puede utilizar en su lugar. Volumen `hot` consta de un disco SSD (`fast_ssd`), y el tamaño máximo de una pieza que se puede almacenar en este volumen es de 1 GB. Todas las piezas con el tamaño más grande que 1GB serán almacenadas directamente en `cold` volumen, que contiene un disco duro `disk1`.
-Además, una vez que el disco `fast_ssd` se llena en más del 80%, los datos se transferirán al `disk1` por un proceso en segundo plano.
-
-El orden de enumeración de volúmenes dentro de una directiva de almacenamiento es importante. Una vez que un volumen está sobrellenado, los datos se mueven al siguiente. El orden de la enumeración del disco también es importante porque los datos se almacenan en ellos por turnos.
-
-Al crear una tabla, se puede aplicarle una de las directivas de almacenamiento configuradas:
-
-``` sql
-CREATE TABLE table_with_non_default_policy (
-    EventDate Date,
-    OrderID UInt64,
-    BannerID UInt64,
-    SearchPhrase String
-) ENGINE = MergeTree
-ORDER BY (OrderID, BannerID)
-PARTITION BY toYYYYMM(EventDate)
-SETTINGS storage_policy = 'moving_from_ssd_to_hdd'
-```
-
-El `default` política de almacenamiento implica el uso de un solo volumen, que consiste en un solo disco dado en `<path>`. Una vez que se crea una tabla, no se puede cambiar su política de almacenamiento.
-
-### Detalles {#details}
-
-En el caso de `MergeTree` tablas, los datos están llegando al disco de diferentes maneras:
-
--   Como resultado de un inserto (`INSERT` consulta).
--   Durante las fusiones de fondo y [mutación](../../../sql-reference/statements/alter.md#alter-mutations).
--   Al descargar desde otra réplica.
--   Como resultado de la congelación de particiones [ALTER TABLE … FREEZE PARTITION](../../../sql-reference/statements/alter.md#alter_freeze-partition).
-
-En todos estos casos, excepto las mutaciones y la congelación de particiones, una pieza se almacena en un volumen y un disco de acuerdo con la política de almacenamiento dada:
-
-1.  El primer volumen (en el orden de definición) que tiene suficiente espacio en disco para almacenar una pieza (`unreserved_space > current_part_size`) y permite almacenar partes de un tamaño determinado (`max_data_part_size_bytes > current_part_size`) se elige.
-2.  Dentro de este volumen, se elige ese disco que sigue al que se utilizó para almacenar el fragmento de datos anterior y que tiene espacio libre más que el tamaño de la pieza (`unreserved_space - keep_free_space_bytes > current_part_size`).
-
-Bajo el capó, las mutaciones y la congelación de particiones hacen uso de [enlaces duros](https://en.wikipedia.org/wiki/Hard_link). Los enlaces duros entre diferentes discos no son compatibles, por lo tanto, en tales casos las partes resultantes se almacenan en los mismos discos que los iniciales.
-
-En el fondo, las partes se mueven entre volúmenes en función de la cantidad de espacio libre (`move_factor` parámetro) según el orden en que se declaran los volúmenes en el archivo de configuración.
-Los datos nunca se transfieren desde el último y al primero. Uno puede usar tablas del sistema [sistema.part_log](../../../operations/system-tables.md#system_tables-part-log) (campo `type = MOVE_PART`) y [sistema.parte](../../../operations/system-tables.md#system_tables-parts) (campo `path` y `disk`) para monitorear movimientos de fondo. Además, la información detallada se puede encontrar en los registros del servidor.
-
-El usuario puede forzar el movimiento de una pieza o una partición de un volumen a otro mediante la consulta [ALTER TABLE … MOVE PART\|PARTITION … TO VOLUME\|DISK …](../../../sql-reference/statements/alter.md#alter_move-partition), todas las restricciones para las operaciones en segundo plano se tienen en cuenta. La consulta inicia un movimiento por sí misma y no espera a que se completen las operaciones en segundo plano. El usuario recibirá un mensaje de error si no hay suficiente espacio libre disponible o si no se cumple alguna de las condiciones requeridas.
-
-Mover datos no interfiere con la replicación de datos. Por lo tanto, se pueden especificar diferentes directivas de almacenamiento para la misma tabla en diferentes réplicas.
-
-Después de la finalización de las fusiones y mutaciones de fondo, las partes viejas se eliminan solo después de un cierto período de tiempo (`old_parts_lifetime`).
-Durante este tiempo, no se mueven a otros volúmenes o discos. Por lo tanto, hasta que las partes finalmente se eliminen, aún se tienen en cuenta para la evaluación del espacio en disco ocupado.
-
-[Artículo Original](https://clickhouse.tech/docs/ru/operations/table_engines/mergetree/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/replacingmergetree.md b/docs/es/engines/table-engines/mergetree-family/replacingmergetree.md
deleted file mode 100644
index a1e95c5b5f46..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/replacingmergetree.md
+++ /dev/null
@@ -1,69 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: ReplacingMergeTree
----
-
-# ReplacingMergeTree {#replacingmergetree}
-
-El motor difiere de [Método de codificación de datos:](mergetree.md#table_engines-mergetree) en que elimina las entradas duplicadas con el mismo valor de clave principal (o más exactamente, con el mismo [clave de clasificación](mergetree.md) valor).
-
-La desduplicación de datos solo se produce durante una fusión. La fusión ocurre en segundo plano en un momento desconocido, por lo que no puede planificarla. Algunos de los datos pueden permanecer sin procesar. Aunque puede ejecutar una fusión no programada utilizando el `OPTIMIZE` consulta, no cuente con usarlo, porque el `OPTIMIZE` consulta leerá y escribirá una gran cantidad de datos.
-
-Así, `ReplacingMergeTree` es adecuado para borrar datos duplicados en segundo plano para ahorrar espacio, pero no garantiza la ausencia de duplicados.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = ReplacingMergeTree([ver])
-[PARTITION BY expr]
-[ORDER BY expr]
-[PRIMARY KEY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Para obtener una descripción de los parámetros de solicitud, consulte [descripción de la solicitud](../../../sql-reference/statements/create.md).
-
-**ReplacingMergeTree Parámetros**
-
--   `ver` — column with version. Type `UInt*`, `Date` o `DateTime`. Parámetro opcional.
-
-    Al fusionar, `ReplacingMergeTree` de todas las filas con la misma clave primaria deja solo una:
-
-    -   Último en la selección, si `ver` no establecido.
-    -   Con la versión máxima, si `ver` indicado.
-
-**Cláusulas de consulta**
-
-Al crear un `ReplacingMergeTree` mesa de la misma [clausula](mergetree.md) se requieren, como al crear un `MergeTree` tabla.
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No use este método en proyectos nuevos y, si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] ReplacingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [ver])
-```
-
-Todos los parámetros excepto `ver` el mismo significado que en `MergeTree`.
-
--   `ver` - columna con la versión. Parámetro opcional. Para una descripción, vea el texto anterior.
-
-</details>
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/replacingmergetree/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/replication.md b/docs/es/engines/table-engines/mergetree-family/replication.md
deleted file mode 100644
index 505f5223800e..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/replication.md
+++ /dev/null
@@ -1,218 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 31
-toc_title: "Replicaci\xF3n de datos"
----
-
-# Replicación de datos {#table_engines-replication}
-
-La replicación solo se admite para tablas de la familia MergeTree:
-
--   ReplicatedMergeTree
--   ReplicatedSummingMergeTree
--   ReplicatedReplacingMergeTree
--   ReplicatedAggregatingMergeTree
--   ReplicatedCollapsingMergeTree
--   ReplicatedVersionedCollapsingMergetree
--   ReplicatedGraphiteMergeTree
-
-La replicación funciona a nivel de una tabla individual, no de todo el servidor. Un servidor puede almacenar tablas replicadas y no replicadas al mismo tiempo.
-
-La replicación no depende de la fragmentación. Cada fragmento tiene su propia replicación independiente.
-
-Datos comprimidos para `INSERT` y `ALTER` se replica (para obtener más información, consulte la documentación para [ALTER](../../../sql-reference/statements/alter.md#query_language_queries_alter)).
-
-`CREATE`, `DROP`, `ATTACH`, `DETACH` y `RENAME` las consultas se ejecutan en un único servidor y no se replican:
-
--   El `CREATE TABLE` query crea una nueva tabla replicable en el servidor donde se ejecuta la consulta. Si esta tabla ya existe en otros servidores, agrega una nueva réplica.
--   El `DROP TABLE` query elimina la réplica ubicada en el servidor donde se ejecuta la consulta.
--   El `RENAME` query cambia el nombre de la tabla en una de las réplicas. En otras palabras, las tablas replicadas pueden tener diferentes nombres en diferentes réplicas.
-
-Uso de ClickHouse [Apache ZooKeeper](https://zookeeper.apache.org) para almacenar metainformación de réplicas. Utilice ZooKeeper versión 3.4.5 o posterior.
-
-Para utilizar la replicación, establezca los parámetros [Zookeeper](../../../operations/server-configuration-parameters/settings.md#server-settings_zookeeper) sección de configuración del servidor.
-
-!!! attention "Atención"
-    No descuides la configuración de seguridad. ClickHouse soporta el `digest` [Esquema de ACL](https://zookeeper.apache.org/doc/current/zookeeperProgrammers.html#sc_ZooKeeperAccessControl) del subsistema de seguridad ZooKeeper.
-
-Ejemplo de configuración de las direcciones del clúster ZooKeeper:
-
-``` xml
-<zookeeper>
-    <node index="1">
-        <host>example1</host>
-        <port>2181</port>
-    </node>
-    <node index="2">
-        <host>example2</host>
-        <port>2181</port>
-    </node>
-    <node index="3">
-        <host>example3</host>
-        <port>2181</port>
-    </node>
-</zookeeper>
-```
-
-Puede especificar cualquier clúster ZooKeeper existente y el sistema utilizará un directorio en él para sus propios datos (el directorio se especifica al crear una tabla replicable).
-
-Si ZooKeeper no está establecido en el archivo de configuración, no puede crear tablas replicadas y las tablas replicadas existentes serán de solo lectura.
-
-ZooKeeper no se utiliza en `SELECT` consultas porque la replicación no afecta al rendimiento de `SELECT` y las consultas se ejecutan tan rápido como lo hacen para las tablas no replicadas. Al consultar tablas replicadas distribuidas, el comportamiento de ClickHouse se controla mediante la configuración [max_replica_delay_for_distributed_queries](../../../operations/settings/settings.md#settings-max_replica_delay_for_distributed_queries) y [fallback_to_stale_replicas_for_distributed_queries](../../../operations/settings/settings.md#settings-fallback_to_stale_replicas_for_distributed_queries).
-
-Para cada `INSERT` consulta, aproximadamente diez entradas se agregan a ZooKeeper a través de varias transacciones. (Para ser más precisos, esto es para cada bloque de datos insertado; una consulta INSERT contiene un bloque o un bloque por `max_insert_block_size = 1048576` filas.) Esto conduce a latencias ligeramente más largas para `INSERT` en comparación con las tablas no replicadas. Pero si sigue las recomendaciones para insertar datos en lotes de no más de uno `INSERT` por segundo, no crea ningún problema. Todo el clúster ClickHouse utilizado para coordinar un clúster ZooKeeper tiene un total de varios cientos `INSERTs` por segundo. El rendimiento en las inserciones de datos (el número de filas por segundo) es tan alto como para los datos no replicados.
-
-Para clústeres muy grandes, puede usar diferentes clústeres de ZooKeeper para diferentes fragmentos. Sin embargo, esto no ha demostrado ser necesario en el Yandex.Clúster Metrica (aproximadamente 300 servidores).
-
-La replicación es asíncrona y multi-master. `INSERT` consultas (así como `ALTER`) se puede enviar a cualquier servidor disponible. Los datos se insertan en el servidor donde se ejecuta la consulta y, a continuación, se copian a los demás servidores. Debido a que es asincrónico, los datos insertados recientemente aparecen en las otras réplicas con cierta latencia. Si parte de las réplicas no está disponible, los datos se escriben cuando estén disponibles. Si hay una réplica disponible, la latencia es la cantidad de tiempo que tarda en transferir el bloque de datos comprimidos a través de la red.
-
-De forma predeterminada, una consulta INSERT espera la confirmación de la escritura de los datos de una sola réplica. Si los datos fue correctamente escrito a sólo una réplica y el servidor con esta réplica deja de existir, los datos almacenados se perderán. Para habilitar la confirmación de las escrituras de datos de varias réplicas, utilice `insert_quorum` opcion.
-
-Cada bloque de datos se escribe atómicamente. La consulta INSERT se divide en bloques hasta `max_insert_block_size = 1048576` filas. En otras palabras, si el `INSERT` consulta tiene menos de 1048576 filas, se hace atómicamente.
-
-Los bloques de datos se deduplican. Para varias escrituras del mismo bloque de datos (bloques de datos del mismo tamaño que contienen las mismas filas en el mismo orden), el bloque solo se escribe una vez. La razón de esto es en caso de fallas de red cuando la aplicación cliente no sabe si los datos se escribieron en la base de datos, por lo que `INSERT` consulta simplemente se puede repetir. No importa a qué réplica se enviaron los INSERT con datos idénticos. `INSERTs` son idempotentes. Los parámetros de desduplicación son controlados por [merge_tree](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-merge_tree) configuración del servidor.
-
-Durante la replicación, sólo los datos de origen que se van a insertar se transfieren a través de la red. La transformación de datos adicional (fusión) se coordina y se realiza en todas las réplicas de la misma manera. Esto minimiza el uso de la red, lo que significa que la replicación funciona bien cuando las réplicas residen en centros de datos diferentes. (Tenga en cuenta que la duplicación de datos en diferentes centros de datos es el objetivo principal de la replicación.)
-
-Puede tener cualquier número de réplicas de los mismos datos. El Yandex.Metrica utiliza doble replicación en producción. Cada servidor utiliza RAID-5 o RAID-6, y RAID-10 en algunos casos. Esta es una solución relativamente confiable y conveniente.
-
-El sistema supervisa la sincronicidad de los datos en las réplicas y puede recuperarse después de un fallo. La conmutación por error es automática (para pequeñas diferencias en los datos) o semiautomática (cuando los datos difieren demasiado, lo que puede indicar un error de configuración).
-
-## Creación de tablas replicadas {#creating-replicated-tables}
-
-El `Replicated` prefijo se agrega al nombre del motor de tabla. Por ejemplo:`ReplicatedMergeTree`.
-
-**Replicated\*MergeTree parámetros**
-
--   `zoo_path` — The path to the table in ZooKeeper.
--   `replica_name` — The replica name in ZooKeeper.
-
-Ejemplo:
-
-``` sql
-CREATE TABLE table_name
-(
-    EventDate DateTime,
-    CounterID UInt32,
-    UserID UInt32
-) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}')
-PARTITION BY toYYYYMM(EventDate)
-ORDER BY (CounterID, EventDate, intHash32(UserID))
-SAMPLE BY intHash32(UserID)
-```
-
-<details markdown="1">
-
-<summary>Ejemplo en sintaxis obsoleta</summary>
-
-``` sql
-CREATE TABLE table_name
-(
-    EventDate DateTime,
-    CounterID UInt32,
-    UserID UInt32
-) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}', EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192)
-```
-
-</details>
-
-Como muestra el ejemplo, estos parámetros pueden contener sustituciones entre llaves. Los valores sustituidos se toman de la ‘macros’ sección del archivo de configuración. Ejemplo:
-
-``` xml
-<macros>
-    <layer>05</layer>
-    <shard>02</shard>
-    <replica>example05-02-1.yandex.ru</replica>
-</macros>
-```
-
-La ruta de acceso a la tabla en ZooKeeper debe ser única para cada tabla replicada. Las tablas en diferentes fragmentos deben tener rutas diferentes.
-En este caso, la ruta consta de las siguientes partes:
-
-`/clickhouse/tables/` es el prefijo común. Recomendamos usar exactamente este.
-
-`{layer}-{shard}` es el identificador de fragmento. En este ejemplo consta de dos partes, ya que el Yandex.Metrica clúster utiliza sharding de dos niveles. Para la mayoría de las tareas, puede dejar solo la sustitución {shard}, que se expandirá al identificador de fragmento.
-
-`table_name` es el nombre del nodo de la tabla en ZooKeeper. Es una buena idea hacerlo igual que el nombre de la tabla. Se define explícitamente, porque a diferencia del nombre de la tabla, no cambia después de una consulta RENAME.
-*HINT*: podría agregar un nombre de base de datos delante de `table_name` También. Nivel de Cifrado WEP `db_name.table_name`
-
-El nombre de réplica identifica diferentes réplicas de la misma tabla. Puede usar el nombre del servidor para esto, como en el ejemplo. El nombre solo tiene que ser único dentro de cada fragmento.
-
-Puede definir los parámetros explícitamente en lugar de utilizar sustituciones. Esto podría ser conveniente para probar y para configurar clústeres pequeños. Sin embargo, no puede usar consultas DDL distribuidas (`ON CLUSTER` en este caso.
-
-Cuando se trabaja con clústeres grandes, se recomienda utilizar sustituciones porque reducen la probabilidad de error.
-
-Ejecute el `CREATE TABLE` consulta en cada réplica. Esta consulta crea una nueva tabla replicada o agrega una nueva réplica a una existente.
-
-Si agrega una nueva réplica después de que la tabla ya contenga algunos datos en otras réplicas, los datos se copiarán de las otras réplicas a la nueva después de ejecutar la consulta. En otras palabras, la nueva réplica se sincroniza con las demás.
-
-Para eliminar una réplica, ejecute `DROP TABLE`. However, only one replica is deleted – the one that resides on the server where you run the query.
-
-## Recuperación después de fallos {#recovery-after-failures}
-
-Si ZooKeeper no está disponible cuando se inicia un servidor, las tablas replicadas cambian al modo de solo lectura. El sistema intenta conectarse periódicamente a ZooKeeper.
-
-Si ZooKeeper no está disponible durante un `INSERT`, o se produce un error al interactuar con ZooKeeper, se produce una excepción.
-
-Después de conectarse a ZooKeeper, el sistema comprueba si el conjunto de datos en el sistema de archivos local coincide con el conjunto de datos esperado (ZooKeeper almacena esta información). Si hay incoherencias menores, el sistema las resuelve sincronizando datos con las réplicas.
-
-Si el sistema detecta partes de datos rotas (con un tamaño incorrecto de archivos) o partes no reconocidas (partes escritas en el sistema de archivos pero no grabadas en ZooKeeper), las mueve al `detached` subdirectorio (no se eliminan). Las piezas que faltan se copian de las réplicas.
-
-Tenga en cuenta que ClickHouse no realiza ninguna acción destructiva, como eliminar automáticamente una gran cantidad de datos.
-
-Cuando el servidor se inicia (o establece una nueva sesión con ZooKeeper), solo verifica la cantidad y el tamaño de todos los archivos. Si los tamaños de los archivos coinciden pero los bytes se han cambiado en algún punto intermedio, esto no se detecta inmediatamente, sino solo cuando se intenta leer los datos `SELECT` consulta. La consulta produce una excepción sobre una suma de comprobación no coincidente o el tamaño de un bloque comprimido. En este caso, las partes de datos se agregan a la cola de verificación y se copian de las réplicas si es necesario.
-
-Si el conjunto local de datos difiere demasiado del esperado, se activa un mecanismo de seguridad. El servidor ingresa esto en el registro y se niega a iniciarse. La razón de esto es que este caso puede indicar un error de configuración, como si una réplica en un fragmento se configurara accidentalmente como una réplica en un fragmento diferente. Sin embargo, los umbrales para este mecanismo se establecen bastante bajos, y esta situación puede ocurrir durante la recuperación de falla normal. En este caso, los datos se restauran semiautomáticamente, mediante “pushing a button”.
-
-Para iniciar la recuperación, cree el nodo `/path_to_table/replica_name/flags/force_restore_data` en ZooKeeper con cualquier contenido, o ejecute el comando para restaurar todas las tablas replicadas:
-
-``` bash
-sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data
-```
-
-A continuación, reinicie el servidor. Al iniciar, el servidor elimina estos indicadores e inicia la recuperación.
-
-## Recuperación después de la pérdida completa de datos {#recovery-after-complete-data-loss}
-
-Si todos los datos y metadatos desaparecieron de uno de los servidores, siga estos pasos para la recuperación:
-
-1.  Instale ClickHouse en el servidor. Defina correctamente las sustituciones en el archivo de configuración que contiene el identificador de fragmento y las réplicas, si las usa.
-2.  Si tenía tablas no duplicadas que deben duplicarse manualmente en los servidores, copie sus datos desde una réplica (en el directorio `/var/lib/clickhouse/data/db_name/table_name/`).
-3.  Copiar definiciones de tablas ubicadas en `/var/lib/clickhouse/metadata/` de una réplica. Si un identificador de fragmento o réplica se define explícitamente en las definiciones de tabla, corríjalo para que corresponda a esta réplica. (Como alternativa, inicie el servidor y `ATTACH TABLE` consultas que deberían haber estado en el .sql archivos en `/var/lib/clickhouse/metadata/`.)
-4.  Para iniciar la recuperación, cree el nodo ZooKeeper `/path_to_table/replica_name/flags/force_restore_data` con cualquier contenido o ejecute el comando para restaurar todas las tablas replicadas: `sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data`
-
-Luego inicie el servidor (reinicie, si ya se está ejecutando). Los datos se descargarán de las réplicas.
-
-Una opción de recuperación alternativa es eliminar información sobre la réplica perdida de ZooKeeper (`/path_to_table/replica_name`), luego vuelva a crear la réplica como se describe en “[Creación de tablas replicadas](#creating-replicated-tables)”.
-
-No hay restricción en el ancho de banda de la red durante la recuperación. Tenga esto en cuenta si está restaurando muchas réplicas a la vez.
-
-## La conversión de MergeTree a ReplicatedMergeTree {#converting-from-mergetree-to-replicatedmergetree}
-
-Usamos el término `MergeTree` para referirse a todos los motores de mesa en el `MergeTree family`, lo mismo que para `ReplicatedMergeTree`.
-
-Si usted tenía un `MergeTree` tabla replicada manualmente, puede convertirla en una tabla replicada. Es posible que tenga que hacer esto si ya ha recopilado una gran cantidad de datos `MergeTree` y ahora desea habilitar la replicación.
-
-Si los datos difieren en varias réplicas, primero sincronícelos o elimínelos en todas las réplicas, excepto en una.
-
-Cambie el nombre de la tabla MergeTree existente y, a continuación, cree un `ReplicatedMergeTree` mesa con el antiguo nombre.
-Mueva los datos de la tabla antigua a la `detached` subdirectorio dentro del directorio con los nuevos datos de la tabla (`/var/lib/clickhouse/data/db_name/table_name/`).
-Luego ejecuta `ALTER TABLE ATTACH PARTITION` en una de las réplicas para agregar estas partes de datos al conjunto de trabajo.
-
-## La conversión de ReplicatedMergeTree a MergeTree {#converting-from-replicatedmergetree-to-mergetree}
-
-Cree una tabla MergeTree con un nombre diferente. Mueva todos los datos del directorio con el `ReplicatedMergeTree` datos de la tabla al directorio de datos de la nueva tabla. A continuación, elimine el `ReplicatedMergeTree` y reinicie el servidor.
-
-Si desea deshacerse de un `ReplicatedMergeTree` sin iniciar el servidor:
-
--   Eliminar el correspondiente `.sql` archivo en el directorio de metadatos (`/var/lib/clickhouse/metadata/`).
--   Eliminar la ruta correspondiente en ZooKeeper (`/path_to_table/replica_name`).
-
-Después de esto, puede iniciar el servidor, crear un `MergeTree` tabla, mueva los datos a su directorio y, a continuación, reinicie el servidor.
-
-## Recuperación cuando se pierden o se dañan los metadatos del clúster Zookeeper {#recovery-when-metadata-in-the-zookeeper-cluster-is-lost-or-damaged}
-
-Si los datos de ZooKeeper se perdieron o se dañaron, puede guardar los datos moviéndolos a una tabla no duplicada como se describió anteriormente.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/replication/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/summingmergetree.md b/docs/es/engines/table-engines/mergetree-family/summingmergetree.md
deleted file mode 100644
index 3ae9a1515c07..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/summingmergetree.md
+++ /dev/null
@@ -1,141 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: SummingMergeTree
----
-
-# SummingMergeTree {#summingmergetree}
-
-El motor hereda de [Método de codificación de datos:](mergetree.md#table_engines-mergetree). La diferencia es que al fusionar partes de datos para `SummingMergeTree` ClickHouse reemplaza todas las filas con la misma clave primaria (o más exactamente, con la misma [clave de clasificación](mergetree.md)) con una fila que contiene valores resumidos para las columnas con el tipo de datos numérico. Si la clave de ordenación está compuesta de manera que un solo valor de clave corresponde a un gran número de filas, esto reduce significativamente el volumen de almacenamiento y acelera la selección de datos.
-
-Recomendamos usar el motor junto con `MergeTree`. Almacenar datos completos en `MergeTree` mesa, y el uso `SummingMergeTree` para el almacenamiento de datos agregados, por ejemplo, al preparar informes. Tal enfoque evitará que pierda datos valiosos debido a una clave primaria compuesta incorrectamente.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = SummingMergeTree([columns])
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Para obtener una descripción de los parámetros de solicitud, consulte [descripción de la solicitud](../../../sql-reference/statements/create.md).
-
-**Parámetros de SummingMergeTree**
-
--   `columns` - una tupla con los nombres de las columnas donde se resumirán los valores. Parámetro opcional.
-    Las columnas deben ser de tipo numérico y no deben estar en la clave principal.
-
-    Si `columns` no especificado, ClickHouse resume los valores de todas las columnas con un tipo de datos numérico que no están en la clave principal.
-
-**Cláusulas de consulta**
-
-Al crear un `SummingMergeTree` mesa de la misma [clausula](mergetree.md) se requieren, como al crear un `MergeTree` tabla.
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No use este método en proyectos nuevos y, si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] SummingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [columns])
-```
-
-Todos los parámetros excepto `columns` el mismo significado que en `MergeTree`.
-
--   `columns` — tuple with names of columns values of which will be summarized. Optional parameter. For a description, see the text above.
-
-</details>
-
-## Ejemplo de uso {#usage-example}
-
-Considere la siguiente tabla:
-
-``` sql
-CREATE TABLE summtt
-(
-    key UInt32,
-    value UInt32
-)
-ENGINE = SummingMergeTree()
-ORDER BY key
-```
-
-Insertar datos:
-
-``` sql
-INSERT INTO summtt Values(1,1),(1,2),(2,1)
-```
-
-ClickHouse puede sumar todas las filas no completamente ([ver abajo](#data-processing)), entonces usamos una función agregada `sum` y `GROUP BY` cláusula en la consulta.
-
-``` sql
-SELECT key, sum(value) FROM summtt GROUP BY key
-```
-
-``` text
-┌─key─┬─sum(value)─┐
-│   2 │          1 │
-│   1 │          3 │
-└─────┴────────────┘
-```
-
-## Procesamiento de datos {#data-processing}
-
-Cuando los datos se insertan en una tabla, se guardan tal cual. ClickHouse combina las partes insertadas de los datos periódicamente y esto es cuando las filas con la misma clave principal se suman y se reemplazan con una para cada parte resultante de los datos.
-
-ClickHouse can merge the data parts so that different resulting parts of data cat consist rows with the same primary key, i.e. the summation will be incomplete. Therefore (`SELECT`) una función agregada [resumir()](../../../sql-reference/aggregate-functions/reference.md#agg_function-sum) y `GROUP BY` cláusula se debe utilizar en una consulta como se describe en el ejemplo anterior.
-
-### Reglas comunes para la suma {#common-rules-for-summation}
-
-Se resumen los valores de las columnas con el tipo de datos numérico. El conjunto de columnas está definido por el parámetro `columns`.
-
-Si los valores eran 0 en todas las columnas para la suma, se elimina la fila.
-
-Si la columna no está en la clave principal y no se resume, se selecciona un valor arbitrario entre los existentes.
-
-Los valores no se resumen para las columnas de la clave principal.
-
-### La suma en las columnas de función agregada {#the-summation-in-the-aggregatefunction-columns}
-
-Para columnas de [Tipo AggregateFunction](../../../sql-reference/data-types/aggregatefunction.md) ClickHouse se comporta como [AgregaciónMergeTree](aggregatingmergetree.md) agregación del motor según la función.
-
-### Estructuras anidadas {#nested-structures}
-
-La tabla puede tener estructuras de datos anidadas que se procesan de una manera especial.
-
-Si el nombre de una tabla anidada termina con `Map` y contiene al menos dos columnas que cumplen los siguientes criterios:
-
--   la primera columna es numérica `(*Int*, Date, DateTime)` o una cadena `(String, FixedString)`, vamos a llamarlo `key`,
--   las otras columnas son aritméticas `(*Int*, Float32/64)`, vamos a llamarlo `(values...)`,
-
-entonces esta tabla anidada se interpreta como una asignación de `key => (values...)`, y al fusionar sus filas, los elementos de dos conjuntos de datos se fusionan por `key` con una suma de los correspondientes `(values...)`.
-
-Ejemplos:
-
-``` text
-[(1, 100)] + [(2, 150)] -> [(1, 100), (2, 150)]
-[(1, 100)] + [(1, 150)] -> [(1, 250)]
-[(1, 100)] + [(1, 150), (2, 150)] -> [(1, 250), (2, 150)]
-[(1, 100), (2, 150)] + [(1, -100)] -> [(2, 150)]
-```
-
-Al solicitar datos, utilice el [sumMap(clave, valor)](../../../sql-reference/aggregate-functions/reference.md) función para la agregación de `Map`.
-
-Para la estructura de datos anidados, no necesita especificar sus columnas en la tupla de columnas para la suma.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/summingmergetree/) <!--hide-->
diff --git a/docs/es/engines/table-engines/mergetree-family/versionedcollapsingmergetree.md b/docs/es/engines/table-engines/mergetree-family/versionedcollapsingmergetree.md
deleted file mode 100644
index d69bfe9440e0..000000000000
--- a/docs/es/engines/table-engines/mergetree-family/versionedcollapsingmergetree.md
+++ /dev/null
@@ -1,238 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: VersionedCollapsingMergeTree
----
-
-# VersionedCollapsingMergeTree {#versionedcollapsingmergetree}
-
-Este motor:
-
--   Permite la escritura rápida de estados de objetos que cambian continuamente.
--   Elimina los estados de objetos antiguos en segundo plano. Esto reduce significativamente el volumen de almacenamiento.
-
-Vea la sección [Derrumbar](#table_engines_versionedcollapsingmergetree) para más detalles.
-
-El motor hereda de [Método de codificación de datos:](mergetree.md#table_engines-mergetree) y agrega la lógica para colapsar filas al algoritmo para fusionar partes de datos. `VersionedCollapsingMergeTree` tiene el mismo propósito que [ColapsarMergeTree](collapsingmergetree.md) pero usa un algoritmo de colapso diferente que permite insertar los datos en cualquier orden con múltiples hilos. En particular, el `Version` columna ayuda a contraer las filas correctamente, incluso si se insertan en el orden incorrecto. En contraste, `CollapsingMergeTree` sólo permite la inserción estrictamente consecutiva.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = VersionedCollapsingMergeTree(sign, version)
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Para obtener una descripción de los parámetros de consulta, consulte [descripción de la consulta](../../../sql-reference/statements/create.md).
-
-**Parámetros del motor**
-
-``` sql
-VersionedCollapsingMergeTree(sign, version)
-```
-
--   `sign` — Name of the column with the type of row: `1` es una “state” fila, `-1` es una “cancel” fila.
-
-    El tipo de datos de columna debe ser `Int8`.
-
--   `version` — Name of the column with the version of the object state.
-
-    El tipo de datos de columna debe ser `UInt*`.
-
-**Cláusulas de consulta**
-
-Al crear un `VersionedCollapsingMergeTree` mesa, la misma [clausula](mergetree.md) se requieren como al crear un `MergeTree` tabla.
-
-<details markdown="1">
-
-<summary>Método obsoleto para crear una tabla</summary>
-
-!!! attention "Atención"
-    No utilice este método en nuevos proyectos. Si es posible, cambie los proyectos antiguos al método descrito anteriormente.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] VersionedCollapsingMergeTree(date-column [, samp#table_engines_versionedcollapsingmergetreeling_expression], (primary, key), index_granularity, sign, version)
-```
-
-Todos los parámetros excepto `sign` y `version` el mismo significado que en `MergeTree`.
-
--   `sign` — Name of the column with the type of row: `1` es una “state” fila, `-1` es una “cancel” fila.
-
-    Column Data Type — `Int8`.
-
--   `version` — Name of the column with the version of the object state.
-
-    El tipo de datos de columna debe ser `UInt*`.
-
-</details>
-
-## Derrumbar {#table_engines_versionedcollapsingmergetree}
-
-### Datos {#data}
-
-Considere una situación en la que necesite guardar datos que cambien continuamente para algún objeto. Es razonable tener una fila para un objeto y actualizar la fila siempre que haya cambios. Sin embargo, la operación de actualización es costosa y lenta para un DBMS porque requiere volver a escribir los datos en el almacenamiento. La actualización no es aceptable si necesita escribir datos rápidamente, pero puede escribir los cambios en un objeto secuencialmente de la siguiente manera.
-
-Utilice el `Sign` columna al escribir la fila. Si `Sign = 1` significa que la fila es un estado de un objeto (llamémoslo el “state” fila). Si `Sign = -1` indica la cancelación del estado de un objeto con los mismos atributos (llamémoslo el “cancel” fila). También use el `Version` columna, que debe identificar cada estado de un objeto con un número separado.
-
-Por ejemplo, queremos calcular cuántas páginas visitaron los usuarios en algún sitio y cuánto tiempo estuvieron allí. En algún momento escribimos la siguiente fila con el estado de la actividad del usuario:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-En algún momento después registramos el cambio de actividad del usuario y lo escribimos con las siguientes dos filas.
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 |
-│ 4324182021466249494 │         6 │      185 │    1 │       2 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-La primera fila cancela el estado anterior del objeto (usuario). Debe copiar todos los campos del estado cancelado excepto `Sign`.
-
-La segunda fila contiene el estado actual.
-
-Debido a que solo necesitamos el último estado de actividad del usuario, las filas
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 |
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-se puede eliminar, colapsando el estado no válido (antiguo) del objeto. `VersionedCollapsingMergeTree` hace esto mientras fusiona las partes de datos.
-
-Para averiguar por qué necesitamos dos filas para cada cambio, vea [Algoritmo](#table_engines-versionedcollapsingmergetree-algorithm).
-
-**Notas sobre el uso**
-
-1.  El programa que escribe los datos debe recordar el estado de un objeto para cancelarlo. El “cancel” cadena debe ser una copia de la “state” con lo opuesto `Sign`. Esto aumenta el tamaño inicial de almacenamiento, pero permite escribir los datos rápidamente.
-2.  Las matrices de largo crecimiento en columnas reducen la eficiencia del motor debido a la carga para escribir. Cuanto más sencillos sean los datos, mejor será la eficiencia.
-3.  `SELECT` Los resultados dependen en gran medida de la coherencia del historial de cambios de objetos. Sea preciso al preparar los datos para insertarlos. Puede obtener resultados impredecibles con datos incoherentes, como valores negativos para métricas no negativas, como la profundidad de la sesión.
-
-### Algoritmo {#table_engines-versionedcollapsingmergetree-algorithm}
-
-Cuando ClickHouse combina partes de datos, elimina cada par de filas que tienen la misma clave principal y versión y diferentes `Sign`. El orden de las filas no importa.
-
-Cuando ClickHouse inserta datos, ordena filas por la clave principal. Si el `Version` la columna no está en la clave principal, ClickHouse la agrega a la clave principal implícitamente como el último campo y la usa para ordenar.
-
-## Selección de datos {#selecting-data}
-
-ClickHouse no garantiza que todas las filas con la misma clave principal estén en la misma parte de datos resultante o incluso en el mismo servidor físico. Esto es cierto tanto para escribir los datos como para la posterior fusión de las partes de datos. Además, ClickHouse procesa `SELECT` consultas con múltiples subprocesos, y no puede predecir el orden de las filas en el resultado. Esto significa que la agregación es necesaria si hay una necesidad de obtener completamente “collapsed” datos de un `VersionedCollapsingMergeTree` tabla.
-
-Para finalizar el colapso, escriba una consulta con un `GROUP BY` cláusula y funciones agregadas que representan el signo. Por ejemplo, para calcular la cantidad, use `sum(Sign)` en lugar de `count()`. Para calcular la suma de algo, use `sum(Sign * x)` en lugar de `sum(x)` y agregar `HAVING sum(Sign) > 0`.
-
-Los agregados `count`, `sum` y `avg` se puede calcular de esta manera. El agregado `uniq` se puede calcular si un objeto tiene al menos un estado no colapsado. Los agregados `min` y `max` no se puede calcular porque `VersionedCollapsingMergeTree` no guarda el historial de valores de estados colapsados.
-
-Si necesita extraer los datos con “collapsing” pero sin agregación (por ejemplo, para verificar si hay filas presentes cuyos valores más nuevos coinciden con ciertas condiciones), puede usar el `FINAL` modificador para el `FROM` clausula. Este enfoque es ineficiente y no debe usarse con tablas grandes.
-
-## Ejemplo de uso {#example-of-use}
-
-Datos de ejemplo:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 |
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 |
-│ 4324182021466249494 │         6 │      185 │    1 │       2 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-Creación de la tabla:
-
-``` sql
-CREATE TABLE UAct
-(
-    UserID UInt64,
-    PageViews UInt8,
-    Duration UInt8,
-    Sign Int8,
-    Version UInt8
-)
-ENGINE = VersionedCollapsingMergeTree(Sign, Version)
-ORDER BY UserID
-```
-
-Insertar los datos:
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1, 1)
-```
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1, 1),(4324182021466249494, 6, 185, 1, 2)
-```
-
-Usamos dos `INSERT` consultas para crear dos partes de datos diferentes. Si insertamos los datos con una sola consulta, ClickHouse crea una parte de datos y nunca realizará ninguna fusión.
-
-Obtener los datos:
-
-``` sql
-SELECT * FROM UAct
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 │
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 │
-│ 4324182021466249494 │         6 │      185 │    1 │       2 │
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-¿Qué vemos aquí y dónde están las partes colapsadas?
-Creamos dos partes de datos usando dos `INSERT` consulta. El `SELECT` la consulta se realizó en dos subprocesos, y el resultado es un orden aleatorio de filas.
-No se produjo el colapso porque las partes de datos aún no se han fusionado. ClickHouse fusiona partes de datos en un punto desconocido en el tiempo que no podemos predecir.
-
-Es por eso que necesitamos agregación:
-
-``` sql
-SELECT
-    UserID,
-    sum(PageViews * Sign) AS PageViews,
-    sum(Duration * Sign) AS Duration,
-    Version
-FROM UAct
-GROUP BY UserID, Version
-HAVING sum(Sign) > 0
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Version─┐
-│ 4324182021466249494 │         6 │      185 │       2 │
-└─────────────────────┴───────────┴──────────┴─────────┘
-```
-
-Si no necesitamos agregación y queremos forzar el colapso, podemos usar el `FINAL` modificador para el `FROM` clausula.
-
-``` sql
-SELECT * FROM UAct FINAL
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         6 │      185 │    1 │       2 │
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-Esta es una forma muy ineficiente de seleccionar datos. No lo use para mesas grandes.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/versionedcollapsingmergetree/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/buffer.md b/docs/es/engines/table-engines/special/buffer.md
deleted file mode 100644
index b3a26ff356a6..000000000000
--- a/docs/es/engines/table-engines/special/buffer.md
+++ /dev/null
@@ -1,71 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: "B\xFAfer"
----
-
-# Búfer {#buffer}
-
-Almacena los datos para escribir en la memoria RAM, enjuagándolos periódicamente a otra tabla. Durante la operación de lectura, los datos se leen desde el búfer y la otra tabla simultáneamente.
-
-``` sql
-Buffer(database, table, num_layers, min_time, max_time, min_rows, max_rows, min_bytes, max_bytes)
-```
-
-Parámetros del motor:
-
--   `database` – Database name. Instead of the database name, you can use a constant expression that returns a string.
--   `table` – Table to flush data to.
--   `num_layers` – Parallelism layer. Physically, the table will be represented as `num_layers` de búferes independientes. Valor recomendado: 16.
--   `min_time`, `max_time`, `min_rows`, `max_rows`, `min_bytes`, y `max_bytes` – Conditions for flushing data from the buffer.
-
-Los datos se vacían del búfer y se escriben en la tabla de destino si `min*` condiciones o al menos una `max*` condición se cumplen.
-
--   `min_time`, `max_time` – Condition for the time in seconds from the moment of the first write to the buffer.
--   `min_rows`, `max_rows` – Condition for the number of rows in the buffer.
--   `min_bytes`, `max_bytes` – Condition for the number of bytes in the buffer.
-
-Durante la operación de escritura, los datos se insertan en un `num_layers` número de búferes aleatorios. O bien, si la parte de datos para insertar es lo suficientemente grande (mayor que `max_rows` o `max_bytes`), se escribe directamente en la tabla de destino, omitiendo el búfer.
-
-Las condiciones para el lavado de los datos se calculan por separado para cada uno de los `num_layers` búfer. Por ejemplo, si `num_layers = 16` y `max_bytes = 100000000`, el consumo máximo de RAM es de 1.6 GB.
-
-Ejemplo:
-
-``` sql
-CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10, 100, 10000, 1000000, 10000000, 100000000)
-```
-
-Creación de un ‘merge.hits_buffer’ mesa con la misma estructura que ‘merge.hits’ y usando el motor Buffer. Al escribir en esta tabla, los datos se almacenan en la memoria RAM y ‘merge.hits’ tabla. Se crean 16 búferes. Los datos de cada uno de ellos se vacían si han pasado 100 segundos o se han escrito un millón de filas o se han escrito 100 MB de datos; o si simultáneamente han pasado 10 segundos y se han escrito 10.000 filas y 10 MB de datos. Por ejemplo, si solo se ha escrito una fila, después de 100 segundos se vaciará, pase lo que pase. Pero si se han escrito muchas filas, los datos se vaciarán antes.
-
-Cuando se detiene el servidor, con DROP TABLE o DETACH TABLE, los datos del búfer también se vacían a la tabla de destino.
-
-Puede establecer cadenas vacías entre comillas simples para la base de datos y el nombre de la tabla. Esto indica la ausencia de una tabla de destino. En este caso, cuando se alcanzan las condiciones de descarga de datos, el búfer simplemente se borra. Esto puede ser útil para mantener una ventana de datos en la memoria.
-
-Al leer desde una tabla de búfer, los datos se procesan tanto desde el búfer como desde la tabla de destino (si hay uno).
-Tenga en cuenta que las tablas Buffer no admiten un índice. En otras palabras, los datos del búfer se analizan por completo, lo que puede ser lento para los búferes grandes. (Para los datos de una tabla subordinada, se utilizará el índice que admite.)
-
-Si el conjunto de columnas de la tabla Buffer no coincide con el conjunto de columnas de una tabla subordinada, se inserta un subconjunto de columnas que existen en ambas tablas.
-
-Si los tipos no coinciden con una de las columnas de la tabla Búfer y una tabla subordinada, se escribe un mensaje de error en el registro del servidor y se borra el búfer.
-Lo mismo sucede si la tabla subordinada no existe cuando se vacía el búfer.
-
-Si necesita ejecutar ALTER para una tabla subordinada y la tabla de búfer, se recomienda eliminar primero la tabla de búfer, ejecutar ALTER para la tabla subordinada y, a continuación, crear la tabla de búfer de nuevo.
-
-Si el servidor se reinicia de forma anormal, se pierden los datos del búfer.
-
-FINAL y SAMPLE no funcionan correctamente para las tablas Buffer. Estas condiciones se pasan a la tabla de destino, pero no se utilizan para procesar datos en el búfer. Si se requieren estas características, recomendamos usar solo la tabla Buffer para escribir, mientras lee desde la tabla de destino.
-
-Al agregar datos a un búfer, uno de los búferes está bloqueado. Esto provoca retrasos si se realiza una operación de lectura simultáneamente desde la tabla.
-
-Los datos que se insertan en una tabla de búfer pueden terminar en la tabla subordinada en un orden diferente y en bloques diferentes. Debido a esto, una tabla Buffer es difícil de usar para escribir en un CollapsingMergeTree correctamente. Para evitar problemas, puede establecer ‘num_layers’ a 1.
-
-Si se replica la tabla de destino, se pierden algunas características esperadas de las tablas replicadas al escribir en una tabla de búfer. Los cambios aleatorios en el orden de las filas y los tamaños de las partes de datos hacen que la desduplicación de datos deje de funcionar, lo que significa que no es posible tener un ‘exactly once’ escribir en tablas replicadas.
-
-Debido a estas desventajas, solo podemos recomendar el uso de una tabla Buffer en casos raros.
-
-Una tabla de búfer se usa cuando se reciben demasiados INSERT de un gran número de servidores durante una unidad de tiempo y los datos no se pueden almacenar en búfer antes de la inserción, lo que significa que los INSERT no pueden ejecutarse lo suficientemente rápido.
-
-Tenga en cuenta que no tiene sentido insertar datos una fila a la vez, incluso para las tablas de búfer. Esto solo producirá una velocidad de unos pocos miles de filas por segundo, mientras que la inserción de bloques de datos más grandes puede producir más de un millón de filas por segundo (consulte la sección “Performance”).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/buffer/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/dictionary.md b/docs/es/engines/table-engines/special/dictionary.md
deleted file mode 100644
index 6d9136a6a231..000000000000
--- a/docs/es/engines/table-engines/special/dictionary.md
+++ /dev/null
@@ -1,97 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: Diccionario
----
-
-# Diccionario {#dictionary}
-
-El `Dictionary` el motor muestra el [diccionario](../../../sql-reference/dictionaries/external-dictionaries/external-dicts.md) datos como una tabla ClickHouse.
-
-Como ejemplo, considere un diccionario de `products` con la siguiente configuración:
-
-``` xml
-<dictionaries>
-<dictionary>
-        <name>products</name>
-        <source>
-            <odbc>
-                <table>products</table>
-                <connection_string>DSN=some-db-server</connection_string>
-            </odbc>
-        </source>
-        <lifetime>
-            <min>300</min>
-            <max>360</max>
-        </lifetime>
-        <layout>
-            <flat/>
-        </layout>
-        <structure>
-            <id>
-                <name>product_id</name>
-            </id>
-            <attribute>
-                <name>title</name>
-                <type>String</type>
-                <null_value></null_value>
-            </attribute>
-        </structure>
-</dictionary>
-</dictionaries>
-```
-
-Consultar los datos del diccionario:
-
-``` sql
-SELECT
-    name,
-    type,
-    key,
-    attribute.names,
-    attribute.types,
-    bytes_allocated,
-    element_count,
-    source
-FROM system.dictionaries
-WHERE name = 'products'
-```
-
-``` text
-┌─name─────┬─type─┬─key────┬─attribute.names─┬─attribute.types─┬─bytes_allocated─┬─element_count─┬─source──────────┐
-│ products │ Flat │ UInt64 │ ['title']       │ ['String']      │        23065376 │        175032 │ ODBC: .products │
-└──────────┴──────┴────────┴─────────────────┴─────────────────┴─────────────────┴───────────────┴─────────────────┘
-```
-
-Puede usar el [dictGet\*](../../../sql-reference/functions/ext-dict-functions.md#ext_dict_functions) función para obtener los datos del diccionario en este formato.
-
-Esta vista no es útil cuando necesita obtener datos sin procesar o cuando `JOIN` operación. Para estos casos, puede usar el `Dictionary` motor, que muestra los datos del diccionario en una tabla.
-
-Sintaxis:
-
-``` sql
-CREATE TABLE %table_name% (%fields%) engine = Dictionary(%dictionary_name%)`
-```
-
-Ejemplo de uso:
-
-``` sql
-create table products (product_id UInt64, title String) Engine = Dictionary(products);
-```
-
-      Ok
-
-Echa un vistazo a lo que hay en la mesa.
-
-``` sql
-select * from products limit 1;
-```
-
-``` text
-┌────product_id─┬─title───────────┐
-│        152689 │ Some item       │
-└───────────────┴─────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/dictionary/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/distributed.md b/docs/es/engines/table-engines/special/distributed.md
deleted file mode 100644
index bac407a651a8..000000000000
--- a/docs/es/engines/table-engines/special/distributed.md
+++ /dev/null
@@ -1,152 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: Distribuido
----
-
-# Distribuido {#distributed}
-
-**Las tablas con motor distribuido no almacenan ningún dato por sí mismas**, pero permite el procesamiento de consultas distribuidas en varios servidores.
-La lectura se paralela automáticamente. Durante una lectura, se utilizan los índices de tabla en servidores remotos, si los hay.
-
-El motor distribuido acepta parámetros:
-
--   el nombre del clúster en el archivo de configuración del servidor
-
--   el nombre de una base de datos remota
-
--   el nombre de una tabla remota
-
--   (opcionalmente) clave de fragmentación
-
--   nombre de política (opcionalmente), se usará para almacenar archivos temporales para el envío asíncrono
-
-    Ver también:
-
-    -   `insert_distributed_sync` configuración
-    -   [Método de codificación de datos:](../mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes) para los ejemplos
-
-Ejemplo:
-
-``` sql
-Distributed(logs, default, hits[, sharding_key[, policy_name]])
-```
-
-Los datos se leerán desde todos los servidores ‘logs’ clúster, desde el valor predeterminado.tabla de éxitos ubicada en cada servidor del clúster.
-Los datos no solo se leen sino que se procesan parcialmente en los servidores remotos (en la medida en que esto sea posible).
-Por ejemplo, para una consulta con GROUP BY, los datos se agregarán en servidores remotos y los estados intermedios de las funciones agregadas se enviarán al servidor solicitante. Luego, los datos se agregarán más.
-
-En lugar del nombre de la base de datos, puede usar una expresión constante que devuelva una cadena. Por ejemplo: currentDatabase().
-
-logs – The cluster name in the server's config file.
-
-Los clústeres se establecen así:
-
-``` xml
-<remote_servers>
-    <logs>
-        <shard>
-            <!-- Optional. Shard weight when writing data. Default: 1. -->
-            <weight>1</weight>
-            <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->
-            <internal_replication>false</internal_replication>
-            <replica>
-                <host>example01-01-1</host>
-                <port>9000</port>
-            </replica>
-            <replica>
-                <host>example01-01-2</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-        <shard>
-            <weight>2</weight>
-            <internal_replication>false</internal_replication>
-            <replica>
-                <host>example01-02-1</host>
-                <port>9000</port>
-            </replica>
-            <replica>
-                <host>example01-02-2</host>
-                <secure>1</secure>
-                <port>9440</port>
-            </replica>
-        </shard>
-    </logs>
-</remote_servers>
-```
-
-Aquí se define un clúster con el nombre ‘logs’ que consta de dos fragmentos, cada uno de los cuales contiene dos réplicas.
-Los fragmentos se refieren a los servidores que contienen diferentes partes de los datos (para leer todos los datos, debe acceder a todos los fragmentos).
-Las réplicas están duplicando servidores (para leer todos los datos, puede acceder a los datos en cualquiera de las réplicas).
-
-Los nombres de clúster no deben contener puntos.
-
-Los parámetros `host`, `port`, y opcionalmente `user`, `password`, `secure`, `compression` se especifican para cada servidor:
-- `host` – The address of the remote server. You can use either the domain or the IPv4 or IPv6 address. If you specify the domain, the server makes a DNS request when it starts, and the result is stored as long as the server is running. If the DNS request fails, the server doesn't start. If you change the DNS record, restart the server.
-- `port` – The TCP port for messenger activity (‘tcp_port’ en la configuración, generalmente establecido en 9000). No lo confundas con http_port.
-- `user` – Name of the user for connecting to a remote server. Default value: default. This user must have access to connect to the specified server. Access is configured in the users.xml file. For more information, see the section [Derechos de acceso](../../../operations/access-rights.md).
-- `password` – The password for connecting to a remote server (not masked). Default value: empty string.
-- `secure` - Use ssl para la conexión, por lo general también debe definir `port` = 9440. El servidor debe escuchar en `<tcp_port_secure>9440</tcp_port_secure>` y tener certificados correctos.
-- `compression` - Utilice la compresión de datos. Valor predeterminado: true.
-
-When specifying replicas, one of the available replicas will be selected for each of the shards when reading. You can configure the algorithm for load balancing (the preference for which replica to access) – see the [load_balancing](../../../operations/settings/settings.md#settings-load_balancing) configuración.
-Si no se establece la conexión con el servidor, habrá un intento de conectarse con un breve tiempo de espera. Si la conexión falla, se seleccionará la siguiente réplica, y así sucesivamente para todas las réplicas. Si el intento de conexión falló para todas las réplicas, el intento se repetirá de la misma manera, varias veces.
-Esto funciona a favor de la resiliencia, pero no proporciona una tolerancia completa a errores: un servidor remoto podría aceptar la conexión, pero podría no funcionar o funcionar mal.
-
-Puede especificar solo uno de los fragmentos (en este caso, el procesamiento de consultas debe denominarse remoto, en lugar de distribuido) o hasta cualquier número de fragmentos. En cada fragmento, puede especificar entre una y cualquier número de réplicas. Puede especificar un número diferente de réplicas para cada fragmento.
-
-Puede especificar tantos clústeres como desee en la configuración.
-
-Para ver los clústeres, utilice el ‘system.clusters’ tabla.
-
-El motor distribuido permite trabajar con un clúster como un servidor local. Sin embargo, el clúster es inextensible: debe escribir su configuración en el archivo de configuración del servidor (mejor aún, para todos los servidores del clúster).
-
-The Distributed engine requires writing clusters to the config file. Clusters from the config file are updated on the fly, without restarting the server. If you need to send a query to an unknown set of shards and replicas each time, you don't need to create a Distributed table – use the ‘remote’ función de tabla en su lugar. Vea la sección [Funciones de tabla](../../../sql-reference/table-functions/index.md).
-
-Hay dos métodos para escribir datos en un clúster:
-
-Primero, puede definir a qué servidores escribir en qué datos y realizar la escritura directamente en cada fragmento. En otras palabras, realice INSERT en las tablas que la tabla distribuida “looks at”. Esta es la solución más flexible, ya que puede usar cualquier esquema de fragmentación, que podría ser no trivial debido a los requisitos del área temática. Esta es también la solución más óptima ya que los datos se pueden escribir en diferentes fragmentos de forma completamente independiente.
-
-En segundo lugar, puede realizar INSERT en una tabla distribuida. En este caso, la tabla distribuirá los datos insertados a través de los propios servidores. Para escribir en una tabla distribuida, debe tener un conjunto de claves de fragmentación (el último parámetro). Además, si solo hay un fragmento, la operación de escritura funciona sin especificar la clave de fragmentación, ya que no significa nada en este caso.
-
-Cada fragmento puede tener un peso definido en el archivo de configuración. Por defecto, el peso es igual a uno. Los datos se distribuyen entre fragmentos en la cantidad proporcional al peso del fragmento. Por ejemplo, si hay dos fragmentos y el primero tiene un peso de 9 mientras que el segundo tiene un peso de 10, el primero se enviará 9 / 19 partes de las filas, y el segundo se enviará 10 / 19.
-
-Cada fragmento puede tener el ‘internal_replication’ parámetro definido en el archivo de configuración.
-
-Si este parámetro se establece en ‘true’, la operación de escritura selecciona la primera réplica en buen estado y escribe datos en ella. Utilice esta alternativa si la tabla Distribuida “looks at” tablas replicadas. En otras palabras, si la tabla donde se escribirán los datos los replicará por sí misma.
-
-Si se establece en ‘false’ (el valor predeterminado), los datos se escriben en todas las réplicas. En esencia, esto significa que la tabla distribuida replica los datos en sí. Esto es peor que usar tablas replicadas, porque no se verifica la consistencia de las réplicas y, con el tiempo, contendrán datos ligeramente diferentes.
-
-Para seleccionar el fragmento al que se envía una fila de datos, se analiza la expresión de fragmentación y su resto se toma de dividirlo por el peso total de los fragmentos. La fila se envía al fragmento que corresponde al medio intervalo de los restos de ‘prev_weight’ a ‘prev_weights + weight’, donde ‘prev_weights’ es el peso total de los fragmentos con el número más pequeño, y ‘weight’ es el peso de este fragmento. Por ejemplo, si hay dos fragmentos, y el primero tiene un peso de 9 mientras que el segundo tiene un peso de 10, la fila se enviará al primer fragmento para los restos del rango \[0, 9), y al segundo para los restos del rango \[9, 19).
-
-La expresión de fragmentación puede ser cualquier expresión de constantes y columnas de tabla que devuelva un entero. Por ejemplo, puede usar la expresión ‘rand()’ para la distribución aleatoria de datos, o ‘UserID’ para la distribución por el resto de dividir la ID del usuario (entonces los datos de un solo usuario residirán en un solo fragmento, lo que simplifica la ejecución de IN y JOIN por los usuarios). Si una de las columnas no se distribuye lo suficientemente uniformemente, puede envolverla en una función hash: intHash64(UserID) .
-
-Un simple recordatorio de la división es una solución limitada para sharding y no siempre es apropiado. Funciona para volúmenes medianos y grandes de datos (docenas de servidores), pero no para volúmenes muy grandes de datos (cientos de servidores o más). En este último caso, use el esquema de fragmentación requerido por el área asunto, en lugar de usar entradas en Tablas distribuidas.
-
-SELECT queries are sent to all the shards and work regardless of how data is distributed across the shards (they can be distributed completely randomly). When you add a new shard, you don't have to transfer the old data to it. You can write new data with a heavier weight – the data will be distributed slightly unevenly, but queries will work correctly and efficiently.
-
-Debería preocuparse por el esquema de fragmentación en los siguientes casos:
-
--   Se utilizan consultas que requieren unir datos (IN o JOIN) mediante una clave específica. Si esta clave fragmenta datos, puede usar IN local o JOIN en lugar de GLOBAL IN o GLOBAL JOIN, que es mucho más eficiente.
--   Se usa una gran cantidad de servidores (cientos o más) con una gran cantidad de consultas pequeñas (consultas de clientes individuales: sitios web, anunciantes o socios). Para que las pequeñas consultas no afecten a todo el clúster, tiene sentido ubicar datos para un solo cliente en un solo fragmento. Alternativamente, como lo hemos hecho en Yandex.Metrica, puede configurar sharding de dos niveles: divida todo el clúster en “layers”, donde una capa puede consistir en varios fragmentos. Los datos de un único cliente se encuentran en una sola capa, pero los fragmentos se pueden agregar a una capa según sea necesario y los datos se distribuyen aleatoriamente dentro de ellos. Las tablas distribuidas se crean para cada capa y se crea una única tabla distribuida compartida para consultas globales.
-
-Los datos se escriben de forma asíncrona. Cuando se inserta en la tabla, el bloque de datos se acaba de escribir en el sistema de archivos local. Los datos se envían a los servidores remotos en segundo plano tan pronto como sea posible. El período de envío de datos está gestionado por el [Distributed_directory_monitor_sleep_time_ms](../../../operations/settings/settings.md#distributed_directory_monitor_sleep_time_ms) y [Distributed_directory_monitor_max_sleep_time_ms](../../../operations/settings/settings.md#distributed_directory_monitor_max_sleep_time_ms) configuración. El `Distributed` el motor envía cada archivo con datos insertados por separado, pero puede habilitar el envío por lotes de archivos [distributed_directory_monitor_batch_inserts](../../../operations/settings/settings.md#distributed_directory_monitor_batch_inserts) configuración. Esta configuración mejora el rendimiento del clúster al utilizar mejor los recursos de red y servidor local. Debe comprobar si los datos se envían correctamente comprobando la lista de archivos (datos en espera de ser enviados) en el directorio de la tabla: `/var/lib/clickhouse/data/database/table/`.
-
-Si el servidor dejó de existir o tuvo un reinicio aproximado (por ejemplo, después de un error de dispositivo) después de un INSERT en una tabla distribuida, es posible que se pierdan los datos insertados. Si se detecta un elemento de datos dañado en el directorio de la tabla, se transfiere al ‘broken’ subdirectorio y ya no se utiliza.
-
-Cuando la opción max_parallel_replicas está habilitada, el procesamiento de consultas se paralela en todas las réplicas dentro de un solo fragmento. Para obtener más información, consulte la sección [max_parallel_replicas](../../../operations/settings/settings.md#settings-max_parallel_replicas).
-
-## Virtual Columnas {#virtual-columns}
-
--   `_shard_num` — Contains the `shard_num` (de `system.clusters`). Tipo: [UInt32](../../../sql-reference/data-types/int-uint.md).
-
-!!! note "Nota"
-    Ya [`remote`](../../../sql-reference/table-functions/remote.md)/`cluster` funciones de tabla crean internamente instancia temporal del mismo motor distribuido, `_shard_num` está disponible allí también.
-
-**Ver también**
-
--   [Virtual columnas](index.md#table_engines-virtual_columns)
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/distributed/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/external-data.md b/docs/es/engines/table-engines/special/external-data.md
deleted file mode 100644
index f2ce4abbb0f1..000000000000
--- a/docs/es/engines/table-engines/special/external-data.md
+++ /dev/null
@@ -1,68 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: Datos externos
----
-
-# Datos externos para el procesamiento de consultas {#external-data-for-query-processing}
-
-ClickHouse permite enviar a un servidor los datos necesarios para procesar una consulta, junto con una consulta SELECT. Estos datos se colocan en una tabla temporal (consulte la sección “Temporary tables”) y se puede utilizar en la consulta (por ejemplo, en operadores IN).
-
-Por ejemplo, si tiene un archivo de texto con identificadores de usuario importantes, puede cargarlo en el servidor junto con una consulta que utilice la filtración de esta lista.
-
-Si necesita ejecutar más de una consulta con un gran volumen de datos externos, no utilice esta función. Es mejor cargar los datos a la base de datos con anticipación.
-
-Los datos externos se pueden cargar mediante el cliente de línea de comandos (en modo no interactivo) o mediante la interfaz HTTP.
-
-En el cliente de línea de comandos, puede especificar una sección de parámetros en el formato
-
-``` bash
---external --file=... [--name=...] [--format=...] [--types=...|--structure=...]
-```
-
-Puede tener varias secciones como esta, para el número de tablas que se transmiten.
-
-**–external** – Marks the beginning of a clause.
-**–file** – Path to the file with the table dump, or -, which refers to stdin.
-Solo se puede recuperar una sola tabla de stdin.
-
-Los siguientes parámetros son opcionales: **–name**– Name of the table. If omitted, _data is used.
-**–format** – Data format in the file. If omitted, TabSeparated is used.
-
-Se requiere uno de los siguientes parámetros:**–types** – A list of comma-separated column types. For example: `UInt64,String`. The columns will be named _1, _2, …
-**–structure**– The table structure in the format`UserID UInt64`, `URL String`. Define los nombres y tipos de columna.
-
-Los archivos especificados en ‘file’ se analizará mediante el formato especificado en ‘format’ utilizando los tipos de datos especificados en ‘types’ o ‘structure’. La mesa será cargado en el servidor y accesibles, como una tabla temporal con el nombre de ‘name’.
-
-Ejemplos:
-
-``` bash
-$ echo -ne "1
2
3
" | clickhouse-client --query="SELECT count() FROM test.visits WHERE TraficSourceID IN _data" --external --file=- --types=Int8
-849897
-$ cat /etc/passwd | sed 's/:/\t/g' | clickhouse-client --query="SELECT shell, count() AS c FROM passwd GROUP BY shell ORDER BY c DESC" --external --file=- --name=passwd --structure='login String, unused String, uid UInt16, gid UInt16, comment String, home String, shell String'
-/bin/sh 20
-/bin/false      5
-/bin/bash       4
-/usr/sbin/nologin       1
-/bin/sync       1
-```
-
-Cuando se utiliza la interfaz HTTP, los datos externos se pasan en el formato multipart/form-data. Cada tabla se transmite como un archivo separado. El nombre de la tabla se toma del nombre del archivo. El ‘query_string’ se pasa los parámetros ‘name_format’, ‘name_types’, y ‘name_structure’, donde ‘name’ es el nombre de la tabla a la que corresponden estos parámetros. El significado de los parámetros es el mismo que cuando se usa el cliente de línea de comandos.
-
-Ejemplo:
-
-``` bash
-$ cat /etc/passwd | sed 's/:/\t/g' > passwd.tsv
-
-$ curl -F 'passwd=@passwd.tsv;' 'http://localhost:8123/?query=SELECT+shell,+count()+AS+c+FROM+passwd+GROUP+BY+shell+ORDER+BY+c+DESC&passwd_structure=login+String,+unused+String,+uid+UInt16,+gid+UInt16,+comment+String,+home+String,+shell+String'
-/bin/sh 20
-/bin/false      5
-/bin/bash       4
-/usr/sbin/nologin       1
-/bin/sync       1
-```
-
-Para el procesamiento de consultas distribuidas, las tablas temporales se envían a todos los servidores remotos.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/external_data/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/file.md b/docs/es/engines/table-engines/special/file.md
deleted file mode 100644
index fb739506a22e..000000000000
--- a/docs/es/engines/table-engines/special/file.md
+++ /dev/null
@@ -1,90 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: File
----
-
-# File {#table_engines-file}
-
-El motor de tabla de archivos mantiene los datos en un archivo en uno de los [file
-formato](../../../interfaces/formats.md#formats) (TabSeparated, Native, etc.).
-
-Ejemplos de uso:
-
--   Exportación de datos de ClickHouse a archivo.
--   Convertir datos de un formato a otro.
--   Actualización de datos en ClickHouse mediante la edición de un archivo en un disco.
-
-## Uso en el servidor ClickHouse {#usage-in-clickhouse-server}
-
-``` sql
-File(Format)
-```
-
-El `Format` parámetro especifica uno de los formatos de archivo disponibles. Realizar
-`SELECT` consultas, el formato debe ser compatible para la entrada, y para realizar
-`INSERT` queries – for output. The available formats are listed in the
-[Formato](../../../interfaces/formats.md#formats) apartado.
-
-ClickHouse no permite especificar la ruta del sistema de archivos para`File`. Utilizará la carpeta definida por [camino](../../../operations/server-configuration-parameters/settings.md) configuración en la configuración del servidor.
-
-Al crear una tabla usando `File(Format)` crea un subdirectorio vacío en esa carpeta. Cuando los datos se escriben en esa tabla, se colocan en `data.Format` en ese subdirectorio.
-
-Puede crear manualmente esta subcarpeta y archivo en el sistema de archivos del servidor y luego [ATTACH](../../../sql-reference/statements/misc.md) para mostrar información con el nombre coincidente, para que pueda consultar datos desde ese archivo.
-
-!!! warning "Advertencia"
-    Tenga cuidado con esta funcionalidad, ya que ClickHouse no realiza un seguimiento de los cambios externos en dichos archivos. El resultado de las escrituras simultáneas a través de ClickHouse y fuera de ClickHouse no está definido.
-
-**Ejemplo:**
-
-**1.** Configurar el `file_engine_table` tabla:
-
-``` sql
-CREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(TabSeparated)
-```
-
-Por defecto, ClickHouse creará una carpeta `/var/lib/clickhouse/data/default/file_engine_table`.
-
-**2.** Crear manualmente `/var/lib/clickhouse/data/default/file_engine_table/data.TabSeparated` contener:
-
-``` bash
-$ cat data.TabSeparated
-one 1
-two 2
-```
-
-**3.** Consultar los datos:
-
-``` sql
-SELECT * FROM file_engine_table
-```
-
-``` text
-┌─name─┬─value─┐
-│ one  │     1 │
-│ two  │     2 │
-└──────┴───────┘
-```
-
-## Uso en ClickHouse-local {#usage-in-clickhouse-local}
-
-En [Sistema abierto.](../../../operations/utilities/clickhouse-local.md#clickhouse-local) El motor de archivos acepta la ruta del archivo además de `Format`. Los flujos de entrada / salida predeterminados se pueden especificar utilizando nombres numéricos o legibles por humanos como `0` o `stdin`, `1` o `stdout`.
-**Ejemplo:**
-
-``` bash
-$ echo -e "1,2
3,4" | clickhouse-local -q "CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table"
-```
-
-## Detalles de la implementación {#details-of-implementation}
-
--   Multiple `SELECT` las consultas se pueden realizar simultáneamente, pero `INSERT` las consultas se esperarán entre sí.
--   Apoyado la creación de nuevos archivos por `INSERT` consulta.
--   Si el archivo existe, `INSERT` añadiría nuevos valores en él.
--   No soportado:
-    -   `ALTER`
-    -   `SELECT ... SAMPLE`
-    -   Indice
-    -   Replicación
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/file/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/generate.md b/docs/es/engines/table-engines/special/generate.md
deleted file mode 100644
index 67e664284b41..000000000000
--- a/docs/es/engines/table-engines/special/generate.md
+++ /dev/null
@@ -1,61 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: GenerateRandom
----
-
-# Generaterandom {#table_engines-generate}
-
-El motor de tabla GenerateRandom produce datos aleatorios para el esquema de tabla determinado.
-
-Ejemplos de uso:
-
--   Se usa en la prueba para poblar una tabla grande reproducible.
--   Generar entrada aleatoria para pruebas de fuzzing.
-
-## Uso en el servidor ClickHouse {#usage-in-clickhouse-server}
-
-``` sql
-ENGINE = GenerateRandom(random_seed, max_string_length, max_array_length)
-```
-
-El `max_array_length` y `max_string_length` parámetros especifican la longitud máxima de todos
-columnas y cadenas de matriz correspondientemente en los datos generados.
-
-Generar motor de tabla sólo admite `SELECT` consulta.
-
-Es compatible con todos [Tipos de datos](../../../sql-reference/data-types/index.md) que se pueden almacenar en una tabla excepto `LowCardinality` y `AggregateFunction`.
-
-**Ejemplo:**
-
-**1.** Configurar el `generate_engine_table` tabla:
-
-``` sql
-CREATE TABLE generate_engine_table (name String, value UInt32) ENGINE = GenerateRandom(1, 5, 3)
-```
-
-**2.** Consultar los datos:
-
-``` sql
-SELECT * FROM generate_engine_table LIMIT 3
-```
-
-``` text
-┌─name─┬──────value─┐
-│ c4xJ │ 1412771199 │
-│ r    │ 1791099446 │
-│ 7#$  │  124312908 │
-└──────┴────────────┘
-```
-
-## Detalles de la implementación {#details-of-implementation}
-
--   No soportado:
-    -   `ALTER`
-    -   `SELECT ... SAMPLE`
-    -   `INSERT`
-    -   Indice
-    -   Replicación
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/generate/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/index.md b/docs/es/engines/table-engines/special/index.md
deleted file mode 100644
index 9927a1f61d9f..000000000000
--- a/docs/es/engines/table-engines/special/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Especial
-toc_priority: 31
----
-
-
diff --git a/docs/es/engines/table-engines/special/join.md b/docs/es/engines/table-engines/special/join.md
deleted file mode 100644
index 83e21b7c8cc6..000000000000
--- a/docs/es/engines/table-engines/special/join.md
+++ /dev/null
@@ -1,111 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: Unir
----
-
-# Unir {#join}
-
-Estructura de datos preparada para usar en [JOIN](../../../sql-reference/statements/select/join.md#select-join) operación.
-
-## Creación de una tabla {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2],
-) ENGINE = Join(join_strictness, join_type, k1[, k2, ...])
-```
-
-Vea la descripción detallada del [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) consulta.
-
-**Parámetros del motor**
-
--   `join_strictness` – [ÚNETE a la rigurosidad](../../../sql-reference/statements/select/join.md#select-join-types).
--   `join_type` – [Tipo de unión](../../../sql-reference/statements/select/join.md#select-join-types).
--   `k1[, k2, ...]` – Key columns from the `USING` cláusula que el `JOIN` operación se hace con.
-
-Entrar `join_strictness` y `join_type` parámetros sin comillas, por ejemplo, `Join(ANY, LEFT, col1)`. Deben coincidir con el `JOIN` operación para la que se utilizará la tabla. Si los parámetros no coinciden, ClickHouse no lanza una excepción y puede devolver datos incorrectos.
-
-## Uso de la tabla {#table-usage}
-
-### Ejemplo {#example}
-
-Creación de la tabla del lado izquierdo:
-
-``` sql
-CREATE TABLE id_val(`id` UInt32, `val` UInt32) ENGINE = TinyLog
-```
-
-``` sql
-INSERT INTO id_val VALUES (1,11)(2,12)(3,13)
-```
-
-Creando el lado derecho `Join` tabla:
-
-``` sql
-CREATE TABLE id_val_join(`id` UInt32, `val` UInt8) ENGINE = Join(ANY, LEFT, id)
-```
-
-``` sql
-INSERT INTO id_val_join VALUES (1,21)(1,22)(3,23)
-```
-
-Unirse a las tablas:
-
-``` sql
-SELECT * FROM id_val ANY LEFT JOIN id_val_join USING (id) SETTINGS join_use_nulls = 1
-```
-
-``` text
-┌─id─┬─val─┬─id_val_join.val─┐
-│  1 │  11 │              21 │
-│  2 │  12 │            ᴺᵁᴸᴸ │
-│  3 │  13 │              23 │
-└────┴─────┴─────────────────┘
-```
-
-Como alternativa, puede recuperar datos del `Join` tabla, especificando el valor de la clave de unión:
-
-``` sql
-SELECT joinGet('id_val_join', 'val', toUInt32(1))
-```
-
-``` text
-┌─joinGet('id_val_join', 'val', toUInt32(1))─┐
-│                                         21 │
-└────────────────────────────────────────────┘
-```
-
-### Selección e inserción de datos {#selecting-and-inserting-data}
-
-Usted puede utilizar `INSERT` consultas para agregar datos al `Join`-mesas de motor. Si la tabla se creó con el `ANY` estricta, se ignoran los datos de las claves duplicadas. Con el `ALL` estricta, se agregan todas las filas.
-
-No se puede realizar una `SELECT` consulta directamente desde la tabla. En su lugar, use uno de los siguientes métodos:
-
--   Coloque la mesa hacia el lado derecho en un `JOIN` clausula.
--   Llame al [joinGet](../../../sql-reference/functions/other-functions.md#joinget) función, que le permite extraer datos de la tabla de la misma manera que de un diccionario.
-
-### Limitaciones y ajustes {#join-limitations-and-settings}
-
-Al crear una tabla, se aplican los siguientes valores:
-
--   [Sistema abierto.](../../../operations/settings/settings.md#join_use_nulls)
--   [Método de codificación de datos:](../../../operations/settings/query-complexity.md#settings-max_rows_in_join)
--   [Método de codificación de datos:](../../../operations/settings/query-complexity.md#settings-max_bytes_in_join)
--   [join_overflow_mode](../../../operations/settings/query-complexity.md#settings-join_overflow_mode)
--   [join_any_take_last_row](../../../operations/settings/settings.md#settings-join_any_take_last_row)
-
-El `Join`-las tablas del motor no se pueden usar en `GLOBAL JOIN` operación.
-
-El `Join`-motor permite el uso [Sistema abierto.](../../../operations/settings/settings.md#join_use_nulls) ajuste en el `CREATE TABLE` instrucción. Y [SELECT](../../../sql-reference/statements/select/index.md) consulta permite el uso `join_use_nulls` demasiado. Si tienes diferentes `join_use_nulls` configuración, puede obtener un error al unirse a la tabla. Depende del tipo de JOIN. Cuando se utiliza [joinGet](../../../sql-reference/functions/other-functions.md#joinget) función, usted tiene que utilizar el mismo `join_use_nulls` ajuste en `CRATE TABLE` y `SELECT` instrucción.
-
-## Almacenamiento de datos {#data-storage}
-
-`Join` datos de la tabla siempre se encuentra en la memoria RAM. Al insertar filas en una tabla, ClickHouse escribe bloques de datos en el directorio del disco para que puedan restaurarse cuando se reinicie el servidor.
-
-Si el servidor se reinicia incorrectamente, el bloque de datos en el disco puede perderse o dañarse. En este caso, es posible que deba eliminar manualmente el archivo con datos dañados.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/join/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/materializedview.md b/docs/es/engines/table-engines/special/materializedview.md
deleted file mode 100644
index 87e5218eb6a2..000000000000
--- a/docs/es/engines/table-engines/special/materializedview.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: "M\xE9todo de codificaci\xF3n de datos:"
----
-
-# Método de codificación de datos: {#materializedview}
-
-Se utiliza para implementar vistas materializadas (para obtener más información, consulte [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query)). Para almacenar datos, utiliza un motor diferente que se especificó al crear la vista. Al leer desde una tabla, solo usa este motor.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/materializedview/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/memory.md b/docs/es/engines/table-engines/special/memory.md
deleted file mode 100644
index 3d4f8ddff549..000000000000
--- a/docs/es/engines/table-engines/special/memory.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: Memoria
----
-
-# Memoria {#memory}
-
-El motor de memoria almacena datos en RAM, en forma sin comprimir. Los datos se almacenan exactamente en la misma forma en que se reciben cuando se leen. En otras palabras, la lectura de esta tabla es completamente gratuita.
-El acceso a los datos simultáneos está sincronizado. Los bloqueos son cortos: las operaciones de lectura y escritura no se bloquean entre sí.
-Los índices no son compatibles. La lectura está paralelizada.
-La productividad máxima (más de 10 GB/s) se alcanza en consultas simples, porque no hay lectura del disco, descomprimir o deserializar datos. (Cabe señalar que, en muchos casos, la productividad del motor MergeTree es casi tan alta.)
-Al reiniciar un servidor, los datos desaparecen de la tabla y la tabla queda vacía.
-Normalmente, el uso de este motor de tabla no está justificado. Sin embargo, se puede usar para pruebas y para tareas donde se requiere la velocidad máxima en un número relativamente pequeño de filas (hasta aproximadamente 100,000,000).
-
-El sistema utiliza el motor de memoria para tablas temporales con datos de consulta externos (consulte la sección “External data for processing a query”), y para la implementación de GLOBAL IN (véase la sección “IN operators”).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/memory/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/merge.md b/docs/es/engines/table-engines/special/merge.md
deleted file mode 100644
index 6ed2c272914d..000000000000
--- a/docs/es/engines/table-engines/special/merge.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: Fusionar
----
-
-# Fusionar {#merge}
-
-El `Merge` motor (no debe confundirse con `MergeTree`) no almacena datos en sí, pero permite leer de cualquier número de otras tablas simultáneamente.
-La lectura se paralela automáticamente. No se admite la escritura en una tabla. Al leer, se usan los índices de las tablas que realmente se están leyendo, si existen.
-El `Merge` engine acepta parámetros: el nombre de la base de datos y una expresión regular para las tablas.
-
-Ejemplo:
-
-``` sql
-Merge(hits, '^WatchLog')
-```
-
-Los datos se leerán de las tablas en el `hits` base de datos que tienen nombres que coinciden con la expresión regular ‘`^WatchLog`’.
-
-En lugar del nombre de la base de datos, puede usar una expresión constante que devuelva una cadena. Por ejemplo, `currentDatabase()`.
-
-Regular expressions — [Re2](https://github.com/google/re2) (soporta un subconjunto de PCRE), sensible a mayúsculas y minúsculas.
-Vea las notas sobre los símbolos de escape en expresiones regulares en el “match” apartado.
-
-Al seleccionar tablas para leer, el `Merge` no se seleccionará la tabla en sí, incluso si coincide con la expresión regular. Esto es para evitar bucles.
-Es posible crear dos `Merge` tablas que intentarán interminablemente leer los datos de los demás, pero esta no es una buena idea.
-
-La forma típica de usar el `Merge` para trabajar con un gran número de `TinyLog` tablas como si con una sola tabla.
-
-Ejemplo 2:
-
-Digamos que tiene una tabla antigua (WatchLog_old) y decidió cambiar la partición sin mover datos a una nueva tabla (WatchLog_new) y necesita ver datos de ambas tablas.
-
-``` sql
-CREATE TABLE WatchLog_old(date Date, UserId Int64, EventType String, Cnt UInt64)
-ENGINE=MergeTree(date, (UserId, EventType), 8192);
-INSERT INTO WatchLog_old VALUES ('2018-01-01', 1, 'hit', 3);
-
-CREATE TABLE WatchLog_new(date Date, UserId Int64, EventType String, Cnt UInt64)
-ENGINE=MergeTree PARTITION BY date ORDER BY (UserId, EventType) SETTINGS index_granularity=8192;
-INSERT INTO WatchLog_new VALUES ('2018-01-02', 2, 'hit', 3);
-
-CREATE TABLE WatchLog as WatchLog_old ENGINE=Merge(currentDatabase(), '^WatchLog');
-
-SELECT *
-FROM WatchLog
-```
-
-``` text
-┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐
-│ 2018-01-01 │      1 │ hit       │   3 │
-└────────────┴────────┴───────────┴─────┘
-┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐
-│ 2018-01-02 │      2 │ hit       │   3 │
-└────────────┴────────┴───────────┴─────┘
-```
-
-## Virtual Columnas {#virtual-columns}
-
--   `_table` — Contains the name of the table from which data was read. Type: [Cadena](../../../sql-reference/data-types/string.md).
-
-    Puede establecer las condiciones constantes en `_table` en el `WHERE/PREWHERE` cláusula (por ejemplo, `WHERE _table='xyz'`). En este caso, la operación de lectura se realiza sólo para las tablas donde la condición en `_table` está satisfecho, por lo que el `_table` columna actúa como un índice.
-
-**Ver también**
-
--   [Virtual columnas](index.md#table_engines-virtual_columns)
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/merge/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/null.md b/docs/es/engines/table-engines/special/null.md
deleted file mode 100644
index cc05e7839c98..000000000000
--- a/docs/es/engines/table-engines/special/null.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: Nulo
----
-
-# Nulo {#null}
-
-Al escribir en una tabla Null, los datos se ignoran. Al leer desde una tabla Null, la respuesta está vacía.
-
-Sin embargo, puede crear una vista materializada en una tabla Null. Entonces los datos escritos en la tabla terminarán en la vista.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/null/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/set.md b/docs/es/engines/table-engines/special/set.md
deleted file mode 100644
index 4ff232024434..000000000000
--- a/docs/es/engines/table-engines/special/set.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: Establecer
----
-
-# Establecer {#set}
-
-Un conjunto de datos que siempre está en la memoria RAM. Está diseñado para su uso en el lado derecho del operador IN (consulte la sección “IN operators”).
-
-Puede usar INSERT para insertar datos en la tabla. Se agregarán nuevos elementos al conjunto de datos, mientras que los duplicados se ignorarán.
-Pero no puede realizar SELECT desde la tabla. La única forma de recuperar datos es usándolos en la mitad derecha del operador IN.
-
-Los datos siempre se encuentran en la memoria RAM. Para INSERT, los bloques de datos insertados también se escriben en el directorio de tablas en el disco. Al iniciar el servidor, estos datos se cargan en la RAM. En otras palabras, después de reiniciar, los datos permanecen en su lugar.
-
-Para un reinicio aproximado del servidor, el bloque de datos en el disco puede perderse o dañarse. En este último caso, es posible que deba eliminar manualmente el archivo con datos dañados.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/set/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/url.md b/docs/es/engines/table-engines/special/url.md
deleted file mode 100644
index 654b8e99a4e7..000000000000
--- a/docs/es/engines/table-engines/special/url.md
+++ /dev/null
@@ -1,82 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: URL
----
-
-# URL(URL, Formato) {#table_engines-url}
-
-Administra datos en un servidor HTTP/HTTPS remoto. Este motor es similar
-a la [File](file.md) motor.
-
-## Uso del motor en el servidor ClickHouse {#using-the-engine-in-the-clickhouse-server}
-
-El `format` debe ser uno que ClickHouse pueda usar en
-`SELECT` consultas y, si es necesario, en `INSERTs`. Para obtener la lista completa de formatos admitidos, consulte
-[Formato](../../../interfaces/formats.md#formats).
-
-El `URL` debe ajustarse a la estructura de un localizador uniforme de recursos. La dirección URL especificada debe apuntar a un servidor
-que utiliza HTTP o HTTPS. Esto no requiere ningún
-encabezados adicionales para obtener una respuesta del servidor.
-
-`INSERT` y `SELECT` las consultas se transforman en `POST` y `GET` peticiones,
-respectivamente. Para el procesamiento `POST` solicitudes, el servidor remoto debe admitir
-[Codificación de transferencia fragmentada](https://en.wikipedia.org/wiki/Chunked_transfer_encoding).
-
-Puede limitar el número máximo de saltos de redirección HTTP GET utilizando el [Nombre de la red inalámbrica (SSID):](../../../operations/settings/settings.md#setting-max_http_get_redirects) configuración.
-
-**Ejemplo:**
-
-**1.** Crear un `url_engine_table` tabla en el servidor :
-
-``` sql
-CREATE TABLE url_engine_table (word String, value UInt64)
-ENGINE=URL('http://127.0.0.1:12345/', CSV)
-```
-
-**2.** Cree un servidor HTTP básico utilizando las herramientas estándar de Python 3 y
-comenzarlo:
-
-``` python3
-from http.server import BaseHTTPRequestHandler, HTTPServer
-
-class CSVHTTPServer(BaseHTTPRequestHandler):
-    def do_GET(self):
-        self.send_response(200)
-        self.send_header('Content-type', 'text/csv')
-        self.end_headers()
-
-        self.wfile.write(bytes('Hello,1
World,2
', "utf-8"))
-
-if __name__ == "__main__":
-    server_address = ('127.0.0.1', 12345)
-    HTTPServer(server_address, CSVHTTPServer).serve_forever()
-```
-
-``` bash
-$ python3 server.py
-```
-
-**3.** Solicitar datos:
-
-``` sql
-SELECT * FROM url_engine_table
-```
-
-``` text
-┌─word──┬─value─┐
-│ Hello │     1 │
-│ World │     2 │
-└───────┴───────┘
-```
-
-## Detalles de la implementación {#details-of-implementation}
-
--   Las lecturas y escrituras pueden ser paralelas
--   No soportado:
-    -   `ALTER` y `SELECT...SAMPLE` operación.
-    -   Índices.
-    -   Replicación.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/url/) <!--hide-->
diff --git a/docs/es/engines/table-engines/special/view.md b/docs/es/engines/table-engines/special/view.md
deleted file mode 100644
index dbb496bcca43..000000000000
--- a/docs/es/engines/table-engines/special/view.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: Vista
----
-
-# Vista {#table_engines-view}
-
-Se utiliza para implementar vistas (para obtener más información, consulte `CREATE VIEW query`). No almacena datos, pero solo almacena los datos especificados `SELECT` consulta. Al leer desde una tabla, ejecuta esta consulta (y elimina todas las columnas innecesarias de la consulta).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/table_engines/view/) <!--hide-->
diff --git a/docs/es/faq/general.md b/docs/es/faq/general.md
deleted file mode 100644
index f8446e991520..000000000000
--- a/docs/es/faq/general.md
+++ /dev/null
@@ -1,60 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 78
-toc_title: Preguntas generales
----
-
-# Preguntas generales {#general-questions}
-
-## ¿Por qué no usar algo como MapReduce? {#why-not-use-something-like-mapreduce}
-
-Podemos referirnos a sistemas como MapReduce como sistemas informáticos distribuidos en los que la operación de reducción se basa en la clasificación distribuida. La solución de código abierto más común en esta clase es [Acerca de nosotros](http://hadoop.apache.org). Yandex utiliza su solución interna, YT.
-
-Estos sistemas no son apropiados para consultas en línea debido a su alta latencia. En otras palabras, no se pueden usar como back-end para una interfaz web. Estos tipos de sistemas no son útiles para actualizaciones de datos en tiempo real. La clasificación distribuida no es la mejor manera de realizar operaciones de reducción si el resultado de la operación y todos los resultados intermedios (si los hay) se encuentran en la RAM de un único servidor, que generalmente es el caso de las consultas en línea. En tal caso, una tabla hash es una forma óptima de realizar operaciones de reducción. Un enfoque común para optimizar las tareas de reducción de mapas es la preagregación (reducción parcial) utilizando una tabla hash en RAM. El usuario realiza esta optimización manualmente. La clasificación distribuida es una de las principales causas de un rendimiento reducido cuando se ejecutan tareas simples de reducción de mapas.
-
-La mayoría de las implementaciones de MapReduce le permiten ejecutar código arbitrario en un clúster. Pero un lenguaje de consulta declarativo es más adecuado para OLAP para ejecutar experimentos rápidamente. Por ejemplo, Hadoop tiene Hive y Pig. También considere Cloudera Impala o Shark (obsoleto) para Spark, así como Spark SQL, Presto y Apache Drill. El rendimiento cuando se ejecutan tales tareas es muy subóptimo en comparación con los sistemas especializados, pero la latencia relativamente alta hace que sea poco realista utilizar estos sistemas como back-end para una interfaz web.
-
-## ¿Qué sucede si tengo un problema con las codificaciones al usar Oracle a través de ODBC? {#oracle-odbc-encodings}
-
-Si utiliza Oracle a través del controlador ODBC como fuente de diccionarios externos, debe establecer el valor `NLS_LANG` variable de entorno en `/etc/default/clickhouse`. Para obtener más información, consulte [Oracle NLS_LANG Preguntas frecuentes](https://www.oracle.com/technetwork/products/globalization/nls-lang-099431.html).
-
-**Ejemplo**
-
-``` sql
-NLS_LANG=RUSSIAN_RUSSIA.UTF8
-```
-
-## Cómo exporto datos de ClickHouse a un archivo? {#how-to-export-to-file}
-
-### Uso de la cláusula INTO OUTFILE {#using-into-outfile-clause}
-
-Añadir un [INTO OUTFILE](../sql-reference/statements/select/into-outfile.md#into-outfile-clause) cláusula a su consulta.
-
-Por ejemplo:
-
-``` sql
-SELECT * FROM table INTO OUTFILE 'file'
-```
-
-De forma predeterminada, ClickHouse usa el [TabSeparated](../interfaces/formats.md#tabseparated) formato de datos de salida. Para seleccionar el [formato de datos](../interfaces/formats.md), utilizar el [Cláusula FORMAT](../sql-reference/statements/select/format.md#format-clause).
-
-Por ejemplo:
-
-``` sql
-SELECT * FROM table INTO OUTFILE 'file' FORMAT CSV
-```
-
-### Uso de una tabla de motor de archivo {#using-a-file-engine-table}
-
-Ver [File](../engines/table-engines/special/file.md).
-
-### Uso de la redirección de línea de comandos {#using-command-line-redirection}
-
-``` sql
-$ clickhouse-client --query "SELECT * from table" --format FormatName > result.txt
-```
-
-Ver [Casa de clics-cliente](../interfaces/cli.md).
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/faq/general/) ##}
diff --git a/docs/es/faq/index.md b/docs/es/faq/index.md
deleted file mode 100644
index a44dbb31e897..000000000000
--- a/docs/es/faq/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: F.A.Q.
-toc_priority: 76
----
-
-
diff --git a/docs/es/getting-started/example-datasets/amplab-benchmark.md b/docs/es/getting-started/example-datasets/amplab-benchmark.md
deleted file mode 100644
index 066bf0362668..000000000000
--- a/docs/es/getting-started/example-datasets/amplab-benchmark.md
+++ /dev/null
@@ -1,129 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 17
-toc_title: Referencia de Big Data de AMPLab
----
-
-# Referencia de Big Data de AMPLab {#amplab-big-data-benchmark}
-
-Ver https://amplab.cs.berkeley.edu/benchmark/
-
-Regístrese para obtener una cuenta gratuita en https://aws.amazon.com. Requiere una tarjeta de crédito, correo electrónico y número de teléfono. Obtenga una nueva clave de acceso en https://console.aws.amazon.com/iam/home?nc2=h_m_sc#security_credential
-
-Ejecute lo siguiente en la consola:
-
-``` bash
-$ sudo apt-get install s3cmd
-$ mkdir tiny; cd tiny;
-$ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/tiny/ .
-$ cd ..
-$ mkdir 1node; cd 1node;
-$ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/1node/ .
-$ cd ..
-$ mkdir 5nodes; cd 5nodes;
-$ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/5nodes/ .
-$ cd ..
-```
-
-Ejecute las siguientes consultas de ClickHouse:
-
-``` sql
-CREATE TABLE rankings_tiny
-(
-    pageURL String,
-    pageRank UInt32,
-    avgDuration UInt32
-) ENGINE = Log;
-
-CREATE TABLE uservisits_tiny
-(
-    sourceIP String,
-    destinationURL String,
-    visitDate Date,
-    adRevenue Float32,
-    UserAgent String,
-    cCode FixedString(3),
-    lCode FixedString(6),
-    searchWord String,
-    duration UInt32
-) ENGINE = MergeTree(visitDate, visitDate, 8192);
-
-CREATE TABLE rankings_1node
-(
-    pageURL String,
-    pageRank UInt32,
-    avgDuration UInt32
-) ENGINE = Log;
-
-CREATE TABLE uservisits_1node
-(
-    sourceIP String,
-    destinationURL String,
-    visitDate Date,
-    adRevenue Float32,
-    UserAgent String,
-    cCode FixedString(3),
-    lCode FixedString(6),
-    searchWord String,
-    duration UInt32
-) ENGINE = MergeTree(visitDate, visitDate, 8192);
-
-CREATE TABLE rankings_5nodes_on_single
-(
-    pageURL String,
-    pageRank UInt32,
-    avgDuration UInt32
-) ENGINE = Log;
-
-CREATE TABLE uservisits_5nodes_on_single
-(
-    sourceIP String,
-    destinationURL String,
-    visitDate Date,
-    adRevenue Float32,
-    UserAgent String,
-    cCode FixedString(3),
-    lCode FixedString(6),
-    searchWord String,
-    duration UInt32
-) ENGINE = MergeTree(visitDate, visitDate, 8192);
-```
-
-Volver a la consola:
-
-``` bash
-$ for i in tiny/rankings/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO rankings_tiny FORMAT CSV"; done
-$ for i in tiny/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO uservisits_tiny FORMAT CSV"; done
-$ for i in 1node/rankings/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO rankings_1node FORMAT CSV"; done
-$ for i in 1node/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO uservisits_1node FORMAT CSV"; done
-$ for i in 5nodes/rankings/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO rankings_5nodes_on_single FORMAT CSV"; done
-$ for i in 5nodes/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO uservisits_5nodes_on_single FORMAT CSV"; done
-```
-
-Consultas para obtener muestras de datos:
-
-``` sql
-SELECT pageURL, pageRank FROM rankings_1node WHERE pageRank > 1000
-
-SELECT substring(sourceIP, 1, 8), sum(adRevenue) FROM uservisits_1node GROUP BY substring(sourceIP, 1, 8)
-
-SELECT
-    sourceIP,
-    sum(adRevenue) AS totalRevenue,
-    avg(pageRank) AS pageRank
-FROM rankings_1node ALL INNER JOIN
-(
-    SELECT
-        sourceIP,
-        destinationURL AS pageURL,
-        adRevenue
-    FROM uservisits_1node
-    WHERE (visitDate > '1980-01-01') AND (visitDate < '1980-04-01')
-) USING pageURL
-GROUP BY sourceIP
-ORDER BY totalRevenue DESC
-LIMIT 1
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/amplab_benchmark/) <!--hide-->
diff --git a/docs/es/getting-started/example-datasets/criteo.md b/docs/es/getting-started/example-datasets/criteo.md
deleted file mode 100644
index 79203b0276d8..000000000000
--- a/docs/es/getting-started/example-datasets/criteo.md
+++ /dev/null
@@ -1,81 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 19
-toc_title: Registros de clics de Terabyte de Criteo
----
-
-# Terabyte de registros de clics de Criteo {#terabyte-of-click-logs-from-criteo}
-
-Descargue los datos de http://labs.criteo.com/downloads/download-terabyte-click-logs/
-
-Cree una tabla para importar el registro:
-
-``` sql
-CREATE TABLE criteo_log (date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, cat1 String, cat2 String, cat3 String, cat4 String, cat5 String, cat6 String, cat7 String, cat8 String, cat9 String, cat10 String, cat11 String, cat12 String, cat13 String, cat14 String, cat15 String, cat16 String, cat17 String, cat18 String, cat19 String, cat20 String, cat21 String, cat22 String, cat23 String, cat24 String, cat25 String, cat26 String) ENGINE = Log
-```
-
-Descargar los datos:
-
-``` bash
-$ for i in {00..23}; do echo $i; zcat datasets/criteo/day_${i#0}.gz | sed -r 's/^/2000-01-'${i/00/24}'\t/' | clickhouse-client --host=example-perftest01j --query="INSERT INTO criteo_log FORMAT TabSeparated"; done
-```
-
-Crear una tabla para los datos convertidos:
-
-``` sql
-CREATE TABLE criteo
-(
-    date Date,
-    clicked UInt8,
-    int1 Int32,
-    int2 Int32,
-    int3 Int32,
-    int4 Int32,
-    int5 Int32,
-    int6 Int32,
-    int7 Int32,
-    int8 Int32,
-    int9 Int32,
-    int10 Int32,
-    int11 Int32,
-    int12 Int32,
-    int13 Int32,
-    icat1 UInt32,
-    icat2 UInt32,
-    icat3 UInt32,
-    icat4 UInt32,
-    icat5 UInt32,
-    icat6 UInt32,
-    icat7 UInt32,
-    icat8 UInt32,
-    icat9 UInt32,
-    icat10 UInt32,
-    icat11 UInt32,
-    icat12 UInt32,
-    icat13 UInt32,
-    icat14 UInt32,
-    icat15 UInt32,
-    icat16 UInt32,
-    icat17 UInt32,
-    icat18 UInt32,
-    icat19 UInt32,
-    icat20 UInt32,
-    icat21 UInt32,
-    icat22 UInt32,
-    icat23 UInt32,
-    icat24 UInt32,
-    icat25 UInt32,
-    icat26 UInt32
-) ENGINE = MergeTree(date, intHash32(icat1), (date, intHash32(icat1)), 8192)
-```
-
-Transforme los datos del registro sin procesar y colóquelos en la segunda tabla:
-
-``` sql
-INSERT INTO criteo SELECT date, clicked, int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, reinterpretAsUInt32(unhex(cat1)) AS icat1, reinterpretAsUInt32(unhex(cat2)) AS icat2, reinterpretAsUInt32(unhex(cat3)) AS icat3, reinterpretAsUInt32(unhex(cat4)) AS icat4, reinterpretAsUInt32(unhex(cat5)) AS icat5, reinterpretAsUInt32(unhex(cat6)) AS icat6, reinterpretAsUInt32(unhex(cat7)) AS icat7, reinterpretAsUInt32(unhex(cat8)) AS icat8, reinterpretAsUInt32(unhex(cat9)) AS icat9, reinterpretAsUInt32(unhex(cat10)) AS icat10, reinterpretAsUInt32(unhex(cat11)) AS icat11, reinterpretAsUInt32(unhex(cat12)) AS icat12, reinterpretAsUInt32(unhex(cat13)) AS icat13, reinterpretAsUInt32(unhex(cat14)) AS icat14, reinterpretAsUInt32(unhex(cat15)) AS icat15, reinterpretAsUInt32(unhex(cat16)) AS icat16, reinterpretAsUInt32(unhex(cat17)) AS icat17, reinterpretAsUInt32(unhex(cat18)) AS icat18, reinterpretAsUInt32(unhex(cat19)) AS icat19, reinterpretAsUInt32(unhex(cat20)) AS icat20, reinterpretAsUInt32(unhex(cat21)) AS icat21, reinterpretAsUInt32(unhex(cat22)) AS icat22, reinterpretAsUInt32(unhex(cat23)) AS icat23, reinterpretAsUInt32(unhex(cat24)) AS icat24, reinterpretAsUInt32(unhex(cat25)) AS icat25, reinterpretAsUInt32(unhex(cat26)) AS icat26 FROM criteo_log;
-
-DROP TABLE criteo_log;
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/criteo/) <!--hide-->
diff --git a/docs/es/getting-started/example-datasets/index.md b/docs/es/getting-started/example-datasets/index.md
deleted file mode 100644
index 28e06987af1c..000000000000
--- a/docs/es/getting-started/example-datasets/index.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Datos De Ejemplo
-toc_priority: 12
-toc_title: "Implantaci\xF3n"
----
-
-# Datos De Ejemplo {#example-datasets}
-
-En esta sección se describe cómo obtener conjuntos de datos de ejemplo e importarlos a ClickHouse.
-Para algunos conjuntos de datos también están disponibles consultas de ejemplo.
-
--   [Yandex anonimizado.Conjunto de datos de Metrica](metrica.md)
--   [Estrella Schema Benchmark](star-schema.md)
--   [Nombre de la red inalámbrica (SSID):](wikistat.md)
--   [Terabyte de registros de clics de Criteo](criteo.md)
--   [Referencia de Big Data de AMPLab](amplab-benchmark.md)
--   [Datos de taxis de Nueva York](nyc-taxi.md)
--   [A tiempo](ontime.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/example_datasets) <!--hide-->
diff --git a/docs/es/getting-started/example-datasets/metrica.md b/docs/es/getting-started/example-datasets/metrica.md
deleted file mode 100644
index 0b3bc8b68337..000000000000
--- a/docs/es/getting-started/example-datasets/metrica.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 14
-toc_title: El Yandex.Metrica Datos
----
-
-# Yandex anonimizado.Metrica Datos {#anonymized-yandex-metrica-data}
-
-El conjunto de datos consta de dos tablas que contienen datos anónimos sobre los hits (`hits_v1`) y visitas (`visits_v1`) el Yandex.Métrica. Puedes leer más sobre Yandex.Metrica en [Historial de ClickHouse](../../introduction/history.md) apartado.
-
-El conjunto de datos consta de dos tablas, cualquiera de ellas se puede descargar como `tsv.xz` o como particiones preparadas. Además, una versión extendida de la `hits` La tabla que contiene 100 millones de filas está disponible como TSV en https://datasets.clickhouse.tech/hits/tsv/hits_100m_obfuscated_v1.tsv.xz y como particiones preparadas en https://datasets.clickhouse.tech/hits/partitions/hits_100m_obfuscated_v1.tar.xz.
-
-## Obtención de tablas a partir de particiones preparadas {#obtaining-tables-from-prepared-partitions}
-
-Descargar e importar tabla de hits:
-
-``` bash
-curl -O https://datasets.clickhouse.tech/hits/partitions/hits_v1.tar
-tar xvf hits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-# check permissions on unpacked data, fix if required
-sudo service clickhouse-server restart
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.hits_v1"
-```
-
-Descargar e importar visitas:
-
-``` bash
-curl -O https://datasets.clickhouse.tech/visits/partitions/visits_v1.tar
-tar xvf visits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-# check permissions on unpacked data, fix if required
-sudo service clickhouse-server restart
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.visits_v1"
-```
-
-## Obtención de tablas a partir de un archivo TSV comprimido {#obtaining-tables-from-compressed-tsv-file}
-
-Descargar e importar hits desde un archivo TSV comprimido:
-
-``` bash
-curl https://datasets.clickhouse.tech/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` > hits_v1.tsv
-# now create table
-clickhouse-client --query "CREATE DATABASE IF NOT EXISTS datasets"
-clickhouse-client --query "CREATE TABLE datasets.hits_v1 ( WatchID UInt64,  JavaEnable UInt8,  Title String,  GoodEvent Int16,  EventTime DateTime,  EventDate Date,  CounterID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RegionID UInt32,  UserID UInt64,  CounterClass Int8,  OS UInt8,  UserAgent UInt8,  URL String,  Referer String,  URLDomain String,  RefererDomain String,  Refresh UInt8,  IsRobot UInt8,  RefererCategories Array(UInt16),  URLCategories Array(UInt16), URLRegions Array(UInt32),  RefererRegions Array(UInt32),  ResolutionWidth UInt16,  ResolutionHeight UInt16,  ResolutionDepth UInt8,  FlashMajor UInt8, FlashMinor UInt8,  FlashMinor2 String,  NetMajor UInt8,  NetMinor UInt8, UserAgentMajor UInt16,  UserAgentMinor FixedString(2),  CookieEnable UInt8, JavascriptEnable UInt8,  IsMobile UInt8,  MobilePhone UInt8,  MobilePhoneModel String,  Params String,  IPNetworkID UInt32,  TraficSourceID Int8, SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  IsArtifical UInt8,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  ClientTimeZone Int16,  ClientEventTime DateTime,  SilverlightVersion1 UInt8, SilverlightVersion2 UInt8,  SilverlightVersion3 UInt32,  SilverlightVersion4 UInt16,  PageCharset String,  CodeVersion UInt32,  IsLink UInt8,  IsDownload UInt8,  IsNotBounce UInt8,  FUniqID UInt64,  HID UInt32,  IsOldCounter UInt8, IsEvent UInt8,  IsParameter UInt8,  DontCountHits UInt8,  WithHash UInt8, HitColor FixedString(1),  UTCEventTime DateTime,  Age UInt8,  Sex UInt8,  Income UInt8,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16), RemoteIP UInt32,  RemoteIP6 FixedString(16),  WindowName Int32,  OpenerName Int32,  HistoryLength Int16,  BrowserLanguage FixedString(2),  BrowserCountry FixedString(2),  SocialNetwork String,  SocialAction String,  HTTPError UInt16, SendTiming Int32,  DNSTiming Int32,  ConnectTiming Int32,  ResponseStartTiming Int32,  ResponseEndTiming Int32,  FetchTiming Int32,  RedirectTiming Int32, DOMInteractiveTiming Int32,  DOMContentLoadedTiming Int32,  DOMCompleteTiming Int32,  LoadEventStartTiming Int32,  LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32,  FirstPaintTiming Int32,  RedirectCount Int8, SocialSourceNetworkID UInt8,  SocialSourcePage String,  ParamPrice Int64, ParamOrderID String,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16, GoalsReached Array(UInt32),  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String, UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String, FromTag String,  HasGCLID UInt8,  RefererHash UInt64,  URLHash UInt64,  CLID UInt32,  YCLID UInt64,  ShareService String,  ShareURL String,  ShareTitle String,  ParsedParams Nested(Key1 String,  Key2 String, Key3 String, Key4 String, Key5 String,  ValueDouble Float64),  IslandID FixedString(16),  RequestNum UInt32,  RequestTry UInt8) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192"
-# import data
-cat hits_v1.tsv | clickhouse-client --query "INSERT INTO datasets.hits_v1 FORMAT TSV" --max_insert_block_size=100000
-# optionally you can optimize table
-clickhouse-client --query "OPTIMIZE TABLE datasets.hits_v1 FINAL"
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.hits_v1"
-```
-
-Descargue e importe visitas desde un archivo tsv comprimido:
-
-``` bash
-curl https://datasets.clickhouse.tech/visits/tsv/visits_v1.tsv.xz | unxz --threads=`nproc` > visits_v1.tsv
-# now create table
-clickhouse-client --query "CREATE DATABASE IF NOT EXISTS datasets"
-clickhouse-client --query "CREATE TABLE datasets.visits_v1 ( CounterID UInt32,  StartDate Date,  Sign Int8,  IsNew UInt8,  VisitID UInt64,  UserID UInt64,  StartTime DateTime,  Duration UInt32,  UTCStartTime DateTime,  PageViews Int32,  Hits Int32,  IsBounce UInt8,  Referer String,  StartURL String,  RefererDomain String,  StartURLDomain String,  EndURL String,  LinkURL String,  IsDownload UInt8,  TraficSourceID Int8,  SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  PlaceID Int32,  RefererCategories Array(UInt16),  URLCategories Array(UInt16),  URLRegions Array(UInt32),  RefererRegions Array(UInt32),  IsYandex UInt8,  GoalReachesDepth Int32,  GoalReachesURL Int32,  GoalReachesAny Int32,  SocialSourceNetworkID UInt8,  SocialSourcePage String,  MobilePhoneModel String,  ClientEventTime DateTime,  RegionID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RemoteIP UInt32,  RemoteIP6 FixedString(16),  IPNetworkID UInt32,  SilverlightVersion3 UInt32,  CodeVersion UInt32,  ResolutionWidth UInt16,  ResolutionHeight UInt16,  UserAgentMajor UInt16,  UserAgentMinor UInt16,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  SilverlightVersion2 UInt8,  SilverlightVersion4 UInt16,  FlashVersion3 UInt16,  FlashVersion4 UInt16,  ClientTimeZone Int16,  OS UInt8,  UserAgent UInt8,  ResolutionDepth UInt8,  FlashMajor UInt8,  FlashMinor UInt8,  NetMajor UInt8,  NetMinor UInt8,  MobilePhone UInt8,  SilverlightVersion1 UInt8,  Age UInt8,  Sex UInt8,  Income UInt8,  JavaEnable UInt8,  CookieEnable UInt8,  JavascriptEnable UInt8,  IsMobile UInt8,  BrowserLanguage UInt16,  BrowserCountry UInt16,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16),  Params Array(String),  Goals Nested(ID UInt32, Serial UInt32, EventTime DateTime,  Price Int64,  OrderID String, CurrencyID UInt32),  WatchIDs Array(UInt64),  ParamSumPrice Int64,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16,  ClickLogID UInt64,  ClickEventID Int32,  ClickGoodEvent Int32,  ClickEventTime DateTime,  ClickPriorityID Int32,  ClickPhraseID Int32,  ClickPageID Int32,  ClickPlaceID Int32,  ClickTypeID Int32,  ClickResourceID Int32,  ClickCost UInt32,  ClickClientIP UInt32,  ClickDomainID UInt32,  ClickURL String,  ClickAttempt UInt8,  ClickOrderID UInt32,  ClickBannerID UInt32,  ClickMarketCategoryID UInt32,  ClickMarketPP UInt32,  ClickMarketCategoryName String,  ClickMarketPPName String,  ClickAWAPSCampaignName String,  ClickPageName String,  ClickTargetType UInt16,  ClickTargetPhraseID UInt64,  ClickContextType UInt8,  ClickSelectType Int8,  ClickOptions String,  ClickGroupBannerID Int32,  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String,  UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String,  FromTag String,  HasGCLID UInt8,  FirstVisit DateTime,  PredLastVisit Date,  LastVisit Date,  TotalVisits UInt32,  TraficSource    Nested(ID Int8,  SearchEngineID UInt16, AdvEngineID UInt8, PlaceID UInt16, SocialSourceNetworkID UInt8, Domain String, SearchPhrase String, SocialSourcePage String),  Attendance FixedString(16),  CLID UInt32,  YCLID UInt64,  NormalizedRefererHash UInt64,  SearchPhraseHash UInt64,  RefererDomainHash UInt64,  NormalizedStartURLHash UInt64,  StartURLDomainHash UInt64,  NormalizedEndURLHash UInt64,  TopLevelDomain UInt64,  URLScheme UInt64,  OpenstatServiceNameHash UInt64,  OpenstatCampaignIDHash UInt64,  OpenstatAdIDHash UInt64,  OpenstatSourceIDHash UInt64,  UTMSourceHash UInt64,  UTMMediumHash UInt64,  UTMCampaignHash UInt64,  UTMContentHash UInt64,  UTMTermHash UInt64,  FromHash UInt64,  WebVisorEnabled UInt8,  WebVisorActivity UInt32,  ParsedParams    Nested(Key1 String,  Key2 String,  Key3 String,  Key4 String, Key5 String, ValueDouble    Float64),  Market Nested(Type UInt8, GoalID UInt32, OrderID String,  OrderPrice Int64,  PP UInt32,  DirectPlaceID UInt32,  DirectOrderID  UInt32,  DirectBannerID UInt32,  GoodID String, GoodName String, GoodQuantity Int32,  GoodPrice Int64),  IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192"
-# import data
-cat visits_v1.tsv | clickhouse-client --query "INSERT INTO datasets.visits_v1 FORMAT TSV" --max_insert_block_size=100000
-# optionally you can optimize table
-clickhouse-client --query "OPTIMIZE TABLE datasets.visits_v1 FINAL"
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.visits_v1"
-```
-
-## Consultas de ejemplo {#example-queries}
-
-[Tutorial de ClickHouse](../../getting-started/tutorial.md) se basa en Yandex.El conjunto de datos de Metrica y la forma recomendada de comenzar con este conjunto de datos es simplemente pasar por el tutorial.
-
-Se pueden encontrar ejemplos adicionales de consultas a estas tablas entre [pruebas estatales](https://github.com/ClickHouse/ClickHouse/tree/master/tests/queries/1_stateful) de ClickHouse (se nombran `test.hists` y `test.visits` alli).
diff --git a/docs/es/getting-started/example-datasets/nyc-taxi.md b/docs/es/getting-started/example-datasets/nyc-taxi.md
deleted file mode 100644
index c6441311c968..000000000000
--- a/docs/es/getting-started/example-datasets/nyc-taxi.md
+++ /dev/null
@@ -1,390 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 16
-toc_title: Datos de taxis de Nueva York
----
-
-# Datos de taxis de Nueva York {#new-york-taxi-data}
-
-Este conjunto de datos se puede obtener de dos maneras:
-
--   importación de datos sin procesar
--   descarga de particiones preparadas
-
-## Cómo importar los datos sin procesar {#how-to-import-the-raw-data}
-
-Consulte https://github.com/toddwschneider/nyc-taxi-data y http://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html para obtener la descripción de un conjunto de datos e instrucciones para descargar.
-
-La descarga dará como resultado aproximadamente 227 GB de datos sin comprimir en archivos CSV. La descarga tarda aproximadamente una hora en una conexión de 1 Gbit (la descarga paralela de s3.amazonaws.com recupera al menos la mitad de un canal de 1 Gbit).
-Es posible que algunos de los archivos no se descarguen por completo. Verifique los tamaños de archivo y vuelva a descargar cualquiera que parezca dudoso.
-
-Algunos de los archivos pueden contener filas no válidas. Puede arreglarlos de la siguiente manera:
-
-``` bash
-sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-02.csv > data/yellow_tripdata_2010-02.csv_
-sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-03.csv > data/yellow_tripdata_2010-03.csv_
-mv data/yellow_tripdata_2010-02.csv_ data/yellow_tripdata_2010-02.csv
-mv data/yellow_tripdata_2010-03.csv_ data/yellow_tripdata_2010-03.csv
-```
-
-Entonces los datos deben ser preprocesados en PostgreSQL. Esto creará selecciones de puntos en los polígonos (para hacer coincidir los puntos en el mapa con los distritos de la ciudad de Nueva York) y combinará todos los datos en una única tabla plana desnormalizada mediante el uso de una unión. Para hacer esto, deberá instalar PostgreSQL con soporte PostGIS.
-
-Tenga cuidado al correr `initialize_database.sh` y volver a verificar manualmente que todas las tablas se crearon correctamente.
-
-Se tarda entre 20 y 30 minutos en procesar los datos de cada mes en PostgreSQL, por un total de aproximadamente 48 horas.
-
-Puede comprobar el número de filas descargadas de la siguiente manera:
-
-``` bash
-$ time psql nyc-taxi-data -c "SELECT count(*) FROM trips;"
-## Count
- 1298979494
-(1 row)
-
-real    7m9.164s
-```
-
-(Esto es un poco más de 1.1 mil millones de filas reportadas por Mark Litwintschik en una serie de publicaciones de blog.)
-
-Los datos en PostgreSQL utilizan 370 GB de espacio.
-
-Exportación de los datos de PostgreSQL:
-
-``` sql
-COPY
-(
-    SELECT trips.id,
-           trips.vendor_id,
-           trips.pickup_datetime,
-           trips.dropoff_datetime,
-           trips.store_and_fwd_flag,
-           trips.rate_code_id,
-           trips.pickup_longitude,
-           trips.pickup_latitude,
-           trips.dropoff_longitude,
-           trips.dropoff_latitude,
-           trips.passenger_count,
-           trips.trip_distance,
-           trips.fare_amount,
-           trips.extra,
-           trips.mta_tax,
-           trips.tip_amount,
-           trips.tolls_amount,
-           trips.ehail_fee,
-           trips.improvement_surcharge,
-           trips.total_amount,
-           trips.payment_type,
-           trips.trip_type,
-           trips.pickup,
-           trips.dropoff,
-
-           cab_types.type cab_type,
-
-           weather.precipitation_tenths_of_mm rain,
-           weather.snow_depth_mm,
-           weather.snowfall_mm,
-           weather.max_temperature_tenths_degrees_celsius max_temp,
-           weather.min_temperature_tenths_degrees_celsius min_temp,
-           weather.average_wind_speed_tenths_of_meters_per_second wind,
-
-           pick_up.gid pickup_nyct2010_gid,
-           pick_up.ctlabel pickup_ctlabel,
-           pick_up.borocode pickup_borocode,
-           pick_up.boroname pickup_boroname,
-           pick_up.ct2010 pickup_ct2010,
-           pick_up.boroct2010 pickup_boroct2010,
-           pick_up.cdeligibil pickup_cdeligibil,
-           pick_up.ntacode pickup_ntacode,
-           pick_up.ntaname pickup_ntaname,
-           pick_up.puma pickup_puma,
-
-           drop_off.gid dropoff_nyct2010_gid,
-           drop_off.ctlabel dropoff_ctlabel,
-           drop_off.borocode dropoff_borocode,
-           drop_off.boroname dropoff_boroname,
-           drop_off.ct2010 dropoff_ct2010,
-           drop_off.boroct2010 dropoff_boroct2010,
-           drop_off.cdeligibil dropoff_cdeligibil,
-           drop_off.ntacode dropoff_ntacode,
-           drop_off.ntaname dropoff_ntaname,
-           drop_off.puma dropoff_puma
-    FROM trips
-    LEFT JOIN cab_types
-        ON trips.cab_type_id = cab_types.id
-    LEFT JOIN central_park_weather_observations_raw weather
-        ON weather.date = trips.pickup_datetime::date
-    LEFT JOIN nyct2010 pick_up
-        ON pick_up.gid = trips.pickup_nyct2010_gid
-    LEFT JOIN nyct2010 drop_off
-        ON drop_off.gid = trips.dropoff_nyct2010_gid
-) TO '/opt/milovidov/nyc-taxi-data/trips.tsv';
-```
-
-La instantánea de datos se crea a una velocidad de aproximadamente 50 MB por segundo. Al crear la instantánea, PostgreSQL lee desde el disco a una velocidad de aproximadamente 28 MB por segundo.
-Esto toma alrededor de 5 horas. El archivo TSV resultante es 590612904969 bytes.
-
-Crear una tabla temporal en ClickHouse:
-
-``` sql
-CREATE TABLE trips
-(
-trip_id                 UInt32,
-vendor_id               String,
-pickup_datetime         DateTime,
-dropoff_datetime        Nullable(DateTime),
-store_and_fwd_flag      Nullable(FixedString(1)),
-rate_code_id            Nullable(UInt8),
-pickup_longitude        Nullable(Float64),
-pickup_latitude         Nullable(Float64),
-dropoff_longitude       Nullable(Float64),
-dropoff_latitude        Nullable(Float64),
-passenger_count         Nullable(UInt8),
-trip_distance           Nullable(Float64),
-fare_amount             Nullable(Float32),
-extra                   Nullable(Float32),
-mta_tax                 Nullable(Float32),
-tip_amount              Nullable(Float32),
-tolls_amount            Nullable(Float32),
-ehail_fee               Nullable(Float32),
-improvement_surcharge   Nullable(Float32),
-total_amount            Nullable(Float32),
-payment_type            Nullable(String),
-trip_type               Nullable(UInt8),
-pickup                  Nullable(String),
-dropoff                 Nullable(String),
-cab_type                Nullable(String),
-precipitation           Nullable(UInt8),
-snow_depth              Nullable(UInt8),
-snowfall                Nullable(UInt8),
-max_temperature         Nullable(UInt8),
-min_temperature         Nullable(UInt8),
-average_wind_speed      Nullable(UInt8),
-pickup_nyct2010_gid     Nullable(UInt8),
-pickup_ctlabel          Nullable(String),
-pickup_borocode         Nullable(UInt8),
-pickup_boroname         Nullable(String),
-pickup_ct2010           Nullable(String),
-pickup_boroct2010       Nullable(String),
-pickup_cdeligibil       Nullable(FixedString(1)),
-pickup_ntacode          Nullable(String),
-pickup_ntaname          Nullable(String),
-pickup_puma             Nullable(String),
-dropoff_nyct2010_gid    Nullable(UInt8),
-dropoff_ctlabel         Nullable(String),
-dropoff_borocode        Nullable(UInt8),
-dropoff_boroname        Nullable(String),
-dropoff_ct2010          Nullable(String),
-dropoff_boroct2010      Nullable(String),
-dropoff_cdeligibil      Nullable(String),
-dropoff_ntacode         Nullable(String),
-dropoff_ntaname         Nullable(String),
-dropoff_puma            Nullable(String)
-) ENGINE = Log;
-```
-
-Es necesario para convertir campos a tipos de datos más correctos y, si es posible, para eliminar NULL.
-
-``` bash
-$ time clickhouse-client --query="INSERT INTO trips FORMAT TabSeparated" < trips.tsv
-
-real    75m56.214s
-```
-
-Los datos se leen a una velocidad de 112-140 Mb / segundo.
-La carga de datos en una tabla de tipos de registro en una secuencia tardó 76 minutos.
-Los datos de esta tabla utilizan 142 GB.
-
-(Importar datos directamente desde Postgres también es posible usando `COPY ... TO PROGRAM`.)
-
-Unfortunately, all the fields associated with the weather (precipitation…average_wind_speed) were filled with NULL. Because of this, we will remove them from the final data set.
-
-Para empezar, crearemos una tabla en un único servidor. Posteriormente haremos la mesa distribuida.
-
-Crear y rellenar una tabla de resumen:
-
-``` sql
-CREATE TABLE trips_mergetree
-ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
-AS SELECT
-
-trip_id,
-CAST(vendor_id AS Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14)) AS vendor_id,
-toDate(pickup_datetime) AS pickup_date,
-ifNull(pickup_datetime, toDateTime(0)) AS pickup_datetime,
-toDate(dropoff_datetime) AS dropoff_date,
-ifNull(dropoff_datetime, toDateTime(0)) AS dropoff_datetime,
-assumeNotNull(store_and_fwd_flag) IN ('Y', '1', '2') AS store_and_fwd_flag,
-assumeNotNull(rate_code_id) AS rate_code_id,
-assumeNotNull(pickup_longitude) AS pickup_longitude,
-assumeNotNull(pickup_latitude) AS pickup_latitude,
-assumeNotNull(dropoff_longitude) AS dropoff_longitude,
-assumeNotNull(dropoff_latitude) AS dropoff_latitude,
-assumeNotNull(passenger_count) AS passenger_count,
-assumeNotNull(trip_distance) AS trip_distance,
-assumeNotNull(fare_amount) AS fare_amount,
-assumeNotNull(extra) AS extra,
-assumeNotNull(mta_tax) AS mta_tax,
-assumeNotNull(tip_amount) AS tip_amount,
-assumeNotNull(tolls_amount) AS tolls_amount,
-assumeNotNull(ehail_fee) AS ehail_fee,
-assumeNotNull(improvement_surcharge) AS improvement_surcharge,
-assumeNotNull(total_amount) AS total_amount,
-CAST((assumeNotNull(payment_type) AS pt) IN ('CSH', 'CASH', 'Cash', 'CAS', 'Cas', '1') ? 'CSH' : (pt IN ('CRD', 'Credit', 'Cre', 'CRE', 'CREDIT', '2') ? 'CRE' : (pt IN ('NOC', 'No Charge', 'No', '3') ? 'NOC' : (pt IN ('DIS', 'Dispute', 'Dis', '4') ? 'DIS' : 'UNK'))) AS Enum8('CSH' = 1, 'CRE' = 2, 'UNK' = 0, 'NOC' = 3, 'DIS' = 4)) AS payment_type_,
-assumeNotNull(trip_type) AS trip_type,
-ifNull(toFixedString(unhex(pickup), 25), toFixedString('', 25)) AS pickup,
-ifNull(toFixedString(unhex(dropoff), 25), toFixedString('', 25)) AS dropoff,
-CAST(assumeNotNull(cab_type) AS Enum8('yellow' = 1, 'green' = 2, 'uber' = 3)) AS cab_type,
-
-assumeNotNull(pickup_nyct2010_gid) AS pickup_nyct2010_gid,
-toFloat32(ifNull(pickup_ctlabel, '0')) AS pickup_ctlabel,
-assumeNotNull(pickup_borocode) AS pickup_borocode,
-CAST(assumeNotNull(pickup_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS pickup_boroname,
-toFixedString(ifNull(pickup_ct2010, '000000'), 6) AS pickup_ct2010,
-toFixedString(ifNull(pickup_boroct2010, '0000000'), 7) AS pickup_boroct2010,
-CAST(assumeNotNull(ifNull(pickup_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS pickup_cdeligibil,
-toFixedString(ifNull(pickup_ntacode, '0000'), 4) AS pickup_ntacode,
-
-CAST(assumeNotNull(pickup_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS pickup_ntaname,
-
-toUInt16(ifNull(pickup_puma, '0')) AS pickup_puma,
-
-assumeNotNull(dropoff_nyct2010_gid) AS dropoff_nyct2010_gid,
-toFloat32(ifNull(dropoff_ctlabel, '0')) AS dropoff_ctlabel,
-assumeNotNull(dropoff_borocode) AS dropoff_borocode,
-CAST(assumeNotNull(dropoff_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS dropoff_boroname,
-toFixedString(ifNull(dropoff_ct2010, '000000'), 6) AS dropoff_ct2010,
-toFixedString(ifNull(dropoff_boroct2010, '0000000'), 7) AS dropoff_boroct2010,
-CAST(assumeNotNull(ifNull(dropoff_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS dropoff_cdeligibil,
-toFixedString(ifNull(dropoff_ntacode, '0000'), 4) AS dropoff_ntacode,
-
-CAST(assumeNotNull(dropoff_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS dropoff_ntaname,
-
-toUInt16(ifNull(dropoff_puma, '0')) AS dropoff_puma
-
-FROM trips
-```
-
-Esto toma 3030 segundos a una velocidad de aproximadamente 428,000 filas por segundo.
-Para cargarlo más rápido, puede crear la tabla con el `Log` motor en lugar de `MergeTree`. En este caso, la descarga funciona más rápido que 200 segundos.
-
-La tabla utiliza 126 GB de espacio en disco.
-
-``` sql
-SELECT formatReadableSize(sum(bytes)) FROM system.parts WHERE table = 'trips_mergetree' AND active
-```
-
-``` text
-┌─formatReadableSize(sum(bytes))─┐
-│ 126.18 GiB                     │
-└────────────────────────────────┘
-```
-
-Entre otras cosas, puede ejecutar la consulta OPTIMIZE en MergeTree. Pero no es necesario ya que todo estará bien sin él.
-
-## Descarga de Prepared Partitions {#download-of-prepared-partitions}
-
-``` bash
-$ curl -O https://datasets.clickhouse.tech/trips_mergetree/partitions/trips_mergetree.tar
-$ tar xvf trips_mergetree.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-$ # check permissions of unpacked data, fix if required
-$ sudo service clickhouse-server restart
-$ clickhouse-client --query "select count(*) from datasets.trips_mergetree"
-```
-
-!!! info "INFO"
-    Si va a ejecutar las consultas que se describen a continuación, debe usar el nombre completo de la tabla, `datasets.trips_mergetree`.
-
-## Resultados en un solo servidor {#results-on-single-server}
-
-Q1:
-
-``` sql
-SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
-```
-
-0.490 segundos.
-
-Q2:
-
-``` sql
-SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenger_count
-```
-
-1.224 segundos.
-
-Q3:
-
-``` sql
-SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetree GROUP BY passenger_count, year
-```
-
-2.104 segundos.
-
-Q4:
-
-``` sql
-SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*)
-FROM trips_mergetree
-GROUP BY passenger_count, year, distance
-ORDER BY year, count(*) DESC
-```
-
-3.593 segundos.
-
-Se utilizó el siguiente servidor:
-
-Dos CPU Intel (R) Xeon (R) E5-2650 v2 @ 2.60GHz, 16 núcleos físicos en total, 128 GiB RAM, 8x6 TB HD en hardware RAID-5
-
-El tiempo de ejecución es el mejor de tres carreras. Pero a partir de la segunda ejecución, las consultas leen datos de la memoria caché del sistema de archivos. No se produce más almacenamiento en caché: los datos se leen y procesan en cada ejecución.
-
-Creación de una tabla en tres servidores:
-
-En cada servidor:
-
-``` sql
-CREATE TABLE default.trips_mergetree_third ( trip_id UInt32,  vendor_id Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14),  pickup_date Date,  pickup_datetime DateTime,  dropoff_date Date,  dropoff_datetime DateTime,  store_and_fwd_flag UInt8,  rate_code_id UInt8,  pickup_longitude Float64,  pickup_latitude Float64,  dropoff_longitude Float64,  dropoff_latitude Float64,  passenger_count UInt8,  trip_distance Float64,  fare_amount Float32,  extra Float32,  mta_tax Float32,  tip_amount Float32,  tolls_amount Float32,  ehail_fee Float32,  improvement_surcharge Float32,  total_amount Float32,  payment_type_ Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4),  trip_type UInt8,  pickup FixedString(25),  dropoff FixedString(25),  cab_type Enum8('yellow' = 1, 'green' = 2, 'uber' = 3),  pickup_nyct2010_gid UInt8,  pickup_ctlabel Float32,  pickup_borocode UInt8,  pickup_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  pickup_ct2010 FixedString(6),  pickup_boroct2010 FixedString(7),  pickup_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  pickup_ntacode FixedString(4),  pickup_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  pickup_puma UInt16,  dropoff_nyct2010_gid UInt8,  dropoff_ctlabel Float32,  dropoff_borocode UInt8,  dropoff_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  dropoff_ct2010 FixedString(6),  dropoff_boroct2010 FixedString(7),  dropoff_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  dropoff_ntacode FixedString(4),  dropoff_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  dropoff_puma UInt16) ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
-```
-
-En el servidor de origen:
-
-``` sql
-CREATE TABLE trips_mergetree_x3 AS trips_mergetree_third ENGINE = Distributed(perftest, default, trips_mergetree_third, rand())
-```
-
-La siguiente consulta redistribuye los datos:
-
-``` sql
-INSERT INTO trips_mergetree_x3 SELECT * FROM trips_mergetree
-```
-
-Esto tarda 2454 segundos.
-
-En tres servidores:
-
-Q1: 0.212 segundos.
-Q2: 0.438 segundos.
-Q3: 0.733 segundos.
-Q4: 1.241 segundos.
-
-No hay sorpresas aquí, ya que las consultas se escalan linealmente.
-
-También tenemos los resultados de un clúster de 140 servidores:
-
-Q1: 0,028 seg.
-Q2: 0,043 seg.
-Q3: 0,051 seg.
-Q4: 0,072 seg.
-
-En este caso, el tiempo de procesamiento de la consulta está determinado sobre todo por la latencia de la red.
-Ejecutamos consultas utilizando un cliente ubicado en un centro de datos de Yandex en Finlandia en un clúster en Rusia, que agregó aproximadamente 20 ms de latencia.
-
-## Resumen {#summary}
-
-| servidor | Q1    | Q2    | Q3    | Q4    |
-|----------|-------|-------|-------|-------|
-| 1        | 0.490 | 1.224 | 2.104 | 3.593 |
-| 3        | 0.212 | 0.438 | 0.733 | 1.241 |
-| 140      | 0.028 | 0.043 | 0.051 | 0.072 |
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/nyc_taxi/) <!--hide-->
diff --git a/docs/es/getting-started/example-datasets/ontime.md b/docs/es/getting-started/example-datasets/ontime.md
deleted file mode 100644
index f89d74048bd3..000000000000
--- a/docs/es/getting-started/example-datasets/ontime.md
+++ /dev/null
@@ -1,412 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 15
-toc_title: A tiempo
----
-
-# A tiempo {#ontime}
-
-Este conjunto de datos se puede obtener de dos maneras:
-
--   importación de datos sin procesar
--   descarga de particiones preparadas
-
-## Importar desde datos sin procesar {#import-from-raw-data}
-
-Descarga de datos:
-
-``` bash
-for s in `seq 1987 2018`
-do
-for m in `seq 1 12`
-do
-wget https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_${s}_${m}.zip
-done
-done
-```
-
-(a partir de https://github.com/Percona-Lab/ontime-airline-performance/blob/master/download.sh )
-
-Creación de una tabla:
-
-``` sql
-CREATE TABLE `ontime` (
-  `Year` UInt16,
-  `Quarter` UInt8,
-  `Month` UInt8,
-  `DayofMonth` UInt8,
-  `DayOfWeek` UInt8,
-  `FlightDate` Date,
-  `UniqueCarrier` FixedString(7),
-  `AirlineID` Int32,
-  `Carrier` FixedString(2),
-  `TailNum` String,
-  `FlightNum` String,
-  `OriginAirportID` Int32,
-  `OriginAirportSeqID` Int32,
-  `OriginCityMarketID` Int32,
-  `Origin` FixedString(5),
-  `OriginCityName` String,
-  `OriginState` FixedString(2),
-  `OriginStateFips` String,
-  `OriginStateName` String,
-  `OriginWac` Int32,
-  `DestAirportID` Int32,
-  `DestAirportSeqID` Int32,
-  `DestCityMarketID` Int32,
-  `Dest` FixedString(5),
-  `DestCityName` String,
-  `DestState` FixedString(2),
-  `DestStateFips` String,
-  `DestStateName` String,
-  `DestWac` Int32,
-  `CRSDepTime` Int32,
-  `DepTime` Int32,
-  `DepDelay` Int32,
-  `DepDelayMinutes` Int32,
-  `DepDel15` Int32,
-  `DepartureDelayGroups` String,
-  `DepTimeBlk` String,
-  `TaxiOut` Int32,
-  `WheelsOff` Int32,
-  `WheelsOn` Int32,
-  `TaxiIn` Int32,
-  `CRSArrTime` Int32,
-  `ArrTime` Int32,
-  `ArrDelay` Int32,
-  `ArrDelayMinutes` Int32,
-  `ArrDel15` Int32,
-  `ArrivalDelayGroups` Int32,
-  `ArrTimeBlk` String,
-  `Cancelled` UInt8,
-  `CancellationCode` FixedString(1),
-  `Diverted` UInt8,
-  `CRSElapsedTime` Int32,
-  `ActualElapsedTime` Int32,
-  `AirTime` Int32,
-  `Flights` Int32,
-  `Distance` Int32,
-  `DistanceGroup` UInt8,
-  `CarrierDelay` Int32,
-  `WeatherDelay` Int32,
-  `NASDelay` Int32,
-  `SecurityDelay` Int32,
-  `LateAircraftDelay` Int32,
-  `FirstDepTime` String,
-  `TotalAddGTime` String,
-  `LongestAddGTime` String,
-  `DivAirportLandings` String,
-  `DivReachedDest` String,
-  `DivActualElapsedTime` String,
-  `DivArrDelay` String,
-  `DivDistance` String,
-  `Div1Airport` String,
-  `Div1AirportID` Int32,
-  `Div1AirportSeqID` Int32,
-  `Div1WheelsOn` String,
-  `Div1TotalGTime` String,
-  `Div1LongestGTime` String,
-  `Div1WheelsOff` String,
-  `Div1TailNum` String,
-  `Div2Airport` String,
-  `Div2AirportID` Int32,
-  `Div2AirportSeqID` Int32,
-  `Div2WheelsOn` String,
-  `Div2TotalGTime` String,
-  `Div2LongestGTime` String,
-  `Div2WheelsOff` String,
-  `Div2TailNum` String,
-  `Div3Airport` String,
-  `Div3AirportID` Int32,
-  `Div3AirportSeqID` Int32,
-  `Div3WheelsOn` String,
-  `Div3TotalGTime` String,
-  `Div3LongestGTime` String,
-  `Div3WheelsOff` String,
-  `Div3TailNum` String,
-  `Div4Airport` String,
-  `Div4AirportID` Int32,
-  `Div4AirportSeqID` Int32,
-  `Div4WheelsOn` String,
-  `Div4TotalGTime` String,
-  `Div4LongestGTime` String,
-  `Div4WheelsOff` String,
-  `Div4TailNum` String,
-  `Div5Airport` String,
-  `Div5AirportID` Int32,
-  `Div5AirportSeqID` Int32,
-  `Div5WheelsOn` String,
-  `Div5TotalGTime` String,
-  `Div5LongestGTime` String,
-  `Div5WheelsOff` String,
-  `Div5TailNum` String
-) ENGINE = MergeTree
-PARTITION BY Year
-ORDER BY (Carrier, FlightDate)
-SETTINGS index_granularity = 8192;
-```
-
-Carga de datos:
-
-``` bash
-$ for i in *.zip; do echo $i; unzip -cq $i '*.csv' | sed 's/\.00//g' | clickhouse-client --host=example-perftest01j --query="INSERT INTO ontime FORMAT CSVWithNames"; done
-```
-
-## Descarga de Prepared Partitions {#download-of-prepared-partitions}
-
-``` bash
-$ curl -O https://datasets.clickhouse.tech/ontime/partitions/ontime.tar
-$ tar xvf ontime.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-$ # check permissions of unpacked data, fix if required
-$ sudo service clickhouse-server restart
-$ clickhouse-client --query "select count(*) from datasets.ontime"
-```
-
-!!! info "INFO"
-    Si va a ejecutar las consultas que se describen a continuación, debe usar el nombre completo de la tabla, `datasets.ontime`.
-
-## Consulta {#queries}
-
-Q0.
-
-``` sql
-SELECT avg(c1)
-FROM
-(
-    SELECT Year, Month, count(*) AS c1
-    FROM ontime
-    GROUP BY Year, Month
-);
-```
-
-Q1. El número de vuelos por día desde el año 2000 hasta 2008
-
-``` sql
-SELECT DayOfWeek, count(*) AS c
-FROM ontime
-WHERE Year>=2000 AND Year<=2008
-GROUP BY DayOfWeek
-ORDER BY c DESC;
-```
-
-Preguntas frecuentes El número de vuelos retrasados por más de 10 minutos, agrupados por el día de la semana, para 2000-2008
-
-``` sql
-SELECT DayOfWeek, count(*) AS c
-FROM ontime
-WHERE DepDelay>10 AND Year>=2000 AND Year<=2008
-GROUP BY DayOfWeek
-ORDER BY c DESC;
-```
-
-Q3. El número de retrasos por parte del aeropuerto para 2000-2008
-
-``` sql
-SELECT Origin, count(*) AS c
-FROM ontime
-WHERE DepDelay>10 AND Year>=2000 AND Year<=2008
-GROUP BY Origin
-ORDER BY c DESC
-LIMIT 10;
-```
-
-Preguntas más frecuentes Número de retrasos por transportista para 2007
-
-``` sql
-SELECT Carrier, count(*)
-FROM ontime
-WHERE DepDelay>10 AND Year=2007
-GROUP BY Carrier
-ORDER BY count(*) DESC;
-```
-
-Q5. El porcentaje de retrasos por transportista para 2007
-
-``` sql
-SELECT Carrier, c, c2, c*100/c2 as c3
-FROM
-(
-    SELECT
-        Carrier,
-        count(*) AS c
-    FROM ontime
-    WHERE DepDelay>10
-        AND Year=2007
-    GROUP BY Carrier
-)
-JOIN
-(
-    SELECT
-        Carrier,
-        count(*) AS c2
-    FROM ontime
-    WHERE Year=2007
-    GROUP BY Carrier
-) USING Carrier
-ORDER BY c3 DESC;
-```
-
-Mejor versión de la misma consulta:
-
-``` sql
-SELECT Carrier, avg(DepDelay>10)*100 AS c3
-FROM ontime
-WHERE Year=2007
-GROUP BY Carrier
-ORDER BY c3 DESC
-```
-
-¿Por qué? La solicitud anterior de una gama más amplia de años, 2000-2008
-
-``` sql
-SELECT Carrier, c, c2, c*100/c2 as c3
-FROM
-(
-    SELECT
-        Carrier,
-        count(*) AS c
-    FROM ontime
-    WHERE DepDelay>10
-        AND Year>=2000 AND Year<=2008
-    GROUP BY Carrier
-)
-JOIN
-(
-    SELECT
-        Carrier,
-        count(*) AS c2
-    FROM ontime
-    WHERE Year>=2000 AND Year<=2008
-    GROUP BY Carrier
-) USING Carrier
-ORDER BY c3 DESC;
-```
-
-Mejor versión de la misma consulta:
-
-``` sql
-SELECT Carrier, avg(DepDelay>10)*100 AS c3
-FROM ontime
-WHERE Year>=2000 AND Year<=2008
-GROUP BY Carrier
-ORDER BY c3 DESC;
-```
-
-Preguntas frecuentes Porcentaje de vuelos retrasados por más de 10 minutos, por año
-
-``` sql
-SELECT Year, c1/c2
-FROM
-(
-    select
-        Year,
-        count(*)*100 as c1
-    from ontime
-    WHERE DepDelay>10
-    GROUP BY Year
-)
-JOIN
-(
-    select
-        Year,
-        count(*) as c2
-    from ontime
-    GROUP BY Year
-) USING (Year)
-ORDER BY Year;
-```
-
-Mejor versión de la misma consulta:
-
-``` sql
-SELECT Year, avg(DepDelay>10)*100
-FROM ontime
-GROUP BY Year
-ORDER BY Year;
-```
-
-¿Por qué? Los destinos más populares por el número de ciudades conectadas directamente para varios rangos de año
-
-``` sql
-SELECT DestCityName, uniqExact(OriginCityName) AS u
-FROM ontime
-WHERE Year >= 2000 and Year <= 2010
-GROUP BY DestCityName
-ORDER BY u DESC LIMIT 10;
-```
-
-Q9.
-
-``` sql
-SELECT Year, count(*) AS c1
-FROM ontime
-GROUP BY Year;
-```
-
-Q10.
-
-``` sql
-SELECT
-   min(Year), max(Year), Carrier, count(*) AS cnt,
-   sum(ArrDelayMinutes>30) AS flights_delayed,
-   round(sum(ArrDelayMinutes>30)/count(*),2) AS rate
-FROM ontime
-WHERE
-   DayOfWeek NOT IN (6,7) AND OriginState NOT IN ('AK', 'HI', 'PR', 'VI')
-   AND DestState NOT IN ('AK', 'HI', 'PR', 'VI')
-   AND FlightDate < '2010-01-01'
-GROUP by Carrier
-HAVING cnt>100000 and max(Year)>1990
-ORDER by rate DESC
-LIMIT 1000;
-```
-
-Bono:
-
-``` sql
-SELECT avg(cnt)
-FROM
-(
-    SELECT Year,Month,count(*) AS cnt
-    FROM ontime
-    WHERE DepDel15=1
-    GROUP BY Year,Month
-);
-
-SELECT avg(c1) FROM
-(
-    SELECT Year,Month,count(*) AS c1
-    FROM ontime
-    GROUP BY Year,Month
-);
-
-SELECT DestCityName, uniqExact(OriginCityName) AS u
-FROM ontime
-GROUP BY DestCityName
-ORDER BY u DESC
-LIMIT 10;
-
-SELECT OriginCityName, DestCityName, count() AS c
-FROM ontime
-GROUP BY OriginCityName, DestCityName
-ORDER BY c DESC
-LIMIT 10;
-
-SELECT OriginCityName, count() AS c
-FROM ontime
-GROUP BY OriginCityName
-ORDER BY c DESC
-LIMIT 10;
-```
-
-Esta prueba de rendimiento fue creada por Vadim Tkachenko. Ver:
-
--   https://www.percona.com/blog/2009/10/02/analyzing-air-traffic-performance-with-infobright-and-monetdb/
--   https://www.percona.com/blog/2009/10/26/air-traffic-queries-in-luciddb/
--   https://www.percona.com/blog/2009/11/02/air-traffic-queries-in-infinidb-early-alpha/
--   https://www.percona.com/blog/2014/04/21/using-apache-hadoop-and-impala-together-with-mysql-for-data-analysis/
--   https://www.percona.com/blog/2016/01/07/apache-spark-with-air-ontime-performance-data/
--   http://nickmakos.blogspot.ru/2012/08/analyzing-air-traffic-performance-with.html
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/ontime/) <!--hide-->
diff --git a/docs/es/getting-started/example-datasets/star-schema.md b/docs/es/getting-started/example-datasets/star-schema.md
deleted file mode 100644
index 43f878eb205e..000000000000
--- a/docs/es/getting-started/example-datasets/star-schema.md
+++ /dev/null
@@ -1,370 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 20
-toc_title: Estrella Schema Benchmark
----
-
-# Estrella Schema Benchmark {#star-schema-benchmark}
-
-Compilación de dbgen:
-
-``` bash
-$ git clone git@github.com:vadimtk/ssb-dbgen.git
-$ cd ssb-dbgen
-$ make
-```
-
-Generación de datos:
-
-!!! warning "Atención"
-    Con `-s 100` dbgen genera 600 millones de filas (67 GB), mientras que `-s 1000` genera 6 mil millones de filas (lo que lleva mucho tiempo)
-
-``` bash
-$ ./dbgen -s 1000 -T c
-$ ./dbgen -s 1000 -T l
-$ ./dbgen -s 1000 -T p
-$ ./dbgen -s 1000 -T s
-$ ./dbgen -s 1000 -T d
-```
-
-Creación de tablas en ClickHouse:
-
-``` sql
-CREATE TABLE customer
-(
-        C_CUSTKEY       UInt32,
-        C_NAME          String,
-        C_ADDRESS       String,
-        C_CITY          LowCardinality(String),
-        C_NATION        LowCardinality(String),
-        C_REGION        LowCardinality(String),
-        C_PHONE         String,
-        C_MKTSEGMENT    LowCardinality(String)
-)
-ENGINE = MergeTree ORDER BY (C_CUSTKEY);
-
-CREATE TABLE lineorder
-(
-    LO_ORDERKEY             UInt32,
-    LO_LINENUMBER           UInt8,
-    LO_CUSTKEY              UInt32,
-    LO_PARTKEY              UInt32,
-    LO_SUPPKEY              UInt32,
-    LO_ORDERDATE            Date,
-    LO_ORDERPRIORITY        LowCardinality(String),
-    LO_SHIPPRIORITY         UInt8,
-    LO_QUANTITY             UInt8,
-    LO_EXTENDEDPRICE        UInt32,
-    LO_ORDTOTALPRICE        UInt32,
-    LO_DISCOUNT             UInt8,
-    LO_REVENUE              UInt32,
-    LO_SUPPLYCOST           UInt32,
-    LO_TAX                  UInt8,
-    LO_COMMITDATE           Date,
-    LO_SHIPMODE             LowCardinality(String)
-)
-ENGINE = MergeTree PARTITION BY toYear(LO_ORDERDATE) ORDER BY (LO_ORDERDATE, LO_ORDERKEY);
-
-CREATE TABLE part
-(
-        P_PARTKEY       UInt32,
-        P_NAME          String,
-        P_MFGR          LowCardinality(String),
-        P_CATEGORY      LowCardinality(String),
-        P_BRAND         LowCardinality(String),
-        P_COLOR         LowCardinality(String),
-        P_TYPE          LowCardinality(String),
-        P_SIZE          UInt8,
-        P_CONTAINER     LowCardinality(String)
-)
-ENGINE = MergeTree ORDER BY P_PARTKEY;
-
-CREATE TABLE supplier
-(
-        S_SUPPKEY       UInt32,
-        S_NAME          String,
-        S_ADDRESS       String,
-        S_CITY          LowCardinality(String),
-        S_NATION        LowCardinality(String),
-        S_REGION        LowCardinality(String),
-        S_PHONE         String
-)
-ENGINE = MergeTree ORDER BY S_SUPPKEY;
-```
-
-Insertar datos:
-
-``` bash
-$ clickhouse-client --query "INSERT INTO customer FORMAT CSV" < customer.tbl
-$ clickhouse-client --query "INSERT INTO part FORMAT CSV" < part.tbl
-$ clickhouse-client --query "INSERT INTO supplier FORMAT CSV" < supplier.tbl
-$ clickhouse-client --query "INSERT INTO lineorder FORMAT CSV" < lineorder.tbl
-```
-
-Conversión “star schema” a desnormalizado “flat schema”:
-
-``` sql
-SET max_memory_usage = 20000000000;
-
-CREATE TABLE lineorder_flat
-ENGINE = MergeTree
-PARTITION BY toYear(LO_ORDERDATE)
-ORDER BY (LO_ORDERDATE, LO_ORDERKEY) AS
-SELECT
-    l.LO_ORDERKEY AS LO_ORDERKEY,
-    l.LO_LINENUMBER AS LO_LINENUMBER,
-    l.LO_CUSTKEY AS LO_CUSTKEY,
-    l.LO_PARTKEY AS LO_PARTKEY,
-    l.LO_SUPPKEY AS LO_SUPPKEY,
-    l.LO_ORDERDATE AS LO_ORDERDATE,
-    l.LO_ORDERPRIORITY AS LO_ORDERPRIORITY,
-    l.LO_SHIPPRIORITY AS LO_SHIPPRIORITY,
-    l.LO_QUANTITY AS LO_QUANTITY,
-    l.LO_EXTENDEDPRICE AS LO_EXTENDEDPRICE,
-    l.LO_ORDTOTALPRICE AS LO_ORDTOTALPRICE,
-    l.LO_DISCOUNT AS LO_DISCOUNT,
-    l.LO_REVENUE AS LO_REVENUE,
-    l.LO_SUPPLYCOST AS LO_SUPPLYCOST,
-    l.LO_TAX AS LO_TAX,
-    l.LO_COMMITDATE AS LO_COMMITDATE,
-    l.LO_SHIPMODE AS LO_SHIPMODE,
-    c.C_NAME AS C_NAME,
-    c.C_ADDRESS AS C_ADDRESS,
-    c.C_CITY AS C_CITY,
-    c.C_NATION AS C_NATION,
-    c.C_REGION AS C_REGION,
-    c.C_PHONE AS C_PHONE,
-    c.C_MKTSEGMENT AS C_MKTSEGMENT,
-    s.S_NAME AS S_NAME,
-    s.S_ADDRESS AS S_ADDRESS,
-    s.S_CITY AS S_CITY,
-    s.S_NATION AS S_NATION,
-    s.S_REGION AS S_REGION,
-    s.S_PHONE AS S_PHONE,
-    p.P_NAME AS P_NAME,
-    p.P_MFGR AS P_MFGR,
-    p.P_CATEGORY AS P_CATEGORY,
-    p.P_BRAND AS P_BRAND,
-    p.P_COLOR AS P_COLOR,
-    p.P_TYPE AS P_TYPE,
-    p.P_SIZE AS P_SIZE,
-    p.P_CONTAINER AS P_CONTAINER
-FROM lineorder AS l
-INNER JOIN customer AS c ON c.C_CUSTKEY = l.LO_CUSTKEY
-INNER JOIN supplier AS s ON s.S_SUPPKEY = l.LO_SUPPKEY
-INNER JOIN part AS p ON p.P_PARTKEY = l.LO_PARTKEY;
-```
-
-Las consultas:
-
-Q1.1
-
-``` sql
-SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue
-FROM lineorder_flat
-WHERE toYear(LO_ORDERDATE) = 1993 AND LO_DISCOUNT BETWEEN 1 AND 3 AND LO_QUANTITY < 25;
-```
-
-Q1.2
-
-``` sql
-SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue
-FROM lineorder_flat
-WHERE toYYYYMM(LO_ORDERDATE) = 199401 AND LO_DISCOUNT BETWEEN 4 AND 6 AND LO_QUANTITY BETWEEN 26 AND 35;
-```
-
-Q1.3
-
-``` sql
-SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue
-FROM lineorder_flat
-WHERE toISOWeek(LO_ORDERDATE) = 6 AND toYear(LO_ORDERDATE) = 1994
-  AND LO_DISCOUNT BETWEEN 5 AND 7 AND LO_QUANTITY BETWEEN 26 AND 35;
-```
-
-Q2.1
-
-``` sql
-SELECT
-    sum(LO_REVENUE),
-    toYear(LO_ORDERDATE) AS year,
-    P_BRAND
-FROM lineorder_flat
-WHERE P_CATEGORY = 'MFGR#12' AND S_REGION = 'AMERICA'
-GROUP BY
-    year,
-    P_BRAND
-ORDER BY
-    year,
-    P_BRAND;
-```
-
-Q2.2
-
-``` sql
-SELECT
-    sum(LO_REVENUE),
-    toYear(LO_ORDERDATE) AS year,
-    P_BRAND
-FROM lineorder_flat
-WHERE P_BRAND >= 'MFGR#2221' AND P_BRAND <= 'MFGR#2228' AND S_REGION = 'ASIA'
-GROUP BY
-    year,
-    P_BRAND
-ORDER BY
-    year,
-    P_BRAND;
-```
-
-Q2.3
-
-``` sql
-SELECT
-    sum(LO_REVENUE),
-    toYear(LO_ORDERDATE) AS year,
-    P_BRAND
-FROM lineorder_flat
-WHERE P_BRAND = 'MFGR#2239' AND S_REGION = 'EUROPE'
-GROUP BY
-    year,
-    P_BRAND
-ORDER BY
-    year,
-    P_BRAND;
-```
-
-Q3.1
-
-``` sql
-SELECT
-    C_NATION,
-    S_NATION,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE C_REGION = 'ASIA' AND S_REGION = 'ASIA' AND year >= 1992 AND year <= 1997
-GROUP BY
-    C_NATION,
-    S_NATION,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q3.2
-
-``` sql
-SELECT
-    C_CITY,
-    S_CITY,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE C_NATION = 'UNITED STATES' AND S_NATION = 'UNITED STATES' AND year >= 1992 AND year <= 1997
-GROUP BY
-    C_CITY,
-    S_CITY,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q3.3
-
-``` sql
-SELECT
-    C_CITY,
-    S_CITY,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND year >= 1992 AND year <= 1997
-GROUP BY
-    C_CITY,
-    S_CITY,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q3.4
-
-``` sql
-SELECT
-    C_CITY,
-    S_CITY,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND toYYYYMM(LO_ORDERDATE) = 199712
-GROUP BY
-    C_CITY,
-    S_CITY,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q4.1
-
-``` sql
-SELECT
-    toYear(LO_ORDERDATE) AS year,
-    C_NATION,
-    sum(LO_REVENUE - LO_SUPPLYCOST) AS profit
-FROM lineorder_flat
-WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2')
-GROUP BY
-    year,
-    C_NATION
-ORDER BY
-    year ASC,
-    C_NATION ASC;
-```
-
-Q4.2
-
-``` sql
-SELECT
-    toYear(LO_ORDERDATE) AS year,
-    S_NATION,
-    P_CATEGORY,
-    sum(LO_REVENUE - LO_SUPPLYCOST) AS profit
-FROM lineorder_flat
-WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (year = 1997 OR year = 1998) AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2')
-GROUP BY
-    year,
-    S_NATION,
-    P_CATEGORY
-ORDER BY
-    year ASC,
-    S_NATION ASC,
-    P_CATEGORY ASC;
-```
-
-Q4.3
-
-``` sql
-SELECT
-    toYear(LO_ORDERDATE) AS year,
-    S_CITY,
-    P_BRAND,
-    sum(LO_REVENUE - LO_SUPPLYCOST) AS profit
-FROM lineorder_flat
-WHERE S_NATION = 'UNITED STATES' AND (year = 1997 OR year = 1998) AND P_CATEGORY = 'MFGR#14'
-GROUP BY
-    year,
-    S_CITY,
-    P_BRAND
-ORDER BY
-    year ASC,
-    S_CITY ASC,
-    P_BRAND ASC;
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/star_schema/) <!--hide-->
diff --git a/docs/es/getting-started/example-datasets/wikistat.md b/docs/es/getting-started/example-datasets/wikistat.md
deleted file mode 100644
index 49d7263cdec6..000000000000
--- a/docs/es/getting-started/example-datasets/wikistat.md
+++ /dev/null
@@ -1,35 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 18
-toc_title: "Nombre de la red inal\xE1mbrica (SSID):"
----
-
-# Nombre de la red inalámbrica (SSID): {#wikistat}
-
-Ver: http://dumps.wikimedia.org/other/pagecounts-raw/
-
-Creación de una tabla:
-
-``` sql
-CREATE TABLE wikistat
-(
-    date Date,
-    time DateTime,
-    project String,
-    subproject String,
-    path String,
-    hits UInt64,
-    size UInt64
-) ENGINE = MergeTree(date, (path, time), 8192);
-```
-
-Carga de datos:
-
-``` bash
-$ for i in {2007..2016}; do for j in {01..12}; do echo $i-$j >&2; curl -sSL "http://dumps.wikimedia.org/other/pagecounts-raw/$i/$i-$j/" | grep -oE 'pagecounts-[0-9]+-[0-9]+\.gz'; done; done | sort | uniq | tee links.txt
-$ cat links.txt | while read link; do wget http://dumps.wikimedia.org/other/pagecounts-raw/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1/')/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1-\2/')/$link; done
-$ ls -1 /opt/wikistat/ | grep gz | while read i; do echo $i; gzip -cd /opt/wikistat/$i | ./wikistat-loader --time="$(echo -n $i | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})([0-9]{2})-([0-9]{2})([0-9]{2})([0-9]{2})\.gz/\1-\2-\3 \4-00-00/')" | clickhouse-client --query="INSERT INTO wikistat FORMAT TabSeparated"; done
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/wikistat/) <!--hide-->
diff --git a/docs/es/getting-started/index.md b/docs/es/getting-started/index.md
deleted file mode 100644
index 681c2017ac1d..000000000000
--- a/docs/es/getting-started/index.md
+++ /dev/null
@@ -1,17 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Primeros pasos
-toc_hidden: true
-toc_priority: 8
-toc_title: oculto
----
-
-# Primeros pasos {#getting-started}
-
-Si eres nuevo en ClickHouse y quieres tener una sensación práctica de su rendimiento, antes que nada, debes pasar por el [proceso de instalación](install.md). Después de eso puedes:
-
--   [Ir a través de tutorial detallado](tutorial.md)
--   [Experimente con conjuntos de datos de ejemplo](example-datasets/ontime.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/) <!--hide-->
diff --git a/docs/es/getting-started/install.md b/docs/es/getting-started/install.md
deleted file mode 100644
index 092ef47b2f74..000000000000
--- a/docs/es/getting-started/install.md
+++ /dev/null
@@ -1,182 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 11
-toc_title: "Instalaci\xF3n"
----
-
-# Instalación {#installation}
-
-## Requisitos del sistema {#system-requirements}
-
-ClickHouse puede ejecutarse en cualquier Linux, FreeBSD o Mac OS X con arquitectura de CPU x86_64, AArch64 o PowerPC64LE.
-
-Los binarios oficiales preconstruidos generalmente se compilan para x86_64 y aprovechan el conjunto de instrucciones SSE 4.2, por lo que, a menos que se indique lo contrario, el uso de la CPU que lo admite se convierte en un requisito adicional del sistema. Aquí está el comando para verificar si la CPU actual tiene soporte para SSE 4.2:
-
-``` bash
-$ grep -q sse4_2 /proc/cpuinfo && echo "SSE 4.2 supported" || echo "SSE 4.2 not supported"
-```
-
-Para ejecutar ClickHouse en procesadores que no admiten SSE 4.2 o tienen arquitectura AArch64 o PowerPC64LE, debe [construir ClickHouse a partir de fuentes](#from-sources) con los ajustes de configuración adecuados.
-
-## Opciones de instalación disponibles {#available-installation-options}
-
-### De paquetes DEB {#install-from-deb-packages}
-
-Se recomienda utilizar pre-compilado oficial `deb` Paquetes para Debian o Ubuntu. Ejecute estos comandos para instalar paquetes:
-
-``` bash
-{% include 'install/deb.sh' %}
-```
-
-Si desea utilizar la versión más reciente, reemplace `stable` con `testing` (esto se recomienda para sus entornos de prueba).
-
-También puede descargar e instalar paquetes manualmente desde [aqui](https://repo.clickhouse.tech/deb/stable/main/).
-
-#### Paquete {#packages}
-
--   `clickhouse-common-static` — Installs ClickHouse compiled binary files.
--   `clickhouse-server` — Creates a symbolic link for `clickhouse-server` e instala la configuración predeterminada del servidor.
--   `clickhouse-client` — Creates a symbolic link for `clickhouse-client` y otras herramientas relacionadas con el cliente. e instala los archivos de configuración del cliente.
--   `clickhouse-common-static-dbg` — Installs ClickHouse compiled binary files with debug info.
-
-### De paquetes RPM {#from-rpm-packages}
-
-Se recomienda utilizar pre-compilado oficial `rpm` También puede utilizar los paquetes para CentOS, RedHat y todas las demás distribuciones de Linux basadas en rpm.
-
-Primero, necesitas agregar el repositorio oficial:
-
-``` bash
-sudo yum install yum-utils
-sudo rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG
-sudo yum-config-manager --add-repo https://repo.clickhouse.tech/rpm/stable/x86_64
-```
-
-Si desea utilizar la versión más reciente, reemplace `stable` con `testing` (esto se recomienda para sus entornos de prueba). El `prestable` etiqueta a veces está disponible también.
-
-A continuación, ejecute estos comandos para instalar paquetes:
-
-``` bash
-sudo yum install clickhouse-server clickhouse-client
-```
-
-También puede descargar e instalar paquetes manualmente desde [aqui](https://repo.clickhouse.tech/rpm/stable/x86_64).
-
-### De archivos Tgz {#from-tgz-archives}
-
-Se recomienda utilizar pre-compilado oficial `tgz` para todas las distribuciones de Linux, donde la instalación de `deb` o `rpm` paquetes no es posible.
-
-La versión requerida se puede descargar con `curl` o `wget` desde el repositorio https://repo.clickhouse.tech/tgz/.
-Después de eso, los archivos descargados deben desempaquetarse e instalarse con scripts de instalación. Ejemplo para la última versión:
-
-``` bash
-export LATEST_VERSION=`curl https://api.github.com/repos/ClickHouse/ClickHouse/tags 2>/dev/null | grep -Eo '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+' | head -n 1`
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-common-static-$LATEST_VERSION.tgz
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-common-static-dbg-$LATEST_VERSION.tgz
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-server-$LATEST_VERSION.tgz
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-client-$LATEST_VERSION.tgz
-
-tar -xzvf clickhouse-common-static-$LATEST_VERSION.tgz
-sudo clickhouse-common-static-$LATEST_VERSION/install/doinst.sh
-
-tar -xzvf clickhouse-common-static-dbg-$LATEST_VERSION.tgz
-sudo clickhouse-common-static-dbg-$LATEST_VERSION/install/doinst.sh
-
-tar -xzvf clickhouse-server-$LATEST_VERSION.tgz
-sudo clickhouse-server-$LATEST_VERSION/install/doinst.sh
-sudo /etc/init.d/clickhouse-server start
-
-tar -xzvf clickhouse-client-$LATEST_VERSION.tgz
-sudo clickhouse-client-$LATEST_VERSION/install/doinst.sh
-```
-
-Para los entornos de producción, se recomienda utilizar las últimas `stable`-versión. Puede encontrar su número en la página de GitHub https://github.com/ClickHouse/ClickHouse/tags con postfix `-stable`.
-
-### Desde Docker Image {#from-docker-image}
-
-Para ejecutar ClickHouse dentro de Docker, siga la guía en [Eje de acoplador](https://hub.docker.com/r/yandex/clickhouse-server/). Esas imágenes usan oficial `deb` paquetes dentro.
-
-### De fuentes {#from-sources}
-
-Para compilar manualmente ClickHouse, siga las instrucciones para [Linux](../development/build.md) o [Mac OS X](../development/build-osx.md).
-
-Puede compilar paquetes e instalarlos o usar programas sin instalar paquetes. Además, al construir manualmente, puede deshabilitar el requisito de SSE 4.2 o compilar para CPU AArch64.
-
-      Client: programs/clickhouse-client
-      Server: programs/clickhouse-server
-
-Tendrá que crear carpetas de datos y metadatos y `chown` para el usuario deseado. Sus rutas se pueden cambiar en la configuración del servidor (src/programs/server/config.xml), por defecto son:
-
-      /opt/clickhouse/data/default/
-      /opt/clickhouse/metadata/default/
-
-En Gentoo, puedes usar `emerge clickhouse` para instalar ClickHouse desde fuentes.
-
-## Lanzar {#launch}
-
-Para iniciar el servidor como demonio, ejecute:
-
-``` bash
-$ sudo service clickhouse-server start
-```
-
-Si no tienes `service` comando ejecutar como
-
-``` bash
-$ sudo /etc/init.d/clickhouse-server start
-```
-
-Vea los registros en el `/var/log/clickhouse-server/` directorio.
-
-Si el servidor no se inicia, compruebe las configuraciones en el archivo `/etc/clickhouse-server/config.xml`.
-
-También puede iniciar manualmente el servidor desde la consola:
-
-``` bash
-$ clickhouse-server --config-file=/etc/clickhouse-server/config.xml
-```
-
-En este caso, el registro se imprimirá en la consola, lo cual es conveniente durante el desarrollo.
-Si el archivo de configuración está en el directorio actual, no es necesario `--config-file` parámetro. De forma predeterminada, utiliza `./config.xml`.
-
-ClickHouse admite la configuración de restricción de acceso. Están ubicados en el `users.xml` archivo (junto a `config.xml`).
-De forma predeterminada, se permite el acceso desde cualquier lugar `default` usuario, sin una contraseña. Ver `user/default/networks`.
-Para obtener más información, consulte la sección [“Configuration Files”](../operations/configuration-files.md).
-
-Después de iniciar el servidor, puede usar el cliente de línea de comandos para conectarse a él:
-
-``` bash
-$ clickhouse-client
-```
-
-Por defecto, se conecta a `localhost:9000` en nombre del usuario `default` sin una contraseña. También se puede usar para conectarse a un servidor remoto usando `--host` argumento.
-
-El terminal debe usar codificación UTF-8.
-Para obtener más información, consulte la sección [“Command-line client”](../interfaces/cli.md).
-
-Ejemplo:
-
-``` bash
-$ ./clickhouse-client
-ClickHouse client version 0.0.18749.
-Connecting to localhost:9000.
-Connected to ClickHouse server version 0.0.18749.
-
-:) SELECT 1
-
-SELECT 1
-
-┌─1─┐
-│ 1 │
-└───┘
-
-1 rows in set. Elapsed: 0.003 sec.
-
-:)
-```
-
-**Felicidades, el sistema funciona!**
-
-Para continuar experimentando, puede descargar uno de los conjuntos de datos de prueba o pasar por [tutorial](https://clickhouse.tech/tutorial.html).
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/install/) <!--hide-->
diff --git a/docs/es/getting-started/playground.md b/docs/es/getting-started/playground.md
deleted file mode 100644
index 1ab7246e2d4d..000000000000
--- a/docs/es/getting-started/playground.md
+++ /dev/null
@@ -1,48 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 14
-toc_title: Infantil
----
-
-# Zona de juegos ClickHouse {#clickhouse-playground}
-
-[Zona de juegos ClickHouse](https://play.clickhouse.tech?file=welcome) permite a las personas experimentar con ClickHouse ejecutando consultas al instante, sin configurar su servidor o clúster.
-Varios conjuntos de datos de ejemplo están disponibles en Playground, así como consultas de ejemplo que muestran las características de ClickHouse.
-
-Las consultas se ejecutan como un usuario de sólo lectura. Implica algunas limitaciones:
-
--   No se permiten consultas DDL
--   Las consultas INSERT no están permitidas
-
-También se aplican los siguientes valores:
-- [`max_result_bytes=10485760`](../operations/settings/query_complexity/#max-result-bytes)
-- [`max_result_rows=2000`](../operations/settings/query_complexity/#setting-max_result_rows)
-- [`result_overflow_mode=break`](../operations/settings/query_complexity/#result-overflow-mode)
-- [`max_execution_time=60000`](../operations/settings/query_complexity/#max-execution-time)
-
-ClickHouse Playground da la experiencia de m2.pequeño
-[Servicio administrado para ClickHouse](https://cloud.yandex.com/services/managed-clickhouse)
-instancia alojada en [El Yandex.Nube](https://cloud.yandex.com/).
-Más información sobre [proveedores de la nube](../commercial/cloud.md).
-
-La interfaz web de ClickHouse Playground realiza solicitudes a través de ClickHouse [HTTP API](../interfaces/http.md).
-El backend Playground es solo un clúster ClickHouse sin ninguna aplicación adicional del lado del servidor.
-El punto final HTTPS de ClickHouse también está disponible como parte de Playground.
-
-Puede realizar consultas al patio de recreo utilizando cualquier cliente HTTP, por ejemplo [rizo](https://curl.haxx.se) o [wget](https://www.gnu.org/software/wget/), o configurar una conexión usando [JDBC](../interfaces/jdbc.md) o [ODBC](../interfaces/odbc.md) controlador.
-Más información sobre los productos de software compatibles con ClickHouse está disponible [aqui](../interfaces/index.md).
-
-| Parámetro   | Valor                                         |
-|:------------|:----------------------------------------------|
-| Punto final | https://play-api.casa de clic.tecnología:8443 |
-| Usuario     | `playground`                                  |
-| Contraseña  | `clickhouse`                                  |
-
-Tenga en cuenta que este extremo requiere una conexión segura.
-
-Ejemplo:
-
-``` bash
-curl "https://play-api.clickhouse.tech:8443/?query=SELECT+'Play+ClickHouse!';&user=playground&password=clickhouse&database=datasets"
-```
diff --git a/docs/es/getting-started/tutorial.md b/docs/es/getting-started/tutorial.md
deleted file mode 100644
index 2cc9339f954d..000000000000
--- a/docs/es/getting-started/tutorial.md
+++ /dev/null
@@ -1,664 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 12
-toc_title: Tutorial
----
-
-# Tutorial de ClickHouse {#clickhouse-tutorial}
-
-## Qué Esperar de Este Tutorial? {#what-to-expect-from-this-tutorial}
-
-Al pasar por este tutorial, aprenderá cómo configurar un clúster de ClickHouse simple. Será pequeño, pero tolerante a fallos y escalable. Luego usaremos uno de los conjuntos de datos de ejemplo para llenarlo con datos y ejecutar algunas consultas de demostración.
-
-## Configuración de nodo único {#single-node-setup}
-
-Para posponer las complejidades de un entorno distribuido, comenzaremos con la implementación de ClickHouse en un único servidor o máquina virtual. ClickHouse generalmente se instala desde [deb](install.md#install-from-deb-packages) o [RPM](install.md#from-rpm-packages) paquetes, pero hay [alternativa](install.md#from-docker-image) para los sistemas operativos que no los admiten.
-
-Por ejemplo, ha elegido `deb` paquetes y ejecutado:
-
-``` bash
-{% include 'install/deb.sh' %}
-```
-
-¿Qué tenemos en los paquetes que tengo instalados:
-
--   `clickhouse-client` el paquete contiene [Casa de clics-cliente](../interfaces/cli.md) aplicación, cliente interactivo de la consola ClickHouse.
--   `clickhouse-common` El paquete contiene un archivo ejecutable ClickHouse.
--   `clickhouse-server` El paquete contiene archivos de configuración para ejecutar ClickHouse como servidor.
-
-Los archivos de configuración del servidor se encuentran en `/etc/clickhouse-server/`. Antes de ir más lejos, tenga en cuenta el `<path>` elemento en `config.xml`. La ruta determina la ubicación para el almacenamiento de datos, por lo que debe ubicarse en un volumen con gran capacidad de disco; el valor predeterminado es `/var/lib/clickhouse/`. Si desea ajustar la configuración, no es útil editar directamente `config.xml` archivo, teniendo en cuenta que podría ser reescrito en futuras actualizaciones de paquetes. La forma recomendada de anular los elementos de configuración es crear [archivos en config.directorio d](../operations/configuration-files.md) que sirven como “patches” de configuración.XML.
-
-Como habrás notado, `clickhouse-server` no se inicia automáticamente después de la instalación del paquete. Tampoco se reiniciará automáticamente después de las actualizaciones. La forma en que inicia el servidor depende de su sistema de inicio, por lo general, es:
-
-``` bash
-sudo service clickhouse-server start
-```
-
-o
-
-``` bash
-sudo /etc/init.d/clickhouse-server start
-```
-
-La ubicación predeterminada para los registros del servidor es `/var/log/clickhouse-server/`. El servidor está listo para manejar las conexiones de cliente una vez que registra el `Ready for connections` mensaje.
-
-Una vez que el `clickhouse-server` está en funcionamiento, podemos usar `clickhouse-client` para conectarse al servidor y ejecutar algunas consultas de prueba como `SELECT "Hello, world!";`.
-
-<details markdown="1">
-
-<summary>Consejos rápidos para clickhouse-cliente</summary>
-
-Modo interactivo:
-
-``` bash
-clickhouse-client
-clickhouse-client --host=... --port=... --user=... --password=...
-```
-
-Habilitar consultas multilínea:
-
-``` bash
-clickhouse-client -m
-clickhouse-client --multiline
-```
-
-Ejecutar consultas en modo por lotes:
-
-``` bash
-clickhouse-client --query='SELECT 1'
-echo 'SELECT 1' | clickhouse-client
-clickhouse-client <<< 'SELECT 1'
-```
-
-Insertar datos de un archivo en el formato especificado:
-
-``` bash
-clickhouse-client --query='INSERT INTO table VALUES' < data.txt
-clickhouse-client --query='INSERT INTO table FORMAT TabSeparated' < data.tsv
-```
-
-</details>
-
-## Importar conjunto de datos de muestra {#import-sample-dataset}
-
-Ahora es el momento de llenar nuestro servidor ClickHouse con algunos datos de muestra. En este tutorial, usaremos los datos anónimos de Yandex.Metrica, el primer servicio que ejecuta ClickHouse en forma de producción antes de que se convirtiera en código abierto (más sobre eso en [sección de historia](../introduction/history.md)). Hay [múltiples formas de importar Yandex.Conjunto de datos de Metrica](example-datasets/metrica.md), y por el bien del tutorial, iremos con el más realista.
-
-### Descargar y extraer datos de tabla {#download-and-extract-table-data}
-
-``` bash
-curl https://datasets.clickhouse.tech/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` > hits_v1.tsv
-curl https://datasets.clickhouse.tech/visits/tsv/visits_v1.tsv.xz | unxz --threads=`nproc` > visits_v1.tsv
-```
-
-Los archivos extraídos tienen un tamaño de aproximadamente 10 GB.
-
-### Crear tablas {#create-tables}
-
-Como en la mayoría de los sistemas de gestión de bases de datos, ClickHouse agrupa lógicamente las tablas en “databases”. Hay un `default` base de datos, pero crearemos una nueva llamada `tutorial`:
-
-``` bash
-clickhouse-client --query "CREATE DATABASE IF NOT EXISTS tutorial"
-```
-
-La sintaxis para crear tablas es mucho más complicada en comparación con las bases de datos (ver [referencia](../sql-reference/statements/create.md). En general `CREATE TABLE` declaración tiene que especificar tres cosas clave:
-
-1.  Nombre de la tabla que se va a crear.
-2.  Table schema, i.e. list of columns and their [tipos de datos](../sql-reference/data-types/index.md).
-3.  [Motor de tabla](../engines/table-engines/index.md) y su configuración, que determina todos los detalles sobre cómo se ejecutarán físicamente las consultas a esta tabla.
-
-El Yandex.Metrica es un servicio de análisis web, y el conjunto de datos de muestra no cubre toda su funcionalidad, por lo que solo hay dos tablas para crear:
-
--   `hits` es una tabla con cada acción realizada por todos los usuarios en todos los sitios web cubiertos por el servicio.
--   `visits` es una tabla que contiene sesiones precompiladas en lugar de acciones individuales.
-
-Veamos y ejecutemos las consultas de tabla de creación real para estas tablas:
-
-``` sql
-CREATE TABLE tutorial.hits_v1
-(
-    `WatchID` UInt64,
-    `JavaEnable` UInt8,
-    `Title` String,
-    `GoodEvent` Int16,
-    `EventTime` DateTime,
-    `EventDate` Date,
-    `CounterID` UInt32,
-    `ClientIP` UInt32,
-    `ClientIP6` FixedString(16),
-    `RegionID` UInt32,
-    `UserID` UInt64,
-    `CounterClass` Int8,
-    `OS` UInt8,
-    `UserAgent` UInt8,
-    `URL` String,
-    `Referer` String,
-    `URLDomain` String,
-    `RefererDomain` String,
-    `Refresh` UInt8,
-    `IsRobot` UInt8,
-    `RefererCategories` Array(UInt16),
-    `URLCategories` Array(UInt16),
-    `URLRegions` Array(UInt32),
-    `RefererRegions` Array(UInt32),
-    `ResolutionWidth` UInt16,
-    `ResolutionHeight` UInt16,
-    `ResolutionDepth` UInt8,
-    `FlashMajor` UInt8,
-    `FlashMinor` UInt8,
-    `FlashMinor2` String,
-    `NetMajor` UInt8,
-    `NetMinor` UInt8,
-    `UserAgentMajor` UInt16,
-    `UserAgentMinor` FixedString(2),
-    `CookieEnable` UInt8,
-    `JavascriptEnable` UInt8,
-    `IsMobile` UInt8,
-    `MobilePhone` UInt8,
-    `MobilePhoneModel` String,
-    `Params` String,
-    `IPNetworkID` UInt32,
-    `TraficSourceID` Int8,
-    `SearchEngineID` UInt16,
-    `SearchPhrase` String,
-    `AdvEngineID` UInt8,
-    `IsArtifical` UInt8,
-    `WindowClientWidth` UInt16,
-    `WindowClientHeight` UInt16,
-    `ClientTimeZone` Int16,
-    `ClientEventTime` DateTime,
-    `SilverlightVersion1` UInt8,
-    `SilverlightVersion2` UInt8,
-    `SilverlightVersion3` UInt32,
-    `SilverlightVersion4` UInt16,
-    `PageCharset` String,
-    `CodeVersion` UInt32,
-    `IsLink` UInt8,
-    `IsDownload` UInt8,
-    `IsNotBounce` UInt8,
-    `FUniqID` UInt64,
-    `HID` UInt32,
-    `IsOldCounter` UInt8,
-    `IsEvent` UInt8,
-    `IsParameter` UInt8,
-    `DontCountHits` UInt8,
-    `WithHash` UInt8,
-    `HitColor` FixedString(1),
-    `UTCEventTime` DateTime,
-    `Age` UInt8,
-    `Sex` UInt8,
-    `Income` UInt8,
-    `Interests` UInt16,
-    `Robotness` UInt8,
-    `GeneralInterests` Array(UInt16),
-    `RemoteIP` UInt32,
-    `RemoteIP6` FixedString(16),
-    `WindowName` Int32,
-    `OpenerName` Int32,
-    `HistoryLength` Int16,
-    `BrowserLanguage` FixedString(2),
-    `BrowserCountry` FixedString(2),
-    `SocialNetwork` String,
-    `SocialAction` String,
-    `HTTPError` UInt16,
-    `SendTiming` Int32,
-    `DNSTiming` Int32,
-    `ConnectTiming` Int32,
-    `ResponseStartTiming` Int32,
-    `ResponseEndTiming` Int32,
-    `FetchTiming` Int32,
-    `RedirectTiming` Int32,
-    `DOMInteractiveTiming` Int32,
-    `DOMContentLoadedTiming` Int32,
-    `DOMCompleteTiming` Int32,
-    `LoadEventStartTiming` Int32,
-    `LoadEventEndTiming` Int32,
-    `NSToDOMContentLoadedTiming` Int32,
-    `FirstPaintTiming` Int32,
-    `RedirectCount` Int8,
-    `SocialSourceNetworkID` UInt8,
-    `SocialSourcePage` String,
-    `ParamPrice` Int64,
-    `ParamOrderID` String,
-    `ParamCurrency` FixedString(3),
-    `ParamCurrencyID` UInt16,
-    `GoalsReached` Array(UInt32),
-    `OpenstatServiceName` String,
-    `OpenstatCampaignID` String,
-    `OpenstatAdID` String,
-    `OpenstatSourceID` String,
-    `UTMSource` String,
-    `UTMMedium` String,
-    `UTMCampaign` String,
-    `UTMContent` String,
-    `UTMTerm` String,
-    `FromTag` String,
-    `HasGCLID` UInt8,
-    `RefererHash` UInt64,
-    `URLHash` UInt64,
-    `CLID` UInt32,
-    `YCLID` UInt64,
-    `ShareService` String,
-    `ShareURL` String,
-    `ShareTitle` String,
-    `ParsedParams` Nested(
-        Key1 String,
-        Key2 String,
-        Key3 String,
-        Key4 String,
-        Key5 String,
-        ValueDouble Float64),
-    `IslandID` FixedString(16),
-    `RequestNum` UInt32,
-    `RequestTry` UInt8
-)
-ENGINE = MergeTree()
-PARTITION BY toYYYYMM(EventDate)
-ORDER BY (CounterID, EventDate, intHash32(UserID))
-SAMPLE BY intHash32(UserID)
-```
-
-``` sql
-CREATE TABLE tutorial.visits_v1
-(
-    `CounterID` UInt32,
-    `StartDate` Date,
-    `Sign` Int8,
-    `IsNew` UInt8,
-    `VisitID` UInt64,
-    `UserID` UInt64,
-    `StartTime` DateTime,
-    `Duration` UInt32,
-    `UTCStartTime` DateTime,
-    `PageViews` Int32,
-    `Hits` Int32,
-    `IsBounce` UInt8,
-    `Referer` String,
-    `StartURL` String,
-    `RefererDomain` String,
-    `StartURLDomain` String,
-    `EndURL` String,
-    `LinkURL` String,
-    `IsDownload` UInt8,
-    `TraficSourceID` Int8,
-    `SearchEngineID` UInt16,
-    `SearchPhrase` String,
-    `AdvEngineID` UInt8,
-    `PlaceID` Int32,
-    `RefererCategories` Array(UInt16),
-    `URLCategories` Array(UInt16),
-    `URLRegions` Array(UInt32),
-    `RefererRegions` Array(UInt32),
-    `IsYandex` UInt8,
-    `GoalReachesDepth` Int32,
-    `GoalReachesURL` Int32,
-    `GoalReachesAny` Int32,
-    `SocialSourceNetworkID` UInt8,
-    `SocialSourcePage` String,
-    `MobilePhoneModel` String,
-    `ClientEventTime` DateTime,
-    `RegionID` UInt32,
-    `ClientIP` UInt32,
-    `ClientIP6` FixedString(16),
-    `RemoteIP` UInt32,
-    `RemoteIP6` FixedString(16),
-    `IPNetworkID` UInt32,
-    `SilverlightVersion3` UInt32,
-    `CodeVersion` UInt32,
-    `ResolutionWidth` UInt16,
-    `ResolutionHeight` UInt16,
-    `UserAgentMajor` UInt16,
-    `UserAgentMinor` UInt16,
-    `WindowClientWidth` UInt16,
-    `WindowClientHeight` UInt16,
-    `SilverlightVersion2` UInt8,
-    `SilverlightVersion4` UInt16,
-    `FlashVersion3` UInt16,
-    `FlashVersion4` UInt16,
-    `ClientTimeZone` Int16,
-    `OS` UInt8,
-    `UserAgent` UInt8,
-    `ResolutionDepth` UInt8,
-    `FlashMajor` UInt8,
-    `FlashMinor` UInt8,
-    `NetMajor` UInt8,
-    `NetMinor` UInt8,
-    `MobilePhone` UInt8,
-    `SilverlightVersion1` UInt8,
-    `Age` UInt8,
-    `Sex` UInt8,
-    `Income` UInt8,
-    `JavaEnable` UInt8,
-    `CookieEnable` UInt8,
-    `JavascriptEnable` UInt8,
-    `IsMobile` UInt8,
-    `BrowserLanguage` UInt16,
-    `BrowserCountry` UInt16,
-    `Interests` UInt16,
-    `Robotness` UInt8,
-    `GeneralInterests` Array(UInt16),
-    `Params` Array(String),
-    `Goals` Nested(
-        ID UInt32,
-        Serial UInt32,
-        EventTime DateTime,
-        Price Int64,
-        OrderID String,
-        CurrencyID UInt32),
-    `WatchIDs` Array(UInt64),
-    `ParamSumPrice` Int64,
-    `ParamCurrency` FixedString(3),
-    `ParamCurrencyID` UInt16,
-    `ClickLogID` UInt64,
-    `ClickEventID` Int32,
-    `ClickGoodEvent` Int32,
-    `ClickEventTime` DateTime,
-    `ClickPriorityID` Int32,
-    `ClickPhraseID` Int32,
-    `ClickPageID` Int32,
-    `ClickPlaceID` Int32,
-    `ClickTypeID` Int32,
-    `ClickResourceID` Int32,
-    `ClickCost` UInt32,
-    `ClickClientIP` UInt32,
-    `ClickDomainID` UInt32,
-    `ClickURL` String,
-    `ClickAttempt` UInt8,
-    `ClickOrderID` UInt32,
-    `ClickBannerID` UInt32,
-    `ClickMarketCategoryID` UInt32,
-    `ClickMarketPP` UInt32,
-    `ClickMarketCategoryName` String,
-    `ClickMarketPPName` String,
-    `ClickAWAPSCampaignName` String,
-    `ClickPageName` String,
-    `ClickTargetType` UInt16,
-    `ClickTargetPhraseID` UInt64,
-    `ClickContextType` UInt8,
-    `ClickSelectType` Int8,
-    `ClickOptions` String,
-    `ClickGroupBannerID` Int32,
-    `OpenstatServiceName` String,
-    `OpenstatCampaignID` String,
-    `OpenstatAdID` String,
-    `OpenstatSourceID` String,
-    `UTMSource` String,
-    `UTMMedium` String,
-    `UTMCampaign` String,
-    `UTMContent` String,
-    `UTMTerm` String,
-    `FromTag` String,
-    `HasGCLID` UInt8,
-    `FirstVisit` DateTime,
-    `PredLastVisit` Date,
-    `LastVisit` Date,
-    `TotalVisits` UInt32,
-    `TraficSource` Nested(
-        ID Int8,
-        SearchEngineID UInt16,
-        AdvEngineID UInt8,
-        PlaceID UInt16,
-        SocialSourceNetworkID UInt8,
-        Domain String,
-        SearchPhrase String,
-        SocialSourcePage String),
-    `Attendance` FixedString(16),
-    `CLID` UInt32,
-    `YCLID` UInt64,
-    `NormalizedRefererHash` UInt64,
-    `SearchPhraseHash` UInt64,
-    `RefererDomainHash` UInt64,
-    `NormalizedStartURLHash` UInt64,
-    `StartURLDomainHash` UInt64,
-    `NormalizedEndURLHash` UInt64,
-    `TopLevelDomain` UInt64,
-    `URLScheme` UInt64,
-    `OpenstatServiceNameHash` UInt64,
-    `OpenstatCampaignIDHash` UInt64,
-    `OpenstatAdIDHash` UInt64,
-    `OpenstatSourceIDHash` UInt64,
-    `UTMSourceHash` UInt64,
-    `UTMMediumHash` UInt64,
-    `UTMCampaignHash` UInt64,
-    `UTMContentHash` UInt64,
-    `UTMTermHash` UInt64,
-    `FromHash` UInt64,
-    `WebVisorEnabled` UInt8,
-    `WebVisorActivity` UInt32,
-    `ParsedParams` Nested(
-        Key1 String,
-        Key2 String,
-        Key3 String,
-        Key4 String,
-        Key5 String,
-        ValueDouble Float64),
-    `Market` Nested(
-        Type UInt8,
-        GoalID UInt32,
-        OrderID String,
-        OrderPrice Int64,
-        PP UInt32,
-        DirectPlaceID UInt32,
-        DirectOrderID UInt32,
-        DirectBannerID UInt32,
-        GoodID String,
-        GoodName String,
-        GoodQuantity Int32,
-        GoodPrice Int64),
-    `IslandID` FixedString(16)
-)
-ENGINE = CollapsingMergeTree(Sign)
-PARTITION BY toYYYYMM(StartDate)
-ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID)
-SAMPLE BY intHash32(UserID)
-```
-
-Puede ejecutar esas consultas utilizando el modo interactivo de `clickhouse-client` (simplemente ejecútelo en un terminal sin especificar una consulta por adelantado) o pruebe algunos [interfaz alternativa](../interfaces/index.md) Si quieres.
-
-Como podemos ver, `hits_v1` utiliza el [motor básico MergeTree](../engines/table-engines/mergetree-family/mergetree.md), mientras que el `visits_v1` utiliza el [Derrumbar](../engines/table-engines/mergetree-family/collapsingmergetree.md) variante.
-
-### Importar datos {#import-data}
-
-La importación de datos a ClickHouse se realiza a través de [INSERT INTO](../sql-reference/statements/insert-into.md) consulta como en muchas otras bases de datos SQL. Sin embargo, los datos generalmente se proporcionan en uno de los [Formatos de serialización compatibles](../interfaces/formats.md) en lugar de `VALUES` cláusula (que también es compatible).
-
-Los archivos que descargamos anteriormente están en formato separado por tabuladores, así que aquí le mostramos cómo importarlos a través del cliente de la consola:
-
-``` bash
-clickhouse-client --query "INSERT INTO tutorial.hits_v1 FORMAT TSV" --max_insert_block_size=100000 < hits_v1.tsv
-clickhouse-client --query "INSERT INTO tutorial.visits_v1 FORMAT TSV" --max_insert_block_size=100000 < visits_v1.tsv
-```
-
-ClickHouse tiene un montón de [ajustes para sintonizar](../operations/settings/index.md) y una forma de especificarlos en el cliente de la consola es a través de argumentos, como podemos ver con `--max_insert_block_size`. La forma más fácil de averiguar qué configuraciones están disponibles, qué significan y cuáles son los valores predeterminados es consultar el `system.settings` tabla:
-
-``` sql
-SELECT name, value, changed, description
-FROM system.settings
-WHERE name LIKE '%max_insert_b%'
-FORMAT TSV
-
-max_insert_block_size    1048576    0    "The maximum block size for insertion, if we control the creation of blocks for insertion."
-```
-
-Opcionalmente se puede [OPTIMIZE](../sql-reference/statements/misc.md#misc_operations-optimize) las tablas después de la importación. Las tablas que están configuradas con un motor de la familia MergeTree siempre fusionan partes de datos en segundo plano para optimizar el almacenamiento de datos (o al menos verificar si tiene sentido). Estas consultas obligan al motor de tablas a realizar la optimización del almacenamiento en este momento en lugar de algún tiempo después:
-
-``` bash
-clickhouse-client --query "OPTIMIZE TABLE tutorial.hits_v1 FINAL"
-clickhouse-client --query "OPTIMIZE TABLE tutorial.visits_v1 FINAL"
-```
-
-Estas consultas inician una operación intensiva de E / S y CPU, por lo que si la tabla recibe datos nuevos de manera consistente, es mejor dejarlos solos y dejar que las fusiones se ejecuten en segundo plano.
-
-Ahora podemos comprobar si la importación de la tabla fue exitosa:
-
-``` bash
-clickhouse-client --query "SELECT COUNT(*) FROM tutorial.hits_v1"
-clickhouse-client --query "SELECT COUNT(*) FROM tutorial.visits_v1"
-```
-
-## Consultas de ejemplo {#example-queries}
-
-``` sql
-SELECT
-    StartURL AS URL,
-    AVG(Duration) AS AvgDuration
-FROM tutorial.visits_v1
-WHERE StartDate BETWEEN '2014-03-23' AND '2014-03-30'
-GROUP BY URL
-ORDER BY AvgDuration DESC
-LIMIT 10
-```
-
-``` sql
-SELECT
-    sum(Sign) AS visits,
-    sumIf(Sign, has(Goals.ID, 1105530)) AS goal_visits,
-    (100. * goal_visits) / visits AS goal_percent
-FROM tutorial.visits_v1
-WHERE (CounterID = 912887) AND (toYYYYMM(StartDate) = 201403) AND (domain(StartURL) = 'yandex.ru')
-```
-
-## Implementación de clúster {#cluster-deployment}
-
-El clúster ClickHouse es un clúster homogéneo. Pasos para configurar:
-
-1.  Instale el servidor ClickHouse en todas las máquinas del clúster
-2.  Configurar configuraciones de clúster en archivos de configuración
-3.  Crear tablas locales en cada instancia
-4.  Crear un [Tabla distribuida](../engines/table-engines/special/distributed.md)
-
-[Tabla distribuida](../engines/table-engines/special/distributed.md) es en realidad una especie de “view” a las tablas locales del clúster ClickHouse. La consulta SELECT de una tabla distribuida se ejecuta utilizando recursos de todos los fragmentos del clúster. Puede especificar configuraciones para varios clústeres y crear varias tablas distribuidas que proporcionen vistas a diferentes clústeres.
-
-Ejemplo de configuración para un clúster con tres fragmentos, una réplica cada uno:
-
-``` xml
-<remote_servers>
-    <perftest_3shards_1replicas>
-        <shard>
-            <replica>
-                <host>example-perftest01j.yandex.ru</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-        <shard>
-            <replica>
-                <host>example-perftest02j.yandex.ru</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-        <shard>
-            <replica>
-                <host>example-perftest03j.yandex.ru</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-    </perftest_3shards_1replicas>
-</remote_servers>
-```
-
-Para más demostraciones, vamos a crear una nueva tabla local con la misma `CREATE TABLE` consulta que utilizamos para `hits_v1`, pero nombre de tabla diferente:
-
-``` sql
-CREATE TABLE tutorial.hits_local (...) ENGINE = MergeTree() ...
-```
-
-Creación de una tabla distribuida que proporcione una vista en las tablas locales del clúster:
-
-``` sql
-CREATE TABLE tutorial.hits_all AS tutorial.hits_local
-ENGINE = Distributed(perftest_3shards_1replicas, tutorial, hits_local, rand());
-```
-
-Una práctica común es crear tablas distribuidas similares en todas las máquinas del clúster. Permite ejecutar consultas distribuidas en cualquier máquina del clúster. También hay una opción alternativa para crear una tabla distribuida temporal para una consulta SELECT determinada usando [remoto](../sql-reference/table-functions/remote.md) función de la tabla.
-
-Vamos a correr [INSERT SELECT](../sql-reference/statements/insert-into.md) en la tabla Distributed para extender la tabla a varios servidores.
-
-``` sql
-INSERT INTO tutorial.hits_all SELECT * FROM tutorial.hits_v1;
-```
-
-!!! warning "Aviso"
-    Este enfoque no es adecuado para la fragmentación de tablas grandes. Hay una herramienta separada [Método de codificación de datos:](../operations/utilities/clickhouse-copier.md) que puede volver a fragmentar tablas grandes arbitrarias.
-
-Como era de esperar, las consultas computacionalmente pesadas se ejecutan N veces más rápido si utilizan 3 servidores en lugar de uno.
-
-En este caso, hemos utilizado un clúster con 3 fragmentos, y cada uno contiene una sola réplica.
-
-Para proporcionar resiliencia en un entorno de producción, se recomienda que cada fragmento contenga 2-3 réplicas distribuidas entre varias zonas de disponibilidad o centros de datos (o al menos racks). Tenga en cuenta que ClickHouse admite un número ilimitado de réplicas.
-
-Ejemplo de configuración para un clúster de un fragmento que contiene tres réplicas:
-
-``` xml
-<remote_servers>
-    ...
-    <perftest_1shards_3replicas>
-        <shard>
-            <replica>
-                <host>example-perftest01j.yandex.ru</host>
-                <port>9000</port>
-             </replica>
-             <replica>
-                <host>example-perftest02j.yandex.ru</host>
-                <port>9000</port>
-             </replica>
-             <replica>
-                <host>example-perftest03j.yandex.ru</host>
-                <port>9000</port>
-             </replica>
-        </shard>
-    </perftest_1shards_3replicas>
-</remote_servers>
-```
-
-Para habilitar la replicación nativa [ZooKeeper](http://zookeeper.apache.org/) se requiere. ClickHouse se encarga de la coherencia de los datos en todas las réplicas y ejecuta el procedimiento de restauración después de la falla automáticamente. Se recomienda implementar el clúster ZooKeeper en servidores independientes (donde no se están ejecutando otros procesos, incluido ClickHouse).
-
-!!! note "Nota"
-    ZooKeeper no es un requisito estricto: en algunos casos simples, puede duplicar los datos escribiéndolos en todas las réplicas de su código de aplicación. Este enfoque es **ni** recomendado, en este caso, ClickHouse no podrá garantizar la coherencia de los datos en todas las réplicas. Por lo tanto, se convierte en responsabilidad de su aplicación.
-
-Las ubicaciones de ZooKeeper se especifican en el archivo de configuración:
-
-``` xml
-<zookeeper>
-    <node>
-        <host>zoo01.yandex.ru</host>
-        <port>2181</port>
-    </node>
-    <node>
-        <host>zoo02.yandex.ru</host>
-        <port>2181</port>
-    </node>
-    <node>
-        <host>zoo03.yandex.ru</host>
-        <port>2181</port>
-    </node>
-</zookeeper>
-```
-
-Además, necesitamos establecer macros para identificar cada fragmento y réplica que se utilizan en la creación de tablas:
-
-``` xml
-<macros>
-    <shard>01</shard>
-    <replica>01</replica>
-</macros>
-```
-
-Si no hay réplicas en este momento en la creación de la tabla replicada, se crea una instancia de una nueva primera réplica. Si ya hay réplicas activas, la nueva réplica clona los datos de las existentes. Tiene la opción de crear primero todas las tablas replicadas y, a continuación, insertar datos en ella. Otra opción es crear algunas réplicas y agregar las otras después o durante la inserción de datos.
-
-``` sql
-CREATE TABLE tutorial.hits_replica (...)
-ENGINE = ReplcatedMergeTree(
-    '/clickhouse_perftest/tables/{shard}/hits',
-    '{replica}'
-)
-...
-```
-
-Aquí usamos [ReplicatedMergeTree](../engines/table-engines/mergetree-family/replication.md) motor de mesa. En los parámetros, especificamos la ruta ZooKeeper que contiene identificadores de fragmentos y réplicas.
-
-``` sql
-INSERT INTO tutorial.hits_replica SELECT * FROM tutorial.hits_local;
-```
-
-La replicación funciona en modo multi-master. Los datos se pueden cargar en cualquier réplica y el sistema los sincroniza automáticamente con otras instancias. La replicación es asíncrona, por lo que en un momento dado, no todas las réplicas pueden contener datos insertados recientemente. Al menos una réplica debe estar disponible para permitir la ingestión de datos. Otros sincronizarán los datos y repararán la coherencia una vez que vuelvan a activarse. Tenga en cuenta que este enfoque permite la baja posibilidad de una pérdida de datos recientemente insertados.
-
-[Artículo Original](https://clickhouse.tech/docs/en/getting_started/tutorial/) <!--hide-->
diff --git a/docs/es/guides/apply-catboost-model.md b/docs/es/guides/apply-catboost-model.md
deleted file mode 100644
index b1fe50f32767..000000000000
--- a/docs/es/guides/apply-catboost-model.md
+++ /dev/null
@@ -1,239 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: "Aplicaci\xF3n de modelos CatBoost"
----
-
-# Aplicación de un modelo Catboost en ClickHouse {#applying-catboost-model-in-clickhouse}
-
-[CatBoost](https://catboost.ai) es una biblioteca de impulso de gradiente libre y de código abierto desarrollada en [Yandex](https://yandex.com/company/) para el aprendizaje automático.
-
-Con esta instrucción, aprenderá a aplicar modelos preentrenados en ClickHouse ejecutando la inferencia de modelos desde SQL.
-
-Para aplicar un modelo CatBoost en ClickHouse:
-
-1.  [Crear una tabla](#create-table).
-2.  [Insertar los datos en la tabla](#insert-data-to-table).
-3.  [Integrar CatBoost en ClickHouse](#integrate-catboost-into-clickhouse) (Paso opcional).
-4.  [Ejecute la inferencia del modelo desde SQL](#run-model-inference).
-
-Para obtener más información sobre la formación de modelos CatBoost, consulte [Entrenamiento y aplicación de modelos](https://catboost.ai/docs/features/training.html#training).
-
-## Requisito {#prerequisites}
-
-Si no tienes el [Acoplador](https://docs.docker.com/install/) sin embargo, instalarlo.
-
-!!! note "Nota"
-    [Acoplador](https://www.docker.com) es una plataforma de software que le permite crear contenedores que aíslan una instalación de CatBoost y ClickHouse del resto del sistema.
-
-Antes de aplicar un modelo CatBoost:
-
-**1.** Tire de la [Imagen de acoplador](https://hub.docker.com/r/yandex/tutorial-catboost-clickhouse) del registro:
-
-``` bash
-$ docker pull yandex/tutorial-catboost-clickhouse
-```
-
-Esta imagen de Docker contiene todo lo que necesita para ejecutar CatBoost y ClickHouse: código, tiempo de ejecución, bibliotecas, variables de entorno y archivos de configuración.
-
-**2.** Asegúrese de que la imagen de Docker se haya extraído correctamente:
-
-``` bash
-$ docker image ls
-REPOSITORY                            TAG                 IMAGE ID            CREATED             SIZE
-yandex/tutorial-catboost-clickhouse   latest              622e4d17945b        22 hours ago        1.37GB
-```
-
-**3.** Inicie un contenedor Docker basado en esta imagen:
-
-``` bash
-$ docker run -it -p 8888:8888 yandex/tutorial-catboost-clickhouse
-```
-
-## 1. Crear una tabla {#create-table}
-
-Para crear una tabla ClickHouse para el ejemplo de capacitación:
-
-**1.** Inicie el cliente de consola ClickHouse en el modo interactivo:
-
-``` bash
-$ clickhouse client
-```
-
-!!! note "Nota"
-    El servidor ClickHouse ya se está ejecutando dentro del contenedor Docker.
-
-**2.** Cree la tabla usando el comando:
-
-``` sql
-:) CREATE TABLE amazon_train
-(
-    date Date MATERIALIZED today(),
-    ACTION UInt8,
-    RESOURCE UInt32,
-    MGR_ID UInt32,
-    ROLE_ROLLUP_1 UInt32,
-    ROLE_ROLLUP_2 UInt32,
-    ROLE_DEPTNAME UInt32,
-    ROLE_TITLE UInt32,
-    ROLE_FAMILY_DESC UInt32,
-    ROLE_FAMILY UInt32,
-    ROLE_CODE UInt32
-)
-ENGINE = MergeTree ORDER BY date
-```
-
-**3.** Salir del cliente de la consola ClickHouse:
-
-``` sql
-:) exit
-```
-
-## 2. Insertar los datos en la tabla {#insert-data-to-table}
-
-Para insertar los datos:
-
-**1.** Ejecute el siguiente comando:
-
-``` bash
-$ clickhouse client --host 127.0.0.1 --query 'INSERT INTO amazon_train FORMAT CSVWithNames' < ~/amazon/train.csv
-```
-
-**2.** Inicie el cliente de consola ClickHouse en el modo interactivo:
-
-``` bash
-$ clickhouse client
-```
-
-**3.** Asegúrese de que los datos se hayan cargado:
-
-``` sql
-:) SELECT count() FROM amazon_train
-
-SELECT count()
-FROM amazon_train
-
-+-count()-+
-|   65538 |
-+-------+
-```
-
-## 3. Integrar CatBoost en ClickHouse {#integrate-catboost-into-clickhouse}
-
-!!! note "Nota"
-    **Paso opcional.** La imagen de Docker contiene todo lo que necesita para ejecutar CatBoost y ClickHouse.
-
-Para integrar CatBoost en ClickHouse:
-
-**1.** Construir la biblioteca de evaluación.
-
-La forma más rápida de evaluar un modelo CatBoost es compilar `libcatboostmodel.<so|dll|dylib>` biblioteca. Para obtener más información acerca de cómo construir la biblioteca, vea [Documentación de CatBoost](https://catboost.ai/docs/concepts/c-plus-plus-api_dynamic-c-pluplus-wrapper.html).
-
-**2.** Cree un nuevo directorio en cualquier lugar y con cualquier nombre, por ejemplo, `data` y poner la biblioteca creada en ella. La imagen de Docker ya contiene la biblioteca `data/libcatboostmodel.so`.
-
-**3.** Cree un nuevo directorio para el modelo de configuración en cualquier lugar y con cualquier nombre, por ejemplo, `models`.
-
-**4.** Cree un archivo de configuración de modelo con cualquier nombre, por ejemplo, `models/amazon_model.xml`.
-
-**5.** Describir la configuración del modelo:
-
-``` xml
-<models>
-    <model>
-        <!-- Model type. Now catboost only. -->
-        <type>catboost</type>
-        <!-- Model name. -->
-        <name>amazon</name>
-        <!-- Path to trained model. -->
-        <path>/home/catboost/tutorial/catboost_model.bin</path>
-        <!-- Update interval. -->
-        <lifetime>0</lifetime>
-    </model>
-</models>
-```
-
-**6.** Agregue la ruta de acceso a CatBoost y la configuración del modelo a la configuración de ClickHouse:
-
-``` xml
-<!-- File etc/clickhouse-server/config.d/models_config.xml. -->
-<catboost_dynamic_library_path>/home/catboost/data/libcatboostmodel.so</catboost_dynamic_library_path>
-<models_config>/home/catboost/models/*_model.xml</models_config>
-```
-
-## 4. Ejecute la inferencia del modelo desde SQL {#run-model-inference}
-
-Para el modelo de prueba, ejecute el cliente ClickHouse `$ clickhouse client`.
-
-Asegurémonos de que el modelo esté funcionando:
-
-``` sql
-:) SELECT
-    modelEvaluate('amazon',
-                RESOURCE,
-                MGR_ID,
-                ROLE_ROLLUP_1,
-                ROLE_ROLLUP_2,
-                ROLE_DEPTNAME,
-                ROLE_TITLE,
-                ROLE_FAMILY_DESC,
-                ROLE_FAMILY,
-                ROLE_CODE) > 0 AS prediction,
-    ACTION AS target
-FROM amazon_train
-LIMIT 10
-```
-
-!!! note "Nota"
-    Función [modelEvaluar](../sql-reference/functions/other-functions.md#function-modelevaluate) devuelve tupla con predicciones sin procesar por clase para modelos multiclase.
-
-Vamos a predecir la probabilidad:
-
-``` sql
-:) SELECT
-    modelEvaluate('amazon',
-                RESOURCE,
-                MGR_ID,
-                ROLE_ROLLUP_1,
-                ROLE_ROLLUP_2,
-                ROLE_DEPTNAME,
-                ROLE_TITLE,
-                ROLE_FAMILY_DESC,
-                ROLE_FAMILY,
-                ROLE_CODE) AS prediction,
-    1. / (1 + exp(-prediction)) AS probability,
-    ACTION AS target
-FROM amazon_train
-LIMIT 10
-```
-
-!!! note "Nota"
-    Más información sobre [exp()](../sql-reference/functions/math-functions.md) función.
-
-Vamos a calcular LogLoss en la muestra:
-
-``` sql
-:) SELECT -avg(tg * log(prob) + (1 - tg) * log(1 - prob)) AS logloss
-FROM
-(
-    SELECT
-        modelEvaluate('amazon',
-                    RESOURCE,
-                    MGR_ID,
-                    ROLE_ROLLUP_1,
-                    ROLE_ROLLUP_2,
-                    ROLE_DEPTNAME,
-                    ROLE_TITLE,
-                    ROLE_FAMILY_DESC,
-                    ROLE_FAMILY,
-                    ROLE_CODE) AS prediction,
-        1. / (1. + exp(-prediction)) AS prob,
-        ACTION AS tg
-    FROM amazon_train
-)
-```
-
-!!! note "Nota"
-    Más información sobre [avg()](../sql-reference/aggregate-functions/reference.md#agg_function-avg) y [registro()](../sql-reference/functions/math-functions.md) función.
-
-[Artículo Original](https://clickhouse.tech/docs/en/guides/apply_catboost_model/) <!--hide-->
diff --git a/docs/es/guides/index.md b/docs/es/guides/index.md
deleted file mode 100644
index c8332ac78462..000000000000
--- a/docs/es/guides/index.md
+++ /dev/null
@@ -1,16 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Guiar
-toc_priority: 38
-toc_title: "Descripci\xF3n"
----
-
-# Guías de ClickHouse {#clickhouse-guides}
-
-Lista de instrucciones detalladas paso a paso que ayudan a resolver varias tareas usando ClickHouse:
-
--   [Tutorial sobre la configuración simple del clúster](../getting-started/tutorial.md)
--   [Aplicación de un modelo CatBoost en ClickHouse](apply-catboost-model.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/guides/) <!--hide-->
diff --git a/docs/es/images/column-oriented.gif b/docs/es/images/column-oriented.gif
deleted file mode 100644
index d5ac7c82848c..000000000000
Binary files a/docs/es/images/column-oriented.gif and /dev/null differ
diff --git a/docs/es/images/logo.svg b/docs/es/images/logo.svg
deleted file mode 100644
index b5ab923ff653..000000000000
--- a/docs/es/images/logo.svg
+++ /dev/null
@@ -1,1 +0,0 @@
-<svg xmlns="http://www.w3.org/2000/svg" width="54" height="48" markdown="1" viewBox="0 0 9 8"><style>.o{fill:#fc0}.r{fill:red}</style><path d="M0,7 h1 v1 h-1 z" class="r"/><path d="M0,0 h1 v7 h-1 z" class="o"/><path d="M2,0 h1 v8 h-1 z" class="o"/><path d="M4,0 h1 v8 h-1 z" class="o"/><path d="M6,0 h1 v8 h-1 z" class="o"/><path d="M8,3.25 h1 v1.5 h-1 z" class="o"/></svg>
\ No newline at end of file
diff --git a/docs/es/images/row-oriented.gif b/docs/es/images/row-oriented.gif
deleted file mode 100644
index 41395b5693e9..000000000000
Binary files a/docs/es/images/row-oriented.gif and /dev/null differ
diff --git a/docs/es/index.md b/docs/es/index.md
deleted file mode 100644
index c76fe32e33bb..000000000000
--- a/docs/es/index.md
+++ /dev/null
@@ -1,97 +0,0 @@
----
-machine_translated: false
-machine_translated_rev: 
-toc_priority: 0
-toc_title: "Descripción"
----
-
-# ¿Qué es ClickHouse? {#what-is-clickhouse}
-
-ClickHouse es un sistema de gestión de bases de datos (DBMS), orientado a columnas, para el procesamiento analítico de consultas en línea (OLAP).
-
-En un DBMS “normal”, orientado a filas, los datos se almacenan en este orden:
-
-| Fila | Argumento   | JavaEnable | Titular                   | GoodEvent | EventTime           |
-|------|-------------|------------|---------------------------|-----------|---------------------|
-| #0  | 89354350662 | 1          | Relaciones con inversores | 1         | 2016-05-18 05:19:20 |
-| #1  | 90329509958 | 0          | Contáctenos               | 1         | 2016-05-18 08:10:20 |
-| #2  | 89953706054 | 1          | Mision                    | 1         | 2016-05-18 07:38:00 |
-| #N  | …           | …          | …                         | …         | …                   |
-
-En otras palabras, todos los valores relacionados con una fila se almacenan físicamente uno junto al otro.
-
-Ejemplos de un DBMS orientado a filas son MySQL, Postgres y MS SQL Server.
-
-En un DBMS orientado a columnas, los datos se almacenan así:
-
-| Fila:       | #0                       | #1                 | #2                 | #N |
-|-------------|---------------------------|---------------------|---------------------|-----|
-| Argumento:  | 89354350662               | 90329509958         | 89953706054         | …   |
-| JavaEnable: | 1                         | 0                   | 1                   | …   |
-| Titular:    | Relaciones con inversores | Contáctenos         | Mision              | …   |
-| GoodEvent:  | 1                         | 1                   | 1                   | …   |
-| EventTime:  | 2016-05-18 05:19:20       | 2016-05-18 08:10:20 | 2016-05-18 07:38:00 | …   |
-
-Estos ejemplos solo muestran el orden en el que se organizan los datos. Los valores de diferentes columnas se almacenan por separado y los datos de la misma columna se almacenan juntos.
-
-Ejemplos de un DBMS orientado a columnas: Vertica, Paraccel (Actian Matrix y Amazon Redshift), Sybase IQ, Exasol, Infobright, InfiniDB, MonetDB (VectorWise y Actian Vector), LucidDB, SAP HANA, Google Dremel, Google PowerDrill, Druid y kdb+.
-
-Los diferentes modos de ordenar los datos al guardarlos se adecúan mejor a diferentes escenarios. El escenario de acceso a los datos se refiere a qué consultas se hacen, con qué frecuencia y en qué proporción; cuántos datos se leen para cada tipo de consulta - filas, columnas y bytes; la relación entre lectura y actualización de datos; el tamaño de trabajo de los datos y qué tan localmente son usados; si se usan transacciones y qué tan aisladas están;requerimientos de replicación de los datos y de integridad lógica, requerimientos de latencia y caudal (throughput) para cada tipo de consulta, y cosas por el estilo.
-
-Cuanto mayor sea la carga en el sistema, más importante es personalizar el sistema configurado para que coincida con los requisitos del escenario de uso, y más fino será esta personalización. No existe un sistema que sea igualmente adecuado para escenarios significativamente diferentes. Si un sistema es adaptable a un amplio conjunto de escenarios, bajo una carga alta, el sistema manejará todos los escenarios igualmente mal, o funcionará bien para solo uno o algunos de los escenarios posibles.
-
-## Propiedades clave del escenario OLAP {#key-properties-of-olap-scenario}
-
--   La gran mayoría de las solicitudes son para acceso de lectura.
--   Los datos se actualizan en lotes bastante grandes (\> 1000 filas), no por filas individuales; o no se actualiza en absoluto.
--   Los datos se agregan a la base de datos pero no se modifican.
--   Para las lecturas, se extrae un número bastante grande de filas de la base de datos, pero solo un pequeño subconjunto de columnas.
--   Las tablas son “wide,” lo que significa que contienen un gran número de columnas.
--   Las consultas son relativamente raras (generalmente cientos de consultas por servidor o menos por segundo).
--   Para consultas simples, se permiten latencias de alrededor de 50 ms.
--   Los valores de columna son bastante pequeños: números y cadenas cortas (por ejemplo, 60 bytes por URL).
--   Requiere un alto rendimiento al procesar una sola consulta (hasta miles de millones de filas por segundo por servidor).
--   Las transacciones no son necesarias.
--   Bajos requisitos para la coherencia de los datos.
--   Hay una tabla grande por consulta. Todas las mesas son pequeñas, excepto una.
--   Un resultado de consulta es significativamente menor que los datos de origen. En otras palabras, los datos se filtran o se agregan, por lo que el resultado se ajusta a la RAM de un solo servidor.
-
-Es fácil ver que el escenario OLAP es muy diferente de otros escenarios populares (como el acceso OLTP o Key-Value). Por lo tanto, no tiene sentido intentar usar OLTP o una base de datos de valor clave para procesar consultas analíticas si desea obtener un rendimiento decente. Por ejemplo, si intenta usar MongoDB o Redis para análisis, obtendrá un rendimiento muy bajo en comparación con las bases de datos OLAP.
-
-## Por qué las bases de datos orientadas a columnas funcionan mejor en el escenario OLAP {#why-column-oriented-databases-work-better-in-the-olap-scenario}
-
-Las bases de datos orientadas a columnas son más adecuadas para los escenarios OLAP: son al menos 100 veces más rápidas en el procesamiento de la mayoría de las consultas. Las razones se explican en detalle a continuación, pero el hecho es más fácil de demostrar visualmente:
-
-**DBMS orientado a filas**
-
-![Row-oriented](images/row-oriented.gif#)
-
-**DBMS orientado a columnas**
-
-![Column-oriented](images/column-oriented.gif#)
-
-Ver la diferencia?
-
-### Entrada/salida {#inputoutput}
-
-1.  Para una consulta analítica, solo es necesario leer un pequeño número de columnas de tabla. En una base de datos orientada a columnas, puede leer solo los datos que necesita. Por ejemplo, si necesita 5 columnas de 100, puede esperar una reducción de 20 veces en E/S.
-2.  Dado que los datos se leen en paquetes, es más fácil de comprimir. Los datos en columnas también son más fáciles de comprimir. Esto reduce aún más el volumen de E/S.
-3.  Debido a la reducción de E / S, más datos se ajustan a la memoria caché del sistema.
-
-Por ejemplo, la consulta “count the number of records for each advertising platform” requiere leer uno “advertising platform ID” columna, que ocupa 1 byte sin comprimir. Si la mayor parte del tráfico no proviene de plataformas publicitarias, puede esperar al menos una compresión de 10 veces de esta columna. Cuando se utiliza un algoritmo de compresión rápida, la descompresión de datos es posible a una velocidad de al menos varios gigabytes de datos sin comprimir por segundo. En otras palabras, esta consulta se puede procesar a una velocidad de aproximadamente varios miles de millones de filas por segundo en un único servidor. Esta velocidad se logra realmente en la práctica.
-
-### CPU {#cpu}
-
-Dado que la ejecución de una consulta requiere procesar un gran número de filas, ayuda enviar todas las operaciones para vectores completos en lugar de para filas separadas, o implementar el motor de consultas para que casi no haya costo de envío. Si no hace esto, con cualquier subsistema de disco medio decente, el intérprete de consultas inevitablemente detiene la CPU. Tiene sentido almacenar datos en columnas y procesarlos, cuando sea posible, por columnas.
-
-Hay dos formas de hacer esto:
-
-1.  Un vector motor. Todas las operaciones se escriben para vectores, en lugar de para valores separados. Esto significa que no necesita llamar a las operaciones con mucha frecuencia, y los costos de envío son insignificantes. El código de operación contiene un ciclo interno optimizado.
-
-2.  Generación de código. El código generado para la consulta tiene todas las llamadas indirectas.
-
-Esto no se hace en “normal” bases de datos, porque no tiene sentido cuando se ejecutan consultas simples. Sin embargo, hay excepciones. Por ejemplo, MemSQL utiliza la generación de código para reducir la latencia al procesar consultas SQL. (A modo de comparación, los DBMS analíticos requieren la optimización del rendimiento, no la latencia.)
-
-Tenga en cuenta que para la eficiencia de la CPU, el lenguaje de consulta debe ser declarativo (SQL o MDX), o al menos un vector (J, K). La consulta solo debe contener bucles implícitos, lo que permite la optimización.
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/) ##}
diff --git a/docs/es/interfaces/cli.md b/docs/es/interfaces/cli.md
deleted file mode 100644
index 395f9831a4e8..000000000000
--- a/docs/es/interfaces/cli.md
+++ /dev/null
@@ -1,149 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 17
-toc_title: "Cliente de l\xEDnea de comandos"
----
-
-# Cliente de línea de comandos {#command-line-client}
-
-ClickHouse proporciona un cliente de línea de comandos nativo: `clickhouse-client`. El cliente admite opciones de línea de comandos y archivos de configuración. Para obtener más información, consulte [Configuración](#interfaces_cli_configuration).
-
-[Instalar](../getting-started/index.md) desde el `clickhouse-client` paquete y ejecútelo con el comando `clickhouse-client`.
-
-``` bash
-$ clickhouse-client
-ClickHouse client version 19.17.1.1579 (official build).
-Connecting to localhost:9000 as user default.
-Connected to ClickHouse server version 19.17.1 revision 54428.
-
-:)
-```
-
-Las diferentes versiones de cliente y servidor son compatibles entre sí, pero es posible que algunas funciones no estén disponibles en clientes anteriores. Se recomienda utilizar la misma versión del cliente que la aplicación de servidor. Cuando intenta usar un cliente de la versión anterior, entonces el servidor, `clickhouse-client` muestra el mensaje:
-
-      ClickHouse client version is older than ClickHouse server. It may lack support for new features.
-
-## Uso {#cli_usage}
-
-El cliente se puede utilizar en modo interactivo y no interactivo (por lotes). Para utilizar el modo por lotes, especifique el ‘query’ parámetro, o enviar datos a ‘stdin’ (verifica que ‘stdin’ no es un terminal), o ambos. Similar a la interfaz HTTP, cuando se utiliza el ‘query’ parámetro y el envío de datos a ‘stdin’ la solicitud es una concatenación de la ‘query’ parámetro, un avance de línea y los datos en ‘stdin’. Esto es conveniente para grandes consultas INSERT.
-
-Ejemplo de uso del cliente para insertar datos:
-
-``` bash
-$ echo -ne "1, 'some text', '2016-08-14 00:00:00'
2, 'some more text', '2016-08-14 00:00:01'" | clickhouse-client --database=test --query="INSERT INTO test FORMAT CSV";
-
-$ cat <<_EOF | clickhouse-client --database=test --query="INSERT INTO test FORMAT CSV";
-3, 'some text', '2016-08-14 00:00:00'
-4, 'some more text', '2016-08-14 00:00:01'
-_EOF
-
-$ cat file.csv | clickhouse-client --database=test --query="INSERT INTO test FORMAT CSV";
-```
-
-En el modo por lotes, el formato de datos predeterminado es TabSeparated. Puede establecer el formato en la cláusula FORMAT de la consulta.
-
-De forma predeterminada, solo puede procesar una única consulta en modo por lotes. Para realizar múltiples consultas desde un “script,” utilizar el `--multiquery` parámetro. Esto funciona para todas las consultas excepto INSERT . Los resultados de la consulta se generan consecutivamente sin separadores adicionales. Del mismo modo, para procesar un gran número de consultas, puede ejecutar ‘clickhouse-client’ para cada consulta. Tenga en cuenta que puede tomar decenas de milisegundos para iniciar el ‘clickhouse-client’ programa.
-
-En el modo interactivo, obtiene una línea de comandos donde puede ingresar consultas.
-
-Si ‘multiline’ no se especifica (el valor predeterminado): Para ejecutar la consulta, pulse Intro. El punto y coma no es necesario al final de la consulta. Para introducir una consulta de varias líneas, introduzca una barra invertida `\` antes de la alimentación de línea. Después de presionar Enter, se le pedirá que ingrese la siguiente línea de la consulta.
-
-Si se especifica multilínea: Para ejecutar una consulta, finalícela con un punto y coma y presione Intro. Si se omitió el punto y coma al final de la línea ingresada, se le pedirá que ingrese la siguiente línea de la consulta.
-
-Solo se ejecuta una sola consulta, por lo que se ignora todo después del punto y coma.
-
-Puede especificar `\G` en lugar o después del punto y coma. Esto indica el formato vertical. En este formato, cada valor se imprime en una línea separada, lo cual es conveniente para tablas anchas. Esta característica inusual se agregó por compatibilidad con la CLI de MySQL.
-
-La línea de comandos se basa en ‘replxx’ (similar a ‘readline’). En otras palabras, utiliza los atajos de teclado familiares y mantiene un historial. La historia está escrita para `~/.clickhouse-client-history`.
-
-De forma predeterminada, el formato utilizado es PrettyCompact. Puede cambiar el formato en la cláusula FORMAT de la consulta o especificando `\G` al final de la consulta, utilizando el `--format` o `--vertical` en la línea de comandos, o utilizando el archivo de configuración del cliente.
-
-Para salir del cliente, presione Ctrl+D o introduzca una de las siguientes opciones en lugar de una consulta: “exit”, “quit”, “logout”, “exit;”, “quit;”, “logout;”, “q”, “Q”, “:q”
-
-Al procesar una consulta, el cliente muestra:
-
-1.  Progreso, que se actualiza no más de 10 veces por segundo (de forma predeterminada). Para consultas rápidas, es posible que el progreso no tenga tiempo para mostrarse.
-2.  La consulta con formato después del análisis, para la depuración.
-3.  El resultado en el formato especificado.
-4.  El número de líneas en el resultado, el tiempo transcurrido y la velocidad promedio de procesamiento de consultas.
-
-Puede cancelar una consulta larga presionando Ctrl + C. Sin embargo, aún tendrá que esperar un poco para que el servidor aborte la solicitud. No es posible cancelar una consulta en determinadas etapas. Si no espera y presiona Ctrl + C por segunda vez, el cliente saldrá.
-
-El cliente de línea de comandos permite pasar datos externos (tablas temporales externas) para consultar. Para obtener más información, consulte la sección “External data for query processing”.
-
-### Consultas con parámetros {#cli-queries-with-parameters}
-
-Puede crear una consulta con parámetros y pasarles valores desde la aplicación cliente. Esto permite evitar formatear consultas con valores dinámicos específicos en el lado del cliente. Por ejemplo:
-
-``` bash
-$ clickhouse-client --param_parName="[1, 2]"  -q "SELECT * FROM table WHERE a = {parName:Array(UInt16)}"
-```
-
-#### Sintaxis de consulta {#cli-queries-with-parameters-syntax}
-
-Formatee una consulta como de costumbre, luego coloque los valores que desea pasar de los parámetros de la aplicación a la consulta entre llaves en el siguiente formato:
-
-``` sql
-{<name>:<data type>}
-```
-
--   `name` — Placeholder identifier. In the console client it should be used in app parameters as `--param_<name> = value`.
--   `data type` — [Tipo de datos](../sql-reference/data-types/index.md) del valor del parámetro de la aplicación. Por ejemplo, una estructura de datos como `(integer, ('string', integer))` puede tener el `Tuple(UInt8, Tuple(String, UInt8))` tipo de datos (también puede usar otro [entero](../sql-reference/data-types/int-uint.md) tipo).
-
-#### Ejemplo {#example}
-
-``` bash
-$ clickhouse-client --param_tuple_in_tuple="(10, ('dt', 10))" -q "SELECT * FROM table WHERE val = {tuple_in_tuple:Tuple(UInt8, Tuple(String, UInt8))}"
-```
-
-## Configuración {#interfaces_cli_configuration}
-
-Puede pasar parámetros a `clickhouse-client` (todos los parámetros tienen un valor predeterminado) usando:
-
--   Desde la línea de comandos
-
-    Las opciones de la línea de comandos anulan los valores y valores predeterminados de los archivos de configuración.
-
--   Archivos de configuración.
-
-    Los valores de los archivos de configuración anulan los valores predeterminados.
-
-### Opciones de línea de comandos {#command-line-options}
-
--   `--host, -h` -– The server name, ‘localhost’ predeterminada. Puede utilizar el nombre o la dirección IPv4 o IPv6.
--   `--port` – The port to connect to. Default value: 9000. Note that the HTTP interface and the native interface use different ports.
--   `--user, -u` – The username. Default value: default.
--   `--password` – The password. Default value: empty string.
--   `--query, -q` – The query to process when using non-interactive mode.
--   `--database, -d` – Select the current default database. Default value: the current database from the server settings (‘default’ predeterminada).
--   `--multiline, -m` – If specified, allow multiline queries (do not send the query on Enter).
--   `--multiquery, -n` – If specified, allow processing multiple queries separated by semicolons.
--   `--format, -f` – Use the specified default format to output the result.
--   `--vertical, -E` – If specified, use the Vertical format by default to output the result. This is the same as ‘–format=Vertical’. En este formato, cada valor se imprime en una línea separada, lo que es útil cuando se muestran tablas anchas.
--   `--time, -t` – If specified, print the query execution time to ‘stderr’ en modo no interactivo.
--   `--stacktrace` – If specified, also print the stack trace if an exception occurs.
--   `--config-file` – The name of the configuration file.
--   `--secure` – If specified, will connect to server over secure connection.
--   `--param_<name>` — Value for a [consulta con parámetros](#cli-queries-with-parameters).
-
-### Archivos de configuración {#configuration_files}
-
-`clickhouse-client` utiliza el primer archivo existente de los siguientes:
-
--   Definido en el `--config-file` parámetro.
--   `./clickhouse-client.xml`
--   `~/.clickhouse-client/config.xml`
--   `/etc/clickhouse-client/config.xml`
-
-Ejemplo de un archivo de configuración:
-
-``` xml
-<config>
-    <user>username</user>
-    <password>password</password>
-    <secure>False</secure>
-</config>
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/cli/) <!--hide-->
diff --git a/docs/es/interfaces/cpp.md b/docs/es/interfaces/cpp.md
deleted file mode 100644
index bc5dc3dbc247..000000000000
--- a/docs/es/interfaces/cpp.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 24
-toc_title: Biblioteca de clientes de C++
----
-
-# Biblioteca de clientes de C++ {#c-client-library}
-
-Ver README en [Bienvenidos](https://github.com/ClickHouse/clickhouse-cpp) repositorio.
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/cpp/) <!--hide-->
diff --git a/docs/es/interfaces/formats.md b/docs/es/interfaces/formats.md
deleted file mode 100644
index 03c1873d306d..000000000000
--- a/docs/es/interfaces/formats.md
+++ /dev/null
@@ -1,1212 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 21
-toc_title: Formatos de entrada y salida
----
-
-# Formatos para datos de entrada y salida {#formats}
-
-ClickHouse puede aceptar y devolver datos en varios formatos. Se puede utilizar un formato admitido para la entrada para analizar los datos proporcionados a `INSERT`s, para llevar a cabo `SELECT`s de una tabla respaldada por archivos como File, URL o HDFS, o para leer un diccionario externo. Se puede utilizar un formato compatible con la salida para organizar el
-resultados de un `SELECT`, y realizar `INSERT`s en una tabla respaldada por archivos.
-
-Los formatos soportados son:
-
-| Formato                                                         | Entrada | Salida |
-|-----------------------------------------------------------------|---------|--------|
-| [TabSeparated](#tabseparated)                                   | ✔       | ✔      |
-| [TabSeparatedRaw](#tabseparatedraw)                             | ✗       | ✔      |
-| [TabSeparatedWithNames](#tabseparatedwithnames)                 | ✔       | ✔      |
-| [TabSeparatedWithNamesAndTypes](#tabseparatedwithnamesandtypes) | ✔       | ✔      |
-| [Plantilla](#format-template)                                   | ✔       | ✔      |
-| [TemplateIgnoreSpaces](#templateignorespaces)                   | ✔       | ✗      |
-| [CSV](#csv)                                                     | ✔       | ✔      |
-| [CSVWithNames](#csvwithnames)                                   | ✔       | ✔      |
-| [CustomSeparated](#format-customseparated)                      | ✔       | ✔      |
-| [Valor](#data-format-values)                                    | ✔       | ✔      |
-| [Vertical](#vertical)                                           | ✗       | ✔      |
-| [VerticalRaw](#verticalraw)                                     | ✗       | ✔      |
-| [JSON](#json)                                                   | ✗       | ✔      |
-| [JSONCompact](#jsoncompact)                                     | ✗       | ✔      |
-| [JSONEachRow](#jsoneachrow)                                     | ✔       | ✔      |
-| [TSKV](#tskv)                                                   | ✔       | ✔      |
-| [Bastante](#pretty)                                             | ✗       | ✔      |
-| [PrettyCompact](#prettycompact)                                 | ✗       | ✔      |
-| [PrettyCompactMonoBlock](#prettycompactmonoblock)               | ✗       | ✔      |
-| [PrettyNoEscapes](#prettynoescapes)                             | ✗       | ✔      |
-| [Bienvenido a WordPress.](#prettyspace)                         | ✗       | ✔      |
-| [Protobuf](#protobuf)                                           | ✔       | ✔      |
-| [Avro](#data-format-avro)                                       | ✔       | ✔      |
-| [AvroConfluent](#data-format-avro-confluent)                    | ✔       | ✗      |
-| [Parquet](#data-format-parquet)                                 | ✔       | ✔      |
-| [ORC](#data-format-orc)                                         | ✔       | ✗      |
-| [RowBinary](#rowbinary)                                         | ✔       | ✔      |
-| [RowBinaryWithNamesAndTypes](#rowbinarywithnamesandtypes)       | ✔       | ✔      |
-| [Nativo](#native)                                               | ✔       | ✔      |
-| [Nulo](#null)                                                   | ✗       | ✔      |
-| [XML](#xml)                                                     | ✗       | ✔      |
-| [CapnProto](#capnproto)                                         | ✔       | ✗      |
-
-Puede controlar algunos parámetros de procesamiento de formato con la configuración de ClickHouse. Para obtener más información, lea el [Configuración](../operations/settings/settings.md) apartado.
-
-## TabSeparated {#tabseparated}
-
-En el formato TabSeparated, los datos se escriben por fila. Cada fila contiene valores separados por pestañas. Cada valor es seguido por una ficha, excepto el último valor de la fila, que es seguido por un avance de línea. Estrictamente las fuentes de línea Unix se asumen en todas partes. La última fila también debe contener un avance de línea al final. Los valores se escriben en formato de texto, sin incluir comillas y con caracteres especiales escapados.
-
-Este formato también está disponible bajo el nombre `TSV`.
-
-El `TabSeparated` es conveniente para procesar datos utilizando programas y scripts personalizados. Se usa de forma predeterminada en la interfaz HTTP y en el modo por lotes del cliente de línea de comandos. Este formato también permite transferir datos entre diferentes DBMS. Por ejemplo, puede obtener un volcado de MySQL y subirlo a ClickHouse, o viceversa.
-
-El `TabSeparated` el formato admite la salida de valores totales (cuando se usa WITH TOTALS) y valores extremos (cuando ‘extremes’ se establece en 1). En estos casos, los valores totales y los extremos se emiten después de los datos principales. El resultado principal, los valores totales y los extremos están separados entre sí por una línea vacía. Ejemplo:
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
-```
-
-``` text
-2014-03-17      1406958
-2014-03-18      1383658
-2014-03-19      1405797
-2014-03-20      1353623
-2014-03-21      1245779
-2014-03-22      1031592
-2014-03-23      1046491
-
-1970-01-01      8873898
-
-2014-03-17      1031592
-2014-03-23      1406958
-```
-
-### Formato de datos {#data-formatting}
-
-Los números enteros se escriben en forma decimal. Los números pueden contener un extra “+” carácter al principio (ignorado al analizar y no grabado al formatear). Los números no negativos no pueden contener el signo negativo. Al leer, se permite analizar una cadena vacía como cero, o (para tipos con signo) una cadena que consiste en solo un signo menos como cero. Los números que no encajan en el tipo de datos correspondiente se pueden analizar como un número diferente, sin un mensaje de error.
-
-Los números de punto flotante se escriben en forma decimal. El punto se usa como separador decimal. Las entradas exponenciales son compatibles, al igual que ‘inf’, ‘+inf’, ‘-inf’, y ‘nan’. Una entrada de números de coma flotante puede comenzar o terminar con un punto decimal.
-Durante el formateo, la precisión puede perderse en los números de coma flotante.
-Durante el análisis, no es estrictamente necesario leer el número representable de la máquina más cercano.
-
-Las fechas se escriben en formato AAAA-MM-DD y se analizan en el mismo formato, pero con los caracteres como separadores.
-Las fechas con horas se escriben en el formato `YYYY-MM-DD hh:mm:ss` y analizado en el mismo formato, pero con cualquier carácter como separadores.
-Todo esto ocurre en la zona horaria del sistema en el momento en que se inicia el cliente o servidor (dependiendo de cuál de ellos formatea los datos). Para fechas con horarios, no se especifica el horario de verano. Por lo tanto, si un volcado tiene tiempos durante el horario de verano, el volcado no coincide inequívocamente con los datos, y el análisis seleccionará una de las dos veces.
-Durante una operación de lectura, las fechas incorrectas y las fechas con horas se pueden analizar con desbordamiento natural o como fechas y horas nulas, sin un mensaje de error.
-
-Como excepción, el análisis de fechas con horas también se admite en el formato de marca de tiempo Unix, si consta de exactamente 10 dígitos decimales. El resultado no depende de la zona horaria. Los formatos AAAA-MM-DD hh:mm:ss y NNNNNNNNNN se diferencian automáticamente.
-
-Las cadenas se generan con caracteres especiales de escape de barra invertida. Las siguientes secuencias de escape se utilizan para la salida: `\b`, `\f`, `\r`, `
`, `\t`, `\0`, `\'`, `\\`. El análisis también admite las secuencias `\a`, `\v`, y `\xHH` (secuencias de escape hexagonales) y cualquier `\c` secuencias, donde `c` es cualquier carácter (estas secuencias se convierten en `c`). Por lo tanto, la lectura de datos admite formatos donde un avance de línea se puede escribir como `
` o `\` o como un avance de línea. Por ejemplo, la cadena `Hello world` con un avance de línea entre las palabras en lugar de espacio se puede analizar en cualquiera de las siguientes variaciones:
-
-``` text
-Hello
world
-
-Hello\
-world
-```
-
-La segunda variante es compatible porque MySQL la usa al escribir volcados separados por tabuladores.
-
-El conjunto mínimo de caracteres que debe escapar al pasar datos en formato TabSeparated: tabulación, salto de línea (LF) y barra invertida.
-
-Solo se escapa un pequeño conjunto de símbolos. Puede tropezar fácilmente con un valor de cadena que su terminal arruinará en la salida.
-
-Las matrices se escriben como una lista de valores separados por comas entre corchetes. Los elementos numéricos de la matriz tienen el formato normal. `Date` y `DateTime` están escritos entre comillas simples. Las cadenas se escriben entre comillas simples con las mismas reglas de escape que las anteriores.
-
-[NULL](../sql-reference/syntax.md) se formatea como `\N`.
-
-Cada elemento de [Anidar](../sql-reference/data-types/nested-data-structures/nested.md) estructuras se representa como una matriz.
-
-Por ejemplo:
-
-``` sql
-CREATE TABLE nestedt
-(
-    `id` UInt8,
-    `aux` Nested(
-        a UInt8,
-        b String
-    )
-)
-ENGINE = TinyLog
-```
-
-``` sql
-INSERT INTO nestedt Values ( 1, [1], ['a'])
-```
-
-``` sql
-SELECT * FROM nestedt FORMAT TSV
-```
-
-``` text
-1  [1]    ['a']
-```
-
-## TabSeparatedRaw {#tabseparatedraw}
-
-Difiere de `TabSeparated` formato en que las filas se escriben sin escapar.
-Este formato solo es apropiado para generar un resultado de consulta, pero no para analizar (recuperar datos para insertar en una tabla).
-
-Este formato también está disponible bajo el nombre `TSVRaw`.
-
-## TabSeparatedWithNames {#tabseparatedwithnames}
-
-Difiere de la `TabSeparated` formato en que los nombres de columna se escriben en la primera fila.
-Durante el análisis, la primera fila se ignora por completo. No puede usar nombres de columna para determinar su posición o para comprobar su corrección.
-(Se puede agregar soporte para analizar la fila de encabezado en el futuro.)
-
-Este formato también está disponible bajo el nombre `TSVWithNames`.
-
-## TabSeparatedWithNamesAndTypes {#tabseparatedwithnamesandtypes}
-
-Difiere de la `TabSeparated` formato en que los nombres de columna se escriben en la primera fila, mientras que los tipos de columna están en la segunda fila.
-Durante el análisis, la primera y la segunda filas se ignoran por completo.
-
-Este formato también está disponible bajo el nombre `TSVWithNamesAndTypes`.
-
-## Plantilla {#format-template}
-
-Este formato permite especificar una cadena de formato personalizado con marcadores de posición para los valores con una regla de escape especificada.
-
-Utiliza la configuración `format_template_resultset`, `format_template_row`, `format_template_rows_between_delimiter` and some settings of other formats (e.g. `output_format_json_quote_64bit_integers` cuando se utiliza `JSON` escapar, ver más)
-
-Configuración `format_template_row` especifica la ruta de acceso al archivo, que contiene una cadena de formato para las filas con la siguiente sintaxis:
-
-`delimiter_1${column_1:serializeAs_1}delimiter_2${column_2:serializeAs_2} ... delimiter_N`,
-
-donde `delimiter_i` es un delimitador entre valores (`$` símbolo se puede escapar como `$$`),
-`column_i` es un nombre o índice de una columna cuyos valores se deben seleccionar o insertar (si está vacío, se omitirá la columna),
-`serializeAs_i` es una regla de escape para los valores de columna. Se admiten las siguientes reglas de escape:
-
--   `CSV`, `JSON`, `XML` (similar a los formatos de los mismos nombres)
--   `Escaped` (similar a `TSV`)
--   `Quoted` (similar a `Values`)
--   `Raw` (sin escapar, de manera similar a `TSVRaw`)
--   `None` (sin regla de escape, ver más)
-
-Si se omite una regla de escape, entonces `None` se utilizará. `XML` y `Raw` son adecuados sólo para la salida.
-
-Entonces, para la siguiente cadena de formato:
-
-      `Search phrase: ${SearchPhrase:Quoted}, count: ${c:Escaped}, ad price: $$${price:JSON};`
-
-los valores de `SearchPhrase`, `c` y `price` columnas, que se escapan como `Quoted`, `Escaped` y `JSON` se imprimirá (para seleccionar) o se esperará (para insertar) entre `Search phrase:`, `, count:`, `, ad price: $` y `;` delimitadores respectivamente. Por ejemplo:
-
-`Search phrase: 'bathroom interior design', count: 2166, ad price: $3;`
-
-El `format_template_rows_between_delimiter` setting especifica el delimitador entre filas, que se imprime (o se espera) después de cada fila, excepto la última (`
` predeterminada)
-
-Configuración `format_template_resultset` especifica la ruta al archivo, que contiene una cadena de formato para el conjunto de resultados. La cadena de formato para el conjunto de resultados tiene la misma sintaxis que una cadena de formato para la fila y permite especificar un prefijo, un sufijo y una forma de imprimir información adicional. Contiene los siguientes marcadores de posición en lugar de nombres de columna:
-
--   `data` son las filas con datos en `format_template_row` formato, separados por `format_template_rows_between_delimiter`. Este marcador de posición debe ser el primer marcador de posición en la cadena de formato.
--   `totals` es la fila con valores totales en `format_template_row` formato (cuando se usa WITH TOTALS)
--   `min` es la fila con valores mínimos en `format_template_row` formato (cuando los extremos se establecen en 1)
--   `max` es la fila con valores máximos en `format_template_row` formato (cuando los extremos se establecen en 1)
--   `rows` es el número total de filas de salida
--   `rows_before_limit` es el número mínimo de filas que habría habido sin LIMIT. Salida solo si la consulta contiene LIMIT. Si la consulta contiene GROUP BY, rows_before_limit_at_least es el número exacto de filas que habría habido sin un LIMIT .
--   `time` es el tiempo de ejecución de la solicitud en segundos
--   `rows_read` es el número de filas que se ha leído
--   `bytes_read` es el número de bytes (sin comprimir) que se ha leído
-
-Marcador `data`, `totals`, `min` y `max` no debe tener una regla de escape especificada (o `None` debe especificarse explícitamente). Los marcadores de posición restantes pueden tener cualquier regla de escape especificada.
-Si el `format_template_resultset` valor es una cadena vacía, `${data}` se utiliza como valor predeterminado.
-Para el formato de consultas de inserción permite omitir algunas columnas o algunos campos si prefijo o sufijo (ver ejemplo).
-
-Seleccionar ejemplo:
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase ORDER BY c DESC LIMIT 5 FORMAT Template SETTINGS
-format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = '
    '
-```
-
-`/some/path/resultset.format`:
-
-``` text
-<!DOCTYPE HTML>
-<html> <head> <title>Search phrases</title> </head>
- <body>
-  <table border="1"> <caption>Search phrases</caption>
-    <tr> <th>Search phrase</th> <th>Count</th> </tr>
-    ${data}
-  </table>
-  <table border="1"> <caption>Max</caption>
-    ${max}
-  </table>
-  <b>Processed ${rows_read:XML} rows in ${time:XML} sec</b>
- </body>
-</html>
-```
-
-`/some/path/row.format`:
-
-``` text
-<tr> <td>${0:XML}</td> <td>${1:XML}</td> </tr>
-```
-
-Resultado:
-
-``` html
-<!DOCTYPE HTML>
-<html> <head> <title>Search phrases</title> </head>
- <body>
-  <table border="1"> <caption>Search phrases</caption>
-    <tr> <th>Search phrase</th> <th>Count</th> </tr>
-    <tr> <td></td> <td>8267016</td> </tr>
-    <tr> <td>bathroom interior design</td> <td>2166</td> </tr>
-    <tr> <td>yandex</td> <td>1655</td> </tr>
-    <tr> <td>spring 2014 fashion</td> <td>1549</td> </tr>
-    <tr> <td>freeform photos</td> <td>1480</td> </tr>
-  </table>
-  <table border="1"> <caption>Max</caption>
-    <tr> <td></td> <td>8873898</td> </tr>
-  </table>
-  <b>Processed 3095973 rows in 0.1569913 sec</b>
- </body>
-</html>
-```
-
-Insertar ejemplo:
-
-``` text
-Some header
-Page views: 5, User id: 4324182021466249494, Useless field: hello, Duration: 146, Sign: -1
-Page views: 6, User id: 4324182021466249494, Useless field: world, Duration: 185, Sign: 1
-Total rows: 2
-```
-
-``` sql
-INSERT INTO UserActivity FORMAT Template SETTINGS
-format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format'
-```
-
-`/some/path/resultset.format`:
-
-``` text
-Some header
${data}
Total rows: ${:CSV}

-```
-
-`/some/path/row.format`:
-
-``` text
-Page views: ${PageViews:CSV}, User id: ${UserID:CSV}, Useless field: ${:CSV}, Duration: ${Duration:CSV}, Sign: ${Sign:CSV}
-```
-
-`PageViews`, `UserID`, `Duration` y `Sign` dentro de los marcadores de posición son nombres de columnas en la tabla. Valores después `Useless field` en filas y después `
Total rows:` en el sufijo será ignorado.
-Todos los delimitadores de los datos de entrada deben ser estrictamente iguales a los delimitadores de las cadenas de formato especificadas.
-
-## TemplateIgnoreSpaces {#templateignorespaces}
-
-Este formato es adecuado sólo para la entrada.
-Similar a `Template`, pero omite caracteres de espacio en blanco entre delimitadores y valores en la secuencia de entrada. Sin embargo, si las cadenas de formato contienen caracteres de espacio en blanco, se esperarán estos caracteres en la secuencia de entrada. También permite especificar marcadores de posición vacíos (`${}` o `${:None}`) para dividir algún delimitador en partes separadas para ignorar los espacios entre ellos. Dichos marcadores de posición se usan solo para omitir caracteres de espacio en blanco.
-Es posible leer `JSON` usando este formato, si los valores de las columnas tienen el mismo orden en todas las filas. Por ejemplo, la siguiente solicitud se puede utilizar para insertar datos del ejemplo de salida de formato [JSON](#json):
-
-``` sql
-INSERT INTO table_name FORMAT TemplateIgnoreSpaces SETTINGS
-format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = ','
-```
-
-`/some/path/resultset.format`:
-
-``` text
-{${}"meta"${}:${:JSON},${}"data"${}:${}[${data}]${},${}"totals"${}:${:JSON},${}"extremes"${}:${:JSON},${}"rows"${}:${:JSON},${}"rows_before_limit_at_least"${}:${:JSON}${}}
-```
-
-`/some/path/row.format`:
-
-``` text
-{${}"SearchPhrase"${}:${}${phrase:JSON}${},${}"c"${}:${}${cnt:JSON}${}}
-```
-
-## TSKV {#tskv}
-
-Similar a TabSeparated , pero genera un valor en formato name=value . Los nombres se escapan de la misma manera que en el formato TabSeparated, y el símbolo = también se escapa.
-
-``` text
-SearchPhrase=   count()=8267016
-SearchPhrase=bathroom interior design    count()=2166
-SearchPhrase=yandex     count()=1655
-SearchPhrase=2014 spring fashion    count()=1549
-SearchPhrase=freeform photos       count()=1480
-SearchPhrase=angelina jolie    count()=1245
-SearchPhrase=omsk       count()=1112
-SearchPhrase=photos of dog breeds    count()=1091
-SearchPhrase=curtain designs        count()=1064
-SearchPhrase=baku       count()=1000
-```
-
-[NULL](../sql-reference/syntax.md) se formatea como `\N`.
-
-``` sql
-SELECT * FROM t_null FORMAT TSKV
-```
-
-``` text
-x=1    y=\N
-```
-
-Cuando hay una gran cantidad de columnas pequeñas, este formato no es efectivo y generalmente no hay razón para usarlo. Sin embargo, no es peor que JSONEachRow en términos de eficiencia.
-
-Both data output and parsing are supported in this format. For parsing, any order is supported for the values of different columns. It is acceptable for some values to be omitted – they are treated as equal to their default values. In this case, zeros and blank rows are used as default values. Complex values that could be specified in the table are not supported as defaults.
-
-El análisis permite la presencia del campo adicional `tskv` sin el signo igual o un valor. Este campo se ignora.
-
-## CSV {#csv}
-
-Formato de valores separados por comas ([RFC](https://tools.ietf.org/html/rfc4180)).
-
-Al formatear, las filas están encerradas en comillas dobles. Una comilla doble dentro de una cadena se genera como dos comillas dobles en una fila. No hay otras reglas para escapar de los personajes. Fecha y fecha-hora están encerrados en comillas dobles. Los números se emiten sin comillas. Los valores están separados por un carácter delimitador, que es `,` predeterminada. El carácter delimitador se define en la configuración [Formato_csv_delimiter](../operations/settings/settings.md#settings-format_csv_delimiter). Las filas se separan usando el avance de línea Unix (LF). Las matrices se serializan en CSV de la siguiente manera: primero, la matriz se serializa en una cadena como en el formato TabSeparated, y luego la cadena resultante se envía a CSV en comillas dobles. Las tuplas en formato CSV se serializan como columnas separadas (es decir, se pierde su anidamiento en la tupla).
-
-``` bash
-$ clickhouse-client --format_csv_delimiter="|" --query="INSERT INTO test.csv FORMAT CSV" < data.csv
-```
-
-\*De forma predeterminada, el delimitador es `,`. Ver el [Formato_csv_delimiter](../operations/settings/settings.md#settings-format_csv_delimiter) para obtener más información.
-
-Al analizar, todos los valores se pueden analizar con o sin comillas. Ambas comillas dobles y simples son compatibles. Las filas también se pueden organizar sin comillas. En este caso, se analizan hasta el carácter delimitador o el avance de línea (CR o LF). En violación del RFC, al analizar filas sin comillas, se ignoran los espacios y pestañas iniciales y finales. Para el avance de línea, se admiten los tipos Unix (LF), Windows (CR LF) y Mac OS Classic (CR LF).
-
-Los valores de entrada vacíos sin comillas se sustituyen por valores predeterminados para las columnas respectivas, si
-[Entrada_format_defaults_for_omitted_fields](../operations/settings/settings.md#session_settings-input_format_defaults_for_omitted_fields)
-está habilitado.
-
-`NULL` se formatea como `\N` o `NULL` o una cadena vacía sin comillas (consulte la configuración [input_format_csv_unquoted_null_literal_as_null](../operations/settings/settings.md#settings-input_format_csv_unquoted_null_literal_as_null) y [Entrada_format_defaults_for_omitted_fields](../operations/settings/settings.md#session_settings-input_format_defaults_for_omitted_fields)).
-
-El formato CSV admite la salida de totales y extremos de la misma manera que `TabSeparated`.
-
-## CSVWithNames {#csvwithnames}
-
-También imprime la fila del encabezado, similar a `TabSeparatedWithNames`.
-
-## CustomSeparated {#format-customseparated}
-
-Similar a [Plantilla](#format-template), pero imprime o lee todas las columnas y usa la regla de escape de la configuración `format_custom_escaping_rule` y delimitadores desde la configuración `format_custom_field_delimiter`, `format_custom_row_before_delimiter`, `format_custom_row_after_delimiter`, `format_custom_row_between_delimiter`, `format_custom_result_before_delimiter` y `format_custom_result_after_delimiter`, no de cadenas de formato.
-También hay `CustomSeparatedIgnoreSpaces` formato, que es similar a `TemplateIgnoreSpaces`.
-
-## JSON {#json}
-
-Salida de datos en formato JSON. Además de las tablas de datos, también genera nombres y tipos de columnas, junto con información adicional: el número total de filas de salida y el número de filas que podrían haberse generado si no hubiera un LIMIT . Ejemplo:
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON
-```
-
-``` json
-{
-        "meta":
-        [
-                {
-                        "name": "SearchPhrase",
-                        "type": "String"
-                },
-                {
-                        "name": "c",
-                        "type": "UInt64"
-                }
-        ],
-
-        "data":
-        [
-                {
-                        "SearchPhrase": "",
-                        "c": "8267016"
-                },
-                {
-                        "SearchPhrase": "bathroom interior design",
-                        "c": "2166"
-                },
-                {
-                        "SearchPhrase": "yandex",
-                        "c": "1655"
-                },
-                {
-                        "SearchPhrase": "spring 2014 fashion",
-                        "c": "1549"
-                },
-                {
-                        "SearchPhrase": "freeform photos",
-                        "c": "1480"
-                }
-        ],
-
-        "totals":
-        {
-                "SearchPhrase": "",
-                "c": "8873898"
-        },
-
-        "extremes":
-        {
-                "min":
-                {
-                        "SearchPhrase": "",
-                        "c": "1480"
-                },
-                "max":
-                {
-                        "SearchPhrase": "",
-                        "c": "8267016"
-                }
-        },
-
-        "rows": 5,
-
-        "rows_before_limit_at_least": 141137
-}
-```
-
-El JSON es compatible con JavaScript. Para garantizar esto, algunos caracteres se escapan adicionalmente: la barra inclinada `/` se escapa como `\/`; saltos de línea alternativos `U+2028` y `U+2029`, que rompen algunos navegadores, se escapan como `\uXXXX`. Los caracteres de control ASCII se escapan: retroceso, avance de formulario, avance de línea, retorno de carro y tabulación horizontal se reemplazan con `\b`, `\f`, `
`, `\r`, `\t` , así como los bytes restantes en el rango 00-1F usando `\uXXXX` sequences. Invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences. For compatibility with JavaScript, Int64 and UInt64 integers are enclosed in double-quotes by default. To remove the quotes, you can set the configuration parameter [output_format_json_quote_64bit_integers](../operations/settings/settings.md#session_settings-output_format_json_quote_64bit_integers) a 0.
-
-`rows` – The total number of output rows.
-
-`rows_before_limit_at_least` El número mínimo de filas habría sido sin LIMIT . Salida solo si la consulta contiene LIMIT.
-Si la consulta contiene GROUP BY, rows_before_limit_at_least es el número exacto de filas que habría habido sin un LIMIT .
-
-`totals` – Total values (when using WITH TOTALS).
-
-`extremes` – Extreme values (when extremes are set to 1).
-
-Este formato solo es apropiado para generar un resultado de consulta, pero no para analizar (recuperar datos para insertar en una tabla).
-
-Soporta ClickHouse [NULL](../sql-reference/syntax.md), que se muestra como `null` en la salida JSON.
-
-Ver también el [JSONEachRow](#jsoneachrow) formato.
-
-## JSONCompact {#jsoncompact}
-
-Difiere de JSON solo en que las filas de datos se generan en matrices, no en objetos.
-
-Ejemplo:
-
-``` json
-{
-        "meta":
-        [
-                {
-                        "name": "SearchPhrase",
-                        "type": "String"
-                },
-                {
-                        "name": "c",
-                        "type": "UInt64"
-                }
-        ],
-
-        "data":
-        [
-                ["", "8267016"],
-                ["bathroom interior design", "2166"],
-                ["yandex", "1655"],
-                ["fashion trends spring 2014", "1549"],
-                ["freeform photo", "1480"]
-        ],
-
-        "totals": ["","8873898"],
-
-        "extremes":
-        {
-                "min": ["","1480"],
-                "max": ["","8267016"]
-        },
-
-        "rows": 5,
-
-        "rows_before_limit_at_least": 141137
-}
-```
-
-Este formato solo es apropiado para generar un resultado de consulta, pero no para analizar (recuperar datos para insertar en una tabla).
-Ver también el `JSONEachRow` formato.
-
-## JSONEachRow {#jsoneachrow}
-
-Al usar este formato, ClickHouse genera filas como objetos JSON separados, delimitados por nuevas líneas, pero los datos en su conjunto no son JSON válidos.
-
-``` json
-{"SearchPhrase":"curtain designs","count()":"1064"}
-{"SearchPhrase":"baku","count()":"1000"}
-{"SearchPhrase":"","count()":"8267016"}
-```
-
-Al insertar los datos, debe proporcionar un objeto JSON independiente para cada fila.
-
-### Insertar datos {#inserting-data}
-
-``` sql
-INSERT INTO UserActivity FORMAT JSONEachRow {"PageViews":5, "UserID":"4324182021466249494", "Duration":146,"Sign":-1} {"UserID":"4324182021466249494","PageViews":6,"Duration":185,"Sign":1}
-```
-
-ClickHouse permite:
-
--   Cualquier orden de pares clave-valor en el objeto.
--   Omitiendo algunos valores.
-
-ClickHouse ignora los espacios entre los elementos y las comas después de los objetos. Puede pasar todos los objetos en una línea. No tiene que separarlos con saltos de línea.
-
-**Procesamiento de valores omitidos**
-
-ClickHouse sustituye los valores omitidos por los valores predeterminados para el [tipos de datos](../sql-reference/data-types/index.md).
-
-Si `DEFAULT expr` se especifica, ClickHouse utiliza diferentes reglas de sustitución dependiendo de la [Entrada_format_defaults_for_omitted_fields](../operations/settings/settings.md#session_settings-input_format_defaults_for_omitted_fields) configuración.
-
-Considere la siguiente tabla:
-
-``` sql
-CREATE TABLE IF NOT EXISTS example_table
-(
-    x UInt32,
-    a DEFAULT x * 2
-) ENGINE = Memory;
-```
-
--   Si `input_format_defaults_for_omitted_fields = 0`, entonces el valor predeterminado para `x` y `a` igual `0` (como el valor predeterminado para el `UInt32` tipo de datos).
--   Si `input_format_defaults_for_omitted_fields = 1`, entonces el valor predeterminado para `x` igual `0` pero el valor predeterminado de `a` igual `x * 2`.
-
-!!! note "Advertencia"
-    Al insertar datos con `insert_sample_with_metadata = 1`, ClickHouse consume más recursos computacionales, en comparación con la inserción con `insert_sample_with_metadata = 0`.
-
-### Selección de datos {#selecting-data}
-
-Considere el `UserActivity` tabla como un ejemplo:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-Consulta `SELECT * FROM UserActivity FORMAT JSONEachRow` devoluciones:
-
-``` text
-{"UserID":"4324182021466249494","PageViews":5,"Duration":146,"Sign":-1}
-{"UserID":"4324182021466249494","PageViews":6,"Duration":185,"Sign":1}
-```
-
-A diferencia de la [JSON](#json) formato, no hay sustitución de secuencias UTF-8 no válidas. Los valores se escapan de la misma manera que para `JSON`.
-
-!!! note "Nota"
-    Cualquier conjunto de bytes se puede generar en las cadenas. Utilice el `JSONEachRow` si está seguro de que los datos de la tabla se pueden formatear como JSON sin perder ninguna información.
-
-### Uso de estructuras anidadas {#jsoneachrow-nested}
-
-Si tienes una mesa con [Anidar](../sql-reference/data-types/nested-data-structures/nested.md) columnas de tipo de datos, puede insertar datos JSON con la misma estructura. Habilite esta función con el [Entrada_format_import_nested_json](../operations/settings/settings.md#settings-input_format_import_nested_json) configuración.
-
-Por ejemplo, considere la siguiente tabla:
-
-``` sql
-CREATE TABLE json_each_row_nested (n Nested (s String, i Int32) ) ENGINE = Memory
-```
-
-Como se puede ver en el `Nested` descripción del tipo de datos, ClickHouse trata cada componente de la estructura anidada como una columna separada (`n.s` y `n.i` para nuestra mesa). Puede insertar datos de la siguiente manera:
-
-``` sql
-INSERT INTO json_each_row_nested FORMAT JSONEachRow {"n.s": ["abc", "def"], "n.i": [1, 23]}
-```
-
-Para insertar datos como un objeto JSON jerárquico, establezca [input_format_import_nested_json=1](../operations/settings/settings.md#settings-input_format_import_nested_json).
-
-``` json
-{
-    "n": {
-        "s": ["abc", "def"],
-        "i": [1, 23]
-    }
-}
-```
-
-Sin esta configuración, ClickHouse produce una excepción.
-
-``` sql
-SELECT name, value FROM system.settings WHERE name = 'input_format_import_nested_json'
-```
-
-``` text
-┌─name────────────────────────────┬─value─┐
-│ input_format_import_nested_json │ 0     │
-└─────────────────────────────────┴───────┘
-```
-
-``` sql
-INSERT INTO json_each_row_nested FORMAT JSONEachRow {"n": {"s": ["abc", "def"], "i": [1, 23]}}
-```
-
-``` text
-Code: 117. DB::Exception: Unknown field found while parsing JSONEachRow format: n: (at row 1)
-```
-
-``` sql
-SET input_format_import_nested_json=1
-INSERT INTO json_each_row_nested FORMAT JSONEachRow {"n": {"s": ["abc", "def"], "i": [1, 23]}}
-SELECT * FROM json_each_row_nested
-```
-
-``` text
-┌─n.s───────────┬─n.i────┐
-│ ['abc','def'] │ [1,23] │
-└───────────────┴────────┘
-```
-
-## Nativo {#native}
-
-El formato más eficiente. Los datos son escritos y leídos por bloques en formato binario. Para cada bloque, el número de filas, número de columnas, nombres y tipos de columnas y partes de columnas de este bloque se registran una tras otra. En otras palabras, este formato es “columnar” – it doesn't convert columns to rows. This is the format used in the native interface for interaction between servers, for using the command-line client, and for C++ clients.
-
-Puede utilizar este formato para generar rápidamente volcados que sólo pueden ser leídos por el DBMS de ClickHouse. No tiene sentido trabajar con este formato usted mismo.
-
-## Nulo {#null}
-
-Nada es salida. Sin embargo, la consulta se procesa y, cuando se utiliza el cliente de línea de comandos, los datos se transmiten al cliente. Esto se usa para pruebas, incluidas las pruebas de rendimiento.
-Obviamente, este formato solo es apropiado para la salida, no para el análisis.
-
-## Bastante {#pretty}
-
-Salidas de datos como tablas de arte Unicode, también utilizando secuencias de escape ANSI para establecer colores en el terminal.
-Se dibuja una cuadrícula completa de la tabla, y cada fila ocupa dos líneas en la terminal.
-Cada bloque de resultados se muestra como una tabla separada. Esto es necesario para que los bloques se puedan generar sin resultados de almacenamiento en búfer (el almacenamiento en búfer sería necesario para calcular previamente el ancho visible de todos los valores).
-
-[NULL](../sql-reference/syntax.md) se emite como `ᴺᵁᴸᴸ`.
-
-Ejemplo (mostrado para el [PrettyCompact](#prettycompact) formato):
-
-``` sql
-SELECT * FROM t_null
-```
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-└───┴──────┘
-```
-
-Las filas no se escapan en formatos Pretty \*. Se muestra un ejemplo para el [PrettyCompact](#prettycompact) formato:
-
-``` sql
-SELECT 'String with \'quotes\' and \t character' AS Escaping_test
-```
-
-``` text
-┌─Escaping_test────────────────────────┐
-│ String with 'quotes' and      character │
-└──────────────────────────────────────┘
-```
-
-Para evitar volcar demasiados datos al terminal, solo se imprimen las primeras 10.000 filas. Si el número de filas es mayor o igual que 10.000, el mensaje “Showed first 10 000” se imprime.
-Este formato solo es apropiado para generar un resultado de consulta, pero no para analizar (recuperar datos para insertar en una tabla).
-
-El formato Pretty admite la salida de valores totales (cuando se usa WITH TOTALS) y extremos (cuando ‘extremes’ se establece en 1). En estos casos, los valores totales y los valores extremos se generan después de los datos principales, en tablas separadas. Ejemplo (mostrado para el [PrettyCompact](#prettycompact) formato):
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact
-```
-
-``` text
-┌──EventDate─┬───────c─┐
-│ 2014-03-17 │ 1406958 │
-│ 2014-03-18 │ 1383658 │
-│ 2014-03-19 │ 1405797 │
-│ 2014-03-20 │ 1353623 │
-│ 2014-03-21 │ 1245779 │
-│ 2014-03-22 │ 1031592 │
-│ 2014-03-23 │ 1046491 │
-└────────────┴─────────┘
-
-Totals:
-┌──EventDate─┬───────c─┐
-│ 1970-01-01 │ 8873898 │
-└────────────┴─────────┘
-
-Extremes:
-┌──EventDate─┬───────c─┐
-│ 2014-03-17 │ 1031592 │
-│ 2014-03-23 │ 1406958 │
-└────────────┴─────────┘
-```
-
-## PrettyCompact {#prettycompact}
-
-Difiere de [Bastante](#pretty) en que la cuadrícula se dibuja entre filas y el resultado es más compacto.
-Este formato se usa de forma predeterminada en el cliente de línea de comandos en modo interactivo.
-
-## PrettyCompactMonoBlock {#prettycompactmonoblock}
-
-Difiere de [PrettyCompact](#prettycompact) en que hasta 10,000 filas se almacenan en búfer, luego se salen como una sola tabla, no por bloques.
-
-## PrettyNoEscapes {#prettynoescapes}
-
-Difiere de Pretty en que las secuencias de escape ANSI no se usan. Esto es necesario para mostrar este formato en un navegador, así como para usar el ‘watch’ utilidad de línea de comandos.
-
-Ejemplo:
-
-``` bash
-$ watch -n1 "clickhouse-client --query='SELECT event, value FROM system.events FORMAT PrettyCompactNoEscapes'"
-```
-
-Puede usar la interfaz HTTP para mostrar en el navegador.
-
-### PrettyCompactNoEscapes {#prettycompactnoescapes}
-
-Lo mismo que el ajuste anterior.
-
-### PrettySpaceNoEscapes {#prettyspacenoescapes}
-
-Lo mismo que el ajuste anterior.
-
-## Bienvenido a WordPress {#prettyspace}
-
-Difiere de [PrettyCompact](#prettycompact) en ese espacio en blanco (caracteres de espacio) se usa en lugar de la cuadrícula.
-
-## RowBinary {#rowbinary}
-
-Formatea y analiza datos por fila en formato binario. Las filas y los valores se enumeran consecutivamente, sin separadores.
-Este formato es menos eficiente que el formato nativo, ya que está basado en filas.
-
-Los integradores usan una representación little-endian de longitud fija. Por ejemplo, UInt64 usa 8 bytes.
-DateTime se representa como UInt32 que contiene la marca de tiempo Unix como el valor.
-Date se representa como un objeto UInt16 que contiene el número de días desde 1970-01-01 como el valor.
-La cadena se representa como una longitud varint (sin signo [LEB128](https://en.wikipedia.org/wiki/LEB128)), seguido de los bytes de la cadena.
-FixedString se representa simplemente como una secuencia de bytes.
-
-La matriz se representa como una longitud varint (sin signo [LEB128](https://en.wikipedia.org/wiki/LEB128)), seguido de elementos sucesivos de la matriz.
-
-Para [NULL](../sql-reference/syntax.md#null-literal) soporte, se añade un byte adicional que contiene 1 o 0 antes de cada [NULL](../sql-reference/data-types/nullable.md) valor. Si 1, entonces el valor es `NULL` y este byte se interpreta como un valor separado. Si es 0, el valor después del byte no es `NULL`.
-
-## RowBinaryWithNamesAndTypes {#rowbinarywithnamesandtypes}
-
-Similar a [RowBinary](#rowbinary), pero con encabezado añadido:
-
--   [LEB128](https://en.wikipedia.org/wiki/LEB128)-número codificado de columnas (N)
--   N `String`s especificando nombres de columna
--   N `String`s especificando tipos de columna
-
-## Valor {#data-format-values}
-
-Imprime cada fila entre paréntesis. Las filas están separadas por comas. No hay coma después de la última fila. Los valores dentro de los corchetes también están separados por comas. Los números se emiten en formato decimal sin comillas. Las matrices se emiten entre corchetes. Las cadenas, fechas y fechas con horas se generan entre comillas. Las reglas de escape y el análisis son similares a las [TabSeparated](#tabseparated) formato. Durante el formateo, los espacios adicionales no se insertan, pero durante el análisis, se permiten y omiten (excepto los espacios dentro de los valores de la matriz, que no están permitidos). [NULL](../sql-reference/syntax.md) se representa como `NULL`.
-
-The minimum set of characters that you need to escape when passing data in Values ​​format: single quotes and backslashes.
-
-Este es el formato que se utiliza en `INSERT INTO t VALUES ...`, pero también puede usarlo para formatear los resultados de la consulta.
-
-Ver también: [input_format_values_interpret_expressions](../operations/settings/settings.md#settings-input_format_values_interpret_expressions) y [input_format_values_deduce_templates_of_expressions](../operations/settings/settings.md#settings-input_format_values_deduce_templates_of_expressions) configuración.
-
-## Vertical {#vertical}
-
-Imprime cada valor en una línea independiente con el nombre de columna especificado. Este formato es conveniente para imprimir solo una o varias filas si cada fila consta de un gran número de columnas.
-
-[NULL](../sql-reference/syntax.md) se emite como `ᴺᵁᴸᴸ`.
-
-Ejemplo:
-
-``` sql
-SELECT * FROM t_null FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-x: 1
-y: ᴺᵁᴸᴸ
-```
-
-Las filas no se escapan en formato vertical:
-
-``` sql
-SELECT 'string with \'quotes\' and \t with some special 
 characters' AS test FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-test: string with 'quotes' and      with some special
- characters
-```
-
-Este formato solo es apropiado para generar un resultado de consulta, pero no para analizar (recuperar datos para insertar en una tabla).
-
-## VerticalRaw {#verticalraw}
-
-Similar a [Vertical](#vertical), pero con escapar deshabilitado. Este formato solo es adecuado para generar resultados de consultas, no para analizar (recibir datos e insertarlos en la tabla).
-
-## XML {#xml}
-
-El formato XML es adecuado solo para la salida, no para el análisis. Ejemplo:
-
-``` xml
-<?xml version='1.0' encoding='UTF-8' ?>
-<result>
-        <meta>
-                <columns>
-                        <column>
-                                <name>SearchPhrase</name>
-                                <type>String</type>
-                        </column>
-                        <column>
-                                <name>count()</name>
-                                <type>UInt64</type>
-                        </column>
-                </columns>
-        </meta>
-        <data>
-                <row>
-                        <SearchPhrase></SearchPhrase>
-                        <field>8267016</field>
-                </row>
-                <row>
-                        <SearchPhrase>bathroom interior design</SearchPhrase>
-                        <field>2166</field>
-                </row>
-                <row>
-                        <SearchPhrase>yandex</SearchPhrase>
-                        <field>1655</field>
-                </row>
-                <row>
-                        <SearchPhrase>2014 spring fashion</SearchPhrase>
-                        <field>1549</field>
-                </row>
-                <row>
-                        <SearchPhrase>freeform photos</SearchPhrase>
-                        <field>1480</field>
-                </row>
-                <row>
-                        <SearchPhrase>angelina jolie</SearchPhrase>
-                        <field>1245</field>
-                </row>
-                <row>
-                        <SearchPhrase>omsk</SearchPhrase>
-                        <field>1112</field>
-                </row>
-                <row>
-                        <SearchPhrase>photos of dog breeds</SearchPhrase>
-                        <field>1091</field>
-                </row>
-                <row>
-                        <SearchPhrase>curtain designs</SearchPhrase>
-                        <field>1064</field>
-                </row>
-                <row>
-                        <SearchPhrase>baku</SearchPhrase>
-                        <field>1000</field>
-                </row>
-        </data>
-        <rows>10</rows>
-        <rows_before_limit_at_least>141137</rows_before_limit_at_least>
-</result>
-```
-
-Si el nombre de la columna no tiene un formato aceptable, simplemente ‘field’ se utiliza como el nombre del elemento. En general, la estructura XML sigue la estructura JSON.
-Just as for JSON, invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences.
-
-En los valores de cadena, los caracteres `<` y `&` se escaparon como `<` y `&`.
-
-Las matrices se emiten como `<array><elem>Hello</elem><elem>World</elem>...</array>`y tuplas como `<tuple><elem>Hello</elem><elem>World</elem>...</tuple>`.
-
-## CapnProto {#capnproto}
-
-Cap'n Proto es un formato de mensaje binario similar a Protocol Buffers y Thrift, pero no como JSON o MessagePack.
-
-Los mensajes de Cap'n Proto están estrictamente escritos y no autodescribidos, lo que significa que necesitan una descripción de esquema externo. El esquema se aplica sobre la marcha y se almacena en caché para cada consulta.
-
-``` bash
-$ cat capnproto_messages.bin | clickhouse-client --query "INSERT INTO test.hits FORMAT CapnProto SETTINGS format_schema='schema:Message'"
-```
-
-Donde `schema.capnp` se ve así:
-
-``` capnp
-struct Message {
-  SearchPhrase @0 :Text;
-  c @1 :Uint64;
-}
-```
-
-La deserialización es efectiva y generalmente no aumenta la carga del sistema.
-
-Ver también [Esquema de formato](#formatschema).
-
-## Protobuf {#protobuf}
-
-Protobuf - es un [Búferes de protocolo](https://developers.google.com/protocol-buffers/) formato.
-
-Este formato requiere un esquema de formato externo. El esquema se almacena en caché entre las consultas.
-ClickHouse soporta ambos `proto2` y `proto3` sintaxis. Se admiten campos repetidos / opcionales / requeridos.
-
-Ejemplos de uso:
-
-``` sql
-SELECT * FROM test.table FORMAT Protobuf SETTINGS format_schema = 'schemafile:MessageType'
-```
-
-``` bash
-cat protobuf_messages.bin | clickhouse-client --query "INSERT INTO test.table FORMAT Protobuf SETTINGS format_schema='schemafile:MessageType'"
-```
-
-donde el archivo `schemafile.proto` se ve así:
-
-``` capnp
-syntax = "proto3";
-
-message MessageType {
-  string name = 1;
-  string surname = 2;
-  uint32 birthDate = 3;
-  repeated string phoneNumbers = 4;
-};
-```
-
-Para encontrar la correspondencia entre las columnas de la tabla y los campos del tipo de mensaje de Protocol Buffers, ClickHouse compara sus nombres.
-Esta comparación no distingue entre mayúsculas y minúsculas y los caracteres `_` (subrayado) y `.` (punto) se consideran iguales.
-Si los tipos de una columna y un campo del mensaje de Protocol Buffers son diferentes, se aplica la conversión necesaria.
-
-Los mensajes anidados son compatibles. Por ejemplo, para el campo `z` en el siguiente tipo de mensaje
-
-``` capnp
-message MessageType {
-  message XType {
-    message YType {
-      int32 z;
-    };
-    repeated YType y;
-  };
-  XType x;
-};
-```
-
-ClickHouse intenta encontrar una columna llamada `x.y.z` (o `x_y_z` o `X.y_Z` y así sucesivamente).
-Los mensajes anidados son adecuados para [estructuras de datos anidados](../sql-reference/data-types/nested-data-structures/nested.md).
-
-Valores predeterminados definidos en un esquema protobuf como este
-
-``` capnp
-syntax = "proto2";
-
-message MessageType {
-  optional int32 result_per_page = 3 [default = 10];
-}
-```
-
-no se aplican; el [valores predeterminados de la tabla](../sql-reference/statements/create.md#create-default-values) se utilizan en lugar de ellos.
-
-ClickHouse entra y emite mensajes protobuf en el `length-delimited` formato.
-Significa que antes de cada mensaje debe escribirse su longitud como un [varint](https://developers.google.com/protocol-buffers/docs/encoding#varints).
-Ver también [cómo leer / escribir mensajes protobuf delimitados por longitud en idiomas populares](https://cwiki.apache.org/confluence/display/GEODE/Delimiting+Protobuf+Messages).
-
-## Avro {#data-format-avro}
-
-[Más información](http://avro.apache.org/) es un marco de serialización de datos orientado a filas desarrollado dentro del proyecto Hadoop de Apache.
-
-El formato ClickHouse Avro admite lectura y escritura [Archivos de datos Avro](http://avro.apache.org/docs/current/spec.html#Object+Container+Files).
-
-### Coincidencia de tipos de datos {#data_types-matching}
-
-La siguiente tabla muestra los tipos de datos admitidos y cómo coinciden con ClickHouse [tipos de datos](../sql-reference/data-types/index.md) en `INSERT` y `SELECT` consulta.
-
-| Tipo de datos Avro `INSERT`                 | Tipo de datos ClickHouse                                                                                                | Tipo de datos Avro `SELECT`  |
-|---------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|------------------------------|
-| `boolean`, `int`, `long`, `float`, `double` | [¿Cómo funciona?)](../sql-reference/data-types/int-uint.md), [UInt(8\|16\|32)](../sql-reference/data-types/int-uint.md) | `int`                        |
-| `boolean`, `int`, `long`, `float`, `double` | [Int64](../sql-reference/data-types/int-uint.md), [UInt64](../sql-reference/data-types/int-uint.md)                     | `long`                       |
-| `boolean`, `int`, `long`, `float`, `double` | [Float32](../sql-reference/data-types/float.md)                                                                         | `float`                      |
-| `boolean`, `int`, `long`, `float`, `double` | [Float64](../sql-reference/data-types/float.md)                                                                         | `double`                     |
-| `bytes`, `string`, `fixed`, `enum`          | [Cadena](../sql-reference/data-types/string.md)                                                                         | `bytes`                      |
-| `bytes`, `string`, `fixed`                  | [Cadena fija (N)](../sql-reference/data-types/fixedstring.md)                                                           | `fixed(N)`                   |
-| `enum`                                      | [Enum (8\|16)](../sql-reference/data-types/enum.md)                                                                     | `enum`                       |
-| `array(T)`                                  | [Matriz (T)](../sql-reference/data-types/array.md)                                                                      | `array(T)`                   |
-| `union(null, T)`, `union(T, null)`          | [Nivel de Cifrado WEP)](../sql-reference/data-types/date.md)                                                            | `union(null, T)`             |
-| `null`                                      | [Nullable (nada)](../sql-reference/data-types/special-data-types/nothing.md)                                            | `null`                       |
-| `int (date)` \*                             | [Fecha](../sql-reference/data-types/date.md)                                                                            | `int (date)` \*              |
-| `long (timestamp-millis)` \*                | [¿Qué puedes encontrar en Neodigit)](../sql-reference/data-types/datetime.md)                                           | `long (timestamp-millis)` \* |
-| `long (timestamp-micros)` \*                | [Cómo hacer esto?)](../sql-reference/data-types/datetime.md)                                                            | `long (timestamp-micros)` \* |
-
-\* [Tipos lógicos Avro](http://avro.apache.org/docs/current/spec.html#Logical+Types)
-
-Tipos de datos Avro no admitidos: `record` (no root), `map`
-
-Tipos de datos lógicos Avro no admitidos: `uuid`, `time-millis`, `time-micros`, `duration`
-
-### Insertar datos {#inserting-data-1}
-
-Para insertar datos de un archivo Avro en la tabla ClickHouse:
-
-``` bash
-$ cat file.avro | clickhouse-client --query="INSERT INTO {some_table} FORMAT Avro"
-```
-
-El esquema raíz del archivo Avro de entrada debe ser de `record` tipo.
-
-Para encontrar la correspondencia entre las columnas de la tabla y los campos de Avro esquema ClickHouse compara sus nombres. Esta comparación distingue entre mayúsculas y minúsculas.
-Los campos no utilizados se omiten.
-
-Los tipos de datos de las columnas de tabla ClickHouse pueden diferir de los campos correspondientes de los datos de Avro insertados. Al insertar datos, ClickHouse interpreta los tipos de datos de acuerdo con la tabla anterior y luego [elenco](../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) los datos al tipo de columna correspondiente.
-
-### Selección de datos {#selecting-data-1}
-
-Para seleccionar datos de la tabla ClickHouse en un archivo Avro:
-
-``` bash
-$ clickhouse-client --query="SELECT * FROM {some_table} FORMAT Avro" > file.avro
-```
-
-Los nombres de columna deben:
-
--   comenzar con `[A-Za-z_]`
--   posteriormente contienen sólo `[A-Za-z0-9_]`
-
-La compresión de archivos Avro de salida y el intervalo de sincronización se pueden configurar con [Sistema abierto.](../operations/settings/settings.md#settings-output_format_avro_codec) y [Sistema abierto.](../operations/settings/settings.md#settings-output_format_avro_sync_interval) respectivamente.
-
-## AvroConfluent {#data-format-avro-confluent}
-
-AvroConfluent admite la decodificación de mensajes Avro de un solo objeto comúnmente utilizados con [Kafka](https://kafka.apache.org/) y [Registro de Esquemas Confluentes](https://docs.confluent.io/current/schema-registry/index.html).
-
-Cada mensaje de Avro incrusta un id de esquema que se puede resolver en el esquema real con la ayuda del Registro de esquemas.
-
-Los esquemas se almacenan en caché una vez resueltos.
-
-La URL del registro de esquemas se configura con [Todos los derechos reservados.](../operations/settings/settings.md#settings-format_avro_schema_registry_url)
-
-### Coincidencia de tipos de datos {#data_types-matching-1}
-
-Lo mismo que [Avro](#data-format-avro)
-
-### Uso {#usage}
-
-Para verificar rápidamente la resolución del esquema, puede usar [Método de codificación de datos:](https://github.com/edenhill/kafkacat) con [Sistema abierto.](../operations/utilities/clickhouse-local.md#clickhouse-local):
-
-``` bash
-$ kafkacat -b kafka-broker  -C -t topic1 -o beginning -f '%s' -c 3 | clickhouse-local   --input-format AvroConfluent --format_avro_schema_registry_url 'http://schema-registry' -S "field1 Int64, field2 String"  -q 'select *  from table'
-1 a
-2 b
-3 c
-```
-
-Utilizar `AvroConfluent` con [Kafka](../engines/table-engines/integrations/kafka.md):
-
-``` sql
-CREATE TABLE topic1_stream
-(
-    field1 String,
-    field2 String
-)
-ENGINE = Kafka()
-SETTINGS
-kafka_broker_list = 'kafka-broker',
-kafka_topic_list = 'topic1',
-kafka_group_name = 'group1',
-kafka_format = 'AvroConfluent';
-
-SET format_avro_schema_registry_url = 'http://schema-registry';
-
-SELECT * FROM topic1_stream;
-```
-
-!!! note "Advertencia"
-    Configuración `format_avro_schema_registry_url` necesita ser configurado en `users.xml` para mantener su valor después de un reinicio.
-
-## Parquet {#data-format-parquet}
-
-[Apache Parquet](http://parquet.apache.org/) es un formato de almacenamiento columnar generalizado en el ecosistema Hadoop. ClickHouse admite operaciones de lectura y escritura para este formato.
-
-### Coincidencia de tipos de datos {#data_types-matching-2}
-
-La siguiente tabla muestra los tipos de datos admitidos y cómo coinciden con ClickHouse [tipos de datos](../sql-reference/data-types/index.md) en `INSERT` y `SELECT` consulta.
-
-| Tipo de datos de parquet (`INSERT`) | Tipo de datos ClickHouse                                  | Tipo de datos de parquet (`SELECT`) |
-|-------------------------------------|-----------------------------------------------------------|-------------------------------------|
-| `UINT8`, `BOOL`                     | [UInt8](../sql-reference/data-types/int-uint.md)          | `UINT8`                             |
-| `INT8`                              | [Int8](../sql-reference/data-types/int-uint.md)           | `INT8`                              |
-| `UINT16`                            | [UInt16](../sql-reference/data-types/int-uint.md)         | `UINT16`                            |
-| `INT16`                             | [Int16](../sql-reference/data-types/int-uint.md)          | `INT16`                             |
-| `UINT32`                            | [UInt32](../sql-reference/data-types/int-uint.md)         | `UINT32`                            |
-| `INT32`                             | [Int32](../sql-reference/data-types/int-uint.md)          | `INT32`                             |
-| `UINT64`                            | [UInt64](../sql-reference/data-types/int-uint.md)         | `UINT64`                            |
-| `INT64`                             | [Int64](../sql-reference/data-types/int-uint.md)          | `INT64`                             |
-| `FLOAT`, `HALF_FLOAT`               | [Float32](../sql-reference/data-types/float.md)           | `FLOAT`                             |
-| `DOUBLE`                            | [Float64](../sql-reference/data-types/float.md)           | `DOUBLE`                            |
-| `DATE32`                            | [Fecha](../sql-reference/data-types/date.md)              | `UINT16`                            |
-| `DATE64`, `TIMESTAMP`               | [FechaHora](../sql-reference/data-types/datetime.md)      | `UINT32`                            |
-| `STRING`, `BINARY`                  | [Cadena](../sql-reference/data-types/string.md)           | `STRING`                            |
-| —                                   | [Cadena fija](../sql-reference/data-types/fixedstring.md) | `STRING`                            |
-| `DECIMAL`                           | [Decimal](../sql-reference/data-types/decimal.md)         | `DECIMAL`                           |
-
-ClickHouse admite una precisión configurable de `Decimal` tipo. El `INSERT` consulta trata el Parquet `DECIMAL` tipo como el ClickHouse `Decimal128` tipo.
-
-Tipos de datos de parquet no admitidos: `DATE32`, `TIME32`, `FIXED_SIZE_BINARY`, `JSON`, `UUID`, `ENUM`.
-
-Los tipos de datos de las columnas de tabla ClickHouse pueden diferir de los campos correspondientes de los datos de Parquet insertados. Al insertar datos, ClickHouse interpreta los tipos de datos de acuerdo con la tabla anterior y luego [elenco](../query_language/functions/type_conversion_functions/#type_conversion_function-cast) los datos de ese tipo de datos que se establece para la columna de tabla ClickHouse.
-
-### Insertar y seleccionar datos {#inserting-and-selecting-data}
-
-Puede insertar datos de Parquet desde un archivo en la tabla ClickHouse mediante el siguiente comando:
-
-``` bash
-$ cat {filename} | clickhouse-client --query="INSERT INTO {some_table} FORMAT Parquet"
-```
-
-Puede seleccionar datos de una tabla ClickHouse y guardarlos en algún archivo en el formato Parquet mediante el siguiente comando:
-
-``` bash
-$ clickhouse-client --query="SELECT * FROM {some_table} FORMAT Parquet" > {some_file.pq}
-```
-
-Para intercambiar datos con Hadoop, puede usar [Motor de mesa HDFS](../engines/table-engines/integrations/hdfs.md).
-
-## ORC {#data-format-orc}
-
-[Apache ORC](https://orc.apache.org/) es un formato de almacenamiento columnar generalizado en el ecosistema Hadoop. Solo puede insertar datos en este formato en ClickHouse.
-
-### Coincidencia de tipos de datos {#data_types-matching-3}
-
-La siguiente tabla muestra los tipos de datos admitidos y cómo coinciden con ClickHouse [tipos de datos](../sql-reference/data-types/index.md) en `INSERT` consulta.
-
-| Tipo de datos ORC (`INSERT`) | Tipo de datos ClickHouse                             |
-|------------------------------|------------------------------------------------------|
-| `UINT8`, `BOOL`              | [UInt8](../sql-reference/data-types/int-uint.md)     |
-| `INT8`                       | [Int8](../sql-reference/data-types/int-uint.md)      |
-| `UINT16`                     | [UInt16](../sql-reference/data-types/int-uint.md)    |
-| `INT16`                      | [Int16](../sql-reference/data-types/int-uint.md)     |
-| `UINT32`                     | [UInt32](../sql-reference/data-types/int-uint.md)    |
-| `INT32`                      | [Int32](../sql-reference/data-types/int-uint.md)     |
-| `UINT64`                     | [UInt64](../sql-reference/data-types/int-uint.md)    |
-| `INT64`                      | [Int64](../sql-reference/data-types/int-uint.md)     |
-| `FLOAT`, `HALF_FLOAT`        | [Float32](../sql-reference/data-types/float.md)      |
-| `DOUBLE`                     | [Float64](../sql-reference/data-types/float.md)      |
-| `DATE32`                     | [Fecha](../sql-reference/data-types/date.md)         |
-| `DATE64`, `TIMESTAMP`        | [FechaHora](../sql-reference/data-types/datetime.md) |
-| `STRING`, `BINARY`           | [Cadena](../sql-reference/data-types/string.md)      |
-| `DECIMAL`                    | [Decimal](../sql-reference/data-types/decimal.md)    |
-
-ClickHouse soporta la precisión configurable de la `Decimal` tipo. El `INSERT` consulta trata el ORC `DECIMAL` tipo como el ClickHouse `Decimal128` tipo.
-
-Tipos de datos ORC no admitidos: `DATE32`, `TIME32`, `FIXED_SIZE_BINARY`, `JSON`, `UUID`, `ENUM`.
-
-Los tipos de datos de las columnas de tabla ClickHouse no tienen que coincidir con los campos de datos ORC correspondientes. Al insertar datos, ClickHouse interpreta los tipos de datos de acuerdo con la tabla anterior y luego [elenco](../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) los datos al tipo de datos establecido para la columna de tabla ClickHouse.
-
-### Insertar datos {#inserting-data-2}
-
-Puede insertar datos ORC de un archivo en la tabla ClickHouse mediante el siguiente comando:
-
-``` bash
-$ cat filename.orc | clickhouse-client --query="INSERT INTO some_table FORMAT ORC"
-```
-
-Para intercambiar datos con Hadoop, puede usar [Motor de mesa HDFS](../engines/table-engines/integrations/hdfs.md).
-
-## Esquema de formato {#formatschema}
-
-El valor establece el nombre de archivo que contiene el esquema de formato `format_schema`.
-Es necesario establecer esta configuración cuando se utiliza uno de los formatos `Cap'n Proto` y `Protobuf`.
-El esquema de formato es una combinación de un nombre de archivo y el nombre de un tipo de mensaje en este archivo, delimitado por dos puntos,
-e.g. `schemafile.proto:MessageType`.
-Si el archivo tiene la extensión estándar para el formato (por ejemplo, `.proto` para `Protobuf`),
-se puede omitir y en este caso, el esquema de formato se ve así `schemafile:MessageType`.
-
-Si introduce o emite datos a través del [cliente](../interfaces/cli.md) en el [modo interactivo](../interfaces/cli.md#cli_usage), el nombre de archivo especificado en el esquema de formato
-puede contener una ruta absoluta o una ruta relativa al directorio actual en el cliente.
-Si utiliza el cliente en el [modo por lotes](../interfaces/cli.md#cli_usage), la ruta de acceso al esquema debe ser relativa por razones de seguridad.
-
-Si introduce o emite datos a través del [Interfaz HTTP](../interfaces/http.md) el nombre de archivo especificado en el esquema de formato
-debe estar ubicado en el directorio especificado en [format_schema_path](../operations/server-configuration-parameters/settings.md#server_configuration_parameters-format_schema_path)
-en la configuración del servidor.
-
-## Salto de errores {#skippingerrors}
-
-Algunos formatos como `CSV`, `TabSeparated`, `TSKV`, `JSONEachRow`, `Template`, `CustomSeparated` y `Protobuf` puede omitir la fila rota si se produjo un error de análisis y continuar el análisis desde el comienzo de la siguiente fila. Ver [Entrada_format_allow_errors_num](../operations/settings/settings.md#settings-input_format_allow_errors_num) y
-[Entrada_format_allow_errors_ratio](../operations/settings/settings.md#settings-input_format_allow_errors_ratio) configuración.
-Limitacion:
-- En caso de error de análisis `JSONEachRow` omite todos los datos hasta la nueva línea (o EOF), por lo que las filas deben estar delimitadas por `
` para contar los errores correctamente.
-- `Template` y `CustomSeparated` use el delimitador después de la última columna y el delimitador entre filas para encontrar el comienzo de la siguiente fila, por lo que omitir errores solo funciona si al menos uno de ellos no está vacío.
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/formats/) <!--hide-->
diff --git a/docs/es/interfaces/http.md b/docs/es/interfaces/http.md
deleted file mode 100644
index ab510a268e3c..000000000000
--- a/docs/es/interfaces/http.md
+++ /dev/null
@@ -1,617 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 19
-toc_title: Interfaz HTTP
----
-
-# Interfaz HTTP {#http-interface}
-
-La interfaz HTTP le permite usar ClickHouse en cualquier plataforma desde cualquier lenguaje de programación. Lo usamos para trabajar desde Java y Perl, así como scripts de shell. En otros departamentos, la interfaz HTTP se usa desde Perl, Python y Go. La interfaz HTTP es más limitada que la interfaz nativa, pero tiene una mejor compatibilidad.
-
-De forma predeterminada, clickhouse-server escucha HTTP en el puerto 8123 (esto se puede cambiar en la configuración).
-
-Si realiza una solicitud GET / sin parámetros, devuelve 200 códigos de respuesta y la cadena que definió en [http_server_default_response](../operations/server-configuration-parameters/settings.md#server_configuration_parameters-http_server_default_response) valor predeterminado “Ok.” (con un avance de línea al final)
-
-``` bash
-$ curl 'http://localhost:8123/'
-Ok.
-```
-
-Use la solicitud GET / ping en los scripts de comprobación de estado. Este controlador siempre devuelve “Ok.” (con un avance de línea al final). Disponible a partir de la versión 18.12.13.
-
-``` bash
-$ curl 'http://localhost:8123/ping'
-Ok.
-```
-
-Enviar la solicitud como una URL ‘query’ parámetro, o como un POST. O envíe el comienzo de la consulta en el ‘query’ parámetro, y el resto en el POST (explicaremos más adelante por qué esto es necesario). El tamaño de la URL está limitado a 16 KB, así que tenga esto en cuenta al enviar consultas grandes.
-
-Si tiene éxito, recibirá el código de respuesta 200 y el resultado en el cuerpo de respuesta.
-Si se produce un error, recibirá el código de respuesta 500 y un texto de descripción de error en el cuerpo de la respuesta.
-
-Al usar el método GET, ‘readonly’ se establece. En otras palabras, para consultas que modifican datos, solo puede usar el método POST. Puede enviar la consulta en sí misma en el cuerpo POST o en el parámetro URL.
-
-Ejemplos:
-
-``` bash
-$ curl 'http://localhost:8123/?query=SELECT%201'
-1
-
-$ wget -nv -O- 'http://localhost:8123/?query=SELECT 1'
-1
-
-$ echo -ne 'GET /?query=SELECT%201 HTTP/1.0\r
\r
' | nc localhost 8123
-HTTP/1.0 200 OK
-Date: Wed, 27 Nov 2019 10:30:18 GMT
-Connection: Close
-Content-Type: text/tab-separated-values; charset=UTF-8
-X-ClickHouse-Server-Display-Name: clickhouse.ru-central1.internal
-X-ClickHouse-Query-Id: 5abe861c-239c-467f-b955-8a201abb8b7f
-X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-
-1
-```
-
-Como puede ver, curl es algo inconveniente ya que los espacios deben ser URL escapadas.
-Aunque wget escapa de todo en sí, no recomendamos usarlo porque no funciona bien sobre HTTP 1.1 cuando se usa keep-alive y Transfer-Encoding: chunked .
-
-``` bash
-$ echo 'SELECT 1' | curl 'http://localhost:8123/' --data-binary @-
-1
-
-$ echo 'SELECT 1' | curl 'http://localhost:8123/?query=' --data-binary @-
-1
-
-$ echo '1' | curl 'http://localhost:8123/?query=SELECT' --data-binary @-
-1
-```
-
-Si se envía parte de la consulta en el parámetro y parte en el POST, se inserta un avance de línea entre estas dos partes de datos.
-Ejemplo (esto no funcionará):
-
-``` bash
-$ echo 'ECT 1' | curl 'http://localhost:8123/?query=SEL' --data-binary @-
-Code: 59, e.displayText() = DB::Exception: Syntax error: failed at position 0: SEL
-ECT 1
-, expected One of: SHOW TABLES, SHOW DATABASES, SELECT, INSERT, CREATE, ATTACH, RENAME, DROP, DETACH, USE, SET, OPTIMIZE., e.what() = DB::Exception
-```
-
-De forma predeterminada, los datos se devuelven en formato TabSeparated (para obtener más información, “Formats” apartado).
-Utilice la cláusula FORMAT de la consulta para solicitar cualquier otro formato.
-
-``` bash
-$ echo 'SELECT 1 FORMAT Pretty' | curl 'http://localhost:8123/?' --data-binary @-
-┏━━━┓
-┃ 1 ┃
-┡━━━┩
-│ 1 │
-└───┘
-```
-
-El método POST de transmitir datos es necesario para las consultas INSERT. En este caso, puede escribir el comienzo de la consulta en el parámetro URL y usar POST para pasar los datos a insertar. Los datos a insertar podrían ser, por ejemplo, un volcado separado por tabuladores de MySQL. De esta manera, la consulta INSERT reemplaza LOAD DATA LOCAL INFILE de MySQL.
-
-Ejemplos: Crear una tabla:
-
-``` bash
-$ echo 'CREATE TABLE t (a UInt8) ENGINE = Memory' | curl 'http://localhost:8123/' --data-binary @-
-```
-
-Uso de la consulta INSERT familiar para la inserción de datos:
-
-``` bash
-$ echo 'INSERT INTO t VALUES (1),(2),(3)' | curl 'http://localhost:8123/' --data-binary @-
-```
-
-Los datos se pueden enviar por separado de la consulta:
-
-``` bash
-$ echo '(4),(5),(6)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20VALUES' --data-binary @-
-```
-
-Puede especificar cualquier formato de datos. El ‘Values’ el formato es el mismo que el que se usa al escribir INSERT INTO t VALUES:
-
-``` bash
-$ echo '(7),(8),(9)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20FORMAT%20Values' --data-binary @-
-```
-
-Para insertar datos de un volcado separado por tabuladores, especifique el formato correspondiente:
-
-``` bash
-$ echo -ne '10
11
12
' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20FORMAT%20TabSeparated' --data-binary @-
-```
-
-Lectura del contenido de la tabla. Los datos se emiten en orden aleatorio debido al procesamiento de consultas paralelas:
-
-``` bash
-$ curl 'http://localhost:8123/?query=SELECT%20a%20FROM%20t'
-7
-8
-9
-10
-11
-12
-1
-2
-3
-4
-5
-6
-```
-
-Eliminando la mesa.
-
-``` bash
-$ echo 'DROP TABLE t' | curl 'http://localhost:8123/' --data-binary @-
-```
-
-Para las solicitudes correctas que no devuelven una tabla de datos, se devuelve un cuerpo de respuesta vacío.
-
-Puede utilizar el formato interno de compresión ClickHouse al transmitir datos. Los datos comprimidos tienen un formato no estándar, y deberá usar el `clickhouse-compressor` programa para trabajar con él (se instala con el `clickhouse-client` paquete). Para aumentar la eficiencia de la inserción de datos, puede deshabilitar la verificación de suma de comprobación [http_native_compression_disable_checksumming_on_decompress](../operations/settings/settings.md#settings-http_native_compression_disable_checksumming_on_decompress) configuración.
-
-Si ha especificado `compress=1` en la URL, el servidor comprime los datos que le envía.
-Si ha especificado `decompress=1` en la dirección URL, el servidor descomprime los mismos datos que `POST` método.
-
-También puede optar por utilizar [Compresión HTTP](https://en.wikipedia.org/wiki/HTTP_compression). Para enviar un `POST` solicitud, agregue el encabezado de solicitud `Content-Encoding: compression_method`. Para que ClickHouse comprima la respuesta, debe agregar `Accept-Encoding: compression_method`. Soporta ClickHouse `gzip`, `br`, y `deflate` [métodos de compresión](https://en.wikipedia.org/wiki/HTTP_compression#Content-Encoding_tokens). Para habilitar la compresión HTTP, debe usar ClickHouse [enable_http_compression](../operations/settings/settings.md#settings-enable_http_compression) configuración. Puede configurar el nivel de compresión de datos [http_zlib_compression_level](#settings-http_zlib_compression_level) para todos los métodos de compresión.
-
-Puede usar esto para reducir el tráfico de red al transmitir una gran cantidad de datos o para crear volcados que se comprimen inmediatamente.
-
-Ejemplos de envío de datos con compresión:
-
-``` bash
-#Sending data to the server:
-$ curl -vsS "http://localhost:8123/?enable_http_compression=1" -d 'SELECT number FROM system.numbers LIMIT 10' -H 'Accept-Encoding: gzip'
-
-#Sending data to the client:
-$ echo "SELECT 1" | gzip -c | curl -sS --data-binary @- -H 'Content-Encoding: gzip' 'http://localhost:8123/'
-```
-
-!!! note "Nota"
-    Algunos clientes HTTP pueden descomprimir datos del servidor de forma predeterminada (con `gzip` y `deflate`) y puede obtener datos descomprimidos incluso si usa la configuración de compresión correctamente.
-
-Puede usar el ‘database’ Parámetro URL para especificar la base de datos predeterminada.
-
-``` bash
-$ echo 'SELECT number FROM numbers LIMIT 10' | curl 'http://localhost:8123/?database=system' --data-binary @-
-0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-```
-
-De forma predeterminada, la base de datos que está registrada en la configuración del servidor se utiliza como base de datos predeterminada. De forma predeterminada, esta es la base de datos llamada ‘default’. Como alternativa, siempre puede especificar la base de datos utilizando un punto antes del nombre de la tabla.
-
-El nombre de usuario y la contraseña se pueden indicar de una de estas tres maneras:
-
-1.  Uso de la autenticación básica HTTP. Ejemplo:
-
-<!-- -->
-
-``` bash
-$ echo 'SELECT 1' | curl 'http://user:password@localhost:8123/' -d @-
-```
-
-1.  En el ‘user’ y ‘password’ Parámetros de URL. Ejemplo:
-
-<!-- -->
-
-``` bash
-$ echo 'SELECT 1' | curl 'http://localhost:8123/?user=user&password=password' -d @-
-```
-
-1.  Utilizar ‘X-ClickHouse-User’ y ‘X-ClickHouse-Key’ cabecera. Ejemplo:
-
-<!-- -->
-
-``` bash
-$ echo 'SELECT 1' | curl -H 'X-ClickHouse-User: user' -H 'X-ClickHouse-Key: password' 'http://localhost:8123/' -d @-
-```
-
-Si no se especifica el nombre de usuario, `default` se utiliza el nombre. Si no se especifica la contraseña, se utiliza la contraseña vacía.
-También puede utilizar los parámetros de URL para especificar cualquier configuración para procesar una sola consulta o perfiles completos de configuración. Ejemplo:http://localhost:8123/?perfil=web&max_rows_to_read=1000000000&consulta=SELECCIONA+1
-
-Para obtener más información, consulte [Configuración](../operations/settings/index.md) apartado.
-
-``` bash
-$ echo 'SELECT number FROM system.numbers LIMIT 10' | curl 'http://localhost:8123/?' --data-binary @-
-0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-```
-
-Para obtener información sobre otros parámetros, consulte la sección “SET”.
-
-Del mismo modo, puede utilizar sesiones ClickHouse en el protocolo HTTP. Para hacer esto, debe agregar el `session_id` GET parámetro a la solicitud. Puede usar cualquier cadena como ID de sesión. De forma predeterminada, la sesión finaliza después de 60 segundos de inactividad. Para cambiar este tiempo de espera, modifique `default_session_timeout` configuración en la configuración del servidor, o `session_timeout` GET parámetro a la solicitud. Para comprobar el estado de la sesión, `session_check=1` parámetro. Solo se puede ejecutar una consulta a la vez en una sola sesión.
-
-Puede recibir información sobre el progreso de una consulta en `X-ClickHouse-Progress` encabezados de respuesta. Para hacer esto, habilite [send_progress_in_http_headers](../operations/settings/settings.md#settings-send_progress_in_http_headers). Ejemplo de la secuencia de encabezado:
-
-``` text
-X-ClickHouse-Progress: {"read_rows":"2752512","read_bytes":"240570816","total_rows_to_read":"8880128"}
-X-ClickHouse-Progress: {"read_rows":"5439488","read_bytes":"482285394","total_rows_to_read":"8880128"}
-X-ClickHouse-Progress: {"read_rows":"8783786","read_bytes":"819092887","total_rows_to_read":"8880128"}
-```
-
-Posibles campos de encabezado:
-
--   `read_rows` — Number of rows read.
--   `read_bytes` — Volume of data read in bytes.
--   `total_rows_to_read` — Total number of rows to be read.
--   `written_rows` — Number of rows written.
--   `written_bytes` — Volume of data written in bytes.
-
-Las solicitudes en ejecución no se detienen automáticamente si se pierde la conexión HTTP. El análisis y el formato de datos se realizan en el lado del servidor, y el uso de la red puede ser ineficaz.
-Opcional ‘query_id’ parámetro se puede pasar como el ID de consulta (cualquier cadena). Para obtener más información, consulte la sección “Settings, replace_running_query”.
-
-Opcional ‘quota_key’ parámetro se puede pasar como la clave de cuota (cualquier cadena). Para obtener más información, consulte la sección “Quotas”.
-
-La interfaz HTTP permite pasar datos externos (tablas temporales externas) para consultar. Para obtener más información, consulte la sección “External data for query processing”.
-
-## Almacenamiento en búfer de respuesta {#response-buffering}
-
-Puede habilitar el almacenamiento en búfer de respuestas en el lado del servidor. El `buffer_size` y `wait_end_of_query` Los parámetros URL se proporcionan para este propósito.
-
-`buffer_size` determina el número de bytes en el resultado para almacenar en búfer en la memoria del servidor. Si un cuerpo de resultado es mayor que este umbral, el búfer se escribe en el canal HTTP y los datos restantes se envían directamente al canal HTTP.
-
-Para asegurarse de que toda la respuesta se almacena en búfer, establezca `wait_end_of_query=1`. En este caso, los datos que no se almacenan en la memoria se almacenarán en un archivo de servidor temporal.
-
-Ejemplo:
-
-``` bash
-$ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&buffer_size=3000000&wait_end_of_query=1' -d 'SELECT toUInt8(number) FROM system.numbers LIMIT 9000000 FORMAT RowBinary'
-```
-
-Utilice el almacenamiento en búfer para evitar situaciones en las que se produjo un error de procesamiento de consultas después de enviar al cliente el código de respuesta y los encabezados HTTP. En esta situación, se escribe un mensaje de error al final del cuerpo de la respuesta y, en el lado del cliente, el error solo se puede detectar en la etapa de análisis.
-
-### Consultas con parámetros {#cli-queries-with-parameters}
-
-Puede crear una consulta con parámetros y pasar valores para ellos desde los parámetros de solicitud HTTP correspondientes. Para obtener más información, consulte [Consultas con parámetros para CLI](cli.md#cli-queries-with-parameters).
-
-### Ejemplo {#example}
-
-``` bash
-$ curl -sS "<address>?param_id=2&param_phrase=test" -d "SELECT * FROM table WHERE int_column = {id:UInt8} and string_column = {phrase:String}"
-```
-
-## Interfaz HTTP predefinida {#predefined_http_interface}
-
-ClickHouse admite consultas específicas a través de la interfaz HTTP. Por ejemplo, puede escribir datos en una tabla de la siguiente manera:
-
-``` bash
-$ echo '(4),(5),(6)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20VALUES' --data-binary @-
-```
-
-ClickHouse también es compatible con la interfaz HTTP predefinida que puede ayudarle a una integración más fácil con herramientas de terceros como [Prometheus exportador](https://github.com/percona-lab/clickhouse_exporter).
-
-Ejemplo:
-
--   En primer lugar, agregue esta sección al archivo de configuración del servidor:
-
-<!-- -->
-
-``` xml
-<http_handlers>
-    <rule>
-        <url>/predefined_query</url>
-        <methods>POST,GET</methods>
-        <handler>
-            <type>predefined_query_handler</type>
-            <query>SELECT * FROM system.metrics LIMIT 5 FORMAT Template SETTINGS format_template_resultset = 'prometheus_template_output_format_resultset', format_template_row = 'prometheus_template_output_format_row', format_template_rows_between_delimiter = '
'</query>
-        </handler>
-    </rule>
-    <rule>...</rule>
-    <rule>...</rule>
-</http_handlers>
-```
-
--   Ahora puede solicitar la url directamente para los datos en el formato Prometheus:
-
-<!-- -->
-
-``` bash
-$ curl -v 'http://localhost:8123/predefined_query'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /predefined_query HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
->
-< HTTP/1.1 200 OK
-< Date: Tue, 28 Apr 2020 08:52:56 GMT
-< Connection: Keep-Alive
-< Content-Type: text/plain; charset=UTF-8
-< X-ClickHouse-Server-Display-Name: i-mloy5trc
-< Transfer-Encoding: chunked
-< X-ClickHouse-Query-Id: 96fe0052-01e6-43ce-b12a-6b7370de6e8a
-< X-ClickHouse-Format: Template
-< X-ClickHouse-Timezone: Asia/Shanghai
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-# HELP "Query" "Number of executing queries"
-# TYPE "Query" counter
-"Query" 1
-
-# HELP "Merge" "Number of executing background merges"
-# TYPE "Merge" counter
-"Merge" 0
-
-# HELP "PartMutation" "Number of mutations (ALTER DELETE/UPDATE)"
-# TYPE "PartMutation" counter
-"PartMutation" 0
-
-# HELP "ReplicatedFetch" "Number of data parts being fetched from replica"
-# TYPE "ReplicatedFetch" counter
-"ReplicatedFetch" 0
-
-# HELP "ReplicatedSend" "Number of data parts being sent to replicas"
-# TYPE "ReplicatedSend" counter
-"ReplicatedSend" 0
-
-* Connection #0 to host localhost left intact
-
-
-* Connection #0 to host localhost left intact
-```
-
-Como puede ver en el ejemplo, si `<http_handlers>` está configurado en la configuración.archivo xml y `<http_handlers>` puede contener muchos `<rule>s`. ClickHouse coincidirá con las solicitudes HTTP recibidas con el tipo predefinido en `<rule>` y el primer emparejado ejecuta el controlador. Luego, ClickHouse ejecutará la consulta predefinida correspondiente si la coincidencia es exitosa.
-
-> Ahora `<rule>` puede configurar `<method>`, `<headers>`, `<url>`,`<handler>`:
-> `<method>` es responsable de hacer coincidir la parte del método de la solicitud HTTP. `<method>` se ajusta plenamente a la definición de [método](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods) en el protocolo HTTP. Es una configuración opcional. Si no está definido en el archivo de configuración, no coincide con la parte del método de la solicitud HTTP.
->
-> `<url>` es responsable de hacer coincidir la parte url de la solicitud HTTP. Es compatible con [RE2](https://github.com/google/re2)expresiones regulares. Es una configuración opcional. Si no está definido en el archivo de configuración, no coincide con la parte url de la solicitud HTTP.
->
-> `<headers>` es responsable de hacer coincidir la parte del encabezado de la solicitud HTTP. Es compatible con las expresiones regulares de RE2. Es una configuración opcional. Si no está definido en el archivo de configuración, no coincide con la parte de encabezado de la solicitud HTTP.
->
-> `<handler>` contiene la parte de procesamiento principal. Ahora `<handler>` puede configurar `<type>`, `<status>`, `<content_type>`, `<response_content>`, `<query>`, `<query_param_name>`.
-> \> `<type>` Actualmente soporta tres tipos: **DirecciÃ³n de correo electrÃ³nico**, **Nombre de la red inalámbrica (SSID):**, **estática**.
-> \>
-> \> `<query>` - utilizar con el tipo predefined_query_handler, ejecuta la consulta cuando se llama al controlador.
-> \>
-> \> `<query_param_name>` - utilizar con el tipo dynamic_query_handler, extrae y ejecuta el valor correspondiente al `<query_param_name>` valor en parámetros de solicitud HTTP.
-> \>
-> \> `<status>` - uso con tipo estático, código de estado de respuesta.
-> \>
-> \> `<content_type>` - uso con tipo estático, respuesta [tipo de contenido](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type).
-> \>
-> \> `<response_content>` - uso con tipo estático, contenido de respuesta enviado al cliente, cuando se usa el prefijo ‘file://’ o ‘config://’, encontrar el contenido del archivo o configuración enviar al cliente.
-
-A continuación están los métodos de configuración para los diferentes `<type>`.
-
-## DirecciÃ³n de correo electrÃ³nico {#predefined_query_handler}
-
-`<predefined_query_handler>` admite la configuración de valores Settings y query_params. Puede configurar `<query>` en el tipo de `<predefined_query_handler>`.
-
-`<query>` valor es una consulta predefinida de `<predefined_query_handler>`, que es ejecutado por ClickHouse cuando se hace coincidir una solicitud HTTP y se devuelve el resultado de la consulta. Es una configuración imprescindible.
-
-En el ejemplo siguiente se definen los valores de `max_threads` y `max_alter_threads` configuración, a continuación, consulta la tabla del sistema para comprobar si estos ajustes se han establecido correctamente.
-
-Ejemplo:
-
-``` xml
-<http_handlers>
-    <rule>
-        <url><![CDATA[/query_param_with_url/\w+/(?P<name_1>[^/]+)(/(?P<name_2>[^/]+))?]]></url>
-        <method>GET</method>
-        <headers>
-            <XXX>TEST_HEADER_VALUE</XXX>
-            <PARAMS_XXX><![CDATA[(?P<name_1>[^/]+)(/(?P<name_2>[^/]+))?]]></PARAMS_XXX>
-        </headers>
-        <handler>
-            <type>predefined_query_handler</type>
-            <query>SELECT value FROM system.settings WHERE name = {name_1:String}</query>
-            <query>SELECT name, value FROM system.settings WHERE name = {name_2:String}</query>
-        </handler>
-    </rule>
-</http_handlers>
-```
-
-``` bash
-$ curl -H 'XXX:TEST_HEADER_VALUE' -H 'PARAMS_XXX:max_threads' 'http://localhost:8123/query_param_with_url/1/max_threads/max_alter_threads?max_threads=1&max_alter_threads=2'
-1
-max_alter_threads   2
-```
-
-!!! note "precaución"
-    En uno `<predefined_query_handler>` sólo es compatible con uno `<query>` de un tipo de plaquita.
-
-## Nombre de la red inalámbrica (SSID): {#dynamic_query_handler}
-
-En `<dynamic_query_handler>`, consulta se escribe en forma de param de la solicitud HTTP. La diferencia es que en `<predefined_query_handler>`, consulta se escribe en el archivo de configuración. Puede configurar `<query_param_name>` en `<dynamic_query_handler>`.
-
-ClickHouse extrae y ejecuta el valor correspondiente al `<query_param_name>` valor en la url de la solicitud HTTP. El valor predeterminado de `<query_param_name>` ser `/query` . Es una configuración opcional. Si no hay una definición en el archivo de configuración, el parámetro no se pasa.
-
-Para experimentar con esta funcionalidad, el ejemplo define los valores de max_threads y max_alter_threads y consulta si la configuración se estableció correctamente.
-
-Ejemplo:
-
-``` xml
-<http_handlers>
-    <rule>
-    <headers>
-        <XXX>TEST_HEADER_VALUE_DYNAMIC</XXX>    </headers>
-    <handler>
-        <type>dynamic_query_handler</type>
-        <query_param_name>query_param</query_param_name>
-    </handler>
-    </rule>
-</http_handlers>
-```
-
-``` bash
-$ curl  -H 'XXX:TEST_HEADER_VALUE_DYNAMIC'  'http://localhost:8123/own?max_threads=1&max_alter_threads=2&param_name_1=max_threads&param_name_2=max_alter_threads&query_param=SELECT%20name,value%20FROM%20system.settings%20where%20name%20=%20%7Bname_1:String%7D%20OR%20name%20=%20%7Bname_2:String%7D'
-max_threads 1
-max_alter_threads   2
-```
-
-## estática {#static}
-
-`<static>` puede volver [Content_type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type), [estatus](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) y response_content. response_content puede devolver el contenido especificado
-
-Ejemplo:
-
-Devuelve un mensaje.
-
-``` xml
-<http_handlers>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/hi</url>
-            <handler>
-                <type>static</type>
-                <status>402</status>
-                <content_type>text/html; charset=UTF-8</content_type>
-                <response_content>Say Hi!</response_content>
-            </handler>
-        </rule>
-<http_handlers>
-```
-
-``` bash
-$ curl -vv  -H 'XXX:xxx' 'http://localhost:8123/hi'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /hi HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 402 Payment Required
-< Date: Wed, 29 Apr 2020 03:51:26 GMT
-< Connection: Keep-Alive
-< Content-Type: text/html; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-* Connection #0 to host localhost left intact
-Say Hi!%
-```
-
-Busque el contenido de la configuración enviada al cliente.
-
-``` xml
-<get_config_static_handler><![CDATA[<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>]]></get_config_static_handler>
-
-<http_handlers>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/get_config_static_handler</url>
-            <handler>
-                <type>static</type>
-                <response_content>config://get_config_static_handler</response_content>
-            </handler>
-        </rule>
-</http_handlers>
-```
-
-``` bash
-$ curl -v  -H 'XXX:xxx' 'http://localhost:8123/get_config_static_handler'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /get_config_static_handler HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 200 OK
-< Date: Wed, 29 Apr 2020 04:01:24 GMT
-< Connection: Keep-Alive
-< Content-Type: text/plain; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-* Connection #0 to host localhost left intact
-<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>%
-```
-
-Encuentra el contenido del archivo enviado al cliente.
-
-``` xml
-<http_handlers>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/get_absolute_path_static_handler</url>
-            <handler>
-                <type>static</type>
-                <content_type>text/html; charset=UTF-8</content_type>
-                <response_content>file:///absolute_path_file.html</response_content>
-            </handler>
-        </rule>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/get_relative_path_static_handler</url>
-            <handler>
-                <type>static</type>
-                <content_type>text/html; charset=UTF-8</content_type>
-                <response_content>file://./relative_path_file.html</response_content>
-            </handler>
-        </rule>
-</http_handlers>
-```
-
-``` bash
-$ user_files_path='/var/lib/clickhouse/user_files'
-$ sudo echo "<html><body>Relative Path File</body></html>" > $user_files_path/relative_path_file.html
-$ sudo echo "<html><body>Absolute Path File</body></html>" > $user_files_path/absolute_path_file.html
-$ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_absolute_path_static_handler'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /get_absolute_path_static_handler HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 200 OK
-< Date: Wed, 29 Apr 2020 04:18:16 GMT
-< Connection: Keep-Alive
-< Content-Type: text/html; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-<html><body>Absolute Path File</body></html>
-* Connection #0 to host localhost left intact
-$ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_relative_path_static_handler'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /get_relative_path_static_handler HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 200 OK
-< Date: Wed, 29 Apr 2020 04:18:31 GMT
-< Connection: Keep-Alive
-< Content-Type: text/html; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-<html><body>Relative Path File</body></html>
-* Connection #0 to host localhost left intact
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/http_interface/) <!--hide-->
diff --git a/docs/es/interfaces/index.md b/docs/es/interfaces/index.md
deleted file mode 100644
index 3632c8a9e296..000000000000
--- a/docs/es/interfaces/index.md
+++ /dev/null
@@ -1,29 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Interfaz
-toc_priority: 14
-toc_title: "Implantaci\xF3n"
----
-
-# Interfaz {#interfaces}
-
-ClickHouse proporciona dos interfaces de red (ambas se pueden ajustar opcionalmente en TLS para mayor seguridad):
-
--   [HTTP](http.md), que está documentado y fácil de usar directamente.
--   [TCP nativo](tcp.md), que tiene menos sobrecarga.
-
-En la mayoría de los casos, se recomienda utilizar la herramienta o biblioteca apropiada en lugar de interactuar con ellos directamente. Oficialmente apoyados por Yandex son los siguientes:
-
--   [Cliente de línea de comandos](cli.md)
--   [Controlador JDBC](jdbc.md)
--   [Controlador ODBC](odbc.md)
--   [Biblioteca cliente de C++](cpp.md)
-
-También hay una amplia gama de bibliotecas de terceros para trabajar con ClickHouse:
-
--   [Bibliotecas de clientes](third-party/client-libraries.md)
--   [Integración](third-party/integrations.md)
--   [Interfaces visuales](third-party/gui.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/) <!--hide-->
diff --git a/docs/es/interfaces/jdbc.md b/docs/es/interfaces/jdbc.md
deleted file mode 100644
index 7303dec89601..000000000000
--- a/docs/es/interfaces/jdbc.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 22
-toc_title: Controlador JDBC
----
-
-# Controlador JDBC {#jdbc-driver}
-
--   **[Conductor oficial](https://github.com/ClickHouse/clickhouse-jdbc)**
--   Controladores de terceros:
-    -   [Sistema abierto.](https://github.com/housepower/ClickHouse-Native-JDBC)
-    -   [Método de codificación de datos:](https://github.com/blynkkk/clickhouse4j)
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/jdbc/) <!--hide-->
diff --git a/docs/es/interfaces/mysql.md b/docs/es/interfaces/mysql.md
deleted file mode 100644
index a5124c61dd5e..000000000000
--- a/docs/es/interfaces/mysql.md
+++ /dev/null
@@ -1,49 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 20
-toc_title: Interfaz MySQL
----
-
-# Interfaz MySQL {#mysql-interface}
-
-ClickHouse soporta el protocolo de cable MySQL. Puede ser habilitado por [mysql_port](../operations/server-configuration-parameters/settings.md#server_configuration_parameters-mysql_port) configuración en el archivo de configuración:
-
-``` xml
-<mysql_port>9004</mysql_port>
-```
-
-Ejemplo de conexión mediante la herramienta de línea de comandos `mysql`:
-
-``` bash
-$ mysql --protocol tcp -u default -P 9004
-```
-
-Salida si una conexión se realizó correctamente:
-
-``` text
-Welcome to the MySQL monitor.  Commands end with ; or \g.
-Your MySQL connection id is 4
-Server version: 20.2.1.1-ClickHouse
-
-Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
-
-Oracle is a registered trademark of Oracle Corporation and/or its
-affiliates. Other names may be trademarks of their respective
-owners.
-
-Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
-
-mysql>
-```
-
-Para la compatibilidad con todos los clientes MySQL, se recomienda especificar la contraseña de usuario con [doble SHA1](../operations/settings/settings-users.md#password_double_sha1_hex) en el archivo de configuración.
-Si la contraseña de usuario se especifica usando [SHA256](../operations/settings/settings-users.md#password_sha256_hex), algunos clientes no podrán autenticarse (mysqljs y versiones antiguas de la herramienta de línea de comandos mysql).
-
-Restricción:
-
--   las consultas preparadas no son compatibles
-
--   algunos tipos de datos se envían como cadenas
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/mysql/) <!--hide-->
diff --git a/docs/es/interfaces/odbc.md b/docs/es/interfaces/odbc.md
deleted file mode 100644
index 6ccb979c7f7e..000000000000
--- a/docs/es/interfaces/odbc.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 23
-toc_title: Conductor ODBC
----
-
-# Conductor ODBC {#odbc-driver}
-
--   [Conductor oficial](https://github.com/ClickHouse/clickhouse-odbc).
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/odbc/) <!--hide-->
diff --git a/docs/es/interfaces/tcp.md b/docs/es/interfaces/tcp.md
deleted file mode 100644
index 47df0d128299..000000000000
--- a/docs/es/interfaces/tcp.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 18
-toc_title: Interfaz nativa (TCP)
----
-
-# Interfaz nativa (TCP) {#native-interface-tcp}
-
-El protocolo nativo se utiliza en el [cliente de línea de comandos](cli.md), para la comunicación entre servidores durante el procesamiento de consultas distribuidas, y también en otros programas de C, Desafortunadamente, el protocolo nativo de ClickHouse aún no tiene especificaciones formales, pero puede ser diseñado de manera inversa desde el código fuente de ClickHouse (comenzando [por aquí](https://github.com/ClickHouse/ClickHouse/tree/master/src/Client)) y/o mediante la interceptación y el análisis del tráfico TCP.
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/tcp/) <!--hide-->
diff --git a/docs/es/interfaces/third-party/client-libraries.md b/docs/es/interfaces/third-party/client-libraries.md
deleted file mode 100644
index b61ab1a5d9c3..000000000000
--- a/docs/es/interfaces/third-party/client-libraries.md
+++ /dev/null
@@ -1,60 +0,0 @@
----
-toc_priority: 26
-toc_title: Client Libraries
----
-
-# Client Libraries from Third-party Developers {#client-libraries-from-third-party-developers}
-
-!!! warning "Disclaimer"
-    Yandex does **not** maintain the libraries listed below and haven’t done any extensive testing to ensure their quality.
-
--   Python
-    -   [infi.clickhouse_orm](https://github.com/Infinidat/infi.clickhouse_orm)
-    -   [clickhouse-driver](https://github.com/mymarilyn/clickhouse-driver)
-    -   [clickhouse-client](https://github.com/yurial/clickhouse-client)
-    -   [aiochclient](https://github.com/maximdanilchenko/aiochclient)
-    -   [asynch](https://github.com/long2ice/asynch)
--   PHP
-    -   [smi2/phpclickhouse](https://packagist.org/packages/smi2/phpClickHouse)
-    -   [8bitov/clickhouse-php-client](https://packagist.org/packages/8bitov/clickhouse-php-client)
-    -   [bozerkins/clickhouse-client](https://packagist.org/packages/bozerkins/clickhouse-client)
-    -   [simpod/clickhouse-client](https://packagist.org/packages/simpod/clickhouse-client)
-    -   [seva-code/php-click-house-client](https://packagist.org/packages/seva-code/php-click-house-client)
-    -   [SeasClick C++ client](https://github.com/SeasX/SeasClick)
--   Go
-    -   [clickhouse](https://github.com/kshvakov/clickhouse/)
-    -   [go-clickhouse](https://github.com/roistat/go-clickhouse)
-    -   [mailrugo-clickhouse](https://github.com/mailru/go-clickhouse)
-    -   [golang-clickhouse](https://github.com/leprosus/golang-clickhouse)
--   NodeJs
-    -   [clickhouse (NodeJs)](https://github.com/TimonKK/clickhouse)
-    -   [node-clickhouse](https://github.com/apla/node-clickhouse)
--   Perl
-    -   [perl-DBD-ClickHouse](https://github.com/elcamlost/perl-DBD-ClickHouse)
-    -   [HTTP-ClickHouse](https://metacpan.org/release/HTTP-ClickHouse)
-    -   [AnyEvent-ClickHouse](https://metacpan.org/release/AnyEvent-ClickHouse)
--   Ruby
-    -   [ClickHouse (Ruby)](https://github.com/shlima/click_house)
-    -   [clickhouse-activerecord](https://github.com/PNixx/clickhouse-activerecord)
--   R
-    -   [clickhouse-r](https://github.com/hannesmuehleisen/clickhouse-r)
-    -   [RClickHouse](https://github.com/IMSMWU/RClickHouse)
--   Java
-    -   [clickhouse-client-java](https://github.com/VirtusAI/clickhouse-client-java)
-    -   [clickhouse-client](https://github.com/Ecwid/clickhouse-client)
--   Scala
-    -   [clickhouse-scala-client](https://github.com/crobox/clickhouse-scala-client)
--   Kotlin
-    -   [AORM](https://github.com/TanVD/AORM)
--   C#
-    -   [Octonica.ClickHouseClient](https://github.com/Octonica/ClickHouseClient)
-    -   [ClickHouse.Ado](https://github.com/killwort/ClickHouse-Net)
-    -   [ClickHouse.Client](https://github.com/DarkWanderer/ClickHouse.Client)
-    -   [ClickHouse.Net](https://github.com/ilyabreev/ClickHouse.Net)
--   Elixir
-    -   [clickhousex](https://github.com/appodeal/clickhousex/)
-    -   [pillar](https://github.com/sofakingworld/pillar)
--   Nim
-    -   [nim-clickhouse](https://github.com/leonardoce/nim-clickhouse)
-
-[Original article](https://clickhouse.tech/docs/en/interfaces/third-party/client_libraries/) <!--hide-->
diff --git a/docs/es/interfaces/third-party/gui.md b/docs/es/interfaces/third-party/gui.md
deleted file mode 100644
index 754c0f68c69e..000000000000
--- a/docs/es/interfaces/third-party/gui.md
+++ /dev/null
@@ -1,156 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 28
-toc_title: Interfaces Visuales
----
-
-# Interfaces visuales de desarrolladores de terceros {#visual-interfaces-from-third-party-developers}
-
-## De código abierto {#open-source}
-
-### Tabix {#tabix}
-
-Interfaz web para ClickHouse en el [Tabix](https://github.com/tabixio/tabix) proyecto.
-
-Función:
-
--   Funciona con ClickHouse directamente desde el navegador, sin la necesidad de instalar software adicional.
--   Editor de consultas con resaltado de sintaxis.
--   Autocompletado de comandos.
--   Herramientas para el análisis gráfico de la ejecución de consultas.
--   Opciones de esquema de color.
-
-[Documentación de Tabix](https://tabix.io/doc/).
-
-### Sistema abierto {#houseops}
-
-[Sistema abierto.](https://github.com/HouseOps/HouseOps) Es una interfaz de usuario / IDE para OSX, Linux y Windows.
-
-Función:
-
--   Generador de consultas con resaltado de sintaxis. Ver la respuesta en una tabla o vista JSON.
--   Exportar resultados de consultas como CSV o JSON.
--   Lista de procesos con descripciones. Modo de escritura. Capacidad de parar (`KILL`) proceso.
--   Gráfico de base de datos. Muestra todas las tablas y sus columnas con información adicional.
--   Una vista rápida del tamaño de la columna.
--   Configuración del servidor.
-
-Las siguientes características están planificadas para el desarrollo:
-
--   Gestión de bases de datos.
--   Gestión de usuarios.
--   Análisis de datos en tiempo real.
--   Supervisión de clúster.
--   Gestión de clústeres.
--   Monitoreo de tablas replicadas y Kafka.
-
-### Faro {#lighthouse}
-
-[Faro](https://github.com/VKCOM/lighthouse) Es una interfaz web ligera para ClickHouse.
-
-Función:
-
--   Lista de tablas con filtrado y metadatos.
--   Vista previa de la tabla con filtrado y clasificación.
--   Ejecución de consultas de sólo lectura.
-
-### Redash {#redash}
-
-[Redash](https://github.com/getredash/redash) es una plataforma para la visualización de datos.
-
-Admite múltiples fuentes de datos, incluido ClickHouse, Redash puede unir los resultados de consultas de diferentes fuentes de datos en un conjunto de datos final.
-
-Función:
-
--   Potente editor de consultas.
--   Explorador de base de datos.
--   Herramientas de visualización, que le permiten representar datos en diferentes formas.
-
-### DBeaver {#dbeaver}
-
-[DBeaver](https://dbeaver.io/) - Cliente de base de datos de escritorio universal con soporte ClickHouse.
-
-Función:
-
--   Desarrollo de consultas con resaltado de sintaxis y autocompletado.
--   Lista de tablas con filtros y búsqueda de metadatos.
--   Vista previa de datos de tabla.
--   Búsqueda de texto completo.
-
-### Sistema abierto {#clickhouse-cli}
-
-[Sistema abierto.](https://github.com/hatarist/clickhouse-cli) es un cliente de línea de comandos alternativo para ClickHouse, escrito en Python 3.
-
-Función:
-
--   Autocompletado.
--   Resaltado de sintaxis para las consultas y la salida de datos.
--   Soporte de buscapersonas para la salida de datos.
--   Comandos similares a PostgreSQL personalizados.
-
-### Sistema abierto {#clickhouse-flamegraph}
-
-[Sistema abierto.](https://github.com/Slach/clickhouse-flamegraph) es una herramienta especializada para visualizar el `system.trace_log` como [Flamegraph](http://www.brendangregg.com/flamegraphs.html).
-
-### Bienvenidos al Portal de LicitaciÃ³n ElectrÃ³nica de LicitaciÃ³n ElectrÃ³nica {#clickhouse-plantuml}
-
-[Método de codificación de datos:](https://pypi.org/project/clickhouse-plantuml/) es un script para generar [PlantUML](https://plantuml.com/) diagrama de esquemas de tablas.
-
-## Comercial {#commercial}
-
-### DataGrip {#datagrip}
-
-[DataGrip](https://www.jetbrains.com/datagrip/) Es un IDE de base de datos de JetBrains con soporte dedicado para ClickHouse. También está integrado en otras herramientas basadas en IntelliJ: PyCharm, IntelliJ IDEA, GoLand, PhpStorm y otros.
-
-Función:
-
--   Finalización de código muy rápida.
--   Resaltado de sintaxis de ClickHouse.
--   Soporte para características específicas de ClickHouse, por ejemplo, columnas anidadas, motores de tablas.
--   Editor de datos.
--   Refactorizaciones.
--   Búsqueda y navegación.
-
-### Yandex DataLens {#yandex-datalens}
-
-[Yandex DataLens](https://cloud.yandex.ru/services/datalens) es un servicio de visualización y análisis de datos.
-
-Función:
-
--   Amplia gama de visualizaciones disponibles, desde simples gráficos de barras hasta paneles complejos.
--   Los paneles podrían ponerse a disposición del público.
--   Soporte para múltiples fuentes de datos, incluyendo ClickHouse.
--   Almacenamiento de datos materializados basados en ClickHouse.
-
-Nivel de Cifrado WEP [disponible de forma gratuita](https://cloud.yandex.com/docs/datalens/pricing) para proyectos de baja carga, incluso para uso comercial.
-
--   [Documentación de DataLens](https://cloud.yandex.com/docs/datalens/).
--   [Tutorial](https://cloud.yandex.com/docs/solutions/datalens/data-from-ch-visualization) en la visualización de datos de una base de datos ClickHouse.
-
-### Software de Holística {#holistics-software}
-
-[Holística](https://www.holistics.io/) es una plataforma de datos de pila completa y una herramienta de inteligencia de negocios.
-
-Función:
-
--   Correo electrónico automatizado, Slack y horarios de informes de Google Sheet.
--   Editor SQL con visualizaciones, control de versiones, autocompletado, componentes de consulta reutilizables y filtros dinámicos.
--   Análisis integrado de informes y cuadros de mando a través de iframe.
--   Preparación de datos y capacidades ETL.
--   Soporte de modelado de datos SQL para mapeo relacional de datos.
-
-### Mirador {#looker}
-
-[Mirador](https://looker.com) Es una plataforma de datos y una herramienta de inteligencia de negocios con soporte para más de 50 dialectos de bases de datos, incluido ClickHouse. Bravo está disponible como una plataforma SaaS y auto-organizada. Los usuarios pueden utilizar Looker a través del navegador para explorar datos, crear visualizaciones y paneles, programar informes y compartir sus conocimientos con colegas. Looker proporciona un amplio conjunto de herramientas para incrustar estas características en otras aplicaciones y una API
-para integrar datos con otras aplicaciones.
-
-Función:
-
--   Desarrollo fácil y ágil utilizando LookML, un lenguaje que soporta curado
-    [Modelado de datos](https://looker.com/platform/data-modeling) para apoyar a los redactores de informes y a los usuarios finales.
--   Potente integración de flujo de trabajo a través de Looker's [Acciones de datos](https://looker.com/platform/actions).
-
-[Cómo configurar ClickHouse en Looker.](https://docs.looker.com/setup-and-management/database-config/clickhouse)
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/third-party/gui/) <!--hide-->
diff --git a/docs/es/interfaces/third-party/index.md b/docs/es/interfaces/third-party/index.md
deleted file mode 100644
index adf50b05cdf1..000000000000
--- a/docs/es/interfaces/third-party/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: tercero
-toc_priority: 24
----
-
-
diff --git a/docs/es/interfaces/third-party/integrations.md b/docs/es/interfaces/third-party/integrations.md
deleted file mode 100644
index 7588bef0230b..000000000000
--- a/docs/es/interfaces/third-party/integrations.md
+++ /dev/null
@@ -1,108 +0,0 @@
----
-toc_priority: 27
-toc_title: Integrations
----
-
-# Integration Libraries from Third-party Developers {#integration-libraries-from-third-party-developers}
-
-!!! warning "Disclaimer"
-    Yandex does **not** maintain the tools and libraries listed below and haven’t done any extensive testing to ensure their quality.
-
-## Infrastructure Products {#infrastructure-products}
-
--   Relational database management systems
-    -   [MySQL](https://www.mysql.com)
-        -   [mysql2ch](https://github.com/long2ice/mysql2ch)
-        -   [ProxySQL](https://github.com/sysown/proxysql/wiki/ClickHouse-Support)
-        -   [clickhouse-mysql-data-reader](https://github.com/Altinity/clickhouse-mysql-data-reader)
-        -   [horgh-replicator](https://github.com/larsnovikov/horgh-replicator)
-    -   [PostgreSQL](https://www.postgresql.org)
-        -   [clickhousedb_fdw](https://github.com/Percona-Lab/clickhousedb_fdw)
-        -   [infi.clickhouse_fdw](https://github.com/Infinidat/infi.clickhouse_fdw) (uses [infi.clickhouse_orm](https://github.com/Infinidat/infi.clickhouse_orm))
-        -   [pg2ch](https://github.com/mkabilov/pg2ch)
-        -   [clickhouse_fdw](https://github.com/adjust/clickhouse_fdw)
-    -   [MSSQL](https://en.wikipedia.org/wiki/Microsoft_SQL_Server)
-        -   [ClickHouseMigrator](https://github.com/zlzforever/ClickHouseMigrator)
--   Message queues
-    -   [Kafka](https://kafka.apache.org)
-        -   [clickhouse_sinker](https://github.com/housepower/clickhouse_sinker) (uses [Go client](https://github.com/ClickHouse/clickhouse-go/))
-        -   [stream-loader-clickhouse](https://github.com/adform/stream-loader)
--   Stream processing
-    -   [Flink](https://flink.apache.org)
-        -   [flink-clickhouse-sink](https://github.com/ivi-ru/flink-clickhouse-sink)
--   Object storages
-    -   [S3](https://en.wikipedia.org/wiki/Amazon_S3)
-        -   [clickhouse-backup](https://github.com/AlexAkulov/clickhouse-backup)
--   Container orchestration
-    -   [Kubernetes](https://kubernetes.io)
-        -   [clickhouse-operator](https://github.com/Altinity/clickhouse-operator)
--   Configuration management
-    -   [puppet](https://puppet.com)
-        -   [innogames/clickhouse](https://forge.puppet.com/innogames/clickhouse)
-        -   [mfedotov/clickhouse](https://forge.puppet.com/mfedotov/clickhouse)
--   Monitoring
-    -   [Graphite](https://graphiteapp.org)
-        -   [graphouse](https://github.com/yandex/graphouse)
-        -   [carbon-clickhouse](https://github.com/lomik/carbon-clickhouse) +
-        -   [graphite-clickhouse](https://github.com/lomik/graphite-clickhouse)
-        -   [graphite-ch-optimizer](https://github.com/innogames/graphite-ch-optimizer) - optimizes staled partitions in [\*GraphiteMergeTree](../../engines/table-engines/mergetree-family/graphitemergetree.md#graphitemergetree) if rules from [rollup configuration](../../engines/table-engines/mergetree-family/graphitemergetree.md#rollup-configuration) could be applied
-    -   [Grafana](https://grafana.com/)
-        -   [clickhouse-grafana](https://github.com/Vertamedia/clickhouse-grafana)
-    -   [Prometheus](https://prometheus.io/)
-        -   [clickhouse_exporter](https://github.com/f1yegor/clickhouse_exporter)
-        -   [PromHouse](https://github.com/Percona-Lab/PromHouse)
-        -   [clickhouse_exporter](https://github.com/hot-wifi/clickhouse_exporter) (uses [Go client](https://github.com/kshvakov/clickhouse/))
-    -   [Nagios](https://www.nagios.org/)
-        -   [check_clickhouse](https://github.com/exogroup/check_clickhouse/)
-        -   [check_clickhouse.py](https://github.com/innogames/igmonplugins/blob/master/src/check_clickhouse.py)
-    -   [Zabbix](https://www.zabbix.com)
-        -   [clickhouse-zabbix-template](https://github.com/Altinity/clickhouse-zabbix-template)
-    -   [Sematext](https://sematext.com/)
-        -   [clickhouse integration](https://github.com/sematext/sematext-agent-integrations/tree/master/clickhouse)
--   Logging
-    -   [rsyslog](https://www.rsyslog.com/)
-        -   [omclickhouse](https://www.rsyslog.com/doc/master/configuration/modules/omclickhouse.html)
-    -   [fluentd](https://www.fluentd.org)
-        -   [loghouse](https://github.com/flant/loghouse) (for [Kubernetes](https://kubernetes.io))
-    -   [logagent](https://www.sematext.com/logagent)
-        -   [logagent output-plugin-clickhouse](https://sematext.com/docs/logagent/output-plugin-clickhouse/)
--   Geo
-    -   [MaxMind](https://dev.maxmind.com/geoip/)
-        -   [clickhouse-maxmind-geoip](https://github.com/AlexeyKupershtokh/clickhouse-maxmind-geoip)
-
-## Programming Language Ecosystems {#programming-language-ecosystems}
-
--   Python
-    -   [SQLAlchemy](https://www.sqlalchemy.org)
-        -   [sqlalchemy-clickhouse](https://github.com/cloudflare/sqlalchemy-clickhouse) (uses [infi.clickhouse_orm](https://github.com/Infinidat/infi.clickhouse_orm))
-    -   [pandas](https://pandas.pydata.org)
-        -   [pandahouse](https://github.com/kszucs/pandahouse)
--   PHP
-    -   [Doctrine](https://www.doctrine-project.org/)
-        -   [dbal-clickhouse](https://packagist.org/packages/friendsofdoctrine/dbal-clickhouse)
--   R
-    -   [dplyr](https://db.rstudio.com/dplyr/)
-        -   [RClickHouse](https://github.com/IMSMWU/RClickHouse) (uses [clickhouse-cpp](https://github.com/artpaul/clickhouse-cpp))
--   Java
-    -   [Hadoop](http://hadoop.apache.org)
-        -   [clickhouse-hdfs-loader](https://github.com/jaykelin/clickhouse-hdfs-loader) (uses [JDBC](../../sql-reference/table-functions/jdbc.md))
--   Scala
-    -   [Akka](https://akka.io)
-        -   [clickhouse-scala-client](https://github.com/crobox/clickhouse-scala-client)
--   C#
-    -   [ADO.NET](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/ado-net-overview)
-        -   [ClickHouse.Ado](https://github.com/killwort/ClickHouse-Net)
-        -   [ClickHouse.Client](https://github.com/DarkWanderer/ClickHouse.Client)
-        -   [ClickHouse.Net](https://github.com/ilyabreev/ClickHouse.Net)
-        -   [ClickHouse.Net.Migrations](https://github.com/ilyabreev/ClickHouse.Net.Migrations)
--   Elixir
-    -   [Ecto](https://github.com/elixir-ecto/ecto)
-        -   [clickhouse_ecto](https://github.com/appodeal/clickhouse_ecto)
--   Ruby
-    -   [Ruby on Rails](https://rubyonrails.org/)
-        -   [activecube](https://github.com/bitquery/activecube)
-        -   [ActiveRecord](https://github.com/PNixx/clickhouse-activerecord)
-    -   [GraphQL](https://github.com/graphql)
-        -   [activecube-graphql](https://github.com/bitquery/activecube-graphql)
-
-[Original article](https://clickhouse.tech/docs/en/interfaces/third-party/integrations/) <!--hide-->
diff --git a/docs/es/interfaces/third-party/proxy.md b/docs/es/interfaces/third-party/proxy.md
deleted file mode 100644
index e1aabf8fce46..000000000000
--- a/docs/es/interfaces/third-party/proxy.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 29
-toc_title: Proxy
----
-
-# Servidores proxy de desarrolladores de terceros {#proxy-servers-from-third-party-developers}
-
-## chproxy {#chproxy}
-
-[chproxy](https://github.com/Vertamedia/chproxy), es un proxy HTTP y equilibrador de carga para la base de datos ClickHouse.
-
-Función:
-
--   Enrutamiento por usuario y almacenamiento en caché de respuestas.
--   Flexible límites.
--   Renovación automática del certificado SSL.
-
-Implementado en Go.
-
-## Bienvenido a WordPress {#kittenhouse}
-
-[Bienvenido a WordPress.](https://github.com/VKCOM/kittenhouse) está diseñado para ser un proxy local entre ClickHouse y el servidor de aplicaciones en caso de que sea imposible o inconveniente almacenar los datos INSERT en el lado de su aplicación.
-
-Función:
-
--   Almacenamiento en búfer de datos en memoria y en disco.
--   Enrutamiento por tabla.
--   Equilibrio de carga y comprobación de estado.
-
-Implementado en Go.
-
-## Bienvenidos al Portal de LicitaciÃ³n ElectrÃ³nica de LicitaciÃ³n ElectrÃ³nica {#clickhouse-bulk}
-
-[Bienvenidos al Portal de LicitaciÃ³n ElectrÃ³nica de LicitaciÃ³n ElectrÃ³nica](https://github.com/nikepan/clickhouse-bulk) es un simple colector de insertos ClickHouse.
-
-Función:
-
--   Agrupe las solicitudes y envíe por umbral o intervalo.
--   Múltiples servidores remotos.
--   Autenticación básica.
-
-Implementado en Go.
-
-[Artículo Original](https://clickhouse.tech/docs/en/interfaces/third-party/proxy/) <!--hide-->
diff --git a/docs/es/introduction/adopters.md b/docs/es/introduction/adopters.md
deleted file mode 100644
index 4c0aa78d57b3..000000000000
--- a/docs/es/introduction/adopters.md
+++ /dev/null
@@ -1,86 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 8
-toc_title: Adoptante
----
-
-# Adoptadores de ClickHouse {#clickhouse-adopters}
-
-!!! warning "Descargo"
-    La siguiente lista de empresas que utilizan ClickHouse y sus historias de éxito se recopila a partir de fuentes públicas, por lo que podría diferir de la realidad actual. Le agradeceríamos que compartiera la historia de adoptar ClickHouse en su empresa y [agregarlo a la lista](https://github.com/ClickHouse/ClickHouse/edit/master/docs/en/introduction/adopters.md), pero por favor asegúrese de que usted no tendrá ningunos problemas de NDA haciendo así. Proporcionar actualizaciones con publicaciones de otras compañías también es útil.
-
-| Empresa                                                                                         | Industria                          | Usecase                     | Tamaño de clúster                                                | (Un)Tamaño de datos comprimidos<abbr title="of single replica"><sup>\*</sup></abbr> | Referencia                                                                                                                                                                                                                             |
-|-------------------------------------------------------------------------------------------------|------------------------------------|-----------------------------|------------------------------------------------------------------|-------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| <a href="https://2gis.ru" class="favicon">2gis</a>                                              | Asignar                            | Monitoreo                   | —                                                                | —                                                                                   | [Charla en ruso, julio 2019](https://youtu.be/58sPkXfq6nw)                                                                                                                                                                             |
-| <a href="https://alohabrowser.com/" class="favicon">Aloha Browser</a>                           | Aplicación móvil                   | Backend del navegador       | —                                                                | —                                                                                   | [Diapositivas en ruso, mayo 2019](https://github.com/yandex/clickhouse-presentations/blob/master/meetup22/aloha.pdf)                                                                                                                   |
-| <a href="https://amadeus.com/" class="favicon">Amadeus</a>                                      | Viaje                              | Analítica                   | —                                                                | —                                                                                   | [Comunicado de prensa, abril de 2018](https://www.altinity.com/blog/2018/4/5/amadeus-technologies-launches-investment-and-insights-tool-based-on-machine-learning-and-strategy-algorithms)                                             |
-| <a href="https://www.appsflyer.com" class="favicon">Appsflyer</a>                               | Análisis móvil                     | Producto principal          | —                                                                | —                                                                                   | [Charla en ruso, julio 2019](https://www.youtube.com/watch?v=M3wbRlcpBbY)                                                                                                                                                              |
-| <a href="https://arenadata.tech/" class="favicon">ArenaData</a>                                 | Plataforma de datos                | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en ruso, diciembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup38/indexes.pdf)                                                                                                        |
-| <a href="https://badoo.com" class="favicon">Badoo</a>                                           | Citas                              | Serie de tiempo             | —                                                                | —                                                                                   | [Diapositivas en ruso, diciembre 2019](https://presentations.clickhouse.tech/meetup38/forecast.pdf)                                                                                                                                    |
-| <a href="https://www.benocs.com/" class="favicon">Benocs</a>                                    | Telemetría y análisis de red       | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en español, octubre de 2017](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup9/lpm.pdf)                                                                                                         |
-| <a href="https://www.bloomberg.com/" class="favicon">Bloomberg</a>                              | Finanzas, Medios                   | Monitoreo                   | 102 servidores                                                   | —                                                                                   | [Diapositivas, Mayo 2018](https://www.slideshare.net/Altinity/http-analytics-for-6m-requests-per-second-using-clickhouse-by-alexander-bocharov)                                                                                        |
-| <a href="https://bloxy.info" class="favicon">Bloxy</a>                                          | Blockchain                         | Analítica                   | —                                                                | —                                                                                   | [Diapositivas en ruso, agosto 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup17/4_bloxy.pptx)                                                                                                          |
-| <a href="https://www.chinatelecomglobal.com/" class="favicon">Dataliance para China Telecom</a> | Telecomunicaciones                 | Analítica                   | —                                                                | —                                                                                   | [Diapositivas en chino, enero 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup12/telecom.pdf)                                                                                                           |
-| <a href="https://carto.com/" class="favicon">CARTO</a>                                          | Inteligencia de negocios           | Análisis geográfico         | —                                                                | —                                                                                   | [Procesamiento geoespacial con ClickHouse](https://carto.com/blog/geospatial-processing-with-clickhouse/)                                                                                                                              |
-| <a href="http://public.web.cern.ch/public/" class="favicon">CERN</a>                            | Investigación                      | Experimento                 | —                                                                | —                                                                                   | [Comunicado de prensa, abril de 2012](https://www.yandex.com/company/press_center/press_releases/2012/2012-04-10/)                                                                                                                     |
-| <a href="http://cisco.com/" class="favicon">Cisco</a>                                           | Red                                | Análisis de tráfico         | —                                                                | —                                                                                   | [Charla relámpago, octubre 2019](https://youtu.be/-hI1vDR2oPY?t=5057)                                                                                                                                                                  |
-| <a href="https://www.citadelsecurities.com/" class="favicon">Citadel Securities</a>             | Financiación                       | —                           | —                                                                | —                                                                                   | [Contribución, marzo 2019](https://github.com/ClickHouse/ClickHouse/pull/4774)                                                                                                                                                         |
-| <a href="https://city-mobil.ru" class="favicon">Más información</a>                             | Taxi                               | Analítica                   | —                                                                | —                                                                                   | [Blog Post en ruso, marzo 2020](https://habr.com/en/company/citymobil/blog/490660/)                                                                                                                                                    |
-| <a href="https://contentsquare.com" class="favicon">ContentSquare</a>                           | Análisis web                       | Producto principal          | —                                                                | —                                                                                   | [Publicación de blog en francés, noviembre 2018](http://souslecapot.net/2018/11/21/patrick-chatain-vp-engineering-chez-contentsquare-penser-davantage-amelioration-continue-que-revolution-constante/)                                 |
-| <a href="https://cloudflare.com" class="favicon">Cloudflare</a>                                 | CDN                                | Análisis de tráfico         | 36 servidores                                                    | —                                                                                   | [Mensaje del blog, Mayo 2017](https://blog.cloudflare.com/how-cloudflare-analyzes-1m-dns-queries-per-second/), [Mensaje del blog, marzo 2018](https://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/) |
-| <a href="https://coru.net/" class="favicon">Corunet</a>                                         | Analítica                          | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en español, Abril 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup21/predictive_models.pdf)                                                                                               |
-| <a href="https://www.creditx.com" class="favicon">CraiditX 氪信</a>                             | Finanzas AI                        | Análisis                    | —                                                                | —                                                                                   | [Diapositivas en español, noviembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup33/udf.pptx)                                                                                                        |
-| <a href="https://www.criteo.com/" class="favicon">Criteo</a>                                    | Menor                              | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en español, octubre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup18/3_storetail.pptx)                                                                                                  |
-| <a href="https://db.com" class="favicon">Deutsche Bank</a>                                      | Financiación                       | BI Analytics                | —                                                                | —                                                                                   | [Diapositivas en español, octubre 2019](https://bigdatadays.ru/wp-content/uploads/2019/10/D2-H3-3_Yakunin-Goihburg.pdf)                                                                                                                |
-| <a href="https://www.diva-e.com" class="favicon">Diva-e</a>                                     | Consultoría digital                | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en español, septiembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup29/ClickHouse-MeetUp-Unusual-Applications-sd-2019-09-17.pdf)                                                       |
-| <a href="https://www.exness.com" class="favicon">Exness</a>                                     | Comercio                           | Métricas, Registro          | —                                                                | —                                                                                   | [Charla en ruso, mayo 2019](https://youtu.be/_rpU-TvSfZ8?t=3215)                                                                                                                                                                       |
-| <a href="https://geniee.co.jp" class="favicon">Sistema abierto.</a>                             | Red Ad                             | Producto principal          | —                                                                | —                                                                                   | [Publicación de blog en japonés, julio 2017](https://tech.geniee.co.jp/entry/2017/07/20/160100)                                                                                                                                        |
-| <a href="https://www.huya.com/" class="favicon">HUYA</a>                                        | Video Streaming                    | Analítica                   | —                                                                | —                                                                                   | [Diapositivas en chino, octubre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/7.%20ClickHouse万亿数据分析实践%20李本旺(sundy-li)%20虎牙.pdf)                                                       |
-| <a href="https://www.idealista.com" class="favicon">Idealista</a>                               | Inmobiliario                       | Analítica                   | —                                                                | —                                                                                   | [Blog Post en Inglés, Abril 2019](https://clickhouse.tech/blog/en/clickhouse-meetup-in-madrid-on-april-2-2019)                                                                                                                       |
-| <a href="https://www.infovista.com/" class="favicon">Infovista</a>                              | Red                                | Analítica                   | —                                                                | —                                                                                   | [Diapositivas en español, octubre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup30/infovista.pdf)                                                                                                     |
-| <a href="https://www.innogames.com" class="favicon">InnoGames</a>                               | Juego                              | Métricas, Registro          | —                                                                | —                                                                                   | [Diapositivas en ruso, septiembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup28/graphite_and_clickHouse.pdf)                                                                                       |
-| <a href="https://integros.com" class="favicon">Integros</a>                                     | Plataforma para servicios de video | Analítica                   | —                                                                | —                                                                                   | [Diapositivas en ruso, mayo 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup22/strategies.pdf)                                                                                                          |
-| <a href="https://www.kodiakdata.com/" class="favicon">Datos de Kodiak</a>                       | Nube                               | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en Engish, Abril 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup13/kodiak_data.pdf)                                                                                                      |
-| <a href="https://kontur.ru" class="favicon">Kontur</a>                                          | Desarrollo de software             | Métricas                    | —                                                                | —                                                                                   | [Charla en ruso, noviembre 2018](https://www.youtube.com/watch?v=U4u4Bd0FtrY)                                                                                                                                                          |
-| <a href="https://lifestreet.com/" class="favicon">Sistema abierto.</a>                          | Red Ad                             | Producto principal          | 75 servidores (3 réplicas)                                       | 5.27 PiB                                                                            | [Publicación de blog en ruso, febrero 2017](https://habr.com/en/post/322620/)                                                                                                                                                          |
-| <a href="https://mcs.mail.ru/" class="favicon">Soluciones en la nube de Mail.ru</a>             | Servicios en la nube               | Producto principal          | —                                                                | —                                                                                   | [Artículo en ruso](https://mcs.mail.ru/help/db-create/clickhouse#)                                                                                                                                                                     |
-| <a href="https://www.messagebird.com" class="favicon">Mensaje de pájaro</a>                     | Telecomunicaciones                 | Estadísticas                | —                                                                | —                                                                                   | [Diapositivas en español, noviembre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup20/messagebird.pdf)                                                                                                 |
-| <a href="https://www.mgid.com/" class="favicon">MGID</a>                                        | Red Ad                             | Analítica Web               | —                                                                | —                                                                                   | [Publicación de blog en ruso, abril 2020](http://gs-studio.com/news-about-it/32777----clickhouse---c)                                                                                                                                  |
-| <a href="https://www.oneapm.com/" class="favicon">UnoAPM</a>                                    | Supervisión y análisis de datos    | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en chino, octubre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/8.%20clickhouse在OneAPM的应用%20杜龙.pdf)                                                                            |
-| <a href="http://www.pragma-innovation.fr/" class="favicon">Pragma Innovation</a>                | Telemetría y Análisis de Big Data  | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en español, octubre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup18/4_pragma_innovation.pdf)                                                                                           |
-| <a href="https://www.qingcloud.com/" class="favicon">QINGCLOUD</a>                              | Servicios en la nube               | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en chino, octubre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/4.%20Cloud%20%2B%20TSDB%20for%20ClickHouse%20张健%20QingCloud.pdf)                                                   |
-| <a href="https://qrator.net" class="favicon">Qrator</a>                                         | Protección DDoS                    | Producto principal          | —                                                                | —                                                                                   | [Blog Post, marzo 2019](https://blog.qrator.net/en/clickhouse-ddos-mitigation_37/)                                                                                                                                                     |
-| <a href="https://www.percent.cn/" class="favicon">Percent 百分点</a>                            | Analítica                          | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en chino, junio 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup24/4.%20ClickHouse万亿数据双中心的设计与实践%20.pdf)                                                                      |
-| <a href="https://rambler.ru" class="favicon">Rambler</a>                                        | Servicios de Internet              | Analítica                   | —                                                                | —                                                                                   | [Charla en ruso, abril 2018](https://medium.com/@ramblertop/разработка-api-clickhouse-для-рамблер-топ-100-f4c7e56f3141)                                                                                                                |
-| <a href="https://www.tencent.com" class="favicon">Tencent</a>                                   | Mensajería                         | Tala                        | —                                                                | —                                                                                   | [Charla en chino, noviembre 2019](https://youtu.be/T-iVQRuw-QY?t=5050)                                                                                                                                                                 |
-| <a href="https://trafficstars.com/" class="favicon">Traffic Stars</a>                           | Red AD                             | —                           | —                                                                | —                                                                                   | [Diapositivas en ruso, mayo 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup15/lightning/ninja.pdf)                                                                                                     |
-| <a href="https://www.s7.ru" class="favicon">S7 Airlines</a>                                     | Aérea                              | Métricas, Registro          | —                                                                | —                                                                                   | [Charla en ruso, marzo 2019](https://www.youtube.com/watch?v=nwG68klRpPg&t=15s)                                                                                                                                                        |
-| <a href="https://www.semrush.com/" class="favicon">SEMrush</a>                                  | Marketing                          | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en ruso, agosto 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup17/5_semrush.pdf)                                                                                                         |
-| <a href="https://www.scireum.de/" class="favicon">scireum GmbH</a>                              | Comercio electrónico               | Producto principal          | —                                                                | —                                                                                   | [Charla en alemán, febrero de 2020](https://www.youtube.com/watch?v=7QWAn5RbyR4)                                                                                                                                                       |
-| <a href="https://sentry.io/" class="favicon">Centinela</a>                                      | Desarrollador de software          | Backend para el producto    | —                                                                | —                                                                                   | [Publicación de blog en inglés, mayo 2019](https://blog.sentry.io/2019/05/16/introducing-snuba-sentrys-new-search-infrastructure)                                                                                                      |
-| <a href="http://www.sgk.gov.tr/wps/portal/sgk/tr" class="favicon">SGK</a>                       | Gobierno Seguridad Social          | Analítica                   | —                                                                | —                                                                                   | [Diapositivas en español, noviembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup35/ClickHouse%20Meetup-Ramazan%20POLAT.pdf)                                                                         |
-| <a href="https://seo.do/" class="favicon">el seo.¿</a>                                          | Analítica                          | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en español, noviembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup35/CH%20Presentation-%20Metehan%20Çetinkaya.pdf)                                                                    |
-| <a href="http://english.sina.com/index.html" class="favicon">Sina</a>                           | Noticia                            | —                           | —                                                                | —                                                                                   | [Diapositivas en chino, octubre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/6.%20ClickHouse最佳实践%20高鹏_新浪.pdf)                                                                             |
-| <a href="https://smi2.ru/" class="favicon">SMI2</a>                                             | Noticia                            | Analítica                   | —                                                                | —                                                                                   | [Blog Post en ruso, noviembre 2017](https://habr.com/ru/company/smi2/blog/314558/)                                                                                                                                                     |
-| <a href="https://www.splunk.com/" class="favicon">Salto</a>                                     | Análisis de negocios               | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en español, enero 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup12/splunk.pdf)                                                                                                          |
-| <a href="https://www.spotify.com" class="favicon">Spotify</a>                                   | Sica                               | Experimentación             | —                                                                | —                                                                                   | [Diapositivas, julio 2018](https://www.slideshare.net/glebus/using-clickhouse-for-experimentation-104247173)                                                                                                                           |
-| <a href="https://www.tencent.com" class="favicon">Tencent</a>                                   | Grandes Datos                      | Procesamiento de datos      | —                                                                | —                                                                                   | [Diapositivas en chino, octubre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/5.%20ClickHouse大数据集群应用_李俊飞腾讯网媒事业部.pdf)                                                              |
-| <a href="https://www.uber.com" class="favicon">Más información</a>                              | Taxi                               | Tala                        | —                                                                | —                                                                                   | [Diapositivas, febrero de 2020](https://presentations.clickhouse.tech/meetup40/uber.pdf)                                                                                                                                               |
-| <a href="https://vk.com" class="favicon">VKontakte</a>                                          | Red social                         | Estadísticas, Registro      | —                                                                | —                                                                                   | [Diapositivas en ruso, agosto 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup17/3_vk.pdf)                                                                                                              |
-| <a href="https://wisebits.com/" class="favicon">Método de codificación de datos:</a>            | Soluciones de TI                   | Analítica                   | —                                                                | —                                                                                   | [Diapositivas en ruso, mayo 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup22/strategies.pdf)                                                                                                          |
-| <a href="http://www.xiaoxintech.cn/" class="favicon">Xiaoxin Tech</a>                           | Educación                          | Propósito común             | —                                                                | —                                                                                   | [Diapositivas en español, noviembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup33/sync-clickhouse-with-mysql-mongodb.pptx)                                                                         |
-| <a href="https://www.ximalaya.com/" class="favicon">Ximalaya</a>                                | Compartir audio                    | OLAP                        | —                                                                | —                                                                                   | [Diapositivas en español, noviembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup33/ximalaya.pdf)                                                                                                    |
-| <a href="https://cloud.yandex.ru/services/managed-clickhouse" class="favicon">Yandex Cloud</a>  | Nube pública                       | Producto principal          | —                                                                | —                                                                                   | [Charla en ruso, diciembre 2019](https://www.youtube.com/watch?v=pgnak9e_E0o)                                                                                                                                                          |
-| <a href="https://cloud.yandex.ru/services/datalens" class="favicon">Yandex DataLens</a>         | Inteligencia de negocios           | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en ruso, diciembre 2019](https://presentations.clickhouse.tech/meetup38/datalens.pdf)                                                                                                                                    |
-| <a href="https://market.yandex.ru/" class="favicon">Yandex Market</a>                           | Comercio electrónico               | Métricas, Registro          | —                                                                | —                                                                                   | [Charla en ruso, enero 2019](https://youtu.be/_l1qP0DyBcA?t=478)                                                                                                                                                                       |
-| <a href="https://metrica.yandex.com" class="favicon">Yandex Metrica</a>                         | Análisis web                       | Producto principal          | 360 servidores en un clúster, 1862 servidores en un departamento | 66.41 PiB / 5.68 PiB                                                                | [Diapositivas, febrero de 2020](https://presentations.clickhouse.tech/meetup40/introduction/#13)                                                                                                                                       |
-| <a href="https://htc-cs.ru/" class="favicon">ЦВТ</a>                                            | Desarrollo de software             | Métricas, Registro          | —                                                                | —                                                                                   | [Blog Post, marzo 2019, en ruso](https://vc.ru/dev/62715-kak-my-stroili-monitoring-na-prometheus-clickhouse-i-elk)                                                                                                                     |
-| <a href="https://mkb.ru/" class="favicon">МКБ</a>                                               | Banco                              | Supervisión del sistema web | —                                                                | —                                                                                   | [Diapositivas en ruso, septiembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup28/mkb.pdf)                                                                                                           |
-| <a href="https://jinshuju.net" class="favicon">Jinshuju 金数据</a>                              | BI Analytics                       | Producto principal          | —                                                                | —                                                                                   | [Diapositivas en chino, octubre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup24/3.%20金数据数据架构调整方案Public.pdf)                                                                               |
-| <a href="https://www.instana.com" class="favicon">Instana</a>                                   | Plataforma APM                     | Producto principal          | —                                                                | —                                                                                   | [Publicación de Twitter](https://twitter.com/mieldonkers/status/1248884119158882304)                                                                                                                                                   |
-| <a href="https://wargaming.com/en/" class="favicon">Wargaming</a>                               | Juego                              |                             | —                                                                | —                                                                                   | [Entrevista](https://habr.com/en/post/496954/)                                                                                                                                                                                         |
-| <a href="https://crazypanda.ru/en/" class="favicon">Crazypanda</a>                              | Juego                              |                             | —                                                                | —                                                                                   | Sesión en vivo en ClickHouse meetup                                                                                                                                                                                                    |
-| <a href="https://fun.co/rp" class="favicon">FunCorp</a>                                         | Juego                              |                             | —                                                                | —                                                                                   | [Artículo](https://www.altinity.com/blog/migrating-from-redshift-to-clickhouse)                                                                                                                                                        |
-
-[Artículo Original](https://clickhouse.tech/docs/en/introduction/adopters/) <!--hide-->
diff --git a/docs/es/introduction/distinctive-features.md b/docs/es/introduction/distinctive-features.md
deleted file mode 100644
index 154b12a65e90..000000000000
--- a/docs/es/introduction/distinctive-features.md
+++ /dev/null
@@ -1,77 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 4
-toc_title: "Caracter\xEDsticas distintivas"
----
-
-# Características distintivas de ClickHouse {#distinctive-features-of-clickhouse}
-
-## DBMS orientado a columnas verdaderas {#true-column-oriented-dbms}
-
-En un verdadero DBMS orientado a columnas, no se almacenan datos adicionales con los valores. Entre otras cosas, esto significa que los valores de longitud constante deben ser compatibles, para evitar almacenar su longitud “number” al lado de los valores. Como ejemplo, mil millones de valores de tipo UInt8 deberían consumir alrededor de 1 GB sin comprimir, o esto afecta fuertemente el uso de la CPU. Es esencial almacenar los datos de forma compacta (sin “garbage”) incluso sin comprimir, ya que la velocidad de descompresión (uso de CPU) depende principalmente del volumen de datos sin comprimir.
-
-Vale la pena señalar porque hay sistemas que pueden almacenar valores de diferentes columnas por separado, pero que no pueden procesar efectivamente las consultas analíticas debido a su optimización para otros escenarios. Los ejemplos son HBase, BigTable, Cassandra e HyperTable. En estos sistemas, obtendría un rendimiento de alrededor de cien mil filas por segundo, pero no cientos de millones de filas por segundo.
-
-También vale la pena señalar que ClickHouse es un sistema de administración de bases de datos, no una sola base de datos. ClickHouse permite crear tablas y bases de datos en tiempo de ejecución, cargar datos y ejecutar consultas sin volver a configurar y reiniciar el servidor.
-
-## Compresión de datos {#data-compression}
-
-Algunos DBMS orientados a columnas (InfiniDB CE y MonetDB) no utilizan la compresión de datos. Sin embargo, la compresión de datos juega un papel clave para lograr un rendimiento excelente.
-
-## Almacenamiento en disco de datos {#disk-storage-of-data}
-
-Mantener los datos físicamente ordenados por clave principal permite extraer datos para sus valores específicos o rangos de valores con baja latencia, menos de unas pocas docenas de milisegundos. Algunos DBMS orientados a columnas (como SAP HANA y Google PowerDrill) solo pueden funcionar en RAM. Este enfoque fomenta la asignación de un presupuesto de hardware más grande que el necesario para el análisis en tiempo real. ClickHouse está diseñado para funcionar en discos duros normales, lo que significa que el costo por GB de almacenamiento de datos es bajo, pero SSD y RAM adicional también se utilizan completamente si están disponibles.
-
-## Procesamiento paralelo en varios núcleos {#parallel-processing-on-multiple-cores}
-
-Las consultas grandes se paralelizan naturalmente, tomando todos los recursos necesarios disponibles en el servidor actual.
-
-## Procesamiento distribuido en varios servidores {#distributed-processing-on-multiple-servers}
-
-Casi ninguno de los DBMS columnar mencionados anteriormente tiene soporte para el procesamiento de consultas distribuidas.
-En ClickHouse, los datos pueden residir en diferentes fragmentos. Cada fragmento puede ser un grupo de réplicas utilizadas para la tolerancia a errores. Todos los fragmentos se utilizan para ejecutar una consulta en paralelo, de forma transparente para el usuario.
-
-## Soporte SQL {#sql-support}
-
-ClickHouse admite un lenguaje de consulta declarativo basado en SQL que es idéntico al estándar SQL en muchos casos.
-Las consultas admitidas incluyen GROUP BY, ORDER BY, subconsultas en cláusulas FROM, IN y JOIN y subconsultas escalares.
-No se admiten subconsultas y funciones de ventana dependientes.
-
-## Motor del vector {#vector-engine}
-
-Los datos no solo se almacenan mediante columnas, sino que se procesan mediante vectores (partes de columnas), lo que permite lograr una alta eficiencia de CPU.
-
-## Actualizaciones de datos en tiempo real {#real-time-data-updates}
-
-ClickHouse admite tablas con una clave principal. Para realizar consultas rápidamente en el rango de la clave principal, los datos se ordenan de forma incremental utilizando el árbol de combinación. Debido a esto, los datos se pueden agregar continuamente a la tabla. No se toman bloqueos cuando se ingieren nuevos datos.
-
-## Indice {#index}
-
-Tener un dato ordenado físicamente por clave principal permite extraer datos para sus valores específicos o rangos de valores con baja latencia, menos de unas pocas docenas de milisegundos.
-
-## Adecuado para consultas en línea {#suitable-for-online-queries}
-
-La baja latencia significa que las consultas se pueden procesar sin demora y sin intentar preparar una respuesta por adelantado, justo en el mismo momento mientras se carga la página de la interfaz de usuario. En otras palabras, en línea.
-
-## Soporte para cálculos aproximados {#support-for-approximated-calculations}
-
-ClickHouse proporciona varias formas de intercambiar precisión por rendimiento:
-
-1.  Funciones agregadas para el cálculo aproximado del número de valores distintos, medianas y cuantiles.
-2.  Ejecutar una consulta basada en una parte (muestra) de datos y obtener un resultado aproximado. En este caso, se recuperan proporcionalmente menos datos del disco.
-3.  Ejecutar una agregación para un número limitado de claves aleatorias, en lugar de para todas las claves. Bajo ciertas condiciones para la distribución de claves en los datos, esto proporciona un resultado razonablemente preciso mientras se utilizan menos recursos.
-
-## Replicación de datos e integridad de datos {#data-replication-and-data-integrity-support}
-
-ClickHouse utiliza la replicación multi-maestro asincrónica. Después de escribir en cualquier réplica disponible, todas las réplicas restantes recuperan su copia en segundo plano. El sistema mantiene datos idénticos en diferentes réplicas. La recuperación después de la mayoría de las fallas se realiza automáticamente, o semiautomáticamente en casos complejos.
-
-Para obtener más información, consulte la sección [Replicación de datos](../engines/table-engines/mergetree-family/replication.md).
-
-## Características que pueden considerarse desventajas {#clickhouse-features-that-can-be-considered-disadvantages}
-
-1.  No hay transacciones completas.
-2.  Falta de capacidad para modificar o eliminar datos ya insertados con alta tasa y baja latencia. Hay eliminaciones y actualizaciones por lotes disponibles para limpiar o modificar datos, por ejemplo, para cumplir con [GDPR](https://gdpr-info.eu).
-3.  El índice disperso hace que ClickHouse no sea tan adecuado para consultas de puntos que recuperan filas individuales por sus claves.
-
-[Artículo Original](https://clickhouse.tech/docs/en/introduction/distinctive_features/) <!--hide-->
diff --git a/docs/es/introduction/history.md b/docs/es/introduction/history.md
deleted file mode 100644
index 7311fa019599..000000000000
--- a/docs/es/introduction/history.md
+++ /dev/null
@@ -1,56 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 7
-toc_title: Historia
----
-
-# Historial de ClickHouse {#clickhouse-history}
-
-ClickHouse se ha desarrollado inicialmente para alimentar [El Yandex.Métrica](https://metrica.yandex.com/), [la segunda plataforma de análisis web más grande del mundo](http://w3techs.com/technologies/overview/traffic_analysis/all), y sigue siendo el componente central de este sistema. Con más de 13 billones de registros en la base de datos y más de 20 mil millones de eventos diarios, ClickHouse permite generar informes personalizados sobre la marcha directamente a partir de datos no agregados. Este artículo cubre brevemente los objetivos de ClickHouse en las primeras etapas de su desarrollo.
-
-El Yandex.Metrica construye informes personalizados sobre la marcha basados en hits y sesiones, con segmentos arbitrarios definidos por el usuario. Hacerlo a menudo requiere construir agregados complejos, como el número de usuarios únicos. Los nuevos datos para crear un informe llegan en tiempo real.
-
-A partir de abril de 2014, Yandex.Metrica estaba rastreando alrededor de 12 mil millones de eventos (vistas de páginas y clics) diariamente. Todos estos eventos deben almacenarse para crear informes personalizados. Una sola consulta puede requerir escanear millones de filas en unos pocos cientos de milisegundos, o cientos de millones de filas en solo unos segundos.
-
-## Uso en Yandex.Metrica y otros servicios de Yandex {#usage-in-yandex-metrica-and-other-yandex-services}
-
-ClickHouse sirve para múltiples propósitos en Yandex.Métrica.
-Su tarea principal es crear informes en modo en línea utilizando datos no agregados. Utiliza un clúster de 374 servidores, que almacenan más de 20,3 billones de filas en la base de datos. El volumen de datos comprimidos es de aproximadamente 2 PB, sin tener en cuenta duplicados y réplicas. El volumen de datos sin comprimir (en formato TSV) sería de aproximadamente 17 PB.
-
-ClickHouse también juega un papel clave en los siguientes procesos:
-
--   Almacenamiento de datos para Session Replay de Yandex.Métrica.
--   Procesamiento de datos intermedios.
--   Creación de informes globales con Analytics.
--   Ejecutar consultas para depurar el Yandex.Motor Metrica.
--   Análisis de registros desde la API y la interfaz de usuario.
-
-Hoy en día, hay varias docenas de instalaciones de ClickHouse en otros servicios y departamentos de Yandex: verticales de búsqueda, comercio electrónico, publicidad, análisis de negocios, desarrollo móvil, servicios personales y otros.
-
-## Datos agregados y no agregados {#aggregated-and-non-aggregated-data}
-
-Existe una opinión generalizada de que para calcular las estadísticas de manera efectiva, debe agregar datos ya que esto reduce el volumen de datos.
-
-Pero la agregación de datos viene con muchas limitaciones:
-
--   Debe tener una lista predefinida de los informes necesarios.
--   El usuario no puede hacer informes personalizados.
--   Al agregar sobre un gran número de claves distintas, el volumen de datos apenas se reduce, por lo que la agregación es inútil.
--   Para un gran número de informes, hay demasiadas variaciones de agregación (explosión combinatoria).
--   Al agregar claves con alta cardinalidad (como las URL), el volumen de datos no se reduce en mucho (menos del doble).
--   Por esta razón, el volumen de datos con agregación podría crecer en lugar de reducirse.
--   Los usuarios no ven todos los informes que generamos para ellos. Una gran parte de esos cálculos es inútil.
--   La integridad lógica de los datos puede ser violada para varias agregaciones.
-
-Si no agregamos nada y trabajamos con datos no agregados, esto podría reducir el volumen de cálculos.
-
-Sin embargo, con la agregación, una parte significativa del trabajo se desconecta y se completa con relativa calma. Por el contrario, los cálculos en línea requieren calcular lo más rápido posible, ya que el usuario está esperando el resultado.
-
-El Yandex.Metrica tiene un sistema especializado para agregar datos llamado Metrage, que se utilizó para la mayoría de los informes.
-A partir de 2009, Yandex.Metrica también utilizó una base de datos OLAP especializada para datos no agregados llamada OLAPServer, que anteriormente se usaba para el generador de informes.
-OLAPServer funcionó bien para datos no agregados, pero tenía muchas restricciones que no permitían que se utilizara para todos los informes según lo deseado. Estos incluyeron la falta de soporte para tipos de datos (solo números) y la incapacidad de actualizar datos de forma incremental en tiempo real (solo se podía hacer reescribiendo datos diariamente). OLAPServer no es un DBMS, sino una base de datos especializada.
-
-El objetivo inicial de ClickHouse era eliminar las limitaciones de OLAPServer y resolver el problema de trabajar con datos no agregados para todos los informes, pero a lo largo de los años, se ha convertido en un sistema de gestión de bases de datos de propósito general adecuado para una amplia gama de tareas analíticas.
-
-[Artículo Original](https://clickhouse.tech/docs/en/introduction/history/) <!--hide-->
diff --git a/docs/es/introduction/index.md b/docs/es/introduction/index.md
deleted file mode 100644
index 7026dc800e45..000000000000
--- a/docs/es/introduction/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Implantaci\xF3n"
-toc_priority: 1
----
-
-
diff --git a/docs/es/introduction/performance.md b/docs/es/introduction/performance.md
deleted file mode 100644
index 01640439128f..000000000000
--- a/docs/es/introduction/performance.md
+++ /dev/null
@@ -1,32 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 6
-toc_title: Rendimiento
----
-
-# Rendimiento {#performance}
-
-De acuerdo con los resultados de las pruebas internas en Yandex, ClickHouse muestra el mejor rendimiento (tanto el mayor rendimiento para consultas largas como la menor latencia en consultas cortas) para escenarios operativos comparables entre los sistemas de su clase que estaban disponibles para pruebas. Puede ver los resultados de la prueba en un [página separada](https://clickhouse.tech/benchmark/dbms/).
-
-Numerosos puntos de referencia independientes llegaron a conclusiones similares. No son difíciles de encontrar mediante una búsqueda en Internet, o se puede ver [nuestra pequeña colección de enlaces relacionados](https://clickhouse.tech/#independent-benchmarks).
-
-## Rendimiento para una única consulta grande {#throughput-for-a-single-large-query}
-
-El rendimiento se puede medir en filas por segundo o megabytes por segundo. Si los datos se colocan en la caché de la página, una consulta que no es demasiado compleja se procesa en hardware moderno a una velocidad de aproximadamente 2-10 GB / s de datos sin comprimir en un solo servidor (para los casos más sencillos, la velocidad puede alcanzar 30 GB / s). Si los datos no se colocan en la memoria caché de la página, la velocidad depende del subsistema de disco y la velocidad de compresión de datos. Por ejemplo, si el subsistema de disco permite leer datos a 400 MB/s y la tasa de compresión de datos es 3, se espera que la velocidad sea de alrededor de 1,2 GB/s. Para obtener la velocidad en filas por segundo, divida la velocidad en bytes por segundo por el tamaño total de las columnas utilizadas en la consulta. Por ejemplo, si se extraen 10 bytes de columnas, se espera que la velocidad sea de alrededor de 100-200 millones de filas por segundo.
-
-La velocidad de procesamiento aumenta casi linealmente para el procesamiento distribuido, pero solo si el número de filas resultantes de la agregación o la clasificación no es demasiado grande.
-
-## Latencia al procesar consultas cortas {#latency-when-processing-short-queries}
-
-Si una consulta usa una clave principal y no selecciona demasiadas columnas y filas para procesar (cientos de miles), puede esperar menos de 50 milisegundos de latencia (dígitos individuales de milisegundos en el mejor de los casos) si los datos se colocan en la memoria caché de la página. De lo contrario, la latencia está dominada principalmente por el número de búsquedas. Si utiliza unidades de disco giratorias, para un sistema que no está sobrecargado, la latencia se puede estimar con esta fórmula: `seek time (10 ms) * count of columns queried * count of data parts`.
-
-## Rendimiento al procesar una gran cantidad de consultas cortas {#throughput-when-processing-a-large-quantity-of-short-queries}
-
-En las mismas condiciones, ClickHouse puede manejar varios cientos de consultas por segundo en un solo servidor (hasta varios miles en el mejor de los casos). Dado que este escenario no es típico para DBMS analíticos, se recomienda esperar un máximo de 100 consultas por segundo.
-
-## Rendimiento al insertar datos {#performance-when-inserting-data}
-
-Recomendamos insertar datos en paquetes de al menos 1000 filas o no más de una sola solicitud por segundo. Al insertar en una tabla MergeTree desde un volcado separado por tabuladores, la velocidad de inserción puede ser de 50 a 200 MB/s. Si las filas insertadas tienen alrededor de 1 Kb de tamaño, la velocidad será de 50,000 a 200,000 filas por segundo. Si las filas son pequeñas, el rendimiento puede ser mayor en filas por segundo (en los datos del sistema Banner -`>` 500.000 filas por segundo; en datos de grafito -`>` 1.000.000 de filas por segundo). Para mejorar el rendimiento, puede realizar varias consultas INSERT en paralelo, que se escala linealmente.
-
-[Artículo Original](https://clickhouse.tech/docs/en/introduction/performance/) <!--hide-->
diff --git a/docs/es/operations/access-rights.md b/docs/es/operations/access-rights.md
deleted file mode 100644
index 6c777d9f081f..000000000000
--- a/docs/es/operations/access-rights.md
+++ /dev/null
@@ -1,143 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 48
-toc_title: "Control de acceso y gesti\xF3n de cuentas"
----
-
-# Control de acceso y gestión de cuentas {#access-control}
-
-ClickHouse admite la administración de control de acceso basada en [RBAC](https://en.wikipedia.org/wiki/Role-based_access_control) enfoque.
-
-Entidades de acceso de ClickHouse:
-- [Cuenta de usuario](#user-account-management)
-- [Rol](#role-management)
-- [Política de fila](#row-policy-management)
-- [Perfil de configuración](#settings-profiles-management)
-- [Cuota](#quotas-management)
-
-Puede configurar entidades de acceso utilizando:
-
--   Flujo de trabajo controlado por SQL.
-
-    Es necesario [permitir](#enabling-access-control) esta funcionalidad.
-
--   Servidor [archivos de configuración](configuration-files.md) `users.xml` y `config.xml`.
-
-Se recomienda utilizar el flujo de trabajo controlado por SQL. Ambos métodos de configuración funcionan simultáneamente, por lo que si utiliza los archivos de configuración del servidor para administrar cuentas y derechos de acceso, puede pasar suavemente al flujo de trabajo controlado por SQL.
-
-!!! note "Advertencia"
-    No puede administrar la misma entidad de acceso mediante ambos métodos de configuración simultáneamente.
-
-## Uso {#access-control-usage}
-
-De forma predeterminada, el servidor ClickHouse proporciona la cuenta de usuario `default` que no está permitido usar control de acceso controlado por SQL y administración de cuentas, pero tiene todos los derechos y permisos. El `default` cuenta de usuario se utiliza en cualquier caso cuando el nombre de usuario no está definido, por ejemplo, al iniciar sesión desde el cliente o en consultas distribuidas. En el procesamiento de consultas distribuidas se utiliza una cuenta de usuario predeterminada, si la configuración del servidor o clúster no [usuario y contraseña](../engines/table-engines/special/distributed.md) propiedad.
-
-Si acaba de comenzar a usar ClickHouse, puede usar el siguiente escenario:
-
-1.  [Permitir](#enabling-access-control) Control de acceso basado en SQL y gestión de cuentas `default` usuario.
-2.  Inicie sesión bajo el `default` cuenta de usuario y crear todos los usuarios. No olvides crear una cuenta de administrador (`GRANT ALL ON *.* WITH GRANT OPTION TO admin_user_account`).
-3.  [Restringir permisos](settings/permissions-for-queries.md#permissions_for_queries) para el `default` usuario y deshabilitar el control de acceso impulsado por SQL y la administración de cuentas para ello.
-
-### Propiedades de la solución actual {#access-control-properties}
-
--   Puede conceder permisos para bases de datos y tablas incluso si no existen.
--   Si se eliminó una tabla, no se revocarán todos los privilegios que corresponden a esta tabla. Por lo tanto, si se crea una nueva tabla más tarde con el mismo nombre, todos los privilegios vuelven a ser reales. Para revocar los privilegios correspondientes a la tabla eliminada, debe realizar, por ejemplo, el `REVOKE ALL PRIVILEGES ON db.table FROM ALL` consulta.
--   No hay ninguna configuración de por vida para los privilegios.
-
-## Cuenta de usuario {#user-account-management}
-
-Una cuenta de usuario es una entidad de acceso que permite autorizar a alguien en ClickHouse. Una cuenta de usuario contiene:
-
--   Información de identificación.
--   [Privilegio](../sql-reference/statements/grant.md#grant-privileges) que definen un ámbito de consultas que el usuario puede realizar.
--   Hosts desde los que se permite la conexión al servidor ClickHouse.
--   Roles otorgados y predeterminados.
--   Configuración con sus restricciones que se aplican de forma predeterminada en el inicio de sesión del usuario.
--   Perfiles de configuración asignados.
-
-Los privilegios a una cuenta de usuario pueden ser otorgados por el [GRANT](../sql-reference/statements/grant.md) consulta o asignando [rol](#role-management). Para revocar privilegios de un usuario, ClickHouse proporciona el [REVOKE](../sql-reference/statements/revoke.md) consulta. Para listar los privilegios de un usuario, utilice - [SHOW GRANTS](../sql-reference/statements/show.md#show-grants-statement) instrucción.
-
-Consultas de gestión:
-
--   [CREATE USER](../sql-reference/statements/create.md#create-user-statement)
--   [ALTER USER](../sql-reference/statements/alter.md#alter-user-statement)
--   [DROP USER](../sql-reference/statements/misc.md#drop-user-statement)
--   [SHOW CREATE USER](../sql-reference/statements/show.md#show-create-user-statement)
-
-### Ajustes Aplicación {#access-control-settings-applying}
-
-Los ajustes se pueden establecer de diferentes maneras: para una cuenta de usuario, en sus roles y perfiles de configuración concedidos. En un inicio de sesión de usuario, si se establece una configuración en diferentes entidades de acceso, el valor y las restricciones de esta configuración se aplican mediante las siguientes prioridades (de mayor a menor):
-
-1.  Configuración de la cuenta de usuario.
-2.  La configuración de los roles predeterminados de la cuenta de usuario. Si se establece una configuración en algunos roles, el orden de la configuración que se aplica no está definido.
-3.  La configuración de los perfiles de configuración asignados a un usuario o a sus roles predeterminados. Si se establece una configuración en algunos perfiles, el orden de aplicación de la configuración no está definido.
-4.  Ajustes aplicados a todo el servidor de forma predeterminada o desde el [perfil predeterminado](server-configuration-parameters/settings.md#default-profile).
-
-## Rol {#role-management}
-
-Role es un contenedor para las entidades de acceso que se pueden conceder a una cuenta de usuario.
-
-El rol contiene:
-
--   [Privilegio](../sql-reference/statements/grant.md#grant-privileges)
--   Configuración y restricciones
--   Lista de funciones concedidas
-
-Consultas de gestión:
-
--   [CREATE ROLE](../sql-reference/statements/create.md#create-role-statement)
--   [ALTER ROLE](../sql-reference/statements/alter.md#alter-role-statement)
--   [DROP ROLE](../sql-reference/statements/misc.md#drop-role-statement)
--   [SET ROLE](../sql-reference/statements/misc.md#set-role-statement)
--   [SET DEFAULT ROLE](../sql-reference/statements/misc.md#set-default-role-statement)
--   [SHOW CREATE ROLE](../sql-reference/statements/show.md#show-create-role-statement)
-
-Los privilegios a un rol pueden ser otorgados por el [GRANT](../sql-reference/statements/grant.md) consulta. Para revocar privilegios de un rol, ClickHouse proporciona el [REVOKE](../sql-reference/statements/revoke.md) consulta.
-
-## Política de fila {#row-policy-management}
-
-La directiva de filas es un filtro que define qué filas está disponible para un usuario o para un rol. La directiva de filas contiene filtros para una tabla específica y una lista de roles y/o usuarios que deben usar esta directiva de filas.
-
-Consultas de gestión:
-
--   [CREATE ROW POLICY](../sql-reference/statements/create.md#create-row-policy-statement)
--   [ALTER ROW POLICY](../sql-reference/statements/alter.md#alter-row-policy-statement)
--   [DROP ROW POLICY](../sql-reference/statements/misc.md#drop-row-policy-statement)
--   [SHOW CREATE ROW POLICY](../sql-reference/statements/show.md#show-create-row-policy-statement)
-
-## Perfil de configuración {#settings-profiles-management}
-
-El perfil de configuración es una colección de [configuración](settings/index.md). El perfil de configuración contiene configuraciones y restricciones, y una lista de roles y/o usuarios a los que se aplica esta cuota.
-
-Consultas de gestión:
-
--   [CREATE SETTINGS PROFILE](../sql-reference/statements/create.md#create-settings-profile-statement)
--   [ALTER SETTINGS PROFILE](../sql-reference/statements/alter.md#alter-settings-profile-statement)
--   [DROP SETTINGS PROFILE](../sql-reference/statements/misc.md#drop-settings-profile-statement)
--   [SHOW CREATE SETTINGS PROFILE](../sql-reference/statements/show.md#show-create-settings-profile-statement)
-
-## Cuota {#quotas-management}
-
-La cuota limita el uso de recursos. Ver [Cuota](quotas.md).
-
-La cuota contiene un conjunto de límites para algunas duraciones y una lista de roles y / o usuarios que deben usar esta cuota.
-
-Consultas de gestión:
-
--   [CREATE QUOTA](../sql-reference/statements/create.md#create-quota-statement)
--   [ALTER QUOTA](../sql-reference/statements/alter.md#alter-quota-statement)
--   [DROP QUOTA](../sql-reference/statements/misc.md#drop-quota-statement)
--   [SHOW CREATE QUOTA](../sql-reference/statements/show.md#show-create-quota-statement)
-
-## Habilitación del control de acceso basado en SQL y la administración de cuentas {#enabling-access-control}
-
--   Configure un directorio para el almacenamiento de configuraciones.
-
-    ClickHouse almacena las configuraciones de entidades de acceso en la carpeta [access_control_path](server-configuration-parameters/settings.md#access_control_path) parámetro de configuración del servidor.
-
--   Habilite el control de acceso controlado por SQL y la administración de cuentas para al menos una cuenta de usuario.
-
-    De forma predeterminada, el control de acceso controlado por SQL y la administración de cuentas se activan para todos los usuarios. Debe configurar al menos un usuario en el `users.xml` archivo de configuración y asigne 1 al [access_management](settings/settings-users.md#access_management-user-setting) configuración.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/access_rights/) <!--hide-->
diff --git a/docs/es/operations/backup.md b/docs/es/operations/backup.md
deleted file mode 100644
index be33851574a9..000000000000
--- a/docs/es/operations/backup.md
+++ /dev/null
@@ -1,39 +0,0 @@
----
-toc_priority: 49
-toc_title: Copia de seguridad de datos
----
-
-# Copia de seguridad de datos {#data-backup}
-
-Mientras que la [replicación](../engines/table-engines/mergetree-family/replication.md) proporciona protección contra fallos de hardware, no protege de errores humanos: el borrado accidental de datos, elminar la tabla equivocada o una tabla en el clúster equivocado, y bugs de software que dan como resultado un procesado incorrecto de los datos o la corrupción de los datos. En muchos casos, errores como estos afectarán a todas las réplicas. ClickHouse dispone de salvaguardas para prevenir algunos tipos de errores — por ejemplo, por defecto [no se puede simplemente eliminar tablas con un motor similar a MergeTree que contenga más de 50 Gb de datos](server-configuration-parameters/settings.md#max-table-size-to-drop). Sin embargo, estas salvaguardas no cubren todos los casos posibles y pueden eludirse.
-
-Para mitigar eficazmente los posibles errores humanos, debe preparar cuidadosamente una estrategia para realizar copias de seguridad y restaurar sus datos **previamente**.
-
-Cada empresa tiene diferentes recursos disponibles y requisitos comerciales, por lo que no existe una solución universal para las copias de seguridad y restauraciones de ClickHouse que se adapten a cada situación. Lo que funciona para un gigabyte de datos probablemente no funcionará para decenas de petabytes. Hay una variedad de posibles enfoques con sus propios pros y contras, que se discutirán a continuación. Es una buena idea utilizar varios enfoques en lugar de uno solo para compensar sus diversas deficiencias.
-
-!!! note "Nota"
-    Tenga en cuenta que si realizó una copia de seguridad de algo y nunca intentó restaurarlo, es probable que la restauración no funcione correctamente cuando realmente la necesite (o al menos tomará más tiempo de lo que las empresas pueden tolerar). Por lo tanto, cualquiera que sea el enfoque de copia de seguridad que elija, asegúrese de automatizar el proceso de restauración también y ponerlo en practica en un clúster de ClickHouse de repuesto regularmente.
-
-## Duplicar datos de origen en otro lugar {#duplicating-source-data-somewhere-else}
-
-A menudo, los datos que se ingieren en ClickHouse se entregan a través de algún tipo de cola persistente, como [Acerca de nosotros](https://kafka.apache.org). En este caso, es posible configurar un conjunto adicional de suscriptores que leerá el mismo flujo de datos mientras se escribe en ClickHouse y lo almacenará en almacenamiento en frío en algún lugar. La mayoría de las empresas ya tienen algún almacenamiento en frío recomendado por defecto, que podría ser un almacén de objetos o un sistema de archivos distribuido como [HDFS](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html).
-
-## Instantáneas del sistema de archivos {#filesystem-snapshots}
-
-Algunos sistemas de archivos locales proporcionan funcionalidad de instantánea (por ejemplo, [ZFS](https://en.wikipedia.org/wiki/ZFS)), pero podrían no ser la mejor opción para servir consultas en vivo. Una posible solución es crear réplicas adicionales con este tipo de sistema de archivos y excluirlas del [Distribuido](../engines/table-engines/special/distributed.md) tablas que se utilizan para `SELECT` consulta. Las instantáneas en tales réplicas estarán fuera del alcance de cualquier consulta que modifique los datos. Como beneficio adicional, estas réplicas podrían tener configuraciones de hardware especiales con más discos conectados por servidor, lo que sería rentable.
-
-## Método de codificación de datos: {#clickhouse-copier}
-
-[Método de codificación de datos:](utilities/clickhouse-copier.md) es una herramienta versátil que se creó inicialmente para volver a dividir tablas de tamaño petabyte. También se puede usar con fines de copia de seguridad y restauración porque copia datos de forma fiable entre tablas y clústeres de ClickHouse.
-
-Para volúmenes de datos más pequeños, un simple `INSERT INTO ... SELECT ...` a tablas remotas podría funcionar también.
-
-## Manipulaciones con piezas {#manipulations-with-parts}
-
-ClickHouse permite usar la consulta `ALTER TABLE ... FREEZE PARTITION ...` para crear una copia local de particiones de tabla. Esto se implementa utilizando enlaces duros a la carpeta `/var/lib/clickhouse/shadow/`, por lo que generalmente no consume espacio adicional en disco para datos antiguos. Las copias creadas de archivos no son manejadas por el servidor ClickHouse, por lo que puede dejarlas allí: tendrá una copia de seguridad simple que no requiere ningún sistema externo adicional, pero seguirá siendo propenso a problemas de hardware. Por esta razón, es mejor copiarlos de forma remota en otra ubicación y luego eliminar las copias locales. Los sistemas de archivos distribuidos y los almacenes de objetos siguen siendo una buena opción para esto, pero los servidores de archivos conectados normales con una capacidad lo suficientemente grande podrían funcionar también (en este caso, la transferencia ocurrirá a través del sistema de archivos de red o tal vez [rsync](https://en.wikipedia.org/wiki/Rsync)).
-
-Para obtener más información sobre las consultas relacionadas con las manipulaciones de particiones, consulte [Documentación de ALTER](../sql-reference/statements/alter.md#alter_manipulations-with-partitions).
-
-Una herramienta de terceros está disponible para automatizar este enfoque: [Haga clic en el botón de copia de seguridad](https://github.com/AlexAkulov/clickhouse-backup).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/backup/) <!--hide-->
diff --git a/docs/es/operations/configuration-files.md b/docs/es/operations/configuration-files.md
deleted file mode 100644
index d9aa85678686..000000000000
--- a/docs/es/operations/configuration-files.md
+++ /dev/null
@@ -1,57 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 50
-toc_title: "Archivos de configuraci\xF3n"
----
-
-# Archivos de configuración {#configuration_files}
-
-ClickHouse admite la administración de configuración de varios archivos. El archivo de configuración del servidor principal es `/etc/clickhouse-server/config.xml`. Otros archivos deben estar en el `/etc/clickhouse-server/config.d` directorio.
-
-!!! note "Nota"
-    Todos los archivos de configuración deben estar en formato XML. Además, deben tener el mismo elemento raíz, generalmente `<yandex>`.
-
-Algunos valores especificados en el archivo de configuración principal se pueden anular en otros archivos de configuración. El `replace` o `remove` se pueden especificar atributos para los elementos de estos archivos de configuración.
-
-Si no se especifica ninguno, combina el contenido de los elementos de forma recursiva, reemplazando los valores de los elementos secundarios duplicados.
-
-Si `replace` se especifica, reemplaza todo el elemento por el especificado.
-
-Si `remove` se especifica, elimina el elemento.
-
-La configuración también puede definir “substitutions”. Si un elemento tiene el `incl` atributo, la sustitución correspondiente del archivo se utilizará como el valor. De forma predeterminada, la ruta al archivo con sustituciones es `/etc/metrika.xml`. Esto se puede cambiar en el [include_from](server-configuration-parameters/settings.md#server_configuration_parameters-include_from) elemento en la configuración del servidor. Los valores de sustitución se especifican en `/yandex/substitution_name` elementos en este archivo. Si una sustitución especificada en `incl` no existe, se registra en el registro. Para evitar que ClickHouse registre las sustituciones que faltan, especifique `optional="true"` atributo (por ejemplo, ajustes para [macro](server-configuration-parameters/settings.md)).
-
-Las sustituciones también se pueden realizar desde ZooKeeper. Para hacer esto, especifique el atributo `from_zk = "/path/to/node"`. El valor del elemento se sustituye por el contenido del nodo en `/path/to/node` en ZooKeeper. También puede colocar un subárbol XML completo en el nodo ZooKeeper y se insertará completamente en el elemento de origen.
-
-El `config.xml` file puede especificar una configuración separada con configuraciones de usuario, perfiles y cuotas. La ruta relativa a esta configuración se establece en el `users_config` elemento. Por defecto, es `users.xml`. Si `users_config` se omite, la configuración de usuario, los perfiles y las cuotas se especifican directamente en `config.xml`.
-
-La configuración de los usuarios se puede dividir en archivos separados similares a `config.xml` y `config.d/`.
-El nombre del directorio se define como `users_config` sin `.xml` postfix concatenado con `.d`.
-Directorio `users.d` se utiliza por defecto, como `users_config` por defecto `users.xml`.
-Por ejemplo, puede tener un archivo de configuración separado para cada usuario como este:
-
-``` bash
-$ cat /etc/clickhouse-server/users.d/alice.xml
-```
-
-``` xml
-<yandex>
-    <users>
-      <alice>
-          <profile>analytics</profile>
-            <networks>
-                  <ip>::/0</ip>
-            </networks>
-          <password_sha256_hex>...</password_sha256_hex>
-          <quota>analytics</quota>
-      </alice>
-    </users>
-</yandex>
-```
-
-Para cada archivo de configuración, el servidor también genera `file-preprocessed.xml` archivos al iniciar. Estos archivos contienen todas las sustituciones y anulaciones completadas, y están destinados para uso informativo. Si se utilizaron sustituciones de ZooKeeper en los archivos de configuración pero ZooKeeper no está disponible en el inicio del servidor, el servidor carga la configuración desde el archivo preprocesado.
-
-El servidor realiza un seguimiento de los cambios en los archivos de configuración, así como archivos y nodos ZooKeeper que se utilizaron al realizar sustituciones y anulaciones, y vuelve a cargar la configuración de los usuarios y clústeres sobre la marcha. Esto significa que puede modificar el clúster, los usuarios y su configuración sin reiniciar el servidor.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/configuration_files/) <!--hide-->
diff --git a/docs/es/operations/index.md b/docs/es/operations/index.md
deleted file mode 100644
index 9a928fa0f01e..000000000000
--- a/docs/es/operations/index.md
+++ /dev/null
@@ -1,28 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Operaci\xF3n"
-toc_priority: 41
-toc_title: "Implantaci\xF3n"
----
-
-# Operación {#operations}
-
-El manual de operaciones de ClickHouse consta de las siguientes secciones principales:
-
--   [Requisito](requirements.md)
--   [Monitoreo](monitoring.md)
--   [Solución de problemas](troubleshooting.md)
--   [Recomendaciones de uso](tips.md)
--   [Procedimiento de actualización](update.md)
--   [Derechos de acceso](access-rights.md)
--   [Copia de seguridad de datos](backup.md)
--   [Archivos de configuración](configuration-files.md)
--   [Cuota](quotas.md)
--   [Tablas del sistema](system-tables.md)
--   [Parámetros de configuración del servidor](server-configuration-parameters/index.md)
--   [Cómo probar su hardware con ClickHouse](performance-test.md)
--   [Configuración](settings/index.md)
--   [Utilidad](utilities/index.md)
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/operations/) ##}
diff --git a/docs/es/operations/monitoring.md b/docs/es/operations/monitoring.md
deleted file mode 100644
index 19912d23f3b5..000000000000
--- a/docs/es/operations/monitoring.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: Monitoreo
----
-
-# Monitoreo {#monitoring}
-
-Usted puede monitorear:
-
--   Utilización de recursos de hardware.
--   Métricas del servidor ClickHouse.
-
-## Utilización de recursos {#resource-utilization}
-
-ClickHouse no supervisa el estado de los recursos de hardware por sí mismo.
-
-Se recomienda encarecidamente configurar la supervisión para:
-
--   Carga y temperatura en los procesadores.
-
-    Usted puede utilizar [dmesg](https://en.wikipedia.org/wiki/Dmesg), [Turbostat](https://www.linux.org/docs/man8/turbostat.html) u otros instrumentos.
-
--   Utilización del sistema de almacenamiento, RAM y red.
-
-## Métricas del servidor ClickHouse {#clickhouse-server-metrics}
-
-El servidor ClickHouse tiene instrumentos integrados para el monitoreo de estado propio.
-
-Para realizar un seguimiento de los eventos del servidor, use los registros del servidor. Ver el [registrador](server-configuration-parameters/settings.md#server_configuration_parameters-logger) sección del archivo de configuración.
-
-ClickHouse recoge:
-
--   Diferentes métricas de cómo el servidor utiliza recursos computacionales.
--   Estadísticas comunes sobre el procesamiento de consultas.
-
-Puede encontrar métricas en el [sistema.métricas](../operations/system-tables.md#system_tables-metrics), [sistema.evento](../operations/system-tables.md#system_tables-events), y [sistema.asynchronous_metrics](../operations/system-tables.md#system_tables-asynchronous_metrics) tabla.
-
-Puede configurar ClickHouse para exportar métricas a [Grafito](https://github.com/graphite-project). Ver el [Sección de grafito](server-configuration-parameters/settings.md#server_configuration_parameters-graphite) en el archivo de configuración del servidor ClickHouse. Antes de configurar la exportación de métricas, debe configurar Graphite siguiendo sus [guiar](https://graphite.readthedocs.io/en/latest/install.html).
-
-Puede configurar ClickHouse para exportar métricas a [Prometeo](https://prometheus.io). Ver el [Sección Prometheus](server-configuration-parameters/settings.md#server_configuration_parameters-prometheus) en el archivo de configuración del servidor ClickHouse. Antes de configurar la exportación de métricas, debe configurar Prometheus siguiendo su oficial [guiar](https://prometheus.io/docs/prometheus/latest/installation/).
-
-Además, puede supervisar la disponibilidad del servidor a través de la API HTTP. Enviar el `HTTP GET` solicitud de `/ping`. Si el servidor está disponible, responde con `200 OK`.
-
-Para supervisar servidores en una configuración de clúster, debe establecer [max_replica_delay_for_distributed_queries](settings/settings.md#settings-max_replica_delay_for_distributed_queries) parámetro y utilizar el recurso HTTP `/replicas_status`. Una solicitud para `/replicas_status` devoluciones `200 OK` si la réplica está disponible y no se retrasa detrás de las otras réplicas. Si una réplica se retrasa, devuelve `503 HTTP_SERVICE_UNAVAILABLE` con información sobre la brecha.
diff --git a/docs/es/operations/optimizing-performance/index.md b/docs/es/operations/optimizing-performance/index.md
deleted file mode 100644
index d2796c6e0d39..000000000000
--- a/docs/es/operations/optimizing-performance/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Optimizaci\xF3n del rendimiento"
-toc_priority: 52
----
-
-
diff --git a/docs/es/operations/optimizing-performance/sampling-query-profiler.md b/docs/es/operations/optimizing-performance/sampling-query-profiler.md
deleted file mode 100644
index a474dde6af2a..000000000000
--- a/docs/es/operations/optimizing-performance/sampling-query-profiler.md
+++ /dev/null
@@ -1,64 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 54
-toc_title: "Generaci\xF3n de perfiles de consultas"
----
-
-# Analizador de consultas de muestreo {#sampling-query-profiler}
-
-ClickHouse ejecuta el generador de perfiles de muestreo que permite analizar la ejecución de consultas. Utilizando el generador de perfiles puede encontrar rutinas de código fuente que se utilizan con más frecuencia durante la ejecución de la consulta. Puede rastrear el tiempo de CPU y el tiempo de reloj de pared invertido, incluido el tiempo de inactividad.
-
-Para usar el generador de perfiles:
-
--   Configurar el [trace_log](../server-configuration-parameters/settings.md#server_configuration_parameters-trace_log) sección de la configuración del servidor.
-
-    Esta sección configura la [trace_log](../../operations/system-tables.md#system_tables-trace_log) tabla del sistema que contiene los resultados del funcionamiento del generador de perfiles. Está configurado de forma predeterminada. Recuerde que los datos de esta tabla solo son válidos para un servidor en ejecución. Después de reiniciar el servidor, ClickHouse no limpia la tabla y toda la dirección de memoria virtual almacenada puede dejar de ser válida.
-
--   Configurar el [Los resultados de la prueba](../settings/settings.md#query_profiler_cpu_time_period_ns) o [query_profiler_real_time_period_ns](../settings/settings.md#query_profiler_real_time_period_ns) configuración. Ambos ajustes se pueden utilizar simultáneamente.
-
-    Estas opciones le permiten configurar temporizadores del generador de perfiles. Como estos son los ajustes de sesión, puede obtener diferentes frecuencias de muestreo para todo el servidor, usuarios individuales o perfiles de usuario, para su sesión interactiva y para cada consulta individual.
-
-La frecuencia de muestreo predeterminada es una muestra por segundo y tanto la CPU como los temporizadores reales están habilitados. Esta frecuencia permite recopilar suficiente información sobre el clúster ClickHouse. Al mismo tiempo, al trabajar con esta frecuencia, el generador de perfiles no afecta el rendimiento del servidor ClickHouse. Si necesita perfilar cada consulta individual, intente usar una mayor frecuencia de muestreo.
-
-Para analizar el `trace_log` tabla del sistema:
-
--   Instale el `clickhouse-common-static-dbg` paquete. Ver [Instalar desde paquetes DEB](../../getting-started/install.md#install-from-deb-packages).
-
--   Permitir funciones de introspección [allow_introspection_functions](../settings/settings.md#settings-allow_introspection_functions) configuración.
-
-    Por razones de seguridad, las funciones de introspección están deshabilitadas de forma predeterminada.
-
--   Utilice el `addressToLine`, `addressToSymbol` y `demangle` [funciones de la introspección](../../sql-reference/functions/introspection.md) para obtener nombres de funciones y sus posiciones en el código ClickHouse. Para obtener un perfil para alguna consulta, debe agregar datos del `trace_log` tabla. Puede agregar datos por funciones individuales o por los seguimientos de pila completos.
-
-Si necesita visualizar `trace_log` información, intente [Flamegraph](../../interfaces/third-party/gui/#clickhouse-flamegraph) y [Nivel de Cifrado WEP](https://github.com/laplab/clickhouse-speedscope).
-
-## Ejemplo {#example}
-
-En este ejemplo nos:
-
--   Filtrado `trace_log` datos por un identificador de consulta y la fecha actual.
-
--   Agregando por seguimiento de pila.
-
--   Usando funciones de introspección, obtendremos un informe de:
-
-    -   Nombres de símbolos y funciones de código fuente correspondientes.
-    -   Ubicaciones del código fuente de estas funciones.
-
-<!-- -->
-
-``` sql
-SELECT
-    count(),
-    arrayStringConcat(arrayMap(x -> concat(demangle(addressToSymbol(x)), '
    ', addressToLine(x)), trace), '
') AS sym
-FROM system.trace_log
-WHERE (query_id = 'ebca3574-ad0a-400a-9cbc-dca382f5998c') AND (event_date = today())
-GROUP BY trace
-ORDER BY count() DESC
-LIMIT 10
-```
-
-``` text
-{% include "examples/sampling_query_profiler_result.txt" %}
-```
diff --git a/docs/es/operations/quotas.md b/docs/es/operations/quotas.md
deleted file mode 100644
index 9d84ce213394..000000000000
--- a/docs/es/operations/quotas.md
+++ /dev/null
@@ -1,112 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 51
-toc_title: Cuota
----
-
-# Cuota {#quotas}
-
-Las cuotas le permiten limitar el uso de recursos durante un período de tiempo o realizar un seguimiento del uso de recursos.
-Las cuotas se configuran en la configuración del usuario, que generalmente ‘users.xml’.
-
-El sistema también tiene una característica para limitar la complejidad de una sola consulta. Vea la sección “Restrictions on query complexity”).
-
-A diferencia de las restricciones de complejidad de consultas, las cuotas:
-
--   Coloque restricciones en un conjunto de consultas que se pueden ejecutar durante un período de tiempo, en lugar de limitar una sola consulta.
--   Tenga en cuenta los recursos gastados en todos los servidores remotos para el procesamiento de consultas distribuidas.
-
-Veamos la sección del ‘users.xml’ fichero que define las cuotas.
-
-``` xml
-<!-- Quotas -->
-<quotas>
-    <!-- Quota name. -->
-    <default>
-        <!-- Restrictions for a time period. You can set many intervals with different restrictions. -->
-        <interval>
-            <!-- Length of the interval. -->
-            <duration>3600</duration>
-
-            <!-- Unlimited. Just collect data for the specified time interval. -->
-            <queries>0</queries>
-            <errors>0</errors>
-            <result_rows>0</result_rows>
-            <read_rows>0</read_rows>
-            <execution_time>0</execution_time>
-        </interval>
-    </default>
-```
-
-De forma predeterminada, la cuota realiza un seguimiento del consumo de recursos para cada hora, sin limitar el uso.
-El consumo de recursos calculado para cada intervalo se envía al registro del servidor después de cada solicitud.
-
-``` xml
-<statbox>
-    <!-- Restrictions for a time period. You can set many intervals with different restrictions. -->
-    <interval>
-        <!-- Length of the interval. -->
-        <duration>3600</duration>
-
-        <queries>1000</queries>
-        <errors>100</errors>
-        <result_rows>1000000000</result_rows>
-        <read_rows>100000000000</read_rows>
-        <execution_time>900</execution_time>
-    </interval>
-
-    <interval>
-        <duration>86400</duration>
-
-        <queries>10000</queries>
-        <errors>1000</errors>
-        <result_rows>5000000000</result_rows>
-        <read_rows>500000000000</read_rows>
-        <execution_time>7200</execution_time>
-    </interval>
-</statbox>
-```
-
-Para el ‘statbox’ Las restricciones se establecen por cada hora y por cada 24 horas (86.400 segundos). El intervalo de tiempo se cuenta, a partir de un momento fijo definido por la implementación en el tiempo. En otras palabras, el intervalo de 24 horas no necesariamente comienza a medianoche.
-
-Cuando finaliza el intervalo, se borran todos los valores recopilados. Para la siguiente hora, el cálculo de la cuota comienza de nuevo.
-
-Estas son las cantidades que se pueden restringir:
-
-`queries` – The total number of requests.
-
-`errors` – The number of queries that threw an exception.
-
-`result_rows` – The total number of rows given as a result.
-
-`read_rows` – The total number of source rows read from tables for running the query on all remote servers.
-
-`execution_time` – The total query execution time, in seconds (wall time).
-
-Si se excede el límite durante al menos un intervalo de tiempo, se lanza una excepción con un texto sobre qué restricción se excedió, para qué intervalo y cuándo comienza el nuevo intervalo (cuando se pueden enviar consultas nuevamente).
-
-Las cuotas pueden usar el “quota key” característica para informar sobre los recursos para múltiples claves de forma independiente. Aquí hay un ejemplo de esto:
-
-``` xml
-<!-- For the global reports designer. -->
-<web_global>
-    <!-- keyed – The quota_key "key" is passed in the query parameter,
-            and the quota is tracked separately for each key value.
-        For example, you can pass a Yandex.Metrica username as the key,
-            so the quota will be counted separately for each username.
-        Using keys makes sense only if quota_key is transmitted by the program, not by a user.
-
-        You can also write <keyed_by_ip />, so the IP address is used as the quota key.
-        (But keep in mind that users can change the IPv6 address fairly easily.)
-    -->
-    <keyed />
-```
-
-La cuota se asigna a los usuarios ‘users’ sección de la configuración. Vea la sección “Access rights”.
-
-Para el procesamiento de consultas distribuidas, los importes acumulados se almacenan en el servidor del solicitante. Entonces, si el usuario va a otro servidor, la cuota allí “start over”.
-
-Cuando se reinicia el servidor, las cuotas se restablecen.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/quotas/) <!--hide-->
diff --git a/docs/es/operations/requirements.md b/docs/es/operations/requirements.md
deleted file mode 100644
index d6f0f25cf215..000000000000
--- a/docs/es/operations/requirements.md
+++ /dev/null
@@ -1,61 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: Requisito
----
-
-# Requisito {#requirements}
-
-## CPU {#cpu}
-
-Para la instalación desde paquetes deb precompilados, utilice una CPU con arquitectura x86_64 y soporte para las instrucciones de SSE 4.2. Para ejecutar ClickHouse con procesadores que no admiten SSE 4.2 o tienen arquitectura AArch64 o PowerPC64LE, debe compilar ClickHouse a partir de fuentes.
-
-ClickHouse implementa el procesamiento de datos paralelo y utiliza todos los recursos de hardware disponibles. Al elegir un procesador, tenga en cuenta que ClickHouse funciona de manera más eficiente en configuraciones con un gran número de núcleos pero con una velocidad de reloj más baja que en configuraciones con menos núcleos y una velocidad de reloj más alta. Por ejemplo, 16 núcleos con 2600 MHz es preferible a 8 núcleos con 3600 MHz.
-
-Se recomienda usar **Impulso de Turbo** y **hiper-threading** tecnología. Mejora significativamente el rendimiento con una carga de trabajo típica.
-
-## RAM {#ram}
-
-Recomendamos utilizar un mínimo de 4 GB de RAM para realizar consultas no triviales. El servidor ClickHouse puede ejecutarse con una cantidad mucho menor de RAM, pero requiere memoria para procesar consultas.
-
-El volumen requerido de RAM depende de:
-
--   La complejidad de las consultas.
--   La cantidad de datos que se procesan en las consultas.
-
-Para calcular el volumen requerido de RAM, debe estimar el tamaño de los datos temporales para [GROUP BY](../sql-reference/statements/select/group-by.md#select-group-by-clause), [DISTINCT](../sql-reference/statements/select/distinct.md#select-distinct), [JOIN](../sql-reference/statements/select/join.md#select-join) y otras operaciones que utilice.
-
-ClickHouse puede usar memoria externa para datos temporales. Ver [GROUP BY en memoria externa](../sql-reference/statements/select/group-by.md#select-group-by-in-external-memory) para más detalles.
-
-## Archivo de intercambio {#swap-file}
-
-Deshabilite el archivo de intercambio para entornos de producción.
-
-## Subsistema de almacenamiento {#storage-subsystem}
-
-Necesita tener 2 GB de espacio libre en disco para instalar ClickHouse.
-
-El volumen de almacenamiento requerido para sus datos debe calcularse por separado. La evaluación debe incluir:
-
--   Estimación del volumen de datos.
-
-    Puede tomar una muestra de los datos y obtener el tamaño promedio de una fila de ella. Luego multiplique el valor por el número de filas que planea almacenar.
-
--   El coeficiente de compresión de datos.
-
-    Para estimar el coeficiente de compresión de datos, cargue una muestra de sus datos en ClickHouse y compare el tamaño real de los datos con el tamaño de la tabla almacenada. Por ejemplo, los datos de clickstream generalmente se comprimen de 6 a 10 veces.
-
-Para calcular el volumen final de datos que se almacenarán, aplique el coeficiente de compresión al volumen de datos estimado. Si planea almacenar datos en varias réplicas, multiplique el volumen estimado por el número de réplicas.
-
-## Red {#network}
-
-Si es posible, use redes de 10G o clase superior.
-
-El ancho de banda de la red es fundamental para procesar consultas distribuidas con una gran cantidad de datos intermedios. Además, la velocidad de la red afecta a los procesos de replicación.
-
-## Software {#software}
-
-ClickHouse está desarrollado principalmente para la familia de sistemas operativos Linux. La distribución de Linux recomendada es Ubuntu. El `tzdata` paquete debe ser instalado en el sistema.
-
-ClickHouse también puede funcionar en otras familias de sistemas operativos. Ver detalles en el [Primeros pasos](../getting-started/index.md) sección de la documentación.
diff --git a/docs/es/operations/server-configuration-parameters/index.md b/docs/es/operations/server-configuration-parameters/index.md
deleted file mode 100644
index e1e2e777b94f..000000000000
--- a/docs/es/operations/server-configuration-parameters/index.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Par\xE1metros de configuraci\xF3n del servidor"
-toc_priority: 54
-toc_title: "Implantaci\xF3n"
----
-
-# Parámetros de configuración del servidor {#server-settings}
-
-Esta sección contiene descripciones de la configuración del servidor que no se puede cambiar en el nivel de sesión o consulta.
-
-Estos ajustes se almacenan en el `config.xml` archivo en el servidor ClickHouse.
-
-Otros ajustes se describen en el “[Configuración](../settings/index.md#session-settings-intro)” apartado.
-
-Antes de estudiar la configuración, lea el [Archivos de configuración](../configuration-files.md#configuration_files) sección y tomar nota del uso de sustituciones (el `incl` y `optional` atributo).
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/server_configuration_parameters/) <!--hide-->
diff --git a/docs/es/operations/server-configuration-parameters/settings.md b/docs/es/operations/server-configuration-parameters/settings.md
deleted file mode 100644
index 86264ed04406..000000000000
--- a/docs/es/operations/server-configuration-parameters/settings.md
+++ /dev/null
@@ -1,906 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 57
-toc_title: "Configuraci\xF3n del servidor"
----
-
-# Configuración del servidor {#server-settings}
-
-## builtin_dictionaries_reload_interval {#builtin-dictionaries-reload-interval}
-
-El intervalo en segundos antes de volver a cargar los diccionarios integrados.
-
-ClickHouse recarga los diccionarios incorporados cada x segundos. Esto hace posible editar diccionarios “on the fly” sin reiniciar el servidor.
-
-Valor predeterminado: 3600.
-
-**Ejemplo**
-
-``` xml
-<builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
-```
-
-## compresión {#server-settings-compression}
-
-Ajustes de compresión de datos para [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md)-mesas de motor.
-
-!!! warning "Advertencia"
-    No lo use si acaba de comenzar a usar ClickHouse.
-
-Plantilla de configuración:
-
-``` xml
-<compression>
-    <case>
-      <min_part_size>...</min_part_size>
-      <min_part_size_ratio>...</min_part_size_ratio>
-      <method>...</method>
-    </case>
-    ...
-</compression>
-```
-
-`<case>` campo:
-
--   `min_part_size` – The minimum size of a data part.
--   `min_part_size_ratio` – The ratio of the data part size to the table size.
--   `method` – Compression method. Acceptable values: `lz4` o `zstd`.
-
-Puede configurar múltiples `<case>` apartado.
-
-Acciones cuando se cumplen las condiciones:
-
--   Si un elemento de datos coincide con un conjunto de condiciones, ClickHouse utiliza el método de compresión especificado.
--   Si un elemento de datos coincide con varios conjuntos de condiciones, ClickHouse utiliza el primer conjunto de condiciones coincidente.
-
-Si no se cumplen condiciones para un elemento de datos, ClickHouse utiliza el `lz4` compresión.
-
-**Ejemplo**
-
-``` xml
-<compression incl="clickhouse_compression">
-    <case>
-        <min_part_size>10000000000</min_part_size>
-        <min_part_size_ratio>0.01</min_part_size_ratio>
-        <method>zstd</method>
-    </case>
-</compression>
-```
-
-## default_database {#default-database}
-
-La base de datos predeterminada.
-
-Para obtener una lista de bases de datos, [SHOW DATABASES](../../sql-reference/statements/show.md#show-databases) consulta.
-
-**Ejemplo**
-
-``` xml
-<default_database>default</default_database>
-```
-
-## default_profile {#default-profile}
-
-Perfil de configuración predeterminado.
-
-Los perfiles de configuración se encuentran en el archivo especificado en el parámetro `user_config`.
-
-**Ejemplo**
-
-``` xml
-<default_profile>default</default_profile>
-```
-
-## Diccionarios_config {#server_configuration_parameters-dictionaries_config}
-
-La ruta de acceso al archivo de configuración para diccionarios externos.
-
-Camino:
-
--   Especifique la ruta absoluta o la ruta relativa al archivo de configuración del servidor.
--   La ruta puede contener comodines \* y ?.
-
-Ver también “[Diccionarios externos](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md)”.
-
-**Ejemplo**
-
-``` xml
-<dictionaries_config>*_dictionary.xml</dictionaries_config>
-```
-
-## Diccionarios_lazy_load {#server_configuration_parameters-dictionaries_lazy_load}
-
-La carga perezosa de los diccionarios.
-
-Si `true`, entonces cada diccionario es creado en el primer uso. Si se produce un error en la creación del diccionario, la función que estaba utilizando el diccionario produce una excepción.
-
-Si `false`, todos los diccionarios se crean cuando se inicia el servidor, y si hay un error, el servidor se apaga.
-
-El valor predeterminado es `true`.
-
-**Ejemplo**
-
-``` xml
-<dictionaries_lazy_load>true</dictionaries_lazy_load>
-```
-
-## format_schema_path {#server_configuration_parameters-format_schema_path}
-
-La ruta de acceso al directorio con los esquemas para los datos de entrada, como los esquemas [CapnProto](../../interfaces/formats.md#capnproto) formato.
-
-**Ejemplo**
-
-``` xml
-  <!-- Directory containing schema files for various input formats. -->
-  <format_schema_path>format_schemas/</format_schema_path>
-```
-
-## grafito {#server_configuration_parameters-graphite}
-
-Envío de datos a [Grafito](https://github.com/graphite-project).
-
-Configuración:
-
--   host – The Graphite server.
--   port – The port on the Graphite server.
--   interval – The interval for sending, in seconds.
--   timeout – The timeout for sending data, in seconds.
--   root_path – Prefix for keys.
--   metrics – Sending data from the [sistema.métricas](../../operations/system-tables.md#system_tables-metrics) tabla.
--   events – Sending deltas data accumulated for the time period from the [sistema.evento](../../operations/system-tables.md#system_tables-events) tabla.
--   events_cumulative – Sending cumulative data from the [sistema.evento](../../operations/system-tables.md#system_tables-events) tabla.
--   asynchronous_metrics – Sending data from the [sistema.asynchronous_metrics](../../operations/system-tables.md#system_tables-asynchronous_metrics) tabla.
-
-Puede configurar múltiples `<graphite>` clausula. Por ejemplo, puede usar esto para enviar datos diferentes a intervalos diferentes.
-
-**Ejemplo**
-
-``` xml
-<graphite>
-    <host>localhost</host>
-    <port>42000</port>
-    <timeout>0.1</timeout>
-    <interval>60</interval>
-    <root_path>one_min</root_path>
-    <metrics>true</metrics>
-    <events>true</events>
-    <events_cumulative>false</events_cumulative>
-    <asynchronous_metrics>true</asynchronous_metrics>
-</graphite>
-```
-
-## graphite_rollup {#server_configuration_parameters-graphite-rollup}
-
-Ajustes para reducir los datos de grafito.
-
-Para obtener más información, consulte [GraphiteMergeTree](../../engines/table-engines/mergetree-family/graphitemergetree.md).
-
-**Ejemplo**
-
-``` xml
-<graphite_rollup_example>
-    <default>
-        <function>max</function>
-        <retention>
-            <age>0</age>
-            <precision>60</precision>
-        </retention>
-        <retention>
-            <age>3600</age>
-            <precision>300</precision>
-        </retention>
-        <retention>
-            <age>86400</age>
-            <precision>3600</precision>
-        </retention>
-    </default>
-</graphite_rollup_example>
-```
-
-## http_port/https_port {#http-porthttps-port}
-
-El puerto para conectarse al servidor a través de HTTP(s).
-
-Si `https_port` se especifica, [openSSL](#server_configuration_parameters-openssl) debe ser configurado.
-
-Si `http_port` se especifica, la configuración de OpenSSL se ignora incluso si está establecida.
-
-**Ejemplo**
-
-``` xml
-<https_port>9999</https_port>
-```
-
-## http_server_default_response {#server_configuration_parameters-http_server_default_response}
-
-La página que se muestra de forma predeterminada al acceder al servidor HTTP de ClickHouse.
-El valor predeterminado es “Ok.” (con un avance de línea al final)
-
-**Ejemplo**
-
-Abrir `https://tabix.io/` al acceder `http://localhost: http_port`.
-
-``` xml
-<http_server_default_response>
-  <![CDATA[<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>]]>
-</http_server_default_response>
-```
-
-## include_from {#server_configuration_parameters-include_from}
-
-La ruta al archivo con sustituciones.
-
-Para obtener más información, consulte la sección “[Archivos de configuración](../configuration-files.md#configuration_files)”.
-
-**Ejemplo**
-
-``` xml
-<include_from>/etc/metrica.xml</include_from>
-```
-
-## Interesante {#interserver-http-port}
-
-Puerto para el intercambio de datos entre servidores ClickHouse.
-
-**Ejemplo**
-
-``` xml
-<interserver_http_port>9009</interserver_http_port>
-```
-
-## Sistema abierto {#interserver-http-host}
-
-El nombre de host que pueden utilizar otros servidores para acceder a este servidor.
-
-Si se omite, se define de la misma manera que el `hostname-f` comando.
-
-Útil para separarse de una interfaz de red específica.
-
-**Ejemplo**
-
-``` xml
-<interserver_http_host>example.yandex.ru</interserver_http_host>
-```
-
-## interserver_http_credentials {#server-settings-interserver-http-credentials}
-
-El nombre de usuario y la contraseña utilizados para [replicación](../../engines/table-engines/mergetree-family/replication.md) con los motores Replicated\*. Estas credenciales sólo se utilizan para la comunicación entre réplicas y no están relacionadas con las credenciales de los clientes de ClickHouse. El servidor está comprobando estas credenciales para conectar réplicas y utiliza las mismas credenciales cuando se conecta a otras réplicas. Por lo tanto, estas credenciales deben establecerse igual para todas las réplicas de un clúster.
-De forma predeterminada, la autenticación no se utiliza.
-
-Esta sección contiene los siguientes parámetros:
-
--   `user` — username.
--   `password` — password.
-
-**Ejemplo**
-
-``` xml
-<interserver_http_credentials>
-    <user>admin</user>
-    <password>222</password>
-</interserver_http_credentials>
-```
-
-## keep_alive_timeout {#keep-alive-timeout}
-
-El número de segundos que ClickHouse espera las solicitudes entrantes antes de cerrar la conexión. El valor predeterminado es de 3 segundos.
-
-**Ejemplo**
-
-``` xml
-<keep_alive_timeout>3</keep_alive_timeout>
-```
-
-## listen_host {#server_configuration_parameters-listen_host}
-
-Restricción en hosts de los que pueden provenir las solicitudes. Si desea que el servidor responda a todos ellos, especifique `::`.
-
-Ejemplos:
-
-``` xml
-<listen_host>::1</listen_host>
-<listen_host>127.0.0.1</listen_host>
-```
-
-## registrador {#server_configuration_parameters-logger}
-
-Configuración de registro.
-
-Claves:
-
--   level – Logging level. Acceptable values: `trace`, `debug`, `information`, `warning`, `error`.
--   log – The log file. Contains all the entries according to `level`.
--   errorlog – Error log file.
--   size – Size of the file. Applies to `log`y`errorlog`. Una vez que el archivo alcanza `size`, ClickHouse archiva y cambia el nombre, y crea un nuevo archivo de registro en su lugar.
--   count – The number of archived log files that ClickHouse stores.
-
-**Ejemplo**
-
-``` xml
-<logger>
-    <level>trace</level>
-    <log>/var/log/clickhouse-server/clickhouse-server.log</log>
-    <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
-    <size>1000M</size>
-    <count>10</count>
-</logger>
-```
-
-También se admite la escritura en el syslog. Config ejemplo:
-
-``` xml
-<logger>
-    <use_syslog>1</use_syslog>
-    <syslog>
-        <address>syslog.remote:10514</address>
-        <hostname>myhost.local</hostname>
-        <facility>LOG_LOCAL6</facility>
-        <format>syslog</format>
-    </syslog>
-</logger>
-```
-
-Claves:
-
--   use_syslog — Required setting if you want to write to the syslog.
--   address — The host\[:port\] of syslogd. If omitted, the local daemon is used.
--   hostname — Optional. The name of the host that logs are sent from.
--   facility — [La palabra clave syslog facility](https://en.wikipedia.org/wiki/Syslog#Facility) en letras mayúsculas con el “LOG_” prefijo: (`LOG_USER`, `LOG_DAEMON`, `LOG_LOCAL3` y así sucesivamente).
-    Valor predeterminado: `LOG_USER` si `address` se especifica, `LOG_DAEMON otherwise.`
--   format – Message format. Possible values: `bsd` y `syslog.`
-
-## macro {#macros}
-
-Sustituciones de parámetros para tablas replicadas.
-
-Se puede omitir si no se utilizan tablas replicadas.
-
-Para obtener más información, consulte la sección “[Creación de tablas replicadas](../../engines/table-engines/mergetree-family/replication.md)”.
-
-**Ejemplo**
-
-``` xml
-<macros incl="macros" optional="true" />
-```
-
-## Método de codificación de datos: {#server-mark-cache-size}
-
-Tamaño aproximado (en bytes) de la memoria caché de marcas utilizadas por los motores de [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md) familia.
-
-La memoria caché se comparte para el servidor y la memoria se asigna según sea necesario. El tamaño de la memoria caché debe ser al menos 5368709120.
-
-**Ejemplo**
-
-``` xml
-<mark_cache_size>5368709120</mark_cache_size>
-```
-
-## max_concurrent_queries {#max-concurrent-queries}
-
-El número máximo de solicitudes procesadas simultáneamente.
-
-**Ejemplo**
-
-``` xml
-<max_concurrent_queries>100</max_concurrent_queries>
-```
-
-## max_connections {#max-connections}
-
-El número máximo de conexiones entrantes.
-
-**Ejemplo**
-
-``` xml
-<max_connections>4096</max_connections>
-```
-
-## max_open_files {#max-open-files}
-
-El número máximo de archivos abiertos.
-
-Predeterminada: `maximum`.
-
-Recomendamos usar esta opción en Mac OS X desde el `getrlimit()` función devuelve un valor incorrecto.
-
-**Ejemplo**
-
-``` xml
-<max_open_files>262144</max_open_files>
-```
-
-## max_table_size_to_drop {#max-table-size-to-drop}
-
-Restricción en la eliminación de tablas.
-
-Si el tamaño de un [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md) mesa excede `max_table_size_to_drop` (en bytes), no puede eliminarlo usando una consulta DROP.
-
-Si aún necesita eliminar la tabla sin reiniciar el servidor ClickHouse, cree el `<clickhouse-path>/flags/force_drop_table` y ejecute la consulta DROP.
-
-Valor predeterminado: 50 GB.
-
-El valor 0 significa que puede eliminar todas las tablas sin restricciones.
-
-**Ejemplo**
-
-``` xml
-<max_table_size_to_drop>0</max_table_size_to_drop>
-```
-
-## merge_tree {#server_configuration_parameters-merge_tree}
-
-Ajuste fino para tablas en el [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md).
-
-Para obtener más información, vea MergeTreeSettings.h archivo de encabezado.
-
-**Ejemplo**
-
-``` xml
-<merge_tree>
-    <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
-</merge_tree>
-```
-
-## openSSL {#server_configuration_parameters-openssl}
-
-Configuración cliente/servidor SSL.
-
-El soporte para SSL es proporcionado por el `libpoco` biblioteca. La interfaz se describe en el archivo [Nombre de la red inalámbrica (SSID):h](https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h)
-
-Claves para la configuración del servidor/cliente:
-
--   privateKeyFile – The path to the file with the secret key of the PEM certificate. The file may contain a key and certificate at the same time.
--   certificateFile – The path to the client/server certificate file in PEM format. You can omit it if `privateKeyFile` contiene el certificado.
--   caConfig – The path to the file or directory that contains trusted root certificates.
--   verificationMode – The method for checking the node's certificates. Details are in the description of the [Contexto](https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/Context.h) clase. Valores posibles: `none`, `relaxed`, `strict`, `once`.
--   verificationDepth – The maximum length of the verification chain. Verification will fail if the certificate chain length exceeds the set value.
--   loadDefaultCAFile – Indicates that built-in CA certificates for OpenSSL will be used. Acceptable values: `true`, `false`. \|
--   cipherList – Supported OpenSSL encryptions. For example: `ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH`.
--   cacheSessions – Enables or disables caching sessions. Must be used in combination with `sessionIdContext`. Valores aceptables: `true`, `false`.
--   sessionIdContext – A unique set of random characters that the server appends to each generated identifier. The length of the string must not exceed `SSL_MAX_SSL_SESSION_ID_LENGTH`. Este parámetro siempre se recomienda ya que ayuda a evitar problemas tanto si el servidor almacena en caché la sesión como si el cliente solicita el almacenamiento en caché. Valor predeterminado: `${application.name}`.
--   sessionCacheSize – The maximum number of sessions that the server caches. Default value: 1024\*20. 0 – Unlimited sessions.
--   sessionTimeout – Time for caching the session on the server.
--   extendedVerification – Automatically extended verification of certificates after the session ends. Acceptable values: `true`, `false`.
--   requireTLSv1 – Require a TLSv1 connection. Acceptable values: `true`, `false`.
--   requireTLSv1_1 – Require a TLSv1.1 connection. Acceptable values: `true`, `false`.
--   requireTLSv1 – Require a TLSv1.2 connection. Acceptable values: `true`, `false`.
--   fips – Activates OpenSSL FIPS mode. Supported if the library's OpenSSL version supports FIPS.
--   privateKeyPassphraseHandler – Class (PrivateKeyPassphraseHandler subclass) that requests the passphrase for accessing the private key. For example: `<privateKeyPassphraseHandler>`, `<name>KeyFileHandler</name>`, `<options><password>test</password></options>`, `</privateKeyPassphraseHandler>`.
--   invalidCertificateHandler – Class (a subclass of CertificateHandler) for verifying invalid certificates. For example: `<invalidCertificateHandler> <name>ConsoleCertificateHandler</name> </invalidCertificateHandler>` .
--   disableProtocols – Protocols that are not allowed to use.
--   preferServerCiphers – Preferred server ciphers on the client.
-
-**Ejemplo de configuración:**
-
-``` xml
-<openSSL>
-    <server>
-        <!-- openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->
-        <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>
-        <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>
-        <!-- openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096 -->
-        <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>
-        <verificationMode>none</verificationMode>
-        <loadDefaultCAFile>true</loadDefaultCAFile>
-        <cacheSessions>true</cacheSessions>
-        <disableProtocols>sslv2,sslv3</disableProtocols>
-        <preferServerCiphers>true</preferServerCiphers>
-    </server>
-    <client>
-        <loadDefaultCAFile>true</loadDefaultCAFile>
-        <cacheSessions>true</cacheSessions>
-        <disableProtocols>sslv2,sslv3</disableProtocols>
-        <preferServerCiphers>true</preferServerCiphers>
-        <!-- Use for self-signed: <verificationMode>none</verificationMode> -->
-        <invalidCertificateHandler>
-            <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->
-            <name>RejectCertificateHandler</name>
-        </invalidCertificateHandler>
-    </client>
-</openSSL>
-```
-
-## part_log {#server_configuration_parameters-part-log}
-
-Registro de eventos asociados con [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md). Por ejemplo, agregar o fusionar datos. Puede utilizar el registro para simular algoritmos de combinación y comparar sus características. Puede visualizar el proceso de fusión.
-
-Las consultas se registran en el [sistema.part_log](../../operations/system-tables.md#system_tables-part-log) tabla, no en un archivo separado. Puede configurar el nombre de esta tabla en el `table` parámetro (ver más abajo).
-
-Utilice los siguientes parámetros para configurar el registro:
-
--   `database` – Name of the database.
--   `table` – Name of the system table.
--   `partition_by` – Sets a [clave de partición personalizada](../../engines/table-engines/mergetree-family/custom-partitioning-key.md).
--   `flush_interval_milliseconds` – Interval for flushing data from the buffer in memory to the table.
-
-**Ejemplo**
-
-``` xml
-<part_log>
-    <database>system</database>
-    <table>part_log</table>
-    <partition_by>toMonday(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</part_log>
-```
-
-## camino {#server_configuration_parameters-path}
-
-La ruta de acceso al directorio que contiene los datos.
-
-!!! note "Nota"
-    La barra diagonal es obligatoria.
-
-**Ejemplo**
-
-``` xml
-<path>/var/lib/clickhouse/</path>
-```
-
-## prometeo {#server_configuration_parameters-prometheus}
-
-Exponer datos de métricas para raspar desde [Prometeo](https://prometheus.io).
-
-Configuración:
-
--   `endpoint` – HTTP endpoint for scraping metrics by prometheus server. Start from ‘/’.
--   `port` – Port for `endpoint`.
--   `metrics` – Flag that sets to expose metrics from the [sistema.métricas](../system-tables.md#system_tables-metrics) tabla.
--   `events` – Flag that sets to expose metrics from the [sistema.evento](../system-tables.md#system_tables-events) tabla.
--   `asynchronous_metrics` – Flag that sets to expose current metrics values from the [sistema.asynchronous_metrics](../system-tables.md#system_tables-asynchronous_metrics) tabla.
-
-**Ejemplo**
-
-``` xml
- <prometheus>
-        <endpoint>/metrics</endpoint>
-        <port>8001</port>
-        <metrics>true</metrics>
-        <events>true</events>
-        <asynchronous_metrics>true</asynchronous_metrics>
-    </prometheus>
-```
-
-## query_log {#server_configuration_parameters-query-log}
-
-Configuración de las consultas de registro recibidas con [log_queries=1](../settings/settings.md) configuración.
-
-Las consultas se registran en el [sistema.query_log](../../operations/system-tables.md#system_tables-query_log) tabla, no en un archivo separado. Puede cambiar el nombre de la tabla en el `table` parámetro (ver más abajo).
-
-Utilice los siguientes parámetros para configurar el registro:
-
--   `database` – Name of the database.
--   `table` – Name of the system table the queries will be logged in.
--   `partition_by` – Sets a [clave de partición personalizada](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) para una mesa.
--   `flush_interval_milliseconds` – Interval for flushing data from the buffer in memory to the table.
-
-Si la tabla no existe, ClickHouse la creará. Si la estructura del registro de consultas cambió cuando se actualizó el servidor ClickHouse, se cambia el nombre de la tabla con la estructura anterior y se crea una nueva tabla automáticamente.
-
-**Ejemplo**
-
-``` xml
-<query_log>
-    <database>system</database>
-    <table>query_log</table>
-    <partition_by>toMonday(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</query_log>
-```
-
-## Sistema abierto {#server_configuration_parameters-query-thread-log}
-
-Configuración de subprocesos de registro de consultas recibidas con [Log_query_threads = 1](../settings/settings.md#settings-log-query-threads) configuración.
-
-Las consultas se registran en el [sistema.Sistema abierto.](../../operations/system-tables.md#system_tables-query-thread-log) tabla, no en un archivo separado. Puede cambiar el nombre de la tabla en el `table` parámetro (ver más abajo).
-
-Utilice los siguientes parámetros para configurar el registro:
-
--   `database` – Name of the database.
--   `table` – Name of the system table the queries will be logged in.
--   `partition_by` – Sets a [clave de partición personalizada](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) para una tabla del sistema.
--   `flush_interval_milliseconds` – Interval for flushing data from the buffer in memory to the table.
-
-Si la tabla no existe, ClickHouse la creará. Si la estructura del registro de subprocesos de consulta cambió cuando se actualizó el servidor ClickHouse, se cambia el nombre de la tabla con la estructura anterior y se crea una nueva tabla automáticamente.
-
-**Ejemplo**
-
-``` xml
-<query_thread_log>
-    <database>system</database>
-    <table>query_thread_log</table>
-    <partition_by>toMonday(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</query_thread_log>
-```
-
-## trace_log {#server_configuration_parameters-trace_log}
-
-Ajustes para el [trace_log](../../operations/system-tables.md#system_tables-trace_log) operación de la tabla del sistema.
-
-Parámetros:
-
--   `database` — Database for storing a table.
--   `table` — Table name.
--   `partition_by` — [Clave de partición personalizada](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) para una tabla del sistema.
--   `flush_interval_milliseconds` — Interval for flushing data from the buffer in memory to the table.
-
-El archivo de configuración del servidor predeterminado `config.xml` contiene la siguiente sección de configuración:
-
-``` xml
-<trace_log>
-    <database>system</database>
-    <table>trace_log</table>
-    <partition_by>toYYYYMM(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</trace_log>
-```
-
-## query_masking_rules {#query-masking-rules}
-
-Reglas basadas en Regexp, que se aplicarán a las consultas, así como a todos los mensajes de registro antes de almacenarlos en los registros del servidor,
-`system.query_log`, `system.text_log`, `system.processes` tabla, y en los registros enviados al cliente. Eso permite prevenir
-fuga de datos sensible de consultas SQL (como nombres, correos electrónicos,
-identificadores o números de tarjetas de crédito) a los registros.
-
-**Ejemplo**
-
-``` xml
-<query_masking_rules>
-    <rule>
-        <name>hide SSN</name>
-        <regexp>(^|\D)\d{3}-\d{2}-\d{4}($|\D)</regexp>
-        <replace>000-00-0000</replace>
-    </rule>
-</query_masking_rules>
-```
-
-Campos de configuración:
-- `name` - nombre de la regla (opcional)
-- `regexp` - Expresión regular compatible con RE2 (obligatoria)
-- `replace` - cadena de sustitución para datos confidenciales (opcional, por defecto - seis asteriscos)
-
-Las reglas de enmascaramiento se aplican a toda la consulta (para evitar fugas de datos confidenciales de consultas mal formadas / no analizables).
-
-`system.events` la tabla tiene contador `QueryMaskingRulesMatch` que tienen un número total de coincidencias de reglas de enmascaramiento de consultas.
-
-Para consultas distribuidas, cada servidor debe configurarse por separado; de lo contrario, las subconsultas pasan a otros
-los nodos se almacenarán sin enmascarar.
-
-## remote_servers {#server-settings-remote-servers}
-
-Configuración de los clústeres utilizados por [Distribuido](../../engines/table-engines/special/distributed.md) motor de mesa y por el `cluster` función de la tabla.
-
-**Ejemplo**
-
-``` xml
-<remote_servers incl="clickhouse_remote_servers" />
-```
-
-Para el valor de la `incl` atributo, consulte la sección “[Archivos de configuración](../configuration-files.md#configuration_files)”.
-
-**Ver también**
-
--   [skip_unavailable_shards](../settings/settings.md#settings-skip_unavailable_shards)
-
-## Zona horaria {#server_configuration_parameters-timezone}
-
-La zona horaria del servidor.
-
-Especificado como un identificador de la IANA para la zona horaria UTC o la ubicación geográfica (por ejemplo, África/Abidjan).
-
-La zona horaria es necesaria para las conversiones entre los formatos String y DateTime cuando los campos DateTime se envían al formato de texto (impreso en la pantalla o en un archivo) y cuando se obtiene DateTime de una cadena. Además, la zona horaria se usa en funciones que funcionan con la hora y la fecha si no recibieron la zona horaria en los parámetros de entrada.
-
-**Ejemplo**
-
-``` xml
-<timezone>Europe/Moscow</timezone>
-```
-
-## Tcp_port {#server_configuration_parameters-tcp_port}
-
-Puerto para comunicarse con clientes a través del protocolo TCP.
-
-**Ejemplo**
-
-``` xml
-<tcp_port>9000</tcp_port>
-```
-
-## Tcp_port_secure {#server_configuration_parameters-tcp_port_secure}
-
-Puerto TCP para una comunicación segura con los clientes. Úselo con [OpenSSL](#server_configuration_parameters-openssl) configuración.
-
-**Valores posibles**
-
-Entero positivo.
-
-**Valor predeterminado**
-
-``` xml
-<tcp_port_secure>9440</tcp_port_secure>
-```
-
-## mysql_port {#server_configuration_parameters-mysql_port}
-
-Puerto para comunicarse con clientes a través del protocolo MySQL.
-
-**Valores posibles**
-
-Entero positivo.
-
-Ejemplo
-
-``` xml
-<mysql_port>9004</mysql_port>
-```
-
-## tmp_path {#server-settings-tmp_path}
-
-Ruta de acceso a datos temporales para procesar consultas grandes.
-
-!!! note "Nota"
-    La barra diagonal es obligatoria.
-
-**Ejemplo**
-
-``` xml
-<tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
-```
-
-## tmp_policy {#server-settings-tmp-policy}
-
-Política de [`storage_configuration`](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes) para almacenar archivos temporales.
-Si no se establece [`tmp_path`](#server-settings-tmp_path) se utiliza, de lo contrario se ignora.
-
-!!! note "Nota"
-    - `move_factor` se ignora
-- `keep_free_space_bytes` se ignora
-- `max_data_part_size_bytes` se ignora
-- debe tener exactamente un volumen en esa política
-
-## Uncompressed_cache_size {#server-settings-uncompressed_cache_size}
-
-Tamaño de la memoria caché (en bytes) para los datos sin comprimir utilizados por los motores de [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md).
-
-Hay una caché compartida para el servidor. La memoria se asigna a pedido. La caché se usa si la opción [Use_uncompressed_cache](../settings/settings.md#setting-use_uncompressed_cache) está habilitado.
-
-La caché sin comprimir es ventajosa para consultas muy cortas en casos individuales.
-
-**Ejemplo**
-
-``` xml
-<uncompressed_cache_size>8589934592</uncompressed_cache_size>
-```
-
-## user_files_path {#server_configuration_parameters-user_files_path}
-
-El directorio con archivos de usuario. Utilizado en la función de tabla [file()](../../sql-reference/table-functions/file.md).
-
-**Ejemplo**
-
-``` xml
-<user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
-```
-
-## users_config {#users-config}
-
-Ruta de acceso al archivo que contiene:
-
--   Configuraciones de usuario.
--   Derechos de acceso.
--   Perfiles de configuración.
--   Configuración de cuota.
-
-**Ejemplo**
-
-``` xml
-<users_config>users.xml</users_config>
-```
-
-## Zookeeper {#server-settings_zookeeper}
-
-Contiene la configuración que permite a ClickHouse interactuar con [ZooKeeper](http://zookeeper.apache.org/) Cluster.
-
-ClickHouse utiliza ZooKeeper para almacenar metadatos de réplicas cuando se utilizan tablas replicadas. Si no se utilizan tablas replicadas, se puede omitir esta sección de parámetros.
-
-Esta sección contiene los siguientes parámetros:
-
--   `node` — ZooKeeper endpoint. You can set multiple endpoints.
-
-    Por ejemplo:
-
-<!-- -->
-
-``` xml
-    <node index="1">
-        <host>example_host</host>
-        <port>2181</port>
-    </node>
-```
-
-      The `index` attribute specifies the node order when trying to connect to the ZooKeeper cluster.
-
--   `session_timeout` — Maximum timeout for the client session in milliseconds.
--   `root` — The [Znode](http://zookeeper.apache.org/doc/r3.5.5/zookeeperOver.html#Nodes+and+ephemeral+nodes) que se utiliza como la raíz de los znodes utilizados por el servidor ClickHouse. Opcional.
--   `identity` — User and password, that can be required by ZooKeeper to give access to requested znodes. Optional.
-
-**Ejemplo de configuración**
-
-``` xml
-<zookeeper>
-    <node>
-        <host>example1</host>
-        <port>2181</port>
-    </node>
-    <node>
-        <host>example2</host>
-        <port>2181</port>
-    </node>
-    <session_timeout_ms>30000</session_timeout_ms>
-    <operation_timeout_ms>10000</operation_timeout_ms>
-    <!-- Optional. Chroot suffix. Should exist. -->
-    <root>/path/to/zookeeper/node</root>
-    <!-- Optional. Zookeeper digest ACL string. -->
-    <identity>user:password</identity>
-</zookeeper>
-```
-
-**Ver también**
-
--   [Replicación](../../engines/table-engines/mergetree-family/replication.md)
--   [Guía del programador ZooKeeper](http://zookeeper.apache.org/doc/current/zookeeperProgrammers.html)
-
-## use_minimalistic_part_header_in_zookeeper {#server-settings-use_minimalistic_part_header_in_zookeeper}
-
-Método de almacenamiento para encabezados de parte de datos en ZooKeeper.
-
-Esta configuración sólo se aplica a `MergeTree` familia. Se puede especificar:
-
--   A nivel mundial en el [merge_tree](#server_configuration_parameters-merge_tree) sección de la `config.xml` file.
-
-    ClickHouse utiliza la configuración para todas las tablas del servidor. Puede cambiar la configuración en cualquier momento. Las tablas existentes cambian su comportamiento cuando cambia la configuración.
-
--   Para cada tabla.
-
-    Al crear una tabla, especifique la correspondiente [ajuste del motor](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table). El comportamiento de una tabla existente con esta configuración no cambia, incluso si la configuración global cambia.
-
-**Valores posibles**
-
--   0 — Functionality is turned off.
--   1 — Functionality is turned on.
-
-Si `use_minimalistic_part_header_in_zookeeper = 1`, entonces [repetición](../../engines/table-engines/mergetree-family/replication.md) las tablas almacenan los encabezados de las partes de datos de forma compacta `znode`. Si la tabla contiene muchas columnas, este método de almacenamiento reduce significativamente el volumen de los datos almacenados en Zookeeper.
-
-!!! attention "Atención"
-    Después de aplicar `use_minimalistic_part_header_in_zookeeper = 1`, no puede degradar el servidor ClickHouse a una versión que no admite esta configuración. Tenga cuidado al actualizar ClickHouse en servidores de un clúster. No actualice todos los servidores a la vez. Es más seguro probar nuevas versiones de ClickHouse en un entorno de prueba o solo en unos pocos servidores de un clúster.
-
-      Data part headers already stored with this setting can't be restored to their previous (non-compact) representation.
-
-**Valor predeterminado:** 0.
-
-## disable_internal_dns_cache {#server-settings-disable-internal-dns-cache}
-
-Deshabilita la memoria caché DNS interna. Recomendado para operar ClickHouse en sistemas
-con infraestructura que cambia frecuentemente como Kubernetes.
-
-**Valor predeterminado:** 0.
-
-## dns_cache_update_period {#server-settings-dns-cache-update-period}
-
-El período de actualización de las direcciones IP almacenadas en la caché DNS interna de ClickHouse (en segundos).
-La actualización se realiza de forma asíncrona, en un subproceso del sistema separado.
-
-**Valor predeterminado**: 15.
-
-## access_control_path {#access_control_path}
-
-Ruta de acceso a una carpeta donde un servidor ClickHouse almacena configuraciones de usuario y rol creadas por comandos SQL.
-
-Valor predeterminado: `/var/lib/clickhouse/access/`.
-
-**Ver también**
-
--   [Control de acceso y gestión de cuentas](../access-rights.md#access-control)
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/server_configuration_parameters/settings/) <!--hide-->
diff --git a/docs/es/operations/settings/constraints-on-settings.md b/docs/es/operations/settings/constraints-on-settings.md
deleted file mode 100644
index fe385f6ddbba..000000000000
--- a/docs/es/operations/settings/constraints-on-settings.md
+++ /dev/null
@@ -1,75 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 62
-toc_title: "Restricciones en la configuraci\xF3n"
----
-
-# Restricciones en la configuración {#constraints-on-settings}
-
-Las restricciones en los ajustes se pueden definir en el `profiles` sección de la `user.xml` el archivo de configuración y prohíba a los usuarios cambiar algunos de los ajustes `SET` consulta.
-Las restricciones se definen como las siguientes:
-
-``` xml
-<profiles>
-  <user_name>
-    <constraints>
-      <setting_name_1>
-        <min>lower_boundary</min>
-      </setting_name_1>
-      <setting_name_2>
-        <max>upper_boundary</max>
-      </setting_name_2>
-      <setting_name_3>
-        <min>lower_boundary</min>
-        <max>upper_boundary</max>
-      </setting_name_3>
-      <setting_name_4>
-        <readonly/>
-      </setting_name_4>
-    </constraints>
-  </user_name>
-</profiles>
-```
-
-Si el usuario intenta violar las restricciones, se lanza una excepción y la configuración no se cambia.
-Se admiten tres tipos de restricciones: `min`, `max`, `readonly`. El `min` y `max` Las restricciones especifican los límites superior e inferior para una configuración numérica y se pueden usar en combinación. El `readonly` constraint especifica que el usuario no puede cambiar la configuración correspondiente en absoluto.
-
-**Ejemplo:** Dejar `users.xml` incluye líneas:
-
-``` xml
-<profiles>
-  <default>
-    <max_memory_usage>10000000000</max_memory_usage>
-    <force_index_by_date>0</force_index_by_date>
-    ...
-    <constraints>
-      <max_memory_usage>
-        <min>5000000000</min>
-        <max>20000000000</max>
-      </max_memory_usage>
-      <force_index_by_date>
-        <readonly/>
-      </force_index_by_date>
-    </constraints>
-  </default>
-</profiles>
-```
-
-Las siguientes consultas arrojan excepciones:
-
-``` sql
-SET max_memory_usage=20000000001;
-SET max_memory_usage=4999999999;
-SET force_index_by_date=1;
-```
-
-``` text
-Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be greater than 20000000000.
-Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be less than 5000000000.
-Code: 452, e.displayText() = DB::Exception: Setting force_index_by_date should not be changed.
-```
-
-**Nota:** el `default` perfil tiene un manejo especial: todas las restricciones definidas para el `default` profile se convierten en las restricciones predeterminadas, por lo que restringen a todos los usuarios hasta que se anulan explícitamente para estos usuarios.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/settings/constraints_on_settings/) <!--hide-->
diff --git a/docs/es/operations/settings/index.md b/docs/es/operations/settings/index.md
deleted file mode 100644
index 37aab0a7e1bc..000000000000
--- a/docs/es/operations/settings/index.md
+++ /dev/null
@@ -1,33 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Configuraci\xF3n"
-toc_priority: 55
-toc_title: "Implantaci\xF3n"
----
-
-# Configuración {#session-settings-intro}
-
-Hay varias maneras de realizar todos los ajustes descritos en esta sección de documentación.
-
-Los ajustes se configuran en capas, por lo que cada capa subsiguiente redefine los ajustes anteriores.
-
-Formas de configurar los ajustes, por orden de prioridad:
-
--   Ajustes en el `users.xml` archivo de configuración del servidor.
-
-    Establecer en el elemento `<profiles>`.
-
--   Configuración de la sesión.
-
-    Enviar `SET setting=value` desde el cliente de consola ClickHouse en modo interactivo.
-    Del mismo modo, puede utilizar sesiones ClickHouse en el protocolo HTTP. Para hacer esto, debe especificar el `session_id` Parámetro HTTP.
-
--   Configuración de consulta.
-
-    -   Al iniciar el cliente de consola de ClickHouse en modo no interactivo, establezca el parámetro de inicio `--setting=value`.
-    -   Al usar la API HTTP, pase los parámetros CGI (`URL?setting_1=value&setting_2=value...`).
-
-Los ajustes que solo se pueden realizar en el archivo de configuración del servidor no se tratan en esta sección.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/settings/) <!--hide-->
diff --git a/docs/es/operations/settings/permissions-for-queries.md b/docs/es/operations/settings/permissions-for-queries.md
deleted file mode 100644
index f9f669b876eb..000000000000
--- a/docs/es/operations/settings/permissions-for-queries.md
+++ /dev/null
@@ -1,61 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: Permisos para consultas
----
-
-# Permisos para consultas {#permissions_for_queries}
-
-Las consultas en ClickHouse se pueden dividir en varios tipos:
-
-1.  Leer consultas de datos: `SELECT`, `SHOW`, `DESCRIBE`, `EXISTS`.
-2.  Escribir consultas de datos: `INSERT`, `OPTIMIZE`.
-3.  Cambiar la consulta de configuración: `SET`, `USE`.
-4.  [DDL](https://en.wikipedia.org/wiki/Data_definition_language) consulta: `CREATE`, `ALTER`, `RENAME`, `ATTACH`, `DETACH`, `DROP` `TRUNCATE`.
-5.  `KILL QUERY`.
-
-La siguiente configuración regula los permisos de usuario según el tipo de consulta:
-
--   [sólo lectura](#settings_readonly) — Restricts permissions for all types of queries except DDL queries.
--   [Método de codificación de datos:](#settings_allow_ddl) — Restricts permissions for DDL queries.
-
-`KILL QUERY` se puede realizar con cualquier configuración.
-
-## sólo lectura {#settings_readonly}
-
-Restringe los permisos para leer datos, escribir datos y cambiar las consultas de configuración.
-
-Vea cómo las consultas se dividen en tipos [arriba](#permissions_for_queries).
-
-Valores posibles:
-
--   0 — All queries are allowed.
--   1 — Only read data queries are allowed.
--   2 — Read data and change settings queries are allowed.
-
-Después de configurar `readonly = 1` el usuario no puede cambiar `readonly` y `allow_ddl` configuración en la sesión actual.
-
-Cuando se utiliza el `GET` método en el [Interfaz HTTP](../../interfaces/http.md), `readonly = 1` se establece automáticamente. Para modificar los datos, `POST` método.
-
-Configuración `readonly = 1` prohibir al usuario cambiar todas las configuraciones. Hay una manera de prohibir al usuario
-de cambiar sólo ajustes específicos, para más detalles ver [restricciones en la configuración](constraints-on-settings.md).
-
-Valor predeterminado: 0
-
-## Método de codificación de datos: {#settings_allow_ddl}
-
-Permite o niega [DDL](https://en.wikipedia.org/wiki/Data_definition_language) consulta.
-
-Vea cómo las consultas se dividen en tipos [arriba](#permissions_for_queries).
-
-Valores posibles:
-
--   0 — DDL queries are not allowed.
--   1 — DDL queries are allowed.
-
-No se puede ejecutar `SET allow_ddl = 1` si `allow_ddl = 0` para la sesión actual.
-
-Valor predeterminado: 1
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/settings/permissions_for_queries/) <!--hide-->
diff --git a/docs/es/operations/settings/query-complexity.md b/docs/es/operations/settings/query-complexity.md
deleted file mode 100644
index 82bc235c30db..000000000000
--- a/docs/es/operations/settings/query-complexity.md
+++ /dev/null
@@ -1,300 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: Restricciones en la complejidad de consultas
----
-
-# Restricciones en la complejidad de consultas {#restrictions-on-query-complexity}
-
-Las restricciones en la complejidad de la consulta forman parte de la configuración.
-Se utilizan para proporcionar una ejecución más segura desde la interfaz de usuario.
-Casi todas las restricciones solo se aplican a `SELECT`. Para el procesamiento de consultas distribuidas, las restricciones se aplican en cada servidor por separado.
-
-ClickHouse comprueba las restricciones para las partes de datos, no para cada fila. Significa que puede exceder el valor de restricción con el tamaño de la parte de datos.
-
-Restricciones en el “maximum amount of something” puede tomar el valor 0, lo que significa “unrestricted”.
-La mayoría de las restricciones también tienen un ‘overflow_mode’ establecer, lo que significa qué hacer cuando se excede el límite.
-Puede tomar uno de dos valores: `throw` o `break`. Las restricciones en la agregación (group_by_overflow_mode) también tienen el valor `any`.
-
-`throw` – Throw an exception (default).
-
-`break` – Stop executing the query and return the partial result, as if the source data ran out.
-
-`any (only for group_by_overflow_mode)` – Continuing aggregation for the keys that got into the set, but don't add new keys to the set.
-
-## Método de codificación de datos: {#settings_max_memory_usage}
-
-La cantidad máxima de RAM que se utiliza para ejecutar una consulta en un único servidor.
-
-En el archivo de configuración predeterminado, el máximo es de 10 GB.
-
-La configuración no tiene en cuenta el volumen de memoria disponible ni el volumen total de memoria en la máquina.
-La restricción se aplica a una sola consulta dentro de un único servidor.
-Usted puede utilizar `SHOW PROCESSLIST` para ver el consumo de memoria actual para cada consulta.
-Además, el consumo máximo de memoria se rastrea para cada consulta y se escribe en el registro.
-
-El uso de memoria no se supervisa para los estados de ciertas funciones agregadas.
-
-El uso de memoria no se realiza un seguimiento completo de los estados de las funciones agregadas `min`, `max`, `any`, `anyLast`, `argMin`, `argMax` de `String` y `Array` argumento.
-
-El consumo de memoria también está restringido por los parámetros `max_memory_usage_for_user` y `max_memory_usage_for_all_queries`.
-
-## Max_memory_usage_for_user {#max-memory-usage-for-user}
-
-La cantidad máxima de RAM que se utilizará para ejecutar las consultas de un usuario en un único servidor.
-
-Los valores predeterminados se definen en [Configuración.h](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.h#L288). De forma predeterminada, el importe no está restringido (`max_memory_usage_for_user = 0`).
-
-Ver también la descripción de [Método de codificación de datos:](#settings_max_memory_usage).
-
-## Todos los derechos reservados {#max-memory-usage-for-all-queries}
-
-La cantidad máxima de RAM que se utilizará para ejecutar todas las consultas en un único servidor.
-
-Los valores predeterminados se definen en [Configuración.h](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.h#L289). De forma predeterminada, el importe no está restringido (`max_memory_usage_for_all_queries = 0`).
-
-Ver también la descripción de [Método de codificación de datos:](#settings_max_memory_usage).
-
-## ¿Qué puedes encontrar en Neodigit {#max-rows-to-read}
-
-Las siguientes restricciones se pueden verificar en cada bloque (en lugar de en cada fila). Es decir, las restricciones se pueden romper un poco.
-
-Un número máximo de filas que se pueden leer de una tabla al ejecutar una consulta.
-
-## ¿Qué puedes encontrar en Neodigit {#max-bytes-to-read}
-
-Un número máximo de bytes (datos sin comprimir) que se pueden leer de una tabla al ejecutar una consulta.
-
-## Método de codificación de datos: {#read-overflow-mode}
-
-Qué hacer cuando el volumen de datos leídos excede uno de los límites: ‘throw’ o ‘break’. Por defecto, throw.
-
-## Método de codificación de datos: {#settings-max-rows-to-group-by}
-
-Un número máximo de claves únicas recibidas de la agregación. Esta configuración le permite limitar el consumo de memoria al agregar.
-
-## Grupo_by_overflow_mode {#group-by-overflow-mode}
-
-Qué hacer cuando el número de claves únicas para la agregación excede el límite: ‘throw’, ‘break’, o ‘any’. Por defecto, throw.
-Uso de la ‘any’ valor le permite ejecutar una aproximación de GROUP BY. La calidad de esta aproximación depende de la naturaleza estadística de los datos.
-
-## max_bytes_before_external_group_by {#settings-max_bytes_before_external_group_by}
-
-Habilita o deshabilita la ejecución de `GROUP BY` en la memoria externa. Ver [GROUP BY en memoria externa](../../sql-reference/statements/select/group-by.md#select-group-by-in-external-memory).
-
-Valores posibles:
-
--   Volumen máximo de RAM (en bytes) que puede ser utilizado por el único [GROUP BY](../../sql-reference/statements/select/group-by.md#select-group-by-clause) operación.
--   0 — `GROUP BY` en la memoria externa deshabilitada.
-
-Valor predeterminado: 0.
-
-## Método de codificación de datos: {#max-rows-to-sort}
-
-Un número máximo de filas antes de ordenar. Esto le permite limitar el consumo de memoria al ordenar.
-
-## Método de codificación de datos: {#max-bytes-to-sort}
-
-Un número máximo de bytes antes de ordenar.
-
-## sort_overflow_mode {#sort-overflow-mode}
-
-Qué hacer si el número de filas recibidas antes de ordenar excede uno de los límites: ‘throw’ o ‘break’. Por defecto, throw.
-
-## max_result_rows {#setting-max_result_rows}
-
-Límite en el número de filas en el resultado. También se comprueba si hay subconsultas y en servidores remotos cuando se ejecutan partes de una consulta distribuida.
-
-## max_result_bytes {#max-result-bytes}
-
-Límite en el número de bytes en el resultado. Lo mismo que el ajuste anterior.
-
-## result_overflow_mode {#result-overflow-mode}
-
-Qué hacer si el volumen del resultado excede uno de los límites: ‘throw’ o ‘break’. Por defecto, throw.
-
-Utilizar ‘break’ es similar a usar LIMIT. `Break` interrumpe la ejecución sólo en el nivel de bloque. Esto significa que la cantidad de filas devueltas es mayor que [max_result_rows](#setting-max_result_rows), múltiplo de [max_block_size](settings.md#setting-max_block_size) y depende de [max_threads](settings.md#settings-max_threads).
-
-Ejemplo:
-
-``` sql
-SET max_threads = 3, max_block_size = 3333;
-SET max_result_rows = 3334, result_overflow_mode = 'break';
-
-SELECT *
-FROM numbers_mt(100000)
-FORMAT Null;
-```
-
-Resultado:
-
-``` text
-6666 rows in set. ...
-```
-
-## max_execution_time {#max-execution-time}
-
-Tiempo máximo de ejecución de la consulta en segundos.
-En este momento, no se comprueba una de las etapas de clasificación, o al fusionar y finalizar funciones agregadas.
-
-## timeout_overflow_mode {#timeout-overflow-mode}
-
-Qué hacer si la consulta se ejecuta más de ‘max_execution_time’: ‘throw’ o ‘break’. Por defecto, throw.
-
-## Método de codificación de datos: {#min-execution-speed}
-
-Velocidad de ejecución mínima en filas por segundo. Comprobado en cada bloque de datos cuando ‘timeout_before_checking_execution_speed’ expirar. Si la velocidad de ejecución es menor, se produce una excepción.
-
-## Todos los derechos reservados {#min-execution-speed-bytes}
-
-Un número mínimo de bytes de ejecución por segundo. Comprobado en cada bloque de datos cuando ‘timeout_before_checking_execution_speed’ expirar. Si la velocidad de ejecución es menor, se produce una excepción.
-
-## Max_execution_speed {#max-execution-speed}
-
-Un número máximo de filas de ejecución por segundo. Comprobado en cada bloque de datos cuando ‘timeout_before_checking_execution_speed’ expirar. Si la velocidad de ejecución es alta, la velocidad de ejecución se reducirá.
-
-## Max_execution_speed_bytes {#max-execution-speed-bytes}
-
-Un número máximo de bytes de ejecución por segundo. Comprobado en cada bloque de datos cuando ‘timeout_before_checking_execution_speed’ expirar. Si la velocidad de ejecución es alta, la velocidad de ejecución se reducirá.
-
-## Tiempo de espera antes de comprobar_ejecución_velocidad {#timeout-before-checking-execution-speed}
-
-Comprueba que la velocidad de ejecución no sea demasiado lenta (no menos de ‘min_execution_speed’), después de que el tiempo especificado en segundos haya expirado.
-
-## Max_columns_to_read {#max-columns-to-read}
-
-Un número máximo de columnas que se pueden leer de una tabla en una sola consulta. Si una consulta requiere leer un mayor número de columnas, produce una excepción.
-
-## max_temporary_columns {#max-temporary-columns}
-
-Un número máximo de columnas temporales que se deben mantener en la memoria RAM al mismo tiempo cuando se ejecuta una consulta, incluidas las columnas constantes. Si hay más columnas temporales que esto, arroja una excepción.
-
-## max_temporary_non_const_columns {#max-temporary-non-const-columns}
-
-Lo mismo que ‘max_temporary_columns’, pero sin contar columnas constantes.
-Tenga en cuenta que las columnas constantes se forman con bastante frecuencia cuando se ejecuta una consulta, pero requieren aproximadamente cero recursos informáticos.
-
-## max_subquery_depth {#max-subquery-depth}
-
-Profundidad máxima de anidamiento de subconsultas. Si las subconsultas son más profundas, se produce una excepción. De forma predeterminada, 100.
-
-## max_pipeline_depth {#max-pipeline-depth}
-
-Profundidad máxima de la tubería. Corresponde al número de transformaciones que realiza cada bloque de datos durante el procesamiento de consultas. Contado dentro de los límites de un único servidor. Si la profundidad de la canalización es mayor, se produce una excepción. Por defecto, 1000.
-
-## max_ast_depth {#max-ast-depth}
-
-Profundidad máxima de anidamiento de un árbol sintáctico de consulta. Si se supera, se produce una excepción.
-En este momento, no se verifica durante el análisis, sino solo después de analizar la consulta. Es decir, se puede crear un árbol sintáctico demasiado profundo durante el análisis, pero la consulta fallará. Por defecto, 1000.
-
-## max_ast_elements {#max-ast-elements}
-
-Un número máximo de elementos en un árbol sintáctico de consulta. Si se supera, se produce una excepción.
-De la misma manera que la configuración anterior, se verifica solo después de analizar la consulta. De forma predeterminada, 50.000.
-
-## Método de codificación de datos: {#max-rows-in-set}
-
-Un número máximo de filas para un conjunto de datos en la cláusula IN creada a partir de una subconsulta.
-
-## Método de codificación de datos: {#max-bytes-in-set}
-
-Número máximo de bytes (datos sin comprimir) utilizados por un conjunto en la cláusula IN creada a partir de una subconsulta.
-
-## set_overflow_mode {#set-overflow-mode}
-
-Qué hacer cuando la cantidad de datos excede uno de los límites: ‘throw’ o ‘break’. Por defecto, throw.
-
-## Método de codificación de datos: {#max-rows-in-distinct}
-
-Un número máximo de filas diferentes al usar DISTINCT.
-
-## Método de codificación de datos: {#max-bytes-in-distinct}
-
-Un número máximo de bytes utilizados por una tabla hash cuando se utiliza DISTINCT.
-
-## distinct_overflow_mode {#distinct-overflow-mode}
-
-Qué hacer cuando la cantidad de datos excede uno de los límites: ‘throw’ o ‘break’. Por defecto, throw.
-
-## max_rows_to_transfer {#max-rows-to-transfer}
-
-Un número máximo de filas que se pueden pasar a un servidor remoto o guardar en una tabla temporal cuando se utiliza GLOBAL IN.
-
-## max_bytes_to_transfer {#max-bytes-to-transfer}
-
-Un número máximo de bytes (datos sin comprimir) que se pueden pasar a un servidor remoto o guardar en una tabla temporal cuando se utiliza GLOBAL IN.
-
-## transfer_overflow_mode {#transfer-overflow-mode}
-
-Qué hacer cuando la cantidad de datos excede uno de los límites: ‘throw’ o ‘break’. Por defecto, throw.
-
-## Método de codificación de datos: {#settings-max_rows_in_join}
-
-Limita el número de filas de la tabla hash que se utiliza al unir tablas.
-
-Esta configuración se aplica a [SELECT … JOIN](../../sql-reference/statements/select/join.md#select-join) operaciones y la [Unir](../../engines/table-engines/special/join.md) motor de mesa.
-
-Si una consulta contiene varias combinaciones, ClickHouse comprueba esta configuración para cada resultado intermedio.
-
-ClickHouse puede proceder con diferentes acciones cuando se alcanza el límite. Utilice el [join_overflow_mode](#settings-join_overflow_mode) configuración para elegir la acción.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Unlimited number of rows.
-
-Valor predeterminado: 0.
-
-## Método de codificación de datos: {#settings-max_bytes_in_join}
-
-Limita el tamaño en bytes de la tabla hash utilizada al unir tablas.
-
-Esta configuración se aplica a [SELECT … JOIN](../../sql-reference/statements/select/join.md#select-join) operaciones y [Unirse al motor de tabla](../../engines/table-engines/special/join.md).
-
-Si la consulta contiene combinaciones, ClickHouse comprueba esta configuración para cada resultado intermedio.
-
-ClickHouse puede proceder con diferentes acciones cuando se alcanza el límite. Utilizar [join_overflow_mode](#settings-join_overflow_mode) para elegir la acción.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Memory control is disabled.
-
-Valor predeterminado: 0.
-
-## join_overflow_mode {#settings-join_overflow_mode}
-
-Define qué acción realiza ClickHouse cuando se alcanza cualquiera de los siguientes límites de combinación:
-
--   [Método de codificación de datos:](#settings-max_bytes_in_join)
--   [Método de codificación de datos:](#settings-max_rows_in_join)
-
-Valores posibles:
-
--   `THROW` — ClickHouse throws an exception and breaks operation.
--   `BREAK` — ClickHouse breaks operation and doesn't throw an exception.
-
-Valor predeterminado: `THROW`.
-
-**Ver también**
-
--   [Cláusula JOIN](../../sql-reference/statements/select/join.md#select-join)
--   [Unirse al motor de tabla](../../engines/table-engines/special/join.md)
-
-## max_partitions_per_insert_block {#max-partitions-per-insert-block}
-
-Limita el número máximo de particiones en un único bloque insertado.
-
--   Entero positivo.
--   0 — Unlimited number of partitions.
-
-Valor predeterminado: 100.
-
-**Detalles**
-
-Al insertar datos, ClickHouse calcula el número de particiones en el bloque insertado. Si el número de particiones es mayor que `max_partitions_per_insert_block`, ClickHouse lanza una excepción con el siguiente texto:
-
-> “Too many partitions for single INSERT block (more than” ¿Cómo puedo hacerlo? “). The limit is controlled by ‘max_partitions_per_insert_block’ setting. A large number of partitions is a common misconception. It will lead to severe negative performance impact, including slow server startup, slow INSERT queries and slow SELECT queries. Recommended total number of partitions for a table is under 1000..10000. Please note, that partitioning is not intended to speed up SELECT queries (ORDER BY key is sufficient to make range queries fast). Partitions are intended for data manipulation (DROP PARTITION, etc).”
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/settings/query_complexity/) <!--hide-->
diff --git a/docs/es/operations/settings/settings-profiles.md b/docs/es/operations/settings/settings-profiles.md
deleted file mode 100644
index 3d96a2c8fba1..000000000000
--- a/docs/es/operations/settings/settings-profiles.md
+++ /dev/null
@@ -1,81 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: "Perfiles de configuraci\xF3n"
----
-
-# Perfiles de configuración {#settings-profiles}
-
-Un perfil de configuración es una colección de configuraciones agrupadas con el mismo nombre.
-
-!!! note "Información"
-    ClickHouse también es compatible [Flujo de trabajo controlado por SQL](../access-rights.md#access-control) para administrar perfiles de configuración. Recomendamos usarlo.
-
-Un perfil puede tener cualquier nombre. El perfil puede tener cualquier nombre. Puede especificar el mismo perfil para diferentes usuarios. Lo más importante que puede escribir en el perfil de configuración es `readonly=1`, que asegura el acceso de sólo lectura.
-
-Los perfiles de configuración pueden heredar unos de otros. Para usar la herencia, indique una o varias `profile` configuraciones antes de las demás configuraciones que se enumeran en el perfil. En caso de que se defina una configuración en diferentes perfiles, se utiliza la última definida.
-
-Para aplicar todos los ajustes de un perfil, establezca el `profile` configuración.
-
-Ejemplo:
-
-Instale el `web` perfil.
-
-``` sql
-SET profile = 'web'
-```
-
-Los perfiles de configuración se declaran en el archivo de configuración del usuario. Esto suele ser `users.xml`.
-
-Ejemplo:
-
-``` xml
-<!-- Settings profiles -->
-<profiles>
-    <!-- Default settings -->
-    <default>
-        <!-- The maximum number of threads when running a single query. -->
-        <max_threads>8</max_threads>
-    </default>
-
-    <!-- Settings for quries from the user interface -->
-    <web>
-        <max_rows_to_read>1000000000</max_rows_to_read>
-        <max_bytes_to_read>100000000000</max_bytes_to_read>
-
-        <max_rows_to_group_by>1000000</max_rows_to_group_by>
-        <group_by_overflow_mode>any</group_by_overflow_mode>
-
-        <max_rows_to_sort>1000000</max_rows_to_sort>
-        <max_bytes_to_sort>1000000000</max_bytes_to_sort>
-
-        <max_result_rows>100000</max_result_rows>
-        <max_result_bytes>100000000</max_result_bytes>
-        <result_overflow_mode>break</result_overflow_mode>
-
-        <max_execution_time>600</max_execution_time>
-        <min_execution_speed>1000000</min_execution_speed>
-        <timeout_before_checking_execution_speed>15</timeout_before_checking_execution_speed>
-
-        <max_columns_to_read>25</max_columns_to_read>
-        <max_temporary_columns>100</max_temporary_columns>
-        <max_temporary_non_const_columns>50</max_temporary_non_const_columns>
-
-        <max_subquery_depth>2</max_subquery_depth>
-        <max_pipeline_depth>25</max_pipeline_depth>
-        <max_ast_depth>50</max_ast_depth>
-        <max_ast_elements>100</max_ast_elements>
-
-        <readonly>1</readonly>
-    </web>
-</profiles>
-```
-
-El ejemplo especifica dos perfiles: `default` y `web`.
-
-El `default` tiene un propósito especial: siempre debe estar presente y se aplica al iniciar el servidor. En otras palabras, el `default` perfil contiene la configuración predeterminada.
-
-El `web` profile es un perfil regular que se puede establecer utilizando el `SET` consulta o utilizando un parámetro URL en una consulta HTTP.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/settings/settings_profiles/) <!--hide-->
diff --git a/docs/es/operations/settings/settings-users.md b/docs/es/operations/settings/settings-users.md
deleted file mode 100644
index 1c1ac7914f06..000000000000
--- a/docs/es/operations/settings/settings-users.md
+++ /dev/null
@@ -1,164 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 63
-toc_title: "Configuraci\xF3n del usuario"
----
-
-# Configuración del usuario {#user-settings}
-
-El `users` sección de la `user.xml` el archivo de configuración contiene la configuración del usuario.
-
-!!! note "Información"
-    ClickHouse también es compatible [Flujo de trabajo controlado por SQL](../access-rights.md#access-control) para la gestión de usuarios. Recomendamos usarlo.
-
-Estructura del `users` apartado:
-
-``` xml
-<users>
-    <!-- If user name was not specified, 'default' user is used. -->
-    <user_name>
-        <password></password>
-        <!-- Or -->
-        <password_sha256_hex></password_sha256_hex>
-
-        <access_management>0|1</access_management>
-
-        <networks incl="networks" replace="replace">
-        </networks>
-
-        <profile>profile_name</profile>
-
-        <quota>default</quota>
-
-        <databases>
-            <database_name>
-                <table_name>
-                    <filter>expression</filter>
-                <table_name>
-            </database_name>
-        </databases>
-    </user_name>
-    <!-- Other users settings -->
-</users>
-```
-
-### user_name/contraseña {#user-namepassword}
-
-La contraseña se puede especificar en texto sin formato o en SHA256 (formato hexagonal).
-
--   Para asignar una contraseña en texto sin formato (**no se recomienda**), colóquelo en un `password` elemento.
-
-    Por ejemplo, `<password>qwerty</password>`. La contraseña se puede dejar en blanco.
-
-<a id="password_sha256_hex"></a>
-
--   Para asignar una contraseña utilizando su hash SHA256, colóquela en un `password_sha256_hex` elemento.
-
-    Por ejemplo, `<password_sha256_hex>65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5</password_sha256_hex>`.
-
-    Ejemplo de cómo generar una contraseña desde el shell:
-
-          PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'
-
-    La primera línea del resultado es la contraseña. La segunda línea es el hash SHA256 correspondiente.
-
-<a id="password_double_sha1_hex"></a>
-
--   Para la compatibilidad con los clientes MySQL, la contraseña se puede especificar en doble hash SHA1. Colóquelo en `password_double_sha1_hex` elemento.
-
-    Por ejemplo, `<password_double_sha1_hex>08b4a0f1de6ad37da17359e592c8d74788a83eb0</password_double_sha1_hex>`.
-
-    Ejemplo de cómo generar una contraseña desde el shell:
-
-          PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-'
-
-    La primera línea del resultado es la contraseña. La segunda línea es el hash SHA1 doble correspondiente.
-
-### access_management {#access_management-user-setting}
-
-Esta configuración habilita deshabilita el uso de [control de acceso y gestión de cuentas](../access-rights.md#access-control) para el usuario.
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 0.
-
-### user_name/redes {#user-namenetworks}
-
-Lista de redes desde las que el usuario puede conectarse al servidor ClickHouse.
-
-Cada elemento de la lista puede tener una de las siguientes formas:
-
--   `<ip>` — IP address or network mask.
-
-    Ejemplos: `213.180.204.3`, `10.0.0.1/8`, `10.0.0.1/255.255.255.0`, `2a02:6b8::3`, `2a02:6b8::3/64`, `2a02:6b8::3/ffff:ffff:ffff:ffff::`.
-
--   `<host>` — Hostname.
-
-    Ejemplo: `example01.host.ru`.
-
-    Para comprobar el acceso, se realiza una consulta DNS y todas las direcciones IP devueltas se comparan con la dirección del mismo nivel.
-
--   `<host_regexp>` — Regular expression for hostnames.
-
-    Ejemplo, `^example\d\d-\d\d-\d\.host\.ru$`
-
-    Para comprobar el acceso, un [Consulta de DNS PTR](https://en.wikipedia.org/wiki/Reverse_DNS_lookup) se realiza para la dirección del mismo nivel y luego se aplica la expresión regular especificada. A continuación, se realiza otra consulta DNS para los resultados de la consulta PTR y todas las direcciones recibidas se comparan con la dirección del mismo nivel. Recomendamos encarecidamente que regexp termine con $ .
-
-Todos los resultados de las solicitudes DNS se almacenan en caché hasta que el servidor se reinicia.
-
-**Ejemplos**
-
-Para abrir el acceso del usuario desde cualquier red, especifique:
-
-``` xml
-<ip>::/0</ip>
-```
-
-!!! warning "Advertencia"
-    No es seguro abrir el acceso desde cualquier red a menos que tenga un firewall configurado correctamente o el servidor no esté conectado directamente a Internet.
-
-Para abrir el acceso solo desde localhost, especifique:
-
-``` xml
-<ip>::1</ip>
-<ip>127.0.0.1</ip>
-```
-
-### user_name/perfil {#user-nameprofile}
-
-Puede asignar un perfil de configuración para el usuario. Los perfiles de configuración se configuran en una sección separada del `users.xml` file. Para obtener más información, consulte [Perfiles de configuración](settings-profiles.md).
-
-### user_name/cuota {#user-namequota}
-
-Las cuotas le permiten realizar un seguimiento o limitar el uso de recursos durante un período de tiempo. Las cuotas se configuran en el `quotas`
-sección de la `users.xml` archivo de configuración.
-
-Puede asignar un conjunto de cuotas para el usuario. Para obtener una descripción detallada de la configuración de las cuotas, consulte [Cuota](../quotas.md#quotas).
-
-### nombre_usuario/bases de datos {#user-namedatabases}
-
-En esta sección, puede limitar las filas devueltas por ClickHouse para `SELECT` consultas realizadas por el usuario actual, implementando así la seguridad básica a nivel de fila.
-
-**Ejemplo**
-
-La siguiente configuración obliga a que el usuario `user1` sólo puede ver las filas de `table1` como resultado de `SELECT` consultas, donde el valor de la `id` campo es 1000.
-
-``` xml
-<user1>
-    <databases>
-        <database_name>
-            <table1>
-                <filter>id = 1000</filter>
-            </table1>
-        </database_name>
-    </databases>
-</user1>
-```
-
-El `filter` puede ser cualquier expresión que resulte en un [UInt8](../../sql-reference/data-types/int-uint.md)-tipo de valor. Por lo general, contiene comparaciones y operadores lógicos. Filas de `database_name.table1` donde los resultados del filtro a 0 no se devuelven para este usuario. El filtrado es incompatible con `PREWHERE` operaciones y desactiva `WHERE→PREWHERE` optimización.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/settings/settings_users/) <!--hide-->
diff --git a/docs/es/operations/settings/settings.md b/docs/es/operations/settings/settings.md
deleted file mode 100644
index 62511dd9fc00..000000000000
--- a/docs/es/operations/settings/settings.md
+++ /dev/null
@@ -1,1254 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Configuración {#settings}
-
-## distributed_product_mode {#distributed-product-mode}
-
-Cambia el comportamiento de [subconsultas distribuidas](../../sql-reference/operators/in.md).
-
-ClickHouse applies this setting when the query contains the product of distributed tables, i.e. when the query for a distributed table contains a non-GLOBAL subquery for the distributed table.
-
-Restricción:
-
--   Solo se aplica para las subconsultas IN y JOIN.
--   Solo si la sección FROM utiliza una tabla distribuida que contiene más de un fragmento.
--   Si la subconsulta se refiere a una tabla distribuida que contiene más de un fragmento.
--   No se usa para un valor de tabla [remoto](../../sql-reference/table-functions/remote.md) función.
-
-Valores posibles:
-
--   `deny` — Default value. Prohibits using these types of subqueries (returns the “Double-distributed in/JOIN subqueries is denied” salvedad).
--   `local` — Replaces the database and table in the subquery with local ones for the destination server (shard), leaving the normal `IN`/`JOIN.`
--   `global` — Replaces the `IN`/`JOIN` consulta con `GLOBAL IN`/`GLOBAL JOIN.`
--   `allow` — Allows the use of these types of subqueries.
-
-## enable_optimize_predicate_expression {#enable-optimize-predicate-expression}
-
-Activa el pushdown de predicado en `SELECT` consulta.
-
-La extracción de predicados puede reducir significativamente el tráfico de red para consultas distribuidas.
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 1.
-
-Uso
-
-Considere las siguientes consultas:
-
-1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
-2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`
-
-Si `enable_optimize_predicate_expression = 1`, entonces el tiempo de ejecución de estas consultas es igual porque se aplica ClickHouse `WHERE` a la subconsulta al procesarla.
-
-Si `enable_optimize_predicate_expression = 0`, entonces el tiempo de ejecución de la segunda consulta es mucho más largo, porque el `WHERE` cláusula se aplica a todos los datos después de que finalice la subconsulta.
-
-## fallback_to_stale_replicas_for_distributed_queries {#settings-fallback_to_stale_replicas_for_distributed_queries}
-
-Fuerza una consulta a una réplica obsoleta si los datos actualizados no están disponibles. Ver [Replicación](../../engines/table-engines/mergetree-family/replication.md).
-
-ClickHouse selecciona la más relevante de las réplicas obsoletas de la tabla.
-
-Se utiliza al realizar `SELECT` desde una tabla distribuida que apunta a tablas replicadas.
-
-De forma predeterminada, 1 (habilitado).
-
-## Fecha de nacimiento {#settings-force_index_by_date}
-
-Deshabilita la ejecución de consultas si el índice no se puede usar por fecha.
-
-Funciona con tablas de la familia MergeTree.
-
-Si `force_index_by_date=1`, ClickHouse comprueba si la consulta tiene una condición de clave de fecha que se puede usar para restringir intervalos de datos. Si no hay una condición adecuada, arroja una excepción. Sin embargo, no comprueba si la condición reduce la cantidad de datos a leer. Por ejemplo, la condición `Date != ' 2000-01-01 '` es aceptable incluso cuando coincide con todos los datos de la tabla (es decir, ejecutar la consulta requiere un escaneo completo). Para obtener más información acerca de los intervalos de datos en las tablas MergeTree, vea [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md).
-
-## force_primary_key {#force-primary-key}
-
-Deshabilita la ejecución de consultas si no es posible la indexación mediante la clave principal.
-
-Funciona con tablas de la familia MergeTree.
-
-Si `force_primary_key=1`, ClickHouse comprueba si la consulta tiene una condición de clave principal que se puede usar para restringir rangos de datos. Si no hay una condición adecuada, arroja una excepción. Sin embargo, no comprueba si la condición reduce la cantidad de datos a leer. Para obtener más información acerca de los intervalos de datos en las tablas MergeTree, consulte [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md).
-
-## Formato_esquema {#format-schema}
-
-Este parámetro es útil cuando se utilizan formatos que requieren una definición de esquema, como [Cap'n Proto](https://capnproto.org/) o [Protobuf](https://developers.google.com/protocol-buffers/). El valor depende del formato.
-
-## fsync_metadata {#fsync-metadata}
-
-Habilita o deshabilita [fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html) al escribir `.sql` file. Habilitado de forma predeterminada.
-
-Tiene sentido desactivarlo si el servidor tiene millones de pequeñas tablas que se crean y destruyen constantemente.
-
-## enable_http_compression {#settings-enable_http_compression}
-
-Habilita o deshabilita la compresión de datos en la respuesta a una solicitud HTTP.
-
-Para obtener más información, lea el [Descripción de la interfaz HTTP](../../interfaces/http.md).
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 0.
-
-## http_zlib_compression_level {#settings-http_zlib_compression_level}
-
-Establece el nivel de compresión de datos en la respuesta a una solicitud HTTP si [enable_http_compression = 1](#settings-enable_http_compression).
-
-Valores posibles: Números del 1 al 9.
-
-Valor predeterminado: 3.
-
-## http_native_compression_disable_checksumming_on_decompress {#settings-http_native_compression_disable_checksumming_on_decompress}
-
-Habilita o deshabilita la verificación de suma de comprobación al descomprimir los datos HTTP POST del cliente. Se usa solo para el formato de compresión nativa ClickHouse (no se usa con `gzip` o `deflate`).
-
-Para obtener más información, lea el [Descripción de la interfaz HTTP](../../interfaces/http.md).
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 0.
-
-## send_progress_in_http_headers {#settings-send_progress_in_http_headers}
-
-Habilita o deshabilita `X-ClickHouse-Progress` Encabezados de respuesta HTTP en `clickhouse-server` respuesta.
-
-Para obtener más información, lea el [Descripción de la interfaz HTTP](../../interfaces/http.md).
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 0.
-
-## Nombre de la red inalámbrica (SSID): {#setting-max_http_get_redirects}
-
-Limita el número máximo de saltos de redirección HTTP GET para [URL](../../engines/table-engines/special/url.md)-mesas de motor. La configuración se aplica a ambos tipos de tablas: las creadas por [CREATE TABLE](../../sql-reference/statements/create.md#create-table-query) consulta y por el [URL](../../sql-reference/table-functions/url.md) función de la tabla.
-
-Valores posibles:
-
--   Cualquier número entero positivo de saltos.
--   0 — No hops allowed.
-
-Valor predeterminado: 0.
-
-## Entrada_format_allow_errors_num {#settings-input_format_allow_errors_num}
-
-Establece el número máximo de errores aceptables al leer desde formatos de texto (CSV, TSV, etc.).
-
-El valor predeterminado es 0.
-
-Siempre emparejarlo con `input_format_allow_errors_ratio`.
-
-Si se produjo un error al leer filas, pero el contador de errores sigue siendo menor que `input_format_allow_errors_num`, ClickHouse ignora la fila y pasa a la siguiente.
-
-Si ambos `input_format_allow_errors_num` y `input_format_allow_errors_ratio` se exceden, ClickHouse lanza una excepción.
-
-## Entrada_format_allow_errors_ratio {#settings-input_format_allow_errors_ratio}
-
-Establece el porcentaje máximo de errores permitidos al leer desde formatos de texto (CSV, TSV, etc.).
-El porcentaje de errores se establece como un número de punto flotante entre 0 y 1.
-
-El valor predeterminado es 0.
-
-Siempre emparejarlo con `input_format_allow_errors_num`.
-
-Si se produjo un error al leer filas, pero el contador de errores sigue siendo menor que `input_format_allow_errors_ratio`, ClickHouse ignora la fila y pasa a la siguiente.
-
-Si ambos `input_format_allow_errors_num` y `input_format_allow_errors_ratio` se exceden, ClickHouse lanza una excepción.
-
-## input_format_values_interpret_expressions {#settings-input_format_values_interpret_expressions}
-
-Habilita o deshabilita el analizador SQL completo si el analizador de secuencias rápidas no puede analizar los datos. Esta configuración sólo se utiliza para [Valor](../../interfaces/formats.md#data-format-values) formato en la inserción de datos. Para obtener más información sobre el análisis de sintaxis, consulte [Sintaxis](../../sql-reference/syntax.md) apartado.
-
-Valores posibles:
-
--   0 — Disabled.
-
-    En este caso, debe proporcionar datos con formato. Ver el [Formato](../../interfaces/formats.md) apartado.
-
--   1 — Enabled.
-
-    En este caso, puede usar una expresión SQL como valor, pero la inserción de datos es mucho más lenta de esta manera. Si inserta solo datos con formato, ClickHouse se comporta como si el valor de configuración fuera 0.
-
-Valor predeterminado: 1.
-
-Ejemplo de uso
-
-Inserte el [FechaHora](../../sql-reference/data-types/datetime.md) valor de tipo con los diferentes ajustes.
-
-``` sql
-SET input_format_values_interpret_expressions = 0;
-INSERT INTO datetime_t VALUES (now())
-```
-
-``` text
-Exception on client:
-Code: 27. DB::Exception: Cannot parse input: expected ) before: now()): (at row 1)
-```
-
-``` sql
-SET input_format_values_interpret_expressions = 1;
-INSERT INTO datetime_t VALUES (now())
-```
-
-``` text
-Ok.
-```
-
-La última consulta es equivalente a la siguiente:
-
-``` sql
-SET input_format_values_interpret_expressions = 0;
-INSERT INTO datetime_t SELECT now()
-```
-
-``` text
-Ok.
-```
-
-## input_format_values_deduce_templates_of_expressions {#settings-input_format_values_deduce_templates_of_expressions}
-
-Habilita o deshabilita la deducción de plantilla para expresiones SQL en [Valor](../../interfaces/formats.md#data-format-values) formato. Permite analizar e interpretar expresiones en `Values` mucho más rápido si las expresiones en filas consecutivas tienen la misma estructura. ClickHouse intenta deducir la plantilla de una expresión, analizar las siguientes filas utilizando esta plantilla y evaluar la expresión en un lote de filas analizadas correctamente.
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 1.
-
-Para la siguiente consulta:
-
-``` sql
-INSERT INTO test VALUES (lower('Hello')), (lower('world')), (lower('INSERT')), (upper('Values')), ...
-```
-
--   Si `input_format_values_interpret_expressions=1` y `format_values_deduce_templates_of_expressions=0`, las expresiones se interpretan por separado para cada fila (esto es muy lento para un gran número de filas).
--   Si `input_format_values_interpret_expressions=0` y `format_values_deduce_templates_of_expressions=1`, las expresiones en la primera, segunda y tercera filas se analizan usando la plantilla `lower(String)` e interpretados juntos, la expresión en la cuarta fila se analiza con otra plantilla (`upper(String)`).
--   Si `input_format_values_interpret_expressions=1` y `format_values_deduce_templates_of_expressions=1`, lo mismo que en el caso anterior, pero también permite la alternativa a la interpretación de expresiones por separado si no es posible deducir la plantilla.
-
-## Entrada_format_values_accurate_types_of_literals {#settings-input-format-values-accurate-types-of-literals}
-
-Esta configuración sólo se utiliza cuando `input_format_values_deduce_templates_of_expressions = 1`. Puede suceder que las expresiones para alguna columna tengan la misma estructura, pero contengan literales numéricos de diferentes tipos, por ejemplo
-
-``` sql
-(..., abs(0), ...),             -- UInt64 literal
-(..., abs(3.141592654), ...),   -- Float64 literal
-(..., abs(-1), ...),            -- Int64 literal
-```
-
-Valores posibles:
-
--   0 — Disabled.
-
-    In this case, ClickHouse may use a more general type for some literals (e.g., `Float64` o `Int64` en lugar de `UInt64` para `42`), pero puede causar problemas de desbordamiento y precisión.
-
--   1 — Enabled.
-
-    En este caso, ClickHouse comprueba el tipo real de literal y utiliza una plantilla de expresión del tipo correspondiente. En algunos casos, puede ralentizar significativamente la evaluación de expresiones en `Values`.
-
-Valor predeterminado: 1.
-
-## Entrada_format_defaults_for_omitted_fields {#session_settings-input_format_defaults_for_omitted_fields}
-
-Al realizar `INSERT` consultas, reemplace los valores de columna de entrada omitidos con valores predeterminados de las columnas respectivas. Esta opción sólo se aplica a [JSONEachRow](../../interfaces/formats.md#jsoneachrow), [CSV](../../interfaces/formats.md#csv) y [TabSeparated](../../interfaces/formats.md#tabseparated) formato.
-
-!!! note "Nota"
-    Cuando esta opción está habilitada, los metadatos de la tabla extendida se envían del servidor al cliente. Consume recursos informáticos adicionales en el servidor y puede reducir el rendimiento.
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 1.
-
-## input_format_tsv_empty_as_default {#settings-input-format-tsv-empty-as-default}
-
-Cuando esté habilitado, reemplace los campos de entrada vacíos en TSV con valores predeterminados. Para expresiones predeterminadas complejas `input_format_defaults_for_omitted_fields` debe estar habilitado también.
-
-Deshabilitado de forma predeterminada.
-
-## input_format_null_as_default {#settings-input-format-null-as-default}
-
-Habilita o deshabilita el uso de valores predeterminados si los datos de entrada `NULL`, pero el tipo de datos de la columna correspondiente en no `Nullable(T)` (para formatos de entrada de texto).
-
-## input_format_skip_unknown_fields {#settings-input-format-skip-unknown-fields}
-
-Habilita o deshabilita omitir la inserción de datos adicionales.
-
-Al escribir datos, ClickHouse produce una excepción si los datos de entrada contienen columnas que no existen en la tabla de destino. Si la omisión está habilitada, ClickHouse no inserta datos adicionales y no lanza una excepción.
-
-Formatos soportados:
-
--   [JSONEachRow](../../interfaces/formats.md#jsoneachrow)
--   [CSVWithNames](../../interfaces/formats.md#csvwithnames)
--   [TabSeparatedWithNames](../../interfaces/formats.md#tabseparatedwithnames)
--   [TSKV](../../interfaces/formats.md#tskv)
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 0.
-
-## Entrada_format_import_nested_json {#settings-input_format_import_nested_json}
-
-Habilita o deshabilita la inserción de datos JSON con objetos anidados.
-
-Formatos soportados:
-
--   [JSONEachRow](../../interfaces/formats.md#jsoneachrow)
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 0.
-
-Ver también:
-
--   [Uso de estructuras anidadas](../../interfaces/formats.md#jsoneachrow-nested) con el `JSONEachRow` formato.
-
-## Entrada_format_with_names_use_header {#settings-input-format-with-names-use-header}
-
-Habilita o deshabilita la comprobación del orden de las columnas al insertar datos.
-
-Para mejorar el rendimiento de la inserción, se recomienda deshabilitar esta comprobación si está seguro de que el orden de columna de los datos de entrada es el mismo que en la tabla de destino.
-
-Formatos soportados:
-
--   [CSVWithNames](../../interfaces/formats.md#csvwithnames)
--   [TabSeparatedWithNames](../../interfaces/formats.md#tabseparatedwithnames)
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 1.
-
-## Date_time_input_format {#settings-date_time_input_format}
-
-Permite elegir un analizador de la representación de texto de fecha y hora.
-
-La configuración no se aplica a [Funciones de fecha y hora](../../sql-reference/functions/date-time-functions.md).
-
-Valores posibles:
-
--   `'best_effort'` — Enables extended parsing.
-
-    ClickHouse puede analizar el básico `YYYY-MM-DD HH:MM:SS` formato y todo [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) formatos de fecha y hora. Por ejemplo, `'2018-06-08T01:02:03.000Z'`.
-
--   `'basic'` — Use basic parser.
-
-    ClickHouse puede analizar solo lo básico `YYYY-MM-DD HH:MM:SS` formato. Por ejemplo, `'2019-08-20 10:18:56'`.
-
-Valor predeterminado: `'basic'`.
-
-Ver también:
-
--   [Tipo de datos DateTime.](../../sql-reference/data-types/datetime.md)
--   [Funciones para trabajar con fechas y horas.](../../sql-reference/functions/date-time-functions.md)
-
-## Por favor, introduzca su dirección de correo electrónico {#settings-join_default_strictness}
-
-Establece el rigor predeterminado para [Cláusulas JOIN](../../sql-reference/statements/select/join.md#select-join).
-
-Valores posibles:
-
--   `ALL` — If the right table has several matching rows, ClickHouse creates a [Producto cartesiano](https://en.wikipedia.org/wiki/Cartesian_product) de filas coincidentes. Esta es la normal `JOIN` comportamiento de SQL estándar.
--   `ANY` — If the right table has several matching rows, only the first one found is joined. If the right table has only one matching row, the results of `ANY` y `ALL` son los mismos.
--   `ASOF` — For joining sequences with an uncertain match.
--   `Empty string` — If `ALL` o `ANY` no se especifica en la consulta, ClickHouse produce una excepción.
-
-Valor predeterminado: `ALL`.
-
-## join_any_take_last_row {#settings-join_any_take_last_row}
-
-Cambia el comportamiento de las operaciones de unión con `ANY` rigor.
-
-!!! warning "Atención"
-    Esta configuración sólo se aplica a `JOIN` operaciones con [Unir](../../engines/table-engines/special/join.md) mesas de motores.
-
-Valores posibles:
-
--   0 — If the right table has more than one matching row, only the first one found is joined.
--   1 — If the right table has more than one matching row, only the last one found is joined.
-
-Valor predeterminado: 0.
-
-Ver también:
-
--   [Cláusula JOIN](../../sql-reference/statements/select/join.md#select-join)
--   [Unirse al motor de tabla](../../engines/table-engines/special/join.md)
--   [Por favor, introduzca su dirección de correo electrónico](#settings-join_default_strictness)
-
-## Sistema abierto {#join_use_nulls}
-
-Establece el tipo de [JOIN](../../sql-reference/statements/select/join.md) comportamiento. Al fusionar tablas, pueden aparecer celdas vacías. ClickHouse los rellena de manera diferente según esta configuración.
-
-Valores posibles:
-
--   0 — The empty cells are filled with the default value of the corresponding field type.
--   1 — `JOIN` se comporta de la misma manera que en SQL estándar. El tipo del campo correspondiente se convierte en [NULL](../../sql-reference/data-types/nullable.md#data_type-nullable), y las celdas vacías se llenan con [NULL](../../sql-reference/syntax.md).
-
-Valor predeterminado: 0.
-
-## max_block_size {#setting-max_block_size}
-
-En ClickHouse, los datos se procesan mediante bloques (conjuntos de partes de columna). Los ciclos de procesamiento interno para un solo bloque son lo suficientemente eficientes, pero hay gastos notables en cada bloque. El `max_block_size` set es una recomendación para el tamaño del bloque (en un recuento de filas) para cargar desde las tablas. El tamaño del bloque no debe ser demasiado pequeño, por lo que los gastos en cada bloque aún se notan, pero no demasiado grande para que la consulta con LIMIT que se complete después del primer bloque se procese rápidamente. El objetivo es evitar consumir demasiada memoria al extraer un gran número de columnas en múltiples subprocesos y preservar al menos alguna localidad de caché.
-
-Valor predeterminado: 65,536.
-
-Bloquea el tamaño de `max_block_size` no siempre se cargan desde la tabla. Si es obvio que se deben recuperar menos datos, se procesa un bloque más pequeño.
-
-## preferred_block_size_bytes {#preferred-block-size-bytes}
-
-Utilizado para el mismo propósito que `max_block_size`, pero establece el tamaño de bloque recomendado en bytes adaptándolo al número de filas en el bloque.
-Sin embargo, el tamaño del bloque no puede ser más que `max_block_size` filas.
-Por defecto: 1,000,000. Solo funciona cuando se lee desde los motores MergeTree.
-
-## merge_tree_min_rows_for_concurrent_read {#setting-merge-tree-min-rows-for-concurrent-read}
-
-Si el número de filas que se leerán de un fichero [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md) mesa excede `merge_tree_min_rows_for_concurrent_read` luego ClickHouse intenta realizar una lectura simultánea de este archivo en varios hilos.
-
-Valores posibles:
-
--   Cualquier entero positivo.
-
-Valor predeterminado: 163840.
-
-## merge_tree_min_bytes_for_concurrent_read {#setting-merge-tree-min-bytes-for-concurrent-read}
-
-Si el número de bytes a leer de un archivo de un [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md)-La tabla del motor excede `merge_tree_min_bytes_for_concurrent_read`, entonces ClickHouse intenta leer simultáneamente este archivo en varios subprocesos.
-
-Valor posible:
-
--   Cualquier entero positivo.
-
-Valor predeterminado: 251658240.
-
-## Método de codificación de datos: {#setting-merge-tree-min-rows-for-seek}
-
-Si la distancia entre dos bloques de datos que se leen en un archivo es menor que `merge_tree_min_rows_for_seek` filas, luego ClickHouse no busca a través del archivo, sino que lee los datos secuencialmente.
-
-Valores posibles:
-
--   Cualquier entero positivo.
-
-Valor predeterminado: 0.
-
-## merge_tree_min_bytes_for_seek {#setting-merge-tree-min-bytes-for-seek}
-
-Si la distancia entre dos bloques de datos que se leen en un archivo es menor que `merge_tree_min_bytes_for_seek` bytes, luego ClickHouse lee secuencialmente un rango de archivos que contiene ambos bloques, evitando así la búsqueda adicional.
-
-Valores posibles:
-
--   Cualquier entero positivo.
-
-Valor predeterminado: 0.
-
-## merge_tree_coarse_index_granularity {#setting-merge-tree-coarse-index-granularity}
-
-Al buscar datos, ClickHouse comprueba las marcas de datos en el archivo de índice. Si ClickHouse encuentra que las claves requeridas están en algún rango, divide este rango en `merge_tree_coarse_index_granularity` subintervalos y busca las claves necesarias allí de forma recursiva.
-
-Valores posibles:
-
--   Cualquier entero incluso positivo.
-
-Valor predeterminado: 8.
-
-## merge_tree_max_rows_to_use_cache {#setting-merge-tree-max-rows-to-use-cache}
-
-Si ClickHouse debería leer más de `merge_tree_max_rows_to_use_cache` en una consulta, no usa la memoria caché de bloques sin comprimir.
-
-La memoria caché de bloques sin comprimir almacena datos extraídos para consultas. ClickHouse utiliza esta memoria caché para acelerar las respuestas a pequeñas consultas repetidas. Esta configuración protege la memoria caché del deterioro de las consultas que leen una gran cantidad de datos. El [Uncompressed_cache_size](../server-configuration-parameters/settings.md#server-settings-uncompressed_cache_size) configuración del servidor define el tamaño de la memoria caché de bloques sin comprimir.
-
-Valores posibles:
-
--   Cualquier entero positivo.
-
-Default value: 128 ✕ 8192.
-
-## merge_tree_max_bytes_to_use_cache {#setting-merge-tree-max-bytes-to-use-cache}
-
-Si ClickHouse debería leer más de `merge_tree_max_bytes_to_use_cache` bytes en una consulta, no usa el caché de bloques sin comprimir.
-
-La memoria caché de bloques sin comprimir almacena datos extraídos para consultas. ClickHouse utiliza esta memoria caché para acelerar las respuestas a pequeñas consultas repetidas. Esta configuración protege la memoria caché del deterioro de las consultas que leen una gran cantidad de datos. El [Uncompressed_cache_size](../server-configuration-parameters/settings.md#server-settings-uncompressed_cache_size) configuración del servidor define el tamaño de la memoria caché de bloques sin comprimir.
-
-Valor posible:
-
--   Cualquier entero positivo.
-
-Valor predeterminado: 2013265920.
-
-## Todos los derechos reservados {#settings-min-bytes-to-use-direct-io}
-
-El volumen de datos mínimo necesario para utilizar el acceso directo de E/S al disco de almacenamiento.
-
-ClickHouse usa esta configuración al leer datos de tablas. Si el volumen total de almacenamiento de todos los datos a leer excede `min_bytes_to_use_direct_io` luego ClickHouse lee los datos del disco de almacenamiento con el `O_DIRECT` opcion.
-
-Valores posibles:
-
--   0 — Direct I/O is disabled.
--   Entero positivo.
-
-Valor predeterminado: 0.
-
-## Log_queries {#settings-log-queries}
-
-Configuración del registro de consultas.
-
-Las consultas enviadas a ClickHouse con esta configuración se registran de acuerdo con las reglas [query_log](../server-configuration-parameters/settings.md#server_configuration_parameters-query-log) parámetro de configuración del servidor.
-
-Ejemplo:
-
-``` text
-log_queries=1
-```
-
-## Nombre de la red inalámbrica (SSID): {#settings-log-queries-min-type}
-
-`query_log` tipo mínimo para iniciar sesión.
-
-Valores posibles:
-- `QUERY_START` (`=1`)
-- `QUERY_FINISH` (`=2`)
-- `EXCEPTION_BEFORE_START` (`=3`)
-- `EXCEPTION_WHILE_PROCESSING` (`=4`)
-
-Valor predeterminado: `QUERY_START`.
-
-Se puede usar para limitar a qué entiries va `query_log`, digamos que eres interesante solo en errores, entonces puedes usar `EXCEPTION_WHILE_PROCESSING`:
-
-``` text
-log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
-```
-
-## Log_query_threads {#settings-log-query-threads}
-
-Configuración del registro de subprocesos de consulta.
-
-Los subprocesos de consultas ejecutados por ClickHouse con esta configuración se registran de acuerdo con las reglas en el [Sistema abierto.](../server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) parámetro de configuración del servidor.
-
-Ejemplo:
-
-``` text
-log_query_threads=1
-```
-
-## Max_insert_block_size {#settings-max_insert_block_size}
-
-El tamaño de los bloques a formar para su inserción en una tabla.
-Esta configuración solo se aplica en los casos en que el servidor forma los bloques.
-Por ejemplo, para un INSERT a través de la interfaz HTTP, el servidor analiza el formato de datos y forma bloques del tamaño especificado.
-Pero al usar clickhouse-client, el cliente analiza los datos en sí, y el ‘max_insert_block_size’ configuración en el servidor no afecta el tamaño de los bloques insertados.
-La configuración tampoco tiene un propósito cuando se usa INSERT SELECT , ya que los datos se insertan usando los mismos bloques que se forman después de SELECT .
-
-Valor predeterminado: 1.048.576.
-
-El valor predeterminado es ligeramente más que `max_block_size`. La razón de esto se debe a que ciertos motores de mesa (`*MergeTree`) formar una parte de datos en el disco para cada bloque insertado, que es una entidad bastante grande. Similar, `*MergeTree` las tablas ordenan los datos durante la inserción y un tamaño de bloque lo suficientemente grande permiten clasificar más datos en la RAM.
-
-## Nombre de la red inalámbrica (SSID): {#min-insert-block-size-rows}
-
-Establece el número mínimo de filas en el bloque que se pueden insertar en una tabla `INSERT` consulta. Los bloques de menor tamaño se aplastan en otros más grandes.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Squashing disabled.
-
-Valor predeterminado: 1048576.
-
-## Todos los derechos reservados {#min-insert-block-size-bytes}
-
-Establece el número mínimo de bytes en el bloque que se pueden insertar en una tabla `INSERT` consulta. Los bloques de menor tamaño se aplastan en otros más grandes.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Squashing disabled.
-
-Valor predeterminado: 268435456.
-
-## max_replica_delay_for_distributed_queries {#settings-max_replica_delay_for_distributed_queries}
-
-Deshabilita las réplicas rezagadas para consultas distribuidas. Ver [Replicación](../../engines/table-engines/mergetree-family/replication.md).
-
-Establece el tiempo en segundos. Si una réplica tiene un retraso superior al valor establecido, no se utiliza esta réplica.
-
-Valor predeterminado: 300.
-
-Se utiliza al realizar `SELECT` desde una tabla distribuida que apunta a tablas replicadas.
-
-## max_threads {#settings-max_threads}
-
-El número máximo de subprocesos de procesamiento de consultas, excluyendo subprocesos para recuperar datos de servidores ‘max_distributed_connections’ parámetro).
-
-Este parámetro se aplica a los subprocesos que realizan las mismas etapas de la canalización de procesamiento de consultas en paralelo.
-Por ejemplo, al leer desde una tabla, si es posible evaluar expresiones con funciones, filtre con WHERE y preagregue para GROUP BY en paralelo usando al menos ‘max_threads’ número de hilos, entonces ‘max_threads’ se utilizan.
-
-Valor predeterminado: el número de núcleos de CPU físicos.
-
-Si normalmente se ejecuta menos de una consulta SELECT en un servidor a la vez, establezca este parámetro en un valor ligeramente inferior al número real de núcleos de procesador.
-
-Para las consultas que se completan rápidamente debido a un LIMIT, puede establecer un ‘max_threads’. Por ejemplo, si el número necesario de entradas se encuentra en cada bloque y max_threads = 8, entonces se recuperan 8 bloques, aunque hubiera sido suficiente leer solo uno.
-
-Cuanto menor sea el `max_threads` valor, menos memoria se consume.
-
-## Método de codificación de datos: {#settings-max-insert-threads}
-
-El número máximo de subprocesos para ejecutar el `INSERT SELECT` consulta.
-
-Valores posibles:
-
--   0 (or 1) — `INSERT SELECT` sin ejecución paralela.
--   Entero positivo. Más grande que 1.
-
-Valor predeterminado: 0.
-
-Paralelo `INSERT SELECT` sólo tiene efecto si el `SELECT` parte se ejecuta en paralelo, ver [max_threads](#settings-max_threads) configuración.
-Los valores más altos conducirán a un mayor uso de memoria.
-
-## max_compress_block_size {#max-compress-block-size}
-
-El tamaño máximo de bloques de datos sin comprimir antes de comprimir para escribir en una tabla. De forma predeterminada, 1.048.576 (1 MiB). Si se reduce el tamaño, la tasa de compresión se reduce significativamente, la velocidad de compresión y descompresión aumenta ligeramente debido a la localidad de la memoria caché, y se reduce el consumo de memoria. Por lo general, no hay ninguna razón para cambiar esta configuración.
-
-No confunda bloques para la compresión (un fragmento de memoria que consta de bytes) con bloques para el procesamiento de consultas (un conjunto de filas de una tabla).
-
-## Descripción del producto {#min-compress-block-size}
-
-Para [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md)" tabla. Para reducir la latencia al procesar consultas, un bloque se comprime al escribir la siguiente marca si su tamaño es al menos ‘min_compress_block_size’. De forma predeterminada, 65.536.
-
-El tamaño real del bloque, si los datos sin comprimir son menores que ‘max_compress_block_size’, no es menor que este valor y no menor que el volumen de datos para una marca.
-
-Veamos un ejemplo. Supongamos que ‘index_granularity’ se estableció en 8192 durante la creación de la tabla.
-
-Estamos escribiendo una columna de tipo UInt32 (4 bytes por valor). Al escribir 8192 filas, el total será de 32 KB de datos. Como min_compress_block_size = 65,536, se formará un bloque comprimido por cada dos marcas.
-
-Estamos escribiendo una columna URL con el tipo String (tamaño promedio de 60 bytes por valor). Al escribir 8192 filas, el promedio será ligeramente inferior a 500 KB de datos. Como esto es más de 65,536, se formará un bloque comprimido para cada marca. En este caso, al leer datos del disco en el rango de una sola marca, los datos adicionales no se descomprimirán.
-
-Por lo general, no hay ninguna razón para cambiar esta configuración.
-
-## max_query_size {#settings-max_query_size}
-
-La parte máxima de una consulta que se puede llevar a la RAM para analizar con el analizador SQL.
-La consulta INSERT también contiene datos para INSERT que es procesado por un analizador de secuencias independiente (que consume O(1) RAM), que no está incluido en esta restricción.
-
-Valor predeterminado: 256 KiB.
-
-## interactive_delay {#interactive-delay}
-
-El intervalo en microsegundos para comprobar si la ejecución de la solicitud se ha cancelado y enviar el progreso.
-
-Valor predeterminado: 100.000 (comprueba la cancelación y envía el progreso diez veces por segundo).
-
-## ¿Cómo puedo hacerlo? {#connect-timeout-receive-timeout-send-timeout}
-
-Tiempos de espera en segundos en el socket utilizado para comunicarse con el cliente.
-
-Valor predeterminado: 10, 300, 300.
-
-## Cancel_http_readonly_queries_on_client_close {#cancel-http-readonly-queries-on-client-close}
-
-Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response.
-
-Valor predeterminado: 0
-
-## poll_interval {#poll-interval}
-
-Bloquear en un bucle de espera durante el número especificado de segundos.
-
-Valor predeterminado: 10.
-
-## max_distributed_connections {#max-distributed-connections}
-
-El número máximo de conexiones simultáneas con servidores remotos para el procesamiento distribuido de una única consulta a una única tabla distribuida. Se recomienda establecer un valor no menor que el número de servidores en el clúster.
-
-Valor predeterminado: 1024.
-
-Los siguientes parámetros solo se usan al crear tablas distribuidas (y al iniciar un servidor), por lo que no hay ninguna razón para cambiarlas en tiempo de ejecución.
-
-## Distributed_connections_pool_size {#distributed-connections-pool-size}
-
-El número máximo de conexiones simultáneas con servidores remotos para el procesamiento distribuido de todas las consultas a una única tabla distribuida. Se recomienda establecer un valor no menor que el número de servidores en el clúster.
-
-Valor predeterminado: 1024.
-
-## Conecte_timeout_with_failover_ms {#connect-timeout-with-failover-ms}
-
-El tiempo de espera en milisegundos para conectarse a un servidor remoto para un motor de tablas distribuidas ‘shard’ y ‘replica’ secciones se utilizan en la definición de clúster.
-Si no tiene éxito, se realizan varios intentos para conectarse a varias réplicas.
-
-Valor predeterminado: 50.
-
-## connections_with_failover_max_tries {#connections-with-failover-max-tries}
-
-El número máximo de intentos de conexión con cada réplica para el motor de tablas distribuidas.
-
-Valor predeterminado: 3.
-
-## extremo {#extremes}
-
-Ya sea para contar valores extremos (los mínimos y máximos en columnas de un resultado de consulta). Acepta 0 o 1. De forma predeterminada, 0 (deshabilitado).
-Para obtener más información, consulte la sección “Extreme values”.
-
-## Use_uncompressed_cache {#setting-use_uncompressed_cache}
-
-Si se debe usar una memoria caché de bloques sin comprimir. Acepta 0 o 1. De forma predeterminada, 0 (deshabilitado).
-El uso de la memoria caché sin comprimir (solo para tablas de la familia MergeTree) puede reducir significativamente la latencia y aumentar el rendimiento cuando se trabaja con un gran número de consultas cortas. Habilite esta configuración para los usuarios que envían solicitudes cortas frecuentes. También preste atención al [Uncompressed_cache_size](../server-configuration-parameters/settings.md#server-settings-uncompressed_cache_size) configuration parameter (only set in the config file) – the size of uncompressed cache blocks. By default, it is 8 GiB. The uncompressed cache is filled in as needed and the least-used data is automatically deleted.
-
-Para consultas que leen al menos un volumen algo grande de datos (un millón de filas o más), la memoria caché sin comprimir se desactiva automáticamente para ahorrar espacio para consultas realmente pequeñas. Esto significa que puede mantener el ‘use_uncompressed_cache’ ajuste siempre establecido en 1.
-
-## Reemplazar_running_query {#replace-running-query}
-
-Cuando se utiliza la interfaz HTTP, el ‘query_id’ parámetro puede ser pasado. Se trata de cualquier cadena que sirva como identificador de consulta.
-Si una consulta del mismo usuario ‘query_id’ que ya existe en este momento, el comportamiento depende de la ‘replace_running_query’ parámetro.
-
-`0` (default) – Throw an exception (don't allow the query to run if a query with the same ‘query_id’ ya se está ejecutando).
-
-`1` – Cancel the old query and start running the new one.
-
-El Yandex.Metrica utiliza este parámetro establecido en 1 para implementar sugerencias para las condiciones de segmentación. Después de ingresar el siguiente carácter, si la consulta anterior aún no ha finalizado, debe cancelarse.
-
-## Nombre de la red inalámbrica (SSID): {#stream-flush-interval-ms}
-
-Funciona para tablas con streaming en el caso de un tiempo de espera, o cuando un subproceso genera [Max_insert_block_size](#settings-max_insert_block_size) filas.
-
-El valor predeterminado es 7500.
-
-Cuanto menor sea el valor, más a menudo los datos se vacían en la tabla. Establecer el valor demasiado bajo conduce a un rendimiento deficiente.
-
-## load_balancing {#settings-load_balancing}
-
-Especifica el algoritmo de selección de réplicas que se utiliza para el procesamiento de consultas distribuidas.
-
-ClickHouse admite los siguientes algoritmos para elegir réplicas:
-
--   [Aleatorio](#load_balancing-random) (predeterminada)
--   [Nombre de host más cercano](#load_balancing-nearest_hostname)
--   [En orden](#load_balancing-in_order)
--   [Primero o aleatorio](#load_balancing-first_or_random)
-
-### Aleatorio (por defecto) {#load_balancing-random}
-
-``` sql
-load_balancing = random
-```
-
-El número de errores se cuenta para cada réplica. La consulta se envía a la réplica con el menor número de errores, y si hay varios de estos, a cualquiera de ellos.
-Desventajas: La proximidad del servidor no se tiene en cuenta; si las réplicas tienen datos diferentes, también obtendrá datos diferentes.
-
-### Nombre de host más cercano {#load_balancing-nearest_hostname}
-
-``` sql
-load_balancing = nearest_hostname
-```
-
-The number of errors is counted for each replica. Every 5 minutes, the number of errors is integrally divided by 2. Thus, the number of errors is calculated for a recent time with exponential smoothing. If there is one replica with a minimal number of errors (i.e. errors occurred recently on the other replicas), the query is sent to it. If there are multiple replicas with the same minimal number of errors, the query is sent to the replica with a hostname that is most similar to the server's hostname in the config file (for the number of different characters in identical positions, up to the minimum length of both hostnames).
-
-Por ejemplo, example01-01-1 y example01-01-2.yandex.ru son diferentes en una posición, mientras que example01-01-1 y example01-02-2 difieren en dos lugares.
-Este método puede parecer primitivo, pero no requiere datos externos sobre la topología de red, y no compara las direcciones IP, lo que sería complicado para nuestras direcciones IPv6.
-
-Por lo tanto, si hay réplicas equivalentes, se prefiere la más cercana por nombre.
-También podemos suponer que al enviar una consulta al mismo servidor, en ausencia de fallas, una consulta distribuida también irá a los mismos servidores. Por lo tanto, incluso si se colocan datos diferentes en las réplicas, la consulta devolverá principalmente los mismos resultados.
-
-### En orden {#load_balancing-in_order}
-
-``` sql
-load_balancing = in_order
-```
-
-Se accede a las réplicas con el mismo número de errores en el mismo orden en que se especifican en la configuración.
-Este método es apropiado cuando se sabe exactamente qué réplica es preferible.
-
-### Primero o aleatorio {#load_balancing-first_or_random}
-
-``` sql
-load_balancing = first_or_random
-```
-
-Este algoritmo elige la primera réplica del conjunto o una réplica aleatoria si la primera no está disponible. Es efectivo en configuraciones de topología de replicación cruzada, pero inútil en otras configuraciones.
-
-El `first_or_random` resuelve el problema del algoritmo `in_order` algoritmo. Con `in_order`, si una réplica se cae, la siguiente obtiene una carga doble mientras que las réplicas restantes manejan la cantidad habitual de tráfico. Cuando se utiliza el `first_or_random` algoritmo, la carga se distribuye uniformemente entre las réplicas que todavía están disponibles.
-
-## prefer_localhost_replica {#settings-prefer-localhost-replica}
-
-Habilita/deshabilita el uso preferible de la réplica localhost al procesar consultas distribuidas.
-
-Valores posibles:
-
--   1 — ClickHouse always sends a query to the localhost replica if it exists.
--   0 — ClickHouse uses the balancing strategy specified by the [load_balancing](#settings-load_balancing) configuración.
-
-Valor predeterminado: 1.
-
-!!! warning "Advertencia"
-    Deshabilite esta configuración si usa [max_parallel_replicas](#settings-max_parallel_replicas).
-
-## totals_mode {#totals-mode}
-
-Cómo calcular TOTALS cuando HAVING está presente, así como cuando max_rows_to_group_by y group_by_overflow_mode = ‘any’ están presentes.
-Vea la sección “WITH TOTALS modifier”.
-
-## totals_auto_threshold {#totals-auto-threshold}
-
-El umbral para `totals_mode = 'auto'`.
-Vea la sección “WITH TOTALS modifier”.
-
-## max_parallel_replicas {#settings-max_parallel_replicas}
-
-El número máximo de réplicas para cada fragmento al ejecutar una consulta.
-Para obtener coherencia (para obtener diferentes partes de la misma división de datos), esta opción solo funciona cuando se establece la clave de muestreo.
-El retraso de réplica no está controlado.
-
-## compilar {#compile}
-
-Habilitar la compilación de consultas. De forma predeterminada, 0 (deshabilitado).
-
-La compilación solo se usa para parte de la canalización de procesamiento de consultas: para la primera etapa de agregación (GROUP BY).
-Si se compiló esta parte de la canalización, la consulta puede ejecutarse más rápido debido a la implementación de ciclos cortos y a las llamadas de función agregadas en línea. La mejora del rendimiento máximo (hasta cuatro veces más rápido en casos excepcionales) se ve para consultas con múltiples funciones agregadas simples. Por lo general, la ganancia de rendimiento es insignificante. En casos muy raros, puede ralentizar la ejecución de la consulta.
-
-## min_count_to_compile {#min-count-to-compile}
-
-¿Cuántas veces usar potencialmente un fragmento de código compilado antes de ejecutar la compilación? Por defecto, 3.
-For testing, the value can be set to 0: compilation runs synchronously and the query waits for the end of the compilation process before continuing execution. For all other cases, use values ​​starting with 1. Compilation normally takes about 5-10 seconds.
-Si el valor es 1 o más, la compilación se produce de forma asíncrona en un subproceso independiente. El resultado se utilizará tan pronto como esté listo, incluidas las consultas que se están ejecutando actualmente.
-
-Se requiere código compilado para cada combinación diferente de funciones agregadas utilizadas en la consulta y el tipo de claves en la cláusula GROUP BY.
-The results of the compilation are saved in the build directory in the form of .so files. There is no restriction on the number of compilation results since they don't use very much space. Old results will be used after server restarts, except in the case of a server upgrade – in this case, the old results are deleted.
-
-## output_format_json_quote_64bit_integers {#session_settings-output_format_json_quote_64bit_integers}
-
-Si el valor es true, los enteros aparecen entre comillas cuando se usan los formatos JSON\* Int64 y UInt64 (por compatibilidad con la mayoría de las implementaciones de JavaScript); de lo contrario, los enteros se generan sin las comillas.
-
-## Formato_csv_delimiter {#settings-format_csv_delimiter}
-
-El carácter interpretado como un delimitador en los datos CSV. De forma predeterminada, el delimitador es `,`.
-
-## input_format_csv_unquoted_null_literal_as_null {#settings-input_format_csv_unquoted_null_literal_as_null}
-
-Para el formato de entrada CSV, habilita o deshabilita el análisis de `NULL` como literal (sinónimo de `\N`).
-
-## output_format_csv_crlf_end_of_line {#settings-output-format-csv-crlf-end-of-line}
-
-Utilice el separador de línea de estilo DOS / Windows (CRLF) en CSV en lugar de estilo Unix (LF).
-
-## output_format_tsv_crlf_end_of_line {#settings-output-format-tsv-crlf-end-of-line}
-
-Utilice el separador de línea de estilo DOC / Windows (CRLF) en TSV en lugar del estilo Unix (LF).
-
-## insert_quorum {#settings-insert_quorum}
-
-Habilita las escrituras de quórum.
-
--   Si `insert_quorum < 2`, las escrituras de quórum están deshabilitadas.
--   Si `insert_quorum >= 2`, las escrituras de quórum están habilitadas.
-
-Valor predeterminado: 0.
-
-Quorum escribe
-
-`INSERT` solo tiene éxito cuando ClickHouse logra escribir correctamente datos en el `insert_quorum` de réplicas durante el `insert_quorum_timeout`. Si por alguna razón el número de réplicas con escrituras exitosas no alcanza el `insert_quorum`, la escritura se considera fallida y ClickHouse eliminará el bloque insertado de todas las réplicas donde los datos ya se han escrito.
-
-Todas las réplicas del quórum son consistentes, es decir, contienen datos de todas las réplicas anteriores `INSERT` consulta. El `INSERT` la secuencia está linealizada.
-
-Al leer los datos escritos desde el `insert_quorum` usted puede utilizar el [select_sequential_consistency](#settings-select_sequential_consistency) opcion.
-
-ClickHouse genera una excepción
-
--   Si el número de réplicas disponibles en el momento de la consulta es `insert_quorum`.
--   En un intento de escribir datos cuando el bloque anterior aún no se ha insertado en el `insert_quorum` de réplicas. Esta situación puede ocurrir si el usuario intenta realizar una `INSERT` antes de la anterior con el `insert_quorum` se ha completado.
-
-Ver también:
-
--   [insert_quorum_timeout](#settings-insert_quorum_timeout)
--   [select_sequential_consistency](#settings-select_sequential_consistency)
-
-## insert_quorum_timeout {#settings-insert_quorum_timeout}
-
-Escribir en tiempo de espera de quórum en segundos. Si el tiempo de espera ha pasado y aún no se ha realizado ninguna escritura, ClickHouse generará una excepción y el cliente debe repetir la consulta para escribir el mismo bloque en la misma réplica o en cualquier otra réplica.
-
-Valor predeterminado: 60 segundos.
-
-Ver también:
-
--   [insert_quorum](#settings-insert_quorum)
--   [select_sequential_consistency](#settings-select_sequential_consistency)
-
-## select_sequential_consistency {#settings-select_sequential_consistency}
-
-Habilita o deshabilita la coherencia secuencial para `SELECT` consulta:
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 0.
-
-Uso
-
-Cuando se habilita la coherencia secuencial, ClickHouse permite al cliente ejecutar el `SELECT` consulta sólo para aquellas réplicas que contienen datos de todas las `INSERT` consultas ejecutadas con `insert_quorum`. Si el cliente hace referencia a una réplica parcial, ClickHouse generará una excepción. La consulta SELECT no incluirá datos que aún no se hayan escrito en el quórum de réplicas.
-
-Ver también:
-
--   [insert_quorum](#settings-insert_quorum)
--   [insert_quorum_timeout](#settings-insert_quorum_timeout)
-
-## insert_deduplicate {#settings-insert-deduplicate}
-
-Habilita o deshabilita la desduplicación de bloques `INSERT` (para tablas replicadas\*
-
-Valores posibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valor predeterminado: 1.
-
-De forma predeterminada, los bloques insertados en tablas replicadas `INSERT` declaración se deduplican (ver [Replicación de datos](../../engines/table-engines/mergetree-family/replication.md)).
-
-## deduplicate_blocks_in_dependent_materialized_views {#settings-deduplicate-blocks-in-dependent-materialized-views}
-
-Habilita o deshabilita la comprobación de desduplicación para las vistas materializadas que reciben datos de tablas replicadas\*.
-
-Valores posibles:
-
-      0 — Disabled.
-      1 — Enabled.
-
-Valor predeterminado: 0.
-
-Uso
-
-De forma predeterminada, la desduplicación no se realiza para las vistas materializadas, sino que se realiza en sentido ascendente, en la tabla de origen.
-Si se omite un bloque INSERTed debido a la desduplicación en la tabla de origen, no habrá inserción en las vistas materializadas adjuntas. Este comportamiento existe para permitir la inserción de datos altamente agregados en vistas materializadas, para los casos en que los bloques insertados son los mismos después de la agregación de vistas materializadas pero derivados de diferentes INSERT en la tabla de origen.
-Al mismo tiempo, este comportamiento “breaks” `INSERT` idempotencia. Si una `INSERT` en la mesa principal fue exitoso y `INSERT` into a materialized view failed (e.g. because of communication failure with Zookeeper) a client will get an error and can retry the operation. However, the materialized view won't receive the second insert because it will be discarded by deduplication in the main (source) table. The setting `deduplicate_blocks_in_dependent_materialized_views` permite cambiar este comportamiento. Al reintentar, una vista materializada recibirá la inserción de repetición y realizará la comprobación de desduplicación por sí misma,
-ignorando el resultado de la comprobación para la tabla de origen, e insertará filas perdidas debido a la primera falla.
-
-## Método de codificación de datos: {#settings-max-network-bytes}
-
-Limita el volumen de datos (en bytes) que se recibe o se transmite a través de la red al ejecutar una consulta. Esta configuración se aplica a cada consulta individual.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Data volume control is disabled.
-
-Valor predeterminado: 0.
-
-## Método de codificación de datos: {#settings-max-network-bandwidth}
-
-Limita la velocidad del intercambio de datos a través de la red en bytes por segundo. Esta configuración se aplica a todas las consultas.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Bandwidth control is disabled.
-
-Valor predeterminado: 0.
-
-## Todos los derechos reservados {#settings-max-network-bandwidth-for-user}
-
-Limita la velocidad del intercambio de datos a través de la red en bytes por segundo. Esta configuración se aplica a todas las consultas que se ejecutan simultáneamente realizadas por un único usuario.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Control of the data speed is disabled.
-
-Valor predeterminado: 0.
-
-## Todos los derechos reservados {#settings-max-network-bandwidth-for-all-users}
-
-Limita la velocidad a la que se intercambian datos a través de la red en bytes por segundo. Esta configuración se aplica a todas las consultas que se ejecutan simultáneamente en el servidor.
-
-Valores posibles:
-
--   Entero positivo.
--   0 — Control of the data speed is disabled.
-
-Valor predeterminado: 0.
-
-## count_distinct_implementation {#settings-count_distinct_implementation}
-
-Especifica cuál de las `uniq*` se deben utilizar para realizar el [COUNT(DISTINCT …)](../../sql-reference/aggregate-functions/reference.md#agg_function-count) construcción.
-
-Valores posibles:
-
--   [uniq](../../sql-reference/aggregate-functions/reference.md#agg_function-uniq)
--   [uniqCombined](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqcombined)
--   [UniqCombined64](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqcombined64)
--   [uniqHLL12](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqhll12)
--   [uniqExact](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqexact)
-
-Valor predeterminado: `uniqExact`.
-
-## skip_unavailable_shards {#settings-skip_unavailable_shards}
-
-Habilita o deshabilita la omisión silenciosa de fragmentos no disponibles.
-
-El fragmento se considera no disponible si todas sus réplicas no están disponibles. Una réplica no está disponible en los siguientes casos:
-
--   ClickHouse no puede conectarse a la réplica por ningún motivo.
-
-    Al conectarse a una réplica, ClickHouse realiza varios intentos. Si todos estos intentos fallan, la réplica se considera que no está disponible.
-
--   La réplica no se puede resolver a través de DNS.
-
-    Si el nombre de host de la réplica no se puede resolver a través de DNS, puede indicar las siguientes situaciones:
-
-    -   El host de Replica no tiene registro DNS. Puede ocurrir en sistemas con DNS dinámico, por ejemplo, [Kubernetes](https://kubernetes.io), donde los nodos pueden ser irresolubles durante el tiempo de inactividad, y esto no es un error.
-
-    -   Error de configuración. El archivo de configuración de ClickHouse contiene un nombre de host incorrecto.
-
-Valores posibles:
-
--   1 — skipping enabled.
-
-    Si un fragmento no está disponible, ClickHouse devuelve un resultado basado en datos parciales y no informa de problemas de disponibilidad de nodos.
-
--   0 — skipping disabled.
-
-    Si un fragmento no está disponible, ClickHouse produce una excepción.
-
-Valor predeterminado: 0.
-
-## Optize_skip_unused_shards {#settings-optimize_skip_unused_shards}
-
-Habilita o deshabilita la omisión de fragmentos no utilizados para las consultas SELECT que tienen la condición de clave de fragmentación en PREWHERE / WHERE (supone que los datos se distribuyen mediante clave de fragmentación, de lo contrario no hacer nada).
-
-Valor predeterminado: 0
-
-## Fuerza_optimize_skip_unused_shards {#settings-force_optimize_skip_unused_shards}
-
-Habilita o deshabilita la ejecución de consultas si [`optimize_skip_unused_shards`](#settings-optimize_skip_unused_shards) no es posible omitir fragmentos no utilizados. Si la omisión no es posible y la configuración está habilitada, se lanzará una excepción.
-
-Valores posibles:
-
--   0 - Discapacitados (no lanza)
--   1: deshabilite la ejecución de consultas solo si la tabla tiene una clave de fragmentación
--   2: deshabilita la ejecución de consultas independientemente de que se haya definido la clave de fragmentación para la tabla
-
-Valor predeterminado: 0
-
-## Optize_throw_if_noop {#setting-optimize_throw_if_noop}
-
-Habilita o deshabilita el lanzamiento de una excepción [OPTIMIZE](../../sql-reference/statements/misc.md#misc_operations-optimize) la consulta no realizó una fusión.
-
-Predeterminada, `OPTIMIZE` devuelve con éxito incluso si no hizo nada. Esta configuración le permite diferenciar estas situaciones y obtener el motivo en un mensaje de excepción.
-
-Valores posibles:
-
--   1 — Throwing an exception is enabled.
--   0 — Throwing an exception is disabled.
-
-Valor predeterminado: 0.
-
-## distributed_replica_error_half_life {#settings-distributed_replica_error_half_life}
-
--   Tipo: segundos
--   Valor predeterminado: 60 segundos
-
-Controla la rapidez con la que se ponen a cero los errores en las tablas distribuidas. Si una réplica no está disponible durante algún tiempo, acumula 5 errores y distribut_replica_error_half_life se establece en 1 segundo, la réplica se considera normal 3 segundos después del último error.
-
-Ver también:
-
--   [Motor de tabla distribuido](../../engines/table-engines/special/distributed.md)
--   [distributed_replica_error_cap](#settings-distributed_replica_error_cap)
-
-## distributed_replica_error_cap {#settings-distributed_replica_error_cap}
-
--   Tipo: unsigned int
--   Valor predeterminado: 1000
-
-El recuento de errores de cada réplica está limitado a este valor, lo que impide que una sola réplica acumule demasiados errores.
-
-Ver también:
-
--   [Motor de tabla distribuido](../../engines/table-engines/special/distributed.md)
--   [distributed_replica_error_half_life](#settings-distributed_replica_error_half_life)
-
-## Distributed_directory_monitor_sleep_time_ms {#distributed_directory_monitor_sleep_time_ms}
-
-Intervalo base para el [Distribuido](../../engines/table-engines/special/distributed.md) motor de tabla para enviar datos. El intervalo real crece exponencialmente en caso de errores.
-
-Valores posibles:
-
--   Un número entero positivo de milisegundos.
-
-Valor predeterminado: 100 milisegundos.
-
-## Distributed_directory_monitor_max_sleep_time_ms {#distributed_directory_monitor_max_sleep_time_ms}
-
-Intervalo máximo para el [Distribuido](../../engines/table-engines/special/distributed.md) motor de tabla para enviar datos. Limita el crecimiento exponencial del intervalo establecido en el [Distributed_directory_monitor_sleep_time_ms](#distributed_directory_monitor_sleep_time_ms) configuración.
-
-Valores posibles:
-
--   Un número entero positivo de milisegundos.
-
-Valor predeterminado: 30000 milisegundos (30 segundos).
-
-## distributed_directory_monitor_batch_inserts {#distributed_directory_monitor_batch_inserts}
-
-Habilita/deshabilita el envío de datos insertados en lotes.
-
-Cuando el envío por lotes está habilitado, el [Distribuido](../../engines/table-engines/special/distributed.md) El motor de tabla intenta enviar varios archivos de datos insertados en una operación en lugar de enviarlos por separado. El envío por lotes mejora el rendimiento del clúster al utilizar mejor los recursos del servidor y de la red.
-
-Valores posibles:
-
--   1 — Enabled.
--   0 — Disabled.
-
-Valor predeterminado: 0.
-
-## os_thread_priority {#setting-os-thread-priority}
-
-Establece la prioridad ([agradable](https://en.wikipedia.org/wiki/Nice_(Unix))) para subprocesos que ejecutan consultas. El programador del sistema operativo considera esta prioridad al elegir el siguiente hilo para ejecutar en cada núcleo de CPU disponible.
-
-!!! warning "Advertencia"
-    Para utilizar esta configuración, debe establecer el `CAP_SYS_NICE` capacidad. El `clickhouse-server` paquete lo configura durante la instalación. Algunos entornos virtuales no le permiten establecer `CAP_SYS_NICE` capacidad. En este caso, `clickhouse-server` muestra un mensaje al respecto al principio.
-
-Valores posibles:
-
--   Puede establecer valores en el rango `[-20, 19]`.
-
-Los valores más bajos significan mayor prioridad. Hilos con bajo `nice` Los valores de prioridad se ejecutan con más frecuencia que los subprocesos con valores altos. Los valores altos son preferibles para consultas no interactivas de larga ejecución porque les permite renunciar rápidamente a recursos en favor de consultas interactivas cortas cuando llegan.
-
-Valor predeterminado: 0.
-
-## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}
-
-Establece el período para un temporizador de reloj real del [perfilador de consultas](../../operations/optimizing-performance/sampling-query-profiler.md). El temporizador de reloj real cuenta el tiempo del reloj de pared.
-
-Valores posibles:
-
--   Número entero positivo, en nanosegundos.
-
-    Valores recomendados:
-
-            - 10000000 (100 times a second) nanoseconds and less for single queries.
-            - 1000000000 (once a second) for cluster-wide profiling.
-
--   0 para apagar el temporizador.
-
-Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-Valor predeterminado: 1000000000 nanosegundos (una vez por segundo).
-
-Ver también:
-
--   Tabla del sistema [trace_log](../../operations/system-tables.md#system_tables-trace_log)
-
-## Los resultados de la prueba {#query_profiler_cpu_time_period_ns}
-
-Establece el período para un temporizador de reloj de CPU [perfilador de consultas](../../operations/optimizing-performance/sampling-query-profiler.md). Este temporizador solo cuenta el tiempo de CPU.
-
-Valores posibles:
-
--   Un número entero positivo de nanosegundos.
-
-    Valores recomendados:
-
-            - 10000000 (100 times a second) nanoseconds and more for single queries.
-            - 1000000000 (once a second) for cluster-wide profiling.
-
--   0 para apagar el temporizador.
-
-Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-Valor predeterminado: 1000000000 nanosegundos.
-
-Ver también:
-
--   Tabla del sistema [trace_log](../../operations/system-tables.md#system_tables-trace_log)
-
-## allow_introspection_functions {#settings-allow_introspection_functions}
-
-Habilita deshabilita [funciones de introspecciones](../../sql-reference/functions/introspection.md) para la creación de perfiles de consultas.
-
-Valores posibles:
-
--   1 — Introspection functions enabled.
--   0 — Introspection functions disabled.
-
-Valor predeterminado: 0.
-
-**Ver también**
-
--   [Analizador de consultas de muestreo](../optimizing-performance/sampling-query-profiler.md)
--   Tabla del sistema [trace_log](../../operations/system-tables.md#system_tables-trace_log)
-
-## input_format_parallel_parsing {#input-format-parallel-parsing}
-
--   Tipo: bool
--   Valor predeterminado: True
-
-Habilitar el análisis paralelo de los formatos de datos para preservar el orden. Solo se admite para los formatos TSV, TKSV, CSV y JSONEachRow.
-
-## También puede utilizar los siguientes métodos de envío: {#min-chunk-bytes-for-parallel-parsing}
-
--   Tipo: unsigned int
--   Valor predeterminado: 1 MiB
-
-El tamaño mínimo de fragmento en bytes, que cada subproceso analizará en paralelo.
-
-## Sistema abierto {#settings-output_format_avro_codec}
-
-Establece el códec de compresión utilizado para el archivo Avro de salida.
-
-Tipo: cadena
-
-Valores posibles:
-
--   `null` — No compression
--   `deflate` — Compress with Deflate (zlib)
--   `snappy` — Compress with [Rápido](https://google.github.io/snappy/)
-
-Valor predeterminado: `snappy` (si está disponible) o `deflate`.
-
-## Sistema abierto {#settings-output_format_avro_sync_interval}
-
-Establece el tamaño mínimo de datos (en bytes) entre los marcadores de sincronización para el archivo Avro de salida.
-
-Tipo: unsigned int
-
-Valores posibles: 32 (32 bytes) - 1073741824 (1 GiB)
-
-Valor predeterminado: 32768 (32 KiB)
-
-## Todos los derechos reservados {#settings-format_avro_schema_registry_url}
-
-Establece la URL del Registro de esquemas confluentes para usar con [AvroConfluent](../../interfaces/formats.md#data-format-avro-confluent) formato
-
-Tipo: URL
-
-Valor predeterminado: Vacío
-
-## background_pool_size {#background_pool_size}
-
-Establece el número de subprocesos que realizan operaciones en segundo plano en motores de tabla (por ejemplo, fusiona [Motor MergeTree](../../engines/table-engines/mergetree-family/index.md) tabla). Esta configuración se aplica al inicio del servidor ClickHouse y no se puede cambiar en una sesión de usuario. Al ajustar esta configuración, puede administrar la carga de la CPU y el disco. Un tamaño de grupo más pequeño utiliza menos recursos de CPU y disco, pero los procesos en segundo plano avanzan más lentamente, lo que eventualmente podría afectar el rendimiento de la consulta.
-
-Valores posibles:
-
--   Cualquier entero positivo.
-
-Valor predeterminado: 16.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/settings/settings/) <!-- hide -->
diff --git a/docs/es/operations/system-tables.md b/docs/es/operations/system-tables.md
deleted file mode 100644
index 18e7f7227da2..000000000000
--- a/docs/es/operations/system-tables.md
+++ /dev/null
@@ -1,1168 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 52
-toc_title: Tablas del sistema
----
-
-# Tablas del sistema {#system-tables}
-
-Las tablas del sistema se utilizan para implementar parte de la funcionalidad del sistema y para proporcionar acceso a información sobre cómo funciona el sistema.
-No puede eliminar una tabla del sistema (pero puede realizar DETACH).
-Las tablas del sistema no tienen archivos con datos en el disco o archivos con metadatos. El servidor crea todas las tablas del sistema cuando se inicia.
-Las tablas del sistema son de solo lectura.
-Están ubicados en el ‘system’ base.
-
-## sistema.asynchronous_metrics {#system_tables-asynchronous_metrics}
-
-Contiene métricas que se calculan periódicamente en segundo plano. Por ejemplo, la cantidad de RAM en uso.
-
-Columna:
-
--   `metric` ([Cadena](../sql-reference/data-types/string.md)) — Metric name.
--   `value` ([Float64](../sql-reference/data-types/float.md)) — Metric value.
-
-**Ejemplo**
-
-``` sql
-SELECT * FROM system.asynchronous_metrics LIMIT 10
-```
-
-``` text
-┌─metric──────────────────────────────────┬──────value─┐
-│ jemalloc.background_thread.run_interval │          0 │
-│ jemalloc.background_thread.num_runs     │          0 │
-│ jemalloc.background_thread.num_threads  │          0 │
-│ jemalloc.retained                       │  422551552 │
-│ jemalloc.mapped                         │ 1682989056 │
-│ jemalloc.resident                       │ 1656446976 │
-│ jemalloc.metadata_thp                   │          0 │
-│ jemalloc.metadata                       │   10226856 │
-│ UncompressedCacheCells                  │          0 │
-│ MarkCacheFiles                          │          0 │
-└─────────────────────────────────────────┴────────────┘
-```
-
-**Ver también**
-
--   [Monitoreo](monitoring.md) — Base concepts of ClickHouse monitoring.
--   [sistema.métricas](#system_tables-metrics) — Contains instantly calculated metrics.
--   [sistema.evento](#system_tables-events) — Contains a number of events that have occurred.
--   [sistema.metric_log](#system_tables-metric_log) — Contains a history of metrics values from tables `system.metrics` и `system.events`.
-
-## sistema.Cluster {#system-clusters}
-
-Contiene información sobre los clústeres disponibles en el archivo de configuración y los servidores que contienen.
-
-Columna:
-
--   `cluster` (String) — The cluster name.
--   `shard_num` (UInt32) — The shard number in the cluster, starting from 1.
--   `shard_weight` (UInt32) — The relative weight of the shard when writing data.
--   `replica_num` (UInt32) — The replica number in the shard, starting from 1.
--   `host_name` (String) — The host name, as specified in the config.
--   `host_address` (String) — The host IP address obtained from DNS.
--   `port` (UInt16) — The port to use for connecting to the server.
--   `user` (String) — The name of the user for connecting to the server.
--   `errors_count` (UInt32): número de veces que este host no pudo alcanzar la réplica.
--   `estimated_recovery_time` (UInt32): quedan segundos hasta que el recuento de errores de réplica se ponga a cero y se considere que vuelve a la normalidad.
-
-Tenga en cuenta que `errors_count` se actualiza una vez por consulta al clúster, pero `estimated_recovery_time` se vuelve a calcular bajo demanda. Entonces podría haber un caso distinto de cero `errors_count` y cero `estimated_recovery_time`, esa próxima consulta será cero `errors_count` e intente usar la réplica como si no tuviera errores.
-
-**Ver también**
-
--   [Motor de tabla distribuido](../engines/table-engines/special/distributed.md)
--   [distributed_replica_error_cap configuración](settings/settings.md#settings-distributed_replica_error_cap)
--   [distributed_replica_error_half_life configuración](settings/settings.md#settings-distributed_replica_error_half_life)
-
-## sistema.columna {#system-columns}
-
-Contiene información sobre las columnas de todas las tablas.
-
-Puede utilizar esta tabla para obtener información similar a la [DESCRIBE TABLE](../sql-reference/statements/misc.md#misc-describe-table) consulta, pero para varias tablas a la vez.
-
-El `system.columns` tabla contiene las siguientes columnas (el tipo de columna se muestra entre corchetes):
-
--   `database` (String) — Database name.
--   `table` (String) — Table name.
--   `name` (String) — Column name.
--   `type` (String) — Column type.
--   `default_kind` (String) — Expression type (`DEFAULT`, `MATERIALIZED`, `ALIAS`) para el valor predeterminado, o una cadena vacía si no está definida.
--   `default_expression` (String) — Expression for the default value, or an empty string if it is not defined.
--   `data_compressed_bytes` (UInt64) — The size of compressed data, in bytes.
--   `data_uncompressed_bytes` (UInt64) — The size of decompressed data, in bytes.
--   `marks_bytes` (UInt64) — The size of marks, in bytes.
--   `comment` (String) — Comment on the column, or an empty string if it is not defined.
--   `is_in_partition_key` (UInt8) — Flag that indicates whether the column is in the partition expression.
--   `is_in_sorting_key` (UInt8) — Flag that indicates whether the column is in the sorting key expression.
--   `is_in_primary_key` (UInt8) — Flag that indicates whether the column is in the primary key expression.
--   `is_in_sampling_key` (UInt8) — Flag that indicates whether the column is in the sampling key expression.
-
-## sistema.colaborador {#system-contributors}
-
-Contiene información sobre los colaboradores. Todos los constributores en orden aleatorio. El orden es aleatorio en el momento de la ejecución de la consulta.
-
-Columna:
-
--   `name` (String) — Contributor (author) name from git log.
-
-**Ejemplo**
-
-``` sql
-SELECT * FROM system.contributors LIMIT 10
-```
-
-``` text
-┌─name─────────────┐
-│ Olga Khvostikova │
-│ Max Vetrov       │
-│ LiuYangkuan      │
-│ svladykin        │
-│ zamulla          │
-│ Šimon Podlipský  │
-│ BayoNet          │
-│ Ilya Khomutov    │
-│ Amy Krishnevsky  │
-│ Loud_Scream      │
-└──────────────────┘
-```
-
-Para descubrirlo en la tabla, use una consulta:
-
-``` sql
-SELECT * FROM system.contributors WHERE name='Olga Khvostikova'
-```
-
-``` text
-┌─name─────────────┐
-│ Olga Khvostikova │
-└──────────────────┘
-```
-
-## sistema.base {#system-databases}
-
-Esta tabla contiene una sola columna String llamada ‘name’ – the name of a database.
-Cada base de datos que el servidor conoce tiene una entrada correspondiente en la tabla.
-Esta tabla del sistema se utiliza para implementar el `SHOW DATABASES` consulta.
-
-## sistema.detached_parts {#system_tables-detached_parts}
-
-Contiene información sobre piezas separadas de [Método de codificación de datos:](../engines/table-engines/mergetree-family/mergetree.md) tabla. El `reason` columna especifica por qué se separó la pieza. Para las piezas separadas por el usuario, el motivo está vacío. Tales partes se pueden unir con [ALTER TABLE ATTACH PARTITION\|PART](../sql-reference/statements/alter.md#alter_attach-partition) comando. Para obtener la descripción de otras columnas, consulte [sistema.parte](#system_tables-parts). Si el nombre de la pieza no es válido, los valores de algunas columnas pueden ser `NULL`. Tales partes se pueden eliminar con [ALTER TABLE DROP DETACHED PART](../sql-reference/statements/alter.md#alter_drop-detached).
-
-## sistema.diccionario {#system_tables-dictionaries}
-
-Contiene información sobre [diccionarios externos](../sql-reference/dictionaries/external-dictionaries/external-dicts.md).
-
-Columna:
-
--   `database` ([Cadena](../sql-reference/data-types/string.md)) — Name of the database containing the dictionary created by DDL query. Empty string for other dictionaries.
--   `name` ([Cadena](../sql-reference/data-types/string.md)) — [Nombre del diccionario](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md).
--   `status` ([Enum8](../sql-reference/data-types/enum.md)) — Dictionary status. Possible values:
-    -   `NOT_LOADED` — Dictionary was not loaded because it was not used.
-    -   `LOADED` — Dictionary loaded successfully.
-    -   `FAILED` — Unable to load the dictionary as a result of an error.
-    -   `LOADING` — Dictionary is loading now.
-    -   `LOADED_AND_RELOADING` — Dictionary is loaded successfully, and is being reloaded right now (frequent reasons: [SYSTEM RELOAD DICTIONARY](../sql-reference/statements/system.md#query_language-system-reload-dictionary) consulta, tiempo de espera, configuración del diccionario ha cambiado).
-    -   `FAILED_AND_RELOADING` — Could not load the dictionary as a result of an error and is loading now.
--   `origin` ([Cadena](../sql-reference/data-types/string.md)) — Path to the configuration file that describes the dictionary.
--   `type` ([Cadena](../sql-reference/data-types/string.md)) — Type of a dictionary allocation. [Almacenamiento de diccionarios en la memoria](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md).
--   `key` — [Tipo de llave](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-key): Clave numérica ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) or Сomposite key ([Cadena](../sql-reference/data-types/string.md)) — form “(type 1, type 2, …, type n)”.
--   `attribute.names` ([Matriz](../sql-reference/data-types/array.md)([Cadena](../sql-reference/data-types/string.md))) — Array of [nombres de atributos](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes) proporcionada por el diccionario.
--   `attribute.types` ([Matriz](../sql-reference/data-types/array.md)([Cadena](../sql-reference/data-types/string.md))) — Corresponding array of [tipos de atributos](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes) que son proporcionados por el diccionario.
--   `bytes_allocated` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Amount of RAM allocated for the dictionary.
--   `query_count` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Number of queries since the dictionary was loaded or since the last successful reboot.
--   `hit_rate` ([Float64](../sql-reference/data-types/float.md)) — For cache dictionaries, the percentage of uses for which the value was in the cache.
--   `element_count` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Number of items stored in the dictionary.
--   `load_factor` ([Float64](../sql-reference/data-types/float.md)) — Percentage filled in the dictionary (for a hashed dictionary, the percentage filled in the hash table).
--   `source` ([Cadena](../sql-reference/data-types/string.md)) — Text describing the [fuente de datos](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md) para el diccionario.
--   `lifetime_min` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Minimum [vida](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md) del diccionario en la memoria, después de lo cual ClickHouse intenta volver a cargar el diccionario (si `invalidate_query` está configurado, entonces solo si ha cambiado). Establecer en segundos.
--   `lifetime_max` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Maximum [vida](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md) del diccionario en la memoria, después de lo cual ClickHouse intenta volver a cargar el diccionario (si `invalidate_query` está configurado, entonces solo si ha cambiado). Establecer en segundos.
--   `loading_start_time` ([FechaHora](../sql-reference/data-types/datetime.md)) — Start time for loading the dictionary.
--   `last_successful_update_time` ([FechaHora](../sql-reference/data-types/datetime.md)) — End time for loading or updating the dictionary. Helps to monitor some troubles with external sources and investigate causes.
--   `loading_duration` ([Float32](../sql-reference/data-types/float.md)) — Duration of a dictionary loading.
--   `last_exception` ([Cadena](../sql-reference/data-types/string.md)) — Text of the error that occurs when creating or reloading the dictionary if the dictionary couldn't be created.
-
-**Ejemplo**
-
-Configurar el diccionario.
-
-``` sql
-CREATE DICTIONARY dictdb.dict
-(
-    `key` Int64 DEFAULT -1,
-    `value_default` String DEFAULT 'world',
-    `value_expression` String DEFAULT 'xxx' EXPRESSION 'toString(127 * 172)'
-)
-PRIMARY KEY key
-SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'dicttbl' DB 'dictdb'))
-LIFETIME(MIN 0 MAX 1)
-LAYOUT(FLAT())
-```
-
-Asegúrese de que el diccionario esté cargado.
-
-``` sql
-SELECT * FROM system.dictionaries
-```
-
-``` text
-┌─database─┬─name─┬─status─┬─origin──────┬─type─┬─key────┬─attribute.names──────────────────────┬─attribute.types─────┬─bytes_allocated─┬─query_count─┬─hit_rate─┬─element_count─┬───────────load_factor─┬─source─────────────────────┬─lifetime_min─┬─lifetime_max─┬──loading_start_time─┌──last_successful_update_time─┬──────loading_duration─┬─last_exception─┐
-│ dictdb   │ dict │ LOADED │ dictdb.dict │ Flat │ UInt64 │ ['value_default','value_expression'] │ ['String','String'] │           74032 │           0 │        1 │             1 │ 0.0004887585532746823 │ ClickHouse: dictdb.dicttbl │            0 │            1 │ 2020-03-04 04:17:34 │   2020-03-04 04:30:34        │                 0.002 │                │
-└──────────┴──────┴────────┴─────────────┴──────┴────────┴──────────────────────────────────────┴─────────────────────┴─────────────────┴─────────────┴──────────┴───────────────┴───────────────────────┴────────────────────────────┴──────────────┴──────────────┴─────────────────────┴──────────────────────────────┘───────────────────────┴────────────────┘
-```
-
-## sistema.evento {#system_tables-events}
-
-Contiene información sobre el número de eventos que se han producido en el sistema. Por ejemplo, en la tabla, puede encontrar cuántos `SELECT` las consultas se procesaron desde que se inició el servidor ClickHouse.
-
-Columna:
-
--   `event` ([Cadena](../sql-reference/data-types/string.md)) — Event name.
--   `value` ([UInt64](../sql-reference/data-types/int-uint.md)) — Number of events occurred.
--   `description` ([Cadena](../sql-reference/data-types/string.md)) — Event description.
-
-**Ejemplo**
-
-``` sql
-SELECT * FROM system.events LIMIT 5
-```
-
-``` text
-┌─event─────────────────────────────────┬─value─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ Query                                 │    12 │ Number of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries.                  │
-│ SelectQuery                           │     8 │ Same as Query, but only for SELECT queries.                                                                                                                                                                                                                │
-│ FileOpen                              │    73 │ Number of files opened.                                                                                                                                                                                                                                    │
-│ ReadBufferFromFileDescriptorRead      │   155 │ Number of reads (read/pread) from a file descriptor. Does not include sockets.                                                                                                                                                                             │
-│ ReadBufferFromFileDescriptorReadBytes │  9931 │ Number of bytes read from file descriptors. If the file is compressed, this will show the compressed data size.                                                                                                                                              │
-└───────────────────────────────────────┴───────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-**Ver también**
-
--   [sistema.asynchronous_metrics](#system_tables-asynchronous_metrics) — Contains periodically calculated metrics.
--   [sistema.métricas](#system_tables-metrics) — Contains instantly calculated metrics.
--   [sistema.metric_log](#system_tables-metric_log) — Contains a history of metrics values from tables `system.metrics` и `system.events`.
--   [Monitoreo](monitoring.md) — Base concepts of ClickHouse monitoring.
-
-## sistema.función {#system-functions}
-
-Contiene información sobre funciones normales y agregadas.
-
-Columna:
-
--   `name`(`String`) – The name of the function.
--   `is_aggregate`(`UInt8`) — Whether the function is aggregate.
-
-## sistema.graphite_retentions {#system-graphite-retentions}
-
-Contiene información sobre los parámetros [graphite_rollup](server-configuration-parameters/settings.md#server_configuration_parameters-graphite) que se utilizan en tablas con [\*GraphiteMergeTree](../engines/table-engines/mergetree-family/graphitemergetree.md) motor.
-
-Columna:
-
--   `config_name` (Cadena) - `graphite_rollup` nombre del parámetro.
--   `regexp` (Cadena) - Un patrón para el nombre de la métrica.
--   `function` (String) - El nombre de la función de agregación.
--   `age` (UInt64) - La edad mínima de los datos en segundos.
--   `precision` (UInt64) - Cómo definir con precisión la edad de los datos en segundos.
--   `priority` (UInt16) - Prioridad de patrón.
--   `is_default` (UInt8) - Si el patrón es el predeterminado.
--   `Tables.database` (Array(String)) - Matriz de nombres de tablas de base de datos que utilizan `config_name` parámetro.
--   `Tables.table` (Array(String)) - Matriz de nombres de tablas que utilizan `config_name` parámetro.
-
-## sistema.fusionar {#system-merges}
-
-Contiene información sobre fusiones y mutaciones de piezas actualmente en proceso para tablas de la familia MergeTree.
-
-Columna:
-
--   `database` (String) — The name of the database the table is in.
--   `table` (String) — Table name.
--   `elapsed` (Float64) — The time elapsed (in seconds) since the merge started.
--   `progress` (Float64) — The percentage of completed work from 0 to 1.
--   `num_parts` (UInt64) — The number of pieces to be merged.
--   `result_part_name` (String) — The name of the part that will be formed as the result of merging.
--   `is_mutation` (UInt8) - 1 si este proceso es una mutación parte.
--   `total_size_bytes_compressed` (UInt64) — The total size of the compressed data in the merged chunks.
--   `total_size_marks` (UInt64) — The total number of marks in the merged parts.
--   `bytes_read_uncompressed` (UInt64) — Number of bytes read, uncompressed.
--   `rows_read` (UInt64) — Number of rows read.
--   `bytes_written_uncompressed` (UInt64) — Number of bytes written, uncompressed.
--   `rows_written` (UInt64) — Number of rows written.
-
-## sistema.métricas {#system_tables-metrics}
-
-Contiene métricas que pueden calcularse instantáneamente o tener un valor actual. Por ejemplo, el número de consultas procesadas simultáneamente o el retraso de réplica actual. Esta tabla está siempre actualizada.
-
-Columna:
-
--   `metric` ([Cadena](../sql-reference/data-types/string.md)) — Metric name.
--   `value` ([Int64](../sql-reference/data-types/int-uint.md)) — Metric value.
--   `description` ([Cadena](../sql-reference/data-types/string.md)) — Metric description.
-
-La lista de métricas admitidas que puede encontrar en el [src/Common/CurrentMetrics.cpp](https://github.com/ClickHouse/ClickHouse/blob/master/src/Common/CurrentMetrics.cpp) archivo fuente de ClickHouse.
-
-**Ejemplo**
-
-``` sql
-SELECT * FROM system.metrics LIMIT 10
-```
-
-``` text
-┌─metric─────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ Query                      │     1 │ Number of executing queries                                                                                                                                                                      │
-│ Merge                      │     0 │ Number of executing background merges                                                                                                                                                            │
-│ PartMutation               │     0 │ Number of mutations (ALTER DELETE/UPDATE)                                                                                                                                                        │
-│ ReplicatedFetch            │     0 │ Number of data parts being fetched from replicas                                                                                                                                                │
-│ ReplicatedSend             │     0 │ Number of data parts being sent to replicas                                                                                                                                                      │
-│ ReplicatedChecks           │     0 │ Number of data parts checking for consistency                                                                                                                                                    │
-│ BackgroundPoolTask         │     0 │ Number of active tasks in BackgroundProcessingPool (merges, mutations, fetches, or replication queue bookkeeping)                                                                                │
-│ BackgroundSchedulePoolTask │     0 │ Number of active tasks in BackgroundSchedulePool. This pool is used for periodic ReplicatedMergeTree tasks, like cleaning old data parts, altering data parts, replica re-initialization, etc.   │
-│ DiskSpaceReservedForMerge  │     0 │ Disk space reserved for currently running background merges. It is slightly more than the total size of currently merging parts.                                                                     │
-│ DistributedSend            │     0 │ Number of connections to remote servers sending data that was INSERTed into Distributed tables. Both synchronous and asynchronous mode.                                                          │
-└────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-**Ver también**
-
--   [sistema.asynchronous_metrics](#system_tables-asynchronous_metrics) — Contains periodically calculated metrics.
--   [sistema.evento](#system_tables-events) — Contains a number of events that occurred.
--   [sistema.metric_log](#system_tables-metric_log) — Contains a history of metrics values from tables `system.metrics` и `system.events`.
--   [Monitoreo](monitoring.md) — Base concepts of ClickHouse monitoring.
-
-## sistema.metric_log {#system_tables-metric_log}
-
-Contiene el historial de valores de métricas de tablas `system.metrics` y `system.events`, periódicamente enjuagado al disco.
-Para activar la recopilación de historial de métricas en `system.metric_log`, crear `/etc/clickhouse-server/config.d/metric_log.xml` con el siguiente contenido:
-
-``` xml
-<yandex>
-    <metric_log>
-        <database>system</database>
-        <table>metric_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
-    </metric_log>
-</yandex>
-```
-
-**Ejemplo**
-
-``` sql
-SELECT * FROM system.metric_log LIMIT 1 FORMAT Vertical;
-```
-
-``` text
-Row 1:
-──────
-event_date:                                                 2020-02-18
-event_time:                                                 2020-02-18 07:15:33
-milliseconds:                                               554
-ProfileEvent_Query:                                         0
-ProfileEvent_SelectQuery:                                   0
-ProfileEvent_InsertQuery:                                   0
-ProfileEvent_FileOpen:                                      0
-ProfileEvent_Seek:                                          0
-ProfileEvent_ReadBufferFromFileDescriptorRead:              1
-ProfileEvent_ReadBufferFromFileDescriptorReadFailed:        0
-ProfileEvent_ReadBufferFromFileDescriptorReadBytes:         0
-ProfileEvent_WriteBufferFromFileDescriptorWrite:            1
-ProfileEvent_WriteBufferFromFileDescriptorWriteFailed:      0
-ProfileEvent_WriteBufferFromFileDescriptorWriteBytes:       56
-...
-CurrentMetric_Query:                                        0
-CurrentMetric_Merge:                                        0
-CurrentMetric_PartMutation:                                 0
-CurrentMetric_ReplicatedFetch:                              0
-CurrentMetric_ReplicatedSend:                               0
-CurrentMetric_ReplicatedChecks:                             0
-...
-```
-
-**Ver también**
-
--   [sistema.asynchronous_metrics](#system_tables-asynchronous_metrics) — Contains periodically calculated metrics.
--   [sistema.evento](#system_tables-events) — Contains a number of events that occurred.
--   [sistema.métricas](#system_tables-metrics) — Contains instantly calculated metrics.
--   [Monitoreo](monitoring.md) — Base concepts of ClickHouse monitoring.
-
-## sistema.numero {#system-numbers}
-
-Esta tabla contiene una única columna UInt64 llamada ‘number’ que contiene casi todos los números naturales a partir de cero.
-Puede usar esta tabla para pruebas, o si necesita hacer una búsqueda de fuerza bruta.
-Las lecturas de esta tabla no están paralelizadas.
-
-## sistema.Números_mt {#system-numbers-mt}
-
-Lo mismo que ‘system.numbers’ pero las lecturas están paralelizadas. Los números se pueden devolver en cualquier orden.
-Se utiliza para pruebas.
-
-## sistema.una {#system-one}
-
-Esta tabla contiene una sola fila con una ‘dummy’ Columna UInt8 que contiene el valor 0.
-Esta tabla se utiliza si una consulta SELECT no especifica la cláusula FROM.
-Esto es similar a la tabla DUAL que se encuentra en otros DBMS.
-
-## sistema.parte {#system_tables-parts}
-
-Contiene información sobre partes de [Método de codificación de datos:](../engines/table-engines/mergetree-family/mergetree.md) tabla.
-
-Cada fila describe una parte de datos.
-
-Columna:
-
--   `partition` (String) – The partition name. To learn what a partition is, see the description of the [ALTER](../sql-reference/statements/alter.md#query_language_queries_alter) consulta.
-
-    Formato:
-
-    -   `YYYYMM` para la partición automática por mes.
-    -   `any_string` al particionar manualmente.
-
--   `name` (`String`) – Name of the data part.
-
--   `active` (`UInt8`) – Flag that indicates whether the data part is active. If a data part is active, it's used in a table. Otherwise, it's deleted. Inactive data parts remain after merging.
-
--   `marks` (`UInt64`) – The number of marks. To get the approximate number of rows in a data part, multiply `marks` por la granularidad del índice (generalmente 8192) (esta sugerencia no funciona para la granularidad adaptativa).
-
--   `rows` (`UInt64`) – The number of rows.
-
--   `bytes_on_disk` (`UInt64`) – Total size of all the data part files in bytes.
-
--   `data_compressed_bytes` (`UInt64`) – Total size of compressed data in the data part. All the auxiliary files (for example, files with marks) are not included.
-
--   `data_uncompressed_bytes` (`UInt64`) – Total size of uncompressed data in the data part. All the auxiliary files (for example, files with marks) are not included.
-
--   `marks_bytes` (`UInt64`) – The size of the file with marks.
-
--   `modification_time` (`DateTime`) – The time the directory with the data part was modified. This usually corresponds to the time of data part creation.\|
-
--   `remove_time` (`DateTime`) – The time when the data part became inactive.
-
--   `refcount` (`UInt32`) – The number of places where the data part is used. A value greater than 2 indicates that the data part is used in queries or merges.
-
--   `min_date` (`Date`) – The minimum value of the date key in the data part.
-
--   `max_date` (`Date`) – The maximum value of the date key in the data part.
-
--   `min_time` (`DateTime`) – The minimum value of the date and time key in the data part.
-
--   `max_time`(`DateTime`) – The maximum value of the date and time key in the data part.
-
--   `partition_id` (`String`) – ID of the partition.
-
--   `min_block_number` (`UInt64`) – The minimum number of data parts that make up the current part after merging.
-
--   `max_block_number` (`UInt64`) – The maximum number of data parts that make up the current part after merging.
-
--   `level` (`UInt32`) – Depth of the merge tree. Zero means that the current part was created by insert rather than by merging other parts.
-
--   `data_version` (`UInt64`) – Number that is used to determine which mutations should be applied to the data part (mutations with a version higher than `data_version`).
-
--   `primary_key_bytes_in_memory` (`UInt64`) – The amount of memory (in bytes) used by primary key values.
-
--   `primary_key_bytes_in_memory_allocated` (`UInt64`) – The amount of memory (in bytes) reserved for primary key values.
-
--   `is_frozen` (`UInt8`) – Flag that shows that a partition data backup exists. 1, the backup exists. 0, the backup doesn't exist. For more details, see [FREEZE PARTITION](../sql-reference/statements/alter.md#alter_freeze-partition)
-
--   `database` (`String`) – Name of the database.
-
--   `table` (`String`) – Name of the table.
-
--   `engine` (`String`) – Name of the table engine without parameters.
-
--   `path` (`String`) – Absolute path to the folder with data part files.
-
--   `disk` (`String`) – Name of a disk that stores the data part.
-
--   `hash_of_all_files` (`String`) – [sipHash128](../sql-reference/functions/hash-functions.md#hash_functions-siphash128) de archivos comprimidos.
-
--   `hash_of_uncompressed_files` (`String`) – [sipHash128](../sql-reference/functions/hash-functions.md#hash_functions-siphash128) de archivos sin comprimir (archivos con marcas, archivo de índice, etc.).
-
--   `uncompressed_hash_of_compressed_files` (`String`) – [sipHash128](../sql-reference/functions/hash-functions.md#hash_functions-siphash128) de datos en los archivos comprimidos como si estuvieran descomprimidos.
-
--   `bytes` (`UInt64`) – Alias for `bytes_on_disk`.
-
--   `marks_size` (`UInt64`) – Alias for `marks_bytes`.
-
-## sistema.part_log {#system_tables-part-log}
-
-El `system.part_log` se crea sólo si el [part_log](server-configuration-parameters/settings.md#server_configuration_parameters-part-log) se especifica la configuración del servidor.
-
-Esta tabla contiene información sobre eventos que ocurrieron con [partes de datos](../engines/table-engines/mergetree-family/custom-partitioning-key.md) en el [Método de codificación de datos:](../engines/table-engines/mergetree-family/mergetree.md) tablas familiares, como agregar o fusionar datos.
-
-El `system.part_log` contiene las siguientes columnas:
-
--   `event_type` (Enum) — Type of the event that occurred with the data part. Can have one of the following values:
-    -   `NEW_PART` — Inserting of a new data part.
-    -   `MERGE_PARTS` — Merging of data parts.
-    -   `DOWNLOAD_PART` — Downloading a data part.
-    -   `REMOVE_PART` — Removing or detaching a data part using [DETACH PARTITION](../sql-reference/statements/alter.md#alter_detach-partition).
-    -   `MUTATE_PART` — Mutating of a data part.
-    -   `MOVE_PART` — Moving the data part from the one disk to another one.
--   `event_date` (Date) — Event date.
--   `event_time` (DateTime) — Event time.
--   `duration_ms` (UInt64) — Duration.
--   `database` (String) — Name of the database the data part is in.
--   `table` (String) — Name of the table the data part is in.
--   `part_name` (String) — Name of the data part.
--   `partition_id` (String) — ID of the partition that the data part was inserted to. The column takes the ‘all’ valor si la partición es por `tuple()`.
--   `rows` (UInt64) — The number of rows in the data part.
--   `size_in_bytes` (UInt64) — Size of the data part in bytes.
--   `merged_from` (Array(String)) — An array of names of the parts which the current part was made up from (after the merge).
--   `bytes_uncompressed` (UInt64) — Size of uncompressed bytes.
--   `read_rows` (UInt64) — The number of rows was read during the merge.
--   `read_bytes` (UInt64) — The number of bytes was read during the merge.
--   `error` (UInt16) — The code number of the occurred error.
--   `exception` (String) — Text message of the occurred error.
-
-El `system.part_log` se crea después de la primera inserción de datos `MergeTree` tabla.
-
-## sistema.procesa {#system_tables-processes}
-
-Esta tabla del sistema se utiliza para implementar el `SHOW PROCESSLIST` consulta.
-
-Columna:
-
--   `user` (String) – The user who made the query. Keep in mind that for distributed processing, queries are sent to remote servers under the `default` usuario. El campo contiene el nombre de usuario para una consulta específica, no para una consulta que esta consulta inició.
--   `address` (String) – The IP address the request was made from. The same for distributed processing. To track where a distributed query was originally made from, look at `system.processes` en el servidor de solicitud de consulta.
--   `elapsed` (Float64) – The time in seconds since request execution started.
--   `rows_read` (UInt64) – The number of rows read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.
--   `bytes_read` (UInt64) – The number of uncompressed bytes read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.
--   `total_rows_approx` (UInt64) – The approximation of the total number of rows that should be read. For distributed processing, on the requestor server, this is the total for all remote servers. It can be updated during request processing, when new sources to process become known.
--   `memory_usage` (UInt64) – Amount of RAM the request uses. It might not include some types of dedicated memory. See the [Método de codificación de datos:](../operations/settings/query-complexity.md#settings_max_memory_usage) configuración.
--   `query` (String) – The query text. For `INSERT`, no incluye los datos para insertar.
--   `query_id` (String) – Query ID, if defined.
-
-## sistema.text_log {#system_tables-text_log}
-
-Contiene entradas de registro. El nivel de registro que va a esta tabla se puede limitar con `text_log.level` configuración del servidor.
-
-Columna:
-
--   `event_date` (`Date`) - Fecha de la entrada.
--   `event_time` (`DateTime`) - Hora de la entrada.
--   `microseconds` (`UInt32`) - Microsegundos de la entrada.
--   `thread_name` (String) — Name of the thread from which the logging was done.
--   `thread_id` (UInt64) — OS thread ID.
--   `level` (`Enum8`) - Nivel de entrada.
-    -   `'Fatal' = 1`
-    -   `'Critical' = 2`
-    -   `'Error' = 3`
-    -   `'Warning' = 4`
-    -   `'Notice' = 5`
-    -   `'Information' = 6`
-    -   `'Debug' = 7`
-    -   `'Trace' = 8`
--   `query_id` (`String`) - ID de la consulta.
--   `logger_name` (`LowCardinality(String)`) - Name of the logger (i.e. `DDLWorker`)
--   `message` (`String`) - El mensaje en sí.
--   `revision` (`UInt32`) - Revisión de ClickHouse.
--   `source_file` (`LowCardinality(String)`) - Archivo de origen desde el que se realizó el registro.
--   `source_line` (`UInt64`) - Línea de origen desde la que se realizó el registro.
-
-## sistema.query_log {#system_tables-query_log}
-
-Contiene información sobre la ejecución de consultas. Para cada consulta, puede ver la hora de inicio del procesamiento, la duración del procesamiento, los mensajes de error y otra información.
-
-!!! note "Nota"
-    La tabla no contiene datos de entrada para `INSERT` consulta.
-
-ClickHouse crea esta tabla sólo si el [query_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-log) se especifica el parámetro server. Este parámetro establece las reglas de registro, como el intervalo de registro o el nombre de la tabla en la que se registrarán las consultas.
-
-Para habilitar el registro de consultas, [Log_queries](settings/settings.md#settings-log-queries) parámetro a 1. Para obtener más información, consulte el [Configuración](settings/settings.md) apartado.
-
-El `system.query_log` tabla registra dos tipos de consultas:
-
-1.  Consultas iniciales ejecutadas directamente por el cliente.
-2.  Consultas secundarias iniciadas por otras consultas (para la ejecución de consultas distribuidas). Para estos tipos de consultas, la información sobre las consultas principales se muestra en el `initial_*` columna.
-
-Columna:
-
--   `type` (`Enum8`) — Type of event that occurred when executing the query. Values:
-    -   `'QueryStart' = 1` — Successful start of query execution.
-    -   `'QueryFinish' = 2` — Successful end of query execution.
-    -   `'ExceptionBeforeStart' = 3` — Exception before the start of query execution.
-    -   `'ExceptionWhileProcessing' = 4` — Exception during the query execution.
--   `event_date` (Date) — Query starting date.
--   `event_time` (DateTime) — Query starting time.
--   `query_start_time` (DateTime) — Start time of query execution.
--   `query_duration_ms` (UInt64) — Duration of query execution.
--   `read_rows` (UInt64) — Number of read rows.
--   `read_bytes` (UInt64) — Number of read bytes.
--   `written_rows` (UInt64) — For `INSERT` consultas, el número de filas escritas. Para otras consultas, el valor de la columna es 0.
--   `written_bytes` (UInt64) — For `INSERT` consultas, el número de bytes escritos. Para otras consultas, el valor de la columna es 0.
--   `result_rows` (UInt64) — Number of rows in the result.
--   `result_bytes` (UInt64) — Number of bytes in the result.
--   `memory_usage` (UInt64) — Memory consumption by the query.
--   `query` (String) — Query string.
--   `exception` (String) — Exception message.
--   `stack_trace` (String) — Stack trace (a list of methods called before the error occurred). An empty string, if the query is completed successfully.
--   `is_initial_query` (UInt8) — Query type. Possible values:
-    -   1 — Query was initiated by the client.
-    -   0 — Query was initiated by another query for distributed query execution.
--   `user` (String) — Name of the user who initiated the current query.
--   `query_id` (String) — ID of the query.
--   `address` (IPv6) — IP address that was used to make the query.
--   `port` (UInt16) — The client port that was used to make the query.
--   `initial_user` (String) — Name of the user who ran the initial query (for distributed query execution).
--   `initial_query_id` (String) — ID of the initial query (for distributed query execution).
--   `initial_address` (IPv6) — IP address that the parent query was launched from.
--   `initial_port` (UInt16) — The client port that was used to make the parent query.
--   `interface` (UInt8) — Interface that the query was initiated from. Possible values:
-    -   1 — TCP.
-    -   2 — HTTP.
--   `os_user` (String) — OS's username who runs [Casa de clics-cliente](../interfaces/cli.md).
--   `client_hostname` (String) — Hostname of the client machine where the [Casa de clics-cliente](../interfaces/cli.md) o se ejecuta otro cliente TCP.
--   `client_name` (String) — The [Casa de clics-cliente](../interfaces/cli.md) o otro nombre de cliente TCP.
--   `client_revision` (UInt32) — Revision of the [Casa de clics-cliente](../interfaces/cli.md) o otro cliente TCP.
--   `client_version_major` (UInt32) — Major version of the [Casa de clics-cliente](../interfaces/cli.md) o otro cliente TCP.
--   `client_version_minor` (UInt32) — Minor version of the [Casa de clics-cliente](../interfaces/cli.md) o otro cliente TCP.
--   `client_version_patch` (UInt32) — Patch component of the [Casa de clics-cliente](../interfaces/cli.md) o otra versión de cliente TCP.
--   `http_method` (UInt8) — HTTP method that initiated the query. Possible values:
-    -   0 — The query was launched from the TCP interface.
-    -   1 — `GET` se utilizó el método.
-    -   2 — `POST` se utilizó el método.
--   `http_user_agent` (String) — The `UserAgent` encabezado pasado en la solicitud HTTP.
--   `quota_key` (String) — The “quota key” especificado en el [cuota](quotas.md) ajuste (ver `keyed`).
--   `revision` (UInt32) — ClickHouse revision.
--   `thread_numbers` (Array(UInt32)) — Number of threads that are participating in query execution.
--   `ProfileEvents.Names` (Array(String)) — Counters that measure different metrics. The description of them could be found in the table [sistema.evento](#system_tables-events)
--   `ProfileEvents.Values` (Array(UInt64)) — Values of metrics that are listed in the `ProfileEvents.Names` columna.
--   `Settings.Names` (Array(String)) — Names of settings that were changed when the client ran the query. To enable logging changes to settings, set the `log_query_settings` parámetro a 1.
--   `Settings.Values` (Array(String)) — Values of settings that are listed in the `Settings.Names` columna.
-
-Cada consulta crea una o dos filas en el `query_log` tabla, dependiendo del estado de la consulta:
-
-1.  Si la ejecución de la consulta se realiza correctamente, se crean dos eventos con los tipos 1 y 2 (consulte `type` columna).
-2.  Si se produjo un error durante el procesamiento de la consulta, se crean dos eventos con los tipos 1 y 4.
-3.  Si se produjo un error antes de iniciar la consulta, se crea un solo evento con el tipo 3.
-
-De forma predeterminada, los registros se agregan a la tabla a intervalos de 7,5 segundos. Puede establecer este intervalo en el [query_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-log) configuración del servidor (consulte el `flush_interval_milliseconds` parámetro). Para vaciar los registros a la fuerza desde el búfer de memoria a la tabla, utilice `SYSTEM FLUSH LOGS` consulta.
-
-Cuando la tabla se elimina manualmente, se creará automáticamente sobre la marcha. Tenga en cuenta que se eliminarán todos los registros anteriores.
-
-!!! note "Nota"
-    El período de almacenamiento para los registros es ilimitado. Los registros no se eliminan automáticamente de la tabla. Debe organizar la eliminación de registros obsoletos usted mismo.
-
-Puede especificar una clave de partición arbitraria `system.query_log` mesa en el [query_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-log) configuración del servidor (consulte el `partition_by` parámetro).
-
-## sistema.Sistema abierto {#system_tables-query-thread-log}
-
-La tabla contiene información sobre cada subproceso de ejecución de consultas.
-
-ClickHouse crea esta tabla sólo si el [Sistema abierto.](server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) se especifica el parámetro server. Este parámetro establece las reglas de registro, como el intervalo de registro o el nombre de la tabla en la que se registrarán las consultas.
-
-Para habilitar el registro de consultas, [Log_query_threads](settings/settings.md#settings-log-query-threads) parámetro a 1. Para obtener más información, consulte el [Configuración](settings/settings.md) apartado.
-
-Columna:
-
--   `event_date` (Date) — the date when the thread has finished execution of the query.
--   `event_time` (DateTime) — the date and time when the thread has finished execution of the query.
--   `query_start_time` (DateTime) — Start time of query execution.
--   `query_duration_ms` (UInt64) — Duration of query execution.
--   `read_rows` (UInt64) — Number of read rows.
--   `read_bytes` (UInt64) — Number of read bytes.
--   `written_rows` (UInt64) — For `INSERT` consultas, el número de filas escritas. Para otras consultas, el valor de la columna es 0.
--   `written_bytes` (UInt64) — For `INSERT` consultas, el número de bytes escritos. Para otras consultas, el valor de la columna es 0.
--   `memory_usage` (Int64) — The difference between the amount of allocated and freed memory in context of this thread.
--   `peak_memory_usage` (Int64) — The maximum difference between the amount of allocated and freed memory in context of this thread.
--   `thread_name` (String) — Name of the thread.
--   `thread_number` (UInt32) — Internal thread ID.
--   `os_thread_id` (Int32) — OS thread ID.
--   `master_thread_id` (UInt64) — OS initial ID of initial thread.
--   `query` (String) — Query string.
--   `is_initial_query` (UInt8) — Query type. Possible values:
-    -   1 — Query was initiated by the client.
-    -   0 — Query was initiated by another query for distributed query execution.
--   `user` (String) — Name of the user who initiated the current query.
--   `query_id` (String) — ID of the query.
--   `address` (IPv6) — IP address that was used to make the query.
--   `port` (UInt16) — The client port that was used to make the query.
--   `initial_user` (String) — Name of the user who ran the initial query (for distributed query execution).
--   `initial_query_id` (String) — ID of the initial query (for distributed query execution).
--   `initial_address` (IPv6) — IP address that the parent query was launched from.
--   `initial_port` (UInt16) — The client port that was used to make the parent query.
--   `interface` (UInt8) — Interface that the query was initiated from. Possible values:
-    -   1 — TCP.
-    -   2 — HTTP.
--   `os_user` (String) — OS's username who runs [Casa de clics-cliente](../interfaces/cli.md).
--   `client_hostname` (String) — Hostname of the client machine where the [Casa de clics-cliente](../interfaces/cli.md) o se ejecuta otro cliente TCP.
--   `client_name` (String) — The [Casa de clics-cliente](../interfaces/cli.md) o otro nombre de cliente TCP.
--   `client_revision` (UInt32) — Revision of the [Casa de clics-cliente](../interfaces/cli.md) o otro cliente TCP.
--   `client_version_major` (UInt32) — Major version of the [Casa de clics-cliente](../interfaces/cli.md) o otro cliente TCP.
--   `client_version_minor` (UInt32) — Minor version of the [Casa de clics-cliente](../interfaces/cli.md) o otro cliente TCP.
--   `client_version_patch` (UInt32) — Patch component of the [Casa de clics-cliente](../interfaces/cli.md) o otra versión de cliente TCP.
--   `http_method` (UInt8) — HTTP method that initiated the query. Possible values:
-    -   0 — The query was launched from the TCP interface.
-    -   1 — `GET` se utilizó el método.
-    -   2 — `POST` se utilizó el método.
--   `http_user_agent` (String) — The `UserAgent` encabezado pasado en la solicitud HTTP.
--   `quota_key` (String) — The “quota key” especificado en el [cuota](quotas.md) ajuste (ver `keyed`).
--   `revision` (UInt32) — ClickHouse revision.
--   `ProfileEvents.Names` (Array(String)) — Counters that measure different metrics for this thread. The description of them could be found in the table [sistema.evento](#system_tables-events)
--   `ProfileEvents.Values` (Array(UInt64)) — Values of metrics for this thread that are listed in the `ProfileEvents.Names` columna.
-
-De forma predeterminada, los registros se agregan a la tabla a intervalos de 7,5 segundos. Puede establecer este intervalo en el [Sistema abierto.](server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) configuración del servidor (consulte el `flush_interval_milliseconds` parámetro). Para vaciar los registros a la fuerza desde el búfer de memoria a la tabla, utilice `SYSTEM FLUSH LOGS` consulta.
-
-Cuando la tabla se elimina manualmente, se creará automáticamente sobre la marcha. Tenga en cuenta que se eliminarán todos los registros anteriores.
-
-!!! note "Nota"
-    El período de almacenamiento para los registros es ilimitado. Los registros no se eliminan automáticamente de la tabla. Debe organizar la eliminación de registros obsoletos usted mismo.
-
-Puede especificar una clave de partición arbitraria `system.query_thread_log` mesa en el [Sistema abierto.](server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) configuración del servidor (consulte el `partition_by` parámetro).
-
-## sistema.trace_log {#system_tables-trace_log}
-
-Contiene seguimientos de pila recopilados por el generador de perfiles de consultas de muestreo.
-
-ClickHouse crea esta tabla cuando el [trace_log](server-configuration-parameters/settings.md#server_configuration_parameters-trace_log) se establece la sección de configuración del servidor. También el [query_profiler_real_time_period_ns](settings/settings.md#query_profiler_real_time_period_ns) y [Los resultados de la prueba](settings/settings.md#query_profiler_cpu_time_period_ns) los ajustes deben establecerse.
-
-Para analizar los registros, utilice el `addressToLine`, `addressToSymbol` y `demangle` funciones de inspección.
-
-Columna:
-
--   `event_date` ([Fecha](../sql-reference/data-types/date.md)) — Date of sampling moment.
-
--   `event_time` ([FechaHora](../sql-reference/data-types/datetime.md)) — Timestamp of the sampling moment.
-
--   `timestamp_ns` ([UInt64](../sql-reference/data-types/int-uint.md)) — Timestamp of the sampling moment in nanoseconds.
-
--   `revision` ([UInt32](../sql-reference/data-types/int-uint.md)) — ClickHouse server build revision.
-
-    Cuando se conecta al servidor por `clickhouse-client`, ves la cadena similar a `Connected to ClickHouse server version 19.18.1 revision 54429.`. Este campo contiene el `revision`, pero no el `version` de un servidor.
-
--   `timer_type` ([Enum8](../sql-reference/data-types/enum.md)) — Timer type:
-
-    -   `Real` representa el tiempo del reloj de pared.
-    -   `CPU` representa el tiempo de CPU.
-
--   `thread_number` ([UInt32](../sql-reference/data-types/int-uint.md)) — Thread identifier.
-
--   `query_id` ([Cadena](../sql-reference/data-types/string.md)) — Query identifier that can be used to get details about a query that was running from the [query_log](#system_tables-query_log) tabla del sistema.
-
--   `trace` ([Matriz (UInt64)](../sql-reference/data-types/array.md)) — Stack trace at the moment of sampling. Each element is a virtual memory address inside ClickHouse server process.
-
-**Ejemplo**
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:    2019-11-15
-event_time:    2019-11-15 15:09:38
-revision:      54428
-timer_type:    Real
-thread_number: 48
-query_id:      acc4d61f-5bd1-4a3e-bc91-2180be37c915
-trace:         [94222141367858,94222152240175,94222152325351,94222152329944,94222152330796,94222151449980,94222144088167,94222151682763,94222144088167,94222151682763,94222144088167,94222144058283,94222144059248,94222091840750,94222091842302,94222091831228,94222189631488,140509950166747,140509942945935]
-```
-
-## sistema.Replica {#system_tables-replicas}
-
-Contiene información y estado de las tablas replicadas que residen en el servidor local.
-Esta tabla se puede utilizar para el monitoreo. La tabla contiene una fila para cada tabla Replicated\*.
-
-Ejemplo:
-
-``` sql
-SELECT *
-FROM system.replicas
-WHERE table = 'visits'
-FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-database:                   merge
-table:                      visits
-engine:                     ReplicatedCollapsingMergeTree
-is_leader:                  1
-can_become_leader:          1
-is_readonly:                0
-is_session_expired:         0
-future_parts:               1
-parts_to_check:             0
-zookeeper_path:             /clickhouse/tables/01-06/visits
-replica_name:               example01-06-1.yandex.ru
-replica_path:               /clickhouse/tables/01-06/visits/replicas/example01-06-1.yandex.ru
-columns_version:            9
-queue_size:                 1
-inserts_in_queue:           0
-merges_in_queue:            1
-part_mutations_in_queue:    0
-queue_oldest_time:          2020-02-20 08:34:30
-inserts_oldest_time:        1970-01-01 00:00:00
-merges_oldest_time:         2020-02-20 08:34:30
-part_mutations_oldest_time: 1970-01-01 00:00:00
-oldest_part_to_get:
-oldest_part_to_merge_to:    20200220_20284_20840_7
-oldest_part_to_mutate_to:
-log_max_index:              596273
-log_pointer:                596274
-last_queue_update:          2020-02-20 08:34:32
-absolute_delay:             0
-total_replicas:             2
-active_replicas:            2
-```
-
-Columna:
-
--   `database` (`String`) - Nombre de la base de datos
--   `table` (`String`) - Nombre de la tabla
--   `engine` (`String`) - Nombre del motor de tabla
--   `is_leader` (`UInt8`) - Si la réplica es la líder.
-    Sólo una réplica a la vez puede ser el líder. El líder es responsable de seleccionar las fusiones de fondo para realizar.
-    Tenga en cuenta que las escrituras se pueden realizar en cualquier réplica que esté disponible y tenga una sesión en ZK, independientemente de si es un líder.
--   `can_become_leader` (`UInt8`) - Si la réplica puede ser elegida como líder.
--   `is_readonly` (`UInt8`) - Si la réplica está en modo de sólo lectura.
-    Este modo se activa si la configuración no tiene secciones con ZooKeeper, si se produce un error desconocido al reinicializar sesiones en ZooKeeper y durante la reinicialización de sesiones en ZooKeeper.
--   `is_session_expired` (`UInt8`) - la sesión con ZooKeeper ha expirado. Básicamente lo mismo que `is_readonly`.
--   `future_parts` (`UInt32`) - El número de partes de datos que aparecerán como resultado de INSERTs o fusiones que aún no se han realizado.
--   `parts_to_check` (`UInt32`) - El número de partes de datos en la cola para la verificación. Una pieza se coloca en la cola de verificación si existe la sospecha de que podría estar dañada.
--   `zookeeper_path` (`String`) - Ruta de acceso a los datos de la tabla en ZooKeeper.
--   `replica_name` (`String`) - Nombre de réplica en ZooKeeper. Diferentes réplicas de la misma tabla tienen diferentes nombres.
--   `replica_path` (`String`) - Ruta de acceso a los datos de réplica en ZooKeeper. Lo mismo que concatenar ‘zookeeper_path/replicas/replica_path’.
--   `columns_version` (`Int32`) - Número de versión de la estructura de la tabla. Indica cuántas veces se realizó ALTER. Si las réplicas tienen versiones diferentes, significa que algunas réplicas aún no han hecho todas las ALTER.
--   `queue_size` (`UInt32`) - Tamaño de la cola para las operaciones en espera de ser realizadas. Las operaciones incluyen insertar bloques de datos, fusiones y otras acciones. Por lo general, coincide con `future_parts`.
--   `inserts_in_queue` (`UInt32`) - Número de inserciones de bloques de datos que deben realizarse. Las inserciones generalmente se replican con bastante rapidez. Si este número es grande, significa que algo anda mal.
--   `merges_in_queue` (`UInt32`) - El número de fusiones en espera de hacerse. A veces las fusiones son largas, por lo que este valor puede ser mayor que cero durante mucho tiempo.
--   `part_mutations_in_queue` (`UInt32`) - El número de mutaciones a la espera de hacerse.
--   `queue_oldest_time` (`DateTime`) - Si `queue_size` mayor que 0, muestra cuándo se agregó la operación más antigua a la cola.
--   `inserts_oldest_time` (`DateTime`) - Ver `queue_oldest_time`
--   `merges_oldest_time` (`DateTime`) - Ver `queue_oldest_time`
--   `part_mutations_oldest_time` (`DateTime`) - Ver `queue_oldest_time`
-
-Las siguientes 4 columnas tienen un valor distinto de cero solo cuando hay una sesión activa con ZK.
-
--   `log_max_index` (`UInt64`) - Número máximo de inscripción en el registro de actividad general.
--   `log_pointer` (`UInt64`) - Número máximo de entrada en el registro de actividad general que la réplica copió en su cola de ejecución, más uno. Si `log_pointer` es mucho más pequeño que `log_max_index`, algo está mal.
--   `last_queue_update` (`DateTime`) - Cuando la cola se actualizó la última vez.
--   `absolute_delay` (`UInt64`) - ¿Qué tan grande retraso en segundos tiene la réplica actual.
--   `total_replicas` (`UInt8`) - El número total de réplicas conocidas de esta tabla.
--   `active_replicas` (`UInt8`) - El número de réplicas de esta tabla que tienen una sesión en ZooKeeper (es decir, el número de réplicas en funcionamiento).
-
-Si solicita todas las columnas, la tabla puede funcionar un poco lentamente, ya que se realizan varias lecturas de ZooKeeper para cada fila.
-Si no solicita las últimas 4 columnas (log_max_index, log_pointer, total_replicas, active_replicas), la tabla funciona rápidamente.
-
-Por ejemplo, puede verificar que todo funcione correctamente de esta manera:
-
-``` sql
-SELECT
-    database,
-    table,
-    is_leader,
-    is_readonly,
-    is_session_expired,
-    future_parts,
-    parts_to_check,
-    columns_version,
-    queue_size,
-    inserts_in_queue,
-    merges_in_queue,
-    log_max_index,
-    log_pointer,
-    total_replicas,
-    active_replicas
-FROM system.replicas
-WHERE
-       is_readonly
-    OR is_session_expired
-    OR future_parts > 20
-    OR parts_to_check > 10
-    OR queue_size > 20
-    OR inserts_in_queue > 10
-    OR log_max_index - log_pointer > 10
-    OR total_replicas < 2
-    OR active_replicas < total_replicas
-```
-
-Si esta consulta no devuelve nada, significa que todo está bien.
-
-## sistema.configuración {#system-tables-system-settings}
-
-Contiene información sobre la configuración de sesión para el usuario actual.
-
-Columna:
-
--   `name` ([Cadena](../sql-reference/data-types/string.md)) — Setting name.
--   `value` ([Cadena](../sql-reference/data-types/string.md)) — Setting value.
--   `changed` ([UInt8](../sql-reference/data-types/int-uint.md#uint-ranges)) — Shows whether a setting is changed from its default value.
--   `description` ([Cadena](../sql-reference/data-types/string.md)) — Short setting description.
--   `min` ([NULL](../sql-reference/data-types/nullable.md)([Cadena](../sql-reference/data-types/string.md))) — Minimum value of the setting, if any is set via [limitación](settings/constraints-on-settings.md#constraints-on-settings). Si la configuración no tiene ningún valor mínimo, contiene [NULL](../sql-reference/syntax.md#null-literal).
--   `max` ([NULL](../sql-reference/data-types/nullable.md)([Cadena](../sql-reference/data-types/string.md))) — Maximum value of the setting, if any is set via [limitación](settings/constraints-on-settings.md#constraints-on-settings). Si la configuración no tiene ningún valor máximo, contiene [NULL](../sql-reference/syntax.md#null-literal).
--   `readonly` ([UInt8](../sql-reference/data-types/int-uint.md#uint-ranges)) — Shows whether the current user can change the setting:
-    -   `0` — Current user can change the setting.
-    -   `1` — Current user can't change the setting.
-
-**Ejemplo**
-
-En el ejemplo siguiente se muestra cómo obtener información sobre la configuración cuyo nombre contiene `min_i`.
-
-``` sql
-SELECT *
-FROM system.settings
-WHERE name LIKE '%min_i%'
-```
-
-``` text
-┌─name────────────────────────────────────────┬─value─────┬─changed─┬─description───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┐
-│ min_insert_block_size_rows                  │ 1048576   │       0 │ Squash blocks passed to INSERT query to specified size in rows, if blocks are not big enough.                                                                         │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
-│ min_insert_block_size_bytes                 │ 268435456 │       0 │ Squash blocks passed to INSERT query to specified size in bytes, if blocks are not big enough.                                                                        │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
-│ read_backoff_min_interval_between_events_ms │ 1000      │       0 │ Settings to reduce the number of threads in case of slow reads. Do not pay attention to the event, if the previous one has passed less than a certain amount of time. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
-└─────────────────────────────────────────────┴───────────┴─────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┘
-```
-
-Uso de `WHERE changed` puede ser útil, por ejemplo, cuando se desea comprobar:
-
--   Si los ajustes de los archivos de configuración se cargan correctamente y están en uso.
--   Configuración que cambió en la sesión actual.
-
-<!-- -->
-
-``` sql
-SELECT * FROM system.settings WHERE changed AND name='load_balancing'
-```
-
-**Ver también**
-
--   [Configuración](settings/index.md#session-settings-intro)
--   [Permisos para consultas](settings/permissions-for-queries.md#settings_readonly)
--   [Restricciones en la configuración](settings/constraints-on-settings.md)
-
-## sistema.table_engines {#system.table_engines}
-
-``` text
-┌─name───────────────────┬─value───────┐
-│ max_threads            │ 8           │
-│ use_uncompressed_cache │ 0           │
-│ load_balancing         │ random      │
-│ max_memory_usage       │ 10000000000 │
-└────────────────────────┴─────────────┘
-```
-
-## sistema.merge_tree_settings {#system-merge_tree_settings}
-
-Contiene información sobre la configuración `MergeTree` tabla.
-
-Columna:
-
--   `name` (String) — Setting name.
--   `value` (String) — Setting value.
--   `description` (String) — Setting description.
--   `type` (String) — Setting type (implementation specific string value).
--   `changed` (UInt8) — Whether the setting was explicitly defined in the config or explicitly changed.
-
-## sistema.table_engines {#system-table-engines}
-
-Contiene la descripción de los motores de tablas admitidos por el servidor y su información de soporte de características.
-
-Esta tabla contiene las siguientes columnas (el tipo de columna se muestra entre corchetes):
-
--   `name` (String) — The name of table engine.
--   `supports_settings` (UInt8) — Flag that indicates if table engine supports `SETTINGS` clausula.
--   `supports_skipping_indices` (UInt8) — Flag that indicates if table engine supports [Índices de saltos](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-data_skipping-indexes).
--   `supports_ttl` (UInt8) — Flag that indicates if table engine supports [TTL](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-ttl).
--   `supports_sort_order` (UInt8) — Flag that indicates if table engine supports clauses `PARTITION_BY`, `PRIMARY_KEY`, `ORDER_BY` y `SAMPLE_BY`.
--   `supports_replication` (UInt8) — Flag that indicates if table engine supports [Replicación de datos](../engines/table-engines/mergetree-family/replication.md).
--   `supports_duduplication` (UInt8) — Flag that indicates if table engine supports data deduplication.
-
-Ejemplo:
-
-``` sql
-SELECT *
-FROM system.table_engines
-WHERE name in ('Kafka', 'MergeTree', 'ReplicatedCollapsingMergeTree')
-```
-
-``` text
-┌─name──────────────────────────┬─supports_settings─┬─supports_skipping_indices─┬─supports_sort_order─┬─supports_ttl─┬─supports_replication─┬─supports_deduplication─┐
-│ Kafka                         │                 1 │                         0 │                   0 │            0 │                    0 │                      0 │
-│ MergeTree                     │                 1 │                         1 │                   1 │            1 │                    0 │                      0 │
-│ ReplicatedCollapsingMergeTree │                 1 │                         1 │                   1 │            1 │                    1 │                      1 │
-└───────────────────────────────┴───────────────────┴───────────────────────────┴─────────────────────┴──────────────┴──────────────────────┴────────────────────────┘
-```
-
-**Ver también**
-
--   Familia MergeTree [cláusulas de consulta](../engines/table-engines/mergetree-family/mergetree.md#mergetree-query-clauses)
--   Kafka [configuración](../engines/table-engines/integrations/kafka.md#table_engine-kafka-creating-a-table)
--   Unir [configuración](../engines/table-engines/special/join.md#join-limitations-and-settings)
-
-## sistema.tabla {#system-tables}
-
-Contiene metadatos de cada tabla que el servidor conoce. Las tablas separadas no se muestran en `system.tables`.
-
-Esta tabla contiene las siguientes columnas (el tipo de columna se muestra entre corchetes):
-
--   `database` (String) — The name of the database the table is in.
-
--   `name` (String) — Table name.
-
--   `engine` (String) — Table engine name (without parameters).
-
--   `is_temporary` (UInt8): marca que indica si la tabla es temporal.
-
--   `data_path` (String) - Ruta de acceso a los datos de la tabla en el sistema de archivos.
-
--   `metadata_path` (String) - Ruta de acceso a los metadatos de la tabla en el sistema de archivos.
-
--   `metadata_modification_time` (DateTime) - Hora de la última modificación de los metadatos de la tabla.
-
--   `dependencies_database` (Array(String)) - Dependencias de base de datos.
-
--   `dependencies_table` (Array(String)) - Dependencias de tabla ([Método de codificación de datos:](../engines/table-engines/special/materializedview.md) tablas basadas en la tabla actual).
-
--   `create_table_query` (String) - La consulta que se utilizó para crear la tabla.
-
--   `engine_full` (String) - Parámetros del motor de tabla.
-
--   `partition_key` (String) - La expresión de clave de partición especificada en la tabla.
-
--   `sorting_key` (String) - La expresión de clave de ordenación especificada en la tabla.
-
--   `primary_key` (String) - La expresión de clave principal especificada en la tabla.
-
--   `sampling_key` (String) - La expresión de clave de muestreo especificada en la tabla.
-
--   `storage_policy` (String) - La política de almacenamiento:
-
-    -   [Método de codificación de datos:](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes)
-    -   [Distribuido](../engines/table-engines/special/distributed.md#distributed)
-
--   `total_rows` (Nullable(UInt64)) - Número total de filas, si es posible determinar rápidamente el número exacto de filas en la tabla, de lo contrario `Null` (incluyendo underying `Buffer` tabla).
-
--   `total_bytes` (Nullable(UInt64)) - Número total de bytes, si es posible determinar rápidamente el número exacto de bytes para la tabla en el almacenamiento, de lo contrario `Null` (**no** incluye cualquier almacenamiento subyacente).
-
-    -   If the table stores data on disk, returns used space on disk (i.e. compressed).
-    -   Si la tabla almacena datos en la memoria, devuelve el número aproximado de bytes utilizados en la memoria.
-
-El `system.tables` se utiliza en `SHOW TABLES` implementación de consultas.
-
-## sistema.Zookeeper {#system-zookeeper}
-
-La tabla no existe si ZooKeeper no está configurado. Permite leer datos del clúster ZooKeeper definido en la configuración.
-La consulta debe tener un ‘path’ condición de igualdad en la cláusula WHERE. Este es el camino en ZooKeeper para los niños para los que desea obtener datos.
-
-Consulta `SELECT * FROM system.zookeeper WHERE path = '/clickhouse'` salidas de datos para todos los niños en el `/clickhouse` nodo.
-Para generar datos para todos los nodos raíz, escriba path = ‘/’.
-Si la ruta especificada en ‘path’ no existe, se lanzará una excepción.
-
-Columna:
-
--   `name` (String) — The name of the node.
--   `path` (String) — The path to the node.
--   `value` (String) — Node value.
--   `dataLength` (Int32) — Size of the value.
--   `numChildren` (Int32) — Number of descendants.
--   `czxid` (Int64) — ID of the transaction that created the node.
--   `mzxid` (Int64) — ID of the transaction that last changed the node.
--   `pzxid` (Int64) — ID of the transaction that last deleted or added descendants.
--   `ctime` (DateTime) — Time of node creation.
--   `mtime` (DateTime) — Time of the last modification of the node.
--   `version` (Int32) — Node version: the number of times the node was changed.
--   `cversion` (Int32) — Number of added or removed descendants.
--   `aversion` (Int32) — Number of changes to the ACL.
--   `ephemeralOwner` (Int64) — For ephemeral nodes, the ID of the session that owns this node.
-
-Ejemplo:
-
-``` sql
-SELECT *
-FROM system.zookeeper
-WHERE path = '/clickhouse/tables/01-08/visits/replicas'
-FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-name:           example01-08-1.yandex.ru
-value:
-czxid:          932998691229
-mzxid:          932998691229
-ctime:          2015-03-27 16:49:51
-mtime:          2015-03-27 16:49:51
-version:        0
-cversion:       47
-aversion:       0
-ephemeralOwner: 0
-dataLength:     0
-numChildren:    7
-pzxid:          987021031383
-path:           /clickhouse/tables/01-08/visits/replicas
-
-Row 2:
-──────
-name:           example01-08-2.yandex.ru
-value:
-czxid:          933002738135
-mzxid:          933002738135
-ctime:          2015-03-27 16:57:01
-mtime:          2015-03-27 16:57:01
-version:        0
-cversion:       37
-aversion:       0
-ephemeralOwner: 0
-dataLength:     0
-numChildren:    7
-pzxid:          987021252247
-path:           /clickhouse/tables/01-08/visits/replicas
-```
-
-## sistema.mutación {#system_tables-mutations}
-
-La tabla contiene información sobre [mutación](../sql-reference/statements/alter.md#alter-mutations) de las tablas MergeTree y su progreso. Cada comando de mutación está representado por una sola fila. La tabla tiene las siguientes columnas:
-
-**base**, **tabla** - El nombre de la base de datos y la tabla a la que se aplicó la mutación.
-
-**mutation_id** - La identificación de la mutación. Para las tablas replicadas, estos identificadores corresponden a los nombres de znode `<table_path_in_zookeeper>/mutations/` directorio en ZooKeeper. Para las tablas no duplicadas, los ID corresponden a los nombres de archivo en el directorio de datos de la tabla.
-
-**comando** - La cadena de comandos de mutación (la parte de la consulta después de `ALTER TABLE [db.]table`).
-
-**create_time** - Cuando este comando de mutación fue enviado para su ejecución.
-
-**block_numbers.partition_id**, **block_numbers.numero** - Una columna anidada. Para las mutaciones de tablas replicadas, contiene un registro para cada partición: el ID de partición y el número de bloque que fue adquirido por la mutación (en cada partición, solo se mutarán las partes que contienen bloques con números menores que el número de bloque adquirido por la mutación en esa partición). En tablas no replicadas, los números de bloque en todas las particiones forman una sola secuencia. Esto significa que para las mutaciones de tablas no replicadas, la columna contendrá un registro con un solo número de bloque adquirido por la mutación.
-
-**partes_a_do** - El número de partes de datos que deben mutarse para que finalice la mutación.
-
-**is_done** - Es la mutación hecho? Tenga en cuenta que incluso si `parts_to_do = 0` es posible que aún no se haya realizado una mutación de una tabla replicada debido a un INSERT de larga ejecución que creará una nueva parte de datos que deberá mutarse.
-
-Si hubo problemas con la mutación de algunas partes, las siguientes columnas contienen información adicional:
-
-**Método de codificación de datos:** - El nombre de la parte más reciente que no se pudo mutar.
-
-**Método de codificación de datos:** - El momento del fracaso de la mutación de la parte más reciente.
-
-**Método de codificación de datos:** - El mensaje de excepción que causó el error de mutación de parte más reciente.
-
-## sistema.disco {#system_tables-disks}
-
-Contiene información sobre los discos definidos en el [configuración del servidor](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes_configure).
-
-Columna:
-
--   `name` ([Cadena](../sql-reference/data-types/string.md)) — Name of a disk in the server configuration.
--   `path` ([Cadena](../sql-reference/data-types/string.md)) — Path to the mount point in the file system.
--   `free_space` ([UInt64](../sql-reference/data-types/int-uint.md)) — Free space on disk in bytes.
--   `total_space` ([UInt64](../sql-reference/data-types/int-uint.md)) — Disk volume in bytes.
--   `keep_free_space` ([UInt64](../sql-reference/data-types/int-uint.md)) — Amount of disk space that should stay free on disk in bytes. Defined in the `keep_free_space_bytes` parámetro de configuración del disco.
-
-## sistema.almacenamiento_policies {#system_tables-storage_policies}
-
-Contiene información sobre las directivas de almacenamiento y los volúmenes [configuración del servidor](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes_configure).
-
-Columna:
-
--   `policy_name` ([Cadena](../sql-reference/data-types/string.md)) — Name of the storage policy.
--   `volume_name` ([Cadena](../sql-reference/data-types/string.md)) — Volume name defined in the storage policy.
--   `volume_priority` ([UInt64](../sql-reference/data-types/int-uint.md)) — Volume order number in the configuration.
--   `disks` ([Array(Cadena)](../sql-reference/data-types/array.md)) — Disk names, defined in the storage policy.
--   `max_data_part_size` ([UInt64](../sql-reference/data-types/int-uint.md)) — Maximum size of a data part that can be stored on volume disks (0 — no limit).
--   `move_factor` ([Float64](../sql-reference/data-types/float.md)) — Ratio of free disk space. When the ratio exceeds the value of configuration parameter, ClickHouse start to move data to the next volume in order.
-
-Si la directiva de almacenamiento contiene más de un volumen, la información de cada volumen se almacena en la fila individual de la tabla.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/system_tables/) <!--hide-->
diff --git a/docs/es/operations/tips.md b/docs/es/operations/tips.md
deleted file mode 100644
index deb226450aa0..000000000000
--- a/docs/es/operations/tips.md
+++ /dev/null
@@ -1,251 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: Recomendaciones de uso
----
-
-# Recomendaciones de uso {#usage-recommendations}
-
-## CPU Scaling Governor {#cpu-scaling-governor}
-
-Utilice siempre el `performance` gobernador de escala. El `on-demand` regulador de escala funciona mucho peor con una demanda constante.
-
-``` bash
-$ echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
-```
-
-## Limitaciones de la CPU {#cpu-limitations}
-
-Los procesadores pueden sobrecalentarse. Utilizar `dmesg` para ver si la velocidad de reloj de la CPU era limitada debido al sobrecalentamiento.
-La restricción también se puede establecer externamente en el nivel del centro de datos. Usted puede utilizar `turbostat` para controlarlo bajo una carga.
-
-## RAM {#ram}
-
-Para pequeñas cantidades de datos (hasta ~200 GB comprimidos), es mejor usar tanta memoria como el volumen de datos.
-Para grandes cantidades de datos y al procesar consultas interactivas (en línea), debe usar una cantidad razonable de RAM (128 GB o más) para que el subconjunto de datos en caliente quepa en la memoria caché de páginas.
-Incluso para volúmenes de datos de ~ 50 TB por servidor, el uso de 128 GB de RAM mejora significativamente el rendimiento de las consultas en comparación con 64 GB.
-
-No deshabilite el sobrecompromiso. Valor `cat /proc/sys/vm/overcommit_memory` debe ser 0 o 1. Ejecutar
-
-``` bash
-$ echo 0 | sudo tee /proc/sys/vm/overcommit_memory
-```
-
-## Páginas enormes {#huge-pages}
-
-Siempre deshabilite las páginas enormes transparentes. Interfiere con los asignadores de memoria, lo que conduce a una degradación significativa del rendimiento.
-
-``` bash
-$ echo 'never' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled
-```
-
-Utilizar `perf top` para ver el tiempo pasado en el kernel para la administración de memoria.
-Las páginas enormes permanentes tampoco necesitan ser asignadas.
-
-## Subsistema de almacenamiento {#storage-subsystem}
-
-Si su presupuesto le permite usar SSD, use SSD.
-Si no, use HDD. Los discos duros SATA 7200 RPM servirán.
-
-Dar preferencia a una gran cantidad de servidores con discos duros locales sobre un número menor de servidores con estantes de discos conectados.
-Pero para almacenar archivos con consultas raras, los estantes funcionarán.
-
-## RAID {#raid}
-
-Al usar HDD, puede combinar su RAID-10, RAID-5, RAID-6 o RAID-50.
-Para Linux, el software RAID es mejor (con `mdadm`). No recomendamos usar LVM.
-Al crear RAID-10, seleccione el `far` diseño.
-Si su presupuesto lo permite, elija RAID-10.
-
-Si tiene más de 4 discos, utilice RAID-6 (preferido) o RAID-50, en lugar de RAID-5.
-Cuando use RAID-5, RAID-6 o RAID-50, siempre aumente stripe_cache_size, ya que el valor predeterminado generalmente no es la mejor opción.
-
-``` bash
-$ echo 4096 | sudo tee /sys/block/md2/md/stripe_cache_size
-```
-
-Calcule el número exacto a partir del número de dispositivos y el tamaño del bloque, utilizando la fórmula: `2 * num_devices * chunk_size_in_bytes / 4096`.
-
-Un tamaño de bloque de 1024 KB es suficiente para todas las configuraciones RAID.
-Nunca ajuste el tamaño del bloque demasiado pequeño o demasiado grande.
-
-Puede usar RAID-0 en SSD.
-Independientemente del uso de RAID, utilice siempre la replicación para la seguridad de los datos.
-
-Habilite NCQ con una cola larga. Para HDD, elija el programador CFQ, y para SSD, elija noop. No reduzca el ‘readahead’ configuración.
-Para HDD, habilite la memoria caché de escritura.
-
-## Sistema de archivos {#file-system}
-
-Ext4 es la opción más confiable. Establecer las opciones de montaje `noatime, nobarrier`.
-XFS también es adecuado, pero no ha sido probado tan a fondo con ClickHouse.
-La mayoría de los otros sistemas de archivos también deberían funcionar bien. Los sistemas de archivos con asignación retrasada funcionan mejor.
-
-## Núcleo de Linux {#linux-kernel}
-
-No use un kernel de Linux obsoleto.
-
-## Red {#network}
-
-Si está utilizando IPv6, aumente el tamaño de la caché de ruta.
-El kernel de Linux anterior a 3.2 tenía una multitud de problemas con la implementación de IPv6.
-
-Utilice al menos una red de 10 GB, si es posible. 1 Gb también funcionará, pero será mucho peor para parchear réplicas con decenas de terabytes de datos, o para procesar consultas distribuidas con una gran cantidad de datos intermedios.
-
-## ZooKeeper {#zookeeper}
-
-Probablemente ya esté utilizando ZooKeeper para otros fines. Puede usar la misma instalación de ZooKeeper, si aún no está sobrecargada.
-
-It's best to use a fresh version of ZooKeeper – 3.4.9 or later. The version in stable Linux distributions may be outdated.
-
-Nunca debe usar scripts escritos manualmente para transferir datos entre diferentes clústeres de ZooKeeper, ya que el resultado será incorrecto para los nodos secuenciales. Nunca utilice el “zkcopy” utilidad por la misma razón: https://github.com/ksprojects/zkcopy/issues/15
-
-Si desea dividir un clúster ZooKeeper existente en dos, la forma correcta es aumentar el número de sus réplicas y, a continuación, volver a configurarlo como dos clústeres independientes.
-
-No ejecute ZooKeeper en los mismos servidores que ClickHouse. Porque ZooKeeper es muy sensible a la latencia y ClickHouse puede utilizar todos los recursos del sistema disponibles.
-
-Con la configuración predeterminada, ZooKeeper es una bomba de tiempo:
-
-> El servidor ZooKeeper no eliminará archivos de instantáneas y registros antiguos cuando utilice la configuración predeterminada (consulte autopurge), y esto es responsabilidad del operador.
-
-Esta bomba debe ser desactivada.
-
-La configuración ZooKeeper (3.5.1) a continuación se usa en Yandex.Entorno de producción de Métrica al 20 de mayo de 2017:
-
-zoológico.Cómo:
-
-``` bash
-# http://hadoop.apache.org/zookeeper/docs/current/zookeeperAdmin.html
-
-# The number of milliseconds of each tick
-tickTime=2000
-# The number of ticks that the initial
-# synchronization phase can take
-initLimit=30000
-# The number of ticks that can pass between
-# sending a request and getting an acknowledgement
-syncLimit=10
-
-maxClientCnxns=2000
-
-maxSessionTimeout=60000000
-# the directory where the snapshot is stored.
-dataDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/data
-# Place the dataLogDir to a separate physical disc for better performance
-dataLogDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/logs
-
-autopurge.snapRetainCount=10
-autopurge.purgeInterval=1
-
-
-# To avoid seeks ZooKeeper allocates space in the transaction log file in
-# blocks of preAllocSize kilobytes. The default block size is 64M. One reason
-# for changing the size of the blocks is to reduce the block size if snapshots
-# are taken more often. (Also, see snapCount).
-preAllocSize=131072
-
-# Clients can submit requests faster than ZooKeeper can process them,
-# especially if there are a lot of clients. To prevent ZooKeeper from running
-# out of memory due to queued requests, ZooKeeper will throttle clients so that
-# there is no more than globalOutstandingLimit outstanding requests in the
-# system. The default limit is 1,000.ZooKeeper logs transactions to a
-# transaction log. After snapCount transactions are written to a log file a
-# snapshot is started and a new transaction log file is started. The default
-# snapCount is 10,000.
-snapCount=3000000
-
-# If this option is defined, requests will be will logged to a trace file named
-# traceFile.year.month.day.
-#traceFile=
-
-# Leader accepts client connections. Default value is "yes". The leader machine
-# coordinates updates. For higher update throughput at thes slight expense of
-# read throughput the leader can be configured to not accept clients and focus
-# on coordination.
-leaderServes=yes
-
-standaloneEnabled=false
-dynamicConfigFile=/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/zoo.cfg.dynamic
-```
-
-Versión Java:
-
-``` text
-Java(TM) SE Runtime Environment (build 1.8.0_25-b17)
-Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
-```
-
-Parámetros de JVM:
-
-``` bash
-NAME=zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}
-ZOOCFGDIR=/etc/$NAME/conf
-
-# TODO this is really ugly
-# How to find out, which jars are needed?
-# seems, that log4j requires the log4j.properties file to be in the classpath
-CLASSPATH="$ZOOCFGDIR:/usr/build/classes:/usr/build/lib/*.jar:/usr/share/zookeeper/zookeeper-3.5.1-metrika.jar:/usr/share/zookeeper/slf4j-log4j12-1.7.5.jar:/usr/share/zookeeper/slf4j-api-1.7.5.jar:/usr/share/zookeeper/servlet-api-2.5-20081211.jar:/usr/share/zookeeper/netty-3.7.0.Final.jar:/usr/share/zookeeper/log4j-1.2.16.jar:/usr/share/zookeeper/jline-2.11.jar:/usr/share/zookeeper/jetty-util-6.1.26.jar:/usr/share/zookeeper/jetty-6.1.26.jar:/usr/share/zookeeper/javacc.jar:/usr/share/zookeeper/jackson-mapper-asl-1.9.11.jar:/usr/share/zookeeper/jackson-core-asl-1.9.11.jar:/usr/share/zookeeper/commons-cli-1.2.jar:/usr/src/java/lib/*.jar:/usr/etc/zookeeper"
-
-ZOOCFG="$ZOOCFGDIR/zoo.cfg"
-ZOO_LOG_DIR=/var/log/$NAME
-USER=zookeeper
-GROUP=zookeeper
-PIDDIR=/var/run/$NAME
-PIDFILE=$PIDDIR/$NAME.pid
-SCRIPTNAME=/etc/init.d/$NAME
-JAVA=/usr/bin/java
-ZOOMAIN="org.apache.zookeeper.server.quorum.QuorumPeerMain"
-ZOO_LOG4J_PROP="INFO,ROLLINGFILE"
-JMXLOCALONLY=false
-JAVA_OPTS="-Xms{{ '{{' }} cluster.get('xms','128M') {{ '}}' }} \
-    -Xmx{{ '{{' }} cluster.get('xmx','1G') {{ '}}' }} \
-    -Xloggc:/var/log/$NAME/zookeeper-gc.log \
-    -XX:+UseGCLogFileRotation \
-    -XX:NumberOfGCLogFiles=16 \
-    -XX:GCLogFileSize=16M \
-    -verbose:gc \
-    -XX:+PrintGCTimeStamps \
-    -XX:+PrintGCDateStamps \
-    -XX:+PrintGCDetails
-    -XX:+PrintTenuringDistribution \
-    -XX:+PrintGCApplicationStoppedTime \
-    -XX:+PrintGCApplicationConcurrentTime \
-    -XX:+PrintSafepointStatistics \
-    -XX:+UseParNewGC \
-    -XX:+UseConcMarkSweepGC \
--XX:+CMSParallelRemarkEnabled"
-```
-
-Sal init:
-
-``` text
-description "zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} centralized coordination service"
-
-start on runlevel [2345]
-stop on runlevel [!2345]
-
-respawn
-
-limit nofile 8192 8192
-
-pre-start script
-    [ -r "/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment" ] || exit 0
-    . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment
-    [ -d $ZOO_LOG_DIR ] || mkdir -p $ZOO_LOG_DIR
-    chown $USER:$GROUP $ZOO_LOG_DIR
-end script
-
-script
-    . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment
-    [ -r /etc/default/zookeeper ] && . /etc/default/zookeeper
-    if [ -z "$JMXDISABLE" ]; then
-        JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=$JMXLOCALONLY"
-    fi
-    exec start-stop-daemon --start -c $USER --exec $JAVA --name zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} \
-        -- -cp $CLASSPATH $JAVA_OPTS -Dzookeeper.log.dir=${ZOO_LOG_DIR} \
-        -Dzookeeper.root.logger=${ZOO_LOG4J_PROP} $ZOOMAIN $ZOOCFG
-end script
-```
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/operations/tips/) ##}
diff --git a/docs/es/operations/troubleshooting.md b/docs/es/operations/troubleshooting.md
deleted file mode 100644
index 9e8d2caca59a..000000000000
--- a/docs/es/operations/troubleshooting.md
+++ /dev/null
@@ -1,146 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: "Soluci\xF3n de problemas"
----
-
-# Solución de problemas {#troubleshooting}
-
--   [Instalación](#troubleshooting-installation-errors)
--   [Conexión al servidor](#troubleshooting-accepts-no-connections)
--   [Procesamiento de consultas](#troubleshooting-does-not-process-queries)
--   [Eficiencia del procesamiento de consultas](#troubleshooting-too-slow)
-
-## Instalación {#troubleshooting-installation-errors}
-
-### No puede obtener paquetes Deb del repositorio ClickHouse con Apt-get {#you-cannot-get-deb-packages-from-clickhouse-repository-with-apt-get}
-
--   Compruebe la configuración del firewall.
--   Si no puede acceder al repositorio por cualquier motivo, descargue los paquetes como se describe en el [Primeros pasos](../getting-started/index.md) artículo e instálelos manualmente usando el `sudo dpkg -i <packages>` comando. También necesitará el `tzdata` paquete.
-
-## Conexión al servidor {#troubleshooting-accepts-no-connections}
-
-Posibles problemas:
-
--   El servidor no se está ejecutando.
--   Parámetros de configuración inesperados o incorrectos.
-
-### El servidor no se está ejecutando {#server-is-not-running}
-
-**Compruebe si el servidor está ejecutado**
-
-Comando:
-
-``` bash
-$ sudo service clickhouse-server status
-```
-
-Si el servidor no se está ejecutando, inícielo con el comando:
-
-``` bash
-$ sudo service clickhouse-server start
-```
-
-**Comprobar registros**
-
-El registro principal de `clickhouse-server` está en `/var/log/clickhouse-server/clickhouse-server.log` predeterminada.
-
-Si el servidor se inició correctamente, debería ver las cadenas:
-
--   `<Information> Application: starting up.` — Server started.
--   `<Information> Application: Ready for connections.` — Server is running and ready for connections.
-
-Si `clickhouse-server` error de inicio con un error de configuración, debería ver el `<Error>` cadena con una descripción de error. Por ejemplo:
-
-``` text
-2019.01.11 15:23:25.549505 [ 45 ] {} <Error> ExternalDictionaries: Failed reloading 'event2id' external dictionary: Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused, e.what() = Connection refused
-```
-
-Si no ve un error al final del archivo, revise todo el archivo a partir de la cadena:
-
-``` text
-<Information> Application: starting up.
-```
-
-Si intenta iniciar una segunda instancia de `clickhouse-server` en el servidor, verá el siguiente registro:
-
-``` text
-2019.01.11 15:25:11.151730 [ 1 ] {} <Information> : Starting ClickHouse 19.1.0 with revision 54413
-2019.01.11 15:25:11.154578 [ 1 ] {} <Information> Application: starting up
-2019.01.11 15:25:11.156361 [ 1 ] {} <Information> StatusFile: Status file ./status already exists - unclean restart. Contents:
-PID: 8510
-Started at: 2019-01-11 15:24:23
-Revision: 54413
-
-2019.01.11 15:25:11.156673 [ 1 ] {} <Error> Application: DB::Exception: Cannot lock file ./status. Another server instance in same directory is already running.
-2019.01.11 15:25:11.156682 [ 1 ] {} <Information> Application: shutting down
-2019.01.11 15:25:11.156686 [ 1 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem
-2019.01.11 15:25:11.156716 [ 2 ] {} <Information> BaseDaemon: Stop SignalListener thread
-```
-
-**Ver sistema.d registros**
-
-Si no encuentra ninguna información útil en `clickhouse-server` registros o no hay registros, puede ver `system.d` registros usando el comando:
-
-``` bash
-$ sudo journalctl -u clickhouse-server
-```
-
-**Iniciar clickhouse-server en modo interactivo**
-
-``` bash
-$ sudo -u clickhouse /usr/bin/clickhouse-server --config-file /etc/clickhouse-server/config.xml
-```
-
-Este comando inicia el servidor como una aplicación interactiva con parámetros estándar del script de inicio automático. En este modo `clickhouse-server` imprime todos los mensajes de eventos en la consola.
-
-### Parámetros de configuración {#configuration-parameters}
-
-Comprobar:
-
--   Configuración de Docker.
-
-    Si ejecuta ClickHouse en Docker en una red IPv6, asegúrese de que `network=host` se establece.
-
--   Configuración del punto final.
-
-    Comprobar [listen_host](server-configuration-parameters/settings.md#server_configuration_parameters-listen_host) y [Tcp_port](server-configuration-parameters/settings.md#server_configuration_parameters-tcp_port) configuración.
-
-    El servidor ClickHouse acepta conexiones localhost solo de forma predeterminada.
-
--   Configuración del protocolo HTTP.
-
-    Compruebe la configuración del protocolo para la API HTTP.
-
--   Configuración de conexión segura.
-
-    Comprobar:
-
-    -   El [Tcp_port_secure](server-configuration-parameters/settings.md#server_configuration_parameters-tcp_port_secure) configuración.
-    -   Ajustes para [Sertificados SSL](server-configuration-parameters/settings.md#server_configuration_parameters-openssl).
-
-    Utilice los parámetros adecuados mientras se conecta. Por ejemplo, utilice el `port_secure` parámetro con `clickhouse_client`.
-
--   Configuración del usuario.
-
-    Es posible que esté utilizando el nombre de usuario o la contraseña incorrectos.
-
-## Procesamiento de consultas {#troubleshooting-does-not-process-queries}
-
-Si ClickHouse no puede procesar la consulta, envía una descripción de error al cliente. En el `clickhouse-client` obtienes una descripción del error en la consola. Si está utilizando la interfaz HTTP, ClickHouse envía la descripción del error en el cuerpo de la respuesta. Por ejemplo:
-
-``` bash
-$ curl 'http://localhost:8123/' --data-binary "SELECT a"
-Code: 47, e.displayText() = DB::Exception: Unknown identifier: a. Note that there are no tables (FROM clause) in your query, context: required_names: 'a' source_tables: table_aliases: private_aliases: column_aliases: public_columns: 'a' masked_columns: array_join_columns: source_columns: , e.what() = DB::Exception
-```
-
-Si empiezas `clickhouse-client` con el `stack-trace` parámetro, ClickHouse devuelve el seguimiento de la pila del servidor con la descripción de un error.
-
-Es posible que vea un mensaje sobre una conexión rota. En este caso, puede repetir la consulta. Si la conexión se rompe cada vez que realiza la consulta, compruebe si hay errores en los registros del servidor.
-
-## Eficiencia del procesamiento de consultas {#troubleshooting-too-slow}
-
-Si ve que ClickHouse funciona demasiado lentamente, debe perfilar la carga en los recursos del servidor y la red para sus consultas.
-
-Puede utilizar la utilidad clickhouse-benchmark para crear perfiles de consultas. Muestra el número de consultas procesadas por segundo, el número de filas procesadas por segundo y percentiles de tiempos de procesamiento de consultas.
diff --git a/docs/es/operations/update.md b/docs/es/operations/update.md
deleted file mode 100644
index 11d15381d72e..000000000000
--- a/docs/es/operations/update.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: "Actualizaci\xF3n de ClickHouse"
----
-
-# Actualización de ClickHouse {#clickhouse-update}
-
-Si se instaló ClickHouse desde paquetes deb, ejecute los siguientes comandos en el servidor:
-
-``` bash
-$ sudo apt-get update
-$ sudo apt-get install clickhouse-client clickhouse-server
-$ sudo service clickhouse-server restart
-```
-
-Si ha instalado ClickHouse utilizando algo distinto de los paquetes deb recomendados, utilice el método de actualización adecuado.
-
-ClickHouse no admite una actualización distribuida. La operación debe realizarse consecutivamente en cada servidor separado. No actualice todos los servidores de un clúster simultáneamente, o el clúster no estará disponible durante algún tiempo.
diff --git a/docs/es/operations/utilities/clickhouse-benchmark.md b/docs/es/operations/utilities/clickhouse-benchmark.md
deleted file mode 100644
index 9bcafa40dfea..000000000000
--- a/docs/es/operations/utilities/clickhouse-benchmark.md
+++ /dev/null
@@ -1,156 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: Sistema abierto.
----
-
-# Sistema abierto {#clickhouse-benchmark}
-
-Se conecta a un servidor ClickHouse y envía repetidamente las consultas especificadas.
-
-Sintaxis:
-
-``` bash
-$ echo "single query" | clickhouse-benchmark [keys]
-```
-
-o
-
-``` bash
-$ clickhouse-benchmark [keys] <<< "single query"
-```
-
-Si desea enviar un conjunto de consultas, cree un archivo de texto y coloque cada consulta en la cadena individual de este archivo. Por ejemplo:
-
-``` sql
-SELECT * FROM system.numbers LIMIT 10000000
-SELECT 1
-```
-
-Luego pase este archivo a una entrada estándar de `clickhouse-benchmark`.
-
-``` bash
-clickhouse-benchmark [keys] < queries_file
-```
-
-## Claves {#clickhouse-benchmark-keys}
-
--   `-c N`, `--concurrency=N` — Number of queries that `clickhouse-benchmark` se envía simultáneamente. Valor predeterminado: 1.
--   `-d N`, `--delay=N` — Interval in seconds between intermediate reports (set 0 to disable reports). Default value: 1.
--   `-h WORD`, `--host=WORD` — Server host. Default value: `localhost`. Para el [modo de comparación](#clickhouse-benchmark-comparison-mode) puedes usar múltiples `-h` claves.
--   `-p N`, `--port=N` — Server port. Default value: 9000. For the [modo de comparación](#clickhouse-benchmark-comparison-mode) puedes usar múltiples `-p` claves.
--   `-i N`, `--iterations=N` — Total number of queries. Default value: 0.
--   `-r`, `--randomize` — Random order of queries execution if there is more then one input query.
--   `-s`, `--secure` — Using TLS connection.
--   `-t N`, `--timelimit=N` — Time limit in seconds. `clickhouse-benchmark` detiene el envío de consultas cuando se alcanza el límite de tiempo especificado. Valor predeterminado: 0 (límite de tiempo desactivado).
--   `--confidence=N` — Level of confidence for T-test. Possible values: 0 (80%), 1 (90%), 2 (95%), 3 (98%), 4 (99%), 5 (99.5%). Default value: 5. In the [modo de comparación](#clickhouse-benchmark-comparison-mode) `clickhouse-benchmark` realiza el [Prueba t independiente de dos muestras para estudiantes](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) prueba para determinar si las dos distribuciones no son diferentes con el nivel de confianza seleccionado.
--   `--cumulative` — Printing cumulative data instead of data per interval.
--   `--database=DATABASE_NAME` — ClickHouse database name. Default value: `default`.
--   `--json=FILEPATH` — JSON output. When the key is set, `clickhouse-benchmark` emite un informe al archivo JSON especificado.
--   `--user=USERNAME` — ClickHouse user name. Default value: `default`.
--   `--password=PSWD` — ClickHouse user password. Default value: empty string.
--   `--stacktrace` — Stack traces output. When the key is set, `clickhouse-bencmark` las salidas acumulan rastros de excepciones.
--   `--stage=WORD` — Query processing stage at server. ClickHouse stops query processing and returns answer to `clickhouse-benchmark` en la etapa especificada. Valores posibles: `complete`, `fetch_columns`, `with_mergeable_state`. Valor predeterminado: `complete`.
--   `--help` — Shows the help message.
-
-Si desea aplicar alguna [configuración](../../operations/settings/index.md) para consultas, páselas como una clave `--<session setting name>= SETTING_VALUE`. Por ejemplo, `--max_memory_usage=1048576`.
-
-## Salida {#clickhouse-benchmark-output}
-
-Predeterminada, `clickhouse-benchmark` informes para cada `--delay` intervalo.
-
-Ejemplo del informe:
-
-``` text
-Queries executed: 10.
-
-localhost:9000, queries 10, QPS: 6.772, RPS: 67904487.440, MiB/s: 518.070, result RPS: 67721584.984, result MiB/s: 516.675.
-
-0.000%      0.145 sec.
-10.000%     0.146 sec.
-20.000%     0.146 sec.
-30.000%     0.146 sec.
-40.000%     0.147 sec.
-50.000%     0.148 sec.
-60.000%     0.148 sec.
-70.000%     0.148 sec.
-80.000%     0.149 sec.
-90.000%     0.150 sec.
-95.000%     0.150 sec.
-99.000%     0.150 sec.
-99.900%     0.150 sec.
-99.990%     0.150 sec.
-```
-
-En el informe puedes encontrar:
-
--   Número de consultas en el `Queries executed:` campo.
-
--   Cadena de estado que contiene (en orden):
-
-    -   Punto final del servidor ClickHouse.
-    -   Número de consultas procesadas.
-    -   QPS: QPS: ¿Cuántas consultas realizó el servidor por segundo durante un período `--delay` argumento.
-    -   RPS: ¿Cuántas filas lee el servidor por segundo durante un período `--delay` argumento.
-    -   MiB/s: ¿Cuántos mebibytes servidor leído por segundo durante un período especificado en el `--delay` argumento.
-    -   resultado RPS: ¿Cuántas filas colocadas por el servidor al resultado de una consulta por segundo durante un período `--delay` argumento.
-    -   resultado MiB/s. ¿Cuántos mebibytes colocados por el servidor al resultado de una consulta por segundo durante un período especificado en el `--delay` argumento.
-
--   Percentiles de tiempo de ejecución de consultas.
-
-## Modo de comparación {#clickhouse-benchmark-comparison-mode}
-
-`clickhouse-benchmark` puede comparar el rendimiento de dos servidores ClickHouse en ejecución.
-
-Para utilizar el modo de comparación, especifique los puntos finales de ambos servidores `--host`, `--port` claves. Las claves coinciden entre sí por posición en la lista de argumentos, la primera `--host` se empareja con la primera `--port` y así sucesivamente. `clickhouse-benchmark` establece conexiones a ambos servidores, luego envía consultas. Cada consulta dirigida a un servidor seleccionado al azar. Los resultados se muestran para cada servidor por separado.
-
-## Ejemplo {#clickhouse-benchmark-example}
-
-``` bash
-$ echo "SELECT * FROM system.numbers LIMIT 10000000 OFFSET 10000000" | clickhouse-benchmark -i 10
-```
-
-``` text
-Loaded 1 queries.
-
-Queries executed: 6.
-
-localhost:9000, queries 6, QPS: 6.153, RPS: 123398340.957, MiB/s: 941.455, result RPS: 61532982.200, result MiB/s: 469.459.
-
-0.000%      0.159 sec.
-10.000%     0.159 sec.
-20.000%     0.159 sec.
-30.000%     0.160 sec.
-40.000%     0.160 sec.
-50.000%     0.162 sec.
-60.000%     0.164 sec.
-70.000%     0.165 sec.
-80.000%     0.166 sec.
-90.000%     0.166 sec.
-95.000%     0.167 sec.
-99.000%     0.167 sec.
-99.900%     0.167 sec.
-99.990%     0.167 sec.
-
-
-
-Queries executed: 10.
-
-localhost:9000, queries 10, QPS: 6.082, RPS: 121959604.568, MiB/s: 930.478, result RPS: 60815551.642, result MiB/s: 463.986.
-
-0.000%      0.159 sec.
-10.000%     0.159 sec.
-20.000%     0.160 sec.
-30.000%     0.163 sec.
-40.000%     0.164 sec.
-50.000%     0.165 sec.
-60.000%     0.166 sec.
-70.000%     0.166 sec.
-80.000%     0.167 sec.
-90.000%     0.167 sec.
-95.000%     0.170 sec.
-99.000%     0.172 sec.
-99.900%     0.172 sec.
-99.990%     0.172 sec.
-```
diff --git a/docs/es/operations/utilities/clickhouse-copier.md b/docs/es/operations/utilities/clickhouse-copier.md
deleted file mode 100644
index 5717ffaa737f..000000000000
--- a/docs/es/operations/utilities/clickhouse-copier.md
+++ /dev/null
@@ -1,176 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: "M\xE9todo de codificaci\xF3n de datos:"
----
-
-# Método de codificación de datos: {#clickhouse-copier}
-
-Copia datos de las tablas de un clúster en tablas de otro (o del mismo) clúster.
-
-Puede ejecutar varios `clickhouse-copier` instancias en diferentes servidores para realizar el mismo trabajo. ZooKeeper se utiliza para sincronizar los procesos.
-
-Después de comenzar, `clickhouse-copier`:
-
--   Se conecta a ZooKeeper y recibe:
-
-    -   Copia de trabajos.
-    -   El estado de los trabajos de copia.
-
--   Realiza los trabajos.
-
-    Cada proceso en ejecución elige el “closest” el fragmento del clúster de origen y copia los datos en el clúster de destino, reafirmando los datos si es necesario.
-
-`clickhouse-copier` realiza un seguimiento de los cambios en ZooKeeper y los aplica sobre la marcha.
-
-Para reducir el tráfico de red, recomendamos ejecutar `clickhouse-copier` en el mismo servidor donde se encuentran los datos de origen.
-
-## Ejecución de Clickhouse-copiadora {#running-clickhouse-copier}
-
-La utilidad debe ejecutarse manualmente:
-
-``` bash
-$ clickhouse-copier copier --daemon --config zookeeper.xml --task-path /task/path --base-dir /path/to/dir
-```
-
-Parámetros:
-
--   `daemon` — Starts `clickhouse-copier` en modo daemon.
--   `config` — The path to the `zookeeper.xml` con los parámetros para la conexión a ZooKeeper.
--   `task-path` — The path to the ZooKeeper node. This node is used for syncing `clickhouse-copier` procesos y tareas de almacenamiento. Las tareas se almacenan en `$task-path/description`.
--   `task-file` — Optional path to file with task configuration for initial upload to ZooKeeper.
--   `task-upload-force` — Force upload `task-file` incluso si el nodo ya existe.
--   `base-dir` — The path to logs and auxiliary files. When it starts, `clickhouse-copier` crear `clickhouse-copier_YYYYMMHHSS_<PID>` subdirectorios en `$base-dir`. Si se omite este parámetro, los directorios se crean en el directorio donde `clickhouse-copier` se puso en marcha.
-
-## Formato de Zookeeper.XML {#format-of-zookeeper-xml}
-
-``` xml
-<yandex>
-    <logger>
-        <level>trace</level>
-        <size>100M</size>
-        <count>3</count>
-    </logger>
-
-    <zookeeper>
-        <node index="1">
-            <host>127.0.0.1</host>
-            <port>2181</port>
-        </node>
-    </zookeeper>
-</yandex>
-```
-
-## Configuración de tareas de copia {#configuration-of-copying-tasks}
-
-``` xml
-<yandex>
-    <!-- Configuration of clusters as in an ordinary server config -->
-    <remote_servers>
-        <source_cluster>
-            <shard>
-                <internal_replication>false</internal_replication>
-                    <replica>
-                        <host>127.0.0.1</host>
-                        <port>9000</port>
-                    </replica>
-            </shard>
-            ...
-        </source_cluster>
-
-        <destination_cluster>
-        ...
-        </destination_cluster>
-    </remote_servers>
-
-    <!-- How many simultaneously active workers are possible. If you run more workers superfluous workers will sleep. -->
-    <max_workers>2</max_workers>
-
-    <!-- Setting used to fetch (pull) data from source cluster tables -->
-    <settings_pull>
-        <readonly>1</readonly>
-    </settings_pull>
-
-    <!-- Setting used to insert (push) data to destination cluster tables -->
-    <settings_push>
-        <readonly>0</readonly>
-    </settings_push>
-
-    <!-- Common setting for fetch (pull) and insert (push) operations. Also, copier process context uses it.
-         They are overlaid by <settings_pull/> and <settings_push/> respectively. -->
-    <settings>
-        <connect_timeout>3</connect_timeout>
-        <!-- Sync insert is set forcibly, leave it here just in case. -->
-        <insert_distributed_sync>1</insert_distributed_sync>
-    </settings>
-
-    <!-- Copying tasks description.
-         You could specify several table task in the same task description (in the same ZooKeeper node), they will be performed
-         sequentially.
-    -->
-    <tables>
-        <!-- A table task, copies one table. -->
-        <table_hits>
-            <!-- Source cluster name (from <remote_servers/> section) and tables in it that should be copied -->
-            <cluster_pull>source_cluster</cluster_pull>
-            <database_pull>test</database_pull>
-            <table_pull>hits</table_pull>
-
-            <!-- Destination cluster name and tables in which the data should be inserted -->
-            <cluster_push>destination_cluster</cluster_push>
-            <database_push>test</database_push>
-            <table_push>hits2</table_push>
-
-            <!-- Engine of destination tables.
-                 If destination tables have not be created, workers create them using columns definition from source tables and engine
-                 definition from here.
-
-                 NOTE: If the first worker starts insert data and detects that destination partition is not empty then the partition will
-                 be dropped and refilled, take it into account if you already have some data in destination tables. You could directly
-                 specify partitions that should be copied in <enabled_partitions/>, they should be in quoted format like partition column of
-                 system.parts table.
-            -->
-            <engine>
-            ENGINE=ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/hits2', '{replica}')
-            PARTITION BY toMonday(date)
-            ORDER BY (CounterID, EventDate)
-            </engine>
-
-            <!-- Sharding key used to insert data to destination cluster -->
-            <sharding_key>jumpConsistentHash(intHash64(UserID), 2)</sharding_key>
-
-            <!-- Optional expression that filter data while pull them from source servers -->
-            <where_condition>CounterID != 0</where_condition>
-
-            <!-- This section specifies partitions that should be copied, other partition will be ignored.
-                 Partition names should have the same format as
-                 partition column of system.parts table (i.e. a quoted text).
-                 Since partition key of source and destination cluster could be different,
-                 these partition names specify destination partitions.
-
-                 NOTE: In spite of this section is optional (if it is not specified, all partitions will be copied),
-                 it is strictly recommended to specify them explicitly.
-                 If you already have some ready partitions on destination cluster they
-                 will be removed at the start of the copying since they will be interpeted
-                 as unfinished data from the previous copying!!!
-            -->
-            <enabled_partitions>
-                <partition>'2018-02-26'</partition>
-                <partition>'2018-03-05'</partition>
-                ...
-            </enabled_partitions>
-        </table_hits>
-
-        <!-- Next table to copy. It is not copied until previous table is copying. -->
-        </table_visits>
-        ...
-        </table_visits>
-        ...
-    </tables>
-</yandex>
-```
-
-`clickhouse-copier` seguimiento de los cambios en `/task/path/description` y los aplica sobre la marcha. Por ejemplo, si cambia el valor de `max_workers`, el número de procesos que ejecutan tareas también cambiará.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/utils/clickhouse-copier/) <!--hide-->
diff --git a/docs/es/operations/utilities/clickhouse-local.md b/docs/es/operations/utilities/clickhouse-local.md
deleted file mode 100644
index e122f668f538..000000000000
--- a/docs/es/operations/utilities/clickhouse-local.md
+++ /dev/null
@@ -1,81 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: clickhouse-local
----
-
-# clickhouse-local {#clickhouse-local}
-
-El `clickhouse-local` El programa le permite realizar un procesamiento rápido en archivos locales, sin tener que implementar y configurar el servidor ClickHouse.
-
-Acepta datos que representan tablas y las consulta usando [Nombre de la red inalámbrica (SSID):](../../sql-reference/index.md).
-
-`clickhouse-local` utiliza el mismo núcleo que el servidor ClickHouse, por lo que es compatible con la mayoría de las características y el mismo conjunto de formatos y motores de tabla.
-
-Predeterminada `clickhouse-local` no tiene acceso a los datos en el mismo host, pero admite la carga de la configuración del servidor `--config-file` argumento.
-
-!!! warning "Advertencia"
-    No se recomienda cargar la configuración del servidor de producción en `clickhouse-local` Porque los datos pueden dañarse en caso de error humano.
-
-## Uso {#usage}
-
-Uso básico:
-
-``` bash
-$ clickhouse-local --structure "table_structure" --input-format "format_of_incoming_data" -q "query"
-```
-
-Argumento:
-
--   `-S`, `--structure` — table structure for input data.
--   `-if`, `--input-format` — input format, `TSV` predeterminada.
--   `-f`, `--file` — path to data, `stdin` predeterminada.
--   `-q` `--query` — queries to execute with `;` como delimitador.
--   `-N`, `--table` — table name where to put output data, `table` predeterminada.
--   `-of`, `--format`, `--output-format` — output format, `TSV` predeterminada.
--   `--stacktrace` — whether to dump debug output in case of exception.
--   `--verbose` — more details on query execution.
--   `-s` — disables `stderr` tala.
--   `--config-file` — path to configuration file in same format as for ClickHouse server, by default the configuration empty.
--   `--help` — arguments references for `clickhouse-local`.
-
-También hay argumentos para cada variable de configuración de ClickHouse que se usan más comúnmente en lugar de `--config-file`.
-
-## Ejemplos {#examples}
-
-``` bash
-$ echo -e "1,2
3,4" | clickhouse-local -S "a Int64, b Int64" -if "CSV" -q "SELECT * FROM table"
-Read 2 rows, 32.00 B in 0.000 sec., 5182 rows/sec., 80.97 KiB/sec.
-1   2
-3   4
-```
-
-El ejemplo anterior es el mismo que:
-
-``` bash
-$ echo -e "1,2
3,4" | clickhouse-local -q "CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table"
-Read 2 rows, 32.00 B in 0.000 sec., 4987 rows/sec., 77.93 KiB/sec.
-1   2
-3   4
-```
-
-Ahora vamos a usuario de memoria de salida para cada usuario de Unix:
-
-``` bash
-$ ps aux | tail -n +2 | awk '{ printf("%s\t%s
", $1, $4) }' | clickhouse-local -S "user String, mem Float64" -q "SELECT user, round(sum(mem), 2) as memTotal FROM table GROUP BY user ORDER BY memTotal DESC FORMAT Pretty"
-```
-
-``` text
-Read 186 rows, 4.15 KiB in 0.035 sec., 5302 rows/sec., 118.34 KiB/sec.
-┏━━━━━━━━━━┳━━━━━━━━━━┓
-┃ user     ┃ memTotal ┃
-┡━━━━━━━━━━╇━━━━━━━━━━┩
-│ bayonet  │    113.5 │
-├──────────┼──────────┤
-│ root     │      8.8 │
-├──────────┼──────────┤
-...
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/utils/clickhouse-local/) <!--hide-->
diff --git a/docs/es/operations/utilities/index.md b/docs/es/operations/utilities/index.md
deleted file mode 100644
index a69397a326c7..000000000000
--- a/docs/es/operations/utilities/index.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Utilidad
-toc_priority: 56
-toc_title: "Descripci\xF3n"
----
-
-# Utilidad ClickHouse {#clickhouse-utility}
-
--   [Sistema abierto.](clickhouse-local.md#clickhouse-local) — Allows running SQL queries on data without stopping the ClickHouse server, similar to how `awk` hace esto.
--   [Método de codificación de datos:](clickhouse-copier.md) — Copies (and reshards) data from one cluster to another cluster.
--   [Sistema abierto.](clickhouse-benchmark.md) — Loads server with the custom queries and settings.
-
-[Artículo Original](https://clickhouse.tech/docs/en/operations/utils/) <!--hide-->
diff --git a/docs/es/roadmap.md b/docs/es/roadmap.md
deleted file mode 100644
index 60db1c608dfd..000000000000
--- a/docs/es/roadmap.md
+++ /dev/null
@@ -1,16 +0,0 @@
----
-machine_translated: true
----
-
-# Hoja De Ruta {#roadmap}
-
-## Q1 2020 {#q1-2020}
-
--   Control de acceso basado en roles
-
-## Q2 2020 {#q2-2020}
-
--   Integración con servicios de autenticación externos
--   Grupos de recursos para una distribución más precisa de la capacidad del clúster entre los usuarios
-
-{## [Artículo Original](https://clickhouse.tech/docs/es/roadmap/) ##}
diff --git a/docs/es/sql-reference/aggregate-functions/combinators.md b/docs/es/sql-reference/aggregate-functions/combinators.md
deleted file mode 100644
index c9fdcb9478f8..000000000000
--- a/docs/es/sql-reference/aggregate-functions/combinators.md
+++ /dev/null
@@ -1,245 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: Combinadores
----
-
-# Combinadores de funciones agregadas {#aggregate_functions_combinators}
-
-El nombre de una función agregada puede tener un sufijo anexado. Esto cambia la forma en que funciona la función de agregado.
-
-## -Si {#agg-functions-combinator-if}
-
-The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument – a condition (Uint8 type). The aggregate function processes only the rows that trigger the condition. If the condition was not triggered even once, it returns a default value (usually zeros or empty strings).
-
-Ejemplos: `sumIf(column, cond)`, `countIf(cond)`, `avgIf(x, cond)`, `quantilesTimingIf(level1, level2)(x, cond)`, `argMinIf(arg, val, cond)` y así sucesivamente.
-
-Con las funciones de agregado condicional, puede calcular agregados para varias condiciones a la vez, sin utilizar subconsultas y `JOIN`Por ejemplo, en Yandex.Metrica, las funciones de agregado condicional se utilizan para implementar la funcionalidad de comparación de segmentos.
-
-## -Matriz {#agg-functions-combinator-array}
-
-El sufijo -Array se puede agregar a cualquier función agregada. En este caso, la función de agregado toma argumentos del ‘Array(T)’ tipo (arrays) en lugar de ‘T’ argumentos de tipo. Si la función de agregado acepta varios argumentos, deben ser matrices de igual longitud. Al procesar matrices, la función de agregado funciona como la función de agregado original en todos los elementos de la matriz.
-
-Ejemplo 1: `sumArray(arr)` - Totales de todos los elementos de todos ‘arr’ matriz. En este ejemplo, podría haber sido escrito más simplemente: `sum(arraySum(arr))`.
-
-Ejemplo 2: `uniqArray(arr)` – Counts the number of unique elements in all ‘arr’ matriz. Esto podría hacerse de una manera más fácil: `uniq(arrayJoin(arr))`, pero no siempre es posible agregar ‘arrayJoin’ a una consulta.
-
--If y -Array se pueden combinar. Obstante, ‘Array’ debe venir primero, entonces ‘If’. Ejemplos: `uniqArrayIf(arr, cond)`, `quantilesTimingArrayIf(level1, level2)(arr, cond)`. Debido a este pedido, el ‘cond’ argumento no será una matriz.
-
-## -Estado {#agg-functions-combinator-state}
-
-Si aplica este combinador, la función de agregado no devuelve el valor resultante (como el número de valores únicos para el [uniq](reference.md#agg_function-uniq) función), pero un estado intermedio de la agregación (para `uniq`, esta es la tabla hash para calcular el número de valores únicos). Este es un `AggregateFunction(...)` que puede ser utilizado para su posterior procesamiento o almacenado en una tabla para terminar de agregar más tarde.
-
-Para trabajar con estos estados, use:
-
--   [AgregaciónMergeTree](../../engines/table-engines/mergetree-family/aggregatingmergetree.md) motor de mesa.
--   [finalizeAggregation](../../sql-reference/functions/other-functions.md#function-finalizeaggregation) función.
--   [runningAccumulate](../../sql-reference/functions/other-functions.md#function-runningaccumulate) función.
--   [-Fusionar](#aggregate_functions_combinators-merge) combinador.
--   [-MergeState](#aggregate_functions_combinators-mergestate) combinador.
-
-## -Fusionar {#aggregate_functions_combinators-merge}
-
-Si aplica este combinador, la función de agregado toma el estado de agregación intermedio como argumento, combina los estados para finalizar la agregación y devuelve el valor resultante.
-
-## -MergeState {#aggregate_functions_combinators-mergestate}
-
-Combina los estados de agregación intermedios de la misma manera que el combinador -Merge. Sin embargo, no devuelve el valor resultante, sino un estado de agregación intermedio, similar al combinador -State.
-
-## -ForEach {#agg-functions-combinator-foreach}
-
-Convierte una función de agregado para tablas en una función de agregado para matrices que agrega los elementos de matriz correspondientes y devuelve una matriz de resultados. Por ejemplo, `sumForEach` para las matrices `[1, 2]`, `[3, 4, 5]`y`[6, 7]`devuelve el resultado `[10, 13, 5]` después de agregar los elementos de la matriz correspondientes.
-
-## -OPor defecto {#agg-functions-combinator-ordefault}
-
-Cambia el comportamiento de una función agregada.
-
-Si una función agregada no tiene valores de entrada, con este combinador devuelve el valor predeterminado para su tipo de datos de retorno. Se aplica a las funciones agregadas que pueden tomar datos de entrada vacíos.
-
-`-OrDefault` se puede utilizar con otros combinadores.
-
-**Sintaxis**
-
-``` sql
-<aggFunction>OrDefault(x)
-```
-
-**Parámetros**
-
--   `x` — Aggregate function parameters.
-
-**Valores devueltos**
-
-Devuelve el valor predeterminado del tipo devuelto de una función de agregado si no hay nada que agregar.
-
-El tipo depende de la función de agregado utilizada.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT avg(number), avgOrDefault(number) FROM numbers(0)
-```
-
-Resultado:
-
-``` text
-┌─avg(number)─┬─avgOrDefault(number)─┐
-│         nan │                    0 │
-└─────────────┴──────────────────────┘
-```
-
-También `-OrDefault` se puede utilizar con otros combinadores. Es útil cuando la función de agregado no acepta la entrada vacía.
-
-Consulta:
-
-``` sql
-SELECT avgOrDefaultIf(x, x > 10)
-FROM
-(
-    SELECT toDecimal32(1.23, 2) AS x
-)
-```
-
-Resultado:
-
-``` text
-┌─avgOrDefaultIf(x, greater(x, 10))─┐
-│                              0.00 │
-└───────────────────────────────────┘
-```
-
-## -OrNull {#agg-functions-combinator-ornull}
-
-Cambia el comportamiento de una función agregada.
-
-Este combinador convierte un resultado de una función agregada en [NULL](../data-types/nullable.md) tipo de datos. Si la función de agregado no tiene valores para calcular devuelve [NULL](../syntax.md#null-literal).
-
-`-OrNull` se puede utilizar con otros combinadores.
-
-**Sintaxis**
-
-``` sql
-<aggFunction>OrNull(x)
-```
-
-**Parámetros**
-
--   `x` — Aggregate function parameters.
-
-**Valores devueltos**
-
--   El resultado de la función de agregado, convertida a la `Nullable` tipo de datos.
--   `NULL`, si no hay nada que agregar.
-
-Tipo: `Nullable(aggregate function return type)`.
-
-**Ejemplo**
-
-Añadir `-orNull` hasta el final de la función agregada.
-
-Consulta:
-
-``` sql
-SELECT sumOrNull(number), toTypeName(sumOrNull(number)) FROM numbers(10) WHERE number > 10
-```
-
-Resultado:
-
-``` text
-┌─sumOrNull(number)─┬─toTypeName(sumOrNull(number))─┐
-│              ᴺᵁᴸᴸ │ Nullable(UInt64)              │
-└───────────────────┴───────────────────────────────┘
-```
-
-También `-OrNull` se puede utilizar con otros combinadores. Es útil cuando la función de agregado no acepta la entrada vacía.
-
-Consulta:
-
-``` sql
-SELECT avgOrNullIf(x, x > 10)
-FROM
-(
-    SELECT toDecimal32(1.23, 2) AS x
-)
-```
-
-Resultado:
-
-``` text
-┌─avgOrNullIf(x, greater(x, 10))─┐
-│                           ᴺᵁᴸᴸ │
-└────────────────────────────────┘
-```
-
-## -Remuestrear {#agg-functions-combinator-resample}
-
-Permite dividir los datos en grupos y, a continuación, agregar por separado los datos de esos grupos. Los grupos se crean dividiendo los valores de una columna en intervalos.
-
-``` sql
-<aggFunction>Resample(start, end, step)(<aggFunction_params>, resampling_key)
-```
-
-**Parámetros**
-
--   `start` — Starting value of the whole required interval for `resampling_key` valor.
--   `stop` — Ending value of the whole required interval for `resampling_key` valor. Todo el intervalo no incluye el `stop` valor `[start, stop)`.
--   `step` — Step for separating the whole interval into subintervals. The `aggFunction` se ejecuta sobre cada uno de esos subintervalos de forma independiente.
--   `resampling_key` — Column whose values are used for separating data into intervals.
--   `aggFunction_params` — `aggFunction` parámetros.
-
-**Valores devueltos**
-
--   Matriz de `aggFunction` resultados para cada subintervalo.
-
-**Ejemplo**
-
-Considere el `people` con los siguientes datos:
-
-``` text
-┌─name───┬─age─┬─wage─┐
-│ John   │  16 │   10 │
-│ Alice  │  30 │   15 │
-│ Mary   │  35 │    8 │
-│ Evelyn │  48 │ 11.5 │
-│ David  │  62 │  9.9 │
-│ Brian  │  60 │   16 │
-└────────┴─────┴──────┘
-```
-
-Obtengamos los nombres de las personas cuya edad se encuentra en los intervalos de `[30,60)` y `[60,75)`. Como usamos la representación entera para la edad, obtenemos edades en el `[30, 59]` y `[60,74]` intervalo.
-
-Para agregar nombres en una matriz, usamos el [Método de codificación de datos:](reference.md#agg_function-grouparray) función de agregado. Se necesita un argumento. En nuestro caso, es el `name` columna. El `groupArrayResample` función debe utilizar el `age` columna para agregar nombres por edad. Para definir los intervalos requeridos, pasamos el `30, 75, 30` discusiones sobre el `groupArrayResample` función.
-
-``` sql
-SELECT groupArrayResample(30, 75, 30)(name, age) FROM people
-```
-
-``` text
-┌─groupArrayResample(30, 75, 30)(name, age)─────┐
-│ [['Alice','Mary','Evelyn'],['David','Brian']] │
-└───────────────────────────────────────────────┘
-```
-
-Considera los resultados.
-
-`Jonh` est? fuera de la muestra porque es demasiado joven. Otras personas se distribuyen de acuerdo con los intervalos de edad especificados.
-
-Ahora vamos a contar el número total de personas y su salario promedio en los intervalos de edad especificados.
-
-``` sql
-SELECT
-    countResample(30, 75, 30)(name, age) AS amount,
-    avgResample(30, 75, 30)(wage, age) AS avg_wage
-FROM people
-```
-
-``` text
-┌─amount─┬─avg_wage──────────────────┐
-│ [3,2]  │ [11.5,12.949999809265137] │
-└────────┴───────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/agg_functions/combinators/) <!--hide-->
diff --git a/docs/es/sql-reference/aggregate-functions/index.md b/docs/es/sql-reference/aggregate-functions/index.md
deleted file mode 100644
index 7c7d58d5f948..000000000000
--- a/docs/es/sql-reference/aggregate-functions/index.md
+++ /dev/null
@@ -1,62 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Funciones agregadas
-toc_priority: 33
-toc_title: "Implantaci\xF3n"
----
-
-# Funciones agregadas {#aggregate-functions}
-
-Las funciones agregadas funcionan en el [normal](http://www.sql-tutorial.com/sql-aggregate-functions-sql-tutorial) forma esperada por los expertos en bases de datos.
-
-ClickHouse también es compatible:
-
--   [Funciones agregadas paramétricas](parametric-functions.md#aggregate_functions_parametric) que aceptan otros parámetros además de las columnas.
--   [Combinadores](combinators.md#aggregate_functions_combinators), que cambian el comportamiento de las funciones agregadas.
-
-## Procesamiento NULL {#null-processing}
-
-Durante la agregación, todos `NULL`s se omiten.
-
-**Ejemplos:**
-
-Considere esta tabla:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │    2 │
-│ 2 │ ᴺᵁᴸᴸ │
-│ 3 │    2 │
-│ 3 │    3 │
-│ 3 │ ᴺᵁᴸᴸ │
-└───┴──────┘
-```
-
-Digamos que necesita sumar los valores en el `y` columna:
-
-``` sql
-SELECT sum(y) FROM t_null_big
-```
-
-    ┌─sum(y)─┐
-    │      7 │
-    └────────┘
-
-El `sum` función interpreta `NULL` como `0`. En particular, esto significa que si la función recibe la entrada de una selección donde todos los valores son `NULL`, entonces el resultado será `0`, ni `NULL`.
-
-Ahora puedes usar el `groupArray` función para crear una matriz a partir de la `y` columna:
-
-``` sql
-SELECT groupArray(y) FROM t_null_big
-```
-
-``` text
-┌─groupArray(y)─┐
-│ [2,2,3]       │
-└───────────────┘
-```
-
-`groupArray` no incluye `NULL` en la matriz resultante.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/agg_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/aggregate-functions/parametric-functions.md b/docs/es/sql-reference/aggregate-functions/parametric-functions.md
deleted file mode 100644
index ea32920401ba..000000000000
--- a/docs/es/sql-reference/aggregate-functions/parametric-functions.md
+++ /dev/null
@@ -1,499 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: "Param\xE9trico"
----
-
-# Funciones agregadas paramétricas {#aggregate_functions_parametric}
-
-Some aggregate functions can accept not only argument columns (used for compression), but a set of parameters – constants for initialization. The syntax is two pairs of brackets instead of one. The first is for parameters, and the second is for arguments.
-
-## histograma {#histogram}
-
-Calcula un histograma adaptativo. No garantiza resultados precisos.
-
-``` sql
-histogram(number_of_bins)(values)
-```
-
-Las funciones utiliza [Un algoritmo de árbol de decisión paralelo de transmisión](http://jmlr.org/papers/volume11/ben-haim10a/ben-haim10a.pdf). Los bordes de los contenedores de histograma se ajustan a medida que los nuevos datos entran en una función. En caso común, los anchos de los contenedores no son iguales.
-
-**Parámetros**
-
-`number_of_bins` — Upper limit for the number of bins in the histogram. The function automatically calculates the number of bins. It tries to reach the specified number of bins, but if it fails, it uses fewer bins.
-`values` — [Expresion](../syntax.md#syntax-expressions) resultando en valores de entrada.
-
-**Valores devueltos**
-
--   [Matriz](../../sql-reference/data-types/array.md) de [Tuples](../../sql-reference/data-types/tuple.md) del siguiente formato:
-
-        ```
-        [(lower_1, upper_1, height_1), ... (lower_N, upper_N, height_N)]
-        ```
-
-        - `lower` — Lower bound of the bin.
-        - `upper` — Upper bound of the bin.
-        - `height` — Calculated height of the bin.
-
-**Ejemplo**
-
-``` sql
-SELECT histogram(5)(number + 1)
-FROM (
-    SELECT *
-    FROM system.numbers
-    LIMIT 20
-)
-```
-
-``` text
-┌─histogram(5)(plus(number, 1))───────────────────────────────────────────┐
-│ [(1,4.5,4),(4.5,8.5,4),(8.5,12.75,4.125),(12.75,17,4.625),(17,20,3.25)] │
-└─────────────────────────────────────────────────────────────────────────┘
-```
-
-Puede visualizar un histograma con el [Bar](../../sql-reference/functions/other-functions.md#function-bar) función, por ejemplo:
-
-``` sql
-WITH histogram(5)(rand() % 100) AS hist
-SELECT
-    arrayJoin(hist).3 AS height,
-    bar(height, 0, 6, 5) AS bar
-FROM
-(
-    SELECT *
-    FROM system.numbers
-    LIMIT 20
-)
-```
-
-``` text
-┌─height─┬─bar───┐
-│  2.125 │ █▋    │
-│   3.25 │ ██▌   │
-│  5.625 │ ████▏ │
-│  5.625 │ ████▏ │
-│  3.375 │ ██▌   │
-└────────┴───────┘
-```
-
-En este caso, debe recordar que no conoce los bordes del contenedor del histograma.
-
-## sequenceMatch(pattern)(timestamp, cond1, cond2, …) {#function-sequencematch}
-
-Comprueba si la secuencia contiene una cadena de eventos que coincida con el patrón.
-
-``` sql
-sequenceMatch(pattern)(timestamp, cond1, cond2, ...)
-```
-
-!!! warning "Advertencia"
-    Los eventos que ocurren en el mismo segundo pueden estar en la secuencia en un orden indefinido que afecta el resultado.
-
-**Parámetros**
-
--   `pattern` — Pattern string. See [Sintaxis de patrón](#sequence-function-pattern-syntax).
-
--   `timestamp` — Column considered to contain time data. Typical data types are `Date` y `DateTime`. También puede utilizar cualquiera de los [UInt](../../sql-reference/data-types/int-uint.md) tipos de datos.
-
--   `cond1`, `cond2` — Conditions that describe the chain of events. Data type: `UInt8`. Puede pasar hasta 32 argumentos de condición. La función sólo tiene en cuenta los eventos descritos en estas condiciones. Si la secuencia contiene datos que no se describen en una condición, la función los omite.
-
-**Valores devueltos**
-
--   1, si el patrón coincide.
--   0, si el patrón no coincide.
-
-Tipo: `UInt8`.
-
-<a name="sequence-function-pattern-syntax"></a>
-**Sintaxis de patrón**
-
--   `(?N)` — Matches the condition argument at position `N`. Las condiciones están numeradas en el `[1, 32]` gama. Por ejemplo, `(?1)` coincide con el argumento pasado al `cond1` parámetro.
-
--   `.*` — Matches any number of events. You don't need conditional arguments to match this element of the pattern.
-
--   `(?t operator value)` — Sets the time in seconds that should separate two events. For example, pattern `(?1)(?t>1800)(?2)` coincide con los eventos que ocurren a más de 1800 segundos el uno del otro. Un número arbitrario de cualquier evento puede estar entre estos eventos. Puede usar el `>=`, `>`, `<`, `<=` operador.
-
-**Ejemplos**
-
-Considere los datos en el `t` tabla:
-
-``` text
-┌─time─┬─number─┐
-│    1 │      1 │
-│    2 │      3 │
-│    3 │      2 │
-└──────┴────────┘
-```
-
-Realizar la consulta:
-
-``` sql
-SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2) FROM t
-```
-
-``` text
-┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2))─┐
-│                                                                     1 │
-└───────────────────────────────────────────────────────────────────────┘
-```
-
-La función encontró la cadena de eventos donde el número 2 sigue al número 1. Se saltó el número 3 entre ellos, porque el número no se describe como un evento. Si queremos tener en cuenta este número al buscar la cadena de eventos dada en el ejemplo, debemos establecer una condición para ello.
-
-``` sql
-SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 3) FROM t
-```
-
-``` text
-┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 3))─┐
-│                                                                                        0 │
-└──────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-En este caso, la función no pudo encontrar la cadena de eventos que coincida con el patrón, porque el evento para el número 3 ocurrió entre 1 y 2. Si en el mismo caso comprobamos la condición para el número 4, la secuencia coincidiría con el patrón.
-
-``` sql
-SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 4) FROM t
-```
-
-``` text
-┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 4))─┐
-│                                                                                        1 │
-└──────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-**Ver también**
-
--   [sequenceCount](#function-sequencecount)
-
-## sequenceCount(pattern)(time, cond1, cond2, …) {#function-sequencecount}
-
-Cuenta el número de cadenas de eventos que coinciden con el patrón. La función busca cadenas de eventos que no se superponen. Comienza a buscar la siguiente cadena después de que se haga coincidir la cadena actual.
-
-!!! warning "Advertencia"
-    Los eventos que ocurren en el mismo segundo pueden estar en la secuencia en un orden indefinido que afecta el resultado.
-
-``` sql
-sequenceCount(pattern)(timestamp, cond1, cond2, ...)
-```
-
-**Parámetros**
-
--   `pattern` — Pattern string. See [Sintaxis de patrón](#sequence-function-pattern-syntax).
-
--   `timestamp` — Column considered to contain time data. Typical data types are `Date` y `DateTime`. También puede utilizar cualquiera de los [UInt](../../sql-reference/data-types/int-uint.md) tipos de datos.
-
--   `cond1`, `cond2` — Conditions that describe the chain of events. Data type: `UInt8`. Puede pasar hasta 32 argumentos de condición. La función sólo tiene en cuenta los eventos descritos en estas condiciones. Si la secuencia contiene datos que no se describen en una condición, la función los omite.
-
-**Valores devueltos**
-
--   Número de cadenas de eventos no superpuestas que coinciden.
-
-Tipo: `UInt64`.
-
-**Ejemplo**
-
-Considere los datos en el `t` tabla:
-
-``` text
-┌─time─┬─number─┐
-│    1 │      1 │
-│    2 │      3 │
-│    3 │      2 │
-│    4 │      1 │
-│    5 │      3 │
-│    6 │      2 │
-└──────┴────────┘
-```
-
-Cuente cuántas veces ocurre el número 2 después del número 1 con cualquier cantidad de otros números entre ellos:
-
-``` sql
-SELECT sequenceCount('(?1).*(?2)')(time, number = 1, number = 2) FROM t
-```
-
-``` text
-┌─sequenceCount('(?1).*(?2)')(time, equals(number, 1), equals(number, 2))─┐
-│                                                                       2 │
-└─────────────────────────────────────────────────────────────────────────┘
-```
-
-**Ver también**
-
--   [sequenceMatch](#function-sequencematch)
-
-## ventanaEmbudo {#windowfunnel}
-
-Busca cadenas de eventos en una ventana de tiempo deslizante y calcula el número máximo de eventos que ocurrieron desde la cadena.
-
-La función funciona de acuerdo con el algoritmo:
-
--   La función busca datos que desencadenan la primera condición en la cadena y establece el contador de eventos en 1. Este es el momento en que comienza la ventana deslizante.
-
--   Si los eventos de la cadena ocurren secuencialmente dentro de la ventana, el contador se incrementa. Si se interrumpe la secuencia de eventos, el contador no se incrementa.
-
--   Si los datos tienen varias cadenas de eventos en diferentes puntos de finalización, la función solo generará el tamaño de la cadena más larga.
-
-**Sintaxis**
-
-``` sql
-windowFunnel(window, [mode])(timestamp, cond1, cond2, ..., condN)
-```
-
-**Parámetros**
-
--   `window` — Length of the sliding window in seconds.
--   `mode` - Es un argumento opcional.
-    -   `'strict'` - Cuando el `'strict'` se establece, windowFunnel() aplica condiciones solo para los valores únicos.
--   `timestamp` — Name of the column containing the timestamp. Data types supported: [Fecha](../../sql-reference/data-types/date.md), [FechaHora](../../sql-reference/data-types/datetime.md#data_type-datetime) y otros tipos de enteros sin signo (tenga en cuenta que aunque timestamp admite el `UInt64` tipo, su valor no puede exceder el máximo de Int64, que es 2 ^ 63 - 1).
--   `cond` — Conditions or data describing the chain of events. [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valor devuelto**
-
-El número máximo de condiciones desencadenadas consecutivas de la cadena dentro de la ventana de tiempo deslizante.
-Se analizan todas las cadenas en la selección.
-
-Tipo: `Integer`.
-
-**Ejemplo**
-
-Determine si un período de tiempo establecido es suficiente para que el usuario seleccione un teléfono y lo compre dos veces en la tienda en línea.
-
-Establezca la siguiente cadena de eventos:
-
-1.  El usuario inició sesión en su cuenta en la tienda (`eventID = 1003`).
-2.  El usuario busca un teléfono (`eventID = 1007, product = 'phone'`).
-3.  El usuario realizó un pedido (`eventID = 1009`).
-4.  El usuario volvió a realizar el pedido (`eventID = 1010`).
-
-Tabla de entrada:
-
-``` text
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-01-28 │       1 │ 2019-01-29 10:00:00 │    1003 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-01-31 │       1 │ 2019-01-31 09:00:00 │    1007 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-01-30 │       1 │ 2019-01-30 08:00:00 │    1009 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-02-01 │       1 │ 2019-02-01 08:00:00 │    1010 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-```
-
-Averigüe hasta qué punto el usuario `user_id` podría atravesar la cadena en un período de enero a febrero de 2019.
-
-Consulta:
-
-``` sql
-SELECT
-    level,
-    count() AS c
-FROM
-(
-    SELECT
-        user_id,
-        windowFunnel(6048000000000000)(timestamp, eventID = 1003, eventID = 1009, eventID = 1007, eventID = 1010) AS level
-    FROM trend
-    WHERE (event_date >= '2019-01-01') AND (event_date <= '2019-02-02')
-    GROUP BY user_id
-)
-GROUP BY level
-ORDER BY level ASC
-```
-
-Resultado:
-
-``` text
-┌─level─┬─c─┐
-│     4 │ 1 │
-└───────┴───┘
-```
-
-## retención {#retention}
-
-La función toma como argumentos un conjunto de condiciones de 1 a 32 argumentos de tipo `UInt8` que indican si se cumplió una determinada condición para el evento.
-Cualquier condición se puede especificar como un argumento (como en [WHERE](../../sql-reference/statements/select/where.md#select-where)).
-
-Las condiciones, excepto la primera, se aplican en pares: el resultado del segundo será verdadero si el primero y el segundo son verdaderos, del tercero si el primero y el fird son verdaderos, etc.
-
-**Sintaxis**
-
-``` sql
-retention(cond1, cond2, ..., cond32);
-```
-
-**Parámetros**
-
--   `cond` — an expression that returns a `UInt8` resultado (1 o 0).
-
-**Valor devuelto**
-
-La matriz de 1 o 0.
-
--   1 — condition was met for the event.
--   0 — condition wasn't met for the event.
-
-Tipo: `UInt8`.
-
-**Ejemplo**
-
-Consideremos un ejemplo de cálculo del `retention` función para determinar el tráfico del sitio.
-
-**1.** Сreate a table to illustrate an example.
-
-``` sql
-CREATE TABLE retention_test(date Date, uid Int32) ENGINE = Memory;
-
-INSERT INTO retention_test SELECT '2020-01-01', number FROM numbers(5);
-INSERT INTO retention_test SELECT '2020-01-02', number FROM numbers(10);
-INSERT INTO retention_test SELECT '2020-01-03', number FROM numbers(15);
-```
-
-Tabla de entrada:
-
-Consulta:
-
-``` sql
-SELECT * FROM retention_test
-```
-
-Resultado:
-
-``` text
-┌───────date─┬─uid─┐
-│ 2020-01-01 │   0 │
-│ 2020-01-01 │   1 │
-│ 2020-01-01 │   2 │
-│ 2020-01-01 │   3 │
-│ 2020-01-01 │   4 │
-└────────────┴─────┘
-┌───────date─┬─uid─┐
-│ 2020-01-02 │   0 │
-│ 2020-01-02 │   1 │
-│ 2020-01-02 │   2 │
-│ 2020-01-02 │   3 │
-│ 2020-01-02 │   4 │
-│ 2020-01-02 │   5 │
-│ 2020-01-02 │   6 │
-│ 2020-01-02 │   7 │
-│ 2020-01-02 │   8 │
-│ 2020-01-02 │   9 │
-└────────────┴─────┘
-┌───────date─┬─uid─┐
-│ 2020-01-03 │   0 │
-│ 2020-01-03 │   1 │
-│ 2020-01-03 │   2 │
-│ 2020-01-03 │   3 │
-│ 2020-01-03 │   4 │
-│ 2020-01-03 │   5 │
-│ 2020-01-03 │   6 │
-│ 2020-01-03 │   7 │
-│ 2020-01-03 │   8 │
-│ 2020-01-03 │   9 │
-│ 2020-01-03 │  10 │
-│ 2020-01-03 │  11 │
-│ 2020-01-03 │  12 │
-│ 2020-01-03 │  13 │
-│ 2020-01-03 │  14 │
-└────────────┴─────┘
-```
-
-**2.** Agrupar usuarios por ID único `uid` utilizando el `retention` función.
-
-Consulta:
-
-``` sql
-SELECT
-    uid,
-    retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r
-FROM retention_test
-WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03')
-GROUP BY uid
-ORDER BY uid ASC
-```
-
-Resultado:
-
-``` text
-┌─uid─┬─r───────┐
-│   0 │ [1,1,1] │
-│   1 │ [1,1,1] │
-│   2 │ [1,1,1] │
-│   3 │ [1,1,1] │
-│   4 │ [1,1,1] │
-│   5 │ [0,0,0] │
-│   6 │ [0,0,0] │
-│   7 │ [0,0,0] │
-│   8 │ [0,0,0] │
-│   9 │ [0,0,0] │
-│  10 │ [0,0,0] │
-│  11 │ [0,0,0] │
-│  12 │ [0,0,0] │
-│  13 │ [0,0,0] │
-│  14 │ [0,0,0] │
-└─────┴─────────┘
-```
-
-**3.** Calcule el número total de visitas al sitio por día.
-
-Consulta:
-
-``` sql
-SELECT
-    sum(r[1]) AS r1,
-    sum(r[2]) AS r2,
-    sum(r[3]) AS r3
-FROM
-(
-    SELECT
-        uid,
-        retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r
-    FROM retention_test
-    WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03')
-    GROUP BY uid
-)
-```
-
-Resultado:
-
-``` text
-┌─r1─┬─r2─┬─r3─┐
-│  5 │  5 │  5 │
-└────┴────┴────┘
-```
-
-Donde:
-
--   `r1`- el número de visitantes únicos que visitaron el sitio durante 2020-01-01 (la `cond1` condición).
--   `r2`- el número de visitantes únicos que visitaron el sitio durante un período de tiempo específico entre 2020-01-01 y 2020-01-02 (`cond1` y `cond2` condición).
--   `r3`- el número de visitantes únicos que visitaron el sitio durante un período de tiempo específico entre 2020-01-01 y 2020-01-03 (`cond1` y `cond3` condición).
-
-## UniqUpTo(N)(x) {#uniquptonx}
-
-Calculates the number of different argument values ​​if it is less than or equal to N. If the number of different argument values is greater than N, it returns N + 1.
-
-Recomendado para usar con Ns pequeños, hasta 10. El valor máximo de N es 100.
-
-Para el estado de una función agregada, utiliza la cantidad de memoria igual a 1 + N \* el tamaño de un valor de bytes.
-Para las cadenas, almacena un hash no criptográfico de 8 bytes. Es decir, el cálculo se aproxima a las cadenas.
-
-La función también funciona para varios argumentos.
-
-Funciona lo más rápido posible, excepto en los casos en que se usa un valor N grande y el número de valores únicos es ligeramente menor que N.
-
-Ejemplo de uso:
-
-``` text
-Problem: Generate a report that shows only keywords that produced at least 5 unique users.
-Solution: Write in the GROUP BY query SearchPhrase HAVING uniqUpTo(4)(UserID) >= 5
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/agg_functions/parametric_functions/) <!--hide-->
-
-## sumMapFiltered(keys_to_keep)(claves, valores) {#summapfilteredkeys-to-keepkeys-values}
-
-El mismo comportamiento que [sumMap](reference.md#agg_functions-summap) excepto que una matriz de claves se pasa como un parámetro. Esto puede ser especialmente útil cuando se trabaja con una alta cardinalidad de claves.
diff --git a/docs/es/sql-reference/aggregate-functions/reference.md b/docs/es/sql-reference/aggregate-functions/reference.md
deleted file mode 100644
index 572c4d010513..000000000000
--- a/docs/es/sql-reference/aggregate-functions/reference.md
+++ /dev/null
@@ -1,1914 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: Referencia
----
-
-# Referencia de función agregada {#aggregate-functions-reference}
-
-## contar {#agg_function-count}
-
-Cuenta el número de filas o valores no NULL.
-
-ClickHouse admite las siguientes sintaxis para `count`:
-- `count(expr)` o `COUNT(DISTINCT expr)`.
-- `count()` o `COUNT(*)`. El `count()` la sintaxis es específica de ClickHouse.
-
-**Parámetros**
-
-La función puede tomar:
-
--   Cero parámetros.
--   Una [expresion](../syntax.md#syntax-expressions).
-
-**Valor devuelto**
-
--   Si se llama a la función sin parámetros, cuenta el número de filas.
--   Si el [expresion](../syntax.md#syntax-expressions) se pasa, entonces la función cuenta cuántas veces esta expresión devuelve no nula. Si la expresión devuelve un [NULL](../../sql-reference/data-types/nullable.md)-type valor, entonces el resultado de `count` no se queda `Nullable`. La función devuelve 0 si la expresión devuelta `NULL` para todas las filas.
-
-En ambos casos el tipo del valor devuelto es [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Detalles**
-
-ClickHouse soporta el `COUNT(DISTINCT ...)` sintaxis. El comportamiento de esta construcción depende del [count_distinct_implementation](../../operations/settings/settings.md#settings-count_distinct_implementation) configuración. Define cuál de las [uniq\*](#agg_function-uniq) se utiliza para realizar la operación. El valor predeterminado es el [uniqExact](#agg_function-uniqexact) función.
-
-El `SELECT count() FROM table` consulta no está optimizado, porque el número de entradas en la tabla no se almacena por separado. Elige una pequeña columna de la tabla y cuenta el número de valores en ella.
-
-**Ejemplos**
-
-Ejemplo 1:
-
-``` sql
-SELECT count() FROM t
-```
-
-``` text
-┌─count()─┐
-│       5 │
-└─────────┘
-```
-
-Ejemplo 2:
-
-``` sql
-SELECT name, value FROM system.settings WHERE name = 'count_distinct_implementation'
-```
-
-``` text
-┌─name──────────────────────────┬─value─────┐
-│ count_distinct_implementation │ uniqExact │
-└───────────────────────────────┴───────────┘
-```
-
-``` sql
-SELECT count(DISTINCT num) FROM t
-```
-
-``` text
-┌─uniqExact(num)─┐
-│              3 │
-└────────────────┘
-```
-
-Este ejemplo muestra que `count(DISTINCT num)` se realiza por el `uniqExact` función según el `count_distinct_implementation` valor de ajuste.
-
-## cualquiera (x) {#agg_function-any}
-
-Selecciona el primer valor encontrado.
-La consulta se puede ejecutar en cualquier orden e incluso en un orden diferente cada vez, por lo que el resultado de esta función es indeterminado.
-Para obtener un resultado determinado, puede usar el ‘min’ o ‘max’ función en lugar de ‘any’.
-
-En algunos casos, puede confiar en el orden de ejecución. Esto se aplica a los casos en que SELECT proviene de una subconsulta que usa ORDER BY.
-
-Cuando un `SELECT` consulta tiene el `GROUP BY` cláusula o al menos una función agregada, ClickHouse (en contraste con MySQL) requiere que todas las expresiones `SELECT`, `HAVING`, y `ORDER BY` las cláusulas pueden calcularse a partir de claves o de funciones agregadas. En otras palabras, cada columna seleccionada de la tabla debe usarse en claves o dentro de funciones agregadas. Para obtener un comportamiento como en MySQL, puede colocar las otras columnas en el `any` función de agregado.
-
-## Cualquier pesado (x) {#anyheavyx}
-
-Selecciona un valor que ocurre con frecuencia [pesos pesados](http://www.cs.umd.edu/~samir/498/karp.pdf) algoritmo. Si hay un valor que se produce más de la mitad de los casos en cada uno de los subprocesos de ejecución de la consulta, se devuelve este valor. Normalmente, el resultado es no determinista.
-
-``` sql
-anyHeavy(column)
-```
-
-**Argumento**
-
--   `column` – The column name.
-
-**Ejemplo**
-
-Tome el [A tiempo](../../getting-started/example-datasets/ontime.md) conjunto de datos y seleccione cualquier valor que ocurra con frecuencia `AirlineID` columna.
-
-``` sql
-SELECT anyHeavy(AirlineID) AS res
-FROM ontime
-```
-
-``` text
-┌───res─┐
-│ 19690 │
-└───────┘
-```
-
-## Cualquier último (x) {#anylastx}
-
-Selecciona el último valor encontrado.
-El resultado es tan indeterminado como para el `any` función.
-
-## Método de codificación de datos: {#groupbitand}
-
-Se aplica bit a bit `AND` para la serie de números.
-
-``` sql
-groupBitAnd(expr)
-```
-
-**Parámetros**
-
-`expr` – An expression that results in `UInt*` tipo.
-
-**Valor de retorno**
-
-Valor de la `UInt*` tipo.
-
-**Ejemplo**
-
-Datos de prueba:
-
-``` text
-binary     decimal
-00101100 = 44
-00011100 = 28
-00001101 = 13
-01010101 = 85
-```
-
-Consulta:
-
-``` sql
-SELECT groupBitAnd(num) FROM t
-```
-
-Donde `num` es la columna con los datos de prueba.
-
-Resultado:
-
-``` text
-binary     decimal
-00000100 = 4
-```
-
-## GrupoBitO {#groupbitor}
-
-Se aplica bit a bit `OR` para la serie de números.
-
-``` sql
-groupBitOr(expr)
-```
-
-**Parámetros**
-
-`expr` – An expression that results in `UInt*` tipo.
-
-**Valor de retorno**
-
-Valor de la `UInt*` tipo.
-
-**Ejemplo**
-
-Datos de prueba:
-
-``` text
-binary     decimal
-00101100 = 44
-00011100 = 28
-00001101 = 13
-01010101 = 85
-```
-
-Consulta:
-
-``` sql
-SELECT groupBitOr(num) FROM t
-```
-
-Donde `num` es la columna con los datos de prueba.
-
-Resultado:
-
-``` text
-binary     decimal
-01111101 = 125
-```
-
-## GrupoBitXor {#groupbitxor}
-
-Se aplica bit a bit `XOR` para la serie de números.
-
-``` sql
-groupBitXor(expr)
-```
-
-**Parámetros**
-
-`expr` – An expression that results in `UInt*` tipo.
-
-**Valor de retorno**
-
-Valor de la `UInt*` tipo.
-
-**Ejemplo**
-
-Datos de prueba:
-
-``` text
-binary     decimal
-00101100 = 44
-00011100 = 28
-00001101 = 13
-01010101 = 85
-```
-
-Consulta:
-
-``` sql
-SELECT groupBitXor(num) FROM t
-```
-
-Donde `num` es la columna con los datos de prueba.
-
-Resultado:
-
-``` text
-binary     decimal
-01101000 = 104
-```
-
-## Método de codificación de datos: {#groupbitmap}
-
-Mapa de bits o cálculos agregados de una columna entera sin signo, devuelve cardinalidad de tipo UInt64, si agrega el sufijo -State, luego devuelve [objeto de mapa de bits](../../sql-reference/functions/bitmap-functions.md).
-
-``` sql
-groupBitmap(expr)
-```
-
-**Parámetros**
-
-`expr` – An expression that results in `UInt*` tipo.
-
-**Valor de retorno**
-
-Valor de la `UInt64` tipo.
-
-**Ejemplo**
-
-Datos de prueba:
-
-``` text
-UserID
-1
-1
-2
-3
-```
-
-Consulta:
-
-``` sql
-SELECT groupBitmap(UserID) as num FROM t
-```
-
-Resultado:
-
-``` text
-num
-3
-```
-
-## Mínimo (x) {#agg_function-min}
-
-Calcula el mínimo.
-
-## máximo (x) {#agg_function-max}
-
-Calcula el máximo.
-
-## ¿Cómo puedo hacerlo?) {#agg-function-argmin}
-
-Calcula el ‘arg’ para un valor mínimo ‘val’ valor. Si hay varios valores diferentes de ‘arg’ para valores mínimos de ‘val’, el primero de estos valores encontrados es la salida.
-
-**Ejemplo:**
-
-``` text
-┌─user─────┬─salary─┐
-│ director │   5000 │
-│ manager  │   3000 │
-│ worker   │   1000 │
-└──────────┴────────┘
-```
-
-``` sql
-SELECT argMin(user, salary) FROM salary
-```
-
-``` text
-┌─argMin(user, salary)─┐
-│ worker               │
-└──────────────────────┘
-```
-
-## Descripción) {#agg-function-argmax}
-
-Calcula el ‘arg’ para un valor máximo ‘val’ valor. Si hay varios valores diferentes de ‘arg’ para valores máximos de ‘val’, el primero de estos valores encontrados es la salida.
-
-## suma (x) {#agg_function-sum}
-
-Calcula la suma.
-Solo funciona para números.
-
-## ¿Cómo puedo obtener más información?) {#sumwithoverflowx}
-
-Calcula la suma de los números, utilizando el mismo tipo de datos para el resultado que para los parámetros de entrada. Si la suma supera el valor máximo para este tipo de datos, la función devuelve un error.
-
-Solo funciona para números.
-
-## Por ejemplo, el valor es el siguiente:)) {#agg_functions-summap}
-
-Totals el ‘value’ matriz de acuerdo con las claves especificadas en el ‘key’ matriz.
-Pasar una tupla de matrices de claves y valores es sinónimo de pasar dos matrices de claves y valores.
-El número de elementos en ‘key’ y ‘value’ debe ser el mismo para cada fila que se sume.
-Returns a tuple of two arrays: keys in sorted order, and values ​​summed for the corresponding keys.
-
-Ejemplo:
-
-``` sql
-CREATE TABLE sum_map(
-    date Date,
-    timeslot DateTime,
-    statusMap Nested(
-        status UInt16,
-        requests UInt64
-    ),
-    statusMapTuple Tuple(Array(Int32), Array(Int32))
-) ENGINE = Log;
-INSERT INTO sum_map VALUES
-    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10], ([1, 2, 3], [10, 10, 10])),
-    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10], ([3, 4, 5], [10, 10, 10])),
-    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10], ([4, 5, 6], [10, 10, 10])),
-    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10], ([6, 7, 8], [10, 10, 10]));
-
-SELECT
-    timeslot,
-    sumMap(statusMap.status, statusMap.requests),
-    sumMap(statusMapTuple)
-FROM sum_map
-GROUP BY timeslot
-```
-
-``` text
-┌────────────timeslot─┬─sumMap(statusMap.status, statusMap.requests)─┬─sumMap(statusMapTuple)─────────┐
-│ 2000-01-01 00:00:00 │ ([1,2,3,4,5],[10,10,20,10,10])               │ ([1,2,3,4,5],[10,10,20,10,10]) │
-│ 2000-01-01 00:01:00 │ ([4,5,6,7,8],[10,10,20,10,10])               │ ([4,5,6,7,8],[10,10,20,10,10]) │
-└─────────────────────┴──────────────────────────────────────────────┴────────────────────────────────┘
-```
-
-## SkewPop {#skewpop}
-
-Calcula el [la asimetría](https://en.wikipedia.org/wiki/Skewness) de una secuencia.
-
-``` sql
-skewPop(expr)
-```
-
-**Parámetros**
-
-`expr` — [Expresion](../syntax.md#syntax-expressions) devolviendo un número.
-
-**Valor devuelto**
-
-The skewness of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md)
-
-**Ejemplo**
-
-``` sql
-SELECT skewPop(value) FROM series_with_value_column
-```
-
-## Sistema abierto {#skewsamp}
-
-Calcula el [asimetría de la muestra](https://en.wikipedia.org/wiki/Skewness) de una secuencia.
-
-Representa una estimación imparcial de la asimetría de una variable aleatoria si los valores pasados forman su muestra.
-
-``` sql
-skewSamp(expr)
-```
-
-**Parámetros**
-
-`expr` — [Expresion](../syntax.md#syntax-expressions) devolviendo un número.
-
-**Valor devuelto**
-
-The skewness of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md). Si `n <= 1` (`n` es el tamaño de la muestra), luego la función devuelve `nan`.
-
-**Ejemplo**
-
-``` sql
-SELECT skewSamp(value) FROM series_with_value_column
-```
-
-## KurtPop {#kurtpop}
-
-Calcula el [curtosis](https://en.wikipedia.org/wiki/Kurtosis) de una secuencia.
-
-``` sql
-kurtPop(expr)
-```
-
-**Parámetros**
-
-`expr` — [Expresion](../syntax.md#syntax-expressions) devolviendo un número.
-
-**Valor devuelto**
-
-The kurtosis of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md)
-
-**Ejemplo**
-
-``` sql
-SELECT kurtPop(value) FROM series_with_value_column
-```
-
-## KurtSamp {#kurtsamp}
-
-Calcula el [curtosis muestra](https://en.wikipedia.org/wiki/Kurtosis) de una secuencia.
-
-Representa una estimación imparcial de la curtosis de una variable aleatoria si los valores pasados forman su muestra.
-
-``` sql
-kurtSamp(expr)
-```
-
-**Parámetros**
-
-`expr` — [Expresion](../syntax.md#syntax-expressions) devolviendo un número.
-
-**Valor devuelto**
-
-The kurtosis of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md). Si `n <= 1` (`n` es un tamaño de la muestra), luego la función devuelve `nan`.
-
-**Ejemplo**
-
-``` sql
-SELECT kurtSamp(value) FROM series_with_value_column
-```
-
-## Acerca de) {#agg_function-avg}
-
-Calcula el promedio.
-Solo funciona para números.
-El resultado es siempre Float64.
-
-## avgPonderado {#avgweighted}
-
-Calcula el [media aritmética ponderada](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean).
-
-**Sintaxis**
-
-``` sql
-avgWeighted(x, weight)
-```
-
-**Parámetros**
-
--   `x` — Values. [Entero](../data-types/int-uint.md) o [punto flotante](../data-types/float.md).
--   `weight` — Weights of the values. [Entero](../data-types/int-uint.md) o [punto flotante](../data-types/float.md).
-
-Tipo de `x` y `weight` debe ser el mismo.
-
-**Valor devuelto**
-
--   Media ponderada.
--   `NaN`. Si todos los pesos son iguales a 0.
-
-Tipo: [Float64](../data-types/float.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT avgWeighted(x, w)
-FROM values('x Int8, w Int8', (4, 1), (1, 0), (10, 2))
-```
-
-Resultado:
-
-``` text
-┌─avgWeighted(x, weight)─┐
-│                      8 │
-└────────────────────────┘
-```
-
-## uniq {#agg_function-uniq}
-
-Calcula el número aproximado de diferentes valores del argumento.
-
-``` sql
-uniq(x[, ...])
-```
-
-**Parámetros**
-
-La función toma un número variable de parámetros. Los parámetros pueden ser `Tuple`, `Array`, `Date`, `DateTime`, `String`, o tipos numéricos.
-
-**Valor devuelto**
-
--   A [UInt64](../../sql-reference/data-types/int-uint.md)-tipo número.
-
-**Detalles de implementación**
-
-Función:
-
--   Calcula un hash para todos los parámetros en el agregado, luego lo usa en los cálculos.
-
--   Utiliza un algoritmo de muestreo adaptativo. Para el estado de cálculo, la función utiliza una muestra de valores hash de elemento de hasta 65536.
-
-        This algorithm is very accurate and very efficient on the CPU. When the query contains several of these functions, using `uniq` is almost as fast as using other aggregate functions.
-
--   Proporciona el resultado de forma determinista (no depende del orden de procesamiento de la consulta).
-
-Recomendamos usar esta función en casi todos los escenarios.
-
-**Ver también**
-
--   [uniqCombined](#agg_function-uniqcombined)
--   [UniqCombined64](#agg_function-uniqcombined64)
--   [uniqHLL12](#agg_function-uniqhll12)
--   [uniqExact](#agg_function-uniqexact)
-
-## uniqCombined {#agg_function-uniqcombined}
-
-Calcula el número aproximado de diferentes valores de argumento.
-
-``` sql
-uniqCombined(HLL_precision)(x[, ...])
-```
-
-El `uniqCombined` es una buena opción para calcular el número de valores diferentes.
-
-**Parámetros**
-
-La función toma un número variable de parámetros. Los parámetros pueden ser `Tuple`, `Array`, `Date`, `DateTime`, `String`, o tipos numéricos.
-
-`HLL_precision` es el logaritmo base-2 del número de células en [HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog). Opcional, puede utilizar la función como `uniqCombined(x[, ...])`. El valor predeterminado para `HLL_precision` es 17, que es efectivamente 96 KiB de espacio (2 ^ 17 celdas, 6 bits cada una).
-
-**Valor devuelto**
-
--   Numero [UInt64](../../sql-reference/data-types/int-uint.md)-tipo número.
-
-**Detalles de implementación**
-
-Función:
-
--   Calcula un hash (hash de 64 bits para `String` y 32 bits de lo contrario) para todos los parámetros en el agregado, luego lo usa en los cálculos.
-
--   Utiliza una combinación de tres algoritmos: matriz, tabla hash e HyperLogLog con una tabla de corrección de errores.
-
-        For a small number of distinct elements, an array is used. When the set size is larger, a hash table is used. For a larger number of elements, HyperLogLog is used, which will occupy a fixed amount of memory.
-
--   Proporciona el resultado de forma determinista (no depende del orden de procesamiento de la consulta).
-
-!!! note "Nota"
-    Dado que usa hash de 32 bits para no-`String` tipo, el resultado tendrá un error muy alto para cardinalidades significativamente mayores que `UINT_MAX` (el error aumentará rápidamente después de unas pocas decenas de miles de millones de valores distintos), por lo tanto, en este caso debe usar [UniqCombined64](#agg_function-uniqcombined64)
-
-En comparación con el [uniq](#agg_function-uniq) función, el `uniqCombined`:
-
--   Consume varias veces menos memoria.
--   Calcula con una precisión varias veces mayor.
--   Por lo general, tiene un rendimiento ligeramente menor. En algunos escenarios, `uniqCombined` puede funcionar mejor que `uniq`, por ejemplo, con consultas distribuidas que transmiten un gran número de estados de agregación a través de la red.
-
-**Ver también**
-
--   [uniq](#agg_function-uniq)
--   [UniqCombined64](#agg_function-uniqcombined64)
--   [uniqHLL12](#agg_function-uniqhll12)
--   [uniqExact](#agg_function-uniqexact)
-
-## UniqCombined64 {#agg_function-uniqcombined64}
-
-Lo mismo que [uniqCombined](#agg_function-uniqcombined), pero utiliza hash de 64 bits para todos los tipos de datos.
-
-## uniqHLL12 {#agg_function-uniqhll12}
-
-Calcula el número aproximado de diferentes valores de argumento [HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog) algoritmo.
-
-``` sql
-uniqHLL12(x[, ...])
-```
-
-**Parámetros**
-
-La función toma un número variable de parámetros. Los parámetros pueden ser `Tuple`, `Array`, `Date`, `DateTime`, `String`, o tipos numéricos.
-
-**Valor devuelto**
-
--   A [UInt64](../../sql-reference/data-types/int-uint.md)-tipo número.
-
-**Detalles de implementación**
-
-Función:
-
--   Calcula un hash para todos los parámetros en el agregado, luego lo usa en los cálculos.
-
--   Utiliza el algoritmo HyperLogLog para aproximar el número de valores de argumento diferentes.
-
-        212 5-bit cells are used. The size of the state is slightly more than 2.5 KB. The result is not very accurate (up to ~10% error) for small data sets (<10K elements). However, the result is fairly accurate for high-cardinality data sets (10K-100M), with a maximum error of ~1.6%. Starting from 100M, the estimation error increases, and the function will return very inaccurate results for data sets with extremely high cardinality (1B+ elements).
-
--   Proporciona el resultado determinado (no depende del orden de procesamiento de la consulta).
-
-No recomendamos usar esta función. En la mayoría de los casos, use el [uniq](#agg_function-uniq) o [uniqCombined](#agg_function-uniqcombined) función.
-
-**Ver también**
-
--   [uniq](#agg_function-uniq)
--   [uniqCombined](#agg_function-uniqcombined)
--   [uniqExact](#agg_function-uniqexact)
-
-## uniqExact {#agg_function-uniqexact}
-
-Calcula el número exacto de diferentes valores de argumento.
-
-``` sql
-uniqExact(x[, ...])
-```
-
-Utilice el `uniqExact` función si necesita absolutamente un resultado exacto. De lo contrario, use el [uniq](#agg_function-uniq) función.
-
-El `uniqExact` función utiliza más memoria que `uniq`, porque el tamaño del estado tiene un crecimiento ilimitado a medida que aumenta el número de valores diferentes.
-
-**Parámetros**
-
-La función toma un número variable de parámetros. Los parámetros pueden ser `Tuple`, `Array`, `Date`, `DateTime`, `String`, o tipos numéricos.
-
-**Ver también**
-
--   [uniq](#agg_function-uniq)
--   [uniqCombined](#agg_function-uniqcombined)
--   [uniqHLL12](#agg_function-uniqhll12)
-
-## ¿Cómo puedo hacerlo?) {#agg_function-grouparray}
-
-Crea una matriz de valores de argumento.
-Los valores se pueden agregar a la matriz en cualquier orden (indeterminado).
-
-La segunda versión (con el `max_size` parámetro) limita el tamaño de la matriz resultante a `max_size` elemento.
-Por ejemplo, `groupArray (1) (x)` es equivalente a `[any (x)]`.
-
-En algunos casos, aún puede confiar en el orden de ejecución. Esto se aplica a los casos en que `SELECT` procede de una subconsulta que utiliza `ORDER BY`.
-
-## GrupoArrayInsertAt {#grouparrayinsertat}
-
-Inserta un valor en la matriz en la posición especificada.
-
-**Sintaxis**
-
-``` sql
-groupArrayInsertAt(default_x, size)(x, pos);
-```
-
-Si en una consulta se insertan varios valores en la misma posición, la función se comporta de las siguientes maneras:
-
--   Si se ejecuta una consulta en un solo subproceso, se utiliza el primero de los valores insertados.
--   Si una consulta se ejecuta en varios subprocesos, el valor resultante es uno indeterminado de los valores insertados.
-
-**Parámetros**
-
--   `x` — Value to be inserted. [Expresion](../syntax.md#syntax-expressions) lo que resulta en uno de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
--   `pos` — Position at which the specified element `x` se va a insertar. La numeración de índices en la matriz comienza desde cero. [UInt32](../../sql-reference/data-types/int-uint.md#uint-ranges).
--   `default_x`— Default value for substituting in empty positions. Optional parameter. [Expresion](../syntax.md#syntax-expressions) dando como resultado el tipo de datos configurado para `x` parámetro. Si `default_x` no está definido, el [valores predeterminados](../../sql-reference/statements/create.md#create-default-values) se utilizan.
--   `size`— Length of the resulting array. Optional parameter. When using this parameter, the default value `default_x` debe ser especificado. [UInt32](../../sql-reference/data-types/int-uint.md#uint-ranges).
-
-**Valor devuelto**
-
--   Matriz con valores insertados.
-
-Tipo: [Matriz](../../sql-reference/data-types/array.md#data-type-array).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT groupArrayInsertAt(toString(number), number * 2) FROM numbers(5);
-```
-
-Resultado:
-
-``` text
-┌─groupArrayInsertAt(toString(number), multiply(number, 2))─┐
-│ ['0','','1','','2','','3','','4']                         │
-└───────────────────────────────────────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT groupArrayInsertAt('-')(toString(number), number * 2) FROM numbers(5);
-```
-
-Resultado:
-
-``` text
-┌─groupArrayInsertAt('-')(toString(number), multiply(number, 2))─┐
-│ ['0','-','1','-','2','-','3','-','4']                          │
-└────────────────────────────────────────────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT groupArrayInsertAt('-', 5)(toString(number), number * 2) FROM numbers(5);
-```
-
-Resultado:
-
-``` text
-┌─groupArrayInsertAt('-', 5)(toString(number), multiply(number, 2))─┐
-│ ['0','-','1','-','2']                                             │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-Inserción multihilo de elementos en una posición.
-
-Consulta:
-
-``` sql
-SELECT groupArrayInsertAt(number, 0) FROM numbers_mt(10) SETTINGS max_block_size = 1;
-```
-
-Como resultado de esta consulta, obtiene un entero aleatorio en el `[0,9]` gama. Por ejemplo:
-
-``` text
-┌─groupArrayInsertAt(number, 0)─┐
-│ [7]                           │
-└───────────────────────────────┘
-```
-
-## groupArrayMovingSum {#agg_function-grouparraymovingsum}
-
-Calcula la suma móvil de los valores de entrada.
-
-``` sql
-groupArrayMovingSum(numbers_for_summing)
-groupArrayMovingSum(window_size)(numbers_for_summing)
-```
-
-La función puede tomar el tamaño de la ventana como un parámetro. Si no se especifica, la función toma el tamaño de ventana igual al número de filas de la columna.
-
-**Parámetros**
-
--   `numbers_for_summing` — [Expresion](../syntax.md#syntax-expressions) dando como resultado un valor de tipo de datos numérico.
--   `window_size` — Size of the calculation window.
-
-**Valores devueltos**
-
--   Matriz del mismo tamaño y tipo que los datos de entrada.
-
-**Ejemplo**
-
-La tabla de ejemplo:
-
-``` sql
-CREATE TABLE t
-(
-    `int` UInt8,
-    `float` Float32,
-    `dec` Decimal32(2)
-)
-ENGINE = TinyLog
-```
-
-``` text
-┌─int─┬─float─┬──dec─┐
-│   1 │   1.1 │ 1.10 │
-│   2 │   2.2 │ 2.20 │
-│   4 │   4.4 │ 4.40 │
-│   7 │  7.77 │ 7.77 │
-└─────┴───────┴──────┘
-```
-
-Consulta:
-
-``` sql
-SELECT
-    groupArrayMovingSum(int) AS I,
-    groupArrayMovingSum(float) AS F,
-    groupArrayMovingSum(dec) AS D
-FROM t
-```
-
-``` text
-┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐
-│ [1,3,7,14] │ [1.1,3.3000002,7.7000003,15.47] │ [1.10,3.30,7.70,15.47] │
-└────────────┴─────────────────────────────────┴────────────────────────┘
-```
-
-``` sql
-SELECT
-    groupArrayMovingSum(2)(int) AS I,
-    groupArrayMovingSum(2)(float) AS F,
-    groupArrayMovingSum(2)(dec) AS D
-FROM t
-```
-
-``` text
-┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐
-│ [1,3,6,11] │ [1.1,3.3000002,6.6000004,12.17] │ [1.10,3.30,6.60,12.17] │
-└────────────┴─────────────────────────────────┴────────────────────────┘
-```
-
-## Método de codificación de datos: {#agg_function-grouparraymovingavg}
-
-Calcula la media móvil de los valores de entrada.
-
-``` sql
-groupArrayMovingAvg(numbers_for_summing)
-groupArrayMovingAvg(window_size)(numbers_for_summing)
-```
-
-La función puede tomar el tamaño de la ventana como un parámetro. Si no se especifica, la función toma el tamaño de ventana igual al número de filas de la columna.
-
-**Parámetros**
-
--   `numbers_for_summing` — [Expresion](../syntax.md#syntax-expressions) dando como resultado un valor de tipo de datos numérico.
--   `window_size` — Size of the calculation window.
-
-**Valores devueltos**
-
--   Matriz del mismo tamaño y tipo que los datos de entrada.
-
-La función utiliza [redondeando hacia cero](https://en.wikipedia.org/wiki/Rounding#Rounding_towards_zero). Trunca los decimales insignificantes para el tipo de datos resultante.
-
-**Ejemplo**
-
-La tabla de ejemplo `b`:
-
-``` sql
-CREATE TABLE t
-(
-    `int` UInt8,
-    `float` Float32,
-    `dec` Decimal32(2)
-)
-ENGINE = TinyLog
-```
-
-``` text
-┌─int─┬─float─┬──dec─┐
-│   1 │   1.1 │ 1.10 │
-│   2 │   2.2 │ 2.20 │
-│   4 │   4.4 │ 4.40 │
-│   7 │  7.77 │ 7.77 │
-└─────┴───────┴──────┘
-```
-
-Consulta:
-
-``` sql
-SELECT
-    groupArrayMovingAvg(int) AS I,
-    groupArrayMovingAvg(float) AS F,
-    groupArrayMovingAvg(dec) AS D
-FROM t
-```
-
-``` text
-┌─I─────────┬─F───────────────────────────────────┬─D─────────────────────┐
-│ [0,0,1,3] │ [0.275,0.82500005,1.9250001,3.8675] │ [0.27,0.82,1.92,3.86] │
-└───────────┴─────────────────────────────────────┴───────────────────────┘
-```
-
-``` sql
-SELECT
-    groupArrayMovingAvg(2)(int) AS I,
-    groupArrayMovingAvg(2)(float) AS F,
-    groupArrayMovingAvg(2)(dec) AS D
-FROM t
-```
-
-``` text
-┌─I─────────┬─F────────────────────────────────┬─D─────────────────────┐
-│ [0,1,3,5] │ [0.55,1.6500001,3.3000002,6.085] │ [0.55,1.65,3.30,6.08] │
-└───────────┴──────────────────────────────────┴───────────────────────┘
-```
-
-## ¿Cómo puedo obtener más información?) {#groupuniqarrayx-groupuniqarraymax-sizex}
-
-Crea una matriz a partir de diferentes valores de argumento. El consumo de memoria es el mismo que para el `uniqExact` función.
-
-La segunda versión (con el `max_size` parámetro) limita el tamaño de la matriz resultante a `max_size` elemento.
-Por ejemplo, `groupUniqArray(1)(x)` es equivalente a `[any(x)]`.
-
-## cuantil {#quantile}
-
-Calcula un aproximado [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos.
-
-Esta función se aplica [muestreo de embalses](https://en.wikipedia.org/wiki/Reservoir_sampling) con un tamaño de depósito de hasta 8192 y un generador de números aleatorios para el muestreo. El resultado es no determinista. Para obtener un cuantil exacto, use el [quantileExact](#quantileexact) función.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantile(level)(expr)
-```
-
-Apodo: `median`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [tipos de datos](../../sql-reference/data-types/index.md#data_types), [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
-
-**Valor devuelto**
-
--   Cuantil aproximado del nivel especificado.
-
-Tipo:
-
--   [Float64](../../sql-reference/data-types/float.md) para la entrada de tipo de datos numéricos.
--   [Fecha](../../sql-reference/data-types/date.md) si los valores de entrada tienen `Date` tipo.
--   [FechaHora](../../sql-reference/data-types/datetime.md) si los valores de entrada tienen `DateTime` tipo.
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` text
-┌─val─┐
-│   1 │
-│   1 │
-│   2 │
-│   3 │
-└─────┘
-```
-
-Consulta:
-
-``` sql
-SELECT quantile(val) FROM t
-```
-
-Resultado:
-
-``` text
-┌─quantile(val)─┐
-│           1.5 │
-└───────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## quantileDeterminista {#quantiledeterministic}
-
-Calcula un aproximado [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos.
-
-Esta función se aplica [muestreo de embalses](https://en.wikipedia.org/wiki/Reservoir_sampling) con un tamaño de depósito de hasta 8192 y algoritmo determinista de muestreo. El resultado es determinista. Para obtener un cuantil exacto, use el [quantileExact](#quantileexact) función.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantileDeterministic(level)(expr, determinator)
-```
-
-Apodo: `medianDeterministic`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [tipos de datos](../../sql-reference/data-types/index.md#data_types), [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
--   `determinator` — Number whose hash is used instead of a random number generator in the reservoir sampling algorithm to make the result of sampling deterministic. As a determinator you can use any deterministic positive number, for example, a user id or an event id. If the same determinator value occures too often, the function works incorrectly.
-
-**Valor devuelto**
-
--   Cuantil aproximado del nivel especificado.
-
-Tipo:
-
--   [Float64](../../sql-reference/data-types/float.md) para la entrada de tipo de datos numéricos.
--   [Fecha](../../sql-reference/data-types/date.md) si los valores de entrada tienen `Date` tipo.
--   [FechaHora](../../sql-reference/data-types/datetime.md) si los valores de entrada tienen `DateTime` tipo.
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` text
-┌─val─┐
-│   1 │
-│   1 │
-│   2 │
-│   3 │
-└─────┘
-```
-
-Consulta:
-
-``` sql
-SELECT quantileDeterministic(val, 1) FROM t
-```
-
-Resultado:
-
-``` text
-┌─quantileDeterministic(val, 1)─┐
-│                           1.5 │
-└───────────────────────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## quantileExact {#quantileexact}
-
-Calcula exactamente el [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos.
-
-To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes `O(n)` memoria, donde `n` es un número de valores que se pasaron. Sin embargo, para un pequeño número de valores, la función es muy efectiva.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantileExact(level)(expr)
-```
-
-Apodo: `medianExact`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [tipos de datos](../../sql-reference/data-types/index.md#data_types), [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
-
-**Valor devuelto**
-
--   Cuantil del nivel especificado.
-
-Tipo:
-
--   [Float64](../../sql-reference/data-types/float.md) para la entrada de tipo de datos numéricos.
--   [Fecha](../../sql-reference/data-types/date.md) si los valores de entrada tienen `Date` tipo.
--   [FechaHora](../../sql-reference/data-types/datetime.md) si los valores de entrada tienen `DateTime` tipo.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT quantileExact(number) FROM numbers(10)
-```
-
-Resultado:
-
-``` text
-┌─quantileExact(number)─┐
-│                     5 │
-└───────────────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## quantileExactWeighted {#quantileexactweighted}
-
-Calcula exactamente el [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos, teniendo en cuenta el peso de cada elemento.
-
-To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Each value is counted with its weight, as if it is present `weight` times. A hash table is used in the algorithm. Because of this, if the passed values ​​are frequently repeated, the function consumes less RAM than [quantileExact](#quantileexact). Puede usar esta función en lugar de `quantileExact` y especifique el peso 1.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantileExactWeighted(level)(expr, weight)
-```
-
-Apodo: `medianExactWeighted`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [tipos de datos](../../sql-reference/data-types/index.md#data_types), [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
--   `weight` — Column with weights of sequence members. Weight is a number of value occurrences.
-
-**Valor devuelto**
-
--   Cuantil del nivel especificado.
-
-Tipo:
-
--   [Float64](../../sql-reference/data-types/float.md) para la entrada de tipo de datos numéricos.
--   [Fecha](../../sql-reference/data-types/date.md) si los valores de entrada tienen `Date` tipo.
--   [FechaHora](../../sql-reference/data-types/datetime.md) si los valores de entrada tienen `DateTime` tipo.
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` text
-┌─n─┬─val─┐
-│ 0 │   3 │
-│ 1 │   2 │
-│ 2 │   1 │
-│ 5 │   4 │
-└───┴─────┘
-```
-
-Consulta:
-
-``` sql
-SELECT quantileExactWeighted(n, val) FROM t
-```
-
-Resultado:
-
-``` text
-┌─quantileExactWeighted(n, val)─┐
-│                             1 │
-└───────────────────────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## quantileTiming {#quantiletiming}
-
-Con la precisión determinada calcula el [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos.
-
-El resultado es determinista (no depende del orden de procesamiento de la consulta). La función está optimizada para trabajar con secuencias que describen distribuciones como tiempos de carga de páginas web o tiempos de respuesta de back-end.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantileTiming(level)(expr)
-```
-
-Apodo: `medianTiming`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
-
--   `expr` — [Expresion](../syntax.md#syntax-expressions) sobre una columna valores que devuelven un [Flotante\*](../../sql-reference/data-types/float.md)-tipo número.
-
-        - If negative values are passed to the function, the behavior is undefined.
-        - If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000.
-
-**Exactitud**
-
-El cálculo es preciso si:
-
--   El número total de valores no supera los 5670.
--   El número total de valores supera los 5670, pero el tiempo de carga de la página es inferior a 1024 ms.
-
-De lo contrario, el resultado del cálculo se redondea al múltiplo más cercano de 16 ms.
-
-!!! note "Nota"
-    Para calcular los cuantiles de tiempo de carga de la página, esta función es más efectiva y precisa que [cuantil](#quantile).
-
-**Valor devuelto**
-
--   Cuantil del nivel especificado.
-
-Tipo: `Float32`.
-
-!!! note "Nota"
-    Si no se pasan valores a la función (cuando se `quantileTimingIf`), [NaN](../../sql-reference/data-types/float.md#data_type-float-nan-inf) se devuelve. El propósito de esto es diferenciar estos casos de los casos que resultan en cero. Ver [ORDER BY cláusula](../statements/select/order-by.md#select-order-by) para notas sobre la clasificación `NaN` valor.
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` text
-┌─response_time─┐
-│            72 │
-│           112 │
-│           126 │
-│           145 │
-│           104 │
-│           242 │
-│           313 │
-│           168 │
-│           108 │
-└───────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT quantileTiming(response_time) FROM t
-```
-
-Resultado:
-
-``` text
-┌─quantileTiming(response_time)─┐
-│                           126 │
-└───────────────────────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## quantileTimingWeighted {#quantiletimingweighted}
-
-Con la precisión determinada calcula el [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos según el peso de cada miembro de secuencia.
-
-El resultado es determinista (no depende del orden de procesamiento de la consulta). La función está optimizada para trabajar con secuencias que describen distribuciones como tiempos de carga de páginas web o tiempos de respuesta de back-end.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantileTimingWeighted(level)(expr, weight)
-```
-
-Apodo: `medianTimingWeighted`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
-
--   `expr` — [Expresion](../syntax.md#syntax-expressions) sobre una columna valores que devuelven un [Flotante\*](../../sql-reference/data-types/float.md)-tipo número.
-
-        - If negative values are passed to the function, the behavior is undefined.
-        - If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000.
-
--   `weight` — Column with weights of sequence elements. Weight is a number of value occurrences.
-
-**Exactitud**
-
-El cálculo es preciso si:
-
--   El número total de valores no supera los 5670.
--   El número total de valores supera los 5670, pero el tiempo de carga de la página es inferior a 1024 ms.
-
-De lo contrario, el resultado del cálculo se redondea al múltiplo más cercano de 16 ms.
-
-!!! note "Nota"
-    Para calcular los cuantiles de tiempo de carga de la página, esta función es más efectiva y precisa que [cuantil](#quantile).
-
-**Valor devuelto**
-
--   Cuantil del nivel especificado.
-
-Tipo: `Float32`.
-
-!!! note "Nota"
-    Si no se pasan valores a la función (cuando se `quantileTimingIf`), [NaN](../../sql-reference/data-types/float.md#data_type-float-nan-inf) se devuelve. El propósito de esto es diferenciar estos casos de los casos que resultan en cero. Ver [ORDER BY cláusula](../statements/select/order-by.md#select-order-by) para notas sobre la clasificación `NaN` valor.
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` text
-┌─response_time─┬─weight─┐
-│            68 │      1 │
-│           104 │      2 │
-│           112 │      3 │
-│           126 │      2 │
-│           138 │      1 │
-│           162 │      1 │
-└───────────────┴────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT quantileTimingWeighted(response_time, weight) FROM t
-```
-
-Resultado:
-
-``` text
-┌─quantileTimingWeighted(response_time, weight)─┐
-│                                           112 │
-└───────────────────────────────────────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## quantileTDigest {#quantiletdigest}
-
-Calcula un aproximado [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos usando el [T-digest](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf) algoritmo.
-
-El error máximo es 1%. El consumo de memoria es `log(n)`, donde `n` es un número de valores. El resultado depende del orden de ejecución de la consulta y no es determinista.
-
-El rendimiento de la función es menor que el rendimiento de [cuantil](#quantile) o [quantileTiming](#quantiletiming). En términos de la relación entre el tamaño del estado y la precisión, esta función es mucho mejor que `quantile`.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantileTDigest(level)(expr)
-```
-
-Apodo: `medianTDigest`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [tipos de datos](../../sql-reference/data-types/index.md#data_types), [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
-
-**Valor devuelto**
-
--   Cuantil aproximado del nivel especificado.
-
-Tipo:
-
--   [Float64](../../sql-reference/data-types/float.md) para la entrada de tipo de datos numéricos.
--   [Fecha](../../sql-reference/data-types/date.md) si los valores de entrada tienen `Date` tipo.
--   [FechaHora](../../sql-reference/data-types/datetime.md) si los valores de entrada tienen `DateTime` tipo.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT quantileTDigest(number) FROM numbers(10)
-```
-
-Resultado:
-
-``` text
-┌─quantileTDigest(number)─┐
-│                     4.5 │
-└─────────────────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## quantileTDigestWeighted {#quantiletdigestweighted}
-
-Calcula un aproximado [cuantil](https://en.wikipedia.org/wiki/Quantile) de una secuencia de datos numéricos usando el [T-digest](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf) algoritmo. La función tiene en cuenta el peso de cada miembro de secuencia. El error máximo es 1%. El consumo de memoria es `log(n)`, donde `n` es un número de valores.
-
-El rendimiento de la función es menor que el rendimiento de [cuantil](#quantile) o [quantileTiming](#quantiletiming). En términos de la relación entre el tamaño del estado y la precisión, esta función es mucho mejor que `quantile`.
-
-El resultado depende del orden de ejecución de la consulta y no es determinista.
-
-Cuando se utilizan múltiples `quantile*` funciones con diferentes niveles en una consulta, los estados internos no se combinan (es decir, la consulta funciona de manera menos eficiente de lo que podría). En este caso, use el [cantiles](#quantiles) función.
-
-**Sintaxis**
-
-``` sql
-quantileTDigest(level)(expr)
-```
-
-Apodo: `medianTDigest`.
-
-**Parámetros**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` valor en el rango de `[0.01, 0.99]`. Valor predeterminado: 0.5. En `level=0.5` la función calcula [mediana](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [tipos de datos](../../sql-reference/data-types/index.md#data_types), [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
--   `weight` — Column with weights of sequence elements. Weight is a number of value occurrences.
-
-**Valor devuelto**
-
--   Cuantil aproximado del nivel especificado.
-
-Tipo:
-
--   [Float64](../../sql-reference/data-types/float.md) para la entrada de tipo de datos numéricos.
--   [Fecha](../../sql-reference/data-types/date.md) si los valores de entrada tienen `Date` tipo.
--   [FechaHora](../../sql-reference/data-types/datetime.md) si los valores de entrada tienen `DateTime` tipo.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT quantileTDigestWeighted(number, 1) FROM numbers(10)
-```
-
-Resultado:
-
-``` text
-┌─quantileTDigestWeighted(number, 1)─┐
-│                                4.5 │
-└────────────────────────────────────┘
-```
-
-**Ver también**
-
--   [mediana](#median)
--   [cantiles](#quantiles)
-
-## mediana {#median}
-
-El `median*` funciones son los alias para el `quantile*` función. Calculan la mediana de una muestra de datos numéricos.
-
-Función:
-
--   `median` — Alias for [cuantil](#quantile).
--   `medianDeterministic` — Alias for [quantileDeterminista](#quantiledeterministic).
--   `medianExact` — Alias for [quantileExact](#quantileexact).
--   `medianExactWeighted` — Alias for [quantileExactWeighted](#quantileexactweighted).
--   `medianTiming` — Alias for [quantileTiming](#quantiletiming).
--   `medianTimingWeighted` — Alias for [quantileTimingWeighted](#quantiletimingweighted).
--   `medianTDigest` — Alias for [quantileTDigest](#quantiletdigest).
--   `medianTDigestWeighted` — Alias for [quantileTDigestWeighted](#quantiletdigestweighted).
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` text
-┌─val─┐
-│   1 │
-│   1 │
-│   2 │
-│   3 │
-└─────┘
-```
-
-Consulta:
-
-``` sql
-SELECT medianDeterministic(val, 1) FROM t
-```
-
-Resultado:
-
-``` text
-┌─medianDeterministic(val, 1)─┐
-│                         1.5 │
-└─────────────────────────────┘
-```
-
-## quantiles(level1, level2, …)(x) {#quantiles}
-
-Todas las funciones de cuantiles también tienen funciones de cuantiles correspondientes: `quantiles`, `quantilesDeterministic`, `quantilesTiming`, `quantilesTimingWeighted`, `quantilesExact`, `quantilesExactWeighted`, `quantilesTDigest`. Estas funciones calculan todos los cuantiles de los niveles enumerados en una sola pasada y devuelven una matriz de los valores resultantes.
-
-## Acerca de Nosotros) {#varsampx}
-
-Calcula la cantidad `Σ((x - x̅)^2) / (n - 1)`, donde `n` es el tamaño de la muestra y `x̅`es el valor promedio de `x`.
-
-Representa una estimación imparcial de la varianza de una variable aleatoria si los valores pasados forman su muestra.
-
-Devoluciones `Float64`. Cuando `n <= 1`, devoluciones `+∞`.
-
-!!! note "Nota"
-    Esta función utiliza un algoritmo numéricamente inestable. Si necesita [estabilidad numérica](https://en.wikipedia.org/wiki/Numerical_stability) en los cálculos, utilice el `varSampStable` función. Funciona más lento, pero proporciona un menor error computacional.
-
-## Nombre de la red inalámbrica (SSID):) {#varpopx}
-
-Calcula la cantidad `Σ((x - x̅)^2) / n`, donde `n` es el tamaño de la muestra y `x̅`es el valor promedio de `x`.
-
-En otras palabras, dispersión para un conjunto de valores. Devoluciones `Float64`.
-
-!!! note "Nota"
-    Esta función utiliza un algoritmo numéricamente inestable. Si necesita [estabilidad numérica](https://en.wikipedia.org/wiki/Numerical_stability) en los cálculos, utilice el `varPopStable` función. Funciona más lento, pero proporciona un menor error computacional.
-
-## Soporte técnico) {#stddevsampx}
-
-El resultado es igual a la raíz cuadrada de `varSamp(x)`.
-
-!!! note "Nota"
-    Esta función utiliza un algoritmo numéricamente inestable. Si necesita [estabilidad numérica](https://en.wikipedia.org/wiki/Numerical_stability) en los cálculos, utilice el `stddevSampStable` función. Funciona más lento, pero proporciona un menor error computacional.
-
-## stddevPop(x) {#stddevpopx}
-
-El resultado es igual a la raíz cuadrada de `varPop(x)`.
-
-!!! note "Nota"
-    Esta función utiliza un algoritmo numéricamente inestable. Si necesita [estabilidad numérica](https://en.wikipedia.org/wiki/Numerical_stability) en los cálculos, utilice el `stddevPopStable` función. Funciona más lento, pero proporciona un menor error computacional.
-
-## topK(N)(x) {#topknx}
-
-Devuelve una matriz de los valores aproximadamente más frecuentes de la columna especificada. La matriz resultante se ordena en orden descendente de frecuencia aproximada de valores (no por los valores mismos).
-
-Implementa el [Ahorro de espacio filtrado](http://www.l2f.inesc-id.pt/~fmmb/wiki/uploads/Work/misnis.ref0a.pdf) algoritmo para analizar TopK, basado en el algoritmo de reducción y combinación de [Ahorro de espacio paralelo](https://arxiv.org/pdf/1401.0702.pdf).
-
-``` sql
-topK(N)(column)
-```
-
-Esta función no proporciona un resultado garantizado. En ciertas situaciones, pueden producirse errores y pueden devolver valores frecuentes que no son los valores más frecuentes.
-
-Recomendamos usar el `N < 10` valor; el rendimiento se reduce con grandes `N` valor. Valor máximo de `N = 65536`.
-
-**Parámetros**
-
--   ‘N’ es el número de elementos a devolver.
-
-Si se omite el parámetro, se utiliza el valor predeterminado 10.
-
-**Argumento**
-
--   ' x ' – The value to calculate frequency.
-
-**Ejemplo**
-
-Tome el [A tiempo](../../getting-started/example-datasets/ontime.md) conjunto de datos y seleccione los tres valores más frecuentes `AirlineID` columna.
-
-``` sql
-SELECT topK(3)(AirlineID) AS res
-FROM ontime
-```
-
-``` text
-┌─res─────────────────┐
-│ [19393,19790,19805] │
-└─────────────────────┘
-```
-
-## topKPeso {#topkweighted}
-
-Similar a `topK` pero toma un argumento adicional de tipo entero - `weight`. Cada valor se contabiliza `weight` veces para el cálculo de la frecuencia.
-
-**Sintaxis**
-
-``` sql
-topKWeighted(N)(x, weight)
-```
-
-**Parámetros**
-
--   `N` — The number of elements to return.
-
-**Argumento**
-
--   `x` – The value.
--   `weight` — The weight. [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valor devuelto**
-
-Devuelve una matriz de los valores con la suma aproximada máxima de pesos.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT topKWeighted(10)(number, number) FROM numbers(1000)
-```
-
-Resultado:
-
-``` text
-┌─topKWeighted(10)(number, number)──────────┐
-│ [999,998,997,996,995,994,993,992,991,990] │
-└───────────────────────────────────────────┘
-```
-
-## covarSamp(x, y) {#covarsampx-y}
-
-Calcula el valor de `Σ((x - x̅)(y - y̅)) / (n - 1)`.
-
-Devuelve Float64. Cuando `n <= 1`, returns +∞.
-
-!!! note "Nota"
-    Esta función utiliza un algoritmo numéricamente inestable. Si necesita [estabilidad numérica](https://en.wikipedia.org/wiki/Numerical_stability) en los cálculos, utilice el `covarSampStable` función. Funciona más lento, pero proporciona un menor error computacional.
-
-## covarPop(x, y) {#covarpopx-y}
-
-Calcula el valor de `Σ((x - x̅)(y - y̅)) / n`.
-
-!!! note "Nota"
-    Esta función utiliza un algoritmo numéricamente inestable. Si necesita [estabilidad numérica](https://en.wikipedia.org/wiki/Numerical_stability) en los cálculos, utilice el `covarPopStable` función. Funciona más lento pero proporciona un menor error computacional.
-
-## corr(x, y) {#corrx-y}
-
-Calcula el coeficiente de correlación de Pearson: `Σ((x - x̅)(y - y̅)) / sqrt(Σ((x - x̅)^2) * Σ((y - y̅)^2))`.
-
-!!! note "Nota"
-    Esta función utiliza un algoritmo numéricamente inestable. Si necesita [estabilidad numérica](https://en.wikipedia.org/wiki/Numerical_stability) en los cálculos, utilice el `corrStable` función. Funciona más lento, pero proporciona un menor error computacional.
-
-## categoricalInformationValue {#categoricalinformationvalue}
-
-Calcula el valor de `(P(tag = 1) - P(tag = 0))(log(P(tag = 1)) - log(P(tag = 0)))` para cada categoría.
-
-``` sql
-categoricalInformationValue(category1, category2, ..., tag)
-```
-
-El resultado indica cómo una característica discreta (categórica `[category1, category2, ...]` contribuir a un modelo de aprendizaje que predice el valor de `tag`.
-
-## SimpleLinearRegression {#simplelinearregression}
-
-Realiza una regresión lineal simple (unidimensional).
-
-``` sql
-simpleLinearRegression(x, y)
-```
-
-Parámetros:
-
--   `x` — Column with dependent variable values.
--   `y` — Column with explanatory variable values.
-
-Valores devueltos:
-
-Constante `(a, b)` de la línea resultante `y = a*x + b`.
-
-**Ejemplos**
-
-``` sql
-SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3])
-```
-
-``` text
-┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3])─┐
-│ (1,0)                                                             │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6])
-```
-
-``` text
-┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6])─┐
-│ (1,3)                                                             │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-## stochasticLinearRegression {#agg_functions-stochasticlinearregression}
-
-Esta función implementa la regresión lineal estocástica. Admite parámetros personalizados para la tasa de aprendizaje, el coeficiente de regularización L2, el tamaño de mini lote y tiene pocos métodos para actualizar los pesos ([Adán](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) (utilizado por defecto), [SGD simple](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), [Impulso](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum), [Nesterov](https://mipt.ru/upload/medialibrary/d7e/41-91.pdf)).
-
-### Parámetros {#agg_functions-stochasticlinearregression-parameters}
-
-Hay 4 parámetros personalizables. Se pasan a la función secuencialmente, pero no es necesario pasar los cuatro; se usarán valores predeterminados, sin embargo, un buen modelo requirió algún ajuste de parámetros.
-
-``` text
-stochasticLinearRegression(1.0, 1.0, 10, 'SGD')
-```
-
-1.  `learning rate` es el coeficiente en la longitud del paso, cuando se realiza el paso de descenso de gradiente. Una tasa de aprendizaje demasiado grande puede causar pesos infinitos del modelo. El valor predeterminado es `0.00001`.
-2.  `l2 regularization coefficient` que puede ayudar a prevenir el sobreajuste. El valor predeterminado es `0.1`.
-3.  `mini-batch size` establece el número de elementos, cuyos gradientes se calcularán y sumarán para realizar un paso de descenso de gradiente. El descenso estocástico puro usa un elemento, sin embargo, tener lotes pequeños (aproximadamente 10 elementos) hace que los pasos de gradiente sean más estables. El valor predeterminado es `15`.
-4.  `method for updating weights`, son: `Adam` (predeterminada), `SGD`, `Momentum`, `Nesterov`. `Momentum` y `Nesterov` requieren un poco más de cálculos y memoria, sin embargo, resultan útiles en términos de velocidad de convergencia y estabilidad de los métodos de gradiente estocásticos.
-
-### Uso {#agg_functions-stochasticlinearregression-usage}
-
-`stochasticLinearRegression` se utiliza en dos pasos: ajustar el modelo y predecir nuevos datos. Para ajustar el modelo y guardar su estado para su uso posterior, utilizamos `-State` combinador, que básicamente guarda el estado (pesos del modelo, etc.).
-Para predecir usamos la función [evalMLMethod](../functions/machine-learning-functions.md#machine_learning_methods-evalmlmethod), que toma un estado como argumento, así como características para predecir.
-
-<a name="stochasticlinearregression-usage-fitting"></a>
-
-**1.** Accesorio
-
-Dicha consulta puede ser utilizada.
-
-``` sql
-CREATE TABLE IF NOT EXISTS train_data
-(
-    param1 Float64,
-    param2 Float64,
-    target Float64
-) ENGINE = Memory;
-
-CREATE TABLE your_model ENGINE = Memory AS SELECT
-stochasticLinearRegressionState(0.1, 0.0, 5, 'SGD')(target, param1, param2)
-AS state FROM train_data;
-```
-
-Aquí también tenemos que insertar datos en `train_data` tabla. El número de parámetros no es fijo, depende solo del número de argumentos, pasados a `linearRegressionState`. Todos deben ser valores numéricos.
-Tenga en cuenta que la columna con valor objetivo (que nos gustaría aprender a predecir) se inserta como primer argumento.
-
-**2.** Predecir
-
-Después de guardar un estado en la tabla, podemos usarlo varias veces para la predicción, o incluso fusionarlo con otros estados y crear nuevos modelos aún mejores.
-
-``` sql
-WITH (SELECT state FROM your_model) AS model SELECT
-evalMLMethod(model, param1, param2) FROM test_data
-```
-
-La consulta devolverá una columna de valores predichos. Tenga en cuenta que el primer argumento de `evalMLMethod` ser `AggregateFunctionState` objeto, siguiente son columnas de características.
-
-`test_data` es una mesa como `train_data` pero puede no contener el valor objetivo.
-
-### Nota {#agg_functions-stochasticlinearregression-notes}
-
-1.  Para fusionar dos modelos, el usuario puede crear dicha consulta:
-    `sql  SELECT state1 + state2 FROM your_models`
-    donde `your_models` la tabla contiene ambos modelos. Esta consulta devolverá un nuevo `AggregateFunctionState` objeto.
-
-2.  El usuario puede obtener pesos del modelo creado para sus propios fines sin guardar el modelo si no `-State` combinador se utiliza.
-    `sql  SELECT stochasticLinearRegression(0.01)(target, param1, param2) FROM train_data`
-    Dicha consulta se ajustará al modelo y devolverá sus pesos: primero son los pesos, que corresponden a los parámetros del modelo, el último es el sesgo. Entonces, en el ejemplo anterior, la consulta devolverá una columna con 3 valores.
-
-**Ver también**
-
--   [stochasticLogisticRegression](#agg_functions-stochasticlogisticregression)
--   [Diferencia entre regresiones lineales y logísticas](https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression)
-
-## stochasticLogisticRegression {#agg_functions-stochasticlogisticregression}
-
-Esta función implementa la regresión logística estocástica. Se puede usar para problemas de clasificación binaria, admite los mismos parámetros personalizados que stochasticLinearRegression y funciona de la misma manera.
-
-### Parámetros {#agg_functions-stochasticlogisticregression-parameters}
-
-Los parámetros son exactamente los mismos que en stochasticLinearRegression:
-`learning rate`, `l2 regularization coefficient`, `mini-batch size`, `method for updating weights`.
-Para obtener más información, consulte [parámetros](#agg_functions-stochasticlinearregression-parameters).
-
-``` text
-stochasticLogisticRegression(1.0, 1.0, 10, 'SGD')
-```
-
-1.  Accesorio
-
-<!-- -->
-
-    See the `Fitting` section in the [stochasticLinearRegression](#stochasticlinearregression-usage-fitting) description.
-
-    Predicted labels have to be in \[-1, 1\].
-
-1.  Predecir
-
-<!-- -->
-
-    Using saved state we can predict probability of object having label `1`.
-
-    ``` sql
-    WITH (SELECT state FROM your_model) AS model SELECT
-    evalMLMethod(model, param1, param2) FROM test_data
-    ```
-
-    The query will return a column of probabilities. Note that first argument of `evalMLMethod` is `AggregateFunctionState` object, next are columns of features.
-
-    We can also set a bound of probability, which assigns elements to different labels.
-
-    ``` sql
-    SELECT ans < 1.1 AND ans > 0.5 FROM
-    (WITH (SELECT state FROM your_model) AS model SELECT
-    evalMLMethod(model, param1, param2) AS ans FROM test_data)
-    ```
-
-    Then the result will be labels.
-
-    `test_data` is a table like `train_data` but may not contain target value.
-
-**Ver también**
-
--   [stochasticLinearRegression](#agg_functions-stochasticlinearregression)
--   [Diferencia entre regresiones lineales y logísticas.](https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression)
-
-## Método de codificación de datos: {#groupbitmapand}
-
-Calcula el AND de una columna de mapa de bits, devuelve la cardinalidad del tipo UInt64, si agrega el sufijo -State, luego devuelve [objeto de mapa de bits](../../sql-reference/functions/bitmap-functions.md).
-
-``` sql
-groupBitmapAnd(expr)
-```
-
-**Parámetros**
-
-`expr` – An expression that results in `AggregateFunction(groupBitmap, UInt*)` tipo.
-
-**Valor de retorno**
-
-Valor de la `UInt64` tipo.
-
-**Ejemplo**
-
-``` sql
-DROP TABLE IF EXISTS bitmap_column_expr_test2;
-CREATE TABLE bitmap_column_expr_test2
-(
-    tag_id String,
-    z AggregateFunction(groupBitmap, UInt32)
-)
-ENGINE = MergeTree
-ORDER BY tag_id;
-
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32))));
-
-SELECT groupBitmapAnd(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─groupBitmapAnd(z)─┐
-│               3   │
-└───────────────────┘
-
-SELECT arraySort(bitmapToArray(groupBitmapAndState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─arraySort(bitmapToArray(groupBitmapAndState(z)))─┐
-│ [6,8,10]                                         │
-└──────────────────────────────────────────────────┘
-```
-
-## Método de codificación de datos: {#groupbitmapor}
-
-Calcula el OR de una columna de mapa de bits, devuelve la cardinalidad del tipo UInt64, si agrega el sufijo -State, luego devuelve [objeto de mapa de bits](../../sql-reference/functions/bitmap-functions.md). Esto es equivalente a `groupBitmapMerge`.
-
-``` sql
-groupBitmapOr(expr)
-```
-
-**Parámetros**
-
-`expr` – An expression that results in `AggregateFunction(groupBitmap, UInt*)` tipo.
-
-**Valor de retorno**
-
-Valor de la `UInt64` tipo.
-
-**Ejemplo**
-
-``` sql
-DROP TABLE IF EXISTS bitmap_column_expr_test2;
-CREATE TABLE bitmap_column_expr_test2
-(
-    tag_id String,
-    z AggregateFunction(groupBitmap, UInt32)
-)
-ENGINE = MergeTree
-ORDER BY tag_id;
-
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32))));
-
-SELECT groupBitmapOr(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─groupBitmapOr(z)─┐
-│             15   │
-└──────────────────┘
-
-SELECT arraySort(bitmapToArray(groupBitmapOrState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─arraySort(bitmapToArray(groupBitmapOrState(z)))─┐
-│ [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]           │
-└─────────────────────────────────────────────────┘
-```
-
-## Método de codificación de datos: {#groupbitmapxor}
-
-Calcula el XOR de una columna de mapa de bits, devuelve la cardinalidad del tipo UInt64, si agrega el sufijo -State, luego devuelve [objeto de mapa de bits](../../sql-reference/functions/bitmap-functions.md).
-
-``` sql
-groupBitmapOr(expr)
-```
-
-**Parámetros**
-
-`expr` – An expression that results in `AggregateFunction(groupBitmap, UInt*)` tipo.
-
-**Valor de retorno**
-
-Valor de la `UInt64` tipo.
-
-**Ejemplo**
-
-``` sql
-DROP TABLE IF EXISTS bitmap_column_expr_test2;
-CREATE TABLE bitmap_column_expr_test2
-(
-    tag_id String,
-    z AggregateFunction(groupBitmap, UInt32)
-)
-ENGINE = MergeTree
-ORDER BY tag_id;
-
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32))));
-
-SELECT groupBitmapXor(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─groupBitmapXor(z)─┐
-│              10   │
-└───────────────────┘
-
-SELECT arraySort(bitmapToArray(groupBitmapXorState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─arraySort(bitmapToArray(groupBitmapXorState(z)))─┐
-│ [1,3,5,6,8,10,11,13,14,15]                       │
-└──────────────────────────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/agg_functions/reference/) <!--hide-->
diff --git a/docs/es/sql-reference/ansi.md b/docs/es/sql-reference/ansi.md
deleted file mode 100644
index 29e2c5b12e92..000000000000
--- a/docs/es/sql-reference/ansi.md
+++ /dev/null
@@ -1,180 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: ad252bbb4f7e2899c448eb42ecc39ff195c8faa1
-toc_priority: 40
-toc_title: Compatibilidad con ANSI
----
-
-# Compatibilidad de SQL ANSI de ClickHouse SQL Dialect {#ansi-sql-compatibility-of-clickhouse-sql-dialect}
-
-!!! note "Nota"
-    Este artículo se basa en la Tabla 38, “Feature taxonomy and definition for mandatory features”, Annex F of ISO/IEC CD 9075-2:2013.
-
-## Diferencias en el comportamiento {#differences-in-behaviour}
-
-En la tabla siguiente se enumeran los casos en que la característica de consulta funciona en ClickHouse, pero no se comporta como se especifica en ANSI SQL.
-
-| Feature ID | Nombre de la función                               | Diferencia                                                                                                                                 |
-|------------|----------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|
-| E011       | Tipos de datos numéricos                           | El literal numérico con punto se interpreta como aproximado (`Float64`) en lugar de exacta (`Decimal`)                                     |
-| E051-05    | Los elementos seleccionados pueden ser renombrados | Los cambios de nombre de los elementos tienen un alcance de visibilidad más amplio que solo el resultado SELECT                            |
-| E141-01    | Restricciones NOT NULL                             | `NOT NULL` está implícito para las columnas de tabla de forma predeterminada                                                               |
-| E011-04    | Operadores aritméticos                             | ClickHouse se desborda en lugar de la aritmética comprobada y cambia el tipo de datos de resultado en función de las reglas personalizadas |
-
-## Estado de la función {#feature-status}
-
-| Feature ID | Nombre de la función                                                                                                                                    | Estatus                    | Comentario                                                                                                                                                                                                 |
-|------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| **E011**   | **Tipos de datos numéricos**                                                                                                                            | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E011-01    | Tipos de datos INTEGER y SMALLINT                                                                                                                       | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E011-02    | REAL, DOUBLE PRECISION y FLOAT tipos de datos tipos de datos                                                                                            | Parcial {.text-warning}     | `FLOAT(<binary_precision>)`, `REAL` y `DOUBLE PRECISION` no son compatibles                                                                                                                                |
-| E011-03    | Tipos de datos DECIMAL y NUMERIC                                                                                                                        | Parcial {.text-warning}     | Solo `DECIMAL(p,s)` es compatible, no `NUMERIC`                                                                                                                                                            |
-| E011-04    | Operadores aritméticos                                                                                                                                  | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E011-05    | Comparación numérica                                                                                                                                    | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E011-06    | Conversión implícita entre los tipos de datos numéricos                                                                                                 | No {.text-danger}           | ANSI SQL permite la conversión implícita arbitraria entre tipos numéricos, mientras que ClickHouse se basa en funciones que tienen múltiples sobrecargas en lugar de conversión implícita                  |
-| **E021**   | **Tipos de cadena de caracteres**                                                                                                                       | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E021-01    | Tipo de datos CHARACTER                                                                                                                                 | No {.text-danger}           |                                                                                                                                                                                                            |
-| E021-02    | Tipo de datos CHARACTER VARYING                                                                                                                         | No {.text-danger}           | `String` se comporta de manera similar, pero sin límite de longitud entre paréntesis                                                                                                                       |
-| E021-03    | Literales de caracteres                                                                                                                                 | Parcial {.text-warning}     | Sin concatenación automática de literales consecutivos y compatibilidad con el conjunto de caracteres                                                                                                      |
-| E021-04    | Función CHARACTER_LENGTH                                                                                                                               | Parcial {.text-warning}     | No `USING` clausula                                                                                                                                                                                        |
-| E021-05    | Función OCTET_LENGTH                                                                                                                                   | No {.text-danger}           | `LENGTH` se comporta de manera similar                                                                                                                                                                     |
-| E021-06    | SUBSTRING                                                                                                                                               | Parcial {.text-warning}     | No hay soporte para `SIMILAR` y `ESCAPE` cláusulas, no `SUBSTRING_REGEX` variante                                                                                                                          |
-| E021-07    | Concatenación de caracteres                                                                                                                             | Parcial {.text-warning}     | No `COLLATE` clausula                                                                                                                                                                                      |
-| E021-08    | Funciones SUPERIOR e INFERIOR                                                                                                                           | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E021-09    | Función TRIM                                                                                                                                            | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E021-10    | Conversión implícita entre los tipos de cadena de caracteres de longitud fija y longitud variable                                                       | No {.text-danger}           | ANSI SQL permite la conversión implícita arbitraria entre tipos de cadena, mientras que ClickHouse se basa en funciones que tienen múltiples sobrecargas en lugar de conversión implícita                  |
-| E021-11    | Función POSITION                                                                                                                                        | Parcial {.text-warning}     | No hay soporte para `IN` y `USING` cláusulas, no `POSITION_REGEX` variante                                                                                                                                 |
-| E021-12    | Comparación de caracteres                                                                                                                               | Sí {.text-success}          |                                                                                                                                                                                                            |
-| **E031**   | **Identificador**                                                                                                                                       | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E031-01    | Identificadores delimitados                                                                                                                             | Parcial {.text-warning}     | El soporte literal Unicode es limitado                                                                                                                                                                     |
-| E031-02    | Identificadores de minúsculas                                                                                                                           | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E031-03    | Trailing subrayado                                                                                                                                      | Sí {.text-success}          |                                                                                                                                                                                                            |
-| **E051**   | **Especificación básica de la consulta**                                                                                                                | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E051-01    | SELECT DISTINCT                                                                                                                                         | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E051-02    | Cláusula GROUP BY                                                                                                                                       | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E051-04    | GROUP BY puede contener columnas que no estén en `<select list>`                                                                                        | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E051-05    | Los elementos seleccionados pueden ser renombrados                                                                                                      | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E051-06    | Cláusula HAVING                                                                                                                                         | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E051-07    | Calificado \* en la lista de selección                                                                                                                  | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E051-08    | Nombre de correlación en la cláusula FROM                                                                                                               | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E051-09    | Cambiar el nombre de las columnas en la cláusula FROM                                                                                                   | No {.text-danger}           |                                                                                                                                                                                                            |
-| **E061**   | **Predicados básicos y condiciones de búsqueda**                                                                                                        | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E061-01    | Predicado de comparación                                                                                                                                | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E061-02    | ENTRE predicado                                                                                                                                         | Parcial {.text-warning}     | No `SYMMETRIC` y `ASYMMETRIC` clausula                                                                                                                                                                     |
-| E061-03    | Predicado IN con lista de valores                                                                                                                       | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E061-04    | COMO predicado                                                                                                                                          | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E061-05    | Predicado LIKE: cláusula ESCAPE                                                                                                                         | No {.text-danger}           |                                                                                                                                                                                                            |
-| E061-06    | Predicado NULL                                                                                                                                          | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E061-07    | Predicado de comparación cuantificado                                                                                                                   | No {.text-danger}           |                                                                                                                                                                                                            |
-| E061-08    | Predicado EXISTS                                                                                                                                        | No {.text-danger}           |                                                                                                                                                                                                            |
-| E061-09    | Subconsultas en predicado de comparación                                                                                                                | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E061-11    | Subconsultas en el predicado IN                                                                                                                         | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E061-12    | Subconsultas en predicado de comparación cuantificado                                                                                                   | No {.text-danger}           |                                                                                                                                                                                                            |
-| E061-13    | Subconsultas correlacionadas                                                                                                                            | No {.text-danger}           |                                                                                                                                                                                                            |
-| E061-14    | Condición de búsqueda                                                                                                                                   | Sí {.text-success}          |                                                                                                                                                                                                            |
-| **E071**   | **Expresiones de consulta básicas**                                                                                                                     | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E071-01    | Operador de tabla UNION DISTINCT                                                                                                                        | No {.text-danger}           |                                                                                                                                                                                                            |
-| E071-02    | Operador de tabla UNION ALL                                                                                                                             | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E071-03    | EXCEPTO operador de tabla DISTINCT                                                                                                                      | No {.text-danger}           |                                                                                                                                                                                                            |
-| E071-05    | Las columnas combinadas a través de operadores de tabla no necesitan tener exactamente el mismo tipo de datos                                           | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E071-06    | Operadores de tabla en subconsultas                                                                                                                     | Sí {.text-success}          |                                                                                                                                                                                                            |
-| **E081**   | **Privilegios básicos**                                                                                                                                 | **Parcial**{.text-warning} | Trabajo en curso                                                                                                                                                                                           |
-| **E091**   | **Establecer funciones**                                                                                                                                | **Sí**{.text-success}      |                                                                                                                                                                                                            |
-| E091-01    | AVG                                                                                                                                                     | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E091-02    | COUNT                                                                                                                                                   | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E091-03    | MAX                                                                                                                                                     | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E091-04    | MIN                                                                                                                                                     | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E091-05    | SUM                                                                                                                                                     | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E091-06    | Cuantificador ALL                                                                                                                                       | No {.text-danger}           |                                                                                                                                                                                                            |
-| E091-07    | Cuantificador DISTINCT                                                                                                                                  | Parcial {.text-warning}     | No se admiten todas las funciones agregadas                                                                                                                                                                |
-| **E101**   | **Manipulación de datos básicos**                                                                                                                       | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E101-01    | Instrucción INSERT                                                                                                                                      | Sí {.text-success}          | Nota: la clave principal en ClickHouse no implica el `UNIQUE` limitación                                                                                                                                   |
-| E101-03    | Instrucción UPDATE buscada                                                                                                                              | No {.text-danger}           | Hay una `ALTER UPDATE` declaración para la modificación de datos por lotes                                                                                                                                 |
-| E101-04    | Instrucción DELETE buscada                                                                                                                              | No {.text-danger}           | Hay una `ALTER DELETE` declaración para la eliminación de datos por lotes                                                                                                                                  |
-| **E111**   | **Instrucción SELECT de una sola fila**                                                                                                                 | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| **E121**   | **Soporte básico del cursor**                                                                                                                           | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| E121-01    | DECLARE CURSOR                                                                                                                                          | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-02    | Las columnas PEDIR POR no necesitan estar en la lista de selección                                                                                      | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-03    | Expresiones de valor en la cláusula ORDER BY                                                                                                            | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-04    | Declaración ABIERTA                                                                                                                                     | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-06    | Instrucción UPDATE posicionada                                                                                                                          | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-07    | Instrucción DELETE posicionada                                                                                                                          | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-08    | Declaración CERRAR                                                                                                                                      | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-10    | Declaración FETCH: implícita NEXT                                                                                                                       | No {.text-danger}           |                                                                                                                                                                                                            |
-| E121-17    | CON Cursores HOLD                                                                                                                                       | No {.text-danger}           |                                                                                                                                                                                                            |
-| **E131**   | **Soporte de valor nulo (nulos en lugar de valores)**                                                                                                   | **Parcial**{.text-warning} | Se aplican algunas restricciones                                                                                                                                                                           |
-| **E141**   | **Restricciones de integridad básicas**                                                                                                                 | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| E141-01    | Restricciones NOT NULL                                                                                                                                  | Sí {.text-success}          | Nota: `NOT NULL` está implícito para las columnas de tabla de forma predeterminada                                                                                                                         |
-| E141-02    | Restricción UNIQUE de columnas NOT NULL                                                                                                                 | No {.text-danger}           |                                                                                                                                                                                                            |
-| E141-03    | Restricciones PRIMARY KEY                                                                                                                               | No {.text-danger}           |                                                                                                                                                                                                            |
-| E141-04    | Restricción básica FOREIGN KEY con el valor predeterminado NO ACTION para la acción de eliminación referencial y la acción de actualización referencial | No {.text-danger}           |                                                                                                                                                                                                            |
-| E141-06    | Restricción CHECK                                                                                                                                       | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E141-07    | Valores predeterminados de columna                                                                                                                      | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E141-08    | NO NULL inferido en CLAVE PRIMARIA                                                                                                                      | Sí {.text-success}          |                                                                                                                                                                                                            |
-| E141-10    | Los nombres de una clave externa se pueden especificar en cualquier orden                                                                               | No {.text-danger}           |                                                                                                                                                                                                            |
-| **E151**   | **Soporte de transacciones**                                                                                                                            | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| E151-01    | Declaración COMMIT                                                                                                                                      | No {.text-danger}           |                                                                                                                                                                                                            |
-| E151-02    | Instrucción ROLLBACK                                                                                                                                    | No {.text-danger}           |                                                                                                                                                                                                            |
-| **E152**   | **Instrucción SET TRANSACTION básica**                                                                                                                  | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| E152-01    | Instrucción SET TRANSACTION: cláusula ISOLATION LEVEL SERIALIZABLE                                                                                      | No {.text-danger}           |                                                                                                                                                                                                            |
-| E152-02    | Instrucción SET TRANSACTION: cláusulas READ ONLY y READ WRITE                                                                                           | No {.text-danger}           |                                                                                                                                                                                                            |
-| **E153**   | **Consultas actualizables con subconsultas**                                                                                                            | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| **E161**   | **Comentarios SQL usando doble menos inicial**                                                                                                          | **Sí**{.text-success}      |                                                                                                                                                                                                            |
-| **E171**   | **Soporte SQLSTATE**                                                                                                                                    | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| **E182**   | **Enlace de idioma de host**                                                                                                                            | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| **F031**   | **Manipulación básica del esquema**                                                                                                                     | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| F031-01    | Instrucción CREATE TABLE para crear tablas base persistentes                                                                                            | Parcial {.text-warning}     | No `SYSTEM VERSIONING`, `ON COMMIT`, `GLOBAL`, `LOCAL`, `PRESERVE`, `DELETE`, `REF IS`, `WITH OPTIONS`, `UNDER`, `LIKE`, `PERIOD FOR` cláusulas y sin soporte para tipos de datos resueltos por el usuario |
-| F031-02    | Instrucción CREATE VIEW                                                                                                                                 | Parcial {.text-warning}     | No `RECURSIVE`, `CHECK`, `UNDER`, `WITH OPTIONS` cláusulas y sin soporte para tipos de datos resueltos por el usuario                                                                                      |
-| F031-03    | Declaración GRANT                                                                                                                                       | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F031-04    | Sentencia ALTER TABLE: cláusula ADD COLUMN                                                                                                              | Parcial {.text-warning}     | No hay soporte para `GENERATED` cláusula y período de tiempo del sistema                                                                                                                                   |
-| F031-13    | Instrucción DROP TABLE: cláusula RESTRICT                                                                                                               | No {.text-danger}           |                                                                                                                                                                                                            |
-| F031-16    | Instrucción DROP VIEW: cláusula RESTRICT                                                                                                                | No {.text-danger}           |                                                                                                                                                                                                            |
-| F031-19    | Declaración REVOKE: cláusula RESTRICT                                                                                                                   | No {.text-danger}           |                                                                                                                                                                                                            |
-| **F041**   | **Tabla unida básica**                                                                                                                                  | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| F041-01    | Unión interna (pero no necesariamente la palabra clave INNER)                                                                                           | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F041-02    | Palabra clave INTERNA                                                                                                                                   | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F041-03    | LEFT OUTER JOIN                                                                                                                                         | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F041-04    | RIGHT OUTER JOIN                                                                                                                                        | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F041-05    | Las uniones externas se pueden anidar                                                                                                                   | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F041-07    | La tabla interna en una combinación externa izquierda o derecha también se puede usar en una combinación interna                                        | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F041-08    | Todos los operadores de comparación son compatibles (en lugar de solo =)                                                                                | No {.text-danger}           |                                                                                                                                                                                                            |
-| **F051**   | **Fecha y hora básicas**                                                                                                                                | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| F051-01    | Tipo de datos DATE (incluido el soporte del literal DATE)                                                                                               | Parcial {.text-warning}     | No literal                                                                                                                                                                                                 |
-| F051-02    | Tipo de datos TIME (incluido el soporte del literal TIME) con una precisión de segundos fraccionarios de al menos 0                                     | No {.text-danger}           |                                                                                                                                                                                                            |
-| F051-03    | Tipo de datos TIMESTAMP (incluido el soporte del literal TIMESTAMP) con una precisión de segundos fraccionarios de al menos 0 y 6                       | No {.text-danger}           | `DateTime64` tiempo proporciona una funcionalidad similar                                                                                                                                                  |
-| F051-04    | Predicado de comparación en los tipos de datos DATE, TIME y TIMESTAMP                                                                                   | Parcial {.text-warning}     | Sólo un tipo de datos disponible                                                                                                                                                                           |
-| F051-05    | CAST explícito entre tipos de fecha y hora y tipos de cadena de caracteres                                                                              | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F051-06    | CURRENT_DATE                                                                                                                                           | No {.text-danger}           | `today()` es similar                                                                                                                                                                                       |
-| F051-07    | LOCALTIME                                                                                                                                               | No {.text-danger}           | `now()` es similar                                                                                                                                                                                         |
-| F051-08    | LOCALTIMESTAMP                                                                                                                                          | No {.text-danger}           |                                                                                                                                                                                                            |
-| **F081**   | **UNIÓN y EXCEPTO en vistas**                                                                                                                           | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| **F131**   | **Operaciones agrupadas**                                                                                                                               | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| F131-01    | Cláusulas WHERE, GROUP BY y HAVING admitidas en consultas con vistas agrupadas                                                                          | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F131-02    | Múltiples tablas admitidas en consultas con vistas agrupadas                                                                                            | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F131-03    | Establecer funciones admitidas en consultas con vistas agrupadas                                                                                        | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F131-04    | Subconsultas con cláusulas GROUP BY y HAVING y vistas agrupadas                                                                                         | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F131-05    | SELECCIONAR una sola fila con cláusulas GROUP BY y HAVING y vistas agrupadas                                                                            | No {.text-danger}           |                                                                                                                                                                                                            |
-| **F181**   | **Múltiples módulos de apoyo**                                                                                                                          | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| **F201**   | **Función de fundición**                                                                                                                                | **Sí**{.text-success}      |                                                                                                                                                                                                            |
-| **F221**   | **Valores predeterminados explícitos**                                                                                                                  | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| **F261**   | **Expresión CASE**                                                                                                                                      | **Sí**{.text-success}      |                                                                                                                                                                                                            |
-| F261-01    | Caso simple                                                                                                                                             | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F261-02    | CASO buscado                                                                                                                                            | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F261-03    | NULLIF                                                                                                                                                  | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F261-04    | COALESCE                                                                                                                                                | Sí {.text-success}          |                                                                                                                                                                                                            |
-| **F311**   | **Instrucción de definición de esquema**                                                                                                                | **Parcial**{.text-warning} |                                                                                                                                                                                                            |
-| F311-01    | CREATE SCHEMA                                                                                                                                           | No {.text-danger}           |                                                                                                                                                                                                            |
-| F311-02    | CREATE TABLE para tablas base persistentes                                                                                                              | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F311-03    | CREATE VIEW                                                                                                                                             | Sí {.text-success}          |                                                                                                                                                                                                            |
-| F311-04    | CREATE VIEW: WITH CHECK OPTION                                                                                                                          | No {.text-danger}           |                                                                                                                                                                                                            |
-| F311-05    | Declaración GRANT                                                                                                                                       | Sí {.text-success}          |                                                                                                                                                                                                            |
-| **F471**   | **Valores escalares de la subconsulta**                                                                                                                 | **Sí**{.text-success}      |                                                                                                                                                                                                            |
-| **F481**   | **Predicado NULL expandido**                                                                                                                            | **Sí**{.text-success}      |                                                                                                                                                                                                            |
-| **F812**   | **Marcado básico**                                                                                                                                      | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| **T321**   | **Rutinas básicas invocadas por SQL**                                                                                                                   | **No**{.text-danger}       |                                                                                                                                                                                                            |
-| T321-01    | Funciones definidas por el usuario sin sobrecarga                                                                                                       | No {.text-danger}           |                                                                                                                                                                                                            |
-| T321-02    | Procedimientos almacenados definidos por el usuario sin sobrecarga                                                                                      | No {.text-danger}           |                                                                                                                                                                                                            |
-| T321-03    | Invocación de función                                                                                                                                   | No {.text-danger}           |                                                                                                                                                                                                            |
-| T321-04    | Declaración de LLAMADA                                                                                                                                  | No {.text-danger}           |                                                                                                                                                                                                            |
-| T321-05    | Declaración DEVOLUCIÓN                                                                                                                                  | No {.text-danger}           |                                                                                                                                                                                                            |
-| **T631**   | **Predicado IN con un elemento de lista**                                                                                                               | **Sí**{.text-success}      |                                                                                                                                                                                                            |
diff --git a/docs/es/sql-reference/data-types/aggregatefunction.md b/docs/es/sql-reference/data-types/aggregatefunction.md
deleted file mode 100644
index c7eab28c6ce6..000000000000
--- a/docs/es/sql-reference/data-types/aggregatefunction.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 52
-toc_title: "Agregar funci\xF3n (nombre, types_of_arguments)...)"
----
-
-# AggregateFunction(name, types_of_arguments…) {#data-type-aggregatefunction}
-
-Aggregate functions can have an implementation-defined intermediate state that can be serialized to an AggregateFunction(…) data type and stored in a table, usually, by means of [una vista materializada](../../sql-reference/statements/create.md#create-view). La forma común de producir un estado de función agregada es llamando a la función agregada con el `-State` sufijo. Para obtener el resultado final de la agregación en el futuro, debe utilizar la misma función de agregado con el `-Merge`sufijo.
-
-`AggregateFunction` — parametric data type.
-
-**Parámetros**
-
--   Nombre de la función de agregado.
-
-        If the function is parametric, specify its parameters too.
-
--   Tipos de los argumentos de la función agregada.
-
-**Ejemplo**
-
-``` sql
-CREATE TABLE t
-(
-    column1 AggregateFunction(uniq, UInt64),
-    column2 AggregateFunction(anyIf, String, UInt8),
-    column3 AggregateFunction(quantiles(0.5, 0.9), UInt64)
-) ENGINE = ...
-```
-
-[uniq](../../sql-reference/aggregate-functions/reference.md#agg_function-uniq), anyIf ([cualquier](../../sql-reference/aggregate-functions/reference.md#agg_function-any)+[Si](../../sql-reference/aggregate-functions/combinators.md#agg-functions-combinator-if)) y [cantiles](../../sql-reference/aggregate-functions/reference.md) son las funciones agregadas admitidas en ClickHouse.
-
-## Uso {#usage}
-
-### Inserción de datos {#data-insertion}
-
-Para insertar datos, utilice `INSERT SELECT` con agregado `-State`- función.
-
-**Ejemplos de funciones**
-
-``` sql
-uniqState(UserID)
-quantilesState(0.5, 0.9)(SendTiming)
-```
-
-En contraste con las funciones correspondientes `uniq` y `quantiles`, `-State`- funciones devuelven el estado, en lugar del valor final. En otras palabras, devuelven un valor de `AggregateFunction` tipo.
-
-En los resultados de `SELECT` consulta, los valores de `AggregateFunction` tipo tiene representación binaria específica de la implementación para todos los formatos de salida de ClickHouse. Si volcar datos en, por ejemplo, `TabSeparated` formato con `SELECT` consulta, entonces este volcado se puede cargar de nuevo usando `INSERT` consulta.
-
-### Selección de datos {#data-selection}
-
-Al seleccionar datos de `AggregatingMergeTree` mesa, uso `GROUP BY` cláusula y las mismas funciones agregadas que al insertar datos, pero usando `-Merge`sufijo.
-
-Una función agregada con `-Merge` sufijo toma un conjunto de estados, los combina y devuelve el resultado de la agregación de datos completa.
-
-Por ejemplo, las siguientes dos consultas devuelven el mismo resultado:
-
-``` sql
-SELECT uniq(UserID) FROM table
-
-SELECT uniqMerge(state) FROM (SELECT uniqState(UserID) AS state FROM table GROUP BY RegionID)
-```
-
-## Ejemplo de uso {#usage-example}
-
-Ver [AgregaciónMergeTree](../../engines/table-engines/mergetree-family/aggregatingmergetree.md) Descripción del motor.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/nested_data_structures/aggregatefunction/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/array.md b/docs/es/sql-reference/data-types/array.md
deleted file mode 100644
index d100361906e0..000000000000
--- a/docs/es/sql-reference/data-types/array.md
+++ /dev/null
@@ -1,77 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 51
-toc_title: Matriz (T)
----
-
-# Matriz (t) {#data-type-array}
-
-Una matriz de `T`-tipo de artículos. `T` puede ser cualquier tipo de datos, incluida una matriz.
-
-## Creación de una matriz {#creating-an-array}
-
-Puede usar una función para crear una matriz:
-
-``` sql
-array(T)
-```
-
-También puede usar corchetes.
-
-``` sql
-[]
-```
-
-Ejemplo de creación de una matriz:
-
-``` sql
-SELECT array(1, 2) AS x, toTypeName(x)
-```
-
-``` text
-┌─x─────┬─toTypeName(array(1, 2))─┐
-│ [1,2] │ Array(UInt8)            │
-└───────┴─────────────────────────┘
-```
-
-``` sql
-SELECT [1, 2] AS x, toTypeName(x)
-```
-
-``` text
-┌─x─────┬─toTypeName([1, 2])─┐
-│ [1,2] │ Array(UInt8)       │
-└───────┴────────────────────┘
-```
-
-## Trabajar con tipos de datos {#working-with-data-types}
-
-Al crear una matriz sobre la marcha, ClickHouse define automáticamente el tipo de argumento como el tipo de datos más estrecho que puede almacenar todos los argumentos enumerados. Si hay alguna [NULL](nullable.md#data_type-nullable) o literal [NULL](../../sql-reference/syntax.md#null-literal) valores, el tipo de un elemento de matriz también se convierte en [NULL](nullable.md).
-
-Si ClickHouse no pudo determinar el tipo de datos, genera una excepción. Por ejemplo, esto sucede cuando se intenta crear una matriz con cadenas y números simultáneamente (`SELECT array(1, 'a')`).
-
-Ejemplos de detección automática de tipos de datos:
-
-``` sql
-SELECT array(1, 2, NULL) AS x, toTypeName(x)
-```
-
-``` text
-┌─x──────────┬─toTypeName(array(1, 2, NULL))─┐
-│ [1,2,NULL] │ Array(Nullable(UInt8))        │
-└────────────┴───────────────────────────────┘
-```
-
-Si intenta crear una matriz de tipos de datos incompatibles, ClickHouse produce una excepción:
-
-``` sql
-SELECT array(1, 'a')
-```
-
-``` text
-Received exception from server (version 1.1.54388):
-Code: 386. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: There is no supertype for types UInt8, String because some of them are String/FixedString and some of them are not.
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/array/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/boolean.md b/docs/es/sql-reference/data-types/boolean.md
deleted file mode 100644
index 09a05c7369df..000000000000
--- a/docs/es/sql-reference/data-types/boolean.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: Booleana
----
-
-# Valores booleanos {#boolean-values}
-
-No hay un tipo separado para los valores booleanos. Utilice el tipo UInt8, restringido a los valores 0 o 1.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/boolean/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/date.md b/docs/es/sql-reference/data-types/date.md
deleted file mode 100644
index 71828fd82768..000000000000
--- a/docs/es/sql-reference/data-types/date.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: Fecha
----
-
-# Fecha {#date}
-
-Fecha. Almacenado en dos bytes como el número de días desde 1970-01-01 (sin signo). Permite almacenar valores desde justo después del comienzo de la Época Unix hasta el umbral superior definido por una constante en la etapa de compilación (actualmente, esto es hasta el año 2106, pero el último año totalmente soportado es 2105).
-
-El valor de fecha se almacena sin la zona horaria.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/date/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/datetime.md b/docs/es/sql-reference/data-types/datetime.md
deleted file mode 100644
index 0163da2002a7..000000000000
--- a/docs/es/sql-reference/data-types/datetime.md
+++ /dev/null
@@ -1,129 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 48
-toc_title: FechaHora
----
-
-# Datetime {#data_type-datetime}
-
-Permite almacenar un instante en el tiempo, que se puede expresar como una fecha del calendario y una hora de un día.
-
-Sintaxis:
-
-``` sql
-DateTime([timezone])
-```
-
-Rango de valores: \[1970-01-01 00:00:00, 2105-12-31 23:59:59\].
-
-Resolución: 1 segundo.
-
-## Observaciones de uso {#usage-remarks}
-
-El punto en el tiempo se guarda como un [Timestamp de Unix](https://en.wikipedia.org/wiki/Unix_time), independientemente de la zona horaria o el horario de verano. Además, el `DateTime` tipo puede almacenar zona horaria que es la misma para toda la columna, que afecta a cómo los valores de la `DateTime` Los valores de tipo se muestran en formato de texto y cómo se analizan los valores especificados como cadenas (‘2020-01-01 05:00:01’). La zona horaria no se almacena en las filas de la tabla (o en el conjunto de resultados), sino que se almacena en los metadatos de la columna.
-Se puede encontrar una lista de zonas horarias compatibles en el [Base de datos de zonas horarias de IANA](https://www.iana.org/time-zones).
-El `tzdata` paquete, que contiene [Base de datos de zonas horarias de IANA](https://www.iana.org/time-zones), debe instalarse en el sistema. Utilice el `timedatectl list-timezones` comando para listar zonas horarias conocidas por un sistema local.
-
-Puede establecer explícitamente una zona horaria para `DateTime`-type columnas al crear una tabla. Si la zona horaria no está establecida, ClickHouse usa el valor [Zona horaria](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-timezone) parámetro en la configuración del servidor o la configuración del sistema operativo en el momento del inicio del servidor ClickHouse.
-
-El [Casa de clics-cliente](../../interfaces/cli.md) aplica la zona horaria del servidor de forma predeterminada si una zona horaria no se establece explícitamente al inicializar el tipo de datos. Para utilizar la zona horaria del cliente, ejecute `clickhouse-client` con el `--use_client_time_zone` parámetro.
-
-ClickHouse genera valores en `YYYY-MM-DD hh:mm:ss` formato de texto por defecto. Puede cambiar la salida con el [formatDateTime](../../sql-reference/functions/date-time-functions.md#formatdatetime) función.
-
-Al insertar datos en ClickHouse, puede usar diferentes formatos de cadenas de fecha y hora, dependiendo del valor de la [Date_time_input_format](../../operations/settings/settings.md#settings-date_time_input_format) configuración.
-
-## Ejemplos {#examples}
-
-**1.** Creación de una tabla con un `DateTime`-tipo de columna e insertar datos en ella:
-
-``` sql
-CREATE TABLE dt
-(
-    `timestamp` DateTime('Europe/Moscow'),
-    `event_id` UInt8
-)
-ENGINE = TinyLog;
-```
-
-``` sql
-INSERT INTO dt Values (1546300800, 1), ('2019-01-01 00:00:00', 2);
-```
-
-``` sql
-SELECT * FROM dt;
-```
-
-``` text
-┌───────────timestamp─┬─event_id─┐
-│ 2019-01-01 03:00:00 │        1 │
-│ 2019-01-01 00:00:00 │        2 │
-└─────────────────────┴──────────┘
-```
-
--   Al insertar datetime como un entero, se trata como Unix Timestamp (UTC). `1546300800` representar `'2019-01-01 00:00:00'` UTC. Sin embargo, como `timestamp` columna tiene `Europe/Moscow` (UTC + 3) zona horaria especificada, al emitir como cadena, el valor se mostrará como `'2019-01-01 03:00:00'`
--   Al insertar el valor de cadena como fecha y hora, se trata como si estuviera en la zona horaria de la columna. `'2019-01-01 00:00:00'` será tratado como estar en `Europe/Moscow` zona horaria y guardado como `1546290000`.
-
-**2.** Filtrado en `DateTime` valor
-
-``` sql
-SELECT * FROM dt WHERE timestamp = toDateTime('2019-01-01 00:00:00', 'Europe/Moscow')
-```
-
-``` text
-┌───────────timestamp─┬─event_id─┐
-│ 2019-01-01 00:00:00 │        2 │
-└─────────────────────┴──────────┘
-```
-
-`DateTime` se pueden filtrar usando un valor de cadena en `WHERE` predicado. Se convertirá a `DateTime` automática:
-
-``` sql
-SELECT * FROM dt WHERE timestamp = '2019-01-01 00:00:00'
-```
-
-``` text
-┌───────────timestamp─┬─event_id─┐
-│ 2019-01-01 03:00:00 │        1 │
-└─────────────────────┴──────────┘
-```
-
-**3.** Obtener una zona horaria para un `DateTime`-tipo columna:
-
-``` sql
-SELECT toDateTime(now(), 'Europe/Moscow') AS column, toTypeName(column) AS x
-```
-
-``` text
-┌──────────────column─┬─x─────────────────────────┐
-│ 2019-10-16 04:12:04 │ DateTime('Europe/Moscow') │
-└─────────────────────┴───────────────────────────┘
-```
-
-**4.** Conversión de zona horaria
-
-``` sql
-SELECT
-toDateTime(timestamp, 'Europe/London') as lon_time,
-toDateTime(timestamp, 'Europe/Moscow') as mos_time
-FROM dt
-```
-
-``` text
-┌───────────lon_time──┬────────────mos_time─┐
-│ 2019-01-01 00:00:00 │ 2019-01-01 03:00:00 │
-│ 2018-12-31 21:00:00 │ 2019-01-01 00:00:00 │
-└─────────────────────┴─────────────────────┘
-```
-
-## Ver también {#see-also}
-
--   [Funciones de conversión de tipos](../../sql-reference/functions/type-conversion-functions.md)
--   [Funciones para trabajar con fechas y horas](../../sql-reference/functions/date-time-functions.md)
--   [Funciones para trabajar con matrices](../../sql-reference/functions/array-functions.md)
--   [El `date_time_input_format` configuración](../../operations/settings/settings.md#settings-date_time_input_format)
--   [El `timezone` parámetro de configuración del servidor](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-timezone)
--   [Operadores para trabajar con fechas y horas](../../sql-reference/operators/index.md#operators-datetime)
--   [El `Date` tipo de datos](date.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/datetime/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/datetime64.md b/docs/es/sql-reference/data-types/datetime64.md
deleted file mode 100644
index 19ae28bf4f86..000000000000
--- a/docs/es/sql-reference/data-types/datetime64.md
+++ /dev/null
@@ -1,104 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 49
-toc_title: DateTime64
----
-
-# Datetime64 {#data_type-datetime64}
-
-Permite almacenar un instante en el tiempo, que se puede expresar como una fecha de calendario y una hora de un día, con una precisión de subsegundo definida
-
-Tamaño de la marca (precisión): 10<sup>-Precision</sup> segundo
-
-Sintaxis:
-
-``` sql
-DateTime64(precision, [timezone])
-```
-
-Internamente, almacena datos como un número de ‘ticks’ desde el inicio de la época (1970-01-01 00:00:00 UTC) como Int64. La resolución de tick está determinada por el parámetro precision. Además, el `DateTime64` tipo puede almacenar zona horaria que es la misma para toda la columna, que afecta a cómo los valores de la `DateTime64` Los valores de tipo se muestran en formato de texto y cómo se analizan los valores especificados como cadenas (‘2020-01-01 05:00:01.000’). La zona horaria no se almacena en las filas de la tabla (o en el conjunto de resultados), sino que se almacena en los metadatos de la columna. Ver detalles en [FechaHora](datetime.md).
-
-## Ejemplos {#examples}
-
-**1.** Creación de una tabla con `DateTime64`-tipo de columna e insertar datos en ella:
-
-``` sql
-CREATE TABLE dt
-(
-    `timestamp` DateTime64(3, 'Europe/Moscow'),
-    `event_id` UInt8
-)
-ENGINE = TinyLog
-```
-
-``` sql
-INSERT INTO dt Values (1546300800000, 1), ('2019-01-01 00:00:00', 2)
-```
-
-``` sql
-SELECT * FROM dt
-```
-
-``` text
-┌───────────────timestamp─┬─event_id─┐
-│ 2019-01-01 03:00:00.000 │        1 │
-│ 2019-01-01 00:00:00.000 │        2 │
-└─────────────────────────┴──────────┘
-```
-
--   Al insertar datetime como un entero, se trata como una marca de tiempo Unix (UTC) apropiadamente escalada. `1546300800000` (con precisión 3) representa `'2019-01-01 00:00:00'` UTC. Sin embargo, como `timestamp` columna tiene `Europe/Moscow` (UTC + 3) zona horaria especificada, al emitir como una cadena, el valor se mostrará como `'2019-01-01 03:00:00'`
--   Al insertar el valor de cadena como fecha y hora, se trata como si estuviera en la zona horaria de la columna. `'2019-01-01 00:00:00'` será tratado como estar en `Europe/Moscow` zona horaria y se almacena como `1546290000000`.
-
-**2.** Filtrado en `DateTime64` valor
-
-``` sql
-SELECT * FROM dt WHERE timestamp = toDateTime64('2019-01-01 00:00:00', 3, 'Europe/Moscow')
-```
-
-``` text
-┌───────────────timestamp─┬─event_id─┐
-│ 2019-01-01 00:00:00.000 │        2 │
-└─────────────────────────┴──────────┘
-```
-
-A diferencia de `DateTime`, `DateTime64` los valores no se convierten desde `String` automática
-
-**3.** Obtener una zona horaria para un `DateTime64`-tipo de valor:
-
-``` sql
-SELECT toDateTime64(now(), 3, 'Europe/Moscow') AS column, toTypeName(column) AS x
-```
-
-``` text
-┌──────────────────column─┬─x──────────────────────────────┐
-│ 2019-10-16 04:12:04.000 │ DateTime64(3, 'Europe/Moscow') │
-└─────────────────────────┴────────────────────────────────┘
-```
-
-**4.** Conversión de zona horaria
-
-``` sql
-SELECT
-toDateTime64(timestamp, 3, 'Europe/London') as lon_time,
-toDateTime64(timestamp, 3, 'Europe/Moscow') as mos_time
-FROM dt
-```
-
-``` text
-┌───────────────lon_time──┬────────────────mos_time─┐
-│ 2019-01-01 00:00:00.000 │ 2019-01-01 03:00:00.000 │
-│ 2018-12-31 21:00:00.000 │ 2019-01-01 00:00:00.000 │
-└─────────────────────────┴─────────────────────────┘
-```
-
-## Ver también {#see-also}
-
--   [Funciones de conversión de tipos](../../sql-reference/functions/type-conversion-functions.md)
--   [Funciones para trabajar con fechas y horas](../../sql-reference/functions/date-time-functions.md)
--   [Funciones para trabajar con matrices](../../sql-reference/functions/array-functions.md)
--   [El `date_time_input_format` configuración](../../operations/settings/settings.md#settings-date_time_input_format)
--   [El `timezone` parámetro de configuración del servidor](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-timezone)
--   [Operadores para trabajar con fechas y horas](../../sql-reference/operators/index.md#operators-datetime)
--   [`Date` tipo de datos](date.md)
--   [`DateTime` tipo de datos](datetime.md)
diff --git a/docs/es/sql-reference/data-types/decimal.md b/docs/es/sql-reference/data-types/decimal.md
deleted file mode 100644
index 78cb296c9d33..000000000000
--- a/docs/es/sql-reference/data-types/decimal.md
+++ /dev/null
@@ -1,109 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: Decimal
----
-
-# Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S) {#decimalp-s-decimal32s-decimal64s-decimal128s}
-
-Números de punto fijo firmados que mantienen la precisión durante las operaciones de suma, resta y multiplicación. Para la división se descartan los dígitos menos significativos (no redondeados).
-
-## Parámetros {#parameters}
-
--   P - precisión. Rango válido: \[ 1 : 38 \]. Determina cuántos dígitos decimales puede tener el número (incluida la fracción).
--   S - escala. Rango válido: \[ 0 : P \]. Determina cuántos dígitos decimales puede tener la fracción.
-
-Dependiendo del valor del parámetro P Decimal(P, S) es un sinónimo de:
-- P de \[ 1 : 9 \] - para Decimal32(S)
-- P de \[ 10 : 18 \] - para Decimal64(S)
-- P de \[ 19 : 38 \] - para Decimal128(S)
-
-## Rangos de valores decimales {#decimal-value-ranges}
-
--   Decimal32(S) - ( -1 \* 10^(9 - S), 1 \* 10^(9 - S) )
--   Decimal64(S) - ( -1 \* 10^(18 - S), 1 \* 10^(18 - S) )
--   Decimal128(S) - ( -1 \* 10^(38 - S), 1 \* 10^(38 - S) )
-
-Por ejemplo, Decimal32(4) puede contener números de -99999.9999 a 99999.9999 con el paso 0.0001.
-
-## Representación interna {#internal-representation}
-
-Internamente, los datos se representan como enteros con signo normal con el ancho de bits respectivo. Los rangos de valores reales que se pueden almacenar en la memoria son un poco más grandes que los especificados anteriormente, que se verifican solo en la conversión de una cadena.
-
-Debido a que las CPU modernas no admiten enteros de 128 bits de forma nativa, las operaciones en Decimal128 se emulan. Debido a esto, Decimal128 funciona significativamente más lento que Decimal32 / Decimal64.
-
-## Operaciones y tipo de resultado {#operations-and-result-type}
-
-Las operaciones binarias en Decimal dan como resultado un tipo de resultado más amplio (con cualquier orden de argumentos).
-
--   `Decimal64(S1) <op> Decimal32(S2) -> Decimal64(S)`
--   `Decimal128(S1) <op> Decimal32(S2) -> Decimal128(S)`
--   `Decimal128(S1) <op> Decimal64(S2) -> Decimal128(S)`
-
-Reglas para la escala:
-
--   Sumar, restar: S = max(S1, S2).
--   multuply: S = S1 + S2.
--   división: S = S1.
-
-Para operaciones similares entre Decimal y enteros, el resultado es Decimal del mismo tamaño que un argumento.
-
-Las operaciones entre Decimal y Float32 / Float64 no están definidas. Si los necesita, puede convertir explícitamente uno de los argumentos utilizando toDecimal32, toDecimal64, toDecimal128 o toFloat32, toFloat64 builtins. Tenga en cuenta que el resultado perderá precisión y la conversión de tipo es una operación computacionalmente costosa.
-
-Algunas funciones en Decimal devuelven el resultado como Float64 (por ejemplo, var o stddev). Los cálculos intermedios aún se pueden realizar en Decimal, lo que podría dar lugar a resultados diferentes entre las entradas Float64 y Decimal con los mismos valores.
-
-## Comprobaciones de desbordamiento {#overflow-checks}
-
-Durante los cálculos en Decimal, pueden producirse desbordamientos de enteros. Los dígitos excesivos en una fracción se descartan (no se redondean). Los dígitos excesivos en la parte entera conducirán a una excepción.
-
-``` sql
-SELECT toDecimal32(2, 4) AS x, x / 3
-```
-
-``` text
-┌──────x─┬─divide(toDecimal32(2, 4), 3)─┐
-│ 2.0000 │                       0.6666 │
-└────────┴──────────────────────────────┘
-```
-
-``` sql
-SELECT toDecimal32(4.2, 8) AS x, x * x
-```
-
-``` text
-DB::Exception: Scale is out of bounds.
-```
-
-``` sql
-SELECT toDecimal32(4.2, 8) AS x, 6 * x
-```
-
-``` text
-DB::Exception: Decimal math overflow.
-```
-
-Las comprobaciones de desbordamiento conducen a la desaceleración de las operaciones. Si se sabe que los desbordamientos no son posibles, tiene sentido deshabilitar las verificaciones usando `decimal_check_overflow` configuración. Cuando las comprobaciones están deshabilitadas y se produce el desbordamiento, el resultado será incorrecto:
-
-``` sql
-SET decimal_check_overflow = 0;
-SELECT toDecimal32(4.2, 8) AS x, 6 * x
-```
-
-``` text
-┌──────────x─┬─multiply(6, toDecimal32(4.2, 8))─┐
-│ 4.20000000 │                     -17.74967296 │
-└────────────┴──────────────────────────────────┘
-```
-
-Las comprobaciones de desbordamiento ocurren no solo en operaciones aritméticas sino también en la comparación de valores:
-
-``` sql
-SELECT toDecimal32(1, 8) < 100
-```
-
-``` text
-DB::Exception: Can't compare.
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/decimal/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/domains/index.md b/docs/es/sql-reference/data-types/domains/index.md
deleted file mode 100644
index 136058e35c85..000000000000
--- a/docs/es/sql-reference/data-types/domains/index.md
+++ /dev/null
@@ -1,33 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 56
-toc_folder_title: Dominio
-toc_title: "Descripci\xF3n"
----
-
-# Dominio {#domains}
-
-Los dominios son tipos de propósito especial que agregan algunas características adicionales encima del tipo base existente, pero dejando intacto el formato en cable y en disco del tipo de datos subyacente. Por el momento, ClickHouse no admite dominios definidos por el usuario.
-
-Puede usar dominios en cualquier lugar que se pueda usar el tipo base correspondiente, por ejemplo:
-
--   Crear una columna de un tipo de dominio
--   Leer/escribir valores desde/a la columna de dominio
--   Úselo como un índice si un tipo base se puede usar como un índice
--   Funciones de llamada con valores de la columna de dominio
-
-### Características adicionales de los dominios {#extra-features-of-domains}
-
--   Nombre de tipo de columna explícito en `SHOW CREATE TABLE` o `DESCRIBE TABLE`
--   Entrada del formato humano-amistoso con `INSERT INTO domain_table(domain_column) VALUES(...)`
--   Salida al formato humano-amistoso para `SELECT domain_column FROM domain_table`
--   Carga de datos desde una fuente externa en el formato de uso humano: `INSERT INTO domain_table FORMAT CSV ...`
-
-### Limitacion {#limitations}
-
--   No se puede convertir la columna de índice del tipo base al tipo de dominio a través de `ALTER TABLE`.
--   No se pueden convertir implícitamente valores de cadena en valores de dominio al insertar datos de otra columna o tabla.
--   Domain no agrega restricciones en los valores almacenados.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/domains/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/domains/ipv4.md b/docs/es/sql-reference/data-types/domains/ipv4.md
deleted file mode 100644
index 6e271f10fd21..000000000000
--- a/docs/es/sql-reference/data-types/domains/ipv4.md
+++ /dev/null
@@ -1,84 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: IPv4
----
-
-## IPv4 {#ipv4}
-
-`IPv4` es un dominio basado en `UInt32` tipo y sirve como un reemplazo con tipo para almacenar valores IPv4. Proporciona un almacenamiento compacto con el formato de entrada-salida amigable para los humanos y la información sobre el tipo de columna en la inspección.
-
-### Uso básico {#basic-usage}
-
-``` sql
-CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY url;
-
-DESCRIBE TABLE hits;
-```
-
-``` text
-┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐
-│ url  │ String │              │                    │         │                  │
-│ from │ IPv4   │              │                    │         │                  │
-└──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘
-```
-
-O puede usar el dominio IPv4 como clave:
-
-``` sql
-CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY from;
-```
-
-`IPv4` domain admite formato de entrada personalizado como cadenas IPv4:
-
-``` sql
-INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '116.253.40.133')('https://clickhouse.tech', '183.247.232.58')('https://clickhouse.tech/docs/en/', '116.106.34.242');
-
-SELECT * FROM hits;
-```
-
-``` text
-┌─url────────────────────────────────┬───────────from─┐
-│ https://clickhouse.tech/docs/en/ │ 116.106.34.242 │
-│ https://wikipedia.org              │ 116.253.40.133 │
-│ https://clickhouse.tech          │ 183.247.232.58 │
-└────────────────────────────────────┴────────────────┘
-```
-
-Los valores se almacenan en forma binaria compacta:
-
-``` sql
-SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(from)─┬─hex(from)─┐
-│ IPv4             │ B7F7E83A  │
-└──────────────────┴───────────┘
-```
-
-Los valores de dominio no se pueden convertir implícitamente en tipos distintos de `UInt32`.
-Si desea convertir `IPv4` valor a una cadena, tienes que hacer eso explícitamente con `IPv4NumToString()` función:
-
-``` sql
-SELECT toTypeName(s), IPv4NumToString(from) as s FROM hits LIMIT 1;
-```
-
-    ┌─toTypeName(IPv4NumToString(from))─┬─s──────────────┐
-    │ String                            │ 183.247.232.58 │
-    └───────────────────────────────────┴────────────────┘
-
-O echar a un `UInt32` valor:
-
-``` sql
-SELECT toTypeName(i), CAST(from as UInt32) as i FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(CAST(from, 'UInt32'))─┬──────────i─┐
-│ UInt32                           │ 3086477370 │
-└──────────────────────────────────┴────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/domains/ipv4) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/domains/ipv6.md b/docs/es/sql-reference/data-types/domains/ipv6.md
deleted file mode 100644
index 2f45a3530533..000000000000
--- a/docs/es/sql-reference/data-types/domains/ipv6.md
+++ /dev/null
@@ -1,86 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: IPv6
----
-
-## IPv6 {#ipv6}
-
-`IPv6` es un dominio basado en `FixedString(16)` tipo y sirve como un reemplazo con tipo para almacenar valores IPv6. Proporciona un almacenamiento compacto con el formato de entrada-salida amigable para los humanos y la información sobre el tipo de columna en la inspección.
-
-### Uso básico {#basic-usage}
-
-``` sql
-CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY url;
-
-DESCRIBE TABLE hits;
-```
-
-``` text
-┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐
-│ url  │ String │              │                    │         │                  │
-│ from │ IPv6   │              │                    │         │                  │
-└──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘
-```
-
-O puedes usar `IPv6` dominio como clave:
-
-``` sql
-CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY from;
-```
-
-`IPv6` domain admite entradas personalizadas como cadenas IPv6:
-
-``` sql
-INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '2a02:aa08:e000:3100::2')('https://clickhouse.tech', '2001:44c8:129:2632:33:0:252:2')('https://clickhouse.tech/docs/en/', '2a02:e980:1e::1');
-
-SELECT * FROM hits;
-```
-
-``` text
-┌─url────────────────────────────────┬─from──────────────────────────┐
-│ https://clickhouse.tech          │ 2001:44c8:129:2632:33:0:252:2 │
-│ https://clickhouse.tech/docs/en/ │ 2a02:e980:1e::1               │
-│ https://wikipedia.org              │ 2a02:aa08:e000:3100::2        │
-└────────────────────────────────────┴───────────────────────────────┘
-```
-
-Los valores se almacenan en forma binaria compacta:
-
-``` sql
-SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(from)─┬─hex(from)────────────────────────┐
-│ IPv6             │ 200144C8012926320033000002520002 │
-└──────────────────┴──────────────────────────────────┘
-```
-
-Los valores de dominio no se pueden convertir implícitamente en tipos distintos de `FixedString(16)`.
-Si desea convertir `IPv6` valor a una cadena, tienes que hacer eso explícitamente con `IPv6NumToString()` función:
-
-``` sql
-SELECT toTypeName(s), IPv6NumToString(from) as s FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(IPv6NumToString(from))─┬─s─────────────────────────────┐
-│ String                            │ 2001:44c8:129:2632:33:0:252:2 │
-└───────────────────────────────────┴───────────────────────────────┘
-```
-
-O echar a un `FixedString(16)` valor:
-
-``` sql
-SELECT toTypeName(i), CAST(from as FixedString(16)) as i FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(CAST(from, 'FixedString(16)'))─┬─i───────┐
-│ FixedString(16)                           │  ��� │
-└───────────────────────────────────────────┴─────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/domains/ipv6) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/enum.md b/docs/es/sql-reference/data-types/enum.md
deleted file mode 100644
index aa990dcaef97..000000000000
--- a/docs/es/sql-reference/data-types/enum.md
+++ /dev/null
@@ -1,132 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 50
-toc_title: Enum
----
-
-# Enum {#enum}
-
-Tipo enumerado que consta de valores con nombre.
-
-Los valores con nombre deben declararse como `'string' = integer` par. ClickHouse almacena solo números, pero admite operaciones con los valores a través de sus nombres.
-
-Soporta ClickHouse:
-
--   de 8 bits `Enum`. Puede contener hasta 256 valores enumerados en el `[-128, 127]` gama.
--   de 16 bits `Enum`. Puede contener hasta 65536 valores enumerados en el `[-32768, 32767]` gama.
-
-ClickHouse elige automáticamente el tipo de `Enum` cuando se insertan datos. También puede utilizar `Enum8` o `Enum16` para estar seguro en el tamaño de almacenamiento.
-
-## Ejemplos de uso {#usage-examples}
-
-Aquí creamos una tabla con un `Enum8('hello' = 1, 'world' = 2)` tipo columna:
-
-``` sql
-CREATE TABLE t_enum
-(
-    x Enum('hello' = 1, 'world' = 2)
-)
-ENGINE = TinyLog
-```
-
-Columna `x` sólo puede almacenar valores que se enumeran en la definición de tipo: `'hello'` o `'world'`. Si intenta guardar cualquier otro valor, ClickHouse generará una excepción. Tamaño de 8 bits para esto `Enum` se elige automáticamente.
-
-``` sql
-INSERT INTO t_enum VALUES ('hello'), ('world'), ('hello')
-```
-
-``` text
-Ok.
-```
-
-``` sql
-INSERT INTO t_enum values('a')
-```
-
-``` text
-Exception on client:
-Code: 49. DB::Exception: Unknown element 'a' for type Enum('hello' = 1, 'world' = 2)
-```
-
-Al consultar datos de la tabla, ClickHouse genera los valores de cadena de `Enum`.
-
-``` sql
-SELECT * FROM t_enum
-```
-
-``` text
-┌─x─────┐
-│ hello │
-│ world │
-│ hello │
-└───────┘
-```
-
-Si necesita ver los equivalentes numéricos de las filas, debe `Enum` valor a tipo entero.
-
-``` sql
-SELECT CAST(x, 'Int8') FROM t_enum
-```
-
-``` text
-┌─CAST(x, 'Int8')─┐
-│               1 │
-│               2 │
-│               1 │
-└─────────────────┘
-```
-
-Para crear un valor Enum en una consulta, también debe usar `CAST`.
-
-``` sql
-SELECT toTypeName(CAST('a', 'Enum(\'a\' = 1, \'b\' = 2)'))
-```
-
-``` text
-┌─toTypeName(CAST('a', 'Enum(\'a\' = 1, \'b\' = 2)'))─┐
-│ Enum8('a' = 1, 'b' = 2)                             │
-└─────────────────────────────────────────────────────┘
-```
-
-## Reglas generales y uso {#general-rules-and-usage}
-
-A cada uno de los valores se le asigna un número en el rango `-128 ... 127` para `Enum8` o en el rango `-32768 ... 32767` para `Enum16`. Todas las cadenas y números deben ser diferentes. Se permite una cadena vacía. Si se especifica este tipo (en una definición de tabla), los números pueden estar en un orden arbitrario. Sin embargo, el orden no importa.
-
-Ni la cadena ni el valor numérico en un `Enum` puede ser [NULL](../../sql-reference/syntax.md).
-
-Un `Enum` puede estar contenido en [NULL](nullable.md) tipo. Entonces, si crea una tabla usando la consulta
-
-``` sql
-CREATE TABLE t_enum_nullable
-(
-    x Nullable( Enum8('hello' = 1, 'world' = 2) )
-)
-ENGINE = TinyLog
-```
-
-puede almacenar no sólo `'hello'` y `'world'`, pero `NULL`, también.
-
-``` sql
-INSERT INTO t_enum_nullable Values('hello'),('world'),(NULL)
-```
-
-En RAM, un `Enum` columna se almacena de la misma manera que `Int8` o `Int16` de los valores numéricos correspondientes.
-
-Al leer en forma de texto, ClickHouse analiza el valor como una cadena y busca la cadena correspondiente del conjunto de valores Enum. Si no se encuentra, se lanza una excepción. Al leer en formato de texto, se lee la cadena y se busca el valor numérico correspondiente. Se lanzará una excepción si no se encuentra.
-Al escribir en forma de texto, escribe el valor como la cadena correspondiente. Si los datos de columna contienen elementos no utilizados (números que no son del conjunto válido), se produce una excepción. Al leer y escribir en forma binaria, funciona de la misma manera que para los tipos de datos Int8 e Int16.
-El valor predeterminado es el valor con el número más bajo.
-
-Durante `ORDER BY`, `GROUP BY`, `IN`, `DISTINCT` y así sucesivamente, las enumeraciones se comportan de la misma manera que los números correspondientes. Por ejemplo, ORDER BY los ordena numéricamente. Los operadores de igualdad y comparación funcionan de la misma manera en enumeraciones que en los valores numéricos subyacentes.
-
-Los valores de Enum no se pueden comparar con los números. Las enumeraciones se pueden comparar con una cadena constante. Si la cadena en comparación con no es un valor válido para el Enum, se lanzará una excepción. El operador IN es compatible con el Enum en el lado izquierdo y un conjunto de cadenas en el lado derecho. Las cadenas son los valores del Enum correspondiente.
-
-Most numeric and string operations are not defined for Enum values, e.g. adding a number to an Enum or concatenating a string to an Enum.
-Sin embargo, el Enum tiene un `toString` función que devuelve su valor de cadena.
-
-Los valores de Enum también se pueden convertir a tipos numéricos utilizando el `toT` función, donde T es un tipo numérico. Cuando T corresponde al tipo numérico subyacente de la enumeración, esta conversión es de costo cero.
-El tipo Enum se puede cambiar sin costo usando ALTER, si solo se cambia el conjunto de valores. Es posible agregar y eliminar miembros del Enum usando ALTER (eliminar es seguro solo si el valor eliminado nunca se ha usado en la tabla). Como salvaguardia, al cambiar el valor numérico de un miembro Enum definido previamente se producirá una excepción.
-
-Usando ALTER, es posible cambiar un Enum8 a un Enum16 o viceversa, al igual que cambiar un Int8 a Int16.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/enum/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/fixedstring.md b/docs/es/sql-reference/data-types/fixedstring.md
deleted file mode 100644
index f12a308b7de9..000000000000
--- a/docs/es/sql-reference/data-types/fixedstring.md
+++ /dev/null
@@ -1,63 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: Cadena fija (N)
----
-
-# Cuerda fija {#fixedstring}
-
-Una cadena de longitud fija de `N` bytes (ni caracteres ni puntos de código).
-
-Para declarar una columna de `FixedString` tipo, utilice la siguiente sintaxis:
-
-``` sql
-<column_name> FixedString(N)
-```
-
-Donde `N` es un número natural.
-
-El `FixedString` tipo es eficiente cuando los datos tienen la longitud de `N` byte. En todos los demás casos, es probable que reduzca la eficiencia.
-
-Ejemplos de los valores que se pueden almacenar eficientemente en `FixedString`escrito columnas:
-
--   La representación binaria de direcciones IP (`FixedString(16)` para IPv6).
--   Language codes (ru_RU, en_US … ).
--   Currency codes (USD, RUB … ).
--   Representación binaria de hashes (`FixedString(16)` para MD5, `FixedString(32)` para SHA256).
-
-Para almacenar valores UUID, utilice el [UUID](uuid.md) tipo de datos.
-
-Al insertar los datos, ClickHouse:
-
--   Complementa una cadena con bytes nulos si la cadena contiene menos de `N` byte.
--   Lanza el `Too large value for FixedString(N)` excepción si la cadena contiene más de `N` byte.
-
-Al seleccionar los datos, ClickHouse no elimina los bytes nulos al final de la cadena. Si utiliza el `WHERE` cláusula, debe agregar bytes nulos manualmente para que coincida con el `FixedString` valor. En el ejemplo siguiente se muestra cómo utilizar el `WHERE` cláusula con `FixedString`.
-
-Consideremos la siguiente tabla con el único `FixedString(2)` columna:
-
-``` text
-┌─name──┐
-│ b     │
-└───────┘
-```
-
-Consulta `SELECT * FROM FixedStringTable WHERE a = 'b'` no devuelve ningún dato como resultado. Debemos complementar el patrón de filtro con bytes nulos.
-
-``` sql
-SELECT * FROM FixedStringTable
-WHERE a = 'b\0'
-```
-
-``` text
-┌─a─┐
-│ b │
-└───┘
-```
-
-Este comportamiento difiere de MySQL para el `CHAR` tipo (donde las cadenas se rellenan con espacios y los espacios se eliminan para la salida).
-
-Tenga en cuenta que la longitud del `FixedString(N)` el valor es constante. El [longitud](../../sql-reference/functions/array-functions.md#array_functions-length) función devuelve `N` incluso si el `FixedString(N)` sólo se rellena con bytes nulos, pero el valor [vaciar](../../sql-reference/functions/string-functions.md#empty) función devuelve `1` en este caso.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/fixedstring/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/float.md b/docs/es/sql-reference/data-types/float.md
deleted file mode 100644
index 3102dc9c14b7..000000000000
--- a/docs/es/sql-reference/data-types/float.md
+++ /dev/null
@@ -1,87 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: "Descripci\xF3n del producto"
----
-
-# Descripción del producto {#float32-float64}
-
-[Números de punto flotante](https://en.wikipedia.org/wiki/IEEE_754).
-
-Los tipos son equivalentes a los tipos de C:
-
--   `Float32` - `float`
--   `Float64` - `double`
-
-Le recomendamos que almacene los datos en formato entero siempre que sea posible. Por ejemplo, convierta números de precisión fija en valores enteros, como importes monetarios o tiempos de carga de página en milisegundos.
-
-## Uso de números de punto flotante {#using-floating-point-numbers}
-
--   Los cálculos con números de punto flotante pueden producir un error de redondeo.
-
-<!-- -->
-
-``` sql
-SELECT 1 - 0.9
-```
-
-``` text
-┌───────minus(1, 0.9)─┐
-│ 0.09999999999999998 │
-└─────────────────────┘
-```
-
--   El resultado del cálculo depende del método de cálculo (el tipo de procesador y la arquitectura del sistema informático).
--   Los cálculos de puntos flotantes pueden dar como resultado números como el infinito (`Inf`) y “not-a-number” (`NaN`). Esto debe tenerse en cuenta al procesar los resultados de los cálculos.
--   Al analizar números de punto flotante a partir de texto, el resultado puede no ser el número representable por máquina más cercano.
-
-## NaN y Inf {#data_type-float-nan-inf}
-
-A diferencia de SQL estándar, ClickHouse admite las siguientes categorías de números de punto flotante:
-
--   `Inf` – Infinity.
-
-<!-- -->
-
-``` sql
-SELECT 0.5 / 0
-```
-
-``` text
-┌─divide(0.5, 0)─┐
-│            inf │
-└────────────────┘
-```
-
--   `-Inf` – Negative infinity.
-
-<!-- -->
-
-``` sql
-SELECT -0.5 / 0
-```
-
-``` text
-┌─divide(-0.5, 0)─┐
-│            -inf │
-└─────────────────┘
-```
-
--   `NaN` – Not a number.
-
-<!-- -->
-
-``` sql
-SELECT 0 / 0
-```
-
-``` text
-┌─divide(0, 0)─┐
-│          nan │
-└──────────────┘
-```
-
-    See the rules for `NaN` sorting in the section [ORDER BY clause](../sql_reference/statements/select/order-by.md).
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/float/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/index.md b/docs/es/sql-reference/data-types/index.md
deleted file mode 100644
index e26a42ebc440..000000000000
--- a/docs/es/sql-reference/data-types/index.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Tipos de datos
-toc_priority: 37
-toc_title: "Implantaci\xF3n"
----
-
-# Tipos de datos {#data_types}
-
-ClickHouse puede almacenar varios tipos de datos en celdas de tabla.
-
-En esta sección se describen los tipos de datos admitidos y las consideraciones especiales para usarlos o implementarlos, si los hubiere.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/int-uint.md b/docs/es/sql-reference/data-types/int-uint.md
deleted file mode 100644
index 976e1eb07822..000000000000
--- a/docs/es/sql-reference/data-types/int-uint.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64
----
-
-# UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64 {#uint8-uint16-uint32-uint64-int8-int16-int32-int64}
-
-Enteros de longitud fija, con o sin signo.
-
-## Rangos Int {#int-ranges}
-
--   Int8 - \[-128 : 127\]
--   Int16 - \[-32768 : 32767\]
--   Int32 - \[-2147483648 : 2147483647\]
--   Int64 - \[-9223372036854775808 : 9223372036854775807\]
-
-## Rangos de Uint {#uint-ranges}
-
--   UInt8 - \[0 : 255\]
--   UInt16 - \[0 : 65535\]
--   UInt32 - \[0 : 4294967295\]
--   UInt64 - \[0 : 18446744073709551615\]
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/int_uint/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/nested-data-structures/index.md b/docs/es/sql-reference/data-types/nested-data-structures/index.md
deleted file mode 100644
index 89ce044f7b45..000000000000
--- a/docs/es/sql-reference/data-types/nested-data-structures/index.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Estructuras de datos anidados
-toc_hidden: true
-toc_priority: 54
-toc_title: oculto
----
-
-# Estructuras de datos anidados {#nested-data-structures}
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/nested_data_structures/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/nested-data-structures/nested.md b/docs/es/sql-reference/data-types/nested-data-structures/nested.md
deleted file mode 100644
index bf48b9159541..000000000000
--- a/docs/es/sql-reference/data-types/nested-data-structures/nested.md
+++ /dev/null
@@ -1,106 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 57
-toc_title: Anidado (Nombre1 Tipo1, Nombre2 Tipo2, ...)
----
-
-# Nested(name1 Type1, Name2 Type2, …) {#nestedname1-type1-name2-type2}
-
-A nested data structure is like a table inside a cell. The parameters of a nested data structure – the column names and types – are specified the same way as in a [CREATE TABLE](../../../sql-reference/statements/create.md) consulta. Cada fila de la tabla puede corresponder a cualquier número de filas en una estructura de datos anidada.
-
-Ejemplo:
-
-``` sql
-CREATE TABLE test.visits
-(
-    CounterID UInt32,
-    StartDate Date,
-    Sign Int8,
-    IsNew UInt8,
-    VisitID UInt64,
-    UserID UInt64,
-    ...
-    Goals Nested
-    (
-        ID UInt32,
-        Serial UInt32,
-        EventTime DateTime,
-        Price Int64,
-        OrderID String,
-        CurrencyID UInt32
-    ),
-    ...
-) ENGINE = CollapsingMergeTree(StartDate, intHash32(UserID), (CounterID, StartDate, intHash32(UserID), VisitID), 8192, Sign)
-```
-
-Este ejemplo declara la `Goals` estructura de datos anidada, que contiene datos sobre conversiones (objetivos alcanzados). Cada fila en el ‘visits’ la tabla puede corresponder a cero o cualquier número de conversiones.
-
-Solo se admite un único nivel de anidamiento. Las columnas de estructuras anidadas que contienen matrices son equivalentes a matrices multidimensionales, por lo que tienen un soporte limitado (no hay soporte para almacenar estas columnas en tablas con el motor MergeTree).
-
-En la mayoría de los casos, cuando se trabaja con una estructura de datos anidada, sus columnas se especifican con nombres de columna separados por un punto. Estas columnas forman una matriz de tipos coincidentes. Todas las matrices de columnas de una sola estructura de datos anidados tienen la misma longitud.
-
-Ejemplo:
-
-``` sql
-SELECT
-    Goals.ID,
-    Goals.EventTime
-FROM test.visits
-WHERE CounterID = 101500 AND length(Goals.ID) < 5
-LIMIT 10
-```
-
-``` text
-┌─Goals.ID───────────────────────┬─Goals.EventTime───────────────────────────────────────────────────────────────────────────┐
-│ [1073752,591325,591325]        │ ['2014-03-17 16:38:10','2014-03-17 16:38:48','2014-03-17 16:42:27']                       │
-│ [1073752]                      │ ['2014-03-17 00:28:25']                                                                   │
-│ [1073752]                      │ ['2014-03-17 10:46:20']                                                                   │
-│ [1073752,591325,591325,591325] │ ['2014-03-17 13:59:20','2014-03-17 22:17:55','2014-03-17 22:18:07','2014-03-17 22:18:51'] │
-│ []                             │ []                                                                                        │
-│ [1073752,591325,591325]        │ ['2014-03-17 11:37:06','2014-03-17 14:07:47','2014-03-17 14:36:21']                       │
-│ []                             │ []                                                                                        │
-│ []                             │ []                                                                                        │
-│ [591325,1073752]               │ ['2014-03-17 00:46:05','2014-03-17 00:46:05']                                             │
-│ [1073752,591325,591325,591325] │ ['2014-03-17 13:28:33','2014-03-17 13:30:26','2014-03-17 18:51:21','2014-03-17 18:51:45'] │
-└────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-Es más fácil pensar en una estructura de datos anidados como un conjunto de múltiples matrices de columnas de la misma longitud.
-
-El único lugar donde una consulta SELECT puede especificar el nombre de una estructura de datos anidada completa en lugar de columnas individuales es la cláusula ARRAY JOIN. Para obtener más información, consulte “ARRAY JOIN clause”. Ejemplo:
-
-``` sql
-SELECT
-    Goal.ID,
-    Goal.EventTime
-FROM test.visits
-ARRAY JOIN Goals AS Goal
-WHERE CounterID = 101500 AND length(Goals.ID) < 5
-LIMIT 10
-```
-
-``` text
-┌─Goal.ID─┬──────Goal.EventTime─┐
-│ 1073752 │ 2014-03-17 16:38:10 │
-│  591325 │ 2014-03-17 16:38:48 │
-│  591325 │ 2014-03-17 16:42:27 │
-│ 1073752 │ 2014-03-17 00:28:25 │
-│ 1073752 │ 2014-03-17 10:46:20 │
-│ 1073752 │ 2014-03-17 13:59:20 │
-│  591325 │ 2014-03-17 22:17:55 │
-│  591325 │ 2014-03-17 22:18:07 │
-│  591325 │ 2014-03-17 22:18:51 │
-│ 1073752 │ 2014-03-17 11:37:06 │
-└─────────┴─────────────────────┘
-```
-
-No puede realizar SELECT para toda una estructura de datos anidados. Solo puede enumerar explícitamente columnas individuales que forman parte de él.
-
-Para una consulta INSERT, debe pasar todas las matrices de columnas de componentes de una estructura de datos anidada por separado (como si fueran matrices de columnas individuales). Durante la inserción, el sistema comprueba que tienen la misma longitud.
-
-Para una consulta DESCRIBE, las columnas de una estructura de datos anidada se enumeran por separado de la misma manera.
-
-La consulta ALTER para elementos en una estructura de datos anidados tiene limitaciones.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/nested_data_structures/nested/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/nullable.md b/docs/es/sql-reference/data-types/nullable.md
deleted file mode 100644
index bec05e1c756d..000000000000
--- a/docs/es/sql-reference/data-types/nullable.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 54
-toc_title: 'NULL'
----
-
-# Nivel de Cifrado WEP) {#data_type-nullable}
-
-Permite almacenar marcador especial ([NULL](../../sql-reference/syntax.md)) que denota “missing value” junto con los valores normales permitidos por `TypeName`. Por ejemplo, un `Nullable(Int8)` tipo columna puede almacenar `Int8` valores de tipo, y las filas que no tienen un valor almacenarán `NULL`.
-
-Para un `TypeName`, no puede usar tipos de datos compuestos [Matriz](array.md) y [Tupla](tuple.md). Los tipos de datos compuestos pueden contener `Nullable` valores de tipo, como `Array(Nullable(Int8))`.
-
-A `Nullable` no se puede incluir en los índices de tabla.
-
-`NULL` es el valor predeterminado para cualquier `Nullable` tipo, a menos que se especifique lo contrario en la configuración del servidor ClickHouse.
-
-## Características de almacenamiento {#storage-features}
-
-Almacenar `Nullable` en una columna de tabla, ClickHouse usa un archivo separado con `NULL` máscaras además del archivo normal con valores. Las entradas en el archivo de máscaras permiten ClickHouse distinguir entre `NULL` y un valor predeterminado del tipo de datos correspondiente para cada fila de la tabla. Debido a un archivo adicional, `Nullable` La columna consume espacio de almacenamiento adicional en comparación con una normal similar.
-
-!!! info "Nota"
-    Utilizar `Nullable` casi siempre afecta negativamente al rendimiento, tenga esto en cuenta al diseñar sus bases de datos.
-
-## Ejemplo de uso {#usage-example}
-
-``` sql
-CREATE TABLE t_null(x Int8, y Nullable(Int8)) ENGINE TinyLog
-```
-
-``` sql
-INSERT INTO t_null VALUES (1, NULL), (2, 3)
-```
-
-``` sql
-SELECT x + y FROM t_null
-```
-
-``` text
-┌─plus(x, y)─┐
-│       ᴺᵁᴸᴸ │
-│          5 │
-└────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/nullable/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/simpleaggregatefunction.md b/docs/es/sql-reference/data-types/simpleaggregatefunction.md
deleted file mode 100644
index 48591993ceaf..000000000000
--- a/docs/es/sql-reference/data-types/simpleaggregatefunction.md
+++ /dev/null
@@ -1,33 +0,0 @@
-# SimpleAggregateFunction {#data-type-simpleaggregatefunction}
-
-El tipo de dato `SimpleAggregateFunction(name, types_of_arguments…)` almacena el valor actual de la función agregada, no almacena su estado completo como hace [`AggregateFunction`](aggregatefunction.md). Esta optimización se puede aplicar a las funciones con la siguiente propiedad: el resultado de aplicar una función `f` a un conjunto de filas `S1 UNION ALL S2` se puede obtener aplicando `f` a partes de la fila establecida por separado, y luego aplicar de nuevo `f` los resultados: `f(S1 UNION ALL S2) = f(f(S1) UNION ALL f(S2))`. Un ejemplo de función con esta propiedad es la suma. Esta propiedad garantiza que los resultados de agregación parcial son suficientes para calcular el combinado, por lo que no tenemos que almacenar y procesar ningún dato adicional.
-
-La lista de functiones de agregación soportadas son:
-
--   [`any`](../../sql-reference/aggregate-functions/reference.md#agg_function-any)
--   [`anyLast`](../../sql-reference/aggregate-functions/reference.md#anylastx)
--   [`min`](../../sql-reference/aggregate-functions/reference.md#agg_function-min)
--   [`max`](../../sql-reference/aggregate-functions/reference.md#agg_function-max)
--   [`sum`](../../sql-reference/aggregate-functions/reference.md#agg_function-sum)
--   [`groupBitAnd`](../../sql-reference/aggregate-functions/reference.md#groupbitand)
--   [`groupBitOr`](../../sql-reference/aggregate-functions/reference.md#groupbitor)
--   [`groupBitXor`](../../sql-reference/aggregate-functions/reference.md#groupbitxor)
-
-Cuando se usa `SimpleAggregateFunction(func, Type)` el resultado es almacenado datos `Type` así que no necesita aplicar los suficos `-Merge`/`-State` a las funciones de agregación para usarlas. Para la misma función de agregación `SimpleAggregateFunction` tiene un mejor rendimiento que `AggregateFunction`.
-
-**Parámetros**
-
--   Nombre de la función de agregado.
--   Tipos de los argumentos de la función agregada.
-
-**Ejemplo**
-
-``` sql
-CREATE TABLE t
-(
-    column1 SimpleAggregateFunction(sum, UInt64),
-    column2 SimpleAggregateFunction(any, String)
-) ENGINE = ...
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/simpleaggregatefunction/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/special-data-types/expression.md b/docs/es/sql-reference/data-types/special-data-types/expression.md
deleted file mode 100644
index d1e170ab3439..000000000000
--- a/docs/es/sql-reference/data-types/special-data-types/expression.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: Expresion
----
-
-# Expresion {#expression}
-
-Las expresiones se utilizan para representar lambdas en funciones de orden superior.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/special_data_types/expression/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/special-data-types/index.md b/docs/es/sql-reference/data-types/special-data-types/index.md
deleted file mode 100644
index 8c14a96bca99..000000000000
--- a/docs/es/sql-reference/data-types/special-data-types/index.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Tipos de datos especiales
-toc_hidden: true
-toc_priority: 55
-toc_title: oculto
----
-
-# Tipos de datos especiales {#special-data-types}
-
-Los valores de tipo de datos especiales no se pueden serializar para guardar en una tabla o salida en los resultados de la consulta, pero se pueden usar como un resultado intermedio durante la ejecución de la consulta.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/special_data_types/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/special-data-types/interval.md b/docs/es/sql-reference/data-types/special-data-types/interval.md
deleted file mode 100644
index c4c4a67c8cdd..000000000000
--- a/docs/es/sql-reference/data-types/special-data-types/interval.md
+++ /dev/null
@@ -1,85 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: Intervalo
----
-
-# Intervalo {#data-type-interval}
-
-La familia de tipos de datos que representan intervalos de fecha y hora. Los tipos resultantes del [INTERVAL](../../../sql-reference/operators/index.md#operator-interval) operador.
-
-!!! warning "Advertencia"
-    `Interval` los valores de tipo de datos no se pueden almacenar en tablas.
-
-Estructura:
-
--   Intervalo de tiempo como un valor entero sin signo.
--   Tipo de intervalo.
-
-Tipos de intervalo admitidos:
-
--   `SECOND`
--   `MINUTE`
--   `HOUR`
--   `DAY`
--   `WEEK`
--   `MONTH`
--   `QUARTER`
--   `YEAR`
-
-Para cada tipo de intervalo, hay un tipo de datos independiente. Por ejemplo, el `DAY` el intervalo corresponde a la `IntervalDay` tipo de datos:
-
-``` sql
-SELECT toTypeName(INTERVAL 4 DAY)
-```
-
-``` text
-┌─toTypeName(toIntervalDay(4))─┐
-│ IntervalDay                  │
-└──────────────────────────────┘
-```
-
-## Observaciones de uso {#data-type-interval-usage-remarks}
-
-Usted puede utilizar `Interval`-type valores en operaciones aritméticas con [Fecha](../../../sql-reference/data-types/date.md) y [FechaHora](../../../sql-reference/data-types/datetime.md)-type valores. Por ejemplo, puede agregar 4 días a la hora actual:
-
-``` sql
-SELECT now() as current_date_time, current_date_time + INTERVAL 4 DAY
-```
-
-``` text
-┌───current_date_time─┬─plus(now(), toIntervalDay(4))─┐
-│ 2019-10-23 10:58:45 │           2019-10-27 10:58:45 │
-└─────────────────────┴───────────────────────────────┘
-```
-
-Los intervalos con diferentes tipos no se pueden combinar. No puedes usar intervalos como `4 DAY 1 HOUR`. Especifique los intervalos en unidades que son más pequeñas o iguales que la unidad más pequeña del intervalo, por ejemplo, el intervalo `1 day and an hour` se puede expresar como `25 HOUR` o `90000 SECOND`.
-
-No puede realizar operaciones aritméticas con `Interval`-type valores, pero puede agregar intervalos de diferentes tipos en consecuencia a los valores en `Date` o `DateTime` tipos de datos. Por ejemplo:
-
-``` sql
-SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR
-```
-
-``` text
-┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐
-│ 2019-10-23 11:16:28 │                                    2019-10-27 14:16:28 │
-└─────────────────────┴────────────────────────────────────────────────────────┘
-```
-
-La siguiente consulta provoca una excepción:
-
-``` sql
-select now() AS current_date_time, current_date_time + (INTERVAL 4 DAY + INTERVAL 3 HOUR)
-```
-
-``` text
-Received exception from server (version 19.14.1):
-Code: 43. DB::Exception: Received from localhost:9000. DB::Exception: Wrong argument types for function plus: if one argument is Interval, then another must be Date or DateTime..
-```
-
-## Ver también {#see-also}
-
--   [INTERVAL](../../../sql-reference/operators/index.md#operator-interval) operador
--   [ToInterval](../../../sql-reference/functions/type-conversion-functions.md#function-tointerval) funciones de conversión de tipo
diff --git a/docs/es/sql-reference/data-types/special-data-types/nothing.md b/docs/es/sql-reference/data-types/special-data-types/nothing.md
deleted file mode 100644
index 31c881d1cc6b..000000000000
--- a/docs/es/sql-reference/data-types/special-data-types/nothing.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: Nada
----
-
-# Nada {#nothing}
-
-El único propósito de este tipo de datos es representar casos en los que no se espera un valor. Entonces no puedes crear un `Nothing` valor de tipo.
-
-Por ejemplo, literal [NULL](../../../sql-reference/syntax.md#null-literal) tiene tipo de `Nullable(Nothing)`. Ver más sobre [NULL](../../../sql-reference/data-types/nullable.md).
-
-El `Nothing` tipo puede también se utiliza para denotar matrices vacías:
-
-``` sql
-SELECT toTypeName(array())
-```
-
-``` text
-┌─toTypeName(array())─┐
-│ Array(Nothing)      │
-└─────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/special_data_types/nothing/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/special-data-types/set.md b/docs/es/sql-reference/data-types/special-data-types/set.md
deleted file mode 100644
index 70b6d5d7383d..000000000000
--- a/docs/es/sql-reference/data-types/special-data-types/set.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: Establecer
----
-
-# Establecer {#set}
-
-Utilizado para la mitad derecha de un [IN](../../operators/in.md#select-in-operators) expresion.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/special_data_types/set/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/string.md b/docs/es/sql-reference/data-types/string.md
deleted file mode 100644
index 603dd36cd8df..000000000000
--- a/docs/es/sql-reference/data-types/string.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: Cadena
----
-
-# Cadena {#string}
-
-Cuerdas de una longitud arbitraria. La longitud no está limitada. El valor puede contener un conjunto arbitrario de bytes, incluidos los bytes nulos.
-El tipo String reemplaza los tipos VARCHAR, BLOB, CLOB y otros de otros DBMS.
-
-## Codificación {#encodings}
-
-ClickHouse no tiene el concepto de codificaciones. Las cadenas pueden contener un conjunto arbitrario de bytes, que se almacenan y salen tal cual.
-Si necesita almacenar textos, le recomendamos que utilice la codificación UTF-8. Como mínimo, si su terminal usa UTF-8 (según lo recomendado), puede leer y escribir sus valores sin realizar conversiones.
-Del mismo modo, ciertas funciones para trabajar con cadenas tienen variaciones separadas que funcionan bajo el supuesto de que la cadena contiene un conjunto de bytes que representan un texto codificado en UTF-8.
-Por ejemplo, el ‘length’ función calcula la longitud de cadena en bytes, mientras que la ‘lengthUTF8’ función calcula la longitud de la cadena en puntos de código Unicode, suponiendo que el valor está codificado UTF-8.
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/string/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/tuple.md b/docs/es/sql-reference/data-types/tuple.md
deleted file mode 100644
index d6411df5290f..000000000000
--- a/docs/es/sql-reference/data-types/tuple.md
+++ /dev/null
@@ -1,52 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 53
-toc_title: Tuple (T1, T2, ...)
----
-
-# Tuple(t1, T2, …) {#tuplet1-t2}
-
-Una tupla de elementos, cada uno con un individuo [tipo](index.md#data_types).
-
-Las tuplas se utilizan para la agrupación temporal de columnas. Las columnas se pueden agrupar cuando se usa una expresión IN en una consulta y para especificar ciertos parámetros formales de las funciones lambda. Para obtener más información, consulte las secciones [IN operadores](../../sql-reference/operators/in.md) y [Funciones de orden superior](../../sql-reference/functions/higher-order-functions.md).
-
-Las tuplas pueden ser el resultado de una consulta. En este caso, para formatos de texto distintos de JSON, los valores están separados por comas entre corchetes. En formatos JSON, las tuplas se generan como matrices (entre corchetes).
-
-## Creación de una tupla {#creating-a-tuple}
-
-Puedes usar una función para crear una tupla:
-
-``` sql
-tuple(T1, T2, ...)
-```
-
-Ejemplo de creación de una tupla:
-
-``` sql
-SELECT tuple(1,'a') AS x, toTypeName(x)
-```
-
-``` text
-┌─x───────┬─toTypeName(tuple(1, 'a'))─┐
-│ (1,'a') │ Tuple(UInt8, String)      │
-└─────────┴───────────────────────────┘
-```
-
-## Trabajar con tipos de datos {#working-with-data-types}
-
-Al crear una tupla sobre la marcha, ClickHouse detecta automáticamente el tipo de cada argumento como el mínimo de los tipos que pueden almacenar el valor del argumento. Si el argumento es [NULL](../../sql-reference/syntax.md#null-literal), el tipo del elemento de tupla es [NULL](nullable.md).
-
-Ejemplo de detección automática de tipos de datos:
-
-``` sql
-SELECT tuple(1, NULL) AS x, toTypeName(x)
-```
-
-``` text
-┌─x────────┬─toTypeName(tuple(1, NULL))──────┐
-│ (1,NULL) │ Tuple(UInt8, Nullable(Nothing)) │
-└──────────┴─────────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/tuple/) <!--hide-->
diff --git a/docs/es/sql-reference/data-types/uuid.md b/docs/es/sql-reference/data-types/uuid.md
deleted file mode 100644
index 633db62d65f5..000000000000
--- a/docs/es/sql-reference/data-types/uuid.md
+++ /dev/null
@@ -1,77 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: UUID
----
-
-# UUID {#uuid-data-type}
-
-Un identificador único universal (UUID) es un número de 16 bytes utilizado para identificar registros. Para obtener información detallada sobre el UUID, consulte [Wikipedia](https://en.wikipedia.org/wiki/Universally_unique_identifier).
-
-El ejemplo de valor de tipo UUID se representa a continuación:
-
-``` text
-61f0c404-5cb3-11e7-907b-a6006ad3dba0
-```
-
-Si no especifica el valor de la columna UUID al insertar un nuevo registro, el valor UUID se rellena con cero:
-
-``` text
-00000000-0000-0000-0000-000000000000
-```
-
-## Cómo generar {#how-to-generate}
-
-Para generar el valor UUID, ClickHouse proporciona el [GenerateUUIDv4](../../sql-reference/functions/uuid-functions.md) función.
-
-## Ejemplo de uso {#usage-example}
-
-**Ejemplo 1**
-
-En este ejemplo se muestra la creación de una tabla con la columna de tipo UUID e insertar un valor en la tabla.
-
-``` sql
-CREATE TABLE t_uuid (x UUID, y String) ENGINE=TinyLog
-```
-
-``` sql
-INSERT INTO t_uuid SELECT generateUUIDv4(), 'Example 1'
-```
-
-``` sql
-SELECT * FROM t_uuid
-```
-
-``` text
-┌────────────────────────────────────x─┬─y─────────┐
-│ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │
-└──────────────────────────────────────┴───────────┘
-```
-
-**Ejemplo 2**
-
-En este ejemplo, el valor de la columna UUID no se especifica al insertar un nuevo registro.
-
-``` sql
-INSERT INTO t_uuid (y) VALUES ('Example 2')
-```
-
-``` sql
-SELECT * FROM t_uuid
-```
-
-``` text
-┌────────────────────────────────────x─┬─y─────────┐
-│ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │
-│ 00000000-0000-0000-0000-000000000000 │ Example 2 │
-└──────────────────────────────────────┴───────────┘
-```
-
-## Restricción {#restrictions}
-
-El tipo de datos UUID sólo admite funciones que [Cadena](string.md) tipo de datos también soporta (por ejemplo, [minuto](../../sql-reference/aggregate-functions/reference.md#agg_function-min), [máximo](../../sql-reference/aggregate-functions/reference.md#agg_function-max), y [contar](../../sql-reference/aggregate-functions/reference.md#agg_function-count)).
-
-El tipo de datos UUID no es compatible con operaciones aritméticas (por ejemplo, [abdominales](../../sql-reference/functions/arithmetic-functions.md#arithm_func-abs)) o funciones agregadas, tales como [resumir](../../sql-reference/aggregate-functions/reference.md#agg_function-sum) y [avg](../../sql-reference/aggregate-functions/reference.md#agg_function-avg).
-
-[Artículo Original](https://clickhouse.tech/docs/en/data_types/uuid/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md b/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md
deleted file mode 100644
index 68f18367872f..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: "Diccionarios jer\xE1rquicos"
----
-
-# Diccionarios jerárquicos {#hierarchical-dictionaries}
-
-ClickHouse soporta diccionarios jerárquicos con un [llave numérica](external-dicts-dict-structure.md#ext_dict-numeric-key).
-
-Mira la siguiente estructura jerárquica:
-
-``` text
-0 (Common parent)
-│
-├── 1 (Russia)
-│   │
-│   └── 2 (Moscow)
-│       │
-│       └── 3 (Center)
-│
-└── 4 (Great Britain)
-    │
-    └── 5 (London)
-```
-
-Esta jerarquía se puede expresar como la siguiente tabla de diccionario.
-
-| region_id | parent_region | nombre_región |
-|------------|----------------|----------------|
-| 1          | 0              | Rusia          |
-| 2          | 1              | Moscu          |
-| 3          | 2              | Centrar        |
-| 4          | 0              | Gran Bretaña   |
-| 5          | 4              | Londres        |
-
-Esta tabla contiene una columna `parent_region` que contiene la clave del padre más cercano para el elemento.
-
-ClickHouse soporta el [jerárquica](external-dicts-dict-structure.md#hierarchical-dict-attr) propiedad para [diccionario externo](index.md) atributo. Esta propiedad le permite configurar el diccionario jerárquico similar al descrito anteriormente.
-
-El [dictGetHierarchy](../../../sql-reference/functions/ext-dict-functions.md#dictgethierarchy) función le permite obtener la cadena principal de un elemento.
-
-Para nuestro ejemplo, la estructura del diccionario puede ser la siguiente:
-
-``` xml
-<dictionary>
-    <structure>
-        <id>
-            <name>region_id</name>
-        </id>
-
-        <attribute>
-            <name>parent_region</name>
-            <type>UInt64</type>
-            <null_value>0</null_value>
-            <hierarchical>true</hierarchical>
-        </attribute>
-
-        <attribute>
-            <name>region_name</name>
-            <type>String</type>
-            <null_value></null_value>
-        </attribute>
-
-    </structure>
-</dictionary>
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_hierarchical/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md b/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md
deleted file mode 100644
index 85001c181eb4..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md
+++ /dev/null
@@ -1,396 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: Almacenamiento de diccionarios en la memoria
----
-
-# Almacenamiento de diccionarios en la memoria {#dicts-external-dicts-dict-layout}
-
-Hay una variedad de formas de almacenar diccionarios en la memoria.
-
-Recomendamos [plano](#flat), [Hashed](#dicts-external_dicts_dict_layout-hashed) y [Método de codificación de datos:](#complex-key-hashed). que proporcionan una velocidad de procesamiento óptima.
-
-No se recomienda el almacenamiento en caché debido al rendimiento potencialmente bajo y las dificultades para seleccionar los parámetros óptimos. Lea más en la sección “[cache](#cache)”.
-
-Hay varias formas de mejorar el rendimiento del diccionario:
-
--   Llame a la función para trabajar con el diccionario después `GROUP BY`.
--   Marque los atributos para extraer como inyectivos. Un atributo se llama injective si diferentes valores de atributo corresponden a claves diferentes. Entonces, cuando `GROUP BY` utiliza una función que obtiene un valor de atributo mediante la clave, esta función se elimina automáticamente de `GROUP BY`.
-
-ClickHouse genera una excepción para errores con diccionarios. Ejemplos de errores:
-
--   No se pudo cargar el diccionario al que se accede.
--   Error al consultar un `cached` diccionario.
-
-Puede ver la lista de diccionarios externos y sus estados en el `system.dictionaries` tabla.
-
-La configuración se ve así:
-
-``` xml
-<yandex>
-    <dictionary>
-        ...
-        <layout>
-            <layout_type>
-                <!-- layout settings -->
-            </layout_type>
-        </layout>
-        ...
-    </dictionary>
-</yandex>
-```
-
-Correspondiente [Consulta DDL](../../statements/create.md#create-dictionary-query):
-
-``` sql
-CREATE DICTIONARY (...)
-...
-LAYOUT(LAYOUT_TYPE(param value)) -- layout settings
-...
-```
-
-## Maneras de almacenar diccionarios en la memoria {#ways-to-store-dictionaries-in-memory}
-
--   [plano](#flat)
--   [Hashed](#dicts-external_dicts_dict_layout-hashed)
--   [Sistema abierto.](#dicts-external_dicts_dict_layout-sparse_hashed)
--   [cache](#cache)
--   [directo](#direct)
--   [range_hashed](#range-hashed)
--   [Método de codificación de datos:](#complex-key-hashed)
--   [complejo_key_cache](#complex-key-cache)
--   [Método de codificación de datos:](#ip-trie)
-
-### plano {#flat}
-
-El diccionario está completamente almacenado en la memoria en forma de matrices planas. ¿Cuánta memoria usa el diccionario? La cantidad es proporcional al tamaño de la clave más grande (en el espacio utilizado).
-
-La clave del diccionario tiene el `UInt64` tipo y el valor está limitado a 500.000. Si se descubre una clave más grande al crear el diccionario, ClickHouse produce una excepción y no crea el diccionario.
-
-Se admiten todos los tipos de fuentes. Al actualizar, los datos (de un archivo o de una tabla) se leen en su totalidad.
-
-Este método proporciona el mejor rendimiento entre todos los métodos disponibles para almacenar el diccionario.
-
-Ejemplo de configuración:
-
-``` xml
-<layout>
-  <flat />
-</layout>
-```
-
-o
-
-``` sql
-LAYOUT(FLAT())
-```
-
-### Hashed {#dicts-external_dicts_dict_layout-hashed}
-
-El diccionario está completamente almacenado en la memoria en forma de una tabla hash. El diccionario puede contener cualquier número de elementos con cualquier identificador En la práctica, el número de claves puede alcanzar decenas de millones de elementos.
-
-Se admiten todos los tipos de fuentes. Al actualizar, los datos (de un archivo o de una tabla) se leen en su totalidad.
-
-Ejemplo de configuración:
-
-``` xml
-<layout>
-  <hashed />
-</layout>
-```
-
-o
-
-``` sql
-LAYOUT(HASHED())
-```
-
-### Sistema abierto {#dicts-external_dicts_dict_layout-sparse_hashed}
-
-Similar a `hashed`, pero usa menos memoria a favor más uso de CPU.
-
-Ejemplo de configuración:
-
-``` xml
-<layout>
-  <sparse_hashed />
-</layout>
-```
-
-``` sql
-LAYOUT(SPARSE_HASHED())
-```
-
-### Método de codificación de datos: {#complex-key-hashed}
-
-Este tipo de almacenamiento es para su uso con material compuesto [claves](external-dicts-dict-structure.md). Similar a `hashed`.
-
-Ejemplo de configuración:
-
-``` xml
-<layout>
-  <complex_key_hashed />
-</layout>
-```
-
-``` sql
-LAYOUT(COMPLEX_KEY_HASHED())
-```
-
-### range_hashed {#range-hashed}
-
-El diccionario se almacena en la memoria en forma de una tabla hash con una matriz ordenada de rangos y sus valores correspondientes.
-
-Este método de almacenamiento funciona de la misma manera que hash y permite el uso de intervalos de fecha / hora (tipo numérico arbitrario) además de la clave.
-
-Ejemplo: La tabla contiene descuentos para cada anunciante en el formato:
-
-``` text
-+---------|-------------|-------------|------+
-| advertiser id | discount start date | discount end date | amount |
-+===============+=====================+===================+========+
-| 123           | 2015-01-01          | 2015-01-15        | 0.15   |
-+---------|-------------|-------------|------+
-| 123           | 2015-01-16          | 2015-01-31        | 0.25   |
-+---------|-------------|-------------|------+
-| 456           | 2015-01-01          | 2015-01-15        | 0.05   |
-+---------|-------------|-------------|------+
-```
-
-Para utilizar un ejemplo para intervalos de fechas, defina el `range_min` y `range_max` elementos en el [estructura](external-dicts-dict-structure.md). Estos elementos deben contener elementos `name` y`type` (si `type` no se especifica, se utilizará el tipo predeterminado - Fecha). `type` puede ser de cualquier tipo numérico (Fecha / DateTime / UInt64 / Int32 / otros).
-
-Ejemplo:
-
-``` xml
-<structure>
-    <id>
-        <name>Id</name>
-    </id>
-    <range_min>
-        <name>first</name>
-        <type>Date</type>
-    </range_min>
-    <range_max>
-        <name>last</name>
-        <type>Date</type>
-    </range_max>
-    ...
-```
-
-o
-
-``` sql
-CREATE DICTIONARY somedict (
-    id UInt64,
-    first Date,
-    last Date
-)
-PRIMARY KEY id
-LAYOUT(RANGE_HASHED())
-RANGE(MIN first MAX last)
-```
-
-Para trabajar con estos diccionarios, debe pasar un argumento adicional al `dictGetT` función, para la que se selecciona un rango:
-
-``` sql
-dictGetT('dict_name', 'attr_name', id, date)
-```
-
-Esta función devuelve el valor para el `id`s y el intervalo de fechas que incluye la fecha pasada.
-
-Detalles del algoritmo:
-
--   Si el `id` no se encuentra o no se encuentra un rango para el `id` devuelve el valor predeterminado para el diccionario.
--   Si hay rangos superpuestos, puede usar cualquiera.
--   Si el delimitador de rango es `NULL` o una fecha no válida (como 1900-01-01 o 2039-01-01), el rango se deja abierto. La gama puede estar abierta en ambos lados.
-
-Ejemplo de configuración:
-
-``` xml
-<yandex>
-        <dictionary>
-
-                ...
-
-                <layout>
-                        <range_hashed />
-                </layout>
-
-                <structure>
-                        <id>
-                                <name>Abcdef</name>
-                        </id>
-                        <range_min>
-                                <name>StartTimeStamp</name>
-                                <type>UInt64</type>
-                        </range_min>
-                        <range_max>
-                                <name>EndTimeStamp</name>
-                                <type>UInt64</type>
-                        </range_max>
-                        <attribute>
-                                <name>XXXType</name>
-                                <type>String</type>
-                                <null_value />
-                        </attribute>
-                </structure>
-
-        </dictionary>
-</yandex>
-```
-
-o
-
-``` sql
-CREATE DICTIONARY somedict(
-    Abcdef UInt64,
-    StartTimeStamp UInt64,
-    EndTimeStamp UInt64,
-    XXXType String DEFAULT ''
-)
-PRIMARY KEY Abcdef
-RANGE(MIN StartTimeStamp MAX EndTimeStamp)
-```
-
-### cache {#cache}
-
-El diccionario se almacena en una memoria caché que tiene un número fijo de celdas. Estas celdas contienen elementos de uso frecuente.
-
-Al buscar un diccionario, primero se busca en la memoria caché. Para cada bloque de datos, todas las claves que no se encuentran en la memoria caché o están desactualizadas se solicitan desde el origen utilizando `SELECT attrs... FROM db.table WHERE id IN (k1, k2, ...)`. Los datos recibidos se escriben en la memoria caché.
-
-Para los diccionarios de caché, la caducidad [vida](external-dicts-dict-lifetime.md) de datos en la memoria caché se puede establecer. Si más tiempo que `lifetime` ha pasado desde que se cargaron los datos en una celda, el valor de la celda no se usa y se vuelve a solicitar la próxima vez que se deba usar.
-Esta es la menos efectiva de todas las formas de almacenar diccionarios. La velocidad de la memoria caché depende en gran medida de la configuración correcta y del escenario de uso. Un diccionario de tipo de caché funciona bien solo cuando las tasas de aciertos son lo suficientemente altas (recomendado 99% y superior). Puede ver la tasa de aciertos promedio en el `system.dictionaries` tabla.
-
-Para mejorar el rendimiento de la caché, utilice una subconsulta con `LIMIT`, y llame a la función con el diccionario externamente.
-
-Apoyar [fuente](external-dicts-dict-sources.md): MySQL, ClickHouse, ejecutable, HTTP.
-
-Ejemplo de configuración:
-
-``` xml
-<layout>
-    <cache>
-        <!-- The size of the cache, in number of cells. Rounded up to a power of two. -->
-        <size_in_cells>1000000000</size_in_cells>
-    </cache>
-</layout>
-```
-
-o
-
-``` sql
-LAYOUT(CACHE(SIZE_IN_CELLS 1000000000))
-```
-
-Establezca un tamaño de caché lo suficientemente grande. Necesitas experimentar para seleccionar el número de celdas:
-
-1.  Establecer algún valor.
-2.  Ejecute consultas hasta que la memoria caché esté completamente llena.
-3.  Evalúe el consumo de memoria utilizando el `system.dictionaries` tabla.
-4.  Aumente o disminuya el número de celdas hasta que se alcance el consumo de memoria requerido.
-
-!!! warning "Advertencia"
-    No use ClickHouse como fuente, ya que es lento procesar consultas con lecturas aleatorias.
-
-### complejo_key_cache {#complex-key-cache}
-
-Este tipo de almacenamiento es para su uso con material compuesto [claves](external-dicts-dict-structure.md). Similar a `cache`.
-
-### directo {#direct}
-
-El diccionario no se almacena en la memoria y va directamente a la fuente durante el procesamiento de una solicitud.
-
-La clave del diccionario tiene el `UInt64` tipo.
-
-Todos los tipos de [fuente](external-dicts-dict-sources.md), excepto los archivos locales, son compatibles.
-
-Ejemplo de configuración:
-
-``` xml
-<layout>
-  <direct />
-</layout>
-```
-
-o
-
-``` sql
-LAYOUT(DIRECT())
-```
-
-### Método de codificación de datos: {#ip-trie}
-
-Este tipo de almacenamiento sirve para asignar prefijos de red (direcciones IP) a metadatos como ASN.
-
-Ejemplo: La tabla contiene prefijos de red y su correspondiente número AS y código de país:
-
-``` text
-  +-----------|-----|------+
-  | prefix          | asn   | cca2   |
-  +=================+=======+========+
-  | 202.79.32.0/20  | 17501 | NP     |
-  +-----------|-----|------+
-  | 2620:0:870::/48 | 3856  | US     |
-  +-----------|-----|------+
-  | 2a02:6b8:1::/48 | 13238 | RU     |
-  +-----------|-----|------+
-  | 2001:db8::/32   | 65536 | ZZ     |
-  +-----------|-----|------+
-```
-
-Cuando se utiliza este tipo de diseño, la estructura debe tener una clave compuesta.
-
-Ejemplo:
-
-``` xml
-<structure>
-    <key>
-        <attribute>
-            <name>prefix</name>
-            <type>String</type>
-        </attribute>
-    </key>
-    <attribute>
-            <name>asn</name>
-            <type>UInt32</type>
-            <null_value />
-    </attribute>
-    <attribute>
-            <name>cca2</name>
-            <type>String</type>
-            <null_value>??</null_value>
-    </attribute>
-    ...
-```
-
-o
-
-``` sql
-CREATE DICTIONARY somedict (
-    prefix String,
-    asn UInt32,
-    cca2 String DEFAULT '??'
-)
-PRIMARY KEY prefix
-```
-
-La clave debe tener solo un atributo de tipo String que contenga un prefijo IP permitido. Todavía no se admiten otros tipos.
-
-Para consultas, debe utilizar las mismas funciones (`dictGetT` con una tupla) como para diccionarios con claves compuestas:
-
-``` sql
-dictGetT('dict_name', 'attr_name', tuple(ip))
-```
-
-La función toma cualquiera `UInt32` para IPv4, o `FixedString(16)` para IPv6:
-
-``` sql
-dictGetString('prefix', 'asn', tuple(IPv6StringToNum('2001:db8::1')))
-```
-
-Todavía no se admiten otros tipos. La función devuelve el atributo para el prefijo que corresponde a esta dirección IP. Si hay prefijos superpuestos, se devuelve el más específico.
-
-Los datos se almacenan en un `trie`. Debe encajar completamente en la RAM.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_layout/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md b/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md
deleted file mode 100644
index a1b50e401385..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md
+++ /dev/null
@@ -1,91 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: Actualizaciones del diccionario
----
-
-# Actualizaciones del diccionario {#dictionary-updates}
-
-ClickHouse actualiza periódicamente los diccionarios. El intervalo de actualización para los diccionarios completamente descargados y el intervalo de invalidación para los diccionarios almacenados en caché se `<lifetime>` etiqueta en segundos.
-
-Las actualizaciones del diccionario (aparte de la carga para el primer uso) no bloquean las consultas. Durante las actualizaciones, se utiliza la versión anterior de un diccionario. Si se produce un error durante una actualización, el error se escribe en el registro del servidor y las consultas continúan utilizando la versión anterior de los diccionarios.
-
-Ejemplo de configuración:
-
-``` xml
-<dictionary>
-    ...
-    <lifetime>300</lifetime>
-    ...
-</dictionary>
-```
-
-``` sql
-CREATE DICTIONARY (...)
-...
-LIFETIME(300)
-...
-```
-
-Configuración `<lifetime>0</lifetime>` (`LIFETIME(0)`) impide que los diccionarios se actualicen.
-
-Puede establecer un intervalo de tiempo para las actualizaciones, y ClickHouse elegirá un tiempo uniformemente aleatorio dentro de este rango. Esto es necesario para distribuir la carga en la fuente del diccionario cuando se actualiza en una gran cantidad de servidores.
-
-Ejemplo de configuración:
-
-``` xml
-<dictionary>
-    ...
-    <lifetime>
-        <min>300</min>
-        <max>360</max>
-    </lifetime>
-    ...
-</dictionary>
-```
-
-o
-
-``` sql
-LIFETIME(MIN 300 MAX 360)
-```
-
-Si `<min>0</min>` y `<max>0</max>`, ClickHouse no vuelve a cargar el diccionario por tiempo de espera.
-En este caso, ClickHouse puede volver a cargar el diccionario anteriormente si el archivo de configuración del diccionario `SYSTEM RELOAD DICTIONARY` se ejecutó el comando.
-
-Al actualizar los diccionarios, el servidor ClickHouse aplica una lógica diferente según el tipo de [fuente](external-dicts-dict-sources.md):
-
-Al actualizar los diccionarios, el servidor ClickHouse aplica una lógica diferente según el tipo de [fuente](external-dicts-dict-sources.md):
-
--   Para un archivo de texto, comprueba el tiempo de modificación. Si la hora difiere de la hora previamente grabada, el diccionario se actualiza.
--   Para las tablas MyISAM, el tiempo de modificación se comprueba utilizando un `SHOW TABLE STATUS` consulta.
--   Los diccionarios de otras fuentes se actualizan cada vez de forma predeterminada.
-
-Para fuentes MySQL (InnoDB), ODBC y ClickHouse, puede configurar una consulta que actualizará los diccionarios solo si realmente cambiaron, en lugar de cada vez. Para ello, siga estos pasos:
-
--   La tabla del diccionario debe tener un campo que siempre cambie cuando se actualizan los datos de origen.
--   La configuración del origen debe especificar una consulta que recupere el campo de cambio. El servidor ClickHouse interpreta el resultado de la consulta como una fila, y si esta fila ha cambiado en relación con su estado anterior, el diccionario se actualiza. Especifique la consulta en el `<invalidate_query>` en la configuración de la [fuente](external-dicts-dict-sources.md).
-
-Ejemplo de configuración:
-
-``` xml
-<dictionary>
-    ...
-    <odbc>
-      ...
-      <invalidate_query>SELECT update_time FROM dictionary_source where id = 1</invalidate_query>
-    </odbc>
-    ...
-</dictionary>
-```
-
-o
-
-``` sql
-...
-SOURCE(ODBC(... invalidate_query 'SELECT update_time FROM dictionary_source where id = 1'))
-...
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_lifetime/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md b/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md
deleted file mode 100644
index afeef7b44802..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md
+++ /dev/null
@@ -1,630 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: Fuentes de diccionarios externos
----
-
-# Fuentes de diccionarios externos {#dicts-external-dicts-dict-sources}
-
-Un diccionario externo se puede conectar desde muchas fuentes diferentes.
-
-Si el diccionario se configura usando xml-file, la configuración se ve así:
-
-``` xml
-<yandex>
-  <dictionary>
-    ...
-    <source>
-      <source_type>
-        <!-- Source configuration -->
-      </source_type>
-    </source>
-    ...
-  </dictionary>
-  ...
-</yandex>
-```
-
-En caso de [Consulta DDL](../../statements/create.md#create-dictionary-query), la configuración igual parecerá:
-
-``` sql
-CREATE DICTIONARY dict_name (...)
-...
-SOURCE(SOURCE_TYPE(param1 val1 ... paramN valN)) -- Source configuration
-...
-```
-
-El origen está configurado en el `source` apartado.
-
-Para tipos de origen [Archivo Local](#dicts-external_dicts_dict_sources-local_file), [Archivo ejecutable](#dicts-external_dicts_dict_sources-executable), [HTTP(s))](#dicts-external_dicts_dict_sources-http), [Haga clic en Casa](#dicts-external_dicts_dict_sources-clickhouse)
-ajustes opcionales están disponibles:
-
-``` xml
-<source>
-  <file>
-    <path>/opt/dictionaries/os.tsv</path>
-    <format>TabSeparated</format>
-  </file>
-  <settings>
-      <format_csv_allow_single_quotes>0</format_csv_allow_single_quotes>
-  </settings>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(FILE(path '/opt/dictionaries/os.tsv' format 'TabSeparated'))
-SETTINGS(format_csv_allow_single_quotes = 0)
-```
-
-Tipos de fuentes (`source_type`):
-
--   [Archivo Local](#dicts-external_dicts_dict_sources-local_file)
--   [Archivo ejecutable](#dicts-external_dicts_dict_sources-executable)
--   [HTTP(s))](#dicts-external_dicts_dict_sources-http)
--   DBMS
-    -   [ODBC](#dicts-external_dicts_dict_sources-odbc)
-    -   [MySQL](#dicts-external_dicts_dict_sources-mysql)
-    -   [Haga clic en Casa](#dicts-external_dicts_dict_sources-clickhouse)
-    -   [MongoDB](#dicts-external_dicts_dict_sources-mongodb)
-    -   [Redis](#dicts-external_dicts_dict_sources-redis)
-
-## Archivo Local {#dicts-external_dicts_dict_sources-local_file}
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-  <file>
-    <path>/opt/dictionaries/os.tsv</path>
-    <format>TabSeparated</format>
-  </file>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(FILE(path '/opt/dictionaries/os.tsv' format 'TabSeparated'))
-```
-
-Configuración de campos:
-
--   `path` – The absolute path to the file.
--   `format` – The file format. All the formats described in “[Formato](../../../interfaces/formats.md#formats)” son compatibles.
-
-## Archivo ejecutable {#dicts-external_dicts_dict_sources-executable}
-
-Trabajar con archivos ejecutables depende de [cómo se almacena el diccionario en la memoria](external-dicts-dict-layout.md). Si el diccionario se almacena usando `cache` y `complex_key_cache`, ClickHouse solicita las claves necesarias enviando una solicitud al STDIN del archivo ejecutable. De lo contrario, ClickHouse inicia el archivo ejecutable y trata su salida como datos del diccionario.
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-    <executable>
-        <command>cat /opt/dictionaries/os.tsv</command>
-        <format>TabSeparated</format>
-    </executable>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(EXECUTABLE(command 'cat /opt/dictionaries/os.tsv' format 'TabSeparated'))
-```
-
-Configuración de campos:
-
--   `command` – The absolute path to the executable file, or the file name (if the program directory is written to `PATH`).
--   `format` – The file format. All the formats described in “[Formato](../../../interfaces/formats.md#formats)” son compatibles.
-
-## Http(s) {#dicts-external_dicts_dict_sources-http}
-
-Trabajar con un servidor HTTP depende de [cómo se almacena el diccionario en la memoria](external-dicts-dict-layout.md). Si el diccionario se almacena usando `cache` y `complex_key_cache`, ClickHouse solicita las claves necesarias enviando una solicitud a través del `POST` método.
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-    <http>
-        <url>http://[::1]/os.tsv</url>
-        <format>TabSeparated</format>
-        <credentials>
-            <user>user</user>
-            <password>password</password>
-        </credentials>
-        <headers>
-            <header>
-                <name>API-KEY</name>
-                <value>key</value>
-            </header>
-        </headers>
-    </http>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(HTTP(
-    url 'http://[::1]/os.tsv'
-    format 'TabSeparated'
-    credentials(user 'user' password 'password')
-    headers(header(name 'API-KEY' value 'key'))
-))
-```
-
-Para que ClickHouse tenga acceso a un recurso HTTPS, debe [configurar openSSL](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-openssl) en la configuración del servidor.
-
-Configuración de campos:
-
--   `url` – The source URL.
--   `format` – The file format. All the formats described in “[Formato](../../../interfaces/formats.md#formats)” son compatibles.
--   `credentials` – Basic HTTP authentication. Optional parameter.
-    -   `user` – Username required for the authentication.
-    -   `password` – Password required for the authentication.
--   `headers` – All custom HTTP headers entries used for the HTTP request. Optional parameter.
-    -   `header` – Single HTTP header entry.
-    -   `name` – Identifiant name used for the header send on the request.
-    -   `value` – Value set for a specific identifiant name.
-
-## ODBC {#dicts-external_dicts_dict_sources-odbc}
-
-Puede utilizar este método para conectar cualquier base de datos que tenga un controlador ODBC.
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-    <odbc>
-        <db>DatabaseName</db>
-        <table>ShemaName.TableName</table>
-        <connection_string>DSN=some_parameters</connection_string>
-        <invalidate_query>SQL_QUERY</invalidate_query>
-    </odbc>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(ODBC(
-    db 'DatabaseName'
-    table 'SchemaName.TableName'
-    connection_string 'DSN=some_parameters'
-    invalidate_query 'SQL_QUERY'
-))
-```
-
-Configuración de campos:
-
--   `db` – Name of the database. Omit it if the database name is set in the `<connection_string>` parámetros.
--   `table` – Name of the table and schema if exists.
--   `connection_string` – Connection string.
--   `invalidate_query` – Query for checking the dictionary status. Optional parameter. Read more in the section [Actualización de diccionarios](external-dicts-dict-lifetime.md).
-
-ClickHouse recibe símbolos de cotización del controlador ODBC y cita todas las configuraciones en las consultas al controlador, por lo que es necesario establecer el nombre de la tabla de acuerdo con el caso del nombre de la tabla en la base de datos.
-
-Si tiene problemas con las codificaciones al utilizar Oracle, consulte el [FAQ](../../../faq/general.md#oracle-odbc-encodings) artículo.
-
-### Vulnerabilidad conocida de la funcionalidad del diccionario ODBC {#known-vulnerability-of-the-odbc-dictionary-functionality}
-
-!!! attention "Atención"
-    Cuando se conecta a la base de datos a través del parámetro de conexión del controlador ODBC `Servername` puede ser sustituido. En este caso los valores de `USERNAME` y `PASSWORD` de `odbc.ini` se envían al servidor remoto y pueden verse comprometidos.
-
-**Ejemplo de uso inseguro**
-
-Vamos a configurar unixODBC para PostgreSQL. Contenido de `/etc/odbc.ini`:
-
-``` text
-[gregtest]
-Driver = /usr/lib/psqlodbca.so
-Servername = localhost
-PORT = 5432
-DATABASE = test_db
-#OPTION = 3
-USERNAME = test
-PASSWORD = test
-```
-
-Si luego realiza una consulta como
-
-``` sql
-SELECT * FROM odbc('DSN=gregtest;Servername=some-server.com', 'test_db');
-```
-
-El controlador ODBC enviará valores de `USERNAME` y `PASSWORD` de `odbc.ini` a `some-server.com`.
-
-### Ejemplo de Connecting Postgresql {#example-of-connecting-postgresql}
-
-Sistema operativo Ubuntu.
-
-Instalación de unixODBC y el controlador ODBC para PostgreSQL:
-
-``` bash
-$ sudo apt-get install -y unixodbc odbcinst odbc-postgresql
-```
-
-Configuración `/etc/odbc.ini` (o `~/.odbc.ini`):
-
-``` text
-    [DEFAULT]
-    Driver = myconnection
-
-    [myconnection]
-    Description         = PostgreSQL connection to my_db
-    Driver              = PostgreSQL Unicode
-    Database            = my_db
-    Servername          = 127.0.0.1
-    UserName            = username
-    Password            = password
-    Port                = 5432
-    Protocol            = 9.3
-    ReadOnly            = No
-    RowVersioning       = No
-    ShowSystemTables    = No
-    ConnSettings        =
-```
-
-La configuración del diccionario en ClickHouse:
-
-``` xml
-<yandex>
-    <dictionary>
-        <name>table_name</name>
-        <source>
-            <odbc>
-                <!-- You can specify the following parameters in connection_string: -->
-                <!-- DSN=myconnection;UID=username;PWD=password;HOST=127.0.0.1;PORT=5432;DATABASE=my_db -->
-                <connection_string>DSN=myconnection</connection_string>
-                <table>postgresql_table</table>
-            </odbc>
-        </source>
-        <lifetime>
-            <min>300</min>
-            <max>360</max>
-        </lifetime>
-        <layout>
-            <hashed/>
-        </layout>
-        <structure>
-            <id>
-                <name>id</name>
-            </id>
-            <attribute>
-                <name>some_column</name>
-                <type>UInt64</type>
-                <null_value>0</null_value>
-            </attribute>
-        </structure>
-    </dictionary>
-</yandex>
-```
-
-o
-
-``` sql
-CREATE DICTIONARY table_name (
-    id UInt64,
-    some_column UInt64 DEFAULT 0
-)
-PRIMARY KEY id
-SOURCE(ODBC(connection_string 'DSN=myconnection' table 'postgresql_table'))
-LAYOUT(HASHED())
-LIFETIME(MIN 300 MAX 360)
-```
-
-Es posible que tenga que editar `odbc.ini` para especificar la ruta completa a la biblioteca con el controlador `DRIVER=/usr/local/lib/psqlodbcw.so`.
-
-### Ejemplo de conexión de MS SQL Server {#example-of-connecting-ms-sql-server}
-
-Sistema operativo Ubuntu.
-
-Instalación del controlador: :
-
-``` bash
-$ sudo apt-get install tdsodbc freetds-bin sqsh
-```
-
-Configuración del controlador:
-
-``` bash
-    $ cat /etc/freetds/freetds.conf
-    ...
-
-    [MSSQL]
-    host = 192.168.56.101
-    port = 1433
-    tds version = 7.0
-    client charset = UTF-8
-
-    $ cat /etc/odbcinst.ini
-    ...
-
-    [FreeTDS]
-    Description     = FreeTDS
-    Driver          = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so
-    Setup           = /usr/lib/x86_64-linux-gnu/odbc/libtdsS.so
-    FileUsage       = 1
-    UsageCount      = 5
-
-    $ cat ~/.odbc.ini
-    ...
-
-    [MSSQL]
-    Description     = FreeTDS
-    Driver          = FreeTDS
-    Servername      = MSSQL
-    Database        = test
-    UID             = test
-    PWD             = test
-    Port            = 1433
-```
-
-Configuración del diccionario en ClickHouse:
-
-``` xml
-<yandex>
-    <dictionary>
-        <name>test</name>
-        <source>
-            <odbc>
-                <table>dict</table>
-                <connection_string>DSN=MSSQL;UID=test;PWD=test</connection_string>
-            </odbc>
-        </source>
-
-        <lifetime>
-            <min>300</min>
-            <max>360</max>
-        </lifetime>
-
-        <layout>
-            <flat />
-        </layout>
-
-        <structure>
-            <id>
-                <name>k</name>
-            </id>
-            <attribute>
-                <name>s</name>
-                <type>String</type>
-                <null_value></null_value>
-            </attribute>
-        </structure>
-    </dictionary>
-</yandex>
-```
-
-o
-
-``` sql
-CREATE DICTIONARY test (
-    k UInt64,
-    s String DEFAULT ''
-)
-PRIMARY KEY k
-SOURCE(ODBC(table 'dict' connection_string 'DSN=MSSQL;UID=test;PWD=test'))
-LAYOUT(FLAT())
-LIFETIME(MIN 300 MAX 360)
-```
-
-## DBMS {#dbms}
-
-### Mysql {#dicts-external_dicts_dict_sources-mysql}
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-  <mysql>
-      <port>3306</port>
-      <user>clickhouse</user>
-      <password>qwerty</password>
-      <replica>
-          <host>example01-1</host>
-          <priority>1</priority>
-      </replica>
-      <replica>
-          <host>example01-2</host>
-          <priority>1</priority>
-      </replica>
-      <db>db_name</db>
-      <table>table_name</table>
-      <where>id=10</where>
-      <invalidate_query>SQL_QUERY</invalidate_query>
-  </mysql>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(MYSQL(
-    port 3306
-    user 'clickhouse'
-    password 'qwerty'
-    replica(host 'example01-1' priority 1)
-    replica(host 'example01-2' priority 1)
-    db 'db_name'
-    table 'table_name'
-    where 'id=10'
-    invalidate_query 'SQL_QUERY'
-))
-```
-
-Configuración de campos:
-
--   `port` – The port on the MySQL server. You can specify it for all replicas, or for each one individually (inside `<replica>`).
-
--   `user` – Name of the MySQL user. You can specify it for all replicas, or for each one individually (inside `<replica>`).
-
--   `password` – Password of the MySQL user. You can specify it for all replicas, or for each one individually (inside `<replica>`).
-
--   `replica` – Section of replica configurations. There can be multiple sections.
-
-        - `replica/host` – The MySQL host.
-        - `replica/priority` – The replica priority. When attempting to connect, ClickHouse traverses the replicas in order of priority. The lower the number, the higher the priority.
-
--   `db` – Name of the database.
-
--   `table` – Name of the table.
-
--   `where` – The selection criteria. The syntax for conditions is the same as for `WHERE` cláusula en MySQL, por ejemplo, `id > 10 AND id < 20`. Parámetro opcional.
-
--   `invalidate_query` – Query for checking the dictionary status. Optional parameter. Read more in the section [Actualización de diccionarios](external-dicts-dict-lifetime.md).
-
-MySQL se puede conectar en un host local a través de sockets. Para hacer esto, establezca `host` y `socket`.
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-  <mysql>
-      <host>localhost</host>
-      <socket>/path/to/socket/file.sock</socket>
-      <user>clickhouse</user>
-      <password>qwerty</password>
-      <db>db_name</db>
-      <table>table_name</table>
-      <where>id=10</where>
-      <invalidate_query>SQL_QUERY</invalidate_query>
-  </mysql>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(MYSQL(
-    host 'localhost'
-    socket '/path/to/socket/file.sock'
-    user 'clickhouse'
-    password 'qwerty'
-    db 'db_name'
-    table 'table_name'
-    where 'id=10'
-    invalidate_query 'SQL_QUERY'
-))
-```
-
-### Haga clic en Casa {#dicts-external_dicts_dict_sources-clickhouse}
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-    <clickhouse>
-        <host>example01-01-1</host>
-        <port>9000</port>
-        <user>default</user>
-        <password></password>
-        <db>default</db>
-        <table>ids</table>
-        <where>id=10</where>
-    </clickhouse>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(CLICKHOUSE(
-    host 'example01-01-1'
-    port 9000
-    user 'default'
-    password ''
-    db 'default'
-    table 'ids'
-    where 'id=10'
-))
-```
-
-Configuración de campos:
-
--   `host` – The ClickHouse host. If it is a local host, the query is processed without any network activity. To improve fault tolerance, you can create a [Distribuido](../../../engines/table-engines/special/distributed.md) tabla e ingrésela en configuraciones posteriores.
--   `port` – The port on the ClickHouse server.
--   `user` – Name of the ClickHouse user.
--   `password` – Password of the ClickHouse user.
--   `db` – Name of the database.
--   `table` – Name of the table.
--   `where` – The selection criteria. May be omitted.
--   `invalidate_query` – Query for checking the dictionary status. Optional parameter. Read more in the section [Actualización de diccionarios](external-dicts-dict-lifetime.md).
-
-### Mongodb {#dicts-external_dicts_dict_sources-mongodb}
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-    <mongodb>
-        <host>localhost</host>
-        <port>27017</port>
-        <user></user>
-        <password></password>
-        <db>test</db>
-        <collection>dictionary_source</collection>
-    </mongodb>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(MONGO(
-    host 'localhost'
-    port 27017
-    user ''
-    password ''
-    db 'test'
-    collection 'dictionary_source'
-))
-```
-
-Configuración de campos:
-
--   `host` – The MongoDB host.
--   `port` – The port on the MongoDB server.
--   `user` – Name of the MongoDB user.
--   `password` – Password of the MongoDB user.
--   `db` – Name of the database.
--   `collection` – Name of the collection.
-
-### Redis {#dicts-external_dicts_dict_sources-redis}
-
-Ejemplo de configuración:
-
-``` xml
-<source>
-    <redis>
-        <host>localhost</host>
-        <port>6379</port>
-        <storage_type>simple</storage_type>
-        <db_index>0</db_index>
-    </redis>
-</source>
-```
-
-o
-
-``` sql
-SOURCE(REDIS(
-    host 'localhost'
-    port 6379
-    storage_type 'simple'
-    db_index 0
-))
-```
-
-Configuración de campos:
-
--   `host` – The Redis host.
--   `port` – The port on the Redis server.
--   `storage_type` – The structure of internal Redis storage using for work with keys. `simple` es para fuentes simples y para fuentes de clave única hash, `hash_map` es para fuentes hash con dos teclas. Los orígenes a distancia y los orígenes de caché con clave compleja no son compatibles. Puede omitirse, el valor predeterminado es `simple`.
--   `db_index` – The specific numeric index of Redis logical database. May be omitted, default value is 0.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_sources/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md b/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md
deleted file mode 100644
index 74e8ccde0746..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md
+++ /dev/null
@@ -1,175 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: Clave y campos del diccionario
----
-
-# Clave y campos del diccionario {#dictionary-key-and-fields}
-
-El `<structure>` cláusula describe la clave del diccionario y los campos disponibles para consultas.
-
-Descripción XML:
-
-``` xml
-<dictionary>
-    <structure>
-        <id>
-            <name>Id</name>
-        </id>
-
-        <attribute>
-            <!-- Attribute parameters -->
-        </attribute>
-
-        ...
-
-    </structure>
-</dictionary>
-```
-
-Los atributos se describen en los elementos:
-
--   `<id>` — [Columna clave](external-dicts-dict-structure.md#ext_dict_structure-key).
--   `<attribute>` — [Columna de datos](external-dicts-dict-structure.md#ext_dict_structure-attributes). Puede haber un número múltiple de atributos.
-
-Consulta DDL:
-
-``` sql
-CREATE DICTIONARY dict_name (
-    Id UInt64,
-    -- attributes
-)
-PRIMARY KEY Id
-...
-```
-
-Los atributos se describen en el cuerpo de la consulta:
-
--   `PRIMARY KEY` — [Columna clave](external-dicts-dict-structure.md#ext_dict_structure-key)
--   `AttrName AttrType` — [Columna de datos](external-dicts-dict-structure.md#ext_dict_structure-attributes). Puede haber un número múltiple de atributos.
-
-## Clave {#ext_dict_structure-key}
-
-ClickHouse admite los siguientes tipos de claves:
-
--   Tecla numérica. `UInt64`. Definido en el `<id>` etiqueta o usando `PRIMARY KEY` palabra clave.
--   Clave compuesta. Conjunto de valores de diferentes tipos. Definido en la etiqueta `<key>` o `PRIMARY KEY` palabra clave.
-
-Una estructura xml puede contener `<id>` o `<key>`. La consulta DDL debe contener `PRIMARY KEY`.
-
-!!! warning "Advertencia"
-    No debe describir la clave como un atributo.
-
-### Tecla numérica {#ext_dict-numeric-key}
-
-Tipo: `UInt64`.
-
-Ejemplo de configuración:
-
-``` xml
-<id>
-    <name>Id</name>
-</id>
-```
-
-Campos de configuración:
-
--   `name` – The name of the column with keys.
-
-Para consulta DDL:
-
-``` sql
-CREATE DICTIONARY (
-    Id UInt64,
-    ...
-)
-PRIMARY KEY Id
-...
-```
-
--   `PRIMARY KEY` – The name of the column with keys.
-
-### Clave compuesta {#composite-key}
-
-La clave puede ser un `tuple` de cualquier tipo de campo. El [diseño](external-dicts-dict-layout.md) en este caso debe ser `complex_key_hashed` o `complex_key_cache`.
-
-!!! tip "Consejo"
-    Una clave compuesta puede consistir en un solo elemento. Esto hace posible usar una cadena como clave, por ejemplo.
-
-La estructura clave se establece en el elemento `<key>`. Los campos clave se especifican en el mismo formato que el diccionario [atributo](external-dicts-dict-structure.md). Ejemplo:
-
-``` xml
-<structure>
-    <key>
-        <attribute>
-            <name>field1</name>
-            <type>String</type>
-        </attribute>
-        <attribute>
-            <name>field2</name>
-            <type>UInt32</type>
-        </attribute>
-        ...
-    </key>
-...
-```
-
-o
-
-``` sql
-CREATE DICTIONARY (
-    field1 String,
-    field2 String
-    ...
-)
-PRIMARY KEY field1, field2
-...
-```
-
-Para una consulta al `dictGet*` función, una tupla se pasa como la clave. Ejemplo: `dictGetString('dict_name', 'attr_name', tuple('string for field1', num_for_field2))`.
-
-## Atributo {#ext_dict_structure-attributes}
-
-Ejemplo de configuración:
-
-``` xml
-<structure>
-    ...
-    <attribute>
-        <name>Name</name>
-        <type>ClickHouseDataType</type>
-        <null_value></null_value>
-        <expression>rand64()</expression>
-        <hierarchical>true</hierarchical>
-        <injective>true</injective>
-        <is_object_id>true</is_object_id>
-    </attribute>
-</structure>
-```
-
-o
-
-``` sql
-CREATE DICTIONARY somename (
-    Name ClickHouseDataType DEFAULT '' EXPRESSION rand64() HIERARCHICAL INJECTIVE IS_OBJECT_ID
-)
-```
-
-Campos de configuración:
-
-| Etiqueta                                             | Descripci                                                                                                                                                                                                                                                                                                                                                                                 | Requerir |
-|------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|
-| `name`                                               | Nombre de columna.                                                                                                                                                                                                                                                                                                                                                                        | Sí       |
-| `type`                                               | Tipo de datos ClickHouse.<br/>ClickHouse intenta convertir el valor del diccionario al tipo de datos especificado. Por ejemplo, para MySQL, el campo podría ser `TEXT`, `VARCHAR`, o `BLOB` en la tabla fuente de MySQL, pero se puede cargar como `String` en ClickHouse.<br/>[NULL](../../../sql-reference/data-types/nullable.md) no es compatible.                                    | Sí       |
-| `null_value`                                         | Valor predeterminado para un elemento no existente.<br/>En el ejemplo, es una cadena vacía. No se puede utilizar `NULL` en este campo.                                                                                                                                                                                                                                                    | Sí       |
-| `expression`                                         | [Expresion](../../syntax.md#syntax-expressions) que ClickHouse ejecuta en el valor.<br/>La expresión puede ser un nombre de columna en la base de datos SQL remota. Por lo tanto, puede usarlo para crear un alias para la columna remota.<br/><br/>Valor predeterminado: sin expresión.                                                                                                  | No       |
-| <a name="hierarchical-dict-attr"></a> `hierarchical` | Si `true` el atributo contiene el valor de una clave primaria para la clave actual. Ver [Diccionarios jerárquicos](external-dicts-dict-hierarchical.md).<br/><br/>Valor predeterminado: `false`.                                                                                                                                                                                          | No       |
-| `injective`                                          | Indicador que muestra si el `id -> attribute` la imagen es [inyectivo](https://en.wikipedia.org/wiki/Injective_function).<br/>Si `true`, ClickHouse puede colocar automáticamente después de la `GROUP BY` cláusula las solicitudes a los diccionarios con inyección. Por lo general, reduce significativamente la cantidad de tales solicitudes.<br/><br/>Valor predeterminado: `false`. | No       |
-| `is_object_id`                                       | Indicador que muestra si la consulta se ejecuta para un documento MongoDB mediante `ObjectID`.<br/><br/>Valor predeterminado: `false`.                                                                                                                                                                                                                                                    | No       |
-
-## Ver también {#see-also}
-
--   [Funciones para trabajar con diccionarios externos](../../../sql-reference/functions/ext-dict-functions.md).
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_structure/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md b/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md
deleted file mode 100644
index 87026003388a..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md
+++ /dev/null
@@ -1,53 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: "Configuraci\xF3n de un diccionario externo"
----
-
-# Configuración de un diccionario externo {#dicts-external-dicts-dict}
-
-Si el diccionario se configura usando un archivo xml, la configuración del diccionario tiene la siguiente estructura:
-
-``` xml
-<dictionary>
-    <name>dict_name</name>
-
-    <structure>
-      <!-- Complex key configuration -->
-    </structure>
-
-    <source>
-      <!-- Source configuration -->
-    </source>
-
-    <layout>
-      <!-- Memory layout configuration -->
-    </layout>
-
-    <lifetime>
-      <!-- Lifetime of dictionary in memory -->
-    </lifetime>
-</dictionary>
-```
-
-Correspondiente [Consulta DDL](../../statements/create.md#create-dictionary-query) tiene la siguiente estructura:
-
-``` sql
-CREATE DICTIONARY dict_name
-(
-    ... -- attributes
-)
-PRIMARY KEY ... -- complex or single key configuration
-SOURCE(...) -- Source configuration
-LAYOUT(...) -- Memory layout configuration
-LIFETIME(...) -- Lifetime of dictionary in memory
-```
-
--   `name` – The identifier that can be used to access the dictionary. Use the characters `[a-zA-Z0-9_\-]`.
--   [fuente](external-dicts-dict-sources.md) — Source of the dictionary.
--   [diseño](external-dicts-dict-layout.md) — Dictionary layout in memory.
--   [estructura](external-dicts-dict-structure.md) — Structure of the dictionary . A key and attributes that can be retrieved by this key.
--   [vida](external-dicts-dict-lifetime.md) — Frequency of dictionary updates.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts.md b/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts.md
deleted file mode 100644
index ecb4fa9fc631..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/external-dicts.md
+++ /dev/null
@@ -1,62 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: "Descripci\xF3n General"
----
-
-# Diccionarios externos {#dicts-external-dicts}
-
-Puede agregar sus propios diccionarios de varias fuentes de datos. El origen de datos de un diccionario puede ser un archivo ejecutable o de texto local, un recurso HTTP u otro DBMS. Para obtener más información, consulte “[Fuentes para diccionarios externos](external-dicts-dict-sources.md)”.
-
-Haga clic en Casa:
-
--   Almacena total o parcialmente los diccionarios en RAM.
--   Actualiza periódicamente los diccionarios y carga dinámicamente los valores que faltan. En otras palabras, los diccionarios se pueden cargar dinámicamente.
--   Permite crear diccionarios externos con archivos xml o [Consultas DDL](../../statements/create.md#create-dictionary-query).
-
-La configuración de diccionarios externos se puede ubicar en uno o más archivos xml. La ruta de acceso a la configuración se especifica en el [Diccionarios_config](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-dictionaries_config) parámetro.
-
-Los diccionarios se pueden cargar en el inicio del servidor o en el primer uso, dependiendo de la [Diccionarios_lazy_load](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-dictionaries_lazy_load) configuración.
-
-El [diccionario](../../../operations/system-tables.md#system_tables-dictionaries) La tabla del sistema contiene información sobre los diccionarios configurados en el servidor. Para cada diccionario se puede encontrar allí:
-
--   Estado del diccionario.
--   Parámetros de configuración.
--   Métricas como la cantidad de RAM asignada para el diccionario o un número de consultas desde que el diccionario se cargó correctamente.
-
-El archivo de configuración del diccionario tiene el siguiente formato:
-
-``` xml
-<yandex>
-    <comment>An optional element with any content. Ignored by the ClickHouse server.</comment>
-
-    <!--Optional element. File name with substitutions-->
-    <include_from>/etc/metrika.xml</include_from>
-
-
-    <dictionary>
-        <!-- Dictionary configuration. -->
-        <!-- There can be any number of <dictionary> sections in the configuration file. -->
-    </dictionary>
-
-</yandex>
-```
-
-Usted puede [configurar](external-dicts-dict.md) cualquier número de diccionarios en el mismo archivo.
-
-[Consultas DDL para diccionarios](../../statements/create.md#create-dictionary-query) no requiere ningún registro adicional en la configuración del servidor. Permiten trabajar con diccionarios como entidades de primera clase, como tablas o vistas.
-
-!!! attention "Atención"
-    Puede convertir valores para un diccionario pequeño describiéndolo en un `SELECT` consulta (ver el [transformar](../../../sql-reference/functions/other-functions.md) función). Esta funcionalidad no está relacionada con diccionarios externos.
-
-## Ver también {#ext-dicts-see-also}
-
--   [Configuración de un diccionario externo](external-dicts-dict.md)
--   [Almacenamiento de diccionarios en la memoria](external-dicts-dict-layout.md)
--   [Actualizaciones del diccionario](external-dicts-dict-lifetime.md)
--   [Fuentes de diccionarios externos](external-dicts-dict-sources.md)
--   [Clave y campos del diccionario](external-dicts-dict-structure.md)
--   [Funciones para trabajar con diccionarios externos](../../../sql-reference/functions/ext-dict-functions.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/external-dictionaries/index.md b/docs/es/sql-reference/dictionaries/external-dictionaries/index.md
deleted file mode 100644
index 290b54cb7b86..000000000000
--- a/docs/es/sql-reference/dictionaries/external-dictionaries/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Diccionarios externos
-toc_priority: 37
----
-
-
diff --git a/docs/es/sql-reference/dictionaries/index.md b/docs/es/sql-reference/dictionaries/index.md
deleted file mode 100644
index 21219fb3688f..000000000000
--- a/docs/es/sql-reference/dictionaries/index.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Diccionario
-toc_priority: 35
-toc_title: "Implantaci\xF3n"
----
-
-# Diccionario {#dictionaries}
-
-Un diccionario es un mapeo (`key -> attributes`) que es conveniente para varios tipos de listas de referencia.
-
-ClickHouse admite funciones especiales para trabajar con diccionarios que se pueden usar en consultas. Es más fácil y más eficiente usar diccionarios con funciones que un `JOIN` con tablas de referencia.
-
-[NULL](../../sql-reference/syntax.md#null-literal) los valores no se pueden almacenar en un diccionario.
-
-Soporta ClickHouse:
-
--   [Diccionarios incorporados](internal-dicts.md#internal_dicts) con una específica [conjunto de funciones](../../sql-reference/functions/ym-dict-functions.md).
--   [Diccionarios complementarios (externos)](external-dictionaries/external-dicts.md#dicts-external-dicts) con un [conjunto de funciones](../../sql-reference/functions/ext-dict-functions.md).
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/) <!--hide-->
diff --git a/docs/es/sql-reference/dictionaries/internal-dicts.md b/docs/es/sql-reference/dictionaries/internal-dicts.md
deleted file mode 100644
index 9f5217e6fc3b..000000000000
--- a/docs/es/sql-reference/dictionaries/internal-dicts.md
+++ /dev/null
@@ -1,55 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: Diccionarios internos
----
-
-# Diccionarios internos {#internal_dicts}
-
-ClickHouse contiene una función integrada para trabajar con una geobase.
-
-Esto le permite:
-
--   Utilice el ID de una región para obtener su nombre en el idioma deseado.
--   Utilice el ID de una región para obtener el ID de una ciudad, área, distrito federal, país o continente.
--   Compruebe si una región es parte de otra región.
--   Obtener una cadena de regiones principales.
-
-Todas las funciones de apoyo “translocality,” la capacidad de utilizar simultáneamente diferentes perspectivas sobre la propiedad de la región. Para obtener más información, consulte la sección “Functions for working with Yandex.Metrica dictionaries”.
-
-Los diccionarios internos están deshabilitados en el paquete predeterminado.
-Para habilitarlos, descomente los parámetros `path_to_regions_hierarchy_file` y `path_to_regions_names_files` en el archivo de configuración del servidor.
-
-La geobase se carga desde archivos de texto.
-
-Coloque el `regions_hierarchy*.txt` archivos en el `path_to_regions_hierarchy_file` directorio. Este parámetro de configuración debe contener la ruta `regions_hierarchy.txt` archivo (la jerarquía regional predeterminada), y los otros archivos (`regions_hierarchy_ua.txt`) debe estar ubicado en el mismo directorio.
-
-Ponga el `regions_names_*.txt` archivos en el `path_to_regions_names_files` directorio.
-
-También puede crear estos archivos usted mismo. El formato de archivo es el siguiente:
-
-`regions_hierarchy*.txt`: TabSeparated (sin encabezado), columnas:
-
--   ID de la región (`UInt32`)
--   ID de región padre (`UInt32`)
--   tipo de región (`UInt8`): 1 - continente, 3 - país, 4 - distrito federal, 5 - región, 6 - ciudad; otros tipos no tienen valores
--   población (`UInt32`) — optional column
-
-`regions_names_*.txt`: TabSeparated (sin encabezado), columnas:
-
--   ID de la región (`UInt32`)
--   nombre de la región (`String`) — Can't contain tabs or line feeds, even escaped ones.
-
-Una matriz plana se usa para almacenar en RAM. Por esta razón, los ID no deberían ser más de un millón.
-
-Los diccionarios se pueden actualizar sin reiniciar el servidor. Sin embargo, el conjunto de diccionarios disponibles no se actualiza.
-Para las actualizaciones, se comprueban los tiempos de modificación de archivos. Si un archivo ha cambiado, el diccionario se actualiza.
-El intervalo para comprobar si hay cambios se configura en el `builtin_dictionaries_reload_interval` parámetro.
-Las actualizaciones del diccionario (aparte de la carga al primer uso) no bloquean las consultas. Durante las actualizaciones, las consultas utilizan las versiones anteriores de los diccionarios. Si se produce un error durante una actualización, el error se escribe en el registro del servidor y las consultas continúan utilizando la versión anterior de los diccionarios.
-
-Recomendamos actualizar periódicamente los diccionarios con la geobase. Durante una actualización, genere nuevos archivos y escríbalos en una ubicación separada. Cuando todo esté listo, cambie el nombre a los archivos utilizados por el servidor.
-
-También hay funciones para trabajar con identificadores de sistema operativo y Yandex.Motores de búsqueda Metrica, pero no deben ser utilizados.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/dicts/internal_dicts/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/arithmetic-functions.md b/docs/es/sql-reference/functions/arithmetic-functions.md
deleted file mode 100644
index 02bd37a81691..000000000000
--- a/docs/es/sql-reference/functions/arithmetic-functions.md
+++ /dev/null
@@ -1,87 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: "Aritm\xE9tica"
----
-
-# Funciones aritméticas {#arithmetic-functions}
-
-Para todas las funciones aritméticas, el tipo de resultado se calcula como el tipo de número más pequeño en el que encaja el resultado, si existe dicho tipo. El mínimo se toma simultáneamente en función del número de bits, si está firmado y si flota. Si no hay suficientes bits, se toma el tipo de bit más alto.
-
-Ejemplo:
-
-``` sql
-SELECT toTypeName(0), toTypeName(0 + 0), toTypeName(0 + 0 + 0), toTypeName(0 + 0 + 0 + 0)
-```
-
-``` text
-┌─toTypeName(0)─┬─toTypeName(plus(0, 0))─┬─toTypeName(plus(plus(0, 0), 0))─┬─toTypeName(plus(plus(plus(0, 0), 0), 0))─┐
-│ UInt8         │ UInt16                 │ UInt32                          │ UInt64                                   │
-└───────────────┴────────────────────────┴─────────────────────────────────┴──────────────────────────────────────────┘
-```
-
-Las funciones aritméticas funcionan para cualquier par de tipos de UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64, Float32 o Float64.
-
-El desbordamiento se produce de la misma manera que en C ++.
-
-## más (a, b), a + b operador {#plusa-b-a-b-operator}
-
-Calcula la suma de los números.
-También puede agregar números enteros con una fecha o fecha y hora. En el caso de una fecha, agregar un entero significa agregar el número correspondiente de días. Para una fecha con hora, significa agregar el número correspondiente de segundos.
-
-## menos(a, b), a - b operador {#minusa-b-a-b-operator}
-
-Calcula la diferencia. El resultado siempre está firmado.
-
-You can also calculate integer numbers from a date or date with time. The idea is the same – see above for ‘plus’.
-
-## multiplicar(a, b) a \* b operador {#multiplya-b-a-b-operator}
-
-Calcula el producto de los números.
-
-## divide (a, b), operador a / b {#dividea-b-a-b-operator}
-
-Calcula el cociente de los números. El tipo de resultado es siempre un tipo de punto flotante.
-No es una división entera. Para la división de enteros, use el ‘intDiv’ función.
-Al dividir por cero obtienes ‘inf’, ‘-inf’, o ‘nan’.
-
-## Información de uso) {#intdiva-b}
-
-Calcula el cociente de los números. Se divide en enteros, redondeando hacia abajo (por el valor absoluto).
-Se produce una excepción al dividir por cero o al dividir un número negativo mínimo por menos uno.
-
-## IntDivOrZero (a, b) {#intdivorzeroa-b}
-
-Difiere de ‘intDiv’ en que devuelve cero al dividir por cero o al dividir un número negativo mínimo por menos uno.
-
-## modulo(a, b), a % b operador {#moduloa-b-a-b-operator}
-
-Calcula el resto después de la división.
-Si los argumentos son números de coma flotante, se convierten previamente en enteros eliminando la parte decimal.
-El resto se toma en el mismo sentido que en C ++. La división truncada se usa para números negativos.
-Se produce una excepción al dividir por cero o al dividir un número negativo mínimo por menos uno.
-
-## moduloOrZero (a, b) {#moduloorzeroa-b}
-
-Difiere de ‘modulo’ en que devuelve cero cuando el divisor es cero.
-
-## negate(a), -un operador {#negatea-a-operator}
-
-Calcula un número con el signo inverso. El resultado siempre está firmado.
-
-## abs (a) {#arithm_func-abs}
-
-Calcula el valor absoluto del número (a). Es decir, si un \<0, devuelve -a . Para los tipos sin firmar no hace nada. Para los tipos de enteros con signo, devuelve un número sin signo.
-
-## GCD (a, b) {#gcda-b}
-
-Devuelve el mayor divisor común de los números.
-Se produce una excepción al dividir por cero o al dividir un número negativo mínimo por menos uno.
-
-## Lcm(a, b) {#lcma-b}
-
-Devuelve el mínimo múltiplo común de los números.
-Se produce una excepción al dividir por cero o al dividir un número negativo mínimo por menos uno.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/arithmetic_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/array-functions.md b/docs/es/sql-reference/functions/array-functions.md
deleted file mode 100644
index 460e4775b44f..000000000000
--- a/docs/es/sql-reference/functions/array-functions.md
+++ /dev/null
@@ -1,1061 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: Trabajar con matrices
----
-
-# Funciones para trabajar con matrices {#functions-for-working-with-arrays}
-
-## vaciar {#function-empty}
-
-Devuelve 1 para una matriz vacía, o 0 para una matriz no vacía.
-El tipo de resultado es UInt8.
-La función también funciona para cadenas.
-
-## notEmpty {#function-notempty}
-
-Devuelve 0 para una matriz vacía, o 1 para una matriz no vacía.
-El tipo de resultado es UInt8.
-La función también funciona para cadenas.
-
-## longitud {#array_functions-length}
-
-Devuelve el número de elementos de la matriz.
-El tipo de resultado es UInt64.
-La función también funciona para cadenas.
-
-## Para obtener más información, consulta nuestra Política de privacidad y nuestras Condiciones de uso {#emptyarrayuint8-emptyarrayuint16-emptyarrayuint32-emptyarrayuint64}
-
-## Para obtener más información, consulta nuestra Política de privacidad y nuestras Condiciones de uso {#emptyarrayint8-emptyarrayint16-emptyarrayint32-emptyarrayint64}
-
-## Para obtener más información, consulta nuestra Política de privacidad y nuestras Condiciones de uso {#emptyarrayfloat32-emptyarrayfloat64}
-
-## emptyArrayDate, emptyArrayDateTime {#emptyarraydate-emptyarraydatetime}
-
-## emptyArrayString {#emptyarraystring}
-
-Acepta cero argumentos y devuelve una matriz vacía del tipo apropiado.
-
-## emptyArrayToSingle {#emptyarraytosingle}
-
-Acepta una matriz vacía y devuelve una matriz de un elemento que es igual al valor predeterminado.
-
-## rango(final), rango(inicio, fin \[, paso\]) {#rangeend-rangestart-end-step}
-
-Devuelve una matriz de números de principio a fin-1 por paso.
-Si el argumento `start` no se especifica, el valor predeterminado es 0.
-Si el argumento `step` no se especifica, el valor predeterminado es 1.
-Se comporta casi como pitónico `range`. Pero la diferencia es que todos los tipos de argumentos deben ser `UInt` numero.
-Por si acaso, se produce una excepción si se crean matrices con una longitud total de más de 100,000,000 de elementos en un bloque de datos.
-
-## array(x1, …), operator \[x1, …\] {#arrayx1-operator-x1}
-
-Crea una matriz a partir de los argumentos de la función.
-Los argumentos deben ser constantes y tener tipos que tengan el tipo común más pequeño. Se debe pasar al menos un argumento, porque de lo contrario no está claro qué tipo de matriz crear. Es decir, no puede usar esta función para crear una matriz vacía (para hacerlo, use el ‘emptyArray\*’ función descrita anteriormente).
-Devuelve un ‘Array(T)’ tipo resultado, donde ‘T’ es el tipo común más pequeño de los argumentos pasados.
-
-## arrayConcat {#arrayconcat}
-
-Combina matrices pasadas como argumentos.
-
-``` sql
-arrayConcat(arrays)
-```
-
-**Parámetros**
-
--   `arrays` – Arbitrary number of arguments of [Matriz](../../sql-reference/data-types/array.md) tipo.
-    **Ejemplo**
-
-<!-- -->
-
-``` sql
-SELECT arrayConcat([1, 2], [3, 4], [5, 6]) AS res
-```
-
-``` text
-┌─res───────────┐
-│ [1,2,3,4,5,6] │
-└───────────────┘
-```
-
-## Por ejemplo, el operador arr\[n\] {#arrayelementarr-n-operator-arrn}
-
-Obtener el elemento con el índice `n` de la matriz `arr`. `n` debe ser de cualquier tipo entero.
-Los índices de una matriz comienzan desde uno.
-Los índices negativos son compatibles. En este caso, selecciona el elemento correspondiente numerado desde el final. Por ejemplo, `arr[-1]` es el último elemento de la matriz.
-
-Si el índice cae fuera de los límites de una matriz, devuelve algún valor predeterminado (0 para números, una cadena vacía para cadenas, etc.), a excepción del caso con una matriz no constante y un índice constante 0 (en este caso habrá un error `Array indices are 1-based`).
-
-## Tiene(arr, elem) {#hasarr-elem}
-
-Comprueba si el ‘arr’ la matriz tiene el ‘elem’ elemento.
-Devuelve 0 si el elemento no está en la matriz, o 1 si es.
-
-`NULL` se procesa como un valor.
-
-``` sql
-SELECT has([1, 2, NULL], NULL)
-```
-
-``` text
-┌─has([1, 2, NULL], NULL)─┐
-│                       1 │
-└─────────────────────────┘
-```
-
-## TieneTodo {#hasall}
-
-Comprueba si una matriz es un subconjunto de otra.
-
-``` sql
-hasAll(set, subset)
-```
-
-**Parámetros**
-
--   `set` – Array of any type with a set of elements.
--   `subset` – Array of any type with elements that should be tested to be a subset of `set`.
-
-**Valores de retorno**
-
--   `1`, si `set` contiene todos los elementos de `subset`.
--   `0`, de lo contrario.
-
-**Propiedades peculiares**
-
--   Una matriz vacía es un subconjunto de cualquier matriz.
--   `Null` procesado como un valor.
--   El orden de los valores en ambas matrices no importa.
-
-**Ejemplos**
-
-`SELECT hasAll([], [])` devoluciones 1.
-
-`SELECT hasAll([1, Null], [Null])` devoluciones 1.
-
-`SELECT hasAll([1.0, 2, 3, 4], [1, 3])` devoluciones 1.
-
-`SELECT hasAll(['a', 'b'], ['a'])` devoluciones 1.
-
-`SELECT hasAll([1], ['a'])` devuelve 0.
-
-`SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [3, 5]])` devuelve 0.
-
-## TieneCualquier {#hasany}
-
-Comprueba si dos matrices tienen intersección por algunos elementos.
-
-``` sql
-hasAny(array1, array2)
-```
-
-**Parámetros**
-
--   `array1` – Array of any type with a set of elements.
--   `array2` – Array of any type with a set of elements.
-
-**Valores de retorno**
-
--   `1`, si `array1` y `array2` tienen un elemento similar al menos.
--   `0`, de lo contrario.
-
-**Propiedades peculiares**
-
--   `Null` procesado como un valor.
--   El orden de los valores en ambas matrices no importa.
-
-**Ejemplos**
-
-`SELECT hasAny([1], [])` devoluciones `0`.
-
-`SELECT hasAny([Null], [Null, 1])` devoluciones `1`.
-
-`SELECT hasAny([-128, 1., 512], [1])` devoluciones `1`.
-
-`SELECT hasAny([[1, 2], [3, 4]], ['a', 'c'])` devoluciones `0`.
-
-`SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [1, 2]])` devoluciones `1`.
-
-## ¿Cómo puedo hacerlo?) {#indexofarr-x}
-
-Devuelve el índice de la primera ‘x’ elemento (comenzando desde 1) si está en la matriz, o 0 si no lo está.
-
-Ejemplo:
-
-``` sql
-SELECT indexOf([1, 3, NULL, NULL], NULL)
-```
-
-``` text
-┌─indexOf([1, 3, NULL, NULL], NULL)─┐
-│                                 3 │
-└───────────────────────────────────┘
-```
-
-Elementos establecidos en `NULL` se manejan como valores normales.
-
-## Cuenta igual (arr, x) {#countequalarr-x}
-
-Devuelve el número de elementos de la matriz igual a x. Equivalente a arrayCount (elem -\> elem = x, arr).
-
-`NULL` los elementos se manejan como valores separados.
-
-Ejemplo:
-
-``` sql
-SELECT countEqual([1, 2, NULL, NULL], NULL)
-```
-
-``` text
-┌─countEqual([1, 2, NULL, NULL], NULL)─┐
-│                                    2 │
-└──────────────────────────────────────┘
-```
-
-## Información detallada) {#array_functions-arrayenumerate}
-
-Returns the array \[1, 2, 3, …, length (arr) \]
-
-Esta función se utiliza normalmente con ARRAY JOIN. Permite contar algo solo una vez para cada matriz después de aplicar ARRAY JOIN . Ejemplo:
-
-``` sql
-SELECT
-    count() AS Reaches,
-    countIf(num = 1) AS Hits
-FROM test.hits
-ARRAY JOIN
-    GoalsReached,
-    arrayEnumerate(GoalsReached) AS num
-WHERE CounterID = 160656
-LIMIT 10
-```
-
-``` text
-┌─Reaches─┬──Hits─┐
-│   95606 │ 31406 │
-└─────────┴───────┘
-```
-
-En este ejemplo, Reaches es el número de conversiones (las cadenas recibidas después de aplicar ARRAY JOIN) y Hits es el número de páginas vistas (cadenas antes de ARRAY JOIN). En este caso particular, puede obtener el mismo resultado de una manera más fácil:
-
-``` sql
-SELECT
-    sum(length(GoalsReached)) AS Reaches,
-    count() AS Hits
-FROM test.hits
-WHERE (CounterID = 160656) AND notEmpty(GoalsReached)
-```
-
-``` text
-┌─Reaches─┬──Hits─┐
-│   95606 │ 31406 │
-└─────────┴───────┘
-```
-
-Esta función también se puede utilizar en funciones de orden superior. Por ejemplo, puede usarlo para obtener índices de matriz para elementos que coinciden con una condición.
-
-## arrayEnumerateUniq(arr, …) {#arrayenumerateuniqarr}
-
-Devuelve una matriz del mismo tamaño que la matriz de origen, indicando para cada elemento cuál es su posición entre los elementos con el mismo valor.
-Por ejemplo: arrayEnumerateUniq(\[10, 20, 10, 30\]) = \[1, 1, 2, 1\].
-
-Esta función es útil cuando se utiliza ARRAY JOIN y la agregación de elementos de matriz.
-Ejemplo:
-
-``` sql
-SELECT
-    Goals.ID AS GoalID,
-    sum(Sign) AS Reaches,
-    sumIf(Sign, num = 1) AS Visits
-FROM test.visits
-ARRAY JOIN
-    Goals,
-    arrayEnumerateUniq(Goals.ID) AS num
-WHERE CounterID = 160656
-GROUP BY GoalID
-ORDER BY Reaches DESC
-LIMIT 10
-```
-
-``` text
-┌──GoalID─┬─Reaches─┬─Visits─┐
-│   53225 │    3214 │   1097 │
-│ 2825062 │    3188 │   1097 │
-│   56600 │    2803 │    488 │
-│ 1989037 │    2401 │    365 │
-│ 2830064 │    2396 │    910 │
-│ 1113562 │    2372 │    373 │
-│ 3270895 │    2262 │    812 │
-│ 1084657 │    2262 │    345 │
-│   56599 │    2260 │    799 │
-│ 3271094 │    2256 │    812 │
-└─────────┴─────────┴────────┘
-```
-
-En este ejemplo, cada ID de objetivo tiene un cálculo del número de conversiones (cada elemento de la estructura de datos anidados Objetivos es un objetivo alcanzado, al que nos referimos como conversión) y el número de sesiones. Sin ARRAY JOIN, habríamos contado el número de sesiones como sum(Sign) . Pero en este caso particular, las filas se multiplicaron por la estructura de Objetivos anidados, por lo que para contar cada sesión una vez después de esto, aplicamos una condición al valor de la función arrayEnumerateUniq(Goals.ID) .
-
-La función arrayEnumerateUniq puede tomar varias matrices del mismo tamaño que los argumentos. En este caso, la singularidad se considera para tuplas de elementos en las mismas posiciones en todas las matrices.
-
-``` sql
-SELECT arrayEnumerateUniq([1, 1, 1, 2, 2, 2], [1, 1, 2, 1, 1, 2]) AS res
-```
-
-``` text
-┌─res───────────┐
-│ [1,2,1,1,2,1] │
-└───────────────┘
-```
-
-Esto es necesario cuando se utiliza ARRAY JOIN con una estructura de datos anidados y una agregación adicional a través de múltiples elementos de esta estructura.
-
-## arrayPopBack {#arraypopback}
-
-Quita el último elemento de la matriz.
-
-``` sql
-arrayPopBack(array)
-```
-
-**Parámetros**
-
--   `array` – Array.
-
-**Ejemplo**
-
-``` sql
-SELECT arrayPopBack([1, 2, 3]) AS res
-```
-
-``` text
-┌─res───┐
-│ [1,2] │
-└───────┘
-```
-
-## arrayPopFront {#arraypopfront}
-
-Quita el primer elemento de la matriz.
-
-``` sql
-arrayPopFront(array)
-```
-
-**Parámetros**
-
--   `array` – Array.
-
-**Ejemplo**
-
-``` sql
-SELECT arrayPopFront([1, 2, 3]) AS res
-```
-
-``` text
-┌─res───┐
-│ [2,3] │
-└───────┘
-```
-
-## arrayPushBack {#arraypushback}
-
-Agrega un elemento al final de la matriz.
-
-``` sql
-arrayPushBack(array, single_value)
-```
-
-**Parámetros**
-
--   `array` – Array.
--   `single_value` – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the `single_value` tipo para el tipo de datos de la matriz. Para obtener más información sobre los tipos de datos en ClickHouse, consulte “[Tipos de datos](../../sql-reference/data-types/index.md#data_types)”. Puede ser `NULL`. La función agrega un `NULL` elemento de matriz a una matriz, y el tipo de elementos de matriz se convierte en `Nullable`.
-
-**Ejemplo**
-
-``` sql
-SELECT arrayPushBack(['a'], 'b') AS res
-```
-
-``` text
-┌─res───────┐
-│ ['a','b'] │
-└───────────┘
-```
-
-## arrayPushFront {#arraypushfront}
-
-Agrega un elemento al principio de la matriz.
-
-``` sql
-arrayPushFront(array, single_value)
-```
-
-**Parámetros**
-
--   `array` – Array.
--   `single_value` – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the `single_value` tipo para el tipo de datos de la matriz. Para obtener más información sobre los tipos de datos en ClickHouse, consulte “[Tipos de datos](../../sql-reference/data-types/index.md#data_types)”. Puede ser `NULL`. La función agrega un `NULL` elemento de matriz a una matriz, y el tipo de elementos de matriz se convierte en `Nullable`.
-
-**Ejemplo**
-
-``` sql
-SELECT arrayPushFront(['b'], 'a') AS res
-```
-
-``` text
-┌─res───────┐
-│ ['a','b'] │
-└───────────┘
-```
-
-## arrayResize {#arrayresize}
-
-Cambia la longitud de la matriz.
-
-``` sql
-arrayResize(array, size[, extender])
-```
-
-**Parámetros:**
-
--   `array` — Array.
--   `size` — Required length of the array.
-    -   Si `size` es menor que el tamaño original de la matriz, la matriz se trunca desde la derecha.
--   Si `size` es mayor que el tamaño inicial de la matriz, la matriz se extiende a la derecha con `extender` valores predeterminados para el tipo de datos de los elementos de la matriz.
--   `extender` — Value for extending an array. Can be `NULL`.
-
-**Valor devuelto:**
-
-Una matriz de longitud `size`.
-
-**Ejemplos de llamadas**
-
-``` sql
-SELECT arrayResize([1], 3)
-```
-
-``` text
-┌─arrayResize([1], 3)─┐
-│ [1,0,0]             │
-└─────────────────────┘
-```
-
-``` sql
-SELECT arrayResize([1], 3, NULL)
-```
-
-``` text
-┌─arrayResize([1], 3, NULL)─┐
-│ [1,NULL,NULL]             │
-└───────────────────────────┘
-```
-
-## arraySlice {#arrayslice}
-
-Devuelve una porción de la matriz.
-
-``` sql
-arraySlice(array, offset[, length])
-```
-
-**Parámetros**
-
--   `array` – Array of data.
--   `offset` – Indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. Numbering of the array items begins with 1.
--   `length` - La longitud de la porción requerida. Si especifica un valor negativo, la función devuelve un segmento abierto `[offset, array_length - length)`. Si omite el valor, la función devuelve el sector `[offset, the_end_of_array]`.
-
-**Ejemplo**
-
-``` sql
-SELECT arraySlice([1, 2, NULL, 4, 5], 2, 3) AS res
-```
-
-``` text
-┌─res────────┐
-│ [2,NULL,4] │
-└────────────┘
-```
-
-Elementos de matriz establecidos en `NULL` se manejan como valores normales.
-
-## arraySort(\[func,\] arr, …) {#array_functions-sort}
-
-Ordena los elementos del `arr` matriz en orden ascendente. Si el `func` se especifica la función, el orden de clasificación está determinado por el resultado `func` función aplicada a los elementos de la matriz. Si `func` acepta múltiples argumentos, el `arraySort` función se pasa varias matrices que los argumentos de `func` corresponderá a. Los ejemplos detallados se muestran al final de `arraySort` descripci.
-
-Ejemplo de clasificación de valores enteros:
-
-``` sql
-SELECT arraySort([1, 3, 3, 0]);
-```
-
-``` text
-┌─arraySort([1, 3, 3, 0])─┐
-│ [0,1,3,3]               │
-└─────────────────────────┘
-```
-
-Ejemplo de ordenación de valores de cadena:
-
-``` sql
-SELECT arraySort(['hello', 'world', '!']);
-```
-
-``` text
-┌─arraySort(['hello', 'world', '!'])─┐
-│ ['!','hello','world']              │
-└────────────────────────────────────┘
-```
-
-Considere el siguiente orden de clasificación `NULL`, `NaN` y `Inf` valor:
-
-``` sql
-SELECT arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]);
-```
-
-``` text
-┌─arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf])─┐
-│ [-inf,-4,1,2,3,inf,nan,nan,NULL,NULL]                     │
-└───────────────────────────────────────────────────────────┘
-```
-
--   `-Inf` los valores son los primeros en la matriz.
--   `NULL` los valores son los últimos en la matriz.
--   `NaN` los valores están justo antes `NULL`.
--   `Inf` los valores están justo antes `NaN`.
-
-Tenga en cuenta que `arraySort` es una [función de orden superior](higher-order-functions.md). Puede pasarle una función lambda como primer argumento. En este caso, el orden de clasificación está determinado por el resultado de la función lambda aplicada a los elementos de la matriz.
-
-Consideremos el siguiente ejemplo:
-
-``` sql
-SELECT arraySort((x) -> -x, [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [3,2,1] │
-└─────────┘
-```
-
-For each element of the source array, the lambda function returns the sorting key, that is, \[1 –\> -1, 2 –\> -2, 3 –\> -3\]. Since the `arraySort` función ordena las teclas en orden ascendente, el resultado es \[3, 2, 1\]. Por lo tanto, el `(x) –> -x` la función lambda establece la [orden descendente](#array_functions-reverse-sort) en una clasificación.
-
-La función lambda puede aceptar múltiples argumentos. En este caso, debe pasar el `arraySort` función varias matrices de idéntica longitud a las que corresponderán los argumentos de la función lambda. La matriz resultante constará de elementos de la primera matriz de entrada; los elementos de la siguiente matriz de entrada especifican las claves de clasificación. Por ejemplo:
-
-``` sql
-SELECT arraySort((x, y) -> y, ['hello', 'world'], [2, 1]) as res;
-```
-
-``` text
-┌─res────────────────┐
-│ ['world', 'hello'] │
-└────────────────────┘
-```
-
-Aquí, los elementos que se pasan en la segunda matriz (\[2, 1\]) definen una clave de ordenación para el elemento correspondiente de la matriz de origen (\[‘hello’, ‘world’Es decir,, \[‘hello’ –\> 2, ‘world’ –\> 1\]. Since the lambda function doesn't use `x`, los valores reales de la matriz de origen no afectan el orden en el resultado. Tan, ‘hello’ será el segundo elemento en el resultado, y ‘world’ será la primera.
-
-Otros ejemplos se muestran a continuación.
-
-``` sql
-SELECT arraySort((x, y) -> y, [0, 1, 2], ['c', 'b', 'a']) as res;
-```
-
-``` text
-┌─res─────┐
-│ [2,1,0] │
-└─────────┘
-```
-
-``` sql
-SELECT arraySort((x, y) -> -y, [0, 1, 2], [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [2,1,0] │
-└─────────┘
-```
-
-!!! note "Nota"
-    Para mejorar la eficiencia de clasificación, el [Transformación de Schwartzian](https://en.wikipedia.org/wiki/Schwartzian_transform) se utiliza.
-
-## arrayReverseSort(\[func,\] arr, …) {#array_functions-reverse-sort}
-
-Ordena los elementos del `arr` matriz en orden descendente. Si el `func` se especifica la función, `arr` se ordena de acuerdo con el resultado de la `func` función aplicada a los elementos de la matriz, y luego la matriz ordenada se invierte. Si `func` acepta múltiples argumentos, el `arrayReverseSort` función se pasa varias matrices que los argumentos de `func` corresponderá a. Los ejemplos detallados se muestran al final de `arrayReverseSort` descripci.
-
-Ejemplo de clasificación de valores enteros:
-
-``` sql
-SELECT arrayReverseSort([1, 3, 3, 0]);
-```
-
-``` text
-┌─arrayReverseSort([1, 3, 3, 0])─┐
-│ [3,3,1,0]                      │
-└────────────────────────────────┘
-```
-
-Ejemplo de ordenación de valores de cadena:
-
-``` sql
-SELECT arrayReverseSort(['hello', 'world', '!']);
-```
-
-``` text
-┌─arrayReverseSort(['hello', 'world', '!'])─┐
-│ ['world','hello','!']                     │
-└───────────────────────────────────────────┘
-```
-
-Considere el siguiente orden de clasificación `NULL`, `NaN` y `Inf` valor:
-
-``` sql
-SELECT arrayReverseSort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]) as res;
-```
-
-``` text
-┌─res───────────────────────────────────┐
-│ [inf,3,2,1,-4,-inf,nan,nan,NULL,NULL] │
-└───────────────────────────────────────┘
-```
-
--   `Inf` los valores son los primeros en la matriz.
--   `NULL` los valores son los últimos en la matriz.
--   `NaN` los valores están justo antes `NULL`.
--   `-Inf` los valores están justo antes `NaN`.
-
-Tenga en cuenta que el `arrayReverseSort` es una [función de orden superior](higher-order-functions.md). Puede pasarle una función lambda como primer argumento. Ejemplo se muestra a continuación.
-
-``` sql
-SELECT arrayReverseSort((x) -> -x, [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [1,2,3] │
-└─────────┘
-```
-
-La matriz se ordena de la siguiente manera:
-
-1.  Al principio, la matriz de origen (\[1, 2, 3\]) se ordena de acuerdo con el resultado de la función lambda aplicada a los elementos de la matriz. El resultado es una matriz \[3, 2, 1\].
-2.  Matriz que se obtiene en el paso anterior, se invierte. Entonces, el resultado final es \[1, 2, 3\].
-
-La función lambda puede aceptar múltiples argumentos. En este caso, debe pasar el `arrayReverseSort` función varias matrices de idéntica longitud a las que corresponderán los argumentos de la función lambda. La matriz resultante constará de elementos de la primera matriz de entrada; los elementos de la siguiente matriz de entrada especifican las claves de clasificación. Por ejemplo:
-
-``` sql
-SELECT arrayReverseSort((x, y) -> y, ['hello', 'world'], [2, 1]) as res;
-```
-
-``` text
-┌─res───────────────┐
-│ ['hello','world'] │
-└───────────────────┘
-```
-
-En este ejemplo, la matriz se ordena de la siguiente manera:
-
-1.  Al principio, la matriz de origen (\[‘hello’, ‘world’\]) se ordena de acuerdo con el resultado de la función lambda aplicada a los elementos de las matrices. Los elementos que se pasan en la segunda matriz (\[2, 1\]), definen las claves de ordenación para los elementos correspondientes de la matriz de origen. El resultado es una matriz \[‘world’, ‘hello’\].
-2.  Matriz que se ordenó en el paso anterior, se invierte. Entonces, el resultado final es \[‘hello’, ‘world’\].
-
-Otros ejemplos se muestran a continuación.
-
-``` sql
-SELECT arrayReverseSort((x, y) -> y, [4, 3, 5], ['a', 'b', 'c']) AS res;
-```
-
-``` text
-┌─res─────┐
-│ [5,3,4] │
-└─────────┘
-```
-
-``` sql
-SELECT arrayReverseSort((x, y) -> -y, [4, 3, 5], [1, 2, 3]) AS res;
-```
-
-``` text
-┌─res─────┐
-│ [4,3,5] │
-└─────────┘
-```
-
-## arrayUniq(arr, …) {#arrayuniqarr}
-
-Si se pasa un argumento, cuenta el número de elementos diferentes en la matriz.
-Si se pasan varios argumentos, cuenta el número de tuplas diferentes de elementos en las posiciones correspondientes en múltiples matrices.
-
-Si desea obtener una lista de elementos únicos en una matriz, puede usar arrayReduce(‘groupUniqArray’ arr).
-
-## Información adicional) {#array-functions-join}
-
-Una función especial. Vea la sección [“ArrayJoin function”](array-join.md#functions_arrayjoin).
-
-## arrayDifference {#arraydifference}
-
-Calcula la diferencia entre los elementos de matriz adyacentes. Devuelve una matriz donde el primer elemento será 0, el segundo es la diferencia entre `a[1] - a[0]`, etc. The type of elements in the resulting array is determined by the type inference rules for subtraction (e.g. `UInt8` - `UInt8` = `Int16`).
-
-**Sintaxis**
-
-``` sql
-arrayDifference(array)
-```
-
-**Parámetros**
-
--   `array` – [Matriz](https://clickhouse.tech/docs/en/data_types/array/).
-
-**Valores devueltos**
-
-Devuelve una matriz de diferencias entre los elementos adyacentes.
-
-Tipo: [UInt\*](https://clickhouse.tech/docs/en/data_types/int_uint/#uint-ranges), [En\*](https://clickhouse.tech/docs/en/data_types/int_uint/#int-ranges), [Flotante\*](https://clickhouse.tech/docs/en/data_types/float/).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT arrayDifference([1, 2, 3, 4])
-```
-
-Resultado:
-
-``` text
-┌─arrayDifference([1, 2, 3, 4])─┐
-│ [0,1,1,1]                     │
-└───────────────────────────────┘
-```
-
-Ejemplo del desbordamiento debido al tipo de resultado Int64:
-
-Consulta:
-
-``` sql
-SELECT arrayDifference([0, 10000000000000000000])
-```
-
-Resultado:
-
-``` text
-┌─arrayDifference([0, 10000000000000000000])─┐
-│ [0,-8446744073709551616]                   │
-└────────────────────────────────────────────┘
-```
-
-## arrayDistinct {#arraydistinct}
-
-Toma una matriz, devuelve una matriz que contiene solo los elementos distintos.
-
-**Sintaxis**
-
-``` sql
-arrayDistinct(array)
-```
-
-**Parámetros**
-
--   `array` – [Matriz](https://clickhouse.tech/docs/en/data_types/array/).
-
-**Valores devueltos**
-
-Devuelve una matriz que contiene los elementos distintos.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT arrayDistinct([1, 2, 2, 3, 1])
-```
-
-Resultado:
-
-``` text
-┌─arrayDistinct([1, 2, 2, 3, 1])─┐
-│ [1,2,3]                        │
-└────────────────────────────────┘
-```
-
-## Aquí hay algunas opciones) {#array_functions-arrayenumeratedense}
-
-Devuelve una matriz del mismo tamaño que la matriz de origen, lo que indica dónde aparece cada elemento por primera vez en la matriz de origen.
-
-Ejemplo:
-
-``` sql
-SELECT arrayEnumerateDense([10, 20, 10, 30])
-```
-
-``` text
-┌─arrayEnumerateDense([10, 20, 10, 30])─┐
-│ [1,2,1,3]                             │
-└───────────────────────────────────────┘
-```
-
-## Información detallada) {#array-functions-arrayintersect}
-
-Toma varias matrices, devuelve una matriz con elementos que están presentes en todas las matrices de origen. El orden de los elementos en la matriz resultante es el mismo que en la primera matriz.
-
-Ejemplo:
-
-``` sql
-SELECT
-    arrayIntersect([1, 2], [1, 3], [2, 3]) AS no_intersect,
-    arrayIntersect([1, 2], [1, 3], [1, 4]) AS intersect
-```
-
-``` text
-┌─no_intersect─┬─intersect─┐
-│ []           │ [1]       │
-└──────────────┴───────────┘
-```
-
-## arrayReduce {#arrayreduce}
-
-Aplica una función de agregado a los elementos de la matriz y devuelve su resultado. El nombre de la función de agregación se pasa como una cadena entre comillas simples `'max'`, `'sum'`. Cuando se utilizan funciones de agregado paramétrico, el parámetro se indica después del nombre de la función entre paréntesis `'uniqUpTo(6)'`.
-
-**Sintaxis**
-
-``` sql
-arrayReduce(agg_func, arr1, arr2, ..., arrN)
-```
-
-**Parámetros**
-
--   `agg_func` — The name of an aggregate function which should be a constant [cadena](../../sql-reference/data-types/string.md).
--   `arr` — Any number of [matriz](../../sql-reference/data-types/array.md) escriba columnas como los parámetros de la función de agregación.
-
-**Valor devuelto**
-
-**Ejemplo**
-
-``` sql
-SELECT arrayReduce('max', [1, 2, 3])
-```
-
-``` text
-┌─arrayReduce('max', [1, 2, 3])─┐
-│                             3 │
-└───────────────────────────────┘
-```
-
-Si una función agregada toma varios argumentos, esta función debe aplicarse a varias matrices del mismo tamaño.
-
-``` sql
-SELECT arrayReduce('maxIf', [3, 5], [1, 0])
-```
-
-``` text
-┌─arrayReduce('maxIf', [3, 5], [1, 0])─┐
-│                                    3 │
-└──────────────────────────────────────┘
-```
-
-Ejemplo con una función de agregado paramétrico:
-
-``` sql
-SELECT arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
-```
-
-``` text
-┌─arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])─┐
-│                                                           4 │
-└─────────────────────────────────────────────────────────────┘
-```
-
-## arrayReduceInRanges {#arrayreduceinranges}
-
-Aplica una función de agregado a los elementos de matriz en rangos dados y devuelve una matriz que contiene el resultado correspondiente a cada rango. La función devolverá el mismo resultado que múltiples `arrayReduce(agg_func, arraySlice(arr1, index, length), ...)`.
-
-**Sintaxis**
-
-``` sql
-arrayReduceInRanges(agg_func, ranges, arr1, arr2, ..., arrN)
-```
-
-**Parámetros**
-
--   `agg_func` — The name of an aggregate function which should be a constant [cadena](../../sql-reference/data-types/string.md).
--   `ranges` — The ranges to aggretate which should be an [matriz](../../sql-reference/data-types/array.md) de [tuplas](../../sql-reference/data-types/tuple.md) que contiene el índice y la longitud de cada rango.
--   `arr` — Any number of [matriz](../../sql-reference/data-types/array.md) escriba columnas como los parámetros de la función de agregación.
-
-**Valor devuelto**
-
-**Ejemplo**
-
-``` sql
-SELECT arrayReduceInRanges(
-    'sum',
-    [(1, 5), (2, 3), (3, 4), (4, 4)],
-    [1000000, 200000, 30000, 4000, 500, 60, 7]
-) AS res
-```
-
-``` text
-┌─res─────────────────────────┐
-│ [1234500,234000,34560,4567] │
-└─────────────────────────────┘
-```
-
-## arrayReverse (arr) {#arrayreverse}
-
-Devuelve una matriz del mismo tamaño que la matriz original que contiene los elementos en orden inverso.
-
-Ejemplo:
-
-``` sql
-SELECT arrayReverse([1, 2, 3])
-```
-
-``` text
-┌─arrayReverse([1, 2, 3])─┐
-│ [3,2,1]                 │
-└─────────────────────────┘
-```
-
-## inversa(arr) {#array-functions-reverse}
-
-Sinónimo de [“arrayReverse”](#arrayreverse)
-
-## arrayFlatten {#arrayflatten}
-
-Convierte una matriz de matrices en una matriz plana.
-
-Función:
-
--   Se aplica a cualquier profundidad de matrices anidadas.
--   No cambia las matrices que ya son planas.
-
-La matriz aplanada contiene todos los elementos de todas las matrices de origen.
-
-**Sintaxis**
-
-``` sql
-flatten(array_of_arrays)
-```
-
-Apodo: `flatten`.
-
-**Parámetros**
-
--   `array_of_arrays` — [Matriz](../../sql-reference/data-types/array.md) de matrices. Por ejemplo, `[[1,2,3], [4,5]]`.
-
-**Ejemplos**
-
-``` sql
-SELECT flatten([[[1]], [[2], [3]]])
-```
-
-``` text
-┌─flatten(array(array([1]), array([2], [3])))─┐
-│ [1,2,3]                                     │
-└─────────────────────────────────────────────┘
-```
-
-## arrayCompact {#arraycompact}
-
-Elimina elementos duplicados consecutivos de una matriz. El orden de los valores de resultado está determinado por el orden de la matriz de origen.
-
-**Sintaxis**
-
-``` sql
-arrayCompact(arr)
-```
-
-**Parámetros**
-
-`arr` — The [matriz](../../sql-reference/data-types/array.md) inspeccionar.
-
-**Valor devuelto**
-
-La matriz sin duplicado.
-
-Tipo: `Array`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT arrayCompact([1, 1, nan, nan, 2, 3, 3, 3])
-```
-
-Resultado:
-
-``` text
-┌─arrayCompact([1, 1, nan, nan, 2, 3, 3, 3])─┐
-│ [1,nan,nan,2,3]                            │
-└────────────────────────────────────────────┘
-```
-
-## arrayZip {#arrayzip}
-
-Combina varias matrices en una sola matriz. La matriz resultante contiene los elementos correspondientes de las matrices de origen agrupadas en tuplas en el orden de argumentos enumerado.
-
-**Sintaxis**
-
-``` sql
-arrayZip(arr1, arr2, ..., arrN)
-```
-
-**Parámetros**
-
--   `arrN` — [Matriz](../data-types/array.md).
-
-La función puede tomar cualquier cantidad de matrices de diferentes tipos. Todas las matrices de entrada deben ser del mismo tamaño.
-
-**Valor devuelto**
-
--   Matriz con elementos de las matrices de origen agrupadas en [tuplas](../data-types/tuple.md). Los tipos de datos en la tupla son los mismos que los tipos de las matrices de entrada y en el mismo orden en que se pasan las matrices.
-
-Tipo: [Matriz](../data-types/array.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT arrayZip(['a', 'b', 'c'], [5, 2, 1])
-```
-
-Resultado:
-
-``` text
-┌─arrayZip(['a', 'b', 'c'], [5, 2, 1])─┐
-│ [('a',5),('b',2),('c',1)]            │
-└──────────────────────────────────────┘
-```
-
-## arrayAUC {#arrayauc}
-
-Calcule AUC (Área bajo la curva, que es un concepto en el aprendizaje automático, vea más detalles: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve ).
-
-**Sintaxis**
-
-``` sql
-arrayAUC(arr_scores, arr_labels)
-```
-
-**Parámetros**
-- `arr_scores` — scores prediction model gives.
-- `arr_labels` — labels of samples, usually 1 for positive sample and 0 for negtive sample.
-
-**Valor devuelto**
-Devuelve el valor AUC con el tipo Float64.
-
-**Ejemplo**
-Consulta:
-
-``` sql
-select arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1])
-```
-
-Resultado:
-
-``` text
-┌─arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1])─┐
-│                                          0.75 │
-└────────────────────────────────────────---──┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/array_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/array-join.md b/docs/es/sql-reference/functions/array-join.md
deleted file mode 100644
index 555b609c5d2c..000000000000
--- a/docs/es/sql-reference/functions/array-join.md
+++ /dev/null
@@ -1,37 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: arrayJoin
----
-
-# arrayJoin función {#functions_arrayjoin}
-
-Esta es una función muy inusual.
-
-Las funciones normales no cambian un conjunto de filas, sino que simplemente cambian los valores en cada fila (mapa).
-Las funciones agregadas comprimen un conjunto de filas (doblar o reducir).
-El ‘arrayJoin’ función toma cada fila y genera un conjunto de filas (desplegar).
-
-Esta función toma una matriz como argumento y propaga la fila de origen a varias filas para el número de elementos de la matriz.
-Todos los valores de las columnas simplemente se copian, excepto los valores de la columna donde se aplica esta función; se reemplaza con el valor de matriz correspondiente.
-
-Una consulta puede usar múltiples `arrayJoin` función. En este caso, la transformación se realiza varias veces.
-
-Tenga en cuenta la sintaxis ARRAY JOIN en la consulta SELECT, que proporciona posibilidades más amplias.
-
-Ejemplo:
-
-``` sql
-SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src
-```
-
-``` text
-┌─dst─┬─\'Hello\'─┬─src─────┐
-│   1 │ Hello     │ [1,2,3] │
-│   2 │ Hello     │ [1,2,3] │
-│   3 │ Hello     │ [1,2,3] │
-└─────┴───────────┴─────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/array_join/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/bit-functions.md b/docs/es/sql-reference/functions/bit-functions.md
deleted file mode 100644
index b5743051b7d0..000000000000
--- a/docs/es/sql-reference/functions/bit-functions.md
+++ /dev/null
@@ -1,255 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 48
-toc_title: Trozo
----
-
-# Bit Funciones {#bit-functions}
-
-Las funciones de bits funcionan para cualquier par de tipos de UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64, Float32 o Float64.
-
-El tipo de resultado es un entero con bits iguales a los bits máximos de sus argumentos. Si al menos uno de los argumentos está firmado, el resultado es un número firmado. Si un argumento es un número de coma flotante, se convierte en Int64.
-
-## pocoY(a, b) {#bitanda-b}
-
-## bitOr (a, b) {#bitora-b}
-
-## ¿Por qué?) {#bitxora-b}
-
-## ¿Por qué?) {#bitnota}
-
-## ¿Cómo puedo hacerlo?) {#bitshiftlefta-b}
-
-## ¿Cómo puedo hacerlo?) {#bitshiftrighta-b}
-
-## ¿Cómo puedo hacerlo?) {#bitrotatelefta-b}
-
-## ¿Cómo puedo hacerlo?) {#bitrotaterighta-b}
-
-## bitTest {#bittest}
-
-Toma cualquier entero y lo convierte en [forma binaria](https://en.wikipedia.org/wiki/Binary_number) devuelve el valor de un bit en la posición especificada. La cuenta atrás comienza desde 0 de derecha a izquierda.
-
-**Sintaxis**
-
-``` sql
-SELECT bitTest(number, index)
-```
-
-**Parámetros**
-
--   `number` – integer number.
--   `index` – position of bit.
-
-**Valores devueltos**
-
-Devuelve un valor de bit en la posición especificada.
-
-Tipo: `UInt8`.
-
-**Ejemplo**
-
-Por ejemplo, el número 43 en el sistema numérico base-2 (binario) es 101011.
-
-Consulta:
-
-``` sql
-SELECT bitTest(43, 1)
-```
-
-Resultado:
-
-``` text
-┌─bitTest(43, 1)─┐
-│              1 │
-└────────────────┘
-```
-
-Otro ejemplo:
-
-Consulta:
-
-``` sql
-SELECT bitTest(43, 2)
-```
-
-Resultado:
-
-``` text
-┌─bitTest(43, 2)─┐
-│              0 │
-└────────────────┘
-```
-
-## bitTestAll {#bittestall}
-
-Devuelve el resultado de [conjunción lógica](https://en.wikipedia.org/wiki/Logical_conjunction) (Operador AND) de todos los bits en posiciones dadas. La cuenta atrás comienza desde 0 de derecha a izquierda.
-
-La conjucción para operaciones bit a bit:
-
-0 AND 0 = 0
-
-0 AND 1 = 0
-
-1 AND 0 = 0
-
-1 AND 1 = 1
-
-**Sintaxis**
-
-``` sql
-SELECT bitTestAll(number, index1, index2, index3, index4, ...)
-```
-
-**Parámetros**
-
--   `number` – integer number.
--   `index1`, `index2`, `index3`, `index4` – positions of bit. For example, for set of positions (`index1`, `index2`, `index3`, `index4`) es verdadero si y solo si todas sus posiciones son verdaderas (`index1` ⋀ `index2`, ⋀ `index3` ⋀ `index4`).
-
-**Valores devueltos**
-
-Devuelve el resultado de la conjunción lógica.
-
-Tipo: `UInt8`.
-
-**Ejemplo**
-
-Por ejemplo, el número 43 en el sistema numérico base-2 (binario) es 101011.
-
-Consulta:
-
-``` sql
-SELECT bitTestAll(43, 0, 1, 3, 5)
-```
-
-Resultado:
-
-``` text
-┌─bitTestAll(43, 0, 1, 3, 5)─┐
-│                          1 │
-└────────────────────────────┘
-```
-
-Otro ejemplo:
-
-Consulta:
-
-``` sql
-SELECT bitTestAll(43, 0, 1, 3, 5, 2)
-```
-
-Resultado:
-
-``` text
-┌─bitTestAll(43, 0, 1, 3, 5, 2)─┐
-│                             0 │
-└───────────────────────────────┘
-```
-
-## bitTestAny {#bittestany}
-
-Devuelve el resultado de [disyunción lógica](https://en.wikipedia.org/wiki/Logical_disjunction) (O operador) de todos los bits en posiciones dadas. La cuenta atrás comienza desde 0 de derecha a izquierda.
-
-La disyunción para las operaciones bit a bit:
-
-0 OR 0 = 0
-
-0 OR 1 = 1
-
-1 OR 0 = 1
-
-1 OR 1 = 1
-
-**Sintaxis**
-
-``` sql
-SELECT bitTestAny(number, index1, index2, index3, index4, ...)
-```
-
-**Parámetros**
-
--   `number` – integer number.
--   `index1`, `index2`, `index3`, `index4` – positions of bit.
-
-**Valores devueltos**
-
-Devuelve el resultado de disjuction lógico.
-
-Tipo: `UInt8`.
-
-**Ejemplo**
-
-Por ejemplo, el número 43 en el sistema numérico base-2 (binario) es 101011.
-
-Consulta:
-
-``` sql
-SELECT bitTestAny(43, 0, 2)
-```
-
-Resultado:
-
-``` text
-┌─bitTestAny(43, 0, 2)─┐
-│                    1 │
-└──────────────────────┘
-```
-
-Otro ejemplo:
-
-Consulta:
-
-``` sql
-SELECT bitTestAny(43, 4, 2)
-```
-
-Resultado:
-
-``` text
-┌─bitTestAny(43, 4, 2)─┐
-│                    0 │
-└──────────────────────┘
-```
-
-## bitCount {#bitcount}
-
-Calcula el número de bits establecido en uno en la representación binaria de un número.
-
-**Sintaxis**
-
-``` sql
-bitCount(x)
-```
-
-**Parámetros**
-
--   `x` — [Entero](../../sql-reference/data-types/int-uint.md) o [punto flotante](../../sql-reference/data-types/float.md) numero. La función utiliza la representación de valor en la memoria. Permite admitir números de punto flotante.
-
-**Valor devuelto**
-
--   Número de bits establecido en uno en el número de entrada.
-
-La función no convierte el valor de entrada a un tipo más grande ([extensión de signo](https://en.wikipedia.org/wiki/Sign_extension)). Entonces, por ejemplo, `bitCount(toUInt8(-1)) = 8`.
-
-Tipo: `UInt8`.
-
-**Ejemplo**
-
-Tomemos por ejemplo el número 333. Su representación binaria: 0000000101001101.
-
-Consulta:
-
-``` sql
-SELECT bitCount(333)
-```
-
-Resultado:
-
-``` text
-┌─bitCount(333)─┐
-│             5 │
-└───────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/bit_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/bitmap-functions.md b/docs/es/sql-reference/functions/bitmap-functions.md
deleted file mode 100644
index 1643801c77b5..000000000000
--- a/docs/es/sql-reference/functions/bitmap-functions.md
+++ /dev/null
@@ -1,496 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 49
-toc_title: Bits
----
-
-# Funciones de mapa de bits {#bitmap-functions}
-
-Las funciones de mapa de bits funcionan para dos mapas de bits Cálculo del valor del objeto, es devolver un nuevo mapa de bits o cardinalidad mientras se usa el cálculo de la fórmula, como y, o, xor, y no, etc.
-
-Hay 2 tipos de métodos de construcción para Bitmap Object. Uno debe ser construido por la función de agregación groupBitmap con -State , el otro debe ser construido por Array Object . También es para convertir objeto de mapa de bits a objeto de matriz.
-
-RoaringBitmap se envuelve en una estructura de datos, mientras que el almacenamiento real de los objetos de mapa de bits. Cuando la cardinalidad es menor o igual que 32, utiliza Set objet. Cuando la cardinalidad es mayor que 32, utiliza el objeto RoaringBitmap. Es por eso que el almacenamiento del conjunto de baja cardinalidad es más rápido.
-
-Para obtener más información sobre RoaringBitmap, consulte: [CRoaring](https://github.com/RoaringBitmap/CRoaring).
-
-## bitmapBuild {#bitmap_functions-bitmapbuild}
-
-Construya un mapa de bits a partir de una matriz de enteros sin signo.
-
-``` sql
-bitmapBuild(array)
-```
-
-**Parámetros**
-
--   `array` – unsigned integer array.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapBuild([1, 2, 3, 4, 5]) AS res, toTypeName(res)
-```
-
-``` text
-┌─res─┬─toTypeName(bitmapBuild([1, 2, 3, 4, 5]))─────┐
-│     │ AggregateFunction(groupBitmap, UInt8)    │
-└─────┴──────────────────────────────────────────────┘
-```
-
-## bitmapToArray {#bitmaptoarray}
-
-Convertir mapa de bits a matriz entera.
-
-``` sql
-bitmapToArray(bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapToArray(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-``` text
-┌─res─────────┐
-│ [1,2,3,4,5] │
-└─────────────┘
-```
-
-## bitmapSubsetInRange {#bitmap-functions-bitmapsubsetinrange}
-
-Devuelve el subconjunto en el rango especificado (no incluye range_end).
-
-``` sql
-bitmapSubsetInRange(bitmap, range_start, range_end)
-```
-
-**Parámetros**
-
--   `bitmap` – [Objeto de mapa de bits](#bitmap_functions-bitmapbuild).
--   `range_start` – range start point. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
--   `range_end` – range end point(excluded). Type: [UInt32](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapToArray(bitmapSubsetInRange(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res
-```
-
-``` text
-┌─res───────────────┐
-│ [30,31,32,33,100] │
-└───────────────────┘
-```
-
-## bitmapSubsetLimit {#bitmapsubsetlimit}
-
-Crea un subconjunto de mapa de bits con n elementos tomados entre `range_start` y `cardinality_limit`.
-
-**Sintaxis**
-
-``` sql
-bitmapSubsetLimit(bitmap, range_start, cardinality_limit)
-```
-
-**Parámetros**
-
--   `bitmap` – [Objeto de mapa de bits](#bitmap_functions-bitmapbuild).
--   `range_start` – The subset starting point. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
--   `cardinality_limit` – The subset cardinality upper limit. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
-
-**Valor devuelto**
-
-Subconjunto.
-
-Tipo: `Bitmap object`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT bitmapToArray(bitmapSubsetLimit(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res
-```
-
-Resultado:
-
-``` text
-┌─res───────────────────────┐
-│ [30,31,32,33,100,200,500] │
-└───────────────────────────┘
-```
-
-## bitmapContains {#bitmap_functions-bitmapcontains}
-
-Comprueba si el mapa de bits contiene un elemento.
-
-``` sql
-bitmapContains(haystack, needle)
-```
-
-**Parámetros**
-
--   `haystack` – [Objeto de mapa de bits](#bitmap_functions-bitmapbuild), donde la función busca.
--   `needle` – Value that the function searches. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   0 — If `haystack` no contiene `needle`.
--   1 — If `haystack` contener `needle`.
-
-Tipo: `UInt8`.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapContains(bitmapBuild([1,5,7,9]), toUInt32(9)) AS res
-```
-
-``` text
-┌─res─┐
-│  1  │
-└─────┘
-```
-
-## bitmapHasAny {#bitmaphasany}
-
-Comprueba si dos mapas de bits tienen intersección por algunos elementos.
-
-``` sql
-bitmapHasAny(bitmap1, bitmap2)
-```
-
-Si está seguro de que `bitmap2` contiene estrictamente un elemento, considere usar el [bitmapContains](#bitmap_functions-bitmapcontains) función. Funciona de manera más eficiente.
-
-**Parámetros**
-
--   `bitmap*` – bitmap object.
-
-**Valores de retorno**
-
--   `1`, si `bitmap1` y `bitmap2` tienen un elemento similar al menos.
--   `0`, de lo contrario.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapHasAny(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res
-```
-
-``` text
-┌─res─┐
-│  1  │
-└─────┘
-```
-
-## bitmapHasAll {#bitmaphasall}
-
-Análogo a `hasAll(array, array)` devuelve 1 si el primer mapa de bits contiene todos los elementos del segundo, 0 de lo contrario.
-Si el segundo argumento es un mapa de bits vacío, devuelve 1.
-
-``` sql
-bitmapHasAll(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapHasAll(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res
-```
-
-``` text
-┌─res─┐
-│  0  │
-└─────┘
-```
-
-## bitmapCardinalidad {#bitmapcardinality}
-
-Vuelva a ejecutar la cardinalidad del mapa de bits de tipo UInt64.
-
-``` sql
-bitmapCardinality(bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapCardinality(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-``` text
-┌─res─┐
-│   5 │
-└─────┘
-```
-
-## bitmapMin {#bitmapmin}
-
-Vuelva a ejecutar el valor más pequeño de tipo UInt64 en el conjunto, UINT32_MAX si el conjunto está vacío.
-
-    bitmapMin(bitmap)
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapMin(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-    ┌─res─┐
-    │   1 │
-    └─────┘
-
-## bitmapMax {#bitmapmax}
-
-Vuelva a ejecutar el mayor valor de tipo UInt64 en el conjunto, 0 si el conjunto está vacío.
-
-    bitmapMax(bitmap)
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapMax(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-    ┌─res─┐
-    │   5 │
-    └─────┘
-
-## bitmapTransform {#bitmaptransform}
-
-Transformar una matriz de valores en un mapa de bits a otra matriz de valores, el resultado es un nuevo mapa de bits.
-
-    bitmapTransform(bitmap, from_array, to_array)
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
--   `from_array` – UInt32 array. For idx in range \[0, from_array.size()), if bitmap contains from_array\[idx\], then replace it with to_array\[idx\]. Note that the result depends on array ordering if there are common elements between from_array and to_array.
--   `to_array` – UInt32 array, its size shall be the same to from_array.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapToArray(bitmapTransform(bitmapBuild([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), cast([5,999,2] as Array(UInt32)), cast([2,888,20] as Array(UInt32)))) AS res
-```
-
-    ┌─res───────────────────┐
-    │ [1,3,4,6,7,8,9,10,20] │
-    └───────────────────────┘
-
-## bitmapAnd {#bitmapand}
-
-Dos mapa de bits y cálculo, el resultado es un nuevo mapa de bits.
-
-``` sql
-bitmapAnd(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapToArray(bitmapAnd(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res─┐
-│ [3] │
-└─────┘
-```
-
-## bitmapO {#bitmapor}
-
-Dos mapa de bits o cálculo, el resultado es un nuevo mapa de bits.
-
-``` sql
-bitmapOr(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapToArray(bitmapOr(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res─────────┐
-│ [1,2,3,4,5] │
-└─────────────┘
-```
-
-## bitmapXor {#bitmapxor}
-
-Dos bitmap xor cálculo, el resultado es un nuevo mapa de bits.
-
-``` sql
-bitmapXor(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapToArray(bitmapXor(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res───────┐
-│ [1,2,4,5] │
-└───────────┘
-```
-
-## bitmapAndnot {#bitmapandnot}
-
-Dos mapas de bits y no cálculo, el resultado es un nuevo mapa de bits.
-
-``` sql
-bitmapAndnot(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapToArray(bitmapAndnot(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res───┐
-│ [1,2] │
-└───────┘
-```
-
-## Bienvenido a WordPress {#bitmapandcardinality}
-
-Dos mapa de bits y cálculo, devuelven cardinalidad de tipo UInt64.
-
-``` sql
-bitmapAndCardinality(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapAndCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   1 │
-└─────┘
-```
-
-## bitmapOrCardinalidad {#bitmaporcardinality}
-
-Dos mapa de bits o cálculo, retorno cardinalidad de tipo UInt64.
-
-``` sql
-bitmapOrCardinality(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapOrCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   5 │
-└─────┘
-```
-
-## bitmapXorCardinalidad {#bitmapxorcardinality}
-
-Dos bitmap xor cálculo, retorno cardinalidad de tipo UInt64.
-
-``` sql
-bitmapXorCardinality(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapXorCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   4 │
-└─────┘
-```
-
-## Por favor, introduzca su dirección de correo electrónico {#bitmapandnotcardinality}
-
-Dos mapas de bits yno cálculo, devuelve cardinalidad de tipo UInt64.
-
-``` sql
-bitmapAndnotCardinality(bitmap,bitmap)
-```
-
-**Parámetros**
-
--   `bitmap` – bitmap object.
-
-**Ejemplo**
-
-``` sql
-SELECT bitmapAndnotCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   2 │
-└─────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/bitmap_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/comparison-functions.md b/docs/es/sql-reference/functions/comparison-functions.md
deleted file mode 100644
index 316288c9bc13..000000000000
--- a/docs/es/sql-reference/functions/comparison-functions.md
+++ /dev/null
@@ -1,37 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: "Comparaci\xF3n"
----
-
-# Funciones de comparación {#comparison-functions}
-
-Las funciones de comparación siempre devuelven 0 o 1 (Uint8).
-
-Se pueden comparar los siguientes tipos:
-
--   numero
--   cuerdas y cuerdas fijas
--   fechas
--   fechas con tiempos
-
-dentro de cada grupo, pero no entre diferentes grupos.
-
-Por ejemplo, no puede comparar una fecha con una cadena. Debe usar una función para convertir la cadena a una fecha, o viceversa.
-
-Las cadenas se comparan por bytes. Una cadena más corta es más pequeña que todas las cadenas que comienzan con ella y que contienen al menos un carácter más.
-
-## iguales, a = b y a == b operador {#function-equals}
-
-## notEquals, un ! operador = b y un \<\> b {#function-notequals}
-
-## menos, operador \<  {#function-less}
-
-## Saludos {#function-greater}
-
-## lessOrEquals, operador \<=  {#function-lessorequals}
-
-## greaterOrEquals, operador \>=  {#function-greaterorequals}
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/comparison_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/conditional-functions.md b/docs/es/sql-reference/functions/conditional-functions.md
deleted file mode 100644
index 5e6097d7c19c..000000000000
--- a/docs/es/sql-reference/functions/conditional-functions.md
+++ /dev/null
@@ -1,207 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: 'Condicional '
----
-
-# Funciones condicionales {#conditional-functions}
-
-## si {#if}
-
-Controla la bifurcación condicional. A diferencia de la mayoría de los sistemas, ClickHouse siempre evalúa ambas expresiones `then` y `else`.
-
-**Sintaxis**
-
-``` sql
-SELECT if(cond, then, else)
-```
-
-Si la condición `cond` evalúa a un valor distinto de cero, devuelve el resultado de la expresión `then` y el resultado de la expresión `else` si está presente, se omite. Si el `cond` es cero o `NULL` el resultado de la `then` expresión se omite y el resultado de la `else` expresión, si está presente, se devuelve.
-
-**Parámetros**
-
--   `cond` – The condition for evaluation that can be zero or not. The type is UInt8, Nullable(UInt8) or NULL.
--   `then` - La expresión que se va a devolver si se cumple la condición.
--   `else` - La expresión a devolver si no se cumple la condición.-
-
-**Valores devueltos**
-
-La función se ejecuta `then` y `else` expresiones y devuelve su resultado, dependiendo de si la condición `cond` terminó siendo cero o no.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT if(1, plus(2, 2), plus(2, 6))
-```
-
-Resultado:
-
-``` text
-┌─plus(2, 2)─┐
-│          4 │
-└────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT if(0, plus(2, 2), plus(2, 6))
-```
-
-Resultado:
-
-``` text
-┌─plus(2, 6)─┐
-│          8 │
-└────────────┘
-```
-
--   `then` y `else` debe tener el tipo común más bajo.
-
-**Ejemplo:**
-
-Toma esto `LEFT_RIGHT` tabla:
-
-``` sql
-SELECT *
-FROM LEFT_RIGHT
-
-┌─left─┬─right─┐
-│ ᴺᵁᴸᴸ │     4 │
-│    1 │     3 │
-│    2 │     2 │
-│    3 │     1 │
-│    4 │  ᴺᵁᴸᴸ │
-└──────┴───────┘
-```
-
-La siguiente consulta compara `left` y `right` valor:
-
-``` sql
-SELECT
-    left,
-    right,
-    if(left < right, 'left is smaller than right', 'right is greater or equal than left') AS is_smaller
-FROM LEFT_RIGHT
-WHERE isNotNull(left) AND isNotNull(right)
-
-┌─left─┬─right─┬─is_smaller──────────────────────────┐
-│    1 │     3 │ left is smaller than right          │
-│    2 │     2 │ right is greater or equal than left │
-│    3 │     1 │ right is greater or equal than left │
-└──────┴───────┴─────────────────────────────────────┘
-```
-
-Nota: `NULL` los valores no se utilizan en este ejemplo, compruebe [Valores NULL en condicionales](#null-values-in-conditionals) apartado.
-
-## Operador ternario {#ternary-operator}
-
-Funciona igual que `if` función.
-
-Sintaxis: `cond ? then : else`
-
-Devoluciones `then` si el `cond` evalúa que es verdadero (mayor que cero), de lo contrario devuelve `else`.
-
--   `cond` debe ser de tipo de `UInt8`, y `then` y `else` debe tener el tipo común más bajo.
-
--   `then` y `else` puede ser `NULL`
-
-**Ver también**
-
--   [ifNotFinite](other-functions.md#ifnotfinite).
-
-## MultiIf {#multiif}
-
-Le permite escribir el [CASE](../operators/index.md#operator_case) operador más compacto en la consulta.
-
-Sintaxis: `multiIf(cond_1, then_1, cond_2, then_2, ..., else)`
-
-**Parámetros:**
-
--   `cond_N` — The condition for the function to return `then_N`.
--   `then_N` — The result of the function when executed.
--   `else` — The result of the function if none of the conditions is met.
-
-La función acepta `2N+1` parámetros.
-
-**Valores devueltos**
-
-La función devuelve uno de los valores `then_N` o `else` dependiendo de las condiciones `cond_N`.
-
-**Ejemplo**
-
-De nuevo usando `LEFT_RIGHT` tabla.
-
-``` sql
-SELECT
-    left,
-    right,
-    multiIf(left < right, 'left is smaller', left > right, 'left is greater', left = right, 'Both equal', 'Null value') AS result
-FROM LEFT_RIGHT
-
-┌─left─┬─right─┬─result──────────┐
-│ ᴺᵁᴸᴸ │     4 │ Null value      │
-│    1 │     3 │ left is smaller │
-│    2 │     2 │ Both equal      │
-│    3 │     1 │ left is greater │
-│    4 │  ᴺᵁᴸᴸ │ Null value      │
-└──────┴───────┴─────────────────┘
-```
-
-## Uso directo de resultados condicionales {#using-conditional-results-directly}
-
-Los condicionales siempre dan como resultado `0`, `1` o `NULL`. Entonces puedes usar resultados condicionales directamente como este:
-
-``` sql
-SELECT left < right AS is_small
-FROM LEFT_RIGHT
-
-┌─is_small─┐
-│     ᴺᵁᴸᴸ │
-│        1 │
-│        0 │
-│        0 │
-│     ᴺᵁᴸᴸ │
-└──────────┘
-```
-
-## Valores NULL en condicionales {#null-values-in-conditionals}
-
-Cuando `NULL` están involucrados en condicionales, el resultado también será `NULL`.
-
-``` sql
-SELECT
-    NULL < 1,
-    2 < NULL,
-    NULL < NULL,
-    NULL = NULL
-
-┌─less(NULL, 1)─┬─less(2, NULL)─┬─less(NULL, NULL)─┬─equals(NULL, NULL)─┐
-│ ᴺᵁᴸᴸ          │ ᴺᵁᴸᴸ          │ ᴺᵁᴸᴸ             │ ᴺᵁᴸᴸ               │
-└───────────────┴───────────────┴──────────────────┴────────────────────┘
-```
-
-Por lo tanto, debe construir sus consultas cuidadosamente si los tipos son `Nullable`.
-
-El siguiente ejemplo demuestra esto al no agregar la condición equals a `multiIf`.
-
-``` sql
-SELECT
-    left,
-    right,
-    multiIf(left < right, 'left is smaller', left > right, 'right is smaller', 'Both equal') AS faulty_result
-FROM LEFT_RIGHT
-
-┌─left─┬─right─┬─faulty_result────┐
-│ ᴺᵁᴸᴸ │     4 │ Both equal       │
-│    1 │     3 │ left is smaller  │
-│    2 │     2 │ Both equal       │
-│    3 │     1 │ right is smaller │
-│    4 │  ᴺᵁᴸᴸ │ Both equal       │
-└──────┴───────┴──────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/conditional_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/date-time-functions.md b/docs/es/sql-reference/functions/date-time-functions.md
deleted file mode 100644
index f80805d05580..000000000000
--- a/docs/es/sql-reference/functions/date-time-functions.md
+++ /dev/null
@@ -1,450 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: Trabajar con fechas y horas
----
-
-# Funciones para trabajar con fechas y horas {#functions-for-working-with-dates-and-times}
-
-Soporte para zonas horarias
-
-Todas las funciones para trabajar con la fecha y la hora que tienen un uso lógico para la zona horaria pueden aceptar un segundo argumento de zona horaria opcional. Ejemplo: Asia/Ekaterimburgo. En este caso, utilizan la zona horaria especificada en lugar de la local (predeterminada).
-
-``` sql
-SELECT
-    toDateTime('2016-06-15 23:00:00') AS time,
-    toDate(time) AS date_local,
-    toDate(time, 'Asia/Yekaterinburg') AS date_yekat,
-    toString(time, 'US/Samoa') AS time_samoa
-```
-
-``` text
-┌────────────────time─┬─date_local─┬─date_yekat─┬─time_samoa──────────┐
-│ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-16 │ 2016-06-15 09:00:00 │
-└─────────────────────┴────────────┴────────────┴─────────────────────┘
-```
-
-Solo se admiten las zonas horarias que difieren de UTC por un número entero de horas.
-
-## Todos los derechos reservados {#totimezone}
-
-Convierta la hora o la fecha y la hora a la zona horaria especificada.
-
-## paraAño {#toyear}
-
-Convierte una fecha o fecha con hora en un número UInt16 que contiene el número de año (AD).
-
-## aTrimestre {#toquarter}
-
-Convierte una fecha o fecha con hora en un número UInt8 que contiene el número de trimestre.
-
-## ParaMes {#tomonth}
-
-Convierte una fecha o fecha con hora en un número UInt8 que contiene el número de mes (1-12).
-
-## Todos los derechos reservados {#todayofyear}
-
-Convierte una fecha o fecha con hora en un número UInt16 que contiene el número del día del año (1-366).
-
-## Todos los derechos reservados {#todayofmonth}
-
-Convierte una fecha o fecha con hora en un número UInt8 que contiene el número del día del mes (1-31).
-
-## Todos los derechos reservados {#todayofweek}
-
-Convierte una fecha o fecha con hora en un número UInt8 que contiene el número del día de la semana (el lunes es 1 y el domingo es 7).
-
-## ParaHora {#tohour}
-
-Convierte una fecha con hora en un número UInt8 que contiene el número de la hora en el tiempo de 24 horas (0-23).
-This function assumes that if clocks are moved ahead, it is by one hour and occurs at 2 a.m., and if clocks are moved back, it is by one hour and occurs at 3 a.m. (which is not always true – even in Moscow the clocks were twice changed at a different time).
-
-## ToMinute {#tominute}
-
-Convierte una fecha con hora en un número UInt8 que contiene el número del minuto de la hora (0-59).
-
-## aSegundo {#tosecond}
-
-Convierte una fecha con hora en un número UInt8 que contiene el número del segundo en el minuto (0-59).
-Los segundos de salto no se contabilizan.
-
-## Todos los derechos reservados {#to-unix-timestamp}
-
-Para el argumento DateTime: convierte el valor a su representación numérica interna (Unix Timestamp).
-Para el argumento String: analice la fecha y hora de la cadena de acuerdo con la zona horaria (segundo argumento opcional, la zona horaria del servidor se usa de forma predeterminada) y devuelve la marca de tiempo de Unix correspondiente.
-Para el argumento Date : el comportamiento no está especificado.
-
-**Sintaxis**
-
-``` sql
-toUnixTimestamp(datetime)
-toUnixTimestamp(str, [timezone])
-```
-
-**Valor devuelto**
-
--   Devuelve la marca de tiempo de Unix.
-
-Tipo: `UInt32`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT toUnixTimestamp('2017-11-05 08:07:47', 'Asia/Tokyo') AS unix_timestamp
-```
-
-Resultado:
-
-``` text
-┌─unix_timestamp─┐
-│     1509836867 │
-└────────────────┘
-```
-
-## Todos los derechos reservados {#tostartofyear}
-
-Redondea una fecha o fecha con la hora hasta el primer día del año.
-Devuelve la fecha.
-
-## Todos los derechos reservados {#tostartofisoyear}
-
-Redondea una fecha o fecha con la hora hasta el primer día del año ISO.
-Devuelve la fecha.
-
-## Todos los derechos reservados {#tostartofquarter}
-
-Redondea una fecha o fecha con la hora hasta el primer día del trimestre.
-El primer día del trimestre es el 1 de enero, el 1 de abril, el 1 de julio o el 1 de octubre.
-Devuelve la fecha.
-
-## Todos los derechos reservados {#tostartofmonth}
-
-Redondea una fecha o fecha con la hora hasta el primer día del mes.
-Devuelve la fecha.
-
-!!! attention "Atención"
-    El comportamiento de analizar fechas incorrectas es específico de la implementación. ClickHouse puede devolver una fecha cero, lanzar una excepción o hacer “natural” desbordamiento.
-
-## paraLunes {#tomonday}
-
-Redondea una fecha o fecha con hora al lunes más cercano.
-Devuelve la fecha.
-
-## ¿Cómo puedo hacerlo?\]) {#tostartofweektmode}
-
-Redondea una fecha o fecha con hora al domingo o lunes más cercano por modo.
-Devuelve la fecha.
-El argumento mode funciona exactamente igual que el argumento mode a toWeek() . Para la sintaxis de argumento único, se utiliza un valor de modo de 0.
-
-## Todos los derechos reservados {#tostartofday}
-
-Redondea una fecha con el tiempo hasta el comienzo del día.
-
-## Todos los derechos reservados {#tostartofhour}
-
-Redondea una fecha con el tiempo hasta el comienzo de la hora.
-
-## Todos los derechos reservados {#tostartofminute}
-
-Redondea una fecha con el tiempo hasta el inicio del minuto.
-
-## Acerca de nosotros {#tostartoffiveminute}
-
-Redondea una fecha con el tiempo hasta el inicio del intervalo de cinco minutos.
-
-## Acerca de Nosotros {#tostartoftenminutes}
-
-Redondea una fecha con el tiempo hasta el inicio del intervalo de diez minutos.
-
-## Para comenzar de quince minutos {#tostartoffifteenminutes}
-
-Redondea la fecha con el tiempo hasta el inicio del intervalo de quince minutos.
-
-## También puede usar una interfaz de usuario.\]) {#tostartofintervaltime-or-data-interval-x-unit-time-zone}
-
-Esta es una generalización de otras funciones llamadas `toStartOf*`. Por ejemplo,
-`toStartOfInterval(t, INTERVAL 1 year)` devuelve lo mismo que `toStartOfYear(t)`,
-`toStartOfInterval(t, INTERVAL 1 month)` devuelve lo mismo que `toStartOfMonth(t)`,
-`toStartOfInterval(t, INTERVAL 1 day)` devuelve lo mismo que `toStartOfDay(t)`,
-`toStartOfInterval(t, INTERVAL 15 minute)` devuelve lo mismo que `toStartOfFifteenMinutes(t)` sucesivamente.
-
-## Tiempo {#totime}
-
-Convierte una fecha con hora en una fecha fija determinada, preservando al mismo tiempo la hora.
-
-## Todos los derechos reservados {#torelativeyearnum}
-
-Convierte una fecha con hora o fecha en el número del año, a partir de un determinado punto fijo en el pasado.
-
-## Nombre de la red inalámbrica (SSID): {#torelativequarternum}
-
-Convierte una fecha con hora o fecha en el número del trimestre, a partir de un determinado punto fijo en el pasado.
-
-## Nombre de la red inalámbrica (SSID): {#torelativemonthnum}
-
-Convierte una fecha con hora o fecha en el número del mes, a partir de un determinado punto fijo en el pasado.
-
-## Nombre de la red inalámbrica (SSID): {#torelativeweeknum}
-
-Convierte una fecha con hora o fecha en el número de la semana, a partir de un determinado punto fijo en el pasado.
-
-## Nombre de la red inalámbrica (SSID): {#torelativedaynum}
-
-Convierte una fecha con hora o fecha en el número del día, a partir de un determinado punto fijo en el pasado.
-
-## Nombre de la red inalámbrica (SSID): {#torelativehournum}
-
-Convierte una fecha con hora o fecha en el número de la hora, a partir de un determinado punto fijo en el pasado.
-
-## Todos los derechos reservados {#torelativeminutenum}
-
-Convierte una fecha con hora o fecha en el número del minuto, a partir de un cierto punto fijo en el pasado.
-
-## Todos los derechos reservados {#torelativesecondnum}
-
-Convierte una fecha con hora o fecha en el número de la segunda, a partir de un cierto punto fijo en el pasado.
-
-## AISOAño {#toisoyear}
-
-Convierte una fecha o fecha con hora en un número UInt16 que contiene el número ISO Year.
-
-## paraISOWeek {#toisoweek}
-
-Convierte una fecha o fecha con hora en un número UInt8 que contiene el número de semana ISO.
-
-## ToWeek(fecha\[,modo\]) {#toweekdatemode}
-
-Esta función devuelve el número de semana para la fecha o la fecha y hora. La forma de dos argumentos de toWeek() le permite especificar si la semana comienza el domingo o el lunes y si el valor de retorno debe estar en el rango de 0 a 53 o de 1 a 53. Si se omite el argumento mode, el modo predeterminado es 0.
-`toISOWeek()`es una función de compatibilidad que es equivalente a `toWeek(date,3)`.
-La siguiente tabla describe cómo funciona el argumento mode.
-
-| Modo | Primer día de la semana | Gama | Week 1 is the first week … |
-|------|-------------------------|------|----------------------------|
-| 0    | Domingo                 | 0-53 | con un domingo de este año |
-| 1    | Lunes                   | 0-53 | con 4 o más días este año  |
-| 2    | Domingo                 | 1-53 | con un domingo de este año |
-| 3    | Lunes                   | 1-53 | con 4 o más días este año  |
-| 4    | Domingo                 | 0-53 | con 4 o más días este año  |
-| 5    | Lunes                   | 0-53 | con un lunes de este año   |
-| 6    | Domingo                 | 1-53 | con 4 o más días este año  |
-| 7    | Lunes                   | 1-53 | con un lunes de este año   |
-| 8    | Domingo                 | 1-53 | contiene 1 de enero        |
-| 9    | Lunes                   | 1-53 | contiene 1 de enero        |
-
-Para valores de modo con un significado de “with 4 or more days this year,” semanas están numeradas según ISO 8601:1988:
-
--   Si la semana que contiene el 1 de enero tiene 4 o más días en el nuevo año, es la semana 1.
-
--   De lo contrario, es la última semana del año anterior, y la semana siguiente es la semana 1.
-
-Para valores de modo con un significado de “contains January 1”, la semana contiene 1 de enero es la semana 1. No importa cuántos días en el nuevo año contenía la semana, incluso si contenía solo un día.
-
-``` sql
-toWeek(date, [, mode][, Timezone])
-```
-
-**Parámetros**
-
--   `date` – Date or DateTime.
--   `mode` – Optional parameter, Range of values is \[0,9\], default is 0.
--   `Timezone` – Optional parameter, it behaves like any other conversion function.
-
-**Ejemplo**
-
-``` sql
-SELECT toDate('2016-12-27') AS date, toWeek(date) AS week0, toWeek(date,1) AS week1, toWeek(date,9) AS week9;
-```
-
-``` text
-┌───────date─┬─week0─┬─week1─┬─week9─┐
-│ 2016-12-27 │    52 │    52 │     1 │
-└────────────┴───────┴───────┴───────┘
-```
-
-## aYearWeek(fecha\[,modo\]) {#toyearweekdatemode}
-
-Devuelve año y semana para una fecha. El año en el resultado puede ser diferente del año en el argumento de fecha para la primera y la última semana del año.
-
-El argumento mode funciona exactamente igual que el argumento mode a toWeek() . Para la sintaxis de argumento único, se utiliza un valor de modo de 0.
-
-`toISOYear()`es una función de compatibilidad que es equivalente a `intDiv(toYearWeek(date,3),100)`.
-
-**Ejemplo**
-
-``` sql
-SELECT toDate('2016-12-27') AS date, toYearWeek(date) AS yearWeek0, toYearWeek(date,1) AS yearWeek1, toYearWeek(date,9) AS yearWeek9;
-```
-
-``` text
-┌───────date─┬─yearWeek0─┬─yearWeek1─┬─yearWeek9─┐
-│ 2016-12-27 │    201652 │    201652 │    201701 │
-└────────────┴───────────┴───────────┴───────────┘
-```
-
-## ahora {#now}
-
-Acepta cero argumentos y devuelve la hora actual en uno de los momentos de ejecución de la solicitud.
-Esta función devuelve una constante, incluso si la solicitud tardó mucho en completarse.
-
-## hoy {#today}
-
-Acepta cero argumentos y devuelve la fecha actual en uno de los momentos de ejecución de la solicitud.
-Lo mismo que ‘toDate(now())’.
-
-## ayer {#yesterday}
-
-Acepta cero argumentos y devuelve la fecha de ayer en uno de los momentos de ejecución de la solicitud.
-Lo mismo que ‘today() - 1’.
-
-## timeSlot {#timeslot}
-
-Redondea el tiempo a la media hora.
-Esta función es específica de Yandex.Metrica, ya que media hora es la cantidad mínima de tiempo para dividir una sesión en dos sesiones si una etiqueta de seguimiento muestra las páginas vistas consecutivas de un solo usuario que difieren en el tiempo en estrictamente más de esta cantidad. Esto significa que las tuplas (el ID de etiqueta, el ID de usuario y el intervalo de tiempo) se pueden usar para buscar páginas vistas que se incluyen en la sesión correspondiente.
-
-## paraYYYYMM {#toyyyymm}
-
-Convierte una fecha o fecha con hora en un número UInt32 que contiene el número de año y mes (YYYY \* 100 + MM).
-
-## paraYYYYMMDD {#toyyyymmdd}
-
-Convierte una fecha o fecha con hora en un número UInt32 que contiene el número de año y mes (AAAA \* 10000 + MM \* 100 + DD).
-
-## paraYYYYMMDDhhmmss {#toyyyymmddhhmmss}
-
-Convierte una fecha o fecha con hora en un número UInt64 que contiene el número de año y mes (YYYY \* 10000000000 + MM \* 100000000 + DD \* 1000000 + hh \* 10000 + mm \* 100 + ss).
-
-## Por ejemplo, en el caso de que el usuario pueda acceder a la página de inicio de sesión, seleccione la página de inicio de sesión {#addyears-addmonths-addweeks-adddays-addhours-addminutes-addseconds-addquarters}
-
-La función agrega un intervalo de fecha / fecha y hora a una fecha / fecha y hora y luego devuelve la fecha / fecha y hora. Por ejemplo:
-
-``` sql
-WITH
-    toDate('2018-01-01') AS date,
-    toDateTime('2018-01-01 00:00:00') AS date_time
-SELECT
-    addYears(date, 1) AS add_years_with_date,
-    addYears(date_time, 1) AS add_years_with_date_time
-```
-
-``` text
-┌─add_years_with_date─┬─add_years_with_date_time─┐
-│          2019-01-01 │      2019-01-01 00:00:00 │
-└─────────────────────┴──────────────────────────┘
-```
-
-## restarAños, restarMeses, restarSemanas, restarDías, restarHoras, restarMinutos, restarSegundos, restarCuartos {#subtractyears-subtractmonths-subtractweeks-subtractdays-subtracthours-subtractminutes-subtractseconds-subtractquarters}
-
-La función resta un intervalo de fecha / fecha y hora a una fecha / fecha y hora y luego devuelve la fecha / fecha y hora. Por ejemplo:
-
-``` sql
-WITH
-    toDate('2019-01-01') AS date,
-    toDateTime('2019-01-01 00:00:00') AS date_time
-SELECT
-    subtractYears(date, 1) AS subtract_years_with_date,
-    subtractYears(date_time, 1) AS subtract_years_with_date_time
-```
-
-``` text
-┌─subtract_years_with_date─┬─subtract_years_with_date_time─┐
-│               2018-01-01 │           2018-01-01 00:00:00 │
-└──────────────────────────┴───────────────────────────────┘
-```
-
-## dateDiff {#datediff}
-
-Devuelve la diferencia entre dos valores Date o DateTime.
-
-**Sintaxis**
-
-``` sql
-dateDiff('unit', startdate, enddate, [timezone])
-```
-
-**Parámetros**
-
--   `unit` — Time unit, in which the returned value is expressed. [Cadena](../syntax.md#syntax-string-literal).
-
-        Supported values:
-
-        | unit   |
-        | ---- |
-        |second  |
-        |minute  |
-        |hour    |
-        |day     |
-        |week    |
-        |month   |
-        |quarter |
-        |year    |
-
--   `startdate` — The first time value to compare. [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
-
--   `enddate` — The second time value to compare. [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
-
--   `timezone` — Optional parameter. If specified, it is applied to both `startdate` y `enddate`. Si no se especifica, las zonas horarias `startdate` y `enddate` se utilizan. Si no son lo mismo, el resultado no está especificado.
-
-**Valor devuelto**
-
-Diferencia entre `startdate` y `enddate` expresado en `unit`.
-
-Tipo: `int`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'));
-```
-
-Resultado:
-
-``` text
-┌─dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'))─┐
-│                                                                                     25 │
-└────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-## timeSlots(Hora de inicio, Duración, \[, Tamaño\]) {#timeslotsstarttime-duration-size}
-
-Para un intervalo de tiempo a partir de ‘StartTime’ y continuando por ‘Duration’ segundos, devuelve una matriz de momentos en el tiempo, que consiste en puntos de este intervalo ‘Size’ en segundos. ‘Size’ es un parámetro opcional: una constante UInt32, establecida en 1800 por defecto.
-Por ejemplo, `timeSlots(toDateTime('2012-01-01 12:20:00'), 600) = [toDateTime('2012-01-01 12:00:00'), toDateTime('2012-01-01 12:30:00')]`.
-Esto es necesario para buscar páginas vistas en la sesión correspondiente.
-
-## formatDateTime(Hora, Formato\[, Zona horaria\]) {#formatdatetime}
-
-Function formats a Time according given Format string. N.B.: Format is a constant expression, e.g. you can not have multiple formats for single result column.
-
-Modificadores compatibles para Formato:
-(“Example” columna muestra el resultado de formateo para el tiempo `2018-01-02 22:33:44`)
-
-| Modificador     | Descripci                                                        | Ejemplo    |
-|-----------------|------------------------------------------------------------------|------------|
-| %C              | año dividido por 100 y truncado a entero (00-99)                 | 20         |
-| %d              | día del mes, cero acolchado (01-31)                              | 02         |
-| %D              | Fecha corta de MM/DD/YY, equivalente a %m/%d/%y                  | 01/02/18   |
-| %e              | día del mes, espacio acolchado ( 1-31)                           | 2          |
-| %F              | fecha corta AAAA-MM-DD, equivalente a %Y-%m-%d                   | 2018-01-02 |
-| %H              | hora en formato 24h (00-23)                                      | 22         |
-| %I              | hora en formato 12h (01-12)                                      | 10         |
-| %j              | día del año (001-366)                                            | 002        |
-| Más información | mes como un número decimal (01-12)                               | 01         |
-| %M              | minutos (00-59)                                                  | 33         |
-| %y              | carácter de nueva línea ('')                                     |            |
-| %p              | Designación AM o PM                                              | PM         |
-| %R              | HH de 24 horas: Tiempo del milímetro, equivalente a %H: %M       | 22:33      |
-| %S              | segundo (00-59)                                                  | 44         |
-| % t             | carácter de pestaña horizontal (')                               |            |
-| %T              | Formato de hora ISO 8601 (HH:MM:SS), equivalente a %H:%M:%S      | 22:33:44   |
-| %u              | ISO 8601 día de la semana como número con el lunes como 1 (1-7)  | 2          |
-| %V              | Número de semana ISO 8601 (01-53)                                | 01         |
-| % w             | día de la semana como un número decimal con domingo como 0 (0-6) | 2          |
-| %y              | Año, últimos dos dígitos (00-99)                                 | 18         |
-| %Y              | Año                                                              | 2018       |
-| %%              | signo                                                            | %          |
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/date_time_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/encoding-functions.md b/docs/es/sql-reference/functions/encoding-functions.md
deleted file mode 100644
index d3bbd68f3355..000000000000
--- a/docs/es/sql-reference/functions/encoding-functions.md
+++ /dev/null
@@ -1,175 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 52
-toc_title: "Codificaci\xF3n"
----
-
-# Funciones de codificación {#encoding-functions}
-
-## char {#char}
-
-Devuelve la cadena con la longitud como el número de argumentos pasados y cada byte tiene el valor del argumento correspondiente. Acepta varios argumentos de tipos numéricos. Si el valor del argumento está fuera del rango del tipo de datos UInt8, se convierte a UInt8 con posible redondeo y desbordamiento.
-
-**Sintaxis**
-
-``` sql
-char(number_1, [number_2, ..., number_n]);
-```
-
-**Parámetros**
-
--   `number_1, number_2, ..., number_n` — Numerical arguments interpreted as integers. Types: [En](../../sql-reference/data-types/int-uint.md), [Flotante](../../sql-reference/data-types/float.md).
-
-**Valor devuelto**
-
--   una cadena de bytes.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT char(104.1, 101, 108.9, 108.9, 111) AS hello
-```
-
-Resultado:
-
-``` text
-┌─hello─┐
-│ hello │
-└───────┘
-```
-
-Puede construir una cadena de codificación arbitraria pasando los bytes correspondientes. Aquí hay un ejemplo para UTF-8:
-
-Consulta:
-
-``` sql
-SELECT char(0xD0, 0xBF, 0xD1, 0x80, 0xD0, 0xB8, 0xD0, 0xB2, 0xD0, 0xB5, 0xD1, 0x82) AS hello;
-```
-
-Resultado:
-
-``` text
-┌─hello──┐
-│ привет │
-└────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT char(0xE4, 0xBD, 0xA0, 0xE5, 0xA5, 0xBD) AS hello;
-```
-
-Resultado:
-
-``` text
-┌─hello─┐
-│ 你好  │
-└───────┘
-```
-
-## hexagonal {#hex}
-
-Devuelve una cadena que contiene la representación hexadecimal del argumento.
-
-**Sintaxis**
-
-``` sql
-hex(arg)
-```
-
-La función está usando letras mayúsculas `A-F` y no usar ningún prefijo (como `0x`) o sufijos (como `h`).
-
-Para argumentos enteros, imprime dígitos hexadecimales (“nibbles”) del más significativo al menos significativo (big endian o “human readable” orden). Comienza con el byte distinto de cero más significativo (se omiten los cero bytes principales) pero siempre imprime ambos dígitos de cada byte incluso si el dígito inicial es cero.
-
-Ejemplo:
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT hex(1);
-```
-
-Resultado:
-
-``` text
-01
-```
-
-Valores de tipo `Date` y `DateTime` están formateados como enteros correspondientes (el número de días desde Epoch para Date y el valor de Unix Timestamp para DateTime).
-
-Para `String` y `FixedString`, todos los bytes son simplemente codificados como dos números hexadecimales. No se omiten cero bytes.
-
-Los valores de los tipos de coma flotante y Decimal se codifican como su representación en la memoria. Como apoyamos la pequeña arquitectura endian, están codificados en little endian. No se omiten cero bytes iniciales / finales.
-
-**Parámetros**
-
--   `arg` — A value to convert to hexadecimal. Types: [Cadena](../../sql-reference/data-types/string.md), [UInt](../../sql-reference/data-types/int-uint.md), [Flotante](../../sql-reference/data-types/float.md), [Decimal](../../sql-reference/data-types/decimal.md), [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md).
-
-**Valor devuelto**
-
--   Una cadena con la representación hexadecimal del argumento.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT hex(toFloat32(number)) as hex_presentation FROM numbers(15, 2);
-```
-
-Resultado:
-
-``` text
-┌─hex_presentation─┐
-│ 00007041         │
-│ 00008041         │
-└──────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT hex(toFloat64(number)) as hex_presentation FROM numbers(15, 2);
-```
-
-Resultado:
-
-``` text
-┌─hex_presentation─┐
-│ 0000000000002E40 │
-│ 0000000000003040 │
-└──────────────────┘
-```
-
-## unhex(str) {#unhexstr}
-
-Acepta una cadena que contiene cualquier número de dígitos hexadecimales y devuelve una cadena que contiene los bytes correspondientes. Admite letras mayúsculas y minúsculas A-F. El número de dígitos hexadecimales no tiene que ser par. Si es impar, el último dígito se interpreta como la mitad menos significativa del byte 00-0F. Si la cadena de argumento contiene algo que no sean dígitos hexadecimales, se devuelve algún resultado definido por la implementación (no se produce una excepción).
-Si desea convertir el resultado en un número, puede usar el ‘reverse’ y ‘reinterpretAsType’ función.
-
-## UUIDStringToNum (str) {#uuidstringtonumstr}
-
-Acepta una cadena que contiene 36 caracteres en el formato `123e4567-e89b-12d3-a456-426655440000`, y lo devuelve como un conjunto de bytes en un FixedString(16).
-
-## UUIDNumToString (str) {#uuidnumtostringstr}
-
-Acepta un valor de FixedString(16). Devuelve una cadena que contiene 36 caracteres en formato de texto.
-
-## ¿Cómo puedo hacerlo?) {#bitmasktolistnum}
-
-Acepta un entero. Devuelve una cadena que contiene la lista de potencias de dos que suman el número de origen cuando se suma. Están separados por comas sin espacios en formato de texto, en orden ascendente.
-
-## ¿Qué puedes encontrar en Neodigit) {#bitmasktoarraynum}
-
-Acepta un entero. Devuelve una matriz de números UInt64 que contiene la lista de potencias de dos que suman el número de origen cuando se suma. Los números en la matriz están en orden ascendente.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/encoding_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/ext-dict-functions.md b/docs/es/sql-reference/functions/ext-dict-functions.md
deleted file mode 100644
index 329e0b75664d..000000000000
--- a/docs/es/sql-reference/functions/ext-dict-functions.md
+++ /dev/null
@@ -1,205 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: Trabajar con diccionarios externos
----
-
-# Funciones para trabajar con diccionarios externos {#ext_dict_functions}
-
-Para obtener información sobre cómo conectar y configurar diccionarios externos, consulte [Diccionarios externos](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md).
-
-## dictGet {#dictget}
-
-Recupera un valor de un diccionario externo.
-
-``` sql
-dictGet('dict_name', 'attr_name', id_expr)
-dictGetOrDefault('dict_name', 'attr_name', id_expr, default_value_expr)
-```
-
-**Parámetros**
-
--   `dict_name` — Name of the dictionary. [Literal de cadena](../syntax.md#syntax-string-literal).
--   `attr_name` — Name of the column of the dictionary. [Literal de cadena](../syntax.md#syntax-string-literal).
--   `id_expr` — Key value. [Expresion](../syntax.md#syntax-expressions) devolviendo un [UInt64](../../sql-reference/data-types/int-uint.md) o [Tupla](../../sql-reference/data-types/tuple.md)valor -type dependiendo de la configuración del diccionario.
--   `default_value_expr` — Value returned if the dictionary doesn't contain a row with the `id_expr` clave. [Expresion](../syntax.md#syntax-expressions) devolviendo el valor en el tipo de datos configurado para `attr_name` atributo.
-
-**Valor devuelto**
-
--   Si ClickHouse analiza el atributo correctamente en el [tipo de datos del atributo](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes), funciones devuelven el valor del atributo de diccionario que corresponde a `id_expr`.
-
--   Si no hay la clave, correspondiente a `id_expr` en el diccionario, entonces:
-
-        - `dictGet` returns the content of the `<null_value>` element specified for the attribute in the dictionary configuration.
-        - `dictGetOrDefault` returns the value passed as the `default_value_expr` parameter.
-
-ClickHouse produce una excepción si no puede analizar el valor del atributo o si el valor no coincide con el tipo de datos del atributo.
-
-**Ejemplo**
-
-Crear un archivo de texto `ext-dict-text.csv` que contiene los siguientes:
-
-``` text
-1,1
-2,2
-```
-
-La primera columna es `id` la segunda columna es `c1`.
-
-Configurar el diccionario externo:
-
-``` xml
-<yandex>
-    <dictionary>
-        <name>ext-dict-test</name>
-        <source>
-            <file>
-                <path>/path-to/ext-dict-test.csv</path>
-                <format>CSV</format>
-            </file>
-        </source>
-        <layout>
-            <flat />
-        </layout>
-        <structure>
-            <id>
-                <name>id</name>
-            </id>
-            <attribute>
-                <name>c1</name>
-                <type>UInt32</type>
-                <null_value></null_value>
-            </attribute>
-        </structure>
-        <lifetime>0</lifetime>
-    </dictionary>
-</yandex>
-```
-
-Realizar la consulta:
-
-``` sql
-SELECT
-    dictGetOrDefault('ext-dict-test', 'c1', number + 1, toUInt32(number * 10)) AS val,
-    toTypeName(val) AS type
-FROM system.numbers
-LIMIT 3
-```
-
-``` text
-┌─val─┬─type───┐
-│   1 │ UInt32 │
-│   2 │ UInt32 │
-│  20 │ UInt32 │
-└─────┴────────┘
-```
-
-**Ver también**
-
--   [Diccionarios externos](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md)
-
-## dictHas {#dicthas}
-
-Comprueba si hay una clave en un diccionario.
-
-``` sql
-dictHas('dict_name', id_expr)
-```
-
-**Parámetros**
-
--   `dict_name` — Name of the dictionary. [Literal de cadena](../syntax.md#syntax-string-literal).
--   `id_expr` — Key value. [Expresion](../syntax.md#syntax-expressions) devolviendo un [UInt64](../../sql-reference/data-types/int-uint.md)-tipo de valor.
-
-**Valor devuelto**
-
--   0, si no hay clave.
--   1, si hay una llave.
-
-Tipo: `UInt8`.
-
-## dictGetHierarchy {#dictgethierarchy}
-
-Crea una matriz, que contiene todos los padres de una clave [diccionario jerárquico](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md).
-
-**Sintaxis**
-
-``` sql
-dictGetHierarchy('dict_name', key)
-```
-
-**Parámetros**
-
--   `dict_name` — Name of the dictionary. [Literal de cadena](../syntax.md#syntax-string-literal).
--   `key` — Key value. [Expresion](../syntax.md#syntax-expressions) devolviendo un [UInt64](../../sql-reference/data-types/int-uint.md)-tipo de valor.
-
-**Valor devuelto**
-
--   Padres por la llave.
-
-Tipo: [Matriz (UInt64)](../../sql-reference/data-types/array.md).
-
-## DictIsIn {#dictisin}
-
-Comprueba el antecesor de una clave a través de toda la cadena jerárquica en el diccionario.
-
-``` sql
-dictIsIn('dict_name', child_id_expr, ancestor_id_expr)
-```
-
-**Parámetros**
-
--   `dict_name` — Name of the dictionary. [Literal de cadena](../syntax.md#syntax-string-literal).
--   `child_id_expr` — Key to be checked. [Expresion](../syntax.md#syntax-expressions) devolviendo un [UInt64](../../sql-reference/data-types/int-uint.md)-tipo de valor.
--   `ancestor_id_expr` — Alleged ancestor of the `child_id_expr` clave. [Expresion](../syntax.md#syntax-expressions) devolviendo un [UInt64](../../sql-reference/data-types/int-uint.md)-tipo de valor.
-
-**Valor devuelto**
-
--   0, si `child_id_expr` no es un niño de `ancestor_id_expr`.
--   1, si `child_id_expr` es un niño de `ancestor_id_expr` o si `child_id_expr` es una `ancestor_id_expr`.
-
-Tipo: `UInt8`.
-
-## Otras funciones {#ext_dict_functions-other}
-
-ClickHouse admite funciones especializadas que convierten los valores de atributo de diccionario a un tipo de datos específico, independientemente de la configuración del diccionario.
-
-Función:
-
--   `dictGetInt8`, `dictGetInt16`, `dictGetInt32`, `dictGetInt64`
--   `dictGetUInt8`, `dictGetUInt16`, `dictGetUInt32`, `dictGetUInt64`
--   `dictGetFloat32`, `dictGetFloat64`
--   `dictGetDate`
--   `dictGetDateTime`
--   `dictGetUUID`
--   `dictGetString`
-
-Todas estas funciones tienen el `OrDefault` modificación. Por ejemplo, `dictGetDateOrDefault`.
-
-Sintaxis:
-
-``` sql
-dictGet[Type]('dict_name', 'attr_name', id_expr)
-dictGet[Type]OrDefault('dict_name', 'attr_name', id_expr, default_value_expr)
-```
-
-**Parámetros**
-
--   `dict_name` — Name of the dictionary. [Literal de cadena](../syntax.md#syntax-string-literal).
--   `attr_name` — Name of the column of the dictionary. [Literal de cadena](../syntax.md#syntax-string-literal).
--   `id_expr` — Key value. [Expresion](../syntax.md#syntax-expressions) devolviendo un [UInt64](../../sql-reference/data-types/int-uint.md)-tipo de valor.
--   `default_value_expr` — Value which is returned if the dictionary doesn't contain a row with the `id_expr` clave. [Expresion](../syntax.md#syntax-expressions) devolviendo un valor en el tipo de datos configurado para `attr_name` atributo.
-
-**Valor devuelto**
-
--   Si ClickHouse analiza el atributo correctamente en el [tipo de datos del atributo](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes), funciones devuelven el valor del atributo de diccionario que corresponde a `id_expr`.
-
--   Si no se solicita `id_expr` en el diccionario entonces:
-
-        - `dictGet[Type]` returns the content of the `<null_value>` element specified for the attribute in the dictionary configuration.
-        - `dictGet[Type]OrDefault` returns the value passed as the `default_value_expr` parameter.
-
-ClickHouse produce una excepción si no puede analizar el valor del atributo o si el valor no coincide con el tipo de datos del atributo.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/ext_dict_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/functions-for-nulls.md b/docs/es/sql-reference/functions/functions-for-nulls.md
deleted file mode 100644
index e5b066d72288..000000000000
--- a/docs/es/sql-reference/functions/functions-for-nulls.md
+++ /dev/null
@@ -1,312 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 63
-toc_title: Trabajar con argumentos Nullable
----
-
-# Funciones para trabajar con agregados anulables {#functions-for-working-with-nullable-aggregates}
-
-## IsNull {#isnull}
-
-Comprueba si el argumento es [NULL](../../sql-reference/syntax.md#null-literal).
-
-``` sql
-isNull(x)
-```
-
-**Parámetros**
-
--   `x` — A value with a non-compound data type.
-
-**Valor devuelto**
-
--   `1` si `x` ser `NULL`.
--   `0` si `x` no es `NULL`.
-
-**Ejemplo**
-
-Tabla de entrada
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-Consulta
-
-``` sql
-SELECT x FROM t_null WHERE isNull(y)
-```
-
-``` text
-┌─x─┐
-│ 1 │
-└───┘
-```
-
-## isNotNull {#isnotnull}
-
-Comprueba si el argumento es [NULL](../../sql-reference/syntax.md#null-literal).
-
-``` sql
-isNotNull(x)
-```
-
-**Parámetros:**
-
--   `x` — A value with a non-compound data type.
-
-**Valor devuelto**
-
--   `0` si `x` ser `NULL`.
--   `1` si `x` no es `NULL`.
-
-**Ejemplo**
-
-Tabla de entrada
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-Consulta
-
-``` sql
-SELECT x FROM t_null WHERE isNotNull(y)
-```
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-## Coalesce {#coalesce}
-
-Comprueba de izquierda a derecha si `NULL` se aprobaron argumentos y devuelve el primer no-`NULL` argumento.
-
-``` sql
-coalesce(x,...)
-```
-
-**Parámetros:**
-
--   Cualquier número de parámetros de un tipo no compuesto. Todos los parámetros deben ser compatibles por tipo de datos.
-
-**Valores devueltos**
-
--   El primer no-`NULL` argumento.
--   `NULL` si todos los argumentos son `NULL`.
-
-**Ejemplo**
-
-Considere una lista de contactos que pueden especificar varias formas de contactar a un cliente.
-
-``` text
-┌─name─────┬─mail─┬─phone─────┬──icq─┐
-│ client 1 │ ᴺᵁᴸᴸ │ 123-45-67 │  123 │
-│ client 2 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ      │ ᴺᵁᴸᴸ │
-└──────────┴──────┴───────────┴──────┘
-```
-
-El `mail` y `phone` los campos son de tipo String, pero el `icq` campo `UInt32`, por lo que necesita ser convertido a `String`.
-
-Obtenga el primer método de contacto disponible para el cliente de la lista de contactos:
-
-``` sql
-SELECT coalesce(mail, phone, CAST(icq,'Nullable(String)')) FROM aBook
-```
-
-``` text
-┌─name─────┬─coalesce(mail, phone, CAST(icq, 'Nullable(String)'))─┐
-│ client 1 │ 123-45-67                                            │
-│ client 2 │ ᴺᵁᴸᴸ                                                 │
-└──────────┴──────────────────────────────────────────────────────┘
-```
-
-## ifNull {#ifnull}
-
-Devuelve un valor alternativo si el argumento principal es `NULL`.
-
-``` sql
-ifNull(x,alt)
-```
-
-**Parámetros:**
-
--   `x` — The value to check for `NULL`.
--   `alt` — The value that the function returns if `x` ser `NULL`.
-
-**Valores devueltos**
-
--   Valor `x`, si `x` no es `NULL`.
--   Valor `alt`, si `x` ser `NULL`.
-
-**Ejemplo**
-
-``` sql
-SELECT ifNull('a', 'b')
-```
-
-``` text
-┌─ifNull('a', 'b')─┐
-│ a                │
-└──────────────────┘
-```
-
-``` sql
-SELECT ifNull(NULL, 'b')
-```
-
-``` text
-┌─ifNull(NULL, 'b')─┐
-│ b                 │
-└───────────────────┘
-```
-
-## nullIf {#nullif}
-
-Devoluciones `NULL` si los argumentos son iguales.
-
-``` sql
-nullIf(x, y)
-```
-
-**Parámetros:**
-
-`x`, `y` — Values for comparison. They must be compatible types, or ClickHouse will generate an exception.
-
-**Valores devueltos**
-
--   `NULL` si los argumentos son iguales.
--   El `x` valor, si los argumentos no son iguales.
-
-**Ejemplo**
-
-``` sql
-SELECT nullIf(1, 1)
-```
-
-``` text
-┌─nullIf(1, 1)─┐
-│         ᴺᵁᴸᴸ │
-└──────────────┘
-```
-
-``` sql
-SELECT nullIf(1, 2)
-```
-
-``` text
-┌─nullIf(1, 2)─┐
-│            1 │
-└──────────────┘
-```
-
-## assumeNotNull {#assumenotnull}
-
-Resultados en un valor de tipo [NULL](../../sql-reference/data-types/nullable.md) para un no- `Nullable` si el valor no es `NULL`.
-
-``` sql
-assumeNotNull(x)
-```
-
-**Parámetros:**
-
--   `x` — The original value.
-
-**Valores devueltos**
-
--   El valor original del-`Nullable` tipo, si no es `NULL`.
--   El valor predeterminado para el-`Nullable` tipo si el valor original fue `NULL`.
-
-**Ejemplo**
-
-Considere el `t_null` tabla.
-
-``` sql
-SHOW CREATE TABLE t_null
-```
-
-``` text
-┌─statement─────────────────────────────────────────────────────────────────┐
-│ CREATE TABLE default.t_null ( x Int8,  y Nullable(Int8)) ENGINE = TinyLog │
-└───────────────────────────────────────────────────────────────────────────┘
-```
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-Aplicar el `assumeNotNull` función a la `y` columna.
-
-``` sql
-SELECT assumeNotNull(y) FROM t_null
-```
-
-``` text
-┌─assumeNotNull(y)─┐
-│                0 │
-│                3 │
-└──────────────────┘
-```
-
-``` sql
-SELECT toTypeName(assumeNotNull(y)) FROM t_null
-```
-
-``` text
-┌─toTypeName(assumeNotNull(y))─┐
-│ Int8                         │
-│ Int8                         │
-└──────────────────────────────┘
-```
-
-## Acerca de Nosotros {#tonullable}
-
-Convierte el tipo de argumento a `Nullable`.
-
-``` sql
-toNullable(x)
-```
-
-**Parámetros:**
-
--   `x` — The value of any non-compound type.
-
-**Valor devuelto**
-
--   El valor de entrada con un `Nullable` tipo.
-
-**Ejemplo**
-
-``` sql
-SELECT toTypeName(10)
-```
-
-``` text
-┌─toTypeName(10)─┐
-│ UInt8          │
-└────────────────┘
-```
-
-``` sql
-SELECT toTypeName(toNullable(10))
-```
-
-``` text
-┌─toTypeName(toNullable(10))─┐
-│ Nullable(UInt8)            │
-└────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/functions_for_nulls/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/geo.md b/docs/es/sql-reference/functions/geo.md
deleted file mode 100644
index b8e3a3b5ffd3..000000000000
--- a/docs/es/sql-reference/functions/geo.md
+++ /dev/null
@@ -1,510 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 62
-toc_title: "Trabajar con coordenadas geogr\xE1ficas"
----
-
-# Funciones para trabajar con coordenadas geográficas {#functions-for-working-with-geographical-coordinates}
-
-## GreatCircleDistance {#greatcircledistance}
-
-Calcule la distancia entre dos puntos en la superficie de la Tierra usando [la fórmula del gran círculo](https://en.wikipedia.org/wiki/Great-circle_distance).
-
-``` sql
-greatCircleDistance(lon1Deg, lat1Deg, lon2Deg, lat2Deg)
-```
-
-**Parámetros de entrada**
-
--   `lon1Deg` — Longitude of the first point in degrees. Range: `[-180°, 180°]`.
--   `lat1Deg` — Latitude of the first point in degrees. Range: `[-90°, 90°]`.
--   `lon2Deg` — Longitude of the second point in degrees. Range: `[-180°, 180°]`.
--   `lat2Deg` — Latitude of the second point in degrees. Range: `[-90°, 90°]`.
-
-Los valores positivos corresponden a latitud norte y longitud este, y los valores negativos corresponden a latitud sur y longitud oeste.
-
-**Valor devuelto**
-
-La distancia entre dos puntos en la superficie de la Tierra, en metros.
-
-Genera una excepción cuando los valores de los parámetros de entrada están fuera del intervalo.
-
-**Ejemplo**
-
-``` sql
-SELECT greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)
-```
-
-``` text
-┌─greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)─┐
-│                                                14132374.194975413 │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-## pointInEllipses {#pointinellipses}
-
-Comprueba si el punto pertenece al menos a una de las elipses.
-Las coordenadas son geométricas en el sistema de coordenadas cartesianas.
-
-``` sql
-pointInEllipses(x, y, x₀, y₀, a₀, b₀,...,xₙ, yₙ, aₙ, bₙ)
-```
-
-**Parámetros de entrada**
-
--   `x, y` — Coordinates of a point on the plane.
--   `xᵢ, yᵢ` — Coordinates of the center of the `i`-ésimo puntos suspensivos.
--   `aᵢ, bᵢ` — Axes of the `i`-ésimo puntos suspensivos en unidades de coordenadas x, y.
-
-Los parámetros de entrada deben ser `2+4⋅n`, donde `n` es el número de puntos suspensivos.
-
-**Valores devueltos**
-
-`1` si el punto está dentro de al menos una de las elipses; `0`si no lo es.
-
-**Ejemplo**
-
-``` sql
-SELECT pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)
-```
-
-``` text
-┌─pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)─┐
-│                                               1 │
-└─────────────────────────────────────────────────┘
-```
-
-## pointInPolygon {#pointinpolygon}
-
-Comprueba si el punto pertenece al polígono en el plano.
-
-``` sql
-pointInPolygon((x, y), [(a, b), (c, d) ...], ...)
-```
-
-**Valores de entrada**
-
--   `(x, y)` — Coordinates of a point on the plane. Data type — [Tupla](../../sql-reference/data-types/tuple.md) — A tuple of two numbers.
--   `[(a, b), (c, d) ...]` — Polygon vertices. Data type — [Matriz](../../sql-reference/data-types/array.md). Cada vértice está representado por un par de coordenadas `(a, b)`. Los vértices deben especificarse en sentido horario o antihorario. El número mínimo de vértices es 3. El polígono debe ser constante.
--   La función también admite polígonos con agujeros (secciones recortadas). En este caso, agregue polígonos que definan las secciones recortadas utilizando argumentos adicionales de la función. La función no admite polígonos no simplemente conectados.
-
-**Valores devueltos**
-
-`1` si el punto está dentro del polígono, `0` si no lo es.
-Si el punto está en el límite del polígono, la función puede devolver 0 o 1.
-
-**Ejemplo**
-
-``` sql
-SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res
-```
-
-``` text
-┌─res─┐
-│   1 │
-└─────┘
-```
-
-## geohashEncode {#geohashencode}
-
-Codifica la latitud y la longitud como una cadena geohash, consulte (http://geohash.org/, https://en.wikipedia.org/wiki/Geohash).
-
-``` sql
-geohashEncode(longitude, latitude, [precision])
-```
-
-**Valores de entrada**
-
--   longitud - longitud parte de la coordenada que desea codificar. Flotando en el rango`[-180°, 180°]`
--   latitude : parte de latitud de la coordenada que desea codificar. Flotando en el rango `[-90°, 90°]`
--   precision - Opcional, longitud de la cadena codificada resultante, por defecto es `12`. Entero en el rango `[1, 12]`. Cualquier valor menor que `1` o mayor que `12` se convierte silenciosamente a `12`.
-
-**Valores devueltos**
-
--   alfanumérico `String` de coordenadas codificadas (se utiliza la versión modificada del alfabeto de codificación base32).
-
-**Ejemplo**
-
-``` sql
-SELECT geohashEncode(-5.60302734375, 42.593994140625, 0) AS res
-```
-
-``` text
-┌─res──────────┐
-│ ezs42d000000 │
-└──────────────┘
-```
-
-## geohashDecode {#geohashdecode}
-
-Decodifica cualquier cadena codificada por geohash en longitud y latitud.
-
-**Valores de entrada**
-
--   encoded string - cadena codificada geohash.
-
-**Valores devueltos**
-
--   (longitud, latitud) - 2-tupla de `Float64` valores de longitud y latitud.
-
-**Ejemplo**
-
-``` sql
-SELECT geohashDecode('ezs42') AS res
-```
-
-``` text
-┌─res─────────────────────────────┐
-│ (-5.60302734375,42.60498046875) │
-└─────────────────────────────────┘
-```
-
-## geoToH3 {#geotoh3}
-
-Devoluciones [H3](https://uber.github.io/h3/#/documentation/overview/introduction) índice de punto `(lon, lat)` con la resolución especificada.
-
-[H3](https://uber.github.io/h3/#/documentation/overview/introduction) es un sistema de indexación geográfica donde la superficie de la Tierra se divide en incluso azulejos hexagonales. Este sistema es jerárquico, es decir, cada hexágono en el nivel superior se puede dividir en siete incluso pero más pequeños y así sucesivamente.
-
-Este índice se utiliza principalmente para ubicaciones de bucketing y otras manipulaciones geoespaciales.
-
-**Sintaxis**
-
-``` sql
-geoToH3(lon, lat, resolution)
-```
-
-**Parámetros**
-
--   `lon` — Longitude. Type: [Float64](../../sql-reference/data-types/float.md).
--   `lat` — Latitude. Type: [Float64](../../sql-reference/data-types/float.md).
--   `resolution` — Index resolution. Range: `[0, 15]`. Tipo: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Número de índice hexagonal.
--   0 en caso de error.
-
-Tipo: `UInt64`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT geoToH3(37.79506683, 55.71290588, 15) as h3Index
-```
-
-Resultado:
-
-``` text
-┌────────────h3Index─┐
-│ 644325524701193974 │
-└────────────────────┘
-```
-
-## Información adicional {#geohashesinbox}
-
-Devuelve una matriz de cadenas codificadas por geohash de precisión dada que caen dentro e intersecan los límites de un cuadro dado, básicamente una cuadrícula 2D aplanada en una matriz.
-
-**Valores de entrada**
-
--   longitude_min - longitud mínima, valor flotante en el rango `[-180°, 180°]`
--   latitude_min - latitud mínima, valor flotante en el rango `[-90°, 90°]`
--   longitude_max - longitud máxima, valor flotante en el rango `[-180°, 180°]`
--   latitude_max - latitud máxima, valor flotante en el rango `[-90°, 90°]`
--   precisión - precisión del geohash, `UInt8` en el rango `[1, 12]`
-
-Tenga en cuenta que todos los parámetros de coordenadas deben ser del mismo tipo: `Float32` o `Float64`.
-
-**Valores devueltos**
-
--   matriz de cadenas de precisión largas de geohash-cajas que cubren el área proporcionada, no debe confiar en el orden de los artículos.
--   \[\] - matriz vacía si *minuto* valores de *latitud* y *longitud* no son menos que los correspondientes *máximo* valor.
-
-Tenga en cuenta que la función arrojará una excepción si la matriz resultante tiene más de 10'000'000 de elementos.
-
-**Ejemplo**
-
-``` sql
-SELECT geohashesInBox(24.48, 40.56, 24.785, 40.81, 4) AS thasos
-```
-
-``` text
-┌─thasos──────────────────────────────────────┐
-│ ['sx1q','sx1r','sx32','sx1w','sx1x','sx38'] │
-└─────────────────────────────────────────────┘
-```
-
-## h3GetBaseCell {#h3getbasecell}
-
-Devuelve el número de celda base del índice.
-
-**Sintaxis**
-
-``` sql
-h3GetBaseCell(index)
-```
-
-**Parámetros**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Número de celda base hexagonal. Tipo: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT h3GetBaseCell(612916788725809151) as basecell
-```
-
-Resultado:
-
-``` text
-┌─basecell─┐
-│       12 │
-└──────────┘
-```
-
-## H3HexAreaM2 {#h3hexaream2}
-
-Área hexagonal promedio en metros cuadrados a la resolución dada.
-
-**Sintaxis**
-
-``` sql
-h3HexAreaM2(resolution)
-```
-
-**Parámetros**
-
--   `resolution` — Index resolution. Range: `[0, 15]`. Tipo: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Area in m². Type: [Float64](../../sql-reference/data-types/float.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT h3HexAreaM2(13) as area
-```
-
-Resultado:
-
-``` text
-┌─area─┐
-│ 43.9 │
-└──────┘
-```
-
-## h3IndexesAreNeighbors {#h3indexesareneighbors}
-
-Devuelve si los H3Indexes proporcionados son vecinos o no.
-
-**Sintaxis**
-
-``` sql
-h3IndexesAreNeighbors(index1, index2)
-```
-
-**Parámetros**
-
--   `index1` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
--   `index2` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Devoluciones `1` si los índices son vecinos, `0` de lo contrario. Tipo: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT h3IndexesAreNeighbors(617420388351344639, 617420388352655359) AS n
-```
-
-Resultado:
-
-``` text
-┌─n─┐
-│ 1 │
-└───┘
-```
-
-## H3ToChildren {#h3tochildren}
-
-Devuelve una matriz con los índices secundarios del índice dado.
-
-**Sintaxis**
-
-``` sql
-h3ToChildren(index, resolution)
-```
-
-**Parámetros**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
--   `resolution` — Index resolution. Range: `[0, 15]`. Tipo: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Matriz con los índices H3 hijo. Matriz de tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT h3ToChildren(599405990164561919, 6) AS children
-```
-
-Resultado:
-
-``` text
-┌─children───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ [603909588852408319,603909588986626047,603909589120843775,603909589255061503,603909589389279231,603909589523496959,603909589657714687] │
-└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-## H3ToParent {#h3toparent}
-
-Devuelve el índice primario (más grueso) que contiene el índice dado.
-
-**Sintaxis**
-
-``` sql
-h3ToParent(index, resolution)
-```
-
-**Parámetros**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
--   `resolution` — Index resolution. Range: `[0, 15]`. Tipo: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Índice padre H3. Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT h3ToParent(599405990164561919, 3) as parent
-```
-
-Resultado:
-
-``` text
-┌─────────────parent─┐
-│ 590398848891879423 │
-└────────────────────┘
-```
-
-## H3ToString {#h3tostring}
-
-Convierte la representación H3Index del índice en la representación de cadena.
-
-``` sql
-h3ToString(index)
-```
-
-**Parámetros**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Representación de cadena del índice H3. Tipo: [Cadena](../../sql-reference/data-types/string.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT h3ToString(617420388352917503) as h3_string
-```
-
-Resultado:
-
-``` text
-┌─h3_string───────┐
-│ 89184926cdbffff │
-└─────────────────┘
-```
-
-## stringToH3 {#stringtoh3}
-
-Convierte la representación de cadena en representación H3Index (UInt64).
-
-``` sql
-stringToH3(index_str)
-```
-
-**Parámetros**
-
--   `index_str` — String representation of the H3 index. Type: [Cadena](../../sql-reference/data-types/string.md).
-
-**Valores devueltos**
-
--   Número de índice hexagonal. Devuelve 0 en caso de error. Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT stringToH3('89184926cc3ffff') as index
-```
-
-Resultado:
-
-``` text
-┌──────────────index─┐
-│ 617420388351344639 │
-└────────────────────┘
-```
-
-## h3GetResolution {#h3getresolution}
-
-Devuelve la resolución del índice.
-
-**Sintaxis**
-
-``` sql
-h3GetResolution(index)
-```
-
-**Parámetros**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valores devueltos**
-
--   Resolución del índice. Gama: `[0, 15]`. Tipo: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT h3GetResolution(617420388352917503) as res
-```
-
-Resultado:
-
-``` text
-┌─res─┐
-│   9 │
-└─────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/geo/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/hash-functions.md b/docs/es/sql-reference/functions/hash-functions.md
deleted file mode 100644
index 165ca2be3d77..000000000000
--- a/docs/es/sql-reference/functions/hash-functions.md
+++ /dev/null
@@ -1,484 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 50
-toc_title: Hash
----
-
-# Funciones de Hash {#hash-functions}
-
-Las funciones Hash se pueden usar para la barajada pseudoaleatoria determinista de elementos.
-
-## HalfMD5 {#hash-functions-halfmd5}
-
-[Interpretar](../../sql-reference/functions/type-conversion-functions.md#type_conversion_functions-reinterpretAsString) todos los parámetros de entrada como cadenas y calcula el [MD5](https://en.wikipedia.org/wiki/MD5) valor hash para cada uno de ellos. Luego combina hashes, toma los primeros 8 bytes del hash de la cadena resultante y los interpreta como `UInt64` en orden de bytes de big-endian.
-
-``` sql
-halfMD5(par1, ...)
-```
-
-La función es relativamente lenta (5 millones de cadenas cortas por segundo por núcleo del procesador).
-Considere usar el [sipHash64](#hash_functions-siphash64) función en su lugar.
-
-**Parámetros**
-
-La función toma un número variable de parámetros de entrada. Los parámetros pueden ser cualquiera de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
-
-**Valor devuelto**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) tipo de datos valor hash.
-
-**Ejemplo**
-
-``` sql
-SELECT halfMD5(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS halfMD5hash, toTypeName(halfMD5hash) AS type
-```
-
-``` text
-┌────────halfMD5hash─┬─type───┐
-│ 186182704141653334 │ UInt64 │
-└────────────────────┴────────┘
-```
-
-## MD5 {#hash_functions-md5}
-
-Calcula el MD5 de una cadena y devuelve el conjunto de bytes resultante como FixedString(16).
-Si no necesita MD5 en particular, pero necesita un hash criptográfico decente de 128 bits, use el ‘sipHash128’ función en su lugar.
-Si desea obtener el mismo resultado que la salida de la utilidad md5sum, use lower(hex(MD5(s)) .
-
-## sipHash64 {#hash_functions-siphash64}
-
-Produce un [SipHash](https://131002.net/siphash/) valor hash.
-
-``` sql
-sipHash64(par1,...)
-```
-
-Esta es una función hash criptográfica. Funciona al menos tres veces más rápido que el [MD5](#hash_functions-md5) función.
-
-Función [interpretar](../../sql-reference/functions/type-conversion-functions.md#type_conversion_functions-reinterpretAsString) todos los parámetros de entrada como cadenas y calcula el valor hash para cada uno de ellos. Luego combina hashes por el siguiente algoritmo:
-
-1.  Después de hash todos los parámetros de entrada, la función obtiene la matriz de hashes.
-2.  La función toma el primero y el segundo elementos y calcula un hash para la matriz de ellos.
-3.  Luego, la función toma el valor hash, calculado en el paso anterior, y el tercer elemento de la matriz hash inicial, y calcula un hash para la matriz de ellos.
-4.  El paso anterior se repite para todos los elementos restantes de la matriz hash inicial.
-
-**Parámetros**
-
-La función toma un número variable de parámetros de entrada. Los parámetros pueden ser cualquiera de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
-
-**Valor devuelto**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) tipo de datos valor hash.
-
-**Ejemplo**
-
-``` sql
-SELECT sipHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS SipHash, toTypeName(SipHash) AS type
-```
-
-``` text
-┌──────────────SipHash─┬─type───┐
-│ 13726873534472839665 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## sipHash128 {#hash_functions-siphash128}
-
-Calcula SipHash a partir de una cadena.
-Acepta un argumento de tipo String. Devuelve FixedString(16).
-Difiere de sipHash64 en que el estado final de plegado xor solo se realiza hasta 128 bits.
-
-## cityHash64 {#cityhash64}
-
-Produce un [Método de codificación de datos:](https://github.com/google/cityhash) valor hash.
-
-``` sql
-cityHash64(par1,...)
-```
-
-Esta es una función hash rápida no criptográfica. Utiliza el algoritmo CityHash para parámetros de cadena y la función hash no criptográfica rápida específica de la implementación para parámetros con otros tipos de datos. La función utiliza el combinador CityHash para obtener los resultados finales.
-
-**Parámetros**
-
-La función toma un número variable de parámetros de entrada. Los parámetros pueden ser cualquiera de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
-
-**Valor devuelto**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) tipo de datos valor hash.
-
-**Ejemplos**
-
-Ejemplo de llamada:
-
-``` sql
-SELECT cityHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS CityHash, toTypeName(CityHash) AS type
-```
-
-``` text
-┌─────────────CityHash─┬─type───┐
-│ 12072650598913549138 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-En el ejemplo siguiente se muestra cómo calcular la suma de comprobación de toda la tabla con precisión hasta el orden de fila:
-
-``` sql
-SELECT groupBitXor(cityHash64(*)) FROM table
-```
-
-## intHash32 {#inthash32}
-
-Calcula un código hash de 32 bits a partir de cualquier tipo de entero.
-Esta es una función hash no criptográfica relativamente rápida de calidad media para los números.
-
-## intHash64 {#inthash64}
-
-Calcula un código hash de 64 bits a partir de cualquier tipo de entero.
-Funciona más rápido que intHash32. Calidad media.
-
-## SHA1 {#sha1}
-
-## SHA224 {#sha224}
-
-## SHA256 {#sha256}
-
-Calcula SHA-1, SHA-224 o SHA-256 de una cadena y devuelve el conjunto de bytes resultante como FixedString(20), FixedString(28) o FixedString(32).
-La función funciona bastante lentamente (SHA-1 procesa alrededor de 5 millones de cadenas cortas por segundo por núcleo del procesador, mientras que SHA-224 y SHA-256 procesan alrededor de 2.2 millones).
-Recomendamos usar esta función solo en los casos en que necesite una función hash específica y no pueda seleccionarla.
-Incluso en estos casos, recomendamos aplicar la función offline y precalcular valores al insertarlos en la tabla, en lugar de aplicarlo en SELECTS.
-
-## Nombre de la red inalámbrica (SSID):\]) {#urlhashurl-n}
-
-Una función hash no criptográfica rápida y de calidad decente para una cadena obtenida de una URL utilizando algún tipo de normalización.
-`URLHash(s)` – Calculates a hash from a string without one of the trailing symbols `/`,`?` o `#` al final, si está presente.
-`URLHash(s, N)` – Calculates a hash from a string up to the N level in the URL hierarchy, without one of the trailing symbols `/`,`?` o `#` al final, si está presente.
-Los niveles son los mismos que en URLHierarchy. Esta función es específica de Yandex.Métrica.
-
-## Método de codificación de datos: {#farmhash64}
-
-Produce un [Método de codificación de datos:](https://github.com/google/farmhash) valor hash.
-
-``` sql
-farmHash64(par1, ...)
-```
-
-La función utiliza el `Hash64` de todos [métodos disponibles](https://github.com/google/farmhash/blob/master/src/farmhash.h).
-
-**Parámetros**
-
-La función toma un número variable de parámetros de entrada. Los parámetros pueden ser cualquiera de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
-
-**Valor devuelto**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) tipo de datos valor hash.
-
-**Ejemplo**
-
-``` sql
-SELECT farmHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS FarmHash, toTypeName(FarmHash) AS type
-```
-
-``` text
-┌─────────────FarmHash─┬─type───┐
-│ 17790458267262532859 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## Nombre de la red inalámbrica (SSID): {#hash_functions-javahash}
-
-Calcular [Nivel de Cifrado WEP](http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/478a4add975b/src/share/classes/java/lang/String.java#l1452) de una cuerda. Esta función hash no es rápida ni tiene una buena calidad. La única razón para usarlo es cuando este algoritmo ya se usa en otro sistema y debe calcular exactamente el mismo resultado.
-
-**Sintaxis**
-
-``` sql
-SELECT javaHash('');
-```
-
-**Valor devuelto**
-
-A `Int32` tipo de datos valor hash.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT javaHash('Hello, world!');
-```
-
-Resultado:
-
-``` text
-┌─javaHash('Hello, world!')─┐
-│               -1880044555 │
-└───────────────────────────┘
-```
-
-## javaHashUTF16LE {#javahashutf16le}
-
-Calcular [Nivel de Cifrado WEP](http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/478a4add975b/src/share/classes/java/lang/String.java#l1452) de una cadena, suponiendo que contiene bytes que representan una cadena en codificación UTF-16LE.
-
-**Sintaxis**
-
-``` sql
-javaHashUTF16LE(stringUtf16le)
-```
-
-**Parámetros**
-
--   `stringUtf16le` — a string in UTF-16LE encoding.
-
-**Valor devuelto**
-
-A `Int32` tipo de datos valor hash.
-
-**Ejemplo**
-
-Consulta correcta con cadena codificada UTF-16LE.
-
-Consulta:
-
-``` sql
-SELECT javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'))
-```
-
-Resultado:
-
-``` text
-┌─javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'))─┐
-│                                                      3556498 │
-└──────────────────────────────────────────────────────────────┘
-```
-
-## HiveHash {#hash-functions-hivehash}
-
-Calcular `HiveHash` de una cuerda.
-
-``` sql
-SELECT hiveHash('');
-```
-
-Esto es sólo [Nivel de Cifrado WEP](#hash_functions-javahash) con poco de signo puesto a cero. Esta función se utiliza en [Colmena de Apache](https://en.wikipedia.org/wiki/Apache_Hive) para versiones anteriores a la 3.0. Esta función hash no es rápida ni tiene una buena calidad. La única razón para usarlo es cuando este algoritmo ya se usa en otro sistema y debe calcular exactamente el mismo resultado.
-
-**Valor devuelto**
-
-A `Int32` tipo de datos valor hash.
-
-Tipo: `hiveHash`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT hiveHash('Hello, world!');
-```
-
-Resultado:
-
-``` text
-┌─hiveHash('Hello, world!')─┐
-│                 267439093 │
-└───────────────────────────┘
-```
-
-## Método de codificación de datos: {#metrohash64}
-
-Produce un [Método de codificación de datos:](http://www.jandrewrogers.com/2015/05/27/metrohash/) valor hash.
-
-``` sql
-metroHash64(par1, ...)
-```
-
-**Parámetros**
-
-La función toma un número variable de parámetros de entrada. Los parámetros pueden ser cualquiera de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
-
-**Valor devuelto**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) tipo de datos valor hash.
-
-**Ejemplo**
-
-``` sql
-SELECT metroHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MetroHash, toTypeName(MetroHash) AS type
-```
-
-``` text
-┌────────────MetroHash─┬─type───┐
-│ 14235658766382344533 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## SaltarConsistentHash {#jumpconsistenthash}
-
-Calcula JumpConsistentHash forma un UInt64.
-Acepta dos argumentos: una clave de tipo UInt64 y el número de cubos. Devuelve Int32.
-Para obtener más información, consulte el enlace: [SaltarConsistentHash](https://arxiv.org/pdf/1406.2294.pdf)
-
-## murmurHash2_32, murmurHash2_64 {#murmurhash2-32-murmurhash2-64}
-
-Produce un [Método de codificación de datos:](https://github.com/aappleby/smhasher) valor hash.
-
-``` sql
-murmurHash2_32(par1, ...)
-murmurHash2_64(par1, ...)
-```
-
-**Parámetros**
-
-Ambas funciones toman un número variable de parámetros de entrada. Los parámetros pueden ser cualquiera de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
-
-**Valor devuelto**
-
--   El `murmurHash2_32` función devuelve el valor hash que tiene el [UInt32](../../sql-reference/data-types/int-uint.md) tipo de datos.
--   El `murmurHash2_64` función devuelve el valor hash que tiene el [UInt64](../../sql-reference/data-types/int-uint.md) tipo de datos.
-
-**Ejemplo**
-
-``` sql
-SELECT murmurHash2_64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash2, toTypeName(MurmurHash2) AS type
-```
-
-``` text
-┌──────────MurmurHash2─┬─type───┐
-│ 11832096901709403633 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## GccMurmurHash {#gccmurmurhash}
-
-Calcula un valor de 64 bits [Método de codificación de datos:](https://github.com/aappleby/smhasher) valor hash usando la misma semilla de hash que [Gcc](https://github.com/gcc-mirror/gcc/blob/41d6b10e96a1de98e90a7c0378437c3255814b16/libstdc%2B%2B-v3/include/bits/functional_hash.h#L191). Es portátil entre las compilaciones CLang y GCC.
-
-**Sintaxis**
-
-``` sql
-gccMurmurHash(par1, ...);
-```
-
-**Parámetros**
-
--   `par1, ...` — A variable number of parameters that can be any of the [tipos de datos compatibles](../../sql-reference/data-types/index.md#data_types).
-
-**Valor devuelto**
-
--   Valor hash calculado.
-
-Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT
-    gccMurmurHash(1, 2, 3) AS res1,
-    gccMurmurHash(('a', [1, 2, 3], 4, (4, ['foo', 'bar'], 1, (1, 2)))) AS res2
-```
-
-Resultado:
-
-``` text
-┌─────────────────res1─┬────────────────res2─┐
-│ 12384823029245979431 │ 1188926775431157506 │
-└──────────────────────┴─────────────────────┘
-```
-
-## murmurHash3_32, murmurHash3_64 {#murmurhash3-32-murmurhash3-64}
-
-Produce un [Método de codificación de datos:](https://github.com/aappleby/smhasher) valor hash.
-
-``` sql
-murmurHash3_32(par1, ...)
-murmurHash3_64(par1, ...)
-```
-
-**Parámetros**
-
-Ambas funciones toman un número variable de parámetros de entrada. Los parámetros pueden ser cualquiera de los [tipos de datos compatibles](../../sql-reference/data-types/index.md).
-
-**Valor devuelto**
-
--   El `murmurHash3_32` función devuelve un [UInt32](../../sql-reference/data-types/int-uint.md) tipo de datos valor hash.
--   El `murmurHash3_64` función devuelve un [UInt64](../../sql-reference/data-types/int-uint.md) tipo de datos valor hash.
-
-**Ejemplo**
-
-``` sql
-SELECT murmurHash3_32(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash3, toTypeName(MurmurHash3) AS type
-```
-
-``` text
-┌─MurmurHash3─┬─type───┐
-│     2152717 │ UInt32 │
-└─────────────┴────────┘
-```
-
-## murmurHash3_128 {#murmurhash3-128}
-
-Produce un [Método de codificación de datos:](https://github.com/aappleby/smhasher) valor hash.
-
-``` sql
-murmurHash3_128( expr )
-```
-
-**Parámetros**
-
--   `expr` — [Expresiones](../syntax.md#syntax-expressions) devolviendo un [Cadena](../../sql-reference/data-types/string.md)-tipo de valor.
-
-**Valor devuelto**
-
-A [Cadena fija (16)](../../sql-reference/data-types/fixedstring.md) tipo de datos valor hash.
-
-**Ejemplo**
-
-``` sql
-SELECT murmurHash3_128('example_string') AS MurmurHash3, toTypeName(MurmurHash3) AS type
-```
-
-``` text
-┌─MurmurHash3──────┬─type────────────┐
-│ 6�1�4"S5KT�~~q │ FixedString(16) │
-└──────────────────┴─────────────────┘
-```
-
-## xxHash32, xxHash64 {#hash-functions-xxhash32}
-
-Calcular `xxHash` de una cuerda. Se propone en dos sabores, 32 y 64 bits.
-
-``` sql
-SELECT xxHash32('');
-
-OR
-
-SELECT xxHash64('');
-```
-
-**Valor devuelto**
-
-A `Uint32` o `Uint64` tipo de datos valor hash.
-
-Tipo: `xxHash`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT xxHash32('Hello, world!');
-```
-
-Resultado:
-
-``` text
-┌─xxHash32('Hello, world!')─┐
-│                 834093149 │
-└───────────────────────────┘
-```
-
-**Ver también**
-
--   [xxHash](http://cyan4973.github.io/xxHash/).
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/hash_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/higher-order-functions.md b/docs/es/sql-reference/functions/higher-order-functions.md
deleted file mode 100644
index a4c97d813613..000000000000
--- a/docs/es/sql-reference/functions/higher-order-functions.md
+++ /dev/null
@@ -1,264 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 57
-toc_title: Orden superior
----
-
-# Funciones de orden superior {#higher-order-functions}
-
-## `->` operador, función lambda (params, expr) {#operator-lambdaparams-expr-function}
-
-Allows describing a lambda function for passing to a higher-order function. The left side of the arrow has a formal parameter, which is any ID, or multiple formal parameters – any IDs in a tuple. The right side of the arrow has an expression that can use these formal parameters, as well as any table columns.
-
-Ejemplos: `x -> 2 * x, str -> str != Referer.`
-
-Las funciones de orden superior solo pueden aceptar funciones lambda como su argumento funcional.
-
-Una función lambda que acepta múltiples argumentos se puede pasar a una función de orden superior. En este caso, a la función de orden superior se le pasan varias matrices de idéntica longitud a las que corresponderán estos argumentos.
-
-Para algunas funciones, tales como [arrayCount](#higher_order_functions-array-count) o [arraySum](#higher_order_functions-array-count), el primer argumento (la función lambda) se puede omitir. En este caso, se supone un mapeo idéntico.
-
-No se puede omitir una función lambda para las siguientes funciones:
-
--   [arrayMap](#higher_order_functions-array-map)
--   [arrayFilter](#higher_order_functions-array-filter)
--   [arrayFill](#higher_order_functions-array-fill)
--   [arrayReverseFill](#higher_order_functions-array-reverse-fill)
--   [arraySplit](#higher_order_functions-array-split)
--   [arrayReverseSplit](#higher_order_functions-array-reverse-split)
--   [arrayFirst](#higher_order_functions-array-first)
--   [arrayFirstIndex](#higher_order_functions-array-first-index)
-
-### arrayMap(func, arr1, …) {#higher_order_functions-array-map}
-
-Devuelve una matriz obtenida de la aplicación original `func` función a cada elemento en el `arr` matriz.
-
-Ejemplos:
-
-``` sql
-SELECT arrayMap(x -> (x + 2), [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [3,4,5] │
-└─────────┘
-```
-
-En el ejemplo siguiente se muestra cómo crear una tupla de elementos de diferentes matrices:
-
-``` sql
-SELECT arrayMap((x, y) -> (x, y), [1, 2, 3], [4, 5, 6]) AS res
-```
-
-``` text
-┌─res─────────────────┐
-│ [(1,4),(2,5),(3,6)] │
-└─────────────────────┘
-```
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arrayMap` función.
-
-### arrayFilter(func, arr1, …) {#higher_order_functions-array-filter}
-
-Devuelve una matriz que contiene sólo los elementos en `arr1` para los cuales `func` devuelve algo distinto de 0.
-
-Ejemplos:
-
-``` sql
-SELECT arrayFilter(x -> x LIKE '%World%', ['Hello', 'abc World']) AS res
-```
-
-``` text
-┌─res───────────┐
-│ ['abc World'] │
-└───────────────┘
-```
-
-``` sql
-SELECT
-    arrayFilter(
-        (i, x) -> x LIKE '%World%',
-        arrayEnumerate(arr),
-        ['Hello', 'abc World'] AS arr)
-    AS res
-```
-
-``` text
-┌─res─┐
-│ [2] │
-└─────┘
-```
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arrayFilter` función.
-
-### arrayFill(func, arr1, …) {#higher_order_functions-array-fill}
-
-Escanear a través de `arr1` desde el primer elemento hasta el último elemento y reemplazar `arr1[i]` por `arr1[i - 1]` si `func` devuelve 0. El primer elemento de `arr1` no será reemplazado.
-
-Ejemplos:
-
-``` sql
-SELECT arrayFill(x -> not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res
-```
-
-``` text
-┌─res──────────────────────────────┐
-│ [1,1,3,11,12,12,12,5,6,14,14,14] │
-└──────────────────────────────────┘
-```
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arrayFill` función.
-
-### arrayReverseFill(func, arr1, …) {#higher_order_functions-array-reverse-fill}
-
-Escanear a través de `arr1` del último elemento al primer elemento y reemplace `arr1[i]` por `arr1[i + 1]` si `func` devuelve 0. El último elemento de `arr1` no será reemplazado.
-
-Ejemplos:
-
-``` sql
-SELECT arrayReverseFill(x -> not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res
-```
-
-``` text
-┌─res────────────────────────────────┐
-│ [1,3,3,11,12,5,5,5,6,14,NULL,NULL] │
-└────────────────────────────────────┘
-```
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arrayReverseFill` función.
-
-### arraySplit(func, arr1, …) {#higher_order_functions-array-split}
-
-Dividir `arr1` en múltiples matrices. Cuando `func` devuelve algo distinto de 0, la matriz se dividirá en el lado izquierdo del elemento. La matriz no se dividirá antes del primer elemento.
-
-Ejemplos:
-
-``` sql
-SELECT arraySplit((x, y) -> y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res
-```
-
-``` text
-┌─res─────────────┐
-│ [[1,2,3],[4,5]] │
-└─────────────────┘
-```
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arraySplit` función.
-
-### arrayReverseSplit(func, arr1, …) {#higher_order_functions-array-reverse-split}
-
-Dividir `arr1` en múltiples matrices. Cuando `func` devuelve algo distinto de 0, la matriz se dividirá en el lado derecho del elemento. La matriz no se dividirá después del último elemento.
-
-Ejemplos:
-
-``` sql
-SELECT arrayReverseSplit((x, y) -> y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res
-```
-
-``` text
-┌─res───────────────┐
-│ [[1],[2,3,4],[5]] │
-└───────────────────┘
-```
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arraySplit` función.
-
-### arrayCount(\[func,\] arr1, …) {#higher_order_functions-array-count}
-
-Devuelve el número de elementos de la matriz arr para los cuales func devuelve algo distinto de 0. Si ‘func’ no se especifica, devuelve el número de elementos distintos de cero en la matriz.
-
-### arrayExists(\[func,\] arr1, …) {#arrayexistsfunc-arr1}
-
-Devuelve 1 si hay al menos un elemento en ‘arr’ para los cuales ‘func’ devuelve algo distinto de 0. De lo contrario, devuelve 0.
-
-### arrayAll(\[func,\] arr1, …) {#arrayallfunc-arr1}
-
-Devuelve 1 si ‘func’ devuelve algo distinto de 0 para todos los elementos en ‘arr’. De lo contrario, devuelve 0.
-
-### arraySum(\[func,\] arr1, …) {#higher-order-functions-array-sum}
-
-Devuelve la suma de la ‘func’ valor. Si se omite la función, simplemente devuelve la suma de los elementos de la matriz.
-
-### arrayFirst(func, arr1, …) {#higher_order_functions-array-first}
-
-Devuelve el primer elemento en el ‘arr1’ matriz para la cual ‘func’ devuelve algo distinto de 0.
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arrayFirst` función.
-
-### arrayFirstIndex(func, arr1, …) {#higher_order_functions-array-first-index}
-
-Devuelve el índice del primer elemento ‘arr1’ matriz para la cual ‘func’ devuelve algo distinto de 0.
-
-Tenga en cuenta que el primer argumento (función lambda) no se puede omitir en el `arrayFirstIndex` función.
-
-### arrayCumSum(\[func,\] arr1, …) {#arraycumsumfunc-arr1}
-
-Devuelve una matriz de sumas parciales de elementos en la matriz de origen (una suma en ejecución). Si el `func` se especifica la función, luego los valores de los elementos de la matriz se convierten mediante esta función antes de sumar.
-
-Ejemplo:
-
-``` sql
-SELECT arrayCumSum([1, 1, 1, 1]) AS res
-```
-
-``` text
-┌─res──────────┐
-│ [1, 2, 3, 4] │
-└──────────────┘
-```
-
-### Información de archivo) {#arraycumsumnonnegativearr}
-
-Lo mismo que `arrayCumSum`, devuelve una matriz de sumas parciales de elementos en la matriz de origen (una suma en ejecución). Diferente `arrayCumSum`, cuando el valor devuelto contiene un valor menor que cero, el valor se reemplaza con cero y el cálculo posterior se realiza con cero parámetros. Por ejemplo:
-
-``` sql
-SELECT arrayCumSumNonNegative([1, 1, -4, 1]) AS res
-```
-
-``` text
-┌─res───────┐
-│ [1,2,0,1] │
-└───────────┘
-```
-
-### arraySort(\[func,\] arr1, …) {#arraysortfunc-arr1}
-
-Devuelve una matriz como resultado de ordenar los elementos de `arr1` en orden ascendente. Si el `func` se especifica la función, el orden de clasificación se determina por el resultado de la función `func` aplicado a los elementos de la matriz (arrays)
-
-El [Transformación de Schwartzian](https://en.wikipedia.org/wiki/Schwartzian_transform) se utiliza para mejorar la eficiencia de clasificación.
-
-Ejemplo:
-
-``` sql
-SELECT arraySort((x, y) -> y, ['hello', 'world'], [2, 1]);
-```
-
-``` text
-┌─res────────────────┐
-│ ['world', 'hello'] │
-└────────────────────┘
-```
-
-Para obtener más información sobre el `arraySort` método, véase el [Funciones para trabajar con matrices](array-functions.md#array_functions-sort) apartado.
-
-### arrayReverseSort(\[func,\] arr1, …) {#arrayreversesortfunc-arr1}
-
-Devuelve una matriz como resultado de ordenar los elementos de `arr1` en orden descendente. Si el `func` se especifica la función, el orden de clasificación se determina por el resultado de la función `func` aplicado a los elementos de la matriz (arrays).
-
-Ejemplo:
-
-``` sql
-SELECT arrayReverseSort((x, y) -> y, ['hello', 'world'], [2, 1]) as res;
-```
-
-``` text
-┌─res───────────────┐
-│ ['hello','world'] │
-└───────────────────┘
-```
-
-Para obtener más información sobre el `arrayReverseSort` método, véase el [Funciones para trabajar con matrices](array-functions.md#array_functions-reverse-sort) apartado.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/higher_order_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/in-functions.md b/docs/es/sql-reference/functions/in-functions.md
deleted file mode 100644
index 3788582fa604..000000000000
--- a/docs/es/sql-reference/functions/in-functions.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: "Implementaci\xF3n del operador IN"
----
-
-# Funciones para implementar el operador IN {#functions-for-implementing-the-in-operator}
-
-## Información de uso {#in-functions}
-
-Vea la sección [IN operadores](../operators/in.md#select-in-operators).
-
-## tuple(x, y, …), operator (x, y, …) {#tuplex-y-operator-x-y}
-
-Una función que permite agrupar varias columnas.
-For columns with the types T1, T2, …, it returns a Tuple(T1, T2, …) type tuple containing these columns. There is no cost to execute the function.
-Las tuplas se usan normalmente como valores intermedios para un argumento de operadores IN, o para crear una lista de parámetros formales de funciones lambda. Las tuplas no se pueden escribir en una tabla.
-
-## Puede utilizar el siguiente ejemplo: {#tupleelementtuple-n-operator-x-n}
-
-Una función que permite obtener una columna de una tupla.
-‘N’ es el índice de columna, comenzando desde 1. N debe ser una constante. ‘N’ debe ser una constante. ‘N’ debe ser un entero postivo estricto no mayor que el tamaño de la tupla.
-No hay ningún costo para ejecutar la función.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/in_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/index.md b/docs/es/sql-reference/functions/index.md
deleted file mode 100644
index bbb0cfc7386b..000000000000
--- a/docs/es/sql-reference/functions/index.md
+++ /dev/null
@@ -1,74 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Funci\xF3n"
-toc_priority: 32
-toc_title: "Implantaci\xF3n"
----
-
-# Función {#functions}
-
-Hay al menos \* dos tipos de funciones: funciones regulares (simplemente se llaman “functions”) and aggregate functions. These are completely different concepts. Regular functions work as if they are applied to each row separately (for each row, the result of the function doesn't depend on the other rows). Aggregate functions accumulate a set of values from various rows (i.e. they depend on the entire set of rows).
-
-En esta sección discutimos las funciones regulares. Para las funciones agregadas, consulte la sección “Aggregate functions”.
-
-\* - Existe un tercer tipo de función que el ‘arrayJoin’ la función pertenece a; las funciones de la tabla también se pueden mencionar por separado.\*
-
-## Fuerte mecanografía {#strong-typing}
-
-A diferencia del SQL estándar, ClickHouse tiene una tipificación fuerte. En otras palabras, no hace conversiones implícitas entre tipos. Cada función funciona para un conjunto específico de tipos. Esto significa que a veces necesita usar funciones de conversión de tipos.
-
-## Eliminación de subexpresiones comunes {#common-subexpression-elimination}
-
-Se considera que todas las expresiones de una consulta que tienen el mismo AST (el mismo registro o el mismo resultado del análisis sintáctico) tienen valores idénticos. Tales expresiones se concatenan y se ejecutan una vez. Las subconsultas idénticas también se eliminan de esta manera.
-
-## Tipos de resultados {#types-of-results}
-
-Todas las funciones devuelven un único retorno como resultado (no varios valores, y no valores cero). El tipo de resultado generalmente se define solo por los tipos de argumentos, no por los valores. Las excepciones son la función tupleElement (el operador a.N) y la función toFixedString.
-
-## Constante {#constants}
-
-Para simplificar, ciertas funciones solo pueden funcionar con constantes para algunos argumentos. Por ejemplo, el argumento correcto del operador LIKE debe ser una constante.
-Casi todas las funciones devuelven una constante para argumentos constantes. La excepción son las funciones que generan números aleatorios.
-El ‘now’ function devuelve valores diferentes para las consultas que se ejecutaron en diferentes momentos, pero el resultado se considera una constante, ya que la constancia solo es importante dentro de una sola consulta.
-Una expresión constante también se considera una constante (por ejemplo, la mitad derecha del operador LIKE se puede construir a partir de múltiples constantes).
-
-Las funciones se pueden implementar de diferentes maneras para argumentos constantes y no constantes (se ejecuta un código diferente). Pero los resultados para una constante y para una columna verdadera que contiene solo el mismo valor deben coincidir entre sí.
-
-## Procesamiento NULL {#null-processing}
-
-Las funciones tienen los siguientes comportamientos:
-
--   Si al menos uno de los argumentos de la función es `NULL` el resultado de la función es también `NULL`.
--   Comportamiento especial que se especifica individualmente en la descripción de cada función. En el código fuente de ClickHouse, estas funciones tienen `UseDefaultImplementationForNulls=false`.
-
-## Constancia {#constancy}
-
-Functions can't change the values of their arguments – any changes are returned as the result. Thus, the result of calculating separate functions does not depend on the order in which the functions are written in the query.
-
-## Manejo de errores {#error-handling}
-
-Algunas funciones pueden producir una excepción si los datos no son válidos. En este caso, la consulta se cancela y se devuelve un texto de error al cliente. Para el procesamiento distribuido, cuando se produce una excepción en uno de los servidores, los otros servidores también intentan anular la consulta.
-
-## Evaluación de las expresiones de argumento {#evaluation-of-argument-expressions}
-
-En casi todos los lenguajes de programación, uno de los argumentos puede no evaluarse para ciertos operadores. Esto suele ser los operadores `&&`, `||`, y `?:`.
-Pero en ClickHouse, los argumentos de las funciones (operadores) siempre se evalúan. Esto se debe a que partes enteras de columnas se evalúan a la vez, en lugar de calcular cada fila por separado.
-
-## Realización de funciones para el procesamiento de consultas distribuidas {#performing-functions-for-distributed-query-processing}
-
-Para el procesamiento de consultas distribuidas, se realizan tantas etapas de procesamiento de consultas como sea posible en servidores remotos, y el resto de las etapas (fusionando resultados intermedios y todo lo posterior) se realizan en el servidor solicitante.
-
-Esto significa que las funciones se pueden realizar en diferentes servidores.
-Por ejemplo, en la consulta `SELECT f(sum(g(x))) FROM distributed_table GROUP BY h(y),`
-
--   si una `distributed_table` tiene al menos dos fragmentos, las funciones ‘g’ y ‘h’ se realizan en servidores remotos, y la función ‘f’ se realiza en el servidor solicitante.
--   si una `distributed_table` tiene sólo un fragmento, todos los ‘f’, ‘g’, y ‘h’ funciones se realizan en el servidor de este fragmento.
-
-El resultado de una función generalmente no depende del servidor en el que se realice. Sin embargo, a veces esto es importante.
-Por ejemplo, las funciones que funcionan con diccionarios utilizan el diccionario que existe en el servidor en el que se están ejecutando.
-Otro ejemplo es el `hostName` función, que devuelve el nombre del servidor en el que se está ejecutando para `GROUP BY` por servidores en un `SELECT` consulta.
-
-Si se realiza una función en una consulta en el servidor solicitante, pero debe realizarla en servidores remotos, puede envolverla en un ‘any’ agregar o agregarlo a una clave en `GROUP BY`.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/introspection.md b/docs/es/sql-reference/functions/introspection.md
deleted file mode 100644
index 1482bbc3190c..000000000000
--- a/docs/es/sql-reference/functions/introspection.md
+++ /dev/null
@@ -1,310 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 65
-toc_title: "Introspecci\xF3n"
----
-
-# Funciones de introspección {#introspection-functions}
-
-Puede utilizar las funciones descritas en este capítulo para [ELF](https://en.wikipedia.org/wiki/Executable_and_Linkable_Format) y [DWARF](https://en.wikipedia.org/wiki/DWARF) para la creación de perfiles de consultas.
-
-!!! warning "Advertencia"
-    Estas funciones son lentas y pueden imponer consideraciones de seguridad.
-
-Para el correcto funcionamiento de las funciones de introspección:
-
--   Instale el `clickhouse-common-static-dbg` paquete.
-
--   Establezca el [allow_introspection_functions](../../operations/settings/settings.md#settings-allow_introspection_functions) a 1.
-
-        For security reasons introspection functions are disabled by default.
-
-ClickHouse guarda los informes del generador de perfiles [trace_log](../../operations/system-tables.md#system_tables-trace_log) tabla del sistema. Asegúrese de que la tabla y el generador de perfiles estén configurados correctamente.
-
-## addressToLine {#addresstoline}
-
-Convierte la dirección de memoria virtual dentro del proceso del servidor ClickHouse en el nombre de archivo y el número de línea en el código fuente de ClickHouse.
-
-Si utiliza paquetes oficiales de ClickHouse, debe instalar el `clickhouse-common-static-dbg` paquete.
-
-**Sintaxis**
-
-``` sql
-addressToLine(address_of_binary_instruction)
-```
-
-**Parámetros**
-
--   `address_of_binary_instruction` ([UInt64](../../sql-reference/data-types/int-uint.md)) — Address of instruction in a running process.
-
-**Valor devuelto**
-
--   Nombre de archivo del código fuente y el número de línea en este archivo delimitado por dos puntos.
-
-        For example, `/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199`, where `199` is a line number.
-
--   Nombre de un binario, si la función no pudo encontrar la información de depuración.
-
--   Cadena vacía, si la dirección no es válida.
-
-Tipo: [Cadena](../../sql-reference/data-types/string.md).
-
-**Ejemplo**
-
-Habilitación de las funciones de introspección:
-
-``` sql
-SET allow_introspection_functions=1
-```
-
-Seleccionando la primera cadena de la `trace_log` tabla del sistema:
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:              2019-11-19
-event_time:              2019-11-19 18:57:23
-revision:                54429
-timer_type:              Real
-thread_number:           48
-query_id:                421b6855-1858-45a5-8f37-f383409d6d72
-trace:                   [140658411141617,94784174532828,94784076370703,94784076372094,94784076361020,94784175007680,140658411116251,140658403895439]
-```
-
-El `trace` campo contiene el seguimiento de la pila en el momento del muestreo.
-
-Obtener el nombre de archivo del código fuente y el número de línea para una sola dirección:
-
-``` sql
-SELECT addressToLine(94784076370703) \G
-```
-
-``` text
-Row 1:
-──────
-addressToLine(94784076370703): /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199
-```
-
-Aplicando la función a todo el seguimiento de la pila:
-
-``` sql
-SELECT
-    arrayStringConcat(arrayMap(x -> addressToLine(x), trace), '
') AS trace_source_code_lines
-FROM system.trace_log
-LIMIT 1
-\G
-```
-
-El [arrayMap](higher-order-functions.md#higher_order_functions-array-map) permite procesar cada elemento individual de la `trace` matriz por el `addressToLine` función. El resultado de este procesamiento se ve en el `trace_source_code_lines` columna de salida.
-
-``` text
-Row 1:
-──────
-trace_source_code_lines: /lib/x86_64-linux-gnu/libpthread-2.27.so
-/usr/lib/debug/usr/bin/clickhouse
-/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199
-/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:155
-/usr/include/c++/9/bits/atomic_base.h:551
-/usr/lib/debug/usr/bin/clickhouse
-/lib/x86_64-linux-gnu/libpthread-2.27.so
-/build/glibc-OTsEL5/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97
-```
-
-## addressToSymbol {#addresstosymbol}
-
-Convierte la dirección de memoria virtual dentro del proceso del servidor ClickHouse en el símbolo de los archivos de objetos ClickHouse.
-
-**Sintaxis**
-
-``` sql
-addressToSymbol(address_of_binary_instruction)
-```
-
-**Parámetros**
-
--   `address_of_binary_instruction` ([UInt64](../../sql-reference/data-types/int-uint.md)) — Address of instruction in a running process.
-
-**Valor devuelto**
-
--   Símbolo de archivos de objetos ClickHouse.
--   Cadena vacía, si la dirección no es válida.
-
-Tipo: [Cadena](../../sql-reference/data-types/string.md).
-
-**Ejemplo**
-
-Habilitación de las funciones de introspección:
-
-``` sql
-SET allow_introspection_functions=1
-```
-
-Seleccionando la primera cadena de la `trace_log` tabla del sistema:
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:    2019-11-20
-event_time:    2019-11-20 16:57:59
-revision:      54429
-timer_type:    Real
-thread_number: 48
-query_id:      724028bf-f550-45aa-910d-2af6212b94ac
-trace:         [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]
-```
-
-El `trace` campo contiene el seguimiento de la pila en el momento del muestreo.
-
-Obtener un símbolo para una sola dirección:
-
-``` sql
-SELECT addressToSymbol(94138803686098) \G
-```
-
-``` text
-Row 1:
-──────
-addressToSymbol(94138803686098): _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE
-```
-
-Aplicando la función a todo el seguimiento de la pila:
-
-``` sql
-SELECT
-    arrayStringConcat(arrayMap(x -> addressToSymbol(x), trace), '
') AS trace_symbols
-FROM system.trace_log
-LIMIT 1
-\G
-```
-
-El [arrayMap](higher-order-functions.md#higher_order_functions-array-map) permite procesar cada elemento individual de la `trace` matriz por el `addressToSymbols` función. El resultado de este procesamiento se ve en el `trace_symbols` columna de salida.
-
-``` text
-Row 1:
-──────
-trace_symbols: _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE
-_ZNK2DB10Aggregator21executeWithoutKeyImplERPcmPNS0_28AggregateFunctionInstructionEPNS_5ArenaE
-_ZN2DB10Aggregator14executeOnBlockESt6vectorIN3COWINS_7IColumnEE13immutable_ptrIS3_EESaIS6_EEmRNS_22AggregatedDataVariantsERS1_IPKS3_SaISC_EERS1_ISE_SaISE_EERb
-_ZN2DB10Aggregator14executeOnBlockERKNS_5BlockERNS_22AggregatedDataVariantsERSt6vectorIPKNS_7IColumnESaIS9_EERS6_ISB_SaISB_EERb
-_ZN2DB10Aggregator7executeERKSt10shared_ptrINS_17IBlockInputStreamEERNS_22AggregatedDataVariantsE
-_ZN2DB27AggregatingBlockInputStream8readImplEv
-_ZN2DB17IBlockInputStream4readEv
-_ZN2DB26ExpressionBlockInputStream8readImplEv
-_ZN2DB17IBlockInputStream4readEv
-_ZN2DB26ExpressionBlockInputStream8readImplEv
-_ZN2DB17IBlockInputStream4readEv
-_ZN2DB28AsynchronousBlockInputStream9calculateEv
-_ZNSt17_Function_handlerIFvvEZN2DB28AsynchronousBlockInputStream4nextEvEUlvE_E9_M_invokeERKSt9_Any_data
-_ZN14ThreadPoolImplI20ThreadFromGlobalPoolE6workerESt14_List_iteratorIS0_E
-_ZZN20ThreadFromGlobalPoolC4IZN14ThreadPoolImplIS_E12scheduleImplIvEET_St8functionIFvvEEiSt8optionalImEEUlvE1_JEEEOS4_DpOT0_ENKUlvE_clEv
-_ZN14ThreadPoolImplISt6threadE6workerESt14_List_iteratorIS0_E
-execute_native_thread_routine
-start_thread
-clone
-```
-
-## demangle {#demangle}
-
-Convierte un símbolo que puede obtener utilizando el [addressToSymbol](#addresstosymbol) función a la función de C++ nombre.
-
-**Sintaxis**
-
-``` sql
-demangle(symbol)
-```
-
-**Parámetros**
-
--   `symbol` ([Cadena](../../sql-reference/data-types/string.md)) — Symbol from an object file.
-
-**Valor devuelto**
-
--   Nombre de la función C++
--   Cadena vacía si un símbolo no es válido.
-
-Tipo: [Cadena](../../sql-reference/data-types/string.md).
-
-**Ejemplo**
-
-Habilitación de las funciones de introspección:
-
-``` sql
-SET allow_introspection_functions=1
-```
-
-Seleccionando la primera cadena de la `trace_log` tabla del sistema:
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:    2019-11-20
-event_time:    2019-11-20 16:57:59
-revision:      54429
-timer_type:    Real
-thread_number: 48
-query_id:      724028bf-f550-45aa-910d-2af6212b94ac
-trace:         [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]
-```
-
-El `trace` campo contiene el seguimiento de la pila en el momento del muestreo.
-
-Obtener un nombre de función para una sola dirección:
-
-``` sql
-SELECT demangle(addressToSymbol(94138803686098)) \G
-```
-
-``` text
-Row 1:
-──────
-demangle(addressToSymbol(94138803686098)): DB::IAggregateFunctionHelper<DB::AggregateFunctionSum<unsigned long, unsigned long, DB::AggregateFunctionSumData<unsigned long> > >::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const
-```
-
-Aplicando la función a todo el seguimiento de la pila:
-
-``` sql
-SELECT
-    arrayStringConcat(arrayMap(x -> demangle(addressToSymbol(x)), trace), '
') AS trace_functions
-FROM system.trace_log
-LIMIT 1
-\G
-```
-
-El [arrayMap](higher-order-functions.md#higher_order_functions-array-map) permite procesar cada elemento individual de la `trace` matriz por el `demangle` función. El resultado de este procesamiento se ve en el `trace_functions` columna de salida.
-
-``` text
-Row 1:
-──────
-trace_functions: DB::IAggregateFunctionHelper<DB::AggregateFunctionSum<unsigned long, unsigned long, DB::AggregateFunctionSumData<unsigned long> > >::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const
-DB::Aggregator::executeWithoutKeyImpl(char*&, unsigned long, DB::Aggregator::AggregateFunctionInstruction*, DB::Arena*) const
-DB::Aggregator::executeOnBlock(std::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >, unsigned long, DB::AggregatedDataVariants&, std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >&, std::vector<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >, std::allocator<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> > > >&, bool&)
-DB::Aggregator::executeOnBlock(DB::Block const&, DB::AggregatedDataVariants&, std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >&, std::vector<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >, std::allocator<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> > > >&, bool&)
-DB::Aggregator::execute(std::shared_ptr<DB::IBlockInputStream> const&, DB::AggregatedDataVariants&)
-DB::AggregatingBlockInputStream::readImpl()
-DB::IBlockInputStream::read()
-DB::ExpressionBlockInputStream::readImpl()
-DB::IBlockInputStream::read()
-DB::ExpressionBlockInputStream::readImpl()
-DB::IBlockInputStream::read()
-DB::AsynchronousBlockInputStream::calculate()
-std::_Function_handler<void (), DB::AsynchronousBlockInputStream::next()::{lambda()#1}>::_M_invoke(std::_Any_data const&)
-ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::_List_iterator<ThreadFromGlobalPool>)
-ThreadFromGlobalPool::ThreadFromGlobalPool<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}&&)::{lambda()#1}::operator()() const
-ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>)
-execute_native_thread_routine
-start_thread
-clone
-```
diff --git a/docs/es/sql-reference/functions/ip-address-functions.md b/docs/es/sql-reference/functions/ip-address-functions.md
deleted file mode 100644
index 6db0fe0584d3..000000000000
--- a/docs/es/sql-reference/functions/ip-address-functions.md
+++ /dev/null
@@ -1,248 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 55
-toc_title: Trabajar con direcciones IP
----
-
-# Funciones para trabajar con direcciones IP {#functions-for-working-with-ip-addresses}
-
-## Número de código: IPv4NumToString (num) {#ipv4numtostringnum}
-
-Toma un número UInt32. Lo interpreta como una dirección IPv4 en big endian. Devuelve una cadena que contiene la dirección IPv4 correspondiente en el formato A.B.C.d (números separados por puntos en forma decimal).
-
-## Nombre de la red inalámbrica (SSID):) {#ipv4stringtonums}
-
-La función inversa de IPv4NumToString. Si la dirección IPv4 tiene un formato no válido, devuelve 0.
-
-## Cómo hacer esto?) {#ipv4numtostringclasscnum}
-
-Similar a IPv4NumToString, pero usando xxx en lugar del último octeto.
-
-Ejemplo:
-
-``` sql
-SELECT
-    IPv4NumToStringClassC(ClientIP) AS k,
-    count() AS c
-FROM test.hits
-GROUP BY k
-ORDER BY c DESC
-LIMIT 10
-```
-
-``` text
-┌─k──────────────┬─────c─┐
-│ 83.149.9.xxx   │ 26238 │
-│ 217.118.81.xxx │ 26074 │
-│ 213.87.129.xxx │ 25481 │
-│ 83.149.8.xxx   │ 24984 │
-│ 217.118.83.xxx │ 22797 │
-│ 78.25.120.xxx  │ 22354 │
-│ 213.87.131.xxx │ 21285 │
-│ 78.25.121.xxx  │ 20887 │
-│ 188.162.65.xxx │ 19694 │
-│ 83.149.48.xxx  │ 17406 │
-└────────────────┴───────┘
-```
-
-Desde el uso ‘xxx’ es altamente inusual, esto puede cambiarse en el futuro. Le recomendamos que no confíe en el formato exacto de este fragmento.
-
-### ¿Cómo puedo hacerlo?) {#ipv6numtostringx}
-
-Acepta un valor FixedString(16) que contiene la dirección IPv6 en formato binario. Devuelve una cadena que contiene esta dirección en formato de texto.
-Las direcciones IPv4 asignadas a IPv6 se emiten en el formato ::ffff:111.222.33.44. Ejemplos:
-
-``` sql
-SELECT IPv6NumToString(toFixedString(unhex('2A0206B8000000000000000000000011'), 16)) AS addr
-```
-
-``` text
-┌─addr─────────┐
-│ 2a02:6b8::11 │
-└──────────────┘
-```
-
-``` sql
-SELECT
-    IPv6NumToString(ClientIP6 AS k),
-    count() AS c
-FROM hits_all
-WHERE EventDate = today() AND substring(ClientIP6, 1, 12) != unhex('00000000000000000000FFFF')
-GROUP BY k
-ORDER BY c DESC
-LIMIT 10
-```
-
-``` text
-┌─IPv6NumToString(ClientIP6)──────────────┬─────c─┐
-│ 2a02:2168:aaa:bbbb::2                   │ 24695 │
-│ 2a02:2698:abcd:abcd:abcd:abcd:8888:5555 │ 22408 │
-│ 2a02:6b8:0:fff::ff                      │ 16389 │
-│ 2a01:4f8:111:6666::2                    │ 16016 │
-│ 2a02:2168:888:222::1                    │ 15896 │
-│ 2a01:7e00::ffff:ffff:ffff:222           │ 14774 │
-│ 2a02:8109:eee:ee:eeee:eeee:eeee:eeee    │ 14443 │
-│ 2a02:810b:8888:888:8888:8888:8888:8888  │ 14345 │
-│ 2a02:6b8:0:444:4444:4444:4444:4444      │ 14279 │
-│ 2a01:7e00::ffff:ffff:ffff:ffff          │ 13880 │
-└─────────────────────────────────────────┴───────┘
-```
-
-``` sql
-SELECT
-    IPv6NumToString(ClientIP6 AS k),
-    count() AS c
-FROM hits_all
-WHERE EventDate = today()
-GROUP BY k
-ORDER BY c DESC
-LIMIT 10
-```
-
-``` text
-┌─IPv6NumToString(ClientIP6)─┬──────c─┐
-│ ::ffff:94.26.111.111       │ 747440 │
-│ ::ffff:37.143.222.4        │ 529483 │
-│ ::ffff:5.166.111.99        │ 317707 │
-│ ::ffff:46.38.11.77         │ 263086 │
-│ ::ffff:79.105.111.111      │ 186611 │
-│ ::ffff:93.92.111.88        │ 176773 │
-│ ::ffff:84.53.111.33        │ 158709 │
-│ ::ffff:217.118.11.22       │ 154004 │
-│ ::ffff:217.118.11.33       │ 148449 │
-│ ::ffff:217.118.11.44       │ 148243 │
-└────────────────────────────┴────────┘
-```
-
-## Nombre de la red inalámbrica (SSID):) {#ipv6stringtonums}
-
-La función inversa de IPv6NumToString. Si la dirección IPv6 tiene un formato no válido, devuelve una cadena de bytes nulos.
-HEX puede ser mayúscula o minúscula.
-
-## IPv4ToIPv6 (x) {#ipv4toipv6x}
-
-Toma un `UInt32` numero. Lo interpreta como una dirección IPv4 en [gran endian](https://en.wikipedia.org/wiki/Endianness). Devuelve un `FixedString(16)` valor que contiene la dirección IPv6 en formato binario. Ejemplos:
-
-``` sql
-SELECT IPv6NumToString(IPv4ToIPv6(IPv4StringToNum('192.168.0.1'))) AS addr
-```
-
-``` text
-┌─addr───────────────┐
-│ ::ffff:192.168.0.1 │
-└────────────────────┘
-```
-
-## Para obtener más información, consulta nuestra Política de privacidad y nuestras Condiciones de uso) {#cutipv6x-bytestocutforipv6-bytestocutforipv4}
-
-Acepta un valor FixedString(16) que contiene la dirección IPv6 en formato binario. Devuelve una cadena que contiene la dirección del número especificado de bytes eliminados en formato de texto. Por ejemplo:
-
-``` sql
-WITH
-    IPv6StringToNum('2001:0DB8:AC10:FE01:FEED:BABE:CAFE:F00D') AS ipv6,
-    IPv4ToIPv6(IPv4StringToNum('192.168.0.1')) AS ipv4
-SELECT
-    cutIPv6(ipv6, 2, 0),
-    cutIPv6(ipv4, 0, 2)
-```
-
-``` text
-┌─cutIPv6(ipv6, 2, 0)─────────────────┬─cutIPv6(ipv4, 0, 2)─┐
-│ 2001:db8:ac10:fe01:feed:babe:cafe:0 │ ::ffff:192.168.0.0  │
-└─────────────────────────────────────┴─────────────────────┘
-```
-
-## IPv4CIDRToRange (ipv4, Cidr), {#ipv4cidrtorangeipv4-cidr}
-
-Acepta un valor IPv4 y UInt8 que contiene el valor [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing). Devuelve una tupla con dos IPv4 que contienen el rango inferior y el rango superior de la subred.
-
-``` sql
-SELECT IPv4CIDRToRange(toIPv4('192.168.5.2'), 16)
-```
-
-``` text
-┌─IPv4CIDRToRange(toIPv4('192.168.5.2'), 16)─┐
-│ ('192.168.0.0','192.168.255.255')          │
-└────────────────────────────────────────────┘
-```
-
-## IPv6CIDRToRange(ipv6, Cidr), {#ipv6cidrtorangeipv6-cidr}
-
-Acepta un valor IPv6 y UInt8 que contiene el CIDR. Devuelve una tupla con dos IPv6 que contienen el rango inferior y el rango superior de la subred.
-
-``` sql
-SELECT IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32);
-```
-
-``` text
-┌─IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32)─┐
-│ ('2001:db8::','2001:db8:ffff:ffff:ffff:ffff:ffff:ffff')                │
-└────────────────────────────────────────────────────────────────────────┘
-```
-
-## Acerca de nosotros) {#toipv4string}
-
-Un alias para `IPv4StringToNum()` que toma una forma de cadena de dirección IPv4 y devuelve el valor de [IPv4](../../sql-reference/data-types/domains/ipv4.md) tipo, que es binario igual al valor devuelto por `IPv4StringToNum()`.
-
-``` sql
-WITH
-    '171.225.130.45' as IPv4_string
-SELECT
-    toTypeName(IPv4StringToNum(IPv4_string)),
-    toTypeName(toIPv4(IPv4_string))
-```
-
-``` text
-┌─toTypeName(IPv4StringToNum(IPv4_string))─┬─toTypeName(toIPv4(IPv4_string))─┐
-│ UInt32                                   │ IPv4                            │
-└──────────────────────────────────────────┴─────────────────────────────────┘
-```
-
-``` sql
-WITH
-    '171.225.130.45' as IPv4_string
-SELECT
-    hex(IPv4StringToNum(IPv4_string)),
-    hex(toIPv4(IPv4_string))
-```
-
-``` text
-┌─hex(IPv4StringToNum(IPv4_string))─┬─hex(toIPv4(IPv4_string))─┐
-│ ABE1822D                          │ ABE1822D                 │
-└───────────────────────────────────┴──────────────────────────┘
-```
-
-## Acerca de nosotros) {#toipv6string}
-
-Un alias para `IPv6StringToNum()` que toma una forma de cadena de dirección IPv6 y devuelve el valor de [IPv6](../../sql-reference/data-types/domains/ipv6.md) tipo, que es binario igual al valor devuelto por `IPv6StringToNum()`.
-
-``` sql
-WITH
-    '2001:438:ffff::407d:1bc1' as IPv6_string
-SELECT
-    toTypeName(IPv6StringToNum(IPv6_string)),
-    toTypeName(toIPv6(IPv6_string))
-```
-
-``` text
-┌─toTypeName(IPv6StringToNum(IPv6_string))─┬─toTypeName(toIPv6(IPv6_string))─┐
-│ FixedString(16)                          │ IPv6                            │
-└──────────────────────────────────────────┴─────────────────────────────────┘
-```
-
-``` sql
-WITH
-    '2001:438:ffff::407d:1bc1' as IPv6_string
-SELECT
-    hex(IPv6StringToNum(IPv6_string)),
-    hex(toIPv6(IPv6_string))
-```
-
-``` text
-┌─hex(IPv6StringToNum(IPv6_string))─┬─hex(toIPv6(IPv6_string))─────────┐
-│ 20010438FFFF000000000000407D1BC1  │ 20010438FFFF000000000000407D1BC1 │
-└───────────────────────────────────┴──────────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/ip_address_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/json-functions.md b/docs/es/sql-reference/functions/json-functions.md
deleted file mode 100644
index 09b66abb2b85..000000000000
--- a/docs/es/sql-reference/functions/json-functions.md
+++ /dev/null
@@ -1,297 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 56
-toc_title: Trabajar con JSON
----
-
-# Funciones para trabajar con JSON {#functions-for-working-with-json}
-
-En el Yandex.Metrica, JSON es transmitido por los usuarios como parámetros de sesión. Hay algunas funciones especiales para trabajar con este JSON. (Aunque en la mayoría de los casos, los JSON también se procesan previamente, y los valores resultantes se colocan en columnas separadas en su formato procesado.) Todas estas funciones se basan en sólidas suposiciones sobre lo que puede ser el JSON, pero tratan de hacer lo menos posible para hacer el trabajo.
-
-Se hacen las siguientes suposiciones:
-
-1.  El nombre de campo (argumento de función) debe ser una constante.
-2.  El nombre del campo de alguna manera está codificado canónicamente en JSON. Por ejemplo: `visitParamHas('{"abc":"def"}', 'abc') = 1`, pero `visitParamHas('{"\\u0061\\u0062\\u0063":"def"}', 'abc') = 0`
-3.  Los campos se buscan en cualquier nivel de anidación, indiscriminadamente. Si hay varios campos coincidentes, se utiliza la primera aparición.
-4.  El JSON no tiene caracteres de espacio fuera de los literales de cadena.
-
-## visitParamHas (params, nombre) {#visitparamhasparams-name}
-
-Comprueba si hay un campo con el ‘name’ nombre.
-
-## visitParamExtractUInt (params, nombre) {#visitparamextractuintparams-name}
-
-Analiza UInt64 a partir del valor del campo denominado ‘name’. Si se trata de un campo de cadena, intenta analizar un número desde el principio de la cadena. Si el campo no existe, o existe pero no contiene un número, devuelve 0.
-
-## visitParamExtractInt (params, nombre) {#visitparamextractintparams-name}
-
-Lo mismo que para Int64.
-
-## visitParamExtractFloat (params, nombre) {#visitparamextractfloatparams-name}
-
-Lo mismo que para Float64.
-
-## visitParamExtractBool (params, nombre) {#visitparamextractboolparams-name}
-
-Analiza un valor verdadero/falso. El resultado es UInt8.
-
-## visitParamExtractRaw (params, nombre) {#visitparamextractrawparams-name}
-
-Devuelve el valor de un campo, incluidos los separadores.
-
-Ejemplos:
-
-``` sql
-visitParamExtractRaw('{"abc":"\
\\u0000"}', 'abc') = '"\
\\u0000"'
-visitParamExtractRaw('{"abc":{"def":[1,2,3]}}', 'abc') = '{"def":[1,2,3]}'
-```
-
-## visitParamExtractString(params, nombre) {#visitparamextractstringparams-name}
-
-Analiza la cadena entre comillas dobles. El valor es sin escape. Si no se pudo desescapar, devuelve una cadena vacía.
-
-Ejemplos:
-
-``` sql
-visitParamExtractString('{"abc":"\
\\u0000"}', 'abc') = '
\0'
-visitParamExtractString('{"abc":"\\u263a"}', 'abc') = '☺'
-visitParamExtractString('{"abc":"\\u263"}', 'abc') = ''
-visitParamExtractString('{"abc":"hello}', 'abc') = ''
-```
-
-Actualmente no hay soporte para puntos de código en el formato `\uXXXX\uYYYY` que no son del plano multilingüe básico (se convierten a CESU-8 en lugar de UTF-8).
-
-Las siguientes funciones se basan en [simdjson](https://github.com/lemire/simdjson) diseñado para requisitos de análisis JSON más complejos. La suposición 2 mencionada anteriormente todavía se aplica.
-
-## ¿Qué puedes encontrar en Neodigit) {#isvalidjsonjson}
-
-Comprueba que la cadena pasada es un json válido.
-
-Ejemplos:
-
-``` sql
-SELECT isValidJSON('{"a": "hello", "b": [-100, 200.0, 300]}') = 1
-SELECT isValidJSON('not a json') = 0
-```
-
-## JSONHas(json\[, indices_or_keys\]…) {#jsonhasjson-indices-or-keys}
-
-Si el valor existe en el documento JSON, `1` serán devueltos.
-
-Si el valor no existe, `0` serán devueltos.
-
-Ejemplos:
-
-``` sql
-SELECT JSONHas('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = 1
-SELECT JSONHas('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 4) = 0
-```
-
-`indices_or_keys` es una lista de cero o más argumentos, cada uno de ellos puede ser de cadena o entero.
-
--   Cadena = miembro del objeto de acceso por clave.
--   Entero positivo = acceder al n-ésimo miembro / clave desde el principio.
--   Entero negativo = acceder al n-ésimo miembro / clave desde el final.
-
-El índice mínimo del elemento es 1. Por lo tanto, el elemento 0 no existe.
-
-Puede usar enteros para acceder a matrices JSON y objetos JSON.
-
-Entonces, por ejemplo:
-
-``` sql
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', 1) = 'a'
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', 2) = 'b'
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', -1) = 'b'
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', -2) = 'a'
-SELECT JSONExtractString('{"a": "hello", "b": [-100, 200.0, 300]}', 1) = 'hello'
-```
-
-## JSONLength(json\[, indices_or_keys\]…) {#jsonlengthjson-indices-or-keys}
-
-Devuelve la longitud de una matriz JSON o un objeto JSON.
-
-Si el valor no existe o tiene un tipo incorrecto, `0` serán devueltos.
-
-Ejemplos:
-
-``` sql
-SELECT JSONLength('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = 3
-SELECT JSONLength('{"a": "hello", "b": [-100, 200.0, 300]}') = 2
-```
-
-## JSONType(json\[, indices_or_keys\]…) {#jsontypejson-indices-or-keys}
-
-Devuelve el tipo de un valor JSON.
-
-Si el valor no existe, `Null` serán devueltos.
-
-Ejemplos:
-
-``` sql
-SELECT JSONType('{"a": "hello", "b": [-100, 200.0, 300]}') = 'Object'
-SELECT JSONType('{"a": "hello", "b": [-100, 200.0, 300]}', 'a') = 'String'
-SELECT JSONType('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = 'Array'
-```
-
-## JSONExtractUInt(json\[, indices_or_keys\]…) {#jsonextractuintjson-indices-or-keys}
-
-## JSONExtractInt(json\[, indices_or_keys\]…) {#jsonextractintjson-indices-or-keys}
-
-## JSONExtractFloat(json\[, indices_or_keys\]…) {#jsonextractfloatjson-indices-or-keys}
-
-## JSONExtractBool(json\[, indices_or_keys\]…) {#jsonextractbooljson-indices-or-keys}
-
-Analiza un JSON y extrae un valor. Estas funciones son similares a `visitParam` función.
-
-Si el valor no existe o tiene un tipo incorrecto, `0` serán devueltos.
-
-Ejemplos:
-
-``` sql
-SELECT JSONExtractInt('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 1) = -100
-SELECT JSONExtractFloat('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 2) = 200.0
-SELECT JSONExtractUInt('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', -1) = 300
-```
-
-## JSONExtractString(json\[, indices_or_keys\]…) {#jsonextractstringjson-indices-or-keys}
-
-Analiza un JSON y extrae una cadena. Esta función es similar a `visitParamExtractString` función.
-
-Si el valor no existe o tiene un tipo incorrecto, se devolverá una cadena vacía.
-
-El valor es sin escape. Si no se pudo desescapar, devuelve una cadena vacía.
-
-Ejemplos:
-
-``` sql
-SELECT JSONExtractString('{"a": "hello", "b": [-100, 200.0, 300]}', 'a') = 'hello'
-SELECT JSONExtractString('{"abc":"\
\\u0000"}', 'abc') = '
\0'
-SELECT JSONExtractString('{"abc":"\\u263a"}', 'abc') = '☺'
-SELECT JSONExtractString('{"abc":"\\u263"}', 'abc') = ''
-SELECT JSONExtractString('{"abc":"hello}', 'abc') = ''
-```
-
-## JSONExtract(json\[, indices_or_keys…\], Return_type) {#jsonextractjson-indices-or-keys-return-type}
-
-Analiza un JSON y extrae un valor del tipo de datos ClickHouse dado.
-
-Esta es una generalización de la anterior `JSONExtract<type>` función.
-Esto significa
-`JSONExtract(..., 'String')` devuelve exactamente lo mismo que `JSONExtractString()`,
-`JSONExtract(..., 'Float64')` devuelve exactamente lo mismo que `JSONExtractFloat()`.
-
-Ejemplos:
-
-``` sql
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'Tuple(String, Array(Float64))') = ('hello',[-100,200,300])
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'Tuple(b Array(Float64), a String)') = ([-100,200,300],'hello')
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 'Array(Nullable(Int8))') = [-100, NULL, NULL]
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 4, 'Nullable(Int64)') = NULL
-SELECT JSONExtract('{"passed": true}', 'passed', 'UInt8') = 1
-SELECT JSONExtract('{"day": "Thursday"}', 'day', 'Enum8(\'Sunday\' = 0, \'Monday\' = 1, \'Tuesday\' = 2, \'Wednesday\' = 3, \'Thursday\' = 4, \'Friday\' = 5, \'Saturday\' = 6)') = 'Thursday'
-SELECT JSONExtract('{"day": 5}', 'day', 'Enum8(\'Sunday\' = 0, \'Monday\' = 1, \'Tuesday\' = 2, \'Wednesday\' = 3, \'Thursday\' = 4, \'Friday\' = 5, \'Saturday\' = 6)') = 'Friday'
-```
-
-## JSONExtractKeysAndValues(json\[, indices_or_keys…\], Value_type) {#jsonextractkeysandvaluesjson-indices-or-keys-value-type}
-
-Analiza los pares clave-valor de un JSON donde los valores son del tipo de datos ClickHouse especificado.
-
-Ejemplo:
-
-``` sql
-SELECT JSONExtractKeysAndValues('{"x": {"a": 5, "b": 7, "c": 11}}', 'x', 'Int8') = [('a',5),('b',7),('c',11)]
-```
-
-## JSONExtractRaw(json\[, indices_or_keys\]…) {#jsonextractrawjson-indices-or-keys}
-
-Devuelve una parte de JSON como cadena sin analizar.
-
-Si la pieza no existe o tiene un tipo incorrecto, se devolverá una cadena vacía.
-
-Ejemplo:
-
-``` sql
-SELECT JSONExtractRaw('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = '[-100, 200.0, 300]'
-```
-
-## JSONExtractArrayRaw(json\[, indices_or_keys…\]) {#jsonextractarrayrawjson-indices-or-keys}
-
-Devuelve una matriz con elementos de matriz JSON, cada uno representado como cadena sin analizar.
-
-Si la parte no existe o no es una matriz, se devolverá una matriz vacía.
-
-Ejemplo:
-
-``` sql
-SELECT JSONExtractArrayRaw('{"a": "hello", "b": [-100, 200.0, "hello"]}', 'b') = ['-100', '200.0', '"hello"']'
-```
-
-## JSONExtractKeysAndValuesRaw {#json-extract-keys-and-values-raw}
-
-Extrae datos sin procesar de un objeto JSON.
-
-**Sintaxis**
-
-``` sql
-JSONExtractKeysAndValuesRaw(json[, p, a, t, h])
-```
-
-**Parámetros**
-
--   `json` — [Cadena](../data-types/string.md) con JSON válido.
--   `p, a, t, h` — Comma-separated indices or keys that specify the path to the inner field in a nested JSON object. Each argument can be either a [cadena](../data-types/string.md) para obtener el campo por la clave o un [entero](../data-types/int-uint.md) para obtener el campo N-ésimo (indexado desde 1, los enteros negativos cuentan desde el final). Si no se establece, todo el JSON se analiza como el objeto de nivel superior. Parámetro opcional.
-
-**Valores devueltos**
-
--   Matriz con `('key', 'value')` tuplas. Ambos miembros de tupla son cadenas.
--   Vacíe la matriz si el objeto solicitado no existe o si la entrada JSON no es válida.
-
-Tipo: [Matriz](../data-types/array.md)([Tupla](../data-types/tuple.md)([Cadena](../data-types/string.md), [Cadena](../data-types/string.md)).
-
-**Ejemplos**
-
-Consulta:
-
-``` sql
-SELECT JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}')
-```
-
-Resultado:
-
-``` text
-┌─JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}')─┐
-│ [('a','[-100,200]'),('b','{"c":{"d":"hello","f":"world"}}')]                                 │
-└──────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', 'b')
-```
-
-Resultado:
-
-``` text
-┌─JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', 'b')─┐
-│ [('c','{"d":"hello","f":"world"}')]                                                               │
-└───────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', -1, 'c')
-```
-
-Resultado:
-
-``` text
-┌─JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', -1, 'c')─┐
-│ [('d','"hello"'),('f','"world"')]                                                                     │
-└───────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/json_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/logical-functions.md b/docs/es/sql-reference/functions/logical-functions.md
deleted file mode 100644
index 9588c219128c..000000000000
--- a/docs/es/sql-reference/functions/logical-functions.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: "L\xF3gico"
----
-
-# Funciones lógicas {#logical-functions}
-
-Las funciones lógicas aceptan cualquier tipo numérico, pero devuelven un número UInt8 igual a 0 o 1.
-
-Cero como argumento se considera “false,” mientras que cualquier valor distinto de cero se considera “true”.
-
-## y, Y operador {#and-and-operator}
-
-## o, operador O {#or-or-operator}
-
-## no, NO operador {#not-not-operator}
-
-## xor {#xor}
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/logical_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/machine-learning-functions.md b/docs/es/sql-reference/functions/machine-learning-functions.md
deleted file mode 100644
index 70f8e3baf1b2..000000000000
--- a/docs/es/sql-reference/functions/machine-learning-functions.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 64
-toc_title: "Funciones de aprendizaje autom\xE1tico"
----
-
-# Funciones de aprendizaje automático {#machine-learning-functions}
-
-## evalMLMethod (predicción) {#machine_learning_methods-evalmlmethod}
-
-Predicción utilizando modelos de regresión ajustados utiliza `evalMLMethod` función. Ver enlace en `linearRegression`.
-
-### Regresión lineal estocástica {#stochastic-linear-regression}
-
-El [stochasticLinearRegression](../../sql-reference/aggregate-functions/reference.md#agg_functions-stochasticlinearregression) la función agregada implementa el método de descenso de gradiente estocástico utilizando el modelo lineal y la función de pérdida MSE. Utilizar `evalMLMethod` para predecir sobre nuevos datos.
-
-### Regresión logística estocástica {#stochastic-logistic-regression}
-
-El [stochasticLogisticRegression](../../sql-reference/aggregate-functions/reference.md#agg_functions-stochasticlogisticregression) la función de agregado implementa el método de descenso de gradiente estocástico para el problema de clasificación binaria. Utilizar `evalMLMethod` para predecir sobre nuevos datos.
diff --git a/docs/es/sql-reference/functions/math-functions.md b/docs/es/sql-reference/functions/math-functions.md
deleted file mode 100644
index fe1a3615ef21..000000000000
--- a/docs/es/sql-reference/functions/math-functions.md
+++ /dev/null
@@ -1,116 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: "Matem\xE1tica"
----
-
-# Funciones matemáticas {#mathematical-functions}
-
-Todas las funciones devuelven un número Float64. La precisión del resultado es cercana a la precisión máxima posible, pero el resultado puede no coincidir con el número representable de la máquina más cercano al número real correspondiente.
-
-## e() {#e}
-
-Devuelve un número Float64 que está cerca del número e.
-
-## Ciudad() {#pi}
-
-Returns a Float64 number that is close to the number π.
-
-## exp(x) {#expx}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano al exponente del argumento.
-
-## Información) {#logx-lnx}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano al logaritmo natural del argumento.
-
-## exp2(x) {#exp2x}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano a 2 a la potencia de x.
-
-## log2 (x) {#log2x}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano al logaritmo binario del argumento.
-
-## exp10 (x) {#exp10x}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano a 10 a la potencia de x.
-
-## log10 (x) {#log10x}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano al logaritmo decimal del argumento.
-
-## sqrt(x) {#sqrtx}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano a la raíz cuadrada del argumento.
-
-## Cbrt (x) {#cbrtx}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano a la raíz cúbica del argumento.
-
-## erf(x) {#erfx}
-
-Si ‘x’ no es negativo, entonces `erf(x / σ√2)` es la probabilidad de que una variable aleatoria tenga una distribución normal con desviación estándar ‘σ’ toma el valor que está separado del valor esperado en más de ‘x’.
-
-Ejemplo (regla de tres sigma):
-
-``` sql
-SELECT erf(3 / sqrt(2))
-```
-
-``` text
-┌─erf(divide(3, sqrt(2)))─┐
-│      0.9973002039367398 │
-└─────────────────────────┘
-```
-
-## erfc(x) {#erfcx}
-
-Acepta un argumento numérico y devuelve un número Float64 cercano a 1 - erf(x), pero sin pérdida de precisión para grandes ‘x’ valor.
-
-## Lgamma (x) {#lgammax}
-
-El logaritmo de la función gamma.
-
-## ¿Qué puedes encontrar en Neodigit) {#tgammax}
-
-Función gamma.
-
-## sin(x) {#sinx}
-
-Sinusoidal.
-
-## cos(x) {#cosx}
-
-El coseno.
-
-## pantalla) {#tanx}
-
-Tangente.
-
-## (x) {#asinx}
-
-El arco sinusoidal.
-
-## Acerca de) {#acosx}
-
-El arco coseno.
-
-## atan (x) {#atanx}
-
-La tangente del arco.
-
-## pow(x, y), potencia(x, y) {#powx-y-powerx-y}
-
-Toma dos argumentos numéricos x e y. Devuelve un número Float64 cercano a x a la potencia de y.
-
-## IntExp2 {#intexp2}
-
-Acepta un argumento numérico y devuelve un número UInt64 cercano a 2 a la potencia de x.
-
-## IntExp10 {#intexp10}
-
-Acepta un argumento numérico y devuelve un número UInt64 cercano a 10 a la potencia de x.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/math_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/other-functions.md b/docs/es/sql-reference/functions/other-functions.md
deleted file mode 100644
index 3704355167aa..000000000000
--- a/docs/es/sql-reference/functions/other-functions.md
+++ /dev/null
@@ -1,1205 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 66
-toc_title: Otro
----
-
-# Otras funciones {#other-functions}
-
-## nombre de host() {#hostname}
-
-Devuelve una cadena con el nombre del host en el que se realizó esta función. Para el procesamiento distribuido, este es el nombre del host del servidor remoto, si la función se realiza en un servidor remoto.
-
-## getMacro {#getmacro}
-
-Obtiene un valor con nombre del [macro](../../operations/server-configuration-parameters/settings.md#macros) sección de la configuración del servidor.
-
-**Sintaxis**
-
-``` sql
-getMacro(name);
-```
-
-**Parámetros**
-
--   `name` — Name to retrieve from the `macros` apartado. [Cadena](../../sql-reference/data-types/string.md#string).
-
-**Valor devuelto**
-
--   Valor de la macro especificada.
-
-Tipo: [Cadena](../../sql-reference/data-types/string.md).
-
-**Ejemplo**
-
-Ejemplo `macros` sección en el archivo de configuración del servidor:
-
-``` xml
-<macros>
-    <test>Value</test>
-</macros>
-```
-
-Consulta:
-
-``` sql
-SELECT getMacro('test');
-```
-
-Resultado:
-
-``` text
-┌─getMacro('test')─┐
-│ Value            │
-└──────────────────┘
-```
-
-Una forma alternativa de obtener el mismo valor:
-
-``` sql
-SELECT * FROM system.macros
-WHERE macro = 'test';
-```
-
-``` text
-┌─macro─┬─substitution─┐
-│ test  │ Value        │
-└───────┴──────────────┘
-```
-
-## FQDN {#fqdn}
-
-Devuelve el nombre de dominio completo.
-
-**Sintaxis**
-
-``` sql
-fqdn();
-```
-
-Esta función no distingue entre mayúsculas y minúsculas.
-
-**Valor devuelto**
-
--   Cadena con el nombre de dominio completo.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT FQDN();
-```
-
-Resultado:
-
-``` text
-┌─FQDN()──────────────────────────┐
-│ clickhouse.ru-central1.internal │
-└─────────────────────────────────┘
-```
-
-## Nombre básico {#basename}
-
-Extrae la parte final de una cadena después de la última barra o barra invertida. Esta función se utiliza a menudo para extraer el nombre de archivo de una ruta.
-
-``` sql
-basename( expr )
-```
-
-**Parámetros**
-
--   `expr` — Expression resulting in a [Cadena](../../sql-reference/data-types/string.md) valor de tipo. Todas las barras diagonales inversas deben escaparse en el valor resultante.
-
-**Valor devuelto**
-
-Una cadena que contiene:
-
--   La parte final de una cadena después de la última barra o barra invertida.
-
-        If the input string contains a path ending with slash or backslash, for example, `/` or `c:\`, the function returns an empty string.
-
--   La cadena original si no hay barras diagonales o barras diagonales inversas.
-
-**Ejemplo**
-
-``` sql
-SELECT 'some/long/path/to/file' AS a, basename(a)
-```
-
-``` text
-┌─a──────────────────────┬─basename('some\\long\\path\\to\\file')─┐
-│ some\long\path\to\file │ file                                   │
-└────────────────────────┴────────────────────────────────────────┘
-```
-
-``` sql
-SELECT 'some\\long\\path\\to\\file' AS a, basename(a)
-```
-
-``` text
-┌─a──────────────────────┬─basename('some\\long\\path\\to\\file')─┐
-│ some\long\path\to\file │ file                                   │
-└────────────────────────┴────────────────────────────────────────┘
-```
-
-``` sql
-SELECT 'some-file-name' AS a, basename(a)
-```
-
-``` text
-┌─a──────────────┬─basename('some-file-name')─┐
-│ some-file-name │ some-file-name             │
-└────────────────┴────────────────────────────┘
-```
-
-## Ancho visible (x) {#visiblewidthx}
-
-Calcula el ancho aproximado al enviar valores a la consola en formato de texto (separado por tabuladores).
-Esta función es utilizada por el sistema para implementar formatos Pretty.
-
-`NULL` se representa como una cadena correspondiente a `NULL` en `Pretty` formato.
-
-``` sql
-SELECT visibleWidth(NULL)
-```
-
-``` text
-┌─visibleWidth(NULL)─┐
-│                  4 │
-└────────────────────┘
-```
-
-## ¿Cómo puedo hacerlo?) {#totypenamex}
-
-Devuelve una cadena que contiene el nombre de tipo del argumento pasado.
-
-Si `NULL` se pasa a la función como entrada, luego devuelve el `Nullable(Nothing)` tipo, que corresponde a un tipo interno `NULL` representación en ClickHouse.
-
-## BlockSize() {#function-blocksize}
-
-Obtiene el tamaño del bloque.
-En ClickHouse, las consultas siempre se ejecutan en bloques (conjuntos de partes de columna). Esta función permite obtener el tamaño del bloque al que lo llamó.
-
-## materializar (x) {#materializex}
-
-Convierte una constante en una columna completa que contiene solo un valor.
-En ClickHouse, las columnas completas y las constantes se representan de manera diferente en la memoria. Las funciones funcionan de manera diferente para argumentos constantes y argumentos normales (se ejecuta un código diferente), aunque el resultado es casi siempre el mismo. Esta función es para depurar este comportamiento.
-
-## ignore(…) {#ignore}
-
-Acepta cualquier argumento, incluyendo `NULL`. Siempre devuelve 0.
-Sin embargo, el argumento aún se evalúa. Esto se puede usar para puntos de referencia.
-
-## sueño (segundos) {#sleepseconds}
-
-Dormir ‘seconds’ segundos en cada bloque de datos. Puede especificar un número entero o un número de punto flotante.
-
-## sleepEachRow(segundos) {#sleepeachrowseconds}
-
-Dormir ‘seconds’ segundos en cada fila. Puede especificar un número entero o un número de punto flotante.
-
-## currentDatabase() {#currentdatabase}
-
-Devuelve el nombre de la base de datos actual.
-Puede utilizar esta función en los parámetros del motor de tablas en una consulta CREATE TABLE donde debe especificar la base de datos.
-
-## currentUser() {#other-function-currentuser}
-
-Devuelve el inicio de sesión del usuario actual. El inicio de sesión del usuario, que inició la consulta, se devolverá en caso de consulta distibuted.
-
-``` sql
-SELECT currentUser();
-```
-
-Apodo: `user()`, `USER()`.
-
-**Valores devueltos**
-
--   Inicio de sesión del usuario actual.
--   Inicio de sesión del usuario que inició la consulta en caso de consulta distribuida.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT currentUser();
-```
-
-Resultado:
-
-``` text
-┌─currentUser()─┐
-│ default       │
-└───────────────┘
-```
-
-## isConstant {#is-constant}
-
-Comprueba si el argumento es una expresión constante.
-
-A constant expression means an expression whose resulting value is known at the query analysis (i.e. before execution). For example, expressions over [literal](../syntax.md#literals) son expresiones constantes.
-
-La función está destinada al desarrollo, depuración y demostración.
-
-**Sintaxis**
-
-``` sql
-isConstant(x)
-```
-
-**Parámetros**
-
--   `x` — Expression to check.
-
-**Valores devueltos**
-
--   `1` — `x` es constante.
--   `0` — `x` no es constante.
-
-Tipo: [UInt8](../data-types/int-uint.md).
-
-**Ejemplos**
-
-Consulta:
-
-``` sql
-SELECT isConstant(x + 1) FROM (SELECT 43 AS x)
-```
-
-Resultado:
-
-``` text
-┌─isConstant(plus(x, 1))─┐
-│                      1 │
-└────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-WITH 3.14 AS pi SELECT isConstant(cos(pi))
-```
-
-Resultado:
-
-``` text
-┌─isConstant(cos(pi))─┐
-│                   1 │
-└─────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT isConstant(number) FROM numbers(1)
-```
-
-Resultado:
-
-``` text
-┌─isConstant(number)─┐
-│                  0 │
-└────────────────────┘
-```
-
-## isFinite(x) {#isfinitex}
-
-Acepta Float32 y Float64 y devuelve UInt8 igual a 1 si el argumento no es infinito y no es un NaN, de lo contrario 0.
-
-## IsInfinite(x) {#isinfinitex}
-
-Acepta Float32 y Float64 y devuelve UInt8 igual a 1 si el argumento es infinito, de lo contrario 0. Tenga en cuenta que se devuelve 0 para un NaN.
-
-## ifNotFinite {#ifnotfinite}
-
-Comprueba si el valor de punto flotante es finito.
-
-**Sintaxis**
-
-    ifNotFinite(x,y)
-
-**Parámetros**
-
--   `x` — Value to be checked for infinity. Type: [Flotante\*](../../sql-reference/data-types/float.md).
--   `y` — Fallback value. Type: [Flotante\*](../../sql-reference/data-types/float.md).
-
-**Valor devuelto**
-
--   `x` si `x` es finito.
--   `y` si `x` no es finito.
-
-**Ejemplo**
-
-Consulta:
-
-    SELECT 1/0 as infimum, ifNotFinite(infimum,42)
-
-Resultado:
-
-    ┌─infimum─┬─ifNotFinite(divide(1, 0), 42)─┐
-    │     inf │                            42 │
-    └─────────┴───────────────────────────────┘
-
-Puede obtener un resultado similar usando [operador ternario](conditional-functions.md#ternary-operator): `isFinite(x) ? x : y`.
-
-## isNaN(x) {#isnanx}
-
-Acepta Float32 y Float64 y devuelve UInt8 igual a 1 si el argumento es un NaN, de lo contrario 0.
-
-## hasColumnInTable(\[‘hostname’\[, ‘username’\[, ‘password’\]\],\] ‘database’, ‘table’, ‘column’) {#hascolumnintablehostname-username-password-database-table-column}
-
-Acepta cadenas constantes: nombre de base de datos, nombre de tabla y nombre de columna. Devuelve una expresión constante UInt8 igual a 1 si hay una columna; de lo contrario, 0. Si se establece el parámetro hostname, la prueba se ejecutará en un servidor remoto.
-La función produce una excepción si la tabla no existe.
-Para los elementos de una estructura de datos anidada, la función comprueba la existencia de una columna. Para la propia estructura de datos anidados, la función devuelve 0.
-
-## Bar {#function-bar}
-
-Permite construir un diagrama unicode-art.
-
-`bar(x, min, max, width)` dibuja una banda con un ancho proporcional a `(x - min)` e igual a `width` caracteres cuando `x = max`.
-
-Parámetros:
-
--   `x` — Size to display.
--   `min, max` — Integer constants. The value must fit in `Int64`.
--   `width` — Constant, positive integer, can be fractional.
-
-La banda se dibuja con precisión a un octavo de un símbolo.
-
-Ejemplo:
-
-``` sql
-SELECT
-    toHour(EventTime) AS h,
-    count() AS c,
-    bar(c, 0, 600000, 20) AS bar
-FROM test.hits
-GROUP BY h
-ORDER BY h ASC
-```
-
-``` text
-┌──h─┬──────c─┬─bar────────────────┐
-│  0 │ 292907 │ █████████▋         │
-│  1 │ 180563 │ ██████             │
-│  2 │ 114861 │ ███▋               │
-│  3 │  85069 │ ██▋                │
-│  4 │  68543 │ ██▎                │
-│  5 │  78116 │ ██▌                │
-│  6 │ 113474 │ ███▋               │
-│  7 │ 170678 │ █████▋             │
-│  8 │ 278380 │ █████████▎         │
-│  9 │ 391053 │ █████████████      │
-│ 10 │ 457681 │ ███████████████▎   │
-│ 11 │ 493667 │ ████████████████▍  │
-│ 12 │ 509641 │ ████████████████▊  │
-│ 13 │ 522947 │ █████████████████▍ │
-│ 14 │ 539954 │ █████████████████▊ │
-│ 15 │ 528460 │ █████████████████▌ │
-│ 16 │ 539201 │ █████████████████▊ │
-│ 17 │ 523539 │ █████████████████▍ │
-│ 18 │ 506467 │ ████████████████▊  │
-│ 19 │ 520915 │ █████████████████▎ │
-│ 20 │ 521665 │ █████████████████▍ │
-│ 21 │ 542078 │ ██████████████████ │
-│ 22 │ 493642 │ ████████████████▍  │
-│ 23 │ 400397 │ █████████████▎     │
-└────┴────────┴────────────────────┘
-```
-
-## transformar {#transform}
-
-Transforma un valor de acuerdo con la asignación explícitamente definida de algunos elementos a otros.
-Hay dos variaciones de esta función:
-
-### ¿Cómo puedo hacerlo?) {#transformx-array-from-array-to-default}
-
-`x` – What to transform.
-
-`array_from` – Constant array of values for converting.
-
-`array_to` – Constant array of values to convert the values in ‘from’ a.
-
-`default` – Which value to use if ‘x’ no es igual a ninguno de los valores en ‘from’.
-
-`array_from` y `array_to` – Arrays of the same size.
-
-Tipo:
-
-`transform(T, Array(T), Array(U), U) -> U`
-
-`T` y `U` pueden ser tipos numéricos, de cadena o de fecha o de fecha y hora.
-Cuando se indica la misma letra (T o U), para los tipos numéricos pueden no ser tipos coincidentes, sino tipos que tienen un tipo común.
-Por ejemplo, el primer argumento puede tener el tipo Int64, mientras que el segundo tiene el tipo Array(UInt16).
-
-Si el ‘x’ valor es igual a uno de los elementos en el ‘array_from’ matriz, devuelve el elemento existente (que está numerado igual) de la ‘array_to’ matriz. De lo contrario, devuelve ‘default’. Si hay varios elementos coincidentes en ‘array_from’, devuelve una de las coincidencias.
-
-Ejemplo:
-
-``` sql
-SELECT
-    transform(SearchEngineID, [2, 3], ['Yandex', 'Google'], 'Other') AS title,
-    count() AS c
-FROM test.hits
-WHERE SearchEngineID != 0
-GROUP BY title
-ORDER BY c DESC
-```
-
-``` text
-┌─title─────┬──────c─┐
-│ Yandex    │ 498635 │
-│ Google    │ 229872 │
-│ Other     │ 104472 │
-└───────────┴────────┘
-```
-
-### ¿Cómo puedo hacerlo?) {#transformx-array-from-array-to}
-
-Difiere de la primera variación en que el ‘default’ se omite el argumento.
-Si el ‘x’ valor es igual a uno de los elementos en el ‘array_from’ matriz, devuelve el elemento coincidente (que está numerado igual) de la ‘array_to’ matriz. De lo contrario, devuelve ‘x’.
-
-Tipo:
-
-`transform(T, Array(T), Array(T)) -> T`
-
-Ejemplo:
-
-``` sql
-SELECT
-    transform(domain(Referer), ['yandex.ru', 'google.ru', 'vk.com'], ['www.yandex', 'example.com']) AS s,
-    count() AS c
-FROM test.hits
-GROUP BY domain(Referer)
-ORDER BY count() DESC
-LIMIT 10
-```
-
-``` text
-┌─s──────────────┬───────c─┐
-│                │ 2906259 │
-│ www.yandex     │  867767 │
-│ ███████.ru     │  313599 │
-│ mail.yandex.ru │  107147 │
-│ ██████.ru      │  100355 │
-│ █████████.ru   │   65040 │
-│ news.yandex.ru │   64515 │
-│ ██████.net     │   59141 │
-│ example.com    │   57316 │
-└────────────────┴─────────┘
-```
-
-## Tamaño de formatoReadable (x) {#formatreadablesizex}
-
-Acepta el tamaño (número de bytes). Devuelve un tamaño redondeado con un sufijo (KiB, MiB, etc.) como una cadena.
-
-Ejemplo:
-
-``` sql
-SELECT
-    arrayJoin([1, 1024, 1024*1024, 192851925]) AS filesize_bytes,
-    formatReadableSize(filesize_bytes) AS filesize
-```
-
-``` text
-┌─filesize_bytes─┬─filesize───┐
-│              1 │ 1.00 B     │
-│           1024 │ 1.00 KiB   │
-│        1048576 │ 1.00 MiB   │
-│      192851925 │ 183.92 MiB │
-└────────────────┴────────────┘
-```
-
-## menos (a, b) {#leasta-b}
-
-Devuelve el valor más pequeño de a y b.
-
-## mayor(a, b) {#greatesta-b}
-
-Devuelve el valor más grande de a y b.
-
-## operatividad() {#uptime}
-
-Devuelve el tiempo de actividad del servidor en segundos.
-
-## versión() {#version}
-
-Devuelve la versión del servidor como una cadena.
-
-## Zona horaria() {#timezone}
-
-Devuelve la zona horaria del servidor.
-
-## blockNumber {#blocknumber}
-
-Devuelve el número de secuencia del bloque de datos donde se encuentra la fila.
-
-## rowNumberInBlock {#function-rownumberinblock}
-
-Devuelve el número ordinal de la fila en el bloque de datos. Los diferentes bloques de datos siempre se recalculan.
-
-## rowNumberInAllBlocks() {#rownumberinallblocks}
-
-Devuelve el número ordinal de la fila en el bloque de datos. Esta función solo considera los bloques de datos afectados.
-
-## vecino {#neighbor}
-
-La función de ventana que proporciona acceso a una fila en un desplazamiento especificado que viene antes o después de la fila actual de una columna determinada.
-
-**Sintaxis**
-
-``` sql
-neighbor(column, offset[, default_value])
-```
-
-El resultado de la función depende de los bloques de datos afectados y del orden de los datos en el bloque.
-Si realiza una subconsulta con ORDER BY y llama a la función desde fuera de la subconsulta, puede obtener el resultado esperado.
-
-**Parámetros**
-
--   `column` — A column name or scalar expression.
--   `offset` — The number of rows forwards or backwards from the current row of `column`. [Int64](../../sql-reference/data-types/int-uint.md).
--   `default_value` — Optional. The value to be returned if offset goes beyond the scope of the block. Type of data blocks affected.
-
-**Valores devueltos**
-
--   Valor para `column` en `offset` distancia de la fila actual si `offset` valor no está fuera de los límites del bloque.
--   Valor predeterminado para `column` si `offset` valor está fuera de los límites del bloque. Si `default_value` se da, entonces será utilizado.
-
-Tipo: tipo de bloques de datos afectados o tipo de valor predeterminado.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT number, neighbor(number, 2) FROM system.numbers LIMIT 10;
-```
-
-Resultado:
-
-``` text
-┌─number─┬─neighbor(number, 2)─┐
-│      0 │                   2 │
-│      1 │                   3 │
-│      2 │                   4 │
-│      3 │                   5 │
-│      4 │                   6 │
-│      5 │                   7 │
-│      6 │                   8 │
-│      7 │                   9 │
-│      8 │                   0 │
-│      9 │                   0 │
-└────────┴─────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT number, neighbor(number, 2, 999) FROM system.numbers LIMIT 10;
-```
-
-Resultado:
-
-``` text
-┌─number─┬─neighbor(number, 2, 999)─┐
-│      0 │                        2 │
-│      1 │                        3 │
-│      2 │                        4 │
-│      3 │                        5 │
-│      4 │                        6 │
-│      5 │                        7 │
-│      6 │                        8 │
-│      7 │                        9 │
-│      8 │                      999 │
-│      9 │                      999 │
-└────────┴──────────────────────────┘
-```
-
-Esta función se puede utilizar para calcular el valor métrico interanual:
-
-Consulta:
-
-``` sql
-WITH toDate('2018-01-01') AS start_date
-SELECT
-    toStartOfMonth(start_date + (number * 32)) AS month,
-    toInt32(month) % 100 AS money,
-    neighbor(money, -12) AS prev_year,
-    round(prev_year / money, 2) AS year_over_year
-FROM numbers(16)
-```
-
-Resultado:
-
-``` text
-┌──────month─┬─money─┬─prev_year─┬─year_over_year─┐
-│ 2018-01-01 │    32 │         0 │              0 │
-│ 2018-02-01 │    63 │         0 │              0 │
-│ 2018-03-01 │    91 │         0 │              0 │
-│ 2018-04-01 │    22 │         0 │              0 │
-│ 2018-05-01 │    52 │         0 │              0 │
-│ 2018-06-01 │    83 │         0 │              0 │
-│ 2018-07-01 │    13 │         0 │              0 │
-│ 2018-08-01 │    44 │         0 │              0 │
-│ 2018-09-01 │    75 │         0 │              0 │
-│ 2018-10-01 │     5 │         0 │              0 │
-│ 2018-11-01 │    36 │         0 │              0 │
-│ 2018-12-01 │    66 │         0 │              0 │
-│ 2019-01-01 │    97 │        32 │           0.33 │
-│ 2019-02-01 │    28 │        63 │           2.25 │
-│ 2019-03-01 │    56 │        91 │           1.62 │
-│ 2019-04-01 │    87 │        22 │           0.25 │
-└────────────┴───────┴───────────┴────────────────┘
-```
-
-## EjecuciónDiferencia (x) {#other_functions-runningdifference}
-
-Calculates the difference between successive row values ​​in the data block.
-Devuelve 0 para la primera fila y la diferencia con respecto a la fila anterior para cada fila subsiguiente.
-
-El resultado de la función depende de los bloques de datos afectados y del orden de los datos en el bloque.
-Si realiza una subconsulta con ORDER BY y llama a la función desde fuera de la subconsulta, puede obtener el resultado esperado.
-
-Ejemplo:
-
-``` sql
-SELECT
-    EventID,
-    EventTime,
-    runningDifference(EventTime) AS delta
-FROM
-(
-    SELECT
-        EventID,
-        EventTime
-    FROM events
-    WHERE EventDate = '2016-11-24'
-    ORDER BY EventTime ASC
-    LIMIT 5
-)
-```
-
-``` text
-┌─EventID─┬───────────EventTime─┬─delta─┐
-│    1106 │ 2016-11-24 00:00:04 │     0 │
-│    1107 │ 2016-11-24 00:00:05 │     1 │
-│    1108 │ 2016-11-24 00:00:05 │     0 │
-│    1109 │ 2016-11-24 00:00:09 │     4 │
-│    1110 │ 2016-11-24 00:00:10 │     1 │
-└─────────┴─────────────────────┴───────┘
-```
-
-Tenga en cuenta que el tamaño del bloque afecta el resultado. Con cada nuevo bloque, el `runningDifference` estado de reset.
-
-``` sql
-SELECT
-    number,
-    runningDifference(number + 1) AS diff
-FROM numbers(100000)
-WHERE diff != 1
-```
-
-``` text
-┌─number─┬─diff─┐
-│      0 │    0 │
-└────────┴──────┘
-┌─number─┬─diff─┐
-│  65536 │    0 │
-└────────┴──────┘
-```
-
-``` sql
-set max_block_size=100000 -- default value is 65536!
-
-SELECT
-    number,
-    runningDifference(number + 1) AS diff
-FROM numbers(100000)
-WHERE diff != 1
-```
-
-``` text
-┌─number─┬─diff─┐
-│      0 │    0 │
-└────────┴──────┘
-```
-
-## runningDifferenceStartingWithFirstvalue {#runningdifferencestartingwithfirstvalue}
-
-Lo mismo que para [runningDifference](./other-functions.md#other_functions-runningdifference), la diferencia es el valor de la primera fila, devolvió el valor de la primera fila, y cada fila subsiguiente devuelve la diferencia de la fila anterior.
-
-## ¿Cómo puedo hacerlo?) {#macnumtostringnum}
-
-Acepta un número UInt64. Lo interpreta como una dirección MAC en big endian. Devuelve una cadena que contiene la dirección MAC correspondiente con el formato AA:BB:CC:DD:EE:FF (números separados por dos puntos en forma hexadecimal).
-
-## Sistema abierto.) {#macstringtonums}
-
-La función inversa de MACNumToString. Si la dirección MAC tiene un formato no válido, devuelve 0.
-
-## Sistema abierto.) {#macstringtoouis}
-
-Acepta una dirección MAC con el formato AA:BB:CC:DD:EE:FF (números separados por dos puntos en forma hexadecimal). Devuelve los primeros tres octetos como un número UInt64. Si la dirección MAC tiene un formato no válido, devuelve 0.
-
-## getSizeOfEnumType {#getsizeofenumtype}
-
-Devuelve el número de campos en [Enum](../../sql-reference/data-types/enum.md).
-
-``` sql
-getSizeOfEnumType(value)
-```
-
-**Parámetros:**
-
--   `value` — Value of type `Enum`.
-
-**Valores devueltos**
-
--   El número de campos con `Enum` valores de entrada.
--   Se produce una excepción si el tipo no es `Enum`.
-
-**Ejemplo**
-
-``` sql
-SELECT getSizeOfEnumType( CAST('a' AS Enum8('a' = 1, 'b' = 2) ) ) AS x
-```
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-## BlockSerializedSize {#blockserializedsize}
-
-Devuelve el tamaño en el disco (sin tener en cuenta la compresión).
-
-``` sql
-blockSerializedSize(value[, value[, ...]])
-```
-
-**Parámetros:**
-
--   `value` — Any value.
-
-**Valores devueltos**
-
--   El número de bytes que se escribirán en el disco para el bloque de valores (sin compresión).
-
-**Ejemplo**
-
-``` sql
-SELECT blockSerializedSize(maxState(1)) as x
-```
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-## ToColumnTypeName {#tocolumntypename}
-
-Devuelve el nombre de la clase que representa el tipo de datos de la columna en la RAM.
-
-``` sql
-toColumnTypeName(value)
-```
-
-**Parámetros:**
-
--   `value` — Any type of value.
-
-**Valores devueltos**
-
--   Una cadena con el nombre de la clase que se utiliza para representar la `value` tipo de datos en la memoria RAM.
-
-**Ejemplo de la diferencia entre`toTypeName ' and ' toColumnTypeName`**
-
-``` sql
-SELECT toTypeName(CAST('2018-01-01 01:02:03' AS DateTime))
-```
-
-``` text
-┌─toTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐
-│ DateTime                                            │
-└─────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT toColumnTypeName(CAST('2018-01-01 01:02:03' AS DateTime))
-```
-
-``` text
-┌─toColumnTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐
-│ Const(UInt32)                                             │
-└───────────────────────────────────────────────────────────┘
-```
-
-El ejemplo muestra que el `DateTime` tipo de datos se almacena en la memoria como `Const(UInt32)`.
-
-## dumpColumnStructure {#dumpcolumnstructure}
-
-Produce una descripción detallada de las estructuras de datos en la memoria RAM
-
-``` sql
-dumpColumnStructure(value)
-```
-
-**Parámetros:**
-
--   `value` — Any type of value.
-
-**Valores devueltos**
-
--   Una cadena que describe la estructura que se utiliza para representar el `value` tipo de datos en la memoria RAM.
-
-**Ejemplo**
-
-``` sql
-SELECT dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))
-```
-
-``` text
-┌─dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))─┐
-│ DateTime, Const(size = 1, UInt32(size = 1))                  │
-└──────────────────────────────────────────────────────────────┘
-```
-
-## defaultValueOfArgumentType {#defaultvalueofargumenttype}
-
-Genera el valor predeterminado para el tipo de datos.
-
-No incluye valores predeterminados para columnas personalizadas establecidas por el usuario.
-
-``` sql
-defaultValueOfArgumentType(expression)
-```
-
-**Parámetros:**
-
--   `expression` — Arbitrary type of value or an expression that results in a value of an arbitrary type.
-
-**Valores devueltos**
-
--   `0` para los números.
--   Cadena vacía para cadenas.
--   `ᴺᵁᴸᴸ` para [NULL](../../sql-reference/data-types/nullable.md).
-
-**Ejemplo**
-
-``` sql
-SELECT defaultValueOfArgumentType( CAST(1 AS Int8) )
-```
-
-``` text
-┌─defaultValueOfArgumentType(CAST(1, 'Int8'))─┐
-│                                           0 │
-└─────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT defaultValueOfArgumentType( CAST(1 AS Nullable(Int8) ) )
-```
-
-``` text
-┌─defaultValueOfArgumentType(CAST(1, 'Nullable(Int8)'))─┐
-│                                                  ᴺᵁᴸᴸ │
-└───────────────────────────────────────────────────────┘
-```
-
-## replicar {#other-functions-replicate}
-
-Crea una matriz con un solo valor.
-
-Utilizado para la implementación interna de [arrayJoin](array-join.md#functions_arrayjoin).
-
-``` sql
-SELECT replicate(x, arr);
-```
-
-**Parámetros:**
-
--   `arr` — Original array. ClickHouse creates a new array of the same length as the original and fills it with the value `x`.
--   `x` — The value that the resulting array will be filled with.
-
-**Valor devuelto**
-
-Una matriz llena con el valor `x`.
-
-Tipo: `Array`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT replicate(1, ['a', 'b', 'c'])
-```
-
-Resultado:
-
-``` text
-┌─replicate(1, ['a', 'b', 'c'])─┐
-│ [1,1,1]                       │
-└───────────────────────────────┘
-```
-
-## Sistema de archivosDisponible {#filesystemavailable}
-
-Devuelve la cantidad de espacio restante en el sistema de archivos donde se encuentran los archivos de las bases de datos. Siempre es más pequeño que el espacio libre total ([Sistema de archivosLibre](#filesystemfree)) porque algo de espacio está reservado para el sistema operativo.
-
-**Sintaxis**
-
-``` sql
-filesystemAvailable()
-```
-
-**Valor devuelto**
-
--   La cantidad de espacio restante disponible en bytes.
-
-Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT formatReadableSize(filesystemAvailable()) AS "Available space", toTypeName(filesystemAvailable()) AS "Type";
-```
-
-Resultado:
-
-``` text
-┌─Available space─┬─Type───┐
-│ 30.75 GiB       │ UInt64 │
-└─────────────────┴────────┘
-```
-
-## Sistema de archivosLibre {#filesystemfree}
-
-Devuelve la cantidad total del espacio libre en el sistema de archivos donde se encuentran los archivos de las bases de datos. Ver también `filesystemAvailable`
-
-**Sintaxis**
-
-``` sql
-filesystemFree()
-```
-
-**Valor devuelto**
-
--   Cantidad de espacio libre en bytes.
-
-Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT formatReadableSize(filesystemFree()) AS "Free space", toTypeName(filesystemFree()) AS "Type";
-```
-
-Resultado:
-
-``` text
-┌─Free space─┬─Type───┐
-│ 32.39 GiB  │ UInt64 │
-└────────────┴────────┘
-```
-
-## sistema de archivosCapacidad {#filesystemcapacity}
-
-Devuelve la capacidad del sistema de archivos en bytes. Para la evaluación, el [camino](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-path) al directorio de datos debe estar configurado.
-
-**Sintaxis**
-
-``` sql
-filesystemCapacity()
-```
-
-**Valor devuelto**
-
--   Información de capacidad del sistema de archivos en bytes.
-
-Tipo: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT formatReadableSize(filesystemCapacity()) AS "Capacity", toTypeName(filesystemCapacity()) AS "Type"
-```
-
-Resultado:
-
-``` text
-┌─Capacity──┬─Type───┐
-│ 39.32 GiB │ UInt64 │
-└───────────┴────────┘
-```
-
-## finalizeAggregation {#function-finalizeaggregation}
-
-Toma el estado de la función agregada. Devuelve el resultado de la agregación (estado finalizado).
-
-## runningAccumulate {#function-runningaccumulate}
-
-Toma los estados de la función agregada y devuelve una columna con valores, son el resultado de la acumulación de estos estados para un conjunto de líneas de bloque, desde la primera hasta la línea actual.
-Por ejemplo, toma el estado de la función agregada (ejemplo runningAccumulate(uniqState(UserID)) ), y para cada fila de bloque, devuelve el resultado de la función agregada en la fusión de estados de todas las filas anteriores y la fila actual.
-Por lo tanto, el resultado de la función depende de la partición de los datos en los bloques y del orden de los datos en el bloque.
-
-## joinGet {#joinget}
-
-La función le permite extraer datos de la tabla de la misma manera que [diccionario](../../sql-reference/dictionaries/index.md).
-
-Obtiene datos de [Unir](../../engines/table-engines/special/join.md#creating-a-table) usando la clave de unión especificada.
-
-Solo admite tablas creadas con `ENGINE = Join(ANY, LEFT, <join_keys>)` instrucción.
-
-**Sintaxis**
-
-``` sql
-joinGet(join_storage_table_name, `value_column`, join_keys)
-```
-
-**Parámetros**
-
--   `join_storage_table_name` — an [identificador](../syntax.md#syntax-identifiers) indica dónde se realiza la búsqueda. El identificador se busca en la base de datos predeterminada (ver parámetro `default_database` en el archivo de configuración). Para reemplazar la base de datos predeterminada, utilice `USE db_name` o especifique la base de datos y la tabla a través del separador `db_name.db_table`, ver el ejemplo.
--   `value_column` — name of the column of the table that contains required data.
--   `join_keys` — list of keys.
-
-**Valor devuelto**
-
-Devuelve la lista de valores correspondientes a la lista de claves.
-
-Si cierto no existe en la tabla fuente, entonces `0` o `null` será devuelto basado en [Sistema abierto.](../../operations/settings/settings.md#join_use_nulls) configuración.
-
-Más información sobre `join_use_nulls` en [Únase a la operación](../../engines/table-engines/special/join.md).
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` sql
-CREATE DATABASE db_test
-CREATE TABLE db_test.id_val(`id` UInt32, `val` UInt32) ENGINE = Join(ANY, LEFT, id) SETTINGS join_use_nulls = 1
-INSERT INTO db_test.id_val VALUES (1,11)(2,12)(4,13)
-```
-
-``` text
-┌─id─┬─val─┐
-│  4 │  13 │
-│  2 │  12 │
-│  1 │  11 │
-└────┴─────┘
-```
-
-Consulta:
-
-``` sql
-SELECT joinGet(db_test.id_val,'val',toUInt32(number)) from numbers(4) SETTINGS join_use_nulls = 1
-```
-
-Resultado:
-
-``` text
-┌─joinGet(db_test.id_val, 'val', toUInt32(number))─┐
-│                                                0 │
-│                                               11 │
-│                                               12 │
-│                                                0 │
-└──────────────────────────────────────────────────┘
-```
-
-## modelEvaluate(model_name, …) {#function-modelevaluate}
-
-Evaluar modelo externo.
-Acepta un nombre de modelo y argumentos de modelo. Devuelve Float64.
-
-## ¿Cómo puedo hacerlo?\]) {#throwifx-custom-message}
-
-Lance una excepción si el argumento no es cero.
-custom_message - es un parámetro opcional: una cadena constante, proporciona un mensaje de error
-
-``` sql
-SELECT throwIf(number = 3, 'Too many') FROM numbers(10);
-```
-
-``` text
-↙ Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) Received exception from server (version 19.14.1):
-Code: 395. DB::Exception: Received from localhost:9000. DB::Exception: Too many.
-```
-
-## identidad {#identity}
-
-Devuelve el mismo valor que se usó como argumento. Se utiliza para la depuración y pruebas, permite cancelar el uso de índice, y obtener el rendimiento de la consulta de un análisis completo. Cuando se analiza la consulta para el posible uso del índice, el analizador no mira dentro `identity` función.
-
-**Sintaxis**
-
-``` sql
-identity(x)
-```
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT identity(42)
-```
-
-Resultado:
-
-``` text
-┌─identity(42)─┐
-│           42 │
-└──────────────┘
-```
-
-## randomPrintableASCII {#randomascii}
-
-Genera una cadena con un conjunto aleatorio de [ASCII](https://en.wikipedia.org/wiki/ASCII#Printable_characters) caracteres imprimibles.
-
-**Sintaxis**
-
-``` sql
-randomPrintableASCII(length)
-```
-
-**Parámetros**
-
--   `length` — Resulting string length. Positive integer.
-
-        If you pass `length < 0`, behavior of the function is undefined.
-
-**Valor devuelto**
-
--   Cadena con un conjunto aleatorio de [ASCII](https://en.wikipedia.org/wiki/ASCII#Printable_characters) caracteres imprimibles.
-
-Tipo: [Cadena](../../sql-reference/data-types/string.md)
-
-**Ejemplo**
-
-``` sql
-SELECT number, randomPrintableASCII(30) as str, length(str) FROM system.numbers LIMIT 3
-```
-
-``` text
-┌─number─┬─str────────────────────────────┬─length(randomPrintableASCII(30))─┐
-│      0 │ SuiCOSTvC0csfABSw=UcSzp2.`rv8x │                               30 │
-│      1 │ 1Ag NlJ &RCN:*>HVPG;PE-nO"SUFD │                               30 │
-│      2 │ /"+<"wUTh:=LjJ Vm!c&hI*m#XTfzz │                               30 │
-└────────┴────────────────────────────────┴──────────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/other_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/random-functions.md b/docs/es/sql-reference/functions/random-functions.md
deleted file mode 100644
index a614c217546b..000000000000
--- a/docs/es/sql-reference/functions/random-functions.md
+++ /dev/null
@@ -1,65 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 51
-toc_title: "Generaci\xF3n de n\xFAmeros pseudo-aleatorios"
----
-
-# Funciones para generar números pseudoaleatorios {#functions-for-generating-pseudo-random-numbers}
-
-Se utilizan generadores no criptográficos de números pseudoaleatorios.
-
-Todas las funciones aceptan cero argumentos o un argumento.
-Si se pasa un argumento, puede ser de cualquier tipo y su valor no se usa para nada.
-El único propósito de este argumento es evitar la eliminación de subexpresiones comunes, de modo que dos instancias diferentes de la misma función devuelvan columnas diferentes con números aleatorios diferentes.
-
-## rand {#rand}
-
-Devuelve un número pseudoaleatorio UInt32, distribuido uniformemente entre todos los números de tipo UInt32.
-Utiliza un generador congruente lineal.
-
-## rand64 {#rand64}
-
-Devuelve un número pseudoaleatorio UInt64, distribuido uniformemente entre todos los números de tipo UInt64.
-Utiliza un generador congruente lineal.
-
-## randConstant {#randconstant}
-
-Produce una columna constante con un valor aleatorio.
-
-**Sintaxis**
-
-``` sql
-randConstant([x])
-```
-
-**Parámetros**
-
--   `x` — [Expresion](../syntax.md#syntax-expressions) resultante en cualquiera de los [tipos de datos compatibles](../data-types/index.md#data_types). El valor resultante se descarta, pero la expresión en sí si se usa para omitir [Eliminación de subexpresiones común](index.md#common-subexpression-elimination) si la función se llama varias veces en una consulta. Parámetro opcional.
-
-**Valor devuelto**
-
--   Número pseudoaleatorio.
-
-Tipo: [UInt32](../data-types/int-uint.md).
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT rand(), rand(1), rand(number), randConstant(), randConstant(1), randConstant(number)
-FROM numbers(3)
-```
-
-Resultado:
-
-``` text
-┌─────rand()─┬────rand(1)─┬─rand(number)─┬─randConstant()─┬─randConstant(1)─┬─randConstant(number)─┐
-│ 3047369878 │ 4132449925 │   4044508545 │     2740811946 │      4229401477 │           1924032898 │
-│ 2938880146 │ 1267722397 │   4154983056 │     2740811946 │      4229401477 │           1924032898 │
-│  956619638 │ 4238287282 │   1104342490 │     2740811946 │      4229401477 │           1924032898 │
-└────────────┴────────────┴──────────────┴────────────────┴─────────────────┴──────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/random_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/rounding-functions.md b/docs/es/sql-reference/functions/rounding-functions.md
deleted file mode 100644
index 1c231d4df1eb..000000000000
--- a/docs/es/sql-reference/functions/rounding-functions.md
+++ /dev/null
@@ -1,190 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: Redondeo
----
-
-# Funciones de redondeo {#rounding-functions}
-
-## piso(x\[, N\]) {#floorx-n}
-
-Devuelve el número de ronda más grande que es menor o igual que `x`. Un número redondo es un múltiplo de 1 / 10N, o el número más cercano del tipo de datos apropiado si 1 / 10N no es exacto.
-‘N’ es una constante entera, parámetro opcional. Por defecto es cero, lo que significa redondear a un entero.
-‘N’ puede ser negativo.
-
-Ejemplos: `floor(123.45, 1) = 123.4, floor(123.45, -1) = 120.`
-
-`x` es cualquier tipo numérico. El resultado es un número del mismo tipo.
-Para argumentos enteros, tiene sentido redondear con un negativo `N` valor no negativo `N` la función no hace nada).
-Si el redondeo causa desbordamiento (por ejemplo, floor(-128, -1)), se devuelve un resultado específico de la implementación.
-
-## Por ejemplo:\]) {#ceilx-n-ceilingx-n}
-
-Devuelve el número redondo más pequeño que es mayor o igual que `x`. En todos los demás sentidos, es lo mismo que el `floor` función (véase más arriba).
-
-## ¿Cómo puedo hacerlo?\]) {#truncx-n-truncatex-n}
-
-Devuelve el número redondo con el valor absoluto más grande que tiene un valor absoluto menor o igual que `x`‘s. In every other way, it is the same as the ’floor’ función (véase más arriba).
-
-## Ronda (x\[, N\]) {#rounding_functions-round}
-
-Redondea un valor a un número especificado de decimales.
-
-La función devuelve el número más cercano del orden especificado. En caso de que el número dado tenga la misma distancia que los números circundantes, la función utiliza el redondeo del banquero para los tipos de números flotantes y se redondea desde cero para los otros tipos de números.
-
-``` sql
-round(expression [, decimal_places])
-```
-
-**Parámetros:**
-
--   `expression` — A number to be rounded. Can be any [expresion](../syntax.md#syntax-expressions) devolviendo el numérico [tipo de datos](../../sql-reference/data-types/index.md#data_types).
--   `decimal-places` — An integer value.
-    -   Si `decimal-places > 0` luego la función redondea el valor a la derecha del punto decimal.
-    -   Si `decimal-places < 0` luego la función redondea el valor a la izquierda del punto decimal.
-    -   Si `decimal-places = 0` entonces la función redondea el valor a entero. En este caso, el argumento puede omitirse.
-
-**Valor devuelto:**
-
-El número redondeado del mismo tipo que el número de entrada.
-
-### Ejemplos {#examples}
-
-**Ejemplo de uso**
-
-``` sql
-SELECT number / 2 AS x, round(x) FROM system.numbers LIMIT 3
-```
-
-``` text
-┌───x─┬─round(divide(number, 2))─┐
-│   0 │                        0 │
-│ 0.5 │                        0 │
-│   1 │                        1 │
-└─────┴──────────────────────────┘
-```
-
-**Ejemplos de redondeo**
-
-Redondeando al número más cercano.
-
-``` text
-round(3.2, 0) = 3
-round(4.1267, 2) = 4.13
-round(22,-1) = 20
-round(467,-2) = 500
-round(-467,-2) = -500
-```
-
-Redondeo del banquero.
-
-``` text
-round(3.5) = 4
-round(4.5) = 4
-round(3.55, 1) = 3.6
-round(3.65, 1) = 3.6
-```
-
-**Ver también**
-
--   [roundBankers](#roundbankers)
-
-## roundBankers {#roundbankers}
-
-Redondea un número a una posición decimal especificada.
-
--   Si el número de redondeo está a medio camino entre dos números, la función utiliza el redondeo del banquero.
-
-        Banker's rounding is a method of rounding fractional numbers. When the rounding number is halfway between two numbers, it's rounded to the nearest even digit at the specified decimal position. For example: 3.5 rounds up to 4, 2.5 rounds down to 2.
-
-        It's the default rounding method for floating point numbers defined in [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754#Roundings_to_nearest). The [round](#rounding_functions-round) function performs the same rounding for floating point numbers. The `roundBankers` function also rounds integers the same way, for example, `roundBankers(45, -1) = 40`.
-
--   En otros casos, la función redondea los números al entero más cercano.
-
-Usando el redondeo del banquero, puede reducir el efecto que tiene el redondeo de números en los resultados de sumar o restar estos números.
-
-Por ejemplo, suma números 1.5, 2.5, 3.5, 4.5 con redondeo diferente:
-
--   Sin redondeo: 1.5 + 2.5 + 3.5 + 4.5 = 12.
--   Redondeo del banquero: 2 + 2 + 4 + 4 = 12.
--   Redondeando al entero más cercano: 2 + 3 + 4 + 5 = 14.
-
-**Sintaxis**
-
-``` sql
-roundBankers(expression [, decimal_places])
-```
-
-**Parámetros**
-
--   `expression` — A number to be rounded. Can be any [expresion](../syntax.md#syntax-expressions) devolviendo el numérico [tipo de datos](../../sql-reference/data-types/index.md#data_types).
--   `decimal-places` — Decimal places. An integer number.
-    -   `decimal-places > 0` — The function rounds the number to the given position right of the decimal point. Example: `roundBankers(3.55, 1) = 3.6`.
-    -   `decimal-places < 0` — The function rounds the number to the given position left of the decimal point. Example: `roundBankers(24.55, -1) = 20`.
-    -   `decimal-places = 0` — The function rounds the number to an integer. In this case the argument can be omitted. Example: `roundBankers(2.5) = 2`.
-
-**Valor devuelto**
-
-Un valor redondeado por el método de redondeo del banquero.
-
-### Ejemplos {#examples-1}
-
-**Ejemplo de uso**
-
-Consulta:
-
-``` sql
- SELECT number / 2 AS x, roundBankers(x, 0) AS b fROM system.numbers limit 10
-```
-
-Resultado:
-
-``` text
-┌───x─┬─b─┐
-│   0 │ 0 │
-│ 0.5 │ 0 │
-│   1 │ 1 │
-│ 1.5 │ 2 │
-│   2 │ 2 │
-│ 2.5 │ 2 │
-│   3 │ 3 │
-│ 3.5 │ 4 │
-│   4 │ 4 │
-│ 4.5 │ 4 │
-└─────┴───┘
-```
-
-**Ejemplos de redondeo de Banker**
-
-``` text
-roundBankers(0.4) = 0
-roundBankers(-3.5) = -4
-roundBankers(4.5) = 4
-roundBankers(3.55, 1) = 3.6
-roundBankers(3.65, 1) = 3.6
-roundBankers(10.35, 1) = 10.4
-roundBankers(10.755, 2) = 11,76
-```
-
-**Ver también**
-
--   [ronda](#rounding_functions-round)
-
-## ¿Cómo puedo hacerlo?) {#roundtoexp2num}
-
-Acepta un número. Si el número es menor que uno, devuelve 0. De lo contrario, redondea el número al grado más cercano (todo no negativo) de dos.
-
-## RondaDuración(num) {#rounddurationnum}
-
-Acepta un número. Si el número es menor que uno, devuelve 0. De lo contrario, redondea el número a números del conjunto: 1, 10, 30, 60, 120, 180, 240, 300, 600, 1200, 1800, 3600, 7200, 18000, 36000. Esta función es específica de Yandex.Métrica y se utiliza para aplicar el informe sobre la duración del período de sesiones.
-
-## RondaEdad(num) {#roundagenum}
-
-Acepta un número. Si el número es menor que 18, devuelve 0. De lo contrario, redondea el número a un número del conjunto: 18, 25, 35, 45, 55. Esta función es específica de Yandex.Métrica y se utiliza para la aplicación del informe sobre la edad del usuario.
-
-## ¿Cómo puedo hacerlo?) {#rounddownnum-arr}
-
-Acepta un número y lo redondea a un elemento en la matriz especificada. Si el valor es menor que el límite más bajo, se devuelve el límite más bajo.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/rounding_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/splitting-merging-functions.md b/docs/es/sql-reference/functions/splitting-merging-functions.md
deleted file mode 100644
index d3809f8bf72c..000000000000
--- a/docs/es/sql-reference/functions/splitting-merging-functions.md
+++ /dev/null
@@ -1,116 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: "Divisi\xF3n y fusi\xF3n de cuerdas y matrices"
----
-
-# Funciones para dividir y fusionar cuerdas y matrices {#functions-for-splitting-and-merging-strings-and-arrays}
-
-## Por ejemplo:) {#splitbycharseparator-s}
-
-Divide una cadena en subcadenas separadas por un carácter especificado. Utiliza una cadena constante `separator` que consiste en exactamente un carácter.
-Devuelve una matriz de subcadenas seleccionadas. Se pueden seleccionar subcadenas vacías si el separador aparece al principio o al final de la cadena, o si hay varios separadores consecutivos.
-
-**Sintaxis**
-
-``` sql
-splitByChar(<separator>, <s>)
-```
-
-**Parámetros**
-
--   `separator` — The separator which should contain exactly one character. [Cadena](../../sql-reference/data-types/string.md).
--   `s` — The string to split. [Cadena](../../sql-reference/data-types/string.md).
-
-**Valores devueltos)**
-
-Devuelve una matriz de subcadenas seleccionadas. Las subcadenas vacías se pueden seleccionar cuando:
-
--   Se produce un separador al principio o al final de la cadena;
--   Hay varios separadores consecutivos;
--   La cadena original `s` está vacío.
-
-Tipo: [Matriz](../../sql-reference/data-types/array.md) de [Cadena](../../sql-reference/data-types/string.md).
-
-**Ejemplo**
-
-``` sql
-SELECT splitByChar(',', '1,2,3,abcde')
-```
-
-``` text
-┌─splitByChar(',', '1,2,3,abcde')─┐
-│ ['1','2','3','abcde']           │
-└─────────────────────────────────┘
-```
-
-## Por ejemplo:) {#splitbystringseparator-s}
-
-Divide una cadena en subcadenas separadas por una cadena. Utiliza una cadena constante `separator` de múltiples caracteres como separador. Si la cadena `separator` está vacío, dividirá la cadena `s` en una matriz de caracteres individuales.
-
-**Sintaxis**
-
-``` sql
-splitByString(<separator>, <s>)
-```
-
-**Parámetros**
-
--   `separator` — The separator. [Cadena](../../sql-reference/data-types/string.md).
--   `s` — The string to split. [Cadena](../../sql-reference/data-types/string.md).
-
-**Valores devueltos)**
-
-Devuelve una matriz de subcadenas seleccionadas. Las subcadenas vacías se pueden seleccionar cuando:
-
-Tipo: [Matriz](../../sql-reference/data-types/array.md) de [Cadena](../../sql-reference/data-types/string.md).
-
--   Se produce un separador no vacío al principio o al final de la cadena;
--   Hay varios separadores consecutivos no vacíos;
--   La cadena original `s` está vacío mientras el separador no está vacío.
-
-**Ejemplo**
-
-``` sql
-SELECT splitByString(', ', '1, 2 3, 4,5, abcde')
-```
-
-``` text
-┌─splitByString(', ', '1, 2 3, 4,5, abcde')─┐
-│ ['1','2 3','4,5','abcde']                 │
-└───────────────────────────────────────────┘
-```
-
-``` sql
-SELECT splitByString('', 'abcde')
-```
-
-``` text
-┌─splitByString('', 'abcde')─┐
-│ ['a','b','c','d','e']      │
-└────────────────────────────┘
-```
-
-## Por ejemplo, se puede usar una matriz.\]) {#arraystringconcatarr-separator}
-
-Concatena las cadenas enumeradas en la matriz con el separador.'separador' es un parámetro opcional: una constante de cadena, establece una cadena vacía por defecto.
-Devuelve la cadena.
-
-## Sistema abierto.) {#alphatokenss}
-
-Selecciona subcadenas de bytes consecutivos de los rangos a-z y A-Z.Devuelve una matriz de subcadenas.
-
-**Ejemplo**
-
-``` sql
-SELECT alphaTokens('abca1abc')
-```
-
-``` text
-┌─alphaTokens('abca1abc')─┐
-│ ['abca','abc']          │
-└─────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/splitting_merging_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/string-functions.md b/docs/es/sql-reference/functions/string-functions.md
deleted file mode 100644
index 43025465b31c..000000000000
--- a/docs/es/sql-reference/functions/string-functions.md
+++ /dev/null
@@ -1,489 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: Trabajar con cadenas
----
-
-# Funciones para trabajar con cadenas {#functions-for-working-with-strings}
-
-## vaciar {#empty}
-
-Devuelve 1 para una cadena vacía o 0 para una cadena no vacía.
-El tipo de resultado es UInt8.
-Una cadena se considera no vacía si contiene al menos un byte, incluso si se trata de un espacio o un byte nulo.
-La función también funciona para matrices.
-
-## notEmpty {#notempty}
-
-Devuelve 0 para una cadena vacía o 1 para una cadena no vacía.
-El tipo de resultado es UInt8.
-La función también funciona para matrices.
-
-## longitud {#length}
-
-Devuelve la longitud de una cadena en bytes (no en caracteres y no en puntos de código).
-El tipo de resultado es UInt64.
-La función también funciona para matrices.
-
-## longitudUTF8 {#lengthutf8}
-
-Devuelve la longitud de una cadena en puntos de código Unicode (no en caracteres), suponiendo que la cadena contiene un conjunto de bytes que componen texto codificado en UTF-8. Si no se cumple esta suposición, devuelve algún resultado (no arroja una excepción).
-El tipo de resultado es UInt64.
-
-## char_length, CHAR_LENGTH {#char-length}
-
-Devuelve la longitud de una cadena en puntos de código Unicode (no en caracteres), suponiendo que la cadena contiene un conjunto de bytes que componen texto codificado en UTF-8. Si no se cumple esta suposición, devuelve algún resultado (no arroja una excepción).
-El tipo de resultado es UInt64.
-
-## character_length, CHARACTER_LENGTH {#character-length}
-
-Devuelve la longitud de una cadena en puntos de código Unicode (no en caracteres), suponiendo que la cadena contiene un conjunto de bytes que componen texto codificado en UTF-8. Si no se cumple esta suposición, devuelve algún resultado (no arroja una excepción).
-El tipo de resultado es UInt64.
-
-## inferior, lcase {#lower}
-
-Convierte símbolos latinos ASCII en una cadena a minúsculas.
-
-## superior, ucase {#upper}
-
-Convierte los símbolos latinos ASCII en una cadena a mayúsculas.
-
-## Método de codificación de datos: {#lowerutf8}
-
-Convierte una cadena en minúsculas, suponiendo que la cadena contiene un conjunto de bytes que componen un texto codificado en UTF-8.
-No detecta el idioma. Entonces, para el turco, el resultado podría no ser exactamente correcto.
-Si la longitud de la secuencia de bytes UTF-8 es diferente para mayúsculas y minúsculas de un punto de código, el resultado puede ser incorrecto para este punto de código.
-Si la cadena contiene un conjunto de bytes que no es UTF-8, entonces el comportamiento no está definido.
-
-## superiorUTF8 {#upperutf8}
-
-Convierte una cadena en mayúsculas, suponiendo que la cadena contiene un conjunto de bytes que componen un texto codificado en UTF-8.
-No detecta el idioma. Entonces, para el turco, el resultado podría no ser exactamente correcto.
-Si la longitud de la secuencia de bytes UTF-8 es diferente para mayúsculas y minúsculas de un punto de código, el resultado puede ser incorrecto para este punto de código.
-Si la cadena contiene un conjunto de bytes que no es UTF-8, entonces el comportamiento no está definido.
-
-## Sistema abierto {#isvalidutf8}
-
-Devuelve 1, si el conjunto de bytes es válido codificado en UTF-8, de lo contrario 0.
-
-## Acerca de Nosotros {#tovalidutf8}
-
-Reemplaza los caracteres UTF-8 no válidos por `�` (U+FFFD) carácter. Todos los caracteres no válidos que se ejecutan en una fila se contraen en el único carácter de reemplazo.
-
-``` sql
-toValidUTF8( input_string )
-```
-
-Parámetros:
-
--   input_string — Any set of bytes represented as the [Cadena](../../sql-reference/data-types/string.md) objeto de tipo de datos.
-
-Valor devuelto: cadena UTF-8 válida.
-
-**Ejemplo**
-
-``` sql
-SELECT toValidUTF8('\x61\xF0\x80\x80\x80b')
-```
-
-``` text
-┌─toValidUTF8('a����b')─┐
-│ a�b                   │
-└───────────────────────┘
-```
-
-## repetir {#repeat}
-
-Repite una cadena tantas veces como se especifique y concatena los valores replicados como una única cadena.
-
-**Sintaxis**
-
-``` sql
-repeat(s, n)
-```
-
-**Parámetros**
-
--   `s` — The string to repeat. [Cadena](../../sql-reference/data-types/string.md).
--   `n` — The number of times to repeat the string. [UInt](../../sql-reference/data-types/int-uint.md).
-
-**Valor devuelto**
-
-La cadena única, que contiene la cadena `s` repetir `n` tiempo. Si `n` \< 1, la función devuelve cadena vacía.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT repeat('abc', 10)
-```
-
-Resultado:
-
-``` text
-┌─repeat('abc', 10)──────────────┐
-│ abcabcabcabcabcabcabcabcabcabc │
-└────────────────────────────────┘
-```
-
-## inverso {#reverse}
-
-Invierte la cadena (como una secuencia de bytes).
-
-## reverseUTF8 {#reverseutf8}
-
-Invierte una secuencia de puntos de código Unicode, suponiendo que la cadena contiene un conjunto de bytes que representan un texto UTF-8. De lo contrario, hace otra cosa (no arroja una excepción).
-
-## format(pattern, s0, s1, …) {#format}
-
-Formatear el patrón constante con la cadena enumerada en los argumentos. `pattern` es un patrón de formato de Python simplificado. La cadena de formato contiene “replacement fields” rodeado de llaves `{}`. Cualquier cosa que no esté contenida entre llaves se considera texto literal, que se copia sin cambios en la salida. Si necesita incluir un carácter de llave en el texto literal, se puede escapar duplicando: `{{ '{{' }}` y `{{ '}}' }}`. Los nombres de campo pueden ser números (comenzando desde cero) o vacíos (luego se tratan como números de consecuencia).
-
-``` sql
-SELECT format('{1} {0} {1}', 'World', 'Hello')
-```
-
-``` text
-┌─format('{1} {0} {1}', 'World', 'Hello')─┐
-│ Hello World Hello                       │
-└─────────────────────────────────────────┘
-```
-
-``` sql
-SELECT format('{} {}', 'Hello', 'World')
-```
-
-``` text
-┌─format('{} {}', 'Hello', 'World')─┐
-│ Hello World                       │
-└───────────────────────────────────┘
-```
-
-## concat {#concat}
-
-Concatena las cadenas enumeradas en los argumentos, sin un separador.
-
-**Sintaxis**
-
-``` sql
-concat(s1, s2, ...)
-```
-
-**Parámetros**
-
-Valores de tipo String o FixedString.
-
-**Valores devueltos**
-
-Devuelve la cadena que resulta de concatenar los argumentos.
-
-Si alguno de los valores de argumento `NULL`, `concat` devoluciones `NULL`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT concat('Hello, ', 'World!')
-```
-
-Resultado:
-
-``` text
-┌─concat('Hello, ', 'World!')─┐
-│ Hello, World!               │
-└─────────────────────────────┘
-```
-
-## ConcatAssumeInjective {#concatassumeinjective}
-
-Lo mismo que [concat](#concat), la diferencia es que usted necesita asegurar eso `concat(s1, s2, ...) → sn` es inyectivo, se utilizará para la optimización de GROUP BY.
-
-La función se llama “injective” si siempre devuelve un resultado diferente para diferentes valores de argumentos. En otras palabras: diferentes argumentos nunca arrojan un resultado idéntico.
-
-**Sintaxis**
-
-``` sql
-concatAssumeInjective(s1, s2, ...)
-```
-
-**Parámetros**
-
-Valores de tipo String o FixedString.
-
-**Valores devueltos**
-
-Devuelve la cadena que resulta de concatenar los argumentos.
-
-Si alguno de los valores de argumento `NULL`, `concatAssumeInjective` devoluciones `NULL`.
-
-**Ejemplo**
-
-Tabla de entrada:
-
-``` sql
-CREATE TABLE key_val(`key1` String, `key2` String, `value` UInt32) ENGINE = TinyLog;
-INSERT INTO key_val VALUES ('Hello, ','World',1), ('Hello, ','World',2), ('Hello, ','World!',3), ('Hello',', World!',2);
-SELECT * from key_val;
-```
-
-``` text
-┌─key1────┬─key2─────┬─value─┐
-│ Hello,  │ World    │     1 │
-│ Hello,  │ World    │     2 │
-│ Hello,  │ World!   │     3 │
-│ Hello   │ , World! │     2 │
-└─────────┴──────────┴───────┘
-```
-
-Consulta:
-
-``` sql
-SELECT concat(key1, key2), sum(value) FROM key_val GROUP BY concatAssumeInjective(key1, key2)
-```
-
-Resultado:
-
-``` text
-┌─concat(key1, key2)─┬─sum(value)─┐
-│ Hello, World!      │          3 │
-│ Hello, World!      │          2 │
-│ Hello, World       │          3 │
-└────────────────────┴────────────┘
-```
-
-## substring(s, desplazamiento, longitud), mid(s, desplazamiento, longitud), substr(s, desplazamiento, longitud) {#substring}
-
-Devuelve una subcadena que comienza con el byte ‘offset’ índice que es ‘length’ bytes de largo. La indexación de caracteres comienza desde uno (como en SQL estándar). El ‘offset’ y ‘length’ los argumentos deben ser constantes.
-
-## substringUTF8(s, desplazamiento, longitud) {#substringutf8}
-
-Lo mismo que ‘substring’, pero para puntos de código Unicode. Funciona bajo el supuesto de que la cadena contiene un conjunto de bytes que representan un texto codificado en UTF-8. Si no se cumple esta suposición, devuelve algún resultado (no arroja una excepción).
-
-## Aquí hay algunas opciones) {#appendtrailingcharifabsent}
-
-Si el ‘s’ cadena no está vacía y no contiene el ‘c’ carácter al final, se añade el ‘c’ carácter hasta el final.
-
-## convertirCharset(s), de, a) {#convertcharset}
-
-Devuelve la cadena ‘s’ que se convirtió de la codificación en ‘from’ a la codificación en ‘to’.
-
-## Sistema abierto.) {#base64encode}
-
-Codificar ‘s’ cadena en base64
-
-## base64Decode(s)) {#base64decode}
-
-Decodificar cadena codificada en base64 ‘s’ en la cadena original. En caso de fallo plantea una excepción.
-
-## tryBase64Decode(s) {#trybase64decode}
-
-Similar a base64Decode, pero en caso de error se devolverá una cadena vacía.
-
-## endsWith(s, sufijo) {#endswith}
-
-Devuelve si se debe terminar con el sufijo especificado. Devuelve 1 si la cadena termina con el sufijo especificado, de lo contrario devuelve 0.
-
-## startsWith(str, prefijo) {#startswith}
-
-Devuelve 1 si la cadena comienza con el prefijo especificado, de lo contrario devuelve 0.
-
-``` sql
-SELECT startsWith('Spider-Man', 'Spi');
-```
-
-**Valores devueltos**
-
--   1, si la cadena comienza con el prefijo especificado.
--   0, si la cadena no comienza con el prefijo especificado.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT startsWith('Hello, world!', 'He');
-```
-
-Resultado:
-
-``` text
-┌─startsWith('Hello, world!', 'He')─┐
-│                                 1 │
-└───────────────────────────────────┘
-```
-
-## recortar {#trim}
-
-Quita todos los caracteres especificados del inicio o el final de una cadena.
-De forma predeterminada, elimina todas las apariciones consecutivas de espacios en blanco comunes (carácter ASCII 32) de ambos extremos de una cadena.
-
-**Sintaxis**
-
-``` sql
-trim([[LEADING|TRAILING|BOTH] trim_character FROM] input_string)
-```
-
-**Parámetros**
-
--   `trim_character` — specified characters for trim. [Cadena](../../sql-reference/data-types/string.md).
--   `input_string` — string for trim. [Cadena](../../sql-reference/data-types/string.md).
-
-**Valor devuelto**
-
-Una cadena sin caracteres especificados iniciales y (o) finales.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT trim(BOTH ' ()' FROM '(   Hello, world!   )')
-```
-
-Resultado:
-
-``` text
-┌─trim(BOTH ' ()' FROM '(   Hello, world!   )')─┐
-│ Hello, world!                                 │
-└───────────────────────────────────────────────┘
-```
-
-## trimLeft {#trimleft}
-
-Quita todas las apariciones consecutivas de espacios en blanco comunes (carácter ASCII 32) desde el principio de una cadena. No elimina otros tipos de caracteres de espacios en blanco (tab, espacio sin interrupción, etc.).
-
-**Sintaxis**
-
-``` sql
-trimLeft(input_string)
-```
-
-Apodo: `ltrim(input_string)`.
-
-**Parámetros**
-
--   `input_string` — string to trim. [Cadena](../../sql-reference/data-types/string.md).
-
-**Valor devuelto**
-
-Una cadena sin espacios en blanco comunes iniciales.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT trimLeft('     Hello, world!     ')
-```
-
-Resultado:
-
-``` text
-┌─trimLeft('     Hello, world!     ')─┐
-│ Hello, world!                       │
-└─────────────────────────────────────┘
-```
-
-## trimRight {#trimright}
-
-Quita todas las apariciones consecutivas de espacios en blanco comunes (carácter ASCII 32) del final de una cadena. No elimina otros tipos de caracteres de espacios en blanco (tab, espacio sin interrupción, etc.).
-
-**Sintaxis**
-
-``` sql
-trimRight(input_string)
-```
-
-Apodo: `rtrim(input_string)`.
-
-**Parámetros**
-
--   `input_string` — string to trim. [Cadena](../../sql-reference/data-types/string.md).
-
-**Valor devuelto**
-
-Una cadena sin espacios en blanco comunes finales.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT trimRight('     Hello, world!     ')
-```
-
-Resultado:
-
-``` text
-┌─trimRight('     Hello, world!     ')─┐
-│      Hello, world!                   │
-└──────────────────────────────────────┘
-```
-
-## AjusteTanto {#trimboth}
-
-Quita todas las apariciones consecutivas de espacios en blanco comunes (carácter ASCII 32) de ambos extremos de una cadena. No elimina otros tipos de caracteres de espacios en blanco (tab, espacio sin interrupción, etc.).
-
-**Sintaxis**
-
-``` sql
-trimBoth(input_string)
-```
-
-Apodo: `trim(input_string)`.
-
-**Parámetros**
-
--   `input_string` — string to trim. [Cadena](../../sql-reference/data-types/string.md).
-
-**Valor devuelto**
-
-Una cadena sin espacios en blanco comunes iniciales y finales.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT trimBoth('     Hello, world!     ')
-```
-
-Resultado:
-
-``` text
-┌─trimBoth('     Hello, world!     ')─┐
-│ Hello, world!                       │
-└─────────────────────────────────────┘
-```
-
-## CRC32(s)) {#crc32}
-
-Devuelve la suma de comprobación CRC32 de una cadena, utilizando el polinomio CRC-32-IEEE 802.3 y el valor inicial `0xffffffff` (implementación zlib).
-
-El tipo de resultado es UInt32.
-
-## CRC32IEEE(s) {#crc32ieee}
-
-Devuelve la suma de comprobación CRC32 de una cadena, utilizando el polinomio CRC-32-IEEE 802.3.
-
-El tipo de resultado es UInt32.
-
-## CRC64(s)) {#crc64}
-
-Devuelve la suma de comprobación CRC64 de una cadena, utilizando el polinomio CRC-64-ECMA.
-
-El tipo de resultado es UInt64.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/string_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/string-replace-functions.md b/docs/es/sql-reference/functions/string-replace-functions.md
deleted file mode 100644
index 979d16d842bb..000000000000
--- a/docs/es/sql-reference/functions/string-replace-functions.md
+++ /dev/null
@@ -1,94 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: Para reemplazar en cadenas
----
-
-# Funciones para buscar y reemplazar en cadenas {#functions-for-searching-and-replacing-in-strings}
-
-## replaceOne(pajar, patrón, reemplazo) {#replaceonehaystack-pattern-replacement}
-
-Sustituye la primera aparición, si existe, de la ‘pattern’ subcadena en ‘haystack’ con el ‘replacement’ subcadena.
-Sucesivo, ‘pattern’ y ‘replacement’ deben ser constantes.
-
-## replaceAll (pajar, patrón, reemplazo), replace (pajar, patrón, reemplazo) {#replaceallhaystack-pattern-replacement-replacehaystack-pattern-replacement}
-
-Sustituye todas las apariciones del ‘pattern’ subcadena en ‘haystack’ con el ‘replacement’ subcadena.
-
-## replaceRegexpOne (pajar, patrón, reemplazo) {#replaceregexponehaystack-pattern-replacement}
-
-Reemplazo usando el ‘pattern’ expresión regular. Una expresión regular re2.
-Sustituye sólo la primera ocurrencia, si existe.
-Un patrón se puede especificar como ‘replacement’. Este patrón puede incluir sustituciones `\0-\9`.
-Sustitución `\0` incluye toda la expresión regular. Sustitución `\1-\9` corresponden a los números de subpatrón. `\` en una plantilla, escapar de ella usando `\`.
-También tenga en cuenta que un literal de cadena requiere un escape adicional.
-
-Ejemplo 1. Conversión de la fecha a formato americano:
-
-``` sql
-SELECT DISTINCT
-    EventDate,
-    replaceRegexpOne(toString(EventDate), '(\\d{4})-(\\d{2})-(\\d{2})', '\\2/\\3/\\1') AS res
-FROM test.hits
-LIMIT 7
-FORMAT TabSeparated
-```
-
-``` text
-2014-03-17      03/17/2014
-2014-03-18      03/18/2014
-2014-03-19      03/19/2014
-2014-03-20      03/20/2014
-2014-03-21      03/21/2014
-2014-03-22      03/22/2014
-2014-03-23      03/23/2014
-```
-
-Ejemplo 2. Copiar una cadena diez veces:
-
-``` sql
-SELECT replaceRegexpOne('Hello, World!', '.*', '\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0') AS res
-```
-
-``` text
-┌─res────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World! │
-└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-## replaceRegexpAll (pajar, patrón, reemplazo) {#replaceregexpallhaystack-pattern-replacement}
-
-Esto hace lo mismo, pero reemplaza todas las ocurrencias. Ejemplo:
-
-``` sql
-SELECT replaceRegexpAll('Hello, World!', '.', '\\0\\0') AS res
-```
-
-``` text
-┌─res────────────────────────┐
-│ HHeelllloo,,  WWoorrlldd!! │
-└────────────────────────────┘
-```
-
-Como excepción, si una expresión regular funcionó en una subcadena vacía, el reemplazo no se realiza más de una vez.
-Ejemplo:
-
-``` sql
-SELECT replaceRegexpAll('Hello, World!', '^', 'here: ') AS res
-```
-
-``` text
-┌─res─────────────────┐
-│ here: Hello, World! │
-└─────────────────────┘
-```
-
-## Sistema abierto.) {#regexpquotemetas}
-
-La función agrega una barra invertida antes de algunos caracteres predefinidos en la cadena.
-Caracteres predefinidos: ‘0’, ‘\\’, ‘\|’, ‘(’, ‘)’, ‘^’, ‘$’, ‘.’, ‘\[’, '\]', ‘?’, '\*‘,’+‘,’{‘,’:‘,’-'.
-Esta implementación difiere ligeramente de re2::RE2::QuoteMeta. Escapa de byte cero como \\0 en lugar de 00 y escapa solo de los caracteres requeridos.
-Para obtener más información, consulte el enlace: [RE2](https://github.com/google/re2/blob/master/re2/re2.cc#L473)
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/string_replace_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/string-search-functions.md b/docs/es/sql-reference/functions/string-search-functions.md
deleted file mode 100644
index c448872a186f..000000000000
--- a/docs/es/sql-reference/functions/string-search-functions.md
+++ /dev/null
@@ -1,383 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: Para buscar cadenas
----
-
-# Funciones para buscar cadenas {#functions-for-searching-strings}
-
-La búsqueda distingue entre mayúsculas y minúsculas de forma predeterminada en todas estas funciones. Hay variantes separadas para la búsqueda insensible a mayúsculas y minúsculas.
-
-## posición (pajar, aguja), localizar (pajar, aguja) {#position}
-
-Devuelve la posición (en bytes) de la subcadena encontrada en la cadena, comenzando desde 1.
-
-Funciona bajo el supuesto de que la cadena contiene un conjunto de bytes que representan un texto codificado de un solo byte. Si no se cumple esta suposición y un carácter no se puede representar usando un solo byte, la función no lanza una excepción y devuelve algún resultado inesperado. Si el carácter se puede representar usando dos bytes, usará dos bytes y así sucesivamente.
-
-Para una búsqueda sin distinción de mayúsculas y minúsculas, utilice la función [positionCaseInsensitive](#positioncaseinsensitive).
-
-**Sintaxis**
-
-``` sql
-position(haystack, needle[, start_pos])
-```
-
-Apodo: `locate(haystack, needle[, start_pos])`.
-
-**Parámetros**
-
--   `haystack` — string, in which substring will to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `start_pos` – Optional parameter, position of the first character in the string to start search. [UInt](../../sql-reference/data-types/int-uint.md)
-
-**Valores devueltos**
-
--   Posición inicial en bytes (contando desde 1), si se encontró subcadena.
--   0, si no se encontró la subcadena.
-
-Tipo: `Integer`.
-
-**Ejemplos**
-
-Frase “Hello, world!” contiene un conjunto de bytes que representan un texto codificado de un solo byte. La función devuelve algún resultado esperado:
-
-Consulta:
-
-``` sql
-SELECT position('Hello, world!', '!')
-```
-
-Resultado:
-
-``` text
-┌─position('Hello, world!', '!')─┐
-│                             13 │
-└────────────────────────────────┘
-```
-
-La misma frase en ruso contiene caracteres que no se pueden representar usando un solo byte. La función devuelve algún resultado inesperado (uso [PosiciónUTF8](#positionutf8) función para texto codificado de varios bytes):
-
-Consulta:
-
-``` sql
-SELECT position('Привет, мир!', '!')
-```
-
-Resultado:
-
-``` text
-┌─position('Привет, мир!', '!')─┐
-│                            21 │
-└───────────────────────────────┘
-```
-
-## positionCaseInsensitive {#positioncaseinsensitive}
-
-Lo mismo que [posición](#position) devuelve la posición (en bytes) de la subcadena encontrada en la cadena, comenzando desde 1. Utilice la función para una búsqueda que no distingue entre mayúsculas y minúsculas.
-
-Funciona bajo el supuesto de que la cadena contiene un conjunto de bytes que representan un texto codificado de un solo byte. Si no se cumple esta suposición y un carácter no se puede representar usando un solo byte, la función no lanza una excepción y devuelve algún resultado inesperado. Si el carácter se puede representar usando dos bytes, usará dos bytes y así sucesivamente.
-
-**Sintaxis**
-
-``` sql
-positionCaseInsensitive(haystack, needle[, start_pos])
-```
-
-**Parámetros**
-
--   `haystack` — string, in which substring will to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `start_pos` – Optional parameter, position of the first character in the string to start search. [UInt](../../sql-reference/data-types/int-uint.md)
-
-**Valores devueltos**
-
--   Posición inicial en bytes (contando desde 1), si se encontró subcadena.
--   0, si no se encontró la subcadena.
-
-Tipo: `Integer`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT positionCaseInsensitive('Hello, world!', 'hello')
-```
-
-Resultado:
-
-``` text
-┌─positionCaseInsensitive('Hello, world!', 'hello')─┐
-│                                                 1 │
-└───────────────────────────────────────────────────┘
-```
-
-## PosiciónUTF8 {#positionutf8}
-
-Devuelve la posición (en puntos Unicode) de la subcadena encontrada en la cadena, comenzando desde 1.
-
-Funciona bajo el supuesto de que la cadena contiene un conjunto de bytes que representan un texto codificado en UTF-8. Si no se cumple esta suposición, la función no produce una excepción y devuelve algún resultado inesperado. Si el carácter se puede representar usando dos puntos Unicode, usará dos y así sucesivamente.
-
-Para una búsqueda sin distinción de mayúsculas y minúsculas, utilice la función [PosiciónCasoInsensitiveUTF8](#positioncaseinsensitiveutf8).
-
-**Sintaxis**
-
-``` sql
-positionUTF8(haystack, needle[, start_pos])
-```
-
-**Parámetros**
-
--   `haystack` — string, in which substring will to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `start_pos` – Optional parameter, position of the first character in the string to start search. [UInt](../../sql-reference/data-types/int-uint.md)
-
-**Valores devueltos**
-
--   Posición inicial en puntos Unicode (contando desde 1), si se encontró subcadena.
--   0, si no se encontró la subcadena.
-
-Tipo: `Integer`.
-
-**Ejemplos**
-
-Frase “Hello, world!” en ruso contiene un conjunto de puntos Unicode que representan un texto codificado de un solo punto. La función devuelve algún resultado esperado:
-
-Consulta:
-
-``` sql
-SELECT positionUTF8('Привет, мир!', '!')
-```
-
-Resultado:
-
-``` text
-┌─positionUTF8('Привет, мир!', '!')─┐
-│                                12 │
-└───────────────────────────────────┘
-```
-
-Frase “Salut, étudiante!”, donde el carácter `é` puede ser representado usando un un punto (`U+00E9`) o dos puntos (`U+0065U+0301`) la función se puede devolver algún resultado inesperado:
-
-Consulta de la carta `é`, que se representa un punto Unicode `U+00E9`:
-
-``` sql
-SELECT positionUTF8('Salut, étudiante!', '!')
-```
-
-Resultado:
-
-``` text
-┌─positionUTF8('Salut, étudiante!', '!')─┐
-│                                     17 │
-└────────────────────────────────────────┘
-```
-
-Consulta de la carta `é`, que se representa dos puntos Unicode `U+0065U+0301`:
-
-``` sql
-SELECT positionUTF8('Salut, étudiante!', '!')
-```
-
-Resultado:
-
-``` text
-┌─positionUTF8('Salut, étudiante!', '!')─┐
-│                                     18 │
-└────────────────────────────────────────┘
-```
-
-## PosiciónCasoInsensitiveUTF8 {#positioncaseinsensitiveutf8}
-
-Lo mismo que [PosiciónUTF8](#positionutf8), pero no distingue entre mayúsculas y minúsculas. Devuelve la posición (en puntos Unicode) de la subcadena encontrada en la cadena, comenzando desde 1.
-
-Funciona bajo el supuesto de que la cadena contiene un conjunto de bytes que representan un texto codificado en UTF-8. Si no se cumple esta suposición, la función no produce una excepción y devuelve algún resultado inesperado. Si el carácter se puede representar usando dos puntos Unicode, usará dos y así sucesivamente.
-
-**Sintaxis**
-
-``` sql
-positionCaseInsensitiveUTF8(haystack, needle[, start_pos])
-```
-
-**Parámetros**
-
--   `haystack` — string, in which substring will to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `start_pos` – Optional parameter, position of the first character in the string to start search. [UInt](../../sql-reference/data-types/int-uint.md)
-
-**Valor devuelto**
-
--   Posición inicial en puntos Unicode (contando desde 1), si se encontró subcadena.
--   0, si no se encontró la subcadena.
-
-Tipo: `Integer`.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT positionCaseInsensitiveUTF8('Привет, мир!', 'Мир')
-```
-
-Resultado:
-
-``` text
-┌─positionCaseInsensitiveUTF8('Привет, мир!', 'Мир')─┐
-│                                                  9 │
-└────────────────────────────────────────────────────┘
-```
-
-## multiSearchAllPositions {#multisearchallpositions}
-
-Lo mismo que [posición](string-search-functions.md#position) pero devuelve `Array` posiciones (en bytes) de las subcadenas correspondientes encontradas en la cadena. Las posiciones se indexan a partir de 1.
-
-La búsqueda se realiza en secuencias de bytes sin respecto a la codificación de cadenas y la intercalación.
-
--   Para la búsqueda ASCII sin distinción de mayúsculas y minúsculas, utilice la función `multiSearchAllPositionsCaseInsensitive`.
--   Para buscar en UTF-8, use la función [MultiSearchAllPositionsUTF8](#multiSearchAllPositionsUTF8).
--   Para la búsqueda UTF-8 sin distinción de mayúsculas y minúsculas, utilice la función multiSearchAllPositionsCaseInsensitiveUTF8.
-
-**Sintaxis**
-
-``` sql
-multiSearchAllPositions(haystack, [needle1, needle2, ..., needlen])
-```
-
-**Parámetros**
-
--   `haystack` — string, in which substring will to be searched. [Cadena](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Cadena](../syntax.md#syntax-string-literal).
-
-**Valores devueltos**
-
--   Matriz de posiciones iniciales en bytes (contando desde 1), si se encontró la subcadena correspondiente y 0 si no se encuentra.
-
-**Ejemplo**
-
-Consulta:
-
-``` sql
-SELECT multiSearchAllPositions('Hello, World!', ['hello', '!', 'world'])
-```
-
-Resultado:
-
-``` text
-┌─multiSearchAllPositions('Hello, World!', ['hello', '!', 'world'])─┐
-│ [0,13,0]                                                          │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-## MultiSearchAllPositionsUTF8 {#multiSearchAllPositionsUTF8}
-
-Ver `multiSearchAllPositions`.
-
-## multiSearchFirstPosition(pajar, \[aguja<sub>1</sub>, aguja<sub>2</sub>, …, needle<sub>y</sub>\]) {#multisearchfirstposition}
-
-Lo mismo que `position` pero devuelve el desplazamiento más a la izquierda de la cadena `haystack` que se corresponde con algunas de las agujas.
-
-Para una búsqueda que no distingue entre mayúsculas y minúsculas o / y en formato UTF-8, use funciones `multiSearchFirstPositionCaseInsensitive, multiSearchFirstPositionUTF8, multiSearchFirstPositionCaseInsensitiveUTF8`.
-
-## multiSearchFirstIndex(pajar, \[aguja<sub>1</sub>, aguja<sub>2</sub>, …, needle<sub>y</sub>\]) {#multisearchfirstindexhaystack-needle1-needle2-needlen}
-
-Devuelve el índice `i` (a partir de 1) de la aguja encontrada más a la izquierda<sub>me</sub> en la cadena `haystack` y 0 de lo contrario.
-
-Para una búsqueda que no distingue entre mayúsculas y minúsculas o / y en formato UTF-8, use funciones `multiSearchFirstIndexCaseInsensitive, multiSearchFirstIndexUTF8, multiSearchFirstIndexCaseInsensitiveUTF8`.
-
-## multiSearchAny(pajar, \[aguja<sub>1</sub>, aguja<sub>2</sub>, …, needle<sub>y</sub>\]) {#function-multisearchany}
-
-Devuelve 1, si al menos una aguja de cuerda<sub>me</sub> coincide con la cadena `haystack` y 0 de lo contrario.
-
-Para una búsqueda que no distingue entre mayúsculas y minúsculas o / y en formato UTF-8, use funciones `multiSearchAnyCaseInsensitive, multiSearchAnyUTF8, multiSearchAnyCaseInsensitiveUTF8`.
-
-!!! note "Nota"
-    En todos `multiSearch*` el número de agujas debe ser inferior a 2<sup>8</sup> debido a la especificación de implementación.
-
-## match (pajar, patrón) {#matchhaystack-pattern}
-
-Comprueba si la cadena coincide con la `pattern` expresión regular. Un `re2` expresión regular. El [sintaxis](https://github.com/google/re2/wiki/Syntax) de la `re2` expresiones regulares es más limitada que la sintaxis de las expresiones regulares de Perl.
-
-Devuelve 0 si no coincide, o 1 si coincide.
-
-Tenga en cuenta que el símbolo de barra invertida (`\`) se utiliza para escapar en la expresión regular. El mismo símbolo se usa para escapar en literales de cadena. Por lo tanto, para escapar del símbolo en una expresión regular, debe escribir dos barras invertidas (\\) en un literal de cadena.
-
-La expresión regular funciona con la cadena como si fuera un conjunto de bytes. La expresión regular no puede contener bytes nulos.
-Para que los patrones busquen subcadenas en una cadena, es mejor usar LIKE o ‘position’, ya que trabajan mucho más rápido.
-
-## multiMatchAny(pajar, \[patrón<sub>1</sub>, patrón<sub>2</sub>, …, pattern<sub>y</sub>\]) {#multimatchanyhaystack-pattern1-pattern2-patternn}
-
-Lo mismo que `match`, pero devuelve 0 si ninguna de las expresiones regulares coincide y 1 si alguno de los patrones coincide. Se utiliza [hyperscan](https://github.com/intel/hyperscan) biblioteca. Para que los patrones busquen subcadenas en una cadena, es mejor usar `multiSearchAny` ya que funciona mucho más rápido.
-
-!!! note "Nota"
-    La longitud de cualquiera de los `haystack` cadena debe ser inferior a 2<sup>32</sup> bytes de lo contrario, se lanza la excepción. Esta restricción tiene lugar debido a la API de hiperscan.
-
-## multiMatchAnyIndex(pajar, \[patrón<sub>1</sub>, patrón<sub>2</sub>, …, pattern<sub>y</sub>\]) {#multimatchanyindexhaystack-pattern1-pattern2-patternn}
-
-Lo mismo que `multiMatchAny`, pero devuelve cualquier índice que coincida con el pajar.
-
-## ¿Cómo puedo obtener más información?<sub>1</sub>, patrón<sub>2</sub>, …, pattern<sub>y</sub>\]) {#multimatchallindiceshaystack-pattern1-pattern2-patternn}
-
-Lo mismo que `multiMatchAny`, pero devuelve la matriz de todas las indicaciones que coinciden con el pajar en cualquier orden.
-
-## multiFuzzyMatchAny(pajar, distancia, \[patrón<sub>1</sub>, patrón<sub>2</sub>, …, pattern<sub>y</sub>\]) {#multifuzzymatchanyhaystack-distance-pattern1-pattern2-patternn}
-
-Lo mismo que `multiMatchAny`, pero devuelve 1 si algún patrón coincide con el pajar dentro de una constante [editar distancia](https://en.wikipedia.org/wiki/Edit_distance). Esta función también está en modo experimental y puede ser extremadamente lenta. Para obtener más información, consulte [documentación de hyperscan](https://intel.github.io/hyperscan/dev-reference/compilation.html#approximate-matching).
-
-## multiFuzzyMatchAnyIndex(pajar, distancia, \[patrón<sub>1</sub>, patrón<sub>2</sub>, …, pattern<sub>y</sub>\]) {#multifuzzymatchanyindexhaystack-distance-pattern1-pattern2-patternn}
-
-Lo mismo que `multiFuzzyMatchAny`, pero devuelve cualquier índice que coincida con el pajar dentro de una distancia de edición constante.
-
-## multiFuzzyMatchAllIndices(pajar, distancia, \[patrón<sub>1</sub>, patrón<sub>2</sub>, …, pattern<sub>y</sub>\]) {#multifuzzymatchallindiceshaystack-distance-pattern1-pattern2-patternn}
-
-Lo mismo que `multiFuzzyMatchAny`, pero devuelve la matriz de todos los índices en cualquier orden que coincida con el pajar dentro de una distancia de edición constante.
-
-!!! note "Nota"
-    `multiFuzzyMatch*` las funciones no admiten expresiones regulares UTF-8, y dichas expresiones se tratan como bytes debido a la restricción de hiperscan.
-
-!!! note "Nota"
-    Para desactivar todas las funciones que utilizan hyperscan, utilice la configuración `SET allow_hyperscan = 0;`.
-
-## extracto(pajar, patrón) {#extracthaystack-pattern}
-
-Extrae un fragmento de una cadena utilizando una expresión regular. Si ‘haystack’ no coincide con el ‘pattern’ regex, se devuelve una cadena vacía. Si la expresión regular no contiene subpatrones, toma el fragmento que coincide con toda la expresión regular. De lo contrario, toma el fragmento que coincide con el primer subpatrón.
-
-## extractAll(pajar, patrón) {#extractallhaystack-pattern}
-
-Extrae todos los fragmentos de una cadena utilizando una expresión regular. Si ‘haystack’ no coincide con el ‘pattern’ regex, se devuelve una cadena vacía. Devuelve una matriz de cadenas que consiste en todas las coincidencias con la expresión regular. En general, el comportamiento es el mismo que el ‘extract’ función (toma el primer subpatrón, o la expresión completa si no hay un subpatrón).
-
-## como (pajar, patrón), operador de patrón COMO pajar {#function-like}
-
-Comprueba si una cadena coincide con una expresión regular simple.
-La expresión regular puede contener los metasímbolos `%` y `_`.
-
-`%` indica cualquier cantidad de bytes (incluidos cero caracteres).
-
-`_` indica cualquier byte.
-
-Utilice la barra invertida (`\`) para escapar de metasímbolos. Vea la nota sobre el escape en la descripción del ‘match’ función.
-
-Para expresiones regulares como `%needle%`, el código es más óptimo y trabaja tan rápido como el `position` función.
-Para otras expresiones regulares, el código es el mismo que para ‘match’ función.
-
-## notLike(haystack, pattern), haystack NOT LIKE operador de patrón {#function-notlike}
-
-Lo mismo que ‘like’ pero negativo.
-
-## ngramDistance(pajar, aguja) {#ngramdistancehaystack-needle}
-
-Calcula la distancia de 4 gramos entre `haystack` y `needle`: counts the symmetric difference between two multisets of 4-grams and normalizes it by the sum of their cardinalities. Returns float number from 0 to 1 – the closer to zero, the more strings are similar to each other. If the constant `needle` o `haystack` es más de 32Kb, arroja una excepción. Si algunos de los no constantes `haystack` o `needle` Las cadenas son más de 32Kb, la distancia es siempre una.
-
-Para la búsqueda sin distinción de mayúsculas y minúsculas o / y en formato UTF-8, use funciones `ngramDistanceCaseInsensitive, ngramDistanceUTF8, ngramDistanceCaseInsensitiveUTF8`.
-
-## ngramSearch(pajar, aguja) {#ngramsearchhaystack-needle}
-
-Lo mismo que `ngramDistance` pero calcula la diferencia no simétrica entre `needle` y `haystack` – the number of n-grams from needle minus the common number of n-grams normalized by the number of `needle` n-gramas. Cuanto más cerca de uno, más probable es `needle` está en el `haystack`. Puede ser útil para la búsqueda de cadenas difusas.
-
-Para la búsqueda sin distinción de mayúsculas y minúsculas o / y en formato UTF-8, use funciones `ngramSearchCaseInsensitive, ngramSearchUTF8, ngramSearchCaseInsensitiveUTF8`.
-
-!!! note "Nota"
-    For UTF-8 case we use 3-gram distance. All these are not perfectly fair n-gram distances. We use 2-byte hashes to hash n-grams and then calculate the (non-)symmetric difference between these hash tables – collisions may occur. With UTF-8 case-insensitive format we do not use fair `tolower` function – we zero the 5-th bit (starting from zero) of each codepoint byte and first bit of zeroth byte if bytes more than one – this works for Latin and mostly for all Cyrillic letters.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/string_search_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/type-conversion-functions.md b/docs/es/sql-reference/functions/type-conversion-functions.md
deleted file mode 100644
index d5e41ee0e52f..000000000000
--- a/docs/es/sql-reference/functions/type-conversion-functions.md
+++ /dev/null
@@ -1,534 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: "Conversi\xF3n de tipo"
----
-
-# Funciones de conversión de tipos {#type-conversion-functions}
-
-## Problemas comunes de conversiones numéricas {#numeric-conversion-issues}
-
-Cuando convierte un valor de uno a otro tipo de datos, debe recordar que, en un caso común, es una operación insegura que puede provocar una pérdida de datos. Puede producirse una pérdida de datos si intenta ajustar el valor de un tipo de datos más grande a un tipo de datos más pequeño, o si convierte valores entre diferentes tipos de datos.
-
-ClickHouse tiene el [mismo comportamiento que los programas de C++](https://en.cppreference.com/w/cpp/language/implicit_conversion).
-
-## ¿Cómo puedo obtener más información?) {#toint8163264}
-
-Convierte un valor de entrada en el [En](../../sql-reference/data-types/int-uint.md) tipo de datos. Esta familia de funciones incluye:
-
--   `toInt8(expr)` — Results in the `Int8` tipo de datos.
--   `toInt16(expr)` — Results in the `Int16` tipo de datos.
--   `toInt32(expr)` — Results in the `Int32` tipo de datos.
--   `toInt64(expr)` — Results in the `Int64` tipo de datos.
-
-**Parámetros**
-
--   `expr` — [Expresion](../syntax.md#syntax-expressions) devolviendo un número o una cadena con la representación decimal de un número. No se admiten representaciones binarias, octales y hexadecimales de números. Los ceros principales son despojados.
-
-**Valor devuelto**
-
-Valor entero en el `Int8`, `Int16`, `Int32`, o `Int64` tipo de datos.
-
-Funciones de uso [redondeando hacia cero](https://en.wikipedia.org/wiki/Rounding#Rounding_towards_zero), lo que significa que truncan dígitos fraccionarios de números.
-
-El comportamiento de las funciones [NaN y Inf](../../sql-reference/data-types/float.md#data_type-float-nan-inf) los argumentos no están definidos. Recuerde acerca de [problemas de conversión numérica](#numeric-conversion-issues), al usar las funciones.
-
-**Ejemplo**
-
-``` sql
-SELECT toInt64(nan), toInt32(32), toInt16('16'), toInt8(8.8)
-```
-
-``` text
-┌─────────toInt64(nan)─┬─toInt32(32)─┬─toInt16('16')─┬─toInt8(8.8)─┐
-│ -9223372036854775808 │          32 │            16 │           8 │
-└──────────────────────┴─────────────┴───────────────┴─────────────┘
-```
-
-## ¿Cómo puedo obtener más información? {#toint8163264orzero}
-
-Toma un argumento de tipo String e intenta analizarlo en Int (8 \| 16 \| 32 \| 64). Si falla, devuelve 0.
-
-**Ejemplo**
-
-``` sql
-select toInt64OrZero('123123'), toInt8OrZero('123qwe123')
-```
-
-``` text
-┌─toInt64OrZero('123123')─┬─toInt8OrZero('123qwe123')─┐
-│                  123123 │                         0 │
-└─────────────────────────┴───────────────────────────┘
-```
-
-## ¿Cómo puedo hacerlo? {#toint8163264ornull}
-
-Toma un argumento de tipo String e intenta analizarlo en Int (8 \| 16 \| 32 \| 64). Si falla, devuelve NULL.
-
-**Ejemplo**
-
-``` sql
-select toInt64OrNull('123123'), toInt8OrNull('123qwe123')
-```
-
-``` text
-┌─toInt64OrNull('123123')─┬─toInt8OrNull('123qwe123')─┐
-│                  123123 │                      ᴺᵁᴸᴸ │
-└─────────────────────────┴───────────────────────────┘
-```
-
-## ¿Cómo puedo obtener más información?) {#touint8163264}
-
-Convierte un valor de entrada en el [UInt](../../sql-reference/data-types/int-uint.md) tipo de datos. Esta familia de funciones incluye:
-
--   `toUInt8(expr)` — Results in the `UInt8` tipo de datos.
--   `toUInt16(expr)` — Results in the `UInt16` tipo de datos.
--   `toUInt32(expr)` — Results in the `UInt32` tipo de datos.
--   `toUInt64(expr)` — Results in the `UInt64` tipo de datos.
-
-**Parámetros**
-
--   `expr` — [Expresion](../syntax.md#syntax-expressions) devolviendo un número o una cadena con la representación decimal de un número. No se admiten representaciones binarias, octales y hexadecimales de números. Los ceros principales son despojados.
-
-**Valor devuelto**
-
-Valor entero en el `UInt8`, `UInt16`, `UInt32`, o `UInt64` tipo de datos.
-
-Funciones de uso [redondeando hacia cero](https://en.wikipedia.org/wiki/Rounding#Rounding_towards_zero), lo que significa que truncan dígitos fraccionarios de números.
-
-El comportamiento de las funciones para los instrumentos negativos y para [NaN y Inf](../../sql-reference/data-types/float.md#data_type-float-nan-inf) los argumentos no están definidos. Si pasa una cadena con un número negativo, por ejemplo `'-32'`, ClickHouse genera una excepción. Recuerde acerca de [problemas de conversión numérica](#numeric-conversion-issues), al usar las funciones.
-
-**Ejemplo**
-
-``` sql
-SELECT toUInt64(nan), toUInt32(-32), toUInt16('16'), toUInt8(8.8)
-```
-
-``` text
-┌───────toUInt64(nan)─┬─toUInt32(-32)─┬─toUInt16('16')─┬─toUInt8(8.8)─┐
-│ 9223372036854775808 │    4294967264 │             16 │            8 │
-└─────────────────────┴───────────────┴────────────────┴──────────────┘
-```
-
-## ¿Cómo puedo obtener más información? {#touint8163264orzero}
-
-## ¿Cómo puedo hacerlo? {#touint8163264ornull}
-
-## ¿Cómo puedo obtener más información?) {#tofloat3264}
-
-## ¿Cómo puedo hacerlo? {#tofloat3264orzero}
-
-## ¿Cómo puedo hacerlo? {#tofloat3264ornull}
-
-## Fecha {#todate}
-
-## Todos los derechos reservados {#todateorzero}
-
-## ToDateOrNull {#todateornull}
-
-## toDateTime {#todatetime}
-
-## ToDateTimeOrZero {#todatetimeorzero}
-
-## ToDateTimeOrNull {#todatetimeornull}
-
-## toDecimal(32/64/128) {#todecimal3264128}
-
-Convertir `value` a la [Decimal](../../sql-reference/data-types/decimal.md) tipo de datos con precisión de `S`. El `value` puede ser un número o una cadena. El `S` (escala) parámetro especifica el número de decimales.
-
--   `toDecimal32(value, S)`
--   `toDecimal64(value, S)`
--   `toDecimal128(value, S)`
-
-## ¿Cómo puedo hacer esto? {#todecimal3264128ornull}
-
-Convierte una cadena de entrada en un [Información detallada))](../../sql-reference/data-types/decimal.md) valor de tipo de datos. Esta familia de funciones incluye:
-
--   `toDecimal32OrNull(expr, S)` — Results in `Nullable(Decimal32(S))` tipo de datos.
--   `toDecimal64OrNull(expr, S)` — Results in `Nullable(Decimal64(S))` tipo de datos.
--   `toDecimal128OrNull(expr, S)` — Results in `Nullable(Decimal128(S))` tipo de datos.
-
-Estas funciones deben usarse en lugar de `toDecimal*()` funciones, si usted prefiere conseguir un `NULL` valor de entrada en lugar de una excepción en el caso de un error de análisis de valor de entrada.
-
-**Parámetros**
-
--   `expr` — [Expresion](../syntax.md#syntax-expressions), devuelve un valor en el [Cadena](../../sql-reference/data-types/string.md) tipo de datos. ClickHouse espera la representación textual del número decimal. Por ejemplo, `'1.111'`.
--   `S` — Scale, the number of decimal places in the resulting value.
-
-**Valor devuelto**
-
-Un valor en el `Nullable(Decimal(P,S))` tipo de datos. El valor contiene:
-
--   Número con `S` lugares decimales, si ClickHouse interpreta la cadena de entrada como un número.
--   `NULL`, si ClickHouse no puede interpretar la cadena de entrada como un número o si el número de entrada contiene más de `S` lugares decimales.
-
-**Ejemplos**
-
-``` sql
-SELECT toDecimal32OrNull(toString(-1.111), 5) AS val, toTypeName(val)
-```
-
-``` text
-┌──────val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 5))─┐
-│ -1.11100 │ Nullable(Decimal(9, 5))                            │
-└──────────┴────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT toDecimal32OrNull(toString(-1.111), 2) AS val, toTypeName(val)
-```
-
-``` text
-┌──val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 2))─┐
-│ ᴺᵁᴸᴸ │ Nullable(Decimal(9, 2))                            │
-└──────┴────────────────────────────────────────────────────┘
-```
-
-## Por ejemplo: {#todecimal3264128orzero}
-
-Convierte un valor de entrada en el [Decimal (P, S)](../../sql-reference/data-types/decimal.md) tipo de datos. Esta familia de funciones incluye:
-
--   `toDecimal32OrZero( expr, S)` — Results in `Decimal32(S)` tipo de datos.
--   `toDecimal64OrZero( expr, S)` — Results in `Decimal64(S)` tipo de datos.
--   `toDecimal128OrZero( expr, S)` — Results in `Decimal128(S)` tipo de datos.
-
-Estas funciones deben usarse en lugar de `toDecimal*()` funciones, si usted prefiere conseguir un `0` valor de entrada en lugar de una excepción en el caso de un error de análisis de valor de entrada.
-
-**Parámetros**
-
--   `expr` — [Expresion](../syntax.md#syntax-expressions), devuelve un valor en el [Cadena](../../sql-reference/data-types/string.md) tipo de datos. ClickHouse espera la representación textual del número decimal. Por ejemplo, `'1.111'`.
--   `S` — Scale, the number of decimal places in the resulting value.
-
-**Valor devuelto**
-
-Un valor en el `Nullable(Decimal(P,S))` tipo de datos. El valor contiene:
-
--   Número con `S` lugares decimales, si ClickHouse interpreta la cadena de entrada como un número.
--   0 con `S` decimales, si ClickHouse no puede interpretar la cadena de entrada como un número o si el número de entrada contiene más de `S` lugares decimales.
-
-**Ejemplo**
-
-``` sql
-SELECT toDecimal32OrZero(toString(-1.111), 5) AS val, toTypeName(val)
-```
-
-``` text
-┌──────val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 5))─┐
-│ -1.11100 │ Decimal(9, 5)                                      │
-└──────────┴────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT toDecimal32OrZero(toString(-1.111), 2) AS val, toTypeName(val)
-```
-
-``` text
-┌──val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 2))─┐
-│ 0.00 │ Decimal(9, 2)                                      │
-└──────┴────────────────────────────────────────────────────┘
-```
-
-## ToString {#tostring}
-
-Funciones para convertir entre números, cadenas (pero no cadenas fijas), fechas y fechas con horas.
-Todas estas funciones aceptan un argumento.
-
-Al convertir a o desde una cadena, el valor se formatea o se analiza utilizando las mismas reglas que para el formato TabSeparated (y casi todos los demás formatos de texto). Si la cadena no se puede analizar, se lanza una excepción y se cancela la solicitud.
-
-Al convertir fechas a números o viceversa, la fecha corresponde al número de días desde el comienzo de la época Unix.
-Al convertir fechas con horas a números o viceversa, la fecha con hora corresponde al número de segundos desde el comienzo de la época Unix.
-
-Los formatos de fecha y fecha con hora para las funciones toDate/toDateTime se definen de la siguiente manera:
-
-``` text
-YYYY-MM-DD
-YYYY-MM-DD hh:mm:ss
-```
-
-Como excepción, si convierte de tipos numéricos UInt32, Int32, UInt64 o Int64 a Date, y si el número es mayor o igual que 65536, el número se interpreta como una marca de tiempo Unix (y no como el número de días) y se redondea a la fecha. Esto permite soporte para la ocurrencia común de la escritura ‘toDate(unix_timestamp)’, que de otra manera sería un error y requeriría escribir el más engorroso ‘toDate(toDateTime(unix_timestamp))’.
-
-La conversión entre una fecha y una fecha con la hora se realiza de la manera natural: agregando un tiempo nulo o eliminando el tiempo.
-
-La conversión entre tipos numéricos utiliza las mismas reglas que las asignaciones entre diferentes tipos numéricos en C++.
-
-Además, la función toString del argumento DateTime puede tomar un segundo argumento String que contiene el nombre de la zona horaria. Ejemplo: `Asia/Yekaterinburg` En este caso, la hora se formatea de acuerdo con la zona horaria especificada.
-
-``` sql
-SELECT
-    now() AS now_local,
-    toString(now(), 'Asia/Yekaterinburg') AS now_yekat
-```
-
-``` text
-┌───────────now_local─┬─now_yekat───────────┐
-│ 2016-06-15 00:11:21 │ 2016-06-15 02:11:21 │
-└─────────────────────┴─────────────────────┘
-```
-
-Ver también el `toUnixTimestamp` función.
-
-## ¿Qué puedes encontrar en Neodigit) {#tofixedstrings-n}
-
-Convierte un argumento de tipo String en un tipo FixedString(N) (una cadena con longitud fija N). N debe ser una constante.
-Si la cadena tiene menos bytes que N, se rellena con bytes nulos a la derecha. Si la cadena tiene más bytes que N, se produce una excepción.
-
-## Todos los derechos reservados.) {#tostringcuttozeros}
-
-Acepta un argumento String o FixedString. Devuelve la cadena con el contenido truncado en el primer byte cero encontrado.
-
-Ejemplo:
-
-``` sql
-SELECT toFixedString('foo', 8) AS s, toStringCutToZero(s) AS s_cut
-```
-
-``` text
-┌─s─────────────┬─s_cut─┐
-│ foo\0\0\0\0\0 │ foo   │
-└───────────────┴───────┘
-```
-
-``` sql
-SELECT toFixedString('foo\0bar', 8) AS s, toStringCutToZero(s) AS s_cut
-```
-
-``` text
-┌─s──────────┬─s_cut─┐
-│ foo\0bar\0 │ foo   │
-└────────────┴───────┘
-```
-
-## ¿Cómo puedo obtener más información?) {#reinterpretasuint8163264}
-
-## ¿Cómo puedo obtener más información?) {#reinterpretasint8163264}
-
-## ¿Cómo puedo obtener más información?) {#reinterpretasfloat3264}
-
-## reinterpretAsDate {#reinterpretasdate}
-
-## reinterpretAsDateTime {#reinterpretasdatetime}
-
-Estas funciones aceptan una cadena e interpretan los bytes colocados al principio de la cadena como un número en orden de host (little endian). Si la cadena no es lo suficientemente larga, las funciones funcionan como si la cadena estuviera rellenada con el número necesario de bytes nulos. Si la cadena es más larga de lo necesario, se ignoran los bytes adicionales. Una fecha se interpreta como el número de días desde el comienzo de la época Unix, y una fecha con hora se interpreta como el número de segundos desde el comienzo de la época Unix.
-
-## reinterpretAsString {#type_conversion_functions-reinterpretAsString}
-
-Esta función acepta un número o fecha o fecha con hora, y devuelve una cadena que contiene bytes que representan el valor correspondiente en orden de host (little endian). Los bytes nulos se eliminan desde el final. Por ejemplo, un valor de tipo UInt32 de 255 es una cadena que tiene un byte de longitud.
-
-## reinterpretAsFixedString {#reinterpretasfixedstring}
-
-Esta función acepta un número o fecha o fecha con hora, y devuelve un FixedString que contiene bytes que representan el valor correspondiente en orden de host (little endian). Los bytes nulos se eliminan desde el final. Por ejemplo, un valor de tipo UInt32 de 255 es un FixedString que tiene un byte de longitud.
-
-## CAST(x, T) {#type_conversion_function-cast}
-
-Convertir ‘x’ a la ‘t’ tipo de datos. La sintaxis CAST(x AS t) también es compatible.
-
-Ejemplo:
-
-``` sql
-SELECT
-    '2016-06-15 23:00:00' AS timestamp,
-    CAST(timestamp AS DateTime) AS datetime,
-    CAST(timestamp AS Date) AS date,
-    CAST(timestamp, 'String') AS string,
-    CAST(timestamp, 'FixedString(22)') AS fixed_string
-```
-
-``` text
-┌─timestamp───────────┬────────────datetime─┬───────date─┬─string──────────────┬─fixed_string──────────────┐
-│ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00\0\0\0 │
-└─────────────────────┴─────────────────────┴────────────┴─────────────────────┴───────────────────────────┘
-```
-
-La conversión a FixedString(N) solo funciona para argumentos de tipo String o FixedString(N).
-
-Conversión de tipo a [NULL](../../sql-reference/data-types/nullable.md) y la espalda es compatible. Ejemplo:
-
-``` sql
-SELECT toTypeName(x) FROM t_null
-```
-
-``` text
-┌─toTypeName(x)─┐
-│ Int8          │
-│ Int8          │
-└───────────────┘
-```
-
-``` sql
-SELECT toTypeName(CAST(x, 'Nullable(UInt16)')) FROM t_null
-```
-
-``` text
-┌─toTypeName(CAST(x, 'Nullable(UInt16)'))─┐
-│ Nullable(UInt16)                        │
-│ Nullable(UInt16)                        │
-└─────────────────────────────────────────┘
-```
-
-## toInterval(Year\|Quarter\|Month\|Week\|Day\|Hour\|Minute\|Second) {#function-tointerval}
-
-Convierte un argumento de tipo Number en un [Intervalo](../../sql-reference/data-types/special-data-types/interval.md) tipo de datos.
-
-**Sintaxis**
-
-``` sql
-toIntervalSecond(number)
-toIntervalMinute(number)
-toIntervalHour(number)
-toIntervalDay(number)
-toIntervalWeek(number)
-toIntervalMonth(number)
-toIntervalQuarter(number)
-toIntervalYear(number)
-```
-
-**Parámetros**
-
--   `number` — Duration of interval. Positive integer number.
-
-**Valores devueltos**
-
--   El valor en `Interval` tipo de datos.
-
-**Ejemplo**
-
-``` sql
-WITH
-    toDate('2019-01-01') AS date,
-    INTERVAL 1 WEEK AS interval_week,
-    toIntervalWeek(1) AS interval_to_week
-SELECT
-    date + interval_week,
-    date + interval_to_week
-```
-
-``` text
-┌─plus(date, interval_week)─┬─plus(date, interval_to_week)─┐
-│                2019-01-08 │                   2019-01-08 │
-└───────────────────────────┴──────────────────────────────┘
-```
-
-## parseDateTimeBestEffort {#parsedatetimebesteffort}
-
-Convierte una fecha y una hora en el [Cadena](../../sql-reference/data-types/string.md) representación a [FechaHora](../../sql-reference/data-types/datetime.md#data_type-datetime) tipo de datos.
-
-La función analiza [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601), [RFC 1123 - 5.2.14 RFC-822 Especificación de fecha y hora](https://tools.ietf.org/html/rfc1123#page-55), ClickHouse y algunos otros formatos de fecha y hora.
-
-**Sintaxis**
-
-``` sql
-parseDateTimeBestEffort(time_string [, time_zone]);
-```
-
-**Parámetros**
-
--   `time_string` — String containing a date and time to convert. [Cadena](../../sql-reference/data-types/string.md).
--   `time_zone` — Time zone. The function parses `time_string` según la zona horaria. [Cadena](../../sql-reference/data-types/string.md).
-
-**Formatos no estándar admitidos**
-
--   Una cadena que contiene 9..10 dígitos [marca de tiempo unix](https://en.wikipedia.org/wiki/Unix_time).
--   Una cadena con un componente de fecha y hora: `YYYYMMDDhhmmss`, `DD/MM/YYYY hh:mm:ss`, `DD-MM-YY hh:mm`, `YYYY-MM-DD hh:mm:ss`, sucesivamente.
--   Una cadena con una fecha, pero sin componente de hora: `YYYY`, `YYYYMM`, `YYYY*MM`, `DD/MM/YYYY`, `DD-MM-YY` sucesivamente.
--   Una cadena con un día y una hora: `DD`, `DD hh`, `DD hh:mm`. En este caso `YYYY-MM` se sustituyen como `2000-01`.
--   Una cadena que incluye la fecha y la hora junto con la información de desplazamiento de zona horaria: `YYYY-MM-DD hh:mm:ss ±h:mm`, sucesivamente. Por ejemplo, `2020-12-12 17:36:00 -5:00`.
-
-Para todos los formatos con separador, la función analiza los nombres de meses expresados por su nombre completo o por las primeras tres letras de un nombre de mes. Ejemplos: `24/DEC/18`, `24-Dec-18`, `01-September-2018`.
-
-**Valor devuelto**
-
--   `time_string` convertido a la `DateTime` tipo de datos.
-
-**Ejemplos**
-
-Consulta:
-
-``` sql
-SELECT parseDateTimeBestEffort('12/12/2020 12:12:57')
-AS parseDateTimeBestEffort;
-```
-
-Resultado:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2020-12-12 12:12:57 │
-└─────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT parseDateTimeBestEffort('Sat, 18 Aug 2018 07:22:16 GMT', 'Europe/Moscow')
-AS parseDateTimeBestEffort
-```
-
-Resultado:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2018-08-18 10:22:16 │
-└─────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT parseDateTimeBestEffort('1284101485')
-AS parseDateTimeBestEffort
-```
-
-Resultado:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2015-07-07 12:04:41 │
-└─────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT parseDateTimeBestEffort('2018-12-12 10:12:12')
-AS parseDateTimeBestEffort
-```
-
-Resultado:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2018-12-12 10:12:12 │
-└─────────────────────────┘
-```
-
-Consulta:
-
-``` sql
-SELECT parseDateTimeBestEffort('10 20:19')
-```
-
-Resultado:
-
-``` text
-┌─parseDateTimeBestEffort('10 20:19')─┐
-│                 2000-01-10 20:19:00 │
-└─────────────────────────────────────┘
-```
-
-**Ver también**
-
--   \[Anuncio de ISO 8601 por @xkcd¿Por qué?/)
--   [RFC 1123](https://tools.ietf.org/html/rfc1123)
--   [Fecha](#todate)
--   [toDateTime](#todatetime)
-
-## parseDateTimeBestEffortOrNull {#parsedatetimebesteffortornull}
-
-Lo mismo que para [parseDateTimeBestEffort](#parsedatetimebesteffort) excepto que devuelve null cuando encuentra un formato de fecha que no se puede procesar.
-
-## parseDateTimeBestEffortOrZero {#parsedatetimebesteffortorzero}
-
-Lo mismo que para [parseDateTimeBestEffort](#parsedatetimebesteffort) excepto que devuelve una fecha cero o una fecha cero cuando encuentra un formato de fecha que no se puede procesar.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/type_conversion_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/url-functions.md b/docs/es/sql-reference/functions/url-functions.md
deleted file mode 100644
index de4c32ead2b5..000000000000
--- a/docs/es/sql-reference/functions/url-functions.md
+++ /dev/null
@@ -1,209 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 54
-toc_title: Trabajar con URL
----
-
-# Funciones para trabajar con URL {#functions-for-working-with-urls}
-
-Todas estas funciones no siguen el RFC. Se simplifican al máximo para mejorar el rendimiento.
-
-## Funciones que extraen partes de una URL {#functions-that-extract-parts-of-a-url}
-
-Si la parte relevante no está presente en una URL, se devuelve una cadena vacía.
-
-### protocolo {#protocol}
-
-Extrae el protocolo de una URL.
-
-Examples of typical returned values: http, https, ftp, mailto, tel, magnet…
-
-### dominio {#domain}
-
-Extrae el nombre de host de una dirección URL.
-
-``` sql
-domain(url)
-```
-
-**Parámetros**
-
--   `url` — URL. Type: [Cadena](../../sql-reference/data-types/string.md).
-
-La URL se puede especificar con o sin un esquema. Ejemplos:
-
-``` text
-svn+ssh://some.svn-hosting.com:80/repo/trunk
-some.svn-hosting.com:80/repo/trunk
-https://yandex.com/time/
-```
-
-Para estos ejemplos, el `domain` función devuelve los siguientes resultados:
-
-``` text
-some.svn-hosting.com
-some.svn-hosting.com
-yandex.com
-```
-
-**Valores devueltos**
-
--   Nombre de host. Si ClickHouse puede analizar la cadena de entrada como una URL.
--   Cadena vacía. Si ClickHouse no puede analizar la cadena de entrada como una URL.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-``` sql
-SELECT domain('svn+ssh://some.svn-hosting.com:80/repo/trunk')
-```
-
-``` text
-┌─domain('svn+ssh://some.svn-hosting.com:80/repo/trunk')─┐
-│ some.svn-hosting.com                                   │
-└────────────────────────────────────────────────────────┘
-```
-
-### Nuestros servicios {#domainwithoutwww}
-
-Devuelve el dominio y no elimina más de uno ‘www.’ desde el principio de la misma, si está presente.
-
-### topLevelDomain {#topleveldomain}
-
-Extrae el dominio de nivel superior de una URL.
-
-``` sql
-topLevelDomain(url)
-```
-
-**Parámetros**
-
--   `url` — URL. Type: [Cadena](../../sql-reference/data-types/string.md).
-
-La URL se puede especificar con o sin un esquema. Ejemplos:
-
-``` text
-svn+ssh://some.svn-hosting.com:80/repo/trunk
-some.svn-hosting.com:80/repo/trunk
-https://yandex.com/time/
-```
-
-**Valores devueltos**
-
--   Nombre de dominio. Si ClickHouse puede analizar la cadena de entrada como una URL.
--   Cadena vacía. Si ClickHouse no puede analizar la cadena de entrada como una URL.
-
-Tipo: `String`.
-
-**Ejemplo**
-
-``` sql
-SELECT topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk')
-```
-
-``` text
-┌─topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk')─┐
-│ com                                                                │
-└────────────────────────────────────────────────────────────────────┘
-```
-
-### FirstSignificantSubdomain {#firstsignificantsubdomain}
-
-Devuelve el “first significant subdomain”. Este es un concepto no estándar específico de Yandex.Métrica. El primer subdominio significativo es un dominio de segundo nivel si es ‘com’, ‘net’, ‘org’, o ‘co’. De lo contrario, es un dominio de tercer nivel. Por ejemplo, `firstSignificantSubdomain (‘https://news.yandex.ru/’) = ‘yandex’, firstSignificantSubdomain (‘https://news.yandex.com.tr/’) = ‘yandex’`. La lista de “insignificant” dominios de segundo nivel y otros detalles de implementación pueden cambiar en el futuro.
-
-### cutToFirstSignificantSubdomain {#cuttofirstsignificantsubdomain}
-
-Devuelve la parte del dominio que incluye subdominios de nivel superior “first significant subdomain” (véase la explicación anterior).
-
-Por ejemplo, `cutToFirstSignificantSubdomain('https://news.yandex.com.tr/') = 'yandex.com.tr'`.
-
-### camino {#path}
-
-Devuelve la ruta de acceso. Ejemplo: `/top/news.html` La ruta de acceso no incluye la cadena de consulta.
-
-### pathFull {#pathfull}
-
-Lo mismo que el anterior, pero incluyendo cadena de consulta y fragmento. Ejemplo: /top/news.html?Página = 2 # comentarios
-
-### queryString {#querystring}
-
-Devuelve la cadena de consulta. Ejemplo: page=1&lr=213. query-string no incluye el signo de interrogación inicial, así como # y todo después de #.
-
-### fragmento {#fragment}
-
-Devuelve el identificador de fragmento. el fragmento no incluye el símbolo hash inicial.
-
-### queryStringAndFragment {#querystringandfragment}
-
-Devuelve la cadena de consulta y el identificador de fragmento. Ejemplo: page=1#29390.
-
-### extractURLParameter(URL, nombre) {#extracturlparameterurl-name}
-
-Devuelve el valor de la ‘name’ parámetro en la URL, si está presente. De lo contrario, una cadena vacía. Si hay muchos parámetros con este nombre, devuelve la primera aparición. Esta función funciona bajo el supuesto de que el nombre del parámetro está codificado en la URL exactamente de la misma manera que en el argumento pasado.
-
-### extractURLParameters (URL) {#extracturlparametersurl}
-
-Devuelve una matriz de cadenas name=value correspondientes a los parámetros de URL. Los valores no se decodifican de ninguna manera.
-
-### ExtractURLParameterNames (URL) {#extracturlparameternamesurl}
-
-Devuelve una matriz de cadenas de nombre correspondientes a los nombres de los parámetros de URL. Los valores no se decodifican de ninguna manera.
-
-### URLJerarquía (URL) {#urlhierarchyurl}
-
-Devuelve una matriz que contiene la URL, truncada al final por los símbolos /,? en la ruta y la cadena de consulta. Los caracteres separadores consecutivos se cuentan como uno. El corte se realiza en la posición después de todos los caracteres separadores consecutivos.
-
-### URLPathHierarchy (URL) {#urlpathhierarchyurl}
-
-Lo mismo que el anterior, pero sin el protocolo y el host en el resultado. El elemento / (raíz) no está incluido. Ejemplo: la función se utiliza para implementar informes de árbol de la URL en Yandex. Métrica.
-
-``` text
-URLPathHierarchy('https://example.com/browse/CONV-6788') =
-[
-    '/browse/',
-    '/browse/CONV-6788'
-]
-```
-
-### decodeURLComponent (URL) {#decodeurlcomponenturl}
-
-Devuelve la dirección URL decodificada.
-Ejemplo:
-
-``` sql
-SELECT decodeURLComponent('http://127.0.0.1:8123/?query=SELECT%201%3B') AS DecodedURL;
-```
-
-``` text
-┌─DecodedURL─────────────────────────────┐
-│ http://127.0.0.1:8123/?query=SELECT 1; │
-└────────────────────────────────────────┘
-```
-
-## Funciones que eliminan parte de una URL {#functions-that-remove-part-of-a-url}
-
-Si la URL no tiene nada similar, la URL permanece sin cambios.
-
-### Sistema abierto {#cutwww}
-
-Elimina no más de uno ‘www.’ desde el principio del dominio de la URL, si está presente.
-
-### cutQueryString {#cutquerystring}
-
-Quita la cadena de consulta. El signo de interrogación también se elimina.
-
-### cutFragment {#cutfragment}
-
-Quita el identificador de fragmento. El signo de número también se elimina.
-
-### cutQueryStringAndFragment {#cutquerystringandfragment}
-
-Quita la cadena de consulta y el identificador de fragmento. El signo de interrogación y el signo de número también se eliminan.
-
-### cutURLParameter(URL, nombre) {#cuturlparameterurl-name}
-
-Elimina el ‘name’ Parámetro URL, si está presente. Esta función funciona bajo el supuesto de que el nombre del parámetro está codificado en la URL exactamente de la misma manera que en el argumento pasado.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/url_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/uuid-functions.md b/docs/es/sql-reference/functions/uuid-functions.md
deleted file mode 100644
index c99e16e17e31..000000000000
--- a/docs/es/sql-reference/functions/uuid-functions.md
+++ /dev/null
@@ -1,122 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 53
-toc_title: Trabajando con UUID
----
-
-# Funciones para trabajar con UUID {#functions-for-working-with-uuid}
-
-Las funciones para trabajar con UUID se enumeran a continuación.
-
-## GenerateUUIDv4 {#uuid-function-generate}
-
-Genera el [UUID](../../sql-reference/data-types/uuid.md) de [versión 4](https://tools.ietf.org/html/rfc4122#section-4.4).
-
-``` sql
-generateUUIDv4()
-```
-
-**Valor devuelto**
-
-El valor de tipo UUID.
-
-**Ejemplo de uso**
-
-En este ejemplo se muestra la creación de una tabla con la columna de tipo UUID e insertar un valor en la tabla.
-
-``` sql
-CREATE TABLE t_uuid (x UUID) ENGINE=TinyLog
-
-INSERT INTO t_uuid SELECT generateUUIDv4()
-
-SELECT * FROM t_uuid
-```
-
-``` text
-┌────────────────────────────────────x─┐
-│ f4bf890f-f9dc-4332-ad5c-0c18e73f28e9 │
-└──────────────────────────────────────┘
-```
-
-## paraUUID (x) {#touuid-x}
-
-Convierte el valor de tipo de cadena en tipo UUID.
-
-``` sql
-toUUID(String)
-```
-
-**Valor devuelto**
-
-El valor de tipo UUID.
-
-**Ejemplo de uso**
-
-``` sql
-SELECT toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0') AS uuid
-```
-
-``` text
-┌─────────────────────────────────uuid─┐
-│ 61f0c404-5cb3-11e7-907b-a6006ad3dba0 │
-└──────────────────────────────────────┘
-```
-
-## UUIDStringToNum {#uuidstringtonum}
-
-Acepta una cadena que contiene 36 caracteres en el formato `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`, y lo devuelve como un conjunto de bytes en un [Cadena fija (16)](../../sql-reference/data-types/fixedstring.md).
-
-``` sql
-UUIDStringToNum(String)
-```
-
-**Valor devuelto**
-
-Cadena fija (16)
-
-**Ejemplos de uso**
-
-``` sql
-SELECT
-    '612f3c40-5d3b-217e-707b-6a546a3d7b29' AS uuid,
-    UUIDStringToNum(uuid) AS bytes
-```
-
-``` text
-┌─uuid─────────────────────────────────┬─bytes────────────┐
-│ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │ a/<@];!~p{jTj={) │
-└──────────────────────────────────────┴──────────────────┘
-```
-
-## UUIDNumToString {#uuidnumtostring}
-
-Acepta un [Cadena fija (16)](../../sql-reference/data-types/fixedstring.md) valor, y devuelve una cadena que contiene 36 caracteres en formato de texto.
-
-``` sql
-UUIDNumToString(FixedString(16))
-```
-
-**Valor devuelto**
-
-Cadena.
-
-**Ejemplo de uso**
-
-``` sql
-SELECT
-    'a/<@];!~p{jTj={)' AS bytes,
-    UUIDNumToString(toFixedString(bytes, 16)) AS uuid
-```
-
-``` text
-┌─bytes────────────┬─uuid─────────────────────────────────┐
-│ a/<@];!~p{jTj={) │ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │
-└──────────────────┴──────────────────────────────────────┘
-```
-
-## Ver también {#see-also}
-
--   [dictGetUUID](ext-dict-functions.md#ext_dict_functions-other)
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/uuid_function/) <!--hide-->
diff --git a/docs/es/sql-reference/functions/ym-dict-functions.md b/docs/es/sql-reference/functions/ym-dict-functions.md
deleted file mode 100644
index 311bbe9f5e85..000000000000
--- a/docs/es/sql-reference/functions/ym-dict-functions.md
+++ /dev/null
@@ -1,155 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: Trabajando con Yandex.Diccionarios de Metrica
----
-
-# Funciones para trabajar con Yandex.Diccionarios de Metrica {#functions-for-working-with-yandex-metrica-dictionaries}
-
-Para que las funciones a continuación funcionen, la configuración del servidor debe especificar las rutas y direcciones para obtener todo el Yandex.Diccionarios Metrica. Los diccionarios se cargan en la primera llamada de cualquiera de estas funciones. Si las listas de referencia no se pueden cargar, se lanza una excepción.
-
-Para obtener información sobre cómo crear listas de referencia, consulte la sección “Dictionaries”.
-
-## Múltiples Geobases {#multiple-geobases}
-
-ClickHouse admite trabajar con múltiples geobases alternativas (jerarquías regionales) simultáneamente, con el fin de soportar diversas perspectivas sobre a qué países pertenecen ciertas regiones.
-
-El ‘clickhouse-server’ config especifica el archivo con la jerarquía regional::`<path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file>`
-
-Además de este archivo, también busca archivos cercanos que tengan el símbolo _ y cualquier sufijo anexado al nombre (antes de la extensión del archivo).
-Por ejemplo, también encontrará el archivo `/opt/geo/regions_hierarchy_ua.txt` si está presente.
-
-`ua` se llama la clave del diccionario. Para un diccionario sin un sufijo, la clave es una cadena vacía.
-
-Todos los diccionarios se vuelven a cargar en tiempo de ejecución (una vez cada cierto número de segundos, como se define en el parámetro de configuración builtin_dictionaries_reload_interval , o una vez por hora por defecto). Sin embargo, la lista de diccionarios disponibles se define una vez, cuando se inicia el servidor.
-
-All functions for working with regions have an optional argument at the end – the dictionary key. It is referred to as the geobase.
-Ejemplo:
-
-``` sql
-regionToCountry(RegionID) – Uses the default dictionary: /opt/geo/regions_hierarchy.txt
-regionToCountry(RegionID, '') – Uses the default dictionary: /opt/geo/regions_hierarchy.txt
-regionToCountry(RegionID, 'ua') – Uses the dictionary for the 'ua' key: /opt/geo/regions_hierarchy_ua.txt
-```
-
-### ¿Cómo puedo hacerlo?\]) {#regiontocityid-geobase}
-
-Accepts a UInt32 number – the region ID from the Yandex geobase. If this region is a city or part of a city, it returns the region ID for the appropriate city. Otherwise, returns 0.
-
-### ¿Cómo puedo hacerlo?\]) {#regiontoareaid-geobase}
-
-Convierte una región en un área (tipo 5 en la geobase). En todos los demás sentidos, esta función es la misma que ‘regionToCity’.
-
-``` sql
-SELECT DISTINCT regionToName(regionToArea(toUInt32(number), 'ua'))
-FROM system.numbers
-LIMIT 15
-```
-
-``` text
-┌─regionToName(regionToArea(toUInt32(number), \'ua\'))─┐
-│                                                      │
-│ Moscow and Moscow region                             │
-│ St. Petersburg and Leningrad region                  │
-│ Belgorod region                                      │
-│ Ivanovsk region                                      │
-│ Kaluga region                                        │
-│ Kostroma region                                      │
-│ Kursk region                                         │
-│ Lipetsk region                                       │
-│ Orlov region                                         │
-│ Ryazan region                                        │
-│ Smolensk region                                      │
-│ Tambov region                                        │
-│ Tver region                                          │
-│ Tula region                                          │
-└──────────────────────────────────────────────────────┘
-```
-
-### ¿Cómo puedo hacerlo?\]) {#regiontodistrictid-geobase}
-
-Convierte una región en un distrito federal (tipo 4 en la geobase). En todos los demás sentidos, esta función es la misma que ‘regionToCity’.
-
-``` sql
-SELECT DISTINCT regionToName(regionToDistrict(toUInt32(number), 'ua'))
-FROM system.numbers
-LIMIT 15
-```
-
-``` text
-┌─regionToName(regionToDistrict(toUInt32(number), \'ua\'))─┐
-│                                                          │
-│ Central federal district                                 │
-│ Northwest federal district                               │
-│ South federal district                                   │
-│ North Caucases federal district                          │
-│ Privolga federal district                                │
-│ Ural federal district                                    │
-│ Siberian federal district                                │
-│ Far East federal district                                │
-│ Scotland                                                 │
-│ Faroe Islands                                            │
-│ Flemish region                                           │
-│ Brussels capital region                                  │
-│ Wallonia                                                 │
-│ Federation of Bosnia and Herzegovina                     │
-└──────────────────────────────────────────────────────────┘
-```
-
-### ¿Cómo puedo hacerlo?\]) {#regiontocountryid-geobase}
-
-Convierte una región en un país. En todos los demás sentidos, esta función es la misma que ‘regionToCity’.
-Ejemplo: `regionToCountry(toUInt32(213)) = 225` convierte Moscú (213) a Rusia (225).
-
-### Aquí está el código de identificación.\]) {#regiontocontinentid-geobase}
-
-Convierte una región en un continente. En todos los demás sentidos, esta función es la misma que ‘regionToCity’.
-Ejemplo: `regionToContinent(toUInt32(213)) = 10001` convierte Moscú (213) a Eurasia (10001).
-
-### Nuestra misiÃ³n es brindarle un servicio de calidad y confianza a nuestros clientes.) {#regiontotopcontinent-regiontotopcontinent}
-
-Encuentra el continente más alto en la jerarquía de la región.
-
-**Sintaxis**
-
-``` sql
-regionToTopContinent(id[, geobase]);
-```
-
-**Parámetros**
-
--   `id` — Region ID from the Yandex geobase. [UInt32](../../sql-reference/data-types/int-uint.md).
--   `geobase` — Dictionary key. See [Múltiples Geobases](#multiple-geobases). [Cadena](../../sql-reference/data-types/string.md). Opcional.
-
-**Valor devuelto**
-
--   Identificador del continente de nivel superior (este último cuando subes la jerarquía de regiones).
--   0, si no hay ninguno.
-
-Tipo: `UInt32`.
-
-### Aquí está el código de identificación de la población.\]) {#regiontopopulationid-geobase}
-
-Obtiene la población de una región.
-La población se puede registrar en archivos con la geobase. Vea la sección “External dictionaries”.
-Si la población no se registra para la región, devuelve 0.
-En la geobase de Yandex, la población podría registrarse para las regiones secundarias, pero no para las regiones parentales.
-
-### ¿Cómo puedo hacerlo?\]) {#regioninlhs-rhs-geobase}
-
-Comprueba si un ‘lhs’ región pertenece a un ‘rhs’ regi. Devuelve un número UInt8 igual a 1 si pertenece, o 0 si no pertenece.
-The relationship is reflexive – any region also belongs to itself.
-
-### RegiónJerarquía (id\[, geobase\]) {#regionhierarchyid-geobase}
-
-Accepts a UInt32 number – the region ID from the Yandex geobase. Returns an array of region IDs consisting of the passed region and all parents along the chain.
-Ejemplo: `regionHierarchy(toUInt32(213)) = [213,1,3,225,10001,10000]`.
-
-### ¿Cómo puedo hacerlo?\]) {#regiontonameid-lang}
-
-Accepts a UInt32 number – the region ID from the Yandex geobase. A string with the name of the language can be passed as a second argument. Supported languages are: ru, en, ua, uk, by, kz, tr. If the second argument is omitted, the language ‘ru’ is used. If the language is not supported, an exception is thrown. Returns a string – the name of the region in the corresponding language. If the region with the specified ID doesn't exist, an empty string is returned.
-
-`ua` y `uk` ambos significan ucraniano.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/functions/ym_dict_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/index.md b/docs/es/sql-reference/index.md
deleted file mode 100644
index c57c17bcaa88..000000000000
--- a/docs/es/sql-reference/index.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Referencia SQL
-toc_hidden: true
-toc_priority: 28
-toc_title: oculto
----
-
-# Referencia SQL {#sql-reference}
-
-ClickHouse admite los siguientes tipos de consultas:
-
--   [SELECT](statements/select/index.md)
--   [INSERT INTO](statements/insert-into.md)
--   [CREATE](statements/create.md)
--   [ALTER](statements/alter.md#query_language_queries_alter)
--   [Otros tipos de consultas](statements/misc.md)
-
-[Artículo Original](https://clickhouse.tech/docs/en/sql-reference/) <!--hide-->
diff --git a/docs/es/sql-reference/operators/in.md b/docs/es/sql-reference/operators/in.md
deleted file mode 100644
index 988aafb25cb2..000000000000
--- a/docs/es/sql-reference/operators/in.md
+++ /dev/null
@@ -1,204 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-### IN Operadores {#select-in-operators}
-
-El `IN`, `NOT IN`, `GLOBAL IN`, y `GLOBAL NOT IN` están cubiertos por separado, ya que su funcionalidad es bastante rica.
-
-El lado izquierdo del operador es una sola columna o una tupla.
-
-Ejemplos:
-
-``` sql
-SELECT UserID IN (123, 456) FROM ...
-SELECT (CounterID, UserID) IN ((34, 123), (101500, 456)) FROM ...
-```
-
-Si el lado izquierdo es una sola columna que está en el índice, y el lado derecho es un conjunto de constantes, el sistema usa el índice para procesar la consulta.
-
-Don't list too many values explicitly (i.e. millions). If a data set is large, put it in a temporary table (for example, see the section “External data for query processing”), luego use una subconsulta.
-
-El lado derecho del operador puede ser un conjunto de expresiones constantes, un conjunto de tuplas con expresiones constantes (mostradas en los ejemplos anteriores) o el nombre de una tabla de base de datos o subconsulta SELECT entre paréntesis.
-
-Si el lado derecho del operador es el nombre de una tabla (por ejemplo, `UserID IN users`), esto es equivalente a la subconsulta `UserID IN (SELECT * FROM users)`. Úselo cuando trabaje con datos externos que se envían junto con la consulta. Por ejemplo, la consulta se puede enviar junto con un conjunto de ID de usuario ‘users’ tabla temporal, que debe ser filtrada.
-
-Si el lado derecho del operador es un nombre de tabla que tiene el motor Set (un conjunto de datos preparado que siempre está en RAM), el conjunto de datos no se volverá a crear para cada consulta.
-
-La subconsulta puede especificar más de una columna para filtrar tuplas.
-Ejemplo:
-
-``` sql
-SELECT (CounterID, UserID) IN (SELECT CounterID, UserID FROM ...) FROM ...
-```
-
-Las columnas a la izquierda y a la derecha del operador IN deben tener el mismo tipo.
-
-El operador IN y la subconsulta pueden aparecer en cualquier parte de la consulta, incluidas las funciones agregadas y las funciones lambda.
-Ejemplo:
-
-``` sql
-SELECT
-    EventDate,
-    avg(UserID IN
-    (
-        SELECT UserID
-        FROM test.hits
-        WHERE EventDate = toDate('2014-03-17')
-    )) AS ratio
-FROM test.hits
-GROUP BY EventDate
-ORDER BY EventDate ASC
-```
-
-``` text
-┌──EventDate─┬────ratio─┐
-│ 2014-03-17 │        1 │
-│ 2014-03-18 │ 0.807696 │
-│ 2014-03-19 │ 0.755406 │
-│ 2014-03-20 │ 0.723218 │
-│ 2014-03-21 │ 0.697021 │
-│ 2014-03-22 │ 0.647851 │
-│ 2014-03-23 │ 0.648416 │
-└────────────┴──────────┘
-```
-
-Para cada día después del 17 de marzo, cuente el porcentaje de páginas vistas realizadas por los usuarios que visitaron el sitio el 17 de marzo.
-Una subconsulta en la cláusula IN siempre se ejecuta una sola vez en un único servidor. No hay subconsultas dependientes.
-
-## Procesamiento NULL {#null-processing-1}
-
-Durante el procesamiento de la solicitud, el operador IN asume que el resultado de una operación [NULL](../syntax.md#null-literal) siempre es igual a `0`, independientemente de si `NULL` está en el lado derecho o izquierdo del operador. `NULL` Los valores no se incluyen en ningún conjunto de datos, no se corresponden entre sí y no se pueden comparar.
-
-Aquí hay un ejemplo con el `t_null` tabla:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-Ejecución de la consulta `SELECT x FROM t_null WHERE y IN (NULL,3)` da el siguiente resultado:
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-Se puede ver que la fila en la que `y = NULL` se expulsa de los resultados de la consulta. Esto se debe a que ClickHouse no puede decidir si `NULL` está incluido en el `(NULL,3)` conjunto, devuelve `0` como resultado de la operación, y `SELECT` excluye esta fila de la salida final.
-
-``` sql
-SELECT y IN (NULL, 3)
-FROM t_null
-```
-
-``` text
-┌─in(y, tuple(NULL, 3))─┐
-│                     0 │
-│                     1 │
-└───────────────────────┘
-```
-
-## Subconsultas distribuidas {#select-distributed-subqueries}
-
-Hay dos opciones para IN-s con subconsultas (similar a JOINs): normal `IN` / `JOIN` y `GLOBAL IN` / `GLOBAL JOIN`. Se diferencian en cómo se ejecutan para el procesamiento de consultas distribuidas.
-
-!!! attention "Atención"
-    Recuerde que los algoritmos descritos a continuación pueden funcionar de manera diferente dependiendo de la [configuración](../../operations/settings/settings.md) `distributed_product_mode` configuración.
-
-Cuando se utiliza el IN normal, la consulta se envía a servidores remotos, y cada uno de ellos ejecuta las subconsultas en el `IN` o `JOIN` clausula.
-
-Cuando se utiliza `GLOBAL IN` / `GLOBAL JOINs`, primero todas las subconsultas se ejecutan para `GLOBAL IN` / `GLOBAL JOINs`, y los resultados se recopilan en tablas temporales. A continuación, las tablas temporales se envían a cada servidor remoto, donde las consultas se ejecutan utilizando estos datos temporales.
-
-Para una consulta no distribuida, utilice el `IN` / `JOIN`.
-
-Tenga cuidado al usar subconsultas en el `IN` / `JOIN` para el procesamiento de consultas distribuidas.
-
-Veamos algunos ejemplos. Supongamos que cada servidor del clúster tiene un **local_table**. Cada servidor también tiene un **distributed_table** mesa con el **Distribuido** tipo, que mira todos los servidores del clúster.
-
-Para una consulta al **distributed_table**, la consulta se enviará a todos los servidores remotos y se ejecutará en ellos usando el **local_table**.
-
-Por ejemplo, la consulta
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table
-```
-
-se enviará a todos los servidores remotos como
-
-``` sql
-SELECT uniq(UserID) FROM local_table
-```
-
-y ejecutar en cada uno de ellos en paralelo, hasta que llegue a la etapa donde se pueden combinar resultados intermedios. Luego, los resultados intermedios se devolverán al servidor solicitante y se fusionarán en él, y el resultado final se enviará al cliente.
-
-Ahora examinemos una consulta con IN:
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
-```
-
--   Cálculo de la intersección de audiencias de dos sitios.
-
-Esta consulta se enviará a todos los servidores remotos como
-
-``` sql
-SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
-```
-
-En otras palabras, los datos establecidos en la cláusula IN se recopilarán en cada servidor de forma independiente, solo a través de los datos que se almacenan localmente en cada uno de los servidores.
-
-Esto funcionará correctamente y de manera óptima si está preparado para este caso y ha distribuido datos en los servidores de clúster de modo que los datos de un único ID de usuario residen completamente en un único servidor. En este caso, todos los datos necesarios estarán disponibles localmente en cada servidor. De lo contrario, el resultado será inexacto. Nos referimos a esta variación de la consulta como “local IN”.
-
-Para corregir cómo funciona la consulta cuando los datos se distribuyen aleatoriamente entre los servidores de clúster, puede especificar **distributed_table** dentro de una subconsulta. La consulta se vería así:
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
-```
-
-Esta consulta se enviará a todos los servidores remotos como
-
-``` sql
-SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
-```
-
-La subconsulta comenzará a ejecutarse en cada servidor remoto. Dado que la subconsulta utiliza una tabla distribuida, la subconsulta que se encuentra en cada servidor remoto se reenviará a cada servidor remoto como
-
-``` sql
-SELECT UserID FROM local_table WHERE CounterID = 34
-```
-
-Por ejemplo, si tiene un clúster de 100 servidores, la ejecución de toda la consulta requerirá 10.000 solicitudes elementales, lo que generalmente se considera inaceptable.
-
-En tales casos, siempre debe usar GLOBAL IN en lugar de IN. Veamos cómo funciona para la consulta
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID GLOBAL IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
-```
-
-El servidor del solicitante ejecutará la subconsulta
-
-``` sql
-SELECT UserID FROM distributed_table WHERE CounterID = 34
-```
-
-y el resultado se colocará en una tabla temporal en la RAM. A continuación, la solicitud se enviará a cada servidor remoto como
-
-``` sql
-SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID GLOBAL IN _data1
-```
-
-y la tabla temporal `_data1` se enviará a cada servidor remoto con la consulta (el nombre de la tabla temporal está definido por la implementación).
-
-Esto es más óptimo que usar el IN normal. Sin embargo, tenga en cuenta los siguientes puntos:
-
-1.  Al crear una tabla temporal, los datos no se hacen únicos. Para reducir el volumen de datos transmitidos a través de la red, especifique DISTINCT en la subconsulta. (No necesita hacer esto para un IN normal.)
-2.  La tabla temporal se enviará a todos los servidores remotos. La transmisión no tiene en cuenta la topología de red. Por ejemplo, si 10 servidores remotos residen en un centro de datos que es muy remoto en relación con el servidor solicitante, los datos se enviarán 10 veces a través del canal al centro de datos remoto. Intente evitar grandes conjuntos de datos cuando use GLOBAL IN.
-3.  Al transmitir datos a servidores remotos, las restricciones en el ancho de banda de la red no son configurables. Puede sobrecargar la red.
-4.  Intente distribuir datos entre servidores para que no necesite usar GLOBAL IN de forma regular.
-5.  Si necesita utilizar GLOBAL IN con frecuencia, planifique la ubicación del clúster ClickHouse para que un único grupo de réplicas resida en no más de un centro de datos con una red rápida entre ellos, de modo que una consulta se pueda procesar completamente dentro de un único centro de datos.
-
-También tiene sentido especificar una tabla local en el `GLOBAL IN` cláusula, en caso de que esta tabla local solo esté disponible en el servidor solicitante y desee usar datos de ella en servidores remotos.
diff --git a/docs/es/sql-reference/operators/index.md b/docs/es/sql-reference/operators/index.md
deleted file mode 100644
index cc0eed530898..000000000000
--- a/docs/es/sql-reference/operators/index.md
+++ /dev/null
@@ -1,277 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: Operador
----
-
-# Operador {#operators}
-
-ClickHouse transforma los operadores a sus funciones correspondientes en la etapa de análisis de consultas de acuerdo con su prioridad, precedencia y asociatividad.
-
-## Operadores de acceso {#access-operators}
-
-`a[N]` – Access to an element of an array. The `arrayElement(a, N)` función.
-
-`a.N` – Access to a tuple element. The `tupleElement(a, N)` función.
-
-## Operador de negación numérica {#numeric-negation-operator}
-
-`-a` – The `negate (a)` función.
-
-## Operadores de multiplicación y división {#multiplication-and-division-operators}
-
-`a * b` – The `multiply (a, b)` función.
-
-`a / b` – The `divide(a, b)` función.
-
-`a % b` – The `modulo(a, b)` función.
-
-## Operadores de suma y resta {#addition-and-subtraction-operators}
-
-`a + b` – The `plus(a, b)` función.
-
-`a - b` – The `minus(a, b)` función.
-
-## Operadores de comparación {#comparison-operators}
-
-`a = b` – The `equals(a, b)` función.
-
-`a == b` – The `equals(a, b)` función.
-
-`a != b` – The `notEquals(a, b)` función.
-
-`a <> b` – The `notEquals(a, b)` función.
-
-`a <= b` – The `lessOrEquals(a, b)` función.
-
-`a >= b` – The `greaterOrEquals(a, b)` función.
-
-`a < b` – The `less(a, b)` función.
-
-`a > b` – The `greater(a, b)` función.
-
-`a LIKE s` – The `like(a, b)` función.
-
-`a NOT LIKE s` – The `notLike(a, b)` función.
-
-`a BETWEEN b AND c` – The same as `a >= b AND a <= c`.
-
-`a NOT BETWEEN b AND c` – The same as `a < b OR a > c`.
-
-## Operadores para trabajar con conjuntos de datos {#operators-for-working-with-data-sets}
-
-*Ver [IN operadores](in.md).*
-
-`a IN ...` – The `in(a, b)` función.
-
-`a NOT IN ...` – The `notIn(a, b)` función.
-
-`a GLOBAL IN ...` – The `globalIn(a, b)` función.
-
-`a GLOBAL NOT IN ...` – The `globalNotIn(a, b)` función.
-
-## Operadores para trabajar con fechas y horas {#operators-datetime}
-
-### EXTRACT {#operator-extract}
-
-``` sql
-EXTRACT(part FROM date);
-```
-
-Extraer partes de una fecha determinada. Por ejemplo, puede recuperar un mes a partir de una fecha determinada o un segundo a partir de una hora.
-
-El `part` parámetro especifica qué parte de la fecha se va a recuperar. Los siguientes valores están disponibles:
-
--   `DAY` — The day of the month. Possible values: 1–31.
--   `MONTH` — The number of a month. Possible values: 1–12.
--   `YEAR` — The year.
--   `SECOND` — The second. Possible values: 0–59.
--   `MINUTE` — The minute. Possible values: 0–59.
--   `HOUR` — The hour. Possible values: 0–23.
-
-El `part` El parámetro no distingue entre mayúsculas y minúsculas.
-
-El `date` parámetro especifica la fecha o la hora a procesar. Bien [Fecha](../../sql-reference/data-types/date.md) o [FechaHora](../../sql-reference/data-types/datetime.md) tipo es compatible.
-
-Ejemplos:
-
-``` sql
-SELECT EXTRACT(DAY FROM toDate('2017-06-15'));
-SELECT EXTRACT(MONTH FROM toDate('2017-06-15'));
-SELECT EXTRACT(YEAR FROM toDate('2017-06-15'));
-```
-
-En el siguiente ejemplo creamos una tabla e insertamos en ella un valor con el `DateTime` tipo.
-
-``` sql
-CREATE TABLE test.Orders
-(
-    OrderId UInt64,
-    OrderName String,
-    OrderDate DateTime
-)
-ENGINE = Log;
-```
-
-``` sql
-INSERT INTO test.Orders VALUES (1, 'Jarlsberg Cheese', toDateTime('2008-10-11 13:23:44'));
-```
-
-``` sql
-SELECT
-    toYear(OrderDate) AS OrderYear,
-    toMonth(OrderDate) AS OrderMonth,
-    toDayOfMonth(OrderDate) AS OrderDay,
-    toHour(OrderDate) AS OrderHour,
-    toMinute(OrderDate) AS OrderMinute,
-    toSecond(OrderDate) AS OrderSecond
-FROM test.Orders;
-```
-
-``` text
-┌─OrderYear─┬─OrderMonth─┬─OrderDay─┬─OrderHour─┬─OrderMinute─┬─OrderSecond─┐
-│      2008 │         10 │       11 │        13 │          23 │          44 │
-└───────────┴────────────┴──────────┴───────────┴─────────────┴─────────────┘
-```
-
-Puedes ver más ejemplos en [prueba](https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/00619_extract.sql).
-
-### INTERVAL {#operator-interval}
-
-Crea un [Intervalo](../../sql-reference/data-types/special-data-types/interval.md)-type valor que debe utilizarse en operaciones aritméticas con [Fecha](../../sql-reference/data-types/date.md) y [FechaHora](../../sql-reference/data-types/datetime.md)-type valores.
-
-Tipos de intervalos:
-- `SECOND`
-- `MINUTE`
-- `HOUR`
-- `DAY`
-- `WEEK`
-- `MONTH`
-- `QUARTER`
-- `YEAR`
-
-!!! warning "Advertencia"
-    Los intervalos con diferentes tipos no se pueden combinar. No puedes usar expresiones como `INTERVAL 4 DAY 1 HOUR`. Especifique intervalos en unidades que sean más pequeñas o iguales que la unidad más pequeña del intervalo, por ejemplo, `INTERVAL 25 HOUR`. Puede usar operaciones consecutivas, como en el siguiente ejemplo.
-
-Ejemplo:
-
-``` sql
-SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR
-```
-
-``` text
-┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐
-│ 2019-10-23 11:16:28 │                                    2019-10-27 14:16:28 │
-└─────────────────────┴────────────────────────────────────────────────────────┘
-```
-
-**Ver también**
-
--   [Intervalo](../../sql-reference/data-types/special-data-types/interval.md) tipo de datos
--   [ToInterval](../../sql-reference/functions/type-conversion-functions.md#function-tointerval) funciones de conversión de tipo
-
-## Operador de Negación Lógica {#logical-negation-operator}
-
-`NOT a` – The `not(a)` función.
-
-## Operador lógico and {#logical-and-operator}
-
-`a AND b` – The`and(a, b)` función.
-
-## Operador lógico or {#logical-or-operator}
-
-`a OR b` – The `or(a, b)` función.
-
-## Operador condicional {#conditional-operator}
-
-`a ? b : c` – The `if(a, b, c)` función.
-
-Nota:
-
-El operador condicional calcula los valores de b y c, luego verifica si se cumple la condición a y luego devuelve el valor correspondiente. Si `b` o `C` es una [arrayJoin()](../../sql-reference/functions/array-join.md#functions_arrayjoin) función, cada fila se replicará independientemente de la “a” condición.
-
-## Expresión condicional {#operator_case}
-
-``` sql
-CASE [x]
-    WHEN a THEN b
-    [WHEN ... THEN ...]
-    [ELSE c]
-END
-```
-
-Si `x` se especifica, entonces `transform(x, [a, ...], [b, ...], c)` function is used. Otherwise – `multiIf(a, b, ..., c)`.
-
-Si no hay `ELSE c` cláusula en la expresión, el valor predeterminado es `NULL`.
-
-El `transform` no funciona con `NULL`.
-
-## Operador de Concatenación {#concatenation-operator}
-
-`s1 || s2` – The `concat(s1, s2) function.`
-
-## Operador de Creación Lambda {#lambda-creation-operator}
-
-`x -> expr` – The `lambda(x, expr) function.`
-
-Los siguientes operadores no tienen prioridad ya que son corchetes:
-
-## Operador de creación de matrices {#array-creation-operator}
-
-`[x1, ...]` – The `array(x1, ...) function.`
-
-## Operador de creación de tupla {#tuple-creation-operator}
-
-`(x1, x2, ...)` – The `tuple(x2, x2, ...) function.`
-
-## Asociatividad {#associativity}
-
-Todos los operadores binarios han dejado asociatividad. Por ejemplo, `1 + 2 + 3` se transforma a `plus(plus(1, 2), 3)`.
-A veces esto no funciona de la manera que esperas. Por ejemplo, `SELECT 4 > 2 > 3` resultará en 0.
-
-Para la eficiencia, el `and` y `or` funciones aceptan cualquier número de argumentos. Las cadenas correspondientes de `AND` y `OR` operadores se transforman en una sola llamada de estas funciones.
-
-## Comprobación de `NULL` {#checking-for-null}
-
-ClickHouse soporta el `IS NULL` y `IS NOT NULL` operador.
-
-### IS NULL {#operator-is-null}
-
--   Para [NULL](../../sql-reference/data-types/nullable.md) valores de tipo, el `IS NULL` operador devuelve:
-    -   `1` si el valor es `NULL`.
-    -   `0` de lo contrario.
--   Para otros valores, el `IS NULL` operador siempre devuelve `0`.
-
-<!-- -->
-
-``` sql
-SELECT x+100 FROM t_null WHERE y IS NULL
-```
-
-``` text
-┌─plus(x, 100)─┐
-│          101 │
-└──────────────┘
-```
-
-### IS NOT NULL {#is-not-null}
-
--   Para [NULL](../../sql-reference/data-types/nullable.md) valores de tipo, el `IS NOT NULL` operador devuelve:
-    -   `0` si el valor es `NULL`.
-    -   `1` de lo contrario.
--   Para otros valores, el `IS NOT NULL` operador siempre devuelve `1`.
-
-<!-- -->
-
-``` sql
-SELECT * FROM t_null WHERE y IS NOT NULL
-```
-
-``` text
-┌─x─┬─y─┐
-│ 2 │ 3 │
-└───┴───┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/operators/) <!--hide-->
diff --git a/docs/es/sql-reference/statements/alter.md b/docs/es/sql-reference/statements/alter.md
deleted file mode 100644
index 194b0cea00f6..000000000000
--- a/docs/es/sql-reference/statements/alter.md
+++ /dev/null
@@ -1,602 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: ALTER
----
-
-## ALTER {#query_language_queries_alter}
-
-El `ALTER` consulta sólo se admite para `*MergeTree` mesas, así como `Merge`y`Distributed`. La consulta tiene varias variaciones.
-
-### Manipulaciones de columna {#column-manipulations}
-
-Cambiar la estructura de la tabla.
-
-``` sql
-ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|CLEAR|COMMENT|MODIFY COLUMN ...
-```
-
-En la consulta, especifique una lista de una o más acciones separadas por comas.
-Cada acción es una operación en una columna.
-
-Se admiten las siguientes acciones:
-
--   [ADD COLUMN](#alter_add-column) — Adds a new column to the table.
--   [DROP COLUMN](#alter_drop-column) — Deletes the column.
--   [CLEAR COLUMN](#alter_clear-column) — Resets column values.
--   [COMMENT COLUMN](#alter_comment-column) — Adds a text comment to the column.
--   [MODIFY COLUMN](#alter_modify-column) — Changes column's type, default expression and TTL.
-
-Estas acciones se describen en detalle a continuación.
-
-#### ADD COLUMN {#alter_add-column}
-
-``` sql
-ADD COLUMN [IF NOT EXISTS] name [type] [default_expr] [codec] [AFTER name_after]
-```
-
-Agrega una nueva columna a la tabla con el `name`, `type`, [`codec`](create.md#codecs) y `default_expr` (ver la sección [Expresiones predeterminadas](create.md#create-default-values)).
-
-Si el `IF NOT EXISTS` cláusula, la consulta no devolverá un error si la columna ya existe. Si especifica `AFTER name_after` (el nombre de otra columna), la columna se agrega después de la especificada en la lista de columnas de tabla. De lo contrario, la columna se agrega al final de la tabla. Tenga en cuenta que no hay forma de agregar una columna al principio de una tabla. Para una cadena de acciones, `name_after` puede ser el nombre de una columna que se agrega en una de las acciones anteriores.
-
-Agregar una columna solo cambia la estructura de la tabla, sin realizar ninguna acción con datos. Los datos no aparecen en el disco después de `ALTER`. Si faltan los datos para una columna al leer de la tabla, se rellena con valores predeterminados (realizando la expresión predeterminada si hay una, o usando ceros o cadenas vacías). La columna aparece en el disco después de fusionar partes de datos (consulte [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md)).
-
-Este enfoque nos permite completar el `ALTER` consulta al instante, sin aumentar el volumen de datos antiguos.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE visits ADD COLUMN browser String AFTER user_id
-```
-
-#### DROP COLUMN {#alter_drop-column}
-
-``` sql
-DROP COLUMN [IF EXISTS] name
-```
-
-Elimina la columna con el nombre `name`. Si el `IF EXISTS` se especifica una cláusula, la consulta no devolverá un error si la columna no existe.
-
-Elimina datos del sistema de archivos. Dado que esto elimina archivos completos, la consulta se completa casi al instante.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE visits DROP COLUMN browser
-```
-
-#### CLEAR COLUMN {#alter_clear-column}
-
-``` sql
-CLEAR COLUMN [IF EXISTS] name IN PARTITION partition_name
-```
-
-Restablece todos los datos de una columna para una partición especificada. Obtenga más información sobre cómo configurar el nombre de la partición en la sección [Cómo especificar la expresión de partición](#alter-how-to-specify-part-expr).
-
-Si el `IF EXISTS` se especifica una cláusula, la consulta no devolverá un error si la columna no existe.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE visits CLEAR COLUMN browser IN PARTITION tuple()
-```
-
-#### COMMENT COLUMN {#alter_comment-column}
-
-``` sql
-COMMENT COLUMN [IF EXISTS] name 'comment'
-```
-
-Agrega un comentario a la columna. Si el `IF EXISTS` se especifica una cláusula, la consulta no devolverá un error si la columna no existe.
-
-Cada columna puede tener un comentario. Si ya existe un comentario para la columna, un nuevo comentario sobrescribe el comentario anterior.
-
-Los comentarios se almacenan en el `comment_expression` columna devuelta por el [DESCRIBE TABLE](misc.md#misc-describe-table) consulta.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE visits COMMENT COLUMN browser 'The table shows the browser used for accessing the site.'
-```
-
-#### MODIFY COLUMN {#alter_modify-column}
-
-``` sql
-MODIFY COLUMN [IF EXISTS] name [type] [default_expr] [TTL]
-```
-
-Esta consulta cambia el `name` propiedades de la columna:
-
--   Tipo
-
--   Expresión predeterminada
-
--   TTL
-
-        For examples of columns TTL modifying, see [Column TTL](../engines/table_engines/mergetree_family/mergetree.md#mergetree-column-ttl).
-
-Si el `IF EXISTS` se especifica una cláusula, la consulta no devolverá un error si la columna no existe.
-
-Al cambiar el tipo, los valores se convierten como si [ToType](../../sql-reference/functions/type-conversion-functions.md) se les aplicaron funciones. Si solo se cambia la expresión predeterminada, la consulta no hace nada complejo y se completa casi al instante.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE visits MODIFY COLUMN browser Array(String)
-```
-
-Changing the column type is the only complex action – it changes the contents of files with data. For large tables, this may take a long time.
-
-Hay varias etapas de procesamiento:
-
--   Preparación de archivos temporales (nuevos) con datos modificados.
--   Cambiar el nombre de los archivos antiguos.
--   Cambiar el nombre de los archivos temporales (nuevos) a los nombres antiguos.
--   Eliminar los archivos antiguos.
-
-Solo la primera etapa lleva tiempo. Si hay un error en esta etapa, los datos no se cambian.
-Si hay un error durante una de las etapas sucesivas, los datos se pueden restaurar manualmente. La excepción es si los archivos antiguos se eliminaron del sistema de archivos, pero los datos de los nuevos archivos no se escribieron en el disco y se perdieron.
-
-El `ALTER` se replica la consulta para cambiar columnas. Las instrucciones se guardan en ZooKeeper, luego cada réplica las aplica. Todo `ALTER` las consultas se ejecutan en el mismo orden. La consulta espera a que se completen las acciones adecuadas en las otras réplicas. Sin embargo, una consulta para cambiar columnas en una tabla replicada se puede interrumpir y todas las acciones se realizarán de forma asincrónica.
-
-#### Limitaciones de consulta ALTER {#alter-query-limitations}
-
-El `ALTER` query le permite crear y eliminar elementos separados (columnas) en estructuras de datos anidadas, pero no en estructuras de datos anidadas completas. Para agregar una estructura de datos anidada, puede agregar columnas con un nombre como `name.nested_name` y el tipo `Array(T)`. Una estructura de datos anidada es equivalente a varias columnas de matriz con un nombre que tiene el mismo prefijo antes del punto.
-
-No hay soporte para eliminar columnas en la clave principal o la clave de muestreo (columnas que se utilizan en el `ENGINE` expresion). Solo es posible cambiar el tipo de las columnas que se incluyen en la clave principal si este cambio no provoca que se modifiquen los datos (por ejemplo, puede agregar valores a un Enum o cambiar un tipo de `DateTime` a `UInt32`).
-
-Si el `ALTER` la consulta no es suficiente para realizar los cambios en la tabla que necesita, puede crear una nueva tabla, copiar los datos [INSERT SELECT](insert-into.md#insert_query_insert-select) consulta, luego cambie las tablas usando el [RENAME](misc.md#misc_operations-rename) consulta y elimina la tabla anterior. Puede usar el [Método de codificación de datos:](../../operations/utilities/clickhouse-copier.md) como una alternativa a la `INSERT SELECT` consulta.
-
-El `ALTER` query bloquea todas las lecturas y escrituras para la tabla. En otras palabras, si un largo `SELECT` se está ejecutando en el momento de la `ALTER` consulta, el `ALTER` la consulta esperará a que se complete. Al mismo tiempo, todas las consultas nuevas a la misma tabla esperarán `ALTER` se está ejecutando.
-
-Para tablas que no almacenan datos por sí mismas (como `Merge` y `Distributed`), `ALTER` simplemente cambia la estructura de la tabla, y no cambia la estructura de las tablas subordinadas. Por ejemplo, cuando se ejecuta ALTER para un `Distributed` mesa, también tendrá que ejecutar `ALTER` para las tablas en todos los servidores remotos.
-
-### Manipulaciones con expresiones clave {#manipulations-with-key-expressions}
-
-Se admite el siguiente comando:
-
-``` sql
-MODIFY ORDER BY new_expression
-```
-
-Solo funciona para tablas en el [`MergeTree`](../../engines/table-engines/mergetree-family/mergetree.md) familia (incluyendo
-[repetición](../../engines/table-engines/mergetree-family/replication.md) tabla). El comando cambia el
-[clave de clasificación](../../engines/table-engines/mergetree-family/mergetree.md) de la mesa
-a `new_expression` (una expresión o una tupla de expresiones). La clave principal sigue siendo la misma.
-
-El comando es liviano en el sentido de que solo cambia los metadatos. Para mantener la propiedad esa parte de datos
-las filas están ordenadas por la expresión de clave de ordenación, no puede agregar expresiones que contengan columnas existentes
-a la clave de ordenación (sólo las columnas añadidas `ADD COLUMN` comando en el mismo `ALTER` consulta).
-
-### Manipulaciones con índices de saltos de datos {#manipulations-with-data-skipping-indices}
-
-Solo funciona para tablas en el [`*MergeTree`](../../engines/table-engines/mergetree-family/mergetree.md) familia (incluyendo
-[repetición](../../engines/table-engines/mergetree-family/replication.md) tabla). Las siguientes operaciones
-están disponibles:
-
--   `ALTER TABLE [db].name ADD INDEX name expression TYPE type GRANULARITY value AFTER name [AFTER name2]` - Agrega la descripción del índice a los metadatos de las tablas.
-
--   `ALTER TABLE [db].name DROP INDEX name` - Elimina la descripción del índice de los metadatos de las tablas y elimina los archivos de índice del disco.
-
-Estos comandos son livianos en el sentido de que solo cambian los metadatos o eliminan archivos.
-Además, se replican (sincronizando metadatos de índices a través de ZooKeeper).
-
-### Manipulaciones con restricciones {#manipulations-with-constraints}
-
-Ver más en [limitación](create.md#constraints)
-
-Las restricciones se pueden agregar o eliminar utilizando la siguiente sintaxis:
-
-``` sql
-ALTER TABLE [db].name ADD CONSTRAINT constraint_name CHECK expression;
-ALTER TABLE [db].name DROP CONSTRAINT constraint_name;
-```
-
-Las consultas agregarán o eliminarán metadatos sobre restricciones de la tabla para que se procesen inmediatamente.
-
-Comprobación de restricciones *no se ejecutará* en los datos existentes si se agregaron.
-
-Todos los cambios en las tablas replicadas se transmiten a ZooKeeper, por lo que se aplicarán en otras réplicas.
-
-### Manipulaciones con particiones y piezas {#alter_manipulations-with-partitions}
-
-Las siguientes operaciones con [partición](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) están disponibles:
-
--   [DETACH PARTITION](#alter_detach-partition) – Moves a partition to the `detached` directorio y olvidarlo.
--   [DROP PARTITION](#alter_drop-partition) – Deletes a partition.
--   [ATTACH PART\|PARTITION](#alter_attach-partition) – Adds a part or partition from the `detached` directorio a la tabla.
--   [ATTACH PARTITION FROM](#alter_attach-partition-from) – Copies the data partition from one table to another and adds.
--   [REPLACE PARTITION](#alter_replace-partition) - Copia la partición de datos de una tabla a otra y reemplaza.
--   [MOVE PARTITION TO TABLE](#alter_move_to_table-partition)(#alter_move_to_table-partition) - Mover la partición de datos de una tabla a otra.
--   [CLEAR COLUMN IN PARTITION](#alter_clear-column-partition) - Restablece el valor de una columna especificada en una partición.
--   [CLEAR INDEX IN PARTITION](#alter_clear-index-partition) - Restablece el índice secundario especificado en una partición.
--   [FREEZE PARTITION](#alter_freeze-partition) – Creates a backup of a partition.
--   [FETCH PARTITION](#alter_fetch-partition) – Downloads a partition from another server.
--   [MOVE PARTITION\|PART](#alter_move-partition) – Move partition/data part to another disk or volume.
-
-<!-- -->
-
-#### DETACH PARTITION {#alter_detach-partition}
-
-``` sql
-ALTER TABLE table_name DETACH PARTITION partition_expr
-```
-
-Mueve todos los datos de la partición especificada `detached` directorio. El servidor se olvida de la partición de datos separada como si no existiera. El servidor no sabrá acerca de estos datos hasta que [ATTACH](#alter_attach-partition) consulta.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE visits DETACH PARTITION 201901
-```
-
-Lea cómo configurar la expresión de partición en una sección [Cómo especificar la expresión de partición](#alter-how-to-specify-part-expr).
-
-Después de ejecutar la consulta, puede hacer lo que quiera con los datos en el `detached` directory — delete it from the file system, or just leave it.
-
-This query is replicated – it moves the data to the `detached` directorio en todas las réplicas. Tenga en cuenta que solo puede ejecutar esta consulta en una réplica de líder. Para averiguar si una réplica es un líder, realice `SELECT` consulta a la [sistema.Replica](../../operations/system-tables.md#system_tables-replicas) tabla. Alternativamente, es más fácil hacer un `DETACH` consulta en todas las réplicas: todas las réplicas producen una excepción, excepto la réplica líder.
-
-#### DROP PARTITION {#alter_drop-partition}
-
-``` sql
-ALTER TABLE table_name DROP PARTITION partition_expr
-```
-
-Elimina la partición especificada de la tabla. Esta consulta etiqueta la partición como inactiva y elimina los datos por completo, aproximadamente en 10 minutos.
-
-Lea cómo configurar la expresión de partición en una sección [Cómo especificar la expresión de partición](#alter-how-to-specify-part-expr).
-
-The query is replicated – it deletes data on all replicas.
-
-#### DROP DETACHED PARTITION\|PART {#alter_drop-detached}
-
-``` sql
-ALTER TABLE table_name DROP DETACHED PARTITION|PART partition_expr
-```
-
-Quita la parte especificada o todas las partes de la partición especificada de `detached`.
-Más información sobre cómo establecer la expresión de partición en una sección [Cómo especificar la expresión de partición](#alter-how-to-specify-part-expr).
-
-#### ATTACH PARTITION\|PART {#alter_attach-partition}
-
-``` sql
-ALTER TABLE table_name ATTACH PARTITION|PART partition_expr
-```
-
-Agrega datos a la tabla desde el `detached` directorio. Es posible agregar datos para una partición completa o para una parte separada. Ejemplos:
-
-``` sql
-ALTER TABLE visits ATTACH PARTITION 201901;
-ALTER TABLE visits ATTACH PART 201901_2_2_0;
-```
-
-Más información sobre cómo establecer la expresión de partición en una sección [Cómo especificar la expresión de partición](#alter-how-to-specify-part-expr).
-
-Esta consulta se replica. El iniciador de réplica comprueba si hay datos en el `detached` directorio. Si existen datos, la consulta comprueba su integridad. Si todo es correcto, la consulta agrega los datos a la tabla. Todas las demás réplicas descargan los datos del iniciador de réplica.
-
-Entonces puedes poner datos en el `detached` en una réplica, y utilice el directorio `ALTER ... ATTACH` consulta para agregarlo a la tabla en todas las réplicas.
-
-#### ATTACH PARTITION FROM {#alter_attach-partition-from}
-
-``` sql
-ALTER TABLE table2 ATTACH PARTITION partition_expr FROM table1
-```
-
-Esta consulta copia la partición de datos `table1` a `table2` añade datos a los que existen en el `table2`. Tenga en cuenta que los datos no se eliminarán de `table1`.
-
-Para que la consulta se ejecute correctamente, se deben cumplir las siguientes condiciones:
-
--   Ambas tablas deben tener la misma estructura.
--   Ambas tablas deben tener la misma clave de partición.
-
-#### REPLACE PARTITION {#alter_replace-partition}
-
-``` sql
-ALTER TABLE table2 REPLACE PARTITION partition_expr FROM table1
-```
-
-Esta consulta copia la partición de datos `table1` a `table2` y reemplaza la partición existente en el `table2`. Tenga en cuenta que los datos no se eliminarán de `table1`.
-
-Para que la consulta se ejecute correctamente, se deben cumplir las siguientes condiciones:
-
--   Ambas tablas deben tener la misma estructura.
--   Ambas tablas deben tener la misma clave de partición.
-
-#### MOVE PARTITION TO TABLE {#alter_move_to_table-partition}
-
-``` sql
-ALTER TABLE table_source MOVE PARTITION partition_expr TO TABLE table_dest
-```
-
-Esta consulta mueve la partición de datos `table_source` a `table_dest` con la eliminación de los datos de `table_source`.
-
-Para que la consulta se ejecute correctamente, se deben cumplir las siguientes condiciones:
-
--   Ambas tablas deben tener la misma estructura.
--   Ambas tablas deben tener la misma clave de partición.
--   Ambas tablas deben ser de la misma familia de motores. (replicado o no replicado)
--   Ambas tablas deben tener la misma política de almacenamiento.
-
-#### CLEAR COLUMN IN PARTITION {#alter_clear-column-partition}
-
-``` sql
-ALTER TABLE table_name CLEAR COLUMN column_name IN PARTITION partition_expr
-```
-
-Restablece todos los valores de la columna especificada en una partición. Si el `DEFAULT` cláusula se determinó al crear una tabla, esta consulta establece el valor de columna en un valor predeterminado especificado.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE visits CLEAR COLUMN hour in PARTITION 201902
-```
-
-#### FREEZE PARTITION {#alter_freeze-partition}
-
-``` sql
-ALTER TABLE table_name FREEZE [PARTITION partition_expr]
-```
-
-Esta consulta crea una copia de seguridad local de una partición especificada. Si el `PARTITION` se omite la cláusula, la consulta crea la copia de seguridad de todas las particiones a la vez.
-
-!!! note "Nota"
-    Todo el proceso de copia de seguridad se realiza sin detener el servidor.
-
-Tenga en cuenta que para las tablas de estilo antiguo puede especificar el prefijo del nombre de la partición (por ejemplo, ‘2019’) - entonces la consulta crea la copia de seguridad para todas las particiones correspondientes. Lea cómo configurar la expresión de partición en una sección [Cómo especificar la expresión de partición](#alter-how-to-specify-part-expr).
-
-En el momento de la ejecución, para una instantánea de datos, la consulta crea vínculos rígidos a los datos de una tabla. Los enlaces duros se colocan en el directorio `/var/lib/clickhouse/shadow/N/...`, donde:
-
--   `/var/lib/clickhouse/` es el directorio ClickHouse de trabajo especificado en la configuración.
--   `N` es el número incremental de la copia de seguridad.
-
-!!! note "Nota"
-    Si usted usa [un conjunto de discos para el almacenamiento de datos en una tabla](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes), el `shadow/N` directorio aparece en cada disco, almacenando partes de datos que coinciden con el `PARTITION` expresion.
-
-La misma estructura de directorios se crea dentro de la copia de seguridad que dentro `/var/lib/clickhouse/`. La consulta realiza ‘chmod’ para todos los archivos, prohibiendo escribir en ellos.
-
-Después de crear la copia de seguridad, puede copiar los datos desde `/var/lib/clickhouse/shadow/` al servidor remoto y, a continuación, elimínelo del servidor local. Tenga en cuenta que el `ALTER t FREEZE PARTITION` consulta no se replica. Crea una copia de seguridad local solo en el servidor local.
-
-La consulta crea una copia de seguridad casi instantáneamente (pero primero espera a que las consultas actuales a la tabla correspondiente terminen de ejecutarse).
-
-`ALTER TABLE t FREEZE PARTITION` copia solo los datos, no los metadatos de la tabla. Para hacer una copia de seguridad de los metadatos de la tabla, copie el archivo `/var/lib/clickhouse/metadata/database/table.sql`
-
-Para restaurar los datos de una copia de seguridad, haga lo siguiente:
-
-1.  Cree la tabla si no existe. Para ver la consulta, utilice el .archivo sql (reemplazar `ATTACH` en ella con `CREATE`).
-2.  Copie los datos de la `data/database/table/` directorio dentro de la copia de seguridad a la `/var/lib/clickhouse/data/database/table/detached/` directorio.
-3.  Ejecutar `ALTER TABLE t ATTACH PARTITION` consultas para agregar los datos a una tabla.
-
-La restauración desde una copia de seguridad no requiere detener el servidor.
-
-Para obtener más información sobre las copias de seguridad y la restauración de datos, consulte [Copia de seguridad de datos](../../operations/backup.md) apartado.
-
-#### CLEAR INDEX IN PARTITION {#alter_clear-index-partition}
-
-``` sql
-ALTER TABLE table_name CLEAR INDEX index_name IN PARTITION partition_expr
-```
-
-La consulta funciona de forma similar a `CLEAR COLUMN`, pero restablece un índice en lugar de una columna de datos.
-
-#### FETCH PARTITION {#alter_fetch-partition}
-
-``` sql
-ALTER TABLE table_name FETCH PARTITION partition_expr FROM 'path-in-zookeeper'
-```
-
-Descarga una partición desde otro servidor. Esta consulta solo funciona para las tablas replicadas.
-
-La consulta hace lo siguiente:
-
-1.  Descarga la partición del fragmento especificado. En ‘path-in-zookeeper’ debe especificar una ruta al fragmento en ZooKeeper.
-2.  Luego, la consulta coloca los datos descargados en el `detached` directorio de la `table_name` tabla. Utilice el [ATTACH PARTITION\|PART](#alter_attach-partition) consulta para agregar los datos a la tabla.
-
-Por ejemplo:
-
-``` sql
-ALTER TABLE users FETCH PARTITION 201902 FROM '/clickhouse/tables/01-01/visits';
-ALTER TABLE users ATTACH PARTITION 201902;
-```
-
-Tenga en cuenta que:
-
--   El `ALTER ... FETCH PARTITION` consulta no se replica. Coloca la partición en el `detached` sólo en el servidor local.
--   El `ALTER TABLE ... ATTACH` consulta se replica. Agrega los datos a todas las réplicas. Los datos se agregan a una de las réplicas desde el `detached` directorio, y para los demás - de réplicas vecinas.
-
-Antes de descargar, el sistema verifica si la partición existe y la estructura de la tabla coincide. La réplica más adecuada se selecciona automáticamente de las réplicas en buen estado.
-
-Aunque se llama a la consulta `ALTER TABLE`, no cambia la estructura de la tabla y no cambiar inmediatamente los datos disponibles en la tabla.
-
-#### MOVE PARTITION\|PART {#alter_move-partition}
-
-Mueve particiones o partes de datos a otro volumen o disco para `MergeTree`-mesas de motor. Ver [Uso de varios dispositivos de bloque para el almacenamiento de datos](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes).
-
-``` sql
-ALTER TABLE table_name MOVE PARTITION|PART partition_expr TO DISK|VOLUME 'disk_name'
-```
-
-El `ALTER TABLE t MOVE` consulta:
-
--   No replicado, porque diferentes réplicas pueden tener diferentes directivas de almacenamiento.
--   Devuelve un error si el disco o volumen especificado no está configurado. La consulta también devuelve un error si no se pueden aplicar las condiciones de movimiento de datos especificadas en la directiva de almacenamiento.
--   Puede devolver un error en el caso, cuando los datos que se moverán ya se mueven por un proceso en segundo plano, concurrente `ALTER TABLE t MOVE` consulta o como resultado de la fusión de datos de fondo. Un usuario no debe realizar ninguna acción adicional en este caso.
-
-Ejemplo:
-
-``` sql
-ALTER TABLE hits MOVE PART '20190301_14343_16206_438' TO VOLUME 'slow'
-ALTER TABLE hits MOVE PARTITION '2019-09-01' TO DISK 'fast_ssd'
-```
-
-#### Cómo establecer la expresión de partición {#alter-how-to-specify-part-expr}
-
-Puede especificar la expresión de partición en `ALTER ... PARTITION` de diferentes maneras:
-
--   Como valor de la `partition` columna de la `system.parts` tabla. Por ejemplo, `ALTER TABLE visits DETACH PARTITION 201901`.
--   Como la expresión de la columna de la tabla. Se admiten constantes y expresiones constantes. Por ejemplo, `ALTER TABLE visits DETACH PARTITION toYYYYMM(toDate('2019-01-25'))`.
--   Usando el ID de partición. El ID de partición es un identificador de cadena de la partición (legible por humanos, si es posible) que se usa como nombres de particiones en el sistema de archivos y en ZooKeeper. El ID de partición debe especificarse en el `PARTITION ID` cláusula, entre comillas simples. Por ejemplo, `ALTER TABLE visits DETACH PARTITION ID '201901'`.
--   En el [ALTER ATTACH PART](#alter_attach-partition) y [DROP DETACHED PART](#alter_drop-detached) consulta, para especificar el nombre de una parte, utilice un literal de cadena con un valor `name` columna de la [sistema.detached_parts](../../operations/system-tables.md#system_tables-detached_parts) tabla. Por ejemplo, `ALTER TABLE visits ATTACH PART '201901_1_1_0'`.
-
-El uso de comillas al especificar la partición depende del tipo de expresión de partición. Por ejemplo, para el `String` tipo, debe especificar su nombre entre comillas (`'`). Para el `Date` y `Int*` tipos no se necesitan comillas.
-
-Para las tablas de estilo antiguo, puede especificar la partición como un número `201901` o una cadena `'201901'`. La sintaxis para las tablas de nuevo estilo es más estricta con los tipos (similar al analizador para el formato de entrada VALUES).
-
-Todas las reglas anteriores también son ciertas para el [OPTIMIZE](misc.md#misc_operations-optimize) consulta. Si necesita especificar la única partición al optimizar una tabla no particionada, establezca la expresión `PARTITION tuple()`. Por ejemplo:
-
-``` sql
-OPTIMIZE TABLE table_not_partitioned PARTITION tuple() FINAL;
-```
-
-Los ejemplos de `ALTER ... PARTITION` las consultas se demuestran en las pruebas [`00502_custom_partitioning_local`](https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/00502_custom_partitioning_local.sql) y [`00502_custom_partitioning_replicated_zookeeper`](https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/00502_custom_partitioning_replicated_zookeeper.sql).
-
-### Manipulaciones con Tabla TTL {#manipulations-with-table-ttl}
-
-Usted puede cambiar [tabla TTL](../../engines/table-engines/mergetree-family/mergetree.md#mergetree-table-ttl) con una solicitud del siguiente formulario:
-
-``` sql
-ALTER TABLE table-name MODIFY TTL ttl-expression
-```
-
-### Sincronicidad de las consultas ALTER {#synchronicity-of-alter-queries}
-
-Para tablas no replicables, todas `ALTER` las consultas se realizan de forma sincrónica. Para las tablas replicables, la consulta solo agrega instrucciones para las acciones apropiadas para `ZooKeeper`, y las acciones mismas se realizan tan pronto como sea posible. Sin embargo, la consulta puede esperar a que estas acciones se completen en todas las réplicas.
-
-Para `ALTER ... ATTACH|DETACH|DROP` consultas, puede utilizar el `replication_alter_partitions_sync` configuración para configurar la espera.
-Valores posibles: `0` – do not wait; `1` – only wait for own execution (default); `2` – wait for all.
-
-### Mutación {#alter-mutations}
-
-Las mutaciones son una variante de consulta ALTER que permite cambiar o eliminar filas en una tabla. En contraste con el estándar `UPDATE` y `DELETE` consultas destinadas a cambios de datos puntuales, las mutaciones están destinadas a operaciones pesadas que cambian muchas filas en una tabla. Apoyado para el `MergeTree` familia de motores de mesa, incluidos los motores con soporte de replicación.
-
-Las tablas existentes están listas para las mutaciones tal como están (no es necesaria la conversión), pero después de que la primera mutación se aplica a una tabla, su formato de metadatos se vuelve incompatible con las versiones anteriores del servidor y volver a una versión anterior se vuelve imposible.
-
-Comandos disponibles actualmente:
-
-``` sql
-ALTER TABLE [db.]table DELETE WHERE filter_expr
-```
-
-El `filter_expr` debe ser de tipo `UInt8`. La consulta elimina las filas de la tabla para la que esta expresión toma un valor distinto de cero.
-
-``` sql
-ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr
-```
-
-El `filter_expr` debe ser de tipo `UInt8`. Esta consulta actualiza los valores de las columnas especificadas a los valores de las expresiones correspondientes `filter_expr` toma un valor distinto de cero. Los valores se convierten al tipo de columna utilizando el `CAST` operador. No se admite la actualización de columnas que se utilizan en el cálculo de la clave principal o de partición.
-
-``` sql
-ALTER TABLE [db.]table MATERIALIZE INDEX name IN PARTITION partition_name
-```
-
-La consulta reconstruye el índice secundario `name` en la partición `partition_name`.
-
-Una consulta puede contener varios comandos separados por comas.
-
-Para las tablas \*MergeTree, las mutaciones se ejecutan reescribiendo partes de datos completas. No hay atomicidad - las partes se sustituyen por partes mutadas tan pronto como están listas y una `SELECT` La consulta que comenzó a ejecutarse durante una mutación verá datos de partes que ya han sido mutadas junto con datos de partes que aún no han sido mutadas.
-
-Las mutaciones están totalmente ordenadas por su orden de creación y se aplican a cada parte en ese orden. Las mutaciones también se ordenan parcialmente con INSERTs: los datos que se insertaron en la tabla antes de que se enviara la mutación se mutarán y los datos que se insertaron después de eso no se mutarán. Tenga en cuenta que las mutaciones no bloquean INSERTs de ninguna manera.
-
-Una consulta de mutación regresa inmediatamente después de agregar la entrada de mutación (en el caso de tablas replicadas a ZooKeeper, para tablas no replicadas, al sistema de archivos). La mutación en sí se ejecuta de forma asíncrona utilizando la configuración del perfil del sistema. Para realizar un seguimiento del progreso de las mutaciones, puede usar el [`system.mutations`](../../operations/system-tables.md#system_tables-mutations) tabla. Una mutación que se envió correctamente continuará ejecutándose incluso si se reinician los servidores ClickHouse. No hay forma de revertir la mutación una vez que se presenta, pero si la mutación está atascada por alguna razón, puede cancelarse con el [`KILL MUTATION`](misc.md#kill-mutation) consulta.
-
-Las entradas de mutaciones terminadas no se eliminan de inmediato (el número de entradas conservadas viene determinado por el `finished_mutations_to_keep` parámetro del motor de almacenamiento). Las entradas de mutación más antiguas se eliminan.
-
-## ALTER USER {#alter-user-statement}
-
-Cambia las cuentas de usuario de ClickHouse.
-
-### Sintaxis {#alter-user-syntax}
-
-``` sql
-ALTER USER [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [IDENTIFIED [WITH {PLAINTEXT_PASSWORD|SHA256_PASSWORD|DOUBLE_SHA1_PASSWORD}] BY {'password'|'hash'}]
-    [[ADD|DROP] HOST {LOCAL | NAME 'name' | REGEXP 'name_regexp' | IP 'address' | LIKE 'pattern'} [,...] | ANY | NONE]
-    [DEFAULT ROLE role [,...] | ALL | ALL EXCEPT role [,...] ]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-### Descripci {#alter-user-dscr}
-
-Utilizar `ALTER USER` debe tener el [ALTER USER](grant.md#grant-access-management) privilegio.
-
-### Ejemplos {#alter-user-examples}
-
-Establecer roles concedidos como predeterminados:
-
-``` sql
-ALTER USER user DEFAULT ROLE role1, role2
-```
-
-Si los roles no se otorgan previamente a un usuario, ClickHouse produce una excepción.
-
-Establezca todas las funciones concedidas como predeterminadas:
-
-``` sql
-ALTER USER user DEFAULT ROLE ALL
-```
-
-Si se otorga un rol a un usuario en el futuro, se convertirá en predeterminado automáticamente.
-
-Establezca todas las funciones otorgadas a excepción predeterminada `role1` y `role2`:
-
-``` sql
-ALTER USER user DEFAULT ROLE ALL EXCEPT role1, role2
-```
-
-## ALTER ROLE {#alter-role-statement}
-
-Cambia los roles.
-
-### Sintaxis {#alter-role-syntax}
-
-``` sql
-ALTER ROLE [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-## ALTER ROW POLICY {#alter-row-policy-statement}
-
-Cambia la política de fila.
-
-### Sintaxis {#alter-row-policy-syntax}
-
-``` sql
-ALTER [ROW] POLICY [IF EXISTS] name [ON CLUSTER cluster_name] ON [database.]table
-    [RENAME TO new_name]
-    [AS {PERMISSIVE | RESTRICTIVE}]
-    [FOR SELECT]
-    [USING {condition | NONE}][,...]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-## ALTER QUOTA {#alter-quota-statement}
-
-Cambia las cuotas.
-
-### Sintaxis {#alter-quota-syntax}
-
-``` sql
-ALTER QUOTA [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [KEYED BY {'none' | 'user name' | 'ip address' | 'client key' | 'client key or user name' | 'client key or ip address'}]
-    [FOR [RANDOMIZED] INTERVAL number {SECOND | MINUTE | HOUR | DAY | WEEK | MONTH | QUARTER | YEAR}
-        {MAX { {QUERIES | ERRORS | RESULT ROWS | RESULT BYTES | READ ROWS | READ BYTES | EXECUTION TIME} = number } [,...] |
-        NO LIMITS | TRACKING ONLY} [,...]]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-## ALTER SETTINGS PROFILE {#alter-settings-profile-statement}
-
-Cambia las cuotas.
-
-### Sintaxis {#alter-settings-profile-syntax}
-
-``` sql
-ALTER SETTINGS PROFILE [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | INHERIT 'profile_name'] [,...]
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/alter/) <!--hide-->
diff --git a/docs/es/sql-reference/statements/create.md b/docs/es/sql-reference/statements/create.md
deleted file mode 100644
index db3194ae1142..000000000000
--- a/docs/es/sql-reference/statements/create.md
+++ /dev/null
@@ -1,502 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: CREATE
----
-
-# CREATE Consultas {#create-queries}
-
-## CREATE DATABASE {#query-language-create-database}
-
-Crea una base de datos.
-
-``` sql
-CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] [ENGINE = engine(...)]
-```
-
-### Clausula {#clauses}
-
--   `IF NOT EXISTS`
-    Si el `db_name` base de datos ya existe, entonces ClickHouse no crea una nueva base de datos y:
-
-    -   No lanza una excepción si se especifica una cláusula.
-    -   Lanza una excepción si no se especifica la cláusula.
-
--   `ON CLUSTER`
-    ClickHouse crea el `db_name` base de datos en todos los servidores de un clúster especificado.
-
--   `ENGINE`
-
-    -   [MySQL](../../engines/database-engines/mysql.md)
-        Le permite recuperar datos del servidor MySQL remoto.
-        De forma predeterminada, ClickHouse usa su propio [motor de base de datos](../../engines/database-engines/index.md).
-
-## CREATE TABLE {#create-table-query}
-
-El `CREATE TABLE` consulta puede tener varias formas.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [compression_codec] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [compression_codec] [TTL expr2],
-    ...
-) ENGINE = engine
-```
-
-Crea una tabla llamada ‘name’ en el ‘db’ base de datos o la base de datos actual si ‘db’ no está establecida, con la estructura especificada entre paréntesis y ‘engine’ motor.
-La estructura de la tabla es una lista de descripciones de columnas. Si los índices son compatibles con el motor, se indican como parámetros para el motor de tablas.
-
-Una descripción de columna es `name type` en el caso más simple. Ejemplo: `RegionID UInt32`.
-Las expresiones también se pueden definir para los valores predeterminados (ver más abajo).
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name AS [db2.]name2 [ENGINE = engine]
-```
-
-Crea una tabla con la misma estructura que otra tabla. Puede especificar un motor diferente para la tabla. Si no se especifica el motor, se utilizará el mismo motor que para el `db2.name2` tabla.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name AS table_function()
-```
-
-Crea una tabla con la estructura y los datos [función de la tabla](../table-functions/index.md#table-functions).
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name ENGINE = engine AS SELECT ...
-```
-
-Crea una tabla con una estructura como el resultado de la `SELECT` consulta, con el ‘engine’ motor, y lo llena con datos de SELECT.
-
-En todos los casos, si `IF NOT EXISTS` se especifica, la consulta no devolverá un error si la tabla ya existe. En este caso, la consulta no hará nada.
-
-Puede haber otras cláusulas después del `ENGINE` cláusula en la consulta. Consulte la documentación detallada sobre cómo crear tablas en las descripciones de [motores de mesa](../../engines/table-engines/index.md#table_engines).
-
-### Valores predeterminados {#create-default-values}
-
-La descripción de la columna puede especificar una expresión para un valor predeterminado, de una de las siguientes maneras:`DEFAULT expr`, `MATERIALIZED expr`, `ALIAS expr`.
-Ejemplo: `URLDomain String DEFAULT domain(URL)`.
-
-Si no se define una expresión para el valor predeterminado, los valores predeterminados se establecerán en ceros para números, cadenas vacías para cadenas, matrices vacías para matrices y `1970-01-01` para fechas o zero unix timestamp para las fechas con el tiempo. Los NULL no son compatibles.
-
-Si se define la expresión predeterminada, el tipo de columna es opcional. Si no hay un tipo definido explícitamente, se utiliza el tipo de expresión predeterminado. Ejemplo: `EventDate DEFAULT toDate(EventTime)` – the ‘Date’ tipo será utilizado para el ‘EventDate’ columna.
-
-Si el tipo de datos y la expresión predeterminada se definen explícitamente, esta expresión se convertirá al tipo especificado utilizando funciones de conversión de tipos. Ejemplo: `Hits UInt32 DEFAULT 0` significa lo mismo que `Hits UInt32 DEFAULT toUInt32(0)`.
-
-Default expressions may be defined as an arbitrary expression from table constants and columns. When creating and changing the table structure, it checks that expressions don't contain loops. For INSERT, it checks that expressions are resolvable – that all columns they can be calculated from have been passed.
-
-`DEFAULT expr`
-
-Valor predeterminado Normal. Si la consulta INSERT no especifica la columna correspondiente, se completará calculando la expresión correspondiente.
-
-`MATERIALIZED expr`
-
-Expresión materializada. Dicha columna no se puede especificar para INSERT, porque siempre se calcula.
-Para un INSERT sin una lista de columnas, estas columnas no se consideran.
-Además, esta columna no se sustituye cuando se utiliza un asterisco en una consulta SELECT. Esto es para preservar el invariante que el volcado obtuvo usando `SELECT *` se puede volver a insertar en la tabla usando INSERT sin especificar la lista de columnas.
-
-`ALIAS expr`
-
-Sinónimo. Tal columna no se almacena en la tabla en absoluto.
-Sus valores no se pueden insertar en una tabla, y no se sustituye cuando se usa un asterisco en una consulta SELECT.
-Se puede usar en SELECT si el alias se expande durante el análisis de consultas.
-
-Cuando se utiliza la consulta ALTER para agregar nuevas columnas, no se escriben datos antiguos para estas columnas. En su lugar, al leer datos antiguos que no tienen valores para las nuevas columnas, las expresiones se calculan sobre la marcha de forma predeterminada. Sin embargo, si la ejecución de las expresiones requiere diferentes columnas que no están indicadas en la consulta, estas columnas se leerán adicionalmente, pero solo para los bloques de datos que lo necesitan.
-
-Si agrega una nueva columna a una tabla pero luego cambia su expresión predeterminada, los valores utilizados para los datos antiguos cambiarán (para los datos donde los valores no se almacenaron en el disco). Tenga en cuenta que cuando se ejecutan combinaciones en segundo plano, los datos de las columnas que faltan en una de las partes de combinación se escriben en la parte combinada.
-
-No es posible establecer valores predeterminados para elementos en estructuras de datos anidadas.
-
-### Limitación {#constraints}
-
-Junto con las descripciones de columnas, se podrían definir restricciones:
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [compression_codec] [TTL expr1],
-    ...
-    CONSTRAINT constraint_name_1 CHECK boolean_expr_1,
-    ...
-) ENGINE = engine
-```
-
-`boolean_expr_1` podría por cualquier expresión booleana. Si se definen restricciones para la tabla, cada una de ellas se verificará para cada fila en `INSERT` query. If any constraint is not satisfied — server will raise an exception with constraint name and checking expression.
-
-Agregar una gran cantidad de restricciones puede afectar negativamente el rendimiento de grandes `INSERT` consulta.
-
-### Expresión TTL {#ttl-expression}
-
-Define el tiempo de almacenamiento de los valores. Solo se puede especificar para tablas de la familia MergeTree. Para la descripción detallada, ver [TTL para columnas y tablas](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-ttl).
-
-### Códecs de compresión de columna {#codecs}
-
-De forma predeterminada, ClickHouse aplica el `lz4` método de compresión. Para `MergeTree`- familia de motor puede cambiar el método de compresión predeterminado en el [compresión](../../operations/server-configuration-parameters/settings.md#server-settings-compression) sección de una configuración de servidor. También puede definir el método de compresión para cada columna `CREATE TABLE` consulta.
-
-``` sql
-CREATE TABLE codec_example
-(
-    dt Date CODEC(ZSTD),
-    ts DateTime CODEC(LZ4HC),
-    float_value Float32 CODEC(NONE),
-    double_value Float64 CODEC(LZ4HC(9))
-    value Float32 CODEC(Delta, ZSTD)
-)
-ENGINE = <Engine>
-...
-```
-
-Si se especifica un códec, el códec predeterminado no se aplica. Los códecs se pueden combinar en una tubería, por ejemplo, `CODEC(Delta, ZSTD)`. Para seleccionar la mejor combinación de códecs para su proyecto, pase puntos de referencia similares a los descritos en Altinity [Nuevas codificaciones para mejorar la eficiencia de ClickHouse](https://www.altinity.com/blog/2019/7/new-encodings-to-improve-clickhouse) artículo.
-
-!!! warning "Advertencia"
-    No puede descomprimir archivos de base de datos ClickHouse con utilidades externas como `lz4`. En su lugar, use el especial [Compresor de clickhouse](https://github.com/ClickHouse/ClickHouse/tree/master/programs/compressor) utilidad.
-
-La compresión es compatible con los siguientes motores de tablas:
-
--   [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md) familia. Admite códecs de compresión de columnas y selecciona el método de compresión predeterminado mediante [compresión](../../operations/server-configuration-parameters/settings.md#server-settings-compression) configuración.
--   [Registro](../../engines/table-engines/log-family/index.md) familia. Utiliza el `lz4` método de compresión por defecto y soporta códecs de compresión de columna.
--   [Establecer](../../engines/table-engines/special/set.md). Solo admite la compresión predeterminada.
--   [Unir](../../engines/table-engines/special/join.md). Solo admite la compresión predeterminada.
-
-ClickHouse admite códecs de propósito común y códecs especializados.
-
-#### Especializados Codecs {#create-query-specialized-codecs}
-
-Estos códecs están diseñados para hacer que la compresión sea más efectiva mediante el uso de características específicas de los datos. Algunos de estos códecs no comprimen los datos por sí mismos. En su lugar, preparan los datos para un códec de propósito común, que lo comprime mejor que sin esta preparación.
-
-Especializados codecs:
-
--   `Delta(delta_bytes)` — Compression approach in which raw values are replaced by the difference of two neighboring values, except for the first value that stays unchanged. Up to `delta_bytes` se utilizan para almacenar valores delta, por lo que `delta_bytes` es el tamaño máximo de los valores brutos. Posible `delta_bytes` valores: 1, 2, 4, 8. El valor predeterminado para `delta_bytes` ser `sizeof(type)` si es igual a 1, 2, 4 u 8. En todos los demás casos, es 1.
--   `DoubleDelta` — Calculates delta of deltas and writes it in compact binary form. Optimal compression rates are achieved for monotonic sequences with a constant stride, such as time series data. Can be used with any fixed-width type. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. Uses 1 extra bit for 32-byte deltas: 5-bit prefixes instead of 4-bit prefixes. For additional information, see Compressing Time Stamps in [Gorila: Una base de datos de series temporales rápida, escalable y en memoria](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf).
--   `Gorilla` — Calculates XOR between current and previous value and writes it in compact binary form. Efficient when storing a series of floating point values that change slowly, because the best compression rate is achieved when neighboring values are binary equal. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. For additional information, see Compressing Values in [Gorila: Una base de datos de series temporales rápida, escalable y en memoria](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf).
--   `T64` — Compression approach that crops unused high bits of values in integer data types (including `Enum`, `Date` y `DateTime`). En cada paso de su algoritmo, el códec toma un bloque de 64 valores, los coloca en una matriz de 64x64 bits, lo transpone, recorta los bits de valores no utilizados y devuelve el resto como una secuencia. Los bits no utilizados son los bits, que no difieren entre los valores máximo y mínimo en toda la parte de datos para la que se utiliza la compresión.
-
-`DoubleDelta` y `Gorilla` códecs se utilizan en Gorilla TSDB como los componentes de su algoritmo de compresión. El enfoque de gorila es efectivo en escenarios en los que hay una secuencia de valores que cambian lentamente con sus marcas de tiempo. Las marcas de tiempo se comprimen efectivamente por el `DoubleDelta` códec, y los valores son efectivamente comprimidos por el `Gorilla` códec. Por ejemplo, para obtener una tabla almacenada efectivamente, puede crearla en la siguiente configuración:
-
-``` sql
-CREATE TABLE codec_example
-(
-    timestamp DateTime CODEC(DoubleDelta),
-    slow_values Float32 CODEC(Gorilla)
-)
-ENGINE = MergeTree()
-```
-
-#### Códecs de uso general {#create-query-general-purpose-codecs}
-
-Códecs:
-
--   `NONE` — No compression.
--   `LZ4` — Lossless [algoritmo de compresión de datos](https://github.com/lz4/lz4) utilizado por defecto. Aplica compresión rápida LZ4.
--   `LZ4HC[(level)]` — LZ4 HC (high compression) algorithm with configurable level. Default level: 9. Setting `level <= 0` aplica el nivel predeterminado. Niveles posibles: \[1, 12\]. Rango de nivel recomendado: \[4, 9\].
--   `ZSTD[(level)]` — [Algoritmo de compresión ZSTD](https://en.wikipedia.org/wiki/Zstandard) con configurable `level`. Niveles posibles: \[1, 22\]. Valor predeterminado: 1.
-
-Los altos niveles de compresión son útiles para escenarios asimétricos, como comprimir una vez, descomprimir repetidamente. Los niveles más altos significan una mejor compresión y un mayor uso de la CPU.
-
-## Tablas temporales {#temporary-tables}
-
-ClickHouse admite tablas temporales que tienen las siguientes características:
-
--   Las tablas temporales desaparecen cuando finaliza la sesión, incluso si se pierde la conexión.
--   Una tabla temporal solo utiliza el motor de memoria.
--   No se puede especificar la base de datos para una tabla temporal. Se crea fuera de las bases de datos.
--   Imposible crear una tabla temporal con consulta DDL distribuida en todos los servidores de clúster (mediante `ON CLUSTER`): esta tabla sólo existe en la sesión actual.
--   Si una tabla temporal tiene el mismo nombre que otra y una consulta especifica el nombre de la tabla sin especificar la base de datos, se utilizará la tabla temporal.
--   Para el procesamiento de consultas distribuidas, las tablas temporales utilizadas en una consulta se pasan a servidores remotos.
-
-Para crear una tabla temporal, utilice la siguiente sintaxis:
-
-``` sql
-CREATE TEMPORARY TABLE [IF NOT EXISTS] table_name
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-)
-```
-
-En la mayoría de los casos, las tablas temporales no se crean manualmente, sino cuando se utilizan datos externos para una consulta o para `(GLOBAL) IN`. Para obtener más información, consulte las secciones correspondientes
-
-Es posible usar tablas con [MOTOR = Memoria](../../engines/table-engines/special/memory.md) en lugar de tablas temporales.
-
-## Consultas DDL distribuidas (cláusula ON CLUSTER) {#distributed-ddl-queries-on-cluster-clause}
-
-El `CREATE`, `DROP`, `ALTER`, y `RENAME` las consultas admiten la ejecución distribuida en un clúster.
-Por ejemplo, la siguiente consulta crea el `all_hits` `Distributed` la tabla en cada host `cluster`:
-
-``` sql
-CREATE TABLE IF NOT EXISTS all_hits ON CLUSTER cluster (p Date, i Int32) ENGINE = Distributed(cluster, default, hits)
-```
-
-Para ejecutar estas consultas correctamente, cada host debe tener la misma definición de clúster (para simplificar la sincronización de configuraciones, puede usar sustituciones de ZooKeeper). También deben conectarse a los servidores ZooKeeper.
-La versión local de la consulta finalmente se implementará en cada host del clúster, incluso si algunos hosts no están disponibles actualmente. El orden para ejecutar consultas dentro de un único host está garantizado.
-
-## CREATE VIEW {#create-view}
-
-``` sql
-CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]table_name [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...
-```
-
-Crea una vista. Hay dos tipos de vistas: normal y MATERIALIZADO.
-
-Las vistas normales no almacenan ningún dato, solo realizan una lectura desde otra tabla. En otras palabras, una vista normal no es más que una consulta guardada. Al leer desde una vista, esta consulta guardada se utiliza como una subconsulta en la cláusula FROM.
-
-Como ejemplo, suponga que ha creado una vista:
-
-``` sql
-CREATE VIEW view AS SELECT ...
-```
-
-y escribió una consulta:
-
-``` sql
-SELECT a, b, c FROM view
-```
-
-Esta consulta es totalmente equivalente a usar la subconsulta:
-
-``` sql
-SELECT a, b, c FROM (SELECT ...)
-```
-
-Las vistas materializadas almacenan datos transformados por la consulta SELECT correspondiente.
-
-Al crear una vista materializada sin `TO [db].[table]`, you must specify ENGINE – the table engine for storing data.
-
-Al crear una vista materializada con `TO [db].[table]` usted no debe usar `POPULATE`.
-
-Una vista materializada se organiza de la siguiente manera: al insertar datos en la tabla especificada en SELECT, parte de los datos insertados se convierte mediante esta consulta SELECT y el resultado se inserta en la vista.
-
-Si especifica POPULATE, los datos de tabla existentes se insertan en la vista al crearlos, como si `CREATE TABLE ... AS SELECT ...` . De lo contrario, la consulta solo contiene los datos insertados en la tabla después de crear la vista. No recomendamos usar POPULATE, ya que los datos insertados en la tabla durante la creación de la vista no se insertarán en ella.
-
-A `SELECT` consulta puede contener `DISTINCT`, `GROUP BY`, `ORDER BY`, `LIMIT`… Note that the corresponding conversions are performed independently on each block of inserted data. For example, if `GROUP BY` se establece, los datos se agregan durante la inserción, pero solo dentro de un solo paquete de datos insertados. Los datos no se agregarán más. La excepción es cuando se utiliza un ENGINE que realiza de forma independiente la agregación de datos, como `SummingMergeTree`.
-
-La ejecución de `ALTER` las consultas sobre vistas materializadas no se han desarrollado completamente, por lo que podrían ser inconvenientes. Si la vista materializada utiliza la construcción `TO [db.]name` puede `DETACH` la vista, ejecutar `ALTER` para la tabla de destino, y luego `ATTACH` el previamente separado (`DETACH`) vista.
-
-Las vistas tienen el mismo aspecto que las tablas normales. Por ejemplo, se enumeran en el resultado de la `SHOW TABLES` consulta.
-
-No hay una consulta separada para eliminar vistas. Para eliminar una vista, utilice `DROP TABLE`.
-
-## CREATE DICTIONARY {#create-dictionary-query}
-
-``` sql
-CREATE DICTIONARY [IF NOT EXISTS] [db.]dictionary_name [ON CLUSTER cluster]
-(
-    key1 type1  [DEFAULT|EXPRESSION expr1] [HIERARCHICAL|INJECTIVE|IS_OBJECT_ID],
-    key2 type2  [DEFAULT|EXPRESSION expr2] [HIERARCHICAL|INJECTIVE|IS_OBJECT_ID],
-    attr1 type2 [DEFAULT|EXPRESSION expr3],
-    attr2 type2 [DEFAULT|EXPRESSION expr4]
-)
-PRIMARY KEY key1, key2
-SOURCE(SOURCE_NAME([param1 value1 ... paramN valueN]))
-LAYOUT(LAYOUT_NAME([param_name param_value]))
-LIFETIME({MIN min_val MAX max_val | max_val})
-```
-
-Crear [diccionario externo](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md) con dado [estructura](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md), [fuente](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md), [diseño](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md) y [vida](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md).
-
-La estructura del diccionario externo consta de atributos. Los atributos de diccionario se especifican de manera similar a las columnas de la tabla. La única propiedad de atributo requerida es su tipo, todas las demás propiedades pueden tener valores predeterminados.
-
-Dependiendo del diccionario [diseño](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md) se pueden especificar uno o más atributos como claves de diccionario.
-
-Para obtener más información, consulte [Diccionarios externos](../dictionaries/external-dictionaries/external-dicts.md) apartado.
-
-## CREATE USER {#create-user-statement}
-
-Crea un [cuenta de usuario](../../operations/access-rights.md#user-account-management).
-
-### Sintaxis {#create-user-syntax}
-
-``` sql
-CREATE USER [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name]
-    [IDENTIFIED [WITH {NO_PASSWORD|PLAINTEXT_PASSWORD|SHA256_PASSWORD|SHA256_HASH|DOUBLE_SHA1_PASSWORD|DOUBLE_SHA1_HASH}] BY {'password'|'hash'}]
-    [HOST {LOCAL | NAME 'name' | REGEXP 'name_regexp' | IP 'address' | LIKE 'pattern'} [,...] | ANY | NONE]
-    [DEFAULT ROLE role [,...]]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-#### Identificación {#identification}
-
-Hay múltiples formas de identificación del usuario:
-
--   `IDENTIFIED WITH no_password`
--   `IDENTIFIED WITH plaintext_password BY 'qwerty'`
--   `IDENTIFIED WITH sha256_password BY 'qwerty'` o `IDENTIFIED BY 'password'`
--   `IDENTIFIED WITH sha256_hash BY 'hash'`
--   `IDENTIFIED WITH double_sha1_password BY 'qwerty'`
--   `IDENTIFIED WITH double_sha1_hash BY 'hash'`
-
-#### Anfitrión del usuario {#user-host}
-
-El host de usuario es un host desde el que se podría establecer una conexión con el servidor ClickHouse. El host se puede especificar en el `HOST` sección de consulta de las siguientes maneras:
-
--   `HOST IP 'ip_address_or_subnetwork'` — User can connect to ClickHouse server only from the specified IP address or a [subred](https://en.wikipedia.org/wiki/Subnetwork). Ejemplos: `HOST IP '192.168.0.0/16'`, `HOST IP '2001:DB8::/32'`. Para su uso en producción, sólo especifique `HOST IP` elementos (direcciones IP y sus máscaras), ya que usan `host` y `host_regexp` podría causar latencia adicional.
--   `HOST ANY` — User can connect from any location. This is default option.
--   `HOST LOCAL` — User can connect only locally.
--   `HOST NAME 'fqdn'` — User host can be specified as FQDN. For example, `HOST NAME 'mysite.com'`.
--   `HOST NAME REGEXP 'regexp'` — You can use [pcre](http://www.pcre.org/) expresiones regulares al especificar hosts de usuario. Por ejemplo, `HOST NAME REGEXP '.*\.mysite\.com'`.
--   `HOST LIKE 'template'` — Allows you use the [LIKE](../functions/string-search-functions.md#function-like) operador para filtrar los hosts de usuario. Por ejemplo, `HOST LIKE '%'` es equivalente a `HOST ANY`, `HOST LIKE '%.mysite.com'` filtros todos los anfitriones en el `mysite.com` dominio.
-
-Otra forma de especificar el host es usar `@` sintaxis con el nombre de usuario. Ejemplos:
-
--   `CREATE USER mira@'127.0.0.1'` — Equivalent to the `HOST IP` sintaxis.
--   `CREATE USER mira@'localhost'` — Equivalent to the `HOST LOCAL` sintaxis.
--   `CREATE USER mira@'192.168.%.%'` — Equivalent to the `HOST LIKE` sintaxis.
-
-!!! info "Advertencia"
-    ClickHouse trata `user_name@'address'` como un nombre de usuario en su conjunto. Por lo tanto, técnicamente puede crear múltiples usuarios con `user_name` y diferentes construcciones después `@`. No recomendamos hacerlo.
-
-### Ejemplos {#create-user-examples}
-
-Crear la cuenta de usuario `mira` protegido por la contraseña `qwerty`:
-
-``` sql
-CREATE USER mira HOST IP '127.0.0.1' IDENTIFIED WITH sha256_password BY 'qwerty'
-```
-
-`mira` debe iniciar la aplicación cliente en el host donde se ejecuta el servidor ClickHouse.
-
-Crear la cuenta de usuario `john`, asignarle roles y hacer que estos roles sean predeterminados:
-
-``` sql
-CREATE USER john DEFAULT ROLE role1, role2
-```
-
-Crear la cuenta de usuario `john` y hacer todos sus roles futuros por defecto:
-
-``` sql
-ALTER USER user DEFAULT ROLE ALL
-```
-
-Cuando se asignará algún rol a `john` en el futuro se convertirá en predeterminado automáticamente.
-
-Crear la cuenta de usuario `john` y hacer todos sus futuros roles por defecto excepto `role1` y `role2`:
-
-``` sql
-ALTER USER john DEFAULT ROLE ALL EXCEPT role1, role2
-```
-
-## CREATE ROLE {#create-role-statement}
-
-Crea un [rol](../../operations/access-rights.md#role-management).
-
-### Sintaxis {#create-role-syntax}
-
-``` sql
-CREATE ROLE [IF NOT EXISTS | OR REPLACE] name
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-### Descripci {#create-role-description}
-
-El rol es un conjunto de [privilegio](grant.md#grant-privileges). Un usuario concedido con un rol obtiene todos los privilegios de este rol.
-
-A un usuario se le pueden asignar varios roles. Los usuarios pueden aplicar sus roles otorgados en combinaciones arbitrarias por el [SET ROLE](misc.md#set-role-statement) instrucción. El ámbito final de los privilegios es un conjunto combinado de todos los privilegios de todos los roles aplicados. Si un usuario tiene privilegios otorgados directamente a su cuenta de usuario, también se combinan con los privilegios otorgados por roles.
-
-El usuario puede tener roles predeterminados que se aplican al iniciar sesión del usuario. Para establecer roles predeterminados, utilice el [SET DEFAULT ROLE](misc.md#set-default-role-statement) declaración o el [ALTER USER](alter.md#alter-user-statement) instrucción.
-
-Para revocar un rol, utilice el [REVOKE](revoke.md) instrucción.
-
-Para eliminar el rol, utilice el [DROP ROLE](misc.md#drop-role-statement) instrucción. El rol eliminado se revoca automáticamente de todos los usuarios y roles a los que se concedió.
-
-### Ejemplos {#create-role-examples}
-
-``` sql
-CREATE ROLE accountant;
-GRANT SELECT ON db.* TO accountant;
-```
-
-Esta secuencia de consultas crea el rol `accountant` que tiene el privilegio de leer datos del `accounting` base.
-
-Conceder el rol al usuario `mira`:
-
-``` sql
-GRANT accountant TO mira;
-```
-
-Después de conceder el rol, el usuario puede usarlo y realizar las consultas permitidas. Por ejemplo:
-
-``` sql
-SET ROLE accountant;
-SELECT * FROM db.*;
-```
-
-## CREATE ROW POLICY {#create-row-policy-statement}
-
-Crea un [filtro para filas](../../operations/access-rights.md#row-policy-management) que un usuario puede leer de una tabla.
-
-### Sintaxis {#create-row-policy-syntax}
-
-``` sql
-CREATE [ROW] POLICY [IF NOT EXISTS | OR REPLACE] policy_name [ON CLUSTER cluster_name] ON [db.]table
-    [AS {PERMISSIVE | RESTRICTIVE}]
-    [FOR SELECT]
-    [USING condition]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-#### Sección AS {#create-row-policy-as}
-
-Con esta sección puede crear políticas permisivas o restrictivas.
-
-La política permisiva concede acceso a las filas. Las políticas permisivas que se aplican a la misma tabla se combinan usando el valor booleano `OR` operador. Las políticas son permisivas de forma predeterminada.
-
-La política restrictiva restringe el acceso a la fila. Las políticas restrictivas que se aplican a la misma tabla se combinan usando el valor booleano `AND` operador.
-
-Las políticas restrictivas se aplican a las filas que pasaron los filtros permisivos. Si establece directivas restrictivas pero no directivas permisivas, el usuario no puede obtener ninguna fila de la tabla.
-
-#### Sección A {#create-row-policy-to}
-
-En la sección `TO` puede dar una lista mixta de roles y usuarios, por ejemplo, `CREATE ROW POLICY ... TO accountant, john@localhost`.
-
-Palabra clave `ALL` significa todos los usuarios de ClickHouse, incluido el usuario actual. Palabras clave `ALL EXCEPT` permitir excluir a algunos usuarios de la lista de todos los usuarios, por ejemplo `CREATE ROW POLICY ... TO ALL EXCEPT accountant, john@localhost`
-
-### Ejemplos {#examples}
-
--   `CREATE ROW POLICY filter ON mydb.mytable FOR SELECT USING a<1000 TO accountant, john@localhost`
--   `CREATE ROW POLICY filter ON mydb.mytable FOR SELECT USING a<1000 TO ALL EXCEPT mira`
-
-## CREATE QUOTA {#create-quota-statement}
-
-Crea un [cuota](../../operations/access-rights.md#quotas-management) que se puede asignar a un usuario o a un rol.
-
-### Sintaxis {#create-quota-syntax}
-
-``` sql
-CREATE QUOTA [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name]
-    [KEYED BY {'none' | 'user name' | 'ip address' | 'client key' | 'client key or user name' | 'client key or ip address'}]
-    [FOR [RANDOMIZED] INTERVAL number {SECOND | MINUTE | HOUR | DAY | WEEK | MONTH | QUARTER | YEAR}
-        {MAX { {QUERIES | ERRORS | RESULT ROWS | RESULT BYTES | READ ROWS | READ BYTES | EXECUTION TIME} = number } [,...] |
-         NO LIMITS | TRACKING ONLY} [,...]]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-### Ejemplo {#create-quota-example}
-
-Limite el número máximo de consultas para el usuario actual con 123 consultas en una restricción de 15 meses:
-
-``` sql
-CREATE QUOTA qA FOR INTERVAL 15 MONTH MAX QUERIES 123 TO CURRENT_USER
-```
-
-## CREATE SETTINGS PROFILE {#create-settings-profile-statement}
-
-Crea un [perfil de configuración](../../operations/access-rights.md#settings-profiles-management) que se puede asignar a un usuario o a un rol.
-
-### Sintaxis {#create-settings-profile-syntax}
-
-``` sql
-CREATE SETTINGS PROFILE [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | INHERIT 'profile_name'] [,...]
-```
-
-# Ejemplo {#create-settings-profile-syntax}
-
-Crear el `max_memory_usage_profile` perfil de configuración con valor y restricciones para el `max_memory_usage` configuración. Asignarlo a `robin`:
-
-``` sql
-CREATE SETTINGS PROFILE max_memory_usage_profile SETTINGS max_memory_usage = 100000001 MIN 90000000 MAX 110000000 TO robin
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/create/) <!--hide-->
diff --git a/docs/es/sql-reference/statements/grant.md b/docs/es/sql-reference/statements/grant.md
deleted file mode 100644
index 783324e22dbd..000000000000
--- a/docs/es/sql-reference/statements/grant.md
+++ /dev/null
@@ -1,476 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: GRANT
----
-
-# GRANT {#grant}
-
--   Conceder [privilegio](#grant-privileges) a las cuentas o roles de usuario de ClickHouse.
--   Asigna roles a cuentas de usuario u otros roles.
-
-Para revocar privilegios, utilice el [REVOKE](revoke.md) instrucción. También puede enumerar los privilegios concedidos por el [SHOW GRANTS](show.md#show-grants-statement) instrucción.
-
-## Concesión de sintaxis de privilegios {#grant-privigele-syntax}
-
-``` sql
-GRANT [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} TO {user | role | CURRENT_USER} [,...] [WITH GRANT OPTION]
-```
-
--   `privilege` — Type of privilege.
--   `role` — ClickHouse user role.
--   `user` — ClickHouse user account.
-
-El `WITH GRANT OPTION` subvenciones de cláusula `user` o `role` con permiso para realizar el `GRANT` consulta. Los usuarios pueden otorgar privilegios del mismo alcance que tienen y menos.
-
-## Concesión de sintaxis de roles {#assign-role-syntax}
-
-``` sql
-GRANT [ON CLUSTER cluster_name] role [,...] TO {user | another_role | CURRENT_USER} [,...] [WITH ADMIN OPTION]
-```
-
--   `role` — ClickHouse user role.
--   `user` — ClickHouse user account.
-
-El `WITH ADMIN OPTION` conjuntos de cláusulas [ADMIN OPTION](#admin-option-privilege) privilegio para `user` o `role`.
-
-## Uso {#grant-usage}
-
-Utilizar `GRANT` su cuenta debe tener el `GRANT OPTION` privilegio. Puede otorgar privilegios solo dentro del ámbito de los privilegios de su cuenta.
-
-Por ejemplo, el administrador ha otorgado privilegios a `john` cuenta por la consulta:
-
-``` sql
-GRANT SELECT(x,y) ON db.table TO john WITH GRANT OPTION
-```
-
-Significa que `john` tiene el permiso para realizar:
-
--   `SELECT x,y FROM db.table`.
--   `SELECT x FROM db.table`.
--   `SELECT y FROM db.table`.
-
-`john` no puede realizar `SELECT z FROM db.table`. El `SELECT * FROM db.table` también no está disponible. Al procesar esta consulta, ClickHouse no devuelve ningún dato, incluso `x` y `y`. La única excepción es si una tabla `x` y `y` columnas, en este caso ClickHouse devuelve todos los datos.
-
-También `john` tiene el `GRANT OPTION` privilegios, por lo que puede otorgar a otros usuarios con privilegios del mismo o del menor alcance.
-
-Especificación de privilegios que puede utilizar asterisco (`*`) en lugar de una tabla o un nombre de base de datos. Por ejemplo, el `GRANT SELECT ON db.* TO john` consulta permite `john` para realizar el `SELECT` consulta sobre todas las tablas en `db` base. Además, puede omitir el nombre de la base de datos. En este caso, se otorgan privilegios para la base de datos actual, por ejemplo: `GRANT SELECT ON * TO john` otorga el privilegio en todas las tablas de la base de datos actual, `GRANT SELECT ON mytable TO john` otorga el privilegio sobre el `mytable` tabla en la base de datos actual.
-
-Acceso a la `system` base de datos siempre está permitida (ya que esta base de datos se utiliza para procesar consultas).
-
-Puede conceder varios privilegios a varias cuentas en una consulta. Consulta `GRANT SELECT, INSERT ON *.* TO john, robin` permite cuentas `john` y `robin` para realizar el `INSERT` y `SELECT` consultas sobre todas las tablas en todas las bases de datos en el servidor.
-
-## Privilegio {#grant-privileges}
-
-Privilegio es un permiso para realizar un tipo específico de consultas.
-
-Los privilegios tienen una estructura jerárquica. Un conjunto de consultas permitidas depende del ámbito de privilegios.
-
-Jerarquía de privilegios:
-
--   [SELECT](#grant-select)
--   [INSERT](#grant-insert)
--   [ALTER](#grant-alter)
-    -   `ALTER TABLE`
-        -   `ALTER UPDATE`
-        -   `ALTER DELETE`
-        -   `ALTER COLUMN`
-            -   `ALTER ADD COLUMN`
-            -   `ALTER DROP COLUMN`
-            -   `ALTER MODIFY COLUMN`
-            -   `ALTER COMMENT COLUMN`
-            -   `ALTER CLEAR COLUMN`
-            -   `ALTER RENAME COLUMN`
-        -   `ALTER INDEX`
-            -   `ALTER ORDER BY`
-            -   `ALTER ADD INDEX`
-            -   `ALTER DROP INDEX`
-            -   `ALTER MATERIALIZE INDEX`
-            -   `ALTER CLEAR INDEX`
-        -   `ALTER CONSTRAINT`
-            -   `ALTER ADD CONSTRAINT`
-            -   `ALTER DROP CONSTRAINT`
-        -   `ALTER TTL`
-        -   `ALTER MATERIALIZE TTL`
-        -   `ALTER SETTINGS`
-        -   `ALTER MOVE PARTITION`
-        -   `ALTER FETCH PARTITION`
-        -   `ALTER FREEZE PARTITION`
-    -   `ALTER VIEW`
-        -   `ALTER VIEW REFRESH`
-        -   `ALTER VIEW MODIFY QUERY`
--   [CREATE](#grant-create)
-    -   `CREATE DATABASE`
-    -   `CREATE TABLE`
-    -   `CREATE VIEW`
-    -   `CREATE DICTIONARY`
-    -   `CREATE TEMPORARY TABLE`
--   [DROP](#grant-drop)
-    -   `DROP DATABASE`
-    -   `DROP TABLE`
-    -   `DROP VIEW`
-    -   `DROP DICTIONARY`
--   [TRUNCATE](#grant-truncate)
--   [OPTIMIZE](#grant-optimize)
--   [SHOW](#grant-show)
-    -   `SHOW DATABASES`
-    -   `SHOW TABLES`
-    -   `SHOW COLUMNS`
-    -   `SHOW DICTIONARIES`
--   [KILL QUERY](#grant-kill-query)
--   [ACCESS MANAGEMENT](#grant-access-management)
-    -   `CREATE USER`
-    -   `ALTER USER`
-    -   `DROP USER`
-    -   `CREATE ROLE`
-    -   `ALTER ROLE`
-    -   `DROP ROLE`
-    -   `CREATE ROW POLICY`
-    -   `ALTER ROW POLICY`
-    -   `DROP ROW POLICY`
-    -   `CREATE QUOTA`
-    -   `ALTER QUOTA`
-    -   `DROP QUOTA`
-    -   `CREATE SETTINGS PROFILE`
-    -   `ALTER SETTINGS PROFILE`
-    -   `DROP SETTINGS PROFILE`
-    -   `SHOW ACCESS`
-        -   `SHOW_USERS`
-        -   `SHOW_ROLES`
-        -   `SHOW_ROW_POLICIES`
-        -   `SHOW_QUOTAS`
-        -   `SHOW_SETTINGS_PROFILES`
-    -   `ROLE ADMIN`
--   [SYSTEM](#grant-system)
-    -   `SYSTEM SHUTDOWN`
-    -   `SYSTEM DROP CACHE`
-        -   `SYSTEM DROP DNS CACHE`
-        -   `SYSTEM DROP MARK CACHE`
-        -   `SYSTEM DROP UNCOMPRESSED CACHE`
-    -   `SYSTEM RELOAD`
-        -   `SYSTEM RELOAD CONFIG`
-        -   `SYSTEM RELOAD DICTIONARY`
-        -   `SYSTEM RELOAD EMBEDDED DICTIONARIES`
-    -   `SYSTEM MERGES`
-    -   `SYSTEM TTL MERGES`
-    -   `SYSTEM FETCHES`
-    -   `SYSTEM MOVES`
-    -   `SYSTEM SENDS`
-        -   `SYSTEM DISTRIBUTED SENDS`
-        -   `SYSTEM REPLICATED SENDS`
-    -   `SYSTEM REPLICATION QUEUES`
-    -   `SYSTEM SYNC REPLICA`
-    -   `SYSTEM RESTART REPLICA`
-    -   `SYSTEM FLUSH`
-        -   `SYSTEM FLUSH DISTRIBUTED`
-        -   `SYSTEM FLUSH LOGS`
--   [INTROSPECTION](#grant-introspection)
-    -   `addressToLine`
-    -   `addressToSymbol`
-    -   `demangle`
--   [SOURCES](#grant-sources)
-    -   `FILE`
-    -   `URL`
-    -   `REMOTE`
-    -   `YSQL`
-    -   `ODBC`
-    -   `JDBC`
-    -   `HDFS`
-    -   `S3`
--   [dictGet](#grant-dictget)
-
-Ejemplos de cómo se trata esta jerarquía:
-
--   El `ALTER` privilegio incluye todos los demás `ALTER*` privilegio.
--   `ALTER CONSTRAINT` incluir `ALTER ADD CONSTRAINT` y `ALTER DROP CONSTRAINT` privilegio.
-
-Los privilegios se aplican a diferentes niveles. Conocer un nivel sugiere sintaxis disponible para privilegios.
-
-Niveles (de menor a mayor):
-
--   `COLUMN` — Privilege can be granted for column, table, database, or globally.
--   `TABLE` — Privilege can be granted for table, database, or globally.
--   `VIEW` — Privilege can be granted for view, database, or globally.
--   `DICTIONARY` — Privilege can be granted for dictionary, database, or globally.
--   `DATABASE` — Privilege can be granted for database or globally.
--   `GLOBAL` — Privilege can be granted only globally.
--   `GROUP` — Groups privileges of different levels. When `GROUP`-level privilegio se concede, sólo los privilegios del grupo se conceden que corresponden a la sintaxis utilizada.
-
-Ejemplos de sintaxis permitida:
-
--   `GRANT SELECT(x) ON db.table TO user`
--   `GRANT SELECT ON db.* TO user`
-
-Ejemplos de sintaxis no permitida:
-
--   `GRANT CREATE USER(x) ON db.table TO user`
--   `GRANT CREATE USER ON db.* TO user`
-
-El privilegio especial [ALL](#grant-all) otorga todos los privilegios a una cuenta de usuario o un rol.
-
-De forma predeterminada, una cuenta de usuario o un rol no tiene privilegios.
-
-Si un usuario o rol no tiene privilegios, se muestra como [NONE](#grant-none) privilegio.
-
-Algunas consultas por su implementación requieren un conjunto de privilegios. Por ejemplo, para realizar el [RENAME](misc.md#misc_operations-rename) consulta necesita los siguientes privilegios: `SELECT`, `CREATE TABLE`, `INSERT` y `DROP TABLE`.
-
-### SELECT {#grant-select}
-
-Permite realizar [SELECT](select/index.md) consulta.
-
-Nivel de privilegio: `COLUMN`.
-
-**Descripci**
-
-El usuario concedido con este privilegio puede realizar `SELECT` consultas sobre una lista especificada de columnas en la tabla y la base de datos especificadas. Si el usuario incluye otras columnas, entonces se especifica una consulta no devuelve datos.
-
-Considere el siguiente privilegio:
-
-``` sql
-GRANT SELECT(x,y) ON db.table TO john
-```
-
-Este privilegio permite `john` realizar cualquier `SELECT` consulta que involucra datos de la `x` y/o `y` columnas en `db.table`. Por ejemplo, `SELECT x FROM db.table`. `john` no puede realizar `SELECT z FROM db.table`. El `SELECT * FROM db.table` también no está disponible. Al procesar esta consulta, ClickHouse no devuelve ningún dato, incluso `x` y `y`. La única excepción es si una tabla `x` y `y` columnas, en este caso ClickHouse devuelve todos los datos.
-
-### INSERT {#grant-insert}
-
-Permite realizar [INSERT](insert-into.md) consulta.
-
-Nivel de privilegio: `COLUMN`.
-
-**Descripci**
-
-El usuario concedido con este privilegio puede realizar `INSERT` consultas sobre una lista especificada de columnas en la tabla y la base de datos especificadas. Si el usuario incluye otras columnas, entonces se especifica una consulta no inserta ningún dato.
-
-**Ejemplo**
-
-``` sql
-GRANT INSERT(x,y) ON db.table TO john
-```
-
-El privilegio concedido permite `john` para insertar datos en el `x` y/o `y` columnas en `db.table`.
-
-### ALTER {#grant-alter}
-
-Permite realizar [ALTER](alter.md) consultas correspondientes a la siguiente jerarquía de privilegios:
-
--   `ALTER`. Nivel: `COLUMN`.
-    -   `ALTER TABLE`. Nivel: `GROUP`
-        -   `ALTER UPDATE`. Nivel: `COLUMN`. Apodo: `UPDATE`
-        -   `ALTER DELETE`. Nivel: `COLUMN`. Apodo: `DELETE`
-        -   `ALTER COLUMN`. Nivel: `GROUP`
-            -   `ALTER ADD COLUMN`. Nivel: `COLUMN`. Apodo: `ADD COLUMN`
-            -   `ALTER DROP COLUMN`. Nivel: `COLUMN`. Apodo: `DROP COLUMN`
-            -   `ALTER MODIFY COLUMN`. Nivel: `COLUMN`. Apodo: `MODIFY COLUMN`
-            -   `ALTER COMMENT COLUMN`. Nivel: `COLUMN`. Apodo: `COMMENT COLUMN`
-            -   `ALTER CLEAR COLUMN`. Nivel: `COLUMN`. Apodo: `CLEAR COLUMN`
-            -   `ALTER RENAME COLUMN`. Nivel: `COLUMN`. Apodo: `RENAME COLUMN`
-        -   `ALTER INDEX`. Nivel: `GROUP`. Apodo: `INDEX`
-            -   `ALTER ORDER BY`. Nivel: `TABLE`. Apodo: `ALTER MODIFY ORDER BY`, `MODIFY ORDER BY`
-            -   `ALTER ADD INDEX`. Nivel: `TABLE`. Apodo: `ADD INDEX`
-            -   `ALTER DROP INDEX`. Nivel: `TABLE`. Apodo: `DROP INDEX`
-            -   `ALTER MATERIALIZE INDEX`. Nivel: `TABLE`. Apodo: `MATERIALIZE INDEX`
-            -   `ALTER CLEAR INDEX`. Nivel: `TABLE`. Apodo: `CLEAR INDEX`
-        -   `ALTER CONSTRAINT`. Nivel: `GROUP`. Apodo: `CONSTRAINT`
-            -   `ALTER ADD CONSTRAINT`. Nivel: `TABLE`. Apodo: `ADD CONSTRAINT`
-            -   `ALTER DROP CONSTRAINT`. Nivel: `TABLE`. Apodo: `DROP CONSTRAINT`
-        -   `ALTER TTL`. Nivel: `TABLE`. Apodo: `ALTER MODIFY TTL`, `MODIFY TTL`
-        -   `ALTER MATERIALIZE TTL`. Nivel: `TABLE`. Apodo: `MATERIALIZE TTL`
-        -   `ALTER SETTINGS`. Nivel: `TABLE`. Apodo: `ALTER SETTING`, `ALTER MODIFY SETTING`, `MODIFY SETTING`
-        -   `ALTER MOVE PARTITION`. Nivel: `TABLE`. Apodo: `ALTER MOVE PART`, `MOVE PARTITION`, `MOVE PART`
-        -   `ALTER FETCH PARTITION`. Nivel: `TABLE`. Apodo: `FETCH PARTITION`
-        -   `ALTER FREEZE PARTITION`. Nivel: `TABLE`. Apodo: `FREEZE PARTITION`
-    -   `ALTER VIEW` Nivel: `GROUP`
-        -   `ALTER VIEW REFRESH`. Nivel: `VIEW`. Apodo: `ALTER LIVE VIEW REFRESH`, `REFRESH VIEW`
-        -   `ALTER VIEW MODIFY QUERY`. Nivel: `VIEW`. Apodo: `ALTER TABLE MODIFY QUERY`
-
-Ejemplos de cómo se trata esta jerarquía:
-
--   El `ALTER` privilegio incluye todos los demás `ALTER*` privilegio.
--   `ALTER CONSTRAINT` incluir `ALTER ADD CONSTRAINT` y `ALTER DROP CONSTRAINT` privilegio.
-
-**Nota**
-
--   El `MODIFY SETTING` privilege permite modificar la configuración del motor de tablas. In no afecta la configuración o los parámetros de configuración del servidor.
--   El `ATTACH` operación necesita el [CREATE](#grant-create) privilegio.
--   El `DETACH` operación necesita el [DROP](#grant-drop) privilegio.
--   Para detener la mutación por el [KILL MUTATION](misc.md#kill-mutation) consulta, necesita tener un privilegio para iniciar esta mutación. Por ejemplo, si desea detener el `ALTER UPDATE` consulta, necesita el `ALTER UPDATE`, `ALTER TABLE`, o `ALTER` privilegio.
-
-### CREATE {#grant-create}
-
-Permite realizar [CREATE](create.md) y [ATTACH](misc.md#attach) Consultas DDL correspondientes a la siguiente jerarquía de privilegios:
-
--   `CREATE`. Nivel: `GROUP`
-    -   `CREATE DATABASE`. Nivel: `DATABASE`
-    -   `CREATE TABLE`. Nivel: `TABLE`
-    -   `CREATE VIEW`. Nivel: `VIEW`
-    -   `CREATE DICTIONARY`. Nivel: `DICTIONARY`
-    -   `CREATE TEMPORARY TABLE`. Nivel: `GLOBAL`
-
-**Nota**
-
--   Para eliminar la tabla creada, un usuario necesita [DROP](#grant-drop).
-
-### DROP {#grant-drop}
-
-Permite realizar [DROP](misc.md#drop) y [DETACH](misc.md#detach) consultas correspondientes a la siguiente jerarquía de privilegios:
-
--   `DROP`. Nivel:
-    -   `DROP DATABASE`. Nivel: `DATABASE`
-    -   `DROP TABLE`. Nivel: `TABLE`
-    -   `DROP VIEW`. Nivel: `VIEW`
-    -   `DROP DICTIONARY`. Nivel: `DICTIONARY`
-
-### TRUNCATE {#grant-truncate}
-
-Permite realizar [TRUNCATE](misc.md#truncate-statement) consulta.
-
-Nivel de privilegio: `TABLE`.
-
-### OPTIMIZE {#grant-optimize}
-
-Permite realizar el [OPTIMIZE TABLE](misc.md#misc_operations-optimize) consulta.
-
-Nivel de privilegio: `TABLE`.
-
-### SHOW {#grant-show}
-
-Permite realizar `SHOW`, `DESCRIBE`, `USE`, y `EXISTS` consultas, correspondientes a la siguiente jerarquía de privilegios:
-
--   `SHOW`. Nivel: `GROUP`
-    -   `SHOW DATABASES`. Nivel: `DATABASE`. Permite ejecutar `SHOW DATABASES`, `SHOW CREATE DATABASE`, `USE <database>` consulta.
-    -   `SHOW TABLES`. Nivel: `TABLE`. Permite ejecutar `SHOW TABLES`, `EXISTS <table>`, `CHECK <table>` consulta.
-    -   `SHOW COLUMNS`. Nivel: `COLUMN`. Permite ejecutar `SHOW CREATE TABLE`, `DESCRIBE` consulta.
-    -   `SHOW DICTIONARIES`. Nivel: `DICTIONARY`. Permite ejecutar `SHOW DICTIONARIES`, `SHOW CREATE DICTIONARY`, `EXISTS <dictionary>` consulta.
-
-**Nota**
-
-Un usuario tiene el `SHOW` privilegio si tiene algún otro privilegio relativo a la tabla, diccionario o base de datos especificados.
-
-### KILL QUERY {#grant-kill-query}
-
-Permite realizar el [KILL](misc.md#kill-query-statement) consultas correspondientes a la siguiente jerarquía de privilegios:
-
-Nivel de privilegio: `GLOBAL`.
-
-**Nota**
-
-`KILL QUERY` privilege permite a un usuario matar consultas de otros usuarios.
-
-### ACCESS MANAGEMENT {#grant-access-management}
-
-Permite a un usuario realizar consultas que administran usuarios, roles y directivas de fila.
-
--   `ACCESS MANAGEMENT`. Nivel: `GROUP`
-    -   `CREATE USER`. Nivel: `GLOBAL`
-    -   `ALTER USER`. Nivel: `GLOBAL`
-    -   `DROP USER`. Nivel: `GLOBAL`
-    -   `CREATE ROLE`. Nivel: `GLOBAL`
-    -   `ALTER ROLE`. Nivel: `GLOBAL`
-    -   `DROP ROLE`. Nivel: `GLOBAL`
-    -   `ROLE ADMIN`. Nivel: `GLOBAL`
-    -   `CREATE ROW POLICY`. Nivel: `GLOBAL`. Apodo: `CREATE POLICY`
-    -   `ALTER ROW POLICY`. Nivel: `GLOBAL`. Apodo: `ALTER POLICY`
-    -   `DROP ROW POLICY`. Nivel: `GLOBAL`. Apodo: `DROP POLICY`
-    -   `CREATE QUOTA`. Nivel: `GLOBAL`
-    -   `ALTER QUOTA`. Nivel: `GLOBAL`
-    -   `DROP QUOTA`. Nivel: `GLOBAL`
-    -   `CREATE SETTINGS PROFILE`. Nivel: `GLOBAL`. Apodo: `CREATE PROFILE`
-    -   `ALTER SETTINGS PROFILE`. Nivel: `GLOBAL`. Apodo: `ALTER PROFILE`
-    -   `DROP SETTINGS PROFILE`. Nivel: `GLOBAL`. Apodo: `DROP PROFILE`
-    -   `SHOW ACCESS`. Nivel: `GROUP`
-        -   `SHOW_USERS`. Nivel: `GLOBAL`. Apodo: `SHOW CREATE USER`
-        -   `SHOW_ROLES`. Nivel: `GLOBAL`. Apodo: `SHOW CREATE ROLE`
-        -   `SHOW_ROW_POLICIES`. Nivel: `GLOBAL`. Apodo: `SHOW POLICIES`, `SHOW CREATE ROW POLICY`, `SHOW CREATE POLICY`
-        -   `SHOW_QUOTAS`. Nivel: `GLOBAL`. Apodo: `SHOW CREATE QUOTA`
-        -   `SHOW_SETTINGS_PROFILES`. Nivel: `GLOBAL`. Apodo: `SHOW PROFILES`, `SHOW CREATE SETTINGS PROFILE`, `SHOW CREATE PROFILE`
-
-El `ROLE ADMIN` privilege permite a un usuario otorgar y revocar cualquier función, incluidas aquellas que no se otorgan al usuario con la opción de administrador.
-
-### SYSTEM {#grant-system}
-
-Permite a un usuario realizar el [SYSTEM](system.md) consultas correspondientes a la siguiente jerarquía de privilegios.
-
--   `SYSTEM`. Nivel: `GROUP`
-    -   `SYSTEM SHUTDOWN`. Nivel: `GLOBAL`. Apodo: `SYSTEM KILL`, `SHUTDOWN`
-    -   `SYSTEM DROP CACHE`. Apodo: `DROP CACHE`
-        -   `SYSTEM DROP DNS CACHE`. Nivel: `GLOBAL`. Apodo: `SYSTEM DROP DNS`, `DROP DNS CACHE`, `DROP DNS`
-        -   `SYSTEM DROP MARK CACHE`. Nivel: `GLOBAL`. Apodo: `SYSTEM DROP MARK`, `DROP MARK CACHE`, `DROP MARKS`
-        -   `SYSTEM DROP UNCOMPRESSED CACHE`. Nivel: `GLOBAL`. Apodo: `SYSTEM DROP UNCOMPRESSED`, `DROP UNCOMPRESSED CACHE`, `DROP UNCOMPRESSED`
-    -   `SYSTEM RELOAD`. Nivel: `GROUP`
-        -   `SYSTEM RELOAD CONFIG`. Nivel: `GLOBAL`. Apodo: `RELOAD CONFIG`
-        -   `SYSTEM RELOAD DICTIONARY`. Nivel: `GLOBAL`. Apodo: `SYSTEM RELOAD DICTIONARIES`, `RELOAD DICTIONARY`, `RELOAD DICTIONARIES`
-        -   `SYSTEM RELOAD EMBEDDED DICTIONARIES`. Nivel: `GLOBAL`. Aliases: R`ELOAD EMBEDDED DICTIONARIES`
-    -   `SYSTEM MERGES`. Nivel: `TABLE`. Apodo: `SYSTEM STOP MERGES`, `SYSTEM START MERGES`, `STOP MERGES`, `START MERGES`
-    -   `SYSTEM TTL MERGES`. Nivel: `TABLE`. Apodo: `SYSTEM STOP TTL MERGES`, `SYSTEM START TTL MERGES`, `STOP TTL MERGES`, `START TTL MERGES`
-    -   `SYSTEM FETCHES`. Nivel: `TABLE`. Apodo: `SYSTEM STOP FETCHES`, `SYSTEM START FETCHES`, `STOP FETCHES`, `START FETCHES`
-    -   `SYSTEM MOVES`. Nivel: `TABLE`. Apodo: `SYSTEM STOP MOVES`, `SYSTEM START MOVES`, `STOP MOVES`, `START MOVES`
-    -   `SYSTEM SENDS`. Nivel: `GROUP`. Apodo: `SYSTEM STOP SENDS`, `SYSTEM START SENDS`, `STOP SENDS`, `START SENDS`
-        -   `SYSTEM DISTRIBUTED SENDS`. Nivel: `TABLE`. Apodo: `SYSTEM STOP DISTRIBUTED SENDS`, `SYSTEM START DISTRIBUTED SENDS`, `STOP DISTRIBUTED SENDS`, `START DISTRIBUTED SENDS`
-        -   `SYSTEM REPLICATED SENDS`. Nivel: `TABLE`. Apodo: `SYSTEM STOP REPLICATED SENDS`, `SYSTEM START REPLICATED SENDS`, `STOP REPLICATED SENDS`, `START REPLICATED SENDS`
-    -   `SYSTEM REPLICATION QUEUES`. Nivel: `TABLE`. Apodo: `SYSTEM STOP REPLICATION QUEUES`, `SYSTEM START REPLICATION QUEUES`, `STOP REPLICATION QUEUES`, `START REPLICATION QUEUES`
-    -   `SYSTEM SYNC REPLICA`. Nivel: `TABLE`. Apodo: `SYNC REPLICA`
-    -   `SYSTEM RESTART REPLICA`. Nivel: `TABLE`. Apodo: `RESTART REPLICA`
-    -   `SYSTEM FLUSH`. Nivel: `GROUP`
-        -   `SYSTEM FLUSH DISTRIBUTED`. Nivel: `TABLE`. Apodo: `FLUSH DISTRIBUTED`
-        -   `SYSTEM FLUSH LOGS`. Nivel: `GLOBAL`. Apodo: `FLUSH LOGS`
-
-El `SYSTEM RELOAD EMBEDDED DICTIONARIES` privilegios otorgados implícitamente por el `SYSTEM RELOAD DICTIONARY ON *.*` privilegio.
-
-### INTROSPECTION {#grant-introspection}
-
-Permite usar [introspección](../../operations/optimizing-performance/sampling-query-profiler.md) función.
-
--   `INTROSPECTION`. Nivel: `GROUP`. Apodo: `INTROSPECTION FUNCTIONS`
-    -   `addressToLine`. Nivel: `GLOBAL`
-    -   `addressToSymbol`. Nivel: `GLOBAL`
-    -   `demangle`. Nivel: `GLOBAL`
-
-### SOURCES {#grant-sources}
-
-Permite utilizar fuentes de datos externas. Se aplica a [motores de mesa](../../engines/table-engines/index.md) y [funciones de la tabla](../table-functions/index.md#table-functions).
-
--   `SOURCES`. Nivel: `GROUP`
-    -   `FILE`. Nivel: `GLOBAL`
-    -   `URL`. Nivel: `GLOBAL`
-    -   `REMOTE`. Nivel: `GLOBAL`
-    -   `YSQL`. Nivel: `GLOBAL`
-    -   `ODBC`. Nivel: `GLOBAL`
-    -   `JDBC`. Nivel: `GLOBAL`
-    -   `HDFS`. Nivel: `GLOBAL`
-    -   `S3`. Nivel: `GLOBAL`
-
-El `SOURCES` privilege permite el uso de todas las fuentes. También puede otorgar un privilegio para cada fuente individualmente. Para usar fuentes, necesita privilegios adicionales.
-
-Ejemplos:
-
--   Para crear una tabla con el [Motor de tablas MySQL](../../engines/table-engines/integrations/mysql.md) usted necesita `CREATE TABLE (ON db.table_name)` y `MYSQL` privilegio.
--   Para utilizar el [función de tabla mysql](../table-functions/mysql.md) usted necesita `CREATE TEMPORARY TABLE` y `MYSQL` privilegio.
-
-### dictGet {#grant-dictget}
-
--   `dictGet`. Apodo: `dictHas`, `dictGetHierarchy`, `dictIsIn`
-
-Permite a un usuario ejecutar [dictGet](../functions/ext-dict-functions.md#dictget), [dictHas](../functions/ext-dict-functions.md#dicthas), [dictGetHierarchy](../functions/ext-dict-functions.md#dictgethierarchy), [DictIsIn](../functions/ext-dict-functions.md#dictisin) función.
-
-Nivel de privilegio: `DICTIONARY`.
-
-**Ejemplos**
-
--   `GRANT dictGet ON mydb.mydictionary TO john`
--   `GRANT dictGet ON mydictionary TO john`
-
-### ALL {#grant-all}
-
-Otorga todos los privilegios de la entidad regulada a una cuenta de usuario o un rol.
-
-### NONE {#grant-none}
-
-No otorga ningún privilegio.
-
-### ADMIN OPTION {#admin-option-privilege}
-
-El `ADMIN OPTION` privilege permite a un usuario otorgar su rol a otro usuario.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/grant/) <!--hide-->
diff --git a/docs/es/sql-reference/statements/index.md b/docs/es/sql-reference/statements/index.md
deleted file mode 100644
index befef794c1c4..000000000000
--- a/docs/es/sql-reference/statements/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Instrucci\xF3n"
-toc_priority: 31
----
-
-
diff --git a/docs/es/sql-reference/statements/insert-into.md b/docs/es/sql-reference/statements/insert-into.md
deleted file mode 100644
index 4f64af830e7a..000000000000
--- a/docs/es/sql-reference/statements/insert-into.md
+++ /dev/null
@@ -1,80 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: INSERT INTO
----
-
-## INSERT {#insert}
-
-Adición de datos.
-
-Formato de consulta básico:
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ...
-```
-
-La consulta puede especificar una lista de columnas para insertar `[(c1, c2, c3)]`. En este caso, el resto de las columnas se llenan con:
-
--   Los valores calculados a partir del `DEFAULT` expresiones especificadas en la definición de la tabla.
--   Ceros y cadenas vacías, si `DEFAULT` expresiones no están definidas.
-
-Si [strict_insert_defaults=1](../../operations/settings/settings.md), columnas que no tienen `DEFAULT` definido debe figurar en la consulta.
-
-Los datos se pueden pasar al INSERT en cualquier [formato](../../interfaces/formats.md#formats) con el apoyo de ClickHouse. El formato debe especificarse explícitamente en la consulta:
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] FORMAT format_name data_set
-```
-
-For example, the following query format is identical to the basic version of INSERT … VALUES:
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] FORMAT Values (v11, v12, v13), (v21, v22, v23), ...
-```
-
-ClickHouse elimina todos los espacios y un avance de línea (si hay uno) antes de los datos. Al formar una consulta, recomendamos colocar los datos en una nueva línea después de los operadores de consulta (esto es importante si los datos comienzan con espacios).
-
-Ejemplo:
-
-``` sql
-INSERT INTO t FORMAT TabSeparated
-11  Hello, world!
-22  Qwerty
-```
-
-Puede insertar datos por separado de la consulta mediante el cliente de línea de comandos o la interfaz HTTP. Para obtener más información, consulte la sección “[Interfaz](../../interfaces/index.md#interfaces)”.
-
-### Limitación {#constraints}
-
-Si la tabla tiene [limitación](create.md#constraints), their expressions will be checked for each row of inserted data. If any of those constraints is not satisfied — server will raise an exception containing constraint name and expression, the query will be stopped.
-
-### Insertar los resultados de `SELECT` {#insert_query_insert-select}
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...
-```
-
-Las columnas se asignan de acuerdo con su posición en la cláusula SELECT. Sin embargo, sus nombres en la expresión SELECT y la tabla para INSERT pueden diferir. Si es necesario, se realiza la fundición de tipo.
-
-Ninguno de los formatos de datos, excepto Valores, permite establecer valores para expresiones como `now()`, `1 + 2` y así sucesivamente. El formato Values permite el uso limitado de expresiones, pero esto no se recomienda, porque en este caso se usa código ineficiente para su ejecución.
-
-No se admiten otras consultas para modificar partes de datos: `UPDATE`, `DELETE`, `REPLACE`, `MERGE`, `UPSERT`, `INSERT UPDATE`.
-Sin embargo, puede eliminar datos antiguos usando `ALTER TABLE ... DROP PARTITION`.
-
-`FORMAT` cláusula debe especificarse al final de la consulta si `SELECT` cláusula contiene la función de tabla [entrada()](../table-functions/input.md).
-
-### Consideraciones de rendimiento {#performance-considerations}
-
-`INSERT` ordena los datos de entrada por clave principal y los divide en particiones por una clave de partición. Si inserta datos en varias particiones a la vez, puede reducir significativamente el rendimiento del `INSERT` consulta. Para evitar esto:
-
--   Agregue datos en lotes bastante grandes, como 100.000 filas a la vez.
--   Agrupe los datos por una clave de partición antes de cargarlos en ClickHouse.
-
-El rendimiento no disminuirá si:
-
--   Los datos se agregan en tiempo real.
--   Carga datos que normalmente están ordenados por tiempo.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/insert_into/) <!--hide-->
diff --git a/docs/es/sql-reference/statements/misc.md b/docs/es/sql-reference/statements/misc.md
deleted file mode 100644
index 49cbc4fe558f..000000000000
--- a/docs/es/sql-reference/statements/misc.md
+++ /dev/null
@@ -1,358 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: Otro
----
-
-# Consultas Misceláneas {#miscellaneous-queries}
-
-## ATTACH {#attach}
-
-Esta consulta es exactamente la misma que `CREATE`, pero
-
--   En lugar de la palabra `CREATE` utiliza la palabra `ATTACH`.
--   La consulta no crea datos en el disco, pero supone que los datos ya están en los lugares apropiados, y simplemente agrega información sobre la tabla al servidor.
-    Después de ejecutar una consulta ATTACH, el servidor sabrá sobre la existencia de la tabla.
-
-Si la tabla se separó previamente (`DETACH`), lo que significa que su estructura es conocida, puede usar taquigrafía sin definir la estructura.
-
-``` sql
-ATTACH TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Esta consulta se utiliza al iniciar el servidor. El servidor almacena los metadatos de la tabla como archivos con `ATTACH` consultas, que simplemente se ejecuta en el lanzamiento (con la excepción de las tablas del sistema, que se crean explícitamente en el servidor).
-
-## CHECK TABLE {#check-table}
-
-Comprueba si los datos de la tabla están dañados.
-
-``` sql
-CHECK TABLE [db.]name
-```
-
-El `CHECK TABLE` query compara los tamaños de archivo reales con los valores esperados que se almacenan en el servidor. Si los tamaños de archivo no coinciden con los valores almacenados, significa que los datos están dañados. Esto puede deberse, por ejemplo, a un bloqueo del sistema durante la ejecución de la consulta.
-
-La respuesta de consulta contiene el `result` columna con una sola fila. La fila tiene un valor de
-[Booleana](../../sql-reference/data-types/boolean.md) tipo:
-
--   0 - Los datos de la tabla están dañados.
--   1 - Los datos mantienen la integridad.
-
-El `CHECK TABLE` query admite los siguientes motores de tablas:
-
--   [Registro](../../engines/table-engines/log-family/log.md)
--   [TinyLog](../../engines/table-engines/log-family/tinylog.md)
--   [StripeLog](../../engines/table-engines/log-family/stripelog.md)
--   [Familia MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)
-
-Realizado sobre las tablas con otros motores de tabla causa una excepción.
-
-Motores del `*Log` la familia no proporciona la recuperación automática de datos en caso de fallo. Utilice el `CHECK TABLE` consulta para rastrear la pérdida de datos de manera oportuna.
-
-Para `MergeTree` motores familiares, el `CHECK TABLE` query muestra un estado de comprobación para cada parte de datos individual de una tabla en el servidor local.
-
-**Si los datos están dañados**
-
-Si la tabla está dañada, puede copiar los datos no dañados a otra tabla. Para hacer esto:
-
-1.  Cree una nueva tabla con la misma estructura que la tabla dañada. Para ello, ejecute la consulta `CREATE TABLE <new_table_name> AS <damaged_table_name>`.
-2.  Establezca el [max_threads](../../operations/settings/settings.md#settings-max_threads) valor a 1 para procesar la siguiente consulta en un único subproceso. Para ello, ejecute la consulta `SET max_threads = 1`.
-3.  Ejecutar la consulta `INSERT INTO <new_table_name> SELECT * FROM <damaged_table_name>`. Esta solicitud copia los datos no dañados de la tabla dañada a otra tabla. Solo se copiarán los datos anteriores a la parte dañada.
-4.  Reinicie el `clickhouse-client` para restablecer el `max_threads` valor.
-
-## DESCRIBE TABLE {#misc-describe-table}
-
-``` sql
-DESC|DESCRIBE TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
-```
-
-Devuelve lo siguiente `String` tipo columnas:
-
--   `name` — Column name.
--   `type`— Column type.
--   `default_type` — Clause that is used in [expresión predeterminada](create.md#create-default-values) (`DEFAULT`, `MATERIALIZED` o `ALIAS`). La columna contiene una cadena vacía, si no se especifica la expresión predeterminada.
--   `default_expression` — Value specified in the `DEFAULT` clausula.
--   `comment_expression` — Comment text.
-
-Las estructuras de datos anidadas se generan en “expanded” formato. Cada columna se muestra por separado, con el nombre después de un punto.
-
-## DETACH {#detach}
-
-Elimina información sobre el ‘name’ tabla desde el servidor. El servidor deja de saber sobre la existencia de la tabla.
-
-``` sql
-DETACH TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Esto no elimina los datos ni los metadatos de la tabla. En el próximo lanzamiento del servidor, el servidor leerá los metadatos y volverá a conocer la tabla.
-Del mismo modo, un “detached” se puede volver a conectar usando el `ATTACH` consulta (con la excepción de las tablas del sistema, que no tienen metadatos almacenados para ellas).
-
-No hay `DETACH DATABASE` consulta.
-
-## DROP {#drop}
-
-Esta consulta tiene dos tipos: `DROP DATABASE` y `DROP TABLE`.
-
-``` sql
-DROP DATABASE [IF EXISTS] db [ON CLUSTER cluster]
-```
-
-Elimina todas las tablas dentro del ‘db’ base de datos, a continuación, elimina ‘db’ base de datos en sí.
-Si `IF EXISTS` se especifica, no devuelve un error si la base de datos no existe.
-
-``` sql
-DROP [TEMPORARY] TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Elimina la tabla.
-Si `IF EXISTS` se especifica, no devuelve un error si la tabla no existe o la base de datos no existe.
-
-    DROP DICTIONARY [IF EXISTS] [db.]name
-
-Elimina el diccionario.
-Si `IF EXISTS` se especifica, no devuelve un error si la tabla no existe o la base de datos no existe.
-
-## DROP USER {#drop-user-statement}
-
-Elimina un usuario.
-
-### Sintaxis {#drop-user-syntax}
-
-``` sql
-DROP USER [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP ROLE {#drop-role-statement}
-
-Elimina un rol.
-
-El rol eliminado se revoca de todas las entidades donde se concedió.
-
-### Sintaxis {#drop-role-syntax}
-
-``` sql
-DROP ROLE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP ROW POLICY {#drop-row-policy-statement}
-
-Elimina una directiva de fila.
-
-La directiva de filas eliminadas se revoca de todas las entidades a las que se asignó.
-
-### Sintaxis {#drop-row-policy-syntax}
-
-``` sql
-DROP [ROW] POLICY [IF EXISTS] name [,...] ON [database.]table [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP QUOTA {#drop-quota-statement}
-
-Elimina una cuota.
-
-La cuota eliminada se revoca de todas las entidades a las que se asignó.
-
-### Sintaxis {#drop-quota-syntax}
-
-``` sql
-DROP QUOTA [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP SETTINGS PROFILE {#drop-settings-profile-statement}
-
-Elimina una cuota.
-
-La cuota eliminada se revoca de todas las entidades a las que se asignó.
-
-### Sintaxis {#drop-settings-profile-syntax}
-
-``` sql
-DROP [SETTINGS] PROFILE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## EXISTS {#exists-statement}
-
-``` sql
-EXISTS [TEMPORARY] [TABLE|DICTIONARY] [db.]name [INTO OUTFILE filename] [FORMAT format]
-```
-
-Devuelve una sola `UInt8`columna -type, que contiene el valor único `0` si la tabla o base de datos no existe, o `1` si la tabla existe en la base de datos especificada.
-
-## KILL QUERY {#kill-query-statement}
-
-``` sql
-KILL QUERY [ON CLUSTER cluster]
-  WHERE <where expression to SELECT FROM system.processes query>
-  [SYNC|ASYNC|TEST]
-  [FORMAT format]
-```
-
-Intenta terminar por la fuerza las consultas que se están ejecutando actualmente.
-Las consultas a finalizar se seleccionan en el sistema.tabla de procesos utilizando los criterios definidos en el `WHERE` cláusula de la `KILL` consulta.
-
-Ejemplos:
-
-``` sql
--- Forcibly terminates all queries with the specified query_id:
-KILL QUERY WHERE query_id='2-857d-4a57-9ee0-327da5d60a90'
-
--- Synchronously terminates all queries run by 'username':
-KILL QUERY WHERE user='username' SYNC
-```
-
-Los usuarios de solo lectura solo pueden detener sus propias consultas.
-
-De forma predeterminada, se utiliza la versión asincrónica de las consultas (`ASYNC`), que no espera la confirmación de que las consultas se han detenido.
-
-La versión síncrona (`SYNC`) espera a que se detengan todas las consultas y muestra información sobre cada proceso a medida que se detiene.
-La respuesta contiene el `kill_status` columna, que puede tomar los siguientes valores:
-
-1.  ‘finished’ – The query was terminated successfully.
-2.  ‘waiting’ – Waiting for the query to end after sending it a signal to terminate.
-3.  The other values ​​explain why the query can't be stopped.
-
-Una consulta de prueba (`TEST`) sólo comprueba los derechos del usuario y muestra una lista de consultas para detener.
-
-## KILL MUTATION {#kill-mutation}
-
-``` sql
-KILL MUTATION [ON CLUSTER cluster]
-  WHERE <where expression to SELECT FROM system.mutations query>
-  [TEST]
-  [FORMAT format]
-```
-
-Intenta cancelar y quitar [mutación](alter.md#alter-mutations) que se están ejecutando actualmente. Las mutaciones para cancelar se seleccionan en el [`system.mutations`](../../operations/system-tables.md#system_tables-mutations) utilizando el filtro especificado por el `WHERE` cláusula de la `KILL` consulta.
-
-Una consulta de prueba (`TEST`) sólo comprueba los derechos del usuario y muestra una lista de consultas para detener.
-
-Ejemplos:
-
-``` sql
--- Cancel and remove all mutations of the single table:
-KILL MUTATION WHERE database = 'default' AND table = 'table'
-
--- Cancel the specific mutation:
-KILL MUTATION WHERE database = 'default' AND table = 'table' AND mutation_id = 'mutation_3.txt'
-```
-
-The query is useful when a mutation is stuck and cannot finish (e.g. if some function in the mutation query throws an exception when applied to the data contained in the table).
-
-Los cambios ya realizados por la mutación no se revierten.
-
-## OPTIMIZE {#misc_operations-optimize}
-
-``` sql
-OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition | PARTITION ID 'partition_id'] [FINAL] [DEDUPLICATE]
-```
-
-Esta consulta intenta inicializar una combinación no programada de partes de datos para tablas con un motor de tablas [Método de codificación de datos:](../../engines/table-engines/mergetree-family/mergetree.md) familia.
-
-El `OPTMIZE` consulta también es compatible con el [Método de codificación de datos:](../../engines/table-engines/special/materializedview.md) y el [Búfer](../../engines/table-engines/special/buffer.md) motor. No se admiten otros motores de tabla.
-
-Cuando `OPTIMIZE` se utiliza con el [ReplicatedMergeTree](../../engines/table-engines/mergetree-family/replication.md) la familia de motores de tablas, ClickHouse crea una tarea para fusionar y espera la ejecución en todos los nodos (si `replication_alter_partitions_sync` está habilitada la configuración).
-
--   Si `OPTIMIZE` no realiza una fusión por ningún motivo, no notifica al cliente. Para habilitar las notificaciones, [Optize_throw_if_noop](../../operations/settings/settings.md#setting-optimize_throw_if_noop) configuración.
--   Si especifica un `PARTITION`, sólo la partición especificada está optimizada. [Cómo establecer la expresión de partición](alter.md#alter-how-to-specify-part-expr).
--   Si especifica `FINAL`, la optimización se realiza incluso cuando todos los datos ya están en una parte.
--   Si especifica `DEDUPLICATE`, luego se deduplicarán filas completamente idénticas (se comparan todas las columnas), tiene sentido solo para el motor MergeTree.
-
-!!! warning "Advertencia"
-    `OPTIMIZE` no se puede arreglar el “Too many parts” error.
-
-## RENAME {#misc_operations-rename}
-
-Cambia el nombre de una o más tablas.
-
-``` sql
-RENAME TABLE [db11.]name11 TO [db12.]name12, [db21.]name21 TO [db22.]name22, ... [ON CLUSTER cluster]
-```
-
-Todas las tablas se renombran bajo bloqueo global. Cambiar el nombre de las tablas es una operación ligera. Si ha indicado otra base de datos después de TO, la tabla se moverá a esta base de datos. Sin embargo, los directorios con bases de datos deben residir en el mismo sistema de archivos (de lo contrario, se devuelve un error).
-
-## SET {#query-set}
-
-``` sql
-SET param = value
-```
-
-Asignar `value` a la `param` [configuración](../../operations/settings/index.md) para la sesión actual. No se puede cambiar [configuración del servidor](../../operations/server-configuration-parameters/index.md) de esta manera.
-
-También puede establecer todos los valores del perfil de configuración especificado en una sola consulta.
-
-``` sql
-SET profile = 'profile-name-from-the-settings-file'
-```
-
-Para obtener más información, consulte [Configuración](../../operations/settings/settings.md).
-
-## SET ROLE {#set-role-statement}
-
-Activa roles para el usuario actual.
-
-### Sintaxis {#set-role-syntax}
-
-``` sql
-SET ROLE {DEFAULT | NONE | role [,...] | ALL | ALL EXCEPT role [,...]}
-```
-
-## SET DEFAULT ROLE {#set-default-role-statement}
-
-Establece roles predeterminados para un usuario.
-
-Los roles predeterminados se activan automáticamente al iniciar sesión del usuario. Puede establecer como predeterminado sólo los roles concedidos anteriormente. Si el rol no se concede a un usuario, ClickHouse produce una excepción.
-
-### Sintaxis {#set-default-role-syntax}
-
-``` sql
-SET DEFAULT ROLE {NONE | role [,...] | ALL | ALL EXCEPT role [,...]} TO {user|CURRENT_USER} [,...]
-```
-
-### Ejemplos {#set-default-role-examples}
-
-Establecer varios roles predeterminados para un usuario:
-
-``` sql
-SET DEFAULT ROLE role1, role2, ... TO user
-```
-
-Establezca todos los roles concedidos como predeterminados para un usuario:
-
-``` sql
-SET DEFAULT ROLE ALL TO user
-```
-
-Borrar roles predeterminados de un usuario:
-
-``` sql
-SET DEFAULT ROLE NONE TO user
-```
-
-Establezca todos los roles concedidos como predeterminados excepto algunos de ellos:
-
-``` sql
-SET DEFAULT ROLE ALL EXCEPT role1, role2 TO user
-```
-
-## TRUNCATE {#truncate-statement}
-
-``` sql
-TRUNCATE TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Elimina todos los datos de una tabla. Cuando la cláusula `IF EXISTS` se omite, la consulta devuelve un error si la tabla no existe.
-
-El `TRUNCATE` consulta no es compatible con [Vista](../../engines/table-engines/special/view.md), [File](../../engines/table-engines/special/file.md), [URL](../../engines/table-engines/special/url.md) y [Nulo](../../engines/table-engines/special/null.md) motores de mesa.
-
-## USE {#use}
-
-``` sql
-USE db
-```
-
-Permite establecer la base de datos actual para la sesión.
-La base de datos actual se utiliza para buscar tablas si la base de datos no está definida explícitamente en la consulta con un punto antes del nombre de la tabla.
-Esta consulta no se puede realizar cuando se usa el protocolo HTTP, ya que no existe un concepto de sesión.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/misc/) <!--hide-->
diff --git a/docs/es/sql-reference/statements/revoke.md b/docs/es/sql-reference/statements/revoke.md
deleted file mode 100644
index 2c5ee0a40f73..000000000000
--- a/docs/es/sql-reference/statements/revoke.md
+++ /dev/null
@@ -1,50 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: REVOKE
----
-
-# REVOKE {#revoke}
-
-Revoca privilegios de usuarios o roles.
-
-## Sintaxis {#revoke-syntax}
-
-**Revocación de privilegios de usuarios**
-
-``` sql
-REVOKE [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} FROM {user | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user | CURRENT_USER} [,...]
-```
-
-**Revocación de roles de usuarios**
-
-``` sql
-REVOKE [ON CLUSTER cluster_name] [ADMIN OPTION FOR] role [,...] FROM {user | role | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user_name | role_name | CURRENT_USER} [,...]
-```
-
-## Descripci {#revoke-description}
-
-Para revocar algún privilegio, puede usar un privilegio de alcance más amplio que planea revocar. Por ejemplo, si un usuario tiene `SELECT (x,y)` privilegio, administrador puede realizar `REVOKE SELECT(x,y) ...`, o `REVOKE SELECT * ...` o incluso `REVOKE ALL PRIVILEGES ...` consulta para revocar este privilegio.
-
-### Revocaciones parciales {#partial-revokes-dscr}
-
-Puede revocar una parte de un privilegio. Por ejemplo, si un usuario tiene `SELECT *.*` privilegio puede revocarle un privilegio para leer datos de alguna tabla o una base de datos.
-
-## Ejemplos {#revoke-example}
-
-Otorgue el `john` cuenta de usuario con un privilegio para seleccionar de todas las bases de datos `accounts` una:
-
-``` sql
-GRANT SELECT ON *.* TO john;
-REVOKE SELECT ON accounts.* FROM john;
-```
-
-Otorgue el `mira` cuenta de usuario con un privilegio para seleccionar entre todas las columnas `accounts.staff` excepción de la tabla `wage` una.
-
-``` sql
-GRANT SELECT ON accounts.staff TO mira;
-REVOKE SELECT(wage) ON accounts.staff FROM mira;
-```
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/operations/settings/settings/) ##}
diff --git a/docs/es/sql-reference/statements/select/array-join.md b/docs/es/sql-reference/statements/select/array-join.md
deleted file mode 100644
index d0393dce76ae..000000000000
--- a/docs/es/sql-reference/statements/select/array-join.md
+++ /dev/null
@@ -1,282 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# ARRAY JOIN Cláusula {#select-array-join-clause}
-
-Es una operación común para las tablas que contienen una columna de matriz para producir una nueva tabla que tiene una columna con cada elemento de matriz individual de esa columna inicial, mientras que los valores de otras columnas se duplican. Este es el caso básico de lo que `ARRAY JOIN` cláusula hace.
-
-Su nombre proviene del hecho de que se puede considerar como una ejecución `JOIN` con una matriz o estructura de datos anidada. La intención es similar a la [arrayJoin](../../functions/array-join.md#functions_arrayjoin) función, pero la funcionalidad de la cláusula es más amplia.
-
-Sintaxis:
-
-``` sql
-SELECT <expr_list>
-FROM <left_subquery>
-[LEFT] ARRAY JOIN <array>
-[WHERE|PREWHERE <expr>]
-...
-```
-
-Solo puede especificar uno `ARRAY JOIN` cláusula en una `SELECT` consulta.
-
-Tipos admitidos de `ARRAY JOIN` se enumeran a continuación:
-
--   `ARRAY JOIN` - En el caso base, las matrices vacías no se incluyen en el resultado de `JOIN`.
--   `LEFT ARRAY JOIN` - El resultado de `JOIN` contiene filas con matrices vacías. El valor de una matriz vacía se establece en el valor predeterminado para el tipo de elemento de matriz (normalmente 0, cadena vacía o NULL).
-
-## Ejemplos básicos de ARRAY JOIN {#basic-array-join-examples}
-
-Los siguientes ejemplos demuestran el uso de la `ARRAY JOIN` y `LEFT ARRAY JOIN` clausula. Vamos a crear una tabla con un [Matriz](../../../sql-reference/data-types/array.md) escriba la columna e inserte valores en ella:
-
-``` sql
-CREATE TABLE arrays_test
-(
-    s String,
-    arr Array(UInt8)
-) ENGINE = Memory;
-
-INSERT INTO arrays_test
-VALUES ('Hello', [1,2]), ('World', [3,4,5]), ('Goodbye', []);
-```
-
-``` text
-┌─s───────────┬─arr─────┐
-│ Hello       │ [1,2]   │
-│ World       │ [3,4,5] │
-│ Goodbye     │ []      │
-└─────────────┴─────────┘
-```
-
-El siguiente ejemplo utiliza el `ARRAY JOIN` clausula:
-
-``` sql
-SELECT s, arr
-FROM arrays_test
-ARRAY JOIN arr;
-```
-
-``` text
-┌─s─────┬─arr─┐
-│ Hello │   1 │
-│ Hello │   2 │
-│ World │   3 │
-│ World │   4 │
-│ World │   5 │
-└───────┴─────┘
-```
-
-El siguiente ejemplo utiliza el `LEFT ARRAY JOIN` clausula:
-
-``` sql
-SELECT s, arr
-FROM arrays_test
-LEFT ARRAY JOIN arr;
-```
-
-``` text
-┌─s───────────┬─arr─┐
-│ Hello       │   1 │
-│ Hello       │   2 │
-│ World       │   3 │
-│ World       │   4 │
-│ World       │   5 │
-│ Goodbye     │   0 │
-└─────────────┴─────┘
-```
-
-## Uso de alias {#using-aliases}
-
-Se puede especificar un alias para una matriz en el `ARRAY JOIN` clausula. En este caso, este alias puede acceder a un elemento de matriz, pero el nombre original tiene acceso a la matriz en sí. Ejemplo:
-
-``` sql
-SELECT s, arr, a
-FROM arrays_test
-ARRAY JOIN arr AS a;
-```
-
-``` text
-┌─s─────┬─arr─────┬─a─┐
-│ Hello │ [1,2]   │ 1 │
-│ Hello │ [1,2]   │ 2 │
-│ World │ [3,4,5] │ 3 │
-│ World │ [3,4,5] │ 4 │
-│ World │ [3,4,5] │ 5 │
-└───────┴─────────┴───┘
-```
-
-Usando alias, puede realizar `ARRAY JOIN` con una matriz externa. Por ejemplo:
-
-``` sql
-SELECT s, arr_external
-FROM arrays_test
-ARRAY JOIN [1, 2, 3] AS arr_external;
-```
-
-``` text
-┌─s───────────┬─arr_external─┐
-│ Hello       │            1 │
-│ Hello       │            2 │
-│ Hello       │            3 │
-│ World       │            1 │
-│ World       │            2 │
-│ World       │            3 │
-│ Goodbye     │            1 │
-│ Goodbye     │            2 │
-│ Goodbye     │            3 │
-└─────────────┴──────────────┘
-```
-
-Múltiples matrices se pueden separar por comas en el `ARRAY JOIN` clausula. En este caso, `JOIN` se realiza con ellos simultáneamente (la suma directa, no el producto cartesiano). Tenga en cuenta que todas las matrices deben tener el mismo tamaño. Ejemplo:
-
-``` sql
-SELECT s, arr, a, num, mapped
-FROM arrays_test
-ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num, arrayMap(x -> x + 1, arr) AS mapped;
-```
-
-``` text
-┌─s─────┬─arr─────┬─a─┬─num─┬─mapped─┐
-│ Hello │ [1,2]   │ 1 │   1 │      2 │
-│ Hello │ [1,2]   │ 2 │   2 │      3 │
-│ World │ [3,4,5] │ 3 │   1 │      4 │
-│ World │ [3,4,5] │ 4 │   2 │      5 │
-│ World │ [3,4,5] │ 5 │   3 │      6 │
-└───────┴─────────┴───┴─────┴────────┘
-```
-
-El siguiente ejemplo utiliza el [arrayEnumerate](../../../sql-reference/functions/array-functions.md#array_functions-arrayenumerate) función:
-
-``` sql
-SELECT s, arr, a, num, arrayEnumerate(arr)
-FROM arrays_test
-ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num;
-```
-
-``` text
-┌─s─────┬─arr─────┬─a─┬─num─┬─arrayEnumerate(arr)─┐
-│ Hello │ [1,2]   │ 1 │   1 │ [1,2]               │
-│ Hello │ [1,2]   │ 2 │   2 │ [1,2]               │
-│ World │ [3,4,5] │ 3 │   1 │ [1,2,3]             │
-│ World │ [3,4,5] │ 4 │   2 │ [1,2,3]             │
-│ World │ [3,4,5] │ 5 │   3 │ [1,2,3]             │
-└───────┴─────────┴───┴─────┴─────────────────────┘
-```
-
-## ARRAY JOIN con estructura de datos anidada {#array-join-with-nested-data-structure}
-
-`ARRAY JOIN` también funciona con [estructuras de datos anidados](../../../sql-reference/data-types/nested-data-structures/nested.md):
-
-``` sql
-CREATE TABLE nested_test
-(
-    s String,
-    nest Nested(
-    x UInt8,
-    y UInt32)
-) ENGINE = Memory;
-
-INSERT INTO nested_test
-VALUES ('Hello', [1,2], [10,20]), ('World', [3,4,5], [30,40,50]), ('Goodbye', [], []);
-```
-
-``` text
-┌─s───────┬─nest.x──┬─nest.y─────┐
-│ Hello   │ [1,2]   │ [10,20]    │
-│ World   │ [3,4,5] │ [30,40,50] │
-│ Goodbye │ []      │ []         │
-└─────────┴─────────┴────────────┘
-```
-
-``` sql
-SELECT s, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN nest;
-```
-
-``` text
-┌─s─────┬─nest.x─┬─nest.y─┐
-│ Hello │      1 │     10 │
-│ Hello │      2 │     20 │
-│ World │      3 │     30 │
-│ World │      4 │     40 │
-│ World │      5 │     50 │
-└───────┴────────┴────────┘
-```
-
-Al especificar nombres de estructuras de datos anidadas en `ARRAY JOIN` el significado es el mismo `ARRAY JOIN` con todos los elementos de la matriz en los que consiste. Los ejemplos se enumeran a continuación:
-
-``` sql
-SELECT s, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN `nest.x`, `nest.y`;
-```
-
-``` text
-┌─s─────┬─nest.x─┬─nest.y─┐
-│ Hello │      1 │     10 │
-│ Hello │      2 │     20 │
-│ World │      3 │     30 │
-│ World │      4 │     40 │
-│ World │      5 │     50 │
-└───────┴────────┴────────┘
-```
-
-Esta variación también tiene sentido:
-
-``` sql
-SELECT s, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN `nest.x`;
-```
-
-``` text
-┌─s─────┬─nest.x─┬─nest.y─────┐
-│ Hello │      1 │ [10,20]    │
-│ Hello │      2 │ [10,20]    │
-│ World │      3 │ [30,40,50] │
-│ World │      4 │ [30,40,50] │
-│ World │      5 │ [30,40,50] │
-└───────┴────────┴────────────┘
-```
-
-Se puede usar un alias para una estructura de datos anidada, con el fin de seleccionar `JOIN` resultado o la matriz de origen. Ejemplo:
-
-``` sql
-SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN nest AS n;
-```
-
-``` text
-┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┐
-│ Hello │   1 │  10 │ [1,2]   │ [10,20]    │
-│ Hello │   2 │  20 │ [1,2]   │ [10,20]    │
-│ World │   3 │  30 │ [3,4,5] │ [30,40,50] │
-│ World │   4 │  40 │ [3,4,5] │ [30,40,50] │
-│ World │   5 │  50 │ [3,4,5] │ [30,40,50] │
-└───────┴─────┴─────┴─────────┴────────────┘
-```
-
-Ejemplo de uso del [arrayEnumerate](../../../sql-reference/functions/array-functions.md#array_functions-arrayenumerate) función:
-
-``` sql
-SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`, num
-FROM nested_test
-ARRAY JOIN nest AS n, arrayEnumerate(`nest.x`) AS num;
-```
-
-``` text
-┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┬─num─┐
-│ Hello │   1 │  10 │ [1,2]   │ [10,20]    │   1 │
-│ Hello │   2 │  20 │ [1,2]   │ [10,20]    │   2 │
-│ World │   3 │  30 │ [3,4,5] │ [30,40,50] │   1 │
-│ World │   4 │  40 │ [3,4,5] │ [30,40,50] │   2 │
-│ World │   5 │  50 │ [3,4,5] │ [30,40,50] │   3 │
-└───────┴─────┴─────┴─────────┴────────────┴─────┘
-```
-
-## Detalles de implementación {#implementation-details}
-
-El orden de ejecución de la consulta se optimiza cuando se ejecuta `ARRAY JOIN`. Aunque `ARRAY JOIN` debe especificarse siempre antes de la [WHERE](where.md)/[PREWHERE](prewhere.md) cláusula en una consulta, técnicamente se pueden realizar en cualquier orden, a menos que resultado de `ARRAY JOIN` se utiliza para filtrar. El optimizador de consultas controla el orden de procesamiento.
diff --git a/docs/es/sql-reference/statements/select/distinct.md b/docs/es/sql-reference/statements/select/distinct.md
deleted file mode 100644
index bf90f9f7d60b..000000000000
--- a/docs/es/sql-reference/statements/select/distinct.md
+++ /dev/null
@@ -1,63 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Cláusula DISTINCT {#select-distinct}
-
-Si `SELECT DISTINCT` se especifica, sólo las filas únicas permanecerán en un resultado de consulta. Por lo tanto, solo quedará una sola fila de todos los conjuntos de filas completamente coincidentes en el resultado.
-
-## Procesamiento nulo {#null-processing}
-
-`DISTINCT` trabaja con [NULL](../../syntax.md#null-literal) como si `NULL` Era un valor específico, y `NULL==NULL`. En otras palabras, en el `DISTINCT` resultados, diferentes combinaciones con `NULL` se producen sólo una vez. Se diferencia de `NULL` procesamiento en la mayoría de los otros contextos.
-
-## Alternativa {#alternatives}
-
-Es posible obtener el mismo resultado aplicando [GROUP BY](group-by.md) en el mismo conjunto de valores especificados como `SELECT` cláusula, sin utilizar ninguna función agregada. Pero hay pocas diferencias de `GROUP BY` enfoque:
-
--   `DISTINCT` se puede aplicar junto con `GROUP BY`.
--   Cuando [ORDER BY](order-by.md) se omite y [LIMIT](limit.md) se define, la consulta deja de ejecutarse inmediatamente después de que se haya leído el número requerido de filas diferentes.
--   Los bloques de datos se generan a medida que se procesan, sin esperar a que finalice la ejecución de toda la consulta.
-
-## Limitacion {#limitations}
-
-`DISTINCT` no se admite si `SELECT` tiene al menos una columna de matriz.
-
-## Ejemplos {#examples}
-
-ClickHouse admite el uso de `DISTINCT` y `ORDER BY` para diferentes columnas en una consulta. El `DISTINCT` cláusula se ejecuta antes de `ORDER BY` clausula.
-
-Tabla de ejemplo:
-
-``` text
-┌─a─┬─b─┐
-│ 2 │ 1 │
-│ 1 │ 2 │
-│ 3 │ 3 │
-│ 2 │ 4 │
-└───┴───┘
-```
-
-Al seleccionar datos con el `SELECT DISTINCT a FROM t1 ORDER BY b ASC` consulta, obtenemos el siguiente resultado:
-
-``` text
-┌─a─┐
-│ 2 │
-│ 1 │
-│ 3 │
-└───┘
-```
-
-Si cambiamos la dirección de clasificación `SELECT DISTINCT a FROM t1 ORDER BY b DESC`, obtenemos el siguiente resultado:
-
-``` text
-┌─a─┐
-│ 3 │
-│ 1 │
-│ 2 │
-└───┘
-```
-
-Fila `2, 4` se cortó antes de clasificar.
-
-Tenga en cuenta esta especificidad de implementación al programar consultas.
diff --git a/docs/es/sql-reference/statements/select/format.md b/docs/es/sql-reference/statements/select/format.md
deleted file mode 100644
index 00ca29ec7558..000000000000
--- a/docs/es/sql-reference/statements/select/format.md
+++ /dev/null
@@ -1,18 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# FORMAT Cláusula {#format-clause}
-
-ClickHouse soporta una amplia gama de [formatos de serialización](../../../interfaces/formats.md) que se pueden usar en los resultados de consultas, entre otras cosas. Hay varias formas de elegir un formato para `SELECT` salida, uno de ellos es especificar `FORMAT format` al final de la consulta para obtener los datos resultantes en cualquier formato específico.
-
-El formato específico se puede utilizar ya sea por conveniencia, integración con otros sistemas o ganancia de rendimiento.
-
-## Formato predeterminado {#default-format}
-
-Si el `FORMAT` se omite la cláusula, se utiliza el formato predeterminado, que depende tanto de la configuración como de la interfaz utilizada para acceder al servidor ClickHouse. Para el [Interfaz HTTP](../../../interfaces/http.md) y el [cliente de línea de comandos](../../../interfaces/cli.md) En el modo batch, el formato predeterminado es `TabSeparated`. Para el cliente de línea de comandos en modo interactivo, el formato predeterminado es `PrettyCompact` (produce tablas compactas legibles por humanos).
-
-## Detalles de implementación {#implementation-details}
-
-Cuando se utiliza el cliente de línea de comandos, los datos siempre se pasan a través de la red en un formato interno eficiente (`Native`). El cliente interpreta independientemente el `FORMAT` cláusula de la consulta y formatea los datos en sí (aliviando así la red y el servidor de la carga adicional).
diff --git a/docs/es/sql-reference/statements/select/from.md b/docs/es/sql-reference/statements/select/from.md
deleted file mode 100644
index 718df7b952c5..000000000000
--- a/docs/es/sql-reference/statements/select/from.md
+++ /dev/null
@@ -1,44 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Cláusula FROM {#select-from}
-
-El `FROM` cláusula especifica la fuente de la que se leen los datos:
-
--   [Tabla](../../../engines/table-engines/index.md)
--   [Subconsultas](index.md) {## TODO: mejor enlace ##}
--   [Función de la tabla](../../table-functions/index.md#table-functions)
-
-[JOIN](join.md) y [ARRAY JOIN](array-join.md) también pueden utilizarse para ampliar la funcionalidad del `FROM` clausula.
-
-La subconsulta es otra `SELECT` consulta que se puede especificar entre paréntesis dentro `FROM` clausula.
-
-`FROM` cláusula puede contener múltiples fuentes de datos, separadas por comas, que es equivalente a realizar [CROSS JOIN](join.md) en ellos.
-
-## Modificador FINAL {#select-from-final}
-
-Cuando `FINAL` se especifica, ClickHouse fusiona completamente los datos antes de devolver el resultado y, por lo tanto, realiza todas las transformaciones de datos que ocurren durante las fusiones para el motor de tabla dado.
-
-Es aplicable cuando se seleccionan datos de tablas que utilizan [Método de codificación de datos:](../../../engines/table-engines/mergetree-family/mergetree.md)- familia del motor (excepto `GraphiteMergeTree`). También soportado para:
-
--   [Replicado](../../../engines/table-engines/mergetree-family/replication.md) versiones de `MergeTree` motor.
--   [Vista](../../../engines/table-engines/special/view.md), [Búfer](../../../engines/table-engines/special/buffer.md), [Distribuido](../../../engines/table-engines/special/distributed.md), y [Método de codificación de datos:](../../../engines/table-engines/special/materializedview.md) motores que funcionan sobre otros motores, siempre que se hayan creado sobre `MergeTree`-mesas de motor.
-
-### Inconveniente {#drawbacks}
-
-Consultas que usan `FINAL` se ejecutan no tan rápido como consultas similares que no lo hacen, porque:
-
--   La consulta se ejecuta en un solo subproceso y los datos se combinan durante la ejecución de la consulta.
--   Consultas con `FINAL` leer columnas de clave primaria además de las columnas especificadas en la consulta.
-
-**En la mayoría de los casos, evite usar `FINAL`.** El enfoque común es utilizar diferentes consultas que asumen los procesos en segundo plano de la `MergeTree` el motor aún no ha sucedido y tratar con él mediante la aplicación de agregación (por ejemplo, para descartar duplicados). {## TODO: ejemplos ##}
-
-## Detalles de implementación {#implementation-details}
-
-Si el `FROM` se omite la cláusula, los datos se leerán desde el `system.one` tabla.
-El `system.one` table contiene exactamente una fila (esta tabla cumple el mismo propósito que la tabla DUAL que se encuentra en otros DBMS).
-
-Para ejecutar una consulta, todas las columnas enumeradas en la consulta se extraen de la tabla adecuada. Las columnas no necesarias para la consulta externa se eliminan de las subconsultas.
-Si una consulta no muestra ninguna columnas (por ejemplo, `SELECT count() FROM t`), alguna columna se extrae de la tabla de todos modos (se prefiere la más pequeña), para calcular el número de filas.
diff --git a/docs/es/sql-reference/statements/select/group-by.md b/docs/es/sql-reference/statements/select/group-by.md
deleted file mode 100644
index d6be9dc06e3b..000000000000
--- a/docs/es/sql-reference/statements/select/group-by.md
+++ /dev/null
@@ -1,132 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# GRUPO POR Cláusula {#select-group-by-clause}
-
-`GROUP BY` cláusula cambia el `SELECT` consulta en un modo de agregación, que funciona de la siguiente manera:
-
--   `GROUP BY` clause contiene una lista de expresiones (o una sola expresión, que se considera la lista de longitud uno). Esta lista actúa como un “grouping key”, mientras que cada expresión individual será referida como un “key expressions”.
--   Todas las expresiones en el [SELECT](index.md), [HAVING](having.md), y [ORDER BY](order-by.md) clausula **deber** se calculará basándose en expresiones clave **o** en [funciones agregadas](../../../sql-reference/aggregate-functions/index.md) sobre expresiones no clave (incluidas columnas simples). En otras palabras, cada columna seleccionada de la tabla debe usarse en una expresión clave o dentro de una función agregada, pero no en ambas.
--   Resultado de la agregación `SELECT` la consulta contendrá tantas filas como valores únicos de “grouping key” en la tabla de origen. Por lo general, esto reduce significativamente el recuento de filas, a menudo en órdenes de magnitud, pero no necesariamente: el recuento de filas se mantiene igual si todo “grouping key” los valores fueron distintos.
-
-!!! note "Nota"
-    Hay una forma adicional de ejecutar la agregación sobre una tabla. Si una consulta contiene columnas de tabla solo dentro de funciones agregadas, el `GROUP BY clause` se puede omitir, y se asume la agregación por un conjunto vacío de claves. Tales consultas siempre devuelven exactamente una fila.
-
-## Procesamiento NULL {#null-processing}
-
-Para agrupar, ClickHouse interpreta [NULL](../../syntax.md#null-literal) como valor, y `NULL==NULL`. Se diferencia de `NULL` procesamiento en la mayoría de los otros contextos.
-
-Aquí hay un ejemplo para mostrar lo que esto significa.
-
-Supongamos que tienes esta tabla:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │    2 │
-│ 2 │ ᴺᵁᴸᴸ │
-│ 3 │    2 │
-│ 3 │    3 │
-│ 3 │ ᴺᵁᴸᴸ │
-└───┴──────┘
-```
-
-Consulta `SELECT sum(x), y FROM t_null_big GROUP BY y` resultados en:
-
-``` text
-┌─sum(x)─┬────y─┐
-│      4 │    2 │
-│      3 │    3 │
-│      5 │ ᴺᵁᴸᴸ │
-└────────┴──────┘
-```
-
-Se puede ver que `GROUP BY` para `y = NULL` resumir `x` como si `NULL` es este valor.
-
-Si pasa varias teclas a `GROUP BY` el resultado le dará todas las combinaciones de la selección, como si `NULL` fueron un valor específico.
-
-## CON TOTALS Modificador {#with-totals-modifier}
-
-Si el `WITH TOTALS` se especifica el modificador, se calculará otra fila. Esta fila tendrá columnas clave que contienen valores predeterminados (zeros o líneas vacías) y columnas de funciones agregadas con los valores calculados en todas las filas (el “total” valor).
-
-Esta fila adicional solo se produce en `JSON*`, `TabSeparated*`, y `Pretty*` formatos, por separado de las otras filas:
-
--   En `JSON*` formatos, esta fila se muestra como una ‘totals’ campo.
--   En `TabSeparated*` formatea, la fila viene después del resultado principal, precedida por una fila vacía (después de los otros datos).
--   En `Pretty*` formatea, la fila se muestra como una tabla separada después del resultado principal.
--   En los otros formatos no está disponible.
-
-`WITH TOTALS` se puede ejecutar de diferentes maneras cuando HAVING está presente. El comportamiento depende de la ‘totals_mode’ configuración.
-
-### Configuración del procesamiento de totales {#configuring-totals-processing}
-
-Predeterminada, `totals_mode = 'before_having'`. En este caso, ‘totals’ se calcula en todas las filas, incluidas las que no pasan por HAVING y `max_rows_to_group_by`.
-
-Las otras alternativas incluyen solo las filas que pasan por HAVING en ‘totals’, y comportarse de manera diferente con el ajuste `max_rows_to_group_by` y `group_by_overflow_mode = 'any'`.
-
-`after_having_exclusive` – Don't include rows that didn't pass through `max_rows_to_group_by`. En otras palabras, ‘totals’ tendrá menos o el mismo número de filas que si `max_rows_to_group_by` se omitieron.
-
-`after_having_inclusive` – Include all the rows that didn't pass through ‘max_rows_to_group_by’ en ‘totals’. En otras palabras, ‘totals’ tendrá más o el mismo número de filas como lo haría si `max_rows_to_group_by` se omitieron.
-
-`after_having_auto` – Count the number of rows that passed through HAVING. If it is more than a certain amount (by default, 50%), include all the rows that didn't pass through ‘max_rows_to_group_by’ en ‘totals’. De lo contrario, no los incluya.
-
-`totals_auto_threshold` – By default, 0.5. The coefficient for `after_having_auto`.
-
-Si `max_rows_to_group_by` y `group_by_overflow_mode = 'any'` no se utilizan, todas las variaciones de `after_having` son los mismos, y se puede utilizar cualquiera de ellos (por ejemplo, `after_having_auto`).
-
-Puede usar WITH TOTALS en subconsultas, incluidas las subconsultas en la cláusula JOIN (en este caso, se combinan los valores totales respectivos).
-
-## Ejemplos {#examples}
-
-Ejemplo:
-
-``` sql
-SELECT
-    count(),
-    median(FetchTiming > 60 ? 60 : FetchTiming),
-    count() - sum(Refresh)
-FROM hits
-```
-
-Sin embargo, a diferencia del SQL estándar, si la tabla no tiene ninguna fila (o no hay ninguna, o no hay ninguna después de usar WHERE para filtrar), se devuelve un resultado vacío, y no el resultado de una de las filas que contienen los valores iniciales de las funciones agregadas.
-
-A diferencia de MySQL (y conforme a SQL estándar), no puede obtener algún valor de alguna columna que no esté en una función clave o agregada (excepto expresiones constantes). Para evitar esto, puede usar el ‘any’ función de agregado (obtener el primer valor encontrado) o ‘min/max’.
-
-Ejemplo:
-
-``` sql
-SELECT
-    domainWithoutWWW(URL) AS domain,
-    count(),
-    any(Title) AS title -- getting the first occurred page header for each domain.
-FROM hits
-GROUP BY domain
-```
-
-Para cada valor de clave diferente encontrado, GROUP BY calcula un conjunto de valores de función agregados.
-
-GROUP BY no se admite para columnas de matriz.
-
-No se puede especificar una constante como argumentos para funciones agregadas. Ejemplo: sum(1). En lugar de esto, puedes deshacerte de la constante. Ejemplo: `count()`.
-
-## Detalles de implementación {#implementation-details}
-
-La agregación es una de las características más importantes de un DBMS orientado a columnas, y por lo tanto su implementación es una de las partes más optimizadas de ClickHouse. De forma predeterminada, la agregación se realiza en la memoria utilizando una tabla hash. Tiene más de 40 especializaciones que se eligen automáticamente dependiendo de “grouping key” tipos de datos.
-
-### GROUP BY en memoria externa {#select-group-by-in-external-memory}
-
-Puede habilitar el volcado de datos temporales en el disco para restringir el uso de memoria durante `GROUP BY`.
-El [max_bytes_before_external_group_by](../../../operations/settings/settings.md#settings-max_bytes_before_external_group_by) determina el umbral de consumo de RAM para el dumping `GROUP BY` datos temporales al sistema de archivos. Si se establece en 0 (el valor predeterminado), está deshabilitado.
-
-Cuando se utiliza `max_bytes_before_external_group_by`, le recomendamos que establezca `max_memory_usage` aproximadamente el doble de alto. Esto es necesario porque hay dos etapas para la agregación: leer los datos y formar datos intermedios (1) y fusionar los datos intermedios (2). El volcado de datos al sistema de archivos solo puede ocurrir durante la etapa 1. Si los datos temporales no se volcaron, entonces la etapa 2 puede requerir hasta la misma cantidad de memoria que en la etapa 1.
-
-Por ejemplo, si [Método de codificación de datos:](../../../operations/settings/settings.md#settings_max_memory_usage) se estableció en 10000000000 y desea usar agregación externa, tiene sentido establecer `max_bytes_before_external_group_by` a 10000000000, y `max_memory_usage` a 20000000000. Cuando se activa la agregación externa (si hubo al menos un volcado de datos temporales), el consumo máximo de RAM es solo un poco más que `max_bytes_before_external_group_by`.
-
-Con el procesamiento de consultas distribuidas, la agregación externa se realiza en servidores remotos. Para que el servidor solicitante use solo una pequeña cantidad de RAM, establezca `distributed_aggregation_memory_efficient` a 1.
-
-Al fusionar datos en el disco, así como al fusionar resultados de servidores remotos cuando `distributed_aggregation_memory_efficient` la configuración está habilitada, consume hasta `1/256 * the_number_of_threads` de la cantidad total de RAM.
-
-Cuando la agregación externa está habilitada, si `max_bytes_before_external_group_by` of data (i.e. data was not flushed), the query runs just as fast as without external aggregation. If any temporary data was flushed, the run time will be several times longer (approximately three times).
-
-Si usted tiene un [ORDER BY](order-by.md) con un [LIMIT](limit.md) despues `GROUP BY`, entonces la cantidad de RAM usada depende de la cantidad de datos en `LIMIT`, no en toda la tabla. Pero si el `ORDER BY` no tiene `LIMIT`, no se olvide de habilitar la clasificación externa (`max_bytes_before_external_sort`).
diff --git a/docs/es/sql-reference/statements/select/having.md b/docs/es/sql-reference/statements/select/having.md
deleted file mode 100644
index ce65b95e54ff..000000000000
--- a/docs/es/sql-reference/statements/select/having.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Cláusula HAVING {#having-clause}
-
-Permite filtrar los resultados de agregación producidos por [GROUP BY](group-by.md). Es similar a la [WHERE](where.md) cláusula, pero la diferencia es que `WHERE` se realiza antes de la agregación, mientras que `HAVING` se realiza después de eso.
-
-Es posible hacer referencia a los resultados de la agregación de `SELECT` cláusula en `HAVING` cláusula por su alias. Alternativamente, `HAVING` cláusula puede filtrar los resultados de agregados adicionales que no se devuelven en los resultados de la consulta.
-
-## Limitacion {#limitations}
-
-`HAVING` no se puede usar si no se realiza la agregación. Utilizar `WHERE` en su lugar.
diff --git a/docs/es/sql-reference/statements/select/index.md b/docs/es/sql-reference/statements/select/index.md
deleted file mode 100644
index 653f737b1d0b..000000000000
--- a/docs/es/sql-reference/statements/select/index.md
+++ /dev/null
@@ -1,158 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: SELECT
----
-
-# SELECCIONAR consultas Sintaxis {#select-queries-syntax}
-
-`SELECT` realiza la recuperación de datos.
-
-``` sql
-[WITH expr_list|(subquery)]
-SELECT [DISTINCT] expr_list
-[FROM [db.]table | (subquery) | table_function] [FINAL]
-[SAMPLE sample_coeff]
-[ARRAY JOIN ...]
-[GLOBAL] [ANY|ALL|ASOF] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI] JOIN (subquery)|table (ON <expr_list>)|(USING <column_list>)
-[PREWHERE expr]
-[WHERE expr]
-[GROUP BY expr_list] [WITH TOTALS]
-[HAVING expr]
-[ORDER BY expr_list] [WITH FILL] [FROM expr] [TO expr] [STEP expr] 
-[LIMIT [offset_value, ]n BY columns]
-[LIMIT [n, ]m] [WITH TIES]
-[UNION ALL ...]
-[INTO OUTFILE filename]
-[FORMAT format]
-```
-
-Todas las cláusulas son opcionales, excepto la lista requerida de expresiones inmediatamente después `SELECT` que está cubierto con más detalle [debajo](#select-clause).
-
-Los detalles de cada cláusula opcional se cubren en secciones separadas, que se enumeran en el mismo orden en que se ejecutan:
-
--   [CON cláusula](with.md)
--   [Cláusula DISTINCT](distinct.md)
--   [Cláusula FROM](from.md)
--   [Cláusula SAMPLE](sample.md)
--   [Cláusula JOIN](join.md)
--   [Cláusula PREWHERE](prewhere.md)
--   [Cláusula WHERE](where.md)
--   [Cláusula GROUP BY](group-by.md)
--   [Cláusula LIMIT BY](limit-by.md)
--   [Cláusula HAVING](having.md)
--   [Cláusula SELECT](#select-clause)
--   [Cláusula LIMIT](limit.md)
--   [UNION ALL cláusula](union.md)
-
-## SELECT Cláusula {#select-clause}
-
-[Expresiones](../../syntax.md#syntax-expressions) especificado en el `SELECT` cláusula se calculan después de que todas las operaciones en las cláusulas descritas anteriormente hayan finalizado. Estas expresiones funcionan como si se aplicaran a filas separadas en el resultado. Si las expresiones en el `SELECT` cláusula contiene funciones agregadas, a continuación, ClickHouse procesa funciones agregadas y expresiones utilizadas como sus argumentos durante el [GROUP BY](group-by.md) agregación.
-
-Si desea incluir todas las columnas en el resultado, use el asterisco (`*`) simbolo. Por ejemplo, `SELECT * FROM ...`.
-
-Para hacer coincidir algunas columnas en el resultado con un [Re2](https://en.wikipedia.org/wiki/RE2_(software)) expresión regular, puede utilizar el `COLUMNS` expresion.
-
-``` sql
-COLUMNS('regexp')
-```
-
-Por ejemplo, considere la tabla:
-
-``` sql
-CREATE TABLE default.col_names (aa Int8, ab Int8, bc Int8) ENGINE = TinyLog
-```
-
-La siguiente consulta selecciona datos de todas las columnas que contienen `a` símbolo en su nombre.
-
-``` sql
-SELECT COLUMNS('a') FROM col_names
-```
-
-``` text
-┌─aa─┬─ab─┐
-│  1 │  1 │
-└────┴────┘
-```
-
-Las columnas seleccionadas no se devuelven en orden alfabético.
-
-Puede utilizar múltiples `COLUMNS` expresiones en una consulta y aplicarles funciones.
-
-Por ejemplo:
-
-``` sql
-SELECT COLUMNS('a'), COLUMNS('c'), toTypeName(COLUMNS('c')) FROM col_names
-```
-
-``` text
-┌─aa─┬─ab─┬─bc─┬─toTypeName(bc)─┐
-│  1 │  1 │  1 │ Int8           │
-└────┴────┴────┴────────────────┘
-```
-
-Cada columna devuelta por el `COLUMNS` expresión se pasa a la función como un argumento separado. También puede pasar otros argumentos a la función si los admite. Tenga cuidado al usar funciones. Si una función no admite la cantidad de argumentos que le ha pasado, ClickHouse lanza una excepción.
-
-Por ejemplo:
-
-``` sql
-SELECT COLUMNS('a') + COLUMNS('c') FROM col_names
-```
-
-``` text
-Received exception from server (version 19.14.1):
-Code: 42. DB::Exception: Received from localhost:9000. DB::Exception: Number of arguments for function plus doesn't match: passed 3, should be 2.
-```
-
-En este ejemplo, `COLUMNS('a')` devuelve dos columnas: `aa` y `ab`. `COLUMNS('c')` devuelve el `bc` columna. El `+` el operador no puede aplicar a 3 argumentos, por lo que ClickHouse lanza una excepción con el mensaje relevante.
-
-Columnas que coinciden con el `COLUMNS` expresión puede tener diferentes tipos de datos. Si `COLUMNS` no coincide con ninguna columna y es la única expresión en `SELECT`, ClickHouse lanza una excepción.
-
-### Asterisco {#asterisk}
-
-Puede poner un asterisco en cualquier parte de una consulta en lugar de una expresión. Cuando se analiza la consulta, el asterisco se expande a una lista de todas las columnas de la tabla `MATERIALIZED` y `ALIAS` columna). Solo hay unos pocos casos en los que se justifica el uso de un asterisco:
-
--   Al crear un volcado de tabla.
--   Para tablas que contienen solo unas pocas columnas, como las tablas del sistema.
--   Para obtener información sobre qué columnas están en una tabla. En este caso, establezca `LIMIT 1`. Pero es mejor usar el `DESC TABLE` consulta.
--   Cuando hay una filtración fuerte en un pequeño número de columnas usando `PREWHERE`.
--   En subconsultas (ya que las columnas que no son necesarias para la consulta externa están excluidas de las subconsultas).
-
-En todos los demás casos, no recomendamos usar el asterisco, ya que solo le da los inconvenientes de un DBMS columnar en lugar de las ventajas. En otras palabras, no se recomienda usar el asterisco.
-
-### Valores extremos {#extreme-values}
-
-Además de los resultados, también puede obtener valores mínimos y máximos para las columnas de resultados. Para hacer esto, establezca el **extremo** a 1. Los mínimos y máximos se calculan para tipos numéricos, fechas y fechas con horas. Para otras columnas, se generan los valores predeterminados.
-
-An extra two rows are calculated – the minimums and maximums, respectively. These extra two rows are output in `JSON*`, `TabSeparated*`, y `Pretty*` [formato](../../../interfaces/formats.md), separado de las otras filas. No se emiten para otros formatos.
-
-En `JSON*` los valores extremos se emiten en un formato separado. ‘extremes’ campo. En `TabSeparated*` , la fila viene después del resultado principal, y después de ‘totals’ si está presente. Está precedido por una fila vacía (después de los otros datos). En `Pretty*` formatea, la fila se muestra como una tabla separada después del resultado principal, y después de `totals` si está presente.
-
-Los valores extremos se calculan para las filas anteriores `LIMIT`, pero después `LIMIT BY`. Sin embargo, cuando se usa `LIMIT offset, size`, las filas antes `offset` están incluidos en `extremes`. En las solicitudes de secuencia, el resultado también puede incluir un pequeño número de filas que pasaron por `LIMIT`.
-
-### Nota {#notes}
-
-Puedes usar sinónimos (`AS` aliases) en cualquier parte de una consulta.
-
-El `GROUP BY` y `ORDER BY` las cláusulas no admiten argumentos posicionales. Esto contradice MySQL, pero se ajusta al SQL estándar. Por ejemplo, `GROUP BY 1, 2` will be interpreted as grouping by constants (i.e. aggregation of all rows into one).
-
-## Detalles de implementación {#implementation-details}
-
-Si la consulta omite el `DISTINCT`, `GROUP BY` y `ORDER BY` cláusulas y el `IN` y `JOIN` subconsultas, la consulta se procesará por completo, utilizando O (1) cantidad de RAM. De lo contrario, la consulta podría consumir mucha RAM si no se especifican las restricciones adecuadas:
-
--   `max_memory_usage`
--   `max_rows_to_group_by`
--   `max_rows_to_sort`
--   `max_rows_in_distinct`
--   `max_bytes_in_distinct`
--   `max_rows_in_set`
--   `max_bytes_in_set`
--   `max_rows_in_join`
--   `max_bytes_in_join`
--   `max_bytes_before_external_sort`
--   `max_bytes_before_external_group_by`
-
-Para obtener más información, consulte la sección “Settings”. Es posible utilizar la clasificación externa (guardar tablas temporales en un disco) y la agregación externa.
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/sql-reference/statements/select/) ##}
diff --git a/docs/es/sql-reference/statements/select/into-outfile.md b/docs/es/sql-reference/statements/select/into-outfile.md
deleted file mode 100644
index 70f5687611e2..000000000000
--- a/docs/es/sql-reference/statements/select/into-outfile.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# INTO OUTFILE Cláusula {#into-outfile-clause}
-
-Añadir el `INTO OUTFILE filename` cláusula (donde filename es un literal de cadena) para `SELECT query` para redirigir su salida al archivo especificado en el lado del cliente.
-
-## Detalles de implementación {#implementation-details}
-
--   Esta funcionalidad está disponible en el [cliente de línea de comandos](../../../interfaces/cli.md) y [Sistema abierto.](../../../operations/utilities/clickhouse-local.md#clickhouse-local). Por lo tanto, una consulta enviada a través de [Interfaz HTTP](../../../interfaces/http.md) fallará.
--   La consulta fallará si ya existe un archivo con el mismo nombre de archivo.
--   Predeterminado [formato de salida](../../../interfaces/formats.md) ser `TabSeparated` (como en el modo por lotes de cliente de línea de comandos).
diff --git a/docs/es/sql-reference/statements/select/join.md b/docs/es/sql-reference/statements/select/join.md
deleted file mode 100644
index 4f0c50950085..000000000000
--- a/docs/es/sql-reference/statements/select/join.md
+++ /dev/null
@@ -1,187 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Cláusula JOIN {#select-join}
-
-Join produce una nueva tabla combinando columnas de una o varias tablas utilizando valores comunes a cada una. Es una operación común en bases de datos con soporte SQL, que corresponde a [álgebra relacional](https://en.wikipedia.org/wiki/Relational_algebra#Joins_and_join-like_operators) unir. El caso especial de una combinación de tabla a menudo se conoce como “self-join”.
-
-Sintaxis:
-
-``` sql
-SELECT <expr_list>
-FROM <left_table>
-[GLOBAL] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI|ANY|ASOF] JOIN <right_table>
-(ON <expr_list>)|(USING <column_list>) ...
-```
-
-Expresiones de `ON` y columnas de `USING` cláusula se llaman “join keys”. A menos que se indique lo contrario, join produce un [Producto cartesiano](https://en.wikipedia.org/wiki/Cartesian_product) de filas con coincidencia “join keys”, lo que podría producir resultados con muchas más filas que las tablas de origen.
-
-## Tipos admitidos de JOIN {#select-join-types}
-
-Todo estándar [SQL JOIN](https://en.wikipedia.org/wiki/Join_(SQL)) tipos son compatibles:
-
--   `INNER JOIN`, sólo se devuelven las filas coincidentes.
--   `LEFT OUTER JOIN`, filas no coincidentes de la tabla izquierda se devuelven además de las filas coincidentes.
--   `RIGHT OUTER JOIN`, filas no coincidentes de la tabla izquierda se devuelven además de las filas coincidentes.
--   `FULL OUTER JOIN`, las filas que no coinciden de ambas tablas se devuelven además de las filas coincidentes.
--   `CROSS JOIN`, produce el producto cartesiano de tablas enteras, “join keys” ser **ni** indicado.
-
-`JOIN` sin tipo especificado implica `INNER`. Palabra clave `OUTER` se puede omitir con seguridad. Sintaxis alternativa para `CROSS JOIN` está especificando múltiples tablas en [Cláusula FROM](from.md) separados por comas.
-
-Tipos de unión adicionales disponibles en ClickHouse:
-
--   `LEFT SEMI JOIN` y `RIGHT SEMI JOIN`, una lista blanca en “join keys”, sin producir un producto cartesiano.
--   `LEFT ANTI JOIN` y `RIGHT ANTI JOIN`, una lista negra sobre “join keys”, sin producir un producto cartesiano.
--   `LEFT ANY JOIN`, `RIGHT ANY JOIN` and `INNER ANY JOIN`, partially (for opposite side of `LEFT` and `RIGHT`) or completely (for `INNER` and `FULL`) disables the cartesian product for standard `JOIN` types.
--   `ASOF JOIN` and `LEFT ASOF JOIN`, joining sequences with a non-exact match. `ASOF JOIN` usage is described below.
-
-## Setting {#join-settings}
-
-!!! note "Nota"
-    El valor de rigor predeterminado se puede anular usando [Por favor, introduzca su dirección de correo electrónico](../../../operations/settings/settings.md#settings-join_default_strictness) configuración.
-
-### ASOF JOIN Uso {#asof-join-usage}
-
-`ASOF JOIN` es útil cuando necesita unir registros que no tienen una coincidencia exacta.
-
-Tablas para `ASOF JOIN` debe tener una columna de secuencia ordenada. Esta columna no puede estar sola en una tabla y debe ser uno de los tipos de datos: `UInt32`, `UInt64`, `Float32`, `Float64`, `Date`, y `DateTime`.
-
-Sintaxis `ASOF JOIN ... ON`:
-
-``` sql
-SELECT expressions_list
-FROM table_1
-ASOF LEFT JOIN table_2
-ON equi_cond AND closest_match_cond
-```
-
-Puede usar cualquier número de condiciones de igualdad y exactamente una condición de coincidencia más cercana. Por ejemplo, `SELECT count() FROM table_1 ASOF LEFT JOIN table_2 ON table_1.a == table_2.b AND table_2.t <= table_1.t`.
-
-Condiciones admitidas para la coincidencia más cercana: `>`, `>=`, `<`, `<=`.
-
-Sintaxis `ASOF JOIN ... USING`:
-
-``` sql
-SELECT expressions_list
-FROM table_1
-ASOF JOIN table_2
-USING (equi_column1, ... equi_columnN, asof_column)
-```
-
-`ASOF JOIN` utilizar `equi_columnX` para unirse a la igualdad y `asof_column` para unirse en el partido más cercano con el `table_1.asof_column >= table_2.asof_column` condición. El `asof_column` columna siempre el último en el `USING` clausula.
-
-Por ejemplo, considere las siguientes tablas:
-
-         table_1                           table_2
-      event   | ev_time | user_id       event   | ev_time | user_id
-    ----------|---------|----------   ----------|---------|----------
-                  ...                               ...
-    event_1_1 |  12:00  |  42         event_2_1 |  11:59  |   42
-                  ...                 event_2_2 |  12:30  |   42
-    event_1_2 |  13:00  |  42         event_2_3 |  13:00  |   42
-                  ...                               ...
-
-`ASOF JOIN` puede tomar la marca de tiempo de un evento de usuario de `table_1` y encontrar un evento en `table_2` donde la marca de tiempo es la más cercana a la marca de tiempo del evento `table_1` correspondiente a la condición de coincidencia más cercana. Los valores de marca de tiempo iguales son los más cercanos si están disponibles. Aquí, el `user_id` se puede utilizar para unirse a la igualdad y el `ev_time` columna se puede utilizar para unirse en el partido más cercano. En nuestro ejemplo, `event_1_1` se puede unir con `event_2_1` y `event_1_2` se puede unir con `event_2_3`, pero `event_2_2` no se puede unir.
-
-!!! note "Nota"
-    `ASOF` unirse es **ni** apoyado en el [Unir](../../../engines/table-engines/special/join.md) motor de mesa.
-
-## Unión distribuida {#global-join}
-
-Hay dos formas de ejecutar la unión que involucra tablas distribuidas:
-
--   Cuando se utiliza una normal `JOIN`, la consulta se envía a servidores remotos. Las subconsultas se ejecutan en cada una de ellas para crear la tabla correcta, y la unión se realiza con esta tabla. En otras palabras, la tabla correcta se forma en cada servidor por separado.
--   Cuando se utiliza `GLOBAL ... JOIN`, primero el servidor requestor ejecuta una subconsulta para calcular la tabla correcta. Esta tabla temporal se pasa a cada servidor remoto y las consultas se ejecutan en ellos utilizando los datos temporales que se transmitieron.
-
-Tenga cuidado al usar `GLOBAL`. Para obtener más información, consulte [Subconsultas distribuidas](../../operators/in.md#select-distributed-subqueries) apartado.
-
-## Recomendaciones de uso {#usage-recommendations}
-
-### Procesamiento de celdas vacías o NULL {#processing-of-empty-or-null-cells}
-
-Al unir tablas, pueden aparecer las celdas vacías. Configuración [Sistema abierto.](../../../operations/settings/settings.md#join_use_nulls) definir cómo ClickHouse llena estas celdas.
-
-Si el `JOIN` las llaves son [NULL](../../data-types/nullable.md) campos, las filas donde al menos una de las claves tiene el valor [NULL](../../../sql-reference/syntax.md#null-literal) no se unen.
-
-### Sintaxis {#syntax}
-
-Las columnas especificadas en `USING` debe tener los mismos nombres en ambas subconsultas, y las otras columnas deben tener un nombre diferente. Puede utilizar alias para cambiar los nombres de las columnas en subconsultas.
-
-El `USING` clause especifica una o más columnas a unir, lo que establece la igualdad de estas columnas. La lista de columnas se establece sin corchetes. No se admiten condiciones de unión más complejas.
-
-### Limitaciones de sintaxis {#syntax-limitations}
-
-Para múltiples `JOIN` cláusulas en una sola `SELECT` consulta:
-
--   Tomando todas las columnas a través de `*` está disponible solo si se unen tablas, no subconsultas.
--   El `PREWHERE` cláusula no está disponible.
-
-Para `ON`, `WHERE`, y `GROUP BY` clausula:
-
--   Las expresiones arbitrarias no se pueden utilizar en `ON`, `WHERE`, y `GROUP BY` cláusulas, pero puede definir una expresión en un `SELECT` cláusula y luego usarla en estas cláusulas a través de un alias.
-
-### Rendimiento {#performance}
-
-Cuando se ejecuta un `JOIN`, no hay optimización del orden de ejecución en relación con otras etapas de la consulta. La combinación (una búsqueda en la tabla de la derecha) se ejecuta antes de filtrar `WHERE` y antes de la agregación.
-
-Cada vez que se ejecuta una consulta `JOIN`, la subconsulta se ejecuta de nuevo porque el resultado no se almacena en caché. Para evitar esto, use el especial [Unir](../../../engines/table-engines/special/join.md) motor de tabla, que es una matriz preparada para unirse que siempre está en RAM.
-
-En algunos casos, es más eficiente de usar [IN](../../operators/in.md) en lugar de `JOIN`.
-
-Si necesita un `JOIN` para unirse a tablas de dimensión (son tablas relativamente pequeñas que contienen propiedades de dimensión, como nombres para campañas publicitarias), un `JOIN` podría no ser muy conveniente debido al hecho de que se vuelve a acceder a la tabla correcta para cada consulta. Para tales casos, hay un “external dictionaries” característica que debe utilizar en lugar de `JOIN`. Para obtener más información, consulte [Diccionarios externos](../../dictionaries/external-dictionaries/external-dicts.md) apartado.
-
-### Limitaciones de memoria {#memory-limitations}
-
-De forma predeterminada, ClickHouse usa el [hash unirse](https://en.wikipedia.org/wiki/Hash_join) algoritmo. ClickHouse toma el `<right_table>` y crea una tabla hash para ello en RAM. Después de algún umbral de consumo de memoria, ClickHouse vuelve a fusionar el algoritmo de unión.
-
-Si necesita restringir el consumo de memoria de la operación de unión, use la siguiente configuración:
-
--   [Método de codificación de datos:](../../../operations/settings/query-complexity.md#settings-max_rows_in_join) — Limits number of rows in the hash table.
--   [Método de codificación de datos:](../../../operations/settings/query-complexity.md#settings-max_bytes_in_join) — Limits size of the hash table.
-
-Cuando se alcanza cualquiera de estos límites, ClickHouse actúa como el [join_overflow_mode](../../../operations/settings/query-complexity.md#settings-join_overflow_mode) configuración instruye.
-
-## Ejemplos {#examples}
-
-Ejemplo:
-
-``` sql
-SELECT
-    CounterID,
-    hits,
-    visits
-FROM
-(
-    SELECT
-        CounterID,
-        count() AS hits
-    FROM test.hits
-    GROUP BY CounterID
-) ANY LEFT JOIN
-(
-    SELECT
-        CounterID,
-        sum(Sign) AS visits
-    FROM test.visits
-    GROUP BY CounterID
-) USING CounterID
-ORDER BY hits DESC
-LIMIT 10
-```
-
-``` text
-┌─CounterID─┬───hits─┬─visits─┐
-│   1143050 │ 523264 │  13665 │
-│    731962 │ 475698 │ 102716 │
-│    722545 │ 337212 │ 108187 │
-│    722889 │ 252197 │  10547 │
-│   2237260 │ 196036 │   9522 │
-│  23057320 │ 147211 │   7689 │
-│    722818 │  90109 │  17847 │
-│     48221 │  85379 │   4652 │
-│  19762435 │  77807 │   7026 │
-│    722884 │  77492 │  11056 │
-└───────────┴────────┴────────┘
-```
diff --git a/docs/es/sql-reference/statements/select/limit-by.md b/docs/es/sql-reference/statements/select/limit-by.md
deleted file mode 100644
index 92bc8426b78d..000000000000
--- a/docs/es/sql-reference/statements/select/limit-by.md
+++ /dev/null
@@ -1,71 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# LIMITAR POR Cláusula {#limit-by-clause}
-
-Una consulta con el `LIMIT n BY expressions` cláusula selecciona la primera `n` para cada valor distinto de `expressions`. La clave para `LIMIT BY` puede contener cualquier número de [expresiones](../../syntax.md#syntax-expressions).
-
-ClickHouse admite las siguientes variantes de sintaxis:
-
--   `LIMIT [offset_value, ]n BY expressions`
--   `LIMIT n OFFSET offset_value BY expressions`
-
-Durante el procesamiento de consultas, ClickHouse selecciona los datos ordenados por clave de ordenación. La clave de ordenación se establece explícitamente utilizando un [ORDER BY](order-by.md) cláusula o implícitamente como una propiedad del motor de tablas. Entonces se aplica ClickHouse `LIMIT n BY expressions` y devuelve la primera `n` filas para cada combinación distinta de `expressions`. Si `OFFSET` se especifica, a continuación, para cada bloque de datos que pertenece a una combinación distinta de `expressions`, ClickHouse salta `offset_value` número de filas desde el principio del bloque y devuelve un máximo de `n` filas como resultado. Si `offset_value` es mayor que el número de filas en el bloque de datos, ClickHouse devuelve cero filas del bloque.
-
-!!! note "Nota"
-    `LIMIT BY` no está relacionado con [LIMIT](limit.md). Ambos se pueden usar en la misma consulta.
-
-## Ejemplos {#examples}
-
-Tabla de muestra:
-
-``` sql
-CREATE TABLE limit_by(id Int, val Int) ENGINE = Memory;
-INSERT INTO limit_by VALUES (1, 10), (1, 11), (1, 12), (2, 20), (2, 21);
-```
-
-Consulta:
-
-``` sql
-SELECT * FROM limit_by ORDER BY id, val LIMIT 2 BY id
-```
-
-``` text
-┌─id─┬─val─┐
-│  1 │  10 │
-│  1 │  11 │
-│  2 │  20 │
-│  2 │  21 │
-└────┴─────┘
-```
-
-``` sql
-SELECT * FROM limit_by ORDER BY id, val LIMIT 1, 2 BY id
-```
-
-``` text
-┌─id─┬─val─┐
-│  1 │  11 │
-│  1 │  12 │
-│  2 │  21 │
-└────┴─────┘
-```
-
-El `SELECT * FROM limit_by ORDER BY id, val LIMIT 2 OFFSET 1 BY id` query devuelve el mismo resultado.
-
-La siguiente consulta devuelve las 5 referencias principales para cada `domain, device_type` par con un máximo de 100 filas en total (`LIMIT n BY + LIMIT`).
-
-``` sql
-SELECT
-    domainWithoutWWW(URL) AS domain,
-    domainWithoutWWW(REFERRER_URL) AS referrer,
-    device_type,
-    count() cnt
-FROM hits
-GROUP BY domain, referrer, device_type
-ORDER BY cnt DESC
-LIMIT 5 BY domain, device_type
-LIMIT 100
-```
diff --git a/docs/es/sql-reference/statements/select/limit.md b/docs/es/sql-reference/statements/select/limit.md
deleted file mode 100644
index 60caf7f09b98..000000000000
--- a/docs/es/sql-reference/statements/select/limit.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Cláusula LIMIT {#limit-clause}
-
-`LIMIT m` permite seleccionar la primera `m` filas del resultado.
-
-`LIMIT n, m` permite seleccionar el `m` el resultado después de omitir la primera `n` filas. El `LIMIT m OFFSET n` sintaxis es equivalente.
-
-`n` y `m` deben ser enteros no negativos.
-
-Si no hay [ORDER BY](order-by.md) cláusula que ordena explícitamente los resultados, la elección de las filas para el resultado puede ser arbitraria y no determinista.
diff --git a/docs/es/sql-reference/statements/select/order-by.md b/docs/es/sql-reference/statements/select/order-by.md
deleted file mode 100644
index 3955cc1665a3..000000000000
--- a/docs/es/sql-reference/statements/select/order-by.md
+++ /dev/null
@@ -1,72 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# ORDEN POR CLÁUSULA {#select-order-by}
-
-El `ORDER BY` cláusula contiene una lista de expresiones, cada una de las cuales puede atribuirse con `DESC` (descendente) o `ASC` modificador (ascendente) que determina la dirección de clasificación. Si no se especifica la dirección, `ASC` se supone, por lo que generalmente se omite. La dirección de ordenación se aplica a una sola expresión, no a toda la lista. Ejemplo: `ORDER BY Visits DESC, SearchPhrase`
-
-Las filas que tienen valores idénticos para la lista de expresiones de clasificación se generan en un orden arbitrario, que también puede ser no determinista (diferente cada vez).
-Si se omite la cláusula ORDER BY, el orden de las filas tampoco está definido, y también puede ser no determinista.
-
-## Clasificación de valores especiales {#sorting-of-special-values}
-
-Hay dos enfoques para `NaN` y `NULL` orden de clasificación:
-
--   Por defecto o con el `NULLS LAST` modificador: primero los valores, luego `NaN`, entonces `NULL`.
--   Con el `NULLS FIRST` modificador: primero `NULL`, entonces `NaN`, luego otros valores.
-
-### Ejemplo {#example}
-
-Para la mesa
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    2 │
-│ 1 │  nan │
-│ 2 │    2 │
-│ 3 │    4 │
-│ 5 │    6 │
-│ 6 │  nan │
-│ 7 │ ᴺᵁᴸᴸ │
-│ 6 │    7 │
-│ 8 │    9 │
-└───┴──────┘
-```
-
-Ejecute la consulta `SELECT * FROM t_null_nan ORDER BY y NULLS FIRST` conseguir:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 7 │ ᴺᵁᴸᴸ │
-│ 1 │  nan │
-│ 6 │  nan │
-│ 2 │    2 │
-│ 2 │    2 │
-│ 3 │    4 │
-│ 5 │    6 │
-│ 6 │    7 │
-│ 8 │    9 │
-└───┴──────┘
-```
-
-Cuando se ordenan los números de coma flotante, los NaN están separados de los otros valores. Independientemente del orden de clasificación, los NaN vienen al final. En otras palabras, para la clasificación ascendente se colocan como si fueran más grandes que todos los demás números, mientras que para la clasificación descendente se colocan como si fueran más pequeños que el resto.
-
-## Soporte de colación {#collation-support}
-
-Para ordenar por valores de cadena, puede especificar la intercalación (comparación). Ejemplo: `ORDER BY SearchPhrase COLLATE 'tr'` - para ordenar por palabra clave en orden ascendente, utilizando el alfabeto turco, insensible a mayúsculas y minúsculas, suponiendo que las cadenas están codificadas en UTF-8. COLLATE se puede especificar o no para cada expresión en ORDER BY de forma independiente. Si se especifica ASC o DESC, se especifica COLLATE después de él. Cuando se usa COLLATE, la clasificación siempre distingue entre mayúsculas y minúsculas.
-
-Solo recomendamos usar COLLATE para la clasificación final de un pequeño número de filas, ya que la clasificación con COLLATE es menos eficiente que la clasificación normal por bytes.
-
-## Detalles de implementación {#implementation-details}
-
-Menos RAM se utiliza si un pequeño suficiente [LIMIT](limit.md) se especifica además `ORDER BY`. De lo contrario, la cantidad de memoria gastada es proporcional al volumen de datos para clasificar. Para el procesamiento de consultas distribuidas, si [GROUP BY](group-by.md) se omite, la clasificación se realiza parcialmente en servidores remotos y los resultados se fusionan en el servidor solicitante. Esto significa que para la ordenación distribuida, el volumen de datos a ordenar puede ser mayor que la cantidad de memoria en un único servidor.
-
-Si no hay suficiente RAM, es posible realizar la clasificación en la memoria externa (creando archivos temporales en un disco). Utilice el ajuste `max_bytes_before_external_sort` para este propósito. Si se establece en 0 (el valor predeterminado), la ordenación externa está deshabilitada. Si está habilitada, cuando el volumen de datos a ordenar alcanza el número especificado de bytes, los datos recopilados se ordenan y se vuelcan en un archivo temporal. Después de leer todos los datos, todos los archivos ordenados se fusionan y se generan los resultados. Los archivos se escriben en el `/var/lib/clickhouse/tmp/` directorio en la configuración (de forma predeterminada, pero puede usar el `tmp_path` parámetro para cambiar esta configuración).
-
-La ejecución de una consulta puede usar más memoria que `max_bytes_before_external_sort`. Por este motivo, esta configuración debe tener un valor significativamente menor que `max_memory_usage`. Como ejemplo, si su servidor tiene 128 GB de RAM y necesita ejecutar una sola consulta, establezca `max_memory_usage` de hasta 100 GB, y `max_bytes_before_external_sort` para 80 GB.
-
-La clasificación externa funciona con mucha menos eficacia que la clasificación en RAM.
diff --git a/docs/es/sql-reference/statements/select/prewhere.md b/docs/es/sql-reference/statements/select/prewhere.md
deleted file mode 100644
index 17d283f59f14..000000000000
--- a/docs/es/sql-reference/statements/select/prewhere.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# PREWHERE Cláusula {#prewhere-clause}
-
-Prewhere es una optimización para aplicar el filtrado de manera más eficiente. Está habilitado de forma predeterminada incluso si `PREWHERE` cláusula no se especifica explícitamente. Funciona moviendo automáticamente parte de [WHERE](where.md) condición a la etapa prewhere. El papel de `PREWHERE` cláusula es sólo para controlar esta optimización si usted piensa que usted sabe cómo hacerlo mejor de lo que sucede por defecto.
-
-Con la optimización prewhere, al principio solo se leen las columnas necesarias para ejecutar la expresión prewhere. Luego se leen las otras columnas que son necesarias para ejecutar el resto de la consulta, pero solo aquellos bloques donde está la expresión prewhere “true” al menos para algunas filas. Si hay muchos bloques donde la expresión prewhere es “false” para todas las filas y prewhere necesita menos columnas que otras partes de la consulta, esto a menudo permite leer muchos menos datos del disco para la ejecución de la consulta.
-
-## Control de Prewhere manualmente {#controlling-prewhere-manually}
-
-La cláusula tiene el mismo significado que la `WHERE` clausula. La diferencia radica en qué datos se leen de la tabla. Al controlar manualmente `PREWHERE` para las condiciones de filtración utilizadas por una minoría de las columnas de la consulta, pero que proporcionan una filtración de datos segura. Esto reduce el volumen de datos a leer.
-
-Una consulta puede especificar simultáneamente `PREWHERE` y `WHERE`. En este caso, `PREWHERE` preceder `WHERE`.
-
-Si el `optimize_move_to_prewhere` se establece en 0, heurística para mover automáticamente partes de expresiones de `WHERE` a `PREWHERE` están deshabilitados.
-
-## Limitacion {#limitations}
-
-`PREWHERE` sólo es compatible con tablas de la `*MergeTree` familia.
diff --git a/docs/es/sql-reference/statements/select/sample.md b/docs/es/sql-reference/statements/select/sample.md
deleted file mode 100644
index 54b3f69e19ce..000000000000
--- a/docs/es/sql-reference/statements/select/sample.md
+++ /dev/null
@@ -1,113 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Cláusula SAMPLE {#select-sample-clause}
-
-El `SAMPLE` cláusula permite aproximadamente `SELECT` procesamiento de consultas.
-
-Cuando se habilita el muestreo de datos, la consulta no se realiza en todos los datos, sino solo en una cierta fracción de datos (muestra). Por ejemplo, si necesita calcular estadísticas para todas las visitas, es suficiente ejecutar la consulta en la fracción 1/10 de todas las visitas y luego multiplicar el resultado por 10.
-
-El procesamiento de consultas aproximado puede ser útil en los siguientes casos:
-
--   Cuando tiene requisitos de temporización estrictos (como \<100 ms) pero no puede justificar el costo de recursos de hardware adicionales para cumplirlos.
--   Cuando sus datos sin procesar no son precisos, la aproximación no degrada notablemente la calidad.
--   Los requisitos comerciales se centran en los resultados aproximados (por rentabilidad o para comercializar los resultados exactos a los usuarios premium).
-
-!!! note "Nota"
-    Sólo puede utilizar el muestreo con las tablas en el [Método de codificación de datos:](../../../engines/table-engines/mergetree-family/mergetree.md) familia, y sólo si la expresión de muestreo se especificó durante la creación de la tabla (ver [Motor MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table)).
-
-Las características del muestreo de datos se enumeran a continuación:
-
--   El muestreo de datos es un mecanismo determinista. El resultado de la misma `SELECT .. SAMPLE` la consulta es siempre la misma.
--   El muestreo funciona consistentemente para diferentes tablas. Para tablas con una sola clave de muestreo, una muestra con el mismo coeficiente siempre selecciona el mismo subconjunto de datos posibles. Por ejemplo, una muestra de ID de usuario toma filas con el mismo subconjunto de todos los ID de usuario posibles de diferentes tablas. Esto significa que puede utilizar el ejemplo en subconsultas [IN](../../operators/in.md) clausula. Además, puede unir muestras usando el [JOIN](join.md) clausula.
--   El muestreo permite leer menos datos de un disco. Tenga en cuenta que debe especificar la clave de muestreo correctamente. Para obtener más información, consulte [Creación de una tabla MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table).
-
-Para el `SAMPLE` cláusula se admite la siguiente sintaxis:
-
-| SAMPLE Clause Syntax | Descripci                                                                                                                                                                                                                                                                     |
-|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| `SAMPLE k`           | Aqui `k` es el número de 0 a 1.</br>La consulta se ejecuta en `k` de datos. Por ejemplo, `SAMPLE 0.1` ejecuta la consulta en el 10% de los datos. [Leer más](#select-sample-k)                                                                                                |
-| `SAMPLE n`           | Aqui `n` es un entero suficientemente grande.</br>La consulta se ejecuta en una muestra de al menos `n` filas (pero no significativamente más que esto). Por ejemplo, `SAMPLE 10000000` ejecuta la consulta en un mínimo de 10.000.000 de filas. [Leer más](#select-sample-n) |
-| `SAMPLE k OFFSET m`  | Aqui `k` y `m` son los números del 0 al 1.</br>La consulta se ejecuta en una muestra de `k` de los datos. Los datos utilizados para el ejemplo se compensan por `m` fracción. [Leer más](#select-sample-offset)                                                               |
-
-## SAMPLE K {#select-sample-k}
-
-Aqui `k` es el número de 0 a 1 (se admiten notaciones fraccionarias y decimales). Por ejemplo, `SAMPLE 1/2` o `SAMPLE 0.5`.
-
-En un `SAMPLE k` cláusula, la muestra se toma de la `k` de datos. El ejemplo se muestra a continuación:
-
-``` sql
-SELECT
-    Title,
-    count() * 10 AS PageViews
-FROM hits_distributed
-SAMPLE 0.1
-WHERE
-    CounterID = 34
-GROUP BY Title
-ORDER BY PageViews DESC LIMIT 1000
-```
-
-En este ejemplo, la consulta se ejecuta en una muestra de 0,1 (10%) de datos. Los valores de las funciones agregadas no se corrigen automáticamente, por lo que para obtener un resultado aproximado, el valor `count()` se multiplica manualmente por 10.
-
-## SAMPLE N {#select-sample-n}
-
-Aqui `n` es un entero suficientemente grande. Por ejemplo, `SAMPLE 10000000`.
-
-En este caso, la consulta se ejecuta en una muestra de al menos `n` filas (pero no significativamente más que esto). Por ejemplo, `SAMPLE 10000000` ejecuta la consulta en un mínimo de 10.000.000 de filas.
-
-Dado que la unidad mínima para la lectura de datos es un gránulo (su tamaño se establece mediante el `index_granularity` ajuste), tiene sentido establecer una muestra que es mucho más grande que el tamaño del gránulo.
-
-Cuando se utiliza el `SAMPLE n` cláusula, no sabe qué porcentaje relativo de datos se procesó. Por lo tanto, no sabe el coeficiente por el que se deben multiplicar las funciones agregadas. Utilice el `_sample_factor` columna virtual para obtener el resultado aproximado.
-
-El `_sample_factor` columna contiene coeficientes relativos que se calculan dinámicamente. Esta columna se crea automáticamente cuando [crear](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table) una tabla con la clave de muestreo especificada. Los ejemplos de uso del `_sample_factor` columna se muestran a continuación.
-
-Consideremos la tabla `visits`, que contiene las estadísticas sobre las visitas al sitio. El primer ejemplo muestra cómo calcular el número de páginas vistas:
-
-``` sql
-SELECT sum(PageViews * _sample_factor)
-FROM visits
-SAMPLE 10000000
-```
-
-El siguiente ejemplo muestra cómo calcular el número total de visitas:
-
-``` sql
-SELECT sum(_sample_factor)
-FROM visits
-SAMPLE 10000000
-```
-
-El siguiente ejemplo muestra cómo calcular la duración media de la sesión. Tenga en cuenta que no necesita usar el coeficiente relativo para calcular los valores promedio.
-
-``` sql
-SELECT avg(Duration)
-FROM visits
-SAMPLE 10000000
-```
-
-## SAMPLE K OFFSET M {#select-sample-offset}
-
-Aqui `k` y `m` son números del 0 al 1. Los ejemplos se muestran a continuación.
-
-**Ejemplo 1**
-
-``` sql
-SAMPLE 1/10
-```
-
-En este ejemplo, la muestra es 1/10 de todos los datos:
-
-`[++------------]`
-
-**Ejemplo 2**
-
-``` sql
-SAMPLE 1/10 OFFSET 1/2
-```
-
-Aquí, se toma una muestra del 10% de la segunda mitad de los datos.
-
-`[------++------]`
diff --git a/docs/es/sql-reference/statements/select/union.md b/docs/es/sql-reference/statements/select/union.md
deleted file mode 100644
index d3aec34ba4b6..000000000000
--- a/docs/es/sql-reference/statements/select/union.md
+++ /dev/null
@@ -1,35 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# UNION Cláusula {#union-clause}
-
-Usted puede utilizar `UNION ALL` combinar cualquier número de `SELECT` consultas extendiendo sus resultados. Ejemplo:
-
-``` sql
-SELECT CounterID, 1 AS table, toInt64(count()) AS c
-    FROM test.hits
-    GROUP BY CounterID
-
-UNION ALL
-
-SELECT CounterID, 2 AS table, sum(Sign) AS c
-    FROM test.visits
-    GROUP BY CounterID
-    HAVING c > 0
-```
-
-Las columnas de resultados coinciden con su índice (orden dentro `SELECT`). Si los nombres de columna no coinciden, los nombres del resultado final se toman de la primera consulta.
-
-La fundición de tipo se realiza para uniones. Por ejemplo, si dos consultas que se combinan tienen el mismo campo-`Nullable` y `Nullable` tipos de un tipo compatible, el resultado `UNION ALL` tiene una `Nullable` campo de tipo.
-
-Consultas que son parte de `UNION ALL` no se puede encerrar entre corchetes redondos. [ORDER BY](order-by.md) y [LIMIT](limit.md) se aplican a consultas separadas, no al resultado final. Si necesita aplicar una conversión al resultado final, puede colocar todas las consultas con `UNION ALL` en una subconsulta en el [FROM](from.md) clausula.
-
-## Limitacion {#limitations}
-
-Solo `UNION ALL` se admite. Regular `UNION` (`UNION DISTINCT`) no es compatible. Si necesita `UNION DISTINCT` usted puede escribir `SELECT DISTINCT` de una subconsulta que contiene `UNION ALL`.
-
-## Detalles de implementación {#implementation-details}
-
-Consultas que son parte de `UNION ALL` se puede ejecutar simultáneamente, y sus resultados se pueden mezclar juntos.
diff --git a/docs/es/sql-reference/statements/select/where.md b/docs/es/sql-reference/statements/select/where.md
deleted file mode 100644
index 18caecdf7a51..000000000000
--- a/docs/es/sql-reference/statements/select/where.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# DONDE Cláusula {#select-where}
-
-`WHERE` cláusula permite filtrar los datos que provienen de [FROM](from.md) cláusula de `SELECT`.
-
-Si hay un `WHERE` cláusula, debe contener una expresión con el `UInt8` tipo. Esta suele ser una expresión con comparación y operadores lógicos. Las filas en las que esta expresión se evalúa como 0 se explican a partir de otras transformaciones o resultados.
-
-`WHERE` expresión se evalúa en la capacidad de utilizar índices y poda de partición, si el motor de tabla subyacente lo admite.
-
-!!! note "Nota"
-    Hay una optimización de filtrado llamada [preliminar](prewhere.md).
diff --git a/docs/es/sql-reference/statements/select/with.md b/docs/es/sql-reference/statements/select/with.md
deleted file mode 100644
index 7629226bbcca..000000000000
--- a/docs/es/sql-reference/statements/select/with.md
+++ /dev/null
@@ -1,80 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# CON Cláusula {#with-clause}
-
-Esta sección proporciona soporte para expresiones de tabla común ([CTE](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL)), por lo que los resultados de `WITH` cláusula se puede utilizar en el interior `SELECT` clausula.
-
-## Limitacion {#limitations}
-
-1.  No se admiten consultas recursivas.
-2.  Cuando se usa subconsulta dentro de la sección WITH , su resultado debe ser escalar con exactamente una fila.
-3.  Los resultados de la expresión no están disponibles en las subconsultas.
-
-## Ejemplos {#examples}
-
-**Ejemplo 1:** Usando expresión constante como “variable”
-
-``` sql
-WITH '2019-08-01 15:23:00' as ts_upper_bound
-SELECT *
-FROM hits
-WHERE
-    EventDate = toDate(ts_upper_bound) AND
-    EventTime <= ts_upper_bound
-```
-
-**Ejemplo 2:** Evictar el resultado de la expresión de sum (bytes) de la lista de columnas de la cláusula SELECT
-
-``` sql
-WITH sum(bytes) as s
-SELECT
-    formatReadableSize(s),
-    table
-FROM system.parts
-GROUP BY table
-ORDER BY s
-```
-
-**Ejemplo 3:** Uso de los resultados de la subconsulta escalar
-
-``` sql
-/* this example would return TOP 10 of most huge tables */
-WITH
-    (
-        SELECT sum(bytes)
-        FROM system.parts
-        WHERE active
-    ) AS total_disk_usage
-SELECT
-    (sum(bytes) / total_disk_usage) * 100 AS table_disk_usage,
-    table
-FROM system.parts
-GROUP BY table
-ORDER BY table_disk_usage DESC
-LIMIT 10
-```
-
-**Ejemplo 4:** Reutilización de la expresión en subconsulta
-
-Como solución alternativa para la limitación actual para el uso de expresiones en subconsultas, puede duplicarla.
-
-``` sql
-WITH ['hello'] AS hello
-SELECT
-    hello,
-    *
-FROM
-(
-    WITH ['hello'] AS hello
-    SELECT hello
-)
-```
-
-``` text
-┌─hello─────┬─hello─────┐
-│ ['hello'] │ ['hello'] │
-└───────────┴───────────┘
-```
diff --git a/docs/es/sql-reference/statements/show.md b/docs/es/sql-reference/statements/show.md
deleted file mode 100644
index e77836924521..000000000000
--- a/docs/es/sql-reference/statements/show.md
+++ /dev/null
@@ -1,169 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: SHOW
----
-
-# MOSTRAR consultas {#show-queries}
-
-## SHOW CREATE TABLE {#show-create-table}
-
-``` sql
-SHOW CREATE [TEMPORARY] [TABLE|DICTIONARY] [db.]table [INTO OUTFILE filename] [FORMAT format]
-```
-
-Devuelve una sola `String`-tipo ‘statement’ column, which contains a single value – the `CREATE` consulta utilizada para crear el objeto especificado.
-
-## SHOW DATABASES {#show-databases}
-
-``` sql
-SHOW DATABASES [INTO OUTFILE filename] [FORMAT format]
-```
-
-Imprime una lista de todas las bases de datos.
-Esta consulta es idéntica a `SELECT name FROM system.databases [INTO OUTFILE filename] [FORMAT format]`.
-
-## SHOW PROCESSLIST {#show-processlist}
-
-``` sql
-SHOW PROCESSLIST [INTO OUTFILE filename] [FORMAT format]
-```
-
-Envía el contenido de la [sistema.procesa](../../operations/system-tables.md#system_tables-processes) tabla, que contiene una lista de consultas que se están procesando en este momento, exceptuando `SHOW PROCESSLIST` consulta.
-
-El `SELECT * FROM system.processes` query devuelve datos sobre todas las consultas actuales.
-
-Consejo (ejecutar en la consola):
-
-``` bash
-$ watch -n1 "clickhouse-client --query='SHOW PROCESSLIST'"
-```
-
-## SHOW TABLES {#show-tables}
-
-Muestra una lista de tablas.
-
-``` sql
-SHOW [TEMPORARY] TABLES [{FROM | IN} <db>] [LIKE '<pattern>' | WHERE expr] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-Si el `FROM` no se especifica la cláusula, la consulta devuelve la lista de tablas de la base de datos actual.
-
-Puede obtener los mismos resultados que el `SHOW TABLES` consulta de la siguiente manera:
-
-``` sql
-SELECT name FROM system.tables WHERE database = <db> [AND name LIKE <pattern>] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-**Ejemplo**
-
-La siguiente consulta selecciona las dos primeras filas de la lista de tablas `system` base de datos, cuyos nombres contienen `co`.
-
-``` sql
-SHOW TABLES FROM system LIKE '%co%' LIMIT 2
-```
-
-``` text
-┌─name───────────────────────────┐
-│ aggregate_function_combinators │
-│ collations                     │
-└────────────────────────────────┘
-```
-
-## SHOW DICTIONARIES {#show-dictionaries}
-
-Muestra una lista de [diccionarios externos](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md).
-
-``` sql
-SHOW DICTIONARIES [FROM <db>] [LIKE '<pattern>'] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-Si el `FROM` no se especifica la cláusula, la consulta devuelve la lista de diccionarios de la base de datos actual.
-
-Puede obtener los mismos resultados que el `SHOW DICTIONARIES` consulta de la siguiente manera:
-
-``` sql
-SELECT name FROM system.dictionaries WHERE database = <db> [AND name LIKE <pattern>] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-**Ejemplo**
-
-La siguiente consulta selecciona las dos primeras filas de la lista de tablas `system` base de datos, cuyos nombres contienen `reg`.
-
-``` sql
-SHOW DICTIONARIES FROM db LIKE '%reg%' LIMIT 2
-```
-
-``` text
-┌─name─────────┐
-│ regions      │
-│ region_names │
-└──────────────┘
-```
-
-## SHOW GRANTS {#show-grants-statement}
-
-Muestra privilegios para un usuario.
-
-### Sintaxis {#show-grants-syntax}
-
-``` sql
-SHOW GRANTS [FOR user]
-```
-
-Si no se especifica user, la consulta devuelve privilegios para el usuario actual.
-
-## SHOW CREATE USER {#show-create-user-statement}
-
-Muestra los parámetros que se usaron en un [creación de usuario](create.md#create-user-statement).
-
-`SHOW CREATE USER` no genera contraseñas de usuario.
-
-### Sintaxis {#show-create-user-syntax}
-
-``` sql
-SHOW CREATE USER [name | CURRENT_USER]
-```
-
-## SHOW CREATE ROLE {#show-create-role-statement}
-
-Muestra los parámetros que se usaron en un [creación de roles](create.md#create-role-statement)
-
-### Sintaxis {#show-create-role-syntax}
-
-``` sql
-SHOW CREATE ROLE name
-```
-
-## SHOW CREATE ROW POLICY {#show-create-row-policy-statement}
-
-Muestra los parámetros que se usaron en un [creación de políticas de fila](create.md#create-row-policy-statement)
-
-### Sintaxis {#show-create-row-policy-syntax}
-
-``` sql
-SHOW CREATE [ROW] POLICY name ON [database.]table
-```
-
-## SHOW CREATE QUOTA {#show-create-quota-statement}
-
-Muestra los parámetros que se usaron en un [creación de cuotas](create.md#create-quota-statement)
-
-### Sintaxis {#show-create-row-policy-syntax}
-
-``` sql
-SHOW CREATE QUOTA [name | CURRENT]
-```
-
-## SHOW CREATE SETTINGS PROFILE {#show-create-settings-profile-statement}
-
-Muestra los parámetros que se usaron en un [configuración creación de perfil](create.md#create-settings-profile-statement)
-
-### Sintaxis {#show-create-row-policy-syntax}
-
-``` sql
-SHOW CREATE [SETTINGS] PROFILE name
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/show/) <!--hide-->
diff --git a/docs/es/sql-reference/statements/system.md b/docs/es/sql-reference/statements/system.md
deleted file mode 100644
index 1443dd9a7402..000000000000
--- a/docs/es/sql-reference/statements/system.md
+++ /dev/null
@@ -1,113 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: SYSTEM
----
-
-# Consultas del sistema {#query-language-system}
-
--   [RELOAD DICTIONARIES](#query_language-system-reload-dictionaries)
--   [RELOAD DICTIONARY](#query_language-system-reload-dictionary)
--   [DROP DNS CACHE](#query_language-system-drop-dns-cache)
--   [DROP MARK CACHE](#query_language-system-drop-mark-cache)
--   [FLUSH LOGS](#query_language-system-flush_logs)
--   [RELOAD CONFIG](#query_language-system-reload-config)
--   [SHUTDOWN](#query_language-system-shutdown)
--   [KILL](#query_language-system-kill)
--   [STOP DISTRIBUTED SENDS](#query_language-system-stop-distributed-sends)
--   [FLUSH DISTRIBUTED](#query_language-system-flush-distributed)
--   [START DISTRIBUTED SENDS](#query_language-system-start-distributed-sends)
--   [STOP MERGES](#query_language-system-stop-merges)
--   [START MERGES](#query_language-system-start-merges)
-
-## RELOAD DICTIONARIES {#query_language-system-reload-dictionaries}
-
-Vuelve a cargar todos los diccionarios que se han cargado correctamente antes.
-De forma predeterminada, los diccionarios se cargan perezosamente (ver [Diccionarios_lazy_load](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-dictionaries_lazy_load)), por lo que en lugar de cargarse automáticamente al inicio, se inicializan en el primer acceso a través de la función dictGet o SELECT desde tablas con ENGINE = Dictionary . El `SYSTEM RELOAD DICTIONARIES` consulta vuelve a cargar dichos diccionarios (LOADED).
-Siempre vuelve `Ok.` independientemente del resultado de la actualización del diccionario.
-
-## RELOAD DICTIONARY Dictionary_name {#query_language-system-reload-dictionary}
-
-Recarga completamente un diccionario `dictionary_name`, independientemente del estado del diccionario (LOADED / NOT_LOADED / FAILED).
-Siempre vuelve `Ok.` independientemente del resultado de la actualización del diccionario.
-El estado del diccionario se puede comprobar consultando el `system.dictionaries` tabla.
-
-``` sql
-SELECT name, status FROM system.dictionaries;
-```
-
-## DROP DNS CACHE {#query_language-system-drop-dns-cache}
-
-Restablece la caché DNS interna de ClickHouse. A veces (para versiones anteriores de ClickHouse) es necesario usar este comando al cambiar la infraestructura (cambiar la dirección IP de otro servidor de ClickHouse o el servidor utilizado por los diccionarios).
-
-Para obtener una administración de caché más conveniente (automática), consulte disable_internal_dns_cache, dns_cache_update_period parameters.
-
-## DROP MARK CACHE {#query_language-system-drop-mark-cache}
-
-Restablece la caché de marcas. Utilizado en el desarrollo de ClickHouse y pruebas de rendimiento.
-
-## FLUSH LOGS {#query_language-system-flush_logs}
-
-Flushes buffers of log messages to system tables (e.g. system.query_log). Allows you to not wait 7.5 seconds when debugging.
-
-## RELOAD CONFIG {#query_language-system-reload-config}
-
-Vuelve a cargar la configuración de ClickHouse. Se usa cuando la configuración se almacena en ZooKeeeper.
-
-## SHUTDOWN {#query_language-system-shutdown}
-
-Normalmente se apaga ClickHouse (como `service clickhouse-server stop` / `kill {$pid_clickhouse-server}`)
-
-## KILL {#query_language-system-kill}
-
-Anula el proceso de ClickHouse (como `kill -9 {$ pid_clickhouse-server}`)
-
-## Administración de tablas distribuidas {#query-language-system-distributed}
-
-ClickHouse puede administrar [distribuido](../../engines/table-engines/special/distributed.md) tabla. Cuando un usuario inserta datos en estas tablas, ClickHouse primero crea una cola de los datos que se deben enviar a los nodos del clúster y, a continuación, los envía de forma asincrónica. Puede administrar el procesamiento de colas con el [STOP DISTRIBUTED SENDS](#query_language-system-stop-distributed-sends), [FLUSH DISTRIBUTED](#query_language-system-flush-distributed), y [START DISTRIBUTED SENDS](#query_language-system-start-distributed-sends) consulta. También puede insertar sincrónicamente datos distribuidos con el `insert_distributed_sync` configuración.
-
-### STOP DISTRIBUTED SENDS {#query_language-system-stop-distributed-sends}
-
-Deshabilita la distribución de datos en segundo plano al insertar datos en tablas distribuidas.
-
-``` sql
-SYSTEM STOP DISTRIBUTED SENDS [db.]<distributed_table_name>
-```
-
-### FLUSH DISTRIBUTED {#query_language-system-flush-distributed}
-
-Obliga a ClickHouse a enviar datos a nodos de clúster de forma sincrónica. Si algún nodo no está disponible, ClickHouse produce una excepción y detiene la ejecución de la consulta. Puede volver a intentar la consulta hasta que tenga éxito, lo que sucederá cuando todos los nodos estén nuevamente en línea.
-
-``` sql
-SYSTEM FLUSH DISTRIBUTED [db.]<distributed_table_name>
-```
-
-### START DISTRIBUTED SENDS {#query_language-system-start-distributed-sends}
-
-Habilita la distribución de datos en segundo plano al insertar datos en tablas distribuidas.
-
-``` sql
-SYSTEM START DISTRIBUTED SENDS [db.]<distributed_table_name>
-```
-
-### STOP MERGES {#query_language-system-stop-merges}
-
-Proporciona la posibilidad de detener las fusiones en segundo plano para las tablas de la familia MergeTree:
-
-``` sql
-SYSTEM STOP MERGES [[db.]merge_tree_family_table_name]
-```
-
-!!! note "Nota"
-    `DETACH / ATTACH` la tabla comenzará las fusiones de fondo para la tabla, incluso en caso de que las fusiones se hayan detenido para todas las tablas MergeTree antes.
-
-### START MERGES {#query_language-system-start-merges}
-
-Proporciona la posibilidad de iniciar fusiones en segundo plano para tablas de la familia MergeTree:
-
-``` sql
-SYSTEM START MERGES [[db.]merge_tree_family_table_name]
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/system/) <!--hide-->
diff --git a/docs/es/sql-reference/syntax.md b/docs/es/sql-reference/syntax.md
deleted file mode 100644
index 93cd44c2b91e..000000000000
--- a/docs/es/sql-reference/syntax.md
+++ /dev/null
@@ -1,187 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 31
-toc_title: Sintaxis
----
-
-# Sintaxis {#syntax}
-
-Hay dos tipos de analizadores en el sistema: el analizador SQL completo (un analizador de descenso recursivo) y el analizador de formato de datos (un analizador de flujo rápido).
-En todos los casos, excepto el `INSERT` consulta, sólo se utiliza el analizador SQL completo.
-El `INSERT` consulta utiliza ambos analizadores:
-
-``` sql
-INSERT INTO t VALUES (1, 'Hello, world'), (2, 'abc'), (3, 'def')
-```
-
-El `INSERT INTO t VALUES` fragmento es analizado por el analizador completo, y los datos `(1, 'Hello, world'), (2, 'abc'), (3, 'def')` es analizado por el analizador de flujo rápido. También puede activar el analizador completo de los datos mediante el [input_format_values_interpret_expressions](../operations/settings/settings.md#settings-input_format_values_interpret_expressions) configuración. Cuando `input_format_values_interpret_expressions = 1`, ClickHouse primero intenta analizar valores con el analizador de flujo rápido. Si falla, ClickHouse intenta usar el analizador completo para los datos, tratándolo como un SQL [expresion](#syntax-expressions).
-
-Los datos pueden tener cualquier formato. Cuando se recibe una consulta, el servidor no calcula más de [max_query_size](../operations/settings/settings.md#settings-max_query_size) bytes de la solicitud en RAM (por defecto, 1 MB), y el resto se analiza la secuencia.
-Permite evitar problemas con grandes `INSERT` consulta.
-
-Cuando se utiliza el `Values` formato en un `INSERT` consulta, puede parecer que los datos se analizan igual que las expresiones en un `SELECT` consulta, pero esto no es cierto. El `Values` formato es mucho más limitado.
-
-El resto de este artículo cubre el analizador completo. Para obtener más información sobre los analizadores de formato, consulte [Formato](../interfaces/formats.md) apartado.
-
-## Espacio {#spaces}
-
-Puede haber cualquier número de símbolos de espacio entre las construcciones sintácticas (incluidos el principio y el final de una consulta). Los símbolos de espacio incluyen el espacio, tabulación, avance de línea, CR y avance de formulario.
-
-## Comentario {#comments}
-
-ClickHouse admite comentarios de estilo SQL y de estilo C.
-Los comentarios de estilo SQL comienzan con `--` y continuar hasta el final de la línea, un espacio después `--` se puede omitir.
-C-estilo son de `/*` a `*/`y puede ser multilínea, tampoco se requieren espacios.
-
-## Palabras clave {#syntax-keywords}
-
-Las palabras clave no distinguen entre mayúsculas y minúsculas cuando corresponden a:
-
--   Estándar SQL. Por ejemplo, `SELECT`, `select` y `SeLeCt` son todos válidos.
--   Implementación en algunos DBMS populares (MySQL o Postgres). Por ejemplo, `DateTime` es lo mismo que `datetime`.
-
-Si el nombre del tipo de datos distingue entre mayúsculas y minúsculas `system.data_type_families` tabla.
-
-A diferencia de SQL estándar, todas las demás palabras clave (incluidos los nombres de funciones) son **minúsculas**.
-
-Las palabras clave no están reservadas; se tratan como tales solo en el contexto correspondiente. Si usted usa [identificador](#syntax-identifiers) con el mismo nombre que las palabras clave, encerrarlas en comillas dobles o backticks. Por ejemplo, la consulta `SELECT "FROM" FROM table_name` es válido si la tabla `table_name` tiene columna con el nombre `"FROM"`.
-
-## Identificador {#syntax-identifiers}
-
-Los identificadores son:
-
--   Nombres de clúster, base de datos, tabla, partición y columna.
--   Función.
--   Tipos de datos.
--   [Alias de expresión](#syntax-expression_aliases).
-
-Los identificadores pueden ser citados o no citados. Este último es preferido.
-
-Los identificadores no citados deben coincidir con la expresión regular `^[a-zA-Z_][0-9a-zA-Z_]*$` y no puede ser igual a [Palabras clave](#syntax-keywords). Ejemplos: `x, _1, X_y__Z123_.`
-
-Si desea utilizar identificadores iguales a las palabras clave o si desea utilizar otros símbolos en los identificadores, cítelo con comillas dobles o retrocesos, por ejemplo, `"id"`, `` `id` ``.
-
-## Literal {#literals}
-
-Hay numérico, cadena, compuesto, y `NULL` literal.
-
-### Numérico {#numeric}
-
-Literal numérico, intenta ser analizado:
-
--   Primero, como un número firmado de 64 bits, usando el [strtoull](https://en.cppreference.com/w/cpp/string/byte/strtoul) función.
--   Si no tiene éxito, como un número de 64 bits sin signo, [Sistema abierto.](https://en.cppreference.com/w/cpp/string/byte/strtol) función.
--   Si no tiene éxito, como un número de punto flotante [strtod](https://en.cppreference.com/w/cpp/string/byte/strtof) función.
--   De lo contrario, devuelve un error.
-
-El valor literal tiene el tipo más pequeño en el que se ajusta el valor.
-Por ejemplo, 1 se analiza como `UInt8` pero 256 se analiza como `UInt16`. Para obtener más información, consulte [Tipos de datos](../sql-reference/data-types/index.md).
-
-Ejemplos: `1`, `18446744073709551615`, `0xDEADBEEF`, `01`, `0.1`, `1e100`, `-1e-100`, `inf`, `nan`.
-
-### Cadena {#syntax-string-literal}
-
-Solo se admiten literales de cadena entre comillas simples. Los caracteres incluidos se pueden escapar de barra invertida. Las siguientes secuencias de escape tienen un valor especial correspondiente: `\b`, `\f`, `\r`, `
`, `\t`, `\0`, `\a`, `\v`, `\xHH`. En todos los demás casos, secuencias de escape en el formato `\c`, donde `c` cualquier carácter, se convierten a `c`. Significa que puedes usar las secuencias `\'`y`\\`. El valor tendrá el [Cadena](../sql-reference/data-types/string.md) tipo.
-
-En literales de cadena, necesitas escapar al menos `'` y `\`. Las comillas simples se pueden escapar con las comillas simples, literales `'It\'s'` y `'It''s'` son iguales.
-
-### Compuesto {#compound}
-
-Las matrices se construyen con corchetes `[1, 2, 3]`. Nuples están construidos con paréntesis `(1, 'Hello, world!', 2)`.
-Técnicamente, estos no son literales, sino expresiones con el operador de creación de matriz y el operador de creación de tuplas, respectivamente.
-Una matriz debe constar de al menos un elemento y una tupla debe tener al menos dos elementos.
-Hay un caso separado cuando aparecen tuplas en el `IN` cláusula de un `SELECT` consulta. Los resultados de la consulta pueden incluir tuplas, pero las tuplas no se pueden guardar en una base de datos (excepto las tablas con [Memoria](../engines/table-engines/special/memory.md) motor).
-
-### NULL {#null-literal}
-
-Indica que falta el valor.
-
-Para almacenar `NULL` en un campo de tabla, debe ser del [NULL](../sql-reference/data-types/nullable.md) tipo.
-
-Dependiendo del formato de datos (entrada o salida), `NULL` puede tener una representación diferente. Para obtener más información, consulte la documentación de [Formatos de datos](../interfaces/formats.md#formats).
-
-Hay muchos matices para el procesamiento `NULL`. Por ejemplo, si al menos uno de los argumentos de una operación de comparación es `NULL`, el resultado de esta operación también es `NULL`. Lo mismo es cierto para la multiplicación, la suma y otras operaciones. Para obtener más información, lea la documentación de cada operación.
-
-En las consultas, puede verificar `NULL` utilizando el [IS NULL](operators/index.md#operator-is-null) y [IS NOT NULL](operators/index.md) operadores y las funciones relacionadas `isNull` y `isNotNull`.
-
-## Función {#functions}
-
-Las llamadas a funciones se escriben como un identificador con una lista de argumentos (posiblemente vacíos) entre corchetes redondos. A diferencia de SQL estándar, los corchetes son necesarios, incluso para una lista de argumentos vacía. Ejemplo: `now()`.
-Hay funciones regulares y agregadas (ver la sección “Aggregate functions”). Algunas funciones agregadas pueden contener dos listas de argumentos entre paréntesis. Ejemplo: `quantile (0.9) (x)`. Estas funciones agregadas se llaman “parametric” funciones, y los argumentos en la primera lista se llaman “parameters”. La sintaxis de las funciones agregadas sin parámetros es la misma que para las funciones regulares.
-
-## Operador {#operators}
-
-Los operadores se convierten a sus funciones correspondientes durante el análisis de consultas, teniendo en cuenta su prioridad y asociatividad.
-Por ejemplo, la expresión `1 + 2 * 3 + 4` se transforma a `plus(plus(1, multiply(2, 3)), 4)`.
-
-## Tipos de datos y motores de tabla de base de datos {#data_types-and-database-table-engines}
-
-Tipos de datos y motores de tablas en el `CREATE` las consultas se escriben de la misma manera que los identificadores o funciones. En otras palabras, pueden o no contener una lista de argumentos entre corchetes. Para obtener más información, consulte las secciones “Data types,” “Table engines,” y “CREATE”.
-
-## Alias de expresión {#syntax-expression_aliases}
-
-Un alias es un nombre definido por el usuario para la expresión en una consulta.
-
-``` sql
-expr AS alias
-```
-
--   `AS` — The keyword for defining aliases. You can define the alias for a table name or a column name in a `SELECT` cláusula sin usar el `AS` palabra clave.
-
-        For example, `SELECT table_name_alias.column_name FROM table_name table_name_alias`.
-
-        In the [CAST](sql_reference/functions/type_conversion_functions.md#type_conversion_function-cast) function, the `AS` keyword has another meaning. See the description of the function.
-
--   `expr` — Any expression supported by ClickHouse.
-
-        For example, `SELECT column_name * 2 AS double FROM some_table`.
-
--   `alias` — Name for `expr`. Los alias deben cumplir con el [identificador](#syntax-identifiers) sintaxis.
-
-        For example, `SELECT "table t".column_name FROM table_name AS "table t"`.
-
-### Notas sobre el uso {#notes-on-usage}
-
-Los alias son globales para una consulta o subconsulta, y puede definir un alias en cualquier parte de una consulta para cualquier expresión. Por ejemplo, `SELECT (1 AS n) + 2, n`.
-
-Los alias no son visibles en subconsultas y entre subconsultas. Por ejemplo, al ejecutar la consulta `SELECT (SELECT sum(b.a) + num FROM b) - a.a AS num FROM a` ClickHouse genera la excepción `Unknown identifier: num`.
-
-Si se define un alias para las columnas de resultados `SELECT` cláusula de una subconsulta, estas columnas son visibles en la consulta externa. Por ejemplo, `SELECT n + m FROM (SELECT 1 AS n, 2 AS m)`.
-
-Tenga cuidado con los alias que son iguales a los nombres de columna o tabla. Consideremos el siguiente ejemplo:
-
-``` sql
-CREATE TABLE t
-(
-    a Int,
-    b Int
-)
-ENGINE = TinyLog()
-```
-
-``` sql
-SELECT
-    argMax(a, b),
-    sum(b) AS b
-FROM t
-```
-
-``` text
-Received exception from server (version 18.14.17):
-Code: 184. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: Aggregate function sum(b) is found inside another aggregate function in query.
-```
-
-En este ejemplo, declaramos tabla `t` con columna `b`. Luego, al seleccionar los datos, definimos el `sum(b) AS b` apodo. Como los alias son globales, ClickHouse sustituyó el literal `b` en la expresión `argMax(a, b)` con la expresión `sum(b)`. Esta sustitución causó la excepción.
-
-## Asterisco {#asterisk}
-
-En un `SELECT` consulta, un asterisco puede reemplazar la expresión. Para obtener más información, consulte la sección “SELECT”.
-
-## Expresiones {#syntax-expressions}
-
-Una expresión es una función, identificador, literal, aplicación de un operador, expresión entre paréntesis, subconsulta o asterisco. También puede contener un alias.
-Una lista de expresiones es una o más expresiones separadas por comas.
-Las funciones y los operadores, a su vez, pueden tener expresiones como argumentos.
-
-[Artículo Original](https://clickhouse.tech/docs/en/sql_reference/syntax/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/file.md b/docs/es/sql-reference/table-functions/file.md
deleted file mode 100644
index 0d1c9b66016e..000000000000
--- a/docs/es/sql-reference/table-functions/file.md
+++ /dev/null
@@ -1,121 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: file
----
-
-# file {#file}
-
-Crea una tabla a partir de un archivo. Esta función de tabla es similar a [URL](url.md) y [Hdfs](hdfs.md) aquel.
-
-``` sql
-file(path, format, structure)
-```
-
-**Parámetros de entrada**
-
--   `path` — The relative path to the file from [user_files_path](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-user_files_path). Soporte de ruta a archivo siguiendo globs en modo de solo lectura: `*`, `?`, `{abc,def}` y `{N..M}` donde `N`, `M` — numbers, \``'abc', 'def'` — strings.
--   `format` — The [formato](../../interfaces/formats.md#formats) del archivo.
--   `structure` — Structure of the table. Format `'column1_name column1_type, column2_name column2_type, ...'`.
-
-**Valor devuelto**
-
-Una tabla con la estructura especificada para leer o escribir datos en el archivo especificado.
-
-**Ejemplo**
-
-Configuración `user_files_path` y el contenido del archivo `test.csv`:
-
-``` bash
-$ grep user_files_path /etc/clickhouse-server/config.xml
-    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
-
-$ cat /var/lib/clickhouse/user_files/test.csv
-    1,2,3
-    3,2,1
-    78,43,45
-```
-
-Tabla de`test.csv` y selección de las dos primeras filas de ella:
-
-``` sql
-SELECT *
-FROM file('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')
-LIMIT 2
-```
-
-``` text
-┌─column1─┬─column2─┬─column3─┐
-│       1 │       2 │       3 │
-│       3 │       2 │       1 │
-└─────────┴─────────┴─────────┘
-```
-
-``` sql
--- getting the first 10 lines of a table that contains 3 columns of UInt32 type from a CSV file
-SELECT * FROM file('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32') LIMIT 10
-```
-
-**Globs en el camino**
-
-Múltiples componentes de ruta de acceso pueden tener globs. Para ser procesado, el archivo debe existir y coincidir con todo el patrón de ruta (no solo el sufijo o el prefijo).
-
--   `*` — Substitutes any number of any characters except `/` incluyendo cadena vacía.
--   `?` — Substitutes any single character.
--   `{some_string,another_string,yet_another_one}` — Substitutes any of strings `'some_string', 'another_string', 'yet_another_one'`.
--   `{N..M}` — Substitutes any number in range from N to M including both borders.
-
-Construcciones con `{}` son similares a la [función de tabla remota](../../sql-reference/table-functions/remote.md)).
-
-**Ejemplo**
-
-1.  Supongamos que tenemos varios archivos con las siguientes rutas relativas:
-
--   ‘some_dir/some_file_1’
--   ‘some_dir/some_file_2’
--   ‘some_dir/some_file_3’
--   ‘another_dir/some_file_1’
--   ‘another_dir/some_file_2’
--   ‘another_dir/some_file_3’
-
-1.  Consulta la cantidad de filas en estos archivos:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM file('{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32')
-```
-
-1.  Consulta la cantidad de filas en todos los archivos de estos dos directorios:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM file('{some,another}_dir/*', 'TSV', 'name String, value UInt32')
-```
-
-!!! warning "Advertencia"
-    Si su lista de archivos contiene rangos de números con ceros a la izquierda, use la construcción con llaves para cada dígito por separado o use `?`.
-
-**Ejemplo**
-
-Consultar los datos desde archivos nombrados `file000`, `file001`, … , `file999`:
-
-``` sql
-SELECT count(*)
-FROM file('big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32')
-```
-
-## Virtual Columnas {#virtual-columns}
-
--   `_path` — Path to the file.
--   `_file` — Name of the file.
-
-**Ver también**
-
--   [Virtual columnas](https://clickhouse.tech/docs/en/operations/table_engines/#table_engines-virtual_columns)
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/file/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/generate.md b/docs/es/sql-reference/table-functions/generate.md
deleted file mode 100644
index fd107370481f..000000000000
--- a/docs/es/sql-reference/table-functions/generate.md
+++ /dev/null
@@ -1,44 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: generateRandom
----
-
-# generateRandom {#generaterandom}
-
-Genera datos aleatorios con un esquema dado.
-Permite rellenar tablas de prueba con datos.
-Admite todos los tipos de datos que se pueden almacenar en la tabla, excepto `LowCardinality` y `AggregateFunction`.
-
-``` sql
-generateRandom('name TypeName[, name TypeName]...', [, 'random_seed'[, 'max_string_length'[, 'max_array_length']]]);
-```
-
-**Parámetros**
-
--   `name` — Name of corresponding column.
--   `TypeName` — Type of corresponding column.
--   `max_array_length` — Maximum array length for all generated arrays. Defaults to `10`.
--   `max_string_length` — Maximum string length for all generated strings. Defaults to `10`.
--   `random_seed` — Specify random seed manually to produce stable results. If NULL — seed is randomly generated.
-
-**Valor devuelto**
-
-Un objeto de tabla con el esquema solicitado.
-
-## Ejemplo de uso {#usage-example}
-
-``` sql
-SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)', 1, 10, 2) LIMIT 3;
-```
-
-``` text
-┌─a────────┬────────────d─┬─c──────────────────────────────────────────────────────────────────┐
-│ [77]     │ -124167.6723 │ ('2061-04-17 21:59:44.573','3f72f405-ec3e-13c8-44ca-66ef335f7835') │
-│ [32,110] │ -141397.7312 │ ('1979-02-09 03:43:48.526','982486d1-5a5d-a308-e525-7bd8b80ffa73') │
-│ [68]     │  -67417.0770 │ ('2080-03-12 14:17:31.269','110425e5-413f-10a6-05ba-fa6b3e929f15') │
-└──────────┴──────────────┴────────────────────────────────────────────────────────────────────┘
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/generate/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/hdfs.md b/docs/es/sql-reference/table-functions/hdfs.md
deleted file mode 100644
index 8c90f1d49cae..000000000000
--- a/docs/es/sql-reference/table-functions/hdfs.md
+++ /dev/null
@@ -1,104 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: Hdfs
----
-
-# Hdfs {#hdfs}
-
-Crea una tabla a partir de archivos en HDFS. Esta función de tabla es similar a [URL](url.md) y [file](file.md) aquel.
-
-``` sql
-hdfs(URI, format, structure)
-```
-
-**Parámetros de entrada**
-
--   `URI` — The relative URI to the file in HDFS. Path to file support following globs in readonly mode: `*`, `?`, `{abc,def}` y `{N..M}` donde `N`, `M` — numbers, \``'abc', 'def'` — strings.
--   `format` — The [formato](../../interfaces/formats.md#formats) del archivo.
--   `structure` — Structure of the table. Format `'column1_name column1_type, column2_name column2_type, ...'`.
-
-**Valor devuelto**
-
-Una tabla con la estructura especificada para leer o escribir datos en el archivo especificado.
-
-**Ejemplo**
-
-Tabla de `hdfs://hdfs1:9000/test` y selección de las dos primeras filas de ella:
-
-``` sql
-SELECT *
-FROM hdfs('hdfs://hdfs1:9000/test', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32')
-LIMIT 2
-```
-
-``` text
-┌─column1─┬─column2─┬─column3─┐
-│       1 │       2 │       3 │
-│       3 │       2 │       1 │
-└─────────┴─────────┴─────────┘
-```
-
-**Globs en el camino**
-
-Múltiples componentes de ruta de acceso pueden tener globs. Para ser procesado, el archivo debe existir y coincidir con todo el patrón de ruta (no solo el sufijo o el prefijo).
-
--   `*` — Substitutes any number of any characters except `/` incluyendo cadena vacía.
--   `?` — Substitutes any single character.
--   `{some_string,another_string,yet_another_one}` — Substitutes any of strings `'some_string', 'another_string', 'yet_another_one'`.
--   `{N..M}` — Substitutes any number in range from N to M including both borders.
-
-Construcciones con `{}` son similares a la [función de tabla remota](../../sql-reference/table-functions/remote.md)).
-
-**Ejemplo**
-
-1.  Supongamos que tenemos varios archivos con los siguientes URI en HDFS:
-
--   ‘hdfs://hdfs1:9000/some_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_3’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_3’
-
-1.  Consulta la cantidad de filas en estos archivos:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM hdfs('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32')
-```
-
-1.  Consulta la cantidad de filas en todos los archivos de estos dos directorios:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM hdfs('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV', 'name String, value UInt32')
-```
-
-!!! warning "Advertencia"
-    Si su lista de archivos contiene rangos de números con ceros a la izquierda, use la construcción con llaves para cada dígito por separado o use `?`.
-
-**Ejemplo**
-
-Consultar los datos desde archivos nombrados `file000`, `file001`, … , `file999`:
-
-``` sql
-SELECT count(*)
-FROM hdfs('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32')
-```
-
-## Virtual Columnas {#virtual-columns}
-
--   `_path` — Path to the file.
--   `_file` — Name of the file.
-
-**Ver también**
-
--   [Virtual columnas](https://clickhouse.tech/docs/en/operations/table_engines/#table_engines-virtual_columns)
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/hdfs/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/index.md b/docs/es/sql-reference/table-functions/index.md
deleted file mode 100644
index 60882327e6aa..000000000000
--- a/docs/es/sql-reference/table-functions/index.md
+++ /dev/null
@@ -1,38 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Funciones de tabla
-toc_priority: 34
-toc_title: "Implantaci\xF3n"
----
-
-# Funciones de tabla {#table-functions}
-
-Las funciones de tabla son métodos para construir tablas.
-
-Puede usar funciones de tabla en:
-
--   [FROM](../statements/select/from.md) cláusula de la `SELECT` consulta.
-
-        The method for creating a temporary table that is available only in the current query. The table is deleted when the query finishes.
-
--   [CREAR TABLA COMO \<table_function()\>](../statements/create.md#create-table-query) consulta.
-
-        It's one of the methods of creating a table.
-
-!!! warning "Advertencia"
-    No puede usar funciones de tabla si el [Método de codificación de datos:](../../operations/settings/permissions-for-queries.md#settings_allow_ddl) la configuración está deshabilitada.
-
-| Función              | Descripci                                                                                                                              |
-|----------------------|----------------------------------------------------------------------------------------------------------------------------------------|
-| [file](file.md)      | Crea un [File](../../engines/table-engines/special/file.md)-mesa del motor.                                                            |
-| [fusionar](merge.md) | Crea un [Fusionar](../../engines/table-engines/special/merge.md)-mesa del motor.                                                       |
-| [numero](numbers.md) | Crea una tabla con una sola columna llena de números enteros.                                                                          |
-| [remoto](remote.md)  | Le permite acceder a servidores remotos sin crear un [Distribuido](../../engines/table-engines/special/distributed.md)-mesa del motor. |
-| [URL](url.md)        | Crea un [URL](../../engines/table-engines/special/url.md)-mesa del motor.                                                              |
-| [mysql](mysql.md)    | Crea un [MySQL](../../engines/table-engines/integrations/mysql.md)-mesa del motor.                                                     |
-| [jdbc](jdbc.md)      | Crea un [JDBC](../../engines/table-engines/integrations/jdbc.md)-mesa del motor.                                                       |
-| [Nosotros](odbc.md)  | Crea un [ODBC](../../engines/table-engines/integrations/odbc.md)-mesa del motor.                                                       |
-| [Hdfs](hdfs.md)      | Crea un [HDFS](../../engines/table-engines/integrations/hdfs.md)-mesa del motor.                                                       |
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/input.md b/docs/es/sql-reference/table-functions/input.md
deleted file mode 100644
index c56e1c86ba92..000000000000
--- a/docs/es/sql-reference/table-functions/input.md
+++ /dev/null
@@ -1,47 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: entrada
----
-
-# entrada {#input}
-
-`input(structure)` - función de tabla que permite convertir e insertar efectivamente los datos enviados al
-servidor con estructura dada a la tabla con otra estructura.
-
-`structure` - estructura de los datos enviados al servidor en el siguiente formato `'column1_name column1_type, column2_name column2_type, ...'`.
-Por ejemplo, `'id UInt32, name String'`.
-
-Esta función sólo se puede utilizar en `INSERT SELECT` consulta y sólo una vez, pero por lo demás se comporta como función de tabla ordinaria
-(por ejemplo, se puede usar en subconsulta, etc.).
-
-Los datos se pueden enviar de cualquier manera como para ordinario `INSERT` consulta y pasado en cualquier disponible [formato](../../interfaces/formats.md#formats)
-que debe especificarse al final de la consulta (a diferencia de lo ordinario `INSERT SELECT`).
-
-La característica principal de esta función es que cuando el servidor recibe datos del cliente, los convierte simultáneamente
-según la lista de expresiones en el `SELECT` cláusula e inserta en la tabla de destino. Tabla temporal
-con todos los datos transferidos no se crea.
-
-**Ejemplos**
-
--   Deje que el `test` tiene la siguiente estructura `(a String, b String)`
-    y datos en `data.csv` tiene una estructura diferente `(col1 String, col2 Date, col3 Int32)`. Consulta de inserción
-    datos de la `data.csv` en el `test` con conversión simultánea se ve así:
-
-<!-- -->
-
-``` bash
-$ cat data.csv | clickhouse-client --query="INSERT INTO test SELECT lower(col1), col3 * col3 FROM input('col1 String, col2 Date, col3 Int32') FORMAT CSV";
-```
-
--   Si `data.csv` contiene datos de la misma estructura `test_structure` como la mesa `test` entonces estas dos consultas son iguales:
-
-<!-- -->
-
-``` bash
-$ cat data.csv | clickhouse-client --query="INSERT INTO test FORMAT CSV"
-$ cat data.csv | clickhouse-client --query="INSERT INTO test SELECT * FROM input('test_structure') FORMAT CSV"
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/input/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/jdbc.md b/docs/es/sql-reference/table-functions/jdbc.md
deleted file mode 100644
index fd848ca78a2f..000000000000
--- a/docs/es/sql-reference/table-functions/jdbc.md
+++ /dev/null
@@ -1,29 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: jdbc
----
-
-# jdbc {#table-function-jdbc}
-
-`jdbc(jdbc_connection_uri, schema, table)` - devuelve la tabla que está conectado a través del controlador JDBC.
-
-Esta función de tabla requiere `clickhouse-jdbc-bridge` programa para estar en ejecución.
-Admite tipos Nullable (basados en DDL de la tabla remota que se consulta).
-
-**Ejemplos**
-
-``` sql
-SELECT * FROM jdbc('jdbc:mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
-```
-
-``` sql
-SELECT * FROM jdbc('mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
-```
-
-``` sql
-SELECT * FROM jdbc('datasource://mysql-local', 'schema', 'table')
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/jdbc/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/merge.md b/docs/es/sql-reference/table-functions/merge.md
deleted file mode 100644
index 137e764c84d9..000000000000
--- a/docs/es/sql-reference/table-functions/merge.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: fusionar
----
-
-# fusionar {#merge}
-
-`merge(db_name, 'tables_regexp')` – Creates a temporary Merge table. For more information, see the section “Table engines, Merge”.
-
-La estructura de la tabla se toma de la primera tabla encontrada que coincide con la expresión regular.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/merge/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/mysql.md b/docs/es/sql-reference/table-functions/mysql.md
deleted file mode 100644
index b48c18a9472c..000000000000
--- a/docs/es/sql-reference/table-functions/mysql.md
+++ /dev/null
@@ -1,86 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: mysql
----
-
-# mysql {#mysql}
-
-Permitir `SELECT` consultas que se realizarán en los datos que se almacenan en un servidor MySQL remoto.
-
-``` sql
-mysql('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']);
-```
-
-**Parámetros**
-
--   `host:port` — MySQL server address.
-
--   `database` — Remote database name.
-
--   `table` — Remote table name.
-
--   `user` — MySQL user.
-
--   `password` — User password.
-
--   `replace_query` — Flag that converts `INSERT INTO` consultas a `REPLACE INTO`. Si `replace_query=1`, la consulta se reemplaza.
-
--   `on_duplicate_clause` — The `ON DUPLICATE KEY on_duplicate_clause` expresión que se añade a la `INSERT` consulta.
-
-        Example: `INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1`, where `on_duplicate_clause` is `UPDATE c2 = c2 + 1`. See the MySQL documentation to find which `on_duplicate_clause` you can use with the `ON DUPLICATE KEY` clause.
-
-        To specify `on_duplicate_clause` you need to pass `0` to the `replace_query` parameter. If you simultaneously pass `replace_query = 1` and `on_duplicate_clause`, ClickHouse generates an exception.
-
-Simple `WHERE` cláusulas tales como `=, !=, >, >=, <, <=` se ejecutan actualmente en el servidor MySQL.
-
-El resto de las condiciones y el `LIMIT` La restricción de muestreo se ejecuta en ClickHouse solo después de que finalice la consulta a MySQL.
-
-**Valor devuelto**
-
-Un objeto de tabla con las mismas columnas que la tabla MySQL original.
-
-## Ejemplo de uso {#usage-example}
-
-Tabla en MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Selección de datos de ClickHouse:
-
-``` sql
-SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123')
-```
-
-``` text
-┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐
-│      1 │         ᴺᵁᴸᴸ │     2 │           ᴺᵁᴸᴸ │
-└────────┴──────────────┴───────┴────────────────┘
-```
-
-## Ver también {#see-also}
-
--   [El ‘MySQL’ motor de mesa](../../engines/table-engines/integrations/mysql.md)
--   [Uso de MySQL como fuente de diccionario externo](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-mysql)
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/mysql/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/numbers.md b/docs/es/sql-reference/table-functions/numbers.md
deleted file mode 100644
index 5901a50fae21..000000000000
--- a/docs/es/sql-reference/table-functions/numbers.md
+++ /dev/null
@@ -1,30 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: numero
----
-
-# numero {#numbers}
-
-`numbers(N)` – Returns a table with the single ‘number’ columna (UInt64) que contiene enteros de 0 a N-1.
-`numbers(N, M)` - Devuelve una tabla con el único ‘number’ columna (UInt64) que contiene enteros de N a (N + M - 1).
-
-Similar a la `system.numbers` tabla, puede ser utilizado para probar y generar valores sucesivos, `numbers(N, M)` más eficiente que `system.numbers`.
-
-Las siguientes consultas son equivalentes:
-
-``` sql
-SELECT * FROM numbers(10);
-SELECT * FROM numbers(0, 10);
-SELECT * FROM system.numbers LIMIT 10;
-```
-
-Ejemplos:
-
-``` sql
--- Generate a sequence of dates from 2010-01-01 to 2010-12-31
-select toDate('2010-01-01') + number as d FROM numbers(365);
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/numbers/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/odbc.md b/docs/es/sql-reference/table-functions/odbc.md
deleted file mode 100644
index 9ba2bfeeaac5..000000000000
--- a/docs/es/sql-reference/table-functions/odbc.md
+++ /dev/null
@@ -1,108 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: Nosotros
----
-
-# Nosotros {#table-functions-odbc}
-
-Devuelve la tabla que está conectada a través de [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity).
-
-``` sql
-odbc(connection_settings, external_database, external_table)
-```
-
-Parámetros:
-
--   `connection_settings` — Name of the section with connection settings in the `odbc.ini` file.
--   `external_database` — Name of a database in an external DBMS.
--   `external_table` — Name of a table in the `external_database`.
-
-Para implementar con seguridad conexiones ODBC, ClickHouse usa un programa separado `clickhouse-odbc-bridge`. Si el controlador ODBC se carga directamente desde `clickhouse-server`, problemas de controlador pueden bloquear el servidor ClickHouse. ClickHouse se inicia automáticamente `clickhouse-odbc-bridge` cuando se requiere. El programa de puente ODBC se instala desde el mismo paquete que el `clickhouse-server`.
-
-Los campos con el `NULL` Los valores de la tabla externa se convierten en los valores predeterminados para el tipo de datos base. Por ejemplo, si un campo de tabla MySQL remoto tiene `INT NULL` tipo se convierte a 0 (el valor predeterminado para ClickHouse `Int32` tipo de datos).
-
-## Ejemplo de uso {#usage-example}
-
-**Obtener datos de la instalación local de MySQL a través de ODBC**
-
-Este ejemplo se comprueba para Ubuntu Linux 18.04 y el servidor MySQL 5.7.
-
-Asegúrese de que unixODBC y MySQL Connector están instalados.
-
-De forma predeterminada (si se instala desde paquetes), ClickHouse comienza como usuario `clickhouse`. Por lo tanto, debe crear y configurar este usuario en el servidor MySQL.
-
-``` bash
-$ sudo mysql
-```
-
-``` sql
-mysql> CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse';
-mysql> GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;
-```
-
-A continuación, configure la conexión en `/etc/odbc.ini`.
-
-``` bash
-$ cat /etc/odbc.ini
-[mysqlconn]
-DRIVER = /usr/local/lib/libmyodbc5w.so
-SERVER = 127.0.0.1
-PORT = 3306
-DATABASE = test
-USERNAME = clickhouse
-PASSWORD = clickhouse
-```
-
-Puede verificar la conexión usando el `isql` utilidad desde la instalación de unixODBC.
-
-``` bash
-$ isql -v mysqlconn
-+-------------------------+
-| Connected!                            |
-|                                       |
-...
-```
-
-Tabla en MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Recuperación de datos de la tabla MySQL en ClickHouse:
-
-``` sql
-SELECT * FROM odbc('DSN=mysqlconn', 'test', 'test')
-```
-
-``` text
-┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐
-│      1 │            0 │     2 │              0 │
-└────────┴──────────────┴───────┴────────────────┘
-```
-
-## Ver también {#see-also}
-
--   [Diccionarios externos ODBC](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-odbc)
--   [Motor de tabla ODBC](../../engines/table-engines/integrations/odbc.md).
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/jdbc/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/remote.md b/docs/es/sql-reference/table-functions/remote.md
deleted file mode 100644
index e9784cb4cd1d..000000000000
--- a/docs/es/sql-reference/table-functions/remote.md
+++ /dev/null
@@ -1,85 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: remoto
----
-
-# remoto, remoteSecure {#remote-remotesecure}
-
-Le permite acceder a servidores remotos sin crear un `Distributed` tabla.
-
-Firma:
-
-``` sql
-remote('addresses_expr', db, table[, 'user'[, 'password']])
-remote('addresses_expr', db.table[, 'user'[, 'password']])
-remoteSecure('addresses_expr', db, table[, 'user'[, 'password']])
-remoteSecure('addresses_expr', db.table[, 'user'[, 'password']])
-```
-
-`addresses_expr` – An expression that generates addresses of remote servers. This may be just one server address. The server address is `host:port` o simplemente `host`. El host se puede especificar como nombre de servidor o como dirección IPv4 o IPv6. Una dirección IPv6 se especifica entre corchetes. El puerto es el puerto TCP del servidor remoto. Si se omite el puerto, utiliza `tcp_port` del archivo de configuración del servidor (por defecto, 9000).
-
-!!! important "Importante"
-    El puerto es necesario para una dirección IPv6.
-
-Ejemplos:
-
-``` text
-example01-01-1
-example01-01-1:9000
-localhost
-127.0.0.1
-[::]:9000
-[2a02:6b8:0:1111::11]:9000
-```
-
-Se pueden separar varias direcciones por comas. En este caso, ClickHouse usará procesamiento distribuido, por lo que enviará la consulta a todas las direcciones especificadas (como a fragmentos con datos diferentes).
-
-Ejemplo:
-
-``` text
-example01-01-1,example01-02-1
-```
-
-Parte de la expresión se puede especificar entre llaves. El ejemplo anterior se puede escribir de la siguiente manera:
-
-``` text
-example01-0{1,2}-1
-```
-
-Los corchetes rizados pueden contener un rango de números separados por dos puntos (enteros no negativos). En este caso, el rango se expande a un conjunto de valores que generan direcciones de fragmentos. Si el primer número comienza con cero, los valores se forman con la misma alineación cero. El ejemplo anterior se puede escribir de la siguiente manera:
-
-``` text
-example01-{01..02}-1
-```
-
-Si tiene varios pares de llaves, genera el producto directo de los conjuntos correspondientes.
-
-Las direcciones y partes de las direcciones entre llaves se pueden separar mediante el símbolo de tubería (\|). En este caso, los conjuntos de direcciones correspondientes se interpretan como réplicas y la consulta se enviará a la primera réplica en buen estado. Sin embargo, las réplicas se iteran en el orden establecido actualmente en el [load_balancing](../../operations/settings/settings.md) configuración.
-
-Ejemplo:
-
-``` text
-example01-{01..02}-{1|2}
-```
-
-En este ejemplo se especifican dos fragmentos que tienen dos réplicas cada uno.
-
-El número de direcciones generadas está limitado por una constante. En este momento esto es 1000 direcciones.
-
-Uso de la `remote` función de la tabla es menos óptima que la creación de un `Distributed` mesa, porque en este caso, la conexión del servidor se restablece para cada solicitud. Además, si se establecen nombres de host, los nombres se resuelven y los errores no se cuentan cuando se trabaja con varias réplicas. Cuando procese un gran número de consultas, cree siempre el `Distributed` mesa antes de tiempo, y no utilice el `remote` función de la tabla.
-
-El `remote` puede ser útil en los siguientes casos:
-
--   Acceder a un servidor específico para la comparación de datos, la depuración y las pruebas.
--   Consultas entre varios clústeres de ClickHouse con fines de investigación.
--   Solicitudes distribuidas poco frecuentes que se realizan manualmente.
--   Solicitudes distribuidas donde el conjunto de servidores se redefine cada vez.
-
-Si el usuario no está especificado, `default` se utiliza.
-Si no se especifica la contraseña, se utiliza una contraseña vacía.
-
-`remoteSecure` - igual que `remote` but with secured connection. Default port — [Tcp_port_secure](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-tcp_port_secure) de config o 9440.
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/remote/) <!--hide-->
diff --git a/docs/es/sql-reference/table-functions/url.md b/docs/es/sql-reference/table-functions/url.md
deleted file mode 100644
index 6f86a5b2068e..000000000000
--- a/docs/es/sql-reference/table-functions/url.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: URL
----
-
-# URL {#url}
-
-`url(URL, format, structure)` - devuelve una tabla creada a partir del `URL` con dado
-`format` y `structure`.
-
-URL - Dirección de servidor HTTP o HTTPS, que puede aceptar `GET` y/o `POST` peticiones.
-
-formato - [formato](../../interfaces/formats.md#formats) de los datos.
-
-estructura - estructura de la tabla en `'UserID UInt64, Name String'` formato. Determina los nombres y tipos de columna.
-
-**Ejemplo**
-
-``` sql
--- getting the first 3 lines of a table that contains columns of String and UInt32 type from HTTP-server which answers in CSV format.
-SELECT * FROM url('http://127.0.0.1:12345/', CSV, 'column1 String, column2 UInt32') LIMIT 3
-```
-
-[Artículo Original](https://clickhouse.tech/docs/en/query_language/table_functions/url/) <!--hide-->
diff --git a/docs/es/whats-new/changelog/2017.md b/docs/es/whats-new/changelog/2017.md
deleted file mode 120000
index d581cbbb4222..000000000000
--- a/docs/es/whats-new/changelog/2017.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/2017.md
\ No newline at end of file
diff --git a/docs/es/whats-new/changelog/2018.md b/docs/es/whats-new/changelog/2018.md
deleted file mode 120000
index 22874fcae857..000000000000
--- a/docs/es/whats-new/changelog/2018.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/2018.md
\ No newline at end of file
diff --git a/docs/es/whats-new/changelog/2019.md b/docs/es/whats-new/changelog/2019.md
deleted file mode 120000
index 0f3f095f8a16..000000000000
--- a/docs/es/whats-new/changelog/2019.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/2019.md
\ No newline at end of file
diff --git a/docs/es/whats-new/changelog/index.md b/docs/es/whats-new/changelog/index.md
deleted file mode 120000
index 5461b93ec8c1..000000000000
--- a/docs/es/whats-new/changelog/index.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/index.md
\ No newline at end of file
diff --git a/docs/es/whats-new/index.md b/docs/es/whats-new/index.md
deleted file mode 100644
index f1336b241ec6..000000000000
--- a/docs/es/whats-new/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Qu\xE9 hay de Nuevo"
-toc_priority: 72
----
-
-
diff --git a/docs/es/whats-new/roadmap.md b/docs/es/whats-new/roadmap.md
deleted file mode 100644
index d32833b7d142..000000000000
--- a/docs/es/whats-new/roadmap.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 74
-toc_title: Hoja de ruta
----
-
-# Hoja de ruta {#roadmap}
-
-## Q1 2020 {#q1-2020}
-
--   Control de acceso basado en roles
-
-## Q2 2020 {#q2-2020}
-
--   Integración con servicios de autenticación externos
--   Grupos de recursos para una distribución más precisa de la capacidad del clúster entre los usuarios
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/roadmap/) ##}
diff --git a/docs/es/whats-new/security-changelog.md b/docs/es/whats-new/security-changelog.md
deleted file mode 100644
index 86d2e7df2f73..000000000000
--- a/docs/es/whats-new/security-changelog.md
+++ /dev/null
@@ -1,76 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 76
-toc_title: Seguridad Changelog
----
-
-## Corregido en la versión de ClickHouse 19.14.3.3, 2019-09-10 {#fixed-in-clickhouse-release-19-14-3-3-2019-09-10}
-
-### CVE-2019-15024 {#cve-2019-15024}
-
-Аn attacker that has write access to ZooKeeper and who ican run a custom server available from the network where ClickHouse runs, can create a custom-built malicious server that will act as a ClickHouse replica and register it in ZooKeeper. When another replica will fetch data part from the malicious replica, it can force clickhouse-server to write to arbitrary path on filesystem.
-
-Créditos: Eldar Zaitov del equipo de seguridad de la información de Yandex
-
-### CVE-2019-16535 {#cve-2019-16535}
-
-Аn OOB read, OOB write and integer underflow in decompression algorithms can be used to achieve RCE or DoS via native protocol.
-
-Créditos: Eldar Zaitov del equipo de seguridad de la información de Yandex
-
-### CVE-2019-16536 {#cve-2019-16536}
-
-Un cliente autenticado malintencionado puede desencadenar el desbordamiento de pila que conduce a DoS.
-
-Créditos: Eldar Zaitov del equipo de seguridad de la información de Yandex
-
-## Corregido en la versión de ClickHouse 19.13.6.1, 2019-09-20 {#fixed-in-clickhouse-release-19-13-6-1-2019-09-20}
-
-### CVE-2019-18657 {#cve-2019-18657}
-
-Función de la tabla `url` la vulnerabilidad permitió al atacante inyectar encabezados HTTP arbitrarios en la solicitud.
-
-Crédito: [Nikita Tikhomirov](https://github.com/NSTikhomirov)
-
-## Corregido en la versión de ClickHouse 18.12.13, 2018-09-10 {#fixed-in-clickhouse-release-18-12-13-2018-09-10}
-
-### CVE-2018-14672 {#cve-2018-14672}
-
-Las funciones para cargar modelos CatBoost permitieron el recorrido de ruta y la lectura de archivos arbitrarios a través de mensajes de error.
-
-Créditos: Andrey Krasichkov del equipo de seguridad de la información de Yandex
-
-## Corregido en la versión de ClickHouse 18.10.3, 2018-08-13 {#fixed-in-clickhouse-release-18-10-3-2018-08-13}
-
-### CVE-2018-14671 {#cve-2018-14671}
-
-unixODBC permitía cargar objetos compartidos arbitrarios desde el sistema de archivos, lo que provocó una vulnerabilidad de ejecución remota de código.
-
-Créditos: Andrey Krasichkov y Evgeny Sidorov del equipo de seguridad de la información de Yandex
-
-## Corregido en la versión de ClickHouse 1.1.54388, 2018-06-28 {#fixed-in-clickhouse-release-1-1-54388-2018-06-28}
-
-### CVE-2018-14668 {#cve-2018-14668}
-
-“remote” función de tabla permitió símbolos arbitrarios en “user”, “password” y “default_database” campos que llevaron a ataques de falsificación de solicitudes de protocolo cruzado.
-
-Créditos: Andrey Krasichkov del equipo de seguridad de la información de Yandex
-
-## Corregido en la versión de ClickHouse 1.1.54390, 2018-07-06 {#fixed-in-clickhouse-release-1-1-54390-2018-07-06}
-
-### CVE-2018-14669 {#cve-2018-14669}
-
-ClickHouse cliente MySQL tenía “LOAD DATA LOCAL INFILE” funcionalidad habilitada que permitió a una base de datos MySQL maliciosa leer archivos arbitrarios desde el servidor ClickHouse conectado.
-
-Créditos: Andrey Krasichkov y Evgeny Sidorov del equipo de seguridad de la información de Yandex
-
-## Corregido en la versión de ClickHouse 1.1.54131, 2017-01-10 {#fixed-in-clickhouse-release-1-1-54131-2017-01-10}
-
-### CVE-2018-14670 {#cve-2018-14670}
-
-Una configuración incorrecta en el paquete deb podría conducir al uso no autorizado de la base de datos.
-
-Créditos: Centro Nacional de Seguridad Cibernética del Reino Unido (NCSC)
-
-{## [Artículo Original](https://clickhouse.tech/docs/en/security_changelog/) ##}
diff --git a/docs/fr/commercial/cloud.md b/docs/fr/commercial/cloud.md
deleted file mode 100644
index 13013065bfe6..000000000000
--- a/docs/fr/commercial/cloud.md
+++ /dev/null
@@ -1,23 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 1
-toc_title: Nuage
----
-
-# Fournisseurs De Services Cloud ClickHouse {#clickhouse-cloud-service-providers}
-
-!!! info "Info"
-    Si vous avez lancé un cloud public avec un service clickhouse géré, n'hésitez pas à [ouvrir une demande d'extraction](https://github.com/ClickHouse/ClickHouse/edit/master/docs/en/commercial/cloud.md) ajouter à la liste suivante.
-
-## Yandex Cloud {#yandex-cloud}
-
-[Service géré Yandex pour ClickHouse](https://cloud.yandex.com/services/managed-clickhouse?utm_source=referrals&utm_medium=clickhouseofficialsite&utm_campaign=link3) offre les fonctionnalités suivantes:
-
--   Service ZooKeeper entièrement géré pour [Réplication de ClickHouse](../engines/table-engines/mergetree-family/replication.md)
--   Choix multiples de type de stockage
--   Répliques dans différentes zones de disponibilité
--   Le chiffrement et l'isolement
--   Automatisation de la maintenance
-
-{## [Article Original](https://clickhouse.tech/docs/en/commercial/cloud/) ##}
diff --git a/docs/fr/commercial/index.md b/docs/fr/commercial/index.md
deleted file mode 100644
index 388f9a47fc80..000000000000
--- a/docs/fr/commercial/index.md
+++ /dev/null
@@ -1,9 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Commercial
-toc_priority: 70
-toc_title: Commercial
----
-
-
diff --git a/docs/fr/commercial/support.md b/docs/fr/commercial/support.md
deleted file mode 100644
index f5ce091272f2..000000000000
--- a/docs/fr/commercial/support.md
+++ /dev/null
@@ -1,23 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 3
-toc_title: Soutien
----
-
-# Fournisseurs De Services De Soutien Commercial ClickHouse {#clickhouse-commercial-support-service-providers}
-
-!!! info "Info"
-    Si vous avez lancé un service de support commercial ClickHouse, n'hésitez pas à [ouvrir une demande d'extraction](https://github.com/ClickHouse/ClickHouse/edit/master/docs/en/commercial/support.md) ajouter à la liste suivante.
-
-## Altinity {#altinity}
-
-Altinity offre le support et les services de ClickHouse d'entreprise depuis 2017. Les clients d'Altinity vont des entreprises Fortune 100 aux startups. Visiter [www.altinity.com](https://www.altinity.com/) pour plus d'informations.
-
-## Mafiree {#mafiree}
-
-[Description du Service](http://mafiree.com/clickhouse-analytics-services.php)
-
-## MinervaDB {#minervadb}
-
-[Description du Service](https://minervadb.com/index.php/clickhouse-consulting-and-support-by-minervadb/)
diff --git a/docs/fr/development/architecture.md b/docs/fr/development/architecture.md
deleted file mode 100644
index 7c105d430f61..000000000000
--- a/docs/fr/development/architecture.md
+++ /dev/null
@@ -1,203 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 62
-toc_title: Vue d'ensemble de L'Architecture ClickHouse
----
-
-# Vue d'ensemble de L'Architecture ClickHouse {#overview-of-clickhouse-architecture}
-
-ClickHouse est un véritable SGBD orienté colonne. Les données sont stockées par colonnes et lors de l'exécution de tableaux (vecteurs ou morceaux de colonnes). Dans la mesure du possible, les opérations sont distribuées sur des tableaux, plutôt que sur des valeurs individuelles. Il est appelé “vectorized query execution,” et cela aide à réduire le coût du traitement des données réel.
-
-> Cette idée n'est pas nouvelle. Il remonte à la `APL` langage de programmation et ses descendants: `A +`, `J`, `K`, et `Q`. La programmation de tableau est utilisée dans le traitement des données scientifiques. Cette idée n'est pas non plus nouvelle dans les bases de données relationnelles: par exemple, elle est utilisée dans le `Vectorwise` système.
-
-Il existe deux approches différentes pour accélérer le traitement des requêtes: l'exécution vectorisée des requêtes et la génération de code d'exécution. Ce dernier supprime toute indirection et expédition dynamique. Aucune de ces approches est strictement meilleure que l'autre. La génération de code d'exécution peut être meilleure lorsqu'elle fusionne de nombreuses opérations, utilisant ainsi pleinement les unités D'exécution du processeur et le pipeline. L'exécution de requête vectorisée peut être moins pratique car elle implique des vecteurs temporaires qui doivent être écrits dans le cache et lus. Si les données temporaires ne rentre pas dans le cache L2, cela devient un problème. Mais l'exécution de requête vectorisée utilise plus facilement les capacités SIMD de la CPU. Un [document de recherche](http://15721.courses.cs.cmu.edu/spring2016/papers/p5-sompolski.pdf) écrit par nos amis montre qu'il est préférable de combiner les deux approches. ClickHouse utilise l'exécution de requête vectorisée et a un support initial limité pour la génération de code d'exécution.
-
-## Colonne {#columns}
-
-`IColumn` l'interface est utilisée pour représenter des colonnes en mémoire (en fait, des morceaux de colonnes). Cette interface fournit des méthodes d'aide pour la mise en œuvre de divers opérateurs relationnels. Presque toutes les opérations sont immuables: elles ne modifient pas la colonne d'origine, mais en créent une nouvelle modifiée. Par exemple, l' `IColumn :: filter` méthode accepte un masque d'octet de filtre. Il est utilisé pour le `WHERE` et `HAVING` opérateurs relationnels. Exemples supplémentaires: `IColumn :: permute` méthode de soutien `ORDER BY`, le `IColumn :: cut` méthode de soutien `LIMIT`.
-
-Divers `IColumn` application (`ColumnUInt8`, `ColumnString` et ainsi de suite) sont responsables de la mémoire disposition de colonnes. La disposition de la mémoire est généralement un tableau contigu. Pour le type entier de colonnes, c'est juste un contiguë tableau, comme `std :: vector`. Pour `String` et `Array` colonnes, il s'agit de deux vecteurs: Un pour tous les éléments du tableau, placé de manière contiguë, et un second pour les décalages au début de chaque tableau. Il y a aussi `ColumnConst` cela stocke une seule valeur en mémoire, mais ressemble à une colonne.
-
-## Champ {#field}
-
-Néanmoins, il est possible de travailler avec des valeurs individuelles ainsi. Pour représenter une valeur individuelle, la `Field` est utilisée. `Field` est juste une union discriminée de `UInt64`, `Int64`, `Float64`, `String` et `Array`. `IColumn` a l' `operator[]` méthode pour obtenir la n-ème valeur en tant que `Field` et la `insert` méthode pour ajouter un `Field` à la fin d'une colonne. Ces méthodes ne sont pas très efficaces, car ils nécessitent de traiter avec temporaire `Field` des objets représentant une valeur individuelle. Il existe des méthodes plus efficaces, telles que `insertFrom`, `insertRangeFrom` et ainsi de suite.
-
-`Field` ne pas avoir assez d'informations sur un type de données spécifique pour une table. Exemple, `UInt8`, `UInt16`, `UInt32`, et `UInt64` tous sont représentés comme `UInt64` dans un `Field`.
-
-## Abstractions Qui Fuient {#leaky-abstractions}
-
-`IColumn` a des méthodes pour les transformations relationnelles communes des données, mais elles ne répondent pas à tous les besoins. Exemple, `ColumnUInt64` ne pas avoir une méthode pour calculer la somme des deux colonnes, et `ColumnString` n'a pas de méthode pour exécuter une recherche de sous-chaîne. Ces innombrables routines sont mises en œuvre en dehors de `IColumn`.
-
-Diverses fonctions sur les colonnes peuvent être implémentées de manière générique et non efficace en utilisant `IColumn` méthodes pour extraire `Field` valeurs, ou d'une manière spécialisée en utilisant la connaissance de la disposition de la mémoire interne des données dans un `IColumn` application. Il est implémenté en lançant des fonctions à un `IColumn` tapez et traitez directement la représentation interne. Exemple, `ColumnUInt64` a l' `getData` méthode qui renvoie une référence à un tableau interne, puis une autre routine lit ou remplit ce tableau directement. Nous avons “leaky abstractions” permettent de spécialisations diverses routines.
-
-## Types De Données {#data_types}
-
-`IDataType` est responsable de la sérialisation et de la désérialisation: pour la lecture et l'écriture de morceaux de colonnes ou de valeurs individuelles sous forme binaire ou de texte. `IDataType` correspond directement aux types de données dans les tables. Par exemple, il y a `DataTypeUInt32`, `DataTypeDateTime`, `DataTypeString` et ainsi de suite.
-
-`IDataType` et `IColumn` ne sont que faiblement liés les uns aux autres. Différents types de données peuvent être représentés en mémoire par le même `IColumn` application. Exemple, `DataTypeUInt32` et `DataTypeDateTime` sont tous deux représentés par `ColumnUInt32` ou `ColumnConstUInt32`. En outre, le même type de données peut être représentée par différents `IColumn` application. Exemple, `DataTypeUInt8` peut être représenté par `ColumnUInt8` ou `ColumnConstUInt8`.
-
-`IDataType` stocke uniquement les métadonnées. Par exemple, `DataTypeUInt8` ne stocke rien du tout (sauf vptr) et `DataTypeFixedString` magasins juste `N` (la taille des chaînes de taille fixe).
-
-`IDataType` a des méthodes d'aide pour différents formats de données. Des exemples sont des méthodes pour sérialiser une valeur avec des guillemets possibles, pour sérialiser une valeur pour JSON et pour sérialiser une valeur dans le format XML. Il n'y a pas de correspondance directe avec les formats de données. Par exemple, les différents formats de données `Pretty` et `TabSeparated` pouvez utiliser le même `serializeTextEscaped` méthode d'aide à partir de la `IDataType` interface.
-
-## Bloc {#block}
-
-A `Block` est un conteneur qui représente un sous-ensemble (morceau) d'une table en mémoire. C'est juste un ensemble de triplets: `(IColumn, IDataType, column name)`. Pendant l'exécution de la requête, les données sont traitées par `Block`s. Si nous avons un `Block`, nous disposons de données (dans le `IColumn` objet), nous avons des informations sur son type (dans `IDataType`) qui nous indique comment traiter cette colonne, et nous avons le nom de la colonne. Il peut s'agir du nom de colonne d'origine de la table ou d'un nom artificiel attribué pour obtenir des résultats temporaires de calculs.
-
-Lorsque nous calculons une fonction sur des colonnes dans un bloc, nous ajoutons une autre colonne avec son résultat au bloc, et nous ne touchons pas les colonnes pour les arguments de la fonction car les opérations sont immuables. Plus tard, les colonnes inutiles peuvent être supprimées du bloc, mais pas modifiées. Il est pratique pour l'élimination des sous-expressions communes.
-
-Des blocs sont créés pour chaque bloc de données traité. Notez que pour le même type de calcul, les noms et les types de colonnes restent les mêmes pour différents blocs, et seules les données de colonne changent. Il est préférable de diviser les données de bloc de l'en-tête de bloc car les petites tailles de Bloc ont une surcharge élevée de chaînes temporaires pour copier shared_ptrs et les noms de colonnes.
-
-## Bloquer Les Flux {#block-streams}
-
-Les flux de blocs sont destinés au traitement des données. Nous utilisons des flux de blocs pour lire des données quelque part, effectuer des transformations de données ou écrire des données quelque part. `IBlockInputStream` a l' `read` méthode pour récupérer le bloc suivant, tandis que des. `IBlockOutputStream` a l' `write` méthode pour pousser le bloc quelque part.
-
-Les flux sont responsables de:
-
-1.  De la lecture ou de l'écriture dans une table. La table renvoie simplement un flux pour lire ou écrire des blocs.
-2.  Mise en œuvre des formats de données. Par exemple, si vous souhaitez envoyer des données vers un terminal `Pretty` format, vous créez un flux de sortie de bloc où vous poussez des blocs, et il les formate.
-3.  Effectuer des transformations de données. Disons que vous avez `IBlockInputStream` et veulent créer un flux filtré. Vous créez `FilterBlockInputStream` et l'initialiser avec votre flux de données. Puis quand vous tirez un bloc de `FilterBlockInputStream`, il extrait un bloc de votre flux, le filtre et vous renvoie le bloc filtré. Les pipelines d'exécution des requêtes sont représentés de cette façon.
-
-Il y a des transformations plus sophistiquées. Par exemple, lorsque vous tirez de `AggregatingBlockInputStream` il lit toutes les données à partir de sa source, agrégats, puis renvoie un flux de données agrégées pour vous. Un autre exemple: `UnionBlockInputStream` accepte de nombreuses sources d'entrée dans le constructeur et également un certain nombre de threads. Il lance plusieurs threads et lit à partir de plusieurs sources en parallèle.
-
-> Les flux de blocs utilisent le “pull” approche pour contrôler le flux: lorsque vous extrayez un bloc du premier flux, il extrait par conséquent les blocs requis des flux imbriqués, et l'ensemble du pipeline d'exécution fonctionnera. Ni “pull” ni “push” est la meilleure solution, car le flux de contrôle est implicite, ce qui limite l'implémentation de diverses fonctionnalités telles que l'exécution simultanée de plusieurs requêtes (fusion de plusieurs pipelines ensemble). Cette limitation pourrait être surmontée avec des coroutines ou simplement en exécutant des threads supplémentaires qui s'attendent les uns aux autres. Nous pouvons avoir plus de possibilités si nous rendons le flux de contrôle explicite: si nous localisons la logique pour passer des données d'une unité de calcul à une autre en dehors de ces unités de calcul. Lire ce [article](http://journal.stuffwithstuff.com/2013/01/13/iteration-inside-and-out/) pour plus de pensées.
-
-Il convient de noter que le pipeline d'exécution de la requête crée des données temporaires à chaque étape. Nous essayons de garder la taille du bloc suffisamment petite pour que les données temporaires tiennent dans le cache du processeur. Avec cette hypothèse, l'écriture et la lecture de données temporaires sont presque libres en comparaison avec d'autres calculs. Nous pourrions envisager une alternative, qui est de fusionner de nombreuses opérations dans le pipeline ensemble. Cela pourrait rendre le pipeline aussi court que possible et supprimer une grande partie des données temporaires, ce qui pourrait être un avantage, mais cela présente également des inconvénients. Par exemple, un pipeline divisé facilite l'implémentation de la mise en cache de données intermédiaires, le vol de données intermédiaires à partir de requêtes similaires exécutées en même temps et la fusion de pipelines pour des requêtes similaires.
-
-## Format {#formats}
-
-Les formats de données sont implémentés avec des flux de blocs. Il y a “presentational” formats appropriés uniquement pour la sortie de données vers le client, tels que `Pretty` format, qui fournit seulement `IBlockOutputStream`. Et il existe des formats d'entrée / sortie, tels que `TabSeparated` ou `JSONEachRow`.
-
-Il y a aussi des flux de lignes: `IRowInputStream` et `IRowOutputStream`. Ils vous permettent de tirer/pousser des données par des lignes individuelles, pas par des blocs. Et ils ne sont nécessaires que pour simplifier la mise en œuvre des formats orientés ligne. Wrapper `BlockInputStreamFromRowInputStream` et `BlockOutputStreamFromRowOutputStream` vous permet de convertir des flux orientés ligne en flux orientés blocs réguliers.
-
-## I/O {#io}
-
-Pour l'entrée/sortie orientée octet, il y a `ReadBuffer` et `WriteBuffer` les classes abstraites. Ils sont utilisés à la place de C++ `iostream`s. Ne vous inquiétez pas: chaque projet c++ mature utilise autre chose que `iostream`s pour de bonnes raisons.
-
-`ReadBuffer` et `WriteBuffer` sont juste un tampon contigu et un curseur pointant vers la position dans ce tampon. Les implémentations peuvent posséder ou non la mémoire du tampon. Il existe une méthode virtuelle pour remplir le tampon avec les données suivantes (pour `ReadBuffer`) ou pour vider le tampon quelque part (pour `WriteBuffer`). Les méthodes virtuelles sont rarement cités.
-
-Les implémentations de `ReadBuffer`/`WriteBuffer` sont utilisés pour travailler avec des fichiers et des descripteurs de fichiers et des sockets réseau, pour implémenter la compression (`CompressedWriteBuffer` is initialized with another WriteBuffer and performs compression before writing data to it), and for other purposes – the names `ConcatReadBuffer`, `LimitReadBuffer`, et `HashingWriteBuffer` parler pour eux-mêmes.
-
-Read / WriteBuffers ne traite que les octets. Il y a des fonctions de `ReadHelpers` et `WriteHelpers` fichiers d'en-tête pour aider à formater l'entrée / sortie. Par exemple, il existe des assistants pour écrire un nombre au format décimal.
-
-Regardons ce qui se passe lorsque vous voulez écrire un ensemble de résultats dans `JSON` format de sortie standard (stdout). Vous avez un jeu de résultats prêt à être récupéré `IBlockInputStream`. Vous créez `WriteBufferFromFileDescriptor(STDOUT_FILENO)` pour écrire des octets dans stdout. Vous créez `JSONRowOutputStream`, initialisé avec qui `WriteBuffer`, pour écrire des lignes dans `JSON` à stdout. Vous créez `BlockOutputStreamFromRowOutputStream` de plus, pour la représenter comme `IBlockOutputStream`. Ensuite, vous appelez `copyData` pour transférer des données de `IBlockInputStream` de `IBlockOutputStream` et tout fonctionne. Interne, `JSONRowOutputStream` écrira divers délimiteurs JSON et appellera `IDataType::serializeTextJSON` méthode avec une référence à `IColumn` et le numéro de ligne comme arguments. Conséquent, `IDataType::serializeTextJSON` appellera une méthode de `WriteHelpers.h`: exemple, `writeText` pour les types numériques et `writeJSONString` pour `DataTypeString`.
-
-## Table {#tables}
-
-Le `IStorage` l'interface représente les tables. Différentes implémentations de cette interface sont des moteurs de table différents. Les exemples sont `StorageMergeTree`, `StorageMemory` et ainsi de suite. Les Instances de ces classes ne sont que des tables.
-
-Clé `IStorage` les méthodes sont `read` et `write`. Il y a aussi des `alter`, `rename`, `drop` et ainsi de suite. Le `read` méthode accepte les arguments suivants: l'ensemble de colonnes à lire à partir d'un tableau, l' `AST` requête à considérer, et le nombre souhaité de flux de retour. Il renvoie un ou plusieurs `IBlockInputStream` objets et informations sur l'étape de traitement des données qui a été effectuée dans un moteur de table lors de l'exécution de la requête.
-
-Dans la plupart des cas, la méthode read n'est responsable que de la lecture des colonnes spécifiées à partir d'une table, et non d'un traitement ultérieur des données. Tout traitement ultérieur des données est effectué par l'interpréteur de requêtes et n'est pas de la responsabilité de `IStorage`.
-
-Mais il y a des exceptions notables:
-
--   La requête AST est transmise au `read` et le moteur de table peut l'utiliser pour dériver l'utilisation de l'index et pour lire moins de données à partir d'une table.
--   Parfois, le moteur de table peut traiter les données lui-même à une étape spécifique. Exemple, `StorageDistributed` peut envoyer une requête aux serveurs distants, leur demander de traiter les données à une étape où les données de différents serveurs distants peuvent être fusionnées, et renvoyer ces données prétraitées. L'interpréteur de requête termine ensuite le traitement des données.
-
-Table `read` la méthode peut retourner plusieurs `IBlockInputStream` objets permettant le traitement parallèle des données. Ces flux d'entrée de bloc multiples peuvent lire à partir d'une table en parallèle. Ensuite, vous pouvez envelopper ces flux avec diverses transformations (telles que l'évaluation d'expression ou le filtrage) qui peuvent être calculées indépendamment et créer un `UnionBlockInputStream` en plus d'eux, pour lire à partir de plusieurs flux en parallèle.
-
-Il y a aussi des `TableFunction`s. Ce sont des fonctions qui renvoient un `IStorage` objet à utiliser dans le `FROM` la clause d'une requête.
-
-Pour avoir une idée rapide de la façon d'implémenter votre moteur de table, regardez quelque chose de simple, comme `StorageMemory` ou `StorageTinyLog`.
-
-> Comme le résultat de l' `read` méthode, `IStorage` retourner `QueryProcessingStage` – information about what parts of the query were already calculated inside storage.
-
-## Analyseur {#parsers}
-
-Un analyseur de descente récursif écrit à la main analyse une requête. Exemple, `ParserSelectQuery` appelle simplement récursivement les analyseurs sous-jacents pour diverses parties de la requête. Les analyseurs créent un `AST`. Le `AST` est représenté par des nœuds, qui sont des instances de `IAST`.
-
-> Les générateurs d'analyseurs ne sont pas utilisés pour des raisons historiques.
-
-## Interprète {#interpreters}
-
-Les interprètes sont responsables de la création du pipeline d'exécution des requêtes à partir `AST`. Il existe des interprètes simples, tels que `InterpreterExistsQuery` et `InterpreterDropQuery` ou le plus sophistiqué de `InterpreterSelectQuery`. Le pipeline d'exécution de requête est une combinaison de flux d'entrée ou de sortie de bloc. Par exemple, le résultat de l'interprétation de la `SELECT` la requête est la `IBlockInputStream` pour lire le jeu de résultats; le résultat de la requête d'INSERTION est l' `IBlockOutputStream` pour écrire des données à insérer, et le résultat de l'interprétation `INSERT SELECT` la requête est la `IBlockInputStream` cela renvoie un jeu de résultats vide lors de la première lecture, mais qui copie `SELECT` de `INSERT` dans le même temps.
-
-`InterpreterSelectQuery` utiliser `ExpressionAnalyzer` et `ExpressionActions` machines pour l'analyse des requêtes et des transformations. C'est là que la plupart des optimisations de requêtes basées sur des règles sont effectuées. `ExpressionAnalyzer` est assez désordonné et devrait être réécrit: diverses transformations et optimisations de requête doivent être extraites dans des classes séparées pour permettre des transformations modulaires ou une requête.
-
-## Fonction {#functions}
-
-Il y a des fonctions ordinaires et des fonctions agrégées. Pour les fonctions d'agrégation, voir la section suivante.
-
-Ordinary functions don't change the number of rows – they work as if they are processing each row independently. In fact, functions are not called for individual rows, but for `Block`'s de données pour implémenter l'exécution de requête vectorisée.
-
-Il y a quelques fonctions diverses, comme [la taille de bloc](../sql-reference/functions/other-functions.md#function-blocksize), [rowNumberInBlock](../sql-reference/functions/other-functions.md#function-rownumberinblock), et [runningAccumulate](../sql-reference/functions/other-functions.md#function-runningaccumulate), qui exploitent le traitement de bloc et violent l'indépendance des lignes.
-
-ClickHouse a un typage fort, donc il n'y a pas de conversion de type implicite. Si une fonction ne prend pas en charge une combinaison spécifique de types, elle lève une exception. Mais les fonctions peuvent fonctionner (être surchargées) pour de nombreuses combinaisons de types différentes. Par exemple, l' `plus` fonction (pour mettre en œuvre la `+` opérateur) fonctionne pour toute combinaison de types numériques: `UInt8` + `Float32`, `UInt16` + `Int8` et ainsi de suite. En outre, certaines fonctions variadiques peuvent accepter n'importe quel nombre d'arguments, tels que `concat` fonction.
-
-L'implémentation d'une fonction peut être légèrement gênante car une fonction distribue explicitement les types de données pris en charge et pris en charge `IColumns`. Par exemple, l' `plus` la fonction a du code généré par l'instanciation D'un modèle C++ pour chaque combinaison de types numériques, et des arguments gauche et droit constants ou non constants.
-
-C'est un excellent endroit pour implémenter la génération de code d'exécution pour éviter le gonflement du code de modèle. En outre, il permet d'ajouter des fonctions fusionnées comme Fusionné Multiplier-Ajouter ou de faire plusieurs comparaisons dans une itération de boucle.
-
-En raison de l'exécution de requête vectorisée, les fonctions ne sont pas court-circuitées. Par exemple, si vous écrivez `WHERE f(x) AND g(y)` les deux faces sont calculés, même pour les lignes, quand `f(x)` est égal à zéro (sauf quand `f(x)` est une expression constante nulle). Mais si la sélectivité de l' `f(x)` la condition est élevée, et le calcul de `f(x)` est beaucoup moins cher que `g(y)`, il est préférable d'implémenter le calcul multi-pass. Il serait d'abord calculer `f(x)` puis filtrer les colonnes par la suite, puis de calculer `g(y)` uniquement pour les petits morceaux de données filtrés.
-
-## Les Fonctions D'Agrégation {#aggregate-functions}
-
-Les fonctions d'agrégation sont des fonctions avec État. Ils accumulent les valeurs passées dans certains etats et vous permettent d'obtenir des résultats de cet état. Ils sont gérés avec le `IAggregateFunction` interface. Les États peuvent être assez simples (l'État pour `AggregateFunctionCount` est juste un seul `UInt64` valeur) ou très complexes (l'état de `AggregateFunctionUniqCombined` est une combinaison linéaire du tableau, d'une table de hachage, et un `HyperLogLog` structure probabiliste des données).
-
-Les États sont répartis en `Arena` (un pool de mémoire) pour traiter plusieurs états lors de l'exécution d'une cardinalité élevée `GROUP BY` requête. Les États peuvent avoir un constructeur et un destructeur non triviaux: par exemple, les États d'agrégation compliqués peuvent allouer eux-mêmes de la mémoire supplémentaire. Il faut accorder une certaine attention à la création et à la destruction des États et à la transmission appropriée de leur propriété et de leur ordre de destruction.
-
-Les États d'agrégation peuvent être sérialisés et désérialisés pour passer sur le réseau pendant l'exécution de la requête distribuée ou pour les écrire sur le disque où il n'y a pas assez de RAM. Ils peuvent même être stockés dans une table avec le `DataTypeAggregateFunction` pour permettre l'agrégation incrémentielle des données.
-
-> Le format de données sérialisé pour les états de fonction d'agrégat n'est pas versionné pour le moment. C'est ok si les États d'agrégat ne sont stockés que temporairement. Mais nous avons l' `AggregatingMergeTree` moteur de table pour l'agrégation incrémentielle, et les gens l'utilisent déjà en production. C'est la raison pour laquelle la rétrocompatibilité est requise lors de la modification du format sérialisé pour toute fonction d'agrégat à l'avenir.
-
-## Serveur {#server}
-
-Le serveur implémente plusieurs interfaces différentes:
-
--   Une interface HTTP pour tous les clients étrangers.
--   Une interface TCP pour le client clickhouse natif et pour la communication inter-serveur lors de l'exécution de la requête distribuée.
--   Une interface pour transférer des données pour la réplication.
-
-En interne, il s'agit simplement d'un serveur multithread primitif sans coroutines ni fibres. Étant donné que le serveur n'est pas conçu pour traiter un taux élevé de requêtes simples, mais pour traiter un taux relativement faible de requêtes complexes, chacun d'eux peut traiter une grande quantité de données à des fins d'analyse.
-
-Le serveur initialise le `Context` classe avec l'environnement nécessaire à l'exécution des requêtes: la liste des bases de données disponibles, des utilisateurs et des droits d'accès, des paramètres, des clusters, la liste des processus, le journal des requêtes, etc. Les interprètes utilisent cet environnement.
-
-Nous maintenons une compatibilité ascendante et descendante complète pour le protocole TCP du serveur: les anciens clients peuvent parler à de nouveaux serveurs, et les nouveaux clients peuvent parler à d'anciens serveurs. Mais nous ne voulons pas le maintenir éternellement, et nous supprimons le support pour les anciennes versions après environ un an.
-
-!!! note "Note"
-    Pour la plupart des applications externes, nous vous recommandons d'utiliser L'interface HTTP car elle est simple et facile à utiliser. Le protocole TCP est plus étroitement lié aux structures de données internes: il utilise un format interne pour passer des blocs de données, et il utilise un cadrage personnalisé pour les données compressées. Nous n'avons pas publié de bibliothèque C pour ce protocole car elle nécessite de lier la plupart de la base de code ClickHouse, ce qui n'est pas pratique.
-
-## Exécution De Requête Distribuée {#distributed-query-execution}
-
-Les serveurs d'une configuration de cluster sont pour la plupart indépendants. Vous pouvez créer un `Distributed` table sur un ou tous les serveurs dans un cluster. Le `Distributed` table does not store data itself – it only provides a “view” à toutes les tables sur plusieurs nœuds d'un cluster. Lorsque vous sélectionnez à partir d'un `Distributed` table, il réécrit cette requête, choisit les nœuds distants en fonction des paramètres d'équilibrage de charge et leur envoie la requête. Le `Distributed` table demande aux serveurs distants de traiter une requête jusqu'à une étape où les résultats intermédiaires de différents serveurs peuvent être fusionnés. Puis il reçoit les résultats intermédiaires et les fusionne. La table distribuée essaie de distribuer autant de travail que possible aux serveurs distants et n'envoie pas beaucoup de données intermédiaires sur le réseau.
-
-Les choses deviennent plus compliquées lorsque vous avez des sous-requêtes dans des clauses IN ou JOIN, et que chacune d'elles utilise un `Distributed` table. Nous avons différentes stratégies pour l'exécution de ces requêtes.
-
-Il n'existe pas de plan de requête global pour l'exécution des requêtes distribuées. Chaque nœud a son plan de requête local pour sa partie du travail. Nous n'avons qu'une simple exécution de requête distribuée en une seule passe: nous envoyons des requêtes pour les nœuds distants, puis fusionnons les résultats. Mais cela n'est pas possible pour les requêtes compliquées avec des groupes de cardinalité élevés ou avec une grande quantité de données temporaires pour la jointure. Dans de tels cas, nous avons besoin de “reshuffle” données entre les serveurs, ce qui nécessite une coordination supplémentaire. ClickHouse ne supporte pas ce type d'exécution de requête, et nous devons y travailler.
-
-## Fusion De L'Arbre {#merge-tree}
-
-`MergeTree` est une famille de moteurs de stockage qui prend en charge l'indexation par clé primaire. La clé primaire peut être un tuple arbitraire de colonnes ou d'expressions. De données dans un `MergeTree` la table est stockée dans “parts”. Chaque partie stocke les données dans l'ordre de la clé primaire, de sorte que les données sont ordonnées lexicographiquement par le tuple de clé primaire. Toutes les colonnes du tableau sont stockés dans différents `column.bin` les fichiers dans ces régions. Les fichiers sont constitués de blocs compressés. Chaque bloc est généralement de 64 KO à 1 Mo de données non compressées, en fonction de la taille de la valeur moyenne. Les blocs sont constitués de valeurs de colonne placées de manière contiguë l'une après l'autre. Les valeurs de colonne sont dans le même ordre pour chaque colonne (la clé primaire définit l'ordre), donc lorsque vous itérez par plusieurs colonnes, vous obtenez des valeurs pour les lignes correspondantes.
-
-La clé primaire elle-même est “sparse”. Il ne traite pas chaque ligne, mais seulement certaines plages de données. Séparé `primary.idx` fichier a la valeur de la clé primaire pour chaque N-ième ligne, où N est appelé `index_granularity` (habituellement, N = 8192). Aussi, pour chaque colonne, nous avons `column.mrk` les fichiers avec l' “marks,” qui sont des décalages à chaque N-ème ligne dans le fichier de données. Chaque marque est une paire: le décalage dans le fichier au début du bloc compressé, et le décalage dans le bloc décompressé au début des données. Habituellement, les blocs compressés sont alignés par des marques, et le décalage dans le bloc décompressé est nul. Les données pour `primary.idx` réside toujours dans la mémoire, et les données pour `column.mrk` les fichiers sont mis en cache.
-
-Quand nous allons lire quelque chose d'une partie dans `MergeTree` nous regardons `primary.idx` données et locate plages qui pourraient contenir des données demandées, puis regardez `column.mrk` données et calculer des décalages pour savoir où commencer à lire ces plages. En raison de la rareté, les données excédentaires peuvent être lues. ClickHouse ne convient pas à une charge élevée de requêtes ponctuelles simples, car toute la gamme avec `index_granularity` les lignes doivent être lues pour chaque clé, et le bloc compressé entier doit être décompressé pour chaque colonne. Nous avons rendu l'index clairsemé parce que nous devons être en mesure de maintenir des milliards de lignes par serveur unique sans consommation de mémoire notable pour l'index. De plus, comme la clé primaire est clairsemée, elle n'est pas unique: elle ne peut pas vérifier l'existence de la clé dans la table au moment de l'insertion. Vous pourriez avoir plusieurs lignes avec la même clé dans une table.
-
-Lorsque vous `INSERT` un tas de données dans `MergeTree`, ce groupe est trié par ordre de clé primaire et forme une nouvelle partie. Il existe des threads d'arrière-plan qui sélectionnent périodiquement certaines parties et les fusionnent en une seule partie triée pour maintenir le nombre de parties relativement faible. C'est pourquoi il est appelé `MergeTree`. Bien sûr, la fusion conduit à “write amplification”. Toutes les parties sont immuables: elles sont seulement créées et supprimées, mais pas modifiées. Lorsque SELECT est exécuté, il contient un instantané de la table (un ensemble de parties). Après la Fusion, nous conservons également les anciennes pièces pendant un certain temps pour faciliter une récupération après une défaillance, donc si nous voyons qu'une partie fusionnée est probablement cassée, nous pouvons la remplacer par ses parties sources.
-
-`MergeTree` n'est pas un arbre LSM car il ne contient pas “memtable” et “log”: inserted data is written directly to the filesystem. This makes it suitable only to INSERT data in batches, not by individual row and not very frequently – about once per second is ok, but a thousand times a second is not. We did it this way for simplicity's sake, and because we are already inserting data in batches in our applications.
-
-> Les tables MergeTree ne peuvent avoir qu'un seul index (primaire): il n'y a pas d'index secondaires. Il serait bon d'autoriser plusieurs représentations physiques sous une table logique, par exemple, pour stocker des données dans plus d'un ordre physique ou même pour autoriser des représentations avec des données pré-agrégées avec des données originales.
-
-Il existe des moteurs MergeTree qui effectuent un travail supplémentaire lors des fusions en arrière-plan. Les exemples sont `CollapsingMergeTree` et `AggregatingMergeTree`. Cela pourrait être traité comme un support spécial pour les mises à jour. Gardez à l'esprit que ce ne sont pas de vraies mises à jour car les utilisateurs n'ont généralement aucun contrôle sur le moment où les fusions en arrière-plan sont exécutées et les données dans un `MergeTree` la table est presque toujours stockée dans plus d'une partie, pas sous une forme complètement fusionnée.
-
-## Réplication {#replication}
-
-La réplication dans ClickHouse peut être configurée sur une base par table. Vous pouvez avoir des tables répliquées et des tables non répliquées sur le même serveur. Vous pouvez également avoir des tables répliquées de différentes manières, comme une table avec une réplication à deux facteurs et une autre avec trois facteurs.
-
-La réplication est implémentée dans le `ReplicatedMergeTree` moteur de stockage. Le chemin d'accès dans `ZooKeeper` est spécifié comme paramètre pour le moteur de stockage. Toutes les tables avec le même chemin dans `ZooKeeper` devenez des répliques les unes des autres: elles synchronisent leurs données et maintiennent la cohérence. Les répliques peuvent être ajoutées et supprimées dynamiquement simplement en créant ou en supprimant une table.
-
-La réplication utilise un schéma multi-maître asynchrone. Vous pouvez insérer des données dans n'importe quel réplica qui a une session avec `ZooKeeper`, et les données sont répliquées à toutes les autres répliques de manière asynchrone. Parce que ClickHouse ne prend pas en charge les mises à jour, la réplication est sans conflit. Comme il n'y a pas d'accusé de réception de quorum des insertions, les données juste insérées peuvent être perdues si un nœud échoue.
-
-Les métadonnées pour la réplication sont stockées dans ZooKeeper. Il existe un journal de réplication qui répertorie les actions à effectuer. Les Actions sont: obtenir une partie; fusionner des parties; déposer une partition, et ainsi de suite. Chaque réplica copie le journal de réplication dans sa file d'attente, puis exécute les actions de la file d'attente. Par exemple, sur l'insertion, l' “get the part” l'action est créée dans le journal, et chaque réplique téléchargements de la partie. Les fusions sont coordonnées entre les répliques pour obtenir des résultats identiques aux octets. Toutes les parties sont fusionnées de la même manière sur toutes les répliques. Il est réalisé en élisant une réplique en tant que leader, et cette réplique initie fusionne et écrit “merge parts” actions dans le journal.
-
-La réplication est physique: seules les parties compressées sont transférées entre les nœuds, pas les requêtes. Les fusions sont traitées sur chaque réplique indépendamment dans la plupart des cas pour réduire les coûts du réseau en évitant l'amplification du réseau. Grand fusionné les pièces sont envoyées sur le réseau uniquement en cas de retard de réplication.
-
-En outre, chaque réplique stocke son état dans ZooKeeper comme l'ensemble des pièces et ses sommes de contrôle. Lorsque l'état sur le système de fichiers local diverge de l'état de référence dans ZooKeeper, le réplica restaure sa cohérence en téléchargeant les parties manquantes et brisées à partir d'autres réplicas. Lorsqu'il y a des données inattendues ou brisées dans le système de fichiers local, ClickHouse ne les supprime pas, mais les déplace dans un répertoire séparé et les oublie.
-
-!!! note "Note"
-    Le cluster ClickHouse est constitué de fragments indépendants, et chaque fragment est constitué de répliques. Le cluster est **pas élastique**, donc, après avoir ajouté un nouveau fragment, les données ne sont pas rééquilibrées automatiquement entre les fragments. Au lieu de cela, la charge du cluster est censée être ajustée pour être inégale. Cette implémentation vous donne plus de contrôle, et c'est ok pour des clusters relativement petits, tels que des dizaines de nœuds. Mais pour les clusters avec des centaines de nœuds que nous utilisons en production, cette approche devient un inconvénient important. Nous devrions implémenter un moteur de table qui s'étend sur le cluster avec des régions répliquées dynamiquement qui pourraient être divisées et équilibrées automatiquement entre les clusters.
-
-{## [Article Original](https://clickhouse.tech/docs/en/development/architecture/) ##}
diff --git a/docs/fr/development/browse-code.md b/docs/fr/development/browse-code.md
deleted file mode 100644
index ef8beb600c60..000000000000
--- a/docs/fr/development/browse-code.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 63
-toc_title: Parcourir Le Code Source
----
-
-# Parcourir Le Code Source De ClickHouse {#browse-clickhouse-source-code}
-
-Vous pouvez utiliser **Woboq** navigateur de code en ligne disponible [ici](https://clickhouse.tech/codebrowser/html_report/ClickHouse/src/index.html). Il fournit la navigation de code et la mise en évidence sémantique, la recherche et l'indexation. L'instantané de code est mis à jour quotidiennement.
-
-Aussi, vous pouvez parcourir les sources sur [GitHub](https://github.com/ClickHouse/ClickHouse) comme à l'habitude.
-
-Si vous êtes intéressé par L'IDE à utiliser, nous vous recommandons CLion, Qt Creator, VS Code et KDevelop (avec des mises en garde). Vous pouvez utiliser N'importe quel IDE préféré. Vim et Emacs comptent également.
diff --git a/docs/fr/development/build-cross-arm.md b/docs/fr/development/build-cross-arm.md
deleted file mode 100644
index 472862eabe59..000000000000
--- a/docs/fr/development/build-cross-arm.md
+++ /dev/null
@@ -1,43 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 67
-toc_title: Comment Construire ClickHouse sur Linux pour AARCH64 (ARM64)
----
-
-# Comment Construire ClickHouse sur Linux pour L'Architecture AARCH64 (ARM64)  {#how-to-build-clickhouse-on-linux-for-aarch64-arm64-architecture}
-
-C'est pour le cas où vous avez machine Linux et que vous voulez utiliser pour construire `clickhouse` binaire qui fonctionnera sur une autre machine Linux avec une architecture CPU AARCH64. Ceci est destiné aux contrôles d'intégration continus qui s'exécutent sur des serveurs Linux.
-
-La construction croisée pour AARCH64 est basée sur [Instructions de construction](build.md), suivez d'abord.
-
-# Installer Clang-8 {#install-clang-8}
-
-Suivez les instructions de https://apt.llvm.org/ pour votre configuration Ubuntu ou Debian.
-Par exemple, dans Ubuntu Bionic vous pouvez utiliser les commandes suivantes:
-
-``` bash
-echo "deb [trusted=yes] http://apt.llvm.org/bionic/ llvm-toolchain-bionic-8 main" | sudo tee /etc/apt/sources.list.d/llvm.list
-sudo apt-get update
-sudo apt-get install clang-8
-```
-
-# Installer Un Ensemble D'Outils De Compilation Croisée {#install-cross-compilation-toolset}
-
-``` bash
-cd ClickHouse
-mkdir -p build-aarch64/cmake/toolchain/linux-aarch64
-wget 'https://developer.arm.com/-/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz?revision=2e88a73f-d233-4f96-b1f4-d8b36e9bb0b9&la=en' -O gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz
-tar xJf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C build-aarch64/cmake/toolchain/linux-aarch64 --strip-components=1
-```
-
-# Construire ClickHouse {#build-clickhouse}
-
-``` bash
-cd ClickHouse
-mkdir build-arm64
-CC=clang-8 CXX=clang++-8 cmake . -Bbuild-arm64 -DCMAKE_TOOLCHAIN_FILE=cmake/linux/toolchain-aarch64.cmake
-ninja -C build-arm64
-```
-
-Le binaire résultant s'exécutera uniquement sur Linux avec l'architecture CPU AARCH64.
diff --git a/docs/fr/development/build-cross-osx.md b/docs/fr/development/build-cross-osx.md
deleted file mode 100644
index a15d4650edb9..000000000000
--- a/docs/fr/development/build-cross-osx.md
+++ /dev/null
@@ -1,64 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 66
-toc_title: Comment Construire ClickHouse sur Linux pour Mac OS X
----
-
-# Comment Construire ClickHouse sur Linux pour Mac OS X {#how-to-build-clickhouse-on-linux-for-mac-os-x}
-
-C'est pour le cas où vous avez machine Linux et que vous voulez utiliser pour construire `clickhouse` binaire qui s'exécutera sur OS X. Ceci est destiné aux contrôles d'intégration continus qui s'exécutent sur des serveurs Linux. Si vous voulez construire ClickHouse directement sur Mac OS X, puis procéder à [une autre instruction](build-osx.md).
-
-Le cross-build pour Mac OS X est basé sur le [Instructions de construction](build.md), suivez d'abord.
-
-# Installer Clang-8 {#install-clang-8}
-
-Suivez les instructions de https://apt.llvm.org/ pour votre configuration Ubuntu ou Debian.
-Par exemple les commandes pour Bionic sont comme:
-
-``` bash
-sudo echo "deb [trusted=yes] http://apt.llvm.org/bionic/ llvm-toolchain-bionic-8 main" >> /etc/apt/sources.list
-sudo apt-get install clang-8
-```
-
-# Installer Un Ensemble D'Outils De Compilation Croisée {#install-cross-compilation-toolset}
-
-Souvenons nous du chemin où nous installons `cctools` comme ${CCTOOLS}
-
-``` bash
-mkdir ${CCTOOLS}
-
-git clone https://github.com/tpoechtrager/apple-libtapi.git
-cd apple-libtapi
-INSTALLPREFIX=${CCTOOLS} ./build.sh
-./install.sh
-cd ..
-
-git clone https://github.com/tpoechtrager/cctools-port.git
-cd cctools-port/cctools
-./configure --prefix=${CCTOOLS} --with-libtapi=${CCTOOLS} --target=x86_64-apple-darwin
-make install
-```
-
-En outre, nous devons télécharger macOS X SDK dans l'arbre de travail.
-
-``` bash
-cd ClickHouse
-wget 'https://github.com/phracker/MacOSX-SDKs/releases/download/10.15/MacOSX10.15.sdk.tar.xz'
-mkdir -p build-darwin/cmake/toolchain/darwin-x86_64
-tar xJf MacOSX10.15.sdk.tar.xz -C build-darwin/cmake/toolchain/darwin-x86_64 --strip-components=1
-```
-
-# Construire ClickHouse {#build-clickhouse}
-
-``` bash
-cd ClickHouse
-mkdir build-osx
-CC=clang-8 CXX=clang++-8 cmake . -Bbuild-osx -DCMAKE_TOOLCHAIN_FILE=cmake/darwin/toolchain-x86_64.cmake \
-    -DCMAKE_AR:FILEPATH=${CCTOOLS}/bin/x86_64-apple-darwin-ar \
-    -DCMAKE_RANLIB:FILEPATH=${CCTOOLS}/bin/x86_64-apple-darwin-ranlib \
-    -DLINKER_NAME=${CCTOOLS}/bin/x86_64-apple-darwin-ld
-ninja -C build-osx
-```
-
-Le binaire résultant aura un format exécutable Mach-O et ne pourra pas être exécuté sous Linux.
diff --git a/docs/fr/development/build-osx.md b/docs/fr/development/build-osx.md
deleted file mode 100644
index 677e257a6374..000000000000
--- a/docs/fr/development/build-osx.md
+++ /dev/null
@@ -1,93 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 65
-toc_title: Comment Construire ClickHouse sur Mac OS X
----
-
-# Comment Construire ClickHouse sur Mac OS X {#how-to-build-clickhouse-on-mac-os-x}
-
-Build devrait fonctionner sur Mac OS X 10.15 (Catalina)
-
-## Installer Homebrew {#install-homebrew}
-
-``` bash
-$ /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
-```
-
-## Installez les compilateurs, outils et bibliothèques requis {#install-required-compilers-tools-and-libraries}
-
-``` bash
-$ brew install cmake ninja libtool gettext
-```
-
-## Commander Clickhouse Sources {#checkout-clickhouse-sources}
-
-``` bash
-$ git clone --recursive git@github.com:ClickHouse/ClickHouse.git
-```
-
-ou
-
-``` bash
-$ git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-
-$ cd ClickHouse
-```
-
-## Construire ClickHouse {#build-clickhouse}
-
-``` bash
-$ mkdir build
-$ cd build
-$ cmake .. -DCMAKE_CXX_COMPILER=`which clang++` -DCMAKE_C_COMPILER=`which clang`
-$ ninja
-$ cd ..
-```
-
-## Mises en garde {#caveats}
-
-Si vous avez l'intention d'exécuter clickhouse-server, assurez-vous d'augmenter la variable maxfiles du système.
-
-!!! info "Note"
-    Vous aurez besoin d'utiliser sudo.
-
-Pour ce faire, créez le fichier suivant:
-
-/ Bibliothèque / LaunchDaemons / limite.maxfiles.plist:
-
-``` xml
-<?xml version="1.0" encoding="UTF-8"?>
-<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"
-        "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
-<plist version="1.0">
-  <dict>
-    <key>Label</key>
-    <string>limit.maxfiles</string>
-    <key>ProgramArguments</key>
-    <array>
-      <string>launchctl</string>
-      <string>limit</string>
-      <string>maxfiles</string>
-      <string>524288</string>
-      <string>524288</string>
-    </array>
-    <key>RunAtLoad</key>
-    <true/>
-    <key>ServiceIPC</key>
-    <false/>
-  </dict>
-</plist>
-```
-
-Exécutez la commande suivante:
-
-``` bash
-$ sudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist
-```
-
-Redémarrer.
-
-Pour vérifier si elle fonctionne, vous pouvez utiliser `ulimit -n` commande.
-
-[Article Original](https://clickhouse.tech/docs/en/development/build_osx/) <!--hide-->
diff --git a/docs/fr/development/build.md b/docs/fr/development/build.md
deleted file mode 100644
index 4889373c52ad..000000000000
--- a/docs/fr/development/build.md
+++ /dev/null
@@ -1,141 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 64
-toc_title: Comment Construire ClickHouse sur Linux
----
-
-# Comment Construire ClickHouse pour le développement {#how-to-build-clickhouse-for-development}
-
-Le tutoriel suivant est basé sur le système Linux Ubuntu.
-Avec les modifications appropriées, il devrait également fonctionner sur toute autre distribution Linux.
-Plates-formes prises en charge: x86_64 et AArch64. La prise en charge de Power9 est expérimentale.
-
-## Installez Git, CMake, Python et Ninja {#install-git-cmake-python-and-ninja}
-
-``` bash
-$ sudo apt-get install git cmake python ninja-build
-```
-
-Ou cmake3 au lieu de cmake sur les systèmes plus anciens.
-
-## Installer GCC 10 {#install-gcc-10}
-
-Il y a plusieurs façons de le faire.
-
-### Installer à partir d'un paquet PPA {#install-from-a-ppa-package}
-
-``` bash
-$ sudo apt-get install software-properties-common
-$ sudo apt-add-repository ppa:ubuntu-toolchain-r/test
-$ sudo apt-get update
-$ sudo apt-get install gcc-10 g++-10
-```
-
-### Installer à partir de Sources {#install-from-sources}
-
-Regarder [utils/ci/build-gcc-from-sources.sh](https://github.com/ClickHouse/ClickHouse/blob/master/utils/ci/build-gcc-from-sources.sh)
-
-## Utilisez GCC 10 pour les Builds {#use-gcc-10-for-builds}
-
-``` bash
-$ export CC=gcc-10
-$ export CXX=g++-10
-```
-
-## Commander Clickhouse Sources {#checkout-clickhouse-sources}
-
-``` bash
-$ git clone --recursive git@github.com:ClickHouse/ClickHouse.git
-```
-
-ou
-
-``` bash
-$ git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-```
-
-## Construire ClickHouse {#build-clickhouse}
-
-``` bash
-$ cd ClickHouse
-$ mkdir build
-$ cd build
-$ cmake ..
-$ ninja
-$ cd ..
-```
-
-Pour créer un exécutable, exécutez `ninja clickhouse`.
-Cela va créer de l' `programs/clickhouse` exécutable, qui peut être utilisé avec `client` ou `server` argument.
-
-# Comment Construire ClickHouse sur N'importe quel Linux {#how-to-build-clickhouse-on-any-linux}
-
-La construction nécessite les composants suivants:
-
--   Git (est utilisé uniquement pour extraire les sources, ce n'est pas nécessaire pour la construction)
--   CMake 3.10 ou plus récent
--   Ninja (recommandé) ou faire
--   Compilateur C++: gcc 10 ou clang 8 ou plus récent
--   Linker: lld ou gold (le classique GNU LD ne fonctionnera pas)
--   Python (est seulement utilisé dans la construction LLVM et il est facultatif)
-
-Si tous les composants sont installés, vous pouvez construire de la même manière que les étapes ci-dessus.
-
-Exemple pour Ubuntu Eoan:
-
-    sudo apt update
-    sudo apt install git cmake ninja-build g++ python
-    git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-    mkdir build && cd build
-    cmake ../ClickHouse
-    ninja
-
-Exemple Pour openSUSE Tumbleweed:
-
-    sudo zypper install git cmake ninja gcc-c++ python lld
-    git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-    mkdir build && cd build
-    cmake ../ClickHouse
-    ninja
-
-Exemple Pour Fedora Rawhide:
-
-    sudo yum update
-    yum --nogpg install git cmake make gcc-c++ python3
-    git clone --recursive https://github.com/ClickHouse/ClickHouse.git
-    mkdir build && cd build
-    cmake ../ClickHouse
-    make -j $(nproc)
-
-# Vous N'avez pas à construire ClickHouse {#you-dont-have-to-build-clickhouse}
-
-ClickHouse est disponible dans des binaires et des paquets pré-construits. Les binaires sont portables et peuvent être exécutés sur N'importe quelle saveur Linux.
-
-Ils sont conçus pour les versions stables, préconfigurables et de test aussi longtemps que pour chaque commit à master et pour chaque requête d'extraction.
-
-Pour trouver la construction la plus fraîche de `master`, aller à [page commits](https://github.com/ClickHouse/ClickHouse/commits/master), cliquez sur la première coche verte ou Croix Rouge près de commit, et cliquez sur le “Details” lien à droite après “ClickHouse Build Check”.
-
-# Comment construire le paquet ClickHouse Debian {#how-to-build-clickhouse-debian-package}
-
-## Installer Git et Pbuilder {#install-git-and-pbuilder}
-
-``` bash
-$ sudo apt-get update
-$ sudo apt-get install git python pbuilder debhelper lsb-release fakeroot sudo debian-archive-keyring debian-keyring
-```
-
-## Commander Clickhouse Sources {#checkout-clickhouse-sources-1}
-
-``` bash
-$ git clone --recursive --branch master https://github.com/ClickHouse/ClickHouse.git
-$ cd ClickHouse
-```
-
-## Exécuter Le Script De Publication {#run-release-script}
-
-``` bash
-$ ./release
-```
-
-[Article Original](https://clickhouse.tech/docs/en/development/build/) <!--hide-->
diff --git a/docs/fr/development/contrib.md b/docs/fr/development/contrib.md
deleted file mode 100644
index 6909ef905bd5..000000000000
--- a/docs/fr/development/contrib.md
+++ /dev/null
@@ -1,41 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 70
-toc_title: "Biblioth\xE8ques Tierces Utilis\xE9es"
----
-
-# Bibliothèques Tierces Utilisées {#third-party-libraries-used}
-
-| Bibliothèque         | Licence                                                                                                                                        |
-|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------|
-| base64               | [Licence BSD 2-Clause](https://github.com/aklomp/base64/blob/a27c565d1b6c676beaf297fe503c4518185666f7/LICENSE)                                 |
-| stimuler             | [Licence Logicielle Boost 1.0](https://github.com/ClickHouse-Extras/boost-extra/blob/6883b40449f378019aec792f9983ce3afc7ff16e/LICENSE_1_0.txt) |
-| brotli               | [MIT](https://github.com/google/brotli/blob/master/LICENSE)                                                                                    |
-| capnproto            | [MIT](https://github.com/capnproto/capnproto/blob/master/LICENSE)                                                                              |
-| cctz                 | [Licence Apache 2.0](https://github.com/google/cctz/blob/4f9776a310f4952454636363def82c2bf6641d5f/LICENSE.txt)                                 |
-| double-conversion    | [Licence BSD 3-Clause](https://github.com/google/double-conversion/blob/cf2f0f3d547dc73b4612028a155b80536902ba02/LICENSE)                      |
-| FastMemcpy           | [MIT](https://github.com/ClickHouse/ClickHouse/blob/master/libs/libmemcpy/impl/LICENSE)                                                        |
-| googletest           | [Licence BSD 3-Clause](https://github.com/google/googletest/blob/master/LICENSE)                                                               |
-| h3                   | [Licence Apache 2.0](https://github.com/uber/h3/blob/master/LICENSE)                                                                           |
-| hyperscan            | [Licence BSD 3-Clause](https://github.com/intel/hyperscan/blob/master/LICENSE)                                                                 |
-| libcxxabi            | [BSD + MIT](https://github.com/ClickHouse/ClickHouse/blob/master/libs/libglibc-compatibility/libcxxabi/LICENSE.TXT)                            |
-| libdivide            | [Licence Zlib](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/libdivide/LICENSE.txt)                                             |
-| libgsasl             | [LGPL v2.1](https://github.com/ClickHouse-Extras/libgsasl/blob/3b8948a4042e34fb00b4fb987535dc9e02e39040/LICENSE)                               |
-| libhdfs3             | [Licence Apache 2.0](https://github.com/ClickHouse-Extras/libhdfs3/blob/bd6505cbb0c130b0db695305b9a38546fa880e5a/LICENSE.txt)                  |
-| libmetrohash         | [Licence Apache 2.0](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/libmetrohash/LICENSE)                                        |
-| libpcg-aléatoire     | [Licence Apache 2.0](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/libpcg-random/LICENSE-APACHE.txt)                            |
-| libressl             | [Licence OpenSSL](https://github.com/ClickHouse-Extras/ssl/blob/master/COPYING)                                                                |
-| librdkafka           | [Licence BSD 2-Clause](https://github.com/edenhill/librdkafka/blob/363dcad5a23dc29381cc626620e68ae418b3af19/LICENSE)                           |
-| libwidechar_width   | [CC0 1.0 universel](https://github.com/ClickHouse/ClickHouse/blob/master/libs/libwidechar_width/LICENSE)                                       |
-| llvm                 | [Licence BSD 3-Clause](https://github.com/ClickHouse-Extras/llvm/blob/163def217817c90fb982a6daf384744d8472b92b/llvm/LICENSE.TXT)               |
-| lz4                  | [Licence BSD 2-Clause](https://github.com/lz4/lz4/blob/c10863b98e1503af90616ae99725ecd120265dfb/LICENSE)                                       |
-| mariadb-connecteur-c | [LGPL v2.1](https://github.com/ClickHouse-Extras/mariadb-connector-c/blob/3.1/COPYING.LIB)                                                     |
-| murmurhash           | [Domaine Public](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/murmurhash/LICENSE)                                              |
-| pdqsort              | [Licence Zlib](https://github.com/ClickHouse/ClickHouse/blob/master/contrib/pdqsort/license.txt)                                               |
-| poco                 | [Licence Du Logiciel Boost-Version 1.0](https://github.com/ClickHouse-Extras/poco/blob/fe5505e56c27b6ecb0dcbc40c49dc2caf4e9637f/LICENSE)       |
-| protobuf             | [Licence BSD 3-Clause](https://github.com/ClickHouse-Extras/protobuf/blob/12735370922a35f03999afff478e1c6d7aa917a4/LICENSE)                    |
-| re2                  | [Licence BSD 3-Clause](https://github.com/google/re2/blob/7cf8b88e8f70f97fd4926b56aa87e7f53b2717e0/LICENSE)                                    |
-| UnixODBC             | [LGPL v2.1](https://github.com/ClickHouse-Extras/UnixODBC/tree/b0ad30f7f6289c12b76f04bfb9d466374bb32168)                                       |
-| zlib-ng              | [Licence Zlib](https://github.com/ClickHouse-Extras/zlib-ng/blob/develop/LICENSE.md)                                                           |
-| zstd                 | [Licence BSD 3-Clause](https://github.com/facebook/zstd/blob/dev/LICENSE)                                                                      |
diff --git a/docs/fr/development/developer-instruction.md b/docs/fr/development/developer-instruction.md
deleted file mode 100644
index 610216925c34..000000000000
--- a/docs/fr/development/developer-instruction.md
+++ /dev/null
@@ -1,287 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: "Le D\xE9butant Clickhouse Developer Instruction"
----
-
-La construction de ClickHouse est prise en charge sous Linux, FreeBSD et Mac OS X.
-
-# Si Vous Utilisez Windows {#if-you-use-windows}
-
-Si vous utilisez Windows, vous devez créer une machine virtuelle avec Ubuntu. Pour commencer à travailler avec une machine virtuelle, installez VirtualBox. Vous pouvez télécharger Ubuntu sur le site: https://www.ubuntu.com/#download. veuillez créer une machine virtuelle à partir de l'image téléchargée (vous devez réserver au moins 4 Go de RAM pour cela). Pour exécuter un terminal de ligne de commande dans Ubuntu, recherchez un programme contenant le mot “terminal” dans son nom (gnome-terminal, konsole etc.) ou appuyez simplement sur Ctrl + Alt + T.
-
-# Si vous utilisez un système 32 bits {#if-you-use-a-32-bit-system}
-
-ClickHouse ne peut pas fonctionner ou construire sur un système 32 bits. Vous devez acquérir l'accès à un système 64 bits et vous pouvez continuer la lecture.
-
-# Création d'un référentiel sur GitHub {#creating-a-repository-on-github}
-
-Pour commencer à travailler avec clickhouse repository, vous aurez besoin d'un compte GitHub.
-
-Vous en avez probablement déjà un, mais si vous ne le faites pas, veuillez vous inscrire à https://github.com. dans le cas où vous n'avez pas de clés SSH, vous devez les générer, puis les télécharger sur GitHub. Il est nécessaire pour l'envoi de vos correctifs. Il est également possible d'utiliser les mêmes clés SSH que vous utilisez avec d'autres serveurs SSH - vous les avez probablement déjà.
-
-Créer un fork de clickhouse repository. Pour ce faire, cliquez sur l' “fork” bouton dans le coin supérieur droit à https://github.com/ClickHouse/ClickHouse. il fourche votre propre copie de ClickHouse / ClickHouse à votre compte.
-
-Le processus de développement consiste d'abord à valider les modifications prévues dans votre fork de ClickHouse, puis à créer un “pull request” pour que ces modifications soient acceptées dans le référentiel principal (ClickHouse/ClickHouse).
-
-Pour travailler avec les dépôts git, veuillez installer `git`.
-
-Pour ce faire dans Ubuntu vous exécutez dans le terminal de ligne de commande:
-
-    sudo apt update
-    sudo apt install git
-
-Un bref manuel sur l'utilisation de Git peut être trouvé ici: https://education.github.com/git-cheat-sheet-education.pdf.
-Pour un manuel détaillé sur Git voir https://git-scm.com/book/en/v2.
-
-# Clonage D'un référentiel sur votre machine de développement {#cloning-a-repository-to-your-development-machine}
-
-Ensuite, vous devez télécharger les fichiers source sur votre machine de travail. Ceci est appelé “to clone a repository” parce qu'il crée une copie locale du dépôt sur votre machine de travail.
-
-Dans le terminal de ligne de commande exécuter:
-
-    git clone --recursive git@github.com:your_github_username/ClickHouse.git
-    cd ClickHouse
-
-Remarque: Veuillez remplacer *your_github_username* avec ce qui est approprié!
-
-Cette commande va créer un répertoire `ClickHouse` contenant la copie de travail du projet.
-
-Il est important que le chemin d'accès au répertoire de travail ne contienne aucun espace, car cela peut entraîner des problèmes lors de l'exécution du système de construction.
-
-Veuillez noter que clickhouse repository utilise `submodules`. That is what the references to additional repositories are called (i.e. external libraries on which the project depends). It means that when cloning the repository you need to specify the `--recursive` drapeau comme dans l'exemple ci-dessus. Si le dépôt a été cloné sans submodules, pour télécharger, vous devez exécuter les opérations suivantes:
-
-    git submodule init
-    git submodule update
-
-Vous pouvez vérifier l'état avec la commande: `git submodule status`.
-
-Si vous obtenez le message d'erreur suivantes:
-
-    Permission denied (publickey).
-    fatal: Could not read from remote repository.
-
-    Please make sure you have the correct access rights
-    and the repository exists.
-
-Cela signifie généralement que les clés SSH pour la connexion à GitHub sont manquantes. Ces clés sont normalement situés dans `~/.ssh`. Pour que les clés SSH soient acceptées, vous devez les télécharger dans la section Paramètres de L'interface utilisateur GitHub.
-
-Vous pouvez également cloner le référentiel via le protocole https:
-
-    git clone https://github.com/ClickHouse/ClickHouse.git
-
-Ceci, cependant, ne vous permettra pas d'envoyer vos modifications sur le serveur. Vous pouvez toujours l'utiliser temporairement et ajouter les clés SSH plus tard en remplaçant l'adresse distante du référentiel par `git remote` commande.
-
-Vous pouvez également ajouter l'adresse du dépôt clickhouse original à votre référentiel local pour extraire les mises à jour à partir de là:
-
-    git remote add upstream git@github.com:ClickHouse/ClickHouse.git
-
-Après avoir exécuté avec succès cette commande vous serez en mesure de tirer les mises à jour du repo clickhouse principal en exécutant `git pull upstream master`.
-
-## Travailler avec des Submodules {#working-with-submodules}
-
-Travailler avec des sous-modules dans git pourrait être douloureux. Prochaines commandes aidera à gérer:
-
-    # ! each command accepts --recursive
-    # Update remote URLs for submodules. Barely rare case
-    git submodule sync
-    # Add new submodules
-    git submodule init
-    # Update existing submodules to the current state
-    git submodule update
-    # Two last commands could be merged together
-    git submodule update --init
-
-Les commandes suivantes vous aideront à réinitialiser tous les sous-modules à l'état initial (!AVERTISSEMENT! - tout changement à l'intérieur sera supprimé):
-
-    # Synchronizes submodules' remote URL with .gitmodules
-    git submodule sync --recursive
-    # Update the registered submodules with initialize not yet initialized
-    git submodule update --init --recursive
-    # Reset all changes done after HEAD
-    git submodule foreach git reset --hard
-    # Clean files from .gitignore
-    git submodule foreach git clean -xfd
-    # Repeat last 4 commands for all submodule
-    git submodule foreach git submodule sync --recursive
-    git submodule foreach git submodule update --init --recursive
-    git submodule foreach git submodule foreach git reset --hard
-    git submodule foreach git submodule foreach git clean -xfd
-
-# Système De Construction {#build-system}
-
-ClickHouse utilise CMake et Ninja pour la construction.
-
-CMake - un système de méta-construction qui peut générer des fichiers Ninja (tâches de construction).
-Ninja - un système de construction plus petit avec un accent sur la vitesse utilisée pour exécuter ces tâches générées cmake.
-
-Pour installer sur Ubuntu, Debian ou mint run `sudo apt install cmake ninja-build`.
-
-Sur CentOS, RedHat run `sudo yum install cmake ninja-build`.
-
-Si vous utilisez Arch ou Gentoo, vous savez probablement vous - même comment installer CMake.
-
-Pour installer CMake et Ninja sur Mac OS X installez D'abord Homebrew puis installez tout le reste via brew:
-
-    /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
-    brew install cmake ninja
-
-Ensuite, vérifiez la version de CMake: `cmake --version`. Si elle est inférieure à 3.3, vous devez installer une version plus récente du site web: https://cmake.org/download/.
-
-# Bibliothèques Externes Facultatives {#optional-external-libraries}
-
-ClickHouse utilise plusieurs bibliothèques externes pour la construction. Tous n'ont pas besoin d'être installés séparément car ils sont construits avec ClickHouse à partir des sources situées dans les sous-modules. Vous pouvez vérifier la liste dans `contrib`.
-
-# Compilateur C++  {#c-compiler}
-
-Les compilateurs GCC à partir de la version 10 et Clang version 8 ou supérieure sont pris en charge pour construire ClickHouse.
-
-Les builds officiels de Yandex utilisent actuellement GCC car ils génèrent du code machine de performances légèrement meilleures (ce qui donne une différence allant jusqu'à plusieurs pour cent selon nos benchmarks). Et Clang est plus pratique pour le développement habituellement. Cependant, notre plate-forme d'intégration continue (CI) vérifie environ une douzaine de combinaisons de construction.
-
-Pour installer GCC sur Ubuntu Exécutez: `sudo apt install gcc g++`
-
-Vérifiez la version de gcc: `gcc --version`. Si elle est inférieure à 10, suivez les instructions ici: https://clickhouse.tech/docs/fr/development/build/#install-gcc-10.
-
-Mac OS X build est pris en charge uniquement pour Clang. Il suffit d'exécuter `brew install llvm`
-
-Si vous décidez d'utiliser Clang, vous pouvez également installer `libc++` et `lld` si vous savez ce que c'est. Utiliser `ccache` est également recommandé.
-
-# Le Processus De Construction {#the-building-process}
-
-Maintenant que vous êtes prêt à construire ClickHouse nous vous conseillons de créer un répertoire séparé `build` à l'intérieur de `ClickHouse` qui contiendra tous les artefacts de construction:
-
-    mkdir build
-    cd build
-
-Vous pouvez avoir plusieurs répertoires différents (build_release, build_debug, etc.) pour les différents types de construction.
-
-Tandis qu'à l'intérieur de la `build` répertoire, configurez votre build en exécutant CMake. Avant la première exécution, vous devez définir des variables d'environnement qui spécifient le compilateur (compilateur gcc version 10 dans cet exemple).
-
-Linux:
-
-    export CC=gcc-10 CXX=g++-10
-    cmake ..
-
-Mac OS X:
-
-    export CC=clang CXX=clang++
-    cmake ..
-
-Le `CC` variable spécifie le compilateur pour C (abréviation de compilateur C), et `CXX` variable indique quel compilateur C++ doit être utilisé pour la construction.
-
-Pour une version plus rapide, vous pouvez recourir à l' `debug` build type - Une construction sans optimisations. Pour cela fournissez le paramètre suivant `-D CMAKE_BUILD_TYPE=Debug`:
-
-    cmake -D CMAKE_BUILD_TYPE=Debug ..
-
-Vous pouvez modifier le type de génération en exécutant cette commande dans le `build` répertoire.
-
-Exécutez ninja pour construire:
-
-    ninja clickhouse-server clickhouse-client
-
-Seules les binaires vont être construites dans cet exemple.
-
-Si vous avez besoin de construire tous les binaires (utilitaires et tests), vous devez exécuter ninja sans paramètres:
-
-    ninja
-
-La construction complète nécessite environ 30 Go d'espace disque libre ou 15 Go pour construire les binaires principaux.
-
-Lorsqu'une grande quantité de RAM est disponible sur la machine de construction vous devez limiter le nombre de tâches de construction exécutées en parallèle avec `-j` param:
-
-    ninja -j 1 clickhouse-server clickhouse-client
-
-Sur les machines avec 4 Go de RAM, il est recommandé de spécifier 1, pour 8 Go de RAM `-j 2` est recommandé.
-
-Si vous recevez le message: `ninja: error: loading 'build.ninja': No such file or directory`, cela signifie que la génération d'une configuration de construction a échoué et que vous devez inspecter le message ci-dessus.
-
-Après le démarrage réussi du processus de construction, vous verrez la progression de la construction - le nombre de tâches traitées et le nombre total de tâches.
-
-Lors de la construction de messages sur les fichiers protobuf dans la bibliothèque libhdfs2 comme `libprotobuf WARNING` peuvent apparaître. Ils touchent rien et sont sûrs d'être ignoré.
-
-Lors de la construction, vous obtenez un fichier exécutable `ClickHouse/<build_dir>/programs/clickhouse`:
-
-    ls -l programs/clickhouse
-
-# Exécution de L'exécutable construit de ClickHouse {#running-the-built-executable-of-clickhouse}
-
-Pour exécuter le serveur sous l'utilisateur actuel vous devez naviguer vers `ClickHouse/programs/server/` (situé à l'extérieur de `build` et les exécuter:
-
-    ../../build/programs/clickhouse server
-
-Dans ce cas, ClickHouse utilisera les fichiers de configuration situés dans le répertoire courant. Vous pouvez l'exécuter `clickhouse server` depuis n'importe quel répertoire spécifiant le chemin d'accès à un fichier de configuration en tant que paramètre de ligne de commande `--config-file`.
-
-Pour vous connecter à ClickHouse avec clickhouse-client dans un autre terminal, accédez à `ClickHouse/build/programs/` et exécuter `./clickhouse client`.
-
-Si vous obtenez `Connection refused` message sur Mac OS X ou FreeBSD, essayez de spécifier l'adresse hôte 127.0.0.1:
-
-    clickhouse client --host 127.0.0.1
-
-Vous pouvez remplacer la version de production de clickhouse binary installée dans votre système par votre clickhouse Binary sur mesure. Pour ce faire, installez ClickHouse sur votre machine en suivant les instructions du site officiel. Ensuite, exécutez ce qui suit:
-
-    sudo service clickhouse-server stop
-    sudo cp ClickHouse/build/programs/clickhouse /usr/bin/
-    sudo service clickhouse-server start
-
-Notez que `clickhouse-client`, `clickhouse-server` et d'autres sont des liens symboliques à la commune `clickhouse` binaire.
-
-Vous pouvez également exécuter votre binaire ClickHouse personnalisé avec le fichier de configuration du package clickhouse installé sur votre système:
-
-    sudo service clickhouse-server stop
-    sudo -u clickhouse ClickHouse/build/programs/clickhouse server --config-file /etc/clickhouse-server/config.xml
-
-# IDE (environnement de développement intégré) {#ide-integrated-development-environment}
-
-Si vous ne savez pas quel IDE utiliser, nous vous recommandons D'utiliser CLion. CLion est un logiciel commercial, mais il offre une période d'essai gratuite de 30 jours. Il est également gratuit pour les étudiants. CLion peut être utilisé à la fois sur Linux et sur Mac OS X.
-
-KDevelop et QTCreator sont d'autres grandes alternatives D'un IDE pour développer ClickHouse. KDevelop est un IDE très pratique bien qu'instable. Si KDevelop se bloque après un certain temps lors de l'ouverture du projet, vous devez cliquer sur “Stop All” bouton dès qu'il a ouvert la liste des fichiers du projet. Après cela, KDevelop devrait être bien pour travailler avec.
-
-En tant qu'éditeurs de code simples, vous pouvez utiliser Sublime Text ou Visual Studio Code, ou Kate (qui sont tous disponibles sur Linux).
-
-Juste au cas où, il convient de mentionner que CLion crée `build` chemin sur son propre, il aussi sur son propre sélectionne `debug` pour le type de construction, pour la configuration, il utilise une version de CMake définie dans CLion et non celle installée par vous, et enfin, CLion utilisera `make` pour exécuter construire des tâches au lieu de `ninja`. C'est un comportement normal, gardez cela à l'esprit pour éviter toute confusion.
-
-# L'Écriture De Code {#writing-code}
-
-La description de l'architecture ClickHouse peut être trouvée ici: https://clickhouse.tech/docs/fr/développement/architecture/
-
-Le code Style Guide: https://clickhouse.tech/docs/fr/développement/style/
-
-Rédaction de tests: https://clickhouse.tech/docs/fr/développement/tests/
-
-Liste des tâches: https://github.com/ClickHouse/ClickHouse/issues?q=is%3Aopen+is%3Aissue+label%3A%22easy+task%22
-
-# Des Données De Test {#test-data}
-
-Le développement de ClickHouse nécessite souvent le chargement d'ensembles de données réalistes. Il est particulièrement important pour les tests de performance. Nous avons un ensemble spécialement préparé de données anonymisées de Yandex.Metrica. Il nécessite en outre quelques 3 Go d'espace disque libre. Notez que ces données ne sont pas requises pour accomplir la plupart des tâches de développement.
-
-    sudo apt install wget xz-utils
-
-    wget https://datasets.clickhouse.tech/hits/tsv/hits_v1.tsv.xz
-    wget https://datasets.clickhouse.tech/visits/tsv/visits_v1.tsv.xz
-
-    xz -v -d hits_v1.tsv.xz
-    xz -v -d visits_v1.tsv.xz
-
-    clickhouse-client
-
-    CREATE DATABASE IF NOT EXISTS test
-
-    CREATE TABLE test.hits ( WatchID UInt64,  JavaEnable UInt8,  Title String,  GoodEvent Int16,  EventTime DateTime,  EventDate Date,  CounterID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RegionID UInt32,  UserID UInt64,  CounterClass Int8,  OS UInt8,  UserAgent UInt8,  URL String,  Referer String,  URLDomain String,  RefererDomain String,  Refresh UInt8,  IsRobot UInt8,  RefererCategories Array(UInt16),  URLCategories Array(UInt16),  URLRegions Array(UInt32),  RefererRegions Array(UInt32),  ResolutionWidth UInt16,  ResolutionHeight UInt16,  ResolutionDepth UInt8,  FlashMajor UInt8,  FlashMinor UInt8,  FlashMinor2 String,  NetMajor UInt8,  NetMinor UInt8,  UserAgentMajor UInt16,  UserAgentMinor FixedString(2),  CookieEnable UInt8,  JavascriptEnable UInt8,  IsMobile UInt8,  MobilePhone UInt8,  MobilePhoneModel String,  Params String,  IPNetworkID UInt32,  TraficSourceID Int8,  SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  IsArtifical UInt8,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  ClientTimeZone Int16,  ClientEventTime DateTime,  SilverlightVersion1 UInt8,  SilverlightVersion2 UInt8,  SilverlightVersion3 UInt32,  SilverlightVersion4 UInt16,  PageCharset String,  CodeVersion UInt32,  IsLink UInt8,  IsDownload UInt8,  IsNotBounce UInt8,  FUniqID UInt64,  HID UInt32,  IsOldCounter UInt8,  IsEvent UInt8,  IsParameter UInt8,  DontCountHits UInt8,  WithHash UInt8,  HitColor FixedString(1),  UTCEventTime DateTime,  Age UInt8,  Sex UInt8,  Income UInt8,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16),  RemoteIP UInt32,  RemoteIP6 FixedString(16),  WindowName Int32,  OpenerName Int32,  HistoryLength Int16,  BrowserLanguage FixedString(2),  BrowserCountry FixedString(2),  SocialNetwork String,  SocialAction String,  HTTPError UInt16,  SendTiming Int32,  DNSTiming Int32,  ConnectTiming Int32,  ResponseStartTiming Int32,  ResponseEndTiming Int32,  FetchTiming Int32,  RedirectTiming Int32,  DOMInteractiveTiming Int32,  DOMContentLoadedTiming Int32,  DOMCompleteTiming Int32,  LoadEventStartTiming Int32,  LoadEventEndTiming Int32,  NSToDOMContentLoadedTiming Int32,  FirstPaintTiming Int32,  RedirectCount Int8,  SocialSourceNetworkID UInt8,  SocialSourcePage String,  ParamPrice Int64,  ParamOrderID String,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16,  GoalsReached Array(UInt32),  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String,  UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String,  FromTag String,  HasGCLID UInt8,  RefererHash UInt64,  URLHash UInt64,  CLID UInt32,  YCLID UInt64,  ShareService String,  ShareURL String,  ShareTitle String,  `ParsedParams.Key1` Array(String),  `ParsedParams.Key2` Array(String),  `ParsedParams.Key3` Array(String),  `ParsedParams.Key4` Array(String),  `ParsedParams.Key5` Array(String),  `ParsedParams.ValueDouble` Array(Float64),  IslandID FixedString(16),  RequestNum UInt32,  RequestTry UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(EventDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID), EventTime);
-
-    CREATE TABLE test.visits ( CounterID UInt32,  StartDate Date,  Sign Int8,  IsNew UInt8,  VisitID UInt64,  UserID UInt64,  StartTime DateTime,  Duration UInt32,  UTCStartTime DateTime,  PageViews Int32,  Hits Int32,  IsBounce UInt8,  Referer String,  StartURL String,  RefererDomain String,  StartURLDomain String,  EndURL String,  LinkURL String,  IsDownload UInt8,  TraficSourceID Int8,  SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  PlaceID Int32,  RefererCategories Array(UInt16),  URLCategories Array(UInt16),  URLRegions Array(UInt32),  RefererRegions Array(UInt32),  IsYandex UInt8,  GoalReachesDepth Int32,  GoalReachesURL Int32,  GoalReachesAny Int32,  SocialSourceNetworkID UInt8,  SocialSourcePage String,  MobilePhoneModel String,  ClientEventTime DateTime,  RegionID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RemoteIP UInt32,  RemoteIP6 FixedString(16),  IPNetworkID UInt32,  SilverlightVersion3 UInt32,  CodeVersion UInt32,  ResolutionWidth UInt16,  ResolutionHeight UInt16,  UserAgentMajor UInt16,  UserAgentMinor UInt16,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  SilverlightVersion2 UInt8,  SilverlightVersion4 UInt16,  FlashVersion3 UInt16,  FlashVersion4 UInt16,  ClientTimeZone Int16,  OS UInt8,  UserAgent UInt8,  ResolutionDepth UInt8,  FlashMajor UInt8,  FlashMinor UInt8,  NetMajor UInt8,  NetMinor UInt8,  MobilePhone UInt8,  SilverlightVersion1 UInt8,  Age UInt8,  Sex UInt8,  Income UInt8,  JavaEnable UInt8,  CookieEnable UInt8,  JavascriptEnable UInt8,  IsMobile UInt8,  BrowserLanguage UInt16,  BrowserCountry UInt16,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16),  Params Array(String),  `Goals.ID` Array(UInt32),  `Goals.Serial` Array(UInt32),  `Goals.EventTime` Array(DateTime),  `Goals.Price` Array(Int64),  `Goals.OrderID` Array(String),  `Goals.CurrencyID` Array(UInt32),  WatchIDs Array(UInt64),  ParamSumPrice Int64,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16,  ClickLogID UInt64,  ClickEventID Int32,  ClickGoodEvent Int32,  ClickEventTime DateTime,  ClickPriorityID Int32,  ClickPhraseID Int32,  ClickPageID Int32,  ClickPlaceID Int32,  ClickTypeID Int32,  ClickResourceID Int32,  ClickCost UInt32,  ClickClientIP UInt32,  ClickDomainID UInt32,  ClickURL String,  ClickAttempt UInt8,  ClickOrderID UInt32,  ClickBannerID UInt32,  ClickMarketCategoryID UInt32,  ClickMarketPP UInt32,  ClickMarketCategoryName String,  ClickMarketPPName String,  ClickAWAPSCampaignName String,  ClickPageName String,  ClickTargetType UInt16,  ClickTargetPhraseID UInt64,  ClickContextType UInt8,  ClickSelectType Int8,  ClickOptions String,  ClickGroupBannerID Int32,  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String,  UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String,  FromTag String,  HasGCLID UInt8,  FirstVisit DateTime,  PredLastVisit Date,  LastVisit Date,  TotalVisits UInt32,  `TraficSource.ID` Array(Int8),  `TraficSource.SearchEngineID` Array(UInt16),  `TraficSource.AdvEngineID` Array(UInt8),  `TraficSource.PlaceID` Array(UInt16),  `TraficSource.SocialSourceNetworkID` Array(UInt8),  `TraficSource.Domain` Array(String),  `TraficSource.SearchPhrase` Array(String),  `TraficSource.SocialSourcePage` Array(String),  Attendance FixedString(16),  CLID UInt32,  YCLID UInt64,  NormalizedRefererHash UInt64,  SearchPhraseHash UInt64,  RefererDomainHash UInt64,  NormalizedStartURLHash UInt64,  StartURLDomainHash UInt64,  NormalizedEndURLHash UInt64,  TopLevelDomain UInt64,  URLScheme UInt64,  OpenstatServiceNameHash UInt64,  OpenstatCampaignIDHash UInt64,  OpenstatAdIDHash UInt64,  OpenstatSourceIDHash UInt64,  UTMSourceHash UInt64,  UTMMediumHash UInt64,  UTMCampaignHash UInt64,  UTMContentHash UInt64,  UTMTermHash UInt64,  FromHash UInt64,  WebVisorEnabled UInt8,  WebVisorActivity UInt32,  `ParsedParams.Key1` Array(String),  `ParsedParams.Key2` Array(String),  `ParsedParams.Key3` Array(String),  `ParsedParams.Key4` Array(String),  `ParsedParams.Key5` Array(String),  `ParsedParams.ValueDouble` Array(Float64),  `Market.Type` Array(UInt8),  `Market.GoalID` Array(UInt32),  `Market.OrderID` Array(String),  `Market.OrderPrice` Array(Int64),  `Market.PP` Array(UInt32),  `Market.DirectPlaceID` Array(UInt32),  `Market.DirectOrderID` Array(UInt32),  `Market.DirectBannerID` Array(UInt32),  `Market.GoodID` Array(String),  `Market.GoodName` Array(String),  `Market.GoodQuantity` Array(Int32),  `Market.GoodPrice` Array(Int64),  IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID);
-
-    clickhouse-client --max_insert_block_size 100000 --query "INSERT INTO test.hits FORMAT TSV" < hits_v1.tsv
-    clickhouse-client --max_insert_block_size 100000 --query "INSERT INTO test.visits FORMAT TSV" < visits_v1.tsv
-
-# La Création De Pull Request {#creating-pull-request}
-
-Accédez à votre référentiel fork dans L'interface utilisateur de GitHub. Si vous avez développé dans une branche, vous devez sélectionner cette branche. Il y aura un “Pull request” bouton situé sur l'écran. En substance, cela signifie “create a request for accepting my changes into the main repository”.
-
-Une demande d'extraction peuvent être créés, même si le travail n'est pas encore terminée. Dans ce cas veuillez mettre le mot “WIP” (travaux en cours) au début du titre, il peut être modifié plus tard. Ceci est utile pour l'examen coopératif et la discussion des changements ainsi que pour l'exécution de tous les tests disponibles. Il est important que vous fournissiez une brève description de vos modifications, il sera ensuite utilisé pour générer des journaux de modifications de version.
-
-Les tests commenceront dès que les employés de Yandex étiqueteront votre PR avec une étiquette “can be tested”. The results of some first checks (e.g. code style) will come in within several minutes. Build check results will arrive within half an hour. And the main set of tests will report itself within an hour.
-
-Le système préparera les builds binaires ClickHouse pour votre demande de tirage individuellement. Pour récupérer ces versions cliquez sur le “Details” lien à côté “ClickHouse build check” entrée dans la liste de vérifications. Vous y trouverez des liens directs vers les construit .paquets deb de ClickHouse que vous pouvez déployer même sur vos serveurs de production (si vous n'avez pas peur).
-
-Très probablement, certaines des constructions échoueront à la première fois. Cela est dû au fait que nous avons vérifier s'appuie à la fois avec gcc, ainsi qu'avec clang, pratiquement tous les avertissements existants (toujours avec le `-Werror` drapeau) activé pour clang. Sur cette même page, vous pouvez trouver tous les journaux de construction afin que vous n'ayez pas à construire ClickHouse de toutes les manières possibles.
diff --git a/docs/fr/development/index.md b/docs/fr/development/index.md
deleted file mode 100644
index 6a6dbe34dda5..000000000000
--- a/docs/fr/development/index.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "D\xE9veloppement"
-toc_hidden: true
-toc_priority: 58
-toc_title: "cach\xE9s"
----
-
-# Développement De ClickHouse {#clickhouse-development}
-
-[Article Original](https://clickhouse.tech/docs/en/development/) <!--hide-->
diff --git a/docs/fr/development/style.md b/docs/fr/development/style.md
deleted file mode 100644
index ad3b470f7b45..000000000000
--- a/docs/fr/development/style.md
+++ /dev/null
@@ -1,841 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 68
-toc_title: "Comment \xE9crire du Code C++ "
----
-
-# Comment écrire du Code C++  {#how-to-write-c-code}
-
-## Recommandations Générales {#general-recommendations}
-
-**1.** Ce qui suit sont des recommandations, pas des exigences.
-
-**2.** Si vous modifiez du code, il est logique de suivre le formatage du code existant.
-
-**3.** Le style de Code est nécessaire pour la cohérence. La cohérence facilite la lecture du code et facilite également la recherche du code.
-
-**4.** Beaucoup de règles n'ont pas de raisons logiques; elles sont dictées par des pratiques établies.
-
-## Formater {#formatting}
-
-**1.** La plupart du formatage se fera automatiquement par `clang-format`.
-
-**2.** Les tirets sont 4 espaces. Configurez votre environnement de développement afin qu'un onglet ajoute quatre espaces.
-
-**3.** Les crochets d'ouverture et de fermeture doivent être sur une ligne séparée.
-
-``` cpp
-inline void readBoolText(bool & x, ReadBuffer & buf)
-{
-    char tmp = '0';
-    readChar(tmp, buf);
-    x = tmp != '0';
-}
-```
-
-**4.** Si le corps entier de la fonction est un `statement` il peut donc être placé sur une seule ligne. Place des espaces autour des accolades (en plus de l'espace à la fin de la ligne).
-
-``` cpp
-inline size_t mask() const                { return buf_size() - 1; }
-inline size_t place(HashValue x) const    { return x & mask(); }
-```
-
-**5.** Pour les fonctions. Ne mettez pas d'espaces entre parenthèses.
-
-``` cpp
-void reinsert(const Value & x)
-```
-
-``` cpp
-memcpy(&buf[place_value], &x, sizeof(x));
-```
-
-**6.** Dans `if`, `for`, `while` et d'autres expressions, un espace est inséré devant le support d'ouverture (par opposition aux appels de fonction).
-
-``` cpp
-for (size_t i = 0; i < rows; i += storage.index_granularity)
-```
-
-**7.** Ajouter des espaces autour des opérateurs binaires (`+`, `-`, `*`, `/`, `%`, …) and the ternary operator `?:`.
-
-``` cpp
-UInt16 year = (s[0] - '0') * 1000 + (s[1] - '0') * 100 + (s[2] - '0') * 10 + (s[3] - '0');
-UInt8 month = (s[5] - '0') * 10 + (s[6] - '0');
-UInt8 day = (s[8] - '0') * 10 + (s[9] - '0');
-```
-
-**8.** Si un saut de ligne est entré, placez l'opérateur sur une nouvelle ligne et augmentez le retrait avant.
-
-``` cpp
-if (elapsed_ns)
-    message << " ("
-        << rows_read_on_server * 1000000000 / elapsed_ns << " rows/s., "
-        << bytes_read_on_server * 1000.0 / elapsed_ns << " MB/s.) ";
-```
-
-**9.** Vous pouvez utiliser des espaces pour l'alignement dans une ligne, si vous le souhaitez.
-
-``` cpp
-dst.ClickLogID         = click.LogID;
-dst.ClickEventID       = click.EventID;
-dst.ClickGoodEvent     = click.GoodEvent;
-```
-
-**10.** N'utilisez pas d'espaces autour des opérateurs `.`, `->`.
-
-Si nécessaire, l'opérateur peut être renvoyé à la ligne suivante. Dans ce cas, le décalage devant celui-ci est augmenté.
-
-**11.** N'utilisez pas d'espace pour séparer les opérateurs unaires (`--`, `++`, `*`, `&`, …) from the argument.
-
-**12.** Mettre un espace après une virgule, mais pas avant. La même règle vaut pour un point-virgule à l'intérieur d'un `for` expression.
-
-**13.** Ne pas utiliser des espaces pour séparer les `[]` opérateur.
-
-**14.** Dans un `template <...>` expression, utiliser un espace entre les `template` et `<`; pas d'espace après `<` ou avant `>`.
-
-``` cpp
-template <typename TKey, typename TValue>
-struct AggregatedStatElement
-{}
-```
-
-**15.** Dans les classes et les structures, écrivez `public`, `private`, et `protected` sur le même niveau que `class/struct` et tiret le reste du code.
-
-``` cpp
-template <typename T>
-class MultiVersion
-{
-public:
-    /// Version of object for usage. shared_ptr manage lifetime of version.
-    using Version = std::shared_ptr<const T>;
-    ...
-}
-```
-
-**16.** Si le même `namespace` est utilisé pour l'ensemble du fichier, et il n'y a rien d'autre significatif, un décalage n'est pas nécessaire à l'intérieur `namespace`.
-
-**17.** Si le bloc pour un `if`, `for`, `while` ou autres expressions se compose d'un seul `statement`, les accolades sont facultatives. Place de la `statement` sur une ligne séparée, à la place. Cette règle est également valable pour les imbriqués `if`, `for`, `while`, …
-
-Mais si l'intérieur `statement` contient des accolades ou `else` le bloc externe doit être écrit dans les accolades.
-
-``` cpp
-/// Finish write.
-for (auto & stream : streams)
-    stream.second->finalize();
-```
-
-**18.** Il ne devrait pas y avoir d'espaces aux extrémités des lignes.
-
-**19.** Les fichiers Source sont encodés en UTF-8.
-
-**20.** Les caractères non-ASCII peuvent être utilisés dans les littéraux de chaîne.
-
-``` cpp
-<< ", " << (timer.elapsed() / chunks_stats.hits) << " μsec/hit.";
-```
-
-**21.** N'écrivez pas plusieurs expressions sur une seule ligne.
-
-**22.** Groupez les sections de code à l'intérieur des fonctions et séparez-les avec pas plus d'une ligne vide.
-
-**23.** Séparez les fonctions, les classes, etc. avec une ou deux lignes vides.
-
-**24.** `A const` (liés à une valeur) doit être écrit avant le nom du type.
-
-``` cpp
-//correct
-const char * pos
-const std::string & s
-//incorrect
-char const * pos
-```
-
-**25.** Lors de la déclaration d'un pointeur ou d'une référence, le `*` et `&` les symboles doivent être séparés par des espaces des deux côtés.
-
-``` cpp
-//correct
-const char * pos
-//incorrect
-const char* pos
-const char *pos
-```
-
-**26.** Lors de l'utilisation de types de modèles, les alias avec le `using` mot-clé (sauf dans les cas les plus simples).
-
-En d'autres termes, les paramètres du modèle sont indiquées que dans `using` et ne sont pas répétés dans le code.
-
-`using` peut être déclaré localement, comme dans une fonction.
-
-``` cpp
-//correct
-using FileStreams = std::map<std::string, std::shared_ptr<Stream>>;
-FileStreams streams;
-//incorrect
-std::map<std::string, std::shared_ptr<Stream>> streams;
-```
-
-**27.** Ne déclarez pas plusieurs variables de types différents dans une instruction.
-
-``` cpp
-//incorrect
-int x, *y;
-```
-
-**28.** N'utilisez pas de moulages de style C.
-
-``` cpp
-//incorrect
-std::cerr << (int)c <<; std::endl;
-//correct
-std::cerr << static_cast<int>(c) << std::endl;
-```
-
-**29.** Dans les classes et les structures, groupez les membres et les fonctions séparément dans chaque portée de visibilité.
-
-**30.** Pour les petites classes et structures, il n'est pas nécessaire de séparer la déclaration de méthode de l'implémentation.
-
-La même chose est vraie pour les petites méthodes dans toutes les classes ou structures.
-
-Pour les classes et les structures modélisées, ne séparez pas les déclarations de méthode de l'implémentation (car sinon elles doivent être définies dans la même unité de traduction).
-
-**31.** Vous pouvez envelopper des lignes à 140 caractères, au lieu de 80.
-
-**32.** Utilisez toujours les opérateurs d'incrémentation/décrémentation de préfixe si postfix n'est pas requis.
-
-``` cpp
-for (Names::const_iterator it = column_names.begin(); it != column_names.end(); ++it)
-```
-
-## Commentaire {#comments}
-
-**1.** Assurez-vous d'ajouter des commentaires pour toutes les parties non triviales du code.
-
-C'est très important. Écrit le commentaire peut vous aider à réaliser que le code n'est pas nécessaire, ou qu'il est mal conçu.
-
-``` cpp
-/** Part of piece of memory, that can be used.
-  * For example, if internal_buffer is 1MB, and there was only 10 bytes loaded to buffer from file for reading,
-  * then working_buffer will have size of only 10 bytes
-  * (working_buffer.end() will point to position right after those 10 bytes available for read).
-  */
-```
-
-**2.** Les commentaires peuvent être aussi détaillées que nécessaire.
-
-**3.** Placez les commentaires avant le code qu'ils décrivent. Dans de rares cas, des commentaires peuvent venir après le code, sur la même ligne.
-
-``` cpp
-/** Parses and executes the query.
-*/
-void executeQuery(
-    ReadBuffer & istr, /// Where to read the query from (and data for INSERT, if applicable)
-    WriteBuffer & ostr, /// Where to write the result
-    Context & context, /// DB, tables, data types, engines, functions, aggregate functions...
-    BlockInputStreamPtr & query_plan, /// Here could be written the description on how query was executed
-    QueryProcessingStage::Enum stage = QueryProcessingStage::Complete /// Up to which stage process the SELECT query
-    )
-```
-
-**4.** Les commentaires doivent être rédigés en anglais seulement.
-
-**5.** Si vous écrivez une bibliothèque, incluez des commentaires détaillés l'expliquant dans le fichier d'en-tête principal.
-
-**6.** N'ajoutez pas de commentaires qui ne fournissent pas d'informations supplémentaires. En particulier, ne laissez pas de commentaires vides comme celui-ci:
-
-``` cpp
-/*
-* Procedure Name:
-* Original procedure name:
-* Author:
-* Date of creation:
-* Dates of modification:
-* Modification authors:
-* Original file name:
-* Purpose:
-* Intent:
-* Designation:
-* Classes used:
-* Constants:
-* Local variables:
-* Parameters:
-* Date of creation:
-* Purpose:
-*/
-```
-
-L'exemple est emprunté à partir de la ressource http://home.tamk.fi/~jaalto/cours/coding-style/doc/désuète-code/.
-
-**7.** Ne pas écrire des commentaires de déchets (auteur, date de création .. au début de chaque fichier.
-
-**8.** Les commentaires sur une seule ligne commencent par trois barres obliques: `///` et les commentaires multi-lignes commencer avec `/**`. Ces commentaires sont pris en considération “documentation”.
-
-REMARQUE: Vous pouvez utiliser Doxygen pour générer de la documentation à partir de ces commentaires. Mais Doxygen n'est généralement pas utilisé car il est plus pratique de naviguer dans le code dans L'IDE.
-
-**9.** Les commentaires multilignes ne doivent pas avoir de lignes vides au début et à la fin (sauf la ligne qui ferme un commentaire multilignes).
-
-**10.** Pour commenter le code, utilisez des commentaires de base, pas “documenting” commentaire.
-
-**11.** Supprimez les parties commentées du code avant de valider.
-
-**12.** N'utilisez pas de blasphème dans les commentaires ou le code.
-
-**13.** N'utilisez pas de majuscules. N'utilisez pas de ponctuation excessive.
-
-``` cpp
-/// WHAT THE FAIL???
-```
-
-**14.** N'utilisez pas de commentaires pour créer des délimiteurs.
-
-``` cpp
-///******************************************************
-```
-
-**15.** Ne commencez pas les discussions dans les commentaires.
-
-``` cpp
-/// Why did you do this stuff?
-```
-
-**16.** Il n'est pas nécessaire d'écrire un commentaire à la fin d'un bloc décrivant de quoi il s'agissait.
-
-``` cpp
-/// for
-```
-
-## Nom {#names}
-
-**1.** Utilisez des lettres minuscules avec des traits de soulignement dans les noms des variables et des membres de la classe.
-
-``` cpp
-size_t max_block_size;
-```
-
-**2.** Pour les noms de fonctions (méthodes), utilisez camelCase commençant par une lettre minuscule.
-
-``` cpp
-std::string getName() const override { return "Memory"; }
-```
-
-**3.** Pour les noms de classes (structures), utilisez CamelCase commençant par une lettre majuscule. Les préfixes autres que I ne sont pas utilisés pour les interfaces.
-
-``` cpp
-class StorageMemory : public IStorage
-```
-
-**4.** `using` sont nommées de la même manière que les classes, ou avec `_t` sur la fin.
-
-**5.** Noms des arguments de type de modèle: dans les cas simples, utilisez `T`; `T`, `U`; `T1`, `T2`.
-
-Pour les cas plus complexes, suivez les règles pour les noms de classe ou ajoutez le préfixe `T`.
-
-``` cpp
-template <typename TKey, typename TValue>
-struct AggregatedStatElement
-```
-
-**6.** Noms des arguments constants du modèle: suivez les règles pour les noms de variables ou utilisez `N` dans les cas simples.
-
-``` cpp
-template <bool without_www>
-struct ExtractDomain
-```
-
-**7.** Pour les classes abstraites (interfaces), vous pouvez ajouter `I` préfixe.
-
-``` cpp
-class IBlockInputStream
-```
-
-**8.** Si vous utilisez une variable localement, vous pouvez utiliser le nom court.
-
-Dans tous les autres cas, utilisez un nom qui décrit la signification.
-
-``` cpp
-bool info_successfully_loaded = false;
-```
-
-**9.** Les noms de `define`les constantes s et globales utilisent ALL_CAPS avec des traits de soulignement.
-
-``` cpp
-#define MAX_SRC_TABLE_NAMES_TO_STORE 1000
-```
-
-**10.** Les noms de fichiers doivent utiliser le même style que leur contenu.
-
-Si un fichier contient une seule classe, nommez-le de la même manière que la classe (CamelCase).
-
-Si le fichier contient une seule fonction, nommez le fichier de la même manière que la fonction (camelCase).
-
-**11.** Si le nom contient une abréviation, puis:
-
--   Pour les noms de variables, l'abréviation doit utiliser des lettres minuscules `mysql_connection` (pas `mySQL_connection`).
--   Pour les noms de classes et de fonctions, conservez les majuscules dans l'abréviation`MySQLConnection` (pas `MySqlConnection`).
-
-**12.** Les arguments du constructeur utilisés uniquement pour initialiser les membres de la classe doivent être nommés de la même manière que les membres de la classe, mais avec un trait de soulignement à la fin.
-
-``` cpp
-FileQueueProcessor(
-    const std::string & path_,
-    const std::string & prefix_,
-    std::shared_ptr<FileHandler> handler_)
-    : path(path_),
-    prefix(prefix_),
-    handler(handler_),
-    log(&Logger::get("FileQueueProcessor"))
-{
-}
-```
-
-Le suffixe de soulignement peut être omis si l'argument n'est pas utilisé dans le corps du constructeur.
-
-**13.** Il n'y a pas de différence dans les noms des variables locales et des membres de classe (aucun préfixe requis).
-
-``` cpp
-timer (not m_timer)
-```
-
-**14.** Pour les constantes dans un `enum`, utilisez CamelCase avec une lettre majuscule. ALL_CAPS est également acceptable. Si l' `enum` est non local, utilisez un `enum class`.
-
-``` cpp
-enum class CompressionMethod
-{
-    QuickLZ = 0,
-    LZ4     = 1,
-};
-```
-
-**15.** Tous les noms doivent être en anglais. La translittération des mots russes n'est pas autorisé.
-
-    not Stroka
-
-**16.** Les abréviations sont acceptables si elles sont bien connues (quand vous pouvez facilement trouver la signification de l'abréviation dans Wikipédia ou dans un moteur de recherche).
-
-    `AST`, `SQL`.
-
-    Not `NVDH` (some random letters)
-
-Les mots incomplets sont acceptables si la version abrégée est d'usage courant.
-
-Vous pouvez également utiliser une abréviation si le nom complet est ensuite incluse dans les commentaires.
-
-**17.** Les noms de fichiers avec le code source C++ doivent avoir `.cpp` extension. Fichiers d'en-tête doit avoir la `.h` extension.
-
-## Comment écrire du Code {#how-to-write-code}
-
-**1.** Gestion de la mémoire.
-
-Désallocation manuelle de la mémoire (`delete`) ne peut être utilisé que dans le code de la bibliothèque.
-
-Dans le code de la bibliothèque, de la `delete` l'opérateur ne peut être utilisé dans des destructeurs.
-
-Dans le code de l'application, la mémoire doit être libérée par l'objet qui la possède.
-
-Exemple:
-
--   Le plus simple est de placer un objet sur la pile, ou d'en faire un membre d'une autre classe.
--   Pour un grand nombre de petits objets, utiliser des récipients.
--   Pour la désallocation automatique d'un petit nombre d'objets qui résident dans le tas, utilisez `shared_ptr/unique_ptr`.
-
-**2.** La gestion des ressources.
-
-Utiliser `RAII` et voir ci-dessus.
-
-**3.** La gestion des erreurs.
-
-Utilisez des exceptions. Dans la plupart des cas, vous avez seulement besoin de lancer une exception, et n'avez pas besoin de l'attraper (à cause de `RAII`).
-
-Dans les applications de traitement de données hors ligne, il est souvent acceptable de ne pas attraper d'exceptions.
-
-Dans les serveurs qui gèrent les demandes des utilisateurs, il suffit généralement d'attraper des exceptions au niveau supérieur du gestionnaire de connexion.
-
-Dans les fonctions de thread, vous devez attraper et conserver toutes les exceptions pour les repasser dans le thread principal après `join`.
-
-``` cpp
-/// If there weren't any calculations yet, calculate the first block synchronously
-if (!started)
-{
-    calculate();
-    started = true;
-}
-else /// If calculations are already in progress, wait for the result
-    pool.wait();
-
-if (exception)
-    exception->rethrow();
-```
-
-Ne cachez jamais les exceptions sans les manipuler. Ne mettez jamais aveuglément toutes les exceptions au journal.
-
-``` cpp
-//Not correct
-catch (...) {}
-```
-
-Si vous devez ignorer certaines exceptions, ne le faites que pour des exceptions spécifiques et repensez le reste.
-
-``` cpp
-catch (const DB::Exception & e)
-{
-    if (e.code() == ErrorCodes::UNKNOWN_AGGREGATE_FUNCTION)
-        return nullptr;
-    else
-        throw;
-}
-```
-
-Lorsque vous utilisez des fonctions avec des codes de réponse ou `errno` toujours vérifier le résultat et de lever une exception en cas d'erreur.
-
-``` cpp
-if (0 != close(fd))
-    throwFromErrno("Cannot close file " + file_name, ErrorCodes::CANNOT_CLOSE_FILE);
-```
-
-`Do not use assert`.
-
-**4.** Les types d'Exception.
-
-Il n'est pas nécessaire d'utiliser une hiérarchie d'exceptions complexe dans le code de l'application. Le texte d'exception doit être compréhensible pour un administrateur système.
-
-**5.** Lancer des exceptions de destructeurs.
-
-Ce n'est pas recommandé, mais il est permis.
-
-Utilisez les options suivantes:
-
--   Créer une fonction (`done()` ou `finalize()`) qui vont faire tout le travail en amont qui pourrait conduire à une exception. Si cette fonction a été appelée, il ne devrait y avoir aucune exception dans le destructeur plus tard.
--   Les tâches trop complexes (comme l'envoi de messages sur le réseau) peuvent être placées dans une méthode distincte que l'utilisateur de la classe devra appeler avant la destruction.
--   Si il y a une exception dans le destructeur, il est préférable de l'enregistrer que de le cacher (si l'enregistreur est disponible).
--   Dans les applications simples, il est acceptable de compter sur `std::terminate` (pour les cas de `noexcept` par défaut en C++11) pour gérer les exceptions.
-
-**6.** Blocs de code anonymes.
-
-Vous pouvez créer un bloc de code séparé à l'intérieur d'une seule fonction afin de rendre certaines variables locales, de sorte que les destructeurs sont appelés à la sortie du bloc.
-
-``` cpp
-Block block = data.in->read();
-
-{
-    std::lock_guard<std::mutex> lock(mutex);
-    data.ready = true;
-    data.block = block;
-}
-
-ready_any.set();
-```
-
-**7.** Multithreading.
-
-Dans les programmes de traitement de données hors ligne:
-
--   Essayez d'obtenir les meilleures performances possibles sur un seul noyau CPU. Vous pouvez ensuite paralléliser votre code si nécessaire.
-
-Dans les applications serveur:
-
--   Utiliser le pool de threads pour traiter les demandes. À ce stade, nous n'avons pas eu de tâches nécessitant un changement de contexte dans l'espace utilisateur.
-
-La fourche n'est pas utilisé pour la parallélisation.
-
-**8.** Synchronisation des threads.
-
-Souvent, il est possible de faire en sorte que différents threads utilisent différentes cellules de mémoire (encore mieux: différentes lignes de cache,) et de ne pas utiliser de synchronisation de thread (sauf `joinAll`).
-
-Si la synchronisation est nécessaire, dans la plupart des cas, il suffit d'utiliser mutex sous `lock_guard`.
-
-Dans d'autres cas, utilisez des primitives de synchronisation système. Ne pas utiliser occupé attendre.
-
-Les opérations atomiques ne doivent être utilisées que dans les cas les plus simples.
-
-N'essayez pas d'implémenter des structures de données sans verrou à moins qu'il ne s'agisse de votre principal domaine d'expertise.
-
-**9.** Pointeurs vs références.
-
-Dans la plupart des cas, préférez les références.
-
-**10.** const.
-
-Utiliser des références constantes, des pointeurs vers des constantes, `const_iterator` et const méthodes.
-
-Considérer `const` pour être par défaut et utiliser non-`const` seulement quand c'est nécessaire.
-
-Lors du passage de variables par valeur, en utilisant `const` habituellement ne fait pas de sens.
-
-**11.** non signé.
-
-Utiliser `unsigned` si nécessaire.
-
-**12.** Les types numériques.
-
-Utiliser les types `UInt8`, `UInt16`, `UInt32`, `UInt64`, `Int8`, `Int16`, `Int32`, et `Int64` ainsi que `size_t`, `ssize_t`, et `ptrdiff_t`.
-
-N'utilisez pas ces types pour les nombres: `signed/unsigned long`, `long long`, `short`, `signed/unsigned char`, `char`.
-
-**13.** Passer des arguments.
-
-Passer des valeurs complexes par référence (y compris `std::string`).
-
-Si une fonction capture la propriété d'un objet créé dans le tas, définissez le type d'argument `shared_ptr` ou `unique_ptr`.
-
-**14.** Les valeurs de retour.
-
-Dans la plupart des cas, il suffit d'utiliser `return`. Ne pas écrire `return std::move(res)`.
-
-Si la fonction alloue un objet sur le tas et le renvoie, utilisez `shared_ptr` ou `unique_ptr`.
-
-Dans de rares cas, vous devrez peut-être renvoyer la valeur via un argument. Dans ce cas, l'argument doit être une référence.
-
-``` cpp
-using AggregateFunctionPtr = std::shared_ptr<IAggregateFunction>;
-
-/** Allows creating an aggregate function by its name.
-  */
-class AggregateFunctionFactory
-{
-public:
-    AggregateFunctionFactory();
-    AggregateFunctionPtr get(const String & name, const DataTypes & argument_types) const;
-```
-
-**15.** espace de noms.
-
-Il n'est pas nécessaire d'utiliser une `namespace` pour le code de l'application.
-
-Les petites bibliothèques n'ont pas besoin de cela non plus.
-
-Pour les bibliothèques moyennes et grandes, mettez tout dans un `namespace`.
-
-Dans la bibliothèque `.h` fichier, vous pouvez utiliser `namespace detail` pour masquer les détails d'implémentation non nécessaires pour le code de l'application.
-
-Dans un `.cpp` fichier, vous pouvez utiliser un `static` ou un espace de noms anonyme pour masquer les symboles.
-
-Aussi, un `namespace` peut être utilisé pour un `enum` pour éviter que les noms correspondants ne tombent dans un `namespace` (mais il est préférable d'utiliser un `enum class`).
-
-**16.** Initialisation différée.
-
-Si des arguments sont requis pour l'initialisation, vous ne devriez normalement pas écrire de constructeur par défaut.
-
-Si plus tard, vous devez retarder l'initialisation, vous pouvez ajouter un constructeur par défaut qui créera un objet invalide. Ou, pour un petit nombre d'objets, vous pouvez utiliser `shared_ptr/unique_ptr`.
-
-``` cpp
-Loader(DB::Connection * connection_, const std::string & query, size_t max_block_size_);
-
-/// For deferred initialization
-Loader() {}
-```
-
-**17.** Des fonctions virtuelles.
-
-Si la classe n'est pas destinée à une utilisation polymorphe, vous n'avez pas besoin de rendre les fonctions virtuelles. Ceci s'applique également pour le destructeur.
-
-**18.** Encodage.
-
-Utilisez UTF-8 partout. Utiliser `std::string`et`char *`. Ne pas utiliser de `std::wstring`et`wchar_t`.
-
-**19.** Journalisation.
-
-Voir les exemples partout dans le code.
-
-Avant de valider, supprimez toute journalisation sans signification et de débogage, ainsi que tout autre type de sortie de débogage.
-
-L'enregistrement des cycles doit être évité, même au niveau de la Trace.
-
-Les journaux doivent être lisibles à tout niveau d'enregistrement.
-
-La journalisation ne doit être utilisée que dans le code de l'application, pour la plupart.
-
-Les messages du journal doivent être écrits en anglais.
-
-Le journal devrait de préférence être compréhensible pour l'administrateur système.
-
-N'utilisez pas de blasphème dans le journal.
-
-Utilisez L'encodage UTF-8 dans le journal. Dans de rares cas, vous pouvez utiliser des caractères non-ASCII dans le journal.
-
-**20.** D'entrée-sortie.
-
-Ne pas utiliser de `iostreams` dans les cycles internes qui sont critiques pour les performances de l'application (et ne jamais utiliser `stringstream`).
-
-L'utilisation de la `DB/IO` la bibliothèque la place.
-
-**21.** La Date et l'heure.
-
-Voir la `DateLUT` bibliothèque.
-
-**22.** comprendre.
-
-Toujours utiliser `#pragma once` au lieu d'inclure des gardes.
-
-**23.** utiliser.
-
-`using namespace` n'est pas utilisé. Vous pouvez utiliser `using` avec quelque chose de spécifique. Mais faire local à l'intérieur d'une classe ou d'une fonction.
-
-**24.** Ne pas utiliser de `trailing return type` pour les fonctions, sauf si nécessaire.
-
-``` cpp
-auto f() -> void
-```
-
-**25.** Déclaration et initialisation des variables.
-
-``` cpp
-//right way
-std::string s = "Hello";
-std::string s{"Hello"};
-
-//wrong way
-auto s = std::string{"Hello"};
-```
-
-**26.** Pour les fonctions virtuelles, écrire `virtual` dans la classe de base, mais d'écrire `override` plutôt `virtual` dans les classes descendantes.
-
-## Fonctionnalités inutilisées de C++ {#unused-features-of-c}
-
-**1.** L'héritage virtuel n'est pas utilisé.
-
-**2.** Les spécificateurs d'Exception de C++03 ne sont pas utilisés.
-
-## Plate {#platform}
-
-**1.** Nous écrivons du code pour une plate-forme spécifique.
-
-Mais toutes choses étant égales par ailleurs, le code multi-plateforme ou portable est préféré.
-
-**2.** Langue: C++20.
-
-**3.** Compilateur: `gcc`. En ce moment (août 2020), le code est compilé en utilisant la version 9.3. (Il peut également être compilé en utilisant `clang 8`.)
-
-La bibliothèque standard est utilisée (`libc++`).
-
-**4.**OS: Linux Ubuntu, pas plus vieux que précis.
-
-**5.**Le Code est écrit pour l'architecture CPU x86_64.
-
-Le jeu D'instructions CPU est l'ensemble minimum pris en charge parmi nos serveurs. Actuellement, il s'agit de SSE 4.2.
-
-**6.** Utiliser `-Wall -Wextra -Werror` drapeaux de compilation.
-
-**7.** Utilisez la liaison statique avec toutes les bibliothèques sauf celles qui sont difficiles à connecter statiquement (voir la sortie de la `ldd` commande).
-
-**8.** Le Code est développé et débogué avec les paramètres de version.
-
-## Outils {#tools}
-
-**1.** KDevelop est un bon IDE.
-
-**2.** Pour le débogage, utilisez `gdb`, `valgrind` (`memcheck`), `strace`, `-fsanitize=...`, ou `tcmalloc_minimal_debug`.
-
-**3.** Pour le profilage, utilisez `Linux Perf`, `valgrind` (`callgrind`), ou `strace -cf`.
-
-**4.** Les Sources sont dans Git.
-
-**5.** Assemblée utilise `CMake`.
-
-**6.** Les programmes sont libérés en utilisant `deb` paquet.
-
-**7.** Les Commits à master ne doivent pas casser la construction.
-
-Bien que seules les révisions sélectionnées soient considérées comme réalisables.
-
-**8.** Faire s'engage aussi souvent que possible, même si le code n'est que partiellement prêt.
-
-Utilisez des branches à cet effet.
-
-Si votre code dans le `master` la branche n'est pas constructible pourtant, l'exclure de la construction avant que le `push`. Vous devrez le terminer ou l'enlever dans quelques jours.
-
-**9.** Pour les modifications non triviales, utilisez les branches et publiez-les sur le serveur.
-
-**10.** Le code inutilisé est supprimé du référentiel.
-
-## Bibliothèque {#libraries}
-
-**1.** La bibliothèque standard C++20 est utilisée (les extensions expérimentales sont autorisées), ainsi que `boost` et `Poco` Framework.
-
-**2.** Si nécessaire, vous pouvez utiliser toutes les bibliothèques bien connues disponibles dans le package OS.
-
-S'il existe déjà une bonne solution, utilisez-la, même si cela signifie que vous devez installer une autre bibliothèque.
-
-(Mais soyez prêt à supprimer les mauvaises bibliothèques du code.)
-
-**3.** Vous pouvez installer une bibliothèque qui n'est pas dans les paquets, les paquets n'ont pas ce que vous souhaitez ou avez une version périmée ou le mauvais type de compilation.
-
-**4.** Si la Bibliothèque est petite et n'a pas son propre système de construction complexe, placez les fichiers source dans le `contrib` dossier.
-
-**5.** La préférence est toujours donnée aux bibliothèques déjà utilisées.
-
-## Recommandations Générales {#general-recommendations-1}
-
-**1.** Écrivez aussi peu de code que possible.
-
-**2.** Essayez la solution la plus simple.
-
-**3.** N'écrivez pas de code tant que vous ne savez pas comment cela va fonctionner et comment la boucle interne fonctionnera.
-
-**4.** Dans les cas les plus simples, utilisez `using` au lieu de classes ou des structures.
-
-**5.** Si possible, n'écrivez pas de constructeurs de copie, d'opérateurs d'affectation, de destructeurs (autres que Virtuels, si la classe contient au moins une fonction virtuelle), de constructeurs de déplacement ou d'opérateurs d'affectation de déplacement. En d'autres termes, les fonctions générées par le compilateur doivent fonctionner correctement. Vous pouvez utiliser `default`.
-
-**6.** La simplification du Code est encouragée. Réduire la taille de votre code si possible.
-
-## Recommandations Supplémentaires {#additional-recommendations}
-
-**1.** Spécifier explicitement `std::` pour les types de `stddef.h`
-
-n'est pas recommandé. En d'autres termes, nous vous recommandons d'écriture `size_t` plutôt `std::size_t` parce que c'est plus court.
-
-Il est acceptable d'ajouter `std::`.
-
-**2.** Spécifier explicitement `std::` pour les fonctions de la bibliothèque C standard
-
-n'est pas recommandé. En d'autres termes, écrire `memcpy` plutôt `std::memcpy`.
-
-La raison en est qu'il existe des fonctions non standard similaires, telles que `memmem`. Nous utilisons ces fonctions à l'occasion. Ces fonctions n'existent pas dans `namespace std`.
-
-Si vous écrivez `std::memcpy` plutôt `memcpy` partout, puis `memmem` sans `std::` va sembler étrange.
-
-Néanmoins, vous pouvez toujours utiliser `std::` si vous le souhaitez.
-
-**3.** Utilisation des fonctions de C lorsque les mêmes sont disponibles dans la bibliothèque C++ standard.
-
-Ceci est acceptable s'il est plus efficace.
-
-Par exemple, l'utilisation `memcpy` plutôt `std::copy` pour copier de gros morceaux de mémoire.
-
-**4.** Arguments de fonction multiligne.
-
-L'un des styles d'emballage suivants est autorisé:
-
-``` cpp
-function(
-  T1 x1,
-  T2 x2)
-```
-
-``` cpp
-function(
-  size_t left, size_t right,
-  const & RangesInDataParts ranges,
-  size_t limit)
-```
-
-``` cpp
-function(size_t left, size_t right,
-  const & RangesInDataParts ranges,
-  size_t limit)
-```
-
-``` cpp
-function(size_t left, size_t right,
-      const & RangesInDataParts ranges,
-      size_t limit)
-```
-
-``` cpp
-function(
-      size_t left,
-      size_t right,
-      const & RangesInDataParts ranges,
-      size_t limit)
-```
-
-[Article Original](https://clickhouse.tech/docs/en/development/style/) <!--hide-->
diff --git a/docs/fr/engines/database-engines/atomic.md b/docs/fr/engines/database-engines/atomic.md
deleted file mode 100644
index f019b94a00b2..000000000000
--- a/docs/fr/engines/database-engines/atomic.md
+++ /dev/null
@@ -1,17 +0,0 @@
----
-toc_priority: 32
-toc_title: Atomic
----
-
-
-# Atomic {#atomic}
-
-It is supports non-blocking `DROP` and `RENAME TABLE` queries and atomic `EXCHANGE TABLES t1 AND t2` queries. Atomic database engine is used by default.
-
-## Creating a Database {#creating-a-database}
-
-```sql
-CREATE DATABASE test ENGINE = Atomic;
-```
-
-[Original article](https://clickhouse.tech/docs/en/engines/database_engines/atomic/) <!--hide-->
diff --git a/docs/fr/engines/database-engines/index.md b/docs/fr/engines/database-engines/index.md
deleted file mode 100644
index a5b08b634530..000000000000
--- a/docs/fr/engines/database-engines/index.md
+++ /dev/null
@@ -1,21 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Moteurs De Base De Donn\xE9es"
-toc_priority: 27
-toc_title: Introduction
----
-
-# Moteurs De Base De Données {#database-engines}
-
-Moteurs de base de données vous permettent de travailler avec des tables.
-
-Par défaut, ClickHouse utilise son moteur de base de données natif, qui fournit [moteurs de table](../../engines/table-engines/index.md) et un [Dialecte SQL](../../sql-reference/syntax.md).
-
-Vous pouvez également utiliser les moteurs de base de données suivants:
-
--   [MySQL](mysql.md)
-
--   [Paresseux](lazy.md)
-
-[Article Original](https://clickhouse.tech/docs/en/database_engines/) <!--hide-->
diff --git a/docs/fr/engines/database-engines/lazy.md b/docs/fr/engines/database-engines/lazy.md
deleted file mode 100644
index 545936ae861b..000000000000
--- a/docs/fr/engines/database-engines/lazy.md
+++ /dev/null
@@ -1,18 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 31
-toc_title: Paresseux
----
-
-# Paresseux {#lazy}
-
-Conserve les tables en RAM uniquement `expiration_time_in_seconds` secondes après le dernier accès. Peut être utilisé uniquement avec les tables \* Log.
-
-Il est optimisé pour stocker de nombreuses petites tables \*Log, pour lesquelles il y a un long intervalle de temps entre les accès.
-
-## La création d'une Base de données {#creating-a-database}
-
-    CREATE DATABASE testlazy ENGINE = Lazy(expiration_time_in_seconds);
-
-[Article Original](https://clickhouse.tech/docs/en/database_engines/lazy/) <!--hide-->
diff --git a/docs/fr/engines/database-engines/mysql.md b/docs/fr/engines/database-engines/mysql.md
deleted file mode 100644
index b40c8961b92c..000000000000
--- a/docs/fr/engines/database-engines/mysql.md
+++ /dev/null
@@ -1,135 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 30
-toc_title: MySQL
----
-
-# MySQL {#mysql}
-
-Permet de se connecter à des bases de données sur un serveur MySQL distant et `INSERT` et `SELECT` requêtes pour échanger des données entre Clickhouse et MySQL.
-
-Le `MySQL` moteur de base de données traduire les requêtes sur le serveur MySQL afin que vous puissiez effectuer des opérations telles que `SHOW TABLES` ou `SHOW CREATE TABLE`.
-
-Vous ne pouvez pas effectuer les requêtes suivantes:
-
--   `RENAME`
--   `CREATE TABLE`
--   `ALTER`
-
-## La création d'une Base de données {#creating-a-database}
-
-``` sql
-CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster]
-ENGINE = MySQL('host:port', ['database' | database], 'user', 'password')
-```
-
-**Les Paramètres Du Moteur**
-
--   `host:port` — MySQL server address.
--   `database` — Remote database name.
--   `user` — MySQL user.
--   `password` — User password.
-
-## Types De Données Soutien {#data_types-support}
-
-| MySQL                            | ClickHouse                                                   |
-|----------------------------------|--------------------------------------------------------------|
-| UNSIGNED TINYINT                 | [UInt8](../../sql-reference/data-types/int-uint.md)          |
-| TINYINT                          | [Int8](../../sql-reference/data-types/int-uint.md)           |
-| UNSIGNED SMALLINT                | [UInt16](../../sql-reference/data-types/int-uint.md)         |
-| SMALLINT                         | [Int16](../../sql-reference/data-types/int-uint.md)          |
-| UNSIGNED INT, UNSIGNED MEDIUMINT | [UInt32](../../sql-reference/data-types/int-uint.md)         |
-| INT, MEDIUMINT                   | [Int32](../../sql-reference/data-types/int-uint.md)          |
-| UNSIGNED BIGINT                  | [UInt64](../../sql-reference/data-types/int-uint.md)         |
-| BIGINT                           | [Int64](../../sql-reference/data-types/int-uint.md)          |
-| FLOAT                            | [Float32](../../sql-reference/data-types/float.md)           |
-| DOUBLE                           | [Float64](../../sql-reference/data-types/float.md)           |
-| DATE                             | [Date](../../sql-reference/data-types/date.md)               |
-| DATETIME, TIMESTAMP              | [DateTime](../../sql-reference/data-types/datetime.md)       |
-| BINARY                           | [FixedString](../../sql-reference/data-types/fixedstring.md) |
-
-Tous les autres types de données MySQL sont convertis en [Chaîne](../../sql-reference/data-types/string.md).
-
-[Nullable](../../sql-reference/data-types/nullable.md) est pris en charge.
-
-## Exemples D'utilisation {#examples-of-use}
-
-Table dans MySQL:
-
-``` text
-mysql> USE test;
-Database changed
-
-mysql> CREATE TABLE `mysql_table` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `float` FLOAT NOT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into mysql_table (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from mysql_table;
-+------+-----+
-| int_id | value |
-+------+-----+
-|      1 |     2 |
-+------+-----+
-1 row in set (0,00 sec)
-```
-
-Base de données dans ClickHouse, échange de données avec le serveur MySQL:
-
-``` sql
-CREATE DATABASE mysql_db ENGINE = MySQL('localhost:3306', 'test', 'my_user', 'user_password')
-```
-
-``` sql
-SHOW DATABASES
-```
-
-``` text
-┌─name─────┐
-│ default  │
-│ mysql_db │
-│ system   │
-└──────────┘
-```
-
-``` sql
-SHOW TABLES FROM mysql_db
-```
-
-``` text
-┌─name─────────┐
-│  mysql_table │
-└──────────────┘
-```
-
-``` sql
-SELECT * FROM mysql_db.mysql_table
-```
-
-``` text
-┌─int_id─┬─value─┐
-│      1 │     2 │
-└────────┴───────┘
-```
-
-``` sql
-INSERT INTO mysql_db.mysql_table VALUES (3,4)
-```
-
-``` sql
-SELECT * FROM mysql_db.mysql_table
-```
-
-``` text
-┌─int_id─┬─value─┐
-│      1 │     2 │
-│      3 │     4 │
-└────────┴───────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/database_engines/mysql/) <!--hide-->
diff --git a/docs/fr/engines/index.md b/docs/fr/engines/index.md
deleted file mode 100644
index 3dfb7bf37482..000000000000
--- a/docs/fr/engines/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Moteur
-toc_priority: 25
----
-
-
diff --git a/docs/fr/engines/table-engines/index.md b/docs/fr/engines/table-engines/index.md
deleted file mode 100644
index 51654fd6bb27..000000000000
--- a/docs/fr/engines/table-engines/index.md
+++ /dev/null
@@ -1,85 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Moteurs De Table
-toc_priority: 26
-toc_title: Introduction
----
-
-# Moteurs De Table {#table_engines}
-
-Le moteur de table (type de table) détermine:
-
--   Comment et où les données sont stockées, où les écrire et où les lire.
--   Quelles requêtes sont prises en charge et comment.
--   Accès simultané aux données.
--   Utilisation des index, si elle est présente.
--   Indique si l'exécution d'une requête multithread est possible.
--   Paramètres de réplication des données.
-
-## Familles De Moteurs {#engine-families}
-
-### MergeTree {#mergetree}
-
-Les moteurs de table les plus universels et fonctionnels pour les tâches à forte charge. La propriété partagée par ces moteurs est l'insertion rapide des données avec traitement ultérieur des données d'arrière-plan. `MergeTree` les moteurs de la famille prennent en charge la réplication des données (avec [Répliqué\*](mergetree-family/replication.md#table_engines-replication) versions de moteurs), le partitionnement, et d'autres fonctionnalités non prises en charge dans d'autres moteurs.
-
-Moteurs dans la famille:
-
--   [MergeTree](mergetree-family/mergetree.md#mergetree)
--   [ReplacingMergeTree](mergetree-family/replacingmergetree.md#replacingmergetree)
--   [SummingMergeTree](mergetree-family/summingmergetree.md#summingmergetree)
--   [AggregatingMergeTree](mergetree-family/aggregatingmergetree.md#aggregatingmergetree)
--   [CollapsingMergeTree](mergetree-family/collapsingmergetree.md#table_engine-collapsingmergetree)
--   [VersionedCollapsingMergeTree](mergetree-family/versionedcollapsingmergetree.md#versionedcollapsingmergetree)
--   [GraphiteMergeTree](mergetree-family/graphitemergetree.md#graphitemergetree)
-
-### Journal {#log}
-
-Léger [moteur](log-family/index.md) avec une fonctionnalité minimale. Ils sont les plus efficaces lorsque vous devez écrire rapidement de nombreuses petites tables (jusqu'à environ 1 million de lignes) et les lire plus tard dans leur ensemble.
-
-Moteurs dans la famille:
-
--   [TinyLog](log-family/tinylog.md#tinylog)
--   [StripeLog](log-family/stripelog.md#stripelog)
--   [Journal](log-family/log.md#log)
-
-### Moteurs D'Intégration {#integration-engines}
-
-Moteurs de communication avec d'autres systèmes de stockage et de traitement de données.
-
-Moteurs dans la famille:
-
--   [Kafka](integrations/kafka.md#kafka)
--   [MySQL](integrations/mysql.md#mysql)
--   [ODBC](integrations/odbc.md#table-engine-odbc)
--   [JDBC](integrations/jdbc.md#table-engine-jdbc)
--   [HDFS](integrations/hdfs.md#hdfs)
-
-### Moteurs Spéciaux {#special-engines}
-
-Moteurs dans la famille:
-
--   [Distribué](special/distributed.md#distributed)
--   [MaterializedView](special/materializedview.md#materializedview)
--   [Dictionnaire](special/dictionary.md#dictionary)
--   \[Fusion\](spécial/de fusion.md#fusion
--   [Fichier](special/file.md#file)
--   [NULL](special/null.md#null)
--   [Définir](special/set.md#set)
--   [Rejoindre](special/join.md#join)
--   [URL](special/url.md#table_engines-url)
--   [Vue](special/view.md#table_engines-view)
--   [Mémoire](special/memory.md#memory)
--   [Tampon](special/buffer.md#buffer)
-
-## Les Colonnes Virtuelles {#table_engines-virtual_columns}
-
-Colonne virtuelle est un attribut de moteur de table intégrale qui est défini dans le code source du moteur.
-
-Vous ne devez pas spécifier de colonnes virtuelles dans `CREATE TABLE` requête et vous ne pouvez pas les voir dans `SHOW CREATE TABLE` et `DESCRIBE TABLE` les résultats de la requête. Les colonnes virtuelles sont également en lecture seule, vous ne pouvez donc pas insérer de données dans des colonnes virtuelles.
-
-Pour sélectionner des données dans une colonne virtuelle, vous devez spécifier son nom `SELECT` requête. `SELECT *` ne renvoie pas de valeurs à partir de colonnes virtuelles.
-
-Si vous créez une table avec une colonne portant le même nom que l'une des colonnes virtuelles de la table, la colonne virtuelle devient inaccessible. Nous ne recommandons pas de faire cela. Pour éviter les conflits, les noms de colonnes virtuelles sont généralement précédés d'un trait de soulignement.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/integrations/hdfs.md b/docs/fr/engines/table-engines/integrations/hdfs.md
deleted file mode 100644
index 7416ed7ba91b..000000000000
--- a/docs/fr/engines/table-engines/integrations/hdfs.md
+++ /dev/null
@@ -1,123 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: HDFS
----
-
-# HDFS {#table_engines-hdfs}
-
-Ce moteur fournit l'intégration avec [Apache Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop) l'écosystème en permettant de gérer les données sur [HDFS](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)via ClickHouse. Ce moteur est similaire
-à l' [Fichier](../special/file.md#table_engines-file) et [URL](../special/url.md#table_engines-url) moteurs, mais fournit des fonctionnalités spécifiques Hadoop.
-
-## Utilisation {#usage}
-
-``` sql
-ENGINE = HDFS(URI, format)
-```
-
-Le `URI` paramètre est L'URI du fichier entier dans HDFS.
-Le `format` paramètre spécifie l'un des formats de fichier disponibles. Effectuer
-`SELECT` requêtes, le format doit être pris en charge pour l'entrée, et à effectuer
-`INSERT` queries – for output. The available formats are listed in the
-[Format](../../../interfaces/formats.md#formats) section.
-Le chemin le cadre de `URI` peut contenir des globules. Dans ce cas, le tableau serait en lecture seule.
-
-**Exemple:**
-
-**1.** Configurer le `hdfs_engine_table` table:
-
-``` sql
-CREATE TABLE hdfs_engine_table (name String, value UInt32) ENGINE=HDFS('hdfs://hdfs1:9000/other_storage', 'TSV')
-```
-
-**2.** Remplir le fichier:
-
-``` sql
-INSERT INTO hdfs_engine_table VALUES ('one', 1), ('two', 2), ('three', 3)
-```
-
-**3.** Interroger les données:
-
-``` sql
-SELECT * FROM hdfs_engine_table LIMIT 2
-```
-
-``` text
-┌─name─┬─value─┐
-│ one  │     1 │
-│ two  │     2 │
-└──────┴───────┘
-```
-
-## Détails De Mise En Œuvre {#implementation-details}
-
--   Les lectures et les écritures peuvent être parallèles
--   Pas pris en charge:
-    -   `ALTER` et `SELECT...SAMPLE` opérations.
-    -   Index.
-    -   Réplication.
-
-**Globs dans le chemin**
-
-Plusieurs composants de chemin peuvent avoir des globs. Pour être traité, le fichier devrait exister et correspondre au modèle de chemin entier. Liste des fichiers détermine pendant `SELECT` (pas à l' `CREATE` moment).
-
--   `*` — Substitutes any number of any characters except `/` y compris la chaîne vide.
--   `?` — Substitutes any single character.
--   `{some_string,another_string,yet_another_one}` — Substitutes any of strings `'some_string', 'another_string', 'yet_another_one'`.
--   `{N..M}` — Substitutes any number in range from N to M including both borders.
-
-Les Constructions avec `{}` sont similaires à l' [distant](../../../sql-reference/table-functions/remote.md) table de fonction.
-
-**Exemple**
-
-1.  Supposons que nous ayons plusieurs fichiers au format TSV avec les URI suivants sur HDFS:
-
--   ‘hdfs://hdfs1:9000/some_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_3’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_3’
-
-1.  Il y a plusieurs façons de faire une table composée des six fichiers:
-
-<!-- -->
-
-``` sql
-CREATE TABLE table_with_range (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV')
-```
-
-Une autre façon:
-
-``` sql
-CREATE TABLE table_with_question_mark (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_?', 'TSV')
-```
-
-Table se compose de tous les fichiers dans les deux répertoires (tous les fichiers doivent satisfaire le format et le schéma décrits dans la requête):
-
-``` sql
-CREATE TABLE table_with_asterisk (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV')
-```
-
-!!! warning "Avertissement"
-    Si la liste des fichiers contient des plages de nombres avec des zéros en tête, utilisez la construction avec des accolades pour chaque chiffre séparément ou utilisez `?`.
-
-**Exemple**
-
-Créer une table avec des fichiers nommés `file000`, `file001`, … , `file999`:
-
-``` sql
-CREARE TABLE big_table (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV')
-```
-
-## Les Colonnes Virtuelles {#virtual-columns}
-
--   `_path` — Path to the file.
--   `_file` — Name of the file.
-
-**Voir Aussi**
-
--   [Les colonnes virtuelles](../index.md#table_engines-virtual_columns)
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/hdfs/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/integrations/index.md b/docs/fr/engines/table-engines/integrations/index.md
deleted file mode 100644
index 205b631880b0..000000000000
--- a/docs/fr/engines/table-engines/integrations/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Int\xE9gration"
-toc_priority: 30
----
-
-
diff --git a/docs/fr/engines/table-engines/integrations/jdbc.md b/docs/fr/engines/table-engines/integrations/jdbc.md
deleted file mode 100644
index e67209f27090..000000000000
--- a/docs/fr/engines/table-engines/integrations/jdbc.md
+++ /dev/null
@@ -1,90 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: JDBC
----
-
-# JDBC {#table-engine-jdbc}
-
-Permet à ClickHouse de se connecter à des bases de données externes via [JDBC](https://en.wikipedia.org/wiki/Java_Database_Connectivity).
-
-Pour implémenter la connexion JDBC, ClickHouse utilise le programme séparé [clickhouse-JDBC-pont](https://github.com/alex-krash/clickhouse-jdbc-bridge) cela devrait fonctionner comme un démon.
-
-Ce moteur prend en charge le [Nullable](../../../sql-reference/data-types/nullable.md) type de données.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name
-(
-    columns list...
-)
-ENGINE = JDBC(dbms_uri, external_database, external_table)
-```
-
-**Les Paramètres Du Moteur**
-
--   `dbms_uri` — URI of an external DBMS.
-
-    Format: `jdbc:<driver_name>://<host_name>:<port>/?user=<username>&password=<password>`.
-    Exemple pour MySQL: `jdbc:mysql://localhost:3306/?user=root&password=root`.
-
--   `external_database` — Database in an external DBMS.
-
--   `external_table` — Name of the table in `external_database`.
-
-## Exemple D'Utilisation {#usage-example}
-
-Création d'une table dans le serveur MySQL en se connectant directement avec son client console:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Création d'une table dans le serveur ClickHouse et sélection des données:
-
-``` sql
-CREATE TABLE jdbc_table
-(
-    `int_id` Int32,
-    `int_nullable` Nullable(Int32),
-    `float` Float32,
-    `float_nullable` Nullable(Float32)
-)
-ENGINE JDBC('jdbc:mysql://localhost:3306/?user=root&password=root', 'test', 'test')
-```
-
-``` sql
-SELECT *
-FROM jdbc_table
-```
-
-``` text
-┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐
-│      1 │         ᴺᵁᴸᴸ │     2 │           ᴺᵁᴸᴸ │
-└────────┴──────────────┴───────┴────────────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [Fonction de table JDBC](../../../sql-reference/table-functions/jdbc.md).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/jdbc/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/integrations/kafka.md b/docs/fr/engines/table-engines/integrations/kafka.md
deleted file mode 100644
index c8c16f2c3c4d..000000000000
--- a/docs/fr/engines/table-engines/integrations/kafka.md
+++ /dev/null
@@ -1,180 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 32
-toc_title: Kafka
----
-
-# Kafka {#kafka}
-
-Ce moteur fonctionne avec [Apache Kafka](http://kafka.apache.org/).
-
-Kafka vous permet de:
-
--   Publier ou s'abonner aux flux de données.
--   Organiser le stockage tolérant aux pannes.
--   Traiter les flux à mesure qu'ils deviennent disponibles.
-
-## Création d'une Table {#table_engine-kafka-creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = Kafka()
-SETTINGS
-    kafka_broker_list = 'host:port',
-    kafka_topic_list = 'topic1,topic2,...',
-    kafka_group_name = 'group_name',
-    kafka_format = 'data_format'[,]
-    [kafka_row_delimiter = 'delimiter_symbol',]
-    [kafka_schema = '',]
-    [kafka_num_consumers = N,]
-    [kafka_max_block_size = 0,]
-    [kafka_skip_broken_messages = N,]
-    [kafka_commit_every_batch = 0]
-```
-
-Les paramètres requis:
-
--   `kafka_broker_list` – A comma-separated list of brokers (for example, `localhost:9092`).
--   `kafka_topic_list` – A list of Kafka topics.
--   `kafka_group_name` – A group of Kafka consumers. Reading margins are tracked for each group separately. If you don't want messages to be duplicated in the cluster, use the same group name everywhere.
--   `kafka_format` – Message format. Uses the same notation as the SQL `FORMAT` la fonction, tels que `JSONEachRow`. Pour plus d'informations, voir le [Format](../../../interfaces/formats.md) section.
-
-Paramètres facultatifs:
-
--   `kafka_row_delimiter` – Delimiter character, which ends the message.
--   `kafka_schema` – Parameter that must be used if the format requires a schema definition. For example, [Cap'n Proto](https://capnproto.org/) nécessite le chemin d'accès du fichier de schéma et le nom de la racine `schema.capnp:Message` objet.
--   `kafka_num_consumers` – The number of consumers per table. Default: `1`. Spécifiez plus de consommateurs si le débit d'un consommateur est insuffisant. Le nombre total de consommateurs ne doit pas dépasser le nombre de partitions dans la rubrique, car un seul consommateur peut être affecté par partition.
--   `kafka_max_block_size` - La taille maximale du lot (dans les messages) pour le sondage (par défaut: `max_block_size`).
--   `kafka_skip_broken_messages` – Kafka message parser tolerance to schema-incompatible messages per block. Default: `0`. Si `kafka_skip_broken_messages = N` puis le moteur saute *N* Messages Kafka qui ne peuvent pas être analysés (un message est égal à une ligne de données).
--   `kafka_commit_every_batch` - Commit chaque lot consommé et traité au lieu d'un seul commit après avoir écrit un bloc entier (par défaut: `0`).
-
-Exemple:
-
-``` sql
-  CREATE TABLE queue (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow');
-
-  SELECT * FROM queue LIMIT 5;
-
-  CREATE TABLE queue2 (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka SETTINGS kafka_broker_list = 'localhost:9092',
-                            kafka_topic_list = 'topic',
-                            kafka_group_name = 'group1',
-                            kafka_format = 'JSONEachRow',
-                            kafka_num_consumers = 4;
-
-  CREATE TABLE queue2 (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka('localhost:9092', 'topic', 'group1')
-              SETTINGS kafka_format = 'JSONEachRow',
-                       kafka_num_consumers = 4;
-```
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets. Si possible, optez anciens projets à la méthode décrite ci-dessus.
-
-``` sql
-Kafka(kafka_broker_list, kafka_topic_list, kafka_group_name, kafka_format
-      [, kafka_row_delimiter, kafka_schema, kafka_num_consumers, kafka_skip_broken_messages])
-```
-
-</details>
-
-## Description {#description}
-
-Les messages livrés sont suivis automatiquement, de sorte que chaque message d'un groupe n'est compté qu'une seule fois. Si vous souhaitez obtenir les données deux fois, créez une copie de la table avec un autre nom de groupe.
-
-Les groupes sont flexibles et synchronisés sur le cluster. Par exemple, si vous avez 10 thèmes et 5 copies d'une table dans un cluster, chaque copie obtient 2 sujets. Si le nombre de copies change, les rubriques sont redistribuées automatiquement entre les copies. En savoir plus à ce sujet à http://kafka.apache.org/intro.
-
-`SELECT` n'est pas particulièrement utile pour la lecture de messages (sauf pour le débogage), car chaque message ne peut être lu qu'une seule fois. Il est plus pratique de créer des threads en temps réel à l'aide de vues matérialisées. Pour ce faire:
-
-1.  Utilisez le moteur pour créer un consommateur Kafka et considérez-le comme un flux de données.
-2.  Créez une table avec la structure souhaitée.
-3.  Créer une vue matérialisée qui convertit les données du moteur et le met dans une table créée précédemment.
-
-Lorsque l' `MATERIALIZED VIEW` rejoint le moteur, il commence à collecter des données en arrière-plan. Cela vous permet de recevoir continuellement des messages de Kafka et de les convertir au format requis en utilisant `SELECT`.
-Une table kafka peut avoir autant de vues matérialisées que vous le souhaitez, elles ne lisent pas directement les données de la table kafka, mais reçoivent de nouveaux enregistrements( en blocs), de cette façon vous pouvez écrire sur plusieurs tables avec différents niveaux de détail (avec regroupement - agrégation et sans).
-
-Exemple:
-
-``` sql
-  CREATE TABLE queue (
-    timestamp UInt64,
-    level String,
-    message String
-  ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow');
-
-  CREATE TABLE daily (
-    day Date,
-    level String,
-    total UInt64
-  ) ENGINE = SummingMergeTree(day, (day, level), 8192);
-
-  CREATE MATERIALIZED VIEW consumer TO daily
-    AS SELECT toDate(toDateTime(timestamp)) AS day, level, count() as total
-    FROM queue GROUP BY day, level;
-
-  SELECT level, sum(total) FROM daily GROUP BY level;
-```
-
-Pour améliorer les performances, les messages reçus sont regroupées en blocs de la taille de [max_insert_block_size](../../../operations/server-configuration-parameters/settings.md#settings-max_insert_block_size). Si le bloc n'a pas été formé à l'intérieur [stream_flush_interval_ms](../../../operations/server-configuration-parameters/settings.md) millisecondes, les données seront vidées dans le tableau, indépendamment de l'intégralité du bloc.
-
-Pour arrêter de recevoir des données de rubrique ou pour modifier la logique de conversion, détachez la vue matérialisée:
-
-``` sql
-  DETACH TABLE consumer;
-  ATTACH TABLE consumer;
-```
-
-Si vous souhaitez modifier la table cible en utilisant `ALTER`, nous vous recommandons de désactiver la vue matériel pour éviter les divergences entre la table cible et les données de la vue.
-
-## Configuration {#configuration}
-
-Similaire à GraphiteMergeTree, le moteur Kafka prend en charge la configuration étendue à l'aide du fichier de configuration ClickHouse. Il y a deux clés de configuration que vous pouvez utiliser: global (`kafka`) et des rubriques (`kafka_*`). La configuration globale est appliquée en premier, puis la configuration au niveau de la rubrique est appliquée (si elle existe).
-
-``` xml
-  <!-- Global configuration options for all tables of Kafka engine type -->
-  <kafka>
-    <debug>cgrp</debug>
-    <auto_offset_reset>smallest</auto_offset_reset>
-  </kafka>
-
-  <!-- Configuration specific for topic "logs" -->
-  <kafka_logs>
-    <retry_backoff_ms>250</retry_backoff_ms>
-    <fetch_min_bytes>100000</fetch_min_bytes>
-  </kafka_logs>
-```
-
-Pour obtenir une liste des options de configuration possibles, consultez [librdkafka référence de configuration](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md). Utilisez le trait de soulignement (`_`) au lieu d'un point dans la configuration ClickHouse. Exemple, `check.crcs=true` sera `<check_crcs>true</check_crcs>`.
-
-## Les Colonnes Virtuelles {#virtual-columns}
-
--   `_topic` — Kafka topic.
--   `_key` — Key of the message.
--   `_offset` — Offset of the message.
--   `_timestamp` — Timestamp of the message.
--   `_partition` — Partition of Kafka topic.
-
-**Voir Aussi**
-
--   [Les colonnes virtuelles](../index.md#table_engines-virtual_columns)
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/kafka/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/integrations/mysql.md b/docs/fr/engines/table-engines/integrations/mysql.md
deleted file mode 100644
index bc291e0aebe4..000000000000
--- a/docs/fr/engines/table-engines/integrations/mysql.md
+++ /dev/null
@@ -1,105 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: MySQL
----
-
-# Mysql {#mysql}
-
-Le moteur MySQL vous permet d'effectuer `SELECT` requêtes sur les données stockées sur un serveur MySQL distant.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2],
-    ...
-) ENGINE = MySQL('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']);
-```
-
-Voir une description détaillée de la [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) requête.
-
-La structure de la table peut différer de la structure de la table MySQL d'origine:
-
--   Les noms de colonnes doivent être les mêmes que dans la table MySQL d'origine, mais vous pouvez utiliser seulement certaines de ces colonnes et dans n'importe quel ordre.
--   Les types de colonnes peuvent différer de ceux de la table MySQL d'origine. ClickHouse essaie de [jeter](../../../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) valeurs des types de données ClickHouse.
-
-**Les Paramètres Du Moteur**
-
--   `host:port` — MySQL server address.
-
--   `database` — Remote database name.
-
--   `table` — Remote table name.
-
--   `user` — MySQL user.
-
--   `password` — User password.
-
--   `replace_query` — Flag that converts `INSERT INTO` les requêtes de `REPLACE INTO`. Si `replace_query=1` la requête est substitué.
-
--   `on_duplicate_clause` — The `ON DUPLICATE KEY on_duplicate_clause` expression qui est ajoutée à la `INSERT` requête.
-
-    Exemple: `INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1`, où `on_duplicate_clause` être `UPDATE c2 = c2 + 1`. Voir la [Documentation de MySQL](https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html) pour trouver lequel `on_duplicate_clause` vous pouvez utiliser avec le `ON DUPLICATE KEY` clause.
-
-    Spécifier `on_duplicate_clause` vous avez besoin de passer `0` à l' `replace_query` paramètre. Si vous passez simultanément `replace_query = 1` et `on_duplicate_clause`, Clickhouse génère une exception.
-
-Simple `WHERE` des clauses telles que `=, !=, >, >=, <, <=` sont exécutés sur le serveur MySQL.
-
-Le reste des conditions et le `LIMIT` les contraintes d'échantillonnage sont exécutées dans ClickHouse uniquement après la fin de la requête à MySQL.
-
-## Exemple D'Utilisation {#usage-example}
-
-Table dans MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Table dans ClickHouse, récupération des données de la table MySQL créée ci-dessus:
-
-``` sql
-CREATE TABLE mysql_table
-(
-    `float_nullable` Nullable(Float32),
-    `int_id` Int32
-)
-ENGINE = MySQL('localhost:3306', 'test', 'test', 'bayonet', '123')
-```
-
-``` sql
-SELECT * FROM mysql_table
-```
-
-``` text
-┌─float_nullable─┬─int_id─┐
-│           ᴺᵁᴸᴸ │      1 │
-└────────────────┴────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [Le ‘mysql’ fonction de table](../../../sql-reference/table-functions/mysql.md)
--   [Utilisation de MySQL comme source de dictionnaire externe](../../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-mysql)
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/mysql/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/integrations/odbc.md b/docs/fr/engines/table-engines/integrations/odbc.md
deleted file mode 100644
index ee0ffb171d2f..000000000000
--- a/docs/fr/engines/table-engines/integrations/odbc.md
+++ /dev/null
@@ -1,132 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: ODBC
----
-
-# ODBC {#table-engine-odbc}
-
-Permet à ClickHouse de se connecter à des bases de données externes via [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity).
-
-Pour implémenter en toute sécurité les connexions ODBC, ClickHouse utilise un programme distinct `clickhouse-odbc-bridge`. Si le pilote ODBC est chargé directement depuis `clickhouse-server`, les problèmes de pilote peuvent planter le serveur ClickHouse. Clickhouse démarre automatiquement `clickhouse-odbc-bridge` lorsque cela est nécessaire. Le programme ODBC bridge est installé à partir du même package que `clickhouse-server`.
-
-Ce moteur prend en charge le [Nullable](../../../sql-reference/data-types/nullable.md) type de données.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1],
-    name2 [type2],
-    ...
-)
-ENGINE = ODBC(connection_settings, external_database, external_table)
-```
-
-Voir une description détaillée de la [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) requête.
-
-La structure de la table peut différer de la structure de la table source:
-
--   Les noms de colonnes doivent être les mêmes que dans la table source, mais vous pouvez utiliser quelques-unes de ces colonnes et dans n'importe quel ordre.
--   Les types de colonnes peuvent différer de ceux de la table source. ClickHouse essaie de [jeter](../../../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) valeurs des types de données ClickHouse.
-
-**Les Paramètres Du Moteur**
-
--   `connection_settings` — Name of the section with connection settings in the `odbc.ini` fichier.
--   `external_database` — Name of a database in an external DBMS.
--   `external_table` — Name of a table in the `external_database`.
-
-## Exemple D'Utilisation {#usage-example}
-
-**Récupération des données de L'installation MySQL locale via ODBC**
-
-Cet exemple est vérifié pour Ubuntu Linux 18.04 et MySQL server 5.7.
-
-Assurez-vous que unixODBC et MySQL Connector sont installés.
-
-Par défaut (si installé à partir de paquets), ClickHouse démarre en tant qu'utilisateur `clickhouse`. Ainsi, vous devez créer et configurer cet utilisateur dans le serveur MySQL.
-
-``` bash
-$ sudo mysql
-```
-
-``` sql
-mysql> CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse';
-mysql> GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;
-```
-
-Puis configurez la connexion dans `/etc/odbc.ini`.
-
-``` bash
-$ cat /etc/odbc.ini
-[mysqlconn]
-DRIVER = /usr/local/lib/libmyodbc5w.so
-SERVER = 127.0.0.1
-PORT = 3306
-DATABASE = test
-USERNAME = clickhouse
-PASSWORD = clickhouse
-```
-
-Vous pouvez vérifier la connexion en utilisant le `isql` utilitaire de l'installation unixODBC.
-
-``` bash
-$ isql -v mysqlconn
-+-------------------------+
-| Connected!                            |
-|                                       |
-...
-```
-
-Table dans MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Table dans ClickHouse, récupération des données de la table MySQL:
-
-``` sql
-CREATE TABLE odbc_t
-(
-    `int_id` Int32,
-    `float_nullable` Nullable(Float32)
-)
-ENGINE = ODBC('DSN=mysqlconn', 'test', 'test')
-```
-
-``` sql
-SELECT * FROM odbc_t
-```
-
-``` text
-┌─int_id─┬─float_nullable─┐
-│      1 │           ᴺᵁᴸᴸ │
-└────────┴────────────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [Dictionnaires externes ODBC](../../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-odbc)
--   [Fonction de table ODBC](../../../sql-reference/table-functions/odbc.md)
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/odbc/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/log-family/index.md b/docs/fr/engines/table-engines/log-family/index.md
deleted file mode 100644
index a4005fcbd665..000000000000
--- a/docs/fr/engines/table-engines/log-family/index.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Journal De La Famille
-toc_title: Introduction
----
-
-# Famille De Moteurs En Rondins {#log-engine-family}
-
-Ces moteurs ont été développés pour les scénarios où vous devez écrire rapidement de nombreuses petites tables (jusqu'à environ 1 million de lignes) et les lire plus tard dans leur ensemble.
-
-Les moteurs de la famille:
-
--   [StripeLog](stripelog.md)
--   [Journal](log.md)
--   [TinyLog](tinylog.md)
-
-## Propriétés Communes {#common-properties}
-
-Moteur:
-
--   Stocker des données sur un disque.
-
--   Ajouter des données à la fin du fichier lors de l'écriture.
-
--   Bloque simultanées dans l'accès aux données.
-
-    Lors `INSERT` requêtes, la table est verrouillée, et d'autres requêtes pour la lecture et l'écriture de données attendent que la table se déverrouille. S'il n'y a pas de requêtes d'écriture de données, un certain nombre de requêtes de lecture de données peuvent être effectuées simultanément.
-
--   Ne prennent pas en charge [mutation](../../../sql-reference/statements/alter.md#alter-mutations) opérations.
-
--   Ne prennent pas en charge les index.
-
-    Cela signifie que `SELECT` les requêtes pour les plages de données ne sont pas efficaces.
-
--   N'écrivez pas de données de manière atomique.
-
-    Vous pouvez obtenir une table avec des données corrompues si quelque chose interrompt l'opération d'écriture, par exemple, un arrêt anormal du serveur.
-
-## Différence {#differences}
-
-Le `TinyLog` le moteur est le plus simple de la famille et offre la fonctionnalité la plus pauvre et la plus faible efficacité. Le `TinyLog` le moteur ne prend pas en charge la lecture de données parallèles par plusieurs threads. Il lit les données plus lentement que les autres moteurs de la famille qui prennent en charge la lecture parallèle et utilise presque autant de descripteurs que `Log` moteur, car il stocke chaque colonne dans un fichier séparé. Utilisez-le dans des scénarios simples à faible charge.
-
-Le `Log` et `StripeLog` les moteurs prennent en charge la lecture de données parallèle. Lors de la lecture de données, ClickHouse utilise plusieurs threads. Chaque thread traite un bloc de données séparé. Le `Log` le moteur utilise un fichier distinct pour chaque colonne de la table. `StripeLog` stocke toutes les données dans un seul fichier. En conséquence, la `StripeLog` moteur utilise moins de descripteurs dans le système d'exploitation, mais le `Log` moteur fournit une plus grande efficacité lors de la lecture des données.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/log_family/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/log-family/log.md b/docs/fr/engines/table-engines/log-family/log.md
deleted file mode 100644
index f0d06af03d6a..000000000000
--- a/docs/fr/engines/table-engines/log-family/log.md
+++ /dev/null
@@ -1,16 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: Journal
----
-
-# Journal {#log}
-
-Moteur appartient à la famille de journal des moteurs. Voir les propriétés communes des moteurs de journal et leurs différences dans le [Famille De Moteurs En Rondins](index.md) article.
-
-Journal diffère de [TinyLog](tinylog.md) dans un petit fichier de “marks” réside avec les fichiers de colonne. Ces marques sont écrites sur chaque bloc de données et contiennent des décalages qui indiquent où commencer à lire le fichier afin d'ignorer le nombre de lignes spécifié. Cela permet de lire les données de table dans plusieurs threads.
-Pour l'accès aux données simultanées, les opérations de lecture peuvent être effectuées simultanément, tandis que les opérations d'écriture bloc lit et l'autre.
-Le moteur de journal ne prend pas en charge les index. De même, si l'écriture dans une table a échoué, la table est cassée et la lecture de celle-ci renvoie une erreur. Le moteur de journal est approprié pour les données temporaires, les tables en écriture unique, et à des fins de test ou de démonstration.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/log/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/log-family/stripelog.md b/docs/fr/engines/table-engines/log-family/stripelog.md
deleted file mode 100644
index 2c2919aec216..000000000000
--- a/docs/fr/engines/table-engines/log-family/stripelog.md
+++ /dev/null
@@ -1,95 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 32
-toc_title: StripeLog
----
-
-# Stripelog {#stripelog}
-
-Ce moteur appartient à la famille des moteurs en rondins. Voir les propriétés communes des moteurs de journal et leurs différences dans le [Famille De Moteurs En Rondins](index.md) article.
-
-Utilisez ce moteur dans des scénarios lorsque vous devez écrire de nombreuses tables avec une petite quantité de données (moins de 1 million de lignes).
-
-## Création d'une Table {#table_engines-stripelog-creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    column1_name [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    column2_name [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = StripeLog
-```
-
-Voir la description détaillée de la [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) requête.
-
-## L'écriture des Données {#table_engines-stripelog-writing-the-data}
-
-Le `StripeLog` moteur stocke toutes les colonnes dans un fichier. Pour chaque `INSERT` requête, ClickHouse ajoute le bloc de données à la fin d'un fichier de table, en écrivant des colonnes une par une.
-
-Pour chaque table ClickHouse écrit les fichiers:
-
--   `data.bin` — Data file.
--   `index.mrk` — File with marks. Marks contain offsets for each column of each data block inserted.
-
-Le `StripeLog` moteur ne prend pas en charge la `ALTER UPDATE` et `ALTER DELETE` opérations.
-
-## La lecture des Données {#table_engines-stripelog-reading-the-data}
-
-Le fichier avec des marques permet à ClickHouse de paralléliser la lecture des données. Cela signifie qu'une `SELECT` la requête renvoie des lignes dans un ordre imprévisible. L'utilisation de la `ORDER BY` clause pour trier les lignes.
-
-## Exemple D'utilisation {#table_engines-stripelog-example-of-use}
-
-Création d'une table:
-
-``` sql
-CREATE TABLE stripe_log_table
-(
-    timestamp DateTime,
-    message_type String,
-    message String
-)
-ENGINE = StripeLog
-```
-
-Insertion de données:
-
-``` sql
-INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The first regular message')
-INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The second regular message'),(now(),'WARNING','The first warning message')
-```
-
-Nous avons utilisé deux `INSERT` requêtes pour créer deux blocs de données `data.bin` fichier.
-
-ClickHouse utilise plusieurs threads lors de la sélection des données. Chaque thread lit un bloc de données séparé et renvoie les lignes résultantes indépendamment à la fin. En conséquence, l'ordre des blocs de lignes dans le résultat ne correspond pas à l'ordre des mêmes blocs dans l'entrée, dans la plupart des cas. Exemple:
-
-``` sql
-SELECT * FROM stripe_log_table
-```
-
-``` text
-┌───────────timestamp─┬─message_type─┬─message────────────────────┐
-│ 2019-01-18 14:27:32 │ REGULAR      │ The second regular message │
-│ 2019-01-18 14:34:53 │ WARNING      │ The first warning message  │
-└─────────────────────┴──────────────┴────────────────────────────┘
-┌───────────timestamp─┬─message_type─┬─message───────────────────┐
-│ 2019-01-18 14:23:43 │ REGULAR      │ The first regular message │
-└─────────────────────┴──────────────┴───────────────────────────┘
-```
-
-Trier les résultats (ordre croissant par défaut):
-
-``` sql
-SELECT * FROM stripe_log_table ORDER BY timestamp
-```
-
-``` text
-┌───────────timestamp─┬─message_type─┬─message────────────────────┐
-│ 2019-01-18 14:23:43 │ REGULAR      │ The first regular message  │
-│ 2019-01-18 14:27:32 │ REGULAR      │ The second regular message │
-│ 2019-01-18 14:34:53 │ WARNING      │ The first warning message  │
-└─────────────────────┴──────────────┴────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/stripelog/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/log-family/tinylog.md b/docs/fr/engines/table-engines/log-family/tinylog.md
deleted file mode 100644
index 275b621718cb..000000000000
--- a/docs/fr/engines/table-engines/log-family/tinylog.md
+++ /dev/null
@@ -1,16 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: TinyLog
----
-
-# TinyLog {#tinylog}
-
-Le moteur appartient à la famille de moteurs en rondins. Voir [Famille De Moteurs En Rondins](index.md) pour les propriétés communes des moteurs en rondins et leurs différences.
-
-Ce moteur de table est généralement utilisé avec la méthode write-once: écrivez des données une fois, puis lisez-les autant de fois que nécessaire. Par exemple, vous pouvez utiliser `TinyLog`- tapez des tables pour les données intermédiaires qui sont traitées en petits lots. Notez que le stockage des données dans un grand nombre de petites tables est inefficace.
-
-Les requêtes sont exécutées dans un flux unique. En d'autres termes, ce moteur est destiné à des tables relativement petites (jusqu'à environ 1 000 000 de lignes). Il est logique d'utiliser ce moteur de table si vous avez beaucoup de petites tables, car il est plus simple que le [Journal](log.md) moteur (moins de fichiers doivent être ouverts).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/tinylog/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/aggregatingmergetree.md b/docs/fr/engines/table-engines/mergetree-family/aggregatingmergetree.md
deleted file mode 100644
index e8e74960545a..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/aggregatingmergetree.md
+++ /dev/null
@@ -1,105 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: AggregatingMergeTree
----
-
-# Aggregatingmergetree {#aggregatingmergetree}
-
-Le moteur hérite de [MergeTree](mergetree.md#table_engines-mergetree), modifier la logique pour les parties de données Fusion. ClickHouse remplace toutes les lignes avec la même clé primaire (ou, plus précisément, avec la même [clé de tri](mergetree.md)) avec une seule ligne (dans un rayon d'une partie des données) qui stocke une combinaison d'états de fonctions d'agrégation.
-
-Vous pouvez utiliser `AggregatingMergeTree` tables pour l'agrégation incrémentielle des données, y compris pour les vues matérialisées agrégées.
-
-Le moteur traite toutes les colonnes avec les types suivants:
-
--   [AggregateFunction](../../../sql-reference/data-types/aggregatefunction.md)
--   [SimpleAggregateFunction](../../../sql-reference/data-types/simpleaggregatefunction.md)
-
-Il est approprié d'utiliser `AggregatingMergeTree` si elle réduit le nombre de lignes par commande.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = AggregatingMergeTree()
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[TTL expr]
-[SETTINGS name=value, ...]
-```
-
-Pour une description des paramètres de requête, voir [demande de description](../../../sql-reference/statements/create.md).
-
-**Les clauses de requête**
-
-Lors de la création d'un `AggregatingMergeTree` la table de la même [clause](mergetree.md) sont nécessaires, comme lors de la création d'un `MergeTree` table.
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets et, si possible, remplacez les anciens projets par la méthode décrite ci-dessus.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] AggregatingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity)
-```
-
-Tous les paramètres ont la même signification que dans `MergeTree`.
-</details>
-
-## Sélectionner et insérer {#select-and-insert}
-
-Pour insérer des données, utilisez [INSERT SELECT](../../../sql-reference/statements/insert-into.md) requête avec l'ensemble-l'État des fonctions.
-Lors de la sélection des données `AggregatingMergeTree` table, utilisez `GROUP BY` et les mêmes fonctions d'agrégat que lors de l'insertion de données, mais en utilisant `-Merge` suffixe.
-
-Dans les résultats de `SELECT` requête, les valeurs de `AggregateFunction` type ont une représentation binaire spécifique à l'implémentation pour tous les formats de sortie ClickHouse. Si les données de vidage dans, par exemple, `TabSeparated` format avec `SELECT` requête alors ce vidage peut être chargé en utilisant `INSERT` requête.
-
-## Exemple D'une vue matérialisée agrégée {#example-of-an-aggregated-materialized-view}
-
-`AggregatingMergeTree` vue matérialisée qui regarde le `test.visits` table:
-
-``` sql
-CREATE MATERIALIZED VIEW test.basic
-ENGINE = AggregatingMergeTree() PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate)
-AS SELECT
-    CounterID,
-    StartDate,
-    sumState(Sign)    AS Visits,
-    uniqState(UserID) AS Users
-FROM test.visits
-GROUP BY CounterID, StartDate;
-```
-
-Insertion de données dans la `test.visits` table.
-
-``` sql
-INSERT INTO test.visits ...
-```
-
-Les données sont insérées dans la table et la vue `test.basic` que va effectuer l'agrégation.
-
-Pour obtenir les données agrégées, nous devons exécuter une requête telle que `SELECT ... GROUP BY ...` à partir de la vue `test.basic`:
-
-``` sql
-SELECT
-    StartDate,
-    sumMerge(Visits) AS Visits,
-    uniqMerge(Users) AS Users
-FROM test.basic
-GROUP BY StartDate
-ORDER BY StartDate;
-```
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/aggregatingmergetree/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/collapsingmergetree.md b/docs/fr/engines/table-engines/mergetree-family/collapsingmergetree.md
deleted file mode 100644
index 80f61622539d..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/collapsingmergetree.md
+++ /dev/null
@@ -1,306 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: CollapsingMergeTree
----
-
-# CollapsingMergeTree {#table_engine-collapsingmergetree}
-
-Le moteur hérite de [MergeTree](mergetree.md) et ajoute la logique de l'effondrement des lignes de données de pièces algorithme de fusion.
-
-`CollapsingMergeTree` supprime de manière asynchrone (réduit) les paires de lignes si tous les champs d'une clé de tri (`ORDER BY`) sont équivalents à l'exception du champ particulier `Sign` ce qui peut avoir `1` et `-1` valeur. Les lignes sans paire sont conservées. Pour plus de détails, voir le [Effondrer](#table_engine-collapsingmergetree-collapsing) la section du document.
-
-Le moteur peut réduire considérablement le volume de stockage et augmenter l'efficacité de `SELECT` requête en conséquence.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = CollapsingMergeTree(sign)
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Pour une description des paramètres de requête, voir [description de la requête](../../../sql-reference/statements/create.md).
-
-**Paramètres CollapsingMergeTree**
-
--   `sign` — Name of the column with the type of row: `1` est un “state” rangée, `-1` est un “cancel” rangée.
-
-    Column data type — `Int8`.
-
-**Les clauses de requête**
-
-Lors de la création d'un `CollapsingMergeTree` de table, de la même [les clauses de requête](mergetree.md#table_engine-mergetree-creating-a-table) sont nécessaires, comme lors de la création d'un `MergeTree` table.
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets et, si possible, remplacez les anciens projets par la méthode décrite ci-dessus.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] CollapsingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, sign)
-```
-
-Tous les paramètres excepté `sign` ont la même signification que dans `MergeTree`.
-
--   `sign` — Name of the column with the type of row: `1` — “state” rangée, `-1` — “cancel” rangée.
-
-    Column Data Type — `Int8`.
-
-</details>
-
-## Effondrer {#table_engine-collapsingmergetree-collapsing}
-
-### Données {#data}
-
-Considérez la situation où vous devez enregistrer des données en constante évolution pour un objet. Il semble logique d'avoir une ligne pour un objet et de la mettre à jour à tout changement, mais l'opération de mise à jour est coûteuse et lente pour le SGBD car elle nécessite une réécriture des données dans le stockage. Si vous avez besoin d'écrire des données rapidement, la mise à jour n'est pas acceptable, mais vous pouvez écrire les modifications d'un objet de manière séquentielle comme suit.
-
-Utilisez la colonne particulière `Sign`. Si `Sign = 1` cela signifie que la ligne est un état d'un objet, appelons-la “state” rangée. Si `Sign = -1` il signifie l'annulation de l'état d'un objet avec les mêmes attributs, nous allons l'appeler “cancel” rangée.
-
-Par exemple, nous voulons calculer combien de pages les utilisateurs ont vérifié sur un site et combien de temps ils étaient là. À un certain moment nous écrire la ligne suivante avec l'état de l'activité de l'utilisateur:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-À un moment donné, nous enregistrons le changement d'activité de l'utilisateur et l'écrivons avec les deux lignes suivantes.
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-La première ligne annule le précédent état de l'objet (utilisateur). Il doit copier les champs de clé de tri de l'état annulé sauf `Sign`.
-
-La deuxième ligne contient l'état actuel.
-
-Comme nous avons besoin seulement le dernier état de l'activité de l'utilisateur, les lignes
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-│ 4324182021466249494 │         5 │      146 │   -1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-peut être supprimé en réduisant l'état invalide (ancien) d'un objet. `CollapsingMergeTree` fait cela lors de la fusion des parties de données.
-
-Pourquoi nous avons besoin de 2 lignes pour chaque changement lu dans le [Algorithme](#table_engine-collapsingmergetree-collapsing-algorithm) paragraphe.
-
-**Propriétés particulières d'une telle approche**
-
-1.  Le programme qui écrit les données doit se souvenir de l'état d'un objet pour pouvoir l'annuler. “Cancel” string doit contenir des copies des champs de clé de tri du “state” chaîne et le contraire `Sign`. Il augmente la taille initiale de stockage, mais permet d'écrire les données rapidement.
-2.  Les tableaux de plus en plus longs dans les colonnes réduisent l'efficacité du moteur en raison de la charge pour l'écriture. Plus les données sont simples, plus l'efficacité est élevée.
-3.  Le `SELECT` les résultats dépendent fortement de la cohérence de l'historique des modifications d'objet. Être précis lors de la préparation des données pour l'insertion. Vous pouvez obtenir des résultats imprévisibles dans des données incohérentes, par exemple des valeurs négatives pour des mesures non négatives telles que la profondeur de session.
-
-### Algorithme {#table_engine-collapsingmergetree-collapsing-algorithm}
-
-Lorsque ClickHouse fusionne des parties de données, chaque groupe de lignes consécutives avec la même clé de tri (`ORDER BY`) est réduit à pas plus de deux rangées, une avec `Sign = 1` (“state” ligne) et l'autre avec `Sign = -1` (“cancel” rangée). En d'autres termes, les entrées de l'effondrement.
-
-Pour chaque partie de données résultante clickhouse enregistre:
-
-1.  Première “cancel” et la dernière “state” lignes, si le nombre de “state” et “cancel” lignes correspond et la dernière ligne est un “state” rangée.
-2.  La dernière “state” ligne, si il y a plus de “state” les lignes de “cancel” rangée.
-3.  Première “cancel” ligne, si il y a plus de “cancel” les lignes de “state” rangée.
-4.  Aucune des lignes, dans tous les autres cas.
-
-Aussi quand il y a au moins 2 plus “state” les lignes de “cancel” les lignes, ou au moins 2 de plus “cancel” rangs puis “state” la fusion continue, mais ClickHouse traite cette situation comme une erreur logique et l'enregistre dans le journal du serveur. Cette erreur peut se produire si les mêmes données ont été insérées plus d'une fois.
-
-Ainsi, l'effondrement ne devrait pas changer les résultats du calcul des statistiques.
-Les changements se sont progressivement effondrés de sorte qu'à la fin seul le dernier état de presque tous les objets à gauche.
-
-Le `Sign` est nécessaire car l'algorithme de fusion ne garantit pas que toutes les lignes avec la même clé de tri seront dans la même partie de données résultante et même sur le même serveur physique. Processus de ClickHouse `SELECT` les requêtes avec plusieurs threads, et il ne peut pas prédire l'ordre des lignes dans le résultat. L'agrégation est nécessaire s'il y a un besoin d'obtenir complètement “collapsed” les données de `CollapsingMergeTree` table.
-
-Pour finaliser la réduction, écrivez une requête avec `GROUP BY` fonctions de clause et d'agrégation qui tiennent compte du signe. Par exemple, pour calculer la quantité, l'utilisation `sum(Sign)` plutôt `count()`. Pour calculer la somme de quelque chose, utilisez `sum(Sign * x)` plutôt `sum(x)` et ainsi de suite , et également ajouter `HAVING sum(Sign) > 0`.
-
-Aggregate `count`, `sum` et `avg` pourrait être calculée de cette manière. Aggregate `uniq` peut être calculé si un objet a au moins un état non réduit. Aggregate `min` et `max` impossible de calculer parce que `CollapsingMergeTree` n'enregistre pas l'historique des valeurs des États réduits.
-
-Si vous avez besoin d'extraire des données sans agrégation (par exemple, pour vérifier si des lignes sont présentes dont les valeurs les plus récentes correspondent à certaines conditions), vous pouvez utiliser `FINAL` le modificateur du `FROM` clause. Cette approche est nettement moins efficace.
-
-## Exemple D'utilisation {#example-of-use}
-
-Les données de l'exemple:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-Création de la table:
-
-``` sql
-CREATE TABLE UAct
-(
-    UserID UInt64,
-    PageViews UInt8,
-    Duration UInt8,
-    Sign Int8
-)
-ENGINE = CollapsingMergeTree(Sign)
-ORDER BY UserID
-```
-
-Insertion des données:
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1)
-```
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1),(4324182021466249494, 6, 185, 1)
-```
-
-Nous utilisons deux `INSERT` requêtes pour créer deux parties de données différentes. Si nous insérons les données avec une requête, ClickHouse crée une partie de données et n'effectuera aucune fusion.
-
-L'obtention de données:
-
-``` sql
-SELECT * FROM UAct
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-Que voyons-nous et où s'effondre?
-
-Avec deux `INSERT` requêtes, nous avons créé 2 parties de données. Le `SELECT` la requête a été effectuée dans 2 threads, et nous avons obtenu un ordre aléatoire de lignes. L'effondrement n'a pas eu lieu car il n'y avait pas encore de fusion des parties de données. ClickHouse fusionne une partie des données dans un moment inconnu que nous ne pouvons pas prédire.
-
-Nous avons donc besoin d'agrégation:
-
-``` sql
-SELECT
-    UserID,
-    sum(PageViews * Sign) AS PageViews,
-    sum(Duration * Sign) AS Duration
-FROM UAct
-GROUP BY UserID
-HAVING sum(Sign) > 0
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┐
-│ 4324182021466249494 │         6 │      185 │
-└─────────────────────┴───────────┴──────────┘
-```
-
-Si nous n'avons pas besoin d'agrégation et de vouloir forcer l'effondrement, nous pouvons utiliser `FINAL` le modificateur `FROM` clause.
-
-``` sql
-SELECT * FROM UAct FINAL
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-Cette façon de sélectionner les données est très inefficace. Ne l'utilisez pas pour les grandes tables.
-
-## Exemple D'une autre approche {#example-of-another-approach}
-
-Les données de l'exemple:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │    1 │
-│ 4324182021466249494 │        -5 │     -146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-L'idée est que les fusions ne prennent en compte que les champs clés. Et dans le “Cancel” ligne nous pouvons spécifier des valeurs négatives qui égalisent la version précédente de la ligne lors de la sommation sans utiliser la colonne de signe. Pour cette approche, il est nécessaire de changer le type de données `PageViews`,`Duration` pour stocker les valeurs négatives de UInt8 - \> Int16.
-
-``` sql
-CREATE TABLE UAct
-(
-    UserID UInt64,
-    PageViews Int16,
-    Duration Int16,
-    Sign Int8
-)
-ENGINE = CollapsingMergeTree(Sign)
-ORDER BY UserID
-```
-
-Nous allons tester l'approche:
-
-``` sql
-insert into UAct values(4324182021466249494,  5,  146,  1);
-insert into UAct values(4324182021466249494, -5, -146, -1);
-insert into UAct values(4324182021466249494,  6,  185,  1);
-
-select * from UAct final; // avoid using final in production (just for a test or small tables)
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-``` sql
-SELECT
-    UserID,
-    sum(PageViews) AS PageViews,
-    sum(Duration) AS Duration
-FROM UAct
-GROUP BY UserID
-```text
-┌──────────────UserID─┬─PageViews─┬─Duration─┐
-│ 4324182021466249494 │         6 │      185 │
-└─────────────────────┴───────────┴──────────┘
-```
-
-``` sqk
-select count() FROM UAct
-```
-
-``` text
-┌─count()─┐
-│       3 │
-└─────────┘
-```
-
-``` sql
-optimize table UAct final;
-
-select * FROM UAct
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/collapsingmergetree/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/custom-partitioning-key.md b/docs/fr/engines/table-engines/mergetree-family/custom-partitioning-key.md
deleted file mode 100644
index 2fa80236adcd..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/custom-partitioning-key.md
+++ /dev/null
@@ -1,127 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 32
-toc_title: "Cl\xE9 De Partitionnement Personnalis\xE9e"
----
-
-# Clé De Partitionnement Personnalisée {#custom-partitioning-key}
-
-Le partitionnement est disponible pour [MergeTree](mergetree.md) table de famille (y compris les [répliqué](replication.md) table). [Les vues matérialisées](../special/materializedview.md#materializedview) basé sur les tables MergeTree prennent également en charge le partitionnement.
-
-Une partition est une combinaison logique d'enregistrements dans une table selon un critère spécifié. Vous pouvez définir une partition par un critère arbitraire, comme, par mois, par jour, ou par type d'événement. Chaque partition est stockée séparément pour simplifier les manipulations de ces données. Lors de l'accès aux données, ClickHouse utilise le plus petit sous-ensemble de partitions possible.
-
-La partition est spécifiée dans le `PARTITION BY expr` clause lors de [création d'une table](mergetree.md#table_engine-mergetree-creating-a-table). La clé de partition peut être n'importe quelle expression des colonnes de la table. Par exemple, pour spécifier le partitionnement par mois, utilisez l'expression `toYYYYMM(date_column)`:
-
-``` sql
-CREATE TABLE visits
-(
-    VisitDate Date,
-    Hour UInt8,
-    ClientID UUID
-)
-ENGINE = MergeTree()
-PARTITION BY toYYYYMM(VisitDate)
-ORDER BY Hour;
-```
-
-La clé de partition peut également être un tuple d'expressions (similaire à la [clé primaire](mergetree.md#primary-keys-and-indexes-in-queries)). Exemple:
-
-``` sql
-ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)
-PARTITION BY (toMonday(StartDate), EventType)
-ORDER BY (CounterID, StartDate, intHash32(UserID));
-```
-
-Dans cet exemple, nous définissons le partitionnement par les types d'événements qui se sont produits au cours de la semaine en cours.
-
-Lors de l'insertion de nouvelles données dans une table, ces données sont stockées en tant que partie séparée (bloc) triée par la clé primaire. Dans 10-15 minutes après l'insertion, les parties de la même partition sont fusionnées dans la partie entière.
-
-!!! info "Info"
-    Une fusion ne fonctionne que pour les parties de données qui ont la même valeur pour l'expression de partitionnement. Cela signifie **vous ne devriez pas faire des partitions trop granulaires** (plus d'un millier de partitions). Sinon, l' `SELECT` la requête fonctionne mal en raison d'un nombre déraisonnablement élevé de fichiers dans le système de fichiers et des descripteurs de fichiers ouverts.
-
-L'utilisation de la [système.partie](../../../operations/system-tables.md#system_tables-parts) table pour afficher les parties et les partitions de la table. Par exemple, supposons que nous avons une `visits` table avec partitionnement par mois. Nous allons effectuer le `SELECT` la requête pour l' `system.parts` table:
-
-``` sql
-SELECT
-    partition,
-    name,
-    active
-FROM system.parts
-WHERE table = 'visits'
-```
-
-``` text
-┌─partition─┬─name───────────┬─active─┐
-│ 201901    │ 201901_1_3_1   │      0 │
-│ 201901    │ 201901_1_9_2   │      1 │
-│ 201901    │ 201901_8_8_0   │      0 │
-│ 201901    │ 201901_9_9_0   │      0 │
-│ 201902    │ 201902_4_6_1   │      1 │
-│ 201902    │ 201902_10_10_0 │      1 │
-│ 201902    │ 201902_11_11_0 │      1 │
-└───────────┴────────────────┴────────┘
-```
-
-Le `partition` colonne contient les noms des partitions. Il y a deux partitions dans cet exemple: `201901` et `201902`. Vous pouvez utiliser cette valeur de colonne pour spécifier le nom de [ALTER … PARTITION](#alter_manipulations-with-partitions) requête.
-
-Le `name` colonne contient les noms des parties de données de partition. Vous pouvez utiliser cette colonne pour spécifier le nom de la partie dans la [ALTER ATTACH PART](#alter_attach-partition) requête.
-
-Décomposons le nom de la première partie: `201901_1_3_1`:
-
--   `201901` est le nom de la partition.
--   `1` est le nombre minimum du bloc de données.
--   `3` est le nombre maximal de blocs de données.
--   `1` est le niveau de bloc (la profondeur de l'arbre de fusion à partir duquel il est formé).
-
-!!! info "Info"
-    Les parties des tables de type ancien ont le nom: `20190117_20190123_2_2_0` (date minimale - date maximale - numéro de bloc minimum - numéro de bloc maximum-niveau).
-
-Le `active` colonne indique le statut de la partie. `1` est active; `0` est inactif. Les parties inactives sont, par exemple, des parties source restant après la fusion à une partie plus grande. Les parties de données corrompues sont également indiquées comme inactives.
-
-Comme vous pouvez le voir dans l'exemple, il y a plusieurs parties séparées de la même partition (par exemple, `201901_1_3_1` et `201901_1_9_2`). Cela signifie que ces parties ne sont pas encore fusionnées. Clickhouse fusionne les parties insérées des données périodiquement, environ 15 minutes après l'insertion. En outre, vous pouvez effectuer une fusion non planifiée en utilisant [OPTIMIZE](../../../sql-reference/statements/misc.md#misc_operations-optimize) requête. Exemple:
-
-``` sql
-OPTIMIZE TABLE visits PARTITION 201902;
-```
-
-``` text
-┌─partition─┬─name───────────┬─active─┐
-│ 201901    │ 201901_1_3_1   │      0 │
-│ 201901    │ 201901_1_9_2   │      1 │
-│ 201901    │ 201901_8_8_0   │      0 │
-│ 201901    │ 201901_9_9_0   │      0 │
-│ 201902    │ 201902_4_6_1   │      0 │
-│ 201902    │ 201902_4_11_2  │      1 │
-│ 201902    │ 201902_10_10_0 │      0 │
-│ 201902    │ 201902_11_11_0 │      0 │
-└───────────┴────────────────┴────────┘
-```
-
-Les parties inactives seront supprimées environ 10 minutes après la fusion.
-
-Une autre façon d'afficher un ensemble de pièces et de partitions est d'aller dans le répertoire de la table: `/var/lib/clickhouse/data/<database>/<table>/`. Exemple:
-
-``` bash
-/var/lib/clickhouse/data/default/visits$ ls -l
-total 40
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  1 16:48 201901_1_3_1
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:17 201901_1_9_2
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 15:52 201901_8_8_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 15:52 201901_9_9_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:17 201902_10_10_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:17 201902_11_11_0
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 16:19 201902_4_11_2
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  5 12:09 201902_4_6_1
-drwxr-xr-x 2 clickhouse clickhouse 4096 Feb  1 16:48 detached
-```
-
-Dossier ‘201901_1_1_0’, ‘201901_1_7_1’ et ainsi de suite sont les répertoires des parties. Chaque partie se rapporte à une partition correspondante et contient des données juste pour un certain mois (la table dans cet exemple a partitionnement par mois).
-
-Le `detached` le répertoire contient des parties qui ont été détachées de la table à l'aide [DETACH](../../../sql-reference/statements/alter.md#alter_detach-partition) requête. Les parties corrompues sont également déplacées dans ce répertoire, au lieu d'être supprimées. Le serveur n'utilise pas les pièces de la `detached` directory. You can add, delete, or modify the data in this directory at any time – the server will not know about this until you run the [ATTACH](../../../sql-reference/statements/alter.md#alter_attach-partition) requête.
-
-Notez que sur le serveur d'exploitation, vous ne pouvez pas modifier manuellement l'ensemble de pièces ou leurs données sur le système de fichiers, car le serveur ne le saura pas. Pour les tables non répliquées, vous pouvez le faire lorsque le serveur est arrêté, mais ce n'est pas recommandé. Pour les tables répliquées, l'ensemble de pièces ne peut en aucun cas être modifié.
-
-ClickHouse vous permet d'effectuer des opérations avec les partitions: les supprimer, copier d'une table à une autre, ou créer une sauvegarde. Voir la liste de toutes les opérations de la section [Manipulations avec des Partitions et des pièces](../../../sql-reference/statements/alter.md#alter_manipulations-with-partitions).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/custom_partitioning_key/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/graphitemergetree.md b/docs/fr/engines/table-engines/mergetree-family/graphitemergetree.md
deleted file mode 100644
index 03fae8ea261f..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/graphitemergetree.md
+++ /dev/null
@@ -1,174 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: GraphiteMergeTree
----
-
-# GraphiteMergeTree {#graphitemergetree}
-
-Ce moteur est conçu pour l'amincissement et l'agrégation / moyenne (cumul) [Graphite](http://graphite.readthedocs.io/en/latest/index.html) données. Il peut être utile aux développeurs qui veulent utiliser ClickHouse comme un magasin de données pour Graphite.
-
-Vous pouvez utiliser N'importe quel moteur de table ClickHouse pour stocker les données Graphite si vous n'avez pas besoin de cumul, mais si vous avez besoin d'un cumul, utilisez `GraphiteMergeTree`. Le moteur réduit le volume de stockage et augmente l'efficacité des requêtes de Graphite.
-
-Le moteur hérite des propriétés de [MergeTree](mergetree.md).
-
-## Création d'une Table {#creating-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    Path String,
-    Time DateTime,
-    Value <Numeric_type>,
-    Version <Numeric_type>
-    ...
-) ENGINE = GraphiteMergeTree(config_section)
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Voir une description détaillée de la [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) requête.
-
-Un tableau pour les données de Graphite devrait avoir les colonnes suivantes pour les données suivantes:
-
--   Nom métrique (Capteur De Graphite). Type de données: `String`.
-
--   Temps de mesure de la métrique. Type de données: `DateTime`.
-
--   La valeur de la métrique. Type de données: tout numérique.
-
--   La Version de la métrique. Type de données: tout numérique.
-
-    ClickHouse enregistre les lignes avec la version la plus élevée ou la dernière écrite si les versions sont les mêmes. Les autres lignes sont supprimées lors de la fusion des parties de données.
-
-Les noms de ces colonnes doivent être définis dans la configuration de cumul.
-
-**GraphiteMergeTree paramètres**
-
--   `config_section` — Name of the section in the configuration file, where are the rules of rollup set.
-
-**Les clauses de requête**
-
-Lors de la création d'un `GraphiteMergeTree` de table, de la même [clause](mergetree.md#table_engine-mergetree-creating-a-table) sont nécessaires, comme lors de la création d'un `MergeTree` table.
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets et, si possible, remplacez les anciens projets par la méthode décrite ci-dessus.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    EventDate Date,
-    Path String,
-    Time DateTime,
-    Value <Numeric_type>,
-    Version <Numeric_type>
-    ...
-) ENGINE [=] GraphiteMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, config_section)
-```
-
-Tous les paramètres excepté `config_section` ont la même signification que dans `MergeTree`.
-
--   `config_section` — Name of the section in the configuration file, where are the rules of rollup set.
-
-</details>
-
-## Configuration De Cumul {#rollup-configuration}
-
-Les paramètres de cumul sont définis par [graphite_rollup](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-graphite) paramètre dans la configuration du serveur. Le nom du paramètre pourrait être tout. Vous pouvez créer plusieurs configurations et les utiliser pour différentes tables.
-
-Structure de configuration de cumul:
-
-      required-columns
-      patterns
-
-### Les Colonnes Requises {#required-columns}
-
--   `path_column_name` — The name of the column storing the metric name (Graphite sensor). Default value: `Path`.
--   `time_column_name` — The name of the column storing the time of measuring the metric. Default value: `Time`.
--   `value_column_name` — The name of the column storing the value of the metric at the time set in `time_column_name`. Valeur par défaut: `Value`.
--   `version_column_name` — The name of the column storing the version of the metric. Default value: `Timestamp`.
-
-### Modèle {#patterns}
-
-La Structure de la `patterns` section:
-
-``` text
-pattern
-    regexp
-    function
-pattern
-    regexp
-    age + precision
-    ...
-pattern
-    regexp
-    function
-    age + precision
-    ...
-pattern
-    ...
-default
-    function
-    age + precision
-    ...
-```
-
-!!! warning "Attention"
-    Les motifs doivent être strictement commandés:
-
-      1. Patterns without `function` or `retention`.
-      1. Patterns with both `function` and `retention`.
-      1. Pattern `default`.
-
-Lors du traitement d'une ligne, ClickHouse vérifie les règles `pattern` section. Chacun `pattern` (comprendre `default`) les articles peuvent contenir des `function` paramètre d'agrégation, `retention` les paramètres ou les deux à la fois. Si le nom de la métrique correspond `regexp` les règles de la `pattern` section (ou sections) sont appliquées; sinon, les règles de la `default` section sont utilisés.
-
-Champs pour `pattern` et `default` section:
-
--   `regexp`– A pattern for the metric name.
--   `age` – The minimum age of the data in seconds.
--   `precision`– How precisely to define the age of the data in seconds. Should be a divisor for 86400 (seconds in a day).
--   `function` – The name of the aggregating function to apply to data whose age falls within the range `[age, age + precision]`.
-
-### Exemple De Configuration {#configuration-example}
-
-``` xml
-<graphite_rollup>
-    <version_column_name>Version</version_column_name>
-    <pattern>
-        <regexp>click_cost</regexp>
-        <function>any</function>
-        <retention>
-            <age>0</age>
-            <precision>5</precision>
-        </retention>
-        <retention>
-            <age>86400</age>
-            <precision>60</precision>
-        </retention>
-    </pattern>
-    <default>
-        <function>max</function>
-        <retention>
-            <age>0</age>
-            <precision>60</precision>
-        </retention>
-        <retention>
-            <age>3600</age>
-            <precision>300</precision>
-        </retention>
-        <retention>
-            <age>86400</age>
-            <precision>3600</precision>
-        </retention>
-    </default>
-</graphite_rollup>
-```
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/graphitemergetree/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/index.md b/docs/fr/engines/table-engines/mergetree-family/index.md
deleted file mode 100644
index e2de11a7591b..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Famille MergeTree
-toc_priority: 28
----
-
-
diff --git a/docs/fr/engines/table-engines/mergetree-family/mergetree.md b/docs/fr/engines/table-engines/mergetree-family/mergetree.md
deleted file mode 100644
index 475682bfc93a..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/mergetree.md
+++ /dev/null
@@ -1,654 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 30
-toc_title: MergeTree
----
-
-# MergeTree {#table_engines-mergetree}
-
-Le `MergeTree` moteur et autres moteurs de cette famille (`*MergeTree`) sont les moteurs de table ClickHouse les plus robustes.
-
-Les moteurs de la `MergeTree` famille sont conçus pour l'insertion d'une très grande quantité de données dans une table. Les données sont rapidement écrites dans la table partie par partie, puis des règles sont appliquées pour fusionner les parties en arrière-plan. Cette méthode est beaucoup plus efficace que de réécrire continuellement les données dans le stockage pendant l'insertion.
-
-Principales caractéristiques:
-
--   Stocke les données triées par clé primaire.
-
-    Cela vous permet de créer un petit index clairsemé qui aide à trouver les données plus rapidement.
-
--   Les Partitions peuvent être utilisées si [clé de partitionnement](custom-partitioning-key.md) est spécifié.
-
-    ClickHouse prend en charge certaines opérations avec des partitions plus efficaces que les opérations générales sur les mêmes données avec le même résultat. ClickHouse Coupe également automatiquement les données de partition où la clé de partitionnement est spécifiée dans la requête. Cela améliore également les performances de la requête.
-
--   Prise en charge de la réplication des données.
-
-    La famille de `ReplicatedMergeTree` tables fournit la réplication des données. Pour plus d'informations, voir [Réplication des données](replication.md).
-
--   Appui d'échantillonnage de données.
-
-    Si nécessaire, vous pouvez définir la méthode d'échantillonnage des données dans le tableau.
-
-!!! info "Info"
-    Le [Fusionner](../special/merge.md#merge) le moteur n'appartient pas à la `*MergeTree` famille.
-
-## Création d'une Table {#table_engine-mergetree-creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2],
-    ...
-    INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1,
-    INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2
-) ENGINE = MergeTree()
-[PARTITION BY expr]
-[ORDER BY expr]
-[PRIMARY KEY expr]
-[SAMPLE BY expr]
-[TTL expr [DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'], ...]
-[SETTINGS name=value, ...]
-```
-
-Pour une description des paramètres, voir [Créer une description de requête](../../../sql-reference/statements/create.md).
-
-!!! note "Note"
-    `INDEX` est une fonctionnalité expérimentale, voir [Index De Saut De Données](#table_engine-mergetree-data_skipping-indexes).
-
-### Les Clauses De Requête {#mergetree-query-clauses}
-
--   `ENGINE` — Name and parameters of the engine. `ENGINE = MergeTree()`. Le `MergeTree` le moteur n'a pas de paramètres.
-
--   `PARTITION BY` — The [clé de partitionnement](custom-partitioning-key.md).
-
-    Pour le partitionnement par mois, utilisez les `toYYYYMM(date_column)` l'expression, où `date_column` est une colonne avec une date du type [Date](../../../sql-reference/data-types/date.md). Les noms de partition ici ont le `"YYYYMM"` format.
-
--   `ORDER BY` — The sorting key.
-
-    Un tuple de colonnes ou d'expressions arbitraires. Exemple: `ORDER BY (CounterID, EventDate)`.
-
--   `PRIMARY KEY` — The primary key if it [diffère de la clé de tri](#choosing-a-primary-key-that-differs-from-the-sorting-key).
-
-    Par défaut, la clé primaire est la même que la clé de tri (qui est spécifiée par `ORDER BY` clause). Ainsi dans la plupart des cas il n'est pas nécessaire de spécifier un `PRIMARY KEY` clause.
-
--   `SAMPLE BY` — An expression for sampling.
-
-    Si un échantillonnage expression est utilisée, la clé primaire doit contenir. Exemple: `SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID))`.
-
--   `TTL` — A list of rules specifying storage duration of rows and defining logic of automatic parts movement [entre disques et volumes](#table_engine-mergetree-multiple-volumes).
-
-    L'Expression doit en avoir une `Date` ou `DateTime` colonne comme un résultat. Exemple:
-    `TTL date + INTERVAL 1 DAY`
-
-    Le Type de la règle `DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'` spécifie une action à effectuer avec la partie si l'expression est satisfaite (atteint l'heure actuelle): suppression des Lignes expirées, déplacement d'une partie (si l'expression est satisfaite pour toutes les lignes d'une partie) sur le disque spécifié (`TO DISK 'xxx'`) ou de volume (`TO VOLUME 'xxx'`). Le type par défaut de la règle est suppression (`DELETE`). Liste de règles multiples peut spécifié, mais il ne devrait pas y avoir plus d'un `DELETE` règle.
-
-    Pour plus de détails, voir [TTL pour les colonnes et les tableaux](#table_engine-mergetree-ttl)
-
--   `SETTINGS` — Additional parameters that control the behavior of the `MergeTree`:
-
-    -   `index_granularity` — Maximum number of data rows between the marks of an index. Default value: 8192. See [Le Stockage De Données](#mergetree-data-storage).
-    -   `index_granularity_bytes` — Maximum size of data granules in bytes. Default value: 10Mb. To restrict the granule size only by number of rows, set to 0 (not recommended). See [Le Stockage De Données](#mergetree-data-storage).
-    -   `enable_mixed_granularity_parts` — Enables or disables transitioning to control the granule size with the `index_granularity_bytes` paramètre. Avant la version 19.11, il n'y avait que le `index_granularity` réglage pour restreindre la taille des granules. Le `index_granularity_bytes` le paramètre améliore les performances de ClickHouse lors de la sélection de données à partir de tables avec de grandes lignes (des dizaines et des centaines de mégaoctets). Si vous avez des tables avec de grandes lignes, vous pouvez activer ce paramètre pour les tables d'améliorer l'efficacité de `SELECT` requête.
-    -   `use_minimalistic_part_header_in_zookeeper` — Storage method of the data parts headers in ZooKeeper. If `use_minimalistic_part_header_in_zookeeper=1`, puis Zookeeper stocke moins de données. Pour plus d'informations, voir le [Description du réglage](../../../operations/server-configuration-parameters/settings.md#server-settings-use_minimalistic_part_header_in_zookeeper) dans “Server configuration parameters”.
-    -   `min_merge_bytes_to_use_direct_io` — The minimum data volume for merge operation that is required for using direct I/O access to the storage disk. When merging data parts, ClickHouse calculates the total storage volume of all the data to be merged. If the volume exceeds `min_merge_bytes_to_use_direct_io` octets, ClickHouse lit et écrit les données sur le disque de stockage en utilisant l'interface d'E/S directe (`O_DIRECT` option). Si `min_merge_bytes_to_use_direct_io = 0`, puis les e/s directes sont désactivées. Valeur par défaut: `10 * 1024 * 1024 * 1024` octet.
-        <a name="mergetree_setting-merge_with_ttl_timeout"></a>
-    -   `merge_with_ttl_timeout` — Minimum delay in seconds before repeating a merge with TTL. Default value: 86400 (1 day).
-    -   `write_final_mark` — Enables or disables writing the final index mark at the end of data part (after the last byte). Default value: 1. Don't turn it off.
-    -   `merge_max_block_size` — Maximum number of rows in block for merge operations. Default value: 8192.
-    -   `storage_policy` — Storage policy. See [Utilisation de plusieurs périphériques de bloc pour le stockage de données](#table_engine-mergetree-multiple-volumes).
-
-**Exemple de réglage des Sections**
-
-``` sql
-ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity=8192
-```
-
-Dans l'exemple, nous définissons le partitionnement par mois.
-
-Nous définissons également une expression pour l'échantillonnage en tant que hachage par l'ID utilisateur. Cela vous permet de pseudorandomiser les données dans la table pour chaque `CounterID` et `EventDate`. Si vous définissez un [SAMPLE](../../../sql-reference/statements/select/sample.md#select-sample-clause) clause lors de la sélection des données, ClickHouse retournera un échantillon de données uniformément pseudo-aléatoire pour un sous-ensemble d'utilisateurs.
-
-Le `index_granularity` paramètre peut être omis, car 8192 est la valeur par défaut.
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets. Si possible, optez anciens projets à la méthode décrite ci-dessus.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] MergeTree(date-column [, sampling_expression], (primary, key), index_granularity)
-```
-
-**Paramètres MergeTree ()**
-
--   `date-column` — The name of a column of the [Date](../../../sql-reference/data-types/date.md) type. ClickHouse crée automatiquement des partitions par mois en fonction de cette colonne. Les noms de partition sont dans le `"YYYYMM"` format.
--   `sampling_expression` — An expression for sampling.
--   `(primary, key)` — Primary key. Type: [Tuple()](../../../sql-reference/data-types/tuple.md)
--   `index_granularity` — The granularity of an index. The number of data rows between the “marks” d'un index. La valeur 8192 est appropriée pour la plupart des tâches.
-
-**Exemple**
-
-``` sql
-MergeTree(EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID)), 8192)
-```
-
-Le `MergeTree` le moteur est configuré de la même manière que dans l'exemple ci-dessus pour la méthode de configuration du moteur principal.
-</details>
-
-## Le Stockage De Données {#mergetree-data-storage}
-
-Une table se compose de parties de données triées par clé primaire.
-
-Lorsque des données sont insérées dans une table, des parties de données distinctes sont créées et chacune d'elles est lexicographiquement triée par clé primaire. Par exemple, si la clé primaire est `(CounterID, Date)`, les données de la pièce sont triées par `CounterID` et au sein de chaque `CounterID` il est commandé par `Date`.
-
-Les données appartenant à différentes partitions sont séparés en différentes parties. En arrière-plan, ClickHouse fusionne des parties de données pour un stockage plus efficace. Les parties appartenant à des partitions différentes ne sont pas fusionnées. La fusion mécanisme ne garantit pas que toutes les lignes avec la même clé primaire sera dans la même partie des données.
-
-Chaque partie de données est logiquement divisée en granules. Un granule est le plus petit ensemble de données indivisible que ClickHouse lit lors de la sélection des données. ClickHouse ne divise pas les lignes ou les valeurs, de sorte que chaque granule contient toujours un nombre entier de lignes. La première rangée de granules est marqué avec la valeur de la clé primaire de la ligne. Pour chaque partie de données, ClickHouse crée un fichier d'index qui stocke les marques. Pour chaque colonne, que ce soit dans la clé primaire ou non, ClickHouse stocke également les mêmes marques. Ces marques vous permettent de trouver des données directement dans les fichiers de colonnes.
-
-La taille de granule est limitée par `index_granularity` et `index_granularity_bytes` paramètres du moteur de table. Le nombre de lignes dans un granule jette dans la `[1, index_granularity]` gamme, en fonction de la taille des lignes. La taille des granulés peut dépasser `index_granularity_bytes` si la taille d'une seule ligne est supérieure à la valeur du paramètre. Dans ce cas, la taille du granule est égale à la taille de la ligne.
-
-## Clés primaires et Index dans les requêtes {#primary-keys-and-indexes-in-queries}
-
-Prendre la `(CounterID, Date)` clé primaire comme un exemple. Dans ce cas, le tri et l'index peuvent être illustrés comme suit:
-
-      Whole data:     [---------------------------------------------]
-      CounterID:      [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]
-      Date:           [1111111222222233331233211111222222333211111112122222223111112223311122333]
-      Marks:           |      |      |      |      |      |      |      |      |      |      |
-                      a,1    a,2    a,3    b,3    e,2    e,3    g,1    h,2    i,1    i,3    l,3
-      Marks numbers:   0      1      2      3      4      5      6      7      8      9      10
-
-Si la requête de données spécifie:
-
--   `CounterID in ('a', 'h')` le serveur lit les données dans les gammes des marques `[0, 3)` et `[6, 8)`.
--   `CounterID IN ('a', 'h') AND Date = 3` le serveur lit les données dans les gammes des marques `[1, 3)` et `[7, 8)`.
--   `Date = 3`, le serveur lit les données de la plage de marque `[1, 10]`.
-
-Les exemples ci-dessus montrent qu'il est toujours plus efficace d'utiliser un indice qu'une analyse complète.
-
-Un index clairsemé permet de lire des données supplémentaires. Lors de la lecture d'une plage unique de la clé primaire, jusqu'à `index_granularity * 2` lignes supplémentaires dans chaque bloc de données peut être lu.
-
-Les index clairsemés vous permettent de travailler avec un très grand nombre de lignes de table, car dans la plupart des cas, ces index tiennent dans la RAM de l'ordinateur.
-
-ClickHouse ne nécessite pas de clé primaire unique. Vous pouvez insérer plusieurs lignes avec la même clé primaire.
-
-### Sélection de la clé primaire {#selecting-the-primary-key}
-
-Le nombre de colonnes de la clé primaire n'est pas explicitement limitée. Selon la structure de données, vous pouvez inclure plus ou moins de colonnes dans la clé primaire. Cela peut:
-
--   Améliorer la performance d'un indice.
-
-    Si la clé primaire est `(a, b)`, puis ajouter une autre colonne `c` pour améliorer les performances si les conditions suivantes sont réunies:
-
-    -   Il y a des requêtes avec une condition sur la colonne `c`.
-    -   Longues plages de données (plusieurs fois plus longues que `index_granularity`) avec des valeurs identiques pour `(a, b)` sont communs. En d'autres termes, lors de l'ajout d'une autre colonne vous permet de passer très longues plages de données.
-
--   Améliorer la compression des données.
-
-    ClickHouse trie les données par clé primaire, donc plus la cohérence est élevée, meilleure est la compression.
-
--   Fournir une logique supplémentaire lors de la fusion de parties de [CollapsingMergeTree](collapsingmergetree.md#table_engine-collapsingmergetree) et [SummingMergeTree](summingmergetree.md) moteur.
-
-    Dans ce cas, on peut spécifier l' *clé de tri* qui est différente de la clé primaire.
-
-Une clé primaire longue affectera négativement les performances d'insertion et la consommation de mémoire, mais des colonnes supplémentaires dans la clé primaire n'affecteront pas les performances de ClickHouse pendant `SELECT` requête.
-
-### Choisir une clé primaire qui diffère de la clé de tri {#choosing-a-primary-key-that-differs-from-the-sorting-key}
-
-Il est possible de spécifier une clé primaire (une expression avec des valeurs qui sont écrites dans le fichier d'index pour chaque marque) qui est différente de la clé de tri (une expression pour trier les lignes dans les parties de données). Dans ce cas, le tuple d'expression de clé primaire doit être un préfixe du tuple d'expression de clé de tri.
-
-Cette fonctionnalité est utile lorsque vous utilisez le [SummingMergeTree](summingmergetree.md) et
-[AggregatingMergeTree](aggregatingmergetree.md) table des moteurs. Dans un cas courant lors de l'utilisation de ces moteurs, la table a deux types de colonnes: *dimension* et *mesure*. Les requêtes typiques agrégent les valeurs des colonnes de mesure avec arbitraire `GROUP BY` et filtrage par dimensions. Comme SummingMergeTree et AggregatingMergeTree regroupent des lignes avec la même valeur de la clé de tri, il est naturel d'y ajouter toutes les dimensions. En conséquence, l'expression se compose d'une longue liste de colonnes, et cette liste doit être mise à jour fréquemment avec nouvellement ajoutée.
-
-Dans ce cas, il est logique de ne laisser que quelques colonnes dans la clé primaire qui fourniront des analyses de plage efficaces et ajouteront les colonnes de dimension restantes au tuple de clé de tri.
-
-[ALTER](../../../sql-reference/statements/alter.md) la clé de tri est une opération légère car lorsqu'une nouvelle colonne est ajoutée simultanément à la table et à la clé de tri, les parties de données existantes n'ont pas besoin d'être modifiées. Comme l'ancienne clé de tri est un préfixe de la nouvelle clé de tri et qu'il n'y a pas de données dans la colonne nouvellement ajoutée, les données sont triées à la fois par l'ancienne et la nouvelle clé de tri au moment de la modification de la table.
-
-### Utilisation D'Index et de Partitions dans les requêtes {#use-of-indexes-and-partitions-in-queries}
-
-Pour `SELECT` requêtes, clickhouse analyse si un index peut être utilisé. Un index peut être utilisé si le `WHERE/PREWHERE` clause a une expression (comme l'un des éléments de conjonction, ou entièrement) qui représente une opération de comparaison d'égalité ou d'inégalité, ou si elle a `IN` ou `LIKE` avec un préfixe fixe sur les colonnes ou les expressions qui sont dans la clé primaire ou la clé de partitionnement, ou sur certaines fonctions partiellement répétitives de ces colonnes, ou les relations logiques de ces expressions.
-
-Ainsi, il est possible d'exécuter des requêtes sur une ou plusieurs plages de la clé primaire. Dans cet exemple, les requêtes seront rapides lorsqu'elles sont exécutées pour une balise de suivi spécifique, pour une balise et une plage de dates spécifiques, pour une balise et une date spécifiques, pour plusieurs balises avec une plage de dates, etc.
-
-Regardons le moteur configuré comme suit:
-
-      ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate) SETTINGS index_granularity=8192
-
-Dans ce cas, dans les requêtes:
-
-``` sql
-SELECT count() FROM table WHERE EventDate = toDate(now()) AND CounterID = 34
-SELECT count() FROM table WHERE EventDate = toDate(now()) AND (CounterID = 34 OR CounterID = 42)
-SELECT count() FROM table WHERE ((EventDate >= toDate('2014-01-01') AND EventDate <= toDate('2014-01-31')) OR EventDate = toDate('2014-05-01')) AND CounterID IN (101500, 731962, 160656) AND (CounterID = 101500 OR EventDate != toDate('2014-05-01'))
-```
-
-ClickHouse utilisera l'index de clé primaire pour supprimer les données incorrectes et la clé de partitionnement mensuel pour supprimer les partitions qui se trouvent dans des plages de dates incorrectes.
-
-Les requêtes ci-dessus montrent que l'index est utilisé même pour les expressions complexes. La lecture de la table est organisée de sorte que l'utilisation de l'index ne peut pas être plus lente qu'une analyse complète.
-
-Dans l'exemple ci-dessous, l'index ne peut pas être utilisé.
-
-``` sql
-SELECT count() FROM table WHERE CounterID = 34 OR URL LIKE '%upyachka%'
-```
-
-Pour vérifier si ClickHouse pouvez utiliser l'index lors de l'exécution d'une requête, utilisez les paramètres [force_index_by_date](../../../operations/settings/settings.md#settings-force_index_by_date) et [force_primary_key](../../../operations/settings/settings.md).
-
-La clé de partitionnement par mois permet de lire uniquement les blocs de données qui contiennent des dates de la plage appropriée. Dans ce cas, le bloc de données peut contenir des données pour plusieurs dates (jusqu'à un mois entier). Dans un bloc, les données sont triées par clé primaire, qui peut ne pas contenir la date comme première colonne. Pour cette raison, l'utilisation d'une requête avec seulement une condition de date qui ne spécifie pas le préfixe de clé primaire entraînera la lecture de plus de données que pour une seule date.
-
-### Utilisation de L'Index pour les clés primaires partiellement monotones {#use-of-index-for-partially-monotonic-primary-keys}
-
-Considérons, par exemple, les jours du mois. Ils forment un [monotone de la séquence](https://en.wikipedia.org/wiki/Monotonic_function) pendant un mois, mais pas monotone pendant des périodes plus longues. C'est une séquence partiellement monotone. Si un utilisateur crée la table avec une clé primaire partiellement monotone, ClickHouse crée un index clairsemé comme d'habitude. Lorsqu'un utilisateur sélectionne des données à partir de ce type de table, ClickHouse analyse les conditions de requête. Si L'utilisateur veut obtenir des données entre deux marques de l'index et que ces deux marques tombent dans un mois, ClickHouse peut utiliser l'index dans ce cas particulier car il peut calculer la distance entre les paramètres d'une requête et les marques d'index.
-
-ClickHouse ne peut pas utiliser un index si les valeurs de la clé primaire dans la plage de paramètres de requête ne représentent pas une séquence monotone. Dans ce cas, ClickHouse utilise la méthode full scan.
-
-ClickHouse utilise cette logique non seulement pour les séquences de jours du mois, mais pour toute clé primaire qui représente une séquence partiellement monotone.
-
-### Index de saut de données (expérimental) {#table_engine-mergetree-data_skipping-indexes}
-
-La déclaration d'index se trouve dans la section colonnes du `CREATE` requête.
-
-``` sql
-INDEX index_name expr TYPE type(...) GRANULARITY granularity_value
-```
-
-Pour les tables de la `*MergeTree` famille, les indices de saut de données peuvent être spécifiés.
-
-Ces indices agrégent certaines informations sur l'expression spécifiée sur les blocs, qui consistent en `granularity_value` granules (la taille du granule est spécifiée en utilisant `index_granularity` réglage dans le moteur de table). Ensuite, ces agrégats sont utilisés dans `SELECT` requêtes pour réduire la quantité de données à lire à partir du disque en ignorant de gros blocs de données `where` la requête ne peut pas être satisfait.
-
-**Exemple**
-
-``` sql
-CREATE TABLE table_name
-(
-    u64 UInt64,
-    i32 Int32,
-    s String,
-    ...
-    INDEX a (u64 * i32, s) TYPE minmax GRANULARITY 3,
-    INDEX b (u64 * length(s)) TYPE set(1000) GRANULARITY 4
-) ENGINE = MergeTree()
-...
-```
-
-Les Indices de L'exemple peuvent être utilisés par ClickHouse pour réduire la quantité de données à lire à partir du disque dans les requêtes suivantes:
-
-``` sql
-SELECT count() FROM table WHERE s < 'z'
-SELECT count() FROM table WHERE u64 * i32 == 10 AND u64 * length(s) >= 1234
-```
-
-#### Types d'Indices disponibles {#available-types-of-indices}
-
--   `minmax`
-
-    Magasins extrêmes de l'expression spécifiée (si l'expression est `tuple` puis il stocke les extrêmes pour chaque élément de `tuple`), utilise les informations stockées pour sauter des blocs de données comme la clé primaire.
-
--   `set(max_rows)`
-
-    Stocke les valeurs uniques de l'expression spécifiée (pas plus de `max_rows` rangée, `max_rows=0` moyen “no limits”). Utilise les valeurs pour vérifier si le `WHERE` l'expression n'est pas satisfiable sur un bloc de données.
-
--   `ngrambf_v1(n, size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)`
-
-    Magasins un [Filtre de Bloom](https://en.wikipedia.org/wiki/Bloom_filter) qui contient tous les ngrams d'un bloc de données. Fonctionne uniquement avec des chaînes. Peut être utilisé pour l'optimisation de `equals`, `like` et `in` expression.
-
-    -   `n` — ngram size,
-    -   `size_of_bloom_filter_in_bytes` — Bloom filter size in bytes (you can use large values here, for example, 256 or 512, because it can be compressed well).
-    -   `number_of_hash_functions` — The number of hash functions used in the Bloom filter.
-    -   `random_seed` — The seed for Bloom filter hash functions.
-
--   `tokenbf_v1(size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)`
-
-    Le même que `ngrambf_v1`, mais stocke des jetons au lieu de ngrams. Les jetons sont des séquences séparées par des caractères non alphanumériques.
-
--   `bloom_filter([false_positive])` — Stores a [Filtre de Bloom](https://en.wikipedia.org/wiki/Bloom_filter) pour les colonnes spécifiées.
-
-    Facultatif `false_positive` le paramètre est la probabilité de recevoir une réponse faussement positive du filtre. Valeurs possibles: (0, 1). Valeur par défaut: 0.025.
-
-    Types de données pris en charge: `Int*`, `UInt*`, `Float*`, `Enum`, `Date`, `DateTime`, `String`, `FixedString`, `Array`, `LowCardinality`, `Nullable`.
-
-    Les fonctions suivantes peuvent l'utiliser: [égal](../../../sql-reference/functions/comparison-functions.md), [notEquals](../../../sql-reference/functions/comparison-functions.md), [dans](../../../sql-reference/functions/in-functions.md), [notIn](../../../sql-reference/functions/in-functions.md), [avoir](../../../sql-reference/functions/array-functions.md).
-
-<!-- -->
-
-``` sql
-INDEX sample_index (u64 * length(s)) TYPE minmax GRANULARITY 4
-INDEX sample_index2 (u64 * length(str), i32 + f64 * 100, date, str) TYPE set(100) GRANULARITY 4
-INDEX sample_index3 (lower(str), str) TYPE ngrambf_v1(3, 256, 2, 0) GRANULARITY 4
-```
-
-#### Les Fonctions De Soutien {#functions-support}
-
-Les Conditions dans le `WHERE` la clause contient des appels des fonctions qui fonctionnent avec des colonnes. Si la colonne fait partie d'un index, ClickHouse essaie d'utiliser cet index lors de l'exécution des fonctions. ClickHouse prend en charge différents sous-ensembles de fonctions pour l'utilisation d'index.
-
-Le `set` l'indice peut être utilisé avec toutes les fonctions. Les sous-ensembles de fonctions pour les autres index sont présentés dans le tableau ci-dessous.
-
-| Fonction (opérateur) / Indice de                                                                             | clé primaire | minmax | ngrambf_v1 | tokenbf_v1 | bloom_filter |
-|--------------------------------------------------------------------------------------------------------------|--------------|--------|-------------|-------------|---------------|
-| [égal (=, ==)](../../../sql-reference/functions/comparison-functions.md#function-equals)                     | ✔            | ✔      | ✔           | ✔           | ✔             |
-| [notEquals (!=, \<\>)](../../../sql-reference/functions/comparison-functions.md#function-notequals)          | ✔            | ✔      | ✔           | ✔           | ✔             |
-| [comme](../../../sql-reference/functions/string-search-functions.md#function-like)                           | ✔            | ✔      | ✔           | ✗           | ✗             |
-| [notLike](../../../sql-reference/functions/string-search-functions.md#function-notlike)                      | ✔            | ✔      | ✔           | ✗           | ✗             |
-| [startsWith](../../../sql-reference/functions/string-functions.md#startswith)                                | ✔            | ✔      | ✔           | ✔           | ✗             |
-| [endsWith](../../../sql-reference/functions/string-functions.md#endswith)                                    | ✗            | ✗      | ✔           | ✔           | ✗             |
-| [multiSearchAny](../../../sql-reference/functions/string-search-functions.md#function-multisearchany)        | ✗            | ✗      | ✔           | ✗           | ✗             |
-| [dans](../../../sql-reference/functions/in-functions.md#in-functions)                                        | ✔            | ✔      | ✔           | ✔           | ✔             |
-| [notIn](../../../sql-reference/functions/in-functions.md#in-functions)                                       | ✔            | ✔      | ✔           | ✔           | ✔             |
-| [peu (\<)](../../../sql-reference/functions/comparison-functions.md#function-less)                           | ✔            | ✔      | ✗           | ✗           | ✗             |
-| [grand (\>)](../../../sql-reference/functions/comparison-functions.md#function-greater)                      | ✔            | ✔      | ✗           | ✗           | ✗             |
-| [lessOrEquals (\<=)](../../../sql-reference/functions/comparison-functions.md#function-lessorequals)         | ✔            | ✔      | ✗           | ✗           | ✗             |
-| [greaterOrEquals ( \> =)](../../../sql-reference/functions/comparison-functions.md#function-greaterorequals) | ✔            | ✔      | ✗           | ✗           | ✗             |
-| [vide](../../../sql-reference/functions/array-functions.md#function-empty)                                   | ✔            | ✔      | ✗           | ✗           | ✗             |
-| [notEmpty](../../../sql-reference/functions/array-functions.md#function-notempty)                            | ✔            | ✔      | ✗           | ✗           | ✗             |
-| hasToken                                                                                                     | ✗            | ✗      | ✗           | ✔           | ✗             |
-
-Les fonctions avec un argument constant inférieur à la taille ngram ne peuvent pas être utilisées par `ngrambf_v1` pour l'optimisation de la requête.
-
-Les filtres Bloom peuvent avoir des correspondances faussement positives, de sorte que le `ngrambf_v1`, `tokenbf_v1`, et `bloom_filter` les index ne peuvent pas être utilisés pour optimiser les requêtes où le résultat d'une fonction est censé être faux, par exemple:
-
--   Peut être optimisé:
-    -   `s LIKE '%test%'`
-    -   `NOT s NOT LIKE '%test%'`
-    -   `s = 1`
-    -   `NOT s != 1`
-    -   `startsWith(s, 'test')`
--   Ne peut pas être optimisé:
-    -   `NOT s LIKE '%test%'`
-    -   `s NOT LIKE '%test%'`
-    -   `NOT s = 1`
-    -   `s != 1`
-    -   `NOT startsWith(s, 'test')`
-
-## Accès Simultané Aux Données {#concurrent-data-access}
-
-Pour l'accès aux tables simultanées, nous utilisons le multi-versioning. En d'autres termes, lorsqu'une table est lue et mise à jour simultanément, les données sont lues à partir d'un ensemble de parties en cours au moment de la requête. Il n'y a pas de longues mèches. Les Inserts ne gênent pas les opérations de lecture.
-
-Lecture à partir d'un tableau est automatiquement parallélisée.
-
-## TTL pour les colonnes et les tableaux {#table_engine-mergetree-ttl}
-
-Détermine la durée de vie de des valeurs.
-
-Le `TTL` clause peut être définie pour la table entière et pour chaque colonne individuelle. Ttl de niveau Table peut également spécifier la logique de déplacement automatique des données entre les disques et les volumes.
-
-Les Expressions doivent évaluer pour [Date](../../../sql-reference/data-types/date.md) ou [DateTime](../../../sql-reference/data-types/datetime.md) type de données.
-
-Exemple:
-
-``` sql
-TTL time_column
-TTL time_column + interval
-```
-
-Définir `interval`, utiliser [intervalle](../../../sql-reference/operators/index.md#operators-datetime) opérateur.
-
-``` sql
-TTL date_time + INTERVAL 1 MONTH
-TTL date_time + INTERVAL 15 HOUR
-```
-
-### Colonne TTL {#mergetree-column-ttl}
-
-Lorsque les valeurs de la colonne expirent, ClickHouse les remplace par les valeurs par défaut du type de données de la colonne. Si toutes les valeurs de colonne de la partie données expirent, ClickHouse supprime cette colonne de la partie données d'un système de fichiers.
-
-Le `TTL` la clause ne peut pas être utilisée pour les colonnes clés.
-
-Exemple:
-
-Création d'une table avec TTL
-
-``` sql
-CREATE TABLE example_table
-(
-    d DateTime,
-    a Int TTL d + INTERVAL 1 MONTH,
-    b Int TTL d + INTERVAL 1 MONTH,
-    c String
-)
-ENGINE = MergeTree
-PARTITION BY toYYYYMM(d)
-ORDER BY d;
-```
-
-Ajout de TTL à une colonne d'une table existante
-
-``` sql
-ALTER TABLE example_table
-    MODIFY COLUMN
-    c String TTL d + INTERVAL 1 DAY;
-```
-
-Modification de TTL de la colonne
-
-``` sql
-ALTER TABLE example_table
-    MODIFY COLUMN
-    c String TTL d + INTERVAL 1 MONTH;
-```
-
-### Tableau TTL {#mergetree-table-ttl}
-
-Table peut avoir une expression pour la suppression de Lignes expirées, et plusieurs expressions pour le déplacement automatique de pièces entre [disques ou volumes](#table_engine-mergetree-multiple-volumes). Lorsque les lignes de la table expirent, ClickHouse supprime toutes les lignes correspondantes. Pour les pièces en mouvement, toutes les lignes d'une pièce doivent satisfaire aux critères d'expression de mouvement.
-
-``` sql
-TTL expr [DELETE|TO DISK 'aaa'|TO VOLUME 'bbb'], ...
-```
-
-Type de règle TTL peut suivre chaque expression TTL. Il affecte une action qui doit être faite une fois que l'expression est satisfaite (atteint l'heure actuelle):
-
--   `DELETE` - supprimer les Lignes expirées (action par défaut);
--   `TO DISK 'aaa'` - déplacer la partie sur le disque `aaa`;
--   `TO VOLUME 'bbb'` - déplacer la partie sur le disque `bbb`.
-
-Exemple:
-
-Création d'une table avec TTL
-
-``` sql
-CREATE TABLE example_table
-(
-    d DateTime,
-    a Int
-)
-ENGINE = MergeTree
-PARTITION BY toYYYYMM(d)
-ORDER BY d
-TTL d + INTERVAL 1 MONTH [DELETE],
-    d + INTERVAL 1 WEEK TO VOLUME 'aaa',
-    d + INTERVAL 2 WEEK TO DISK 'bbb';
-```
-
-Modification de TTL de la table
-
-``` sql
-ALTER TABLE example_table
-    MODIFY TTL d + INTERVAL 1 DAY;
-```
-
-**Suppression De Données**
-
-Les données avec un TTL expiré sont supprimées lorsque ClickHouse fusionne des parties de données.
-
-Lorsque ClickHouse voit que les données sont expirées, il effectue une fusion hors calendrier. Pour contrôler la fréquence de ces fusions, vous pouvez définir `merge_with_ttl_timeout`. Si la valeur est trop faible, il effectuera de nombreuses fusions hors calendrier qui peuvent consommer beaucoup de ressources.
-
-Si vous effectuez la `SELECT` requête entre les fusionne, vous pouvez obtenir des données expirées. Pour éviter cela, utilisez la [OPTIMIZE](../../../sql-reference/statements/misc.md#misc_operations-optimize) requête avant de l' `SELECT`.
-
-## Utilisation de plusieurs périphériques de bloc pour le stockage de données {#table_engine-mergetree-multiple-volumes}
-
-### Introduction {#introduction}
-
-`MergeTree` les moteurs de table de famille peuvent stocker des données sur plusieurs périphériques de bloc. Par exemple, il peut être utile lorsque les données d'un tableau sont implicitement divisé en “hot” et “cold”. Les données les plus récentes sont régulièrement demandées mais ne nécessitent qu'une petite quantité d'espace. Au contraire, les données historiques à queue grasse sont rarement demandées. Si plusieurs disques sont disponibles, la “hot” les données peuvent être situées sur des disques rapides (par exemple, SSD NVMe ou en mémoire), tandis que le “cold” des données relativement lente (par exemple, disque dur).
-
-La partie de données est l'unité mobile minimum pour `MergeTree`-tables de moteur. Les données appartenant à une partie sont stockées sur un disque. Les parties de données peuvent être déplacées entre les disques en arrière-plan (selon les paramètres de l'utilisateur) ainsi qu'au moyen du [ALTER](../../../sql-reference/statements/alter.md#alter_move-partition) requête.
-
-### Terme {#terms}
-
--   Disk — Block device mounted to the filesystem.
--   Default disk — Disk that stores the path specified in the [chemin](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-path) paramètre de serveur.
--   Volume — Ordered set of equal disks (similar to [JBOD](https://en.wikipedia.org/wiki/Non-RAID_drive_architectures)).
--   Storage policy — Set of volumes and the rules for moving data between them.
-
-Les noms donnés aux entités décrites peuvent être trouvés dans les tables système, [système.storage_policies](../../../operations/system-tables.md#system_tables-storage_policies) et [système.disque](../../../operations/system-tables.md#system_tables-disks). Pour appliquer l'une des stratégies de stockage configurées pour une table, utilisez `storage_policy` réglage de `MergeTree`-moteur de table de famille.
-
-### Configuration {#table_engine-mergetree-multiple-volumes_configure}
-
-Les disques, les volumes et les stratégies de stockage doivent être déclarés `<storage_configuration>` étiquette, soit dans le fichier principal `config.xml` ou dans un fichier distinct dans le `config.d` répertoire.
-
-Structure de Configuration:
-
-``` xml
-<storage_configuration>
-    <disks>
-        <disk_name_1> <!-- disk name -->
-            <path>/mnt/fast_ssd/clickhouse/</path>
-        </disk_name_1>
-        <disk_name_2>
-            <path>/mnt/hdd1/clickhouse/</path>
-            <keep_free_space_bytes>10485760</keep_free_space_bytes>
-        </disk_name_2>
-        <disk_name_3>
-            <path>/mnt/hdd2/clickhouse/</path>
-            <keep_free_space_bytes>10485760</keep_free_space_bytes>
-        </disk_name_3>
-
-        ...
-    </disks>
-
-    ...
-</storage_configuration>
-```
-
-Balise:
-
--   `<disk_name_N>` — Disk name. Names must be different for all disks.
--   `path` — path under which a server will store data (`data` et `shadow` des dossiers), doit être terminé par ‘/’.
--   `keep_free_space_bytes` — the amount of free disk space to be reserved.
-
-L'ordre du disque définition n'est pas important.
-
-Stratégies de stockage balisage de configuration:
-
-``` xml
-<storage_configuration>
-    ...
-    <policies>
-        <policy_name_1>
-            <volumes>
-                <volume_name_1>
-                    <disk>disk_name_from_disks_configuration</disk>
-                    <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>
-                </volume_name_1>
-                <volume_name_2>
-                    <!-- configuration -->
-                </volume_name_2>
-                <!-- more volumes -->
-            </volumes>
-            <move_factor>0.2</move_factor>
-        </policy_name_1>
-        <policy_name_2>
-            <!-- configuration -->
-        </policy_name_2>
-
-        <!-- more policies -->
-    </policies>
-    ...
-</storage_configuration>
-```
-
-Balise:
-
--   `policy_name_N` — Policy name. Policy names must be unique.
--   `volume_name_N` — Volume name. Volume names must be unique.
--   `disk` — a disk within a volume.
--   `max_data_part_size_bytes` — the maximum size of a part that can be stored on any of the volume's disks.
--   `move_factor` — when the amount of available space gets lower than this factor, data automatically start to move on the next volume if any (by default, 0.1).
-
-Exemples de Cofiguration:
-
-``` xml
-<storage_configuration>
-    ...
-    <policies>
-        <hdd_in_order> <!-- policy name -->
-            <volumes>
-                <single> <!-- volume name -->
-                    <disk>disk1</disk>
-                    <disk>disk2</disk>
-                </single>
-            </volumes>
-        </hdd_in_order>
-
-        <moving_from_ssd_to_hdd>
-            <volumes>
-                <hot>
-                    <disk>fast_ssd</disk>
-                    <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>
-                </hot>
-                <cold>
-                    <disk>disk1</disk>
-                </cold>
-            </volumes>
-            <move_factor>0.2</move_factor>
-        </moving_from_ssd_to_hdd>
-    </policies>
-    ...
-</storage_configuration>
-```
-
-Dans l'exemple donné, la `hdd_in_order` politique met en œuvre les [round-robin](https://en.wikipedia.org/wiki/Round-robin_scheduling) approche. Ainsi cette politique ne définit qu'un seul volume (`single`), les parties des données sont stockées sur tous ses disques dans l'ordre circulaire. Une telle politique peut être très utile s'il y a plusieurs disques similaires sont montés sur le système, mais RAID N'est pas configuré. Gardez à l'esprit que chaque lecteur de disque n'est pas fiable et vous pouvez compenser avec facteur de réplication de 3 ou plus.
-
-S'il existe différents types de disques disponibles dans le système, `moving_from_ssd_to_hdd` la stratégie peut être utilisée à la place. Volume `hot` se compose d'un disque SSD (`fast_ssd`), et la taille maximale d'une pièce qui peut être stocké sur ce volume est de 1 go. Toutes les pièces avec la taille plus grande que 1 GB sera stocké directement sur le `cold` le volume, qui contient un disque dur de disque `disk1`.
-Aussi, une fois le disque `fast_ssd` est complété par plus de 80%, les données seront transférées à la `disk1` par un processus d'arrière-plan.
-
-L'ordre d'énumération des volumes dans une stratégie de stockage est important. Une fois qu'un volume est surchargé, les données sont déplacées vers le suivant. L'ordre d'énumération des disques est important parce que les données sont stockées dans les virages.
-
-Lors de la création d'une table, on peut lui appliquer l'une des stratégies de stockage configurées:
-
-``` sql
-CREATE TABLE table_with_non_default_policy (
-    EventDate Date,
-    OrderID UInt64,
-    BannerID UInt64,
-    SearchPhrase String
-) ENGINE = MergeTree
-ORDER BY (OrderID, BannerID)
-PARTITION BY toYYYYMM(EventDate)
-SETTINGS storage_policy = 'moving_from_ssd_to_hdd'
-```
-
-Le `default` la Politique de stockage implique d'utiliser un seul volume, qui se compose d'un seul disque donné dans `<path>`. Une fois qu'une table est créée, sa stratégie de stockage ne peut pas être modifiée.
-
-### Détail {#details}
-
-Dans le cas de `MergeTree` les tableaux, les données sont sur le disque de différentes façons:
-
--   En tant que résultat d'un insert (`INSERT` requête).
--   En arrière-plan fusionne et [mutation](../../../sql-reference/statements/alter.md#alter-mutations).
--   Lors du téléchargement à partir d'une autre réplique.
--   À la suite du gel de la partition [ALTER TABLE … FREEZE PARTITION](../../../sql-reference/statements/alter.md#alter_freeze-partition).
-
-Dans tous ces cas, à l'exception des mutations et du gel de partition, une pièce est stockée sur un volume et un disque selon la Politique de stockage donnée:
-
-1.  Le premier volume (dans l'ordre de définition) qui a suffisamment d'espace disque pour stocker une pièce (`unreserved_space > current_part_size`) et permet de stocker des pièces d'une taille donnée (`max_data_part_size_bytes > current_part_size`) est choisi.
-2.  Dans ce volume, ce disque est choisi qui suit celui, qui a été utilisé pour stocker le bloc de données précédent, et qui a de l'espace libre plus que la taille de la pièce (`unreserved_space - keep_free_space_bytes > current_part_size`).
-
-Sous le capot, les mutations et la congélation des cloisons utilisent [des liens en dur](https://en.wikipedia.org/wiki/Hard_link). Les liens durs entre différents disques ne sont pas pris en charge, donc dans de tels cas, les pièces résultantes sont stockées sur les mêmes disques que les disques initiaux.
-
-En arrière - plan, les pièces sont déplacées entre les volumes en fonction de la quantité d'espace libre (`move_factor` paramètre) selon l'ordre les volumes sont déclarées dans le fichier de configuration.
-Les données ne sont jamais transférées du dernier et dans le premier. On peut utiliser des tables système [système.part_log](../../../operations/system-tables.md#system_tables-part-log) (champ `type = MOVE_PART`) et [système.partie](../../../operations/system-tables.md#system_tables-parts) (Fields `path` et `disk`) pour surveiller l'arrière-plan se déplace. Aussi, les informations détaillées peuvent être trouvées dans les journaux du serveur.
-
-L'utilisateur peut forcer le déplacement d'une partie ou d'une partition d'un volume à l'autre à l'aide de la requête [ALTER TABLE … MOVE PART\|PARTITION … TO VOLUME\|DISK …](../../../sql-reference/statements/alter.md#alter_move-partition), toutes les restrictions pour les opérations de fond sont prises en compte. La requête initie un mouvement seul et n'attend pas que les opérations d'arrière-plan soient terminées. L'utilisateur recevra un message d'erreur si pas assez d'espace libre est disponible ou si l'une des conditions requises ne sont pas remplies.
-
-Le déplacement des données n'interfère pas avec la réplication des données. Par conséquent, différentes stratégies de stockage peuvent être spécifiées pour la même table sur différents réplicas.
-
-Après l'achèvement des fusions d'arrière-plan et des mutations, les anciennes parties ne sont supprimées qu'après un certain temps (`old_parts_lifetime`).
-Pendant ce temps, ils ne sont pas déplacés vers d'autres volumes ou des disques. Par conséquent, jusqu'à ce que les pièces soient finalement supprimées, elles sont toujours prises en compte pour l'évaluation de l'espace disque occupé.
-
-[Article Original](https://clickhouse.tech/docs/ru/operations/table_engines/mergetree/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/replacingmergetree.md b/docs/fr/engines/table-engines/mergetree-family/replacingmergetree.md
deleted file mode 100644
index ac3c0f3b0217..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/replacingmergetree.md
+++ /dev/null
@@ -1,69 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: ReplacingMergeTree
----
-
-# ReplacingMergeTree {#replacingmergetree}
-
-Le moteur diffère de [MergeTree](mergetree.md#table_engines-mergetree) en ce qu'il supprime les doublons avec la même valeur de clé primaire (ou, plus précisément, avec la même [clé de tri](mergetree.md) valeur).
-
-La déduplication des données se produit uniquement lors d'une fusion. La fusion se produit en arrière-plan à un moment inconnu, vous ne pouvez donc pas le planifier. Certaines des données peuvent rester non traitées. Bien que vous puissiez exécuter une fusion imprévue en utilisant le `OPTIMIZE` requête, ne comptez pas l'utiliser, parce que la `OPTIMIZE` requête va lire et écrire une grande quantité de données.
-
-Ainsi, `ReplacingMergeTree` convient pour effacer les données en double en arrière-plan afin d'économiser de l'espace, mais cela ne garantit pas l'absence de doublons.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = ReplacingMergeTree([ver])
-[PARTITION BY expr]
-[ORDER BY expr]
-[PRIMARY KEY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Pour une description des paramètres de requête, voir [demande de description](../../../sql-reference/statements/create.md).
-
-**ReplacingMergeTree Paramètres**
-
--   `ver` — column with version. Type `UInt*`, `Date` ou `DateTime`. Paramètre facultatif.
-
-    Lors de la fusion, `ReplacingMergeTree` de toutes les lignes avec la même clé primaire ne laisse qu'un:
-
-    -   Dernier dans la sélection, si `ver` pas ensemble.
-    -   Avec la version maximale, si `ver` défini.
-
-**Les clauses de requête**
-
-Lors de la création d'un `ReplacingMergeTree` la table de la même [clause](mergetree.md) sont nécessaires, comme lors de la création d'un `MergeTree` table.
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets et, si possible, remplacez les anciens projets par la méthode décrite ci-dessus.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] ReplacingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [ver])
-```
-
-Tous les paramètres excepté `ver` ont la même signification que dans `MergeTree`.
-
--   `ver` - colonne avec la version. Paramètre facultatif. Pour une description, voir le texte ci-dessus.
-
-</details>
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/replacingmergetree/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/replication.md b/docs/fr/engines/table-engines/mergetree-family/replication.md
deleted file mode 100644
index 1e74475ee793..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/replication.md
+++ /dev/null
@@ -1,218 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 31
-toc_title: "R\xE9plication Des Donn\xE9es"
----
-
-# Réplication Des Données {#table_engines-replication}
-
-La réplication n'est prise en charge que pour les tables de la famille MergeTree:
-
--   ReplicatedMergeTree
--   ReplicatedSummingMergeTree
--   ReplicatedReplacingMergeTree
--   ReplicatedAggregatingMergeTree
--   ReplicatedCollapsingMergeTree
--   ReplicatedVersionedCollapsingMergetree
--   ReplicatedGraphiteMergeTree
-
-La réplication fonctionne au niveau d'une table individuelle, Pas du serveur entier. Un serveur peut stocker des tables répliquées et non répliquées en même temps.
-
-La réplication ne dépend pas de la fragmentation. Chaque fragment a sa propre réplication indépendante.
-
-Données compressées pour `INSERT` et `ALTER` les requêtes sont répliquées (pour plus d'informations, consultez la documentation de [ALTER](../../../sql-reference/statements/alter.md#query_language_queries_alter)).
-
-`CREATE`, `DROP`, `ATTACH`, `DETACH` et `RENAME` les requêtes sont exécutées sur un seul serveur et ne sont pas répliquées:
-
--   Le `CREATE TABLE` query crée une nouvelle table réplicable sur le serveur où la requête est exécutée. Si cette table existe déjà sur d'autres serveurs, il ajoute une nouvelle réplique.
--   Le `DROP TABLE` requête supprime la réplique situé sur le serveur où l'exécution de la requête.
--   Le `RENAME` requête renomme la table sur l'une des répliques. En d'autres termes, les tables répliquées peuvent avoir des noms différents sur différentes répliques.
-
-Clickhouse utilise [Apache ZooKeeper](https://zookeeper.apache.org) pour stocker des informations méta répliques. Utilisez ZooKeeper version 3.4.5 ou plus récente.
-
-Pour utiliser la réplication, définissez les paramètres [zookeeper](../../../operations/server-configuration-parameters/settings.md#server-settings_zookeeper) section de configuration du serveur.
-
-!!! attention "Attention"
-    Ne négligez pas la sécurité. Clickhouse soutient le `digest` [Schéma ACL](https://zookeeper.apache.org/doc/current/zookeeperProgrammers.html#sc_ZooKeeperAccessControl) du sous-système de sécurité ZooKeeper.
-
-Exemple de définition des adresses du cluster ZooKeeper:
-
-``` xml
-<zookeeper>
-    <node index="1">
-        <host>example1</host>
-        <port>2181</port>
-    </node>
-    <node index="2">
-        <host>example2</host>
-        <port>2181</port>
-    </node>
-    <node index="3">
-        <host>example3</host>
-        <port>2181</port>
-    </node>
-</zookeeper>
-```
-
-Vous pouvez spécifier N'importe quel cluster Zookeeper existant et le système utilisera un répertoire pour ses propres données (le répertoire est spécifié lors de la création d'une table réplicable).
-
-Si ZooKeeper n'est pas défini dans le fichier de configuration, vous ne pouvez pas créer de tables répliquées et toutes les tables répliquées existantes seront en lecture seule.
-
-La gardienne n'est pas utilisé dans `SELECT` requêtes car la réplication n'affecte pas les performances de `SELECT` et les requêtes s'exécutent aussi vite que pour les tables non répliquées. Lors de l'interrogation de tables répliquées distribuées, le comportement de ClickHouse est contrôlé par les paramètres [max_replica_delay_for_distributed_queries](../../../operations/settings/settings.md#settings-max_replica_delay_for_distributed_queries) et [fallback_to_stale_replicas_for_distributed_queries](../../../operations/settings/settings.md#settings-fallback_to_stale_replicas_for_distributed_queries).
-
-Pour chaque `INSERT` requête, environ dix entrées sont ajoutées à ZooKeeper par le biais de plusieurs transactions. (Pour être plus précis, c'est pour chaque bloc de données inséré; une requête D'insertion contient un bloc ou un bloc par `max_insert_block_size = 1048576` rangée.) Cela conduit à des latences légèrement plus longues pour `INSERT` par rapport aux tables non répliquées. Mais si vous suivez les recommandations pour insérer des données dans des lots de pas plus d'un `INSERT` par seconde, cela ne crée aucun problème. L'ensemble du cluster clickhouse utilisé pour coordonner un cluster ZooKeeper a un total de plusieurs centaines `INSERTs` par seconde. Le débit sur les insertions de données (le nombre de lignes par seconde) est aussi élevé que pour les non-données répliquées.
-
-Pour les clusters très volumineux, vous pouvez utiliser différents clusters ZooKeeper pour différents fragments. Cependant, cela ne s'est pas avéré nécessaire sur le Yandex.Cluster Metrica (environ 300 serveurs).
-
-La réplication est asynchrone et multi-maître. `INSERT` les requêtes (ainsi que `ALTER`) peuvent être envoyés à n'importe quel serveur disponible. Les données sont insérées sur le serveur où la requête est exécutée, puis il est copié sur les autres serveurs. Comme il est asynchrone, les données récemment insérées apparaissent sur les autres répliques avec une certaine latence. Si une partie des répliques ne sont pas disponibles, les données sont écrites lorsqu'elles sont disponibles. Si une réplique est disponible, la latence correspond au temps nécessaire pour transférer le bloc de données compressées sur le réseau.
-
-Par défaut, une requête INSERT attend la confirmation de l'écriture des données à partir d'un seul réplica. Si les données ont été correctement écrit sur une seule réplique et le serveur avec cette réplique cesse d'exister, les données enregistrées seront perdues. Pour activer la confirmation des Écritures de données à partir de plusieurs réplicas, utilisez `insert_quorum` option.
-
-Chaque bloc de données est écrit de manière atomique. La requête D'insertion est divisée en blocs jusqu'à `max_insert_block_size = 1048576` rangée. En d'autres termes, si l' `INSERT` la requête a moins de 1048576 lignes, elle est faite de manière atomique.
-
-Les blocs de données sont dédupliquées. Pour plusieurs écritures du même bloc de données (blocs de données de même taille contenant les mêmes lignes dans le même ordre), le bloc n'est écrit qu'une seule fois. La raison en est en cas de défaillance du réseau lorsque l'application cliente ne sait pas si les données ont été écrites dans la base de données, de sorte que le `INSERT` requête peut simplement être répété. Peu importe à quelles insertions de réplica ont été envoyées avec des données identiques. `INSERTs` sont idempotents. Les paramètres de déduplication sont contrôlés par [merge_tree](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-merge_tree) les paramètres du serveur.
-
-Pendant la réplication, seules les données source à insérer sont transférées sur le réseau. D'autres transformations de données (fusion) sont coordonnées et effectuées sur toutes les répliques de la même manière. Cela minimise l'utilisation du réseau, ce qui signifie que la réplication fonctionne bien lorsque les répliques résident dans différents centres de données. (Notez que la duplication de données dans différents centres de données est l'objectif principal de la réplication.)
-
-Vous pouvez avoir n'importe quel nombre de répliques des mêmes données. Yandex.Metrica utilise la double réplication en production. Chaque serveur utilise RAID-5 ou RAID-6, et RAID-10 dans certains cas. C'est une solution relativement fiable et pratique.
-
-Le système surveille la synchronicité des données sur les répliques et est capable de récupérer après une défaillance. Le basculement est automatique (pour les petites différences de données) ou semi-automatique (lorsque les données diffèrent trop, ce qui peut indiquer une erreur de configuration).
-
-## Création De Tables Répliquées {#creating-replicated-tables}
-
-Le `Replicated` le préfixe est ajouté au nom du moteur de table. Exemple:`ReplicatedMergeTree`.
-
-**Répliqué \* MergeTree paramètres**
-
--   `zoo_path` — The path to the table in ZooKeeper.
--   `replica_name` — The replica name in ZooKeeper.
-
-Exemple:
-
-``` sql
-CREATE TABLE table_name
-(
-    EventDate DateTime,
-    CounterID UInt32,
-    UserID UInt32
-) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}')
-PARTITION BY toYYYYMM(EventDate)
-ORDER BY (CounterID, EventDate, intHash32(UserID))
-SAMPLE BY intHash32(UserID)
-```
-
-<details markdown="1">
-
-<summary>Exemple de syntaxe obsolète</summary>
-
-``` sql
-CREATE TABLE table_name
-(
-    EventDate DateTime,
-    CounterID UInt32,
-    UserID UInt32
-) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}', EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192)
-```
-
-</details>
-
-Comme le montre l'exemple, ces paramètres peuvent contenir des substitutions entre crochets. Les valeurs substituées sont tirées de la ‘macros’ section du fichier de configuration. Exemple:
-
-``` xml
-<macros>
-    <layer>05</layer>
-    <shard>02</shard>
-    <replica>example05-02-1.yandex.ru</replica>
-</macros>
-```
-
-Le chemin d'accès à la table dans ZooKeeper doit être unique pour chaque table répliquée. Les Tables sur différents fragments doivent avoir des chemins différents.
-Dans ce cas, le chemin se compose des parties suivantes:
-
-`/clickhouse/tables/` est le préfixe commun. Nous vous recommandons d'utiliser exactement celui-ci.
-
-`{layer}-{shard}` est l'identificateur de fragment. Dans cet exemple, il se compose de deux parties, depuis l'Yandex.Le cluster Metrica utilise le sharding à deux niveaux. Pour la plupart des tâches, vous ne pouvez laisser que la substitution {shard}, qui sera étendue à l'Identificateur de partition.
-
-`table_name` est le nom du nœud de la table dans ZooKeeper. C'est une bonne idée de le rendre identique au nom de la table. Il est défini explicitement, car contrairement au nom de la table, il ne change pas après une requête de renommage.
-*HINT*: vous pouvez ajouter un nom de base de données devant `table_name` Aussi. E. g. `db_name.table_name`
-
-Le nom du réplica identifie différentes réplicas de la même table. Vous pouvez utiliser le nom de serveur pour cela, comme dans l'exemple. Le nom doit seulement être unique dans chaque fragment.
-
-Vous pouvez définir les paramètres explicitement au lieu d'utiliser des substitutions. Cela peut être pratique pour tester et configurer de petits clusters. Cependant, vous ne pouvez pas utiliser de requêtes DDL distribuées (`ON CLUSTER`) dans ce cas.
-
-Lorsque vous travaillez avec de grands clusters, nous vous recommandons d'utiliser des substitutions car elles réduisent la probabilité d'erreur.
-
-Exécutez l' `CREATE TABLE` requête sur chaque réplique. Cette requête crée une nouvelle table répliquée, ou ajoute une nouvelle réplique à un existant.
-
-Si vous ajoutez une nouvelle réplique après que la table contient déjà des données sur d'autres répliques, les données seront copiées des autres répliques vers la nouvelle après l'exécution de la requête. En d'autres termes, la nouvelle réplique synchronise avec les autres.
-
-Pour supprimer une réplique, exécutez `DROP TABLE`. However, only one replica is deleted – the one that resides on the server where you run the query.
-
-## Récupération Après Des Échecs {#recovery-after-failures}
-
-Si ZooKeeper n'est pas disponible au démarrage d'un serveur, les tables répliquées passent en mode Lecture seule. Le système tente périodiquement de se connecter à ZooKeeper.
-
-Si ZooKeeper est indisponible pendant un `INSERT`, ou une erreur se produit lors de l'interaction avec ZooKeeper, une exception est levée.
-
-Après la connexion à ZooKeeper, le système vérifie si l'ensemble de données du système de fichiers local correspond à l'ensemble de données attendu (ZooKeeper stocke ces informations). S'il y a des incohérences mineures, le système les résout en synchronisant les données avec les répliques.
-
-Si le système détecte des parties de données brisées (avec la mauvaise taille des fichiers) ou des parties non reconnues (parties écrites dans le système de fichiers mais non enregistrées dans ZooKeeper), il les déplace vers le `detached` sous-répertoire (ils ne sont pas supprimés). Toutes les pièces manquantes sont copiées à partir des répliques.
-
-Notez que ClickHouse n'effectue aucune action destructrice telle que la suppression automatique d'une grande quantité de données.
-
-Lorsque le serveur démarre (ou établit une nouvelle session avec ZooKeeper), il vérifie uniquement la quantité et la taille de tous les fichiers. Si les tailles de fichier correspondent mais que les octets ont été modifiés quelque part au milieu, cela n'est pas détecté immédiatement, mais uniquement lorsque vous tentez de lire les données `SELECT` requête. La requête lève une exception concernant une somme de contrôle ou une taille non correspondante d'un bloc compressé. Dans ce cas, des parties de données sont ajoutées à la file d'attente de vérification et copiées à partir des répliques si nécessaire.
-
-Si la série de données diffère trop de celle attendue, un mécanisme de sécurité est déclenché. Le serveur entre cela dans le journal et refuse de lancer. La raison en est que ce cas peut indiquer une erreur de configuration, par exemple si une réplique sur un fragment a été accidentellement configurée comme une réplique sur un fragment différent. Cependant, les seuils pour ce mécanisme sont fixés assez bas, et cette situation peut se produire pendant la récupération normale de la défaillance. Dans ce cas, les données sont restaurées semi-automatiquement - par “pushing a button”.
-
-Pour démarrer la récupération, créez le nœud `/path_to_table/replica_name/flags/force_restore_data` dans ZooKeeper avec n'importe quel contenu, ou exécutez la commande pour restaurer toutes les tables répliquées:
-
-``` bash
-sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data
-```
-
-Puis redémarrez le serveur. Au démarrage, le serveur supprime ces indicateurs et démarre la récupération.
-
-## Récupération Après La Perte De Données Complète {#recovery-after-complete-data-loss}
-
-Si toutes les données et métadonnées ont disparu de l'un des serveurs, procédez comme suit pour la récupération:
-
-1.  Installez ClickHouse sur le serveur. Définissez correctement les substitutions dans le fichier de configuration qui contient l'Identificateur de fragment et les répliques, si vous les utilisez.
-2.  Si vous avez des tables non compliquées qui doivent être dupliquées manuellement sur les serveurs, copiez leurs données à partir d'un réplica (dans le répertoire `/var/lib/clickhouse/data/db_name/table_name/`).
-3.  Copier les définitions de table situées dans `/var/lib/clickhouse/metadata/` à partir d'une réplique. Si un identificateur de fragment ou de réplica est défini explicitement dans les définitions de table, corrigez-le de manière à ce qu'il corresponde à ce réplica. (Alternativement, démarrez le serveur et faites tous les `ATTACH TABLE` les requêtes qui auraient dû être dans les .les fichiers sql dans `/var/lib/clickhouse/metadata/`.)
-4.  Pour démarrer la récupération, créez le nœud ZooKeeper `/path_to_table/replica_name/flags/force_restore_data` tout contenu, ou d'exécuter la commande pour restaurer toutes les tables répliquées: `sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data`
-
-Ensuite, démarrez le serveur (redémarrez, s'il est déjà en cours d'exécution). Les données seront téléchargées à partir de répliques.
-
-Une autre option de récupération consiste à supprimer des informations sur la réplique perdue de ZooKeeper (`/path_to_table/replica_name`), puis créez à nouveau la réplique comme décrit dans “[Création de tables répliquées](#creating-replicated-tables)”.
-
-Il n'y a aucune restriction sur la bande passante réseau pendant la récupération. Gardez cela à l'esprit si vous restaurez de nombreuses répliques à la fois.
-
-## Conversion de MergeTree en ReplicatedMergeTree {#converting-from-mergetree-to-replicatedmergetree}
-
-Nous utilisons le terme `MergeTree` pour consulter tous les moteurs de la `MergeTree family` le même que pour `ReplicatedMergeTree`.
-
-Si vous avez eu une `MergeTree` table qui a été répliquée manuellement, vous pouvez le convertir en une table répliquée. Vous devrez peut-être le faire si vous avez déjà recueilli une grande quantité de données dans un `MergeTree` table et maintenant vous voulez activer la réplication.
-
-Si les données diffèrent sur différentes répliques, synchronisez-les d'abord ou supprimez-les sur toutes les répliques sauf une.
-
-Renommez la table mergetree existante, puis créez un `ReplicatedMergeTree` table avec l'ancien nom.
-Déplacez les données de l'ancienne table vers `detached` sous-répertoire à l'intérieur du répertoire avec les nouvelles données de la table (`/var/lib/clickhouse/data/db_name/table_name/`).
-Ensuite, exécutez `ALTER TABLE ATTACH PARTITION` sur l'une des répliques d'ajouter ces données à des parties de l'ensemble de travail.
-
-## Conversion de ReplicatedMergeTree en MergeTree {#converting-from-replicatedmergetree-to-mergetree}
-
-Créez une table MergeTree avec un nom différent. Déplacez toutes les données du répertoire avec le `ReplicatedMergeTree` données de la table dans le répertoire de données de la nouvelle table. Ensuite, supprimer le `ReplicatedMergeTree` table et redémarrez le serveur.
-
-Si vous voulez vous débarrasser d'un `ReplicatedMergeTree` table sans lancer le serveur:
-
--   Supprimer la `.sql` fichier dans le répertoire de métadonnées (`/var/lib/clickhouse/metadata/`).
--   Supprimer le chemin correspondant dans ZooKeeper (`/path_to_table/replica_name`).
-
-Après cela, vous pouvez lancer le serveur, créer un `MergeTree` tableau, déplacer les données de son répertoire, puis redémarrez le serveur.
-
-## Récupération lorsque les métadonnées du Cluster Zookeeper sont perdues ou endommagées {#recovery-when-metadata-in-the-zookeeper-cluster-is-lost-or-damaged}
-
-Si les données de ZooKeeper ont été perdues ou endommagées, vous pouvez les enregistrer en les déplaçant dans une table non compliquée comme décrit ci-dessus.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/replication/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/summingmergetree.md b/docs/fr/engines/table-engines/mergetree-family/summingmergetree.md
deleted file mode 100644
index f4f2b94a93f8..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/summingmergetree.md
+++ /dev/null
@@ -1,141 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: SummingMergeTree
----
-
-# SummingMergeTree {#summingmergetree}
-
-Le moteur hérite de [MergeTree](mergetree.md#table_engines-mergetree). La différence est que lors de la fusion de parties de données pour `SummingMergeTree` tables ClickHouse remplace toutes les lignes avec la même clé primaire (ou, plus précisément, avec la même [clé de tri](mergetree.md)) avec une ligne qui contient des valeurs résumées pour les colonnes avec le type de données numériques. Si la clé de tri est composée de telle sorte qu'une seule valeur de clé correspond à un grand nombre de lignes, cela réduit considérablement le volume de stockage et accélère la sélection des données.
-
-Nous vous recommandons d'utiliser le moteur avec `MergeTree`. Stocker des données complètes dans `MergeTree` table, et l'utilisation `SummingMergeTree` pour le stockage de données agrégées, par exemple, lors de la préparation de rapports. Une telle approche vous empêchera de perdre des données précieuses en raison d'une clé primaire mal composée.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = SummingMergeTree([columns])
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Pour une description des paramètres de requête, voir [demande de description](../../../sql-reference/statements/create.md).
-
-**Paramètres de SummingMergeTree**
-
--   `columns` - un n-uplet avec les noms de colonnes où les valeurs seront résumées. Paramètre facultatif.
-    Les colonnes doivent être d'un type numérique et ne doit pas être dans la clé primaire.
-
-    Si `columns` non spécifié, ClickHouse résume les valeurs dans toutes les colonnes avec un type de données numérique qui ne sont pas dans la clé primaire.
-
-**Les clauses de requête**
-
-Lors de la création d'un `SummingMergeTree` la table de la même [clause](mergetree.md) sont nécessaires, comme lors de la création d'un `MergeTree` table.
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets et, si possible, remplacez les anciens projets par la méthode décrite ci-dessus.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] SummingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [columns])
-```
-
-Tous les paramètres excepté `columns` ont la même signification que dans `MergeTree`.
-
--   `columns` — tuple with names of columns values of which will be summarized. Optional parameter. For a description, see the text above.
-
-</details>
-
-## Exemple D'Utilisation {#usage-example}
-
-Considérons le tableau suivant:
-
-``` sql
-CREATE TABLE summtt
-(
-    key UInt32,
-    value UInt32
-)
-ENGINE = SummingMergeTree()
-ORDER BY key
-```
-
-Insérer des données:
-
-``` sql
-INSERT INTO summtt Values(1,1),(1,2),(2,1)
-```
-
-ClickHouse peut résumer toutes les lignes pas complètement ([voir ci-dessous](#data-processing)), nous utilisons donc une fonction d'agrégation `sum` et `GROUP BY` la clause dans la requête.
-
-``` sql
-SELECT key, sum(value) FROM summtt GROUP BY key
-```
-
-``` text
-┌─key─┬─sum(value)─┐
-│   2 │          1 │
-│   1 │          3 │
-└─────┴────────────┘
-```
-
-## Le Traitement Des Données {#data-processing}
-
-Lorsque les données sont insérées dans une table, elles sont enregistrées telles quelles. Clickhouse fusionne périodiquement les parties de données insérées et c'est à ce moment que les lignes avec la même clé primaire sont additionnées et remplacées par une pour chaque partie de données résultante.
-
-ClickHouse can merge the data parts so that different resulting parts of data cat consist rows with the same primary key, i.e. the summation will be incomplete. Therefore (`SELECT`) une fonction d'agrégation [somme()](../../../sql-reference/aggregate-functions/reference.md#agg_function-sum) et `GROUP BY` la clause doit être utilisé dans une requête comme décrit dans l'exemple ci-dessus.
-
-### Règles communes pour la sommation {#common-rules-for-summation}
-
-Les valeurs dans les colonnes avec le type de données numériques sont résumées. L'ensemble des colonnes est défini par le paramètre `columns`.
-
-Si les valeurs étaient 0 dans toutes les colonnes pour la sommation, la ligne est supprimée.
-
-Si la colonne n'est pas dans la clé primaire et n'est pas résumée, une valeur arbitraire est sélectionnée parmi celles existantes.
-
-Les valeurs ne sont pas résumés des colonnes de la clé primaire.
-
-### La somme dans les colonnes Aggregatefunction {#the-summation-in-the-aggregatefunction-columns}
-
-Pour les colonnes de [Type AggregateFunction](../../../sql-reference/data-types/aggregatefunction.md) ClickHouse se comporte comme [AggregatingMergeTree](aggregatingmergetree.md) moteur d'agrégation selon la fonction.
-
-### Structures Imbriquées {#nested-structures}
-
-Table peut avoir des structures de données imbriquées qui sont traitées d'une manière spéciale.
-
-Si le nom d'une table imbriquée se termine avec `Map` et il contient au moins deux colonnes qui répondent aux critères suivants:
-
--   la première colonne est numérique `(*Int*, Date, DateTime)` ou une chaîne de caractères `(String, FixedString)`, nous allons l'appeler `key`,
--   les autres colonnes sont arithmétique `(*Int*, Float32/64)`, nous allons l'appeler `(values...)`,
-
-ensuite, cette table imbriquée est interprétée comme un mappage de `key => (values...)` et lors de la fusion de ses lignes, les éléments de deux ensembles de données sont regroupées par `key` avec une sommation du correspondant `(values...)`.
-
-Exemple:
-
-``` text
-[(1, 100)] + [(2, 150)] -> [(1, 100), (2, 150)]
-[(1, 100)] + [(1, 150)] -> [(1, 250)]
-[(1, 100)] + [(1, 150), (2, 150)] -> [(1, 250), (2, 150)]
-[(1, 100), (2, 150)] + [(1, -100)] -> [(2, 150)]
-```
-
-Lorsque vous demandez des données, utilisez [sumMap (clé, valeur)](../../../sql-reference/aggregate-functions/reference.md) fonction pour l'agrégation de `Map`.
-
-Pour la structure de données imbriquée, vous n'avez pas besoin de spécifier ses colonnes dans le tuple de colonnes pour la sommation.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/summingmergetree/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/mergetree-family/versionedcollapsingmergetree.md b/docs/fr/engines/table-engines/mergetree-family/versionedcollapsingmergetree.md
deleted file mode 100644
index 9be9fb5e76ed..000000000000
--- a/docs/fr/engines/table-engines/mergetree-family/versionedcollapsingmergetree.md
+++ /dev/null
@@ -1,238 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: VersionedCollapsingMergeTree
----
-
-# VersionedCollapsingMergeTree {#versionedcollapsingmergetree}
-
-Ce moteur:
-
--   Permet l'écriture rapide des États d'objet qui changent continuellement.
--   Supprime les anciens États d'objets en arrière-plan. Cela réduit considérablement le volume de stockage.
-
-Voir la section [Effondrer](#table_engines_versionedcollapsingmergetree) pour plus de détails.
-
-Le moteur hérite de [MergeTree](mergetree.md#table_engines-mergetree) et ajoute la logique de réduction des lignes à l'algorithme de fusion des parties de données. `VersionedCollapsingMergeTree` sert le même but que [CollapsingMergeTree](collapsingmergetree.md) mais utilise un autre effondrement algorithme qui permet d'insérer les données dans n'importe quel ordre avec plusieurs threads. En particulier, l' `Version` la colonne aide à réduire correctement les lignes même si elles sont insérées dans le mauvais ordre. Contrairement, `CollapsingMergeTree` permet uniquement une insertion strictement consécutive.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE = VersionedCollapsingMergeTree(sign, version)
-[PARTITION BY expr]
-[ORDER BY expr]
-[SAMPLE BY expr]
-[SETTINGS name=value, ...]
-```
-
-Pour une description des paramètres de requête, voir les [description de la requête](../../../sql-reference/statements/create.md).
-
-**Les Paramètres Du Moteur**
-
-``` sql
-VersionedCollapsingMergeTree(sign, version)
-```
-
--   `sign` — Name of the column with the type of row: `1` est un “state” rangée, `-1` est un “cancel” rangée.
-
-    Le type de données de colonne doit être `Int8`.
-
--   `version` — Name of the column with the version of the object state.
-
-    Le type de données de colonne doit être `UInt*`.
-
-**Les Clauses De Requête**
-
-Lors de la création d'un `VersionedCollapsingMergeTree` de table, de la même [clause](mergetree.md) sont requis lors de la création d'un `MergeTree` table.
-
-<details markdown="1">
-
-<summary>Méthode obsolète pour créer une Table</summary>
-
-!!! attention "Attention"
-    N'utilisez pas cette méthode dans les nouveaux projets. Si possible, passer les anciens projets à la méthode décrite ci-dessus.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-) ENGINE [=] VersionedCollapsingMergeTree(date-column [, samp#table_engines_versionedcollapsingmergetreeling_expression], (primary, key), index_granularity, sign, version)
-```
-
-Tous les paramètres, à l'exception `sign` et `version` ont la même signification que dans `MergeTree`.
-
--   `sign` — Name of the column with the type of row: `1` est un “state” rangée, `-1` est un “cancel” rangée.
-
-    Column Data Type — `Int8`.
-
--   `version` — Name of the column with the version of the object state.
-
-    Le type de données de colonne doit être `UInt*`.
-
-</details>
-
-## Effondrer {#table_engines_versionedcollapsingmergetree}
-
-### Données {#data}
-
-Considérez une situation où vous devez enregistrer des données en constante évolution pour un objet. Il est raisonnable d'avoir une ligne pour un objet et de mettre à jour la ligne chaque fois qu'il y a des modifications. Cependant, l'opération de mise à jour est coûteuse et lente pour un SGBD car elle nécessite la réécriture des données dans le stockage. La mise à jour n'est pas acceptable si vous devez écrire des données rapidement, mais vous pouvez écrire les modifications sur un objet de manière séquentielle comme suit.
-
-L'utilisation de la `Sign` colonne lors de l'écriture de la ligne. Si `Sign = 1` cela signifie que la ligne est un état d'un objet (appelons-la “state” rangée). Si `Sign = -1` il indique l'annulation de l'état d'un objet avec les mêmes attributs (appelons-la “cancel” rangée). Également utiliser l' `Version` colonne, qui doit identifier chaque état d'un objet avec un numéro distinct.
-
-Par exemple, nous voulons calculer le nombre de pages visitées sur le site et combien de temps ils étaient là. À un moment donné nous écrivons la ligne suivante avec l'état de l'activité de l'utilisateur:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-À un moment donné, nous enregistrons le changement d'activité de l'utilisateur et l'écrivons avec les deux lignes suivantes.
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 |
-│ 4324182021466249494 │         6 │      185 │    1 │       2 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-La première ligne annule le précédent état de l'objet (utilisateur). Il doit copier tous les champs de l'état annulé sauf `Sign`.
-
-La deuxième ligne contient l'état actuel.
-
-Parce que nous avons besoin seulement le dernier état de l'activité de l'utilisateur, les lignes
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 |
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-peut être supprimé, réduisant l'état invalide (ancien) de l'objet. `VersionedCollapsingMergeTree` fait cela lors de la fusion des parties de données.
-
-Pour savoir pourquoi nous avons besoin de deux lignes pour chaque changement, voir [Algorithme](#table_engines-versionedcollapsingmergetree-algorithm).
-
-**Notes sur l'Utilisation de la**
-
-1.  Le programme qui écrit les données devraient se souvenir de l'état d'un objet afin de l'annuler. Le “cancel” chaîne doit être une copie de la “state” chaîne avec le contraire `Sign`. Cela augmente la taille initiale de stockage, mais permet d'écrire les données rapidement.
-2.  Les tableaux de plus en plus longs dans les colonnes réduisent l'efficacité du moteur en raison de la charge d'écriture. Plus les données sont simples, meilleure est l'efficacité.
-3.  `SELECT` les résultats dépendent fortement de la cohérence de l'histoire de l'objet change. Être précis lors de la préparation des données pour l'insertion. Vous pouvez obtenir des résultats imprévisibles avec des données incohérentes, telles que des valeurs négatives pour des métriques non négatives telles que la profondeur de session.
-
-### Algorithme {#table_engines-versionedcollapsingmergetree-algorithm}
-
-Lorsque ClickHouse fusionne des parties de données, il supprime chaque paire de lignes ayant la même clé primaire et la même version et différentes `Sign`. L'ordre des lignes n'a pas d'importance.
-
-Lorsque ClickHouse insère des données, il ordonne les lignes par la clé primaire. Si l' `Version` la colonne n'est pas dans la clé primaire, ClickHouse ajoute à la clé primaire implicitement que le dernier champ et l'utilise pour la commande.
-
-## La Sélection De Données {#selecting-data}
-
-ClickHouse ne garantit pas que toutes les lignes avec la même clé primaire sera dans la même partie des données ou même sur le même serveur physique. Cela est vrai à la fois pour l'écriture des données et pour la fusion ultérieure des parties de données. En outre, les processus ClickHouse `SELECT` requêtes avec plusieurs threads, et il ne peut pas prédire l'ordre des lignes dans le résultat. Cela signifie que le regroupement est nécessaire s'il est nécessaire pour obtenir complètement “collapsed” données à partir d'un `VersionedCollapsingMergeTree` table.
-
-Pour finaliser la réduction, écrivez une requête avec un `GROUP BY` fonctions de clause et d'agrégation qui tiennent compte du signe. Par exemple, pour calculer la quantité, l'utilisation `sum(Sign)` plutôt `count()`. Pour calculer la somme de quelque chose, utilisez `sum(Sign * x)` plutôt `sum(x)` et d'ajouter `HAVING sum(Sign) > 0`.
-
-Aggregate `count`, `sum` et `avg` peut être calculée de cette manière. Aggregate `uniq` peut être calculé si un objet a au moins un non-état effondré. Aggregate `min` et `max` ne peut pas être calculé, car `VersionedCollapsingMergeTree` ne sauvegarde pas l'historique des valeurs des États réduits.
-
-Si vous avez besoin d'extraire les données avec “collapsing” mais sans agrégation (par exemple, pour vérifier si des lignes sont présentes dont les valeurs les plus récentes correspondent à certaines conditions), vous pouvez utiliser `FINAL` le modificateur du `FROM` clause. Cette approche est inefficace et ne devrait pas être utilisée avec de grandes tables.
-
-## Exemple D'utilisation {#example-of-use}
-
-Les données de l'exemple:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 |
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 |
-│ 4324182021466249494 │         6 │      185 │    1 │       2 |
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-Création de la table:
-
-``` sql
-CREATE TABLE UAct
-(
-    UserID UInt64,
-    PageViews UInt8,
-    Duration UInt8,
-    Sign Int8,
-    Version UInt8
-)
-ENGINE = VersionedCollapsingMergeTree(Sign, Version)
-ORDER BY UserID
-```
-
-Insérer les données:
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1, 1)
-```
-
-``` sql
-INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1, 1),(4324182021466249494, 6, 185, 1, 2)
-```
-
-Nous utilisons deux `INSERT` requêtes pour créer deux parties de données différentes. Si nous insérons les données avec une seule requête, ClickHouse crée une partie de données et n'effectuera jamais de fusion.
-
-L'obtention de données:
-
-``` sql
-SELECT * FROM UAct
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │    1 │       1 │
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │       1 │
-│ 4324182021466249494 │         6 │      185 │    1 │       2 │
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-Que voyons-nous ici et où sont les parties effondrées?
-Nous avons créé deux parties de données en utilisant deux `INSERT` requête. Le `SELECT` la requête a été effectuée dans deux threads, et le résultat est un ordre aléatoire de lignes.
-L'effondrement n'a pas eu lieu car les parties de données n'ont pas encore été fusionnées. ClickHouse fusionne des parties de données à un moment inconnu que nous ne pouvons pas prédire.
-
-C'est pourquoi nous avons besoin de l'agrégation:
-
-``` sql
-SELECT
-    UserID,
-    sum(PageViews * Sign) AS PageViews,
-    sum(Duration * Sign) AS Duration,
-    Version
-FROM UAct
-GROUP BY UserID, Version
-HAVING sum(Sign) > 0
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Version─┐
-│ 4324182021466249494 │         6 │      185 │       2 │
-└─────────────────────┴───────────┴──────────┴─────────┘
-```
-
-Si nous n'avons pas besoin d'agrégation et que nous voulons forcer l'effondrement, nous pouvons utiliser le `FINAL` le modificateur du `FROM` clause.
-
-``` sql
-SELECT * FROM UAct FINAL
-```
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐
-│ 4324182021466249494 │         6 │      185 │    1 │       2 │
-└─────────────────────┴───────────┴──────────┴──────┴─────────┘
-```
-
-C'est un moyen très inefficace de sélectionner des données. Ne l'utilisez pas pour les grandes tables.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/versionedcollapsingmergetree/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/buffer.md b/docs/fr/engines/table-engines/special/buffer.md
deleted file mode 100644
index 22fde2d614ec..000000000000
--- a/docs/fr/engines/table-engines/special/buffer.md
+++ /dev/null
@@ -1,71 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: Tampon
----
-
-# Tampon {#buffer}
-
-Met en mémoire tampon les données à écrire dans la RAM, les vidant périodiquement dans une autre table. Pendant l'opération de lecture, les données sont lues à partir de la mémoire tampon, et l'autre simultanément.
-
-``` sql
-Buffer(database, table, num_layers, min_time, max_time, min_rows, max_rows, min_bytes, max_bytes)
-```
-
-Les paramètres du moteur:
-
--   `database` – Database name. Instead of the database name, you can use a constant expression that returns a string.
--   `table` – Table to flush data to.
--   `num_layers` – Parallelism layer. Physically, the table will be represented as `num_layers` indépendant de tampons. Valeur recommandée: 16.
--   `min_time`, `max_time`, `min_rows`, `max_rows`, `min_bytes`, et `max_bytes` – Conditions for flushing data from the buffer.
-
-Les données sont vidées du tampon et écrites dans la table de destination si toutes les `min*` conditions ou au moins un `max*` conditions sont remplies.
-
--   `min_time`, `max_time` – Condition for the time in seconds from the moment of the first write to the buffer.
--   `min_rows`, `max_rows` – Condition for the number of rows in the buffer.
--   `min_bytes`, `max_bytes` – Condition for the number of bytes in the buffer.
-
-Pendant l'opération d'écriture, les données sont insérées dans un `num_layers` nombre aléatoire de tampons. Ou, si la partie de données à insérer est suffisamment grande (supérieure à `max_rows` ou `max_bytes`), il est écrit directement dans la table de destination, en omettant le tampon.
-
-Les conditions de purger les données sont calculées séparément pour chacun des `num_layers` tampon. Par exemple, si `num_layers = 16` et `max_bytes = 100000000`, la consommation maximale de RAM est de 1,6 Go.
-
-Exemple:
-
-``` sql
-CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10, 100, 10000, 1000000, 10000000, 100000000)
-```
-
-La création d'un ‘merge.hits_buffer’ table avec la même structure que ‘merge.hits’ et en utilisant le moteur tampon. Lors de l'écriture dans cette table, les données sont mises en mémoire tampon dans la RAM ‘merge.hits’ table. 16 tampons sont créés. Les données dans chacun d'entre eux est rincé si 100 secondes sont écoulées, ou un million de lignes ont été écrites, ou 100 MO de données ont été écrits; ou si, simultanément, 10 secondes et 10 000 lignes et 10 MO de données ont été écrites. Par exemple, si une ligne a été écrite, après 100 secondes, il sera vidé, n'importe quoi. Mais si plusieurs lignes ont été écrites, les données seront vidées plus tôt.
-
-Lorsque le serveur est arrêté, avec DROP TABLE ou DETACH TABLE, les données du tampon sont également vidées vers la table de destination.
-
-Vous pouvez définir des chaînes vides entre guillemets simples pour le nom de la base de données et de la table. Cela indique l'absence d'une table de destination. Dans ce cas, lorsque les conditions de vidage des données sont atteintes, le tampon est simplement effacé. Cela peut être utile pour garder une fenêtre de données dans la mémoire.
-
-Lors de la lecture à partir d'une table tampon, les données sont traitées à la fois à partir du tampon et de la table de destination (s'il y en a une).
-Notez que les tables de tampon ne prennent pas en charge un index. En d'autres termes, les données dans le tampon sont entièrement analysées, ce qui peut être lent pour les grands tampons. (Pour les données dans une table subordonnée, l'index qu'il prend en charge sera utilisé.)
-
-Si l'ensemble de colonnes de la table tampon ne correspond pas à l'ensemble de colonnes d'une table subordonnée, un sous-ensemble de colonnes existant dans les deux tables est inséré.
-
-Si les types ne correspondent pas à l'une des colonnes de la table tampon et à une table subordonnée, un message d'erreur est entré dans le journal du serveur et le tampon est effacé.
-La même chose se produit si la table subordonnée n'existe pas lorsque le tampon est vidé.
-
-Si vous devez exécuter ALTER pour une table subordonnée et la table tampon, nous vous recommandons de supprimer d'abord la table tampon, d'exécuter ALTER pour la table subordonnée, puis de créer à nouveau la table tampon.
-
-Si le serveur est redémarré anormalement, les données dans le tampon sont perdues.
-
-FINAL et SAMPLE ne fonctionnent pas correctement pour les tables tampon. Ces conditions sont transmises à la table de destination, mais ne sont pas utilisées pour traiter les données dans le tampon. Si ces fonctionnalités sont nécessaires, nous vous recommandons d'utiliser uniquement la table tampon pour l'écriture, lors de la lecture à partir de la table de destination.
-
-Lors de l'ajout de données à un Tampon, un des tampons est verrouillé. Cela entraîne des retards si une opération de lecture est effectuée simultanément à partir de la table.
-
-Les données insérées dans une table tampon peuvent se retrouver dans la table subordonnée dans un ordre différent et dans des blocs différents. Pour cette raison, une table tampon est difficile à utiliser pour écrire correctement dans un CollapsingMergeTree. Pour éviter les problèmes, vous pouvez définir ‘num_layers’ 1.
-
-Si la table de destination est répliquée, certaines caractéristiques attendues des tables répliquées sont perdues lors de l'écriture dans une table tampon. Les modifications aléatoires apportées à l'ordre des lignes et des tailles des parties de données provoquent l'arrêt de la déduplication des données, ce qui signifie qu'il n'est pas possible d'avoir un ‘exactly once’ Ecrire dans des tables répliquées.
-
-En raison de ces inconvénients, nous ne pouvons recommander l'utilisation d'une table tampon que dans de rares cas.
-
-Une table tampon est utilisée lorsque trop D'insertions sont reçues d'un grand nombre de serveurs sur une unité de temps et que les données ne peuvent pas être mises en mémoire tampon avant l'insertion, ce qui signifie que les insertions ne peuvent pas s'exécuter assez rapidement.
-
-Notez qu'il n'est pas judicieux d'insérer des données d'une ligne de temps, même pour Tampon tables. Cela ne produira qu'une vitesse de quelques milliers de lignes par seconde, tandis que l'insertion de blocs de données plus grands peut produire plus d'un million de lignes par seconde (voir la section “Performance”).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/buffer/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/dictionary.md b/docs/fr/engines/table-engines/special/dictionary.md
deleted file mode 100644
index 530d6510d519..000000000000
--- a/docs/fr/engines/table-engines/special/dictionary.md
+++ /dev/null
@@ -1,97 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: Dictionnaire
----
-
-# Dictionnaire {#dictionary}
-
-Le `Dictionary` le moteur affiche le [dictionnaire](../../../sql-reference/dictionaries/external-dictionaries/external-dicts.md) données comme une table ClickHouse.
-
-À titre d'exemple, considérons un dictionnaire de `products` avec la configuration suivante:
-
-``` xml
-<dictionaries>
-<dictionary>
-        <name>products</name>
-        <source>
-            <odbc>
-                <table>products</table>
-                <connection_string>DSN=some-db-server</connection_string>
-            </odbc>
-        </source>
-        <lifetime>
-            <min>300</min>
-            <max>360</max>
-        </lifetime>
-        <layout>
-            <flat/>
-        </layout>
-        <structure>
-            <id>
-                <name>product_id</name>
-            </id>
-            <attribute>
-                <name>title</name>
-                <type>String</type>
-                <null_value></null_value>
-            </attribute>
-        </structure>
-</dictionary>
-</dictionaries>
-```
-
-Interroger les données du dictionnaire:
-
-``` sql
-SELECT
-    name,
-    type,
-    key,
-    attribute.names,
-    attribute.types,
-    bytes_allocated,
-    element_count,
-    source
-FROM system.dictionaries
-WHERE name = 'products'
-```
-
-``` text
-┌─name─────┬─type─┬─key────┬─attribute.names─┬─attribute.types─┬─bytes_allocated─┬─element_count─┬─source──────────┐
-│ products │ Flat │ UInt64 │ ['title']       │ ['String']      │        23065376 │        175032 │ ODBC: .products │
-└──────────┴──────┴────────┴─────────────────┴─────────────────┴─────────────────┴───────────────┴─────────────────┘
-```
-
-Vous pouvez utiliser l' [dictGet\*](../../../sql-reference/functions/ext-dict-functions.md#ext_dict_functions) fonction pour obtenir les données du dictionnaire dans ce format.
-
-Cette vue n'est pas utile lorsque vous avez besoin d'obtenir des données brutes ou `JOIN` opération. Pour ces cas, vous pouvez utiliser le `Dictionary` moteur, qui affiche les données du dictionnaire dans une table.
-
-Syntaxe:
-
-``` sql
-CREATE TABLE %table_name% (%fields%) engine = Dictionary(%dictionary_name%)`
-```
-
-Exemple d'utilisation:
-
-``` sql
-create table products (product_id UInt64, title String) Engine = Dictionary(products);
-```
-
-      Ok
-
-Jetez un oeil à ce qui est dans le tableau.
-
-``` sql
-select * from products limit 1;
-```
-
-``` text
-┌────product_id─┬─title───────────┐
-│        152689 │ Some item       │
-└───────────────┴─────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/dictionary/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/distributed.md b/docs/fr/engines/table-engines/special/distributed.md
deleted file mode 100644
index b40253b1b482..000000000000
--- a/docs/fr/engines/table-engines/special/distributed.md
+++ /dev/null
@@ -1,152 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: "Distribu\xE9"
----
-
-# Distribué {#distributed}
-
-**Les Tables avec moteur distribué ne stockent aucune donnée par elles mêmes**, mais autoriser le traitement des requêtes distribuées sur plusieurs serveurs.
-La lecture est automatiquement parallélisée. Lors d'une lecture, les index de table sur les serveurs distants sont utilisés, s'il y en a.
-
-Le moteur distribué accepte les paramètres:
-
--   le nom du cluster dans le fichier de configuration du serveur
-
--   le nom d'une base de données distante
-
--   le nom d'une table distante
-
--   (en option) sharding clé
-
--   (éventuellement) nom de la stratégie, il sera utilisé pour stocker des fichiers temporaires pour l'envoi asynchrone
-
-    Voir aussi:
-
-    -   `insert_distributed_sync` paramètre
-    -   [MergeTree](../mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes) pour les exemples
-
-Exemple:
-
-``` sql
-Distributed(logs, default, hits[, sharding_key[, policy_name]])
-```
-
-Les données seront lues à partir de tous les serveurs ‘logs’ cluster, à partir de la valeur par défaut.hits table située sur chaque serveur du cluster.
-Les données ne sont pas seulement lues mais sont partiellement traitées sur les serveurs distants (dans la mesure du possible).
-Par exemple, pour une requête avec GROUP BY, les données seront agrégées sur des serveurs distants et les états intermédiaires des fonctions d'agrégation seront envoyés au serveur demandeur. Ensuite, les données seront plus agrégées.
-
-Au lieu du nom de la base de données, vous pouvez utiliser une expression constante qui renvoie une chaîne. Par exemple: currentDatabase().
-
-logs – The cluster name in the server's config file.
-
-Les Clusters sont définis comme ceci:
-
-``` xml
-<remote_servers>
-    <logs>
-        <shard>
-            <!-- Optional. Shard weight when writing data. Default: 1. -->
-            <weight>1</weight>
-            <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->
-            <internal_replication>false</internal_replication>
-            <replica>
-                <host>example01-01-1</host>
-                <port>9000</port>
-            </replica>
-            <replica>
-                <host>example01-01-2</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-        <shard>
-            <weight>2</weight>
-            <internal_replication>false</internal_replication>
-            <replica>
-                <host>example01-02-1</host>
-                <port>9000</port>
-            </replica>
-            <replica>
-                <host>example01-02-2</host>
-                <secure>1</secure>
-                <port>9440</port>
-            </replica>
-        </shard>
-    </logs>
-</remote_servers>
-```
-
-Ici un cluster est défini avec le nom ‘logs’ qui se compose de deux fragments, dont chacune contient deux répliques.
-Les partitions se réfèrent aux serveurs qui contiennent différentes parties des données (pour lire toutes les données, vous devez accéder à tous les partitions).
-Les répliques sont des serveurs de duplication (afin de lire toutes les données, vous pouvez accéder aux données sur l'une des répliques).
-
-Les noms de Cluster ne doivent pas contenir de points.
-
-Paramètre `host`, `port` et , éventuellement, `user`, `password`, `secure`, `compression` sont spécifiés pour chaque serveur:
-- `host` – The address of the remote server. You can use either the domain or the IPv4 or IPv6 address. If you specify the domain, the server makes a DNS request when it starts, and the result is stored as long as the server is running. If the DNS request fails, the server doesn't start. If you change the DNS record, restart the server.
-- `port` – The TCP port for messenger activity (‘tcp_port’ dans la configuration, généralement définie sur 9000). Ne le confondez pas avec http_port.
-- `user` – Name of the user for connecting to a remote server. Default value: default. This user must have access to connect to the specified server. Access is configured in the users.xml file. For more information, see the section [Les droits d'accès](../../../operations/access-rights.md).
-- `password` – The password for connecting to a remote server (not masked). Default value: empty string.
-- `secure` - Utilisez ssl pour la connexion, généralement vous devez également définir `port` = 9440. Le serveur doit écouter `<tcp_port_secure>9440</tcp_port_secure>` et avoir des certificats corrects.
-- `compression` - Utiliser la compression de données. Valeur par défaut: true.
-
-When specifying replicas, one of the available replicas will be selected for each of the shards when reading. You can configure the algorithm for load balancing (the preference for which replica to access) – see the [équilibrage](../../../operations/settings/settings.md#settings-load_balancing) paramètre.
-Si la connexion avec le serveur n'est pas établie, il y aura une tentative de connexion avec un court délai. Si la connexion échoue, la réplique suivante sera sélectionnée, et ainsi de suite pour toutes les répliques. Si la tentative de connexion a échoué pour toutes les répliques, la tentative sera répété de la même façon, plusieurs fois.
-Cela fonctionne en faveur de la résilience, mais ne fournit pas de tolérance aux pannes complète: un serveur distant peut accepter la connexion, mais peut ne pas fonctionner ou fonctionner mal.
-
-Vous pouvez spécifier un seul des fragments (dans ce cas, le traitement de la requête doit être appelé distant, plutôt que distribué) ou jusqu'à un nombre quelconque de fragments. Dans chaque fragment, vous pouvez spécifier un nombre de répliques. Vous pouvez spécifier un nombre différent de répliques pour chaque fragment.
-
-Vous pouvez spécifier autant de clusters que vous souhaitez dans la configuration.
-
-Pour afficher vos clusters, utilisez ‘system.clusters’ table.
-
-Le moteur distribué permet de travailler avec un cluster comme un serveur local. Cependant, le cluster est inextensible: vous devez écrire sa configuration dans le fichier de configuration du serveur (encore mieux, pour tous les serveurs du cluster).
-
-The Distributed engine requires writing clusters to the config file. Clusters from the config file are updated on the fly, without restarting the server. If you need to send a query to an unknown set of shards and replicas each time, you don't need to create a Distributed table – use the ‘remote’ fonction de table à la place. Voir la section [Les fonctions de Table](../../../sql-reference/table-functions/index.md).
-
-Il existe deux méthodes pour écrire des données dans un cluster:
-
-Tout d'abord, vous pouvez définir les serveurs d'écrire les données à et effectuer l'écriture directement sur chaque fragment. En d'autres termes, effectuez INSERT dans les tables que la table distribuée “looks at”. C'est la solution la plus flexible car vous pouvez utiliser n'importe quel schéma de sharding, qui pourrait être non trivial en raison des exigences du sujet. C'est également la solution la plus optimale puisque les données peuvent être écrites sur différents fragments de manière complètement indépendante.
-
-Deuxièmement, vous pouvez effectuer INSERT dans une table distribuée. Dans ce cas, la table distribuera les données insérées sur les serveurs eux-mêmes. Pour écrire dans une table distribuée, elle doit avoir un jeu de clés de sharding (le dernier paramètre). De plus, s'il n'y a qu'un seul fragment, l'opération d'écriture fonctionne sans spécifier la clé de sharding, car cela ne signifie rien dans ce cas.
-
-Chaque fragment peut avoir un poids défini dans le fichier de configuration. Par défaut, le poids est égal à un. Les données sont réparties entre les fragments dans la quantité proportionnelle au poids des fragments. Par exemple, si il y a deux tessons et le premier a un poids de 9 tandis que la seconde a un poids de 10, le premier sera envoyé 9 / 19 parties de lignes, et le second sera envoyé 10 / 19.
-
-Chaque fragment peut avoir le ‘internal_replication’ paramètre défini dans le fichier de configuration.
-
-Si ce paramètre est défini à ‘true’, l'opération d'écriture sélectionne le premier saine réplique et écrit les données. Utilisez cette option si le tableau Distribué “looks at” tables répliquées. En d'autres termes, si la table où les données seront écrites va répliquer elle-même.
-
-Si elle est définie sur ‘false’ (par défaut), les données sont écrites dans toutes les répliques. En substance, cela signifie que la table distribuée réplique les données elle-même. C'est pire que d'utiliser des tables répliquées, car la cohérence des répliques n'est pas vérifiée et, au fil du temps, elles contiendront des données légèrement différentes.
-
-Pour sélectionner le fragment auquel une ligne de données est envoyée, l'expression de sharding est analysée et son reste est extrait de la diviser par le poids total des fragments. La ligne est envoyée au fragment qui correspond au demi-intervalle des restes de ‘prev_weight’ de ‘prev_weights + weight’, où ‘prev_weights’ c'est le poids total des tessons avec le plus petit nombre, et ‘weight’ est le poids de cet éclat. Par exemple, s'il y a deux fragments, et que le premier a un poids de 9 tandis que le second a un poids de 10, la ligne sera envoyée au premier fragment pour les restes de la plage \[0, 9), et au second pour les restes de la plage \[9, 19).
-
-L'expression de sharding peut être n'importe quelle expression de constantes et de colonnes de table qui renvoie un entier. Par exemple, vous pouvez utiliser l'expression ‘rand()’ pour la distribution aléatoire des données, ou ‘UserID’ pour la distribution par le reste de la division de L'ID de l'utilisateur (alors les données d'un seul utilisateur résideront sur un seul fragment, ce qui simplifie l'exécution et la jointure par les utilisateurs). Si l'une des colonnes n'est pas assez répartie uniformément, vous pouvez l'envelopper dans une fonction de hachage: intHash64 (UserID).
-
-Un simple rappel de la division est une solution limitée pour le sharding et n'est pas toujours approprié. Cela fonctionne pour des volumes de données moyens et importants (des dizaines de serveurs), mais pas pour des volumes de données très importants (des centaines de serveurs ou plus). Dans ce dernier cas, utilisez le schéma de répartition requis par le domaine, plutôt que d'utiliser des entrées dans des tableaux distribués.
-
-SELECT queries are sent to all the shards and work regardless of how data is distributed across the shards (they can be distributed completely randomly). When you add a new shard, you don't have to transfer the old data to it. You can write new data with a heavier weight – the data will be distributed slightly unevenly, but queries will work correctly and efficiently.
-
-Vous devriez être préoccupé par le système de sharding dans les cas suivants:
-
--   Les requêtes sont utilisées qui nécessitent des données de jointure (IN ou JOIN) par une clé spécifique. Si les données sont partagées par cette clé, vous pouvez utiliser local in ou JOIN au lieu de GLOBAL IN ou global JOIN, ce qui est beaucoup plus efficace.
--   Un grand nombre de serveurs est utilisé (des centaines ou plus) avec un grand nombre de petites requêtes (requêtes de clients individuels - sites Web, annonceurs ou partenaires). Pour que les petites requêtes n'affectent pas l'ensemble du cluster, il est logique de localiser les données d'un seul client sur un seul fragment. Alternativement, comme nous l'avons fait dans Yandex.Metrica, vous pouvez configurer le sharding à deux niveaux: divisez le cluster entier en “layers”, où une couche peut être constituée de plusieurs éclats. Les données d'un seul client sont situées sur une seule couche, mais des fragments peuvent être ajoutés à une couche si nécessaire, et les données sont distribuées aléatoirement à l'intérieur de celles-ci. Des tables distribuées sont créées pour chaque couche et une seule table distribuée partagée est créée pour les requêtes globales.
-
-Les données sont écrites de manière asynchrone. Lorsqu'il est inséré dans la table, le bloc de données est simplement écrit dans le système de fichiers local. Les données sont envoyées aux serveurs distants en arrière-plan dès que possible. La période d'envoi des données est gérée par [distributed_directory_monitor_sleep_time_ms](../../../operations/settings/settings.md#distributed_directory_monitor_sleep_time_ms) et [distributed_directory_monitor_max_sleep_time_ms](../../../operations/settings/settings.md#distributed_directory_monitor_max_sleep_time_ms) paramètre. Le `Distributed` moteur envoie chaque fichier de données insérées séparément, mais vous pouvez activer le lot envoi de fichiers avec l' [distributed_directory_monitor_batch_inserts](../../../operations/settings/settings.md#distributed_directory_monitor_batch_inserts) paramètre. Ce paramètre améliore les performances du cluster en utilisant mieux les ressources réseau et serveur local. Vous devriez vérifier si les données sont envoyées avec succès en vérifiant la liste des fichiers (données en attente d'envoi) dans le répertoire de la table: `/var/lib/clickhouse/data/database/table/`.
-
-Si le serveur a cessé d'exister ou a subi un redémarrage Brutal (par exemple, après une panne de périphérique) après une insertion dans une table distribuée, les données insérées peuvent être perdues. Si une partie de données endommagée est détectée dans le répertoire de la table, elle est transférée ‘broken’ sous-répertoire et n'est plus utilisé.
-
-Lorsque l'option max_parallel_replicas est activée, le traitement des requêtes est parallélisé entre toutes les répliques d'un seul fragment. Pour plus d'informations, consultez la section [max_parallel_replicas](../../../operations/settings/settings.md#settings-max_parallel_replicas).
-
-## Les Colonnes Virtuelles {#virtual-columns}
-
--   `_shard_num` — Contains the `shard_num` (de `system.clusters`). Type: [UInt32](../../../sql-reference/data-types/int-uint.md).
-
-!!! note "Note"
-    Depuis [`remote`](../../../sql-reference/table-functions/remote.md)/`cluster` les fonctions de table créent en interne une instance temporaire du même moteur distribué, `_shard_num` est disponible là-bas aussi.
-
-**Voir Aussi**
-
--   [Les colonnes virtuelles](index.md#table_engines-virtual_columns)
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/distributed/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/external-data.md b/docs/fr/engines/table-engines/special/external-data.md
deleted file mode 100644
index df867a232150..000000000000
--- a/docs/fr/engines/table-engines/special/external-data.md
+++ /dev/null
@@ -1,68 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: "De donn\xE9es externes"
----
-
-# Données externes pour le traitement des requêtes {#external-data-for-query-processing}
-
-ClickHouse permet d'Envoyer à un serveur les données nécessaires au traitement d'une requête, ainsi qu'une requête SELECT. Ces données sont placées dans une table temporaire (voir la section “Temporary tables”) et peut être utilisé dans la requête (par exemple, dans les DANS les opérateurs).
-
-Par exemple, si vous disposez d'un fichier texte avec important des identifiants d'utilisateur, vous pouvez le télécharger sur le serveur avec une requête qui utilise la filtration par cette liste.
-
-Si vous devez exécuter plusieurs requêtes avec un volume important de données externes, n'utilisez pas cette fonctionnalité. Il est préférable de télécharger les données sur la base de données à l'avance.
-
-Les données externes peuvent être téléchargées à l'aide du client de ligne de commande (en mode non interactif) ou à l'aide de L'interface HTTP.
-
-Dans le client de ligne de commande, vous pouvez spécifier une section paramètres du format
-
-``` bash
---external --file=... [--name=...] [--format=...] [--types=...|--structure=...]
-```
-
-Vous pouvez avoir plusieurs sections comme ça, pour le nombre de tables étant transmis.
-
-**–external** – Marks the beginning of a clause.
-**–file** – Path to the file with the table dump, or -, which refers to stdin.
-Une seule table peut être récupérée à partir de stdin.
-
-Les paramètres suivants sont facultatifs: **–name**– Name of the table. If omitted, _data is used.
-**–format** – Data format in the file. If omitted, TabSeparated is used.
-
-L'un des paramètres suivants est requis:**–types** – A list of comma-separated column types. For example: `UInt64,String`. The columns will be named _1, _2, …
-**–structure**– The table structure in the format`UserID UInt64`, `URL String`. Définit les noms et les types de colonnes.
-
-Les fichiers spécifiés dans ‘file’ sera analysé par le format spécifié dans ‘format’, en utilisant les types de données spécifié dans ‘types’ ou ‘structure’. La table sera téléchargée sur le serveur et accessible en tant que table temporaire avec le nom dans ‘name’.
-
-Exemple:
-
-``` bash
-$ echo -ne "1
2
3
" | clickhouse-client --query="SELECT count() FROM test.visits WHERE TraficSourceID IN _data" --external --file=- --types=Int8
-849897
-$ cat /etc/passwd | sed 's/:/\t/g' | clickhouse-client --query="SELECT shell, count() AS c FROM passwd GROUP BY shell ORDER BY c DESC" --external --file=- --name=passwd --structure='login String, unused String, uid UInt16, gid UInt16, comment String, home String, shell String'
-/bin/sh 20
-/bin/false      5
-/bin/bash       4
-/usr/sbin/nologin       1
-/bin/sync       1
-```
-
-Lors de l'utilisation de L'interface HTTP, les données externes sont transmises au format multipart/form-data. Chaque tableau est transmis en tant que fichier séparé. Le nom de la table est tiré du nom du fichier. Le ‘query_string’ est passé les paramètres ‘name_format’, ‘name_types’, et ‘name_structure’, où ‘name’ est le nom de la table que ces paramètres correspondent. La signification des paramètres est la même que lors de l'utilisation du client de ligne de commande.
-
-Exemple:
-
-``` bash
-$ cat /etc/passwd | sed 's/:/\t/g' > passwd.tsv
-
-$ curl -F 'passwd=@passwd.tsv;' 'http://localhost:8123/?query=SELECT+shell,+count()+AS+c+FROM+passwd+GROUP+BY+shell+ORDER+BY+c+DESC&passwd_structure=login+String,+unused+String,+uid+UInt16,+gid+UInt16,+comment+String,+home+String,+shell+String'
-/bin/sh 20
-/bin/false      5
-/bin/bash       4
-/usr/sbin/nologin       1
-/bin/sync       1
-```
-
-Pour le traitement des requêtes distribuées, les tables temporaires sont envoyées à tous les serveurs distants.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/external_data/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/file.md b/docs/fr/engines/table-engines/special/file.md
deleted file mode 100644
index eedc12065276..000000000000
--- a/docs/fr/engines/table-engines/special/file.md
+++ /dev/null
@@ -1,90 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: Fichier
----
-
-# Fichier {#table_engines-file}
-
-Le moteur de table de fichiers conserve les données dans un fichier dans l'un des [fichier
-format](../../../interfaces/formats.md#formats) (TabSeparated, Native, etc.).
-
-Exemples d'utilisation:
-
--   Exportation de données de ClickHouse vers un fichier.
--   Convertir des données d'un format à un autre.
--   Mise à jour des données dans ClickHouse via l'édition d'un fichier sur un disque.
-
-## Utilisation dans le serveur ClickHouse {#usage-in-clickhouse-server}
-
-``` sql
-File(Format)
-```
-
-Le `Format` paramètre spécifie l'un des formats de fichier disponibles. Effectuer
-`SELECT` requêtes, le format doit être pris en charge pour l'entrée, et à effectuer
-`INSERT` queries – for output. The available formats are listed in the
-[Format](../../../interfaces/formats.md#formats) section.
-
-ClickHouse ne permet pas de spécifier le chemin du système de fichiers pour`File`. Il utilisera le dossier défini par [chemin](../../../operations/server-configuration-parameters/settings.md) réglage dans la configuration du serveur.
-
-Lors de la création de la table en utilisant `File(Format)` il crée sous-répertoire vide dans ce dossier. Lorsque les données sont écrites dans cette table, elles sont mises dans `data.Format` fichier dans ce répertoire.
-
-Vous pouvez créer manuellement ce sous dossier et ce fichier dans le système de fichiers [ATTACH](../../../sql-reference/statements/misc.md) il à la table des informations avec le nom correspondant, de sorte que vous pouvez interroger les données de ce fichier.
-
-!!! warning "Avertissement"
-    Soyez prudent avec cette fonctionnalité, car ClickHouse ne garde pas trace des modifications externes apportées à ces fichiers. Le résultat des Écritures simultanées via ClickHouse et en dehors de ClickHouse n'est pas défini.
-
-**Exemple:**
-
-**1.** Configurer le `file_engine_table` table:
-
-``` sql
-CREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(TabSeparated)
-```
-
-Par défaut ClickHouse va créer un dossier `/var/lib/clickhouse/data/default/file_engine_table`.
-
-**2.** Créer manuellement `/var/lib/clickhouse/data/default/file_engine_table/data.TabSeparated` contenant:
-
-``` bash
-$ cat data.TabSeparated
-one 1
-two 2
-```
-
-**3.** Interroger les données:
-
-``` sql
-SELECT * FROM file_engine_table
-```
-
-``` text
-┌─name─┬─value─┐
-│ one  │     1 │
-│ two  │     2 │
-└──────┴───────┘
-```
-
-## Utilisation dans ClickHouse-local {#usage-in-clickhouse-local}
-
-Dans [clickhouse-local](../../../operations/utilities/clickhouse-local.md) Fichier moteur accepte chemin d'accès au fichier en plus `Format`. Les flux d'entrée / sortie par défaut peuvent être spécifiés en utilisant des noms numériques ou lisibles par l'homme comme `0` ou `stdin`, `1` ou `stdout`.
-**Exemple:**
-
-``` bash
-$ echo -e "1,2
3,4" | clickhouse-local -q "CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table"
-```
-
-## Les détails de mise en Œuvre {#details-of-implementation}
-
--   Plusieurs `SELECT` les requêtes peuvent être effectuées simultanément, mais `INSERT` requêtes s'attendre les uns les autres.
--   Prise en charge de la création d'un nouveau fichier par `INSERT` requête.
--   Si le fichier existe, `INSERT` ajouterait de nouvelles valeurs dedans.
--   Pas pris en charge:
-    -   `ALTER`
-    -   `SELECT ... SAMPLE`
-    -   Index
-    -   Réplication
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/file/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/generate.md b/docs/fr/engines/table-engines/special/generate.md
deleted file mode 100644
index 9b2aa57c3e58..000000000000
--- a/docs/fr/engines/table-engines/special/generate.md
+++ /dev/null
@@ -1,61 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: GenerateRandom
----
-
-# Generaterandom {#table_engines-generate}
-
-Le moteur de table GenerateRandom produit des données aléatoires pour un schéma de table donné.
-
-Exemples d'utilisation:
-
--   Utiliser dans le test pour remplir une grande table reproductible.
--   Générer une entrée aléatoire pour les tests de fuzzing.
-
-## Utilisation dans le serveur ClickHouse {#usage-in-clickhouse-server}
-
-``` sql
-ENGINE = GenerateRandom(random_seed, max_string_length, max_array_length)
-```
-
-Le `max_array_length` et `max_string_length` les paramètres spécifient la longueur maximale de tous
-colonnes et chaînes de tableau en conséquence dans les données générées.
-
-Générer le moteur de table prend en charge uniquement `SELECT` requête.
-
-Il prend en charge tous les [Les types de données](../../../sql-reference/data-types/index.md) cela peut être stocké dans une table sauf `LowCardinality` et `AggregateFunction`.
-
-**Exemple:**
-
-**1.** Configurer le `generate_engine_table` table:
-
-``` sql
-CREATE TABLE generate_engine_table (name String, value UInt32) ENGINE = GenerateRandom(1, 5, 3)
-```
-
-**2.** Interroger les données:
-
-``` sql
-SELECT * FROM generate_engine_table LIMIT 3
-```
-
-``` text
-┌─name─┬──────value─┐
-│ c4xJ │ 1412771199 │
-│ r    │ 1791099446 │
-│ 7#$  │  124312908 │
-└──────┴────────────┘
-```
-
-## Les détails de mise en Œuvre {#details-of-implementation}
-
--   Pas pris en charge:
-    -   `ALTER`
-    -   `SELECT ... SAMPLE`
-    -   `INSERT`
-    -   Index
-    -   Réplication
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/generate/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/index.md b/docs/fr/engines/table-engines/special/index.md
deleted file mode 100644
index 424a80c511d0..000000000000
--- a/docs/fr/engines/table-engines/special/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Sp\xE9cial"
-toc_priority: 31
----
-
-
diff --git a/docs/fr/engines/table-engines/special/join.md b/docs/fr/engines/table-engines/special/join.md
deleted file mode 100644
index bc10be5b9bbd..000000000000
--- a/docs/fr/engines/table-engines/special/join.md
+++ /dev/null
@@ -1,111 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: Rejoindre
----
-
-# Rejoindre {#join}
-
-Structure de données préparée pour l'utilisation dans [JOIN](../../../sql-reference/statements/select/join.md#select-join) opérations.
-
-## Création d'une Table {#creating-a-table}
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2],
-) ENGINE = Join(join_strictness, join_type, k1[, k2, ...])
-```
-
-Voir la description détaillée de la [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query) requête.
-
-**Les Paramètres Du Moteur**
-
--   `join_strictness` – [ADHÉRER à la rigueur](../../../sql-reference/statements/select/join.md#select-join-types).
--   `join_type` – [Type de jointure](../../../sql-reference/statements/select/join.md#select-join-types).
--   `k1[, k2, ...]` – Key columns from the `USING` la clause que l' `JOIN` l'opération est faite avec de la.
-
-Entrer `join_strictness` et `join_type` paramètres sans guillemets, par exemple, `Join(ANY, LEFT, col1)`. Ils doivent correspondre à la `JOIN` fonctionnement que le tableau sera utilisé pour. Si les paramètres ne correspondent pas, ClickHouse ne lance pas d'exception et peut renvoyer des données incorrectes.
-
-## Utilisation Du Tableau {#table-usage}
-
-### Exemple {#example}
-
-Création de la table de gauche:
-
-``` sql
-CREATE TABLE id_val(`id` UInt32, `val` UInt32) ENGINE = TinyLog
-```
-
-``` sql
-INSERT INTO id_val VALUES (1,11)(2,12)(3,13)
-```
-
-Création du côté droit `Join` table:
-
-``` sql
-CREATE TABLE id_val_join(`id` UInt32, `val` UInt8) ENGINE = Join(ANY, LEFT, id)
-```
-
-``` sql
-INSERT INTO id_val_join VALUES (1,21)(1,22)(3,23)
-```
-
-Rejoindre les tables:
-
-``` sql
-SELECT * FROM id_val ANY LEFT JOIN id_val_join USING (id) SETTINGS join_use_nulls = 1
-```
-
-``` text
-┌─id─┬─val─┬─id_val_join.val─┐
-│  1 │  11 │              21 │
-│  2 │  12 │            ᴺᵁᴸᴸ │
-│  3 │  13 │              23 │
-└────┴─────┴─────────────────┘
-```
-
-Comme alternative, vous pouvez récupérer des données de la `Join` table, spécifiant la valeur de la clé de jointure:
-
-``` sql
-SELECT joinGet('id_val_join', 'val', toUInt32(1))
-```
-
-``` text
-┌─joinGet('id_val_join', 'val', toUInt32(1))─┐
-│                                         21 │
-└────────────────────────────────────────────┘
-```
-
-### Sélection et insertion de données {#selecting-and-inserting-data}
-
-Vous pouvez utiliser `INSERT` requêtes pour ajouter des données au `Join`-tables de moteur. Si la table a été créée avec `ANY` rigueur, les données pour les clés en double sont ignorées. Avec l' `ALL` rigueur, toutes les lignes sont ajoutées.
-
-Vous ne pouvez pas effectuer un `SELECT` requête directement à partir de la table. Au lieu de cela, utilisez l'une des méthodes suivantes:
-
--   Placez la table sur le côté droit dans un `JOIN` clause.
--   Appelez le [joinGet](../../../sql-reference/functions/other-functions.md#joinget) fonction, qui vous permet d'extraire des données de la table de la même manière que d'un dictionnaire.
-
-### Limitations et paramètres {#join-limitations-and-settings}
-
-Lors de la création d'un tableau, les paramètres suivants sont appliqués:
-
--   [join_use_nulls](../../../operations/settings/settings.md#join_use_nulls)
--   [max_rows_in_join](../../../operations/settings/query-complexity.md#settings-max_rows_in_join)
--   [max_bytes_in_join](../../../operations/settings/query-complexity.md#settings-max_bytes_in_join)
--   [join_overflow_mode](../../../operations/settings/query-complexity.md#settings-join_overflow_mode)
--   [join_any_take_last_row](../../../operations/settings/settings.md#settings-join_any_take_last_row)
-
-Le `Join`- les tables de moteur ne peuvent pas être utilisées dans `GLOBAL JOIN` opérations.
-
-Le `Join`-moteur permet d'utiliser [join_use_nulls](../../../operations/settings/settings.md#join_use_nulls) réglage de la `CREATE TABLE` déclaration. Et [SELECT](../../../sql-reference/statements/select/index.md) requête permet d'utiliser `join_use_nulls` trop. Si vous avez différents `join_use_nulls` paramètres, vous pouvez obtenir une table de jointure d'erreur. Il dépend de type de JOINTURE. Lorsque vous utilisez [joinGet](../../../sql-reference/functions/other-functions.md#joinget) fonction, vous devez utiliser le même `join_use_nulls` réglage en `CRATE TABLE` et `SELECT` déclaration.
-
-## Le Stockage De Données {#data-storage}
-
-`Join` les données de la table sont toujours situées dans la RAM. Lors de l'insertion de lignes dans une table, ClickHouse écrit des blocs de données dans le répertoire du disque afin qu'ils puissent être restaurés lorsque le serveur redémarre.
-
-Si le serveur redémarre incorrectement, le bloc de données sur le disque peut être perdu ou endommagé. Dans ce cas, vous devrez peut-être supprimer manuellement le fichier contenant des données endommagées.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/join/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/materializedview.md b/docs/fr/engines/table-engines/special/materializedview.md
deleted file mode 100644
index 700a95b4d65a..000000000000
--- a/docs/fr/engines/table-engines/special/materializedview.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: MaterializedView
----
-
-# Materializedview {#materializedview}
-
-Utilisé pour implémenter des vues matérialisées (pour plus d'informations, voir [CREATE TABLE](../../../sql-reference/statements/create.md#create-table-query)). Pour stocker des données, il utilise un moteur différent qui a été spécifié lors de la création de la vue. Lors de la lecture d'une table, il utilise juste ce moteur.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/materializedview/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/memory.md b/docs/fr/engines/table-engines/special/memory.md
deleted file mode 100644
index 94e619eae073..000000000000
--- a/docs/fr/engines/table-engines/special/memory.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: "M\xE9moire"
----
-
-# Mémoire {#memory}
-
-Le moteur de mémoire stocke les données en RAM, sous forme non compressée. Les données sont stockées exactement sous la même forme qu'elles sont reçues lors de la lecture. En d'autres termes, la lecture de ce tableau est entièrement gratuit.
-L'accès aux données simultanées est synchronisé. Les verrous sont courts: les opérations de lecture et d'écriture ne se bloquent pas.
-Les index ne sont pas pris en charge. La lecture est parallélisée.
-La productivité maximale (plus de 10 Go / s) est atteinte sur les requêtes simples, car il n'y a pas de lecture à partir du disque, de décompression ou de désérialisation des données. (Il convient de noter que dans de nombreux cas, la productivité du moteur MergeTree est presque aussi élevée.)
-Lors du redémarrage d'un serveur, les données disparaissent de la table et la table devient vide.
-Normalement, l'utilisation de ce moteur de table n'est pas justifiée. Cependant, il peut être utilisé pour des tests, et pour des tâches où la vitesse maximale est requise sur un nombre relativement faible de lignes (jusqu'à environ 100 000 000).
-
-Le moteur de mémoire est utilisé par le système pour les tables temporaires avec des données de requête externes (voir la section “External data for processing a query”), et pour la mise en œuvre globale dans (voir la section “IN operators”).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/memory/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/merge.md b/docs/fr/engines/table-engines/special/merge.md
deleted file mode 100644
index 6b41e4901135..000000000000
--- a/docs/fr/engines/table-engines/special/merge.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: Fusionner
----
-
-# Fusionner {#merge}
-
-Le `Merge` moteur (à ne pas confondre avec `MergeTree`) ne stocke pas les données elles-mêmes, mais permet la lecture de n'importe quel nombre d'autres tables simultanément.
-La lecture est automatiquement parallélisée. L'écriture dans une table n'est pas prise en charge. Lors de la lecture, les index des tables en cours de lecture sont utilisés, s'ils existent.
-Le `Merge` engine accepte les paramètres: le nom de la base de données et une expression régulière pour les tables.
-
-Exemple:
-
-``` sql
-Merge(hits, '^WatchLog')
-```
-
-Les données seront lues à partir des tableaux du `hits` base de données dont les noms correspondent à l'expression régulière ‘`^WatchLog`’.
-
-Au lieu du nom de la base de données, vous pouvez utiliser une expression constante qui renvoie une chaîne. Exemple, `currentDatabase()`.
-
-Regular expressions — [re2](https://github.com/google/re2) (prend en charge un sous-ensemble de PCRE), sensible à la casse.
-Voir les notes sur les symboles d'échappement dans les expressions régulières “match” section.
-
-Lors de la sélection des tables à lire, le `Merge` le tableau lui-même ne sera pas choisie, même si elle correspond à l'expression rationnelle. C'est pour éviter les boucles.
-Il est possible de créer deux `Merge` des tables qui essaieront sans cesse de lire les données des autres, mais ce n'est pas une bonne idée.
-
-L'utilisation traditionnelle de la `Merge` moteur pour travailler avec un grand nombre de `TinyLog` les tables comme si avec une seule table.
-
-Exemple 2:
-
-Disons que vous avez une ancienne table (WatchLog_old) et que vous avez décidé de changer de partitionnement sans déplacer les données vers une nouvelle table (WatchLog_new) et que vous devez voir les données des deux tables.
-
-``` sql
-CREATE TABLE WatchLog_old(date Date, UserId Int64, EventType String, Cnt UInt64)
-ENGINE=MergeTree(date, (UserId, EventType), 8192);
-INSERT INTO WatchLog_old VALUES ('2018-01-01', 1, 'hit', 3);
-
-CREATE TABLE WatchLog_new(date Date, UserId Int64, EventType String, Cnt UInt64)
-ENGINE=MergeTree PARTITION BY date ORDER BY (UserId, EventType) SETTINGS index_granularity=8192;
-INSERT INTO WatchLog_new VALUES ('2018-01-02', 2, 'hit', 3);
-
-CREATE TABLE WatchLog as WatchLog_old ENGINE=Merge(currentDatabase(), '^WatchLog');
-
-SELECT *
-FROM WatchLog
-```
-
-``` text
-┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐
-│ 2018-01-01 │      1 │ hit       │   3 │
-└────────────┴────────┴───────────┴─────┘
-┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐
-│ 2018-01-02 │      2 │ hit       │   3 │
-└────────────┴────────┴───────────┴─────┘
-```
-
-## Les Colonnes Virtuelles {#virtual-columns}
-
--   `_table` — Contains the name of the table from which data was read. Type: [Chaîne](../../../sql-reference/data-types/string.md).
-
-    Vous pouvez définir les conditions constantes sur `_table` dans le `WHERE/PREWHERE` clause (par exemple, `WHERE _table='xyz'`). Dans ce cas l'opération de lecture est effectuée uniquement pour les tables où la condition sur `_table` est satisfaite, pour le `_table` colonne agit comme un index.
-
-**Voir Aussi**
-
--   [Les colonnes virtuelles](index.md#table_engines-virtual_columns)
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/merge/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/null.md b/docs/fr/engines/table-engines/special/null.md
deleted file mode 100644
index 5215fc753be9..000000000000
--- a/docs/fr/engines/table-engines/special/null.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: 'NULL'
----
-
-# NULL {#null}
-
-Lors de l'écriture dans une table Null, Les données sont ignorées. Lors de la lecture à partir d'une table Null, la réponse est vide.
-
-Toutefois, vous pouvez créer une vue matérialisée sur une table Nuls. Ainsi, les données écrites dans la table finira dans la vue.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/null/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/set.md b/docs/fr/engines/table-engines/special/set.md
deleted file mode 100644
index 32bfde636980..000000000000
--- a/docs/fr/engines/table-engines/special/set.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: "D\xE9finir"
----
-
-# Définir {#set}
-
-Un ensemble de données qui est toujours en RAM. Il est conçu pour être utilisé sur le côté droit de l'opérateur (voir la section “IN operators”).
-
-Vous pouvez utiliser INSERT pour insérer des données dans la table. De nouveaux éléments seront ajoutés à l'ensemble de données, tandis que les doublons seront ignorés.
-Mais vous ne pouvez pas effectuer SELECT à partir de la table. La seule façon de récupérer des données est en l'utilisant dans la moitié droite de l'opérateur.
-
-Les données sont toujours situées dans la RAM. Pour INSERT, les blocs de données insérées sont également écrits dans le répertoire des tables sur le disque. Lors du démarrage du serveur, ces données sont chargées dans la RAM. En d'autres termes, après le redémarrage, les données restent en place.
-
-Pour un redémarrage brutal du serveur, le bloc de données sur le disque peut être perdu ou endommagé. Dans ce dernier cas, vous devrez peut-être supprimer manuellement le fichier contenant des données endommagées.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/set/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/url.md b/docs/fr/engines/table-engines/special/url.md
deleted file mode 100644
index 7b8e53b1d738..000000000000
--- a/docs/fr/engines/table-engines/special/url.md
+++ /dev/null
@@ -1,82 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: URL
----
-
-# URL (URL, Format) {#table_engines-url}
-
-Gère les données sur un serveur HTTP / HTTPS distant. Ce moteur est similaire
-à l' [Fichier](file.md) moteur.
-
-## Utilisation du moteur dans le serveur ClickHouse {#using-the-engine-in-the-clickhouse-server}
-
-Le `format` doit être celui que ClickHouse peut utiliser dans
-`SELECT` les requêtes et, si nécessaire, en `INSERTs`. Pour la liste complète des formats pris en charge, voir
-[Format](../../../interfaces/formats.md#formats).
-
-Le `URL` doit être conforme à la structure D'un Localisateur de ressources uniforme. L'URL spécifiée doit pointer vers un serveur
-qui utilise le protocole HTTP ou HTTPS. Cela ne nécessite pas de
-en-têtes supplémentaires pour obtenir une réponse du serveur.
-
-`INSERT` et `SELECT` les requêtes sont transformées en `POST` et `GET` demande,
-respectivement. Pour le traitement `POST` demandes, le serveur distant doit prendre en charge
-[Encodage de transfert en morceaux](https://en.wikipedia.org/wiki/Chunked_transfer_encoding).
-
-Vous pouvez limiter le nombre maximal de sauts de redirection HTTP GET en utilisant [max_http_get_redirects](../../../operations/settings/settings.md#setting-max_http_get_redirects) paramètre.
-
-**Exemple:**
-
-**1.** Créer un `url_engine_table` table sur le serveur :
-
-``` sql
-CREATE TABLE url_engine_table (word String, value UInt64)
-ENGINE=URL('http://127.0.0.1:12345/', CSV)
-```
-
-**2.** Créez un serveur HTTP de base à l'aide des outils Python 3 standard et
-démarrer:
-
-``` python3
-from http.server import BaseHTTPRequestHandler, HTTPServer
-
-class CSVHTTPServer(BaseHTTPRequestHandler):
-    def do_GET(self):
-        self.send_response(200)
-        self.send_header('Content-type', 'text/csv')
-        self.end_headers()
-
-        self.wfile.write(bytes('Hello,1
World,2
', "utf-8"))
-
-if __name__ == "__main__":
-    server_address = ('127.0.0.1', 12345)
-    HTTPServer(server_address, CSVHTTPServer).serve_forever()
-```
-
-``` bash
-$ python3 server.py
-```
-
-**3.** Les données de la demande:
-
-``` sql
-SELECT * FROM url_engine_table
-```
-
-``` text
-┌─word──┬─value─┐
-│ Hello │     1 │
-│ World │     2 │
-└───────┴───────┘
-```
-
-## Les détails de mise en Œuvre {#details-of-implementation}
-
--   Les lectures et les écritures peuvent être parallèles
--   Pas pris en charge:
-    -   `ALTER` et `SELECT...SAMPLE` opérations.
-    -   Index.
-    -   Réplication.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/url/) <!--hide-->
diff --git a/docs/fr/engines/table-engines/special/view.md b/docs/fr/engines/table-engines/special/view.md
deleted file mode 100644
index aaeb6ce90cba..000000000000
--- a/docs/fr/engines/table-engines/special/view.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: Vue
----
-
-# Vue {#table_engines-view}
-
-Utilisé pour implémenter des vues (pour plus d'informations, voir `CREATE VIEW query`). Il ne stocke pas de données, mais stocke uniquement les `SELECT` requête. Lors de la lecture d'une table, il exécute cette requête (et supprime toutes les colonnes inutiles de la requête).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/table_engines/view/) <!--hide-->
diff --git a/docs/fr/faq/general.md b/docs/fr/faq/general.md
deleted file mode 100644
index 797709fdb20f..000000000000
--- a/docs/fr/faq/general.md
+++ /dev/null
@@ -1,60 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 78
-toc_title: "Questions G\xE9n\xE9rales"
----
-
-# Questions Générales {#general-questions}
-
-## Pourquoi Ne Pas Utiliser Quelque Chose Comme MapReduce? {#why-not-use-something-like-mapreduce}
-
-Nous pouvons nous référer à des systèmes comme MapReduce en tant que systèmes informatiques distribués dans lesquels l'opération de réduction est basée sur le tri distribué. La solution open-source la plus courante dans cette classe est [Apache Hadoop](http://hadoop.apache.org). Yandex utilise sa solution interne, YT.
-
-Ces systèmes ne sont pas appropriés pour les requêtes en ligne en raison de leur latence élevée. En d'autres termes, ils ne peuvent pas être utilisés comme back-end pour une interface web. Ces types de systèmes ne sont pas utiles pour les mises à jour de données en temps réel. Le tri distribué n'est pas la meilleure façon d'effectuer des opérations de réduction si le résultat de l'opération et tous les résultats intermédiaires (s'il y en a) sont situés dans la RAM d'un seul serveur, ce qui est généralement le cas pour les requêtes en ligne. Dans un tel cas, une table de hachage est un moyen optimal d'effectuer des opérations de réduction. Une approche courante pour optimiser les tâches map-reduce est la pré-agrégation (réduction partielle) à l'aide d'une table de hachage en RAM. L'utilisateur effectue cette optimisation manuellement. Le tri distribué est l'une des principales causes de réduction des performances lors de l'exécution de tâches simples de réduction de la carte.
-
-La plupart des implémentations MapReduce vous permettent d'exécuter du code arbitraire sur un cluster. Mais un langage de requête déclaratif est mieux adapté à OLAP pour exécuter des expériences rapidement. Par exemple, Hadoop a ruche et Cochon. Considérez également Cloudera Impala ou Shark (obsolète) pour Spark, ainsi que Spark SQL, Presto et Apache Drill. Les performances lors de l'exécution de telles tâches sont très sous-optimales par rapport aux systèmes spécialisés, mais une latence relativement élevée rend irréaliste l'utilisation de ces systèmes comme backend pour une interface web.
-
-## Que Faire si j'ai un problème avec les encodages lors de l'utilisation D'Oracle via ODBC? {#oracle-odbc-encodings}
-
-Si vous utilisez Oracle via le pilote ODBC comme source de dictionnaires externes, vous devez définir la valeur correcte pour `NLS_LANG` variable d'environnement dans `/etc/default/clickhouse`. Pour plus d'informations, voir le [FAQ Oracle NLS_LANG](https://www.oracle.com/technetwork/products/globalization/nls-lang-099431.html).
-
-**Exemple**
-
-``` sql
-NLS_LANG=RUSSIAN_RUSSIA.UTF8
-```
-
-## Comment exporter des données de ClickHouse vers un fichier? {#how-to-export-to-file}
-
-### Utilisation de la Clause INTO OUTFILE {#using-into-outfile-clause}
-
-Ajouter un [INTO OUTFILE](../sql-reference/statements/select/into-outfile.md#into-outfile-clause) clause à votre requête.
-
-Exemple:
-
-``` sql
-SELECT * FROM table INTO OUTFILE 'file'
-```
-
-Par défaut, ClickHouse utilise [TabSeparated](../interfaces/formats.md#tabseparated) format pour les données de sortie. Pour sélectionner le [format de données](../interfaces/formats.md), utiliser le [FORMAT de la clause](../sql-reference/statements/select/format.md#format-clause).
-
-Exemple:
-
-``` sql
-SELECT * FROM table INTO OUTFILE 'file' FORMAT CSV
-```
-
-### Utilisation d'une Table de moteur de fichiers {#using-a-file-engine-table}
-
-Voir [Fichier](../engines/table-engines/special/file.md).
-
-### Utilisation De La Redirection En Ligne De Commande {#using-command-line-redirection}
-
-``` sql
-$ clickhouse-client --query "SELECT * from table" --format FormatName > result.txt
-```
-
-Voir [clickhouse-client](../interfaces/cli.md).
-
-{## [Article Original](https://clickhouse.tech/docs/en/faq/general/) ##}
diff --git a/docs/fr/faq/index.md b/docs/fr/faq/index.md
deleted file mode 100644
index a44dbb31e897..000000000000
--- a/docs/fr/faq/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: F.A.Q.
-toc_priority: 76
----
-
-
diff --git a/docs/fr/getting-started/example-datasets/amplab-benchmark.md b/docs/fr/getting-started/example-datasets/amplab-benchmark.md
deleted file mode 100644
index 4a51defe7fec..000000000000
--- a/docs/fr/getting-started/example-datasets/amplab-benchmark.md
+++ /dev/null
@@ -1,129 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 17
-toc_title: AMPLab Big Data Benchmark
----
-
-# AMPLab Big Data Benchmark {#amplab-big-data-benchmark}
-
-Tu vois https://amplab.cs.berkeley.edu/benchmark/
-
-Inscrivez-vous pour un compte GRATUIT à https://aws.amazon.com. il nécessite une carte de crédit, e-mail et numéro de téléphone. Obtenir une nouvelle clé d'accès à https://console.aws.amazon.com/iam/home?nc2=h_m_sc#security_credential
-
-Exécutez ce qui suit dans la console:
-
-``` bash
-$ sudo apt-get install s3cmd
-$ mkdir tiny; cd tiny;
-$ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/tiny/ .
-$ cd ..
-$ mkdir 1node; cd 1node;
-$ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/1node/ .
-$ cd ..
-$ mkdir 5nodes; cd 5nodes;
-$ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/5nodes/ .
-$ cd ..
-```
-
-Exécutez les requêtes ClickHouse suivantes:
-
-``` sql
-CREATE TABLE rankings_tiny
-(
-    pageURL String,
-    pageRank UInt32,
-    avgDuration UInt32
-) ENGINE = Log;
-
-CREATE TABLE uservisits_tiny
-(
-    sourceIP String,
-    destinationURL String,
-    visitDate Date,
-    adRevenue Float32,
-    UserAgent String,
-    cCode FixedString(3),
-    lCode FixedString(6),
-    searchWord String,
-    duration UInt32
-) ENGINE = MergeTree(visitDate, visitDate, 8192);
-
-CREATE TABLE rankings_1node
-(
-    pageURL String,
-    pageRank UInt32,
-    avgDuration UInt32
-) ENGINE = Log;
-
-CREATE TABLE uservisits_1node
-(
-    sourceIP String,
-    destinationURL String,
-    visitDate Date,
-    adRevenue Float32,
-    UserAgent String,
-    cCode FixedString(3),
-    lCode FixedString(6),
-    searchWord String,
-    duration UInt32
-) ENGINE = MergeTree(visitDate, visitDate, 8192);
-
-CREATE TABLE rankings_5nodes_on_single
-(
-    pageURL String,
-    pageRank UInt32,
-    avgDuration UInt32
-) ENGINE = Log;
-
-CREATE TABLE uservisits_5nodes_on_single
-(
-    sourceIP String,
-    destinationURL String,
-    visitDate Date,
-    adRevenue Float32,
-    UserAgent String,
-    cCode FixedString(3),
-    lCode FixedString(6),
-    searchWord String,
-    duration UInt32
-) ENGINE = MergeTree(visitDate, visitDate, 8192);
-```
-
-Retournez à la console:
-
-``` bash
-$ for i in tiny/rankings/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO rankings_tiny FORMAT CSV"; done
-$ for i in tiny/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO uservisits_tiny FORMAT CSV"; done
-$ for i in 1node/rankings/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO rankings_1node FORMAT CSV"; done
-$ for i in 1node/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO uservisits_1node FORMAT CSV"; done
-$ for i in 5nodes/rankings/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO rankings_5nodes_on_single FORMAT CSV"; done
-$ for i in 5nodes/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i | clickhouse-client --host=example-perftest01j --query="INSERT INTO uservisits_5nodes_on_single FORMAT CSV"; done
-```
-
-Requêtes pour obtenir des échantillons de données:
-
-``` sql
-SELECT pageURL, pageRank FROM rankings_1node WHERE pageRank > 1000
-
-SELECT substring(sourceIP, 1, 8), sum(adRevenue) FROM uservisits_1node GROUP BY substring(sourceIP, 1, 8)
-
-SELECT
-    sourceIP,
-    sum(adRevenue) AS totalRevenue,
-    avg(pageRank) AS pageRank
-FROM rankings_1node ALL INNER JOIN
-(
-    SELECT
-        sourceIP,
-        destinationURL AS pageURL,
-        adRevenue
-    FROM uservisits_1node
-    WHERE (visitDate > '1980-01-01') AND (visitDate < '1980-04-01')
-) USING pageURL
-GROUP BY sourceIP
-ORDER BY totalRevenue DESC
-LIMIT 1
-```
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/amplab_benchmark/) <!--hide-->
diff --git a/docs/fr/getting-started/example-datasets/criteo.md b/docs/fr/getting-started/example-datasets/criteo.md
deleted file mode 100644
index 8b52390cdcf6..000000000000
--- a/docs/fr/getting-started/example-datasets/criteo.md
+++ /dev/null
@@ -1,81 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 19
-toc_title: "T\xE9raoctet click Logs de Criteo"
----
-
-# Téraoctet de journaux de clics de Criteo {#terabyte-of-click-logs-from-criteo}
-
-Télécharger les données à partir de http://labs.criteo.com/downloads/download-terabyte-click-logs/
-
-Créer une table pour importer le journal:
-
-``` sql
-CREATE TABLE criteo_log (date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, cat1 String, cat2 String, cat3 String, cat4 String, cat5 String, cat6 String, cat7 String, cat8 String, cat9 String, cat10 String, cat11 String, cat12 String, cat13 String, cat14 String, cat15 String, cat16 String, cat17 String, cat18 String, cat19 String, cat20 String, cat21 String, cat22 String, cat23 String, cat24 String, cat25 String, cat26 String) ENGINE = Log
-```
-
-Télécharger les données:
-
-``` bash
-$ for i in {00..23}; do echo $i; zcat datasets/criteo/day_${i#0}.gz | sed -r 's/^/2000-01-'${i/00/24}'\t/' | clickhouse-client --host=example-perftest01j --query="INSERT INTO criteo_log FORMAT TabSeparated"; done
-```
-
-Créer une table pour les données converties:
-
-``` sql
-CREATE TABLE criteo
-(
-    date Date,
-    clicked UInt8,
-    int1 Int32,
-    int2 Int32,
-    int3 Int32,
-    int4 Int32,
-    int5 Int32,
-    int6 Int32,
-    int7 Int32,
-    int8 Int32,
-    int9 Int32,
-    int10 Int32,
-    int11 Int32,
-    int12 Int32,
-    int13 Int32,
-    icat1 UInt32,
-    icat2 UInt32,
-    icat3 UInt32,
-    icat4 UInt32,
-    icat5 UInt32,
-    icat6 UInt32,
-    icat7 UInt32,
-    icat8 UInt32,
-    icat9 UInt32,
-    icat10 UInt32,
-    icat11 UInt32,
-    icat12 UInt32,
-    icat13 UInt32,
-    icat14 UInt32,
-    icat15 UInt32,
-    icat16 UInt32,
-    icat17 UInt32,
-    icat18 UInt32,
-    icat19 UInt32,
-    icat20 UInt32,
-    icat21 UInt32,
-    icat22 UInt32,
-    icat23 UInt32,
-    icat24 UInt32,
-    icat25 UInt32,
-    icat26 UInt32
-) ENGINE = MergeTree(date, intHash32(icat1), (date, intHash32(icat1)), 8192)
-```
-
-Transformez les données du journal brut et placez - les dans la deuxième table:
-
-``` sql
-INSERT INTO criteo SELECT date, clicked, int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, reinterpretAsUInt32(unhex(cat1)) AS icat1, reinterpretAsUInt32(unhex(cat2)) AS icat2, reinterpretAsUInt32(unhex(cat3)) AS icat3, reinterpretAsUInt32(unhex(cat4)) AS icat4, reinterpretAsUInt32(unhex(cat5)) AS icat5, reinterpretAsUInt32(unhex(cat6)) AS icat6, reinterpretAsUInt32(unhex(cat7)) AS icat7, reinterpretAsUInt32(unhex(cat8)) AS icat8, reinterpretAsUInt32(unhex(cat9)) AS icat9, reinterpretAsUInt32(unhex(cat10)) AS icat10, reinterpretAsUInt32(unhex(cat11)) AS icat11, reinterpretAsUInt32(unhex(cat12)) AS icat12, reinterpretAsUInt32(unhex(cat13)) AS icat13, reinterpretAsUInt32(unhex(cat14)) AS icat14, reinterpretAsUInt32(unhex(cat15)) AS icat15, reinterpretAsUInt32(unhex(cat16)) AS icat16, reinterpretAsUInt32(unhex(cat17)) AS icat17, reinterpretAsUInt32(unhex(cat18)) AS icat18, reinterpretAsUInt32(unhex(cat19)) AS icat19, reinterpretAsUInt32(unhex(cat20)) AS icat20, reinterpretAsUInt32(unhex(cat21)) AS icat21, reinterpretAsUInt32(unhex(cat22)) AS icat22, reinterpretAsUInt32(unhex(cat23)) AS icat23, reinterpretAsUInt32(unhex(cat24)) AS icat24, reinterpretAsUInt32(unhex(cat25)) AS icat25, reinterpretAsUInt32(unhex(cat26)) AS icat26 FROM criteo_log;
-
-DROP TABLE criteo_log;
-```
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/criteo/) <!--hide-->
diff --git a/docs/fr/getting-started/example-datasets/index.md b/docs/fr/getting-started/example-datasets/index.md
deleted file mode 100644
index 03b2f15890b8..000000000000
--- a/docs/fr/getting-started/example-datasets/index.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Exemple De Jeux De Donn\xE9es"
-toc_priority: 12
-toc_title: Introduction
----
-
-# Exemple De Jeux De Données {#example-datasets}
-
-Cette section décrit comment obtenir des exemples de jeux de données et les importer dans ClickHouse.
-Pour certains ensembles de données exemple les requêtes sont également disponibles.
-
--   [Yandex Anonymisé.Metrica Dataset](metrica.md)
--   [Référence Du Schéma En Étoile](star-schema.md)
--   [WikiStat](wikistat.md)
--   [Téraoctet de journaux de clics de Criteo](criteo.md)
--   [AMPLab Big Data Benchmark](amplab-benchmark.md)
--   [New York Taxi Données](nyc-taxi.md)
--   [OnTime](ontime.md)
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/example_datasets) <!--hide-->
diff --git a/docs/fr/getting-started/example-datasets/metrica.md b/docs/fr/getting-started/example-datasets/metrica.md
deleted file mode 100644
index 453a30d436f6..000000000000
--- a/docs/fr/getting-started/example-datasets/metrica.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 14
-toc_title: "Yandex.Metrica De Donn\xE9es"
----
-
-# Yandex Anonymisé.Metrica De Données {#anonymized-yandex-metrica-data}
-
-Dataset se compose de deux tables contenant des données anonymisées sur les hits (`hits_v1`) et les visites (`visits_v1`) de Yandex.Metrica. Vous pouvez en savoir plus sur Yandex.Metrica dans [Histoire de ClickHouse](../../introduction/history.md) section.
-
-L'ensemble de données se compose de deux tables, l'une d'elles peut être téléchargée sous forme compressée `tsv.xz` fichier ou comme partitions préparées. En outre, une version étendue de l' `hits` table contenant 100 millions de lignes est disponible comme TSV à https://datasets.clickhouse.tech/hits/tsv/hits_100m_obfuscated_v1.tsv.xz et comme partitions préparées à https://datasets.clickhouse.tech/hits/partitions/hits_100m_obfuscated_v1.tar.xz.
-
-## Obtention de Tables à partir de Partitions préparées {#obtaining-tables-from-prepared-partitions}
-
-Télécharger et importer la table hits:
-
-``` bash
-curl -O https://datasets.clickhouse.tech/hits/partitions/hits_v1.tar
-tar xvf hits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-# check permissions on unpacked data, fix if required
-sudo service clickhouse-server restart
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.hits_v1"
-```
-
-Télécharger et importer des visites:
-
-``` bash
-curl -O https://datasets.clickhouse.tech/visits/partitions/visits_v1.tar
-tar xvf visits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-# check permissions on unpacked data, fix if required
-sudo service clickhouse-server restart
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.visits_v1"
-```
-
-## Obtention de Tables à partir D'un fichier TSV compressé {#obtaining-tables-from-compressed-tsv-file}
-
-Télécharger et importer des hits à partir du fichier TSV compressé:
-
-``` bash
-curl https://datasets.clickhouse.tech/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` > hits_v1.tsv
-# now create table
-clickhouse-client --query "CREATE DATABASE IF NOT EXISTS datasets"
-clickhouse-client --query "CREATE TABLE datasets.hits_v1 ( WatchID UInt64,  JavaEnable UInt8,  Title String,  GoodEvent Int16,  EventTime DateTime,  EventDate Date,  CounterID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RegionID UInt32,  UserID UInt64,  CounterClass Int8,  OS UInt8,  UserAgent UInt8,  URL String,  Referer String,  URLDomain String,  RefererDomain String,  Refresh UInt8,  IsRobot UInt8,  RefererCategories Array(UInt16),  URLCategories Array(UInt16), URLRegions Array(UInt32),  RefererRegions Array(UInt32),  ResolutionWidth UInt16,  ResolutionHeight UInt16,  ResolutionDepth UInt8,  FlashMajor UInt8, FlashMinor UInt8,  FlashMinor2 String,  NetMajor UInt8,  NetMinor UInt8, UserAgentMajor UInt16,  UserAgentMinor FixedString(2),  CookieEnable UInt8, JavascriptEnable UInt8,  IsMobile UInt8,  MobilePhone UInt8,  MobilePhoneModel String,  Params String,  IPNetworkID UInt32,  TraficSourceID Int8, SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  IsArtifical UInt8,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  ClientTimeZone Int16,  ClientEventTime DateTime,  SilverlightVersion1 UInt8, SilverlightVersion2 UInt8,  SilverlightVersion3 UInt32,  SilverlightVersion4 UInt16,  PageCharset String,  CodeVersion UInt32,  IsLink UInt8,  IsDownload UInt8,  IsNotBounce UInt8,  FUniqID UInt64,  HID UInt32,  IsOldCounter UInt8, IsEvent UInt8,  IsParameter UInt8,  DontCountHits UInt8,  WithHash UInt8, HitColor FixedString(1),  UTCEventTime DateTime,  Age UInt8,  Sex UInt8,  Income UInt8,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16), RemoteIP UInt32,  RemoteIP6 FixedString(16),  WindowName Int32,  OpenerName Int32,  HistoryLength Int16,  BrowserLanguage FixedString(2),  BrowserCountry FixedString(2),  SocialNetwork String,  SocialAction String,  HTTPError UInt16, SendTiming Int32,  DNSTiming Int32,  ConnectTiming Int32,  ResponseStartTiming Int32,  ResponseEndTiming Int32,  FetchTiming Int32,  RedirectTiming Int32, DOMInteractiveTiming Int32,  DOMContentLoadedTiming Int32,  DOMCompleteTiming Int32,  LoadEventStartTiming Int32,  LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32,  FirstPaintTiming Int32,  RedirectCount Int8, SocialSourceNetworkID UInt8,  SocialSourcePage String,  ParamPrice Int64, ParamOrderID String,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16, GoalsReached Array(UInt32),  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String, UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String, FromTag String,  HasGCLID UInt8,  RefererHash UInt64,  URLHash UInt64,  CLID UInt32,  YCLID UInt64,  ShareService String,  ShareURL String,  ShareTitle String,  ParsedParams Nested(Key1 String,  Key2 String, Key3 String, Key4 String, Key5 String,  ValueDouble Float64),  IslandID FixedString(16),  RequestNum UInt32,  RequestTry UInt8) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192"
-# import data
-cat hits_v1.tsv | clickhouse-client --query "INSERT INTO datasets.hits_v1 FORMAT TSV" --max_insert_block_size=100000
-# optionally you can optimize table
-clickhouse-client --query "OPTIMIZE TABLE datasets.hits_v1 FINAL"
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.hits_v1"
-```
-
-Télécharger et importer des visites à partir du fichier TSV compressé:
-
-``` bash
-curl https://datasets.clickhouse.tech/visits/tsv/visits_v1.tsv.xz | unxz --threads=`nproc` > visits_v1.tsv
-# now create table
-clickhouse-client --query "CREATE DATABASE IF NOT EXISTS datasets"
-clickhouse-client --query "CREATE TABLE datasets.visits_v1 ( CounterID UInt32,  StartDate Date,  Sign Int8,  IsNew UInt8,  VisitID UInt64,  UserID UInt64,  StartTime DateTime,  Duration UInt32,  UTCStartTime DateTime,  PageViews Int32,  Hits Int32,  IsBounce UInt8,  Referer String,  StartURL String,  RefererDomain String,  StartURLDomain String,  EndURL String,  LinkURL String,  IsDownload UInt8,  TraficSourceID Int8,  SearchEngineID UInt16,  SearchPhrase String,  AdvEngineID UInt8,  PlaceID Int32,  RefererCategories Array(UInt16),  URLCategories Array(UInt16),  URLRegions Array(UInt32),  RefererRegions Array(UInt32),  IsYandex UInt8,  GoalReachesDepth Int32,  GoalReachesURL Int32,  GoalReachesAny Int32,  SocialSourceNetworkID UInt8,  SocialSourcePage String,  MobilePhoneModel String,  ClientEventTime DateTime,  RegionID UInt32,  ClientIP UInt32,  ClientIP6 FixedString(16),  RemoteIP UInt32,  RemoteIP6 FixedString(16),  IPNetworkID UInt32,  SilverlightVersion3 UInt32,  CodeVersion UInt32,  ResolutionWidth UInt16,  ResolutionHeight UInt16,  UserAgentMajor UInt16,  UserAgentMinor UInt16,  WindowClientWidth UInt16,  WindowClientHeight UInt16,  SilverlightVersion2 UInt8,  SilverlightVersion4 UInt16,  FlashVersion3 UInt16,  FlashVersion4 UInt16,  ClientTimeZone Int16,  OS UInt8,  UserAgent UInt8,  ResolutionDepth UInt8,  FlashMajor UInt8,  FlashMinor UInt8,  NetMajor UInt8,  NetMinor UInt8,  MobilePhone UInt8,  SilverlightVersion1 UInt8,  Age UInt8,  Sex UInt8,  Income UInt8,  JavaEnable UInt8,  CookieEnable UInt8,  JavascriptEnable UInt8,  IsMobile UInt8,  BrowserLanguage UInt16,  BrowserCountry UInt16,  Interests UInt16,  Robotness UInt8,  GeneralInterests Array(UInt16),  Params Array(String),  Goals Nested(ID UInt32, Serial UInt32, EventTime DateTime,  Price Int64,  OrderID String, CurrencyID UInt32),  WatchIDs Array(UInt64),  ParamSumPrice Int64,  ParamCurrency FixedString(3),  ParamCurrencyID UInt16,  ClickLogID UInt64,  ClickEventID Int32,  ClickGoodEvent Int32,  ClickEventTime DateTime,  ClickPriorityID Int32,  ClickPhraseID Int32,  ClickPageID Int32,  ClickPlaceID Int32,  ClickTypeID Int32,  ClickResourceID Int32,  ClickCost UInt32,  ClickClientIP UInt32,  ClickDomainID UInt32,  ClickURL String,  ClickAttempt UInt8,  ClickOrderID UInt32,  ClickBannerID UInt32,  ClickMarketCategoryID UInt32,  ClickMarketPP UInt32,  ClickMarketCategoryName String,  ClickMarketPPName String,  ClickAWAPSCampaignName String,  ClickPageName String,  ClickTargetType UInt16,  ClickTargetPhraseID UInt64,  ClickContextType UInt8,  ClickSelectType Int8,  ClickOptions String,  ClickGroupBannerID Int32,  OpenstatServiceName String,  OpenstatCampaignID String,  OpenstatAdID String,  OpenstatSourceID String,  UTMSource String,  UTMMedium String,  UTMCampaign String,  UTMContent String,  UTMTerm String,  FromTag String,  HasGCLID UInt8,  FirstVisit DateTime,  PredLastVisit Date,  LastVisit Date,  TotalVisits UInt32,  TraficSource    Nested(ID Int8,  SearchEngineID UInt16, AdvEngineID UInt8, PlaceID UInt16, SocialSourceNetworkID UInt8, Domain String, SearchPhrase String, SocialSourcePage String),  Attendance FixedString(16),  CLID UInt32,  YCLID UInt64,  NormalizedRefererHash UInt64,  SearchPhraseHash UInt64,  RefererDomainHash UInt64,  NormalizedStartURLHash UInt64,  StartURLDomainHash UInt64,  NormalizedEndURLHash UInt64,  TopLevelDomain UInt64,  URLScheme UInt64,  OpenstatServiceNameHash UInt64,  OpenstatCampaignIDHash UInt64,  OpenstatAdIDHash UInt64,  OpenstatSourceIDHash UInt64,  UTMSourceHash UInt64,  UTMMediumHash UInt64,  UTMCampaignHash UInt64,  UTMContentHash UInt64,  UTMTermHash UInt64,  FromHash UInt64,  WebVisorEnabled UInt8,  WebVisorActivity UInt32,  ParsedParams    Nested(Key1 String,  Key2 String,  Key3 String,  Key4 String, Key5 String, ValueDouble    Float64),  Market Nested(Type UInt8, GoalID UInt32, OrderID String,  OrderPrice Int64,  PP UInt32,  DirectPlaceID UInt32,  DirectOrderID  UInt32,  DirectBannerID UInt32,  GoodID String, GoodName String, GoodQuantity Int32,  GoodPrice Int64),  IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192"
-# import data
-cat visits_v1.tsv | clickhouse-client --query "INSERT INTO datasets.visits_v1 FORMAT TSV" --max_insert_block_size=100000
-# optionally you can optimize table
-clickhouse-client --query "OPTIMIZE TABLE datasets.visits_v1 FINAL"
-clickhouse-client --query "SELECT COUNT(*) FROM datasets.visits_v1"
-```
-
-## Exemple De Requêtes {#example-queries}
-
-[Tutoriel ClickHouse](../../getting-started/tutorial.md) est basé sur Yandex.Metrica dataset et la façon recommandée pour commencer avec cet ensemble de données est de simplement passer par tutoriel.
-
-D'autres exemples de requêtes pour ces tables peuvent être trouvés parmi [tests avec État](https://github.com/ClickHouse/ClickHouse/tree/master/tests/queries/1_stateful) de ClickHouse (ils sont nommés `test.hists` et `test.visits` y).
diff --git a/docs/fr/getting-started/example-datasets/nyc-taxi.md b/docs/fr/getting-started/example-datasets/nyc-taxi.md
deleted file mode 100644
index 46aa944e7181..000000000000
--- a/docs/fr/getting-started/example-datasets/nyc-taxi.md
+++ /dev/null
@@ -1,390 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 16
-toc_title: "New York Taxi Donn\xE9es"
----
-
-# New York Taxi Données {#new-york-taxi-data}
-
-Ce jeu de données peut être obtenu de deux façons:
-
--   importation à partir de données brutes
--   téléchargement de partitions
-
-## Comment importer les données brutes {#how-to-import-the-raw-data}
-
-Tu vois https://github.com/toddwschneider/nyc-taxi-data et http://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html pour la description d'un ensemble de données et les instructions de téléchargement.
-
-Le téléchargement entraînera environ 227 Go de données non compressées dans des fichiers CSV. Le téléchargement prend environ une heure sur une connexion 1 Gbit (téléchargement parallèle depuis s3.amazonaws.com récupère au moins la moitié d'un canal 1 Gbit).
-Certains fichiers peuvent ne pas télécharger entièrement. Vérifiez la taille des fichiers et re-télécharger tout ce qui semble douteux.
-
-Certains fichiers peuvent contenir des lignes invalides. Vous pouvez les corriger comme suit:
-
-``` bash
-sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-02.csv > data/yellow_tripdata_2010-02.csv_
-sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-03.csv > data/yellow_tripdata_2010-03.csv_
-mv data/yellow_tripdata_2010-02.csv_ data/yellow_tripdata_2010-02.csv
-mv data/yellow_tripdata_2010-03.csv_ data/yellow_tripdata_2010-03.csv
-```
-
-Ensuite, les données doivent être pré-traitées dans PostgreSQL. Cela créera des sélections de points dans les polygones (pour faire correspondre les points sur la carte avec les arrondissements de New York) et combinera toutes les données en une seule table plate dénormalisée à l'aide d'une jointure. Pour ce faire, vous devrez installer PostgreSQL avec le support PostGIS.
-
-Soyez prudent lors de l'exécution `initialize_database.sh` et vérifiez à nouveau manuellement que toutes les tables ont été créées correctement.
-
-Il faut environ 20-30 minutes pour traiter la valeur de chaque mois de données dans PostgreSQL, pour un total d'environ 48 heures.
-
-Vous pouvez vérifier le nombre de téléchargé lignes comme suit:
-
-``` bash
-$ time psql nyc-taxi-data -c "SELECT count(*) FROM trips;"
-## Count
- 1298979494
-(1 row)
-
-real    7m9.164s
-```
-
-(C'est un peu plus de 1,1 milliard de lignes rapportées par Mark Litwintschik dans une série de billets de blog.)
-
-Les données de PostgreSQL utilisent 370 GO d'espace.
-
-Exportation des données depuis PostgreSQL:
-
-``` sql
-COPY
-(
-    SELECT trips.id,
-           trips.vendor_id,
-           trips.pickup_datetime,
-           trips.dropoff_datetime,
-           trips.store_and_fwd_flag,
-           trips.rate_code_id,
-           trips.pickup_longitude,
-           trips.pickup_latitude,
-           trips.dropoff_longitude,
-           trips.dropoff_latitude,
-           trips.passenger_count,
-           trips.trip_distance,
-           trips.fare_amount,
-           trips.extra,
-           trips.mta_tax,
-           trips.tip_amount,
-           trips.tolls_amount,
-           trips.ehail_fee,
-           trips.improvement_surcharge,
-           trips.total_amount,
-           trips.payment_type,
-           trips.trip_type,
-           trips.pickup,
-           trips.dropoff,
-
-           cab_types.type cab_type,
-
-           weather.precipitation_tenths_of_mm rain,
-           weather.snow_depth_mm,
-           weather.snowfall_mm,
-           weather.max_temperature_tenths_degrees_celsius max_temp,
-           weather.min_temperature_tenths_degrees_celsius min_temp,
-           weather.average_wind_speed_tenths_of_meters_per_second wind,
-
-           pick_up.gid pickup_nyct2010_gid,
-           pick_up.ctlabel pickup_ctlabel,
-           pick_up.borocode pickup_borocode,
-           pick_up.boroname pickup_boroname,
-           pick_up.ct2010 pickup_ct2010,
-           pick_up.boroct2010 pickup_boroct2010,
-           pick_up.cdeligibil pickup_cdeligibil,
-           pick_up.ntacode pickup_ntacode,
-           pick_up.ntaname pickup_ntaname,
-           pick_up.puma pickup_puma,
-
-           drop_off.gid dropoff_nyct2010_gid,
-           drop_off.ctlabel dropoff_ctlabel,
-           drop_off.borocode dropoff_borocode,
-           drop_off.boroname dropoff_boroname,
-           drop_off.ct2010 dropoff_ct2010,
-           drop_off.boroct2010 dropoff_boroct2010,
-           drop_off.cdeligibil dropoff_cdeligibil,
-           drop_off.ntacode dropoff_ntacode,
-           drop_off.ntaname dropoff_ntaname,
-           drop_off.puma dropoff_puma
-    FROM trips
-    LEFT JOIN cab_types
-        ON trips.cab_type_id = cab_types.id
-    LEFT JOIN central_park_weather_observations_raw weather
-        ON weather.date = trips.pickup_datetime::date
-    LEFT JOIN nyct2010 pick_up
-        ON pick_up.gid = trips.pickup_nyct2010_gid
-    LEFT JOIN nyct2010 drop_off
-        ON drop_off.gid = trips.dropoff_nyct2010_gid
-) TO '/opt/milovidov/nyc-taxi-data/trips.tsv';
-```
-
-L'instantané de données est créé à une vitesse d'environ 50 Mo par seconde. Lors de la création de l'instantané, PostgreSQL lit à partir du disque à une vitesse d'environ 28 Mo par seconde.
-Cela prend environ 5 heures. Le fichier TSV résultant est 590612904969 octets.
-
-Créer une table temporaire dans ClickHouse:
-
-``` sql
-CREATE TABLE trips
-(
-trip_id                 UInt32,
-vendor_id               String,
-pickup_datetime         DateTime,
-dropoff_datetime        Nullable(DateTime),
-store_and_fwd_flag      Nullable(FixedString(1)),
-rate_code_id            Nullable(UInt8),
-pickup_longitude        Nullable(Float64),
-pickup_latitude         Nullable(Float64),
-dropoff_longitude       Nullable(Float64),
-dropoff_latitude        Nullable(Float64),
-passenger_count         Nullable(UInt8),
-trip_distance           Nullable(Float64),
-fare_amount             Nullable(Float32),
-extra                   Nullable(Float32),
-mta_tax                 Nullable(Float32),
-tip_amount              Nullable(Float32),
-tolls_amount            Nullable(Float32),
-ehail_fee               Nullable(Float32),
-improvement_surcharge   Nullable(Float32),
-total_amount            Nullable(Float32),
-payment_type            Nullable(String),
-trip_type               Nullable(UInt8),
-pickup                  Nullable(String),
-dropoff                 Nullable(String),
-cab_type                Nullable(String),
-precipitation           Nullable(UInt8),
-snow_depth              Nullable(UInt8),
-snowfall                Nullable(UInt8),
-max_temperature         Nullable(UInt8),
-min_temperature         Nullable(UInt8),
-average_wind_speed      Nullable(UInt8),
-pickup_nyct2010_gid     Nullable(UInt8),
-pickup_ctlabel          Nullable(String),
-pickup_borocode         Nullable(UInt8),
-pickup_boroname         Nullable(String),
-pickup_ct2010           Nullable(String),
-pickup_boroct2010       Nullable(String),
-pickup_cdeligibil       Nullable(FixedString(1)),
-pickup_ntacode          Nullable(String),
-pickup_ntaname          Nullable(String),
-pickup_puma             Nullable(String),
-dropoff_nyct2010_gid    Nullable(UInt8),
-dropoff_ctlabel         Nullable(String),
-dropoff_borocode        Nullable(UInt8),
-dropoff_boroname        Nullable(String),
-dropoff_ct2010          Nullable(String),
-dropoff_boroct2010      Nullable(String),
-dropoff_cdeligibil      Nullable(String),
-dropoff_ntacode         Nullable(String),
-dropoff_ntaname         Nullable(String),
-dropoff_puma            Nullable(String)
-) ENGINE = Log;
-```
-
-Il est nécessaire pour convertir les champs en types de données plus corrects et, si possible, pour éliminer les valeurs Null.
-
-``` bash
-$ time clickhouse-client --query="INSERT INTO trips FORMAT TabSeparated" < trips.tsv
-
-real    75m56.214s
-```
-
-Les données sont lues à une vitesse de 112-140 Mo/seconde.
-Le chargement de données dans une table de type de journal dans un flux a pris 76 minutes.
-Les données de ce tableau utilisent 142 GO.
-
-(L'importation de données directement depuis Postgres est également possible en utilisant `COPY ... TO PROGRAM`.)
-
-Unfortunately, all the fields associated with the weather (precipitation…average_wind_speed) were filled with NULL. Because of this, we will remove them from the final data set.
-
-Pour commencer, nous allons créer une table sur un serveur unique. Plus tard, nous ferons le tableau distribué.
-
-Créer et remplir un tableau récapitulatif:
-
-``` sql
-CREATE TABLE trips_mergetree
-ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
-AS SELECT
-
-trip_id,
-CAST(vendor_id AS Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14)) AS vendor_id,
-toDate(pickup_datetime) AS pickup_date,
-ifNull(pickup_datetime, toDateTime(0)) AS pickup_datetime,
-toDate(dropoff_datetime) AS dropoff_date,
-ifNull(dropoff_datetime, toDateTime(0)) AS dropoff_datetime,
-assumeNotNull(store_and_fwd_flag) IN ('Y', '1', '2') AS store_and_fwd_flag,
-assumeNotNull(rate_code_id) AS rate_code_id,
-assumeNotNull(pickup_longitude) AS pickup_longitude,
-assumeNotNull(pickup_latitude) AS pickup_latitude,
-assumeNotNull(dropoff_longitude) AS dropoff_longitude,
-assumeNotNull(dropoff_latitude) AS dropoff_latitude,
-assumeNotNull(passenger_count) AS passenger_count,
-assumeNotNull(trip_distance) AS trip_distance,
-assumeNotNull(fare_amount) AS fare_amount,
-assumeNotNull(extra) AS extra,
-assumeNotNull(mta_tax) AS mta_tax,
-assumeNotNull(tip_amount) AS tip_amount,
-assumeNotNull(tolls_amount) AS tolls_amount,
-assumeNotNull(ehail_fee) AS ehail_fee,
-assumeNotNull(improvement_surcharge) AS improvement_surcharge,
-assumeNotNull(total_amount) AS total_amount,
-CAST((assumeNotNull(payment_type) AS pt) IN ('CSH', 'CASH', 'Cash', 'CAS', 'Cas', '1') ? 'CSH' : (pt IN ('CRD', 'Credit', 'Cre', 'CRE', 'CREDIT', '2') ? 'CRE' : (pt IN ('NOC', 'No Charge', 'No', '3') ? 'NOC' : (pt IN ('DIS', 'Dispute', 'Dis', '4') ? 'DIS' : 'UNK'))) AS Enum8('CSH' = 1, 'CRE' = 2, 'UNK' = 0, 'NOC' = 3, 'DIS' = 4)) AS payment_type_,
-assumeNotNull(trip_type) AS trip_type,
-ifNull(toFixedString(unhex(pickup), 25), toFixedString('', 25)) AS pickup,
-ifNull(toFixedString(unhex(dropoff), 25), toFixedString('', 25)) AS dropoff,
-CAST(assumeNotNull(cab_type) AS Enum8('yellow' = 1, 'green' = 2, 'uber' = 3)) AS cab_type,
-
-assumeNotNull(pickup_nyct2010_gid) AS pickup_nyct2010_gid,
-toFloat32(ifNull(pickup_ctlabel, '0')) AS pickup_ctlabel,
-assumeNotNull(pickup_borocode) AS pickup_borocode,
-CAST(assumeNotNull(pickup_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS pickup_boroname,
-toFixedString(ifNull(pickup_ct2010, '000000'), 6) AS pickup_ct2010,
-toFixedString(ifNull(pickup_boroct2010, '0000000'), 7) AS pickup_boroct2010,
-CAST(assumeNotNull(ifNull(pickup_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS pickup_cdeligibil,
-toFixedString(ifNull(pickup_ntacode, '0000'), 4) AS pickup_ntacode,
-
-CAST(assumeNotNull(pickup_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS pickup_ntaname,
-
-toUInt16(ifNull(pickup_puma, '0')) AS pickup_puma,
-
-assumeNotNull(dropoff_nyct2010_gid) AS dropoff_nyct2010_gid,
-toFloat32(ifNull(dropoff_ctlabel, '0')) AS dropoff_ctlabel,
-assumeNotNull(dropoff_borocode) AS dropoff_borocode,
-CAST(assumeNotNull(dropoff_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS dropoff_boroname,
-toFixedString(ifNull(dropoff_ct2010, '000000'), 6) AS dropoff_ct2010,
-toFixedString(ifNull(dropoff_boroct2010, '0000000'), 7) AS dropoff_boroct2010,
-CAST(assumeNotNull(ifNull(dropoff_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS dropoff_cdeligibil,
-toFixedString(ifNull(dropoff_ntacode, '0000'), 4) AS dropoff_ntacode,
-
-CAST(assumeNotNull(dropoff_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS dropoff_ntaname,
-
-toUInt16(ifNull(dropoff_puma, '0')) AS dropoff_puma
-
-FROM trips
-```
-
-Cela prend 3030 secondes à une vitesse d'environ 428 000 lignes par seconde.
-Pour le charger plus rapidement, vous pouvez créer la table avec le `Log` le moteur de `MergeTree`. Dans ce cas, le téléchargement fonctionne plus rapidement que 200 secondes.
-
-La table utilise 126 GO d'espace disque.
-
-``` sql
-SELECT formatReadableSize(sum(bytes)) FROM system.parts WHERE table = 'trips_mergetree' AND active
-```
-
-``` text
-┌─formatReadableSize(sum(bytes))─┐
-│ 126.18 GiB                     │
-└────────────────────────────────┘
-```
-
-Entre autres choses, vous pouvez exécuter la requête OPTIMIZE sur MergeTree. Mais ce n'est pas nécessaire puisque tout ira bien sans elle.
-
-## Téléchargement des Partitions préparées {#download-of-prepared-partitions}
-
-``` bash
-$ curl -O https://datasets.clickhouse.tech/trips_mergetree/partitions/trips_mergetree.tar
-$ tar xvf trips_mergetree.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-$ # check permissions of unpacked data, fix if required
-$ sudo service clickhouse-server restart
-$ clickhouse-client --query "select count(*) from datasets.trips_mergetree"
-```
-
-!!! info "Info"
-    Si vous exécutez les requêtes décrites ci-dessous, vous devez utiliser le nom complet de la table, `datasets.trips_mergetree`.
-
-## Résultats sur un seul serveur {#results-on-single-server}
-
-Q1:
-
-``` sql
-SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
-```
-
-0.490 secondes.
-
-Q2:
-
-``` sql
-SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenger_count
-```
-
-1.224 secondes.
-
-Q3:
-
-``` sql
-SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetree GROUP BY passenger_count, year
-```
-
-2.104 secondes.
-
-Q4:
-
-``` sql
-SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*)
-FROM trips_mergetree
-GROUP BY passenger_count, year, distance
-ORDER BY year, count(*) DESC
-```
-
-3.593 secondes.
-
-Le serveur suivant a été utilisé:
-
-Deux Intel (R) Xeon (R) CPU E5-2650 v2 @ 2.60 GHz, 16 noyaux physiques total, 128 GiB RAM, 8x6 TB HD sur le matériel RAID-5
-
-Temps d'exécution est le meilleur des trois pistes. Mais à partir de la deuxième exécution, les requêtes lisent les données du cache du système de fichiers. Aucune autre mise en cache ne se produit: les données sont lues et traitées à chaque exécution.
-
-La création d'un tableau sur trois serveurs:
-
-Sur chaque serveur:
-
-``` sql
-CREATE TABLE default.trips_mergetree_third ( trip_id UInt32,  vendor_id Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14),  pickup_date Date,  pickup_datetime DateTime,  dropoff_date Date,  dropoff_datetime DateTime,  store_and_fwd_flag UInt8,  rate_code_id UInt8,  pickup_longitude Float64,  pickup_latitude Float64,  dropoff_longitude Float64,  dropoff_latitude Float64,  passenger_count UInt8,  trip_distance Float64,  fare_amount Float32,  extra Float32,  mta_tax Float32,  tip_amount Float32,  tolls_amount Float32,  ehail_fee Float32,  improvement_surcharge Float32,  total_amount Float32,  payment_type_ Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4),  trip_type UInt8,  pickup FixedString(25),  dropoff FixedString(25),  cab_type Enum8('yellow' = 1, 'green' = 2, 'uber' = 3),  pickup_nyct2010_gid UInt8,  pickup_ctlabel Float32,  pickup_borocode UInt8,  pickup_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  pickup_ct2010 FixedString(6),  pickup_boroct2010 FixedString(7),  pickup_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  pickup_ntacode FixedString(4),  pickup_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  pickup_puma UInt16,  dropoff_nyct2010_gid UInt8,  dropoff_ctlabel Float32,  dropoff_borocode UInt8,  dropoff_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  dropoff_ct2010 FixedString(6),  dropoff_boroct2010 FixedString(7),  dropoff_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  dropoff_ntacode FixedString(4),  dropoff_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  dropoff_puma UInt16) ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
-```
-
-Sur le serveur source:
-
-``` sql
-CREATE TABLE trips_mergetree_x3 AS trips_mergetree_third ENGINE = Distributed(perftest, default, trips_mergetree_third, rand())
-```
-
-La requête suivante redistribue les données:
-
-``` sql
-INSERT INTO trips_mergetree_x3 SELECT * FROM trips_mergetree
-```
-
-Cela prend 2454 secondes.
-
-Sur les trois serveurs:
-
-Q1: 0.212 secondes.
-Q2: 0.438 secondes.
-Q3: 0.733 secondes.
-Q4: 1.241 secondes.
-
-Pas de surprise ici, depuis les requêtes sont réparties linéairement.
-
-Nous avons également les résultats d'un cluster de 140 serveurs:
-
-Q1: 0,028 sec.
-Q2: 0,043 sec.
-Q3: 0,051 sec.
-Q4: 0,072 sec.
-
-Dans ce cas, le temps de traitement des requêtes est déterminé surtout par la latence du réseau.
-Nous avons exécuté des requêtes en utilisant un client situé dans un centre de données Yandex en Finlande sur un cluster en Russie, ce qui a ajouté environ 20 ms de latence.
-
-## Résumé {#summary}
-
-| serveur | Q1    | Q2    | Q3    | Q4    |
-|---------|-------|-------|-------|-------|
-| 1       | 0.490 | 1.224 | 2.104 | 3.593 |
-| 3       | 0.212 | 0.438 | 0.733 | 1.241 |
-| 140     | 0.028 | 0.043 | 0.051 | 0.072 |
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/nyc_taxi/) <!--hide-->
diff --git a/docs/fr/getting-started/example-datasets/ontime.md b/docs/fr/getting-started/example-datasets/ontime.md
deleted file mode 100644
index 8d036901b99f..000000000000
--- a/docs/fr/getting-started/example-datasets/ontime.md
+++ /dev/null
@@ -1,412 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 15
-toc_title: OnTime
----
-
-# OnTime {#ontime}
-
-Ce jeu de données peut être obtenu de deux façons:
-
--   importation à partir de données brutes
--   téléchargement de partitions
-
-## Importation à partir de données brutes {#import-from-raw-data}
-
-Téléchargement de données:
-
-``` bash
-for s in `seq 1987 2018`
-do
-for m in `seq 1 12`
-do
-wget https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_${s}_${m}.zip
-done
-done
-```
-
-(à partir de https://github.com/Percona-Lab/ontime-airline-performance/blob/master/download.sh )
-
-Création d'une table:
-
-``` sql
-CREATE TABLE `ontime` (
-  `Year` UInt16,
-  `Quarter` UInt8,
-  `Month` UInt8,
-  `DayofMonth` UInt8,
-  `DayOfWeek` UInt8,
-  `FlightDate` Date,
-  `UniqueCarrier` FixedString(7),
-  `AirlineID` Int32,
-  `Carrier` FixedString(2),
-  `TailNum` String,
-  `FlightNum` String,
-  `OriginAirportID` Int32,
-  `OriginAirportSeqID` Int32,
-  `OriginCityMarketID` Int32,
-  `Origin` FixedString(5),
-  `OriginCityName` String,
-  `OriginState` FixedString(2),
-  `OriginStateFips` String,
-  `OriginStateName` String,
-  `OriginWac` Int32,
-  `DestAirportID` Int32,
-  `DestAirportSeqID` Int32,
-  `DestCityMarketID` Int32,
-  `Dest` FixedString(5),
-  `DestCityName` String,
-  `DestState` FixedString(2),
-  `DestStateFips` String,
-  `DestStateName` String,
-  `DestWac` Int32,
-  `CRSDepTime` Int32,
-  `DepTime` Int32,
-  `DepDelay` Int32,
-  `DepDelayMinutes` Int32,
-  `DepDel15` Int32,
-  `DepartureDelayGroups` String,
-  `DepTimeBlk` String,
-  `TaxiOut` Int32,
-  `WheelsOff` Int32,
-  `WheelsOn` Int32,
-  `TaxiIn` Int32,
-  `CRSArrTime` Int32,
-  `ArrTime` Int32,
-  `ArrDelay` Int32,
-  `ArrDelayMinutes` Int32,
-  `ArrDel15` Int32,
-  `ArrivalDelayGroups` Int32,
-  `ArrTimeBlk` String,
-  `Cancelled` UInt8,
-  `CancellationCode` FixedString(1),
-  `Diverted` UInt8,
-  `CRSElapsedTime` Int32,
-  `ActualElapsedTime` Int32,
-  `AirTime` Int32,
-  `Flights` Int32,
-  `Distance` Int32,
-  `DistanceGroup` UInt8,
-  `CarrierDelay` Int32,
-  `WeatherDelay` Int32,
-  `NASDelay` Int32,
-  `SecurityDelay` Int32,
-  `LateAircraftDelay` Int32,
-  `FirstDepTime` String,
-  `TotalAddGTime` String,
-  `LongestAddGTime` String,
-  `DivAirportLandings` String,
-  `DivReachedDest` String,
-  `DivActualElapsedTime` String,
-  `DivArrDelay` String,
-  `DivDistance` String,
-  `Div1Airport` String,
-  `Div1AirportID` Int32,
-  `Div1AirportSeqID` Int32,
-  `Div1WheelsOn` String,
-  `Div1TotalGTime` String,
-  `Div1LongestGTime` String,
-  `Div1WheelsOff` String,
-  `Div1TailNum` String,
-  `Div2Airport` String,
-  `Div2AirportID` Int32,
-  `Div2AirportSeqID` Int32,
-  `Div2WheelsOn` String,
-  `Div2TotalGTime` String,
-  `Div2LongestGTime` String,
-  `Div2WheelsOff` String,
-  `Div2TailNum` String,
-  `Div3Airport` String,
-  `Div3AirportID` Int32,
-  `Div3AirportSeqID` Int32,
-  `Div3WheelsOn` String,
-  `Div3TotalGTime` String,
-  `Div3LongestGTime` String,
-  `Div3WheelsOff` String,
-  `Div3TailNum` String,
-  `Div4Airport` String,
-  `Div4AirportID` Int32,
-  `Div4AirportSeqID` Int32,
-  `Div4WheelsOn` String,
-  `Div4TotalGTime` String,
-  `Div4LongestGTime` String,
-  `Div4WheelsOff` String,
-  `Div4TailNum` String,
-  `Div5Airport` String,
-  `Div5AirportID` Int32,
-  `Div5AirportSeqID` Int32,
-  `Div5WheelsOn` String,
-  `Div5TotalGTime` String,
-  `Div5LongestGTime` String,
-  `Div5WheelsOff` String,
-  `Div5TailNum` String
-) ENGINE = MergeTree
-PARTITION BY Year
-ORDER BY (Carrier, FlightDate)
-SETTINGS index_granularity = 8192;
-```
-
-Le chargement des données:
-
-``` bash
-$ for i in *.zip; do echo $i; unzip -cq $i '*.csv' | sed 's/\.00//g' | clickhouse-client --host=example-perftest01j --query="INSERT INTO ontime FORMAT CSVWithNames"; done
-```
-
-## Téléchargement des Partitions préparées {#download-of-prepared-partitions}
-
-``` bash
-$ curl -O https://datasets.clickhouse.tech/ontime/partitions/ontime.tar
-$ tar xvf ontime.tar -C /var/lib/clickhouse # path to ClickHouse data directory
-$ # check permissions of unpacked data, fix if required
-$ sudo service clickhouse-server restart
-$ clickhouse-client --query "select count(*) from datasets.ontime"
-```
-
-!!! info "Info"
-    Si vous exécutez les requêtes décrites ci-dessous, vous devez utiliser le nom complet de la table, `datasets.ontime`.
-
-## Requête {#queries}
-
-Q0.
-
-``` sql
-SELECT avg(c1)
-FROM
-(
-    SELECT Year, Month, count(*) AS c1
-    FROM ontime
-    GROUP BY Year, Month
-);
-```
-
-T1. Le nombre de vols par jour de l'année 2000 à 2008
-
-``` sql
-SELECT DayOfWeek, count(*) AS c
-FROM ontime
-WHERE Year>=2000 AND Year<=2008
-GROUP BY DayOfWeek
-ORDER BY c DESC;
-```
-
-T2. Le nombre de vols retardés de plus de 10 minutes, regroupés par jour de la semaine, pour 2000-2008
-
-``` sql
-SELECT DayOfWeek, count(*) AS c
-FROM ontime
-WHERE DepDelay>10 AND Year>=2000 AND Year<=2008
-GROUP BY DayOfWeek
-ORDER BY c DESC;
-```
-
-T3. Le nombre de retards par l'aéroport pour 2000-2008
-
-``` sql
-SELECT Origin, count(*) AS c
-FROM ontime
-WHERE DepDelay>10 AND Year>=2000 AND Year<=2008
-GROUP BY Origin
-ORDER BY c DESC
-LIMIT 10;
-```
-
-T4. Nombre de retards par transporteur pour 2007
-
-``` sql
-SELECT Carrier, count(*)
-FROM ontime
-WHERE DepDelay>10 AND Year=2007
-GROUP BY Carrier
-ORDER BY count(*) DESC;
-```
-
-Q5. Pourcentage de retards par transporteur pour 2007
-
-``` sql
-SELECT Carrier, c, c2, c*100/c2 as c3
-FROM
-(
-    SELECT
-        Carrier,
-        count(*) AS c
-    FROM ontime
-    WHERE DepDelay>10
-        AND Year=2007
-    GROUP BY Carrier
-)
-JOIN
-(
-    SELECT
-        Carrier,
-        count(*) AS c2
-    FROM ontime
-    WHERE Year=2007
-    GROUP BY Carrier
-) USING Carrier
-ORDER BY c3 DESC;
-```
-
-Meilleure version de la même requête:
-
-``` sql
-SELECT Carrier, avg(DepDelay>10)*100 AS c3
-FROM ontime
-WHERE Year=2007
-GROUP BY Carrier
-ORDER BY c3 DESC
-```
-
-Q6. La demande précédente pour une plus large gamme d'années, 2000-2008
-
-``` sql
-SELECT Carrier, c, c2, c*100/c2 as c3
-FROM
-(
-    SELECT
-        Carrier,
-        count(*) AS c
-    FROM ontime
-    WHERE DepDelay>10
-        AND Year>=2000 AND Year<=2008
-    GROUP BY Carrier
-)
-JOIN
-(
-    SELECT
-        Carrier,
-        count(*) AS c2
-    FROM ontime
-    WHERE Year>=2000 AND Year<=2008
-    GROUP BY Carrier
-) USING Carrier
-ORDER BY c3 DESC;
-```
-
-Meilleure version de la même requête:
-
-``` sql
-SELECT Carrier, avg(DepDelay>10)*100 AS c3
-FROM ontime
-WHERE Year>=2000 AND Year<=2008
-GROUP BY Carrier
-ORDER BY c3 DESC;
-```
-
-Q7. Pourcentage de vols retardés de plus de 10 minutes, par année
-
-``` sql
-SELECT Year, c1/c2
-FROM
-(
-    select
-        Year,
-        count(*)*100 as c1
-    from ontime
-    WHERE DepDelay>10
-    GROUP BY Year
-)
-JOIN
-(
-    select
-        Year,
-        count(*) as c2
-    from ontime
-    GROUP BY Year
-) USING (Year)
-ORDER BY Year;
-```
-
-Meilleure version de la même requête:
-
-``` sql
-SELECT Year, avg(DepDelay>10)*100
-FROM ontime
-GROUP BY Year
-ORDER BY Year;
-```
-
-Q8. Les destinations les plus populaires par le nombre de villes directement connectées pour différentes plages d'années
-
-``` sql
-SELECT DestCityName, uniqExact(OriginCityName) AS u
-FROM ontime
-WHERE Year >= 2000 and Year <= 2010
-GROUP BY DestCityName
-ORDER BY u DESC LIMIT 10;
-```
-
-Q9.
-
-``` sql
-SELECT Year, count(*) AS c1
-FROM ontime
-GROUP BY Year;
-```
-
-Q10.
-
-``` sql
-SELECT
-   min(Year), max(Year), Carrier, count(*) AS cnt,
-   sum(ArrDelayMinutes>30) AS flights_delayed,
-   round(sum(ArrDelayMinutes>30)/count(*),2) AS rate
-FROM ontime
-WHERE
-   DayOfWeek NOT IN (6,7) AND OriginState NOT IN ('AK', 'HI', 'PR', 'VI')
-   AND DestState NOT IN ('AK', 'HI', 'PR', 'VI')
-   AND FlightDate < '2010-01-01'
-GROUP by Carrier
-HAVING cnt>100000 and max(Year)>1990
-ORDER by rate DESC
-LIMIT 1000;
-```
-
-Bonus:
-
-``` sql
-SELECT avg(cnt)
-FROM
-(
-    SELECT Year,Month,count(*) AS cnt
-    FROM ontime
-    WHERE DepDel15=1
-    GROUP BY Year,Month
-);
-
-SELECT avg(c1) FROM
-(
-    SELECT Year,Month,count(*) AS c1
-    FROM ontime
-    GROUP BY Year,Month
-);
-
-SELECT DestCityName, uniqExact(OriginCityName) AS u
-FROM ontime
-GROUP BY DestCityName
-ORDER BY u DESC
-LIMIT 10;
-
-SELECT OriginCityName, DestCityName, count() AS c
-FROM ontime
-GROUP BY OriginCityName, DestCityName
-ORDER BY c DESC
-LIMIT 10;
-
-SELECT OriginCityName, count() AS c
-FROM ontime
-GROUP BY OriginCityName
-ORDER BY c DESC
-LIMIT 10;
-```
-
-Ce test de performance a été créé par Vadim Tkachenko. Voir:
-
--   https://www.percona.com/blog/2009/10/02/analyzing-air-traffic-performance-with-infobright-and-monetdb/
--   https://www.percona.com/blog/2009/10/26/air-traffic-queries-in-luciddb/
--   https://www.percona.com/blog/2009/11/02/air-traffic-queries-in-infinidb-early-alpha/
--   https://www.percona.com/blog/2014/04/21/using-apache-hadoop-and-impala-together-with-mysql-for-data-analysis/
--   https://www.percona.com/blog/2016/01/07/apache-spark-with-air-ontime-performance-data/
--   http://nickmakos.blogspot.ru/2012/08/analyzing-air-traffic-performance-with.html
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/ontime/) <!--hide-->
diff --git a/docs/fr/getting-started/example-datasets/star-schema.md b/docs/fr/getting-started/example-datasets/star-schema.md
deleted file mode 100644
index 6a32faaa3577..000000000000
--- a/docs/fr/getting-started/example-datasets/star-schema.md
+++ /dev/null
@@ -1,370 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 20
-toc_title: "R\xE9f\xE9rence Du Sch\xE9ma En \xC9toile"
----
-
-# Référence Du Schéma En Étoile {#star-schema-benchmark}
-
-Compilation de dbgen:
-
-``` bash
-$ git clone git@github.com:vadimtk/ssb-dbgen.git
-$ cd ssb-dbgen
-$ make
-```
-
-La production de données:
-
-!!! warning "Attention"
-    Avec `-s 100` dbgen génère 600 millions de lignes (67 Go), tandis que while `-s 1000` il génère 6 milliards de lignes (ce qui prend beaucoup de temps)
-
-``` bash
-$ ./dbgen -s 1000 -T c
-$ ./dbgen -s 1000 -T l
-$ ./dbgen -s 1000 -T p
-$ ./dbgen -s 1000 -T s
-$ ./dbgen -s 1000 -T d
-```
-
-Création de tables dans ClickHouse:
-
-``` sql
-CREATE TABLE customer
-(
-        C_CUSTKEY       UInt32,
-        C_NAME          String,
-        C_ADDRESS       String,
-        C_CITY          LowCardinality(String),
-        C_NATION        LowCardinality(String),
-        C_REGION        LowCardinality(String),
-        C_PHONE         String,
-        C_MKTSEGMENT    LowCardinality(String)
-)
-ENGINE = MergeTree ORDER BY (C_CUSTKEY);
-
-CREATE TABLE lineorder
-(
-    LO_ORDERKEY             UInt32,
-    LO_LINENUMBER           UInt8,
-    LO_CUSTKEY              UInt32,
-    LO_PARTKEY              UInt32,
-    LO_SUPPKEY              UInt32,
-    LO_ORDERDATE            Date,
-    LO_ORDERPRIORITY        LowCardinality(String),
-    LO_SHIPPRIORITY         UInt8,
-    LO_QUANTITY             UInt8,
-    LO_EXTENDEDPRICE        UInt32,
-    LO_ORDTOTALPRICE        UInt32,
-    LO_DISCOUNT             UInt8,
-    LO_REVENUE              UInt32,
-    LO_SUPPLYCOST           UInt32,
-    LO_TAX                  UInt8,
-    LO_COMMITDATE           Date,
-    LO_SHIPMODE             LowCardinality(String)
-)
-ENGINE = MergeTree PARTITION BY toYear(LO_ORDERDATE) ORDER BY (LO_ORDERDATE, LO_ORDERKEY);
-
-CREATE TABLE part
-(
-        P_PARTKEY       UInt32,
-        P_NAME          String,
-        P_MFGR          LowCardinality(String),
-        P_CATEGORY      LowCardinality(String),
-        P_BRAND         LowCardinality(String),
-        P_COLOR         LowCardinality(String),
-        P_TYPE          LowCardinality(String),
-        P_SIZE          UInt8,
-        P_CONTAINER     LowCardinality(String)
-)
-ENGINE = MergeTree ORDER BY P_PARTKEY;
-
-CREATE TABLE supplier
-(
-        S_SUPPKEY       UInt32,
-        S_NAME          String,
-        S_ADDRESS       String,
-        S_CITY          LowCardinality(String),
-        S_NATION        LowCardinality(String),
-        S_REGION        LowCardinality(String),
-        S_PHONE         String
-)
-ENGINE = MergeTree ORDER BY S_SUPPKEY;
-```
-
-Insertion de données:
-
-``` bash
-$ clickhouse-client --query "INSERT INTO customer FORMAT CSV" < customer.tbl
-$ clickhouse-client --query "INSERT INTO part FORMAT CSV" < part.tbl
-$ clickhouse-client --query "INSERT INTO supplier FORMAT CSV" < supplier.tbl
-$ clickhouse-client --query "INSERT INTO lineorder FORMAT CSV" < lineorder.tbl
-```
-
-Conversion “star schema” pour dénormalisée “flat schema”:
-
-``` sql
-SET max_memory_usage = 20000000000;
-
-CREATE TABLE lineorder_flat
-ENGINE = MergeTree
-PARTITION BY toYear(LO_ORDERDATE)
-ORDER BY (LO_ORDERDATE, LO_ORDERKEY) AS
-SELECT
-    l.LO_ORDERKEY AS LO_ORDERKEY,
-    l.LO_LINENUMBER AS LO_LINENUMBER,
-    l.LO_CUSTKEY AS LO_CUSTKEY,
-    l.LO_PARTKEY AS LO_PARTKEY,
-    l.LO_SUPPKEY AS LO_SUPPKEY,
-    l.LO_ORDERDATE AS LO_ORDERDATE,
-    l.LO_ORDERPRIORITY AS LO_ORDERPRIORITY,
-    l.LO_SHIPPRIORITY AS LO_SHIPPRIORITY,
-    l.LO_QUANTITY AS LO_QUANTITY,
-    l.LO_EXTENDEDPRICE AS LO_EXTENDEDPRICE,
-    l.LO_ORDTOTALPRICE AS LO_ORDTOTALPRICE,
-    l.LO_DISCOUNT AS LO_DISCOUNT,
-    l.LO_REVENUE AS LO_REVENUE,
-    l.LO_SUPPLYCOST AS LO_SUPPLYCOST,
-    l.LO_TAX AS LO_TAX,
-    l.LO_COMMITDATE AS LO_COMMITDATE,
-    l.LO_SHIPMODE AS LO_SHIPMODE,
-    c.C_NAME AS C_NAME,
-    c.C_ADDRESS AS C_ADDRESS,
-    c.C_CITY AS C_CITY,
-    c.C_NATION AS C_NATION,
-    c.C_REGION AS C_REGION,
-    c.C_PHONE AS C_PHONE,
-    c.C_MKTSEGMENT AS C_MKTSEGMENT,
-    s.S_NAME AS S_NAME,
-    s.S_ADDRESS AS S_ADDRESS,
-    s.S_CITY AS S_CITY,
-    s.S_NATION AS S_NATION,
-    s.S_REGION AS S_REGION,
-    s.S_PHONE AS S_PHONE,
-    p.P_NAME AS P_NAME,
-    p.P_MFGR AS P_MFGR,
-    p.P_CATEGORY AS P_CATEGORY,
-    p.P_BRAND AS P_BRAND,
-    p.P_COLOR AS P_COLOR,
-    p.P_TYPE AS P_TYPE,
-    p.P_SIZE AS P_SIZE,
-    p.P_CONTAINER AS P_CONTAINER
-FROM lineorder AS l
-INNER JOIN customer AS c ON c.C_CUSTKEY = l.LO_CUSTKEY
-INNER JOIN supplier AS s ON s.S_SUPPKEY = l.LO_SUPPKEY
-INNER JOIN part AS p ON p.P_PARTKEY = l.LO_PARTKEY;
-```
-
-Exécution des requêtes:
-
-Q1.1
-
-``` sql
-SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue
-FROM lineorder_flat
-WHERE toYear(LO_ORDERDATE) = 1993 AND LO_DISCOUNT BETWEEN 1 AND 3 AND LO_QUANTITY < 25;
-```
-
-Q1.2
-
-``` sql
-SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue
-FROM lineorder_flat
-WHERE toYYYYMM(LO_ORDERDATE) = 199401 AND LO_DISCOUNT BETWEEN 4 AND 6 AND LO_QUANTITY BETWEEN 26 AND 35;
-```
-
-Q1.3
-
-``` sql
-SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue
-FROM lineorder_flat
-WHERE toISOWeek(LO_ORDERDATE) = 6 AND toYear(LO_ORDERDATE) = 1994
-  AND LO_DISCOUNT BETWEEN 5 AND 7 AND LO_QUANTITY BETWEEN 26 AND 35;
-```
-
-Q2.1
-
-``` sql
-SELECT
-    sum(LO_REVENUE),
-    toYear(LO_ORDERDATE) AS year,
-    P_BRAND
-FROM lineorder_flat
-WHERE P_CATEGORY = 'MFGR#12' AND S_REGION = 'AMERICA'
-GROUP BY
-    year,
-    P_BRAND
-ORDER BY
-    year,
-    P_BRAND;
-```
-
-Q2.2
-
-``` sql
-SELECT
-    sum(LO_REVENUE),
-    toYear(LO_ORDERDATE) AS year,
-    P_BRAND
-FROM lineorder_flat
-WHERE P_BRAND >= 'MFGR#2221' AND P_BRAND <= 'MFGR#2228' AND S_REGION = 'ASIA'
-GROUP BY
-    year,
-    P_BRAND
-ORDER BY
-    year,
-    P_BRAND;
-```
-
-Q2.3
-
-``` sql
-SELECT
-    sum(LO_REVENUE),
-    toYear(LO_ORDERDATE) AS year,
-    P_BRAND
-FROM lineorder_flat
-WHERE P_BRAND = 'MFGR#2239' AND S_REGION = 'EUROPE'
-GROUP BY
-    year,
-    P_BRAND
-ORDER BY
-    year,
-    P_BRAND;
-```
-
-Q3.1
-
-``` sql
-SELECT
-    C_NATION,
-    S_NATION,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE C_REGION = 'ASIA' AND S_REGION = 'ASIA' AND year >= 1992 AND year <= 1997
-GROUP BY
-    C_NATION,
-    S_NATION,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q3.2
-
-``` sql
-SELECT
-    C_CITY,
-    S_CITY,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE C_NATION = 'UNITED STATES' AND S_NATION = 'UNITED STATES' AND year >= 1992 AND year <= 1997
-GROUP BY
-    C_CITY,
-    S_CITY,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q3.3
-
-``` sql
-SELECT
-    C_CITY,
-    S_CITY,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND year >= 1992 AND year <= 1997
-GROUP BY
-    C_CITY,
-    S_CITY,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q3.4
-
-``` sql
-SELECT
-    C_CITY,
-    S_CITY,
-    toYear(LO_ORDERDATE) AS year,
-    sum(LO_REVENUE) AS revenue
-FROM lineorder_flat
-WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND toYYYYMM(LO_ORDERDATE) = 199712
-GROUP BY
-    C_CITY,
-    S_CITY,
-    year
-ORDER BY
-    year ASC,
-    revenue DESC;
-```
-
-Q4.1
-
-``` sql
-SELECT
-    toYear(LO_ORDERDATE) AS year,
-    C_NATION,
-    sum(LO_REVENUE - LO_SUPPLYCOST) AS profit
-FROM lineorder_flat
-WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2')
-GROUP BY
-    year,
-    C_NATION
-ORDER BY
-    year ASC,
-    C_NATION ASC;
-```
-
-Q4.2
-
-``` sql
-SELECT
-    toYear(LO_ORDERDATE) AS year,
-    S_NATION,
-    P_CATEGORY,
-    sum(LO_REVENUE - LO_SUPPLYCOST) AS profit
-FROM lineorder_flat
-WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (year = 1997 OR year = 1998) AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2')
-GROUP BY
-    year,
-    S_NATION,
-    P_CATEGORY
-ORDER BY
-    year ASC,
-    S_NATION ASC,
-    P_CATEGORY ASC;
-```
-
-Q4.3
-
-``` sql
-SELECT
-    toYear(LO_ORDERDATE) AS year,
-    S_CITY,
-    P_BRAND,
-    sum(LO_REVENUE - LO_SUPPLYCOST) AS profit
-FROM lineorder_flat
-WHERE S_NATION = 'UNITED STATES' AND (year = 1997 OR year = 1998) AND P_CATEGORY = 'MFGR#14'
-GROUP BY
-    year,
-    S_CITY,
-    P_BRAND
-ORDER BY
-    year ASC,
-    S_CITY ASC,
-    P_BRAND ASC;
-```
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/star_schema/) <!--hide-->
diff --git a/docs/fr/getting-started/example-datasets/wikistat.md b/docs/fr/getting-started/example-datasets/wikistat.md
deleted file mode 100644
index cecd5ef07287..000000000000
--- a/docs/fr/getting-started/example-datasets/wikistat.md
+++ /dev/null
@@ -1,35 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 18
-toc_title: WikiStat
----
-
-# WikiStat {#wikistat}
-
-Voir: http://dumps.wikimedia.org/other/pagecounts-raw/
-
-Création d'une table:
-
-``` sql
-CREATE TABLE wikistat
-(
-    date Date,
-    time DateTime,
-    project String,
-    subproject String,
-    path String,
-    hits UInt64,
-    size UInt64
-) ENGINE = MergeTree(date, (path, time), 8192);
-```
-
-Le chargement des données:
-
-``` bash
-$ for i in {2007..2016}; do for j in {01..12}; do echo $i-$j >&2; curl -sSL "http://dumps.wikimedia.org/other/pagecounts-raw/$i/$i-$j/" | grep -oE 'pagecounts-[0-9]+-[0-9]+\.gz'; done; done | sort | uniq | tee links.txt
-$ cat links.txt | while read link; do wget http://dumps.wikimedia.org/other/pagecounts-raw/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1/')/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1-\2/')/$link; done
-$ ls -1 /opt/wikistat/ | grep gz | while read i; do echo $i; gzip -cd /opt/wikistat/$i | ./wikistat-loader --time="$(echo -n $i | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})([0-9]{2})-([0-9]{2})([0-9]{2})([0-9]{2})\.gz/\1-\2-\3 \4-00-00/')" | clickhouse-client --query="INSERT INTO wikistat FORMAT TabSeparated"; done
-```
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/example_datasets/wikistat/) <!--hide-->
diff --git a/docs/fr/getting-started/index.md b/docs/fr/getting-started/index.md
deleted file mode 100644
index 3b90ff92b896..000000000000
--- a/docs/fr/getting-started/index.md
+++ /dev/null
@@ -1,17 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Prise En Main
-toc_hidden: true
-toc_priority: 8
-toc_title: "cach\xE9s"
----
-
-# Prise En Main {#getting-started}
-
-Si vous êtes nouveau à ClickHouse et que vous voulez obtenir un sentiment pratique de sa performance, tout d'abord, vous devez passer par le [processus d'installation](install.md). Après cela, vous pouvez:
-
--   [Passez par tutoriel détaillé](tutorial.md)
--   [Expérience avec des exemples de jeux de données](example-datasets/ontime.md)
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/) <!--hide-->
diff --git a/docs/fr/getting-started/install.md b/docs/fr/getting-started/install.md
deleted file mode 100644
index db014bec9741..000000000000
--- a/docs/fr/getting-started/install.md
+++ /dev/null
@@ -1,182 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 11
-toc_title: Installation
----
-
-# Installation {#installation}
-
-## Configuration Système Requise {#system-requirements}
-
-ClickHouse peut fonctionner sur N'importe quel Linux, FreeBSD ou Mac OS X avec une architecture CPU x86_64, AArch64 ou PowerPC64LE.
-
-Les binaires pré-construits officiels sont généralement compilés pour le jeu d'instructions x86_64 et leverage SSE 4.2, donc sauf indication contraire, l'utilisation du processeur qui le prend en charge devient une exigence système supplémentaire. Voici la commande pour vérifier si le processeur actuel prend en charge SSE 4.2:
-
-``` bash
-$ grep -q sse4_2 /proc/cpuinfo && echo "SSE 4.2 supported" || echo "SSE 4.2 not supported"
-```
-
-Pour exécuter ClickHouse sur des processeurs qui ne prennent pas en charge SSE 4.2 ou qui ont une architecture AArch64 ou PowerPC64LE, vous devez [construire ClickHouse à partir de sources](#from-sources) avec des ajustements de configuration appropriés.
-
-## Options D'Installation Disponibles {#available-installation-options}
-
-### À partir de paquets DEB {#install-from-deb-packages}
-
-Il est recommandé d'utiliser officiel pré-compilé `deb` Paquets Pour Debian ou Ubuntu. Exécutez ces commandes pour installer les paquets:
-
-``` bash
-{% include 'install/deb.sh' %}
-```
-
-Si vous souhaitez utiliser la version la plus récente, remplacer `stable` avec `testing` (ceci est recommandé pour vos environnements de test).
-
-Vous pouvez également télécharger et installer des paquets manuellement à partir de [ici](https://repo.clickhouse.tech/deb/stable/main/).
-
-#### Paquet {#packages}
-
--   `clickhouse-common-static` — Installs ClickHouse compiled binary files.
--   `clickhouse-server` — Creates a symbolic link for `clickhouse-server` et installe la configuration du serveur par défaut.
--   `clickhouse-client` — Creates a symbolic link for `clickhouse-client` et d'autres outils. et installe les fichiers de configuration du client.
--   `clickhouse-common-static-dbg` — Installs ClickHouse compiled binary files with debug info.
-
-### À partir de paquets RPM {#from-rpm-packages}
-
-Il est recommandé d'utiliser officiel pré-compilé `rpm` packages pour CentOS, RedHat et toutes les autres distributions Linux basées sur rpm.
-
-Tout d'abord, vous devez ajouter le dépôt officiel:
-
-``` bash
-sudo yum install yum-utils
-sudo rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG
-sudo yum-config-manager --add-repo https://repo.clickhouse.tech/rpm/stable/x86_64
-```
-
-Si vous souhaitez utiliser la version la plus récente, remplacer `stable` avec `testing` (ceci est recommandé pour vos environnements de test). Le `prestable` la balise est parfois trop.
-
-Exécutez ensuite ces commandes pour installer les paquets:
-
-``` bash
-sudo yum install clickhouse-server clickhouse-client
-```
-
-Vous pouvez également télécharger et installer des paquets manuellement à partir de [ici](https://repo.clickhouse.tech/rpm/stable/x86_64).
-
-### À Partir D'Archives Tgz {#from-tgz-archives}
-
-Il est recommandé d'utiliser officiel pré-compilé `tgz` archives pour toutes les distributions Linux, où l'installation de `deb` ou `rpm` les emballages n'est pas possible.
-
-La version requise peut être téléchargée avec `curl` ou `wget` depuis le référentiel https://repo.clickhouse.tech/tgz/.
-Après cela, les archives téléchargées doivent être décompressées et installées avec des scripts d'installation. Exemple pour la dernière version:
-
-``` bash
-export LATEST_VERSION=`curl https://api.github.com/repos/ClickHouse/ClickHouse/tags 2>/dev/null | grep -Eo '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+' | head -n 1`
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-common-static-$LATEST_VERSION.tgz
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-common-static-dbg-$LATEST_VERSION.tgz
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-server-$LATEST_VERSION.tgz
-curl -O https://repo.clickhouse.tech/tgz/clickhouse-client-$LATEST_VERSION.tgz
-
-tar -xzvf clickhouse-common-static-$LATEST_VERSION.tgz
-sudo clickhouse-common-static-$LATEST_VERSION/install/doinst.sh
-
-tar -xzvf clickhouse-common-static-dbg-$LATEST_VERSION.tgz
-sudo clickhouse-common-static-dbg-$LATEST_VERSION/install/doinst.sh
-
-tar -xzvf clickhouse-server-$LATEST_VERSION.tgz
-sudo clickhouse-server-$LATEST_VERSION/install/doinst.sh
-sudo /etc/init.d/clickhouse-server start
-
-tar -xzvf clickhouse-client-$LATEST_VERSION.tgz
-sudo clickhouse-client-$LATEST_VERSION/install/doinst.sh
-```
-
-Pour les environnements de production, il est recommandé d'utiliser la dernière `stable`-version. Vous pouvez trouver son numéro sur la page GitHub https://github.com/ClickHouse/ClickHouse/tags avec postfix `-stable`.
-
-### À Partir De L'Image Docker {#from-docker-image}
-
-Pour exécuter Clickhouse à L'intérieur Docker suivez le guide sur [Hub Docker](https://hub.docker.com/r/yandex/clickhouse-server/). Ces images utilisent officiel `deb` les paquets à l'intérieur.
-
-### À Partir De Sources {#from-sources}
-
-Pour compiler manuellement ClickHouse, suivez les instructions pour [Linux](../development/build.md) ou [Mac OS X](../development/build-osx.md).
-
-Vous pouvez compiler des paquets et les installer ou utiliser des programmes sans installer de paquets. En outre, en construisant manuellement, vous pouvez désactiver L'exigence SSE 4.2 ou construire pour les processeurs AArch64.
-
-      Client: programs/clickhouse-client
-      Server: programs/clickhouse-server
-
-Vous devrez créer un dossier de données et de métadonnées et `chown` pour l'utilisateur souhaité. Leurs chemins peuvent être modifiés dans la configuration du serveur (src / programs / server / config.xml), par défaut, ils sont:
-
-      /opt/clickhouse/data/default/
-      /opt/clickhouse/metadata/default/
-
-Sur Gentoo, vous pouvez simplement utiliser `emerge clickhouse` pour installer ClickHouse à partir de sources.
-
-## Lancer {#launch}
-
-Pour démarrer le serveur en tant que démon, exécutez:
-
-``` bash
-$ sudo service clickhouse-server start
-```
-
-Si vous n'avez pas `service` commande, exécuter comme
-
-``` bash
-$ sudo /etc/init.d/clickhouse-server start
-```
-
-Voir les journaux dans le `/var/log/clickhouse-server/` répertoire.
-
-Si le serveur ne démarre pas, vérifiez les configurations dans le fichier `/etc/clickhouse-server/config.xml`.
-
-Vous pouvez également lancer manuellement le serveur à partir de la console:
-
-``` bash
-$ clickhouse-server --config-file=/etc/clickhouse-server/config.xml
-```
-
-Dans ce cas, le journal sera imprimé sur la console, ce qui est pratique lors du développement.
-Si le fichier de configuration se trouve dans le répertoire courant, vous n'avez pas besoin `--config-file` paramètre. Par défaut, il utilise `./config.xml`.
-
-ClickHouse prend en charge les paramètres de restriction d'accès. Ils sont situés dans la `users.xml` fichier (à côté de `config.xml`).
-Par défaut, l'accès est autorisé depuis n'importe où pour `default` l'utilisateur, sans un mot de passe. Voir `user/default/networks`.
-Pour plus d'informations, consultez la section [“Configuration Files”](../operations/configuration-files.md).
-
-Après le lancement du serveur, vous pouvez utiliser le client de ligne de commande pour vous y connecter:
-
-``` bash
-$ clickhouse-client
-```
-
-Par défaut, il se connecte à `localhost:9000` au nom de l'utilisateur `default` sans un mot de passe. Il peut également être utilisé pour se connecter à un serveur distant en utilisant `--host` argument.
-
-Le terminal doit utiliser L'encodage UTF-8.
-Pour plus d'informations, consultez la section [“Command-line client”](../interfaces/cli.md).
-
-Exemple:
-
-``` bash
-$ ./clickhouse-client
-ClickHouse client version 0.0.18749.
-Connecting to localhost:9000.
-Connected to ClickHouse server version 0.0.18749.
-
-:) SELECT 1
-
-SELECT 1
-
-┌─1─┐
-│ 1 │
-└───┘
-
-1 rows in set. Elapsed: 0.003 sec.
-
-:)
-```
-
-**Félicitations, le système fonctionne!**
-
-Pour continuer à expérimenter, vous pouvez télécharger l'un des jeux de données de test ou passer par [tutoriel](https://clickhouse.tech/tutorial.html).
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/install/) <!--hide-->
diff --git a/docs/fr/getting-started/playground.md b/docs/fr/getting-started/playground.md
deleted file mode 100644
index 6ddded137821..000000000000
--- a/docs/fr/getting-started/playground.md
+++ /dev/null
@@ -1,48 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 14
-toc_title: "R\xE9cr\xE9ation"
----
-
-# Clickhouse Aire De Jeux {#clickhouse-playground}
-
-[Clickhouse Aire De Jeux](https://play.clickhouse.tech?file=welcome) permet aux utilisateurs d'expérimenter avec ClickHouse en exécutant des requêtes instantanément, sans configurer leur serveur ou leur cluster.
-Plusieurs exemples de jeux de données sont disponibles dans le terrain de jeu ainsi que des exemples de requêtes qui montrent les fonctionnalités de ClickHouse.
-
-Les requêtes sont exécutées comme un utilisateur en lecture seule. Cela implique certaines limites:
-
--   Les requêtes DDL ne sont pas autorisées
--   Les requêtes D'insertion ne sont pas autorisées
-
-Les paramètres suivants sont également appliquées:
-- [`max_result_bytes=10485760`](../operations/settings/query_complexity/#max-result-bytes)
-- [`max_result_rows=2000`](../operations/settings/query_complexity/#setting-max_result_rows)
-- [`result_overflow_mode=break`](../operations/settings/query_complexity/#result-overflow-mode)
-- [`max_execution_time=60000`](../operations/settings/query_complexity/#max-execution-time)
-
-Clickhouse Playground donne l'expérience du m2.Petite
-[Service géré pour ClickHouse](https://cloud.yandex.com/services/managed-clickhouse)
-exemple hébergé dans [Yandex.Nuage](https://cloud.yandex.com/).
-Plus d'informations sur [les fournisseurs de cloud](../commercial/cloud.md).
-
-Clickhouse Playground interface web fait des demandes via ClickHouse [HTTP API](../interfaces/http.md).
-Le backend Playground est juste un cluster ClickHouse sans aucune application Côté Serveur supplémentaire.
-ClickHouse HTTPS endpoint est également disponible dans le cadre du terrain de jeu.
-
-Vous pouvez effectuer des requêtes sur playground en utilisant n'importe quel client HTTP, par exemple [curl](https://curl.haxx.se) ou [wget](https://www.gnu.org/software/wget/), ou configurer une connexion en utilisant [JDBC](../interfaces/jdbc.md) ou [ODBC](../interfaces/odbc.md) pilote.
-Plus d'informations sur les produits logiciels qui prennent en charge ClickHouse est disponible [ici](../interfaces/index.md).
-
-| Paramètre    | Valeur                                        |
-|:-------------|:----------------------------------------------|
-| Terminaison  | https://play-api.clickhouse.technologie: 8443 |
-| Utilisateur  | `playground`                                  |
-| Mot de passe | `clickhouse`                                  |
-
-Notez que ce paramètre nécessite une connexion sécurisée.
-
-Exemple:
-
-``` bash
-curl "https://play-api.clickhouse.tech:8443/?query=SELECT+'Play+ClickHouse!';&user=playground&password=clickhouse&database=datasets"
-```
diff --git a/docs/fr/getting-started/tutorial.md b/docs/fr/getting-started/tutorial.md
deleted file mode 100644
index 839af7aeb845..000000000000
--- a/docs/fr/getting-started/tutorial.md
+++ /dev/null
@@ -1,664 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 12
-toc_title: Tutoriel
----
-
-# Tutoriel ClickHouse {#clickhouse-tutorial}
-
-## À quoi S'attendre de ce tutoriel? {#what-to-expect-from-this-tutorial}
-
-En parcourant ce tutoriel, vous apprendrez à configurer un cluster ClickHouse simple. Ce sera petit, mais tolérant aux pannes et évolutif. Ensuite, nous utiliserons l'un des exemples de jeux de données pour le remplir de données et exécuter des requêtes de démonstration.
-
-## Configuration De Noeud Unique {#single-node-setup}
-
-Pour retarder les complexités d'un environnement distribué, nous allons commencer par déployer ClickHouse sur un seul serveur ou une machine virtuelle. ClickHouse est généralement installé à partir de [deb](install.md#install-from-deb-packages) ou [tr / min](install.md#from-rpm-packages) les paquets, mais il y a [alternative](install.md#from-docker-image) pour les systèmes d'exploitation qui ne sont pas les soutenir.
-
-Par exemple, vous avez choisi `deb` paquets et exécutés:
-
-``` bash
-{% include 'install/deb.sh' %}
-```
-
-Quelles sont les paquets installés:
-
--   `clickhouse-client` package contient [clickhouse-client](../interfaces/cli.md) application, client interactif de console de ClickHouse.
--   `clickhouse-common` paquet contient un fichier exécutable ClickHouse.
--   `clickhouse-server` package contient des fichiers de configuration pour exécuter ClickHouse en tant que serveur.
-
-Les fichiers de configuration du serveur sont `/etc/clickhouse-server/`. Avant d'aller plus loin, notez le `<path>` élément `config.xml`. Path détermine l'emplacement pour le stockage des données, il doit donc être situé sur le volume avec une grande capacité de disque; la valeur par défaut est `/var/lib/clickhouse/`. Si vous souhaitez ajuster la configuration, il n'est pas pratique de modifier directement `config.xml` fichier, considérant qu'il pourrait obtenir réécrit sur les futures mises à jour du progiciel. La façon recommandée de remplacer les éléments de configuration est de créer [fichiers dans config.d: répertoire](../operations/configuration-files.md) qui servent de “patches” config.XML.
-
-Comme vous l'avez peut-être remarqué, `clickhouse-server` n'est pas lancé automatiquement après l'installation du paquet. Il ne sera pas redémarré automatiquement après les mises à jour, non plus. La façon dont vous démarrez le serveur dépend de votre système d'initialisation, généralement, c'est:
-
-``` bash
-sudo service clickhouse-server start
-```
-
-ou
-
-``` bash
-sudo /etc/init.d/clickhouse-server start
-```
-
-L'emplacement par défaut pour les journaux du serveur est `/var/log/clickhouse-server/`. Le serveur est prêt à gérer les connexions client une fois `Ready for connections` message.
-
-Une fois l' `clickhouse-server` est opérationnel, nous pouvons utiliser `clickhouse-client` pour se connecter au serveur et effectuer des tests de requêtes comme `SELECT "Hello, world!";`.
-
-<details markdown="1">
-
-<summary>Conseils rapides pour clickhouse-client</summary>
-
-Mode interactif:
-
-``` bash
-clickhouse-client
-clickhouse-client --host=... --port=... --user=... --password=...
-```
-
-Activer les requêtes multilignes:
-
-``` bash
-clickhouse-client -m
-clickhouse-client --multiline
-```
-
-Exécuter des requêtes en mode batch:
-
-``` bash
-clickhouse-client --query='SELECT 1'
-echo 'SELECT 1' | clickhouse-client
-clickhouse-client <<< 'SELECT 1'
-```
-
-Insérer des données à partir d'un fichier au format spécifié:
-
-``` bash
-clickhouse-client --query='INSERT INTO table VALUES' < data.txt
-clickhouse-client --query='INSERT INTO table FORMAT TabSeparated' < data.tsv
-```
-
-</details>
-
-## Importer Un Échantillon De Données {#import-sample-dataset}
-
-Maintenant, il est temps de remplir notre serveur ClickHouse avec quelques exemples de données. Dans ce tutoriel, nous allons utiliser les données anonymisées de Yandex.Metrica, le premier service qui exécute ClickHouse en production avant de devenir open-source (plus à ce sujet dans [section d'histoire](../introduction/history.md)). Il y a [plusieurs façons d'importer Yandex.Metrica dataset](example-datasets/metrica.md), et pour le bien du tutoriel, nous irons avec le plus réaliste.
-
-### Télécharger et extraire les données de la Table {#download-and-extract-table-data}
-
-``` bash
-curl https://datasets.clickhouse.tech/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` > hits_v1.tsv
-curl https://datasets.clickhouse.tech/visits/tsv/visits_v1.tsv.xz | unxz --threads=`nproc` > visits_v1.tsv
-```
-
-Les fichiers extraits ont une taille d'environ 10 Go.
-
-### Créer Des Tables {#create-tables}
-
-Comme dans la plupart des systèmes de gestion de bases de données, clickhouse regroupe logiquement les tables en “databases”. Il y a un `default` base de données, mais nous allons en créer une nouvelle nommée `tutorial`:
-
-``` bash
-clickhouse-client --query "CREATE DATABASE IF NOT EXISTS tutorial"
-```
-
-La syntaxe pour créer des tables est beaucoup plus compliquée par rapport aux bases de données (voir [référence](../sql-reference/statements/create.md). En général `CREATE TABLE` déclaration doit spécifier trois choses clés:
-
-1.  Nom de la table à créer.
-2.  Table schema, i.e. list of columns and their [types de données](../sql-reference/data-types/index.md).
-3.  [Tableau moteur](../engines/table-engines/index.md) et ses paramètres, qui détermine tous les détails sur la façon dont les requêtes à cette table seront physiquement exécutées.
-
-Yandex.Metrica est un service d'analyse web, et l'exemple de jeu de données ne couvre pas toutes ses fonctionnalités, il n'y a donc que deux tables à créer:
-
--   `hits` est un tableau avec chaque action effectuée par tous les utilisateurs sur tous les sites couverts par le service.
--   `visits` est une table qui contient des sessions pré-construites au lieu d'actions individuelles.
-
-Voyons et exécutons les vraies requêtes create table pour ces tables:
-
-``` sql
-CREATE TABLE tutorial.hits_v1
-(
-    `WatchID` UInt64,
-    `JavaEnable` UInt8,
-    `Title` String,
-    `GoodEvent` Int16,
-    `EventTime` DateTime,
-    `EventDate` Date,
-    `CounterID` UInt32,
-    `ClientIP` UInt32,
-    `ClientIP6` FixedString(16),
-    `RegionID` UInt32,
-    `UserID` UInt64,
-    `CounterClass` Int8,
-    `OS` UInt8,
-    `UserAgent` UInt8,
-    `URL` String,
-    `Referer` String,
-    `URLDomain` String,
-    `RefererDomain` String,
-    `Refresh` UInt8,
-    `IsRobot` UInt8,
-    `RefererCategories` Array(UInt16),
-    `URLCategories` Array(UInt16),
-    `URLRegions` Array(UInt32),
-    `RefererRegions` Array(UInt32),
-    `ResolutionWidth` UInt16,
-    `ResolutionHeight` UInt16,
-    `ResolutionDepth` UInt8,
-    `FlashMajor` UInt8,
-    `FlashMinor` UInt8,
-    `FlashMinor2` String,
-    `NetMajor` UInt8,
-    `NetMinor` UInt8,
-    `UserAgentMajor` UInt16,
-    `UserAgentMinor` FixedString(2),
-    `CookieEnable` UInt8,
-    `JavascriptEnable` UInt8,
-    `IsMobile` UInt8,
-    `MobilePhone` UInt8,
-    `MobilePhoneModel` String,
-    `Params` String,
-    `IPNetworkID` UInt32,
-    `TraficSourceID` Int8,
-    `SearchEngineID` UInt16,
-    `SearchPhrase` String,
-    `AdvEngineID` UInt8,
-    `IsArtifical` UInt8,
-    `WindowClientWidth` UInt16,
-    `WindowClientHeight` UInt16,
-    `ClientTimeZone` Int16,
-    `ClientEventTime` DateTime,
-    `SilverlightVersion1` UInt8,
-    `SilverlightVersion2` UInt8,
-    `SilverlightVersion3` UInt32,
-    `SilverlightVersion4` UInt16,
-    `PageCharset` String,
-    `CodeVersion` UInt32,
-    `IsLink` UInt8,
-    `IsDownload` UInt8,
-    `IsNotBounce` UInt8,
-    `FUniqID` UInt64,
-    `HID` UInt32,
-    `IsOldCounter` UInt8,
-    `IsEvent` UInt8,
-    `IsParameter` UInt8,
-    `DontCountHits` UInt8,
-    `WithHash` UInt8,
-    `HitColor` FixedString(1),
-    `UTCEventTime` DateTime,
-    `Age` UInt8,
-    `Sex` UInt8,
-    `Income` UInt8,
-    `Interests` UInt16,
-    `Robotness` UInt8,
-    `GeneralInterests` Array(UInt16),
-    `RemoteIP` UInt32,
-    `RemoteIP6` FixedString(16),
-    `WindowName` Int32,
-    `OpenerName` Int32,
-    `HistoryLength` Int16,
-    `BrowserLanguage` FixedString(2),
-    `BrowserCountry` FixedString(2),
-    `SocialNetwork` String,
-    `SocialAction` String,
-    `HTTPError` UInt16,
-    `SendTiming` Int32,
-    `DNSTiming` Int32,
-    `ConnectTiming` Int32,
-    `ResponseStartTiming` Int32,
-    `ResponseEndTiming` Int32,
-    `FetchTiming` Int32,
-    `RedirectTiming` Int32,
-    `DOMInteractiveTiming` Int32,
-    `DOMContentLoadedTiming` Int32,
-    `DOMCompleteTiming` Int32,
-    `LoadEventStartTiming` Int32,
-    `LoadEventEndTiming` Int32,
-    `NSToDOMContentLoadedTiming` Int32,
-    `FirstPaintTiming` Int32,
-    `RedirectCount` Int8,
-    `SocialSourceNetworkID` UInt8,
-    `SocialSourcePage` String,
-    `ParamPrice` Int64,
-    `ParamOrderID` String,
-    `ParamCurrency` FixedString(3),
-    `ParamCurrencyID` UInt16,
-    `GoalsReached` Array(UInt32),
-    `OpenstatServiceName` String,
-    `OpenstatCampaignID` String,
-    `OpenstatAdID` String,
-    `OpenstatSourceID` String,
-    `UTMSource` String,
-    `UTMMedium` String,
-    `UTMCampaign` String,
-    `UTMContent` String,
-    `UTMTerm` String,
-    `FromTag` String,
-    `HasGCLID` UInt8,
-    `RefererHash` UInt64,
-    `URLHash` UInt64,
-    `CLID` UInt32,
-    `YCLID` UInt64,
-    `ShareService` String,
-    `ShareURL` String,
-    `ShareTitle` String,
-    `ParsedParams` Nested(
-        Key1 String,
-        Key2 String,
-        Key3 String,
-        Key4 String,
-        Key5 String,
-        ValueDouble Float64),
-    `IslandID` FixedString(16),
-    `RequestNum` UInt32,
-    `RequestTry` UInt8
-)
-ENGINE = MergeTree()
-PARTITION BY toYYYYMM(EventDate)
-ORDER BY (CounterID, EventDate, intHash32(UserID))
-SAMPLE BY intHash32(UserID)
-```
-
-``` sql
-CREATE TABLE tutorial.visits_v1
-(
-    `CounterID` UInt32,
-    `StartDate` Date,
-    `Sign` Int8,
-    `IsNew` UInt8,
-    `VisitID` UInt64,
-    `UserID` UInt64,
-    `StartTime` DateTime,
-    `Duration` UInt32,
-    `UTCStartTime` DateTime,
-    `PageViews` Int32,
-    `Hits` Int32,
-    `IsBounce` UInt8,
-    `Referer` String,
-    `StartURL` String,
-    `RefererDomain` String,
-    `StartURLDomain` String,
-    `EndURL` String,
-    `LinkURL` String,
-    `IsDownload` UInt8,
-    `TraficSourceID` Int8,
-    `SearchEngineID` UInt16,
-    `SearchPhrase` String,
-    `AdvEngineID` UInt8,
-    `PlaceID` Int32,
-    `RefererCategories` Array(UInt16),
-    `URLCategories` Array(UInt16),
-    `URLRegions` Array(UInt32),
-    `RefererRegions` Array(UInt32),
-    `IsYandex` UInt8,
-    `GoalReachesDepth` Int32,
-    `GoalReachesURL` Int32,
-    `GoalReachesAny` Int32,
-    `SocialSourceNetworkID` UInt8,
-    `SocialSourcePage` String,
-    `MobilePhoneModel` String,
-    `ClientEventTime` DateTime,
-    `RegionID` UInt32,
-    `ClientIP` UInt32,
-    `ClientIP6` FixedString(16),
-    `RemoteIP` UInt32,
-    `RemoteIP6` FixedString(16),
-    `IPNetworkID` UInt32,
-    `SilverlightVersion3` UInt32,
-    `CodeVersion` UInt32,
-    `ResolutionWidth` UInt16,
-    `ResolutionHeight` UInt16,
-    `UserAgentMajor` UInt16,
-    `UserAgentMinor` UInt16,
-    `WindowClientWidth` UInt16,
-    `WindowClientHeight` UInt16,
-    `SilverlightVersion2` UInt8,
-    `SilverlightVersion4` UInt16,
-    `FlashVersion3` UInt16,
-    `FlashVersion4` UInt16,
-    `ClientTimeZone` Int16,
-    `OS` UInt8,
-    `UserAgent` UInt8,
-    `ResolutionDepth` UInt8,
-    `FlashMajor` UInt8,
-    `FlashMinor` UInt8,
-    `NetMajor` UInt8,
-    `NetMinor` UInt8,
-    `MobilePhone` UInt8,
-    `SilverlightVersion1` UInt8,
-    `Age` UInt8,
-    `Sex` UInt8,
-    `Income` UInt8,
-    `JavaEnable` UInt8,
-    `CookieEnable` UInt8,
-    `JavascriptEnable` UInt8,
-    `IsMobile` UInt8,
-    `BrowserLanguage` UInt16,
-    `BrowserCountry` UInt16,
-    `Interests` UInt16,
-    `Robotness` UInt8,
-    `GeneralInterests` Array(UInt16),
-    `Params` Array(String),
-    `Goals` Nested(
-        ID UInt32,
-        Serial UInt32,
-        EventTime DateTime,
-        Price Int64,
-        OrderID String,
-        CurrencyID UInt32),
-    `WatchIDs` Array(UInt64),
-    `ParamSumPrice` Int64,
-    `ParamCurrency` FixedString(3),
-    `ParamCurrencyID` UInt16,
-    `ClickLogID` UInt64,
-    `ClickEventID` Int32,
-    `ClickGoodEvent` Int32,
-    `ClickEventTime` DateTime,
-    `ClickPriorityID` Int32,
-    `ClickPhraseID` Int32,
-    `ClickPageID` Int32,
-    `ClickPlaceID` Int32,
-    `ClickTypeID` Int32,
-    `ClickResourceID` Int32,
-    `ClickCost` UInt32,
-    `ClickClientIP` UInt32,
-    `ClickDomainID` UInt32,
-    `ClickURL` String,
-    `ClickAttempt` UInt8,
-    `ClickOrderID` UInt32,
-    `ClickBannerID` UInt32,
-    `ClickMarketCategoryID` UInt32,
-    `ClickMarketPP` UInt32,
-    `ClickMarketCategoryName` String,
-    `ClickMarketPPName` String,
-    `ClickAWAPSCampaignName` String,
-    `ClickPageName` String,
-    `ClickTargetType` UInt16,
-    `ClickTargetPhraseID` UInt64,
-    `ClickContextType` UInt8,
-    `ClickSelectType` Int8,
-    `ClickOptions` String,
-    `ClickGroupBannerID` Int32,
-    `OpenstatServiceName` String,
-    `OpenstatCampaignID` String,
-    `OpenstatAdID` String,
-    `OpenstatSourceID` String,
-    `UTMSource` String,
-    `UTMMedium` String,
-    `UTMCampaign` String,
-    `UTMContent` String,
-    `UTMTerm` String,
-    `FromTag` String,
-    `HasGCLID` UInt8,
-    `FirstVisit` DateTime,
-    `PredLastVisit` Date,
-    `LastVisit` Date,
-    `TotalVisits` UInt32,
-    `TraficSource` Nested(
-        ID Int8,
-        SearchEngineID UInt16,
-        AdvEngineID UInt8,
-        PlaceID UInt16,
-        SocialSourceNetworkID UInt8,
-        Domain String,
-        SearchPhrase String,
-        SocialSourcePage String),
-    `Attendance` FixedString(16),
-    `CLID` UInt32,
-    `YCLID` UInt64,
-    `NormalizedRefererHash` UInt64,
-    `SearchPhraseHash` UInt64,
-    `RefererDomainHash` UInt64,
-    `NormalizedStartURLHash` UInt64,
-    `StartURLDomainHash` UInt64,
-    `NormalizedEndURLHash` UInt64,
-    `TopLevelDomain` UInt64,
-    `URLScheme` UInt64,
-    `OpenstatServiceNameHash` UInt64,
-    `OpenstatCampaignIDHash` UInt64,
-    `OpenstatAdIDHash` UInt64,
-    `OpenstatSourceIDHash` UInt64,
-    `UTMSourceHash` UInt64,
-    `UTMMediumHash` UInt64,
-    `UTMCampaignHash` UInt64,
-    `UTMContentHash` UInt64,
-    `UTMTermHash` UInt64,
-    `FromHash` UInt64,
-    `WebVisorEnabled` UInt8,
-    `WebVisorActivity` UInt32,
-    `ParsedParams` Nested(
-        Key1 String,
-        Key2 String,
-        Key3 String,
-        Key4 String,
-        Key5 String,
-        ValueDouble Float64),
-    `Market` Nested(
-        Type UInt8,
-        GoalID UInt32,
-        OrderID String,
-        OrderPrice Int64,
-        PP UInt32,
-        DirectPlaceID UInt32,
-        DirectOrderID UInt32,
-        DirectBannerID UInt32,
-        GoodID String,
-        GoodName String,
-        GoodQuantity Int32,
-        GoodPrice Int64),
-    `IslandID` FixedString(16)
-)
-ENGINE = CollapsingMergeTree(Sign)
-PARTITION BY toYYYYMM(StartDate)
-ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID)
-SAMPLE BY intHash32(UserID)
-```
-
-Vous pouvez exécuter ces requêtes en utilisant le mode interactif de `clickhouse-client` (lancez - le simplement dans un terminal sans spécifier une requête à l'avance) ou essayez-en [interface de rechange](../interfaces/index.md) Si tu veux.
-
-Comme nous pouvons le voir, `hits_v1` utilise la [moteur MergeTree de base](../engines/table-engines/mergetree-family/mergetree.md) tandis que le `visits_v1` utilise la [Effondrer](../engines/table-engines/mergetree-family/collapsingmergetree.md) variante.
-
-### Importer Des Données {#import-data}
-
-L'importation de données vers ClickHouse se fait via [INSERT INTO](../sql-reference/statements/insert-into.md) requête comme dans de nombreuses autres bases de données SQL. Toutefois, les données sont généralement fournies dans l'une des [formats de sérialisation pris en charge](../interfaces/formats.md) plutôt `VALUES` clause (qui est également pris en charge).
-
-Les fichiers que nous avons téléchargés plus tôt sont au format séparé par des onglets, alors voici comment les importer via le client console:
-
-``` bash
-clickhouse-client --query "INSERT INTO tutorial.hits_v1 FORMAT TSV" --max_insert_block_size=100000 < hits_v1.tsv
-clickhouse-client --query "INSERT INTO tutorial.visits_v1 FORMAT TSV" --max_insert_block_size=100000 < visits_v1.tsv
-```
-
-ClickHouse a beaucoup de [les paramètres de tune](../operations/settings/index.md) et une façon de Les spécifier dans le client console est via des arguments, comme nous pouvons le voir avec `--max_insert_block_size`. La façon la plus simple de comprendre quels paramètres sont disponibles, que signifient-ils et quelles sont les valeurs par défaut est d'interroger le `system.settings` table:
-
-``` sql
-SELECT name, value, changed, description
-FROM system.settings
-WHERE name LIKE '%max_insert_b%'
-FORMAT TSV
-
-max_insert_block_size    1048576    0    "The maximum block size for insertion, if we control the creation of blocks for insertion."
-```
-
-Optionnellement, vous pouvez [OPTIMIZE](../sql-reference/statements/misc.md#misc_operations-optimize) les tables après l'importation. Les Tables configurées avec un moteur de MergeTree-family font toujours des fusions de parties de données en arrière-plan pour optimiser le stockage des données (ou au moins vérifier si cela a du sens). Ces requêtes forcent le moteur de table à optimiser le stockage dès maintenant au lieu d'un certain temps plus tard:
-
-``` bash
-clickhouse-client --query "OPTIMIZE TABLE tutorial.hits_v1 FINAL"
-clickhouse-client --query "OPTIMIZE TABLE tutorial.visits_v1 FINAL"
-```
-
-Ces requêtes démarrent une opération intensive D'E/S et de CPU, donc si la table reçoit systématiquement de nouvelles données, il est préférable de la laisser seule et de laisser les fusions s'exécuter en arrière-plan.
-
-Maintenant, nous pouvons vérifier si l'importation de table a réussi:
-
-``` bash
-clickhouse-client --query "SELECT COUNT(*) FROM tutorial.hits_v1"
-clickhouse-client --query "SELECT COUNT(*) FROM tutorial.visits_v1"
-```
-
-## Exemple De Requêtes {#example-queries}
-
-``` sql
-SELECT
-    StartURL AS URL,
-    AVG(Duration) AS AvgDuration
-FROM tutorial.visits_v1
-WHERE StartDate BETWEEN '2014-03-23' AND '2014-03-30'
-GROUP BY URL
-ORDER BY AvgDuration DESC
-LIMIT 10
-```
-
-``` sql
-SELECT
-    sum(Sign) AS visits,
-    sumIf(Sign, has(Goals.ID, 1105530)) AS goal_visits,
-    (100. * goal_visits) / visits AS goal_percent
-FROM tutorial.visits_v1
-WHERE (CounterID = 912887) AND (toYYYYMM(StartDate) = 201403) AND (domain(StartURL) = 'yandex.ru')
-```
-
-## Déploiement De Cluster {#cluster-deployment}
-
-Clickhouse cluster est un cluster homogène. Étapes pour configurer:
-
-1.  Installer clickhouse server sur toutes les machines du cluster
-2.  Configurer les configurations de cluster dans les fichiers de configuration
-3.  Créer des tables locales sur chaque instance
-4.  Créer un [Distribué table](../engines/table-engines/special/distributed.md)
-
-[Distribué table](../engines/table-engines/special/distributed.md) est en fait une sorte de “view” aux tables locales du cluster ClickHouse. SELECT query from a distributed table s'exécute à l'aide des ressources de tous les fragments du cluster. Vous pouvez spécifier des configurations pour plusieurs clusters et créer plusieurs tables distribuées fournissant des vues à différents clusters.
-
-Exemple de configuration pour un cluster avec trois fragments, une réplique chacun:
-
-``` xml
-<remote_servers>
-    <perftest_3shards_1replicas>
-        <shard>
-            <replica>
-                <host>example-perftest01j.yandex.ru</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-        <shard>
-            <replica>
-                <host>example-perftest02j.yandex.ru</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-        <shard>
-            <replica>
-                <host>example-perftest03j.yandex.ru</host>
-                <port>9000</port>
-            </replica>
-        </shard>
-    </perftest_3shards_1replicas>
-</remote_servers>
-```
-
-Pour plus de démonstration, créons une nouvelle table locale avec le même `CREATE TABLE` la requête que nous avons utilisé pour `hits_v1`, mais nom de table différent:
-
-``` sql
-CREATE TABLE tutorial.hits_local (...) ENGINE = MergeTree() ...
-```
-
-Création d'une table distribuée fournissant une vue dans les tables locales du cluster:
-
-``` sql
-CREATE TABLE tutorial.hits_all AS tutorial.hits_local
-ENGINE = Distributed(perftest_3shards_1replicas, tutorial, hits_local, rand());
-```
-
-Une pratique courante consiste à créer des tables distribuées similaires sur toutes les machines du cluster. Il permet d'exécuter des requêtes distribuées sur n'importe quelle machine du cluster. Il existe également une autre option pour créer une table distribuée temporaire pour une requête SELECT donnée en utilisant [distant](../sql-reference/table-functions/remote.md) table de fonction.
-
-Passons à l'exécution de [INSERT SELECT](../sql-reference/statements/insert-into.md) dans les Distribué table la table à plusieurs serveurs.
-
-``` sql
-INSERT INTO tutorial.hits_all SELECT * FROM tutorial.hits_v1;
-```
-
-!!! warning "Avis"
-    Cette approche ne convient pas au sharding de grandes tables. Il y a un outil séparé [clickhouse-copieur](../operations/utilities/clickhouse-copier.md) cela peut re-fragmenter de grandes tables arbitraires.
-
-Comme vous pouvez vous y attendre, les requêtes lourdes de calcul s'exécutent N fois plus vite si elles utilisent 3 serveurs au lieu d'un.
-
-Dans ce cas, nous avons utilisé un cluster avec 3 fragments, et chacun contient une seule réplique.
-
-Pour assurer la résilience dans un environnement de production, nous recommandons que chaque fragment contienne 2-3 répliques réparties entre plusieurs zones de disponibilité ou centres de données (ou au moins des racks). Notez que ClickHouse prend en charge un nombre illimité de répliques.
-
-Exemple de configuration pour un cluster d'un fragment contenant trois répliques:
-
-``` xml
-<remote_servers>
-    ...
-    <perftest_1shards_3replicas>
-        <shard>
-            <replica>
-                <host>example-perftest01j.yandex.ru</host>
-                <port>9000</port>
-             </replica>
-             <replica>
-                <host>example-perftest02j.yandex.ru</host>
-                <port>9000</port>
-             </replica>
-             <replica>
-                <host>example-perftest03j.yandex.ru</host>
-                <port>9000</port>
-             </replica>
-        </shard>
-    </perftest_1shards_3replicas>
-</remote_servers>
-```
-
-Pour activer la réplication native [ZooKeeper](http://zookeeper.apache.org/) est requis. ClickHouse s'occupe de la cohérence des données sur toutes les répliques et exécute automatiquement la procédure de restauration après l'échec. Il est recommandé de déployer le cluster ZooKeeper sur des serveurs séparés (où aucun autre processus, y compris ClickHouse, n'est en cours d'exécution).
-
-!!! note "Note"
-    ZooKeeper est pas une exigence stricte: dans certains cas simples, vous pouvez dupliquer les données par écrit dans tous les réplicas de votre code d'application. Cette approche est **pas** recommandé, dans ce cas, ClickHouse ne sera pas en mesure de garantir la cohérence des données sur toutes les répliques. Ainsi, il devient la responsabilité de votre application.
-
-Les emplacements ZooKeeper sont spécifiés dans le fichier de configuration:
-
-``` xml
-<zookeeper>
-    <node>
-        <host>zoo01.yandex.ru</host>
-        <port>2181</port>
-    </node>
-    <node>
-        <host>zoo02.yandex.ru</host>
-        <port>2181</port>
-    </node>
-    <node>
-        <host>zoo03.yandex.ru</host>
-        <port>2181</port>
-    </node>
-</zookeeper>
-```
-
-En outre, nous devons définir des macros pour identifier chaque fragment et chaque réplique utilisés lors de la création de la table:
-
-``` xml
-<macros>
-    <shard>01</shard>
-    <replica>01</replica>
-</macros>
-```
-
-S'il n'y a pas de répliques pour le moment lors de la création de la table répliquée, une nouvelle première réplique est instanciée. S'il existe déjà des répliques en direct, la nouvelle réplique clone les données de celles existantes. Vous avez la possibilité de créer toutes les tables répliquées d'abord, et ensuite insérer les données. Une autre option consiste à créer des répliques et à en ajouter d'autres Après ou pendant l'insertion des données.
-
-``` sql
-CREATE TABLE tutorial.hits_replica (...)
-ENGINE = ReplcatedMergeTree(
-    '/clickhouse_perftest/tables/{shard}/hits',
-    '{replica}'
-)
-...
-```
-
-Ici, nous utilisons [ReplicatedMergeTree](../engines/table-engines/mergetree-family/replication.md) tableau moteur. Dans les paramètres, nous spécifions le chemin Zookeeper contenant des identificateurs de fragments et de répliques.
-
-``` sql
-INSERT INTO tutorial.hits_replica SELECT * FROM tutorial.hits_local;
-```
-
-La réplication fonctionne en mode multi-maître. Les données peuvent être chargées dans n'importe quel réplica, et le système les synchronise ensuite automatiquement avec d'autres instances. La réplication est asynchrone, donc à un moment donné, toutes les répliques ne peuvent pas contenir de données récemment insérées. Au moins une réplique devrait être en place pour permettre l'ingestion de données. D'autres synchroniseront les données et répareront la cohérence une fois qu'ils redeviendront actifs. Notez que cette approche permet une faible possibilité de perte de données récemment insérées.
-
-[Article Original](https://clickhouse.tech/docs/en/getting_started/tutorial/) <!--hide-->
diff --git a/docs/fr/guides/apply-catboost-model.md b/docs/fr/guides/apply-catboost-model.md
deleted file mode 100644
index 1b6aae9bbbdf..000000000000
--- a/docs/fr/guides/apply-catboost-model.md
+++ /dev/null
@@ -1,239 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: "Application Des Mod\xE8les CatBoost"
----
-
-# Application D'un modèle Catboost dans ClickHouse {#applying-catboost-model-in-clickhouse}
-
-[CatBoost](https://catboost.ai) est une bibliothèque de dynamisation de gradient libre et open-source développée à [Yandex](https://yandex.com/company/) pour l'apprentissage automatique.
-
-Avec cette instruction, vous apprendrez à appliquer des modèles pré-formés dans ClickHouse en exécutant l'inférence de modèle à partir de SQL.
-
-Pour appliquer un modèle CatBoost dans ClickHouse:
-
-1.  [Créer une Table](#create-table).
-2.  [Insérez les données dans la Table](#insert-data-to-table).
-3.  [Intégrer CatBoost dans ClickHouse](#integrate-catboost-into-clickhouse) (Étape facultative).
-4.  [Exécutez L'inférence du modèle à partir de SQL](#run-model-inference).
-
-Pour plus d'informations sur la formation des modèles CatBoost, voir [Formation et application de modèles](https://catboost.ai/docs/features/training.html#training).
-
-## Préalable {#prerequisites}
-
-Si vous n'avez pas le [Docker](https://docs.docker.com/install/) pourtant, l'installer.
-
-!!! note "Note"
-    [Docker](https://www.docker.com) est une plate-forme logicielle qui vous permet de créer des conteneurs qui isolent une installation CatBoost et ClickHouse du reste du système.
-
-Avant d'appliquer un modèle CatBoost:
-
-**1.** Tirez la [Docker image](https://hub.docker.com/r/yandex/tutorial-catboost-clickhouse) à partir du registre:
-
-``` bash
-$ docker pull yandex/tutorial-catboost-clickhouse
-```
-
-Cette image Docker contient tout ce dont vous avez besoin pour exécuter CatBoost et ClickHouse: code, runtime, bibliothèques, variables d'environnement et fichiers de configuration.
-
-**2.** Assurez-vous que l'image Docker a été tirée avec succès:
-
-``` bash
-$ docker image ls
-REPOSITORY                            TAG                 IMAGE ID            CREATED             SIZE
-yandex/tutorial-catboost-clickhouse   latest              622e4d17945b        22 hours ago        1.37GB
-```
-
-**3.** Démarrer un conteneur Docker basé sur cette image:
-
-``` bash
-$ docker run -it -p 8888:8888 yandex/tutorial-catboost-clickhouse
-```
-
-## 1. Créer une Table {#create-table}
-
-Pour créer une table ClickHouse pour l'exemple de formation:
-
-**1.** Démarrez clickhouse console client en mode interactif:
-
-``` bash
-$ clickhouse client
-```
-
-!!! note "Note"
-    Le serveur ClickHouse est déjà en cours d'exécution dans le conteneur Docker.
-
-**2.** Créer la table à l'aide de la commande:
-
-``` sql
-:) CREATE TABLE amazon_train
-(
-    date Date MATERIALIZED today(),
-    ACTION UInt8,
-    RESOURCE UInt32,
-    MGR_ID UInt32,
-    ROLE_ROLLUP_1 UInt32,
-    ROLE_ROLLUP_2 UInt32,
-    ROLE_DEPTNAME UInt32,
-    ROLE_TITLE UInt32,
-    ROLE_FAMILY_DESC UInt32,
-    ROLE_FAMILY UInt32,
-    ROLE_CODE UInt32
-)
-ENGINE = MergeTree ORDER BY date
-```
-
-**3.** Quitter le client de la console ClickHouse:
-
-``` sql
-:) exit
-```
-
-## 2. Insérez les données dans la Table {#insert-data-to-table}
-
-Pour insérer les données:
-
-**1.** Exécutez la commande suivante:
-
-``` bash
-$ clickhouse client --host 127.0.0.1 --query 'INSERT INTO amazon_train FORMAT CSVWithNames' < ~/amazon/train.csv
-```
-
-**2.** Démarrez clickhouse console client en mode interactif:
-
-``` bash
-$ clickhouse client
-```
-
-**3.** Assurez-vous que les données ont été téléchargées:
-
-``` sql
-:) SELECT count() FROM amazon_train
-
-SELECT count()
-FROM amazon_train
-
-+-count()-+
-|   65538 |
-+-------+
-```
-
-## 3. Intégrer CatBoost dans ClickHouse {#integrate-catboost-into-clickhouse}
-
-!!! note "Note"
-    **Étape facultative.** L'image Docker contient tout ce dont vous avez besoin pour exécuter CatBoost et ClickHouse.
-
-Pour intégrer CatBoost dans ClickHouse:
-
-**1.** Construire la bibliothèque d'évaluation.
-
-Le moyen le plus rapide d'évaluer un modèle CatBoost est la compilation `libcatboostmodel.<so|dll|dylib>` bibliothèque. Pour plus d'informations sur la création de la bibliothèque, voir [Documentation CatBoost](https://catboost.ai/docs/concepts/c-plus-plus-api_dynamic-c-pluplus-wrapper.html).
-
-**2.** Créez un nouveau répertoire n'importe où et avec n'importe quel nom, par exemple, `data` et mettez la bibliothèque créée dedans. L'image Docker contient déjà la bibliothèque `data/libcatboostmodel.so`.
-
-**3.** Créez un nouveau répertoire pour le modèle de configuration n'importe où et avec n'importe quel nom, par exemple, `models`.
-
-**4.** Créez un fichier de configuration de modèle avec n'importe quel nom, par exemple, `models/amazon_model.xml`.
-
-**5.** Décrire la configuration du modèle:
-
-``` xml
-<models>
-    <model>
-        <!-- Model type. Now catboost only. -->
-        <type>catboost</type>
-        <!-- Model name. -->
-        <name>amazon</name>
-        <!-- Path to trained model. -->
-        <path>/home/catboost/tutorial/catboost_model.bin</path>
-        <!-- Update interval. -->
-        <lifetime>0</lifetime>
-    </model>
-</models>
-```
-
-**6.** Ajoutez le chemin D'accès à CatBoost et la configuration du modèle à la configuration de ClickHouse:
-
-``` xml
-<!-- File etc/clickhouse-server/config.d/models_config.xml. -->
-<catboost_dynamic_library_path>/home/catboost/data/libcatboostmodel.so</catboost_dynamic_library_path>
-<models_config>/home/catboost/models/*_model.xml</models_config>
-```
-
-## 4. Exécutez L'inférence du modèle à partir de SQL {#run-model-inference}
-
-Pour le modèle de test exécutez le client ClickHouse `$ clickhouse client`.
-
-Assurons nous que le modèle fonctionne:
-
-``` sql
-:) SELECT
-    modelEvaluate('amazon',
-                RESOURCE,
-                MGR_ID,
-                ROLE_ROLLUP_1,
-                ROLE_ROLLUP_2,
-                ROLE_DEPTNAME,
-                ROLE_TITLE,
-                ROLE_FAMILY_DESC,
-                ROLE_FAMILY,
-                ROLE_CODE) > 0 AS prediction,
-    ACTION AS target
-FROM amazon_train
-LIMIT 10
-```
-
-!!! note "Note"
-    Fonction [modelEvaluate](../sql-reference/functions/other-functions.md#function-modelevaluate) retourne tuple avec des prédictions brutes par classe pour les modèles multiclasse.
-
-Prédisons la probabilité:
-
-``` sql
-:) SELECT
-    modelEvaluate('amazon',
-                RESOURCE,
-                MGR_ID,
-                ROLE_ROLLUP_1,
-                ROLE_ROLLUP_2,
-                ROLE_DEPTNAME,
-                ROLE_TITLE,
-                ROLE_FAMILY_DESC,
-                ROLE_FAMILY,
-                ROLE_CODE) AS prediction,
-    1. / (1 + exp(-prediction)) AS probability,
-    ACTION AS target
-FROM amazon_train
-LIMIT 10
-```
-
-!!! note "Note"
-    Plus d'infos sur [exp()](../sql-reference/functions/math-functions.md) fonction.
-
-Calculons LogLoss sur l'échantillon:
-
-``` sql
-:) SELECT -avg(tg * log(prob) + (1 - tg) * log(1 - prob)) AS logloss
-FROM
-(
-    SELECT
-        modelEvaluate('amazon',
-                    RESOURCE,
-                    MGR_ID,
-                    ROLE_ROLLUP_1,
-                    ROLE_ROLLUP_2,
-                    ROLE_DEPTNAME,
-                    ROLE_TITLE,
-                    ROLE_FAMILY_DESC,
-                    ROLE_FAMILY,
-                    ROLE_CODE) AS prediction,
-        1. / (1. + exp(-prediction)) AS prob,
-        ACTION AS tg
-    FROM amazon_train
-)
-```
-
-!!! note "Note"
-    Plus d'infos sur [avg()](../sql-reference/aggregate-functions/reference.md#agg_function-avg) et [journal()](../sql-reference/functions/math-functions.md) fonction.
-
-[Article Original](https://clickhouse.tech/docs/en/guides/apply_catboost_model/) <!--hide-->
diff --git a/docs/fr/guides/index.md b/docs/fr/guides/index.md
deleted file mode 100644
index 5f6552fdb459..000000000000
--- a/docs/fr/guides/index.md
+++ /dev/null
@@ -1,16 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Guide
-toc_priority: 38
-toc_title: "Aper\xE7u"
----
-
-# ClickHouse Guides {#clickhouse-guides}
-
-Liste des instructions détaillées étape par étape qui aident à résoudre diverses tâches en utilisant ClickHouse:
-
--   [Tutoriel sur la configuration simple du cluster](../getting-started/tutorial.md)
--   [Application D'un modèle CatBoost dans ClickHouse](apply-catboost-model.md)
-
-[Article Original](https://clickhouse.tech/docs/en/guides/) <!--hide-->
diff --git a/docs/fr/images b/docs/fr/images
deleted file mode 120000
index 73937c941ecf..000000000000
--- a/docs/fr/images
+++ /dev/null
@@ -1,1 +0,0 @@
-../en/images
\ No newline at end of file
diff --git a/docs/fr/index.md b/docs/fr/index.md
deleted file mode 100644
index d11744f1ab03..000000000000
--- a/docs/fr/index.md
+++ /dev/null
@@ -1,97 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 0
-toc_title: "Aper\xE7u"
----
-
-# Qu'Est-Ce Que ClickHouse? {#what-is-clickhouse}
-
-ClickHouse est un système de gestion de base de données orienté colonne (SGBD) pour le traitement analytique en ligne des requêtes (OLAP).
-
-Dans un “normal” SGBD orienté ligne, les données sont stockées dans cet ordre:
-
-| Rangée | WatchID     | JavaEnable | Intitulé                         | GoodEvent | EventTime           |
-|--------|-------------|------------|----------------------------------|-----------|---------------------|
-| #0    | 89354350662 | 1          | Relations Avec Les Investisseurs | 1         | 2016-05-18 05:19:20 |
-| #1    | 90329509958 | 0          | Contacter                        | 1         | 2016-05-18 08:10:20 |
-| #2    | 89953706054 | 1          | Mission                          | 1         | 2016-05-18 07:38:00 |
-| #N    | …           | …          | …                                | …         | …                   |
-
-En d'autres termes, toutes les valeurs liées à une ligne sont physiquement stockées l'une à côté de l'autre.
-
-Des exemples d'un SGBD orienté ligne sont MySQL, Postgres et MS SQL Server.
-
-Dans un SGBD orienté colonne, les données sont stockées comme ceci:
-
-| Rangée:     | #0                              | #1                 | #2                 | #N |
-|-------------|----------------------------------|---------------------|---------------------|-----|
-| WatchID:    | 89354350662                      | 90329509958         | 89953706054         | …   |
-| JavaEnable: | 1                                | 0                   | 1                   | …   |
-| Intitulé:   | Relations Avec Les Investisseurs | Contacter           | Mission             | …   |
-| GoodEvent:  | 1                                | 1                   | 1                   | …   |
-| EventTime:  | 2016-05-18 05:19:20              | 2016-05-18 08:10:20 | 2016-05-18 07:38:00 | …   |
-
-Ces exemples montrent l'ordre que les données sont organisées en. Les valeurs de différentes colonnes sont stockés séparément, et les données de la même colonne sont stockées ensemble.
-
-Exemples D'un SGBD orienté colonne: Vertica, Paraccel (matrice Actian et Amazon Redshift), Sybase IQ, Exasol, Infobright, InfiniDB, MonetDB (VectorWise et Actian Vector), LucidDB, SAP HANA, Google Dremel, Google PowerDrill, Druid et kdb+.
-
-Different orders for storing data are better suited to different scenarios. The data access scenario refers to what queries are made, how often, and in what proportion; how much data is read for each type of query – rows, columns, and bytes; the relationship between reading and updating data; the working size of the data and how locally it is used; whether transactions are used, and how isolated they are; requirements for data replication and logical integrity; requirements for latency and throughput for each type of query, and so on.
-
-Plus la charge sur le système est élevée, plus il est important de personnaliser le système configuré pour correspondre aux exigences du scénario d'utilisation, et plus cette personnalisation devient fine. Il n'y a pas de système qui soit aussi bien adapté à des scénarios significativement différents. Si un système est adaptable à un large ensemble de scénarios, sous une charge élevée, le système traitera tous les scénarios de manière également médiocre, ou fonctionnera bien pour un ou quelques-uns des scénarios possibles.
-
-## Propriétés clés du scénario OLAP {#key-properties-of-olap-scenario}
-
--   La grande majorité des demandes concernent l'accès en lecture.
--   Les données sont mises à jour en lots assez importants (\> 1000 lignes), pas par des lignes simples; ou elles ne sont pas mises à jour du tout.
--   Les données sont ajoutées à la base de données mais ne sont pas modifiées.
--   Pour les lectures, un assez grand nombre de lignes sont extraites de la base de données, mais seulement un petit sous-ensemble de colonnes.
--   Les Tables sont “wide,” ce qui signifie qu'ils contiennent un grand nombre de colonnes.
--   Les requêtes sont relativement rares (généralement des centaines de requêtes par serveur ou moins par seconde).
--   Pour les requêtes simples, les latences autour de 50 ms sont autorisées.
--   Les valeurs de colonne sont assez petites: nombres et chaînes courtes (par exemple, 60 octets par URL).
--   Nécessite un débit élevé lors du traitement d'une seule requête (jusqu'à des milliards de lignes par seconde par serveur).
--   Les Transactions ne sont pas nécessaires.
--   Faibles exigences en matière de cohérence des données.
--   Il y a une grande table par requête. Toutes les tables sont petites, sauf une.
--   Un résultat de requête est significativement plus petit que les données source. En d'autres termes, les données sont filtrées ou agrégées, de sorte que le résultat s'intègre dans la RAM d'un seul serveur.
-
-Il est facile de voir que le scénario OLAP est très différent des autres scénarios populaires (tels que OLTP ou key-Value access). Il n'est donc pas logique d'essayer D'utiliser OLTP ou une base de données clé-valeur pour traiter les requêtes analytiques si vous voulez obtenir des performances décentes. Par exemple, si vous essayez D'utiliser MongoDB ou Redis pour l'analyse, vous obtiendrez des performances très médiocres par rapport aux bases de données OLAP.
-
-## Pourquoi les bases de données orientées colonne fonctionnent mieux dans le scénario OLAP {#why-column-oriented-databases-work-better-in-the-olap-scenario}
-
-Les bases de données orientées colonne sont mieux adaptées aux scénarios OLAP: elles sont au moins 100 fois plus rapides dans le traitement de la plupart des requêtes. Les raisons sont expliquées en détail ci-dessous, mais le fait est plus facile de démontrer visuellement:
-
-**SGBD orienté ligne**
-
-![Row-oriented](images/row-oriented.gif#)
-
-**SGBD orienté colonne**
-
-![Column-oriented](images/column-oriented.gif#)
-
-Vous voyez la différence?
-
-### D'entrée/sortie {#inputoutput}
-
-1.  Pour une requête analytique, seul un petit nombre de colonnes de table doit être lu. Dans une base de données orientée colonne, vous pouvez lire uniquement les données dont vous avez besoin. Par exemple, si vous avez besoin de 5 colonnes sur 100, Vous pouvez vous attendre à une réduction de 20 fois des e / s.
-2.  Puisque les données sont lues en paquets, il est plus facile de les compresser. Les données dans les colonnes sont également plus faciles à compresser. Cela réduit d'autant le volume d'e/S.
-3.  En raison de la réduction des E / S, Plus de données s'insèrent dans le cache du système.
-
-Par exemple, la requête “count the number of records for each advertising platform” nécessite la lecture d'un “advertising platform ID” colonne, qui prend 1 octet non compressé. Si la majeure partie du trafic ne provenait pas de plates-formes publicitaires, vous pouvez vous attendre à une compression d'au moins 10 fois de cette colonne. Lors de l'utilisation d'un algorithme de compression rapide, la décompression des données est possible à une vitesse d'au moins plusieurs gigaoctets de données non compressées par seconde. En d'autres termes, cette requête ne peut être traitée qu'à une vitesse d'environ plusieurs milliards de lignes par seconde sur un seul serveur. Cette vitesse est effectivement atteinte dans la pratique.
-
-### CPU {#cpu}
-
-Étant donné que l'exécution d'une requête nécessite le traitement d'un grand nombre de lignes, il est utile de répartir toutes les opérations pour des vecteurs entiers au lieu de lignes séparées, ou d'implémenter le moteur de requête de sorte qu'il n'y ait presque aucun coût d'expédition. Si vous ne le faites pas, avec un sous-système de disque à moitié décent, l'interpréteur de requête bloque inévitablement le processeur. Il est logique de stocker des données dans des colonnes et de les traiter, si possible, par des colonnes.
-
-Il y a deux façons de le faire:
-
-1.  Un moteur vectoriel. Toutes les opérations sont écrites pour les vecteurs, au lieu de valeurs séparées. Cela signifie que vous n'avez pas besoin d'appeler les opérations très souvent, et les coûts d'expédition sont négligeables. Le code d'opération contient un cycle interne optimisé.
-
-2.  La génération de Code. Le code généré pour la requête contient tous les appels indirects.
-
-Ce n'est pas fait dans “normal” bases de données, car cela n'a pas de sens lors de l'exécution de requêtes simples. Cependant, il y a des exceptions. Par exemple, MemSQL utilise la génération de code pour réduire la latence lors du traitement des requêtes SQL. (À titre de comparaison, les SGBD analytiques nécessitent une optimisation du débit, et non une latence.)
-
-Notez que pour l'efficacité du processeur, le langage de requête doit être déclaratif (SQL ou MDX), ou au moins un vecteur (J, K). La requête ne doit contenir que des boucles implicites, permettant une optimisation.
-
-{## [Article Original](https://clickhouse.tech/docs/en/) ##}
diff --git a/docs/fr/interfaces/cli.md b/docs/fr/interfaces/cli.md
deleted file mode 100644
index 5b8ee67c31a0..000000000000
--- a/docs/fr/interfaces/cli.md
+++ /dev/null
@@ -1,149 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 17
-toc_title: Client De Ligne De Commande
----
-
-# Client de ligne de commande {#command-line-client}
-
-ClickHouse fournit un client de ligne de commande natif: `clickhouse-client`. Le client prend en charge les options de ligne de commande et les fichiers de configuration. Pour plus d'informations, voir [Configuration](#interfaces_cli_configuration).
-
-[Installer](../getting-started/index.md) à partir de la `clickhouse-client` package et l'exécuter avec la commande `clickhouse-client`.
-
-``` bash
-$ clickhouse-client
-ClickHouse client version 19.17.1.1579 (official build).
-Connecting to localhost:9000 as user default.
-Connected to ClickHouse server version 19.17.1 revision 54428.
-
-:)
-```
-
-Différentes versions client et serveur sont compatibles les unes avec les autres, mais certaines fonctionnalités peuvent ne pas être disponibles dans les anciens clients. Nous vous recommandons d'utiliser la même version du client que l'application serveur. Lorsque vous essayez d'utiliser un client de l'ancienne version, puis le serveur, `clickhouse-client` affiche le message:
-
-      ClickHouse client version is older than ClickHouse server. It may lack support for new features.
-
-## Utilisation {#cli_usage}
-
-Le client peut être utilisé en mode interactif et non interactif (batch). Pour utiliser le mode batch, spécifiez ‘query’ d'un paramètre ou d'envoyer des données à ‘stdin’ (il vérifie que ‘stdin’ n'est pas un terminal), ou les deux. Similaire à L'interface HTTP, lors de l'utilisation ‘query’ paramètre et envoi de données à ‘stdin’ la demande est une concaténation de la ‘query’ paramètre, un saut de ligne, et les données de ‘stdin’. Ceci est pratique pour les grandes requêtes D'insertion.
-
-Exemple d'utilisation du client pour insérer des données:
-
-``` bash
-$ echo -ne "1, 'some text', '2016-08-14 00:00:00'
2, 'some more text', '2016-08-14 00:00:01'" | clickhouse-client --database=test --query="INSERT INTO test FORMAT CSV";
-
-$ cat <<_EOF | clickhouse-client --database=test --query="INSERT INTO test FORMAT CSV";
-3, 'some text', '2016-08-14 00:00:00'
-4, 'some more text', '2016-08-14 00:00:01'
-_EOF
-
-$ cat file.csv | clickhouse-client --database=test --query="INSERT INTO test FORMAT CSV";
-```
-
-En mode batch, le format de données par défaut est TabSeparated. Vous pouvez définir le format dans la clause FORMAT de la requête.
-
-Par défaut, vous ne pouvez traiter une seule requête en mode batch. De faire plusieurs requêtes à partir d'un “script,” l'utilisation de la `--multiquery` paramètre. Cela fonctionne pour toutes les requêtes sauf INSERT. Les résultats de la requête sont produits consécutivement sans séparateurs supplémentaires. De même, pour traiter un grand nombre de requêtes, vous pouvez exécuter ‘clickhouse-client’ pour chaque requête. Notez que cela peut prendre des dizaines de millisecondes pour lancer le ‘clickhouse-client’ programme.
-
-En mode interactif, vous obtenez une ligne de commande où vous pouvez entrer des requêtes.
-
-Si ‘multiline’ n'est pas spécifié (par défaut): Pour exécuter la requête, appuyez sur Entrée. Le point-virgule n'est pas nécessaire à la fin de la requête. Pour entrer une requête multiligne, entrez une barre oblique inverse `\` avant le saut de ligne. Après avoir appuyé sur Entrée, il vous sera demandé d'entrer la ligne suivante de la requête.
-
-Si multiline est spécifié: pour exécuter une requête, terminez-la par un point-virgule et appuyez sur Entrée. Si le point-virgule a été omis à la fin de l'entrée ligne, vous serez invité à entrer la ligne suivante de la requête.
-
-Une seule requête est exécutée, donc tout après le point-virgule est ignoré.
-
-Vous pouvez spécifier `\G` au lieu de le ou après le point-virgule. Cela indique le format Vertical. Dans ce format, chaque valeur est imprimée sur une ligne distincte, ce qui est pratique pour les grandes tables. Cette fonctionnalité inhabituelle a été ajoutée pour la compatibilité avec la CLI MySQL.
-
-La ligne de commande est basé sur ‘replxx’ (similaire à ‘readline’). En d'autres termes, il utilise les raccourcis clavier familiers et conserve un historique. L'histoire est écrite à `~/.clickhouse-client-history`.
-
-Par défaut, le format utilisé est Jolicompact. Vous pouvez modifier le format dans la clause FORMAT de la requête, ou en spécifiant `\G` à la fin de la requête, en utilisant le `--format` ou `--vertical` dans la ligne de commande, ou en utilisant le fichier de configuration client.
-
-Pour quitter le client, appuyez sur Ctrl + D (ou Ctrl + C), ou entrez l'une des options suivantes au lieu d'une requête: “exit”, “quit”, “logout”, “exit;”, “quit;”, “logout;”, “q”, “Q”, “:q”
-
-Lors du traitement d'une requête, le client affiche:
-
-1.  Progrès, qui est mis à jour pas plus de 10 fois par seconde (par défaut). Pour les requêtes rapides, la progression peut ne pas avoir le temps d'être affichée.
-2.  La requête formatée après l'analyse, pour le débogage.
-3.  Le résultat dans le format spécifié.
-4.  Le nombre de lignes dans le résultat, le temps passé et la vitesse moyenne de traitement des requêtes.
-
-Vous pouvez annuler une requête longue en appuyant sur Ctrl + C. Cependant, vous devrez toujours attendre un peu pour que le serveur abandonne la requête. Il n'est pas possible d'annuler une requête à certaines étapes. Si vous n'attendez pas et appuyez une seconde fois sur Ctrl+C, le client quittera.
-
-Le client de ligne de commande permet de passer des données externes (tables temporaires externes) pour l'interrogation. Pour plus d'informations, consultez la section “External data for query processing”.
-
-### Requêtes avec paramètres {#cli-queries-with-parameters}
-
-Vous pouvez créer une requête avec des paramètres et leur transmettre des valeurs à partir de l'application cliente. Cela permet d'éviter le formatage de la requête avec des valeurs dynamiques spécifiques côté client. Exemple:
-
-``` bash
-$ clickhouse-client --param_parName="[1, 2]"  -q "SELECT * FROM table WHERE a = {parName:Array(UInt16)}"
-```
-
-#### La Syntaxe De La Requête {#cli-queries-with-parameters-syntax}
-
-Formatez une requête comme d'habitude, puis placez les valeurs que vous souhaitez transmettre des paramètres de l'application à la requête entre accolades au format suivant:
-
-``` sql
-{<name>:<data type>}
-```
-
--   `name` — Placeholder identifier. In the console client it should be used in app parameters as `--param_<name> = value`.
--   `data type` — [Type de données](../sql-reference/data-types/index.md) de l'application valeur de paramètre. Par exemple, une structure de données comme `(integer, ('string', integer))` peut avoir la `Tuple(UInt8, Tuple(String, UInt8))` type de données (vous pouvez également utiliser un autre [entier](../sql-reference/data-types/int-uint.md) type).
-
-#### Exemple {#example}
-
-``` bash
-$ clickhouse-client --param_tuple_in_tuple="(10, ('dt', 10))" -q "SELECT * FROM table WHERE val = {tuple_in_tuple:Tuple(UInt8, Tuple(String, UInt8))}"
-```
-
-## Configuration {#interfaces_cli_configuration}
-
-Vous pouvez passer des paramètres à `clickhouse-client` (tous les paramètres ont une valeur par défaut) en utilisant:
-
--   À partir de la ligne de commande
-
-    Les options de ligne de commande remplacent les valeurs et les paramètres par défaut dans les fichiers de configuration.
-
--   Les fichiers de Configuration.
-
-    Les paramètres des fichiers de configuration remplacent les valeurs par défaut.
-
-### Options De Ligne De Commande {#command-line-options}
-
--   `--host, -h` -– The server name, ‘localhost’ par défaut. Vous pouvez utiliser le nom ou L'adresse IPv4 ou IPv6.
--   `--port` – The port to connect to. Default value: 9000. Note that the HTTP interface and the native interface use different ports.
--   `--user, -u` – The username. Default value: default.
--   `--password` – The password. Default value: empty string.
--   `--query, -q` – The query to process when using non-interactive mode.
--   `--database, -d` – Select the current default database. Default value: the current database from the server settings (‘default’ par défaut).
--   `--multiline, -m` – If specified, allow multiline queries (do not send the query on Enter).
--   `--multiquery, -n` – If specified, allow processing multiple queries separated by semicolons.
--   `--format, -f` – Use the specified default format to output the result.
--   `--vertical, -E` – If specified, use the Vertical format by default to output the result. This is the same as ‘–format=Vertical’. Dans ce format, chaque valeur est imprimée sur une ligne séparée, ce qui est utile lors de l'affichage de tables larges.
--   `--time, -t` – If specified, print the query execution time to ‘stderr’ en mode non interactif.
--   `--stacktrace` – If specified, also print the stack trace if an exception occurs.
--   `--config-file` – The name of the configuration file.
--   `--secure` – If specified, will connect to server over secure connection.
--   `--param_<name>` — Value for a [requête avec paramètres](#cli-queries-with-parameters).
-
-### Fichiers De Configuration {#configuration_files}
-
-`clickhouse-client` utilise le premier fichier existant de suite:
-
--   Défini dans le `--config-file` paramètre.
--   `./clickhouse-client.xml`
--   `~/.clickhouse-client/config.xml`
--   `/etc/clickhouse-client/config.xml`
-
-Exemple de fichier de configuration:
-
-``` xml
-<config>
-    <user>username</user>
-    <password>password</password>
-    <secure>False</secure>
-</config>
-```
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/cli/) <!--hide-->
diff --git a/docs/fr/interfaces/cpp.md b/docs/fr/interfaces/cpp.md
deleted file mode 100644
index 0451b0722aa5..000000000000
--- a/docs/fr/interfaces/cpp.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 24
-toc_title: "Biblioth\xE8que Client C++ "
----
-
-# Bibliothèque Client C++  {#c-client-library}
-
-Voir le fichier README à [clickhouse-cpp](https://github.com/ClickHouse/clickhouse-cpp) référentiel.
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/cpp/) <!--hide-->
diff --git a/docs/fr/interfaces/formats.md b/docs/fr/interfaces/formats.md
deleted file mode 100644
index 2022683e9408..000000000000
--- a/docs/fr/interfaces/formats.md
+++ /dev/null
@@ -1,1212 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 21
-toc_title: "Formats d'entr\xE9e et de sortie"
----
-
-# Formats pour les données D'entrée et de sortie {#formats}
-
-ClickHouse peut accepter et renvoyer des données dans différents formats. Un format pris en charge pour l'entrée peut être utilisée pour analyser les données fournies à `INSERT`s, pour effectuer `SELECT`s à partir d'une table sauvegardée par fichier, telle Qu'un fichier, une URL ou un HDFS, ou pour lire un dictionnaire externe. Un format pris en charge pour la sortie peut être utilisé pour organiser
-les résultats d'une `SELECT` et pour effectuer `INSERT`s dans une table sauvegardée par fichier.
-
-Les formats pris en charge sont:
-
-| Format                                                          | Entrée | Sortie |
-|-----------------------------------------------------------------|--------|--------|
-| [TabSeparated](#tabseparated)                                   | ✔      | ✔      |
-| [TabSeparatedRaw](#tabseparatedraw)                             | ✗      | ✔      |
-| [TabSeparatedWithNames](#tabseparatedwithnames)                 | ✔      | ✔      |
-| [TabSeparatedWithNamesAndTypes](#tabseparatedwithnamesandtypes) | ✔      | ✔      |
-| [Modèle](#format-template)                                      | ✔      | ✔      |
-| [TemplateIgnoreSpaces](#templateignorespaces)                   | ✔      | ✗      |
-| [CSV](#csv)                                                     | ✔      | ✔      |
-| [CSVWithNames](#csvwithnames)                                   | ✔      | ✔      |
-| [CustomSeparated](#format-customseparated)                      | ✔      | ✔      |
-| [Valeur](#data-format-values)                                   | ✔      | ✔      |
-| [Vertical](#vertical)                                           | ✗      | ✔      |
-| [VerticalRaw](#verticalraw)                                     | ✗      | ✔      |
-| [JSON](#json)                                                   | ✗      | ✔      |
-| [JSONCompact](#jsoncompact)                                     | ✗      | ✔      |
-| [JSONEachRow](#jsoneachrow)                                     | ✔      | ✔      |
-| [TSKV](#tskv)                                                   | ✔      | ✔      |
-| [Joli](#pretty)                                                 | ✗      | ✔      |
-| [PrettyCompact](#prettycompact)                                 | ✗      | ✔      |
-| [PrettyCompactMonoBlock](#prettycompactmonoblock)               | ✗      | ✔      |
-| [PrettyNoEscapes](#prettynoescapes)                             | ✗      | ✔      |
-| [PrettySpace](#prettyspace)                                     | ✗      | ✔      |
-| [Protobuf](#protobuf)                                           | ✔      | ✔      |
-| [Avro](#data-format-avro)                                       | ✔      | ✔      |
-| [AvroConfluent](#data-format-avro-confluent)                    | ✔      | ✗      |
-| [Parquet](#data-format-parquet)                                 | ✔      | ✔      |
-| [ORC](#data-format-orc)                                         | ✔      | ✗      |
-| [RowBinary](#rowbinary)                                         | ✔      | ✔      |
-| [Rowbinarywithnamesettypes](#rowbinarywithnamesandtypes)        | ✔      | ✔      |
-| [Natif](#native)                                                | ✔      | ✔      |
-| [NULL](#null)                                                   | ✗      | ✔      |
-| [XML](#xml)                                                     | ✗      | ✔      |
-| [CapnProto](#capnproto)                                         | ✔      | ✗      |
-
-Vous pouvez contrôler certains paramètres de traitement de format avec les paramètres ClickHouse. Pour plus d'informations, lisez le [Paramètre](../operations/settings/settings.md) section.
-
-## TabSeparated {#tabseparated}
-
-Au format TabSeparated, les données sont écrites par ligne. Chaque ligne contient des valeurs séparées par des onglets. Chaque valeur est suivie par un onglet, à l'exception de la dernière valeur dans la ligne, qui est suivi par un saut de ligne. Les flux de ligne strictement Unix sont supposés partout. La dernière ligne doit également contenir un saut de ligne à la fin. Les valeurs sont écrites au format texte, sans guillemets et avec des caractères spéciaux échappés.
-
-Ce format est également disponible sous le nom de `TSV`.
-
-Le `TabSeparated` le format est pratique pour le traitement des données à l'aide de programmes et de scripts personnalisés. Il est utilisé par défaut dans L'interface HTTP et dans le mode batch du client de ligne de commande. Ce format permet également de transférer des données entre différents SGBD. Par exemple, vous pouvez obtenir un dump de MySQL et le télécharger sur ClickHouse, ou vice versa.
-
-Le `TabSeparated` format prend en charge la sortie des valeurs totales (lors de L'utilisation avec des totaux) et des valeurs extrêmes (lorsque ‘extremes’ est réglé sur 1). Dans ces cas, les valeurs totales et les extrêmes sont sorties après les données principales. Le résultat principal, les valeurs totales et les extrêmes sont séparés les uns des autres par une ligne vide. Exemple:
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
-```
-
-``` text
-2014-03-17      1406958
-2014-03-18      1383658
-2014-03-19      1405797
-2014-03-20      1353623
-2014-03-21      1245779
-2014-03-22      1031592
-2014-03-23      1046491
-
-1970-01-01      8873898
-
-2014-03-17      1031592
-2014-03-23      1406958
-```
-
-### Mise En Forme Des Données {#data-formatting}
-
-Les nombres entiers sont écrits sous forme décimale. Les numéros peuvent contenir un supplément “+” caractère au début (ignoré lors de l'analyse, et non enregistré lors du formatage). Les nombres non négatifs ne peuvent pas contenir le signe négatif. Lors de la lecture, il est permis d'analyser une chaîne vide en tant que zéro, ou (pour les types signés) une chaîne composée d'un signe moins en tant que zéro. Les nombres qui ne correspondent pas au type de données correspondant peuvent être analysés comme un nombre différent, sans message d'erreur.
-
-Les nombres à virgule flottante sont écrits sous forme décimale. Le point est utilisé comme séparateur décimal. Les entrées exponentielles sont prises en charge, tout comme ‘inf’, ‘+inf’, ‘-inf’, et ‘nan’. Une entrée de nombres à virgule flottante peut commencer ou se terminer par une virgule décimale.
-Pendant le formatage, la précision peut être perdue sur les nombres à virgule flottante.
-Pendant l'analyse, il n'est pas strictement nécessaire de lire le nombre représentable de la machine le plus proche.
-
-Les Dates sont écrites au format AAAA-MM-JJ et analysées dans le même format, mais avec tous les caractères comme séparateurs.
-Les Dates avec les heures sont écrites dans le format `YYYY-MM-DD hh:mm:ss` et analysé dans le même format, mais avec des caractères comme séparateurs.
-Tout cela se produit dans le fuseau horaire du système au moment où le client ou le serveur démarre (selon lequel d'entre eux formate les données). Pour les dates avec des heures, l'heure d'été n'est pas spécifiée. Donc, si un vidage a des temps pendant l'heure d'été, le vidage ne correspond pas sans équivoque aux données, et l'analyse sélectionnera l'une des deux fois.
-Lors d'une opération de lecture, les dates et dates incorrectes avec des heures peuvent être analysées avec un débordement naturel ou en tant que dates et heures nulles, sans message d'erreur.
-
-À titre d'exception, l'analyse des dates avec des heures est également prise en charge au format d'horodatage Unix, si elle se compose exactement de 10 chiffres décimaux. Le résultat n'est pas dépendant du fuseau horaire. Les formats AAAA-MM-JJ hh:mm:ss et NNNNNNNNNN sont différenciés automatiquement.
-
-Les chaînes sont sorties avec des caractères spéciaux échappés par une barre oblique inverse. Les séquences d'échappement suivantes sont utilisées pour la sortie: `\b`, `\f`, `\r`, `
`, `\t`, `\0`, `\'`, `\\`. L'analyse prend également en charge les séquences `\a`, `\v`, et `\xHH` (séquences d'échappement hexadécimales) et `\c` séquences, où `c` est un caractère (ces séquences sont convertis à `c`). Ainsi, la lecture de données prend en charge les formats où un saut de ligne peut être écrit comme `
` ou `\` ou comme un saut de ligne. Par exemple, la chaîne `Hello world` avec un saut de ligne entre les mots au lieu de l'espace peut être analysé dans l'une des variantes suivantes:
-
-``` text
-Hello
world
-
-Hello\
-world
-```
-
-La deuxième variante est prise en charge car MySQL l'utilise lors de l'écriture de vidages séparés par des tabulations.
-
-L'ensemble minimum de caractères que vous devez échapper lors du passage de données au format TabSeparated: tab, saut de ligne (LF) et barre oblique inverse.
-
-Seul un petit ensemble de symboles sont échappés. Vous pouvez facilement tomber sur une valeur de chaîne que votre terminal va ruiner en sortie.
-
-Les tableaux sont écrits sous la forme d'une liste de valeurs séparées par des virgules entre crochets. Le nombre d'éléments dans le tableau sont formatés comme normalement. `Date` et `DateTime` les types sont écrits entre guillemets simples. Les chaînes sont écrites entre guillemets simples avec les mêmes règles d'échappement que ci-dessus.
-
-[NULL](../sql-reference/syntax.md) est formaté en tant qu' `\N`.
-
-Chaque élément de [Imbriqué](../sql-reference/data-types/nested-data-structures/nested.md) structures est représenté sous forme de tableau.
-
-Exemple:
-
-``` sql
-CREATE TABLE nestedt
-(
-    `id` UInt8,
-    `aux` Nested(
-        a UInt8,
-        b String
-    )
-)
-ENGINE = TinyLog
-```
-
-``` sql
-INSERT INTO nestedt Values ( 1, [1], ['a'])
-```
-
-``` sql
-SELECT * FROM nestedt FORMAT TSV
-```
-
-``` text
-1  [1]    ['a']
-```
-
-## TabSeparatedRaw {#tabseparatedraw}
-
-Diffère de `TabSeparated` format en ce que les lignes sont écrites sans échappement.
-Ce format n'est approprié que pour la sortie d'un résultat de requête, mais pas pour l'analyse (récupération des données à insérer dans une table).
-
-Ce format est également disponible sous le nom de `TSVRaw`.
-
-## TabSeparatedWithNames {#tabseparatedwithnames}
-
-Diffère de la `TabSeparated` formater en ce que les noms de colonne sont écrits dans la première ligne.
-Pendant l'analyse, la première ligne est complètement ignorée. Vous ne pouvez pas utiliser les noms de colonnes pour déterminer leur position ou vérifier leur exactitude.
-(La prise en charge de l'analyse de la ligne d'en-tête peut être ajoutée à l'avenir.)
-
-Ce format est également disponible sous le nom de `TSVWithNames`.
-
-## TabSeparatedWithNamesAndTypes {#tabseparatedwithnamesandtypes}
-
-Diffère de la `TabSeparated` format que les noms de colonne sont écrits à la première ligne, tandis que les types de colonne sont dans la deuxième rangée.
-Pendant l'analyse, les première et deuxième lignes sont complètement ignorées.
-
-Ce format est également disponible sous le nom de `TSVWithNamesAndTypes`.
-
-## Modèle {#format-template}
-
-Ce format permet de spécifier une chaîne de format personnalisée avec des espaces réservés pour les valeurs avec une règle d'échappement spécifiée.
-
-Il utilise les paramètres `format_template_resultset`, `format_template_row`, `format_template_rows_between_delimiter` and some settings of other formats (e.g. `output_format_json_quote_64bit_integers` lors de l'utilisation de `JSON` s'échapper, voir plus loin)
-
-Paramètre `format_template_row` spécifie le chemin d'accès au fichier, qui contient une chaîne de format pour les lignes avec la syntaxe suivante:
-
-`delimiter_1${column_1:serializeAs_1}delimiter_2${column_2:serializeAs_2} ... delimiter_N`,
-
-où `delimiter_i` est un délimiteur entre les valeurs (`$` symbole peut être échappé comme `$$`),
-`column_i` est un nom ou un index d'une colonne dont les valeurs sont choisies ou inséré (si vide, alors la colonne sera ignoré),
-`serializeAs_i` est une règle d'échappement pour les valeurs de colonne. Les règles d'échappement suivantes sont prises en charge:
-
--   `CSV`, `JSON`, `XML` (de même pour les formats des mêmes noms)
--   `Escaped` (de la même manière à `TSV`)
--   `Quoted` (de la même manière à `Values`)
--   `Raw` (sans s'échapper, de même pour `TSVRaw`)
--   `None` (pas de règle d'échappement, voir plus loin)
-
-Si une règle d'échappement est omise, alors `None` sera utilisé. `XML` et `Raw` sont adaptés uniquement pour la sortie.
-
-Donc, pour la chaîne de format suivante:
-
-      `Search phrase: ${SearchPhrase:Quoted}, count: ${c:Escaped}, ad price: $$${price:JSON};`
-
-les valeurs de `SearchPhrase`, `c` et `price` colonnes, qui sont échappées comme `Quoted`, `Escaped` et `JSON` imprimées (pour sélectionner) ou sera prévu (insert) entre `Search phrase:`, `, count:`, `, ad price: $` et `;` délimiteurs respectivement. Exemple:
-
-`Search phrase: 'bathroom interior design', count: 2166, ad price: $3;`
-
-Le `format_template_rows_between_delimiter` paramètre spécifie délimiteur entre les lignes, qui est imprimé (ou attendu) après chaque ligne, sauf la dernière (`
` par défaut)
-
-Paramètre `format_template_resultset` spécifie le chemin d'accès au fichier, qui contient une chaîne de format pour resultset. Format string for resultset a la même syntaxe qu'une chaîne de format pour row et permet de spécifier un préfixe, un suffixe et un moyen d'imprimer des informations supplémentaires. Il contient les espaces réservés suivants au lieu des noms de colonnes:
-
--   `data` est les lignes avec des données dans `format_template_row` format, séparés par des `format_template_rows_between_delimiter`. Cet espace doit être le premier espace réservé dans la chaîne de format.
--   `totals` est la ligne avec des valeurs totales dans `format_template_row` format (lors de L'utilisation avec des totaux)
--   `min` est la ligne avec des valeurs minimales dans `format_template_row` format (lorsque les extrêmes sont définis sur 1)
--   `max` est la ligne avec des valeurs maximales en `format_template_row` format (lorsque les extrêmes sont définis sur 1)
--   `rows` le nombre total de lignes de sortie
--   `rows_before_limit` est le nombre minimal de lignes qu'il y aurait eu sans limite. Sortie uniquement si la requête contient LIMIT. Si la requête contient GROUP BY, rows_before_limit_at_least est le nombre exact de lignes qu'il y aurait eu sans limite.
--   `time` est le temps d'exécution de la requête en secondes
--   `rows_read` est le nombre de lignes a été lu
--   `bytes_read` est le nombre d'octets (non compressé) a été lu
-
-Réservé `data`, `totals`, `min` et `max` ne doit pas avoir de règle d'échappement spécifiée (ou `None` doit être spécifié explicitement). Les espaces réservés restants peuvent avoir une règle d'échappement spécifiée.
-Si l' `format_template_resultset` paramètre est une chaîne vide, `${data}` est utilisé comme valeur par défaut.
-Pour insérer des requêtes format permet de sauter certaines colonnes ou certains champs si préfixe ou suffixe (voir Exemple).
-
-Sélectionnez exemple:
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase ORDER BY c DESC LIMIT 5 FORMAT Template SETTINGS
-format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = '
    '
-```
-
-`/some/path/resultset.format`:
-
-``` text
-<!DOCTYPE HTML>
-<html> <head> <title>Search phrases</title> </head>
- <body>
-  <table border="1"> <caption>Search phrases</caption>
-    <tr> <th>Search phrase</th> <th>Count</th> </tr>
-    ${data}
-  </table>
-  <table border="1"> <caption>Max</caption>
-    ${max}
-  </table>
-  <b>Processed ${rows_read:XML} rows in ${time:XML} sec</b>
- </body>
-</html>
-```
-
-`/some/path/row.format`:
-
-``` text
-<tr> <td>${0:XML}</td> <td>${1:XML}</td> </tr>
-```
-
-Résultat:
-
-``` html
-<!DOCTYPE HTML>
-<html> <head> <title>Search phrases</title> </head>
- <body>
-  <table border="1"> <caption>Search phrases</caption>
-    <tr> <th>Search phrase</th> <th>Count</th> </tr>
-    <tr> <td></td> <td>8267016</td> </tr>
-    <tr> <td>bathroom interior design</td> <td>2166</td> </tr>
-    <tr> <td>yandex</td> <td>1655</td> </tr>
-    <tr> <td>spring 2014 fashion</td> <td>1549</td> </tr>
-    <tr> <td>freeform photos</td> <td>1480</td> </tr>
-  </table>
-  <table border="1"> <caption>Max</caption>
-    <tr> <td></td> <td>8873898</td> </tr>
-  </table>
-  <b>Processed 3095973 rows in 0.1569913 sec</b>
- </body>
-</html>
-```
-
-Insérez exemple:
-
-``` text
-Some header
-Page views: 5, User id: 4324182021466249494, Useless field: hello, Duration: 146, Sign: -1
-Page views: 6, User id: 4324182021466249494, Useless field: world, Duration: 185, Sign: 1
-Total rows: 2
-```
-
-``` sql
-INSERT INTO UserActivity FORMAT Template SETTINGS
-format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format'
-```
-
-`/some/path/resultset.format`:
-
-``` text
-Some header
${data}
Total rows: ${:CSV}

-```
-
-`/some/path/row.format`:
-
-``` text
-Page views: ${PageViews:CSV}, User id: ${UserID:CSV}, Useless field: ${:CSV}, Duration: ${Duration:CSV}, Sign: ${Sign:CSV}
-```
-
-`PageViews`, `UserID`, `Duration` et `Sign` à l'intérieur des espaces réservés sont des noms de colonnes dans la table. Les valeurs après `Useless field` en rangées et après `
Total rows:` dans le suffixe sera ignoré.
-Tous les séparateurs dans les données d'entrée doivent être strictement égal à délimiteurs dans les chaînes de format.
-
-## TemplateIgnoreSpaces {#templateignorespaces}
-
-Ce format est adapté uniquement pour l'entrée.
-Semblable à `Template`, mais ignore les espaces entre les séparateurs et les valeurs dans le flux d'entrée. Toutefois, si les chaînes de format contiennent des espaces, ces caractères seront attendus dans le flux d'entrée. Permet également de spécifier des espaces réservés vides (`${}` ou `${:None}`) pour diviser un délimiteur en parties séparées pour ignorer les espaces entre eux. Ces espaces réservés sont utilisés uniquement pour ignorer les caractères d'espace.
-Il est possible de lire `JSON` l'utilisation de ce format, si les valeurs des colonnes ont le même ordre dans toutes les lignes. Par exemple, la requête suivante peut être utilisée pour insérer des données à partir d'un exemple de format de sortie [JSON](#json):
-
-``` sql
-INSERT INTO table_name FORMAT TemplateIgnoreSpaces SETTINGS
-format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = ','
-```
-
-`/some/path/resultset.format`:
-
-``` text
-{${}"meta"${}:${:JSON},${}"data"${}:${}[${data}]${},${}"totals"${}:${:JSON},${}"extremes"${}:${:JSON},${}"rows"${}:${:JSON},${}"rows_before_limit_at_least"${}:${:JSON}${}}
-```
-
-`/some/path/row.format`:
-
-``` text
-{${}"SearchPhrase"${}:${}${phrase:JSON}${},${}"c"${}:${}${cnt:JSON}${}}
-```
-
-## TSKV {#tskv}
-
-Similaire à TabSeparated, mais affiche une valeur au format name = value. Les noms sont échappés de la même manière qu'au format TabSeparated, et le symbole = est également échappé.
-
-``` text
-SearchPhrase=   count()=8267016
-SearchPhrase=bathroom interior design    count()=2166
-SearchPhrase=yandex     count()=1655
-SearchPhrase=2014 spring fashion    count()=1549
-SearchPhrase=freeform photos       count()=1480
-SearchPhrase=angelina jolie    count()=1245
-SearchPhrase=omsk       count()=1112
-SearchPhrase=photos of dog breeds    count()=1091
-SearchPhrase=curtain designs        count()=1064
-SearchPhrase=baku       count()=1000
-```
-
-[NULL](../sql-reference/syntax.md) est formaté en tant qu' `\N`.
-
-``` sql
-SELECT * FROM t_null FORMAT TSKV
-```
-
-``` text
-x=1    y=\N
-```
-
-Quand il y a un grand nombre de petites colonnes, ce format est inefficace, et il n'y a généralement pas de raison de l'utiliser. Néanmoins, ce n'est pas pire que JSONEachRow en termes d'efficacité.
-
-Both data output and parsing are supported in this format. For parsing, any order is supported for the values of different columns. It is acceptable for some values to be omitted – they are treated as equal to their default values. In this case, zeros and blank rows are used as default values. Complex values that could be specified in the table are not supported as defaults.
-
-L'analyse permet la présence du champ supplémentaire `tskv` sans le signe égal ou de valeur. Ce champ est ignoré.
-
-## CSV {#csv}
-
-Format des valeurs séparées par des virgules ([RFC](https://tools.ietf.org/html/rfc4180)).
-
-Lors du formatage, les lignes sont entourées de guillemets doubles. Un guillemet double à l'intérieur d'une chaîne est affiché sous la forme de deux guillemets doubles dans une rangée. Il n'y a pas d'autres règles pour échapper les caractères. Date et date-heure sont entre guillemets. Les nombres sont produits sans guillemets. Les valeurs sont séparées par un caractère délimiteur, qui est `,` par défaut. Le caractère délimiteur est défini dans le paramètre [format_csv_delimiter](../operations/settings/settings.md#settings-format_csv_delimiter). Les lignes sont séparées à L'aide du saut de ligne Unix (LF). Les tableaux sont sérialisés au format CSV comme suit: tout d'abord, le tableau est sérialisé en une chaîne comme au format TabSeparated, puis la chaîne résultante est sortie au format CSV entre guillemets doubles. Les Tuples au format CSV sont sérialisés en tant que colonnes séparées (c'est-à-dire que leur imbrication dans le tuple est perdue).
-
-``` bash
-$ clickhouse-client --format_csv_delimiter="|" --query="INSERT INTO test.csv FORMAT CSV" < data.csv
-```
-
-\*Par défaut, le délimiteur est `,`. Voir la [format_csv_delimiter](../operations/settings/settings.md#settings-format_csv_delimiter) réglage pour plus d'informations.
-
-Lors de l'analyse, toutes les valeurs peuvent être analysés avec ou sans guillemets. Les guillemets doubles et simples sont pris en charge. Les lignes peuvent également être organisées sans guillemets. Dans ce cas, ils sont analysés jusqu'au caractère délimiteur ou au saut de ligne (CR ou LF). En violation de la RFC, lors de l'analyse des lignes sans guillemets, les espaces et les onglets de début et de fin sont ignorés. Pour le saut de ligne, les types Unix (LF), Windows (CR LF) et Mac OS Classic (CR LF) sont tous pris en charge.
-
-Les valeurs d'entrée non cotées vides sont remplacées par des valeurs par défaut pour les colonnes respectives, si
-[input_format_defaults_for_omitted_fields](../operations/settings/settings.md#session_settings-input_format_defaults_for_omitted_fields)
-est activé.
-
-`NULL` est formaté en tant qu' `\N` ou `NULL` ou une chaîne vide non cotée (voir paramètres [input_format_csv_unquoted_null_literal_as_null](../operations/settings/settings.md#settings-input_format_csv_unquoted_null_literal_as_null) et [input_format_defaults_for_omitted_fields](../operations/settings/settings.md#session_settings-input_format_defaults_for_omitted_fields)).
-
-Le format CSV prend en charge la sortie des totaux et des extrêmes de la même manière que `TabSeparated`.
-
-## CSVWithNames {#csvwithnames}
-
-Imprime également la ligne d'en-tête, semblable à `TabSeparatedWithNames`.
-
-## CustomSeparated {#format-customseparated}
-
-Semblable à [Modèle](#format-template), mais il imprime ou lit toutes les colonnes et utilise la règle d'échappement du paramètre `format_custom_escaping_rule` et délimiteurs de paramètres `format_custom_field_delimiter`, `format_custom_row_before_delimiter`, `format_custom_row_after_delimiter`, `format_custom_row_between_delimiter`, `format_custom_result_before_delimiter` et `format_custom_result_after_delimiter`, pas à partir de chaînes de format.
-Il y a aussi `CustomSeparatedIgnoreSpaces` le format, qui est similaire à `TemplateIgnoreSpaces`.
-
-## JSON {#json}
-
-Sorties de données au format JSON. Outre les tables de données, il génère également des noms et des types de colonnes, ainsi que des informations supplémentaires: le nombre total de lignes de sortie et le nombre de lignes qui auraient pu être sorties s'il n'y avait pas de limite. Exemple:
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON
-```
-
-``` json
-{
-        "meta":
-        [
-                {
-                        "name": "SearchPhrase",
-                        "type": "String"
-                },
-                {
-                        "name": "c",
-                        "type": "UInt64"
-                }
-        ],
-
-        "data":
-        [
-                {
-                        "SearchPhrase": "",
-                        "c": "8267016"
-                },
-                {
-                        "SearchPhrase": "bathroom interior design",
-                        "c": "2166"
-                },
-                {
-                        "SearchPhrase": "yandex",
-                        "c": "1655"
-                },
-                {
-                        "SearchPhrase": "spring 2014 fashion",
-                        "c": "1549"
-                },
-                {
-                        "SearchPhrase": "freeform photos",
-                        "c": "1480"
-                }
-        ],
-
-        "totals":
-        {
-                "SearchPhrase": "",
-                "c": "8873898"
-        },
-
-        "extremes":
-        {
-                "min":
-                {
-                        "SearchPhrase": "",
-                        "c": "1480"
-                },
-                "max":
-                {
-                        "SearchPhrase": "",
-                        "c": "8267016"
-                }
-        },
-
-        "rows": 5,
-
-        "rows_before_limit_at_least": 141137
-}
-```
-
-Le JSON est compatible avec JavaScript. Pour ce faire, certains caractères sont en outre échappés: la barre oblique `/` s'est échappée comme l' `\/`; sauts de ligne alternatifs `U+2028` et `U+2029`, qui cassent certains navigateurs, sont échappés comme `\uXXXX`. Les caractères de contrôle ASCII sont échappés: retour arrière, flux de formulaire, saut de ligne, retour chariot et tabulation horizontale sont remplacés par `\b`, `\f`, `
`, `\r`, `\t` , ainsi que les octets restants dans la plage 00-1F en utilisant `\uXXXX` sequences. Invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences. For compatibility with JavaScript, Int64 and UInt64 integers are enclosed in double-quotes by default. To remove the quotes, you can set the configuration parameter [output_format_json_quote_64bit_integers](../operations/settings/settings.md#session_settings-output_format_json_quote_64bit_integers) à 0.
-
-`rows` – The total number of output rows.
-
-`rows_before_limit_at_least` Le nombre minimal de lignes aurait été sans limite. Sortie uniquement si la requête contient LIMIT.
-Si la requête contient GROUP BY, rows_before_limit_at_least est le nombre exact de lignes qu'il y aurait eu sans limite.
-
-`totals` – Total values (when using WITH TOTALS).
-
-`extremes` – Extreme values (when extremes are set to 1).
-
-Ce format n'est approprié que pour la sortie d'un résultat de requête, mais pas pour l'analyse (récupération des données à insérer dans une table).
-
-Supports ClickHouse [NULL](../sql-reference/syntax.md) s'affiche à l'écran `null` dans la sortie JSON.
-
-Voir aussi l' [JSONEachRow](#jsoneachrow) format.
-
-## JSONCompact {#jsoncompact}
-
-Diffère de JSON uniquement en ce que les lignes de données sont sorties dans des tableaux, pas dans des objets.
-
-Exemple:
-
-``` json
-{
-        "meta":
-        [
-                {
-                        "name": "SearchPhrase",
-                        "type": "String"
-                },
-                {
-                        "name": "c",
-                        "type": "UInt64"
-                }
-        ],
-
-        "data":
-        [
-                ["", "8267016"],
-                ["bathroom interior design", "2166"],
-                ["yandex", "1655"],
-                ["fashion trends spring 2014", "1549"],
-                ["freeform photo", "1480"]
-        ],
-
-        "totals": ["","8873898"],
-
-        "extremes":
-        {
-                "min": ["","1480"],
-                "max": ["","8267016"]
-        },
-
-        "rows": 5,
-
-        "rows_before_limit_at_least": 141137
-}
-```
-
-Ce format n'est approprié que pour la sortie d'un résultat de requête, mais pas pour l'analyse (récupération des données à insérer dans une table).
-Voir aussi l' `JSONEachRow` format.
-
-## JSONEachRow {#jsoneachrow}
-
-Lorsque vous utilisez ce format, ClickHouse affiche les lignes en tant qu'objets JSON séparés et délimités par des retours à la ligne, mais les données dans leur ensemble ne sont pas JSON valides.
-
-``` json
-{"SearchPhrase":"curtain designs","count()":"1064"}
-{"SearchPhrase":"baku","count()":"1000"}
-{"SearchPhrase":"","count()":"8267016"}
-```
-
-Lors de l'insertion des données, vous devez fournir un objet JSON distinct pour chaque ligne.
-
-### Insertion De Données {#inserting-data}
-
-``` sql
-INSERT INTO UserActivity FORMAT JSONEachRow {"PageViews":5, "UserID":"4324182021466249494", "Duration":146,"Sign":-1} {"UserID":"4324182021466249494","PageViews":6,"Duration":185,"Sign":1}
-```
-
-Clickhouse permet:
-
--   Toute commande de paires clé-valeur dans l'objet.
--   Omettre certaines valeurs.
-
-ClickHouse ignore les espaces entre les éléments et les virgules après les objets. Vous pouvez passer tous les objets en une seule ligne. Vous n'avez pas à les séparer avec des sauts de ligne.
-
-**Valeurs omises traitement**
-
-Clickhouse remplace les valeurs omises par les valeurs par défaut pour le [types de données](../sql-reference/data-types/index.md).
-
-Si `DEFAULT expr` clickhouse utilise différentes règles de substitution en fonction de [input_format_defaults_for_omitted_fields](../operations/settings/settings.md#session_settings-input_format_defaults_for_omitted_fields) paramètre.
-
-Considérons le tableau suivant:
-
-``` sql
-CREATE TABLE IF NOT EXISTS example_table
-(
-    x UInt32,
-    a DEFAULT x * 2
-) ENGINE = Memory;
-```
-
--   Si `input_format_defaults_for_omitted_fields = 0`, alors la valeur par défaut pour `x` et `a` égal `0` (la valeur par défaut pour le `UInt32` type de données).
--   Si `input_format_defaults_for_omitted_fields = 1`, alors la valeur par défaut pour `x` égal `0` mais la valeur par défaut de `a` égal `x * 2`.
-
-!!! note "Avertissement"
-    Lors de l'insertion de données avec `insert_sample_with_metadata = 1`, ClickHouse consomme plus de ressources de calcul, par rapport à l'insertion avec `insert_sample_with_metadata = 0`.
-
-### La Sélection De Données {#selecting-data}
-
-Envisager l' `UserActivity` table comme un exemple:
-
-``` text
-┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐
-│ 4324182021466249494 │         5 │      146 │   -1 │
-│ 4324182021466249494 │         6 │      185 │    1 │
-└─────────────────────┴───────────┴──────────┴──────┘
-```
-
-Requête `SELECT * FROM UserActivity FORMAT JSONEachRow` retourner:
-
-``` text
-{"UserID":"4324182021466249494","PageViews":5,"Duration":146,"Sign":-1}
-{"UserID":"4324182021466249494","PageViews":6,"Duration":185,"Sign":1}
-```
-
-Contrairement à l' [JSON](#json) format, il n'y a pas de substitution de séquences UTF-8 non valides. Les valeurs sont échappés de la même manière que pour `JSON`.
-
-!!! note "Note"
-    Tout ensemble d'octets peut être sortie dans les cordes. L'utilisation de la `JSONEachRow` formater si vous êtes sûr que les données de la table peuvent être formatées en tant que JSON sans perdre aucune information.
-
-### Utilisation de Structures imbriquées {#jsoneachrow-nested}
-
-Si vous avez une table avec [Imbriqué](../sql-reference/data-types/nested-data-structures/nested.md) colonnes de type de données, vous pouvez insérer des données JSON avec la même structure. Activer cette fonctionnalité avec le [input_format_import_nested_json](../operations/settings/settings.md#settings-input_format_import_nested_json) paramètre.
-
-Par exemple, considérez le tableau suivant:
-
-``` sql
-CREATE TABLE json_each_row_nested (n Nested (s String, i Int32) ) ENGINE = Memory
-```
-
-Comme vous pouvez le voir dans la `Nested` description du type de données, ClickHouse traite chaque composant de la structure imbriquée comme une colonne distincte (`n.s` et `n.i` pour notre table). Vous pouvez insérer des données de la manière suivante:
-
-``` sql
-INSERT INTO json_each_row_nested FORMAT JSONEachRow {"n.s": ["abc", "def"], "n.i": [1, 23]}
-```
-
-Pour insérer des données en tant qu'objet JSON hiérarchique, définissez [input_format_import_nested_json=1](../operations/settings/settings.md#settings-input_format_import_nested_json).
-
-``` json
-{
-    "n": {
-        "s": ["abc", "def"],
-        "i": [1, 23]
-    }
-}
-```
-
-Sans ce paramètre, ClickHouse lance une exception.
-
-``` sql
-SELECT name, value FROM system.settings WHERE name = 'input_format_import_nested_json'
-```
-
-``` text
-┌─name────────────────────────────┬─value─┐
-│ input_format_import_nested_json │ 0     │
-└─────────────────────────────────┴───────┘
-```
-
-``` sql
-INSERT INTO json_each_row_nested FORMAT JSONEachRow {"n": {"s": ["abc", "def"], "i": [1, 23]}}
-```
-
-``` text
-Code: 117. DB::Exception: Unknown field found while parsing JSONEachRow format: n: (at row 1)
-```
-
-``` sql
-SET input_format_import_nested_json=1
-INSERT INTO json_each_row_nested FORMAT JSONEachRow {"n": {"s": ["abc", "def"], "i": [1, 23]}}
-SELECT * FROM json_each_row_nested
-```
-
-``` text
-┌─n.s───────────┬─n.i────┐
-│ ['abc','def'] │ [1,23] │
-└───────────────┴────────┘
-```
-
-## Natif {#native}
-
-Le format le plus efficace. Les données sont écrites et lues par des blocs au format binaire. Pour chaque bloc, le nombre de lignes, le nombre de colonnes, les noms et types de colonnes et les parties de colonnes de ce bloc sont enregistrés les uns après les autres. En d'autres termes, ce format est “columnar” – it doesn't convert columns to rows. This is the format used in the native interface for interaction between servers, for using the command-line client, and for C++ clients.
-
-Vous pouvez utiliser ce format pour générer rapidement des vidages qui ne peuvent être lus que par le SGBD ClickHouse. Cela n'a pas de sens de travailler avec ce format vous-même.
-
-## NULL {#null}
-
-Rien n'est sortie. Cependant, la requête est traitée et, lors de l'utilisation du client de ligne de commande, les données sont transmises au client. Ceci est utilisé pour les tests, y compris les tests de performance.
-Évidemment, ce format n'est approprié que pour la sortie, pas pour l'analyse.
-
-## Joli {#pretty}
-
-Affiche les données sous forme de tables Unicode-art, en utilisant également des séquences d'échappement ANSI pour définir les couleurs dans le terminal.
-Une grille complète de la table est dessinée, et chaque ligne occupe deux lignes dans le terminal.
-Chaque bloc de résultat est sorti sous la forme d'une table séparée. Ceci est nécessaire pour que les blocs puissent être sortis sans résultats de mise en mémoire tampon (la mise en mémoire tampon serait nécessaire pour pré-calculer la largeur visible de toutes les valeurs).
-
-[NULL](../sql-reference/syntax.md) est sortie `ᴺᵁᴸᴸ`.
-
-Exemple (montré pour le [PrettyCompact](#prettycompact) format):
-
-``` sql
-SELECT * FROM t_null
-```
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-└───┴──────┘
-```
-
-Les lignes ne sont pas échappées dans les formats Pretty\*. Exemple est montré pour la [PrettyCompact](#prettycompact) format:
-
-``` sql
-SELECT 'String with \'quotes\' and \t character' AS Escaping_test
-```
-
-``` text
-┌─Escaping_test────────────────────────┐
-│ String with 'quotes' and      character │
-└──────────────────────────────────────┘
-```
-
-Pour éviter de déverser trop de données sur le terminal, seules les 10 000 premières lignes sont imprimées. Si le nombre de lignes est supérieur ou égal à 10 000, le message “Showed first 10 000” est imprimé.
-Ce format n'est approprié que pour la sortie d'un résultat de requête, mais pas pour l'analyse (récupération des données à insérer dans une table).
-
-Le joli format prend en charge la sortie des valeurs totales (lors de L'utilisation avec des totaux) et des extrêmes (lorsque ‘extremes’ est réglé sur 1). Dans ces cas, les valeurs totales et les valeurs extrêmes sont sorties après les données principales, dans des tableaux séparés. Exemple (montré pour le [PrettyCompact](#prettycompact) format):
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact
-```
-
-``` text
-┌──EventDate─┬───────c─┐
-│ 2014-03-17 │ 1406958 │
-│ 2014-03-18 │ 1383658 │
-│ 2014-03-19 │ 1405797 │
-│ 2014-03-20 │ 1353623 │
-│ 2014-03-21 │ 1245779 │
-│ 2014-03-22 │ 1031592 │
-│ 2014-03-23 │ 1046491 │
-└────────────┴─────────┘
-
-Totals:
-┌──EventDate─┬───────c─┐
-│ 1970-01-01 │ 8873898 │
-└────────────┴─────────┘
-
-Extremes:
-┌──EventDate─┬───────c─┐
-│ 2014-03-17 │ 1031592 │
-│ 2014-03-23 │ 1406958 │
-└────────────┴─────────┘
-```
-
-## PrettyCompact {#prettycompact}
-
-Diffère de [Joli](#pretty) en ce que la grille est dessinée entre les lignes et le résultat est plus compact.
-Ce format est utilisé par défaut dans le client de ligne de commande en mode interactif.
-
-## PrettyCompactMonoBlock {#prettycompactmonoblock}
-
-Diffère de [PrettyCompact](#prettycompact) dans ce cas, jusqu'à 10 000 lignes sont mises en mémoire tampon, puis sorties en tant que table unique, pas par blocs.
-
-## PrettyNoEscapes {#prettynoescapes}
-
-Diffère de Pretty en ce que les séquences d'échappement ANSI ne sont pas utilisées. Ceci est nécessaire pour afficher ce format dans un navigateur, ainsi que pour utiliser le ‘watch’ utilitaire de ligne de commande.
-
-Exemple:
-
-``` bash
-$ watch -n1 "clickhouse-client --query='SELECT event, value FROM system.events FORMAT PrettyCompactNoEscapes'"
-```
-
-Vous pouvez utiliser L'interface HTTP pour afficher dans le navigateur.
-
-### Joliscompactnoescapes {#prettycompactnoescapes}
-
-Le même que le réglage précédent.
-
-### PrettySpaceNoEscapes {#prettyspacenoescapes}
-
-Le même que le réglage précédent.
-
-## PrettySpace {#prettyspace}
-
-Diffère de [PrettyCompact](#prettycompact) dans cet espace (caractères d'espace) est utilisé à la place de la grille.
-
-## RowBinary {#rowbinary}
-
-Formats et analyse les données par ligne au format binaire. Les lignes et les valeurs sont répertoriées consécutivement, sans séparateurs.
-Ce format est moins efficace que le format natif car il est basé sur des lignes.
-
-Les entiers utilisent une représentation little-endian de longueur fixe. Par exemple, UInt64 utilise 8 octets.
-DateTime est représenté par UInt32 contenant L'horodatage Unix comme valeur.
-Date est représenté comme un objet UInt16 qui contient le nombre de jours depuis 1970-01-01 comme valeur.
-La chaîne est représentée par une longueur varint (non signée [LEB128](https://en.wikipedia.org/wiki/LEB128)), suivie par les octets de la chaîne.
-FixedString est représenté simplement comme une séquence d'octets.
-
-Le tableau est représenté sous la forme d'une longueur varint (non signée [LEB128](https://en.wikipedia.org/wiki/LEB128)), suivie par les éléments de la matrice.
-
-Pour [NULL](../sql-reference/syntax.md#null-literal) un soutien, un octet supplémentaire contenant 1 ou 0 est ajouté avant chaque [Nullable](../sql-reference/data-types/nullable.md) valeur. Si la valeur est 1, alors la valeur est `NULL` et cet octet est interprétée comme une valeur distincte. Si 0, la valeur après l'octet n'est pas `NULL`.
-
-## Rowbinarywithnamesettypes {#rowbinarywithnamesandtypes}
-
-Semblable à [RowBinary](#rowbinary) mais avec l'ajout de l'en-tête:
-
--   [LEB128](https://en.wikipedia.org/wiki/LEB128)- nombre codé de colonnes (N)
--   N `String`s spécification des noms de colonnes
--   N `String`s spécification des types de colonnes
-
-## Valeur {#data-format-values}
-
-Imprime chaque ligne entre parenthèses. Les lignes sont séparées par des virgules. Il n'y a pas de virgule après la dernière ligne. Les valeurs entre parenthèses sont également séparées par des virgules. Les nombres sont produits dans un format décimal sans guillemets. Les tableaux sont affichés entre crochets. Les chaînes, les dates et les dates avec des heures sont affichées entre guillemets. Les règles d'échappement et l'analyse sont similaires à [TabSeparated](#tabseparated) format. Pendant le formatage, les espaces supplémentaires ne sont pas insérés, mais pendant l'analyse, ils sont autorisés et ignorés (sauf pour les espaces à l'intérieur des valeurs de tableau, qui ne sont pas autorisés). [NULL](../sql-reference/syntax.md) est représentée comme `NULL`.
-
-The minimum set of characters that you need to escape when passing data in Values ​​format: single quotes and backslashes.
-
-C'est le format qui est utilisé dans `INSERT INTO t VALUES ...`, mais vous pouvez également l'utiliser pour le formatage des résultats de requête.
-
-Voir aussi: [input_format_values_interpret_expressions](../operations/settings/settings.md#settings-input_format_values_interpret_expressions) et [input_format_values_deduce_templates_of_expressions](../operations/settings/settings.md#settings-input_format_values_deduce_templates_of_expressions) paramètre.
-
-## Vertical {#vertical}
-
-Imprime chaque valeur sur une ligne distincte avec le nom de colonne spécifié. Ce format est pratique pour imprimer une ou plusieurs lignes si chaque ligne est constituée d'un grand nombre de colonnes.
-
-[NULL](../sql-reference/syntax.md) est sortie `ᴺᵁᴸᴸ`.
-
-Exemple:
-
-``` sql
-SELECT * FROM t_null FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-x: 1
-y: ᴺᵁᴸᴸ
-```
-
-Les lignes ne sont pas échappées au format Vertical:
-
-``` sql
-SELECT 'string with \'quotes\' and \t with some special 
 characters' AS test FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-test: string with 'quotes' and      with some special
- characters
-```
-
-Ce format n'est approprié que pour la sortie d'un résultat de requête, mais pas pour l'analyse (récupération des données à insérer dans une table).
-
-## VerticalRaw {#verticalraw}
-
-Semblable à [Vertical](#vertical), mais avec échapper désactivé. Ce format ne convient que pour la sortie des résultats de requête, pas pour l'analyse (recevoir des données et les insérer dans la table).
-
-## XML {#xml}
-
-Le format XML ne convient que pour la sortie, pas pour l'analyse. Exemple:
-
-``` xml
-<?xml version='1.0' encoding='UTF-8' ?>
-<result>
-        <meta>
-                <columns>
-                        <column>
-                                <name>SearchPhrase</name>
-                                <type>String</type>
-                        </column>
-                        <column>
-                                <name>count()</name>
-                                <type>UInt64</type>
-                        </column>
-                </columns>
-        </meta>
-        <data>
-                <row>
-                        <SearchPhrase></SearchPhrase>
-                        <field>8267016</field>
-                </row>
-                <row>
-                        <SearchPhrase>bathroom interior design</SearchPhrase>
-                        <field>2166</field>
-                </row>
-                <row>
-                        <SearchPhrase>yandex</SearchPhrase>
-                        <field>1655</field>
-                </row>
-                <row>
-                        <SearchPhrase>2014 spring fashion</SearchPhrase>
-                        <field>1549</field>
-                </row>
-                <row>
-                        <SearchPhrase>freeform photos</SearchPhrase>
-                        <field>1480</field>
-                </row>
-                <row>
-                        <SearchPhrase>angelina jolie</SearchPhrase>
-                        <field>1245</field>
-                </row>
-                <row>
-                        <SearchPhrase>omsk</SearchPhrase>
-                        <field>1112</field>
-                </row>
-                <row>
-                        <SearchPhrase>photos of dog breeds</SearchPhrase>
-                        <field>1091</field>
-                </row>
-                <row>
-                        <SearchPhrase>curtain designs</SearchPhrase>
-                        <field>1064</field>
-                </row>
-                <row>
-                        <SearchPhrase>baku</SearchPhrase>
-                        <field>1000</field>
-                </row>
-        </data>
-        <rows>10</rows>
-        <rows_before_limit_at_least>141137</rows_before_limit_at_least>
-</result>
-```
-
-Si le nom de colonne n'a pas un format acceptable, juste ‘field’ est utilisé comme le nom de l'élément. En général, la structure XML suit la structure JSON.
-Just as for JSON, invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences.
-
-Dans les valeurs de chaîne, les caractères `<` et `&` sont échappés comme `<` et `&`.
-
-Les tableaux sont produits comme `<array><elem>Hello</elem><elem>World</elem>...</array>`,et n-uplets d' `<tuple><elem>Hello</elem><elem>World</elem>...</tuple>`.
-
-## CapnProto {#capnproto}
-
-Cap'n Proto est un format de message binaire similaire aux tampons de protocole et Thrift, mais pas comme JSON ou MessagePack.
-
-Les messages Cap'n Proto sont strictement typés et ne sont pas auto-descriptifs, ce qui signifie qu'ils ont besoin d'une description de schéma externe. Le schéma est appliqué à la volée et mise en cache pour chaque requête.
-
-``` bash
-$ cat capnproto_messages.bin | clickhouse-client --query "INSERT INTO test.hits FORMAT CapnProto SETTINGS format_schema='schema:Message'"
-```
-
-Où `schema.capnp` ressemble à ceci:
-
-``` capnp
-struct Message {
-  SearchPhrase @0 :Text;
-  c @1 :Uint64;
-}
-```
-
-La désérialisation est efficace et n'augmente généralement pas la charge du système.
-
-Voir aussi [Schéma De Format](#formatschema).
-
-## Protobuf {#protobuf}
-
-Protobuf-est un [Protocol Buffers](https://developers.google.com/protocol-buffers/) format.
-
-Ce format nécessite un schéma de format externe. Le schéma est mis en cache entre les requêtes.
-Clickhouse prend en charge les deux `proto2` et `proto3` syntaxe. Les champs répétés/optionnels/obligatoires sont pris en charge.
-
-Exemples d'utilisation:
-
-``` sql
-SELECT * FROM test.table FORMAT Protobuf SETTINGS format_schema = 'schemafile:MessageType'
-```
-
-``` bash
-cat protobuf_messages.bin | clickhouse-client --query "INSERT INTO test.table FORMAT Protobuf SETTINGS format_schema='schemafile:MessageType'"
-```
-
-où le fichier `schemafile.proto` ressemble à ceci:
-
-``` capnp
-syntax = "proto3";
-
-message MessageType {
-  string name = 1;
-  string surname = 2;
-  uint32 birthDate = 3;
-  repeated string phoneNumbers = 4;
-};
-```
-
-Pour trouver la correspondance entre les colonnes de table et les champs du type de message des tampons de protocole, ClickHouse compare leurs noms.
-Cette comparaison est insensible à la casse et les caractères `_` (trait de soulignement) et `.` (dot) sont considérés comme égaux.
-Si les types d'une colonne et d'un champ de message des tampons de protocole sont différents, la conversion nécessaire est appliquée.
-
-Les messages imbriqués sont pris en charge. Par exemple, pour le champ `z` dans le type de message suivant
-
-``` capnp
-message MessageType {
-  message XType {
-    message YType {
-      int32 z;
-    };
-    repeated YType y;
-  };
-  XType x;
-};
-```
-
-ClickHouse tente de trouver une colonne nommée `x.y.z` (ou `x_y_z` ou `X.y_Z` et ainsi de suite).
-Les messages imbriqués conviennent à l'entrée ou à la sortie d'un [structures de données imbriquées](../sql-reference/data-types/nested-data-structures/nested.md).
-
-Valeurs par défaut définies dans un schéma protobuf comme ceci
-
-``` capnp
-syntax = "proto2";
-
-message MessageType {
-  optional int32 result_per_page = 3 [default = 10];
-}
-```
-
-ne sont pas appliquées; la [valeurs par défaut de la table](../sql-reference/statements/create.md#create-default-values) sont utilisés à leur place.
-
-Clickhouse entrées et sorties messages protobuf dans le `length-delimited` format.
-Cela signifie avant que chaque message devrait être écrit sa longueur comme un [varint](https://developers.google.com/protocol-buffers/docs/encoding#varints).
-Voir aussi [comment lire / écrire des messages protobuf délimités par la longueur dans les langues populaires](https://cwiki.apache.org/confluence/display/GEODE/Delimiting+Protobuf+Messages).
-
-## Avro {#data-format-avro}
-
-[Apache Avro](http://avro.apache.org/) est un cadre de sérialisation de données orienté ligne développé dans le projet Hadoop D'Apache.
-
-ClickHouse Avro format prend en charge la lecture et l'écriture [Fichiers de données Avro](http://avro.apache.org/docs/current/spec.html#Object+Container+Files).
-
-### Types De Données Correspondant {#data_types-matching}
-
-Le tableau ci-dessous montre les types de données pris en charge et comment ils correspondent à ClickHouse [types de données](../sql-reference/data-types/index.md) dans `INSERT` et `SELECT` requête.
-
-| Type de données Avro `INSERT`               | Type de données ClickHouse                                                                                          | Type de données Avro `SELECT` |
-|---------------------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------------------------|
-| `boolean`, `int`, `long`, `float`, `double` | [Int (8/16/32)](../sql-reference/data-types/int-uint.md), [UInt (8/16/32)](../sql-reference/data-types/int-uint.md) | `int`                         |
-| `boolean`, `int`, `long`, `float`, `double` | [Int64](../sql-reference/data-types/int-uint.md), [UInt64](../sql-reference/data-types/int-uint.md)                 | `long`                        |
-| `boolean`, `int`, `long`, `float`, `double` | [Float32](../sql-reference/data-types/float.md)                                                                     | `float`                       |
-| `boolean`, `int`, `long`, `float`, `double` | [Float64](../sql-reference/data-types/float.md)                                                                     | `double`                      |
-| `bytes`, `string`, `fixed`, `enum`          | [Chaîne](../sql-reference/data-types/string.md)                                                                     | `bytes`                       |
-| `bytes`, `string`, `fixed`                  | [FixedString (N)](../sql-reference/data-types/fixedstring.md)                                                       | `fixed(N)`                    |
-| `enum`                                      | [Enum (8/16)](../sql-reference/data-types/enum.md)                                                                  | `enum`                        |
-| `array(T)`                                  | [Array(T)](../sql-reference/data-types/array.md)                                                                    | `array(T)`                    |
-| `union(null, T)`, `union(T, null)`          | [Nullable (T)](../sql-reference/data-types/date.md)                                                                 | `union(null, T)`              |
-| `null`                                      | [Les Valeurs Null(Nothing)](../sql-reference/data-types/special-data-types/nothing.md)                              | `null`                        |
-| `int (date)` \*                             | [Date](../sql-reference/data-types/date.md)                                                                         | `int (date)` \*               |
-| `long (timestamp-millis)` \*                | [DateTime64 (3)](../sql-reference/data-types/datetime.md)                                                           | `long (timestamp-millis)` \*  |
-| `long (timestamp-micros)` \*                | [DateTime64 (6)](../sql-reference/data-types/datetime.md)                                                           | `long (timestamp-micros)` \*  |
-
-\* [Types logiques Avro](http://avro.apache.org/docs/current/spec.html#Logical+Types)
-
-Types de données Avro non pris en charge: `record` (non-root), `map`
-
-Types de données logiques Avro non pris en charge: `uuid`, `time-millis`, `time-micros`, `duration`
-
-### Insertion De Données {#inserting-data-1}
-
-Pour insérer des données d'un fichier Avro dans la table ClickHouse:
-
-``` bash
-$ cat file.avro | clickhouse-client --query="INSERT INTO {some_table} FORMAT Avro"
-```
-
-Le schéma racine du fichier Avro d'entrée doit être de `record` type.
-
-Pour trouver la correspondance entre les colonnes de table et les champs du schéma Avro ClickHouse compare leurs noms. Cette comparaison est sensible à la casse.
-Les champs inutilisés sont ignorés.
-
-Les types de données des colonnes de la table ClickHouse peuvent différer des champs correspondants des données Avro insérées. Lors de l'insertion de données, ClickHouse interprète les types de données selon le tableau ci-dessus, puis [jeter](../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) les données au type de colonne correspondant.
-
-### La Sélection De Données {#selecting-data-1}
-
-Pour sélectionner des données de la table ClickHouse dans un fichier Avro:
-
-``` bash
-$ clickhouse-client --query="SELECT * FROM {some_table} FORMAT Avro" > file.avro
-```
-
-Les noms de colonnes doivent:
-
--   commencer avec `[A-Za-z_]`
--   par la suite contenir uniquement `[A-Za-z0-9_]`
-
-Sortie Avro fichier de compression et sync intervalle peut être configuré avec [output_format_avro_codec](../operations/settings/settings.md#settings-output_format_avro_codec) et [output_format_avro_sync_interval](../operations/settings/settings.md#settings-output_format_avro_sync_interval) respectivement.
-
-## AvroConfluent {#data-format-avro-confluent}
-
-Avroconfluent prend en charge le décodage des messages Avro à objet unique couramment utilisés avec [Kafka](https://kafka.apache.org/) et [Confluentes Schéma De Registre](https://docs.confluent.io/current/schema-registry/index.html).
-
-Chaque message Avro intègre un id de schéma qui peut être résolu dans le schéma réel à l'aide du registre de schéma.
-
-Les schémas sont mis en cache une fois résolus.
-
-L'URL du registre de schéma est configurée avec [format_avro_schema_registry_url](../operations/settings/settings.md#settings-format_avro_schema_registry_url)
-
-### Types De Données Correspondant {#data_types-matching-1}
-
-Même que [Avro](#data-format-avro)
-
-### Utilisation {#usage}
-
-Pour vérifier rapidement la résolution du schéma, vous pouvez utiliser [kafkacat](https://github.com/edenhill/kafkacat) avec [clickhouse-local](../operations/utilities/clickhouse-local.md):
-
-``` bash
-$ kafkacat -b kafka-broker  -C -t topic1 -o beginning -f '%s' -c 3 | clickhouse-local   --input-format AvroConfluent --format_avro_schema_registry_url 'http://schema-registry' -S "field1 Int64, field2 String"  -q 'select *  from table'
-1 a
-2 b
-3 c
-```
-
-Utiliser `AvroConfluent` avec [Kafka](../engines/table-engines/integrations/kafka.md):
-
-``` sql
-CREATE TABLE topic1_stream
-(
-    field1 String,
-    field2 String
-)
-ENGINE = Kafka()
-SETTINGS
-kafka_broker_list = 'kafka-broker',
-kafka_topic_list = 'topic1',
-kafka_group_name = 'group1',
-kafka_format = 'AvroConfluent';
-
-SET format_avro_schema_registry_url = 'http://schema-registry';
-
-SELECT * FROM topic1_stream;
-```
-
-!!! note "Avertissement"
-    Paramètre `format_avro_schema_registry_url` doit être configuré dans `users.xml` afin de maintenir sa valeur après un redémarrage.
-
-## Parquet {#data-format-parquet}
-
-[Apache Parquet](http://parquet.apache.org/) est un format de stockage colonnaire répandu dans L'écosystème Hadoop. ClickHouse prend en charge les opérations de lecture et d'écriture pour ce format.
-
-### Types De Données Correspondant {#data_types-matching-2}
-
-Le tableau ci-dessous montre les types de données pris en charge et comment ils correspondent à ClickHouse [types de données](../sql-reference/data-types/index.md) dans `INSERT` et `SELECT` requête.
-
-| Type de données Parquet (`INSERT`) | Type de données ClickHouse                                | Type de données Parquet (`SELECT`) |
-|------------------------------------|-----------------------------------------------------------|------------------------------------|
-| `UINT8`, `BOOL`                    | [UInt8](../sql-reference/data-types/int-uint.md)          | `UINT8`                            |
-| `INT8`                             | [Int8](../sql-reference/data-types/int-uint.md)           | `INT8`                             |
-| `UINT16`                           | [UInt16](../sql-reference/data-types/int-uint.md)         | `UINT16`                           |
-| `INT16`                            | [Int16](../sql-reference/data-types/int-uint.md)          | `INT16`                            |
-| `UINT32`                           | [UInt32](../sql-reference/data-types/int-uint.md)         | `UINT32`                           |
-| `INT32`                            | [Int32](../sql-reference/data-types/int-uint.md)          | `INT32`                            |
-| `UINT64`                           | [UInt64](../sql-reference/data-types/int-uint.md)         | `UINT64`                           |
-| `INT64`                            | [Int64](../sql-reference/data-types/int-uint.md)          | `INT64`                            |
-| `FLOAT`, `HALF_FLOAT`              | [Float32](../sql-reference/data-types/float.md)           | `FLOAT`                            |
-| `DOUBLE`                           | [Float64](../sql-reference/data-types/float.md)           | `DOUBLE`                           |
-| `DATE32`                           | [Date](../sql-reference/data-types/date.md)               | `UINT16`                           |
-| `DATE64`, `TIMESTAMP`              | [DateTime](../sql-reference/data-types/datetime.md)       | `UINT32`                           |
-| `STRING`, `BINARY`                 | [Chaîne](../sql-reference/data-types/string.md)           | `STRING`                           |
-| —                                  | [FixedString](../sql-reference/data-types/fixedstring.md) | `STRING`                           |
-| `DECIMAL`                          | [Décimal](../sql-reference/data-types/decimal.md)         | `DECIMAL`                          |
-
-Clickhouse prend en charge la précision configurable de `Decimal` type. Le `INSERT` requête traite le Parquet `DECIMAL` tapez comme le ClickHouse `Decimal128` type.
-
-Types de données Parquet non pris en charge: `DATE32`, `TIME32`, `FIXED_SIZE_BINARY`, `JSON`, `UUID`, `ENUM`.
-
-Les types de données des colonnes de table ClickHouse peuvent différer des champs correspondants des données de Parquet insérées. Lors de l'insertion de données, ClickHouse interprète les types de données selon le tableau ci-dessus, puis [jeter](../query_language/functions/type_conversion_functions/#type_conversion_function-cast) les données de ce type de données qui est défini pour la colonne de la table ClickHouse.
-
-### Insertion et sélection de données {#inserting-and-selecting-data}
-
-Vous pouvez insérer des données Parquet à partir d'un fichier dans la table ClickHouse par la commande suivante:
-
-``` bash
-$ cat {filename} | clickhouse-client --query="INSERT INTO {some_table} FORMAT Parquet"
-```
-
-Vous pouvez sélectionner des données à partir d'une table de ClickHouse et les enregistrer dans un fichier au format Parquet par la commande suivante:
-
-``` bash
-$ clickhouse-client --query="SELECT * FROM {some_table} FORMAT Parquet" > {some_file.pq}
-```
-
-Pour échanger des données avec Hadoop, vous pouvez utiliser [Moteur de table HDFS](../engines/table-engines/integrations/hdfs.md).
-
-## ORC {#data-format-orc}
-
-[Apache ORC](https://orc.apache.org/) est un format de stockage colonnaire répandu dans L'écosystème Hadoop. Vous ne pouvez insérer des données dans ce format à ClickHouse.
-
-### Types De Données Correspondant {#data_types-matching-3}
-
-Le tableau ci-dessous montre les types de données pris en charge et comment ils correspondent à ClickHouse [types de données](../sql-reference/data-types/index.md) dans `INSERT` requête.
-
-| Type de données ORC (`INSERT`) | Type de données ClickHouse                          |
-|--------------------------------|-----------------------------------------------------|
-| `UINT8`, `BOOL`                | [UInt8](../sql-reference/data-types/int-uint.md)    |
-| `INT8`                         | [Int8](../sql-reference/data-types/int-uint.md)     |
-| `UINT16`                       | [UInt16](../sql-reference/data-types/int-uint.md)   |
-| `INT16`                        | [Int16](../sql-reference/data-types/int-uint.md)    |
-| `UINT32`                       | [UInt32](../sql-reference/data-types/int-uint.md)   |
-| `INT32`                        | [Int32](../sql-reference/data-types/int-uint.md)    |
-| `UINT64`                       | [UInt64](../sql-reference/data-types/int-uint.md)   |
-| `INT64`                        | [Int64](../sql-reference/data-types/int-uint.md)    |
-| `FLOAT`, `HALF_FLOAT`          | [Float32](../sql-reference/data-types/float.md)     |
-| `DOUBLE`                       | [Float64](../sql-reference/data-types/float.md)     |
-| `DATE32`                       | [Date](../sql-reference/data-types/date.md)         |
-| `DATE64`, `TIMESTAMP`          | [DateTime](../sql-reference/data-types/datetime.md) |
-| `STRING`, `BINARY`             | [Chaîne](../sql-reference/data-types/string.md)     |
-| `DECIMAL`                      | [Décimal](../sql-reference/data-types/decimal.md)   |
-
-Clickhouse prend en charge la précision configurable de la `Decimal` type. Le `INSERT` requête traite de l'ORC `DECIMAL` tapez comme le ClickHouse `Decimal128` type.
-
-Types de données ORC non pris en charge: `DATE32`, `TIME32`, `FIXED_SIZE_BINARY`, `JSON`, `UUID`, `ENUM`.
-
-Les types de données des colonnes de la table ClickHouse ne doivent pas correspondre aux champs de données Orc correspondants. Lors de l'insertion de données, ClickHouse interprète les types de données selon le tableau ci-dessus, puis [jeter](../sql-reference/functions/type-conversion-functions.md#type_conversion_function-cast) les données du type de données défini pour la colonne clickhouse table.
-
-### Insertion De Données {#inserting-data-2}
-
-Vous pouvez insérer des données ORC à partir d'un fichier dans la table ClickHouse par la commande suivante:
-
-``` bash
-$ cat filename.orc | clickhouse-client --query="INSERT INTO some_table FORMAT ORC"
-```
-
-Pour échanger des données avec Hadoop, vous pouvez utiliser [Moteur de table HDFS](../engines/table-engines/integrations/hdfs.md).
-
-## Schéma De Format {#formatschema}
-
-Le nom du fichier contenant le schéma de format est défini par le paramètre `format_schema`.
-Il est nécessaire de définir ce paramètre lorsqu'il est utilisé dans l'un des formats `Cap'n Proto` et `Protobuf`.
-Le format de schéma est une combinaison d'un nom de fichier et le nom d'un type de message dans ce fichier, délimité par une virgule,
-e.g. `schemafile.proto:MessageType`.
-Si le fichier possède l'extension standard pour le format (par exemple, `.proto` pour `Protobuf`),
-il peut être omis et dans ce cas, le format de schéma ressemble `schemafile:MessageType`.
-
-Si vous entrez ou sortez des données via le [client](../interfaces/cli.md) dans le [mode interactif](../interfaces/cli.md#cli_usage) le nom de fichier spécifié dans le format de schéma
-peut contenir un chemin absolu, soit un chemin relatif au répertoire courant sur le client.
-Si vous utilisez le client dans le [mode batch](../interfaces/cli.md#cli_usage), le chemin d'accès au schéma doit être relatif pour des raisons de sécurité.
-
-Si vous entrez ou sortez des données via le [Interface HTTP](../interfaces/http.md) le nom de fichier spécifié dans le format de schéma
-doit être situé dans le répertoire spécifié dans [format_schema_path](../operations/server-configuration-parameters/settings.md#server_configuration_parameters-format_schema_path)
-dans la configuration du serveur.
-
-## Sauter Les Erreurs {#skippingerrors}
-
-Certains formats tels que `CSV`, `TabSeparated`, `TSKV`, `JSONEachRow`, `Template`, `CustomSeparated` et `Protobuf` pouvez ignorer brisé ligne si erreur d'analyse s'est produite et poursuivre l'analyse à partir du début de la ligne suivante. Voir [input_format_allow_errors_num](../operations/settings/settings.md#settings-input_format_allow_errors_num) et
-[input_format_allow_errors_ratio](../operations/settings/settings.md#settings-input_format_allow_errors_ratio) paramètre.
-Limitation:
-- En cas d'erreur d'analyse `JSONEachRow` ignore toutes les données jusqu'à la nouvelle ligne (ou EOF), donc les lignes doivent être délimitées par `
` pour compter les erreurs correctement.
-- `Template` et `CustomSeparated` utilisez delimiter après la dernière colonne et delimiter entre les lignes pour trouver le début de la ligne suivante, donc sauter les erreurs ne fonctionne que si au moins l'une d'entre elles n'est pas vide.
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/formats/) <!--hide-->
diff --git a/docs/fr/interfaces/http.md b/docs/fr/interfaces/http.md
deleted file mode 100644
index ea590a2b870a..000000000000
--- a/docs/fr/interfaces/http.md
+++ /dev/null
@@ -1,617 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 19
-toc_title: Interface HTTP
----
-
-# Interface HTTP {#http-interface}
-
-L'interface HTTP vous permet D'utiliser ClickHouse sur n'importe quelle plate-forme à partir de n'importe quel langage de programmation. Nous l'utilisons pour travailler à partir de Java et Perl, ainsi que des scripts shell. Dans d'autres départements, L'interface HTTP est utilisée à partir de Perl, Python et Go. L'interface HTTP est plus limitée que l'interface native, mais elle a une meilleure compatibilité.
-
-Par défaut, clickhouse-server écoute HTTP sur le port 8123 (cela peut être modifié dans la configuration).
-
-Si vous faites une requête GET / sans Paramètres, elle renvoie le code de réponse 200 et la chaîne définie dans [http_server_default_response](../operations/server-configuration-parameters/settings.md#server_configuration_parameters-http_server_default_response) valeur par défaut “Ok.” (avec un saut de ligne à la fin)
-
-``` bash
-$ curl 'http://localhost:8123/'
-Ok.
-```
-
-Utilisez la requête GET /ping dans les scripts de vérification de la santé. Ce gestionnaire revient toujours “Ok.” (avec un saut de ligne à la fin). Disponible à partir de la version 18.12.13.
-
-``` bash
-$ curl 'http://localhost:8123/ping'
-Ok.
-```
-
-Envoyer la demande sous forme D'URL ‘query’ paramètre, ou comme un POSTE. Ou envoyer le début de la requête dans l' ‘query’ paramètre, et le reste dans le POST (nous expliquerons plus tard pourquoi cela est nécessaire). La taille de L'URL est limitée à 16 Ko, alors gardez cela à l'esprit lors de l'envoi de requêtes volumineuses.
-
-En cas de succès, vous recevez le code de réponse 200 et le résultat dans le corps de réponse.
-Si une erreur se produit, vous recevez le code de réponse 500 et un texte de description de l'erreur dans le corps de la réponse.
-
-Lorsque vous utilisez la méthode GET, ‘readonly’ est définie. En d'autres termes, pour les requêtes qui modifient les données, vous ne pouvez utiliser que la méthode POST. Vous pouvez envoyer la requête elle-même dans le corps du message ou dans le paramètre URL.
-
-Exemple:
-
-``` bash
-$ curl 'http://localhost:8123/?query=SELECT%201'
-1
-
-$ wget -nv -O- 'http://localhost:8123/?query=SELECT 1'
-1
-
-$ echo -ne 'GET /?query=SELECT%201 HTTP/1.0\r
\r
' | nc localhost 8123
-HTTP/1.0 200 OK
-Date: Wed, 27 Nov 2019 10:30:18 GMT
-Connection: Close
-Content-Type: text/tab-separated-values; charset=UTF-8
-X-ClickHouse-Server-Display-Name: clickhouse.ru-central1.internal
-X-ClickHouse-Query-Id: 5abe861c-239c-467f-b955-8a201abb8b7f
-X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-
-1
-```
-
-Comme vous pouvez le voir, curl est un peu gênant en ce sens que les espaces doivent être échappés URL.
-Bien que wget échappe à tout lui-même, nous ne recommandons pas de l'utiliser car il ne fonctionne pas bien sur HTTP 1.1 lors de l'utilisation de keep-alive et Transfer-Encoding: chunked.
-
-``` bash
-$ echo 'SELECT 1' | curl 'http://localhost:8123/' --data-binary @-
-1
-
-$ echo 'SELECT 1' | curl 'http://localhost:8123/?query=' --data-binary @-
-1
-
-$ echo '1' | curl 'http://localhost:8123/?query=SELECT' --data-binary @-
-1
-```
-
-Si une partie de la requête est envoyée dans le paramètre et une partie dans la publication, un saut de ligne est inséré entre ces deux parties de données.
-Exemple (cela ne fonctionnera pas):
-
-``` bash
-$ echo 'ECT 1' | curl 'http://localhost:8123/?query=SEL' --data-binary @-
-Code: 59, e.displayText() = DB::Exception: Syntax error: failed at position 0: SEL
-ECT 1
-, expected One of: SHOW TABLES, SHOW DATABASES, SELECT, INSERT, CREATE, ATTACH, RENAME, DROP, DETACH, USE, SET, OPTIMIZE., e.what() = DB::Exception
-```
-
-Par défaut, les données sont renvoyées au format TabSeparated (pour plus d'informations, voir “Formats” section).
-Vous utilisez la clause FORMAT de la requête pour demander tout autre format.
-
-``` bash
-$ echo 'SELECT 1 FORMAT Pretty' | curl 'http://localhost:8123/?' --data-binary @-
-┏━━━┓
-┃ 1 ┃
-┡━━━┩
-│ 1 │
-└───┘
-```
-
-La méthode POST de transmission des données est nécessaire pour les requêtes INSERT. Dans ce cas, vous pouvez écrire le début de la requête dans le paramètre URL et utiliser POST pour transmettre les données à insérer. Les données à insérer pourraient être, par exemple, un vidage séparé par tabulation de MySQL. De cette façon, la requête INSERT remplace LOAD DATA LOCAL INFILE de MySQL.
-
-Exemples: création d'une table:
-
-``` bash
-$ echo 'CREATE TABLE t (a UInt8) ENGINE = Memory' | curl 'http://localhost:8123/' --data-binary @-
-```
-
-Utilisation de la requête D'insertion familière pour l'insertion de données:
-
-``` bash
-$ echo 'INSERT INTO t VALUES (1),(2),(3)' | curl 'http://localhost:8123/' --data-binary @-
-```
-
-Les données peuvent être envoyées séparément de la requête:
-
-``` bash
-$ echo '(4),(5),(6)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20VALUES' --data-binary @-
-```
-
-Vous pouvez spécifier n'importe quel format de données. Le ‘Values’ le format est le même que ce qui est utilisé lors de L'écriture INSERT dans les valeurs t:
-
-``` bash
-$ echo '(7),(8),(9)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20FORMAT%20Values' --data-binary @-
-```
-
-Pour insérer des données à partir d'un vidage séparé par des tabulations, spécifiez le format correspondant:
-
-``` bash
-$ echo -ne '10
11
12
' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20FORMAT%20TabSeparated' --data-binary @-
-```
-
-La lecture de la table des matières. Les données sont sorties dans un ordre aléatoire en raison d'un traitement de requête parallèle:
-
-``` bash
-$ curl 'http://localhost:8123/?query=SELECT%20a%20FROM%20t'
-7
-8
-9
-10
-11
-12
-1
-2
-3
-4
-5
-6
-```
-
-Suppression de la table.
-
-``` bash
-$ echo 'DROP TABLE t' | curl 'http://localhost:8123/' --data-binary @-
-```
-
-Pour les requêtes réussies qui ne renvoient pas de table de données, un corps de réponse vide est renvoyé.
-
-Vous pouvez utiliser le format de compression ClickHouse interne lors de la transmission de données. Les données compressées ont un format non standard, et vous devrez utiliser le spécial `clickhouse-compressor` programme de travail (il s'est installé avec le `clickhouse-client` paquet). Pour augmenter l'efficacité de l'insertion de données, vous pouvez désactiver la vérification de la somme de contrôle côté serveur en utilisant [http_native_compression_disable_checksumming_on_decompress](../operations/settings/settings.md#settings-http_native_compression_disable_checksumming_on_decompress) paramètre.
-
-Si vous avez spécifié `compress=1` dans l'URL, le serveur compresse les données qu'il vous envoie.
-Si vous avez spécifié `decompress=1` dans L'URL, le serveur décompresse les mêmes données que vous transmettez `POST` méthode.
-
-Vous pouvez également choisir d'utiliser [La compression HTTP](https://en.wikipedia.org/wiki/HTTP_compression). Pour envoyer un compressé `POST` demande, ajouter l'en-tête de requête `Content-Encoding: compression_method`. Pour que ClickHouse compresse la réponse, vous devez ajouter `Accept-Encoding: compression_method`. Supports ClickHouse `gzip`, `br`, et `deflate` [méthodes de compression](https://en.wikipedia.org/wiki/HTTP_compression#Content-Encoding_tokens). Pour activer la compression HTTP, vous devez utiliser le ClickHouse [enable_http_compression](../operations/settings/settings.md#settings-enable_http_compression) paramètre. Vous pouvez configurer le niveau de compression des données dans le [http_zlib_compression_level](#settings-http_zlib_compression_level) pour toutes les méthodes de compression.
-
-Vous pouvez l'utiliser pour réduire le trafic réseau lors de la transmission d'une grande quantité de données, ou pour créer des vidages qui sont immédiatement compressés.
-
-Exemples d'envoi de données avec compression:
-
-``` bash
-#Sending data to the server:
-$ curl -vsS "http://localhost:8123/?enable_http_compression=1" -d 'SELECT number FROM system.numbers LIMIT 10' -H 'Accept-Encoding: gzip'
-
-#Sending data to the client:
-$ echo "SELECT 1" | gzip -c | curl -sS --data-binary @- -H 'Content-Encoding: gzip' 'http://localhost:8123/'
-```
-
-!!! note "Note"
-    Certains clients HTTP peuvent décompresser les données du serveur par défaut (avec `gzip` et `deflate`) et vous pouvez obtenir des données décompressées même si vous utilisez les paramètres de compression correctement.
-
-Vous pouvez utiliser l' ‘database’ Paramètre URL pour spécifier la base de données par défaut.
-
-``` bash
-$ echo 'SELECT number FROM numbers LIMIT 10' | curl 'http://localhost:8123/?database=system' --data-binary @-
-0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-```
-
-Par défaut, la base de données enregistrée dans les paramètres du serveur est utilisée comme base de données par défaut. Par défaut, c'est la base de données appelée ‘default’. Alternativement, vous pouvez toujours spécifier la base de données en utilisant un point avant le nom de la table.
-
-Le nom d'utilisateur et le mot de passe peuvent être indiqués de l'une des trois façons suivantes:
-
-1.  Utilisation de L'authentification de base HTTP. Exemple:
-
-<!-- -->
-
-``` bash
-$ echo 'SELECT 1' | curl 'http://user:password@localhost:8123/' -d @-
-```
-
-1.  Dans le ‘user’ et ‘password’ Les paramètres d'URL. Exemple:
-
-<!-- -->
-
-``` bash
-$ echo 'SELECT 1' | curl 'http://localhost:8123/?user=user&password=password' -d @-
-```
-
-1.  Utiliser ‘X-ClickHouse-User’ et ‘X-ClickHouse-Key’ tête. Exemple:
-
-<!-- -->
-
-``` bash
-$ echo 'SELECT 1' | curl -H 'X-ClickHouse-User: user' -H 'X-ClickHouse-Key: password' 'http://localhost:8123/' -d @-
-```
-
-Si le nom d'utilisateur n'est spécifié, le `default` le nom est utilisé. Si le mot de passe n'est spécifié, le mot de passe vide est utilisé.
-Vous pouvez également utiliser les paramètres D'URL pour spécifier des paramètres pour le traitement d'une seule requête ou de profils entiers de paramètres. Exemple: http: / / localhost: 8123/?profil = web & max_rows_to_read=1000000000 & query=sélectionner + 1
-
-Pour plus d'informations, voir le [Paramètre](../operations/settings/index.md) section.
-
-``` bash
-$ echo 'SELECT number FROM system.numbers LIMIT 10' | curl 'http://localhost:8123/?' --data-binary @-
-0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-```
-
-Pour plus d'informations sur les autres paramètres, consultez la section “SET”.
-
-De même, vous pouvez utiliser des sessions ClickHouse dans le protocole HTTP. Pour ce faire, vous devez ajouter l' `session_id` GET paramètre à la demande. Vous pouvez utiliser n'importe quelle chaîne comme ID de session. Par défaut, la session est terminée après 60 secondes d'inactivité. Pour modifier ce délai d'attente, de modifier la `default_session_timeout` dans la configuration du serveur, ou ajoutez le `session_timeout` GET paramètre à la demande. Pour vérifier l'état de la session, utilisez `session_check=1` paramètre. Une seule requête à la fois peut être exécutée dans une seule session.
-
-Vous pouvez recevoir des informations sur le déroulement d'une requête en `X-ClickHouse-Progress` en-têtes de réponse. Pour ce faire, activez [send_progress_in_http_headers](../operations/settings/settings.md#settings-send_progress_in_http_headers). Exemple de l'en-tête de séquence:
-
-``` text
-X-ClickHouse-Progress: {"read_rows":"2752512","read_bytes":"240570816","total_rows_to_read":"8880128"}
-X-ClickHouse-Progress: {"read_rows":"5439488","read_bytes":"482285394","total_rows_to_read":"8880128"}
-X-ClickHouse-Progress: {"read_rows":"8783786","read_bytes":"819092887","total_rows_to_read":"8880128"}
-```
-
-Possibles champs d'en-tête:
-
--   `read_rows` — Number of rows read.
--   `read_bytes` — Volume of data read in bytes.
--   `total_rows_to_read` — Total number of rows to be read.
--   `written_rows` — Number of rows written.
--   `written_bytes` — Volume of data written in bytes.
-
-Les requêtes en cours d'exécution ne s'arrêtent pas automatiquement si la connexion HTTP est perdue. L'analyse et le formatage des données sont effectués côté serveur et l'utilisation du réseau peut s'avérer inefficace.
-Facultatif ‘query_id’ le paramètre peut être passé comme ID de requête (n'importe quelle chaîne). Pour plus d'informations, consultez la section “Settings, replace_running_query”.
-
-Facultatif ‘quota_key’ le paramètre peut être passé comme clé de quota (n'importe quelle chaîne). Pour plus d'informations, consultez la section “Quotas”.
-
-L'interface HTTP permet de transmettre des données externes (tables temporaires externes) pour l'interrogation. Pour plus d'informations, consultez la section “External data for query processing”.
-
-## Tampon De Réponse {#response-buffering}
-
-Vous pouvez activer la mise en mémoire tampon des réponses côté serveur. Le `buffer_size` et `wait_end_of_query` Les paramètres D'URL sont fournis à cette fin.
-
-`buffer_size` détermine le nombre d'octets dans le résultat de tampon dans la mémoire du serveur. Si un corps de résultat est supérieur à ce seuil, le tampon est écrit sur le canal HTTP et les données restantes sont envoyées directement au canal HTTP.
-
-Pour vous assurer que la réponse entière est mise en mémoire tampon, définissez `wait_end_of_query=1`. Dans ce cas, les données ne sont pas stockées dans la mémoire tampon temporaire du serveur de fichiers.
-
-Exemple:
-
-``` bash
-$ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&buffer_size=3000000&wait_end_of_query=1' -d 'SELECT toUInt8(number) FROM system.numbers LIMIT 9000000 FORMAT RowBinary'
-```
-
-Utilisez la mise en mémoire tampon pour éviter les situations où une erreur de traitement de requête s'est produite après l'envoi du code de réponse et des en-têtes HTTP au client. Dans cette situation, un message d'erreur est écrit à la fin du corps de la réponse, et du côté client, l'erreur ne peut être détectée qu'à l'étape d'analyse.
-
-### Requêtes avec paramètres {#cli-queries-with-parameters}
-
-Vous pouvez créer une requête avec paramètres et transmettre des valeurs des paramètres de la requête HTTP. Pour plus d'informations, voir [Requêtes avec des paramètres pour CLI](cli.md#cli-queries-with-parameters).
-
-### Exemple {#example}
-
-``` bash
-$ curl -sS "<address>?param_id=2&param_phrase=test" -d "SELECT * FROM table WHERE int_column = {id:UInt8} and string_column = {phrase:String}"
-```
-
-## Interface HTTP prédéfinie {#predefined_http_interface}
-
-ClickHouse prend en charge des requêtes spécifiques via L'interface HTTP. Par exemple, vous pouvez écrire des données dans un tableau comme suit:
-
-``` bash
-$ echo '(4),(5),(6)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20VALUES' --data-binary @-
-```
-
-ClickHouse prend également en charge L'Interface HTTP prédéfinie qui peut vous aider à une intégration plus facile avec des outils tiers tels que [Prometheus exportateur](https://github.com/percona-lab/clickhouse_exporter).
-
-Exemple:
-
--   Tout d'abord, ajoutez cette section au fichier de configuration du serveur:
-
-<!-- -->
-
-``` xml
-<http_handlers>
-    <rule>
-        <url>/predefined_query</url>
-        <methods>POST,GET</methods>
-        <handler>
-            <type>predefined_query_handler</type>
-            <query>SELECT * FROM system.metrics LIMIT 5 FORMAT Template SETTINGS format_template_resultset = 'prometheus_template_output_format_resultset', format_template_row = 'prometheus_template_output_format_row', format_template_rows_between_delimiter = '
'</query>
-        </handler>
-    </rule>
-    <rule>...</rule>
-    <rule>...</rule>
-</http_handlers>
-```
-
--   Vous pouvez maintenant demander l'url directement pour les données au format Prometheus:
-
-<!-- -->
-
-``` bash
-$ curl -v 'http://localhost:8123/predefined_query'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /predefined_query HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
->
-< HTTP/1.1 200 OK
-< Date: Tue, 28 Apr 2020 08:52:56 GMT
-< Connection: Keep-Alive
-< Content-Type: text/plain; charset=UTF-8
-< X-ClickHouse-Server-Display-Name: i-mloy5trc
-< Transfer-Encoding: chunked
-< X-ClickHouse-Query-Id: 96fe0052-01e6-43ce-b12a-6b7370de6e8a
-< X-ClickHouse-Format: Template
-< X-ClickHouse-Timezone: Asia/Shanghai
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-# HELP "Query" "Number of executing queries"
-# TYPE "Query" counter
-"Query" 1
-
-# HELP "Merge" "Number of executing background merges"
-# TYPE "Merge" counter
-"Merge" 0
-
-# HELP "PartMutation" "Number of mutations (ALTER DELETE/UPDATE)"
-# TYPE "PartMutation" counter
-"PartMutation" 0
-
-# HELP "ReplicatedFetch" "Number of data parts being fetched from replica"
-# TYPE "ReplicatedFetch" counter
-"ReplicatedFetch" 0
-
-# HELP "ReplicatedSend" "Number of data parts being sent to replicas"
-# TYPE "ReplicatedSend" counter
-"ReplicatedSend" 0
-
-* Connection #0 to host localhost left intact
-
-
-* Connection #0 to host localhost left intact
-```
-
-Comme vous pouvez le voir dans l'exemple, si `<http_handlers>` est configuré dans la configuration.fichier xml et `<http_handlers>` peut contenir beaucoup `<rule>s`. ClickHouse fera correspondre les requêtes HTTP reçues au type prédéfini `<rule>` et la première appariés exécute le gestionnaire. Ensuite, ClickHouse exécutera la requête prédéfinie correspondante si la correspondance est réussie.
-
-> Maintenant `<rule>` pouvez configurer `<method>`, `<headers>`, `<url>`,`<handler>`:
-> `<method>` est responsable de la correspondance de la partie méthode de la requête HTTP. `<method>` entièrement conforme à la définition de [méthode](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods) dans le protocole HTTP. C'est une option de configuration. S'il n'est pas défini dans le fichier de configuration, il ne correspond pas à la partie méthode de la requête HTTP.
->
-> `<url>` est responsable de la correspondance de la partie url de la requête HTTP. Il est compatible avec [RE2](https://github.com/google/re2)s 'expressions régulières. C'est une option de configuration. S'il n'est pas défini dans le fichier de configuration, il ne correspond pas à la partie url de la requête HTTP.
->
-> `<headers>` est responsable de la correspondance de la partie d'en-tête de la requête HTTP. Il est compatible avec les expressions régulières de RE2. C'est une option de configuration. S'il n'est pas défini dans le fichier de configuration, il ne correspond pas à la partie d'en-tête de la requête HTTP.
->
-> `<handler>` contient la partie de traitement principale. Maintenant `<handler>` pouvez configurer `<type>`, `<status>`, `<content_type>`, `<response_content>`, `<query>`, `<query_param_name>`.
-> \> `<type>` prend actuellement en charge trois types: **predefined_query_handler**, **dynamic_query_handler**, **statique**.
-> \>
-> \> `<query>` - utiliser avec le type predefined_query_handler, exécute la requête lorsque le gestionnaire est appelé.
-> \>
-> \> `<query_param_name>` - utiliser avec le type dynamic_query_handler, extrait et exécute la valeur correspondant au `<query_param_name>` valeur dans les paramètres de requête HTTP.
-> \>
-> \> `<status>` - utiliser avec le type statique, code d'état de réponse.
-> \>
-> \> `<content_type>` - utiliser avec statique type, réponse [content-type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type).
-> \>
-> \> `<response_content>` - utiliser avec le type statique, le contenu de la réponse envoyé au client, lors de l'utilisation du préfixe ‘file://’ ou ‘config://’, trouver le contenu du fichier ou de la configuration envoyer au client.
-
-Viennent ensuite les méthodes de configuration pour les différents `<type>`.
-
-## predefined_query_handler {#predefined_query_handler}
-
-`<predefined_query_handler>` prend en charge les paramètres de réglage et les valeurs query_params. Vous pouvez configurer `<query>` dans le type de `<predefined_query_handler>`.
-
-`<query>` la valeur est une requête prédéfinie de `<predefined_query_handler>`, qui est exécuté par ClickHouse lorsqu'une requête HTTP est mise en correspondance et que le résultat de la requête est renvoyé. C'est une configuration incontournable.
-
-L'exemple suivant définit les valeurs de `max_threads` et `max_alter_threads` Paramètres, puis interroge la table système pour vérifier si ces paramètres ont été définis avec succès.
-
-Exemple:
-
-``` xml
-<http_handlers>
-    <rule>
-        <url><![CDATA[/query_param_with_url/\w+/(?P<name_1>[^/]+)(/(?P<name_2>[^/]+))?]]></url>
-        <method>GET</method>
-        <headers>
-            <XXX>TEST_HEADER_VALUE</XXX>
-            <PARAMS_XXX><![CDATA[(?P<name_1>[^/]+)(/(?P<name_2>[^/]+))?]]></PARAMS_XXX>
-        </headers>
-        <handler>
-            <type>predefined_query_handler</type>
-            <query>SELECT value FROM system.settings WHERE name = {name_1:String}</query>
-            <query>SELECT name, value FROM system.settings WHERE name = {name_2:String}</query>
-        </handler>
-    </rule>
-</http_handlers>
-```
-
-``` bash
-$ curl -H 'XXX:TEST_HEADER_VALUE' -H 'PARAMS_XXX:max_threads' 'http://localhost:8123/query_param_with_url/1/max_threads/max_alter_threads?max_threads=1&max_alter_threads=2'
-1
-max_alter_threads   2
-```
-
-!!! note "précaution"
-    Dans un `<predefined_query_handler>` prend en charge un seul `<query>` d'un type d'insertion.
-
-## dynamic_query_handler {#dynamic_query_handler}
-
-Dans `<dynamic_query_handler>`, la requête est écrite sous la forme de param de la requête HTTP. La différence est que dans `<predefined_query_handler>`, la requête est écrite dans le fichier de configuration. Vous pouvez configurer `<query_param_name>` dans `<dynamic_query_handler>`.
-
-Clickhouse extrait et exécute la valeur correspondant au `<query_param_name>` valeur dans l'url de la requête HTTP. La valeur par défaut de `<query_param_name>` être `/query` . C'est une option de configuration. Si il n'y a pas de définition dans le fichier de configuration, le paramètre n'est pas passé.
-
-Pour expérimenter cette fonctionnalité, l'exemple définit les valeurs de max_threads et max_alter_threads et demande si les paramètres ont été définis avec succès.
-
-Exemple:
-
-``` xml
-<http_handlers>
-    <rule>
-    <headers>
-        <XXX>TEST_HEADER_VALUE_DYNAMIC</XXX>    </headers>
-    <handler>
-        <type>dynamic_query_handler</type>
-        <query_param_name>query_param</query_param_name>
-    </handler>
-    </rule>
-</http_handlers>
-```
-
-``` bash
-$ curl  -H 'XXX:TEST_HEADER_VALUE_DYNAMIC'  'http://localhost:8123/own?max_threads=1&max_alter_threads=2&param_name_1=max_threads&param_name_2=max_alter_threads&query_param=SELECT%20name,value%20FROM%20system.settings%20where%20name%20=%20%7Bname_1:String%7D%20OR%20name%20=%20%7Bname_2:String%7D'
-max_threads 1
-max_alter_threads   2
-```
-
-## statique {#static}
-
-`<static>` peut-retour [content_type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type), [statut](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) et response_content. response_content peut renvoyer le contenu spécifié
-
-Exemple:
-
-De retour d'un message.
-
-``` xml
-<http_handlers>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/hi</url>
-            <handler>
-                <type>static</type>
-                <status>402</status>
-                <content_type>text/html; charset=UTF-8</content_type>
-                <response_content>Say Hi!</response_content>
-            </handler>
-        </rule>
-<http_handlers>
-```
-
-``` bash
-$ curl -vv  -H 'XXX:xxx' 'http://localhost:8123/hi'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /hi HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 402 Payment Required
-< Date: Wed, 29 Apr 2020 03:51:26 GMT
-< Connection: Keep-Alive
-< Content-Type: text/html; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-* Connection #0 to host localhost left intact
-Say Hi!%
-```
-
-Trouvez le contenu de la configuration envoyer au client.
-
-``` xml
-<get_config_static_handler><![CDATA[<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>]]></get_config_static_handler>
-
-<http_handlers>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/get_config_static_handler</url>
-            <handler>
-                <type>static</type>
-                <response_content>config://get_config_static_handler</response_content>
-            </handler>
-        </rule>
-</http_handlers>
-```
-
-``` bash
-$ curl -v  -H 'XXX:xxx' 'http://localhost:8123/get_config_static_handler'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /get_config_static_handler HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 200 OK
-< Date: Wed, 29 Apr 2020 04:01:24 GMT
-< Connection: Keep-Alive
-< Content-Type: text/plain; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-* Connection #0 to host localhost left intact
-<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>%
-```
-
-Trouvez le contenu du fichier envoyé au client.
-
-``` xml
-<http_handlers>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/get_absolute_path_static_handler</url>
-            <handler>
-                <type>static</type>
-                <content_type>text/html; charset=UTF-8</content_type>
-                <response_content>file:///absolute_path_file.html</response_content>
-            </handler>
-        </rule>
-        <rule>
-            <methods>GET</methods>
-            <headers><XXX>xxx</XXX></headers>
-            <url>/get_relative_path_static_handler</url>
-            <handler>
-                <type>static</type>
-                <content_type>text/html; charset=UTF-8</content_type>
-                <response_content>file://./relative_path_file.html</response_content>
-            </handler>
-        </rule>
-</http_handlers>
-```
-
-``` bash
-$ user_files_path='/var/lib/clickhouse/user_files'
-$ sudo echo "<html><body>Relative Path File</body></html>" > $user_files_path/relative_path_file.html
-$ sudo echo "<html><body>Absolute Path File</body></html>" > $user_files_path/absolute_path_file.html
-$ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_absolute_path_static_handler'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /get_absolute_path_static_handler HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 200 OK
-< Date: Wed, 29 Apr 2020 04:18:16 GMT
-< Connection: Keep-Alive
-< Content-Type: text/html; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-<html><body>Absolute Path File</body></html>
-* Connection #0 to host localhost left intact
-$ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_relative_path_static_handler'
-*   Trying ::1...
-* Connected to localhost (::1) port 8123 (#0)
-> GET /get_relative_path_static_handler HTTP/1.1
-> Host: localhost:8123
-> User-Agent: curl/7.47.0
-> Accept: */*
-> XXX:xxx
->
-< HTTP/1.1 200 OK
-< Date: Wed, 29 Apr 2020 04:18:31 GMT
-< Connection: Keep-Alive
-< Content-Type: text/html; charset=UTF-8
-< Transfer-Encoding: chunked
-< Keep-Alive: timeout=3
-< X-ClickHouse-Summary: {"read_rows":"0","read_bytes":"0","written_rows":"0","written_bytes":"0","total_rows_to_read":"0"}
-<
-<html><body>Relative Path File</body></html>
-* Connection #0 to host localhost left intact
-```
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/http_interface/) <!--hide-->
diff --git a/docs/fr/interfaces/index.md b/docs/fr/interfaces/index.md
deleted file mode 100644
index bba875be1f25..000000000000
--- a/docs/fr/interfaces/index.md
+++ /dev/null
@@ -1,29 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Interface
-toc_priority: 14
-toc_title: Introduction
----
-
-# Interface {#interfaces}
-
-ClickHouse fournit deux interfaces réseau (les deux peuvent être encapsulées en option dans TLS pour plus de sécurité):
-
--   [HTTP](http.md) qui est documenté et facile à utiliser directement.
--   [Natif de TCP](tcp.md) qui a moins de frais généraux.
-
-Dans la plupart des cas, il est recommandé d'utiliser un outil ou une bibliothèque approprié au lieu d'interagir directement avec ceux-ci. Officiellement pris en charge par Yandex sont les suivants:
-
--   [Client de ligne de commande](cli.md)
--   [JDBC](jdbc.md)
--   [Pilote ODBC](odbc.md)
--   [Bibliothèque client c++ ](cpp.md)
-
-Il existe également un large éventail de bibliothèques tierces pour travailler avec ClickHouse:
-
--   [Bibliothèques clientes](third-party/client-libraries.md)
--   [Intégration](third-party/integrations.md)
--   [Les interfaces visuelles](third-party/gui.md)
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/) <!--hide-->
diff --git a/docs/fr/interfaces/jdbc.md b/docs/fr/interfaces/jdbc.md
deleted file mode 100644
index dc68dcb98316..000000000000
--- a/docs/fr/interfaces/jdbc.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 22
-toc_title: JDBC
----
-
-# JDBC {#jdbc-driver}
-
--   **[Pilote officiel](https://github.com/ClickHouse/clickhouse-jdbc)**
--   Les pilotes tiers:
-    -   [ClickHouse-natif-JDBC](https://github.com/housepower/ClickHouse-Native-JDBC)
-    -   [clickhouse4j](https://github.com/blynkkk/clickhouse4j)
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/jdbc/) <!--hide-->
diff --git a/docs/fr/interfaces/mysql.md b/docs/fr/interfaces/mysql.md
deleted file mode 100644
index 051538d45b23..000000000000
--- a/docs/fr/interfaces/mysql.md
+++ /dev/null
@@ -1,49 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 20
-toc_title: Interface MySQL
----
-
-# Interface MySQL {#mysql-interface}
-
-ClickHouse prend en charge le protocole de fil MySQL. Il peut être activé par [mysql_port](../operations/server-configuration-parameters/settings.md#server_configuration_parameters-mysql_port) paramètre dans le fichier de configuration:
-
-``` xml
-<mysql_port>9004</mysql_port>
-```
-
-Exemple de connexion à l'aide d'outil de ligne de commande `mysql`:
-
-``` bash
-$ mysql --protocol tcp -u default -P 9004
-```
-
-Sortie si une connexion a réussi:
-
-``` text
-Welcome to the MySQL monitor.  Commands end with ; or \g.
-Your MySQL connection id is 4
-Server version: 20.2.1.1-ClickHouse
-
-Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
-
-Oracle is a registered trademark of Oracle Corporation and/or its
-affiliates. Other names may be trademarks of their respective
-owners.
-
-Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
-
-mysql>
-```
-
-Pour la compatibilité avec tous les clients MySQL, il est recommandé de spécifier le mot de passe utilisateur avec [double SHA1](../operations/settings/settings-users.md#password_double_sha1_hex) dans le fichier de configuration.
-Si le mot de passe de l'utilisateur est spécifié [SHA256](../operations/settings/settings-users.md#password_sha256_hex), certains clients ne pourront pas s'authentifier (mysqljs et anciennes versions de l'outil de ligne de commande mysql).
-
-Restriction:
-
--   les requêtes préparées ne sont pas prises en charge
-
--   certains types de données sont envoyés sous forme de chaînes
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/mysql/) <!--hide-->
diff --git a/docs/fr/interfaces/odbc.md b/docs/fr/interfaces/odbc.md
deleted file mode 100644
index 991dc833502a..000000000000
--- a/docs/fr/interfaces/odbc.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 23
-toc_title: Pilote ODBC
----
-
-# Pilote ODBC {#odbc-driver}
-
--   [Pilote officiel](https://github.com/ClickHouse/clickhouse-odbc).
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/odbc/) <!--hide-->
diff --git a/docs/fr/interfaces/tcp.md b/docs/fr/interfaces/tcp.md
deleted file mode 100644
index d1fa251e0e49..000000000000
--- a/docs/fr/interfaces/tcp.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 18
-toc_title: Interface Native (TCP)
----
-
-# Interface Native (TCP) {#native-interface-tcp}
-
-Le protocole natif est utilisé dans le [client de ligne de commande](cli.md), pour la communication inter-serveur pendant le traitement de requête distribué, et également dans d'autres programmes C++. Malheureusement, le protocole clickhouse natif n'a pas encore de spécification formelle, mais il peut être rétro-conçu à partir du code source ClickHouse (démarrage [ici](https://github.com/ClickHouse/ClickHouse/tree/master/src/Client)) et/ou en interceptant et en analysant le trafic TCP.
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/tcp/) <!--hide-->
diff --git a/docs/fr/interfaces/third-party/client-libraries.md b/docs/fr/interfaces/third-party/client-libraries.md
deleted file mode 100644
index 7949aa1d7cf7..000000000000
--- a/docs/fr/interfaces/third-party/client-libraries.md
+++ /dev/null
@@ -1,62 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 26
-toc_title: "Biblioth\xE8ques Clientes"
----
-
-# Bibliothèques clientes de développeurs tiers {#client-libraries-from-third-party-developers}
-
-!!! warning "Avertissement"
-    Yandex ne **pas** maintenir les bibliothèques énumérées ci-dessous et n'ont pas fait de tests approfondis pour assurer leur qualité.
-
--   Python
-    -   [infi.clickhouse_orm](https://github.com/Infinidat/infi.clickhouse_orm)
-    -   [clickhouse-chauffeur](https://github.com/mymarilyn/clickhouse-driver)
-    -   [clickhouse-client](https://github.com/yurial/clickhouse-client)
-    -   [aiochclient](https://github.com/maximdanilchenko/aiochclient)
-    -   [asynch](https://github.com/long2ice/asynch)
--   PHP
-    -   [smi2 / phpclickhouse](https://packagist.org/packages/smi2/phpClickHouse)
-    -   [8bitov / clickhouse-PHP-client](https://packagist.org/packages/8bitov/clickhouse-php-client)
-    -   [bozerkins / clickhouse-client](https://packagist.org/packages/bozerkins/clickhouse-client)
-    -   [simpod / clickhouse-client](https://packagist.org/packages/simpod/clickhouse-client)
-    -   [seva-code/php-cliquez-maison-client](https://packagist.org/packages/seva-code/php-click-house-client)
-    -   [Client Seasclick c++ ](https://github.com/SeasX/SeasClick)
--   Aller
-    -   [clickhouse](https://github.com/kshvakov/clickhouse/)
-    -   [aller-clickhouse](https://github.com/roistat/go-clickhouse)
-    -   [mailrugo-clickhouse](https://github.com/mailru/go-clickhouse)
-    -   [golang-clickhouse](https://github.com/leprosus/golang-clickhouse)
--   NodeJs
-    -   [clickhouse (NodeJs)](https://github.com/TimonKK/clickhouse)
-    -   [node-clickhouse](https://github.com/apla/node-clickhouse)
--   Perl
-    -   [perl-DBD-ClickHouse](https://github.com/elcamlost/perl-DBD-ClickHouse)
-    -   [HTTP-ClickHouse](https://metacpan.org/release/HTTP-ClickHouse)
-    -   [AnyEvent-ClickHouse](https://metacpan.org/release/AnyEvent-ClickHouse)
--   Rubis
-    -   [ClickHouse (Ruby)](https://github.com/shlima/click_house)
-    -   [clickhouse-activerecord](https://github.com/PNixx/clickhouse-activerecord)
--   R
-    -   [clickhouse-r](https://github.com/hannesmuehleisen/clickhouse-r)
-    -   [RClickHouse](https://github.com/IMSMWU/RClickHouse)
--   Java
-    -   [clickhouse-client-java](https://github.com/VirtusAI/clickhouse-client-java)
-    -   [clickhouse-client](https://github.com/Ecwid/clickhouse-client)
--   Scala
-    -   [clickhouse-Scala-client](https://github.com/crobox/clickhouse-scala-client)
--   Kotlin
-    -   [AORM](https://github.com/TanVD/AORM)
--   C#
-    -   [Octonica.ClickHouseClient](https://github.com/Octonica/ClickHouseClient)
-    -   [ClickHouse.Ado](https://github.com/killwort/ClickHouse-Net)
-    -   [ClickHouse.Client](https://github.com/DarkWanderer/ClickHouse.Client)
-    -   [ClickHouse.Net](https://github.com/ilyabreev/ClickHouse.Net)
--   Elixir
-    -   [clickhousex](https://github.com/appodeal/clickhousex/)
-    -   [pilier](https://github.com/sofakingworld/pillar)
--   Nim
-    -   [nim-clickhouse](https://github.com/leonardoce/nim-clickhouse)
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/third-party/client_libraries/) <!--hide-->
diff --git a/docs/fr/interfaces/third-party/gui.md b/docs/fr/interfaces/third-party/gui.md
deleted file mode 100644
index af25f86ee545..000000000000
--- a/docs/fr/interfaces/third-party/gui.md
+++ /dev/null
@@ -1,156 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 28
-toc_title: Les Interfaces Visuelles
----
-
-# Interfaces visuelles de développeurs tiers {#visual-interfaces-from-third-party-developers}
-
-## Open-Source {#open-source}
-
-### Tabix {#tabix}
-
-Interface Web pour ClickHouse dans le [Tabix](https://github.com/tabixio/tabix) projet.
-
-Caractéristique:
-
--   Fonctionne avec ClickHouse directement à partir du navigateur, sans avoir besoin d'installer un logiciel supplémentaire.
--   Éditeur de requête avec coloration syntaxique.
--   L'Auto-complétion des commandes.
--   Outils d'analyse graphique de l'exécution des requêtes.
--   Options de jeu de couleurs.
-
-[Documentation Tabix](https://tabix.io/doc/).
-
-### HouseOps {#houseops}
-
-[HouseOps](https://github.com/HouseOps/HouseOps) est une interface utilisateur / IDE pour OSX, Linux et Windows.
-
-Caractéristique:
-
--   Générateur de requêtes avec coloration syntaxique. Affichez la réponse dans une table ou une vue JSON.
--   Exporter les résultats de la requête au format CSV ou JSON.
--   Liste des processus avec des descriptions. Le mode d'écriture. Capacité à arrêter (`KILL`) processus.
--   Base de données du graphique. Affiche toutes les tables et leurs colonnes avec des informations supplémentaires.
--   Une vue rapide de la taille de la colonne.
--   La configuration du serveur.
-
-Les fonctionnalités suivantes sont prévues pour le développement:
-
--   Gestion de base de données.
--   La gestion des utilisateurs.
--   En temps réel l'analyse des données.
--   Surveillance de Cluster.
--   La gestion de Cluster.
--   Suivi des tables répliquées et Kafka.
-
-### Phare {#lighthouse}
-
-[Phare](https://github.com/VKCOM/lighthouse) est une interface web légère pour ClickHouse.
-
-Caractéristique:
-
--   Liste de Table avec filtrage et métadonnées.
--   Aperçu de la Table avec filtrage et tri.
--   Les requêtes en lecture seule exécution.
-
-### Redash {#redash}
-
-[Redash](https://github.com/getredash/redash) est une plate-forme pour la visualisation des données.
-
-Prise en charge de plusieurs sources de données, y compris ClickHouse, Redash peut joindre les résultats des requêtes provenant de différentes sources de données dans un ensemble de données final.
-
-Caractéristique:
-
--   Puissant éditeur de requêtes.
--   Explorateur de base de données.
--   Des outils de visualisation qui vous permettent de représenter des données sous différentes formes.
-
-### DBeaver {#dbeaver}
-
-[DBeaver](https://dbeaver.io/) - client de base de données de bureau universel avec support ClickHouse.
-
-Caractéristique:
-
--   Développement de requêtes avec mise en évidence de la syntaxe et complétion automatique.
--   Liste de Table avec filtres et recherche de métadonnées.
--   Aperçu des données de la Table.
--   Recherche en texte intégral.
-
-### clickhouse-cli {#clickhouse-cli}
-
-[clickhouse-cli](https://github.com/hatarist/clickhouse-cli) est un client de ligne de commande alternatif pour ClickHouse, écrit en Python 3.
-
-Caractéristique:
-
--   Complétion.
--   Coloration syntaxique pour les requêtes et la sortie de données.
--   Support Pager pour la sortie de données.
--   Commandes personnalisées de type PostgreSQL.
-
-### clickhouse-flamegraph {#clickhouse-flamegraph}
-
-[clickhouse-flamegraph](https://github.com/Slach/clickhouse-flamegraph) est un outil spécialisé pour visualiser la `system.trace_log` comme [flamegraph](http://www.brendangregg.com/flamegraphs.html).
-
-### clickhouse-plantuml {#clickhouse-plantuml}
-
-[cickhouse-plantuml](https://pypi.org/project/clickhouse-plantuml/) est un script à générer [PlantUML](https://plantuml.com/) schéma des schémas des tableaux.
-
-## Commercial {#commercial}
-
-### DataGrip {#datagrip}
-
-[DataGrip](https://www.jetbrains.com/datagrip/) est un IDE de base de données de JetBrains avec un support dédié pour ClickHouse. Il est également intégré dans D'autres outils basés sur IntelliJ: PyCharm, IntelliJ IDEA, GoLand, PhpStorm et autres.
-
-Caractéristique:
-
--   Achèvement du code très rapide.
--   Mise en évidence de la syntaxe ClickHouse.
--   Prise en charge des fonctionnalités spécifiques à ClickHouse, par exemple, les colonnes imbriquées, les moteurs de table.
--   Éditeur De Données.
--   Refactoring.
--   Recherche et Navigation.
-
-### Yandex DataLens {#yandex-datalens}
-
-[Yandex DataLens](https://cloud.yandex.ru/services/datalens) est un service de visualisation et d'analyse de données.
-
-Caractéristique:
-
--   Large gamme de visualisations disponibles, des graphiques à barres simples aux tableaux de bord complexes.
--   Les tableaux de bord pourraient être rendus publics.
--   Prise en charge de plusieurs sources de données, y compris ClickHouse.
--   Stockage de données matérialisées basé sur ClickHouse.
-
-DataLens est [disponible gratuitement](https://cloud.yandex.com/docs/datalens/pricing) pour les projets à faible charge, même pour un usage commercial.
-
--   [Documentation DataLens](https://cloud.yandex.com/docs/datalens/).
--   [Tutoriel](https://cloud.yandex.com/docs/solutions/datalens/data-from-ch-visualization) sur la visualisation des données à partir d'une base de données ClickHouse.
-
-### Logiciel Holistics {#holistics-software}
-
-[Holistics](https://www.holistics.io/) est une plate-forme de données à pile complète et un outil de business intelligence.
-
-Caractéristique:
-
--   E-mail automatisé, Slack et Google Sheet horaires de rapports.
--   Éditeur SQL avec visualisations, contrôle de version, auto-complétion, composants de requête réutilisables et filtres dynamiques.
--   Analyse intégrée des rapports et des tableaux de bord via iframe.
--   Préparation des données et capacités ETL.
--   Prise en charge de la modélisation des données SQL pour la cartographie relationnelle des données.
-
-### Looker {#looker}
-
-[Looker](https://looker.com) est une plate-forme de données et un outil de business intelligence avec prise en charge de plus de 50 dialectes de base de données, y compris ClickHouse. Looker est disponible en tant que plate-forme SaaS et auto-hébergé. Les utilisateurs peuvent utiliser Looker via le navigateur pour explorer les données, créer des visualisations et des tableaux de bord, planifier des rapports et partager leurs idées avec des collègues. Looker fournit un riche ensemble d'outils pour intégrer ces fonctionnalités dans d'autres applications, et une API
-pour intégrer les données avec d'autres applications.
-
-Caractéristique:
-
--   Développement facile et agile en utilisant LookML, un langage qui prend en charge curated
-    [La Modélisation Des Données](https://looker.com/platform/data-modeling) pour soutenir les auteurs et les utilisateurs finaux.
--   Intégration de flux de travail puissante via Looker [Actions De Données](https://looker.com/platform/actions).
-
-[Comment configurer ClickHouse dans Looker.](https://docs.looker.com/setup-and-management/database-config/clickhouse)
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/third-party/gui/) <!--hide-->
diff --git a/docs/fr/interfaces/third-party/index.md b/docs/fr/interfaces/third-party/index.md
deleted file mode 100644
index fd721a39c3eb..000000000000
--- a/docs/fr/interfaces/third-party/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: tiers
-toc_priority: 24
----
-
-
diff --git a/docs/fr/interfaces/third-party/integrations.md b/docs/fr/interfaces/third-party/integrations.md
deleted file mode 100644
index 1b74a49dc894..000000000000
--- a/docs/fr/interfaces/third-party/integrations.md
+++ /dev/null
@@ -1,110 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 27
-toc_title: "Int\xE9gration"
----
-
-# Bibliothèques d'intégration de développeurs tiers {#integration-libraries-from-third-party-developers}
-
-!!! warning "Avertissement"
-    Yandex ne **pas** maintenir les outils et les bibliothèques énumérés ci-dessous et n'ont pas fait de tests approfondis pour assurer leur qualité.
-
-## Produits D'Infrastructure {#infrastructure-products}
-
--   Systèmes de gestion de bases de données relationnelles
-    -   [MySQL](https://www.mysql.com)
-        -   [mysql2ch](https://github.com/long2ice/mysql2ch)
-        -   [ProxySQL](https://github.com/sysown/proxysql/wiki/ClickHouse-Support)
-        -   [clickhouse-mysql-lecteur de données](https://github.com/Altinity/clickhouse-mysql-data-reader)
-        -   [horgh-réplicateur](https://github.com/larsnovikov/horgh-replicator)
-    -   [PostgreSQL](https://www.postgresql.org)
-        -   [clickhousedb_fdw](https://github.com/Percona-Lab/clickhousedb_fdw)
-        -   [infi.clickhouse_fdw](https://github.com/Infinidat/infi.clickhouse_fdw) (utiliser [infi.clickhouse_orm](https://github.com/Infinidat/infi.clickhouse_orm))
-        -   [pg2ch](https://github.com/mkabilov/pg2ch)
-        -   [clickhouse_fdw](https://github.com/adjust/clickhouse_fdw)
-    -   [MSSQL](https://en.wikipedia.org/wiki/Microsoft_SQL_Server)
-        -   [ClickHouseMigrator](https://github.com/zlzforever/ClickHouseMigrator)
--   Files d'attente de messages
-    -   [Kafka](https://kafka.apache.org)
-        -   [clickhouse_sinker](https://github.com/housepower/clickhouse_sinker) (utiliser [Allez client](https://github.com/ClickHouse/clickhouse-go/))
-        -   [stream-loader-clickhouse](https://github.com/adform/stream-loader)
--   Traitement de flux
-    -   [Flink](https://flink.apache.org)
-        -   [flink-clickhouse-évier](https://github.com/ivi-ru/flink-clickhouse-sink)
--   Objet de stockages
-    -   [S3](https://en.wikipedia.org/wiki/Amazon_S3)
-        -   [clickhouse-sauvegarde](https://github.com/AlexAkulov/clickhouse-backup)
--   Orchestration de conteneur
-    -   [Kubernetes](https://kubernetes.io)
-        -   [clickhouse-opérateur](https://github.com/Altinity/clickhouse-operator)
--   Gestion de la Configuration
-    -   [marionnette](https://puppet.com)
-        -   [innogames / clickhouse](https://forge.puppet.com/innogames/clickhouse)
-        -   [mfedotov / clickhouse](https://forge.puppet.com/mfedotov/clickhouse)
--   Surveiller
-    -   [Graphite](https://graphiteapp.org)
-        -   [graphouse](https://github.com/yandex/graphouse)
-        -   [carbone-clickhouse](https://github.com/lomik/carbon-clickhouse) +
-        -   [graphite-clickhouse](https://github.com/lomik/graphite-clickhouse)
-        -   [graphite-CH-optimizer](https://github.com/innogames/graphite-ch-optimizer) - optimise les partitions calées dans [\* GraphiteMergeTree](../../engines/table-engines/mergetree-family/graphitemergetree.md#graphitemergetree) si les règles de [configuration de cumul](../../engines/table-engines/mergetree-family/graphitemergetree.md#rollup-configuration) pourrait être appliquée
-    -   [Grafana](https://grafana.com/)
-        -   [clickhouse-grafana](https://github.com/Vertamedia/clickhouse-grafana)
-    -   [Prometheus](https://prometheus.io/)
-        -   [clickhouse_exporter](https://github.com/f1yegor/clickhouse_exporter)
-        -   [PromHouse](https://github.com/Percona-Lab/PromHouse)
-        -   [clickhouse_exporter](https://github.com/hot-wifi/clickhouse_exporter) (utiliser [Allez client](https://github.com/kshvakov/clickhouse/))
-    -   [Nagios](https://www.nagios.org/)
-        -   [check_clickhouse](https://github.com/exogroup/check_clickhouse/)
-        -   [check_clickhouse.py](https://github.com/innogames/igmonplugins/blob/master/src/check_clickhouse.py)
-    -   [Zabbix](https://www.zabbix.com)
-        -   [clickhouse-Zabbix-modèle](https://github.com/Altinity/clickhouse-zabbix-template)
-    -   [Sematext](https://sematext.com/)
-        -   [intégration de clickhouse](https://github.com/sematext/sematext-agent-integrations/tree/master/clickhouse)
--   Journalisation
-    -   [rsyslog](https://www.rsyslog.com/)
-        -   [omclickhouse](https://www.rsyslog.com/doc/master/configuration/modules/omclickhouse.html)
-    -   [fluentd](https://www.fluentd.org)
-        -   [maison de bois](https://github.com/flant/loghouse) (pour [Kubernetes](https://kubernetes.io))
-    -   [logagent](https://www.sematext.com/logagent)
-        -   [sortie logagent-plugin-clickhouse](https://sematext.com/docs/logagent/output-plugin-clickhouse/)
--   Geo
-    -   [MaxMind](https://dev.maxmind.com/geoip/)
-        -   [clickhouse-maxmind-geoip](https://github.com/AlexeyKupershtokh/clickhouse-maxmind-geoip)
-
-## Écosystèmes De Langage De Programmation {#programming-language-ecosystems}
-
--   Python
-    -   [SQLAlchemy](https://www.sqlalchemy.org)
-        -   [sqlalchemy-clickhouse](https://github.com/cloudflare/sqlalchemy-clickhouse) (utiliser [infi.clickhouse_orm](https://github.com/Infinidat/infi.clickhouse_orm))
-    -   [Panda](https://pandas.pydata.org)
-        -   [pandahouse](https://github.com/kszucs/pandahouse)
--   PHP
-    -   [Doctrine](https://www.doctrine-project.org/)
-        -   [dbal-clickhouse](https://packagist.org/packages/friendsofdoctrine/dbal-clickhouse)
--   R
-    -   [dplyr](https://db.rstudio.com/dplyr/)
-        -   [RClickHouse](https://github.com/IMSMWU/RClickHouse) (utiliser [clickhouse-cpp](https://github.com/artpaul/clickhouse-cpp))
--   Java
-    -   [Hadoop](http://hadoop.apache.org)
-        -   [clickhouse-HDFS-chargeur](https://github.com/jaykelin/clickhouse-hdfs-loader) (utiliser [JDBC](../../sql-reference/table-functions/jdbc.md))
--   Scala
-    -   [Akka](https://akka.io)
-        -   [clickhouse-Scala-client](https://github.com/crobox/clickhouse-scala-client)
--   C#
-    -   [ADO.NET](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/ado-net-overview)
-        -   [ClickHouse.Ado](https://github.com/killwort/ClickHouse-Net)
-        -   [ClickHouse.Client](https://github.com/DarkWanderer/ClickHouse.Client)
-        -   [ClickHouse.Net](https://github.com/ilyabreev/ClickHouse.Net)
-        -   [ClickHouse.Net.Les Migrations](https://github.com/ilyabreev/ClickHouse.Net.Migrations)
--   Elixir
-    -   [Ecto](https://github.com/elixir-ecto/ecto)
-        -   [clickhouse_ecto](https://github.com/appodeal/clickhouse_ecto)
--   Ruby
-    -   [Ruby on Rails](https://rubyonrails.org/)
-        -   [activecube](https://github.com/bitquery/activecube)
-        -   [ActiveRecord](https://github.com/PNixx/clickhouse-activerecord)
-    -   [GraphQL](https://github.com/graphql)
-        -   [activecube-graphql](https://github.com/bitquery/activecube-graphql)
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/third-party/integrations/) <!--hide-->
diff --git a/docs/fr/interfaces/third-party/proxy.md b/docs/fr/interfaces/third-party/proxy.md
deleted file mode 100644
index 0283cc7fbe63..000000000000
--- a/docs/fr/interfaces/third-party/proxy.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 29
-toc_title: Proxy
----
-
-# Serveurs Proxy de développeurs tiers {#proxy-servers-from-third-party-developers}
-
-## chproxy {#chproxy}
-
-[chproxy](https://github.com/Vertamedia/chproxy), est un proxy HTTP et un équilibreur de charge pour la base de données ClickHouse.
-
-Caractéristique:
-
--   Routage par utilisateur et mise en cache des réponses.
--   Limites flexibles.
--   Renouvellement automatique du certificat SSL.
-
-Mis en œuvre dans Go.
-
-## KittenHouse {#kittenhouse}
-
-[KittenHouse](https://github.com/VKCOM/kittenhouse) est conçu pour être un proxy local entre ClickHouse et serveur d'applications dans le cas où il est impossible ou gênant d'insérer des données en mémoire tampon du côté de votre application.
-
-Caractéristique:
-
--   En mémoire et sur disque de données en mémoire tampon.
--   Routage par table.
--   Équilibrage de charge et vérification de la santé.
-
-Mis en œuvre dans Go.
-
-## ClickHouse-Vrac {#clickhouse-bulk}
-
-[ClickHouse-Vrac](https://github.com/nikepan/clickhouse-bulk) est un collecteur simple D'insertion de ClickHouse.
-
-Caractéristique:
-
--   Groupez les demandes et envoyez-les par seuil ou intervalle.
--   Plusieurs serveurs distants.
--   L'authentification de base.
-
-Mis en œuvre dans Go.
-
-[Article Original](https://clickhouse.tech/docs/en/interfaces/third-party/proxy/) <!--hide-->
diff --git a/docs/fr/introduction/adopters.md b/docs/fr/introduction/adopters.md
deleted file mode 100644
index e970c61955ca..000000000000
--- a/docs/fr/introduction/adopters.md
+++ /dev/null
@@ -1,86 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 8
-toc_title: Adoptant
----
-
-# Clickhouse Adopteurs {#clickhouse-adopters}
-
-!!! warning "Avertissement"
-    La liste suivante des entreprises utilisant ClickHouse et leurs histoires de réussite est assemblé à partir de sources publiques, pourrait donc différer de la réalité actuelle. Nous vous serions reconnaissants si vous partager l'histoire de l'adoption de ClickHouse dans votre entreprise et [ajouter à la liste](https://github.com/ClickHouse/ClickHouse/edit/master/docs/en/introduction/adopters.md), mais assurez-vous que vous n'aurez aucun problème de NDA en le faisant. Fournir des mises à jour avec des publications d'autres entreprises est également utile.
-
-| Entreprise                                                                                      | Industrie                           | Cas d'utilisation            | La Taille De Cluster                                            | (Onu)Taille Des Données Compressées<abbr title="of single replica"><sup>\*</sup></abbr> | Référence                                                                                                                                                                                                                        |
-|-------------------------------------------------------------------------------------------------|-------------------------------------|------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| <a href="https://2gis.ru" class="favicon">2gis</a>                                              | Cartographie                        | Surveiller                   | —                                                               | —                                                                                       | [Parler en russe, juillet 2019](https://youtu.be/58sPkXfq6nw)                                                                                                                                                                    |
-| <a href="https://alohabrowser.com/" class="favicon">Aloha Browser</a>                           | Mobile App                          | Navigateur backend           | —                                                               | —                                                                                       | [Diapositives en russe, mai 2019](https://github.com/yandex/clickhouse-presentations/blob/master/meetup22/aloha.pdf)                                                                                                             |
-| <a href="https://amadeus.com/" class="favicon">Amadeus</a>                                      | Voyage                              | Analytics                    | —                                                               | —                                                                                       | [Communiqué De Presse, Avril 2018](https://www.altinity.com/blog/2018/4/5/amadeus-technologies-launches-investment-and-insights-tool-based-on-machine-learning-and-strategy-algorithms)                                          |
-| <a href="https://www.appsflyer.com" class="favicon">Appsflyer</a>                               | Mobile analytics                    | Produit principal            | —                                                               | —                                                                                       | [Parler en russe, juillet 2019](https://www.youtube.com/watch?v=M3wbRlcpBbY)                                                                                                                                                     |
-| <a href="https://arenadata.tech/" class="favicon">ArenaData</a>                                 | Plate-Forme De Données              | Produit principal            | —                                                               | —                                                                                       | [Diapositives en russe, décembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup38/indexes.pdf)                                                                                                  |
-| <a href="https://badoo.com" class="favicon">Badoo</a>                                           | Rencontres                          | Timeseries                   | —                                                               | —                                                                                       | [Diapositives en russe, décembre 2019](https://presentations.clickhouse.tech/meetup38/forecast.pdf)                                                                                                                              |
-| <a href="https://www.benocs.com/" class="favicon">Benocs</a>                                    | Télémétrie et analyse de réseau     | Produit Principal            | —                                                               | —                                                                                       | [Diapositives en anglais, octobre 2017](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup9/lpm.pdf)                                                                                                      |
-| <a href="https://www.bloomberg.com/" class="favicon">Bloomberg</a>                              | Les Finances, Les Médias            | Surveiller                   | 102 serveurs                                                    | —                                                                                       | [Diapositives, Mai 2018](https://www.slideshare.net/Altinity/http-analytics-for-6m-requests-per-second-using-clickhouse-by-alexander-bocharov)                                                                                   |
-| <a href="https://bloxy.info" class="favicon">Bloxy</a>                                          | Blockchain                          | Analytics                    | —                                                               | —                                                                                       | [Diapositives en russe, août 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup17/4_bloxy.pptx)                                                                                                     |
-| <a href="https://www.chinatelecomglobal.com/" class="favicon">Dataliance pour China Telecom</a> | Télécommunication                   | Analytics                    | —                                                               | —                                                                                       | [Diapositives en chinois, janvier 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup12/telecom.pdf)                                                                                                 |
-| <a href="https://carto.com/" class="favicon">CARTO</a>                                          | Business Intelligence               | GEO analytics                | —                                                               | —                                                                                       | [Traitement géospatial avec ClickHouse](https://carto.com/blog/geospatial-processing-with-clickhouse/)                                                                                                                           |
-| <a href="http://public.web.cern.ch/public/" class="favicon">CERN</a>                            | Recherche                           | Expérience                   | —                                                               | —                                                                                       | [Communiqué De Presse, avril 2012](https://www.yandex.com/company/press_center/press_releases/2012/2012-04-10/)                                                                                                                  |
-| <a href="http://cisco.com/" class="favicon">Cisco</a>                                           | Réseau                              | L'analyse de trafic          | —                                                               | —                                                                                       | [Lightning talk, octobre 2019](https://youtu.be/-hI1vDR2oPY?t=5057)                                                                                                                                                              |
-| <a href="https://www.citadelsecurities.com/" class="favicon">Citadel Securities</a>             | Finance                             | —                            | —                                                               | —                                                                                       | [Contribution, Mars 2019](https://github.com/ClickHouse/ClickHouse/pull/4774)                                                                                                                                                    |
-| <a href="https://city-mobil.ru" class="favicon">Citymobil</a>                                   | Taxi                                | Analytics                    | —                                                               | —                                                                                       | [Billet de Blog en russe, mars 2020](https://habr.com/en/company/citymobil/blog/490660/)                                                                                                                                         |
-| <a href="https://contentsquare.com" class="favicon">ContentSquare</a>                           | Web analytics                       | Produit principal            | —                                                               | —                                                                                       | [Billet de Blog, novembre 2018](http://souslecapot.net/2018/11/21/patrick-chatain-vp-engineering-chez-contentsquare-penser-davantage-amelioration-continue-que-revolution-constante/)                                            |
-| <a href="https://cloudflare.com" class="favicon">Cloudflare</a>                                 | CDN                                 | L'analyse de trafic          | 36 serveurs                                                     | —                                                                                       | [Billet de Blog, mai 2017](https://blog.cloudflare.com/how-cloudflare-analyzes-1m-dns-queries-per-second/), [Billet de Blog, mars 2018](https://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/) |
-| <a href="https://coru.net/" class="favicon">Corunet</a>                                         | Analytics                           | Produit principal            | —                                                               | —                                                                                       | [Diapositives en anglais, avril 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup21/predictive_models.pdf)                                                                                         |
-| <a href="https://www.creditx.com" class="favicon">CraiditX 氪信</a>                             | Les finances de l'IA                | Analyse                      | —                                                               | —                                                                                       | [Diapositives en anglais, novembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup33/udf.pptx)                                                                                                   |
-| <a href="https://www.criteo.com/" class="favicon">Criteo</a>                                    | Détail                              | Produit principal            | —                                                               | —                                                                                       | [Diapositives en anglais, octobre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup18/3_storetail.pptx)                                                                                            |
-| <a href="https://db.com" class="favicon">Deutsche Bank</a>                                      | Finance                             | BI Analytics                 | —                                                               | —                                                                                       | [Diapositives en anglais, octobre 2019](https://bigdatadays.ru/wp-content/uploads/2019/10/D2-H3-3_Yakunin-Goihburg.pdf)                                                                                                          |
-| <a href="https://www.diva-e.com" class="favicon">Diva-e</a>                                     | Conseil Digital                     | Produit Principal            | —                                                               | —                                                                                       | [Diapositives en anglais, septembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup29/ClickHouse-MeetUp-Unusual-Applications-sd-2019-09-17.pdf)                                                  |
-| <a href="https://www.exness.com" class="favicon">Exness</a>                                     | Trading                             | Métriques, Journalisation    | —                                                               | —                                                                                       | [Parler en russe, mai 2019](https://youtu.be/_rpU-TvSfZ8?t=3215)                                                                                                                                                                 |
-| <a href="https://geniee.co.jp" class="favicon">Geniee</a>                                       | Réseau publicitaire                 | Produit principal            | —                                                               | —                                                                                       | [Billet de Blog en japonais, juillet 2017](https://tech.geniee.co.jp/entry/2017/07/20/160100)                                                                                                                                    |
-| <a href="https://www.huya.com/" class="favicon">HUYA</a>                                        | Le Streaming Vidéo                  | Analytics                    | —                                                               | —                                                                                       | [Diapositives en chinois, octobre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/7.%20ClickHouse万亿数据分析实践%20李本旺(sundy-li)%20虎牙.pdf)                                               |
-| <a href="https://www.idealista.com" class="favicon">Idealista</a>                               | Immobilier                          | Analytics                    | —                                                               | —                                                                                       | [Billet de Blog en anglais, avril 2019](https://clickhouse.tech/blog/en/clickhouse-meetup-in-madrid-on-april-2-2019)                                                                                                           |
-| <a href="https://www.infovista.com/" class="favicon">Infovista</a>                              | Réseau                              | Analytics                    | —                                                               | —                                                                                       | [Diapositives en anglais, octobre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup30/infovista.pdf)                                                                                               |
-| <a href="https://www.innogames.com" class="favicon">InnoGames</a>                               | Jeu                                 | Métriques, Journalisation    | —                                                               | —                                                                                       | [Diapositives en russe, septembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup28/graphite_and_clickHouse.pdf)                                                                                 |
-| <a href="https://integros.com" class="favicon">Integros</a>                                     | Plate-forme pour les services vidéo | Analytics                    | —                                                               | —                                                                                       | [Diapositives en russe, mai 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup22/strategies.pdf)                                                                                                    |
-| <a href="https://www.kodiakdata.com/" class="favicon">Données Kodiak</a>                        | Nuage                               | Produit principal            | —                                                               | —                                                                                       | [Diapositives en Engish, avril 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup13/kodiak_data.pdf)                                                                                                |
-| <a href="https://kontur.ru" class="favicon">Kontur</a>                                          | Le Développement De Logiciels       | Métrique                     | —                                                               | —                                                                                       | [Parler en russe, novembre 2018](https://www.youtube.com/watch?v=U4u4Bd0FtrY)                                                                                                                                                    |
-| <a href="https://lifestreet.com/" class="favicon">LifeStreet</a>                                | Réseau publicitaire                 | Produit principal            | 75 serveurs (3 répliques)                                       | 5.27 FRP                                                                                | [Billet de Blog en russe, février 2017](https://habr.com/en/post/322620/)                                                                                                                                                        |
-| <a href="https://mcs.mail.ru/" class="favicon">Mail.ru Solutions Cloud</a>                      | Les services de Cloud               | Produit principal            | —                                                               | —                                                                                       | [Article en russe](https://mcs.mail.ru/help/db-create/clickhouse#)                                                                                                                                                               |
-| <a href="https://www.messagebird.com" class="favicon">MessageBird</a>                           | Télécommunication                   | Statistique                  | —                                                               | —                                                                                       | [Diapositives en anglais, novembre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup20/messagebird.pdf)                                                                                            |
-| <a href="https://www.mgid.com/" class="favicon">MGID</a>                                        | Réseau publicitaire                 | Web-analytics                | —                                                               | —                                                                                       | [Billet de Blog en russe, avril 2020](http://gs-studio.com/news-about-it/32777----clickhouse---c)                                                                                                                                |
-| <a href="https://www.oneapm.com/" class="favicon">OneAPM</a>                                    | Monitorings et analyse des données  | Produit principal            | —                                                               | —                                                                                       | [Diapositives en chinois, octobre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/8.%20clickhouse在OneAPM的应用%20杜龙.pdf)                                                                    |
-| <a href="http://www.pragma-innovation.fr/" class="favicon">Pragma Innovation</a>                | Télémétrie et analyse Big Data      | Produit principal            | —                                                               | —                                                                                       | [Diapositives en anglais, octobre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup18/4_pragma_innovation.pdf)                                                                                     |
-| <a href="https://www.qingcloud.com/" class="favicon">QINGCLOUD</a>                              | Les services de Cloud               | Produit principal            | —                                                               | —                                                                                       | [Diapositives en chinois, octobre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/4.%20Cloud%20%2B%20TSDB%20for%20ClickHouse%20张健%20QingCloud.pdf)                                           |
-| <a href="https://qrator.net" class="favicon">Qrator</a>                                         | Protection DDoS                     | Produit principal            | —                                                               | —                                                                                       | [Billet De Blog, Mars 2019](https://blog.qrator.net/en/clickhouse-ddos-mitigation_37/)                                                                                                                                           |
-| <a href="https://www.percent.cn/" class="favicon">Percent 百分点</a>                            | Analytics                           | Produit Principal            | —                                                               | —                                                                                       | [Diapositives en chinois, juin 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup24/4.%20ClickHouse万亿数据双中心的设计与实践%20.pdf)                                                               |
-| <a href="https://rambler.ru" class="favicon">Rambler</a>                                        | Services Internet                   | Analytics                    | —                                                               | —                                                                                       | [Parler en russe, avril 2018](https://medium.com/@ramblertop/разработка-api-clickhouse-для-рамблер-топ-100-f4c7e56f3141)                                                                                                         |
-| <a href="https://www.tencent.com" class="favicon">Tencent</a>                                   | Messagerie                          | Journalisation               | —                                                               | —                                                                                       | [Parler en chinois, novembre 2019](https://youtu.be/T-iVQRuw-QY?t=5050)                                                                                                                                                          |
-| <a href="https://trafficstars.com/" class="favicon">Traffic Stars</a>                           | Réseau publicitaire                 | —                            | —                                                               | —                                                                                       | [Diapositives en russe, mai 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup15/lightning/ninja.pdf)                                                                                               |
-| <a href="https://www.s7.ru" class="favicon">S7 Airlines</a>                                     | Avion                               | Métriques, Journalisation    | —                                                               | —                                                                                       | [Parler en russe, mars 2019](https://www.youtube.com/watch?v=nwG68klRpPg&t=15s)                                                                                                                                                  |
-| <a href="https://www.semrush.com/" class="favicon">SEMrush</a>                                  | Marketing                           | Produit principal            | —                                                               | —                                                                                       | [Diapositives en russe, août 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup17/5_semrush.pdf)                                                                                                    |
-| <a href="https://www.scireum.de/" class="favicon">scireum GmbH</a>                              | Ecommerce                           | Produit principal            | —                                                               | —                                                                                       | [Présentation en allemand, février 2020](https://www.youtube.com/watch?v=7QWAn5RbyR4)                                                                                                                                            |
-| <a href="https://sentry.io/" class="favicon">Sentry</a>                                         | Développeur de logiciels            | Backend pour le produit      | —                                                               | —                                                                                       | [Billet de Blog en anglais, mai 2019](https://blog.sentry.io/2019/05/16/introducing-snuba-sentrys-new-search-infrastructure)                                                                                                     |
-| <a href="http://www.sgk.gov.tr/wps/portal/sgk/tr" class="favicon">SGK</a>                       | Gouvernement Sécurité Sociale       | Analytics                    | —                                                               | —                                                                                       | [Diapositives en anglais, novembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup35/ClickHouse%20Meetup-Ramazan%20POLAT.pdf)                                                                    |
-| <a href="https://seo.do/" class="favicon">seo.do</a>                                            | Analytics                           | Produit principal            | —                                                               | —                                                                                       | [Diapositives en anglais, novembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup35/CH%20Presentation-%20Metehan%20Çetinkaya.pdf)                                                               |
-| <a href="http://english.sina.com/index.html" class="favicon">Sina</a>                           | Nouvelles                           | —                            | —                                                               | —                                                                                       | [Diapositives en chinois, octobre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/6.%20ClickHouse最佳实践%20高鹏_新浪.pdf)                                                                     |
-| <a href="https://smi2.ru/" class="favicon">SMI2</a>                                             | Nouvelles                           | Analytics                    | —                                                               | —                                                                                       | [Billet de Blog en russe, novembre 2017](https://habr.com/ru/company/smi2/blog/314558/)                                                                                                                                          |
-| <a href="https://www.splunk.com/" class="favicon">Splunk</a>                                    | Business Analytics                  | Produit principal            | —                                                               | —                                                                                       | [Diapositives en anglais, janvier 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup12/splunk.pdf)                                                                                                  |
-| <a href="https://www.spotify.com" class="favicon">Spotify</a>                                   | Musical                             | Expérimentation              | —                                                               | —                                                                                       | [Diapositives, Juillet 2018](https://www.slideshare.net/glebus/using-clickhouse-for-experimentation-104247173)                                                                                                                   |
-| <a href="https://www.tencent.com" class="favicon">Tencent</a>                                   | Big Data                            | Le traitement des données    | —                                                               | —                                                                                       | [Diapositives en chinois, octobre 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup19/5.%20ClickHouse大数据集群应用_李俊飞腾讯网媒事业部.pdf)                                                      |
-| <a href="https://www.uber.com" class="favicon">Uber</a>                                         | Taxi                                | Journalisation               | —                                                               | —                                                                                       | [Diapositives, Février 2020](https://presentations.clickhouse.tech/meetup40/uber.pdf)                                                                                                                                            |
-| <a href="https://vk.com" class="favicon">VKontakte</a>                                          | Réseau Social                       | Statistiques, Journalisation | —                                                               | —                                                                                       | [Diapositives en russe, août 2018](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup17/3_vk.pdf)                                                                                                         |
-| <a href="https://wisebits.com/" class="favicon">Wisebits</a>                                    | Solutions informatiques             | Analytics                    | —                                                               | —                                                                                       | [Diapositives en russe, mai 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup22/strategies.pdf)                                                                                                    |
-| <a href="http://www.xiaoxintech.cn/" class="favicon">Xiaoxin Tech</a>                           | Éducation                           | But commun                   | —                                                               | —                                                                                       | [Diapositives en anglais, novembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup33/sync-clickhouse-with-mysql-mongodb.pptx)                                                                    |
-| <a href="https://www.ximalaya.com/" class="favicon">Ximalaya</a>                                | Partage Audio                       | OLAP                         | —                                                               | —                                                                                       | [Diapositives en anglais, novembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup33/ximalaya.pdf)                                                                                               |
-| <a href="https://cloud.yandex.ru/services/managed-clickhouse" class="favicon">Yandex Cloud</a>  | Le Cloud Public                     | Produit principal            | —                                                               | —                                                                                       | [Parler en russe, décembre 2019](https://www.youtube.com/watch?v=pgnak9e_E0o)                                                                                                                                                    |
-| <a href="https://cloud.yandex.ru/services/datalens" class="favicon">Yandex DataLens</a>         | Business Intelligence               | Produit principal            | —                                                               | —                                                                                       | [Diapositives en russe, décembre 2019](https://presentations.clickhouse.tech/meetup38/datalens.pdf)                                                                                                                              |
-| <a href="https://market.yandex.ru/" class="favicon">Yandex Market</a>                           | Ecommerce                           | Métriques, Journalisation    | —                                                               | —                                                                                       | [Parler en russe, janvier 2019](https://youtu.be/_l1qP0DyBcA?t=478)                                                                                                                                                              |
-| <a href="https://metrica.yandex.com" class="favicon">Yandex Metrica</a>                         | Web analytics                       | Produit principal            | 360 serveurs dans un cluster, 1862 serveurs dans un département | 66,41 FRP / 5,68 FRP                                                                    | [Diapositives, Février 2020](https://presentations.clickhouse.tech/meetup40/introduction/#13)                                                                                                                                    |
-| <a href="https://htc-cs.ru/" class="favicon">ЦВТ</a>                                            | Le Développement De Logiciels       | Métriques, Journalisation    | —                                                               | —                                                                                       | [Billet de Blog, mars 2019, en russe](https://vc.ru/dev/62715-kak-my-stroili-monitoring-na-prometheus-clickhouse-i-elk)                                                                                                          |
-| <a href="https://mkb.ru/" class="favicon">МКБ</a>                                               | Banque                              | Surveillance du système Web  | —                                                               | —                                                                                       | [Diapositives en russe, septembre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup28/mkb.pdf)                                                                                                     |
-| <a href="https://jinshuju.net" class="favicon">Jinshuju 金数据</a>                              | BI Analytics                        | Produit principal            | —                                                               | —                                                                                       | [Diapositives en chinois, octobre 2019](https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup24/3.%20金数据数据架构调整方案Public.pdf)                                                                       |
-| <a href="https://www.instana.com" class="favicon">Instana</a>                                   | Plate-forme APM                     | Produit principal            | —                                                               | —                                                                                       | [Après Twitter](https://twitter.com/mieldonkers/status/1248884119158882304)                                                                                                                                                      |
-| <a href="https://wargaming.com/en/" class="favicon">Wargaming</a>                               | Jeu                                 |                              | —                                                               | —                                                                                       | [Entrevue](https://habr.com/en/post/496954/)                                                                                                                                                                                     |
-| <a href="https://crazypanda.ru/en/" class="favicon">Crazypanda</a>                              | Jeu                                 |                              | —                                                               | —                                                                                       | Session en direct sur clickhouse meetup                                                                                                                                                                                          |
-| <a href="https://fun.co/rp" class="favicon">FunCorp</a>                                         | Jeu                                 |                              | —                                                               | —                                                                                       | [Article](https://www.altinity.com/blog/migrating-from-redshift-to-clickhouse)                                                                                                                                                   |
-
-[Article Original](https://clickhouse.tech/docs/en/introduction/adopters/) <!--hide-->
diff --git a/docs/fr/introduction/distinctive-features.md b/docs/fr/introduction/distinctive-features.md
deleted file mode 100644
index 9d01d245c4e4..000000000000
--- a/docs/fr/introduction/distinctive-features.md
+++ /dev/null
@@ -1,77 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 4
-toc_title: "particularit\xE9"
----
-
-# Caractéristiques distinctives de ClickHouse {#distinctive-features-of-clickhouse}
-
-## Vrai SGBD orienté colonne {#true-column-oriented-dbms}
-
-Dans un vrai SGBD orienté colonne, aucune donnée supplémentaire n'est stockée avec les valeurs. Entre autres choses, cela signifie que les valeurs de longueur constante doivent être prises en charge, pour éviter de stocker leur longueur “number” à côté de ces valeurs. Par exemple, un milliard de valeurs de type UInt8 devrait consommer environ 1 Go non compressé, ou cela affecte fortement l'utilisation du processeur. Il est essentiel de stocker des données de manière compacte (sans “garbage”) même lorsqu'il n'est pas compressé, puisque la vitesse de décompression (utilisation du processeur) dépend principalement du volume de données non compressées.
-
-Il est à noter car il existe des systèmes qui peuvent stocker des valeurs de différentes colonnes séparément, mais qui ne peuvent pas traiter efficacement les requêtes analytiques en raison de leur optimisation pour d'autres scénarios. Les exemples sont HBase, BigTable, Cassandra et HyperTable. Dans ces systèmes, vous obtiendriez un débit d'environ cent mille lignes par seconde, mais pas des centaines de millions de lignes par seconde.
-
-Il est également intéressant de noter que ClickHouse est un système de gestion de base de données, pas une seule base de données. ClickHouse permet de créer des tables et des bases de données en cours d'exécution, de charger des données et d'exécuter des requêtes sans reconfigurer et redémarrer le serveur.
-
-## Compression De Données {#data-compression}
-
-Certains SGBD orientés colonne (InfiniDB CE et MonetDB) n'utilisent pas la compression de données. Cependant, la compression des données joue un rôle clé dans la réalisation d'excellentes performances.
-
-## Stockage de données sur disque {#disk-storage-of-data}
-
-Garder les données physiquement triées par clé primaire permet d'extraire des données pour ses valeurs spécifiques ou plages de valeurs avec une faible latence, moins de quelques dizaines de millisecondes. Certains SGBD orientés colonne (tels que SAP HANA et Google PowerDrill) ne peuvent fonctionner qu'en RAM. Cette approche encourage l'allocation d'un budget matériel plus important que ce qui est nécessaire pour l'analyse en temps réel. ClickHouse est conçu pour fonctionner sur des disques durs réguliers, ce qui signifie que le coût par Go de stockage de données est faible, mais SSD et RAM supplémentaire sont également entièrement utilisés si disponible.
-
-## Traitement parallèle sur plusieurs cœurs {#parallel-processing-on-multiple-cores}
-
-Les grandes requêtes sont parallélisées naturellement, en prenant toutes les ressources nécessaires disponibles sur le serveur actuel.
-
-## Traitement distribué sur plusieurs serveurs {#distributed-processing-on-multiple-servers}
-
-Presque aucun des SGBD en colonnes mentionnés ci-dessus ne prend en charge le traitement des requêtes distribuées.
-Dans ClickHouse, les données peuvent résider sur différents fragments. Chaque fragment peut être un groupe de répliques utilisées pour la tolérance aux pannes. Tous les fragments sont utilisés pour exécuter une requête en parallèle, de façon transparente pour l'utilisateur.
-
-## Prise en charge SQL {#sql-support}
-
-ClickHouse prend en charge un langage de requête déclarative basé sur SQL qui est identique à la norme SQL dans de nombreux cas.
-Les requêtes prises en charge incluent les clauses GROUP BY, ORDER BY, les sous-requêtes in FROM, IN et JOIN, ainsi que les sous-requêtes scalaires.
-Les sous-requêtes dépendantes et les fonctions de fenêtre ne sont pas prises en charge.
-
-## Moteur Vectoriel {#vector-engine}
-
-Les données ne sont pas seulement stockées par des colonnes, mais sont traitées par des vecteurs (parties de colonnes), ce qui permet d'atteindre une efficacité élevée du processeur.
-
-## Données en temps réel des Mises à jour {#real-time-data-updates}
-
-ClickHouse prend en charge les tables avec une clé primaire. Pour effectuer rapidement des requêtes sur la plage de la clé primaire, les données sont triées progressivement à l'aide de l'arborescence de fusion. Pour cette raison, les données peuvent être continuellement ajoutées à la table. Pas de verrouillage lorsque de nouvelles données sont ingérés.
-
-## Index {#index}
-
-Avoir une donnée physiquement triée par clé primaire permet d'extraire des données pour ses valeurs spécifiques ou plages de valeurs avec une faible latence, moins de quelques dizaines de millisecondes.
-
-## Convient pour les requêtes en ligne {#suitable-for-online-queries}
-
-Faible latence signifie que les requêtes peuvent être traitées sans délai et sans essayer de préparer une réponse à l'avance, au même moment pendant le chargement de la page de l'interface utilisateur. En d'autres termes, en ligne.
-
-## Prise en charge des calculs approximatifs {#support-for-approximated-calculations}
-
-ClickHouse offre différentes façons d'échanger la précision pour la performance:
-
-1.  Fonctions d'agrégation pour le calcul approximatif du nombre de valeurs distinctes, de médianes et de quantiles.
-2.  L'exécution d'une requête basée sur une partie (échantillon) de données et obtenir un pseudo résultat. Dans ce cas, proportionnellement, moins de données sont récupérées à partir du disque.
-3.  L'exécution d'une agrégation pour un nombre limité de clés aléatoires, au lieu de toutes les clés. Sous certaines conditions pour la distribution des clés dans les données, cela fournit un résultat raisonnablement précis tout en utilisant moins de ressources.
-
-## Prise en charge de la réplication et de l'intégrité des données {#data-replication-and-data-integrity-support}
-
-ClickHouse utilise la réplication multi-maître asynchrone. Après avoir été écrit dans n'importe quelle réplique disponible, toutes les répliques restantes récupèrent leur copie en arrière-plan. Le système conserve des données identiques sur différentes répliques. La récupération après la plupart des échecs est effectuée automatiquement ou semi-automatiquement dans les cas complexes.
-
-Pour plus d'informations, consultez la section [Réplication des données](../engines/table-engines/mergetree-family/replication.md).
-
-## Caractéristiques qui peuvent être considérées comme des inconvénients {#clickhouse-features-that-can-be-considered-disadvantages}
-
-1.  Pas de transactions à part entière.
-2.  Manque de capacité à modifier ou supprimer des données déjà insérées avec un taux élevé et une faible latence. Des suppressions et des mises à jour par lots sont disponibles pour nettoyer ou modifier les données, par exemple pour [GDPR](https://gdpr-info.eu).
-3.  L'index clairsemé rend ClickHouse pas si approprié pour les requêtes ponctuelles récupérant des lignes simples par leurs clés.
-
-[Article Original](https://clickhouse.tech/docs/en/introduction/distinctive_features/) <!--hide-->
diff --git a/docs/fr/introduction/history.md b/docs/fr/introduction/history.md
deleted file mode 100644
index 8bf1400d7f70..000000000000
--- a/docs/fr/introduction/history.md
+++ /dev/null
@@ -1,56 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 7
-toc_title: Histoire
----
-
-# Histoire De ClickHouse {#clickhouse-history}
-
-ClickHouse a été développé initialement au pouvoir [Yandex.Metrica](https://metrica.yandex.com/), [la deuxième plus grande plateforme d'analyse dans le monde](http://w3techs.com/technologies/overview/traffic_analysis/all) et continue à être le composant de base de ce système. Avec plus de 13 Billions d'enregistrements dans la base de données et plus de 20 milliards d'événements par jour, ClickHouse permet de générer des rapports personnalisés à la volée directement à partir de données non agrégées. Cet article couvre brièvement les objectifs de ClickHouse dans les premiers stades de son développement.
-
-Yandex.Metrica construit des rapports personnalisés à la volée en fonction des hits et des sessions, avec des segments arbitraires définis par l'utilisateur. Faisant souvent requiert la construction d'agrégats complexes, tels que le nombre d'utilisateurs uniques. De nouvelles données pour la création d'un rapport arrivent en temps réel.
-
-En avril 2014, Yandex.Metrica suivait environ 12 milliards d'événements (pages vues et clics) par jour. Tous ces événements doivent être stockés à créer des rapports personnalisés. Une seule requête peut exiger de la numérisation de millions de lignes en quelques centaines de millisecondes, ou des centaines de millions de lignes en quelques secondes.
-
-## Utilisation dans Yandex.Metrica et autres Services Yandex {#usage-in-yandex-metrica-and-other-yandex-services}
-
-ClickHouse sert à des fins multiples dans Yandex.Metrica.
-Sa tâche principale est de créer des rapports en mode en ligne en utilisant des données non agrégées. Il utilise un cluster de 374 serveurs qui stockent plus de 20,3 billions de lignes dans la base de données. Le volume de données compressées est d'environ 2 PB, sans tenir compte des doublons et des répliques. Le volume de données non compressées (au format TSV) serait d'environ 17 PB.
-
-ClickHouse joue également un rôle clé dans les processus suivants:
-
--   Stockage des données pour la relecture de Session de Yandex.Metrica.
--   Traitement des données intermédiaires.
--   Création de rapports globaux avec Analytics.
--   Exécution de requêtes pour le débogage du Yandex.Moteur Metrica.
--   Analyse des journaux de L'API et de l'interface utilisateur.
-
-De nos jours, il existe plusieurs dizaines d'installations ClickHouse dans D'autres services et départements Yandex: recherche verticale, E-commerce, Publicité, business analytics, développement mobile, Services personnels et autres.
-
-## Données agrégées et non agrégées {#aggregated-and-non-aggregated-data}
-
-Il y a une opinion répandue que pour calculer efficacement les statistiques, vous devez agréger les données car cela réduit le volume de données.
-
-Mais l'agrégation de données est livré avec beaucoup de limitations:
-
--   Vous devez disposer d'une liste prédéfinie des rapports requis.
--   L'utilisateur ne peut pas créer de rapports personnalisés.
--   Lors de l'agrégation sur un grand nombre de clés distinctes, le volume de données est à peine réduit, l'agrégation est donc inutile.
--   Pour un grand nombre de rapports, il y a trop de variations d'agrégation (explosion combinatoire).
--   Lors de l'agrégation de clés avec une cardinalité élevée (telles que les URL), le volume de données n'est pas réduit de beaucoup (moins de deux fois).
--   Pour cette raison, le volume de données avec l'agrégation peut augmenter au lieu de diminuer.
--   Les utilisateurs ne voient pas tous les rapports que nous générons pour eux. Une grande partie de ces calculs est inutile.
--   L'intégrité logique des données peut être violée pour diverses agrégations.
-
-Si nous n'agrégeons rien et travaillons avec des données non agrégées, cela pourrait réduire le volume des calculs.
-
-Cependant, avec l'agrégation, une partie importante du travail est déconnectée et achevée relativement calmement. En revanche, les calculs en ligne nécessitent un calcul aussi rapide que possible, car l'utilisateur attend le résultat.
-
-Yandex.Metrica dispose d'un système spécialisé d'agrégation des données appelé Metrage, qui a été utilisé pour la majorité des rapports.
-À partir de 2009, Yandex.Metrica a également utilisé une base de données OLAP spécialisée pour les données non agrégées appelée OLAPServer, qui était auparavant utilisée pour le générateur de rapports.
-OLAPServer a bien fonctionné pour les données non agrégées, mais il avait de nombreuses restrictions qui ne lui permettaient pas d'être utilisé pour tous les rapports comme souhaité. Ceux-ci comprenaient le manque de prise en charge des types de données (uniquement des nombres) et l'incapacité de mettre à jour progressivement les données en temps réel (cela ne pouvait être fait qu'en réécrivant les données quotidiennement). OLAPServer n'est pas un SGBD, mais une base de données spécialisée.
-
-L'objectif initial de ClickHouse était de supprimer les limites D'OLAPServer et de résoudre le problème du travail avec des données non agrégées pour tous les rapports, mais au fil des ans, il est devenu un système de gestion de base de données polyvalent adapté à un large éventail de tâches analytiques.
-
-[Article Original](https://clickhouse.tech/docs/en/introduction/history/) <!--hide-->
diff --git a/docs/fr/introduction/index.md b/docs/fr/introduction/index.md
deleted file mode 100644
index e0496b30f005..000000000000
--- a/docs/fr/introduction/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Introduction
-toc_priority: 1
----
-
-
diff --git a/docs/fr/introduction/performance.md b/docs/fr/introduction/performance.md
deleted file mode 100644
index 30d521e11cd5..000000000000
--- a/docs/fr/introduction/performance.md
+++ /dev/null
@@ -1,32 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 6
-toc_title: Performance
----
-
-# Performance {#performance}
-
-Selon les résultats des tests internes chez Yandex, ClickHouse affiche les meilleures performances (à la fois le débit le plus élevé pour les requêtes longues et la latence la plus faible pour les requêtes courtes) pour des scénarios d'exploitation comparables parmi les systèmes de sa classe disponibles pour les tests. Vous pouvez afficher les résultats du test sur un [page séparée](https://clickhouse.tech/benchmark/dbms/).
-
-De nombreux points de repère indépendants sont arrivés à des conclusions similaires. Ils ne sont pas difficiles à trouver en utilisant une recherche sur internet, ou vous pouvez voir [notre petite collection de liens](https://clickhouse.tech/#independent-benchmarks).
-
-## Débit pour une seule grande requête {#throughput-for-a-single-large-query}
-
-Le débit peut être mesuré en lignes par seconde ou en mégaoctets par seconde. Si les données sont placées dans le cache de page, une requête pas trop complexe est traitée sur du matériel moderne à une vitesse d'environ 2-10 GB / s de données non compressées sur un seul serveur (pour les cas les plus simples, la vitesse peut atteindre 30 GB/s). Si les données ne sont pas placées dans le cache de page, la vitesse dépend du sous-système de disque et du taux de compression des données. Par exemple, si le sous-système de disque permet de lire des données à 400 Mo/s et que le taux de compression des données est de 3, la vitesse devrait être d'environ 1,2 Go/s. Pour obtenir la vitesse en lignes par seconde, divisez la vitesse en octets par seconde par la taille totale des colonnes utilisées dans la requête. Par exemple, si 10 octets de colonnes sont extraites, la vitesse devrait être d'environ 100 à 200 millions de lignes par seconde.
-
-La vitesse de traitement augmente presque linéairement pour le traitement distribué, mais seulement si le nombre de lignes résultant de l'agrégation ou du tri n'est pas trop important.
-
-## Latence Lors Du Traitement Des Requêtes Courtes {#latency-when-processing-short-queries}
-
-Si une requête utilise une clé primaire et ne sélectionne pas trop de colonnes et de lignes à traiter (des centaines de milliers), Vous pouvez vous attendre à moins de 50 millisecondes de latence (un seul chiffre de millisecondes dans le meilleur des cas) si les données sont placées dans le cache de page. Sinon, la latence est principalement dominée par le nombre de recherches. Si vous utilisez des lecteurs de disque rotatifs, pour un système qui n'est pas surchargé, la latence peut être estimée avec cette formule: `seek time (10 ms) * count of columns queried * count of data parts`.
-
-## Débit lors du traitement d'une grande quantité de requêtes courtes {#throughput-when-processing-a-large-quantity-of-short-queries}
-
-Dans les mêmes conditions, ClickHouse peut traiter plusieurs centaines de requêtes par seconde sur un seul serveur (jusqu'à plusieurs milliers dans le meilleur des cas). Étant donné que ce scénario n'est pas typique pour les SGBD analytiques, nous vous recommandons d'attendre un maximum de 100 requêtes par seconde.
-
-## Performances Lors De L'Insertion De Données {#performance-when-inserting-data}
-
-Nous vous recommandons d'insérer des données dans des paquets d'au moins 1000 lignes, ou pas plus qu'une seule demande par seconde. Lors de l'insertion dans une table MergeTree à partir d'un dump séparé par des tabulations, la vitesse d'insertion peut être de 50 à 200 Mo/s. Si les lignes insérées ont une taille d'environ 1 KO, La vitesse sera de 50 000 à 200 000 lignes par seconde. Si les lignes sont petites, les performances peuvent être plus élevées en lignes par seconde (sur les données du système de bannière -`>` 500 000 lignes par seconde; sur les données de Graphite -`>` 1 000 000 lignes par seconde). Pour améliorer les performances, vous pouvez effectuer plusieurs requêtes D'insertion en parallèle, qui s'adaptent linéairement.
-
-[Article Original](https://clickhouse.tech/docs/en/introduction/performance/) <!--hide-->
diff --git a/docs/fr/operations/access-rights.md b/docs/fr/operations/access-rights.md
deleted file mode 100644
index c5bf9f2b6959..000000000000
--- a/docs/fr/operations/access-rights.md
+++ /dev/null
@@ -1,143 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 48
-toc_title: "Le Contr\xF4le d'acc\xE8s et de Gestion de Compte"
----
-
-# Le Contrôle d'accès et de Gestion de Compte {#access-control}
-
-Clickhouse prend en charge la gestion du contrôle d'accès basée sur [RBAC](https://en.wikipedia.org/wiki/Role-based_access_control) approche.
-
-Entités d'accès ClickHouse:
-- [Compte d'utilisateur](#user-account-management)
-- [Rôle](#role-management)
-- [La Ligne Politique](#row-policy-management)
-- [Les Paramètres De Profil](#settings-profiles-management)
-- [Quota](#quotas-management)
-
-Vous pouvez configurer des entités d'accès à l'aide de:
-
--   Flux de travail piloté par SQL.
-
-    Vous avez besoin de [permettre](#enabling-access-control) cette fonctionnalité.
-
--   Serveur [les fichiers de configuration](configuration-files.md) `users.xml` et `config.xml`.
-
-Nous vous recommandons D'utiliser un workflow piloté par SQL. Les deux méthodes de configuration fonctionnent simultanément, donc si vous utilisez les fichiers de configuration du serveur pour gérer les comptes et les droits d'accès, vous pouvez passer doucement au flux de travail piloté par SQL.
-
-!!! note "Avertissement"
-    Vous ne pouvez pas gérer la même entité d'accès par les deux méthodes de configuration simultanément.
-
-## Utilisation {#access-control-usage}
-
-Par défaut, le serveur ClickHouse fournit le compte utilisateur `default` ce qui n'est pas autorisé à utiliser le contrôle D'accès piloté par SQL et la gestion de compte, mais a tous les droits et autorisations. Le `default` compte d'utilisateur est utilisé dans tous les cas, lorsque l'utilisateur n'est pas défini, par exemple, lors de la connexion du client ou dans les requêtes distribuées. Dans le traitement des requêtes distribuées, un compte utilisateur par défaut est utilisé si la configuration du serveur ou du cluster ne spécifie pas [d'utilisateur et mot de passe](../engines/table-engines/special/distributed.md) propriété.
-
-Si vous commencez simplement à utiliser ClickHouse, vous pouvez utiliser le scénario suivant:
-
-1.  [Permettre](#enabling-access-control) Contrôle D'accès piloté par SQL et gestion de compte pour le `default` utilisateur.
-2.  Connexion en vertu de la `default` compte d'utilisateur et de créer tous les utilisateurs. N'oubliez pas de créer un compte d'administrateur (`GRANT ALL ON *.* WITH GRANT OPTION TO admin_user_account`).
-3.  [Restreindre les autorisations](settings/permissions-for-queries.md#permissions_for_queries) pour l' `default` utilisateur et désactiver le contrôle D'accès piloté par SQL et la gestion des comptes pour elle.
-
-### Propriétés de la Solution actuelle {#access-control-properties}
-
--   Vous pouvez accorder des autorisations pour les bases de données et les tables même si elles n'existent pas.
--   Si une table a été supprimée, tous les privilèges correspondant à cette table ne sont pas révoqués. Ainsi, si une nouvelle table est créée plus tard avec le même nom, tous les privilèges redeviennent réels. Pour révoquer les privilèges correspondant à la table supprimée, vous devez effectuer, par exemple, l' `REVOKE ALL PRIVILEGES ON db.table FROM ALL` requête.
--   Il n'y a pas de paramètres de durée de vie pour les privilèges.
-
-## Compte d'utilisateur {#user-account-management}
-
-Un compte d'utilisateur est une entité qui permet d'autoriser quelqu'un à ClickHouse. Un compte utilisateur contient:
-
--   Informations d'Identification.
--   [Privilège](../sql-reference/statements/grant.md#grant-privileges) qui définissent l'étendue des requêtes que l'utilisateur peut effectuer.
--   Hôtes à partir desquels la connexion au serveur ClickHouse est autorisée.
--   Rôles accordés et par défaut.
--   Paramètres avec leurs contraintes qui s'appliquent par défaut lors de la connexion de l'utilisateur.
--   Profils de paramètres assignés.
-
-Des privilèges à un compte d'utilisateur peuvent être accordés par [GRANT](../sql-reference/statements/grant.md) requête ou en attribuant [rôle](#role-management). Pour révoquer les privilèges d'un utilisateur, ClickHouse fournit [REVOKE](../sql-reference/statements/revoke.md) requête. Pour lister les privilèges d'un utilisateur, utilisez - [SHOW GRANTS](../sql-reference/statements/show.md#show-grants-statement) déclaration.
-
-Gestion des requêtes:
-
--   [CREATE USER](../sql-reference/statements/create.md#create-user-statement)
--   [ALTER USER](../sql-reference/statements/alter.md#alter-user-statement)
--   [DROP USER](../sql-reference/statements/misc.md#drop-user-statement)
--   [SHOW CREATE USER](../sql-reference/statements/show.md#show-create-user-statement)
-
-### Paramètres Application {#access-control-settings-applying}
-
-Les paramètres peuvent être définis de différentes manières: pour un compte utilisateur, dans ses profils de rôles et de paramètres accordés. Lors d'une connexion utilisateur, si un paramètre est défini dans différentes entités d'accès, la valeur et les contraintes de ce paramètre sont appliquées par les priorités suivantes (de plus haut à plus bas):
-
-1.  Paramètre de compte utilisateur.
-2.  Les paramètres de rôles par défaut du compte d'utilisateur. Si un paramètre est défini dans certains rôles, l'ordre de la mise en application n'est pas défini.
-3.  Les paramètres dans les profils de paramètres attribués à un utilisateur ou à ses rôles par défaut. Si un paramètre est défini dans certains profils, l'ordre d'application des paramètres n'est pas défini.
-4.  Paramètres appliqués à l'ensemble du serveur par défaut [profil par défaut](server-configuration-parameters/settings.md#default-profile).
-
-## Rôle {#role-management}
-
-Le rôle est un conteneur pour l'accès des entités qui peuvent être accordées à un compte d'utilisateur.
-
-Rôle contient:
-
--   [Privilège](../sql-reference/statements/grant.md#grant-privileges)
--   Paramètres et contraintes
--   Liste des rôles attribués
-
-Gestion des requêtes:
-
--   [CREATE ROLE](../sql-reference/statements/create.md#create-role-statement)
--   [ALTER ROLE](../sql-reference/statements/alter.md#alter-role-statement)
--   [DROP ROLE](../sql-reference/statements/misc.md#drop-role-statement)
--   [SET ROLE](../sql-reference/statements/misc.md#set-role-statement)
--   [SET DEFAULT ROLE](../sql-reference/statements/misc.md#set-default-role-statement)
--   [SHOW CREATE ROLE](../sql-reference/statements/show.md#show-create-role-statement)
-
-Les privilèges d'un rôle peuvent être accordés par [GRANT](../sql-reference/statements/grant.md) requête. Pour révoquer les privilèges D'un rôle ClickHouse fournit [REVOKE](../sql-reference/statements/revoke.md) requête.
-
-## La Ligne Politique {#row-policy-management}
-
-La stratégie de ligne est un filtre qui définit les lignes disponibles pour un utilisateur ou pour un rôle. La stratégie de ligne contient des filtres pour une table spécifique et une liste de rôles et / ou d'utilisateurs qui doivent utiliser cette stratégie de ligne.
-
-Gestion des requêtes:
-
--   [CREATE ROW POLICY](../sql-reference/statements/create.md#create-row-policy-statement)
--   [ALTER ROW POLICY](../sql-reference/statements/alter.md#alter-row-policy-statement)
--   [DROP ROW POLICY](../sql-reference/statements/misc.md#drop-row-policy-statement)
--   [SHOW CREATE ROW POLICY](../sql-reference/statements/show.md#show-create-row-policy-statement)
-
-## Les Paramètres De Profil {#settings-profiles-management}
-
-Paramètres profil est une collection de [paramètre](settings/index.md). Le profil paramètres contient les paramètres et les contraintes, ainsi que la liste des rôles et/ou des utilisateurs auxquels ce quota est appliqué.
-
-Gestion des requêtes:
-
--   [CREATE SETTINGS PROFILE](../sql-reference/statements/create.md#create-settings-profile-statement)
--   [ALTER SETTINGS PROFILE](../sql-reference/statements/alter.md#alter-settings-profile-statement)
--   [DROP SETTINGS PROFILE](../sql-reference/statements/misc.md#drop-settings-profile-statement)
--   [SHOW CREATE SETTINGS PROFILE](../sql-reference/statements/show.md#show-create-settings-profile-statement)
-
-## Quota {#quotas-management}
-
-Le Quota limite l'utilisation des ressources. Voir [Quota](quotas.md).
-
-Quota contient un ensemble de limites pour certaines durées, et la liste des rôles et/ou des utilisateurs qui devrait utiliser ce quota.
-
-Gestion des requêtes:
-
--   [CREATE QUOTA](../sql-reference/statements/create.md#create-quota-statement)
--   [ALTER QUOTA](../sql-reference/statements/alter.md#alter-quota-statement)
--   [DROP QUOTA](../sql-reference/statements/misc.md#drop-quota-statement)
--   [SHOW CREATE QUOTA](../sql-reference/statements/show.md#show-create-quota-statement)
-
-## Activation du contrôle D'accès piloté par SQL et de la gestion de Compte {#enabling-access-control}
-
--   Configurez un répertoire pour le stockage des configurations.
-
-    Clickhouse stocke les configurations d'entité d'accès dans le dossier défini dans [access_control_path](server-configuration-parameters/settings.md#access_control_path) paramètre de configuration du serveur.
-
--   Activez le contrôle D'accès piloté par SQL et la gestion de compte pour au moins un compte d'utilisateur.
-
-    Par défaut, le contrôle D'accès piloté par SQL et la gestion des comptes sont activés pour tous les utilisateurs. Vous devez configurer au moins un utilisateur dans le `users.xml` fichier de configuration et affecter 1 au [access_management](settings/settings-users.md#access_management-user-setting) paramètre.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/access_rights/) <!--hide-->
diff --git a/docs/fr/operations/backup.md b/docs/fr/operations/backup.md
deleted file mode 100644
index 953a96a04eb7..000000000000
--- a/docs/fr/operations/backup.md
+++ /dev/null
@@ -1,41 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 49
-toc_title: "La Sauvegarde Des Donn\xE9es"
----
-
-# La Sauvegarde Des Données {#data-backup}
-
-Alors [réplication](../engines/table-engines/mergetree-family/replication.md) provides protection from hardware failures, it does not protect against human errors: accidental deletion of data, deletion of the wrong table or a table on the wrong cluster, and software bugs that result in incorrect data processing or data corruption. In many cases mistakes like these will affect all replicas. ClickHouse has built-in safeguards to prevent some types of mistakes — for example, by default [vous ne pouvez pas simplement supprimer des tables avec un moteur de type MergeTree contenant plus de 50 Go de données](server-configuration-parameters/settings.md#max-table-size-to-drop). Toutefois, ces garanties ne couvrent pas tous les cas possibles et peuvent être contournés.
-
-Afin d'atténuer efficacement les erreurs humaines possibles, vous devez préparer soigneusement une stratégie de sauvegarde et de restauration de vos données **préalablement**.
-
-Chaque entreprise a différentes ressources disponibles et les exigences de l'entreprise, donc il n'y a pas de solution universelle pour les sauvegardes et restaurations ClickHouse qui s'adaptera à toutes les situations. Ce qui fonctionne pour un gigaoctet de données ne fonctionnera probablement pas pour des dizaines de pétaoctets. Il existe une variété d'approches possibles avec leurs propres avantages et inconvénients, qui sera discuté ci-dessous. C'est une bonne idée d'utiliser plusieurs approches au lieu d'un seul, afin de compenser leurs lacunes.
-
-!!! note "Note"
-    Gardez à l'esprit que si vous avez sauvegardé quelque chose et que vous n'avez jamais essayé de le restaurer, il est probable que la restauration ne fonctionnera pas correctement lorsque vous en avez réellement besoin (ou du moins cela prendra plus de temps que ce que les entreprises peuvent tolérer). Donc, quelle que soit l'approche de sauvegarde que vous choisissez, assurez-vous d'automatiser le processus de restauration et de le pratiquer sur un cluster clickhouse de rechange régulièrement.
-
-## Dupliquer Les Données Sources Ailleurs {#duplicating-source-data-somewhere-else}
-
-Souvent, les données qui sont ingérées dans ClickHouse sont livrées via une sorte de file d'attente persistante, telle que [Apache Kafka](https://kafka.apache.org). Dans ce cas, il est possible de configurer un ensemble supplémentaire d'abonnés qui liront le même flux de données pendant qu'il est écrit dans ClickHouse et le stockeront dans un stockage à froid quelque part. La plupart des entreprises ont déjà un stockage à froid recommandé par défaut, qui pourrait être un magasin d'objets ou un système de fichiers distribué comme [HDFS](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html).
-
-## Instantanés Du Système De Fichiers {#filesystem-snapshots}
-
-Certains systèmes de fichiers locaux fournissent des fonctionnalités d'instantané (par exemple, [ZFS](https://en.wikipedia.org/wiki/ZFS)), mais ils pourraient ne pas être le meilleur choix pour servir les requêtes actives. Une solution possible consiste à créer des répliques supplémentaires avec ce type de système de fichiers et à les exclure du [Distribué](../engines/table-engines/special/distributed.md) les tables qui sont utilisés pour `SELECT` requête. Les instantanés sur ces répliques seront hors de portée des requêtes qui modifient les données. En prime, ces répliques pourraient avoir des configurations matérielles spéciales avec plus de disques attachés par serveur, ce qui serait rentable.
-
-## clickhouse-copieur {#clickhouse-copier}
-
-[clickhouse-copieur](utilities/clickhouse-copier.md) est un outil polyvalent qui a été initialement créé pour re-shard petaoctet - sized tables. Il peut également être utilisé à des fins de sauvegarde et de restauration car il copie de manière fiable les données entre les tables ClickHouse et les clusters.
-
-Pour de plus petits volumes de données, un simple `INSERT INTO ... SELECT ...` les tables distantes peuvent également fonctionner.
-
-## Manipulations avec des pièces {#manipulations-with-parts}
-
-ClickHouse permet d'utiliser le `ALTER TABLE ... FREEZE PARTITION ...` requête pour créer une copie locale des partitions de table. Ceci est implémenté en utilisant des liens durs vers le `/var/lib/clickhouse/shadow/` dossier, donc il ne consomme généralement pas d'espace disque supplémentaire pour les anciennes données. Les copies créées des fichiers ne sont pas gérées par clickhouse server, vous pouvez donc les laisser là: vous aurez une sauvegarde simple qui ne nécessite aucun système externe supplémentaire, mais elle sera toujours sujette à des problèmes matériels. Pour cette raison, il est préférable de les copier à distance vers un autre emplacement, puis de supprimer les copies locales. Les systèmes de fichiers distribués et les magasins d'objets sont toujours une bonne option pour cela, mais les serveurs de fichiers attachés normaux avec une capacité suffisante peuvent également fonctionner (dans ce cas, le transfert se fera via le système de fichiers réseau ou peut-être [rsync](https://en.wikipedia.org/wiki/Rsync)).
-
-Pour plus d'informations sur les requêtes liées aux manipulations de [Modifier la documentation](../sql-reference/statements/alter.md#alter_manipulations-with-partitions).
-
-Un outil tiers est disponible pour automatiser cette approche: [clickhouse-sauvegarde](https://github.com/AlexAkulov/clickhouse-backup).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/backup/) <!--hide-->
diff --git a/docs/fr/operations/configuration-files.md b/docs/fr/operations/configuration-files.md
deleted file mode 100644
index 8ecb1ed0bb04..000000000000
--- a/docs/fr/operations/configuration-files.md
+++ /dev/null
@@ -1,57 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 50
-toc_title: Fichiers De Configuration
----
-
-# Fichiers De Configuration {#configuration_files}
-
-ClickHouse prend en charge la gestion de la configuration multi-fichiers. Le fichier de configuration du serveur principal est `/etc/clickhouse-server/config.xml`. Les autres fichiers doivent être dans le `/etc/clickhouse-server/config.d` répertoire.
-
-!!! note "Note"
-    Tous les fichiers de configuration doivent être au format XML. Aussi, ils doivent avoir le même élément racine, généralement `<yandex>`.
-
-Certains paramètres spécifiés dans le fichier de configuration principal peuvent être remplacés dans d'autres fichiers de configuration. Le `replace` ou `remove` les attributs peuvent être spécifiés pour les éléments de ces fichiers de configuration.
-
-Si ni l'un ni l'autre n'est spécifié, il combine le contenu des éléments de manière récursive, remplaçant les valeurs des enfants en double.
-
-Si `replace` est spécifié, il remplace l'élément entier par celui spécifié.
-
-Si `remove` est spécifié, il supprime l'élément.
-
-La configuration peut également définir “substitutions”. Si un élément a le `incl` attribut, la substitution correspondante du fichier sera utilisée comme valeur. Par défaut, le chemin d'accès au fichier avec des substitutions est `/etc/metrika.xml`. Ceci peut être changé dans le [include_from](server-configuration-parameters/settings.md#server_configuration_parameters-include_from) élément dans la configuration du serveur. Les valeurs de substitution sont spécifiées dans `/yandex/substitution_name` les éléments de ce fichier. Si une substitution spécifiée dans `incl` n'existe pas, il est enregistré dans le journal. Pour empêcher ClickHouse de consigner les substitutions manquantes, spécifiez `optional="true"` attribut (par exemple, les paramètres de [macro](server-configuration-parameters/settings.md)).
-
-Les Substitutions peuvent également être effectuées à partir de ZooKeeper. Pour ce faire, spécifiez l'attribut `from_zk = "/path/to/node"`. La valeur de l'élément est remplacé par le contenu du noeud au `/path/to/node` dans ZooKeeper. Vous pouvez également placer un sous-arbre XML entier sur le nœud ZooKeeper et il sera entièrement inséré dans l'élément source.
-
-Le `config.xml` le fichier peut spécifier une configuration distincte avec les paramètres utilisateur, les profils et les quotas. Le chemin relatif à cette configuration est défini dans `users_config` élément. Par défaut, il est `users.xml`. Si `users_config` est omis, les paramètres utilisateur, les profils et les quotas sont `config.xml`.
-
-La configuration des utilisateurs peut être divisée en fichiers séparés similaires à `config.xml` et `config.d/`.
-Nom du répertoire est défini comme `users_config` sans `.xml` postfix concaténé avec `.d`.
-Répertoire `users.d` est utilisé par défaut, comme `users_config` par défaut `users.xml`.
-Par exemple, vous pouvez avoir séparé fichier de configuration pour chaque utilisateur comme ceci:
-
-``` bash
-$ cat /etc/clickhouse-server/users.d/alice.xml
-```
-
-``` xml
-<yandex>
-    <users>
-      <alice>
-          <profile>analytics</profile>
-            <networks>
-                  <ip>::/0</ip>
-            </networks>
-          <password_sha256_hex>...</password_sha256_hex>
-          <quota>analytics</quota>
-      </alice>
-    </users>
-</yandex>
-```
-
-Pour chaque fichier de configuration, le serveur génère également `file-preprocessed.xml` les fichiers lors du démarrage. Ces fichiers contiennent toutes les remplacements et des remplacements, et ils sont destinés à l'usage informatif. Si des substitutions ZooKeeper ont été utilisées dans les fichiers de configuration mais que ZooKeeper n'est pas disponible au démarrage du serveur, le serveur charge la configuration à partir du fichier prétraité.
-
-Le serveur suit les changements dans les fichiers de configuration, ainsi que les fichiers et les nœuds ZooKeeper utilisés lors des substitutions et des remplacements, et recharge les paramètres pour les utilisateurs et les clusters à la volée. Cela signifie que vous pouvez modifier le cluster, les utilisateurs et leurs paramètres sans redémarrer le serveur.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/configuration_files/) <!--hide-->
diff --git a/docs/fr/operations/index.md b/docs/fr/operations/index.md
deleted file mode 100644
index 23f98bdaed22..000000000000
--- a/docs/fr/operations/index.md
+++ /dev/null
@@ -1,28 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Op\xE9rations"
-toc_priority: 41
-toc_title: Introduction
----
-
-# Opérations {#operations}
-
-Le manuel d'exploitation de ClickHouse comprend les principales sections suivantes:
-
--   [Exigence](requirements.md)
--   [Surveiller](monitoring.md)
--   [Dépannage](troubleshooting.md)
--   [Recommandations D'Utilisation](tips.md)
--   [Procédure De Mise À Jour](update.md)
--   [Les Droits D'Accès](access-rights.md)
--   [La Sauvegarde Des Données](backup.md)
--   [Fichiers De Configuration](configuration-files.md)
--   [Quota](quotas.md)
--   [Les Tables Système](system-tables.md)
--   [Paramètres De Configuration Du Serveur](server-configuration-parameters/index.md)
--   [Comment Tester Votre Matériel Avec ClickHouse](performance-test.md)
--   [Paramètre](settings/index.md)
--   [Utilitaire](utilities/index.md)
-
-{## [Article Original](https://clickhouse.tech/docs/en/operations/) ##}
diff --git a/docs/fr/operations/monitoring.md b/docs/fr/operations/monitoring.md
deleted file mode 100644
index 91aee4bc1f8a..000000000000
--- a/docs/fr/operations/monitoring.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: Surveiller
----
-
-# Surveiller {#monitoring}
-
-Vous pouvez surveiller:
-
--   L'utilisation des ressources matérielles.
--   Statistiques du serveur ClickHouse.
-
-## L'Utilisation Des Ressources {#resource-utilization}
-
-ClickHouse ne surveille pas l'état des ressources matérielles par lui-même.
-
-Il est fortement recommandé de configurer la surveillance de:
-
--   Charge et température sur les processeurs.
-
-    Vous pouvez utiliser [dmesg](https://en.wikipedia.org/wiki/Dmesg), [turbostat](https://www.linux.org/docs/man8/turbostat.html) ou d'autres instruments.
-
--   Utilisation du système de stockage, de la RAM et du réseau.
-
-## Métriques Du Serveur ClickHouse {#clickhouse-server-metrics}
-
-Clickhouse server a des instruments embarqués pour la surveillance de l'auto-état.
-
-Pour suivre les événements du serveur, utilisez les journaux du serveur. Voir la [enregistreur](server-configuration-parameters/settings.md#server_configuration_parameters-logger) section du fichier de configuration.
-
-Clickhouse recueille:
-
--   Différentes mesures de la façon dont le serveur utilise les ressources de calcul.
--   Statistiques communes sur le traitement des requêtes.
-
-Vous pouvez trouver des mesures dans le [système.métrique](../operations/system-tables.md#system_tables-metrics), [système.événement](../operations/system-tables.md#system_tables-events), et [système.asynchronous_metrics](../operations/system-tables.md#system_tables-asynchronous_metrics) table.
-
-Vous pouvez configurer ClickHouse pour exporter des métriques vers [Graphite](https://github.com/graphite-project). Voir la [Graphite section](server-configuration-parameters/settings.md#server_configuration_parameters-graphite) dans le fichier de configuration du serveur ClickHouse. Avant de configurer l'exportation des métriques, vous devez configurer Graphite en suivant leur [guide](https://graphite.readthedocs.io/en/latest/install.html).
-
-Vous pouvez configurer ClickHouse pour exporter des métriques vers [Prometheus](https://prometheus.io). Voir la [Prometheus section](server-configuration-parameters/settings.md#server_configuration_parameters-prometheus) dans le fichier de configuration du serveur ClickHouse. Avant de configurer l'exportation des métriques, vous devez configurer Prometheus en suivant leur [guide](https://prometheus.io/docs/prometheus/latest/installation/).
-
-De plus, vous pouvez surveiller la disponibilité du serveur via L'API HTTP. Envoyer la `HTTP GET` demande à `/ping`. Si le serveur est disponible, il répond avec `200 OK`.
-
-Pour surveiller les serveurs dans une configuration de cluster, vous devez [max_replica_delay_for_distributed_queries](settings/settings.md#settings-max_replica_delay_for_distributed_queries) paramètre et utiliser la ressource HTTP `/replicas_status`. Une demande de `/replicas_status` retourner `200 OK` si la réplique est disponible et n'est pas retardé derrière les autres réplicas. Si une réplique est retardée, elle revient `503 HTTP_SERVICE_UNAVAILABLE` avec des informations sur l'écart.
diff --git a/docs/fr/operations/optimizing-performance/index.md b/docs/fr/operations/optimizing-performance/index.md
deleted file mode 100644
index 3877d558eca9..000000000000
--- a/docs/fr/operations/optimizing-performance/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Optimisation Des Performances
-toc_priority: 52
----
-
-
diff --git a/docs/fr/operations/optimizing-performance/sampling-query-profiler.md b/docs/fr/operations/optimizing-performance/sampling-query-profiler.md
deleted file mode 100644
index eb2cdf7f12c0..000000000000
--- a/docs/fr/operations/optimizing-performance/sampling-query-profiler.md
+++ /dev/null
@@ -1,64 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 54
-toc_title: "Profilage De Requ\xEAte"
----
-
-# Échantillonnage Du Profileur De Requête {#sampling-query-profiler}
-
-ClickHouse exécute un profileur d'échantillonnage qui permet d'analyser l'exécution des requêtes. En utilisant profiler, vous pouvez trouver des routines de code source qui ont utilisé le plus fréquemment lors de l'exécution de la requête. Vous pouvez suivre le temps CPU et le temps d'horloge murale passé, y compris le temps d'inactivité.
-
-Utilisation du générateur de profils:
-
--   Installation de la [trace_log](../server-configuration-parameters/settings.md#server_configuration_parameters-trace_log) la section de la configuration du serveur.
-
-    Cette section configure le [trace_log](../../operations/system-tables.md#system_tables-trace_log) tableau système contenant les résultats du fonctionnement du profileur. Il est configuré par défaut. Rappelez-vous que les données de ce tableau est valable que pour un serveur en cours d'exécution. Après le redémarrage du serveur, ClickHouse ne nettoie pas la table et toute l'adresse de mémoire virtuelle stockée peut devenir invalide.
-
--   Installation de la [query_profiler_cpu_time_period_ns](../settings/settings.md#query_profiler_cpu_time_period_ns) ou [query_profiler_real_time_period_ns](../settings/settings.md#query_profiler_real_time_period_ns) paramètre. Les deux paramètres peuvent être utilisés simultanément.
-
-    Ces paramètres vous permettent de configurer les minuteries du profileur. Comme il s'agit des paramètres de session, vous pouvez obtenir une fréquence d'échantillonnage différente pour l'ensemble du serveur, les utilisateurs individuels ou les profils d'utilisateurs, pour votre session interactive et pour chaque requête individuelle.
-
-La fréquence d'échantillonnage par défaut est d'un échantillon par seconde et le processeur et les minuteries réelles sont activés. Cette fréquence permet de collecter suffisamment d'informations sur le cluster ClickHouse. En même temps, en travaillant avec cette fréquence, profiler n'affecte pas les performances du serveur ClickHouse. Si vous avez besoin de profiler chaque requête individuelle, essayez d'utiliser une fréquence d'échantillonnage plus élevée.
-
-Pour analyser les `trace_log` système de table:
-
--   Installer le `clickhouse-common-static-dbg` paquet. Voir [Installer à partir de paquets DEB](../../getting-started/install.md#install-from-deb-packages).
-
--   Autoriser les fonctions d'introspection par [allow_introspection_functions](../settings/settings.md#settings-allow_introspection_functions) paramètre.
-
-    Pour des raisons de sécurité, les fonctions d'introspection sont désactivées par défaut.
-
--   L'utilisation de la `addressToLine`, `addressToSymbol` et `demangle` [fonctions d'introspection](../../sql-reference/functions/introspection.md) pour obtenir les noms de fonctions et leurs positions dans le code ClickHouse. Pour obtenir un profil pour une requête, vous devez agréger les données du `trace_log` table. Vous pouvez agréger des données par des fonctions individuelles ou par l'ensemble des traces de la pile.
-
-Si vous avez besoin de visualiser `trace_log` info, essayez [flamegraph](../../interfaces/third-party/gui/#clickhouse-flamegraph) et [speedscope](https://github.com/laplab/clickhouse-speedscope).
-
-## Exemple {#example}
-
-Dans cet exemple, nous:
-
--   Filtrage `trace_log` données par un identifiant de requête et la date actuelle.
-
--   Agrégation par trace de pile.
-
--   En utilisant les fonctions d'introspection, nous obtiendrons un rapport de:
-
-    -   Noms des symboles et des fonctions de code source correspondantes.
-    -   Emplacements de code Source de ces fonctions.
-
-<!-- -->
-
-``` sql
-SELECT
-    count(),
-    arrayStringConcat(arrayMap(x -> concat(demangle(addressToSymbol(x)), '
    ', addressToLine(x)), trace), '
') AS sym
-FROM system.trace_log
-WHERE (query_id = 'ebca3574-ad0a-400a-9cbc-dca382f5998c') AND (event_date = today())
-GROUP BY trace
-ORDER BY count() DESC
-LIMIT 10
-```
-
-``` text
-{% include "examples/sampling_query_profiler_result.txt" %}
-```
diff --git a/docs/fr/operations/quotas.md b/docs/fr/operations/quotas.md
deleted file mode 100644
index ae15c774eafc..000000000000
--- a/docs/fr/operations/quotas.md
+++ /dev/null
@@ -1,112 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 51
-toc_title: Quota
----
-
-# Quota {#quotas}
-
-Les Quotas permettent de limiter l'utilisation des ressources au cours d'une période de temps ou de suivre l'utilisation des ressources.
-Les Quotas sont configurés dans la configuration utilisateur, qui est généralement ‘users.xml’.
-
-Le système dispose également d'une fonctionnalité pour limiter la complexité d'une seule requête. Voir la section “Restrictions on query complexity”).
-
-Contrairement aux restrictions de complexité des requêtes, les quotas:
-
--   Placez des restrictions sur un ensemble de requêtes qui peuvent être exécutées sur une période de temps, au lieu de limiter une seule requête.
--   Compte des ressources dépensées sur tous les serveurs distants pour le traitement des requêtes distribuées.
-
-Regardons la section de la ‘users.xml’ fichier qui définit les quotas.
-
-``` xml
-<!-- Quotas -->
-<quotas>
-    <!-- Quota name. -->
-    <default>
-        <!-- Restrictions for a time period. You can set many intervals with different restrictions. -->
-        <interval>
-            <!-- Length of the interval. -->
-            <duration>3600</duration>
-
-            <!-- Unlimited. Just collect data for the specified time interval. -->
-            <queries>0</queries>
-            <errors>0</errors>
-            <result_rows>0</result_rows>
-            <read_rows>0</read_rows>
-            <execution_time>0</execution_time>
-        </interval>
-    </default>
-```
-
-Par défaut, le quota suit la consommation de ressources pour chaque heure, sans limiter l'utilisation.
-La consommation de ressources calculé pour chaque intervalle est sortie dans le journal du serveur après chaque demande.
-
-``` xml
-<statbox>
-    <!-- Restrictions for a time period. You can set many intervals with different restrictions. -->
-    <interval>
-        <!-- Length of the interval. -->
-        <duration>3600</duration>
-
-        <queries>1000</queries>
-        <errors>100</errors>
-        <result_rows>1000000000</result_rows>
-        <read_rows>100000000000</read_rows>
-        <execution_time>900</execution_time>
-    </interval>
-
-    <interval>
-        <duration>86400</duration>
-
-        <queries>10000</queries>
-        <errors>1000</errors>
-        <result_rows>5000000000</result_rows>
-        <read_rows>500000000000</read_rows>
-        <execution_time>7200</execution_time>
-    </interval>
-</statbox>
-```
-
-Pour l' ‘statbox’ quota, restrictions sont fixées pour toutes les heures et pour toutes les 24 heures (86 400 secondes). L'intervalle de temps est compté, à partir d'un moment fixe défini par l'implémentation. En d'autres termes, l'intervalle de 24 heures ne commence pas nécessairement à minuit.
-
-Lorsque l'intervalle se termine, toutes les valeurs collectées sont effacées. Pour l'heure suivante, le calcul du quota recommence.
-
-Voici les montants qui peuvent être restreint:
-
-`queries` – The total number of requests.
-
-`errors` – The number of queries that threw an exception.
-
-`result_rows` – The total number of rows given as a result.
-
-`read_rows` – The total number of source rows read from tables for running the query on all remote servers.
-
-`execution_time` – The total query execution time, in seconds (wall time).
-
-Si la limite est dépassée pendant au moins un intervalle de temps, une exception est levée avec un texte indiquant quelle restriction a été dépassée, pour quel intervalle et quand le nouvel intervalle commence (lorsque les requêtes peuvent être envoyées à nouveau).
-
-Les Quotas peuvent utiliser le “quota key” fonctionnalité de rapport sur les ressources pour plusieurs clés indépendamment. Voici un exemple de ce:
-
-``` xml
-<!-- For the global reports designer. -->
-<web_global>
-    <!-- keyed – The quota_key "key" is passed in the query parameter,
-            and the quota is tracked separately for each key value.
-        For example, you can pass a Yandex.Metrica username as the key,
-            so the quota will be counted separately for each username.
-        Using keys makes sense only if quota_key is transmitted by the program, not by a user.
-
-        You can also write <keyed_by_ip />, so the IP address is used as the quota key.
-        (But keep in mind that users can change the IPv6 address fairly easily.)
-    -->
-    <keyed />
-```
-
-Le quota est attribué aux utilisateurs dans le ‘users’ section de la configuration. Voir la section “Access rights”.
-
-Pour le traitement des requêtes distribuées, les montants accumulés sont stockés sur le serveur demandeur. Donc, si l'utilisateur se rend sur un autre serveur, le quota y sera “start over”.
-
-Lorsque le serveur est redémarré, les quotas sont réinitialisés.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/quotas/) <!--hide-->
diff --git a/docs/fr/operations/requirements.md b/docs/fr/operations/requirements.md
deleted file mode 100644
index 1d79f3fcf82f..000000000000
--- a/docs/fr/operations/requirements.md
+++ /dev/null
@@ -1,61 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: Exigence
----
-
-# Exigence {#requirements}
-
-## CPU {#cpu}
-
-Pour l'installation à partir de paquets deb prédéfinis, utilisez un processeur avec l'architecture x86_64 et la prise en charge des instructions SSE 4.2. Pour exécuter ClickHouse avec des processeurs qui ne prennent pas en charge SSE 4.2 ou qui ont une architecture AArch64 ou PowerPC64LE, vous devez créer ClickHouse à partir de sources.
-
-ClickHouse implémente le traitement parallèle des données et utilise toutes les ressources matérielles disponibles. Lors du choix d'un processeur, tenez compte du fait que ClickHouse fonctionne plus efficacement dans les configurations avec un grand nombre de cœurs mais une fréquence d'horloge plus faible que dans les configurations avec moins de cœurs et une fréquence d'horloge plus élevée. Par exemple, 16 cœurs avec 2600 MHz est préférable à 8 cœurs avec 3600 MHz.
-
-Il est recommandé d'utiliser **Turbo Boost** et **la technologie hyper-threading** technologie. Il améliore considérablement les performances avec une charge de travail typique.
-
-## RAM {#ram}
-
-Nous vous recommandons d'utiliser un minimum de 4 Go de RAM pour effectuer des requêtes non triviales. Le serveur ClickHouse peut fonctionner avec une quantité beaucoup plus petite de RAM, mais il nécessite de la mémoire pour traiter les requêtes.
-
-Le volume de RAM requis dépend de:
-
--   La complexité des requêtes.
--   La quantité de données traitées dans les requêtes.
-
-Pour calculer le volume de RAM requis, vous devez estimer la taille des données temporaires pour [GROUP BY](../sql-reference/statements/select/group-by.md#select-group-by-clause), [DISTINCT](../sql-reference/statements/select/distinct.md#select-distinct), [JOIN](../sql-reference/statements/select/join.md#select-join) et d'autres opérations que vous utilisez.
-
-ClickHouse peut utiliser la mémoire externe pour les données temporaires. Voir [Groupe par dans la mémoire externe](../sql-reference/statements/select/group-by.md#select-group-by-in-external-memory) pour plus de détails.
-
-## Fichier D'Échange {#swap-file}
-
-Désactiver le fichier d'échange pour les environnements de production.
-
-## Sous-Système De Stockage {#storage-subsystem}
-
-Vous devez avoir 2 Go d'espace disque libre pour installer ClickHouse.
-
-Le volume de stockage requis pour vos données doit être calculé séparément. L'évaluation devrait inclure:
-
--   Estimation du volume de données.
-
-    Vous pouvez prendre un échantillon des données et obtenir la taille moyenne d'une ligne. Ensuite, multipliez la valeur par le nombre de lignes que vous souhaitez stocker.
-
--   Le coefficient de compression des données.
-
-    Pour estimer le coefficient de compression des données, chargez un échantillon de vos données dans ClickHouse et comparez la taille réelle des données avec la taille de la table stockée. Par exemple, les données de flux de clics sont généralement compressées de 6 à 10 fois.
-
-Pour calculer le volume final de données à stocker, appliquez le coefficient de compression au volume de données estimé. Si vous prévoyez de stocker des données dans plusieurs répliques, puis multipliez le volume estimé par le nombre de réplicas.
-
-## Réseau {#network}
-
-Si possible, utilisez des réseaux de classe 10G ou supérieure.
-
-La bande passante du réseau est essentielle pour traiter les requêtes distribuées avec une grande quantité de données intermédiaires. En outre, la vitesse du réseau affecte les processus de réplication.
-
-## Logiciel {#software}
-
-ClickHouse est développé principalement pour la famille de systèmes D'exploitation Linux. La distribution Linux recommandée est Ubuntu. Le `tzdata` paquet doit être installé dans le système.
-
-ClickHouse peut également fonctionner dans d'autres familles de systèmes d'exploitation. Voir les détails dans le [Prise en main](../getting-started/index.md) section de la documentation.
diff --git a/docs/fr/operations/server-configuration-parameters/index.md b/docs/fr/operations/server-configuration-parameters/index.md
deleted file mode 100644
index 0ecfb4a44cc9..000000000000
--- a/docs/fr/operations/server-configuration-parameters/index.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Param\xE8tres De Configuration Du Serveur"
-toc_priority: 54
-toc_title: Introduction
----
-
-# Paramètres De Configuration Du Serveur {#server-settings}
-
-Cette section contient des descriptions des paramètres du serveur qui ne peuvent pas être modifiés au niveau de la session ou de la requête.
-
-Ces paramètres sont stockés dans la `config.xml` fichier sur le serveur ClickHouse.
-
-D'autres paramètres sont décrits dans le “[Paramètre](../settings/index.md#session-settings-intro)” section.
-
-Avant d'étudier les paramètres, lire la [Fichiers de Configuration](../configuration-files.md#configuration_files) section et notez l'utilisation de substitutions (le `incl` et `optional` attribut).
-
-[Article Original](https://clickhouse.tech/docs/en/operations/server_configuration_parameters/) <!--hide-->
diff --git a/docs/fr/operations/server-configuration-parameters/settings.md b/docs/fr/operations/server-configuration-parameters/settings.md
deleted file mode 100644
index af83f3f5ed77..000000000000
--- a/docs/fr/operations/server-configuration-parameters/settings.md
+++ /dev/null
@@ -1,906 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 57
-toc_title: "Les Param\xE8tres Du Serveur"
----
-
-# Les Paramètres Du Serveur {#server-settings}
-
-## builtin_dictionaries_reload_interval {#builtin-dictionaries-reload-interval}
-
-L'intervalle en secondes avant de recharger les dictionnaires intégrés.
-
-Clickhouse recharge les dictionnaires intégrés toutes les X secondes. Cela permet d'éditer des dictionnaires “on the fly” sans redémarrer le serveur.
-
-Valeur par défaut: 3600.
-
-**Exemple**
-
-``` xml
-<builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
-```
-
-## compression {#server-settings-compression}
-
-Paramètres de compression de données pour [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)-tables de moteur.
-
-!!! warning "Avertissement"
-    Ne l'utilisez pas si vous venez de commencer à utiliser ClickHouse.
-
-Modèle de Configuration:
-
-``` xml
-<compression>
-    <case>
-      <min_part_size>...</min_part_size>
-      <min_part_size_ratio>...</min_part_size_ratio>
-      <method>...</method>
-    </case>
-    ...
-</compression>
-```
-
-`<case>` Fields:
-
--   `min_part_size` – The minimum size of a data part.
--   `min_part_size_ratio` – The ratio of the data part size to the table size.
--   `method` – Compression method. Acceptable values: `lz4` ou `zstd`.
-
-Vous pouvez configurer plusieurs `<case>` section.
-
-Actions lorsque les conditions sont remplies:
-
--   Si une partie de données correspond à un ensemble de conditions, ClickHouse utilise la méthode de compression spécifiée.
--   Si une partie de données correspond à plusieurs ensembles de conditions, ClickHouse utilise le premier ensemble de conditions correspondant.
-
-Si aucune condition n'est remplie pour une partie de données, ClickHouse utilise `lz4` compression.
-
-**Exemple**
-
-``` xml
-<compression incl="clickhouse_compression">
-    <case>
-        <min_part_size>10000000000</min_part_size>
-        <min_part_size_ratio>0.01</min_part_size_ratio>
-        <method>zstd</method>
-    </case>
-</compression>
-```
-
-## default_database {#default-database}
-
-La base de données par défaut.
-
-Pour obtenir une liste de bases de données, utilisez la [SHOW DATABASES](../../sql-reference/statements/show.md#show-databases) requête.
-
-**Exemple**
-
-``` xml
-<default_database>default</default_database>
-```
-
-## default_profile {#default-profile}
-
-Profil des paramètres par défaut.
-
-Les paramètres des profils sont situés dans le fichier spécifié dans le paramètre `user_config`.
-
-**Exemple**
-
-``` xml
-<default_profile>default</default_profile>
-```
-
-## dictionaries_config {#server_configuration_parameters-dictionaries_config}
-
-Chemin d'accès au fichier de configuration des dictionnaires externes.
-
-Chemin:
-
--   Spécifiez le chemin absolu ou le chemin relatif au fichier de configuration du serveur.
--   Le chemin peut contenir des caractères génériques \* et ?.
-
-Voir aussi “[Dictionnaires externes](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md)”.
-
-**Exemple**
-
-``` xml
-<dictionaries_config>*_dictionary.xml</dictionaries_config>
-```
-
-## dictionaries_lazy_load {#server_configuration_parameters-dictionaries_lazy_load}
-
-Chargement paresseux des dictionnaires.
-
-Si `true` chaque dictionnaire est créé lors de la première utilisation. Si la création du dictionnaire a échoué, la fonction qui utilisait le dictionnaire lève une exception.
-
-Si `false`, tous les dictionnaires sont créés lorsque le serveur démarre, et si il y a une erreur, le serveur s'arrête.
-
-La valeur par défaut est `true`.
-
-**Exemple**
-
-``` xml
-<dictionaries_lazy_load>true</dictionaries_lazy_load>
-```
-
-## format_schema_path {#server_configuration_parameters-format_schema_path}
-
-Le chemin d'accès au répertoire avec des régimes pour l'entrée de données, tels que les schémas pour l' [CapnProto](../../interfaces/formats.md#capnproto) format.
-
-**Exemple**
-
-``` xml
-  <!-- Directory containing schema files for various input formats. -->
-  <format_schema_path>format_schemas/</format_schema_path>
-```
-
-## graphite {#server_configuration_parameters-graphite}
-
-Envoi de données à [Graphite](https://github.com/graphite-project).
-
-Paramètre:
-
--   host – The Graphite server.
--   port – The port on the Graphite server.
--   interval – The interval for sending, in seconds.
--   timeout – The timeout for sending data, in seconds.
--   root_path – Prefix for keys.
--   metrics – Sending data from the [système.métrique](../../operations/system-tables.md#system_tables-metrics) table.
--   events – Sending deltas data accumulated for the time period from the [système.événement](../../operations/system-tables.md#system_tables-events) table.
--   events_cumulative – Sending cumulative data from the [système.événement](../../operations/system-tables.md#system_tables-events) table.
--   asynchronous_metrics – Sending data from the [système.asynchronous_metrics](../../operations/system-tables.md#system_tables-asynchronous_metrics) table.
-
-Vous pouvez configurer plusieurs `<graphite>` clause. Par exemple, vous pouvez l'utiliser pour envoyer des données différentes à différents intervalles.
-
-**Exemple**
-
-``` xml
-<graphite>
-    <host>localhost</host>
-    <port>42000</port>
-    <timeout>0.1</timeout>
-    <interval>60</interval>
-    <root_path>one_min</root_path>
-    <metrics>true</metrics>
-    <events>true</events>
-    <events_cumulative>false</events_cumulative>
-    <asynchronous_metrics>true</asynchronous_metrics>
-</graphite>
-```
-
-## graphite_rollup {#server_configuration_parameters-graphite-rollup}
-
-Paramètres pour l'amincissement des données pour le Graphite.
-
-Pour plus de détails, voir [GraphiteMergeTree](../../engines/table-engines/mergetree-family/graphitemergetree.md).
-
-**Exemple**
-
-``` xml
-<graphite_rollup_example>
-    <default>
-        <function>max</function>
-        <retention>
-            <age>0</age>
-            <precision>60</precision>
-        </retention>
-        <retention>
-            <age>3600</age>
-            <precision>300</precision>
-        </retention>
-        <retention>
-            <age>86400</age>
-            <precision>3600</precision>
-        </retention>
-    </default>
-</graphite_rollup_example>
-```
-
-## http_port/https_port {#http-porthttps-port}
-
-Port de connexion au serveur via HTTP(S).
-
-Si `https_port` est spécifié, [openSSL](#server_configuration_parameters-openssl) doit être configuré.
-
-Si `http_port` est spécifié, la configuration OpenSSL est ignorée même si elle est définie.
-
-**Exemple**
-
-``` xml
-<https_port>9999</https_port>
-```
-
-## http_server_default_response {#server_configuration_parameters-http_server_default_response}
-
-Page affichée par défaut lorsque vous accédez au serveur HTTP(S) ClickHouse.
-La valeur par défaut est “Ok.” (avec un saut de ligne à la fin)
-
-**Exemple**
-
-Ouvrir `https://tabix.io/` lors de l'accès à `http://localhost: http_port`.
-
-``` xml
-<http_server_default_response>
-  <![CDATA[<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>]]>
-</http_server_default_response>
-```
-
-## include_from {#server_configuration_parameters-include_from}
-
-Le chemin d'accès au fichier avec des substitutions.
-
-Pour plus d'informations, consultez la section “[Fichiers de Configuration](../configuration-files.md#configuration_files)”.
-
-**Exemple**
-
-``` xml
-<include_from>/etc/metrica.xml</include_from>
-```
-
-## interserver_http_port {#interserver-http-port}
-
-Port pour l'échange de données entre les serveurs ClickHouse.
-
-**Exemple**
-
-``` xml
-<interserver_http_port>9009</interserver_http_port>
-```
-
-## interserver_http_host {#interserver-http-host}
-
-Le nom d'hôte qui peut être utilisé par d'autres serveurs pour accéder à ce serveur.
-
-Si elle est omise, elle est définie de la même manière que `hostname-f` commande.
-
-Utile pour rompre avec une interface réseau spécifique.
-
-**Exemple**
-
-``` xml
-<interserver_http_host>example.yandex.ru</interserver_http_host>
-```
-
-## interserver_http_credentials {#server-settings-interserver-http-credentials}
-
-Le nom d'utilisateur et le mot de passe utilisés pour [réplication](../../engines/table-engines/mergetree-family/replication.md) avec les moteurs \* répliqués. Ces informations d'identification sont utilisées uniquement pour la communication entre les répliques et ne sont pas liées aux informations d'identification des clients ClickHouse. Le serveur vérifie ces informations d'identification pour la connexion de répliques et utilise les mêmes informations d'identification lors de la connexion à d'autres répliques. Donc, ces informations d'identification doivent être identiques pour tous les réplicas dans un cluster.
-Par défaut, l'authentification n'est pas utilisé.
-
-Cette section contient les paramètres suivants:
-
--   `user` — username.
--   `password` — password.
-
-**Exemple**
-
-``` xml
-<interserver_http_credentials>
-    <user>admin</user>
-    <password>222</password>
-</interserver_http_credentials>
-```
-
-## keep_alive_timeout {#keep-alive-timeout}
-
-Le nombre de secondes que ClickHouse attend pour les demandes entrantes avant de fermer la connexion. Par défaut est de 3 secondes.
-
-**Exemple**
-
-``` xml
-<keep_alive_timeout>3</keep_alive_timeout>
-```
-
-## listen_host {#server_configuration_parameters-listen_host}
-
-Restriction sur les hôtes dont les demandes peuvent provenir. Si vous voulez que le serveur réponde à tous, spécifiez `::`.
-
-Exemple:
-
-``` xml
-<listen_host>::1</listen_host>
-<listen_host>127.0.0.1</listen_host>
-```
-
-## enregistreur {#server_configuration_parameters-logger}
-
-Paramètres de journalisation.
-
-Touches:
-
--   level – Logging level. Acceptable values: `trace`, `debug`, `information`, `warning`, `error`.
--   log – The log file. Contains all the entries according to `level`.
--   errorlog – Error log file.
--   size – Size of the file. Applies to `log`et`errorlog`. Une fois que le fichier atteint `size`, Archives ClickHouse et le renomme, et crée un nouveau fichier journal à sa place.
--   count – The number of archived log files that ClickHouse stores.
-
-**Exemple**
-
-``` xml
-<logger>
-    <level>trace</level>
-    <log>/var/log/clickhouse-server/clickhouse-server.log</log>
-    <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
-    <size>1000M</size>
-    <count>10</count>
-</logger>
-```
-
-L'écriture dans le syslog est également prise en charge. Exemple de Config:
-
-``` xml
-<logger>
-    <use_syslog>1</use_syslog>
-    <syslog>
-        <address>syslog.remote:10514</address>
-        <hostname>myhost.local</hostname>
-        <facility>LOG_LOCAL6</facility>
-        <format>syslog</format>
-    </syslog>
-</logger>
-```
-
-Touches:
-
--   use_syslog — Required setting if you want to write to the syslog.
--   address — The host\[:port\] of syslogd. If omitted, the local daemon is used.
--   hostname — Optional. The name of the host that logs are sent from.
--   facility — [Le mot clé syslog facility](https://en.wikipedia.org/wiki/Syslog#Facility) en majuscules avec la “LOG_” préfixe: (`LOG_USER`, `LOG_DAEMON`, `LOG_LOCAL3` et ainsi de suite).
-    Valeur par défaut: `LOG_USER` si `address` est spécifié, `LOG_DAEMON otherwise.`
--   format – Message format. Possible values: `bsd` et `syslog.`
-
-## macro {#macros}
-
-Substitutions de paramètres pour les tables répliquées.
-
-Peut être omis si les tables répliquées ne sont pas utilisées.
-
-Pour plus d'informations, consultez la section “[Création de tables répliquées](../../engines/table-engines/mergetree-family/replication.md)”.
-
-**Exemple**
-
-``` xml
-<macros incl="macros" optional="true" />
-```
-
-## mark_cache_size {#server-mark-cache-size}
-
-Taille approximative (en octets) du cache des marques utilisées par les [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) famille.
-
-Le cache est partagé pour le serveur et la mémoire est allouée au besoin. La taille du cache doit être d'au moins 5368709120.
-
-**Exemple**
-
-``` xml
-<mark_cache_size>5368709120</mark_cache_size>
-```
-
-## max_concurrent_queries {#max-concurrent-queries}
-
-Nombre maximal de demandes traitées simultanément.
-
-**Exemple**
-
-``` xml
-<max_concurrent_queries>100</max_concurrent_queries>
-```
-
-## max_connections {#max-connections}
-
-Le nombre maximal de connexions entrantes.
-
-**Exemple**
-
-``` xml
-<max_connections>4096</max_connections>
-```
-
-## max_open_files {#max-open-files}
-
-Le nombre maximal de fichiers ouverts.
-
-Par défaut: `maximum`.
-
-Nous vous recommandons d'utiliser cette option sous Mac OS X depuis le `getrlimit()` la fonction renvoie une valeur incorrecte.
-
-**Exemple**
-
-``` xml
-<max_open_files>262144</max_open_files>
-```
-
-## max_table_size_to_drop {#max-table-size-to-drop}
-
-Restriction sur la suppression de tables.
-
-Si la taille d'un [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) table dépasse `max_table_size_to_drop` (en octets), vous ne pouvez pas le supprimer à l'aide d'une requête DROP.
-
-Si vous devez toujours supprimer la table sans redémarrer le serveur ClickHouse, créez le `<clickhouse-path>/flags/force_drop_table` fichier et exécutez la requête DROP.
-
-Valeur par défaut: 50 Go.
-
-La valeur 0 signifie que vous pouvez supprimer toutes les tables sans aucune restriction.
-
-**Exemple**
-
-``` xml
-<max_table_size_to_drop>0</max_table_size_to_drop>
-```
-
-## merge_tree {#server_configuration_parameters-merge_tree}
-
-Réglage fin des tables dans le [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md).
-
-Pour plus d'informations, consultez MergeTreeSettings.h fichier d'en-tête.
-
-**Exemple**
-
-``` xml
-<merge_tree>
-    <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
-</merge_tree>
-```
-
-## openSSL {#server_configuration_parameters-openssl}
-
-Configuration client/serveur SSL.
-
-Le Support pour SSL est fourni par le `libpoco` bibliothèque. L'interface est décrite dans le fichier [SSLManager.h](https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h)
-
-Clés pour les paramètres Serveur/client:
-
--   privateKeyFile – The path to the file with the secret key of the PEM certificate. The file may contain a key and certificate at the same time.
--   certificateFile – The path to the client/server certificate file in PEM format. You can omit it if `privateKeyFile` contient le certificat.
--   caConfig – The path to the file or directory that contains trusted root certificates.
--   verificationMode – The method for checking the node's certificates. Details are in the description of the [Cadre](https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/Context.h) classe. Valeurs possibles: `none`, `relaxed`, `strict`, `once`.
--   verificationDepth – The maximum length of the verification chain. Verification will fail if the certificate chain length exceeds the set value.
--   loadDefaultCAFile – Indicates that built-in CA certificates for OpenSSL will be used. Acceptable values: `true`, `false`. \|
--   cipherList – Supported OpenSSL encryptions. For example: `ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH`.
--   cacheSessions – Enables or disables caching sessions. Must be used in combination with `sessionIdContext`. Les valeurs acceptables: `true`, `false`.
--   sessionIdContext – A unique set of random characters that the server appends to each generated identifier. The length of the string must not exceed `SSL_MAX_SSL_SESSION_ID_LENGTH`. Ce paramètre est toujours recommandé car il permet d'éviter les problèmes à la fois si le serveur met en cache la session et si le client demande la mise en cache. Valeur par défaut: `${application.name}`.
--   sessionCacheSize – The maximum number of sessions that the server caches. Default value: 1024\*20. 0 – Unlimited sessions.
--   sessionTimeout – Time for caching the session on the server.
--   extendedVerification – Automatically extended verification of certificates after the session ends. Acceptable values: `true`, `false`.
--   requireTLSv1 – Require a TLSv1 connection. Acceptable values: `true`, `false`.
--   requireTLSv1_1 – Require a TLSv1.1 connection. Acceptable values: `true`, `false`.
--   requireTLSv1 – Require a TLSv1.2 connection. Acceptable values: `true`, `false`.
--   fips – Activates OpenSSL FIPS mode. Supported if the library's OpenSSL version supports FIPS.
--   privateKeyPassphraseHandler – Class (PrivateKeyPassphraseHandler subclass) that requests the passphrase for accessing the private key. For example: `<privateKeyPassphraseHandler>`, `<name>KeyFileHandler</name>`, `<options><password>test</password></options>`, `</privateKeyPassphraseHandler>`.
--   invalidCertificateHandler – Class (a subclass of CertificateHandler) for verifying invalid certificates. For example: `<invalidCertificateHandler> <name>ConsoleCertificateHandler</name> </invalidCertificateHandler>` .
--   disableProtocols – Protocols that are not allowed to use.
--   preferServerCiphers – Preferred server ciphers on the client.
-
-**Exemple de paramètres:**
-
-``` xml
-<openSSL>
-    <server>
-        <!-- openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->
-        <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>
-        <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>
-        <!-- openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096 -->
-        <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>
-        <verificationMode>none</verificationMode>
-        <loadDefaultCAFile>true</loadDefaultCAFile>
-        <cacheSessions>true</cacheSessions>
-        <disableProtocols>sslv2,sslv3</disableProtocols>
-        <preferServerCiphers>true</preferServerCiphers>
-    </server>
-    <client>
-        <loadDefaultCAFile>true</loadDefaultCAFile>
-        <cacheSessions>true</cacheSessions>
-        <disableProtocols>sslv2,sslv3</disableProtocols>
-        <preferServerCiphers>true</preferServerCiphers>
-        <!-- Use for self-signed: <verificationMode>none</verificationMode> -->
-        <invalidCertificateHandler>
-            <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->
-            <name>RejectCertificateHandler</name>
-        </invalidCertificateHandler>
-    </client>
-</openSSL>
-```
-
-## part_log {#server_configuration_parameters-part-log}
-
-Journalisation des événements associés à [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md). Par exemple, ajouter ou fusionner des données. Vous pouvez utiliser le journal pour simuler des algorithmes de fusion et comparer leurs caractéristiques. Vous pouvez visualiser le processus de fusion.
-
-Les requêtes sont enregistrées dans le [système.part_log](../../operations/system-tables.md#system_tables-part-log) table, pas dans un fichier séparé. Vous pouvez configurer le nom de cette table dans le `table` paramètre (voir ci-dessous).
-
-Utilisez les paramètres suivants pour configurer la journalisation:
-
--   `database` – Name of the database.
--   `table` – Name of the system table.
--   `partition_by` – Sets a [partitionnement personnalisé clé](../../engines/table-engines/mergetree-family/custom-partitioning-key.md).
--   `flush_interval_milliseconds` – Interval for flushing data from the buffer in memory to the table.
-
-**Exemple**
-
-``` xml
-<part_log>
-    <database>system</database>
-    <table>part_log</table>
-    <partition_by>toMonday(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</part_log>
-```
-
-## chemin {#server_configuration_parameters-path}
-
-Chemin d'accès au répertoire contenant des données.
-
-!!! note "Note"
-    La barre oblique de fin est obligatoire.
-
-**Exemple**
-
-``` xml
-<path>/var/lib/clickhouse/</path>
-```
-
-## prometheus {#server_configuration_parameters-prometheus}
-
-Exposer les données de métriques pour le raclage à partir [Prometheus](https://prometheus.io).
-
-Paramètre:
-
--   `endpoint` – HTTP endpoint for scraping metrics by prometheus server. Start from ‘/’.
--   `port` – Port for `endpoint`.
--   `metrics` – Flag that sets to expose metrics from the [système.métrique](../system-tables.md#system_tables-metrics) table.
--   `events` – Flag that sets to expose metrics from the [système.événement](../system-tables.md#system_tables-events) table.
--   `asynchronous_metrics` – Flag that sets to expose current metrics values from the [système.asynchronous_metrics](../system-tables.md#system_tables-asynchronous_metrics) table.
-
-**Exemple**
-
-``` xml
- <prometheus>
-        <endpoint>/metrics</endpoint>
-        <port>8001</port>
-        <metrics>true</metrics>
-        <events>true</events>
-        <asynchronous_metrics>true</asynchronous_metrics>
-    </prometheus>
-```
-
-## query_log {#server_configuration_parameters-query-log}
-
-Réglage de la journalisation des requêtes reçues avec [log_queries=1](../settings/settings.md) paramètre.
-
-Les requêtes sont enregistrées dans le [système.query_log](../../operations/system-tables.md#system_tables-query_log) table, pas dans un fichier séparé. Vous pouvez modifier le nom de la table dans le `table` paramètre (voir ci-dessous).
-
-Utilisez les paramètres suivants pour configurer la journalisation:
-
--   `database` – Name of the database.
--   `table` – Name of the system table the queries will be logged in.
--   `partition_by` – Sets a [partitionnement personnalisé clé](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) pour une table.
--   `flush_interval_milliseconds` – Interval for flushing data from the buffer in memory to the table.
-
-Si la table n'existe pas, ClickHouse la créera. Si la structure du journal des requêtes a été modifiée lors de la mise à jour du serveur ClickHouse, la table avec l'ancienne structure est renommée et une nouvelle table est créée automatiquement.
-
-**Exemple**
-
-``` xml
-<query_log>
-    <database>system</database>
-    <table>query_log</table>
-    <partition_by>toMonday(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</query_log>
-```
-
-## query_thread_log {#server_configuration_parameters-query-thread-log}
-
-Réglage de la journalisation des threads de requêtes reçues avec [log_query_threads=1](../settings/settings.md#settings-log-query-threads) paramètre.
-
-Les requêtes sont enregistrées dans le [système.query_thread_log](../../operations/system-tables.md#system_tables-query-thread-log) table, pas dans un fichier séparé. Vous pouvez modifier le nom de la table dans le `table` paramètre (voir ci-dessous).
-
-Utilisez les paramètres suivants pour configurer la journalisation:
-
--   `database` – Name of the database.
--   `table` – Name of the system table the queries will be logged in.
--   `partition_by` – Sets a [partitionnement personnalisé clé](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) pour un système de tableau.
--   `flush_interval_milliseconds` – Interval for flushing data from the buffer in memory to the table.
-
-Si la table n'existe pas, ClickHouse la créera. Si la structure du journal des threads de requête a été modifiée lors de la mise à jour du serveur ClickHouse, la table avec l'ancienne structure est renommée et une nouvelle table est créée automatiquement.
-
-**Exemple**
-
-``` xml
-<query_thread_log>
-    <database>system</database>
-    <table>query_thread_log</table>
-    <partition_by>toMonday(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</query_thread_log>
-```
-
-## trace_log {#server_configuration_parameters-trace_log}
-
-Paramètres pour le [trace_log](../../operations/system-tables.md#system_tables-trace_log) opération de table de système.
-
-Paramètre:
-
--   `database` — Database for storing a table.
--   `table` — Table name.
--   `partition_by` — [Partitionnement personnalisé clé](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) pour un système de tableau.
--   `flush_interval_milliseconds` — Interval for flushing data from the buffer in memory to the table.
-
-Le fichier de configuration du serveur par défaut `config.xml` contient la section Paramètres suivante:
-
-``` xml
-<trace_log>
-    <database>system</database>
-    <table>trace_log</table>
-    <partition_by>toYYYYMM(event_date)</partition_by>
-    <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-</trace_log>
-```
-
-## query_masking_rules {#query-masking-rules}
-
-Règles basées sur Regexp, qui seront appliquées aux requêtes ainsi qu'à tous les messages de journal avant de les stocker dans les journaux du serveur,
-`system.query_log`, `system.text_log`, `system.processes` table, et dans les journaux envoyés au client. Qui permet à la prévention de
-fuite de données sensibles à partir de requêtes SQL (comme les noms, e-mails,
-identificateurs ou numéros de carte de crédit) aux journaux.
-
-**Exemple**
-
-``` xml
-<query_masking_rules>
-    <rule>
-        <name>hide SSN</name>
-        <regexp>(^|\D)\d{3}-\d{2}-\d{4}($|\D)</regexp>
-        <replace>000-00-0000</replace>
-    </rule>
-</query_masking_rules>
-```
-
-Config champs:
-- `name` - nom de la règle (facultatif)
-- `regexp` - Expression régulière compatible RE2 (obligatoire)
-- `replace` - chaîne de substitution pour les données sensibles (facultatif, par défaut - six astérisques)
-
-Les règles de masquage sont appliquées à l'ensemble de la requête (pour éviter les fuites de données sensibles provenant de requêtes malformées / Non analysables).
-
-`system.events` table ont compteur `QueryMaskingRulesMatch` qui ont un nombre global de requête de masquage des règles de correspondances.
-
-Pour les requêtes distribuées chaque serveur doivent être configurés séparément, sinon, les sous-requêtes transmises à d'autres
-les nœuds seront stockés sans masquage.
-
-## remote_servers {#server-settings-remote-servers}
-
-Configuration des clusters utilisés par le [Distribué](../../engines/table-engines/special/distributed.md) moteur de table et par le `cluster` table de fonction.
-
-**Exemple**
-
-``` xml
-<remote_servers incl="clickhouse_remote_servers" />
-```
-
-Pour la valeur de l' `incl` attribut, voir la section “[Fichiers de Configuration](../configuration-files.md#configuration_files)”.
-
-**Voir Aussi**
-
--   [skip_unavailable_shards](../settings/settings.md#settings-skip_unavailable_shards)
-
-## fuseau {#server_configuration_parameters-timezone}
-
-Le fuseau horaire du serveur.
-
-Spécifié comme identifiant IANA pour le fuseau horaire UTC ou l'emplacement géographique (par exemple, Afrique / Abidjan).
-
-Le fuseau horaire est nécessaire pour les conversions entre les formats String et DateTime lorsque les champs DateTime sont sortis au format texte (imprimés à l'écran ou dans un fichier) et lors de L'obtention de DateTime à partir d'une chaîne. En outre, le fuseau horaire est utilisé dans les fonctions qui fonctionnent avec l'heure et la date si elles ne reçoivent pas le fuseau horaire dans les paramètres d'entrée.
-
-**Exemple**
-
-``` xml
-<timezone>Europe/Moscow</timezone>
-```
-
-## tcp_port {#server_configuration_parameters-tcp_port}
-
-Port pour communiquer avec les clients via le protocole TCP.
-
-**Exemple**
-
-``` xml
-<tcp_port>9000</tcp_port>
-```
-
-## tcp_port_secure {#server_configuration_parameters-tcp_port_secure}
-
-Port TCP pour une communication sécurisée avec les clients. Utilisez le avec [OpenSSL](#server_configuration_parameters-openssl) paramètre.
-
-**Valeurs possibles**
-
-Entier positif.
-
-**Valeur par défaut**
-
-``` xml
-<tcp_port_secure>9440</tcp_port_secure>
-```
-
-## mysql_port {#server_configuration_parameters-mysql_port}
-
-Port pour communiquer avec les clients via le protocole MySQL.
-
-**Valeurs possibles**
-
-Entier positif.
-
-Exemple
-
-``` xml
-<mysql_port>9004</mysql_port>
-```
-
-## tmp_path {#server-settings-tmp_path}
-
-Chemin d'accès aux données temporaires pour le traitement des requêtes volumineuses.
-
-!!! note "Note"
-    La barre oblique de fin est obligatoire.
-
-**Exemple**
-
-``` xml
-<tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
-```
-
-## tmp_policy {#server-settings-tmp-policy}
-
-La politique de [`storage_configuration`](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes) pour stocker des fichiers temporaires.
-Si cela n'est pas [`tmp_path`](#server-settings-tmp_path) est utilisé, sinon elle est ignorée.
-
-!!! note "Note"
-    - `move_factor` est ignoré
-- `keep_free_space_bytes` est ignoré
-- `max_data_part_size_bytes` est ignoré
-- vous devez avoir exactement un volume dans cette politique
-
-## uncompressed_cache_size {#server-settings-uncompressed_cache_size}
-
-Taille du Cache (en octets) pour les données non compressées utilisées par les [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md).
-
-Il y a un cache partagé pour le serveur. La mémoire est allouée à la demande. Le cache est utilisé si l'option [use_uncompressed_cache](../settings/settings.md#setting-use_uncompressed_cache) est activé.
-
-Le cache non compressé est avantageux pour les requêtes très courtes dans des cas individuels.
-
-**Exemple**
-
-``` xml
-<uncompressed_cache_size>8589934592</uncompressed_cache_size>
-```
-
-## user_files_path {#server_configuration_parameters-user_files_path}
-
-Le répertoire avec les fichiers utilisateur. Utilisé dans la fonction de table [fichier()](../../sql-reference/table-functions/file.md).
-
-**Exemple**
-
-``` xml
-<user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
-```
-
-## users_config {#users-config}
-
-Chemin d'accès au fichier qui contient:
-
--   Les configurations de l'utilisateur.
--   Les droits d'accès.
--   Les paramètres des profils.
--   Les paramètres de Quota.
-
-**Exemple**
-
-``` xml
-<users_config>users.xml</users_config>
-```
-
-## zookeeper {#server-settings_zookeeper}
-
-Contient des paramètres qui permettent à ClickHouse d'interagir avec [ZooKeeper](http://zookeeper.apache.org/) cluster.
-
-ClickHouse utilise ZooKeeper pour stocker les métadonnées des répliques lors de l'utilisation de tables répliquées. Si les tables répliquées ne sont pas utilisées, cette section de paramètres peut être omise.
-
-Cette section contient les paramètres suivants:
-
--   `node` — ZooKeeper endpoint. You can set multiple endpoints.
-
-    Exemple:
-
-<!-- -->
-
-``` xml
-    <node index="1">
-        <host>example_host</host>
-        <port>2181</port>
-    </node>
-```
-
-      The `index` attribute specifies the node order when trying to connect to the ZooKeeper cluster.
-
--   `session_timeout` — Maximum timeout for the client session in milliseconds.
--   `root` — The [znode](http://zookeeper.apache.org/doc/r3.5.5/zookeeperOver.html#Nodes+and+ephemeral+nodes) qui est utilisé comme racine pour les znodes utilisés par le serveur ClickHouse. Facultatif.
--   `identity` — User and password, that can be required by ZooKeeper to give access to requested znodes. Optional.
-
-**Exemple de configuration**
-
-``` xml
-<zookeeper>
-    <node>
-        <host>example1</host>
-        <port>2181</port>
-    </node>
-    <node>
-        <host>example2</host>
-        <port>2181</port>
-    </node>
-    <session_timeout_ms>30000</session_timeout_ms>
-    <operation_timeout_ms>10000</operation_timeout_ms>
-    <!-- Optional. Chroot suffix. Should exist. -->
-    <root>/path/to/zookeeper/node</root>
-    <!-- Optional. Zookeeper digest ACL string. -->
-    <identity>user:password</identity>
-</zookeeper>
-```
-
-**Voir Aussi**
-
--   [Réplication](../../engines/table-engines/mergetree-family/replication.md)
--   [Guide du programmeur ZooKeeper](http://zookeeper.apache.org/doc/current/zookeeperProgrammers.html)
-
-## use_minimalistic_part_header_in_zookeeper {#server-settings-use_minimalistic_part_header_in_zookeeper}
-
-Méthode de stockage pour les en-têtes de partie de données dans ZooKeeper.
-
-Ce paramètre s'applique uniquement à l' `MergeTree` famille. Il peut être spécifié:
-
--   À l'échelle mondiale dans le [merge_tree](#server_configuration_parameters-merge_tree) la section de la `config.xml` fichier.
-
-    ClickHouse utilise le paramètre pour toutes les tables du serveur. Vous pouvez modifier le réglage à tout moment. Les tables existantes changent de comportement lorsque le paramètre change.
-
--   Pour chaque table.
-
-    Lors de la création d'un tableau, indiquer la [moteur de réglage](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table). Le comportement d'une table existante avec ce paramètre ne change pas, même si le paramètre global des changements.
-
-**Valeurs possibles**
-
--   0 — Functionality is turned off.
--   1 — Functionality is turned on.
-
-Si `use_minimalistic_part_header_in_zookeeper = 1`, puis [répliqué](../../engines/table-engines/mergetree-family/replication.md) les tables stockent les en-têtes des parties de données de manière compacte à l'aide `znode`. Si la table contient plusieurs colonnes, cette méthode de stockage réduit considérablement le volume des données stockées dans Zookeeper.
-
-!!! attention "Attention"
-    Après l'application de `use_minimalistic_part_header_in_zookeeper = 1`, vous ne pouvez pas rétrograder le serveur ClickHouse vers une version qui ne prend pas en charge ce paramètre. Soyez prudent lors de la mise à niveau de ClickHouse sur les serveurs d'un cluster. Ne mettez pas à niveau tous les serveurs à la fois. Il est plus sûr de tester de nouvelles versions de ClickHouse dans un environnement de test, ou sur quelques serveurs d'un cluster.
-
-      Data part headers already stored with this setting can't be restored to their previous (non-compact) representation.
-
-**Valeur par défaut:** 0.
-
-## disable_internal_dns_cache {#server-settings-disable-internal-dns-cache}
-
-Désactive le cache DNS interne. Recommandé pour l'utilisation de ClickHouse dans les systèmes
-avec des infrastructures en constante évolution telles que Kubernetes.
-
-**Valeur par défaut:** 0.
-
-## dns_cache_update_period {#server-settings-dns-cache-update-period}
-
-La période de mise à jour des adresses IP stockées dans le cache DNS interne de ClickHouse (en secondes).
-La mise à jour est effectuée de manière asynchrone, dans un thread système séparé.
-
-**Valeur par défaut**: 15.
-
-## access_control_path {#access_control_path}
-
-Chemin d'accès à un dossier dans lequel un serveur clickhouse stocke les configurations utilisateur et rôle créées par les commandes SQL.
-
-Valeur par défaut: `/var/lib/clickhouse/access/`.
-
-**Voir aussi**
-
--   [Le Contrôle d'accès et de Gestion de Compte](../access-rights.md#access-control)
-
-[Article Original](https://clickhouse.tech/docs/en/operations/server_configuration_parameters/settings/) <!--hide-->
diff --git a/docs/fr/operations/settings/constraints-on-settings.md b/docs/fr/operations/settings/constraints-on-settings.md
deleted file mode 100644
index b9c34841da1d..000000000000
--- a/docs/fr/operations/settings/constraints-on-settings.md
+++ /dev/null
@@ -1,75 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 62
-toc_title: "Contraintes sur les param\xE8tres"
----
-
-# Contraintes sur les paramètres {#constraints-on-settings}
-
-Les contraintes sur les paramètres peuvent être définis dans le `profiles` la section de la `user.xml` fichier de configuration et interdire aux utilisateurs de modifier certains `SET` requête.
-Les contraintes sont définies comme suit:
-
-``` xml
-<profiles>
-  <user_name>
-    <constraints>
-      <setting_name_1>
-        <min>lower_boundary</min>
-      </setting_name_1>
-      <setting_name_2>
-        <max>upper_boundary</max>
-      </setting_name_2>
-      <setting_name_3>
-        <min>lower_boundary</min>
-        <max>upper_boundary</max>
-      </setting_name_3>
-      <setting_name_4>
-        <readonly/>
-      </setting_name_4>
-    </constraints>
-  </user_name>
-</profiles>
-```
-
-Si l'utilisateur tente de violer les contraintes une exception est levée et le réglage n'est pas modifié.
-Trois types de contraintes sont pris en charge: `min`, `max`, `readonly`. Le `min` et `max` les contraintes spécifient les limites supérieure et inférieure pour un paramètre numérique et peuvent être utilisées en combinaison. Le `readonly` contrainte spécifie que l'utilisateur ne peut pas modifier le paramètre correspondant à tous.
-
-**Exemple:** Laisser `users.xml` comprend des lignes:
-
-``` xml
-<profiles>
-  <default>
-    <max_memory_usage>10000000000</max_memory_usage>
-    <force_index_by_date>0</force_index_by_date>
-    ...
-    <constraints>
-      <max_memory_usage>
-        <min>5000000000</min>
-        <max>20000000000</max>
-      </max_memory_usage>
-      <force_index_by_date>
-        <readonly/>
-      </force_index_by_date>
-    </constraints>
-  </default>
-</profiles>
-```
-
-Les requêtes suivantes toutes les exceptions throw:
-
-``` sql
-SET max_memory_usage=20000000001;
-SET max_memory_usage=4999999999;
-SET force_index_by_date=1;
-```
-
-``` text
-Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be greater than 20000000000.
-Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be less than 5000000000.
-Code: 452, e.displayText() = DB::Exception: Setting force_index_by_date should not be changed.
-```
-
-**Note:** le `default` le profil a une manipulation particulière: toutes les contraintes définies pour `default` le profil devient les contraintes par défaut, de sorte qu'ils restreignent tous les utilisateurs jusqu'à ce qu'ils soient explicitement remplacés pour ces utilisateurs.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/settings/constraints_on_settings/) <!--hide-->
diff --git a/docs/fr/operations/settings/index.md b/docs/fr/operations/settings/index.md
deleted file mode 100644
index 0c81a2487c4f..000000000000
--- a/docs/fr/operations/settings/index.md
+++ /dev/null
@@ -1,33 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Param\xE8tre"
-toc_priority: 55
-toc_title: Introduction
----
-
-# Paramètre {#session-settings-intro}
-
-Il existe plusieurs façons d'effectuer tous les paramètres décrits dans cette section de la documentation.
-
-Les paramètres sont configurés en couches, de sorte que chaque couche suivante redéfinit les paramètres précédents.
-
-Façons de configurer les paramètres, par ordre de priorité:
-
--   Paramètres dans l' `users.xml` fichier de configuration du serveur.
-
-    Situé dans l'élément `<profiles>`.
-
--   Les paramètres de la Session.
-
-    Envoyer `SET setting=value` depuis le client de la console ClickHouse en mode interactif.
-    De même, vous pouvez utiliser des sessions ClickHouse dans le protocole HTTP. Pour ce faire, vous devez spécifier le `session_id` Paramètre HTTP.
-
--   Les paramètres de requête.
-
-    -   Lorsque vous démarrez le client clickhouse console en mode non interactif, définissez le paramètre startup `--setting=value`.
-    -   Lors de l'utilisation de L'API HTTP, passez les paramètres CGI (`URL?setting_1=value&setting_2=value...`).
-
-Les paramètres qui ne peuvent être effectués que dans le fichier de configuration du serveur ne sont pas couverts dans cette section.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/settings/) <!--hide-->
diff --git a/docs/fr/operations/settings/permissions-for-queries.md b/docs/fr/operations/settings/permissions-for-queries.md
deleted file mode 100644
index afab946adb35..000000000000
--- a/docs/fr/operations/settings/permissions-for-queries.md
+++ /dev/null
@@ -1,61 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: "Autorisations pour les requ\xEAtes"
----
-
-# Autorisations pour les requêtes {#permissions_for_queries}
-
-Les requêtes dans ClickHouse peuvent être divisées en plusieurs types:
-
-1.  Lire les requêtes de données: `SELECT`, `SHOW`, `DESCRIBE`, `EXISTS`.
-2.  Écrire des requêtes de données: `INSERT`, `OPTIMIZE`.
-3.  Modifier les paramètres requête: `SET`, `USE`.
-4.  [DDL](https://en.wikipedia.org/wiki/Data_definition_language) requête: `CREATE`, `ALTER`, `RENAME`, `ATTACH`, `DETACH`, `DROP` `TRUNCATE`.
-5.  `KILL QUERY`.
-
-Les paramètres suivants règlent les autorisations utilisateur selon le type de requête:
-
--   [ReadOnly](#settings_readonly) — Restricts permissions for all types of queries except DDL queries.
--   [allow_ddl](#settings_allow_ddl) — Restricts permissions for DDL queries.
-
-`KILL QUERY` peut être réalisée avec tous les paramètres.
-
-## ReadOnly {#settings_readonly}
-
-Restreint les autorisations pour lire des données, écrire des données et modifier les requêtes de paramètres.
-
-Voyez comment les requêtes sont divisées en types [surtout](#permissions_for_queries).
-
-Valeurs possibles:
-
--   0 — All queries are allowed.
--   1 — Only read data queries are allowed.
--   2 — Read data and change settings queries are allowed.
-
-Après le réglage de `readonly = 1` l'utilisateur ne peut pas changer `readonly` et `allow_ddl` les paramètres de la session en cours.
-
-Lors de l'utilisation de la `GET` méthode dans le [Interface HTTP](../../interfaces/http.md), `readonly = 1` est définie automatiquement. Pour modifier les données, utilisez `POST` méthode.
-
-Paramètre `readonly = 1` interdire à l'utilisateur de modifier tous les paramètres. Il y a un moyen d'interdire à l'utilisateur
-de modifier uniquement des paramètres spécifiques, pour plus de détails, voir [contraintes sur les paramètres](constraints-on-settings.md).
-
-Valeur par défaut: 0
-
-## allow_ddl {#settings_allow_ddl}
-
-Permet ou interdit [DDL](https://en.wikipedia.org/wiki/Data_definition_language) requête.
-
-Voyez comment les requêtes sont divisées en types [surtout](#permissions_for_queries).
-
-Valeurs possibles:
-
--   0 — DDL queries are not allowed.
--   1 — DDL queries are allowed.
-
-Vous ne pouvez pas exécuter `SET allow_ddl = 1` si `allow_ddl = 0` pour la session en cours.
-
-Valeur par défaut: 1
-
-[Article Original](https://clickhouse.tech/docs/en/operations/settings/permissions_for_queries/) <!--hide-->
diff --git a/docs/fr/operations/settings/query-complexity.md b/docs/fr/operations/settings/query-complexity.md
deleted file mode 100644
index bdeb3e352427..000000000000
--- a/docs/fr/operations/settings/query-complexity.md
+++ /dev/null
@@ -1,301 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: "Restrictions sur la complexit\xE9 des requ\xEAtes"
----
-
-# Restrictions sur la complexité des requêtes {#restrictions-on-query-complexity}
-
-Les Restrictions sur la complexité des requêtes font partie des paramètres.
-Ils sont utilisés pour fournir une exécution plus sûre à partir de l'interface utilisateur.
-Presque toutes les restrictions ne s'appliquent qu'à `SELECT`. Pour le traitement des requêtes distribuées, des restrictions sont appliquées sur chaque serveur séparément.
-
-ClickHouse vérifie les restrictions pour les parties de données, pas pour chaque ligne. Cela signifie que vous pouvez dépasser la valeur de restriction de la taille de la partie données.
-
-Restrictions sur l' “maximum amount of something” peut prendre la valeur 0, ce qui signifie “unrestricted”.
-La plupart des restrictions ont également un ‘overflow_mode’ paramètre signification que faire lorsque la limite est dépassée.
-Il peut prendre deux valeurs: `throw` ou `break`. Les Restrictions sur l'agrégation (group_by_overflow_mode) ont également la valeur `any`.
-
-`throw` – Throw an exception (default).
-
-`break` – Stop executing the query and return the partial result, as if the source data ran out.
-
-`any (only for group_by_overflow_mode)` – Continuing aggregation for the keys that got into the set, but don't add new keys to the set.
-
-## max_memory_usage {#settings_max_memory_usage}
-
-La quantité maximale de RAM à utiliser pour exécuter une requête sur un seul serveur.
-
-Dans le fichier de configuration par défaut, le maximum est de 10 Go.
-
-Le réglage ne tient pas compte du volume de mémoire disponible ou du volume total de mémoire sur la machine.
-La restriction s'applique à une seule requête au sein d'un seul serveur.
-Vous pouvez utiliser `SHOW PROCESSLIST` pour vérifier la consommation de mémoire pour chaque requête.
-En outre, la consommation de mémoire maximale est suivie pour chaque requête et écrite dans le journal.
-
-L'utilisation de la mémoire n'est pas surveillée pour les membres de certaines fonctions d'agrégation.
-
-L'utilisation de la mémoire n'est pas totalement suivies pour les états des fonctions d'agrégation `min`, `max`, `any`, `anyLast`, `argMin`, `argMax` de `String` et `Array` argument.
-
-La consommation de mémoire est également limitée par les paramètres `max_memory_usage_for_user` et `max_memory_usage_for_all_queries`.
-
-## max_memory_usage_for_user {#max-memory-usage-for-user}
-
-Quantité maximale de RAM à utiliser pour exécuter les requêtes d'un utilisateur sur un seul serveur.
-
-Les valeurs par défaut sont définies dans [Paramètre.h](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.h#L288). Par défaut, le montant n'est pas limité (`max_memory_usage_for_user = 0`).
-
-Voir aussi la description de [max_memory_usage](#settings_max_memory_usage).
-
-## max_memory_usage_for_all_queries {#max-memory-usage-for-all-queries}
-
-La quantité maximale de RAM à utiliser pour exécuter toutes les requêtes sur un seul serveur.
-
-Les valeurs par défaut sont définies dans [Paramètre.h](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.h#L289). Par défaut, le montant n'est pas limité (`max_memory_usage_for_all_queries = 0`).
-
-Voir aussi la description de [max_memory_usage](#settings_max_memory_usage).
-
-## max_rows_to_read {#max-rows-to-read}
-
-Les restrictions suivantes peut être vérifiée sur chaque bloc (au lieu de sur chaque ligne). Autrement dit, les restrictions peuvent être brisées un peu.
-Lors de l'exécution d'une requête dans plusieurs threads, les restrictions suivantes s'appliquent à chaque thread séparément.
-
-Un nombre maximum de lignes pouvant être lues à partir d'un tableau lors de l'exécution d'une requête.
-
-## max_bytes_to_read {#max-bytes-to-read}
-
-Nombre maximal d'octets (données non compressées) pouvant être lus à partir d'une table lors de l'exécution d'une requête.
-
-## read_overflow_mode {#read-overflow-mode}
-
-Que faire lorsque le volume de lecture de données dépasse l'une des limites: ‘throw’ ou ‘break’. Par défaut, les jeter.
-
-## max_rows_to_group_by {#settings-max-rows-to-group-by}
-
-Un nombre maximum de clés uniques reçues de l'agrégation. Ce paramètre permet de limiter la consommation de mémoire lors de l'agrégation.
-
-## group_by_overflow_mode {#group-by-overflow-mode}
-
-Que faire lorsque le nombre de clés uniques pour l'agrégation dépasse la limite: ‘throw’, ‘break’, ou ‘any’. Par défaut, les jeter.
-À l'aide de la ‘any’ valeur vous permet d'exécuter une approximation de GROUP BY. La qualité de cette approximation dépend de la nature statistique des données.
-
-## max_bytes_before_external_group_by {#settings-max_bytes_before_external_group_by}
-
-Active ou désactive l'exécution de `GROUP BY` clauses dans la mémoire externe. Voir [Groupe par dans la mémoire externe](../../sql-reference/statements/select/group-by.md#select-group-by-in-external-memory).
-
-Valeurs possibles:
-
--   Volume maximal de RAM (en octets) pouvant être utilisé par le [GROUP BY](../../sql-reference/statements/select/group-by.md#select-group-by-clause) opération.
--   0 — `GROUP BY` dans la mémoire externe désactivé.
-
-Valeur par défaut: 0.
-
-## max_rows_to_sort {#max-rows-to-sort}
-
-Un nombre maximum de lignes avant le tri. Cela vous permet de limiter la consommation de mémoire lors du tri.
-
-## max_bytes_to_sort {#max-bytes-to-sort}
-
-Un nombre maximal d'octets avant le tri.
-
-## sort_overflow_mode {#sort-overflow-mode}
-
-Que faire si le nombre de lignes reçues avant le tri dépasse l'une des limites: ‘throw’ ou ‘break’. Par défaut, les jeter.
-
-## max_result_rows {#setting-max_result_rows}
-
-Limite sur le nombre de lignes dans le résultat. Également vérifié pour les sous-requêtes, et sur des serveurs distants lors de l'exécution de parties d'une requête distribuée.
-
-## max_result_bytes {#max-result-bytes}
-
-Limite sur le nombre d'octets dans le résultat. Le même que le réglage précédent.
-
-## result_overflow_mode {#result-overflow-mode}
-
-Que faire si le volume du résultat dépasse l'une des limites: ‘throw’ ou ‘break’. Par défaut, les jeter.
-
-Utiliser ‘break’ est similaire à L'utilisation de LIMIT. `Break` interrompt l'exécution seulement au niveau du bloc. Cela signifie que la quantité de lignes renvoyées est supérieure à [max_result_rows](#setting-max_result_rows) multiples de [max_block_size](settings.md#setting-max_block_size) et dépend de l' [max_threads](settings.md#settings-max_threads).
-
-Exemple:
-
-``` sql
-SET max_threads = 3, max_block_size = 3333;
-SET max_result_rows = 3334, result_overflow_mode = 'break';
-
-SELECT *
-FROM numbers_mt(100000)
-FORMAT Null;
-```
-
-Résultat:
-
-``` text
-6666 rows in set. ...
-```
-
-## max_execution_time {#max-execution-time}
-
-Durée maximale d'exécution de la requête en secondes.
-Pour le moment, il n'est pas vérifié pour l'une des étapes de tri, ni lors de la fusion et de la finalisation des fonctions d'agrégat.
-
-## timeout_overflow_mode {#timeout-overflow-mode}
-
-Que faire si la requête est exécutée plus de ‘max_execution_time’: ‘throw’ ou ‘break’. Par défaut, les jeter.
-
-## min_execution_speed {#min-execution-speed}
-
-Vitesse d'exécution minimale en lignes par seconde. Vérifié sur chaque bloc de données quand ‘timeout_before_checking_execution_speed’ expirer. Si la vitesse d'exécution est inférieure, une exception est levée.
-
-## min_execution_speed_bytes {#min-execution-speed-bytes}
-
-Un nombre minimum d'exécution d'octets par seconde. Vérifié sur chaque bloc de données quand ‘timeout_before_checking_execution_speed’ expirer. Si la vitesse d'exécution est inférieure, une exception est levée.
-
-## max_execution_speed {#max-execution-speed}
-
-Un nombre maximal d'exécution de lignes par seconde. Vérifié sur chaque bloc de données quand ‘timeout_before_checking_execution_speed’ expirer. Si la vitesse d'exécution est élevée, la vitesse d'exécution sera réduit.
-
-## max_execution_speed_bytes {#max-execution-speed-bytes}
-
-Un nombre maximal d'exécution d'octets par seconde. Vérifié sur chaque bloc de données quand ‘timeout_before_checking_execution_speed’ expirer. Si la vitesse d'exécution est élevée, la vitesse d'exécution sera réduit.
-
-## timeout_before_checking_execution_speed {#timeout-before-checking-execution-speed}
-
-Vérifie que la vitesse d'exécution n'est pas trop lent (pas moins de ‘min_execution_speed’), après l'expiration du temps spécifié en secondes.
-
-## max_columns_to_read {#max-columns-to-read}
-
-Nombre maximal de colonnes pouvant être lues à partir d'une table dans une seule requête. Si une requête nécessite la lecture d'un plus grand nombre de colonnes, il lève une exception.
-
-## max_temporary_columns {#max-temporary-columns}
-
-Nombre maximal de colonnes temporaires qui doivent être conservées en RAM en même temps lors de l'exécution d'une requête, y compris les colonnes constantes. S'il y a plus de colonnes temporaires que cela, il lève une exception.
-
-## max_temporary_non_const_columns {#max-temporary-non-const-columns}
-
-La même chose que ‘max_temporary_columns’ mais sans compter constante colonnes.
-Notez que les colonnes constantes sont formées assez souvent lors de l'exécution d'une requête, mais elles nécessitent environ zéro ressource informatique.
-
-## max_subquery_depth {#max-subquery-depth}
-
-Profondeur maximale de sous-requêtes. Si les sous-requêtes sont plus profondes, une exception est levée. Par défaut, 100.
-
-## max_pipeline_depth {#max-pipeline-depth}
-
-Profondeur maximale du pipeline. Correspond au nombre de transformations que chaque bloc de données lors du traitement des requêtes. Compté dans les limites d'un seul serveur. Si la profondeur du pipeline est supérieure, une exception est levée. Par défaut, 1000.
-
-## max_ast_depth {#max-ast-depth}
-
-Profondeur maximale d'une requête arbre syntaxique. En cas de dépassement, une exception est levée.
-À ce moment, il n'est pas vérifié pendant l'analyse, mais seulement après l'analyse de la requête. Autrement dit, un arbre syntaxique trop profond peut être créé pendant l'analyse, mais la requête échouera. Par défaut, 1000.
-
-## max_ast_elements {#max-ast-elements}
-
-Un nombre maximal d'éléments dans une requête arbre syntaxique. En cas de dépassement, une exception est levée.
-De la même manière que le paramètre précédent, il est vérifié qu'après l'analyse de la requête. Par défaut, 50 000.
-
-## max_rows_in_set {#max-rows-in-set}
-
-Nombre maximal de lignes pour un ensemble de données dans la clause in créée à partir d'une sous-requête.
-
-## max_bytes_in_set {#max-bytes-in-set}
-
-Nombre maximal d'octets (données non compressées) utilisés par un ensemble de la clause in créé à partir d'une sous-requête.
-
-## set_overflow_mode {#set-overflow-mode}
-
-Que faire lorsque la quantité de données dépasse l'une des limites: ‘throw’ ou ‘break’. Par défaut, les jeter.
-
-## max_rows_in_distinct {#max-rows-in-distinct}
-
-Un nombre maximum de lignes différentes lors de L'utilisation de DISTINCT.
-
-## max_bytes_in_distinct {#max-bytes-in-distinct}
-
-Nombre maximal d'octets utilisés par une table de hachage lors de L'utilisation de DISTINCT.
-
-## distinct_overflow_mode {#distinct-overflow-mode}
-
-Que faire lorsque la quantité de données dépasse l'une des limites: ‘throw’ ou ‘break’. Par défaut, les jeter.
-
-## max_rows_to_transfer {#max-rows-to-transfer}
-
-Nombre maximal de lignes pouvant être transmises à un serveur distant ou enregistrées dans une table temporaire lors de L'utilisation de GLOBAL IN.
-
-## max_bytes_to_transfer {#max-bytes-to-transfer}
-
-Nombre maximal d'octets (données non compressées) pouvant être transmis à un serveur distant ou enregistrés dans une table temporaire lors de L'utilisation de GLOBAL IN.
-
-## transfer_overflow_mode {#transfer-overflow-mode}
-
-Que faire lorsque la quantité de données dépasse l'une des limites: ‘throw’ ou ‘break’. Par défaut, les jeter.
-
-## max_rows_in_join {#settings-max_rows_in_join}
-
-Limite le nombre de lignes dans la table de hachage utilisée lors de la jonction de tables.
-
-Ce réglage s'applique à [SELECT … JOIN](../../sql-reference/statements/select/join.md#select-join) les opérations et les [Rejoindre](../../engines/table-engines/special/join.md) tableau moteur.
-
-Si une requête contient plusieurs jointures, ClickHouse vérifie ce paramètre pour chaque résultat intermédiaire.
-
-ClickHouse peut procéder à différentes actions lorsque la limite est atteinte. L'utilisation de la [join_overflow_mode](#settings-join_overflow_mode) réglage pour choisir l'action.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Unlimited number of rows.
-
-Valeur par défaut: 0.
-
-## max_bytes_in_join {#settings-max_bytes_in_join}
-
-Limite la taille en octets de la table de hachage utilisée lors de l'assemblage de tables.
-
-Ce réglage s'applique à [SELECT … JOIN](../../sql-reference/statements/select/join.md#select-join) les opérations et les [Rejoindre le moteur de table](../../engines/table-engines/special/join.md).
-
-Si la requête contient des jointures, ClickHouse vérifie ce paramètre pour chaque résultat intermédiaire.
-
-ClickHouse peut procéder à différentes actions lorsque la limite est atteinte. Utiliser [join_overflow_mode](#settings-join_overflow_mode) paramètres pour choisir l'action.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Memory control is disabled.
-
-Valeur par défaut: 0.
-
-## join_overflow_mode {#settings-join_overflow_mode}
-
-Définit l'action que ClickHouse effectue lorsque l'une des limites de jointure suivantes est atteinte:
-
--   [max_bytes_in_join](#settings-max_bytes_in_join)
--   [max_rows_in_join](#settings-max_rows_in_join)
-
-Valeurs possibles:
-
--   `THROW` — ClickHouse throws an exception and breaks operation.
--   `BREAK` — ClickHouse breaks operation and doesn't throw an exception.
-
-Valeur par défaut: `THROW`.
-
-**Voir Aussi**
-
--   [Clause de JOINTURE](../../sql-reference/statements/select/join.md#select-join)
--   [Rejoindre le moteur de table](../../engines/table-engines/special/join.md)
-
-## max_partitions_per_insert_block {#max-partitions-per-insert-block}
-
-Limite le nombre maximal de partitions dans un seul bloc inséré.
-
--   Entier positif.
--   0 — Unlimited number of partitions.
-
-Valeur par défaut: 100.
-
-**Détail**
-
-Lors de l'insertion de données, ClickHouse calcule le nombre de partitions dans le bloc inséré. Si le nombre de partitions est plus que `max_partitions_per_insert_block`, ClickHouse lève une exception avec le texte suivant:
-
-> “Too many partitions for single INSERT block (more than” + toString (max_parts) + “). The limit is controlled by ‘max_partitions_per_insert_block’ setting. A large number of partitions is a common misconception. It will lead to severe negative performance impact, including slow server startup, slow INSERT queries and slow SELECT queries. Recommended total number of partitions for a table is under 1000..10000. Please note, that partitioning is not intended to speed up SELECT queries (ORDER BY key is sufficient to make range queries fast). Partitions are intended for data manipulation (DROP PARTITION, etc).”
-
-[Article Original](https://clickhouse.tech/docs/en/operations/settings/query_complexity/) <!--hide-->
diff --git a/docs/fr/operations/settings/settings-profiles.md b/docs/fr/operations/settings/settings-profiles.md
deleted file mode 100644
index 2a0338e78731..000000000000
--- a/docs/fr/operations/settings/settings-profiles.md
+++ /dev/null
@@ -1,81 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: "Les Param\xE8tres Des Profils"
----
-
-# Les Paramètres Des Profils {#settings-profiles}
-
-Un profil de paramètres est une collection de paramètres regroupés sous le même nom.
-
-!!! note "Information"
-    Clickhouse prend également en charge [Flux de travail piloté par SQL](../access-rights.md#access-control) pour gérer les profils de paramètres. Nous vous conseillons de l'utiliser.
-
-Un profil peut avoir n'importe quel nom. Le profil peut avoir n'importe quel nom. Vous pouvez spécifier le même profil pour différents utilisateurs. La chose la plus importante que vous pouvez écrire dans les paramètres de profil `readonly=1` qui assure un accès en lecture seule.
-
-Paramètres les profils peuvent hériter les uns des autres. Pour utiliser l'héritage, indiquer un ou plusieurs `profile` paramètres avant les autres paramètres répertoriés dans le profil. Dans le cas où un paramètre est défini dans les différents profils, les dernières définie est utilisée.
-
-Pour appliquer tous les paramètres d'un profil, définissez `profile` paramètre.
-
-Exemple:
-
-Installer le `web` profil.
-
-``` sql
-SET profile = 'web'
-```
-
-Les profils de paramètres sont déclarés dans le fichier de configuration utilisateur. Ce n'est généralement `users.xml`.
-
-Exemple:
-
-``` xml
-<!-- Settings profiles -->
-<profiles>
-    <!-- Default settings -->
-    <default>
-        <!-- The maximum number of threads when running a single query. -->
-        <max_threads>8</max_threads>
-    </default>
-
-    <!-- Settings for quries from the user interface -->
-    <web>
-        <max_rows_to_read>1000000000</max_rows_to_read>
-        <max_bytes_to_read>100000000000</max_bytes_to_read>
-
-        <max_rows_to_group_by>1000000</max_rows_to_group_by>
-        <group_by_overflow_mode>any</group_by_overflow_mode>
-
-        <max_rows_to_sort>1000000</max_rows_to_sort>
-        <max_bytes_to_sort>1000000000</max_bytes_to_sort>
-
-        <max_result_rows>100000</max_result_rows>
-        <max_result_bytes>100000000</max_result_bytes>
-        <result_overflow_mode>break</result_overflow_mode>
-
-        <max_execution_time>600</max_execution_time>
-        <min_execution_speed>1000000</min_execution_speed>
-        <timeout_before_checking_execution_speed>15</timeout_before_checking_execution_speed>
-
-        <max_columns_to_read>25</max_columns_to_read>
-        <max_temporary_columns>100</max_temporary_columns>
-        <max_temporary_non_const_columns>50</max_temporary_non_const_columns>
-
-        <max_subquery_depth>2</max_subquery_depth>
-        <max_pipeline_depth>25</max_pipeline_depth>
-        <max_ast_depth>50</max_ast_depth>
-        <max_ast_elements>100</max_ast_elements>
-
-        <readonly>1</readonly>
-    </web>
-</profiles>
-```
-
-L'exemple spécifie deux profils: `default` et `web`.
-
-Le `default` profil a un but particulier: il doit toujours être présent et est appliquée lors du démarrage du serveur. En d'autres termes, l' `default` profil contient les paramètres par défaut.
-
-Le `web` profil est un profil régulier qui peut être défini à l'aide `SET` requête ou en utilisant un paramètre URL dans une requête HTTP.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/settings/settings_profiles/) <!--hide-->
diff --git a/docs/fr/operations/settings/settings-users.md b/docs/fr/operations/settings/settings-users.md
deleted file mode 100644
index d33b8f1335ba..000000000000
--- a/docs/fr/operations/settings/settings-users.md
+++ /dev/null
@@ -1,164 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 63
-toc_title: "Les Param\xE8tres De L'Utilisateur"
----
-
-# Les Paramètres De L'Utilisateur {#user-settings}
-
-Le `users` la section de la `user.xml` le fichier de configuration contient les paramètres utilisateur.
-
-!!! note "Information"
-    Clickhouse prend également en charge [Flux de travail piloté par SQL](../access-rights.md#access-control) pour gérer les utilisateurs. Nous vous conseillons de l'utiliser.
-
-La Structure de la `users` section:
-
-``` xml
-<users>
-    <!-- If user name was not specified, 'default' user is used. -->
-    <user_name>
-        <password></password>
-        <!-- Or -->
-        <password_sha256_hex></password_sha256_hex>
-
-        <access_management>0|1</access_management>
-
-        <networks incl="networks" replace="replace">
-        </networks>
-
-        <profile>profile_name</profile>
-
-        <quota>default</quota>
-
-        <databases>
-            <database_name>
-                <table_name>
-                    <filter>expression</filter>
-                <table_name>
-            </database_name>
-        </databases>
-    </user_name>
-    <!-- Other users settings -->
-</users>
-```
-
-### nom_utilisateur/mot de passe {#user-namepassword}
-
-Le mot de passe peut être spécifié en texte clair ou en SHA256 (format hexadécimal).
-
--   Pour attribuer un mot de passe en clair (**pas recommandé**), la placer dans un `password` élément.
-
-    Exemple, `<password>qwerty</password>`. Le mot de passe peut être laissé en blanc.
-
-<a id="password_sha256_hex"></a>
-
--   Pour attribuer un mot de passe à l'aide de son hachage SHA256, placez-le dans un `password_sha256_hex` élément.
-
-    Exemple, `<password_sha256_hex>65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5</password_sha256_hex>`.
-
-    Exemple de génération d'un mot de passe à partir du shell:
-
-          PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'
-
-    La première ligne du résultat est le mot de passe. La deuxième ligne est le hachage SHA256 correspondant.
-
-<a id="password_double_sha1_hex"></a>
-
--   Pour la compatibilité avec les clients MySQL, le mot de passe peut être spécifié dans le hachage double SHA1. Le placer dans `password_double_sha1_hex` élément.
-
-    Exemple, `<password_double_sha1_hex>08b4a0f1de6ad37da17359e592c8d74788a83eb0</password_double_sha1_hex>`.
-
-    Exemple de génération d'un mot de passe à partir du shell:
-
-          PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-'
-
-    La première ligne du résultat est le mot de passe. La deuxième ligne est le double hachage SHA1 correspondant.
-
-### access_management {#access_management-user-setting}
-
-Ce paramètre active de désactive l'utilisation de SQL-driven [le contrôle d'accès et de gestion de compte](../access-rights.md#access-control) pour l'utilisateur.
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 0.
-
-### nom_utilisateur / réseaux {#user-namenetworks}
-
-Liste des réseaux à partir desquels L'utilisateur peut se connecter au serveur ClickHouse.
-
-Chaque élément de la liste peut avoir l'une des formes suivantes:
-
--   `<ip>` — IP address or network mask.
-
-    Exemple: `213.180.204.3`, `10.0.0.1/8`, `10.0.0.1/255.255.255.0`, `2a02:6b8::3`, `2a02:6b8::3/64`, `2a02:6b8::3/ffff:ffff:ffff:ffff::`.
-
--   `<host>` — Hostname.
-
-    Exemple: `example01.host.ru`.
-
-    Pour vérifier l'accès, une requête DNS est effectuée et toutes les adresses IP renvoyées sont comparées à l'adresse homologue.
-
--   `<host_regexp>` — Regular expression for hostnames.
-
-    Exemple, `^example\d\d-\d\d-\d\.host\.ru$`
-
-    Pour vérifier l'accès, un [Requête DNS PTR](https://en.wikipedia.org/wiki/Reverse_DNS_lookup) est effectuée pour l'adresse homologue, puis l'expression rationnelle spécifiée est appliquée. Ensuite, une autre requête DNS est effectuée pour les résultats de la requête PTR et toutes les adresses reçues sont comparées à l'adresse homologue. Nous recommandons fortement que regexp se termine avec $.
-
-Tous les résultats des requêtes DNS sont mis en cache jusqu'au redémarrage du serveur.
-
-**Exemple**
-
-Pour ouvrir l'accès de l'utilisateur à partir de n'importe quel réseau, spécifiez:
-
-``` xml
-<ip>::/0</ip>
-```
-
-!!! warning "Avertissement"
-    Il n'est pas sûr d'ouvrir l'accès à partir de n'importe quel réseau, sauf si vous avez un pare-feu correctement configuré ou si le serveur n'est pas directement connecté à Internet.
-
-Pour ouvrir l'accès uniquement à partir de localhost, spécifier:
-
-``` xml
-<ip>::1</ip>
-<ip>127.0.0.1</ip>
-```
-
-### nom_utilisateur / profil {#user-nameprofile}
-
-Vous pouvez attribuer un profil des paramètres pour l'utilisateur. Les profils de paramètres sont configurés dans une section distincte du `users.xml` fichier. Pour plus d'informations, voir [Profils des paramètres](settings-profiles.md).
-
-### nom_utilisateur / quota {#user-namequota}
-
-Les Quotas vous permettent de suivre ou de limiter l'utilisation des ressources sur une période donnée. Les Quotas sont configurés dans le `quotas`
-la section de la `users.xml` fichier de configuration.
-
-Vous pouvez attribuer un jeu de quotas à l'utilisateur. Pour une description détaillée de la configuration des quotas, voir [Quota](../quotas.md#quotas).
-
-### nom_utilisateur/bases de données {#user-namedatabases}
-
-Dans cette section, vous pouvez limiter les lignes renvoyées par ClickHouse pour `SELECT` requêtes faites par l'utilisateur actuel, implémentant ainsi la sécurité de base au niveau de la ligne.
-
-**Exemple**
-
-La configuration suivante force cet utilisateur `user1` ne peut voir les lignes de `table1` comme le résultat de `SELECT` requêtes, où la valeur de la `id` le champ est 1000.
-
-``` xml
-<user1>
-    <databases>
-        <database_name>
-            <table1>
-                <filter>id = 1000</filter>
-            </table1>
-        </database_name>
-    </databases>
-</user1>
-```
-
-Le `filter` peut être n'importe quelle expression résultant en un [UInt8](../../sql-reference/data-types/int-uint.md)-le type de la valeur. Il contient généralement des comparaisons et des opérateurs logiques. Les lignes de `database_name.table1` où filtrer les résultats à 0 ne sont pas retournés pour cet utilisateur. Le filtrage est incompatible avec `PREWHERE` opérations et désactive `WHERE→PREWHERE` optimisation.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/settings/settings_users/) <!--hide-->
diff --git a/docs/fr/operations/settings/settings.md b/docs/fr/operations/settings/settings.md
deleted file mode 100644
index 208e4ab1eae1..000000000000
--- a/docs/fr/operations/settings/settings.md
+++ /dev/null
@@ -1,1254 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Paramètre {#settings}
-
-## distributed_product_mode {#distributed-product-mode}
-
-Modifie le comportement de [distribués sous-requêtes](../../sql-reference/operators/in.md).
-
-ClickHouse applies this setting when the query contains the product of distributed tables, i.e. when the query for a distributed table contains a non-GLOBAL subquery for the distributed table.
-
-Restriction:
-
--   Uniquement appliqué pour les sous-requêtes IN et JOIN.
--   Uniquement si la section FROM utilise une table distribuée contenant plus d'un fragment.
--   Si la sous-requête concerne un distribué tableau contenant plus d'un fragment.
--   Pas utilisé pour une table [distant](../../sql-reference/table-functions/remote.md) fonction.
-
-Valeurs possibles:
-
--   `deny` — Default value. Prohibits using these types of subqueries (returns the “Double-distributed in/JOIN subqueries is denied” exception).
--   `local` — Replaces the database and table in the subquery with local ones for the destination server (shard), leaving the normal `IN`/`JOIN.`
--   `global` — Replaces the `IN`/`JOIN` requête avec `GLOBAL IN`/`GLOBAL JOIN.`
--   `allow` — Allows the use of these types of subqueries.
-
-## enable_optimize_predicate_expression {#enable-optimize-predicate-expression}
-
-Active la poussée du prédicat `SELECT` requête.
-
-Prédicat pushdown peut réduire considérablement le trafic réseau pour les requêtes distribuées.
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 1.
-
-Utilisation
-
-Considérez les requêtes suivantes:
-
-1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
-2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`
-
-Si `enable_optimize_predicate_expression = 1`, alors le temps d'exécution de ces requêtes est égal car ClickHouse s'applique `WHERE` à la sous-requête lors du traitement.
-
-Si `enable_optimize_predicate_expression = 0` puis le temps d'exécution de la deuxième requête est beaucoup plus long, parce que le `WHERE` la clause s'applique à toutes les données après la sous-requête des finitions.
-
-## fallback_to_stale_replicas_for_distributed_queries {#settings-fallback_to_stale_replicas_for_distributed_queries}
-
-Force une requête à un réplica obsolète si les données mises à jour ne sont pas disponibles. Voir [Réplication](../../engines/table-engines/mergetree-family/replication.md).
-
-ClickHouse sélectionne le plus pertinent parmi les répliques obsolètes de la table.
-
-Utilisé lors de l'exécution `SELECT` à partir d'une table distribuée qui pointe vers des tables répliquées.
-
-Par défaut, 1 (activé).
-
-## force_index_by_date {#settings-force_index_by_date}
-
-Désactive l'exécution de la requête si l'index ne peut pas être utilisé par jour.
-
-Fonctionne avec les tables de la famille MergeTree.
-
-Si `force_index_by_date=1`, Clickhouse vérifie si la requête a une condition de clé de date qui peut être utilisée pour restreindre les plages de données. S'il n'y a pas de condition appropriée, il lève une exception. Cependant, il ne vérifie pas si la condition réduit la quantité de données à lire. Par exemple, la condition `Date != ' 2000-01-01 '` est acceptable même lorsqu'il correspond à toutes les données de la table (c'est-à-dire que l'exécution de la requête nécessite une analyse complète). Pour plus d'informations sur les plages de données dans les tables MergeTree, voir [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md).
-
-## force_primary_key {#force-primary-key}
-
-Désactive l'exécution de la requête si l'indexation par la clé primaire n'est pas possible.
-
-Fonctionne avec les tables de la famille MergeTree.
-
-Si `force_primary_key=1`, Clickhouse vérifie si la requête a une condition de clé primaire qui peut être utilisée pour restreindre les plages de données. S'il n'y a pas de condition appropriée, il lève une exception. Cependant, il ne vérifie pas si la condition réduit la quantité de données à lire. Pour plus d'informations sur les plages de données dans les tables MergeTree, voir [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md).
-
-## format_schema {#format-schema}
-
-Ce paramètre est utile lorsque vous utilisez des formats nécessitant une définition de schéma, tels que [Cap'n Proto](https://capnproto.org/) ou [Protobuf](https://developers.google.com/protocol-buffers/). La valeur dépend du format.
-
-## fsync_metadata {#fsync-metadata}
-
-Active ou désactive [fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html) lors de l'écriture `.sql` fichier. Activé par défaut.
-
-Il est logique de le désactiver si le serveur a des millions de tables minuscules qui sont constamment créées et détruites.
-
-## enable_http_compression {#settings-enable_http_compression}
-
-Active ou désactive la compression de données dans la réponse à une requête HTTP.
-
-Pour plus d'informations, lire l' [Description de L'interface HTTP](../../interfaces/http.md).
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 0.
-
-## http_zlib_compression_level {#settings-http_zlib_compression_level}
-
-Définit le niveau de compression des données dans la réponse à une requête HTTP si [enable_http_compression = 1](#settings-enable_http_compression).
-
-Valeurs possibles: nombres de 1 à 9.
-
-Valeur par défaut: 3.
-
-## http_native_compression_disable_checksumming_on_decompress {#settings-http_native_compression_disable_checksumming_on_decompress}
-
-Active ou désactive la vérification de la somme de contrôle lors de la décompression des données HTTP POST du client. Utilisé uniquement pour le format de compression natif ClickHouse (non utilisé avec `gzip` ou `deflate`).
-
-Pour plus d'informations, lire l' [Description de L'interface HTTP](../../interfaces/http.md).
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 0.
-
-## send_progress_in_http_headers {#settings-send_progress_in_http_headers}
-
-Active ou désactive `X-ClickHouse-Progress` - Têtes de réponse HTTP dans `clickhouse-server` réponse.
-
-Pour plus d'informations, lire l' [Description de L'interface HTTP](../../interfaces/http.md).
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 0.
-
-## max_http_get_redirects {#setting-max_http_get_redirects}
-
-Limite le nombre maximal de sauts de redirection HTTP GET pour [URL](../../engines/table-engines/special/url.md)-tables de moteur. Le paramètre s'applique aux deux types de tables: celles créées par [CREATE TABLE](../../sql-reference/statements/create.md#create-table-query) requête et par la [URL](../../sql-reference/table-functions/url.md) table de fonction.
-
-Valeurs possibles:
-
--   Tout nombre entier positif de houblon.
--   0 — No hops allowed.
-
-Valeur par défaut: 0.
-
-## input_format_allow_errors_num {#settings-input_format_allow_errors_num}
-
-Définit le nombre maximal d'erreurs acceptables lors de la lecture à partir de formats de texte (CSV, TSV, etc.).
-
-La valeur par défaut est de 0.
-
-Toujours le coupler avec `input_format_allow_errors_ratio`.
-
-Si une erreur s'est produite lors de la lecture de lignes mais que le compteur d'erreurs est toujours inférieur à `input_format_allow_errors_num`, ClickHouse ignore la ligne et passe à la suivante.
-
-Si les deux `input_format_allow_errors_num` et `input_format_allow_errors_ratio` sont dépassés, ClickHouse lève une exception.
-
-## input_format_allow_errors_ratio {#settings-input_format_allow_errors_ratio}
-
-Définit le pourcentage maximal d'erreurs autorisées lors de la lecture à partir de formats de texte (CSV, TSV, etc.).
-Le pourcentage d'erreurs est défini comme un nombre à virgule flottante compris entre 0 et 1.
-
-La valeur par défaut est de 0.
-
-Toujours le coupler avec `input_format_allow_errors_num`.
-
-Si une erreur s'est produite lors de la lecture de lignes mais que le compteur d'erreurs est toujours inférieur à `input_format_allow_errors_ratio`, ClickHouse ignore la ligne et passe à la suivante.
-
-Si les deux `input_format_allow_errors_num` et `input_format_allow_errors_ratio` sont dépassés, ClickHouse lève une exception.
-
-## input_format_values_interpret_expressions {#settings-input_format_values_interpret_expressions}
-
-Active ou désactive L'analyseur SQL complet si l'analyseur de flux rapide ne peut pas analyser les données. Ce paramètre est utilisé uniquement pour la [Valeur](../../interfaces/formats.md#data-format-values) format lors de l'insertion des données. Pour plus d'informations sur l'analyse syntaxique, consultez [Syntaxe](../../sql-reference/syntax.md) section.
-
-Valeurs possibles:
-
--   0 — Disabled.
-
-    Dans ce cas, vous devez fournir des données formatées. Voir la [Format](../../interfaces/formats.md) section.
-
--   1 — Enabled.
-
-    Dans ce cas, vous pouvez utiliser une expression SQL en tant que valeur, mais l'insertion de données est beaucoup plus lente de cette façon. Si vous insérez uniquement des données formatées, ClickHouse se comporte comme si la valeur de réglage était 0.
-
-Valeur par défaut: 1.
-
-Exemple D'utilisation
-
-Insérez le [DateTime](../../sql-reference/data-types/datetime.md) tapez valeur avec les différents paramètres.
-
-``` sql
-SET input_format_values_interpret_expressions = 0;
-INSERT INTO datetime_t VALUES (now())
-```
-
-``` text
-Exception on client:
-Code: 27. DB::Exception: Cannot parse input: expected ) before: now()): (at row 1)
-```
-
-``` sql
-SET input_format_values_interpret_expressions = 1;
-INSERT INTO datetime_t VALUES (now())
-```
-
-``` text
-Ok.
-```
-
-La dernière requête est équivalente à la suivante:
-
-``` sql
-SET input_format_values_interpret_expressions = 0;
-INSERT INTO datetime_t SELECT now()
-```
-
-``` text
-Ok.
-```
-
-## input_format_values_deduce_templates_of_expressions {#settings-input_format_values_deduce_templates_of_expressions}
-
-Active ou désactive la déduction de modèle pour les expressions SQL dans [Valeur](../../interfaces/formats.md#data-format-values) format. Il permet d'analyser et d'interpréter des expressions dans `Values` beaucoup plus rapide si les expressions dans des lignes consécutives ont la même structure. ClickHouse tente de déduire le modèle d'une expression, d'analyser les lignes suivantes à l'aide de ce modèle et d'évaluer l'expression sur un lot de lignes analysées avec succès.
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 1.
-
-Pour la requête suivante:
-
-``` sql
-INSERT INTO test VALUES (lower('Hello')), (lower('world')), (lower('INSERT')), (upper('Values')), ...
-```
-
--   Si `input_format_values_interpret_expressions=1` et `format_values_deduce_templates_of_expressions=0`, les expressions sont interprétées séparément pour chaque ligne (c'est très lent pour un grand nombre de lignes).
--   Si `input_format_values_interpret_expressions=0` et `format_values_deduce_templates_of_expressions=1`, les expressions des première, deuxième et troisième lignes sont analysées à l'aide de template `lower(String)` et interprété ensemble, l'expression dans la quatrième ligne est analysée avec un autre modèle (`upper(String)`).
--   Si `input_format_values_interpret_expressions=1` et `format_values_deduce_templates_of_expressions=1`, le même que dans le cas précédent, mais permet également d'interpréter les expressions séparément s'il n'est pas possible de déduire le modèle.
-
-## input_format_values_accurate_types_of_literals {#settings-input-format-values-accurate-types-of-literals}
-
-Ce paramètre est utilisé uniquement lorsque `input_format_values_deduce_templates_of_expressions = 1`. Il peut arriver que les expressions pour une colonne aient la même structure, mais contiennent des littéraux numériques de types différents, par exemple
-
-``` sql
-(..., abs(0), ...),             -- UInt64 literal
-(..., abs(3.141592654), ...),   -- Float64 literal
-(..., abs(-1), ...),            -- Int64 literal
-```
-
-Valeurs possibles:
-
--   0 — Disabled.
-
-    In this case, ClickHouse may use a more general type for some literals (e.g., `Float64` ou `Int64` plutôt `UInt64` pour `42`), mais cela peut causer des problèmes de débordement et de précision.
-
--   1 — Enabled.
-
-    Dans ce cas, ClickHouse vérifie le type réel de littéral et utilise un modèle d'expression du type correspondant. Dans certains cas, cela peut considérablement ralentir l'évaluation de l'expression dans `Values`.
-
-Valeur par défaut: 1.
-
-## input_format_defaults_for_omitted_fields {#session_settings-input_format_defaults_for_omitted_fields}
-
-Lors de l'exécution de `INSERT` requêtes, remplacez les valeurs de colonne d'entrée omises par les valeurs par défaut des colonnes respectives. Cette option s'applique uniquement aux [JSONEachRow](../../interfaces/formats.md#jsoneachrow), [CSV](../../interfaces/formats.md#csv) et [TabSeparated](../../interfaces/formats.md#tabseparated) format.
-
-!!! note "Note"
-    Lorsque cette option est activée, les métadonnées de table étendues sont envoyées du serveur au client. Il consomme des ressources informatiques supplémentaires sur le serveur et peut réduire les performances.
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 1.
-
-## input_format_tsv_empty_as_default {#settings-input-format-tsv-empty-as-default}
-
-Lorsque cette option est activée, remplacez les champs de saisie vides dans TSV par des valeurs par défaut. Pour les expressions par défaut complexes `input_format_defaults_for_omitted_fields` doit être activé en trop.
-
-Désactivé par défaut.
-
-## input_format_null_as_default {#settings-input-format-null-as-default}
-
-Active ou désactive l'utilisation des valeurs par défaut si les données `NULL` mais le type de données de la colonne correspondante dans pas `Nullable(T)` (pour les formats de saisie de texte).
-
-## input_format_skip_unknown_fields {#settings-input-format-skip-unknown-fields}
-
-Active ou désactive le saut d'insertion de données supplémentaires.
-
-Lors de l'écriture de données, ClickHouse lève une exception si les données d'entrée contiennent des colonnes qui n'existent pas dans la table cible. Si le saut est activé, ClickHouse n'insère pas de données supplémentaires et ne lance pas d'exception.
-
-Formats pris en charge:
-
--   [JSONEachRow](../../interfaces/formats.md#jsoneachrow)
--   [CSVWithNames](../../interfaces/formats.md#csvwithnames)
--   [TabSeparatedWithNames](../../interfaces/formats.md#tabseparatedwithnames)
--   [TSKV](../../interfaces/formats.md#tskv)
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 0.
-
-## input_format_import_nested_json {#settings-input_format_import_nested_json}
-
-Active ou désactive l'insertion de données JSON avec des objets imbriqués.
-
-Formats pris en charge:
-
--   [JSONEachRow](../../interfaces/formats.md#jsoneachrow)
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 0.
-
-Voir aussi:
-
--   [Utilisation de Structures imbriquées](../../interfaces/formats.md#jsoneachrow-nested) avec l' `JSONEachRow` format.
-
-## input_format_with_names_use_header {#settings-input-format-with-names-use-header}
-
-Active ou désactive la vérification de l'ordre des colonnes lors de l'insertion de données.
-
-Pour améliorer les performances d'insertion, nous vous recommandons de désactiver cette vérification si vous êtes sûr que l'ordre des colonnes des données d'entrée est le même que dans la table cible.
-
-Formats pris en charge:
-
--   [CSVWithNames](../../interfaces/formats.md#csvwithnames)
--   [TabSeparatedWithNames](../../interfaces/formats.md#tabseparatedwithnames)
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 1.
-
-## date_time_input_format {#settings-date_time_input_format}
-
-Permet de choisir un analyseur de la représentation textuelle de la date et de l'heure.
-
-Le réglage ne s'applique pas à [fonctions date et heure](../../sql-reference/functions/date-time-functions.md).
-
-Valeurs possibles:
-
--   `'best_effort'` — Enables extended parsing.
-
-    ClickHouse peut analyser la base `YYYY-MM-DD HH:MM:SS` format et tous [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) formats de date et heure. Exemple, `'2018-06-08T01:02:03.000Z'`.
-
--   `'basic'` — Use basic parser.
-
-    ClickHouse ne peut analyser que la base `YYYY-MM-DD HH:MM:SS` format. Exemple, `'2019-08-20 10:18:56'`.
-
-Valeur par défaut: `'basic'`.
-
-Voir aussi:
-
--   [Type de données DateTime.](../../sql-reference/data-types/datetime.md)
--   [Fonctions pour travailler avec des dates et des heures.](../../sql-reference/functions/date-time-functions.md)
-
-## join_default_strictness {#settings-join_default_strictness}
-
-Définit la rigueur par défaut pour [JOIN clauses](../../sql-reference/statements/select/join.md#select-join).
-
-Valeurs possibles:
-
--   `ALL` — If the right table has several matching rows, ClickHouse creates a [Produit cartésien](https://en.wikipedia.org/wiki/Cartesian_product) à partir des lignes correspondantes. C'est normal `JOIN` comportement de SQL standard.
--   `ANY` — If the right table has several matching rows, only the first one found is joined. If the right table has only one matching row, the results of `ANY` et `ALL` sont les mêmes.
--   `ASOF` — For joining sequences with an uncertain match.
--   `Empty string` — If `ALL` ou `ANY` n'est pas spécifié dans la requête, ClickHouse lève une exception.
-
-Valeur par défaut: `ALL`.
-
-## join_any_take_last_row {#settings-join_any_take_last_row}
-
-Modifie le comportement des opérations de jointure avec `ANY` rigueur.
-
-!!! warning "Attention"
-    Ce paramètre s'applique uniquement pour `JOIN` opérations avec [Rejoindre](../../engines/table-engines/special/join.md) le moteur de tables.
-
-Valeurs possibles:
-
--   0 — If the right table has more than one matching row, only the first one found is joined.
--   1 — If the right table has more than one matching row, only the last one found is joined.
-
-Valeur par défaut: 0.
-
-Voir aussi:
-
--   [Clause de JOINTURE](../../sql-reference/statements/select/join.md#select-join)
--   [Rejoindre le moteur de table](../../engines/table-engines/special/join.md)
--   [join_default_strictness](#settings-join_default_strictness)
-
-## join_use_nulls {#join_use_nulls}
-
-Définit le type de [JOIN](../../sql-reference/statements/select/join.md) comportement. Lors de la fusion de tables, des cellules vides peuvent apparaître. ClickHouse les remplit différemment en fonction de ce paramètre.
-
-Valeurs possibles:
-
--   0 — The empty cells are filled with the default value of the corresponding field type.
--   1 — `JOIN` se comporte de la même manière que dans SQL standard. Le type du champ correspondant est converti en [Nullable](../../sql-reference/data-types/nullable.md#data_type-nullable) et les cellules vides sont remplis avec [NULL](../../sql-reference/syntax.md).
-
-Valeur par défaut: 0.
-
-## max_block_size {#setting-max_block_size}
-
-Dans ClickHouse, les données sont traitées par Blocs (Ensembles de parties de colonne). Les cycles de traitement internes pour un seul bloc sont assez efficaces, mais il y a des dépenses notables sur chaque bloc. Le `max_block_size` le paramètre est une recommandation pour la taille du bloc (dans un nombre de lignes) à charger à partir des tables. La taille du bloc ne doit pas être trop petite, de sorte que les dépenses sur chaque bloc sont toujours perceptibles, mais pas trop grande pour que la requête avec limite qui est terminée après le premier bloc soit traitée rapidement. L'objectif est d'éviter de consommer trop de mémoire lors de l'extraction d'un grand nombre de colonnes dans plusieurs threads et de préserver au moins certains localité de cache.
-
-Valeur par défaut: de 65 536.
-
-Les blocs de la taille de `max_block_size` ne sont pas toujours chargées de la table. Si il est évident que moins de données doivent être récupérées, un bloc plus petit est traitée.
-
-## preferred_block_size_bytes {#preferred-block-size-bytes}
-
-Utilisé dans le même but que `max_block_size`, mais il définit la taille de bloc recommandée en octets en l'adaptant au nombre de lignes dans le bloc.
-Cependant, la taille du bloc ne peut pas être supérieure à `max_block_size` rangée.
-Par défaut: 1 000 000. Cela ne fonctionne que lors de la lecture des moteurs MergeTree.
-
-## merge_tree_min_rows_for_concurrent_read {#setting-merge-tree-min-rows-for-concurrent-read}
-
-Si le nombre de lignes à lire à partir d'un fichier d'un [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) table dépasse `merge_tree_min_rows_for_concurrent_read` ensuite, ClickHouse essaie d'effectuer une lecture simultanée de ce fichier sur plusieurs threads.
-
-Valeurs possibles:
-
--   Tout nombre entier positif.
-
-Valeur par défaut: 163840.
-
-## merge_tree_min_bytes_for_concurrent_read {#setting-merge-tree-min-bytes-for-concurrent-read}
-
-Si le nombre d'octets à lire à partir d'un fichier d'un [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)- table de moteur dépasse `merge_tree_min_bytes_for_concurrent_read` puis ClickHouse essaie de lire simultanément à partir de ce fichier dans plusieurs threads.
-
-Valeur Possible:
-
--   Tout nombre entier positif.
-
-Valeur par défaut: 251658240.
-
-## merge_tree_min_rows_for_seek {#setting-merge-tree-min-rows-for-seek}
-
-Si la distance entre deux blocs de données à lire dans un fichier est inférieure à `merge_tree_min_rows_for_seek` lignes, puis ClickHouse ne cherche pas à travers le fichier mais lit les données séquentiellement.
-
-Valeurs possibles:
-
--   Tout nombre entier positif.
-
-Valeur par défaut: 0.
-
-## merge_tree_min_bytes_for_seek {#setting-merge-tree-min-bytes-for-seek}
-
-Si la distance entre deux blocs de données à lire dans un fichier est inférieure à `merge_tree_min_bytes_for_seek` octets, puis ClickHouse lit séquentiellement une plage de fichier qui contient les deux blocs, évitant ainsi la recherche supplémentaire.
-
-Valeurs possibles:
-
--   Tout nombre entier positif.
-
-Valeur par défaut: 0.
-
-## merge_tree_coarse_index_granularité {#setting-merge-tree-coarse-index-granularity}
-
-Lors de la recherche de données, ClickHouse vérifie les marques de données dans le fichier d'index. Si ClickHouse trouve que les clés requises sont dans une certaine plage, il divise cette plage en `merge_tree_coarse_index_granularity` subranges et recherche les clés requises récursivement.
-
-Valeurs possibles:
-
--   Tout entier pair positif.
-
-Valeur par défaut: 8.
-
-## merge_tree_max_rows_to_use_cache {#setting-merge-tree-max-rows-to-use-cache}
-
-Si ClickHouse devrait lire plus de `merge_tree_max_rows_to_use_cache` lignes dans une requête, il n'utilise pas le cache des blocs non compressés.
-
-Le cache des blocs non compressés stocke les données extraites pour les requêtes. ClickHouse utilise ce cache pour accélérer les réponses aux petites requêtes répétées. Ce paramètre protège le cache contre le saccage par les requêtes qui lisent une grande quantité de données. Le [uncompressed_cache_size](../server-configuration-parameters/settings.md#server-settings-uncompressed_cache_size) le paramètre serveur définit la taille du cache des blocs non compressés.
-
-Valeurs possibles:
-
--   Tout nombre entier positif.
-
-Default value: 128 ✕ 8192.
-
-## merge_tree_max_bytes_to_use_cache {#setting-merge-tree-max-bytes-to-use-cache}
-
-Si ClickHouse devrait lire plus de `merge_tree_max_bytes_to_use_cache` octets dans une requête, il n'utilise pas le cache de non compressé blocs.
-
-Le cache des blocs non compressés stocke les données extraites pour les requêtes. ClickHouse utilise ce cache pour accélérer les réponses aux petites requêtes répétées. Ce paramètre protège le cache contre le saccage par les requêtes qui lisent une grande quantité de données. Le [uncompressed_cache_size](../server-configuration-parameters/settings.md#server-settings-uncompressed_cache_size) le paramètre serveur définit la taille du cache des blocs non compressés.
-
-Valeur Possible:
-
--   Tout nombre entier positif.
-
-Valeur par défaut: 2013265920.
-
-## min_bytes_to_use_direct_io {#settings-min-bytes-to-use-direct-io}
-
-Volume de données minimum requis pour utiliser l'accès direct aux E/S sur le disque de stockage.
-
-ClickHouse utilise ce paramètre lors de la lecture de données à partir de tables. Si le volume total de stockage de toutes les données à lire dépasse `min_bytes_to_use_direct_io` octets, puis ClickHouse lit les données du disque de stockage avec le `O_DIRECT` option.
-
-Valeurs possibles:
-
--   0 — Direct I/O is disabled.
--   Entier positif.
-
-Valeur par défaut: 0.
-
-## log_queries {#settings-log-queries}
-
-Configuration de la journalisation des requêtes.
-
-Les requêtes envoyées à ClickHouse avec cette configuration sont enregistrées selon les règles du [query_log](../server-configuration-parameters/settings.md#server_configuration_parameters-query-log) paramètre de configuration du serveur.
-
-Exemple:
-
-``` text
-log_queries=1
-```
-
-## log_queries_min_type {#settings-log-queries-min-type}
-
-`query_log` type minimal à enregistrer.
-
-Valeurs possibles:
-- `QUERY_START` (`=1`)
-- `QUERY_FINISH` (`=2`)
-- `EXCEPTION_BEFORE_START` (`=3`)
-- `EXCEPTION_WHILE_PROCESSING` (`=4`)
-
-Valeur par défaut: `QUERY_START`.
-
-Peut être utilisé pour limiter le nombre de entiries va va `query_log`, dites que vous êtes intéressant que dans les erreurs, alors vous pouvez utiliser `EXCEPTION_WHILE_PROCESSING`:
-
-``` text
-log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
-```
-
-## log_query_threads {#settings-log-query-threads}
-
-Configuration de la journalisation des threads de requête.
-
-Les threads de requêtes exécutés par ClickHouse avec cette configuration sont journalisés selon les règles du [query_thread_log](../server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) paramètre de configuration du serveur.
-
-Exemple:
-
-``` text
-log_query_threads=1
-```
-
-## max_insert_block_size {#settings-max_insert_block_size}
-
-La taille des blocs à former pour l'insertion dans une table.
-Ce paramètre s'applique uniquement dans les cas où le serveur formes les blocs.
-Par exemple, pour une insertion via L'interface HTTP, le serveur analyse le format de données et forme des blocs de la taille spécifiée.
-Mais lors de l'utilisation de clickhouse-client, le client analyse les données ‘max_insert_block_size’ le réglage sur le serveur n'affecte pas la taille des blocs insérés.
-Le paramètre n'a pas non plus de but lors de L'utilisation D'INSERT SELECT, car les données sont insérées à l'aide des mêmes blocs qui sont formés après SELECT.
-
-Valeur par défaut: 1 048 576 octets.
-
-La valeur par défaut est légèrement supérieure à `max_block_size`. La raison en est que certains moteurs de table (`*MergeTree`) former une partie de données sur le disque pour chaque bloc inséré, qui est une entité assez grande. Pareillement, `*MergeTree` les tables trient les données lors de l'insertion et une taille de bloc suffisamment grande permet de trier plus de données dans la RAM.
-
-## min_insert_block_size_rows {#min-insert-block-size-rows}
-
-Définit le nombre minimum de lignes dans le bloc qui peut être inséré dans un tableau par un `INSERT` requête. Les blocs de plus petite taille sont écrasés en plus gros.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Squashing disabled.
-
-Valeur par défaut: 1048576.
-
-## min_insert_block_size_bytes {#min-insert-block-size-bytes}
-
-Définit le nombre minimal d'octets dans le bloc qui peut être inséré dans un tableau par un `INSERT` requête. Les blocs de plus petite taille sont écrasés en plus gros.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Squashing disabled.
-
-Valeur par défaut: 268435456.
-
-## max_replica_delay_for_distributed_queries {#settings-max_replica_delay_for_distributed_queries}
-
-Désactive les répliques en retard pour les requêtes distribuées. Voir [Réplication](../../engines/table-engines/mergetree-family/replication.md).
-
-Définit le temps en secondes. Si une réplique accuse plus de retard que la valeur définie, cette réplique n'est pas utilisée.
-
-Valeur par défaut: 300.
-
-Utilisé lors de l'exécution `SELECT` à partir d'une table distribuée qui pointe vers des tables répliquées.
-
-## max_threads {#settings-max_threads}
-
-Nombre maximal de threads de traitement des requêtes, à l'exclusion des threads de récupération de données à partir de serveurs distants (voir ‘max_distributed_connections’ paramètre).
-
-Ce paramètre s'applique aux threads qui effectuent les mêmes étapes du pipeline de traitement des requêtes en parallèle.
-Par exemple, lors de la lecture d'une table, s'il est possible d'évaluer des expressions avec des fonctions, filtrer avec WHERE et pré-agréger pour GROUP BY en parallèle en utilisant au moins ‘max_threads’ nombre de threads, puis ‘max_threads’ sont utilisés.
-
-Valeur par défaut: nombre de cœurs de processeur physiques.
-
-Si moins d'une requête SELECT est normalement exécutée sur un serveur à la fois, définissez ce paramètre sur une valeur légèrement inférieure au nombre réel de cœurs de processeur.
-
-Pour les requêtes qui sont terminées rapidement en raison d'une limite, vous pouvez définir une valeur inférieure ‘max_threads’. Par exemple, si le nombre d'entrées se trouvent dans chaque bloc et max_threads = 8, 8 blocs sont récupérées, même s'il aurait été suffisante pour lire un seul.
-
-Le plus petit de la `max_threads` valeur, moins la mémoire est consommée.
-
-## max_insert_threads {#settings-max-insert-threads}
-
-Nombre maximal de threads à exécuter `INSERT SELECT` requête.
-
-Valeurs possibles:
-
--   0 (or 1) — `INSERT SELECT` pas d'exécution parallèle.
--   Entier positif. Plus grand que 1.
-
-Valeur par défaut: 0.
-
-Parallèle `INSERT SELECT` n'a d'effet que si l' `SELECT` une partie est exécutée en parallèle, voir [max_threads](#settings-max_threads) paramètre.
-Des valeurs plus élevées conduiront à une utilisation de la mémoire plus élevée.
-
-## max_compress_block_size {#max-compress-block-size}
-
-La taille maximale des blocs de données non compressées avant la compression pour l'écriture dans une table. Par défaut, 1 048 576 (1 MiB). Si la taille est réduite, le taux de compression est considérablement réduit, la vitesse de compression et de décompression augmente légèrement en raison de la localisation du cache, et la consommation de mémoire est réduite. Il n'y aucune raison de modifier ce paramètre.
-
-Ne confondez pas les blocs pour la compression (un morceau de mémoire constitué d'octets) avec des blocs pour le traitement des requêtes (Un ensemble de lignes d'une table).
-
-## min_compress_block_size {#min-compress-block-size}
-
-Pour [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)" table. Afin de réduire la latence lors du traitement des requêtes, un bloc est compressé lors de l'écriture de la marque suivante si sa taille est au moins ‘min_compress_block_size’. Par défaut, 65 536.
-
-La taille réelle du bloc, si les données non compressées sont inférieures à ‘max_compress_block_size’ pas moins de cette valeur et pas moins que le volume de données pour une marque.
-
-Regardons un exemple. Supposons que ‘index_granularity’ a 8192 lors de la création de la table.
-
-Nous écrivons une colonne de type UInt32 (4 octets par valeur). Lors de l'écriture de 8192 lignes, le total sera de 32 KO de données. Puisque min_compress_block_size = 65 536, un bloc compressé sera formé pour toutes les deux marques.
-
-Nous écrivons une colonne URL avec le type de chaîne (taille moyenne de 60 octets par valeur). Lors de l'écriture de 8192 lignes, la moyenne sera légèrement inférieure à 500 Ko de données. Comme il s'agit de plus de 65 536, un bloc compressé sera formé pour chaque marque. Dans ce cas, lors de la lecture de données du disque dans la plage d'une seule marque, les données supplémentaires ne seront pas décompressées.
-
-Il n'y aucune raison de modifier ce paramètre.
-
-## max_query_size {#settings-max_query_size}
-
-La partie maximale d'une requête qui peut être prise en RAM pour l'analyse avec L'analyseur SQL.
-La requête INSERT contient également des données pour INSERT qui sont traitées par un analyseur de flux séparé (qui consomme O (1) RAM), qui n'est pas inclus dans cette restriction.
-
-Valeur par défaut: 256 Ko.
-
-## interactive_delay {#interactive-delay}
-
-Intervalle en microsecondes pour vérifier si l'exécution de la requête a été annulée et envoyer la progression.
-
-Valeur par défaut: 100 000 (vérifie l'Annulation et envoie la progression dix fois par seconde).
-
-## connect_timeout, receive_timeout, send_timeout {#connect-timeout-receive-timeout-send-timeout}
-
-Délais d'attente en secondes sur le socket utilisé pour communiquer avec le client.
-
-Valeur par défaut: 10, 300, 300.
-
-## cancel_http_readonly_queries_on_client_close {#cancel-http-readonly-queries-on-client-close}
-
-Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response.
-
-Valeur par défaut: 0
-
-## poll_interval {#poll-interval}
-
-Verrouillez une boucle d'attente pendant le nombre de secondes spécifié.
-
-Valeur par défaut: 10.
-
-## max_distributed_connections {#max-distributed-connections}
-
-Nombre maximal de connexions simultanées avec des serveurs distants pour le traitement distribué d'une seule requête vers une seule table distribuée. Nous vous recommandons de définir une valeur au moins égale au nombre de serveurs dans le cluster.
-
-Valeur par défaut: 1024.
-
-Les paramètres suivants ne sont utilisés que lors de la création de tables distribuées (et lors du lancement d'un serveur), il n'y a donc aucune raison de les modifier lors de l'exécution.
-
-## distributed_connections_pool_size {#distributed-connections-pool-size}
-
-Nombre maximal de connexions simultanées avec des serveurs distants pour le traitement distribué de toutes les requêtes vers une seule table distribuée. Nous vous recommandons de définir une valeur au moins égale au nombre de serveurs dans le cluster.
-
-Valeur par défaut: 1024.
-
-## connect_timeout_with_failover_ms {#connect-timeout-with-failover-ms}
-
-Délai d'attente en millisecondes pour la connexion à un serveur distant pour un moteur de table distribué, si ‘shard’ et ‘replica’ les sections sont utilisées dans la définition du cluster.
-En cas d'échec, plusieurs tentatives sont faites pour se connecter à diverses répliques.
-
-Valeur par défaut: 50.
-
-## connections_with_failover_max_tries {#connections-with-failover-max-tries}
-
-Nombre maximal de tentatives de connexion avec chaque réplique pour le moteur de table distribué.
-
-Valeur par défaut: 3.
-
-## extrême {#extremes}
-
-Indique s'il faut compter les valeurs extrêmes (les minimums et les maximums dans les colonnes d'un résultat de requête). Accepte 0 ou 1. Par défaut, 0 (désactivé).
-Pour plus d'informations, consultez la section “Extreme values”.
-
-## use_uncompressed_cache {#setting-use_uncompressed_cache}
-
-Indique s'il faut utiliser un cache de blocs non compressés. Accepte 0 ou 1. Par défaut, 0 (désactivé).
-L'utilisation du cache non compressé (uniquement pour les tables de la famille MergeTree) peut réduire considérablement la latence et augmenter le débit lorsque vous travaillez avec un grand nombre de requêtes courtes. Activez ce paramètre pour les utilisateurs qui envoient des requêtes courtes fréquentes. Faites également attention à la [uncompressed_cache_size](../server-configuration-parameters/settings.md#server-settings-uncompressed_cache_size) configuration parameter (only set in the config file) – the size of uncompressed cache blocks. By default, it is 8 GiB. The uncompressed cache is filled in as needed and the least-used data is automatically deleted.
-
-Pour les requêtes qui lisent au moins un volume de données assez important (un million de lignes ou plus), le cache non compressé est désactivé automatiquement pour économiser de l'espace pour les requêtes vraiment petites. Cela signifie que vous pouvez garder la ‘use_uncompressed_cache’ toujours la valeur 1.
-
-## replace_running_query {#replace-running-query}
-
-Lors de l'utilisation de L'interface HTTP, le ‘query_id’ le paramètre peut être passé. C'est n'importe quelle chaîne qui sert d'Identificateur de requête.
-Si une requête d'un utilisateur avec le même ‘query_id’ il existe déjà à ce moment, le comportement dépend de la ‘replace_running_query’ paramètre.
-
-`0` (default) – Throw an exception (don't allow the query to run if a query with the same ‘query_id’ est déjà en cours d'exécution).
-
-`1` – Cancel the old query and start running the new one.
-
-Yandex.Metrica utilise ce paramètre défini sur 1 pour implémenter des suggestions de conditions de segmentation. Après avoir entré le caractère suivant, si l'ancienne requête n'est pas encore terminée, elle doit être annulée.
-
-## stream_flush_interval_ms {#stream-flush-interval-ms}
-
-Fonctionne pour les tables avec des flux dans le cas d'une expiration, ou lorsqu'un thread génère [max_insert_block_size](#settings-max_insert_block_size) rangée.
-
-La valeur par défaut est 7500.
-
-Plus la valeur est petite, plus les données sont vidées dans la table. Régler la valeur trop faible entraîne de mauvaises performances.
-
-## équilibrage {#settings-load_balancing}
-
-Spécifie l'algorithme de sélection des réplicas utilisé pour le traitement des requêtes distribuées.
-
-ClickHouse prend en charge les algorithmes suivants de choix des répliques:
-
--   [Aléatoire](#load_balancing-random) (par défaut)
--   [Nom d'hôte le plus proche](#load_balancing-nearest_hostname)
--   [Afin](#load_balancing-in_order)
--   [Premier ou aléatoire](#load_balancing-first_or_random)
-
-### Aléatoire (par défaut) {#load_balancing-random}
-
-``` sql
-load_balancing = random
-```
-
-Le nombre d'erreurs est compté pour chaque réplique. La requête est envoyée au réplica avec le moins d'erreurs, et s'il y en a plusieurs, à n'importe qui d'entre eux.
-Inconvénients: la proximité du serveur n'est pas prise en compte; si les répliques ont des données différentes, vous obtiendrez également des données différentes.
-
-### Nom D'Hôte Le Plus Proche {#load_balancing-nearest_hostname}
-
-``` sql
-load_balancing = nearest_hostname
-```
-
-The number of errors is counted for each replica. Every 5 minutes, the number of errors is integrally divided by 2. Thus, the number of errors is calculated for a recent time with exponential smoothing. If there is one replica with a minimal number of errors (i.e. errors occurred recently on the other replicas), the query is sent to it. If there are multiple replicas with the same minimal number of errors, the query is sent to the replica with a hostname that is most similar to the server's hostname in the config file (for the number of different characters in identical positions, up to the minimum length of both hostnames).
-
-Par exemple, exemple01-01-1 et example01-01-2.yandex.ru sont différents dans une position, tandis que l'exemple01-01-1 et l'exemple01-02-2 diffèrent dans deux endroits.
-Cette méthode peut sembler primitive, mais elle ne nécessite pas de données externes sur la topologie du réseau, et elle ne compare pas les adresses IP, ce qui serait compliqué pour nos adresses IPv6.
-
-Ainsi, s'il existe des répliques équivalentes, la plus proche par son nom est préférée.
-Nous pouvons également supposer que lors de l'envoi d'une requête au même serveur, en l'absence d'Échecs, une requête distribuée ira également aux mêmes serveurs. Ainsi, même si des données différentes sont placées sur les répliques, la requête retournera principalement les mêmes résultats.
-
-### Afin {#load_balancing-in_order}
-
-``` sql
-load_balancing = in_order
-```
-
-Répliques avec le même nombre d'erreurs sont accessibles dans le même ordre qu'ils sont définis dans la configuration.
-Cette méthode est appropriée lorsque vous savez exactement quelle réplique est préférable.
-
-### Premier ou aléatoire {#load_balancing-first_or_random}
-
-``` sql
-load_balancing = first_or_random
-```
-
-Cet algorithme choisit la première réplique de l'ensemble ou une réplique aléatoire si la première n'est pas disponible. Il est efficace dans les configurations de topologie de réplication croisée, mais inutile dans d'autres configurations.
-
-Le `first_or_random` algorithme résout le problème de la `in_order` algorithme. Avec `in_order`, si une réplique tombe en panne, la suivante obtient une double charge tandis que les répliques restantes gèrent la quantité habituelle de trafic. Lors de l'utilisation de la `first_or_random` algorithme, la charge est répartie uniformément entre les répliques qui sont encore disponibles.
-
-## prefer_localhost_replica {#settings-prefer-localhost-replica}
-
-Active / désactive préférable d'utiliser le réplica localhost lors du traitement des requêtes distribuées.
-
-Valeurs possibles:
-
--   1 — ClickHouse always sends a query to the localhost replica if it exists.
--   0 — ClickHouse uses the balancing strategy specified by the [équilibrage](#settings-load_balancing) paramètre.
-
-Valeur par défaut: 1.
-
-!!! warning "Avertissement"
-    Désactivez ce paramètre si vous utilisez [max_parallel_replicas](#settings-max_parallel_replicas).
-
-## totals_mode {#totals-mode}
-
-Comment calculer les totaux lorsque HAVING est présent, ainsi que lorsque max_rows_to_group_by et group_by_overflow_mode = ‘any’ sont présents.
-Voir la section “WITH TOTALS modifier”.
-
-## totals_auto_threshold {#totals-auto-threshold}
-
-Le seuil de `totals_mode = 'auto'`.
-Voir la section “WITH TOTALS modifier”.
-
-## max_parallel_replicas {#settings-max_parallel_replicas}
-
-Nombre maximal de répliques pour chaque fragment lors de l'exécution d'une requête.
-Par souci de cohérence (pour obtenir différentes parties du même partage de données), Cette option ne fonctionne que lorsque la clé d'échantillonnage est définie.
-Le retard de réplique n'est pas contrôlé.
-
-## compiler {#compile}
-
-Activer la compilation des requêtes. Par défaut, 0 (désactivé).
-
-La compilation n'est utilisée que pour une partie du pipeline de traitement des requêtes: pour la première étape de l'agrégation (GROUP BY).
-Si cette partie du pipeline a été compilée, la requête peut s'exécuter plus rapidement en raison du déploiement de cycles courts et des appels de fonction d'agrégation intégrés. L'amélioration maximale des performances (jusqu'à quatre fois plus rapide dans de rares cas) est observée pour les requêtes avec plusieurs fonctions d'agrégat simples. Typiquement, le gain de performance est insignifiant. Dans de très rares cas, il peut ralentir l'exécution de la requête.
-
-## min_count_to_compile {#min-count-to-compile}
-
-Combien de fois utiliser potentiellement un morceau de code compilé avant d'exécuter la compilation. Par défaut, 3.
-For testing, the value can be set to 0: compilation runs synchronously and the query waits for the end of the compilation process before continuing execution. For all other cases, use values ​​starting with 1. Compilation normally takes about 5-10 seconds.
-Si la valeur est 1 ou plus, la compilation se produit de manière asynchrone dans un thread séparé. Le résultat sera utilisé dès qu'il sera prêt, y compris les requêtes en cours d'exécution.
-
-Le code compilé est requis pour chaque combinaison différente de fonctions d'agrégat utilisées dans la requête et le type de clés dans la clause GROUP BY.
-The results of the compilation are saved in the build directory in the form of .so files. There is no restriction on the number of compilation results since they don't use very much space. Old results will be used after server restarts, except in the case of a server upgrade – in this case, the old results are deleted.
-
-## output_format_json_quote_64bit_integers {#session_settings-output_format_json_quote_64bit_integers}
-
-Si la valeur est true, les entiers apparaissent entre guillemets lors de l'utilisation des formats JSON\* Int64 et UInt64 (pour la compatibilité avec la plupart des implémentations JavaScript); sinon, les entiers sont sortis sans les guillemets.
-
-## format_csv_delimiter {#settings-format_csv_delimiter}
-
-Caractère interprété comme un délimiteur dans les données CSV. Par défaut, le délimiteur est `,`.
-
-## input_format_csv_unquoted_null_literal_as_null {#settings-input_format_csv_unquoted_null_literal_as_null}
-
-Pour le format D'entrée CSV active ou désactive l'analyse des `NULL` comme littéral (synonyme de `\N`).
-
-## output_format_csv_crlf_end_of_line {#settings-output-format-csv-crlf-end-of-line}
-
-Utilisez le séparateur de ligne de style DOS/Windows (CRLF) en CSV au lieu du style Unix (LF).
-
-## output_format_tsv_crlf_end_of_line {#settings-output-format-tsv-crlf-end-of-line}
-
-Utilisez le séparateur de ligne de style DOC/Windows (CRLF) dans TSV au lieu du style Unix (LF).
-
-## insert_quorum {#settings-insert_quorum}
-
-Active les Écritures de quorum.
-
--   Si `insert_quorum < 2`, les Écritures de quorum sont désactivées.
--   Si `insert_quorum >= 2`, les Écritures de quorum sont activées.
-
-Valeur par défaut: 0.
-
-Quorum écrit
-
-`INSERT` ne réussit que lorsque ClickHouse parvient à écrire correctement les données `insert_quorum` des répliques au cours de la `insert_quorum_timeout`. Si, pour une raison quelconque, le nombre de répliques avec succès écrit n'atteint pas le `insert_quorum`, l'écriture est considérée comme ayant échoué et ClickHouse supprimera le bloc inséré de toutes les répliques où les données ont déjà été écrites.
-
-Toutes les répliques du quorum sont cohérentes, c'est-à-dire qu'elles contiennent des données de toutes les `INSERT` requête. Le `INSERT` la séquence est linéarisé.
-
-Lors de la lecture des données écrites à partir du `insert_quorum`, vous pouvez utiliser le [select_sequential_consistency](#settings-select_sequential_consistency) option.
-
-Clickhouse génère une exception
-
--   Si le nombre de répliques au moment de la requête est inférieure à la `insert_quorum`.
--   Lors d'une tentative d'écriture de données lorsque le bloc précédent n'a pas encore été inséré dans le `insert_quorum` des répliques. Cette situation peut se produire si l'utilisateur tente d'effectuer une `INSERT` avant le précédent avec le `insert_quorum` est terminé.
-
-Voir aussi:
-
--   [insert_quorum_timeout](#settings-insert_quorum_timeout)
--   [select_sequential_consistency](#settings-select_sequential_consistency)
-
-## insert_quorum_timeout {#settings-insert_quorum_timeout}
-
-Ecrire dans quorum timeout en secondes. Si le délai d'attente est passé et qu'aucune écriture n'a encore eu lieu, ClickHouse génère une exception et le client doit répéter la requête pour écrire le même bloc dans le même réplica ou tout autre réplica.
-
-Valeur par défaut: 60 secondes.
-
-Voir aussi:
-
--   [insert_quorum](#settings-insert_quorum)
--   [select_sequential_consistency](#settings-select_sequential_consistency)
-
-## select_sequential_consistency {#settings-select_sequential_consistency}
-
-Active ou désactive la cohérence séquentielle pour `SELECT` requête:
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 0.
-
-Utilisation
-
-Lorsque la cohérence séquentielle est activée, ClickHouse permet au client d'exécuter `SELECT` requête uniquement pour les répliques qui contiennent des données de toutes les `INSERT` requêtes exécutées avec `insert_quorum`. Si le client fait référence à une réplique partielle, ClickHouse génère une exception. La requête SELECT n'inclut pas les données qui n'ont pas encore été écrites dans le quorum des répliques.
-
-Voir aussi:
-
--   [insert_quorum](#settings-insert_quorum)
--   [insert_quorum_timeout](#settings-insert_quorum_timeout)
-
-## insert_deduplicate {#settings-insert-deduplicate}
-
-Active ou désactive la déduplication des blocs `INSERT` (Répliqués\* les tableaux).
-
-Valeurs possibles:
-
--   0 — Disabled.
--   1 — Enabled.
-
-Valeur par défaut: 1.
-
-Par défaut, les blocs insérés dans les tables répliquées `INSERT` déclaration sont dédupliquées (voir [Réplication Des Données](../../engines/table-engines/mergetree-family/replication.md)).
-
-## déduplicate_blocks_in_dependent_materialized_views {#settings-deduplicate-blocks-in-dependent-materialized-views}
-
-Active ou désactive la vérification de déduplication des vues matérialisées qui reçoivent des données à partir de tables\* répliquées.
-
-Valeurs possibles:
-
-      0 — Disabled.
-      1 — Enabled.
-
-Valeur par défaut: 0.
-
-Utilisation
-
-Par défaut, la déduplication n'est pas effectuée pour les vues matérialisées mais en amont, dans la table source.
-Si un bloc inséré est ignoré en raison de la déduplication dans la table source, il n'y aura pas d'insertion dans les vues matérialisées attachées. Ce comportement existe pour permettre l'insertion de données hautement agrégées dans des vues matérialisées, dans les cas où les blocs insérés sont les mêmes après l'agrégation de vues matérialisées mais dérivés de différentes insertions dans la table source.
-Dans le même temps, ce comportement “breaks” `INSERT` idempotence. Si un `INSERT` dans la table principale a été un succès et `INSERT` into a materialized view failed (e.g. because of communication failure with Zookeeper) a client will get an error and can retry the operation. However, the materialized view won't receive the second insert because it will be discarded by deduplication in the main (source) table. The setting `deduplicate_blocks_in_dependent_materialized_views` permet de changer ce comportement. Lors d'une nouvelle tentative, une vue matérialisée recevra l'insertion répétée et effectuera une vérification de déduplication par elle-même,
-ignorant le résultat de la vérification pour la table source, et insérera des lignes perdues en raison de la première défaillance.
-
-## max_network_bytes {#settings-max-network-bytes}
-
-Limite le volume de données (en octets) qui est reçu ou transmis sur le réseau lors de l'exécution d'une requête. Ce paramètre s'applique à chaque individu requête.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Data volume control is disabled.
-
-Valeur par défaut: 0.
-
-## max_network_bandwidth {#settings-max-network-bandwidth}
-
-Limite la vitesse de l'échange de données sur le réseau en octets par seconde. Ce paramètre s'applique à toutes les requêtes.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Bandwidth control is disabled.
-
-Valeur par défaut: 0.
-
-## max_network_bandwidth_for_user {#settings-max-network-bandwidth-for-user}
-
-Limite la vitesse de l'échange de données sur le réseau en octets par seconde. Ce paramètre s'applique à toutes les requêtes exécutées simultanément par un seul utilisateur.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Control of the data speed is disabled.
-
-Valeur par défaut: 0.
-
-## max_network_bandwidth_for_all_users {#settings-max-network-bandwidth-for-all-users}
-
-Limite la vitesse à laquelle les données sont échangées sur le réseau en octets par seconde. Ce paramètre s'applique à toutes les requêtes exécutées simultanément sur le serveur.
-
-Valeurs possibles:
-
--   Entier positif.
--   0 — Control of the data speed is disabled.
-
-Valeur par défaut: 0.
-
-## count_distinct_implementation {#settings-count_distinct_implementation}
-
-Spécifie de l' `uniq*` les fonctions doivent être utilisées pour [COUNT(DISTINCT …)](../../sql-reference/aggregate-functions/reference.md#agg_function-count) construction.
-
-Valeurs possibles:
-
--   [uniq](../../sql-reference/aggregate-functions/reference.md#agg_function-uniq)
--   [uniqcombiné](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqcombined)
--   [uniqCombined64](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqcombined64)
--   [uniqHLL12](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqhll12)
--   [uniqExact](../../sql-reference/aggregate-functions/reference.md#agg_function-uniqexact)
-
-Valeur par défaut: `uniqExact`.
-
-## skip_unavailable_shards {#settings-skip_unavailable_shards}
-
-Active ou désactive le saut silencieux des fragments indisponibles.
-
-Tesson est considéré comme indisponible si toutes ses répliques ne sont pas disponibles. Une réplique n'est pas disponible dans les cas suivants:
-
--   ClickHouse ne peut pas se connecter à la réplique pour une raison quelconque.
-
-    Lors de la connexion à une réplique, ClickHouse effectue plusieurs tentatives. Si toutes ces tentatives échouent, la réplique est considéré comme indisponible.
-
--   La réplique ne peut pas être résolue via le DNS.
-
-    Si le nom d'hôte du réplica ne peut pas être résolu via DNS, il peut indiquer les situations suivantes:
-
-    -   L'hôte de la réplique n'a pas d'enregistrement DNS. Il peut se produire dans les systèmes avec DNS dynamique, par exemple, [Kubernetes](https://kubernetes.io), où les nœuds peuvent être insolubles pendant les temps d'arrêt, et ce n'est pas une erreur.
-
-    -   Erreur de Configuration. Le fichier de configuration ClickHouse contient un mauvais nom d'hôte.
-
-Valeurs possibles:
-
--   1 — skipping enabled.
-
-    Si un fragment n'est pas disponible, ClickHouse renvoie un résultat basé sur des données partielles et ne signale pas les problèmes de disponibilité des nœuds.
-
--   0 — skipping disabled.
-
-    Si un fragment n'est pas disponible, ClickHouse lève une exception.
-
-Valeur par défaut: 0.
-
-## optimize_skip_unused_shards {#settings-optimize_skip_unused_shards}
-
-Active ou désactive le saut des fragments inutilisés pour les requêtes SELECT qui ont la condition de clé de sharding dans PREWHERE/WHERE (suppose que les données sont distribuées par la clé de sharding, sinon ne rien faire).
-
-Valeur par défaut: 0
-
-## force_optimize_skip_unused_shards {#settings-force_optimize_skip_unused_shards}
-
-Active ou désactive l'exécution de la requête si [`optimize_skip_unused_shards`](#settings-optimize_skip_unused_shards) activé et sauter des fragments inutilisés n'est pas possible. Si le saut n'est pas possible et le paramètre est activé, une exception sera levée.
-
-Valeurs possibles:
-
--   0 - Désactivé (ne jette)
--   1-Désactiver l'exécution de la requête uniquement si la table a une clé de sharding
--   2-Désactiver l'exécution de la requête quelle que soit la clé de sharding est définie pour la table
-
-Valeur par défaut: 0
-
-## optimize_throw_if_noop {#setting-optimize_throw_if_noop}
-
-Active ou désactive le lancement d'une exception si [OPTIMIZE](../../sql-reference/statements/misc.md#misc_operations-optimize) la requête n'a pas effectué de fusion.
-
-Par défaut, `OPTIMIZE` retourne avec succès même s'il n'a rien fait. Ce paramètre vous permet de différencier ces situations et d'obtenir la raison dans un message d'exception.
-
-Valeurs possibles:
-
--   1 — Throwing an exception is enabled.
--   0 — Throwing an exception is disabled.
-
-Valeur par défaut: 0.
-
-## distributed_replica_error_half_life {#settings-distributed_replica_error_half_life}
-
--   Type: secondes
--   Valeur par défaut: 60 secondes
-
-Contrôle la vitesse à laquelle les erreurs dans les tables distribuées sont mises à zéro. Si une réplique est indisponible pendant un certain temps, accumule 5 erreurs et distributed_replica_error_half_life est défini sur 1 seconde, la réplique est considérée comme normale 3 secondes après la dernière erreur.
-
-Voir aussi:
-
--   [Tableau moteur Distribués](../../engines/table-engines/special/distributed.md)
--   [distributed_replica_error_cap](#settings-distributed_replica_error_cap)
-
-## distributed_replica_error_cap {#settings-distributed_replica_error_cap}
-
--   Type: unsigned int
--   Valeur par défaut: 1000
-
-Le nombre d'erreurs de chaque réplique est plafonné à cette valeur, empêchant une seule réplique d'accumuler trop d'erreurs.
-
-Voir aussi:
-
--   [Tableau moteur Distribués](../../engines/table-engines/special/distributed.md)
--   [distributed_replica_error_half_life](#settings-distributed_replica_error_half_life)
-
-## distributed_directory_monitor_sleep_time_ms {#distributed_directory_monitor_sleep_time_ms}
-
-Intervalle de Base pour le [Distribué](../../engines/table-engines/special/distributed.md) tableau moteur à envoyer des données. L'intervalle réel augmente de façon exponentielle en cas d'erreurs.
-
-Valeurs possibles:
-
--   Un nombre entier positif de millisecondes.
-
-Valeur par défaut: 100 millisecondes.
-
-## distributed_directory_monitor_max_sleep_time_ms {#distributed_directory_monitor_max_sleep_time_ms}
-
-Intervalle maximal pour le [Distribué](../../engines/table-engines/special/distributed.md) tableau moteur à envoyer des données. Limite la croissance exponentielle de l'intervalle défini dans [distributed_directory_monitor_sleep_time_ms](#distributed_directory_monitor_sleep_time_ms) paramètre.
-
-Valeurs possibles:
-
--   Un nombre entier positif de millisecondes.
-
-Valeur par défaut: 30000 millisecondes (30 secondes).
-
-## distributed_directory_monitor_batch_inserts {#distributed_directory_monitor_batch_inserts}
-
-Active / désactive l'envoi des données insérées par lots.
-
-Lorsque l'envoi par lots est activé, le [Distribué](../../engines/table-engines/special/distributed.md) tableau moteur essaie d'envoyer plusieurs fichiers de données insérées dans une seule opération au lieu de les envoyer séparément. L'envoi par lots améliore les performances du cluster en utilisant mieux les ressources du serveur et du réseau.
-
-Valeurs possibles:
-
--   1 — Enabled.
--   0 — Disabled.
-
-Valeur par défaut: 0.
-
-## os_thread_priority {#setting-os-thread-priority}
-
-Définit la priorité ([beau](https://en.wikipedia.org/wiki/Nice_(Unix))) pour les threads qui exécutent des requêtes. Le planificateur du système d'exploitation considère cette priorité lors du choix du prochain thread à exécuter sur chaque noyau CPU disponible.
-
-!!! warning "Avertissement"
-    Pour utiliser ce paramètre, vous devez définir l' `CAP_SYS_NICE` capacité. Le `clickhouse-server` paquet configure lors de l'installation. Certains environnements virtuels ne vous permettent pas de définir `CAP_SYS_NICE` capacité. Dans ce cas, `clickhouse-server` affiche un message à ce sujet au début.
-
-Valeurs possibles:
-
--   Vous pouvez définir des valeurs dans la gamme `[-20, 19]`.
-
-Des valeurs plus faibles signifient une priorité plus élevée. Les discussions avec des bas `nice` les valeurs de priorité sont effectués plus fréquemment que les discussions avec des valeurs élevées. Les valeurs élevées sont préférables pour les requêtes non interactives de longue durée, car elles leur permettent d'abandonner rapidement des ressources au profit de requêtes interactives courtes lorsqu'elles arrivent.
-
-Valeur par défaut: 0.
-
-## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}
-
-Définit la période pour une horloge réelle de la [requête profiler](../../operations/optimizing-performance/sampling-query-profiler.md). La vraie minuterie d'horloge compte le temps d'horloge murale.
-
-Valeurs possibles:
-
--   Nombre entier positif, en nanosecondes.
-
-    Valeurs recommandées:
-
-            - 10000000 (100 times a second) nanoseconds and less for single queries.
-            - 1000000000 (once a second) for cluster-wide profiling.
-
--   0 pour éteindre la minuterie.
-
-Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-Valeur par défaut: 1000000000 nanosecondes (une fois par seconde).
-
-Voir aussi:
-
--   Système de table [trace_log](../../operations/system-tables.md#system_tables-trace_log)
-
-## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns}
-
-Définit la période pour une minuterie D'horloge CPU du [requête profiler](../../operations/optimizing-performance/sampling-query-profiler.md). Cette minuterie ne compte que le temps CPU.
-
-Valeurs possibles:
-
--   Un nombre entier positif de nanosecondes.
-
-    Valeurs recommandées:
-
-            - 10000000 (100 times a second) nanoseconds and more for single queries.
-            - 1000000000 (once a second) for cluster-wide profiling.
-
--   0 pour éteindre la minuterie.
-
-Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-Valeur par défaut: 1000000000 nanosecondes.
-
-Voir aussi:
-
--   Système de table [trace_log](../../operations/system-tables.md#system_tables-trace_log)
-
-## allow_introspection_functions {#settings-allow_introspection_functions}
-
-Active des désactive [obscures fonctions](../../sql-reference/functions/introspection.md) pour le profilage de requête.
-
-Valeurs possibles:
-
--   1 — Introspection functions enabled.
--   0 — Introspection functions disabled.
-
-Valeur par défaut: 0.
-
-**Voir Aussi**
-
--   [Échantillonnage Du Profileur De Requête](../optimizing-performance/sampling-query-profiler.md)
--   Système de table [trace_log](../../operations/system-tables.md#system_tables-trace_log)
-
-## input_format_parallel_parsing {#input-format-parallel-parsing}
-
--   Type: bool
--   Valeur par défaut: True
-
-Activer l'analyse parallèle des formats de données en préservant l'ordre. Pris en charge uniquement pour les formats TSV, TKSV, CSV et jsoneachrow.
-
-## min_chunk_bytes_for_parallel_parsing {#min-chunk-bytes-for-parallel-parsing}
-
--   Type: unsigned int
--   Valeur par défaut: 1 MiB
-
-La taille minimale du bloc en octets, que chaque thread analysera en parallèle.
-
-## output_format_avro_codec {#settings-output_format_avro_codec}
-
-Définit le codec de compression utilisé pour le fichier Avro de sortie.
-
-Type: string
-
-Valeurs possibles:
-
--   `null` — No compression
--   `deflate` — Compress with Deflate (zlib)
--   `snappy` — Compress with [Hargneux](https://google.github.io/snappy/)
-
-Valeur par défaut: `snappy` (si disponible) ou `deflate`.
-
-## output_format_avro_sync_interval {#settings-output_format_avro_sync_interval}
-
-Définit la taille minimale des données (en octets) entre les marqueurs de synchronisation pour le fichier Avro de sortie.
-
-Type: unsigned int
-
-Valeurs possibles: 32 (32 octets) - 1073741824 (1 GiB)
-
-Valeur par défaut: 32768 (32 Ko)
-
-## format_avro_schema_registry_url {#settings-format_avro_schema_registry_url}
-
-Définit L'URL de Registre de schéma Confluent à utiliser avec [AvroConfluent](../../interfaces/formats.md#data-format-avro-confluent) format
-
-Type: URL
-
-Valeur par défaut: vide
-
-## background_pool_size {#background_pool_size}
-
-Définit le nombre de threads effectuant des opérations d'arrière-plan dans les moteurs de table (par exemple, fusionne dans [Moteur MergeTree](../../engines/table-engines/mergetree-family/index.md) table). Ce paramètre est appliqué au démarrage du serveur ClickHouse et ne peut pas être modifié dans une session utilisateur. En ajustant ce paramètre, vous gérez la charge du processeur et du disque. Une taille de pool plus petite utilise moins de ressources CPU et disque, mais les processus d'arrière-plan avancent plus lentement, ce qui pourrait éventuellement avoir un impact sur les performances des requêtes.
-
-Valeurs possibles:
-
--   Tout nombre entier positif.
-
-Valeur par défaut: 16.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/settings/settings/) <!-- hide -->
diff --git a/docs/fr/operations/system-tables.md b/docs/fr/operations/system-tables.md
deleted file mode 100644
index bf8758924789..000000000000
--- a/docs/fr/operations/system-tables.md
+++ /dev/null
@@ -1,1168 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 52
-toc_title: "Les Tables Syst\xE8me"
----
-
-# Les Tables Système {#system-tables}
-
-Les tables système sont utilisées pour implémenter une partie des fonctionnalités du système et pour fournir un accès à des informations sur le fonctionnement du système.
-Vous ne pouvez pas supprimer une table système (mais vous pouvez effectuer un détachement).
-Les tables système n'ont pas de fichiers avec des données sur le disque ou de fichiers avec des métadonnées. Le serveur crée toutes les tables système au démarrage.
-Les tables système sont en lecture seule.
-Ils sont situés dans la ‘system’ la base de données.
-
-## système.asynchronous_metrics {#system_tables-asynchronous_metrics}
-
-Contient des mesures qui sont calculées périodiquement en arrière-plan. Par exemple, la quantité de RAM utilisée.
-
-Colonne:
-
--   `metric` ([Chaîne](../sql-reference/data-types/string.md)) — Metric name.
--   `value` ([Float64](../sql-reference/data-types/float.md)) — Metric value.
-
-**Exemple**
-
-``` sql
-SELECT * FROM system.asynchronous_metrics LIMIT 10
-```
-
-``` text
-┌─metric──────────────────────────────────┬──────value─┐
-│ jemalloc.background_thread.run_interval │          0 │
-│ jemalloc.background_thread.num_runs     │          0 │
-│ jemalloc.background_thread.num_threads  │          0 │
-│ jemalloc.retained                       │  422551552 │
-│ jemalloc.mapped                         │ 1682989056 │
-│ jemalloc.resident                       │ 1656446976 │
-│ jemalloc.metadata_thp                   │          0 │
-│ jemalloc.metadata                       │   10226856 │
-│ UncompressedCacheCells                  │          0 │
-│ MarkCacheFiles                          │          0 │
-└─────────────────────────────────────────┴────────────┘
-```
-
-**Voir Aussi**
-
--   [Surveiller](monitoring.md) — Base concepts of ClickHouse monitoring.
--   [système.métrique](#system_tables-metrics) — Contains instantly calculated metrics.
--   [système.événement](#system_tables-events) — Contains a number of events that have occurred.
--   [système.metric_log](#system_tables-metric_log) — Contains a history of metrics values from tables `system.metrics` и `system.events`.
-
-## système.cluster {#system-clusters}
-
-Contient des informations sur les clusters disponibles dans le fichier de configuration et les serveurs en eux.
-
-Colonne:
-
--   `cluster` (String) — The cluster name.
--   `shard_num` (UInt32) — The shard number in the cluster, starting from 1.
--   `shard_weight` (UInt32) — The relative weight of the shard when writing data.
--   `replica_num` (UInt32) — The replica number in the shard, starting from 1.
--   `host_name` (String) — The host name, as specified in the config.
--   `host_address` (String) — The host IP address obtained from DNS.
--   `port` (UInt16) — The port to use for connecting to the server.
--   `user` (String) — The name of the user for connecting to the server.
--   `errors_count` (UInt32) - nombre de fois que cet hôte n'a pas atteint le réplica.
--   `estimated_recovery_time` (UInt32) - secondes restantes jusqu'à ce que le nombre d'erreurs de réplique soit remis à zéro et qu'il soit considéré comme revenu à la normale.
-
-Veuillez noter que `errors_count` est mise à jour une fois par requête à la grappe, mais `estimated_recovery_time` est recalculé sur-demande. Il pourrait donc y avoir un cas de non-zéro `errors_count` et zéro `estimated_recovery_time`, cette requête suivante sera nulle `errors_count` et essayez d'utiliser des répliques comme si elle ne comporte pas d'erreurs.
-
-**Voir aussi**
-
--   [Tableau moteur Distribués](../engines/table-engines/special/distributed.md)
--   [paramètre distributed_replica_error_cap](settings/settings.md#settings-distributed_replica_error_cap)
--   [paramètre distributed_replica_error_half_life](settings/settings.md#settings-distributed_replica_error_half_life)
-
-## système.colonne {#system-columns}
-
-Contient des informations sur les colonnes de toutes les tables.
-
-Vous pouvez utiliser ce tableau pour obtenir des informations similaires à l' [DESCRIBE TABLE](../sql-reference/statements/misc.md#misc-describe-table) requête, mais pour plusieurs tables à la fois.
-
-Le `system.columns` le tableau contient les colonnes suivantes (la colonne type est indiqué entre parenthèses):
-
--   `database` (String) — Database name.
--   `table` (String) — Table name.
--   `name` (String) — Column name.
--   `type` (String) — Column type.
--   `default_kind` (String) — Expression type (`DEFAULT`, `MATERIALIZED`, `ALIAS`) pour la valeur par défaut, ou une chaîne vide si elle n'est pas définie.
--   `default_expression` (String) — Expression for the default value, or an empty string if it is not defined.
--   `data_compressed_bytes` (UInt64) — The size of compressed data, in bytes.
--   `data_uncompressed_bytes` (UInt64) — The size of decompressed data, in bytes.
--   `marks_bytes` (UInt64) — The size of marks, in bytes.
--   `comment` (String) — Comment on the column, or an empty string if it is not defined.
--   `is_in_partition_key` (UInt8) — Flag that indicates whether the column is in the partition expression.
--   `is_in_sorting_key` (UInt8) — Flag that indicates whether the column is in the sorting key expression.
--   `is_in_primary_key` (UInt8) — Flag that indicates whether the column is in the primary key expression.
--   `is_in_sampling_key` (UInt8) — Flag that indicates whether the column is in the sampling key expression.
-
-## système.contributeur {#system-contributors}
-
-Contient des informations sur les donateurs. Tous les constributors dans un ordre aléatoire. L'ordre est aléatoire au moment de l'exécution de la requête.
-
-Colonne:
-
--   `name` (String) — Contributor (author) name from git log.
-
-**Exemple**
-
-``` sql
-SELECT * FROM system.contributors LIMIT 10
-```
-
-``` text
-┌─name─────────────┐
-│ Olga Khvostikova │
-│ Max Vetrov       │
-│ LiuYangkuan      │
-│ svladykin        │
-│ zamulla          │
-│ Šimon Podlipský  │
-│ BayoNet          │
-│ Ilya Khomutov    │
-│ Amy Krishnevsky  │
-│ Loud_Scream      │
-└──────────────────┘
-```
-
-Trouver vous-même dans le tableau, utilisez une requête:
-
-``` sql
-SELECT * FROM system.contributors WHERE name='Olga Khvostikova'
-```
-
-``` text
-┌─name─────────────┐
-│ Olga Khvostikova │
-└──────────────────┘
-```
-
-## système.les bases de données {#system-databases}
-
-Cette table contient une seule colonne de chaîne appelée ‘name’ – the name of a database.
-Chaque base de données que le serveur connaît a une entrée correspondante dans la table.
-Cette table système est utilisée pour implémenter `SHOW DATABASES` requête.
-
-## système.detached_parts {#system_tables-detached_parts}
-
-Contient des informations sur les pièces détachées de [MergeTree](../engines/table-engines/mergetree-family/mergetree.md) table. Le `reason` colonne spécifie pourquoi la pièce a été détachée. Pour les pièces détachées par l'utilisateur, la raison est vide. De telles pièces peuvent être attachées avec [ALTER TABLE ATTACH PARTITION\|PART](../sql-reference/statements/alter.md#alter_attach-partition) commande. Pour la description des autres colonnes, voir [système.partie](#system_tables-parts). Si le nom de pièce n'est pas valide, les valeurs de certaines colonnes peuvent être `NULL`. Ces pièces peuvent être supprimés avec [ALTER TABLE DROP DETACHED PART](../sql-reference/statements/alter.md#alter_drop-detached).
-
-## système.dictionnaire {#system_tables-dictionaries}
-
-Contient des informations sur [dictionnaires externes](../sql-reference/dictionaries/external-dictionaries/external-dicts.md).
-
-Colonne:
-
--   `database` ([Chaîne](../sql-reference/data-types/string.md)) — Name of the database containing the dictionary created by DDL query. Empty string for other dictionaries.
--   `name` ([Chaîne](../sql-reference/data-types/string.md)) — [Nom du dictionnaire](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md).
--   `status` ([Enum8](../sql-reference/data-types/enum.md)) — Dictionary status. Possible values:
-    -   `NOT_LOADED` — Dictionary was not loaded because it was not used.
-    -   `LOADED` — Dictionary loaded successfully.
-    -   `FAILED` — Unable to load the dictionary as a result of an error.
-    -   `LOADING` — Dictionary is loading now.
-    -   `LOADED_AND_RELOADING` — Dictionary is loaded successfully, and is being reloaded right now (frequent reasons: [SYSTEM RELOAD DICTIONARY](../sql-reference/statements/system.md#query_language-system-reload-dictionary) requête, délai d'attente, configuration du dictionnaire a changé).
-    -   `FAILED_AND_RELOADING` — Could not load the dictionary as a result of an error and is loading now.
--   `origin` ([Chaîne](../sql-reference/data-types/string.md)) — Path to the configuration file that describes the dictionary.
--   `type` ([Chaîne](../sql-reference/data-types/string.md)) — Type of a dictionary allocation. [Stockage des dictionnaires en mémoire](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md).
--   `key` — [Type de clé](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-key): Touche Numérique ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) or Сomposite key ([Chaîne](../sql-reference/data-types/string.md)) — form “(type 1, type 2, …, type n)”.
--   `attribute.names` ([Tableau](../sql-reference/data-types/array.md)([Chaîne](../sql-reference/data-types/string.md))) — Array of [les noms d'attribut](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes) fournis par le dictionnaire.
--   `attribute.types` ([Tableau](../sql-reference/data-types/array.md)([Chaîne](../sql-reference/data-types/string.md))) — Corresponding array of [les types d'attribut](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes) qui sont fournis par le dictionnaire.
--   `bytes_allocated` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Amount of RAM allocated for the dictionary.
--   `query_count` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Number of queries since the dictionary was loaded or since the last successful reboot.
--   `hit_rate` ([Float64](../sql-reference/data-types/float.md)) — For cache dictionaries, the percentage of uses for which the value was in the cache.
--   `element_count` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Number of items stored in the dictionary.
--   `load_factor` ([Float64](../sql-reference/data-types/float.md)) — Percentage filled in the dictionary (for a hashed dictionary, the percentage filled in the hash table).
--   `source` ([Chaîne](../sql-reference/data-types/string.md)) — Text describing the [source de données](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md) pour le dictionnaire.
--   `lifetime_min` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Minimum [vie](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md) du dictionnaire en mémoire, après quoi ClickHouse tente de recharger le dictionnaire (si `invalidate_query` est définie, alors que si elle a changé). Réglez en quelques secondes.
--   `lifetime_max` ([UInt64](../sql-reference/data-types/int-uint.md#uint-ranges)) — Maximum [vie](../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md) du dictionnaire en mémoire, après quoi ClickHouse tente de recharger le dictionnaire (si `invalidate_query` est définie, alors que si elle a changé). Réglez en quelques secondes.
--   `loading_start_time` ([DateTime](../sql-reference/data-types/datetime.md)) — Start time for loading the dictionary.
--   `last_successful_update_time` ([DateTime](../sql-reference/data-types/datetime.md)) — End time for loading or updating the dictionary. Helps to monitor some troubles with external sources and investigate causes.
--   `loading_duration` ([Float32](../sql-reference/data-types/float.md)) — Duration of a dictionary loading.
--   `last_exception` ([Chaîne](../sql-reference/data-types/string.md)) — Text of the error that occurs when creating or reloading the dictionary if the dictionary couldn't be created.
-
-**Exemple**
-
-Configurez le dictionnaire.
-
-``` sql
-CREATE DICTIONARY dictdb.dict
-(
-    `key` Int64 DEFAULT -1,
-    `value_default` String DEFAULT 'world',
-    `value_expression` String DEFAULT 'xxx' EXPRESSION 'toString(127 * 172)'
-)
-PRIMARY KEY key
-SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'dicttbl' DB 'dictdb'))
-LIFETIME(MIN 0 MAX 1)
-LAYOUT(FLAT())
-```
-
-Assurez-vous que le dictionnaire est chargé.
-
-``` sql
-SELECT * FROM system.dictionaries
-```
-
-``` text
-┌─database─┬─name─┬─status─┬─origin──────┬─type─┬─key────┬─attribute.names──────────────────────┬─attribute.types─────┬─bytes_allocated─┬─query_count─┬─hit_rate─┬─element_count─┬───────────load_factor─┬─source─────────────────────┬─lifetime_min─┬─lifetime_max─┬──loading_start_time─┌──last_successful_update_time─┬──────loading_duration─┬─last_exception─┐
-│ dictdb   │ dict │ LOADED │ dictdb.dict │ Flat │ UInt64 │ ['value_default','value_expression'] │ ['String','String'] │           74032 │           0 │        1 │             1 │ 0.0004887585532746823 │ ClickHouse: dictdb.dicttbl │            0 │            1 │ 2020-03-04 04:17:34 │   2020-03-04 04:30:34        │                 0.002 │                │
-└──────────┴──────┴────────┴─────────────┴──────┴────────┴──────────────────────────────────────┴─────────────────────┴─────────────────┴─────────────┴──────────┴───────────────┴───────────────────────┴────────────────────────────┴──────────────┴──────────────┴─────────────────────┴──────────────────────────────┘───────────────────────┴────────────────┘
-```
-
-## système.événement {#system_tables-events}
-
-Contient des informations sur le nombre d'événements survenus dans le système. Par exemple, dans le tableau, vous pouvez trouver combien `SELECT` les requêtes ont été traitées depuis le démarrage du serveur ClickHouse.
-
-Colonne:
-
--   `event` ([Chaîne](../sql-reference/data-types/string.md)) — Event name.
--   `value` ([UInt64](../sql-reference/data-types/int-uint.md)) — Number of events occurred.
--   `description` ([Chaîne](../sql-reference/data-types/string.md)) — Event description.
-
-**Exemple**
-
-``` sql
-SELECT * FROM system.events LIMIT 5
-```
-
-``` text
-┌─event─────────────────────────────────┬─value─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ Query                                 │    12 │ Number of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries.                  │
-│ SelectQuery                           │     8 │ Same as Query, but only for SELECT queries.                                                                                                                                                                                                                │
-│ FileOpen                              │    73 │ Number of files opened.                                                                                                                                                                                                                                    │
-│ ReadBufferFromFileDescriptorRead      │   155 │ Number of reads (read/pread) from a file descriptor. Does not include sockets.                                                                                                                                                                             │
-│ ReadBufferFromFileDescriptorReadBytes │  9931 │ Number of bytes read from file descriptors. If the file is compressed, this will show the compressed data size.                                                                                                                                              │
-└───────────────────────────────────────┴───────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [système.asynchronous_metrics](#system_tables-asynchronous_metrics) — Contains periodically calculated metrics.
--   [système.métrique](#system_tables-metrics) — Contains instantly calculated metrics.
--   [système.metric_log](#system_tables-metric_log) — Contains a history of metrics values from tables `system.metrics` и `system.events`.
--   [Surveiller](monitoring.md) — Base concepts of ClickHouse monitoring.
-
-## système.fonction {#system-functions}
-
-Contient des informations sur les fonctions normales et agrégées.
-
-Colonne:
-
--   `name`(`String`) – The name of the function.
--   `is_aggregate`(`UInt8`) — Whether the function is aggregate.
-
-## système.graphite_retentions {#system-graphite-retentions}
-
-Contient des informations sur les paramètres [graphite_rollup](server-configuration-parameters/settings.md#server_configuration_parameters-graphite) qui sont utilisés dans les tableaux avec [\* GraphiteMergeTree](../engines/table-engines/mergetree-family/graphitemergetree.md) moteur.
-
-Colonne:
-
--   `config_name` (Chaîne) - `graphite_rollup` nom du paramètre.
--   `regexp` (Chaîne) - un modèle pour le nom de la métrique.
--   `function` (Chaîne) - le nom de la fonction d'agrégation.
--   `age` (UInt64) - l'âge minimum des données en secondes.
--   `precision` (UInt64) - comment définir précisément l'âge des données en secondes.
--   `priority` (UInt16) - priorité de motif.
--   `is_default` (UInt8) - indique si le motif est la valeur par défaut.
--   `Tables.database` (Array (String)) - tableau de noms de tables de base de données qui utilisent `config_name` paramètre.
--   `Tables.table` (Array (String)) - tableau de noms de tables qui utilisent `config_name` paramètre.
-
-## système.fusionner {#system-merges}
-
-Contient des informations sur les fusions et les mutations de pièces actuellement en cours pour les tables de la famille MergeTree.
-
-Colonne:
-
--   `database` (String) — The name of the database the table is in.
--   `table` (String) — Table name.
--   `elapsed` (Float64) — The time elapsed (in seconds) since the merge started.
--   `progress` (Float64) — The percentage of completed work from 0 to 1.
--   `num_parts` (UInt64) — The number of pieces to be merged.
--   `result_part_name` (String) — The name of the part that will be formed as the result of merging.
--   `is_mutation` (UInt8) - 1 si ce processus est une mutation partielle.
--   `total_size_bytes_compressed` (UInt64) — The total size of the compressed data in the merged chunks.
--   `total_size_marks` (UInt64) — The total number of marks in the merged parts.
--   `bytes_read_uncompressed` (UInt64) — Number of bytes read, uncompressed.
--   `rows_read` (UInt64) — Number of rows read.
--   `bytes_written_uncompressed` (UInt64) — Number of bytes written, uncompressed.
--   `rows_written` (UInt64) — Number of rows written.
-
-## système.métrique {#system_tables-metrics}
-
-Contient des mesures qui peuvent être calculées instantanément, ou ont une valeur actuelle. Par exemple, le nombre de requêtes traitées ou en cours réplique de retard. Ce tableau est toujours à jour.
-
-Colonne:
-
--   `metric` ([Chaîne](../sql-reference/data-types/string.md)) — Metric name.
--   `value` ([Int64](../sql-reference/data-types/int-uint.md)) — Metric value.
--   `description` ([Chaîne](../sql-reference/data-types/string.md)) — Metric description.
-
-La liste des mesures que vous pouvez trouver dans le [src / Common / CurrentMetrics.rpc](https://github.com/ClickHouse/ClickHouse/blob/master/src/Common/CurrentMetrics.cpp) fichier source de ClickHouse.
-
-**Exemple**
-
-``` sql
-SELECT * FROM system.metrics LIMIT 10
-```
-
-``` text
-┌─metric─────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ Query                      │     1 │ Number of executing queries                                                                                                                                                                      │
-│ Merge                      │     0 │ Number of executing background merges                                                                                                                                                            │
-│ PartMutation               │     0 │ Number of mutations (ALTER DELETE/UPDATE)                                                                                                                                                        │
-│ ReplicatedFetch            │     0 │ Number of data parts being fetched from replicas                                                                                                                                                │
-│ ReplicatedSend             │     0 │ Number of data parts being sent to replicas                                                                                                                                                      │
-│ ReplicatedChecks           │     0 │ Number of data parts checking for consistency                                                                                                                                                    │
-│ BackgroundPoolTask         │     0 │ Number of active tasks in BackgroundProcessingPool (merges, mutations, fetches, or replication queue bookkeeping)                                                                                │
-│ BackgroundSchedulePoolTask │     0 │ Number of active tasks in BackgroundSchedulePool. This pool is used for periodic ReplicatedMergeTree tasks, like cleaning old data parts, altering data parts, replica re-initialization, etc.   │
-│ DiskSpaceReservedForMerge  │     0 │ Disk space reserved for currently running background merges. It is slightly more than the total size of currently merging parts.                                                                     │
-│ DistributedSend            │     0 │ Number of connections to remote servers sending data that was INSERTed into Distributed tables. Both synchronous and asynchronous mode.                                                          │
-└────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [système.asynchronous_metrics](#system_tables-asynchronous_metrics) — Contains periodically calculated metrics.
--   [système.événement](#system_tables-events) — Contains a number of events that occurred.
--   [système.metric_log](#system_tables-metric_log) — Contains a history of metrics values from tables `system.metrics` и `system.events`.
--   [Surveiller](monitoring.md) — Base concepts of ClickHouse monitoring.
-
-## système.metric_log {#system_tables-metric_log}
-
-Contient l'historique des valeurs de métriques des tables `system.metrics` et `system.events` périodiquement vidé sur le disque.
-Pour activer la collection d'historique des métriques `system.metric_log`, créer `/etc/clickhouse-server/config.d/metric_log.xml` avec le contenu suivant:
-
-``` xml
-<yandex>
-    <metric_log>
-        <database>system</database>
-        <table>metric_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
-    </metric_log>
-</yandex>
-```
-
-**Exemple**
-
-``` sql
-SELECT * FROM system.metric_log LIMIT 1 FORMAT Vertical;
-```
-
-``` text
-Row 1:
-──────
-event_date:                                                 2020-02-18
-event_time:                                                 2020-02-18 07:15:33
-milliseconds:                                               554
-ProfileEvent_Query:                                         0
-ProfileEvent_SelectQuery:                                   0
-ProfileEvent_InsertQuery:                                   0
-ProfileEvent_FileOpen:                                      0
-ProfileEvent_Seek:                                          0
-ProfileEvent_ReadBufferFromFileDescriptorRead:              1
-ProfileEvent_ReadBufferFromFileDescriptorReadFailed:        0
-ProfileEvent_ReadBufferFromFileDescriptorReadBytes:         0
-ProfileEvent_WriteBufferFromFileDescriptorWrite:            1
-ProfileEvent_WriteBufferFromFileDescriptorWriteFailed:      0
-ProfileEvent_WriteBufferFromFileDescriptorWriteBytes:       56
-...
-CurrentMetric_Query:                                        0
-CurrentMetric_Merge:                                        0
-CurrentMetric_PartMutation:                                 0
-CurrentMetric_ReplicatedFetch:                              0
-CurrentMetric_ReplicatedSend:                               0
-CurrentMetric_ReplicatedChecks:                             0
-...
-```
-
-**Voir aussi**
-
--   [système.asynchronous_metrics](#system_tables-asynchronous_metrics) — Contains periodically calculated metrics.
--   [système.événement](#system_tables-events) — Contains a number of events that occurred.
--   [système.métrique](#system_tables-metrics) — Contains instantly calculated metrics.
--   [Surveiller](monitoring.md) — Base concepts of ClickHouse monitoring.
-
-## système.nombre {#system-numbers}
-
-Cette table contient une seule colonne UInt64 nommée ‘number’ qui contient presque tous les nombres naturels à partir de zéro.
-Vous pouvez utiliser cette table pour les tests, ou si vous avez besoin de faire une recherche de force brute.
-Les lectures de cette table ne sont pas parallélisées.
-
-## système.numbers_mt {#system-numbers-mt}
-
-Le même que ‘system.numbers’ mais les lectures sont parallélisées. Les nombres peuvent être retournés dans n'importe quel ordre.
-Utilisé pour les tests.
-
-## système.un {#system-one}
-
-Cette table contient une seule ligne avec un ‘dummy’ Colonne UInt8 contenant la valeur 0.
-Cette table est utilisée si une requête SELECT ne spécifie pas la clause FROM.
-Ceci est similaire à la table double trouvée dans d'autres SGBD.
-
-## système.partie {#system_tables-parts}
-
-Contient des informations sur les parties de [MergeTree](../engines/table-engines/mergetree-family/mergetree.md) table.
-
-Chaque ligne décrit une partie des données.
-
-Colonne:
-
--   `partition` (String) – The partition name. To learn what a partition is, see the description of the [ALTER](../sql-reference/statements/alter.md#query_language_queries_alter) requête.
-
-    Format:
-
-    -   `YYYYMM` pour le partitionnement automatique par mois.
-    -   `any_string` lors du partitionnement manuel.
-
--   `name` (`String`) – Name of the data part.
-
--   `active` (`UInt8`) – Flag that indicates whether the data part is active. If a data part is active, it's used in a table. Otherwise, it's deleted. Inactive data parts remain after merging.
-
--   `marks` (`UInt64`) – The number of marks. To get the approximate number of rows in a data part, multiply `marks` par la granularité d'index (généralement 8192) (cet indice ne fonctionne pas pour la granularité adaptative).
-
--   `rows` (`UInt64`) – The number of rows.
-
--   `bytes_on_disk` (`UInt64`) – Total size of all the data part files in bytes.
-
--   `data_compressed_bytes` (`UInt64`) – Total size of compressed data in the data part. All the auxiliary files (for example, files with marks) are not included.
-
--   `data_uncompressed_bytes` (`UInt64`) – Total size of uncompressed data in the data part. All the auxiliary files (for example, files with marks) are not included.
-
--   `marks_bytes` (`UInt64`) – The size of the file with marks.
-
--   `modification_time` (`DateTime`) – The time the directory with the data part was modified. This usually corresponds to the time of data part creation.\|
-
--   `remove_time` (`DateTime`) – The time when the data part became inactive.
-
--   `refcount` (`UInt32`) – The number of places where the data part is used. A value greater than 2 indicates that the data part is used in queries or merges.
-
--   `min_date` (`Date`) – The minimum value of the date key in the data part.
-
--   `max_date` (`Date`) – The maximum value of the date key in the data part.
-
--   `min_time` (`DateTime`) – The minimum value of the date and time key in the data part.
-
--   `max_time`(`DateTime`) – The maximum value of the date and time key in the data part.
-
--   `partition_id` (`String`) – ID of the partition.
-
--   `min_block_number` (`UInt64`) – The minimum number of data parts that make up the current part after merging.
-
--   `max_block_number` (`UInt64`) – The maximum number of data parts that make up the current part after merging.
-
--   `level` (`UInt32`) – Depth of the merge tree. Zero means that the current part was created by insert rather than by merging other parts.
-
--   `data_version` (`UInt64`) – Number that is used to determine which mutations should be applied to the data part (mutations with a version higher than `data_version`).
-
--   `primary_key_bytes_in_memory` (`UInt64`) – The amount of memory (in bytes) used by primary key values.
-
--   `primary_key_bytes_in_memory_allocated` (`UInt64`) – The amount of memory (in bytes) reserved for primary key values.
-
--   `is_frozen` (`UInt8`) – Flag that shows that a partition data backup exists. 1, the backup exists. 0, the backup doesn't exist. For more details, see [FREEZE PARTITION](../sql-reference/statements/alter.md#alter_freeze-partition)
-
--   `database` (`String`) – Name of the database.
-
--   `table` (`String`) – Name of the table.
-
--   `engine` (`String`) – Name of the table engine without parameters.
-
--   `path` (`String`) – Absolute path to the folder with data part files.
-
--   `disk` (`String`) – Name of a disk that stores the data part.
-
--   `hash_of_all_files` (`String`) – [sipHash128](../sql-reference/functions/hash-functions.md#hash_functions-siphash128) de fichiers compressés.
-
--   `hash_of_uncompressed_files` (`String`) – [sipHash128](../sql-reference/functions/hash-functions.md#hash_functions-siphash128) de fichiers non compressés (fichiers avec des marques, fichier d'index, etc.).
-
--   `uncompressed_hash_of_compressed_files` (`String`) – [sipHash128](../sql-reference/functions/hash-functions.md#hash_functions-siphash128) des données dans les fichiers compressés comme s'ils étaient non compressé.
-
--   `bytes` (`UInt64`) – Alias for `bytes_on_disk`.
-
--   `marks_size` (`UInt64`) – Alias for `marks_bytes`.
-
-## système.part_log {#system_tables-part-log}
-
-Le `system.part_log` la table est créée uniquement si [part_log](server-configuration-parameters/settings.md#server_configuration_parameters-part-log) serveur paramètre est spécifié.
-
-Ce tableau contient des informations sur les événements survenus avec [les parties de données](../engines/table-engines/mergetree-family/custom-partitioning-key.md) dans le [MergeTree](../engines/table-engines/mergetree-family/mergetree.md) table de famille, telles que l'ajout ou la fusion de données.
-
-Le `system.part_log` le tableau contient les colonnes suivantes:
-
--   `event_type` (Enum) — Type of the event that occurred with the data part. Can have one of the following values:
-    -   `NEW_PART` — Inserting of a new data part.
-    -   `MERGE_PARTS` — Merging of data parts.
-    -   `DOWNLOAD_PART` — Downloading a data part.
-    -   `REMOVE_PART` — Removing or detaching a data part using [DETACH PARTITION](../sql-reference/statements/alter.md#alter_detach-partition).
-    -   `MUTATE_PART` — Mutating of a data part.
-    -   `MOVE_PART` — Moving the data part from the one disk to another one.
--   `event_date` (Date) — Event date.
--   `event_time` (DateTime) — Event time.
--   `duration_ms` (UInt64) — Duration.
--   `database` (String) — Name of the database the data part is in.
--   `table` (String) — Name of the table the data part is in.
--   `part_name` (String) — Name of the data part.
--   `partition_id` (String) — ID of the partition that the data part was inserted to. The column takes the ‘all’ si le partitionnement est par `tuple()`.
--   `rows` (UInt64) — The number of rows in the data part.
--   `size_in_bytes` (UInt64) — Size of the data part in bytes.
--   `merged_from` (Array(String)) — An array of names of the parts which the current part was made up from (after the merge).
--   `bytes_uncompressed` (UInt64) — Size of uncompressed bytes.
--   `read_rows` (UInt64) — The number of rows was read during the merge.
--   `read_bytes` (UInt64) — The number of bytes was read during the merge.
--   `error` (UInt16) — The code number of the occurred error.
--   `exception` (String) — Text message of the occurred error.
-
-Le `system.part_log` la table est créée après la première insertion de données `MergeTree` table.
-
-## système.processus {#system_tables-processes}
-
-Cette table système est utilisée pour implémenter `SHOW PROCESSLIST` requête.
-
-Colonne:
-
--   `user` (String) – The user who made the query. Keep in mind that for distributed processing, queries are sent to remote servers under the `default` utilisateur. Le champ contient le nom d'utilisateur pour une requête spécifique, pas pour une requête que cette requête lancée.
--   `address` (String) – The IP address the request was made from. The same for distributed processing. To track where a distributed query was originally made from, look at `system.processes` sur le serveur du demandeur de requête.
--   `elapsed` (Float64) – The time in seconds since request execution started.
--   `rows_read` (UInt64) – The number of rows read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.
--   `bytes_read` (UInt64) – The number of uncompressed bytes read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.
--   `total_rows_approx` (UInt64) – The approximation of the total number of rows that should be read. For distributed processing, on the requestor server, this is the total for all remote servers. It can be updated during request processing, when new sources to process become known.
--   `memory_usage` (UInt64) – Amount of RAM the request uses. It might not include some types of dedicated memory. See the [max_memory_usage](../operations/settings/query-complexity.md#settings_max_memory_usage) paramètre.
--   `query` (String) – The query text. For `INSERT` il n'inclut pas les données à insérer.
--   `query_id` (String) – Query ID, if defined.
-
-## système.text_log {#system_tables-text_log}
-
-Contient des entrées de journalisation. Niveau de journalisation qui va à cette table peut être limité `text_log.level` paramètre de serveur.
-
-Colonne:
-
--   `event_date` (`Date`) - Date de l'entrée.
--   `event_time` (`DateTime`) - Temps de l'entrée.
--   `microseconds` (`UInt32`) - Microsecondes de l'entrée.
--   `thread_name` (String) — Name of the thread from which the logging was done.
--   `thread_id` (UInt64) — OS thread ID.
--   `level` (`Enum8`) - Niveau d'entrée.
-    -   `'Fatal' = 1`
-    -   `'Critical' = 2`
-    -   `'Error' = 3`
-    -   `'Warning' = 4`
-    -   `'Notice' = 5`
-    -   `'Information' = 6`
-    -   `'Debug' = 7`
-    -   `'Trace' = 8`
--   `query_id` (`String`)- ID de la requête.
--   `logger_name` (`LowCardinality(String)`) - Name of the logger (i.e. `DDLWorker`)
--   `message` (`String`) - Le message lui-même.
--   `revision` (`UInt32`)- Révision ClickHouse.
--   `source_file` (`LowCardinality(String)`)- Fichier Source à partir duquel la journalisation a été effectuée.
--   `source_line` (`UInt64`)- Ligne Source à partir de laquelle la journalisation a été effectuée.
-
-## système.query_log {#system_tables-query_log}
-
-Contient des informations sur l'exécution de requêtes. Pour chaque requête, vous pouvez voir l'Heure de début du traitement, la durée du traitement, les messages d'erreur et d'autres informations.
-
-!!! note "Note"
-    Le tableau ne contient pas les données d'entrée pour `INSERT` requête.
-
-Clickhouse crée cette table uniquement si [query_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-log) serveur paramètre est spécifié. Ce paramètre définit les règles de journalisation, tels que l'intervalle d'enregistrement ou le nom de la table, la requête sera connecté.
-
-Pour activer la journalisation des requêtes, définissez [log_queries](settings/settings.md#settings-log-queries) paramètre 1. Pour plus de détails, voir le [Paramètre](settings/settings.md) section.
-
-Le `system.query_log` table enregistre deux types de requêtes:
-
-1.  Requêtes initiales qui ont été exécutées directement par le client.
-2.  Requêtes enfants initiées par d'autres requêtes (pour l'exécution de requêtes distribuées). Pour ces types de requêtes, des informations sur les requêtes parentes sont affichées dans `initial_*` colonne.
-
-Colonne:
-
--   `type` (`Enum8`) — Type of event that occurred when executing the query. Values:
-    -   `'QueryStart' = 1` — Successful start of query execution.
-    -   `'QueryFinish' = 2` — Successful end of query execution.
-    -   `'ExceptionBeforeStart' = 3` — Exception before the start of query execution.
-    -   `'ExceptionWhileProcessing' = 4` — Exception during the query execution.
--   `event_date` (Date) — Query starting date.
--   `event_time` (DateTime) — Query starting time.
--   `query_start_time` (DateTime) — Start time of query execution.
--   `query_duration_ms` (UInt64) — Duration of query execution.
--   `read_rows` (UInt64) — Number of read rows.
--   `read_bytes` (UInt64) — Number of read bytes.
--   `written_rows` (UInt64) — For `INSERT` des requêtes, le nombre de lignes. Pour les autres requêtes, la valeur de la colonne est 0.
--   `written_bytes` (UInt64) — For `INSERT` des requêtes, le nombre d'octets écrits. Pour les autres requêtes, la valeur de la colonne est 0.
--   `result_rows` (UInt64) — Number of rows in the result.
--   `result_bytes` (UInt64) — Number of bytes in the result.
--   `memory_usage` (UInt64) — Memory consumption by the query.
--   `query` (String) — Query string.
--   `exception` (String) — Exception message.
--   `stack_trace` (String) — Stack trace (a list of methods called before the error occurred). An empty string, if the query is completed successfully.
--   `is_initial_query` (UInt8) — Query type. Possible values:
-    -   1 — Query was initiated by the client.
-    -   0 — Query was initiated by another query for distributed query execution.
--   `user` (String) — Name of the user who initiated the current query.
--   `query_id` (String) — ID of the query.
--   `address` (IPv6) — IP address that was used to make the query.
--   `port` (UInt16) — The client port that was used to make the query.
--   `initial_user` (String) — Name of the user who ran the initial query (for distributed query execution).
--   `initial_query_id` (String) — ID of the initial query (for distributed query execution).
--   `initial_address` (IPv6) — IP address that the parent query was launched from.
--   `initial_port` (UInt16) — The client port that was used to make the parent query.
--   `interface` (UInt8) — Interface that the query was initiated from. Possible values:
-    -   1 — TCP.
-    -   2 — HTTP.
--   `os_user` (String) — OS's username who runs [clickhouse-client](../interfaces/cli.md).
--   `client_hostname` (String) — Hostname of the client machine where the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP est exécuté.
--   `client_name` (String) — The [clickhouse-client](../interfaces/cli.md) ou un autre nom de client TCP.
--   `client_revision` (UInt32) — Revision of the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP.
--   `client_version_major` (UInt32) — Major version of the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP.
--   `client_version_minor` (UInt32) — Minor version of the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP.
--   `client_version_patch` (UInt32) — Patch component of the [clickhouse-client](../interfaces/cli.md) ou une autre version du client TCP.
--   `http_method` (UInt8) — HTTP method that initiated the query. Possible values:
-    -   0 — The query was launched from the TCP interface.
-    -   1 — `GET` la méthode a été utilisée.
-    -   2 — `POST` la méthode a été utilisée.
--   `http_user_agent` (String) — The `UserAgent` en-tête passé dans la requête HTTP.
--   `quota_key` (String) — The “quota key” spécifié dans le [quota](quotas.md) (voir `keyed`).
--   `revision` (UInt32) — ClickHouse revision.
--   `thread_numbers` (Array(UInt32)) — Number of threads that are participating in query execution.
--   `ProfileEvents.Names` (Array(String)) — Counters that measure different metrics. The description of them could be found in the table [système.événement](#system_tables-events)
--   `ProfileEvents.Values` (Array(UInt64)) — Values of metrics that are listed in the `ProfileEvents.Names` colonne.
--   `Settings.Names` (Array(String)) — Names of settings that were changed when the client ran the query. To enable logging changes to settings, set the `log_query_settings` paramètre 1.
--   `Settings.Values` (Array(String)) — Values of settings that are listed in the `Settings.Names` colonne.
-
-Chaque requête crée une ou deux lignes dans le `query_log` le tableau, en fonction de l'état de la requête:
-
-1.  Si l'exécution de la requête est réussie, deux événements de type 1 et 2 sont créés (voir `type` colonne).
-2.  Si une erreur s'est produite pendant le traitement de la requête, deux événements avec les types 1 et 4 sont créés.
-3.  Si une erreur s'est produite avant le lancement de la requête, un seul événement de type 3 est créé.
-
-Par défaut, les journaux sont ajoutés à la table à des intervalles de 7,5 secondes. Vous pouvez définir cet intervalle dans la [query_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-log) configuration du serveur (voir `flush_interval_milliseconds` paramètre). Pour vider les journaux de force du tampon mémoire dans la table, utilisez le `SYSTEM FLUSH LOGS` requête.
-
-Lorsque la table est supprimée manuellement, il sera automatiquement créé à la volée. Notez que tous les précédents journaux seront supprimés.
-
-!!! note "Note"
-    La période de stockage des journaux est illimitée. Les journaux ne sont pas automatiquement supprimés de la table. Vous devez organiser vous-même la suppression des journaux obsolètes.
-
-Vous pouvez spécifier une clé de partitionnement arbitraire pour `system.query_log` la table dans le [query_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-log) configuration du serveur (voir `partition_by` paramètre).
-
-## système.query_thread_log {#system_tables-query-thread-log}
-
-La table contient des informations sur chaque thread d'exécution de requête.
-
-Clickhouse crée cette table uniquement si [query_thread_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) serveur paramètre est spécifié. Ce paramètre définit les règles de journalisation, tels que l'intervalle d'enregistrement ou le nom de la table, la requête sera connecté.
-
-Pour activer la journalisation des requêtes, définissez [log_query_threads](settings/settings.md#settings-log-query-threads) paramètre 1. Pour plus de détails, voir le [Paramètre](settings/settings.md) section.
-
-Colonne:
-
--   `event_date` (Date) — the date when the thread has finished execution of the query.
--   `event_time` (DateTime) — the date and time when the thread has finished execution of the query.
--   `query_start_time` (DateTime) — Start time of query execution.
--   `query_duration_ms` (UInt64) — Duration of query execution.
--   `read_rows` (UInt64) — Number of read rows.
--   `read_bytes` (UInt64) — Number of read bytes.
--   `written_rows` (UInt64) — For `INSERT` des requêtes, le nombre de lignes. Pour les autres requêtes, la valeur de la colonne est 0.
--   `written_bytes` (UInt64) — For `INSERT` des requêtes, le nombre d'octets écrits. Pour les autres requêtes, la valeur de la colonne est 0.
--   `memory_usage` (Int64) — The difference between the amount of allocated and freed memory in context of this thread.
--   `peak_memory_usage` (Int64) — The maximum difference between the amount of allocated and freed memory in context of this thread.
--   `thread_name` (String) — Name of the thread.
--   `thread_number` (UInt32) — Internal thread ID.
--   `os_thread_id` (Int32) — OS thread ID.
--   `master_thread_id` (UInt64) — OS initial ID of initial thread.
--   `query` (String) — Query string.
--   `is_initial_query` (UInt8) — Query type. Possible values:
-    -   1 — Query was initiated by the client.
-    -   0 — Query was initiated by another query for distributed query execution.
--   `user` (String) — Name of the user who initiated the current query.
--   `query_id` (String) — ID of the query.
--   `address` (IPv6) — IP address that was used to make the query.
--   `port` (UInt16) — The client port that was used to make the query.
--   `initial_user` (String) — Name of the user who ran the initial query (for distributed query execution).
--   `initial_query_id` (String) — ID of the initial query (for distributed query execution).
--   `initial_address` (IPv6) — IP address that the parent query was launched from.
--   `initial_port` (UInt16) — The client port that was used to make the parent query.
--   `interface` (UInt8) — Interface that the query was initiated from. Possible values:
-    -   1 — TCP.
-    -   2 — HTTP.
--   `os_user` (String) — OS's username who runs [clickhouse-client](../interfaces/cli.md).
--   `client_hostname` (String) — Hostname of the client machine where the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP est exécuté.
--   `client_name` (String) — The [clickhouse-client](../interfaces/cli.md) ou un autre nom de client TCP.
--   `client_revision` (UInt32) — Revision of the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP.
--   `client_version_major` (UInt32) — Major version of the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP.
--   `client_version_minor` (UInt32) — Minor version of the [clickhouse-client](../interfaces/cli.md) ou un autre client TCP.
--   `client_version_patch` (UInt32) — Patch component of the [clickhouse-client](../interfaces/cli.md) ou une autre version du client TCP.
--   `http_method` (UInt8) — HTTP method that initiated the query. Possible values:
-    -   0 — The query was launched from the TCP interface.
-    -   1 — `GET` la méthode a été utilisée.
-    -   2 — `POST` la méthode a été utilisée.
--   `http_user_agent` (String) — The `UserAgent` en-tête passé dans la requête HTTP.
--   `quota_key` (String) — The “quota key” spécifié dans le [quota](quotas.md) (voir `keyed`).
--   `revision` (UInt32) — ClickHouse revision.
--   `ProfileEvents.Names` (Array(String)) — Counters that measure different metrics for this thread. The description of them could be found in the table [système.événement](#system_tables-events)
--   `ProfileEvents.Values` (Array(UInt64)) — Values of metrics for this thread that are listed in the `ProfileEvents.Names` colonne.
-
-Par défaut, les journaux sont ajoutés à la table à des intervalles de 7,5 secondes. Vous pouvez définir cet intervalle dans la [query_thread_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) configuration du serveur (voir `flush_interval_milliseconds` paramètre). Pour vider les journaux de force du tampon mémoire dans la table, utilisez le `SYSTEM FLUSH LOGS` requête.
-
-Lorsque la table est supprimée manuellement, il sera automatiquement créé à la volée. Notez que tous les précédents journaux seront supprimés.
-
-!!! note "Note"
-    La période de stockage des journaux est illimitée. Les journaux ne sont pas automatiquement supprimés de la table. Vous devez organiser vous-même la suppression des journaux obsolètes.
-
-Vous pouvez spécifier une clé de partitionnement arbitraire pour `system.query_thread_log` la table dans le [query_thread_log](server-configuration-parameters/settings.md#server_configuration_parameters-query-thread-log) configuration du serveur (voir `partition_by` paramètre).
-
-## système.trace_log {#system_tables-trace_log}
-
-Contient des traces de pile collectées par le profileur de requête d'échantillonnage.
-
-Clickhouse crée cette table lorsque le [trace_log](server-configuration-parameters/settings.md#server_configuration_parameters-trace_log) la section de configuration du serveur est définie. Aussi l' [query_profiler_real_time_period_ns](settings/settings.md#query_profiler_real_time_period_ns) et [query_profiler_cpu_time_period_ns](settings/settings.md#query_profiler_cpu_time_period_ns) paramètres doivent être définis.
-
-Pour analyser les journaux, utilisez `addressToLine`, `addressToSymbol` et `demangle` fonctions d'introspection.
-
-Colonne:
-
--   `event_date` ([Date](../sql-reference/data-types/date.md)) — Date of sampling moment.
-
--   `event_time` ([DateTime](../sql-reference/data-types/datetime.md)) — Timestamp of the sampling moment.
-
--   `timestamp_ns` ([UInt64](../sql-reference/data-types/int-uint.md)) — Timestamp of the sampling moment in nanoseconds.
-
--   `revision` ([UInt32](../sql-reference/data-types/int-uint.md)) — ClickHouse server build revision.
-
-    Lors de la connexion au serveur par `clickhouse-client`, vous voyez la chaîne similaire à `Connected to ClickHouse server version 19.18.1 revision 54429.`. Ce champ contient le `revision` mais pas le `version` d'un serveur.
-
--   `timer_type` ([Enum8](../sql-reference/data-types/enum.md)) — Timer type:
-
-    -   `Real` représente l'horloge murale.
-    -   `CPU` représente le temps CPU.
-
--   `thread_number` ([UInt32](../sql-reference/data-types/int-uint.md)) — Thread identifier.
-
--   `query_id` ([Chaîne](../sql-reference/data-types/string.md)) — Query identifier that can be used to get details about a query that was running from the [query_log](#system_tables-query_log) système de table.
-
--   `trace` ([Tableau (UInt64)](../sql-reference/data-types/array.md)) — Stack trace at the moment of sampling. Each element is a virtual memory address inside ClickHouse server process.
-
-**Exemple**
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:    2019-11-15
-event_time:    2019-11-15 15:09:38
-revision:      54428
-timer_type:    Real
-thread_number: 48
-query_id:      acc4d61f-5bd1-4a3e-bc91-2180be37c915
-trace:         [94222141367858,94222152240175,94222152325351,94222152329944,94222152330796,94222151449980,94222144088167,94222151682763,94222144088167,94222151682763,94222144088167,94222144058283,94222144059248,94222091840750,94222091842302,94222091831228,94222189631488,140509950166747,140509942945935]
-```
-
-## système.réplique {#system_tables-replicas}
-
-Contient des informations et l'état des tables répliquées résidant sur le serveur local.
-Ce tableau peut être utilisé pour la surveillance. La table contient une ligne pour chaque Répliqué\* table.
-
-Exemple:
-
-``` sql
-SELECT *
-FROM system.replicas
-WHERE table = 'visits'
-FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-database:                   merge
-table:                      visits
-engine:                     ReplicatedCollapsingMergeTree
-is_leader:                  1
-can_become_leader:          1
-is_readonly:                0
-is_session_expired:         0
-future_parts:               1
-parts_to_check:             0
-zookeeper_path:             /clickhouse/tables/01-06/visits
-replica_name:               example01-06-1.yandex.ru
-replica_path:               /clickhouse/tables/01-06/visits/replicas/example01-06-1.yandex.ru
-columns_version:            9
-queue_size:                 1
-inserts_in_queue:           0
-merges_in_queue:            1
-part_mutations_in_queue:    0
-queue_oldest_time:          2020-02-20 08:34:30
-inserts_oldest_time:        1970-01-01 00:00:00
-merges_oldest_time:         2020-02-20 08:34:30
-part_mutations_oldest_time: 1970-01-01 00:00:00
-oldest_part_to_get:
-oldest_part_to_merge_to:    20200220_20284_20840_7
-oldest_part_to_mutate_to:
-log_max_index:              596273
-log_pointer:                596274
-last_queue_update:          2020-02-20 08:34:32
-absolute_delay:             0
-total_replicas:             2
-active_replicas:            2
-```
-
-Colonne:
-
--   `database` (`String`) - Nom de base de données
--   `table` (`String`)- Nom de la Table
--   `engine` (`String`)- Nom du moteur de Table
--   `is_leader` (`UInt8`) - Si la réplique est le chef de file.
-    Une seule réplique à la fois peut être le leader. Le leader est responsable de la sélection des fusions d'arrière-plan à effectuer.
-    Notez que les Écritures peuvent être effectuées sur n'importe quel réplica disponible et ayant une session dans ZK, qu'il s'agisse d'un leader.
--   `can_become_leader` (`UInt8`)- Si la réplique peut être élue en tant que leader.
--   `is_readonly` (`UInt8`) - Si la réplique est en mode lecture seule.
-    Ce mode est activé si la configuration n'a pas de sections avec ZooKeeper, si une erreur inconnue s'est produite lors de la réinitialisation des sessions dans ZooKeeper et lors de la réinitialisation des sessions dans ZooKeeper.
--   `is_session_expired` (`UInt8`)- la session avec ZooKeeper a expiré. Fondamentalement le même que `is_readonly`.
--   `future_parts` (`UInt32`)- Le nombre de parties de données qui apparaîtront à la suite D'insertions ou de fusions qui n'ont pas encore été effectuées.
--   `parts_to_check` (`UInt32`) - Le nombre de parties des données dans la file d'attente pour la vérification. Une pièce est placée dans la file d'attente de vérification s'il y a un soupçon qu'elle pourrait être endommagée.
--   `zookeeper_path` (`String`)- Chemin d'accès aux données de la table dans ZooKeeper.
--   `replica_name` (`String`) - Réplique nom de la Gardienne. Différentes répliques d'une même table ont des noms différents.
--   `replica_path` (`String`)- Chemin vers les données de réplique dans ZooKeeper. La même chose que la concaténation ‘zookeeper_path/replicas/replica_path’.
--   `columns_version` (`Int32`)- Numéro de Version de la structure de la table. Indique combien de fois ALTER a été effectué. Si les répliques ont des versions différentes, cela signifie que certaines répliques n'ont pas encore Toutes les modifications.
--   `queue_size` (`UInt32`),- La taille de la file d'attente pour les opérations en attente d'être exécuté. Les opérations comprennent l'insertion de blocs de données, les fusions et certaines autres actions. Il coïncide généralement avec `future_parts`.
--   `inserts_in_queue` (`UInt32`) - Nombre d'insertions de blocs de données qui doivent être faits. Les Insertions sont généralement répliquées assez rapidement. Si ce nombre est grand, cela signifie que quelque chose est faux.
--   `merges_in_queue` (`UInt32`) - Le nombre de fusions en attente d'être fait. Parfois, les fusions sont longues, donc cette valeur peut être supérieure à zéro pendant une longue période.
--   `part_mutations_in_queue` (`UInt32`) - Le nombre de mutations en attente d'être fait.
--   `queue_oldest_time` (`DateTime`) - Si `queue_size` supérieur à 0, indique quand l'opération la plus ancienne a été ajoutée à la file d'attente.
--   `inserts_oldest_time` (`DateTime`) - Voir `queue_oldest_time`
--   `merges_oldest_time` (`DateTime`) - Voir `queue_oldest_time`
--   `part_mutations_oldest_time` (`DateTime`) - Voir `queue_oldest_time`
-
-Les 4 colonnes suivantes ont une valeur non nulle uniquement lorsqu'il y a une session active avec ZK.
-
--   `log_max_index` (`UInt64`) - Maximum nombre d'entrées dans le journal de l'activité générale.
--   `log_pointer` (`UInt64`)- Numéro d'entrée Maximum dans le journal de l'activité générale que le réplica a copié dans sa file d'attente d'exécution, plus un. Si `log_pointer` est beaucoup plus petite que `log_max_index` quelque chose ne va pas.
--   `last_queue_update` (`DateTime`) - Lorsque la file d'attente a été mise à jour la dernière fois.
--   `absolute_delay` (`UInt64`)- Combien de décalage en secondes la réplique actuelle A.
--   `total_replicas` (`UInt8`) - Le nombre total de répliques connues de ce tableau.
--   `active_replicas` (`UInt8`) - Le nombre de répliques de cette table qui ont une session dans ZooKeeper (c'est-à-dire le nombre de répliques fonctionnelles).
-
-Si vous demandez toutes les colonnes, la table peut fonctionner un peu lentement, car plusieurs lectures de ZooKeeper sont faites pour chaque ligne.
-Si vous ne demandez pas les 4 dernières colonnes (log_max_index, log_pointer, total_replicas, active_replicas), la table fonctionne rapidement.
-
-Par exemple, vous pouvez vérifier que tout fonctionne correctement comme ceci:
-
-``` sql
-SELECT
-    database,
-    table,
-    is_leader,
-    is_readonly,
-    is_session_expired,
-    future_parts,
-    parts_to_check,
-    columns_version,
-    queue_size,
-    inserts_in_queue,
-    merges_in_queue,
-    log_max_index,
-    log_pointer,
-    total_replicas,
-    active_replicas
-FROM system.replicas
-WHERE
-       is_readonly
-    OR is_session_expired
-    OR future_parts > 20
-    OR parts_to_check > 10
-    OR queue_size > 20
-    OR inserts_in_queue > 10
-    OR log_max_index - log_pointer > 10
-    OR total_replicas < 2
-    OR active_replicas < total_replicas
-```
-
-Si cette requête ne retourne rien, cela signifie que tout va bien.
-
-## système.paramètre {#system-tables-system-settings}
-
-Contient des informations sur les paramètres de session pour l'utilisateur actuel.
-
-Colonne:
-
--   `name` ([Chaîne](../sql-reference/data-types/string.md)) — Setting name.
--   `value` ([Chaîne](../sql-reference/data-types/string.md)) — Setting value.
--   `changed` ([UInt8](../sql-reference/data-types/int-uint.md#uint-ranges)) — Shows whether a setting is changed from its default value.
--   `description` ([Chaîne](../sql-reference/data-types/string.md)) — Short setting description.
--   `min` ([Nullable](../sql-reference/data-types/nullable.md)([Chaîne](../sql-reference/data-types/string.md))) — Minimum value of the setting, if any is set via [contraintes](settings/constraints-on-settings.md#constraints-on-settings). Si le réglage n'a pas de valeur minimale, contient [NULL](../sql-reference/syntax.md#null-literal).
--   `max` ([Nullable](../sql-reference/data-types/nullable.md)([Chaîne](../sql-reference/data-types/string.md))) — Maximum value of the setting, if any is set via [contraintes](settings/constraints-on-settings.md#constraints-on-settings). Si le réglage n'a pas de valeur maximale, contient [NULL](../sql-reference/syntax.md#null-literal).
--   `readonly` ([UInt8](../sql-reference/data-types/int-uint.md#uint-ranges)) — Shows whether the current user can change the setting:
-    -   `0` — Current user can change the setting.
-    -   `1` — Current user can't change the setting.
-
-**Exemple**
-
-L'exemple suivant montre comment obtenir des informations sur les paramètres dont le nom contient `min_i`.
-
-``` sql
-SELECT *
-FROM system.settings
-WHERE name LIKE '%min_i%'
-```
-
-``` text
-┌─name────────────────────────────────────────┬─value─────┬─changed─┬─description───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┐
-│ min_insert_block_size_rows                  │ 1048576   │       0 │ Squash blocks passed to INSERT query to specified size in rows, if blocks are not big enough.                                                                         │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
-│ min_insert_block_size_bytes                 │ 268435456 │       0 │ Squash blocks passed to INSERT query to specified size in bytes, if blocks are not big enough.                                                                        │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
-│ read_backoff_min_interval_between_events_ms │ 1000      │       0 │ Settings to reduce the number of threads in case of slow reads. Do not pay attention to the event, if the previous one has passed less than a certain amount of time. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
-└─────────────────────────────────────────────┴───────────┴─────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┘
-```
-
-À l'aide de `WHERE changed` peut être utile, par exemple, lorsque vous voulez vérifier:
-
--   Indique si les paramètres des fichiers de configuration sont chargés correctement et sont utilisés.
--   Paramètres modifiés dans la session en cours.
-
-<!-- -->
-
-``` sql
-SELECT * FROM system.settings WHERE changed AND name='load_balancing'
-```
-
-**Voir aussi**
-
--   [Paramètre](settings/index.md#session-settings-intro)
--   [Autorisations pour les requêtes](settings/permissions-for-queries.md#settings_readonly)
--   [Contraintes sur les paramètres](settings/constraints-on-settings.md)
-
-## système.tableau_moteurs {#system.table_engines}
-
-``` text
-┌─name───────────────────┬─value───────┐
-│ max_threads            │ 8           │
-│ use_uncompressed_cache │ 0           │
-│ load_balancing         │ random      │
-│ max_memory_usage       │ 10000000000 │
-└────────────────────────┴─────────────┘
-```
-
-## système.merge_tree_settings {#system-merge_tree_settings}
-
-Contient des informations sur les paramètres pour `MergeTree` table.
-
-Colonne:
-
--   `name` (String) — Setting name.
--   `value` (String) — Setting value.
--   `description` (String) — Setting description.
--   `type` (String) — Setting type (implementation specific string value).
--   `changed` (UInt8) — Whether the setting was explicitly defined in the config or explicitly changed.
-
-## système.tableau_moteurs {#system-table-engines}
-
-Contient une description des moteurs de table pris en charge par le serveur et leurs informations de support de fonctionnalité.
-
-Ce tableau contient les colonnes suivantes (le type de colonne est indiqué entre parenthèses):
-
--   `name` (String) — The name of table engine.
--   `supports_settings` (UInt8) — Flag that indicates if table engine supports `SETTINGS` clause.
--   `supports_skipping_indices` (UInt8) — Flag that indicates if table engine supports [sauter les indices](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-data_skipping-indexes).
--   `supports_ttl` (UInt8) — Flag that indicates if table engine supports [TTL](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-ttl).
--   `supports_sort_order` (UInt8) — Flag that indicates if table engine supports clauses `PARTITION_BY`, `PRIMARY_KEY`, `ORDER_BY` et `SAMPLE_BY`.
--   `supports_replication` (UInt8) — Flag that indicates if table engine supports [réplication des données](../engines/table-engines/mergetree-family/replication.md).
--   `supports_duduplication` (UInt8) — Flag that indicates if table engine supports data deduplication.
-
-Exemple:
-
-``` sql
-SELECT *
-FROM system.table_engines
-WHERE name in ('Kafka', 'MergeTree', 'ReplicatedCollapsingMergeTree')
-```
-
-``` text
-┌─name──────────────────────────┬─supports_settings─┬─supports_skipping_indices─┬─supports_sort_order─┬─supports_ttl─┬─supports_replication─┬─supports_deduplication─┐
-│ Kafka                         │                 1 │                         0 │                   0 │            0 │                    0 │                      0 │
-│ MergeTree                     │                 1 │                         1 │                   1 │            1 │                    0 │                      0 │
-│ ReplicatedCollapsingMergeTree │                 1 │                         1 │                   1 │            1 │                    1 │                      1 │
-└───────────────────────────────┴───────────────────┴───────────────────────────┴─────────────────────┴──────────────┴──────────────────────┴────────────────────────┘
-```
-
-**Voir aussi**
-
--   Famille MergeTree [les clauses de requête](../engines/table-engines/mergetree-family/mergetree.md#mergetree-query-clauses)
--   Kafka [paramètre](../engines/table-engines/integrations/kafka.md#table_engine-kafka-creating-a-table)
--   Rejoindre [paramètre](../engines/table-engines/special/join.md#join-limitations-and-settings)
-
-## système.table {#system-tables}
-
-Contient les métadonnées de chaque table que le serveur connaît. Les tableaux détachés ne sont pas représentés dans `system.tables`.
-
-Ce tableau contient les colonnes suivantes (le type de colonne est indiqué entre parenthèses):
-
--   `database` (String) — The name of the database the table is in.
-
--   `name` (String) — Table name.
-
--   `engine` (String) — Table engine name (without parameters).
-
--   `is_temporary` (UInt8) - indicateur qui indique si la table est temporaire.
-
--   `data_path` (Chaîne) - chemin d'accès aux données de la table dans le système de fichiers.
-
--   `metadata_path` (Chaîne) - chemin d'accès aux métadonnées de la table dans le système de fichiers.
-
--   `metadata_modification_time` (DateTime) - Heure de la dernière modification des métadonnées de la table.
-
--   `dependencies_database` (Array (String)) - dépendances de base de données.
-
--   `dependencies_table` (Array (String)) - dépendances de Table ([MaterializedView](../engines/table-engines/special/materializedview.md) tables basées sur le tableau actuel).
-
--   `create_table_query` (Chaîne) - la requête qui a été utilisée pour créer la table.
-
--   `engine_full` (Chaîne) - paramètres du moteur de table.
-
--   `partition_key` (String) - l'expression de clé de partition spécifiée dans le tableau.
-
--   `sorting_key` (String) - l'expression de clé de tri spécifiée dans la table.
-
--   `primary_key` (String) - l'expression de clé primaire spécifiée dans la table.
-
--   `sampling_key` (String) - l'expression de clé d'échantillonnage spécifiée dans la table.
-
--   `storage_policy` (String) - La politique de stockage:
-
-    -   [MergeTree](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes)
-    -   [Distribué](../engines/table-engines/special/distributed.md#distributed)
-
--   `total_rows` (Nullable (UInt64)) - nombre Total de lignes, s'il est possible de déterminer rapidement le nombre exact de lignes dans la table, sinon `Null` (y compris underying `Buffer` table).
-
--   `total_bytes` (Nullable (UInt64)) - nombre Total d'octets, s'il est possible de déterminer rapidement le nombre exact d'octets pour la table sur le stockage, sinon `Null` (**ne pas** comprend tout de stockage sous-jacent).
-
-    -   If the table stores data on disk, returns used space on disk (i.e. compressed).
-    -   Si la table stocke des données en mémoire, renvoie un nombre approximatif d'octets utilisés en mémoire.
-
-Le `system.tables` le tableau est utilisé dans `SHOW TABLES` implémentation de requête.
-
-## système.zookeeper {#system-zookeeper}
-
-La table n'existe pas si ZooKeeper n'est pas configuré. Permet de lire les données du cluster Zookeeper défini dans la configuration.
-La requête doit avoir un ‘path’ condition d'égalité dans la clause WHERE. C'est le chemin dans ZooKeeper pour les enfants pour lesquels vous souhaitez obtenir des données.
-
-Requête `SELECT * FROM system.zookeeper WHERE path = '/clickhouse'` données de sortie pour tous les enfants `/clickhouse` nœud.
-Pour générer des données pour tous les nœuds racine, écrivez path = ‘/’.
-Si le chemin d'accès spécifié dans ‘path’ n'existe pas, une exception sera levée.
-
-Colonne:
-
--   `name` (String) — The name of the node.
--   `path` (String) — The path to the node.
--   `value` (String) — Node value.
--   `dataLength` (Int32) — Size of the value.
--   `numChildren` (Int32) — Number of descendants.
--   `czxid` (Int64) — ID of the transaction that created the node.
--   `mzxid` (Int64) — ID of the transaction that last changed the node.
--   `pzxid` (Int64) — ID of the transaction that last deleted or added descendants.
--   `ctime` (DateTime) — Time of node creation.
--   `mtime` (DateTime) — Time of the last modification of the node.
--   `version` (Int32) — Node version: the number of times the node was changed.
--   `cversion` (Int32) — Number of added or removed descendants.
--   `aversion` (Int32) — Number of changes to the ACL.
--   `ephemeralOwner` (Int64) — For ephemeral nodes, the ID of the session that owns this node.
-
-Exemple:
-
-``` sql
-SELECT *
-FROM system.zookeeper
-WHERE path = '/clickhouse/tables/01-08/visits/replicas'
-FORMAT Vertical
-```
-
-``` text
-Row 1:
-──────
-name:           example01-08-1.yandex.ru
-value:
-czxid:          932998691229
-mzxid:          932998691229
-ctime:          2015-03-27 16:49:51
-mtime:          2015-03-27 16:49:51
-version:        0
-cversion:       47
-aversion:       0
-ephemeralOwner: 0
-dataLength:     0
-numChildren:    7
-pzxid:          987021031383
-path:           /clickhouse/tables/01-08/visits/replicas
-
-Row 2:
-──────
-name:           example01-08-2.yandex.ru
-value:
-czxid:          933002738135
-mzxid:          933002738135
-ctime:          2015-03-27 16:57:01
-mtime:          2015-03-27 16:57:01
-version:        0
-cversion:       37
-aversion:       0
-ephemeralOwner: 0
-dataLength:     0
-numChildren:    7
-pzxid:          987021252247
-path:           /clickhouse/tables/01-08/visits/replicas
-```
-
-## système.mutation {#system_tables-mutations}
-
-Le tableau contient des informations sur [mutation](../sql-reference/statements/alter.md#alter-mutations) des tables MergeTree et leur progression. Chaque commande de mutation est représentée par une seule ligne. Le tableau comporte les colonnes suivantes:
-
-**base de données**, **table** - Le nom de la base de données et de la table à laquelle la mutation a été appliquée.
-
-**mutation_id** - Le numéro d'identification de la mutation. Pour les tables répliquées ces ID correspondent aux noms znode dans le `<table_path_in_zookeeper>/mutations/` répertoire de la Gardienne. Pour les tables non compliquées, Les Id correspondent aux noms de fichiers dans le répertoire de données de la table.
-
-**commande** - La chaîne de commande mutation (la partie de la requête après `ALTER TABLE [db.]table`).
-
-**create_time** - Quand cette commande de mutation a été soumise pour exécution.
-
-**block_numbers.partition_id**, **block_numbers.nombre** - Une colonne imbriquée. Pour les mutations de tables répliquées, il contient un enregistrement pour chaque partition: l'ID de partition et le numéro de bloc acquis par la mutation (dans chaque partition, seules les parties contenant des blocs avec des nombres inférieurs au numéro de bloc acquis par la mutation dans cette partition seront mutées). Dans les tables non répliquées, les numéros de bloc de toutes les partitions forment une seule séquence. Cela signifie que pour les mutations de tables non répliquées, la colonne contiendra un enregistrement avec un seul numéro de bloc acquis par la mutation.
-
-**parts_to_do** - Le nombre de parties de données qui doivent être mutées pour que la mutation se termine.
-
-**_done** - La mutation est faite? Notez que même si `parts_to_do = 0` il est possible qu'une mutation d'une table répliquée ne soit pas encore effectuée en raison d'un INSERT de longue durée qui créera une nouvelle partie de données qui devra être mutée.
-
-S'il y avait des problèmes avec la mutation de certaines parties, les colonnes suivantes contiennent des informations supplémentaires:
-
-**latest_failed_part** - Le nom de la partie la plus récente qui n'a pas pu être mutée.
-
-**latest_fail_time** - Le temps de la partie la plus récente mutation de l'échec.
-
-**latest_fail_reason** - Le message d'exception qui a provoqué l'échec de la mutation de pièce la plus récente.
-
-## système.disque {#system_tables-disks}
-
-Contient des informations sur les disques définis dans [configuration du serveur](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes_configure).
-
-Colonne:
-
--   `name` ([Chaîne](../sql-reference/data-types/string.md)) — Name of a disk in the server configuration.
--   `path` ([Chaîne](../sql-reference/data-types/string.md)) — Path to the mount point in the file system.
--   `free_space` ([UInt64](../sql-reference/data-types/int-uint.md)) — Free space on disk in bytes.
--   `total_space` ([UInt64](../sql-reference/data-types/int-uint.md)) — Disk volume in bytes.
--   `keep_free_space` ([UInt64](../sql-reference/data-types/int-uint.md)) — Amount of disk space that should stay free on disk in bytes. Defined in the `keep_free_space_bytes` paramètre de configuration du disque.
-
-## système.storage_policies {#system_tables-storage_policies}
-
-Contient des informations sur les stratégies de stockage et les volumes définis [configuration du serveur](../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes_configure).
-
-Colonne:
-
--   `policy_name` ([Chaîne](../sql-reference/data-types/string.md)) — Name of the storage policy.
--   `volume_name` ([Chaîne](../sql-reference/data-types/string.md)) — Volume name defined in the storage policy.
--   `volume_priority` ([UInt64](../sql-reference/data-types/int-uint.md)) — Volume order number in the configuration.
--   `disks` ([Tableau(String)](../sql-reference/data-types/array.md)) — Disk names, defined in the storage policy.
--   `max_data_part_size` ([UInt64](../sql-reference/data-types/int-uint.md)) — Maximum size of a data part that can be stored on volume disks (0 — no limit).
--   `move_factor` ([Float64](../sql-reference/data-types/float.md)) — Ratio of free disk space. When the ratio exceeds the value of configuration parameter, ClickHouse start to move data to the next volume in order.
-
-Si la stratégie de stockage contient plus d'un volume, les informations pour chaque volume sont stockées dans la ligne individuelle de la table.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/system_tables/) <!--hide-->
diff --git a/docs/fr/operations/tips.md b/docs/fr/operations/tips.md
deleted file mode 100644
index dc963e5e27af..000000000000
--- a/docs/fr/operations/tips.md
+++ /dev/null
@@ -1,251 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: Recommandations D'Utilisation
----
-
-# Recommandations D'Utilisation {#usage-recommendations}
-
-## Gouverneur de mise à L'échelle du processeur {#cpu-scaling-governor}
-
-Utilisez toujours la `performance` mise à l'échelle gouverneur. Le `on-demand` gouverneur de mise à l'échelle fonctionne bien pire avec une demande constamment élevée.
-
-``` bash
-$ echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
-```
-
-## CPU Limitations {#cpu-limitations}
-
-Les processeurs peuvent surchauffer. Utiliser `dmesg` pour voir si la fréquence D'horloge du processeur était limitée en raison de la surchauffe.
-La restriction peut également être définie en externe au niveau du centre de données. Vous pouvez utiliser `turbostat` à surveiller sous une charge.
-
-## RAM {#ram}
-
-Pour de petites quantités de données (jusqu'à ~200 GO en mode compressé), il est préférable d'utiliser autant de mémoire que le volume de données.
-Pour de grandes quantités de données et lors du traitement de requêtes interactives (en ligne), vous devez utiliser une quantité raisonnable de RAM (128 Go ou plus) afin que le sous-ensemble de données chaudes s'intègre dans le cache des pages.
-Même pour des volumes de données d'environ 50 To par serveur, l'utilisation de 128 Go de RAM améliore considérablement les performances des requêtes par rapport à 64 Go.
-
-Ne désactivez pas de surcharge. Valeur `cat /proc/sys/vm/overcommit_memory` devrait être 0 ou 1. Exécuter
-
-``` bash
-$ echo 0 | sudo tee /proc/sys/vm/overcommit_memory
-```
-
-## Huge Pages {#huge-pages}
-
-Toujours désactiver les pages énormes transparentes. Il interfère avec les allocateurs de mémoire, ce qui entraîne une dégradation significative des performances.
-
-``` bash
-$ echo 'never' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled
-```
-
-Utiliser `perf top` pour regarder le temps passé dans le noyau pour la gestion de la mémoire.
-Les pages énormes permanentes n'ont pas non plus besoin d'être allouées.
-
-## Sous-Système De Stockage {#storage-subsystem}
-
-Si votre budget vous permet D'utiliser SSD, utilisez SSD.
-Sinon, utilisez un disque dur. Disques durs SATA 7200 RPM fera l'affaire.
-
-Donner la préférence à un grand nombre de serveurs avec des disques durs locaux sur un plus petit nombre de serveurs avec un disque attaché étagères.
-Mais pour stocker des archives avec des requêtes rares, les étagères fonctionneront.
-
-## RAID {#raid}
-
-Lorsque vous utilisez le disque dur, vous pouvez combiner leur RAID-10, RAID-5, RAID-6 ou RAID-50.
-Pour Linux, le RAID logiciel est meilleur (avec `mdadm`). Nous ne recommandons pas d'utiliser LVM.
-Lors de la création de RAID-10, sélectionnez `far` disposition.
-Si votre budget le permet, choisissez RAID-10.
-
-Si vous avez plus de 4 disques, Utilisez RAID-6 (préféré) ou RAID-50, au lieu de RAID-5.
-Lorsque vous utilisez RAID-5, RAID-6 ou RAID-50, augmentez toujours stripe_cache_size, car la valeur par défaut n'est généralement pas le meilleur choix.
-
-``` bash
-$ echo 4096 | sudo tee /sys/block/md2/md/stripe_cache_size
-```
-
-Calculez le nombre exact à partir du nombre de périphériques et de la taille du bloc, en utilisant la formule: `2 * num_devices * chunk_size_in_bytes / 4096`.
-
-Une taille de bloc de 1024 KO est suffisante pour toutes les configurations RAID.
-Ne définissez jamais la taille du bloc trop petite ou trop grande.
-
-Vous pouvez utiliser RAID-0 sur SSD.
-Quelle que soit L'utilisation du RAID, utilisez toujours la réplication pour la sécurité des données.
-
-Activer NCQ avec une longue file d'attente. Pour HDD, choisissez le planificateur CFQ, et pour SSD, choisissez noop. Ne pas réduire le ‘readahead’ paramètre.
-Pour le disque dur, activez le cache d'écriture.
-
-## Système De Fichiers {#file-system}
-
-Ext4 est l'option la plus fiable. Définir les options de montage `noatime, nobarrier`.
-XFS est également adapté, mais il n'a pas été aussi soigneusement testé avec ClickHouse.
-La plupart des autres systèmes de fichiers devraient également fonctionner correctement. Les systèmes de fichiers avec allocation retardée fonctionnent mieux.
-
-## Le Noyau Linux {#linux-kernel}
-
-N'utilisez pas un noyau Linux obsolète.
-
-## Réseau {#network}
-
-Si vous utilisez IPv6, augmenter la taille du cache.
-Le noyau Linux avant 3.2 avait une multitude de problèmes avec l'implémentation D'IPv6.
-
-Utilisez au moins un réseau de 10 Go, si possible. 1 Go fonctionnera également, mais ce sera bien pire pour patcher des répliques avec des dizaines de téraoctets de données, ou pour traiter des requêtes distribuées avec une grande quantité de données intermédiaires.
-
-## ZooKeeper {#zookeeper}
-
-Vous utilisez probablement déjà ZooKeeper à d'autres fins. Vous pouvez utiliser la même installation de ZooKeeper, si elle n'est pas déjà surchargée.
-
-It's best to use a fresh version of ZooKeeper – 3.4.9 or later. The version in stable Linux distributions may be outdated.
-
-Vous ne devez jamais utiliser de scripts écrits manuellement pour transférer des données entre différents clusters ZooKeeper, car le résultat sera incorrect pour les nœuds séquentiels. Ne jamais utiliser de l' “zkcopy” utilitaire pour la même raison: https://github.com/ksprojects/zkcopy/issues/15
-
-Si vous souhaitez diviser un cluster Zookeeper existant en deux, le bon moyen est d'augmenter le nombre de ses répliques, puis de le reconfigurer en deux clusters indépendants.
-
-N'exécutez pas ZooKeeper sur les mêmes serveurs que ClickHouse. Parce que ZooKeeper est très sensible à la latence et ClickHouse peut utiliser toutes les ressources système disponibles.
-
-Avec les paramètres par défaut, ZooKeeper est une bombe à retardement:
-
-> Le serveur ZooKeeper ne supprime pas les fichiers des anciens snapshots et journaux lors de l'utilisation de la configuration par défaut (voir autopurge), et c'est la responsabilité de l'opérateur.
-
-Cette bombe doit être désamorcée.
-
-La configuration ZooKeeper (3.5.1) ci-dessous est utilisée dans le Yandex.Environnement de production Metrica au 20 mai 2017:
-
-zoo.cfg:
-
-``` bash
-# http://hadoop.apache.org/zookeeper/docs/current/zookeeperAdmin.html
-
-# The number of milliseconds of each tick
-tickTime=2000
-# The number of ticks that the initial
-# synchronization phase can take
-initLimit=30000
-# The number of ticks that can pass between
-# sending a request and getting an acknowledgement
-syncLimit=10
-
-maxClientCnxns=2000
-
-maxSessionTimeout=60000000
-# the directory where the snapshot is stored.
-dataDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/data
-# Place the dataLogDir to a separate physical disc for better performance
-dataLogDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/logs
-
-autopurge.snapRetainCount=10
-autopurge.purgeInterval=1
-
-
-# To avoid seeks ZooKeeper allocates space in the transaction log file in
-# blocks of preAllocSize kilobytes. The default block size is 64M. One reason
-# for changing the size of the blocks is to reduce the block size if snapshots
-# are taken more often. (Also, see snapCount).
-preAllocSize=131072
-
-# Clients can submit requests faster than ZooKeeper can process them,
-# especially if there are a lot of clients. To prevent ZooKeeper from running
-# out of memory due to queued requests, ZooKeeper will throttle clients so that
-# there is no more than globalOutstandingLimit outstanding requests in the
-# system. The default limit is 1,000.ZooKeeper logs transactions to a
-# transaction log. After snapCount transactions are written to a log file a
-# snapshot is started and a new transaction log file is started. The default
-# snapCount is 10,000.
-snapCount=3000000
-
-# If this option is defined, requests will be will logged to a trace file named
-# traceFile.year.month.day.
-#traceFile=
-
-# Leader accepts client connections. Default value is "yes". The leader machine
-# coordinates updates. For higher update throughput at thes slight expense of
-# read throughput the leader can be configured to not accept clients and focus
-# on coordination.
-leaderServes=yes
-
-standaloneEnabled=false
-dynamicConfigFile=/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/zoo.cfg.dynamic
-```
-
-Version de Java:
-
-``` text
-Java(TM) SE Runtime Environment (build 1.8.0_25-b17)
-Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
-```
-
-Les paramètres de la JVM:
-
-``` bash
-NAME=zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}
-ZOOCFGDIR=/etc/$NAME/conf
-
-# TODO this is really ugly
-# How to find out, which jars are needed?
-# seems, that log4j requires the log4j.properties file to be in the classpath
-CLASSPATH="$ZOOCFGDIR:/usr/build/classes:/usr/build/lib/*.jar:/usr/share/zookeeper/zookeeper-3.5.1-metrika.jar:/usr/share/zookeeper/slf4j-log4j12-1.7.5.jar:/usr/share/zookeeper/slf4j-api-1.7.5.jar:/usr/share/zookeeper/servlet-api-2.5-20081211.jar:/usr/share/zookeeper/netty-3.7.0.Final.jar:/usr/share/zookeeper/log4j-1.2.16.jar:/usr/share/zookeeper/jline-2.11.jar:/usr/share/zookeeper/jetty-util-6.1.26.jar:/usr/share/zookeeper/jetty-6.1.26.jar:/usr/share/zookeeper/javacc.jar:/usr/share/zookeeper/jackson-mapper-asl-1.9.11.jar:/usr/share/zookeeper/jackson-core-asl-1.9.11.jar:/usr/share/zookeeper/commons-cli-1.2.jar:/usr/src/java/lib/*.jar:/usr/etc/zookeeper"
-
-ZOOCFG="$ZOOCFGDIR/zoo.cfg"
-ZOO_LOG_DIR=/var/log/$NAME
-USER=zookeeper
-GROUP=zookeeper
-PIDDIR=/var/run/$NAME
-PIDFILE=$PIDDIR/$NAME.pid
-SCRIPTNAME=/etc/init.d/$NAME
-JAVA=/usr/bin/java
-ZOOMAIN="org.apache.zookeeper.server.quorum.QuorumPeerMain"
-ZOO_LOG4J_PROP="INFO,ROLLINGFILE"
-JMXLOCALONLY=false
-JAVA_OPTS="-Xms{{ '{{' }} cluster.get('xms','128M') {{ '}}' }} \
-    -Xmx{{ '{{' }} cluster.get('xmx','1G') {{ '}}' }} \
-    -Xloggc:/var/log/$NAME/zookeeper-gc.log \
-    -XX:+UseGCLogFileRotation \
-    -XX:NumberOfGCLogFiles=16 \
-    -XX:GCLogFileSize=16M \
-    -verbose:gc \
-    -XX:+PrintGCTimeStamps \
-    -XX:+PrintGCDateStamps \
-    -XX:+PrintGCDetails
-    -XX:+PrintTenuringDistribution \
-    -XX:+PrintGCApplicationStoppedTime \
-    -XX:+PrintGCApplicationConcurrentTime \
-    -XX:+PrintSafepointStatistics \
-    -XX:+UseParNewGC \
-    -XX:+UseConcMarkSweepGC \
--XX:+CMSParallelRemarkEnabled"
-```
-
-Sel init:
-
-``` text
-description "zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} centralized coordination service"
-
-start on runlevel [2345]
-stop on runlevel [!2345]
-
-respawn
-
-limit nofile 8192 8192
-
-pre-start script
-    [ -r "/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment" ] || exit 0
-    . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment
-    [ -d $ZOO_LOG_DIR ] || mkdir -p $ZOO_LOG_DIR
-    chown $USER:$GROUP $ZOO_LOG_DIR
-end script
-
-script
-    . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment
-    [ -r /etc/default/zookeeper ] && . /etc/default/zookeeper
-    if [ -z "$JMXDISABLE" ]; then
-        JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=$JMXLOCALONLY"
-    fi
-    exec start-stop-daemon --start -c $USER --exec $JAVA --name zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} \
-        -- -cp $CLASSPATH $JAVA_OPTS -Dzookeeper.log.dir=${ZOO_LOG_DIR} \
-        -Dzookeeper.root.logger=${ZOO_LOG4J_PROP} $ZOOMAIN $ZOOCFG
-end script
-```
-
-{## [Article Original](https://clickhouse.tech/docs/en/operations/tips/) ##}
diff --git a/docs/fr/operations/troubleshooting.md b/docs/fr/operations/troubleshooting.md
deleted file mode 100644
index f7d3153a4d80..000000000000
--- a/docs/fr/operations/troubleshooting.md
+++ /dev/null
@@ -1,146 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: "D\xE9pannage"
----
-
-# Dépannage {#troubleshooting}
-
--   [Installation](#troubleshooting-installation-errors)
--   [Connexion au serveur](#troubleshooting-accepts-no-connections)
--   [Traitement des requêtes](#troubleshooting-does-not-process-queries)
--   [Efficacité du traitement des requêtes](#troubleshooting-too-slow)
-
-## Installation {#troubleshooting-installation-errors}
-
-### Vous ne pouvez pas obtenir de paquets deb à partir du référentiel ClickHouse avec Apt-get {#you-cannot-get-deb-packages-from-clickhouse-repository-with-apt-get}
-
--   Vérifiez les paramètres du pare-feu.
--   Si vous ne pouvez pas accéder au référentiel pour quelque raison que ce soit, téléchargez les packages comme décrit dans [Prise en main](../getting-started/index.md) article et les installer manuellement en utilisant le `sudo dpkg -i <packages>` commande. Vous aurez aussi besoin d' `tzdata` paquet.
-
-## Connexion au Serveur {#troubleshooting-accepts-no-connections}
-
-Problèmes possibles:
-
--   Le serveur n'est pas en cours d'exécution.
--   Paramètres de configuration inattendus ou incorrects.
-
-### Le Serveur N'Est Pas En Cours D'Exécution {#server-is-not-running}
-
-**Vérifiez si le serveur est runnnig**
-
-Commande:
-
-``` bash
-$ sudo service clickhouse-server status
-```
-
-Si le serveur n'est pas en cours d'exécution, démarrez-le avec la commande:
-
-``` bash
-$ sudo service clickhouse-server start
-```
-
-**Vérifier les journaux**
-
-Le journal principal de `clickhouse-server` est dans `/var/log/clickhouse-server/clickhouse-server.log` par défaut.
-
-Si le serveur a démarré avec succès, vous devriez voir les chaînes:
-
--   `<Information> Application: starting up.` — Server started.
--   `<Information> Application: Ready for connections.` — Server is running and ready for connections.
-
-Si `clickhouse-server` démarrage a échoué avec une erreur de configuration, vous devriez voir la `<Error>` chaîne avec une description de l'erreur. Exemple:
-
-``` text
-2019.01.11 15:23:25.549505 [ 45 ] {} <Error> ExternalDictionaries: Failed reloading 'event2id' external dictionary: Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused, e.what() = Connection refused
-```
-
-Si vous ne voyez pas d'erreur à la fin du fichier, parcourez le fichier entier à partir de la chaîne:
-
-``` text
-<Information> Application: starting up.
-```
-
-Si vous essayez de démarrer une deuxième instance de `clickhouse-server` sur le serveur, vous voyez le journal suivant:
-
-``` text
-2019.01.11 15:25:11.151730 [ 1 ] {} <Information> : Starting ClickHouse 19.1.0 with revision 54413
-2019.01.11 15:25:11.154578 [ 1 ] {} <Information> Application: starting up
-2019.01.11 15:25:11.156361 [ 1 ] {} <Information> StatusFile: Status file ./status already exists - unclean restart. Contents:
-PID: 8510
-Started at: 2019-01-11 15:24:23
-Revision: 54413
-
-2019.01.11 15:25:11.156673 [ 1 ] {} <Error> Application: DB::Exception: Cannot lock file ./status. Another server instance in same directory is already running.
-2019.01.11 15:25:11.156682 [ 1 ] {} <Information> Application: shutting down
-2019.01.11 15:25:11.156686 [ 1 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem
-2019.01.11 15:25:11.156716 [ 2 ] {} <Information> BaseDaemon: Stop SignalListener thread
-```
-
-**Voir système.d les journaux**
-
-Si vous ne trouvez aucune information utile dans `clickhouse-server` journaux ou il n'y a pas de journaux, vous pouvez afficher `system.d` journaux à l'aide de la commande:
-
-``` bash
-$ sudo journalctl -u clickhouse-server
-```
-
-**Démarrer clickhouse-server en mode interactif**
-
-``` bash
-$ sudo -u clickhouse /usr/bin/clickhouse-server --config-file /etc/clickhouse-server/config.xml
-```
-
-Cette commande démarre le serveur en tant qu'application interactive avec les paramètres standard du script de démarrage automatique. Dans ce mode `clickhouse-server` imprime tous les messages d'événement dans la console.
-
-### Paramètres De Configuration {#configuration-parameters}
-
-Vérifier:
-
--   Le panneau paramètres.
-
-    Si vous exécutez ClickHouse dans Docker dans un réseau IPv6, assurez-vous que `network=host` est définie.
-
--   Paramètres du point de terminaison.
-
-    Vérifier [listen_host](server-configuration-parameters/settings.md#server_configuration_parameters-listen_host) et [tcp_port](server-configuration-parameters/settings.md#server_configuration_parameters-tcp_port) paramètre.
-
-    Clickhouse server accepte les connexions localhost uniquement par défaut.
-
--   Paramètres du protocole HTTP.
-
-    Vérifiez les paramètres de protocole pour L'API HTTP.
-
--   Paramètres de connexion sécurisés.
-
-    Vérifier:
-
-    -   Le [tcp_port_secure](server-configuration-parameters/settings.md#server_configuration_parameters-tcp_port_secure) paramètre.
-    -   Paramètres pour [SSL sertificates](server-configuration-parameters/settings.md#server_configuration_parameters-openssl).
-
-    Utilisez les paramètres appropriés lors de la connexion. Par exemple, l'utilisation de la `port_secure` paramètre avec `clickhouse_client`.
-
--   Les paramètres de l'utilisateur.
-
-    Vous utilisez peut-être un mauvais nom d'utilisateur ou mot de passe.
-
-## Traitement Des Requêtes {#troubleshooting-does-not-process-queries}
-
-Si ClickHouse ne peut pas traiter la requête, il envoie une description d'erreur au client. Dans le `clickhouse-client` vous obtenez une description de l'erreur dans la console. Si vous utilisez L'interface HTTP, ClickHouse envoie la description de l'erreur dans le corps de la réponse. Exemple:
-
-``` bash
-$ curl 'http://localhost:8123/' --data-binary "SELECT a"
-Code: 47, e.displayText() = DB::Exception: Unknown identifier: a. Note that there are no tables (FROM clause) in your query, context: required_names: 'a' source_tables: table_aliases: private_aliases: column_aliases: public_columns: 'a' masked_columns: array_join_columns: source_columns: , e.what() = DB::Exception
-```
-
-Si vous commencez à `clickhouse-client` avec l' `stack-trace` paramètre, ClickHouse renvoie la trace de la pile du serveur avec la description d'une erreur.
-
-Vous pouvez voir un message sur une connexion rompue. Dans ce cas, vous pouvez répéter la requête. Si la connexion se rompt chaque fois que vous effectuez la requête, vérifiez les journaux du serveur pour détecter les erreurs.
-
-## Efficacité du traitement des requêtes {#troubleshooting-too-slow}
-
-Si vous voyez que ClickHouse fonctionne trop lentement, vous devez profiler la charge sur les ressources du serveur et le réseau pour vos requêtes.
-
-Vous pouvez utiliser l'utilitaire clickhouse-benchmark pour profiler les requêtes. Il indique le nombre de requêtes traitées par seconde, le nombre de lignes traitées par seconde, et les percentiles de temps de traitement des requêtes.
diff --git a/docs/fr/operations/update.md b/docs/fr/operations/update.md
deleted file mode 100644
index e1bfe57b0a34..000000000000
--- a/docs/fr/operations/update.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: "Mise \xC0 Jour De ClickHouse"
----
-
-# Mise À Jour De ClickHouse {#clickhouse-update}
-
-Si ClickHouse a été installé à partir de paquets deb, exécutez les commandes suivantes sur le serveur:
-
-``` bash
-$ sudo apt-get update
-$ sudo apt-get install clickhouse-client clickhouse-server
-$ sudo service clickhouse-server restart
-```
-
-Si vous avez installé ClickHouse en utilisant autre chose que les paquets deb recommandés, utilisez la méthode de mise à jour appropriée.
-
-ClickHouse ne prend pas en charge une mise à jour distribuée. L'opération doit être effectuée consécutivement sur chaque serveur séparé. Ne pas mettre à jour tous les serveurs d'un cluster simultanément, ou le cluster sera indisponible pendant un certain temps.
diff --git a/docs/fr/operations/utilities/clickhouse-benchmark.md b/docs/fr/operations/utilities/clickhouse-benchmark.md
deleted file mode 100644
index 975cb09814e5..000000000000
--- a/docs/fr/operations/utilities/clickhouse-benchmark.md
+++ /dev/null
@@ -1,156 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: clickhouse-benchmark
----
-
-# clickhouse-benchmark {#clickhouse-benchmark}
-
-Se connecte à un serveur ClickHouse et envoie à plusieurs reprises des requêtes spécifiées.
-
-Syntaxe:
-
-``` bash
-$ echo "single query" | clickhouse-benchmark [keys]
-```
-
-ou
-
-``` bash
-$ clickhouse-benchmark [keys] <<< "single query"
-```
-
-Si vous souhaitez envoyer un ensemble de requêtes, créez un fichier texte et placez chaque requête sur la chaîne individuelle de ce fichier. Exemple:
-
-``` sql
-SELECT * FROM system.numbers LIMIT 10000000
-SELECT 1
-```
-
-Ensuite, passez ce fichier à une entrée standard de `clickhouse-benchmark`.
-
-``` bash
-clickhouse-benchmark [keys] < queries_file
-```
-
-## Touches {#clickhouse-benchmark-keys}
-
--   `-c N`, `--concurrency=N` — Number of queries that `clickhouse-benchmark` envoie simultanément. Valeur par défaut: 1.
--   `-d N`, `--delay=N` — Interval in seconds between intermediate reports (set 0 to disable reports). Default value: 1.
--   `-h WORD`, `--host=WORD` — Server host. Default value: `localhost`. Pour l' [mode de comparaison](#clickhouse-benchmark-comparison-mode) vous pouvez utiliser plusieurs `-h` touches.
--   `-p N`, `--port=N` — Server port. Default value: 9000. For the [mode de comparaison](#clickhouse-benchmark-comparison-mode) vous pouvez utiliser plusieurs `-p` touches.
--   `-i N`, `--iterations=N` — Total number of queries. Default value: 0.
--   `-r`, `--randomize` — Random order of queries execution if there is more then one input query.
--   `-s`, `--secure` — Using TLS connection.
--   `-t N`, `--timelimit=N` — Time limit in seconds. `clickhouse-benchmark` arrête d'envoyer des requêtes lorsque le délai spécifié est atteint. Valeur par défaut: 0 (limite de temps désactivée).
--   `--confidence=N` — Level of confidence for T-test. Possible values: 0 (80%), 1 (90%), 2 (95%), 3 (98%), 4 (99%), 5 (99.5%). Default value: 5. In the [mode de comparaison](#clickhouse-benchmark-comparison-mode) `clickhouse-benchmark` effectue les [Test T de L'étudiant indépendant à deux échantillons](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) tester pour déterminer si les deux distributions ne sont pas différentes avec le niveau de confiance sélectionné.
--   `--cumulative` — Printing cumulative data instead of data per interval.
--   `--database=DATABASE_NAME` — ClickHouse database name. Default value: `default`.
--   `--json=FILEPATH` — JSON output. When the key is set, `clickhouse-benchmark` génère un rapport dans le fichier JSON spécifié.
--   `--user=USERNAME` — ClickHouse user name. Default value: `default`.
--   `--password=PSWD` — ClickHouse user password. Default value: empty string.
--   `--stacktrace` — Stack traces output. When the key is set, `clickhouse-bencmark` affiche les traces d'exceptions de la pile.
--   `--stage=WORD` — Query processing stage at server. ClickHouse stops query processing and returns answer to `clickhouse-benchmark` à l'étape spécifiée. Valeurs possibles: `complete`, `fetch_columns`, `with_mergeable_state`. Valeur par défaut: `complete`.
--   `--help` — Shows the help message.
-
-Si vous voulez appliquer des [paramètre](../../operations/settings/index.md) pour les requêtes, les passer comme une clé `--<session setting name>= SETTING_VALUE`. Exemple, `--max_memory_usage=1048576`.
-
-## Sortie {#clickhouse-benchmark-output}
-
-Par défaut, `clickhouse-benchmark` les rapports de chaque `--delay` intervalle.
-
-Exemple de rapport:
-
-``` text
-Queries executed: 10.
-
-localhost:9000, queries 10, QPS: 6.772, RPS: 67904487.440, MiB/s: 518.070, result RPS: 67721584.984, result MiB/s: 516.675.
-
-0.000%      0.145 sec.
-10.000%     0.146 sec.
-20.000%     0.146 sec.
-30.000%     0.146 sec.
-40.000%     0.147 sec.
-50.000%     0.148 sec.
-60.000%     0.148 sec.
-70.000%     0.148 sec.
-80.000%     0.149 sec.
-90.000%     0.150 sec.
-95.000%     0.150 sec.
-99.000%     0.150 sec.
-99.900%     0.150 sec.
-99.990%     0.150 sec.
-```
-
-Dans le rapport, vous pouvez trouver:
-
--   Nombre de requêtes dans le `Queries executed:` champ.
-
--   Le statut de chaîne de caractères contenant (dans l'ordre):
-
-    -   Point de terminaison du serveur ClickHouse.
-    -   Nombre de requêtes traitées.
-    -   QPS: QPS: combien de requêtes serveur effectuées par seconde pendant une période spécifiée dans le `--delay` argument.
-    -   RPS: combien de lignes le serveur a lues par seconde pendant une période spécifiée dans `--delay` argument.
-    -   MiB/ s: combien de mebibytes serveur lus par seconde pendant une période spécifiée dans le `--delay` argument.
-    -   résultat RPS: combien de lignes placées par le serveur au résultat d'une requête par seconde pendant une période spécifiée dans le `--delay` argument.
-    -   MiB / s de résultat. combien de mibibytes placés par le serveur au résultat d'une requête par seconde pendant une période spécifiée dans `--delay` argument.
-
--   Percentiles du temps d'exécution des requêtes.
-
-## Mode De Comparaison {#clickhouse-benchmark-comparison-mode}
-
-`clickhouse-benchmark` peut comparer les performances pour deux serveurs clickhouse en cours d'exécution.
-
-Pour utiliser le mode de comparaison, spécifiez les points de terminaison des deux serveurs par deux paires de `--host`, `--port` touches. Clés appariées ensemble par position dans la liste des arguments, la première `--host` est assorti avec le premier `--port` et ainsi de suite. `clickhouse-benchmark` établit les connexions aux serveurs, puis envoie des requêtes. Chaque requête adressée à un serveur sélectionné au hasard. Les résultats sont présentés pour chaque serveur séparément.
-
-## Exemple {#clickhouse-benchmark-example}
-
-``` bash
-$ echo "SELECT * FROM system.numbers LIMIT 10000000 OFFSET 10000000" | clickhouse-benchmark -i 10
-```
-
-``` text
-Loaded 1 queries.
-
-Queries executed: 6.
-
-localhost:9000, queries 6, QPS: 6.153, RPS: 123398340.957, MiB/s: 941.455, result RPS: 61532982.200, result MiB/s: 469.459.
-
-0.000%      0.159 sec.
-10.000%     0.159 sec.
-20.000%     0.159 sec.
-30.000%     0.160 sec.
-40.000%     0.160 sec.
-50.000%     0.162 sec.
-60.000%     0.164 sec.
-70.000%     0.165 sec.
-80.000%     0.166 sec.
-90.000%     0.166 sec.
-95.000%     0.167 sec.
-99.000%     0.167 sec.
-99.900%     0.167 sec.
-99.990%     0.167 sec.
-
-
-
-Queries executed: 10.
-
-localhost:9000, queries 10, QPS: 6.082, RPS: 121959604.568, MiB/s: 930.478, result RPS: 60815551.642, result MiB/s: 463.986.
-
-0.000%      0.159 sec.
-10.000%     0.159 sec.
-20.000%     0.160 sec.
-30.000%     0.163 sec.
-40.000%     0.164 sec.
-50.000%     0.165 sec.
-60.000%     0.166 sec.
-70.000%     0.166 sec.
-80.000%     0.167 sec.
-90.000%     0.167 sec.
-95.000%     0.170 sec.
-99.000%     0.172 sec.
-99.900%     0.172 sec.
-99.990%     0.172 sec.
-```
diff --git a/docs/fr/operations/utilities/clickhouse-copier.md b/docs/fr/operations/utilities/clickhouse-copier.md
deleted file mode 100644
index da96c41e8f2c..000000000000
--- a/docs/fr/operations/utilities/clickhouse-copier.md
+++ /dev/null
@@ -1,176 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: clickhouse-copieur
----
-
-# clickhouse-copieur {#clickhouse-copier}
-
-Copie les données des tables d'un cluster vers des tables d'un autre cluster (ou du même cluster).
-
-Vous pouvez exécuter plusieurs `clickhouse-copier` instances sur différents serveurs pour effectuer le même travail. ZooKeeper est utilisé pour synchroniser les processus.
-
-Après le démarrage de, `clickhouse-copier`:
-
--   Se connecte à ZooKeeper et reçoit:
-
-    -   La copie de tâches.
-    -   L'état de la copie d'emplois.
-
--   Il effectue les travaux.
-
-    Chaque processus en cours choisit le “closest” eclat du cluster source et copie les données dans le cluster de destination, la refragmentation les données si nécessaire.
-
-`clickhouse-copier` suit les changements dans ZooKeeper et les applique à la volée.
-
-Pour réduire le trafic réseau, nous vous recommandons de `clickhouse-copier` sur le même serveur où se trouvent les données source.
-
-## Course Clickhouse-copieur {#running-clickhouse-copier}
-
-L'utilitaire doit être exécuté manuellement:
-
-``` bash
-$ clickhouse-copier copier --daemon --config zookeeper.xml --task-path /task/path --base-dir /path/to/dir
-```
-
-Paramètre:
-
--   `daemon` — Starts `clickhouse-copier` en mode démon.
--   `config` — The path to the `zookeeper.xml` fichier avec les paramètres pour la connexion à la Gardienne.
--   `task-path` — The path to the ZooKeeper node. This node is used for syncing `clickhouse-copier` processus et stockage des tâches. Les tâches sont stockées dans `$task-path/description`.
--   `task-file` — Optional path to file with task configuration for initial upload to ZooKeeper.
--   `task-upload-force` — Force upload `task-file` même si le nœud existe déjà.
--   `base-dir` — The path to logs and auxiliary files. When it starts, `clickhouse-copier` crée `clickhouse-copier_YYYYMMHHSS_<PID>` les sous-répertoires `$base-dir`. Si ce paramètre est omis, les répertoires sont créés dans le répertoire où `clickhouse-copier` a été lancé.
-
-## Format de Zookeeper.XML {#format-of-zookeeper-xml}
-
-``` xml
-<yandex>
-    <logger>
-        <level>trace</level>
-        <size>100M</size>
-        <count>3</count>
-    </logger>
-
-    <zookeeper>
-        <node index="1">
-            <host>127.0.0.1</host>
-            <port>2181</port>
-        </node>
-    </zookeeper>
-</yandex>
-```
-
-## Configuration des tâches de copie {#configuration-of-copying-tasks}
-
-``` xml
-<yandex>
-    <!-- Configuration of clusters as in an ordinary server config -->
-    <remote_servers>
-        <source_cluster>
-            <shard>
-                <internal_replication>false</internal_replication>
-                    <replica>
-                        <host>127.0.0.1</host>
-                        <port>9000</port>
-                    </replica>
-            </shard>
-            ...
-        </source_cluster>
-
-        <destination_cluster>
-        ...
-        </destination_cluster>
-    </remote_servers>
-
-    <!-- How many simultaneously active workers are possible. If you run more workers superfluous workers will sleep. -->
-    <max_workers>2</max_workers>
-
-    <!-- Setting used to fetch (pull) data from source cluster tables -->
-    <settings_pull>
-        <readonly>1</readonly>
-    </settings_pull>
-
-    <!-- Setting used to insert (push) data to destination cluster tables -->
-    <settings_push>
-        <readonly>0</readonly>
-    </settings_push>
-
-    <!-- Common setting for fetch (pull) and insert (push) operations. Also, copier process context uses it.
-         They are overlaid by <settings_pull/> and <settings_push/> respectively. -->
-    <settings>
-        <connect_timeout>3</connect_timeout>
-        <!-- Sync insert is set forcibly, leave it here just in case. -->
-        <insert_distributed_sync>1</insert_distributed_sync>
-    </settings>
-
-    <!-- Copying tasks description.
-         You could specify several table task in the same task description (in the same ZooKeeper node), they will be performed
-         sequentially.
-    -->
-    <tables>
-        <!-- A table task, copies one table. -->
-        <table_hits>
-            <!-- Source cluster name (from <remote_servers/> section) and tables in it that should be copied -->
-            <cluster_pull>source_cluster</cluster_pull>
-            <database_pull>test</database_pull>
-            <table_pull>hits</table_pull>
-
-            <!-- Destination cluster name and tables in which the data should be inserted -->
-            <cluster_push>destination_cluster</cluster_push>
-            <database_push>test</database_push>
-            <table_push>hits2</table_push>
-
-            <!-- Engine of destination tables.
-                 If destination tables have not be created, workers create them using columns definition from source tables and engine
-                 definition from here.
-
-                 NOTE: If the first worker starts insert data and detects that destination partition is not empty then the partition will
-                 be dropped and refilled, take it into account if you already have some data in destination tables. You could directly
-                 specify partitions that should be copied in <enabled_partitions/>, they should be in quoted format like partition column of
-                 system.parts table.
-            -->
-            <engine>
-            ENGINE=ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/hits2', '{replica}')
-            PARTITION BY toMonday(date)
-            ORDER BY (CounterID, EventDate)
-            </engine>
-
-            <!-- Sharding key used to insert data to destination cluster -->
-            <sharding_key>jumpConsistentHash(intHash64(UserID), 2)</sharding_key>
-
-            <!-- Optional expression that filter data while pull them from source servers -->
-            <where_condition>CounterID != 0</where_condition>
-
-            <!-- This section specifies partitions that should be copied, other partition will be ignored.
-                 Partition names should have the same format as
-                 partition column of system.parts table (i.e. a quoted text).
-                 Since partition key of source and destination cluster could be different,
-                 these partition names specify destination partitions.
-
-                 NOTE: In spite of this section is optional (if it is not specified, all partitions will be copied),
-                 it is strictly recommended to specify them explicitly.
-                 If you already have some ready partitions on destination cluster they
-                 will be removed at the start of the copying since they will be interpeted
-                 as unfinished data from the previous copying!!!
-            -->
-            <enabled_partitions>
-                <partition>'2018-02-26'</partition>
-                <partition>'2018-03-05'</partition>
-                ...
-            </enabled_partitions>
-        </table_hits>
-
-        <!-- Next table to copy. It is not copied until previous table is copying. -->
-        </table_visits>
-        ...
-        </table_visits>
-        ...
-    </tables>
-</yandex>
-```
-
-`clickhouse-copier` suit les changements dans `/task/path/description` et les applique à la volée. Par exemple, si vous modifiez la valeur de `max_workers`, le nombre de processus exécutant des tâches changera également.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/utils/clickhouse-copier/) <!--hide-->
diff --git a/docs/fr/operations/utilities/clickhouse-local.md b/docs/fr/operations/utilities/clickhouse-local.md
deleted file mode 100644
index 5dfac0d89caf..000000000000
--- a/docs/fr/operations/utilities/clickhouse-local.md
+++ /dev/null
@@ -1,81 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: clickhouse-local
----
-
-# clickhouse-local {#clickhouse-local}
-
-Le `clickhouse-local` programme vous permet d'effectuer un traitement rapide sur les fichiers locaux, sans avoir à déployer et configurer le serveur ClickHouse.
-
-Accepte les données qui représentent des tables et les interroge en utilisant [Clickhouse dialecte SQL](../../sql-reference/index.md).
-
-`clickhouse-local` utilise le même noyau que clickhouse server, de sorte qu'il prend en charge la plupart des fonctionnalités et le même ensemble de formats et de moteurs de table.
-
-Par défaut `clickhouse-local` n'a pas accès aux données sur le même hôte, mais il prend en charge le chargement de la configuration du serveur à l'aide `--config-file` argument.
-
-!!! warning "Avertissement"
-    Il n'est pas recommandé de charger la configuration du serveur de production dans `clickhouse-local` parce que les données peuvent être endommagées en cas d'erreur humaine.
-
-## Utilisation {#usage}
-
-Utilisation de base:
-
-``` bash
-$ clickhouse-local --structure "table_structure" --input-format "format_of_incoming_data" -q "query"
-```
-
-Argument:
-
--   `-S`, `--structure` — table structure for input data.
--   `-if`, `--input-format` — input format, `TSV` par défaut.
--   `-f`, `--file` — path to data, `stdin` par défaut.
--   `-q` `--query` — queries to execute with `;` comme délimiteur.
--   `-N`, `--table` — table name where to put output data, `table` par défaut.
--   `-of`, `--format`, `--output-format` — output format, `TSV` par défaut.
--   `--stacktrace` — whether to dump debug output in case of exception.
--   `--verbose` — more details on query execution.
--   `-s` — disables `stderr` journalisation.
--   `--config-file` — path to configuration file in same format as for ClickHouse server, by default the configuration empty.
--   `--help` — arguments references for `clickhouse-local`.
-
-Il existe également des arguments pour chaque variable de configuration de ClickHouse qui sont plus couramment utilisés à la place de `--config-file`.
-
-## Exemple {#examples}
-
-``` bash
-$ echo -e "1,2
3,4" | clickhouse-local -S "a Int64, b Int64" -if "CSV" -q "SELECT * FROM table"
-Read 2 rows, 32.00 B in 0.000 sec., 5182 rows/sec., 80.97 KiB/sec.
-1   2
-3   4
-```
-
-Exemple précédent est le même que:
-
-``` bash
-$ echo -e "1,2
3,4" | clickhouse-local -q "CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table"
-Read 2 rows, 32.00 B in 0.000 sec., 4987 rows/sec., 77.93 KiB/sec.
-1   2
-3   4
-```
-
-Maintenant, nous allons sortie utilisateur de mémoire pour chaque utilisateur Unix:
-
-``` bash
-$ ps aux | tail -n +2 | awk '{ printf("%s\t%s
", $1, $4) }' | clickhouse-local -S "user String, mem Float64" -q "SELECT user, round(sum(mem), 2) as memTotal FROM table GROUP BY user ORDER BY memTotal DESC FORMAT Pretty"
-```
-
-``` text
-Read 186 rows, 4.15 KiB in 0.035 sec., 5302 rows/sec., 118.34 KiB/sec.
-┏━━━━━━━━━━┳━━━━━━━━━━┓
-┃ user     ┃ memTotal ┃
-┡━━━━━━━━━━╇━━━━━━━━━━┩
-│ bayonet  │    113.5 │
-├──────────┼──────────┤
-│ root     │      8.8 │
-├──────────┼──────────┤
-...
-```
-
-[Article Original](https://clickhouse.tech/docs/en/operations/utils/clickhouse-local/) <!--hide-->
diff --git a/docs/fr/operations/utilities/index.md b/docs/fr/operations/utilities/index.md
deleted file mode 100644
index 51a250bb9963..000000000000
--- a/docs/fr/operations/utilities/index.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Utilitaire
-toc_priority: 56
-toc_title: "Aper\xE7u"
----
-
-# Clickhouse Utilitaire {#clickhouse-utility}
-
--   [clickhouse-local](clickhouse-local.md) — Allows running SQL queries on data without stopping the ClickHouse server, similar to how `awk` ne ce.
--   [clickhouse-copieur](clickhouse-copier.md) — Copies (and reshards) data from one cluster to another cluster.
--   [clickhouse-benchmark](clickhouse-benchmark.md) — Loads server with the custom queries and settings.
-
-[Article Original](https://clickhouse.tech/docs/en/operations/utils/) <!--hide-->
diff --git a/docs/fr/sql-reference/aggregate-functions/combinators.md b/docs/fr/sql-reference/aggregate-functions/combinators.md
deleted file mode 100644
index 1aabd34051bf..000000000000
--- a/docs/fr/sql-reference/aggregate-functions/combinators.md
+++ /dev/null
@@ -1,245 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: Combinators
----
-
-# Combinateurs De Fonction D'Agrégat {#aggregate_functions_combinators}
-
-Le nom d'une fonction d'agrégat peut avoir un suffixe ajouté. Cela change la façon dont la fonction d'agrégation fonctionne.
-
-## -Si {#agg-functions-combinator-if}
-
-The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument – a condition (Uint8 type). The aggregate function processes only the rows that trigger the condition. If the condition was not triggered even once, it returns a default value (usually zeros or empty strings).
-
-Exemple: `sumIf(column, cond)`, `countIf(cond)`, `avgIf(x, cond)`, `quantilesTimingIf(level1, level2)(x, cond)`, `argMinIf(arg, val, cond)` et ainsi de suite.
-
-Avec les fonctions d'agrégat conditionnel, vous pouvez calculer des agrégats pour plusieurs conditions à la fois, sans utiliser de sous-requêtes et `JOIN`s. Par exemple, dans Yandex.Metrica, les fonctions d'agrégat conditionnel sont utilisées pour implémenter la fonctionnalité de comparaison de segment.
-
-## -Tableau {#agg-functions-combinator-array}
-
-Le Tableau suffixe peut être ajouté à toute fonction d'agrégation. Dans ce cas, la fonction d'agrégation des arguments de la ‘Array(T)’ type (tableaux) au lieu de ‘T’ tapez les arguments. Si la fonction aggregate accepte plusieurs arguments, il doit s'agir de tableaux de longueurs égales. Lors du traitement des tableaux, la fonction d'agrégation fonctionne comme la fonction d'agrégation d'origine sur tous les éléments du tableau.
-
-Exemple 1: `sumArray(arr)` - Totalise tous les éléments de tous ‘arr’ tableau. Dans cet exemple, il aurait pu être écrit plus simplement: `sum(arraySum(arr))`.
-
-Exemple 2: `uniqArray(arr)` – Counts the number of unique elements in all ‘arr’ tableau. Cela pourrait être fait d'une manière plus facile: `uniq(arrayJoin(arr))` mais ce n'est pas toujours possible d'ajouter des ‘arrayJoin’ pour une requête.
-
-\- Si et-tableau peut être combiné. Cependant, ‘Array’ doit venir en premier, puis ‘If’. Exemple: `uniqArrayIf(arr, cond)`, `quantilesTimingArrayIf(level1, level2)(arr, cond)`. En raison de cet ordre, le ‘cond’ argument ne sera pas un tableau.
-
-## -État {#agg-functions-combinator-state}
-
-Si vous appliquez ce combinateur, la fonction d'agrégation ne renvoie pas la valeur résultante (par exemple le nombre de valeurs uniques pour [uniq](reference.md#agg_function-uniq) la fonction), mais un état intermédiaire de l'agrégation (pour `uniq`, c'est la table de hachage pour calculer le nombre de valeurs uniques). C'est un `AggregateFunction(...)` qui peuvent être utilisés pour un traitement ultérieur ou stockés dans un tableau pour terminer l'agrégation plus tard.
-
-Pour travailler avec ces états, utilisez:
-
--   [AggregatingMergeTree](../../engines/table-engines/mergetree-family/aggregatingmergetree.md) tableau moteur.
--   [finalizeAggregation](../../sql-reference/functions/other-functions.md#function-finalizeaggregation) fonction.
--   [runningAccumulate](../../sql-reference/functions/other-functions.md#function-runningaccumulate) fonction.
--   [-Fusionner](#aggregate_functions_combinators-merge) combinator.
--   [-MergeState](#aggregate_functions_combinators-mergestate) combinator.
-
-## -Fusionner {#aggregate_functions_combinators-merge}
-
-Si vous appliquez ce combinateur, la fonction d'agrégation prend l'état d'agrégation intermédiaire comme argument, combine les États pour terminer l'agrégation et renvoie la valeur résultante.
-
-## -MergeState {#aggregate_functions_combinators-mergestate}
-
-Fusionne les États d'agrégation intermédiaires de la même manière que le combinateur-Merge. Cependant, il ne renvoie pas la valeur résultante, mais un État d'agrégation intermédiaire, similaire au combinateur-State.
-
-## - ForEach {#agg-functions-combinator-foreach}
-
-Convertit une fonction d'agrégation pour les tables en une fonction d'agrégation pour les tableaux qui agrège les éléments de tableau correspondants et renvoie un tableau de résultats. Exemple, `sumForEach` pour les tableaux `[1, 2]`, `[3, 4, 5]`et`[6, 7]`renvoie le résultat `[10, 13, 5]` après avoir additionné les éléments de tableau correspondants.
-
-## - OrDefault {#agg-functions-combinator-ordefault}
-
-Modifie le comportement d'une fonction d'agrégat.
-
-Si une fonction d'agrégation n'a pas de valeurs d'entrée, avec ce combinateur, elle renvoie la valeur par défaut pour son type de données de retour. S'applique aux fonctions d'agrégation qui peuvent prendre des données d'entrée vides.
-
-`-OrDefault` peut être utilisé avec d'autres combinators.
-
-**Syntaxe**
-
-``` sql
-<aggFunction>OrDefault(x)
-```
-
-**Paramètre**
-
--   `x` — Aggregate function parameters.
-
-**Valeurs renvoyées**
-
-Renvoie la valeur par défaut du type de retour d'une fonction d'agrégation s'il n'y a rien à agréger.
-
-Le Type dépend de la fonction d'agrégation utilisée.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT avg(number), avgOrDefault(number) FROM numbers(0)
-```
-
-Résultat:
-
-``` text
-┌─avg(number)─┬─avgOrDefault(number)─┐
-│         nan │                    0 │
-└─────────────┴──────────────────────┘
-```
-
-Également `-OrDefault` peut être utilisé avec un autre combinateur. Il est utile lorsque la fonction d'agrégation n'accepte pas l'entrée vide.
-
-Requête:
-
-``` sql
-SELECT avgOrDefaultIf(x, x > 10)
-FROM
-(
-    SELECT toDecimal32(1.23, 2) AS x
-)
-```
-
-Résultat:
-
-``` text
-┌─avgOrDefaultIf(x, greater(x, 10))─┐
-│                              0.00 │
-└───────────────────────────────────┘
-```
-
-## - OrNull {#agg-functions-combinator-ornull}
-
-Modifie le comportement d'une fonction d'agrégat.
-
-Ce combinateur convertit un résultat d'une fonction d'agrégation à l' [Nullable](../data-types/nullable.md) type de données. Si la fonction d'agrégation n'a pas de valeurs à calculer elle renvoie [NULL](../syntax.md#null-literal).
-
-`-OrNull` peut être utilisé avec d'autres combinators.
-
-**Syntaxe**
-
-``` sql
-<aggFunction>OrNull(x)
-```
-
-**Paramètre**
-
--   `x` — Aggregate function parameters.
-
-**Valeurs renvoyées**
-
--   Le résultat de la fonction d'agrégat, converti en `Nullable` type de données.
--   `NULL` si il n'y a rien à s'agréger.
-
-Type: `Nullable(aggregate function return type)`.
-
-**Exemple**
-
-Ajouter `-orNull` à la fin de la fonction d'agrégation.
-
-Requête:
-
-``` sql
-SELECT sumOrNull(number), toTypeName(sumOrNull(number)) FROM numbers(10) WHERE number > 10
-```
-
-Résultat:
-
-``` text
-┌─sumOrNull(number)─┬─toTypeName(sumOrNull(number))─┐
-│              ᴺᵁᴸᴸ │ Nullable(UInt64)              │
-└───────────────────┴───────────────────────────────┘
-```
-
-Également `-OrNull` peut être utilisé avec un autre combinateur. Il est utile lorsque la fonction d'agrégation n'accepte pas l'entrée vide.
-
-Requête:
-
-``` sql
-SELECT avgOrNullIf(x, x > 10)
-FROM
-(
-    SELECT toDecimal32(1.23, 2) AS x
-)
-```
-
-Résultat:
-
-``` text
-┌─avgOrNullIf(x, greater(x, 10))─┐
-│                           ᴺᵁᴸᴸ │
-└────────────────────────────────┘
-```
-
-## -Resample {#agg-functions-combinator-resample}
-
-Permet de diviser les données en groupes, puis séparément agrège les données de ces groupes. Les groupes sont créés en divisant les valeurs d'une colonne en intervalles.
-
-``` sql
-<aggFunction>Resample(start, end, step)(<aggFunction_params>, resampling_key)
-```
-
-**Paramètre**
-
--   `start` — Starting value of the whole required interval for `resampling_key` valeur.
--   `stop` — Ending value of the whole required interval for `resampling_key` valeur. L'ensemble de l'intervalle ne comprend pas les `stop` valeur `[start, stop)`.
--   `step` — Step for separating the whole interval into subintervals. The `aggFunction` est exécuté sur chacun de ces sous-intervalles indépendamment.
--   `resampling_key` — Column whose values are used for separating data into intervals.
--   `aggFunction_params` — `aggFunction` paramètre.
-
-**Valeurs renvoyées**
-
--   Tableau de `aggFunction` les résultats pour chaque subinterval.
-
-**Exemple**
-
-Envisager l' `people` le tableau avec les données suivantes:
-
-``` text
-┌─name───┬─age─┬─wage─┐
-│ John   │  16 │   10 │
-│ Alice  │  30 │   15 │
-│ Mary   │  35 │    8 │
-│ Evelyn │  48 │ 11.5 │
-│ David  │  62 │  9.9 │
-│ Brian  │  60 │   16 │
-└────────┴─────┴──────┘
-```
-
-Obtenons les noms des personnes dont l'âge se trouve dans les intervalles de `[30,60)` et `[60,75)`. Puisque nous utilisons la représentation entière pour l'âge, nous obtenons des âges dans le `[30, 59]` et `[60,74]` intervalle.
-
-Pour agréger des noms dans un tableau, nous utilisons [grouperay](reference.md#agg_function-grouparray) fonction d'agrégation. Il faut un argument. Dans notre cas, c'est l' `name` colonne. Le `groupArrayResample` fonction devrait utiliser le `age` colonne pour agréger les noms par âge. Pour définir les intervalles requis, nous passons le `30, 75, 30` des arguments dans la `groupArrayResample` fonction.
-
-``` sql
-SELECT groupArrayResample(30, 75, 30)(name, age) FROM people
-```
-
-``` text
-┌─groupArrayResample(30, 75, 30)(name, age)─────┐
-│ [['Alice','Mary','Evelyn'],['David','Brian']] │
-└───────────────────────────────────────────────┘
-```
-
-Considérez les résultats.
-
-`Jonh` est hors de l'échantillon parce qu'il est trop jeune. D'autres personnes sont distribués selon les intervalles d'âge.
-
-Maintenant, nous allons compter le nombre total de personnes et leur salaire moyen dans les intervalles d'âge.
-
-``` sql
-SELECT
-    countResample(30, 75, 30)(name, age) AS amount,
-    avgResample(30, 75, 30)(wage, age) AS avg_wage
-FROM people
-```
-
-``` text
-┌─amount─┬─avg_wage──────────────────┐
-│ [3,2]  │ [11.5,12.949999809265137] │
-└────────┴───────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/agg_functions/combinators/) <!--hide-->
diff --git a/docs/fr/sql-reference/aggregate-functions/index.md b/docs/fr/sql-reference/aggregate-functions/index.md
deleted file mode 100644
index 4f4e3f057aee..000000000000
--- a/docs/fr/sql-reference/aggregate-functions/index.md
+++ /dev/null
@@ -1,62 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Les Fonctions D'Agr\xE9gation"
-toc_priority: 33
-toc_title: Introduction
----
-
-# Les Fonctions D'Agrégation {#aggregate-functions}
-
-Les fonctions d'agrégation fonctionnent dans le [normal](http://www.sql-tutorial.com/sql-aggregate-functions-sql-tutorial) comme prévu par les experts de la base de données.
-
-Clickhouse prend également en charge:
-
--   [Fonctions d'agrégat paramétriques](parametric-functions.md#aggregate_functions_parametric) qui acceptent d'autres paramètres en plus des colonnes.
--   [Combinators](combinators.md#aggregate_functions_combinators), qui modifient le comportement des fonctions d'agrégation.
-
-## Le Traitement NULL {#null-processing}
-
-Au cours de l'agrégation, tous les `NULL`s sont ignorés.
-
-**Exemple:**
-
-Considérez ce tableau:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │    2 │
-│ 2 │ ᴺᵁᴸᴸ │
-│ 3 │    2 │
-│ 3 │    3 │
-│ 3 │ ᴺᵁᴸᴸ │
-└───┴──────┘
-```
-
-Disons que vous devez totaliser les valeurs dans le `y` colonne:
-
-``` sql
-SELECT sum(y) FROM t_null_big
-```
-
-    ┌─sum(y)─┐
-    │      7 │
-    └────────┘
-
-Le `sum` la fonction d'interprète `NULL` comme `0`. En particulier, cela signifie que si la fonction reçoit en entrée d'une sélection où toutes les valeurs sont `NULL`, alors le résultat sera `0`, pas `NULL`.
-
-Maintenant, vous pouvez utiliser le `groupArray` fonction pour créer un tableau à partir `y` colonne:
-
-``` sql
-SELECT groupArray(y) FROM t_null_big
-```
-
-``` text
-┌─groupArray(y)─┐
-│ [2,2,3]       │
-└───────────────┘
-```
-
-`groupArray` ne comprend pas `NULL` dans le tableau résultant.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/agg_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/aggregate-functions/parametric-functions.md b/docs/fr/sql-reference/aggregate-functions/parametric-functions.md
deleted file mode 100644
index ed00ad7a0a38..000000000000
--- a/docs/fr/sql-reference/aggregate-functions/parametric-functions.md
+++ /dev/null
@@ -1,499 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: "Param\xE9trique"
----
-
-# Fonctions D'Agrégat Paramétriques {#aggregate_functions_parametric}
-
-Some aggregate functions can accept not only argument columns (used for compression), but a set of parameters – constants for initialization. The syntax is two pairs of brackets instead of one. The first is for parameters, and the second is for arguments.
-
-## histogramme {#histogram}
-
-Calcule un histogramme adaptatif. Cela ne garantit pas des résultats précis.
-
-``` sql
-histogram(number_of_bins)(values)
-```
-
-Les fonctions utilise [Un Algorithme D'Arbre De Décision Parallèle En Continu](http://jmlr.org/papers/volume11/ben-haim10a/ben-haim10a.pdf). Les bordures des bacs d'histogramme sont ajustées au fur et à mesure que de nouvelles données entrent dans une fonction. Dans le cas courant, les largeurs des bacs ne sont pas égales.
-
-**Paramètre**
-
-`number_of_bins` — Upper limit for the number of bins in the histogram. The function automatically calculates the number of bins. It tries to reach the specified number of bins, but if it fails, it uses fewer bins.
-`values` — [Expression](../syntax.md#syntax-expressions) résultant en valeurs d'entrée.
-
-**Valeurs renvoyées**
-
--   [Tableau](../../sql-reference/data-types/array.md) de [Tuple](../../sql-reference/data-types/tuple.md) de le format suivant:
-
-        ```
-        [(lower_1, upper_1, height_1), ... (lower_N, upper_N, height_N)]
-        ```
-
-        - `lower` — Lower bound of the bin.
-        - `upper` — Upper bound of the bin.
-        - `height` — Calculated height of the bin.
-
-**Exemple**
-
-``` sql
-SELECT histogram(5)(number + 1)
-FROM (
-    SELECT *
-    FROM system.numbers
-    LIMIT 20
-)
-```
-
-``` text
-┌─histogram(5)(plus(number, 1))───────────────────────────────────────────┐
-│ [(1,4.5,4),(4.5,8.5,4),(8.5,12.75,4.125),(12.75,17,4.625),(17,20,3.25)] │
-└─────────────────────────────────────────────────────────────────────────┘
-```
-
-Vous pouvez visualiser un histogramme avec la [bar](../../sql-reference/functions/other-functions.md#function-bar) fonction, par exemple:
-
-``` sql
-WITH histogram(5)(rand() % 100) AS hist
-SELECT
-    arrayJoin(hist).3 AS height,
-    bar(height, 0, 6, 5) AS bar
-FROM
-(
-    SELECT *
-    FROM system.numbers
-    LIMIT 20
-)
-```
-
-``` text
-┌─height─┬─bar───┐
-│  2.125 │ █▋    │
-│   3.25 │ ██▌   │
-│  5.625 │ ████▏ │
-│  5.625 │ ████▏ │
-│  3.375 │ ██▌   │
-└────────┴───────┘
-```
-
-Dans ce cas, vous devez vous rappeler que vous ne connaissez pas les frontières de la corbeille d'histogramme.
-
-## sequenceMatch(pattern)(timestamp, cond1, cond2, …) {#function-sequencematch}
-
-Vérifie si la séquence contient une chaîne d'événements qui correspond au modèle.
-
-``` sql
-sequenceMatch(pattern)(timestamp, cond1, cond2, ...)
-```
-
-!!! warning "Avertissement"
-    Les événements qui se produisent à la même seconde peuvent se situer dans la séquence dans un ordre indéfini affectant le résultat.
-
-**Paramètre**
-
--   `pattern` — Pattern string. See [Syntaxe du motif](#sequence-function-pattern-syntax).
-
--   `timestamp` — Column considered to contain time data. Typical data types are `Date` et `DateTime`. Vous pouvez également utiliser les prises en charge [UInt](../../sql-reference/data-types/int-uint.md) types de données.
-
--   `cond1`, `cond2` — Conditions that describe the chain of events. Data type: `UInt8`. Vous pouvez passer jusqu'à 32 arguments de condition. La fonction ne prend en compte que les événements décrits dans ces conditions. Si la séquence contient des données qui ne sont pas décrites dans une condition, la fonction les ignore.
-
-**Valeurs renvoyées**
-
--   1, si le profil correspond.
--   0, si le motif ne correspond pas.
-
-Type: `UInt8`.
-
-<a name="sequence-function-pattern-syntax"></a>
-**Syntaxe du motif**
-
--   `(?N)` — Matches the condition argument at position `N`. Les Conditions sont numérotées dans le `[1, 32]` gamme. Exemple, `(?1)` correspond à l'argument passé au `cond1` paramètre.
-
--   `.*` — Matches any number of events. You don't need conditional arguments to match this element of the pattern.
-
--   `(?t operator value)` — Sets the time in seconds that should separate two events. For example, pattern `(?1)(?t>1800)(?2)` correspond à des événements qui se produisent plus de 1800 secondes les uns des autres. Un nombre arbitraire d'événements peut se trouver entre ces événements. Vous pouvez utiliser l' `>=`, `>`, `<`, `<=` opérateur.
-
-**Exemple**
-
-Considérer les données dans le `t` table:
-
-``` text
-┌─time─┬─number─┐
-│    1 │      1 │
-│    2 │      3 │
-│    3 │      2 │
-└──────┴────────┘
-```
-
-Effectuer la requête:
-
-``` sql
-SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2) FROM t
-```
-
-``` text
-┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2))─┐
-│                                                                     1 │
-└───────────────────────────────────────────────────────────────────────┘
-```
-
-La fonction a trouvé la chaîne d'événements où le numéro 2 suit le numéro 1. Il a sauté le numéro 3 entre eux, car le nombre n'est pas décrit comme un événement. Si nous voulons prendre ce nombre en compte lors de la recherche de l'événement de la chaîne donnée dans l'exemple, nous devrions en faire une condition.
-
-``` sql
-SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 3) FROM t
-```
-
-``` text
-┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 3))─┐
-│                                                                                        0 │
-└──────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-Dans ce cas, la fonction n'a pas pu trouver la chaîne d'événements correspondant au modèle, car l'événement pour le numéro 3 s'est produit entre 1 et 2. Si dans le même cas nous vérifions la condition pour le numéro 4, la séquence correspondrait au motif.
-
-``` sql
-SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 4) FROM t
-```
-
-``` text
-┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 4))─┐
-│                                                                                        1 │
-└──────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [sequenceCount](#function-sequencecount)
-
-## sequenceCount(pattern)(time, cond1, cond2, …) {#function-sequencecount}
-
-Compte le nombre de chaînes d'événements correspondant au motif. La fonction recherche les chaînes d'événements qui ne se chevauchent pas. Il commence à rechercher la chaîne suivante après que la chaîne actuelle est appariée.
-
-!!! warning "Avertissement"
-    Les événements qui se produisent à la même seconde peuvent se situer dans la séquence dans un ordre indéfini affectant le résultat.
-
-``` sql
-sequenceCount(pattern)(timestamp, cond1, cond2, ...)
-```
-
-**Paramètre**
-
--   `pattern` — Pattern string. See [Syntaxe du motif](#sequence-function-pattern-syntax).
-
--   `timestamp` — Column considered to contain time data. Typical data types are `Date` et `DateTime`. Vous pouvez également utiliser les prises en charge [UInt](../../sql-reference/data-types/int-uint.md) types de données.
-
--   `cond1`, `cond2` — Conditions that describe the chain of events. Data type: `UInt8`. Vous pouvez passer jusqu'à 32 arguments de condition. La fonction ne prend en compte que les événements décrits dans ces conditions. Si la séquence contient des données qui ne sont pas décrites dans une condition, la fonction les ignore.
-
-**Valeurs renvoyées**
-
--   Nombre de chaînes d'événements qui ne se chevauchent pas et qui sont mises en correspondance.
-
-Type: `UInt64`.
-
-**Exemple**
-
-Considérer les données dans le `t` table:
-
-``` text
-┌─time─┬─number─┐
-│    1 │      1 │
-│    2 │      3 │
-│    3 │      2 │
-│    4 │      1 │
-│    5 │      3 │
-│    6 │      2 │
-└──────┴────────┘
-```
-
-Comptez combien de fois le nombre 2 se produit après le nombre 1 avec n'importe quelle quantité d'autres nombres entre eux:
-
-``` sql
-SELECT sequenceCount('(?1).*(?2)')(time, number = 1, number = 2) FROM t
-```
-
-``` text
-┌─sequenceCount('(?1).*(?2)')(time, equals(number, 1), equals(number, 2))─┐
-│                                                                       2 │
-└─────────────────────────────────────────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [sequenceMatch](#function-sequencematch)
-
-## fenêtrefunnel {#windowfunnel}
-
-Recherche les chaînes d'événements dans une fenêtre de temps coulissante et calcule le nombre maximum d'événements qui se sont produits à partir de la chaîne.
-
-La fonction fonctionne selon l'algorithme:
-
--   La fonction recherche les données qui déclenchent la première condition de la chaîne et définit le compteur d'événements sur 1. C'est le moment où la fenêtre coulissante commence.
-
--   Si les événements de la chaîne se produisent séquentiellement dans la fenêtre, le compteur est incrémenté. Si la séquence d'événements est perturbée, le compteur n'est pas incrémenté.
-
--   Si les données ont plusieurs chaînes d'événements à différents points d'achèvement, la fonction affichera uniquement la taille de la chaîne la plus longue.
-
-**Syntaxe**
-
-``` sql
-windowFunnel(window, [mode])(timestamp, cond1, cond2, ..., condN)
-```
-
-**Paramètre**
-
--   `window` — Length of the sliding window in seconds.
--   `mode` - C'est un argument facultatif.
-    -   `'strict'` - Lorsque le `'strict'` est défini, le windowFunnel() applique des conditions uniquement pour les valeurs uniques.
--   `timestamp` — Name of the column containing the timestamp. Data types supported: [Date](../../sql-reference/data-types/date.md), [DateTime](../../sql-reference/data-types/datetime.md#data_type-datetime) et d'autres types entiers non signés (notez que même si timestamp prend en charge le `UInt64` type, sa valeur ne peut pas dépasser le maximum Int64, qui est 2^63 - 1).
--   `cond` — Conditions or data describing the chain of events. [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valeur renvoyée**
-
-Nombre maximal de conditions déclenchées consécutives de la chaîne dans la fenêtre de temps de glissement.
-Toutes les chaînes de la sélection sont analysés.
-
-Type: `Integer`.
-
-**Exemple**
-
-Déterminer si une période de temps est suffisant pour l'utilisateur de sélectionner un téléphone et d'acheter deux fois dans la boutique en ligne.
-
-Définissez la chaîne d'événements suivante:
-
-1.  L'utilisateur s'est connecté à son compte sur le magasin (`eventID = 1003`).
-2.  L'utilisateur recherche un téléphone (`eventID = 1007, product = 'phone'`).
-3.  Toute commande de l'utilisateur (`eventID = 1009`).
-4.  L'Utilisateur a fait la commande à nouveau (`eventID = 1010`).
-
-Table d'entrée:
-
-``` text
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-01-28 │       1 │ 2019-01-29 10:00:00 │    1003 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-01-31 │       1 │ 2019-01-31 09:00:00 │    1007 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-01-30 │       1 │ 2019-01-30 08:00:00 │    1009 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐
-│ 2019-02-01 │       1 │ 2019-02-01 08:00:00 │    1010 │ phone   │
-└────────────┴─────────┴─────────────────────┴─────────┴─────────┘
-```
-
-Savoir dans quelle mesure l'utilisateur `user_id` pourrait passer à travers la chaîne dans une période en Janvier-Février de 2019.
-
-Requête:
-
-``` sql
-SELECT
-    level,
-    count() AS c
-FROM
-(
-    SELECT
-        user_id,
-        windowFunnel(6048000000000000)(timestamp, eventID = 1003, eventID = 1009, eventID = 1007, eventID = 1010) AS level
-    FROM trend
-    WHERE (event_date >= '2019-01-01') AND (event_date <= '2019-02-02')
-    GROUP BY user_id
-)
-GROUP BY level
-ORDER BY level ASC
-```
-
-Résultat:
-
-``` text
-┌─level─┬─c─┐
-│     4 │ 1 │
-└───────┴───┘
-```
-
-## rétention {#retention}
-
-La fonction prend comme arguments un ensemble de conditions de 1 à 32 arguments de type `UInt8` qui indiquent si une certaine condition est remplie pour l'événement.
-Toute condition peut être spécifiée comme argument (comme dans [WHERE](../../sql-reference/statements/select/where.md#select-where)).
-
-Les conditions, à l'exception de la première, s'appliquent par paires: le résultat de la seconde sera vrai si la première et la deuxième sont remplies, le troisième si la première et la fird sont vraies, etc.
-
-**Syntaxe**
-
-``` sql
-retention(cond1, cond2, ..., cond32);
-```
-
-**Paramètre**
-
--   `cond` — an expression that returns a `UInt8` résultat (1 ou 0).
-
-**Valeur renvoyée**
-
-Le tableau de 1 ou 0.
-
--   1 — condition was met for the event.
--   0 — condition wasn't met for the event.
-
-Type: `UInt8`.
-
-**Exemple**
-
-Prenons un exemple de calcul de la `retention` fonction pour déterminer le trafic du site.
-
-**1.** Сreate a table to illustrate an example.
-
-``` sql
-CREATE TABLE retention_test(date Date, uid Int32) ENGINE = Memory;
-
-INSERT INTO retention_test SELECT '2020-01-01', number FROM numbers(5);
-INSERT INTO retention_test SELECT '2020-01-02', number FROM numbers(10);
-INSERT INTO retention_test SELECT '2020-01-03', number FROM numbers(15);
-```
-
-Table d'entrée:
-
-Requête:
-
-``` sql
-SELECT * FROM retention_test
-```
-
-Résultat:
-
-``` text
-┌───────date─┬─uid─┐
-│ 2020-01-01 │   0 │
-│ 2020-01-01 │   1 │
-│ 2020-01-01 │   2 │
-│ 2020-01-01 │   3 │
-│ 2020-01-01 │   4 │
-└────────────┴─────┘
-┌───────date─┬─uid─┐
-│ 2020-01-02 │   0 │
-│ 2020-01-02 │   1 │
-│ 2020-01-02 │   2 │
-│ 2020-01-02 │   3 │
-│ 2020-01-02 │   4 │
-│ 2020-01-02 │   5 │
-│ 2020-01-02 │   6 │
-│ 2020-01-02 │   7 │
-│ 2020-01-02 │   8 │
-│ 2020-01-02 │   9 │
-└────────────┴─────┘
-┌───────date─┬─uid─┐
-│ 2020-01-03 │   0 │
-│ 2020-01-03 │   1 │
-│ 2020-01-03 │   2 │
-│ 2020-01-03 │   3 │
-│ 2020-01-03 │   4 │
-│ 2020-01-03 │   5 │
-│ 2020-01-03 │   6 │
-│ 2020-01-03 │   7 │
-│ 2020-01-03 │   8 │
-│ 2020-01-03 │   9 │
-│ 2020-01-03 │  10 │
-│ 2020-01-03 │  11 │
-│ 2020-01-03 │  12 │
-│ 2020-01-03 │  13 │
-│ 2020-01-03 │  14 │
-└────────────┴─────┘
-```
-
-**2.** Grouper les utilisateurs par ID unique `uid` à l'aide de la `retention` fonction.
-
-Requête:
-
-``` sql
-SELECT
-    uid,
-    retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r
-FROM retention_test
-WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03')
-GROUP BY uid
-ORDER BY uid ASC
-```
-
-Résultat:
-
-``` text
-┌─uid─┬─r───────┐
-│   0 │ [1,1,1] │
-│   1 │ [1,1,1] │
-│   2 │ [1,1,1] │
-│   3 │ [1,1,1] │
-│   4 │ [1,1,1] │
-│   5 │ [0,0,0] │
-│   6 │ [0,0,0] │
-│   7 │ [0,0,0] │
-│   8 │ [0,0,0] │
-│   9 │ [0,0,0] │
-│  10 │ [0,0,0] │
-│  11 │ [0,0,0] │
-│  12 │ [0,0,0] │
-│  13 │ [0,0,0] │
-│  14 │ [0,0,0] │
-└─────┴─────────┘
-```
-
-**3.** Calculer le nombre total de visites par jour.
-
-Requête:
-
-``` sql
-SELECT
-    sum(r[1]) AS r1,
-    sum(r[2]) AS r2,
-    sum(r[3]) AS r3
-FROM
-(
-    SELECT
-        uid,
-        retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r
-    FROM retention_test
-    WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03')
-    GROUP BY uid
-)
-```
-
-Résultat:
-
-``` text
-┌─r1─┬─r2─┬─r3─┐
-│  5 │  5 │  5 │
-└────┴────┴────┘
-```
-
-Où:
-
--   `r1`- le nombre de visiteurs uniques qui ont visité le site au cours du 2020-01-01 (le `cond1` condition).
--   `r2`- le nombre de visiteurs uniques qui ont visité le site au cours d'une période donnée entre 2020-01-01 et 2020-01-02 (`cond1` et `cond2` condition).
--   `r3`- le nombre de visiteurs uniques qui ont visité le site au cours d'une période donnée entre 2020-01-01 et 2020-01-03 (`cond1` et `cond3` condition).
-
-## uniqUpTo (N) (x) {#uniquptonx}
-
-Calculates the number of different argument values ​​if it is less than or equal to N. If the number of different argument values is greater than N, it returns N + 1.
-
-Recommandé pour une utilisation avec de petites Ns, jusqu'à 10. La valeur maximale de N est de 100.
-
-Pour l'état d'une fonction d'agrégation, il utilise la quantité de mémoire égale à 1 + N \* de la taille d'une valeur d'octets.
-Pour les chaînes, il stocke un hachage non cryptographique de 8 octets. Soit le calcul est approchée pour les chaînes.
-
-La fonction fonctionne également pour plusieurs arguments.
-
-Cela fonctionne aussi vite que possible, sauf dans les cas où une grande valeur N est utilisée et le nombre de valeurs uniques est légèrement inférieur à N.
-
-Exemple d'utilisation:
-
-``` text
-Problem: Generate a report that shows only keywords that produced at least 5 unique users.
-Solution: Write in the GROUP BY query SearchPhrase HAVING uniqUpTo(4)(UserID) >= 5
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/agg_functions/parametric_functions/) <!--hide-->
-
-## sumMapFiltered(keys_to_keep) (clés, valeurs) {#summapfilteredkeys-to-keepkeys-values}
-
-Même comportement que [sumMap](reference.md#agg_functions-summap) sauf qu'un tableau de clés est passé en paramètre. Cela peut être particulièrement utile lorsque vous travaillez avec une forte cardinalité de touches.
diff --git a/docs/fr/sql-reference/aggregate-functions/reference.md b/docs/fr/sql-reference/aggregate-functions/reference.md
deleted file mode 100644
index 37d150a6f683..000000000000
--- a/docs/fr/sql-reference/aggregate-functions/reference.md
+++ /dev/null
@@ -1,1914 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: "R\xE9f\xE9rence"
----
-
-# Référence De La Fonction Agrégée {#aggregate-functions-reference}
-
-## compter {#agg_function-count}
-
-Compte le nombre de lignes ou de valeurs non NULL.
-
-ClickHouse prend en charge les syntaxes suivantes pour `count`:
-- `count(expr)` ou `COUNT(DISTINCT expr)`.
-- `count()` ou `COUNT(*)`. Le `count()` la syntaxe est spécifique à ClickHouse.
-
-**Paramètre**
-
-La fonction peut prendre:
-
--   Zéro des paramètres.
--   Un [expression](../syntax.md#syntax-expressions).
-
-**Valeur renvoyée**
-
--   Si la fonction est appelée sans paramètres, il compte le nombre de lignes.
--   Si l' [expression](../syntax.md#syntax-expressions) est passé, alors la fonction compte combien de fois cette expression retournée not null. Si l'expression renvoie un [Nullable](../../sql-reference/data-types/nullable.md)- tapez la valeur, puis le résultat de `count` séjours pas `Nullable`. La fonction renvoie 0 si l'expression est retournée `NULL` pour toutes les lignes.
-
-Dans les deux cas le type de la valeur renvoyée est [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Détail**
-
-Clickhouse soutient le `COUNT(DISTINCT ...)` syntaxe. Le comportement de cette construction dépend de la [count_distinct_implementation](../../operations/settings/settings.md#settings-count_distinct_implementation) paramètre. Il définit lequel des [uniq\*](#agg_function-uniq) fonctions est utilisée pour effectuer l'opération. La valeur par défaut est la [uniqExact](#agg_function-uniqexact) fonction.
-
-Le `SELECT count() FROM table` la requête n'est pas optimisé, car le nombre d'entrées dans la table n'est pas stockée séparément. Il choisit une petite colonne de la table et compte le nombre de valeurs qu'il contient.
-
-**Exemple**
-
-Exemple 1:
-
-``` sql
-SELECT count() FROM t
-```
-
-``` text
-┌─count()─┐
-│       5 │
-└─────────┘
-```
-
-Exemple 2:
-
-``` sql
-SELECT name, value FROM system.settings WHERE name = 'count_distinct_implementation'
-```
-
-``` text
-┌─name──────────────────────────┬─value─────┐
-│ count_distinct_implementation │ uniqExact │
-└───────────────────────────────┴───────────┘
-```
-
-``` sql
-SELECT count(DISTINCT num) FROM t
-```
-
-``` text
-┌─uniqExact(num)─┐
-│              3 │
-└────────────────┘
-```
-
-Cet exemple montre que `count(DISTINCT num)` est effectuée par le `uniqExact` en fonction de l' `count_distinct_implementation` valeur de réglage.
-
-## tout(x) {#agg_function-any}
-
-Sélectionne la première valeur rencontrée.
-La requête peut être exécutée dans n'importe quel ordre, et même dans un ordre différent à chaque fois, de sorte que le résultat de cette fonction est indéterminée.
-Pour obtenir un résultat déterminé, vous pouvez utiliser le ‘min’ ou ‘max’ fonction au lieu de ‘any’.
-
-Dans certains cas, vous pouvez compter sur l'ordre de l'exécution. Cela s'applique aux cas où SELECT provient d'une sous-requête qui utilise ORDER BY.
-
-Lorsqu'un `SELECT` la requête a l' `GROUP BY` ou au moins une fonction d'agrégat, ClickHouse (contrairement à MySQL) exige que toutes les expressions du `SELECT`, `HAVING`, et `ORDER BY` clauses être calculée à partir de clés ou de fonctions d'agrégation. En d'autres termes, chaque colonne sélectionnée dans la table doit être utilisée soit dans les clés, soit dans les fonctions d'agrégation. Pour obtenir un comportement comme dans MySQL, vous pouvez mettre les autres colonnes dans le `any` fonction d'agrégation.
-
-## anyHeavy (x) {#anyheavyx}
-
-Sélectionne une valeur fréquente à l'aide [poids lourds](http://www.cs.umd.edu/~samir/498/karp.pdf) algorithme. S'il y a une valeur qui se produit plus de la moitié des cas dans chacun des threads d'exécution de la requête, cette valeur est renvoyée. Normalement, le résultat est non déterministe.
-
-``` sql
-anyHeavy(column)
-```
-
-**Argument**
-
--   `column` – The column name.
-
-**Exemple**
-
-Prendre la [OnTime](../../getting-started/example-datasets/ontime.md) ensemble de données et sélectionnez n'importe quelle valeur `AirlineID` colonne.
-
-``` sql
-SELECT anyHeavy(AirlineID) AS res
-FROM ontime
-```
-
-``` text
-┌───res─┐
-│ 19690 │
-└───────┘
-```
-
-## anyLast (x) {#anylastx}
-
-Sélectionne la dernière valeur rencontrés.
-Le résultat est tout aussi indéterminé que pour le `any` fonction.
-
-## groupBitAnd {#groupbitand}
-
-S'applique au niveau du BIT `AND` pour les séries de nombres.
-
-``` sql
-groupBitAnd(expr)
-```
-
-**Paramètre**
-
-`expr` – An expression that results in `UInt*` type.
-
-**Valeur de retour**
-
-La valeur de la `UInt*` type.
-
-**Exemple**
-
-Des données de Test:
-
-``` text
-binary     decimal
-00101100 = 44
-00011100 = 28
-00001101 = 13
-01010101 = 85
-```
-
-Requête:
-
-``` sql
-SELECT groupBitAnd(num) FROM t
-```
-
-Où `num` est la colonne avec les données de test.
-
-Résultat:
-
-``` text
-binary     decimal
-00000100 = 4
-```
-
-## groupBitOr {#groupbitor}
-
-S'applique au niveau du BIT `OR` pour les séries de nombres.
-
-``` sql
-groupBitOr(expr)
-```
-
-**Paramètre**
-
-`expr` – An expression that results in `UInt*` type.
-
-**Valeur de retour**
-
-La valeur de la `UInt*` type.
-
-**Exemple**
-
-Des données de Test:
-
-``` text
-binary     decimal
-00101100 = 44
-00011100 = 28
-00001101 = 13
-01010101 = 85
-```
-
-Requête:
-
-``` sql
-SELECT groupBitOr(num) FROM t
-```
-
-Où `num` est la colonne avec les données de test.
-
-Résultat:
-
-``` text
-binary     decimal
-01111101 = 125
-```
-
-## groupBitXor {#groupbitxor}
-
-S'applique au niveau du BIT `XOR` pour les séries de nombres.
-
-``` sql
-groupBitXor(expr)
-```
-
-**Paramètre**
-
-`expr` – An expression that results in `UInt*` type.
-
-**Valeur de retour**
-
-La valeur de la `UInt*` type.
-
-**Exemple**
-
-Des données de Test:
-
-``` text
-binary     decimal
-00101100 = 44
-00011100 = 28
-00001101 = 13
-01010101 = 85
-```
-
-Requête:
-
-``` sql
-SELECT groupBitXor(num) FROM t
-```
-
-Où `num` est la colonne avec les données de test.
-
-Résultat:
-
-``` text
-binary     decimal
-01101000 = 104
-```
-
-## groupBitmap {#groupbitmap}
-
-Calculs Bitmap ou agrégés à partir d'une colonne entière non signée, retour cardinalité de type UInt64, si Ajouter suffixe-State, puis retour [objet bitmap](../../sql-reference/functions/bitmap-functions.md).
-
-``` sql
-groupBitmap(expr)
-```
-
-**Paramètre**
-
-`expr` – An expression that results in `UInt*` type.
-
-**Valeur de retour**
-
-La valeur de la `UInt64` type.
-
-**Exemple**
-
-Des données de Test:
-
-``` text
-UserID
-1
-1
-2
-3
-```
-
-Requête:
-
-``` sql
-SELECT groupBitmap(UserID) as num FROM t
-```
-
-Résultat:
-
-``` text
-num
-3
-```
-
-## min (x) {#agg_function-min}
-
-Calcule le minimum.
-
-## max (x) {#agg_function-max}
-
-Calcule le maximum.
-
-## argMin (arg, val) {#agg-function-argmin}
-
-Calcule la ‘arg’ valeur pour un minimum ‘val’ valeur. S'il y a plusieurs valeurs différentes de ‘arg’ pour des valeurs minimales de ‘val’ la première de ces valeurs rencontrées est de sortie.
-
-**Exemple:**
-
-``` text
-┌─user─────┬─salary─┐
-│ director │   5000 │
-│ manager  │   3000 │
-│ worker   │   1000 │
-└──────────┴────────┘
-```
-
-``` sql
-SELECT argMin(user, salary) FROM salary
-```
-
-``` text
-┌─argMin(user, salary)─┐
-│ worker               │
-└──────────────────────┘
-```
-
-## argMax(arg, val) {#agg-function-argmax}
-
-Calcule la ‘arg’ valeur pour un maximum ‘val’ valeur. S'il y a plusieurs valeurs différentes de ‘arg’ pour les valeurs maximales de ‘val’ la première de ces valeurs rencontrées est de sortie.
-
-## sum(x) {#agg_function-sum}
-
-Calcule la somme.
-Ne fonctionne que pour les numéros.
-
-## sumWithOverflow (x) {#sumwithoverflowx}
-
-Calcule la somme des nombres, en utilisant le même type de données pour le résultat que pour les paramètres d'entrée. Si la somme dépasse la valeur maximale pour ce type de données, la fonction renvoie une erreur.
-
-Ne fonctionne que pour les numéros.
-
-## sumMap(clé, valeur), sumMap(Tuple(clé, valeur)) {#agg_functions-summap}
-
-Les totaux de la ‘value’ tableau selon les clés spécifiés dans le ‘key’ tableau.
-Le passage du tuple des tableaux de clés et de valeurs est synonyme du passage de deux tableaux de clés et de valeurs.
-Le nombre d'éléments dans ‘key’ et ‘value’ doit être identique pour chaque ligne totalisée.
-Returns a tuple of two arrays: keys in sorted order, and values ​​summed for the corresponding keys.
-
-Exemple:
-
-``` sql
-CREATE TABLE sum_map(
-    date Date,
-    timeslot DateTime,
-    statusMap Nested(
-        status UInt16,
-        requests UInt64
-    ),
-    statusMapTuple Tuple(Array(Int32), Array(Int32))
-) ENGINE = Log;
-INSERT INTO sum_map VALUES
-    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10], ([1, 2, 3], [10, 10, 10])),
-    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10], ([3, 4, 5], [10, 10, 10])),
-    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10], ([4, 5, 6], [10, 10, 10])),
-    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10], ([6, 7, 8], [10, 10, 10]));
-
-SELECT
-    timeslot,
-    sumMap(statusMap.status, statusMap.requests),
-    sumMap(statusMapTuple)
-FROM sum_map
-GROUP BY timeslot
-```
-
-``` text
-┌────────────timeslot─┬─sumMap(statusMap.status, statusMap.requests)─┬─sumMap(statusMapTuple)─────────┐
-│ 2000-01-01 00:00:00 │ ([1,2,3,4,5],[10,10,20,10,10])               │ ([1,2,3,4,5],[10,10,20,10,10]) │
-│ 2000-01-01 00:01:00 │ ([4,5,6,7,8],[10,10,20,10,10])               │ ([4,5,6,7,8],[10,10,20,10,10]) │
-└─────────────────────┴──────────────────────────────────────────────┴────────────────────────────────┘
-```
-
-## skewPop {#skewpop}
-
-Calcule la [asymétrie](https://en.wikipedia.org/wiki/Skewness) d'une séquence.
-
-``` sql
-skewPop(expr)
-```
-
-**Paramètre**
-
-`expr` — [Expression](../syntax.md#syntax-expressions) retour d'un nombre.
-
-**Valeur renvoyée**
-
-The skewness of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md)
-
-**Exemple**
-
-``` sql
-SELECT skewPop(value) FROM series_with_value_column
-```
-
-## skewSamp {#skewsamp}
-
-Calcule la [asymétrie de l'échantillon](https://en.wikipedia.org/wiki/Skewness) d'une séquence.
-
-Il représente une estimation non biaisée de l'asymétrie d'une variable aléatoire si les valeurs passées forme de son échantillon.
-
-``` sql
-skewSamp(expr)
-```
-
-**Paramètre**
-
-`expr` — [Expression](../syntax.md#syntax-expressions) retour d'un nombre.
-
-**Valeur renvoyée**
-
-The skewness of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md). Si `n <= 1` (`n` est la taille de l'échantillon), alors la fonction renvoie `nan`.
-
-**Exemple**
-
-``` sql
-SELECT skewSamp(value) FROM series_with_value_column
-```
-
-## kurtPop {#kurtpop}
-
-Calcule la [kurtosis](https://en.wikipedia.org/wiki/Kurtosis) d'une séquence.
-
-``` sql
-kurtPop(expr)
-```
-
-**Paramètre**
-
-`expr` — [Expression](../syntax.md#syntax-expressions) retour d'un nombre.
-
-**Valeur renvoyée**
-
-The kurtosis of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md)
-
-**Exemple**
-
-``` sql
-SELECT kurtPop(value) FROM series_with_value_column
-```
-
-## kurtSamp {#kurtsamp}
-
-Calcule la [l'échantillon le coefficient d'aplatissement](https://en.wikipedia.org/wiki/Kurtosis) d'une séquence.
-
-Il représente une estimation non biaisée de la kurtose d'une variable aléatoire si les valeurs passées forment son échantillon.
-
-``` sql
-kurtSamp(expr)
-```
-
-**Paramètre**
-
-`expr` — [Expression](../syntax.md#syntax-expressions) retour d'un nombre.
-
-**Valeur renvoyée**
-
-The kurtosis of the given distribution. Type — [Float64](../../sql-reference/data-types/float.md). Si `n <= 1` (`n` la taille de l'échantillon), alors la fonction renvoie `nan`.
-
-**Exemple**
-
-``` sql
-SELECT kurtSamp(value) FROM series_with_value_column
-```
-
-## avg (x) {#agg_function-avg}
-
-Calcule la moyenne.
-Ne fonctionne que pour les numéros.
-Le résultat est toujours Float64.
-
-## avgWeighted {#avgweighted}
-
-Calcule la [moyenne arithmétique pondérée](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean).
-
-**Syntaxe**
-
-``` sql
-avgWeighted(x, weight)
-```
-
-**Paramètre**
-
--   `x` — Values. [Entier](../data-types/int-uint.md) ou [virgule flottante](../data-types/float.md).
--   `weight` — Weights of the values. [Entier](../data-types/int-uint.md) ou [virgule flottante](../data-types/float.md).
-
-Type de `x` et `weight` doit être le même.
-
-**Valeur renvoyée**
-
--   Moyenne pondérée.
--   `NaN`. Si tous les poids sont égaux à 0.
-
-Type: [Float64](../data-types/float.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT avgWeighted(x, w)
-FROM values('x Int8, w Int8', (4, 1), (1, 0), (10, 2))
-```
-
-Résultat:
-
-``` text
-┌─avgWeighted(x, weight)─┐
-│                      8 │
-└────────────────────────┘
-```
-
-## uniq {#agg_function-uniq}
-
-Calcule le nombre approximatif des différentes valeurs de l'argument.
-
-``` sql
-uniq(x[, ...])
-```
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres. Les paramètres peuvent être `Tuple`, `Array`, `Date`, `DateTime`, `String` ou des types numériques.
-
-**Valeur renvoyée**
-
--   A [UInt64](../../sql-reference/data-types/int-uint.md)numéro de type.
-
-**Détails de mise en œuvre**
-
-Fonction:
-
--   Calcule un hachage pour tous les paramètres de l'agrégat, puis l'utilise dans les calculs.
-
--   Utilise un algorithme d'échantillonnage adaptatif. Pour l'état de calcul, La fonction utilise un échantillon de valeurs de hachage d'éléments jusqu'à 65536.
-
-        This algorithm is very accurate and very efficient on the CPU. When the query contains several of these functions, using `uniq` is almost as fast as using other aggregate functions.
-
--   Fournit le résultat de manière déterministe (cela ne dépend pas de l'ordre de traitement de la requête).
-
-Nous vous recommandons d'utiliser cette fonction dans presque tous les scénarios.
-
-**Voir Aussi**
-
--   [uniqcombiné](#agg_function-uniqcombined)
--   [uniqCombined64](#agg_function-uniqcombined64)
--   [uniqHLL12](#agg_function-uniqhll12)
--   [uniqExact](#agg_function-uniqexact)
-
-## uniqcombiné {#agg_function-uniqcombined}
-
-Calcule le nombre approximatif de différentes valeurs d'argument.
-
-``` sql
-uniqCombined(HLL_precision)(x[, ...])
-```
-
-Le `uniqCombined` la fonction est un bon choix pour calculer le nombre de valeurs différentes.
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres. Les paramètres peuvent être `Tuple`, `Array`, `Date`, `DateTime`, `String` ou des types numériques.
-
-`HLL_precision` est le logarithme en base 2 du nombre de cellules dans [HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog). Facultatif, vous pouvez utiliser la fonction comme `uniqCombined(x[, ...])`. La valeur par défaut pour `HLL_precision` est 17, qui est effectivement 96 Ko d'espace(2 ^ 17 cellules, 6 bits chacune).
-
-**Valeur renvoyée**
-
--   Nombre [UInt64](../../sql-reference/data-types/int-uint.md)numéro de type.
-
-**Détails de mise en œuvre**
-
-Fonction:
-
--   Calcule un hachage (hachage 64 bits pour `String` et 32 bits sinon) pour tous les paramètres dans l'agrégat, puis l'utilise dans les calculs.
-
--   Utilise une combinaison de trois algorithmes: tableau, table de hachage et HyperLogLog avec une table de correction d'erreur.
-
-        For a small number of distinct elements, an array is used. When the set size is larger, a hash table is used. For a larger number of elements, HyperLogLog is used, which will occupy a fixed amount of memory.
-
--   Fournit le résultat de manière déterministe (cela ne dépend pas de l'ordre de traitement de la requête).
-
-!!! note "Note"
-    Comme il utilise le hachage 32 bits pour non-`String` type, le résultat aura une erreur très élevée pour les cardinalités significativement plus grandes que `UINT_MAX` (erreur va augmenter rapidement après quelques dizaines de milliards de valeurs distinctes), donc dans ce cas, vous devez utiliser [uniqCombined64](#agg_function-uniqcombined64)
-
-Par rapport à la [uniq](#agg_function-uniq) la fonction, la `uniqCombined`:
-
--   Consomme plusieurs fois moins de mémoire.
--   Calcule avec plusieurs fois plus de précision.
--   A généralement des performances légèrement inférieures. Dans certains scénarios, `uniqCombined` peut faire mieux que `uniq` par exemple, avec des requêtes distribuées qui transmettent un grand nombre d'agrégation des états sur le réseau.
-
-**Voir Aussi**
-
--   [uniq](#agg_function-uniq)
--   [uniqCombined64](#agg_function-uniqcombined64)
--   [uniqHLL12](#agg_function-uniqhll12)
--   [uniqExact](#agg_function-uniqexact)
-
-## uniqCombined64 {#agg_function-uniqcombined64}
-
-Même que [uniqcombiné](#agg_function-uniqcombined), mais utilise le hachage 64 bits pour tous les types de données.
-
-## uniqHLL12 {#agg_function-uniqhll12}
-
-Calcule le nombre approximatif de différentes valeurs d'argument, en utilisant [HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog) algorithme.
-
-``` sql
-uniqHLL12(x[, ...])
-```
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres. Les paramètres peuvent être `Tuple`, `Array`, `Date`, `DateTime`, `String` ou des types numériques.
-
-**Valeur renvoyée**
-
--   A [UInt64](../../sql-reference/data-types/int-uint.md)numéro de type.
-
-**Détails de mise en œuvre**
-
-Fonction:
-
--   Calcule un hachage pour tous les paramètres de l'agrégat, puis l'utilise dans les calculs.
-
--   Utilise L'algorithme HyperLogLog pour approximer le nombre de valeurs d'argument différentes.
-
-        212 5-bit cells are used. The size of the state is slightly more than 2.5 KB. The result is not very accurate (up to ~10% error) for small data sets (<10K elements). However, the result is fairly accurate for high-cardinality data sets (10K-100M), with a maximum error of ~1.6%. Starting from 100M, the estimation error increases, and the function will return very inaccurate results for data sets with extremely high cardinality (1B+ elements).
-
--   Fournit le résultat déterminé (il ne dépend pas de l'ordre de traitement de la requête).
-
-Nous ne recommandons pas d'utiliser cette fonction. Dans la plupart des cas, l'utilisation de la [uniq](#agg_function-uniq) ou [uniqcombiné](#agg_function-uniqcombined) fonction.
-
-**Voir Aussi**
-
--   [uniq](#agg_function-uniq)
--   [uniqcombiné](#agg_function-uniqcombined)
--   [uniqExact](#agg_function-uniqexact)
-
-## uniqExact {#agg_function-uniqexact}
-
-Calcule le nombre exact de différentes valeurs d'argument.
-
-``` sql
-uniqExact(x[, ...])
-```
-
-L'utilisation de la `uniqExact` fonction si vous avez absolument besoin d'un résultat exact. Sinon l'utilisation de la [uniq](#agg_function-uniq) fonction.
-
-Le `uniqExact` la fonction utilise plus de mémoire que `uniq` parce que la taille de l'état a surabondance de croissance que le nombre de valeurs différentes augmente.
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres. Les paramètres peuvent être `Tuple`, `Array`, `Date`, `DateTime`, `String` ou des types numériques.
-
-**Voir Aussi**
-
--   [uniq](#agg_function-uniq)
--   [uniqcombiné](#agg_function-uniqcombined)
--   [uniqHLL12](#agg_function-uniqhll12)
-
-## groupArray(x), groupArray (max_size) (x) {#agg_function-grouparray}
-
-Crée un tableau de valeurs de l'argument.
-Les valeurs peuvent être ajoutées au tableau dans une (indéterminée) de commande.
-
-La deuxième version (avec le `max_size` paramètre) limite la taille du tableau résultant à `max_size` élément.
-Exemple, `groupArray (1) (x)` est équivalent à `[any (x)]`.
-
-Dans certains cas, vous pouvez toujours compter sur l'ordre de l'exécution. Cela s'applique aux cas où `SELECT` provient d'une sous-requête qui utilise `ORDER BY`.
-
-## groupeparrayinsertat {#grouparrayinsertat}
-
-Insère une valeur dans le tableau à la position spécifiée.
-
-**Syntaxe**
-
-``` sql
-groupArrayInsertAt(default_x, size)(x, pos);
-```
-
-Si dans une requête plusieurs valeurs sont insérées dans la même position, la fonction se comporte de la manière suivante:
-
--   Si une requête est exécutée dans un seul thread, la première des valeurs insérées est utilisée.
--   Si une requête est exécutée dans plusieurs threads, le résultat est indéterminé l'une des valeurs insérées.
-
-**Paramètre**
-
--   `x` — Value to be inserted. [Expression](../syntax.md#syntax-expressions) résultant dans l'un des [types de données pris en charge](../../sql-reference/data-types/index.md).
--   `pos` — Position at which the specified element `x` doit être inséré. L'indice de numérotation dans le tableau commence à partir de zéro. [UInt32](../../sql-reference/data-types/int-uint.md#uint-ranges).
--   `default_x`— Default value for substituting in empty positions. Optional parameter. [Expression](../syntax.md#syntax-expressions) résultant dans le type de données configuré pour le `x` paramètre. Si `default_x` n'est pas définie, la [les valeurs par défaut](../../sql-reference/statements/create.md#create-default-values) sont utilisés.
--   `size`— Length of the resulting array. Optional parameter. When using this parameter, the default value `default_x` doit être spécifié. [UInt32](../../sql-reference/data-types/int-uint.md#uint-ranges).
-
-**Valeur renvoyée**
-
--   Tableau avec des valeurs insérées.
-
-Type: [Tableau](../../sql-reference/data-types/array.md#data-type-array).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT groupArrayInsertAt(toString(number), number * 2) FROM numbers(5);
-```
-
-Résultat:
-
-``` text
-┌─groupArrayInsertAt(toString(number), multiply(number, 2))─┐
-│ ['0','','1','','2','','3','','4']                         │
-└───────────────────────────────────────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT groupArrayInsertAt('-')(toString(number), number * 2) FROM numbers(5);
-```
-
-Résultat:
-
-``` text
-┌─groupArrayInsertAt('-')(toString(number), multiply(number, 2))─┐
-│ ['0','-','1','-','2','-','3','-','4']                          │
-└────────────────────────────────────────────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT groupArrayInsertAt('-', 5)(toString(number), number * 2) FROM numbers(5);
-```
-
-Résultat:
-
-``` text
-┌─groupArrayInsertAt('-', 5)(toString(number), multiply(number, 2))─┐
-│ ['0','-','1','-','2']                                             │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-Insertion multi-thread d'éléments dans une position.
-
-Requête:
-
-``` sql
-SELECT groupArrayInsertAt(number, 0) FROM numbers_mt(10) SETTINGS max_block_size = 1;
-```
-
-Comme un résultat de cette requête, vous obtenez entier aléatoire dans le `[0,9]` gamme. Exemple:
-
-``` text
-┌─groupArrayInsertAt(number, 0)─┐
-│ [7]                           │
-└───────────────────────────────┘
-```
-
-## groupeparraymovingsum {#agg_function-grouparraymovingsum}
-
-Calcule la somme mobile des valeurs d'entrée.
-
-``` sql
-groupArrayMovingSum(numbers_for_summing)
-groupArrayMovingSum(window_size)(numbers_for_summing)
-```
-
-La fonction peut prendre la taille de la fenêtre comme paramètre. Si spécifié, la fonction prend la taille de la fenêtre égal au nombre de lignes dans la colonne.
-
-**Paramètre**
-
--   `numbers_for_summing` — [Expression](../syntax.md#syntax-expressions) résultant en une valeur de type de données Numérique.
--   `window_size` — Size of the calculation window.
-
-**Valeurs renvoyées**
-
--   Tableau de la même taille et de même type que les données d'entrée.
-
-**Exemple**
-
-La table d'échantillon:
-
-``` sql
-CREATE TABLE t
-(
-    `int` UInt8,
-    `float` Float32,
-    `dec` Decimal32(2)
-)
-ENGINE = TinyLog
-```
-
-``` text
-┌─int─┬─float─┬──dec─┐
-│   1 │   1.1 │ 1.10 │
-│   2 │   2.2 │ 2.20 │
-│   4 │   4.4 │ 4.40 │
-│   7 │  7.77 │ 7.77 │
-└─────┴───────┴──────┘
-```
-
-Requête:
-
-``` sql
-SELECT
-    groupArrayMovingSum(int) AS I,
-    groupArrayMovingSum(float) AS F,
-    groupArrayMovingSum(dec) AS D
-FROM t
-```
-
-``` text
-┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐
-│ [1,3,7,14] │ [1.1,3.3000002,7.7000003,15.47] │ [1.10,3.30,7.70,15.47] │
-└────────────┴─────────────────────────────────┴────────────────────────┘
-```
-
-``` sql
-SELECT
-    groupArrayMovingSum(2)(int) AS I,
-    groupArrayMovingSum(2)(float) AS F,
-    groupArrayMovingSum(2)(dec) AS D
-FROM t
-```
-
-``` text
-┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐
-│ [1,3,6,11] │ [1.1,3.3000002,6.6000004,12.17] │ [1.10,3.30,6.60,12.17] │
-└────────────┴─────────────────────────────────┴────────────────────────┘
-```
-
-## groupArrayMovingAvg {#agg_function-grouparraymovingavg}
-
-Calcule la moyenne mobile des valeurs d'entrée.
-
-``` sql
-groupArrayMovingAvg(numbers_for_summing)
-groupArrayMovingAvg(window_size)(numbers_for_summing)
-```
-
-La fonction peut prendre la taille de la fenêtre comme paramètre. Si spécifié, la fonction prend la taille de la fenêtre égal au nombre de lignes dans la colonne.
-
-**Paramètre**
-
--   `numbers_for_summing` — [Expression](../syntax.md#syntax-expressions) résultant en une valeur de type de données Numérique.
--   `window_size` — Size of the calculation window.
-
-**Valeurs renvoyées**
-
--   Tableau de la même taille et de même type que les données d'entrée.
-
-La fonction utilise [l'arrondi vers zéro](https://en.wikipedia.org/wiki/Rounding#Rounding_towards_zero). Il tronque les décimales insignifiantes pour le type de données résultant.
-
-**Exemple**
-
-La table d'échantillon `b`:
-
-``` sql
-CREATE TABLE t
-(
-    `int` UInt8,
-    `float` Float32,
-    `dec` Decimal32(2)
-)
-ENGINE = TinyLog
-```
-
-``` text
-┌─int─┬─float─┬──dec─┐
-│   1 │   1.1 │ 1.10 │
-│   2 │   2.2 │ 2.20 │
-│   4 │   4.4 │ 4.40 │
-│   7 │  7.77 │ 7.77 │
-└─────┴───────┴──────┘
-```
-
-Requête:
-
-``` sql
-SELECT
-    groupArrayMovingAvg(int) AS I,
-    groupArrayMovingAvg(float) AS F,
-    groupArrayMovingAvg(dec) AS D
-FROM t
-```
-
-``` text
-┌─I─────────┬─F───────────────────────────────────┬─D─────────────────────┐
-│ [0,0,1,3] │ [0.275,0.82500005,1.9250001,3.8675] │ [0.27,0.82,1.92,3.86] │
-└───────────┴─────────────────────────────────────┴───────────────────────┘
-```
-
-``` sql
-SELECT
-    groupArrayMovingAvg(2)(int) AS I,
-    groupArrayMovingAvg(2)(float) AS F,
-    groupArrayMovingAvg(2)(dec) AS D
-FROM t
-```
-
-``` text
-┌─I─────────┬─F────────────────────────────────┬─D─────────────────────┐
-│ [0,1,3,5] │ [0.55,1.6500001,3.3000002,6.085] │ [0.55,1.65,3.30,6.08] │
-└───────────┴──────────────────────────────────┴───────────────────────┘
-```
-
-## groupUniqArray(x), groupUniqArray (max_size) (x) {#groupuniqarrayx-groupuniqarraymax-sizex}
-
-Crée un tableau à partir de différentes valeurs d'argument. La consommation de mémoire est la même que pour la `uniqExact` fonction.
-
-La deuxième version (avec le `max_size` paramètre) limite la taille du tableau résultant à `max_size` élément.
-Exemple, `groupUniqArray(1)(x)` est équivalent à `[any(x)]`.
-
-## quantile {#quantile}
-
-Calcule une approximation [quantile](https://en.wikipedia.org/wiki/Quantile) des données numériques de la séquence.
-
-Cette fonction s'applique [réservoir d'échantillonnage](https://en.wikipedia.org/wiki/Reservoir_sampling) avec une taille de réservoir jusqu'à 8192 et un générateur de nombres aléatoires pour l'échantillonnage. Le résultat est non-déterministe. Pour obtenir un quantile exact, Utilisez le [quantileExact](#quantileexact) fonction.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantile(level)(expr)
-```
-
-Alias: `median`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [types de données](../../sql-reference/data-types/index.md#data_types), [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
-
-**Valeur renvoyée**
-
--   Approximative de quantiles de niveau spécifié.
-
-Type:
-
--   [Float64](../../sql-reference/data-types/float.md) pour l'entrée de type de données numériques.
--   [Date](../../sql-reference/data-types/date.md) si les valeurs d'entrée ont le `Date` type.
--   [DateTime](../../sql-reference/data-types/datetime.md) si les valeurs d'entrée ont le `DateTime` type.
-
-**Exemple**
-
-Table d'entrée:
-
-``` text
-┌─val─┐
-│   1 │
-│   1 │
-│   2 │
-│   3 │
-└─────┘
-```
-
-Requête:
-
-``` sql
-SELECT quantile(val) FROM t
-```
-
-Résultat:
-
-``` text
-┌─quantile(val)─┐
-│           1.5 │
-└───────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## quantileDeterministic {#quantiledeterministic}
-
-Calcule une approximation [quantile](https://en.wikipedia.org/wiki/Quantile) des données numériques de la séquence.
-
-Cette fonction s'applique [réservoir d'échantillonnage](https://en.wikipedia.org/wiki/Reservoir_sampling) avec une taille de réservoir jusqu'à 8192 et un algorithme déterministe d'échantillonnage. Le résultat est déterministe. Pour obtenir un quantile exact, Utilisez le [quantileExact](#quantileexact) fonction.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantileDeterministic(level)(expr, determinator)
-```
-
-Alias: `medianDeterministic`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [types de données](../../sql-reference/data-types/index.md#data_types), [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
--   `determinator` — Number whose hash is used instead of a random number generator in the reservoir sampling algorithm to make the result of sampling deterministic. As a determinator you can use any deterministic positive number, for example, a user id or an event id. If the same determinator value occures too often, the function works incorrectly.
-
-**Valeur renvoyée**
-
--   Approximative de quantiles de niveau spécifié.
-
-Type:
-
--   [Float64](../../sql-reference/data-types/float.md) pour l'entrée de type de données numériques.
--   [Date](../../sql-reference/data-types/date.md) si les valeurs d'entrée ont le `Date` type.
--   [DateTime](../../sql-reference/data-types/datetime.md) si les valeurs d'entrée ont le `DateTime` type.
-
-**Exemple**
-
-Table d'entrée:
-
-``` text
-┌─val─┐
-│   1 │
-│   1 │
-│   2 │
-│   3 │
-└─────┘
-```
-
-Requête:
-
-``` sql
-SELECT quantileDeterministic(val, 1) FROM t
-```
-
-Résultat:
-
-``` text
-┌─quantileDeterministic(val, 1)─┐
-│                           1.5 │
-└───────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## quantileExact {#quantileexact}
-
-Exactement calcule l' [quantile](https://en.wikipedia.org/wiki/Quantile) des données numériques de la séquence.
-
-To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes `O(n)` de mémoire, où `n` est un nombre de valeurs qui ont été passées. Cependant, pour un petit nombre de valeurs, la fonction est très efficace.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantileExact(level)(expr)
-```
-
-Alias: `medianExact`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [types de données](../../sql-reference/data-types/index.md#data_types), [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
-
-**Valeur renvoyée**
-
--   Quantiles de niveau spécifié.
-
-Type:
-
--   [Float64](../../sql-reference/data-types/float.md) pour l'entrée de type de données numériques.
--   [Date](../../sql-reference/data-types/date.md) si les valeurs d'entrée ont le `Date` type.
--   [DateTime](../../sql-reference/data-types/datetime.md) si les valeurs d'entrée ont le `DateTime` type.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT quantileExact(number) FROM numbers(10)
-```
-
-Résultat:
-
-``` text
-┌─quantileExact(number)─┐
-│                     5 │
-└───────────────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## quantileExactWeighted {#quantileexactweighted}
-
-Exactement calcule l' [quantile](https://en.wikipedia.org/wiki/Quantile) d'une séquence de données numériques, en tenant compte du poids de chaque élément.
-
-To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Each value is counted with its weight, as if it is present `weight` times. A hash table is used in the algorithm. Because of this, if the passed values ​​are frequently repeated, the function consumes less RAM than [quantileExact](#quantileexact). Vous pouvez utiliser cette fonction au lieu de `quantileExact` et spécifiez le poids 1.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantileExactWeighted(level)(expr, weight)
-```
-
-Alias: `medianExactWeighted`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [types de données](../../sql-reference/data-types/index.md#data_types), [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
--   `weight` — Column with weights of sequence members. Weight is a number of value occurrences.
-
-**Valeur renvoyée**
-
--   Quantiles de niveau spécifié.
-
-Type:
-
--   [Float64](../../sql-reference/data-types/float.md) pour l'entrée de type de données numériques.
--   [Date](../../sql-reference/data-types/date.md) si les valeurs d'entrée ont le `Date` type.
--   [DateTime](../../sql-reference/data-types/datetime.md) si les valeurs d'entrée ont le `DateTime` type.
-
-**Exemple**
-
-Table d'entrée:
-
-``` text
-┌─n─┬─val─┐
-│ 0 │   3 │
-│ 1 │   2 │
-│ 2 │   1 │
-│ 5 │   4 │
-└───┴─────┘
-```
-
-Requête:
-
-``` sql
-SELECT quantileExactWeighted(n, val) FROM t
-```
-
-Résultat:
-
-``` text
-┌─quantileExactWeighted(n, val)─┐
-│                             1 │
-└───────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## quantileTiming {#quantiletiming}
-
-Avec la précision déterminée calcule le [quantile](https://en.wikipedia.org/wiki/Quantile) des données numériques de la séquence.
-
-Le résultat est déterministe (il ne dépend pas de l'ordre de traitement de la requête). La fonction est optimisée pour travailler avec des séquences qui décrivent des distributions comme les temps de chargement des pages web ou les temps de réponse du backend.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantileTiming(level)(expr)
-```
-
-Alias: `medianTiming`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
-
--   `expr` — [Expression](../syntax.md#syntax-expressions) sur une colonne Valeurs renvoyant un [Flottant\*](../../sql-reference/data-types/float.md)numéro de type.
-
-        - If negative values are passed to the function, the behavior is undefined.
-        - If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000.
-
-**Exactitude**
-
-Le calcul est précis si:
-
--   Le nombre Total de valeurs ne dépasse pas 5670.
--   Le nombre Total de valeurs dépasse 5670, mais le temps de chargement de la page est inférieur à 1024ms.
-
-Sinon, le résultat du calcul est arrondi au plus proche multiple de 16 ms.
-
-!!! note "Note"
-    Pour calculer les quantiles de temps de chargement de page, cette fonction est plus efficace et précise que [quantile](#quantile).
-
-**Valeur renvoyée**
-
--   Quantiles de niveau spécifié.
-
-Type: `Float32`.
-
-!!! note "Note"
-    Si aucune valeur n'est transmise à la fonction (lors de l'utilisation de `quantileTimingIf`), [Nan](../../sql-reference/data-types/float.md#data_type-float-nan-inf) est retourné. Le but est de différencier ces cas de cas qui aboutissent à zéro. Voir [Clause ORDER BY](../statements/select/order-by.md#select-order-by) pour des notes sur le tri `NaN` valeur.
-
-**Exemple**
-
-Table d'entrée:
-
-``` text
-┌─response_time─┐
-│            72 │
-│           112 │
-│           126 │
-│           145 │
-│           104 │
-│           242 │
-│           313 │
-│           168 │
-│           108 │
-└───────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT quantileTiming(response_time) FROM t
-```
-
-Résultat:
-
-``` text
-┌─quantileTiming(response_time)─┐
-│                           126 │
-└───────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## quantileTimingWeighted {#quantiletimingweighted}
-
-Avec la précision déterminée calcule le [quantile](https://en.wikipedia.org/wiki/Quantile) d'une séquence de données numériques en fonction du poids de chaque élément de séquence.
-
-Le résultat est déterministe (il ne dépend pas de l'ordre de traitement de la requête). La fonction est optimisée pour travailler avec des séquences qui décrivent des distributions comme les temps de chargement des pages web ou les temps de réponse du backend.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantileTimingWeighted(level)(expr, weight)
-```
-
-Alias: `medianTimingWeighted`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
-
--   `expr` — [Expression](../syntax.md#syntax-expressions) sur une colonne Valeurs renvoyant un [Flottant\*](../../sql-reference/data-types/float.md)numéro de type.
-
-        - If negative values are passed to the function, the behavior is undefined.
-        - If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000.
-
--   `weight` — Column with weights of sequence elements. Weight is a number of value occurrences.
-
-**Exactitude**
-
-Le calcul est précis si:
-
--   Le nombre Total de valeurs ne dépasse pas 5670.
--   Le nombre Total de valeurs dépasse 5670, mais le temps de chargement de la page est inférieur à 1024ms.
-
-Sinon, le résultat du calcul est arrondi au plus proche multiple de 16 ms.
-
-!!! note "Note"
-    Pour calculer les quantiles de temps de chargement de page, cette fonction est plus efficace et précise que [quantile](#quantile).
-
-**Valeur renvoyée**
-
--   Quantiles de niveau spécifié.
-
-Type: `Float32`.
-
-!!! note "Note"
-    Si aucune valeur n'est transmise à la fonction (lors de l'utilisation de `quantileTimingIf`), [Nan](../../sql-reference/data-types/float.md#data_type-float-nan-inf) est retourné. Le but est de différencier ces cas de cas qui aboutissent à zéro. Voir [Clause ORDER BY](../statements/select/order-by.md#select-order-by) pour des notes sur le tri `NaN` valeur.
-
-**Exemple**
-
-Table d'entrée:
-
-``` text
-┌─response_time─┬─weight─┐
-│            68 │      1 │
-│           104 │      2 │
-│           112 │      3 │
-│           126 │      2 │
-│           138 │      1 │
-│           162 │      1 │
-└───────────────┴────────┘
-```
-
-Requête:
-
-``` sql
-SELECT quantileTimingWeighted(response_time, weight) FROM t
-```
-
-Résultat:
-
-``` text
-┌─quantileTimingWeighted(response_time, weight)─┐
-│                                           112 │
-└───────────────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## quantileTDigest {#quantiletdigest}
-
-Calcule une approximation [quantile](https://en.wikipedia.org/wiki/Quantile) d'une séquence de données numériques utilisant [t-digest](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf) algorithme.
-
-L'erreur maximale est de 1%. La consommation de mémoire est `log(n)`, où `n` est un certain nombre de valeurs. Le résultat dépend de l'ordre d'exécution de la requête et n'est pas déterministe.
-
-La performance de la fonction est inférieure à la performance de [quantile](#quantile) ou [quantileTiming](#quantiletiming). En termes de rapport entre la taille de L'état et la précision, cette fonction est bien meilleure que `quantile`.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantileTDigest(level)(expr)
-```
-
-Alias: `medianTDigest`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [types de données](../../sql-reference/data-types/index.md#data_types), [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
-
-**Valeur renvoyée**
-
--   Approximative de quantiles de niveau spécifié.
-
-Type:
-
--   [Float64](../../sql-reference/data-types/float.md) pour l'entrée de type de données numériques.
--   [Date](../../sql-reference/data-types/date.md) si les valeurs d'entrée ont le `Date` type.
--   [DateTime](../../sql-reference/data-types/datetime.md) si les valeurs d'entrée ont le `DateTime` type.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT quantileTDigest(number) FROM numbers(10)
-```
-
-Résultat:
-
-``` text
-┌─quantileTDigest(number)─┐
-│                     4.5 │
-└─────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## quantileTDigestWeighted {#quantiletdigestweighted}
-
-Calcule une approximation [quantile](https://en.wikipedia.org/wiki/Quantile) d'une séquence de données numériques utilisant [t-digest](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf) algorithme. La fonction prend en compte le poids de chaque séquence de membre. L'erreur maximale est de 1%. La consommation de mémoire est `log(n)`, où `n` est un certain nombre de valeurs.
-
-La performance de la fonction est inférieure à la performance de [quantile](#quantile) ou [quantileTiming](#quantiletiming). En termes de rapport entre la taille de L'état et la précision, cette fonction est bien meilleure que `quantile`.
-
-Le résultat dépend de l'ordre d'exécution de la requête et n'est pas déterministe.
-
-Lorsque vous utilisez plusieurs `quantile*` fonctionne avec différents niveaux dans une requête, les états internes ne sont pas combinées (qui est, la requête fonctionne moins efficacement qu'il le pouvait). Dans ce cas, utilisez la [les quantiles](#quantiles) fonction.
-
-**Syntaxe**
-
-``` sql
-quantileTDigest(level)(expr)
-```
-
-Alias: `medianTDigest`.
-
-**Paramètre**
-
--   `level` — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a `level` la valeur dans la plage de `[0.01, 0.99]`. Valeur par défaut: 0.5. À `level=0.5` la fonction calcule [médian](https://en.wikipedia.org/wiki/Median).
--   `expr` — Expression over the column values resulting in numeric [types de données](../../sql-reference/data-types/index.md#data_types), [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
--   `weight` — Column with weights of sequence elements. Weight is a number of value occurrences.
-
-**Valeur renvoyée**
-
--   Approximative de quantiles de niveau spécifié.
-
-Type:
-
--   [Float64](../../sql-reference/data-types/float.md) pour l'entrée de type de données numériques.
--   [Date](../../sql-reference/data-types/date.md) si les valeurs d'entrée ont le `Date` type.
--   [DateTime](../../sql-reference/data-types/datetime.md) si les valeurs d'entrée ont le `DateTime` type.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT quantileTDigestWeighted(number, 1) FROM numbers(10)
-```
-
-Résultat:
-
-``` text
-┌─quantileTDigestWeighted(number, 1)─┐
-│                                4.5 │
-└────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [médian](#median)
--   [les quantiles](#quantiles)
-
-## médian {#median}
-
-Le `median*` les fonctions sont les Alias pour le correspondant `quantile*` fonction. Ils calculent la médiane d'un échantillon de données numériques.
-
-Fonction:
-
--   `median` — Alias for [quantile](#quantile).
--   `medianDeterministic` — Alias for [quantileDeterministic](#quantiledeterministic).
--   `medianExact` — Alias for [quantileExact](#quantileexact).
--   `medianExactWeighted` — Alias for [quantileExactWeighted](#quantileexactweighted).
--   `medianTiming` — Alias for [quantileTiming](#quantiletiming).
--   `medianTimingWeighted` — Alias for [quantileTimingWeighted](#quantiletimingweighted).
--   `medianTDigest` — Alias for [quantileTDigest](#quantiletdigest).
--   `medianTDigestWeighted` — Alias for [quantileTDigestWeighted](#quantiletdigestweighted).
-
-**Exemple**
-
-Table d'entrée:
-
-``` text
-┌─val─┐
-│   1 │
-│   1 │
-│   2 │
-│   3 │
-└─────┘
-```
-
-Requête:
-
-``` sql
-SELECT medianDeterministic(val, 1) FROM t
-```
-
-Résultat:
-
-``` text
-┌─medianDeterministic(val, 1)─┐
-│                         1.5 │
-└─────────────────────────────┘
-```
-
-## quantiles(level1, level2, …)(x) {#quantiles}
-
-Toutes les fonctions quantiles ont également des fonctions quantiles correspondantes: `quantiles`, `quantilesDeterministic`, `quantilesTiming`, `quantilesTimingWeighted`, `quantilesExact`, `quantilesExactWeighted`, `quantilesTDigest`. Ces fonctions calculent tous les quantiles des niveaux listés en une seule passe et renvoient un tableau des valeurs résultantes.
-
-## varSamp (x) {#varsampx}
-
-Calcule le montant `Σ((x - x̅)^2) / (n - 1)`, où `n` est la taille de l'échantillon et `x̅`est la valeur moyenne de `x`.
-
-Il représente une estimation non biaisée de la variance d'une variable aléatoire si les valeurs passées forment son échantillon.
-
-Retourner `Float64`. Lorsque `n <= 1`, retourner `+∞`.
-
-!!! note "Note"
-    Cette fonction utilise un algorithme numériquement instable. Si vous avez besoin d' [stabilité numérique](https://en.wikipedia.org/wiki/Numerical_stability) dans les calculs, utiliser le `varSampStable` fonction. Il fonctionne plus lentement, mais fournit une erreur de calcul inférieure.
-
-## varPop (x) {#varpopx}
-
-Calcule le montant `Σ((x - x̅)^2) / n`, où `n` est la taille de l'échantillon et `x̅`est la valeur moyenne de `x`.
-
-En d'autres termes, dispersion pour un ensemble de valeurs. Retourner `Float64`.
-
-!!! note "Note"
-    Cette fonction utilise un algorithme numériquement instable. Si vous avez besoin d' [stabilité numérique](https://en.wikipedia.org/wiki/Numerical_stability) dans les calculs, utiliser le `varPopStable` fonction. Il fonctionne plus lentement, mais fournit une erreur de calcul inférieure.
-
-## stddevSamp (x) {#stddevsampx}
-
-Le résultat est égal à la racine carrée de `varSamp(x)`.
-
-!!! note "Note"
-    Cette fonction utilise un algorithme numériquement instable. Si vous avez besoin d' [stabilité numérique](https://en.wikipedia.org/wiki/Numerical_stability) dans les calculs, utiliser le `stddevSampStable` fonction. Il fonctionne plus lentement, mais fournit une erreur de calcul inférieure.
-
-## stddevPop (x) {#stddevpopx}
-
-Le résultat est égal à la racine carrée de `varPop(x)`.
-
-!!! note "Note"
-    Cette fonction utilise un algorithme numériquement instable. Si vous avez besoin d' [stabilité numérique](https://en.wikipedia.org/wiki/Numerical_stability) dans les calculs, utiliser le `stddevPopStable` fonction. Il fonctionne plus lentement, mais fournit une erreur de calcul inférieure.
-
-## topK (N) (x) {#topknx}
-
-Renvoie un tableau des valeurs approximativement les plus fréquentes dans la colonne spécifiée. Le tableau est trié par ordre décroissant de fréquence approximative des valeurs (et non par les valeurs elles-mêmes).
-
-Met en œuvre la [Gain De Place Filtré](http://www.l2f.inesc-id.pt/~fmmb/wiki/uploads/Work/misnis.ref0a.pdf) algorithme d'analyse de TopK, basé sur l'algorithme de réduction et de combinaison de [Économie D'Espace Parallèle](https://arxiv.org/pdf/1401.0702.pdf).
-
-``` sql
-topK(N)(column)
-```
-
-Cette fonction ne fournit pas un résultat garanti. Dans certaines situations, des erreurs peuvent se produire et renvoyer des valeurs fréquentes qui ne sont pas les valeurs les plus fréquentes.
-
-Nous vous recommandons d'utiliser l' `N < 10` valeur; performance est réduite avec grand `N` valeur. Valeur maximale de `N = 65536`.
-
-**Paramètre**
-
--   ‘N’ est le nombre d'éléments de retour.
-
-Si le paramètre est omis, la valeur par défaut 10 est utilisé.
-
-**Argument**
-
--   ' x ' – The value to calculate frequency.
-
-**Exemple**
-
-Prendre la [OnTime](../../getting-started/example-datasets/ontime.md) ensemble de données et sélectionnez les trois valeurs les plus fréquentes `AirlineID` colonne.
-
-``` sql
-SELECT topK(3)(AirlineID) AS res
-FROM ontime
-```
-
-``` text
-┌─res─────────────────┐
-│ [19393,19790,19805] │
-└─────────────────────┘
-```
-
-## topKWeighted {#topkweighted}
-
-Semblable à `topK` mais prend un argument de type entier - `weight`. Chaque valeur est comptabilisée `weight` les temps de calcul de fréquence.
-
-**Syntaxe**
-
-``` sql
-topKWeighted(N)(x, weight)
-```
-
-**Paramètre**
-
--   `N` — The number of elements to return.
-
-**Argument**
-
--   `x` – The value.
--   `weight` — The weight. [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valeur renvoyée**
-
-Renvoie un tableau des valeurs avec la somme approximative maximale des poids.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT topKWeighted(10)(number, number) FROM numbers(1000)
-```
-
-Résultat:
-
-``` text
-┌─topKWeighted(10)(number, number)──────────┐
-│ [999,998,997,996,995,994,993,992,991,990] │
-└───────────────────────────────────────────┘
-```
-
-## covarSamp(x, y) {#covarsampx-y}
-
-Calcule la valeur de `Σ((x - x̅)(y - y̅)) / (n - 1)`.
-
-Renvoie Float64. Lorsque `n <= 1`, returns +∞.
-
-!!! note "Note"
-    Cette fonction utilise un algorithme numériquement instable. Si vous avez besoin d' [stabilité numérique](https://en.wikipedia.org/wiki/Numerical_stability) dans les calculs, utiliser le `covarSampStable` fonction. Il fonctionne plus lentement, mais fournit une erreur de calcul inférieure.
-
-## covarPop (x, y) {#covarpopx-y}
-
-Calcule la valeur de `Σ((x - x̅)(y - y̅)) / n`.
-
-!!! note "Note"
-    Cette fonction utilise un algorithme numériquement instable. Si vous avez besoin d' [stabilité numérique](https://en.wikipedia.org/wiki/Numerical_stability) dans les calculs, utiliser le `covarPopStable` fonction. Cela fonctionne plus lentement mais fournit une erreur de calcul inférieure.
-
-## corr (x, y) {#corrx-y}
-
-Calcule le coefficient de corrélation de Pearson: `Σ((x - x̅)(y - y̅)) / sqrt(Σ((x - x̅)^2) * Σ((y - y̅)^2))`.
-
-!!! note "Note"
-    Cette fonction utilise un algorithme numériquement instable. Si vous avez besoin d' [stabilité numérique](https://en.wikipedia.org/wiki/Numerical_stability) dans les calculs, utiliser le `corrStable` fonction. Il fonctionne plus lentement, mais fournit une erreur de calcul inférieure.
-
-## categoricalInformationValue {#categoricalinformationvalue}
-
-Calcule la valeur de `(P(tag = 1) - P(tag = 0))(log(P(tag = 1)) - log(P(tag = 0)))` pour chaque catégorie.
-
-``` sql
-categoricalInformationValue(category1, category2, ..., tag)
-```
-
-Le résultat indique comment une caractéristique discrète (catégorique) `[category1, category2, ...]` contribuer à un modèle d'apprentissage qui prédit la valeur de `tag`.
-
-## simplelineearregression {#simplelinearregression}
-
-Effectue une régression linéaire simple (unidimensionnelle).
-
-``` sql
-simpleLinearRegression(x, y)
-```
-
-Paramètre:
-
--   `x` — Column with dependent variable values.
--   `y` — Column with explanatory variable values.
-
-Valeurs renvoyées:
-
-Constant `(a, b)` de la ligne `y = a*x + b`.
-
-**Exemple**
-
-``` sql
-SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3])
-```
-
-``` text
-┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3])─┐
-│ (1,0)                                                             │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6])
-```
-
-``` text
-┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6])─┐
-│ (1,3)                                                             │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-## stochasticLinearRegression {#agg_functions-stochasticlinearregression}
-
-Cette fonction implémente la régression linéaire stochastique. Il prend en charge les paramètres personnalisés pour le taux d'apprentissage, le coefficient de régularisation L2, la taille de mini-lot et a peu de méthodes pour mettre à jour les poids ([Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) (utilisé par défaut), [simple SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), [Élan](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum), [Nesterov](https://mipt.ru/upload/medialibrary/d7e/41-91.pdf)).
-
-### Paramètre {#agg_functions-stochasticlinearregression-parameters}
-
-Il y a 4 paramètres personnalisables. Ils sont passés à la fonction séquentiellement, mais il n'est pas nécessaire de passer tous les quatre-les valeurs par défaut seront utilisées, mais un bon modèle nécessite un réglage des paramètres.
-
-``` text
-stochasticLinearRegression(1.0, 1.0, 10, 'SGD')
-```
-
-1.  `learning rate` est le coefficient sur la longueur de l'étape, lorsque l'étape de descente de gradient est effectuée. Un taux d'apprentissage trop élevé peut entraîner des poids infinis du modèle. Par défaut est `0.00001`.
-2.  `l2 regularization coefficient` ce qui peut aider à éviter le surajustement. Par défaut est `0.1`.
-3.  `mini-batch size` définit le nombre d'éléments, dont les gradients seront calculés et additionnés pour effectuer une étape de descente de gradient. La descente stochastique Pure utilise un élément, mais avoir de petits lots (environ 10 éléments) rend les étapes de gradient plus stables. Par défaut est `15`.
-4.  `method for updating weights` ils sont: `Adam` (par défaut), `SGD`, `Momentum`, `Nesterov`. `Momentum` et `Nesterov` nécessitent un peu plus de calculs et de mémoire, mais ils sont utiles en termes de vitesse de convergence et de stabilité des méthodes de gradient stochastique.
-
-### Utilisation {#agg_functions-stochasticlinearregression-usage}
-
-`stochasticLinearRegression` est utilisé en deux étapes: ajustement du modèle et prédiction sur de nouvelles données. Afin de correspondre le modèle et l'enregistrer son état pour utilisation ultérieure nous utilisons `-State` combinator, qui enregistre essentiellement l'état (poids du modèle, etc.).
-Pour prédire nous utilisons la fonction [evalMLMethod](../functions/machine-learning-functions.md#machine_learning_methods-evalmlmethod) qui prend un état comme un argument ainsi que des fonctionnalités à prévoir sur.
-
-<a name="stochasticlinearregression-usage-fitting"></a>
-
-**1.** Raccord
-
-Une telle requête peut être utilisé.
-
-``` sql
-CREATE TABLE IF NOT EXISTS train_data
-(
-    param1 Float64,
-    param2 Float64,
-    target Float64
-) ENGINE = Memory;
-
-CREATE TABLE your_model ENGINE = Memory AS SELECT
-stochasticLinearRegressionState(0.1, 0.0, 5, 'SGD')(target, param1, param2)
-AS state FROM train_data;
-```
-
-Ici, nous devons également insérer des données dans `train_data` table. Le nombre de paramètres n'est pas fixe, il dépend uniquement du nombre d'arguments, passés dans `linearRegressionState`. Ils doivent tous être des valeurs numériques.
-Notez que la colonne avec la valeur cible (que nous aimerions apprendre à prédire) est insérée comme premier argument.
-
-**2.** Prédire
-
-Après avoir enregistré un État dans la table, nous pouvons l'utiliser plusieurs fois pour la prédiction, ou même fusionner avec d'autres États et créer de nouveaux modèles encore meilleurs.
-
-``` sql
-WITH (SELECT state FROM your_model) AS model SELECT
-evalMLMethod(model, param1, param2) FROM test_data
-```
-
-La requête renvoie une colonne de valeurs prédites. Notez que le premier argument de `evalMLMethod` être `AggregateFunctionState` objet, sont ensuite des colonnes de fonctionnalités.
-
-`test_data` est un tableau comme `train_data` mais peut ne pas contenir de valeur cible.
-
-### Note {#agg_functions-stochasticlinearregression-notes}
-
-1.  Pour fusionner deux modèles l'utilisateur peut créer une telle requête:
-    `sql  SELECT state1 + state2 FROM your_models`
-    où `your_models` le tableau contient les deux modèles. Cette requête renvoie la nouvelle `AggregateFunctionState` objet.
-
-2.  L'utilisateur peut récupérer les poids du modèle pour ses propres fins, sans enregistrer le modèle, si aucune `-State` combinator est utilisé.
-    `sql  SELECT stochasticLinearRegression(0.01)(target, param1, param2) FROM train_data`
-    Une telle requête s'adaptera au Modèle et retournera ses poids-d'abord sont des poids, qui correspondent aux paramètres du modèle, le dernier est un biais. Ainsi, dans l'exemple ci-dessus, la requête renvoie une colonne avec 3 valeurs.
-
-**Voir Aussi**
-
--   [stochasticLogisticRegression](#agg_functions-stochasticlogisticregression)
--   [Différence entre les régressions linéaires et logistiques](https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression)
-
-## stochasticLogisticRegression {#agg_functions-stochasticlogisticregression}
-
-Cette fonction implémente la régression logistique stochastique. Il peut être utilisé pour le problème de classification binaire, prend en charge les mêmes paramètres personnalisés que stochasticLinearRegression et fonctionne de la même manière.
-
-### Paramètre {#agg_functions-stochasticlogisticregression-parameters}
-
-Les paramètres sont exactement les mêmes que dans stochasticLinearRegression:
-`learning rate`, `l2 regularization coefficient`, `mini-batch size`, `method for updating weights`.
-Pour plus d'informations, voir [paramètre](#agg_functions-stochasticlinearregression-parameters).
-
-``` text
-stochasticLogisticRegression(1.0, 1.0, 10, 'SGD')
-```
-
-1.  Raccord
-
-<!-- -->
-
-    See the `Fitting` section in the [stochasticLinearRegression](#stochasticlinearregression-usage-fitting) description.
-
-    Predicted labels have to be in \[-1, 1\].
-
-1.  Prédire
-
-<!-- -->
-
-    Using saved state we can predict probability of object having label `1`.
-
-    ``` sql
-    WITH (SELECT state FROM your_model) AS model SELECT
-    evalMLMethod(model, param1, param2) FROM test_data
-    ```
-
-    The query will return a column of probabilities. Note that first argument of `evalMLMethod` is `AggregateFunctionState` object, next are columns of features.
-
-    We can also set a bound of probability, which assigns elements to different labels.
-
-    ``` sql
-    SELECT ans < 1.1 AND ans > 0.5 FROM
-    (WITH (SELECT state FROM your_model) AS model SELECT
-    evalMLMethod(model, param1, param2) AS ans FROM test_data)
-    ```
-
-    Then the result will be labels.
-
-    `test_data` is a table like `train_data` but may not contain target value.
-
-**Voir Aussi**
-
--   [stochasticLinearRegression](#agg_functions-stochasticlinearregression)
--   [Différence entre les régressions linéaires et logistiques.](https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression)
-
-## groupBitmapAnd {#groupbitmapand}
-
-Calculs le et d'une colonne bitmap, retour cardinalité de type UInt64, si Ajouter suffixe-État, puis retour [objet bitmap](../../sql-reference/functions/bitmap-functions.md).
-
-``` sql
-groupBitmapAnd(expr)
-```
-
-**Paramètre**
-
-`expr` – An expression that results in `AggregateFunction(groupBitmap, UInt*)` type.
-
-**Valeur de retour**
-
-La valeur de la `UInt64` type.
-
-**Exemple**
-
-``` sql
-DROP TABLE IF EXISTS bitmap_column_expr_test2;
-CREATE TABLE bitmap_column_expr_test2
-(
-    tag_id String,
-    z AggregateFunction(groupBitmap, UInt32)
-)
-ENGINE = MergeTree
-ORDER BY tag_id;
-
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32))));
-
-SELECT groupBitmapAnd(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─groupBitmapAnd(z)─┐
-│               3   │
-└───────────────────┘
-
-SELECT arraySort(bitmapToArray(groupBitmapAndState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─arraySort(bitmapToArray(groupBitmapAndState(z)))─┐
-│ [6,8,10]                                         │
-└──────────────────────────────────────────────────┘
-```
-
-## groupBitmapOr {#groupbitmapor}
-
-Calculs le ou d'une colonne bitmap, retour cardinalité de type UInt64, si Ajouter suffixe-État, puis retour [objet bitmap](../../sql-reference/functions/bitmap-functions.md). C'est l'équivalent de `groupBitmapMerge`.
-
-``` sql
-groupBitmapOr(expr)
-```
-
-**Paramètre**
-
-`expr` – An expression that results in `AggregateFunction(groupBitmap, UInt*)` type.
-
-**Valeur de retour**
-
-La valeur de la `UInt64` type.
-
-**Exemple**
-
-``` sql
-DROP TABLE IF EXISTS bitmap_column_expr_test2;
-CREATE TABLE bitmap_column_expr_test2
-(
-    tag_id String,
-    z AggregateFunction(groupBitmap, UInt32)
-)
-ENGINE = MergeTree
-ORDER BY tag_id;
-
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32))));
-
-SELECT groupBitmapOr(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─groupBitmapOr(z)─┐
-│             15   │
-└──────────────────┘
-
-SELECT arraySort(bitmapToArray(groupBitmapOrState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─arraySort(bitmapToArray(groupBitmapOrState(z)))─┐
-│ [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]           │
-└─────────────────────────────────────────────────┘
-```
-
-## groupBitmapXor {#groupbitmapxor}
-
-Calculs le XOR d'une colonne bitmap, retour cardinalité de type UInt64, si Ajouter suffixe-État, puis retour [objet bitmap](../../sql-reference/functions/bitmap-functions.md).
-
-``` sql
-groupBitmapOr(expr)
-```
-
-**Paramètre**
-
-`expr` – An expression that results in `AggregateFunction(groupBitmap, UInt*)` type.
-
-**Valeur de retour**
-
-La valeur de la `UInt64` type.
-
-**Exemple**
-
-``` sql
-DROP TABLE IF EXISTS bitmap_column_expr_test2;
-CREATE TABLE bitmap_column_expr_test2
-(
-    tag_id String,
-    z AggregateFunction(groupBitmap, UInt32)
-)
-ENGINE = MergeTree
-ORDER BY tag_id;
-
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32))));
-INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32))));
-
-SELECT groupBitmapXor(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─groupBitmapXor(z)─┐
-│              10   │
-└───────────────────┘
-
-SELECT arraySort(bitmapToArray(groupBitmapXorState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%');
-┌─arraySort(bitmapToArray(groupBitmapXorState(z)))─┐
-│ [1,3,5,6,8,10,11,13,14,15]                       │
-└──────────────────────────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/agg_functions/reference/) <!--hide-->
diff --git a/docs/fr/sql-reference/ansi.md b/docs/fr/sql-reference/ansi.md
deleted file mode 100644
index 9fd4ed428a2c..000000000000
--- a/docs/fr/sql-reference/ansi.md
+++ /dev/null
@@ -1,180 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: ad252bbb4f7e2899c448eb42ecc39ff195c8faa1
-toc_priority: 40
-toc_title: "La Compatibilit\xE9 ANSI"
----
-
-# Compatibilité ANSI SQL du dialecte CLICKHOUSE SQL {#ansi-sql-compatibility-of-clickhouse-sql-dialect}
-
-!!! note "Note"
-    Cet article s'appuie sur le tableau 38, “Feature taxonomy and definition for mandatory features”, Annex F of ISO/IEC CD 9075-2:2013.
-
-## Différences de comportement {#differences-in-behaviour}
-
-Le tableau suivant répertorie les cas où la fonctionnalité de requête fonctionne dans ClickHouse, mais ne se comporte pas comme spécifié dans ANSI SQL.
-
-| Feature ID | Nom De La Fonctionnalité                        | Différence                                                                                                                            |
-|------------|-------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|
-| E011       | Types de données numériques                     | Le littéral numérique avec période est interprété comme approximatif (`Float64`) au lieu de exact (`Decimal`)                         |
-| E051-05    | Les éléments sélectionnés peuvent être renommés | Les renommages d'élément ont une portée de visibilité plus large que le simple résultat de sélection                                  |
-| E141-01    | Contraintes non nulles                          | `NOT NULL` est implicite pour les colonnes de table par défaut                                                                        |
-| E011-04    | Opérateurs arithmétiques                        | Clickhouse déborde au lieu de l'arithmétique vérifiée et modifie le type de données de résultat en fonction des règles personnalisées |
-
-## Fonction D'État {#feature-status}
-
-| Feature ID | Nom De La Fonctionnalité                                                                                                                                    | Statut                     | Commentaire                                                                                                                                                                                                           |
-|------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| **E011**   | **Types de données numériques**                                                                                                                             | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E011-01    | Types de données INTEGER et SMALLINT                                                                                                                        | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E011-02    | Types de données réel, double précision et flottant types de données                                                                                        | Partiel {.text-warning}     | `FLOAT(<binary_precision>)`, `REAL` et `DOUBLE PRECISION` ne sont pas pris en charge                                                                                                                                  |
-| E011-03    | Types de données décimales et numériques                                                                                                                    | Partiel {.text-warning}     | Seulement `DECIMAL(p,s)` est pris en charge, pas `NUMERIC`                                                                                                                                                            |
-| E011-04    | Opérateurs arithmétiques                                                                                                                                    | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E011-05    | Comparaison numérique                                                                                                                                       | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E011-06    | Casting implicite parmi les types de données numériques                                                                                                     | Aucun {.text-danger}        | ANSI SQL permet la distribution implicite arbitraire entre les types numériques, tandis que ClickHouse repose sur des fonctions ayant plusieurs surcharges au lieu de la distribution implicite                       |
-| **E021**   | **Types de chaînes de caractères**                                                                                                                          | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E021-01    | Type de données CARACTÈRE                                                                                                                                   | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E021-02    | TYPE DE DONNÉES variable de caractère                                                                                                                       | Aucun {.text-danger}        | `String` se comporte de la même manière, mais sans limite de longueur entre parenthèses                                                                                                                               |
-| E021-03    | Littéraux de caractères                                                                                                                                     | Partiel {.text-warning}     | Aucune concaténation automatique de littéraux consécutifs et prise en charge du jeu de caractères                                                                                                                     |
-| E021-04    | Fonction CHARACTER_LENGTH                                                                                                                                  | Partiel {.text-warning}     | Aucun `USING` clause                                                                                                                                                                                                  |
-| E021-05    | Fonction OCTET_LENGTH                                                                                                                                      | Aucun {.text-danger}        | `LENGTH` se comporte de la même façon                                                                                                                                                                                 |
-| E021-06    | SUBSTRING                                                                                                                                                   | Partiel {.text-warning}     | Pas de support pour `SIMILAR` et `ESCAPE` clauses, pas de `SUBSTRING_REGEX` variante                                                                                                                                  |
-| E021-07    | Concaténation de caractères                                                                                                                                 | Partiel {.text-warning}     | Aucun `COLLATE` clause                                                                                                                                                                                                |
-| E021-08    | Fonctions supérieures et inférieures                                                                                                                        | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E021-09    | La fonction TRIM                                                                                                                                            | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E021-10    | Conversion implicite entre les types de chaînes de caractères de longueur fixe et de longueur variable                                                      | Aucun {.text-danger}        | ANSI SQL permet la distribution implicite arbitraire entre les types de chaîne, tandis que ClickHouse repose sur des fonctions ayant plusieurs surcharges au lieu de la distribution implicite                        |
-| E021-11    | La POSITION de la fonction                                                                                                                                  | Partiel {.text-warning}     | Pas de support pour `IN` et `USING` clauses, pas de `POSITION_REGEX` variante                                                                                                                                         |
-| E021-12    | Comparaison de caractères                                                                                                                                   | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| **E031**   | **Identificateur**                                                                                                                                          | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E031-01    | Identificateurs délimités                                                                                                                                   | Partiel {.text-warning}     | Le support littéral Unicode est limité                                                                                                                                                                                |
-| E031-02    | Identificateurs minuscules                                                                                                                                  | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E031-03    | Fuite de soulignement                                                                                                                                       | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| **E051**   | **Spécification de requête de base**                                                                                                                        | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E051-01    | SELECT DISTINCT                                                                                                                                             | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E051-02    | Groupe par clause                                                                                                                                           | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E051-04    | GROUP BY peut contenir des colonnes `<select list>`                                                                                                         | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E051-05    | Les éléments sélectionnés peuvent être renommés                                                                                                             | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E051-06    | Clause HAVING                                                                                                                                               | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E051-07    | Qualifié \* dans la liste select                                                                                                                            | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E051-08    | Nom de corrélation dans la clause FROM                                                                                                                      | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E051-09    | Renommer les colonnes de la clause FROM                                                                                                                     | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **E061**   | **Prédicats de base et conditions de recherche**                                                                                                            | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E061-01    | Prédicat de comparaison                                                                                                                                     | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E061-02    | Entre prédicat                                                                                                                                              | Partiel {.text-warning}     | Aucun `SYMMETRIC` et `ASYMMETRIC` clause                                                                                                                                                                              |
-| E061-03    | Dans le prédicat avec la liste des valeurs                                                                                                                  | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E061-04    | Comme prédicat                                                                                                                                              | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E061-05    | Comme prédicat: clause D'échappement                                                                                                                        | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E061-06    | Prédicat NULL                                                                                                                                               | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E061-07    | Prédicat de comparaison quantifié                                                                                                                           | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E061-08    | Existe prédicat                                                                                                                                             | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E061-09    | Sous-requêtes dans le prédicat de comparaison                                                                                                               | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E061-11    | Sous-requêtes dans dans le prédicat                                                                                                                         | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E061-12    | Sous-requêtes dans le prédicat de comparaison quantifiée                                                                                                    | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E061-13    | Sous-requêtes corrélées                                                                                                                                     | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E061-14    | Condition de recherche                                                                                                                                      | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| **E071**   | **Expressions de requête de base**                                                                                                                          | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E071-01    | Opérateur de table distinct UNION                                                                                                                           | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E071-02    | Opérateur de table UNION ALL                                                                                                                                | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E071-03    | Sauf opérateur de table DISTINCT                                                                                                                            | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E071-05    | Les colonnes combinées via les opérateurs de table n'ont pas besoin d'avoir exactement le même type de données                                              | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E071-06    | Tableau des opérateurs dans les sous-requêtes                                                                                                               | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| **E081**   | **Les privilèges de base**                                                                                                                                  | **Partiel**{.text-warning} | Les travaux en cours                                                                                                                                                                                                  |
-| **E091**   | **Les fonctions de jeu**                                                                                                                                    | **Oui**{.text-success}     |                                                                                                                                                                                                                       |
-| E091-01    | AVG                                                                                                                                                         | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E091-02    | COUNT                                                                                                                                                       | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E091-03    | MAX                                                                                                                                                         | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E091-04    | MIN                                                                                                                                                         | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E091-05    | SUM                                                                                                                                                         | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E091-06    | TOUS les quantificateurs                                                                                                                                    | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E091-07    | Quantificateur DISTINCT                                                                                                                                     | Partiel {.text-warning}     | Toutes les fonctions d'agrégation ne sont pas prises en charge                                                                                                                                                        |
-| **E101**   | **Manipulation des données de base**                                                                                                                        | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E101-01    | Insérer une déclaration                                                                                                                                     | Oui {.text-success}         | Remarque: la clé primaire dans ClickHouse n'implique pas `UNIQUE` contrainte                                                                                                                                          |
-| E101-03    | Déclaration de mise à jour recherchée                                                                                                                       | Aucun {.text-danger}        | Il y a un `ALTER UPDATE` déclaration pour la modification des données de lot                                                                                                                                          |
-| E101-04    | Requête de suppression recherchée                                                                                                                           | Aucun {.text-danger}        | Il y a un `ALTER DELETE` déclaration pour la suppression de données par lots                                                                                                                                          |
-| **E111**   | **Instruction SELECT à une ligne**                                                                                                                          | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| **E121**   | **Prise en charge du curseur de base**                                                                                                                      | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| E121-01    | DECLARE CURSOR                                                                                                                                              | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-02    | Les colonnes ORDER BY n'ont pas besoin d'être dans la liste select                                                                                          | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-03    | Expressions de valeur dans la clause ORDER BY                                                                                                               | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-04    | Instruction OPEN                                                                                                                                            | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-06    | Déclaration de mise à jour positionnée                                                                                                                      | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-07    | Instruction de suppression positionnée                                                                                                                      | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-08    | Déclaration de fermeture                                                                                                                                    | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-10    | Instruction FETCH: implicite suivant                                                                                                                        | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E121-17    | Avec curseurs HOLD                                                                                                                                          | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **E131**   | **Support de valeur Null (nulls au lieu de valeurs)**                                                                                                       | **Partiel**{.text-warning} | Certaines restrictions s'appliquent                                                                                                                                                                                   |
-| **E141**   | **Contraintes d'intégrité de base**                                                                                                                         | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| E141-01    | Contraintes non nulles                                                                                                                                      | Oui {.text-success}         | Note: `NOT NULL` est implicite pour les colonnes de table par défaut                                                                                                                                                  |
-| E141-02    | Contrainte UNIQUE de colonnes non nulles                                                                                                                    | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E141-03    | Contraintes de clé primaire                                                                                                                                 | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E141-04    | Contrainte de clé étrangère de base avec la valeur par défaut NO ACTION Pour l'action de suppression référentielle et l'action de mise à jour référentielle | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E141-06    | Vérifier la contrainte                                                                                                                                      | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E141-07    | Colonne par défaut                                                                                                                                          | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E141-08    | Non NULL déduit sur la clé primaire                                                                                                                         | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| E141-10    | Les noms dans une clé étrangère peut être spécifié dans n'importe quel ordre                                                                                | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **E151**   | **Support de Transaction**                                                                                                                                  | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| E151-01    | COMMIT déclaration                                                                                                                                          | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E151-02    | Déclaration de restauration                                                                                                                                 | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **E152**   | **Instruction de transaction set de base**                                                                                                                  | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| E152-01    | SET TRANSACTION statement: clause sérialisable de niveau D'isolement                                                                                        | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| E152-02    | SET TRANSACTION statement: clauses en lecture seule et en lecture écriture                                                                                  | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **E153**   | **Requêtes pouvant être mises à jour avec des sous requêtes**                                                                                               | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| **E161**   | **Commentaires SQL en utilisant le premier Double moins**                                                                                                   | **Oui**{.text-success}     |                                                                                                                                                                                                                       |
-| **E171**   | **Support SQLSTATE**                                                                                                                                        | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| **E182**   | **Liaison du langage hôte**                                                                                                                                 | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| **F031**   | **Manipulation de schéma de base**                                                                                                                          | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| F031-01    | Instruction CREATE TABLE pour créer des tables de base persistantes                                                                                         | Partiel {.text-warning}     | Aucun `SYSTEM VERSIONING`, `ON COMMIT`, `GLOBAL`, `LOCAL`, `PRESERVE`, `DELETE`, `REF IS`, `WITH OPTIONS`, `UNDER`, `LIKE`, `PERIOD FOR` clauses et aucun support pour les types de données résolus par l'utilisateur |
-| F031-02    | Instruction créer une vue                                                                                                                                   | Partiel {.text-warning}     | Aucun `RECURSIVE`, `CHECK`, `UNDER`, `WITH OPTIONS` clauses et aucun support pour les types de données résolus par l'utilisateur                                                                                      |
-| F031-03    | Déclaration de subvention                                                                                                                                   | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F031-04    | ALTER TABLE statement: ajouter une clause de colonne                                                                                                        | Partiel {.text-warning}     | Pas de support pour `GENERATED` clause et période de temps du système                                                                                                                                                 |
-| F031-13    | Instruction DROP TABLE: clause RESTRICT                                                                                                                     | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| F031-16    | Instruction DROP VIEW: clause RESTRICT                                                                                                                      | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| F031-19    | REVOKE statement: clause RESTRICT                                                                                                                           | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **F041**   | **Table jointe de base**                                                                                                                                    | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| F041-01    | INNER join (mais pas nécessairement le mot-clé INNER)                                                                                                       | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F041-02    | INTÉRIEURE mot-clé                                                                                                                                          | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F041-03    | LEFT OUTER JOIN                                                                                                                                             | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F041-04    | RIGHT OUTER JOIN                                                                                                                                            | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F041-05    | Les jointures externes peuvent être imbriqués                                                                                                               | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F041-07    | La table intérieure dans une jointure extérieure gauche ou droite peut également être utilisée dans une jointure intérieure                                 | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F041-08    | Tous les opérateurs de comparaison sont pris en charge (plutôt que juste =)                                                                                 | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **F051**   | **Date et heure de base**                                                                                                                                   | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| F051-01    | Type de données de DATE (y compris la prise en charge du littéral de DATE)                                                                                  | Partiel {.text-warning}     | Aucun littéral                                                                                                                                                                                                        |
-| F051-02    | TYPE DE DONNÉES DE TEMPS (y compris la prise en charge du littéral de temps) avec une précision de secondes fractionnaires d'au moins 0                     | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| F051-03    | Type de données D'horodatage (y compris la prise en charge du littéral D'horodatage) avec une précision de secondes fractionnaires d'au moins 0 et 6        | Aucun {.text-danger}        | `DateTime64` temps fournit des fonctionnalités similaires                                                                                                                                                             |
-| F051-04    | Prédicat de comparaison sur les types de données DATE, heure et horodatage                                                                                  | Partiel {.text-warning}     | Un seul type de données disponible                                                                                                                                                                                    |
-| F051-05    | Distribution explicite entre les types datetime et les types de chaînes de caractères                                                                       | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F051-06    | CURRENT_DATE                                                                                                                                               | Aucun {.text-danger}        | `today()` est similaire                                                                                                                                                                                               |
-| F051-07    | LOCALTIME                                                                                                                                                   | Aucun {.text-danger}        | `now()` est similaire                                                                                                                                                                                                 |
-| F051-08    | LOCALTIMESTAMP                                                                                                                                              | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **F081**   | **UNION et sauf dans les vues**                                                                                                                             | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| **F131**   | **Groupées des opérations**                                                                                                                                 | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| F131-01    | WHERE, GROUP BY et ayant des clauses prises en charge dans les requêtes avec des vues groupées                                                              | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F131-02    | Plusieurs tables prises en charge dans les requêtes avec des vues groupées                                                                                  | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F131-03    | Définir les fonctions prises en charge dans les requêtes groupées vues                                                                                      | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F131-04    | Sous requêtes avec des clauses GROUP BY et HAVING et des vues groupées                                                                                      | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F131-05    | Sélectionnez une seule ligne avec des clauses GROUP BY et HAVING et des vues groupées                                                                       | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **F181**   | **Support de module Multiple**                                                                                                                              | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| **F201**   | **Fonction de distribution**                                                                                                                                | **Oui**{.text-success}     |                                                                                                                                                                                                                       |
-| **F221**   | **Valeurs par défaut explicites**                                                                                                                           | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| **F261**   | **Expression de cas**                                                                                                                                       | **Oui**{.text-success}     |                                                                                                                                                                                                                       |
-| F261-01    | Cas Simple                                                                                                                                                  | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F261-02    | Cas recherché                                                                                                                                               | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F261-03    | NULLIF                                                                                                                                                      | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F261-04    | COALESCE                                                                                                                                                    | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| **F311**   | **Déclaration de définition de schéma**                                                                                                                     | **Partiel**{.text-warning} |                                                                                                                                                                                                                       |
-| F311-01    | CREATE SCHEMA                                                                                                                                               | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| F311-02    | Créer une TABLE pour les tables de base persistantes                                                                                                        | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F311-03    | CREATE VIEW                                                                                                                                                 | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| F311-04    | CREATE VIEW: WITH CHECK OPTION                                                                                                                              | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| F311-05    | Déclaration de subvention                                                                                                                                   | Oui {.text-success}         |                                                                                                                                                                                                                       |
-| **F471**   | **Valeurs de sous-requête scalaire**                                                                                                                        | **Oui**{.text-success}     |                                                                                                                                                                                                                       |
-| **F481**   | **Prédicat null étendu**                                                                                                                                    | **Oui**{.text-success}     |                                                                                                                                                                                                                       |
-| **F812**   | **Base de repérage**                                                                                                                                        | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| **T321**   | **Routines SQL-invoked de base**                                                                                                                            | **Aucun**{.text-danger}    |                                                                                                                                                                                                                       |
-| T321-01    | Fonctions définies par l'utilisateur sans surcharge                                                                                                         | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| T321-02    | Procédures stockées définies par l'utilisateur sans surcharge                                                                                               | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| T321-03    | L'invocation de la fonction                                                                                                                                 | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| T321-04    | L'instruction d'APPEL de                                                                                                                                    | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| T321-05    | Déclaration de retour                                                                                                                                       | Aucun {.text-danger}        |                                                                                                                                                                                                                       |
-| **T631**   | **Dans le prédicat avec un élément de liste**                                                                                                               | **Oui**{.text-success}     |                                                                                                                                                                                                                       |
diff --git a/docs/fr/sql-reference/data-types/aggregatefunction.md b/docs/fr/sql-reference/data-types/aggregatefunction.md
deleted file mode 100644
index 18874cd3cb76..000000000000
--- a/docs/fr/sql-reference/data-types/aggregatefunction.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 52
-toc_title: AggregateFunction (nom, types_of_arguments...)
----
-
-# AggregateFunction(name, types_of_arguments…) {#data-type-aggregatefunction}
-
-Aggregate functions can have an implementation-defined intermediate state that can be serialized to an AggregateFunction(…) data type and stored in a table, usually, by means of [une vue matérialisée](../../sql-reference/statements/create.md#create-view). La manière courante de produire un État de fonction d'agrégat est d'appeler la fonction d'agrégat avec le `-State` suffixe. Pour obtenir le résultat final de l'agrégation dans l'avenir, vous devez utiliser la même fonction d'agrégation avec la `-Merge`suffixe.
-
-`AggregateFunction` — parametric data type.
-
-**Paramètre**
-
--   Nom de la fonction d'agrégation.
-
-        If the function is parametric, specify its parameters too.
-
--   Types des arguments de la fonction d'agrégation.
-
-**Exemple**
-
-``` sql
-CREATE TABLE t
-(
-    column1 AggregateFunction(uniq, UInt64),
-    column2 AggregateFunction(anyIf, String, UInt8),
-    column3 AggregateFunction(quantiles(0.5, 0.9), UInt64)
-) ENGINE = ...
-```
-
-[uniq](../../sql-reference/aggregate-functions/reference.md#agg_function-uniq), anyIf ([tout](../../sql-reference/aggregate-functions/reference.md#agg_function-any)+[Si](../../sql-reference/aggregate-functions/combinators.md#agg-functions-combinator-if)) et [les quantiles](../../sql-reference/aggregate-functions/reference.md) les fonctions d'agrégation sont-elles prises en charge dans ClickHouse.
-
-## Utilisation {#usage}
-
-### Insertion De Données {#data-insertion}
-
-Pour insérer des données, utilisez `INSERT SELECT` avec le regroupement d' `-State`- fonction.
-
-**Exemples de fonction**
-
-``` sql
-uniqState(UserID)
-quantilesState(0.5, 0.9)(SendTiming)
-```
-
-Contrairement aux fonctions correspondantes `uniq` et `quantiles`, `-State`- les fonctions renvoient l'état, au lieu de la valeur finale. En d'autres termes, ils renvoient une valeur de `AggregateFunction` type.
-
-Dans les résultats de `SELECT` requête, les valeurs de `AggregateFunction` type ont une représentation binaire spécifique à l'implémentation pour tous les formats de sortie ClickHouse. Si les données de vidage dans, par exemple, `TabSeparated` format avec `SELECT` requête, puis ce vidage peut être chargé en utilisant `INSERT` requête.
-
-### Sélection De Données {#data-selection}
-
-Lors de la sélection des données `AggregatingMergeTree` table, utilisez `GROUP BY` et les mêmes fonctions d'agrégat que lors de l'insertion de données, mais en utilisant `-Merge`suffixe.
-
-Une fonction d'agrégation avec `-Merge` suffixe prend un ensemble d'états, les combine, et renvoie le résultat complet de l'agrégation de données.
-
-Par exemple, les deux requêtes suivantes retournent le même résultat:
-
-``` sql
-SELECT uniq(UserID) FROM table
-
-SELECT uniqMerge(state) FROM (SELECT uniqState(UserID) AS state FROM table GROUP BY RegionID)
-```
-
-## Exemple D'Utilisation {#usage-example}
-
-Voir [AggregatingMergeTree](../../engines/table-engines/mergetree-family/aggregatingmergetree.md) Description du moteur.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/nested_data_structures/aggregatefunction/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/array.md b/docs/fr/sql-reference/data-types/array.md
deleted file mode 100644
index 41772cab177a..000000000000
--- a/docs/fr/sql-reference/data-types/array.md
+++ /dev/null
@@ -1,77 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 51
-toc_title: Array(T)
----
-
-# Array(t) {#data-type-array}
-
-Un tableau de `T`les éléments de type. `T` peut être n'importe quel type de données, y compris un tableau.
-
-## La création d'un Tableau {#creating-an-array}
-
-Vous pouvez utiliser une fonction pour créer un tableau:
-
-``` sql
-array(T)
-```
-
-Vous pouvez également utiliser des crochets.
-
-``` sql
-[]
-```
-
-Exemple de création d'un tableau:
-
-``` sql
-SELECT array(1, 2) AS x, toTypeName(x)
-```
-
-``` text
-┌─x─────┬─toTypeName(array(1, 2))─┐
-│ [1,2] │ Array(UInt8)            │
-└───────┴─────────────────────────┘
-```
-
-``` sql
-SELECT [1, 2] AS x, toTypeName(x)
-```
-
-``` text
-┌─x─────┬─toTypeName([1, 2])─┐
-│ [1,2] │ Array(UInt8)       │
-└───────┴────────────────────┘
-```
-
-## Utilisation de Types de données {#working-with-data-types}
-
-Lors de la création d'un tableau à la volée, ClickHouse définit automatiquement le type d'argument comme le type de données le plus étroit pouvant stocker tous les arguments listés. S'il y a des [Nullable](nullable.md#data_type-nullable) ou littéral [NULL](../../sql-reference/syntax.md#null-literal) les valeurs, le type d'un élément de tableau devient également [Nullable](nullable.md).
-
-Si ClickHouse n'a pas pu déterminer le type de données, il génère une exception. Par exemple, cela se produit lorsque vous essayez de créer un tableau avec des chaînes et des nombres simultanément (`SELECT array(1, 'a')`).
-
-Exemples de détection automatique de type de données:
-
-``` sql
-SELECT array(1, 2, NULL) AS x, toTypeName(x)
-```
-
-``` text
-┌─x──────────┬─toTypeName(array(1, 2, NULL))─┐
-│ [1,2,NULL] │ Array(Nullable(UInt8))        │
-└────────────┴───────────────────────────────┘
-```
-
-Si vous essayez de créer un tableau de types de données incompatibles, ClickHouse lève une exception:
-
-``` sql
-SELECT array(1, 'a')
-```
-
-``` text
-Received exception from server (version 1.1.54388):
-Code: 386. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: There is no supertype for types UInt8, String because some of them are String/FixedString and some of them are not.
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/array/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/boolean.md b/docs/fr/sql-reference/data-types/boolean.md
deleted file mode 100644
index aeb84cf1cc10..000000000000
--- a/docs/fr/sql-reference/data-types/boolean.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: "Bool\xE9en"
----
-
-# Les Valeurs Booléennes {#boolean-values}
-
-Il n'y a pas de type distinct pour les valeurs booléennes. Utilisez le type UInt8, limité aux valeurs 0 ou 1.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/boolean/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/date.md b/docs/fr/sql-reference/data-types/date.md
deleted file mode 100644
index 698639f1d2f4..000000000000
--- a/docs/fr/sql-reference/data-types/date.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: Date
----
-
-# Date {#date}
-
-Date. Stocké en deux octets comme le nombre de jours depuis 1970-01-01 (non signé). Permet de stocker des valeurs juste après le début de L'époque Unix jusqu'au seuil supérieur défini par une constante au stade de la compilation (actuellement, c'est jusqu'à l'année 2106, mais l'année finale entièrement prise en charge est 2105).
-
-La valeur de date est stockée sans le fuseau horaire.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/date/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/datetime.md b/docs/fr/sql-reference/data-types/datetime.md
deleted file mode 100644
index 915270e4d2b5..000000000000
--- a/docs/fr/sql-reference/data-types/datetime.md
+++ /dev/null
@@ -1,129 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 48
-toc_title: DateTime
----
-
-# Datetime {#data_type-datetime}
-
-Permet de stocker un instant dans le temps, qui peut être exprimé comme une date de calendrier et une heure d'une journée.
-
-Syntaxe:
-
-``` sql
-DateTime([timezone])
-```
-
-Plage de valeurs prise en charge: \[1970-01-01 00:00:00, 2105-12-31 23:59:59\].
-
-Résolution: 1 seconde.
-
-## Utilisation Remarques {#usage-remarks}
-
-Le point dans le temps est enregistré en tant que [Le timestamp Unix](https://en.wikipedia.org/wiki/Unix_time), quel que soit le fuseau horaire ou l'heure d'été. En outre, l' `DateTime` type peut stocker le fuseau horaire qui est le même pour la colonne entière, qui affecte la façon dont les valeurs de la `DateTime` les valeurs de type sont affichées au format texte et comment les valeurs spécifiées en tant que chaînes sont analysées (‘2020-01-01 05:00:01’). Le fuseau horaire n'est pas stocké dans les lignes de la table (ou dans resultset), mais est stocké dans les métadonnées de la colonne.
-Une liste des fuseaux horaires pris en charge peut être trouvée dans le [Base de données de fuseau horaire IANA](https://www.iana.org/time-zones).
-Le `tzdata` paquet, contenant [Base de données de fuseau horaire IANA](https://www.iana.org/time-zones), doit être installé dans le système. L'utilisation de la `timedatectl list-timezones` commande pour lister les fuseaux horaires connus par un système local.
-
-Vous pouvez définir explicitement un fuseau horaire `DateTime`- tapez des colonnes lors de la création d'une table. Si le fuseau horaire n'est pas défini, ClickHouse utilise la valeur [fuseau](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-timezone) paramètre dans les paramètres du serveur ou les paramètres du système d'exploitation au moment du démarrage du serveur ClickHouse.
-
-Le [clickhouse-client](../../interfaces/cli.md) applique le fuseau horaire du serveur par défaut si un fuseau horaire n'est pas explicitement défini lors de l'initialisation du type de données. Pour utiliser le fuseau horaire du client, exécutez `clickhouse-client` avec l' `--use_client_time_zone` paramètre.
-
-Clickhouse affiche les valeurs dans `YYYY-MM-DD hh:mm:ss` format de texte par défaut. Vous pouvez modifier la sortie avec le [formatDateTime](../../sql-reference/functions/date-time-functions.md#formatdatetime) fonction.
-
-Lorsque vous insérez des données dans ClickHouse, vous pouvez utiliser différents formats de chaînes de date et d'heure, en fonction de la valeur du [date_time_input_format](../../operations/settings/settings.md#settings-date_time_input_format) paramètre.
-
-## Exemple {#examples}
-
-**1.** Création d'une table avec un `DateTime`- tapez la colonne et insérez des données dedans:
-
-``` sql
-CREATE TABLE dt
-(
-    `timestamp` DateTime('Europe/Moscow'),
-    `event_id` UInt8
-)
-ENGINE = TinyLog;
-```
-
-``` sql
-INSERT INTO dt Values (1546300800, 1), ('2019-01-01 00:00:00', 2);
-```
-
-``` sql
-SELECT * FROM dt;
-```
-
-``` text
-┌───────────timestamp─┬─event_id─┐
-│ 2019-01-01 03:00:00 │        1 │
-│ 2019-01-01 00:00:00 │        2 │
-└─────────────────────┴──────────┘
-```
-
--   Lors de l'insertion de datetime en tant qu'entier, il est traité comme un horodatage Unix (UTC). `1546300800` représenter `'2019-01-01 00:00:00'` L'UTC. Cependant, comme `timestamp` la colonne a `Europe/Moscow` (UTC+3) fuseau horaire spécifié, lors de la sortie en tant que chaîne, la valeur sera affichée comme `'2019-01-01 03:00:00'`
--   Lors de l'insertion d'une valeur de chaîne en tant que datetime, elle est traitée comme étant dans le fuseau horaire de la colonne. `'2019-01-01 00:00:00'` sera considérée comme étant en `Europe/Moscow` fuseau horaire et enregistré sous `1546290000`.
-
-**2.** Le filtrage sur `DateTime` valeur
-
-``` sql
-SELECT * FROM dt WHERE timestamp = toDateTime('2019-01-01 00:00:00', 'Europe/Moscow')
-```
-
-``` text
-┌───────────timestamp─┬─event_id─┐
-│ 2019-01-01 00:00:00 │        2 │
-└─────────────────────┴──────────┘
-```
-
-`DateTime` les valeurs de colonne peuvent être filtrées à l'aide d'une `WHERE` prédicat. Elle sera convertie `DateTime` automatiquement:
-
-``` sql
-SELECT * FROM dt WHERE timestamp = '2019-01-01 00:00:00'
-```
-
-``` text
-┌───────────timestamp─┬─event_id─┐
-│ 2019-01-01 03:00:00 │        1 │
-└─────────────────────┴──────────┘
-```
-
-**3.** Obtenir un fuseau horaire pour un `DateTime`colonne de type:
-
-``` sql
-SELECT toDateTime(now(), 'Europe/Moscow') AS column, toTypeName(column) AS x
-```
-
-``` text
-┌──────────────column─┬─x─────────────────────────┐
-│ 2019-10-16 04:12:04 │ DateTime('Europe/Moscow') │
-└─────────────────────┴───────────────────────────┘
-```
-
-**4.** Conversion de fuseau horaire
-
-``` sql
-SELECT
-toDateTime(timestamp, 'Europe/London') as lon_time,
-toDateTime(timestamp, 'Europe/Moscow') as mos_time
-FROM dt
-```
-
-``` text
-┌───────────lon_time──┬────────────mos_time─┐
-│ 2019-01-01 00:00:00 │ 2019-01-01 03:00:00 │
-│ 2018-12-31 21:00:00 │ 2019-01-01 00:00:00 │
-└─────────────────────┴─────────────────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [Fonctions de conversion de Type](../../sql-reference/functions/type-conversion-functions.md)
--   [Fonctions pour travailler avec des dates et des heures](../../sql-reference/functions/date-time-functions.md)
--   [Fonctions pour travailler avec des tableaux](../../sql-reference/functions/array-functions.md)
--   [Le `date_time_input_format` paramètre](../../operations/settings/settings.md#settings-date_time_input_format)
--   [Le `timezone` paramètre de configuration du serveur](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-timezone)
--   [Opérateurs pour travailler avec des dates et des heures](../../sql-reference/operators/index.md#operators-datetime)
--   [Le `Date` type de données](date.md)
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/datetime/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/datetime64.md b/docs/fr/sql-reference/data-types/datetime64.md
deleted file mode 100644
index 027891c595d0..000000000000
--- a/docs/fr/sql-reference/data-types/datetime64.md
+++ /dev/null
@@ -1,104 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 49
-toc_title: DateTime64
----
-
-# Datetime64 {#data_type-datetime64}
-
-Permet de stocker un instant dans le temps, qui peut être exprimé comme une date de calendrier et une heure d'un jour, avec une précision de sous-seconde définie
-
-Tick taille (précision): 10<sup>-précision</sup> deuxième
-
-Syntaxe:
-
-``` sql
-DateTime64(precision, [timezone])
-```
-
-En interne, stocke les données comme un certain nombre de ‘ticks’ depuis le début de l'époque (1970-01-01 00: 00: 00 UTC) comme Int64. La résolution des tiques est déterminée par le paramètre de précision. En outre, l' `DateTime64` type peut stocker le fuseau horaire qui est le même pour la colonne entière, qui affecte la façon dont les valeurs de la `DateTime64` les valeurs de type sont affichées au format texte et comment les valeurs spécifiées en tant que chaînes sont analysées (‘2020-01-01 05:00:01.000’). Le fuseau horaire n'est pas stocké dans les lignes de la table (ou dans resultset), mais est stocké dans les métadonnées de la colonne. Voir les détails dans [DateTime](datetime.md).
-
-## Exemple {#examples}
-
-**1.** Création d'une table avec `DateTime64`- tapez la colonne et insérez des données dedans:
-
-``` sql
-CREATE TABLE dt
-(
-    `timestamp` DateTime64(3, 'Europe/Moscow'),
-    `event_id` UInt8
-)
-ENGINE = TinyLog
-```
-
-``` sql
-INSERT INTO dt Values (1546300800000, 1), ('2019-01-01 00:00:00', 2)
-```
-
-``` sql
-SELECT * FROM dt
-```
-
-``` text
-┌───────────────timestamp─┬─event_id─┐
-│ 2019-01-01 03:00:00.000 │        1 │
-│ 2019-01-01 00:00:00.000 │        2 │
-└─────────────────────────┴──────────┘
-```
-
--   Lors de l'insertion de datetime en tant qu'entier, il est traité comme un horodatage Unix (UTC) mis à l'échelle de manière appropriée. `1546300800000` (avec précision 3) représente `'2019-01-01 00:00:00'` L'UTC. Cependant, comme `timestamp` la colonne a `Europe/Moscow` (UTC+3) fuseau horaire spécifié, lors de la sortie sous forme de chaîne, la valeur sera affichée comme `'2019-01-01 03:00:00'`
--   Lors de l'insertion d'une valeur de chaîne en tant que datetime, elle est traitée comme étant dans le fuseau horaire de la colonne. `'2019-01-01 00:00:00'` sera considérée comme étant en `Europe/Moscow` fuseau horaire et stocké comme `1546290000000`.
-
-**2.** Le filtrage sur `DateTime64` valeur
-
-``` sql
-SELECT * FROM dt WHERE timestamp = toDateTime64('2019-01-01 00:00:00', 3, 'Europe/Moscow')
-```
-
-``` text
-┌───────────────timestamp─┬─event_id─┐
-│ 2019-01-01 00:00:00.000 │        2 │
-└─────────────────────────┴──────────┘
-```
-
-Contrairement `DateTime`, `DateTime64` les valeurs ne sont pas converties depuis `String` automatiquement
-
-**3.** Obtenir un fuseau horaire pour un `DateTime64`-le type de la valeur:
-
-``` sql
-SELECT toDateTime64(now(), 3, 'Europe/Moscow') AS column, toTypeName(column) AS x
-```
-
-``` text
-┌──────────────────column─┬─x──────────────────────────────┐
-│ 2019-10-16 04:12:04.000 │ DateTime64(3, 'Europe/Moscow') │
-└─────────────────────────┴────────────────────────────────┘
-```
-
-**4.** Conversion de fuseau horaire
-
-``` sql
-SELECT
-toDateTime64(timestamp, 3, 'Europe/London') as lon_time,
-toDateTime64(timestamp, 3, 'Europe/Moscow') as mos_time
-FROM dt
-```
-
-``` text
-┌───────────────lon_time──┬────────────────mos_time─┐
-│ 2019-01-01 00:00:00.000 │ 2019-01-01 03:00:00.000 │
-│ 2018-12-31 21:00:00.000 │ 2019-01-01 00:00:00.000 │
-└─────────────────────────┴─────────────────────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [Fonctions de conversion de Type](../../sql-reference/functions/type-conversion-functions.md)
--   [Fonctions pour travailler avec des dates et des heures](../../sql-reference/functions/date-time-functions.md)
--   [Fonctions pour travailler avec des tableaux](../../sql-reference/functions/array-functions.md)
--   [Le `date_time_input_format` paramètre](../../operations/settings/settings.md#settings-date_time_input_format)
--   [Le `timezone` paramètre de configuration du serveur](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-timezone)
--   [Opérateurs pour travailler avec des dates et des heures](../../sql-reference/operators/index.md#operators-datetime)
--   [`Date` type de données](date.md)
--   [`DateTime` type de données](datetime.md)
diff --git a/docs/fr/sql-reference/data-types/decimal.md b/docs/fr/sql-reference/data-types/decimal.md
deleted file mode 100644
index 171bc1cf6dd3..000000000000
--- a/docs/fr/sql-reference/data-types/decimal.md
+++ /dev/null
@@ -1,109 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: "D\xE9cimal"
----
-
-# Décimal (P, S), Décimal32 (S), Décimal64 (S), Décimal128 (S) {#decimalp-s-decimal32s-decimal64s-decimal128s}
-
-Nombres à points fixes signés qui conservent la précision pendant les opérations d'addition, de soustraction et de multiplication. Pour la division, les chiffres les moins significatifs sont ignorés (non arrondis).
-
-## Paramètre {#parameters}
-
--   P-précision. Plage valide: \[1: 38 \]. Détermine le nombre de chiffres décimaux nombre peut avoir (fraction y compris).
--   S - échelle. Plage valide: \[0: P \]. Détermine le nombre de chiffres décimaux fraction peut avoir.
-
-En fonction de P Paramètre Valeur décimal (P, S) est un synonyme de:
-- P à partir de \[ 1: 9\] - Pour Décimal32 (S)
-- P à partir de \[10: 18\] - pour Décimal64 (S)
-- P à partir de \[19: 38\] - pour Décimal128 (S)
-
-## Plages De Valeurs Décimales {#decimal-value-ranges}
-
--   Décimal32 (S) - ( -1 \* 10^(9 - S), 1 \* 10^(9-S) )
--   Décimal64 (S) - ( -1 \* 10^(18 - S), 1 \* 10^(18-S) )
--   Décimal128 (S) - ( -1 \* 10^(38 - S), 1 \* 10^(38-S) )
-
-Par exemple, Decimal32(4) peut contenir des nombres de -99999.9999 à 99999.9999 avec 0,0001 étape.
-
-## Représentation Interne {#internal-representation}
-
-En interne, les données sont représentées comme des entiers signés normaux avec une largeur de bit respective. Les plages de valeurs réelles qui peuvent être stockées en mémoire sont un peu plus grandes que celles spécifiées ci-dessus, qui sont vérifiées uniquement lors de la conversion à partir d'une chaîne.
-
-Parce que les processeurs modernes ne prennent pas en charge les entiers 128 bits nativement, les opérations sur Decimal128 sont émulées. Pour cette raison, Decimal128 fonctionne significativement plus lentement que Decimal32 / Decimal64.
-
-## Opérations et type de résultat {#operations-and-result-type}
-
-Les opérations binaires sur le résultat décimal dans le type de résultat plus large (avec n'importe quel ordre d'arguments).
-
--   `Decimal64(S1) <op> Decimal32(S2) -> Decimal64(S)`
--   `Decimal128(S1) <op> Decimal32(S2) -> Decimal128(S)`
--   `Decimal128(S1) <op> Decimal64(S2) -> Decimal128(S)`
-
-Règles pour l'échelle:
-
--   ajouter, soustraire: S = max (S1, S2).
--   multuply: S = S1 + S2.
--   diviser: S = S1.
-
-Pour des opérations similaires entre décimal et entier, le résultat est Décimal de la même taille qu'un argument.
-
-Les opérations entre Decimal et Float32 / Float64 ne sont pas définies. Si vous en avez besoin, vous pouvez explicitement lancer l'un des arguments en utilisant les builtins toDecimal32, toDecimal64, toDecimal128 ou toFloat32, toFloat64. Gardez à l'esprit que le résultat perdra de la précision et que la conversion de type est une opération coûteuse en calcul.
-
-Certaines fonctions sur le résultat de retour décimal comme Float64 (par exemple, var ou stddev). Les calculs intermédiaires peuvent toujours être effectués en décimal, ce qui peut conduire à des résultats différents entre les entrées Float64 et Decimal avec les mêmes valeurs.
-
-## Contrôles De Débordement {#overflow-checks}
-
-Pendant les calculs sur Décimal, des débordements entiers peuvent se produire. Les chiffres excessifs dans une fraction sont éliminés (non arrondis). Les chiffres excessifs dans la partie entière conduiront à une exception.
-
-``` sql
-SELECT toDecimal32(2, 4) AS x, x / 3
-```
-
-``` text
-┌──────x─┬─divide(toDecimal32(2, 4), 3)─┐
-│ 2.0000 │                       0.6666 │
-└────────┴──────────────────────────────┘
-```
-
-``` sql
-SELECT toDecimal32(4.2, 8) AS x, x * x
-```
-
-``` text
-DB::Exception: Scale is out of bounds.
-```
-
-``` sql
-SELECT toDecimal32(4.2, 8) AS x, 6 * x
-```
-
-``` text
-DB::Exception: Decimal math overflow.
-```
-
-Les contrôles de débordement entraînent un ralentissement des opérations. S'il est connu que les débordements ne sont pas possibles, il est logique de désactiver les contrôles en utilisant `decimal_check_overflow` paramètre. Lorsque des contrôles sont désactivés et le débordement se produit, le résultat sera faux:
-
-``` sql
-SET decimal_check_overflow = 0;
-SELECT toDecimal32(4.2, 8) AS x, 6 * x
-```
-
-``` text
-┌──────────x─┬─multiply(6, toDecimal32(4.2, 8))─┐
-│ 4.20000000 │                     -17.74967296 │
-└────────────┴──────────────────────────────────┘
-```
-
-Les contrôles de débordement se produisent non seulement sur les opérations arithmétiques mais aussi sur la comparaison de valeurs:
-
-``` sql
-SELECT toDecimal32(1, 8) < 100
-```
-
-``` text
-DB::Exception: Can't compare.
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/decimal/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/domains/index.md b/docs/fr/sql-reference/data-types/domains/index.md
deleted file mode 100644
index 7e11f9a8a68a..000000000000
--- a/docs/fr/sql-reference/data-types/domains/index.md
+++ /dev/null
@@ -1,33 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Domaine
-toc_priority: 56
-toc_title: "Aper\xE7u"
----
-
-# Domaine {#domains}
-
-Les domaines sont des types spéciaux qui ajoutent des fonctionnalités supplémentaires au sommet du type de base existant, mais en laissant le format on-wire et on-disc du type de données sous-jacent intact. À l'heure actuelle, ClickHouse ne prend pas en charge les domaines définis par l'utilisateur.
-
-Vous pouvez utiliser des domaines partout type de base correspondant peut être utilisé, par exemple:
-
--   Créer une colonne d'un type de domaine
--   Valeurs de lecture / écriture depuis / vers la colonne de domaine
--   L'utiliser comme un indice si un type de base peut être utilisée comme un indice
--   Fonctions d'appel avec des valeurs de colonne de domaine
-
-### Fonctionnalités supplémentaires des domaines {#extra-features-of-domains}
-
--   Nom de type de colonne explicite dans `SHOW CREATE TABLE` ou `DESCRIBE TABLE`
--   Entrée du format convivial avec `INSERT INTO domain_table(domain_column) VALUES(...)`
--   Sortie au format convivial pour `SELECT domain_column FROM domain_table`
--   Chargement de données à partir d'une source externe dans un format convivial: `INSERT INTO domain_table FORMAT CSV ...`
-
-### Limitation {#limitations}
-
--   Impossible de convertir la colonne d'index du type de base en type de domaine via `ALTER TABLE`.
--   Impossible de convertir implicitement des valeurs de chaîne en valeurs de domaine lors de l'insertion de données d'une autre colonne ou table.
--   Le domaine n'ajoute aucune contrainte sur les valeurs stockées.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/domains/overview) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/domains/ipv4.md b/docs/fr/sql-reference/data-types/domains/ipv4.md
deleted file mode 100644
index 12895992e77b..000000000000
--- a/docs/fr/sql-reference/data-types/domains/ipv4.md
+++ /dev/null
@@ -1,84 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: IPv4
----
-
-## IPv4 {#ipv4}
-
-`IPv4` est un domaine basé sur `UInt32` tapez et sert de remplacement typé pour stocker des valeurs IPv4. Il fournit un stockage compact avec le format d'entrée-sortie convivial et les informations de type de colonne sur l'inspection.
-
-### Utilisation De Base {#basic-usage}
-
-``` sql
-CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY url;
-
-DESCRIBE TABLE hits;
-```
-
-``` text
-┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐
-│ url  │ String │              │                    │         │                  │
-│ from │ IPv4   │              │                    │         │                  │
-└──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘
-```
-
-Ou vous pouvez utiliser le domaine IPv4 comme clé:
-
-``` sql
-CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY from;
-```
-
-`IPv4` le domaine prend en charge le format d'entrée personnalisé en tant que chaînes IPv4:
-
-``` sql
-INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '116.253.40.133')('https://clickhouse.tech', '183.247.232.58')('https://clickhouse.tech/docs/en/', '116.106.34.242');
-
-SELECT * FROM hits;
-```
-
-``` text
-┌─url────────────────────────────────┬───────────from─┐
-│ https://clickhouse.tech/docs/en/ │ 116.106.34.242 │
-│ https://wikipedia.org              │ 116.253.40.133 │
-│ https://clickhouse.tech          │ 183.247.232.58 │
-└────────────────────────────────────┴────────────────┘
-```
-
-Les valeurs sont stockées sous forme binaire compacte:
-
-``` sql
-SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(from)─┬─hex(from)─┐
-│ IPv4             │ B7F7E83A  │
-└──────────────────┴───────────┘
-```
-
-Les valeurs de domaine ne sont pas implicitement convertibles en types autres que `UInt32`.
-Si vous voulez convertir `IPv4` valeur à une chaîne, vous devez le faire explicitement avec `IPv4NumToString()` fonction:
-
-``` sql
-SELECT toTypeName(s), IPv4NumToString(from) as s FROM hits LIMIT 1;
-```
-
-    ┌─toTypeName(IPv4NumToString(from))─┬─s──────────────┐
-    │ String                            │ 183.247.232.58 │
-    └───────────────────────────────────┴────────────────┘
-
-Ou coulé à un `UInt32` valeur:
-
-``` sql
-SELECT toTypeName(i), CAST(from as UInt32) as i FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(CAST(from, 'UInt32'))─┬──────────i─┐
-│ UInt32                           │ 3086477370 │
-└──────────────────────────────────┴────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/domains/ipv4) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/domains/ipv6.md b/docs/fr/sql-reference/data-types/domains/ipv6.md
deleted file mode 100644
index 77510a950cb4..000000000000
--- a/docs/fr/sql-reference/data-types/domains/ipv6.md
+++ /dev/null
@@ -1,86 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: IPv6
----
-
-## IPv6 {#ipv6}
-
-`IPv6` est un domaine basé sur `FixedString(16)` tapez et sert de remplacement typé pour stocker des valeurs IPv6. Il fournit un stockage compact avec le format d'entrée-sortie convivial et les informations de type de colonne sur l'inspection.
-
-### Utilisation De Base {#basic-usage}
-
-``` sql
-CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY url;
-
-DESCRIBE TABLE hits;
-```
-
-``` text
-┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐
-│ url  │ String │              │                    │         │                  │
-│ from │ IPv6   │              │                    │         │                  │
-└──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘
-```
-
-Ou vous pouvez utiliser `IPv6` domaine comme l'un des principaux:
-
-``` sql
-CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY from;
-```
-
-`IPv6` le domaine prend en charge l'entrée personnalisée en tant que chaînes IPv6:
-
-``` sql
-INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '2a02:aa08:e000:3100::2')('https://clickhouse.tech', '2001:44c8:129:2632:33:0:252:2')('https://clickhouse.tech/docs/en/', '2a02:e980:1e::1');
-
-SELECT * FROM hits;
-```
-
-``` text
-┌─url────────────────────────────────┬─from──────────────────────────┐
-│ https://clickhouse.tech          │ 2001:44c8:129:2632:33:0:252:2 │
-│ https://clickhouse.tech/docs/en/ │ 2a02:e980:1e::1               │
-│ https://wikipedia.org              │ 2a02:aa08:e000:3100::2        │
-└────────────────────────────────────┴───────────────────────────────┘
-```
-
-Les valeurs sont stockées sous forme binaire compacte:
-
-``` sql
-SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(from)─┬─hex(from)────────────────────────┐
-│ IPv6             │ 200144C8012926320033000002520002 │
-└──────────────────┴──────────────────────────────────┘
-```
-
-Les valeurs de domaine ne sont pas implicitement convertibles en types autres que `FixedString(16)`.
-Si vous voulez convertir `IPv6` valeur à une chaîne, vous devez le faire explicitement avec `IPv6NumToString()` fonction:
-
-``` sql
-SELECT toTypeName(s), IPv6NumToString(from) as s FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(IPv6NumToString(from))─┬─s─────────────────────────────┐
-│ String                            │ 2001:44c8:129:2632:33:0:252:2 │
-└───────────────────────────────────┴───────────────────────────────┘
-```
-
-Ou coulé à un `FixedString(16)` valeur:
-
-``` sql
-SELECT toTypeName(i), CAST(from as FixedString(16)) as i FROM hits LIMIT 1;
-```
-
-``` text
-┌─toTypeName(CAST(from, 'FixedString(16)'))─┬─i───────┐
-│ FixedString(16)                           │  ��� │
-└───────────────────────────────────────────┴─────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/domains/ipv6) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/enum.md b/docs/fr/sql-reference/data-types/enum.md
deleted file mode 100644
index b9751c1c8042..000000000000
--- a/docs/fr/sql-reference/data-types/enum.md
+++ /dev/null
@@ -1,132 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 50
-toc_title: Enum
----
-
-# Enum {#enum}
-
-Type énuméré composé de valeurs nommées.
-
-Les valeurs nommées doivent être déclarées comme `'string' = integer` pair. ClickHouse ne stocke que des nombres, mais prend en charge les opérations avec les valeurs à travers leurs noms.
-
-Supports ClickHouse:
-
--   8-bit `Enum`. Il peut contenir jusqu'à 256 valeurs énumérées dans le `[-128, 127]` gamme.
--   16 bits `Enum`. Il peut contenir jusqu'à 65 536 valeurs énumérées dans le `[-32768, 32767]` gamme.
-
-Clickhouse choisit automatiquement le type de `Enum` lorsque les données sont insérées. Vous pouvez également utiliser `Enum8` ou `Enum16` types pour être sûr de la taille de stockage.
-
-## Exemples D'Utilisation {#usage-examples}
-
-Ici, nous créons une table avec une `Enum8('hello' = 1, 'world' = 2)` type de colonne:
-
-``` sql
-CREATE TABLE t_enum
-(
-    x Enum('hello' = 1, 'world' = 2)
-)
-ENGINE = TinyLog
-```
-
-Colonne `x` ne peut stocker que les valeurs répertoriées dans la définition de type: `'hello'` ou `'world'`. Si vous essayez d'enregistrer une autre valeur, ClickHouse déclenchera une exception. Taille 8 bits pour cela `Enum` est choisi automatiquement.
-
-``` sql
-INSERT INTO t_enum VALUES ('hello'), ('world'), ('hello')
-```
-
-``` text
-Ok.
-```
-
-``` sql
-INSERT INTO t_enum values('a')
-```
-
-``` text
-Exception on client:
-Code: 49. DB::Exception: Unknown element 'a' for type Enum('hello' = 1, 'world' = 2)
-```
-
-Lorsque vous interrogez des données de la table, ClickHouse affiche les valeurs de chaîne de `Enum`.
-
-``` sql
-SELECT * FROM t_enum
-```
-
-``` text
-┌─x─────┐
-│ hello │
-│ world │
-│ hello │
-└───────┘
-```
-
-Si vous avez besoin de voir les équivalents numériques des lignes, vous devez `Enum` valeur en type entier.
-
-``` sql
-SELECT CAST(x, 'Int8') FROM t_enum
-```
-
-``` text
-┌─CAST(x, 'Int8')─┐
-│               1 │
-│               2 │
-│               1 │
-└─────────────────┘
-```
-
-Pour créer une valeur d'Enum dans une requête, vous devez également utiliser `CAST`.
-
-``` sql
-SELECT toTypeName(CAST('a', 'Enum(\'a\' = 1, \'b\' = 2)'))
-```
-
-``` text
-┌─toTypeName(CAST('a', 'Enum(\'a\' = 1, \'b\' = 2)'))─┐
-│ Enum8('a' = 1, 'b' = 2)                             │
-└─────────────────────────────────────────────────────┘
-```
-
-## Règles générales et utilisation {#general-rules-and-usage}
-
-Chacune des valeurs se voit attribuer un nombre dans la plage `-128 ... 127` pour `Enum8` ou dans la gamme `-32768 ... 32767` pour `Enum16`. Toutes les chaînes et les nombres doivent être différents. Une chaîne vide est autorisé. Si ce type est spécifié (dans une définition de table), les nombres peuvent être dans un ordre arbitraire. Toutefois, l'ordre n'a pas d'importance.
-
-Ni la chaîne ni la valeur numérique dans un `Enum` peut être [NULL](../../sql-reference/syntax.md).
-
-Un `Enum` peut être contenue dans [Nullable](nullable.md) type. Donc, si vous créez une table en utilisant la requête
-
-``` sql
-CREATE TABLE t_enum_nullable
-(
-    x Nullable( Enum8('hello' = 1, 'world' = 2) )
-)
-ENGINE = TinyLog
-```
-
-il peut stocker non seulement des `'hello'` et `'world'`, mais `NULL`, ainsi.
-
-``` sql
-INSERT INTO t_enum_nullable Values('hello'),('world'),(NULL)
-```
-
-Dans la mémoire RAM, un `Enum` la colonne est stockée dans la même manière que `Int8` ou `Int16` des valeurs numériques correspondantes.
-
-Lors de la lecture sous forme de texte, ClickHouse analyse la valeur sous forme de chaîne et recherche la chaîne correspondante à partir de l'ensemble des valeurs Enum. Si elle n'est pas trouvée, une exception est levée. Lors de la lecture au format texte, la chaîne est lue et la valeur numérique correspondante est recherchée. Une exception sera levée si il n'est pas trouvé.
-Lors de l'écriture sous forme de texte, il écrit la valeur correspondante de la chaîne. Si les données de colonne contiennent des déchets (nombres qui ne proviennent pas de l'ensemble valide), une exception est levée. Lors de la lecture et de l'écriture sous forme binaire, cela fonctionne de la même manière que pour les types de données Int8 et Int16.
-La valeur implicite par défaut est la valeur avec le numéro le plus bas.
-
-Lors `ORDER BY`, `GROUP BY`, `IN`, `DISTINCT` et ainsi de suite, les Énumérations se comportent de la même façon que les nombres correspondants. Par exemple, ORDER BY les trie numériquement. Les opérateurs d'égalité et de comparaison fonctionnent de la même manière sur les énumérations que sur les valeurs numériques sous-jacentes.
-
-Les valeurs Enum ne peuvent pas être comparées aux nombres. Les Enums peuvent être comparés à une chaîne constante. Si la chaîne comparée à n'est pas une valeur valide pour L'énumération, une exception sera levée. L'opérateur est pris en charge avec l'Enum sur le côté gauche, et un ensemble de chaînes sur le côté droit. Les chaînes sont les valeurs de L'énumération correspondante.
-
-Most numeric and string operations are not defined for Enum values, e.g. adding a number to an Enum or concatenating a string to an Enum.
-Cependant, L'énumération a un naturel `toString` fonction qui renvoie sa valeur de chaîne.
-
-Les valeurs Enum sont également convertibles en types numériques en utilisant `toT` fonction, où T est un type numérique. Lorsque T correspond au type numérique sous-jacent de l'énumération, cette conversion est à coût nul.
-Le type Enum peut être modifié sans coût en utilisant ALTER, si seulement l'ensemble des valeurs est modifié. Il est possible d'ajouter et de supprimer des membres de L'énumération en utilisant ALTER (la suppression n'est sûre que si la valeur supprimée n'a jamais été utilisée dans la table). À titre de sauvegarde, la modification de la valeur numérique d'un membre Enum précédemment défini lancera une exception.
-
-En utilisant ALTER, il est possible de changer un Enum8 en Enum16 ou vice versa, tout comme changer un Int8 en Int16.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/enum/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/fixedstring.md b/docs/fr/sql-reference/data-types/fixedstring.md
deleted file mode 100644
index 5ba091875816..000000000000
--- a/docs/fr/sql-reference/data-types/fixedstring.md
+++ /dev/null
@@ -1,63 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: FixedString (N)
----
-
-# Fixedstring {#fixedstring}
-
-Une chaîne de longueur fixe de `N` octets (ni caractères ni points de code).
-
-Pour déclarer une colonne de `FixedString` tapez, utilisez la syntaxe suivante:
-
-``` sql
-<column_name> FixedString(N)
-```
-
-Où `N` est un nombre naturel.
-
-Le `FixedString` type est efficace lorsque les données ont la longueur de précisément `N` octet. Dans tous les autres cas, il est susceptible de réduire l'efficacité.
-
-Exemples de valeurs qui peuvent être stockées efficacement dans `FixedString`-tapé colonnes:
-
--   La représentation binaire des adresses IP (`FixedString(16)` pour IPv6).
--   Language codes (ru_RU, en_US … ).
--   Currency codes (USD, RUB … ).
--   Représentation binaire des hachages (`FixedString(16)` pour MD5, `FixedString(32)` pour SHA256).
-
-Pour stocker les valeurs UUID, utilisez [UUID](uuid.md) type de données.
-
-Lors de l'insertion des données, ClickHouse:
-
--   Complète une chaîne avec des octets null si la chaîne contient moins de `N` octet.
--   Jette le `Too large value for FixedString(N)` exception si la chaîne contient plus de `N` octet.
-
-Lors de la sélection des données, ClickHouse ne supprime pas les octets nuls à la fin de la chaîne. Si vous utilisez le `WHERE` clause, vous devez ajouter des octets null manuellement pour `FixedString` valeur. L'exemple suivant illustre l'utilisation de l' `WHERE` la clause de `FixedString`.
-
-Considérons le tableau suivant avec le seul `FixedString(2)` colonne:
-
-``` text
-┌─name──┐
-│ b     │
-└───────┘
-```
-
-Requête `SELECT * FROM FixedStringTable WHERE a = 'b'` ne renvoie aucune donnée en conséquence. Nous devrions compléter le modèle de filtre avec des octets nuls.
-
-``` sql
-SELECT * FROM FixedStringTable
-WHERE a = 'b\0'
-```
-
-``` text
-┌─a─┐
-│ b │
-└───┘
-```
-
-Ce comportement diffère de MySQL pour le `CHAR` type (où les chaînes sont remplies d'espaces et les espaces sont supprimés pour la sortie).
-
-À noter que la longueur de la `FixedString(N)` la valeur est constante. Le [longueur](../../sql-reference/functions/array-functions.md#array_functions-length) la fonction renvoie `N` même si l' `FixedString(N)` la valeur est remplie uniquement avec des octets [vide](../../sql-reference/functions/string-functions.md#empty) la fonction renvoie `1` dans ce cas.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/fixedstring/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/float.md b/docs/fr/sql-reference/data-types/float.md
deleted file mode 100644
index b269b930110a..000000000000
--- a/docs/fr/sql-reference/data-types/float.md
+++ /dev/null
@@ -1,87 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: Float32, Float64
----
-
-# Float32, Float64 {#float32-float64}
-
-[Les nombres à virgule flottante](https://en.wikipedia.org/wiki/IEEE_754).
-
-Les Types sont équivalents aux types de C:
-
--   `Float32` - `float`
--   `Float64` - `double`
-
-Nous vous recommandons de stocker les données sous forme entière chaque fois que possible. Par exemple, convertissez des nombres de précision fixes en valeurs entières, telles que des montants monétaires ou des temps de chargement de page en millisecondes.
-
-## Utilisation de nombres à virgule flottante {#using-floating-point-numbers}
-
--   Calculs avec des nombres à virgule flottante peut produire une erreur d'arrondi.
-
-<!-- -->
-
-``` sql
-SELECT 1 - 0.9
-```
-
-``` text
-┌───────minus(1, 0.9)─┐
-│ 0.09999999999999998 │
-└─────────────────────┘
-```
-
--   Le résultat du calcul dépend de la méthode de calcul (le type de processeur et de l'architecture du système informatique).
--   Les calculs à virgule flottante peuvent entraîner des nombres tels que l'infini (`Inf`) et “not-a-number” (`NaN`). Cela doit être pris en compte lors du traitement des résultats de calculs.
--   Lors de l'analyse de nombres à virgule flottante à partir de texte, le résultat peut ne pas être le nombre représentable par machine le plus proche.
-
-## NaN et Inf {#data_type-float-nan-inf}
-
-Contrairement à SQL standard, ClickHouse prend en charge les catégories suivantes de nombres à virgule flottante:
-
--   `Inf` – Infinity.
-
-<!-- -->
-
-``` sql
-SELECT 0.5 / 0
-```
-
-``` text
-┌─divide(0.5, 0)─┐
-│            inf │
-└────────────────┘
-```
-
--   `-Inf` – Negative infinity.
-
-<!-- -->
-
-``` sql
-SELECT -0.5 / 0
-```
-
-``` text
-┌─divide(-0.5, 0)─┐
-│            -inf │
-└─────────────────┘
-```
-
--   `NaN` – Not a number.
-
-<!-- -->
-
-``` sql
-SELECT 0 / 0
-```
-
-``` text
-┌─divide(0, 0)─┐
-│          nan │
-└──────────────┘
-```
-
-    See the rules for `NaN` sorting in the section [ORDER BY clause](../sql_reference/statements/select/order-by.md).
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/float/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/index.md b/docs/fr/sql-reference/data-types/index.md
deleted file mode 100644
index 887e2efd69f1..000000000000
--- a/docs/fr/sql-reference/data-types/index.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Types De Donn\xE9es"
-toc_priority: 37
-toc_title: Introduction
----
-
-# Types De Données {#data_types}
-
-ClickHouse peut stocker différents types de données dans des cellules de table.
-
-Cette section décrit les types de données pris en charge et les considérations spéciales pour les utiliser et/ou les implémenter le cas échéant.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/int-uint.md b/docs/fr/sql-reference/data-types/int-uint.md
deleted file mode 100644
index 9b196c164a4b..000000000000
--- a/docs/fr/sql-reference/data-types/int-uint.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64
----
-
-# UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64 {#uint8-uint16-uint32-uint64-int8-int16-int32-int64}
-
-Entiers de longueur fixe, avec ou sans signe.
-
-## Plages Int {#int-ranges}
-
--   Int8 - \[-128: 127\]
--   Int16 - \[-32768: 32767\]
--   Int32 - \[-2147483648: 2147483647\]
--   Int64 - \[-9223372036854775808: 9223372036854775807\]
-
-## Plages Uint {#uint-ranges}
-
--   UInt8 - \[0: 255\]
--   UInt16 - \[0: 65535\]
--   UInt32- \[0: 4294967295\]
--   UInt64- \[0: 18446744073709551615\]
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/int_uint/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/nested-data-structures/index.md b/docs/fr/sql-reference/data-types/nested-data-structures/index.md
deleted file mode 100644
index 528e0bad0cde..000000000000
--- a/docs/fr/sql-reference/data-types/nested-data-structures/index.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Structures De Donn\xE9es Imbriqu\xE9es"
-toc_hidden: true
-toc_priority: 54
-toc_title: "cach\xE9s"
----
-
-# Structures De Données Imbriquées {#nested-data-structures}
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/nested_data_structures/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/nested-data-structures/nested.md b/docs/fr/sql-reference/data-types/nested-data-structures/nested.md
deleted file mode 100644
index 2805780de241..000000000000
--- a/docs/fr/sql-reference/data-types/nested-data-structures/nested.md
+++ /dev/null
@@ -1,106 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 57
-toc_title: "Imbriqu\xE9e(Type1 Nom1, Nom2 Type2, ...)"
----
-
-# Nested(name1 Type1, Name2 Type2, …) {#nestedname1-type1-name2-type2}
-
-A nested data structure is like a table inside a cell. The parameters of a nested data structure – the column names and types – are specified the same way as in a [CREATE TABLE](../../../sql-reference/statements/create.md) requête. Chaque ligne de table peut correspondre à n'importe quel nombre de lignes dans une structure de données imbriquée.
-
-Exemple:
-
-``` sql
-CREATE TABLE test.visits
-(
-    CounterID UInt32,
-    StartDate Date,
-    Sign Int8,
-    IsNew UInt8,
-    VisitID UInt64,
-    UserID UInt64,
-    ...
-    Goals Nested
-    (
-        ID UInt32,
-        Serial UInt32,
-        EventTime DateTime,
-        Price Int64,
-        OrderID String,
-        CurrencyID UInt32
-    ),
-    ...
-) ENGINE = CollapsingMergeTree(StartDate, intHash32(UserID), (CounterID, StartDate, intHash32(UserID), VisitID), 8192, Sign)
-```
-
-Cet exemple déclare le `Goals` structure de données imbriquée, qui contient des données sur les conversions (objectifs atteints). Chaque ligne de la ‘visits’ table peut correspondre à zéro ou n'importe quel nombre de conversions.
-
-Un seul niveau d'imbrication est pris en charge. Les colonnes de structures imbriquées contenant des tableaux sont équivalentes à des tableaux multidimensionnels, elles ont donc un support limité (il n'y a pas de support pour stocker ces colonnes dans des tables avec le moteur MergeTree).
-
-Dans la plupart des cas, lorsque vous travaillez avec une structure de données imbriquée, ses colonnes sont spécifiées avec des noms de colonnes séparés par un point. Ces colonnes constituent un tableau de types correspondants. Tous les tableaux de colonnes d'une structure de données imbriquée unique ont la même longueur.
-
-Exemple:
-
-``` sql
-SELECT
-    Goals.ID,
-    Goals.EventTime
-FROM test.visits
-WHERE CounterID = 101500 AND length(Goals.ID) < 5
-LIMIT 10
-```
-
-``` text
-┌─Goals.ID───────────────────────┬─Goals.EventTime───────────────────────────────────────────────────────────────────────────┐
-│ [1073752,591325,591325]        │ ['2014-03-17 16:38:10','2014-03-17 16:38:48','2014-03-17 16:42:27']                       │
-│ [1073752]                      │ ['2014-03-17 00:28:25']                                                                   │
-│ [1073752]                      │ ['2014-03-17 10:46:20']                                                                   │
-│ [1073752,591325,591325,591325] │ ['2014-03-17 13:59:20','2014-03-17 22:17:55','2014-03-17 22:18:07','2014-03-17 22:18:51'] │
-│ []                             │ []                                                                                        │
-│ [1073752,591325,591325]        │ ['2014-03-17 11:37:06','2014-03-17 14:07:47','2014-03-17 14:36:21']                       │
-│ []                             │ []                                                                                        │
-│ []                             │ []                                                                                        │
-│ [591325,1073752]               │ ['2014-03-17 00:46:05','2014-03-17 00:46:05']                                             │
-│ [1073752,591325,591325,591325] │ ['2014-03-17 13:28:33','2014-03-17 13:30:26','2014-03-17 18:51:21','2014-03-17 18:51:45'] │
-└────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-Il est plus facile de penser à une structure de données imbriquée comme un ensemble de plusieurs tableaux de colonnes de la même longueur.
-
-Le seul endroit où une requête SELECT peut spécifier le nom d'une structure de données imbriquée entière au lieu de colonnes individuelles est la clause de jointure de tableau. Pour plus d'informations, voir “ARRAY JOIN clause”. Exemple:
-
-``` sql
-SELECT
-    Goal.ID,
-    Goal.EventTime
-FROM test.visits
-ARRAY JOIN Goals AS Goal
-WHERE CounterID = 101500 AND length(Goals.ID) < 5
-LIMIT 10
-```
-
-``` text
-┌─Goal.ID─┬──────Goal.EventTime─┐
-│ 1073752 │ 2014-03-17 16:38:10 │
-│  591325 │ 2014-03-17 16:38:48 │
-│  591325 │ 2014-03-17 16:42:27 │
-│ 1073752 │ 2014-03-17 00:28:25 │
-│ 1073752 │ 2014-03-17 10:46:20 │
-│ 1073752 │ 2014-03-17 13:59:20 │
-│  591325 │ 2014-03-17 22:17:55 │
-│  591325 │ 2014-03-17 22:18:07 │
-│  591325 │ 2014-03-17 22:18:51 │
-│ 1073752 │ 2014-03-17 11:37:06 │
-└─────────┴─────────────────────┘
-```
-
-Vous ne pouvez pas effectuer SELECT pour une structure de données imbriquée entière. Vous ne pouvez lister explicitement que les colonnes individuelles qui en font partie.
-
-Pour une requête INSERT, vous devez passer tous les tableaux de colonnes composant d'une structure de données imbriquée séparément (comme s'il s'agissait de tableaux de colonnes individuels). Au cours de l'insertion, le système vérifie qu'ils ont la même longueur.
-
-Pour une requête DESCRIBE, les colonnes d'une structure de données imbriquée sont répertoriées séparément de la même manière.
-
-La requête ALTER pour les éléments d'une structure de données imbriquée a des limites.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/nested_data_structures/nested/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/nullable.md b/docs/fr/sql-reference/data-types/nullable.md
deleted file mode 100644
index 6b37b571a966..000000000000
--- a/docs/fr/sql-reference/data-types/nullable.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 54
-toc_title: Nullable
----
-
-# Nullable(typename) {#data_type-nullable}
-
-Permet de stocker marqueur spécial ([NULL](../../sql-reference/syntax.md)) qui dénote “missing value” aux valeurs normales autorisées par `TypeName`. Par exemple, un `Nullable(Int8)` type colonne peut stocker `Int8` type de valeurs, et les lignes qui n'ont pas de valeur magasin `NULL`.
-
-Pour un `TypeName` vous ne pouvez pas utiliser les types de données composites [Tableau](array.md) et [Tuple](tuple.md). Les types de données composites peuvent contenir `Nullable` valeurs de type, telles que `Array(Nullable(Int8))`.
-
-A `Nullable` le champ type ne peut pas être inclus dans les index de table.
-
-`NULL` est la valeur par défaut pour tout `Nullable` type, sauf indication contraire dans la configuration du serveur ClickHouse.
-
-## Caractéristiques De Stockage {#storage-features}
-
-Stocker `Nullable` valeurs de type dans une colonne de table, ClickHouse utilise un fichier séparé avec `NULL` masques en plus du fichier normal avec des valeurs. Les entrées du fichier masks permettent à ClickHouse de faire la distinction entre `NULL` et une valeur par défaut du type de données correspondant pour chaque ligne de table. En raison d'un fichier supplémentaire, `Nullable` colonne consomme de l'espace de stockage supplémentaire par rapport à une normale similaire.
-
-!!! info "Note"
-    Utiliser `Nullable` affecte presque toujours négativement les performances, gardez cela à l'esprit lors de la conception de vos bases de données.
-
-## Exemple D'Utilisation {#usage-example}
-
-``` sql
-CREATE TABLE t_null(x Int8, y Nullable(Int8)) ENGINE TinyLog
-```
-
-``` sql
-INSERT INTO t_null VALUES (1, NULL), (2, 3)
-```
-
-``` sql
-SELECT x + y FROM t_null
-```
-
-``` text
-┌─plus(x, y)─┐
-│       ᴺᵁᴸᴸ │
-│          5 │
-└────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/nullable/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/simpleaggregatefunction.md b/docs/fr/sql-reference/data-types/simpleaggregatefunction.md
deleted file mode 100644
index 81fcd67cfae9..000000000000
--- a/docs/fr/sql-reference/data-types/simpleaggregatefunction.md
+++ /dev/null
@@ -1,38 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# SimpleAggregateFunction {#data-type-simpleaggregatefunction}
-
-`SimpleAggregateFunction(name, types_of_arguments…)` le type de données stocke la valeur actuelle de la fonction d'agrégat et ne stocke pas son état complet comme [`AggregateFunction`](aggregatefunction.md) faire. Cette optimisation peut être appliquée aux fonctions pour lesquelles la propriété suivante est conservée: le résultat de l'application d'une fonction `f` pour un ensemble de lignes `S1 UNION ALL S2` peut être obtenu en appliquant `f` pour les parties de la ligne définie séparément, puis à nouveau l'application `f` pour les résultats: `f(S1 UNION ALL S2) = f(f(S1) UNION ALL f(S2))`. Cette propriété garantit que les résultats d'agrégation partielle sont suffisants pour calculer le combiné, de sorte que nous n'avons pas à stocker et traiter de données supplémentaires.
-
-Les fonctions d'agrégation suivantes sont prises en charge:
-
--   [`any`](../../sql-reference/aggregate-functions/reference.md#agg_function-any)
--   [`anyLast`](../../sql-reference/aggregate-functions/reference.md#anylastx)
--   [`min`](../../sql-reference/aggregate-functions/reference.md#agg_function-min)
--   [`max`](../../sql-reference/aggregate-functions/reference.md#agg_function-max)
--   [`sum`](../../sql-reference/aggregate-functions/reference.md#agg_function-sum)
--   [`groupBitAnd`](../../sql-reference/aggregate-functions/reference.md#groupbitand)
--   [`groupBitOr`](../../sql-reference/aggregate-functions/reference.md#groupbitor)
--   [`groupBitXor`](../../sql-reference/aggregate-functions/reference.md#groupbitxor)
-
-Les valeurs de la `SimpleAggregateFunction(func, Type)` regarder et stockées de la même manière que `Type`, de sorte que vous n'avez pas besoin d'appliquer des fonctions avec `-Merge`/`-State` suffixe. `SimpleAggregateFunction` a de meilleures performances que `AggregateFunction` avec la même fonction d'agrégation.
-
-**Paramètre**
-
--   Nom de la fonction d'agrégation.
--   Types des arguments de la fonction d'agrégation.
-
-**Exemple**
-
-``` sql
-CREATE TABLE t
-(
-    column1 SimpleAggregateFunction(sum, UInt64),
-    column2 SimpleAggregateFunction(any, String)
-) ENGINE = ...
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/simpleaggregatefunction/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/special-data-types/expression.md b/docs/fr/sql-reference/data-types/special-data-types/expression.md
deleted file mode 100644
index c3ba5e42ba16..000000000000
--- a/docs/fr/sql-reference/data-types/special-data-types/expression.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: Expression
----
-
-# Expression {#expression}
-
-Les Expressions sont utilisées pour représenter des lambdas dans des fonctions d'ordre Élevé.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/special_data_types/expression/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/special-data-types/index.md b/docs/fr/sql-reference/data-types/special-data-types/index.md
deleted file mode 100644
index 6d292dc522e8..000000000000
--- a/docs/fr/sql-reference/data-types/special-data-types/index.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "Types De Donn\xE9es Sp\xE9ciaux"
-toc_hidden: true
-toc_priority: 55
-toc_title: "cach\xE9s"
----
-
-# Types De Données Spéciaux {#special-data-types}
-
-Les valeurs de type de données spéciales ne peuvent pas être sérialisées pour l'enregistrement dans une table ou la sortie dans les résultats de la requête, mais peuvent être utilisées comme résultat intermédiaire lors de l'exécution de la requête.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/special_data_types/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/special-data-types/interval.md b/docs/fr/sql-reference/data-types/special-data-types/interval.md
deleted file mode 100644
index 464de8a10ab4..000000000000
--- a/docs/fr/sql-reference/data-types/special-data-types/interval.md
+++ /dev/null
@@ -1,85 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: Intervalle
----
-
-# Intervalle {#data-type-interval}
-
-Famille de types de données représentant des intervalles d'heure et de date. Les types de la [INTERVAL](../../../sql-reference/operators/index.md#operator-interval) opérateur.
-
-!!! warning "Avertissement"
-    `Interval` les valeurs de type de données ne peuvent pas être stockées dans les tables.
-
-Structure:
-
--   Intervalle de temps en tant que valeur entière non signée.
--   Type de l'intervalle.
-
-Types d'intervalles pris en charge:
-
--   `SECOND`
--   `MINUTE`
--   `HOUR`
--   `DAY`
--   `WEEK`
--   `MONTH`
--   `QUARTER`
--   `YEAR`
-
-Pour chaque type d'intervalle, il existe un type de données distinct. Par exemple, l' `DAY` l'intervalle correspond au `IntervalDay` type de données:
-
-``` sql
-SELECT toTypeName(INTERVAL 4 DAY)
-```
-
-``` text
-┌─toTypeName(toIntervalDay(4))─┐
-│ IntervalDay                  │
-└──────────────────────────────┘
-```
-
-## Utilisation Remarques {#data-type-interval-usage-remarks}
-
-Vous pouvez utiliser `Interval`-tapez des valeurs dans des opérations arithmétiques avec [Date](../../../sql-reference/data-types/date.md) et [DateTime](../../../sql-reference/data-types/datetime.md)-type de valeurs. Par exemple, vous pouvez ajouter 4 jours à l'heure actuelle:
-
-``` sql
-SELECT now() as current_date_time, current_date_time + INTERVAL 4 DAY
-```
-
-``` text
-┌───current_date_time─┬─plus(now(), toIntervalDay(4))─┐
-│ 2019-10-23 10:58:45 │           2019-10-27 10:58:45 │
-└─────────────────────┴───────────────────────────────┘
-```
-
-Les intervalles avec différents types ne peuvent pas être combinés. Vous ne pouvez pas utiliser des intervalles comme `4 DAY 1 HOUR`. Spécifiez des intervalles en unités inférieures ou égales à la plus petite unité de l'intervalle, par exemple, l'intervalle `1 day and an hour` l'intervalle peut être exprimée comme `25 HOUR` ou `90000 SECOND`.
-
-Vous ne pouvez pas effectuer d'opérations arithmétiques avec `Interval`- tapez des valeurs, mais vous pouvez ajouter des intervalles de différents types par conséquent aux valeurs dans `Date` ou `DateTime` types de données. Exemple:
-
-``` sql
-SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR
-```
-
-``` text
-┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐
-│ 2019-10-23 11:16:28 │                                    2019-10-27 14:16:28 │
-└─────────────────────┴────────────────────────────────────────────────────────┘
-```
-
-La requête suivante provoque une exception:
-
-``` sql
-select now() AS current_date_time, current_date_time + (INTERVAL 4 DAY + INTERVAL 3 HOUR)
-```
-
-``` text
-Received exception from server (version 19.14.1):
-Code: 43. DB::Exception: Received from localhost:9000. DB::Exception: Wrong argument types for function plus: if one argument is Interval, then another must be Date or DateTime..
-```
-
-## Voir Aussi {#see-also}
-
--   [INTERVAL](../../../sql-reference/operators/index.md#operator-interval) opérateur
--   [toInterval](../../../sql-reference/functions/type-conversion-functions.md#function-tointerval) type fonctions de conversion
diff --git a/docs/fr/sql-reference/data-types/special-data-types/nothing.md b/docs/fr/sql-reference/data-types/special-data-types/nothing.md
deleted file mode 100644
index 2e3d76b7207f..000000000000
--- a/docs/fr/sql-reference/data-types/special-data-types/nothing.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: Rien
----
-
-# Rien {#nothing}
-
-Le seul but de ce type de données est de représenter les cas où une valeur n'est pas prévu. Donc vous ne pouvez pas créer un `Nothing` type de valeur.
-
-Par exemple, littéral [NULL](../../../sql-reference/syntax.md#null-literal) a type de `Nullable(Nothing)`. Voir plus sur [Nullable](../../../sql-reference/data-types/nullable.md).
-
-Le `Nothing` type peut également être utilisé pour désigner des tableaux vides:
-
-``` sql
-SELECT toTypeName(array())
-```
-
-``` text
-┌─toTypeName(array())─┐
-│ Array(Nothing)      │
-└─────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/special_data_types/nothing/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/special-data-types/set.md b/docs/fr/sql-reference/data-types/special-data-types/set.md
deleted file mode 100644
index 8f50175bb6b0..000000000000
--- a/docs/fr/sql-reference/data-types/special-data-types/set.md
+++ /dev/null
@@ -1,12 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: "D\xE9finir"
----
-
-# Définir {#set}
-
-Utilisé pour la moitié droite d'un [IN](../../operators/in.md#select-in-operators) expression.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/special_data_types/set/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/string.md b/docs/fr/sql-reference/data-types/string.md
deleted file mode 100644
index b82e1fe6c690..000000000000
--- a/docs/fr/sql-reference/data-types/string.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: "Cha\xEEne"
----
-
-# Chaîne {#string}
-
-Les chaînes d'une longueur arbitraire. La longueur n'est pas limitée. La valeur peut contenir un ensemble arbitraire d'octets, y compris des octets nuls.
-Le type de chaîne remplace les types VARCHAR, BLOB, CLOB et autres provenant d'autres SGBD.
-
-## Encodage {#encodings}
-
-ClickHouse n'a pas le concept d'encodages. Les chaînes peuvent contenir un ensemble arbitraire d'octets, qui sont stockés et sortis tels quels.
-Si vous avez besoin de stocker des textes, nous vous recommandons d'utiliser L'encodage UTF-8. À tout le moins, si votre terminal utilise UTF-8 (comme recommandé), vous pouvez lire et écrire vos valeurs sans effectuer de conversions.
-De même, certaines fonctions pour travailler avec des chaînes ont des variations distinctes qui fonctionnent sous l'hypothèse que la chaîne contient un ensemble d'octets représentant un texte codé en UTF-8.
-Par exemple, l' ‘length’ fonction calcule la longueur de la chaîne en octets, tandis que le ‘lengthUTF8’ la fonction calcule la longueur de la chaîne en points de code Unicode, en supposant que la valeur est encodée en UTF-8.
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/string/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/tuple.md b/docs/fr/sql-reference/data-types/tuple.md
deleted file mode 100644
index ab9db7351817..000000000000
--- a/docs/fr/sql-reference/data-types/tuple.md
+++ /dev/null
@@ -1,52 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 53
-toc_title: Tuple (T1, T2,...)
----
-
-# Tuple(t1, T2, …) {#tuplet1-t2}
-
-Un n-uplet d'éléments, chacun ayant une personne [type](index.md#data_types).
-
-Les Tuples sont utilisés pour le regroupement temporaire de colonnes. Les colonnes peuvent être regroupées lorsqu'une expression IN est utilisée dans une requête et pour spécifier certains paramètres formels des fonctions lambda. Pour plus d'informations, voir les sections [Dans les opérateurs](../../sql-reference/operators/in.md) et [Des fonctions d'ordre supérieur](../../sql-reference/functions/higher-order-functions.md).
-
-Les Tuples peuvent être le résultat d'une requête. Dans ce cas, pour les formats de texte autres que JSON, les valeurs sont séparées par des virgules entre parenthèses. Dans les formats JSON, les tuples sont sortis sous forme de tableaux (entre crochets).
-
-## La création d'un Tuple {#creating-a-tuple}
-
-Vous pouvez utiliser une fonction pour créer un tuple:
-
-``` sql
-tuple(T1, T2, ...)
-```
-
-Exemple de création d'un tuple:
-
-``` sql
-SELECT tuple(1,'a') AS x, toTypeName(x)
-```
-
-``` text
-┌─x───────┬─toTypeName(tuple(1, 'a'))─┐
-│ (1,'a') │ Tuple(UInt8, String)      │
-└─────────┴───────────────────────────┘
-```
-
-## Utilisation de Types de données {#working-with-data-types}
-
-Lors de la création d'un tuple à la volée, ClickHouse détecte automatiquement le type de chaque argument comme le minimum des types qui peuvent stocker la valeur de l'argument. Si l'argument est [NULL](../../sql-reference/syntax.md#null-literal) le type de l'élément tuple est [Nullable](nullable.md).
-
-Exemple de détection automatique de type de données:
-
-``` sql
-SELECT tuple(1, NULL) AS x, toTypeName(x)
-```
-
-``` text
-┌─x────────┬─toTypeName(tuple(1, NULL))──────┐
-│ (1,NULL) │ Tuple(UInt8, Nullable(Nothing)) │
-└──────────┴─────────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/tuple/) <!--hide-->
diff --git a/docs/fr/sql-reference/data-types/uuid.md b/docs/fr/sql-reference/data-types/uuid.md
deleted file mode 100644
index 60973a3f855f..000000000000
--- a/docs/fr/sql-reference/data-types/uuid.md
+++ /dev/null
@@ -1,77 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: UUID
----
-
-# UUID {#uuid-data-type}
-
-Un identifiant unique universel (UUID) est un numéro de 16 octets utilisé pour identifier les enregistrements. Pour plus d'informations sur L'UUID, voir [Wikipedia](https://en.wikipedia.org/wiki/Universally_unique_identifier).
-
-L'exemple de valeur de type UUID est représenté ci-dessous:
-
-``` text
-61f0c404-5cb3-11e7-907b-a6006ad3dba0
-```
-
-Si vous ne spécifiez pas la valeur de la colonne UUID lors de l'insertion d'un nouvel enregistrement, la valeur UUID est remplie avec zéro:
-
-``` text
-00000000-0000-0000-0000-000000000000
-```
-
-## Comment générer {#how-to-generate}
-
-Pour générer la valeur UUID, ClickHouse fournit [generateUUIDv4](../../sql-reference/functions/uuid-functions.md) fonction.
-
-## Exemple D'Utilisation {#usage-example}
-
-**Exemple 1**
-
-Cet exemple montre la création d'une table avec la colonne de type UUID et l'insertion d'une valeur dans la table.
-
-``` sql
-CREATE TABLE t_uuid (x UUID, y String) ENGINE=TinyLog
-```
-
-``` sql
-INSERT INTO t_uuid SELECT generateUUIDv4(), 'Example 1'
-```
-
-``` sql
-SELECT * FROM t_uuid
-```
-
-``` text
-┌────────────────────────────────────x─┬─y─────────┐
-│ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │
-└──────────────────────────────────────┴───────────┘
-```
-
-**Exemple 2**
-
-Dans cet exemple, la valeur de la colonne UUID n'est pas spécifiée lors de l'insertion d'un nouvel enregistrement.
-
-``` sql
-INSERT INTO t_uuid (y) VALUES ('Example 2')
-```
-
-``` sql
-SELECT * FROM t_uuid
-```
-
-``` text
-┌────────────────────────────────────x─┬─y─────────┐
-│ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │
-│ 00000000-0000-0000-0000-000000000000 │ Example 2 │
-└──────────────────────────────────────┴───────────┘
-```
-
-## Restriction {#restrictions}
-
-Le type de données UUID ne prend en charge que les fonctions qui [Chaîne](string.md) type de données prend également en charge (par exemple, [min](../../sql-reference/aggregate-functions/reference.md#agg_function-min), [Max](../../sql-reference/aggregate-functions/reference.md#agg_function-max), et [compter](../../sql-reference/aggregate-functions/reference.md#agg_function-count)).
-
-Le type de données UUID n'est pas pris en charge par les opérations arithmétiques (par exemple, [ABS](../../sql-reference/functions/arithmetic-functions.md#arithm_func-abs)) ou des fonctions d'agrégation, comme [somme](../../sql-reference/aggregate-functions/reference.md#agg_function-sum) et [avg](../../sql-reference/aggregate-functions/reference.md#agg_function-avg).
-
-[Article Original](https://clickhouse.tech/docs/en/data_types/uuid/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md
deleted file mode 100644
index cc238f02f3ab..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md
+++ /dev/null
@@ -1,70 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: "Dictionnaires hi\xE9rarchiques"
----
-
-# Dictionnaires Hiérarchiques {#hierarchical-dictionaries}
-
-Clickhouse prend en charge les dictionnaires hiérarchiques avec un [touche numérique](external-dicts-dict-structure.md#ext_dict-numeric-key).
-
-Voici une structure hiérarchique:
-
-``` text
-0 (Common parent)
-│
-├── 1 (Russia)
-│   │
-│   └── 2 (Moscow)
-│       │
-│       └── 3 (Center)
-│
-└── 4 (Great Britain)
-    │
-    └── 5 (London)
-```
-
-Cette hiérarchie peut être exprimée comme la table de dictionnaire suivante.
-
-| id_région | région_parent | nom_région        |
-|------------|----------------|--------------------|
-| 1          | 0              | Russie             |
-| 2          | 1              | Moscou             |
-| 3          | 2              | Center             |
-| 4          | 0              | La Grande-Bretagne |
-| 5          | 4              | Londres            |
-
-Ce tableau contient une colonne `parent_region` qui contient la clé du parent le plus proche de l'élément.
-
-Clickhouse soutient le [hiérarchique](external-dicts-dict-structure.md#hierarchical-dict-attr) propriété pour [externe dictionnaire](index.md) attribut. Cette propriété vous permet de configurer le dictionnaire hiérarchique comme décrit ci-dessus.
-
-Le [dictGetHierarchy](../../../sql-reference/functions/ext-dict-functions.md#dictgethierarchy) la fonction vous permet d'obtenir la chaîne parent d'un élément.
-
-Pour notre exemple, la structure du dictionnaire peut être la suivante:
-
-``` xml
-<dictionary>
-    <structure>
-        <id>
-            <name>region_id</name>
-        </id>
-
-        <attribute>
-            <name>parent_region</name>
-            <type>UInt64</type>
-            <null_value>0</null_value>
-            <hierarchical>true</hierarchical>
-        </attribute>
-
-        <attribute>
-            <name>region_name</name>
-            <type>String</type>
-            <null_value></null_value>
-        </attribute>
-
-    </structure>
-</dictionary>
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_hierarchical/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md
deleted file mode 100644
index 2569329fefd7..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md
+++ /dev/null
@@ -1,407 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: "Stockage des dictionnaires en m\xE9moire"
----
-
-# Stockage des dictionnaires en mémoire {#dicts-external-dicts-dict-layout}
-
-Il existe une variété de façons de stocker les dictionnaires en mémoire.
-
-Nous vous recommandons [plat](#flat), [haché](#dicts-external_dicts_dict_layout-hashed) et [complex_key_hashed](#complex-key-hashed). qui fournissent la vitesse de traitement optimale.
-
-La mise en cache n'est pas recommandée en raison de performances potentiellement médiocres et de difficultés à sélectionner les paramètres optimaux. En savoir plus dans la section “[cache](#cache)”.
-
-Il existe plusieurs façons d'améliorer les performances du dictionnaire:
-
--   Appelez la fonction pour travailler avec le dictionnaire après `GROUP BY`.
--   Marquer les attributs à extraire comme injectifs. Un attribut est appelé injectif si différentes valeurs d'attribut correspondent à différentes clés. Alors, quand `GROUP BY` utilise une fonction qui récupère une valeur d'attribut par la clé, cette fonction est automatiquement retirée de `GROUP BY`.
-
-ClickHouse génère une exception pour les erreurs avec les dictionnaires. Des exemples d'erreurs:
-
--   Le dictionnaire accessible n'a pas pu être chargé.
--   Erreur de la requête d'une `cached` dictionnaire.
-
-Vous pouvez afficher la liste des dictionnaires externes et leurs statuts dans le `system.dictionaries` table.
-
-La configuration ressemble à ceci:
-
-``` xml
-<yandex>
-    <dictionary>
-        ...
-        <layout>
-            <layout_type>
-                <!-- layout settings -->
-            </layout_type>
-        </layout>
-        ...
-    </dictionary>
-</yandex>
-```
-
-Correspondant [DDL-requête](../../statements/create.md#create-dictionary-query):
-
-``` sql
-CREATE DICTIONARY (...)
-...
-LAYOUT(LAYOUT_TYPE(param value)) -- layout settings
-...
-```
-
-## Façons de stocker des dictionnaires en mémoire {#ways-to-store-dictionaries-in-memory}
-
--   [plat](#flat)
--   [haché](#dicts-external_dicts_dict_layout-hashed)
--   [sparse_hashed](#dicts-external_dicts_dict_layout-sparse_hashed)
--   [cache](#cache)
--   [direct](#direct)
--   [range_hashed](#range-hashed)
--   [complex_key_hashed](#complex-key-hashed)
--   [complex_key_cache](#complex-key-cache)
--   [complex_key_direct](#complex-key-direct)
--   [ip_trie](#ip-trie)
-
-### plat {#flat}
-
-Le dictionnaire est complètement stocké en mémoire sous la forme de tableaux plats. Combien de mémoire le dictionnaire utilise-t-il? Le montant est proportionnel à la taille de la plus grande clé (dans l'espace).
-
-La clé du dictionnaire a le `UInt64` type et la valeur est limitée à 500 000. Si une clé plus grande est découverte lors de la création du dictionnaire, ClickHouse lève une exception et ne crée pas le dictionnaire.
-
-Tous les types de sources sont pris en charge. Lors de la mise à jour, les données (à partir d'un fichier ou d'une table) sont lues dans leur intégralité.
-
-Cette méthode fournit les meilleures performances parmi toutes les méthodes disponibles de stockage du dictionnaire.
-
-Exemple de Configuration:
-
-``` xml
-<layout>
-  <flat />
-</layout>
-```
-
-ou
-
-``` sql
-LAYOUT(FLAT())
-```
-
-### haché {#dicts-external_dicts_dict_layout-hashed}
-
-Le dictionnaire est entièrement stockée en mémoire sous la forme d'une table de hachage. Le dictionnaire peut contenir n'importe quel nombre d'éléments avec tous les identificateurs Dans la pratique, le nombre de clés peut atteindre des dizaines de millions d'articles.
-
-Tous les types de sources sont pris en charge. Lors de la mise à jour, les données (à partir d'un fichier ou d'une table) sont lues dans leur intégralité.
-
-Exemple de Configuration:
-
-``` xml
-<layout>
-  <hashed />
-</layout>
-```
-
-ou
-
-``` sql
-LAYOUT(HASHED())
-```
-
-### sparse_hashed {#dicts-external_dicts_dict_layout-sparse_hashed}
-
-Semblable à `hashed`, mais utilise moins de mémoire en faveur de plus D'utilisation du processeur.
-
-Exemple de Configuration:
-
-``` xml
-<layout>
-  <sparse_hashed />
-</layout>
-```
-
-``` sql
-LAYOUT(SPARSE_HASHED())
-```
-
-### complex_key_hashed {#complex-key-hashed}
-
-Ce type de stockage est pour une utilisation avec composite [touches](external-dicts-dict-structure.md). Semblable à `hashed`.
-
-Exemple de Configuration:
-
-``` xml
-<layout>
-  <complex_key_hashed />
-</layout>
-```
-
-``` sql
-LAYOUT(COMPLEX_KEY_HASHED())
-```
-
-### range_hashed {#range-hashed}
-
-Le dictionnaire est stocké en mémoire sous la forme d'une table de hachage avec un tableau ordonné de gammes et leurs valeurs correspondantes.
-
-Cette méthode de stockage fonctionne de la même manière que hachée et permet d'utiliser des plages de date / heure (Type numérique arbitraire) en plus de la clé.
-
-Exemple: Le tableau contient des réductions pour chaque annonceur dans le format:
-
-``` text
-+---------|-------------|-------------|------+
-| advertiser id | discount start date | discount end date | amount |
-+===============+=====================+===================+========+
-| 123           | 2015-01-01          | 2015-01-15        | 0.15   |
-+---------|-------------|-------------|------+
-| 123           | 2015-01-16          | 2015-01-31        | 0.25   |
-+---------|-------------|-------------|------+
-| 456           | 2015-01-01          | 2015-01-15        | 0.05   |
-+---------|-------------|-------------|------+
-```
-
-Pour utiliser un échantillon pour les plages de dates, définissez `range_min` et `range_max` éléments dans le [structure](external-dicts-dict-structure.md). Ces éléments doivent contenir des éléments `name` et`type` (si `type` n'est pas spécifié, le type par défaut sera utilisé-Date). `type` peut être n'importe quel type numérique (Date / DateTime / UInt64 / Int32 / autres).
-
-Exemple:
-
-``` xml
-<structure>
-    <id>
-        <name>Id</name>
-    </id>
-    <range_min>
-        <name>first</name>
-        <type>Date</type>
-    </range_min>
-    <range_max>
-        <name>last</name>
-        <type>Date</type>
-    </range_max>
-    ...
-```
-
-ou
-
-``` sql
-CREATE DICTIONARY somedict (
-    id UInt64,
-    first Date,
-    last Date
-)
-PRIMARY KEY id
-LAYOUT(RANGE_HASHED())
-RANGE(MIN first MAX last)
-```
-
-Pour travailler avec ces dictionnaires, vous devez passer un argument supplémentaire à l' `dictGetT` fonction, pour laquelle une plage est sélectionnée:
-
-``` sql
-dictGetT('dict_name', 'attr_name', id, date)
-```
-
-Cette fonction retourne la valeur pour l' `id`s et la plage de dates qui inclut la date passée.
-
-Détails de l'algorithme:
-
--   Si l' `id` est introuvable ou une plage n'est pas trouvé pour l' `id` il retourne la valeur par défaut pour le dictionnaire.
--   S'il y a des plages qui se chevauchent, vous pouvez en utiliser.
--   Si le délimiteur est `NULL` ou une date non valide (telle que 1900-01-01 ou 2039-01-01), la plage est laissée ouverte. La gamme peut être ouverte des deux côtés.
-
-Exemple de Configuration:
-
-``` xml
-<yandex>
-        <dictionary>
-
-                ...
-
-                <layout>
-                        <range_hashed />
-                </layout>
-
-                <structure>
-                        <id>
-                                <name>Abcdef</name>
-                        </id>
-                        <range_min>
-                                <name>StartTimeStamp</name>
-                                <type>UInt64</type>
-                        </range_min>
-                        <range_max>
-                                <name>EndTimeStamp</name>
-                                <type>UInt64</type>
-                        </range_max>
-                        <attribute>
-                                <name>XXXType</name>
-                                <type>String</type>
-                                <null_value />
-                        </attribute>
-                </structure>
-
-        </dictionary>
-</yandex>
-```
-
-ou
-
-``` sql
-CREATE DICTIONARY somedict(
-    Abcdef UInt64,
-    StartTimeStamp UInt64,
-    EndTimeStamp UInt64,
-    XXXType String DEFAULT ''
-)
-PRIMARY KEY Abcdef
-RANGE(MIN StartTimeStamp MAX EndTimeStamp)
-```
-
-### cache {#cache}
-
-Le dictionnaire est stocké dans un cache qui a un nombre fixe de cellules. Ces cellules contiennent des éléments fréquemment utilisés.
-
-Lors de la recherche d'un dictionnaire, le cache est recherché en premier. Pour chaque bloc de données, toutes les clés qui ne sont pas trouvées dans le cache ou qui sont obsolètes sont demandées à la source en utilisant `SELECT attrs... FROM db.table WHERE id IN (k1, k2, ...)`. Les données reçues sont ensuite écrites dans le cache.
-
-Pour les dictionnaires de cache, l'expiration [vie](external-dicts-dict-lifetime.md) des données dans le cache peuvent être définies. Si plus de temps que `lifetime` passé depuis le chargement des données dans une cellule, la valeur de la cellule n'est pas utilisée et elle est demandée à nouveau la prochaine fois qu'elle doit être utilisée.
-C'est la moins efficace de toutes les façons de stocker les dictionnaires. La vitesse du cache dépend fortement des paramètres corrects et que le scénario d'utilisation. Un dictionnaire de type de cache fonctionne bien uniquement lorsque les taux de réussite sont suffisamment élevés (recommandé 99% et plus). Vous pouvez afficher le taux de réussite moyen dans le `system.dictionaries` table.
-
-Pour améliorer les performances du cache, utilisez une sous-requête avec `LIMIT`, et appelez la fonction avec le dictionnaire en externe.
-
-Soutenu [source](external-dicts-dict-sources.md): MySQL, ClickHouse, exécutable, HTTP.
-
-Exemple de paramètres:
-
-``` xml
-<layout>
-    <cache>
-        <!-- The size of the cache, in number of cells. Rounded up to a power of two. -->
-        <size_in_cells>1000000000</size_in_cells>
-    </cache>
-</layout>
-```
-
-ou
-
-``` sql
-LAYOUT(CACHE(SIZE_IN_CELLS 1000000000))
-```
-
-Définissez une taille de cache suffisamment grande. Vous devez expérimenter pour sélectionner le nombre de cellules:
-
-1.  Définissez une valeur.
-2.  Exécutez les requêtes jusqu'à ce que le cache soit complètement plein.
-3.  Évaluer la consommation de mémoire en utilisant le `system.dictionaries` table.
-4.  Augmentez ou diminuez le nombre de cellules jusqu'à ce que la consommation de mémoire requise soit atteinte.
-
-!!! warning "Avertissement"
-    N'utilisez pas ClickHouse comme source, car le traitement des requêtes avec des lectures aléatoires est lent.
-
-### complex_key_cache {#complex-key-cache}
-
-Ce type de stockage est pour une utilisation avec composite [touches](external-dicts-dict-structure.md). Semblable à `cache`.
-
-### direct {#direct}
-
-Le dictionnaire n'est pas stocké dans la mémoire et va directement à la source, pendant le traitement d'une demande.
-
-La clé du dictionnaire a le `UInt64` type.
-
-Tous les types de [source](external-dicts-dict-sources.md), sauf les fichiers locaux, sont pris en charge.
-
-Exemple de Configuration:
-
-``` xml
-<layout>
-  <direct />
-</layout>
-```
-
-ou
-
-``` sql
-LAYOUT(DIRECT())
-```
-
-### complex_key_direct {#complex-key-direct}
-
-Ce type de stockage est destiné à être utilisé avec des [clés](../../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md) composites. Similaire à `direct`
-
-### ip_trie {#ip-trie}
-
-Ce type de stockage permet de mapper des préfixes de réseau (adresses IP) à des métadonnées telles que ASN.
-
-Exemple: la table contient les préfixes de réseau et leur correspondant en tant que numéro et Code de pays:
-
-``` text
-  +-----------|-----|------+
-  | prefix          | asn   | cca2   |
-  +=================+=======+========+
-  | 202.79.32.0/20  | 17501 | NP     |
-  +-----------|-----|------+
-  | 2620:0:870::/48 | 3856  | US     |
-  +-----------|-----|------+
-  | 2a02:6b8:1::/48 | 13238 | RU     |
-  +-----------|-----|------+
-  | 2001:db8::/32   | 65536 | ZZ     |
-  +-----------|-----|------+
-```
-
-Lorsque vous utilisez ce type de mise en page, la structure doit avoir une clé composite.
-
-Exemple:
-
-``` xml
-<structure>
-    <key>
-        <attribute>
-            <name>prefix</name>
-            <type>String</type>
-        </attribute>
-    </key>
-    <attribute>
-            <name>asn</name>
-            <type>UInt32</type>
-            <null_value />
-    </attribute>
-    <attribute>
-            <name>cca2</name>
-            <type>String</type>
-            <null_value>??</null_value>
-    </attribute>
-    ...
-</structure>
-<layout>
-    <ip_trie>
-        <access_to_key_from_attributes>true</access_to_key_from_attributes>
-    </ip_trie>
-</layout>
-```
-
-ou
-
-``` sql
-CREATE DICTIONARY somedict (
-    prefix String,
-    asn UInt32,
-    cca2 String DEFAULT '??'
-)
-PRIMARY KEY prefix
-```
-
-La clé ne doit avoir qu'un seul attribut de type chaîne contenant un préfixe IP autorisé. Les autres types ne sont pas encore pris en charge.
-
-Pour les requêtes, vous devez utiliser les mêmes fonctions (`dictGetT` avec un n-uplet) comme pour les dictionnaires avec des clés composites:
-
-``` sql
-dictGetT('dict_name', 'attr_name', tuple(ip))
-```
-
-La fonction prend soit `UInt32` pour IPv4, ou `FixedString(16)` pour IPv6:
-
-``` sql
-dictGetString('prefix', 'asn', tuple(IPv6StringToNum('2001:db8::1')))
-```
-
-Les autres types ne sont pas encore pris en charge. La fonction renvoie l'attribut du préfixe correspondant à cette adresse IP. S'il y a chevauchement des préfixes, le plus spécifique est retourné.
-
-Les données doit complètement s'intégrer dans la RAM.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_layout/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md
deleted file mode 100644
index 8ce78919ff11..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md
+++ /dev/null
@@ -1,91 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: "Mises \xC0 Jour Du Dictionnaire"
----
-
-# Mises À Jour Du Dictionnaire {#dictionary-updates}
-
-ClickHouse met périodiquement à jour les dictionnaires. L'intervalle de mise à jour pour les dictionnaires entièrement téléchargés et l'intervalle d'invalidation pour les dictionnaires `<lifetime>` tag en quelques secondes.
-
-Les mises à jour du dictionnaire (autres que le chargement pour la première utilisation) ne bloquent pas les requêtes. Lors des mises à jour, l'ancienne version d'un dictionnaire est utilisée. Si une erreur se produit pendant une mise à jour, l'erreur est écrite dans le journal du serveur et les requêtes continuent d'utiliser l'ancienne version des dictionnaires.
-
-Exemple de paramètres:
-
-``` xml
-<dictionary>
-    ...
-    <lifetime>300</lifetime>
-    ...
-</dictionary>
-```
-
-``` sql
-CREATE DICTIONARY (...)
-...
-LIFETIME(300)
-...
-```
-
-Paramètre `<lifetime>0</lifetime>` (`LIFETIME(0)`) empêche la mise à jour des dictionnaires.
-
-Vous pouvez définir un intervalle de temps pour les mises à niveau, et ClickHouse choisira un temps uniformément aléatoire dans cette plage. Ceci est nécessaire pour répartir la charge sur la source du dictionnaire lors de la mise à niveau sur un grand nombre de serveurs.
-
-Exemple de paramètres:
-
-``` xml
-<dictionary>
-    ...
-    <lifetime>
-        <min>300</min>
-        <max>360</max>
-    </lifetime>
-    ...
-</dictionary>
-```
-
-ou
-
-``` sql
-LIFETIME(MIN 300 MAX 360)
-```
-
-Si `<min>0</min>` et `<max>0</max>`, ClickHouse ne recharge pas le dictionnaire par timeout.
-Dans ce cas, ClickHouse peut recharger le dictionnaire plus tôt si le fichier de configuration du dictionnaire a été `SYSTEM RELOAD DICTIONARY` la commande a été exécutée.
-
-Lors de la mise à niveau des dictionnaires, le serveur ClickHouse applique une logique différente selon le type de [source](external-dicts-dict-sources.md):
-
-Lors de la mise à niveau des dictionnaires, le serveur ClickHouse applique une logique différente selon le type de [source](external-dicts-dict-sources.md):
-
--   Pour un fichier texte, il vérifie l'heure de la modification. Si l'heure diffère de l'heure enregistrée précédemment, le dictionnaire est mis à jour.
--   Pour les tables MyISAM, l'Heure de modification est vérifiée à l'aide d'un `SHOW TABLE STATUS` requête.
--   Les dictionnaires d'autres sources sont mis à jour à chaque fois par défaut.
-
-Pour les sources MySQL (InnoDB), ODBC et ClickHouse, vous pouvez configurer une requête qui mettra à jour les dictionnaires uniquement s'ils ont vraiment changé, plutôt que chaque fois. Pour ce faire, suivez ces étapes:
-
--   La table de dictionnaire doit avoir un champ qui change toujours lorsque les données source sont mises à jour.
--   Les paramètres de la source doivent spécifier une requête qui récupère le champ de modification. Le serveur ClickHouse interprète le résultat de la requête comme une ligne, et si cette ligne a changé par rapport à son état précédent, le dictionnaire est mis à jour. Spécifier la requête dans le `<invalidate_query>` champ dans les paramètres pour le [source](external-dicts-dict-sources.md).
-
-Exemple de paramètres:
-
-``` xml
-<dictionary>
-    ...
-    <odbc>
-      ...
-      <invalidate_query>SELECT update_time FROM dictionary_source where id = 1</invalidate_query>
-    </odbc>
-    ...
-</dictionary>
-```
-
-ou
-
-``` sql
-...
-SOURCE(ODBC(... invalidate_query 'SELECT update_time FROM dictionary_source where id = 1'))
-...
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_lifetime/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md
deleted file mode 100644
index 4c608fa71880..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md
+++ /dev/null
@@ -1,630 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: Sources de dictionnaires externes
----
-
-# Sources de dictionnaires externes {#dicts-external-dicts-dict-sources}
-
-Externe dictionnaire peut être connecté à partir de nombreuses sources différentes.
-
-Si dictionary est configuré à l'aide de xml-file, la configuration ressemble à ceci:
-
-``` xml
-<yandex>
-  <dictionary>
-    ...
-    <source>
-      <source_type>
-        <!-- Source configuration -->
-      </source_type>
-    </source>
-    ...
-  </dictionary>
-  ...
-</yandex>
-```
-
-En cas de [DDL-requête](../../statements/create.md#create-dictionary-query), configuration égale ressemblera à:
-
-``` sql
-CREATE DICTIONARY dict_name (...)
-...
-SOURCE(SOURCE_TYPE(param1 val1 ... paramN valN)) -- Source configuration
-...
-```
-
-La source est configurée dans le `source` section.
-
-Pour les types de source [Fichier Local](#dicts-external_dicts_dict_sources-local_file), [Fichier exécutable](#dicts-external_dicts_dict_sources-executable), [HTTP(S)](#dicts-external_dicts_dict_sources-http), [ClickHouse](#dicts-external_dicts_dict_sources-clickhouse)
-les paramètres optionnels sont disponibles:
-
-``` xml
-<source>
-  <file>
-    <path>/opt/dictionaries/os.tsv</path>
-    <format>TabSeparated</format>
-  </file>
-  <settings>
-      <format_csv_allow_single_quotes>0</format_csv_allow_single_quotes>
-  </settings>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(FILE(path '/opt/dictionaries/os.tsv' format 'TabSeparated'))
-SETTINGS(format_csv_allow_single_quotes = 0)
-```
-
-Les Types de sources (`source_type`):
-
--   [Fichier Local](#dicts-external_dicts_dict_sources-local_file)
--   [Fichier exécutable](#dicts-external_dicts_dict_sources-executable)
--   [HTTP(S)](#dicts-external_dicts_dict_sources-http)
--   DBMS
-    -   [ODBC](#dicts-external_dicts_dict_sources-odbc)
-    -   [MySQL](#dicts-external_dicts_dict_sources-mysql)
-    -   [ClickHouse](#dicts-external_dicts_dict_sources-clickhouse)
-    -   [MongoDB](#dicts-external_dicts_dict_sources-mongodb)
-    -   [Redis](#dicts-external_dicts_dict_sources-redis)
-
-## Fichier Local {#dicts-external_dicts_dict_sources-local_file}
-
-Exemple de paramètres:
-
-``` xml
-<source>
-  <file>
-    <path>/opt/dictionaries/os.tsv</path>
-    <format>TabSeparated</format>
-  </file>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(FILE(path '/opt/dictionaries/os.tsv' format 'TabSeparated'))
-```
-
-Définition des champs:
-
--   `path` – The absolute path to the file.
--   `format` – The file format. All the formats described in “[Format](../../../interfaces/formats.md#formats)” sont pris en charge.
-
-## Fichier Exécutable {#dicts-external_dicts_dict_sources-executable}
-
-Travailler avec des fichiers exécutables en dépend [comment le dictionnaire est stocké dans la mémoire](external-dicts-dict-layout.md). Si le dictionnaire est stocké en utilisant `cache` et `complex_key_cache`, Clickhouse demande les clés nécessaires en envoyant une requête au STDIN du fichier exécutable. Sinon, ClickHouse démarre le fichier exécutable et traite sa sortie comme des données de dictionnaire.
-
-Exemple de paramètres:
-
-``` xml
-<source>
-    <executable>
-        <command>cat /opt/dictionaries/os.tsv</command>
-        <format>TabSeparated</format>
-    </executable>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(EXECUTABLE(command 'cat /opt/dictionaries/os.tsv' format 'TabSeparated'))
-```
-
-Définition des champs:
-
--   `command` – The absolute path to the executable file, or the file name (if the program directory is written to `PATH`).
--   `format` – The file format. All the formats described in “[Format](../../../interfaces/formats.md#formats)” sont pris en charge.
-
-## Http(s) {#dicts-external_dicts_dict_sources-http}
-
-Travailler avec un serveur HTTP (S) dépend de [comment le dictionnaire est stocké dans la mémoire](external-dicts-dict-layout.md). Si le dictionnaire est stocké en utilisant `cache` et `complex_key_cache`, Clickhouse demande les clés nécessaires en envoyant une demande via le `POST` méthode.
-
-Exemple de paramètres:
-
-``` xml
-<source>
-    <http>
-        <url>http://[::1]/os.tsv</url>
-        <format>TabSeparated</format>
-        <credentials>
-            <user>user</user>
-            <password>password</password>
-        </credentials>
-        <headers>
-            <header>
-                <name>API-KEY</name>
-                <value>key</value>
-            </header>
-        </headers>
-    </http>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(HTTP(
-    url 'http://[::1]/os.tsv'
-    format 'TabSeparated'
-    credentials(user 'user' password 'password')
-    headers(header(name 'API-KEY' value 'key'))
-))
-```
-
-Pour que ClickHouse accède à une ressource HTTPS, vous devez [configurer openSSL](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-openssl) dans la configuration du serveur.
-
-Définition des champs:
-
--   `url` – The source URL.
--   `format` – The file format. All the formats described in “[Format](../../../interfaces/formats.md#formats)” sont pris en charge.
--   `credentials` – Basic HTTP authentication. Optional parameter.
-    -   `user` – Username required for the authentication.
-    -   `password` – Password required for the authentication.
--   `headers` – All custom HTTP headers entries used for the HTTP request. Optional parameter.
-    -   `header` – Single HTTP header entry.
-    -   `name` – Identifiant name used for the header send on the request.
-    -   `value` – Value set for a specific identifiant name.
-
-## ODBC {#dicts-external_dicts_dict_sources-odbc}
-
-Vous pouvez utiliser cette méthode pour connecter n'importe quelle base de données dotée d'un pilote ODBC.
-
-Exemple de paramètres:
-
-``` xml
-<source>
-    <odbc>
-        <db>DatabaseName</db>
-        <table>ShemaName.TableName</table>
-        <connection_string>DSN=some_parameters</connection_string>
-        <invalidate_query>SQL_QUERY</invalidate_query>
-    </odbc>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(ODBC(
-    db 'DatabaseName'
-    table 'SchemaName.TableName'
-    connection_string 'DSN=some_parameters'
-    invalidate_query 'SQL_QUERY'
-))
-```
-
-Définition des champs:
-
--   `db` – Name of the database. Omit it if the database name is set in the `<connection_string>` paramètre.
--   `table` – Name of the table and schema if exists.
--   `connection_string` – Connection string.
--   `invalidate_query` – Query for checking the dictionary status. Optional parameter. Read more in the section [Mise à jour des dictionnaires](external-dicts-dict-lifetime.md).
-
-ClickHouse reçoit des symboles de citation D'ODBC-driver et cite tous les paramètres des requêtes au pilote, il est donc nécessaire de définir le nom de la table en conséquence sur le cas du nom de la table dans la base de données.
-
-Si vous avez des problèmes avec des encodages lors de l'utilisation d'Oracle, consultez le [FAQ](../../../faq/general.md#oracle-odbc-encodings) article.
-
-### Vulnérabilité connue de la fonctionnalité du dictionnaire ODBC {#known-vulnerability-of-the-odbc-dictionary-functionality}
-
-!!! attention "Attention"
-    Lors de la connexion à la base de données via le paramètre de connexion du pilote ODBC `Servername` peut être substitué. Dans ce cas, les valeurs de `USERNAME` et `PASSWORD` de `odbc.ini` sont envoyés au serveur distant et peuvent être compromis.
-
-**Exemple d'utilisation non sécurisée**
-
-Configurons unixODBC pour PostgreSQL. Le contenu de `/etc/odbc.ini`:
-
-``` text
-[gregtest]
-Driver = /usr/lib/psqlodbca.so
-Servername = localhost
-PORT = 5432
-DATABASE = test_db
-#OPTION = 3
-USERNAME = test
-PASSWORD = test
-```
-
-Si vous faites alors une requête telle que
-
-``` sql
-SELECT * FROM odbc('DSN=gregtest;Servername=some-server.com', 'test_db');
-```
-
-Le pilote ODBC enverra des valeurs de `USERNAME` et `PASSWORD` de `odbc.ini` de `some-server.com`.
-
-### Exemple de connexion Postgresql {#example-of-connecting-postgresql}
-
-Ubuntu OS.
-
-Installation d'unixODBC et du pilote ODBC pour PostgreSQL:
-
-``` bash
-$ sudo apt-get install -y unixodbc odbcinst odbc-postgresql
-```
-
-Configuration `/etc/odbc.ini` (ou `~/.odbc.ini`):
-
-``` text
-    [DEFAULT]
-    Driver = myconnection
-
-    [myconnection]
-    Description         = PostgreSQL connection to my_db
-    Driver              = PostgreSQL Unicode
-    Database            = my_db
-    Servername          = 127.0.0.1
-    UserName            = username
-    Password            = password
-    Port                = 5432
-    Protocol            = 9.3
-    ReadOnly            = No
-    RowVersioning       = No
-    ShowSystemTables    = No
-    ConnSettings        =
-```
-
-La configuration du dictionnaire dans ClickHouse:
-
-``` xml
-<yandex>
-    <dictionary>
-        <name>table_name</name>
-        <source>
-            <odbc>
-                <!-- You can specify the following parameters in connection_string: -->
-                <!-- DSN=myconnection;UID=username;PWD=password;HOST=127.0.0.1;PORT=5432;DATABASE=my_db -->
-                <connection_string>DSN=myconnection</connection_string>
-                <table>postgresql_table</table>
-            </odbc>
-        </source>
-        <lifetime>
-            <min>300</min>
-            <max>360</max>
-        </lifetime>
-        <layout>
-            <hashed/>
-        </layout>
-        <structure>
-            <id>
-                <name>id</name>
-            </id>
-            <attribute>
-                <name>some_column</name>
-                <type>UInt64</type>
-                <null_value>0</null_value>
-            </attribute>
-        </structure>
-    </dictionary>
-</yandex>
-```
-
-ou
-
-``` sql
-CREATE DICTIONARY table_name (
-    id UInt64,
-    some_column UInt64 DEFAULT 0
-)
-PRIMARY KEY id
-SOURCE(ODBC(connection_string 'DSN=myconnection' table 'postgresql_table'))
-LAYOUT(HASHED())
-LIFETIME(MIN 300 MAX 360)
-```
-
-Vous devrez peut-être modifier `odbc.ini` pour spécifier le chemin d'accès complet à la bibliothèque avec le conducteur `DRIVER=/usr/local/lib/psqlodbcw.so`.
-
-### Exemple de connexion à MS SQL Server {#example-of-connecting-ms-sql-server}
-
-Ubuntu OS.
-
-Installation du pilote: :
-
-``` bash
-$ sudo apt-get install tdsodbc freetds-bin sqsh
-```
-
-Configuration du pilote:
-
-``` bash
-    $ cat /etc/freetds/freetds.conf
-    ...
-
-    [MSSQL]
-    host = 192.168.56.101
-    port = 1433
-    tds version = 7.0
-    client charset = UTF-8
-
-    $ cat /etc/odbcinst.ini
-    ...
-
-    [FreeTDS]
-    Description     = FreeTDS
-    Driver          = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so
-    Setup           = /usr/lib/x86_64-linux-gnu/odbc/libtdsS.so
-    FileUsage       = 1
-    UsageCount      = 5
-
-    $ cat ~/.odbc.ini
-    ...
-
-    [MSSQL]
-    Description     = FreeTDS
-    Driver          = FreeTDS
-    Servername      = MSSQL
-    Database        = test
-    UID             = test
-    PWD             = test
-    Port            = 1433
-```
-
-Configuration du dictionnaire dans ClickHouse:
-
-``` xml
-<yandex>
-    <dictionary>
-        <name>test</name>
-        <source>
-            <odbc>
-                <table>dict</table>
-                <connection_string>DSN=MSSQL;UID=test;PWD=test</connection_string>
-            </odbc>
-        </source>
-
-        <lifetime>
-            <min>300</min>
-            <max>360</max>
-        </lifetime>
-
-        <layout>
-            <flat />
-        </layout>
-
-        <structure>
-            <id>
-                <name>k</name>
-            </id>
-            <attribute>
-                <name>s</name>
-                <type>String</type>
-                <null_value></null_value>
-            </attribute>
-        </structure>
-    </dictionary>
-</yandex>
-```
-
-ou
-
-``` sql
-CREATE DICTIONARY test (
-    k UInt64,
-    s String DEFAULT ''
-)
-PRIMARY KEY k
-SOURCE(ODBC(table 'dict' connection_string 'DSN=MSSQL;UID=test;PWD=test'))
-LAYOUT(FLAT())
-LIFETIME(MIN 300 MAX 360)
-```
-
-## DBMS {#dbms}
-
-### Mysql {#dicts-external_dicts_dict_sources-mysql}
-
-Exemple de paramètres:
-
-``` xml
-<source>
-  <mysql>
-      <port>3306</port>
-      <user>clickhouse</user>
-      <password>qwerty</password>
-      <replica>
-          <host>example01-1</host>
-          <priority>1</priority>
-      </replica>
-      <replica>
-          <host>example01-2</host>
-          <priority>1</priority>
-      </replica>
-      <db>db_name</db>
-      <table>table_name</table>
-      <where>id=10</where>
-      <invalidate_query>SQL_QUERY</invalidate_query>
-  </mysql>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(MYSQL(
-    port 3306
-    user 'clickhouse'
-    password 'qwerty'
-    replica(host 'example01-1' priority 1)
-    replica(host 'example01-2' priority 1)
-    db 'db_name'
-    table 'table_name'
-    where 'id=10'
-    invalidate_query 'SQL_QUERY'
-))
-```
-
-Définition des champs:
-
--   `port` – The port on the MySQL server. You can specify it for all replicas, or for each one individually (inside `<replica>`).
-
--   `user` – Name of the MySQL user. You can specify it for all replicas, or for each one individually (inside `<replica>`).
-
--   `password` – Password of the MySQL user. You can specify it for all replicas, or for each one individually (inside `<replica>`).
-
--   `replica` – Section of replica configurations. There can be multiple sections.
-
-        - `replica/host` – The MySQL host.
-        - `replica/priority` – The replica priority. When attempting to connect, ClickHouse traverses the replicas in order of priority. The lower the number, the higher the priority.
-
--   `db` – Name of the database.
-
--   `table` – Name of the table.
-
--   `where` – The selection criteria. The syntax for conditions is the same as for `WHERE` clause dans MySQL, par exemple, `id > 10 AND id < 20`. Paramètre facultatif.
-
--   `invalidate_query` – Query for checking the dictionary status. Optional parameter. Read more in the section [Mise à jour des dictionnaires](external-dicts-dict-lifetime.md).
-
-MySQL peut être connecté sur un hôte local via des sockets. Pour ce faire, définissez `host` et `socket`.
-
-Exemple de paramètres:
-
-``` xml
-<source>
-  <mysql>
-      <host>localhost</host>
-      <socket>/path/to/socket/file.sock</socket>
-      <user>clickhouse</user>
-      <password>qwerty</password>
-      <db>db_name</db>
-      <table>table_name</table>
-      <where>id=10</where>
-      <invalidate_query>SQL_QUERY</invalidate_query>
-  </mysql>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(MYSQL(
-    host 'localhost'
-    socket '/path/to/socket/file.sock'
-    user 'clickhouse'
-    password 'qwerty'
-    db 'db_name'
-    table 'table_name'
-    where 'id=10'
-    invalidate_query 'SQL_QUERY'
-))
-```
-
-### ClickHouse {#dicts-external_dicts_dict_sources-clickhouse}
-
-Exemple de paramètres:
-
-``` xml
-<source>
-    <clickhouse>
-        <host>example01-01-1</host>
-        <port>9000</port>
-        <user>default</user>
-        <password></password>
-        <db>default</db>
-        <table>ids</table>
-        <where>id=10</where>
-    </clickhouse>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(CLICKHOUSE(
-    host 'example01-01-1'
-    port 9000
-    user 'default'
-    password ''
-    db 'default'
-    table 'ids'
-    where 'id=10'
-))
-```
-
-Définition des champs:
-
--   `host` – The ClickHouse host. If it is a local host, the query is processed without any network activity. To improve fault tolerance, you can create a [Distribué](../../../engines/table-engines/special/distributed.md) table et entrez-le dans les configurations suivantes.
--   `port` – The port on the ClickHouse server.
--   `user` – Name of the ClickHouse user.
--   `password` – Password of the ClickHouse user.
--   `db` – Name of the database.
--   `table` – Name of the table.
--   `where` – The selection criteria. May be omitted.
--   `invalidate_query` – Query for checking the dictionary status. Optional parameter. Read more in the section [Mise à jour des dictionnaires](external-dicts-dict-lifetime.md).
-
-### Mongodb {#dicts-external_dicts_dict_sources-mongodb}
-
-Exemple de paramètres:
-
-``` xml
-<source>
-    <mongodb>
-        <host>localhost</host>
-        <port>27017</port>
-        <user></user>
-        <password></password>
-        <db>test</db>
-        <collection>dictionary_source</collection>
-    </mongodb>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(MONGO(
-    host 'localhost'
-    port 27017
-    user ''
-    password ''
-    db 'test'
-    collection 'dictionary_source'
-))
-```
-
-Définition des champs:
-
--   `host` – The MongoDB host.
--   `port` – The port on the MongoDB server.
--   `user` – Name of the MongoDB user.
--   `password` – Password of the MongoDB user.
--   `db` – Name of the database.
--   `collection` – Name of the collection.
-
-### Redis {#dicts-external_dicts_dict_sources-redis}
-
-Exemple de paramètres:
-
-``` xml
-<source>
-    <redis>
-        <host>localhost</host>
-        <port>6379</port>
-        <storage_type>simple</storage_type>
-        <db_index>0</db_index>
-    </redis>
-</source>
-```
-
-ou
-
-``` sql
-SOURCE(REDIS(
-    host 'localhost'
-    port 6379
-    storage_type 'simple'
-    db_index 0
-))
-```
-
-Définition des champs:
-
--   `host` – The Redis host.
--   `port` – The port on the Redis server.
--   `storage_type` – The structure of internal Redis storage using for work with keys. `simple` est pour les sources simples et pour les sources à clé unique hachées, `hash_map` est pour les sources hachées avec deux clés. Les sources À Distance et les sources de cache à clé complexe ne sont pas prises en charge. Peut être omis, la valeur par défaut est `simple`.
--   `db_index` – The specific numeric index of Redis logical database. May be omitted, default value is 0.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_sources/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md
deleted file mode 100644
index 1b9215baf062..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md
+++ /dev/null
@@ -1,175 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: "Cl\xE9 et champs du dictionnaire"
----
-
-# Clé et champs du dictionnaire {#dictionary-key-and-fields}
-
-Le `<structure>` la clause décrit la clé du dictionnaire et les champs disponibles pour les requêtes.
-
-Description XML:
-
-``` xml
-<dictionary>
-    <structure>
-        <id>
-            <name>Id</name>
-        </id>
-
-        <attribute>
-            <!-- Attribute parameters -->
-        </attribute>
-
-        ...
-
-    </structure>
-</dictionary>
-```
-
-Les attributs sont décrits dans les éléments:
-
--   `<id>` — [La colonne de la clé](external-dicts-dict-structure.md#ext_dict_structure-key).
--   `<attribute>` — [Colonne de données](external-dicts-dict-structure.md#ext_dict_structure-attributes). Il peut y avoir un certain nombre d'attributs.
-
-Requête DDL:
-
-``` sql
-CREATE DICTIONARY dict_name (
-    Id UInt64,
-    -- attributes
-)
-PRIMARY KEY Id
-...
-```
-
-Les attributs sont décrits dans le corps de la requête:
-
--   `PRIMARY KEY` — [La colonne de la clé](external-dicts-dict-structure.md#ext_dict_structure-key)
--   `AttrName AttrType` — [Colonne de données](external-dicts-dict-structure.md#ext_dict_structure-attributes). Il peut y avoir un certain nombre d'attributs.
-
-## Clé {#ext_dict_structure-key}
-
-ClickHouse prend en charge les types de clés suivants:
-
--   Touche numérique. `UInt64`. Défini dans le `<id>` tag ou en utilisant `PRIMARY KEY` mot.
--   Clé Composite. Ensemble de valeurs de types différents. Défini dans la balise `<key>` ou `PRIMARY KEY` mot.
-
-Une structure xml peut contenir `<id>` ou `<key>`. DDL-requête doit contenir unique `PRIMARY KEY`.
-
-!!! warning "Avertissement"
-    Vous ne devez pas décrire clé comme un attribut.
-
-### Touche Numérique {#ext_dict-numeric-key}
-
-Type: `UInt64`.
-
-Exemple de Configuration:
-
-``` xml
-<id>
-    <name>Id</name>
-</id>
-```
-
-Champs de Configuration:
-
--   `name` – The name of the column with keys.
-
-Pour DDL-requête:
-
-``` sql
-CREATE DICTIONARY (
-    Id UInt64,
-    ...
-)
-PRIMARY KEY Id
-...
-```
-
--   `PRIMARY KEY` – The name of the column with keys.
-
-### Clé Composite {#composite-key}
-
-La clé peut être un `tuple` de tous les types de champs. Le [disposition](external-dicts-dict-layout.md) dans ce cas, doit être `complex_key_hashed` ou `complex_key_cache`.
-
-!!! tip "Conseil"
-    Une clé composite peut être constitué d'un seul élément. Cela permet d'utiliser une chaîne comme clé, par exemple.
-
-La structure de clé est définie dans l'élément `<key>`. Les principaux champs sont spécifiés dans le même format que le dictionnaire [attribut](external-dicts-dict-structure.md). Exemple:
-
-``` xml
-<structure>
-    <key>
-        <attribute>
-            <name>field1</name>
-            <type>String</type>
-        </attribute>
-        <attribute>
-            <name>field2</name>
-            <type>UInt32</type>
-        </attribute>
-        ...
-    </key>
-...
-```
-
-ou
-
-``` sql
-CREATE DICTIONARY (
-    field1 String,
-    field2 String
-    ...
-)
-PRIMARY KEY field1, field2
-...
-```
-
-Pour une requête à l' `dictGet*` fonction, un tuple est passé comme clé. Exemple: `dictGetString('dict_name', 'attr_name', tuple('string for field1', num_for_field2))`.
-
-## Attribut {#ext_dict_structure-attributes}
-
-Exemple de Configuration:
-
-``` xml
-<structure>
-    ...
-    <attribute>
-        <name>Name</name>
-        <type>ClickHouseDataType</type>
-        <null_value></null_value>
-        <expression>rand64()</expression>
-        <hierarchical>true</hierarchical>
-        <injective>true</injective>
-        <is_object_id>true</is_object_id>
-    </attribute>
-</structure>
-```
-
-ou
-
-``` sql
-CREATE DICTIONARY somename (
-    Name ClickHouseDataType DEFAULT '' EXPRESSION rand64() HIERARCHICAL INJECTIVE IS_OBJECT_ID
-)
-```
-
-Champs de Configuration:
-
-| Balise                                               | Description                                                                                                                                                                                                                                                                                                                                                                      | Requis |
-|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|
-| `name`                                               | Nom de la colonne.                                                                                                                                                                                                                                                                                                                                                               | Oui    |
-| `type`                                               | Type de données ClickHouse.<br/>ClickHouse tente de convertir la valeur du dictionnaire vers le type de données spécifié. Par exemple, pour MySQL, le champ peut être `TEXT`, `VARCHAR`, ou `BLOB` dans la table source MySQL, mais il peut être téléchargé comme `String` à ClickHouse.<br/>[Nullable](../../../sql-reference/data-types/nullable.md) n'est pas pris en charge. | Oui    |
-| `null_value`                                         | Valeur par défaut pour un élément inexistant.<br/>Dans l'exemple, c'est une chaîne vide. Vous ne pouvez pas utiliser `NULL` dans ce domaine.                                                                                                                                                                                                                                     | Oui    |
-| `expression`                                         | [Expression](../../syntax.md#syntax-expressions) que ClickHouse s'exécute sur la valeur.<br/>L'expression peut être un nom de colonne dans la base de données SQL distante. Ainsi, vous pouvez l'utiliser pour créer un alias pour la colonne à distance.<br/><br/>Valeur par défaut: aucune expression.                                                                         | Aucun  |
-| <a name="hierarchical-dict-attr"></a> `hierarchical` | Si `true`, l'attribut contient la valeur d'un parent clé de la clé actuelle. Voir [Dictionnaires Hiérarchiques](external-dicts-dict-hierarchical.md).<br/><br/>Valeur par défaut: `false`.                                                                                                                                                                                       | Aucun  |
-| `injective`                                          | Indicateur qui indique si le `id -> attribute` l'image est [injective](https://en.wikipedia.org/wiki/Injective_function).<br/>Si `true`, ClickHouse peut automatiquement placer après le `GROUP BY` clause les requêtes aux dictionnaires avec injection. Habituellement, il réduit considérablement le montant de ces demandes.<br/><br/>Valeur par défaut: `false`.            | Aucun  |
-| `is_object_id`                                       | Indicateur qui indique si la requête est exécutée pour un document MongoDB par `ObjectID`.<br/><br/>Valeur par défaut: `false`.                                                                                                                                                                                                                                                  | Aucun  |
-
-## Voir Aussi {#see-also}
-
--   [Fonctions pour travailler avec des dictionnaires externes](../../../sql-reference/functions/ext-dict-functions.md).
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict_structure/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md
deleted file mode 100644
index 3bb8884df2f4..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts-dict.md
+++ /dev/null
@@ -1,53 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: Configuration D'un dictionnaire externe
----
-
-# Configuration D'un dictionnaire externe {#dicts-external-dicts-dict}
-
-Si dictionary est configuré à l'aide d'un fichier xml, than dictionary configuration a la structure suivante:
-
-``` xml
-<dictionary>
-    <name>dict_name</name>
-
-    <structure>
-      <!-- Complex key configuration -->
-    </structure>
-
-    <source>
-      <!-- Source configuration -->
-    </source>
-
-    <layout>
-      <!-- Memory layout configuration -->
-    </layout>
-
-    <lifetime>
-      <!-- Lifetime of dictionary in memory -->
-    </lifetime>
-</dictionary>
-```
-
-Correspondant [DDL-requête](../../statements/create.md#create-dictionary-query) a la structure suivante:
-
-``` sql
-CREATE DICTIONARY dict_name
-(
-    ... -- attributes
-)
-PRIMARY KEY ... -- complex or single key configuration
-SOURCE(...) -- Source configuration
-LAYOUT(...) -- Memory layout configuration
-LIFETIME(...) -- Lifetime of dictionary in memory
-```
-
--   `name` – The identifier that can be used to access the dictionary. Use the characters `[a-zA-Z0-9_\-]`.
--   [source](external-dicts-dict-sources.md) — Source of the dictionary.
--   [disposition](external-dicts-dict-layout.md) — Dictionary layout in memory.
--   [structure](external-dicts-dict-structure.md) — Structure of the dictionary . A key and attributes that can be retrieved by this key.
--   [vie](external-dicts-dict-lifetime.md) — Frequency of dictionary updates.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts_dict/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts.md
deleted file mode 100644
index d68b7a7f1120..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/external-dicts.md
+++ /dev/null
@@ -1,62 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: "Description G\xE9n\xE9rale"
----
-
-# Dictionnaires Externes {#dicts-external-dicts}
-
-Vous pouvez ajouter vos propres dictionnaires à partir de diverses sources de données. La source de données d'un dictionnaire peut être un texte local ou un fichier exécutable, une ressource HTTP(S) ou un autre SGBD. Pour plus d'informations, voir “[Sources pour les dictionnaires externes](external-dicts-dict-sources.md)”.
-
-ClickHouse:
-
--   Stocke entièrement ou partiellement les dictionnaires en RAM.
--   Met à jour périodiquement les dictionnaires et charge dynamiquement les valeurs manquantes. En d'autres mots, les dictionnaires peuvent être chargés dynamiquement.
--   Permet de créer des dictionnaires externes avec des fichiers xml ou [Les requêtes DDL](../../statements/create.md#create-dictionary-query).
-
-La configuration des dictionnaires externes peut être située dans un ou plusieurs fichiers xml. Le chemin d'accès à la configuration spécifiée dans le [dictionaries_config](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-dictionaries_config) paramètre.
-
-Les dictionnaires peuvent être chargés au démarrage du serveur ou à la première utilisation, en fonction [dictionaries_lazy_load](../../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-dictionaries_lazy_load) paramètre.
-
-Le [dictionnaire](../../../operations/system-tables.md#system_tables-dictionaries) la table système contient des informations sur les dictionnaires configurés sur le serveur. Pour chaque dictionnaire, vous pouvez y trouver:
-
--   Statut du dictionnaire.
--   Paramètres de Configuration.
--   Des métriques telles que la quantité de RAM allouée pour le dictionnaire ou un certain nombre de requêtes depuis que le dictionnaire a été chargé avec succès.
-
-Le fichier de configuration du dictionnaire a le format suivant:
-
-``` xml
-<yandex>
-    <comment>An optional element with any content. Ignored by the ClickHouse server.</comment>
-
-    <!--Optional element. File name with substitutions-->
-    <include_from>/etc/metrika.xml</include_from>
-
-
-    <dictionary>
-        <!-- Dictionary configuration. -->
-        <!-- There can be any number of <dictionary> sections in the configuration file. -->
-    </dictionary>
-
-</yandex>
-```
-
-Vous pouvez [configurer](external-dicts-dict.md) le nombre de dictionnaires dans le même fichier.
-
-[Requêtes DDL pour les dictionnaires](../../statements/create.md#create-dictionary-query) ne nécessite aucun enregistrement supplémentaire dans la configuration du serveur. Ils permettent de travailler avec des dictionnaires en tant qu'entités de première classe, comme des tables ou des vues.
-
-!!! attention "Attention"
-    Vous pouvez convertir les valeurs pour un petit dictionnaire en le décrivant dans un `SELECT` requête (voir la [transformer](../../../sql-reference/functions/other-functions.md) fonction). Cette fonctionnalité n'est pas liée aux dictionnaires externes.
-
-## Voir Aussi {#ext-dicts-see-also}
-
--   [Configuration D'un dictionnaire externe](external-dicts-dict.md)
--   [Stockage des dictionnaires en mémoire](external-dicts-dict-layout.md)
--   [Mises À Jour Du Dictionnaire](external-dicts-dict-lifetime.md)
--   [Sources de dictionnaires externes](external-dicts-dict-sources.md)
--   [Clé et champs du dictionnaire](external-dicts-dict-structure.md)
--   [Fonctions pour travailler avec des dictionnaires externes](../../../sql-reference/functions/ext-dict-functions.md)
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/external_dicts/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/external-dictionaries/index.md b/docs/fr/sql-reference/dictionaries/external-dictionaries/index.md
deleted file mode 100644
index 109220205ddc..000000000000
--- a/docs/fr/sql-reference/dictionaries/external-dictionaries/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Dictionnaires Externes
-toc_priority: 37
----
-
-
diff --git a/docs/fr/sql-reference/dictionaries/index.md b/docs/fr/sql-reference/dictionaries/index.md
deleted file mode 100644
index 3ec31085cc50..000000000000
--- a/docs/fr/sql-reference/dictionaries/index.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Dictionnaire
-toc_priority: 35
-toc_title: Introduction
----
-
-# Dictionnaire {#dictionaries}
-
-Un dictionnaire est une cartographie (`key -> attributes`) qui est pratique pour différents types de listes de référence.
-
-ClickHouse prend en charge des fonctions spéciales pour travailler avec des dictionnaires qui peuvent être utilisés dans les requêtes. Il est plus facile et plus efficace d'utiliser des dictionnaires avec des fonctions que par une `JOIN` avec des tableaux de référence.
-
-[NULL](../../sql-reference/syntax.md#null-literal) les valeurs ne peuvent pas être stockées dans un dictionnaire.
-
-Supports ClickHouse:
-
--   [Construit-dans les dictionnaires](internal-dicts.md#internal_dicts) avec un [ensemble de fonctions](../../sql-reference/functions/ym-dict-functions.md).
--   [Plug-in (externe) dictionnaires](external-dictionaries/external-dicts.md#dicts-external-dicts) avec un [ensemble de fonctions](../../sql-reference/functions/ext-dict-functions.md).
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/) <!--hide-->
diff --git a/docs/fr/sql-reference/dictionaries/internal-dicts.md b/docs/fr/sql-reference/dictionaries/internal-dicts.md
deleted file mode 100644
index 607936031a16..000000000000
--- a/docs/fr/sql-reference/dictionaries/internal-dicts.md
+++ /dev/null
@@ -1,55 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: Dictionnaires Internes
----
-
-# Dictionnaires Internes {#internal_dicts}
-
-ClickHouse contient une fonction intégrée pour travailler avec une géobase.
-
-Cela vous permet de:
-
--   Utilisez L'ID d'une région pour obtenir son nom dans la langue souhaitée.
--   Utilisez L'ID d'une région pour obtenir L'ID d'une ville, d'une région, d'un district fédéral, d'un pays ou d'un continent.
--   Vérifiez si une région fait partie d'une autre région.
--   Obtenez une chaîne de régions parentes.
-
-Toutes les fonctions prennent en charge “translocality,” la capacité d'utiliser simultanément différentes perspectives sur la propriété de la région. Pour plus d'informations, consultez la section “Functions for working with Yandex.Metrica dictionaries”.
-
-Les dictionnaires internes sont désactivés dans le package par défaut.
-Pour les activer, décommentez les paramètres `path_to_regions_hierarchy_file` et `path_to_regions_names_files` dans le fichier de configuration du serveur.
-
-La géobase est chargée à partir de fichiers texte.
-
-Place de la `regions_hierarchy*.txt` les fichiers dans le `path_to_regions_hierarchy_file` répertoire. Ce paramètre de configuration doit contenir le chemin `regions_hierarchy.txt` fichier (la hiérarchie régionale par défaut), et les autres fichiers (`regions_hierarchy_ua.txt`) doit être situé dans le même répertoire.
-
-Mettre le `regions_names_*.txt` les fichiers dans le `path_to_regions_names_files` répertoire.
-
-Vous pouvez également créer ces fichiers vous-même. Le format de fichier est le suivant:
-
-`regions_hierarchy*.txt`: TabSeparated (pas d'en-tête), colonnes:
-
--   région de l'ID (`UInt32`)
--   ID de région parent (`UInt32`)
--   type de région (`UInt8`): 1-continent, 3-pays, 4-district fédéral, 5-région, 6-ville; les autres types n'ont pas de valeurs
--   population (`UInt32`) — optional column
-
-`regions_names_*.txt`: TabSeparated (pas d'en-tête), colonnes:
-
--   région de l'ID (`UInt32`)
--   nom de la région (`String`) — Can't contain tabs or line feeds, even escaped ones.
-
-Un tableau plat est utilisé pour stocker dans la RAM. Pour cette raison, les ID ne devraient pas dépasser un million.
-
-Les dictionnaires peuvent être mis à jour sans redémarrer le serveur. Cependant, l'ensemble des dictionnaires n'est pas mis à jour.
-Pour les mises à jour, les temps de modification du fichier sont vérifiés. Si un fichier a été modifié, le dictionnaire est mis à jour.
-L'intervalle de vérification des modifications est configuré dans le `builtin_dictionaries_reload_interval` paramètre.
-Les mises à jour du dictionnaire (autres que le chargement lors de la première utilisation) ne bloquent pas les requêtes. Lors des mises à jour, les requêtes utilisent les anciennes versions des dictionnaires. Si une erreur se produit pendant une mise à jour, l'erreur est écrite dans le journal du serveur et les requêtes continuent d'utiliser l'ancienne version des dictionnaires.
-
-Nous vous recommandons de mettre à jour périodiquement les dictionnaires avec la géobase. Lors d'une mise à jour, générez de nouveaux fichiers et écrivez-les dans un emplacement séparé. Lorsque tout est prêt, renommez - les en fichiers utilisés par le serveur.
-
-Il existe également des fonctions pour travailler avec les identifiants du système d'exploitation et Yandex.Moteurs de recherche Metrica, mais ils ne devraient pas être utilisés.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/dicts/internal_dicts/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/arithmetic-functions.md b/docs/fr/sql-reference/functions/arithmetic-functions.md
deleted file mode 100644
index c35fb1042362..000000000000
--- a/docs/fr/sql-reference/functions/arithmetic-functions.md
+++ /dev/null
@@ -1,87 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: "Arithm\xE9tique"
----
-
-# Fonctions Arithmétiques {#arithmetic-functions}
-
-Pour toutes les fonctions arithmétiques, le type de résultat est calculé comme le plus petit type de nombre dans lequel le résultat correspond, s'il existe un tel type. Le minimum est pris simultanément sur la base du nombre de bits, s'il est signé, et s'il flotte. S'il n'y a pas assez de bits, le type de bits le plus élevé est pris.
-
-Exemple:
-
-``` sql
-SELECT toTypeName(0), toTypeName(0 + 0), toTypeName(0 + 0 + 0), toTypeName(0 + 0 + 0 + 0)
-```
-
-``` text
-┌─toTypeName(0)─┬─toTypeName(plus(0, 0))─┬─toTypeName(plus(plus(0, 0), 0))─┬─toTypeName(plus(plus(plus(0, 0), 0), 0))─┐
-│ UInt8         │ UInt16                 │ UInt32                          │ UInt64                                   │
-└───────────────┴────────────────────────┴─────────────────────────────────┴──────────────────────────────────────────┘
-```
-
-Les fonctions arithmétiques fonctionnent pour n'importe quelle paire de types de UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64, Float32 ou Float64.
-
-Le débordement est produit de la même manière qu'en C++.
-
-## plus (A, B), opérateur a + b {#plusa-b-a-b-operator}
-
-Calcule la somme des nombres.
-Vous pouvez également ajouter des nombres entiers avec une date ou la date et l'heure. Dans le cas d'une date, Ajouter un entier signifie ajouter le nombre de jours correspondant. Pour une date avec l'heure, cela signifie ajouter le nombre de secondes correspondant.
-
-## moins (A, B), opérateur a - b {#minusa-b-a-b-operator}
-
-Calcule la différence. Le résultat est toujours signé.
-
-You can also calculate integer numbers from a date or date with time. The idea is the same – see above for ‘plus’.
-
-## la multiplication(a, b), a \* et b \* de l'opérateur {#multiplya-b-a-b-operator}
-
-Calcule le produit des nombres.
-
-## diviser (A, B), opérateur a / b {#dividea-b-a-b-operator}
-
-Calcule le quotient des nombres. Le type de résultat est toujours un type à virgule flottante.
-Il n'est pas de division entière. Pour la division entière, utilisez le ‘intDiv’ fonction.
-En divisant par zéro vous obtenez ‘inf’, ‘-inf’, ou ‘nan’.
-
-## intDiv (a, b) {#intdiva-b}
-
-Calcule le quotient des nombres. Divise en entiers, arrondi vers le bas (par la valeur absolue).
-Une exception est levée en divisant par zéro ou en divisant un nombre négatif minimal par moins un.
-
-## intDivOrZero(a, b) {#intdivorzeroa-b}
-
-Diffère de ‘intDiv’ en ce sens qu'il renvoie zéro en divisant par zéro ou en divisant un nombre négatif minimal par moins un.
-
-## opérateur modulo(A, B), A % B {#moduloa-b-a-b-operator}
-
-Calcule le reste après la division.
-Si les arguments sont des nombres à virgule flottante, ils sont pré-convertis en entiers en supprimant la partie décimale.
-Le reste est pris dans le même sens qu'en C++. La division tronquée est utilisée pour les nombres négatifs.
-Une exception est levée en divisant par zéro ou en divisant un nombre négatif minimal par moins un.
-
-## moduloOrZero (a, b) {#moduloorzeroa-b}
-
-Diffère de ‘modulo’ en ce sens qu'il renvoie zéro lorsque le diviseur est nul.
-
-## annuler (a), - un opérateur {#negatea-a-operator}
-
-Calcule un nombre avec le signe inverse. Le résultat est toujours signé.
-
-## abs(un) {#arithm_func-abs}
-
-Calcule la valeur absolue d'un nombre (un). Autrement dit, si un \< 0, Il renvoie-A. pour les types non signés, il ne fait rien. Pour les types entiers signés, il renvoie un nombre non signé.
-
-## pgcd(a, b) {#gcda-b}
-
-Renvoie le plus grand diviseur commun des nombres.
-Une exception est levée en divisant par zéro ou en divisant un nombre négatif minimal par moins un.
-
-## ppcm(a, b) {#lcma-b}
-
-Renvoie le multiple le moins commun des nombres.
-Une exception est levée en divisant par zéro ou en divisant un nombre négatif minimal par moins un.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/arithmetic_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/array-functions.md b/docs/fr/sql-reference/functions/array-functions.md
deleted file mode 100644
index 405688413724..000000000000
--- a/docs/fr/sql-reference/functions/array-functions.md
+++ /dev/null
@@ -1,1061 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: Travailler avec des tableaux
----
-
-# Fonctions pour travailler avec des tableaux {#functions-for-working-with-arrays}
-
-## vide {#function-empty}
-
-Retourne 1 pour un tableau vide, ou 0 pour un non-vide.
-Le type de résultat est UInt8.
-La fonction fonctionne également pour les chaînes.
-
-## notEmpty {#function-notempty}
-
-Retourne 0 pour un tableau vide, ou 1 pour un non-vide.
-Le type de résultat est UInt8.
-La fonction fonctionne également pour les chaînes.
-
-## longueur {#array_functions-length}
-
-Retourne le nombre d'éléments dans le tableau.
-Le type de résultat est UInt64.
-La fonction fonctionne également pour les chaînes.
-
-## emptyArrayUInt8, emptyArrayUInt16, emptyArrayUInt32, emptyArrayUInt64 {#emptyarrayuint8-emptyarrayuint16-emptyarrayuint32-emptyarrayuint64}
-
-## emptyArrayInt8, emptyArrayInt16, emptyArrayInt32, emptyArrayInt64 {#emptyarrayint8-emptyarrayint16-emptyarrayint32-emptyarrayint64}
-
-## emptyArrayFloat32, emptyArrayFloat64 {#emptyarrayfloat32-emptyarrayfloat64}
-
-## emptyArrayDate, emptyArrayDateTime {#emptyarraydate-emptyarraydatetime}
-
-## emptyArrayString {#emptyarraystring}
-
-Accepte zéro argument et renvoie un tableau vide du type approprié.
-
-## emptyArrayToSingle {#emptyarraytosingle}
-
-Accepte un tableau vide et renvoie un élément de tableau qui est égal à la valeur par défaut.
-
-## plage (fin), Plage(début, fin \[, étape\]) {#rangeend-rangestart-end-step}
-
-Retourne un tableau de nombres du début à la fin-1 par étape.
-Si l'argument `start` n'est pas spécifié, la valeur par défaut est 0.
-Si l'argument `step` n'est pas spécifié, la valeur par défaut est 1.
-Il se comporte presque comme pythonic `range`. Mais la différence est que tous les types d'arguments doivent être `UInt` nombre.
-Juste au cas où, une exception est levée si des tableaux d'une longueur totale de plus de 100 000 000 d'éléments sont créés dans un bloc de données.
-
-## array(x1, …), operator \[x1, …\] {#arrayx1-operator-x1}
-
-Crée un tableau à partir des arguments de la fonction.
-Les arguments doivent être des constantes et avoir des types qui ont le plus petit type commun. Au moins un argument doit être passé, sinon il n'est pas clair quel type de tableau créer. Qui est, vous ne pouvez pas utiliser cette fonction pour créer un tableau vide (pour ce faire, utilisez la ‘emptyArray\*’ la fonction décrite ci-dessus).
-Retourne un ‘Array(T)’ type de résultat, où ‘T’ est le plus petit type commun parmi les arguments passés.
-
-## arrayConcat {#arrayconcat}
-
-Combine des tableaux passés comme arguments.
-
-``` sql
-arrayConcat(arrays)
-```
-
-**Paramètre**
-
--   `arrays` – Arbitrary number of arguments of [Tableau](../../sql-reference/data-types/array.md) type.
-    **Exemple**
-
-<!-- -->
-
-``` sql
-SELECT arrayConcat([1, 2], [3, 4], [5, 6]) AS res
-```
-
-``` text
-┌─res───────────┐
-│ [1,2,3,4,5,6] │
-└───────────────┘
-```
-
-## arrayElement(arr, n), opérateur arr\[n\] {#arrayelementarr-n-operator-arrn}
-
-Récupérer l'élément avec l'index `n` à partir du tableau `arr`. `n` doit être n'importe quel type entier.
-Les index dans un tableau commencent à partir d'un.
-Les index négatifs sont pris en charge. Dans ce cas, il sélectionne l'élément correspondant numérotées à partir de la fin. Exemple, `arr[-1]` est le dernier élément du tableau.
-
-Si l'index est en dehors des limites d'un tableau, il renvoie une valeur (0 pour les nombres, une chaîne vide pour les cordes, etc.), sauf pour le cas avec un tableau non constant et un index constant 0 (dans ce cas, il y aura une erreur `Array indices are 1-based`).
-
-## a (arr, elem) {#hasarr-elem}
-
-Vérifie si le ‘arr’ tableau a la ‘elem’ élément.
-Retourne 0 si l'élément n'est pas dans le tableau, ou 1 si elle l'est.
-
-`NULL` est traitée comme une valeur.
-
-``` sql
-SELECT has([1, 2, NULL], NULL)
-```
-
-``` text
-┌─has([1, 2, NULL], NULL)─┐
-│                       1 │
-└─────────────────────────┘
-```
-
-## hasAll {#hasall}
-
-Vérifie si un tableau est un sous-ensemble de l'autre.
-
-``` sql
-hasAll(set, subset)
-```
-
-**Paramètre**
-
--   `set` – Array of any type with a set of elements.
--   `subset` – Array of any type with elements that should be tested to be a subset of `set`.
-
-**Les valeurs de retour**
-
--   `1`, si `set` contient tous les éléments de `subset`.
--   `0`, autrement.
-
-**Propriétés particulières**
-
--   Un tableau vide est un sous-ensemble d'un tableau quelconque.
--   `Null` traitée comme une valeur.
--   Ordre des valeurs dans les deux tableaux n'a pas d'importance.
-
-**Exemple**
-
-`SELECT hasAll([], [])` retours 1.
-
-`SELECT hasAll([1, Null], [Null])` retours 1.
-
-`SELECT hasAll([1.0, 2, 3, 4], [1, 3])` retours 1.
-
-`SELECT hasAll(['a', 'b'], ['a'])` retours 1.
-
-`SELECT hasAll([1], ['a'])` renvoie 0.
-
-`SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [3, 5]])` renvoie 0.
-
-## hasAny {#hasany}
-
-Vérifie si deux tableaux ont une intersection par certains éléments.
-
-``` sql
-hasAny(array1, array2)
-```
-
-**Paramètre**
-
--   `array1` – Array of any type with a set of elements.
--   `array2` – Array of any type with a set of elements.
-
-**Les valeurs de retour**
-
--   `1`, si `array1` et `array2` avoir un élément similaire au moins.
--   `0`, autrement.
-
-**Propriétés particulières**
-
--   `Null` traitée comme une valeur.
--   Ordre des valeurs dans les deux tableaux n'a pas d'importance.
-
-**Exemple**
-
-`SELECT hasAny([1], [])` retourner `0`.
-
-`SELECT hasAny([Null], [Null, 1])` retourner `1`.
-
-`SELECT hasAny([-128, 1., 512], [1])` retourner `1`.
-
-`SELECT hasAny([[1, 2], [3, 4]], ['a', 'c'])` retourner `0`.
-
-`SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [1, 2]])` retourner `1`.
-
-## indexOf (arr, x) {#indexofarr-x}
-
-Renvoie l'index de la première ‘x’ élément (à partir de 1) s'il est dans le tableau, ou 0 s'il ne l'est pas.
-
-Exemple:
-
-``` sql
-SELECT indexOf([1, 3, NULL, NULL], NULL)
-```
-
-``` text
-┌─indexOf([1, 3, NULL, NULL], NULL)─┐
-│                                 3 │
-└───────────────────────────────────┘
-```
-
-Ensemble d'éléments de `NULL` sont traités comme des valeurs normales.
-
-## countEqual (arr, x) {#countequalarr-x}
-
-Renvoie le nombre d'éléments dans le tableau égal à X. équivalent à arrayCount (elem - \> elem = x, arr).
-
-`NULL` les éléments sont traités comme des valeurs distinctes.
-
-Exemple:
-
-``` sql
-SELECT countEqual([1, 2, NULL, NULL], NULL)
-```
-
-``` text
-┌─countEqual([1, 2, NULL, NULL], NULL)─┐
-│                                    2 │
-└──────────────────────────────────────┘
-```
-
-## arrayEnumerate (arr) {#array_functions-arrayenumerate}
-
-Returns the array \[1, 2, 3, …, length (arr) \]
-
-Cette fonction est normalement utilisée avec ARRAY JOIN. Il permet de compter quelque chose une seule fois pour chaque tableau après l'application de la jointure de tableau. Exemple:
-
-``` sql
-SELECT
-    count() AS Reaches,
-    countIf(num = 1) AS Hits
-FROM test.hits
-ARRAY JOIN
-    GoalsReached,
-    arrayEnumerate(GoalsReached) AS num
-WHERE CounterID = 160656
-LIMIT 10
-```
-
-``` text
-┌─Reaches─┬──Hits─┐
-│   95606 │ 31406 │
-└─────────┴───────┘
-```
-
-Dans cet exemple, Reaches est le nombre de conversions (les chaînes reçues après l'application de la jointure de tableau), et Hits est le nombre de pages vues (chaînes avant la jointure de tableau). Dans ce cas particulier, vous pouvez obtenir le même résultat dans une voie plus facile:
-
-``` sql
-SELECT
-    sum(length(GoalsReached)) AS Reaches,
-    count() AS Hits
-FROM test.hits
-WHERE (CounterID = 160656) AND notEmpty(GoalsReached)
-```
-
-``` text
-┌─Reaches─┬──Hits─┐
-│   95606 │ 31406 │
-└─────────┴───────┘
-```
-
-Cette fonction peut également être utilisée dans les fonctions d'ordre supérieur. Par exemple, vous pouvez l'utiliser pour obtenir les indices de tableau pour les éléments qui correspondent à une condition.
-
-## arrayEnumerateUniq(arr, …) {#arrayenumerateuniqarr}
-
-Renvoie un tableau de la même taille que le tableau source, indiquant pour chaque élément Quelle est sa position parmi les éléments de même valeur.
-Par exemple: arrayEnumerateUniq(\[10, 20, 10, 30\]) = \[1, 1, 2, 1\].
-
-Cette fonction est utile lors de L'utilisation de la jointure de tableau et de l'agrégation d'éléments de tableau.
-Exemple:
-
-``` sql
-SELECT
-    Goals.ID AS GoalID,
-    sum(Sign) AS Reaches,
-    sumIf(Sign, num = 1) AS Visits
-FROM test.visits
-ARRAY JOIN
-    Goals,
-    arrayEnumerateUniq(Goals.ID) AS num
-WHERE CounterID = 160656
-GROUP BY GoalID
-ORDER BY Reaches DESC
-LIMIT 10
-```
-
-``` text
-┌──GoalID─┬─Reaches─┬─Visits─┐
-│   53225 │    3214 │   1097 │
-│ 2825062 │    3188 │   1097 │
-│   56600 │    2803 │    488 │
-│ 1989037 │    2401 │    365 │
-│ 2830064 │    2396 │    910 │
-│ 1113562 │    2372 │    373 │
-│ 3270895 │    2262 │    812 │
-│ 1084657 │    2262 │    345 │
-│   56599 │    2260 │    799 │
-│ 3271094 │    2256 │    812 │
-└─────────┴─────────┴────────┘
-```
-
-Dans cet exemple, chaque ID d'objectif a un calcul du nombre de conversions (chaque élément de la structure de données imbriquées objectifs est un objectif atteint, que nous appelons une conversion) et le nombre de sessions. Sans array JOIN, nous aurions compté le nombre de sessions comme sum(signe). Mais dans ce cas particulier, les lignes ont été multipliées par la structure des objectifs imbriqués, donc pour compter chaque session une fois après cela, nous appliquons une condition à la valeur de arrayEnumerateUniq(Goals.ID) fonction.
-
-La fonction arrayEnumerateUniq peut prendre plusieurs tableaux de la même taille que les arguments. Dans ce cas, l'unicité est considérée pour les tuples d'éléments dans les mêmes positions dans tous les tableaux.
-
-``` sql
-SELECT arrayEnumerateUniq([1, 1, 1, 2, 2, 2], [1, 1, 2, 1, 1, 2]) AS res
-```
-
-``` text
-┌─res───────────┐
-│ [1,2,1,1,2,1] │
-└───────────────┘
-```
-
-Ceci est nécessaire lors de L'utilisation de Array JOIN avec une structure de données imbriquée et une agrégation supplémentaire entre plusieurs éléments de cette structure.
-
-## arrayPopBack {#arraypopback}
-
-Supprime le dernier élément du tableau.
-
-``` sql
-arrayPopBack(array)
-```
-
-**Paramètre**
-
--   `array` – Array.
-
-**Exemple**
-
-``` sql
-SELECT arrayPopBack([1, 2, 3]) AS res
-```
-
-``` text
-┌─res───┐
-│ [1,2] │
-└───────┘
-```
-
-## arrayPopFront {#arraypopfront}
-
-Supprime le premier élément de la matrice.
-
-``` sql
-arrayPopFront(array)
-```
-
-**Paramètre**
-
--   `array` – Array.
-
-**Exemple**
-
-``` sql
-SELECT arrayPopFront([1, 2, 3]) AS res
-```
-
-``` text
-┌─res───┐
-│ [2,3] │
-└───────┘
-```
-
-## arrayPushBack {#arraypushback}
-
-Ajoute un élément à la fin du tableau.
-
-``` sql
-arrayPushBack(array, single_value)
-```
-
-**Paramètre**
-
--   `array` – Array.
--   `single_value` – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the `single_value` type pour le type de données du tableau. Pour plus d'informations sur les types de données dans ClickHouse, voir “[Types de données](../../sql-reference/data-types/index.md#data_types)”. Peut être `NULL`. La fonction ajoute un `NULL` tableau, et le type d'éléments de tableau convertit en `Nullable`.
-
-**Exemple**
-
-``` sql
-SELECT arrayPushBack(['a'], 'b') AS res
-```
-
-``` text
-┌─res───────┐
-│ ['a','b'] │
-└───────────┘
-```
-
-## arrayPushFront {#arraypushfront}
-
-Ajoute un élément au début du tableau.
-
-``` sql
-arrayPushFront(array, single_value)
-```
-
-**Paramètre**
-
--   `array` – Array.
--   `single_value` – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the `single_value` type pour le type de données du tableau. Pour plus d'informations sur les types de données dans ClickHouse, voir “[Types de données](../../sql-reference/data-types/index.md#data_types)”. Peut être `NULL`. La fonction ajoute un `NULL` tableau, et le type d'éléments de tableau convertit en `Nullable`.
-
-**Exemple**
-
-``` sql
-SELECT arrayPushFront(['b'], 'a') AS res
-```
-
-``` text
-┌─res───────┐
-│ ['a','b'] │
-└───────────┘
-```
-
-## arrayResize {#arrayresize}
-
-Les changements de la longueur du tableau.
-
-``` sql
-arrayResize(array, size[, extender])
-```
-
-**Paramètre:**
-
--   `array` — Array.
--   `size` — Required length of the array.
-    -   Si `size` est inférieure à la taille d'origine du tableau, le tableau est tronqué à partir de la droite.
--   Si `size` est plus grande que la taille initiale du tableau, le tableau est étendu vers la droite avec `extender` valeurs ou valeurs par défaut pour le type de données des éléments du tableau.
--   `extender` — Value for extending an array. Can be `NULL`.
-
-**Valeur renvoyée:**
-
-Un tableau de longueur `size`.
-
-**Exemples d'appels**
-
-``` sql
-SELECT arrayResize([1], 3)
-```
-
-``` text
-┌─arrayResize([1], 3)─┐
-│ [1,0,0]             │
-└─────────────────────┘
-```
-
-``` sql
-SELECT arrayResize([1], 3, NULL)
-```
-
-``` text
-┌─arrayResize([1], 3, NULL)─┐
-│ [1,NULL,NULL]             │
-└───────────────────────────┘
-```
-
-## arraySlice {#arrayslice}
-
-Retourne une tranche du tableau.
-
-``` sql
-arraySlice(array, offset[, length])
-```
-
-**Paramètre**
-
--   `array` – Array of data.
--   `offset` – Indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. Numbering of the array items begins with 1.
--   `length` - La longueur de la nécessaire tranche. Si vous spécifiez une valeur négative, la fonction renvoie un ouvert tranche `[offset, array_length - length)`. Si vous omettez la valeur, la fonction renvoie la tranche `[offset, the_end_of_array]`.
-
-**Exemple**
-
-``` sql
-SELECT arraySlice([1, 2, NULL, 4, 5], 2, 3) AS res
-```
-
-``` text
-┌─res────────┐
-│ [2,NULL,4] │
-└────────────┘
-```
-
-Éléments de tableau définis sur `NULL` sont traités comme des valeurs normales.
-
-## arraySort(\[func,\] arr, …) {#array_functions-sort}
-
-Trie les éléments de la `arr` tableau dans l'ordre croissant. Si l' `func` fonction est spécifiée, l'ordre de tri est déterminé par le résultat de la `func` fonction appliquée aux éléments du tableau. Si `func` accepte plusieurs arguments, le `arraySort` la fonction est passé plusieurs tableaux que les arguments de `func` correspond à. Des exemples détaillés sont présentés à la fin de `arraySort` Description.
-
-Exemple de tri de valeurs entières:
-
-``` sql
-SELECT arraySort([1, 3, 3, 0]);
-```
-
-``` text
-┌─arraySort([1, 3, 3, 0])─┐
-│ [0,1,3,3]               │
-└─────────────────────────┘
-```
-
-Exemple de tri des valeurs de chaîne:
-
-``` sql
-SELECT arraySort(['hello', 'world', '!']);
-```
-
-``` text
-┌─arraySort(['hello', 'world', '!'])─┐
-│ ['!','hello','world']              │
-└────────────────────────────────────┘
-```
-
-Considérez l'ordre de tri suivant pour le `NULL`, `NaN` et `Inf` valeur:
-
-``` sql
-SELECT arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]);
-```
-
-``` text
-┌─arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf])─┐
-│ [-inf,-4,1,2,3,inf,nan,nan,NULL,NULL]                     │
-└───────────────────────────────────────────────────────────┘
-```
-
--   `-Inf` les valeurs sont d'abord dans le tableau.
--   `NULL` les valeurs sont les derniers dans le tableau.
--   `NaN` les valeurs sont juste avant `NULL`.
--   `Inf` les valeurs sont juste avant `NaN`.
-
-Notez que `arraySort` est un [fonction d'ordre supérieur](higher-order-functions.md). Vous pouvez passer d'une fonction lambda comme premier argument. Dans ce cas, l'ordre de classement est déterminé par le résultat de la fonction lambda appliquée aux éléments de la matrice.
-
-Considérons l'exemple suivant:
-
-``` sql
-SELECT arraySort((x) -> -x, [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [3,2,1] │
-└─────────┘
-```
-
-For each element of the source array, the lambda function returns the sorting key, that is, \[1 –\> -1, 2 –\> -2, 3 –\> -3\]. Since the `arraySort` fonction trie les touches dans l'ordre croissant, le résultat est \[3, 2, 1\]. Ainsi, l' `(x) –> -x` fonction lambda définit le [l'ordre décroissant](#array_functions-reverse-sort) dans un tri.
-
-La fonction lambda peut accepter plusieurs arguments. Dans ce cas, vous avez besoin de passer l' `arraySort` fonction plusieurs tableaux de longueur identique à laquelle correspondront les arguments de la fonction lambda. Le tableau résultant sera composé d'éléments du premier tableau d'entrée; les éléments du(des) Tableau (s) d'entrée suivant (s) spécifient les clés de tri. Exemple:
-
-``` sql
-SELECT arraySort((x, y) -> y, ['hello', 'world'], [2, 1]) as res;
-```
-
-``` text
-┌─res────────────────┐
-│ ['world', 'hello'] │
-└────────────────────┘
-```
-
-Ici, les éléments qui sont passés dans le deuxième tableau (\[2, 1\]) définissent une clé de tri pour l'élément correspondant à partir du tableau source (\[‘hello’, ‘world’\]), qui est, \[‘hello’ –\> 2, ‘world’ –\> 1\]. Since the lambda function doesn't use `x`, les valeurs réelles du tableau source n'affectent pas l'ordre dans le résultat. Si, ‘hello’ sera le deuxième élément du résultat, et ‘world’ sera le premier.
-
-D'autres exemples sont présentés ci-dessous.
-
-``` sql
-SELECT arraySort((x, y) -> y, [0, 1, 2], ['c', 'b', 'a']) as res;
-```
-
-``` text
-┌─res─────┐
-│ [2,1,0] │
-└─────────┘
-```
-
-``` sql
-SELECT arraySort((x, y) -> -y, [0, 1, 2], [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [2,1,0] │
-└─────────┘
-```
-
-!!! note "Note"
-    Pour améliorer l'efficacité du tri, de la [Transformation schwartzienne](https://en.wikipedia.org/wiki/Schwartzian_transform) est utilisée.
-
-## arrayReverseSort(\[func,\] arr, …) {#array_functions-reverse-sort}
-
-Trie les éléments de la `arr` tableau dans l'ordre décroissant. Si l' `func` la fonction est spécifiée, `arr` est trié en fonction du résultat de la `func` fonction appliquée aux éléments du tableau, puis le tableau trié est inversé. Si `func` accepte plusieurs arguments, le `arrayReverseSort` la fonction est passé plusieurs tableaux que les arguments de `func` correspond à. Des exemples détaillés sont présentés à la fin de `arrayReverseSort` Description.
-
-Exemple de tri de valeurs entières:
-
-``` sql
-SELECT arrayReverseSort([1, 3, 3, 0]);
-```
-
-``` text
-┌─arrayReverseSort([1, 3, 3, 0])─┐
-│ [3,3,1,0]                      │
-└────────────────────────────────┘
-```
-
-Exemple de tri des valeurs de chaîne:
-
-``` sql
-SELECT arrayReverseSort(['hello', 'world', '!']);
-```
-
-``` text
-┌─arrayReverseSort(['hello', 'world', '!'])─┐
-│ ['world','hello','!']                     │
-└───────────────────────────────────────────┘
-```
-
-Considérez l'ordre de tri suivant pour le `NULL`, `NaN` et `Inf` valeur:
-
-``` sql
-SELECT arrayReverseSort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]) as res;
-```
-
-``` text
-┌─res───────────────────────────────────┐
-│ [inf,3,2,1,-4,-inf,nan,nan,NULL,NULL] │
-└───────────────────────────────────────┘
-```
-
--   `Inf` les valeurs sont d'abord dans le tableau.
--   `NULL` les valeurs sont les derniers dans le tableau.
--   `NaN` les valeurs sont juste avant `NULL`.
--   `-Inf` les valeurs sont juste avant `NaN`.
-
-Notez que l' `arrayReverseSort` est un [fonction d'ordre supérieur](higher-order-functions.md). Vous pouvez passer d'une fonction lambda comme premier argument. Exemple est montré ci-dessous.
-
-``` sql
-SELECT arrayReverseSort((x) -> -x, [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [1,2,3] │
-└─────────┘
-```
-
-Le tableau est trié de la façon suivante:
-
-1.  Dans un premier temps, le tableau source (\[1, 2, 3\]) est trié en fonction du résultat de la fonction lambda appliquée aux éléments du tableau. Le résultat est un tableau \[3, 2, 1\].
-2.  Tableau qui est obtenu à l'étape précédente, est renversé. Donc, le résultat final est \[1, 2, 3\].
-
-La fonction lambda peut accepter plusieurs arguments. Dans ce cas, vous avez besoin de passer l' `arrayReverseSort` fonction plusieurs tableaux de longueur identique à laquelle correspondront les arguments de la fonction lambda. Le tableau résultant sera composé d'éléments du premier tableau d'entrée; les éléments du(des) Tableau (s) d'entrée suivant (s) spécifient les clés de tri. Exemple:
-
-``` sql
-SELECT arrayReverseSort((x, y) -> y, ['hello', 'world'], [2, 1]) as res;
-```
-
-``` text
-┌─res───────────────┐
-│ ['hello','world'] │
-└───────────────────┘
-```
-
-Dans cet exemple, le tableau est trié de la façon suivante:
-
-1.  Au début, le tableau source (\[‘hello’, ‘world’\]) est triée selon le résultat de la fonction lambda appliquée aux éléments de tableaux. Les éléments qui sont passés dans le deuxième tableau (\[2, 1\]), définissent les clés de tri pour les éléments correspondants du tableau source. Le résultat est un tableau \[‘world’, ‘hello’\].
-2.  Tableau trié lors de l'étape précédente, est renversé. Donc, le résultat final est \[‘hello’, ‘world’\].
-
-D'autres exemples sont présentés ci-dessous.
-
-``` sql
-SELECT arrayReverseSort((x, y) -> y, [4, 3, 5], ['a', 'b', 'c']) AS res;
-```
-
-``` text
-┌─res─────┐
-│ [5,3,4] │
-└─────────┘
-```
-
-``` sql
-SELECT arrayReverseSort((x, y) -> -y, [4, 3, 5], [1, 2, 3]) AS res;
-```
-
-``` text
-┌─res─────┐
-│ [4,3,5] │
-└─────────┘
-```
-
-## arrayUniq(arr, …) {#arrayuniqarr}
-
-Si un argument est passé, il compte le nombre de différents éléments dans le tableau.
-Si plusieurs arguments sont passés, il compte le nombre de tuples différents d'éléments aux positions correspondantes dans plusieurs tableaux.
-
-Si vous souhaitez obtenir une liste des éléments dans un tableau, vous pouvez utiliser arrayReduce(‘groupUniqArray’, arr).
-
-## arrayJoin (arr) {#array-functions-join}
-
-Une fonction spéciale. Voir la section [“ArrayJoin function”](array-join.md#functions_arrayjoin).
-
-## tableaudifférence {#arraydifference}
-
-Calcule la différence entre les éléments de tableau adjacents. Renvoie un tableau où le premier élément sera 0, le second est la différence entre `a[1] - a[0]`, etc. The type of elements in the resulting array is determined by the type inference rules for subtraction (e.g. `UInt8` - `UInt8` = `Int16`).
-
-**Syntaxe**
-
-``` sql
-arrayDifference(array)
-```
-
-**Paramètre**
-
--   `array` – [Tableau](https://clickhouse.tech/docs/en/data_types/array/).
-
-**Valeurs renvoyées**
-
-Renvoie un tableau de différences entre les éléments adjacents.
-
-Type: [UInt\*](https://clickhouse.tech/docs/en/data_types/int_uint/#uint-ranges), [Int\*](https://clickhouse.tech/docs/en/data_types/int_uint/#int-ranges), [Flottant\*](https://clickhouse.tech/docs/en/data_types/float/).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT arrayDifference([1, 2, 3, 4])
-```
-
-Résultat:
-
-``` text
-┌─arrayDifference([1, 2, 3, 4])─┐
-│ [0,1,1,1]                     │
-└───────────────────────────────┘
-```
-
-Exemple de débordement dû au type de résultat Int64:
-
-Requête:
-
-``` sql
-SELECT arrayDifference([0, 10000000000000000000])
-```
-
-Résultat:
-
-``` text
-┌─arrayDifference([0, 10000000000000000000])─┐
-│ [0,-8446744073709551616]                   │
-└────────────────────────────────────────────┘
-```
-
-## arrayDistinct {#arraydistinct}
-
-Prend un tableau, retourne un tableau contenant les différents éléments seulement.
-
-**Syntaxe**
-
-``` sql
-arrayDistinct(array)
-```
-
-**Paramètre**
-
--   `array` – [Tableau](https://clickhouse.tech/docs/en/data_types/array/).
-
-**Valeurs renvoyées**
-
-Retourne un tableau contenant les éléments distincts.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT arrayDistinct([1, 2, 2, 3, 1])
-```
-
-Résultat:
-
-``` text
-┌─arrayDistinct([1, 2, 2, 3, 1])─┐
-│ [1,2,3]                        │
-└────────────────────────────────┘
-```
-
-## arrayEnumerateDense(arr) {#array_functions-arrayenumeratedense}
-
-Renvoie un tableau de la même taille que le tableau source, indiquant où chaque élément apparaît en premier dans le tableau source.
-
-Exemple:
-
-``` sql
-SELECT arrayEnumerateDense([10, 20, 10, 30])
-```
-
-``` text
-┌─arrayEnumerateDense([10, 20, 10, 30])─┐
-│ [1,2,1,3]                             │
-└───────────────────────────────────────┘
-```
-
-## arrayIntersect (arr) {#array-functions-arrayintersect}
-
-Prend plusieurs tableaux, retourne un tableau avec des éléments présents dans tous les tableaux source. L'ordre des éléments dans le tableau résultant est le même que dans le premier tableau.
-
-Exemple:
-
-``` sql
-SELECT
-    arrayIntersect([1, 2], [1, 3], [2, 3]) AS no_intersect,
-    arrayIntersect([1, 2], [1, 3], [1, 4]) AS intersect
-```
-
-``` text
-┌─no_intersect─┬─intersect─┐
-│ []           │ [1]       │
-└──────────────┴───────────┘
-```
-
-## arrayReduce {#arrayreduce}
-
-Applique une fonction d'agrégation aux éléments du tableau et renvoie son résultat. Le nom de la fonction d'agrégation est passé sous forme de chaîne entre guillemets simples `'max'`, `'sum'`. Lorsque vous utilisez des fonctions d'agrégat paramétriques, le paramètre est indiqué après le nom de la fonction entre parenthèses `'uniqUpTo(6)'`.
-
-**Syntaxe**
-
-``` sql
-arrayReduce(agg_func, arr1, arr2, ..., arrN)
-```
-
-**Paramètre**
-
--   `agg_func` — The name of an aggregate function which should be a constant [chaîne](../../sql-reference/data-types/string.md).
--   `arr` — Any number of [tableau](../../sql-reference/data-types/array.md) tapez les colonnes comme paramètres de la fonction d'agrégation.
-
-**Valeur renvoyée**
-
-**Exemple**
-
-``` sql
-SELECT arrayReduce('max', [1, 2, 3])
-```
-
-``` text
-┌─arrayReduce('max', [1, 2, 3])─┐
-│                             3 │
-└───────────────────────────────┘
-```
-
-Si une fonction d'agrégation prend plusieurs arguments, cette fonction doit être appliqué à plusieurs ensembles de même taille.
-
-``` sql
-SELECT arrayReduce('maxIf', [3, 5], [1, 0])
-```
-
-``` text
-┌─arrayReduce('maxIf', [3, 5], [1, 0])─┐
-│                                    3 │
-└──────────────────────────────────────┘
-```
-
-Exemple avec une fonction d'agrégat paramétrique:
-
-``` sql
-SELECT arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
-```
-
-``` text
-┌─arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])─┐
-│                                                           4 │
-└─────────────────────────────────────────────────────────────┘
-```
-
-## arrayReduceInRanges {#arrayreduceinranges}
-
-Applique une fonction d'agrégation d'éléments de tableau dans des plages et retourne un tableau contenant le résultat correspondant à chaque gamme. La fonction retourne le même résultat que plusieurs `arrayReduce(agg_func, arraySlice(arr1, index, length), ...)`.
-
-**Syntaxe**
-
-``` sql
-arrayReduceInRanges(agg_func, ranges, arr1, arr2, ..., arrN)
-```
-
-**Paramètre**
-
--   `agg_func` — The name of an aggregate function which should be a constant [chaîne](../../sql-reference/data-types/string.md).
--   `ranges` — The ranges to aggretate which should be an [tableau](../../sql-reference/data-types/array.md) de [tuple](../../sql-reference/data-types/tuple.md) qui contient l'indice et la longueur de chaque plage.
--   `arr` — Any number of [tableau](../../sql-reference/data-types/array.md) tapez les colonnes comme paramètres de la fonction d'agrégation.
-
-**Valeur renvoyée**
-
-**Exemple**
-
-``` sql
-SELECT arrayReduceInRanges(
-    'sum',
-    [(1, 5), (2, 3), (3, 4), (4, 4)],
-    [1000000, 200000, 30000, 4000, 500, 60, 7]
-) AS res
-```
-
-``` text
-┌─res─────────────────────────┐
-│ [1234500,234000,34560,4567] │
-└─────────────────────────────┘
-```
-
-## arrayReverse(arr) {#arrayreverse}
-
-Retourne un tableau de la même taille que l'original tableau contenant les éléments dans l'ordre inverse.
-
-Exemple:
-
-``` sql
-SELECT arrayReverse([1, 2, 3])
-```
-
-``` text
-┌─arrayReverse([1, 2, 3])─┐
-│ [3,2,1]                 │
-└─────────────────────────┘
-```
-
-## inverse (arr) {#array-functions-reverse}
-
-Synonyme de [“arrayReverse”](#arrayreverse)
-
-## arrayFlatten {#arrayflatten}
-
-Convertit un tableau de tableaux dans un tableau associatif.
-
-Fonction:
-
--   S'applique à toute profondeur de tableaux imbriqués.
--   Ne change pas les tableaux qui sont déjà plats.
-
-Le tableau aplati contient tous les éléments de tous les tableaux source.
-
-**Syntaxe**
-
-``` sql
-flatten(array_of_arrays)
-```
-
-Alias: `flatten`.
-
-**Paramètre**
-
--   `array_of_arrays` — [Tableau](../../sql-reference/data-types/array.md) de tableaux. Exemple, `[[1,2,3], [4,5]]`.
-
-**Exemple**
-
-``` sql
-SELECT flatten([[[1]], [[2], [3]]])
-```
-
-``` text
-┌─flatten(array(array([1]), array([2], [3])))─┐
-│ [1,2,3]                                     │
-└─────────────────────────────────────────────┘
-```
-
-## arrayCompact {#arraycompact}
-
-Supprime les éléments en double consécutifs d'un tableau. L'ordre des valeurs de résultat est déterminée par l'ordre dans le tableau source.
-
-**Syntaxe**
-
-``` sql
-arrayCompact(arr)
-```
-
-**Paramètre**
-
-`arr` — The [tableau](../../sql-reference/data-types/array.md) inspecter.
-
-**Valeur renvoyée**
-
-Le tableau sans doublon.
-
-Type: `Array`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT arrayCompact([1, 1, nan, nan, 2, 3, 3, 3])
-```
-
-Résultat:
-
-``` text
-┌─arrayCompact([1, 1, nan, nan, 2, 3, 3, 3])─┐
-│ [1,nan,nan,2,3]                            │
-└────────────────────────────────────────────┘
-```
-
-## arrayZip {#arrayzip}
-
-Combine plusieurs tableaux en un seul tableau. Le tableau résultant contient les éléments correspondants des tableaux source regroupés en tuples dans l'ordre des arguments listés.
-
-**Syntaxe**
-
-``` sql
-arrayZip(arr1, arr2, ..., arrN)
-```
-
-**Paramètre**
-
--   `arrN` — [Tableau](../data-types/array.md).
-
-La fonction peut prendre n'importe quel nombre de tableaux de différents types. Tous les tableaux doivent être de taille égale.
-
-**Valeur renvoyée**
-
--   Tableau avec des éléments des tableaux source regroupés en [tuple](../data-types/tuple.md). Types de données dans le tuple sont les mêmes que les types de l'entrée des tableaux et dans le même ordre que les tableaux sont passés.
-
-Type: [Tableau](../data-types/array.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT arrayZip(['a', 'b', 'c'], [5, 2, 1])
-```
-
-Résultat:
-
-``` text
-┌─arrayZip(['a', 'b', 'c'], [5, 2, 1])─┐
-│ [('a',5),('b',2),('c',1)]            │
-└──────────────────────────────────────┘
-```
-
-## arrayAUC {#arrayauc}
-
-Calculer AUC (zone sous la courbe, qui est un concept dans l'apprentissage automatique, voir plus de détails: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).
-
-**Syntaxe**
-
-``` sql
-arrayAUC(arr_scores, arr_labels)
-```
-
-**Paramètre**
-- `arr_scores` — scores prediction model gives.
-- `arr_labels` — labels of samples, usually 1 for positive sample and 0 for negtive sample.
-
-**Valeur renvoyée**
-Renvoie la valeur AUC avec le type Float64.
-
-**Exemple**
-Requête:
-
-``` sql
-select arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1])
-```
-
-Résultat:
-
-``` text
-┌─arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1])─┐
-│                                          0.75 │
-└────────────────────────────────────────---──┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/array_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/array-join.md b/docs/fr/sql-reference/functions/array-join.md
deleted file mode 100644
index 859e801994df..000000000000
--- a/docs/fr/sql-reference/functions/array-join.md
+++ /dev/null
@@ -1,37 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 61
-toc_title: arrayJoin
----
-
-# fonction arrayJoin {#functions_arrayjoin}
-
-C'est un très inhabituelle de la fonction.
-
-Les fonctions normales ne modifient pas un ensemble de lignes, mais modifient simplement les valeurs de chaque ligne (map).
-Les fonctions d'agrégation compriment un ensemble de lignes (plier ou réduire).
-Le ‘arrayJoin’ la fonction prend chaque ligne et génère un ensemble de lignes (dépliante).
-
-Cette fonction prend un tableau comme argument et propage la ligne source à plusieurs lignes pour le nombre d'éléments dans le tableau.
-Toutes les valeurs des colonnes sont simplement copiés, sauf les valeurs dans la colonne où cette fonction est appliquée; elle est remplacée par la valeur correspondante de tableau.
-
-Une requête peut utiliser plusieurs `arrayJoin` fonction. Dans ce cas, la transformation est effectuée plusieurs fois.
-
-Notez la syntaxe de jointure de tableau dans la requête SELECT, qui offre des possibilités plus larges.
-
-Exemple:
-
-``` sql
-SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src
-```
-
-``` text
-┌─dst─┬─\'Hello\'─┬─src─────┐
-│   1 │ Hello     │ [1,2,3] │
-│   2 │ Hello     │ [1,2,3] │
-│   3 │ Hello     │ [1,2,3] │
-└─────┴───────────┴─────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/array_join/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/bit-functions.md b/docs/fr/sql-reference/functions/bit-functions.md
deleted file mode 100644
index 7b8795815f2c..000000000000
--- a/docs/fr/sql-reference/functions/bit-functions.md
+++ /dev/null
@@ -1,255 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 48
-toc_title: Bit
----
-
-# Peu De Fonctions {#bit-functions}
-
-Les fonctions Bit fonctionnent pour n'importe quelle paire de types de UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64, Float32 ou Float64.
-
-Le type de résultat est un entier avec des bits égaux aux bits maximum de ses arguments. Si au moins l'un des arguments est signé, le résultat est un signé nombre. Si un argument est un nombre à virgule flottante, Il est converti en Int64.
-
-## bitAnd (a, b) {#bitanda-b}
-
-## bitOr (a, b) {#bitora-b}
-
-## bitXor (a, b) {#bitxora-b}
-
-## bitNot (a) {#bitnota}
-
-## bitShiftLeft (A, b) {#bitshiftlefta-b}
-
-## bitShiftRight (A, b) {#bitshiftrighta-b}
-
-## bitRotateLeft (a, b) {#bitrotatelefta-b}
-
-## bitRotateRight (a, b) {#bitrotaterighta-b}
-
-## bitTest {#bittest}
-
-Prend tout entier et le convertit en [forme binaire](https://en.wikipedia.org/wiki/Binary_number) renvoie la valeur d'un bit à la position spécifiée. Le compte à rebours commence à partir de 0 de la droite vers la gauche.
-
-**Syntaxe**
-
-``` sql
-SELECT bitTest(number, index)
-```
-
-**Paramètre**
-
--   `number` – integer number.
--   `index` – position of bit.
-
-**Valeurs renvoyées**
-
-Renvoie une valeur de bit à la position spécifiée.
-
-Type: `UInt8`.
-
-**Exemple**
-
-Par exemple, le nombre 43 dans le système numérique de base-2 (binaire) est 101011.
-
-Requête:
-
-``` sql
-SELECT bitTest(43, 1)
-```
-
-Résultat:
-
-``` text
-┌─bitTest(43, 1)─┐
-│              1 │
-└────────────────┘
-```
-
-Un autre exemple:
-
-Requête:
-
-``` sql
-SELECT bitTest(43, 2)
-```
-
-Résultat:
-
-``` text
-┌─bitTest(43, 2)─┐
-│              0 │
-└────────────────┘
-```
-
-## bitTestAll {#bittestall}
-
-Renvoie le résultat de [logique de conjonction](https://en.wikipedia.org/wiki/Logical_conjunction) (Et opérateur) de tous les bits à des positions données. Le compte à rebours commence à partir de 0 de la droite vers la gauche.
-
-La conjonction pour les opérations bit à bit:
-
-0 AND 0 = 0
-
-0 AND 1 = 0
-
-1 AND 0 = 0
-
-1 AND 1 = 1
-
-**Syntaxe**
-
-``` sql
-SELECT bitTestAll(number, index1, index2, index3, index4, ...)
-```
-
-**Paramètre**
-
--   `number` – integer number.
--   `index1`, `index2`, `index3`, `index4` – positions of bit. For example, for set of positions (`index1`, `index2`, `index3`, `index4`) est vrai si et seulement si toutes ses positions sont remplies (`index1` ⋀ `index2`, ⋀ `index3` ⋀ `index4`).
-
-**Valeurs renvoyées**
-
-Retourne le résultat de la conjonction logique.
-
-Type: `UInt8`.
-
-**Exemple**
-
-Par exemple, le nombre 43 dans le système numérique de base-2 (binaire) est 101011.
-
-Requête:
-
-``` sql
-SELECT bitTestAll(43, 0, 1, 3, 5)
-```
-
-Résultat:
-
-``` text
-┌─bitTestAll(43, 0, 1, 3, 5)─┐
-│                          1 │
-└────────────────────────────┘
-```
-
-Un autre exemple:
-
-Requête:
-
-``` sql
-SELECT bitTestAll(43, 0, 1, 3, 5, 2)
-```
-
-Résultat:
-
-``` text
-┌─bitTestAll(43, 0, 1, 3, 5, 2)─┐
-│                             0 │
-└───────────────────────────────┘
-```
-
-## bitTestAny {#bittestany}
-
-Renvoie le résultat de [disjonction logique](https://en.wikipedia.org/wiki/Logical_disjunction) (Ou opérateur) de tous les bits à des positions données. Le compte à rebours commence à partir de 0 de la droite vers la gauche.
-
-La disjonction pour les opérations binaires:
-
-0 OR 0 = 0
-
-0 OR 1 = 1
-
-1 OR 0 = 1
-
-1 OR 1 = 1
-
-**Syntaxe**
-
-``` sql
-SELECT bitTestAny(number, index1, index2, index3, index4, ...)
-```
-
-**Paramètre**
-
--   `number` – integer number.
--   `index1`, `index2`, `index3`, `index4` – positions of bit.
-
-**Valeurs renvoyées**
-
-Renvoie le résultat de la disjuction logique.
-
-Type: `UInt8`.
-
-**Exemple**
-
-Par exemple, le nombre 43 dans le système numérique de base-2 (binaire) est 101011.
-
-Requête:
-
-``` sql
-SELECT bitTestAny(43, 0, 2)
-```
-
-Résultat:
-
-``` text
-┌─bitTestAny(43, 0, 2)─┐
-│                    1 │
-└──────────────────────┘
-```
-
-Un autre exemple:
-
-Requête:
-
-``` sql
-SELECT bitTestAny(43, 4, 2)
-```
-
-Résultat:
-
-``` text
-┌─bitTestAny(43, 4, 2)─┐
-│                    0 │
-└──────────────────────┘
-```
-
-## bitCount {#bitcount}
-
-Calcule le nombre de bits mis à un dans la représentation binaire d'un nombre.
-
-**Syntaxe**
-
-``` sql
-bitCount(x)
-```
-
-**Paramètre**
-
--   `x` — [Entier](../../sql-reference/data-types/int-uint.md) ou [virgule flottante](../../sql-reference/data-types/float.md) nombre. La fonction utilise la représentation de la valeur en mémoire. Il permet de financer les nombres à virgule flottante.
-
-**Valeur renvoyée**
-
--   Nombre de bits défini sur un dans le numéro d'entrée.
-
-La fonction ne convertit pas la valeur d'entrée en un type plus grand ([l'extension du signe](https://en.wikipedia.org/wiki/Sign_extension)). Ainsi, par exemple, `bitCount(toUInt8(-1)) = 8`.
-
-Type: `UInt8`.
-
-**Exemple**
-
-Prenez par exemple le numéro 333. Sa représentation binaire: 0000000101001101.
-
-Requête:
-
-``` sql
-SELECT bitCount(333)
-```
-
-Résultat:
-
-``` text
-┌─bitCount(333)─┐
-│             5 │
-└───────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/bit_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/bitmap-functions.md b/docs/fr/sql-reference/functions/bitmap-functions.md
deleted file mode 100644
index 15cb68ffc52c..000000000000
--- a/docs/fr/sql-reference/functions/bitmap-functions.md
+++ /dev/null
@@ -1,496 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 49
-toc_title: Bitmap
----
-
-# Fonctions De Bitmap {#bitmap-functions}
-
-Les fonctions Bitmap fonctionnent pour le calcul de la valeur de L'objet de deux bitmaps, il s'agit de renvoyer un nouveau bitmap ou une cardinalité tout en utilisant le calcul de la formule, tel que and, or, xor, and not, etc.
-
-Il existe 2 types de méthodes de construction pour L'objet Bitmap. L'un doit être construit par la fonction d'agrégation groupBitmap avec-State, l'autre doit être construit par L'objet Array. Il est également de convertir L'objet Bitmap en objet tableau.
-
-RoaringBitmap est enveloppé dans une structure de données pendant le stockage réel des objets Bitmap. Lorsque la cardinalité est inférieure ou égale à 32, elle utilise Set objet. Lorsque la cardinalité est supérieure à 32, elle utilise l'objet RoaringBitmap. C'est pourquoi le stockage de faible cardinalité jeu est plus rapide.
-
-Pour plus d'informations sur RoaringBitmap, voir: [CRoaring](https://github.com/RoaringBitmap/CRoaring).
-
-## bitmapBuild {#bitmap_functions-bitmapbuild}
-
-Construire un bitmap à partir d'un tableau entier non signé.
-
-``` sql
-bitmapBuild(array)
-```
-
-**Paramètre**
-
--   `array` – unsigned integer array.
-
-**Exemple**
-
-``` sql
-SELECT bitmapBuild([1, 2, 3, 4, 5]) AS res, toTypeName(res)
-```
-
-``` text
-┌─res─┬─toTypeName(bitmapBuild([1, 2, 3, 4, 5]))─────┐
-│     │ AggregateFunction(groupBitmap, UInt8)    │
-└─────┴──────────────────────────────────────────────┘
-```
-
-## bitmapToArray {#bitmaptoarray}
-
-Convertir bitmap en tableau entier.
-
-``` sql
-bitmapToArray(bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapToArray(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-``` text
-┌─res─────────┐
-│ [1,2,3,4,5] │
-└─────────────┘
-```
-
-## bitmapSubsetInRange {#bitmap-functions-bitmapsubsetinrange}
-
-Retourne le sous-ensemble dans la plage spécifiée (n'inclut pas le range_end).
-
-``` sql
-bitmapSubsetInRange(bitmap, range_start, range_end)
-```
-
-**Paramètre**
-
--   `bitmap` – [Objet Bitmap](#bitmap_functions-bitmapbuild).
--   `range_start` – range start point. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
--   `range_end` – range end point(excluded). Type: [UInt32](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-``` sql
-SELECT bitmapToArray(bitmapSubsetInRange(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res
-```
-
-``` text
-┌─res───────────────┐
-│ [30,31,32,33,100] │
-└───────────────────┘
-```
-
-## bitmapSubsetLimit {#bitmapsubsetlimit}
-
-Crée un sous-ensemble de bitmap avec n éléments pris entre `range_start` et `cardinality_limit`.
-
-**Syntaxe**
-
-``` sql
-bitmapSubsetLimit(bitmap, range_start, cardinality_limit)
-```
-
-**Paramètre**
-
--   `bitmap` – [Objet Bitmap](#bitmap_functions-bitmapbuild).
--   `range_start` – The subset starting point. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
--   `cardinality_limit` – The subset cardinality upper limit. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
-
-**Valeur renvoyée**
-
-Ensemble.
-
-Type: `Bitmap object`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT bitmapToArray(bitmapSubsetLimit(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res
-```
-
-Résultat:
-
-``` text
-┌─res───────────────────────┐
-│ [30,31,32,33,100,200,500] │
-└───────────────────────────┘
-```
-
-## bitmapContains {#bitmap_functions-bitmapcontains}
-
-Vérifie si le bitmap contient un élément.
-
-``` sql
-bitmapContains(haystack, needle)
-```
-
-**Paramètre**
-
--   `haystack` – [Objet Bitmap](#bitmap_functions-bitmapbuild) où la fonction recherche.
--   `needle` – Value that the function searches. Type: [UInt32](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   0 — If `haystack` ne contient pas de `needle`.
--   1 — If `haystack` contenir `needle`.
-
-Type: `UInt8`.
-
-**Exemple**
-
-``` sql
-SELECT bitmapContains(bitmapBuild([1,5,7,9]), toUInt32(9)) AS res
-```
-
-``` text
-┌─res─┐
-│  1  │
-└─────┘
-```
-
-## bitmapHasAny {#bitmaphasany}
-
-Vérifie si deux bitmaps ont une intersection par certains éléments.
-
-``` sql
-bitmapHasAny(bitmap1, bitmap2)
-```
-
-Si vous êtes sûr que `bitmap2` contient strictement un élément, envisagez d'utiliser le [bitmapContains](#bitmap_functions-bitmapcontains) fonction. Cela fonctionne plus efficacement.
-
-**Paramètre**
-
--   `bitmap*` – bitmap object.
-
-**Les valeurs de retour**
-
--   `1`, si `bitmap1` et `bitmap2` avoir un élément similaire au moins.
--   `0`, autrement.
-
-**Exemple**
-
-``` sql
-SELECT bitmapHasAny(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res
-```
-
-``` text
-┌─res─┐
-│  1  │
-└─────┘
-```
-
-## bitmapHasAll {#bitmaphasall}
-
-Analogue à `hasAll(array, array)` renvoie 1 si le premier bitmap contient tous les éléments du second, 0 sinon.
-Si le deuxième argument est un bitmap vide, alors renvoie 1.
-
-``` sql
-bitmapHasAll(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapHasAll(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res
-```
-
-``` text
-┌─res─┐
-│  0  │
-└─────┘
-```
-
-## bitmapCardinality {#bitmapcardinality}
-
-Retrun bitmap cardinalité de type UInt64.
-
-``` sql
-bitmapCardinality(bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapCardinality(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-``` text
-┌─res─┐
-│   5 │
-└─────┘
-```
-
-## bitmapMin {#bitmapmin}
-
-Retrun la plus petite valeur de type UInt64 dans l'ensemble, UINT32_MAX si l'ensemble est vide.
-
-    bitmapMin(bitmap)
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapMin(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-    ┌─res─┐
-    │   1 │
-    └─────┘
-
-## bitmapMax {#bitmapmax}
-
-Retrun la plus grande valeur de type UInt64 dans l'ensemble, 0 si l'ensemble est vide.
-
-    bitmapMax(bitmap)
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapMax(bitmapBuild([1, 2, 3, 4, 5])) AS res
-```
-
-    ┌─res─┐
-    │   5 │
-    └─────┘
-
-## bitmapTransform {#bitmaptransform}
-
-Transformer un tableau de valeurs d'une image à l'autre tableau de valeurs, le résultat est une nouvelle image.
-
-    bitmapTransform(bitmap, from_array, to_array)
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
--   `from_array` – UInt32 array. For idx in range \[0, from_array.size()), if bitmap contains from_array\[idx\], then replace it with to_array\[idx\]. Note that the result depends on array ordering if there are common elements between from_array and to_array.
--   `to_array` – UInt32 array, its size shall be the same to from_array.
-
-**Exemple**
-
-``` sql
-SELECT bitmapToArray(bitmapTransform(bitmapBuild([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), cast([5,999,2] as Array(UInt32)), cast([2,888,20] as Array(UInt32)))) AS res
-```
-
-    ┌─res───────────────────┐
-    │ [1,3,4,6,7,8,9,10,20] │
-    └───────────────────────┘
-
-## bitmapAnd {#bitmapand}
-
-Deux bitmap et calcul, le résultat est un nouveau bitmap.
-
-``` sql
-bitmapAnd(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapToArray(bitmapAnd(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res─┐
-│ [3] │
-└─────┘
-```
-
-## bitmapOr {#bitmapor}
-
-Deux bitmap ou calcul, le résultat est un nouveau bitmap.
-
-``` sql
-bitmapOr(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapToArray(bitmapOr(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res─────────┐
-│ [1,2,3,4,5] │
-└─────────────┘
-```
-
-## bitmapXor {#bitmapxor}
-
-Deux bitmap xor calcul, le résultat est une nouvelle image.
-
-``` sql
-bitmapXor(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapToArray(bitmapXor(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res───────┐
-│ [1,2,4,5] │
-└───────────┘
-```
-
-## bitmapetnot {#bitmapandnot}
-
-Deux Bitmap andnot calcul, le résultat est un nouveau bitmap.
-
-``` sql
-bitmapAndnot(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapToArray(bitmapAndnot(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res
-```
-
-``` text
-┌─res───┐
-│ [1,2] │
-└───────┘
-```
-
-## bitmapetcardinalité {#bitmapandcardinality}
-
-Deux bitmap et calcul, retour cardinalité de type UInt64.
-
-``` sql
-bitmapAndCardinality(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapAndCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   1 │
-└─────┘
-```
-
-## bitmapOrCardinality {#bitmaporcardinality}
-
-Deux bitmap ou calcul, retour cardinalité de type UInt64.
-
-``` sql
-bitmapOrCardinality(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapOrCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   5 │
-└─────┘
-```
-
-## bitmapXorCardinality {#bitmapxorcardinality}
-
-Deux bitmap XOR calcul, retour cardinalité de type UInt64.
-
-``` sql
-bitmapXorCardinality(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapXorCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   4 │
-└─────┘
-```
-
-## bitmapetnotcardinality {#bitmapandnotcardinality}
-
-Deux bitmap andnot calcul, retour cardinalité de type UInt64.
-
-``` sql
-bitmapAndnotCardinality(bitmap,bitmap)
-```
-
-**Paramètre**
-
--   `bitmap` – bitmap object.
-
-**Exemple**
-
-``` sql
-SELECT bitmapAndnotCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;
-```
-
-``` text
-┌─res─┐
-│   2 │
-└─────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/bitmap_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/comparison-functions.md b/docs/fr/sql-reference/functions/comparison-functions.md
deleted file mode 100644
index a5008c676fa0..000000000000
--- a/docs/fr/sql-reference/functions/comparison-functions.md
+++ /dev/null
@@ -1,37 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: Comparaison
----
-
-# Fonctions De Comparaison {#comparison-functions}
-
-Les fonctions de comparaison renvoient toujours 0 ou 1 (Uint8).
-
-Les types suivants peuvent être comparés:
-
--   nombre
--   cordes et cordes fixes
--   date
--   dates avec heures
-
-au sein de chaque groupe, mais pas entre différents groupes.
-
-Par exemple, vous ne pouvez pas comparer une date avec une chaîne. Vous devez utiliser une fonction pour convertir la chaîne en une date, ou vice versa.
-
-Les chaînes sont comparées par octets. Une courte chaîne est plus petite que toutes les chaînes qui commencent par elle et qui contiennent au moins un caractère de plus.
-
-## égal, A = B et a = = b opérateur {#function-equals}
-
-## notEquals, a ! opérateur= b et a \<\> b {#function-notequals}
-
-## moins, opérateur \<  {#function-less}
-
-## de plus, \> opérateur {#function-greater}
-
-## lessOrEquals, \< = opérateur {#function-lessorequals}
-
-## greaterOrEquals, \> = opérateur {#function-greaterorequals}
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/comparison_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/conditional-functions.md b/docs/fr/sql-reference/functions/conditional-functions.md
deleted file mode 100644
index 3912b49aa6a1..000000000000
--- a/docs/fr/sql-reference/functions/conditional-functions.md
+++ /dev/null
@@ -1,207 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: 'Conditionnel '
----
-
-# Fonctions Conditionnelles {#conditional-functions}
-
-## si {#if}
-
-Contrôle la ramification conditionnelle. Contrairement à la plupart des systèmes, ClickHouse évalue toujours les deux expressions `then` et `else`.
-
-**Syntaxe**
-
-``` sql
-SELECT if(cond, then, else)
-```
-
-Si la condition `cond` renvoie une valeur non nulle, retourne le résultat de l'expression `then` et le résultat de l'expression `else`, si présent, est ignoré. Si l' `cond` est égal à zéro ou `NULL` alors le résultat de la `then` l'expression est ignorée et le résultat de `else` expression, si elle est présente, est renvoyée.
-
-**Paramètre**
-
--   `cond` – The condition for evaluation that can be zero or not. The type is UInt8, Nullable(UInt8) or NULL.
--   `then` - L'expression à renvoyer si la condition est remplie.
--   `else` - L'expression à renvoyer si la condition n'est pas remplie.
-
-**Valeurs renvoyées**
-
-La fonction s'exécute `then` et `else` expressions et retourne son résultat, selon que la condition `cond` fini par être zéro ou pas.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT if(1, plus(2, 2), plus(2, 6))
-```
-
-Résultat:
-
-``` text
-┌─plus(2, 2)─┐
-│          4 │
-└────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT if(0, plus(2, 2), plus(2, 6))
-```
-
-Résultat:
-
-``` text
-┌─plus(2, 6)─┐
-│          8 │
-└────────────┘
-```
-
--   `then` et `else` doit avoir le type commun le plus bas.
-
-**Exemple:**
-
-Prendre cette `LEFT_RIGHT` table:
-
-``` sql
-SELECT *
-FROM LEFT_RIGHT
-
-┌─left─┬─right─┐
-│ ᴺᵁᴸᴸ │     4 │
-│    1 │     3 │
-│    2 │     2 │
-│    3 │     1 │
-│    4 │  ᴺᵁᴸᴸ │
-└──────┴───────┘
-```
-
-La requête suivante compare `left` et `right` valeur:
-
-``` sql
-SELECT
-    left,
-    right,
-    if(left < right, 'left is smaller than right', 'right is greater or equal than left') AS is_smaller
-FROM LEFT_RIGHT
-WHERE isNotNull(left) AND isNotNull(right)
-
-┌─left─┬─right─┬─is_smaller──────────────────────────┐
-│    1 │     3 │ left is smaller than right          │
-│    2 │     2 │ right is greater or equal than left │
-│    3 │     1 │ right is greater or equal than left │
-└──────┴───────┴─────────────────────────────────────┘
-```
-
-Note: `NULL` les valeurs ne sont pas utilisés dans cet exemple, vérifier [Valeurs nulles dans les conditions](#null-values-in-conditionals) section.
-
-## Opérateur Ternaire {#ternary-operator}
-
-Il fonctionne même comme `if` fonction.
-
-Syntaxe: `cond ? then : else`
-
-Retourner `then` si l' `cond` renvoie la valeur vrai (supérieur à zéro), sinon renvoie `else`.
-
--   `cond` doit être de type de `UInt8`, et `then` et `else` doit avoir le type commun le plus bas.
-
--   `then` et `else` peut être `NULL`
-
-**Voir aussi**
-
--   [ifNotFinite](other-functions.md#ifnotfinite).
-
-## multiIf {#multiif}
-
-Permet d'écrire le [CASE](../operators/index.md#operator_case) opérateur plus compacte dans la requête.
-
-Syntaxe: `multiIf(cond_1, then_1, cond_2, then_2, ..., else)`
-
-**Paramètre:**
-
--   `cond_N` — The condition for the function to return `then_N`.
--   `then_N` — The result of the function when executed.
--   `else` — The result of the function if none of the conditions is met.
-
-La fonction accepte `2N+1` paramètre.
-
-**Valeurs renvoyées**
-
-La fonction renvoie l'une des valeurs `then_N` ou `else` selon les conditions `cond_N`.
-
-**Exemple**
-
-En utilisant à nouveau `LEFT_RIGHT` table.
-
-``` sql
-SELECT
-    left,
-    right,
-    multiIf(left < right, 'left is smaller', left > right, 'left is greater', left = right, 'Both equal', 'Null value') AS result
-FROM LEFT_RIGHT
-
-┌─left─┬─right─┬─result──────────┐
-│ ᴺᵁᴸᴸ │     4 │ Null value      │
-│    1 │     3 │ left is smaller │
-│    2 │     2 │ Both equal      │
-│    3 │     1 │ left is greater │
-│    4 │  ᴺᵁᴸᴸ │ Null value      │
-└──────┴───────┴─────────────────┘
-```
-
-## Utilisation Directe Des Résultats Conditionnels {#using-conditional-results-directly}
-
-Les conditions entraînent toujours `0`, `1` ou `NULL`. Vous pouvez donc utiliser des résultats conditionnels directement comme ceci:
-
-``` sql
-SELECT left < right AS is_small
-FROM LEFT_RIGHT
-
-┌─is_small─┐
-│     ᴺᵁᴸᴸ │
-│        1 │
-│        0 │
-│        0 │
-│     ᴺᵁᴸᴸ │
-└──────────┘
-```
-
-## Valeurs nulles dans les conditions {#null-values-in-conditionals}
-
-Lorsque `NULL` les valeurs sont impliqués dans des conditions, le résultat sera également `NULL`.
-
-``` sql
-SELECT
-    NULL < 1,
-    2 < NULL,
-    NULL < NULL,
-    NULL = NULL
-
-┌─less(NULL, 1)─┬─less(2, NULL)─┬─less(NULL, NULL)─┬─equals(NULL, NULL)─┐
-│ ᴺᵁᴸᴸ          │ ᴺᵁᴸᴸ          │ ᴺᵁᴸᴸ             │ ᴺᵁᴸᴸ               │
-└───────────────┴───────────────┴──────────────────┴────────────────────┘
-```
-
-Donc, vous devriez construire vos requêtes avec soin si les types sont `Nullable`.
-
-L'exemple suivant le démontre en omettant d'ajouter la condition égale à `multiIf`.
-
-``` sql
-SELECT
-    left,
-    right,
-    multiIf(left < right, 'left is smaller', left > right, 'right is smaller', 'Both equal') AS faulty_result
-FROM LEFT_RIGHT
-
-┌─left─┬─right─┬─faulty_result────┐
-│ ᴺᵁᴸᴸ │     4 │ Both equal       │
-│    1 │     3 │ left is smaller  │
-│    2 │     2 │ Both equal       │
-│    3 │     1 │ right is smaller │
-│    4 │  ᴺᵁᴸᴸ │ Both equal       │
-└──────┴───────┴──────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/conditional_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/date-time-functions.md b/docs/fr/sql-reference/functions/date-time-functions.md
deleted file mode 100644
index d1c16b42d07b..000000000000
--- a/docs/fr/sql-reference/functions/date-time-functions.md
+++ /dev/null
@@ -1,450 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: Travailler avec les Dates et les heures
----
-
-# Fonctions pour travailler avec des Dates et des heures {#functions-for-working-with-dates-and-times}
-
-Support des fuseaux horaires
-
-Toutes les fonctions pour travailler avec la date et l'heure qui ont une logique d'utilisation pour le fuseau horaire peut accepter un second fuseau horaire argument. Exemple: Asie / Ekaterinbourg. Dans ce cas, ils utilisent le fuseau horaire spécifié au lieu du fuseau horaire local (par défaut).
-
-``` sql
-SELECT
-    toDateTime('2016-06-15 23:00:00') AS time,
-    toDate(time) AS date_local,
-    toDate(time, 'Asia/Yekaterinburg') AS date_yekat,
-    toString(time, 'US/Samoa') AS time_samoa
-```
-
-``` text
-┌────────────────time─┬─date_local─┬─date_yekat─┬─time_samoa──────────┐
-│ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-16 │ 2016-06-15 09:00:00 │
-└─────────────────────┴────────────┴────────────┴─────────────────────┘
-```
-
-Seuls les fuseaux horaires qui diffèrent de L'UTC par un nombre entier d'heures sont pris en charge.
-
-## toTimeZone {#totimezone}
-
-Convertir l'heure ou la date et de l'heure au fuseau horaire spécifié.
-
-## toYear {#toyear}
-
-Convertit une date ou une date avec l'heure en un numéro UInt16 contenant le numéro d'année (AD).
-
-## toQuarter {#toquarter}
-
-Convertit une date ou une date avec l'heure en un numéro UInt8 contenant le numéro de trimestre.
-
-## toMonth {#tomonth}
-
-Convertit une date ou une date avec l'heure en un numéro UInt8 contenant le numéro de mois (1-12).
-
-## toDayOfYear {#todayofyear}
-
-Convertit une date ou une date avec l'heure en un numéro UInt16 contenant le numéro du jour de l'année (1-366).
-
-## toDayOfMonth {#todayofmonth}
-
-Convertit une date ou une date avec le temps à un UInt8 contenant le numéro du jour du mois (1-31).
-
-## toDayOfWeek {#todayofweek}
-
-Convertit une date ou une date avec l'heure en un numéro UInt8 contenant le numéro du jour de la semaine (lundi est 1, et dimanche est 7).
-
-## toHour {#tohour}
-
-Convertit une date avec l'heure en un nombre UInt8 contenant le numéro de l'heure dans l'Heure de 24 heures (0-23).
-This function assumes that if clocks are moved ahead, it is by one hour and occurs at 2 a.m., and if clocks are moved back, it is by one hour and occurs at 3 a.m. (which is not always true – even in Moscow the clocks were twice changed at a different time).
-
-## toMinute {#tominute}
-
-Convertit une date avec l'heure en un numéro UInt8 contenant le numéro de la minute de l'heure (0-59).
-
-## toseconde {#tosecond}
-
-Convertit une date avec l'heure en un nombre UInt8 contenant le numéro de la seconde dans la minute (0-59).
-Les secondes intercalaires ne sont pas comptabilisés.
-
-## toUnixTimestamp {#to-unix-timestamp}
-
-Pour L'argument DateTime: convertit la valeur en sa représentation numérique interne (horodatage Unix).
-For String argument: analyse datetime from string en fonction du fuseau horaire (second argument optionnel, le fuseau horaire du serveur est utilisé par défaut) et renvoie l'horodatage unix correspondant.
-Pour L'argument Date: le comportement n'est pas spécifié.
-
-**Syntaxe**
-
-``` sql
-toUnixTimestamp(datetime)
-toUnixTimestamp(str, [timezone])
-```
-
-**Valeur renvoyée**
-
--   Renvoie l'horodatage unix.
-
-Type: `UInt32`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT toUnixTimestamp('2017-11-05 08:07:47', 'Asia/Tokyo') AS unix_timestamp
-```
-
-Résultat:
-
-``` text
-┌─unix_timestamp─┐
-│     1509836867 │
-└────────────────┘
-```
-
-## toStartOfYear {#tostartofyear}
-
-Arrondit une date ou une date avec l'heure jusqu'au premier jour de l'année.
-Renvoie la date.
-
-## toStartOfISOYear {#tostartofisoyear}
-
-Arrondit une date ou une date avec l'heure jusqu'au premier jour de L'année ISO.
-Renvoie la date.
-
-## toStartOfQuarter {#tostartofquarter}
-
-Arrondit une date ou une date avec l'heure jusqu'au premier jour du trimestre.
-Le premier jour du trimestre, soit le 1er janvier, 1er avril, 1er juillet ou 1er octobre.
-Renvoie la date.
-
-## toStartOfMonth {#tostartofmonth}
-
-Arrondit une date ou une date avec l'heure jusqu'au premier jour du mois.
-Renvoie la date.
-
-!!! attention "Attention"
-    Le comportement de l'analyse des dates incorrectes est spécifique à l'implémentation. ClickHouse peut renvoyer la date zéro, lancer une exception ou faire “natural” débordement.
-
-## toMonday {#tomonday}
-
-Arrondit une date ou une date avec l'heure au lundi le plus proche.
-Renvoie la date.
-
-## toStartOfWeek (t \[, mode\]) {#tostartofweektmode}
-
-Arrondit une date ou une date avec l'heure au dimanche ou au lundi le plus proche par mode.
-Renvoie la date.
-L'argument mode fonctionne exactement comme l'argument mode de toWeek(). Pour la syntaxe à argument unique, une valeur de mode de 0 est utilisée.
-
-## toStartOfDay {#tostartofday}
-
-Arrondit une date avec le temps au début de la journée.
-
-## toStartOfHour {#tostartofhour}
-
-Arrondit une date avec le temps au début de l " heure.
-
-## toStartOfMinute {#tostartofminute}
-
-Arrondit une date avec le temps au début de la minute.
-
-## toStartOfFiveMinute {#tostartoffiveminute}
-
-Arrondit à une date avec l'heure de début de l'intervalle de cinq minutes.
-
-## toStartOfTenMinutes {#tostartoftenminutes}
-
-Arrondit une date avec le temps au début de l " intervalle de dix minutes.
-
-## toStartOfFifteenMinutes {#tostartoffifteenminutes}
-
-Arrondit la date avec le temps jusqu'au début de l'intervalle de quinze minutes.
-
-## toStartOfInterval(time_or_data, intervalle x Unité \[, time_zone\]) {#tostartofintervaltime-or-data-interval-x-unit-time-zone}
-
-Ceci est une généralisation d'autres fonctions nommées `toStartOf*`. Exemple,
-`toStartOfInterval(t, INTERVAL 1 year)` renvoie la même chose que `toStartOfYear(t)`,
-`toStartOfInterval(t, INTERVAL 1 month)` renvoie la même chose que `toStartOfMonth(t)`,
-`toStartOfInterval(t, INTERVAL 1 day)` renvoie la même chose que `toStartOfDay(t)`,
-`toStartOfInterval(t, INTERVAL 15 minute)` renvoie la même chose que `toStartOfFifteenMinutes(t)` etc.
-
-## toTime {#totime}
-
-Convertit une date avec l'heure en une certaine date fixe, tout en préservant l'heure.
-
-## toRelativeYearNum {#torelativeyearnum}
-
-Convertit une date avec l'heure ou la date, le numéro de l'année, à partir d'un certain point fixe dans le passé.
-
-## toRelativeQuarterNum {#torelativequarternum}
-
-Convertit une date avec l'heure ou la date au numéro du trimestre, à partir d'un certain point fixe dans le passé.
-
-## toRelativeMonthNum {#torelativemonthnum}
-
-Convertit une date avec l'heure ou la date au numéro du mois, à partir d'un certain point fixe dans le passé.
-
-## toRelativeWeekNum {#torelativeweeknum}
-
-Convertit une date avec l'heure ou la date, le numéro de la semaine, à partir d'un certain point fixe dans le passé.
-
-## toRelativeDayNum {#torelativedaynum}
-
-Convertit une date avec l'heure ou la date au numéro du jour, à partir d'un certain point fixe dans le passé.
-
-## toRelativeHourNum {#torelativehournum}
-
-Convertit une date avec l'heure ou la date au nombre de l'heure, à partir d'un certain point fixe dans le passé.
-
-## toRelativeMinuteNum {#torelativeminutenum}
-
-Convertit une date avec l'heure ou la date au numéro de la minute, à partir d'un certain point fixe dans le passé.
-
-## toRelativeSecondNum {#torelativesecondnum}
-
-Convertit une date avec l'heure ou la date au numéro de la seconde, à partir d'un certain point fixe dans le passé.
-
-## toISOYear {#toisoyear}
-
-Convertit une date ou une date avec l'heure en un numéro UInt16 contenant le numéro D'année ISO.
-
-## toISOWeek {#toisoweek}
-
-Convertit une date ou une date avec l'heure en un numéro UInt8 contenant le numéro de semaine ISO.
-
-## toWeek (date \[, mode\]) {#toweekdatemode}
-
-Cette fonction renvoie le numéro de semaine pour date ou datetime. La forme à deux arguments de toWeek() vous permet de spécifier si la semaine commence le dimanche ou le lundi et si la valeur de retour doit être comprise entre 0 et 53 ou entre 1 et 53. Si l'argument mode est omis, le mode par défaut est 0.
-`toISOWeek()`est une fonction de compatibilité équivalente à `toWeek(date,3)`.
-Le tableau suivant décrit le fonctionnement de l'argument mode.
-
-| Mode | Premier jour de la semaine | Gamme | Week 1 is the first week …       |
-|------|----------------------------|-------|----------------------------------|
-| 0    | Dimanche                   | 0-53  | avec un dimanche cette année     |
-| 1    | Lundi                      | 0-53  | avec 4 jours ou plus cette année |
-| 2    | Dimanche                   | 1-53  | avec un dimanche cette année     |
-| 3    | Lundi                      | 1-53  | avec 4 jours ou plus cette année |
-| 4    | Dimanche                   | 0-53  | avec 4 jours ou plus cette année |
-| 5    | Lundi                      | 0-53  | avec un lundi cette année        |
-| 6    | Dimanche                   | 1-53  | avec 4 jours ou plus cette année |
-| 7    | Lundi                      | 1-53  | avec un lundi cette année        |
-| 8    | Dimanche                   | 1-53  | contient Janvier 1               |
-| 9    | Lundi                      | 1-53  | contient Janvier 1               |
-
-Pour les valeurs de mode avec une signification de “with 4 or more days this year,” les semaines sont numérotées selon ISO 8601: 1988:
-
--   Si la semaine contenant Janvier 1 A 4 jours ou plus dans la nouvelle année, il est Semaine 1.
-
--   Sinon, c'est la dernière semaine de l'année précédente, et la semaine prochaine est la semaine 1.
-
-Pour les valeurs de mode avec une signification de “contains January 1”, la semaine contient Janvier 1 est Semaine 1. Peu importe combien de jours dans la nouvelle année la semaine contenait, même si elle contenait seulement un jour.
-
-``` sql
-toWeek(date, [, mode][, Timezone])
-```
-
-**Paramètre**
-
--   `date` – Date or DateTime.
--   `mode` – Optional parameter, Range of values is \[0,9\], default is 0.
--   `Timezone` – Optional parameter, it behaves like any other conversion function.
-
-**Exemple**
-
-``` sql
-SELECT toDate('2016-12-27') AS date, toWeek(date) AS week0, toWeek(date,1) AS week1, toWeek(date,9) AS week9;
-```
-
-``` text
-┌───────date─┬─week0─┬─week1─┬─week9─┐
-│ 2016-12-27 │    52 │    52 │     1 │
-└────────────┴───────┴───────┴───────┘
-```
-
-## toYearWeek (date \[, mode\]) {#toyearweekdatemode}
-
-Retourne l'année et la semaine pour une date. L'année dans le résultat peut être différente de l'année dans l'argument date pour la première et la dernière semaine de l'année.
-
-L'argument mode fonctionne exactement comme l'argument mode de toWeek(). Pour la syntaxe à argument unique, une valeur de mode de 0 est utilisée.
-
-`toISOYear()`est une fonction de compatibilité équivalente à `intDiv(toYearWeek(date,3),100)`.
-
-**Exemple**
-
-``` sql
-SELECT toDate('2016-12-27') AS date, toYearWeek(date) AS yearWeek0, toYearWeek(date,1) AS yearWeek1, toYearWeek(date,9) AS yearWeek9;
-```
-
-``` text
-┌───────date─┬─yearWeek0─┬─yearWeek1─┬─yearWeek9─┐
-│ 2016-12-27 │    201652 │    201652 │    201701 │
-└────────────┴───────────┴───────────┴───────────┘
-```
-
-## maintenant {#now}
-
-Accepte zéro argument et renvoie l'heure actuelle à l'un des moments de l'exécution de la requête.
-Cette fonction renvoie une constante, même si la requête a pris beaucoup de temps à compléter.
-
-## aujourd' {#today}
-
-Accepte zéro argument et renvoie la date actuelle à l'un des moments de l'exécution de la requête.
-Le même que ‘toDate(now())’.
-
-## hier {#yesterday}
-
-Accepte zéro argument et renvoie la date d'hier à l'un des moments de l'exécution de la requête.
-Le même que ‘today() - 1’.
-
-## l'horaire de diffusion {#timeslot}
-
-Arrondit le temps à la demi-heure.
-Cette fonction est spécifique à Yandex.Metrica, car une demi-heure est le temps minimum pour diviser une session en deux sessions si une balise de suivi affiche les pages vues consécutives d'un seul utilisateur qui diffèrent dans le temps de strictement plus que ce montant. Cela signifie que les tuples (l'ID de balise, l'ID utilisateur et l'intervalle de temps) peuvent être utilisés pour rechercher les pages vues incluses dans la session correspondante.
-
-## toYYYYMM {#toyyyymm}
-
-Convertit une date ou une date avec l'heure en un numéro UInt32 contenant le numéro d'année et de mois (AAAA \* 100 + MM).
-
-## toYYYYMMDD {#toyyyymmdd}
-
-Convertit une date ou une date avec l'heure en un numéro UInt32 contenant le numéro d'année et de mois (AAAA \* 10000 + MM \* 100 + JJ).
-
-## toYYYYMMDDhhmmss {#toyyyymmddhhmmss}
-
-Convertit une date ou une date avec l'heure en un numéro UInt64 contenant le numéro d'année et de mois (AAAA \* 10000000000 + MM \* 100000000 + DD \* 1000000 + hh \* 10000 + mm \* 100 + ss).
-
-## addYears, addMonths, addWeeks, addDays, addHours, addMinutes, addSeconds, addQuarters {#addyears-addmonths-addweeks-adddays-addhours-addminutes-addseconds-addquarters}
-
-Fonction ajoute une date / DateTime intervalle à une Date / DateTime, puis retourner la Date / DateTime. Exemple:
-
-``` sql
-WITH
-    toDate('2018-01-01') AS date,
-    toDateTime('2018-01-01 00:00:00') AS date_time
-SELECT
-    addYears(date, 1) AS add_years_with_date,
-    addYears(date_time, 1) AS add_years_with_date_time
-```
-
-``` text
-┌─add_years_with_date─┬─add_years_with_date_time─┐
-│          2019-01-01 │      2019-01-01 00:00:00 │
-└─────────────────────┴──────────────────────────┘
-```
-
-## subtractYears, subtractMonths, subtractWeeks, subtractDays, subtractHours, subtractMinutes, subtractSeconds, subtractQuarters {#subtractyears-subtractmonths-subtractweeks-subtractdays-subtracthours-subtractminutes-subtractseconds-subtractquarters}
-
-Fonction soustrayez un intervalle de Date / DateTime à une Date / DateTime, puis renvoyez la Date / DateTime. Exemple:
-
-``` sql
-WITH
-    toDate('2019-01-01') AS date,
-    toDateTime('2019-01-01 00:00:00') AS date_time
-SELECT
-    subtractYears(date, 1) AS subtract_years_with_date,
-    subtractYears(date_time, 1) AS subtract_years_with_date_time
-```
-
-``` text
-┌─subtract_years_with_date─┬─subtract_years_with_date_time─┐
-│               2018-01-01 │           2018-01-01 00:00:00 │
-└──────────────────────────┴───────────────────────────────┘
-```
-
-## dateDiff {#datediff}
-
-Renvoie la différence entre deux valeurs Date ou DateTime.
-
-**Syntaxe**
-
-``` sql
-dateDiff('unit', startdate, enddate, [timezone])
-```
-
-**Paramètre**
-
--   `unit` — Time unit, in which the returned value is expressed. [Chaîne](../syntax.md#syntax-string-literal).
-
-        Supported values:
-
-        | unit   |
-        | ---- |
-        |second  |
-        |minute  |
-        |hour    |
-        |day     |
-        |week    |
-        |month   |
-        |quarter |
-        |year    |
-
--   `startdate` — The first time value to compare. [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
-
--   `enddate` — The second time value to compare. [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
-
--   `timezone` — Optional parameter. If specified, it is applied to both `startdate` et `enddate`. Si non spécifié, fuseaux horaires de l' `startdate` et `enddate` sont utilisés. Si elles ne sont pas identiques, le résultat n'est pas spécifié.
-
-**Valeur renvoyée**
-
-Différence entre `startdate` et `enddate` exprimé en `unit`.
-
-Type: `int`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'));
-```
-
-Résultat:
-
-``` text
-┌─dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'))─┐
-│                                                                                     25 │
-└────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-## intervalle de temps (StartTime, Duration, \[, Size\]) {#timeslotsstarttime-duration-size}
-
-Pour un intervalle de temps commençant à ‘StartTime’ et de poursuivre pour ‘Duration’ secondes, il renvoie un tableau de moments dans le temps, composé de points de cet intervalle arrondis vers le bas à la ‘Size’ en quelques secondes. ‘Size’ est un paramètre optionnel: une constante UInt32, définie sur 1800 par défaut.
-Exemple, `timeSlots(toDateTime('2012-01-01 12:20:00'), 600) = [toDateTime('2012-01-01 12:00:00'), toDateTime('2012-01-01 12:30:00')]`.
-Ceci est nécessaire pour rechercher les pages vues dans la session correspondante.
-
-## formatDateTime(Heure, Format \[, fuseau horaire\]) {#formatdatetime}
-
-Function formats a Time according given Format string. N.B.: Format is a constant expression, e.g. you can not have multiple formats for single result column.
-
-Modificateurs pris en charge pour le Format:
-(“Example” colonne affiche le résultat de formatage pour le temps `2018-01-02 22:33:44`)
-
-| Modificateur | Description                                                            | Exemple    |
-|--------------|------------------------------------------------------------------------|------------|
-| %C           | année divisée par 100 et tronquée en entier (00-99)                    | 20         |
-| %d           | jour du mois, zero-rembourré (01-31)                                   | 02         |
-| %D           | Date courte MM / JJ / AA, équivalente à %m / % d / % y                 | 01/02/18   |
-| % e          | jour du mois, rembourré dans l'espace ( 1-31)                          | 2          |
-| %F           | date courte AAAA-MM-JJ, équivalente à % Y - % m - % d                  | 2018-01-02 |
-| %H           | heure en format 24h (00-23)                                            | 22         |
-| %I           | heure en format 12h (01-12)                                            | 10         |
-| %j           | les jours de l'année (001-366)                                         | 002        |
-| %m           | mois en nombre décimal (01-12)                                         | 01         |
-| %M           | minute (00-59)                                                         | 33         |
-| %et          | caractère de nouvelle ligne (")                                        |            |
-| %p           | Désignation AM ou PM                                                   | PM         |
-| %R           | 24 heures HH:MM temps, équivalent à %H: % M                            | 22:33      |
-| %S           | deuxième (00-59)                                                       | 44         |
-| % t          | horizontal-caractère de tabulation (')                                 |            |
-| %T           | Format d'heure ISO 8601 (HH:MM:SS), équivalent à %H: % M:%S            | 22:33:44   |
-| % u          | ISO 8601 jour de la semaine comme numéro avec Lundi comme 1 (1-7)      | 2          |
-| %V           | Numéro de semaine ISO 8601 (01-53)                                     | 01         |
-| %W           | jour de la semaine comme un nombre décimal avec dimanche comme 0 (0-6) | 2          |
-| % y          | Année, deux derniers chiffres (00-99)                                  | 18         |
-| %Y           | An                                                                     | 2018       |
-| %%           | signe                                                                  | %          |
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/date_time_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/encoding-functions.md b/docs/fr/sql-reference/functions/encoding-functions.md
deleted file mode 100644
index 6c99ed4f32e9..000000000000
--- a/docs/fr/sql-reference/functions/encoding-functions.md
+++ /dev/null
@@ -1,175 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 52
-toc_title: Encodage
----
-
-# L'Encodage Des Fonctions {#encoding-functions}
-
-## char {#char}
-
-Retourne la chaîne avec la longueur que le nombre d'arguments passés et chaque octet a la valeur de l'argument correspondant. Accepte plusieurs arguments de types numériques. Si la valeur de l'argument est hors de portée du type de données UInt8, elle est convertie en UInt8 avec arrondi et débordement possibles.
-
-**Syntaxe**
-
-``` sql
-char(number_1, [number_2, ..., number_n]);
-```
-
-**Paramètre**
-
--   `number_1, number_2, ..., number_n` — Numerical arguments interpreted as integers. Types: [Int](../../sql-reference/data-types/int-uint.md), [Flottant](../../sql-reference/data-types/float.md).
-
-**Valeur renvoyée**
-
--   une chaîne d'octets.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT char(104.1, 101, 108.9, 108.9, 111) AS hello
-```
-
-Résultat:
-
-``` text
-┌─hello─┐
-│ hello │
-└───────┘
-```
-
-Vous pouvez construire une chaîne de codage arbitraire en passant les octets correspondants. Voici un exemple pour UTF-8:
-
-Requête:
-
-``` sql
-SELECT char(0xD0, 0xBF, 0xD1, 0x80, 0xD0, 0xB8, 0xD0, 0xB2, 0xD0, 0xB5, 0xD1, 0x82) AS hello;
-```
-
-Résultat:
-
-``` text
-┌─hello──┐
-│ привет │
-└────────┘
-```
-
-Requête:
-
-``` sql
-SELECT char(0xE4, 0xBD, 0xA0, 0xE5, 0xA5, 0xBD) AS hello;
-```
-
-Résultat:
-
-``` text
-┌─hello─┐
-│ 你好  │
-└───────┘
-```
-
-## Hex {#hex}
-
-Renvoie une chaîne contenant la représentation hexadécimale de l'argument.
-
-**Syntaxe**
-
-``` sql
-hex(arg)
-```
-
-La fonction utilise des lettres majuscules `A-F` et ne pas utiliser de préfixes (comme `0x`) ou suffixes (comme `h`).
-
-Pour les arguments entiers, il imprime des chiffres hexadécimaux (“nibbles”) du plus significatif au moins significatif (big endian ou “human readable” ordre). Il commence par l'octet non nul le plus significatif (les octets de début zéro sont omis) mais imprime toujours les deux chiffres de chaque octet même si le chiffre de début est nul.
-
-Exemple:
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT hex(1);
-```
-
-Résultat:
-
-``` text
-01
-```
-
-Les valeurs de type `Date` et `DateTime` sont formatés comme des entiers correspondants (le nombre de jours depuis Epoch pour Date et la valeur de L'horodatage Unix pour DateTime).
-
-Pour `String` et `FixedString`, tous les octets sont simplement codés en deux nombres hexadécimaux. Zéro octets ne sont pas omis.
-
-Les valeurs des types virgule flottante et décimale sont codées comme leur représentation en mémoire. Comme nous soutenons l'architecture little endian, ils sont codés dans little endian. Zéro octets de début / fin ne sont pas omis.
-
-**Paramètre**
-
--   `arg` — A value to convert to hexadecimal. Types: [Chaîne](../../sql-reference/data-types/string.md), [UInt](../../sql-reference/data-types/int-uint.md), [Flottant](../../sql-reference/data-types/float.md), [Décimal](../../sql-reference/data-types/decimal.md), [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md).
-
-**Valeur renvoyée**
-
--   Une chaîne avec la représentation hexadécimale de l'argument.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT hex(toFloat32(number)) as hex_presentation FROM numbers(15, 2);
-```
-
-Résultat:
-
-``` text
-┌─hex_presentation─┐
-│ 00007041         │
-│ 00008041         │
-└──────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT hex(toFloat64(number)) as hex_presentation FROM numbers(15, 2);
-```
-
-Résultat:
-
-``` text
-┌─hex_presentation─┐
-│ 0000000000002E40 │
-│ 0000000000003040 │
-└──────────────────┘
-```
-
-## unhex (str) {#unhexstr}
-
-Accepte une chaîne contenant un nombre quelconque de chiffres hexadécimaux, et renvoie une chaîne contenant le correspondant octets. Prend en charge les lettres majuscules et minuscules A-F. Le nombre de chiffres hexadécimaux ne doit pas être pair. S'il est impair, le dernier chiffre est interprété comme la moitié la moins significative de l'octet 00-0F. Si la chaîne d'argument contient autre chose que des chiffres hexadécimaux, un résultat défini par l'implémentation est renvoyé (une exception n'est pas levée).
-Si vous voulez convertir le résultat en un nombre, vous pouvez utiliser le ‘reverse’ et ‘reinterpretAsType’ fonction.
-
-## UUIDStringToNum (str) {#uuidstringtonumstr}
-
-Accepte une chaîne contenant 36 caractères dans le format `123e4567-e89b-12d3-a456-426655440000`, et le renvoie comme un ensemble d'octets dans un FixedString (16).
-
-## UUIDNumToString (str) {#uuidnumtostringstr}
-
-Accepte une valeur FixedString (16). Renvoie une chaîne contenant 36 caractères au format texte.
-
-## bitmaskToList(num) {#bitmasktolistnum}
-
-Accepte un entier. Renvoie une chaîne contenant la liste des puissances de deux qui totalisent le nombre source lorsqu'il est additionné. Ils sont séparés par des virgules sans espaces au format texte, dans l'ordre croissant.
-
-## bitmaskToArray(num) {#bitmasktoarraynum}
-
-Accepte un entier. Renvoie un tableau de nombres UInt64 contenant la liste des puissances de deux qui totalisent le nombre source lorsqu'il est additionné. Les numéros dans le tableau sont dans l'ordre croissant.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/encoding_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/ext-dict-functions.md b/docs/fr/sql-reference/functions/ext-dict-functions.md
deleted file mode 100644
index 1cec307747d3..000000000000
--- a/docs/fr/sql-reference/functions/ext-dict-functions.md
+++ /dev/null
@@ -1,205 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 58
-toc_title: Travailler avec des dictionnaires externes
----
-
-# Fonctions pour travailler avec des dictionnaires externes {#ext_dict_functions}
-
-Pour plus d'informations sur la connexion et la configuration de dictionnaires externes, voir [Dictionnaires externes](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md).
-
-## dictGet {#dictget}
-
-Récupère une valeur d'un dictionnaire externe.
-
-``` sql
-dictGet('dict_name', 'attr_name', id_expr)
-dictGetOrDefault('dict_name', 'attr_name', id_expr, default_value_expr)
-```
-
-**Paramètre**
-
--   `dict_name` — Name of the dictionary. [Chaîne littérale](../syntax.md#syntax-string-literal).
--   `attr_name` — Name of the column of the dictionary. [Chaîne littérale](../syntax.md#syntax-string-literal).
--   `id_expr` — Key value. [Expression](../syntax.md#syntax-expressions) de retour d'un [UInt64](../../sql-reference/data-types/int-uint.md) ou [Tuple](../../sql-reference/data-types/tuple.md)- tapez la valeur en fonction de la configuration du dictionnaire.
--   `default_value_expr` — Value returned if the dictionary doesn't contain a row with the `id_expr` clé. [Expression](../syntax.md#syntax-expressions) renvoyer la valeur dans le type de données configuré pour `attr_name` attribut.
-
-**Valeur renvoyée**
-
--   Si ClickHouse analyse l'attribut avec succès dans le [l'attribut type de données](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes), les fonctions renvoient la valeur du dictionnaire de l'attribut qui correspond à `id_expr`.
-
--   Si il n'y a pas la clé, correspondant à `id_expr` dans le dictionnaire, puis:
-
-        - `dictGet` returns the content of the `<null_value>` element specified for the attribute in the dictionary configuration.
-        - `dictGetOrDefault` returns the value passed as the `default_value_expr` parameter.
-
-ClickHouse lève une exception si elle ne peut pas analyser la valeur de l'attribut ou si la valeur ne correspond pas au type de données d'attribut.
-
-**Exemple**
-
-Créer un fichier texte `ext-dict-text.csv` contenant les éléments suivants:
-
-``` text
-1,1
-2,2
-```
-
-La première colonne est `id` la deuxième colonne est `c1`.
-
-Configurer le dictionnaire externe:
-
-``` xml
-<yandex>
-    <dictionary>
-        <name>ext-dict-test</name>
-        <source>
-            <file>
-                <path>/path-to/ext-dict-test.csv</path>
-                <format>CSV</format>
-            </file>
-        </source>
-        <layout>
-            <flat />
-        </layout>
-        <structure>
-            <id>
-                <name>id</name>
-            </id>
-            <attribute>
-                <name>c1</name>
-                <type>UInt32</type>
-                <null_value></null_value>
-            </attribute>
-        </structure>
-        <lifetime>0</lifetime>
-    </dictionary>
-</yandex>
-```
-
-Effectuer la requête:
-
-``` sql
-SELECT
-    dictGetOrDefault('ext-dict-test', 'c1', number + 1, toUInt32(number * 10)) AS val,
-    toTypeName(val) AS type
-FROM system.numbers
-LIMIT 3
-```
-
-``` text
-┌─val─┬─type───┐
-│   1 │ UInt32 │
-│   2 │ UInt32 │
-│  20 │ UInt32 │
-└─────┴────────┘
-```
-
-**Voir Aussi**
-
--   [Dictionnaires Externes](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md)
-
-## dictHas {#dicthas}
-
-Vérifie si une clé est présente dans un dictionnaire.
-
-``` sql
-dictHas('dict_name', id_expr)
-```
-
-**Paramètre**
-
--   `dict_name` — Name of the dictionary. [Chaîne littérale](../syntax.md#syntax-string-literal).
--   `id_expr` — Key value. [Expression](../syntax.md#syntax-expressions) de retour d'un [UInt64](../../sql-reference/data-types/int-uint.md)-le type de la valeur.
-
-**Valeur renvoyée**
-
--   0, si il n'y a pas de clé.
--   1, si il y a une clé.
-
-Type: `UInt8`.
-
-## dictGetHierarchy {#dictgethierarchy}
-
-Crée un tableau contenant tous les parents d'une clé dans le [hiérarchique dictionnaire](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical.md).
-
-**Syntaxe**
-
-``` sql
-dictGetHierarchy('dict_name', key)
-```
-
-**Paramètre**
-
--   `dict_name` — Name of the dictionary. [Chaîne littérale](../syntax.md#syntax-string-literal).
--   `key` — Key value. [Expression](../syntax.md#syntax-expressions) de retour d'un [UInt64](../../sql-reference/data-types/int-uint.md)-le type de la valeur.
-
-**Valeur renvoyée**
-
--   Les Parents pour la clé.
-
-Type: [Tableau (UInt64)](../../sql-reference/data-types/array.md).
-
-## dictisine {#dictisin}
-
-Vérifie l'ancêtre d'une clé à travers toute la chaîne hiérarchique dans le dictionnaire.
-
-``` sql
-dictIsIn('dict_name', child_id_expr, ancestor_id_expr)
-```
-
-**Paramètre**
-
--   `dict_name` — Name of the dictionary. [Chaîne littérale](../syntax.md#syntax-string-literal).
--   `child_id_expr` — Key to be checked. [Expression](../syntax.md#syntax-expressions) de retour d'un [UInt64](../../sql-reference/data-types/int-uint.md)-le type de la valeur.
--   `ancestor_id_expr` — Alleged ancestor of the `child_id_expr` clé. [Expression](../syntax.md#syntax-expressions) de retour d'un [UInt64](../../sql-reference/data-types/int-uint.md)-le type de la valeur.
-
-**Valeur renvoyée**
-
--   0, si `child_id_expr` n'est pas un enfant de `ancestor_id_expr`.
--   1, si `child_id_expr` est un enfant de `ancestor_id_expr` ou si `child_id_expr` est un `ancestor_id_expr`.
-
-Type: `UInt8`.
-
-## D'Autres Fonctions {#ext_dict_functions-other}
-
-ClickHouse prend en charge des fonctions spécialisées qui convertissent les valeurs d'attribut de dictionnaire en un type de données spécifique, quelle que soit la configuration du dictionnaire.
-
-Fonction:
-
--   `dictGetInt8`, `dictGetInt16`, `dictGetInt32`, `dictGetInt64`
--   `dictGetUInt8`, `dictGetUInt16`, `dictGetUInt32`, `dictGetUInt64`
--   `dictGetFloat32`, `dictGetFloat64`
--   `dictGetDate`
--   `dictGetDateTime`
--   `dictGetUUID`
--   `dictGetString`
-
-Toutes ces fonctions ont le `OrDefault` modification. Exemple, `dictGetDateOrDefault`.
-
-Syntaxe:
-
-``` sql
-dictGet[Type]('dict_name', 'attr_name', id_expr)
-dictGet[Type]OrDefault('dict_name', 'attr_name', id_expr, default_value_expr)
-```
-
-**Paramètre**
-
--   `dict_name` — Name of the dictionary. [Chaîne littérale](../syntax.md#syntax-string-literal).
--   `attr_name` — Name of the column of the dictionary. [Chaîne littérale](../syntax.md#syntax-string-literal).
--   `id_expr` — Key value. [Expression](../syntax.md#syntax-expressions) de retour d'un [UInt64](../../sql-reference/data-types/int-uint.md)-le type de la valeur.
--   `default_value_expr` — Value which is returned if the dictionary doesn't contain a row with the `id_expr` clé. [Expression](../syntax.md#syntax-expressions) renvoyer une valeur dans le type de données configuré pour `attr_name` attribut.
-
-**Valeur renvoyée**
-
--   Si ClickHouse analyse l'attribut avec succès dans le [l'attribut type de données](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md#ext_dict_structure-attributes), les fonctions renvoient la valeur du dictionnaire de l'attribut qui correspond à `id_expr`.
-
--   Si il n'est pas demandé `id_expr` dans le dictionnaire,:
-
-        - `dictGet[Type]` returns the content of the `<null_value>` element specified for the attribute in the dictionary configuration.
-        - `dictGet[Type]OrDefault` returns the value passed as the `default_value_expr` parameter.
-
-ClickHouse lève une exception si elle ne peut pas analyser la valeur de l'attribut ou si la valeur ne correspond pas au type de données d'attribut.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/ext_dict_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/functions-for-nulls.md b/docs/fr/sql-reference/functions/functions-for-nulls.md
deleted file mode 100644
index ef7be728ce78..000000000000
--- a/docs/fr/sql-reference/functions/functions-for-nulls.md
+++ /dev/null
@@ -1,312 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 63
-toc_title: Travailler avec des arguments nullables
----
-
-# Fonctions pour travailler avec des agrégats nullables {#functions-for-working-with-nullable-aggregates}
-
-## isNull {#isnull}
-
-Vérifie si l'argument est [NULL](../../sql-reference/syntax.md#null-literal).
-
-``` sql
-isNull(x)
-```
-
-**Paramètre**
-
--   `x` — A value with a non-compound data type.
-
-**Valeur renvoyée**
-
--   `1` si `x` être `NULL`.
--   `0` si `x` n'est pas `NULL`.
-
-**Exemple**
-
-Table d'entrée
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-Requête
-
-``` sql
-SELECT x FROM t_null WHERE isNull(y)
-```
-
-``` text
-┌─x─┐
-│ 1 │
-└───┘
-```
-
-## isNotNull {#isnotnull}
-
-Vérifie si l'argument est [NULL](../../sql-reference/syntax.md#null-literal).
-
-``` sql
-isNotNull(x)
-```
-
-**Paramètre:**
-
--   `x` — A value with a non-compound data type.
-
-**Valeur renvoyée**
-
--   `0` si `x` être `NULL`.
--   `1` si `x` n'est pas `NULL`.
-
-**Exemple**
-
-Table d'entrée
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-Requête
-
-``` sql
-SELECT x FROM t_null WHERE isNotNull(y)
-```
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-## fusionner {#coalesce}
-
-Vérifie de gauche à droite si `NULL` les arguments ont été passés et renvoie le premier non-`NULL` argument.
-
-``` sql
-coalesce(x,...)
-```
-
-**Paramètre:**
-
--   N'importe quel nombre de paramètres d'un type non composé. Tous les paramètres doivent être compatibles par type de données.
-
-**Valeurs renvoyées**
-
--   Le premier non-`NULL` argument.
--   `NULL` si tous les arguments sont `NULL`.
-
-**Exemple**
-
-Considérez une liste de contacts qui peuvent spécifier plusieurs façons de contacter un client.
-
-``` text
-┌─name─────┬─mail─┬─phone─────┬──icq─┐
-│ client 1 │ ᴺᵁᴸᴸ │ 123-45-67 │  123 │
-│ client 2 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ      │ ᴺᵁᴸᴸ │
-└──────────┴──────┴───────────┴──────┘
-```
-
-Le `mail` et `phone` les champs sont de type Chaîne de caractères, mais la `icq` le terrain est `UInt32`, de sorte qu'il doit être converti en `String`.
-
-Obtenir la première méthode de contact pour le client à partir de la liste de contacts:
-
-``` sql
-SELECT coalesce(mail, phone, CAST(icq,'Nullable(String)')) FROM aBook
-```
-
-``` text
-┌─name─────┬─coalesce(mail, phone, CAST(icq, 'Nullable(String)'))─┐
-│ client 1 │ 123-45-67                                            │
-│ client 2 │ ᴺᵁᴸᴸ                                                 │
-└──────────┴──────────────────────────────────────────────────────┘
-```
-
-## ifNull {#ifnull}
-
-Renvoie une valeur alternative si l'argument principal est `NULL`.
-
-``` sql
-ifNull(x,alt)
-```
-
-**Paramètre:**
-
--   `x` — The value to check for `NULL`.
--   `alt` — The value that the function returns if `x` être `NULL`.
-
-**Valeurs renvoyées**
-
--   Valeur `x`, si `x` n'est pas `NULL`.
--   Valeur `alt`, si `x` être `NULL`.
-
-**Exemple**
-
-``` sql
-SELECT ifNull('a', 'b')
-```
-
-``` text
-┌─ifNull('a', 'b')─┐
-│ a                │
-└──────────────────┘
-```
-
-``` sql
-SELECT ifNull(NULL, 'b')
-```
-
-``` text
-┌─ifNull(NULL, 'b')─┐
-│ b                 │
-└───────────────────┘
-```
-
-## nullIf {#nullif}
-
-Retourner `NULL` si les arguments sont égaux.
-
-``` sql
-nullIf(x, y)
-```
-
-**Paramètre:**
-
-`x`, `y` — Values for comparison. They must be compatible types, or ClickHouse will generate an exception.
-
-**Valeurs renvoyées**
-
--   `NULL` si les arguments sont égaux.
--   Le `x` valeur, si les arguments ne sont pas égaux.
-
-**Exemple**
-
-``` sql
-SELECT nullIf(1, 1)
-```
-
-``` text
-┌─nullIf(1, 1)─┐
-│         ᴺᵁᴸᴸ │
-└──────────────┘
-```
-
-``` sql
-SELECT nullIf(1, 2)
-```
-
-``` text
-┌─nullIf(1, 2)─┐
-│            1 │
-└──────────────┘
-```
-
-## assumeNotNull {#assumenotnull}
-
-Résultats dans une valeur de type [Nullable](../../sql-reference/data-types/nullable.md) pour un non- `Nullable` si la valeur n'est pas `NULL`.
-
-``` sql
-assumeNotNull(x)
-```
-
-**Paramètre:**
-
--   `x` — The original value.
-
-**Valeurs renvoyées**
-
--   La valeur d'origine du non-`Nullable` type, si elle n'est pas `NULL`.
--   La valeur par défaut pour le non-`Nullable` Tapez si la valeur d'origine était `NULL`.
-
-**Exemple**
-
-Envisager l' `t_null` table.
-
-``` sql
-SHOW CREATE TABLE t_null
-```
-
-``` text
-┌─statement─────────────────────────────────────────────────────────────────┐
-│ CREATE TABLE default.t_null ( x Int8,  y Nullable(Int8)) ENGINE = TinyLog │
-└───────────────────────────────────────────────────────────────────────────┘
-```
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-Appliquer le `assumeNotNull` la fonction de la `y` colonne.
-
-``` sql
-SELECT assumeNotNull(y) FROM t_null
-```
-
-``` text
-┌─assumeNotNull(y)─┐
-│                0 │
-│                3 │
-└──────────────────┘
-```
-
-``` sql
-SELECT toTypeName(assumeNotNull(y)) FROM t_null
-```
-
-``` text
-┌─toTypeName(assumeNotNull(y))─┐
-│ Int8                         │
-│ Int8                         │
-└──────────────────────────────┘
-```
-
-## toNullable {#tonullable}
-
-Convertit le type d'argument en `Nullable`.
-
-``` sql
-toNullable(x)
-```
-
-**Paramètre:**
-
--   `x` — The value of any non-compound type.
-
-**Valeur renvoyée**
-
--   La valeur d'entrée avec un `Nullable` type.
-
-**Exemple**
-
-``` sql
-SELECT toTypeName(10)
-```
-
-``` text
-┌─toTypeName(10)─┐
-│ UInt8          │
-└────────────────┘
-```
-
-``` sql
-SELECT toTypeName(toNullable(10))
-```
-
-``` text
-┌─toTypeName(toNullable(10))─┐
-│ Nullable(UInt8)            │
-└────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/functions_for_nulls/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/geo.md b/docs/fr/sql-reference/functions/geo.md
deleted file mode 100644
index a89f03c7216d..000000000000
--- a/docs/fr/sql-reference/functions/geo.md
+++ /dev/null
@@ -1,510 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 62
-toc_title: "Travailler avec des coordonn\xE9es g\xE9ographiques"
----
-
-# Fonctions pour travailler avec des coordonnées géographiques {#functions-for-working-with-geographical-coordinates}
-
-## greatCircleDistance {#greatcircledistance}
-
-Calculer la distance entre deux points sur la surface de la Terre en utilisant [la formule du grand cercle](https://en.wikipedia.org/wiki/Great-circle_distance).
-
-``` sql
-greatCircleDistance(lon1Deg, lat1Deg, lon2Deg, lat2Deg)
-```
-
-**Les paramètres d'entrée**
-
--   `lon1Deg` — Longitude of the first point in degrees. Range: `[-180°, 180°]`.
--   `lat1Deg` — Latitude of the first point in degrees. Range: `[-90°, 90°]`.
--   `lon2Deg` — Longitude of the second point in degrees. Range: `[-180°, 180°]`.
--   `lat2Deg` — Latitude of the second point in degrees. Range: `[-90°, 90°]`.
-
-Les valeurs positives correspondent à la latitude nord et à la longitude Est, et les valeurs négatives à la latitude Sud et à la longitude ouest.
-
-**Valeur renvoyée**
-
-La distance entre deux points sur la surface de la Terre, en mètres.
-
-Génère une exception lorsque les valeurs des paramètres d'entrée se situent en dehors de la plage.
-
-**Exemple**
-
-``` sql
-SELECT greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)
-```
-
-``` text
-┌─greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)─┐
-│                                                14132374.194975413 │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-## pointInEllipses {#pointinellipses}
-
-Vérifie si le point appartient à au moins une des ellipses.
-Coordonnées géométriques sont dans le système de coordonnées Cartésiennes.
-
-``` sql
-pointInEllipses(x, y, x₀, y₀, a₀, b₀,...,xₙ, yₙ, aₙ, bₙ)
-```
-
-**Les paramètres d'entrée**
-
--   `x, y` — Coordinates of a point on the plane.
--   `xᵢ, yᵢ` — Coordinates of the center of the `i`-ème points de suspension.
--   `aᵢ, bᵢ` — Axes of the `i`- e ellipse en unités de coordonnées x, Y.
-
-Les paramètres d'entrée doivent être `2+4⋅n`, où `n` est le nombre de points de suspension.
-
-**Valeurs renvoyées**
-
-`1` si le point est à l'intérieur d'au moins l'un des ellipses; `0`si elle ne l'est pas.
-
-**Exemple**
-
-``` sql
-SELECT pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)
-```
-
-``` text
-┌─pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)─┐
-│                                               1 │
-└─────────────────────────────────────────────────┘
-```
-
-## pointtinpolygon {#pointinpolygon}
-
-Vérifie si le point appartient au polygone sur l'avion.
-
-``` sql
-pointInPolygon((x, y), [(a, b), (c, d) ...], ...)
-```
-
-**Les valeurs d'entrée**
-
--   `(x, y)` — Coordinates of a point on the plane. Data type — [Tuple](../../sql-reference/data-types/tuple.md) — A tuple of two numbers.
--   `[(a, b), (c, d) ...]` — Polygon vertices. Data type — [Tableau](../../sql-reference/data-types/array.md). Chaque sommet est représenté par une paire de coordonnées `(a, b)`. Les sommets doivent être spécifiés dans le sens horaire ou antihoraire. Le nombre minimum de sommets est 3. Le polygone doit être constante.
--   La fonction prend également en charge les polygones avec des trous (découper des sections). Dans ce cas, ajoutez des polygones qui définissent les sections découpées en utilisant des arguments supplémentaires de la fonction. La fonction ne prend pas en charge les polygones non simplement connectés.
-
-**Valeurs renvoyées**
-
-`1` si le point est à l'intérieur du polygone, `0` si elle ne l'est pas.
-Si le point est sur la limite du polygone, la fonction peut renvoyer 0 ou 1.
-
-**Exemple**
-
-``` sql
-SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res
-```
-
-``` text
-┌─res─┐
-│   1 │
-└─────┘
-```
-
-## geohashEncode {#geohashencode}
-
-Encode la latitude et la longitude en tant que chaîne geohash, voir (http://geohash.org/, https://en.wikipedia.org/wiki/Geohash).
-
-``` sql
-geohashEncode(longitude, latitude, [precision])
-```
-
-**Les valeurs d'entrée**
-
--   longitude longitude partie de la coordonnée que vous souhaitez encoder. Flottant dans la gamme`[-180°, 180°]`
--   latitude latitude partie de la coordonnée que vous souhaitez encoder. Flottant dans la gamme `[-90°, 90°]`
--   precision-facultatif, longueur de la chaîne codée résultante, par défaut `12`. Entier dans la gamme `[1, 12]`. Toute valeur inférieure à `1` ou supérieure à `12` silencieusement converti à `12`.
-
-**Valeurs renvoyées**
-
--   alphanumérique `String` de coordonnées codées (la version modifiée de l'alphabet de codage base32 est utilisée).
-
-**Exemple**
-
-``` sql
-SELECT geohashEncode(-5.60302734375, 42.593994140625, 0) AS res
-```
-
-``` text
-┌─res──────────┐
-│ ezs42d000000 │
-└──────────────┘
-```
-
-## geohashDecode {#geohashdecode}
-
-Décode toute chaîne codée geohash en longitude et latitude.
-
-**Les valeurs d'entrée**
-
--   chaîne codée-chaîne codée geohash.
-
-**Valeurs renvoyées**
-
--   (longitude, latitude) - 2-n-uplet de `Float64` les valeurs de longitude et de latitude.
-
-**Exemple**
-
-``` sql
-SELECT geohashDecode('ezs42') AS res
-```
-
-``` text
-┌─res─────────────────────────────┐
-│ (-5.60302734375,42.60498046875) │
-└─────────────────────────────────┘
-```
-
-## geoToH3 {#geotoh3}
-
-Retourner [H3](https://uber.github.io/h3/#/documentation/overview/introduction) point d'indice `(lon, lat)` avec une résolution spécifiée.
-
-[H3](https://uber.github.io/h3/#/documentation/overview/introduction) est un système d'indexation géographique où la surface de la Terre divisée en carreaux hexagonaux même. Ce système est hiérarchique, c'est-à-dire que chaque hexagone au niveau supérieur peut être divisé en sept, même mais plus petits, etc.
-
-Cet indice est principalement utilisé pour les emplacements de bucketing et d'autres manipulations géospatiales.
-
-**Syntaxe**
-
-``` sql
-geoToH3(lon, lat, resolution)
-```
-
-**Paramètre**
-
--   `lon` — Longitude. Type: [Float64](../../sql-reference/data-types/float.md).
--   `lat` — Latitude. Type: [Float64](../../sql-reference/data-types/float.md).
--   `resolution` — Index resolution. Range: `[0, 15]`. Type: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   Numéro d'indice hexagonal.
--   0 en cas d'erreur.
-
-Type: `UInt64`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT geoToH3(37.79506683, 55.71290588, 15) as h3Index
-```
-
-Résultat:
-
-``` text
-┌────────────h3Index─┐
-│ 644325524701193974 │
-└────────────────────┘
-```
-
-## geohashesInBox {#geohashesinbox}
-
-Renvoie un tableau de chaînes codées geohash de précision donnée qui tombent à l'intérieur et croisent les limites d'une boîte donnée, essentiellement une grille 2D aplatie en tableau.
-
-**Les valeurs d'entrée**
-
--   longitude_min-longitude min, valeur flottante dans la plage `[-180°, 180°]`
--   latitude_min-latitude min, valeur flottante dans la plage `[-90°, 90°]`
--   longitude_max-longitude maximale, valeur flottante dans la plage `[-180°, 180°]`
--   latitude_max-latitude maximale, valeur flottante dans la plage `[-90°, 90°]`
--   précision - geohash précision, `UInt8` dans la gamme `[1, 12]`
-
-Veuillez noter que tous les paramètres de coordonnées doit être du même type: soit `Float32` ou `Float64`.
-
-**Valeurs renvoyées**
-
--   gamme de précision de longues chaînes de geohash-boîtes couvrant la zone, vous ne devriez pas compter sur l'ordre des éléments.
--   \[\] - tableau vide si *min* les valeurs de *latitude* et *longitude* ne sont pas moins de correspondant *Max* valeur.
-
-Veuillez noter que la fonction lancera une exception si le tableau résultant a plus de 10'000'000 éléments.
-
-**Exemple**
-
-``` sql
-SELECT geohashesInBox(24.48, 40.56, 24.785, 40.81, 4) AS thasos
-```
-
-``` text
-┌─thasos──────────────────────────────────────┐
-│ ['sx1q','sx1r','sx32','sx1w','sx1x','sx38'] │
-└─────────────────────────────────────────────┘
-```
-
-## h3GetBaseCell {#h3getbasecell}
-
-Renvoie le numéro de cellule de base de l'index.
-
-**Syntaxe**
-
-``` sql
-h3GetBaseCell(index)
-```
-
-**Paramètre**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   Numéro de cellule de base hexagonale. Type: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT h3GetBaseCell(612916788725809151) as basecell
-```
-
-Résultat:
-
-``` text
-┌─basecell─┐
-│       12 │
-└──────────┘
-```
-
-## h3HexAreaM2 {#h3hexaream2}
-
-Surface hexagonale Moyenne en mètres carrés à la résolution donnée.
-
-**Syntaxe**
-
-``` sql
-h3HexAreaM2(resolution)
-```
-
-**Paramètre**
-
--   `resolution` — Index resolution. Range: `[0, 15]`. Type: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   Area in m². Type: [Float64](../../sql-reference/data-types/float.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT h3HexAreaM2(13) as area
-```
-
-Résultat:
-
-``` text
-┌─area─┐
-│ 43.9 │
-└──────┘
-```
-
-## h3IndexesAreNeighbors {#h3indexesareneighbors}
-
-Renvoie si les H3Indexes fournis sont voisins ou non.
-
-**Syntaxe**
-
-``` sql
-h3IndexesAreNeighbors(index1, index2)
-```
-
-**Paramètre**
-
--   `index1` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
--   `index2` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   Retourner `1` si les index sont voisins, `0` autrement. Type: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT h3IndexesAreNeighbors(617420388351344639, 617420388352655359) AS n
-```
-
-Résultat:
-
-``` text
-┌─n─┐
-│ 1 │
-└───┘
-```
-
-## h3enfants {#h3tochildren}
-
-Retourne un tableau avec les index enfants de l'index donné.
-
-**Syntaxe**
-
-``` sql
-h3ToChildren(index, resolution)
-```
-
-**Paramètre**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
--   `resolution` — Index resolution. Range: `[0, 15]`. Type: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   Tableau avec les index H3 enfants. Tableau de type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT h3ToChildren(599405990164561919, 6) AS children
-```
-
-Résultat:
-
-``` text
-┌─children───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ [603909588852408319,603909588986626047,603909589120843775,603909589255061503,603909589389279231,603909589523496959,603909589657714687] │
-└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-## h3ToParent {#h3toparent}
-
-Renvoie l'index parent (plus grossier) contenant l'index donné.
-
-**Syntaxe**
-
-``` sql
-h3ToParent(index, resolution)
-```
-
-**Paramètre**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
--   `resolution` — Index resolution. Range: `[0, 15]`. Type: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   Parent H3 index. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT h3ToParent(599405990164561919, 3) as parent
-```
-
-Résultat:
-
-``` text
-┌─────────────parent─┐
-│ 590398848891879423 │
-└────────────────────┘
-```
-
-## h3ToString {#h3tostring}
-
-Convertit la représentation H3Index de l'index en représentation de chaîne.
-
-``` sql
-h3ToString(index)
-```
-
-**Paramètre**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   Représentation en chaîne de l'index H3. Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT h3ToString(617420388352917503) as h3_string
-```
-
-Résultat:
-
-``` text
-┌─h3_string───────┐
-│ 89184926cdbffff │
-└─────────────────┘
-```
-
-## stringToH3 {#stringtoh3}
-
-Convertit la représentation de chaîne en représentation H3Index (UInt64).
-
-``` sql
-stringToH3(index_str)
-```
-
-**Paramètre**
-
--   `index_str` — String representation of the H3 index. Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-**Valeurs renvoyées**
-
--   Numéro d'indice hexagonal. Renvoie 0 en cas d'erreur. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT stringToH3('89184926cc3ffff') as index
-```
-
-Résultat:
-
-``` text
-┌──────────────index─┐
-│ 617420388351344639 │
-└────────────────────┘
-```
-
-## h3grésolution {#h3getresolution}
-
-Retourne la résolution de l'index.
-
-**Syntaxe**
-
-``` sql
-h3GetResolution(index)
-```
-
-**Paramètre**
-
--   `index` — Hexagon index number. Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Valeurs renvoyées**
-
--   L'indice de la résolution. Gamme: `[0, 15]`. Type: [UInt8](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT h3GetResolution(617420388352917503) as res
-```
-
-Résultat:
-
-``` text
-┌─res─┐
-│   9 │
-└─────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/geo/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/hash-functions.md b/docs/fr/sql-reference/functions/hash-functions.md
deleted file mode 100644
index 3b0f92dd4f80..000000000000
--- a/docs/fr/sql-reference/functions/hash-functions.md
+++ /dev/null
@@ -1,484 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 50
-toc_title: Hachage
----
-
-# Les Fonctions De Hachage {#hash-functions}
-
-Les fonctions de hachage peuvent être utilisées pour le brassage pseudo-aléatoire déterministe des éléments.
-
-## halfMD5 {#hash-functions-halfmd5}
-
-[Interpréter](../../sql-reference/functions/type-conversion-functions.md#type_conversion_functions-reinterpretAsString) tous les paramètres d'entrée sous forme de chaînes et calcule le [MD5](https://en.wikipedia.org/wiki/MD5) la valeur de hachage pour chacun d'eux. Puis combine les hachages, prend les 8 premiers octets du hachage de la chaîne résultante, et les interprète comme `UInt64` dans l'ordre des octets big-endian.
-
-``` sql
-halfMD5(par1, ...)
-```
-
-La fonction est relativement lente (5 millions de chaînes courtes par seconde par cœur de processeur).
-Envisager l'utilisation de la [sipHash64](#hash_functions-siphash64) la fonction la place.
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres d'entrée. Les paramètres peuvent être tout de la [types de données pris en charge](../../sql-reference/data-types/index.md).
-
-**Valeur Renvoyée**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) valeur de hachage du type de données.
-
-**Exemple**
-
-``` sql
-SELECT halfMD5(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS halfMD5hash, toTypeName(halfMD5hash) AS type
-```
-
-``` text
-┌────────halfMD5hash─┬─type───┐
-│ 186182704141653334 │ UInt64 │
-└────────────────────┴────────┘
-```
-
-## MD5 {#hash_functions-md5}
-
-Calcule le MD5 à partir d'une chaîne et renvoie L'ensemble d'octets résultant en tant que FixedString(16).
-Si vous n'avez pas besoin de MD5 en particulier, mais que vous avez besoin d'un hachage cryptographique 128 bits décent, utilisez le ‘sipHash128’ la fonction la place.
-Si vous voulez obtenir le même résultat que la sortie de l'utilitaire md5sum, utilisez lower (hex(MD5 (s))).
-
-## sipHash64 {#hash_functions-siphash64}
-
-Produit un 64 bits [SipHash](https://131002.net/siphash/) la valeur de hachage.
-
-``` sql
-sipHash64(par1,...)
-```
-
-C'est une fonction de hachage cryptographique. Il fonctionne au moins trois fois plus vite que le [MD5](#hash_functions-md5) fonction.
-
-Fonction [interpréter](../../sql-reference/functions/type-conversion-functions.md#type_conversion_functions-reinterpretAsString) tous les paramètres d'entrée sous forme de chaînes et calcule la valeur de hachage pour chacun d'eux. Puis combine les hachages par l'algorithme suivant:
-
-1.  Après avoir haché tous les paramètres d'entrée, la fonction obtient le tableau de hachages.
-2.  La fonction prend le premier et le second éléments et calcule un hachage pour le tableau d'entre eux.
-3.  Ensuite, la fonction prend la valeur de hachage, calculée à l'étape précédente, et le troisième élément du tableau de hachage initial, et calcule un hachage pour le tableau d'entre eux.
-4.  L'étape précédente est répétée pour tous les éléments restants de la période initiale de hachage tableau.
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres d'entrée. Les paramètres peuvent être tout de la [types de données pris en charge](../../sql-reference/data-types/index.md).
-
-**Valeur Renvoyée**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) valeur de hachage du type de données.
-
-**Exemple**
-
-``` sql
-SELECT sipHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS SipHash, toTypeName(SipHash) AS type
-```
-
-``` text
-┌──────────────SipHash─┬─type───┐
-│ 13726873534472839665 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## sipHash128 {#hash_functions-siphash128}
-
-Calcule SipHash à partir d'une chaîne.
-Accepte un argument de type chaîne. Renvoie FixedString (16).
-Diffère de sipHash64 en ce que l'état de pliage xor final n'est effectué que jusqu'à 128 bits.
-
-## cityHash64 {#cityhash64}
-
-Produit un 64 bits [CityHash](https://github.com/google/cityhash) la valeur de hachage.
-
-``` sql
-cityHash64(par1,...)
-```
-
-Ceci est une fonction de hachage non cryptographique rapide. Il utilise L'algorithme CityHash pour les paramètres de chaîne et la fonction de hachage rapide non cryptographique spécifique à l'implémentation pour les paramètres avec d'autres types de données. La fonction utilise le combinateur CityHash pour obtenir les résultats finaux.
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres d'entrée. Les paramètres peuvent être tout de la [types de données pris en charge](../../sql-reference/data-types/index.md).
-
-**Valeur Renvoyée**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) valeur de hachage du type de données.
-
-**Exemple**
-
-Appelez exemple:
-
-``` sql
-SELECT cityHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS CityHash, toTypeName(CityHash) AS type
-```
-
-``` text
-┌─────────────CityHash─┬─type───┐
-│ 12072650598913549138 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-L'exemple suivant montre comment calculer la somme de l'ensemble de la table avec précision jusqu'à la ligne de commande:
-
-``` sql
-SELECT groupBitXor(cityHash64(*)) FROM table
-```
-
-## intHash32 {#inthash32}
-
-Calcule un code de hachage 32 bits à partir de n'importe quel type d'entier.
-C'est une fonction de hachage non cryptographique relativement rapide de qualité moyenne pour les nombres.
-
-## intHash64 {#inthash64}
-
-Calcule un code de hachage 64 bits à partir de n'importe quel type d'entier.
-Il fonctionne plus vite que intHash32. Qualité moyenne.
-
-## SHA1 {#sha1}
-
-## SHA224 {#sha224}
-
-## SHA256 {#sha256}
-
-Calcule SHA-1, SHA-224 ou SHA-256 à partir d'une chaîne et renvoie l'ensemble d'octets résultant en tant que FixedString(20), FixedString(28) ou FixedString(32).
-La fonction fonctionne assez lentement (SHA-1 traite environ 5 millions de chaînes courtes par seconde par cœur de processeur, tandis que SHA-224 et SHA-256 traitent environ 2,2 millions).
-Nous vous recommandons d'utiliser cette fonction uniquement dans les cas où vous avez besoin d'une fonction de hachage spécifique et que vous ne pouvez pas la sélectionner.
-Même dans ces cas, nous vous recommandons d'appliquer la fonction hors ligne et de pré-calculer les valeurs lors de leur insertion dans la table, au lieu de l'appliquer dans SELECTS.
-
-## URLHash(url \[, N\]) {#urlhashurl-n}
-
-Une fonction de hachage non cryptographique rapide et de qualité décente pour une chaîne obtenue à partir d'une URL en utilisant un type de normalisation.
-`URLHash(s)` – Calculates a hash from a string without one of the trailing symbols `/`,`?` ou `#` à la fin, si elle est présente.
-`URLHash(s, N)` – Calculates a hash from a string up to the N level in the URL hierarchy, without one of the trailing symbols `/`,`?` ou `#` à la fin, si elle est présente.
-Les niveaux sont les mêmes que dans URLHierarchy. Cette fonction est spécifique à Yandex.Metrica.
-
-## farmHash64 {#farmhash64}
-
-Produit un 64 bits [FarmHash](https://github.com/google/farmhash) la valeur de hachage.
-
-``` sql
-farmHash64(par1, ...)
-```
-
-La fonction utilise le `Hash64` la méthode de tous les [les méthodes disponibles](https://github.com/google/farmhash/blob/master/src/farmhash.h).
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres d'entrée. Les paramètres peuvent être tout de la [types de données pris en charge](../../sql-reference/data-types/index.md).
-
-**Valeur Renvoyée**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) valeur de hachage du type de données.
-
-**Exemple**
-
-``` sql
-SELECT farmHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS FarmHash, toTypeName(FarmHash) AS type
-```
-
-``` text
-┌─────────────FarmHash─┬─type───┐
-│ 17790458267262532859 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## javaHash {#hash_functions-javahash}
-
-Calculer [JavaHash](http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/478a4add975b/src/share/classes/java/lang/String.java#l1452) à partir d'une chaîne. Cette fonction de hachage n'est ni rapide ni de bonne qualité. La seule raison de l'utiliser est lorsque cet algorithme est déjà utilisé dans un autre système et que vous devez calculer exactement le même résultat.
-
-**Syntaxe**
-
-``` sql
-SELECT javaHash('');
-```
-
-**Valeur renvoyée**
-
-A `Int32` valeur de hachage du type de données.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT javaHash('Hello, world!');
-```
-
-Résultat:
-
-``` text
-┌─javaHash('Hello, world!')─┐
-│               -1880044555 │
-└───────────────────────────┘
-```
-
-## javaHashUTF16LE {#javahashutf16le}
-
-Calculer [JavaHash](http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/478a4add975b/src/share/classes/java/lang/String.java#l1452) à partir d'une chaîne, en supposant qu'elle contient des octets représentant une chaîne en encodage UTF-16LE.
-
-**Syntaxe**
-
-``` sql
-javaHashUTF16LE(stringUtf16le)
-```
-
-**Paramètre**
-
--   `stringUtf16le` — a string in UTF-16LE encoding.
-
-**Valeur renvoyée**
-
-A `Int32` valeur de hachage du type de données.
-
-**Exemple**
-
-Requête correcte avec une chaîne codée UTF-16LE.
-
-Requête:
-
-``` sql
-SELECT javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'))
-```
-
-Résultat:
-
-``` text
-┌─javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'))─┐
-│                                                      3556498 │
-└──────────────────────────────────────────────────────────────┘
-```
-
-## hiveHash {#hash-functions-hivehash}
-
-Calculer `HiveHash` à partir d'une chaîne.
-
-``` sql
-SELECT hiveHash('');
-```
-
-C'est juste [JavaHash](#hash_functions-javahash) avec le bit de signe mis à zéro. Cette fonction est utilisée dans [Apache Hive](https://en.wikipedia.org/wiki/Apache_Hive) pour les versions antérieures à la version 3.0. Cette fonction de hachage n'est ni rapide ni de bonne qualité. La seule raison de l'utiliser est lorsque cet algorithme est déjà utilisé dans un autre système et que vous devez calculer exactement le même résultat.
-
-**Valeur renvoyée**
-
-A `Int32` valeur de hachage du type de données.
-
-Type: `hiveHash`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT hiveHash('Hello, world!');
-```
-
-Résultat:
-
-``` text
-┌─hiveHash('Hello, world!')─┐
-│                 267439093 │
-└───────────────────────────┘
-```
-
-## metroHash64 {#metrohash64}
-
-Produit un 64 bits [MetroHash](http://www.jandrewrogers.com/2015/05/27/metrohash/) la valeur de hachage.
-
-``` sql
-metroHash64(par1, ...)
-```
-
-**Paramètre**
-
-La fonction prend un nombre variable de paramètres d'entrée. Les paramètres peuvent être tout de la [types de données pris en charge](../../sql-reference/data-types/index.md).
-
-**Valeur Renvoyée**
-
-A [UInt64](../../sql-reference/data-types/int-uint.md) valeur de hachage du type de données.
-
-**Exemple**
-
-``` sql
-SELECT metroHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MetroHash, toTypeName(MetroHash) AS type
-```
-
-``` text
-┌────────────MetroHash─┬─type───┐
-│ 14235658766382344533 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## jumpConsistentHash {#jumpconsistenthash}
-
-Calcule JumpConsistentHash forme un UInt64.
-Accepte deux arguments: une clé de type UInt64 et le nombre de compartiments. Renvoie Int32.
-Pour plus d'informations, voir le lien: [JumpConsistentHash](https://arxiv.org/pdf/1406.2294.pdf)
-
-## murmurHash2_32, murmurHash2_64 {#murmurhash2-32-murmurhash2-64}
-
-Produit un [MurmurHash2](https://github.com/aappleby/smhasher) la valeur de hachage.
-
-``` sql
-murmurHash2_32(par1, ...)
-murmurHash2_64(par1, ...)
-```
-
-**Paramètre**
-
-Les deux fonctions prennent un nombre variable de paramètres d'entrée. Les paramètres peuvent être tout de la [types de données pris en charge](../../sql-reference/data-types/index.md).
-
-**Valeur Renvoyée**
-
--   Le `murmurHash2_32` fonction renvoie la valeur de hachage ayant le [UInt32](../../sql-reference/data-types/int-uint.md) type de données.
--   Le `murmurHash2_64` fonction renvoie la valeur de hachage ayant le [UInt64](../../sql-reference/data-types/int-uint.md) type de données.
-
-**Exemple**
-
-``` sql
-SELECT murmurHash2_64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash2, toTypeName(MurmurHash2) AS type
-```
-
-``` text
-┌──────────MurmurHash2─┬─type───┐
-│ 11832096901709403633 │ UInt64 │
-└──────────────────────┴────────┘
-```
-
-## gccMurmurHash {#gccmurmurhash}
-
-Calcule un 64 bits [MurmurHash2](https://github.com/aappleby/smhasher) valeur de hachage utilisant la même graine de hachage que [gcc](https://github.com/gcc-mirror/gcc/blob/41d6b10e96a1de98e90a7c0378437c3255814b16/libstdc%2B%2B-v3/include/bits/functional_hash.h#L191). Il est portable entre Clang et GCC construit.
-
-**Syntaxe**
-
-``` sql
-gccMurmurHash(par1, ...);
-```
-
-**Paramètre**
-
--   `par1, ...` — A variable number of parameters that can be any of the [types de données pris en charge](../../sql-reference/data-types/index.md#data_types).
-
-**Valeur renvoyée**
-
--   Valeur de hachage calculée.
-
-Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT
-    gccMurmurHash(1, 2, 3) AS res1,
-    gccMurmurHash(('a', [1, 2, 3], 4, (4, ['foo', 'bar'], 1, (1, 2)))) AS res2
-```
-
-Résultat:
-
-``` text
-┌─────────────────res1─┬────────────────res2─┐
-│ 12384823029245979431 │ 1188926775431157506 │
-└──────────────────────┴─────────────────────┘
-```
-
-## murmurHash3_32, murmurHash3_64 {#murmurhash3-32-murmurhash3-64}
-
-Produit un [MurmurHash3](https://github.com/aappleby/smhasher) la valeur de hachage.
-
-``` sql
-murmurHash3_32(par1, ...)
-murmurHash3_64(par1, ...)
-```
-
-**Paramètre**
-
-Les deux fonctions prennent un nombre variable de paramètres d'entrée. Les paramètres peuvent être tout de la [types de données pris en charge](../../sql-reference/data-types/index.md).
-
-**Valeur Renvoyée**
-
--   Le `murmurHash3_32` la fonction retourne un [UInt32](../../sql-reference/data-types/int-uint.md) valeur de hachage du type de données.
--   Le `murmurHash3_64` la fonction retourne un [UInt64](../../sql-reference/data-types/int-uint.md) valeur de hachage du type de données.
-
-**Exemple**
-
-``` sql
-SELECT murmurHash3_32(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash3, toTypeName(MurmurHash3) AS type
-```
-
-``` text
-┌─MurmurHash3─┬─type───┐
-│     2152717 │ UInt32 │
-└─────────────┴────────┘
-```
-
-## murmurHash3_128 {#murmurhash3-128}
-
-Produit de 128 bits [MurmurHash3](https://github.com/aappleby/smhasher) la valeur de hachage.
-
-``` sql
-murmurHash3_128( expr )
-```
-
-**Paramètre**
-
--   `expr` — [Expression](../syntax.md#syntax-expressions) de retour d'un [Chaîne](../../sql-reference/data-types/string.md)-le type de la valeur.
-
-**Valeur Renvoyée**
-
-A [FixedString (16)](../../sql-reference/data-types/fixedstring.md) valeur de hachage du type de données.
-
-**Exemple**
-
-``` sql
-SELECT murmurHash3_128('example_string') AS MurmurHash3, toTypeName(MurmurHash3) AS type
-```
-
-``` text
-┌─MurmurHash3──────┬─type────────────┐
-│ 6�1�4"S5KT�~~q │ FixedString(16) │
-└──────────────────┴─────────────────┘
-```
-
-## xxHash32, xxHash64 {#hash-functions-xxhash32}
-
-Calculer `xxHash` à partir d'une chaîne. Il est proposé en deux saveurs, 32 et 64 bits.
-
-``` sql
-SELECT xxHash32('');
-
-OR
-
-SELECT xxHash64('');
-```
-
-**Valeur renvoyée**
-
-A `Uint32` ou `Uint64` valeur de hachage du type de données.
-
-Type: `xxHash`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT xxHash32('Hello, world!');
-```
-
-Résultat:
-
-``` text
-┌─xxHash32('Hello, world!')─┐
-│                 834093149 │
-└───────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [xxHash](http://cyan4973.github.io/xxHash/).
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/hash_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/higher-order-functions.md b/docs/fr/sql-reference/functions/higher-order-functions.md
deleted file mode 100644
index ac24b67bb97b..000000000000
--- a/docs/fr/sql-reference/functions/higher-order-functions.md
+++ /dev/null
@@ -1,264 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 57
-toc_title: "D'Ordre Sup\xE9rieur"
----
-
-# Fonctions d'ordre supérieur {#higher-order-functions}
-
-## `->` opérateur, fonction lambda (params, expr) {#operator-lambdaparams-expr-function}
-
-Allows describing a lambda function for passing to a higher-order function. The left side of the arrow has a formal parameter, which is any ID, or multiple formal parameters – any IDs in a tuple. The right side of the arrow has an expression that can use these formal parameters, as well as any table columns.
-
-Exemple: `x -> 2 * x, str -> str != Referer.`
-
-Les fonctions d'ordre supérieur ne peuvent accepter que les fonctions lambda comme argument fonctionnel.
-
-Une fonction lambda qui accepte plusieurs arguments peuvent être passés à une fonction d'ordre supérieur. Dans ce cas, la fonction d'ordre supérieur est passé plusieurs tableaux de longueur identique que ces arguments correspondent.
-
-Pour certaines fonctions, telles que [arrayCount](#higher_order_functions-array-count) ou [arraySum](#higher_order_functions-array-count) le premier argument (la fonction lambda) peut être omis. Dans ce cas, un mappage identique est supposé.
-
-Une fonction lambda ne peut pas être omise pour les fonctions suivantes:
-
--   [arrayMap](#higher_order_functions-array-map)
--   [arrayFilter](#higher_order_functions-array-filter)
--   [arrayFill](#higher_order_functions-array-fill)
--   [arrayReverseFill](#higher_order_functions-array-reverse-fill)
--   [arraySplit](#higher_order_functions-array-split)
--   [arrayReverseSplit](#higher_order_functions-array-reverse-split)
--   [arrayFirst](#higher_order_functions-array-first)
--   [arrayFirstIndex](#higher_order_functions-array-first-index)
-
-### arrayMap(func, arr1, …) {#higher_order_functions-array-map}
-
-Renvoie un tableau obtenu à partir de l'application d'origine `func` fonction à chaque élément dans le `arr` tableau.
-
-Exemple:
-
-``` sql
-SELECT arrayMap(x -> (x + 2), [1, 2, 3]) as res;
-```
-
-``` text
-┌─res─────┐
-│ [3,4,5] │
-└─────────┘
-```
-
-L'exemple suivant montre comment créer un n-uplet d'éléments de différents tableaux:
-
-``` sql
-SELECT arrayMap((x, y) -> (x, y), [1, 2, 3], [4, 5, 6]) AS res
-```
-
-``` text
-┌─res─────────────────┐
-│ [(1,4),(2,5),(3,6)] │
-└─────────────────────┘
-```
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arrayMap` fonction.
-
-### arrayFilter(func, arr1, …) {#higher_order_functions-array-filter}
-
-Renvoie un tableau contenant uniquement les éléments `arr1` pour ce qui `func` retourne autre chose que 0.
-
-Exemple:
-
-``` sql
-SELECT arrayFilter(x -> x LIKE '%World%', ['Hello', 'abc World']) AS res
-```
-
-``` text
-┌─res───────────┐
-│ ['abc World'] │
-└───────────────┘
-```
-
-``` sql
-SELECT
-    arrayFilter(
-        (i, x) -> x LIKE '%World%',
-        arrayEnumerate(arr),
-        ['Hello', 'abc World'] AS arr)
-    AS res
-```
-
-``` text
-┌─res─┐
-│ [2] │
-└─────┘
-```
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arrayFilter` fonction.
-
-### arrayFill(func, arr1, …) {#higher_order_functions-array-fill}
-
-Analyse par le biais de `arr1` du premier élément au dernier élément et remplacer `arr1[i]` par `arr1[i - 1]` si `func` renvoie 0. Le premier élément de `arr1` ne sera pas remplacé.
-
-Exemple:
-
-``` sql
-SELECT arrayFill(x -> not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res
-```
-
-``` text
-┌─res──────────────────────────────┐
-│ [1,1,3,11,12,12,12,5,6,14,14,14] │
-└──────────────────────────────────┘
-```
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arrayFill` fonction.
-
-### arrayReverseFill(func, arr1, …) {#higher_order_functions-array-reverse-fill}
-
-Analyse par le biais de `arr1` du dernier élément au premier élément et remplacer `arr1[i]` par `arr1[i + 1]` si `func` renvoie 0. Le dernier élément de `arr1` ne sera pas remplacé.
-
-Exemple:
-
-``` sql
-SELECT arrayReverseFill(x -> not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res
-```
-
-``` text
-┌─res────────────────────────────────┐
-│ [1,3,3,11,12,5,5,5,6,14,NULL,NULL] │
-└────────────────────────────────────┘
-```
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arrayReverseFill` fonction.
-
-### arraySplit(func, arr1, …) {#higher_order_functions-array-split}
-
-Split `arr1` en plusieurs tableaux. Lorsque `func` retourne autre chose que 0, la matrice sera de split sur le côté gauche de l'élément. Le tableau ne sera pas partagé avant le premier élément.
-
-Exemple:
-
-``` sql
-SELECT arraySplit((x, y) -> y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res
-```
-
-``` text
-┌─res─────────────┐
-│ [[1,2,3],[4,5]] │
-└─────────────────┘
-```
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arraySplit` fonction.
-
-### arrayReverseSplit(func, arr1, …) {#higher_order_functions-array-reverse-split}
-
-Split `arr1` en plusieurs tableaux. Lorsque `func` retourne autre chose que 0, la matrice sera de split sur le côté droit de l'élément. Le tableau ne sera pas divisé après le dernier élément.
-
-Exemple:
-
-``` sql
-SELECT arrayReverseSplit((x, y) -> y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res
-```
-
-``` text
-┌─res───────────────┐
-│ [[1],[2,3,4],[5]] │
-└───────────────────┘
-```
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arraySplit` fonction.
-
-### arrayCount(\[func,\] arr1, …) {#higher_order_functions-array-count}
-
-Renvoie le nombre d'éléments dans l'arr tableau pour lequel func renvoie autre chose que 0. Si ‘func’ n'est pas spécifié, il renvoie le nombre d'éléments non nuls dans le tableau.
-
-### arrayExists(\[func,\] arr1, …) {#arrayexistsfunc-arr1}
-
-Renvoie 1 s'il existe au moins un élément ‘arr’ pour ce qui ‘func’ retourne autre chose que 0. Sinon, il renvoie 0.
-
-### arrayAll(\[func,\] arr1, …) {#arrayallfunc-arr1}
-
-Renvoie 1 si ‘func’ retourne autre chose que 0 pour tous les éléments de ‘arr’. Sinon, il renvoie 0.
-
-### arraySum(\[func,\] arr1, …) {#higher-order-functions-array-sum}
-
-Renvoie la somme de la ‘func’ valeur. Si la fonction est omise, elle retourne la somme des éléments du tableau.
-
-### arrayFirst(func, arr1, …) {#higher_order_functions-array-first}
-
-Renvoie le premier élément du ‘arr1’ tableau pour lequel ‘func’ retourne autre chose que 0.
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arrayFirst` fonction.
-
-### arrayFirstIndex(func, arr1, …) {#higher_order_functions-array-first-index}
-
-Renvoie l'index du premier élément de la ‘arr1’ tableau pour lequel ‘func’ retourne autre chose que 0.
-
-Notez que le premier argument (fonction lambda) ne peut pas être omis dans le `arrayFirstIndex` fonction.
-
-### arrayCumSum(\[func,\] arr1, …) {#arraycumsumfunc-arr1}
-
-Retourne un tableau des sommes partielles d'éléments dans le tableau source (une somme). Si l' `func` la fonction est spécifiée, les valeurs des éléments du tableau sont convertis par cette fonction avant l'addition.
-
-Exemple:
-
-``` sql
-SELECT arrayCumSum([1, 1, 1, 1]) AS res
-```
-
-``` text
-┌─res──────────┐
-│ [1, 2, 3, 4] │
-└──────────────┘
-```
-
-### arrayCumSumNonNegative (arr) {#arraycumsumnonnegativearr}
-
-Même que `arrayCumSum`, renvoie un tableau des sommes partielles d'éléments dans le tableau source (une somme). Différent `arrayCumSum`, lorsque la valeur renvoyée contient une valeur inférieure à zéro, la valeur est remplacée par zéro et le calcul ultérieur est effectué avec des paramètres zéro. Exemple:
-
-``` sql
-SELECT arrayCumSumNonNegative([1, 1, -4, 1]) AS res
-```
-
-``` text
-┌─res───────┐
-│ [1,2,0,1] │
-└───────────┘
-```
-
-### arraySort(\[func,\] arr1, …) {#arraysortfunc-arr1}
-
-Renvoie un tableau à la suite du tri des éléments de `arr1` dans l'ordre croissant. Si l' `func` la fonction est spécifiée, l'ordre de classement est déterminé par le résultat de la fonction `func` appliquée aux éléments du tableau (tableaux)
-
-Le [Transformation schwartzienne](https://en.wikipedia.org/wiki/Schwartzian_transform) est utilisé pour améliorer l'efficacité du tri.
-
-Exemple:
-
-``` sql
-SELECT arraySort((x, y) -> y, ['hello', 'world'], [2, 1]);
-```
-
-``` text
-┌─res────────────────┐
-│ ['world', 'hello'] │
-└────────────────────┘
-```
-
-Pour plus d'informations sur la `arraySort` la méthode, voir l' [Fonctions pour travailler avec des tableaux](array-functions.md#array_functions-sort) section.
-
-### arrayReverseSort(\[func,\] arr1, …) {#arrayreversesortfunc-arr1}
-
-Renvoie un tableau à la suite du tri des éléments de `arr1` dans l'ordre décroissant. Si l' `func` la fonction est spécifiée, l'ordre de classement est déterminé par le résultat de la fonction `func` appliquée aux éléments du tableau (tableaux).
-
-Exemple:
-
-``` sql
-SELECT arrayReverseSort((x, y) -> y, ['hello', 'world'], [2, 1]) as res;
-```
-
-``` text
-┌─res───────────────┐
-│ ['hello','world'] │
-└───────────────────┘
-```
-
-Pour plus d'informations sur la `arrayReverseSort` la méthode, voir l' [Fonctions pour travailler avec des tableaux](array-functions.md#array_functions-reverse-sort) section.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/higher_order_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/in-functions.md b/docs/fr/sql-reference/functions/in-functions.md
deleted file mode 100644
index ced5ef73e462..000000000000
--- a/docs/fr/sql-reference/functions/in-functions.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 60
-toc_title: "Mise en \u0153uvre de L'op\xE9rateur IN"
----
-
-# Fonctions de mise en œuvre de L'opérateur IN {#functions-for-implementing-the-in-operator}
-
-## in, notin, globalIn, globalNotIn {#in-functions}
-
-Voir la section [Dans les opérateurs](../operators/in.md#select-in-operators).
-
-## tuple(x, y, …), operator (x, y, …) {#tuplex-y-operator-x-y}
-
-Une fonction qui permet de regrouper plusieurs colonnes.
-For columns with the types T1, T2, …, it returns a Tuple(T1, T2, …) type tuple containing these columns. There is no cost to execute the function.
-Les Tuples sont normalement utilisés comme valeurs intermédiaires pour un argument D'opérateurs IN, ou pour créer une liste de paramètres formels de fonctions lambda. Les Tuples ne peuvent pas être écrits sur une table.
-
-## tupleElement (tuple, n), opérateur X. N {#tupleelementtuple-n-operator-x-n}
-
-Une fonction qui permet d'obtenir une colonne à partir d'un tuple.
-‘N’ est l'index de colonne, à partir de 1. N doit être une constante. ‘N’ doit être une constante. ‘N’ doit être un entier postif strict ne dépassant pas la taille du tuple.
-Il n'y a aucun coût pour exécuter la fonction.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/in_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/index.md b/docs/fr/sql-reference/functions/index.md
deleted file mode 100644
index 6e5333f68f5d..000000000000
--- a/docs/fr/sql-reference/functions/index.md
+++ /dev/null
@@ -1,74 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Fonction
-toc_priority: 32
-toc_title: Introduction
----
-
-# Fonction {#functions}
-
-Il y a au moins\* deux types de fonctions - des fonctions régulières (elles sont simplement appelées “functions”) and aggregate functions. These are completely different concepts. Regular functions work as if they are applied to each row separately (for each row, the result of the function doesn't depend on the other rows). Aggregate functions accumulate a set of values from various rows (i.e. they depend on the entire set of rows).
-
-Dans cette section, nous discutons des fonctions classiques. Pour les fonctions d'agrégation, voir la section “Aggregate functions”.
-
-\* - Il existe un troisième type de fonction ‘arrayJoin’ la fonction appartient à; les fonctions de table peuvent également être mentionnées séparément.\*
-
-## Typage Fort {#strong-typing}
-
-Contrairement à SQL standard, ClickHouse a une forte typage. En d'autres termes, il ne fait pas de conversions implicites entre les types. Chaque fonction fonctionne pour un ensemble spécifique de types. Cela signifie que vous devez parfois utiliser des fonctions de conversion de type.
-
-## Élimination Des Sous-Expressions Courantes {#common-subexpression-elimination}
-
-Toutes les expressions d'une requête qui ont le même AST (le même enregistrement ou le même résultat d'analyse syntaxique) sont considérées comme ayant des valeurs identiques. De telles expressions sont concaténées et exécutées une fois. Les sous-requêtes identiques sont également éliminées de cette façon.
-
-## Types de résultats {#types-of-results}
-
-Toutes les fonctions renvoient un seul retour comme résultat (pas plusieurs valeurs, et pas des valeurs nulles). Le type de résultat est généralement défini uniquement par les types d'arguments, pas par les valeurs. Les Exceptions sont la fonction tupleElement (l'opérateur A. N) et la fonction toFixedString.
-
-## Constant {#constants}
-
-Pour simplifier, certaines fonctions ne peuvent fonctionner qu'avec des constantes pour certains arguments. Par exemple, le bon argument de L'opérateur LIKE doit être une constante.
-Presque toutes les fonctions renvoient une constante pour des arguments constants. L'exception est les fonctions qui génèrent des nombres aléatoires.
-Le ‘now’ function renvoie des valeurs différentes pour les requêtes qui ont été exécutées à des moments différents, mais le résultat est considéré comme une constante, car la constance n'est importante que dans une seule requête.
-Une expression constante est également considérée comme une constante (par exemple, la moitié droite de L'opérateur LIKE peut être construite à partir de plusieurs constantes).
-
-Les fonctions peuvent être implémentées de différentes manières pour des arguments constants et non constants (un code différent est exécuté). Mais les résultats pour une constante et pour une colonne vraie Ne contenant que la même valeur doivent correspondre les uns aux autres.
-
-## Le Traitement NULL {#null-processing}
-
-Les fonctions ont les comportements suivants:
-
--   Si au moins l'un des arguments de la fonction est `NULL` le résultat de la fonction est également `NULL`.
--   Comportement spécial spécifié individuellement dans la description de chaque fonction. Dans le code source de ClickHouse, ces fonctions ont `UseDefaultImplementationForNulls=false`.
-
-## Constance {#constancy}
-
-Functions can't change the values of their arguments – any changes are returned as the result. Thus, the result of calculating separate functions does not depend on the order in which the functions are written in the query.
-
-## Erreur De Manipulation {#error-handling}
-
-Certaines fonctions peuvent lancer une exception si les données ne sont pas valides. Dans ce cas, la requête est annulée et un message d'erreur est retourné au client. Pour le traitement distribué, lorsqu'une exception se produit sur l'un des serveurs, les autres serveurs aussi tenté d'interrompre la requête.
-
-## Évaluation des Expressions D'Argument {#evaluation-of-argument-expressions}
-
-Dans presque tous les langages de programmation, l'un des arguments peut pas être évalué pour certains opérateurs. Ce sont généralement les opérateurs `&&`, `||`, et `?:`.
-Mais dans ClickHouse, les arguments des fonctions (opérateurs) sont toujours évalués. En effet, des parties entières de colonnes sont évaluées à la fois, au lieu de calculer chaque ligne séparément.
-
-## Exécution de fonctions pour le traitement de requêtes distribuées {#performing-functions-for-distributed-query-processing}
-
-Pour le traitement de requête distribué, autant d'étapes de traitement de requête que possible sont effectuées sur des serveurs distants, et le reste des étapes (fusion des résultats intermédiaires et tout ce qui suit) sont effectuées sur le serveur demandeur.
-
-Cela signifie que les fonctions peuvent être effectuées sur différents serveurs.
-Par exemple, dans la requête `SELECT f(sum(g(x))) FROM distributed_table GROUP BY h(y),`
-
--   si un `distributed_table` a au moins deux fragments, les fonctions ‘g’ et ‘h’ sont effectuées sur des serveurs distants, et la fonction ‘f’ est effectuée sur le serveur demandeur.
--   si un `distributed_table` a un seul fragment, tous les ‘f’, ‘g’, et ‘h’ les fonctions sont exécutées sur le serveur de ce fragment.
-
-Le résultat d'une fonction habituellement ne dépendent pas le serveur sur lequel elle est exécutée. Cependant, parfois c'est important.
-Par exemple, les fonctions qui fonctionnent avec des dictionnaires utilisent le dictionnaire qui existe sur le serveur sur lequel elles s'exécutent.
-Un autre exemple est l' `hostName` fonction, qui renvoie le nom du serveur sur lequel il s'exécute afin de `GROUP BY` par les serveurs dans un `SELECT` requête.
-
-Si une fonction dans une requête est effectuée sur le demandeur serveur, mais vous devez l'exécuter sur des serveurs distants, vous pouvez l'envelopper dans un ‘any’ fonction d'agrégation ou l'ajouter à une clé dans `GROUP BY`.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/introspection.md b/docs/fr/sql-reference/functions/introspection.md
deleted file mode 100644
index 91299217dc73..000000000000
--- a/docs/fr/sql-reference/functions/introspection.md
+++ /dev/null
@@ -1,310 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 65
-toc_title: Introspection
----
-
-# Fonctions D'Introspection {#introspection-functions}
-
-Vous pouvez utiliser les fonctions décrites dans ce chapitre pour introspecter [ELF](https://en.wikipedia.org/wiki/Executable_and_Linkable_Format) et [DWARF](https://en.wikipedia.org/wiki/DWARF) pour le profilage de requête.
-
-!!! warning "Avertissement"
-    Ces fonctions sont lentes et peuvent imposer des considérations de sécurité.
-
-Pour le bon fonctionnement des fonctions d'introspection:
-
--   Installer le `clickhouse-common-static-dbg` paquet.
-
--   Définir le [allow_introspection_functions](../../operations/settings/settings.md#settings-allow_introspection_functions) réglage sur 1.
-
-        For security reasons introspection functions are disabled by default.
-
-Clickhouse enregistre les rapports du profileur [trace_log](../../operations/system-tables.md#system_tables-trace_log) système de table. Assurez-vous que la table et le profileur sont correctement configurés.
-
-## addressToLine {#addresstoline}
-
-Convertit l'adresse de mémoire virtuelle dans le processus de serveur ClickHouse en nom de fichier et en numéro de ligne dans le code source de ClickHouse.
-
-Si vous utilisez des paquets clickhouse officiels, vous devez installer le `clickhouse-common-static-dbg` paquet.
-
-**Syntaxe**
-
-``` sql
-addressToLine(address_of_binary_instruction)
-```
-
-**Paramètre**
-
--   `address_of_binary_instruction` ([UInt64](../../sql-reference/data-types/int-uint.md)) — Address of instruction in a running process.
-
-**Valeur renvoyée**
-
--   Nom de fichier du code Source et le numéro de ligne dans ce fichier délimité par deux-points.
-
-        For example, `/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199`, where `199` is a line number.
-
--   Nom d'un binaire, si la fonction n'a pas pu trouver les informations de débogage.
-
--   Chaîne vide, si l'adresse n'est pas valide.
-
-Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-**Exemple**
-
-Activation des fonctions d'introspection:
-
-``` sql
-SET allow_introspection_functions=1
-```
-
-Sélection de la première chaîne de `trace_log` système de table:
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:              2019-11-19
-event_time:              2019-11-19 18:57:23
-revision:                54429
-timer_type:              Real
-thread_number:           48
-query_id:                421b6855-1858-45a5-8f37-f383409d6d72
-trace:                   [140658411141617,94784174532828,94784076370703,94784076372094,94784076361020,94784175007680,140658411116251,140658403895439]
-```
-
-Le `trace` champ contient la trace de pile au moment de l'échantillonnage.
-
-Obtenir le nom de fichier du code source et le numéro de ligne pour une seule adresse:
-
-``` sql
-SELECT addressToLine(94784076370703) \G
-```
-
-``` text
-Row 1:
-──────
-addressToLine(94784076370703): /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199
-```
-
-Application de la fonction à la trace de la pile entière:
-
-``` sql
-SELECT
-    arrayStringConcat(arrayMap(x -> addressToLine(x), trace), '
') AS trace_source_code_lines
-FROM system.trace_log
-LIMIT 1
-\G
-```
-
-Le [arrayMap](higher-order-functions.md#higher_order_functions-array-map) permet de traiter chaque élément individuel de l' `trace` tableau par la `addressToLine` fonction. Le résultat de ce traitement que vous voyez dans l' `trace_source_code_lines` colonne de sortie.
-
-``` text
-Row 1:
-──────
-trace_source_code_lines: /lib/x86_64-linux-gnu/libpthread-2.27.so
-/usr/lib/debug/usr/bin/clickhouse
-/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199
-/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:155
-/usr/include/c++/9/bits/atomic_base.h:551
-/usr/lib/debug/usr/bin/clickhouse
-/lib/x86_64-linux-gnu/libpthread-2.27.so
-/build/glibc-OTsEL5/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97
-```
-
-## adressetosymbol {#addresstosymbol}
-
-Convertit l'adresse de mémoire virtuelle dans le processus de serveur ClickHouse en symbole à partir des fichiers d'objets ClickHouse.
-
-**Syntaxe**
-
-``` sql
-addressToSymbol(address_of_binary_instruction)
-```
-
-**Paramètre**
-
--   `address_of_binary_instruction` ([UInt64](../../sql-reference/data-types/int-uint.md)) — Address of instruction in a running process.
-
-**Valeur renvoyée**
-
--   Symbole des fichiers D'objets ClickHouse.
--   Chaîne vide, si l'adresse n'est pas valide.
-
-Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-**Exemple**
-
-Activation des fonctions d'introspection:
-
-``` sql
-SET allow_introspection_functions=1
-```
-
-Sélection de la première chaîne de `trace_log` système de table:
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:    2019-11-20
-event_time:    2019-11-20 16:57:59
-revision:      54429
-timer_type:    Real
-thread_number: 48
-query_id:      724028bf-f550-45aa-910d-2af6212b94ac
-trace:         [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]
-```
-
-Le `trace` champ contient la trace de pile au moment de l'échantillonnage.
-
-Obtenir un symbole pour une seule adresse:
-
-``` sql
-SELECT addressToSymbol(94138803686098) \G
-```
-
-``` text
-Row 1:
-──────
-addressToSymbol(94138803686098): _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE
-```
-
-Application de la fonction à la trace de la pile entière:
-
-``` sql
-SELECT
-    arrayStringConcat(arrayMap(x -> addressToSymbol(x), trace), '
') AS trace_symbols
-FROM system.trace_log
-LIMIT 1
-\G
-```
-
-Le [arrayMap](higher-order-functions.md#higher_order_functions-array-map) permet de traiter chaque élément individuel de l' `trace` tableau par la `addressToSymbols` fonction. Le résultat de ce traitement que vous voyez dans l' `trace_symbols` colonne de sortie.
-
-``` text
-Row 1:
-──────
-trace_symbols: _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE
-_ZNK2DB10Aggregator21executeWithoutKeyImplERPcmPNS0_28AggregateFunctionInstructionEPNS_5ArenaE
-_ZN2DB10Aggregator14executeOnBlockESt6vectorIN3COWINS_7IColumnEE13immutable_ptrIS3_EESaIS6_EEmRNS_22AggregatedDataVariantsERS1_IPKS3_SaISC_EERS1_ISE_SaISE_EERb
-_ZN2DB10Aggregator14executeOnBlockERKNS_5BlockERNS_22AggregatedDataVariantsERSt6vectorIPKNS_7IColumnESaIS9_EERS6_ISB_SaISB_EERb
-_ZN2DB10Aggregator7executeERKSt10shared_ptrINS_17IBlockInputStreamEERNS_22AggregatedDataVariantsE
-_ZN2DB27AggregatingBlockInputStream8readImplEv
-_ZN2DB17IBlockInputStream4readEv
-_ZN2DB26ExpressionBlockInputStream8readImplEv
-_ZN2DB17IBlockInputStream4readEv
-_ZN2DB26ExpressionBlockInputStream8readImplEv
-_ZN2DB17IBlockInputStream4readEv
-_ZN2DB28AsynchronousBlockInputStream9calculateEv
-_ZNSt17_Function_handlerIFvvEZN2DB28AsynchronousBlockInputStream4nextEvEUlvE_E9_M_invokeERKSt9_Any_data
-_ZN14ThreadPoolImplI20ThreadFromGlobalPoolE6workerESt14_List_iteratorIS0_E
-_ZZN20ThreadFromGlobalPoolC4IZN14ThreadPoolImplIS_E12scheduleImplIvEET_St8functionIFvvEEiSt8optionalImEEUlvE1_JEEEOS4_DpOT0_ENKUlvE_clEv
-_ZN14ThreadPoolImplISt6threadE6workerESt14_List_iteratorIS0_E
-execute_native_thread_routine
-start_thread
-clone
-```
-
-## demangle {#demangle}
-
-Convertit un symbole que vous pouvez obtenir en utilisant le [adressetosymbol](#addresstosymbol) fonction au nom de la fonction c++.
-
-**Syntaxe**
-
-``` sql
-demangle(symbol)
-```
-
-**Paramètre**
-
--   `symbol` ([Chaîne](../../sql-reference/data-types/string.md)) — Symbol from an object file.
-
-**Valeur renvoyée**
-
--   Nom de la fonction C++.
--   Chaîne vide si un symbole n'est pas valide.
-
-Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-**Exemple**
-
-Activation des fonctions d'introspection:
-
-``` sql
-SET allow_introspection_functions=1
-```
-
-Sélection de la première chaîne de `trace_log` système de table:
-
-``` sql
-SELECT * FROM system.trace_log LIMIT 1 \G
-```
-
-``` text
-Row 1:
-──────
-event_date:    2019-11-20
-event_time:    2019-11-20 16:57:59
-revision:      54429
-timer_type:    Real
-thread_number: 48
-query_id:      724028bf-f550-45aa-910d-2af6212b94ac
-trace:         [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]
-```
-
-Le `trace` champ contient la trace de pile au moment de l'échantillonnage.
-
-Obtenir un nom de fonction pour une seule adresse:
-
-``` sql
-SELECT demangle(addressToSymbol(94138803686098)) \G
-```
-
-``` text
-Row 1:
-──────
-demangle(addressToSymbol(94138803686098)): DB::IAggregateFunctionHelper<DB::AggregateFunctionSum<unsigned long, unsigned long, DB::AggregateFunctionSumData<unsigned long> > >::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const
-```
-
-Application de la fonction à la trace de la pile entière:
-
-``` sql
-SELECT
-    arrayStringConcat(arrayMap(x -> demangle(addressToSymbol(x)), trace), '
') AS trace_functions
-FROM system.trace_log
-LIMIT 1
-\G
-```
-
-Le [arrayMap](higher-order-functions.md#higher_order_functions-array-map) permet de traiter chaque élément individuel de l' `trace` tableau par la `demangle` fonction. Le résultat de ce traitement que vous voyez dans l' `trace_functions` colonne de sortie.
-
-``` text
-Row 1:
-──────
-trace_functions: DB::IAggregateFunctionHelper<DB::AggregateFunctionSum<unsigned long, unsigned long, DB::AggregateFunctionSumData<unsigned long> > >::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const
-DB::Aggregator::executeWithoutKeyImpl(char*&, unsigned long, DB::Aggregator::AggregateFunctionInstruction*, DB::Arena*) const
-DB::Aggregator::executeOnBlock(std::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >, unsigned long, DB::AggregatedDataVariants&, std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >&, std::vector<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >, std::allocator<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> > > >&, bool&)
-DB::Aggregator::executeOnBlock(DB::Block const&, DB::AggregatedDataVariants&, std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >&, std::vector<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> >, std::allocator<std::vector<DB::IColumn const*, std::allocator<DB::IColumn const*> > > >&, bool&)
-DB::Aggregator::execute(std::shared_ptr<DB::IBlockInputStream> const&, DB::AggregatedDataVariants&)
-DB::AggregatingBlockInputStream::readImpl()
-DB::IBlockInputStream::read()
-DB::ExpressionBlockInputStream::readImpl()
-DB::IBlockInputStream::read()
-DB::ExpressionBlockInputStream::readImpl()
-DB::IBlockInputStream::read()
-DB::AsynchronousBlockInputStream::calculate()
-std::_Function_handler<void (), DB::AsynchronousBlockInputStream::next()::{lambda()#1}>::_M_invoke(std::_Any_data const&)
-ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::_List_iterator<ThreadFromGlobalPool>)
-ThreadFromGlobalPool::ThreadFromGlobalPool<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}&&)::{lambda()#1}::operator()() const
-ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>)
-execute_native_thread_routine
-start_thread
-clone
-```
diff --git a/docs/fr/sql-reference/functions/ip-address-functions.md b/docs/fr/sql-reference/functions/ip-address-functions.md
deleted file mode 100644
index 8beb40a534b9..000000000000
--- a/docs/fr/sql-reference/functions/ip-address-functions.md
+++ /dev/null
@@ -1,248 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 55
-toc_title: Travailler avec des adresses IP
----
-
-# Fonctions pour travailler avec des adresses IP {#functions-for-working-with-ip-addresses}
-
-## IPv4NumToString (num) {#ipv4numtostringnum}
-
-Prend un numéro UInt32. Interprète comme une adresse IPv4 dans big endian. Renvoie une chaîne contenant l'adresse IPv4 correspondante au format A. B. C. d (Nombres séparés par des points sous forme décimale).
-
-## IPv4StringToNum (s) {#ipv4stringtonums}
-
-La fonction inverse de IPv4NumToString. Si L'adresse IPv4 a un format non valide, elle renvoie 0.
-
-## IPv4NumToStringClassC(num) {#ipv4numtostringclasscnum}
-
-Similaire à IPv4NumToString, mais en utilisant xxx au lieu du dernier octet.
-
-Exemple:
-
-``` sql
-SELECT
-    IPv4NumToStringClassC(ClientIP) AS k,
-    count() AS c
-FROM test.hits
-GROUP BY k
-ORDER BY c DESC
-LIMIT 10
-```
-
-``` text
-┌─k──────────────┬─────c─┐
-│ 83.149.9.xxx   │ 26238 │
-│ 217.118.81.xxx │ 26074 │
-│ 213.87.129.xxx │ 25481 │
-│ 83.149.8.xxx   │ 24984 │
-│ 217.118.83.xxx │ 22797 │
-│ 78.25.120.xxx  │ 22354 │
-│ 213.87.131.xxx │ 21285 │
-│ 78.25.121.xxx  │ 20887 │
-│ 188.162.65.xxx │ 19694 │
-│ 83.149.48.xxx  │ 17406 │
-└────────────────┴───────┘
-```
-
-Depuis l'utilisation de ‘xxx’ est très inhabituel, cela peut être changé à l'avenir. Nous vous recommandons de ne pas compter sur le format exact de ce fragment.
-
-### IPv6NumToString (x) {#ipv6numtostringx}
-
-Accepte une valeur FixedString (16) contenant L'adresse IPv6 au format binaire. Renvoie une chaîne contenant cette adresse au format texte.
-Les adresses IPv4 mappées IPv6 sont sorties au format:: ffff: 111.222.33.44. Exemple:
-
-``` sql
-SELECT IPv6NumToString(toFixedString(unhex('2A0206B8000000000000000000000011'), 16)) AS addr
-```
-
-``` text
-┌─addr─────────┐
-│ 2a02:6b8::11 │
-└──────────────┘
-```
-
-``` sql
-SELECT
-    IPv6NumToString(ClientIP6 AS k),
-    count() AS c
-FROM hits_all
-WHERE EventDate = today() AND substring(ClientIP6, 1, 12) != unhex('00000000000000000000FFFF')
-GROUP BY k
-ORDER BY c DESC
-LIMIT 10
-```
-
-``` text
-┌─IPv6NumToString(ClientIP6)──────────────┬─────c─┐
-│ 2a02:2168:aaa:bbbb::2                   │ 24695 │
-│ 2a02:2698:abcd:abcd:abcd:abcd:8888:5555 │ 22408 │
-│ 2a02:6b8:0:fff::ff                      │ 16389 │
-│ 2a01:4f8:111:6666::2                    │ 16016 │
-│ 2a02:2168:888:222::1                    │ 15896 │
-│ 2a01:7e00::ffff:ffff:ffff:222           │ 14774 │
-│ 2a02:8109:eee:ee:eeee:eeee:eeee:eeee    │ 14443 │
-│ 2a02:810b:8888:888:8888:8888:8888:8888  │ 14345 │
-│ 2a02:6b8:0:444:4444:4444:4444:4444      │ 14279 │
-│ 2a01:7e00::ffff:ffff:ffff:ffff          │ 13880 │
-└─────────────────────────────────────────┴───────┘
-```
-
-``` sql
-SELECT
-    IPv6NumToString(ClientIP6 AS k),
-    count() AS c
-FROM hits_all
-WHERE EventDate = today()
-GROUP BY k
-ORDER BY c DESC
-LIMIT 10
-```
-
-``` text
-┌─IPv6NumToString(ClientIP6)─┬──────c─┐
-│ ::ffff:94.26.111.111       │ 747440 │
-│ ::ffff:37.143.222.4        │ 529483 │
-│ ::ffff:5.166.111.99        │ 317707 │
-│ ::ffff:46.38.11.77         │ 263086 │
-│ ::ffff:79.105.111.111      │ 186611 │
-│ ::ffff:93.92.111.88        │ 176773 │
-│ ::ffff:84.53.111.33        │ 158709 │
-│ ::ffff:217.118.11.22       │ 154004 │
-│ ::ffff:217.118.11.33       │ 148449 │
-│ ::ffff:217.118.11.44       │ 148243 │
-└────────────────────────────┴────────┘
-```
-
-## IPv6StringToNum (s) {#ipv6stringtonums}
-
-La fonction inverse de IPv6NumToString. Si L'adresse IPv6 a un format non valide, elle renvoie une chaîne d'octets null.
-HEX peut être en majuscules ou en minuscules.
-
-## IPv4ToIPv6 (x) {#ipv4toipv6x}
-
-Prend un `UInt32` nombre. Interprète comme une adresse IPv4 dans [big endian](https://en.wikipedia.org/wiki/Endianness). Retourne un `FixedString(16)` valeur contenant l'adresse IPv6 au format binaire. Exemple:
-
-``` sql
-SELECT IPv6NumToString(IPv4ToIPv6(IPv4StringToNum('192.168.0.1'))) AS addr
-```
-
-``` text
-┌─addr───────────────┐
-│ ::ffff:192.168.0.1 │
-└────────────────────┘
-```
-
-## cutIPv6 (x, bytesToCutForIPv6, bytesToCutForIPv4) {#cutipv6x-bytestocutforipv6-bytestocutforipv4}
-
-Accepte une valeur FixedString (16) contenant L'adresse IPv6 au format binaire. Renvoie une chaîne contenant l'adresse du nombre spécifié d'octets retiré au format texte. Exemple:
-
-``` sql
-WITH
-    IPv6StringToNum('2001:0DB8:AC10:FE01:FEED:BABE:CAFE:F00D') AS ipv6,
-    IPv4ToIPv6(IPv4StringToNum('192.168.0.1')) AS ipv4
-SELECT
-    cutIPv6(ipv6, 2, 0),
-    cutIPv6(ipv4, 0, 2)
-```
-
-``` text
-┌─cutIPv6(ipv6, 2, 0)─────────────────┬─cutIPv6(ipv4, 0, 2)─┐
-│ 2001:db8:ac10:fe01:feed:babe:cafe:0 │ ::ffff:192.168.0.0  │
-└─────────────────────────────────────┴─────────────────────┘
-```
-
-## Ipv4cirtorange (ipv4, Cidr), {#ipv4cidrtorangeipv4-cidr}
-
-Accepte un IPv4 et une valeur UInt8 contenant [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing). Renvoie un tuple avec deux IPv4 contenant la plage inférieure et la plage supérieure du sous-réseau.
-
-``` sql
-SELECT IPv4CIDRToRange(toIPv4('192.168.5.2'), 16)
-```
-
-``` text
-┌─IPv4CIDRToRange(toIPv4('192.168.5.2'), 16)─┐
-│ ('192.168.0.0','192.168.255.255')          │
-└────────────────────────────────────────────┘
-```
-
-## Ipv6cirtorange (ipv6, Cidr), {#ipv6cidrtorangeipv6-cidr}
-
-Accepte un IPv6 et une valeur UInt8 contenant le CIDR. Renvoie un tuple avec deux IPv6 contenant la plage inférieure et la plage supérieure du sous-réseau.
-
-``` sql
-SELECT IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32);
-```
-
-``` text
-┌─IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32)─┐
-│ ('2001:db8::','2001:db8:ffff:ffff:ffff:ffff:ffff:ffff')                │
-└────────────────────────────────────────────────────────────────────────┘
-```
-
-## toipv4 (chaîne) {#toipv4string}
-
-Un alias `IPv4StringToNum()` cela prend une forme de chaîne D'adresse IPv4 et renvoie la valeur de [IPv4](../../sql-reference/data-types/domains/ipv4.md) type, qui est binaire égal à la valeur renvoyée par `IPv4StringToNum()`.
-
-``` sql
-WITH
-    '171.225.130.45' as IPv4_string
-SELECT
-    toTypeName(IPv4StringToNum(IPv4_string)),
-    toTypeName(toIPv4(IPv4_string))
-```
-
-``` text
-┌─toTypeName(IPv4StringToNum(IPv4_string))─┬─toTypeName(toIPv4(IPv4_string))─┐
-│ UInt32                                   │ IPv4                            │
-└──────────────────────────────────────────┴─────────────────────────────────┘
-```
-
-``` sql
-WITH
-    '171.225.130.45' as IPv4_string
-SELECT
-    hex(IPv4StringToNum(IPv4_string)),
-    hex(toIPv4(IPv4_string))
-```
-
-``` text
-┌─hex(IPv4StringToNum(IPv4_string))─┬─hex(toIPv4(IPv4_string))─┐
-│ ABE1822D                          │ ABE1822D                 │
-└───────────────────────────────────┴──────────────────────────┘
-```
-
-## toipv6 (chaîne) {#toipv6string}
-
-Un alias `IPv6StringToNum()` cela prend une forme de chaîne D'adresse IPv6 et renvoie la valeur de [IPv6](../../sql-reference/data-types/domains/ipv6.md) type, qui est binaire égal à la valeur renvoyée par `IPv6StringToNum()`.
-
-``` sql
-WITH
-    '2001:438:ffff::407d:1bc1' as IPv6_string
-SELECT
-    toTypeName(IPv6StringToNum(IPv6_string)),
-    toTypeName(toIPv6(IPv6_string))
-```
-
-``` text
-┌─toTypeName(IPv6StringToNum(IPv6_string))─┬─toTypeName(toIPv6(IPv6_string))─┐
-│ FixedString(16)                          │ IPv6                            │
-└──────────────────────────────────────────┴─────────────────────────────────┘
-```
-
-``` sql
-WITH
-    '2001:438:ffff::407d:1bc1' as IPv6_string
-SELECT
-    hex(IPv6StringToNum(IPv6_string)),
-    hex(toIPv6(IPv6_string))
-```
-
-``` text
-┌─hex(IPv6StringToNum(IPv6_string))─┬─hex(toIPv6(IPv6_string))─────────┐
-│ 20010438FFFF000000000000407D1BC1  │ 20010438FFFF000000000000407D1BC1 │
-└───────────────────────────────────┴──────────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/ip_address_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/json-functions.md b/docs/fr/sql-reference/functions/json-functions.md
deleted file mode 100644
index 5f92c99d0f50..000000000000
--- a/docs/fr/sql-reference/functions/json-functions.md
+++ /dev/null
@@ -1,297 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 56
-toc_title: Travailler avec JSON
----
-
-# Fonctions pour travailler avec JSON {#functions-for-working-with-json}
-
-Dans Yandex.Metrica, JSON est transmis par les utilisateurs en tant que paramètres de session. Il y a quelques fonctions spéciales pour travailler avec ce JSON. (Bien que dans la plupart des cas, les JSONs soient en outre prétraités et les valeurs résultantes sont placées dans des colonnes séparées dans leur format traité.) Toutes ces fonctions sont basées sur des hypothèses fortes sur ce que le JSON peut être, mais elles essaient de faire le moins possible pour faire le travail.
-
-Les hypothèses suivantes sont apportées:
-
-1.  Le nom du champ (argument de fonction) doit être une constante.
-2.  Le nom du champ est en quelque sorte codé canoniquement dans JSON. Exemple: `visitParamHas('{"abc":"def"}', 'abc') = 1`, mais `visitParamHas('{"\\u0061\\u0062\\u0063":"def"}', 'abc') = 0`
-3.  Les champs sont recherchés à n'importe quel niveau d'imbrication, sans discrimination. S'il y a plusieurs champs correspondants, la première occurrence est utilisé.
-4.  Le JSON n'a pas de caractères d'espace en dehors des littéraux de chaîne.
-
-## visitParamHas(params, nom) {#visitparamhasparams-name}
-
-Vérifie s'il existe un champ avec ‘name’ nom.
-
-## visitParamExtractUInt(params, nom) {#visitparamextractuintparams-name}
-
-Analyse UInt64 à partir de la valeur du champ nommé ‘name’. Si c'est un champ de type chaîne, il tente d'analyser un numéro à partir du début de la chaîne. Si le champ n'existe pas, ou s'il existe mais ne contient pas de nombre, il renvoie 0.
-
-## visitParamExtractInt(params, name) {#visitparamextractintparams-name}
-
-Le même que pour Int64.
-
-## visitParamExtractFloat(params, nom) {#visitparamextractfloatparams-name}
-
-Le même que pour Float64.
-
-## visitParamExtractBool(params, nom) {#visitparamextractboolparams-name}
-
-Analyse d'une valeur vrai/faux. Le résultat est UInt8.
-
-## visitParamExtractRaw(params, nom) {#visitparamextractrawparams-name}
-
-Retourne la valeur d'un champ, y compris les séparateurs.
-
-Exemple:
-
-``` sql
-visitParamExtractRaw('{"abc":"\
\\u0000"}', 'abc') = '"\
\\u0000"'
-visitParamExtractRaw('{"abc":{"def":[1,2,3]}}', 'abc') = '{"def":[1,2,3]}'
-```
-
-## visitParamExtractString(params, nom) {#visitparamextractstringparams-name}
-
-Analyse la chaîne entre guillemets doubles. La valeur est sans échappement. Si l'échappement échoue, il renvoie une chaîne vide.
-
-Exemple:
-
-``` sql
-visitParamExtractString('{"abc":"\
\\u0000"}', 'abc') = '
\0'
-visitParamExtractString('{"abc":"\\u263a"}', 'abc') = '☺'
-visitParamExtractString('{"abc":"\\u263"}', 'abc') = ''
-visitParamExtractString('{"abc":"hello}', 'abc') = ''
-```
-
-Il n'y a actuellement aucun support pour les points de code dans le format `\uXXXX\uYYYY` qui ne proviennent pas du plan multilingue de base (ils sont convertis en CESU-8 au lieu de UTF-8).
-
-Les fonctions suivantes sont basées sur [simdjson](https://github.com/lemire/simdjson) conçu pour des exigences D'analyse JSON plus complexes. L'hypothèse 2 mentionnée ci-dessus s'applique toujours.
-
-## isValidJSON (json) {#isvalidjsonjson}
-
-Vérifie que la chaîne est un json valide.
-
-Exemple:
-
-``` sql
-SELECT isValidJSON('{"a": "hello", "b": [-100, 200.0, 300]}') = 1
-SELECT isValidJSON('not a json') = 0
-```
-
-## JSONHas(json\[, indices_or_keys\]…) {#jsonhasjson-indices-or-keys}
-
-Si la valeur existe dans le document JSON, `1` sera retourné.
-
-Si la valeur n'existe pas, `0` sera retourné.
-
-Exemple:
-
-``` sql
-SELECT JSONHas('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = 1
-SELECT JSONHas('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 4) = 0
-```
-
-`indices_or_keys` est une liste de zéro ou plusieurs arguments chacun d'entre eux peut être une chaîne ou un entier.
-
--   String = membre d'objet d'accès par clé.
--   Entier positif = accédez au n-ème membre / clé depuis le début.
--   Entier négatif = accédez au n-ème membre / clé à partir de la fin.
-
-Minimum de l'indice de l'élément est 1. Ainsi, l'élément 0 n'existe pas.
-
-Vous pouvez utiliser des entiers pour accéder à la fois aux tableaux JSON et aux objets JSON.
-
-Ainsi, par exemple:
-
-``` sql
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', 1) = 'a'
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', 2) = 'b'
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', -1) = 'b'
-SELECT JSONExtractKey('{"a": "hello", "b": [-100, 200.0, 300]}', -2) = 'a'
-SELECT JSONExtractString('{"a": "hello", "b": [-100, 200.0, 300]}', 1) = 'hello'
-```
-
-## JSONLength(json\[, indices_or_keys\]…) {#jsonlengthjson-indices-or-keys}
-
-Renvoie la longueur D'un tableau JSON ou d'un objet JSON.
-
-Si la valeur n'existe pas ou a un mauvais type, `0` sera retourné.
-
-Exemple:
-
-``` sql
-SELECT JSONLength('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = 3
-SELECT JSONLength('{"a": "hello", "b": [-100, 200.0, 300]}') = 2
-```
-
-## JSONType(json\[, indices_or_keys\]…) {#jsontypejson-indices-or-keys}
-
-De retour le type d'une valeur JSON.
-
-Si la valeur n'existe pas, `Null` sera retourné.
-
-Exemple:
-
-``` sql
-SELECT JSONType('{"a": "hello", "b": [-100, 200.0, 300]}') = 'Object'
-SELECT JSONType('{"a": "hello", "b": [-100, 200.0, 300]}', 'a') = 'String'
-SELECT JSONType('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = 'Array'
-```
-
-## JSONExtractUInt(json\[, indices_or_keys\]…) {#jsonextractuintjson-indices-or-keys}
-
-## JSONExtractInt(json\[, indices_or_keys\]…) {#jsonextractintjson-indices-or-keys}
-
-## JSONExtractFloat(json\[, indices_or_keys\]…) {#jsonextractfloatjson-indices-or-keys}
-
-## JSONExtractBool(json\[, indices_or_keys\]…) {#jsonextractbooljson-indices-or-keys}
-
-Analyse un JSON et extrait une valeur. Ces fonctions sont similaires à `visitParam` fonction.
-
-Si la valeur n'existe pas ou a un mauvais type, `0` sera retourné.
-
-Exemple:
-
-``` sql
-SELECT JSONExtractInt('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 1) = -100
-SELECT JSONExtractFloat('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 2) = 200.0
-SELECT JSONExtractUInt('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', -1) = 300
-```
-
-## JSONExtractString(json\[, indices_or_keys\]…) {#jsonextractstringjson-indices-or-keys}
-
-Analyse un JSON et extrait une chaîne. Cette fonction est similaire à `visitParamExtractString` fonction.
-
-Si la valeur n'existe pas ou a un mauvais type, une chaîne vide est retournée.
-
-La valeur est sans échappement. Si l'échappement échoue, il renvoie une chaîne vide.
-
-Exemple:
-
-``` sql
-SELECT JSONExtractString('{"a": "hello", "b": [-100, 200.0, 300]}', 'a') = 'hello'
-SELECT JSONExtractString('{"abc":"\
\\u0000"}', 'abc') = '
\0'
-SELECT JSONExtractString('{"abc":"\\u263a"}', 'abc') = '☺'
-SELECT JSONExtractString('{"abc":"\\u263"}', 'abc') = ''
-SELECT JSONExtractString('{"abc":"hello}', 'abc') = ''
-```
-
-## JSONExtract(json\[, indices_or_keys…\], Return_type) {#jsonextractjson-indices-or-keys-return-type}
-
-Analyse un JSON et extrait une valeur du type de données clickhouse donné.
-
-C'est une généralisation de la précédente `JSONExtract<type>` fonction.
-Cela signifie
-`JSONExtract(..., 'String')` retourne exactement le même que `JSONExtractString()`,
-`JSONExtract(..., 'Float64')` retourne exactement le même que `JSONExtractFloat()`.
-
-Exemple:
-
-``` sql
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'Tuple(String, Array(Float64))') = ('hello',[-100,200,300])
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'Tuple(b Array(Float64), a String)') = ([-100,200,300],'hello')
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 'Array(Nullable(Int8))') = [-100, NULL, NULL]
-SELECT JSONExtract('{"a": "hello", "b": [-100, 200.0, 300]}', 'b', 4, 'Nullable(Int64)') = NULL
-SELECT JSONExtract('{"passed": true}', 'passed', 'UInt8') = 1
-SELECT JSONExtract('{"day": "Thursday"}', 'day', 'Enum8(\'Sunday\' = 0, \'Monday\' = 1, \'Tuesday\' = 2, \'Wednesday\' = 3, \'Thursday\' = 4, \'Friday\' = 5, \'Saturday\' = 6)') = 'Thursday'
-SELECT JSONExtract('{"day": 5}', 'day', 'Enum8(\'Sunday\' = 0, \'Monday\' = 1, \'Tuesday\' = 2, \'Wednesday\' = 3, \'Thursday\' = 4, \'Friday\' = 5, \'Saturday\' = 6)') = 'Friday'
-```
-
-## JSONExtractKeysAndValues(json\[, indices_or_keys…\], Value_type) {#jsonextractkeysandvaluesjson-indices-or-keys-value-type}
-
-Analyse les paires clé-valeur à partir D'un JSON où les valeurs sont du type de données clickhouse donné.
-
-Exemple:
-
-``` sql
-SELECT JSONExtractKeysAndValues('{"x": {"a": 5, "b": 7, "c": 11}}', 'x', 'Int8') = [('a',5),('b',7),('c',11)]
-```
-
-## JSONExtractRaw(json\[, indices_or_keys\]…) {#jsonextractrawjson-indices-or-keys}
-
-Renvoie une partie de JSON en tant que chaîne non analysée.
-
-Si la pièce n'existe pas ou a un mauvais type, une chaîne vide est retournée.
-
-Exemple:
-
-``` sql
-SELECT JSONExtractRaw('{"a": "hello", "b": [-100, 200.0, 300]}', 'b') = '[-100, 200.0, 300]'
-```
-
-## JSONExtractArrayRaw(json\[, indices_or_keys…\]) {#jsonextractarrayrawjson-indices-or-keys}
-
-Retourne un tableau avec des éléments de tableau JSON, chacun représenté comme une chaîne non analysée.
-
-Si la pièce n'existe pas ou n'est pas de tableau, un tableau vide sera retournée.
-
-Exemple:
-
-``` sql
-SELECT JSONExtractArrayRaw('{"a": "hello", "b": [-100, 200.0, "hello"]}', 'b') = ['-100', '200.0', '"hello"']'
-```
-
-## JSONExtractKeysAndValuesRaw {#json-extract-keys-and-values-raw}
-
-Extrait les données brutes d'un objet JSON.
-
-**Syntaxe**
-
-``` sql
-JSONExtractKeysAndValuesRaw(json[, p, a, t, h])
-```
-
-**Paramètre**
-
--   `json` — [Chaîne](../data-types/string.md) avec JSON valide.
--   `p, a, t, h` — Comma-separated indices or keys that specify the path to the inner field in a nested JSON object. Each argument can be either a [chaîne](../data-types/string.md) pour obtenir le champ par la touche ou un [entier](../data-types/int-uint.md) pour obtenir le N-ème champ (indexé à partir de 1, les entiers négatifs comptent à partir de la fin). S'il n'est pas défini, le JSON entier est analysé en tant qu'objet de niveau supérieur. Paramètre facultatif.
-
-**Valeurs renvoyées**
-
--   Tableau avec `('key', 'value')` tuple. Les deux membres du tuple sont des chaînes.
--   Tableau vide si l'objet demandé n'existe pas, ou entrée JSON n'est pas valide.
-
-Type: [Tableau](../data-types/array.md)([Tuple](../data-types/tuple.md)([Chaîne](../data-types/string.md), [Chaîne](../data-types/string.md)).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}')
-```
-
-Résultat:
-
-``` text
-┌─JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}')─┐
-│ [('a','[-100,200]'),('b','{"c":{"d":"hello","f":"world"}}')]                                 │
-└──────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', 'b')
-```
-
-Résultat:
-
-``` text
-┌─JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', 'b')─┐
-│ [('c','{"d":"hello","f":"world"}')]                                                               │
-└───────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', -1, 'c')
-```
-
-Résultat:
-
-``` text
-┌─JSONExtractKeysAndValuesRaw('{"a": [-100, 200.0], "b":{"c": {"d": "hello", "f": "world"}}}', -1, 'c')─┐
-│ [('d','"hello"'),('f','"world"')]                                                                     │
-└───────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/json_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/logical-functions.md b/docs/fr/sql-reference/functions/logical-functions.md
deleted file mode 100644
index d01d9e020885..000000000000
--- a/docs/fr/sql-reference/functions/logical-functions.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: Logique
----
-
-# Les Fonctions Logiques {#logical-functions}
-
-Les fonctions logiques acceptent tous les types numériques, mais renvoient un nombre UInt8 égal à 0 ou 1.
-
-Zéro comme argument est considéré “false,” alors que toute valeur non nulle est considérée comme “true”.
-
-## et, et opérateur {#and-and-operator}
-
-## ou, ou opérateur {#or-or-operator}
-
-## pas, pas opérateur {#not-not-operator}
-
-## xor {#xor}
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/logical_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/machine-learning-functions.md b/docs/fr/sql-reference/functions/machine-learning-functions.md
deleted file mode 100644
index 2212e0caa5ad..000000000000
--- a/docs/fr/sql-reference/functions/machine-learning-functions.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 64
-toc_title: Fonctions D'Apprentissage Automatique
----
-
-# Fonctions D'Apprentissage Automatique {#machine-learning-functions}
-
-## evalMLMethod (prédiction) {#machine_learning_methods-evalmlmethod}
-
-Prédiction utilisant des modèles de régression ajustés utilise `evalMLMethod` fonction. Voir le lien dans la `linearRegression`.
-
-### Régression Linéaire Stochastique {#stochastic-linear-regression}
-
-Le [stochasticLinearRegression](../../sql-reference/aggregate-functions/reference.md#agg_functions-stochasticlinearregression) la fonction d'agrégat implémente une méthode de descente de gradient stochastique utilisant un modèle linéaire et une fonction de perte MSE. Utiliser `evalMLMethod` prédire sur de nouvelles données.
-
-### Régression Logistique Stochastique {#stochastic-logistic-regression}
-
-Le [stochasticLogisticRegression](../../sql-reference/aggregate-functions/reference.md#agg_functions-stochasticlogisticregression) la fonction d'agrégation implémente la méthode de descente de gradient stochastique pour le problème de classification binaire. Utiliser `evalMLMethod` prédire sur de nouvelles données.
diff --git a/docs/fr/sql-reference/functions/math-functions.md b/docs/fr/sql-reference/functions/math-functions.md
deleted file mode 100644
index f5dff150caae..000000000000
--- a/docs/fr/sql-reference/functions/math-functions.md
+++ /dev/null
@@ -1,116 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: "Math\xE9matique"
----
-
-# Fonctions Mathématiques {#mathematical-functions}
-
-Toutes les fonctions renvoient un nombre Float64. La précision du résultat est proche de la précision maximale possible, mais le résultat peut ne pas coïncider avec le nombre représentable de la machine le plus proche du nombre réel correspondant.
-
-## e() {#e}
-
-Renvoie un nombre Float64 proche du nombre E.
-
-## pi() {#pi}
-
-Returns a Float64 number that is close to the number π.
-
-## exp (x) {#expx}
-
-Accepte un argument numérique et renvoie un Float64 nombre proche de l'exposant de l'argument.
-
-## log(x), ln (x) {#logx-lnx}
-
-Accepte un argument numérique et renvoie un nombre Float64 proche du logarithme naturel de l'argument.
-
-## exp2 (x) {#exp2x}
-
-Accepte un argument numérique et renvoie un nombre Float64 proche de 2 à la puissance de X.
-
-## log2 (x) {#log2x}
-
-Accepte un argument numérique et renvoie un Float64 nombre proximité du logarithme binaire de l'argument.
-
-## exp10 (x) {#exp10x}
-
-Accepte un argument numérique et renvoie un nombre Float64 proche de 10 à la puissance de X.
-
-## log10 (x) {#log10x}
-
-Accepte un argument numérique et renvoie un nombre Float64 proche du logarithme décimal de l'argument.
-
-## sqrt (x) {#sqrtx}
-
-Accepte un argument numérique et renvoie un Float64 nombre proche de la racine carrée de l'argument.
-
-## cbrt (x) {#cbrtx}
-
-Accepte un argument numérique et renvoie un Float64 nombre proche de la racine cubique de l'argument.
-
-## erf (x) {#erfx}
-
-Si ‘x’ est non négatif, alors `erf(x / σ√2)` est la probabilité qu'une variable aléatoire ayant une distribution normale avec un écart type ‘σ’ prend la valeur qui est séparée de la valeur attendue par plus de ‘x’.
-
-Exemple (règle de trois sigma):
-
-``` sql
-SELECT erf(3 / sqrt(2))
-```
-
-``` text
-┌─erf(divide(3, sqrt(2)))─┐
-│      0.9973002039367398 │
-└─────────────────────────┘
-```
-
-## erfc (x) {#erfcx}
-
-Accepte un argument numérique et renvoie un nombre Float64 proche de 1-erf (x), mais sans perte de précision pour ‘x’ valeur.
-
-## lgamma (x) {#lgammax}
-
-Le logarithme de la fonction gamma.
-
-## tgamma (x) {#tgammax}
-
-La fonction Gamma.
-
-## sin (x) {#sinx}
-
-Sine.
-
-## cos (x) {#cosx}
-
-Cosinus.
-
-## tan (x) {#tanx}
-
-Tangente.
-
-## asin (x) {#asinx}
-
-Le sinus d'arc.
-
-## acos (x) {#acosx}
-
-Le cosinus de l'arc.
-
-## atan (x) {#atanx}
-
-L'arc tangente.
-
-## pow(x, y), la puissance(x, y) {#powx-y-powerx-y}
-
-Prend deux arguments numériques x et Y. renvoie un nombre Float64 proche de x à la puissance de Y.
-
-## intExp2 {#intexp2}
-
-Accepte un argument numérique et renvoie un nombre UInt64 proche de 2 à la puissance de X.
-
-## intExp10 {#intexp10}
-
-Accepte un argument numérique et renvoie un nombre UInt64 proche de 10 à la puissance de X.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/math_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/other-functions.md b/docs/fr/sql-reference/functions/other-functions.md
deleted file mode 100644
index e5c6abedd751..000000000000
--- a/docs/fr/sql-reference/functions/other-functions.md
+++ /dev/null
@@ -1,1205 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 66
-toc_title: Autre
----
-
-# D'Autres Fonctions {#other-functions}
-
-## hôte() {#hostname}
-
-Renvoie une chaîne avec le nom de l'hôte sur lequel cette fonction a été exécutée. Pour le traitement distribué, c'est le nom du serveur distant, si la fonction est exécutée sur un serveur distant.
-
-## getMacro {#getmacro}
-
-Obtient une valeur nommée à partir [macro](../../operations/server-configuration-parameters/settings.md#macros) la section de la configuration du serveur.
-
-**Syntaxe**
-
-``` sql
-getMacro(name);
-```
-
-**Paramètre**
-
--   `name` — Name to retrieve from the `macros` section. [Chaîne](../../sql-reference/data-types/string.md#string).
-
-**Valeur renvoyée**
-
--   Valeur de la macro spécifiée.
-
-Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-**Exemple**
-
-Exemple `macros` section dans le fichier de configuration du serveur:
-
-``` xml
-<macros>
-    <test>Value</test>
-</macros>
-```
-
-Requête:
-
-``` sql
-SELECT getMacro('test');
-```
-
-Résultat:
-
-``` text
-┌─getMacro('test')─┐
-│ Value            │
-└──────────────────┘
-```
-
-Une méthode alternative pour obtenir la même valeur:
-
-``` sql
-SELECT * FROM system.macros
-WHERE macro = 'test';
-```
-
-``` text
-┌─macro─┬─substitution─┐
-│ test  │ Value        │
-└───────┴──────────────┘
-```
-
-## FQDN {#fqdn}
-
-Retourne le nom de domaine pleinement qualifié.
-
-**Syntaxe**
-
-``` sql
-fqdn();
-```
-
-Cette fonction est insensible à la casse.
-
-**Valeur renvoyée**
-
--   Chaîne avec le nom de domaine complet.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT FQDN();
-```
-
-Résultat:
-
-``` text
-┌─FQDN()──────────────────────────┐
-│ clickhouse.ru-central1.internal │
-└─────────────────────────────────┘
-```
-
-## basename {#basename}
-
-Extrait la partie finale d'une chaîne après la dernière barre oblique ou barre oblique inverse. Cette fonction est souvent utilisée pour extraire le nom de fichier d'un chemin.
-
-``` sql
-basename( expr )
-```
-
-**Paramètre**
-
--   `expr` — Expression resulting in a [Chaîne](../../sql-reference/data-types/string.md) type de valeur. Tous les antislashs doivent être échappés dans la valeur résultante.
-
-**Valeur Renvoyée**
-
-Une chaîne de caractères qui contient:
-
--   La partie finale d'une chaîne après la dernière barre oblique ou barre oblique inverse.
-
-        If the input string contains a path ending with slash or backslash, for example, `/` or `c:\`, the function returns an empty string.
-
--   La chaîne d'origine s'il n'y a pas de barres obliques ou de barres obliques inverses.
-
-**Exemple**
-
-``` sql
-SELECT 'some/long/path/to/file' AS a, basename(a)
-```
-
-``` text
-┌─a──────────────────────┬─basename('some\\long\\path\\to\\file')─┐
-│ some\long\path\to\file │ file                                   │
-└────────────────────────┴────────────────────────────────────────┘
-```
-
-``` sql
-SELECT 'some\\long\\path\\to\\file' AS a, basename(a)
-```
-
-``` text
-┌─a──────────────────────┬─basename('some\\long\\path\\to\\file')─┐
-│ some\long\path\to\file │ file                                   │
-└────────────────────────┴────────────────────────────────────────┘
-```
-
-``` sql
-SELECT 'some-file-name' AS a, basename(a)
-```
-
-``` text
-┌─a──────────────┬─basename('some-file-name')─┐
-│ some-file-name │ some-file-name             │
-└────────────────┴────────────────────────────┘
-```
-
-## visibleWidth (x) {#visiblewidthx}
-
-Calcule la largeur approximative lors de la sortie des valeurs vers la console au format texte (séparé par des tabulations).
-Cette fonction est utilisée par le système pour implémenter de jolis formats.
-
-`NULL` est représenté comme une chaîne correspondant à `NULL` dans `Pretty` format.
-
-``` sql
-SELECT visibleWidth(NULL)
-```
-
-``` text
-┌─visibleWidth(NULL)─┐
-│                  4 │
-└────────────────────┘
-```
-
-## toTypeName (x) {#totypenamex}
-
-Renvoie une chaîne contenant le nom du type de l'argument passé.
-
-Si `NULL` est passé à la fonction en entrée, puis il renvoie le `Nullable(Nothing)` type, ce qui correspond à un interne `NULL` représentation à ClickHouse.
-
-## la taille de bloc() {#function-blocksize}
-
-Récupère la taille du bloc.
-Dans ClickHouse, les requêtes sont toujours exécutées sur des blocs (ensembles de parties de colonne). Cette fonction permet d'obtenir la taille du bloc pour lequel vous l'avez appelé.
-
-## matérialiser (x) {#materializex}
-
-Transforme une constante dans une colonne contenant une seule valeur.
-Dans ClickHouse, les colonnes complètes et les constantes sont représentées différemment en mémoire. Les fonctions fonctionnent différemment pour les arguments constants et les arguments normaux (un code différent est exécuté), bien que le résultat soit presque toujours le même. Cette fonction sert à déboguer ce comportement.
-
-## ignore(…) {#ignore}
-
-Accepte tous les arguments, y compris `NULL`. Renvoie toujours 0.
-Cependant, l'argument est toujours évalué. Cela peut être utilisé pour les benchmarks.
-
-## sommeil(secondes) {#sleepseconds}
-
-Dormir ‘seconds’ secondes sur chaque bloc de données. Vous pouvez spécifier un nombre entier ou un nombre à virgule flottante.
-
-## sleepEachRow (secondes) {#sleepeachrowseconds}
-
-Dormir ‘seconds’ secondes sur chaque ligne. Vous pouvez spécifier un nombre entier ou un nombre à virgule flottante.
-
-## currentDatabase() {#currentdatabase}
-
-Retourne le nom de la base de données actuelle.
-Vous pouvez utiliser cette fonction dans les paramètres du moteur de table dans une requête CREATE TABLE où vous devez spécifier la base de données.
-
-## currentUser() {#other-function-currentuser}
-
-Renvoie la connexion de l'utilisateur actuel. La connexion de l'utilisateur, cette requête initiée, sera renvoyée en cas de requête distibuted.
-
-``` sql
-SELECT currentUser();
-```
-
-Alias: `user()`, `USER()`.
-
-**Valeurs renvoyées**
-
--   Connexion de l'utilisateur actuel.
--   Connexion de l'utilisateur qui a lancé la requête en cas de requête distribuée.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT currentUser();
-```
-
-Résultat:
-
-``` text
-┌─currentUser()─┐
-│ default       │
-└───────────────┘
-```
-
-## isConstant {#is-constant}
-
-Vérifie si l'argument est une expression constante.
-
-A constant expression means an expression whose resulting value is known at the query analysis (i.e. before execution). For example, expressions over [littéral](../syntax.md#literals) sont des expressions constantes.
-
-La fonction est destinée au développement, au débogage et à la démonstration.
-
-**Syntaxe**
-
-``` sql
-isConstant(x)
-```
-
-**Paramètre**
-
--   `x` — Expression to check.
-
-**Valeurs renvoyées**
-
--   `1` — `x` est constante.
--   `0` — `x` est non constante.
-
-Type: [UInt8](../data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT isConstant(x + 1) FROM (SELECT 43 AS x)
-```
-
-Résultat:
-
-``` text
-┌─isConstant(plus(x, 1))─┐
-│                      1 │
-└────────────────────────┘
-```
-
-Requête:
-
-``` sql
-WITH 3.14 AS pi SELECT isConstant(cos(pi))
-```
-
-Résultat:
-
-``` text
-┌─isConstant(cos(pi))─┐
-│                   1 │
-└─────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT isConstant(number) FROM numbers(1)
-```
-
-Résultat:
-
-``` text
-┌─isConstant(number)─┐
-│                  0 │
-└────────────────────┘
-```
-
-## isFinite (x) {#isfinitex}
-
-Accepte Float32 et Float64 et renvoie UInt8 égal à 1 si l'argument n'est pas infini et pas un NaN, sinon 0.
-
-## isInfinite (x) {#isinfinitex}
-
-Accepte Float32 et Float64 et renvoie UInt8 égal à 1 si l'argument est infini, sinon 0. Notez que 0 est retourné pour un NaN.
-
-## ifNotFinite {#ifnotfinite}
-
-Vérifie si la valeur à virgule flottante est finie.
-
-**Syntaxe**
-
-    ifNotFinite(x,y)
-
-**Paramètre**
-
--   `x` — Value to be checked for infinity. Type: [Flottant\*](../../sql-reference/data-types/float.md).
--   `y` — Fallback value. Type: [Flottant\*](../../sql-reference/data-types/float.md).
-
-**Valeur renvoyée**
-
--   `x` si `x` est finie.
--   `y` si `x` n'est pas finie.
-
-**Exemple**
-
-Requête:
-
-    SELECT 1/0 as infimum, ifNotFinite(infimum,42)
-
-Résultat:
-
-    ┌─infimum─┬─ifNotFinite(divide(1, 0), 42)─┐
-    │     inf │                            42 │
-    └─────────┴───────────────────────────────┘
-
-Vous pouvez obtenir un résultat similaire en utilisant [opérateur ternaire](conditional-functions.md#ternary-operator): `isFinite(x) ? x : y`.
-
-## isNaN (x) {#isnanx}
-
-Accepte Float32 et Float64 et renvoie UInt8 égal à 1 si l'argument est un NaN, sinon 0.
-
-## hasColumnInTable(\[‘hostname’\[, ‘username’\[, ‘password’\]\],\] ‘database’, ‘table’, ‘column’) {#hascolumnintablehostname-username-password-database-table-column}
-
-Accepte les chaînes constantes: nom de la base de données, nom de la table et nom de la colonne. Renvoie une expression constante UInt8 égale à 1 s'il y a une colonne, sinon 0. Si le paramètre hostname est défini, le test s'exécutera sur un serveur distant.
-La fonction renvoie une exception si la table n'existe pas.
-Pour les éléments imbriqués structure des données, la fonction vérifie l'existence d'une colonne. Pour la structure de données imbriquée elle-même, la fonction renvoie 0.
-
-## bar {#function-bar}
-
-Permet de construire un diagramme unicode-art.
-
-`bar(x, min, max, width)` dessine une bande avec une largeur proportionnelle à `(x - min)` et égale à `width` les caractères lors de la `x = max`.
-
-Paramètre:
-
--   `x` — Size to display.
--   `min, max` — Integer constants. The value must fit in `Int64`.
--   `width` — Constant, positive integer, can be fractional.
-
-La bande dessinée avec précision à un huitième d'un symbole.
-
-Exemple:
-
-``` sql
-SELECT
-    toHour(EventTime) AS h,
-    count() AS c,
-    bar(c, 0, 600000, 20) AS bar
-FROM test.hits
-GROUP BY h
-ORDER BY h ASC
-```
-
-``` text
-┌──h─┬──────c─┬─bar────────────────┐
-│  0 │ 292907 │ █████████▋         │
-│  1 │ 180563 │ ██████             │
-│  2 │ 114861 │ ███▋               │
-│  3 │  85069 │ ██▋                │
-│  4 │  68543 │ ██▎                │
-│  5 │  78116 │ ██▌                │
-│  6 │ 113474 │ ███▋               │
-│  7 │ 170678 │ █████▋             │
-│  8 │ 278380 │ █████████▎         │
-│  9 │ 391053 │ █████████████      │
-│ 10 │ 457681 │ ███████████████▎   │
-│ 11 │ 493667 │ ████████████████▍  │
-│ 12 │ 509641 │ ████████████████▊  │
-│ 13 │ 522947 │ █████████████████▍ │
-│ 14 │ 539954 │ █████████████████▊ │
-│ 15 │ 528460 │ █████████████████▌ │
-│ 16 │ 539201 │ █████████████████▊ │
-│ 17 │ 523539 │ █████████████████▍ │
-│ 18 │ 506467 │ ████████████████▊  │
-│ 19 │ 520915 │ █████████████████▎ │
-│ 20 │ 521665 │ █████████████████▍ │
-│ 21 │ 542078 │ ██████████████████ │
-│ 22 │ 493642 │ ████████████████▍  │
-│ 23 │ 400397 │ █████████████▎     │
-└────┴────────┴────────────────────┘
-```
-
-## transformer {#transform}
-
-Transforme une valeur en fonction explicitement définis cartographie de certains éléments à l'autre.
-Il existe deux variantes de cette fonction:
-
-### de transformation(x, array_from, array_to, par défaut) {#transformx-array-from-array-to-default}
-
-`x` – What to transform.
-
-`array_from` – Constant array of values for converting.
-
-`array_to` – Constant array of values to convert the values in ‘from’ de.
-
-`default` – Which value to use if ‘x’ n'est pas égale à une des valeurs de ‘from’.
-
-`array_from` et `array_to` – Arrays of the same size.
-
-Type:
-
-`transform(T, Array(T), Array(U), U) -> U`
-
-`T` et `U` peuvent être des types numériques, chaîne ou Date ou DateTime.
-Lorsque la même lettre est indiquée (T ou U), pour les types numériques, il se peut qu'il ne s'agisse pas de types correspondants, mais de types ayant un type commun.
-Par exemple, le premier argument peut avoir le type Int64, tandis que le second a le type Array(UInt16).
-
-Si l' ‘x’ la valeur est égale à l'un des éléments dans la ‘array_from’ tableau, elle renvoie l'élément existant (qui est numéroté de même) de la ‘array_to’ tableau. Sinon, elle renvoie ‘default’. S'il y a plusieurs éléments correspondants dans ‘array_from’ il renvoie l'un des matches.
-
-Exemple:
-
-``` sql
-SELECT
-    transform(SearchEngineID, [2, 3], ['Yandex', 'Google'], 'Other') AS title,
-    count() AS c
-FROM test.hits
-WHERE SearchEngineID != 0
-GROUP BY title
-ORDER BY c DESC
-```
-
-``` text
-┌─title─────┬──────c─┐
-│ Yandex    │ 498635 │
-│ Google    │ 229872 │
-│ Other     │ 104472 │
-└───────────┴────────┘
-```
-
-### de transformation(x, array_from, array_to) {#transformx-array-from-array-to}
-
-Diffère de la première variation en ce que le ‘default’ l'argument est omis.
-Si l' ‘x’ la valeur est égale à l'un des éléments dans la ‘array_from’ tableau, elle renvoie l'élément correspondant (qui est numéroté de même) de la ‘array_to’ tableau. Sinon, elle renvoie ‘x’.
-
-Type:
-
-`transform(T, Array(T), Array(T)) -> T`
-
-Exemple:
-
-``` sql
-SELECT
-    transform(domain(Referer), ['yandex.ru', 'google.ru', 'vk.com'], ['www.yandex', 'example.com']) AS s,
-    count() AS c
-FROM test.hits
-GROUP BY domain(Referer)
-ORDER BY count() DESC
-LIMIT 10
-```
-
-``` text
-┌─s──────────────┬───────c─┐
-│                │ 2906259 │
-│ www.yandex     │  867767 │
-│ ███████.ru     │  313599 │
-│ mail.yandex.ru │  107147 │
-│ ██████.ru      │  100355 │
-│ █████████.ru   │   65040 │
-│ news.yandex.ru │   64515 │
-│ ██████.net     │   59141 │
-│ example.com    │   57316 │
-└────────────────┴─────────┘
-```
-
-## formatReadableSize (x) {#formatreadablesizex}
-
-Accepte la taille (nombre d'octets). Renvoie une taille arrondie avec un suffixe (KiB, MiB, etc.) comme une chaîne de caractères.
-
-Exemple:
-
-``` sql
-SELECT
-    arrayJoin([1, 1024, 1024*1024, 192851925]) AS filesize_bytes,
-    formatReadableSize(filesize_bytes) AS filesize
-```
-
-``` text
-┌─filesize_bytes─┬─filesize───┐
-│              1 │ 1.00 B     │
-│           1024 │ 1.00 KiB   │
-│        1048576 │ 1.00 MiB   │
-│      192851925 │ 183.92 MiB │
-└────────────────┴────────────┘
-```
-
-## moins (a, b) {#leasta-b}
-
-Renvoie la plus petite valeur de a et b.
-
-## la plus grande(a, b) {#greatesta-b}
-
-Renvoie la plus grande valeur de a et B.
-
-## le temps de disponibilité() {#uptime}
-
-Renvoie la disponibilité du serveur en quelques secondes.
-
-## version() {#version}
-
-Renvoie la version du serveur sous forme de chaîne.
-
-## fuseau() {#timezone}
-
-Retourne le fuseau horaire du serveur.
-
-## blockNumber {#blocknumber}
-
-Renvoie le numéro de séquence du bloc de données où se trouve la ligne.
-
-## rowNumberInBlock {#function-rownumberinblock}
-
-Renvoie le numéro de séquence de la ligne dans le bloc de données. Différents blocs de données sont toujours recalculés.
-
-## rowNumberInAllBlocks() {#rownumberinallblocks}
-
-Renvoie le numéro de séquence de la ligne dans le bloc de données. Cette fonction ne prend en compte que les blocs de données affectés.
-
-## voisin {#neighbor}
-
-La fonction de fenêtre qui donne accès à une ligne à un décalage spécifié qui vient avant ou après la ligne actuelle d'une colonne donnée.
-
-**Syntaxe**
-
-``` sql
-neighbor(column, offset[, default_value])
-```
-
-Le résultat de la fonction dépend du touché des blocs de données et l'ordre des données dans le bloc.
-Si vous créez une sous-requête avec ORDER BY et appelez la fonction depuis l'extérieur de la sous-requête, vous pouvez obtenir le résultat attendu.
-
-**Paramètre**
-
--   `column` — A column name or scalar expression.
--   `offset` — The number of rows forwards or backwards from the current row of `column`. [Int64](../../sql-reference/data-types/int-uint.md).
--   `default_value` — Optional. The value to be returned if offset goes beyond the scope of the block. Type of data blocks affected.
-
-**Valeurs renvoyées**
-
--   De la valeur pour `column` dans `offset` distance de la ligne actuelle si `offset` la valeur n'est pas en dehors des limites du bloc.
--   La valeur par défaut pour `column` si `offset` la valeur est en dehors des limites du bloc. Si `default_value` est donné, alors il sera utilisé.
-
-Type: type de blocs de données affectés ou type de valeur par défaut.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT number, neighbor(number, 2) FROM system.numbers LIMIT 10;
-```
-
-Résultat:
-
-``` text
-┌─number─┬─neighbor(number, 2)─┐
-│      0 │                   2 │
-│      1 │                   3 │
-│      2 │                   4 │
-│      3 │                   5 │
-│      4 │                   6 │
-│      5 │                   7 │
-│      6 │                   8 │
-│      7 │                   9 │
-│      8 │                   0 │
-│      9 │                   0 │
-└────────┴─────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT number, neighbor(number, 2, 999) FROM system.numbers LIMIT 10;
-```
-
-Résultat:
-
-``` text
-┌─number─┬─neighbor(number, 2, 999)─┐
-│      0 │                        2 │
-│      1 │                        3 │
-│      2 │                        4 │
-│      3 │                        5 │
-│      4 │                        6 │
-│      5 │                        7 │
-│      6 │                        8 │
-│      7 │                        9 │
-│      8 │                      999 │
-│      9 │                      999 │
-└────────┴──────────────────────────┘
-```
-
-Cette fonction peut être utilisée pour calculer une année à valeur métrique:
-
-Requête:
-
-``` sql
-WITH toDate('2018-01-01') AS start_date
-SELECT
-    toStartOfMonth(start_date + (number * 32)) AS month,
-    toInt32(month) % 100 AS money,
-    neighbor(money, -12) AS prev_year,
-    round(prev_year / money, 2) AS year_over_year
-FROM numbers(16)
-```
-
-Résultat:
-
-``` text
-┌──────month─┬─money─┬─prev_year─┬─year_over_year─┐
-│ 2018-01-01 │    32 │         0 │              0 │
-│ 2018-02-01 │    63 │         0 │              0 │
-│ 2018-03-01 │    91 │         0 │              0 │
-│ 2018-04-01 │    22 │         0 │              0 │
-│ 2018-05-01 │    52 │         0 │              0 │
-│ 2018-06-01 │    83 │         0 │              0 │
-│ 2018-07-01 │    13 │         0 │              0 │
-│ 2018-08-01 │    44 │         0 │              0 │
-│ 2018-09-01 │    75 │         0 │              0 │
-│ 2018-10-01 │     5 │         0 │              0 │
-│ 2018-11-01 │    36 │         0 │              0 │
-│ 2018-12-01 │    66 │         0 │              0 │
-│ 2019-01-01 │    97 │        32 │           0.33 │
-│ 2019-02-01 │    28 │        63 │           2.25 │
-│ 2019-03-01 │    56 │        91 │           1.62 │
-│ 2019-04-01 │    87 │        22 │           0.25 │
-└────────────┴───────┴───────────┴────────────────┘
-```
-
-## runningDifference(x) {#other_functions-runningdifference}
-
-Calculates the difference between successive row values ​​in the data block.
-Renvoie 0 pour la première ligne et la différence par rapport à la rangée précédente pour chaque nouvelle ligne.
-
-Le résultat de la fonction dépend du touché des blocs de données et l'ordre des données dans le bloc.
-Si vous créez une sous-requête avec ORDER BY et appelez la fonction depuis l'extérieur de la sous-requête, vous pouvez obtenir le résultat attendu.
-
-Exemple:
-
-``` sql
-SELECT
-    EventID,
-    EventTime,
-    runningDifference(EventTime) AS delta
-FROM
-(
-    SELECT
-        EventID,
-        EventTime
-    FROM events
-    WHERE EventDate = '2016-11-24'
-    ORDER BY EventTime ASC
-    LIMIT 5
-)
-```
-
-``` text
-┌─EventID─┬───────────EventTime─┬─delta─┐
-│    1106 │ 2016-11-24 00:00:04 │     0 │
-│    1107 │ 2016-11-24 00:00:05 │     1 │
-│    1108 │ 2016-11-24 00:00:05 │     0 │
-│    1109 │ 2016-11-24 00:00:09 │     4 │
-│    1110 │ 2016-11-24 00:00:10 │     1 │
-└─────────┴─────────────────────┴───────┘
-```
-
-Veuillez noter que la taille du bloc affecte le résultat. Avec chaque nouveau bloc, le `runningDifference` l'état est réinitialisé.
-
-``` sql
-SELECT
-    number,
-    runningDifference(number + 1) AS diff
-FROM numbers(100000)
-WHERE diff != 1
-```
-
-``` text
-┌─number─┬─diff─┐
-│      0 │    0 │
-└────────┴──────┘
-┌─number─┬─diff─┐
-│  65536 │    0 │
-└────────┴──────┘
-```
-
-``` sql
-set max_block_size=100000 -- default value is 65536!
-
-SELECT
-    number,
-    runningDifference(number + 1) AS diff
-FROM numbers(100000)
-WHERE diff != 1
-```
-
-``` text
-┌─number─┬─diff─┐
-│      0 │    0 │
-└────────┴──────┘
-```
-
-## runningDifferenceStartingWithFirstvalue {#runningdifferencestartingwithfirstvalue}
-
-De même que pour [runningDifference](./other-functions.md#other_functions-runningdifference) la différence est la valeur de la première ligne, est retourné à la valeur de la première ligne, et chaque rangée suivante renvoie la différence de la rangée précédente.
-
-## MACNumToString (num) {#macnumtostringnum}
-
-Accepte un numéro UInt64. Interprète comme une adresse MAC dans big endian. Renvoie une chaîne contenant l'adresse MAC correspondante au format AA:BB:CC: DD:EE: FF (Nombres séparés par deux points sous forme hexadécimale).
-
-## MACStringToNum (s) {#macstringtonums}
-
-La fonction inverse de MACNumToString. Si l'adresse MAC a un format non valide, elle renvoie 0.
-
-## MACStringToOUI (s) {#macstringtoouis}
-
-Accepte une adresse MAC au format AA:BB:CC: DD:EE: FF (Nombres séparés par deux points sous forme hexadécimale). Renvoie les trois premiers octets sous la forme D'un nombre UInt64. Si l'adresse MAC a un format non valide, elle renvoie 0.
-
-## getSizeOfEnumType {#getsizeofenumtype}
-
-Retourne le nombre de champs dans [Enum](../../sql-reference/data-types/enum.md).
-
-``` sql
-getSizeOfEnumType(value)
-```
-
-**Paramètre:**
-
--   `value` — Value of type `Enum`.
-
-**Valeurs renvoyées**
-
--   Le nombre de champs avec `Enum` les valeurs d'entrée.
--   Une exception est levée si le type n'est pas `Enum`.
-
-**Exemple**
-
-``` sql
-SELECT getSizeOfEnumType( CAST('a' AS Enum8('a' = 1, 'b' = 2) ) ) AS x
-```
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-## blockSerializedSize {#blockserializedsize}
-
-Retourne la taille sur le disque (sans tenir compte de la compression).
-
-``` sql
-blockSerializedSize(value[, value[, ...]])
-```
-
-**Paramètre:**
-
--   `value` — Any value.
-
-**Valeurs renvoyées**
-
--   Le nombre d'octets qui seront écrites sur le disque pour le bloc de valeurs (sans compression).
-
-**Exemple**
-
-``` sql
-SELECT blockSerializedSize(maxState(1)) as x
-```
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-## toColumnTypeName {#tocolumntypename}
-
-Renvoie le nom de la classe qui représente le type de données de la colonne dans la RAM.
-
-``` sql
-toColumnTypeName(value)
-```
-
-**Paramètre:**
-
--   `value` — Any type of value.
-
-**Valeurs renvoyées**
-
--   Une chaîne avec le nom de la classe utilisée pour représenter `value` type de données dans la mémoire RAM.
-
-**Exemple de la différence entre`toTypeName ' and ' toColumnTypeName`**
-
-``` sql
-SELECT toTypeName(CAST('2018-01-01 01:02:03' AS DateTime))
-```
-
-``` text
-┌─toTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐
-│ DateTime                                            │
-└─────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT toColumnTypeName(CAST('2018-01-01 01:02:03' AS DateTime))
-```
-
-``` text
-┌─toColumnTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐
-│ Const(UInt32)                                             │
-└───────────────────────────────────────────────────────────┘
-```
-
-L'exemple montre que le `DateTime` type de données est stocké dans la mémoire comme `Const(UInt32)`.
-
-## dumpColumnStructure {#dumpcolumnstructure}
-
-Affiche une description détaillée des structures de données en RAM
-
-``` sql
-dumpColumnStructure(value)
-```
-
-**Paramètre:**
-
--   `value` — Any type of value.
-
-**Valeurs renvoyées**
-
--   Une chaîne décrivant la structure utilisée pour représenter `value` type de données dans la mémoire RAM.
-
-**Exemple**
-
-``` sql
-SELECT dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))
-```
-
-``` text
-┌─dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))─┐
-│ DateTime, Const(size = 1, UInt32(size = 1))                  │
-└──────────────────────────────────────────────────────────────┘
-```
-
-## defaultValueOfArgumentType {#defaultvalueofargumenttype}
-
-Affiche la valeur par défaut du type de données.
-
-Ne pas inclure des valeurs par défaut pour les colonnes personnalisées définies par l'utilisateur.
-
-``` sql
-defaultValueOfArgumentType(expression)
-```
-
-**Paramètre:**
-
--   `expression` — Arbitrary type of value or an expression that results in a value of an arbitrary type.
-
-**Valeurs renvoyées**
-
--   `0` pour les nombres.
--   Chaîne vide pour les chaînes.
--   `ᴺᵁᴸᴸ` pour [Nullable](../../sql-reference/data-types/nullable.md).
-
-**Exemple**
-
-``` sql
-SELECT defaultValueOfArgumentType( CAST(1 AS Int8) )
-```
-
-``` text
-┌─defaultValueOfArgumentType(CAST(1, 'Int8'))─┐
-│                                           0 │
-└─────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT defaultValueOfArgumentType( CAST(1 AS Nullable(Int8) ) )
-```
-
-``` text
-┌─defaultValueOfArgumentType(CAST(1, 'Nullable(Int8)'))─┐
-│                                                  ᴺᵁᴸᴸ │
-└───────────────────────────────────────────────────────┘
-```
-
-## reproduire {#other-functions-replicate}
-
-Crée un tableau avec une seule valeur.
-
-Utilisé pour la mise en œuvre interne de [arrayJoin](array-join.md#functions_arrayjoin).
-
-``` sql
-SELECT replicate(x, arr);
-```
-
-**Paramètre:**
-
--   `arr` — Original array. ClickHouse creates a new array of the same length as the original and fills it with the value `x`.
--   `x` — The value that the resulting array will be filled with.
-
-**Valeur renvoyée**
-
-Un tableau rempli de la valeur `x`.
-
-Type: `Array`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT replicate(1, ['a', 'b', 'c'])
-```
-
-Résultat:
-
-``` text
-┌─replicate(1, ['a', 'b', 'c'])─┐
-│ [1,1,1]                       │
-└───────────────────────────────┘
-```
-
-## filesystemAvailable {#filesystemavailable}
-
-Renvoie la quantité d'espace restant sur le système de fichiers où se trouvent les fichiers des bases de données. Il est toujours plus petit que l'espace libre total ([filesystemFree](#filesystemfree)) parce qu'un peu d'espace est réservé au système D'exploitation.
-
-**Syntaxe**
-
-``` sql
-filesystemAvailable()
-```
-
-**Valeur renvoyée**
-
--   La quantité d'espace restant disponible en octets.
-
-Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT formatReadableSize(filesystemAvailable()) AS "Available space", toTypeName(filesystemAvailable()) AS "Type";
-```
-
-Résultat:
-
-``` text
-┌─Available space─┬─Type───┐
-│ 30.75 GiB       │ UInt64 │
-└─────────────────┴────────┘
-```
-
-## filesystemFree {#filesystemfree}
-
-Retourne montant total de l'espace libre sur le système de fichiers où les fichiers des bases de données. Voir aussi `filesystemAvailable`
-
-**Syntaxe**
-
-``` sql
-filesystemFree()
-```
-
-**Valeur renvoyée**
-
--   Quantité d'espace libre en octets.
-
-Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT formatReadableSize(filesystemFree()) AS "Free space", toTypeName(filesystemFree()) AS "Type";
-```
-
-Résultat:
-
-``` text
-┌─Free space─┬─Type───┐
-│ 32.39 GiB  │ UInt64 │
-└────────────┴────────┘
-```
-
-## filesystemCapacity {#filesystemcapacity}
-
-Renvoie la capacité du système de fichiers en octets. Pour l'évaluation, la [chemin](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-path) le répertoire de données doit être configuré.
-
-**Syntaxe**
-
-``` sql
-filesystemCapacity()
-```
-
-**Valeur renvoyée**
-
--   Informations de capacité du système de fichiers en octets.
-
-Type: [UInt64](../../sql-reference/data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT formatReadableSize(filesystemCapacity()) AS "Capacity", toTypeName(filesystemCapacity()) AS "Type"
-```
-
-Résultat:
-
-``` text
-┌─Capacity──┬─Type───┐
-│ 39.32 GiB │ UInt64 │
-└───────────┴────────┘
-```
-
-## finalizeAggregation {#function-finalizeaggregation}
-
-Prend de l'état de la fonction d'agrégation. Renvoie le résultat de l'agrégation (état finalisé).
-
-## runningAccumulate {#function-runningaccumulate}
-
-Prend les membres de la fonction d'agrégation et renvoie une colonne avec des valeurs, sont le résultat de l'accumulation de ces états pour un ensemble de bloc de lignes, de la première à la ligne actuelle.
-Par exemple, prend l'état de la fonction d'agrégat (exemple runningAccumulate(uniqState(UserID))), et pour chaque ligne de bloc, retourne le résultat de la fonction d'agrégat lors de la fusion des états de toutes les lignes précédentes et de la ligne actuelle.
-Ainsi, le résultat de la fonction dépend de la partition des données aux blocs et de l'ordre des données dans le bloc.
-
-## joinGet {#joinget}
-
-La fonction vous permet d'extraire les données de la table de la même manière qu'à partir d'un [dictionnaire](../../sql-reference/dictionaries/index.md).
-
-Obtient les données de [Rejoindre](../../engines/table-engines/special/join.md#creating-a-table) tables utilisant la clé de jointure spécifiée.
-
-Ne prend en charge que les tables créées avec `ENGINE = Join(ANY, LEFT, <join_keys>)` déclaration.
-
-**Syntaxe**
-
-``` sql
-joinGet(join_storage_table_name, `value_column`, join_keys)
-```
-
-**Paramètre**
-
--   `join_storage_table_name` — an [identificateur](../syntax.md#syntax-identifiers) indique l'endroit où la recherche est effectuée. L'identificateur est recherché dans la base de données par défaut (voir paramètre `default_database` dans le fichier de config). Pour remplacer la base de données par défaut, utilisez `USE db_name` ou spécifiez la base de données et la table via le séparateur `db_name.db_table` voir l'exemple.
--   `value_column` — name of the column of the table that contains required data.
--   `join_keys` — list of keys.
-
-**Valeur renvoyée**
-
-Retourne la liste des valeurs correspond à la liste des clés.
-
-Si certain n'existe pas dans la table source alors `0` ou `null` seront renvoyés basé sur [join_use_nulls](../../operations/settings/settings.md#join_use_nulls) paramètre.
-
-Plus d'infos sur `join_use_nulls` dans [Opération de jointure](../../engines/table-engines/special/join.md).
-
-**Exemple**
-
-Table d'entrée:
-
-``` sql
-CREATE DATABASE db_test
-CREATE TABLE db_test.id_val(`id` UInt32, `val` UInt32) ENGINE = Join(ANY, LEFT, id) SETTINGS join_use_nulls = 1
-INSERT INTO db_test.id_val VALUES (1,11)(2,12)(4,13)
-```
-
-``` text
-┌─id─┬─val─┐
-│  4 │  13 │
-│  2 │  12 │
-│  1 │  11 │
-└────┴─────┘
-```
-
-Requête:
-
-``` sql
-SELECT joinGet(db_test.id_val,'val',toUInt32(number)) from numbers(4) SETTINGS join_use_nulls = 1
-```
-
-Résultat:
-
-``` text
-┌─joinGet(db_test.id_val, 'val', toUInt32(number))─┐
-│                                                0 │
-│                                               11 │
-│                                               12 │
-│                                                0 │
-└──────────────────────────────────────────────────┘
-```
-
-## modelEvaluate(model_name, …) {#function-modelevaluate}
-
-Évaluer le modèle externe.
-Accepte un nom de modèle et le modèle de l'argumentation. Renvoie Float64.
-
-## throwIf (x \[, custom_message\]) {#throwifx-custom-message}
-
-Lever une exception si l'argument est non nul.
-custom_message - est un paramètre optionnel: une chaîne constante, fournit un message d'erreur
-
-``` sql
-SELECT throwIf(number = 3, 'Too many') FROM numbers(10);
-```
-
-``` text
-↙ Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) Received exception from server (version 19.14.1):
-Code: 395. DB::Exception: Received from localhost:9000. DB::Exception: Too many.
-```
-
-## identité {#identity}
-
-Renvoie la même valeur qui a été utilisée comme argument. Utilisé pour le débogage et les tests, permet d'annuler l'utilisation de l'index et d'obtenir les performances de requête d'une analyse complète. Lorsque la requête est analysée pour une utilisation possible de l'index, l'analyseur ne regarde pas à l'intérieur `identity` fonction.
-
-**Syntaxe**
-
-``` sql
-identity(x)
-```
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT identity(42)
-```
-
-Résultat:
-
-``` text
-┌─identity(42)─┐
-│           42 │
-└──────────────┘
-```
-
-## randomPrintableASCII {#randomascii}
-
-Génère une chaîne avec un ensemble aléatoire de [ASCII](https://en.wikipedia.org/wiki/ASCII#Printable_characters) caractères imprimables.
-
-**Syntaxe**
-
-``` sql
-randomPrintableASCII(length)
-```
-
-**Paramètre**
-
--   `length` — Resulting string length. Positive integer.
-
-        If you pass `length < 0`, behavior of the function is undefined.
-
-**Valeur renvoyée**
-
--   Chaîne avec un ensemble aléatoire de [ASCII](https://en.wikipedia.org/wiki/ASCII#Printable_characters) caractères imprimables.
-
-Type: [Chaîne](../../sql-reference/data-types/string.md)
-
-**Exemple**
-
-``` sql
-SELECT number, randomPrintableASCII(30) as str, length(str) FROM system.numbers LIMIT 3
-```
-
-``` text
-┌─number─┬─str────────────────────────────┬─length(randomPrintableASCII(30))─┐
-│      0 │ SuiCOSTvC0csfABSw=UcSzp2.`rv8x │                               30 │
-│      1 │ 1Ag NlJ &RCN:*>HVPG;PE-nO"SUFD │                               30 │
-│      2 │ /"+<"wUTh:=LjJ Vm!c&hI*m#XTfzz │                               30 │
-└────────┴────────────────────────────────┴──────────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/other_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/random-functions.md b/docs/fr/sql-reference/functions/random-functions.md
deleted file mode 100644
index 3c4e15507bbc..000000000000
--- a/docs/fr/sql-reference/functions/random-functions.md
+++ /dev/null
@@ -1,65 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 51
-toc_title: "La G\xE9n\xE9ration De Nombres Pseudo-Al\xE9atoires"
----
-
-# Fonctions pour générer des nombres Pseudo-aléatoires {#functions-for-generating-pseudo-random-numbers}
-
-Des générateurs Non cryptographiques de nombres pseudo-aléatoires sont utilisés.
-
-Toutes les fonctions acceptent zéro argument ou un argument.
-Si un argument est passé, il peut être de n'importe quel type, et sa valeur n'est utilisée pour rien.
-Le seul but de cet argument est d'empêcher l'élimination des sous-expressions courantes, de sorte que deux instances différentes de la même fonction renvoient des colonnes différentes avec des nombres aléatoires différents.
-
-## Rand {#rand}
-
-Renvoie un nombre UInt32 pseudo-aléatoire, réparti uniformément entre tous les nombres de type UInt32.
-Utilise un générateur congruentiel linéaire.
-
-## rand64 {#rand64}
-
-Renvoie un nombre UInt64 pseudo-aléatoire, réparti uniformément entre tous les nombres de type UInt64.
-Utilise un générateur congruentiel linéaire.
-
-## randConstant {#randconstant}
-
-Produit une colonne constante avec une valeur aléatoire.
-
-**Syntaxe**
-
-``` sql
-randConstant([x])
-```
-
-**Paramètre**
-
--   `x` — [Expression](../syntax.md#syntax-expressions) résultant de la [types de données pris en charge](../data-types/index.md#data_types). La valeur résultante est ignorée, mais l'expression elle-même si elle est utilisée pour contourner [élimination des sous-expressions courantes](index.md#common-subexpression-elimination) si la fonction est appelée plusieurs fois dans une seule requête. Paramètre facultatif.
-
-**Valeur renvoyée**
-
--   Nombre Pseudo-aléatoire.
-
-Type: [UInt32](../data-types/int-uint.md).
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT rand(), rand(1), rand(number), randConstant(), randConstant(1), randConstant(number)
-FROM numbers(3)
-```
-
-Résultat:
-
-``` text
-┌─────rand()─┬────rand(1)─┬─rand(number)─┬─randConstant()─┬─randConstant(1)─┬─randConstant(number)─┐
-│ 3047369878 │ 4132449925 │   4044508545 │     2740811946 │      4229401477 │           1924032898 │
-│ 2938880146 │ 1267722397 │   4154983056 │     2740811946 │      4229401477 │           1924032898 │
-│  956619638 │ 4238287282 │   1104342490 │     2740811946 │      4229401477 │           1924032898 │
-└────────────┴────────────┴──────────────┴────────────────┴─────────────────┴──────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/random_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/rounding-functions.md b/docs/fr/sql-reference/functions/rounding-functions.md
deleted file mode 100644
index f99e63580261..000000000000
--- a/docs/fr/sql-reference/functions/rounding-functions.md
+++ /dev/null
@@ -1,190 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: Arrondi
----
-
-# Fonctions D'Arrondi {#rounding-functions}
-
-## floor(x\[, N\]) {#floorx-n}
-
-Renvoie le plus grand nombre rond inférieur ou égal à `x`. Un nombre rond est un multiple de 1 / 10N, ou le nombre le plus proche du type de données approprié si 1 / 10N n'est pas exact.
-‘N’ est une constante entière, paramètre facultatif. Par défaut, il est zéro, ce qui signifie arrondir à un entier.
-‘N’ peut être négative.
-
-Exemple: `floor(123.45, 1) = 123.4, floor(123.45, -1) = 120.`
-
-`x` est n'importe quel type numérique. Le résultat est un nombre du même type.
-Pour les arguments entiers, il est logique d'arrondir avec un négatif `N` valeur (pour non négatif `N`, la fonction ne fait rien).
-Si l'arrondi provoque un débordement (par exemple, floor(-128, -1)), un résultat spécifique à l'implémentation est renvoyé.
-
-## ceil(x\[, n\]), plafond (x\[, n\]) {#ceilx-n-ceilingx-n}
-
-Renvoie le plus petit nombre rond supérieur ou égal à `x`. Dans tous les autres sens, il est le même que le `floor` fonction (voir ci-dessus).
-
-## trunc(x \[, N\]), truncate(x \[, N\]) {#truncx-n-truncatex-n}
-
-Renvoie le nombre rond avec la plus grande valeur absolue qui a une valeur absolue inférieure ou égale à `x`‘s. In every other way, it is the same as the ’floor’ fonction (voir ci-dessus).
-
-## round(x\[, N\]) {#rounding_functions-round}
-
-Arrondit une valeur à un nombre spécifié de décimales.
-
-La fonction renvoie le nombre plus proche de l'ordre spécifié. Dans le cas où un nombre donné a une distance égale aux nombres environnants, la fonction utilise l'arrondi de banquier pour les types de nombres flottants et arrondit à partir de zéro pour les autres types de nombres.
-
-``` sql
-round(expression [, decimal_places])
-```
-
-**Paramètre:**
-
--   `expression` — A number to be rounded. Can be any [expression](../syntax.md#syntax-expressions) retour du numérique [type de données](../../sql-reference/data-types/index.md#data_types).
--   `decimal-places` — An integer value.
-    -   Si `decimal-places > 0` alors la fonction arrondit la valeur à droite du point décimal.
-    -   Si `decimal-places < 0` alors la fonction arrondit la valeur à gauche de la virgule décimale.
-    -   Si `decimal-places = 0` alors la fonction arrondit la valeur à l'entier. Dans ce cas, l'argument peut être omis.
-
-**Valeur renvoyée:**
-
-Le nombre arrondi du même type que le nombre d'entrée.
-
-### Exemple {#examples}
-
-**Exemple d'utilisation**
-
-``` sql
-SELECT number / 2 AS x, round(x) FROM system.numbers LIMIT 3
-```
-
-``` text
-┌───x─┬─round(divide(number, 2))─┐
-│   0 │                        0 │
-│ 0.5 │                        0 │
-│   1 │                        1 │
-└─────┴──────────────────────────┘
-```
-
-**Des exemples de l'arrondissement**
-
-Le résultat est arrondi au plus proche.
-
-``` text
-round(3.2, 0) = 3
-round(4.1267, 2) = 4.13
-round(22,-1) = 20
-round(467,-2) = 500
-round(-467,-2) = -500
-```
-
-Le Banquier arrondit.
-
-``` text
-round(3.5) = 4
-round(4.5) = 4
-round(3.55, 1) = 3.6
-round(3.65, 1) = 3.6
-```
-
-**Voir Aussi**
-
--   [roundBankers](#roundbankers)
-
-## roundBankers {#roundbankers}
-
-Arrondit un nombre à une position décimale spécifiée.
-
--   Si le nombre est arrondi à mi-chemin entre deux nombres, la fonction utilise l'arrondi.
-
-        Banker's rounding is a method of rounding fractional numbers. When the rounding number is halfway between two numbers, it's rounded to the nearest even digit at the specified decimal position. For example: 3.5 rounds up to 4, 2.5 rounds down to 2.
-
-        It's the default rounding method for floating point numbers defined in [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754#Roundings_to_nearest). The [round](#rounding_functions-round) function performs the same rounding for floating point numbers. The `roundBankers` function also rounds integers the same way, for example, `roundBankers(45, -1) = 40`.
-
--   Dans d'autres cas, la fonction arrondit les nombres à l'entier le plus proche.
-
-À l'aide de l'arrondi, vous pouvez réduire l'effet qu'arrondir les nombres sur les résultats d'additionner ou de soustraire ces chiffres.
-
-Par exemple, les nombres de somme 1.5, 2.5, 3.5, 4.5 avec des arrondis différents:
-
--   Pas d'arrondi: 1.5 + 2.5 + 3.5 + 4.5 = 12.
--   Arrondi du banquier: 2 + 2 + 4 + 4 = 12.
--   Arrondi à l'entier le plus proche: 2 + 3 + 4 + 5 = 14.
-
-**Syntaxe**
-
-``` sql
-roundBankers(expression [, decimal_places])
-```
-
-**Paramètre**
-
--   `expression` — A number to be rounded. Can be any [expression](../syntax.md#syntax-expressions) retour du numérique [type de données](../../sql-reference/data-types/index.md#data_types).
--   `decimal-places` — Decimal places. An integer number.
-    -   `decimal-places > 0` — The function rounds the number to the given position right of the decimal point. Example: `roundBankers(3.55, 1) = 3.6`.
-    -   `decimal-places < 0` — The function rounds the number to the given position left of the decimal point. Example: `roundBankers(24.55, -1) = 20`.
-    -   `decimal-places = 0` — The function rounds the number to an integer. In this case the argument can be omitted. Example: `roundBankers(2.5) = 2`.
-
-**Valeur renvoyée**
-
-Valeur arrondie par la méthode d'arrondi du banquier.
-
-### Exemple {#examples-1}
-
-**Exemple d'utilisation**
-
-Requête:
-
-``` sql
- SELECT number / 2 AS x, roundBankers(x, 0) AS b fROM system.numbers limit 10
-```
-
-Résultat:
-
-``` text
-┌───x─┬─b─┐
-│   0 │ 0 │
-│ 0.5 │ 0 │
-│   1 │ 1 │
-│ 1.5 │ 2 │
-│   2 │ 2 │
-│ 2.5 │ 2 │
-│   3 │ 3 │
-│ 3.5 │ 4 │
-│   4 │ 4 │
-│ 4.5 │ 4 │
-└─────┴───┘
-```
-
-**Exemples d'arrondi bancaire**
-
-``` text
-roundBankers(0.4) = 0
-roundBankers(-3.5) = -4
-roundBankers(4.5) = 4
-roundBankers(3.55, 1) = 3.6
-roundBankers(3.65, 1) = 3.6
-roundBankers(10.35, 1) = 10.4
-roundBankers(10.755, 2) = 11,76
-```
-
-**Voir Aussi**
-
--   [rond](#rounding_functions-round)
-
-## roundToExp2 (num) {#roundtoexp2num}
-
-Accepte un certain nombre. Si le nombre est inférieur à un, elle renvoie 0. Sinon, il arrondit le nombre au degré le plus proche (entier non négatif) de deux.
-
-## roundDuration (num) {#rounddurationnum}
-
-Accepte un certain nombre. Si le nombre est inférieur à un, elle renvoie 0. Sinon, il arrondit le nombre vers le bas pour les nombres de l'ensemble: 1, 10, 30, 60, 120, 180, 240, 300, 600, 1200, 1800, 3600, 7200, 18000, 36000. Cette fonction est spécifique à Yandex.Metrica et utilisé pour la mise en œuvre du rapport sur la durée de la session.
-
-## roundAge (num) {#roundagenum}
-
-Accepte un certain nombre. Si le nombre est inférieur à 18, il renvoie 0. Sinon, il arrondit le nombre à un nombre de l'ensemble: 18, 25, 35, 45, 55. Cette fonction est spécifique à Yandex.Metrica et utilisé pour la mise en œuvre du rapport sur l'âge des utilisateurs.
-
-## roundDown(num, arr) {#rounddownnum-arr}
-
-Accepte un nombre et l'arrondit à un élément dans le tableau spécifié. Si la valeur est inférieure à la plus basse, la plus basse lié est retourné.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/rounding_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/splitting-merging-functions.md b/docs/fr/sql-reference/functions/splitting-merging-functions.md
deleted file mode 100644
index a1260e918b00..000000000000
--- a/docs/fr/sql-reference/functions/splitting-merging-functions.md
+++ /dev/null
@@ -1,116 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: "Fractionnement et fusion de cha\xEEnes et de tableaux"
----
-
-# Fonctions pour diviser et fusionner des chaînes et des tableaux {#functions-for-splitting-and-merging-strings-and-arrays}
-
-## splitByChar (séparateur, s) {#splitbycharseparator-s}
-
-Divise une chaîne en sous-chaînes séparées par un caractère spécifique. Il utilise une chaîne constante `separator` qui composé d'un seul caractère.
-Retourne un tableau de certaines chaînes. Les sous-chaînes vides peuvent être sélectionnées si le séparateur se produit au début ou à la fin de la chaîne, ou s'il existe plusieurs séparateurs consécutifs.
-
-**Syntaxe**
-
-``` sql
-splitByChar(<separator>, <s>)
-```
-
-**Paramètre**
-
--   `separator` — The separator which should contain exactly one character. [Chaîne](../../sql-reference/data-types/string.md).
--   `s` — The string to split. [Chaîne](../../sql-reference/data-types/string.md).
-
-**Valeur renvoyée(s)**
-
-Retourne un tableau de certaines chaînes. Des sous-chaînes vides peuvent être sélectionnées lorsque:
-
--   Un séparateur se produit au début ou à la fin de la chaîne;
--   Il existe plusieurs séparateurs consécutifs;
--   La chaîne d'origine `s` est vide.
-
-Type: [Tableau](../../sql-reference/data-types/array.md) de [Chaîne](../../sql-reference/data-types/string.md).
-
-**Exemple**
-
-``` sql
-SELECT splitByChar(',', '1,2,3,abcde')
-```
-
-``` text
-┌─splitByChar(',', '1,2,3,abcde')─┐
-│ ['1','2','3','abcde']           │
-└─────────────────────────────────┘
-```
-
-## splitByString(séparateur, s) {#splitbystringseparator-s}
-
-Divise une chaîne en sous-chaînes séparées par une chaîne. Il utilise une chaîne constante `separator` de plusieurs caractères comme séparateur. Si la chaîne `separator` est vide, il va diviser la chaîne `s` dans un tableau de caractères uniques.
-
-**Syntaxe**
-
-``` sql
-splitByString(<separator>, <s>)
-```
-
-**Paramètre**
-
--   `separator` — The separator. [Chaîne](../../sql-reference/data-types/string.md).
--   `s` — The string to split. [Chaîne](../../sql-reference/data-types/string.md).
-
-**Valeur renvoyée(s)**
-
-Retourne un tableau de certaines chaînes. Des sous-chaînes vides peuvent être sélectionnées lorsque:
-
-Type: [Tableau](../../sql-reference/data-types/array.md) de [Chaîne](../../sql-reference/data-types/string.md).
-
--   Un séparateur non vide se produit au début ou à la fin de la chaîne;
--   Il existe plusieurs séparateurs consécutifs non vides;
--   La chaîne d'origine `s` est vide tandis que le séparateur n'est pas vide.
-
-**Exemple**
-
-``` sql
-SELECT splitByString(', ', '1, 2 3, 4,5, abcde')
-```
-
-``` text
-┌─splitByString(', ', '1, 2 3, 4,5, abcde')─┐
-│ ['1','2 3','4,5','abcde']                 │
-└───────────────────────────────────────────┘
-```
-
-``` sql
-SELECT splitByString('', 'abcde')
-```
-
-``` text
-┌─splitByString('', 'abcde')─┐
-│ ['a','b','c','d','e']      │
-└────────────────────────────┘
-```
-
-## arrayStringConcat(arr \[, séparateur\]) {#arraystringconcatarr-separator}
-
-Concatène les chaînes répertoriées dans le tableau avec le séparateur."séparateur" est un paramètre facultatif: une chaîne constante, définie à une chaîne vide par défaut.
-Retourne une chaîne de caractères.
-
-## alphaTokens (s) {#alphatokenss}
-
-Sélectionne des sous-chaînes d'octets consécutifs dans les plages A-z et A-Z. retourne un tableau de sous-chaînes.
-
-**Exemple**
-
-``` sql
-SELECT alphaTokens('abca1abc')
-```
-
-``` text
-┌─alphaTokens('abca1abc')─┐
-│ ['abca','abc']          │
-└─────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/splitting_merging_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/string-functions.md b/docs/fr/sql-reference/functions/string-functions.md
deleted file mode 100644
index 1482952426c5..000000000000
--- a/docs/fr/sql-reference/functions/string-functions.md
+++ /dev/null
@@ -1,489 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: "Travailler avec des cha\xEEnes"
----
-
-# Fonctions pour travailler avec des chaînes {#functions-for-working-with-strings}
-
-## vide {#empty}
-
-Renvoie 1 pour une chaîne vide ou 0 pour une chaîne non vide.
-Le type de résultat est UInt8.
-Une chaîne est considérée comme non vide si elle contient au moins un octet, même s'il s'agit d'un espace ou d'un octet nul.
-La fonction fonctionne également pour les tableaux.
-
-## notEmpty {#notempty}
-
-Renvoie 0 pour une chaîne vide ou 1 pour une chaîne non vide.
-Le type de résultat est UInt8.
-La fonction fonctionne également pour les tableaux.
-
-## longueur {#length}
-
-Renvoie la longueur d'une chaîne en octets (pas en caractères, et pas en points de code).
-Le type de résultat est UInt64.
-La fonction fonctionne également pour les tableaux.
-
-## lengthUTF8 {#lengthutf8}
-
-Renvoie la longueur d'une chaîne en points de code Unicode (pas en caractères), en supposant que la chaîne contient un ensemble d'octets qui composent le texte codé en UTF-8. Si cette hypothèse n'est pas remplie, elle renvoie un résultat (elle ne lance pas d'exception).
-Le type de résultat est UInt64.
-
-## char_length, CHAR_LENGTH {#char-length}
-
-Renvoie la longueur d'une chaîne en points de code Unicode (pas en caractères), en supposant que la chaîne contient un ensemble d'octets qui composent le texte codé en UTF-8. Si cette hypothèse n'est pas remplie, elle renvoie un résultat (elle ne lance pas d'exception).
-Le type de résultat est UInt64.
-
-## character_length, CHARACTER_LENGTH {#character-length}
-
-Renvoie la longueur d'une chaîne en points de code Unicode (pas en caractères), en supposant que la chaîne contient un ensemble d'octets qui composent le texte codé en UTF-8. Si cette hypothèse n'est pas remplie, elle renvoie un résultat (elle ne lance pas d'exception).
-Le type de résultat est UInt64.
-
-## plus bas, lcase {#lower}
-
-Convertit les symboles latins ASCII dans une chaîne en minuscules.
-
-## supérieur, ucase {#upper}
-
-Convertit les symboles latins ASCII dans une chaîne en majuscules.
-
-## lowerUTF8 {#lowerutf8}
-
-Convertit une chaîne en minuscules, en supposant que la chaîne de caractères contient un ensemble d'octets qui composent un texte UTF-8.
-Il ne détecte pas la langue. Donc, pour le turc, le résultat pourrait ne pas être exactement correct.
-Si la longueur de la séquence d'octets UTF-8 est différente pour les majuscules et les minuscules d'un point de code, le résultat peut être incorrect pour ce point de code.
-Si la chaîne contient un ensemble d'octets qui N'est pas UTF-8, le comportement n'est pas défini.
-
-## upperUTF8 {#upperutf8}
-
-Convertit une chaîne en majuscules, en supposant que la chaîne de caractères contient un ensemble d'octets qui composent un texte UTF-8.
-Il ne détecte pas la langue. Donc, pour le turc, le résultat pourrait ne pas être exactement correct.
-Si la longueur de la séquence d'octets UTF-8 est différente pour les majuscules et les minuscules d'un point de code, le résultat peut être incorrect pour ce point de code.
-Si la chaîne contient un ensemble d'octets qui N'est pas UTF-8, le comportement n'est pas défini.
-
-## isValidUTF8 {#isvalidutf8}
-
-Renvoie 1, si l'ensemble d'octets est codé en UTF-8 valide, sinon 0.
-
-## toValidUTF8 {#tovalidutf8}
-
-Remplace les caractères UTF-8 non valides par `�` (U+FFFD) caractère. Tous les caractères non valides s'exécutant dans une rangée sont réduits en un seul caractère de remplacement.
-
-``` sql
-toValidUTF8( input_string )
-```
-
-Paramètre:
-
--   input_string — Any set of bytes represented as the [Chaîne](../../sql-reference/data-types/string.md) type de données objet.
-
-Valeur renvoyée: chaîne UTF-8 valide.
-
-**Exemple**
-
-``` sql
-SELECT toValidUTF8('\x61\xF0\x80\x80\x80b')
-```
-
-``` text
-┌─toValidUTF8('a����b')─┐
-│ a�b                   │
-└───────────────────────┘
-```
-
-## répéter {#repeat}
-
-Répète une corde autant de fois que spécifié et concatène les valeurs répliquées comme une seule chaîne.
-
-**Syntaxe**
-
-``` sql
-repeat(s, n)
-```
-
-**Paramètre**
-
--   `s` — The string to repeat. [Chaîne](../../sql-reference/data-types/string.md).
--   `n` — The number of times to repeat the string. [UInt](../../sql-reference/data-types/int-uint.md).
-
-**Valeur renvoyée**
-
-La chaîne unique, qui contient la chaîne `s` répéter `n` temps. Si `n` \< 1, la fonction renvoie une chaîne vide.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT repeat('abc', 10)
-```
-
-Résultat:
-
-``` text
-┌─repeat('abc', 10)──────────────┐
-│ abcabcabcabcabcabcabcabcabcabc │
-└────────────────────────────────┘
-```
-
-## inverser {#reverse}
-
-Inverse la chaîne (comme une séquence d'octets).
-
-## reverseUTF8 {#reverseutf8}
-
-Inverse une séquence de points de code Unicode, en supposant que la chaîne contient un ensemble d'octets représentant un texte UTF-8. Sinon, il fait autre chose (il ne lance pas d'exception).
-
-## format(pattern, s0, s1, …) {#format}
-
-Formatage du motif constant avec la chaîne listée dans les arguments. `pattern` est un modèle de format Python simplifié. Chaîne de Format contient “replacement fields” entouré par des accolades `{}`. Tout ce qui n'est pas contenu dans les accolades est considéré comme du texte littéral, qui est copié inchangé dans la sortie. Si vous devez inclure un caractère d'Accolade dans le texte littéral, il peut être échappé en doublant: `{{ '{{' }}` et `{{ '}}' }}`. Les noms de champs peuvent être des nombres (à partir de zéro) ou vides (ils sont alors traités comme des nombres de conséquence).
-
-``` sql
-SELECT format('{1} {0} {1}', 'World', 'Hello')
-```
-
-``` text
-┌─format('{1} {0} {1}', 'World', 'Hello')─┐
-│ Hello World Hello                       │
-└─────────────────────────────────────────┘
-```
-
-``` sql
-SELECT format('{} {}', 'Hello', 'World')
-```
-
-``` text
-┌─format('{} {}', 'Hello', 'World')─┐
-│ Hello World                       │
-└───────────────────────────────────┘
-```
-
-## concat {#concat}
-
-Concatène les chaînes répertoriées dans les arguments, sans séparateur.
-
-**Syntaxe**
-
-``` sql
-concat(s1, s2, ...)
-```
-
-**Paramètre**
-
-Valeurs de type String ou FixedString.
-
-**Valeurs renvoyées**
-
-Renvoie la chaîne qui résulte de la concaténation des arguments.
-
-Si l'une des valeurs d'argument est `NULL`, `concat` retourner `NULL`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT concat('Hello, ', 'World!')
-```
-
-Résultat:
-
-``` text
-┌─concat('Hello, ', 'World!')─┐
-│ Hello, World!               │
-└─────────────────────────────┘
-```
-
-## concatAssumeInjective {#concatassumeinjective}
-
-Même que [concat](#concat) la différence est que vous devez vous assurer que `concat(s1, s2, ...) → sn` est injectif, il sera utilisé pour l'optimisation du groupe par.
-
-La fonction est nommée “injective” si elle renvoie toujours un résultat différent pour différentes valeurs d'arguments. En d'autres termes: des arguments différents ne donnent jamais un résultat identique.
-
-**Syntaxe**
-
-``` sql
-concatAssumeInjective(s1, s2, ...)
-```
-
-**Paramètre**
-
-Valeurs de type String ou FixedString.
-
-**Valeurs renvoyées**
-
-Renvoie la chaîne qui résulte de la concaténation des arguments.
-
-Si l'une des valeurs d'argument est `NULL`, `concatAssumeInjective` retourner `NULL`.
-
-**Exemple**
-
-Table d'entrée:
-
-``` sql
-CREATE TABLE key_val(`key1` String, `key2` String, `value` UInt32) ENGINE = TinyLog;
-INSERT INTO key_val VALUES ('Hello, ','World',1), ('Hello, ','World',2), ('Hello, ','World!',3), ('Hello',', World!',2);
-SELECT * from key_val;
-```
-
-``` text
-┌─key1────┬─key2─────┬─value─┐
-│ Hello,  │ World    │     1 │
-│ Hello,  │ World    │     2 │
-│ Hello,  │ World!   │     3 │
-│ Hello   │ , World! │     2 │
-└─────────┴──────────┴───────┘
-```
-
-Requête:
-
-``` sql
-SELECT concat(key1, key2), sum(value) FROM key_val GROUP BY concatAssumeInjective(key1, key2)
-```
-
-Résultat:
-
-``` text
-┌─concat(key1, key2)─┬─sum(value)─┐
-│ Hello, World!      │          3 │
-│ Hello, World!      │          2 │
-│ Hello, World       │          3 │
-└────────────────────┴────────────┘
-```
-
-## substring(s, offset, longueur), mid(s, offset, longueur), substr(s, offset, longueur) {#substring}
-
-Renvoie une sous-chaîne commençant par l'octet du ‘offset’ index ‘length’ octets de long. L'indexation des caractères commence à partir d'un (comme dans SQL standard). Le ‘offset’ et ‘length’ les arguments doivent être des constantes.
-
-## substringUTF8(s, offset, longueur) {#substringutf8}
-
-Le même que ‘substring’, mais pour les points de code Unicode. Fonctionne sous l'hypothèse que la chaîne contient un ensemble d'octets représentant un texte codé en UTF-8. Si cette hypothèse n'est pas remplie, elle renvoie un résultat (elle ne lance pas d'exception).
-
-## appendTrailingCharIfAbsent (s, c) {#appendtrailingcharifabsent}
-
-Si l' ‘s’ la chaîne n'est pas vide et ne contient pas ‘c’ personnage à la fin, il ajoute le ‘c’ personnage à la fin.
-
-## convertCharset(s, à partir de, à) {#convertcharset}
-
-Retourne une chaîne de caractères ‘s’ qui a été converti à partir de l'encodage dans ‘from’ pour l'encodage dans ‘to’.
-
-## base64Encode(s) {#base64encode}
-
-Encodage ‘s’ chaîne dans base64
-
-## base64Decode(s) {#base64decode}
-
-Décoder la chaîne codée en base64 ‘s’ dans la chaîne d'origine. En cas d'échec, une exception est levée.
-
-## tryBase64Decode(s) {#trybase64decode}
-
-Semblable à base64Decode, mais en cas d'erreur, une chaîne vide serait renvoyé.
-
-## endsWith (s, suffixe) {#endswith}
-
-Renvoie s'il faut se terminer par le suffixe spécifié. Retourne 1 si la chaîne se termine par le suffixe spécifié, sinon elle renvoie 0.
-
-## startsWith (STR, préfixe) {#startswith}
-
-Retourne 1 si la chaîne commence par le préfixe spécifié, sinon elle renvoie 0.
-
-``` sql
-SELECT startsWith('Spider-Man', 'Spi');
-```
-
-**Valeurs renvoyées**
-
--   1, si la chaîne commence par le préfixe spécifié.
--   0, si la chaîne ne commence pas par le préfixe spécifié.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT startsWith('Hello, world!', 'He');
-```
-
-Résultat:
-
-``` text
-┌─startsWith('Hello, world!', 'He')─┐
-│                                 1 │
-└───────────────────────────────────┘
-```
-
-## coupe {#trim}
-
-Supprime tous les caractères spécifiés du début ou de la fin d'une chaîne.
-Par défaut supprime toutes les occurrences consécutives d'espaces communs (caractère ASCII 32) des deux extrémités d'une chaîne.
-
-**Syntaxe**
-
-``` sql
-trim([[LEADING|TRAILING|BOTH] trim_character FROM] input_string)
-```
-
-**Paramètre**
-
--   `trim_character` — specified characters for trim. [Chaîne](../../sql-reference/data-types/string.md).
--   `input_string` — string for trim. [Chaîne](../../sql-reference/data-types/string.md).
-
-**Valeur renvoyée**
-
-Une chaîne sans caractères de début et (ou) de fin spécifiés.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT trim(BOTH ' ()' FROM '(   Hello, world!   )')
-```
-
-Résultat:
-
-``` text
-┌─trim(BOTH ' ()' FROM '(   Hello, world!   )')─┐
-│ Hello, world!                                 │
-└───────────────────────────────────────────────┘
-```
-
-## trimLeft {#trimleft}
-
-Supprime toutes les occurrences consécutives d'espaces communs (caractère ASCII 32) depuis le début d'une chaîne. Il ne supprime pas d'autres types de caractères d'espaces (tabulation, espace sans pause, etc.).
-
-**Syntaxe**
-
-``` sql
-trimLeft(input_string)
-```
-
-Alias: `ltrim(input_string)`.
-
-**Paramètre**
-
--   `input_string` — string to trim. [Chaîne](../../sql-reference/data-types/string.md).
-
-**Valeur renvoyée**
-
-Une chaîne sans ouvrir les espaces communs.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT trimLeft('     Hello, world!     ')
-```
-
-Résultat:
-
-``` text
-┌─trimLeft('     Hello, world!     ')─┐
-│ Hello, world!                       │
-└─────────────────────────────────────┘
-```
-
-## trimRight {#trimright}
-
-Supprime toutes les occurrences consécutives d'espaces communs (caractère ASCII 32) de la fin d'une chaîne. Il ne supprime pas d'autres types de caractères d'espaces (tabulation, espace sans pause, etc.).
-
-**Syntaxe**
-
-``` sql
-trimRight(input_string)
-```
-
-Alias: `rtrim(input_string)`.
-
-**Paramètre**
-
--   `input_string` — string to trim. [Chaîne](../../sql-reference/data-types/string.md).
-
-**Valeur renvoyée**
-
-Une chaîne sans espaces communs de fin.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT trimRight('     Hello, world!     ')
-```
-
-Résultat:
-
-``` text
-┌─trimRight('     Hello, world!     ')─┐
-│      Hello, world!                   │
-└──────────────────────────────────────┘
-```
-
-## trimBoth {#trimboth}
-
-Supprime toutes les occurrences consécutives d'espaces communs (caractère ASCII 32) des deux extrémités d'une chaîne. Il ne supprime pas d'autres types de caractères d'espaces (tabulation, espace sans pause, etc.).
-
-**Syntaxe**
-
-``` sql
-trimBoth(input_string)
-```
-
-Alias: `trim(input_string)`.
-
-**Paramètre**
-
--   `input_string` — string to trim. [Chaîne](../../sql-reference/data-types/string.md).
-
-**Valeur renvoyée**
-
-Une chaîne sans espaces communs de début et de fin.
-
-Type: `String`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT trimBoth('     Hello, world!     ')
-```
-
-Résultat:
-
-``` text
-┌─trimBoth('     Hello, world!     ')─┐
-│ Hello, world!                       │
-└─────────────────────────────────────┘
-```
-
-## CRC32 (s) {#crc32}
-
-Renvoie la somme de contrôle CRC32 d'une chaîne, en utilisant le polynôme CRC-32-IEEE 802.3 et la valeur initiale `0xffffffff` (zlib mise en œuvre).
-
-Le type de résultat est UInt32.
-
-## CRC32IEEE (s) {#crc32ieee}
-
-Renvoie la somme de contrôle CRC32 d'une chaîne, en utilisant le polynôme CRC-32-IEEE 802.3.
-
-Le type de résultat est UInt32.
-
-## CRC64 (s) {#crc64}
-
-Renvoie la somme de contrôle CRC64 d'une chaîne, en utilisant le polynôme CRC-64-ECMA.
-
-Le type de résultat est UInt64.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/string_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/string-replace-functions.md b/docs/fr/sql-reference/functions/string-replace-functions.md
deleted file mode 100644
index 5389a2bc927e..000000000000
--- a/docs/fr/sql-reference/functions/string-replace-functions.md
+++ /dev/null
@@ -1,94 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: "Pour remplacer dans les cha\xEEnes"
----
-
-# Fonctions de recherche et de remplacement dans les chaînes {#functions-for-searching-and-replacing-in-strings}
-
-## replaceOne(botte de foin, modèle, remplacement) {#replaceonehaystack-pattern-replacement}
-
-Remplace la première occurrence, si elle existe, ‘pattern’ sous-chaîne dans ‘haystack’ avec l' ‘replacement’ substring.
-Ci-après, ‘pattern’ et ‘replacement’ doivent être constantes.
-
-## replaceAll(botte de foin, motif, remplacement), Remplacer(botte de foin, motif, remplacement) {#replaceallhaystack-pattern-replacement-replacehaystack-pattern-replacement}
-
-Remplace toutes les occurrences du ‘pattern’ sous-chaîne dans ‘haystack’ avec l' ‘replacement’ substring.
-
-## replaceRegexpOne(botte de foin, modèle, remplacement) {#replaceregexponehaystack-pattern-replacement}
-
-Remplacement en utilisant le ‘pattern’ expression régulière. Une expression régulière re2.
-Remplace seulement la première occurrence, si elle existe.
-Un motif peut être spécifié comme ‘replacement’. Ce modèle peut inclure des substitutions `\0-\9`.
-Substitution `\0` inclut l'expression régulière entière. Substitution `\1-\9` correspond au sous-modèle numbers.To utilisez le `\` caractère dans un modèle, échappez-le en utilisant `\`.
-Aussi garder à l'esprit qu'un littéral de chaîne nécessite une évasion.
-
-Exemple 1. Conversion de la date au format américain:
-
-``` sql
-SELECT DISTINCT
-    EventDate,
-    replaceRegexpOne(toString(EventDate), '(\\d{4})-(\\d{2})-(\\d{2})', '\\2/\\3/\\1') AS res
-FROM test.hits
-LIMIT 7
-FORMAT TabSeparated
-```
-
-``` text
-2014-03-17      03/17/2014
-2014-03-18      03/18/2014
-2014-03-19      03/19/2014
-2014-03-20      03/20/2014
-2014-03-21      03/21/2014
-2014-03-22      03/22/2014
-2014-03-23      03/23/2014
-```
-
-Exemple 2. Copier une chaîne dix fois:
-
-``` sql
-SELECT replaceRegexpOne('Hello, World!', '.*', '\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0') AS res
-```
-
-``` text
-┌─res────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
-│ Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World! │
-└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
-```
-
-## replaceRegexpAll(botte de foin, modèle, remplacement) {#replaceregexpallhaystack-pattern-replacement}
-
-Cela fait la même chose, mais remplace toutes les occurrences. Exemple:
-
-``` sql
-SELECT replaceRegexpAll('Hello, World!', '.', '\\0\\0') AS res
-```
-
-``` text
-┌─res────────────────────────┐
-│ HHeelllloo,,  WWoorrlldd!! │
-└────────────────────────────┘
-```
-
-Par exception, si une expression régulière travaillé sur un vide sous-chaîne, le remplacement n'est pas effectué plus d'une fois.
-Exemple:
-
-``` sql
-SELECT replaceRegexpAll('Hello, World!', '^', 'here: ') AS res
-```
-
-``` text
-┌─res─────────────────┐
-│ here: Hello, World! │
-└─────────────────────┘
-```
-
-## regexpQuoteMeta (s) {#regexpquotemetas}
-
-La fonction ajoute une barre oblique inverse avant certains caractères prédéfinis dans la chaîne.
-Les personnages prédéfinis: ‘0’, ‘\\’, ‘\|’, ‘(’, ‘)’, ‘^’, ‘$’, ‘.’, ‘\[’, '\]', ‘?’, '\*‘,’+‘,’{‘,’:‘,’-'.
-Cette implémentation diffère légèrement de re2:: RE2:: QuoteMeta. Il échappe à zéro octet comme \\0 au lieu de 00 et il échappe uniquement les caractères requis.
-Pour plus d'informations, voir le lien: [RE2](https://github.com/google/re2/blob/master/re2/re2.cc#L473)
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/string_replace_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/string-search-functions.md b/docs/fr/sql-reference/functions/string-search-functions.md
deleted file mode 100644
index 20217edd32ce..000000000000
--- a/docs/fr/sql-reference/functions/string-search-functions.md
+++ /dev/null
@@ -1,379 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: "Pour Rechercher Des Cha\xEEnes"
----
-
-# Fonctions de recherche de chaînes {#functions-for-searching-strings}
-
-La recherche est sensible à la casse par défaut dans toutes ces fonctions. Il existe des variantes pour la recherche insensible à la casse.
-
-## position(botte de foin, aiguille), localiser( botte de foin, aiguille) {#position}
-
-Renvoie la position (en octets) de la sous-chaîne trouvée dans la chaîne, à partir de 1.
-
-Fonctionne sous l'hypothèse que la chaîne de caractères contient un ensemble d'octets représentant un octet texte codé. Si cette hypothèse n'est pas remplie et qu'un caractère ne peut pas être représenté à l'aide d'un seul octet, la fonction ne lance pas d'exception et renvoie un résultat inattendu. Si le caractère peut être représenté en utilisant deux octets, il utilisera deux octets et ainsi de suite.
-
-Pour une recherche insensible à la casse, utilisez la fonction [positioncaseinsensible](#positioncaseinsensitive).
-
-**Syntaxe**
-
-``` sql
-position(haystack, needle)
-```
-
-Alias: `locate(haystack, needle)`.
-
-**Paramètre**
-
--   `haystack` — string, in which substring will to be searched. [Chaîne](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Chaîne](../syntax.md#syntax-string-literal).
-
-**Valeurs renvoyées**
-
--   Position de départ en octets (à partir de 1), si la sous-chaîne a été trouvée.
--   0, si la sous-chaîne n'a pas été trouvé.
-
-Type: `Integer`.
-
-**Exemple**
-
-Phrase “Hello, world!” contient un ensemble d'octets représentant un octet texte codé. La fonction renvoie un résultat attendu:
-
-Requête:
-
-``` sql
-SELECT position('Hello, world!', '!')
-```
-
-Résultat:
-
-``` text
-┌─position('Hello, world!', '!')─┐
-│                             13 │
-└────────────────────────────────┘
-```
-
-La même phrase en russe contient des caractères qui ne peuvent pas être représentés en utilisant un seul octet. La fonction renvoie un résultat inattendu (utilisation [positionUTF8](#positionutf8) fonction pour le texte codé sur plusieurs octets):
-
-Requête:
-
-``` sql
-SELECT position('Привет, мир!', '!')
-```
-
-Résultat:
-
-``` text
-┌─position('Привет, мир!', '!')─┐
-│                            21 │
-└───────────────────────────────┘
-```
-
-## positioncaseinsensible {#positioncaseinsensitive}
-
-Le même que [position](#position) renvoie la position (en octets) de la sous-chaîne trouvée dans la chaîne, à partir de 1. Utilisez la fonction pour une recherche insensible à la casse.
-
-Fonctionne sous l'hypothèse que la chaîne de caractères contient un ensemble d'octets représentant un octet texte codé. Si cette hypothèse n'est pas remplie et qu'un caractère ne peut pas être représenté à l'aide d'un seul octet, la fonction ne lance pas d'exception et renvoie un résultat inattendu. Si le caractère peut être représenté en utilisant deux octets, il utilisera deux octets et ainsi de suite.
-
-**Syntaxe**
-
-``` sql
-positionCaseInsensitive(haystack, needle)
-```
-
-**Paramètre**
-
--   `haystack` — string, in which substring will to be searched. [Chaîne](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Chaîne](../syntax.md#syntax-string-literal).
-
-**Valeurs renvoyées**
-
--   Position de départ en octets (à partir de 1), si la sous-chaîne a été trouvée.
--   0, si la sous-chaîne n'a pas été trouvé.
-
-Type: `Integer`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT positionCaseInsensitive('Hello, world!', 'hello')
-```
-
-Résultat:
-
-``` text
-┌─positionCaseInsensitive('Hello, world!', 'hello')─┐
-│                                                 1 │
-└───────────────────────────────────────────────────┘
-```
-
-## positionUTF8 {#positionutf8}
-
-Renvoie la position (en points Unicode) de la sous-chaîne trouvée dans la chaîne, à partir de 1.
-
-Fonctionne sous l'hypothèse que la chaîne contient un ensemble d'octets représentant un texte codé en UTF-8. Si cette hypothèse n'est pas remplie, la fonction ne lance pas d'exception et renvoie un résultat inattendu. Si le caractère peut être représenté en utilisant deux points Unicode, il en utilisera deux et ainsi de suite.
-
-Pour une recherche insensible à la casse, utilisez la fonction [positionCaseInsensitiveUTF8](#positioncaseinsensitiveutf8).
-
-**Syntaxe**
-
-``` sql
-positionUTF8(haystack, needle)
-```
-
-**Paramètre**
-
--   `haystack` — string, in which substring will to be searched. [Chaîne](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Chaîne](../syntax.md#syntax-string-literal).
-
-**Valeurs renvoyées**
-
--   Position de départ dans les points Unicode (à partir de 1), si la sous-chaîne a été trouvée.
--   0, si la sous-chaîne n'a pas été trouvé.
-
-Type: `Integer`.
-
-**Exemple**
-
-Phrase “Hello, world!” en russe contient un ensemble de points Unicode représentant un texte codé à un seul point. La fonction renvoie un résultat attendu:
-
-Requête:
-
-``` sql
-SELECT positionUTF8('Привет, мир!', '!')
-```
-
-Résultat:
-
-``` text
-┌─positionUTF8('Привет, мир!', '!')─┐
-│                                12 │
-└───────────────────────────────────┘
-```
-
-Phrase “Salut, étudiante!” où le caractère `é` peut être représenté en utilisant un point (`U+00E9`) ou deux points (`U+0065U+0301`) la fonction peut être retournée un résultat inattendu:
-
-Requête pour la lettre `é` qui est représenté un point Unicode `U+00E9`:
-
-``` sql
-SELECT positionUTF8('Salut, étudiante!', '!')
-```
-
-Résultat:
-
-``` text
-┌─positionUTF8('Salut, étudiante!', '!')─┐
-│                                     17 │
-└────────────────────────────────────────┘
-```
-
-Requête pour la lettre `é` qui est représenté deux points Unicode `U+0065U+0301`:
-
-``` sql
-SELECT positionUTF8('Salut, étudiante!', '!')
-```
-
-Résultat:
-
-``` text
-┌─positionUTF8('Salut, étudiante!', '!')─┐
-│                                     18 │
-└────────────────────────────────────────┘
-```
-
-## positionCaseInsensitiveUTF8 {#positioncaseinsensitiveutf8}
-
-Le même que [positionUTF8](#positionutf8) mais est sensible à la casse. Renvoie la position (en points Unicode) de la sous-chaîne trouvée dans la chaîne, à partir de 1.
-
-Fonctionne sous l'hypothèse que la chaîne contient un ensemble d'octets représentant un texte codé en UTF-8. Si cette hypothèse n'est pas remplie, la fonction ne lance pas d'exception et renvoie un résultat inattendu. Si le caractère peut être représenté en utilisant deux points Unicode, il en utilisera deux et ainsi de suite.
-
-**Syntaxe**
-
-``` sql
-positionCaseInsensitiveUTF8(haystack, needle)
-```
-
-**Paramètre**
-
--   `haystack` — string, in which substring will to be searched. [Chaîne](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Chaîne](../syntax.md#syntax-string-literal).
-
-**Valeur renvoyée**
-
--   Position de départ dans les points Unicode (à partir de 1), si la sous-chaîne a été trouvée.
--   0, si la sous-chaîne n'a pas été trouvé.
-
-Type: `Integer`.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT positionCaseInsensitiveUTF8('Привет, мир!', 'Мир')
-```
-
-Résultat:
-
-``` text
-┌─positionCaseInsensitiveUTF8('Привет, мир!', 'Мир')─┐
-│                                                  9 │
-└────────────────────────────────────────────────────┘
-```
-
-## multirecherchallpositions {#multisearchallpositions}
-
-Le même que [position](string-search-functions.md#position) mais les retours `Array` des positions (en octets) des sous-chaînes correspondantes trouvées dans la chaîne. Les Positions sont indexées à partir de 1.
-
-La recherche est effectuée sur des séquences d'octets sans tenir compte de l'encodage et du classement des chaînes.
-
--   Pour une recherche ASCII insensible à la casse, utilisez la fonction `multiSearchAllPositionsCaseInsensitive`.
--   Pour la recherche en UTF-8, Utilisez la fonction [multiSearchAllPositionsUTF8](#multiSearchAllPositionsUTF8).
--   Pour la recherche UTF-8 insensible à la casse, utilisez la fonction multiSearchAllPositionsCaseInsensitiveutf8.
-
-**Syntaxe**
-
-``` sql
-multiSearchAllPositions(haystack, [needle1, needle2, ..., needlen])
-```
-
-**Paramètre**
-
--   `haystack` — string, in which substring will to be searched. [Chaîne](../syntax.md#syntax-string-literal).
--   `needle` — substring to be searched. [Chaîne](../syntax.md#syntax-string-literal).
-
-**Valeurs renvoyées**
-
--   Tableau de positions de départ en octets (à partir de 1), si la sous-chaîne correspondante a été trouvée et 0 si elle n'est pas trouvée.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT multiSearchAllPositions('Hello, World!', ['hello', '!', 'world'])
-```
-
-Résultat:
-
-``` text
-┌─multiSearchAllPositions('Hello, World!', ['hello', '!', 'world'])─┐
-│ [0,13,0]                                                          │
-└───────────────────────────────────────────────────────────────────┘
-```
-
-## multiSearchAllPositionsUTF8 {#multiSearchAllPositionsUTF8}
-
-Voir `multiSearchAllPositions`.
-
-## multiSearchFirstPosition(botte de foin, \[aiguille<sub>1</sub>, aiguille<sub>2</sub>, …, needle<sub>et</sub>\]) {#multisearchfirstposition}
-
-Le même que `position` mais renvoie le décalage le plus à gauche de la chaîne `haystack` et qui correspond à certains des aiguilles.
-
-Pour une recherche insensible à la casse ou/et au format UTF-8, utilisez les fonctions `multiSearchFirstPositionCaseInsensitive, multiSearchFirstPositionUTF8, multiSearchFirstPositionCaseInsensitiveUTF8`.
-
-## multiSearchFirstIndex(botte de foin, \[aiguille<sub>1</sub>, aiguille<sub>2</sub>, …, needle<sub>et</sub>\]) {#multisearchfirstindexhaystack-needle1-needle2-needlen}
-
-Renvoie l'index `i` (à partir de 1) de l'aiguille trouvée la plus à gauche<sub>je</sub> dans la chaîne `haystack` et 0 sinon.
-
-Pour une recherche insensible à la casse ou/et au format UTF-8, utilisez les fonctions `multiSearchFirstIndexCaseInsensitive, multiSearchFirstIndexUTF8, multiSearchFirstIndexCaseInsensitiveUTF8`.
-
-## multiSearchAny(botte de foin, \[aiguille<sub>1</sub>, aiguille<sub>2</sub>, …, needle<sub>et</sub>\]) {#function-multisearchany}
-
-Renvoie 1, si au moins une aiguille de chaîne<sub>je</sub> correspond à la chaîne `haystack` et 0 sinon.
-
-Pour une recherche insensible à la casse ou/et au format UTF-8, utilisez les fonctions `multiSearchAnyCaseInsensitive, multiSearchAnyUTF8, multiSearchAnyCaseInsensitiveUTF8`.
-
-!!! note "Note"
-    Dans tous les `multiSearch*` fonctions le nombre d'aiguilles doit être d'au moins 2<sup>8</sup> en raison de la spécification de mise en œuvre.
-
-## match (botte de foin, motif) {#matchhaystack-pattern}
-
-Vérifie si la chaîne correspond au `pattern` expression régulière. Un `re2` expression régulière. Le [syntaxe](https://github.com/google/re2/wiki/Syntax) de la `re2` les expressions régulières sont plus limitées que la syntaxe des expressions régulières Perl.
-
-Renvoie 0 si elle ne correspond pas, ou 1 si elle correspond.
-
-Notez que le symbole antislash (`\`) est utilisé pour s'échapper dans l'expression régulière. Le même symbole est utilisé pour échapper dans les littéraux de chaîne. Donc, pour échapper au symbole dans une expression régulière, vous devez écrire deux barres obliques inverses ( \\ ) dans un littéral de chaîne.
-
-L'expression régulière travaille à la chaîne, comme si c'est un ensemble d'octets. L'expression régulière ne peut pas contenir d'octets nuls.
-Pour que les modèles recherchent des sous-chaînes dans une chaîne, il est préférable D'utiliser LIKE ou ‘position’ depuis ils travaillent beaucoup plus vite.
-
-## multiMatchAny(botte de foin, \[motif<sub>1</sub>, modèle<sub>2</sub>, …, pattern<sub>et</sub>\]) {#multimatchanyhaystack-pattern1-pattern2-patternn}
-
-Le même que `match` mais renvoie 0 si aucune des expressions régulières sont appariés et 1 si l'un des modèles les matchs. Il utilise [hyperscan](https://github.com/intel/hyperscan) bibliothèque. Pour que les modèles recherchent des sous-chaînes dans une chaîne, il est préférable d'utiliser `multiSearchAny` comme cela fonctionne beaucoup plus vite.
-
-!!! note "Note"
-    La longueur de l'un des `haystack` la chaîne doit être inférieure à 2<sup>32</sup> octets sinon l'exception est levée. Cette restriction a lieu en raison de l'API hyperscan.
-
-## multiMatchAnyIndex(botte de foin, \[motif<sub>1</sub>, modèle<sub>2</sub>, …, pattern<sub>et</sub>\]) {#multimatchanyindexhaystack-pattern1-pattern2-patternn}
-
-Le même que `multiMatchAny` mais retourne un index qui correspond à la botte de foin.
-
-## multiMatchAllIndices(botte de foin, \[motif<sub>1</sub>, modèle<sub>2</sub>, …, pattern<sub>et</sub>\]) {#multimatchallindiceshaystack-pattern1-pattern2-patternn}
-
-Le même que `multiMatchAny`, mais renvoie le tableau de tous les indices qui correspondent à la botte de foin dans n'importe quel ordre.
-
-## multiFuzzyMatchAny(botte de foin, distance, \[motif<sub>1</sub>, modèle<sub>2</sub>, …, pattern<sub>et</sub>\]) {#multifuzzymatchanyhaystack-distance-pattern1-pattern2-patternn}
-
-Le même que `multiMatchAny`, mais renvoie 1 si un motif correspond à la botte de foin dans une constante [distance d'édition](https://en.wikipedia.org/wiki/Edit_distance). Cette fonction est également en mode expérimental et peut être extrêmement lente. Pour plus d'informations, voir [documentation hyperscan](https://intel.github.io/hyperscan/dev-reference/compilation.html#approximate-matching).
-
-## multiFuzzyMatchAnyIndex(botte de foin, distance, \[motif<sub>1</sub>, modèle<sub>2</sub>, …, pattern<sub>et</sub>\]) {#multifuzzymatchanyindexhaystack-distance-pattern1-pattern2-patternn}
-
-Le même que `multiFuzzyMatchAny`, mais renvoie tout index qui correspond à la botte de foin à une distance d'édition constante.
-
-## multiFuzzyMatchAllIndices(botte de foin, distance, \[motif<sub>1</sub>, modèle<sub>2</sub>, …, pattern<sub>et</sub>\]) {#multifuzzymatchallindiceshaystack-distance-pattern1-pattern2-patternn}
-
-Le même que `multiFuzzyMatchAny`, mais renvoie le tableau de tous les indices dans n'importe quel ordre qui correspond à la botte de foin à une distance d'édition constante.
-
-!!! note "Note"
-    `multiFuzzyMatch*` les fonctions ne prennent pas en charge les expressions régulières UTF-8, et ces expressions sont traitées comme des octets en raison de la restriction hyperscan.
-
-!!! note "Note"
-    Pour désactiver toutes les fonctions qui utilisent hyperscan, utilisez le réglage `SET allow_hyperscan = 0;`.
-
-## extrait(botte de foin, motif) {#extracthaystack-pattern}
-
-Extraits d'un fragment d'une chaîne à l'aide d'une expression régulière. Si ‘haystack’ ne correspond pas à l' ‘pattern’ regex, une chaîne vide est renvoyée. Si l'expression rationnelle ne contient pas de sous-modèles, elle prend le fragment qui correspond à l'expression rationnelle entière. Sinon, il prend le fragment qui correspond au premier sous-masque.
-
-## extractAll(botte de foin, motif) {#extractallhaystack-pattern}
-
-Extrait tous les fragments d'une chaîne à l'aide d'une expression régulière. Si ‘haystack’ ne correspond pas à l' ‘pattern’ regex, une chaîne vide est renvoyée. Renvoie un tableau de chaînes composé de toutes les correspondances à l'expression rationnelle. En général, le comportement est le même que le ‘extract’ fonction (il prend le premier sous-masque, ou l'expression entière s'il n'y a pas de sous-masque).
-
-## comme (botte de foin, motif), botte de foin comme opérateur de motif {#function-like}
-
-Vérifie si une chaîne correspond à une expression régulière simple.
-L'expression régulière peut contenir les métasymboles `%` et `_`.
-
-`%` indique n'importe quelle quantité d'octets (y compris zéro caractère).
-
-`_` indique un octet.
-
-Utilisez la barre oblique inverse (`\`) pour échapper aux métasymboles. Voir la note sur l'échappement dans la description du ‘match’ fonction.
-
-Pour les expressions régulières comme `%needle%`, le code est plus optimale et fonctionne aussi vite que le `position` fonction.
-Pour d'autres expressions régulières, le code est le même que pour la ‘match’ fonction.
-
-## notLike (botte de foin, motif), botte de foin pas comme opérateur de motif {#function-notlike}
-
-La même chose que ‘like’ mais négative.
-
-## ngramDistance(botte de foin, aiguille) {#ngramdistancehaystack-needle}
-
-Calcule la distance de 4 grammes entre `haystack` et `needle`: counts the symmetric difference between two multisets of 4-grams and normalizes it by the sum of their cardinalities. Returns float number from 0 to 1 – the closer to zero, the more strings are similar to each other. If the constant `needle` ou `haystack` est plus de 32Kb, jette une exception. Si une partie de la non-constante `haystack` ou `needle` les chaînes sont plus que 32Kb, la distance est toujours un.
-
-Pour une recherche insensible à la casse ou/et au format UTF-8, utilisez les fonctions `ngramDistanceCaseInsensitive, ngramDistanceUTF8, ngramDistanceCaseInsensitiveUTF8`.
-
-## ngramSearch(botte de foin, aiguille) {#ngramsearchhaystack-needle}
-
-Même que `ngramDistance` mais calcule la différence non symétrique entre `needle` et `haystack` – the number of n-grams from needle minus the common number of n-grams normalized by the number of `needle` n-grammes. Le plus proche d'un, le plus probable `needle` est dans le `haystack`. Peut être utile pour la recherche de chaîne floue.
-
-Pour une recherche insensible à la casse ou/et au format UTF-8, utilisez les fonctions `ngramSearchCaseInsensitive, ngramSearchUTF8, ngramSearchCaseInsensitiveUTF8`.
-
-!!! note "Note"
-    For UTF-8 case we use 3-gram distance. All these are not perfectly fair n-gram distances. We use 2-byte hashes to hash n-grams and then calculate the (non-)symmetric difference between these hash tables – collisions may occur. With UTF-8 case-insensitive format we do not use fair `tolower` function – we zero the 5-th bit (starting from zero) of each codepoint byte and first bit of zeroth byte if bytes more than one – this works for Latin and mostly for all Cyrillic letters.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/string_search_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/type-conversion-functions.md b/docs/fr/sql-reference/functions/type-conversion-functions.md
deleted file mode 100644
index c17b24c69dcb..000000000000
--- a/docs/fr/sql-reference/functions/type-conversion-functions.md
+++ /dev/null
@@ -1,534 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: La Conversion De Type
----
-
-# Fonctions De Conversion De Type {#type-conversion-functions}
-
-## Problèmes courants des Conversions numériques {#numeric-conversion-issues}
-
-Lorsque vous convertissez une valeur d'un type de données à un autre, vous devez vous rappeler que dans le cas courant, il s'agit d'une opération dangereuse qui peut entraîner une perte de données. Une perte de données peut se produire si vous essayez d'ajuster la valeur d'un type de données plus grand à un type de données plus petit, ou si vous convertissez des valeurs entre différents types de données.
-
-ClickHouse a le [même comportement que les programmes C++ ](https://en.cppreference.com/w/cpp/language/implicit_conversion).
-
-## toInt (8/16/32/64) {#toint8163264}
-
-Convertit une valeur d'entrée en [Int](../../sql-reference/data-types/int-uint.md) type de données. Cette fonction comprend:
-
--   `toInt8(expr)` — Results in the `Int8` type de données.
--   `toInt16(expr)` — Results in the `Int16` type de données.
--   `toInt32(expr)` — Results in the `Int32` type de données.
--   `toInt64(expr)` — Results in the `Int64` type de données.
-
-**Paramètre**
-
--   `expr` — [Expression](../syntax.md#syntax-expressions) renvoyer un nombre ou une chaîne avec la représentation décimale d'un nombre. Les représentations binaires, octales et hexadécimales des nombres ne sont pas prises en charge. Les zéros principaux sont dépouillés.
-
-**Valeur renvoyée**
-
-Valeur entière dans le `Int8`, `Int16`, `Int32`, ou `Int64` type de données.
-
-Fonctions d'utilisation [l'arrondi vers zéro](https://en.wikipedia.org/wiki/Rounding#Rounding_towards_zero), ce qui signifie qu'ils tronquent des chiffres fractionnaires de nombres.
-
-Le comportement des fonctions pour le [NaN et Inf](../../sql-reference/data-types/float.md#data_type-float-nan-inf) arguments est indéfini. Rappelez-vous sur [problèmes de conversion numérique](#numeric-conversion-issues), lorsque vous utilisez les fonctions.
-
-**Exemple**
-
-``` sql
-SELECT toInt64(nan), toInt32(32), toInt16('16'), toInt8(8.8)
-```
-
-``` text
-┌─────────toInt64(nan)─┬─toInt32(32)─┬─toInt16('16')─┬─toInt8(8.8)─┐
-│ -9223372036854775808 │          32 │            16 │           8 │
-└──────────────────────┴─────────────┴───────────────┴─────────────┘
-```
-
-## toInt (8/16/32/64)OrZero {#toint8163264orzero}
-
-Il prend un argument de type String et essaie de l'analyser en Int (8 \| 16 \| 32 \| 64). En cas d'échec, renvoie 0.
-
-**Exemple**
-
-``` sql
-select toInt64OrZero('123123'), toInt8OrZero('123qwe123')
-```
-
-``` text
-┌─toInt64OrZero('123123')─┬─toInt8OrZero('123qwe123')─┐
-│                  123123 │                         0 │
-└─────────────────────────┴───────────────────────────┘
-```
-
-## toInt (8/16/32/64)OrNull {#toint8163264ornull}
-
-Il prend un argument de type String et essaie de l'analyser en Int (8 \| 16 \| 32 \| 64). En cas d'échec, renvoie NULL.
-
-**Exemple**
-
-``` sql
-select toInt64OrNull('123123'), toInt8OrNull('123qwe123')
-```
-
-``` text
-┌─toInt64OrNull('123123')─┬─toInt8OrNull('123qwe123')─┐
-│                  123123 │                      ᴺᵁᴸᴸ │
-└─────────────────────────┴───────────────────────────┘
-```
-
-## toUInt (8/16/32/64) {#touint8163264}
-
-Convertit une valeur d'entrée en [UInt](../../sql-reference/data-types/int-uint.md) type de données. Cette fonction comprend:
-
--   `toUInt8(expr)` — Results in the `UInt8` type de données.
--   `toUInt16(expr)` — Results in the `UInt16` type de données.
--   `toUInt32(expr)` — Results in the `UInt32` type de données.
--   `toUInt64(expr)` — Results in the `UInt64` type de données.
-
-**Paramètre**
-
--   `expr` — [Expression](../syntax.md#syntax-expressions) renvoyer un nombre ou une chaîne avec la représentation décimale d'un nombre. Les représentations binaires, octales et hexadécimales des nombres ne sont pas prises en charge. Les zéros principaux sont dépouillés.
-
-**Valeur renvoyée**
-
-Valeur entière dans le `UInt8`, `UInt16`, `UInt32`, ou `UInt64` type de données.
-
-Fonctions d'utilisation [l'arrondi vers zéro](https://en.wikipedia.org/wiki/Rounding#Rounding_towards_zero), ce qui signifie qu'ils tronquent des chiffres fractionnaires de nombres.
-
-Le comportement des fonctions pour les agruments négatifs et pour le [NaN et Inf](../../sql-reference/data-types/float.md#data_type-float-nan-inf) arguments est indéfini. Si vous passez une chaîne avec un nombre négatif, par exemple `'-32'`, ClickHouse soulève une exception. Rappelez-vous sur [problèmes de conversion numérique](#numeric-conversion-issues), lorsque vous utilisez les fonctions.
-
-**Exemple**
-
-``` sql
-SELECT toUInt64(nan), toUInt32(-32), toUInt16('16'), toUInt8(8.8)
-```
-
-``` text
-┌───────toUInt64(nan)─┬─toUInt32(-32)─┬─toUInt16('16')─┬─toUInt8(8.8)─┐
-│ 9223372036854775808 │    4294967264 │             16 │            8 │
-└─────────────────────┴───────────────┴────────────────┴──────────────┘
-```
-
-## toUInt (8/16/32/64)OrZero {#touint8163264orzero}
-
-## toUInt (8/16/32/64)OrNull {#touint8163264ornull}
-
-## toFloat (32/64) {#tofloat3264}
-
-## toFloat (32/64)OrZero {#tofloat3264orzero}
-
-## toFloat (32/64) OrNull {#tofloat3264ornull}
-
-## jour {#todate}
-
-## toDateOrZero {#todateorzero}
-
-## toDateOrNull {#todateornull}
-
-## toDateTime {#todatetime}
-
-## toDateTimeOrZero {#todatetimeorzero}
-
-## toDateTimeOrNull {#todatetimeornull}
-
-## toDecimal (32/64/128) {#todecimal3264128}
-
-Convertir `value` à l' [Décimal](../../sql-reference/data-types/decimal.md) type de données avec précision de `S`. Le `value` peut être un nombre ou une chaîne. Le `S` (l'échelle) paramètre spécifie le nombre de décimales.
-
--   `toDecimal32(value, S)`
--   `toDecimal64(value, S)`
--   `toDecimal128(value, S)`
-
-## toDecimal (32/64/128) OrNull {#todecimal3264128ornull}
-
-Convertit une chaîne d'entrée en [Nullable (Décimal (P, S))](../../sql-reference/data-types/decimal.md) valeur de type de données. Cette famille de fonctions comprennent:
-
--   `toDecimal32OrNull(expr, S)` — Results in `Nullable(Decimal32(S))` type de données.
--   `toDecimal64OrNull(expr, S)` — Results in `Nullable(Decimal64(S))` type de données.
--   `toDecimal128OrNull(expr, S)` — Results in `Nullable(Decimal128(S))` type de données.
-
-Ces fonctions devraient être utilisées à la place de `toDecimal*()` fonctions, si vous préférez obtenir un `NULL` la valeur au lieu d'une exception dans le cas d'une valeur d'entrée erreur d'analyse.
-
-**Paramètre**
-
--   `expr` — [Expression](../syntax.md#syntax-expressions), retourne une valeur dans l' [Chaîne](../../sql-reference/data-types/string.md) type de données. ClickHouse attend la représentation textuelle du nombre décimal. Exemple, `'1.111'`.
--   `S` — Scale, the number of decimal places in the resulting value.
-
-**Valeur renvoyée**
-
-Une valeur dans l' `Nullable(Decimal(P,S))` type de données. La valeur contient:
-
--   Numéro `S` décimales, si ClickHouse interprète la chaîne d'entrée comme un nombre.
--   `NULL` si ClickHouse ne peut pas interpréter la chaîne d'entrée comme un nombre ou si le nombre d'entrée contient plus de `S` décimale.
-
-**Exemple**
-
-``` sql
-SELECT toDecimal32OrNull(toString(-1.111), 5) AS val, toTypeName(val)
-```
-
-``` text
-┌──────val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 5))─┐
-│ -1.11100 │ Nullable(Decimal(9, 5))                            │
-└──────────┴────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT toDecimal32OrNull(toString(-1.111), 2) AS val, toTypeName(val)
-```
-
-``` text
-┌──val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 2))─┐
-│ ᴺᵁᴸᴸ │ Nullable(Decimal(9, 2))                            │
-└──────┴────────────────────────────────────────────────────┘
-```
-
-## toDecimal (32/64/128)OrZero {#todecimal3264128orzero}
-
-Convertit une valeur d'entrée en [Decimal(P,S)](../../sql-reference/data-types/decimal.md) type de données. Cette famille de fonctions comprennent:
-
--   `toDecimal32OrZero( expr, S)` — Results in `Decimal32(S)` type de données.
--   `toDecimal64OrZero( expr, S)` — Results in `Decimal64(S)` type de données.
--   `toDecimal128OrZero( expr, S)` — Results in `Decimal128(S)` type de données.
-
-Ces fonctions devraient être utilisées à la place de `toDecimal*()` fonctions, si vous préférez obtenir un `0` la valeur au lieu d'une exception dans le cas d'une valeur d'entrée erreur d'analyse.
-
-**Paramètre**
-
--   `expr` — [Expression](../syntax.md#syntax-expressions), retourne une valeur dans l' [Chaîne](../../sql-reference/data-types/string.md) type de données. ClickHouse attend la représentation textuelle du nombre décimal. Exemple, `'1.111'`.
--   `S` — Scale, the number of decimal places in the resulting value.
-
-**Valeur renvoyée**
-
-Une valeur dans l' `Nullable(Decimal(P,S))` type de données. La valeur contient:
-
--   Numéro `S` décimales, si ClickHouse interprète la chaîne d'entrée comme un nombre.
--   0 avec `S` décimales, si ClickHouse ne peut pas interpréter la chaîne d'entrée comme un nombre ou si le nombre d'entrée contient plus de `S` décimale.
-
-**Exemple**
-
-``` sql
-SELECT toDecimal32OrZero(toString(-1.111), 5) AS val, toTypeName(val)
-```
-
-``` text
-┌──────val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 5))─┐
-│ -1.11100 │ Decimal(9, 5)                                      │
-└──────────┴────────────────────────────────────────────────────┘
-```
-
-``` sql
-SELECT toDecimal32OrZero(toString(-1.111), 2) AS val, toTypeName(val)
-```
-
-``` text
-┌──val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 2))─┐
-│ 0.00 │ Decimal(9, 2)                                      │
-└──────┴────────────────────────────────────────────────────┘
-```
-
-## toString {#tostring}
-
-Fonctions de conversion entre des nombres, des chaînes (mais pas des chaînes fixes), des dates et des dates avec des heures.
-Toutes ces fonctions acceptent un argument.
-
-Lors de la conversion vers ou à partir d'une chaîne, la valeur est formatée ou analysée en utilisant les mêmes règles que pour le format TabSeparated (et presque tous les autres formats de texte). Si la chaîne ne peut pas être analysée, une exception est levée et la demande est annulée.
-
-Lors de la conversion de dates en nombres ou vice versa, la date correspond au nombre de jours depuis le début de L'époque Unix.
-Lors de la conversion de dates avec des heures en nombres ou vice versa, la date avec l'heure correspond au nombre de secondes depuis le début de L'époque Unix.
-
-Les formats date et date-avec-heure pour les fonctions toDate/toDateTime sont définis comme suit:
-
-``` text
-YYYY-MM-DD
-YYYY-MM-DD hh:mm:ss
-```
-
-À titre d'exception, si vous convertissez des types numériques UInt32, Int32, UInt64 ou Int64 à Date, et si le nombre est supérieur ou égal à 65536, le nombre est interprété comme un horodatage Unix (et non comme le nombre de jours) et est arrondi à la date. Cela permet de prendre en charge l'occurrence commune de l'écriture ‘toDate(unix_timestamp)’, qui autrement serait une erreur et nécessiterait d'écrire le plus lourd ‘toDate(toDateTime(unix_timestamp))’.
-
-La Conversion entre une date et une date avec l'heure est effectuée de manière naturelle: en ajoutant une heure nulle ou en supprimant l'heure.
-
-La Conversion entre types numériques utilise les mêmes règles que les affectations entre différents types numériques en C++.
-
-De plus, la fonction ToString de L'argument DateTime peut prendre un deuxième argument de chaîne contenant le nom du fuseau horaire. Exemple: `Asia/Yekaterinburg` Dans ce cas, l'heure est formatée en fonction du fuseau horaire spécifié.
-
-``` sql
-SELECT
-    now() AS now_local,
-    toString(now(), 'Asia/Yekaterinburg') AS now_yekat
-```
-
-``` text
-┌───────────now_local─┬─now_yekat───────────┐
-│ 2016-06-15 00:11:21 │ 2016-06-15 02:11:21 │
-└─────────────────────┴─────────────────────┘
-```
-
-Voir aussi l' `toUnixTimestamp` fonction.
-
-## toFixedString (s, N) {#tofixedstrings-n}
-
-Convertit un argument de type String en un type FixedString (N) (une chaîne de longueur fixe N). N doit être une constante.
-Si la chaîne a moins d'octets que N, elle est complétée avec des octets null à droite. Si la chaîne a plus d'octets que N, une exception est levée.
-
-## toStringCutToZero(s) {#tostringcuttozeros}
-
-Accepte un argument String ou FixedString. Renvoie la chaîne avec le contenu tronqué au premier octet zéro trouvé.
-
-Exemple:
-
-``` sql
-SELECT toFixedString('foo', 8) AS s, toStringCutToZero(s) AS s_cut
-```
-
-``` text
-┌─s─────────────┬─s_cut─┐
-│ foo\0\0\0\0\0 │ foo   │
-└───────────────┴───────┘
-```
-
-``` sql
-SELECT toFixedString('foo\0bar', 8) AS s, toStringCutToZero(s) AS s_cut
-```
-
-``` text
-┌─s──────────┬─s_cut─┐
-│ foo\0bar\0 │ foo   │
-└────────────┴───────┘
-```
-
-## reinterpretAsUInt (8/16/32/64) {#reinterpretasuint8163264}
-
-## reinterpretAsInt (8/16/32/64) {#reinterpretasint8163264}
-
-## reinterpretAsFloat (32/64) {#reinterpretasfloat3264}
-
-## réinterprétasdate {#reinterpretasdate}
-
-## reinterpretAsDateTime {#reinterpretasdatetime}
-
-Ces fonctions acceptent une chaîne et interprètent les octets placés au début de la chaîne comme un nombre dans l'ordre de l'hôte (little endian). Si la chaîne n'est pas assez longue, les fonctions fonctionnent comme si la chaîne était remplie avec le nombre nécessaire d'octets nuls. Si la chaîne est plus longue que nécessaire, les octets supplémentaires sont ignorés. Une date est interprétée comme le nombre de jours depuis le début de l'Époque Unix, et une date avec le temps, est interprété comme le nombre de secondes écoulées depuis le début de l'Époque Unix.
-
-## reinterpretAsString {#type_conversion_functions-reinterpretAsString}
-
-Cette fonction accepte un nombre ou une date ou une date avec l'heure, et renvoie une chaîne contenant des octets représentant la valeur correspondante dans l'ordre de l'hôte (little endian). Les octets nuls sont supprimés de la fin. Par exemple, une valeur de type uint32 de 255 est une chaîne longue d'un octet.
-
-## reinterpretAsFixedString {#reinterpretasfixedstring}
-
-Cette fonction accepte un nombre ou une date ou une date avec l'heure, et renvoie une chaîne fixe contenant des octets représentant la valeur correspondante dans l'ordre de l'hôte (little endian). Les octets nuls sont supprimés de la fin. Par exemple, une valeur de type uint32 de 255 est une chaîne fixe longue d'un octet.
-
-## CAST (x, T) {#type_conversion_function-cast}
-
-Convertir ‘x’ à l' ‘t’ type de données. La syntaxe CAST (X comme t) est également prise en charge.
-
-Exemple:
-
-``` sql
-SELECT
-    '2016-06-15 23:00:00' AS timestamp,
-    CAST(timestamp AS DateTime) AS datetime,
-    CAST(timestamp AS Date) AS date,
-    CAST(timestamp, 'String') AS string,
-    CAST(timestamp, 'FixedString(22)') AS fixed_string
-```
-
-``` text
-┌─timestamp───────────┬────────────datetime─┬───────date─┬─string──────────────┬─fixed_string──────────────┐
-│ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00\0\0\0 │
-└─────────────────────┴─────────────────────┴────────────┴─────────────────────┴───────────────────────────┘
-```
-
-La Conversion en FixedString (N) ne fonctionne que pour les arguments de type String ou FixedString (N).
-
-Type conversion en [Nullable](../../sql-reference/data-types/nullable.md) et le dos est pris en charge. Exemple:
-
-``` sql
-SELECT toTypeName(x) FROM t_null
-```
-
-``` text
-┌─toTypeName(x)─┐
-│ Int8          │
-│ Int8          │
-└───────────────┘
-```
-
-``` sql
-SELECT toTypeName(CAST(x, 'Nullable(UInt16)')) FROM t_null
-```
-
-``` text
-┌─toTypeName(CAST(x, 'Nullable(UInt16)'))─┐
-│ Nullable(UInt16)                        │
-│ Nullable(UInt16)                        │
-└─────────────────────────────────────────┘
-```
-
-## toInterval (année / trimestre / Mois / Semaine / Jour / Heure / Minute / Seconde) {#function-tointerval}
-
-Convertit un argument de type Number en [Intervalle](../../sql-reference/data-types/special-data-types/interval.md) type de données.
-
-**Syntaxe**
-
-``` sql
-toIntervalSecond(number)
-toIntervalMinute(number)
-toIntervalHour(number)
-toIntervalDay(number)
-toIntervalWeek(number)
-toIntervalMonth(number)
-toIntervalQuarter(number)
-toIntervalYear(number)
-```
-
-**Paramètre**
-
--   `number` — Duration of interval. Positive integer number.
-
-**Valeurs renvoyées**
-
--   La valeur de `Interval` type de données.
-
-**Exemple**
-
-``` sql
-WITH
-    toDate('2019-01-01') AS date,
-    INTERVAL 1 WEEK AS interval_week,
-    toIntervalWeek(1) AS interval_to_week
-SELECT
-    date + interval_week,
-    date + interval_to_week
-```
-
-``` text
-┌─plus(date, interval_week)─┬─plus(date, interval_to_week)─┐
-│                2019-01-08 │                   2019-01-08 │
-└───────────────────────────┴──────────────────────────────┘
-```
-
-## parseDateTimeBestEffort {#parsedatetimebesteffort}
-
-Convertit une date et une heure dans le [Chaîne](../../sql-reference/data-types/string.md) la représentation de [DateTime](../../sql-reference/data-types/datetime.md#data_type-datetime) type de données.
-
-La fonction d'analyse [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601), [RFC 1123 - 5.2.14 RFC-822 date et heure Spécification](https://tools.ietf.org/html/rfc1123#page-55), ClickHouse et d'autres formats de date et d'heure.
-
-**Syntaxe**
-
-``` sql
-parseDateTimeBestEffort(time_string [, time_zone]);
-```
-
-**Paramètre**
-
--   `time_string` — String containing a date and time to convert. [Chaîne](../../sql-reference/data-types/string.md).
--   `time_zone` — Time zone. The function parses `time_string` selon le fuseau horaire. [Chaîne](../../sql-reference/data-types/string.md).
-
-**Formats non standard pris en charge**
-
--   Une chaîne contenant 9..10 chiffres [le timestamp unix](https://en.wikipedia.org/wiki/Unix_time).
--   Une chaîne avec une date et une heure composant: `YYYYMMDDhhmmss`, `DD/MM/YYYY hh:mm:ss`, `DD-MM-YY hh:mm`, `YYYY-MM-DD hh:mm:ss`, etc.
--   Une chaîne avec une date, mais pas de composant de temps: `YYYY`, `YYYYMM`, `YYYY*MM`, `DD/MM/YYYY`, `DD-MM-YY` etc.
--   Une chaîne avec un jour et une heure: `DD`, `DD hh`, `DD hh:mm`. Dans ce cas `YYYY-MM` sont substitués comme suit `2000-01`.
--   Une chaîne qui inclut la date et l'heure ainsi que des informations de décalage de fuseau horaire: `YYYY-MM-DD hh:mm:ss ±h:mm`, etc. Exemple, `2020-12-12 17:36:00 -5:00`.
-
-Pour tous les formats avec séparateur, la fonction analyse les noms de mois exprimés par leur nom complet ou par les trois premières lettres d'un nom de mois. Exemple: `24/DEC/18`, `24-Dec-18`, `01-September-2018`.
-
-**Valeur renvoyée**
-
--   `time_string` converti à l' `DateTime` type de données.
-
-**Exemple**
-
-Requête:
-
-``` sql
-SELECT parseDateTimeBestEffort('12/12/2020 12:12:57')
-AS parseDateTimeBestEffort;
-```
-
-Résultat:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2020-12-12 12:12:57 │
-└─────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT parseDateTimeBestEffort('Sat, 18 Aug 2018 07:22:16 GMT', 'Europe/Moscow')
-AS parseDateTimeBestEffort
-```
-
-Résultat:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2018-08-18 10:22:16 │
-└─────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT parseDateTimeBestEffort('1284101485')
-AS parseDateTimeBestEffort
-```
-
-Résultat:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2015-07-07 12:04:41 │
-└─────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT parseDateTimeBestEffort('2018-12-12 10:12:12')
-AS parseDateTimeBestEffort
-```
-
-Résultat:
-
-``` text
-┌─parseDateTimeBestEffort─┐
-│     2018-12-12 10:12:12 │
-└─────────────────────────┘
-```
-
-Requête:
-
-``` sql
-SELECT parseDateTimeBestEffort('10 20:19')
-```
-
-Résultat:
-
-``` text
-┌─parseDateTimeBestEffort('10 20:19')─┐
-│                 2000-01-10 20:19:00 │
-└─────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   \[Annonce ISO 8601 par @xkcd\](https://xkcd.com/1179/)
--   [RFC 1123](https://tools.ietf.org/html/rfc1123)
--   [jour](#todate)
--   [toDateTime](#todatetime)
-
-## parseDateTimeBestEffortOrNull {#parsedatetimebesteffortornull}
-
-De même que pour [parseDateTimeBestEffort](#parsedatetimebesteffort) sauf qu'il renvoie null lorsqu'il rencontre un format de date qui ne peut pas être traité.
-
-## parseDateTimeBestEffortOrZero {#parsedatetimebesteffortorzero}
-
-De même que pour [parseDateTimeBestEffort](#parsedatetimebesteffort) sauf qu'il renvoie une date zéro ou une date zéro lorsqu'il rencontre un format de date qui ne peut pas être traité.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/type_conversion_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/url-functions.md b/docs/fr/sql-reference/functions/url-functions.md
deleted file mode 100644
index 2bb2203a10bf..000000000000
--- a/docs/fr/sql-reference/functions/url-functions.md
+++ /dev/null
@@ -1,209 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 54
-toc_title: Travailler avec des URL
----
-
-# Fonctions pour travailler avec des URL {#functions-for-working-with-urls}
-
-Toutes ces fonctions ne suivent pas la RFC. Ils sont simplifiés au maximum pour améliorer les performances.
-
-## Fonctions qui extraient des parties d'une URL {#functions-that-extract-parts-of-a-url}
-
-Si la partie pertinente n'est pas présente dans une URL, une chaîne vide est renvoyée.
-
-### protocole {#protocol}
-
-Extrait le protocole d'une URL.
-
-Examples of typical returned values: http, https, ftp, mailto, tel, magnet…
-
-### domaine {#domain}
-
-Extrait le nom d'hôte d'une URL.
-
-``` sql
-domain(url)
-```
-
-**Paramètre**
-
--   `url` — URL. Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-L'URL peut être spécifiée avec ou sans schéma. Exemple:
-
-``` text
-svn+ssh://some.svn-hosting.com:80/repo/trunk
-some.svn-hosting.com:80/repo/trunk
-https://yandex.com/time/
-```
-
-Pour ces exemples, le `domain` la fonction renvoie les résultats suivants:
-
-``` text
-some.svn-hosting.com
-some.svn-hosting.com
-yandex.com
-```
-
-**Valeurs renvoyées**
-
--   Nom d'hôte. Si ClickHouse peut analyser la chaîne d'entrée en tant QU'URL.
--   Chaîne vide. Si ClickHouse ne peut pas analyser la chaîne d'entrée en tant QU'URL.
-
-Type: `String`.
-
-**Exemple**
-
-``` sql
-SELECT domain('svn+ssh://some.svn-hosting.com:80/repo/trunk')
-```
-
-``` text
-┌─domain('svn+ssh://some.svn-hosting.com:80/repo/trunk')─┐
-│ some.svn-hosting.com                                   │
-└────────────────────────────────────────────────────────┘
-```
-
-### domainWithoutWWW {#domainwithoutwww}
-
-Renvoie le domaine et ne supprime pas plus d'un ‘www.’ dès le début de celui-ci, si présent.
-
-### topLevelDomain {#topleveldomain}
-
-Extrait le domaine de premier niveau d'une URL.
-
-``` sql
-topLevelDomain(url)
-```
-
-**Paramètre**
-
--   `url` — URL. Type: [Chaîne](../../sql-reference/data-types/string.md).
-
-L'URL peut être spécifiée avec ou sans schéma. Exemple:
-
-``` text
-svn+ssh://some.svn-hosting.com:80/repo/trunk
-some.svn-hosting.com:80/repo/trunk
-https://yandex.com/time/
-```
-
-**Valeurs renvoyées**
-
--   Nom de domaine. Si ClickHouse peut analyser la chaîne d'entrée en tant QU'URL.
--   Chaîne vide. Si ClickHouse ne peut pas analyser la chaîne d'entrée en tant QU'URL.
-
-Type: `String`.
-
-**Exemple**
-
-``` sql
-SELECT topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk')
-```
-
-``` text
-┌─topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk')─┐
-│ com                                                                │
-└────────────────────────────────────────────────────────────────────┘
-```
-
-### firstSignificantSubdomain {#firstsignificantsubdomain}
-
-Renvoie la “first significant subdomain”. C'est un concept non standard spécifique à Yandex.Metrica. Le premier sous-domaine significatif est un domaine de deuxième niveau s'il est ‘com’, ‘net’, ‘org’, ou ‘co’. Sinon, il est un domaine de troisième niveau. Exemple, `firstSignificantSubdomain (‘https://news.yandex.ru/’) = ‘yandex’, firstSignificantSubdomain (‘https://news.yandex.com.tr/’) = ‘yandex’`. La liste des “insignificant” les domaines de deuxième niveau et d'autres détails de mise en œuvre peuvent changer à l'avenir.
-
-### cutToFirstSignificantSubdomain {#cuttofirstsignificantsubdomain}
-
-Renvoie la partie du domaine qui inclut les sous-domaines de premier niveau “first significant subdomain” (voir l'explication ci-dessus).
-
-Exemple, `cutToFirstSignificantSubdomain('https://news.yandex.com.tr/') = 'yandex.com.tr'`.
-
-### chemin {#path}
-
-Retourne le chemin d'accès. Exemple: `/top/news.html` Le chemin n'inclut pas la chaîne de requête.
-
-### pathFull {#pathfull}
-
-La même chose que ci-dessus, mais y compris la chaîne de requête et le fragment. Exemple: / top / nouvelles.le html?page = 2 # commentaires
-
-### queryString {#querystring}
-
-Retourne la chaîne de requête. Exemple: page = 1 & lr=213. query-string n'inclut pas le point d'interrogation initial, ainsi que # et tout ce qui suit #.
-
-### fragment {#fragment}
-
-Renvoie l'identificateur de fragment. fragment n'inclut pas le symbole de hachage initial.
-
-### queryStringAndFragment {#querystringandfragment}
-
-Renvoie la chaîne de requête et l'Identificateur de fragment. Exemple: page = 1 # 29390.
-
-### extractURLParameter (URL, nom) {#extracturlparameterurl-name}
-
-Renvoie la valeur de la ‘name’ paramètre dans l'URL, le cas échéant. Sinon, une chaîne vide. S'il y a beaucoup de paramètres avec ce nom, il renvoie la première occurrence. Cette fonction fonctionne en supposant que le nom du paramètre est codé dans L'URL exactement de la même manière que dans l'argument passé.
-
-### extractURLParameters (URL) {#extracturlparametersurl}
-
-Renvoie un tableau de chaînes name = value correspondant aux paramètres D'URL. Les valeurs ne sont en aucun cas décodées.
-
-### extractURLParameterNames (URL) {#extracturlparameternamesurl}
-
-Retourne un tableau de chaînes de noms correspondant aux noms des paramètres d'URL. Les valeurs ne sont en aucun cas décodées.
-
-### URLHierarchy (URL) {#urlhierarchyurl}
-
-Retourne un tableau contenant L'URL, tronquée à la fin par les symboles /,? dans le chemin et la chaîne de requête. Les caractères séparateurs consécutifs sont comptés comme un. La coupe est faite dans la position après tous les caractères de séparation consécutifs.
-
-### URLPathHierarchy (URL) {#urlpathhierarchyurl}
-
-La même chose que ci-dessus, mais sans le protocole et l'hôte dans le résultat. Le / les élément (racine) n'est pas inclus. Exemple: la fonction est utilisée pour implémenter l'arborescence des rapports de L'URL dans Yandex. Métrique.
-
-``` text
-URLPathHierarchy('https://example.com/browse/CONV-6788') =
-[
-    '/browse/',
-    '/browse/CONV-6788'
-]
-```
-
-### decodeURLComponent (URL) {#decodeurlcomponenturl}
-
-Renvoie L'URL décodée.
-Exemple:
-
-``` sql
-SELECT decodeURLComponent('http://127.0.0.1:8123/?query=SELECT%201%3B') AS DecodedURL;
-```
-
-``` text
-┌─DecodedURL─────────────────────────────┐
-│ http://127.0.0.1:8123/?query=SELECT 1; │
-└────────────────────────────────────────┘
-```
-
-## Fonctions qui suppriment une partie D'une URL {#functions-that-remove-part-of-a-url}
-
-Si L'URL n'a rien de similaire, L'URL reste inchangée.
-
-### cutWWW {#cutwww}
-
-Supprime pas plus d'une ‘www.’ depuis le début du domaine de L'URL, s'il est présent.
-
-### cutQueryString {#cutquerystring}
-
-Supprime la chaîne de requête. Le point d'interrogation est également supprimé.
-
-### cutFragment {#cutfragment}
-
-Supprime l'identificateur de fragment. Le signe est également supprimé.
-
-### couperystringandfragment {#cutquerystringandfragment}
-
-Supprime la chaîne de requête et l'Identificateur de fragment. Le point d'interrogation et le signe numérique sont également supprimés.
-
-### cutURLParameter (URL, nom) {#cuturlparameterurl-name}
-
-Supprime le ‘name’ Paramètre URL, si présent. Cette fonction fonctionne en supposant que le nom du paramètre est codé dans L'URL exactement de la même manière que dans l'argument passé.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/url_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/uuid-functions.md b/docs/fr/sql-reference/functions/uuid-functions.md
deleted file mode 100644
index 9f9eb67d3e9b..000000000000
--- a/docs/fr/sql-reference/functions/uuid-functions.md
+++ /dev/null
@@ -1,122 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 53
-toc_title: Travailler avec UUID
----
-
-# Fonctions pour travailler avec UUID {#functions-for-working-with-uuid}
-
-Les fonctions pour travailler avec UUID sont listées ci-dessous.
-
-## generateUUIDv4 {#uuid-function-generate}
-
-Génère le [UUID](../../sql-reference/data-types/uuid.md) de [la version 4](https://tools.ietf.org/html/rfc4122#section-4.4).
-
-``` sql
-generateUUIDv4()
-```
-
-**Valeur renvoyée**
-
-La valeur de type UUID.
-
-**Exemple d'utilisation**
-
-Cet exemple montre la création d'une table avec la colonne de type UUID et l'insertion d'une valeur dans la table.
-
-``` sql
-CREATE TABLE t_uuid (x UUID) ENGINE=TinyLog
-
-INSERT INTO t_uuid SELECT generateUUIDv4()
-
-SELECT * FROM t_uuid
-```
-
-``` text
-┌────────────────────────────────────x─┐
-│ f4bf890f-f9dc-4332-ad5c-0c18e73f28e9 │
-└──────────────────────────────────────┘
-```
-
-## toUUID (x) {#touuid-x}
-
-Convertit la valeur de type de chaîne en type UUID.
-
-``` sql
-toUUID(String)
-```
-
-**Valeur renvoyée**
-
-La valeur de type UUID.
-
-**Exemple d'utilisation**
-
-``` sql
-SELECT toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0') AS uuid
-```
-
-``` text
-┌─────────────────────────────────uuid─┐
-│ 61f0c404-5cb3-11e7-907b-a6006ad3dba0 │
-└──────────────────────────────────────┘
-```
-
-## UUIDStringToNum {#uuidstringtonum}
-
-Accepte une chaîne contenant 36 caractères dans le format `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`, et le renvoie comme un ensemble d'octets dans un [FixedString (16)](../../sql-reference/data-types/fixedstring.md).
-
-``` sql
-UUIDStringToNum(String)
-```
-
-**Valeur renvoyée**
-
-FixedString (16)
-
-**Exemples d'utilisation**
-
-``` sql
-SELECT
-    '612f3c40-5d3b-217e-707b-6a546a3d7b29' AS uuid,
-    UUIDStringToNum(uuid) AS bytes
-```
-
-``` text
-┌─uuid─────────────────────────────────┬─bytes────────────┐
-│ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │ a/<@];!~p{jTj={) │
-└──────────────────────────────────────┴──────────────────┘
-```
-
-## UUIDNumToString {#uuidnumtostring}
-
-Accepte un [FixedString (16)](../../sql-reference/data-types/fixedstring.md) valeur, et renvoie une chaîne contenant 36 caractères au format texte.
-
-``` sql
-UUIDNumToString(FixedString(16))
-```
-
-**Valeur renvoyée**
-
-Chaîne.
-
-**Exemple d'utilisation**
-
-``` sql
-SELECT
-    'a/<@];!~p{jTj={)' AS bytes,
-    UUIDNumToString(toFixedString(bytes, 16)) AS uuid
-```
-
-``` text
-┌─bytes────────────┬─uuid─────────────────────────────────┐
-│ a/<@];!~p{jTj={) │ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │
-└──────────────────┴──────────────────────────────────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [dictGetUUID](ext-dict-functions.md#ext_dict_functions-other)
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/uuid_function/) <!--hide-->
diff --git a/docs/fr/sql-reference/functions/ym-dict-functions.md b/docs/fr/sql-reference/functions/ym-dict-functions.md
deleted file mode 100644
index f1e4461e24ad..000000000000
--- a/docs/fr/sql-reference/functions/ym-dict-functions.md
+++ /dev/null
@@ -1,155 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 59
-toc_title: Travailler avec Yandex.Dictionnaires Metrica
----
-
-# Fonctions pour travailler avec Yandex.Dictionnaires Metrica {#functions-for-working-with-yandex-metrica-dictionaries}
-
-Pour que les fonctions ci-dessous fonctionnent, la configuration du serveur doit spécifier les chemins et les adresses pour obtenir tous les Yandex.Dictionnaires Metrica. Les dictionnaires sont chargés au premier appel de l'une de ces fonctions. Si les listes de référence ne peuvent pas être chargées, une exception est levée.
-
-Pour plus d'informations sur la création de listes de références, consultez la section “Dictionaries”.
-
-## Plusieurs Geobases {#multiple-geobases}
-
-ClickHouse soutient le travail avec plusieurs géobases alternatives (hiérarchies régionales) simultanément, afin de soutenir diverses perspectives sur les pays auxquels appartiennent certaines régions.
-
-Le ‘clickhouse-server’ config spécifie le fichier avec l'échelon régional::`<path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file>`
-
-Outre ce fichier, il recherche également les fichiers à proximité qui ont le symbole _ et tout suffixe ajouté au nom (avant l'extension de fichier).
-Par exemple, il trouvera également le fichier `/opt/geo/regions_hierarchy_ua.txt` si présente.
-
-`ua` est appelée la clé du dictionnaire. Pour un dictionnaire sans suffixe, la clé est une chaîne vide.
-
-Tous les dictionnaires sont rechargés dans l'exécution (une fois toutes les secondes, comme défini dans le paramètre de configuration builtin_dictionaries_reload_interval, ou une fois par heure par défaut). Cependant, la liste des dictionnaires disponibles est définie une fois, lorsque le serveur démarre.
-
-All functions for working with regions have an optional argument at the end – the dictionary key. It is referred to as the geobase.
-Exemple:
-
-``` sql
-regionToCountry(RegionID) – Uses the default dictionary: /opt/geo/regions_hierarchy.txt
-regionToCountry(RegionID, '') – Uses the default dictionary: /opt/geo/regions_hierarchy.txt
-regionToCountry(RegionID, 'ua') – Uses the dictionary for the 'ua' key: /opt/geo/regions_hierarchy_ua.txt
-```
-
-### regionToCity (id \[, geobase\]) {#regiontocityid-geobase}
-
-Accepts a UInt32 number – the region ID from the Yandex geobase. If this region is a city or part of a city, it returns the region ID for the appropriate city. Otherwise, returns 0.
-
-### regionToArea (id \[, geobase\]) {#regiontoareaid-geobase}
-
-Convertit une région en une zone (tapez 5 dans la géobase). Dans tous les autres cas, cette fonction est la même que ‘regionToCity’.
-
-``` sql
-SELECT DISTINCT regionToName(regionToArea(toUInt32(number), 'ua'))
-FROM system.numbers
-LIMIT 15
-```
-
-``` text
-┌─regionToName(regionToArea(toUInt32(number), \'ua\'))─┐
-│                                                      │
-│ Moscow and Moscow region                             │
-│ St. Petersburg and Leningrad region                  │
-│ Belgorod region                                      │
-│ Ivanovsk region                                      │
-│ Kaluga region                                        │
-│ Kostroma region                                      │
-│ Kursk region                                         │
-│ Lipetsk region                                       │
-│ Orlov region                                         │
-│ Ryazan region                                        │
-│ Smolensk region                                      │
-│ Tambov region                                        │
-│ Tver region                                          │
-│ Tula region                                          │
-└──────────────────────────────────────────────────────┘
-```
-
-### regionToDistrict(id \[, geobase\]) {#regiontodistrictid-geobase}
-
-Convertit une région en district fédéral (type 4 dans la géobase). Dans tous les autres cas, cette fonction est la même que ‘regionToCity’.
-
-``` sql
-SELECT DISTINCT regionToName(regionToDistrict(toUInt32(number), 'ua'))
-FROM system.numbers
-LIMIT 15
-```
-
-``` text
-┌─regionToName(regionToDistrict(toUInt32(number), \'ua\'))─┐
-│                                                          │
-│ Central federal district                                 │
-│ Northwest federal district                               │
-│ South federal district                                   │
-│ North Caucases federal district                          │
-│ Privolga federal district                                │
-│ Ural federal district                                    │
-│ Siberian federal district                                │
-│ Far East federal district                                │
-│ Scotland                                                 │
-│ Faroe Islands                                            │
-│ Flemish region                                           │
-│ Brussels capital region                                  │
-│ Wallonia                                                 │
-│ Federation of Bosnia and Herzegovina                     │
-└──────────────────────────────────────────────────────────┘
-```
-
-### regionToCountry (id \[, geobase\]) {#regiontocountryid-geobase}
-
-Convertit une région en un pays. Dans tous les autres cas, cette fonction est la même que ‘regionToCity’.
-Exemple: `regionToCountry(toUInt32(213)) = 225` convertit Moscou (213) en Russie (225).
-
-### regionToContinent(id \[, géobase\]) {#regiontocontinentid-geobase}
-
-Convertit une région en continent. Dans tous les autres cas, cette fonction est la même que ‘regionToCity’.
-Exemple: `regionToContinent(toUInt32(213)) = 10001` convertit Moscou (213) en Eurasie (10001).
-
-### regionToTopContinent (#regiontotopcontinent) {#regiontotopcontinent-regiontotopcontinent}
-
-Trouve le continent le plus élevé dans la hiérarchie de la région.
-
-**Syntaxe**
-
-``` sql
-regionToTopContinent(id[, geobase]);
-```
-
-**Paramètre**
-
--   `id` — Region ID from the Yandex geobase. [UInt32](../../sql-reference/data-types/int-uint.md).
--   `geobase` — Dictionary key. See [Plusieurs Geobases](#multiple-geobases). [Chaîne](../../sql-reference/data-types/string.md). Facultatif.
-
-**Valeur renvoyée**
-
--   Identifiant du continent de haut niveau (ce dernier lorsque vous grimpez dans la hiérarchie des régions).
--   0, si il n'y a aucun.
-
-Type: `UInt32`.
-
-### regionToPopulation (id \[, geobase\]) {#regiontopopulationid-geobase}
-
-Obtient la population d'une région.
-La population peut être enregistrée dans des fichiers avec la géobase. Voir la section “External dictionaries”.
-Si la population n'est pas enregistrée pour la région, elle renvoie 0.
-Dans la géobase Yandex, la population peut être enregistrée pour les régions enfants, mais pas pour les régions parentes.
-
-### regionIn(lhs, rhs \[, géobase\]) {#regioninlhs-rhs-geobase}
-
-Vérifie si un ‘lhs’ région appartient à une ‘rhs’ région. Renvoie un nombre UInt8 égal à 1 s'il appartient, Ou 0 s'il n'appartient pas.
-The relationship is reflexive – any region also belongs to itself.
-
-### regionHierarchy (id \[, geobase\]) {#regionhierarchyid-geobase}
-
-Accepts a UInt32 number – the region ID from the Yandex geobase. Returns an array of region IDs consisting of the passed region and all parents along the chain.
-Exemple: `regionHierarchy(toUInt32(213)) = [213,1,3,225,10001,10000]`.
-
-### regionToName(id \[, lang\]) {#regiontonameid-lang}
-
-Accepts a UInt32 number – the region ID from the Yandex geobase. A string with the name of the language can be passed as a second argument. Supported languages are: ru, en, ua, uk, by, kz, tr. If the second argument is omitted, the language ‘ru’ is used. If the language is not supported, an exception is thrown. Returns a string – the name of the region in the corresponding language. If the region with the specified ID doesn't exist, an empty string is returned.
-
-`ua` et `uk` les deux signifient ukrainien.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/functions/ym_dict_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/index.md b/docs/fr/sql-reference/index.md
deleted file mode 100644
index 04e44892c05c..000000000000
--- a/docs/fr/sql-reference/index.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "R\xE9f\xE9rence SQL"
-toc_hidden: true
-toc_priority: 28
-toc_title: "cach\xE9s"
----
-
-# Référence SQL {#sql-reference}
-
-ClickHouse prend en charge les types de requêtes suivants:
-
--   [SELECT](statements/select/index.md)
--   [INSERT INTO](statements/insert-into.md)
--   [CREATE](statements/create.md)
--   [ALTER](statements/alter.md#query_language_queries_alter)
--   [Autres types de requêtes](statements/misc.md)
-
-[Article Original](https://clickhouse.tech/docs/en/sql-reference/) <!--hide-->
diff --git a/docs/fr/sql-reference/operators/in.md b/docs/fr/sql-reference/operators/in.md
deleted file mode 100644
index d87fe41a04ff..000000000000
--- a/docs/fr/sql-reference/operators/in.md
+++ /dev/null
@@ -1,204 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-### Dans les opérateurs {#select-in-operators}
-
-Le `IN`, `NOT IN`, `GLOBAL IN`, et `GLOBAL NOT IN` les opérateurs sont traitées séparément, car leur fonctionnalité est assez riche.
-
-Le côté gauche de l'opérateur, soit une seule colonne ou un tuple.
-
-Exemple:
-
-``` sql
-SELECT UserID IN (123, 456) FROM ...
-SELECT (CounterID, UserID) IN ((34, 123), (101500, 456)) FROM ...
-```
-
-Si le côté gauche est une colonne unique qui est dans l'index, et le côté droit est un ensemble de constantes, le système utilise l'index pour le traitement de la requête.
-
-Don't list too many values explicitly (i.e. millions). If a data set is large, put it in a temporary table (for example, see the section “External data for query processing”), puis utiliser une sous-requête.
-
-Le côté droit de l'opérateur peut être un ensemble d'expressions constantes, un ensemble de tuples avec des expressions constantes (illustrées dans les exemples ci-dessus), ou le nom d'une table de base de données ou une sous-requête SELECT entre parenthèses.
-
-Si le côté droit de l'opérateur est le nom d'une table (par exemple, `UserID IN users`), ceci est équivalent à la sous-requête `UserID IN (SELECT * FROM users)`. Utilisez ceci lorsque vous travaillez avec des données externes envoyées avec la requête. Par exemple, la requête peut être envoyée avec un ensemble d'ID utilisateur chargés dans le ‘users’ table temporaire, qui doit être filtrée.
-
-Si le côté droit de l'opérateur est un nom de table qui a le moteur Set (un ensemble de données préparé qui est toujours en RAM), l'ensemble de données ne sera pas créé à nouveau pour chaque requête.
-
-La sous-requête peut spécifier plusieurs colonnes pour filtrer les tuples.
-Exemple:
-
-``` sql
-SELECT (CounterID, UserID) IN (SELECT CounterID, UserID FROM ...) FROM ...
-```
-
-Les colonnes à gauche et à droite de l'opérateur doit avoir le même type.
-
-L'opérateur IN et la sous-requête peuvent se produire dans n'importe quelle partie de la requête, y compris dans les fonctions d'agrégation et les fonctions lambda.
-Exemple:
-
-``` sql
-SELECT
-    EventDate,
-    avg(UserID IN
-    (
-        SELECT UserID
-        FROM test.hits
-        WHERE EventDate = toDate('2014-03-17')
-    )) AS ratio
-FROM test.hits
-GROUP BY EventDate
-ORDER BY EventDate ASC
-```
-
-``` text
-┌──EventDate─┬────ratio─┐
-│ 2014-03-17 │        1 │
-│ 2014-03-18 │ 0.807696 │
-│ 2014-03-19 │ 0.755406 │
-│ 2014-03-20 │ 0.723218 │
-│ 2014-03-21 │ 0.697021 │
-│ 2014-03-22 │ 0.647851 │
-│ 2014-03-23 │ 0.648416 │
-└────────────┴──────────┘
-```
-
-Pour chaque jour après le 17 mars, comptez le pourcentage de pages vues par les utilisateurs qui ont visité le site le 17 mars.
-Une sous-requête dans la clause est toujours exécuter une seule fois sur un seul serveur. Il n'y a pas de sous-requêtes dépendantes.
-
-## Le Traitement NULL {#null-processing-1}
-
-Pendant le traitement de la demande, l'opérateur n'assume que le résultat d'une opération avec [NULL](../syntax.md#null-literal) est toujours égale à `0` indépendamment de savoir si `NULL` est sur le côté droit ou gauche de l'opérateur. `NULL` les valeurs ne sont incluses dans aucun jeu de données, ne correspondent pas entre elles et ne peuvent pas être comparées.
-
-Voici un exemple avec le `t_null` table:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    3 │
-└───┴──────┘
-```
-
-L'exécution de la requête `SELECT x FROM t_null WHERE y IN (NULL,3)` vous donne le résultat suivant:
-
-``` text
-┌─x─┐
-│ 2 │
-└───┘
-```
-
-Vous pouvez voir que la ligne dans laquelle `y = NULL` est jeté hors de résultats de la requête. C'est parce que ClickHouse ne peut pas décider si `NULL` est inclus dans le `(NULL,3)` ensemble, les retours `0` comme le résultat de l'opération, et `SELECT` exclut cette ligne de la sortie finale.
-
-``` sql
-SELECT y IN (NULL, 3)
-FROM t_null
-```
-
-``` text
-┌─in(y, tuple(NULL, 3))─┐
-│                     0 │
-│                     1 │
-└───────────────────────┘
-```
-
-## Sous-Requêtes Distribuées {#select-distributed-subqueries}
-
-Il y a deux options pour IN-S avec des sous-requêtes (similaires aux jointures): normal `IN` / `JOIN` et `GLOBAL IN` / `GLOBAL JOIN`. Ils diffèrent dans la façon dont ils sont exécutés pour le traitement des requêtes distribuées.
-
-!!! attention "Attention"
-    Rappelez-vous que les algorithmes décrits ci-dessous peuvent travailler différemment en fonction de la [paramètre](../../operations/settings/settings.md) `distributed_product_mode` paramètre.
-
-Lors de l'utilisation de l'IN régulier, la requête est envoyée à des serveurs distants, et chacun d'eux exécute les sous-requêtes dans le `IN` ou `JOIN` clause.
-
-Lors de l'utilisation de `GLOBAL IN` / `GLOBAL JOINs`, d'abord toutes les sous-requêtes sont exécutées pour `GLOBAL IN` / `GLOBAL JOINs`, et les résultats sont recueillis dans des tableaux temporaires. Ensuite, les tables temporaires sont envoyés à chaque serveur distant, où les requêtes sont exécutées à l'aide temporaire de données.
-
-Pour une requête non distribuée, utilisez `IN` / `JOIN`.
-
-Soyez prudent lorsque vous utilisez des sous-requêtes dans le `IN` / `JOIN` clauses pour le traitement des requêtes distribuées.
-
-Regardons quelques exemples. Supposons que chaque serveur du cluster a un **local_table**. Chaque serveur dispose également d'une **table distributed_table** table avec le **Distribué** type, qui regarde tous les serveurs du cluster.
-
-Pour une requête à l' **table distributed_table**, la requête sera envoyée à tous les serveurs distants et exécutée sur eux en utilisant le **local_table**.
-
-Par exemple, la requête
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table
-```
-
-sera envoyé à tous les serveurs distants
-
-``` sql
-SELECT uniq(UserID) FROM local_table
-```
-
-et l'exécuter sur chacun d'eux en parallèle, jusqu'à ce qu'il atteigne le stade où les résultats intermédiaires peuvent être combinés. Ensuite, les résultats intermédiaires seront retournés au demandeur de serveur et de fusion, et le résultat final sera envoyé au client.
-
-Examinons maintenant une requête avec IN:
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
-```
-
--   Calcul de l'intersection des audiences de deux sites.
-
-Cette requête sera envoyée à tous les serveurs distants
-
-``` sql
-SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
-```
-
-En d'autres termes, l'ensemble de données de la clause IN sera collecté sur chaque serveur indépendamment, uniquement à travers les données stockées localement sur chacun des serveurs.
-
-Cela fonctionnera correctement et de manière optimale si vous êtes prêt pour ce cas et que vous avez réparti les données entre les serveurs de cluster de telle sorte que les données d'un seul ID utilisateur résident entièrement sur un seul serveur. Dans ce cas, toutes les données nécessaires seront disponibles localement sur chaque serveur. Sinon, le résultat sera erroné. Nous nous référons à cette variation de la requête que “local IN”.
-
-Pour corriger le fonctionnement de la requête lorsque les données sont réparties aléatoirement sur les serveurs de cluster, vous pouvez spécifier **table distributed_table** à l'intérieur d'une sous-requête. La requête ressemblerait à ceci:
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
-```
-
-Cette requête sera envoyée à tous les serveurs distants
-
-``` sql
-SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
-```
-
-La sous-requête commencera à s'exécuter sur chaque serveur distant. Étant donné que la sous-requête utilise une table distribuée, la sous-requête qui se trouve sur chaque serveur distant sera renvoyée à chaque serveur distant comme
-
-``` sql
-SELECT UserID FROM local_table WHERE CounterID = 34
-```
-
-Par exemple, si vous avez un cluster de 100 SERVEURS, l'exécution de la requête entière nécessitera 10 000 requêtes élémentaires, ce qui est généralement considéré comme inacceptable.
-
-Dans de tels cas, vous devez toujours utiliser GLOBAL IN au lieu de IN. Voyons comment cela fonctionne pour la requête
-
-``` sql
-SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID GLOBAL IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
-```
-
-Le serveur demandeur exécutera la sous requête
-
-``` sql
-SELECT UserID FROM distributed_table WHERE CounterID = 34
-```
-
-et le résultat sera mis dans une table temporaire en RAM. Ensuite, la demande sera envoyée à chaque serveur distant
-
-``` sql
-SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID GLOBAL IN _data1
-```
-
-et la table temporaire `_data1` sera envoyé à chaque serveur distant avec la requête (le nom de la table temporaire est défini par l'implémentation).
-
-Ceci est plus optimal que d'utiliser la normale dans. Cependant, gardez les points suivants à l'esprit:
-
-1.  Lors de la création d'une table temporaire, les données ne sont pas uniques. Pour réduire le volume de données transmises sur le réseau, spécifiez DISTINCT dans la sous-requête. (Vous n'avez pas besoin de le faire pour un IN normal.)
-2.  La table temporaire sera envoyé à tous les serveurs distants. La Transmission ne tient pas compte de la topologie du réseau. Par exemple, si 10 serveurs distants résident dans un centre de données très distant par rapport au serveur demandeur, les données seront envoyées 10 fois sur le canal au centre de données distant. Essayez d'éviter les grands ensembles de données lorsque vous utilisez GLOBAL IN.
-3.  Lors de la transmission de données à des serveurs distants, les restrictions sur la bande passante réseau ne sont pas configurables. Vous pourriez surcharger le réseau.
-4.  Essayez de distribuer les données entre les serveurs afin que vous n'ayez pas besoin D'utiliser GLOBAL IN sur une base régulière.
-5.  Si vous devez utiliser GLOBAL in souvent, planifiez l'emplacement du cluster ClickHouse de sorte qu'un seul groupe de répliques ne réside pas dans plus d'un centre de données avec un réseau rapide entre eux, de sorte qu'une requête puisse être traitée entièrement dans un seul centre de données.
-
-Il est également judicieux de spécifier une table locale dans le `GLOBAL IN` clause, dans le cas où cette table locale est uniquement disponible sur le serveur demandeur et que vous souhaitez utiliser les données de celui-ci sur des serveurs distants.
diff --git a/docs/fr/sql-reference/operators/index.md b/docs/fr/sql-reference/operators/index.md
deleted file mode 100644
index 1635c7eece34..000000000000
--- a/docs/fr/sql-reference/operators/index.md
+++ /dev/null
@@ -1,277 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: "Op\xE9rateur"
----
-
-# Opérateur {#operators}
-
-ClickHouse transforme les opérateurs en leurs fonctions correspondantes à l'étape d'analyse des requêtes en fonction de leur priorité, de leur priorité et de leur associativité.
-
-## Des Opérateurs D'Accès {#access-operators}
-
-`a[N]` – Access to an element of an array. The `arrayElement(a, N)` fonction.
-
-`a.N` – Access to a tuple element. The `tupleElement(a, N)` fonction.
-
-## Opérateur De Négation Numérique {#numeric-negation-operator}
-
-`-a` – The `negate (a)` fonction.
-
-## Opérateurs de Multiplication et de Division {#multiplication-and-division-operators}
-
-`a * b` – The `multiply (a, b)` fonction.
-
-`a / b` – The `divide(a, b)` fonction.
-
-`a % b` – The `modulo(a, b)` fonction.
-
-## Opérateurs d'Addition et de soustraction {#addition-and-subtraction-operators}
-
-`a + b` – The `plus(a, b)` fonction.
-
-`a - b` – The `minus(a, b)` fonction.
-
-## Opérateurs De Comparaison {#comparison-operators}
-
-`a = b` – The `equals(a, b)` fonction.
-
-`a == b` – The `equals(a, b)` fonction.
-
-`a != b` – The `notEquals(a, b)` fonction.
-
-`a <> b` – The `notEquals(a, b)` fonction.
-
-`a <= b` – The `lessOrEquals(a, b)` fonction.
-
-`a >= b` – The `greaterOrEquals(a, b)` fonction.
-
-`a < b` – The `less(a, b)` fonction.
-
-`a > b` – The `greater(a, b)` fonction.
-
-`a LIKE s` – The `like(a, b)` fonction.
-
-`a NOT LIKE s` – The `notLike(a, b)` fonction.
-
-`a BETWEEN b AND c` – The same as `a >= b AND a <= c`.
-
-`a NOT BETWEEN b AND c` – The same as `a < b OR a > c`.
-
-## Opérateurs pour travailler avec des ensembles de données {#operators-for-working-with-data-sets}
-
-*Voir [Dans les opérateurs](in.md).*
-
-`a IN ...` – The `in(a, b)` fonction.
-
-`a NOT IN ...` – The `notIn(a, b)` fonction.
-
-`a GLOBAL IN ...` – The `globalIn(a, b)` fonction.
-
-`a GLOBAL NOT IN ...` – The `globalNotIn(a, b)` fonction.
-
-## Opérateurs pour travailler avec des Dates et des heures {#operators-datetime}
-
-### EXTRACT {#operator-extract}
-
-``` sql
-EXTRACT(part FROM date);
-```
-
-Extraire des parties d'une date donnée. Par exemple, vous pouvez récupérer un mois à partir d'une date donnée, ou d'une seconde à partir d'un moment.
-
-Le `part` paramètre spécifie la partie de la date à récupérer. Les valeurs suivantes sont disponibles:
-
--   `DAY` — The day of the month. Possible values: 1–31.
--   `MONTH` — The number of a month. Possible values: 1–12.
--   `YEAR` — The year.
--   `SECOND` — The second. Possible values: 0–59.
--   `MINUTE` — The minute. Possible values: 0–59.
--   `HOUR` — The hour. Possible values: 0–23.
-
-Le `part` le paramètre est insensible à la casse.
-
-Le `date` paramètre spécifie la date ou l'heure à traiter. Soit [Date](../../sql-reference/data-types/date.md) ou [DateTime](../../sql-reference/data-types/datetime.md) le type est pris en charge.
-
-Exemple:
-
-``` sql
-SELECT EXTRACT(DAY FROM toDate('2017-06-15'));
-SELECT EXTRACT(MONTH FROM toDate('2017-06-15'));
-SELECT EXTRACT(YEAR FROM toDate('2017-06-15'));
-```
-
-Dans l'exemple suivant, nous créons un tableau et de les insérer dans une valeur avec le `DateTime` type.
-
-``` sql
-CREATE TABLE test.Orders
-(
-    OrderId UInt64,
-    OrderName String,
-    OrderDate DateTime
-)
-ENGINE = Log;
-```
-
-``` sql
-INSERT INTO test.Orders VALUES (1, 'Jarlsberg Cheese', toDateTime('2008-10-11 13:23:44'));
-```
-
-``` sql
-SELECT
-    toYear(OrderDate) AS OrderYear,
-    toMonth(OrderDate) AS OrderMonth,
-    toDayOfMonth(OrderDate) AS OrderDay,
-    toHour(OrderDate) AS OrderHour,
-    toMinute(OrderDate) AS OrderMinute,
-    toSecond(OrderDate) AS OrderSecond
-FROM test.Orders;
-```
-
-``` text
-┌─OrderYear─┬─OrderMonth─┬─OrderDay─┬─OrderHour─┬─OrderMinute─┬─OrderSecond─┐
-│      2008 │         10 │       11 │        13 │          23 │          44 │
-└───────────┴────────────┴──────────┴───────────┴─────────────┴─────────────┘
-```
-
-Vous pouvez voir plus d'exemples de [test](https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/00619_extract.sql).
-
-### INTERVAL {#operator-interval}
-
-Crée un [Intervalle](../../sql-reference/data-types/special-data-types/interval.md)- valeur de type qui doit être utilisée dans les opérations arithmétiques avec [Date](../../sql-reference/data-types/date.md) et [DateTime](../../sql-reference/data-types/datetime.md)-type de valeurs.
-
-Types d'intervalles:
-- `SECOND`
-- `MINUTE`
-- `HOUR`
-- `DAY`
-- `WEEK`
-- `MONTH`
-- `QUARTER`
-- `YEAR`
-
-!!! warning "Avertissement"
-    Les intervalles avec différents types ne peuvent pas être combinés. Vous ne pouvez pas utiliser des expressions comme `INTERVAL 4 DAY 1 HOUR`. Spécifiez des intervalles en unités inférieures ou égales à la plus petite unité de l'intervalle, par exemple, `INTERVAL 25 HOUR`. Vous pouvez utiliser les opérations consécutives, comme dans l'exemple ci-dessous.
-
-Exemple:
-
-``` sql
-SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR
-```
-
-``` text
-┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐
-│ 2019-10-23 11:16:28 │                                    2019-10-27 14:16:28 │
-└─────────────────────┴────────────────────────────────────────────────────────┘
-```
-
-**Voir Aussi**
-
--   [Intervalle](../../sql-reference/data-types/special-data-types/interval.md) type de données
--   [toInterval](../../sql-reference/functions/type-conversion-functions.md#function-tointerval) type fonctions de conversion
-
-## Opérateur De Négation Logique {#logical-negation-operator}
-
-`NOT a` – The `not(a)` fonction.
-
-## Logique ET de l'Opérateur {#logical-and-operator}
-
-`a AND b` – The`and(a, b)` fonction.
-
-## Logique ou opérateur {#logical-or-operator}
-
-`a OR b` – The `or(a, b)` fonction.
-
-## Opérateur Conditionnel {#conditional-operator}
-
-`a ? b : c` – The `if(a, b, c)` fonction.
-
-Note:
-
-L'opérateur conditionnel calcule les valeurs de b et c, puis vérifie si la condition a est remplie, puis renvoie la valeur correspondante. Si `b` ou `C` est un [arrayJoin()](../../sql-reference/functions/array-join.md#functions_arrayjoin) fonction, chaque ligne sera répliquée indépendamment de la “a” condition.
-
-## Expression Conditionnelle {#operator_case}
-
-``` sql
-CASE [x]
-    WHEN a THEN b
-    [WHEN ... THEN ...]
-    [ELSE c]
-END
-```
-
-Si `x` est spécifié, alors `transform(x, [a, ...], [b, ...], c)` function is used. Otherwise – `multiIf(a, b, ..., c)`.
-
-Si il n'y a pas de `ELSE c` dans l'expression, la valeur par défaut est `NULL`.
-
-Le `transform` la fonction ne fonctionne pas avec `NULL`.
-
-## Opérateur De Concaténation {#concatenation-operator}
-
-`s1 || s2` – The `concat(s1, s2) function.`
-
-## Opérateur De Création Lambda {#lambda-creation-operator}
-
-`x -> expr` – The `lambda(x, expr) function.`
-
-Les opérateurs suivants n'ont pas de priorité puisqu'ils sont des parenthèses:
-
-## Opérateur De Création De Tableau {#array-creation-operator}
-
-`[x1, ...]` – The `array(x1, ...) function.`
-
-## Opérateur De Création De Tuple {#tuple-creation-operator}
-
-`(x1, x2, ...)` – The `tuple(x2, x2, ...) function.`
-
-## Associativité {#associativity}
-
-Tous les opérateurs binaires ont associativité gauche. Exemple, `1 + 2 + 3` est transformé à `plus(plus(1, 2), 3)`.
-Parfois, cela ne fonctionne pas de la façon que vous attendez. Exemple, `SELECT 4 > 2 > 3` résultat sera 0.
-
-Pour l'efficacité, le `and` et `or` les fonctions acceptent n'importe quel nombre d'arguments. Les chaînes de `AND` et `OR` les opérateurs se sont transformés en un seul appel de ces fonctions.
-
-## La vérification de `NULL` {#checking-for-null}
-
-Clickhouse soutient le `IS NULL` et `IS NOT NULL` opérateur.
-
-### IS NULL {#operator-is-null}
-
--   Pour [Nullable](../../sql-reference/data-types/nullable.md) type de valeurs, l' `IS NULL` opérateur retourne:
-    -   `1` si la valeur est `NULL`.
-    -   `0` autrement.
--   Pour les autres valeurs, la `IS NULL` l'opérateur renvoie toujours `0`.
-
-<!-- -->
-
-``` sql
-SELECT x+100 FROM t_null WHERE y IS NULL
-```
-
-``` text
-┌─plus(x, 100)─┐
-│          101 │
-└──────────────┘
-```
-
-### IS NOT NULL {#is-not-null}
-
--   Pour [Nullable](../../sql-reference/data-types/nullable.md) type de valeurs, l' `IS NOT NULL` opérateur retourne:
-    -   `0` si la valeur est `NULL`.
-    -   `1` autrement.
--   Pour les autres valeurs, la `IS NOT NULL` l'opérateur renvoie toujours `1`.
-
-<!-- -->
-
-``` sql
-SELECT * FROM t_null WHERE y IS NOT NULL
-```
-
-``` text
-┌─x─┬─y─┐
-│ 2 │ 3 │
-└───┴───┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/operators/) <!--hide-->
diff --git a/docs/fr/sql-reference/statements/alter.md b/docs/fr/sql-reference/statements/alter.md
deleted file mode 100644
index 64fe21046a3a..000000000000
--- a/docs/fr/sql-reference/statements/alter.md
+++ /dev/null
@@ -1,602 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 36
-toc_title: ALTER
----
-
-## ALTER {#query_language_queries_alter}
-
-Le `ALTER` la requête est prise en charge uniquement pour `*MergeTree` des tables, ainsi que `Merge`et`Distributed`. La requête a plusieurs variantes.
-
-### Manipulations De Colonne {#column-manipulations}
-
-Modification de la structure de la table.
-
-``` sql
-ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|CLEAR|COMMENT|MODIFY COLUMN ...
-```
-
-Dans la requête, spécifiez une liste d'une ou plusieurs actions séparées par des virgules.
-Chaque action est une opération sur une colonne.
-
-Les actions suivantes sont prises en charge:
-
--   [ADD COLUMN](#alter_add-column) — Adds a new column to the table.
--   [DROP COLUMN](#alter_drop-column) — Deletes the column.
--   [CLEAR COLUMN](#alter_clear-column) — Resets column values.
--   [COMMENT COLUMN](#alter_comment-column) — Adds a text comment to the column.
--   [MODIFY COLUMN](#alter_modify-column) — Changes column's type, default expression and TTL.
-
-Ces actions sont décrites en détail ci-dessous.
-
-#### ADD COLUMN {#alter_add-column}
-
-``` sql
-ADD COLUMN [IF NOT EXISTS] name [type] [default_expr] [codec] [AFTER name_after]
-```
-
-Ajoute une nouvelle colonne à la table spécifiée `name`, `type`, [`codec`](create.md#codecs) et `default_expr` (voir la section [Expressions par défaut](create.md#create-default-values)).
-
-Si l' `IF NOT EXISTS` la clause est incluse, la requête ne retournera pas d'erreur si la colonne existe déjà. Si vous spécifiez `AFTER name_after` (le nom d'une autre colonne), la colonne est ajoutée après celle spécifiée dans la liste des colonnes de la table. Sinon, la colonne est ajoutée à la fin de la table. Notez qu'il n'existe aucun moyen d'ajouter une colonne au début d'un tableau. Pour une chaîne d'actions, `name_after` peut être le nom d'une colonne est ajoutée dans l'une des actions précédentes.
-
-L'ajout d'une colonne modifie simplement la structure de la table, sans effectuer d'actions avec des données. Les données n'apparaissent pas sur le disque après la `ALTER`. Si les données sont manquantes pour une colonne lors de la lecture de la table, elles sont remplies avec des valeurs par défaut (en exécutant l'expression par défaut s'il y en a une, ou en utilisant des zéros ou des chaînes vides). La colonne apparaît sur le disque après la fusion des parties de données (voir [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)).
-
-Cette approche nous permet de compléter le `ALTER` requête instantanément, sans augmenter le volume de données anciennes.
-
-Exemple:
-
-``` sql
-ALTER TABLE visits ADD COLUMN browser String AFTER user_id
-```
-
-#### DROP COLUMN {#alter_drop-column}
-
-``` sql
-DROP COLUMN [IF EXISTS] name
-```
-
-Supprime la colonne avec le nom `name`. Si l' `IF EXISTS` la clause est spécifiée, la requête ne retournera pas d'erreur si la colonne n'existe pas.
-
-Supprime les données du système de fichiers. Comme cela supprime des fichiers entiers, la requête est terminée presque instantanément.
-
-Exemple:
-
-``` sql
-ALTER TABLE visits DROP COLUMN browser
-```
-
-#### CLEAR COLUMN {#alter_clear-column}
-
-``` sql
-CLEAR COLUMN [IF EXISTS] name IN PARTITION partition_name
-```
-
-Réinitialise toutes les données dans une colonne pour une partition spécifiée. En savoir plus sur la définition du nom de la partition dans la section [Comment spécifier l'expression de partition](#alter-how-to-specify-part-expr).
-
-Si l' `IF EXISTS` la clause est spécifiée, la requête ne retournera pas d'erreur si la colonne n'existe pas.
-
-Exemple:
-
-``` sql
-ALTER TABLE visits CLEAR COLUMN browser IN PARTITION tuple()
-```
-
-#### COMMENT COLUMN {#alter_comment-column}
-
-``` sql
-COMMENT COLUMN [IF EXISTS] name 'comment'
-```
-
-Ajoute un commentaire à la colonne. Si l' `IF EXISTS` la clause est spécifiée, la requête ne retournera pas d'erreur si la colonne n'existe pas.
-
-Chaque colonne peut avoir un commentaire. Si un commentaire existe déjà pour la colonne, un nouveau commentaire remplace le précédent commentaire.
-
-Les commentaires sont stockés dans le `comment_expression` colonne renvoyée par le [DESCRIBE TABLE](misc.md#misc-describe-table) requête.
-
-Exemple:
-
-``` sql
-ALTER TABLE visits COMMENT COLUMN browser 'The table shows the browser used for accessing the site.'
-```
-
-#### MODIFY COLUMN {#alter_modify-column}
-
-``` sql
-MODIFY COLUMN [IF EXISTS] name [type] [default_expr] [TTL]
-```
-
-Cette requête modifie le `name` les propriétés de la colonne:
-
--   Type
-
--   Expression par défaut
-
--   TTL
-
-        For examples of columns TTL modifying, see [Column TTL](../engines/table_engines/mergetree_family/mergetree.md#mergetree-column-ttl).
-
-Si l' `IF EXISTS` la clause est spécifiée, la requête ne retournera pas d'erreur si la colonne n'existe pas.
-
-Lors de la modification du type, les valeurs sont converties comme si [toType](../../sql-reference/functions/type-conversion-functions.md) les fonctions ont été appliquées. Si seule l'expression par défaut est modifiée, la requête ne fait rien de complexe et est terminée presque instantanément.
-
-Exemple:
-
-``` sql
-ALTER TABLE visits MODIFY COLUMN browser Array(String)
-```
-
-Changing the column type is the only complex action – it changes the contents of files with data. For large tables, this may take a long time.
-
-Il y a plusieurs étapes de traitement:
-
--   Préparation de (nouveaux) fichiers temporaires avec des données modifiées.
--   Renommer les anciens fichiers.
--   Renommer les (nouveaux) fichiers temporaires en anciens noms.
--   Suppression des anciens fichiers.
-
-Seule la première étape prend du temps. Si il y a un échec à ce stade, les données ne sont pas modifiées.
-En cas d'échec au cours d'une des étapes successives, les données peuvent être restaurées manuellement. L'exception est si les anciens fichiers ont été supprimés du système de fichiers mais que les données des nouveaux fichiers n'ont pas été écrites sur le disque et ont été perdues.
-
-Le `ALTER` la requête de modification des colonnes est répliquée. Les instructions sont enregistrées dans ZooKeeper, puis chaque réplique les applique. Tout `ALTER` les requêtes sont exécutées dans le même ordre. La requête attend que les actions appropriées soient terminées sur les autres répliques. Cependant, une requête pour modifier des colonnes dans une table répliquée peut être interrompue, et toutes les actions seront effectuées de manière asynchrone.
-
-#### Modifier les limites de la requête {#alter-query-limitations}
-
-Le `ALTER` query vous permet de créer et de supprimer des éléments distincts (colonnes) dans des structures de données imbriquées, mais pas des structures de données imbriquées entières. Pour ajouter une structure de données imbriquée, vous pouvez ajouter des colonnes avec un nom comme `name.nested_name` et le type `Array(T)`. Une structure de données imbriquée est équivalente à plusieurs colonnes de tableau avec un nom qui a le même préfixe avant le point.
-
-Il n'y a pas de support pour supprimer des colonnes dans la clé primaire ou la clé d'échantillonnage (colonnes qui sont utilisées dans le `ENGINE` expression). La modification du type des colonnes incluses dans la clé primaire n'est possible que si cette modification n'entraîne pas la modification des données (par exemple, vous êtes autorisé à ajouter des valeurs à une énumération ou à modifier un type de `DateTime` de `UInt32`).
-
-Si l' `ALTER` la requête n'est pas suffisante pour apporter les modifications de table dont vous avez besoin, vous pouvez créer une nouvelle table, y copier les données en utilisant le [INSERT SELECT](insert-into.md#insert_query_insert-select) requête, puis changer les tables en utilisant le [RENAME](misc.md#misc_operations-rename) requête et supprimer l'ancienne table. Vous pouvez utiliser l' [clickhouse-copieur](../../operations/utilities/clickhouse-copier.md) comme une alternative à la `INSERT SELECT` requête.
-
-Le `ALTER` query bloque toutes les lectures et écritures pour la table. En d'autres termes, si une longue `SELECT` est en cours d'exécution au moment de la `ALTER` requête, la `ALTER` la requête va attendre qu'elle se termine. Dans le même temps, toutes les nouvelles requêtes à la même table attendre que ce `ALTER` est en cours d'exécution.
-
-Pour les tables qui ne stockent pas les données elles-mêmes (telles que `Merge` et `Distributed`), `ALTER` change simplement la structure de la table, et ne change pas la structure des tables subordonnées. Par exemple, lors de L'exécution de ALTER pour un `Distributed` table, vous devrez également exécuter `ALTER` pour les tables sur tous les serveurs distants.
-
-### Manipulations avec des Expressions clés {#manipulations-with-key-expressions}
-
-La commande suivante est prise en charge:
-
-``` sql
-MODIFY ORDER BY new_expression
-```
-
-Cela ne fonctionne que pour les tables du [`MergeTree`](../../engines/table-engines/mergetree-family/mergetree.md) de la famille (y compris les
-[répliqué](../../engines/table-engines/mergetree-family/replication.md) table). La commande change l'
-[clé de tri](../../engines/table-engines/mergetree-family/mergetree.md) de la table
-de `new_expression` (une expression ou un tuple d'expressions). Clé primaire reste le même.
-
-La commande est légère en ce sens qu'elle ne modifie que les métadonnées. Pour conserver la propriété cette partie de données
-les lignes sont ordonnées par l'expression de clé de tri vous ne pouvez pas ajouter d'expressions contenant des colonnes existantes
-à la clé de tri (seules les colonnes ajoutées par `ADD COLUMN` commande dans le même `ALTER` requête).
-
-### Manipulations avec des Indices de saut de données {#manipulations-with-data-skipping-indices}
-
-Cela ne fonctionne que pour les tables du [`*MergeTree`](../../engines/table-engines/mergetree-family/mergetree.md) de la famille (y compris les
-[répliqué](../../engines/table-engines/mergetree-family/replication.md) table). Les opérations suivantes
-sont disponibles:
-
--   `ALTER TABLE [db].name ADD INDEX name expression TYPE type GRANULARITY value AFTER name [AFTER name2]` - Ajoute la description de l'index aux métadonnées des tables.
-
--   `ALTER TABLE [db].name DROP INDEX name` - Supprime la description de l'index des métadonnées des tables et supprime les fichiers d'index du disque.
-
-Ces commandes sont légères dans le sens où elles ne modifient que les métadonnées ou suppriment des fichiers.
-En outre, ils sont répliqués (synchronisation des métadonnées des indices via ZooKeeper).
-
-### Manipulations avec contraintes {#manipulations-with-constraints}
-
-En voir plus sur [contraintes](create.md#constraints)
-
-Les contraintes peuvent être ajoutées ou supprimées à l'aide de la syntaxe suivante:
-
-``` sql
-ALTER TABLE [db].name ADD CONSTRAINT constraint_name CHECK expression;
-ALTER TABLE [db].name DROP CONSTRAINT constraint_name;
-```
-
-Les requêtes ajouteront ou supprimeront des métadonnées sur les contraintes de la table afin qu'elles soient traitées immédiatement.
-
-Contrainte de vérifier *ne sera pas exécuté* sur les données existantes si elle a été ajoutée.
-
-Toutes les modifications sur les tables répliquées sont diffusées sur ZooKeeper et seront donc appliquées sur d'autres répliques.
-
-### Manipulations avec des Partitions et des pièces {#alter_manipulations-with-partitions}
-
-Les opérations suivantes avec [partition](../../engines/table-engines/mergetree-family/custom-partitioning-key.md) sont disponibles:
-
--   [DETACH PARTITION](#alter_detach-partition) – Moves a partition to the `detached` répertoire et de l'oublier.
--   [DROP PARTITION](#alter_drop-partition) – Deletes a partition.
--   [ATTACH PART\|PARTITION](#alter_attach-partition) – Adds a part or partition from the `detached` répertoire à la table.
--   [ATTACH PARTITION FROM](#alter_attach-partition-from) – Copies the data partition from one table to another and adds.
--   [REPLACE PARTITION](#alter_replace-partition) - Copie la partition de données d'une table à l'autre et la remplace.
--   [MOVE PARTITION TO TABLE](#alter_move_to_table-partition)(#alter_move_to_table-partition) - déplace la partition de données d'une table à l'autre.
--   [CLEAR COLUMN IN PARTITION](#alter_clear-column-partition) - Rétablit la valeur d'une colonne spécifiée dans une partition.
--   [CLEAR INDEX IN PARTITION](#alter_clear-index-partition) - Réinitialise l'index secondaire spécifié dans une partition.
--   [FREEZE PARTITION](#alter_freeze-partition) – Creates a backup of a partition.
--   [FETCH PARTITION](#alter_fetch-partition) – Downloads a partition from another server.
--   [MOVE PARTITION\|PART](#alter_move-partition) – Move partition/data part to another disk or volume.
-
-<!-- -->
-
-#### DETACH PARTITION {#alter_detach-partition}
-
-``` sql
-ALTER TABLE table_name DETACH PARTITION partition_expr
-```
-
-Déplace toutes les données de la partition spécifiée vers `detached` répertoire. Le serveur oublie la partition de données détachée comme si elle n'existait pas. Le serveur ne connaîtra pas ces données tant que vous n'aurez pas [ATTACH](#alter_attach-partition) requête.
-
-Exemple:
-
-``` sql
-ALTER TABLE visits DETACH PARTITION 201901
-```
-
-Lisez à propos de la définition de l'expression de partition dans une section [Comment spécifier l'expression de partition](#alter-how-to-specify-part-expr).
-
-Une fois la requête exécutée, vous pouvez faire ce que vous voulez avec les données du `detached` directory — delete it from the file system, or just leave it.
-
-This query is replicated – it moves the data to the `detached` répertoire sur toutes les répliques. Notez que vous ne pouvez exécuter cette requête que sur un réplica leader. Pour savoir si une réplique est un leader, effectuez le `SELECT` requête à l' [système.réplique](../../operations/system-tables.md#system_tables-replicas) table. Alternativement, il est plus facile de faire une `DETACH` requête sur toutes les répliques - toutes les répliques lancent une exception, à l'exception de la réplique leader.
-
-#### DROP PARTITION {#alter_drop-partition}
-
-``` sql
-ALTER TABLE table_name DROP PARTITION partition_expr
-```
-
-Supprime la partition spécifiée de la table. Cette requête marque la partition comme inactive et supprime complètement les données, environ en 10 minutes.
-
-Lisez à propos de la définition de l'expression de partition dans une section [Comment spécifier l'expression de partition](#alter-how-to-specify-part-expr).
-
-The query is replicated – it deletes data on all replicas.
-
-#### DROP DETACHED PARTITION\|PART {#alter_drop-detached}
-
-``` sql
-ALTER TABLE table_name DROP DETACHED PARTITION|PART partition_expr
-```
-
-Supprime la partie spécifiée ou toutes les parties de la partition spécifiée de `detached`.
-En savoir plus sur la définition de l'expression de partition dans une section [Comment spécifier l'expression de partition](#alter-how-to-specify-part-expr).
-
-#### ATTACH PARTITION\|PART {#alter_attach-partition}
-
-``` sql
-ALTER TABLE table_name ATTACH PARTITION|PART partition_expr
-```
-
-Ajoute des données à la table à partir du `detached` répertoire. Il est possible d'ajouter des données dans une partition entière ou pour une partie distincte. Exemple:
-
-``` sql
-ALTER TABLE visits ATTACH PARTITION 201901;
-ALTER TABLE visits ATTACH PART 201901_2_2_0;
-```
-
-En savoir plus sur la définition de l'expression de partition dans une section [Comment spécifier l'expression de partition](#alter-how-to-specify-part-expr).
-
-Cette requête est répliquée. L'initiateur de réplica vérifie s'il y a des données dans le `detached` répertoire. Si des données existent, la requête vérifie son intégrité. Si tout est correct, la requête ajoute les données à la table. Tous les autres réplicas téléchargent les données de l'initiateur de réplica.
-
-Ainsi, vous pouvez mettre des données à la `detached` répertoire sur une réplique, et utilisez le `ALTER ... ATTACH` requête pour l'ajouter à la table sur tous les réplicas.
-
-#### ATTACH PARTITION FROM {#alter_attach-partition-from}
-
-``` sql
-ALTER TABLE table2 ATTACH PARTITION partition_expr FROM table1
-```
-
-Cette requête copie la partition de données du `table1` de `table2` ajoute des données de gratuit dans la `table2`. Notez que les données ne seront pas supprimées de `table1`.
-
-Pour que la requête s'exécute correctement, les conditions suivantes doivent être remplies:
-
--   Les deux tables doivent avoir la même structure.
--   Les deux tables doivent avoir la même clé de partition.
-
-#### REPLACE PARTITION {#alter_replace-partition}
-
-``` sql
-ALTER TABLE table2 REPLACE PARTITION partition_expr FROM table1
-```
-
-Cette requête copie la partition de données du `table1` de `table2` et remplace la partition existante dans le `table2`. Notez que les données ne seront pas supprimées de `table1`.
-
-Pour que la requête s'exécute correctement, les conditions suivantes doivent être remplies:
-
--   Les deux tables doivent avoir la même structure.
--   Les deux tables doivent avoir la même clé de partition.
-
-#### MOVE PARTITION TO TABLE {#alter_move_to_table-partition}
-
-``` sql
-ALTER TABLE table_source MOVE PARTITION partition_expr TO TABLE table_dest
-```
-
-Cette requête déplace la partition de données du `table_source` de `table_dest` avec la suppression des données de `table_source`.
-
-Pour que la requête s'exécute correctement, les conditions suivantes doivent être remplies:
-
--   Les deux tables doivent avoir la même structure.
--   Les deux tables doivent avoir la même clé de partition.
--   Les deux tables doivent appartenir à la même famille de moteurs. (répliqué ou non répliqué)
--   Les deux tables doivent avoir la même stratégie de stockage.
-
-#### CLEAR COLUMN IN PARTITION {#alter_clear-column-partition}
-
-``` sql
-ALTER TABLE table_name CLEAR COLUMN column_name IN PARTITION partition_expr
-```
-
-Réinitialise toutes les valeurs de la colonne spécifiée dans une partition. Si l' `DEFAULT` la clause a été déterminée lors de la création d'une table, cette requête définit la valeur de la colonne à une valeur par défaut spécifiée.
-
-Exemple:
-
-``` sql
-ALTER TABLE visits CLEAR COLUMN hour in PARTITION 201902
-```
-
-#### FREEZE PARTITION {#alter_freeze-partition}
-
-``` sql
-ALTER TABLE table_name FREEZE [PARTITION partition_expr]
-```
-
-Cette requête crée une sauvegarde locale d'une partition spécifiée. Si l' `PARTITION` la clause est omise, la requête crée la sauvegarde de toutes les partitions à la fois.
-
-!!! note "Note"
-    L'ensemble du processus de sauvegarde est effectuée sans arrêter le serveur.
-
-Notez que pour les tables de style ancien, vous pouvez spécifier le préfixe du nom de la partition (par exemple, ‘2019’)- ensuite, la requête crée la sauvegarde pour toutes les partitions correspondantes. Lisez à propos de la définition de l'expression de partition dans une section [Comment spécifier l'expression de partition](#alter-how-to-specify-part-expr).
-
-Au moment de l'exécution, pour un instantané de données, la requête crée des liens rigides vers des données de table. Les liens sont placés dans le répertoire `/var/lib/clickhouse/shadow/N/...`, où:
-
--   `/var/lib/clickhouse/` est le répertoire de travail clickhouse spécifié dans la configuration.
--   `N` est le numéro incrémental de la sauvegarde.
-
-!!! note "Note"
-    Si vous utilisez [un ensemble de disques pour le stockage des données dans une table](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes), le `shadow/N` le répertoire apparaît sur chaque disque, stockant les parties de données correspondant `PARTITION` expression.
-
-La même structure de répertoires est créée à l'intérieur de la sauvegarde qu'à l'intérieur `/var/lib/clickhouse/`. La requête effectue ‘chmod’ pour tous les fichiers, interdisant d'écrire en eux.
-
-Après avoir créé la sauvegarde, vous pouvez copier les données depuis `/var/lib/clickhouse/shadow/` sur le serveur distant, puis supprimez-le du serveur local. Notez que l' `ALTER t FREEZE PARTITION` la requête n'est pas répliqué. Il crée une sauvegarde locale uniquement sur le serveur local.
-
-La requête crée une sauvegarde presque instantanément (mais elle attend d'abord que les requêtes en cours à la table correspondante se terminent).
-
-`ALTER TABLE t FREEZE PARTITION` copie uniquement les données, pas les métadonnées de la table. Faire une sauvegarde des métadonnées de la table, copiez le fichier `/var/lib/clickhouse/metadata/database/table.sql`
-
-Pour restaurer des données à partir d'une sauvegarde, procédez comme suit:
-
-1.  Créer la table si elle n'existe pas. Pour afficher la requête, utilisez la .fichier sql (remplacer `ATTACH` avec `CREATE`).
-2.  Copier les données de la `data/database/table/` répertoire à l'intérieur de la sauvegarde `/var/lib/clickhouse/data/database/table/detached/` répertoire.
-3.  Exécuter `ALTER TABLE t ATTACH PARTITION` les requêtes pour ajouter les données à une table.
-
-La restauration à partir d'une sauvegarde ne nécessite pas l'arrêt du serveur.
-
-Pour plus d'informations sur les sauvegardes et la restauration [La Sauvegarde Des Données](../../operations/backup.md) section.
-
-#### CLEAR INDEX IN PARTITION {#alter_clear-index-partition}
-
-``` sql
-ALTER TABLE table_name CLEAR INDEX index_name IN PARTITION partition_expr
-```
-
-La requête fonctionne de manière similaire à `CLEAR COLUMN` mais il remet un index au lieu d'une colonne de données.
-
-#### FETCH PARTITION {#alter_fetch-partition}
-
-``` sql
-ALTER TABLE table_name FETCH PARTITION partition_expr FROM 'path-in-zookeeper'
-```
-
-Télécharge une partition depuis un autre serveur. Cette requête ne fonctionne que pour les tables répliquées.
-
-La requête effectue les opérations suivantes:
-
-1.  Télécharge la partition à partir du fragment spécifié. Dans ‘path-in-zookeeper’ vous devez spécifier un chemin vers le fragment dans ZooKeeper.
-2.  Ensuite, la requête met les données téléchargées dans le `detached` répertoire de la `table_name` table. L'utilisation de la [ATTACH PARTITION\|PART](#alter_attach-partition) requête pour ajouter les données à la table.
-
-Exemple:
-
-``` sql
-ALTER TABLE users FETCH PARTITION 201902 FROM '/clickhouse/tables/01-01/visits';
-ALTER TABLE users ATTACH PARTITION 201902;
-```
-
-Notez que:
-
--   Le `ALTER ... FETCH PARTITION` la requête n'est pas répliqué. Il place la partition à la `detached` répertoire sur le serveur local.
--   Le `ALTER TABLE ... ATTACH` la requête est répliquée. Il ajoute les données à toutes les répliques. Les données sont ajoutées à l'une des répliques `detached` répertoire, et aux autres-des répliques voisines.
-
-Avant le téléchargement, le système vérifie si la partition existe et si la structure de la table correspond. La réplique la plus appropriée est sélectionnée automatiquement parmi les répliques saines.
-
-Bien que la requête soit appelée `ALTER TABLE`, il ne modifie pas la structure de la table et ne modifie pas immédiatement les données disponibles dans la table.
-
-#### MOVE PARTITION\|PART {#alter_move-partition}
-
-Déplace des partitions ou des parties de données vers un autre volume ou disque pour `MergeTree`-tables de moteur. Voir [Utilisation de plusieurs périphériques de bloc pour le stockage de données](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes).
-
-``` sql
-ALTER TABLE table_name MOVE PARTITION|PART partition_expr TO DISK|VOLUME 'disk_name'
-```
-
-Le `ALTER TABLE t MOVE` requête:
-
--   Non répliqué, car différentes répliques peuvent avoir des stratégies de stockage différentes.
--   Renvoie une erreur si le disque ou le volume n'est pas configuré. Query renvoie également une erreur si les conditions de déplacement des données, spécifiées dans la stratégie de stockage, ne peuvent pas être appliquées.
--   Peut renvoyer une erreur dans le cas, lorsque les données à déplacer sont déjà déplacées par un processus en arrière-plan, simultané `ALTER TABLE t MOVE` requête ou à la suite de la fusion de données d'arrière-plan. Un utilisateur ne doit effectuer aucune action supplémentaire dans ce cas.
-
-Exemple:
-
-``` sql
-ALTER TABLE hits MOVE PART '20190301_14343_16206_438' TO VOLUME 'slow'
-ALTER TABLE hits MOVE PARTITION '2019-09-01' TO DISK 'fast_ssd'
-```
-
-#### Comment définir L'Expression de la Partition {#alter-how-to-specify-part-expr}
-
-Vous pouvez spécifier l'expression de partition dans `ALTER ... PARTITION` requêtes de différentes manières:
-
--   Comme une valeur de l' `partition` la colonne de la `system.parts` table. Exemple, `ALTER TABLE visits DETACH PARTITION 201901`.
--   Comme expression de la colonne de la table. Les constantes et les expressions constantes sont prises en charge. Exemple, `ALTER TABLE visits DETACH PARTITION toYYYYMM(toDate('2019-01-25'))`.
--   À l'aide de l'ID de partition. Partition ID est un identifiant de chaîne de la partition (lisible par l'homme, si possible) qui est utilisé comme noms de partitions dans le système de fichiers et dans ZooKeeper. L'ID de partition doit être spécifié dans `PARTITION ID` clause, entre guillemets simples. Exemple, `ALTER TABLE visits DETACH PARTITION ID '201901'`.
--   Dans le [ALTER ATTACH PART](#alter_attach-partition) et [DROP DETACHED PART](#alter_drop-detached) requête, pour spécifier le nom d'une partie, utilisez le littéral de chaîne avec une valeur de `name` la colonne de la [système.detached_parts](../../operations/system-tables.md#system_tables-detached_parts) table. Exemple, `ALTER TABLE visits ATTACH PART '201901_1_1_0'`.
-
-L'utilisation de guillemets lors de la spécification de la partition dépend du type d'expression de partition. Par exemple, pour la `String` type, vous devez spécifier son nom entre guillemets (`'`). Pour l' `Date` et `Int*` types aucune citation n'est nécessaire.
-
-Pour les tables de style ancien, vous pouvez spécifier la partition sous forme de nombre `201901` ou une chaîne de caractères `'201901'`. La syntaxe des tables new-style est plus stricte avec les types (similaire à l'analyseur pour le format D'entrée des valeurs).
-
-Toutes les règles ci-dessus sont aussi valables pour la [OPTIMIZE](misc.md#misc_operations-optimize) requête. Si vous devez spécifier la seule partition lors de l'optimisation d'une table non partitionnée, définissez l'expression `PARTITION tuple()`. Exemple:
-
-``` sql
-OPTIMIZE TABLE table_not_partitioned PARTITION tuple() FINAL;
-```
-
-Les exemples de `ALTER ... PARTITION` les requêtes sont démontrées dans les tests [`00502_custom_partitioning_local`](https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/00502_custom_partitioning_local.sql) et [`00502_custom_partitioning_replicated_zookeeper`](https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/00502_custom_partitioning_replicated_zookeeper.sql).
-
-### Manipulations avec Table TTL {#manipulations-with-table-ttl}
-
-Vous pouvez modifier [tableau TTL](../../engines/table-engines/mergetree-family/mergetree.md#mergetree-table-ttl) avec une demande du formulaire suivant:
-
-``` sql
-ALTER TABLE table-name MODIFY TTL ttl-expression
-```
-
-### Synchronicité des requêtes ALTER {#synchronicity-of-alter-queries}
-
-Pour les tables non réplicables, tous `ALTER` les requêtes sont exécutées simultanément. Pour les tables réplicables, la requête ajoute simplement des instructions pour les actions appropriées à `ZooKeeper` et les actions elles-mêmes sont effectuées dès que possible. Cependant, la requête peut attendre que ces actions soient terminées sur tous les réplicas.
-
-Pour `ALTER ... ATTACH|DETACH|DROP` les requêtes, vous pouvez utiliser le `replication_alter_partitions_sync` configuration pour configurer l'attente.
-Valeurs possibles: `0` – do not wait; `1` – only wait for own execution (default); `2` – wait for all.
-
-### Mutation {#alter-mutations}
-
-Les Mutations sont une variante ALTER query qui permet de modifier ou de supprimer des lignes dans une table. Contrairement à la norme `UPDATE` et `DELETE` les requêtes qui sont destinées aux changements de données de point, les mutations sont destinées aux opérations lourdes qui modifient beaucoup de lignes dans une table. Pris en charge pour le `MergeTree` famille de moteurs de table, y compris les moteurs avec support de réplication.
-
-Les tables existantes sont prêtes pour les mutations telles quelles (aucune conversion nécessaire), mais après l'application de la première mutation à une table, son format de métadonnées devient incompatible avec les versions précédentes du serveur et il devient impossible de revenir à une version précédente.
-
-Commandes actuellement disponibles:
-
-``` sql
-ALTER TABLE [db.]table DELETE WHERE filter_expr
-```
-
-Le `filter_expr` doit être de type `UInt8`. La requête supprime les lignes de la table pour lesquelles cette expression prend une valeur différente de zéro.
-
-``` sql
-ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr
-```
-
-Le `filter_expr` doit être de type `UInt8`. Cette requête met à jour les valeurs des colonnes spécifiées en les valeurs des expressions correspondantes dans les lignes pour lesquelles `filter_expr` prend une valeur non nulle. Les valeurs sont converties en type de colonne à l'aide `CAST` opérateur. La mise à jour des colonnes utilisées dans le calcul de la clé primaire ou de la clé de partition n'est pas prise en charge.
-
-``` sql
-ALTER TABLE [db.]table MATERIALIZE INDEX name IN PARTITION partition_name
-```
-
-La requête reconstruit l'index secondaire `name` dans la partition `partition_name`.
-
-Une requête peut contenir plusieurs commandes séparées par des virgules.
-
-Pour les tables \* MergeTree, les mutations s'exécutent en réécrivant des parties de données entières. Il n'y a pas d'atomicité-les pièces sont substituées aux pièces mutées dès qu'elles sont prêtes et un `SELECT` la requête qui a commencé à s'exécuter pendant une mutation verra les données des parties qui ont déjà été mutées ainsi que les données des parties qui n'ont pas encore été mutées.
-
-Les Mutations sont totalement ordonnées par leur ordre de création et sont appliquées à chaque partie dans cet ordre. Les Mutations sont également partiellement ordonnées avec des insertions - les données insérées dans la table avant la soumission de la mutation seront mutées et les données insérées après ne seront pas mutées. Notez que les mutations ne bloquent en aucune façon les INSERTs.
-
-Une requête de mutation retourne immédiatement après l'ajout de l'entrée de mutation (dans le cas de tables répliquées à ZooKeeper, pour les tables non compliquées - au système de fichiers). La mutation elle-même s'exécute de manière asynchrone en utilisant les paramètres du profil système. Pour suivre l'avancement des mutations vous pouvez utiliser la [`system.mutations`](../../operations/system-tables.md#system_tables-mutations) table. Une mutation qui a été soumise avec succès continuera à s'exécuter même si les serveurs ClickHouse sont redémarrés. Il n'y a aucun moyen de faire reculer la mutation une fois qu'elle est soumise, mais si la mutation est bloquée pour une raison quelconque, elle peut être annulée avec le [`KILL MUTATION`](misc.md#kill-mutation) requête.
-
-Les entrées pour les mutations finies ne sont pas supprimées immédiatement (le nombre d'entrées conservées est déterminé par `finished_mutations_to_keep` le moteur de stockage de paramètre). Les anciennes entrées de mutation sont supprimées.
-
-## ALTER USER {#alter-user-statement}
-
-Changements clickhouse comptes d'utilisateurs.
-
-### Syntaxe {#alter-user-syntax}
-
-``` sql
-ALTER USER [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [IDENTIFIED [WITH {PLAINTEXT_PASSWORD|SHA256_PASSWORD|DOUBLE_SHA1_PASSWORD}] BY {'password'|'hash'}]
-    [[ADD|DROP] HOST {LOCAL | NAME 'name' | REGEXP 'name_regexp' | IP 'address' | LIKE 'pattern'} [,...] | ANY | NONE]
-    [DEFAULT ROLE role [,...] | ALL | ALL EXCEPT role [,...] ]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-### Description {#alter-user-dscr}
-
-Utiliser `ALTER USER` vous devez avoir le [ALTER USER](grant.md#grant-access-management) privilège.
-
-### Exemple {#alter-user-examples}
-
-Définir les rôles accordés par défaut:
-
-``` sql
-ALTER USER user DEFAULT ROLE role1, role2
-```
-
-Si les rôles ne sont pas précédemment accordés à un utilisateur, ClickHouse lève une exception.
-
-Définissez tous les rôles accordés à défaut:
-
-``` sql
-ALTER USER user DEFAULT ROLE ALL
-```
-
-Si un rôle seront accordés à un utilisateur dans l'avenir, il deviendra automatiquement par défaut.
-
-Définissez tous les rôles accordés sur default excepting `role1` et `role2`:
-
-``` sql
-ALTER USER user DEFAULT ROLE ALL EXCEPT role1, role2
-```
-
-## ALTER ROLE {#alter-role-statement}
-
-Les changements de rôles.
-
-### Syntaxe {#alter-role-syntax}
-
-``` sql
-ALTER ROLE [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-## ALTER ROW POLICY {#alter-row-policy-statement}
-
-Modifie la stratégie de ligne.
-
-### Syntaxe {#alter-row-policy-syntax}
-
-``` sql
-ALTER [ROW] POLICY [IF EXISTS] name [ON CLUSTER cluster_name] ON [database.]table
-    [RENAME TO new_name]
-    [AS {PERMISSIVE | RESTRICTIVE}]
-    [FOR SELECT]
-    [USING {condition | NONE}][,...]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-## ALTER QUOTA {#alter-quota-statement}
-
-Les changements de quotas.
-
-### Syntaxe {#alter-quota-syntax}
-
-``` sql
-ALTER QUOTA [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [KEYED BY {'none' | 'user name' | 'ip address' | 'client key' | 'client key or user name' | 'client key or ip address'}]
-    [FOR [RANDOMIZED] INTERVAL number {SECOND | MINUTE | HOUR | DAY | WEEK | MONTH | QUARTER | YEAR}
-        {MAX { {QUERIES | ERRORS | RESULT ROWS | RESULT BYTES | READ ROWS | READ BYTES | EXECUTION TIME} = number } [,...] |
-        NO LIMITS | TRACKING ONLY} [,...]]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-## ALTER SETTINGS PROFILE {#alter-settings-profile-statement}
-
-Les changements de quotas.
-
-### Syntaxe {#alter-settings-profile-syntax}
-
-``` sql
-ALTER SETTINGS PROFILE [IF EXISTS] name [ON CLUSTER cluster_name]
-    [RENAME TO new_name]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | INHERIT 'profile_name'] [,...]
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/alter/) <!--hide-->
diff --git a/docs/fr/sql-reference/statements/create.md b/docs/fr/sql-reference/statements/create.md
deleted file mode 100644
index e7c8040ee6ef..000000000000
--- a/docs/fr/sql-reference/statements/create.md
+++ /dev/null
@@ -1,502 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 35
-toc_title: CREATE
----
-
-# Créer des requêtes {#create-queries}
-
-## CREATE DATABASE {#query-language-create-database}
-
-Crée la base de données.
-
-``` sql
-CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] [ENGINE = engine(...)]
-```
-
-### Clause {#clauses}
-
--   `IF NOT EXISTS`
-    Si l' `db_name` la base de données existe déjà, alors ClickHouse ne crée pas de nouvelle base de données et:
-
-    -   Ne lance pas d'exception si la clause est spécifiée.
-    -   Lève une exception si la clause n'est pas spécifiée.
-
--   `ON CLUSTER`
-    Clickhouse crée le `db_name` base de données sur tous les serveurs d'un cluster spécifié.
-
--   `ENGINE`
-
-    -   [MySQL](../../engines/database-engines/mysql.md)
-        Vous permet de récupérer des données à partir du serveur MySQL distant.
-        Par défaut, ClickHouse utilise son propre [moteur de base de données](../../engines/database-engines/index.md).
-
-## CREATE TABLE {#create-table-query}
-
-Le `CREATE TABLE` la requête peut avoir plusieurs formes.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [compression_codec] [TTL expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [compression_codec] [TTL expr2],
-    ...
-) ENGINE = engine
-```
-
-Crée une table nommée ‘name’ dans le ‘db’ base de données ou la base de données actuelle si ‘db’ n'est pas définie, avec la structure spécifiée entre parenthèses et l' ‘engine’ moteur.
-La structure de la table est une liste de descriptions de colonnes. Si les index sont pris en charge par le moteur, ils sont indiqués comme paramètres pour le moteur de table.
-
-Une description de colonne est `name type` dans le cas le plus simple. Exemple: `RegionID UInt32`.
-Des Expressions peuvent également être définies pour les valeurs par défaut (voir ci-dessous).
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name AS [db2.]name2 [ENGINE = engine]
-```
-
-Crée une table avec la même structure qu'une autre table. Vous pouvez spécifier un moteur différent pour la table. Si le moteur n'est pas spécifié, le même moteur sera utilisé que pour la `db2.name2` table.
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name AS table_function()
-```
-
-Crée une table avec la structure et les données renvoyées par [fonction de table](../table-functions/index.md#table-functions).
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name ENGINE = engine AS SELECT ...
-```
-
-Crée une table avec une structure comme le résultat de l' `SELECT` une requête avec les ‘engine’ moteur, et le remplit avec des données de SELECT.
-
-Dans tous les cas, si `IF NOT EXISTS` est spécifié, la requête ne renvoie pas une erreur si la table existe déjà. Dans ce cas, la requête ne font rien.
-
-Il peut y avoir d'autres clauses après le `ENGINE` la clause dans la requête. Voir la documentation détaillée sur la façon de créer des tables dans les descriptions de [moteurs de table](../../engines/table-engines/index.md#table_engines).
-
-### Les Valeurs Par Défaut {#create-default-values}
-
-La description de colonne peut spécifier une expression pour une valeur par défaut, de l'une des manières suivantes:`DEFAULT expr`, `MATERIALIZED expr`, `ALIAS expr`.
-Exemple: `URLDomain String DEFAULT domain(URL)`.
-
-Si une expression pour la valeur par défaut n'est pas définie, les valeurs par défaut seront définies sur zéros pour les nombres, chaînes vides pour les chaînes, tableaux vides pour les tableaux et `1970-01-01` pour les dates ou zero unix timestamp pour les dates avec le temps. Les valeurs NULL ne sont pas prises en charge.
-
-Si l'expression par défaut est définie, le type de colonne est facultatif. S'il n'y a pas de type explicitement défini, le type d'expression par défaut est utilisé. Exemple: `EventDate DEFAULT toDate(EventTime)` – the ‘Date’ type sera utilisé pour la ‘EventDate’ colonne.
-
-Si le type de données et l'expression par défaut sont définis explicitement, cette expression sera convertie au type spécifié à l'aide des fonctions de conversion de type. Exemple: `Hits UInt32 DEFAULT 0` signifie la même chose que `Hits UInt32 DEFAULT toUInt32(0)`.
-
-Default expressions may be defined as an arbitrary expression from table constants and columns. When creating and changing the table structure, it checks that expressions don't contain loops. For INSERT, it checks that expressions are resolvable – that all columns they can be calculated from have been passed.
-
-`DEFAULT expr`
-
-Valeur par défaut normale. Si la requête INSERT ne spécifie pas la colonne correspondante, elle sera remplie en calculant l'expression correspondante.
-
-`MATERIALIZED expr`
-
-Expression matérialisée. Une telle colonne ne peut pas être spécifiée pour INSERT, car elle est toujours calculée.
-Pour un INSERT sans Liste de colonnes, ces colonnes ne sont pas prises en compte.
-De plus, cette colonne n'est pas substituée lors de l'utilisation d'un astérisque dans une requête SELECT. C'est pour préserver l'invariant que le dump obtenu en utilisant `SELECT *` peut être inséré dans la table en utilisant INSERT sans spécifier la liste des colonnes.
-
-`ALIAS expr`
-
-Synonyme. Une telle colonne n'est pas du tout stockée dans la table.
-Ses valeurs ne peuvent pas être insérées dans une table et elles ne sont pas substituées lors de l'utilisation d'un astérisque dans une requête SELECT.
-Il peut être utilisé dans SELECTs si l'alias est développé pendant l'analyse des requêtes.
-
-Lorsque vous utilisez la requête ALTER pour ajouter de nouvelles colonnes, les anciennes données de ces colonnes ne sont pas écrites. Au lieu de cela, lors de la lecture d'anciennes données qui n'ont pas de valeurs pour les nouvelles colonnes, les expressions sont calculées à la volée par défaut. Cependant, si l'exécution des expressions nécessite différentes colonnes qui ne sont pas indiquées dans la requête, ces colonnes seront en outre lues, mais uniquement pour les blocs de données qui en ont besoin.
-
-Si vous ajoutez une nouvelle colonne à une table mais modifiez ultérieurement son expression par défaut, les valeurs utilisées pour les anciennes données changeront (pour les données où les valeurs n'ont pas été stockées sur le disque). Notez que lors de l'exécution de fusions d'arrière-plan, les données des colonnes manquantes dans l'une des parties de fusion sont écrites dans la partie fusionnée.
-
-Il n'est pas possible de définir des valeurs par défaut pour les éléments dans les structures de données.
-
-### Contraintes {#constraints}
-
-Avec les descriptions de colonnes des contraintes peuvent être définies:
-
-``` sql
-CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [compression_codec] [TTL expr1],
-    ...
-    CONSTRAINT constraint_name_1 CHECK boolean_expr_1,
-    ...
-) ENGINE = engine
-```
-
-`boolean_expr_1` pourrait par n'importe quelle expression booléenne. Si les contraintes sont définies pour la table, chacun d'eux sera vérifiée pour chaque ligne `INSERT` query. If any constraint is not satisfied — server will raise an exception with constraint name and checking expression.
-
-L'ajout d'une grande quantité de contraintes peut affecter négativement les performances de big `INSERT` requête.
-
-### Expression TTL {#ttl-expression}
-
-Définit la durée de stockage des valeurs. Peut être spécifié uniquement pour les tables mergetree-family. Pour la description détaillée, voir [TTL pour les colonnes et les tableaux](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-ttl).
-
-### Codecs De Compression De Colonne {#codecs}
-
-Par défaut, ClickHouse applique le `lz4` méthode de compression. Pour `MergeTree`- famille de moteurs Vous pouvez modifier la méthode de compression par défaut dans le [compression](../../operations/server-configuration-parameters/settings.md#server-settings-compression) section d'une configuration de serveur. Vous pouvez également définir la méthode de compression pour chaque colonne `CREATE TABLE` requête.
-
-``` sql
-CREATE TABLE codec_example
-(
-    dt Date CODEC(ZSTD),
-    ts DateTime CODEC(LZ4HC),
-    float_value Float32 CODEC(NONE),
-    double_value Float64 CODEC(LZ4HC(9))
-    value Float32 CODEC(Delta, ZSTD)
-)
-ENGINE = <Engine>
-...
-```
-
-Si un codec est spécifié, le codec par défaut ne s'applique pas. Les Codecs peuvent être combinés dans un pipeline, par exemple, `CODEC(Delta, ZSTD)`. Pour sélectionner la meilleure combinaison de codecs pour votre projet, passez des benchmarks similaires à ceux décrits dans Altinity [Nouveaux encodages pour améliorer L'efficacité du ClickHouse](https://www.altinity.com/blog/2019/7/new-encodings-to-improve-clickhouse) article.
-
-!!! warning "Avertissement"
-    Vous ne pouvez pas décompresser les fichiers de base de données ClickHouse avec des utilitaires externes tels que `lz4`. Au lieu de cela, utilisez le spécial [clickhouse-compresseur](https://github.com/ClickHouse/ClickHouse/tree/master/programs/compressor) utilitaire.
-
-La Compression est prise en charge pour les moteurs de tableau suivants:
-
--   [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) famille. Prend en charge les codecs de compression de colonne et la sélection de la méthode de compression par défaut par [compression](../../operations/server-configuration-parameters/settings.md#server-settings-compression) paramètre.
--   [Journal](../../engines/table-engines/log-family/index.md) famille. Utilise le `lz4` méthode de compression par défaut et prend en charge les codecs de compression de colonne.
--   [Définir](../../engines/table-engines/special/set.md). Uniquement pris en charge la compression par défaut.
--   [Rejoindre](../../engines/table-engines/special/join.md). Uniquement pris en charge la compression par défaut.
-
-ClickHouse prend en charge les codecs à usage commun et les codecs spécialisés.
-
-#### Codecs Spécialisés {#create-query-specialized-codecs}
-
-Ces codecs sont conçus pour rendre la compression plus efficace en utilisant des fonctionnalités spécifiques des données. Certains de ces codecs ne compressent pas les données eux-mêmes. Au lieu de cela, ils préparent les données pour un codec à usage commun, qui les compresse mieux que sans cette préparation.
-
-Spécialisé codecs:
-
--   `Delta(delta_bytes)` — Compression approach in which raw values are replaced by the difference of two neighboring values, except for the first value that stays unchanged. Up to `delta_bytes` sont utilisés pour stocker des valeurs delta, donc `delta_bytes` est la taille maximale des valeurs brutes. Possible `delta_bytes` valeurs: 1, 2, 4, 8. La valeur par défaut pour `delta_bytes` être `sizeof(type)` si égale à 1, 2, 4 ou 8. Dans tous les autres cas, c'est 1.
--   `DoubleDelta` — Calculates delta of deltas and writes it in compact binary form. Optimal compression rates are achieved for monotonic sequences with a constant stride, such as time series data. Can be used with any fixed-width type. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. Uses 1 extra bit for 32-byte deltas: 5-bit prefixes instead of 4-bit prefixes. For additional information, see Compressing Time Stamps in [Gorilla: Une Base De Données De Séries Chronologiques Rapide, Évolutive Et En Mémoire](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf).
--   `Gorilla` — Calculates XOR between current and previous value and writes it in compact binary form. Efficient when storing a series of floating point values that change slowly, because the best compression rate is achieved when neighboring values are binary equal. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. For additional information, see Compressing Values in [Gorilla: Une Base De Données De Séries Chronologiques Rapide, Évolutive Et En Mémoire](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf).
--   `T64` — Compression approach that crops unused high bits of values in integer data types (including `Enum`, `Date` et `DateTime`). À chaque étape de son algorithme, le codec prend un bloc de 64 valeurs, les place dans une matrice de 64x64 bits, le transpose, recadre les bits de valeurs inutilisés et renvoie le reste sous forme de séquence. Les bits inutilisés sont les bits, qui ne diffèrent pas entre les valeurs maximum et minimum dans la partie de données entière pour laquelle la compression est utilisée.
-
-`DoubleDelta` et `Gorilla` les codecs sont utilisés dans Gorilla TSDB comme composants de son algorithme de compression. L'approche Gorilla est efficace dans les scénarios où il y a une séquence de valeurs qui changent lentement avec leurs horodatages. Les horodatages sont effectivement compressés par le `DoubleDelta` codec, et les valeurs sont effectivement comprimé par le `Gorilla` codec. Par exemple, pour obtenir une table stockée efficacement, vous pouvez la créer dans la configuration suivante:
-
-``` sql
-CREATE TABLE codec_example
-(
-    timestamp DateTime CODEC(DoubleDelta),
-    slow_values Float32 CODEC(Gorilla)
-)
-ENGINE = MergeTree()
-```
-
-#### Codecs À Usage Général {#create-query-general-purpose-codecs}
-
-Codec:
-
--   `NONE` — No compression.
--   `LZ4` — Lossless [algorithme de compression de données](https://github.com/lz4/lz4) utilisé par défaut. Applique la compression rapide LZ4.
--   `LZ4HC[(level)]` — LZ4 HC (high compression) algorithm with configurable level. Default level: 9. Setting `level <= 0` s'applique le niveau par défaut. Niveaux possibles: \[1, 12\]. Plage de niveau recommandée: \[4, 9\].
--   `ZSTD[(level)]` — [Algorithme de compression ZSTD](https://en.wikipedia.org/wiki/Zstandard) avec configurables `level`. Niveaux possibles: \[1, 22\]. Valeur par défaut: 1.
-
-Des niveaux de compression élevés sont utiles pour les scénarios asymétriques, comme compresser une fois, décompresser à plusieurs reprises. Des niveaux plus élevés signifient une meilleure compression et une utilisation plus élevée du processeur.
-
-## Les Tables Temporaires {#temporary-tables}
-
-Clickhouse prend en charge les tables temporaires qui ont les caractéristiques suivantes:
-
--   Les tables temporaires disparaissent à la fin de la session, y compris si la connexion est perdue.
--   Une table temporaire utilise uniquement le moteur de mémoire.
--   La base de données ne peut pas être spécifiée pour une table temporaire. Il est créé en dehors des bases de données.
--   Impossible de créer une table temporaire avec une requête DDL distribuée sur tous les serveurs de cluster (en utilisant `ON CLUSTER`): ce tableau n'existe que dans la session en cours.
--   Si une table temporaire a le même nom qu'une autre et qu'une requête spécifie le nom de la table sans spécifier la base de données, la table temporaire sera utilisée.
--   Pour le traitement des requêtes distribuées, les tables temporaires utilisées dans une requête sont transmises à des serveurs distants.
-
-Pour créer une table temporaire, utilisez la syntaxe suivante:
-
-``` sql
-CREATE TEMPORARY TABLE [IF NOT EXISTS] table_name
-(
-    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
-    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
-    ...
-)
-```
-
-Dans la plupart des cas, les tables temporaires ne sont pas créées manuellement, mais lors de l'utilisation de données externes pour une requête ou pour `(GLOBAL) IN`. Pour plus d'informations, consultez les sections appropriées
-
-Il est possible d'utiliser des tables avec [Moteur = mémoire](../../engines/table-engines/special/memory.md) au lieu de tables temporaires.
-
-## Requêtes DDL distribuées (sur la clause CLUSTER) {#distributed-ddl-queries-on-cluster-clause}
-
-Le `CREATE`, `DROP`, `ALTER`, et `RENAME` les requêtes prennent en charge l'exécution distribuée sur un cluster.
-Par exemple, la requête suivante crée la `all_hits` `Distributed` tableau sur chaque ordinateur hôte `cluster`:
-
-``` sql
-CREATE TABLE IF NOT EXISTS all_hits ON CLUSTER cluster (p Date, i Int32) ENGINE = Distributed(cluster, default, hits)
-```
-
-Pour exécuter ces requêtes correctement, chaque hôte doit avoir la même définition de cluster (pour simplifier la synchronisation des configs, vous pouvez utiliser des substitutions de ZooKeeper). Ils doivent également se connecter aux serveurs ZooKeeper.
-La version locale de la requête sera finalement implémentée sur chaque hôte du cluster, même si certains hôtes ne sont actuellement pas disponibles. L'ordre d'exécution des requêtes au sein d'un seul hôte est garanti.
-
-## CREATE VIEW {#create-view}
-
-``` sql
-CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]table_name [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...
-```
-
-Crée une vue. Il existe deux types de vues: normale et matérialisée.
-
-Les vues normales ne stockent aucune donnée, mais effectuent simplement une lecture à partir d'une autre table. En d'autres termes, une vue normale n'est rien de plus qu'une requête enregistrée. Lors de la lecture à partir d'une vue, cette requête enregistrée est utilisée comme sous-requête dans la clause FROM.
-
-Par exemple, supposons que vous avez créé une vue:
-
-``` sql
-CREATE VIEW view AS SELECT ...
-```
-
-et écrit une requête:
-
-``` sql
-SELECT a, b, c FROM view
-```
-
-Cette requête est entièrement équivalente à l'utilisation de la sous requête:
-
-``` sql
-SELECT a, b, c FROM (SELECT ...)
-```
-
-Les vues matérialisées stockent les données transformées par la requête SELECT correspondante.
-
-Lors de la création d'une vue matérialisée sans `TO [db].[table]`, you must specify ENGINE – the table engine for storing data.
-
-Lors de la création d'une vue matérialisée avec `TO [db].[table]` vous ne devez pas utiliser `POPULATE`.
-
-Une vue matérialisée est agencée comme suit: lors de l'insertion de données dans la table spécifiée dans SELECT, une partie des données insérées est convertie par cette requête SELECT, et le résultat est inséré dans la vue.
-
-Si vous spécifiez POPULATE, les données de table existantes sont insérées dans la vue lors de sa création, comme si `CREATE TABLE ... AS SELECT ...` . Sinon, la requête ne contient que les données insérées dans la table après la création de la vue. Nous ne recommandons pas D'utiliser POPULATE, car les données insérées dans la table lors de la création de la vue ne seront pas insérées dedans.
-
-A `SELECT` la requête peut contenir `DISTINCT`, `GROUP BY`, `ORDER BY`, `LIMIT`… Note that the corresponding conversions are performed independently on each block of inserted data. For example, if `GROUP BY` est définie, les données sont agrégées lors de l'insertion, mais uniquement dans un seul paquet de données insérées. Les données ne seront pas agrégées davantage. L'exception concerne l'utilisation d'un moteur qui effectue indépendamment l'agrégation de données, par exemple `SummingMergeTree`.
-
-L'exécution de `ALTER` les requêtes sur les vues matérialisées n'ont pas été complètement développées, elles pourraient donc être gênantes. Si la vue matérialisée utilise la construction `TO [db.]name` vous pouvez `DETACH` la vue, exécutez `ALTER` pour la table cible, puis `ATTACH` précédemment détaché (`DETACH`) vue.
-
-Les vues ressemblent aux tables normales. Par exemple, ils sont répertoriés dans le résultat de la `SHOW TABLES` requête.
-
-Il n'y a pas de requête séparée pour supprimer des vues. Pour supprimer une vue, utilisez `DROP TABLE`.
-
-## CREATE DICTIONARY {#create-dictionary-query}
-
-``` sql
-CREATE DICTIONARY [IF NOT EXISTS] [db.]dictionary_name [ON CLUSTER cluster]
-(
-    key1 type1  [DEFAULT|EXPRESSION expr1] [HIERARCHICAL|INJECTIVE|IS_OBJECT_ID],
-    key2 type2  [DEFAULT|EXPRESSION expr2] [HIERARCHICAL|INJECTIVE|IS_OBJECT_ID],
-    attr1 type2 [DEFAULT|EXPRESSION expr3],
-    attr2 type2 [DEFAULT|EXPRESSION expr4]
-)
-PRIMARY KEY key1, key2
-SOURCE(SOURCE_NAME([param1 value1 ... paramN valueN]))
-LAYOUT(LAYOUT_NAME([param_name param_value]))
-LIFETIME({MIN min_val MAX max_val | max_val})
-```
-
-Crée [externe dictionnaire](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md) avec le [structure](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure.md), [source](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md), [disposition](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md) et [vie](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime.md).
-
-Structure de dictionnaire externe se compose d'attributs. Les attributs du dictionnaire sont spécifiés de la même manière que les colonnes du tableau. La seule propriété d'attribut requise est son type, toutes les autres propriétés peuvent avoir des valeurs par défaut.
-
-Selon le dictionnaire [disposition](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout.md) un ou plusieurs attributs peuvent être spécifiés comme les clés de dictionnaire.
-
-Pour plus d'informations, voir [Dictionnaires Externes](../dictionaries/external-dictionaries/external-dicts.md) section.
-
-## CREATE USER {#create-user-statement}
-
-Crée un [compte d'utilisateur](../../operations/access-rights.md#user-account-management).
-
-### Syntaxe {#create-user-syntax}
-
-``` sql
-CREATE USER [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name]
-    [IDENTIFIED [WITH {NO_PASSWORD|PLAINTEXT_PASSWORD|SHA256_PASSWORD|SHA256_HASH|DOUBLE_SHA1_PASSWORD|DOUBLE_SHA1_HASH}] BY {'password'|'hash'}]
-    [HOST {LOCAL | NAME 'name' | REGEXP 'name_regexp' | IP 'address' | LIKE 'pattern'} [,...] | ANY | NONE]
-    [DEFAULT ROLE role [,...]]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-#### Identification {#identification}
-
-Il existe de multiples façons d'identification d'un utilisateur:
-
--   `IDENTIFIED WITH no_password`
--   `IDENTIFIED WITH plaintext_password BY 'qwerty'`
--   `IDENTIFIED WITH sha256_password BY 'qwerty'` ou `IDENTIFIED BY 'password'`
--   `IDENTIFIED WITH sha256_hash BY 'hash'`
--   `IDENTIFIED WITH double_sha1_password BY 'qwerty'`
--   `IDENTIFIED WITH double_sha1_hash BY 'hash'`
-
-#### L'Utilisateur De L'Hôte {#user-host}
-
-L'hôte utilisateur est un hôte à partir duquel une connexion au serveur ClickHouse peut être établie. Hôte peut être spécifié dans le `HOST` section de requête par les moyens suivants:
-
--   `HOST IP 'ip_address_or_subnetwork'` — User can connect to ClickHouse server only from the specified IP address or a [sous-réseau](https://en.wikipedia.org/wiki/Subnetwork). Exemple: `HOST IP '192.168.0.0/16'`, `HOST IP '2001:DB8::/32'`. Pour une utilisation en production, spécifiez uniquement `HOST IP` (adresses IP et leurs masques), depuis l'utilisation `host` et `host_regexp` peut causer une latence supplémentaire.
--   `HOST ANY` — User can connect from any location. This is default option.
--   `HOST LOCAL` — User can connect only locally.
--   `HOST NAME 'fqdn'` — User host can be specified as FQDN. For example, `HOST NAME 'mysite.com'`.
--   `HOST NAME REGEXP 'regexp'` — You can use [pcre](http://www.pcre.org/) expressions régulières lors de la spécification des hôtes utilisateur. Exemple, `HOST NAME REGEXP '.*\.mysite\.com'`.
--   `HOST LIKE 'template'` — Allows you use the [LIKE](../functions/string-search-functions.md#function-like) opérateur de filtre de l'utilisateur hôtes. Exemple, `HOST LIKE '%'` est équivalent à `HOST ANY`, `HOST LIKE '%.mysite.com'` filtre tous les hôtes dans le `mysite.com` domaine.
-
-Une autre façon de spécifier l'hôte est d'utiliser `@` syntaxe avec le nom d'utilisateur. Exemple:
-
--   `CREATE USER mira@'127.0.0.1'` — Equivalent to the `HOST IP` syntaxe.
--   `CREATE USER mira@'localhost'` — Equivalent to the `HOST LOCAL` syntaxe.
--   `CREATE USER mira@'192.168.%.%'` — Equivalent to the `HOST LIKE` syntaxe.
-
-!!! info "Avertissement"
-    Clickhouse traite `user_name@'address'` comme un nom d'utilisateur dans son ensemble. Donc, techniquement, vous pouvez créer plusieurs utilisateurs avec `user_name` et différentes constructions après `@`. Nous ne recommandons pas de le faire.
-
-### Exemple {#create-user-examples}
-
-Créer le compte d'utilisateur `mira` protégé par le mot de passe `qwerty`:
-
-``` sql
-CREATE USER mira HOST IP '127.0.0.1' IDENTIFIED WITH sha256_password BY 'qwerty'
-```
-
-`mira` devrait démarrer l'application client sur l'hôte où le serveur ClickHouse s'exécute.
-
-Créer le compte d'utilisateur `john`, attribuez-lui des rôles et définissez ces rôles par défaut:
-
-``` sql
-CREATE USER john DEFAULT ROLE role1, role2
-```
-
-Créer le compte d'utilisateur `john` et faire tous ses futurs rôles par défaut:
-
-``` sql
-ALTER USER user DEFAULT ROLE ALL
-```
-
-Quand un rôle sera attribué à `john` dans l'avenir, il deviendra automatiquement par défaut.
-
-Créer le compte d'utilisateur `john` et faire tous ses futurs rôles par défaut sauf `role1` et `role2`:
-
-``` sql
-ALTER USER john DEFAULT ROLE ALL EXCEPT role1, role2
-```
-
-## CREATE ROLE {#create-role-statement}
-
-Crée un [rôle](../../operations/access-rights.md#role-management).
-
-### Syntaxe {#create-role-syntax}
-
-``` sql
-CREATE ROLE [IF NOT EXISTS | OR REPLACE] name
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]
-```
-
-### Description {#create-role-description}
-
-Rôle est un ensemble de [privilège](grant.md#grant-privileges). Un utilisateur reçoit un rôle obtient tous les privilèges de ce rôle.
-
-Un utilisateur peut être affecté à plusieurs rôles. Les utilisateurs peuvent appliquer leurs rôles accordés dans des combinaisons arbitraires par le [SET ROLE](misc.md#set-role-statement) déclaration. La finale de la portée des privilèges est un ensemble combiné de tous les privilèges de tous les rôles. Si un utilisateur a des privilèges accordés directement à son compte d'utilisateur, ils sont également combinés avec les privilèges accordés par les rôles.
-
-L'utilisateur peut avoir des rôles par défaut qui s'appliquent à la connexion de l'utilisateur. Pour définir les rôles par défaut, utilisez [SET DEFAULT ROLE](misc.md#set-default-role-statement) - déclaration ou de la [ALTER USER](alter.md#alter-user-statement) déclaration.
-
-Pour révoquer un rôle, utilisez [REVOKE](revoke.md) déclaration.
-
-Pour supprimer le rôle, utilisez [DROP ROLE](misc.md#drop-role-statement) déclaration. Le rôle supprimé est automatiquement révoqué de tous les utilisateurs et rôles auxquels il a été accordé.
-
-### Exemple {#create-role-examples}
-
-``` sql
-CREATE ROLE accountant;
-GRANT SELECT ON db.* TO accountant;
-```
-
-Cette séquence de requêtes crée le rôle `accountant` cela a le privilège de lire les données du `accounting` la base de données.
-
-Octroi du rôle à l'utilisateur `mira`:
-
-``` sql
-GRANT accountant TO mira;
-```
-
-Une fois le rôle accordé, l'utilisateur peut l'utiliser et effectuer les requêtes autorisées. Exemple:
-
-``` sql
-SET ROLE accountant;
-SELECT * FROM db.*;
-```
-
-## CREATE ROW POLICY {#create-row-policy-statement}
-
-Crée un [filtre pour les lignes](../../operations/access-rights.md#row-policy-management) qu'un utilisateur peut lire à partir d'une table.
-
-### Syntaxe {#create-row-policy-syntax}
-
-``` sql
-CREATE [ROW] POLICY [IF NOT EXISTS | OR REPLACE] policy_name [ON CLUSTER cluster_name] ON [db.]table
-    [AS {PERMISSIVE | RESTRICTIVE}]
-    [FOR SELECT]
-    [USING condition]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-#### Section AS {#create-row-policy-as}
-
-À l'aide de cette section, vous pouvez créer des stratégies permissives ou restrictives.
-
-La stratégie Permissive accorde l'accès aux lignes. Les stratégies permissives qui s'appliquent à la même table sont combinées ensemble en utilisant le booléen `OR` opérateur. Les stratégies sont permissives par défaut.
-
-La politique Restrictive limite l'accès à la ligne. Les politiques restrictives qui s'appliquent à la même table sont combinées en utilisant le booléen `AND` opérateur.
-
-Les stratégies restrictives s'appliquent aux lignes qui ont passé les filtres permissifs. Si vous définissez des stratégies restrictives mais aucune politique permissive, l'utilisateur ne peut obtenir aucune ligne de la table.
-
-#### La Section DE {#create-row-policy-to}
-
-Dans la section `TO` vous pouvez donner une liste mixte de rôles et d'utilisateurs, par exemple, `CREATE ROW POLICY ... TO accountant, john@localhost`.
-
-Mot `ALL` signifie Tous les utilisateurs de ClickHouse, y compris l'utilisateur actuel. Mot `ALL EXCEPT` autoriser à exclure certains utilisateurs de la liste tous les utilisateurs, par exemple `CREATE ROW POLICY ... TO ALL EXCEPT accountant, john@localhost`
-
-### Exemple {#examples}
-
--   `CREATE ROW POLICY filter ON mydb.mytable FOR SELECT USING a<1000 TO accountant, john@localhost`
--   `CREATE ROW POLICY filter ON mydb.mytable FOR SELECT USING a<1000 TO ALL EXCEPT mira`
-
-## CREATE QUOTA {#create-quota-statement}
-
-Crée un [quota](../../operations/access-rights.md#quotas-management) qui peut être attribué à un utilisateur ou un rôle.
-
-### Syntaxe {#create-quota-syntax}
-
-``` sql
-CREATE QUOTA [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name]
-    [KEYED BY {'none' | 'user name' | 'ip address' | 'client key' | 'client key or user name' | 'client key or ip address'}]
-    [FOR [RANDOMIZED] INTERVAL number {SECOND | MINUTE | HOUR | DAY | WEEK | MONTH | QUARTER | YEAR}
-        {MAX { {QUERIES | ERRORS | RESULT ROWS | RESULT BYTES | READ ROWS | READ BYTES | EXECUTION TIME} = number } [,...] |
-         NO LIMITS | TRACKING ONLY} [,...]]
-    [TO {role [,...] | ALL | ALL EXCEPT role [,...]}]
-```
-
-### Exemple {#create-quota-example}
-
-Limiter le nombre maximum de requêtes pour l'utilisateur actuel avec 123 requêtes en 15 mois contrainte:
-
-``` sql
-CREATE QUOTA qA FOR INTERVAL 15 MONTH MAX QUERIES 123 TO CURRENT_USER
-```
-
-## CREATE SETTINGS PROFILE {#create-settings-profile-statement}
-
-Crée un [les paramètres de profil](../../operations/access-rights.md#settings-profiles-management) qui peut être attribué à un utilisateur ou un rôle.
-
-### Syntaxe {#create-settings-profile-syntax}
-
-``` sql
-CREATE SETTINGS PROFILE [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name]
-    [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | INHERIT 'profile_name'] [,...]
-```
-
-# Exemple {#create-settings-profile-syntax}
-
-Créer l' `max_memory_usage_profile` paramètres du profil avec valeur et contraintes pour `max_memory_usage` paramètre. L'affecter à `robin`:
-
-``` sql
-CREATE SETTINGS PROFILE max_memory_usage_profile SETTINGS max_memory_usage = 100000001 MIN 90000000 MAX 110000000 TO robin
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/create/) <!--hide-->
diff --git a/docs/fr/sql-reference/statements/grant.md b/docs/fr/sql-reference/statements/grant.md
deleted file mode 100644
index 143c9a36e330..000000000000
--- a/docs/fr/sql-reference/statements/grant.md
+++ /dev/null
@@ -1,476 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: GRANT
----
-
-# GRANT {#grant}
-
--   Accorder [privilège](#grant-privileges) pour ClickHouse comptes d'utilisateurs ou des rôles.
--   Affecte des rôles à des comptes d'utilisateurs ou à d'autres rôles.
-
-Pour révoquer les privilèges, utilisez [REVOKE](revoke.md) déclaration. Vous pouvez également classer les privilèges accordés par le [SHOW GRANTS](show.md#show-grants-statement) déclaration.
-
-## Accorder La Syntaxe Des Privilèges {#grant-privigele-syntax}
-
-``` sql
-GRANT [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} TO {user | role | CURRENT_USER} [,...] [WITH GRANT OPTION]
-```
-
--   `privilege` — Type of privilege.
--   `role` — ClickHouse user role.
--   `user` — ClickHouse user account.
-
-Le `WITH GRANT OPTION` clause de subventions `user` ou `role` avec l'autorisation de réaliser des `GRANT` requête. Les utilisateurs peuvent accorder des privilèges de la même portée qu'ils ont et moins.
-
-## Attribution De La Syntaxe Du Rôle {#assign-role-syntax}
-
-``` sql
-GRANT [ON CLUSTER cluster_name] role [,...] TO {user | another_role | CURRENT_USER} [,...] [WITH ADMIN OPTION]
-```
-
--   `role` — ClickHouse user role.
--   `user` — ClickHouse user account.
-
-Le `WITH ADMIN OPTION` clause de jeux [ADMIN OPTION](#admin-option-privilege) privilège pour `user` ou `role`.
-
-## Utilisation {#grant-usage}
-
-Utiliser `GRANT` votre compte doit avoir le `GRANT OPTION` privilège. Vous ne pouvez accorder des privilèges que dans le cadre de vos privilèges de Compte.
-
-Par exemple, l'administrateur a accordé des privilèges `john` compte par la requête:
-
-``` sql
-GRANT SELECT(x,y) ON db.table TO john WITH GRANT OPTION
-```
-
-Cela signifie que `john` a la permission d'effectuer:
-
--   `SELECT x,y FROM db.table`.
--   `SELECT x FROM db.table`.
--   `SELECT y FROM db.table`.
-
-`john` ne pouvez pas effectuer de `SELECT z FROM db.table`. Le `SELECT * FROM db.table` aussi n'est pas disponible. En traitant cette requête, ClickHouse ne renvoie aucune donnée, même `x` et `y`. La seule exception est si une table contient uniquement `x` et `y` colonnes, dans ce cas ClickHouse renvoie toutes les données.
-
-Également `john` a l' `GRANT OPTION` privilège, de sorte qu'il peut accorder à d'autres utilisateurs avec des privilèges de la même ou de la plus petite portée.
-
-Spécification des privilèges vous pouvez utiliser asterisk (`*`) au lieu d'une table ou d'un nom de base de données. Par exemple, l' `GRANT SELECT ON db.* TO john` requête permet `john` pour effectuer la `SELECT` requête sur toutes les tables dans `db` la base de données. En outre, vous pouvez omettre le nom de la base de données. Dans ce cas, des privilèges sont accordés pour la base de données actuelle, par exemple: `GRANT SELECT ON * TO john` accorde le privilège sur toutes les tables dans la base de données actuelle, `GRANT SELECT ON mytable TO john` accorde le privilège sur le `mytable` table dans la base de données actuelle.
-
-L'accès à la `system` la base de données est toujours autorisée (puisque cette base de données est utilisée pour traiter les requêtes).
-
-Vous pouvez accorder plusieurs privilèges à plusieurs comptes dans une requête. Requête `GRANT SELECT, INSERT ON *.* TO john, robin` permet de comptes `john` et `robin` pour effectuer la `INSERT` et `SELECT` des requêtes sur toutes les tables de toutes les bases de données sur le serveur.
-
-## Privilège {#grant-privileges}
-
-Privilège est une autorisation pour effectuer un type spécifique de requêtes.
-
-Les privilèges ont une structure hiérarchique. Un ensemble de requêtes autorisées dépend de la portée des privilèges.
-
-Hiérarchie des privilèges:
-
--   [SELECT](#grant-select)
--   [INSERT](#grant-insert)
--   [ALTER](#grant-alter)
-    -   `ALTER TABLE`
-        -   `ALTER UPDATE`
-        -   `ALTER DELETE`
-        -   `ALTER COLUMN`
-            -   `ALTER ADD COLUMN`
-            -   `ALTER DROP COLUMN`
-            -   `ALTER MODIFY COLUMN`
-            -   `ALTER COMMENT COLUMN`
-            -   `ALTER CLEAR COLUMN`
-            -   `ALTER RENAME COLUMN`
-        -   `ALTER INDEX`
-            -   `ALTER ORDER BY`
-            -   `ALTER ADD INDEX`
-            -   `ALTER DROP INDEX`
-            -   `ALTER MATERIALIZE INDEX`
-            -   `ALTER CLEAR INDEX`
-        -   `ALTER CONSTRAINT`
-            -   `ALTER ADD CONSTRAINT`
-            -   `ALTER DROP CONSTRAINT`
-        -   `ALTER TTL`
-        -   `ALTER MATERIALIZE TTL`
-        -   `ALTER SETTINGS`
-        -   `ALTER MOVE PARTITION`
-        -   `ALTER FETCH PARTITION`
-        -   `ALTER FREEZE PARTITION`
-    -   `ALTER VIEW`
-        -   `ALTER VIEW REFRESH`
-        -   `ALTER VIEW MODIFY QUERY`
--   [CREATE](#grant-create)
-    -   `CREATE DATABASE`
-    -   `CREATE TABLE`
-    -   `CREATE VIEW`
-    -   `CREATE DICTIONARY`
-    -   `CREATE TEMPORARY TABLE`
--   [DROP](#grant-drop)
-    -   `DROP DATABASE`
-    -   `DROP TABLE`
-    -   `DROP VIEW`
-    -   `DROP DICTIONARY`
--   [TRUNCATE](#grant-truncate)
--   [OPTIMIZE](#grant-optimize)
--   [SHOW](#grant-show)
-    -   `SHOW DATABASES`
-    -   `SHOW TABLES`
-    -   `SHOW COLUMNS`
-    -   `SHOW DICTIONARIES`
--   [KILL QUERY](#grant-kill-query)
--   [ACCESS MANAGEMENT](#grant-access-management)
-    -   `CREATE USER`
-    -   `ALTER USER`
-    -   `DROP USER`
-    -   `CREATE ROLE`
-    -   `ALTER ROLE`
-    -   `DROP ROLE`
-    -   `CREATE ROW POLICY`
-    -   `ALTER ROW POLICY`
-    -   `DROP ROW POLICY`
-    -   `CREATE QUOTA`
-    -   `ALTER QUOTA`
-    -   `DROP QUOTA`
-    -   `CREATE SETTINGS PROFILE`
-    -   `ALTER SETTINGS PROFILE`
-    -   `DROP SETTINGS PROFILE`
-    -   `SHOW ACCESS`
-        -   `SHOW_USERS`
-        -   `SHOW_ROLES`
-        -   `SHOW_ROW_POLICIES`
-        -   `SHOW_QUOTAS`
-        -   `SHOW_SETTINGS_PROFILES`
-    -   `ROLE ADMIN`
--   [SYSTEM](#grant-system)
-    -   `SYSTEM SHUTDOWN`
-    -   `SYSTEM DROP CACHE`
-        -   `SYSTEM DROP DNS CACHE`
-        -   `SYSTEM DROP MARK CACHE`
-        -   `SYSTEM DROP UNCOMPRESSED CACHE`
-    -   `SYSTEM RELOAD`
-        -   `SYSTEM RELOAD CONFIG`
-        -   `SYSTEM RELOAD DICTIONARY`
-        -   `SYSTEM RELOAD EMBEDDED DICTIONARIES`
-    -   `SYSTEM MERGES`
-    -   `SYSTEM TTL MERGES`
-    -   `SYSTEM FETCHES`
-    -   `SYSTEM MOVES`
-    -   `SYSTEM SENDS`
-        -   `SYSTEM DISTRIBUTED SENDS`
-        -   `SYSTEM REPLICATED SENDS`
-    -   `SYSTEM REPLICATION QUEUES`
-    -   `SYSTEM SYNC REPLICA`
-    -   `SYSTEM RESTART REPLICA`
-    -   `SYSTEM FLUSH`
-        -   `SYSTEM FLUSH DISTRIBUTED`
-        -   `SYSTEM FLUSH LOGS`
--   [INTROSPECTION](#grant-introspection)
-    -   `addressToLine`
-    -   `addressToSymbol`
-    -   `demangle`
--   [SOURCES](#grant-sources)
-    -   `FILE`
-    -   `URL`
-    -   `REMOTE`
-    -   `YSQL`
-    -   `ODBC`
-    -   `JDBC`
-    -   `HDFS`
-    -   `S3`
--   [dictGet](#grant-dictget)
-
-Exemples de la façon dont cette hiérarchie est traitée:
-
--   Le `ALTER` privilège comprend tous les autres `ALTER*` privilège.
--   `ALTER CONSTRAINT` comprendre `ALTER ADD CONSTRAINT` et `ALTER DROP CONSTRAINT` privilège.
-
-Les privilèges sont appliqués à différents niveaux. Connaissant un niveau suggère la syntaxe disponible pour le privilège.
-
-Les niveaux (du plus faible au plus élevé):
-
--   `COLUMN` — Privilege can be granted for column, table, database, or globally.
--   `TABLE` — Privilege can be granted for table, database, or globally.
--   `VIEW` — Privilege can be granted for view, database, or globally.
--   `DICTIONARY` — Privilege can be granted for dictionary, database, or globally.
--   `DATABASE` — Privilege can be granted for database or globally.
--   `GLOBAL` — Privilege can be granted only globally.
--   `GROUP` — Groups privileges of different levels. When `GROUP`- le privilège de niveau est accordé, seuls les privilèges du groupe sont accordés qui correspondent à la syntaxe utilisée.
-
-Exemples de syntaxe:
-
--   `GRANT SELECT(x) ON db.table TO user`
--   `GRANT SELECT ON db.* TO user`
-
-Exemples de syntaxe refusée:
-
--   `GRANT CREATE USER(x) ON db.table TO user`
--   `GRANT CREATE USER ON db.* TO user`
-
-Le privilège spécial [ALL](#grant-all) accorde tous les privilèges à un compte d'utilisateur ou à un rôle.
-
-Par défaut, un compte d'utilisateur ou un rôle a pas de privilèges.
-
-Si un utilisateur ou un rôle ont pas de privilèges qu'il s'affiche comme [NONE](#grant-none) privilège.
-
-Certaines requêtes par leur implémentation nécessitent un ensemble de privilèges. Par exemple, pour effectuer la [RENAME](misc.md#misc_operations-rename) requête vous avez besoin des privilèges suivants: `SELECT`, `CREATE TABLE`, `INSERT` et `DROP TABLE`.
-
-### SELECT {#grant-select}
-
-Permet d'effectuer des [SELECT](select/index.md) requête.
-
-Le niveau de privilège: `COLUMN`.
-
-**Description**
-
-L'utilisateur accordé avec ce privilège peut effectuer `SELECT` requêtes sur une liste spécifiée de colonnes dans la table et la base de données spécifiées. Si l'utilisateur inclut d'autres colonnes, une requête ne renvoie aucune donnée.
-
-Considérez le privilège suivant:
-
-``` sql
-GRANT SELECT(x,y) ON db.table TO john
-```
-
-Ce privilège permet à `john` pour effectuer toute `SELECT` requête qui implique des données du `x` et/ou `y` les colonnes en `db.table`. Exemple, `SELECT x FROM db.table`. `john` ne pouvez pas effectuer de `SELECT z FROM db.table`. Le `SELECT * FROM db.table` aussi n'est pas disponible. En traitant cette requête, ClickHouse ne renvoie aucune donnée, même `x` et `y`. La seule exception est si une table contient uniquement `x` et `y` colonnes, dans ce cas ClickHouse renvoie toutes les données.
-
-### INSERT {#grant-insert}
-
-Permet d'effectuer des [INSERT](insert-into.md) requête.
-
-Le niveau de privilège: `COLUMN`.
-
-**Description**
-
-L'utilisateur accordé avec ce privilège peut effectuer `INSERT` requêtes sur une liste spécifiée de colonnes dans la table et la base de données spécifiées. Si l'utilisateur inclut d'autres colonnes, une requête n'insère aucune donnée.
-
-**Exemple**
-
-``` sql
-GRANT INSERT(x,y) ON db.table TO john
-```
-
-Le privilège accordé permet `john` pour insérer des données à l' `x` et/ou `y` les colonnes en `db.table`.
-
-### ALTER {#grant-alter}
-
-Permet d'effectuer des [ALTER](alter.md) requêtes correspondant à la hiérarchie de privilèges suivante:
-
--   `ALTER`. Niveau: `COLUMN`.
-    -   `ALTER TABLE`. Niveau: `GROUP`
-        -   `ALTER UPDATE`. Niveau: `COLUMN`. Alias: `UPDATE`
-        -   `ALTER DELETE`. Niveau: `COLUMN`. Alias: `DELETE`
-        -   `ALTER COLUMN`. Niveau: `GROUP`
-            -   `ALTER ADD COLUMN`. Niveau: `COLUMN`. Alias: `ADD COLUMN`
-            -   `ALTER DROP COLUMN`. Niveau: `COLUMN`. Alias: `DROP COLUMN`
-            -   `ALTER MODIFY COLUMN`. Niveau: `COLUMN`. Alias: `MODIFY COLUMN`
-            -   `ALTER COMMENT COLUMN`. Niveau: `COLUMN`. Alias: `COMMENT COLUMN`
-            -   `ALTER CLEAR COLUMN`. Niveau: `COLUMN`. Alias: `CLEAR COLUMN`
-            -   `ALTER RENAME COLUMN`. Niveau: `COLUMN`. Alias: `RENAME COLUMN`
-        -   `ALTER INDEX`. Niveau: `GROUP`. Alias: `INDEX`
-            -   `ALTER ORDER BY`. Niveau: `TABLE`. Alias: `ALTER MODIFY ORDER BY`, `MODIFY ORDER BY`
-            -   `ALTER ADD INDEX`. Niveau: `TABLE`. Alias: `ADD INDEX`
-            -   `ALTER DROP INDEX`. Niveau: `TABLE`. Alias: `DROP INDEX`
-            -   `ALTER MATERIALIZE INDEX`. Niveau: `TABLE`. Alias: `MATERIALIZE INDEX`
-            -   `ALTER CLEAR INDEX`. Niveau: `TABLE`. Alias: `CLEAR INDEX`
-        -   `ALTER CONSTRAINT`. Niveau: `GROUP`. Alias: `CONSTRAINT`
-            -   `ALTER ADD CONSTRAINT`. Niveau: `TABLE`. Alias: `ADD CONSTRAINT`
-            -   `ALTER DROP CONSTRAINT`. Niveau: `TABLE`. Alias: `DROP CONSTRAINT`
-        -   `ALTER TTL`. Niveau: `TABLE`. Alias: `ALTER MODIFY TTL`, `MODIFY TTL`
-        -   `ALTER MATERIALIZE TTL`. Niveau: `TABLE`. Alias: `MATERIALIZE TTL`
-        -   `ALTER SETTINGS`. Niveau: `TABLE`. Alias: `ALTER SETTING`, `ALTER MODIFY SETTING`, `MODIFY SETTING`
-        -   `ALTER MOVE PARTITION`. Niveau: `TABLE`. Alias: `ALTER MOVE PART`, `MOVE PARTITION`, `MOVE PART`
-        -   `ALTER FETCH PARTITION`. Niveau: `TABLE`. Alias: `FETCH PARTITION`
-        -   `ALTER FREEZE PARTITION`. Niveau: `TABLE`. Alias: `FREEZE PARTITION`
-    -   `ALTER VIEW` Niveau: `GROUP`
-        -   `ALTER VIEW REFRESH`. Niveau: `VIEW`. Alias: `ALTER LIVE VIEW REFRESH`, `REFRESH VIEW`
-        -   `ALTER VIEW MODIFY QUERY`. Niveau: `VIEW`. Alias: `ALTER TABLE MODIFY QUERY`
-
-Exemples de la façon dont cette hiérarchie est traitée:
-
--   Le `ALTER` privilège comprend tous les autres `ALTER*` privilège.
--   `ALTER CONSTRAINT` comprendre `ALTER ADD CONSTRAINT` et `ALTER DROP CONSTRAINT` privilège.
-
-**Note**
-
--   Le `MODIFY SETTING` privilège permet de modifier les paramètres du moteur de table. In n'affecte pas les paramètres ou les paramètres de configuration du serveur.
--   Le `ATTACH` opération a besoin de la [CREATE](#grant-create) privilège.
--   Le `DETACH` opération a besoin de la [DROP](#grant-drop) privilège.
--   Pour arrêter la mutation par le [KILL MUTATION](misc.md#kill-mutation) requête, vous devez avoir un privilège pour commencer cette mutation. Par exemple, si vous voulez arrêter l' `ALTER UPDATE` requête, vous avez besoin du `ALTER UPDATE`, `ALTER TABLE`, ou `ALTER` privilège.
-
-### CREATE {#grant-create}
-
-Permet d'effectuer des [CREATE](create.md) et [ATTACH](misc.md#attach) DDL-requêtes correspondant à la hiérarchie de privilèges suivante:
-
--   `CREATE`. Niveau: `GROUP`
-    -   `CREATE DATABASE`. Niveau: `DATABASE`
-    -   `CREATE TABLE`. Niveau: `TABLE`
-    -   `CREATE VIEW`. Niveau: `VIEW`
-    -   `CREATE DICTIONARY`. Niveau: `DICTIONARY`
-    -   `CREATE TEMPORARY TABLE`. Niveau: `GLOBAL`
-
-**Note**
-
--   Pour supprimer la table créée, l'utilisateur doit [DROP](#grant-drop).
-
-### DROP {#grant-drop}
-
-Permet d'effectuer des [DROP](misc.md#drop) et [DETACH](misc.md#detach) requêtes correspondant à la hiérarchie de privilèges suivante:
-
--   `DROP`. Niveau:
-    -   `DROP DATABASE`. Niveau: `DATABASE`
-    -   `DROP TABLE`. Niveau: `TABLE`
-    -   `DROP VIEW`. Niveau: `VIEW`
-    -   `DROP DICTIONARY`. Niveau: `DICTIONARY`
-
-### TRUNCATE {#grant-truncate}
-
-Permet d'effectuer des [TRUNCATE](misc.md#truncate-statement) requête.
-
-Le niveau de privilège: `TABLE`.
-
-### OPTIMIZE {#grant-optimize}
-
-Permet d'effectuer les [OPTIMIZE TABLE](misc.md#misc_operations-optimize) requête.
-
-Le niveau de privilège: `TABLE`.
-
-### SHOW {#grant-show}
-
-Permet d'effectuer des `SHOW`, `DESCRIBE`, `USE`, et `EXISTS` requêtes, correspondant à la hiérarchie suivante des privilèges:
-
--   `SHOW`. Niveau: `GROUP`
-    -   `SHOW DATABASES`. Niveau: `DATABASE`. Permet d'exécuter des `SHOW DATABASES`, `SHOW CREATE DATABASE`, `USE <database>` requête.
-    -   `SHOW TABLES`. Niveau: `TABLE`. Permet d'exécuter des `SHOW TABLES`, `EXISTS <table>`, `CHECK <table>` requête.
-    -   `SHOW COLUMNS`. Niveau: `COLUMN`. Permet d'exécuter des `SHOW CREATE TABLE`, `DESCRIBE` requête.
-    -   `SHOW DICTIONARIES`. Niveau: `DICTIONARY`. Permet d'exécuter des `SHOW DICTIONARIES`, `SHOW CREATE DICTIONARY`, `EXISTS <dictionary>` requête.
-
-**Note**
-
-Un utilisateur a le `SHOW` privilège s'il a un autre privilège concernant la table, le dictionnaire ou la base de données spécifiés.
-
-### KILL QUERY {#grant-kill-query}
-
-Permet d'effectuer les [KILL](misc.md#kill-query-statement) requêtes correspondant à la hiérarchie de privilèges suivante:
-
-Le niveau de privilège: `GLOBAL`.
-
-**Note**
-
-`KILL QUERY` privilège permet à un utilisateur de tuer les requêtes des autres utilisateurs.
-
-### ACCESS MANAGEMENT {#grant-access-management}
-
-Permet à un utilisateur d'effectuer des requêtes qui gèrent les utilisateurs, les rôles et les stratégies de ligne.
-
--   `ACCESS MANAGEMENT`. Niveau: `GROUP`
-    -   `CREATE USER`. Niveau: `GLOBAL`
-    -   `ALTER USER`. Niveau: `GLOBAL`
-    -   `DROP USER`. Niveau: `GLOBAL`
-    -   `CREATE ROLE`. Niveau: `GLOBAL`
-    -   `ALTER ROLE`. Niveau: `GLOBAL`
-    -   `DROP ROLE`. Niveau: `GLOBAL`
-    -   `ROLE ADMIN`. Niveau: `GLOBAL`
-    -   `CREATE ROW POLICY`. Niveau: `GLOBAL`. Alias: `CREATE POLICY`
-    -   `ALTER ROW POLICY`. Niveau: `GLOBAL`. Alias: `ALTER POLICY`
-    -   `DROP ROW POLICY`. Niveau: `GLOBAL`. Alias: `DROP POLICY`
-    -   `CREATE QUOTA`. Niveau: `GLOBAL`
-    -   `ALTER QUOTA`. Niveau: `GLOBAL`
-    -   `DROP QUOTA`. Niveau: `GLOBAL`
-    -   `CREATE SETTINGS PROFILE`. Niveau: `GLOBAL`. Alias: `CREATE PROFILE`
-    -   `ALTER SETTINGS PROFILE`. Niveau: `GLOBAL`. Alias: `ALTER PROFILE`
-    -   `DROP SETTINGS PROFILE`. Niveau: `GLOBAL`. Alias: `DROP PROFILE`
-    -   `SHOW ACCESS`. Niveau: `GROUP`
-        -   `SHOW_USERS`. Niveau: `GLOBAL`. Alias: `SHOW CREATE USER`
-        -   `SHOW_ROLES`. Niveau: `GLOBAL`. Alias: `SHOW CREATE ROLE`
-        -   `SHOW_ROW_POLICIES`. Niveau: `GLOBAL`. Alias: `SHOW POLICIES`, `SHOW CREATE ROW POLICY`, `SHOW CREATE POLICY`
-        -   `SHOW_QUOTAS`. Niveau: `GLOBAL`. Alias: `SHOW CREATE QUOTA`
-        -   `SHOW_SETTINGS_PROFILES`. Niveau: `GLOBAL`. Alias: `SHOW PROFILES`, `SHOW CREATE SETTINGS PROFILE`, `SHOW CREATE PROFILE`
-
-Le `ROLE ADMIN` le privilège permet à un utilisateur d'accorder et de révoquer tous les rôles, y compris ceux qui ne lui sont pas accordés avec l'option admin.
-
-### SYSTEM {#grant-system}
-
-Permet à un utilisateur d'effectuer la [SYSTEM](system.md) requêtes correspondant à la hiérarchie de privilèges suivante.
-
--   `SYSTEM`. Niveau: `GROUP`
-    -   `SYSTEM SHUTDOWN`. Niveau: `GLOBAL`. Alias: `SYSTEM KILL`, `SHUTDOWN`
-    -   `SYSTEM DROP CACHE`. Alias: `DROP CACHE`
-        -   `SYSTEM DROP DNS CACHE`. Niveau: `GLOBAL`. Alias: `SYSTEM DROP DNS`, `DROP DNS CACHE`, `DROP DNS`
-        -   `SYSTEM DROP MARK CACHE`. Niveau: `GLOBAL`. Alias: `SYSTEM DROP MARK`, `DROP MARK CACHE`, `DROP MARKS`
-        -   `SYSTEM DROP UNCOMPRESSED CACHE`. Niveau: `GLOBAL`. Alias: `SYSTEM DROP UNCOMPRESSED`, `DROP UNCOMPRESSED CACHE`, `DROP UNCOMPRESSED`
-    -   `SYSTEM RELOAD`. Niveau: `GROUP`
-        -   `SYSTEM RELOAD CONFIG`. Niveau: `GLOBAL`. Alias: `RELOAD CONFIG`
-        -   `SYSTEM RELOAD DICTIONARY`. Niveau: `GLOBAL`. Alias: `SYSTEM RELOAD DICTIONARIES`, `RELOAD DICTIONARY`, `RELOAD DICTIONARIES`
-        -   `SYSTEM RELOAD EMBEDDED DICTIONARIES`. Niveau: `GLOBAL`. Alias: R`ELOAD EMBEDDED DICTIONARIES`
-    -   `SYSTEM MERGES`. Niveau: `TABLE`. Alias: `SYSTEM STOP MERGES`, `SYSTEM START MERGES`, `STOP MERGES`, `START MERGES`
-    -   `SYSTEM TTL MERGES`. Niveau: `TABLE`. Alias: `SYSTEM STOP TTL MERGES`, `SYSTEM START TTL MERGES`, `STOP TTL MERGES`, `START TTL MERGES`
-    -   `SYSTEM FETCHES`. Niveau: `TABLE`. Alias: `SYSTEM STOP FETCHES`, `SYSTEM START FETCHES`, `STOP FETCHES`, `START FETCHES`
-    -   `SYSTEM MOVES`. Niveau: `TABLE`. Alias: `SYSTEM STOP MOVES`, `SYSTEM START MOVES`, `STOP MOVES`, `START MOVES`
-    -   `SYSTEM SENDS`. Niveau: `GROUP`. Alias: `SYSTEM STOP SENDS`, `SYSTEM START SENDS`, `STOP SENDS`, `START SENDS`
-        -   `SYSTEM DISTRIBUTED SENDS`. Niveau: `TABLE`. Alias: `SYSTEM STOP DISTRIBUTED SENDS`, `SYSTEM START DISTRIBUTED SENDS`, `STOP DISTRIBUTED SENDS`, `START DISTRIBUTED SENDS`
-        -   `SYSTEM REPLICATED SENDS`. Niveau: `TABLE`. Alias: `SYSTEM STOP REPLICATED SENDS`, `SYSTEM START REPLICATED SENDS`, `STOP REPLICATED SENDS`, `START REPLICATED SENDS`
-    -   `SYSTEM REPLICATION QUEUES`. Niveau: `TABLE`. Alias: `SYSTEM STOP REPLICATION QUEUES`, `SYSTEM START REPLICATION QUEUES`, `STOP REPLICATION QUEUES`, `START REPLICATION QUEUES`
-    -   `SYSTEM SYNC REPLICA`. Niveau: `TABLE`. Alias: `SYNC REPLICA`
-    -   `SYSTEM RESTART REPLICA`. Niveau: `TABLE`. Alias: `RESTART REPLICA`
-    -   `SYSTEM FLUSH`. Niveau: `GROUP`
-        -   `SYSTEM FLUSH DISTRIBUTED`. Niveau: `TABLE`. Alias: `FLUSH DISTRIBUTED`
-        -   `SYSTEM FLUSH LOGS`. Niveau: `GLOBAL`. Alias: `FLUSH LOGS`
-
-Le `SYSTEM RELOAD EMBEDDED DICTIONARIES` privilège implicitement accordé par le `SYSTEM RELOAD DICTIONARY ON *.*` privilège.
-
-### INTROSPECTION {#grant-introspection}
-
-Permet l'utilisation de [introspection](../../operations/optimizing-performance/sampling-query-profiler.md) fonction.
-
--   `INTROSPECTION`. Niveau: `GROUP`. Alias: `INTROSPECTION FUNCTIONS`
-    -   `addressToLine`. Niveau: `GLOBAL`
-    -   `addressToSymbol`. Niveau: `GLOBAL`
-    -   `demangle`. Niveau: `GLOBAL`
-
-### SOURCES {#grant-sources}
-
-Permet d'utiliser des sources de données externes. S'applique à [moteurs de table](../../engines/table-engines/index.md) et [les fonctions de table](../table-functions/index.md#table-functions).
-
--   `SOURCES`. Niveau: `GROUP`
-    -   `FILE`. Niveau: `GLOBAL`
-    -   `URL`. Niveau: `GLOBAL`
-    -   `REMOTE`. Niveau: `GLOBAL`
-    -   `YSQL`. Niveau: `GLOBAL`
-    -   `ODBC`. Niveau: `GLOBAL`
-    -   `JDBC`. Niveau: `GLOBAL`
-    -   `HDFS`. Niveau: `GLOBAL`
-    -   `S3`. Niveau: `GLOBAL`
-
-Le `SOURCES` privilège permet l'utilisation de toutes les sources. Vous pouvez également accorder un privilège pour chaque source individuellement. Pour utiliser les sources, vous avez besoin de privilèges supplémentaires.
-
-Exemple:
-
--   Pour créer une table avec [Moteur de table MySQL](../../engines/table-engines/integrations/mysql.md), vous avez besoin `CREATE TABLE (ON db.table_name)` et `MYSQL` privilège.
--   L'utilisation de la [fonction de table mysql](../table-functions/mysql.md), vous avez besoin `CREATE TEMPORARY TABLE` et `MYSQL` privilège.
-
-### dictGet {#grant-dictget}
-
--   `dictGet`. Alias: `dictHas`, `dictGetHierarchy`, `dictIsIn`
-
-Permet à un utilisateur d'exécuter [dictGet](../functions/ext-dict-functions.md#dictget), [dictHas](../functions/ext-dict-functions.md#dicthas), [dictGetHierarchy](../functions/ext-dict-functions.md#dictgethierarchy), [dictisine](../functions/ext-dict-functions.md#dictisin) fonction.
-
-Niveau de privilège: `DICTIONARY`.
-
-**Exemple**
-
--   `GRANT dictGet ON mydb.mydictionary TO john`
--   `GRANT dictGet ON mydictionary TO john`
-
-### ALL {#grant-all}
-
-Les subventions de tous les privilèges sur l'entité réglementée à un compte d'utilisateur ou un rôle.
-
-### NONE {#grant-none}
-
-N'accorde pas de privilèges.
-
-### ADMIN OPTION {#admin-option-privilege}
-
-Le `ADMIN OPTION` le privilège permet à un utilisateur d'accorder son rôle à un autre utilisateur.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/grant/) <!--hide-->
diff --git a/docs/fr/sql-reference/statements/index.md b/docs/fr/sql-reference/statements/index.md
deleted file mode 100644
index f08d64cee39b..000000000000
--- a/docs/fr/sql-reference/statements/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: "D\xE9claration"
-toc_priority: 31
----
-
-
diff --git a/docs/fr/sql-reference/statements/insert-into.md b/docs/fr/sql-reference/statements/insert-into.md
deleted file mode 100644
index 987594bae658..000000000000
--- a/docs/fr/sql-reference/statements/insert-into.md
+++ /dev/null
@@ -1,80 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 34
-toc_title: INSERT INTO
----
-
-## INSERT {#insert}
-
-L'ajout de données.
-
-Format de requête de base:
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ...
-```
-
-La requête peut spécifier une liste de colonnes à insérer `[(c1, c2, c3)]`. Dans ce cas, le reste des colonnes sont remplis avec:
-
--   Les valeurs calculées à partir `DEFAULT` expressions spécifiées dans la définition de la table.
--   Zéros et chaînes vides, si `DEFAULT` les expressions ne sont pas définies.
-
-Si [strict_insert_defaults=1](../../operations/settings/settings.md), les colonnes qui n'ont pas `DEFAULT` défini doit être répertorié dans la requête.
-
-Les données peuvent être transmises à L'INSERT dans n'importe quel [format](../../interfaces/formats.md#formats) soutenu par ClickHouse. Le format doit être spécifié explicitement dans la requête:
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] FORMAT format_name data_set
-```
-
-For example, the following query format is identical to the basic version of INSERT … VALUES:
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] FORMAT Values (v11, v12, v13), (v21, v22, v23), ...
-```
-
-ClickHouse supprime tous les espaces et un saut de ligne (s'il y en a un) avant les données. Lors de la formation d'une requête, nous recommandons de placer les données sur une nouvelle ligne après les opérateurs de requête (ceci est important si les données commencent par des espaces).
-
-Exemple:
-
-``` sql
-INSERT INTO t FORMAT TabSeparated
-11  Hello, world!
-22  Qwerty
-```
-
-Vous pouvez insérer des données séparément de la requête à l'aide du client de ligne de commande ou de L'interface HTTP. Pour plus d'informations, consultez la section “[Interface](../../interfaces/index.md#interfaces)”.
-
-### Contraintes {#constraints}
-
-Si la table a [contraintes](create.md#constraints), their expressions will be checked for each row of inserted data. If any of those constraints is not satisfied — server will raise an exception containing constraint name and expression, the query will be stopped.
-
-### Insertion des résultats de `SELECT` {#insert_query_insert-select}
-
-``` sql
-INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...
-```
-
-Les colonnes sont mappées en fonction de leur position dans la clause SELECT. Cependant, leurs noms dans L'expression SELECT et la table pour INSERT peuvent différer. Si nécessaire, la coulée de type est effectuée.
-
-Aucun des formats de données à l'exception des Valeurs permettent de définir des valeurs d'expressions telles que `now()`, `1 + 2` et ainsi de suite. Le format des valeurs permet une utilisation limitée des expressions, mais ce n'est pas recommandé, car dans ce cas, un code inefficace est utilisé pour leur exécution.
-
-Les autres requêtes de modification des parties de données ne sont pas prises en charge: `UPDATE`, `DELETE`, `REPLACE`, `MERGE`, `UPSERT`, `INSERT UPDATE`.
-Cependant, vous pouvez supprimer les anciennes données en utilisant `ALTER TABLE ... DROP PARTITION`.
-
-`FORMAT` la clause doit être spécifié à la fin de la requête si `SELECT` la clause contient la fonction de table [entrée()](../table-functions/input.md).
-
-### Considérations De Performance {#performance-considerations}
-
-`INSERT` trie les données d'entrée par la clé primaire et les divise en partitions par une clé de partition. Si vous insérez des données dans plusieurs partitions à la fois, cela peut réduire considérablement les performances de l' `INSERT` requête. Pour éviter cela:
-
--   Ajoutez des données en lots assez importants, tels que 100 000 lignes à la fois.
--   Groupez les données par une clé de partition avant de les télécharger sur ClickHouse.
-
-Les performances ne diminueront pas si:
-
--   Les données sont ajoutées en temps réel.
--   Vous téléchargez des données qui sont généralement triées par heure.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/insert_into/) <!--hide-->
diff --git a/docs/fr/sql-reference/statements/misc.md b/docs/fr/sql-reference/statements/misc.md
deleted file mode 100644
index 4631f856266e..000000000000
--- a/docs/fr/sql-reference/statements/misc.md
+++ /dev/null
@@ -1,358 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: Autre
----
-
-# Diverses Requêtes {#miscellaneous-queries}
-
-## ATTACH {#attach}
-
-Cette requête est exactement la même que `CREATE`, mais
-
--   Au lieu de la parole `CREATE` il utilise le mot `ATTACH`.
--   La requête ne crée pas de données sur le disque, mais suppose que les données sont déjà aux endroits appropriés, et ajoute simplement des informations sur la table au serveur.
-    Après avoir exécuté une requête ATTACH, le serveur connaîtra l'existence de la table.
-
-Si la table a été précédemment détachée (`DETACH`), ce qui signifie que sa structure est connue, vous pouvez utiliser un raccourci sans définir la structure.
-
-``` sql
-ATTACH TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Cette requête est utilisée lors du démarrage du serveur. Le serveur stocke les métadonnées de la table sous forme de fichiers avec `ATTACH` requêtes, qu'il exécute simplement au lancement (à l'exception des tables système, qui sont explicitement créées sur le serveur).
-
-## CHECK TABLE {#check-table}
-
-Vérifie si les données de la table sont corrompues.
-
-``` sql
-CHECK TABLE [db.]name
-```
-
-Le `CHECK TABLE` requête compare réelle des tailles de fichier avec les valeurs attendues qui sont stockés sur le serveur. Si le fichier tailles ne correspondent pas aux valeurs stockées, cela signifie que les données sont endommagées. Cela peut être causé, par exemple, par un plantage du système lors de l'exécution de la requête.
-
-La réponse de la requête contient `result` colonne avec une seule ligne. La ligne a une valeur de
-[Booléen](../../sql-reference/data-types/boolean.md) type:
-
--   0 - les données de la table sont corrompues.
--   1 - les données maintiennent l'intégrité.
-
-Le `CHECK TABLE` query prend en charge les moteurs de table suivants:
-
--   [Journal](../../engines/table-engines/log-family/log.md)
--   [TinyLog](../../engines/table-engines/log-family/tinylog.md)
--   [StripeLog](../../engines/table-engines/log-family/stripelog.md)
--   [Famille MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)
-
-Effectué sur les tables avec un autre moteur de table provoque une exception.
-
-Les moteurs de la `*Log` la famille ne fournit pas de récupération automatique des données en cas d'échec. L'utilisation de la `CHECK TABLE` requête pour suivre la perte de données en temps opportun.
-
-Pour `MergeTree` moteurs de la famille, le `CHECK TABLE` query affiche un État de vérification pour chaque partie de données individuelle d'une table sur le serveur local.
-
-**Si les données sont corrompues**
-
-Si la table est corrompue, vous pouvez copier les données non corrompues dans une autre table. Pour ce faire:
-
-1.  Créez une nouvelle table avec la même structure que la table endommagée. Pour ce faire exécutez la requête `CREATE TABLE <new_table_name> AS <damaged_table_name>`.
-2.  Définir le [max_threads](../../operations/settings/settings.md#settings-max_threads) la valeur 1 pour traiter la requête suivante dans un seul thread. Pour ce faire, exécutez la requête `SET max_threads = 1`.
-3.  Exécuter la requête `INSERT INTO <new_table_name> SELECT * FROM <damaged_table_name>`. Cette demande copie les données non corrompues de la table endommagée vers une autre table. Seules les données avant la partie corrompue seront copiées.
-4.  Redémarrez l' `clickhouse-client` pour réinitialiser l' `max_threads` valeur.
-
-## DESCRIBE TABLE {#misc-describe-table}
-
-``` sql
-DESC|DESCRIBE TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
-```
-
-Renvoie ce qui suit `String` les colonnes de type:
-
--   `name` — Column name.
--   `type`— Column type.
--   `default_type` — Clause that is used in [expression par défaut](create.md#create-default-values) (`DEFAULT`, `MATERIALIZED` ou `ALIAS`). Column contient une chaîne vide, si l'expression par défaut n'est pas spécifiée.
--   `default_expression` — Value specified in the `DEFAULT` clause.
--   `comment_expression` — Comment text.
-
-Les structures de données imbriquées sont sorties dans “expanded” format. Chaque colonne est affichée séparément, avec le nom après un point.
-
-## DETACH {#detach}
-
-Supprime les informations sur le ‘name’ table du serveur. Le serveur cesse de connaître l'existence de la table.
-
-``` sql
-DETACH TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Cela ne supprime pas les données ou les métadonnées de la table. Lors du prochain lancement du serveur, le serveur Lira les métadonnées et découvrira à nouveau la table.
-De même, un “detached” tableau peut être re-attaché en utilisant le `ATTACH` requête (à l'exception des tables système, qui n'ont pas de stocker les métadonnées pour eux).
-
-Il n'y a pas de `DETACH DATABASE` requête.
-
-## DROP {#drop}
-
-Cette requête a deux types: `DROP DATABASE` et `DROP TABLE`.
-
-``` sql
-DROP DATABASE [IF EXISTS] db [ON CLUSTER cluster]
-```
-
-Supprime toutes les tables à l'intérieur de la ‘db’ la base de données, puis supprime le ‘db’ la base de données elle-même.
-Si `IF EXISTS` est spécifié, il ne renvoie pas d'erreur si la base de données n'existe pas.
-
-``` sql
-DROP [TEMPORARY] TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Supprime la table.
-Si `IF EXISTS` est spécifié, il ne renvoie pas d'erreur si la table n'existe pas ou si la base de données n'existe pas.
-
-    DROP DICTIONARY [IF EXISTS] [db.]name
-
-Delets le dictionnaire.
-Si `IF EXISTS` est spécifié, il ne renvoie pas d'erreur si la table n'existe pas ou si la base de données n'existe pas.
-
-## DROP USER {#drop-user-statement}
-
-Supprime un utilisateur.
-
-### Syntaxe {#drop-user-syntax}
-
-``` sql
-DROP USER [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP ROLE {#drop-role-statement}
-
-Supprime un rôle.
-
-Le rôle supprimé est révoqué de toutes les entités où il a été accordé.
-
-### Syntaxe {#drop-role-syntax}
-
-``` sql
-DROP ROLE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP ROW POLICY {#drop-row-policy-statement}
-
-Supprime une stratégie de ligne.
-
-La stratégie de ligne supprimée est révoquée de toutes les entités sur lesquelles elle a été affectée.
-
-### Syntaxe {#drop-row-policy-syntax}
-
-``` sql
-DROP [ROW] POLICY [IF EXISTS] name [,...] ON [database.]table [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP QUOTA {#drop-quota-statement}
-
-Supprime un quota.
-
-Le quota supprimé est révoqué de toutes les entités où il a été affecté.
-
-### Syntaxe {#drop-quota-syntax}
-
-``` sql
-DROP QUOTA [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## DROP SETTINGS PROFILE {#drop-settings-profile-statement}
-
-Supprime un quota.
-
-Le quota supprimé est révoqué de toutes les entités où il a été affecté.
-
-### Syntaxe {#drop-settings-profile-syntax}
-
-``` sql
-DROP [SETTINGS] PROFILE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]
-```
-
-## EXISTS {#exists-statement}
-
-``` sql
-EXISTS [TEMPORARY] [TABLE|DICTIONARY] [db.]name [INTO OUTFILE filename] [FORMAT format]
-```
-
-Renvoie un seul `UInt8`- type colonne, qui contient la valeur unique `0` si la table ou base de données n'existe pas, ou `1` si la table existe dans la base de données spécifiée.
-
-## KILL QUERY {#kill-query-statement}
-
-``` sql
-KILL QUERY [ON CLUSTER cluster]
-  WHERE <where expression to SELECT FROM system.processes query>
-  [SYNC|ASYNC|TEST]
-  [FORMAT format]
-```
-
-Tente de mettre fin de force aux requêtes en cours d'exécution.
-Les requêtes à terminer sont sélectionnées dans le système.processus en utilisant les critères définis dans le `WHERE` la clause de la `KILL` requête.
-
-Exemple:
-
-``` sql
--- Forcibly terminates all queries with the specified query_id:
-KILL QUERY WHERE query_id='2-857d-4a57-9ee0-327da5d60a90'
-
--- Synchronously terminates all queries run by 'username':
-KILL QUERY WHERE user='username' SYNC
-```
-
-Les utilisateurs en lecture seule peuvent uniquement arrêter leurs propres requêtes.
-
-Par défaut, la version asynchrone des requêtes est utilisé (`ASYNC`), qui n'attend pas la confirmation que les requêtes se sont arrêtées.
-
-La version synchrone (`SYNC`) attend que toutes les requêtes d'arrêter et affiche des informations sur chaque processus s'arrête.
-La réponse contient l' `kill_status` la colonne, qui peut prendre les valeurs suivantes:
-
-1.  ‘finished’ – The query was terminated successfully.
-2.  ‘waiting’ – Waiting for the query to end after sending it a signal to terminate.
-3.  The other values ​​explain why the query can't be stopped.
-
-Une requête de test (`TEST`) vérifie uniquement les droits de l'utilisateur et affiche une liste de requêtes à arrêter.
-
-## KILL MUTATION {#kill-mutation}
-
-``` sql
-KILL MUTATION [ON CLUSTER cluster]
-  WHERE <where expression to SELECT FROM system.mutations query>
-  [TEST]
-  [FORMAT format]
-```
-
-Essaie d'annuler et supprimer [mutation](alter.md#alter-mutations) actuellement en cours d'exécution. Les Mutations à annuler sont sélectionnées parmi [`system.mutations`](../../operations/system-tables.md#system_tables-mutations) tableau à l'aide du filtre spécifié par le `WHERE` la clause de la `KILL` requête.
-
-Une requête de test (`TEST`) vérifie uniquement les droits de l'utilisateur et affiche une liste de requêtes à arrêter.
-
-Exemple:
-
-``` sql
--- Cancel and remove all mutations of the single table:
-KILL MUTATION WHERE database = 'default' AND table = 'table'
-
--- Cancel the specific mutation:
-KILL MUTATION WHERE database = 'default' AND table = 'table' AND mutation_id = 'mutation_3.txt'
-```
-
-The query is useful when a mutation is stuck and cannot finish (e.g. if some function in the mutation query throws an exception when applied to the data contained in the table).
-
-Les modifications déjà apportées par la mutation ne sont pas annulées.
-
-## OPTIMIZE {#misc_operations-optimize}
-
-``` sql
-OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition | PARTITION ID 'partition_id'] [FINAL] [DEDUPLICATE]
-```
-
-Cette requête tente d'initialiser une fusion non programmée de parties de données pour les tables avec un moteur de [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) famille.
-
-Le `OPTMIZE` la requête est également prise en charge pour [MaterializedView](../../engines/table-engines/special/materializedview.md) et la [Tampon](../../engines/table-engines/special/buffer.md) moteur. Les autres moteurs de table ne sont pas pris en charge.
-
-Lorsque `OPTIMIZE` est utilisé avec le [ReplicatedMergeTree](../../engines/table-engines/mergetree-family/replication.md) famille de moteurs de table, ClickHouse crée une tâche pour la fusion et attend l'exécution sur tous les nœuds (si le `replication_alter_partitions_sync` paramètre est activé).
-
--   Si `OPTIMIZE` n'effectue pas de fusion pour une raison quelconque, il ne notifie pas le client. Pour activer les notifications, utilisez [optimize_throw_if_noop](../../operations/settings/settings.md#setting-optimize_throw_if_noop) paramètre.
--   Si vous spécifiez un `PARTITION`, seule la partition spécifiée est optimisé. [Comment définir l'expression de la partition](alter.md#alter-how-to-specify-part-expr).
--   Si vous spécifiez `FINAL`, l'optimisation est effectuée, même lorsque toutes les données sont déjà dans une partie.
--   Si vous spécifiez `DEDUPLICATE`, alors des lignes complètement identiques seront dédupliquées (toutes les colonnes sont comparées), cela n'a de sens que pour le moteur MergeTree.
-
-!!! warning "Avertissement"
-    `OPTIMIZE` ne peut pas réparer le “Too many parts” erreur.
-
-## RENAME {#misc_operations-rename}
-
-Renomme une ou plusieurs tables.
-
-``` sql
-RENAME TABLE [db11.]name11 TO [db12.]name12, [db21.]name21 TO [db22.]name22, ... [ON CLUSTER cluster]
-```
-
-Toutes les tables sont renommées sous verrouillage global. Renommer des tables est une opération légère. Si vous avez indiqué une autre base de données après TO, la table sera déplacée vers cette base de données. Cependant, les répertoires contenant des bases de données doivent résider dans le même système de fichiers (sinon, une erreur est renvoyée).
-
-## SET {#query-set}
-
-``` sql
-SET param = value
-```
-
-Assigner `value` à l' `param` [paramètre](../../operations/settings/index.md) pour la session en cours. Vous ne pouvez pas modifier [les paramètres du serveur](../../operations/server-configuration-parameters/index.md) de cette façon.
-
-Vous pouvez également définir toutes les valeurs de certains paramètres de profil dans une seule requête.
-
-``` sql
-SET profile = 'profile-name-from-the-settings-file'
-```
-
-Pour plus d'informations, voir [Paramètre](../../operations/settings/settings.md).
-
-## SET ROLE {#set-role-statement}
-
-Active les rôles pour l'utilisateur actuel.
-
-### Syntaxe {#set-role-syntax}
-
-``` sql
-SET ROLE {DEFAULT | NONE | role [,...] | ALL | ALL EXCEPT role [,...]}
-```
-
-## SET DEFAULT ROLE {#set-default-role-statement}
-
-Définit les rôles par défaut à un utilisateur.
-
-Les rôles par défaut sont automatiquement activés lors de la connexion de l'utilisateur. Vous pouvez définir par défaut uniquement les rôles précédemment accordés. Si le rôle n'est pas accordé à un utilisateur, ClickHouse lève une exception.
-
-### Syntaxe {#set-default-role-syntax}
-
-``` sql
-SET DEFAULT ROLE {NONE | role [,...] | ALL | ALL EXCEPT role [,...]} TO {user|CURRENT_USER} [,...]
-```
-
-### Exemple {#set-default-role-examples}
-
-Définir plusieurs rôles par défaut à un utilisateur:
-
-``` sql
-SET DEFAULT ROLE role1, role2, ... TO user
-```
-
-Définissez tous les rôles accordés par défaut sur un utilisateur:
-
-``` sql
-SET DEFAULT ROLE ALL TO user
-```
-
-Purger les rôles par défaut d'un utilisateur:
-
-``` sql
-SET DEFAULT ROLE NONE TO user
-```
-
-Définissez tous les rôles accordés par défaut à l'exception de certains d'entre eux:
-
-``` sql
-SET DEFAULT ROLE ALL EXCEPT role1, role2 TO user
-```
-
-## TRUNCATE {#truncate-statement}
-
-``` sql
-TRUNCATE TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
-```
-
-Supprime toutes les données d'une table. Lorsque la clause `IF EXISTS` est omis, la requête renvoie une erreur si la table n'existe pas.
-
-Le `TRUNCATE` la requête n'est pas prise en charge pour [Vue](../../engines/table-engines/special/view.md), [Fichier](../../engines/table-engines/special/file.md), [URL](../../engines/table-engines/special/url.md) et [NULL](../../engines/table-engines/special/null.md) table des moteurs.
-
-## USE {#use}
-
-``` sql
-USE db
-```
-
-Vous permet de définir la base de données actuelle pour la session.
-La base de données actuelle est utilisée pour rechercher des tables si la base de données n'est pas explicitement définie dans la requête avec un point avant le nom de la table.
-Cette requête ne peut pas être faite lors de l'utilisation du protocole HTTP, car il n'y a pas de concept de session.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/misc/) <!--hide-->
diff --git a/docs/fr/sql-reference/statements/revoke.md b/docs/fr/sql-reference/statements/revoke.md
deleted file mode 100644
index 6137cc30f8cc..000000000000
--- a/docs/fr/sql-reference/statements/revoke.md
+++ /dev/null
@@ -1,50 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: REVOKE
----
-
-# REVOKE {#revoke}
-
-Révoque les privilèges des utilisateurs ou rôles.
-
-## Syntaxe {#revoke-syntax}
-
-**Révocation des privilèges des utilisateurs**
-
-``` sql
-REVOKE [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} FROM {user | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user | CURRENT_USER} [,...]
-```
-
-**Révocation des rôles des utilisateurs**
-
-``` sql
-REVOKE [ON CLUSTER cluster_name] [ADMIN OPTION FOR] role [,...] FROM {user | role | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user_name | role_name | CURRENT_USER} [,...]
-```
-
-## Description {#revoke-description}
-
-Pour révoquer certains privilèges, vous pouvez utiliser un privilège de portée plus large que vous envisagez de révoquer. Par exemple, si un utilisateur a la `SELECT (x,y)` privilège, administrateur peut effectuer `REVOKE SELECT(x,y) ...`, ou `REVOKE SELECT * ...` ou même `REVOKE ALL PRIVILEGES ...` requête de révoquer ce privilège.
-
-### Révocations Partielles {#partial-revokes-dscr}
-
-Vous pouvez révoquer une partie d'un privilège. Par exemple, si un utilisateur a la `SELECT *.*` Privilège vous pouvez révoquer un privilège pour lire les données d'une table ou d'une base de données.
-
-## Exemple {#revoke-example}
-
-Subvention de l' `john` compte utilisateur avec le privilège de sélectionner parmi toutes les bases de données `accounts` un:
-
-``` sql
-GRANT SELECT ON *.* TO john;
-REVOKE SELECT ON accounts.* FROM john;
-```
-
-Subvention de l' `mira` compte utilisateur avec le privilège de sélectionner parmi toutes les colonnes `accounts.staff` tableau à l'exception de la `wage` un.
-
-``` sql
-GRANT SELECT ON accounts.staff TO mira;
-REVOKE SELECT(wage) ON accounts.staff FROM mira;
-```
-
-{## [Article Original](https://clickhouse.tech/docs/en/operations/settings/settings/) ##}
diff --git a/docs/fr/sql-reference/statements/select/array-join.md b/docs/fr/sql-reference/statements/select/array-join.md
deleted file mode 100644
index 07b27d5d16cf..000000000000
--- a/docs/fr/sql-reference/statements/select/array-join.md
+++ /dev/null
@@ -1,282 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause de jointure de tableau {#select-array-join-clause}
-
-C'est une opération courante pour les tables qui contiennent une colonne de tableau pour produire une nouvelle table qui a une colonne avec chaque élément de tableau individuel de cette colonne initiale, tandis que les valeurs des autres colonnes sont dupliquées. C'est le cas de fond de ce `ARRAY JOIN` la clause le fait.
-
-Son nom vient du fait qu'il peut être regardé comme l'exécution de `JOIN` avec un tableau ou une structure de données imbriquée. L'intention est similaire à la [arrayJoin](../../functions/array-join.md#functions_arrayjoin) fonction, mais la fonctionnalité de la clause est plus large.
-
-Syntaxe:
-
-``` sql
-SELECT <expr_list>
-FROM <left_subquery>
-[LEFT] ARRAY JOIN <array>
-[WHERE|PREWHERE <expr>]
-...
-```
-
-Vous ne pouvez en spécifier qu'un `ARRAY JOIN` la clause dans un `SELECT` requête.
-
-Types pris en charge de `ARRAY JOIN` sont énumérés ci-dessous:
-
--   `ARRAY JOIN` - Dans le cas de base, les tableaux vides ne sont pas inclus dans le résultat de `JOIN`.
--   `LEFT ARRAY JOIN` - Le résultat de `JOIN` contient des lignes avec des tableaux vides. La valeur d'un tableau vide est définie sur la valeur par défaut pour le type d'élément de tableau (généralement 0, chaîne vide ou NULL).
-
-## Exemples de jointure de tableau de base {#basic-array-join-examples}
-
-Les exemples ci-dessous illustrent l'utilisation de la `ARRAY JOIN` et `LEFT ARRAY JOIN` clause. Créons une table avec un [Tableau](../../../sql-reference/data-types/array.md) tapez colonne et insérez des valeurs dedans:
-
-``` sql
-CREATE TABLE arrays_test
-(
-    s String,
-    arr Array(UInt8)
-) ENGINE = Memory;
-
-INSERT INTO arrays_test
-VALUES ('Hello', [1,2]), ('World', [3,4,5]), ('Goodbye', []);
-```
-
-``` text
-┌─s───────────┬─arr─────┐
-│ Hello       │ [1,2]   │
-│ World       │ [3,4,5] │
-│ Goodbye     │ []      │
-└─────────────┴─────────┘
-```
-
-L'exemple ci-dessous utilise la `ARRAY JOIN` clause:
-
-``` sql
-SELECT s, arr
-FROM arrays_test
-ARRAY JOIN arr;
-```
-
-``` text
-┌─s─────┬─arr─┐
-│ Hello │   1 │
-│ Hello │   2 │
-│ World │   3 │
-│ World │   4 │
-│ World │   5 │
-└───────┴─────┘
-```
-
-L'exemple suivant utilise l' `LEFT ARRAY JOIN` clause:
-
-``` sql
-SELECT s, arr
-FROM arrays_test
-LEFT ARRAY JOIN arr;
-```
-
-``` text
-┌─s───────────┬─arr─┐
-│ Hello       │   1 │
-│ Hello       │   2 │
-│ World       │   3 │
-│ World       │   4 │
-│ World       │   5 │
-│ Goodbye     │   0 │
-└─────────────┴─────┘
-```
-
-## À L'Aide D'Alias {#using-aliases}
-
-Un alias peut être spécifié pour un tableau `ARRAY JOIN` clause. Dans ce cas, un élément de tableau peut être consulté par ce pseudonyme, mais le tableau lui-même est accessible par le nom d'origine. Exemple:
-
-``` sql
-SELECT s, arr, a
-FROM arrays_test
-ARRAY JOIN arr AS a;
-```
-
-``` text
-┌─s─────┬─arr─────┬─a─┐
-│ Hello │ [1,2]   │ 1 │
-│ Hello │ [1,2]   │ 2 │
-│ World │ [3,4,5] │ 3 │
-│ World │ [3,4,5] │ 4 │
-│ World │ [3,4,5] │ 5 │
-└───────┴─────────┴───┘
-```
-
-En utilisant des alias, vous pouvez effectuer `ARRAY JOIN` avec un groupe externe. Exemple:
-
-``` sql
-SELECT s, arr_external
-FROM arrays_test
-ARRAY JOIN [1, 2, 3] AS arr_external;
-```
-
-``` text
-┌─s───────────┬─arr_external─┐
-│ Hello       │            1 │
-│ Hello       │            2 │
-│ Hello       │            3 │
-│ World       │            1 │
-│ World       │            2 │
-│ World       │            3 │
-│ Goodbye     │            1 │
-│ Goodbye     │            2 │
-│ Goodbye     │            3 │
-└─────────────┴──────────────┘
-```
-
-Plusieurs tableaux peuvent être séparés par des virgules `ARRAY JOIN` clause. Dans ce cas, `JOIN` est effectuée avec eux simultanément (la somme directe, pas le produit cartésien). Notez que tous les tableaux doivent avoir la même taille. Exemple:
-
-``` sql
-SELECT s, arr, a, num, mapped
-FROM arrays_test
-ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num, arrayMap(x -> x + 1, arr) AS mapped;
-```
-
-``` text
-┌─s─────┬─arr─────┬─a─┬─num─┬─mapped─┐
-│ Hello │ [1,2]   │ 1 │   1 │      2 │
-│ Hello │ [1,2]   │ 2 │   2 │      3 │
-│ World │ [3,4,5] │ 3 │   1 │      4 │
-│ World │ [3,4,5] │ 4 │   2 │      5 │
-│ World │ [3,4,5] │ 5 │   3 │      6 │
-└───────┴─────────┴───┴─────┴────────┘
-```
-
-L'exemple ci-dessous utilise la [arrayEnumerate](../../../sql-reference/functions/array-functions.md#array_functions-arrayenumerate) fonction:
-
-``` sql
-SELECT s, arr, a, num, arrayEnumerate(arr)
-FROM arrays_test
-ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num;
-```
-
-``` text
-┌─s─────┬─arr─────┬─a─┬─num─┬─arrayEnumerate(arr)─┐
-│ Hello │ [1,2]   │ 1 │   1 │ [1,2]               │
-│ Hello │ [1,2]   │ 2 │   2 │ [1,2]               │
-│ World │ [3,4,5] │ 3 │   1 │ [1,2,3]             │
-│ World │ [3,4,5] │ 4 │   2 │ [1,2,3]             │
-│ World │ [3,4,5] │ 5 │   3 │ [1,2,3]             │
-└───────┴─────────┴───┴─────┴─────────────────────┘
-```
-
-## Jointure de tableau avec la Structure de données imbriquée {#array-join-with-nested-data-structure}
-
-`ARRAY JOIN` fonctionne également avec [structures de données imbriquées](../../../sql-reference/data-types/nested-data-structures/nested.md):
-
-``` sql
-CREATE TABLE nested_test
-(
-    s String,
-    nest Nested(
-    x UInt8,
-    y UInt32)
-) ENGINE = Memory;
-
-INSERT INTO nested_test
-VALUES ('Hello', [1,2], [10,20]), ('World', [3,4,5], [30,40,50]), ('Goodbye', [], []);
-```
-
-``` text
-┌─s───────┬─nest.x──┬─nest.y─────┐
-│ Hello   │ [1,2]   │ [10,20]    │
-│ World   │ [3,4,5] │ [30,40,50] │
-│ Goodbye │ []      │ []         │
-└─────────┴─────────┴────────────┘
-```
-
-``` sql
-SELECT s, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN nest;
-```
-
-``` text
-┌─s─────┬─nest.x─┬─nest.y─┐
-│ Hello │      1 │     10 │
-│ Hello │      2 │     20 │
-│ World │      3 │     30 │
-│ World │      4 │     40 │
-│ World │      5 │     50 │
-└───────┴────────┴────────┘
-```
-
-Lorsque vous spécifiez des noms de structures de données imbriquées dans `ARRAY JOIN` le sens est le même que `ARRAY JOIN` avec tous les éléments du tableau qui la compose. Des exemples sont énumérés ci-dessous:
-
-``` sql
-SELECT s, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN `nest.x`, `nest.y`;
-```
-
-``` text
-┌─s─────┬─nest.x─┬─nest.y─┐
-│ Hello │      1 │     10 │
-│ Hello │      2 │     20 │
-│ World │      3 │     30 │
-│ World │      4 │     40 │
-│ World │      5 │     50 │
-└───────┴────────┴────────┘
-```
-
-Cette variation a également du sens:
-
-``` sql
-SELECT s, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN `nest.x`;
-```
-
-``` text
-┌─s─────┬─nest.x─┬─nest.y─────┐
-│ Hello │      1 │ [10,20]    │
-│ Hello │      2 │ [10,20]    │
-│ World │      3 │ [30,40,50] │
-│ World │      4 │ [30,40,50] │
-│ World │      5 │ [30,40,50] │
-└───────┴────────┴────────────┘
-```
-
-Un alias peut être utilisé pour une structure de données imbriquée, afin de sélectionner `JOIN` le résultat ou le tableau source. Exemple:
-
-``` sql
-SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`
-FROM nested_test
-ARRAY JOIN nest AS n;
-```
-
-``` text
-┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┐
-│ Hello │   1 │  10 │ [1,2]   │ [10,20]    │
-│ Hello │   2 │  20 │ [1,2]   │ [10,20]    │
-│ World │   3 │  30 │ [3,4,5] │ [30,40,50] │
-│ World │   4 │  40 │ [3,4,5] │ [30,40,50] │
-│ World │   5 │  50 │ [3,4,5] │ [30,40,50] │
-└───────┴─────┴─────┴─────────┴────────────┘
-```
-
-Exemple d'utilisation de l' [arrayEnumerate](../../../sql-reference/functions/array-functions.md#array_functions-arrayenumerate) fonction:
-
-``` sql
-SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`, num
-FROM nested_test
-ARRAY JOIN nest AS n, arrayEnumerate(`nest.x`) AS num;
-```
-
-``` text
-┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┬─num─┐
-│ Hello │   1 │  10 │ [1,2]   │ [10,20]    │   1 │
-│ Hello │   2 │  20 │ [1,2]   │ [10,20]    │   2 │
-│ World │   3 │  30 │ [3,4,5] │ [30,40,50] │   1 │
-│ World │   4 │  40 │ [3,4,5] │ [30,40,50] │   2 │
-│ World │   5 │  50 │ [3,4,5] │ [30,40,50] │   3 │
-└───────┴─────┴─────┴─────────┴────────────┴─────┘
-```
-
-## Détails De Mise En Œuvre {#implementation-details}
-
-L'ordre d'exécution de la requête est optimisé lors de l'exécution `ARRAY JOIN`. Bien `ARRAY JOIN` doit toujours être spécifié avant l' [WHERE](where.md)/[PREWHERE](prewhere.md) dans une requête, techniquement, ils peuvent être exécutés dans n'importe quel ordre, sauf résultat de `ARRAY JOIN` est utilisé pour le filtrage. L'ordre de traitement est contrôlée par l'optimiseur de requête.
diff --git a/docs/fr/sql-reference/statements/select/distinct.md b/docs/fr/sql-reference/statements/select/distinct.md
deleted file mode 100644
index 94552018c981..000000000000
--- a/docs/fr/sql-reference/statements/select/distinct.md
+++ /dev/null
@@ -1,63 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# La Clause DISTINCT {#select-distinct}
-
-Si `SELECT DISTINCT` est spécifié, seules les lignes uniques restera un résultat de requête. Ainsi, une seule ligne restera hors de tous les ensembles de lignes entièrement correspondantes dans le résultat.
-
-## Le Traitement Null {#null-processing}
-
-`DISTINCT` fonctionne avec [NULL](../../syntax.md#null-literal) comme si `NULL` ont une valeur spécifique, et `NULL==NULL`. En d'autres termes, dans le `DISTINCT` résultats, différentes combinaisons avec `NULL` une fois seulement. Elle diffère de `NULL` traitement dans la plupart des autres contextes.
-
-## Alternative {#alternatives}
-
-Il est possible d'obtenir le même résultat en appliquant [GROUP BY](group-by.md) sur le même ensemble de valeurs, comme spécifié comme `SELECT` clause, sans utiliser de fonctions d'agrégation. Mais il y a peu de différences de `GROUP BY` approche:
-
--   `DISTINCT` peut être utilisé avec d' `GROUP BY`.
--   Lorsque [ORDER BY](order-by.md) est omis et [LIMIT](limit.md) est définie, la requête s'arrête immédiatement après le nombre de lignes différentes, a été lu.
--   Les blocs de données sont produits au fur et à mesure qu'ils sont traités, sans attendre que la requête entière se termine.
-
-## Limitation {#limitations}
-
-`DISTINCT` n'est pas pris en charge si `SELECT` a au moins une colonne de tableau.
-
-## Exemple {#examples}
-
-Clickhouse prend en charge l'utilisation du `DISTINCT` et `ORDER BY` clauses pour différentes colonnes dans une requête. Le `DISTINCT` la clause est exécutée avant la `ORDER BY` clause.
-
-Exemple de table:
-
-``` text
-┌─a─┬─b─┐
-│ 2 │ 1 │
-│ 1 │ 2 │
-│ 3 │ 3 │
-│ 2 │ 4 │
-└───┴───┘
-```
-
-Lors de la sélection de données avec le `SELECT DISTINCT a FROM t1 ORDER BY b ASC` requête, nous obtenons le résultat suivant:
-
-``` text
-┌─a─┐
-│ 2 │
-│ 1 │
-│ 3 │
-└───┘
-```
-
-Si nous changeons la direction de tri `SELECT DISTINCT a FROM t1 ORDER BY b DESC`, nous obtenons le résultat suivant:
-
-``` text
-┌─a─┐
-│ 3 │
-│ 1 │
-│ 2 │
-└───┘
-```
-
-Rangée `2, 4` a été coupé avant de les trier.
-
-Prenez en compte cette spécificité d'implémentation lors de la programmation des requêtes.
diff --git a/docs/fr/sql-reference/statements/select/format.md b/docs/fr/sql-reference/statements/select/format.md
deleted file mode 100644
index a88bb7831ba1..000000000000
--- a/docs/fr/sql-reference/statements/select/format.md
+++ /dev/null
@@ -1,18 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# FORMAT de la Clause {#format-clause}
-
-Clickhouse prend en charge une large gamme de [formats de sérialisation](../../../interfaces/formats.md) qui peut être utilisé sur les résultats de la requête entre autres choses. Il existe plusieurs façons de choisir un format pour `SELECT` de sortie, l'un d'eux est de spécifier `FORMAT format` à la fin de la requête pour obtenir les données résultantes dans tout format spécifique.
-
-Un format spécifique peut être utilisé pour des raisons de commodité, d'intégration avec d'autres systèmes ou d'amélioration des performances.
-
-## Format Par Défaut {#default-format}
-
-Si l' `FORMAT` la clause est omise, le format par défaut est utilisé, ce qui dépend à la fois des paramètres et de l'interface utilisée pour accéder au serveur ClickHouse. Pour l' [Interface HTTP](../../../interfaces/http.md) et la [client de ligne de commande](../../../interfaces/cli.md) en mode batch, le format par défaut est `TabSeparated`. Pour le client de ligne de commande en mode interactif, le format par défaut est `PrettyCompact` (il produit des tables compactes lisibles par l'homme).
-
-## Détails De Mise En Œuvre {#implementation-details}
-
-Lors de l'utilisation du client de ligne de commande, les données sont toujours transmises sur le réseau dans un format efficace interne (`Native`). Le client interprète indépendamment le `FORMAT` clause de la requête et formate les données elles-mêmes (soulageant ainsi le réseau et le serveur de la charge supplémentaire).
diff --git a/docs/fr/sql-reference/statements/select/from.md b/docs/fr/sql-reference/statements/select/from.md
deleted file mode 100644
index 964ffdd13fbb..000000000000
--- a/docs/fr/sql-reference/statements/select/from.md
+++ /dev/null
@@ -1,44 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# De la Clause {#select-from}
-
-Le `FROM` clause spécifie la source à partir de laquelle lire les données:
-
--   [Table](../../../engines/table-engines/index.md)
--   [Sous-requête](index.md) {##TODO: meilleur lien ##}
--   [Fonction de Table](../../table-functions/index.md#table-functions)
-
-[JOIN](join.md) et [ARRAY JOIN](array-join.md) les clauses peuvent également être utilisées pour étendre la fonctionnalité de la `FROM` clause.
-
-Subquery est un autre `SELECT` requête qui peut être spécifié entre parenthèses à l'intérieur `FROM` clause.
-
-`FROM` la clause peut contenir plusieurs sources de données, séparées par des virgules, ce qui équivaut à effectuer [CROSS JOIN](join.md) sur eux.
-
-## Modificateur FINAL {#select-from-final}
-
-Lorsque `FINAL` est spécifié, ClickHouse fusionne complètement les données avant de renvoyer le résultat et effectue ainsi toutes les transformations de données qui se produisent lors des fusions pour le moteur de table donné.
-
-Il est applicable lors de la sélection de données à partir de tables qui utilisent [MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md)-la famille de moteurs (à l'exception de `GraphiteMergeTree`). Également pris en charge pour:
-
--   [Répliqué](../../../engines/table-engines/mergetree-family/replication.md) les versions de `MergeTree` moteur.
--   [Vue](../../../engines/table-engines/special/view.md), [Tampon](../../../engines/table-engines/special/buffer.md), [Distribué](../../../engines/table-engines/special/distributed.md), et [MaterializedView](../../../engines/table-engines/special/materializedview.md) moteurs qui fonctionnent sur d'autres moteurs, à condition qu'ils aient été créés sur `MergeTree`-tables de moteur.
-
-### Inconvénient {#drawbacks}
-
-Requêtes qui utilisent `FINAL` sont exécutés pas aussi vite que les requêtes similaires qui ne le font pas, car:
-
--   La requête est exécutée dans un seul thread et les données sont fusionnées lors de l'exécution de la requête.
--   Les requêtes avec `FINAL` lire les colonnes de clé primaire en plus des colonnes spécifiées dans la requête.
-
-**Dans la plupart des cas, évitez d'utiliser `FINAL`.** L'approche commune consiste à utiliser différentes requêtes qui supposent les processus d'arrière-plan du `MergeTree` le moteur n'est pas encore arrivé et y faire face en appliquant l'agrégation (par exemple, pour éliminer les doublons). {##TODO: exemples ##}
-
-## Détails De Mise En Œuvre {#implementation-details}
-
-Si l' `FROM` la clause est omise, les données seront lues à partir `system.one` table.
-Le `system.one` table contient exactement une ligne (cette table remplit le même but que la table double trouvée dans d'autres SGBD).
-
-Pour exécuter une requête, toutes les colonnes mentionnées dans la requête sont extraites de la table appropriée. Toutes les colonnes non nécessaires pour la requête externe sont rejetées des sous-requêtes.
-Si une requête ne répertorie aucune colonne (par exemple, `SELECT count() FROM t`), une colonne est extraite de la table de toute façon (la plus petite est préférée), afin de calculer le nombre de lignes.
diff --git a/docs/fr/sql-reference/statements/select/group-by.md b/docs/fr/sql-reference/statements/select/group-by.md
deleted file mode 100644
index 9d1b5c276d54..000000000000
--- a/docs/fr/sql-reference/statements/select/group-by.md
+++ /dev/null
@@ -1,132 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause GROUP BY {#select-group-by-clause}
-
-`GROUP BY` la clause change le `SELECT` requête dans un mode d'agrégation, qui fonctionne comme suit:
-
--   `GROUP BY` clause contient une liste des expressions (ou une seule expression, qui est considéré comme la liste de longueur). Cette liste agit comme un “grouping key”, tandis que chaque expression individuelle sera appelée “key expressions”.
--   Toutes les expressions dans le [SELECT](index.md), [HAVING](having.md), et [ORDER BY](order-by.md) clause **devoir** être calculé sur la base d'expressions clés **ou** sur [les fonctions d'agrégation](../../../sql-reference/aggregate-functions/index.md) sur les expressions non-clés (y compris les colonnes simples). En d'autres termes, chaque colonne sélectionnée dans la table doit être utilisée soit dans une expression de clé, soit dans une fonction d'agrégat, mais pas les deux.
--   Résultat de l'agrégation de `SELECT` la requête contiendra autant de lignes qu'il y avait des valeurs uniques de “grouping key” dans la table source. Habituellement, cela réduit considérablement le nombre de lignes, souvent par ordre de grandeur, mais pas nécessairement: le nombre de lignes reste le même si tous “grouping key” les valeurs sont distinctes.
-
-!!! note "Note"
-    Il existe un moyen supplémentaire d'exécuter l'agrégation sur une table. Si une requête ne contient que des colonnes de table à l'intérieur des fonctions `GROUP BY clause` peut être omis, et l'agrégation par un ensemble vide de touches est supposé. Ces interrogations renvoient toujours exactement une ligne.
-
-## Le Traitement NULL {#null-processing}
-
-Pour le regroupement, ClickHouse interprète [NULL](../../syntax.md#null-literal) comme une valeur, et `NULL==NULL`. Elle diffère de `NULL` traitement dans la plupart des autres contextes.
-
-Voici un exemple pour montrer ce que cela signifie.
-
-Supposons que vous avez cette table:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │    2 │
-│ 2 │ ᴺᵁᴸᴸ │
-│ 3 │    2 │
-│ 3 │    3 │
-│ 3 │ ᴺᵁᴸᴸ │
-└───┴──────┘
-```
-
-Requête `SELECT sum(x), y FROM t_null_big GROUP BY y` résultats dans:
-
-``` text
-┌─sum(x)─┬────y─┐
-│      4 │    2 │
-│      3 │    3 │
-│      5 │ ᴺᵁᴸᴸ │
-└────────┴──────┘
-```
-
-Vous pouvez voir que `GROUP BY` pour `y = NULL` résumer `x` comme si `NULL` a cette valeur.
-
-Si vous passez plusieurs clés `GROUP BY` le résultat vous donnera toutes les combinaisons de la sélection, comme si `NULL` ont une valeur spécifique.
-
-## Avec modificateur de totaux {#with-totals-modifier}
-
-Si l' `WITH TOTALS` modificateur est spécifié, une autre ligne sera calculée. Cette ligne aura des colonnes clés contenant des valeurs par défaut (zéros ou lignes vides), et des colonnes de fonctions d'agrégat avec les valeurs calculées sur toutes les lignes (le “total” valeur).
-
-Cette ligne supplémentaire est uniquement produite en `JSON*`, `TabSeparated*`, et `Pretty*` formats, séparément des autres lignes:
-
--   Dans `JSON*` formats, cette ligne est sortie en tant que distinct ‘totals’ champ.
--   Dans `TabSeparated*` formats, la ligne vient après le résultat principal, précédé par une ligne vide (après les autres données).
--   Dans `Pretty*` formats, la ligne est sortie comme une table séparée après le résultat principal.
--   Dans les autres formats, il n'est pas disponible.
-
-`WITH TOTALS` peut être exécuté de différentes manières lorsqu'il est présent. Le comportement dépend de l' ‘totals_mode’ paramètre.
-
-### Configuration Du Traitement Des Totaux {#configuring-totals-processing}
-
-Par défaut, `totals_mode = 'before_having'`. Dans ce cas, ‘totals’ est calculé sur toutes les lignes, y compris celles qui ne passent pas par `max_rows_to_group_by`.
-
-Les autres alternatives incluent uniquement les lignes qui passent à travers avoir dans ‘totals’, et se comporter différemment avec le réglage `max_rows_to_group_by` et `group_by_overflow_mode = 'any'`.
-
-`after_having_exclusive` – Don't include rows that didn't pass through `max_rows_to_group_by`. En d'autres termes, ‘totals’ aura moins ou le même nombre de lignes que si `max_rows_to_group_by` ont été omis.
-
-`after_having_inclusive` – Include all the rows that didn't pass through ‘max_rows_to_group_by’ dans ‘totals’. En d'autres termes, ‘totals’ aura plus ou le même nombre de lignes que si `max_rows_to_group_by` ont été omis.
-
-`after_having_auto` – Count the number of rows that passed through HAVING. If it is more than a certain amount (by default, 50%), include all the rows that didn't pass through ‘max_rows_to_group_by’ dans ‘totals’. Sinon, ne pas les inclure.
-
-`totals_auto_threshold` – By default, 0.5. The coefficient for `after_having_auto`.
-
-Si `max_rows_to_group_by` et `group_by_overflow_mode = 'any'` ne sont pas utilisés, toutes les variations de `after_having` sont les mêmes, et vous pouvez utiliser l'un d'eux (par exemple, `after_having_auto`).
-
-Vous pouvez utiliser avec les totaux dans les sous-requêtes, y compris les sous-requêtes dans la clause JOIN (dans ce cas, les valeurs totales respectives sont combinées).
-
-## Exemple {#examples}
-
-Exemple:
-
-``` sql
-SELECT
-    count(),
-    median(FetchTiming > 60 ? 60 : FetchTiming),
-    count() - sum(Refresh)
-FROM hits
-```
-
-Cependant, contrairement au SQL standard, si la table n'a pas de lignes (soit il n'y en a pas du tout, soit il n'y en a pas après avoir utilisé WHERE to filter), un résultat vide est renvoyé, et non le résultat d'une des lignes contenant les valeurs initiales des fonctions d'agrégat.
-
-Contrairement à MySQL (et conforme à SQL standard), vous ne pouvez pas obtenir une valeur d'une colonne qui n'est pas dans une fonction clé ou agrégée (sauf les expressions constantes). Pour contourner ce problème, vous pouvez utiliser le ‘any’ fonction d'agrégation (récupère la première valeur rencontrée) ou ‘min/max’.
-
-Exemple:
-
-``` sql
-SELECT
-    domainWithoutWWW(URL) AS domain,
-    count(),
-    any(Title) AS title -- getting the first occurred page header for each domain.
-FROM hits
-GROUP BY domain
-```
-
-Pour chaque valeur de clé différente rencontrée, GROUP BY calcule un ensemble de valeurs de fonction d'agrégation.
-
-GROUP BY n'est pas pris en charge pour les colonnes de tableau.
-
-Une constante ne peut pas être spécifiée comme arguments pour les fonctions d'agrégation. Exemple: somme(1). Au lieu de cela, vous pouvez vous débarrasser de la constante. Exemple: `count()`.
-
-## Détails De Mise En Œuvre {#implementation-details}
-
-L'agrégation est l'une des caractéristiques les plus importantes d'un SGBD orienté colonne, et donc son implémentation est l'une des parties les plus optimisées de ClickHouse. Par défaut, l'agrégation se fait en mémoire à l'aide d'une table de hachage. Il a plus de 40 spécialisations qui sont choisies automatiquement en fonction de “grouping key” types de données.
-
-### Groupe par dans la mémoire externe {#select-group-by-in-external-memory}
-
-Vous pouvez activer le dumping des données temporaires sur le disque pour limiter l'utilisation de la mémoire pendant `GROUP BY`.
-Le [max_bytes_before_external_group_by](../../../operations/settings/settings.md#settings-max_bytes_before_external_group_by) réglage détermine le seuil de consommation de RAM pour le dumping `GROUP BY` données temporaires dans le système de fichiers. Si elle est définie sur 0 (valeur par défaut), elle est désactivée.
-
-Lors de l'utilisation de `max_bytes_before_external_group_by`, nous vous recommandons de définir `max_memory_usage` environ deux fois plus élevé. Ceci est nécessaire car il y a deux étapes à l'agrégation: la lecture des données et la formation des données intermédiaires (1) et la fusion des données intermédiaires (2). Le Dumping des données dans le système de fichiers ne peut se produire qu'au cours de l'étape 1. Si les données temporaires n'ont pas été vidées, l'étape 2 peut nécessiter jusqu'à la même quantité de mémoire qu'à l'étape 1.
-
-Par exemple, si [max_memory_usage](../../../operations/settings/settings.md#settings_max_memory_usage) a été défini sur 10000000000 et que vous souhaitez utiliser l'agrégation externe, il est logique de définir `max_bytes_before_external_group_by` à 10000000000, et `max_memory_usage` à 20000000000. Lorsque l'agrégation externe est déclenchée (s'il y a eu au moins un vidage de données temporaires), la consommation maximale de RAM n'est que légèrement supérieure à `max_bytes_before_external_group_by`.
-
-Avec le traitement des requêtes distribuées, l'agrégation externe est effectuée sur des serveurs distants. Pour que le serveur demandeur n'utilise qu'une petite quantité de RAM, définissez `distributed_aggregation_memory_efficient` 1.
-
-Lors de la fusion de données vidées sur le disque, ainsi que lors de la fusion des résultats de serveurs distants lorsque `distributed_aggregation_memory_efficient` paramètre est activé, consomme jusqu'à `1/256 * the_number_of_threads` à partir de la quantité totale de mémoire RAM.
-
-Lorsque l'agrégation externe est activée, s'il y a moins de `max_bytes_before_external_group_by` of data (i.e. data was not flushed), the query runs just as fast as without external aggregation. If any temporary data was flushed, the run time will be several times longer (approximately three times).
-
-Si vous avez un [ORDER BY](order-by.md) avec un [LIMIT](limit.md) après `GROUP BY` puis la quantité de RAM dépend de la quantité de données dans `LIMIT`, pas dans l'ensemble de la table. Mais si l' `ORDER BY` n'a pas `LIMIT`, n'oubliez pas d'activer externe de tri (`max_bytes_before_external_sort`).
diff --git a/docs/fr/sql-reference/statements/select/having.md b/docs/fr/sql-reference/statements/select/having.md
deleted file mode 100644
index 9425830c3d4a..000000000000
--- a/docs/fr/sql-reference/statements/select/having.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause HAVING {#having-clause}
-
-Permet de filtrer les résultats d'agrégation produits par [GROUP BY](group-by.md). Il est similaire à la [WHERE](where.md) la clause, mais la différence est que `WHERE` est effectuée avant l'agrégation, tandis que `HAVING` est effectué d'après elle.
-
-Il est possible de référencer les résultats d'agrégation à partir de `SELECT` la clause dans `HAVING` clause par leur alias. Alternativement, `HAVING` clause peut filtrer sur les résultats d'agrégats supplémentaires qui ne sont pas retournés dans les résultats de la requête.
-
-## Limitation {#limitations}
-
-`HAVING` ne peut pas être utilisé si le regroupement n'est pas effectuée. Utiliser `WHERE` plutôt.
diff --git a/docs/fr/sql-reference/statements/select/index.md b/docs/fr/sql-reference/statements/select/index.md
deleted file mode 100644
index 1d53ae80eb4b..000000000000
--- a/docs/fr/sql-reference/statements/select/index.md
+++ /dev/null
@@ -1,158 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 33
-toc_title: SELECT
----
-
-# Sélectionnez la syntaxe des requêtes {#select-queries-syntax}
-
-`SELECT` effectue la récupération des données.
-
-``` sql
-[WITH expr_list|(subquery)]
-SELECT [DISTINCT] expr_list
-[FROM [db.]table | (subquery) | table_function] [FINAL]
-[SAMPLE sample_coeff]
-[ARRAY JOIN ...]
-[GLOBAL] [ANY|ALL|ASOF] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI] JOIN (subquery)|table (ON <expr_list>)|(USING <column_list>)
-[PREWHERE expr]
-[WHERE expr]
-[GROUP BY expr_list] [WITH TOTALS]
-[HAVING expr]
-[ORDER BY expr_list] [WITH FILL] [FROM expr] [TO expr] [STEP expr] 
-[LIMIT [offset_value, ]n BY columns]
-[LIMIT [n, ]m] [WITH TIES]
-[UNION ALL ...]
-[INTO OUTFILE filename]
-[FORMAT format]
-```
-
-Toutes les clauses sont facultatives, à l'exception de la liste d'expressions requise immédiatement après `SELECT` qui est abordée plus en détail [dessous](#select-clause).
-
-Spécificités de chaque clause facultative, sont couverts dans des sections distinctes, qui sont énumérés dans le même ordre qu'elles sont exécutées:
-
--   [AVEC la clause](with.md)
--   [La clause DISTINCT](distinct.md)
--   [De la clause](from.md)
--   [Exemple de clause](sample.md)
--   [Clause de JOINTURE](join.md)
--   [Clause PREWHERE](prewhere.md)
--   [Clause where](where.md)
--   [Groupe par clause](group-by.md)
--   [Limite par clause](limit-by.md)
--   [Clause HAVING](having.md)
--   [Clause SELECT](#select-clause)
--   [Clause LIMIT](limit.md)
--   [Clause UNION ALL](union.md)
-
-## Clause SELECT {#select-clause}
-
-[Expression](../../syntax.md#syntax-expressions) spécifié dans le `SELECT` clause sont calculés après toutes les opérations dans les clauses décrites ci-dessus sont terminés. Ces expressions fonctionnent comme si elles s'appliquaient à des lignes séparées dans le résultat. Si les expressions dans le `SELECT` la clause contient des fonctions d'agrégation, puis clickhouse traite les fonctions d'agrégation et les expressions utilisées [GROUP BY](group-by.md) agrégation.
-
-Si vous souhaitez inclure toutes les colonnes dans le résultat, utilisez l'astérisque (`*`) symbole. Exemple, `SELECT * FROM ...`.
-
-Pour correspondre à certaines colonnes dans le résultat avec un [re2](https://en.wikipedia.org/wiki/RE2_(software)) expression régulière, vous pouvez utiliser le `COLUMNS` expression.
-
-``` sql
-COLUMNS('regexp')
-```
-
-Par exemple, considérez le tableau:
-
-``` sql
-CREATE TABLE default.col_names (aa Int8, ab Int8, bc Int8) ENGINE = TinyLog
-```
-
-La requête suivante sélectionne les données de toutes les colonnes contenant les `a` symbole dans leur nom.
-
-``` sql
-SELECT COLUMNS('a') FROM col_names
-```
-
-``` text
-┌─aa─┬─ab─┐
-│  1 │  1 │
-└────┴────┘
-```
-
-Les colonnes sélectionnées sont retournés pas dans l'ordre alphabétique.
-
-Vous pouvez utiliser plusieurs `COLUMNS` expressions dans une requête et leur appliquer des fonctions.
-
-Exemple:
-
-``` sql
-SELECT COLUMNS('a'), COLUMNS('c'), toTypeName(COLUMNS('c')) FROM col_names
-```
-
-``` text
-┌─aa─┬─ab─┬─bc─┬─toTypeName(bc)─┐
-│  1 │  1 │  1 │ Int8           │
-└────┴────┴────┴────────────────┘
-```
-
-Chaque colonne renvoyée par le `COLUMNS` expression est passée à la fonction en tant qu'argument séparé. Vous pouvez également passer d'autres arguments à la fonction si elle les supporte. Soyez prudent lorsque vous utilisez des fonctions. Si une fonction ne prend pas en charge le nombre d'arguments que vous lui avez transmis, ClickHouse lève une exception.
-
-Exemple:
-
-``` sql
-SELECT COLUMNS('a') + COLUMNS('c') FROM col_names
-```
-
-``` text
-Received exception from server (version 19.14.1):
-Code: 42. DB::Exception: Received from localhost:9000. DB::Exception: Number of arguments for function plus doesn't match: passed 3, should be 2.
-```
-
-Dans cet exemple, `COLUMNS('a')` retourne deux colonnes: `aa` et `ab`. `COLUMNS('c')` renvoie la `bc` colonne. Le `+` l'opérateur ne peut pas s'appliquer à 3 arguments, donc ClickHouse lève une exception avec le message pertinent.
-
-Colonnes qui correspondent à la `COLUMNS` l'expression peut avoir différents types de données. Si `COLUMNS` ne correspond à aucune colonne et est la seule expression dans `SELECT`, ClickHouse lance une exception.
-
-### Astérisque {#asterisk}
-
-Vous pouvez mettre un astérisque dans quelque partie de la requête au lieu d'une expression. Lorsque la requête est analysée, l'astérisque est étendu à une liste de toutes les colonnes `MATERIALIZED` et `ALIAS` colonne). Il n'y a que quelques cas où l'utilisation d'un astérisque est justifiée:
-
--   Lors de la création d'un vidage de table.
--   Pour les tables contenant seulement quelques colonnes, comme les tables système.
--   Pour obtenir des informations sur ce que sont les colonnes dans une table. Dans ce cas, la valeur `LIMIT 1`. Mais il est préférable d'utiliser la `DESC TABLE` requête.
--   Quand il y a une forte filtration sur un petit nombre de colonnes en utilisant `PREWHERE`.
--   Dans les sous-requêtes (puisque les colonnes qui ne sont pas nécessaires pour la requête externe sont exclues des sous-requêtes).
-
-Dans tous les autres cas, nous ne recommandons pas d'utiliser l'astérisque, car il ne vous donne que les inconvénients d'un SGBD colonnaire au lieu des avantages. En d'autres termes, l'utilisation de l'astérisque n'est pas recommandée.
-
-### Les Valeurs Extrêmes {#extreme-values}
-
-En plus des résultats, vous pouvez également obtenir des valeurs minimales et maximales pour les colonnes de résultats. Pour ce faire, définissez la **extrême** réglage sur 1. Les Minimums et les maximums sont calculés pour les types numériques, les dates et les dates avec des heures. Pour les autres colonnes, les valeurs par défaut sont sorties.
-
-An extra two rows are calculated – the minimums and maximums, respectively. These extra two rows are output in `JSON*`, `TabSeparated*`, et `Pretty*` [format](../../../interfaces/formats.md), séparés des autres lignes. Ils ne sont pas Produits pour d'autres formats.
-
-Dans `JSON*` formats, les valeurs extrêmes sont sorties dans un ‘extremes’ champ. Dans `TabSeparated*` formats, la ligne vient après le résultat principal, et après ‘totals’ si elle est présente. Elle est précédée par une ligne vide (après les autres données). Dans `Pretty*` formats, la ligne est sortie comme une table séparée après le résultat principal, et après `totals` si elle est présente.
-
-Les valeurs extrêmes sont calculées pour les lignes avant `LIMIT` mais après `LIMIT BY`. Cependant, lors de l'utilisation de `LIMIT offset, size`, les lignes avant de les `offset` sont inclus dans `extremes`. Dans les requêtes de flux, le résultat peut également inclure un petit nombre de lignes qui ont traversé `LIMIT`.
-
-### Note {#notes}
-
-Vous pouvez utiliser des synonymes (`AS` alias) dans n'importe quelle partie d'une requête.
-
-Le `GROUP BY` et `ORDER BY` les clauses ne supportent pas les arguments positionnels. Cela contredit MySQL, mais est conforme à SQL standard. Exemple, `GROUP BY 1, 2` will be interpreted as grouping by constants (i.e. aggregation of all rows into one).
-
-## Détails De Mise En Œuvre {#implementation-details}
-
-Si la requête omet le `DISTINCT`, `GROUP BY` et `ORDER BY` les clauses et les `IN` et `JOIN` sous-requêtes, la requête sera complètement traitée en flux, en utilisant O (1) quantité de RAM. Sinon, la requête peut consommer beaucoup de RAM si les restrictions appropriées ne sont pas spécifiées:
-
--   `max_memory_usage`
--   `max_rows_to_group_by`
--   `max_rows_to_sort`
--   `max_rows_in_distinct`
--   `max_bytes_in_distinct`
--   `max_rows_in_set`
--   `max_bytes_in_set`
--   `max_rows_in_join`
--   `max_bytes_in_join`
--   `max_bytes_before_external_sort`
--   `max_bytes_before_external_group_by`
-
-Pour plus d'informations, consultez la section “Settings”. Il est possible d'utiliser le tri externe (sauvegarde des tables temporaires sur un disque) et l'agrégation externe.
-
-{## [Article Original](https://clickhouse.tech/docs/en/sql-reference/statements/select/) ##}
diff --git a/docs/fr/sql-reference/statements/select/into-outfile.md b/docs/fr/sql-reference/statements/select/into-outfile.md
deleted file mode 100644
index 0150de7cb972..000000000000
--- a/docs/fr/sql-reference/statements/select/into-outfile.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Dans OUTFILE Clause {#into-outfile-clause}
-
-Ajouter l' `INTO OUTFILE filename` clause (où filename est un littéral de chaîne) pour `SELECT query` pour rediriger sa sortie vers le fichier spécifié côté client.
-
-## Détails De Mise En Œuvre {#implementation-details}
-
--   Cette fonctionnalité est disponible dans les [client de ligne de commande](../../../interfaces/cli.md) et [clickhouse-local](../../../operations/utilities/clickhouse-local.md). Ainsi, une requête envoyée par [Interface HTTP](../../../interfaces/http.md) va échouer.
--   La requête échouera si un fichier portant le même nom existe déjà.
--   Défaut [le format de sortie](../../../interfaces/formats.md) être `TabSeparated` (comme dans le mode batch client en ligne de commande).
diff --git a/docs/fr/sql-reference/statements/select/join.md b/docs/fr/sql-reference/statements/select/join.md
deleted file mode 100644
index 4233a1206742..000000000000
--- a/docs/fr/sql-reference/statements/select/join.md
+++ /dev/null
@@ -1,187 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause de JOINTURE {#select-join}
-
-Join produit une nouvelle table en combinant des colonnes d'une ou plusieurs tables en utilisant des valeurs communes à chacune. C'est une opération courante dans les bases de données avec support SQL, ce qui correspond à [l'algèbre relationnelle](https://en.wikipedia.org/wiki/Relational_algebra#Joins_and_join-like_operators) rejoindre. Le cas particulier d'une jointure de table est souvent appelé “self-join”.
-
-Syntaxe:
-
-``` sql
-SELECT <expr_list>
-FROM <left_table>
-[GLOBAL] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI|ANY|ASOF] JOIN <right_table>
-(ON <expr_list>)|(USING <column_list>) ...
-```
-
-Les Expressions de `ON` clause et colonnes de `USING` clause sont appelés “join keys”. Sauf indication contraire, joindre un produit [Produit cartésien](https://en.wikipedia.org/wiki/Cartesian_product) des lignes, avec correspondance “join keys”, ce qui pourrait produire des résultats avec beaucoup plus de lignes que les tables source.
-
-## Types de jointure pris en charge {#select-join-types}
-
-Tous les standard [SQL JOIN](https://en.wikipedia.org/wiki/Join_(SQL)) les types sont pris en charge:
-
--   `INNER JOIN`, seules les lignes correspondantes sont retournés.
--   `LEFT OUTER JOIN`, les lignes non correspondantes de la table de gauche sont retournées en plus des lignes correspondantes.
--   `RIGHT OUTER JOIN`, les lignes non correspondantes de la table de gauche sont retournées en plus des lignes correspondantes.
--   `FULL OUTER JOIN`, les lignes non correspondantes des deux tables sont renvoyées en plus des lignes correspondantes.
--   `CROSS JOIN`, produit le produit cartésien des tables entières, “join keys” être **pas** défini.
-
-`JOIN` sans type spécifié implique `INNER`. Mot `OUTER` peut les oublier. Syntaxe Alternative pour `CROSS JOIN` spécifie plusieurs tables dans [De la clause](from.md) séparés par des virgules.
-
-Autres types de jointure disponibles dans ClickHouse:
-
--   `LEFT SEMI JOIN` et `RIGHT SEMI JOIN` une liste blanche sur “join keys”, sans produire un produit cartésien.
--   `LEFT ANTI JOIN` et `RIGHT ANTI JOIN` une liste noire sur “join keys”, sans produire un produit cartésien.
--   `LEFT ANY JOIN`, `RIGHT ANY JOIN` et `INNER ANY JOIN`, partially (for opposite side of `LEFT` and `RIGHT`) or completely (for `INNER` and `FULL`) disables the cartesian product for standard `JOIN` types.
--   `ASOF JOIN` et `LEFT ASOF JOIN`, joining sequences with a non-exact match. `ASOF JOIN` usage is described below.
-
-## Setting {#join-settings}
-
-!!! note "Note"
-    La valeur de rigueur par défaut peut être remplacée à l'aide [join_default_strictness](../../../operations/settings/settings.md#settings-join_default_strictness) paramètre.
-
-### ASOF joindre L'utilisation {#asof-join-usage}
-
-`ASOF JOIN` est utile lorsque vous devez joindre des enregistrements qui n'ont pas de correspondance exacte.
-
-Tables pour `ASOF JOIN` doit avoir une colonne de séquence ordonnée. Cette colonne ne peut pas être seule dans une table et doit être l'un des types de données: `UInt32`, `UInt64`, `Float32`, `Float64`, `Date`, et `DateTime`.
-
-Syntaxe `ASOF JOIN ... ON`:
-
-``` sql
-SELECT expressions_list
-FROM table_1
-ASOF LEFT JOIN table_2
-ON equi_cond AND closest_match_cond
-```
-
-Vous pouvez utiliser n'importe quel nombre de conditions d'égalité et exactement une condition de correspondance la plus proche. Exemple, `SELECT count() FROM table_1 ASOF LEFT JOIN table_2 ON table_1.a == table_2.b AND table_2.t <= table_1.t`.
-
-Conditions prises en charge pour la correspondance la plus proche: `>`, `>=`, `<`, `<=`.
-
-Syntaxe `ASOF JOIN ... USING`:
-
-``` sql
-SELECT expressions_list
-FROM table_1
-ASOF JOIN table_2
-USING (equi_column1, ... equi_columnN, asof_column)
-```
-
-`ASOF JOIN` utiliser `equi_columnX` pour rejoindre sur l'égalité et `asof_column` pour rejoindre le match le plus proche avec le `table_1.asof_column >= table_2.asof_column` condition. Le `asof_column` colonne toujours la dernière dans le `USING` clause.
-
-Par exemple, considérez les tableaux suivants:
-
-         table_1                           table_2
-      event   | ev_time | user_id       event   | ev_time | user_id
-    ----------|---------|----------   ----------|---------|----------
-                  ...                               ...
-    event_1_1 |  12:00  |  42         event_2_1 |  11:59  |   42
-                  ...                 event_2_2 |  12:30  |   42
-    event_1_2 |  13:00  |  42         event_2_3 |  13:00  |   42
-                  ...                               ...
-
-`ASOF JOIN` peut prendre la date d'un événement utilisateur de `table_1` et trouver un événement dans `table_2` où le timestamp est plus proche de l'horodatage de l'événement à partir de `table_1` correspondant à la condition de correspondance la plus proche. Les valeurs d'horodatage égales sont les plus proches si elles sont disponibles. Ici, l' `user_id` la colonne peut être utilisée pour joindre sur l'égalité et le `ev_time` la colonne peut être utilisée pour se joindre à la correspondance la plus proche. Dans notre exemple, `event_1_1` peut être jointe à `event_2_1` et `event_1_2` peut être jointe à `event_2_3`, mais `event_2_2` ne peut pas être rejoint.
-
-!!! note "Note"
-    `ASOF` jointure est **pas** pris en charge dans le [Rejoindre](../../../engines/table-engines/special/join.md) tableau moteur.
-
-## Jointure Distribuée {#global-join}
-
-Il existe deux façons d'exécuter join impliquant des tables distribuées:
-
--   Lors de l'utilisation normale `JOIN` la requête est envoyée aux serveurs distants. Les sous-requêtes sont exécutées sur chacune d'elles afin de créer la bonne table, et la jointure est effectuée avec cette table. En d'autres termes, la table de droite est formée sur chaque serveur séparément.
--   Lors de l'utilisation de `GLOBAL ... JOIN`, d'abord le serveur demandeur exécute une sous-requête pour calculer la bonne table. Cette table temporaire est transmise à chaque serveur distant, et les requêtes sont exécutées sur eux en utilisant les données temporaires qui ont été transmises.
-
-Soyez prudent lorsque vous utilisez `GLOBAL`. Pour plus d'informations, voir le [Sous-requêtes distribuées](../../operators/in.md#select-distributed-subqueries) section.
-
-## Recommandations D'Utilisation {#usage-recommendations}
-
-### Traitement des cellules vides ou nulles {#processing-of-empty-or-null-cells}
-
-Lors de la jonction de tables, les cellules vides peuvent apparaître. Paramètre [join_use_nulls](../../../operations/settings/settings.md#join_use_nulls) définir comment clickhouse remplit ces cellules.
-
-Si l' `JOIN` les touches sont [Nullable](../../data-types/nullable.md) champs, les lignes où au moins une des clés a la valeur [NULL](../../../sql-reference/syntax.md#null-literal) ne sont pas jointes.
-
-### Syntaxe {#syntax}
-
-Les colonnes spécifiées dans `USING` doit avoir les mêmes noms dans les deux sous-requêtes, et les autres colonnes doivent être nommées différemment. Vous pouvez utiliser des alias pour les noms des colonnes dans les sous-requêtes.
-
-Le `USING` clause spécifie une ou plusieurs colonnes de jointure, qui établit l'égalité de ces colonnes. La liste des colonnes est définie sans crochets. Les conditions de jointure plus complexes ne sont pas prises en charge.
-
-### Limitations De Syntaxe {#syntax-limitations}
-
-Pour plusieurs `JOIN` clauses dans un seul `SELECT` requête:
-
--   Prendre toutes les colonnes via `*` n'est disponible que si les tables sont jointes, pas les sous-requêtes.
--   Le `PREWHERE` la clause n'est pas disponible.
-
-Pour `ON`, `WHERE`, et `GROUP BY` clause:
-
--   Les expressions arbitraires ne peuvent pas être utilisées dans `ON`, `WHERE`, et `GROUP BY` mais vous pouvez définir une expression dans un `SELECT` clause et ensuite l'utiliser dans ces clauses via un alias.
-
-### Performance {#performance}
-
-Lors de l'exécution d'un `JOIN`, il n'y a pas d'optimisation de la commande d'exécution par rapport aux autres stades de la requête. La jointure (une recherche dans la table de droite) est exécutée avant de filtrer `WHERE` et avant l'agrégation.
-
-Chaque fois qu'une requête est exécutée avec la même `JOIN`, la sous-requête est exécutée à nouveau car le résultat n'est pas mis en cache. Pour éviter cela, utilisez la spéciale [Rejoindre](../../../engines/table-engines/special/join.md) table engine, qui est un tableau préparé pour l'assemblage qui est toujours en RAM.
-
-Dans certains cas, il est plus efficace d'utiliser [IN](../../operators/in.md) plutôt `JOIN`.
-
-Si vous avez besoin d'un `JOIN` pour se joindre à des tables de dimension (ce sont des tables relativement petites qui contiennent des propriétés de dimension, telles que des noms pour des campagnes publicitaires), un `JOIN` peut-être pas très pratique en raison du fait que la bonne table est ré-accédée pour chaque requête. Pour de tels cas, il y a un “external dictionaries” la fonctionnalité que vous devez utiliser à la place de `JOIN`. Pour plus d'informations, voir le [Dictionnaires externes](../../dictionaries/external-dictionaries/external-dicts.md) section.
-
-### Limitations De Mémoire {#memory-limitations}
-
-Par défaut, ClickHouse utilise [jointure de hachage](https://en.wikipedia.org/wiki/Hash_join) algorithme. ClickHouse prend le `<right_table>` et crée une table de hachage pour cela dans la RAM. Après un certain seuil de consommation de mémoire, ClickHouse revient à fusionner l'algorithme de jointure.
-
-Si vous devez restreindre la consommation de mémoire de l'opération join utilisez les paramètres suivants:
-
--   [max_rows_in_join](../../../operations/settings/query-complexity.md#settings-max_rows_in_join) — Limits number of rows in the hash table.
--   [max_bytes_in_join](../../../operations/settings/query-complexity.md#settings-max_bytes_in_join) — Limits size of the hash table.
-
-Lorsque l'une de ces limites est atteinte, ClickHouse agit comme [join_overflow_mode](../../../operations/settings/query-complexity.md#settings-join_overflow_mode) réglage des instructions.
-
-## Exemple {#examples}
-
-Exemple:
-
-``` sql
-SELECT
-    CounterID,
-    hits,
-    visits
-FROM
-(
-    SELECT
-        CounterID,
-        count() AS hits
-    FROM test.hits
-    GROUP BY CounterID
-) ANY LEFT JOIN
-(
-    SELECT
-        CounterID,
-        sum(Sign) AS visits
-    FROM test.visits
-    GROUP BY CounterID
-) USING CounterID
-ORDER BY hits DESC
-LIMIT 10
-```
-
-``` text
-┌─CounterID─┬───hits─┬─visits─┐
-│   1143050 │ 523264 │  13665 │
-│    731962 │ 475698 │ 102716 │
-│    722545 │ 337212 │ 108187 │
-│    722889 │ 252197 │  10547 │
-│   2237260 │ 196036 │   9522 │
-│  23057320 │ 147211 │   7689 │
-│    722818 │  90109 │  17847 │
-│     48221 │  85379 │   4652 │
-│  19762435 │  77807 │   7026 │
-│    722884 │  77492 │  11056 │
-└───────────┴────────┴────────┘
-```
diff --git a/docs/fr/sql-reference/statements/select/limit-by.md b/docs/fr/sql-reference/statements/select/limit-by.md
deleted file mode 100644
index 4d1bd766ef16..000000000000
--- a/docs/fr/sql-reference/statements/select/limit-by.md
+++ /dev/null
@@ -1,71 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Limite par Clause {#limit-by-clause}
-
-Une requête avec l' `LIMIT n BY expressions` la clause sélectionne le premier `n` lignes pour chaque valeur distincte de `expressions`. La clé pour `LIMIT BY` peut contenir n'importe quel nombre de [expression](../../syntax.md#syntax-expressions).
-
-ClickHouse prend en charge les variantes de syntaxe suivantes:
-
--   `LIMIT [offset_value, ]n BY expressions`
--   `LIMIT n OFFSET offset_value BY expressions`
-
-Pendant le traitement de la requête, ClickHouse sélectionne les données classées par clé de tri. La clé de tri est définie explicitement à l'aide [ORDER BY](order-by.md) clause ou implicitement en tant que propriété du moteur de table. Puis clickhouse s'applique `LIMIT n BY expressions` et renvoie le premier `n` lignes pour chaque combinaison distincte de `expressions`. Si `OFFSET` est spécifié, puis pour chaque bloc de données qui appartient à une combinaison particulière de `expressions`, Clickhouse saute `offset_value` nombre de lignes depuis le début du bloc et renvoie un maximum de `n` les lignes en conséquence. Si `offset_value` est plus grand que le nombre de lignes dans le bloc de données, ClickHouse renvoie zéro lignes du bloc.
-
-!!! note "Note"
-    `LIMIT BY` n'est pas liée à [LIMIT](limit.md). Ils peuvent tous deux être utilisés dans la même requête.
-
-## Exemple {#examples}
-
-Exemple de table:
-
-``` sql
-CREATE TABLE limit_by(id Int, val Int) ENGINE = Memory;
-INSERT INTO limit_by VALUES (1, 10), (1, 11), (1, 12), (2, 20), (2, 21);
-```
-
-Requête:
-
-``` sql
-SELECT * FROM limit_by ORDER BY id, val LIMIT 2 BY id
-```
-
-``` text
-┌─id─┬─val─┐
-│  1 │  10 │
-│  1 │  11 │
-│  2 │  20 │
-│  2 │  21 │
-└────┴─────┘
-```
-
-``` sql
-SELECT * FROM limit_by ORDER BY id, val LIMIT 1, 2 BY id
-```
-
-``` text
-┌─id─┬─val─┐
-│  1 │  11 │
-│  1 │  12 │
-│  2 │  21 │
-└────┴─────┘
-```
-
-Le `SELECT * FROM limit_by ORDER BY id, val LIMIT 2 OFFSET 1 BY id` requête renvoie le même résultat.
-
-La requête suivante renvoie les 5 principaux référents pour chaque `domain, device_type` paire avec un maximum de 100 lignes au total (`LIMIT n BY + LIMIT`).
-
-``` sql
-SELECT
-    domainWithoutWWW(URL) AS domain,
-    domainWithoutWWW(REFERRER_URL) AS referrer,
-    device_type,
-    count() cnt
-FROM hits
-GROUP BY domain, referrer, device_type
-ORDER BY cnt DESC
-LIMIT 5 BY domain, device_type
-LIMIT 100
-```
diff --git a/docs/fr/sql-reference/statements/select/limit.md b/docs/fr/sql-reference/statements/select/limit.md
deleted file mode 100644
index 69334c32cc96..000000000000
--- a/docs/fr/sql-reference/statements/select/limit.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause LIMIT {#limit-clause}
-
-`LIMIT m` permet de sélectionner la première `m` lignes du résultat.
-
-`LIMIT n, m` permet de sélectionner le `m` lignes du résultat après avoir sauté le premier `n` rangée. Le `LIMIT m OFFSET n` la syntaxe est équivalente.
-
-`n` et `m` doivent être des entiers non négatifs.
-
-Si il n'y a pas de [ORDER BY](order-by.md) clause qui trie explicitement les résultats, le choix des lignes pour le résultat peut être arbitraire et non déterministe.
diff --git a/docs/fr/sql-reference/statements/select/order-by.md b/docs/fr/sql-reference/statements/select/order-by.md
deleted file mode 100644
index 2a4ef58d7ade..000000000000
--- a/docs/fr/sql-reference/statements/select/order-by.md
+++ /dev/null
@@ -1,72 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause ORDER BY {#select-order-by}
-
-Le `ORDER BY` clause contient une liste des expressions, qui peuvent être attribuées avec `DESC` (décroissant) ou `ASC` modificateur (ascendant) qui détermine la direction de tri. Si la direction n'est pas spécifié, `ASC` est supposé, donc il est généralement omis. La direction de tri s'applique à une seule expression, pas à la liste entière. Exemple: `ORDER BY Visits DESC, SearchPhrase`
-
-Les lignes qui ont des valeurs identiques pour la liste des expressions de tri sont sorties dans un ordre arbitraire, qui peut également être non déterministe (différent à chaque fois).
-Si la clause ORDER BY est omise, l'ordre des lignes est également indéfini et peut également être non déterministe.
-
-## Tri des valeurs spéciales {#sorting-of-special-values}
-
-Il existe deux approches pour `NaN` et `NULL` ordre de tri:
-
--   Par défaut ou avec le `NULLS LAST` modificateur: d'abord les valeurs, puis `NaN`, puis `NULL`.
--   Avec l' `NULLS FIRST` modificateur: première `NULL`, puis `NaN` puis d'autres valeurs.
-
-### Exemple {#example}
-
-Pour la table
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 2 │    2 │
-│ 1 │  nan │
-│ 2 │    2 │
-│ 3 │    4 │
-│ 5 │    6 │
-│ 6 │  nan │
-│ 7 │ ᴺᵁᴸᴸ │
-│ 6 │    7 │
-│ 8 │    9 │
-└───┴──────┘
-```
-
-Exécuter la requête `SELECT * FROM t_null_nan ORDER BY y NULLS FIRST` obtenir:
-
-``` text
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-│ 7 │ ᴺᵁᴸᴸ │
-│ 1 │  nan │
-│ 6 │  nan │
-│ 2 │    2 │
-│ 2 │    2 │
-│ 3 │    4 │
-│ 5 │    6 │
-│ 6 │    7 │
-│ 8 │    9 │
-└───┴──────┘
-```
-
-Lorsque les nombres à virgule flottante sont triés, les Nan sont séparés des autres valeurs. Quel que soit l'ordre de tri, NaNs viennent à la fin. En d'autres termes, pour le Tri ascendant, ils sont placés comme s'ils étaient plus grands que tous les autres nombres, tandis que pour le Tri descendant, ils sont placés comme s'ils étaient plus petits que les autres.
-
-## Classement De Soutien {#collation-support}
-
-Pour le tri par valeurs de chaîne, vous pouvez spécifier le classement (comparaison). Exemple: `ORDER BY SearchPhrase COLLATE 'tr'` - pour le tri par mot-clé dans l'ordre croissant, en utilisant l'alphabet turc, insensible à la casse, en supposant que les chaînes sont encodées en UTF-8. COLLATE peut être spécifié ou non pour chaque expression dans L'ordre par indépendamment. Si ASC ou DESC est spécifié, COLLATE est spécifié après. Lors de L'utilisation de COLLATE, le tri est toujours insensible à la casse.
-
-Nous recommandons uniquement D'utiliser COLLATE pour le tri final d'un petit nombre de lignes, car le tri avec COLLATE est moins efficace que le tri normal par octets.
-
-## Détails De Mise En Œuvre {#implementation-details}
-
-Moins de RAM est utilisé si un assez petit [LIMIT](limit.md) est précisée en plus `ORDER BY`. Sinon, la quantité de mémoire dépensée est proportionnelle au volume de données à trier. Pour le traitement des requêtes distribuées, si [GROUP BY](group-by.md) est omis, le tri est partiellement effectué sur les serveurs distants et les résultats sont fusionnés Sur le serveur demandeur. Cela signifie que pour le tri distribué, le volume de données à trier peut être supérieur à la quantité de mémoire sur un seul serveur.
-
-S'il N'y a pas assez de RAM, il est possible d'effectuer un tri dans la mémoire externe (création de fichiers temporaires sur un disque). Utilisez le paramètre `max_bytes_before_external_sort` pour ce but. S'il est défini sur 0 (par défaut), le tri externe est désactivé. Si elle est activée, lorsque le volume de données à trier atteint le nombre spécifié d'octets, les données collectées sont triés et déposés dans un fichier temporaire. Une fois toutes les données lues, tous les fichiers triés sont fusionnés et les résultats sont générés. Les fichiers sont écrits dans le `/var/lib/clickhouse/tmp/` dans la configuration (par défaut, mais vous pouvez `tmp_path` paramètre pour modifier ce paramètre).
-
-L'exécution d'une requête peut utiliser plus de mémoire que `max_bytes_before_external_sort`. Pour cette raison, ce paramètre doit avoir une valeur significativement inférieure à `max_memory_usage`. Par exemple, si votre serveur dispose de 128 Go de RAM et que vous devez exécuter une seule requête, définissez `max_memory_usage` à 100 Go, et `max_bytes_before_external_sort` à 80 Go.
-
-Le tri externe fonctionne beaucoup moins efficacement que le tri dans la RAM.
diff --git a/docs/fr/sql-reference/statements/select/prewhere.md b/docs/fr/sql-reference/statements/select/prewhere.md
deleted file mode 100644
index 2c825d050f4c..000000000000
--- a/docs/fr/sql-reference/statements/select/prewhere.md
+++ /dev/null
@@ -1,22 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause PREWHERE {#prewhere-clause}
-
-Prewhere est une optimisation pour appliquer le filtrage plus efficacement. Il est activé par défaut, même si `PREWHERE` la clause n'est pas explicitement spécifié. Il fonctionne en déplaçant automatiquement une partie de [WHERE](where.md) condition à prewhere étape. Le rôle de `PREWHERE` la clause est seulement pour contrôler cette optimisation si vous pensez que vous savez comment le faire mieux que par défaut.
-
-Avec l'optimisation prewhere, au début, seules les colonnes nécessaires à l'exécution de l'expression prewhere sont lues. Ensuite, les autres colonnes sont lues qui sont nécessaires pour exécuter le reste de la requête, mais seulement les blocs où l'expression prewhere est “true” au moins pour certaines lignes. S'il y a beaucoup de blocs où prewhere expression est “false” pour toutes les lignes et prewhere a besoin de moins de colonnes que les autres parties de la requête, cela permet souvent de lire beaucoup moins de données à partir du disque pour l'exécution de la requête.
-
-## Contrôle Manuel De Prewhere {#controlling-prewhere-manually}
-
-La clause a le même sens que la `WHERE` clause. La différence est dans laquelle les données sont lues à partir de la table. Quand à commander manuellement `PREWHERE` pour les conditions de filtration qui sont utilisées par une minorité des colonnes de la requête, mais qui fournissent une filtration de données forte. Cela réduit le volume de données à lire.
-
-Une requête peut spécifier simultanément `PREWHERE` et `WHERE`. Dans ce cas, `PREWHERE` précéder `WHERE`.
-
-Si l' `optimize_move_to_prewhere` le paramètre est défini sur 0, heuristiques pour déplacer automatiquement des parties d'expressions `WHERE` de `PREWHERE` sont désactivés.
-
-## Limitation {#limitations}
-
-`PREWHERE` est uniquement pris en charge par les tables `*MergeTree` famille.
diff --git a/docs/fr/sql-reference/statements/select/sample.md b/docs/fr/sql-reference/statements/select/sample.md
deleted file mode 100644
index b2ddc060a197..000000000000
--- a/docs/fr/sql-reference/statements/select/sample.md
+++ /dev/null
@@ -1,113 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Exemple de Clause {#select-sample-clause}
-
-Le `SAMPLE` clause permet approchée `SELECT` le traitement de la requête.
-
-Lorsque l'échantillonnage de données est activé, la requête n'est pas effectuée sur toutes les données, mais uniquement sur une certaine fraction de données (échantillon). Par exemple, si vous avez besoin de calculer des statistiques pour toutes les visites, il suffit d'exécuter la requête sur le 1/10 de la fraction de toutes les visites, puis multiplier le résultat par 10.
-
-Le traitement approximatif des requêtes peut être utile dans les cas suivants:
-
--   Lorsque vous avez des exigences de synchronisation strictes (comme \<100ms), mais que vous ne pouvez pas justifier le coût des ressources matérielles supplémentaires pour y répondre.
--   Lorsque vos données brutes ne sont pas précises, l'approximation ne dégrade pas sensiblement la qualité.
--   Les exigences commerciales ciblent des résultats approximatifs (pour la rentabilité, ou pour commercialiser des résultats exacts aux utilisateurs premium).
-
-!!! note "Note"
-    Vous ne pouvez utiliser l'échantillonnage qu'avec les tables [MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md) famille, et seulement si l'expression d'échantillonnage a été spécifiée lors de la création de la table (voir [Moteur MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table)).
-
-Les caractéristiques de l'échantillonnage des données sont énumérées ci-dessous:
-
--   L'échantillonnage de données est un mécanisme déterministe. Le résultat de la même `SELECT .. SAMPLE` la requête est toujours le même.
--   L'échantillonnage fonctionne de manière cohérente pour différentes tables. Pour les tables avec une seule clé d'échantillonnage, un échantillon avec le même coefficient sélectionne toujours le même sous-ensemble de données possibles. Par exemple, un exemple d'ID utilisateur prend des lignes avec le même sous-ensemble de tous les ID utilisateur possibles de différentes tables. Cela signifie que vous pouvez utiliser l'exemple dans les sous-requêtes dans la [IN](../../operators/in.md) clause. En outre, vous pouvez joindre des échantillons en utilisant le [JOIN](join.md) clause.
--   L'échantillonnage permet de lire moins de données à partir d'un disque. Notez que vous devez spécifier l'échantillonnage clé correctement. Pour plus d'informations, voir [Création d'une Table MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table).
-
-Pour l' `SAMPLE` clause la syntaxe suivante est prise en charge:
-
-| SAMPLE Clause Syntax | Description                                                                                                                                                                                                                                                                 |
-|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| `SAMPLE k`           | Ici `k` est le nombre de 0 à 1.</br>La requête est exécutée sur `k` fraction des données. Exemple, `SAMPLE 0.1` exécute la requête sur 10% des données. [Lire plus](#select-sample-k)                                                                                       |
-| `SAMPLE n`           | Ici `n` est un entier suffisamment grand.</br>La requête est exécutée sur un échantillon d'au moins `n` lignes (mais pas significativement plus que cela). Exemple, `SAMPLE 10000000` exécute la requête sur un minimum de 10 000 000 lignes. [Lire plus](#select-sample-n) |
-| `SAMPLE k OFFSET m`  | Ici `k` et `m` sont les nombres de 0 à 1.</br>La requête est exécutée sur un échantillon de `k` fraction des données. Les données utilisées pour l'échantillon est compensée par `m` fraction. [Lire plus](#select-sample-offset)                                           |
-
-## SAMPLE K {#select-sample-k}
-
-Ici `k` est le nombre de 0 à 1 (les notations fractionnaires et décimales sont prises en charge). Exemple, `SAMPLE 1/2` ou `SAMPLE 0.5`.
-
-Dans un `SAMPLE k` clause, l'échantillon est prélevé à partir de la `k` fraction des données. L'exemple est illustré ci-dessous:
-
-``` sql
-SELECT
-    Title,
-    count() * 10 AS PageViews
-FROM hits_distributed
-SAMPLE 0.1
-WHERE
-    CounterID = 34
-GROUP BY Title
-ORDER BY PageViews DESC LIMIT 1000
-```
-
-Dans cet exemple, la requête est exécutée sur un échantillon de 0,1 (10%) de données. Les valeurs des fonctions d'agrégat ne sont pas corrigées automatiquement, donc pour obtenir un résultat approximatif, la valeur `count()` est multiplié manuellement par 10.
-
-## SAMPLE N {#select-sample-n}
-
-Ici `n` est un entier suffisamment grand. Exemple, `SAMPLE 10000000`.
-
-Dans ce cas, la requête est exécutée sur un échantillon d'au moins `n` lignes (mais pas significativement plus que cela). Exemple, `SAMPLE 10000000` exécute la requête sur un minimum de 10 000 000 lignes.
-
-Puisque l'unité minimale pour la lecture des données est un granule (sa taille est définie par le `index_granularity` de réglage), il est logique de définir un échantillon beaucoup plus grand que la taille du granule.
-
-Lors de l'utilisation de la `SAMPLE n` clause, vous ne savez pas quel pourcentage relatif de données a été traité. Donc, vous ne connaissez pas le coefficient par lequel les fonctions agrégées doivent être multipliées. L'utilisation de la `_sample_factor` colonne virtuelle pour obtenir le résultat approximatif.
-
-Le `_sample_factor` colonne contient des coefficients relatifs qui sont calculés dynamiquement. Cette colonne est créée automatiquement lorsque vous [créer](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table) une table avec la clé d'échantillonnage spécifiée. Les exemples d'utilisation de la `_sample_factor` colonne sont indiqués ci-dessous.
-
-Considérons la table `visits` qui contient des statistiques sur les visites de site. Le premier exemple montre comment calculer le nombre de pages vues:
-
-``` sql
-SELECT sum(PageViews * _sample_factor)
-FROM visits
-SAMPLE 10000000
-```
-
-L'exemple suivant montre comment calculer le nombre total de visites:
-
-``` sql
-SELECT sum(_sample_factor)
-FROM visits
-SAMPLE 10000000
-```
-
-L'exemple ci-dessous montre comment calculer la durée moyenne de la session. Notez que vous n'avez pas besoin d'utiliser le coefficient relatif pour calculer les valeurs moyennes.
-
-``` sql
-SELECT avg(Duration)
-FROM visits
-SAMPLE 10000000
-```
-
-## SAMPLE K OFFSET M {#select-sample-offset}
-
-Ici `k` et `m` sont des nombres de 0 à 1. Des exemples sont présentés ci-dessous.
-
-**Exemple 1**
-
-``` sql
-SAMPLE 1/10
-```
-
-Dans cet exemple, l'échantillon représente 1 / 10e de toutes les données:
-
-`[++------------]`
-
-**Exemple 2**
-
-``` sql
-SAMPLE 1/10 OFFSET 1/2
-```
-
-Ici, un échantillon de 10% est prélevé à partir de la seconde moitié des données.
-
-`[------++------]`
diff --git a/docs/fr/sql-reference/statements/select/union.md b/docs/fr/sql-reference/statements/select/union.md
deleted file mode 100644
index 9ae65ebcf72e..000000000000
--- a/docs/fr/sql-reference/statements/select/union.md
+++ /dev/null
@@ -1,35 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause UNION ALL {#union-clause}
-
-Vous pouvez utiliser `UNION ALL` à combiner `SELECT` requêtes en étendant leurs résultats. Exemple:
-
-``` sql
-SELECT CounterID, 1 AS table, toInt64(count()) AS c
-    FROM test.hits
-    GROUP BY CounterID
-
-UNION ALL
-
-SELECT CounterID, 2 AS table, sum(Sign) AS c
-    FROM test.visits
-    GROUP BY CounterID
-    HAVING c > 0
-```
-
-Les colonnes de résultat sont appariées par leur index (ordre intérieur `SELECT`). Si les noms de colonne ne correspondent pas, les noms du résultat final sont tirés de la première requête.
-
-La coulée de Type est effectuée pour les syndicats. Par exemple, si deux requêtes combinées ont le même champ avec non-`Nullable` et `Nullable` types d'un type compatible, la `UNION ALL` a un `Nullable` type de champ.
-
-Requêtes qui font partie de `UNION ALL` ne peut pas être placée entre parenthèses. [ORDER BY](order-by.md) et [LIMIT](limit.md) sont appliqués à des requêtes séparées, pas au résultat final. Si vous devez appliquer une conversion au résultat final, vous pouvez mettre toutes les requêtes avec `UNION ALL` dans une sous-requête dans la [FROM](from.md) clause.
-
-## Limitation {#limitations}
-
-Seulement `UNION ALL` est pris en charge. Régulier `UNION` (`UNION DISTINCT`) n'est pas pris en charge. Si vous avez besoin d' `UNION DISTINCT`, vous pouvez écrire `SELECT DISTINCT` à partir d'une sous-requête contenant `UNION ALL`.
-
-## Détails De Mise En Œuvre {#implementation-details}
-
-Requêtes qui font partie de `UNION ALL` peuvent être exécutées simultanément, et leurs résultats peuvent être mélangés ensemble.
diff --git a/docs/fr/sql-reference/statements/select/where.md b/docs/fr/sql-reference/statements/select/where.md
deleted file mode 100644
index a4d7bc5e87a4..000000000000
--- a/docs/fr/sql-reference/statements/select/where.md
+++ /dev/null
@@ -1,15 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# Clause where {#select-where}
-
-`WHERE` clause permet de filtrer les données en provenance de [FROM](from.md) la clause de `SELECT`.
-
-Si il y a un `WHERE` , il doit contenir une expression avec la `UInt8` type. C'est généralement une expression avec comparaison et opérateurs logiques. Les lignes où cette expression est évaluée à 0 sont exclues des transformations ou des résultats ultérieurs.
-
-`WHERE` expression est évaluée sur la possibilité d'utiliser des index et l'élagage de partition, si le moteur de table sous-jacent le prend en charge.
-
-!!! note "Note"
-    Il y a une optimisation de filtrage appelée [prewhere](prewhere.md).
diff --git a/docs/fr/sql-reference/statements/select/with.md b/docs/fr/sql-reference/statements/select/with.md
deleted file mode 100644
index a42aedf460b5..000000000000
--- a/docs/fr/sql-reference/statements/select/with.md
+++ /dev/null
@@ -1,80 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
----
-
-# AVEC la Clause {#with-clause}
-
-Cette section prend en charge les Expressions de Table courantes ([CTE](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL)), de sorte que les résultats de `WITH` la clause peut être utilisé à l'intérieur `SELECT` clause.
-
-## Limitation {#limitations}
-
-1.  Les requêtes récursives ne sont pas prises en charge.
-2.  Lorsque la sous-requête est utilisée à l'intérieur avec section, son résultat doit être scalaire avec exactement une ligne.
-3.  Les résultats d'Expression ne sont pas disponibles dans les sous-requêtes.
-
-## Exemple {#examples}
-
-**Exemple 1:** Utilisation d'une expression constante comme “variable”
-
-``` sql
-WITH '2019-08-01 15:23:00' as ts_upper_bound
-SELECT *
-FROM hits
-WHERE
-    EventDate = toDate(ts_upper_bound) AND
-    EventTime <= ts_upper_bound
-```
-
-**Exemple 2:** De les expulser, somme(octets) résultat de l'expression de clause SELECT de la liste de colonnes
-
-``` sql
-WITH sum(bytes) as s
-SELECT
-    formatReadableSize(s),
-    table
-FROM system.parts
-GROUP BY table
-ORDER BY s
-```
-
-**Exemple 3:** Utilisation des résultats de la sous-requête scalaire
-
-``` sql
-/* this example would return TOP 10 of most huge tables */
-WITH
-    (
-        SELECT sum(bytes)
-        FROM system.parts
-        WHERE active
-    ) AS total_disk_usage
-SELECT
-    (sum(bytes) / total_disk_usage) * 100 AS table_disk_usage,
-    table
-FROM system.parts
-GROUP BY table
-ORDER BY table_disk_usage DESC
-LIMIT 10
-```
-
-**Exemple 4:** Réutilisation de l'expression dans la sous-requête
-
-Comme solution de contournement pour la limitation actuelle de l'utilisation de l'expression dans les sous-requêtes, Vous pouvez la dupliquer.
-
-``` sql
-WITH ['hello'] AS hello
-SELECT
-    hello,
-    *
-FROM
-(
-    WITH ['hello'] AS hello
-    SELECT hello
-)
-```
-
-``` text
-┌─hello─────┬─hello─────┐
-│ ['hello'] │ ['hello'] │
-└───────────┴───────────┘
-```
diff --git a/docs/fr/sql-reference/statements/show.md b/docs/fr/sql-reference/statements/show.md
deleted file mode 100644
index 129c6e30d1c6..000000000000
--- a/docs/fr/sql-reference/statements/show.md
+++ /dev/null
@@ -1,169 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: SHOW
----
-
-# Afficher les requêtes {#show-queries}
-
-## SHOW CREATE TABLE {#show-create-table}
-
-``` sql
-SHOW CREATE [TEMPORARY] [TABLE|DICTIONARY] [db.]table [INTO OUTFILE filename] [FORMAT format]
-```
-
-Renvoie un seul `String`-type ‘statement’ column, which contains a single value – the `CREATE` requête utilisée pour créer l'objet spécifié.
-
-## SHOW DATABASES {#show-databases}
-
-``` sql
-SHOW DATABASES [INTO OUTFILE filename] [FORMAT format]
-```
-
-Imprime une liste de toutes les bases de données.
-Cette requête est identique à `SELECT name FROM system.databases [INTO OUTFILE filename] [FORMAT format]`.
-
-## SHOW PROCESSLIST {#show-processlist}
-
-``` sql
-SHOW PROCESSLIST [INTO OUTFILE filename] [FORMAT format]
-```
-
-Sorties le contenu de la [système.processus](../../operations/system-tables.md#system_tables-processes) table, qui contient une liste de requêtes en cours de traitement en ce moment, à l'exception `SHOW PROCESSLIST` requête.
-
-Le `SELECT * FROM system.processes` requête renvoie des données sur toutes les requêtes en cours.
-
-Astuce (exécuter dans la console):
-
-``` bash
-$ watch -n1 "clickhouse-client --query='SHOW PROCESSLIST'"
-```
-
-## SHOW TABLES {#show-tables}
-
-Affiche une liste de tableaux.
-
-``` sql
-SHOW [TEMPORARY] TABLES [{FROM | IN} <db>] [LIKE '<pattern>' | WHERE expr] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-Si l' `FROM` la clause n'est pas spécifié, la requête renvoie la liste des tables de la base de données actuelle.
-
-Vous pouvez obtenir les mêmes résultats que l' `SHOW TABLES` requête de la façon suivante:
-
-``` sql
-SELECT name FROM system.tables WHERE database = <db> [AND name LIKE <pattern>] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-**Exemple**
-
-La requête suivante sélectionne les deux premières lignes de la liste des tables `system` base de données, dont les noms contiennent `co`.
-
-``` sql
-SHOW TABLES FROM system LIKE '%co%' LIMIT 2
-```
-
-``` text
-┌─name───────────────────────────┐
-│ aggregate_function_combinators │
-│ collations                     │
-└────────────────────────────────┘
-```
-
-## SHOW DICTIONARIES {#show-dictionaries}
-
-Affiche une liste de [dictionnaires externes](../../sql-reference/dictionaries/external-dictionaries/external-dicts.md).
-
-``` sql
-SHOW DICTIONARIES [FROM <db>] [LIKE '<pattern>'] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-Si l' `FROM` la clause n'est pas spécifié, la requête retourne la liste des dictionnaires de la base de données actuelle.
-
-Vous pouvez obtenir les mêmes résultats que l' `SHOW DICTIONARIES` requête de la façon suivante:
-
-``` sql
-SELECT name FROM system.dictionaries WHERE database = <db> [AND name LIKE <pattern>] [LIMIT <N>] [INTO OUTFILE <filename>] [FORMAT <format>]
-```
-
-**Exemple**
-
-La requête suivante sélectionne les deux premières lignes de la liste des tables `system` base de données, dont les noms contiennent `reg`.
-
-``` sql
-SHOW DICTIONARIES FROM db LIKE '%reg%' LIMIT 2
-```
-
-``` text
-┌─name─────────┐
-│ regions      │
-│ region_names │
-└──────────────┘
-```
-
-## SHOW GRANTS {#show-grants-statement}
-
-Montre les privilèges d'un utilisateur.
-
-### Syntaxe {#show-grants-syntax}
-
-``` sql
-SHOW GRANTS [FOR user]
-```
-
-Si l'utilisateur n'est pas spécifié, la requête renvoie les privilèges de l'utilisateur actuel.
-
-## SHOW CREATE USER {#show-create-user-statement}
-
-Affiche les paramètres qui ont été utilisés [la création d'un utilisateur](create.md#create-user-statement).
-
-`SHOW CREATE USER` ne produit pas de mots de passe utilisateur.
-
-### Syntaxe {#show-create-user-syntax}
-
-``` sql
-SHOW CREATE USER [name | CURRENT_USER]
-```
-
-## SHOW CREATE ROLE {#show-create-role-statement}
-
-Affiche les paramètres qui ont été utilisés [la création de rôle](create.md#create-role-statement)
-
-### Syntaxe {#show-create-role-syntax}
-
-``` sql
-SHOW CREATE ROLE name
-```
-
-## SHOW CREATE ROW POLICY {#show-create-row-policy-statement}
-
-Affiche les paramètres qui ont été utilisés [création de stratégie de ligne](create.md#create-row-policy-statement)
-
-### Syntaxe {#show-create-row-policy-syntax}
-
-``` sql
-SHOW CREATE [ROW] POLICY name ON [database.]table
-```
-
-## SHOW CREATE QUOTA {#show-create-quota-statement}
-
-Affiche les paramètres qui ont été utilisés [quota de création](create.md#create-quota-statement)
-
-### Syntaxe {#show-create-row-policy-syntax}
-
-``` sql
-SHOW CREATE QUOTA [name | CURRENT]
-```
-
-## SHOW CREATE SETTINGS PROFILE {#show-create-settings-profile-statement}
-
-Affiche les paramètres qui ont été utilisés [configuration création de profil](create.md#create-settings-profile-statement)
-
-### Syntaxe {#show-create-row-policy-syntax}
-
-``` sql
-SHOW CREATE [SETTINGS] PROFILE name
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/show/) <!--hide-->
diff --git a/docs/fr/sql-reference/statements/system.md b/docs/fr/sql-reference/statements/system.md
deleted file mode 100644
index e8c9ed85cbc3..000000000000
--- a/docs/fr/sql-reference/statements/system.md
+++ /dev/null
@@ -1,113 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: SYSTEM
----
-
-# SYSTÈME de Requêtes {#query-language-system}
-
--   [RELOAD DICTIONARIES](#query_language-system-reload-dictionaries)
--   [RELOAD DICTIONARY](#query_language-system-reload-dictionary)
--   [DROP DNS CACHE](#query_language-system-drop-dns-cache)
--   [DROP MARK CACHE](#query_language-system-drop-mark-cache)
--   [FLUSH LOGS](#query_language-system-flush_logs)
--   [RELOAD CONFIG](#query_language-system-reload-config)
--   [SHUTDOWN](#query_language-system-shutdown)
--   [KILL](#query_language-system-kill)
--   [STOP DISTRIBUTED SENDS](#query_language-system-stop-distributed-sends)
--   [FLUSH DISTRIBUTED](#query_language-system-flush-distributed)
--   [START DISTRIBUTED SENDS](#query_language-system-start-distributed-sends)
--   [STOP MERGES](#query_language-system-stop-merges)
--   [START MERGES](#query_language-system-start-merges)
-
-## RELOAD DICTIONARIES {#query_language-system-reload-dictionaries}
-
-Recharge tous les dictionnaires qui ont déjà été chargés avec succès.
-Par défaut, les dictionnaires sont chargés paresseusement (voir [dictionaries_lazy_load](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-dictionaries_lazy_load)), donc au lieu d'être chargés automatiquement au démarrage, ils sont initialisés lors du premier accès via la fonction dictGet ou sélectionnez dans les tables avec ENGINE = Dictionary . Le `SYSTEM RELOAD DICTIONARIES` query recharge ces dictionnaires (chargés).
-Retourne toujours `Ok.` quel que soit le résultat de la mise à jour du dictionnaire.
-
-## Recharger le dictionnaire Dictionary_name {#query_language-system-reload-dictionary}
-
-Recharge complètement un dictionnaire `dictionary_name`, quel que soit l'état du dictionnaire (LOADED / NOT_LOADED / FAILED).
-Retourne toujours `Ok.` quel que soit le résultat de la mise à jour du dictionnaire.
-L'état du dictionnaire peut être vérifié en interrogeant le `system.dictionaries` table.
-
-``` sql
-SELECT name, status FROM system.dictionaries;
-```
-
-## DROP DNS CACHE {#query_language-system-drop-dns-cache}
-
-Réinitialise le cache DNS interne de ClickHouse. Parfois (pour les anciennes versions de ClickHouse), il est nécessaire d'utiliser cette commande lors de la modification de l'infrastructure (modification de l'adresse IP d'un autre serveur ClickHouse ou du serveur utilisé par les dictionnaires).
-
-Pour une gestion du cache plus pratique (automatique), voir paramètres disable_internal_dns_cache, dns_cache_update_period.
-
-## DROP MARK CACHE {#query_language-system-drop-mark-cache}
-
-Réinitialise le cache de marque. Utilisé dans le développement de ClickHouse et des tests de performance.
-
-## FLUSH LOGS {#query_language-system-flush_logs}
-
-Flushes buffers of log messages to system tables (e.g. system.query_log). Allows you to not wait 7.5 seconds when debugging.
-
-## RELOAD CONFIG {#query_language-system-reload-config}
-
-Recharge la configuration de ClickHouse. Utilisé lorsque la configuration est stockée dans ZooKeeeper.
-
-## SHUTDOWN {#query_language-system-shutdown}
-
-Normalement ferme ClickHouse (comme `service clickhouse-server stop` / `kill {$pid_clickhouse-server}`)
-
-## KILL {#query_language-system-kill}
-
-Annule le processus de ClickHouse (comme `kill -9 {$ pid_clickhouse-server}`)
-
-## Gestion Des Tables Distribuées {#query-language-system-distributed}
-
-ClickHouse peut gérer [distribué](../../engines/table-engines/special/distributed.md) table. Lorsqu'un utilisateur insère des données dans ces tables, ClickHouse crée d'abord une file d'attente des données qui doivent être envoyées aux nœuds de cluster, puis l'envoie de manière asynchrone. Vous pouvez gérer le traitement des files d'attente avec [STOP DISTRIBUTED SENDS](#query_language-system-stop-distributed-sends), [FLUSH DISTRIBUTED](#query_language-system-flush-distributed), et [START DISTRIBUTED SENDS](#query_language-system-start-distributed-sends) requête. Vous pouvez également insérer de manière synchrone des données distribuées avec `insert_distributed_sync` paramètre.
-
-### STOP DISTRIBUTED SENDS {#query_language-system-stop-distributed-sends}
-
-Désactive la distribution de données en arrière-plan lors de l'insertion de données dans des tables distribuées.
-
-``` sql
-SYSTEM STOP DISTRIBUTED SENDS [db.]<distributed_table_name>
-```
-
-### FLUSH DISTRIBUTED {#query_language-system-flush-distributed}
-
-Force ClickHouse à envoyer des données aux nœuds de cluster de manière synchrone. Si des nœuds ne sont pas disponibles, ClickHouse lève une exception et arrête l'exécution de la requête. Vous pouvez réessayer la requête jusqu'à ce qu'elle réussisse, ce qui se produira lorsque tous les nœuds seront de nouveau en ligne.
-
-``` sql
-SYSTEM FLUSH DISTRIBUTED [db.]<distributed_table_name>
-```
-
-### START DISTRIBUTED SENDS {#query_language-system-start-distributed-sends}
-
-Active la distribution de données en arrière-plan lors de l'insertion de données dans des tables distribuées.
-
-``` sql
-SYSTEM START DISTRIBUTED SENDS [db.]<distributed_table_name>
-```
-
-### STOP MERGES {#query_language-system-stop-merges}
-
-Offre la possibilité d'arrêter les fusions d'arrière-plan pour les tables de la famille MergeTree:
-
-``` sql
-SYSTEM STOP MERGES [[db.]merge_tree_family_table_name]
-```
-
-!!! note "Note"
-    `DETACH / ATTACH` table va commencer les fusions d'arrière-plan pour la table même dans le cas où les fusions ont été arrêtées pour toutes les tables MergeTree auparavant.
-
-### START MERGES {#query_language-system-start-merges}
-
-Offre la possibilité de démarrer des fusions en arrière-plan pour les tables de la famille MergeTree:
-
-``` sql
-SYSTEM START MERGES [[db.]merge_tree_family_table_name]
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/system/) <!--hide-->
diff --git a/docs/fr/sql-reference/syntax.md b/docs/fr/sql-reference/syntax.md
deleted file mode 100644
index b8b24c9bbb55..000000000000
--- a/docs/fr/sql-reference/syntax.md
+++ /dev/null
@@ -1,187 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 31
-toc_title: Syntaxe
----
-
-# Syntaxe {#syntax}
-
-Il existe deux types d'analyseurs dans le système: L'analyseur SQL complet (un analyseur de descente récursif) et l'analyseur de format de données (un analyseur de flux rapide).
-Dans tous les cas à l'exception de la `INSERT` requête, seul L'analyseur SQL complet est utilisé.
-Le `INSERT` requête utilise les deux analyseurs:
-
-``` sql
-INSERT INTO t VALUES (1, 'Hello, world'), (2, 'abc'), (3, 'def')
-```
-
-Le `INSERT INTO t VALUES` fragment est analysé par l'analyseur complet, et les données `(1, 'Hello, world'), (2, 'abc'), (3, 'def')` est analysé par l'analyseur de flux rapide. Vous pouvez également activer l'analyseur complet pour les données à l'aide de la [input_format_values_interpret_expressions](../operations/settings/settings.md#settings-input_format_values_interpret_expressions) paramètre. Lorsque `input_format_values_interpret_expressions = 1`, ClickHouse essaie d'abord d'analyser les valeurs avec l'analyseur de flux rapide. S'il échoue, ClickHouse essaie d'utiliser l'analyseur complet pour les données, en le traitant comme un SQL [expression](#syntax-expressions).
-
-Les données peuvent avoir n'importe quel format. Lorsqu'une requête est reçue, le serveur calcule pas plus que [max_query_size](../operations/settings/settings.md#settings-max_query_size) octets de la requête en RAM (par défaut, 1 Mo), et le reste est analysé en flux.
-Il permet d'éviter les problèmes avec de grandes `INSERT` requête.
-
-Lors de l'utilisation de la `Values` format dans un `INSERT` de la requête, il peut sembler que les données sont analysées de même que les expressions dans un `SELECT` requête, mais ce n'est pas vrai. Le `Values` le format est beaucoup plus limitée.
-
-Le reste de cet article couvre l'analyseur complet. Pour plus d'informations sur les analyseurs de format, consultez [Format](../interfaces/formats.md) section.
-
-## Espace {#spaces}
-
-Il peut y avoir n'importe quel nombre de symboles d'espace entre les constructions syntaxiques (y compris le début et la fin d'une requête). Les symboles d'espace incluent l'espace, l'onglet, le saut de ligne, Le CR et le flux de formulaire.
-
-## Commentaire {#comments}
-
-ClickHouse prend en charge les commentaires de style SQL et de style C.
-Les commentaires de style SQL commencent par `--` et continuer jusqu'à la fin de la ligne, un espace après `--` peut être omis.
-C-style sont de `/*` de `*/`et peut être multiligne, les espaces ne sont pas requis non plus.
-
-## Mot {#syntax-keywords}
-
-Les mots clés sont insensibles à la casse lorsqu'ils correspondent à:
-
--   La norme SQL. Exemple, `SELECT`, `select` et `SeLeCt` sont toutes valides.
--   Implémentation dans certains SGBD populaires (MySQL ou Postgres). Exemple, `DateTime` est le même que `datetime`.
-
-Si le nom du type de données est sensible à la casse peut être vérifié `system.data_type_families` table.
-
-Contrairement à SQL standard, tous les autres mots clés (y compris les noms de fonctions) sont **sensible à la casse**.
-
-Mots-clés ne sont pas réservés; ils sont traités comme tels que dans le contexte correspondant. Si vous utilisez [identificateur](#syntax-identifiers) avec le même nom que les mots-clés, placez-les entre guillemets doubles ou backticks. Par exemple, la requête `SELECT "FROM" FROM table_name` est valide si la table `table_name` a colonne avec le nom de `"FROM"`.
-
-## Identificateur {#syntax-identifiers}
-
-Les identificateurs sont:
-
--   Noms de Cluster, de base de données, de table, de partition et de colonne.
--   Fonction.
--   Types de données.
--   [Expression des alias](#syntax-expression_aliases).
-
-Les identificateurs peuvent être cités ou non cités. Ce dernier est préféré.
-
-Non identificateurs doivent correspondre à l'expression régulière `^[a-zA-Z_][0-9a-zA-Z_]*$` et ne peut pas être égale à [mot](#syntax-keywords). Exemple: `x, _1, X_y__Z123_.`
-
-Si vous souhaitez utiliser les identifiants de la même manière que les mots-clés ou si vous souhaitez utiliser d'autres symboles dans les identifiants, citez-le en utilisant des guillemets doubles ou des backticks, par exemple, `"id"`, `` `id` ``.
-
-## Littéral {#literals}
-
-Il y a numérique, chaîne de caractères, composé, et `NULL` littéral.
-
-### Numérique {#numeric}
-
-Littéral numérique tente d'être analysé:
-
--   Tout d'abord, comme un nombre signé 64 bits, en utilisant le [strtoull](https://en.cppreference.com/w/cpp/string/byte/strtoul) fonction.
--   En cas d'échec, en tant que nombre non signé 64 bits, [strtoll](https://en.cppreference.com/w/cpp/string/byte/strtol) fonction.
--   En cas d'échec, en tant que nombre à virgule flottante [strtod](https://en.cppreference.com/w/cpp/string/byte/strtof) fonction.
--   Sinon, elle renvoie une erreur.
-
-La valeur littérale a le plus petit type dans lequel la valeur correspond.
-Par exemple, 1 est analysé comme `UInt8`, mais 256 est analysé comme `UInt16`. Pour plus d'informations, voir [Types de données](../sql-reference/data-types/index.md).
-
-Exemple: `1`, `18446744073709551615`, `0xDEADBEEF`, `01`, `0.1`, `1e100`, `-1e-100`, `inf`, `nan`.
-
-### Chaîne {#syntax-string-literal}
-
-Seuls les littéraux de chaîne entre guillemets simples sont pris en charge. Le clos de caractères barre oblique inverse échappé. Les séquences d'échappement suivantes ont une valeur spéciale correspondante: `\b`, `\f`, `\r`, `
`, `\t`, `\0`, `\a`, `\v`, `\xHH`. Dans tous les autres cas, des séquences d'échappement au format `\c`, où `c` est un caractère, sont convertis à `c`. Cela signifie que vous pouvez utiliser les séquences `\'`et`\\`. La valeur aurez l' [Chaîne](../sql-reference/data-types/string.md) type.
-
-Dans les littéraux de chaîne, vous devez vous échapper d'au moins `'` et `\`. Les guillemets simples peuvent être échappés avec le guillemet simple, littéraux `'It\'s'` et `'It''s'` sont égaux.
-
-### Composé {#compound}
-
-Les tableaux sont construits avec des crochets `[1, 2, 3]`. Nuples sont construits avec des supports ronds `(1, 'Hello, world!', 2)`.
-Techniquement, ce ne sont pas des littéraux, mais des expressions avec l'opérateur de création de tableau et l'opérateur de création de tuple, respectivement.
-Un tableau doit être composé d'au moins un élément, et un tuple doit avoir au moins deux éléments.
-Il y a un cas distinct lorsque les tuples apparaissent dans le `IN` clause de a `SELECT` requête. Les résultats de la requête peuvent inclure des tuples, mais les tuples ne peuvent pas être enregistrés dans une base de données (à l'exception des tables avec [Mémoire](../engines/table-engines/special/memory.md) moteur).
-
-### NULL {#null-literal}
-
-Indique que la valeur est manquante.
-
-Afin de stocker `NULL` dans un champ de table, il doit être de la [Nullable](../sql-reference/data-types/nullable.md) type.
-
-Selon le format de données (entrée ou sortie), `NULL` peut avoir une représentation différente. Pour plus d'informations, consultez la documentation de [formats de données](../interfaces/formats.md#formats).
-
-Il y a beaucoup de nuances au traitement `NULL`. Par exemple, si au moins l'un des arguments d'une opération de comparaison est `NULL` le résultat de cette opération est également `NULL`. Il en va de même pour la multiplication, l'addition et d'autres opérations. Pour plus d'informations, lisez la documentation pour chaque opération.
-
-Dans les requêtes, vous pouvez vérifier `NULL` à l'aide de la [IS NULL](operators/index.md#operator-is-null) et [IS NOT NULL](operators/index.md) opérateurs et les fonctions connexes `isNull` et `isNotNull`.
-
-## Fonction {#functions}
-
-Les appels de fonction sont écrits comme un identifiant avec une liste d'arguments (éventuellement vide) entre parenthèses. Contrairement à SQL standard, les crochets sont requis, même pour une liste d'arguments vide. Exemple: `now()`.
-Il existe des fonctions régulières et agrégées (voir la section “Aggregate functions”). Certaines fonctions d'agrégat peut contenir deux listes d'arguments entre parenthèses. Exemple: `quantile (0.9) (x)`. Ces fonctions d'agrégation sont appelés “parametric” fonctions, et les arguments dans la première liste sont appelés “parameters”. La syntaxe des fonctions d'agrégation sans paramètres est la même que pour les fonctions régulières.
-
-## Opérateur {#operators}
-
-Les opérateurs sont convertis en leurs fonctions correspondantes lors de l'analyse des requêtes, en tenant compte de leur priorité et de leur associativité.
-Par exemple, l'expression `1 + 2 * 3 + 4` est transformé à `plus(plus(1, multiply(2, 3)), 4)`.
-
-## Types de données et moteurs de Table de base de données {#data_types-and-database-table-engines}
-
-Types de données et moteurs de table dans `CREATE` les requêtes sont écrites de la même manière que les identifiants ou les fonctions. En d'autres termes, ils peuvent ou ne peuvent pas contenir une liste d'arguments entre parenthèses. Pour plus d'informations, voir les sections “Data types,” “Table engines,” et “CREATE”.
-
-## Expression Des Alias {#syntax-expression_aliases}
-
-Un alias est un nom défini par l'utilisateur pour l'expression dans une requête.
-
-``` sql
-expr AS alias
-```
-
--   `AS` — The keyword for defining aliases. You can define the alias for a table name or a column name in a `SELECT` clause sans utiliser le `AS` mot.
-
-        For example, `SELECT table_name_alias.column_name FROM table_name table_name_alias`.
-
-        In the [CAST](sql_reference/functions/type_conversion_functions.md#type_conversion_function-cast) function, the `AS` keyword has another meaning. See the description of the function.
-
--   `expr` — Any expression supported by ClickHouse.
-
-        For example, `SELECT column_name * 2 AS double FROM some_table`.
-
--   `alias` — Name for `expr`. Les alias doivent être conformes à la [identificateur](#syntax-identifiers) syntaxe.
-
-        For example, `SELECT "table t".column_name FROM table_name AS "table t"`.
-
-### Notes sur l'Utilisation de la {#notes-on-usage}
-
-Les alias sont globaux pour une requête ou d'une sous-requête, vous pouvez définir un alias dans n'importe quelle partie d'une requête de toute expression. Exemple, `SELECT (1 AS n) + 2, n`.
-
-Les alias ne sont pas visibles dans les sous-requêtes et entre les sous-requêtes. Par exemple, lors de l'exécution de la requête `SELECT (SELECT sum(b.a) + num FROM b) - a.a AS num FROM a` Clickhouse génère l'exception `Unknown identifier: num`.
-
-Si un alias est défini pour les colonnes de `SELECT` la clause d'une sous-requête, ces colonnes sont visibles dans la requête externe. Exemple, `SELECT n + m FROM (SELECT 1 AS n, 2 AS m)`.
-
-Soyez prudent avec les Alias qui sont les mêmes que les noms de colonnes ou de tables. Considérons l'exemple suivant:
-
-``` sql
-CREATE TABLE t
-(
-    a Int,
-    b Int
-)
-ENGINE = TinyLog()
-```
-
-``` sql
-SELECT
-    argMax(a, b),
-    sum(b) AS b
-FROM t
-```
-
-``` text
-Received exception from server (version 18.14.17):
-Code: 184. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: Aggregate function sum(b) is found inside another aggregate function in query.
-```
-
-Dans cet exemple, nous avons déclaré table `t` avec la colonne `b`. Ensuite, lors de la sélection des données, nous avons défini le `sum(b) AS b` alias. Comme les alias sont globaux, ClickHouse a substitué le littéral `b` dans l'expression `argMax(a, b)` avec l'expression `sum(b)`. Cette substitution a provoqué l'exception.
-
-## Astérisque {#asterisk}
-
-Dans un `SELECT` requête, un astérisque peut remplacer l'expression. Pour plus d'informations, consultez la section “SELECT”.
-
-## Expression {#syntax-expressions}
-
-Une expression est une fonction, un identifiant, un littéral, une application d'un opérateur, une expression entre parenthèses, une sous-requête ou un astérisque. Il peut également contenir un alias.
-Une liste des expressions est une ou plusieurs expressions séparées par des virgules.
-Les fonctions et les opérateurs, à leur tour, peuvent avoir des expressions comme arguments.
-
-[Article Original](https://clickhouse.tech/docs/en/sql_reference/syntax/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/file.md b/docs/fr/sql-reference/table-functions/file.md
deleted file mode 100644
index a58821d021d5..000000000000
--- a/docs/fr/sql-reference/table-functions/file.md
+++ /dev/null
@@ -1,121 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 37
-toc_title: fichier
----
-
-# fichier {#file}
-
-Crée un tableau à partir d'un fichier. Cette fonction de table est similaire à [URL](url.md) et [hdfs](hdfs.md) ceux.
-
-``` sql
-file(path, format, structure)
-```
-
-**Les paramètres d'entrée**
-
--   `path` — The relative path to the file from [user_files_path](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-user_files_path). Chemin d'accès à la prise en charge des fichiers suivant les globs en mode Lecture seule: `*`, `?`, `{abc,def}` et `{N..M}` où `N`, `M` — numbers, \``'abc', 'def'` — strings.
--   `format` — The [format](../../interfaces/formats.md#formats) de le fichier.
--   `structure` — Structure of the table. Format `'column1_name column1_type, column2_name column2_type, ...'`.
-
-**Valeur renvoyée**
-
-Une table avec la structure spécifiée pour lire ou écrire des données dans le fichier spécifié.
-
-**Exemple**
-
-Paramètre `user_files_path` et le contenu du fichier `test.csv`:
-
-``` bash
-$ grep user_files_path /etc/clickhouse-server/config.xml
-    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
-
-$ cat /var/lib/clickhouse/user_files/test.csv
-    1,2,3
-    3,2,1
-    78,43,45
-```
-
-Table de`test.csv` et la sélection des deux premières lignes de ce:
-
-``` sql
-SELECT *
-FROM file('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')
-LIMIT 2
-```
-
-``` text
-┌─column1─┬─column2─┬─column3─┐
-│       1 │       2 │       3 │
-│       3 │       2 │       1 │
-└─────────┴─────────┴─────────┘
-```
-
-``` sql
--- getting the first 10 lines of a table that contains 3 columns of UInt32 type from a CSV file
-SELECT * FROM file('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32') LIMIT 10
-```
-
-**Globs dans le chemin**
-
-Plusieurs composants de chemin peuvent avoir des globs. Pour être traité, le fichier doit exister et correspondre à l'ensemble du modèle de chemin (pas seulement le suffixe ou le préfixe).
-
--   `*` — Substitutes any number of any characters except `/` y compris la chaîne vide.
--   `?` — Substitutes any single character.
--   `{some_string,another_string,yet_another_one}` — Substitutes any of strings `'some_string', 'another_string', 'yet_another_one'`.
--   `{N..M}` — Substitutes any number in range from N to M including both borders.
-
-Les Constructions avec `{}` sont similaires à l' [fonction de table à distance](../../sql-reference/table-functions/remote.md)).
-
-**Exemple**
-
-1.  Supposons que nous ayons plusieurs fichiers avec les chemins relatifs suivants:
-
--   ‘some_dir/some_file_1’
--   ‘some_dir/some_file_2’
--   ‘some_dir/some_file_3’
--   ‘another_dir/some_file_1’
--   ‘another_dir/some_file_2’
--   ‘another_dir/some_file_3’
-
-1.  Interroger la quantité de lignes dans ces fichiers:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM file('{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32')
-```
-
-1.  Requête de la quantité de lignes dans tous les fichiers de ces deux répertoires:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM file('{some,another}_dir/*', 'TSV', 'name String, value UInt32')
-```
-
-!!! warning "Avertissement"
-    Si votre liste de fichiers contient des plages de nombres avec des zéros en tête, utilisez la construction avec des accolades pour chaque chiffre séparément ou utilisez `?`.
-
-**Exemple**
-
-Interroger les données des fichiers nommés `file000`, `file001`, … , `file999`:
-
-``` sql
-SELECT count(*)
-FROM file('big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32')
-```
-
-## Les Colonnes Virtuelles {#virtual-columns}
-
--   `_path` — Path to the file.
--   `_file` — Name of the file.
-
-**Voir Aussi**
-
--   [Les colonnes virtuelles](https://clickhouse.tech/docs/en/operations/table_engines/#table_engines-virtual_columns)
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/file/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/generate.md b/docs/fr/sql-reference/table-functions/generate.md
deleted file mode 100644
index 1f7eeddd0e10..000000000000
--- a/docs/fr/sql-reference/table-functions/generate.md
+++ /dev/null
@@ -1,44 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 47
-toc_title: generateRandom
----
-
-# generateRandom {#generaterandom}
-
-Génère des données aléatoires avec un schéma donné.
-Permet de remplir des tables de test avec des données.
-Prend en charge tous les types de données qui peuvent être stockés dans la table sauf `LowCardinality` et `AggregateFunction`.
-
-``` sql
-generateRandom('name TypeName[, name TypeName]...', [, 'random_seed'[, 'max_string_length'[, 'max_array_length']]]);
-```
-
-**Paramètre**
-
--   `name` — Name of corresponding column.
--   `TypeName` — Type of corresponding column.
--   `max_array_length` — Maximum array length for all generated arrays. Defaults to `10`.
--   `max_string_length` — Maximum string length for all generated strings. Defaults to `10`.
--   `random_seed` — Specify random seed manually to produce stable results. If NULL — seed is randomly generated.
-
-**Valeur Renvoyée**
-
-Un objet de table avec le schéma demandé.
-
-## Exemple D'Utilisation {#usage-example}
-
-``` sql
-SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)', 1, 10, 2) LIMIT 3;
-```
-
-``` text
-┌─a────────┬────────────d─┬─c──────────────────────────────────────────────────────────────────┐
-│ [77]     │ -124167.6723 │ ('2061-04-17 21:59:44.573','3f72f405-ec3e-13c8-44ca-66ef335f7835') │
-│ [32,110] │ -141397.7312 │ ('1979-02-09 03:43:48.526','982486d1-5a5d-a308-e525-7bd8b80ffa73') │
-│ [68]     │  -67417.0770 │ ('2080-03-12 14:17:31.269','110425e5-413f-10a6-05ba-fa6b3e929f15') │
-└──────────┴──────────────┴────────────────────────────────────────────────────────────────────┘
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/generate/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/hdfs.md b/docs/fr/sql-reference/table-functions/hdfs.md
deleted file mode 100644
index 51b742d8018f..000000000000
--- a/docs/fr/sql-reference/table-functions/hdfs.md
+++ /dev/null
@@ -1,104 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 45
-toc_title: hdfs
----
-
-# hdfs {#hdfs}
-
-Crée une table à partir de fichiers dans HDFS. Cette fonction de table est similaire à [URL](url.md) et [fichier](file.md) ceux.
-
-``` sql
-hdfs(URI, format, structure)
-```
-
-**Les paramètres d'entrée**
-
--   `URI` — The relative URI to the file in HDFS. Path to file support following globs in readonly mode: `*`, `?`, `{abc,def}` et `{N..M}` où `N`, `M` — numbers, \``'abc', 'def'` — strings.
--   `format` — The [format](../../interfaces/formats.md#formats) de le fichier.
--   `structure` — Structure of the table. Format `'column1_name column1_type, column2_name column2_type, ...'`.
-
-**Valeur renvoyée**
-
-Une table avec la structure spécifiée pour lire ou écrire des données dans le fichier spécifié.
-
-**Exemple**
-
-Table de `hdfs://hdfs1:9000/test` et la sélection des deux premières lignes de ce:
-
-``` sql
-SELECT *
-FROM hdfs('hdfs://hdfs1:9000/test', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32')
-LIMIT 2
-```
-
-``` text
-┌─column1─┬─column2─┬─column3─┐
-│       1 │       2 │       3 │
-│       3 │       2 │       1 │
-└─────────┴─────────┴─────────┘
-```
-
-**Globs dans le chemin**
-
-Plusieurs composants de chemin peuvent avoir des globs. Pour être traité, le fichier doit exister et correspondre à l'ensemble du modèle de chemin (pas seulement le suffixe ou le préfixe).
-
--   `*` — Substitutes any number of any characters except `/` y compris la chaîne vide.
--   `?` — Substitutes any single character.
--   `{some_string,another_string,yet_another_one}` — Substitutes any of strings `'some_string', 'another_string', 'yet_another_one'`.
--   `{N..M}` — Substitutes any number in range from N to M including both borders.
-
-Les Constructions avec `{}` sont similaires à l' [fonction de table à distance](../../sql-reference/table-functions/remote.md)).
-
-**Exemple**
-
-1.  Supposons que nous ayons plusieurs fichiers avec les URI suivants sur HDFS:
-
--   ‘hdfs://hdfs1:9000/some_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/some_dir/some_file_3’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_1’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_2’
--   ‘hdfs://hdfs1:9000/another_dir/some_file_3’
-
-1.  Interroger la quantité de lignes dans ces fichiers:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM hdfs('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32')
-```
-
-1.  Requête de la quantité de lignes dans tous les fichiers de ces deux répertoires:
-
-<!-- -->
-
-``` sql
-SELECT count(*)
-FROM hdfs('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV', 'name String, value UInt32')
-```
-
-!!! warning "Avertissement"
-    Si votre liste de fichiers contient des plages de nombres avec des zéros en tête, utilisez la construction avec des accolades pour chaque chiffre séparément ou utilisez `?`.
-
-**Exemple**
-
-Interroger les données des fichiers nommés `file000`, `file001`, … , `file999`:
-
-``` sql
-SELECT count(*)
-FROM hdfs('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32')
-```
-
-## Les Colonnes Virtuelles {#virtual-columns}
-
--   `_path` — Path to the file.
--   `_file` — Name of the file.
-
-**Voir Aussi**
-
--   [Les colonnes virtuelles](https://clickhouse.tech/docs/en/operations/table_engines/#table_engines-virtual_columns)
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/hdfs/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/index.md b/docs/fr/sql-reference/table-functions/index.md
deleted file mode 100644
index 89a8200e385d..000000000000
--- a/docs/fr/sql-reference/table-functions/index.md
+++ /dev/null
@@ -1,38 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Les Fonctions De Table
-toc_priority: 34
-toc_title: Introduction
----
-
-# Les Fonctions De Table {#table-functions}
-
-Les fonctions de Table sont des méthodes pour construire des tables.
-
-Vous pouvez utiliser les fonctions de table dans:
-
--   [FROM](../statements/select/from.md) la clause de la `SELECT` requête.
-
-        The method for creating a temporary table that is available only in the current query. The table is deleted when the query finishes.
-
--   [Créer une TABLE en tant que \< table_function ()\>](../statements/create.md#create-table-query) requête.
-
-        It's one of the methods of creating a table.
-
-!!! warning "Avertissement"
-    Vous ne pouvez pas utiliser les fonctions de table si [allow_ddl](../../operations/settings/permissions-for-queries.md#settings_allow_ddl) paramètre est désactivé.
-
-| Fonction              | Description                                                                                                                         |
-|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------|
-| [fichier](file.md)    | Crée un [Fichier](../../engines/table-engines/special/file.md)-moteur de table.                                                     |
-| [fusionner](merge.md) | Crée un [Fusionner](../../engines/table-engines/special/merge.md)-moteur de table.                                                  |
-| [nombre](numbers.md)  | Crée une table avec une seule colonne remplie de nombres entiers.                                                                   |
-| [distant](remote.md)  | Vous permet d'accéder à des serveurs distants sans [Distribué](../../engines/table-engines/special/distributed.md)-moteur de table. |
-| [URL](url.md)         | Crée un [URL](../../engines/table-engines/special/url.md)-moteur de table.                                                          |
-| [mysql](mysql.md)     | Crée un [MySQL](../../engines/table-engines/integrations/mysql.md)-moteur de table.                                                 |
-| [jdbc](jdbc.md)       | Crée un [JDBC](../../engines/table-engines/integrations/jdbc.md)-moteur de table.                                                   |
-| [ODBC](odbc.md)       | Crée un [ODBC](../../engines/table-engines/integrations/odbc.md)-moteur de table.                                                   |
-| [hdfs](hdfs.md)       | Crée un [HDFS](../../engines/table-engines/integrations/hdfs.md)-moteur de table.                                                   |
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/input.md b/docs/fr/sql-reference/table-functions/input.md
deleted file mode 100644
index 21e0eacb5c19..000000000000
--- a/docs/fr/sql-reference/table-functions/input.md
+++ /dev/null
@@ -1,47 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 46
-toc_title: "entr\xE9e"
----
-
-# entrée {#input}
-
-`input(structure)` - fonction de table qui permet effectivement convertir et insérer des données envoyées à la
-serveur avec une structure donnée à la table avec une autre structure.
-
-`structure` - structure de données envoyées au serveur dans le format suivant `'column1_name column1_type, column2_name column2_type, ...'`.
-Exemple, `'id UInt32, name String'`.
-
-Cette fonction peut être utilisée uniquement dans `INSERT SELECT` requête et une seule fois mais se comporte autrement comme une fonction de table ordinaire
-(par exemple, il peut être utilisé dans la sous-requête, etc.).
-
-Les données peuvent être envoyées de quelque manière que ce soit comme pour ordinaire `INSERT` requête et passé dans tout disponible [format](../../interfaces/formats.md#formats)
-qui doit être spécifié à la fin de la requête (contrairement à l'ordinaire `INSERT SELECT`).
-
-La caractéristique principale de cette fonction est que lorsque le serveur reçoit des données du client il les convertit simultanément
-selon la liste des expressions dans le `SELECT` clause et insère dans la table cible. Table temporaire
-avec toutes les données transférées n'est pas créé.
-
-**Exemple**
-
--   Laissez le `test` le tableau a la structure suivante `(a String, b String)`
-    et les données `data.csv` a une structure différente `(col1 String, col2 Date, col3 Int32)`. Requête pour insérer
-    les données de l' `data.csv` dans le `test` table avec conversion simultanée ressemble à ceci:
-
-<!-- -->
-
-``` bash
-$ cat data.csv | clickhouse-client --query="INSERT INTO test SELECT lower(col1), col3 * col3 FROM input('col1 String, col2 Date, col3 Int32') FORMAT CSV";
-```
-
--   Si `data.csv` contient les données de la même structure `test_structure` comme la table `test` puis ces deux requêtes sont égales:
-
-<!-- -->
-
-``` bash
-$ cat data.csv | clickhouse-client --query="INSERT INTO test FORMAT CSV"
-$ cat data.csv | clickhouse-client --query="INSERT INTO test SELECT * FROM input('test_structure') FORMAT CSV"
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/input/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/jdbc.md b/docs/fr/sql-reference/table-functions/jdbc.md
deleted file mode 100644
index 76dea0e0930d..000000000000
--- a/docs/fr/sql-reference/table-functions/jdbc.md
+++ /dev/null
@@ -1,29 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 43
-toc_title: jdbc
----
-
-# jdbc {#table-function-jdbc}
-
-`jdbc(jdbc_connection_uri, schema, table)` - retourne la table qui est connectée via le pilote JDBC.
-
-Ce tableau fonction nécessite séparé `clickhouse-jdbc-bridge` programme en cours d'exécution.
-Il prend en charge les types Nullable (basé sur DDL de la table distante qui est interrogée).
-
-**Exemple**
-
-``` sql
-SELECT * FROM jdbc('jdbc:mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
-```
-
-``` sql
-SELECT * FROM jdbc('mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
-```
-
-``` sql
-SELECT * FROM jdbc('datasource://mysql-local', 'schema', 'table')
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/jdbc/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/merge.md b/docs/fr/sql-reference/table-functions/merge.md
deleted file mode 100644
index 1ec264b06bd0..000000000000
--- a/docs/fr/sql-reference/table-functions/merge.md
+++ /dev/null
@@ -1,14 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 38
-toc_title: fusionner
----
-
-# fusionner {#merge}
-
-`merge(db_name, 'tables_regexp')` – Creates a temporary Merge table. For more information, see the section “Table engines, Merge”.
-
-La structure de la table est tirée de la première table rencontrée qui correspond à l'expression régulière.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/merge/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/mysql.md b/docs/fr/sql-reference/table-functions/mysql.md
deleted file mode 100644
index 295456914f0f..000000000000
--- a/docs/fr/sql-reference/table-functions/mysql.md
+++ /dev/null
@@ -1,86 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 42
-toc_title: mysql
----
-
-# mysql {#mysql}
-
-Permettre `SELECT` requêtes à effectuer sur des données stockées sur un serveur MySQL distant.
-
-``` sql
-mysql('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']);
-```
-
-**Paramètre**
-
--   `host:port` — MySQL server address.
-
--   `database` — Remote database name.
-
--   `table` — Remote table name.
-
--   `user` — MySQL user.
-
--   `password` — User password.
-
--   `replace_query` — Flag that converts `INSERT INTO` les requêtes de `REPLACE INTO`. Si `replace_query=1` la requête est remplacé.
-
--   `on_duplicate_clause` — The `ON DUPLICATE KEY on_duplicate_clause` expression qui est ajoutée à la `INSERT` requête.
-
-        Example: `INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1`, where `on_duplicate_clause` is `UPDATE c2 = c2 + 1`. See the MySQL documentation to find which `on_duplicate_clause` you can use with the `ON DUPLICATE KEY` clause.
-
-        To specify `on_duplicate_clause` you need to pass `0` to the `replace_query` parameter. If you simultaneously pass `replace_query = 1` and `on_duplicate_clause`, ClickHouse generates an exception.
-
-Simple `WHERE` des clauses telles que `=, !=, >, >=, <, <=` sont actuellement exécutés sur le serveur MySQL.
-
-Le reste des conditions et le `LIMIT` les contraintes d'échantillonnage sont exécutées dans ClickHouse uniquement après la fin de la requête à MySQL.
-
-**Valeur Renvoyée**
-
-Un objet table avec les mêmes colonnes que la table MySQL d'origine.
-
-## Exemple D'Utilisation {#usage-example}
-
-Table dans MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Sélection des données de ClickHouse:
-
-``` sql
-SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123')
-```
-
-``` text
-┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐
-│      1 │         ᴺᵁᴸᴸ │     2 │           ᴺᵁᴸᴸ │
-└────────┴──────────────┴───────┴────────────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [Le ‘MySQL’ tableau moteur](../../engines/table-engines/integrations/mysql.md)
--   [Utilisation de MySQL comme source de dictionnaire externe](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-mysql)
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/mysql/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/numbers.md b/docs/fr/sql-reference/table-functions/numbers.md
deleted file mode 100644
index 50a5ad610020..000000000000
--- a/docs/fr/sql-reference/table-functions/numbers.md
+++ /dev/null
@@ -1,30 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 39
-toc_title: nombre
----
-
-# nombre {#numbers}
-
-`numbers(N)` – Returns a table with the single ‘number’ colonne (UInt64) qui contient des entiers de 0 à n-1.
-`numbers(N, M)` - Retourne un tableau avec le seul ‘number’ colonne (UInt64) qui contient des entiers de N À (N + M-1).
-
-Similaire à la `system.numbers` table, il peut être utilisé pour tester et générer des valeurs successives, `numbers(N, M)` plus efficace que `system.numbers`.
-
-Les requêtes suivantes sont équivalentes:
-
-``` sql
-SELECT * FROM numbers(10);
-SELECT * FROM numbers(0, 10);
-SELECT * FROM system.numbers LIMIT 10;
-```
-
-Exemple:
-
-``` sql
--- Generate a sequence of dates from 2010-01-01 to 2010-12-31
-select toDate('2010-01-01') + number as d FROM numbers(365);
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/numbers/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/odbc.md b/docs/fr/sql-reference/table-functions/odbc.md
deleted file mode 100644
index aae636a5eb21..000000000000
--- a/docs/fr/sql-reference/table-functions/odbc.md
+++ /dev/null
@@ -1,108 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 44
-toc_title: ODBC
----
-
-# ODBC {#table-functions-odbc}
-
-Renvoie la table connectée via [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity).
-
-``` sql
-odbc(connection_settings, external_database, external_table)
-```
-
-Paramètre:
-
--   `connection_settings` — Name of the section with connection settings in the `odbc.ini` fichier.
--   `external_database` — Name of a database in an external DBMS.
--   `external_table` — Name of a table in the `external_database`.
-
-Pour implémenter en toute sécurité les connexions ODBC, ClickHouse utilise un programme distinct `clickhouse-odbc-bridge`. Si le pilote ODBC est chargé directement depuis `clickhouse-server`, les problèmes de pilote peuvent planter le serveur ClickHouse. Clickhouse démarre automatiquement `clickhouse-odbc-bridge` lorsque cela est nécessaire. Le programme ODBC bridge est installé à partir du même package que `clickhouse-server`.
-
-Les champs avec l' `NULL` les valeurs de la table externe sont converties en valeurs par défaut pour le type de données de base. Par exemple, si un champ de table MySQL distant a `INT NULL` type il est converti en 0 (la valeur par défaut pour ClickHouse `Int32` type de données).
-
-## Exemple D'Utilisation {#usage-example}
-
-**Obtenir des données de L'installation MySQL locale via ODBC**
-
-Cet exemple est vérifié pour Ubuntu Linux 18.04 et MySQL server 5.7.
-
-Assurez-vous que unixODBC et MySQL Connector sont installés.
-
-Par défaut (si installé à partir de paquets), ClickHouse démarre en tant qu'utilisateur `clickhouse`. Ainsi, vous devez créer et configurer cet utilisateur dans le serveur MySQL.
-
-``` bash
-$ sudo mysql
-```
-
-``` sql
-mysql> CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse';
-mysql> GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;
-```
-
-Puis configurez la connexion dans `/etc/odbc.ini`.
-
-``` bash
-$ cat /etc/odbc.ini
-[mysqlconn]
-DRIVER = /usr/local/lib/libmyodbc5w.so
-SERVER = 127.0.0.1
-PORT = 3306
-DATABASE = test
-USERNAME = clickhouse
-PASSWORD = clickhouse
-```
-
-Vous pouvez vérifier la connexion en utilisant le `isql` utilitaire de l'installation unixODBC.
-
-``` bash
-$ isql -v mysqlconn
-+-------------------------+
-| Connected!                            |
-|                                       |
-...
-```
-
-Table dans MySQL:
-
-``` text
-mysql> CREATE TABLE `test`.`test` (
-    ->   `int_id` INT NOT NULL AUTO_INCREMENT,
-    ->   `int_nullable` INT NULL DEFAULT NULL,
-    ->   `float` FLOAT NOT NULL,
-    ->   `float_nullable` FLOAT NULL DEFAULT NULL,
-    ->   PRIMARY KEY (`int_id`));
-Query OK, 0 rows affected (0,09 sec)
-
-mysql> insert into test (`int_id`, `float`) VALUES (1,2);
-Query OK, 1 row affected (0,00 sec)
-
-mysql> select * from test;
-+------+----------+-----+----------+
-| int_id | int_nullable | float | float_nullable |
-+------+----------+-----+----------+
-|      1 |         NULL |     2 |           NULL |
-+------+----------+-----+----------+
-1 row in set (0,00 sec)
-```
-
-Récupération des données de la table MySQL dans ClickHouse:
-
-``` sql
-SELECT * FROM odbc('DSN=mysqlconn', 'test', 'test')
-```
-
-``` text
-┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐
-│      1 │            0 │     2 │              0 │
-└────────┴──────────────┴───────┴────────────────┘
-```
-
-## Voir Aussi {#see-also}
-
--   [Dictionnaires externes ODBC](../../sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources.md#dicts-external_dicts_dict_sources-odbc)
--   [Moteur de table ODBC](../../engines/table-engines/integrations/odbc.md).
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/jdbc/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/remote.md b/docs/fr/sql-reference/table-functions/remote.md
deleted file mode 100644
index 380a9986116b..000000000000
--- a/docs/fr/sql-reference/table-functions/remote.md
+++ /dev/null
@@ -1,85 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 40
-toc_title: distant
----
-
-# à distance, remoteSecure {#remote-remotesecure}
-
-Vous permet d'accéder à des serveurs distants sans `Distributed` table.
-
-Signature:
-
-``` sql
-remote('addresses_expr', db, table[, 'user'[, 'password']])
-remote('addresses_expr', db.table[, 'user'[, 'password']])
-remoteSecure('addresses_expr', db, table[, 'user'[, 'password']])
-remoteSecure('addresses_expr', db.table[, 'user'[, 'password']])
-```
-
-`addresses_expr` – An expression that generates addresses of remote servers. This may be just one server address. The server address is `host:port` ou juste `host`. L'hôte peut être spécifié comme nom de serveur ou l'adresse IPv4 ou IPv6. Une adresse IPv6 est indiquée entre crochets. Le port est le port TCP sur le serveur distant. Si le port est omis, il utilise `tcp_port` à partir du fichier de configuration du serveur (par défaut, 9000).
-
-!!! important "Important"
-    Le port est requis pour une adresse IPv6.
-
-Exemple:
-
-``` text
-example01-01-1
-example01-01-1:9000
-localhost
-127.0.0.1
-[::]:9000
-[2a02:6b8:0:1111::11]:9000
-```
-
-Plusieurs adresses séparées par des virgules. Dans ce cas, ClickHouse utilisera le traitement distribué, donc il enverra la requête à toutes les adresses spécifiées (comme les fragments avec des données différentes).
-
-Exemple:
-
-``` text
-example01-01-1,example01-02-1
-```
-
-Une partie de l'expression peut être spécifiée entre crochets. L'exemple précédent peut être écrite comme suit:
-
-``` text
-example01-0{1,2}-1
-```
-
-Les accolades peuvent contenir une plage de Nombres séparés par deux points (entiers non négatifs). Dans ce cas, la gamme est étendue à un ensemble de valeurs qui génèrent fragment d'adresses. Si le premier nombre commence par zéro, les valeurs sont formées avec le même alignement zéro. L'exemple précédent peut être écrite comme suit:
-
-``` text
-example01-{01..02}-1
-```
-
-Si vous avez plusieurs paires d'accolades, il génère le produit direct des ensembles correspondants.
-
-Les adresses et les parties d'adresses entre crochets peuvent être séparées par le symbole de tuyau (\|). Dans ce cas, les ensembles correspondants de adresses sont interprétés comme des répliques, et la requête sera envoyée à la première sain réplique. Cependant, les répliques sont itérées dans l'ordre actuellement défini dans [équilibrage](../../operations/settings/settings.md) paramètre.
-
-Exemple:
-
-``` text
-example01-{01..02}-{1|2}
-```
-
-Cet exemple spécifie deux fragments qui ont chacun deux répliques.
-
-Le nombre d'adresses générées est limitée par une constante. En ce moment, c'est 1000 adresses.
-
-À l'aide de la `remote` la fonction de table est moins optimale que la création d'un `Distributed` table, car dans ce cas, la connexion au serveur est rétablie pour chaque requête. En outre, si des noms d'hôte, les noms sont résolus, et les erreurs ne sont pas comptés lors de travail avec diverses répliques. Lors du traitement d'un grand nombre de requêtes, créez toujours `Distributed` table à l'avance, et ne pas utiliser la `remote` table de fonction.
-
-Le `remote` table de fonction peut être utile dans les cas suivants:
-
--   Accès à un serveur spécifique pour la comparaison de données, le débogage et les tests.
--   Requêtes entre différents clusters ClickHouse à des fins de recherche.
--   Demandes distribuées peu fréquentes qui sont faites manuellement.
--   Distribué demandes où l'ensemble des serveurs est redéfinie à chaque fois.
-
-Si l'utilisateur n'est pas spécifié, `default` est utilisée.
-Si le mot de passe n'est spécifié, un mot de passe vide est utilisé.
-
-`remoteSecure` - la même chose que `remote` but with secured connection. Default port — [tcp_port_secure](../../operations/server-configuration-parameters/settings.md#server_configuration_parameters-tcp_port_secure) de config ou 9440.
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/remote/) <!--hide-->
diff --git a/docs/fr/sql-reference/table-functions/url.md b/docs/fr/sql-reference/table-functions/url.md
deleted file mode 100644
index 1df5cf555268..000000000000
--- a/docs/fr/sql-reference/table-functions/url.md
+++ /dev/null
@@ -1,26 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 41
-toc_title: URL
----
-
-# URL {#url}
-
-`url(URL, format, structure)` - retourne une table créée à partir du `URL` avec le
-`format` et `structure`.
-
-URL-adresse du serveur HTTP ou HTTPS, qui peut accepter `GET` et/ou `POST` demande.
-
-format - [format](../../interfaces/formats.md#formats) des données.
-
-structure - structure de table dans `'UserID UInt64, Name String'` format. Détermine les noms et les types de colonnes.
-
-**Exemple**
-
-``` sql
--- getting the first 3 lines of a table that contains columns of String and UInt32 type from HTTP-server which answers in CSV format.
-SELECT * FROM url('http://127.0.0.1:12345/', CSV, 'column1 String, column2 UInt32') LIMIT 3
-```
-
-[Article Original](https://clickhouse.tech/docs/en/query_language/table_functions/url/) <!--hide-->
diff --git a/docs/fr/whats-new/changelog/2017.md b/docs/fr/whats-new/changelog/2017.md
deleted file mode 120000
index d581cbbb4222..000000000000
--- a/docs/fr/whats-new/changelog/2017.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/2017.md
\ No newline at end of file
diff --git a/docs/fr/whats-new/changelog/2018.md b/docs/fr/whats-new/changelog/2018.md
deleted file mode 120000
index 22874fcae857..000000000000
--- a/docs/fr/whats-new/changelog/2018.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/2018.md
\ No newline at end of file
diff --git a/docs/fr/whats-new/changelog/2019.md b/docs/fr/whats-new/changelog/2019.md
deleted file mode 120000
index 0f3f095f8a16..000000000000
--- a/docs/fr/whats-new/changelog/2019.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/2019.md
\ No newline at end of file
diff --git a/docs/fr/whats-new/changelog/index.md b/docs/fr/whats-new/changelog/index.md
deleted file mode 120000
index 5461b93ec8c1..000000000000
--- a/docs/fr/whats-new/changelog/index.md
+++ /dev/null
@@ -1,1 +0,0 @@
-../../../en/whats-new/changelog/index.md
\ No newline at end of file
diff --git a/docs/fr/whats-new/index.md b/docs/fr/whats-new/index.md
deleted file mode 100644
index 51a77da8ef44..000000000000
--- a/docs/fr/whats-new/index.md
+++ /dev/null
@@ -1,8 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_folder_title: Ce qui est Nouveau
-toc_priority: 72
----
-
-
diff --git a/docs/fr/whats-new/roadmap.md b/docs/fr/whats-new/roadmap.md
deleted file mode 100644
index 87d64208f67e..000000000000
--- a/docs/fr/whats-new/roadmap.md
+++ /dev/null
@@ -1,19 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 74
-toc_title: Feuille de route
----
-
-# Feuille de route {#roadmap}
-
-## Q1 2020 {#q1-2020}
-
--   Contrôle d'accès par rôle
-
-## Q2 2020 {#q2-2020}
-
--   Intégration avec les services d'authentification externes
--   Pools de ressources pour une répartition plus précise de la capacité du cluster entre les utilisateurs
-
-{## [Article Original](https://clickhouse.tech/docs/en/roadmap/) ##}
diff --git a/docs/fr/whats-new/security-changelog.md b/docs/fr/whats-new/security-changelog.md
deleted file mode 100644
index 6046ef96bb2c..000000000000
--- a/docs/fr/whats-new/security-changelog.md
+++ /dev/null
@@ -1,76 +0,0 @@
----
-machine_translated: true
-machine_translated_rev: 72537a2d527c63c07aa5d2361a8829f3895cf2bd
-toc_priority: 76
-toc_title: "S\xE9curit\xE9 Changelog"
----
-
-## Correction dans la version 19.14.3.3 de ClickHouse, 2019-09-10 {#fixed-in-clickhouse-release-19-14-3-3-2019-09-10}
-
-### CVE-2019-15024 {#cve-2019-15024}
-
-Аn attacker that has write access to ZooKeeper and who ican run a custom server available from the network where ClickHouse runs, can create a custom-built malicious server that will act as a ClickHouse replica and register it in ZooKeeper. When another replica will fetch data part from the malicious replica, it can force clickhouse-server to write to arbitrary path on filesystem.
-
-Crédits: Eldar Zaitov de L'équipe de sécurité de L'Information Yandex
-
-### CVE-2019-16535 {#cve-2019-16535}
-
-Аn OOB read, OOB write and integer underflow in decompression algorithms can be used to achieve RCE or DoS via native protocol.
-
-Crédits: Eldar Zaitov de L'équipe de sécurité de L'Information Yandex
-
-### CVE-2019-16536 {#cve-2019-16536}
-
-Le débordement de pile menant à DoS peut être déclenché par un client authentifié malveillant.
-
-Crédits: Eldar Zaitov de L'équipe de sécurité de L'Information Yandex
-
-## Correction de la version 19.13.6.1 de ClickHouse, 2019-09-20 {#fixed-in-clickhouse-release-19-13-6-1-2019-09-20}
-
-### CVE-2019-18657 {#cve-2019-18657}
-
-Fonction de Table `url` la vulnérabilité avait-elle permis à l'attaquant d'injecter des en-têtes HTTP arbitraires dans la requête.
-
-Crédit: [Nikita Tikhomirov](https://github.com/NSTikhomirov)
-
-## Correction dans la version ClickHouse 18.12.13, 2018-09-10 {#fixed-in-clickhouse-release-18-12-13-2018-09-10}
-
-### CVE-2018-14672 {#cve-2018-14672}
-
-Les fonctions de chargement des modèles CatBoost permettaient de parcourir les chemins et de lire des fichiers arbitraires via des messages d'erreur.
-
-Crédits: Andrey Krasichkov de L'équipe de sécurité de L'Information Yandex
-
-## Correction dans la version 18.10.3 de ClickHouse, 2018-08-13 {#fixed-in-clickhouse-release-18-10-3-2018-08-13}
-
-### CVE-2018-14671 {#cve-2018-14671}
-
-unixODBC a permis de charger des objets partagés arbitraires à partir du système de fichiers, ce qui a conduit à une vulnérabilité D'exécution de Code À Distance.
-
-Crédits: Andrey Krasichkov et Evgeny Sidorov de Yandex Information Security Team
-
-## Correction dans la version 1.1.54388 de ClickHouse, 2018-06-28 {#fixed-in-clickhouse-release-1-1-54388-2018-06-28}
-
-### CVE-2018-14668 {#cve-2018-14668}
-
-“remote” la fonction de table a permis des symboles arbitraires dans “user”, “password” et “default_database” champs qui ont conduit à des attaques de falsification de requêtes inter-protocoles.
-
-Crédits: Andrey Krasichkov de L'équipe de sécurité de L'Information Yandex
-
-## Correction dans la version 1.1.54390 de ClickHouse, 2018-07-06 {#fixed-in-clickhouse-release-1-1-54390-2018-07-06}
-
-### CVE-2018-14669 {#cve-2018-14669}
-
-Clickhouse client MySQL avait “LOAD DATA LOCAL INFILE” fonctionnalité activée permettant à une base de données MySQL malveillante de lire des fichiers arbitraires à partir du serveur clickhouse connecté.
-
-Crédits: Andrey Krasichkov et Evgeny Sidorov de Yandex Information Security Team
-
-## Correction dans la version 1.1.54131 de ClickHouse, 2017-01-10 {#fixed-in-clickhouse-release-1-1-54131-2017-01-10}
-
-### CVE-2018-14670 {#cve-2018-14670}
-
-Configuration incorrecte dans le paquet deb pourrait conduire à l'utilisation non autorisée de la base de données.
-
-Crédits: National Cyber Security Centre (NCSC)
-
-{## [Article Original](https://clickhouse.tech/docs/en/security_changelog/) ##}
diff --git a/docs/tools/build.py b/docs/tools/build.py
index dfb9661c3263..5a1f10268ab8 100755
--- a/docs/tools/build.py
+++ b/docs/tools/build.py
@@ -65,8 +65,6 @@ def build_for_lang(lang, args):
         languages = {
             'en': 'English',
             'zh': '中文',
-            'es': 'Español',
-            'fr': 'Français',
             'ru': 'Русский',
             'ja': '日本語'
         }
@@ -74,8 +72,6 @@ def build_for_lang(lang, args):
         site_names = {
             'en': 'ClickHouse %s Documentation',
             'zh': 'ClickHouse文档 %s',
-            'es': 'Documentación de ClickHouse %s',
-            'fr': 'Documentation ClickHouse %s',
             'ru': 'Документация ClickHouse %s',
             'ja': 'ClickHouseドキュメント %s'
         }
@@ -183,7 +179,7 @@ def build(args):
     website_dir = os.path.join(src_dir, 'website')
 
     arg_parser = argparse.ArgumentParser()
-    arg_parser.add_argument('--lang', default='en,es,fr,ru,zh,ja')
+    arg_parser.add_argument('--lang', default='en,ru,zh,ja')
     arg_parser.add_argument('--blog-lang', default='en,ru')
     arg_parser.add_argument('--docs-dir', default='.')
     arg_parser.add_argument('--theme-dir', default=website_dir)
diff --git a/docs/tools/make_links.sh b/docs/tools/make_links.sh
index c1194901f8fb..801086178bfa 100755
--- a/docs/tools/make_links.sh
+++ b/docs/tools/make_links.sh
@@ -8,7 +8,7 @@ BASE_DIR=$(dirname $(readlink -f $0))
 function do_make_links()
 {
     set -x
-    langs=(en es zh fr ru ja)
+    langs=(en zh ru ja)
     src_file="$1"
     for lang in "${langs[@]}"
     do
diff --git a/website/locale/es/LC_MESSAGES/messages.mo b/website/locale/es/LC_MESSAGES/messages.mo
deleted file mode 100644
index 888d7a76c4eb..000000000000
Binary files a/website/locale/es/LC_MESSAGES/messages.mo and /dev/null differ
diff --git a/website/locale/es/LC_MESSAGES/messages.po b/website/locale/es/LC_MESSAGES/messages.po
deleted file mode 100644
index 531a0001c526..000000000000
--- a/website/locale/es/LC_MESSAGES/messages.po
+++ /dev/null
@@ -1,326 +0,0 @@
-# Translations template for PROJECT.
-# Copyright (C) 2020 ORGANIZATION
-# This file is distributed under the same license as the PROJECT project.
-# Automatically generated, 2020.
-#
-msgid ""
-msgstr ""
-"Project-Id-Version: PROJECT VERSION
"
-"Report-Msgid-Bugs-To: EMAIL@ADDRESS
"
-"POT-Creation-Date: 2020-06-17 12:20+0300
"
-"PO-Revision-Date: 2020-06-17 12:20+0300
"
-"Last-Translator: Automatically generated
"
-"Language-Team: none
"
-"MIME-Version: 1.0
"
-"Content-Type: text/plain; charset=UTF-8
"
-"Content-Transfer-Encoding: 8bit
"
-"Generated-By: Babel 2.8.0
"
-"Language: es
"
-"Plural-Forms: nplurals=2; plural=(n != 1);
"
-
-#: templates/common_meta.html:1
-msgid ""
-"ClickHouse is a fast open-source column-oriented database management system "
-"that allows generating analytical data reports in real-time using SQL queries"
-msgstr ""
-
-#: templates/common_meta.html:6
-msgid "ClickHouse - fast open-source OLAP DBMS"
-msgstr ""
-
-#: templates/common_meta.html:10
-msgid "ClickHouse DBMS"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "open-source"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "relational"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "analytics"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "analytical"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "Big Data"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "web-analytics"
-msgstr ""
-
-#: templates/footer.html:8
-msgid "ClickHouse source code is published under the Apache 2.0 License."
-msgstr ""
-
-#: templates/footer.html:8
-msgid ""
-"Software is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR "
-"CONDITIONS OF ANY KIND, either express or implied."
-msgstr ""
-
-#: templates/footer.html:11
-msgid "Yandex LLC"
-msgstr ""
-
-#: templates/blog/content.html:20 templates/blog/content.html:25
-#: templates/blog/content.html:30
-msgid "Share on"
-msgstr ""
-
-#: templates/blog/content.html:37
-msgid "Published date"
-msgstr ""
-
-#: templates/blog/nav.html:20
-msgid "New&nbsp;post"
-msgstr ""
-
-#: templates/blog/nav.html:25
-msgid "Documentation"
-msgstr ""
-
-#: templates/docs/footer.html:3
-msgid "Rating"
-msgstr ""
-
-#: templates/docs/footer.html:3
-msgid "votes"
-msgstr ""
-
-#: templates/docs/footer.html:4
-msgid "Article Rating"
-msgstr ""
-
-#: templates/docs/footer.html:4
-msgid "Was this content helpful?"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Unusable"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Poor"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Good"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Excellent"
-msgstr ""
-
-#: templates/docs/footer.html:8
-msgid "documentation"
-msgstr ""
-
-#: templates/docs/footer.html:15
-msgid "Built from"
-msgstr ""
-
-#: templates/docs/footer.html:15
-msgid "published on"
-msgstr ""
-
-#: templates/docs/footer.html:15
-msgid "modified on"
-msgstr ""
-
-#: templates/docs/machine-translated.html:3
-msgid "Help wanted!"
-msgstr ""
-
-#: templates/docs/machine-translated.html:4
-msgid ""
-"The following content of this documentation page has been machine-"
-"translated. But unlike other websites, it is not done on the fly. This "
-"translated text lives on GitHub repository alongside main ClickHouse "
-"codebase and waits for fellow native speakers to make it more human-readable."
-msgstr ""
-
-#: templates/docs/machine-translated.html:4
-msgid "You can also use the original English version as a reference."
-msgstr ""
-
-#: templates/docs/machine-translated.html:7
-msgid "Help ClickHouse documentation by editing this page"
-msgstr ""
-
-#: templates/docs/sidebar.html:3
-msgid "Multi-page or single-page"
-msgstr ""
-
-#: templates/docs/sidebar.html:5
-msgid "Multi-page version"
-msgstr ""
-
-#: templates/docs/sidebar.html:8
-msgid "Single-page version"
-msgstr ""
-
-#: templates/docs/sidebar.html:13
-msgid "Version"
-msgstr ""
-
-#: templates/docs/sidebar.html:13 templates/docs/sidebar.html:19
-msgid "latest"
-msgstr ""
-
-#: templates/docs/sidebar.html:36
-msgid "PDF version"
-msgstr ""
-
-#: templates/docs/toc.html:8
-msgid "Table of Contents"
-msgstr ""
-
-#: templates/index/community.html:4
-msgid "ClickHouse community"
-msgstr ""
-
-#: templates/index/community.html:13 templates/index/community.html:14
-msgid "ClickHouse YouTube Channel"
-msgstr ""
-
-#: templates/index/community.html:25 templates/index/community.html:26
-msgid "ClickHouse Official Twitter Account"
-msgstr ""
-
-#: templates/index/community.html:36 templates/index/community.html:37
-msgid "ClickHouse at Telegram"
-msgstr ""
-
-#: templates/index/community.html:41
-msgid "Chat with real users in "
-msgstr ""
-
-#: templates/index/community.html:44 templates/index/community.html:116
-msgid "English"
-msgstr ""
-
-#: templates/index/community.html:45
-msgid "or in"
-msgstr ""
-
-#: templates/index/community.html:47 templates/index/community.html:117
-msgid "Russian"
-msgstr ""
-
-#: templates/index/community.html:65
-msgid "Open GitHub issue to ask for help or to file a feature request"
-msgstr ""
-
-#: templates/index/community.html:76 templates/index/community.html:77
-msgid "ClickHouse Slack Workspace"
-msgstr ""
-
-#: templates/index/community.html:82
-msgid "Multipurpose public hangout"
-msgstr ""
-
-#: templates/index/community.html:101
-msgid "Ask any questions"
-msgstr ""
-
-#: templates/index/community.html:115
-msgid "ClickHouse Blog"
-msgstr ""
-
-#: templates/index/community.html:116
-msgid "in"
-msgstr ""
-
-#: templates/index/community.html:128 templates/index/community.html:129
-msgid "ClickHouse at Google Groups"
-msgstr ""
-
-#: templates/index/community.html:133
-msgid "Email discussions"
-msgstr ""
-
-#: templates/index/community.html:142
-msgid "Like ClickHouse?"
-msgstr ""
-
-#: templates/index/community.html:143
-msgid "Help to spread the word about it via"
-msgstr ""
-
-#: templates/index/community.html:144
-msgid "and"
-msgstr ""
-
-#: templates/index/community.html:153
-msgid "Hosting ClickHouse Meetups"
-msgstr ""
-
-#: templates/index/community.html:157
-msgid ""
-"ClickHouse meetups are essential for strengthening community worldwide, but "
-"they couldn't be possible without the help of local organizers. Please, fill "
-"this form if you want to become one or want to meet ClickHouse core team for "
-"any other reason."
-msgstr ""
-
-#: templates/index/community.html:159
-msgid "ClickHouse Meetup"
-msgstr ""
-
-#: templates/index/community.html:165
-msgid "Name"
-msgstr ""
-
-#: templates/index/community.html:168
-msgid "Email"
-msgstr ""
-
-#: templates/index/community.html:171
-msgid "Company"
-msgstr ""
-
-#: templates/index/community.html:174
-msgid "City"
-msgstr ""
-
-#: templates/index/community.html:179
-msgid "We'd like to host a public ClickHouse Meetup"
-msgstr ""
-
-#: templates/index/community.html:185
-msgid "We'd like to invite Yandex ClickHouse team to our office"
-msgstr ""
-
-#: templates/index/community.html:191
-msgid "We'd like to invite Yandex ClickHouse team to another event we organize"
-msgstr ""
-
-#: templates/index/community.html:197
-msgid "We're interested in commercial consulting, support or managed service"
-msgstr ""
-
-#: templates/index/community.html:201
-msgid "Additional comments"
-msgstr ""
-
-#: templates/index/community.html:203
-msgid "Send"
-msgstr ""
-
-#: templates/index/community.html:212
-msgid ""
-"If you have any more thoughts or questions, feel free to contact Yandex "
-"ClickHouse team directly at"
-msgstr ""
-
-#: templates/index/community.html:213
-msgid "turn on JavaScript to see email address"
-msgstr ""
diff --git a/website/locale/fr/LC_MESSAGES/messages.mo b/website/locale/fr/LC_MESSAGES/messages.mo
deleted file mode 100644
index 43fcad3bd730..000000000000
Binary files a/website/locale/fr/LC_MESSAGES/messages.mo and /dev/null differ
diff --git a/website/locale/fr/LC_MESSAGES/messages.po b/website/locale/fr/LC_MESSAGES/messages.po
deleted file mode 100644
index 5ccc7c3c87db..000000000000
--- a/website/locale/fr/LC_MESSAGES/messages.po
+++ /dev/null
@@ -1,326 +0,0 @@
-# Translations template for PROJECT.
-# Copyright (C) 2020 ORGANIZATION
-# This file is distributed under the same license as the PROJECT project.
-# Automatically generated, 2020.
-#
-msgid ""
-msgstr ""
-"Project-Id-Version: PROJECT VERSION
"
-"Report-Msgid-Bugs-To: EMAIL@ADDRESS
"
-"POT-Creation-Date: 2020-06-17 12:20+0300
"
-"PO-Revision-Date: 2020-06-17 12:20+0300
"
-"Last-Translator: Automatically generated
"
-"Language-Team: none
"
-"MIME-Version: 1.0
"
-"Content-Type: text/plain; charset=UTF-8
"
-"Content-Transfer-Encoding: 8bit
"
-"Generated-By: Babel 2.8.0
"
-"Language: fr
"
-"Plural-Forms: nplurals=2; plural=(n > 1);
"
-
-#: templates/common_meta.html:1
-msgid ""
-"ClickHouse is a fast open-source column-oriented database management system "
-"that allows generating analytical data reports in real-time using SQL queries"
-msgstr ""
-
-#: templates/common_meta.html:6
-msgid "ClickHouse - fast open-source OLAP DBMS"
-msgstr ""
-
-#: templates/common_meta.html:10
-msgid "ClickHouse DBMS"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "open-source"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "relational"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "analytics"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "analytical"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "Big Data"
-msgstr ""
-
-#: templates/common_meta.html:32
-msgid "web-analytics"
-msgstr ""
-
-#: templates/footer.html:8
-msgid "ClickHouse source code is published under the Apache 2.0 License."
-msgstr ""
-
-#: templates/footer.html:8
-msgid ""
-"Software is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR "
-"CONDITIONS OF ANY KIND, either express or implied."
-msgstr ""
-
-#: templates/footer.html:11
-msgid "Yandex LLC"
-msgstr ""
-
-#: templates/blog/content.html:20 templates/blog/content.html:25
-#: templates/blog/content.html:30
-msgid "Share on"
-msgstr ""
-
-#: templates/blog/content.html:37
-msgid "Published date"
-msgstr ""
-
-#: templates/blog/nav.html:20
-msgid "New&nbsp;post"
-msgstr ""
-
-#: templates/blog/nav.html:25
-msgid "Documentation"
-msgstr ""
-
-#: templates/docs/footer.html:3
-msgid "Rating"
-msgstr ""
-
-#: templates/docs/footer.html:3
-msgid "votes"
-msgstr ""
-
-#: templates/docs/footer.html:4
-msgid "Article Rating"
-msgstr ""
-
-#: templates/docs/footer.html:4
-msgid "Was this content helpful?"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Unusable"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Poor"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Good"
-msgstr ""
-
-#: templates/docs/footer.html:7
-msgid "Excellent"
-msgstr ""
-
-#: templates/docs/footer.html:8
-msgid "documentation"
-msgstr ""
-
-#: templates/docs/footer.html:15
-msgid "Built from"
-msgstr ""
-
-#: templates/docs/footer.html:15
-msgid "published on"
-msgstr ""
-
-#: templates/docs/footer.html:15
-msgid "modified on"
-msgstr ""
-
-#: templates/docs/machine-translated.html:3
-msgid "Help wanted!"
-msgstr ""
-
-#: templates/docs/machine-translated.html:4
-msgid ""
-"The following content of this documentation page has been machine-"
-"translated. But unlike other websites, it is not done on the fly. This "
-"translated text lives on GitHub repository alongside main ClickHouse "
-"codebase and waits for fellow native speakers to make it more human-readable."
-msgstr ""
-
-#: templates/docs/machine-translated.html:4
-msgid "You can also use the original English version as a reference."
-msgstr ""
-
-#: templates/docs/machine-translated.html:7
-msgid "Help ClickHouse documentation by editing this page"
-msgstr ""
-
-#: templates/docs/sidebar.html:3
-msgid "Multi-page or single-page"
-msgstr ""
-
-#: templates/docs/sidebar.html:5
-msgid "Multi-page version"
-msgstr ""
-
-#: templates/docs/sidebar.html:8
-msgid "Single-page version"
-msgstr ""
-
-#: templates/docs/sidebar.html:13
-msgid "Version"
-msgstr ""
-
-#: templates/docs/sidebar.html:13 templates/docs/sidebar.html:19
-msgid "latest"
-msgstr ""
-
-#: templates/docs/sidebar.html:36
-msgid "PDF version"
-msgstr ""
-
-#: templates/docs/toc.html:8
-msgid "Table of Contents"
-msgstr ""
-
-#: templates/index/community.html:4
-msgid "ClickHouse community"
-msgstr ""
-
-#: templates/index/community.html:13 templates/index/community.html:14
-msgid "ClickHouse YouTube Channel"
-msgstr ""
-
-#: templates/index/community.html:25 templates/index/community.html:26
-msgid "ClickHouse Official Twitter Account"
-msgstr ""
-
-#: templates/index/community.html:36 templates/index/community.html:37
-msgid "ClickHouse at Telegram"
-msgstr ""
-
-#: templates/index/community.html:41
-msgid "Chat with real users in "
-msgstr ""
-
-#: templates/index/community.html:44 templates/index/community.html:116
-msgid "English"
-msgstr ""
-
-#: templates/index/community.html:45
-msgid "or in"
-msgstr ""
-
-#: templates/index/community.html:47 templates/index/community.html:117
-msgid "Russian"
-msgstr ""
-
-#: templates/index/community.html:65
-msgid "Open GitHub issue to ask for help or to file a feature request"
-msgstr ""
-
-#: templates/index/community.html:76 templates/index/community.html:77
-msgid "ClickHouse Slack Workspace"
-msgstr ""
-
-#: templates/index/community.html:82
-msgid "Multipurpose public hangout"
-msgstr ""
-
-#: templates/index/community.html:101
-msgid "Ask any questions"
-msgstr ""
-
-#: templates/index/community.html:115
-msgid "ClickHouse Blog"
-msgstr ""
-
-#: templates/index/community.html:116
-msgid "in"
-msgstr ""
-
-#: templates/index/community.html:128 templates/index/community.html:129
-msgid "ClickHouse at Google Groups"
-msgstr ""
-
-#: templates/index/community.html:133
-msgid "Email discussions"
-msgstr ""
-
-#: templates/index/community.html:142
-msgid "Like ClickHouse?"
-msgstr ""
-
-#: templates/index/community.html:143
-msgid "Help to spread the word about it via"
-msgstr ""
-
-#: templates/index/community.html:144
-msgid "and"
-msgstr ""
-
-#: templates/index/community.html:153
-msgid "Hosting ClickHouse Meetups"
-msgstr ""
-
-#: templates/index/community.html:157
-msgid ""
-"ClickHouse meetups are essential for strengthening community worldwide, but "
-"they couldn't be possible without the help of local organizers. Please, fill "
-"this form if you want to become one or want to meet ClickHouse core team for "
-"any other reason."
-msgstr ""
-
-#: templates/index/community.html:159
-msgid "ClickHouse Meetup"
-msgstr ""
-
-#: templates/index/community.html:165
-msgid "Name"
-msgstr ""
-
-#: templates/index/community.html:168
-msgid "Email"
-msgstr ""
-
-#: templates/index/community.html:171
-msgid "Company"
-msgstr ""
-
-#: templates/index/community.html:174
-msgid "City"
-msgstr ""
-
-#: templates/index/community.html:179
-msgid "We'd like to host a public ClickHouse Meetup"
-msgstr ""
-
-#: templates/index/community.html:185
-msgid "We'd like to invite Yandex ClickHouse team to our office"
-msgstr ""
-
-#: templates/index/community.html:191
-msgid "We'd like to invite Yandex ClickHouse team to another event we organize"
-msgstr ""
-
-#: templates/index/community.html:197
-msgid "We're interested in commercial consulting, support or managed service"
-msgstr ""
-
-#: templates/index/community.html:201
-msgid "Additional comments"
-msgstr ""
-
-#: templates/index/community.html:203
-msgid "Send"
-msgstr ""
-
-#: templates/index/community.html:212
-msgid ""
-"If you have any more thoughts or questions, feel free to contact Yandex "
-"ClickHouse team directly at"
-msgstr ""
-
-#: templates/index/community.html:213
-msgid "turn on JavaScript to see email address"
-msgstr ""
diff --git a/website/sitemap-index.xml b/website/sitemap-index.xml
index 75fdc75973c8..3fbdd99d372d 100644
--- a/website/sitemap-index.xml
+++ b/website/sitemap-index.xml
@@ -6,21 +6,12 @@
    <sitemap>
       <loc>https://clickhouse.tech/docs/zh/sitemap.xml</loc>
    </sitemap>
-   <sitemap>
-      <loc>https://clickhouse.tech/docs/es/sitemap.xml</loc>
-   </sitemap>
-   <sitemap>
-      <loc>https://clickhouse.tech/docs/fr/sitemap.xml</loc>
-   </sitemap>
    <sitemap>
       <loc>https://clickhouse.tech/docs/ru/sitemap.xml</loc>
    </sitemap>
    <sitemap>
       <loc>https://clickhouse.tech/docs/ja/sitemap.xml</loc>
    </sitemap>
-   <sitemap>
-      <loc>https://clickhouse.tech/docs/fa/sitemap.xml</loc>
-   </sitemap>
    <sitemap>
       <loc>https://clickhouse.tech/blog/en/sitemap.xml</loc>
    </sitemap>
