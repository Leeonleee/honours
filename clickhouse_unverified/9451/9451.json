{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 9451,
  "instance_id": "ClickHouse__ClickHouse-9451",
  "issue_numbers": [
    "8504"
  ],
  "base_commit": "be221a2f1adc6ee69bcdeaee6ff5a09420a4a78f",
  "patch": "diff --git a/dbms/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h b/dbms/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h\nindex 0a96a4832630..209d7181b668 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h\n+++ b/dbms/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h\n@@ -2,7 +2,7 @@\n #include <IO/WriteBufferFromFile.h>\n #include <IO/ReadBufferFromFile.h>\n \n-#include <unordered_map>\n+#include <map>\n \n namespace DB\n {\n@@ -33,7 +33,8 @@ struct MergeTreeDataPartTTLInfo\n /// PartTTLInfo for all columns and table with minimal ttl for whole part\n struct MergeTreeDataPartTTLInfos\n {\n-    std::unordered_map<String, MergeTreeDataPartTTLInfo> columns_ttl;\n+    /// Order is important as it would be serialized and hashed for checksums\n+    std::map<String, MergeTreeDataPartTTLInfo> columns_ttl;\n     MergeTreeDataPartTTLInfo table_ttl;\n \n     /// `part_min_ttl` and `part_max_ttl` are TTLs which are used for selecting parts\n@@ -41,7 +42,8 @@ struct MergeTreeDataPartTTLInfos\n     time_t part_min_ttl = 0;\n     time_t part_max_ttl = 0;\n \n-    std::unordered_map<String, MergeTreeDataPartTTLInfo> moves_ttl;\n+    /// Order is important as it would be serialized and hashed for checksums\n+    std::map<String, MergeTreeDataPartTTLInfo> moves_ttl;\n \n     void read(ReadBuffer & in);\n     void write(WriteBuffer & out) const;\n",
  "test_patch": "diff --git a/dbms/tests/integration/test_ttl_replicated/test.py b/dbms/tests/integration/test_ttl_replicated/test.py\nindex ae4fa8404ac3..8ab470e6f609 100644\n--- a/dbms/tests/integration/test_ttl_replicated/test.py\n+++ b/dbms/tests/integration/test_ttl_replicated/test.py\n@@ -44,7 +44,43 @@ def test_ttl_columns(started_cluster):\n     expected = \"1\\t0\\t0\\n2\\t0\\t0\\n\"\n     assert TSV(node1.query(\"SELECT id, a, b FROM test_ttl ORDER BY id\")) == TSV(expected)\n     assert TSV(node2.query(\"SELECT id, a, b FROM test_ttl ORDER BY id\")) == TSV(expected)\n+\n+\n+def test_ttl_many_columns(started_cluster):\n+    drop_table([node1, node2], \"test_ttl_2\")\n+    for node in [node1, node2]:\n+        node.query(\n+        '''\n+            CREATE TABLE test_ttl_2(date DateTime, id UInt32,\n+                a Int32 TTL date,\n+                _idx Int32 TTL date,\n+                _offset Int32 TTL date,\n+                _partition Int32 TTL date)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/test_ttl_2', '{replica}')\n+            ORDER BY id PARTITION BY toDayOfMonth(date) SETTINGS merge_with_ttl_timeout=0;\n+        '''.format(replica=node.name))\n+\n+    node1.query(\"SYSTEM STOP MERGES test_ttl_2\")\n+    node2.query(\"SYSTEM STOP MERGES test_ttl_2\")\n+\n+    node1.query(\"INSERT INTO test_ttl_2 VALUES (toDateTime('2000-10-10 00:00:00'), 1, 2, 3, 4, 5)\")\n+    node1.query(\"INSERT INTO test_ttl_2 VALUES (toDateTime('2100-10-10 10:00:00'), 6, 7, 8, 9, 10)\")\n+\n+    # Check that part will appear in result of merge\n+    node1.query(\"SYSTEM STOP FETCHES test_ttl_2\")\n+    node2.query(\"SYSTEM STOP FETCHES test_ttl_2\")\n+\n+    node1.query(\"SYSTEM START MERGES test_ttl_2\")\n+    node2.query(\"SYSTEM START MERGES test_ttl_2\")\n+\n+    time.sleep(1) # sleep to allow use ttl merge selector for second time\n+    node1.query(\"OPTIMIZE TABLE test_ttl_2 FINAL\", timeout=5)\n+\n+    expected = \"1\\t0\\t0\\t0\\t0\\n6\\t7\\t8\\t9\\t10\\n\"\n+    assert TSV(node1.query(\"SELECT id, a, _idx, _offset, _partition FROM test_ttl_2 ORDER BY id\")) == TSV(expected)\n+    assert TSV(node2.query(\"SELECT id, a, _idx, _offset, _partition FROM test_ttl_2 ORDER BY id\")) == TSV(expected)\n  \n+\n @pytest.mark.parametrize(\"delete_suffix\", [\n     \"\",\n     \"DELETE\",\n",
  "problem_statement": "Multiple columns with TTL cause checksum failures on merge\nWe have some large tables (schema at the end).  On a shard with three replicas, if I alter the table to add two column TTL values, the following messages appear in the logs on two of the replicas:  \r\n\r\n`DB::Exception: Checksums of parts don't match: hash of uncompressed files doesn't match (version 19.17.4.11). Data after merge is not byte-identical to data on another replicas. There could be several reasons: 1. Using newer version of compression library after server update. 2. Using another compression method. 3. Non-deterministic compression algorithm (highly unlikely). 4. Non-deterministic merge algorithm due to logical error in code. 5. Data corruption in memory due to bug in code. 6. Data corruption in memory due to hardware issue. 7. Manual modification of source data after server startup. 8. Manual modification of checksums stored in ZooKeeper. We will download merged part from replica to force byte-identical result.\r\n`\r\n\r\nClickHouse version 19.17.4.11 on all three replicas.  The TTLs added were to the `parent_id `and `trace_id `columns, each 14 days.  If I add the TTL to only a single column, such as `trace_id` or `uas`, there are no checksum problems reported in the logs.  Type of column does not seem to matter --reproduced by adding a TTL to the `uas` column and the `php` column also.  \r\n\r\nIt appears the the checksum issues are related to deeper merges with mixed sizes of parts.  (Not all merges cause the checksum error).  Checksum messages occur on newly inserted data/parts, every few seconds.\r\n\r\nTable is partitioned by `toDate(datetime)`.  `use_minimalistic_part_header_in_zookeeper` is 1.  No other relevant changes from default configuration.\r\n\r\nAfter checksum issues start, removing the TTL from a single column (via manually modifying the zookeeper `columns` node and detaching/attaching the table on all three replicas) does not fix the issue.  Removing all column TTLs does fix the issue.\r\n\r\n\r\nTable Schema:\r\n\r\n> CREATE TABLE cdn.ats_access\r\n(\r\n    `datetime`   DateTime,\r\n    `ms`         UInt16,\r\n    `host`       LowCardinality(String),\r\n    `ptcl`       Enum8('undef' = -2, 'UNKNOWN' = -1, 'http' = 0, 'https' = 1, 'ftp' = 2, 'mailto' = 3, 'file' = 4, 'data' = 5),\r\n    `domain`     String,\r\n    `path`       String,\r\n    `qry`        String,\r\n    `frag`       String,\r\n    `b`          UInt32,\r\n    `cfsc`       Enum8('FIN' = 0, 'TIMEOUT' = 1, 'INTR' = 2),\r\n    `chi`        FixedString(16),\r\n    `cqhm`       Enum8('INVALID' = 0, 'GET' = 1, 'POST' = 2, 'PUT' = 3, 'DELETE' = 4, 'HEAD' = 5, 'OPTIONS' = 6, 'CONNECT' = 7, 'TRACE' = 8, 'PROPFIND' = 9),\r\n    `cqhv`       Enum8('UNKNOWN' = 0, 'HTTP/0.9' = 1, 'HTTP/1.0' = 2, 'HTTP/1.1' = 3, 'HTTP/2.0' = 4, 'HTTP/3.0' = 5, 'HTTP' = 100),\r\n    `crc`        Enum8('TCP_HIT' = 1, 'TCP_IMS_HIT' = 2, 'TCP_IMS_MISS' = 3, 'TCP_MEM_HIT' = 4, 'TCP_MISS' = 5, 'TCP_REFRESH_FAIL_HIT' = 6, 'TCP_REFRESH_HIT' = 7, 'TCP_REFRESH_MISS' = 8, 'TCP_SWAPFAIL_MISS' = 9, 'ERR_CLIENT_ABORT' = 10, 'ERR_CONNECT_FAIL' = 11, 'ERR_INVALID_REQ' = 12, 'ERR_INVALID_URL' = 13, 'ERR_PROXY_DENIED' = 14, 'ERR_READ_ERROR' = 15, 'ERR_READ_TIMEOUT' = 16, 'ERR_UNKNOWN' = 17, 'TCP_MISS_REDIRECT' = 18, 'TCP_CLIENT_REFRESH' = 19, 'TCP_REF_FAIL_HIT' = 20, 'ERR_CLIENT_READ_ERROR' = 21, 'ERR_DNS_FAIL' = 22),\r\n    `pfsc`       Enum8('FIN' = 0, 'TIMEOUT' = 1, 'INTR' = 2),\r\n    `php`        UInt16,\r\n    `phr`        Enum8('NONE' = 0, 'PARENT_HIT' = 1, 'DIRECT' = 2, 'TIMEOUT_DIRECT' = 3, 'EMPTY' = 4),\r\n    `pqsn`       LowCardinality(String),\r\n    `pssc`       UInt16,\r\n    `sscl`       UInt32,\r\n    `sssc`       UInt16,\r\n    `ttms`       UInt32,\r\n    `uas`        String,\r\n    `svc`        LowCardinality(String),\r\n    `svc_type`   LowCardinality(String),\r\n    `kafka_time` DateTime,\r\n    `span_id`    Int64,\r\n    `parent_id`  Int64 TTL datetime + toIntervalDay(14),\r\n    `trace_id`   UUID TTL datetime + toIntervalDay(14),\r\n    `uas_type`   LowCardinality(String),\r\n    `uas_device` String\r\n) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/cdn.ats_access',\r\n           '{replica}') PARTITION BY toDate(datetime) ORDER BY (datetime, chi, path, ms) SETTINGS index_granularity = 16384\n",
  "hints_text": "Stupid question: are clocks in sync between replicas? Do you have ntpd running?\nYes, we have ntpd running, and other tables on these servers replicate fine.  We also are running 7 other shards (three replicas each) with exactly the same tables and configuration without issue.  I only see the checksum problems when I alter a table to include multiple TTL columns.",
  "created_at": "2020-02-28T20:30:26Z"
}