{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 59173,
  "instance_id": "ClickHouse__ClickHouse-59173",
  "issue_numbers": [
    "56640"
  ],
  "base_commit": "43b088dbd03a9156181a9378979d907957be8bf5",
  "patch": "diff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex bdea46a8210d..c2e0e7782209 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -1313,6 +1313,17 @@ void IMergeTreeDataPart::loadRowsCount()\n         auto buf = metadata_manager->read(\"count.txt\");\n         readIntText(rows_count, *buf);\n         assertEOF(*buf);\n+\n+        if (!index_granularity.empty() && rows_count < index_granularity.getTotalRows() && index_granularity_info.fixed_index_granularity)\n+        {\n+            /// Adjust last granule size to match the number of rows in the part in case of fixed index_granularity.\n+            index_granularity.popMark();\n+            index_granularity.appendMark(rows_count % index_granularity_info.fixed_index_granularity);\n+            if (rows_count != index_granularity.getTotalRows())\n+                throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                    \"Index granularity total rows in part {} does not match rows_count: {}, instead of {}\",\n+                    name, index_granularity.getTotalRows(), rows_count);\n+        }\n     };\n \n     if (index_granularity.empty())\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex 5ba326cef0c1..a69d21de8e7b 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -558,7 +558,10 @@ void MergeTreeDataPartWriterWide::validateColumnOfFixedSize(const NameAndTypePai\n \n         if (index_granularity_rows != index_granularity.getMarkRows(mark_num))\n         {\n-            throw Exception(\n+            /// With fixed granularity we can have last mark with less rows than granularity\n+            const bool is_last_mark = (mark_num + 1 == index_granularity.getMarksCount());\n+            if (!index_granularity_info.fixed_index_granularity || !is_last_mark)\n+                throw Exception(\n                             ErrorCodes::LOGICAL_ERROR,\n                             \"Incorrect mark rows for part {} for mark #{}\"\n                             \" (compressed offset {}, decompressed offset {}), in-memory {}, on disk {}, total marks {}\",\n@@ -821,7 +824,14 @@ void MergeTreeDataPartWriterWide::adjustLastMarkIfNeedAndFlushToDisk(size_t new_\n             /// Without offset\n             rows_written_in_last_mark = 0;\n         }\n+\n+        if (compute_granularity)\n+        {\n+            index_granularity.popMark();\n+            index_granularity.appendMark(new_rows_in_last_mark);\n+        }\n     }\n+\n }\n \n }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02967_prewhere_no_columns.reference b/tests/queries/0_stateless/02967_prewhere_no_columns.reference\nnew file mode 100644\nindex 000000000000..df1052546188\n--- /dev/null\n+++ b/tests/queries/0_stateless/02967_prewhere_no_columns.reference\n@@ -0,0 +1,2 @@\n+105\n+105\ndiff --git a/tests/queries/0_stateless/02967_prewhere_no_columns.sql b/tests/queries/0_stateless/02967_prewhere_no_columns.sql\nnew file mode 100644\nindex 000000000000..efcc952caa23\n--- /dev/null\n+++ b/tests/queries/0_stateless/02967_prewhere_no_columns.sql\n@@ -0,0 +1,51 @@\n+CREATE TABLE t_02967\n+(\n+  `key` Date,\n+  `value` UInt16\n+)\n+ENGINE = MergeTree\n+ORDER BY key\n+SETTINGS\n+  index_granularity_bytes = 0 --8192 --, min_index_granularity_bytes = 2  \n+ , index_granularity = 100\n+ , min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0\n+--\n+--  , min_bytes_for_wide_part = 2\n+AS SELECT\n+  number,\n+  repeat(toString(number), 5)\n+FROM numbers(105.);\n+\n+\n+\n+-- Check with newly inserted data part. It's in-memory structured are filled at insert time.\n+SELECT\n+  count(ignore(*))\n+FROM t_02967\n+PREWHERE CAST(ignore() + 1 as UInt8)\n+GROUP BY \n+  ignore(65535, *),\n+  ignore(255, 256, *)\n+SETTINGS\n+ --send_logs_level='test', \n+ max_threads=1;\n+\n+\n+\n+-- Reload part form disk to check that in-meory structures where properly serilaized-deserialized\n+DETACH TABLE t_02967;\n+ATTACH TABLE t_02967;\n+\n+\n+SELECT\n+  count(ignore(*))\n+FROM t_02967\n+PREWHERE CAST(ignore() + 1 as UInt8)\n+GROUP BY \n+  ignore(65535, *),\n+  ignore(255, 256, *)\n+SETTINGS\n+ --send_logs_level='test', \n+ max_threads=1;\n+\n+DROP TABLE t_02967;\ndiff --git a/tests/queries/0_stateless/03198_non_adaptive_granularity_no_errors.reference b/tests/queries/0_stateless/03198_non_adaptive_granularity_no_errors.reference\nnew file mode 100644\nindex 000000000000..fcd78da1283d\n--- /dev/null\n+++ b/tests/queries/0_stateless/03198_non_adaptive_granularity_no_errors.reference\n@@ -0,0 +1,2 @@\n+1000000\n+1000000\ndiff --git a/tests/queries/0_stateless/03198_non_adaptive_granularity_no_errors.sql b/tests/queries/0_stateless/03198_non_adaptive_granularity_no_errors.sql\nnew file mode 100644\nindex 000000000000..25798ef6d33f\n--- /dev/null\n+++ b/tests/queries/0_stateless/03198_non_adaptive_granularity_no_errors.sql\n@@ -0,0 +1,12 @@\n+DROP TABLE IF EXISTS data_02051__fuzz_24;\n+\n+CREATE TABLE data_02051__fuzz_24 (`key` Int16, `value` String) ENGINE = MergeTree ORDER BY key SETTINGS index_granularity_bytes = 0, min_rows_for_wide_part = 0, min_bytes_for_wide_part=0  AS SELECT number, repeat(toString(number), 5) FROM numbers(1000000.);\n+\n+SELECT  count(ignore(*)) FROM data_02051__fuzz_24 PREWHERE materialize(1) GROUP BY ignore(*);\n+\n+detach table data_02051__fuzz_24;\n+attach table data_02051__fuzz_24;\n+\n+SELECT  count(ignore(*)) FROM data_02051__fuzz_24 PREWHERE materialize(1) GROUP BY ignore(*);\n+\n+DROP TABLE data_02051__fuzz_24;\ndiff --git a/tests/queries/1_stateful/00166_explain_estimate.reference b/tests/queries/1_stateful/00166_explain_estimate.reference\nindex 71ddd6815812..85ecd0b9a71a 100644\n--- a/tests/queries/1_stateful/00166_explain_estimate.reference\n+++ b/tests/queries/1_stateful/00166_explain_estimate.reference\n@@ -1,5 +1,5 @@\n test\thits\t1\t57344\t7\n-test\thits\t1\t8839168\t1079\n-test\thits\t1\t835584\t102\n+test\thits\t1\t8832938\t1079\n+test\thits\t1\t829354\t102\n test\thits\t1\t8003584\t977\n test\thits\t2\t581632\t71\n",
  "problem_statement": "Logical error: 'RangeReader read 576 rows, but 8192 expected.'\nhttps://s3.amazonaws.com/clickhouse-test-reports/56516/857b47de2eaffef8995217f20c8f348c5510fbff/fuzzer_astfuzzertsan/report.html\r\n\r\n```\r\n2023.11.11 11:54:41.536045 [ 676 ] {9f4b4aaa-956f-4065-be8f-500452cecacc} <Fatal> : Logical error: 'RangeReader read 576 rows, but 8192 expected.'.\r\n2023.11.11 11:54:41.537064 [ 908 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2023.11.11 11:54:41.537136 [ 908 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 696598418FD730A935D80DF7AB04D7B4E875F36D, git hash: 72e7c28ef4bd490451062cab14879261eaf11181) (from thread 676) Received signal 6\r\n2023.11.11 11:54:41.537209 [ 908 ] {} <Fatal> BaseDaemon: Signal description: Aborted\r\n2023.11.11 11:54:41.537246 [ 908 ] {} <Fatal> BaseDaemon: \r\n2023.11.11 11:54:41.537321 [ 908 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fca136ed9fc 0x00007fca13699476 0x00007fca1367f7f3 0x0000559f58cf4573 0x0000559f60db254b 0x0000559f60db2f54 0x0000559f5a09c3c6 0x0000559f6c7e9feb 0x0000559f6c7e7bb8 0x0000559f6c7e7b86 0x0000559f6c7f228b 0x0000559f6d3671aa 0x0000559f6c7d9af4 0x0000559f6d36a46a 0x0000559f6cce3dcb 0x0000559f6cd06c55 0x0000559f6ccf9c51 0x0000559f6ccfac91 0x0000559f60efdcf2 0x0000559f60f02b9d 0x0000559f60f02b02 0x0000559f60ef9967 0x0000559f60effbd2 0x0000559f58ceda4f 0x00007fca136ebac3 0x00007fca1377da40\r\n2023.11.11 11:54:41.537415 [ 908 ] {} <Fatal> BaseDaemon: ########################################\r\n2023.11.11 11:54:41.537648 [ 908 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 696598418FD730A935D80DF7AB04D7B4E875F36D, git hash: 72e7c28ef4bd490451062cab14879261eaf11181) (from thread 676) (query_id: 9f4b4aaa-956f-4065-be8f-500452cecacc) (query: SELECT count(ignore(*)) FROM data_02051__fuzz_3 PREWHERE 1023 AND ignore(10, 1024 AND 1025 AND ignore(ignore(1048577, (NULL AND NULL) AND NULL, 1025, ignore(-2147483649 AND 1048576 AND NULL AND 1, *), NULL AND NULL), ignore(100000000000000000000., (NULL AND NULL) AND 2147483646, -2147483647, NULL AND NULL, 1023 AND ignore(ignore(NULL, (NULL AND NULL) AND -2147483648, 10, ignore(0.0001, (NULL AND NULL) AND 1048577, 9223372036854775807, 65536 AND NULL, 0 AND 10), 65537 AND 65535), *) AND 0 AND 65535), *) AND NULL AND (NULL AND NULL), -9223372036854775807 AND (NULL AND NULL AND NULL) AND -2147483648, 7, 10 AND 65537 AND -1) AND 1025 AND NULL AND (NULL AND NULL) SETTINGS min_bytes_to_use_direct_io = 0, local_filesystem_read_method = 'mmap', local_filesystem_read_prefetch = 0, read_priority = 0, max_read_buffer_size = 1048576) Received signal Aborted (6)\r\n2023.11.11 11:54:41.537799 [ 908 ] {} <Fatal> BaseDaemon: \r\n2023.11.11 11:54:41.537924 [ 908 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fca136ed9fc 0x00007fca13699476 0x00007fca1367f7f3 0x0000559f58cf4573 0x0000559f60db254b 0x0000559f60db2f54 0x0000559f5a09c3c6 0x0000559f6c7e9feb 0x0000559f6c7e7bb8 0x0000559f6c7e7b86 0x0000559f6c7f228b 0x0000559f6d3671aa 0x0000559f6c7d9af4 0x0000559f6d36a46a 0x0000559f6cce3dcb 0x0000559f6cd06c55 0x0000559f6ccf9c51 0x0000559f6ccfac91 0x0000559f60efdcf2 0x0000559f60f02b9d 0x0000559f60f02b02 0x0000559f60ef9967 0x0000559f60effbd2 0x0000559f58ceda4f 0x00007fca136ebac3 0x00007fca1377da40\r\n2023.11.11 11:54:41.538108 [ 908 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007fca136ed9fc in ?\r\n2023.11.11 11:54:41.538236 [ 908 ] {} <Fatal> BaseDaemon: 6. ? @ 0x00007fca13699476 in ?\r\n2023.11.11 11:54:41.538335 [ 908 ] {} <Fatal> BaseDaemon: 7. ? @ 0x00007fca1367f7f3 in ?\r\n2023.11.11 11:54:53.333418 [ 908 ] {} <Fatal> BaseDaemon: 8. __interceptor_abort @ 0x0000000007e99573 in /workspace/clickhouse\r\n2023.11.11 11:54:53.661430 [ 908 ] {} <Fatal> BaseDaemon: 9. ./build_docker/./src/Common/Exception.cpp:0: DB::abortOnFailedAssertion(String const&) @ 0x000000000ff5754b in /workspace/clickhouse\r\n2023.11.11 11:54:53.988327 [ 908 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Common/Exception.cpp:0: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000ff57f54 in /workspace/clickhouse\r\n2023.11.11 11:55:05.114904 [ 908 ] {} <Fatal> BaseDaemon: 11. DB::Exception::Exception<unsigned long&, unsigned long const&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type, std::type_identity<unsigned long const&>::type>, unsigned long&, unsigned long const&) @ 0x00000000092413c6 in /workspace/clickhouse\r\n2023.11.11 11:55:05.513819 [ 908 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:1242: DB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult const&, unsigned long&) @ 0x000000001b98efeb in /workspace/clickhouse\r\n2023.11.11 11:55:05.796150 [ 908 ] {} <Fatal> BaseDaemon: 13. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:987: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x000000001b98cbb8 in /workspace/clickhouse\r\n2023.11.11 11:55:06.079903 [ 908 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:0: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x000000001b98cb86 in /workspace/clickhouse\r\n2023.11.11 11:55:06.316604 [ 908 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./src/Storages/MergeTree/MergeTreeReadTask.cpp:163: DB::MergeTreeReadTask::read(DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001b99728b in /workspace/clickhouse\r\n2023.11.11 11:55:08.352510 [ 908 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectAlgorithms.h:38: DB::MergeTreeThreadSelectAlgorithm::readFromTask(DB::MergeTreeReadTask&, DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001c50c1aa in /workspace/clickhouse\r\n2023.11.11 11:55:08.783246 [ 908 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:162: DB::MergeTreeSelectProcessor::read() @ 0x000000001b97eaf4 in /workspace/clickhouse\r\n2023.11.11 11:55:08.965197 [ 908 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:181: DB::MergeTreeSource::processReadResult(DB::ChunkAndProgress)\r\n2023.11.11 11:55:08.965410 [ 908 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:226: DB::MergeTreeSource::tryGenerate() @ 0x000000001c50f46a in /workspace/clickhouse\r\n2023.11.11 11:55:09.116249 [ 908 ] {} <Fatal> BaseDaemon: 19.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/optional:344: std::__optional_storage_base<DB::Chunk, false>::has_value[abi:v15000]() const\r\n2023.11.11 11:55:09.116481 [ 908 ] {} <Fatal> BaseDaemon: 19.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/optional:998: std::optional<DB::Chunk>::operator bool[abi:v15000]() const\r\n2023.11.11 11:55:09.116600 [ 908 ] {} <Fatal> BaseDaemon: 19. ./build_docker/./src/Processors/ISource.cpp:108: DB::ISource::work() @ 0x000000001be88dcb in /workspace/clickhouse\r\n2023.11.11 11:55:09.166808 [ 908 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:588: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::__sz[abi:v15000]() const\r\n2023.11.11 11:55:09.167023 [ 908 ] {} <Fatal> BaseDaemon: 20.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:616: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.11.11 11:55:09.167216 [ 908 ] {} <Fatal> BaseDaemon: 20.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:918: std::list<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.11.11 11:55:09.167368 [ 908 ] {} <Fatal> BaseDaemon: 20.4. inlined from ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:50: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*)\r\n2023.11.11 11:55:09.167477 [ 908 ] {} <Fatal> BaseDaemon: 20. ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:95: DB::ExecutionThreadContext::executeTask() @ 0x000000001beabc55 in /workspace/clickhouse\r\n2023.11.11 11:55:09.372272 [ 908 ] {} <Fatal> BaseDaemon: 21. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:272: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x000000001be9ec51 in /workspace/clickhouse\r\n2023.11.11 11:55:09.613200 [ 908 ] {} <Fatal> BaseDaemon: 22.1. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:370: operator()\r\n2023.11.11 11:55:09.613497 [ 908 ] {} <Fatal> BaseDaemon: 22.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: decltype(std::declval<DB::PipelineExecutor::spawnThreads()::$_0&>()()) std::__invoke[abi:v15000]<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)\r\n2023.11.11 11:55:09.613688 [ 908 ] {} <Fatal> BaseDaemon: 22.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: void std::__invoke_void_return_wrapper<void, true>::__call<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)\r\n2023.11.11 11:55:09.613844 [ 908 ] {} <Fatal> BaseDaemon: 22.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:235: std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>::operator()[abi:v15000]()\r\n2023.11.11 11:55:09.613964 [ 908 ] {} <Fatal> BaseDaemon: 22. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:716: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x000000001be9fc91 in /workspace/clickhouse\r\n2023.11.11 11:55:09.825954 [ 908 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:09.826179 [ 908 ] {} <Fatal> BaseDaemon: 23.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:09.826316 [ 908 ] {} <Fatal> BaseDaemon: 23.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.11.11 11:55:09.826443 [ 908 ] {} <Fatal> BaseDaemon: 23.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.11.11 11:55:09.826564 [ 908 ] {} <Fatal> BaseDaemon: 23. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x00000000100a2cf2 in /workspace/clickhouse\r\n2023.11.11 11:55:09.983893 [ 908 ] {} <Fatal> BaseDaemon: 24. ./build_docker/./src/Common/ThreadPool.cpp:0: ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'()::operator()() @ 0x00000000100a7b9d in /workspace/clickhouse\r\n2023.11.11 11:55:10.222417 [ 908 ] {} <Fatal> BaseDaemon: 25. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:717: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x00000000100a7b02 in /workspace/clickhouse\r\n2023.11.11 11:55:10.401473 [ 908 ] {} <Fatal> BaseDaemon: 26.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:10.401724 [ 908 ] {} <Fatal> BaseDaemon: 26.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:10.401879 [ 908 ] {} <Fatal> BaseDaemon: 26.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.11.11 11:55:10.402008 [ 908 ] {} <Fatal> BaseDaemon: 26.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.11.11 11:55:10.402125 [ 908 ] {} <Fatal> BaseDaemon: 26. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000001009e967 in /workspace/clickhouse\r\n2023.11.11 11:55:10.642451 [ 908 ] {} <Fatal> BaseDaemon: 27.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>::reset[abi:v15000](std::__thread_struct*)\r\n2023.11.11 11:55:10.642651 [ 908 ] {} <Fatal> BaseDaemon: 27.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr\r\n2023.11.11 11:55:10.642800 [ 908 ] {} <Fatal> BaseDaemon: 27.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:265: ~__tuple_leaf\r\n2023.11.11 11:55:10.642931 [ 908 ] {} <Fatal> BaseDaemon: 27.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:538: ~tuple\r\n2023.11.11 11:55:10.643170 [ 908 ] {} <Fatal> BaseDaemon: 27.5. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:48: std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>::operator()[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*) const\r\n2023.11.11 11:55:10.643439 [ 908 ] {} <Fatal> BaseDaemon: 27.6. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:305: std::unique_ptr<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>, std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>>::reset[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*)\r\n2023.11.11 11:55:10.643564 [ 908 ] {} <Fatal> BaseDaemon: 27.7. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr\r\n2023.11.11 11:55:10.643679 [ 908 ] {} <Fatal> BaseDaemon: 27. ./build_docker/./contrib/llvm-project/libcxx/include/thread:297: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x00000000100a4bd2 in /workspace/clickhouse\r\n2023.11.11 11:55:22.746228 [ 908 ] {} <Fatal> BaseDaemon: 28. __tsan_thread_start_func @ 0x0000000007e92a4f in /workspace/clickhouse\r\n2023.11.11 11:55:22.746457 [ 908 ] {} <Fatal> BaseDaemon: 29. ? @ 0x00007fca136ebac3 in ?\r\n2023.11.11 11:55:22.746564 [ 908 ] {} <Fatal> BaseDaemon: 30. ? @ 0x00007fca1377da40 in ?\r\n```\r\n\n",
  "hints_text": "cc @davenger \nduplicate of #56202?\n> duplicate of https://github.com/ClickHouse/ClickHouse/issues/56202?\r\n\r\n\r\nNo, the error message and the stacktrace are different.\nhttps://s3.amazonaws.com/clickhouse-test-reports/57205/82270627ed1c74ce96c1eb8100430de3b81b3efe/fuzzer_astfuzzermsan/report.html\nhttps://s3.amazonaws.com/clickhouse-test-reports/57467/434c2113d207b4507cac7f7cac88dbf553cc3474/fuzzer_astfuzzermsan/report.html\r\n\r\n```\r\n2023.12.04 15:11:49.999517 [ 1182 ] {49219680-1cd9-43de-af98-fc8eeceeaa2a} <Fatal> : Logical error: 'RangeReader read 10000 rows, but 16384 expected.'.\r\n2023.12.04 15:11:50.000383 [ 1255 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2023.12.04 15:11:50.000463 [ 1255 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 30C754DCB0A11F170684E63F74476BC05C09FECE, git hash: e0ff7b34220657b70e39d83d78a780d5675fc396) (from thread 1182) Received signal 6\r\n2023.12.04 15:11:50.000524 [ 1255 ] {} <Fatal> BaseDaemon: Signal description: Aborted\r\n2023.12.04 15:11:50.000583 [ 1255 ] {} <Fatal> BaseDaemon: \r\n2023.12.04 15:11:50.000658 [ 1255 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007f8bd45419fc 0x00007f8bd44ed476 0x00007f8bd44d37f3 0x000055bffb5460ca 0x000055bffb547404 0x000055bfe7526a6c 0x000055c01613b442 0x000055c0161353b7 0x000055c016154ee1 0x000055c017f7ba94 0x000055c01610d8ea 0x000055c017f17f00 0x000055c016e2b36c 0x000055c016e89097 0x000055c016e5ea71 0x000055c016e5a5bc 0x000055c016e5a028 0x000055c016e94d86 0x000055bffb84e26b 0x000055bffb85c8eb 0x00007f8bd453fac3 0x00007f8bd45d1a40\r\n2023.12.04 15:11:50.000727 [ 1255 ] {} <Fatal> BaseDaemon: ########################################\r\n2023.12.04 15:11:50.000877 [ 1255 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 30C754DCB0A11F170684E63F74476BC05C09FECE, git hash: e0ff7b34220657b70e39d83d78a780d5675fc396) (from thread 1182) (query_id: 49219680-1cd9-43de-af98-fc8eeceeaa2a) (query: WITH [1.1920928955078125e-7, 1.] AS reference_vec SELECT vec, [0.0001, 10000000000.], 65535, -2147483647, L2Distance(vec, reference_vec) FROM tab__fuzz_0 PREWHERE 3 ORDER BY 1000.0001 DESC NULLS FIRST, -inf DESC NULLS LAST LIMIT 9223372036854775807) Received signal Aborted (6)\r\n2023.12.04 15:11:50.001031 [ 1255 ] {} <Fatal> BaseDaemon: \r\n2023.12.04 15:11:50.001161 [ 1255 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007f8bd45419fc 0x00007f8bd44ed476 0x00007f8bd44d37f3 0x000055bffb5460ca 0x000055bffb547404 0x000055bfe7526a6c 0x000055c01613b442 0x000055c0161353b7 0x000055c016154ee1 0x000055c017f7ba94 0x000055c01610d8ea 0x000055c017f17f00 0x000055c016e2b36c 0x000055c016e89097 0x000055c016e5ea71 0x000055c016e5a5bc 0x000055c016e5a028 0x000055c016e94d86 0x000055bffb84e26b 0x000055bffb85c8eb 0x00007f8bd453fac3 0x00007f8bd45d1a40\r\n2023.12.04 15:11:50.001300 [ 1255 ] {} <Fatal> BaseDaemon: 4. ? @ 0x00007f8bd45419fc in ?\r\n2023.12.04 15:11:50.001425 [ 1255 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007f8bd44ed476 in ?\r\n2023.12.04 15:11:50.001545 [ 1255 ] {} <Fatal> BaseDaemon: 6. ? @ 0x00007f8bd44d37f3 in ?\r\n2023.12.04 15:11:50.175753 [ 1255 ] {} <Fatal> BaseDaemon: 7. ./build_docker/./src/Common/Exception.cpp:0: DB::abortOnFailedAssertion(String const&) @ 0x000000001f3c50ca in /workspace/clickhouse\r\n2023.12.04 15:11:50.333573 [ 1255 ] {} <Fatal> BaseDaemon: 8.1. inlined from ./build_docker/./src/Common/Exception.cpp:194: DB::Exception::getStackFramePointers() const\r\n2023.12.04 15:11:50.333790 [ 1255 ] {} <Fatal> BaseDaemon: 8. ./build_docker/./src/Common/Exception.cpp:101: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000001f3c6404 in /workspace/clickhouse\r\n2023.12.04 15:11:54.377579 [ 1255 ] {} <Fatal> BaseDaemon: 9. DB::Exception::Exception<unsigned long&, unsigned long const&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type, std::type_identity<unsigned long const&>::type>, unsigned long&, unsigned long const&) @ 0x000000000b3a5a6c in /workspace/clickhouse\r\n2023.12.04 15:11:54.609886 [ 1255 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:1246: DB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult const&, unsigned long&) @ 0x0000000039fba442 in /workspace/clickhouse\r\n2023.12.04 15:11:54.773283 [ 1255 ] {} <Fatal> BaseDaemon: 11. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:991: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x0000000039fb43b7 in /workspace/clickhouse\r\n2023.12.04 15:11:54.902637 [ 1255 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Storages/MergeTree/MergeTreeReadTask.cpp:163: DB::MergeTreeReadTask::read(DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x0000000039fd3ee1 in /workspace/clickhouse\r\n2023.12.04 15:11:54.988551 [ 1255 ] {} <Fatal> BaseDaemon: 13. ./src/Storages/MergeTree/MergeTreeSelectAlgorithms.h:53: DB::MergeTreeInOrderSelectAlgorithm::readFromTask(DB::MergeTreeReadTask&, DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000003bdfaa94 in /workspace/clickhouse\r\n2023.12.04 15:11:55.203020 [ 1255 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:162: DB::MergeTreeSelectProcessor::read() @ 0x0000000039f8c8ea in /workspace/clickhouse\r\n2023.12.04 15:11:55.303242 [ 1255 ] {} <Fatal> BaseDaemon: 15.1. inlined from ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:181: DB::MergeTreeSource::processReadResult(DB::ChunkAndProgress)\r\n2023.12.04 15:11:55.303353 [ 1255 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:226: DB::MergeTreeSource::tryGenerate() @ 0x000000003bd96f00 in /workspace/clickhouse\r\n2023.12.04 15:11:55.380176 [ 1255 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./contrib/llvm-project/libcxx/include/optional:344: std::__optional_storage_base<DB::Chunk, false>::has_value[abi:v15000]() const\r\n2023.12.04 15:11:55.380298 [ 1255 ] {} <Fatal> BaseDaemon: 16.2. inlined from ./contrib/llvm-project/libcxx/include/optional:998: std::optional<DB::Chunk>::operator bool[abi:v15000]() const\r\n2023.12.04 15:11:55.380347 [ 1255 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Processors/ISource.cpp:108: DB::ISource::work() @ 0x000000003acaa36c in /workspace/clickhouse\r\n2023.12.04 15:11:55.407046 [ 1255 ] {} <Fatal> BaseDaemon: 17.1. inlined from ./contrib/llvm-project/libcxx/include/list:588: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::__sz[abi:v15000]() const\r\n2023.12.04 15:11:55.407162 [ 1255 ] {} <Fatal> BaseDaemon: 17.2. inlined from ./contrib/llvm-project/libcxx/include/list:616: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.12.04 15:11:55.407205 [ 1255 ] {} <Fatal> BaseDaemon: 17.3. inlined from ./contrib/llvm-project/libcxx/include/list:918: std::list<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.12.04 15:11:55.407251 [ 1255 ] {} <Fatal> BaseDaemon: 17.4. inlined from ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:50: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*)\r\n2023.12.04 15:11:55.407285 [ 1255 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:95: DB::ExecutionThreadContext::executeTask() @ 0x000000003ad08097 in /workspace/clickhouse\r\n2023.12.04 15:11:55.518238 [ 1255 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:273: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*)\r\n2023.12.04 15:11:55.518348 [ 1255 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:239: DB::PipelineExecutor::executeSingleThread(unsigned long) @ 0x000000003acdda71 in /workspace/clickhouse\r\n2023.12.04 15:11:55.617272 [ 1255 ] {} <Fatal> BaseDaemon: 19. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:409: DB::PipelineExecutor::executeImpl(unsigned long, bool) @ 0x000000003acd95bc in /workspace/clickhouse\r\n2023.12.04 15:11:55.725568 [ 1255 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:274: std::unique_ptr<DB::ExecutingGraph, std::default_delete<DB::ExecutingGraph>>::operator->[abi:v15000]() const\r\n2023.12.04 15:11:55.725689 [ 1255 ] {} <Fatal> BaseDaemon: 20. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:114: DB::PipelineExecutor::execute(unsigned long, bool) @ 0x000000003acd9028 in /workspace/clickhouse\r\n2023.12.04 15:11:55.833436 [ 1255 ] {} <Fatal> BaseDaemon: 21.1. inlined from ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:96: DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::shared_ptr<DB::ThreadGroup>, unsigned long, bool)\r\n2023.12.04 15:11:55.833553 [ 1255 ] {} <Fatal> BaseDaemon: 21.2. inlined from ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112: operator()\r\n2023.12.04 15:11:55.833588 [ 1255 ] {} <Fatal> BaseDaemon: 21.3. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ?\r\n2023.12.04 15:11:55.833700 [ 1255 ] {} <Fatal> BaseDaemon: 21.4. inlined from ./contrib/llvm-project/libcxx/include/tuple:1789: decltype(auto) std::__apply_tuple_impl[abi:v15000]<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&, std::__tuple_indices<>)\r\n2023.12.04 15:11:55.833754 [ 1255 ] {} <Fatal> BaseDaemon: 21.5. inlined from ./contrib/llvm-project/libcxx/include/tuple:1798: decltype(auto) std::apply[abi:v15000]<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&)\r\n2023.12.04 15:11:55.833791 [ 1255 ] {} <Fatal> BaseDaemon: 21.6. inlined from ./src/Common/ThreadPool.h:250: operator()\r\n2023.12.04 15:11:55.833832 [ 1255 ] {} <Fatal> BaseDaemon: 21.7. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ?\r\n2023.12.04 15:11:55.833862 [ 1255 ] {} <Fatal> BaseDaemon: 21.8. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: ?\r\n2023.12.04 15:11:55.833893 [ 1255 ] {} <Fatal> BaseDaemon: 21.9. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:235: ?\r\n2023.12.04 15:11:55.833925 [ 1255 ] {} <Fatal> BaseDaemon: 21. ./contrib/llvm-project/libcxx/include/__functional/function.h:716: ? @ 0x000000003ad13d86 in /workspace/clickhouse\r\n2023.12.04 15:11:55.928560 [ 1255 ] {} <Fatal> BaseDaemon: 22.1. inlined from ./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.12.04 15:11:55.928675 [ 1255 ] {} <Fatal> BaseDaemon: 22.2. inlined from ./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.12.04 15:11:55.928723 [ 1255 ] {} <Fatal> BaseDaemon: 22.3. inlined from ./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.12.04 15:11:55.928764 [ 1255 ] {} <Fatal> BaseDaemon: 22.4. inlined from ./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.12.04 15:11:55.928798 [ 1255 ] {} <Fatal> BaseDaemon: 22. ./build_docker/./src/Common/ThreadPool.cpp:423: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000001f6cd26b in /workspace/clickhouse\r\n2023.12.04 15:11:56.054154 [ 1255 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:303: std::unique_ptr<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>, std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>>::reset[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*)\r\n2023.12.04 15:11:56.054260 [ 1255 ] {} <Fatal> BaseDaemon: 23.2. inlined from ./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr\r\n2023.12.04 15:11:56.054296 [ 1255 ] {} <Fatal> BaseDaemon: 23. ./contrib/llvm-project/libcxx/include/thread:297: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000001f6db8eb in /workspace/clickhouse\r\n2023.12.04 15:11:56.054357 [ 1255 ] {} <Fatal> BaseDaemon: 24. ? @ 0x00007f8bd453fac3 in ?\r\n2023.12.04 15:11:56.054388 [ 1255 ] {} <Fatal> BaseDaemon: 25. ? @ 0x00007f8bd45d1a40 in ?\r\n2023.12.04 15:11:56.054439 [ 1255 ] {} <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.\r\n2023.12.04 15:12:01.827812 [ 1255 ] {} <Fatal> BaseDaemon: This ClickHouse version is not official and should be upgraded to the official build.\r\n2023.12.04 15:12:01.828389 [ 1255 ] {} <Fatal> BaseDaemon: Changed settings: receive_timeout = 10., receive_data_timeout_ms = 10000, allow_suspicious_low_cardinality_types = true, log_queries = true, table_function_remote_max_addresses = 200, allow_experimental_analyzer = true, max_execution_time = 10., max_memory_usage = 10000000000, log_comment = '/workspace/ch/tests/queries/0_stateless/02798_substring_index.sql', send_logs_level = 'fatal', allow_introspection_functions = true, allow_experimental_annoy_index = true, allow_experimental_usearch_index = true\r\n2023.12.04 15:12:13.407295 [ 158 ] {} <Fatal> Application: Child process was terminated by signal 6\r\n```\nReproducer with the analyzer:\r\n\r\n```\r\nDROP TABLE IF EXISTS tab__fuzz_7;\r\nCREATE TABLE tab__fuzz_7 (`id` UInt16, `vec` Array(Float32)) ENGINE = MergeTree\r\nORDER BY id\r\nSETTINGS index_granularity_bytes = 0, min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0, index_granularity = 8192;\r\n\r\nINSERT INTO tab__fuzz_7 values (0, [2.2, 2.3]) (1, [3.1, 3.2]);\r\nINSERT INTO tab__fuzz_7 values (2, [2.2, 2.3, 2.4]) (3, [3.1, 3.2, 3.3]);\r\n\r\n\r\nWITH NULL AS reference_vec SELECT (NULL, 3.4028234663852886e38, NULL, NULL), tuple('9223372036854775807'), 1023, -2147483647, (NULL, NULL), NULL FROM tab__fuzz_7 PREWHERE 8 GROUP BY GROUPING SETS ((('0.0000001024', '104857.5')), ((NULL, NULL, NULL, NULL, NULL))) ORDER BY tuple('102.4') DESC NULLS FIRST, tuple(NULL) ASC LIMIT 9223372036854775806 SETTINGS allow_experimental_analyzer = true;\r\n```\r\n\r\n```\r\nCode: 49. DB::Exception: Received from clickhouse-01:49000. DB::Exception: RangeReader read 2 rows, but 8192 expected.: While executing MergeTreeSelect(pool: ReadPool, algorithm: Thread). (LOGICAL_ERROR)\r\n(query: WITH NULL AS reference_vec SELECT (NULL, 3.4028234663852886e38, NULL, NULL), tuple('9223372036854775807'), 1023, -2147483647, (NULL, NULL), NULL FROM tab__fuzz_7 PREWHERE 8 GROUP BY GROUPING SETS ((('0.0000001024', '104857.5')), ((NULL, NULL, NULL, NULL, NULL))) ORDER BY tuple('102.4') DESC NULLS FIRST, tuple(NULL) ASC LIMIT 9223372036854775806 SETTINGS allow_experimental_analyzer = true;)\r\n```\nIt's unrelated to the analyzer. The problem is about the execution of PREWHERE clause. Sometimes PrewhereInfo is incorrect.\r\n\r\nReproduction with old analyzer:\r\n\r\n```sql\r\nCREATE TABLE data_02051__fuzz_33\r\n(\r\n  `key` Date,\r\n  `value` UInt16\r\n)\r\nENGINE = MergeTree\r\nORDER BY key\r\nSETTINGS\r\n  index_granularity_bytes = 0\r\nAS SELECT\r\n  number,\r\n  repeat(toString(number), 5)\r\nFROM numbers(1000000.);\r\n\r\nSET allow_experimental_analyzer = 0;\r\n-- SET optimize_move_to_prewhere = 0;\r\n-- set send_logs_level = 'debug';\r\n\r\nSELECT\r\n  count(ignore(*))\r\nFROM data_02051__fuzz_33\r\nPREWHERE CAST(ignore() + 1 as UInt8)\r\nGROUP BY \r\n  ignore(65535, *),\r\n  ignore(255, 256, *)\r\n```\r\n\r\nTest for new analyzer:\r\n```sql\r\nCREATE TABLE data_02051__fuzz_33\r\n(\r\n  `key` Date,\r\n  `value` UInt16\r\n)\r\nENGINE = MergeTree\r\nORDER BY key\r\nSETTINGS\r\n  index_granularity_bytes = 0\r\nAS SELECT\r\n  number,\r\n  repeat(toString(number), 5)\r\nFROM numbers(1000000.);\r\n\r\nSET allow_experimental_analyzer = 1;\r\n\r\nSELECT\r\n  count(ignore(*))\r\nFROM data_02051__fuzz_33\r\nPREWHERE 255\r\nGROUP BY \r\n  ignore(65535, *),\r\n  ignore(255, 256, *)\r\n```\nWorks on 23.6 https://fiddle.clickhouse.com/1dca2559-27f5-4c0b-a63d-ad58def955ac\r\nFails on 23.7 https://fiddle.clickhouse.com/f5f1d9e4-4532-457a-a765-14231ca2d4df\r\n\r\nThe difference is that in 23.6 the first reader in the chain of read steps reads 1 column (`key`)\r\n```\r\nMergeTreeRangeReader: First reader returned: num_rows: 10000, columns: 1, total_rows_per_granule: 10000, no filter, column[0]:  UInt16(size = 10000), requested columns: key\r\n```\r\nbut in 23.7 it somehow ends up reading not columns so it returns row count of 2 full granules  \r\n```\r\nMergeTreeRangeReader: First reader returned: num_rows: 16384, columns: 0, total_rows_per_granule: 16384, no filter, requested columns: \r\n```\r\n\nLooks like this started failing after https://github.com/ClickHouse/ClickHouse/pull/52689 Before that we would add a real column to the read step if doesn't read anything.\r\nBut the real problem here seems to be that with fixed index granularity of let's say 8192 rows and a part with 10k rows we have index_granularity structure like this:\r\n```\r\np index_granularity\r\n(DB::MergeTreeIndexGranularity &) 0x000051b0002c0150: {\r\n  marks_rows_partial_sums = size=2 {\r\n    [0] = 8192\r\n    [1] = 16384\r\n  }\r\n  initialized = true\r\n}\r\n```\r\nthe last granule size is rounded up 2*8192 while it only has 10000-8192 rows\nhttps://s3.amazonaws.com/clickhouse-test-reports/59027/838f0f4d08c49b04053f7cdc2ae02bbc579bd4e1/fuzzer_astfuzzerasan/report.html ",
  "created_at": "2024-01-24T17:21:42Z"
}