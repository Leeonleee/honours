{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 10887,
  "instance_id": "ClickHouse__ClickHouse-10887",
  "issue_numbers": [
    "10747"
  ],
  "base_commit": "11b4bc7189e5ecff9d68f491cb0090e7a104cb3b",
  "patch": "diff --git a/src/Storages/StorageDistributed.cpp b/src/Storages/StorageDistributed.cpp\nindex 00fc22c34aae..f8202cbcac3c 100644\n--- a/src/Storages/StorageDistributed.cpp\n+++ b/src/Storages/StorageDistributed.cpp\n@@ -550,7 +550,8 @@ void StorageDistributed::checkAlterIsPossible(const AlterCommands & commands, co\n         if (command.type != AlterCommand::Type::ADD_COLUMN\n             && command.type != AlterCommand::Type::MODIFY_COLUMN\n             && command.type != AlterCommand::Type::DROP_COLUMN\n-            && command.type != AlterCommand::Type::COMMENT_COLUMN)\n+            && command.type != AlterCommand::Type::COMMENT_COLUMN\n+            && command.type != AlterCommand::Type::RENAME_COLUMN)\n \n             throw Exception(\"Alter of type '\" + alterTypeToString(command.type) + \"' is not supported by storage \" + getName(),\n                 ErrorCodes::NOT_IMPLEMENTED);\n",
  "test_patch": "diff --git a/tests/integration/test_rename_column/__init__.py b/tests/integration/test_rename_column/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_rename_column/configs/config.d/instant_moves.xml b/tests/integration/test_rename_column/configs/config.d/instant_moves.xml\nnew file mode 100644\nindex 000000000000..7b68c6946caf\n--- /dev/null\n+++ b/tests/integration/test_rename_column/configs/config.d/instant_moves.xml\n@@ -0,0 +1,4 @@\n+<yandex>\n+    <background_move_processing_pool_thread_sleep_seconds>0.5</background_move_processing_pool_thread_sleep_seconds>\n+    <background_move_processing_pool_task_sleep_seconds_when_no_work_max>0.5</background_move_processing_pool_task_sleep_seconds_when_no_work_max>\n+</yandex>\ndiff --git a/tests/integration/test_rename_column/configs/config.d/part_log.xml b/tests/integration/test_rename_column/configs/config.d/part_log.xml\nnew file mode 100644\nindex 000000000000..53ea0a8fc132\n--- /dev/null\n+++ b/tests/integration/test_rename_column/configs/config.d/part_log.xml\n@@ -0,0 +1,7 @@\n+<yandex>\n+    <part_log>\n+        <database>system</database>\n+        <table>part_log</table>\n+        <flush_interval_milliseconds>500</flush_interval_milliseconds>\n+    </part_log>\n+</yandex>\ndiff --git a/tests/integration/test_rename_column/configs/config.d/storage_configuration.xml b/tests/integration/test_rename_column/configs/config.d/storage_configuration.xml\nnew file mode 100644\nindex 000000000000..131219abf3de\n--- /dev/null\n+++ b/tests/integration/test_rename_column/configs/config.d/storage_configuration.xml\n@@ -0,0 +1,28 @@\n+<yandex>\n+\n+<storage_configuration>\n+    <disks>\n+        <default>\n+        </default>\n+        <internal>\n+            <path>/internal/</path>\n+        </internal>\n+        <external>\n+            <path>/external/</path>\n+        </external>\n+    </disks>\n+    <policies>\n+        <default_with_external>\n+            <volumes>\n+                <internal>\n+                    <disk>internal</disk>\n+                </internal>\n+                <external>\n+                    <disk>external</disk>\n+                </external>\n+            </volumes>\n+        </default_with_external>\n+    </policies>\n+</storage_configuration>\n+\n+</yandex>\ndiff --git a/tests/integration/test_rename_column/configs/config.d/zookeeper_session_timeout.xml b/tests/integration/test_rename_column/configs/config.d/zookeeper_session_timeout.xml\nnew file mode 100644\nindex 000000000000..caa0ff11137d\n--- /dev/null\n+++ b/tests/integration/test_rename_column/configs/config.d/zookeeper_session_timeout.xml\n@@ -0,0 +1,6 @@\n+<yandex>\n+    <zookeeper>\n+        <!-- Required for correct timing in current test case -->\n+        <session_timeout_ms replace=\"1\">15000</session_timeout_ms>\n+    </zookeeper>\n+</yandex>\ndiff --git a/tests/integration/test_rename_column/configs/remote_servers.xml b/tests/integration/test_rename_column/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..4c3de4b39054\n--- /dev/null\n+++ b/tests/integration/test_rename_column/configs/remote_servers.xml\n@@ -0,0 +1,28 @@\n+<yandex>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node2</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node3</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node4</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+</yandex>\ndiff --git a/tests/integration/test_rename_column/test.py b/tests/integration/test_rename_column/test.py\nnew file mode 100644\nindex 000000000000..9fac783e7120\n--- /dev/null\n+++ b/tests/integration/test_rename_column/test.py\n@@ -0,0 +1,555 @@\n+from __future__ import print_function\n+\n+import time\n+import random\n+import pytest\n+\n+from multiprocessing.dummy import Pool\n+from helpers.cluster import ClickHouseCluster\n+from helpers.client import QueryRuntimeException\n+from helpers.network import PartitionManager\n+from helpers.test_tools import TSV\n+\n+node_options = dict(\n+    with_zookeeper=True,\n+    main_configs=['configs/remote_servers.xml'],\n+    config_dir='configs',\n+    tmpfs=['/external:size=200M', '/internal:size=1M'])\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('node1', macros={\"shard\": 0, \"replica\": 1}, **node_options)\n+node2 = cluster.add_instance('node2', macros={\"shard\": 0, \"replica\": 2}, **node_options)\n+node3 = cluster.add_instance('node3', macros={\"shard\": 1, \"replica\": 1}, **node_options)\n+node4 = cluster.add_instance('node4', macros={\"shard\": 1, \"replica\": 2}, **node_options)\n+nodes = [node1, node2, node3, node4]\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    except Exception as ex:\n+        print(ex)\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def drop_table(nodes, table_name):\n+    for node in nodes:\n+        node.query(\"DROP TABLE IF EXISTS {} NO DELAY\".format(table_name))\n+    time.sleep(1)\n+\n+\n+def create_table(nodes, table_name, with_storage_policy=False, with_time_column=False,\n+                 with_ttl_move=False, with_ttl_delete=False):\n+    extra_columns = \"\"\n+    settings = []\n+\n+    for node in nodes:\n+        sql = \"\"\"\n+            CREATE TABLE {table_name}\n+                (\n+                    num UInt32,\n+                    num2 UInt32 DEFAULT num + 1{extra_columns}\n+                )\n+                ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/{table_name}', '{replica}')\n+                ORDER BY num PARTITION BY num % 100\n+            \"\"\"\n+        if with_ttl_move:\n+            sql += \"\"\"\n+                TTL time + INTERVAL (num2 % 1) SECOND TO DISK 'external'\n+            \"\"\"\n+        if with_ttl_delete:\n+            sql += \"\"\"\n+                TTL time + INTERVAL (num2 % 1) SECOND DELETE\n+            \"\"\"\n+            settings.append(\"merge_with_ttl_timeout = 1\")\n+\n+        if with_storage_policy:\n+            settings.append(\"storage_policy='default_with_external'\")\n+\n+        if settings:\n+            sql += \"\"\"\n+                SETTINGS {}\n+            \"\"\".format(\", \".join(settings))\n+\n+        if with_time_column:\n+            extra_columns = \"\"\",\n+                    time DateTime\n+                \"\"\"\n+        node.query(sql.format(table_name=table_name, replica=node.name, extra_columns=extra_columns))\n+\n+\n+def create_distributed_table(node, table_name):\n+        sql = \"\"\"\n+            CREATE TABLE %(table_name)s_replicated ON CLUSTER test_cluster\n+            (\n+                num UInt32,\n+                num2 UInt32 DEFAULT num + 1\n+            )\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/{shard}/%(table_name)s_replicated', '{replica}')\n+            ORDER BY num PARTITION BY num %% 100;\n+        \"\"\" % dict(table_name=table_name)\n+        node.query(sql)\n+        sql = \"\"\"\n+            CREATE TABLE %(table_name)s ON CLUSTER test_cluster AS %(table_name)s_replicated\n+            ENGINE = Distributed(test_cluster, default, %(table_name)s_replicated, rand())\n+        \"\"\" % dict(table_name=table_name)\n+        node.query(sql)\n+\n+\n+def drop_distributed_table(node, table_name):\n+    node.query(\"DROP TABLE IF EXISTS {} ON CLUSTER test_cluster\".format(table_name))\n+    node.query(\"DROP TABLE IF EXISTS {}_replicated ON CLUSTER test_cluster\".format(table_name))\n+    time.sleep(1)\n+\n+\n+def insert(node, table_name, chunk=1000, col_names=None, iterations=1, ignore_exception=False,\n+        slow=False, with_many_parts=False, offset=0, with_time_column=False):\n+    if col_names is None:\n+        col_names = ['num', 'num2']\n+    for i in range(iterations):\n+        try:\n+            query = [\"SET max_partitions_per_insert_block = 10000000\"]\n+            if with_many_parts:\n+                query.append(\"SET max_insert_block_size = 64\")\n+            if with_time_column:\n+                query.append(\n+                    \"INSERT INTO {table_name} ({col0}, {col1}, time) SELECT number AS {col0}, number + 1 AS {col1}, now() + 10 AS time FROM numbers_mt({chunk})\"\n+                .format(table_name=table_name, chunk=chunk, col0=col_names[0], col1=col_names[1]))\n+            elif slow:\n+                query.append(\n+                    \"INSERT INTO {table_name} ({col0}, {col1}) SELECT number + sleepEachRow(0.001) AS {col0}, number + 1 AS {col1} FROM numbers_mt({chunk})\"\n+                .format(table_name=table_name, chunk=chunk, col0=col_names[0], col1=col_names[1]))\n+            else:\n+                query.append(\n+                    \"INSERT INTO {table_name} ({col0},{col1}) SELECT number + {offset} AS {col0}, number + 1 + {offset} AS {col1} FROM numbers_mt({chunk})\"\n+                .format(table_name=table_name, chunk=chunk, col0=col_names[0], col1=col_names[1], offset=str(offset)))\n+            node.query(\";\\n\".join(query))\n+        except QueryRuntimeException as ex:\n+            if not ignore_exception:\n+                raise\n+\n+\n+def select(node, table_name, col_name=\"num\", expected_result=None, iterations=1, ignore_exception=False, slow=False, poll=None):\n+    for i in range(iterations):\n+        start_time = time.time()\n+        while True:\n+            try:\n+                if slow:\n+                    r = node.query(\"SELECT count() FROM (SELECT num2, sleepEachRow(0.5) FROM {} WHERE {} % 1000 > 0)\".format(table_name, col_name))\n+                else:\n+                    r = node.query(\"SELECT count() FROM {} WHERE {} % 1000 > 0\".format(table_name, col_name))\n+                if expected_result:\n+                    if r != expected_result and poll and time.time() - start_time < poll:\n+                        continue\n+                    assert r == expected_result\n+            except QueryRuntimeException as ex:\n+                if not ignore_exception:\n+                    raise\n+            break\n+\n+\n+def rename_column(node, table_name, name, new_name, iterations=1, ignore_exception=False):\n+    for i in range(iterations):\n+        try:\n+            node.query(\"ALTER TABLE {table_name} RENAME COLUMN {name} to {new_name}\".format(\n+                table_name=table_name, name=name, new_name=new_name\n+            ))\n+        except QueryRuntimeException as ex:\n+            if not ignore_exception:\n+                raise\n+\n+\n+def rename_column_on_cluster(node, table_name, name, new_name, iterations=1, ignore_exception=False):\n+    for i in range(iterations):\n+        try:\n+            node.query(\"ALTER TABLE {table_name} ON CLUSTER test_cluster RENAME COLUMN {name} to {new_name}\".format(\n+                table_name=table_name, name=name, new_name=new_name\n+            ))\n+        except QueryRuntimeException as ex:\n+            if not ignore_exception:\n+                raise\n+\n+\n+def alter_move(node, table_name, iterations=1, ignore_exception=False):\n+    for i in range(iterations):\n+        move_part = random.randint(0, 99)\n+        move_volume = 'external'\n+        try:\n+            node.query(\"ALTER TABLE {table_name} MOVE PARTITION '{move_part}' TO VOLUME '{move_volume}'\"\n+                .format(table_name=table_name, move_part=move_part, move_volume=move_volume))\n+        except QueryRuntimeException as ex:\n+            if not ignore_exception:\n+                raise\n+\n+\n+def test_rename_parallel_same_node(started_cluster):\n+    table_name = \"test_rename_parallel_same_node\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        p = Pool(15)\n+        tasks = []\n+        for i in range(1):\n+            tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node1, table_name, \"foo2\", \"foo3\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node1, table_name, \"foo3\", \"num2\", 5, True)))\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"999\\n\")\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+def test_rename_parallel(started_cluster):\n+    table_name = \"test_rename_parallel\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        p = Pool(15)\n+        tasks = []\n+        for i in range(1):\n+            tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node2, table_name, \"foo2\", \"foo3\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node3, table_name, \"foo3\", \"num2\", 5, True)))\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"999\\n\")\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+def test_rename_with_parallel_select(started_cluster):\n+    table_name = \"test_rename_with_parallel_select\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        select(node1, table_name, \"num2\", \"999\\n\", poll=30)\n+        select(node2, table_name, \"num2\", \"999\\n\", poll=30)\n+        select(node3, table_name, \"num2\", \"999\\n\", poll=30)\n+\n+        p = Pool(15)\n+        tasks = []\n+        for i in range(1):\n+            tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node2, table_name, \"foo2\", \"foo3\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node3, table_name, \"foo3\", \"num2\", 5, True)))\n+            tasks.append(p.apply_async(select, (node1, table_name, \"foo3\", \"999\\n\", 5, True)))\n+            tasks.append(p.apply_async(select, (node2, table_name, \"num2\", \"999\\n\", 5, True)))\n+            tasks.append(p.apply_async(select, (node3, table_name, \"foo2\", \"999\\n\", 5, True)))\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # rename column back to original name\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"999\\n\")\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+def test_rename_with_parallel_insert(started_cluster):\n+    table_name = \"test_rename_with_parallel_insert\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        p = Pool(15)\n+        tasks = []\n+        for i in range(1):\n+            tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node2, table_name, \"foo2\", \"foo3\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node3, table_name, \"foo3\", \"num2\", 5, True)))\n+            tasks.append(p.apply_async(insert, (node1, table_name, 100, [\"num\", \"foo3\"], 5, True)))\n+            tasks.append(p.apply_async(insert, (node2, table_name, 100, [\"num\", \"num2\"], 5, True)))\n+            tasks.append(p.apply_async(insert, (node3, table_name, 100, [\"num\", \"foo2\"], 5, True)))\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\")\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+@pytest.mark.skip(reason=\"For unknown reason one of these tests kill Zookeeper\")\n+def test_rename_with_parallel_merges(started_cluster):\n+    table_name = \"test_rename_with_parallel_merges\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name)\n+        for i in range(20):\n+            insert(node1, table_name, 100, [\"num\",\"num2\"], 1, False, False, True, offset=i*100)\n+\n+        def merge_parts(node, table_name, iterations=1):\n+            for i in range(iterations):\n+                node.query(\"OPTIMIZE TABLE %s FINAL\" % table_name)\n+\n+        p = Pool(15)\n+        tasks = []\n+        for i in range(1):\n+            tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node2, table_name, \"foo2\", \"foo3\", 5, True)))\n+            tasks.append(p.apply_async(rename_column, (node3, table_name, \"foo3\", \"num2\", 5, True)))\n+            tasks.append(p.apply_async(merge_parts, (node1, table_name, 5)))\n+            tasks.append(p.apply_async(merge_parts, (node2, table_name, 5)))\n+            tasks.append(p.apply_async(merge_parts, (node3, table_name, 5)))\n+\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # rename column back to the original name\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"1998\\n\")\n+        select(node2, table_name, \"num2\", \"1998\\n\")\n+        select(node3, table_name, \"num2\", \"1998\\n\")\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+@pytest.mark.skip(reason=\"For unknown reason one of these tests kill Zookeeper\")\n+def test_rename_with_parallel_slow_insert(started_cluster):\n+    table_name = \"test_rename_with_parallel_slow_insert\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        p = Pool(15)\n+        tasks = []\n+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, [\"num\", \"num2\"], 1, False, True)))\n+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, [\"num\", \"num2\"], 1, True, True))) # deduplicated\n+        time.sleep(0.5)\n+        tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\")))\n+\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        insert(node1, table_name, 100, [\"num\", \"foo2\"])\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo2\", \"num2\")\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"11089\\n\")\n+        select(node2, table_name, \"num2\", \"11089\\n\", poll=30)\n+        select(node3, table_name, \"num2\", \"11089\\n\", poll=30)\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+def test_rename_with_parallel_slow_select(started_cluster):\n+    table_name = \"test_rename_with_parallel_slow_select\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        p = Pool(15)\n+        tasks = []\n+\n+        tasks.append(p.apply_async(select, (node1, table_name, \"num2\", \"999\\n\", 1, True, True)))\n+        time.sleep(0.5)\n+        tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\")))\n+\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        insert(node1, table_name, 100, [\"num\", \"foo2\"])\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo2\", \"num2\")\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"1099\\n\")\n+        select(node2, table_name, \"num2\", \"1099\\n\", poll=30)\n+        select(node3, table_name, \"num2\", \"1099\\n\", poll=30)\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+def test_rename_with_parallel_moves(started_cluster):\n+    table_name = \"test_rename_with_parallel_moves\"\n+    drop_table(nodes, table_name)\n+    try:\n+        create_table(nodes, table_name, with_storage_policy=True)\n+        insert(node1, table_name, 1000)\n+\n+        p = Pool(15)\n+        tasks = []\n+\n+        tasks.append(p.apply_async(alter_move, (node1, table_name, 20, True)))\n+        tasks.append(p.apply_async(alter_move, (node2, table_name, 20, True)))\n+        tasks.append(p.apply_async(alter_move, (node3, table_name, 20, True)))\n+        tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 20, True)))\n+        tasks.append(p.apply_async(rename_column, (node2, table_name, \"foo2\", \"foo3\", 20, True)))\n+        tasks.append(p.apply_async(rename_column, (node3, table_name, \"num3\", \"num2\", 20, True)))\n+\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"999\\n\")\n+        select(node2, table_name, \"num2\", \"999\\n\", poll=30)\n+        select(node3, table_name, \"num2\", \"999\\n\", poll=30)\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+def test_rename_with_parallel_ttl_move(started_cluster):\n+    table_name = 'test_rename_with_parallel_ttl_move'\n+    try:\n+        create_table(nodes, table_name, with_storage_policy=True, with_time_column=True, with_ttl_move=True)\n+        rename_column(node1, table_name, \"time\", \"time2\", 1, False)\n+        rename_column(node1, table_name, \"time2\", \"time\", 1, False)\n+\n+        p = Pool(15)\n+        tasks = []\n+\n+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, [\"num\", \"num2\"], 1, False, False, True, 0, True)))\n+        time.sleep(5)\n+        rename_column(node1, table_name, \"time\", \"time2\", 1, False)\n+        time.sleep(4)\n+        tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 20, True)))\n+        tasks.append(p.apply_async(rename_column, (node2, table_name, \"foo2\", \"foo3\", 20, True)))\n+        tasks.append(p.apply_async(rename_column, (node3, table_name, \"num3\", \"num2\", 20, True)))\n+\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # check some parts got moved\n+        assert \"external\" in set(node1.query(\"SELECT disk_name FROM system.parts WHERE table == '{}' AND active=1 ORDER BY modification_time\".format(table_name)).strip().splitlines())\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+\n+        # check that select still works\n+        select(node1, table_name, \"num2\", \"9990\\n\")\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+def test_rename_with_parallel_ttl_delete(started_cluster):\n+    table_name = 'test_rename_with_parallel_ttl_delete'\n+    try:\n+        create_table(nodes, table_name, with_time_column=True, with_ttl_delete=True)\n+        rename_column(node1, table_name, \"time\", \"time2\", 1, False)\n+        rename_column(node1, table_name, \"time2\", \"time\", 1, False)\n+\n+        def merge_parts(node, table_name, iterations=1):\n+            for i in range(iterations):\n+                node.query(\"OPTIMIZE TABLE {}\".format(table_name))\n+\n+        p = Pool(15)\n+        tasks = []\n+\n+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, [\"num\", \"num2\"], 1, False, False, True, 0, True)))\n+        time.sleep(15)\n+        tasks.append(p.apply_async(rename_column, (node1, table_name, \"num2\", \"foo2\", 20, True)))\n+        tasks.append(p.apply_async(rename_column, (node2, table_name, \"foo2\", \"foo3\", 20, True)))\n+        tasks.append(p.apply_async(rename_column, (node3, table_name, \"num3\", \"num2\", 20, True)))\n+        tasks.append(p.apply_async(merge_parts, (node1, table_name, 20)))\n+        tasks.append(p.apply_async(merge_parts, (node2, table_name, 20)))\n+        tasks.append(p.apply_async(merge_parts, (node3, table_name, 20)))\n+\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        # rename column back to original\n+        rename_column(node1, table_name, \"foo2\", \"num2\", 1, True)\n+        rename_column(node1, table_name, \"foo3\", \"num2\", 1, True)\n+\n+        assert int(node1.query(\"SELECT count() FROM {}\".format(table_name)).strip()) < 10000\n+    finally:\n+        drop_table(nodes, table_name)\n+\n+\n+@pytest.mark.skip(reason=\"For unknown reason one of these tests kill Zookeeper\")\n+def test_rename_distributed(started_cluster):\n+    table_name = 'test_rename_distributed'\n+    try:\n+        create_distributed_table(node1, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        rename_column_on_cluster(node1, table_name, 'num2', 'foo2')\n+        rename_column_on_cluster(node1, '%s_replicated' % table_name, 'num2', 'foo2')\n+\n+        insert(node1, table_name, 1000, col_names=['num','foo2'])\n+\n+        select(node1, table_name, \"foo2\", '1998\\n', poll=30)\n+    finally:\n+        drop_distributed_table(node1, table_name)\n+\n+\n+@pytest.mark.skip(reason=\"For unknown reason one of these tests kill Zookeeper\")\n+def test_rename_distributed_parallel_insert_and_select(started_cluster):\n+    table_name = 'test_rename_distributed_parallel_insert_and_select'\n+    try:\n+        create_distributed_table(node1, table_name)\n+        insert(node1, table_name, 1000)\n+\n+        p = Pool(15)\n+        tasks = []\n+        for i in range(1):\n+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, table_name, 'num2', 'foo2', 3, True)))\n+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, '%s_replicated' % table_name, 'num2', 'foo2', 3, True)))\n+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, table_name, 'foo2', 'foo3', 3, True)))\n+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, '%s_replicated' % table_name, 'foo2', 'foo3', 3, True)))\n+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, table_name, 'foo3', 'num2', 3, True)))\n+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, '%s_replicated' % table_name, 'foo3', 'num2', 3, True)))\n+            tasks.append(p.apply_async(insert, (node1, table_name, 10, [\"num\", \"foo3\"], 5, True)))\n+            tasks.append(p.apply_async(insert, (node2, table_name, 10, [\"num\", \"num2\"], 5, True)))\n+            tasks.append(p.apply_async(insert, (node3, table_name, 10, [\"num\", \"foo2\"], 5, True)))\n+            tasks.append(p.apply_async(select, (node1, table_name, \"foo2\", None, 5, True)))\n+            tasks.append(p.apply_async(select, (node2, table_name, \"foo3\", None, 5, True)))\n+            tasks.append(p.apply_async(select, (node3, table_name, \"num2\", None, 5, True)))\n+        for task in tasks:\n+            task.get(timeout=240)\n+\n+        rename_column_on_cluster(node1, table_name, 'foo2', 'num2', 1, True)\n+        rename_column_on_cluster(node1, '%s_replicated' % table_name, 'foo2', 'num2', 1, True)\n+        rename_column_on_cluster(node1, table_name, 'foo3', 'num2', 1, True)\n+        rename_column_on_cluster(node1, '%s_replicated' % table_name, 'foo3', 'num2', 1, True)\n+\n+        insert(node1, table_name, 1000, col_names=['num','num2'])\n+        select(node1, table_name, \"num2\")\n+        select(node2, table_name, \"num2\")\n+        select(node3, table_name, \"num2\")\n+        select(node4, table_name, \"num2\")\n+    finally:\n+        drop_distributed_table(node1, table_name)\ndiff --git a/tests/queries/0_stateless/01274_alter_rename_column_distributed.reference b/tests/queries/0_stateless/01274_alter_rename_column_distributed.reference\nnew file mode 100644\nindex 000000000000..7ed221867c19\n--- /dev/null\n+++ b/tests/queries/0_stateless/01274_alter_rename_column_distributed.reference\n@@ -0,0 +1,4 @@\n+2020-01-01\thello\n+2020-01-01\thello\n+2020-01-02\thello2\n+2020-01-02\thello2\ndiff --git a/tests/queries/0_stateless/01274_alter_rename_column_distributed.sql b/tests/queries/0_stateless/01274_alter_rename_column_distributed.sql\nnew file mode 100644\nindex 000000000000..a35dc7cca567\n--- /dev/null\n+++ b/tests/queries/0_stateless/01274_alter_rename_column_distributed.sql\n@@ -0,0 +1,17 @@\n+DROP TABLE IF EXISTS visits;\n+DROP TABLE IF EXISTS visits_dist;\n+\n+CREATE TABLE visits(StartDate Date, Name String) ENGINE MergeTree ORDER BY(StartDate);\n+CREATE TABLE visits_dist AS visits ENGINE Distributed(test_cluster_two_shards_localhost,  currentDatabase(), 'visits', rand());\n+\n+INSERT INTO visits_dist (StartDate, Name) VALUES ('2020-01-01', 'hello');\n+INSERT INTO visits_dist (StartDate, Name) VALUES ('2020-01-02', 'hello2');\n+\n+ALTER TABLE visits RENAME COLUMN Name TO Name2;\n+ALTER TABLE visits_dist RENAME COLUMN Name TO Name2;\n+\n+SELECT * FROM visits_dist ORDER BY StartDate, Name2;\n+\n+DROP TABLE visits;\n+DROP TABLE visits_dist;\n+\ndiff --git a/tests/queries/0_stateless/01275_alter_rename_column_default_expr.reference b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.reference\nnew file mode 100644\nindex 000000000000..d81601b92c5c\n--- /dev/null\n+++ b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.reference\n@@ -0,0 +1,87 @@\n+2019-10-01\t0\t0\t1\t0 + 1\n+2019-10-02\t1\t1\t2\t1 + 2\n+2019-10-03\t2\t2\t3\t2 + 3\n+2019-10-01\t3\t3\t4\t3 + 4\n+2019-10-02\t4\t4\t5\t4 + 5\n+2019-10-03\t5\t5\t6\t5 + 6\n+2019-10-01\t6\t6\t7\t6 + 7\n+2019-10-02\t7\t7\t8\t7 + 8\n+2019-10-03\t8\t8\t9\t8 + 9\n+CREATE TABLE default.table_for_rename\\n(\\n    `date` Date, \\n    `key` UInt64, \\n    `value4` String, \\n    `value5` String, \\n    `value3` String DEFAULT concat(value4, \\' + \\', value5)\\n)\\nENGINE = MergeTree()\\nPARTITION BY date\\nORDER BY key\\nSETTINGS index_granularity = 8192\n+2019-10-01\t0\t0\t1\t0 + 1\n+2019-10-02\t1\t1\t2\t1 + 2\n+2019-10-03\t2\t2\t3\t2 + 3\n+2019-10-01\t3\t3\t4\t3 + 4\n+2019-10-02\t4\t4\t5\t4 + 5\n+2019-10-03\t5\t5\t6\t5 + 6\n+2019-10-01\t6\t6\t7\t6 + 7\n+2019-10-02\t7\t7\t8\t7 + 8\n+2019-10-03\t8\t8\t9\t8 + 9\n+2019-10-01\t0\t0\t1\t0 + 1\n+2019-10-02\t1\t1\t2\t1 + 2\n+2019-10-03\t2\t2\t3\t2 + 3\n+2019-10-01\t3\t3\t4\t3 + 4\n+2019-10-02\t4\t4\t5\t4 + 5\n+2019-10-03\t5\t5\t6\t5 + 6\n+2019-10-01\t6\t6\t7\t6 + 7\n+2019-10-02\t7\t7\t8\t7 + 8\n+2019-10-03\t8\t8\t9\t8 + 9\n+2019-10-02\t10\t10\t11\t10 + 11\n+2019-10-03\t11\t11\t12\t11 + 12\n+2019-10-01\t12\t12\t13\t12 + 13\n+2019-10-02\t13\t13\t14\t13 + 14\n+2019-10-03\t14\t14\t15\t14 + 15\n+2019-10-01\t15\t15\t16\t15 + 16\n+2019-10-02\t16\t16\t17\t16 + 17\n+2019-10-03\t17\t17\t18\t17 + 18\n+2019-10-01\t18\t18\t19\t18 + 19\n+2019-10-02\t19\t19\t20\t19 + 20\n+CREATE TABLE default.table_for_rename\\n(\\n    `date` Date, \\n    `key` UInt64, \\n    `value1` String, \\n    `value2` String, \\n    `value3` String DEFAULT concat(value1, \\' + \\', value2)\\n)\\nENGINE = MergeTree()\\nPARTITION BY date\\nORDER BY key\\nSETTINGS index_granularity = 8192\n+2019-10-01\t0\t0\t1\t0 + 1\n+2019-10-02\t1\t1\t2\t1 + 2\n+2019-10-03\t2\t2\t3\t2 + 3\n+2019-10-01\t3\t3\t4\t3 + 4\n+2019-10-02\t4\t4\t5\t4 + 5\n+2019-10-03\t5\t5\t6\t5 + 6\n+2019-10-01\t6\t6\t7\t6 + 7\n+2019-10-02\t7\t7\t8\t7 + 8\n+2019-10-03\t8\t8\t9\t8 + 9\n+2019-10-02\t10\t10\t11\t10 + 11\n+2019-10-03\t11\t11\t12\t11 + 12\n+2019-10-01\t12\t12\t13\t12 + 13\n+2019-10-02\t13\t13\t14\t13 + 14\n+2019-10-03\t14\t14\t15\t14 + 15\n+2019-10-01\t15\t15\t16\t15 + 16\n+2019-10-02\t16\t16\t17\t16 + 17\n+2019-10-03\t17\t17\t18\t17 + 18\n+2019-10-01\t18\t18\t19\t18 + 19\n+2019-10-02\t19\t19\t20\t19 + 20\n+2019-10-01\t0\t0\t1\t0 + 1\n+2019-10-02\t1\t1\t2\t1 + 2\n+2019-10-03\t2\t2\t3\t2 + 3\n+2019-10-01\t3\t3\t4\t3 + 4\n+2019-10-02\t4\t4\t5\t4 + 5\n+2019-10-03\t5\t5\t6\t5 + 6\n+2019-10-01\t6\t6\t7\t6 + 7\n+2019-10-02\t7\t7\t8\t7 + 8\n+2019-10-03\t8\t8\t9\t8 + 9\n+2019-10-02\t10\t10\t11\t10 + 11\n+2019-10-03\t11\t11\t12\t11 + 12\n+2019-10-01\t12\t12\t13\t12 + 13\n+2019-10-02\t13\t13\t14\t13 + 14\n+2019-10-03\t14\t14\t15\t14 + 15\n+2019-10-01\t15\t15\t16\t15 + 16\n+2019-10-02\t16\t16\t17\t16 + 17\n+2019-10-03\t17\t17\t18\t17 + 18\n+2019-10-01\t18\t18\t19\t18 + 19\n+2019-10-02\t19\t19\t20\t19 + 20\n+2019-10-03\t20\t20\t21\t20 + 21\n+2019-10-01\t21\t21\t22\t21 + 22\n+2019-10-02\t22\t22\t23\t22 + 23\n+2019-10-03\t23\t23\t24\t23 + 24\n+2019-10-01\t24\t24\t25\t24 + 25\n+2019-10-02\t25\t25\t26\t25 + 26\n+2019-10-03\t26\t26\t27\t26 + 27\n+2019-10-01\t27\t27\t28\t27 + 28\n+2019-10-02\t28\t28\t29\t28 + 29\n+2019-10-03\t29\t29\t30\t29 + 30\ndiff --git a/tests/queries/0_stateless/01275_alter_rename_column_default_expr.sql b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.sql\nnew file mode 100644\nindex 000000000000..21106d200afc\n--- /dev/null\n+++ b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.sql\n@@ -0,0 +1,34 @@\n+DROP TABLE IF EXISTS table_for_rename;\n+\n+CREATE TABLE table_for_rename\n+(\n+  date Date,\n+  key UInt64,\n+  value1 String,\n+  value2 String,\n+  value3 String DEFAULT concat(value1, ' + ', value2) \n+)\n+ENGINE = MergeTree()\n+PARTITION BY date\n+ORDER BY key;\n+\n+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(9);\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+ALTER TABLE table_for_rename RENAME COLUMN value1 TO value4;\n+ALTER TABLE table_for_rename RENAME COLUMN value2 TO value5;\n+SHOW CREATE TABLE table_for_rename;\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+INSERT INTO table_for_rename (date, key, value4, value5) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(10, 10);\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+ALTER TABLE table_for_rename RENAME COLUMN value4 TO value1;\n+ALTER TABLE table_for_rename RENAME COLUMN value5 TO value2;\n+SHOW CREATE TABLE table_for_rename;\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(20,10);\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+DROP TABLE IF EXISTS table_for_rename;\ndiff --git a/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.reference b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.reference\nnew file mode 100644\nindex 000000000000..5d721230db30\n--- /dev/null\n+++ b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.reference\n@@ -0,0 +1,90 @@\n+2019-10-01\t0\t0\t1\n+2019-10-02\t1\t1\t2\n+2019-10-03\t2\t2\t3\n+2019-10-01\t3\t3\t4\n+2019-10-02\t4\t4\t5\n+2019-10-03\t5\t5\t6\n+2019-10-01\t6\t6\t7\n+2019-10-02\t7\t7\t8\n+2019-10-03\t8\t8\t9\n+CREATE TABLE default.table_for_rename\\n(\\n    `date` Date, \\n    `key` UInt64, \\n    `value4` String, \\n    `value5` String, \\n    `value3` String MATERIALIZED concat(value4, \\' + \\', value5)\\n)\\nENGINE = MergeTree()\\nPARTITION BY date\\nORDER BY key\\nSETTINGS index_granularity = 8192\n+2019-10-01\t0\t0\t1\n+2019-10-02\t1\t1\t2\n+2019-10-03\t2\t2\t3\n+2019-10-01\t3\t3\t4\n+2019-10-02\t4\t4\t5\n+2019-10-03\t5\t5\t6\n+2019-10-01\t6\t6\t7\n+2019-10-02\t7\t7\t8\n+2019-10-03\t8\t8\t9\n+-- insert after rename --\n+2019-10-01\t0\t0\t1\n+2019-10-02\t1\t1\t2\n+2019-10-03\t2\t2\t3\n+2019-10-01\t3\t3\t4\n+2019-10-02\t4\t4\t5\n+2019-10-03\t5\t5\t6\n+2019-10-01\t6\t6\t7\n+2019-10-02\t7\t7\t8\n+2019-10-03\t8\t8\t9\n+2019-10-02\t10\t10\t11\n+2019-10-03\t11\t11\t12\n+2019-10-01\t12\t12\t13\n+2019-10-02\t13\t13\t14\n+2019-10-03\t14\t14\t15\n+2019-10-01\t15\t15\t16\n+2019-10-02\t16\t16\t17\n+2019-10-03\t17\t17\t18\n+2019-10-01\t18\t18\t19\n+2019-10-02\t19\t19\t20\n+-- rename columns back --\n+CREATE TABLE default.table_for_rename\\n(\\n    `date` Date, \\n    `key` UInt64, \\n    `value1` String, \\n    `value2` String, \\n    `value3` String MATERIALIZED concat(value1, \\' + \\', value2)\\n)\\nENGINE = MergeTree()\\nPARTITION BY date\\nORDER BY key\\nSETTINGS index_granularity = 8192\n+2019-10-01\t0\t0\t1\n+2019-10-02\t1\t1\t2\n+2019-10-03\t2\t2\t3\n+2019-10-01\t3\t3\t4\n+2019-10-02\t4\t4\t5\n+2019-10-03\t5\t5\t6\n+2019-10-01\t6\t6\t7\n+2019-10-02\t7\t7\t8\n+2019-10-03\t8\t8\t9\n+2019-10-02\t10\t10\t11\n+2019-10-03\t11\t11\t12\n+2019-10-01\t12\t12\t13\n+2019-10-02\t13\t13\t14\n+2019-10-03\t14\t14\t15\n+2019-10-01\t15\t15\t16\n+2019-10-02\t16\t16\t17\n+2019-10-03\t17\t17\t18\n+2019-10-01\t18\t18\t19\n+2019-10-02\t19\t19\t20\n+-- insert after rename column --\n+2019-10-01\t0\t0\t1\n+2019-10-02\t1\t1\t2\n+2019-10-03\t2\t2\t3\n+2019-10-01\t3\t3\t4\n+2019-10-02\t4\t4\t5\n+2019-10-03\t5\t5\t6\n+2019-10-01\t6\t6\t7\n+2019-10-02\t7\t7\t8\n+2019-10-03\t8\t8\t9\n+2019-10-02\t10\t10\t11\n+2019-10-03\t11\t11\t12\n+2019-10-01\t12\t12\t13\n+2019-10-02\t13\t13\t14\n+2019-10-03\t14\t14\t15\n+2019-10-01\t15\t15\t16\n+2019-10-02\t16\t16\t17\n+2019-10-03\t17\t17\t18\n+2019-10-01\t18\t18\t19\n+2019-10-02\t19\t19\t20\n+2019-10-03\t20\t20\t21\n+2019-10-01\t21\t21\t22\n+2019-10-02\t22\t22\t23\n+2019-10-03\t23\t23\t24\n+2019-10-01\t24\t24\t25\n+2019-10-02\t25\t25\t26\n+2019-10-03\t26\t26\t27\n+2019-10-01\t27\t27\t28\n+2019-10-02\t28\t28\t29\n+2019-10-03\t29\t29\t30\ndiff --git a/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.sql b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.sql\nnew file mode 100644\nindex 000000000000..9089c52edf60\n--- /dev/null\n+++ b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.sql\n@@ -0,0 +1,37 @@\n+DROP TABLE IF EXISTS table_for_rename;\n+\n+CREATE TABLE table_for_rename\n+(\n+  date Date,\n+  key UInt64,\n+  value1 String,\n+  value2 String,\n+  value3 String MATERIALIZED concat(value1, ' + ', value2) \n+)\n+ENGINE = MergeTree()\n+PARTITION BY date\n+ORDER BY key;\n+\n+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(9);\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+ALTER TABLE table_for_rename RENAME COLUMN value1 TO value4;\n+ALTER TABLE table_for_rename RENAME COLUMN value2 TO value5;\n+SHOW CREATE TABLE table_for_rename;\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+SELECT '-- insert after rename --';\n+INSERT INTO table_for_rename (date, key, value4, value5) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(10, 10);\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+SELECT '-- rename columns back --';\n+ALTER TABLE table_for_rename RENAME COLUMN value4 TO value1;\n+ALTER TABLE table_for_rename RENAME COLUMN value5 TO value2;\n+SHOW CREATE TABLE table_for_rename;\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+SELECT '-- insert after rename column --';\n+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(20,10);\n+SELECT * FROM table_for_rename ORDER BY key;\n+\n+DROP TABLE IF EXISTS table_for_rename;\n",
  "problem_statement": "Renaming column that is used inside the TTL expression breaks TTL operations\nRenaming column that is used inside the TTL expression breaks TTL operations.\r\n\r\nTo reproduce the issue see test https://github.com/ClickHouse/ClickHouse/blob/089371ddfb596a7b052ac709a5c2778e76f349c0/tests/integration/test_rename_column/test.py#L408 and comment out line https://github.com/ClickHouse/ClickHouse/blob/089371ddfb596a7b052ac709a5c2778e76f349c0/tests/integration/test_rename_column/test.py#L421 where we rename the column back to its original name.\r\nWithout this line test will fail and no parts are moved according to the TTL expression.\r\n\n",
  "hints_text": "",
  "created_at": "2020-05-13T13:02:25Z"
}