diff --git a/tests/integration/test_rename_column/__init__.py b/tests/integration/test_rename_column/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/tests/integration/test_rename_column/configs/config.d/instant_moves.xml b/tests/integration/test_rename_column/configs/config.d/instant_moves.xml
new file mode 100644
index 000000000000..7b68c6946caf
--- /dev/null
+++ b/tests/integration/test_rename_column/configs/config.d/instant_moves.xml
@@ -0,0 +1,4 @@
+<yandex>
+    <background_move_processing_pool_thread_sleep_seconds>0.5</background_move_processing_pool_thread_sleep_seconds>
+    <background_move_processing_pool_task_sleep_seconds_when_no_work_max>0.5</background_move_processing_pool_task_sleep_seconds_when_no_work_max>
+</yandex>
diff --git a/tests/integration/test_rename_column/configs/config.d/part_log.xml b/tests/integration/test_rename_column/configs/config.d/part_log.xml
new file mode 100644
index 000000000000..53ea0a8fc132
--- /dev/null
+++ b/tests/integration/test_rename_column/configs/config.d/part_log.xml
@@ -0,0 +1,7 @@
+<yandex>
+    <part_log>
+        <database>system</database>
+        <table>part_log</table>
+        <flush_interval_milliseconds>500</flush_interval_milliseconds>
+    </part_log>
+</yandex>
diff --git a/tests/integration/test_rename_column/configs/config.d/storage_configuration.xml b/tests/integration/test_rename_column/configs/config.d/storage_configuration.xml
new file mode 100644
index 000000000000..131219abf3de
--- /dev/null
+++ b/tests/integration/test_rename_column/configs/config.d/storage_configuration.xml
@@ -0,0 +1,28 @@
+<yandex>
+
+<storage_configuration>
+    <disks>
+        <default>
+        </default>
+        <internal>
+            <path>/internal/</path>
+        </internal>
+        <external>
+            <path>/external/</path>
+        </external>
+    </disks>
+    <policies>
+        <default_with_external>
+            <volumes>
+                <internal>
+                    <disk>internal</disk>
+                </internal>
+                <external>
+                    <disk>external</disk>
+                </external>
+            </volumes>
+        </default_with_external>
+    </policies>
+</storage_configuration>
+
+</yandex>
diff --git a/tests/integration/test_rename_column/configs/config.d/zookeeper_session_timeout.xml b/tests/integration/test_rename_column/configs/config.d/zookeeper_session_timeout.xml
new file mode 100644
index 000000000000..caa0ff11137d
--- /dev/null
+++ b/tests/integration/test_rename_column/configs/config.d/zookeeper_session_timeout.xml
@@ -0,0 +1,6 @@
+<yandex>
+    <zookeeper>
+        <!-- Required for correct timing in current test case -->
+        <session_timeout_ms replace="1">15000</session_timeout_ms>
+    </zookeeper>
+</yandex>
diff --git a/tests/integration/test_rename_column/configs/remote_servers.xml b/tests/integration/test_rename_column/configs/remote_servers.xml
new file mode 100644
index 000000000000..4c3de4b39054
--- /dev/null
+++ b/tests/integration/test_rename_column/configs/remote_servers.xml
@@ -0,0 +1,28 @@
+<yandex>
+    <remote_servers>
+        <test_cluster>
+            <shard>
+                <internal_replication>true</internal_replication>
+                <replica>
+                    <host>node1</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>node2</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+            <shard>
+                <internal_replication>true</internal_replication>
+                <replica>
+                    <host>node3</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>node4</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </test_cluster>
+    </remote_servers>
+</yandex>
diff --git a/tests/integration/test_rename_column/test.py b/tests/integration/test_rename_column/test.py
new file mode 100644
index 000000000000..9fac783e7120
--- /dev/null
+++ b/tests/integration/test_rename_column/test.py
@@ -0,0 +1,555 @@
+from __future__ import print_function
+
+import time
+import random
+import pytest
+
+from multiprocessing.dummy import Pool
+from helpers.cluster import ClickHouseCluster
+from helpers.client import QueryRuntimeException
+from helpers.network import PartitionManager
+from helpers.test_tools import TSV
+
+node_options = dict(
+    with_zookeeper=True,
+    main_configs=['configs/remote_servers.xml'],
+    config_dir='configs',
+    tmpfs=['/external:size=200M', '/internal:size=1M'])
+
+cluster = ClickHouseCluster(__file__)
+node1 = cluster.add_instance('node1', macros={"shard": 0, "replica": 1}, **node_options)
+node2 = cluster.add_instance('node2', macros={"shard": 0, "replica": 2}, **node_options)
+node3 = cluster.add_instance('node3', macros={"shard": 1, "replica": 1}, **node_options)
+node4 = cluster.add_instance('node4', macros={"shard": 1, "replica": 2}, **node_options)
+nodes = [node1, node2, node3, node4]
+
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    try:
+        cluster.start()
+        yield cluster
+    except Exception as ex:
+        print(ex)
+    finally:
+        cluster.shutdown()
+
+
+def drop_table(nodes, table_name):
+    for node in nodes:
+        node.query("DROP TABLE IF EXISTS {} NO DELAY".format(table_name))
+    time.sleep(1)
+
+
+def create_table(nodes, table_name, with_storage_policy=False, with_time_column=False,
+                 with_ttl_move=False, with_ttl_delete=False):
+    extra_columns = ""
+    settings = []
+
+    for node in nodes:
+        sql = """
+            CREATE TABLE {table_name}
+                (
+                    num UInt32,
+                    num2 UInt32 DEFAULT num + 1{extra_columns}
+                )
+                ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/{table_name}', '{replica}')
+                ORDER BY num PARTITION BY num % 100
+            """
+        if with_ttl_move:
+            sql += """
+                TTL time + INTERVAL (num2 % 1) SECOND TO DISK 'external'
+            """
+        if with_ttl_delete:
+            sql += """
+                TTL time + INTERVAL (num2 % 1) SECOND DELETE
+            """
+            settings.append("merge_with_ttl_timeout = 1")
+
+        if with_storage_policy:
+            settings.append("storage_policy='default_with_external'")
+
+        if settings:
+            sql += """
+                SETTINGS {}
+            """.format(", ".join(settings))
+
+        if with_time_column:
+            extra_columns = """,
+                    time DateTime
+                """
+        node.query(sql.format(table_name=table_name, replica=node.name, extra_columns=extra_columns))
+
+
+def create_distributed_table(node, table_name):
+        sql = """
+            CREATE TABLE %(table_name)s_replicated ON CLUSTER test_cluster
+            (
+                num UInt32,
+                num2 UInt32 DEFAULT num + 1
+            )
+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/{shard}/%(table_name)s_replicated', '{replica}')
+            ORDER BY num PARTITION BY num %% 100;
+        """ % dict(table_name=table_name)
+        node.query(sql)
+        sql = """
+            CREATE TABLE %(table_name)s ON CLUSTER test_cluster AS %(table_name)s_replicated
+            ENGINE = Distributed(test_cluster, default, %(table_name)s_replicated, rand())
+        """ % dict(table_name=table_name)
+        node.query(sql)
+
+
+def drop_distributed_table(node, table_name):
+    node.query("DROP TABLE IF EXISTS {} ON CLUSTER test_cluster".format(table_name))
+    node.query("DROP TABLE IF EXISTS {}_replicated ON CLUSTER test_cluster".format(table_name))
+    time.sleep(1)
+
+
+def insert(node, table_name, chunk=1000, col_names=None, iterations=1, ignore_exception=False,
+        slow=False, with_many_parts=False, offset=0, with_time_column=False):
+    if col_names is None:
+        col_names = ['num', 'num2']
+    for i in range(iterations):
+        try:
+            query = ["SET max_partitions_per_insert_block = 10000000"]
+            if with_many_parts:
+                query.append("SET max_insert_block_size = 64")
+            if with_time_column:
+                query.append(
+                    "INSERT INTO {table_name} ({col0}, {col1}, time) SELECT number AS {col0}, number + 1 AS {col1}, now() + 10 AS time FROM numbers_mt({chunk})"
+                .format(table_name=table_name, chunk=chunk, col0=col_names[0], col1=col_names[1]))
+            elif slow:
+                query.append(
+                    "INSERT INTO {table_name} ({col0}, {col1}) SELECT number + sleepEachRow(0.001) AS {col0}, number + 1 AS {col1} FROM numbers_mt({chunk})"
+                .format(table_name=table_name, chunk=chunk, col0=col_names[0], col1=col_names[1]))
+            else:
+                query.append(
+                    "INSERT INTO {table_name} ({col0},{col1}) SELECT number + {offset} AS {col0}, number + 1 + {offset} AS {col1} FROM numbers_mt({chunk})"
+                .format(table_name=table_name, chunk=chunk, col0=col_names[0], col1=col_names[1], offset=str(offset)))
+            node.query(";
".join(query))
+        except QueryRuntimeException as ex:
+            if not ignore_exception:
+                raise
+
+
+def select(node, table_name, col_name="num", expected_result=None, iterations=1, ignore_exception=False, slow=False, poll=None):
+    for i in range(iterations):
+        start_time = time.time()
+        while True:
+            try:
+                if slow:
+                    r = node.query("SELECT count() FROM (SELECT num2, sleepEachRow(0.5) FROM {} WHERE {} % 1000 > 0)".format(table_name, col_name))
+                else:
+                    r = node.query("SELECT count() FROM {} WHERE {} % 1000 > 0".format(table_name, col_name))
+                if expected_result:
+                    if r != expected_result and poll and time.time() - start_time < poll:
+                        continue
+                    assert r == expected_result
+            except QueryRuntimeException as ex:
+                if not ignore_exception:
+                    raise
+            break
+
+
+def rename_column(node, table_name, name, new_name, iterations=1, ignore_exception=False):
+    for i in range(iterations):
+        try:
+            node.query("ALTER TABLE {table_name} RENAME COLUMN {name} to {new_name}".format(
+                table_name=table_name, name=name, new_name=new_name
+            ))
+        except QueryRuntimeException as ex:
+            if not ignore_exception:
+                raise
+
+
+def rename_column_on_cluster(node, table_name, name, new_name, iterations=1, ignore_exception=False):
+    for i in range(iterations):
+        try:
+            node.query("ALTER TABLE {table_name} ON CLUSTER test_cluster RENAME COLUMN {name} to {new_name}".format(
+                table_name=table_name, name=name, new_name=new_name
+            ))
+        except QueryRuntimeException as ex:
+            if not ignore_exception:
+                raise
+
+
+def alter_move(node, table_name, iterations=1, ignore_exception=False):
+    for i in range(iterations):
+        move_part = random.randint(0, 99)
+        move_volume = 'external'
+        try:
+            node.query("ALTER TABLE {table_name} MOVE PARTITION '{move_part}' TO VOLUME '{move_volume}'"
+                .format(table_name=table_name, move_part=move_part, move_volume=move_volume))
+        except QueryRuntimeException as ex:
+            if not ignore_exception:
+                raise
+
+
+def test_rename_parallel_same_node(started_cluster):
+    table_name = "test_rename_parallel_same_node"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name)
+        insert(node1, table_name, 1000)
+
+        p = Pool(15)
+        tasks = []
+        for i in range(1):
+            tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node1, table_name, "foo2", "foo3", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node1, table_name, "foo3", "num2", 5, True)))
+        for task in tasks:
+            task.get(timeout=240)
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+
+        # check that select still works
+        select(node1, table_name, "num2", "999
")
+    finally:
+        drop_table(nodes, table_name)
+
+
+def test_rename_parallel(started_cluster):
+    table_name = "test_rename_parallel"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name)
+        insert(node1, table_name, 1000)
+
+        p = Pool(15)
+        tasks = []
+        for i in range(1):
+            tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node2, table_name, "foo2", "foo3", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node3, table_name, "foo3", "num2", 5, True)))
+        for task in tasks:
+            task.get(timeout=240)
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+
+        # check that select still works
+        select(node1, table_name, "num2", "999
")
+    finally:
+        drop_table(nodes, table_name)
+
+
+def test_rename_with_parallel_select(started_cluster):
+    table_name = "test_rename_with_parallel_select"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name)
+        insert(node1, table_name, 1000)
+
+        select(node1, table_name, "num2", "999
", poll=30)
+        select(node2, table_name, "num2", "999
", poll=30)
+        select(node3, table_name, "num2", "999
", poll=30)
+
+        p = Pool(15)
+        tasks = []
+        for i in range(1):
+            tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node2, table_name, "foo2", "foo3", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node3, table_name, "foo3", "num2", 5, True)))
+            tasks.append(p.apply_async(select, (node1, table_name, "foo3", "999
", 5, True)))
+            tasks.append(p.apply_async(select, (node2, table_name, "num2", "999
", 5, True)))
+            tasks.append(p.apply_async(select, (node3, table_name, "foo2", "999
", 5, True)))
+        for task in tasks:
+            task.get(timeout=240)
+
+        # rename column back to original name
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+
+        # check that select still works
+        select(node1, table_name, "num2", "999
")
+    finally:
+        drop_table(nodes, table_name)
+
+
+def test_rename_with_parallel_insert(started_cluster):
+    table_name = "test_rename_with_parallel_insert"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name)
+        insert(node1, table_name, 1000)
+
+        p = Pool(15)
+        tasks = []
+        for i in range(1):
+            tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node2, table_name, "foo2", "foo3", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node3, table_name, "foo3", "num2", 5, True)))
+            tasks.append(p.apply_async(insert, (node1, table_name, 100, ["num", "foo3"], 5, True)))
+            tasks.append(p.apply_async(insert, (node2, table_name, 100, ["num", "num2"], 5, True)))
+            tasks.append(p.apply_async(insert, (node3, table_name, 100, ["num", "foo2"], 5, True)))
+        for task in tasks:
+            task.get(timeout=240)
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+
+        # check that select still works
+        select(node1, table_name, "num2")
+    finally:
+        drop_table(nodes, table_name)
+
+
+@pytest.mark.skip(reason="For unknown reason one of these tests kill Zookeeper")
+def test_rename_with_parallel_merges(started_cluster):
+    table_name = "test_rename_with_parallel_merges"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name)
+        for i in range(20):
+            insert(node1, table_name, 100, ["num","num2"], 1, False, False, True, offset=i*100)
+
+        def merge_parts(node, table_name, iterations=1):
+            for i in range(iterations):
+                node.query("OPTIMIZE TABLE %s FINAL" % table_name)
+
+        p = Pool(15)
+        tasks = []
+        for i in range(1):
+            tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node2, table_name, "foo2", "foo3", 5, True)))
+            tasks.append(p.apply_async(rename_column, (node3, table_name, "foo3", "num2", 5, True)))
+            tasks.append(p.apply_async(merge_parts, (node1, table_name, 5)))
+            tasks.append(p.apply_async(merge_parts, (node2, table_name, 5)))
+            tasks.append(p.apply_async(merge_parts, (node3, table_name, 5)))
+
+        for task in tasks:
+            task.get(timeout=240)
+
+        # rename column back to the original name
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+
+        # check that select still works
+        select(node1, table_name, "num2", "1998
")
+        select(node2, table_name, "num2", "1998
")
+        select(node3, table_name, "num2", "1998
")
+    finally:
+        drop_table(nodes, table_name)
+
+
+@pytest.mark.skip(reason="For unknown reason one of these tests kill Zookeeper")
+def test_rename_with_parallel_slow_insert(started_cluster):
+    table_name = "test_rename_with_parallel_slow_insert"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name)
+        insert(node1, table_name, 1000)
+
+        p = Pool(15)
+        tasks = []
+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, ["num", "num2"], 1, False, True)))
+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, ["num", "num2"], 1, True, True))) # deduplicated
+        time.sleep(0.5)
+        tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2")))
+
+        for task in tasks:
+            task.get(timeout=240)
+
+        insert(node1, table_name, 100, ["num", "foo2"])
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo2", "num2")
+
+        # check that select still works
+        select(node1, table_name, "num2", "11089
")
+        select(node2, table_name, "num2", "11089
", poll=30)
+        select(node3, table_name, "num2", "11089
", poll=30)
+    finally:
+        drop_table(nodes, table_name)
+
+
+def test_rename_with_parallel_slow_select(started_cluster):
+    table_name = "test_rename_with_parallel_slow_select"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name)
+        insert(node1, table_name, 1000)
+
+        p = Pool(15)
+        tasks = []
+
+        tasks.append(p.apply_async(select, (node1, table_name, "num2", "999
", 1, True, True)))
+        time.sleep(0.5)
+        tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2")))
+
+        for task in tasks:
+            task.get(timeout=240)
+
+        insert(node1, table_name, 100, ["num", "foo2"])
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo2", "num2")
+
+        # check that select still works
+        select(node1, table_name, "num2", "1099
")
+        select(node2, table_name, "num2", "1099
", poll=30)
+        select(node3, table_name, "num2", "1099
", poll=30)
+    finally:
+        drop_table(nodes, table_name)
+
+
+def test_rename_with_parallel_moves(started_cluster):
+    table_name = "test_rename_with_parallel_moves"
+    drop_table(nodes, table_name)
+    try:
+        create_table(nodes, table_name, with_storage_policy=True)
+        insert(node1, table_name, 1000)
+
+        p = Pool(15)
+        tasks = []
+
+        tasks.append(p.apply_async(alter_move, (node1, table_name, 20, True)))
+        tasks.append(p.apply_async(alter_move, (node2, table_name, 20, True)))
+        tasks.append(p.apply_async(alter_move, (node3, table_name, 20, True)))
+        tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 20, True)))
+        tasks.append(p.apply_async(rename_column, (node2, table_name, "foo2", "foo3", 20, True)))
+        tasks.append(p.apply_async(rename_column, (node3, table_name, "num3", "num2", 20, True)))
+
+        for task in tasks:
+            task.get(timeout=240)
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+
+        # check that select still works
+        select(node1, table_name, "num2", "999
")
+        select(node2, table_name, "num2", "999
", poll=30)
+        select(node3, table_name, "num2", "999
", poll=30)
+    finally:
+        drop_table(nodes, table_name)
+
+
+def test_rename_with_parallel_ttl_move(started_cluster):
+    table_name = 'test_rename_with_parallel_ttl_move'
+    try:
+        create_table(nodes, table_name, with_storage_policy=True, with_time_column=True, with_ttl_move=True)
+        rename_column(node1, table_name, "time", "time2", 1, False)
+        rename_column(node1, table_name, "time2", "time", 1, False)
+
+        p = Pool(15)
+        tasks = []
+
+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, ["num", "num2"], 1, False, False, True, 0, True)))
+        time.sleep(5)
+        rename_column(node1, table_name, "time", "time2", 1, False)
+        time.sleep(4)
+        tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 20, True)))
+        tasks.append(p.apply_async(rename_column, (node2, table_name, "foo2", "foo3", 20, True)))
+        tasks.append(p.apply_async(rename_column, (node3, table_name, "num3", "num2", 20, True)))
+
+        for task in tasks:
+            task.get(timeout=240)
+
+        # check some parts got moved
+        assert "external" in set(node1.query("SELECT disk_name FROM system.parts WHERE table == '{}' AND active=1 ORDER BY modification_time".format(table_name)).strip().splitlines())
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+
+        # check that select still works
+        select(node1, table_name, "num2", "9990
")
+    finally:
+        drop_table(nodes, table_name)
+
+
+def test_rename_with_parallel_ttl_delete(started_cluster):
+    table_name = 'test_rename_with_parallel_ttl_delete'
+    try:
+        create_table(nodes, table_name, with_time_column=True, with_ttl_delete=True)
+        rename_column(node1, table_name, "time", "time2", 1, False)
+        rename_column(node1, table_name, "time2", "time", 1, False)
+
+        def merge_parts(node, table_name, iterations=1):
+            for i in range(iterations):
+                node.query("OPTIMIZE TABLE {}".format(table_name))
+
+        p = Pool(15)
+        tasks = []
+
+        tasks.append(p.apply_async(insert, (node1, table_name, 10000, ["num", "num2"], 1, False, False, True, 0, True)))
+        time.sleep(15)
+        tasks.append(p.apply_async(rename_column, (node1, table_name, "num2", "foo2", 20, True)))
+        tasks.append(p.apply_async(rename_column, (node2, table_name, "foo2", "foo3", 20, True)))
+        tasks.append(p.apply_async(rename_column, (node3, table_name, "num3", "num2", 20, True)))
+        tasks.append(p.apply_async(merge_parts, (node1, table_name, 20)))
+        tasks.append(p.apply_async(merge_parts, (node2, table_name, 20)))
+        tasks.append(p.apply_async(merge_parts, (node3, table_name, 20)))
+
+        for task in tasks:
+            task.get(timeout=240)
+
+        # rename column back to original
+        rename_column(node1, table_name, "foo2", "num2", 1, True)
+        rename_column(node1, table_name, "foo3", "num2", 1, True)
+
+        assert int(node1.query("SELECT count() FROM {}".format(table_name)).strip()) < 10000
+    finally:
+        drop_table(nodes, table_name)
+
+
+@pytest.mark.skip(reason="For unknown reason one of these tests kill Zookeeper")
+def test_rename_distributed(started_cluster):
+    table_name = 'test_rename_distributed'
+    try:
+        create_distributed_table(node1, table_name)
+        insert(node1, table_name, 1000)
+
+        rename_column_on_cluster(node1, table_name, 'num2', 'foo2')
+        rename_column_on_cluster(node1, '%s_replicated' % table_name, 'num2', 'foo2')
+
+        insert(node1, table_name, 1000, col_names=['num','foo2'])
+
+        select(node1, table_name, "foo2", '1998
', poll=30)
+    finally:
+        drop_distributed_table(node1, table_name)
+
+
+@pytest.mark.skip(reason="For unknown reason one of these tests kill Zookeeper")
+def test_rename_distributed_parallel_insert_and_select(started_cluster):
+    table_name = 'test_rename_distributed_parallel_insert_and_select'
+    try:
+        create_distributed_table(node1, table_name)
+        insert(node1, table_name, 1000)
+
+        p = Pool(15)
+        tasks = []
+        for i in range(1):
+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, table_name, 'num2', 'foo2', 3, True)))
+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, '%s_replicated' % table_name, 'num2', 'foo2', 3, True)))
+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, table_name, 'foo2', 'foo3', 3, True)))
+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, '%s_replicated' % table_name, 'foo2', 'foo3', 3, True)))
+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, table_name, 'foo3', 'num2', 3, True)))
+            tasks.append(p.apply_async(rename_column_on_cluster, (node1, '%s_replicated' % table_name, 'foo3', 'num2', 3, True)))
+            tasks.append(p.apply_async(insert, (node1, table_name, 10, ["num", "foo3"], 5, True)))
+            tasks.append(p.apply_async(insert, (node2, table_name, 10, ["num", "num2"], 5, True)))
+            tasks.append(p.apply_async(insert, (node3, table_name, 10, ["num", "foo2"], 5, True)))
+            tasks.append(p.apply_async(select, (node1, table_name, "foo2", None, 5, True)))
+            tasks.append(p.apply_async(select, (node2, table_name, "foo3", None, 5, True)))
+            tasks.append(p.apply_async(select, (node3, table_name, "num2", None, 5, True)))
+        for task in tasks:
+            task.get(timeout=240)
+
+        rename_column_on_cluster(node1, table_name, 'foo2', 'num2', 1, True)
+        rename_column_on_cluster(node1, '%s_replicated' % table_name, 'foo2', 'num2', 1, True)
+        rename_column_on_cluster(node1, table_name, 'foo3', 'num2', 1, True)
+        rename_column_on_cluster(node1, '%s_replicated' % table_name, 'foo3', 'num2', 1, True)
+
+        insert(node1, table_name, 1000, col_names=['num','num2'])
+        select(node1, table_name, "num2")
+        select(node2, table_name, "num2")
+        select(node3, table_name, "num2")
+        select(node4, table_name, "num2")
+    finally:
+        drop_distributed_table(node1, table_name)
diff --git a/tests/queries/0_stateless/01274_alter_rename_column_distributed.reference b/tests/queries/0_stateless/01274_alter_rename_column_distributed.reference
new file mode 100644
index 000000000000..7ed221867c19
--- /dev/null
+++ b/tests/queries/0_stateless/01274_alter_rename_column_distributed.reference
@@ -0,0 +1,4 @@
+2020-01-01	hello
+2020-01-01	hello
+2020-01-02	hello2
+2020-01-02	hello2
diff --git a/tests/queries/0_stateless/01274_alter_rename_column_distributed.sql b/tests/queries/0_stateless/01274_alter_rename_column_distributed.sql
new file mode 100644
index 000000000000..a35dc7cca567
--- /dev/null
+++ b/tests/queries/0_stateless/01274_alter_rename_column_distributed.sql
@@ -0,0 +1,17 @@
+DROP TABLE IF EXISTS visits;
+DROP TABLE IF EXISTS visits_dist;
+
+CREATE TABLE visits(StartDate Date, Name String) ENGINE MergeTree ORDER BY(StartDate);
+CREATE TABLE visits_dist AS visits ENGINE Distributed(test_cluster_two_shards_localhost,  currentDatabase(), 'visits', rand());
+
+INSERT INTO visits_dist (StartDate, Name) VALUES ('2020-01-01', 'hello');
+INSERT INTO visits_dist (StartDate, Name) VALUES ('2020-01-02', 'hello2');
+
+ALTER TABLE visits RENAME COLUMN Name TO Name2;
+ALTER TABLE visits_dist RENAME COLUMN Name TO Name2;
+
+SELECT * FROM visits_dist ORDER BY StartDate, Name2;
+
+DROP TABLE visits;
+DROP TABLE visits_dist;
+
diff --git a/tests/queries/0_stateless/01275_alter_rename_column_default_expr.reference b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.reference
new file mode 100644
index 000000000000..d81601b92c5c
--- /dev/null
+++ b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.reference
@@ -0,0 +1,87 @@
+2019-10-01	0	0	1	0 + 1
+2019-10-02	1	1	2	1 + 2
+2019-10-03	2	2	3	2 + 3
+2019-10-01	3	3	4	3 + 4
+2019-10-02	4	4	5	4 + 5
+2019-10-03	5	5	6	5 + 6
+2019-10-01	6	6	7	6 + 7
+2019-10-02	7	7	8	7 + 8
+2019-10-03	8	8	9	8 + 9
+CREATE TABLE default.table_for_rename
(
    `date` Date, 
    `key` UInt64, 
    `value4` String, 
    `value5` String, 
    `value3` String DEFAULT concat(value4, \' + \', value5)
)
ENGINE = MergeTree()
PARTITION BY date
ORDER BY key
SETTINGS index_granularity = 8192
+2019-10-01	0	0	1	0 + 1
+2019-10-02	1	1	2	1 + 2
+2019-10-03	2	2	3	2 + 3
+2019-10-01	3	3	4	3 + 4
+2019-10-02	4	4	5	4 + 5
+2019-10-03	5	5	6	5 + 6
+2019-10-01	6	6	7	6 + 7
+2019-10-02	7	7	8	7 + 8
+2019-10-03	8	8	9	8 + 9
+2019-10-01	0	0	1	0 + 1
+2019-10-02	1	1	2	1 + 2
+2019-10-03	2	2	3	2 + 3
+2019-10-01	3	3	4	3 + 4
+2019-10-02	4	4	5	4 + 5
+2019-10-03	5	5	6	5 + 6
+2019-10-01	6	6	7	6 + 7
+2019-10-02	7	7	8	7 + 8
+2019-10-03	8	8	9	8 + 9
+2019-10-02	10	10	11	10 + 11
+2019-10-03	11	11	12	11 + 12
+2019-10-01	12	12	13	12 + 13
+2019-10-02	13	13	14	13 + 14
+2019-10-03	14	14	15	14 + 15
+2019-10-01	15	15	16	15 + 16
+2019-10-02	16	16	17	16 + 17
+2019-10-03	17	17	18	17 + 18
+2019-10-01	18	18	19	18 + 19
+2019-10-02	19	19	20	19 + 20
+CREATE TABLE default.table_for_rename
(
    `date` Date, 
    `key` UInt64, 
    `value1` String, 
    `value2` String, 
    `value3` String DEFAULT concat(value1, \' + \', value2)
)
ENGINE = MergeTree()
PARTITION BY date
ORDER BY key
SETTINGS index_granularity = 8192
+2019-10-01	0	0	1	0 + 1
+2019-10-02	1	1	2	1 + 2
+2019-10-03	2	2	3	2 + 3
+2019-10-01	3	3	4	3 + 4
+2019-10-02	4	4	5	4 + 5
+2019-10-03	5	5	6	5 + 6
+2019-10-01	6	6	7	6 + 7
+2019-10-02	7	7	8	7 + 8
+2019-10-03	8	8	9	8 + 9
+2019-10-02	10	10	11	10 + 11
+2019-10-03	11	11	12	11 + 12
+2019-10-01	12	12	13	12 + 13
+2019-10-02	13	13	14	13 + 14
+2019-10-03	14	14	15	14 + 15
+2019-10-01	15	15	16	15 + 16
+2019-10-02	16	16	17	16 + 17
+2019-10-03	17	17	18	17 + 18
+2019-10-01	18	18	19	18 + 19
+2019-10-02	19	19	20	19 + 20
+2019-10-01	0	0	1	0 + 1
+2019-10-02	1	1	2	1 + 2
+2019-10-03	2	2	3	2 + 3
+2019-10-01	3	3	4	3 + 4
+2019-10-02	4	4	5	4 + 5
+2019-10-03	5	5	6	5 + 6
+2019-10-01	6	6	7	6 + 7
+2019-10-02	7	7	8	7 + 8
+2019-10-03	8	8	9	8 + 9
+2019-10-02	10	10	11	10 + 11
+2019-10-03	11	11	12	11 + 12
+2019-10-01	12	12	13	12 + 13
+2019-10-02	13	13	14	13 + 14
+2019-10-03	14	14	15	14 + 15
+2019-10-01	15	15	16	15 + 16
+2019-10-02	16	16	17	16 + 17
+2019-10-03	17	17	18	17 + 18
+2019-10-01	18	18	19	18 + 19
+2019-10-02	19	19	20	19 + 20
+2019-10-03	20	20	21	20 + 21
+2019-10-01	21	21	22	21 + 22
+2019-10-02	22	22	23	22 + 23
+2019-10-03	23	23	24	23 + 24
+2019-10-01	24	24	25	24 + 25
+2019-10-02	25	25	26	25 + 26
+2019-10-03	26	26	27	26 + 27
+2019-10-01	27	27	28	27 + 28
+2019-10-02	28	28	29	28 + 29
+2019-10-03	29	29	30	29 + 30
diff --git a/tests/queries/0_stateless/01275_alter_rename_column_default_expr.sql b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.sql
new file mode 100644
index 000000000000..21106d200afc
--- /dev/null
+++ b/tests/queries/0_stateless/01275_alter_rename_column_default_expr.sql
@@ -0,0 +1,34 @@
+DROP TABLE IF EXISTS table_for_rename;
+
+CREATE TABLE table_for_rename
+(
+  date Date,
+  key UInt64,
+  value1 String,
+  value2 String,
+  value3 String DEFAULT concat(value1, ' + ', value2) 
+)
+ENGINE = MergeTree()
+PARTITION BY date
+ORDER BY key;
+
+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(9);
+SELECT * FROM table_for_rename ORDER BY key;
+
+ALTER TABLE table_for_rename RENAME COLUMN value1 TO value4;
+ALTER TABLE table_for_rename RENAME COLUMN value2 TO value5;
+SHOW CREATE TABLE table_for_rename;
+SELECT * FROM table_for_rename ORDER BY key;
+
+INSERT INTO table_for_rename (date, key, value4, value5) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(10, 10);
+SELECT * FROM table_for_rename ORDER BY key;
+
+ALTER TABLE table_for_rename RENAME COLUMN value4 TO value1;
+ALTER TABLE table_for_rename RENAME COLUMN value5 TO value2;
+SHOW CREATE TABLE table_for_rename;
+SELECT * FROM table_for_rename ORDER BY key;
+
+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(20,10);
+SELECT * FROM table_for_rename ORDER BY key;
+
+DROP TABLE IF EXISTS table_for_rename;
diff --git a/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.reference b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.reference
new file mode 100644
index 000000000000..5d721230db30
--- /dev/null
+++ b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.reference
@@ -0,0 +1,90 @@
+2019-10-01	0	0	1
+2019-10-02	1	1	2
+2019-10-03	2	2	3
+2019-10-01	3	3	4
+2019-10-02	4	4	5
+2019-10-03	5	5	6
+2019-10-01	6	6	7
+2019-10-02	7	7	8
+2019-10-03	8	8	9
+CREATE TABLE default.table_for_rename
(
    `date` Date, 
    `key` UInt64, 
    `value4` String, 
    `value5` String, 
    `value3` String MATERIALIZED concat(value4, \' + \', value5)
)
ENGINE = MergeTree()
PARTITION BY date
ORDER BY key
SETTINGS index_granularity = 8192
+2019-10-01	0	0	1
+2019-10-02	1	1	2
+2019-10-03	2	2	3
+2019-10-01	3	3	4
+2019-10-02	4	4	5
+2019-10-03	5	5	6
+2019-10-01	6	6	7
+2019-10-02	7	7	8
+2019-10-03	8	8	9
+-- insert after rename --
+2019-10-01	0	0	1
+2019-10-02	1	1	2
+2019-10-03	2	2	3
+2019-10-01	3	3	4
+2019-10-02	4	4	5
+2019-10-03	5	5	6
+2019-10-01	6	6	7
+2019-10-02	7	7	8
+2019-10-03	8	8	9
+2019-10-02	10	10	11
+2019-10-03	11	11	12
+2019-10-01	12	12	13
+2019-10-02	13	13	14
+2019-10-03	14	14	15
+2019-10-01	15	15	16
+2019-10-02	16	16	17
+2019-10-03	17	17	18
+2019-10-01	18	18	19
+2019-10-02	19	19	20
+-- rename columns back --
+CREATE TABLE default.table_for_rename
(
    `date` Date, 
    `key` UInt64, 
    `value1` String, 
    `value2` String, 
    `value3` String MATERIALIZED concat(value1, \' + \', value2)
)
ENGINE = MergeTree()
PARTITION BY date
ORDER BY key
SETTINGS index_granularity = 8192
+2019-10-01	0	0	1
+2019-10-02	1	1	2
+2019-10-03	2	2	3
+2019-10-01	3	3	4
+2019-10-02	4	4	5
+2019-10-03	5	5	6
+2019-10-01	6	6	7
+2019-10-02	7	7	8
+2019-10-03	8	8	9
+2019-10-02	10	10	11
+2019-10-03	11	11	12
+2019-10-01	12	12	13
+2019-10-02	13	13	14
+2019-10-03	14	14	15
+2019-10-01	15	15	16
+2019-10-02	16	16	17
+2019-10-03	17	17	18
+2019-10-01	18	18	19
+2019-10-02	19	19	20
+-- insert after rename column --
+2019-10-01	0	0	1
+2019-10-02	1	1	2
+2019-10-03	2	2	3
+2019-10-01	3	3	4
+2019-10-02	4	4	5
+2019-10-03	5	5	6
+2019-10-01	6	6	7
+2019-10-02	7	7	8
+2019-10-03	8	8	9
+2019-10-02	10	10	11
+2019-10-03	11	11	12
+2019-10-01	12	12	13
+2019-10-02	13	13	14
+2019-10-03	14	14	15
+2019-10-01	15	15	16
+2019-10-02	16	16	17
+2019-10-03	17	17	18
+2019-10-01	18	18	19
+2019-10-02	19	19	20
+2019-10-03	20	20	21
+2019-10-01	21	21	22
+2019-10-02	22	22	23
+2019-10-03	23	23	24
+2019-10-01	24	24	25
+2019-10-02	25	25	26
+2019-10-03	26	26	27
+2019-10-01	27	27	28
+2019-10-02	28	28	29
+2019-10-03	29	29	30
diff --git a/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.sql b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.sql
new file mode 100644
index 000000000000..9089c52edf60
--- /dev/null
+++ b/tests/queries/0_stateless/01276_alter_rename_column_materialized_expr.sql
@@ -0,0 +1,37 @@
+DROP TABLE IF EXISTS table_for_rename;
+
+CREATE TABLE table_for_rename
+(
+  date Date,
+  key UInt64,
+  value1 String,
+  value2 String,
+  value3 String MATERIALIZED concat(value1, ' + ', value2) 
+)
+ENGINE = MergeTree()
+PARTITION BY date
+ORDER BY key;
+
+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(9);
+SELECT * FROM table_for_rename ORDER BY key;
+
+ALTER TABLE table_for_rename RENAME COLUMN value1 TO value4;
+ALTER TABLE table_for_rename RENAME COLUMN value2 TO value5;
+SHOW CREATE TABLE table_for_rename;
+SELECT * FROM table_for_rename ORDER BY key;
+
+SELECT '-- insert after rename --';
+INSERT INTO table_for_rename (date, key, value4, value5) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(10, 10);
+SELECT * FROM table_for_rename ORDER BY key;
+
+SELECT '-- rename columns back --';
+ALTER TABLE table_for_rename RENAME COLUMN value4 TO value1;
+ALTER TABLE table_for_rename RENAME COLUMN value5 TO value2;
+SHOW CREATE TABLE table_for_rename;
+SELECT * FROM table_for_rename ORDER BY key;
+
+SELECT '-- insert after rename column --';
+INSERT INTO table_for_rename (date, key, value1, value2) SELECT toDate('2019-10-01') + number % 3, number, toString(number), toString(number + 1) from numbers(20,10);
+SELECT * FROM table_for_rename ORDER BY key;
+
+DROP TABLE IF EXISTS table_for_rename;
