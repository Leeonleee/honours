{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 34571,
  "instance_id": "ClickHouse__ClickHouse-34571",
  "issue_numbers": [
    "31221"
  ],
  "base_commit": "17bb7f175b8f5b3ed7ff90ed552c40a85133e696",
  "patch": "diff --git a/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp b/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp\nindex f94f6b44c9fd..56720c2af2ba 100644\n--- a/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp\n+++ b/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp\n@@ -46,7 +46,7 @@ SeekableReadBufferPtr ReadBufferFromS3Gather::createImplementationBuffer(const S\n     auto remote_file_reader_creator = [=, this]()\n     {\n         return std::make_unique<ReadBufferFromS3>(\n-            client_ptr, bucket, remote_path, max_single_read_retries,\n+            client_ptr, bucket, remote_path, version_id, max_single_read_retries,\n             settings, /* use_external_buffer */true, /* offset */ 0, read_until_position, /* restricted_seek */true);\n     };\n \ndiff --git a/src/Disks/IO/ReadBufferFromRemoteFSGather.h b/src/Disks/IO/ReadBufferFromRemoteFSGather.h\nindex d12513cba1fe..ac3f21303609 100644\n--- a/src/Disks/IO/ReadBufferFromRemoteFSGather.h\n+++ b/src/Disks/IO/ReadBufferFromRemoteFSGather.h\n@@ -103,6 +103,7 @@ class ReadBufferFromS3Gather final : public ReadBufferFromRemoteFSGather\n     ReadBufferFromS3Gather(\n         std::shared_ptr<Aws::S3::S3Client> client_ptr_,\n         const String & bucket_,\n+        const String & version_id_,\n         const std::string & common_path_prefix_,\n         const BlobsPathToSize & blobs_to_read_,\n         size_t max_single_read_retries_,\n@@ -110,6 +111,7 @@ class ReadBufferFromS3Gather final : public ReadBufferFromRemoteFSGather\n         : ReadBufferFromRemoteFSGather(common_path_prefix_, blobs_to_read_, settings_)\n         , client_ptr(std::move(client_ptr_))\n         , bucket(bucket_)\n+        , version_id(version_id_)\n         , max_single_read_retries(max_single_read_retries_)\n     {\n     }\n@@ -119,6 +121,7 @@ class ReadBufferFromS3Gather final : public ReadBufferFromRemoteFSGather\n private:\n     std::shared_ptr<Aws::S3::S3Client> client_ptr;\n     String bucket;\n+    String version_id;\n     UInt64 max_single_read_retries;\n };\n #endif\ndiff --git a/src/Disks/S3/DiskS3.cpp b/src/Disks/S3/DiskS3.cpp\nindex 1e8894632b8c..b74676e608d9 100644\n--- a/src/Disks/S3/DiskS3.cpp\n+++ b/src/Disks/S3/DiskS3.cpp\n@@ -109,6 +109,7 @@ DiskS3::DiskS3(\n     String name_,\n     String bucket_,\n     String s3_root_path_,\n+    String version_id_,\n     DiskPtr metadata_disk_,\n     FileCachePtr cache_,\n     ContextPtr context_,\n@@ -116,6 +117,7 @@ DiskS3::DiskS3(\n     GetDiskSettings settings_getter_)\n     : IDiskRemote(name_, s3_root_path_, metadata_disk_, std::move(cache_), \"DiskS3\", settings_->thread_pool_size)\n     , bucket(std::move(bucket_))\n+    , version_id(std::move(version_id_))\n     , current_settings(std::move(settings_))\n     , settings_getter(settings_getter_)\n     , context(context_)\n@@ -196,7 +198,7 @@ std::unique_ptr<ReadBufferFromFileBase> DiskS3::readFile(const String & path, co\n     }\n \n     auto s3_impl = std::make_unique<ReadBufferFromS3Gather>(\n-        settings->client, bucket, metadata.remote_fs_root_path, metadata.remote_fs_objects,\n+        settings->client, bucket, version_id, metadata.remote_fs_root_path, metadata.remote_fs_objects,\n         settings->s3_settings.max_single_read_retries, disk_read_settings);\n \n     if (read_settings.remote_fs_method == RemoteFSReadMethod::threadpool)\n@@ -354,6 +356,7 @@ int DiskS3::readSchemaVersion(const String & source_bucket, const String & sourc\n         settings->client,\n         source_bucket,\n         source_path + SCHEMA_VERSION_OBJECT,\n+        version_id,\n         settings->s3_settings.max_single_read_retries,\n         context->getReadSettings());\n \ndiff --git a/src/Disks/S3/DiskS3.h b/src/Disks/S3/DiskS3.h\nindex 8b9776b7b61a..6f11ce86b063 100644\n--- a/src/Disks/S3/DiskS3.h\n+++ b/src/Disks/S3/DiskS3.h\n@@ -66,6 +66,7 @@ class DiskS3 final : public IDiskRemote\n         String name_,\n         String bucket_,\n         String s3_root_path_,\n+        String version_id_,\n         DiskPtr metadata_disk_,\n         FileCachePtr cache_,\n         ContextPtr context_,\n@@ -157,6 +158,8 @@ class DiskS3 final : public IDiskRemote\n \n     const String bucket;\n \n+    const String version_id;\n+\n     MultiVersion<DiskS3Settings> current_settings;\n     /// Gets disk settings from context.\n     GetDiskSettings settings_getter;\ndiff --git a/src/Disks/S3/registerDiskS3.cpp b/src/Disks/S3/registerDiskS3.cpp\nindex fc9108e89a7c..b9e2cf5afe29 100644\n--- a/src/Disks/S3/registerDiskS3.cpp\n+++ b/src/Disks/S3/registerDiskS3.cpp\n@@ -195,6 +195,7 @@ void registerDiskS3(DiskFactory & factory)\n             name,\n             uri.bucket,\n             uri.key,\n+            uri.version_id,\n             metadata_disk,\n             std::move(cache),\n             context,\ndiff --git a/src/IO/ReadBufferFromS3.cpp b/src/IO/ReadBufferFromS3.cpp\nindex d19fbd282659..52e855a85846 100644\n--- a/src/IO/ReadBufferFromS3.cpp\n+++ b/src/IO/ReadBufferFromS3.cpp\n@@ -40,6 +40,7 @@ ReadBufferFromS3::ReadBufferFromS3(\n     std::shared_ptr<Aws::S3::S3Client> client_ptr_,\n     const String & bucket_,\n     const String & key_,\n+    const String & version_id_,\n     UInt64 max_single_read_retries_,\n     const ReadSettings & settings_,\n     bool use_external_buffer_,\n@@ -50,6 +51,7 @@ ReadBufferFromS3::ReadBufferFromS3(\n     , client_ptr(std::move(client_ptr_))\n     , bucket(bucket_)\n     , key(key_)\n+    , version_id(version_id_)\n     , max_single_read_retries(max_single_read_retries_)\n     , offset(offset_)\n     , read_until_position(read_until_position_)\n@@ -128,8 +130,15 @@ bool ReadBufferFromS3::nextImpl()\n             ProfileEvents::increment(ProfileEvents::S3ReadMicroseconds, watch.elapsedMicroseconds());\n             ProfileEvents::increment(ProfileEvents::S3ReadRequestsErrors, 1);\n \n-            LOG_DEBUG(log, \"Caught exception while reading S3 object. Bucket: {}, Key: {}, Offset: {}, Attempt: {}, Message: {}\",\n-                    bucket, key, getPosition(), attempt, e.message());\n+            LOG_DEBUG(\n+                log,\n+                \"Caught exception while reading S3 object. Bucket: {}, Key: {}, Version: {}, Offset: {}, Attempt: {}, Message: {}\",\n+                bucket,\n+                key,\n+                version_id.empty() ? \"Latest\" : version_id,\n+                getPosition(),\n+                attempt,\n+                e.message());\n \n             if (attempt + 1 == max_single_read_retries)\n                 throw;\n@@ -213,7 +222,7 @@ std::optional<size_t> ReadBufferFromS3::getTotalSize()\n     if (file_size)\n         return file_size;\n \n-    auto object_size = S3::getObjectSize(client_ptr, bucket, key, false);\n+    auto object_size = S3::getObjectSize(client_ptr, bucket, key, version_id, false);\n \n     if (!object_size)\n     {\n@@ -248,6 +257,10 @@ std::unique_ptr<ReadBuffer> ReadBufferFromS3::initialize()\n     Aws::S3::Model::GetObjectRequest req;\n     req.SetBucket(bucket);\n     req.SetKey(key);\n+    if (!version_id.empty())\n+    {\n+        req.SetVersionId(version_id);\n+    }\n \n     /**\n      * If remote_filesystem_read_method = 'threadpool', then for MergeTree family tables\n@@ -259,13 +272,26 @@ std::unique_ptr<ReadBuffer> ReadBufferFromS3::initialize()\n             throw Exception(ErrorCodes::LOGICAL_ERROR, \"Attempt to read beyond right offset ({} > {})\", offset, read_until_position - 1);\n \n         req.SetRange(fmt::format(\"bytes={}-{}\", offset, read_until_position - 1));\n-        LOG_TEST(log, \"Read S3 object. Bucket: {}, Key: {}, Range: {}-{}\", bucket, key, offset, read_until_position - 1);\n+        LOG_TEST(\n+            log,\n+            \"Read S3 object. Bucket: {}, Key: {}, Version: {}, Range: {}-{}\",\n+            bucket,\n+            key,\n+            version_id.empty() ? \"Latest\" : version_id,\n+            offset,\n+            read_until_position - 1);\n     }\n     else\n     {\n         if (offset)\n             req.SetRange(fmt::format(\"bytes={}-\", offset));\n-        LOG_TEST(log, \"Read S3 object. Bucket: {}, Key: {}, Offset: {}\", bucket, key, offset);\n+        LOG_TEST(\n+            log,\n+            \"Read S3 object. Bucket: {}, Key: {}, Version: {}, Offset: {}\",\n+            bucket,\n+            key,\n+            version_id.empty() ? \"Latest\" : version_id,\n+            offset);\n     }\n \n     Aws::S3::Model::GetObjectOutcome outcome = client_ptr->GetObject(req);\n@@ -293,6 +319,7 @@ SeekableReadBufferPtr ReadBufferS3Factory::getReader()\n         client_ptr,\n         bucket,\n         key,\n+        version_id,\n         s3_max_single_read_retries,\n         read_settings,\n         false /*use_external_buffer*/,\ndiff --git a/src/IO/ReadBufferFromS3.h b/src/IO/ReadBufferFromS3.h\nindex 2a94d286da6f..8c582d3a0d2b 100644\n--- a/src/IO/ReadBufferFromS3.h\n+++ b/src/IO/ReadBufferFromS3.h\n@@ -32,6 +32,7 @@ class ReadBufferFromS3 : public SeekableReadBufferWithSize, public WithFileName\n     std::shared_ptr<Aws::S3::S3Client> client_ptr;\n     String bucket;\n     String key;\n+    String version_id;\n     UInt64 max_single_read_retries;\n \n     /// These variables are atomic because they can be used for `logging only`\n@@ -50,6 +51,7 @@ class ReadBufferFromS3 : public SeekableReadBufferWithSize, public WithFileName\n         std::shared_ptr<Aws::S3::S3Client> client_ptr_,\n         const String & bucket_,\n         const String & key_,\n+        const String & version_id_,\n         UInt64 max_single_read_retries_,\n         const ReadSettings & settings_,\n         bool use_external_buffer = false,\n@@ -93,6 +95,7 @@ class ReadBufferS3Factory : public ParallelReadBuffer::ReadBufferFactory, public\n         std::shared_ptr<Aws::S3::S3Client> client_ptr_,\n         const String & bucket_,\n         const String & key_,\n+        const String & version_id_,\n         size_t range_step_,\n         size_t object_size_,\n         UInt64 s3_max_single_read_retries_,\n@@ -100,6 +103,7 @@ class ReadBufferS3Factory : public ParallelReadBuffer::ReadBufferFactory, public\n         : client_ptr(client_ptr_)\n         , bucket(bucket_)\n         , key(key_)\n+        , version_id(version_id_)\n         , read_settings(read_settings_)\n         , range_generator(object_size_, range_step_)\n         , range_step(range_step_)\n@@ -122,6 +126,7 @@ class ReadBufferS3Factory : public ParallelReadBuffer::ReadBufferFactory, public\n     std::shared_ptr<Aws::S3::S3Client> client_ptr;\n     const String bucket;\n     const String key;\n+    const String version_id;\n     ReadSettings read_settings;\n \n     RangeGenerator range_generator;\ndiff --git a/src/IO/S3Common.cpp b/src/IO/S3Common.cpp\nindex fefc296a200b..4853a90542ed 100644\n--- a/src/IO/S3Common.cpp\n+++ b/src/IO/S3Common.cpp\n@@ -779,13 +779,27 @@ namespace S3\n         static constexpr auto OBS = \"OBS\";\n         static constexpr auto OSS = \"OSS\";\n \n-\n         uri = uri_;\n         storage_name = S3;\n \n         if (uri.getHost().empty())\n             throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Host is empty in S3 URI.\");\n \n+        /// Extract object version ID from query string.\n+        {\n+            version_id = \"\";\n+            const String version_key = \"versionId=\";\n+            const auto query_string = uri.getQuery();\n+\n+            auto start = query_string.rfind(version_key);\n+            if (start != std::string::npos)\n+            {\n+                start += version_key.length();\n+                auto end = query_string.find_first_of('&', start);\n+                version_id = query_string.substr(start, end == std::string::npos ? std::string::npos : end - start);\n+            }\n+        }\n+\n         String name;\n         String endpoint_authority_from_uri;\n \n@@ -842,12 +856,15 @@ namespace S3\n                             quoteString(bucket), !uri.empty() ? \" (\" + uri.toString() + \")\" : \"\");\n     }\n \n-    size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, bool throw_on_error)\n+    size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, const String & version_id, bool throw_on_error)\n     {\n         Aws::S3::Model::HeadObjectRequest req;\n         req.SetBucket(bucket);\n         req.SetKey(key);\n \n+        if (!version_id.empty())\n+            req.SetVersionId(version_id);\n+\n         Aws::S3::Model::HeadObjectOutcome outcome = client_ptr->HeadObject(req);\n \n         if (outcome.IsSuccess())\ndiff --git a/src/IO/S3Common.h b/src/IO/S3Common.h\nindex c33ce427e66a..16134f173d56 100644\n--- a/src/IO/S3Common.h\n+++ b/src/IO/S3Common.h\n@@ -66,6 +66,7 @@ struct URI\n     String endpoint;\n     String bucket;\n     String key;\n+    String version_id;\n     String storage_name;\n \n     bool is_virtual_hosted_style;\n@@ -75,7 +76,7 @@ struct URI\n     static void validateBucket(const String & bucket, const Poco::URI & uri);\n };\n \n-size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, bool throw_on_error = true);\n+size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, const String & version_id = {}, bool throw_on_error = true);\n \n }\n \ndiff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp\nindex dd617517acc5..e43fedebd864 100644\n--- a/src/Storages/StorageS3.cpp\n+++ b/src/Storages/StorageS3.cpp\n@@ -232,12 +232,14 @@ StorageS3Source::StorageS3Source(\n     String compression_hint_,\n     const std::shared_ptr<Aws::S3::S3Client> & client_,\n     const String & bucket_,\n+    const String & version_id_,\n     std::shared_ptr<IteratorWrapper> file_iterator_,\n     const size_t download_thread_num_)\n     : SourceWithProgress(getHeader(sample_block_, requested_virtual_columns_))\n     , WithContext(context_)\n     , name(std::move(name_))\n     , bucket(bucket_)\n+    , version_id(version_id_)\n     , format(format_)\n     , columns_desc(columns_)\n     , max_block_size(max_block_size_)\n@@ -291,7 +293,7 @@ bool StorageS3Source::initialize()\n \n std::unique_ptr<ReadBuffer> StorageS3Source::createS3ReadBuffer(const String & key)\n {\n-    const size_t object_size = DB::S3::getObjectSize(client, bucket, key, false);\n+    const size_t object_size = DB::S3::getObjectSize(client, bucket, key, version_id, false);\n \n     auto download_buffer_size = getContext()->getSettings().max_download_buffer_size;\n     const bool use_parallel_download = download_buffer_size > 0 && download_thread_num > 1;\n@@ -299,7 +301,7 @@ std::unique_ptr<ReadBuffer> StorageS3Source::createS3ReadBuffer(const String & k\n     if (!use_parallel_download || object_too_small)\n     {\n         LOG_TRACE(log, \"Downloading object of size {} from S3 in single thread\", object_size);\n-        return std::make_unique<ReadBufferFromS3>(client, bucket, key, max_single_read_retries, getContext()->getReadSettings());\n+        return std::make_unique<ReadBufferFromS3>(client, bucket, key, version_id, max_single_read_retries, getContext()->getReadSettings());\n     }\n \n     assert(object_size > 0);\n@@ -311,7 +313,7 @@ std::unique_ptr<ReadBuffer> StorageS3Source::createS3ReadBuffer(const String & k\n     }\n \n     auto factory = std::make_unique<ReadBufferS3Factory>(\n-        client, bucket, key, download_buffer_size, object_size, max_single_read_retries, getContext()->getReadSettings());\n+        client, bucket, key, version_id, download_buffer_size, object_size, max_single_read_retries, getContext()->getReadSettings());\n     LOG_TRACE(\n         log, \"Downloading from S3 in {} threads. Object size: {}, Range size: {}.\", download_thread_num, object_size, download_buffer_size);\n \n@@ -693,6 +695,7 @@ Pipe StorageS3::read(\n             compression_method,\n             s3_configuration.client,\n             s3_configuration.uri.bucket,\n+            s3_configuration.uri.version_id,\n             iterator_wrapper,\n             max_download_threads));\n     }\n@@ -966,7 +969,7 @@ ColumnsDescription StorageS3::getTableStructureFromDataImpl(\n         first = false;\n         return wrapReadBufferWithCompressionMethod(\n             std::make_unique<ReadBufferFromS3>(\n-                s3_configuration.client, s3_configuration.uri.bucket, key, s3_configuration.rw_settings.max_single_read_retries, ctx->getReadSettings()),\n+                s3_configuration.client, s3_configuration.uri.bucket, key, s3_configuration.uri.version_id, s3_configuration.rw_settings.max_single_read_retries, ctx->getReadSettings()),\n             chooseCompressionMethod(key, compression_method));\n     };\n \ndiff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h\nindex 191806ede95d..da176d805bbc 100644\n--- a/src/Storages/StorageS3.h\n+++ b/src/Storages/StorageS3.h\n@@ -73,6 +73,7 @@ class StorageS3Source : public SourceWithProgress, WithContext\n         String compression_hint_,\n         const std::shared_ptr<Aws::S3::S3Client> & client_,\n         const String & bucket,\n+        const String & version_id,\n         std::shared_ptr<IteratorWrapper> file_iterator_,\n         size_t download_thread_num);\n \n@@ -85,6 +86,7 @@ class StorageS3Source : public SourceWithProgress, WithContext\n private:\n     String name;\n     String bucket;\n+    String version_id;\n     String file_path;\n     String format;\n     ColumnsDescription columns_desc;\n",
  "test_patch": "diff --git a/src/IO/tests/gtest_s3_uri.cpp b/src/IO/tests/gtest_s3_uri.cpp\nindex 20d19437c64d..41ce102ca8ae 100644\n--- a/src/IO/tests/gtest_s3_uri.cpp\n+++ b/src/IO/tests/gtest_s3_uri.cpp\n@@ -3,12 +3,79 @@\n \n #if USE_AWS_S3\n \n-#    include <IO/S3Common.h>\n+#include <IO/S3Common.h>\n \n namespace\n {\n using namespace DB;\n \n+struct TestCase\n+{\n+    S3::URI uri;\n+    String endpoint;\n+    String bucket;\n+    String key;\n+    String version_id;\n+    bool is_virtual_hosted_style;\n+};\n+\n+const TestCase TestCases[] = {\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?firstKey=someKey&secondKey=anotherKey\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?versionId=testVersionId&anotherKey=someOtherKey\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"testVersionId\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?firstKey=someKey&versionId=testVersionId&anotherKey=someOtherKey\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"testVersionId\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?anotherKey=someOtherKey&versionId=testVersionId\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"testVersionId\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?versionId=testVersionId\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"testVersionId\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?versionId=\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?versionId&\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"\",\n+     true},\n+    {S3::URI(Poco::URI(\"https://bucketname.s3.us-east-2.amazonaws.com/data?versionId\")),\n+     \"https://s3.us-east-2.amazonaws.com\",\n+     \"bucketname\",\n+     \"data\",\n+     \"\",\n+     true},\n+};\n+\n class S3UriTest : public testing::TestWithParam<std::string>\n {\n };\n@@ -20,6 +87,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://s3.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"jokserfn\", uri.bucket);\n         ASSERT_EQ(\"\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(true, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -27,6 +95,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://s3.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"jokserfn\", uri.bucket);\n         ASSERT_EQ(\"\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(false, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -34,6 +103,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"bucket\", uri.bucket);\n         ASSERT_EQ(\"\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(false, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -41,6 +111,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://s3.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"jokserfn\", uri.bucket);\n         ASSERT_EQ(\"data\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(true, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -48,6 +119,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://storage.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"jokserfn\", uri.bucket);\n         ASSERT_EQ(\"data\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(false, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -55,6 +127,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://cos.ap-beijing.myqcloud.com\", uri.endpoint);\n         ASSERT_EQ(\"bucketname\", uri.bucket);\n         ASSERT_EQ(\"data\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(true, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -62,6 +135,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://s3.us-east-2.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"bucketname\", uri.bucket);\n         ASSERT_EQ(\"data\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(true, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -69,6 +143,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://s3.us-east-2.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"bucketname\", uri.bucket);\n         ASSERT_EQ(\"data\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(false, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -76,6 +151,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://s3-us-east-2.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"bucketname\", uri.bucket);\n         ASSERT_EQ(\"data\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(true, uri.is_virtual_hosted_style);\n     }\n     {\n@@ -83,6 +159,7 @@ TEST(S3UriTest, validPatterns)\n         ASSERT_EQ(\"https://s3-us-east-2.amazonaws.com\", uri.endpoint);\n         ASSERT_EQ(\"bucketname\", uri.bucket);\n         ASSERT_EQ(\"data\", uri.key);\n+        ASSERT_EQ(\"\", uri.version_id);\n         ASSERT_EQ(false, uri.is_virtual_hosted_style);\n     }\n }\n@@ -92,6 +169,18 @@ TEST_P(S3UriTest, invalidPatterns)\n     ASSERT_ANY_THROW(S3::URI(Poco::URI(GetParam())));\n }\n \n+TEST(S3UriTest, versionIdChecks)\n+{\n+    for (const auto& test_case : TestCases)\n+    {\n+        ASSERT_EQ(test_case.endpoint, test_case.uri.endpoint);\n+        ASSERT_EQ(test_case.bucket, test_case.uri.bucket);\n+        ASSERT_EQ(test_case.key, test_case.uri.key);\n+        ASSERT_EQ(test_case.version_id, test_case.uri.version_id);\n+        ASSERT_EQ(test_case.is_virtual_hosted_style, test_case.uri.is_virtual_hosted_style);\n+    }\n+}\n+\n INSTANTIATE_TEST_SUITE_P(\n     S3,\n     S3UriTest,\n",
  "problem_statement": "S3 Table Function Ignoring VersionId\nSummary: S3 versionId is being ignored by the S3 table function\r\n\r\nS3 has object versioning and for the object url to specify a version other than the latest the parameter \"versionId=x\" is added.\r\n\r\nWhen querying on clickhouse with the S3 Table Function, the following is expected to select from the s3 object with versionId=testVersionId\r\n`SELECT * FROM s3('https://s3.us-east-2.amazonaws.com/my-test-bucket-768/data.csv?versionId=testVersionId', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32') LIMIT 2;`\r\n\r\nUnfortunately it instead returns the latest. \r\n\r\nThis has been reproduced on the following versions:\r\nv21.8.4\r\nv21.9.4.35 (latest)\r\n\r\nThis can be worked around by instead creating a signed url and using the url table function instead, however it would be preferable to not have to do that.\n",
  "hints_text": "@excitoon will fix it.\n@alex-zaitsev Does not look like that.\nWe should extract `versionId` parameter from the URL and set the corresponding header explicitly.\n`Aws::S3::Model::GetObjectRequest::SetVersionId`\nIf no one is working on this I would like to give it a try. I am looking at the source code in this directory:\r\nhttps://github.com/ClickHouse/ClickHouse/tree/master/src/IO/S3\r\n\r\nThe following line appears to be extracting the `URI` string for the resource in S3:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/d680a017e09f409da64a71f994a3a49ec5b8f2ab/src/IO/S3/PocoHTTPClient.cpp#L121\r\n\r\nAm I on the right track?\n@surahman Yes, looks like you are looking at the right place :+1: \n## Change Proposal\r\n\r\nWould you like to handle all of the URI request options for [GetObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html#API_GetObject_RequestSyntax) in this PR?\r\n\r\nIt is best to deal with all of these options at once since there is some very useful functionality such as ~encryption and~ ranged gets (`Range` and `partNumber`). The latter allows for segment `Get`s on files which can be useful when you know the chunk you need from a large file.\r\n\r\nI have a very quick look and believe the items that can be set in a `Get` request URL are as follows:\r\n* partNumber=PartNumber\r\n* response-cache-control=ResponseCacheControl\r\n* response-content-disposition=ResponseContentDisposition\r\n* response-content-encoding=ResponseContentEncoding\r\n* response-content-language=ResponseContentLanguage\r\n* response-content-type=ResponseContentType\r\n* response-expires=ResponseExpires\r\n* versionId=VersionId\r\n\r\n### Changes\r\n\r\n* `private` method `parseGetObjectParameters`\r\n* Split request URI at `?` and generate a `vector<string key, string value>`. Individual keys are split on `&`. Complexity: *O(len(request))*\r\n* Switch statement to iterate over parsed key-value pairs and populate request header. Complexity: *O((len(longest key) + len(options string)) * number of keys)*, since we know the length of the longest key this is *O(len(options string) * (number of keys))*.\r\n* Total Complexity: *O(len(request) + (len(options string) * (number of keys))).*\r\n* Testing using a functional test on a mock HTTP server. I shall have to see how the AWS S3 client handles mock requests.\n@surahman There are a few downsides with this approach:\r\n\r\n1. Key names in s3 can contain `?` character. It means that you cannot disambiguate between the cases when the object name is `name?versionId=123` and when it is `name` and you are requesting 123th version. We can sacrifice this, but have to take care.\r\n2. `range` and `partNumber` parameters will be difficult to use correctly, because most likely you don't know at what offset the object has to be read to still have data be correctly parsed.\r\n3. Some of the parameters may have internal technical meaning or have no use in our context, so better to avoid supporting them to avoid potential misunderstanding from user.\r\n\r\nI suggest to implement mostly generic approach but to allow only `versionId` handling and keep other parameters intact.\r\nWe will leave a room for supporting other parameters but will not support it for now.\r\n\r\nFor example:\r\n- if the URL was `.../hello?versionId=123`, we will take out versionId and request `.../hello`.\r\n- if the URL was `.../hello?a=b&versionId=123`, we will take out versionId and request `.../hello?a=b`.\r\n- if the URL was `.../hello?a=b&versionId=123&abc=def`, we will take out versionId and request `.../hello?a=b&abc=def` (this probably does not make sense, but ok to keep the implementation natural).\r\n\nValid points, I believe the parameters aside from `partNumber` and `versionId` are for content encoding and responses. I have affected changes to extract the `URI` and `versionId` and have kicked off a local build for testing.\r\n\r\nCH is using the Poco HTTP library for requests and not the AWS S3 Client so the `versionId` request will need to be manually set.\r\nThe AWS client achieves this through this routine:\r\nhttps://github.com/aws/aws-sdk-cpp/blob/6e854b6a8bc7945f150c3a11551196bda341962a/aws-cpp-sdk-s3/include/aws/s3/model/GetObjectRequest.h#L766\r\nThe following line then configures the `versionId` request:\r\nhttps://github.com/aws/aws-sdk-cpp/blob/6e854b6a8bc7945f150c3a11551196bda341962a/aws-cpp-sdk-s3/source/model/GetObjectRequest.cpp#L99\r\n[`AddQueryStringParameter`](https://github.com/aws/aws-sdk-cpp/blob/6e854b6a8bc7945f150c3a11551196bda341962a/aws-cpp-sdk-core/source/http/URI.cpp#L360) seems to be generating the `HTTP Get`. \r\n[`Aws::String m_queryString`](https://github.com/aws/aws-sdk-cpp/blob/6e854b6a8bc7945f150c3a11551196bda341962a/aws-cpp-sdk-core/include/aws/core/http/URI.h#L205).\r\n\r\nIn short, the AWS Client seems to be generating the `HTTP Get` URI using the above routines.\r\n\r\nWhat I think happens in CH is that the S3 URI is configured here:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/2cfe8578b7246404368c423306a7e200f5f6d6a7/src/IO/S3/PocoHTTPClient.cpp#L218\r\nand then just the path is extracted and appended to the extracted query string, which is empty:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/2cfe8578b7246404368c423306a7e200f5f6d6a7/src/IO/S3/PocoHTTPClient.cpp#L219\r\n\r\nI think this needs to become something like this:\r\n```c++\r\n            Aws::Http::URI aws_target_uri(uri);\r\n            aws_target_uri.AddQueryStringParameter(\"versionId\", version_number);\r\n            poco_request.setURI(aws_target_uri.GetPath() + aws_target_uri.GetQueryString());\r\n```",
  "created_at": "2022-02-14T04:27:14Z",
  "modified_files": [
    "src/Disks/IO/ReadBufferFromRemoteFSGather.cpp",
    "src/Disks/IO/ReadBufferFromRemoteFSGather.h",
    "src/Disks/S3/DiskS3.cpp",
    "src/Disks/S3/DiskS3.h",
    "src/Disks/S3/registerDiskS3.cpp",
    "src/IO/ReadBufferFromS3.cpp",
    "src/IO/ReadBufferFromS3.h",
    "src/IO/S3Common.cpp",
    "src/IO/S3Common.h",
    "src/Storages/StorageS3.cpp",
    "src/Storages/StorageS3.h"
  ],
  "modified_test_files": [
    "src/IO/tests/gtest_s3_uri.cpp"
  ]
}