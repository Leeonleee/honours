{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 21267,
  "instance_id": "ClickHouse__ClickHouse-21267",
  "issue_numbers": [
    "21118"
  ],
  "base_commit": "82b8d45cd71128378b334e0aa4abc756898e688d",
  "patch": "diff --git a/contrib/cppkafka b/contrib/cppkafka\nindex 57a599d99c54..5a119f689f8a 160000\n--- a/contrib/cppkafka\n+++ b/contrib/cppkafka\n@@ -1,1 +1,1 @@\n-Subproject commit 57a599d99c540e647bcd0eb9ea77c523cca011b3\n+Subproject commit 5a119f689f8a4d90d10a9635e7ee2bee5c127de1\ndiff --git a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\nindex b3ca1579bd13..bd25607a5f39 100644\n--- a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n+++ b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n@@ -42,7 +42,15 @@ ReadBufferFromKafkaConsumer::ReadBufferFromKafkaConsumer(\n     // called (synchronously, during poll) when we enter the consumer group\n     consumer->set_assignment_callback([this](const cppkafka::TopicPartitionList & topic_partitions)\n     {\n-        LOG_TRACE(log, \"Topics/partitions assigned: {}\", topic_partitions);\n+        if (topic_partitions.empty())\n+        {\n+            LOG_INFO(log, \"Got empty assignment: Not enough partitions in the topic for all consumers?\");\n+        }\n+        else\n+        {\n+            LOG_TRACE(log, \"Topics/partitions assigned: {}\", topic_partitions);\n+        }\n+\n         assignment = topic_partitions;\n     });\n \n@@ -63,7 +71,7 @@ ReadBufferFromKafkaConsumer::ReadBufferFromKafkaConsumer(\n         cleanUnprocessed();\n \n         stalled_status = REBALANCE_HAPPENED;\n-        assignment.clear();\n+        assignment.reset();\n         waited_for_assignment = 0;\n \n         // for now we use slower (but reliable) sync commit in main loop, so no need to repeat\n@@ -232,7 +240,16 @@ void ReadBufferFromKafkaConsumer::commit()\n void ReadBufferFromKafkaConsumer::subscribe()\n {\n     LOG_TRACE(log, \"Already subscribed to topics: [{}]\", boost::algorithm::join(consumer->get_subscription(), \", \"));\n-    LOG_TRACE(log, \"Already assigned to: {}\", assignment);\n+\n+    if (assignment.has_value())\n+    {\n+        LOG_TRACE(log, \"Already assigned to: {}\", assignment.value());\n+    }\n+    else\n+    {\n+        LOG_TRACE(log, \"No assignment\");\n+    }\n+\n \n     size_t max_retries = 5;\n \n@@ -295,7 +312,7 @@ void ReadBufferFromKafkaConsumer::unsubscribe()\n \n void ReadBufferFromKafkaConsumer::resetToLastCommitted(const char * msg)\n {\n-    if (assignment.empty())\n+    if (!assignment.has_value() || assignment->empty())\n     {\n         LOG_TRACE(log, \"Not assignned. Can't reset to last committed position.\");\n         return;\n@@ -360,7 +377,7 @@ bool ReadBufferFromKafkaConsumer::poll()\n         {\n             // While we wait for an assignment after subscription, we'll poll zero messages anyway.\n             // If we're doing a manual select then it's better to get something after a wait, then immediate nothing.\n-            if (assignment.empty())\n+            if (!assignment.has_value())\n             {\n                 waited_for_assignment += poll_timeout; // slightly innaccurate, but rough calculation is ok.\n                 if (waited_for_assignment < MAX_TIME_TO_WAIT_FOR_ASSIGNMENT_MS)\n@@ -369,11 +386,15 @@ bool ReadBufferFromKafkaConsumer::poll()\n                 }\n                 else\n                 {\n-                    LOG_WARNING(log, \"Can't get assignment. It can be caused by some issue with consumer group (not enough partitions?). Will keep trying.\");\n+                    LOG_WARNING(log, \"Can't get assignment. Will keep trying.\");\n                     stalled_status = NO_ASSIGNMENT;\n                     return false;\n                 }\n-\n+            }\n+            else if (assignment->empty())\n+            {\n+                LOG_TRACE(log, \"Empty assignment.\");\n+                return false;\n             }\n             else\n             {\ndiff --git a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\nindex 49d3df0e180d..ef829b86a247 100644\n--- a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n+++ b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n@@ -97,7 +97,7 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n     Messages::const_iterator current;\n \n     // order is important, need to be destructed before consumer\n-    cppkafka::TopicPartitionList assignment;\n+    std::optional<cppkafka::TopicPartitionList> assignment;\n     const Names topics;\n \n     void drain();\n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex def78b824f9e..24f570c51d9c 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -2976,6 +2976,140 @@ def test_kafka_formats_with_broken_message(kafka_cluster):\n         # print(errors_expected.strip())\n         assert  errors_result.strip() == errors_expected.strip(), 'Proper errors for format: {}'.format(format_name)\n \n+def wait_for_new_data(table_name, prev_count = 0, max_retries = 120):\n+    retries = 0\n+    while True:\n+        new_count = int(instance.query(\"SELECT count() FROM {}\".format(table_name)))\n+        print(new_count)\n+        if new_count > prev_count:\n+            return new_count\n+        else:\n+            retries += 1\n+            time.sleep(0.5)\n+            if retries > max_retries:\n+                raise Exception(\"No new data :(\")\n+\n+def test_kafka_consumer_failover(kafka_cluster):\n+\n+    # for backporting:\n+    # admin_client = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n+    admin_client = KafkaAdminClient(bootstrap_servers=\"localhost:{}\".format(kafka_cluster.kafka_port))\n+\n+    topic_list = []\n+    topic_list.append(NewTopic(name=\"kafka_consumer_failover\", num_partitions=2, replication_factor=1))\n+    admin_client.create_topics(new_topics=topic_list, validate_only=False)\n+\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.kafka;\n+        DROP TABLE IF EXISTS test.kafka2;\n+\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'kafka_consumer_failover',\n+                     kafka_group_name = 'kafka_consumer_failover_group',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_max_block_size = 1,\n+                     kafka_poll_timeout_ms = 200;\n+\n+        CREATE TABLE test.kafka2 (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'kafka_consumer_failover',\n+                     kafka_group_name = 'kafka_consumer_failover_group',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_max_block_size = 1,\n+                     kafka_poll_timeout_ms = 200;\n+\n+        CREATE TABLE test.kafka3 (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'kafka_consumer_failover',\n+                     kafka_group_name = 'kafka_consumer_failover_group',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_max_block_size = 1,\n+                     kafka_poll_timeout_ms = 200;\n+\n+        CREATE TABLE test.destination (\n+            key UInt64,\n+            value UInt64,\n+            _consumed_by LowCardinality(String)\n+        )\n+        ENGINE = MergeTree()\n+        ORDER BY key;\n+\n+        CREATE MATERIALIZED VIEW test.kafka_mv TO test.destination AS\n+        SELECT key, value, 'kafka' as _consumed_by\n+        FROM test.kafka;\n+\n+        CREATE MATERIALIZED VIEW test.kafka2_mv TO test.destination AS\n+        SELECT key, value, 'kafka2' as _consumed_by\n+        FROM test.kafka2;\n+\n+        CREATE MATERIALIZED VIEW test.kafka3_mv TO test.destination AS\n+        SELECT key, value, 'kafka3' as _consumed_by\n+        FROM test.kafka3;\n+        ''')\n+\n+\n+    producer = KafkaProducer(bootstrap_servers=\"localhost:{}\".format(cluster.kafka_port), value_serializer=producer_serializer, key_serializer=producer_serializer)\n+\n+    ## all 3 attached, 2 working\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':1,'value': 1}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':1,'value': 1}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination')\n+\n+    ## 2 attached, 2 working\n+    instance.query('DETACH TABLE test.kafka')\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':2,'value': 2}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':2,'value': 2}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination', prev_count)\n+\n+    ## 1 attached, 1 working\n+    instance.query('DETACH TABLE test.kafka2')\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':3,'value': 3}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':3,'value': 3}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination', prev_count)\n+\n+    ## 2 attached, 2 working\n+    instance.query('ATTACH TABLE test.kafka')\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':4,'value': 4}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':4,'value': 4}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination', prev_count)\n+\n+    ## 1 attached, 1 working\n+    instance.query('DETACH TABLE test.kafka3')\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':5,'value': 5}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':5,'value': 5}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination', prev_count)\n+\n+    ## 2 attached, 2 working\n+    instance.query('ATTACH TABLE test.kafka2')\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':6,'value': 6}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':6,'value': 6}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination', prev_count)\n+\n+    ## 3 attached, 2 working\n+    instance.query('ATTACH TABLE test.kafka3')\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':7,'value': 7}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':7,'value': 7}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination', prev_count)\n+\n+    ## 2 attached, same 2 working\n+    instance.query('DETACH TABLE test.kafka3')\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':8,'value': 8}), partition=0)\n+    producer.send(topic='kafka_consumer_failover', value=json.dumps({'key':8,'value': 8}), partition=1)\n+    producer.flush()\n+    prev_count = wait_for_new_data('test.destination', prev_count)\n+\n+\n if __name__ == '__main__':\n     cluster.start()\n     input(\"Cluster created, press any key to destroy...\")\n",
  "problem_statement": "Having consumer group member failover problem in ClickHouse Kafka engine\n## Environment\r\n* ClickHouse server version: 21.2.4 revision 54447\r\n\r\n## Background\r\nI m in the process of setting up a 2-node ClickHouse cluster, each node is with identical tables (as follows) for fetching data from Kafka topic.\r\n\r\n```\r\nCREATE TABLE `queue_05` (\r\n  `id` UInt64,\r\n  ...\r\n)\r\nENGINE = Kafka()\r\nSETTINGS\r\n  kafka_broker_list = 'kafka01:9092,kafka02:9092,kafka03:9092',\r\n  kafka_topic_list = 'topic03',\r\n  kafka_group_name = 'g03',\r\n  kafka_client_id = 'c03',\r\n  kafka_format = 'JSONEachRow',\r\n  kafka_num_consumers = 1\r\n```\r\n\r\n```\r\nCREATE TABLE `table_03` (\r\n  `id` UInt64,\r\n  ...\r\n)\r\nENGINE = ReplicatedReplacingMergeTree(\r\n  '/clickhouse/tables/{shard}/table_03',\r\n  '{replica}',\r\n  updated_at\r\n)\r\nPARTITION BY toYYYYMM(`time`)\r\nPRIMARY KEY id\r\n```\r\n\r\n```\r\nCREATE MATERIALIZED VIEW `mv_03` TO `table_03`\r\n  AS SELECT * FROM `queue_05`;\r\n```\r\n\r\nThe Kafka topic `topic03` is with single partition. The ClickHouse tables `queue_05` in db01, db02 are in the same consumer group `g03` and are expected to work for failover purpose.\r\n\r\n## Problem\r\nWhen both tables `queue_05` in db01, db02 are attached, one of the table fetches data from single partition of topic `topic03`, the other table are in standby; when one node is down or one of the tables `queue_05` is detached, consumer group `g03` is then rebalancing and find no active members after a while, no failover happens.\r\n\r\nHere is the log snippet of clickhouse-server.log in db02, right after I manually detach table `queue_05` in db01.\r\n```\r\n2021.02.24 11:43:46.110365 [ 1023 ] {} <Trace> StorageKafka (queue_05): Nothing to commit.\r\n2021.02.24 11:43:46.110429 [ 1023 ] {} <Trace> StorageKafka (queue_05): Stream(s) stalled. Reschedule.\r\n2021.02.24 11:43:46.610572 [ 1023 ] {} <Debug> StorageKafka (queue_05): Started streaming to 1 attached views\r\n2021.02.24 11:43:46.611817 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_SUBSCRIPTION in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:46.611916 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_SUBSCRIPTION in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:46.611949 [ 1023 ] {} <Trace> StorageKafka (queue_05): Already subscribed to topics: [topic03]\r\n2021.02.24 11:43:46.611959 [ 1023 ] {} <Trace> StorageKafka (queue_05): Already assigned to: [  ]\r\n2021.02.24 11:43:46.611989 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_SUBSCRIPTION in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:46.662303 [ 1023 ] {} <Warning> StorageKafka (queue_05): Can't get assignment. It can be caused by some issue with consumer group (not enough partitions?). Will keep trying.\r\n2021.02.24 11:43:46.662403 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_ASSIGNMENT in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:46.662465 [ 1023 ] {} <Trace> StorageKafka (queue_05): Nothing to commit.\r\n2021.02.24 11:43:46.662582 [ 1023 ] {} <Trace> StorageKafka (queue_05): Stream(s) stalled. Reschedule.\r\n2021.02.24 11:43:47.162758 [ 1023 ] {} <Debug> StorageKafka (queue_05): Started streaming to 1 attached views\r\n2021.02.24 11:43:47.163558 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_SUBSCRIPTION in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:47.163614 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_SUBSCRIPTION in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:47.163642 [ 1023 ] {} <Trace> StorageKafka (queue_05): Already subscribed to topics: [topic03]\r\n2021.02.24 11:43:47.163653 [ 1023 ] {} <Trace> StorageKafka (queue_05): Already assigned to: [  ]\r\n2021.02.24 11:43:47.163680 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_SUBSCRIPTION in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:47.213860 [ 1023 ] {} <Warning> StorageKafka (queue_05): Can't get assignment. It can be caused by some issue with consumer group (not enough partitions?). Will keep trying.\r\n2021.02.24 11:43:47.213967 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_ASSIGNMENT in state up (join-state wait-assign-call)\r\n2021.02.24 11:43:47.214040 [ 1023 ] {} <Trace> StorageKafka (queue_05): Nothing to commit.\r\n2021.02.24 11:43:47.214134 [ 1023 ] {} <Trace> StorageKafka (queue_05): Stream(s) stalled. Reschedule.\r\n2021.02.24 11:43:47.714289 [ 1023 ] {} <Debug> StorageKafka (queue_05): Started streaming to 1 attached views\r\n2021.02.24 11:43:47.715333 [ 1271 ] {} <Debug> StorageKafka (queue_05): [rdk:CGRPOP] [thrd:main]: Group \"g03\" received op GET_SUBSCRIPTION in state up (join-state wait-assign-call)\r\n```\r\n\r\nWith the same Kafka broker settings, I setup another consumer group with 2 members (by `kafka-console-consumer.sh`) to consume data from single partition of topic `topic03`, when one member is down, consumer group rebalances and assigns the topic partition to the rest member successfully.\r\n\r\n\r\n## Question\r\nI guess I might have wrong configurations in ClickHouse and find no clue myself. Do you have idea of Kafka consumer group failover issue in my case? Any comment is appreciated.\r\n\n",
  "hints_text": "",
  "created_at": "2021-02-26T23:27:49Z"
}