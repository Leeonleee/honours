{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 25969,
  "instance_id": "ClickHouse__ClickHouse-25969",
  "issue_numbers": [
    "6953"
  ],
  "base_commit": "8986de7e507cdce8ed6918ed697578d5b5cd2055",
  "patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 7808edeff9bb..1790ddc04515 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -581,6 +581,7 @@ include (cmake/find/yaml-cpp.cmake)\n include (cmake/find/s2geometry.cmake)\n include (cmake/find/nlp.cmake)\n include (cmake/find/bzip2.cmake)\n+include (cmake/find/filelog.cmake)\n \n if(NOT USE_INTERNAL_PARQUET_LIBRARY)\n     set (ENABLE_ORC OFF CACHE INTERNAL \"\")\ndiff --git a/cmake/find/filelog.cmake b/cmake/find/filelog.cmake\nnew file mode 100644\nindex 000000000000..4d2f94f3f20a\n--- /dev/null\n+++ b/cmake/find/filelog.cmake\n@@ -0,0 +1,15 @@\n+option (ENABLE_FILELOG \"Enable FILELOG\" ON)\n+\n+if (NOT ENABLE_FILELOG)\n+\tmessage (${RECONFIGURE_MESSAGE_LEVEL} \"Can't use StorageFileLog with ENABLE_FILELOG=OFF\")\n+    return()\n+endif()\n+\n+# StorageFileLog only support Linux platform\n+if (OS_LINUX)\n+    set (USE_FILELOG 1)\n+    message (STATUS \"Using StorageFileLog = 1\")\n+else()\n+    message(STATUS \"StorageFileLog is only supported on Linux\")\n+endif ()\n+\ndiff --git a/contrib/poco b/contrib/poco\nindex 46c80daf1b01..39fd359765a3 160000\n--- a/contrib/poco\n+++ b/contrib/poco\n@@ -1,1 +1,1 @@\n-Subproject commit 46c80daf1b015aa10474ce82e3d24b578c6ae422\n+Subproject commit 39fd359765a3a77b46d94ec3c5def3c7802a920f\ndiff --git a/src/CMakeLists.txt b/src/CMakeLists.txt\nindex 09aaa85c394b..24014db98cd3 100644\n--- a/src/CMakeLists.txt\n+++ b/src/CMakeLists.txt\n@@ -109,6 +109,10 @@ if (USE_HDFS)\n     add_headers_and_sources(dbms Disks/HDFS)\n endif()\n \n+if(USE_FILELOG)\n+    add_headers_and_sources(dbms Storages/FileLog)\n+endif()\n+\n list (APPEND clickhouse_common_io_sources ${CONFIG_BUILD})\n list (APPEND clickhouse_common_io_headers ${CONFIG_VERSION} ${CONFIG_COMMON})\n \ndiff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp\nindex 1aff14601253..2aadda426e75 100644\n--- a/src/Common/ErrorCodes.cpp\n+++ b/src/Common/ErrorCodes.cpp\n@@ -591,6 +591,8 @@\n     M(621, CANNOT_NORMALIZE_STRING) \\\n     M(622, CANNOT_PARSE_CAPN_PROTO_SCHEMA) \\\n     M(623, CAPN_PROTO_BAD_CAST) \\\n+    M(624, BAD_FILE_TYPE) \\\n+    M(625, IO_SETUP_ERROR) \\\n     \\\n     M(999, KEEPER_EXCEPTION) \\\n     M(1000, POCO_EXCEPTION) \\\ndiff --git a/src/Core/config_core.h.in b/src/Core/config_core.h.in\nindex cc9c993b2055..11dd9bf96f13 100644\n--- a/src/Core/config_core.h.in\n+++ b/src/Core/config_core.h.in\n@@ -17,3 +17,4 @@\n #cmakedefine01 USE_NURAFT\n #cmakedefine01 USE_NLP\n #cmakedefine01 USE_KRB5\n+#cmakedefine01 USE_FILELOG\ndiff --git a/src/Databases/DatabaseAtomic.cpp b/src/Databases/DatabaseAtomic.cpp\nindex 5c75f6f1036b..ae90f1a69008 100644\n--- a/src/Databases/DatabaseAtomic.cpp\n+++ b/src/Databases/DatabaseAtomic.cpp\n@@ -140,6 +140,9 @@ void DatabaseAtomic::dropTable(ContextPtr local_context, const String & table_na\n     if (table->storesDataOnDisk())\n         tryRemoveSymlink(table_name);\n \n+    if (table->dropTableImmediately())\n+        table->drop();\n+\n     /// Notify DatabaseCatalog that table was dropped. It will remove table data in background.\n     /// Cleanup is performed outside of database to allow easily DROP DATABASE without waiting for cleanup to complete.\n     DatabaseCatalog::instance().enqueueDroppedTableCleanup(table->getStorageID(), table, table_metadata_path_drop, no_delay);\ndiff --git a/src/Storages/FileLog/Buffer_fwd.h b/src/Storages/FileLog/Buffer_fwd.h\nnew file mode 100644\nindex 000000000000..ec644aa7d369\n--- /dev/null\n+++ b/src/Storages/FileLog/Buffer_fwd.h\n@@ -0,0 +1,10 @@\n+#pragma once\n+\n+#include <memory>\n+\n+namespace DB\n+{\n+class ReadBufferFromFileLog;\n+\n+using ReadBufferFromFileLogPtr = std::shared_ptr<ReadBufferFromFileLog>;\n+}\ndiff --git a/src/Storages/FileLog/DirectoryWatcherBase.cpp b/src/Storages/FileLog/DirectoryWatcherBase.cpp\nnew file mode 100644\nindex 000000000000..c459079ec06d\n--- /dev/null\n+++ b/src/Storages/FileLog/DirectoryWatcherBase.cpp\n@@ -0,0 +1,148 @@\n+#include <Interpreters/Context.h>\n+#include <Storages/FileLog/DirectoryWatcherBase.h>\n+#include <Storages/FileLog/FileLogDirectoryWatcher.h>\n+#include <Storages/FileLog/StorageFileLog.h>\n+#include <base/sleep.h>\n+\n+#include <filesystem>\n+#include <unistd.h>\n+#include <sys/inotify.h>\n+#include <sys/poll.h>\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int FILE_DOESNT_EXIST;\n+    extern const int BAD_FILE_TYPE;\n+    extern const int IO_SETUP_ERROR;\n+}\n+\n+static constexpr int buffer_size = 4096;\n+\n+DirectoryWatcherBase::DirectoryWatcherBase(\n+    FileLogDirectoryWatcher & owner_, const std::string & path_, ContextPtr context_, int event_mask_)\n+    : WithContext(context_)\n+    , owner(owner_)\n+    , path(path_)\n+    , event_mask(event_mask_)\n+    , milliseconds_to_wait(owner.storage.getFileLogSettings()->poll_directory_watch_events_backoff_init.totalMilliseconds())\n+{\n+    if (!std::filesystem::exists(path))\n+        throw Exception(ErrorCodes::FILE_DOESNT_EXIST, \"Path {} does not exist\", path);\n+\n+    if (!std::filesystem::is_directory(path))\n+        throw Exception(ErrorCodes::BAD_FILE_TYPE, \"Path {} is not a directory\", path);\n+\n+    fd = inotify_init();\n+    if (fd == -1)\n+        throw Exception(\"Cannot initialize inotify\", ErrorCodes::IO_SETUP_ERROR);\n+\n+    watch_task = getContext()->getSchedulePool().createTask(\"directory_watch\", [this] { watchFunc(); });\n+    start();\n+}\n+\n+void DirectoryWatcherBase::watchFunc()\n+{\n+    int mask = 0;\n+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_ADDED)\n+        mask |= IN_CREATE;\n+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_REMOVED)\n+        mask |= IN_DELETE;\n+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_MODIFIED)\n+        mask |= IN_MODIFY;\n+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_FROM)\n+        mask |= IN_MOVED_FROM;\n+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_TO)\n+        mask |= IN_MOVED_TO;\n+\n+    int wd = inotify_add_watch(fd, path.c_str(), mask);\n+    if (wd == -1)\n+    {\n+        owner.onError(Exception(ErrorCodes::IO_SETUP_ERROR, \"Watch directory {} failed\", path));\n+    }\n+\n+    std::string buffer;\n+    buffer.resize(buffer_size);\n+    pollfd pfd;\n+    pfd.fd = fd;\n+    pfd.events = POLLIN;\n+    while (!stopped)\n+    {\n+        const auto & settings = owner.storage.getFileLogSettings();\n+        if (poll(&pfd, 1, milliseconds_to_wait) > 0 && pfd.revents & POLLIN)\n+        {\n+            milliseconds_to_wait = settings->poll_directory_watch_events_backoff_init.totalMilliseconds();\n+            int n = read(fd, buffer.data(), buffer.size());\n+            int i = 0;\n+            if (n > 0)\n+            {\n+                while (n > 0)\n+                {\n+                    struct inotify_event * p_event = reinterpret_cast<struct inotify_event *>(buffer.data() + i);\n+\n+                    if (p_event->len > 0)\n+                    {\n+                        if ((p_event->mask & IN_CREATE) && (eventMask() & DirectoryWatcherBase::DW_ITEM_ADDED))\n+                        {\n+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_ADDED);\n+                            owner.onItemAdded(ev);\n+                        }\n+                        if ((p_event->mask & IN_DELETE) && (eventMask() & DirectoryWatcherBase::DW_ITEM_REMOVED))\n+                        {\n+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_REMOVED);\n+                            owner.onItemRemoved(ev);\n+                        }\n+                        if ((p_event->mask & IN_MODIFY) && (eventMask() & DirectoryWatcherBase::DW_ITEM_MODIFIED))\n+                        {\n+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_MODIFIED);\n+                            owner.onItemModified(ev);\n+                        }\n+                        if ((p_event->mask & IN_MOVED_FROM) && (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_FROM))\n+                        {\n+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_MOVED_FROM);\n+                            owner.onItemMovedFrom(ev);\n+                        }\n+                        if ((p_event->mask & IN_MOVED_TO) && (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_TO))\n+                        {\n+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_MOVED_TO);\n+                            owner.onItemMovedTo(ev);\n+                        }\n+                    }\n+\n+                    i += sizeof(inotify_event) + p_event->len;\n+                    n -= sizeof(inotify_event) + p_event->len;\n+                }\n+            }\n+\n+            /// Wake up reader thread\n+            owner.storage.wakeUp();\n+        }\n+        else\n+        {\n+            if (milliseconds_to_wait < static_cast<uint64_t>(settings->poll_directory_watch_events_backoff_max.totalMilliseconds()))\n+                milliseconds_to_wait *= settings->poll_directory_watch_events_backoff_factor.value;\n+        }\n+    }\n+}\n+\n+DirectoryWatcherBase::~DirectoryWatcherBase()\n+{\n+    stop();\n+    close(fd);\n+}\n+\n+void DirectoryWatcherBase::start()\n+{\n+    if (watch_task)\n+        watch_task->activateAndSchedule();\n+}\n+\n+void DirectoryWatcherBase::stop()\n+{\n+    stopped = true;\n+    if (watch_task)\n+        watch_task->deactivate();\n+}\n+\n+}\ndiff --git a/src/Storages/FileLog/DirectoryWatcherBase.h b/src/Storages/FileLog/DirectoryWatcherBase.h\nnew file mode 100644\nindex 000000000000..a640f686c8a8\n--- /dev/null\n+++ b/src/Storages/FileLog/DirectoryWatcherBase.h\n@@ -0,0 +1,108 @@\n+#pragma once\n+\n+#include <Core/BackgroundSchedulePool.h>\n+\n+#include <atomic>\n+#include <memory>\n+#include <string>\n+\n+namespace DB\n+{\n+class FileLogDirectoryWatcher;\n+\n+class DirectoryWatcherBase : WithContext\n+{\n+    /// Most of code in this class is copy from the Poco project:\n+    /// https://github.com/ClickHouse-Extras/poco/blob/clickhouse/Foundation/src/DirectoryWatcher.cpp\n+    /// This class is used to get notifications about changes\n+    /// to the filesystem, more precisely, to a specific\n+    /// directory. Changes to a directory are reported via\n+    /// events.\n+    ///\n+    /// A thread will be created that watches the specified\n+    /// directory for changes. Events are reported in the context\n+    /// of this thread.\n+    ///\n+    /// Note that changes to files in subdirectories of the watched\n+    /// directory are not reported. Separate DirectoryWatcher objects\n+    /// must be created for these directories if they should be watched.\n+public:\n+    enum DirectoryEventType\n+    {\n+        DW_ITEM_ADDED = 1,\n+        /// A new item has been created and added to the directory.\n+\n+        DW_ITEM_REMOVED = 2,\n+        /// An item has been removed from the directory.\n+\n+        DW_ITEM_MODIFIED = 4,\n+        /// An item has been modified.\n+\n+        DW_ITEM_MOVED_FROM = 8,\n+        /// An item has been renamed or moved. This event delivers the old name.\n+\n+        DW_ITEM_MOVED_TO = 16\n+        /// An item has been renamed or moved. This event delivers the new name.\n+    };\n+\n+    enum DirectoryEventMask\n+    {\n+        /// Enables all event types.\n+        DW_FILTER_ENABLE_ALL = 31,\n+\n+        /// Disables all event types.\n+        DW_FILTER_DISABLE_ALL = 0\n+    };\n+\n+    struct DirectoryEvent\n+    {\n+        DirectoryEvent(const std::string & f, DirectoryEventType ev) : path(f), event(ev) { }\n+\n+        /// The directory or file that has been changed.\n+        const std::string path;\n+        /// The kind of event.\n+        DirectoryEventType event;\n+    };\n+\n+\n+    DirectoryWatcherBase() = delete;\n+    DirectoryWatcherBase(const DirectoryWatcherBase &) = delete;\n+    DirectoryWatcherBase & operator=(const DirectoryWatcherBase &) = delete;\n+\n+    /// Creates a DirectoryWatcher for the directory given in path.\n+    /// To enable only specific events, an eventMask can be specified by\n+    /// OR-ing the desired event IDs (e.g., DW_ITEM_ADDED | DW_ITEM_MODIFIED).\n+    explicit DirectoryWatcherBase(\n+        FileLogDirectoryWatcher & owner_, const std::string & path_, ContextPtr context_, int event_mask_ = DW_FILTER_ENABLE_ALL);\n+\n+    ~DirectoryWatcherBase();\n+\n+    /// Returns the value of the eventMask passed to the constructor.\n+    int eventMask() const { return event_mask; }\n+\n+    /// Returns the directory being watched.\n+    const std::string & directory() const;\n+\n+    void watchFunc();\n+\n+protected:\n+    void start();\n+    void stop();\n+\n+private:\n+    FileLogDirectoryWatcher & owner;\n+\n+    using TaskThread = BackgroundSchedulePool::TaskHolder;\n+    TaskThread watch_task;\n+\n+    std::atomic<bool> stopped{false};\n+\n+\n+    const std::string path;\n+    int event_mask;\n+    uint64_t milliseconds_to_wait;\n+\n+    int fd;\n+};\n+\n+}\ndiff --git a/src/Storages/FileLog/FileLogDirectoryWatcher.cpp b/src/Storages/FileLog/FileLogDirectoryWatcher.cpp\nnew file mode 100644\nindex 000000000000..192721f9f3c3\n--- /dev/null\n+++ b/src/Storages/FileLog/FileLogDirectoryWatcher.cpp\n@@ -0,0 +1,139 @@\n+#include <Storages/FileLog/FileLogDirectoryWatcher.h>\n+\n+namespace DB\n+{\n+FileLogDirectoryWatcher::FileLogDirectoryWatcher(const std::string & path_, StorageFileLog & storage_, ContextPtr context_)\n+    : path(path_)\n+    , storage(storage_)\n+    , log(&Poco::Logger::get(\"FileLogDirectoryWatcher(\" + path + \")\"))\n+    , dw(std::make_unique<DirectoryWatcherBase>(*this, path, context_))\n+{\n+}\n+\n+FileLogDirectoryWatcher::Events FileLogDirectoryWatcher::getEventsAndReset()\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+    Events res;\n+    res.swap(events);\n+    return res;\n+}\n+\n+FileLogDirectoryWatcher::Error FileLogDirectoryWatcher::getErrorAndReset()\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+    Error old_error = error;\n+    error = {};\n+    return old_error;\n+}\n+\n+const std::string & FileLogDirectoryWatcher::getPath() const\n+{\n+    return path;\n+}\n+\n+void FileLogDirectoryWatcher::onItemAdded(DirectoryWatcherBase::DirectoryEvent ev)\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+\n+    EventInfo info{ev.event, \"onItemAdded\"};\n+    std::string event_path = ev.path;\n+\n+    if (auto it = events.find(event_path); it != events.end())\n+    {\n+        it->second.file_events.emplace_back(info);\n+    }\n+    else\n+    {\n+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});\n+    }\n+}\n+\n+\n+void FileLogDirectoryWatcher::onItemRemoved(DirectoryWatcherBase::DirectoryEvent ev)\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+\n+    EventInfo info{ev.event, \"onItemRemoved\"};\n+    std::string event_path = ev.path;\n+\n+    if (auto it = events.find(event_path); it != events.end())\n+    {\n+        it->second.file_events.emplace_back(info);\n+    }\n+    else\n+    {\n+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});\n+    }\n+}\n+\n+/// Optimize for MODIFY event, during a streamToViews period, since the log files\n+/// are append only, there are may a lots of MODIFY events produced for one file.\n+/// For example, appending 10000 logs into one file will result in 10000 MODIFY event.\n+/// So, if we record all of these events, it will use a lot of memory, and then we\n+/// need to handle it one by one in StorageFileLog::updateFileInfos, this is unnecessary\n+/// because it is equal to just record and handle one MODIY event\n+void FileLogDirectoryWatcher::onItemModified(DirectoryWatcherBase::DirectoryEvent ev)\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+\n+    auto event_path = ev.path;\n+    EventInfo info{ev.event, \"onItemModified\"};\n+    if (auto it = events.find(event_path); it != events.end())\n+    {\n+        /// Already have MODIFY event for this file\n+        if (it->second.received_modification_event)\n+            return;\n+        else\n+        {\n+            it->second.received_modification_event = true;\n+            it->second.file_events.emplace_back(info);\n+        }\n+    }\n+    else\n+    {\n+        events.emplace(event_path, FileEvents{.received_modification_event = true, .file_events = std::vector<EventInfo>{info}});\n+    }\n+}\n+\n+void FileLogDirectoryWatcher::onItemMovedFrom(DirectoryWatcherBase::DirectoryEvent ev)\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+\n+    EventInfo info{ev.event, \"onItemMovedFrom\"};\n+    std::string event_path = ev.path;\n+\n+    if (auto it = events.find(event_path); it != events.end())\n+    {\n+        it->second.file_events.emplace_back(info);\n+    }\n+    else\n+    {\n+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});\n+    }\n+}\n+\n+void FileLogDirectoryWatcher::onItemMovedTo(DirectoryWatcherBase::DirectoryEvent ev)\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+\n+    EventInfo info{ev.event, \"onItemMovedTo\"};\n+    std::string event_path = ev.path;\n+\n+    if (auto it = events.find(event_path); it != events.end())\n+    {\n+        it->second.file_events.emplace_back(info);\n+    }\n+    else\n+    {\n+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});\n+    }\n+}\n+\n+void FileLogDirectoryWatcher::onError(Exception e)\n+{\n+    std::lock_guard<std::mutex> lock(mutex);\n+    LOG_ERROR(log, \"Error happened during watching directory: {}\", error.error_msg);\n+    error.has_error = true;\n+    error.error_msg = e.message();\n+}\n+}\ndiff --git a/src/Storages/FileLog/FileLogDirectoryWatcher.h b/src/Storages/FileLog/FileLogDirectoryWatcher.h\nnew file mode 100644\nindex 000000000000..0b0c86397aab\n--- /dev/null\n+++ b/src/Storages/FileLog/FileLogDirectoryWatcher.h\n@@ -0,0 +1,77 @@\n+#pragma once\n+\n+#include <Storages/FileLog/DirectoryWatcherBase.h>\n+\n+#include <base/logger_useful.h>\n+\n+#include <memory>\n+#include <mutex>\n+\n+namespace DB\n+{\n+class StorageFileLog;\n+\n+class FileLogDirectoryWatcher\n+{\n+public:\n+    struct EventInfo\n+    {\n+        DirectoryWatcherBase::DirectoryEventType type;\n+        std::string callback;\n+    };\n+\n+    struct FileEvents\n+    {\n+        bool received_modification_event = false;\n+        std::vector<EventInfo> file_events;\n+    };\n+\n+    using Events = std::unordered_map<std::string, FileEvents>;\n+\n+    struct Error\n+    {\n+        bool has_error = false;\n+        std::string error_msg = {};\n+    };\n+\n+    FileLogDirectoryWatcher(const std::string & path_, StorageFileLog & storage_, ContextPtr context_);\n+    ~FileLogDirectoryWatcher() = default;\n+\n+    Events getEventsAndReset();\n+\n+    Error getErrorAndReset();\n+\n+    const std::string & getPath() const;\n+\n+private:\n+    friend class DirectoryWatcherBase;\n+    /// Here must pass by value, otherwise will lead to stack-use-of-scope\n+    void onItemAdded(DirectoryWatcherBase::DirectoryEvent ev);\n+    void onItemRemoved(DirectoryWatcherBase::DirectoryEvent ev);\n+    void onItemModified(DirectoryWatcherBase::DirectoryEvent ev);\n+    void onItemMovedFrom(DirectoryWatcherBase::DirectoryEvent ev);\n+    void onItemMovedTo(DirectoryWatcherBase::DirectoryEvent ev);\n+    void onError(Exception);\n+\n+    const std::string path;\n+\n+    StorageFileLog & storage;\n+\n+    /// Note, in order to avoid data race found by fuzzer, put events before dw,\n+    /// such that when this class destruction, dw will be destructed before events.\n+    /// The data race is because dw create a separate thread to monitor file events\n+    /// and put into events, then if we destruct events first, the monitor thread still\n+    /// running, it may access events during events destruction, leads to data race.\n+    /// And we should put other members before dw as well, because all of them can be\n+    /// accessed in thread created by dw.\n+    Events events;\n+\n+    Poco::Logger * log;\n+\n+    std::mutex mutex;\n+\n+    Error error;\n+\n+    std::unique_ptr<DirectoryWatcherBase> dw;\n+};\n+}\ndiff --git a/src/Storages/FileLog/FileLogSettings.cpp b/src/Storages/FileLog/FileLogSettings.cpp\nnew file mode 100644\nindex 000000000000..2cd42c35870c\n--- /dev/null\n+++ b/src/Storages/FileLog/FileLogSettings.cpp\n@@ -0,0 +1,41 @@\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTSetQuery.h>\n+#include <Storages/FileLog/FileLogSettings.h>\n+#include <Common/Exception.h>\n+\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int UNKNOWN_SETTING;\n+}\n+\n+IMPLEMENT_SETTINGS_TRAITS(FileLogSettingsTraits, LIST_OF_FILELOG_SETTINGS)\n+\n+void FileLogSettings::loadFromQuery(ASTStorage & storage_def)\n+{\n+    if (storage_def.settings)\n+    {\n+        try\n+        {\n+            applyChanges(storage_def.settings->changes);\n+        }\n+        catch (Exception & e)\n+        {\n+            if (e.code() == ErrorCodes::UNKNOWN_SETTING)\n+                e.addMessage(\"for storage \" + storage_def.engine->name);\n+            throw;\n+        }\n+    }\n+    else\n+    {\n+        auto settings_ast = std::make_shared<ASTSetQuery>();\n+        settings_ast->is_standalone = false;\n+        storage_def.set(storage_def.settings, settings_ast);\n+    }\n+}\n+\n+}\ndiff --git a/src/Storages/FileLog/FileLogSettings.h b/src/Storages/FileLog/FileLogSettings.h\nnew file mode 100644\nindex 000000000000..d14120d0ba0d\n--- /dev/null\n+++ b/src/Storages/FileLog/FileLogSettings.h\n@@ -0,0 +1,37 @@\n+#pragma once\n+\n+#include <Core/BaseSettings.h>\n+#include <Core/Settings.h>\n+\n+\n+namespace DB\n+{\n+class ASTStorage;\n+\n+\n+#define FILELOG_RELATED_SETTINGS(M) \\\n+    /* default is stream_poll_timeout_ms */ \\\n+    M(Milliseconds, poll_timeout_ms, 0, \"Timeout for single poll from StorageFileLog.\", 0) \\\n+    M(UInt64, poll_max_batch_size, 0, \"Maximum amount of messages to be polled in a single StorageFileLog poll.\", 0) \\\n+    M(UInt64, max_block_size, 0, \"Number of row collected by poll(s) for flushing data from StorageFileLog.\", 0) \\\n+    M(UInt64, max_threads, 8, \"Number of max threads to parse files, default is 8\", 0) \\\n+    M(Milliseconds, poll_directory_watch_events_backoff_init, 500, \"The initial sleep value for watch directory thread.\", 0) \\\n+    M(Milliseconds, poll_directory_watch_events_backoff_max, 32000, \"The max sleep value for watch directory thread.\", 0) \\\n+    M(UInt64, poll_directory_watch_events_backoff_factor, 2, \"The speed of backoff, exponential by default\", 0)\n+\n+#define LIST_OF_FILELOG_SETTINGS(M) \\\n+    FILELOG_RELATED_SETTINGS(M) \\\n+    FORMAT_FACTORY_SETTINGS(M)\n+\n+DECLARE_SETTINGS_TRAITS(FileLogSettingsTraits, LIST_OF_FILELOG_SETTINGS)\n+\n+\n+/** Settings for the FileLog engine.\n+  * Could be loaded from a CREATE TABLE query (SETTINGS clause).\n+  */\n+struct FileLogSettings : public BaseSettings<FileLogSettingsTraits>\n+{\n+    void loadFromQuery(ASTStorage & storage_def);\n+};\n+\n+}\ndiff --git a/src/Storages/FileLog/FileLogSource.cpp b/src/Storages/FileLog/FileLogSource.cpp\nnew file mode 100644\nindex 000000000000..a8da34f32041\n--- /dev/null\n+++ b/src/Storages/FileLog/FileLogSource.cpp\n@@ -0,0 +1,143 @@\n+#include <Formats/FormatFactory.h>\n+#include <Interpreters/Context.h>\n+#include <Processors/Executors/StreamingFormatExecutor.h>\n+#include <Storages/FileLog/FileLogSource.h>\n+#include <Storages/FileLog/ReadBufferFromFileLog.h>\n+#include <Common/Stopwatch.h>\n+#include <base/logger_useful.h>\n+\n+namespace DB\n+{\n+static constexpr auto MAX_FAILED_POLL_ATTEMPTS = 10;\n+\n+FileLogSource::FileLogSource(\n+    StorageFileLog & storage_,\n+    const StorageMetadataPtr & metadata_snapshot_,\n+    const ContextPtr & context_,\n+    const Names & columns,\n+    size_t max_block_size_,\n+    size_t poll_time_out_,\n+    size_t stream_number_,\n+    size_t max_streams_number_)\n+    : SourceWithProgress(metadata_snapshot_->getSampleBlockForColumns(columns, storage_.getVirtuals(), storage_.getStorageID()))\n+    , storage(storage_)\n+    , metadata_snapshot(metadata_snapshot_)\n+    , context(context_)\n+    , column_names(columns)\n+    , max_block_size(max_block_size_)\n+    , poll_time_out(poll_time_out_)\n+    , stream_number(stream_number_)\n+    , max_streams_number(max_streams_number_)\n+    , non_virtual_header(metadata_snapshot_->getSampleBlockNonMaterialized())\n+    , virtual_header(\n+          metadata_snapshot->getSampleBlockForColumns(storage.getVirtualColumnNames(), storage.getVirtuals(), storage.getStorageID()))\n+{\n+    buffer = std::make_unique<ReadBufferFromFileLog>(storage, max_block_size, poll_time_out, context, stream_number_, max_streams_number_);\n+\n+    const auto & file_infos = storage.getFileInfos();\n+\n+    size_t files_per_stream = file_infos.file_names.size() / max_streams_number;\n+    start = stream_number * files_per_stream;\n+    end = stream_number == max_streams_number - 1 ? file_infos.file_names.size() : (stream_number + 1) * files_per_stream;\n+\n+    storage.increaseStreams();\n+}\n+\n+FileLogSource::~FileLogSource()\n+{\n+    try\n+    {\n+        if (!finished)\n+            onFinish();\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(__PRETTY_FUNCTION__);\n+    }\n+    storage.reduceStreams();\n+}\n+\n+void FileLogSource::onFinish()\n+{\n+    storage.closeFilesAndStoreMeta(start, end);\n+    finished = true;\n+}\n+\n+Chunk FileLogSource::generate()\n+{\n+    /// Store metas of last written chunk into disk\n+    storage.storeMetas(start, end);\n+\n+    if (!buffer || buffer->noRecords())\n+    {\n+        /// There is no onFinish for ISource, we call it\n+        /// when no records return to close files\n+        onFinish();\n+        return {};\n+    }\n+\n+    MutableColumns virtual_columns = virtual_header.cloneEmptyColumns();\n+\n+    auto input_format\n+        = FormatFactory::instance().getInputFormat(storage.getFormatName(), *buffer, non_virtual_header, context, max_block_size);\n+\n+    StreamingFormatExecutor executor(non_virtual_header, input_format);\n+\n+    size_t total_rows = 0;\n+    size_t failed_poll_attempts = 0;\n+\n+    Stopwatch watch;\n+    while (true)\n+    {\n+        size_t new_rows = 0;\n+        if (buffer->poll())\n+            new_rows = executor.execute();\n+\n+        if (new_rows)\n+        {\n+            auto file_name = buffer->getFileName();\n+            auto offset = buffer->getOffset();\n+            for (size_t i = 0; i < new_rows; ++i)\n+            {\n+                virtual_columns[0]->insert(file_name);\n+                virtual_columns[1]->insert(offset);\n+            }\n+            total_rows = total_rows + new_rows;\n+        }\n+        else /// poll succeed, but parse failed\n+        {\n+            ++failed_poll_attempts;\n+        }\n+\n+        if (!buffer->hasMorePolledRecords()\n+            && ((total_rows >= max_block_size) || watch.elapsedMilliseconds() > poll_time_out\n+                || failed_poll_attempts >= MAX_FAILED_POLL_ATTEMPTS))\n+        {\n+            break;\n+        }\n+    }\n+\n+    if (total_rows == 0)\n+    {\n+        onFinish();\n+        return {};\n+    }\n+\n+    auto result_block = non_virtual_header.cloneWithColumns(executor.getResultColumns());\n+    auto virtual_block = virtual_header.cloneWithColumns(std::move(virtual_columns));\n+\n+    for (const auto & column : virtual_block.getColumnsWithTypeAndName())\n+        result_block.insert(column);\n+\n+    auto converting_dag = ActionsDAG::makeConvertingActions(\n+        result_block.cloneEmpty().getColumnsWithTypeAndName(),\n+        getPort().getHeader().getColumnsWithTypeAndName(),\n+        ActionsDAG::MatchColumnsMode::Name);\n+\n+    auto converting_actions = std::make_shared<ExpressionActions>(std::move(converting_dag));\n+    converting_actions->execute(result_block);\n+\n+    return Chunk(result_block.getColumns(), result_block.rows());\n+}\n+\n+}\ndiff --git a/src/Storages/FileLog/FileLogSource.h b/src/Storages/FileLog/FileLogSource.h\nnew file mode 100644\nindex 000000000000..e1348de5bdf7\n--- /dev/null\n+++ b/src/Storages/FileLog/FileLogSource.h\n@@ -0,0 +1,61 @@\n+#pragma once\n+\n+#include <Processors/Sources/SourceWithProgress.h>\n+\n+#include <Storages/FileLog/ReadBufferFromFileLog.h>\n+#include <Storages/FileLog/StorageFileLog.h>\n+\n+namespace Poco\n+{\n+    class Logger;\n+}\n+namespace DB\n+{\n+class FileLogSource : public SourceWithProgress\n+{\n+public:\n+    FileLogSource(\n+        StorageFileLog & storage_,\n+        const StorageMetadataPtr & metadata_snapshot_,\n+        const ContextPtr & context_,\n+        const Names & columns,\n+        size_t max_block_size_,\n+        size_t poll_time_out_,\n+        size_t stream_number_,\n+        size_t max_streams_number_);\n+\n+    String getName() const override { return \"FileLog\"; }\n+\n+    bool noRecords() { return !buffer || buffer->noRecords(); }\n+\n+    void onFinish();\n+\n+    virtual ~FileLogSource() override;\n+\n+protected:\n+    Chunk generate() override;\n+\n+private:\n+    StorageFileLog & storage;\n+    StorageMetadataPtr metadata_snapshot;\n+    ContextPtr context;\n+    Names column_names;\n+    UInt64 max_block_size;\n+\n+    size_t poll_time_out;\n+\n+    size_t stream_number;\n+    size_t max_streams_number;\n+\n+    std::unique_ptr<ReadBufferFromFileLog> buffer;\n+\n+    Block non_virtual_header;\n+    Block virtual_header;\n+\n+    /// The start pos and end pos of files responsible by this stream,\n+    /// does not include end\n+    size_t start;\n+    size_t end;\n+};\n+\n+}\ndiff --git a/src/Storages/FileLog/ReadBufferFromFileLog.cpp b/src/Storages/FileLog/ReadBufferFromFileLog.cpp\nnew file mode 100644\nindex 000000000000..a55df9fe09e2\n--- /dev/null\n+++ b/src/Storages/FileLog/ReadBufferFromFileLog.cpp\n@@ -0,0 +1,176 @@\n+#include <Interpreters/Context.h>\n+#include <Storages/FileLog/ReadBufferFromFileLog.h>\n+#include <Common/Stopwatch.h>\n+\n+#include <base/logger_useful.h>\n+\n+#include <algorithm>\n+#include <filesystem>\n+#include <boost/algorithm/string/join.hpp>\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int CANNOT_READ_ALL_DATA;\n+}\n+\n+ReadBufferFromFileLog::ReadBufferFromFileLog(\n+    StorageFileLog & storage_,\n+    size_t max_batch_size,\n+    size_t poll_timeout_,\n+    ContextPtr context_,\n+    size_t stream_number_,\n+    size_t max_streams_number_)\n+    : ReadBuffer(nullptr, 0)\n+    , log(&Poco::Logger::get(\"ReadBufferFromFileLog \" + toString(stream_number_)))\n+    , storage(storage_)\n+    , batch_size(max_batch_size)\n+    , poll_timeout(poll_timeout_)\n+    , context(context_)\n+    , stream_number(stream_number_)\n+    , max_streams_number(max_streams_number_)\n+{\n+    current = records.begin();\n+    allowed = false;\n+}\n+\n+bool ReadBufferFromFileLog::poll()\n+{\n+    if (hasMorePolledRecords())\n+    {\n+        allowed = true;\n+        return true;\n+    }\n+\n+    auto new_records = pollBatch(batch_size);\n+    if (new_records.empty())\n+    {\n+        buffer_status = BufferStatus::NO_RECORD_RETURNED;\n+        LOG_TRACE(log, \"No new records to read\");\n+        return false;\n+    }\n+    else\n+    {\n+        records = std::move(new_records);\n+        current = records.begin();\n+\n+        LOG_TRACE(log, \"Polled batch of {} records. \", records.size());\n+\n+        buffer_status = BufferStatus::POLLED_OK;\n+        allowed = true;\n+        return true;\n+    }\n+}\n+\n+ReadBufferFromFileLog::Records ReadBufferFromFileLog::pollBatch(size_t batch_size_)\n+{\n+    Records new_records;\n+    new_records.reserve(batch_size_);\n+\n+    readNewRecords(new_records, batch_size);\n+    if (new_records.size() == batch_size_ || stream_out)\n+        return new_records;\n+\n+    Stopwatch watch;\n+    while (watch.elapsedMilliseconds() < poll_timeout && new_records.size() != batch_size_)\n+    {\n+        readNewRecords(new_records, batch_size);\n+        /// All ifstrem reach end, no need to wait for timeout,\n+        /// since file status can not be updated during a streamToViews\n+        if (stream_out)\n+            break;\n+    }\n+\n+    return new_records;\n+}\n+\n+void ReadBufferFromFileLog::readNewRecords(ReadBufferFromFileLog::Records & new_records, size_t batch_size_)\n+{\n+    size_t need_records_size = batch_size_ - new_records.size();\n+    size_t read_records_size = 0;\n+\n+    auto & file_infos = storage.getFileInfos();\n+\n+    size_t files_per_stream = file_infos.file_names.size() / max_streams_number;\n+    size_t start = stream_number * files_per_stream;\n+    size_t end = stream_number == max_streams_number - 1 ? file_infos.file_names.size() : (stream_number + 1) * files_per_stream;\n+\n+    for (size_t i = start; i < end; ++i)\n+    {\n+        const auto & file_name = file_infos.file_names[i];\n+\n+        auto & file_ctx = StorageFileLog::findInMap(file_infos.context_by_name, file_name);\n+        if (file_ctx.status == StorageFileLog::FileStatus::NO_CHANGE)\n+            continue;\n+\n+        auto & file_meta = StorageFileLog::findInMap(file_infos.meta_by_inode, file_ctx.inode);\n+\n+        if (!file_ctx.reader)\n+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Ifstream for file {} is not initialized\", file_meta.file_name);\n+\n+        auto & reader = file_ctx.reader.value();\n+        StorageFileLog::assertStreamGood(reader);\n+\n+        Record record;\n+        while (read_records_size < need_records_size)\n+        {\n+            /// Need to get offset before reading record from stream\n+            auto offset = reader.tellg();\n+            if (static_cast<UInt64>(offset) >= file_meta.last_open_end)\n+                break;\n+            record.offset = offset;\n+            StorageFileLog::assertStreamGood(reader);\n+\n+            record.file_name = file_name;\n+\n+\n+            std::getline(reader, record.data);\n+            StorageFileLog::assertStreamGood(reader);\n+\n+            new_records.emplace_back(record);\n+            ++read_records_size;\n+        }\n+\n+        UInt64 current_position = reader.tellg();\n+        StorageFileLog::assertStreamGood(reader);\n+\n+        file_meta.last_writen_position = current_position;\n+\n+        /// stream reach to end\n+        if (current_position == file_meta.last_open_end)\n+        {\n+            file_ctx.status = StorageFileLog::FileStatus::NO_CHANGE;\n+        }\n+\n+        /// All ifstream reach end\n+        if (i == end - 1 && (file_ctx.status == StorageFileLog::FileStatus::NO_CHANGE))\n+        {\n+            stream_out = true;\n+        }\n+\n+        if (read_records_size == need_records_size)\n+        {\n+            break;\n+        }\n+    }\n+}\n+\n+bool ReadBufferFromFileLog::nextImpl()\n+{\n+    if (!allowed || !hasMorePolledRecords())\n+        return false;\n+\n+    auto * new_position = const_cast<char *>(current->data.data());\n+    BufferBase::set(new_position, current->data.size(), 0);\n+    allowed = false;\n+\n+    current_file = current->file_name;\n+    current_offset = current->offset;\n+\n+    ++current;\n+\n+    return true;\n+}\n+\n+}\ndiff --git a/src/Storages/FileLog/ReadBufferFromFileLog.h b/src/Storages/FileLog/ReadBufferFromFileLog.h\nnew file mode 100644\nindex 000000000000..117a858de3b1\n--- /dev/null\n+++ b/src/Storages/FileLog/ReadBufferFromFileLog.h\n@@ -0,0 +1,93 @@\n+#pragma once\n+\n+#include <Core/BackgroundSchedulePool.h>\n+#include <Core/Names.h>\n+#include <IO/ReadBuffer.h>\n+#include <Storages/FileLog/StorageFileLog.h>\n+#include <base/types.h>\n+\n+#include <fstream>\n+#include <mutex>\n+\n+namespace Poco\n+{\n+    class Logger;\n+}\n+\n+namespace DB\n+{\n+class ReadBufferFromFileLog : public ReadBuffer\n+{\n+public:\n+    ReadBufferFromFileLog(\n+        StorageFileLog & storage_,\n+        size_t max_batch_size,\n+        size_t poll_timeout_,\n+        ContextPtr context_,\n+        size_t stream_number_,\n+        size_t max_streams_number_);\n+\n+    ~ReadBufferFromFileLog() override = default;\n+\n+    auto pollTimeout() const { return poll_timeout; }\n+\n+    bool hasMorePolledRecords() const { return current != records.end(); }\n+\n+    bool poll();\n+\n+    bool noRecords() { return buffer_status == BufferStatus::NO_RECORD_RETURNED; }\n+\n+    auto getFileName() const { return current_file; }\n+    auto getOffset() const { return current_offset; }\n+\n+private:\n+    enum class BufferStatus\n+    {\n+        INIT,\n+        NO_RECORD_RETURNED,\n+        POLLED_OK,\n+    };\n+\n+    BufferStatus buffer_status = BufferStatus::INIT;\n+\n+    Poco::Logger * log;\n+\n+    StorageFileLog & storage;\n+\n+    bool stream_out = false;\n+\n+    size_t batch_size;\n+    size_t poll_timeout;\n+\n+    ContextPtr context;\n+\n+    size_t stream_number;\n+    size_t max_streams_number;\n+\n+    bool allowed = true;\n+\n+    using RecordData = std::string;\n+    struct Record\n+    {\n+        RecordData data;\n+        std::string file_name;\n+        /// Offset is the start of a row, which is needed for virtual columns.\n+        UInt64 offset;\n+    };\n+    using Records = std::vector<Record>;\n+\n+    Records records;\n+    Records::const_iterator current;\n+\n+    String current_file;\n+    UInt64 current_offset = 0;\n+\n+    using TaskThread = BackgroundSchedulePool::TaskHolder;\n+\n+    Records pollBatch(size_t batch_size_);\n+\n+    void readNewRecords(Records & new_records, size_t batch_size_);\n+\n+    bool nextImpl() override;\n+};\n+}\ndiff --git a/src/Storages/FileLog/StorageFileLog.cpp b/src/Storages/FileLog/StorageFileLog.cpp\nnew file mode 100644\nindex 000000000000..463ad77b1426\n--- /dev/null\n+++ b/src/Storages/FileLog/StorageFileLog.cpp\n@@ -0,0 +1,976 @@\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Disks/StoragePolicy.h>\n+#include <IO/ReadBufferFromFile.h>\n+#include <IO/ReadHelpers.h>\n+#include <IO/WriteBufferFromFile.h>\n+#include <IO/WriteIntText.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/InterpreterInsertQuery.h>\n+#include <Interpreters/evaluateConstantExpression.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Parsers/ASTExpressionList.h>\n+#include <Parsers/ASTInsertQuery.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Processors/Executors/CompletedPipelineExecutor.h>\n+#include <QueryPipeline/Pipe.h>\n+#include <Storages/FileLog/FileLogSource.h>\n+#include <Storages/FileLog/ReadBufferFromFileLog.h>\n+#include <Storages/FileLog/StorageFileLog.h>\n+#include <Storages/StorageFactory.h>\n+#include <Storages/StorageMaterializedView.h>\n+#include <base/logger_useful.h>\n+#include <Common/Exception.h>\n+#include <Common/Macros.h>\n+#include <Common/filesystemHelpers.h>\n+#include <Common/getNumberOfPhysicalCPUCores.h>\n+#include <Common/quoteString.h>\n+#include <Common/typeid_cast.h>\n+\n+#include <sys/stat.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n+    extern const int BAD_ARGUMENTS;\n+    extern const int CANNOT_STAT;\n+    extern const int BAD_FILE_TYPE;\n+    extern const int CANNOT_READ_ALL_DATA;\n+    extern const int LOGICAL_ERROR;\n+    extern const int TABLE_METADATA_ALREADY_EXISTS;\n+    extern const int CANNOT_SELECT;\n+    extern const int QUERY_NOT_ALLOWED;\n+}\n+\n+namespace\n+{\n+    const auto MAX_THREAD_WORK_DURATION_MS = 60000;\n+}\n+\n+StorageFileLog::StorageFileLog(\n+    const StorageID & table_id_,\n+    ContextPtr context_,\n+    const ColumnsDescription & columns_,\n+    const String & path_,\n+    const String & format_name_,\n+    std::unique_ptr<FileLogSettings> settings,\n+    const String & comment,\n+    bool attach)\n+    : IStorage(table_id_)\n+    , WithContext(context_->getGlobalContext())\n+    , filelog_settings(std::move(settings))\n+    , path(path_)\n+    , format_name(format_name_)\n+    , log(&Poco::Logger::get(\"StorageFileLog (\" + table_id_.table_name + \")\"))\n+    , milliseconds_to_wait(filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds())\n+{\n+    StorageInMemoryMetadata storage_metadata;\n+    storage_metadata.setColumns(columns_);\n+    storage_metadata.setComment(comment);\n+    setInMemoryMetadata(storage_metadata);\n+\n+    try\n+    {\n+        loadMetaFiles(attach);\n+        loadFiles();\n+\n+        assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());\n+        assert(file_infos.file_names.size() == file_infos.context_by_name.size());\n+\n+        if (path_is_directory)\n+            directory_watch = std::make_unique<FileLogDirectoryWatcher>(root_data_path, *this, getContext());\n+\n+        auto thread = getContext()->getSchedulePool().createTask(log->name(), [this] { threadFunc(); });\n+        task = std::make_shared<TaskContext>(std::move(thread));\n+    }\n+    catch (...)\n+    {\n+        if (!attach)\n+            throw;\n+        tryLogCurrentException(__PRETTY_FUNCTION__);\n+    }\n+}\n+\n+void StorageFileLog::loadMetaFiles(bool attach)\n+{\n+    const auto & storage = getStorageID();\n+    root_meta_path\n+        = std::filesystem::path(getContext()->getPath()) / \".filelog_storage_metadata\" / storage.getDatabaseName() / storage.getTableName();\n+\n+    /// Attach table\n+    if (attach)\n+    {\n+        /// Meta file may lost, log and create directory\n+        if (!std::filesystem::exists(root_meta_path))\n+        {\n+            /// Create root_meta_path directory when store meta data\n+            LOG_ERROR(log, \"Metadata files of table {} are lost.\", getStorageID().getTableName());\n+        }\n+        /// Load all meta info to file_infos;\n+        deserialize();\n+    }\n+    /// Create table, just create meta data directory\n+    else\n+    {\n+        if (std::filesystem::exists(root_meta_path))\n+        {\n+            throw Exception(\n+                ErrorCodes::TABLE_METADATA_ALREADY_EXISTS,\n+                \"Metadata files already exist by path: {}, remove them manually if it is intended\",\n+                root_meta_path);\n+        }\n+        std::filesystem::create_directories(root_meta_path);\n+    }\n+}\n+\n+void StorageFileLog::loadFiles()\n+{\n+    if (!fileOrSymlinkPathStartsWith(path, getContext()->getUserFilesPath()))\n+    {\n+        throw Exception(\n+            ErrorCodes::BAD_ARGUMENTS, \"The absolute data path should be inside `user_files_path`({})\", getContext()->getUserFilesPath());\n+    }\n+\n+    auto absolute_path = std::filesystem::absolute(path);\n+    absolute_path = absolute_path.lexically_normal(); /// Normalize path.\n+\n+    if (std::filesystem::is_regular_file(absolute_path))\n+    {\n+        path_is_directory = false;\n+        root_data_path = absolute_path.parent_path();\n+\n+        file_infos.file_names.push_back(absolute_path.filename());\n+    }\n+    else if (std::filesystem::is_directory(absolute_path))\n+    {\n+        root_data_path = absolute_path;\n+        /// Just consider file with depth 1\n+        for (const auto & dir_entry : std::filesystem::directory_iterator{absolute_path})\n+        {\n+            if (dir_entry.is_regular_file())\n+            {\n+                file_infos.file_names.push_back(dir_entry.path().filename());\n+            }\n+        }\n+    }\n+    else\n+    {\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"The path {} neither a regular file, nor a directory\", absolute_path.c_str());\n+    }\n+\n+    /// Get files inode\n+    for (const auto & file : file_infos.file_names)\n+    {\n+        auto inode = getInode(getFullDataPath(file));\n+        file_infos.context_by_name.emplace(file, FileContext{.inode = inode});\n+    }\n+\n+    /// Update file meta or create file meta\n+    for (const auto & [file, ctx] : file_infos.context_by_name)\n+    {\n+        if (auto it = file_infos.meta_by_inode.find(ctx.inode); it != file_infos.meta_by_inode.end())\n+        {\n+            /// data file have been renamed, need update meta file's name\n+            if (it->second.file_name != file)\n+            {\n+                std::filesystem::rename(getFullMetaPath(it->second.file_name), getFullMetaPath(file));\n+                it->second.file_name = file;\n+            }\n+        }\n+        /// New file\n+        else\n+        {\n+            FileMeta meta{file, 0, 0};\n+            file_infos.meta_by_inode.emplace(ctx.inode, meta);\n+        }\n+    }\n+\n+    /// Clear unneeded meta file, because data files may be deleted\n+    if (file_infos.meta_by_inode.size() > file_infos.context_by_name.size())\n+    {\n+        InodeToFileMeta valid_metas;\n+        valid_metas.reserve(file_infos.context_by_name.size());\n+        for (const auto & [inode, meta] : file_infos.meta_by_inode)\n+        {\n+            /// Note, here we need to use inode to judge does the meta file is valid.\n+            /// In the case that when a file deleted, then we create new file with the\n+            /// same name, it will have different inode number with stored meta file,\n+            /// so the stored meta file is invalid\n+            if (auto it = file_infos.context_by_name.find(meta.file_name);\n+                it != file_infos.context_by_name.end() && it->second.inode == inode)\n+                valid_metas.emplace(inode, meta);\n+            /// Delete meta file from filesystem\n+            else\n+                std::filesystem::remove(getFullMetaPath(meta.file_name));\n+        }\n+        file_infos.meta_by_inode.swap(valid_metas);\n+    }\n+}\n+\n+void StorageFileLog::serialize() const\n+{\n+    if (!std::filesystem::exists(root_meta_path))\n+    {\n+        std::filesystem::create_directories(root_meta_path);\n+    }\n+    for (const auto & [inode, meta] : file_infos.meta_by_inode)\n+    {\n+        auto full_name = getFullMetaPath(meta.file_name);\n+        if (!std::filesystem::exists(full_name))\n+        {\n+            FS::createFile(full_name);\n+        }\n+        else\n+        {\n+            checkOffsetIsValid(full_name, meta.last_writen_position);\n+        }\n+        WriteBufferFromFile out(full_name);\n+        writeIntText(inode, out);\n+        writeChar('\\n', out);\n+        writeIntText(meta.last_writen_position, out);\n+    }\n+}\n+\n+void StorageFileLog::serialize(UInt64 inode, const FileMeta & file_meta) const\n+{\n+    if (!std::filesystem::exists(root_meta_path))\n+    {\n+        std::filesystem::create_directories(root_meta_path);\n+    }\n+    auto full_name = getFullMetaPath(file_meta.file_name);\n+    if (!std::filesystem::exists(full_name))\n+    {\n+        FS::createFile(full_name);\n+    }\n+    else\n+    {\n+        checkOffsetIsValid(full_name, file_meta.last_writen_position);\n+    }\n+    WriteBufferFromFile out(full_name);\n+    writeIntText(inode, out);\n+    writeChar('\\n', out);\n+    writeIntText(file_meta.last_writen_position, out);\n+}\n+\n+void StorageFileLog::deserialize()\n+{\n+    if (!std::filesystem::exists(root_meta_path))\n+        return;\n+    /// In case of single file (not a watched directory),\n+    /// iterated directory always has one file inside.\n+    for (const auto & dir_entry : std::filesystem::directory_iterator{root_meta_path})\n+    {\n+        if (!dir_entry.is_regular_file())\n+        {\n+            throw Exception(\n+                ErrorCodes::BAD_FILE_TYPE,\n+                \"The file {} under {} is not a regular file when deserializing meta files\",\n+                dir_entry.path().c_str(),\n+                root_meta_path);\n+        }\n+\n+        ReadBufferFromFile in(dir_entry.path().c_str());\n+        FileMeta meta;\n+        UInt64 inode, last_written_pos;\n+\n+        if (!tryReadIntText(inode, in))\n+        {\n+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", dir_entry.path().c_str());\n+        }\n+        assertChar('\\n', in);\n+        if (!tryReadIntText(last_written_pos, in))\n+        {\n+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", dir_entry.path().c_str());\n+        }\n+\n+        meta.file_name = dir_entry.path().filename();\n+        meta.last_writen_position = last_written_pos;\n+\n+        file_infos.meta_by_inode.emplace(inode, meta);\n+    }\n+}\n+\n+UInt64 StorageFileLog::getInode(const String & file_name)\n+{\n+    struct stat file_stat;\n+    if (stat(file_name.c_str(), &file_stat))\n+    {\n+        throw Exception(ErrorCodes::CANNOT_STAT, \"Can not get stat info of file {}\", file_name);\n+    }\n+    return file_stat.st_ino;\n+}\n+\n+Pipe StorageFileLog::read(\n+    const Names & column_names,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    SelectQueryInfo & /* query_info */,\n+    ContextPtr local_context,\n+    QueryProcessingStage::Enum /* processed_stage */,\n+    size_t /* max_block_size */,\n+    unsigned /* num_streams */)\n+{\n+    /// If there are MVs depended on this table, we just forbid reading\n+    if (has_dependent_mv)\n+    {\n+        throw Exception(\n+            ErrorCodes::QUERY_NOT_ALLOWED,\n+            \"Can not make `SELECT` query from table {}, because it has attached dependencies. Remove dependent materialized views if \"\n+            \"needed\",\n+            getStorageID().getTableName());\n+    }\n+\n+    std::lock_guard<std::mutex> lock(file_infos_mutex);\n+    if (running_streams)\n+    {\n+        throw Exception(\"Another select query is running on this table, need to wait it finish.\", ErrorCodes::CANNOT_SELECT);\n+    }\n+\n+    updateFileInfos();\n+\n+    /// No files to parse\n+    if (file_infos.file_names.empty())\n+    {\n+        LOG_WARNING(log, \"There is a idle table named {}, no files need to parse.\", getName());\n+        return Pipe{};\n+    }\n+\n+    auto modified_context = Context::createCopy(local_context);\n+\n+    auto max_streams_number = std::min<UInt64>(filelog_settings->max_threads, file_infos.file_names.size());\n+\n+    /// Each stream responsible for closing it's files and store meta\n+    openFilesAndSetPos();\n+\n+    Pipes pipes;\n+    pipes.reserve(max_streams_number);\n+    for (size_t stream_number = 0; stream_number < max_streams_number; ++stream_number)\n+    {\n+        pipes.emplace_back(std::make_shared<FileLogSource>(\n+            *this,\n+            metadata_snapshot,\n+            modified_context,\n+            column_names,\n+            getMaxBlockSize(),\n+            getPollTimeoutMillisecond(),\n+            stream_number,\n+            max_streams_number));\n+    }\n+\n+    return Pipe::unitePipes(std::move(pipes));\n+}\n+\n+void StorageFileLog::increaseStreams()\n+{\n+    running_streams += 1;\n+}\n+\n+void StorageFileLog::reduceStreams()\n+{\n+    running_streams -= 1;\n+}\n+\n+void StorageFileLog::drop()\n+{\n+    try\n+    {\n+        if (std::filesystem::exists(root_meta_path))\n+            std::filesystem::remove_all(root_meta_path);\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(__PRETTY_FUNCTION__);\n+    }\n+}\n+\n+void StorageFileLog::startup()\n+{\n+    try\n+    {\n+        if (task)\n+        {\n+            task->holder->activateAndSchedule();\n+        }\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(__PRETTY_FUNCTION__);\n+    }\n+}\n+\n+void StorageFileLog::shutdown()\n+{\n+    try\n+    {\n+        if (task)\n+        {\n+            task->stream_cancelled = true;\n+\n+            /// Reader thread may wait for wake up\n+            wakeUp();\n+\n+            LOG_TRACE(log, \"Waiting for cleanup\");\n+            task->holder->deactivate();\n+        }\n+        /// If no reading call and threadFunc, the log files will never\n+        /// be opened, also just leave the work of close files and\n+        /// store meta to streams. because if we close files in here,\n+        /// may result in data race with unfinishing reading pipeline\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(__PRETTY_FUNCTION__);\n+        task->holder->deactivate();\n+    }\n+}\n+\n+void StorageFileLog::assertStreamGood(const std::ifstream & reader)\n+{\n+    if (!reader.good())\n+    {\n+        throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Stream is in bad state\");\n+    }\n+}\n+\n+void StorageFileLog::openFilesAndSetPos()\n+{\n+    for (const auto & file : file_infos.file_names)\n+    {\n+        auto & file_ctx = findInMap(file_infos.context_by_name, file);\n+        if (file_ctx.status != FileStatus::NO_CHANGE)\n+        {\n+            file_ctx.reader.emplace(getFullDataPath(file));\n+            auto & reader = file_ctx.reader.value();\n+            assertStreamGood(reader);\n+\n+            reader.seekg(0, reader.end);\n+            assertStreamGood(reader);\n+\n+            auto file_end = reader.tellg();\n+            assertStreamGood(reader);\n+\n+            auto & meta = findInMap(file_infos.meta_by_inode, file_ctx.inode);\n+            if (meta.last_writen_position > static_cast<UInt64>(file_end))\n+            {\n+                throw Exception(\n+                    ErrorCodes::CANNOT_READ_ALL_DATA,\n+                    \"Last saved offsset for File {} is bigger than file size ({} > {})\",\n+                    file,\n+                    meta.last_writen_position,\n+                    file_end);\n+            }\n+            /// update file end at the moment, used in ReadBuffer and serialize\n+            meta.last_open_end = file_end;\n+\n+            reader.seekg(meta.last_writen_position);\n+            assertStreamGood(reader);\n+        }\n+    }\n+    serialize();\n+}\n+\n+void StorageFileLog::closeFilesAndStoreMeta(size_t start, size_t end)\n+{\n+    assert(start >= 0);\n+    assert(start < end);\n+    assert(end <= file_infos.file_names.size());\n+\n+    for (size_t i = start; i < end; ++i)\n+    {\n+        auto & file_ctx = findInMap(file_infos.context_by_name, file_infos.file_names[i]);\n+\n+        if (file_ctx.reader)\n+        {\n+            if (file_ctx.reader->is_open())\n+                file_ctx.reader->close();\n+        }\n+\n+        auto & meta = findInMap(file_infos.meta_by_inode, file_ctx.inode);\n+        serialize(file_ctx.inode, meta);\n+    }\n+}\n+\n+void StorageFileLog::storeMetas(size_t start, size_t end)\n+{\n+    assert(start >= 0);\n+    assert(start < end);\n+    assert(end <= file_infos.file_names.size());\n+\n+    for (size_t i = start; i < end; ++i)\n+    {\n+        auto & file_ctx = findInMap(file_infos.context_by_name, file_infos.file_names[i]);\n+\n+        auto & meta = findInMap(file_infos.meta_by_inode, file_ctx.inode);\n+        serialize(file_ctx.inode, meta);\n+    }\n+}\n+\n+void StorageFileLog::checkOffsetIsValid(const String & full_name, UInt64 offset)\n+{\n+    ReadBufferFromFile in(full_name);\n+    UInt64 _, last_written_pos;\n+\n+    if (!tryReadIntText(_, in))\n+    {\n+        throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", full_name);\n+    }\n+    assertChar('\\n', in);\n+    if (!tryReadIntText(last_written_pos, in))\n+    {\n+        throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", full_name);\n+    }\n+    if (last_written_pos > offset)\n+        throw Exception(\n+            ErrorCodes::LOGICAL_ERROR, \"Last stored last_written_pos in meta file {} is bigger than current last_written_pos\", full_name);\n+}\n+\n+size_t StorageFileLog::getMaxBlockSize() const\n+{\n+    return filelog_settings->max_block_size.changed ? filelog_settings->max_block_size.value\n+                                                    : getContext()->getSettingsRef().max_insert_block_size.value;\n+}\n+\n+size_t StorageFileLog::getPollMaxBatchSize() const\n+{\n+    size_t batch_size = filelog_settings->poll_max_batch_size.changed ? filelog_settings->poll_max_batch_size.value\n+                                                                      : getContext()->getSettingsRef().max_block_size.value;\n+    return std::min(batch_size, getMaxBlockSize());\n+}\n+\n+size_t StorageFileLog::getPollTimeoutMillisecond() const\n+{\n+    return filelog_settings->poll_timeout_ms.changed ? filelog_settings->poll_timeout_ms.totalMilliseconds()\n+                                                     : getContext()->getSettingsRef().stream_poll_timeout_ms.totalMilliseconds();\n+}\n+\n+bool StorageFileLog::checkDependencies(const StorageID & table_id)\n+{\n+    // Check if all dependencies are attached\n+    auto dependencies = DatabaseCatalog::instance().getDependencies(table_id);\n+    if (dependencies.empty())\n+        return true;\n+\n+    for (const auto & storage : dependencies)\n+    {\n+        auto table = DatabaseCatalog::instance().tryGetTable(storage, getContext());\n+        if (!table)\n+            return false;\n+\n+        // If it materialized view, check it's target table\n+        auto * materialized_view = dynamic_cast<StorageMaterializedView *>(table.get());\n+        if (materialized_view && !materialized_view->tryGetTargetTable())\n+            return false;\n+\n+        // Check all its dependencies\n+        if (!checkDependencies(storage))\n+            return false;\n+    }\n+\n+    return true;\n+}\n+\n+size_t StorageFileLog::getTableDependentCount() const\n+{\n+    auto table_id = getStorageID();\n+    // Check if at least one direct dependency is attached\n+    return DatabaseCatalog::instance().getDependencies(table_id).size();\n+}\n+\n+void StorageFileLog::threadFunc()\n+{\n+    bool reschedule = false;\n+    try\n+    {\n+        auto table_id = getStorageID();\n+\n+        auto dependencies_count = getTableDependentCount();\n+\n+        if (dependencies_count)\n+        {\n+            has_dependent_mv = true;\n+            auto start_time = std::chrono::steady_clock::now();\n+\n+            // Keep streaming as long as there are attached views and streaming is not cancelled\n+            while (!task->stream_cancelled)\n+            {\n+                if (!checkDependencies(table_id))\n+                {\n+                    /// For this case, we can not wait for watch thread to wake up\n+                    reschedule = true;\n+                    break;\n+                }\n+\n+                LOG_DEBUG(log, \"Started streaming to {} attached views\", dependencies_count);\n+\n+                if (streamToViews())\n+                {\n+                    LOG_TRACE(log, \"Stream stalled. Reschedule.\");\n+                    if (milliseconds_to_wait\n+                        < static_cast<uint64_t>(filelog_settings->poll_directory_watch_events_backoff_max.totalMilliseconds()))\n+                        milliseconds_to_wait *= filelog_settings->poll_directory_watch_events_backoff_factor.value;\n+                    break;\n+                }\n+                else\n+                {\n+                    milliseconds_to_wait = filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds();\n+                }\n+\n+                auto ts = std::chrono::steady_clock::now();\n+                auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(ts-start_time);\n+                if (duration.count() > MAX_THREAD_WORK_DURATION_MS)\n+                {\n+                    LOG_TRACE(log, \"Thread work duration limit exceeded. Reschedule.\");\n+                    reschedule = true;\n+                    break;\n+                }\n+            }\n+        }\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(__PRETTY_FUNCTION__);\n+    }\n+\n+    // Wait for attached views\n+    if (!task->stream_cancelled)\n+    {\n+        if (path_is_directory)\n+        {\n+            if (!getTableDependentCount() || reschedule)\n+                task->holder->scheduleAfter(milliseconds_to_wait);\n+            else\n+            {\n+                std::unique_lock<std::mutex> lock(mutex);\n+                /// Waiting for watch directory thread to wake up\n+                cv.wait(lock, [this] { return has_new_events; });\n+                has_new_events = false;\n+\n+                if (task->stream_cancelled)\n+                    return;\n+                task->holder->schedule();\n+            }\n+        }\n+        else\n+            task->holder->scheduleAfter(milliseconds_to_wait);\n+    }\n+}\n+\n+bool StorageFileLog::streamToViews()\n+{\n+    std::lock_guard<std::mutex> lock(file_infos_mutex);\n+    if (running_streams)\n+    {\n+        LOG_INFO(log, \"Another select query is running on this table, need to wait it finish.\");\n+        return true;\n+    }\n+\n+    Stopwatch watch;\n+\n+    auto table_id = getStorageID();\n+    auto table = DatabaseCatalog::instance().getTable(table_id, getContext());\n+    if (!table)\n+        throw Exception(\"Engine table \" + table_id.getNameForLogs() + \" doesn't exist\", ErrorCodes::LOGICAL_ERROR);\n+    auto metadata_snapshot = getInMemoryMetadataPtr();\n+\n+    auto max_streams_number = std::min<UInt64>(filelog_settings->max_threads.value, file_infos.file_names.size());\n+    /// No files to parse\n+    if (max_streams_number == 0)\n+    {\n+        LOG_INFO(log, \"There is a idle table named {}, no files need to parse.\", getName());\n+        return updateFileInfos();\n+    }\n+\n+    // Create an INSERT query for streaming data\n+    auto insert = std::make_shared<ASTInsertQuery>();\n+    insert->table_id = table_id;\n+\n+    auto new_context = Context::createCopy(getContext());\n+\n+    InterpreterInsertQuery interpreter(insert, new_context, false, true, true);\n+    auto block_io = interpreter.execute();\n+\n+    /// Each stream responsible for closing it's files and store meta\n+    openFilesAndSetPos();\n+\n+    Pipes pipes;\n+    pipes.reserve(max_streams_number);\n+    for (size_t stream_number = 0; stream_number < max_streams_number; ++stream_number)\n+    {\n+        pipes.emplace_back(std::make_shared<FileLogSource>(\n+            *this,\n+            metadata_snapshot,\n+            new_context,\n+            block_io.pipeline.getHeader().getNames(),\n+            getPollMaxBatchSize(),\n+            getPollTimeoutMillisecond(),\n+            stream_number,\n+            max_streams_number));\n+    }\n+\n+    auto input= Pipe::unitePipes(std::move(pipes));\n+\n+    assertBlocksHaveEqualStructure(input.getHeader(), block_io.pipeline.getHeader(), \"StorageFileLog streamToViews\");\n+\n+    size_t rows = 0;\n+    {\n+        block_io.pipeline.complete(std::move(input));\n+        block_io.pipeline.setProgressCallback([&](const Progress & progress) { rows += progress.read_rows.load(); });\n+        CompletedPipelineExecutor executor(block_io.pipeline);\n+        executor.execute();\n+    }\n+\n+    UInt64 milliseconds = watch.elapsedMilliseconds();\n+    LOG_DEBUG(log, \"Pushing {} rows to {} took {} ms.\", rows, table_id.getNameForLogs(), milliseconds);\n+\n+    return updateFileInfos();\n+}\n+\n+void StorageFileLog::wakeUp()\n+{\n+    std::unique_lock<std::mutex> lock(mutex);\n+    has_new_events = true;\n+    lock.unlock();\n+    cv.notify_one();\n+}\n+\n+void registerStorageFileLog(StorageFactory & factory)\n+{\n+    auto creator_fn = [](const StorageFactory::Arguments & args)\n+    {\n+        ASTs & engine_args = args.engine_args;\n+        size_t args_count = engine_args.size();\n+\n+        bool has_settings = args.storage_def->settings;\n+\n+        auto filelog_settings = std::make_unique<FileLogSettings>();\n+        if (has_settings)\n+        {\n+            filelog_settings->loadFromQuery(*args.storage_def);\n+        }\n+\n+        auto physical_cpu_cores = getNumberOfPhysicalCPUCores();\n+        auto num_threads = filelog_settings->max_threads.value;\n+\n+        if (num_threads > physical_cpu_cores)\n+        {\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Number of threads to parse files can not be bigger than {}\", physical_cpu_cores);\n+        }\n+        else if (num_threads < 1)\n+        {\n+            throw Exception(\"Number of threads to parse files can not be lower than 1\", ErrorCodes::BAD_ARGUMENTS);\n+        }\n+\n+        if (filelog_settings->max_block_size.changed && filelog_settings->max_block_size.value < 1)\n+        {\n+            throw Exception(\"filelog_max_block_size can not be lower than 1\", ErrorCodes::BAD_ARGUMENTS);\n+        }\n+\n+        if (filelog_settings->poll_max_batch_size.changed && filelog_settings->poll_max_batch_size.value < 1)\n+        {\n+            throw Exception(\"filelog_poll_max_batch_size can not be lower than 1\", ErrorCodes::BAD_ARGUMENTS);\n+        }\n+\n+        size_t init_sleep_time = filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds();\n+        size_t max_sleep_time = filelog_settings->poll_directory_watch_events_backoff_max.totalMilliseconds();\n+        if (init_sleep_time > max_sleep_time)\n+        {\n+            throw Exception(\n+                \"poll_directory_watch_events_backoff_init can not be greater than poll_directory_watch_events_backoff_max\",\n+                ErrorCodes::BAD_ARGUMENTS);\n+        }\n+\n+        if (filelog_settings->poll_directory_watch_events_backoff_factor.changed\n+            && !filelog_settings->poll_directory_watch_events_backoff_factor.value)\n+            throw Exception(\"poll_directory_watch_events_backoff_factor can not be 0\", ErrorCodes::BAD_ARGUMENTS);\n+\n+        if (args_count != 2)\n+            throw Exception(\n+                \"Arguments size of StorageFileLog should be 2, path and format name\", ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);\n+\n+        auto path_ast = evaluateConstantExpressionAsLiteral(engine_args[0], args.getContext());\n+        auto format_ast = evaluateConstantExpressionAsLiteral(engine_args[1], args.getContext());\n+\n+        auto path = path_ast->as<ASTLiteral &>().value.safeGet<String>();\n+        auto format = format_ast->as<ASTLiteral &>().value.safeGet<String>();\n+\n+        return StorageFileLog::create(\n+            args.table_id,\n+            args.getContext(),\n+            args.columns,\n+            path,\n+            format,\n+            std::move(filelog_settings),\n+            args.comment,\n+            args.attach);\n+    };\n+\n+    factory.registerStorage(\n+        \"FileLog\",\n+        creator_fn,\n+        StorageFactory::StorageFeatures{\n+            .supports_settings = true,\n+        });\n+}\n+\n+bool StorageFileLog::updateFileInfos()\n+{\n+    if (!directory_watch)\n+    {\n+        /// For table just watch one file, we can not use directory monitor to watch it\n+        if (!path_is_directory)\n+        {\n+            assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());\n+            assert(file_infos.file_names.size() == file_infos.context_by_name.size());\n+            assert(file_infos.file_names.size() == 1);\n+\n+            if (auto it = file_infos.context_by_name.find(file_infos.file_names[0]); it != file_infos.context_by_name.end())\n+            {\n+                it->second.status = FileStatus::UPDATED;\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+    /// Do not need to hold file_status lock, since it will be holded\n+    /// by caller when call this function\n+    auto error = directory_watch->getErrorAndReset();\n+    if (error.has_error)\n+        LOG_ERROR(log, \"Error happened during watching directory {}: {}\", directory_watch->getPath(), error.error_msg);\n+\n+    /// These file infos should always have same size(one for one) before update and after update\n+    assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());\n+    assert(file_infos.file_names.size() == file_infos.context_by_name.size());\n+\n+    auto events = directory_watch->getEventsAndReset();\n+\n+    for (const auto & [file_name, event_infos] : events)\n+    {\n+        String file_path = getFullDataPath(file_name);\n+        for (const auto & event_info : event_infos.file_events)\n+        {\n+            switch (event_info.type)\n+            {\n+                case DirectoryWatcherBase::DW_ITEM_ADDED:\n+                {\n+                    LOG_TRACE(log, \"New event {} watched, file_name: {}\", event_info.callback, file_name);\n+                    /// Check if it is a regular file, and new file may be renamed or removed\n+                    if (std::filesystem::is_regular_file(file_path))\n+                    {\n+                        auto inode = getInode(file_path);\n+\n+                        file_infos.file_names.push_back(file_name);\n+\n+                        if (auto it = file_infos.meta_by_inode.find(inode); it != file_infos.meta_by_inode.end())\n+                            it->second = FileMeta{.file_name = file_name};\n+                        else\n+                            file_infos.meta_by_inode.emplace(inode, FileMeta{.file_name = file_name});\n+\n+                        if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())\n+                            it->second = FileContext{.status = FileStatus::OPEN, .inode = inode};\n+                        else\n+                            file_infos.context_by_name.emplace(file_name, FileContext{.inode = inode});\n+                    }\n+                    break;\n+                }\n+\n+                case DirectoryWatcherBase::DW_ITEM_MODIFIED:\n+                {\n+                    LOG_TRACE(log, \"New event {} watched, file_name: {}\", event_info.callback, file_name);\n+                    /// When new file added and appended, it has two event: DW_ITEM_ADDED\n+                    /// and DW_ITEM_MODIFIED, since the order of these two events in the\n+                    /// sequence is uncentain, so we may can not find it in file_infos, just\n+                    /// skip it, the file info will be handled in DW_ITEM_ADDED case.\n+                    if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())\n+                        it->second.status = FileStatus::UPDATED;\n+                    break;\n+                }\n+\n+                case DirectoryWatcherBase::DW_ITEM_REMOVED:\n+                case DirectoryWatcherBase::DW_ITEM_MOVED_FROM:\n+                {\n+                    LOG_TRACE(log, \"New event {} watched, file_name: {}\", event_info.callback, file_name);\n+                    if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())\n+                        it->second.status = FileStatus::REMOVED;\n+                    break;\n+                }\n+                case DirectoryWatcherBase::DW_ITEM_MOVED_TO:\n+                {\n+                    LOG_TRACE(log, \"New event {} watched, file_name: {}\", event_info.callback, file_name);\n+\n+                    /// Similar to DW_ITEM_ADDED, but if it removed from an old file\n+                    /// should obtain old meta file and rename meta file\n+                    if (std::filesystem::is_regular_file(file_path))\n+                    {\n+                        file_infos.file_names.push_back(file_name);\n+                        auto inode = getInode(file_path);\n+\n+                        if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())\n+                            it->second = FileContext{.inode = inode};\n+                        else\n+                            file_infos.context_by_name.emplace(file_name, FileContext{.inode = inode});\n+\n+                        /// File has been renamed, we should also rename meta file\n+                        if (auto it = file_infos.meta_by_inode.find(inode); it != file_infos.meta_by_inode.end())\n+                        {\n+                            auto old_name = it->second.file_name;\n+                            it->second.file_name = file_name;\n+                            if (std::filesystem::exists(getFullMetaPath(old_name)))\n+                                std::filesystem::rename(getFullMetaPath(old_name), getFullMetaPath(file_name));\n+                        }\n+                        /// May move from other place, adding new meta info\n+                        else\n+                            file_infos.meta_by_inode.emplace(inode, FileMeta{.file_name = file_name});\n+                    }\n+                }\n+            }\n+        }\n+    }\n+    std::vector<String> valid_files;\n+\n+    /// Remove file infos with REMOVE status\n+    for (const auto & file_name : file_infos.file_names)\n+    {\n+        if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())\n+        {\n+            if (it->second.status == FileStatus::REMOVED)\n+            {\n+                /// We need to check that this inode does not hold by other file(mv),\n+                /// otherwise, we can not destroy it.\n+                auto inode = it->second.inode;\n+                /// If it's now hold by other file, than the file_name should has\n+                /// been changed during updating file_infos\n+                if (auto meta = file_infos.meta_by_inode.find(inode);\n+                    meta != file_infos.meta_by_inode.end() && meta->second.file_name == file_name)\n+                    file_infos.meta_by_inode.erase(meta);\n+\n+                if (std::filesystem::exists(getFullMetaPath(file_name)))\n+                    std::filesystem::remove(getFullMetaPath(file_name));\n+                file_infos.context_by_name.erase(it);\n+            }\n+            else\n+            {\n+                valid_files.push_back(file_name);\n+            }\n+        }\n+    }\n+    file_infos.file_names.swap(valid_files);\n+\n+    /// These file infos should always have same size(one for one)\n+    assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());\n+    assert(file_infos.file_names.size() == file_infos.context_by_name.size());\n+\n+    return events.empty() || file_infos.file_names.empty();\n+}\n+\n+NamesAndTypesList StorageFileLog::getVirtuals() const\n+{\n+    return NamesAndTypesList{{\"_filename\", std::make_shared<DataTypeString>()}, {\"_offset\", std::make_shared<DataTypeUInt64>()}};\n+}\n+\n+Names StorageFileLog::getVirtualColumnNames()\n+{\n+    return {\"_filename\", \"_offset\"};\n+}\n+}\ndiff --git a/src/Storages/FileLog/StorageFileLog.h b/src/Storages/FileLog/StorageFileLog.h\nnew file mode 100644\nindex 000000000000..e1d95ae736fe\n--- /dev/null\n+++ b/src/Storages/FileLog/StorageFileLog.h\n@@ -0,0 +1,215 @@\n+#pragma once\n+\n+#include <Storages/FileLog/Buffer_fwd.h>\n+#include <Storages/FileLog/FileLogDirectoryWatcher.h>\n+#include <Storages/FileLog/FileLogSettings.h>\n+\n+#include <Core/BackgroundSchedulePool.h>\n+#include <Storages/IStorage.h>\n+#include <Common/SettingsChanges.h>\n+\n+#include <base/shared_ptr_helper.h>\n+\n+#include <atomic>\n+#include <condition_variable>\n+#include <filesystem>\n+#include <fstream>\n+#include <mutex>\n+#include <optional>\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+class FileLogDirectoryWatcher;\n+\n+class StorageFileLog final : public shared_ptr_helper<StorageFileLog>, public IStorage, WithContext\n+{\n+    friend struct shared_ptr_helper<StorageFileLog>;\n+\n+public:\n+\n+    using Files = std::vector<String>;\n+\n+    std::string getName() const override { return \"FileLog\"; }\n+\n+    bool noPushingToViews() const override { return true; }\n+\n+    void startup() override;\n+    void shutdown() override;\n+\n+    Pipe read(\n+        const Names & column_names,\n+        const StorageMetadataPtr & /*metadata_snapshot*/,\n+        SelectQueryInfo & query_info,\n+        ContextPtr context,\n+        QueryProcessingStage::Enum processed_stage,\n+        size_t max_block_size,\n+        unsigned num_streams) override;\n+\n+    void drop() override;\n+\n+    /// We need to call drop() immediately to remove meta data directory,\n+    /// otherwise, if another filelog table with same name created before\n+    /// the table be dropped finally, then its meta data directory will\n+    /// be deleted by this table drop finally\n+    bool dropTableImmediately() override { return true; }\n+\n+    const auto & getFormatName() const { return format_name; }\n+\n+    enum class FileStatus\n+    {\n+        OPEN, /// first time open file after table start up\n+        NO_CHANGE,\n+        UPDATED,\n+        REMOVED,\n+    };\n+\n+    struct FileContext\n+    {\n+        FileStatus status = FileStatus::OPEN;\n+        UInt64 inode{};\n+        std::optional<std::ifstream> reader = std::nullopt;\n+    };\n+\n+    struct FileMeta\n+    {\n+        String file_name;\n+        UInt64 last_writen_position = 0;\n+        UInt64 last_open_end = 0;\n+    };\n+\n+    using InodeToFileMeta = std::unordered_map<UInt64, FileMeta>;\n+    using FileNameToContext = std::unordered_map<String, FileContext>;\n+\n+    struct FileInfos\n+    {\n+        InodeToFileMeta meta_by_inode;\n+        FileNameToContext context_by_name;\n+        /// file names without path\n+        Names file_names;\n+    };\n+\n+    auto & getFileInfos() { return file_infos; }\n+\n+    String getFullMetaPath(const String & file_name) const { return std::filesystem::path(root_meta_path) / file_name; }\n+    String getFullDataPath(const String & file_name) const { return std::filesystem::path(root_data_path) / file_name; }\n+\n+    NamesAndTypesList getVirtuals() const override;\n+\n+    static Names getVirtualColumnNames();\n+\n+    static UInt64 getInode(const String & file_name);\n+\n+    void openFilesAndSetPos();\n+\n+    /// Used in FileLogSource when finish generating all blocks.\n+    /// Each stream responsible for close its files and store meta.\n+    void closeFilesAndStoreMeta(size_t start, size_t end);\n+\n+    /// Used in FileLogSource after generating every block\n+    void storeMetas(size_t start, size_t end);\n+\n+    static void assertStreamGood(const std::ifstream & reader);\n+\n+    template <typename K, typename V>\n+    static V & findInMap(std::unordered_map<K, V> & map, const K & key)\n+    {\n+        if (auto it = map.find(key); it != map.end())\n+            return it->second;\n+        else\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"The key {} doesn't exist.\", key);\n+    }\n+\n+    void increaseStreams();\n+    void reduceStreams();\n+\n+    void wakeUp();\n+\n+    const auto & getFileLogSettings() const { return filelog_settings; }\n+\n+protected:\n+    StorageFileLog(\n+        const StorageID & table_id_,\n+        ContextPtr context_,\n+        const ColumnsDescription & columns_,\n+        const String & path_,\n+        const String & format_name_,\n+        std::unique_ptr<FileLogSettings> settings,\n+        const String & comment,\n+        bool attach);\n+\n+private:\n+    std::unique_ptr<FileLogSettings> filelog_settings;\n+\n+    const String path;\n+    bool path_is_directory = true;\n+\n+    /// If path argument of the table is a regular file, it equals to user_files_path\n+    /// otherwise, it equals to user_files_path/ + path_argument/, e.g. path\n+    String root_data_path;\n+    String root_meta_path;\n+\n+    FileInfos file_infos;\n+\n+    const String format_name;\n+    Poco::Logger * log;\n+\n+    uint64_t milliseconds_to_wait;\n+\n+    /// In order to avoid data race, using a naive trick to forbid execute two select\n+    /// simultaneously, although read is not useful in this engine. Using an atomic\n+    /// variable to records current unfinishing streams, then if have unfinishing streams,\n+    /// later select should forbid to execute.\n+    std::atomic<int> running_streams = 0;\n+\n+    std::mutex mutex;\n+    bool has_new_events = false;\n+    std::condition_variable cv;\n+\n+    bool has_dependent_mv = false;\n+\n+    std::mutex file_infos_mutex;\n+\n+    struct TaskContext\n+    {\n+        BackgroundSchedulePool::TaskHolder holder;\n+        std::atomic<bool> stream_cancelled {false};\n+        explicit TaskContext(BackgroundSchedulePool::TaskHolder&& task_) : holder(std::move(task_))\n+        {\n+        }\n+    };\n+    std::shared_ptr<TaskContext> task;\n+\n+    std::unique_ptr<FileLogDirectoryWatcher> directory_watch = nullptr;\n+\n+    void loadFiles();\n+\n+    void loadMetaFiles(bool attach);\n+\n+    void threadFunc();\n+\n+    size_t getPollMaxBatchSize() const;\n+    size_t getMaxBlockSize() const;\n+    size_t getPollTimeoutMillisecond() const;\n+\n+    bool streamToViews();\n+    bool checkDependencies(const StorageID & table_id);\n+\n+    bool updateFileInfos();\n+\n+    size_t getTableDependentCount() const;\n+\n+    /// Used in shutdown()\n+    void serialize() const;\n+    /// Used in FileSource closeFileAndStoreMeta(file_name);\n+    void serialize(UInt64 inode, const FileMeta & file_meta) const;\n+\n+    void deserialize();\n+    static void checkOffsetIsValid(const String & full_name, UInt64 offset);\n+};\n+\n+}\ndiff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h\nindex 74e17442fe8e..701cf1275217 100644\n--- a/src/Storages/IStorage.h\n+++ b/src/Storages/IStorage.h\n@@ -585,6 +585,10 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo\n     /// Does not takes underlying Storage (if any) into account.\n     virtual std::optional<UInt64> lifetimeBytes() const { return {}; }\n \n+    /// Should table->drop be called at once or with delay (in case of atomic database engine).\n+    /// Needed for integration engines, when there must be no delay for calling drop() method.\n+    virtual bool dropTableImmediately() { return false; }\n+\n private:\n     /// Lock required for alter queries (lockForAlter). Always taken for write\n     /// (actually can be replaced with std::mutex, but for consistency we use\ndiff --git a/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in b/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in\nindex 6bb97355151d..e087475695c1 100644\n--- a/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in\n+++ b/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in\n@@ -50,6 +50,7 @@ const char * auto_config_build[]\n     \"USE_LDAP\", \"@USE_LDAP@\",\n     \"TZDATA_VERSION\", \"@TZDATA_VERSION@\",\n     \"USE_KRB5\", \"@USE_KRB5@\",\n+    \"USE_FILELOG\", \"@USE_FILELOG@\",\n     \"USE_BZIP2\", \"@USE_BZIP2@\",\n \n     nullptr, nullptr\ndiff --git a/src/Storages/registerStorages.cpp b/src/Storages/registerStorages.cpp\nindex af2e47328032..9f6c18f53d3e 100644\n--- a/src/Storages/registerStorages.cpp\n+++ b/src/Storages/registerStorages.cpp\n@@ -68,6 +68,10 @@ void registerStorageMaterializedPostgreSQL(StorageFactory & factory);\n void registerStorageExternalDistributed(StorageFactory & factory);\n #endif\n \n+#if USE_FILELOG\n+void registerStorageFileLog(StorageFactory & factory);\n+#endif\n+\n #if USE_SQLITE\n void registerStorageSQLite(StorageFactory & factory);\n #endif\n@@ -119,7 +123,11 @@ void registerStorages()\n     registerStorageKafka(factory);\n     #endif\n \n-    #if USE_AMQPCPP\n+#if USE_FILELOG\n+    registerStorageFileLog(factory);\n+#endif\n+\n+#if USE_AMQPCPP\n     registerStorageRabbitMQ(factory);\n     #endif\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01645_system_table_engines.sql b/tests/queries/0_stateless/01645_system_table_engines.sql\nindex 5e8eef5508b8..c2048427670b 100644\n--- a/tests/queries/0_stateless/01645_system_table_engines.sql\n+++ b/tests/queries/0_stateless/01645_system_table_engines.sql\n@@ -1,1 +1,1 @@\n-SELECT * FROM system.table_engines WHERE name in ('MergeTree', 'ReplicatedCollapsingMergeTree') FORMAT PrettyCompactNoEscapes;\n+SELECT * FROM system.table_engines WHERE name in ('MergeTree', 'ReplicatedCollapsingMergeTree') ORDER BY name FORMAT PrettyCompactNoEscapes;\ndiff --git a/tests/queries/0_stateless/02022_storage_filelog_one_file.reference b/tests/queries/0_stateless/02022_storage_filelog_one_file.reference\nnew file mode 100644\nindex 000000000000..88332de037f2\n--- /dev/null\n+++ b/tests/queries/0_stateless/02022_storage_filelog_one_file.reference\n@@ -0,0 +1,41 @@\n+1\t1\n+2\t2\n+3\t3\n+4\t4\n+5\t5\n+6\t6\n+7\t7\n+8\t8\n+9\t9\n+10\t10\n+11\t11\n+12\t12\n+13\t13\n+14\t14\n+15\t15\n+16\t16\n+17\t17\n+18\t18\n+19\t19\n+20\t20\n+100\t100\n+101\t101\n+102\t102\n+103\t103\n+104\t104\n+105\t105\n+106\t106\n+107\t107\n+108\t108\n+109\t109\n+110\t110\n+111\t111\n+112\t112\n+113\t113\n+114\t114\n+115\t115\n+116\t116\n+117\t117\n+118\t118\n+119\t119\n+120\t120\ndiff --git a/tests/queries/0_stateless/02022_storage_filelog_one_file.sh b/tests/queries/0_stateless/02022_storage_filelog_one_file.sh\nnew file mode 100755\nindex 000000000000..600e537a352d\n--- /dev/null\n+++ b/tests/queries/0_stateless/02022_storage_filelog_one_file.sh\n@@ -0,0 +1,42 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel\n+\n+set -eu\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# Data preparation.\n+# Now we can get the user_files_path by use the table file function for trick. also we can get it by query as:\n+#  \"insert into function file('exist.txt', 'CSV', 'val1 char') values ('aaaa'); select _path from file('exist.txt', 'CSV', 'val1 char')\"\n+user_files_path=$(clickhouse-client --query \"select _path,_file from file('nonexist.txt', 'CSV', 'val1 char')\" 2>&1 | grep Exception | awk '{gsub(\"/nonexist.txt\",\"\",$9); print $9}')\n+\n+for i in {1..20}\n+do\n+\techo $i, $i >> ${user_files_path}/a.txt\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt8, v UInt8) engine=FileLog('${user_files_path}/a.txt', 'CSV');\"\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+for i in {100..120}\n+do\n+\techo $i, $i >> ${user_files_path}/a.txt\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+# touch does not change file content, no event\n+touch ${user_files_path}/a.txt\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n+\n+# should no records return\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+rm -rf ${user_files_path}/a.txt\ndiff --git a/tests/queries/0_stateless/02023_storage_filelog.reference b/tests/queries/0_stateless/02023_storage_filelog.reference\nnew file mode 100644\nindex 000000000000..c787d2047db1\n--- /dev/null\n+++ b/tests/queries/0_stateless/02023_storage_filelog.reference\n@@ -0,0 +1,287 @@\n+1\t1\n+2\t2\n+3\t3\n+4\t4\n+5\t5\n+6\t6\n+7\t7\n+8\t8\n+9\t9\n+10\t10\n+11\t11\n+12\t12\n+13\t13\n+14\t14\n+15\t15\n+16\t16\n+17\t17\n+18\t18\n+19\t19\n+20\t20\n+1\t1\n+2\t2\n+3\t3\n+4\t4\n+5\t5\n+6\t6\n+7\t7\n+8\t8\n+9\t9\n+10\t10\n+11\t11\n+12\t12\n+13\t13\n+14\t14\n+15\t15\n+16\t16\n+17\t17\n+18\t18\n+19\t19\n+20\t20\n+1\t1\n+1\t1\n+2\t2\n+2\t2\n+3\t3\n+3\t3\n+4\t4\n+4\t4\n+5\t5\n+5\t5\n+6\t6\n+6\t6\n+7\t7\n+7\t7\n+8\t8\n+8\t8\n+9\t9\n+9\t9\n+10\t10\n+10\t10\n+11\t11\n+11\t11\n+12\t12\n+12\t12\n+13\t13\n+13\t13\n+14\t14\n+14\t14\n+15\t15\n+15\t15\n+16\t16\n+16\t16\n+17\t17\n+17\t17\n+18\t18\n+18\t18\n+19\t19\n+19\t19\n+20\t20\n+20\t20\n+100\t100\n+100\t100\n+100\t100\n+101\t101\n+101\t101\n+101\t101\n+102\t102\n+102\t102\n+102\t102\n+103\t103\n+103\t103\n+103\t103\n+104\t104\n+104\t104\n+104\t104\n+105\t105\n+105\t105\n+105\t105\n+106\t106\n+106\t106\n+106\t106\n+107\t107\n+107\t107\n+107\t107\n+108\t108\n+108\t108\n+108\t108\n+109\t109\n+109\t109\n+109\t109\n+110\t110\n+110\t110\n+110\t110\n+111\t111\n+111\t111\n+111\t111\n+112\t112\n+112\t112\n+112\t112\n+113\t113\n+113\t113\n+113\t113\n+114\t114\n+114\t114\n+114\t114\n+115\t115\n+115\t115\n+115\t115\n+116\t116\n+116\t116\n+116\t116\n+117\t117\n+117\t117\n+117\t117\n+118\t118\n+118\t118\n+118\t118\n+119\t119\n+119\t119\n+119\t119\n+120\t120\n+120\t120\n+120\t120\n+1\t1\n+2\t2\n+3\t3\n+4\t4\n+5\t5\n+6\t6\n+7\t7\n+8\t8\n+9\t9\n+10\t10\n+11\t11\n+12\t12\n+13\t13\n+14\t14\n+15\t15\n+16\t16\n+17\t17\n+18\t18\n+19\t19\n+20\t20\n+100\t100\n+101\t101\n+102\t102\n+103\t103\n+104\t104\n+105\t105\n+106\t106\n+107\t107\n+108\t108\n+109\t109\n+110\t110\n+111\t111\n+112\t112\n+113\t113\n+114\t114\n+115\t115\n+116\t116\n+117\t117\n+118\t118\n+119\t119\n+120\t120\n+150\t150\n+151\t151\n+152\t152\n+153\t153\n+154\t154\n+155\t155\n+156\t156\n+157\t157\n+158\t158\n+159\t159\n+160\t160\n+161\t161\n+162\t162\n+163\t163\n+164\t164\n+165\t165\n+166\t166\n+167\t167\n+168\t168\n+169\t169\n+170\t170\n+171\t171\n+172\t172\n+173\t173\n+174\t174\n+175\t175\n+176\t176\n+177\t177\n+178\t178\n+179\t179\n+180\t180\n+181\t181\n+182\t182\n+183\t183\n+184\t184\n+185\t185\n+186\t186\n+187\t187\n+188\t188\n+189\t189\n+190\t190\n+191\t191\n+192\t192\n+193\t193\n+194\t194\n+195\t195\n+196\t196\n+197\t197\n+198\t198\n+199\t199\n+200\t200\n+200\t200\n+201\t201\n+202\t202\n+203\t203\n+204\t204\n+205\t205\n+206\t206\n+207\t207\n+208\t208\n+209\t209\n+210\t210\n+211\t211\n+212\t212\n+213\t213\n+214\t214\n+215\t215\n+216\t216\n+217\t217\n+218\t218\n+219\t219\n+220\t220\n+221\t221\n+222\t222\n+223\t223\n+224\t224\n+225\t225\n+226\t226\n+227\t227\n+228\t228\n+229\t229\n+230\t230\n+231\t231\n+232\t232\n+233\t233\n+234\t234\n+235\t235\n+236\t236\n+237\t237\n+238\t238\n+239\t239\n+240\t240\n+241\t241\n+242\t242\n+243\t243\n+244\t244\n+245\t245\n+246\t246\n+247\t247\n+248\t248\n+249\t249\n+250\t250\n+OK\ndiff --git a/tests/queries/0_stateless/02023_storage_filelog.sh b/tests/queries/0_stateless/02023_storage_filelog.sh\nnew file mode 100755\nindex 000000000000..8279e09ff65a\n--- /dev/null\n+++ b/tests/queries/0_stateless/02023_storage_filelog.sh\n@@ -0,0 +1,79 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel\n+\n+set -eu\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# Data preparation.\n+# Now we can get the user_files_path by use the table file function for trick. also we can get it by query as:\n+#  \"insert into function file('exist.txt', 'CSV', 'val1 char') values ('aaaa'); select _path from file('exist.txt', 'CSV', 'val1 char')\"\n+user_files_path=$(clickhouse-client --query \"select _path,_file from file('nonexist.txt', 'CSV', 'val1 char')\" 2>&1 | grep Exception | awk '{gsub(\"/nonexist.txt\",\"\",$9); print $9}')\n+\n+mkdir -p ${user_files_path}/logs/\n+\n+rm -rf ${user_files_path}/logs/*\n+\n+for i in {1..20}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/a.txt\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt8, v UInt8) engine=FileLog('${user_files_path}/logs/', 'CSV');\"\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/b.txt\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+for i in {100..120}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/a.txt\n+done\n+\n+# touch does not change file content, no event\n+touch ${user_files_path}/logs/a.txt\n+\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/c.txt\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/d.txt\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/e.txt\n+mv ${user_files_path}/logs/b.txt ${user_files_path}/logs/j.txt\n+\n+rm ${user_files_path}/logs/d.txt\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n+cp ${user_files_path}/logs/e.txt ${user_files_path}/logs/f.txt\n+mv ${user_files_path}/logs/e.txt ${user_files_path}/logs/g.txt\n+mv ${user_files_path}/logs/c.txt ${user_files_path}/logs/h.txt\n+for i in {150..200}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/h.txt\n+done\n+for i in {200..250}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/i.txt\n+done\n+${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n+\n+# should no records return\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+\n+truncate ${user_files_path}/logs/a.txt --size 0\n+\n+# exception happend\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\" 2>&1 | grep -q \"Code: 33\" && echo 'OK' || echo 'FAIL'\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n+\n+rm -rf ${user_files_path}/logs\ndiff --git a/tests/queries/0_stateless/02024_storage_filelog_mv.reference b/tests/queries/0_stateless/02024_storage_filelog_mv.reference\nnew file mode 100644\nindex 000000000000..2ebb200a43a1\n--- /dev/null\n+++ b/tests/queries/0_stateless/02024_storage_filelog_mv.reference\n@@ -0,0 +1,121 @@\n+1\t1\n+2\t2\n+3\t3\n+4\t4\n+5\t5\n+6\t6\n+7\t7\n+8\t8\n+9\t9\n+10\t10\n+11\t11\n+12\t12\n+13\t13\n+14\t14\n+15\t15\n+16\t16\n+17\t17\n+18\t18\n+19\t19\n+20\t20\n+1\t1\n+1\t1\n+1\t1\n+1\t1\n+2\t2\n+2\t2\n+2\t2\n+2\t2\n+3\t3\n+3\t3\n+3\t3\n+3\t3\n+4\t4\n+4\t4\n+4\t4\n+4\t4\n+5\t5\n+5\t5\n+5\t5\n+5\t5\n+6\t6\n+6\t6\n+6\t6\n+6\t6\n+7\t7\n+7\t7\n+7\t7\n+7\t7\n+8\t8\n+8\t8\n+8\t8\n+8\t8\n+9\t9\n+9\t9\n+9\t9\n+9\t9\n+10\t10\n+10\t10\n+10\t10\n+10\t10\n+11\t11\n+11\t11\n+11\t11\n+11\t11\n+12\t12\n+12\t12\n+12\t12\n+12\t12\n+13\t13\n+13\t13\n+13\t13\n+13\t13\n+14\t14\n+14\t14\n+14\t14\n+14\t14\n+15\t15\n+15\t15\n+15\t15\n+15\t15\n+16\t16\n+16\t16\n+16\t16\n+16\t16\n+17\t17\n+17\t17\n+17\t17\n+17\t17\n+18\t18\n+18\t18\n+18\t18\n+18\t18\n+19\t19\n+19\t19\n+19\t19\n+19\t19\n+20\t20\n+20\t20\n+20\t20\n+20\t20\n+100\t100\n+101\t101\n+102\t102\n+103\t103\n+104\t104\n+105\t105\n+106\t106\n+107\t107\n+108\t108\n+109\t109\n+110\t110\n+111\t111\n+112\t112\n+113\t113\n+114\t114\n+115\t115\n+116\t116\n+117\t117\n+118\t118\n+119\t119\n+120\t120\ndiff --git a/tests/queries/0_stateless/02024_storage_filelog_mv.sh b/tests/queries/0_stateless/02024_storage_filelog_mv.sh\nnew file mode 100755\nindex 000000000000..9ac0c95eae09\n--- /dev/null\n+++ b/tests/queries/0_stateless/02024_storage_filelog_mv.sh\n@@ -0,0 +1,65 @@\n+#!/usr/bin/env bash\n+# Tags: long, no-parallel\n+\n+set -eu\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# Data preparation.\n+# Now we can get the user_files_path by use the table file function for trick. also we can get it by query as:\n+#  \"insert into function file('exist.txt', 'CSV', 'val1 char') values ('aaaa'); select _path from file('exist.txt', 'CSV', 'val1 char')\"\n+user_files_path=$(clickhouse-client --query \"select _path,_file from file('nonexist.txt', 'CSV', 'val1 char')\" 2>&1 | grep Exception | awk '{gsub(\"/nonexist.txt\",\"\",$9); print $9}')\n+\n+mkdir -p ${user_files_path}/logs/\n+rm -rf ${user_files_path}/logs/*\n+\n+for i in {1..20}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/a.txt\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt8, v UInt8) engine=FileLog('${user_files_path}/logs/', 'CSV');\"\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table if exists mv;\"\n+${CLICKHOUSE_CLIENT} --query \"create Materialized View mv engine=MergeTree order by k as select * from file_log;\"\n+\n+function count()\n+{\n+\tCOUNT=$(${CLICKHOUSE_CLIENT} --query \"select count() from mv;\")\n+\techo $COUNT\n+}\n+\n+while true; do\n+\t[[ $(count) == 20 ]] && break\n+\tsleep 1\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from mv order by k;\"\n+\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/b.txt\n+\n+# touch does not change file content, no event\n+touch ${user_files_path}/logs/a.txt\n+\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/c.txt\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/d.txt\n+\n+for i in {100..120}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/d.txt\n+done\n+\n+while true; do\n+\t[[ $(count) == 101 ]] && break\n+\tsleep 1\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"select * from mv order by k;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table mv;\"\n+${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n+\n+rm -rf ${user_files_path}/logs\ndiff --git a/tests/queries/0_stateless/02025_storage_filelog_virtual_col.reference b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.reference\nnew file mode 100644\nindex 000000000000..aaa25ebb4518\n--- /dev/null\n+++ b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.reference\n@@ -0,0 +1,144 @@\n+1\t1\ta.txt\t0\n+2\t2\ta.txt\t5\n+3\t3\ta.txt\t10\n+4\t4\ta.txt\t15\n+5\t5\ta.txt\t20\n+6\t6\ta.txt\t25\n+7\t7\ta.txt\t30\n+8\t8\ta.txt\t35\n+9\t9\ta.txt\t40\n+10\t10\ta.txt\t45\n+11\t11\ta.txt\t52\n+12\t12\ta.txt\t59\n+13\t13\ta.txt\t66\n+14\t14\ta.txt\t73\n+15\t15\ta.txt\t80\n+16\t16\ta.txt\t87\n+17\t17\ta.txt\t94\n+18\t18\ta.txt\t101\n+19\t19\ta.txt\t108\n+20\t20\ta.txt\t115\n+1\t1\tb.txt\t0\n+2\t2\tb.txt\t5\n+3\t3\tb.txt\t10\n+4\t4\tb.txt\t15\n+5\t5\tb.txt\t20\n+6\t6\tb.txt\t25\n+7\t7\tb.txt\t30\n+8\t8\tb.txt\t35\n+9\t9\tb.txt\t40\n+10\t10\tb.txt\t45\n+11\t11\tb.txt\t52\n+12\t12\tb.txt\t59\n+13\t13\tb.txt\t66\n+14\t14\tb.txt\t73\n+15\t15\tb.txt\t80\n+16\t16\tb.txt\t87\n+17\t17\tb.txt\t94\n+18\t18\tb.txt\t101\n+19\t19\tb.txt\t108\n+20\t20\tb.txt\t115\n+100\t100\ta.txt\t122\n+101\t101\ta.txt\t131\n+102\t102\ta.txt\t140\n+103\t103\ta.txt\t149\n+104\t104\ta.txt\t158\n+105\t105\ta.txt\t167\n+106\t106\ta.txt\t176\n+107\t107\ta.txt\t185\n+108\t108\ta.txt\t194\n+109\t109\ta.txt\t203\n+110\t110\ta.txt\t212\n+111\t111\ta.txt\t221\n+112\t112\ta.txt\t230\n+113\t113\ta.txt\t239\n+114\t114\ta.txt\t248\n+115\t115\ta.txt\t257\n+116\t116\ta.txt\t266\n+117\t117\ta.txt\t275\n+118\t118\ta.txt\t284\n+119\t119\ta.txt\t293\n+120\t120\ta.txt\t302\n+1\t1\tc.txt\t0\n+2\t2\tc.txt\t5\n+3\t3\tc.txt\t10\n+4\t4\tc.txt\t15\n+5\t5\tc.txt\t20\n+6\t6\tc.txt\t25\n+7\t7\tc.txt\t30\n+8\t8\tc.txt\t35\n+9\t9\tc.txt\t40\n+10\t10\tc.txt\t45\n+11\t11\tc.txt\t52\n+12\t12\tc.txt\t59\n+13\t13\tc.txt\t66\n+14\t14\tc.txt\t73\n+15\t15\tc.txt\t80\n+16\t16\tc.txt\t87\n+17\t17\tc.txt\t94\n+18\t18\tc.txt\t101\n+19\t19\tc.txt\t108\n+20\t20\tc.txt\t115\n+100\t100\tc.txt\t122\n+101\t101\tc.txt\t131\n+102\t102\tc.txt\t140\n+103\t103\tc.txt\t149\n+104\t104\tc.txt\t158\n+105\t105\tc.txt\t167\n+106\t106\tc.txt\t176\n+107\t107\tc.txt\t185\n+108\t108\tc.txt\t194\n+109\t109\tc.txt\t203\n+110\t110\tc.txt\t212\n+111\t111\tc.txt\t221\n+112\t112\tc.txt\t230\n+113\t113\tc.txt\t239\n+114\t114\tc.txt\t248\n+115\t115\tc.txt\t257\n+116\t116\tc.txt\t266\n+117\t117\tc.txt\t275\n+118\t118\tc.txt\t284\n+119\t119\tc.txt\t293\n+120\t120\tc.txt\t302\n+1\t1\te.txt\t0\n+2\t2\te.txt\t5\n+3\t3\te.txt\t10\n+4\t4\te.txt\t15\n+5\t5\te.txt\t20\n+6\t6\te.txt\t25\n+7\t7\te.txt\t30\n+8\t8\te.txt\t35\n+9\t9\te.txt\t40\n+10\t10\te.txt\t45\n+11\t11\te.txt\t52\n+12\t12\te.txt\t59\n+13\t13\te.txt\t66\n+14\t14\te.txt\t73\n+15\t15\te.txt\t80\n+16\t16\te.txt\t87\n+17\t17\te.txt\t94\n+18\t18\te.txt\t101\n+19\t19\te.txt\t108\n+20\t20\te.txt\t115\n+100\t100\te.txt\t122\n+101\t101\te.txt\t131\n+102\t102\te.txt\t140\n+103\t103\te.txt\t149\n+104\t104\te.txt\t158\n+105\t105\te.txt\t167\n+106\t106\te.txt\t176\n+107\t107\te.txt\t185\n+108\t108\te.txt\t194\n+109\t109\te.txt\t203\n+110\t110\te.txt\t212\n+111\t111\te.txt\t221\n+112\t112\te.txt\t230\n+113\t113\te.txt\t239\n+114\t114\te.txt\t248\n+115\t115\te.txt\t257\n+116\t116\te.txt\t266\n+117\t117\te.txt\t275\n+118\t118\te.txt\t284\n+119\t119\te.txt\t293\n+120\t120\te.txt\t302\n+OK\ndiff --git a/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh\nnew file mode 100755\nindex 000000000000..a92f93991efe\n--- /dev/null\n+++ b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh\n@@ -0,0 +1,62 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel\n+\n+set -eu\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# Data preparation.\n+# Now we can get the user_files_path by use the table file function for trick. also we can get it by query as:\n+#  \"insert into function file('exist.txt', 'CSV', 'val1 char') values ('aaaa'); select _path from file('exist.txt', 'CSV', 'val1 char')\"\n+user_files_path=$(clickhouse-client --query \"select _path,_file from file('nonexist.txt', 'CSV', 'val1 char')\" 2>&1 | grep Exception | awk '{gsub(\"/nonexist.txt\",\"\",$9); print $9}')\n+\n+mkdir -p ${user_files_path}/logs/\n+\n+rm -rf ${user_files_path}/logs/*\n+\n+for i in {1..20}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/a.txt\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt8, v UInt8) engine=FileLog('${user_files_path}/logs/', 'CSV');\"\n+\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/b.txt\n+\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+\n+for i in {100..120}\n+do\n+\techo $i, $i >> ${user_files_path}/logs/a.txt\n+done\n+\n+# touch does not change file content, no event\n+touch ${user_files_path}/logs/a.txt\n+\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/c.txt\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/d.txt\n+cp ${user_files_path}/logs/a.txt ${user_files_path}/logs/e.txt\n+\n+rm ${user_files_path}/logs/d.txt\n+\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n+\n+# should no records return\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+\n+truncate ${user_files_path}/logs/a.txt --size 0\n+\n+# exception happend\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\" 2>&1 | grep -q \"Code: 33\" && echo 'OK' || echo 'FAIL'\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n+\n+rm -rf ${user_files_path}/logs\ndiff --git a/tests/queries/0_stateless/02026_storage_filelog_largefile.reference b/tests/queries/0_stateless/02026_storage_filelog_largefile.reference\nnew file mode 100644\nindex 000000000000..95240890a95b\n--- /dev/null\n+++ b/tests/queries/0_stateless/02026_storage_filelog_largefile.reference\n@@ -0,0 +1,3 @@\n+2000000\n+2000000\n+2000000\ndiff --git a/tests/queries/0_stateless/02026_storage_filelog_largefile.sh b/tests/queries/0_stateless/02026_storage_filelog_largefile.sh\nnew file mode 100755\nindex 000000000000..a6c5d19287e3\n--- /dev/null\n+++ b/tests/queries/0_stateless/02026_storage_filelog_largefile.sh\n@@ -0,0 +1,47 @@\n+#!/usr/bin/env bash\n+# Tags: long, no-parallel\n+\n+set -eu\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# Data preparation.\n+# Now we can get the user_files_path by use the table file function for trick. also we can get it by query as:\n+#  \"insert into function file('exist.txt', 'CSV', 'val1 char') values ('aaaa'); select _path from file('exist.txt', 'CSV', 'val1 char')\"\n+user_files_path=$(clickhouse-client --query \"select _path,_file from file('nonexist.txt', 'CSV', 'val1 char')\" 2>&1 | grep Exception | awk '{gsub(\"/nonexist.txt\",\"\",$9); print $9}')\n+\n+mkdir -p ${user_files_path}/logs/\n+\n+rm -rf ${user_files_path}/logs/*\n+\n+chmod 777 ${user_files_path}/logs/\n+\n+for i in {1..200}\n+do\n+\t${CLICKHOUSE_CLIENT} --query \"insert into function file('${user_files_path}/logs/test$i.csv', 'CSV', 'k UInt32, v UInt32') select number, number from numbers(10000);\"\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n+${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt32, v UInt32) engine=FileLog('${user_files_path}/logs/', 'CSV');\"\n+\n+${CLICKHOUSE_CLIENT} --query \"select count() from file_log \"\n+\n+for i in {201..400}\n+do\n+\t${CLICKHOUSE_CLIENT} --query \"insert into function file('${user_files_path}/logs/test$i.csv', 'CSV', 'k UInt32, v UInt32') select number, number from numbers(10000);\"\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"select count() from file_log \"\n+\n+for i in {401..600}\n+do\n+\t${CLICKHOUSE_CLIENT} --query \"insert into function file('${user_files_path}/logs/test$i.csv', 'CSV', 'k UInt32, v UInt32') select number, number from numbers(10000);\"\n+done\n+\n+${CLICKHOUSE_CLIENT} --query \"select count() from file_log \"\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n+\n+rm -rf ${user_files_path}/logs\n",
  "problem_statement": "RFC. Allow to subscribe to File table.\n**Use case**\r\nSpecify a file or directory with logs. Create a materialized view that will look at the changes (appends) in these files, track offsets and parse them incrementally. \r\n\r\n- realtime logs import into ClickHouse;\r\n- realtime calculation of metrics from logs.\r\n\n",
  "hints_text": "Sounds interesting. \r\n\r\nActually log analysis a VERY popular usecase for ClickHouse.\r\nBUT: usually you have logs to analyze on one hosts (set of hosts), and clickhouse on another. \r\nBecause of that there are tools which do something like 'tail' on one host and pass the logs to remote server.\r\n* logstash + lot of '[beats](https://www.elastic.co/products/beats/)' (including filebeat)\r\n* rsyslog \r\n* syslog-ng\r\n* fluentd\r\n* logagent \r\netc.\r\n\r\nSo as for me more interesting alternative would be - to be able to parse and process \r\ndata coming to simple tcp / udp socket, or mimic logstash and reuse their beats (which are tiny and cool). \r\n\r\nLogstash is able to listen TCP / UDP, accept incoming data coming there (usually from beats), parse it according to predefined set of regexps, and put the data to Elastic (currenlty there is 3rd party output plugin for ClickHouse). Logstash is written on jruby (read: quite slow), but is very handy and a lot of people is using it as a part of ELK stack.\r\n\r\nIn that case it can be something like \r\n```\r\nCREATE TABLE foo Engine=UDP(8000, 'Template ...', 'time DateTime, url String, useragent String ... );\r\ncreate materialized view bar as select * from foo;  \r\n\r\n# and after that\r\ntail -f file.log >  >/dev/udp/localhost/8000\r\n# or \r\necho hello | nc -4u -w0 localhost 8000\r\n# ot use filebeat, or telegraph, or whataver\r\n```\nAssigned to @ucasFL \n> Sounds interesting.\r\n> \r\n> Actually log analysis a VERY popular usecase for ClickHouse.\r\n> BUT: usually you have logs to analyze on one hosts (set of hosts), and clickhouse on another.\r\n> Because of that there are tools which do something like 'tail' on one host and pass the logs to remote server.\r\n> \r\n> * logstash + lot of '[beats](https://www.elastic.co/products/beats/)' (including filebeat)\r\n> * rsyslog\r\n> * syslog-ng\r\n> * fluentd\r\n> * logagent\r\n>   etc.\r\n> \r\n> So as for me more interesting alternative would be - to be able to parse and process\r\n> data coming to simple tcp / udp socket, or mimic logstash and reuse their beats (which are tiny and cool).\r\n> \r\n> Logstash is able to listen TCP / UDP, accept incoming data coming there (usually from beats), parse it according to predefined set of regexps, and put the data to Elastic (currenlty there is 3rd party output plugin for ClickHouse). Logstash is written on jruby (read: quite slow), but is very handy and a lot of people is using it as a part of ELK stack.\r\n> \r\n> In that case it can be something like\r\n> \r\n> ```\r\n> CREATE TABLE foo Engine=UDP(8000, 'Template ...', 'time DateTime, url String, useragent String ... );\r\n> create materialized view bar as select * from foo;  \r\n> \r\n> # and after that\r\n> tail -f file.log >  >/dev/udp/localhost/8000\r\n> # or \r\n> echo hello | nc -4u -w0 localhost 8000\r\n> # ot use filebeat, or telegraph, or whataver\r\n> ```\r\n\r\nI used php scripts to make a log collector\uff0cIt has been running for some time and it is very stable\u3002 https://github.com/lizhichao/log2ck \r\nlike this \r\n```shell\r\ntail -F apapche/access.log | php apache_log.php -h tcp://127.0.0.1:9000 -u default -p 123456 -d logs -t apache_log\r\n```\r\n\r\n\r\n",
  "created_at": "2021-07-04T06:28:17Z",
  "modified_files": [
    "CMakeLists.txt",
    "b/cmake/find/filelog.cmake",
    "contrib/poco",
    "src/CMakeLists.txt",
    "src/Common/ErrorCodes.cpp",
    "src/Core/config_core.h.in",
    "src/Databases/DatabaseAtomic.cpp",
    "b/src/Storages/FileLog/Buffer_fwd.h",
    "b/src/Storages/FileLog/DirectoryWatcherBase.cpp",
    "b/src/Storages/FileLog/DirectoryWatcherBase.h",
    "b/src/Storages/FileLog/FileLogDirectoryWatcher.cpp",
    "b/src/Storages/FileLog/FileLogDirectoryWatcher.h",
    "b/src/Storages/FileLog/FileLogSettings.cpp",
    "b/src/Storages/FileLog/FileLogSettings.h",
    "b/src/Storages/FileLog/FileLogSource.cpp",
    "b/src/Storages/FileLog/FileLogSource.h",
    "b/src/Storages/FileLog/ReadBufferFromFileLog.cpp",
    "b/src/Storages/FileLog/ReadBufferFromFileLog.h",
    "b/src/Storages/FileLog/StorageFileLog.cpp",
    "b/src/Storages/FileLog/StorageFileLog.h",
    "src/Storages/IStorage.h",
    "src/Storages/System/StorageSystemBuildOptions.generated.cpp.in",
    "src/Storages/registerStorages.cpp"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/01645_system_table_engines.sql",
    "b/tests/queries/0_stateless/02022_storage_filelog_one_file.reference",
    "b/tests/queries/0_stateless/02022_storage_filelog_one_file.sh",
    "b/tests/queries/0_stateless/02023_storage_filelog.reference",
    "b/tests/queries/0_stateless/02023_storage_filelog.sh",
    "b/tests/queries/0_stateless/02024_storage_filelog_mv.reference",
    "b/tests/queries/0_stateless/02024_storage_filelog_mv.sh",
    "b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.reference",
    "b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh",
    "b/tests/queries/0_stateless/02026_storage_filelog_largefile.reference",
    "b/tests/queries/0_stateless/02026_storage_filelog_largefile.sh"
  ]
}