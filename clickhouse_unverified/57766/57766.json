{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 57766,
  "instance_id": "ClickHouse__ClickHouse-57766",
  "issue_numbers": [
    "57635"
  ],
  "base_commit": "82180db7a862101501d49169f433800e13cda2fb",
  "patch": "diff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 4f4e96a9be74..672454381568 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -2942,7 +2942,6 @@ void InterpreterSelectQuery::executeWindow(QueryPlan & query_plan)\n             auto sorting_step = std::make_unique<SortingStep>(\n                 query_plan.getCurrentDataStream(),\n                 window.full_sort_description,\n-                window.partition_by,\n                 0 /* LIMIT */,\n                 sort_settings,\n                 settings.optimize_sorting_by_input_stream_properties);\ndiff --git a/src/Planner/Planner.cpp b/src/Planner/Planner.cpp\nindex d6e0f42a06dd..12e8d7953477 100644\n--- a/src/Planner/Planner.cpp\n+++ b/src/Planner/Planner.cpp\n@@ -915,7 +915,6 @@ void addWindowSteps(QueryPlan & query_plan,\n             auto sorting_step = std::make_unique<SortingStep>(\n                 query_plan.getCurrentDataStream(),\n                 window_description.full_sort_description,\n-                window_description.partition_by,\n                 0 /*limit*/,\n                 sort_settings,\n                 settings.optimize_sorting_by_input_stream_properties);\ndiff --git a/src/Processors/QueryPlan/SortingStep.cpp b/src/Processors/QueryPlan/SortingStep.cpp\nindex 641b9036d4c5..55ce763575ec 100644\n--- a/src/Processors/QueryPlan/SortingStep.cpp\n+++ b/src/Processors/QueryPlan/SortingStep.cpp\n@@ -1,4 +1,3 @@\n-#include <memory>\n #include <stdexcept>\n #include <IO/Operators.h>\n #include <Processors/Merges/MergingSortedTransform.h>\n@@ -10,8 +9,6 @@\n #include <QueryPipeline/QueryPipelineBuilder.h>\n #include <Common/JSONBuilder.h>\n \n-#include <Processors/ResizeProcessor.h>\n-#include <Processors/Transforms/ScatterByPartitionTransform.h>\n \n namespace CurrentMetrics\n {\n@@ -79,21 +76,6 @@ SortingStep::SortingStep(\n     output_stream->sort_scope = DataStream::SortScope::Global;\n }\n \n-SortingStep::SortingStep(\n-        const DataStream & input_stream,\n-        const SortDescription & description_,\n-        const SortDescription & partition_by_description_,\n-        UInt64 limit_,\n-        const Settings & settings_,\n-        bool optimize_sorting_by_input_stream_properties_)\n-    : SortingStep(input_stream, description_, limit_, settings_, optimize_sorting_by_input_stream_properties_)\n-{\n-    partition_by_description = partition_by_description_;\n-\n-    output_stream->sort_description = result_description;\n-    output_stream->sort_scope = DataStream::SortScope::Stream;\n-}\n-\n SortingStep::SortingStep(\n     const DataStream & input_stream_,\n     SortDescription prefix_description_,\n@@ -135,11 +117,7 @@ void SortingStep::updateOutputStream()\n {\n     output_stream = createOutputStream(input_streams.front(), input_streams.front().header, getDataStreamTraits());\n     output_stream->sort_description = result_description;\n-\n-    if (partition_by_description.empty())\n-        output_stream->sort_scope = DataStream::SortScope::Global;\n-    else\n-        output_stream->sort_scope = DataStream::SortScope::Stream;\n+    output_stream->sort_scope = DataStream::SortScope::Global;\n }\n \n void SortingStep::updateLimit(size_t limit_)\n@@ -157,55 +135,6 @@ void SortingStep::convertToFinishSorting(SortDescription prefix_description_)\n     prefix_description = std::move(prefix_description_);\n }\n \n-void SortingStep::scatterByPartitionIfNeeded(QueryPipelineBuilder& pipeline)\n-{\n-    size_t threads = pipeline.getNumThreads();\n-    size_t streams = pipeline.getNumStreams();\n-\n-    if (!partition_by_description.empty() && threads > 1)\n-    {\n-        Block stream_header = pipeline.getHeader();\n-\n-        ColumnNumbers key_columns;\n-        key_columns.reserve(partition_by_description.size());\n-        for (auto & col : partition_by_description)\n-        {\n-            key_columns.push_back(stream_header.getPositionByName(col.column_name));\n-        }\n-\n-        pipeline.transform([&](OutputPortRawPtrs ports)\n-        {\n-            Processors processors;\n-            for (auto * port : ports)\n-            {\n-                auto scatter = std::make_shared<ScatterByPartitionTransform>(stream_header, threads, key_columns);\n-                connect(*port, scatter->getInputs().front());\n-                processors.push_back(scatter);\n-            }\n-            return processors;\n-        });\n-\n-        if (streams > 1)\n-        {\n-            pipeline.transform([&](OutputPortRawPtrs ports)\n-            {\n-                Processors processors;\n-                for (size_t i = 0; i < threads; ++i)\n-                {\n-                    size_t output_it = i;\n-                    auto resize = std::make_shared<ResizeProcessor>(stream_header, streams, 1);\n-                    auto & inputs = resize->getInputs();\n-\n-                    for (auto input_it = inputs.begin(); input_it != inputs.end(); output_it += threads, ++input_it)\n-                        connect(*ports[output_it], *input_it);\n-                    processors.push_back(resize);\n-                }\n-                return processors;\n-            });\n-        }\n-    }\n-}\n-\n void SortingStep::finishSorting(\n     QueryPipelineBuilder & pipeline, const SortDescription & input_sort_desc, const SortDescription & result_sort_desc, const UInt64 limit_)\n {\n@@ -331,12 +260,10 @@ void SortingStep::fullSortStreams(\n void SortingStep::fullSort(\n     QueryPipelineBuilder & pipeline, const SortDescription & result_sort_desc, const UInt64 limit_, const bool skip_partial_sort)\n {\n-    scatterByPartitionIfNeeded(pipeline);\n-\n     fullSortStreams(pipeline, sort_settings, result_sort_desc, limit_, skip_partial_sort);\n \n     /// If there are several streams, then we merge them into one\n-    if (pipeline.getNumStreams() > 1 && (partition_by_description.empty() || pipeline.getNumThreads() == 1))\n+    if (pipeline.getNumStreams() > 1)\n     {\n         auto transform = std::make_shared<MergingSortedTransform>(\n             pipeline.getHeader(),\n@@ -368,7 +295,6 @@ void SortingStep::transformPipeline(QueryPipelineBuilder & pipeline, const Build\n     {\n         bool need_finish_sorting = (prefix_description.size() < result_description.size());\n         mergingSorted(pipeline, prefix_description, (need_finish_sorting ? 0 : limit));\n-\n         if (need_finish_sorting)\n         {\n             finishSorting(pipeline, prefix_description, result_description, limit);\ndiff --git a/src/Processors/QueryPlan/SortingStep.h b/src/Processors/QueryPlan/SortingStep.h\nindex 52f48f66a32b..371a24ac6f2d 100644\n--- a/src/Processors/QueryPlan/SortingStep.h\n+++ b/src/Processors/QueryPlan/SortingStep.h\n@@ -40,15 +40,6 @@ class SortingStep : public ITransformingStep\n         const Settings & settings_,\n         bool optimize_sorting_by_input_stream_properties_);\n \n-    /// Full with partitioning\n-    SortingStep(\n-        const DataStream & input_stream,\n-        const SortDescription & description_,\n-        const SortDescription & partition_by_description_,\n-        UInt64 limit_,\n-        const Settings & settings_,\n-        bool optimize_sorting_by_input_stream_properties_);\n-\n     /// FinishSorting\n     SortingStep(\n         const DataStream & input_stream_,\n@@ -92,24 +83,14 @@ class SortingStep : public ITransformingStep\n         bool skip_partial_sort = false);\n \n private:\n-    void scatterByPartitionIfNeeded(QueryPipelineBuilder& pipeline);\n     void updateOutputStream() override;\n \n-    static void mergeSorting(\n-        QueryPipelineBuilder & pipeline,\n-        const Settings & sort_settings,\n-        const SortDescription & result_sort_desc,\n-        UInt64 limit_);\n+    static void\n+    mergeSorting(QueryPipelineBuilder & pipeline, const Settings & sort_settings, const SortDescription & result_sort_desc, UInt64 limit_);\n \n-    void mergingSorted(\n-        QueryPipelineBuilder & pipeline,\n-        const SortDescription & result_sort_desc,\n-        UInt64 limit_);\n+    void mergingSorted(QueryPipelineBuilder & pipeline, const SortDescription & result_sort_desc, UInt64 limit_);\n     void finishSorting(\n-        QueryPipelineBuilder & pipeline,\n-        const SortDescription & input_sort_desc,\n-        const SortDescription & result_sort_desc,\n-        UInt64 limit_);\n+        QueryPipelineBuilder & pipeline, const SortDescription & input_sort_desc, const SortDescription & result_sort_desc, UInt64 limit_);\n     void fullSort(\n         QueryPipelineBuilder & pipeline,\n         const SortDescription & result_sort_desc,\n@@ -120,9 +101,6 @@ class SortingStep : public ITransformingStep\n \n     SortDescription prefix_description;\n     const SortDescription result_description;\n-\n-    SortDescription partition_by_description;\n-\n     UInt64 limit;\n     bool always_read_till_end = false;\n \ndiff --git a/src/Processors/QueryPlan/WindowStep.cpp b/src/Processors/QueryPlan/WindowStep.cpp\nindex bb4f429d6268..9c68a4b73d1f 100644\n--- a/src/Processors/QueryPlan/WindowStep.cpp\n+++ b/src/Processors/QueryPlan/WindowStep.cpp\n@@ -67,8 +67,7 @@ void WindowStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQ\n     // This resize is needed for cases such as `over ()` when we don't have a\n     // sort node, and the input might have multiple streams. The sort node would\n     // have resized it.\n-    if (window_description.full_sort_description.empty())\n-        pipeline.resize(1);\n+    pipeline.resize(1);\n \n     pipeline.addSimpleTransform(\n         [&](const Block & /*header*/)\ndiff --git a/src/Processors/Transforms/ScatterByPartitionTransform.cpp b/src/Processors/Transforms/ScatterByPartitionTransform.cpp\ndeleted file mode 100644\nindex 6e3cdc0fda11..000000000000\n--- a/src/Processors/Transforms/ScatterByPartitionTransform.cpp\n+++ /dev/null\n@@ -1,129 +0,0 @@\n-#include <Processors/Transforms/ScatterByPartitionTransform.h>\n-\n-#include <Common/PODArray.h>\n-#include <Core/ColumnNumbers.h>\n-\n-namespace DB\n-{\n-ScatterByPartitionTransform::ScatterByPartitionTransform(Block header, size_t output_size_, ColumnNumbers key_columns_)\n-    : IProcessor(InputPorts{header}, OutputPorts{output_size_, header})\n-    , output_size(output_size_)\n-    , key_columns(std::move(key_columns_))\n-    , hash(0)\n-{}\n-\n-IProcessor::Status ScatterByPartitionTransform::prepare()\n-{\n-    auto & input = getInputs().front();\n-\n-    /// Check all outputs are finished or ready to get data.\n-\n-    bool all_finished = true;\n-    for (auto & output : outputs)\n-    {\n-        if (output.isFinished())\n-            continue;\n-\n-        all_finished = false;\n-    }\n-\n-    if (all_finished)\n-    {\n-        input.close();\n-        return Status::Finished;\n-    }\n-\n-    if (!all_outputs_processed)\n-    {\n-        auto output_it = outputs.begin();\n-        bool can_push = false;\n-        for (size_t i = 0; i < output_size; ++i, ++output_it)\n-            if (!was_output_processed[i] && output_it->canPush())\n-                can_push = true;\n-        if (!can_push)\n-            return Status::PortFull;\n-        return Status::Ready;\n-    }\n-    /// Try get chunk from input.\n-\n-    if (input.isFinished())\n-    {\n-        for (auto & output : outputs)\n-            output.finish();\n-\n-        return Status::Finished;\n-    }\n-\n-    input.setNeeded();\n-    if (!input.hasData())\n-        return Status::NeedData;\n-\n-    chunk = input.pull();\n-    has_data = true;\n-    was_output_processed.assign(outputs.size(), false);\n-\n-    return Status::Ready;\n-}\n-\n-void ScatterByPartitionTransform::work()\n-{\n-    if (all_outputs_processed)\n-        generateOutputChunks();\n-    all_outputs_processed = true;\n-\n-    size_t chunk_number = 0;\n-    for (auto & output : outputs)\n-    {\n-        auto & was_processed = was_output_processed[chunk_number];\n-        auto & output_chunk = output_chunks[chunk_number];\n-        ++chunk_number;\n-\n-        if (was_processed)\n-            continue;\n-\n-        if (output.isFinished())\n-            continue;\n-\n-        if (!output.canPush())\n-        {\n-            all_outputs_processed = false;\n-            continue;\n-        }\n-\n-        output.push(std::move(output_chunk));\n-        was_processed = true;\n-    }\n-\n-    if (all_outputs_processed)\n-    {\n-        has_data = false;\n-        output_chunks.clear();\n-    }\n-}\n-\n-void ScatterByPartitionTransform::generateOutputChunks()\n-{\n-    auto num_rows = chunk.getNumRows();\n-    const auto & columns = chunk.getColumns();\n-\n-    hash.reset(num_rows);\n-\n-    for (const auto & column_number : key_columns)\n-        columns[column_number]->updateWeakHash32(hash);\n-\n-    const auto & hash_data = hash.getData();\n-    IColumn::Selector selector(num_rows);\n-\n-    for (size_t row = 0; row < num_rows; ++row)\n-        selector[row] = hash_data[row] % output_size;\n-\n-    output_chunks.resize(output_size);\n-    for (const auto & column : columns)\n-    {\n-        auto filtered_columns = column->scatter(output_size, selector);\n-        for (size_t i = 0; i < output_size; ++i)\n-            output_chunks[i].addColumn(std::move(filtered_columns[i]));\n-    }\n-}\n-\n-}\ndiff --git a/src/Processors/Transforms/ScatterByPartitionTransform.h b/src/Processors/Transforms/ScatterByPartitionTransform.h\ndeleted file mode 100644\nindex 327f6dd62b45..000000000000\n--- a/src/Processors/Transforms/ScatterByPartitionTransform.h\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-#pragma once\n-#include <Common/WeakHash.h>\n-#include <Core/ColumnNumbers.h>\n-#include <Processors/IProcessor.h>\n-\n-namespace DB\n-{\n-\n-struct ScatterByPartitionTransform : IProcessor\n-{\n-    ScatterByPartitionTransform(Block header, size_t output_size_, ColumnNumbers key_columns_);\n-\n-    String getName() const override { return \"ScatterByPartitionTransform\"; }\n-\n-    Status prepare() override;\n-    void work() override;\n-\n-private:\n-\n-    void generateOutputChunks();\n-\n-    size_t output_size;\n-    ColumnNumbers key_columns;\n-\n-    bool has_data = false;\n-    bool all_outputs_processed = true;\n-    std::vector<char> was_output_processed;\n-    Chunk chunk;\n-\n-    WeakHash32 hash;\n-    Chunks output_chunks;\n-};\n-\n-}\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01568_window_functions_distributed.reference b/tests/queries/0_stateless/01568_window_functions_distributed.reference\nindex 29ff2e7133c6..13ac0769a245 100644\n--- a/tests/queries/0_stateless/01568_window_functions_distributed.reference\n+++ b/tests/queries/0_stateless/01568_window_functions_distributed.reference\n@@ -22,16 +22,6 @@ select sum(number) over w as x, max(number) over w as y from t_01568 window w as\n 21\t8\n 21\t8\n 21\t8\n-select sum(number) over w, max(number) over w from t_01568 window w as (partition by p) order by p;\n-3\t2\n-3\t2\n-3\t2\n-12\t5\n-12\t5\n-12\t5\n-21\t8\n-21\t8\n-21\t8\n select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n 6\t2\n 6\t2\n@@ -51,25 +41,6 @@ select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,\n 42\t8\n 42\t8\n 42\t8\n-select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y SETTINGS max_threads = 1;\n-6\t2\n-6\t2\n-6\t2\n-6\t2\n-6\t2\n-6\t2\n-24\t5\n-24\t5\n-24\t5\n-24\t5\n-24\t5\n-24\t5\n-42\t8\n-42\t8\n-42\t8\n-42\t8\n-42\t8\n-42\t8\n select distinct sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n 6\t2\n 24\t5\ndiff --git a/tests/queries/0_stateless/01568_window_functions_distributed.sql b/tests/queries/0_stateless/01568_window_functions_distributed.sql\nindex ecce7b412ba7..95072d6460f1 100644\n--- a/tests/queries/0_stateless/01568_window_functions_distributed.sql\n+++ b/tests/queries/0_stateless/01568_window_functions_distributed.sql\n@@ -15,12 +15,8 @@ from numbers(9);\n \n select sum(number) over w as x, max(number) over w as y from t_01568 window w as (partition by p) order by x, y;\n \n-select sum(number) over w, max(number) over w from t_01568 window w as (partition by p) order by p;\n-\n select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n \n-select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y SETTINGS max_threads = 1;\n-\n select distinct sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n \n -- window functions + aggregation w/shards\ndiff --git a/tests/queries/0_stateless/02884_parallel_window_functions.reference b/tests/queries/0_stateless/02884_parallel_window_functions.reference\ndeleted file mode 100644\nindex bac15838dc2a..000000000000\n--- a/tests/queries/0_stateless/02884_parallel_window_functions.reference\n+++ /dev/null\n@@ -1,100 +0,0 @@\n-1\n--- { echoOn }\n-\n-SELECT\n-    nw,\n-    sum(WR) AS R,\n-    sumIf(WR, uniq_rows = 1) AS UNR\n-FROM\n-(\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    GROUP BY ac, nw\n-)\n-GROUP BY nw\n-ORDER BY nw ASC, R DESC\n-LIMIT 10;\n-0\t2\t0\n-1\t2\t0\n-2\t2\t0\n-SELECT\n-    nw,\n-    sum(WR) AS R,\n-    sumIf(WR, uniq_rows = 1) AS UNR\n-FROM\n-(\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    GROUP BY ac, nw\n-)\n-GROUP BY nw\n-ORDER BY nw ASC, R DESC\n-LIMIT 10\n-SETTINGS max_threads = 1;\n-0\t2\t0\n-1\t2\t0\n-2\t2\t0\n-SELECT\n-    nw,\n-    sum(WR) AS R,\n-    sumIf(WR, uniq_rows = 1) AS UNR\n-FROM\n-(\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 0\n-    GROUP BY\n-        ac,\n-        nw\n-    UNION ALL\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 1\n-    GROUP BY\n-        ac,\n-        nw\n-    UNION ALL\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 2\n-    GROUP BY\n-        ac,\n-        nw\n-    UNION ALL\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 3\n-    GROUP BY\n-        ac,\n-        nw\n-)\n-GROUP BY nw\n-ORDER BY nw ASC, R DESC\n-LIMIT 10;\n-0\t2\t0\n-1\t2\t0\n-2\t2\t0\ndiff --git a/tests/queries/0_stateless/02884_parallel_window_functions.sql b/tests/queries/0_stateless/02884_parallel_window_functions.sql\ndeleted file mode 100644\nindex 3151b42f8960..000000000000\n--- a/tests/queries/0_stateless/02884_parallel_window_functions.sql\n+++ /dev/null\n@@ -1,119 +0,0 @@\n-CREATE TABLE window_funtion_threading\n-Engine = MergeTree\n-ORDER BY (ac, nw)\n-AS SELECT\n-        toUInt64(toFloat32(number % 2) % 20000000) as ac,\n-        toFloat32(1) as wg,        \n-        toUInt16(toFloat32(number % 3) % 400) as nw\n-FROM numbers_mt(10000000);\n-\n-SELECT count() FROM (EXPLAIN PIPELINE SELECT\n-    nw,\n-    sum(WR) AS R,\n-    sumIf(WR, uniq_rows = 1) AS UNR\n-FROM\n-(\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    GROUP BY ac, nw\n-)\n-GROUP BY nw\n-ORDER BY nw ASC, R DESC\n-LIMIT 10) where explain ilike '%ScatterByPartitionTransform%' SETTINGS max_threads = 4;\n-\n--- { echoOn }\n-\n-SELECT\n-    nw,\n-    sum(WR) AS R,\n-    sumIf(WR, uniq_rows = 1) AS UNR\n-FROM\n-(\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    GROUP BY ac, nw\n-)\n-GROUP BY nw\n-ORDER BY nw ASC, R DESC\n-LIMIT 10;\n-\n-SELECT\n-    nw,\n-    sum(WR) AS R,\n-    sumIf(WR, uniq_rows = 1) AS UNR\n-FROM\n-(\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    GROUP BY ac, nw\n-)\n-GROUP BY nw\n-ORDER BY nw ASC, R DESC\n-LIMIT 10\n-SETTINGS max_threads = 1;\n-\n-SELECT\n-    nw,\n-    sum(WR) AS R,\n-    sumIf(WR, uniq_rows = 1) AS UNR\n-FROM\n-(\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 0\n-    GROUP BY\n-        ac,\n-        nw\n-    UNION ALL\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 1\n-    GROUP BY\n-        ac,\n-        nw\n-    UNION ALL\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 2\n-    GROUP BY\n-        ac,\n-        nw\n-    UNION ALL\n-    SELECT\n-        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n-        AVG(wg) AS WR,\n-        ac,\n-        nw\n-    FROM window_funtion_threading\n-    WHERE (ac % 4) = 3\n-    GROUP BY\n-        ac,\n-        nw\n-)\n-GROUP BY nw\n-ORDER BY nw ASC, R DESC\n-LIMIT 10;\ndiff --git a/tests/queries/0_stateless/02942_window_functions_logical_error.reference b/tests/queries/0_stateless/02942_window_functions_logical_error.reference\nnew file mode 100644\nindex 000000000000..73f8351d9dfd\n--- /dev/null\n+++ b/tests/queries/0_stateless/02942_window_functions_logical_error.reference\n@@ -0,0 +1,216 @@\n+1\t901\t19\n+1\t911\t19\n+1\t921\t19\n+1\t931\t19\n+1\t941\t19\n+1\t951\t20\n+1\t961\t20\n+1\t971\t20\n+1\t981\t20\n+1\t991\t20\n+2\t902\t19\n+2\t912\t19\n+2\t922\t19\n+2\t932\t19\n+2\t942\t19\n+2\t952\t20\n+2\t962\t20\n+2\t972\t20\n+2\t982\t20\n+2\t992\t20\n+3\t903\t19\n+3\t913\t19\n+3\t923\t19\n+3\t933\t19\n+3\t943\t19\n+3\t953\t20\n+3\t963\t20\n+3\t973\t20\n+3\t983\t20\n+3\t993\t20\n+4\t904\t19\n+4\t914\t19\n+4\t924\t19\n+4\t934\t19\n+4\t944\t19\n+4\t954\t20\n+4\t964\t20\n+4\t974\t20\n+4\t984\t20\n+4\t994\t20\n+5\t905\t19\n+5\t915\t19\n+5\t925\t19\n+5\t935\t19\n+5\t945\t19\n+5\t955\t20\n+5\t965\t20\n+5\t975\t20\n+5\t985\t20\n+5\t995\t20\n+6\t906\t19\n+6\t916\t19\n+6\t926\t19\n+6\t936\t19\n+6\t946\t19\n+6\t956\t20\n+6\t966\t20\n+6\t976\t20\n+6\t986\t20\n+6\t996\t20\n+7\t907\t19\n+7\t917\t19\n+7\t927\t19\n+7\t937\t19\n+7\t947\t19\n+7\t957\t20\n+7\t967\t20\n+7\t977\t20\n+7\t987\t20\n+7\t997\t20\n+8\t908\t19\n+8\t918\t19\n+8\t928\t19\n+8\t938\t19\n+8\t948\t19\n+8\t958\t20\n+8\t968\t20\n+8\t978\t20\n+8\t988\t20\n+8\t998\t20\n+9\t909\t19\n+9\t919\t19\n+9\t929\t19\n+9\t939\t19\n+9\t949\t19\n+9\t959\t20\n+9\t969\t20\n+9\t979\t20\n+9\t989\t20\n+9\t999\t20\n+1\t1301\t19\n+1\t1311\t19\n+1\t1321\t19\n+1\t1331\t19\n+1\t1341\t19\n+1\t1351\t19\n+1\t1361\t19\n+1\t1371\t20\n+1\t1381\t20\n+1\t1391\t20\n+1\t1401\t20\n+1\t1411\t20\n+1\t1421\t20\n+1\t1431\t20\n+2\t1302\t19\n+2\t1312\t19\n+2\t1322\t19\n+2\t1332\t19\n+2\t1342\t19\n+2\t1352\t19\n+2\t1362\t19\n+2\t1372\t20\n+2\t1382\t20\n+2\t1392\t20\n+2\t1402\t20\n+2\t1412\t20\n+2\t1422\t20\n+2\t1432\t20\n+3\t1303\t19\n+3\t1313\t19\n+3\t1323\t19\n+3\t1333\t19\n+3\t1343\t19\n+3\t1353\t19\n+3\t1363\t19\n+3\t1373\t20\n+3\t1383\t20\n+3\t1393\t20\n+3\t1403\t20\n+3\t1413\t20\n+3\t1423\t20\n+3\t1433\t20\n+4\t1304\t19\n+4\t1314\t19\n+4\t1324\t19\n+4\t1334\t19\n+4\t1344\t19\n+4\t1354\t19\n+4\t1364\t19\n+4\t1374\t20\n+4\t1384\t20\n+4\t1394\t20\n+4\t1404\t20\n+4\t1414\t20\n+4\t1424\t20\n+4\t1434\t20\n+5\t1305\t19\n+5\t1315\t19\n+5\t1325\t19\n+5\t1335\t19\n+5\t1345\t19\n+5\t1355\t19\n+5\t1365\t19\n+5\t1375\t20\n+5\t1385\t20\n+5\t1395\t20\n+5\t1405\t20\n+5\t1415\t20\n+5\t1425\t20\n+5\t1435\t20\n+6\t1306\t19\n+6\t1316\t19\n+6\t1326\t19\n+6\t1336\t19\n+6\t1346\t19\n+6\t1356\t19\n+6\t1366\t19\n+6\t1376\t20\n+6\t1386\t20\n+6\t1396\t20\n+6\t1406\t20\n+6\t1416\t20\n+6\t1426\t20\n+6\t1436\t20\n+7\t1307\t19\n+7\t1317\t19\n+7\t1327\t19\n+7\t1337\t19\n+7\t1347\t19\n+7\t1357\t19\n+7\t1367\t19\n+7\t1377\t20\n+7\t1387\t20\n+7\t1397\t20\n+7\t1407\t20\n+7\t1417\t20\n+7\t1427\t20\n+7\t1437\t20\n+8\t1308\t19\n+8\t1318\t19\n+8\t1328\t19\n+8\t1338\t19\n+8\t1348\t19\n+8\t1358\t19\n+8\t1368\t19\n+8\t1378\t20\n+8\t1388\t20\n+8\t1398\t20\n+8\t1408\t20\n+8\t1418\t20\n+8\t1428\t20\n+8\t1438\t20\n+9\t1309\t19\n+9\t1319\t19\n+9\t1329\t19\n+9\t1339\t19\n+9\t1349\t19\n+9\t1359\t19\n+9\t1369\t19\n+9\t1379\t20\n+9\t1389\t20\n+9\t1399\t20\n+9\t1409\t20\n+9\t1419\t20\n+9\t1429\t20\n+9\t1439\t20\ndiff --git a/tests/queries/0_stateless/02942_window_functions_logical_error.sql b/tests/queries/0_stateless/02942_window_functions_logical_error.sql\nnew file mode 100644\nindex 000000000000..1e4371a134ff\n--- /dev/null\n+++ b/tests/queries/0_stateless/02942_window_functions_logical_error.sql\n@@ -0,0 +1,158 @@\n+DROP TABLE IF EXISTS posts;\n+DROP TABLE IF EXISTS post_metrics;\n+\n+CREATE TABLE IF NOT EXISTS posts\n+(\n+    `page_id` LowCardinality(String),\n+    `post_id` String CODEC(LZ4),\n+    `host_id` UInt32 CODEC(T64, LZ4),\n+    `path_id` UInt32,\n+    `created` DateTime CODEC(T64, LZ4),\n+    `as_of` DateTime CODEC(T64, LZ4)\n+)\n+ENGINE = ReplacingMergeTree(as_of)\n+PARTITION BY toStartOfMonth(created)\n+ORDER BY (page_id, post_id)\n+TTL created + toIntervalMonth(26);\n+\n+\n+INSERT INTO posts SELECT\n+    repeat('a', (number % 10) + 1),\n+    toString(number),\n+    number % 10,\n+    number,\n+    now() - toIntervalMinute(number),\n+    now()\n+FROM numbers(1000);\n+\n+\n+CREATE TABLE IF NOT EXISTS post_metrics\n+(\n+    `page_id` LowCardinality(String),\n+    `post_id` String CODEC(LZ4),\n+    `created` DateTime CODEC(T64, LZ4),\n+    `impressions` UInt32 CODEC(T64, LZ4),\n+    `clicks` UInt32 CODEC(T64, LZ4),\n+    `as_of` DateTime CODEC(T64, LZ4)\n+)\n+ENGINE = ReplacingMergeTree(as_of)\n+PARTITION BY toStartOfMonth(created)\n+ORDER BY (page_id, post_id)\n+TTL created + toIntervalMonth(26);\n+\n+\n+INSERT INTO post_metrics SELECT\n+    repeat('a', (number % 10) + 1),\n+    toString(number),\n+    now() - toIntervalMinute(number),\n+    number * 100,\n+    number * 10,\n+    now()\n+FROM numbers(1000);\n+\n+\n+SELECT\n+    host_id,\n+    path_id,\n+    max(rank) AS rank\n+FROM\n+(\n+    WITH\n+        as_of_posts AS\n+        (\n+            SELECT\n+                *,\n+                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\n+            FROM posts\n+            WHERE (created >= subtractHours(now(), 24)) AND (host_id > 0)\n+        ),\n+        as_of_post_metrics AS\n+        (\n+            SELECT\n+                *,\n+                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\n+            FROM post_metrics\n+            WHERE created >= subtractHours(now(), 24)\n+        )\n+    SELECT\n+        page_id,\n+        post_id,\n+        host_id,\n+        path_id,\n+        impressions,\n+        clicks,\n+        ntile(20) OVER (PARTITION BY page_id ORDER BY clicks ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS rank\n+    FROM as_of_posts\n+    GLOBAL LEFT JOIN as_of_post_metrics USING (page_id, post_id, row_num)\n+    WHERE (row_num = 1) AND (impressions > 0)\n+) AS t\n+WHERE t.rank > 18\n+GROUP BY\n+    host_id,\n+    path_id\n+ORDER BY host_id, path_id;\n+\n+\n+INSERT INTO posts SELECT\n+    repeat('a', (number % 10) + 1),\n+    toString(number),\n+    number % 10,\n+    number,\n+    now() - toIntervalMinute(number),\n+    now()\n+FROM numbers(100000);\n+\n+\n+INSERT INTO post_metrics SELECT\n+    repeat('a', (number % 10) + 1),\n+    toString(number),\n+    now() - toIntervalMinute(number),\n+    number * 100,\n+    number * 10,\n+    now()\n+FROM numbers(100000);\n+\n+\n+SELECT\n+    host_id,\n+    path_id,\n+    max(rank) AS rank\n+FROM\n+(\n+    WITH\n+        as_of_posts AS\n+        (\n+            SELECT\n+                *,\n+                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\n+            FROM posts\n+            WHERE (created >= subtractHours(now(), 24)) AND (host_id > 0)\n+        ),\n+        as_of_post_metrics AS\n+        (\n+            SELECT\n+                *,\n+                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\n+            FROM post_metrics\n+            WHERE created >= subtractHours(now(), 24)\n+        )\n+    SELECT\n+        page_id,\n+        post_id,\n+        host_id,\n+        path_id,\n+        impressions,\n+        clicks,\n+        ntile(20) OVER (PARTITION BY page_id ORDER BY clicks ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS rank\n+    FROM as_of_posts\n+    GLOBAL LEFT JOIN as_of_post_metrics USING (page_id, post_id, row_num)\n+    WHERE (row_num = 1) AND (impressions > 0)\n+) AS t\n+WHERE t.rank > 18\n+GROUP BY\n+    host_id,\n+    path_id\n+ORDER BY host_id, path_id;\n+\n+DROP TABLE posts;\n+DROP TABLE post_metrics;\n",
  "problem_statement": "Segmentation fault (NULL pointer) in release 23.11.1.2711\nIn release **23.11.1.2711**, a query using the `ntile` window function causes the ClickHouse server process to fatally exit. This bug is not present in release **23.10.5.20**. The ClickHouse VM is running Ubuntu 22.04.3 LTS x86_64 and ClickHouse was installed using the deb packages.\r\n\r\nThe following are details extracted from the logs:\r\n```\r\n\u2026\r\n2023.12.07 10:11:58.292314 [ 21534 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2023.12.07 10:11:58.292432 [ 21534 ] {} <Fatal> BaseDaemon: (version 23.11.1.2711 (official build), build id: C607B991623D3FAB7FC1EB24A1567BDE1F7BD7AD, git hash: 05bc8ef1e02b9c7332f08091775b255d191341bf) (from thread 21484) Received signal 11\r\n2023.12.07 10:11:58.292542 [ 21534 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault\r\n2023.12.07 10:11:58.292559 [ 21534 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Unknown si_code.\r\n2023.12.07 10:11:58.292577 [ 21534 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000000012596edf 0x0000000012590d53 0x00000000122a117a 0x0000000012297e90 0x0000000012298f0f 0x000000000c5e1a30 0x000000000c5e57bc 0x000000000c5e4053 0x00007f9468a94ac3 0x00007f9468b26a40\r\n2023.12.07 10:11:58.292492 [ 21535 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2023.12.07 10:11:58.292591 [ 21534 ] {} <Fatal> BaseDaemon: ########################################\r\n\u2026\r\n) Received signal Segmentation fault (11)\r\n2023.12.07 10:11:58.292991 [ 21534 ] {} <Fatal> BaseDaemon: 4. DB::ExecutionThreadContext::executeTask() @ 0x00000000122a117a in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293016 [ 21535 ] {} <Fatal> BaseDaemon: 2. DB::WindowFunctionNtile::windowInsertResultInto(DB::WindowTransform const*, unsigned long) @ 0x0000000012596edf in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293028 [ 21536 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Unknown si_code.\r\n2023.12.07 10:11:58.293044 [ 21536 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000000012596edf 0x0000000012590d53 0x00000000122a117a 0x0000000012297e90 0x0000000012298f0f 0x000000000c5e1a30 0x000000000c5e57bc 0x000000000c5e4053 0x00007f9468a94ac3 0x00007f9468b26a40\r\n2023.12.07 10:11:58.293045 [ 21534 ] {} <Fatal> BaseDaemon: 5. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000012297e90 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293063 [ 21535 ] {} <Fatal> BaseDaemon: 3. DB::WindowTransform::work() @ 0x0000000012590d53 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293075 [ 21534 ] {} <Fatal> BaseDaemon: 6. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x0000000012298f0f in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293084 [ 21535 ] {} <Fatal> BaseDaemon: 4. DB::ExecutionThreadContext::executeTask() @ 0x00000000122a117a in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293088 [ 21536 ] {} <Fatal> BaseDaemon: 2. DB::WindowFunctionNtile::windowInsertResultInto(DB::WindowTransform const*, unsigned long) @ 0x0000000012596edf in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293110 [ 21536 ] {} <Fatal> BaseDaemon: 3. DB::WindowTransform::work() @ 0x0000000012590d53 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293117 [ 21534 ] {} <Fatal> BaseDaemon: 7. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x000000000c5e1a30 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293126 [ 21536 ] {} <Fatal> BaseDaemon: 4. DB::ExecutionThreadContext::executeTask() @ 0x00000000122a117a in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293126 [ 21535 ] {} <Fatal> BaseDaemon: 5. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000012297e90 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293156 [ 21534 ] {} <Fatal> BaseDaemon: 8. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000c5e57bc in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293171 [ 21535 ] {} <Fatal> BaseDaemon: 6. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x0000000012298f0f in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293162 [ 21536 ] {} <Fatal> BaseDaemon: 5. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000012297e90 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293249 [ 21536 ] {} <Fatal> BaseDaemon: 6. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x0000000012298f0f in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293269 [ 21534 ] {} <Fatal> BaseDaemon: 9. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c5e4053 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293276 [ 21535 ] {} <Fatal> BaseDaemon: 7. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x000000000c5e1a30 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293289 [ 21534 ] {} <Fatal> BaseDaemon: 10. ? @ 0x00007f9468a94ac3 in ?\r\n2023.12.07 10:11:58.293278 [ 21536 ] {} <Fatal> BaseDaemon: 7. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x000000000c5e1a30 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293325 [ 21534 ] {} <Fatal> BaseDaemon: 11. ? @ 0x00007f9468b26a40 in ?\r\n2023.12.07 10:11:58.293329 [ 21536 ] {} <Fatal> BaseDaemon: 8. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000c5e57bc in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293326 [ 21535 ] {} <Fatal> BaseDaemon: 8. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000c5e57bc in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293347 [ 21536 ] {} <Fatal> BaseDaemon: 9. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c5e4053 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293382 [ 21535 ] {} <Fatal> BaseDaemon: 9. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c5e4053 in /usr/bin/clickhouse\r\n2023.12.07 10:11:58.293388 [ 21536 ] {} <Fatal> BaseDaemon: 10. ? @ 0x00007f9468a94ac3 in ?\r\n2023.12.07 10:11:58.293405 [ 21535 ] {} <Fatal> BaseDaemon: 10. ? @ 0x00007f9468a94ac3 in ?\r\n2023.12.07 10:11:58.293408 [ 21536 ] {} <Fatal> BaseDaemon: 11. ? @ 0x00007f9468b26a40 in ?\r\n2023.12.07 10:11:58.293423 [ 21535 ] {} <Fatal> BaseDaemon: 11. ? @ 0x00007f9468b26a40 in ?\r\n2023.12.07 10:11:58.418501 [ 21536 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 407BFCBD6FDD409E3B38D2FAC9282A24)\r\n2023.12.07 10:11:58.418561 [ 21535 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 407BFCBD6FDD409E3B38D2FAC9282A24)\r\n2023.12.07 10:11:58.418574 [ 21536 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\r\n2023.12.07 10:11:58.418642 [ 21535 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\r\n2023.12.07 10:11:58.418717 [ 21536 ] {} <Fatal> BaseDaemon: Changed settings: use_uncompressed_cache = false, load_balancing = 'random', log_queries = false, enable_http_compression = true, max_memory_usage = 10000000000, log_query_threads = false, format_avro_schema_registry_url = 'http://10.142.0.125'\r\n2023.12.07 10:11:58.418751 [ 21535 ] {} <Fatal> BaseDaemon: Changed settings: use_uncompressed_cache = false, load_balancing = 'random', log_queries = false, enable_http_compression = true, max_memory_usage = 10000000000, log_query_threads = false, format_avro_schema_registry_url = 'http://10.142.0.125'\r\n2023.12.07 10:11:58.418812 [ 21534 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 407BFCBD6FDD409E3B38D2FAC9282A24)\r\n2023.12.07 10:11:58.418852 [ 21534 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\r\n\u2026\r\n```\r\n\n",
  "hints_text": "can you please share a reproducible example or at least the table structure and a query?\n> can you please share a reproducible example or at least the table structure and a query?\r\n\r\nYes. I plan to work on a reproducible form this weekend.\nOne addition oddity from the logs that preceded the crash is:\r\n\r\n```\r\n2023.12.07 10:10:52.641395 [ 21522 ] {69cd5ba4-d0e8-46e8-8f27-f8191a72b004} <Error> executeQuery: Code: 49. DB::Exception: Invalid number of rows in Chunk column UInt64 position 4: expected 98, got 100. (LOGICAL_ERROR) (version 23.11.1.2711 (official build))\r\n```\nI was not able to recreate the crash on a single server but I was able to recreate the above \"Invalid number of rows\" error that was a precursor to the crash. I connected using `clickhouse client` to a single container instance (`docker run -d --name ch --ulimit nofile=262144:262144 clickhouse/clickhouse-server:23.11.1.2711-alpine`):\r\n\r\n```\r\nCREATE TABLE IF NOT EXISTS default.posts\r\n(\r\n    `page_id` LowCardinality(String),\r\n    `post_id` String CODEC(LZ4),\r\n    `host_id` UInt32 CODEC(T64, LZ4),\r\n    `path_id` UInt32,\r\n    `created` DateTime CODEC(T64, LZ4),\r\n    `as_of` DateTime CODEC(T64, LZ4)\r\n)\r\nENGINE = ReplacingMergeTree(as_of)\r\nPARTITION BY toStartOfMonth(created)\r\nORDER BY (page_id, post_id)\r\nTTL created + toIntervalMonth(26);\r\n\r\n\u2026\r\n\r\nINSERT INTO default.posts SELECT\r\n    repeat('a', (number % 10) + 1),\r\n    toString(number),\r\n    number % 10,\r\n    number,\r\n    now() - toIntervalMinute(number),\r\n    now()\r\nFROM numbers(1000);\r\n\r\n\u2026\r\n\r\nCREATE TABLE IF NOT EXISTS default.post_metrics\r\n(\r\n    `page_id` LowCardinality(String),\r\n    `post_id` String CODEC(LZ4),\r\n    `created` DateTime CODEC(T64, LZ4),\r\n    `impressions` UInt32 CODEC(T64, LZ4),\r\n    `clicks` UInt32 CODEC(T64, LZ4),\r\n    `as_of` DateTime CODEC(T64, LZ4)\r\n)\r\nENGINE = ReplacingMergeTree(as_of)\r\nPARTITION BY toStartOfMonth(created)\r\nORDER BY (page_id, post_id)\r\nTTL created + toIntervalMonth(26);\r\n\r\n\u2026\r\n\r\nINSERT INTO default.post_metrics SELECT\r\n    repeat('a', (number % 10) + 1),\r\n    toString(number),\r\n    now() - toIntervalMinute(number),\r\n    number * 100,\r\n    number * 10,\r\n    now()\r\nFROM numbers(1000);\r\n\r\n\u2026\r\n\r\nSELECT\r\n    host_id,\r\n    path_id,\r\n    max(rank) AS rank\r\nFROM\r\n(\r\n    WITH\r\n        as_of_posts AS\r\n        (\r\n            SELECT\r\n                *,\r\n                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\r\n            FROM default.posts\r\n            WHERE (created >= subtractHours(now(), 24)) AND (host_id > 0)\r\n        ),\r\n        as_of_post_metrics AS\r\n        (\r\n            SELECT\r\n                *,\r\n                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\r\n            FROM default.post_metrics\r\n            WHERE created >= subtractHours(now(), 24)\r\n        )\r\n    SELECT\r\n        page_id,\r\n        post_id,\r\n        host_id,\r\n        path_id,\r\n        impressions,\r\n        clicks,\r\n        ntile(20) OVER (PARTITION BY page_id ORDER BY clicks ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS rank\r\n    FROM as_of_posts\r\n    GLOBAL LEFT JOIN as_of_post_metrics USING (page_id, post_id, row_num)\r\n    WHERE (row_num = 1) AND (impressions > 0)\r\n) AS t\r\nWHERE t.rank > 18\r\nGROUP BY\r\n    host_id,\r\n    path_id;\r\n\r\n\u2026\r\n\r\nINSERT INTO default.posts SELECT\r\n    repeat('a', (number % 10) + 1),\r\n    toString(number),\r\n    number % 10,\r\n    number,\r\n    now() - toIntervalMinute(number),\r\n    now()\r\nFROM numbers(100000);\r\n\r\n\u2026\r\n\r\nINSERT INTO default.post_metrics SELECT\r\n    repeat('a', (number % 10) + 1),\r\n    toString(number),\r\n    now() - toIntervalMinute(number),\r\n    number * 100,\r\n    number * 10,\r\n    now()\r\nFROM numbers(100000);\r\n\r\n\u2026\r\n\r\nSELECT\r\n    host_id,\r\n    path_id,\r\n    max(rank) AS rank\r\nFROM\r\n(\r\n    WITH\r\n        as_of_posts AS\r\n        (\r\n            SELECT\r\n                *,\r\n                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\r\n            FROM default.posts\r\n            WHERE (created >= subtractHours(now(), 24)) AND (host_id > 0)\r\n        ),\r\n        as_of_post_metrics AS\r\n        (\r\n            SELECT\r\n                *,\r\n                row_number() OVER (PARTITION BY (page_id, post_id) ORDER BY as_of DESC) AS row_num\r\n            FROM default.post_metrics\r\n            WHERE created >= subtractHours(now(), 24)\r\n        )\r\n    SELECT\r\n        page_id,\r\n        post_id,\r\n        host_id,\r\n        path_id,\r\n        impressions,\r\n        clicks,\r\n        ntile(20) OVER (PARTITION BY page_id ORDER BY clicks ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS rank\r\n    FROM as_of_posts\r\n    GLOBAL LEFT JOIN as_of_post_metrics USING (page_id, post_id, row_num)\r\n    WHERE (row_num = 1) AND (impressions > 0)\r\n) AS t\r\nWHERE t.rank > 18\r\nGROUP BY\r\n    host_id,\r\n    path_id;\r\n\u2026\r\nReceived exception from server (version 23.11.1):\r\nCode: 49. DB::Exception: Received from ch.orb.local:9000. DB::Exception: Invalid number of rows in Chunk column UInt64 position 5: expected 432, got 306. (LOGICAL_ERROR)\r\n```\nThank you! Reproduced.",
  "created_at": "2023-12-12T02:46:30Z"
}