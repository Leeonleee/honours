{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 81833,
  "instance_id": "ClickHouse__ClickHouse-81833",
  "issue_numbers": [
    "81740"
  ],
  "base_commit": "8b78e120453714ca200d31d5cbc0c81902bc3d2d",
  "patch": "diff --git a/src/Storages/KVStorageUtils.cpp b/src/Storages/KVStorageUtils.cpp\nindex 151717371e2f..1520348e3668 100644\n--- a/src/Storages/KVStorageUtils.cpp\n+++ b/src/Storages/KVStorageUtils.cpp\n@@ -3,9 +3,7 @@\n #include <Columns/ColumnSet.h>\n #include <Columns/ColumnConst.h>\n \n-#include <Parsers/ASTFunction.h>\n #include <Parsers/ASTIdentifier.h>\n-#include <Parsers/ASTSubquery.h>\n #include <Parsers/ASTLiteral.h>\n #include <Parsers/ASTSelectQuery.h>\n \n@@ -15,7 +13,6 @@\n \n #include <Functions/IFunction.h>\n \n-\n namespace DB\n {\n \n@@ -26,106 +23,8 @@ namespace ErrorCodes\n \n namespace\n {\n-// returns keys may be filter by condition\n-bool traverseASTFilter(\n-    const std::string & primary_key, const DataTypePtr & primary_key_type, const ASTPtr & elem, const PreparedSetsPtr & prepared_sets, const ContextPtr & context, FieldVectorPtr & res)\n-{\n-    const auto * function = elem->as<ASTFunction>();\n-    if (!function)\n-        return false;\n-\n-    if (function->name == \"and\")\n-    {\n-        // one child has the key filter condition is ok\n-        for (const auto & child : function->arguments->children)\n-            if (traverseASTFilter(primary_key, primary_key_type, child, prepared_sets, context, res))\n-                return true;\n-        return false;\n-    }\n-    if (function->name == \"or\")\n-    {\n-        // make sure every child has the key filter condition\n-        for (const auto & child : function->arguments->children)\n-            if (!traverseASTFilter(primary_key, primary_key_type, child, prepared_sets, context, res))\n-                return false;\n-        return true;\n-    }\n-    if (function->name == \"equals\" || function->name == \"in\")\n-    {\n-        const auto & args = function->arguments->as<ASTExpressionList &>();\n-        const ASTIdentifier * ident;\n-        std::shared_ptr<IAST> value;\n-\n-        if (args.children.size() != 2)\n-            return false;\n-\n-        if (function->name == \"in\")\n-        {\n-            if (!prepared_sets)\n-                return false;\n-\n-            ident = args.children.at(0)->as<ASTIdentifier>();\n-            if (!ident)\n-                return false;\n-\n-            if (ident->name() != primary_key)\n-                return false;\n-            value = args.children.at(1);\n-\n-            PreparedSets::Hash set_key = value->getTreeHash(/*ignore_aliases=*/true);\n-            FutureSetPtr future_set;\n-\n-            if ((value->as<ASTSubquery>() || value->as<ASTIdentifier>()))\n-                future_set = prepared_sets->findSubquery(set_key);\n-            else\n-                future_set = prepared_sets->findTuple(set_key, {primary_key_type});\n-\n-            if (!future_set)\n-                return false;\n-\n-            future_set->buildOrderedSetInplace(context);\n-\n-            auto set = future_set->get();\n-            if (!set)\n-                return false;\n-\n-            if (!set->hasExplicitSetElements())\n-                return false;\n-\n-            set->checkColumnsNumber(1);\n-            const auto & set_column = *set->getSetElements()[0];\n-\n-            if (set_column.getDataType() != primary_key_type->getTypeId())\n-                return false;\n-\n-            for (size_t row = 0; row < set_column.size(); ++row)\n-                res->push_back(set_column[row]);\n-            return true;\n-        }\n-\n-        if ((ident = args.children.at(0)->as<ASTIdentifier>()))\n-            value = args.children.at(1);\n-        else if ((ident = args.children.at(1)->as<ASTIdentifier>()))\n-            value = args.children.at(0);\n-        else\n-            return false;\n-\n-        if (ident->name() != primary_key)\n-            return false;\n-\n-        const auto node = evaluateConstantExpressionAsLiteral(value, context);\n-        /// function->name == \"equals\"\n-        if (const auto * literal = node->as<ASTLiteral>())\n-        {\n-            auto converted_field = convertFieldToType(literal->value, *primary_key_type);\n-            if (!converted_field.isNull())\n-                res->push_back(converted_field);\n-            return true;\n-        }\n-    }\n-    return false;\n-}\n \n+// returns keys may be filtered by condition\n bool traverseDAGFilter(\n     const std::string & primary_key, const DataTypePtr & primary_key_type, const ActionsDAG::Node * elem, const ContextPtr & context, FieldVectorPtr & res)\n {\n@@ -239,18 +138,6 @@ std::pair<FieldVectorPtr, bool> getFilterKeys(\n     return std::make_pair(res, !matched_keys);\n }\n \n-std::pair<FieldVectorPtr, bool> getFilterKeys(\n-    const String & primary_key, const DataTypePtr & primary_key_type, const SelectQueryInfo & query_info, const ContextPtr & context)\n-{\n-    const auto & select = query_info.query->as<ASTSelectQuery &>();\n-    if (!select.where())\n-        return {{}, true};\n-\n-    FieldVectorPtr res = std::make_shared<FieldVector>();\n-    auto matched_keys = traverseASTFilter(primary_key, primary_key_type, select.where(), query_info.prepared_sets, context, res);\n-    return std::make_pair(res, !matched_keys);\n-}\n-\n std::vector<std::string> serializeKeysToRawString(\n     FieldVector::const_iterator & it,\n     FieldVector::const_iterator end,\ndiff --git a/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp b/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp\nindex 6a2df5ae5897..05faef2c1250 100644\n--- a/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp\n+++ b/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp\n@@ -6,6 +6,8 @@\n \n #include <Storages/StorageFactory.h>\n #include <Storages/KVStorageUtils.h>\n+#include <Storages/AlterCommands.h>\n+#include <Storages/RocksDB/RocksDBSettings.h>\n \n #include <Parsers/ASTCreateQuery.h>\n \n@@ -29,9 +31,9 @@\n #include <Common/Logger.h>\n #include <Common/logger_useful.h>\n #include <Common/Exception.h>\n+#include <Common/JSONBuilder.h>\n #include <Core/Settings.h>\n-#include <Storages/AlterCommands.h>\n-#include <Storages/RocksDB/RocksDBSettings.h>\n+\n #include <IO/SharedThreadPools.h>\n #include <Disks/DiskLocal.h>\n #include <base/sort.h>\n@@ -566,6 +568,8 @@ class ReadFromEmbeddedRocksDB : public SourceStepWithFilter\n     std::string getName() const override { return \"ReadFromEmbeddedRocksDB\"; }\n     void initializePipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &) override;\n     void applyFilters(ActionDAGNodes added_filter_nodes) override;\n+    void describeActions(FormatSettings & format_settings) const override;\n+    void describeActions(JSONBuilder::JSONMap & map) const override;\n \n     ReadFromEmbeddedRocksDB(\n         const Names & column_names_,\n@@ -666,6 +670,29 @@ void ReadFromEmbeddedRocksDB::applyFilters(ActionDAGNodes added_filter_nodes)\n     std::tie(keys, all_scan) = getFilterKeys(storage.primary_key, primary_key_data_type, filter_actions_dag, context);\n }\n \n+void ReadFromEmbeddedRocksDB::describeActions(FormatSettings & format_settings) const\n+{\n+    std::string prefix(format_settings.offset, format_settings.indent_char);\n+    if (!all_scan)\n+    {\n+        format_settings.out << prefix << \"ReadType: GetKeys\\n\";\n+        format_settings.out << prefix << \"Keys: \" << keys->size() << '\\n';\n+    }\n+    else\n+        format_settings.out << prefix << \"ReadType: FullScan\\n\";\n+}\n+\n+void ReadFromEmbeddedRocksDB::describeActions(JSONBuilder::JSONMap & map) const\n+{\n+    if (!all_scan)\n+    {\n+        map.add(\"Read Type\", \"GetKeys\");\n+        map.add(\"Keys\", keys->size());\n+    }\n+    else\n+        map.add(\"Read Type\", \"FullScan\");\n+}\n+\n SinkToStoragePtr StorageEmbeddedRocksDB::write(\n     const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, ContextPtr  query_context, bool /*async_insert*/)\n {\ndiff --git a/src/Storages/SelectQueryInfo.h b/src/Storages/SelectQueryInfo.h\nindex feadaf0290c1..291f52e8611b 100644\n--- a/src/Storages/SelectQueryInfo.h\n+++ b/src/Storages/SelectQueryInfo.h\n@@ -190,6 +190,7 @@ struct SelectQueryInfo\n     InputOrderInfoPtr input_order_info;\n \n     /// Prepared sets are used for indices by storage engine.\n+    /// New analyzer stores prepared sets in planner_context and hashes computed of QueryTree instead of AST.\n     /// Example: x IN (1, 2, 3)\n     PreparedSetsPtr prepared_sets;\n \ndiff --git a/src/Storages/StorageKeeperMap.cpp b/src/Storages/StorageKeeperMap.cpp\nindex e91c6213e996..6720fc75bd8a 100644\n--- a/src/Storages/StorageKeeperMap.cpp\n+++ b/src/Storages/StorageKeeperMap.cpp\n@@ -23,12 +23,12 @@\n #include <Compression/CompressedReadBufferFromFile.h>\n \n #include <Parsers/ASTCreateQuery.h>\n-#include <Parsers/ASTExpressionList.h>\n-#include <Parsers/ASTFunction.h>\n-#include <Parsers/ASTIdentifier.h>\n #include <Parsers/ASTSelectQuery.h>\n \n #include <Processors/ISource.h>\n+#include <Processors/QueryPlan/SourceStepWithFilter.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+#include <Processors/Sources/NullSource.h>\n #include <Processors/Sinks/SinkToStorage.h>\n #include <Processors/Executors/PullingPipelineExecutor.h>\n \n@@ -47,6 +47,7 @@\n #include <Common/ZooKeeper/ZooKeeper.h>\n #include <Common/ZooKeeper/ZooKeeperConstants.h>\n #include <Common/ZooKeeper/ZooKeeperRetries.h>\n+#include <Common/JSONBuilder.h>\n \n #include <Backups/BackupEntriesCollector.h>\n #include <Backups/IBackupCoordination.h>\n@@ -284,13 +285,8 @@ class StorageKeeperMapSource : public ISource, WithContext\n \n     bool with_version_column = false;\n \n-    static Block getHeader(Block header, bool with_version_column)\n+    static Block getHeader(Block header)\n     {\n-        if (with_version_column)\n-            header.insert(\n-                    {DataTypeInt32{}.createColumn(),\n-                    std::make_shared<DataTypeInt32>(), std::string{version_column_name}});\n-\n         return header;\n     }\n \n@@ -304,7 +300,7 @@ class StorageKeeperMapSource : public ISource, WithContext\n         KeyContainerIter end_,\n         bool with_version_column_,\n         ContextPtr context_)\n-        : ISource(getHeader(header, with_version_column_))\n+        : ISource(getHeader(header))\n         , WithContext(std::move(context_))\n         , storage(storage_)\n         , max_block_size(max_block_size_)\n@@ -601,25 +597,62 @@ StorageKeeperMap::StorageKeeperMap(\n         zk_root_path);\n }\n \n+class ReadFromKeeperMap : public SourceStepWithFilter\n+{\n+public:\n+    std::string getName() const override { return \"ReadFromKeeperMap\"; }\n+    void initializePipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &) override;\n+    void applyFilters(ActionDAGNodes added_filter_nodes) override;\n+    void describeActions(FormatSettings & format_settings) const override;\n+    void describeActions(JSONBuilder::JSONMap & map) const override;\n+\n+    ReadFromKeeperMap(\n+        const Names & column_names_,\n+        const SelectQueryInfo & query_info_,\n+        const StorageSnapshotPtr & storage_snapshot_,\n+        const ContextPtr & context_,\n+        Block sample_block,\n+        const StorageKeeperMap & storage_,\n+        size_t max_block_size_,\n+        size_t num_streams_,\n+        bool with_version_column_)\n+        : SourceStepWithFilter(std::move(sample_block), column_names_, query_info_, storage_snapshot_, context_)\n+        , storage(storage_)\n+        , max_block_size(max_block_size_)\n+        , num_streams(num_streams_)\n+        , with_version_column(with_version_column_)\n+    {\n+    }\n \n-Pipe StorageKeeperMap::read(\n-    const Names & column_names,\n-    const StorageSnapshotPtr & storage_snapshot,\n-    SelectQueryInfo & query_info,\n-    ContextPtr context_,\n-    QueryProcessingStage::Enum /*processed_stage*/,\n-    size_t max_block_size,\n-    size_t num_streams)\n+private:\n+    const StorageKeeperMap & storage;\n+\n+    size_t max_block_size;\n+    size_t num_streams;\n+    bool with_version_column;\n+\n+    FieldVectorPtr keys;\n+    bool all_scan = false;\n+\n+    template<typename KeyContainerPtr>\n+    void initializePipelineImpl(QueryPipelineBuilder & pipeline, KeyContainerPtr key_container);\n+\n+    Strings getAllKeys() const;\n+};\n+\n+void StorageKeeperMap::read(\n+        QueryPlan & query_plan,\n+        const Names & column_names,\n+        const StorageSnapshotPtr & storage_snapshot,\n+        SelectQueryInfo & query_info,\n+        ContextPtr context_,\n+        QueryProcessingStage::Enum /*processed_stage*/,\n+        size_t max_block_size,\n+        size_t num_streams)\n {\n     checkTable<true>(context_);\n     storage_snapshot->check(column_names);\n-\n-    FieldVectorPtr filtered_keys;\n-    bool all_scan;\n-\n     Block sample_block = storage_snapshot->metadata->getSampleBlock();\n-    auto primary_key_type = sample_block.getByName(primary_key).type;\n-    std::tie(filtered_keys, all_scan) = getFilterKeys(primary_key, primary_key_type, query_info, context_);\n \n     bool with_version_column = false;\n     for (const auto & column : column_names)\n@@ -631,56 +664,110 @@ Pipe StorageKeeperMap::read(\n         }\n     }\n \n-    const auto process_keys = [&]<typename KeyContainerPtr>(KeyContainerPtr keys) -> Pipe\n+    if (with_version_column)\n+        sample_block.insert({DataTypeInt32{}.createColumn(),\n+                    std::make_shared<DataTypeInt32>(), std::string{version_column_name}});\n+\n+    auto reading = std::make_unique<ReadFromKeeperMap>(\n+        column_names, query_info, storage_snapshot, context_, std::move(sample_block), *this, max_block_size, num_streams, with_version_column);\n+\n+    query_plan.addStep(std::move(reading));\n+}\n+\n+void ReadFromKeeperMap::initializePipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &)\n+{\n+    if (all_scan)\n+        initializePipelineImpl(pipeline, std::make_shared<Strings>(getAllKeys()));\n+    else\n+        initializePipelineImpl(pipeline, keys);\n+}\n+\n+void ReadFromKeeperMap::applyFilters(ActionDAGNodes added_filter_nodes)\n+{\n+    SourceStepWithFilter::applyFilters(std::move(added_filter_nodes));\n+\n+    const auto & sample_block = getOutputHeader();\n+    auto primary_key_data_type = sample_block.getByName(storage.primary_key).type;\n+    std::tie(keys, all_scan) = getFilterKeys(storage.primary_key, primary_key_data_type, filter_actions_dag, context);\n+}\n+\n+template<typename KeyContainerPtr>\n+void ReadFromKeeperMap::initializePipelineImpl(QueryPipelineBuilder & pipeline, KeyContainerPtr key_container)\n+{\n+    const auto & sample_block = getOutputHeader();\n+\n+    if (key_container->empty())\n     {\n-        if (keys->empty())\n-            return {};\n+        pipeline.init(Pipe(std::make_shared<NullSource>(sample_block)));\n+        return;\n+    }\n \n-        ::sort(keys->begin(), keys->end());\n-        keys->erase(std::unique(keys->begin(), keys->end()), keys->end());\n+    ::sort(key_container->begin(), key_container->end());\n+    key_container->erase(std::unique(key_container->begin(), key_container->end()), key_container->end());\n \n-        Pipes pipes;\n+    Pipes pipes;\n \n-        size_t num_keys = keys->size();\n-        size_t num_threads = std::min<size_t>(num_streams, keys->size());\n+    size_t num_keys = key_container->size();\n+    size_t num_threads = std::min<size_t>(num_streams, key_container->size());\n \n-        chassert(num_keys <= std::numeric_limits<uint32_t>::max());\n-        chassert(num_threads <= std::numeric_limits<uint32_t>::max());\n+    chassert(num_keys <= std::numeric_limits<uint32_t>::max());\n+    chassert(num_threads <= std::numeric_limits<uint32_t>::max());\n \n-        for (size_t thread_idx = 0; thread_idx < num_threads; ++thread_idx)\n-        {\n-            size_t begin = num_keys * thread_idx / num_threads;\n-            size_t end = num_keys * (thread_idx + 1) / num_threads;\n+    for (size_t thread_idx = 0; thread_idx < num_threads; ++thread_idx)\n+    {\n+        size_t begin = num_keys * thread_idx / num_threads;\n+        size_t end = num_keys * (thread_idx + 1) / num_threads;\n \n-            using KeyContainer = typename KeyContainerPtr::element_type;\n-            pipes.emplace_back(std::make_shared<StorageKeeperMapSource<KeyContainer>>(\n-                *this, sample_block, max_block_size, keys, keys->begin() + begin, keys->begin() + end, with_version_column, context_));\n-        }\n-        return Pipe::unitePipes(std::move(pipes));\n-    };\n+        using KeyContainer = typename KeyContainerPtr::element_type;\n+        pipes.emplace_back(std::make_shared<StorageKeeperMapSource<KeyContainer>>(\n+            storage, sample_block, max_block_size, key_container, key_container->begin() + begin, key_container->begin() + end, with_version_column, context));\n+    }\n+    pipeline.init(Pipe::unitePipes(std::move(pipes)));\n+}\n \n-    if (all_scan)\n+Strings ReadFromKeeperMap::getAllKeys() const\n+{\n+    const auto & settings = context->getSettingsRef();\n+    ZooKeeperRetriesControl zk_retry{\n+        getName(),\n+        getLogger(getName()),\n+        ZooKeeperRetriesInfo{\n+            settings[Setting::keeper_max_retries],\n+            settings[Setting::keeper_retry_initial_backoff_ms],\n+            settings[Setting::keeper_retry_max_backoff_ms],\n+            context->getProcessListElement()}};\n+\n+    Strings children;\n+    zk_retry.retryLoop([&]\n     {\n-        const auto & settings = context_->getSettingsRef();\n-        ZooKeeperRetriesControl zk_retry{\n-            getName(),\n-            getLogger(getName()),\n-            ZooKeeperRetriesInfo{\n-                settings[Setting::keeper_max_retries],\n-                settings[Setting::keeper_retry_initial_backoff_ms],\n-                settings[Setting::keeper_retry_max_backoff_ms],\n-                context_->getProcessListElement()}};\n+        auto client = storage.getClient();\n+        children = client->getChildren(storage.zk_data_path);\n+    });\n \n-        std::vector<std::string> children;\n-        zk_retry.retryLoop([&]\n-        {\n-            auto client = getClient();\n-            children = client->getChildren(zk_data_path);\n-        });\n-        return process_keys(std::make_shared<std::vector<std::string>>(std::move(children)));\n+    return children;\n+}\n+\n+void ReadFromKeeperMap::describeActions(FormatSettings & format_settings) const\n+{\n+    std::string prefix(format_settings.offset, format_settings.indent_char);\n+    if (!all_scan)\n+    {\n+        format_settings.out << prefix << \"ReadType: GetKeys\\n\";\n+        format_settings.out << prefix << \"Keys: \" << keys->size() << '\\n';\n     }\n+    else\n+        format_settings.out << prefix << \"ReadType: FullScan\\n\";\n+}\n \n-    return process_keys(std::move(filtered_keys));\n+void ReadFromKeeperMap::describeActions(JSONBuilder::JSONMap & map) const\n+{\n+    if (!all_scan)\n+    {\n+        map.add(\"Read Type\", \"GetKeys\");\n+        map.add(\"Keys\", keys->size());\n+    }\n+    else\n+        map.add(\"Read Type\", \"FullScan\");\n }\n \n SinkToStoragePtr StorageKeeperMap::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, ContextPtr local_context, bool /*async_insert*/)\ndiff --git a/src/Storages/StorageKeeperMap.h b/src/Storages/StorageKeeperMap.h\nindex a2c6d4cfcb03..ec920c974cea 100644\n--- a/src/Storages/StorageKeeperMap.h\n+++ b/src/Storages/StorageKeeperMap.h\n@@ -26,6 +26,7 @@ namespace ErrorCodes\n // KV store using (Zoo|CH)Keeper\n class StorageKeeperMap final : public IStorage, public IKeyValueEntity, WithContext\n {\n+    friend class ReadFromKeeperMap;\n public:\n     StorageKeeperMap(\n         ContextPtr context_,\n@@ -36,12 +37,13 @@ class StorageKeeperMap final : public IStorage, public IKeyValueEntity, WithCont\n         const std::string & root_path_,\n         UInt64 keys_limit_);\n \n-    Pipe read(\n+    void read(\n+        QueryPlan & query_plan,\n         const Names & column_names,\n         const StorageSnapshotPtr & storage_snapshot,\n         SelectQueryInfo & query_info,\n-        ContextPtr context,\n-        QueryProcessingStage::Enum processed_stage,\n+        ContextPtr context_,\n+        QueryProcessingStage::Enum /*processed_stage*/,\n         size_t max_block_size,\n         size_t num_streams) override;\n \ndiff --git a/src/Storages/StorageRedis.cpp b/src/Storages/StorageRedis.cpp\nindex fa0be4364827..d447321be041 100644\n--- a/src/Storages/StorageRedis.cpp\n+++ b/src/Storages/StorageRedis.cpp\n@@ -8,7 +8,10 @@\n #include <Parsers/ASTLiteral.h>\n #include <Processors/Executors/PullingPipelineExecutor.h>\n #include <Processors/Sinks/SinkToStorage.h>\n+#include <Processors/QueryPlan/SourceStepWithFilter.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n #include <Processors/ISource.h>\n+#include <Processors/Sources/NullSource.h>\n #include <QueryPipeline/Pipe.h>\n #include <QueryPipeline/QueryPipelineBuilder.h>\n \n@@ -25,6 +28,7 @@\n #include <Common/checkStackSize.h>\n #include <Common/logger_useful.h>\n #include <Common/parseAddress.h>\n+#include <Common/JSONBuilder.h>\n \n namespace DB\n {\n@@ -215,53 +219,134 @@ StorageRedis::StorageRedis(\n     setInMemoryMetadata(storage_metadata);\n }\n \n-Pipe StorageRedis::read(\n-    const Names & column_names,\n-    const StorageSnapshotPtr & storage_snapshot,\n-    SelectQueryInfo & query_info,\n-    ContextPtr context_,\n-    QueryProcessingStage::Enum /*processed_stage*/,\n-    size_t max_block_size,\n-    size_t num_streams)\n+class ReadFromRedis : public SourceStepWithFilter\n {\n-    storage_snapshot->check(column_names);\n+public:\n+    std::string getName() const override { return \"ReadFromRedis\"; }\n+    void initializePipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &) override;\n+    void applyFilters(ActionDAGNodes added_filter_nodes) override;\n+    void describeActions(FormatSettings & format_settings) const override;\n+    void describeActions(JSONBuilder::JSONMap & map) const override;\n+\n+    ReadFromRedis(\n+        const Names & column_names_,\n+        const SelectQueryInfo & query_info_,\n+        const StorageSnapshotPtr & storage_snapshot_,\n+        const ContextPtr & context_,\n+        Block sample_block,\n+        StorageRedis & storage_,\n+        size_t max_block_size_,\n+        size_t num_streams_)\n+        : SourceStepWithFilter(std::move(sample_block), column_names_, query_info_, storage_snapshot_, context_)\n+        , storage(storage_)\n+        , max_block_size(max_block_size_)\n+        , num_streams(num_streams_)\n+    {\n+    }\n+\n+private:\n+    StorageRedis & storage;\n+\n+    size_t max_block_size;\n+    size_t num_streams;\n \n     FieldVectorPtr keys;\n     bool all_scan = false;\n+};\n \n-    Block header = storage_snapshot->metadata->getSampleBlock();\n-    auto primary_key_data_type = header.getByName(primary_key).type;\n+void StorageRedis::read(\n+        QueryPlan & query_plan,\n+        const Names & column_names,\n+        const StorageSnapshotPtr & storage_snapshot,\n+        SelectQueryInfo & query_info,\n+        ContextPtr context_,\n+        QueryProcessingStage::Enum /*processed_stage*/,\n+        size_t max_block_size,\n+        size_t num_streams)\n+{\n+    storage_snapshot->check(column_names);\n+    Block sample_block = storage_snapshot->metadata->getSampleBlock();\n \n-    std::tie(keys, all_scan) = getFilterKeys(primary_key, primary_key_data_type, query_info, context_);\n+    auto reading = std::make_unique<ReadFromRedis>(\n+        column_names, query_info, storage_snapshot, context_, std::move(sample_block), *this, max_block_size, num_streams);\n+\n+    query_plan.addStep(std::move(reading));\n+}\n+\n+void ReadFromRedis::initializePipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &)\n+{\n+    const auto & sample_block = getOutputHeader();\n \n     if (all_scan)\n     {\n-        return Pipe(std::make_shared<RedisDataSource>(*this, header, max_block_size));\n+        auto source = std::make_shared<RedisDataSource>(storage, sample_block, max_block_size);\n+        source->setStorageLimits(query_info.storage_limits);\n+        pipeline.init(Pipe(std::move(source)));\n     }\n+    else\n+    {\n+        if (keys->empty())\n+        {\n+            pipeline.init(Pipe(std::make_shared<NullSource>(sample_block)));\n+            return;\n+        }\n \n-    if (keys->empty())\n-        return {};\n+        ::sort(keys->begin(), keys->end());\n+        keys->erase(std::unique(keys->begin(), keys->end()), keys->end());\n+\n+        Pipes pipes;\n \n-    Pipes pipes;\n+        size_t num_keys = keys->size();\n+        size_t num_threads = std::min<size_t>(num_streams, keys->size());\n+        num_threads = std::min<size_t>(num_threads, storage.configuration.pool_size);\n \n-    ::sort(keys->begin(), keys->end());\n-    keys->erase(std::unique(keys->begin(), keys->end()), keys->end());\n+        assert(num_keys <= std::numeric_limits<uint32_t>::max());\n+        assert(num_threads <= std::numeric_limits<uint32_t>::max());\n+\n+        for (size_t thread_idx = 0; thread_idx < num_threads; ++thread_idx)\n+        {\n+            size_t begin = num_keys * thread_idx / num_threads;\n+            size_t end = num_keys * (thread_idx + 1) / num_threads;\n+\n+            auto source = std::make_shared<RedisDataSource>(\n+                    storage, sample_block, keys, keys->begin() + begin, keys->begin() + end, max_block_size);\n+            source->setStorageLimits(query_info.storage_limits);\n+            pipes.emplace_back(std::move(source));\n+        }\n+        pipeline.init(Pipe::unitePipes(std::move(pipes)));\n+    }\n+}\n \n-    size_t num_keys = keys->size();\n-    size_t num_threads = std::min<size_t>(num_streams, keys->size());\n+void ReadFromRedis::applyFilters(ActionDAGNodes added_filter_nodes)\n+{\n+    SourceStepWithFilter::applyFilters(std::move(added_filter_nodes));\n \n-    num_threads = std::min<size_t>(num_threads, configuration.pool_size);\n-    assert(num_keys <= std::numeric_limits<uint32_t>::max());\n+    const auto & sample_block = getOutputHeader();\n+    auto primary_key_data_type = sample_block.getByName(storage.primary_key).type;\n+    std::tie(keys, all_scan) = getFilterKeys(storage.primary_key, primary_key_data_type, filter_actions_dag, context);\n+}\n \n-    for (size_t thread_idx = 0; thread_idx < num_threads; ++thread_idx)\n+void ReadFromRedis::describeActions(FormatSettings & format_settings) const\n+{\n+    std::string prefix(format_settings.offset, format_settings.indent_char);\n+    if (!all_scan)\n     {\n-        size_t begin = num_keys * thread_idx / num_threads;\n-        size_t end = num_keys * (thread_idx + 1) / num_threads;\n+        format_settings.out << prefix << \"ReadType: GetKeys\\n\";\n+        format_settings.out << prefix << \"Keys: \" << keys->size() << '\\n';\n+    }\n+    else\n+        format_settings.out << prefix << \"ReadType: FullScan\\n\";\n+}\n \n-        pipes.emplace_back(\n-            std::make_shared<RedisDataSource>(*this, header, keys, keys->begin() + begin, keys->begin() + end, max_block_size));\n+void ReadFromRedis::describeActions(JSONBuilder::JSONMap & map) const\n+{\n+    if (!all_scan)\n+    {\n+        map.add(\"Read Type\", \"GetKeys\");\n+        map.add(\"Keys\", keys->size());\n     }\n-    return Pipe::unitePipes(std::move(pipes));\n+    else\n+        map.add(\"Read Type\", \"FullScan\");\n }\n \n namespace\ndiff --git a/src/Storages/StorageRedis.h b/src/Storages/StorageRedis.h\nindex e91cd1023f3f..8fdf2c2c2f34 100644\n--- a/src/Storages/StorageRedis.h\n+++ b/src/Storages/StorageRedis.h\n@@ -14,6 +14,7 @@ namespace DB\n  */\n class StorageRedis : public IStorage, public IKeyValueEntity, WithContext\n {\n+    friend class ReadFromRedis;\n public:\n     StorageRedis(\n         const StorageID & table_id_,\n@@ -24,12 +25,13 @@ class StorageRedis : public IStorage, public IKeyValueEntity, WithContext\n \n     std::string getName() const override { return \"Redis\"; }\n \n-    Pipe read(\n+    void read(\n+        QueryPlan & query_plan,\n         const Names & column_names,\n         const StorageSnapshotPtr & storage_snapshot,\n         SelectQueryInfo & query_info,\n         ContextPtr context_,\n-        QueryProcessingStage::Enum processed_stage,\n+        QueryProcessingStage::Enum /*processed_stage*/,\n         size_t max_block_size,\n         size_t num_streams) override;\n \n",
  "test_patch": "diff --git a/tests/integration/test_storage_redis/test.py b/tests/integration/test_storage_redis/test.py\nindex d54718531707..5a931aa2f4ee 100644\n--- a/tests/integration/test_storage_redis/test.py\n+++ b/tests/integration/test_storage_redis/test.py\n@@ -1,4 +1,5 @@\n ## sudo -H pip install redis\n+import json\n import struct\n import sys\n \n@@ -430,3 +431,33 @@ def test_direct_join(started_cluster):\n                                     \"test_direct_join.k = test_mt.k FORMAT TSV\"))\n     assert len(response) == 1\n     assert response[0] == [\"1\", \"1\"]\n+\n+\n+def test_get_keys(started_cluster):\n+    \"\"\"\n+    Checks that ClickHouse reads by key instead of full scan if possible.\n+    \"\"\"\n+    address = get_address_for_ch()\n+\n+    # clean all\n+    drop_table(\"test_get_keys\")\n+\n+    # create table\n+    node.query(f\"\"\"\n+               CREATE TABLE test_get_keys(k Int) Engine=Redis('{address}', 2, 'clickhouse') PRIMARY KEY (k);\n+               INSERT INTO test_get_keys VALUES (1), (2), (3);\n+               \"\"\")\n+\n+    def check_query(query, read_type, keys_count, rows_read):\n+        plan = node.query(f'EXPLAIN actions=1 {query}')\n+        assert 'ReadFromRedis' in plan\n+        assert f'ReadType: {read_type}' in plan\n+        if read_type == 'GetKeys':\n+            assert f'Keys: {keys_count}' in plan\n+\n+        res = node.query(f'{query} FORMAT JSON')\n+        assert json.loads(res)['statistics']['rows_read'] == rows_read, res\n+\n+    check_query(\"SELECT * FROM test_get_keys\", \"FullScan\", 0, 3)\n+    check_query(\"SELECT * FROM test_get_keys WHERE k = 1\", \"GetKeys\", 1, 1)\n+    check_query(\"SELECT * FROM test_get_keys WHERE k in (3, 5)\", \"GetKeys\", 2, 1)\ndiff --git a/tests/queries/0_stateless/02375_rocksdb_with_filters.reference b/tests/queries/0_stateless/02375_rocksdb_with_filters.reference\nindex 2d6a0e0ad78a..77201bc4c43d 100644\n--- a/tests/queries/0_stateless/02375_rocksdb_with_filters.reference\n+++ b/tests/queries/0_stateless/02375_rocksdb_with_filters.reference\n@@ -1,5 +1,10 @@\n+    ReadFromEmbeddedRocksDB\n+    ReadType: FullScan\n 1\n \"rows_read\":1,\n+    ReadFromEmbeddedRocksDB\n+    ReadType: GetKeys\n+    Keys: 1\n 2\n \"rows_read\":2,\n 1\ndiff --git a/tests/queries/0_stateless/02375_rocksdb_with_filters.sh b/tests/queries/0_stateless/02375_rocksdb_with_filters.sh\nindex 1f9922100cb0..3a03fb89accb 100755\n--- a/tests/queries/0_stateless/02375_rocksdb_with_filters.sh\n+++ b/tests/queries/0_stateless/02375_rocksdb_with_filters.sh\n@@ -10,8 +10,11 @@ $CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS rocksdb_with_filter;\"\n $CLICKHOUSE_CLIENT --query=\"CREATE TABLE rocksdb_with_filter (key String, value String) ENGINE=EmbeddedRocksDB PRIMARY KEY key;\"\n $CLICKHOUSE_CLIENT --query=\"INSERT INTO rocksdb_with_filter (*) SELECT n.number, n.number*10 FROM numbers(10000) n;\"\n \n+$CLICKHOUSE_CLIENT --query \"EXPLAIN actions=1 SELECT value FROM rocksdb_with_filter LIMIT 1\" | grep -A 2 \"ReadFromEmbeddedRocksDB\"\n+\n $CLICKHOUSE_CLIENT --query \"SELECT count() FROM rocksdb_with_filter WHERE key = '5000'\"\n $CLICKHOUSE_CLIENT --query \"SELECT value FROM rocksdb_with_filter WHERE key = '5000' FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+$CLICKHOUSE_CLIENT --query \"EXPLAIN actions=1 SELECT value FROM rocksdb_with_filter WHERE key = '5000'\" | grep -A 3 \"ReadFromEmbeddedRocksDB\"\n \n $CLICKHOUSE_CLIENT --query \"SELECT count() FROM rocksdb_with_filter WHERE key = '5000' OR key = '6000'\"\n $CLICKHOUSE_CLIENT --query \"SELECT value FROM rocksdb_with_filter WHERE key = '5000' OR key = '6000' FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\ndiff --git a/tests/queries/0_stateless/03541_keeper_map_filter_keys.reference b/tests/queries/0_stateless/03541_keeper_map_filter_keys.reference\nnew file mode 100644\nindex 000000000000..12a3bbcf41a9\n--- /dev/null\n+++ b/tests/queries/0_stateless/03541_keeper_map_filter_keys.reference\n@@ -0,0 +1,24 @@\n+    ReadFromKeeperMap\n+    ReadType: FullScan\n+1\n+\"rows_read\":1,\n+    ReadFromKeeperMap\n+    ReadType: GetKeys\n+    Keys: 1\n+2\n+\"rows_read\":2,\n+1\n+\"rows_read\":1,\n+2\n+\"rows_read\":2,\n+1\n+\"rows_read\":1,\n+    ReadFromKeeperMap\n+    ReadType: GetKeys\n+    Keys: 1\n+2\n+\"rows_read\":2,\n+1\n+\"rows_read\":1,\n+2\n+\"rows_read\":2,\ndiff --git a/tests/queries/0_stateless/03541_keeper_map_filter_keys.sh b/tests/queries/0_stateless/03541_keeper_map_filter_keys.sh\nnew file mode 100755\nindex 000000000000..b862bda95799\n--- /dev/null\n+++ b/tests/queries/0_stateless/03541_keeper_map_filter_keys.sh\n@@ -0,0 +1,48 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest, no-ordinary-database\n+# Tag no-ordinary-database: KeeperMap doesn't support Ordinary database\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS keeper_map_with_filter;\"\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE keeper_map_with_filter (key String, value String) ENGINE=KeeperMap(concat(currentDatabase(), '_simple')) PRIMARY KEY key;\"\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO keeper_map_with_filter (*) SELECT n.number, n.number*10 FROM numbers(10) n;\"\n+\n+$CLICKHOUSE_CLIENT --query \"EXPLAIN actions=1 SELECT value FROM keeper_map_with_filter LIMIT 1\" | grep -A 2 \"ReadFromKeeperMap\"\n+\n+$CLICKHOUSE_CLIENT --query \"SELECT count() FROM keeper_map_with_filter WHERE key = '5'\"\n+$CLICKHOUSE_CLIENT --query \"SELECT value FROM keeper_map_with_filter WHERE key = '5' FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+$CLICKHOUSE_CLIENT --query \"EXPLAIN actions=1 SELECT value FROM keeper_map_with_filter WHERE key = '5'\" | grep -A 3 \"ReadFromKeeperMap\"\n+\n+$CLICKHOUSE_CLIENT --query \"SELECT count() FROM keeper_map_with_filter WHERE key = '5' OR key = '6'\"\n+$CLICKHOUSE_CLIENT --query \"SELECT value FROM keeper_map_with_filter WHERE key = '5' OR key = '6' FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+\n+$CLICKHOUSE_CLIENT \"--param_key=5\" --query \"SELECT count() FROM keeper_map_with_filter WHERE key = {key:String}\"\n+$CLICKHOUSE_CLIENT \"--param_key=5\" --query \"SELECT value FROM keeper_map_with_filter WHERE key = {key:String} FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+\n+$CLICKHOUSE_CLIENT --query \"SELECT count() FROM keeper_map_with_filter WHERE key IN ('5', '6')\"\n+$CLICKHOUSE_CLIENT --query \"SELECT value FROM keeper_map_with_filter WHERE key IN ('5', '6') FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE keeper_map_with_filter;\"\n+\n+# Same test, but with complex key\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS keeper_map_with_filter_and_complex_key;\"\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE keeper_map_with_filter_and_complex_key (key String, value String) ENGINE=KeeperMap(concat(currentDatabase(), '_complex')) PRIMARY KEY hex(toFloat32(key));\"\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO keeper_map_with_filter_and_complex_key (*) SELECT n.number, n.number*10 FROM numbers(10) n;\"\n+\n+$CLICKHOUSE_CLIENT --query \"SELECT count() FROM keeper_map_with_filter_and_complex_key WHERE key = '5'\"\n+$CLICKHOUSE_CLIENT --query \"SELECT value FROM keeper_map_with_filter_and_complex_key WHERE key = '5' FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+$CLICKHOUSE_CLIENT --query \"EXPLAIN actions=1 SELECT value FROM keeper_map_with_filter_and_complex_key WHERE key = '5'\" | grep -A 3 \"ReadFromKeeperMap\"\n+\n+$CLICKHOUSE_CLIENT --query \"SELECT count() FROM keeper_map_with_filter_and_complex_key WHERE key = '5' OR key = '6'\"\n+$CLICKHOUSE_CLIENT --query \"SELECT value FROM keeper_map_with_filter_and_complex_key WHERE key = '5' OR key = '6' FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+\n+$CLICKHOUSE_CLIENT \"--param_key=5\" --query \"SELECT count() FROM keeper_map_with_filter_and_complex_key WHERE key = {key:String}\"\n+$CLICKHOUSE_CLIENT \"--param_key=5\" --query \"SELECT value FROM keeper_map_with_filter_and_complex_key WHERE key = {key:String} FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+\n+$CLICKHOUSE_CLIENT --query \"SELECT count() FROM keeper_map_with_filter_and_complex_key WHERE key IN ('5', '6')\"\n+$CLICKHOUSE_CLIENT --query \"SELECT value FROM keeper_map_with_filter_and_complex_key WHERE key IN ('5', '6') FORMAT JSON\" | grep \"rows_read\" | tr -d \"[:blank:]\"\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE keeper_map_with_filter_and_complex_key;\"\n",
  "problem_statement": "Redis Storage engine support for point queries\n### Company or project name\n\n_No response_\n\n### Describe the unexpected behaviour\n\nWe've identified a major performance regression when running point queries against Redis. Instead of performing a direct key lookup, the engine is now scanning all keys using a SCAN followed by an MGET, even for simple lookups by key like:\n\n`SELECT * FROM redis_table WHERE id = 'user:123';`\nshould be a performant point query\n\n\n### Which ClickHouse versions are affected?\n\nThis was introduced between 23 and 25, not clear when this was introduced, but the recent latest stable versions have the issue.\n\n### How to reproduce\n\nexample:\n```\n-- Create a Redis-backed table\nCREATE TABLE redis_table\n(\n    id String,\n    value String\n)\nENGINE = Redis('localhost:6379')\nprimary key id;\n\n-- Insert test data into Redis separately\n-- For example, using redis-cli:\n-- SET user:123 '{\"value\":\"John\"}'\n-- SET user:456 '{\"value\":\"Alice\"}'\n\n-- Run a point query\nSELECT * FROM redis_table WHERE id = 'user:123';\n\n```\nObserve that the logs in redis show a SCAN of all keys followed by an MGET and the query is unacceptably slow for this usecase\n\n### Expected behavior\n\n`SELECT * FROM redis_table WHERE id = 'user:123';` should generate a `GET` on redis\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\nThis regression turns what should be a constant-time key lookup into a slow linear scan, I am thinking it likely stems from changes made in recent ClickHouse versions that broke this feature for the Redis engine. To clarify,  we have been utilizing this integration in production in the past 1-2 years with high performance on clickhouse 23.8.4.69\n",
  "hints_text": "I found it too. It seems that using the new analyzer, it performs scans every time.\nTry to use `SET enable_analyzer=0` and check the performance.\n@GrigoryPervakov Thank you for the reply, I tried this setting on the latest version of clickhouse and got a scan still for a simple point query",
  "created_at": "2025-06-13T18:16:05Z"
}