{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 40148,
  "instance_id": "ClickHouse__ClickHouse-40148",
  "issue_numbers": [
    "40105"
  ],
  "base_commit": "e703dd72bf4afe2d0ce7162d1a563e2399dda64b",
  "patch": "diff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp\nindex f8d8deab08b2..f65711a8521a 100644\n--- a/src/Common/ErrorCodes.cpp\n+++ b/src/Common/ErrorCodes.cpp\n@@ -635,6 +635,7 @@\n     M(664, ACCESS_STORAGE_DOESNT_ALLOW_BACKUP) \\\n     M(665, CANNOT_CONNECT_NATS) \\\n     M(666, CANNOT_USE_CACHE) \\\n+    M(667, NOT_INITIALIZED) \\\n     \\\n     M(999, KEEPER_EXCEPTION) \\\n     M(1000, POCO_EXCEPTION) \\\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 89081fe924f0..07659b1c9dc3 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -95,6 +95,7 @@ struct Settings;\n     M(Seconds, replicated_fetches_http_receive_timeout, 0, \"HTTP receive timeout for fetch part requests. Inherited from default profile `http_receive_timeout` if not set explicitly.\", 0) \\\n     M(Bool, replicated_can_become_leader, true, \"If true, Replicated tables replicas on this node will try to acquire leadership.\", 0) \\\n     M(Seconds, zookeeper_session_expiration_check_period, 60, \"ZooKeeper session expiration check period, in seconds.\", 0) \\\n+    M(Seconds, initialization_retry_period, 60, \"Retry period for table initialization, in seconds.\", 0) \\\n     M(Bool, detach_old_local_parts_when_cloning_replica, true, \"Do not remove old local parts when repairing lost replica.\", 0) \\\n     M(Bool, detach_not_byte_identical_parts, false, \"Do not remove non byte-idential parts for ReplicatedMergeTree, instead detach them (maybe useful for further analysis).\", 0) \\\n     M(UInt64, max_replicated_fetches_network_bandwidth, 0, \"The maximum speed of data exchange over the network in bytes per second for replicated fetches. Zero means unlimited.\", 0) \\\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\nnew file mode 100644\nindex 000000000000..566d5dc3fa3d\n--- /dev/null\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\n@@ -0,0 +1,167 @@\n+#include <Storages/MergeTree/ReplicatedMergeTreeAttachThread.h>\n+#include <Storages/StorageReplicatedMergeTree.h>\n+#include <Common/ZooKeeper/IKeeper.h>\n+\n+namespace DB\n+{\n+\n+ReplicatedMergeTreeAttachThread::ReplicatedMergeTreeAttachThread(StorageReplicatedMergeTree & storage_)\n+    : storage(storage_)\n+    , log_name(storage.getStorageID().getFullTableName() + \" (ReplicatedMergeTreeAttachThread)\")\n+    , log(&Poco::Logger::get(log_name))\n+{\n+    task = storage.getContext()->getSchedulePool().createTask(log_name, [this] { run(); });\n+    const auto storage_settings = storage.getSettings();\n+    retry_period = storage_settings->initialization_retry_period.totalSeconds();\n+}\n+\n+ReplicatedMergeTreeAttachThread::~ReplicatedMergeTreeAttachThread()\n+{\n+    shutdown();\n+}\n+\n+void ReplicatedMergeTreeAttachThread::shutdown()\n+{\n+    if (!shutdown_called.exchange(true))\n+    {\n+        task->deactivate();\n+        LOG_INFO(log, \"Attach thread finished\");\n+    }\n+}\n+\n+void ReplicatedMergeTreeAttachThread::run()\n+{\n+    bool needs_retry{false};\n+    try\n+    {\n+        // we delay the first reconnect if the storage failed to connect to ZK initially\n+        if (!first_try_done && !storage.current_zookeeper)\n+        {\n+            needs_retry = true;\n+        }\n+        else\n+        {\n+            runImpl();\n+            finalizeInitialization();\n+        }\n+    }\n+    catch (const Exception & e)\n+    {\n+        if (const auto * coordination_exception = dynamic_cast<const Coordination::Exception *>(&e))\n+            needs_retry = Coordination::isHardwareError(coordination_exception->code);\n+\n+        if (needs_retry)\n+        {\n+            LOG_ERROR(log, \"Initialization failed. Error: {}\", e.message());\n+        }\n+        else\n+        {\n+            LOG_ERROR(log, \"Initialization failed, table will remain readonly. Error: {}\", e.message());\n+            storage.initialization_done = true;\n+        }\n+    }\n+\n+    if (!first_try_done.exchange(true))\n+        first_try_done.notify_one();\n+\n+    if (shutdown_called)\n+    {\n+        LOG_WARNING(log, \"Shutdown called, cancelling initialization\");\n+        return;\n+    }\n+\n+    if (needs_retry)\n+    {\n+        LOG_INFO(log, \"Will retry initialization in {}s\", retry_period);\n+        task->scheduleAfter(retry_period * 1000);\n+    }\n+}\n+\n+void ReplicatedMergeTreeAttachThread::runImpl()\n+{\n+    storage.setZooKeeper();\n+\n+    auto zookeeper = storage.getZooKeeper();\n+    const auto & zookeeper_path = storage.zookeeper_path;\n+    bool metadata_exists = zookeeper->exists(zookeeper_path + \"/metadata\");\n+    if (!metadata_exists)\n+    {\n+        LOG_WARNING(log, \"No metadata in ZooKeeper for {}: table will stay in readonly mode.\", zookeeper_path);\n+        storage.has_metadata_in_zookeeper = false;\n+        return;\n+    }\n+\n+    auto metadata_snapshot = storage.getInMemoryMetadataPtr();\n+\n+    const auto & replica_path = storage.replica_path;\n+    /// May it be ZK lost not the whole root, so the upper check passed, but only the /replicas/replica\n+    /// folder.\n+    bool replica_path_exists = zookeeper->exists(replica_path);\n+    if (!replica_path_exists)\n+    {\n+        LOG_WARNING(log, \"No metadata in ZooKeeper for {}: table will stay in readonly mode\", replica_path);\n+        storage.has_metadata_in_zookeeper = false;\n+        return;\n+    }\n+\n+    storage.has_metadata_in_zookeeper = true;\n+\n+    /// In old tables this node may missing or be empty\n+    String replica_metadata;\n+    const bool replica_metadata_exists = zookeeper->tryGet(replica_path + \"/metadata\", replica_metadata);\n+\n+    if (!replica_metadata_exists || replica_metadata.empty())\n+    {\n+        /// We have to check shared node granularity before we create ours.\n+        storage.other_replicas_fixed_granularity = storage.checkFixedGranularityInZookeeper();\n+\n+        ReplicatedMergeTreeTableMetadata current_metadata(storage, metadata_snapshot);\n+\n+        zookeeper->createOrUpdate(replica_path + \"/metadata\", current_metadata.toString(), zkutil::CreateMode::Persistent);\n+    }\n+\n+    storage.checkTableStructure(replica_path, metadata_snapshot);\n+    storage.checkParts(skip_sanity_checks);\n+\n+    if (zookeeper->exists(replica_path + \"/metadata_version\"))\n+    {\n+        storage.metadata_version = parse<int>(zookeeper->get(replica_path + \"/metadata_version\"));\n+    }\n+    else\n+    {\n+        /// This replica was created with old clickhouse version, so we have\n+        /// to take version of global node. If somebody will alter our\n+        /// table, then we will fill /metadata_version node in zookeeper.\n+        /// Otherwise on the next restart we can again use version from\n+        /// shared metadata node because it was not changed.\n+        Coordination::Stat metadata_stat;\n+        zookeeper->get(zookeeper_path + \"/metadata\", &metadata_stat);\n+        storage.metadata_version = metadata_stat.version;\n+    }\n+\n+    /// Temporary directories contain uninitialized results of Merges or Fetches (after forced restart),\n+    /// don't allow to reinitialize them, delete each of them immediately.\n+    storage.clearOldTemporaryDirectories(0, {\"tmp_\", \"delete_tmp_\", \"tmp-fetch_\"});\n+    storage.clearOldWriteAheadLogs();\n+    if (storage.getSettings()->merge_tree_enable_clear_old_broken_detached)\n+        storage.clearOldBrokenPartsFromDetachedDirecory();\n+\n+    storage.createNewZooKeeperNodes();\n+    storage.syncPinnedPartUUIDs();\n+\n+    storage.createTableSharedID();\n+};\n+\n+void ReplicatedMergeTreeAttachThread::finalizeInitialization() TSA_NO_THREAD_SAFETY_ANALYSIS\n+{\n+    storage.startupImpl();\n+    storage.initialization_done = true;\n+    LOG_INFO(log, \"Table is initialized\");\n+}\n+\n+void ReplicatedMergeTreeAttachThread::setSkipSanityChecks(bool skip_sanity_checks_)\n+{\n+    skip_sanity_checks = skip_sanity_checks_;\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h\nnew file mode 100644\nindex 000000000000..4697077bec5b\n--- /dev/null\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h\n@@ -0,0 +1,52 @@\n+#pragma once\n+\n+#include <thread>\n+#include <Core/BackgroundSchedulePool.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n+#include <Common/logger_useful.h>\n+\n+namespace DB\n+{\n+\n+class StorageReplicatedMergeTree;\n+\n+// Attach table to the existing data.\n+// Initialize the table by creating all the necessary nodes and do the required checks.\n+// Initialization is repeated if an operation fails because of a ZK request or connection loss.\n+class ReplicatedMergeTreeAttachThread\n+{\n+public:\n+    explicit ReplicatedMergeTreeAttachThread(StorageReplicatedMergeTree & storage_);\n+\n+    ~ReplicatedMergeTreeAttachThread();\n+\n+    void start() { task->activateAndSchedule(); }\n+\n+    void shutdown();\n+\n+    void waitFirstTry() { first_try_done.wait(false); }\n+\n+    void setSkipSanityChecks(bool skip_sanity_checks_);\n+\n+private:\n+    StorageReplicatedMergeTree & storage;\n+    BackgroundSchedulePool::TaskHolder task;\n+\n+    std::string log_name;\n+    Poco::Logger * log;\n+\n+    std::atomic<bool> first_try_done{false};\n+\n+    std::atomic<bool> shutdown_called{false};\n+\n+    UInt64 retry_period;\n+\n+    bool skip_sanity_checks{false};\n+\n+    void run();\n+    void runImpl();\n+\n+    void finalizeInitialization();\n+};\n+\n+}\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\nindex 11f668bafbe1..9d95189b611e 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n@@ -103,7 +103,6 @@ void ReplicatedMergeTreeRestartingThread::run()\n }\n \n bool ReplicatedMergeTreeRestartingThread::runImpl()\n-\n {\n     if (!storage.is_readonly && !storage.getZooKeeper()->expired())\n         return true;\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 02b3422f7d20..863b58963f73 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -24,6 +24,7 @@\n #include <Storages/MergeTree/MergeTreeBackgroundExecutor.h>\n #include <Storages/MergeTree/MergedBlockOutputStream.h>\n #include <Storages/MergeTree/PinnedPartUUIDs.h>\n+#include <Storages/MergeTree/ReplicatedMergeTreeAttachThread.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeTableMetadata.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeSink.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeQuorumEntry.h>\n@@ -159,6 +160,7 @@ namespace ErrorCodes\n     extern const int BAD_ARGUMENTS;\n     extern const int CONCURRENT_ACCESS_NOT_SUPPORTED;\n     extern const int CHECKSUM_DOESNT_MATCH;\n+    extern const int NOT_INITIALIZED;\n }\n \n namespace ActionLocks\n@@ -296,7 +298,8 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     mutations_finalizing_task = getContext()->getSchedulePool().createTask(\n         getStorageID().getFullTableName() + \" (StorageReplicatedMergeTree::mutationsFinalizingTask)\", [this] { mutationsFinalizingTask(); });\n \n-    if (getContext()->hasZooKeeper() || getContext()->hasAuxiliaryZooKeeper(zookeeper_name))\n+    bool has_zookeeper = getContext()->hasZooKeeper() || getContext()->hasAuxiliaryZooKeeper(zookeeper_name);\n+    if (has_zookeeper)\n     {\n         /// It's possible for getZooKeeper() to timeout if  zookeeper host(s) can't\n         /// be reached. In such cases Poco::Exception is thrown after a connection\n@@ -325,8 +328,14 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n         catch (...)\n         {\n             if (!attach)\n+            {\n                 dropIfEmpty();\n-            throw;\n+                throw;\n+            }\n+            else\n+            {\n+                current_zookeeper = nullptr;\n+            }\n         }\n     }\n \n@@ -359,124 +368,77 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n             throw Exception(\"Can't create replicated table without ZooKeeper\", ErrorCodes::NO_ZOOKEEPER);\n         }\n \n-        /// Do not activate the replica. It will be readonly.\n-        LOG_ERROR(log, \"No ZooKeeper: table will be in readonly mode.\");\n         has_metadata_in_zookeeper = std::nullopt;\n-        return;\n+\n+        if (!has_zookeeper)\n+        {\n+            /// Do not activate the replica. It will be readonly.\n+            LOG_ERROR(log, \"No ZooKeeper defined: table will stay in readonly mode.\");\n+            return;\n+        }\n     }\n \n-    if (attach && !current_zookeeper->exists(zookeeper_path + \"/metadata\"))\n+    if (attach)\n     {\n-        LOG_WARNING(log, \"No metadata in ZooKeeper for {}: table will be in readonly mode.\", zookeeper_path);\n-        has_metadata_in_zookeeper = false;\n+        LOG_INFO(log, \"Table will be in readonly mode until initialization is finished\");\n+        attach_thread.emplace(*this);\n+        attach_thread->setSkipSanityChecks(skip_sanity_checks);\n         return;\n     }\n \n     auto metadata_snapshot = getInMemoryMetadataPtr();\n \n-    /// May it be ZK lost not the whole root, so the upper check passed, but only the /replicas/replica\n-    /// folder.\n-    if (attach && !current_zookeeper->exists(replica_path))\n-    {\n-        LOG_WARNING(log, \"No metadata in ZooKeeper for {}: table will be in readonly mode\", replica_path);\n-        has_metadata_in_zookeeper = false;\n-        return;\n-    }\n-\n     has_metadata_in_zookeeper = true;\n \n-    if (!attach)\n+    if (!getDataPartsForInternalUsage().empty())\n+        throw Exception(\"Data directory for table already contains data parts\"\n+            \" - probably it was unclean DROP table or manual intervention.\"\n+            \" You must either clear directory by hand or use ATTACH TABLE\"\n+            \" instead of CREATE TABLE if you need to use that parts.\", ErrorCodes::INCORRECT_DATA);\n+\n+    try\n     {\n-        if (!getDataPartsForInternalUsage().empty())\n-            throw Exception(\"Data directory for table already contains data parts\"\n-                \" - probably it was unclean DROP table or manual intervention.\"\n-                \" You must either clear directory by hand or use ATTACH TABLE\"\n-                \" instead of CREATE TABLE if you need to use that parts.\", ErrorCodes::INCORRECT_DATA);\n+        bool is_first_replica = createTableIfNotExists(metadata_snapshot);\n \n         try\n         {\n-            bool is_first_replica = createTableIfNotExists(metadata_snapshot);\n+            /// NOTE If it's the first replica, these requests to ZooKeeper look redundant, we already know everything.\n \n-            try\n-            {\n-                /// NOTE If it's the first replica, these requests to ZooKeeper look redundant, we already know everything.\n-\n-                /// We have to check granularity on other replicas. If it's fixed we\n-                /// must create our new replica with fixed granularity and store this\n-                /// information in /replica/metadata.\n-                other_replicas_fixed_granularity = checkFixedGranularityInZookeeper();\n-\n-                checkTableStructure(zookeeper_path, metadata_snapshot);\n-\n-                Coordination::Stat metadata_stat;\n-                current_zookeeper->get(zookeeper_path + \"/metadata\", &metadata_stat);\n-                metadata_version = metadata_stat.version;\n-            }\n-            catch (Coordination::Exception & e)\n-            {\n-                if (!is_first_replica && e.code == Coordination::Error::ZNONODE)\n-                    throw Exception(\"Table \" + zookeeper_path + \" was suddenly removed.\", ErrorCodes::ALL_REPLICAS_LOST);\n-                else\n-                    throw;\n-            }\n-\n-            if (!is_first_replica)\n-                createReplica(metadata_snapshot);\n-        }\n-        catch (...)\n-        {\n-            /// If replica was not created, rollback creation of data directory.\n-            dropIfEmpty();\n-            throw;\n-        }\n-    }\n-    else\n-    {\n-        /// In old tables this node may missing or be empty\n-        String replica_metadata;\n-        const bool replica_metadata_exists = current_zookeeper->tryGet(replica_path + \"/metadata\", replica_metadata);\n-\n-        if (!replica_metadata_exists || replica_metadata.empty())\n-        {\n-            /// We have to check shared node granularity before we create ours.\n+            /// We have to check granularity on other replicas. If it's fixed we\n+            /// must create our new replica with fixed granularity and store this\n+            /// information in /replica/metadata.\n             other_replicas_fixed_granularity = checkFixedGranularityInZookeeper();\n \n-            ReplicatedMergeTreeTableMetadata current_metadata(*this, metadata_snapshot);\n-\n-            current_zookeeper->createOrUpdate(replica_path + \"/metadata\", current_metadata.toString(),\n-                zkutil::CreateMode::Persistent);\n-        }\n-\n-        checkTableStructure(replica_path, metadata_snapshot);\n-        checkParts(skip_sanity_checks);\n+            checkTableStructure(zookeeper_path, metadata_snapshot);\n \n-        if (current_zookeeper->exists(replica_path + \"/metadata_version\"))\n-        {\n-            metadata_version = parse<int>(current_zookeeper->get(replica_path + \"/metadata_version\"));\n-        }\n-        else\n-        {\n-            /// This replica was created with old clickhouse version, so we have\n-            /// to take version of global node. If somebody will alter our\n-            /// table, then we will fill /metadata_version node in zookeeper.\n-            /// Otherwise on the next restart we can again use version from\n-            /// shared metadata node because it was not changed.\n             Coordination::Stat metadata_stat;\n             current_zookeeper->get(zookeeper_path + \"/metadata\", &metadata_stat);\n             metadata_version = metadata_stat.version;\n         }\n-        /// Temporary directories contain uninitialized results of Merges or Fetches (after forced restart),\n-        /// don't allow to reinitialize them, delete each of them immediately.\n-        clearOldTemporaryDirectories(0, {\"tmp_\", \"delete_tmp_\", \"tmp-fetch_\"});\n-        clearOldWriteAheadLogs();\n-        if (getSettings()->merge_tree_enable_clear_old_broken_detached)\n-            clearOldBrokenPartsFromDetachedDirecory();\n+        catch (Coordination::Exception & e)\n+        {\n+            if (!is_first_replica && e.code == Coordination::Error::ZNONODE)\n+                throw Exception(\"Table \" + zookeeper_path + \" was suddenly removed.\", ErrorCodes::ALL_REPLICAS_LOST);\n+            else\n+                throw;\n+        }\n+\n+        if (!is_first_replica)\n+            createReplica(metadata_snapshot);\n+    }\n+    catch (...)\n+    {\n+        /// If replica was not created, rollback creation of data directory.\n+        dropIfEmpty();\n+        throw;\n     }\n \n     createNewZooKeeperNodes();\n     syncPinnedPartUUIDs();\n \n     createTableSharedID();\n+\n+    initialization_done = true;\n }\n \n \n@@ -874,7 +836,6 @@ void StorageReplicatedMergeTree::drop()\n         if (!zookeeper)\n             throw Exception(\"Can't drop readonly replicated table (need to drop data in ZooKeeper as well)\", ErrorCodes::TABLE_IS_READ_ONLY);\n \n-        shutdown();\n         dropReplica(zookeeper, zookeeper_path, replica_name, log, getSettings());\n     }\n \n@@ -3481,13 +3442,15 @@ void StorageReplicatedMergeTree::removePartAndEnqueueFetch(const String & part_n\n \n void StorageReplicatedMergeTree::startBeingLeader()\n {\n+    auto zookeeper = getZooKeeper();\n+\n     if (!getSettings()->replicated_can_become_leader)\n     {\n         LOG_INFO(log, \"Will not enter leader election because replicated_can_become_leader=0\");\n         return;\n     }\n \n-    zkutil::checkNoOldLeaders(log, *current_zookeeper, fs::path(zookeeper_path) / \"leader_election\");\n+    zkutil::checkNoOldLeaders(log, *zookeeper, fs::path(zookeeper_path) / \"leader_election\");\n \n     LOG_INFO(log, \"Became leader\");\n     is_leader = true;\n@@ -4186,8 +4149,19 @@ DataPartStoragePtr StorageReplicatedMergeTree::fetchExistsPart(\n     return part->data_part_storage;\n }\n \n-\n void StorageReplicatedMergeTree::startup()\n+{\n+    if (attach_thread)\n+    {\n+        attach_thread->start();\n+        attach_thread->waitFirstTry();\n+        return;\n+    }\n+\n+    startupImpl();\n+}\n+\n+void StorageReplicatedMergeTree::startupImpl()\n {\n     /// Do not start replication if ZooKeeper is not configured or there is no metadata in zookeeper\n     if (!has_metadata_in_zookeeper.has_value() || !*has_metadata_in_zookeeper)\n@@ -4195,6 +4169,7 @@ void StorageReplicatedMergeTree::startup()\n \n     try\n     {\n+        auto zookeeper = getZooKeeper();\n         InterserverIOEndpointPtr data_parts_exchange_ptr = std::make_shared<DataPartsExchange::Service>(*this);\n         [[maybe_unused]] auto prev_ptr = std::atomic_exchange(&data_parts_exchange_endpoint, data_parts_exchange_ptr);\n         assert(prev_ptr == nullptr);\n@@ -4254,6 +4229,8 @@ void StorageReplicatedMergeTree::shutdown()\n     mutations_finalizing_task->deactivate();\n     stopBeingLeader();\n \n+    if (attach_thread)\n+        attach_thread->shutdown();\n     restarting_thread.shutdown();\n     background_operations_assignee.finish();\n     part_moves_between_shards_orchestrator.shutdown();\n@@ -4993,9 +4970,14 @@ bool StorageReplicatedMergeTree::getFakePartCoveringAllPartsInPartition(const St\n void StorageReplicatedMergeTree::restoreMetadataInZooKeeper()\n {\n     LOG_INFO(log, \"Restoring replica metadata\");\n+\n+    if (!initialization_done)\n+        throw Exception(ErrorCodes::NOT_INITIALIZED, \"Table is not initialized yet\");\n+\n     if (!is_readonly)\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Replica must be readonly\");\n \n+\n     if (getZooKeeper()->exists(replica_path))\n         throw Exception(ErrorCodes::BAD_ARGUMENTS,\n                         \"Replica path is present at {} - nothing to restore. \"\n@@ -5047,7 +5029,7 @@ void StorageReplicatedMergeTree::restoreMetadataInZooKeeper()\n \n     LOG_INFO(log, \"Attached all partitions, starting table\");\n \n-    startup();\n+    startupImpl();\n }\n \n void StorageReplicatedMergeTree::dropPartNoWaitNoThrow(const String & part_name)\n@@ -7439,7 +7421,7 @@ void StorageReplicatedMergeTree::createTableSharedID()\n     if (table_shared_id != UUIDHelpers::Nil)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Table shared id already initialized\");\n \n-    zkutil::ZooKeeperPtr zookeeper = getZooKeeper();\n+    auto zookeeper = getZooKeeper();\n     String zookeeper_table_id_path = fs::path(zookeeper_path) / \"table_shared_id\";\n     String id;\n     if (!zookeeper->tryGet(zookeeper_table_id_path, id))\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex d4eb49eba0dd..24b4a4d5634b 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -13,6 +13,7 @@\n #include <Storages/MergeTree/ReplicatedMergeTreeQueue.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeRestartingThread.h>\n+#include <Storages/MergeTree/ReplicatedMergeTreeAttachThread.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeMergeStrategyPicker.h>\n #include <Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeTableMetadata.h>\n@@ -28,6 +29,7 @@\n #include <Common/randomSeed.h>\n #include <Common/ZooKeeper/ZooKeeper.h>\n #include <Common/Throttler.h>\n+#include <base/defines.h>\n #include <Core/BackgroundSchedulePool.h>\n #include <QueryPipeline/Pipe.h>\n #include <Storages/MergeTree/BackgroundJobsAssignee.h>\n@@ -339,6 +341,7 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     friend class ReplicatedMergeTreeCleanupThread;\n     friend class ReplicatedMergeTreeAlterThread;\n     friend class ReplicatedMergeTreeRestartingThread;\n+    friend class ReplicatedMergeTreeAttachThread;\n     friend class ReplicatedMergeTreeMergeStrategyPicker;\n     friend struct ReplicatedMergeTreeLogEntry;\n     friend class ScopedPartitionMergeLock;\n@@ -444,8 +447,13 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     /// A thread that processes reconnection to ZooKeeper when the session expires.\n     ReplicatedMergeTreeRestartingThread restarting_thread;\n \n+    /// A thread that attaches the table using ZooKeeper\n+    std::optional<ReplicatedMergeTreeAttachThread> attach_thread;\n+\n     PartMovesBetweenShardsOrchestrator part_moves_between_shards_orchestrator;\n \n+    std::atomic<bool> initialization_done{false};\n+\n     /// True if replica was created for existing table with fixed granularity\n     bool other_replicas_fixed_granularity = false;\n \n@@ -835,6 +843,8 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     /// Create ephemeral lock in zookeeper for part and disk which support zero copy replication.\n     /// If somebody already holding the lock -- return std::nullopt.\n     std::optional<ZeroCopyLock> tryCreateZeroCopyExclusiveLock(const String & part_name, const DiskPtr & disk) override;\n+\n+    void startupImpl();\n };\n \n String getPartNamePossiblyFake(MergeTreeDataFormatVersion format_version, const MergeTreePartInfo & part_info);\n",
  "test_patch": "diff --git a/tests/integration/test_replicated_database/configs/config.xml b/tests/integration/test_replicated_database/configs/config.xml\nindex 9d217943bd72..16cd942e975d 100644\n--- a/tests/integration/test_replicated_database/configs/config.xml\n+++ b/tests/integration/test_replicated_database/configs/config.xml\n@@ -1,3 +1,6 @@\n <clickhouse>\n     <database_atomic_delay_before_drop_table_sec>10</database_atomic_delay_before_drop_table_sec>\n+    <merge_tree>\n+        <initialization_retry_period>10</initialization_retry_period>\n+    </merge_tree>\n </clickhouse>\ndiff --git a/tests/integration/test_replicated_database/test.py b/tests/integration/test_replicated_database/test.py\nindex f716fac85082..0cf237d57f35 100644\n--- a/tests/integration/test_replicated_database/test.py\n+++ b/tests/integration/test_replicated_database/test.py\n@@ -788,23 +788,24 @@ def test_startup_without_zk(started_cluster):\n     main_node.query(\n         \"CREATE DATABASE startup ENGINE = Replicated('/clickhouse/databases/startup', 'shard1', 'replica1');\"\n     )\n-    # main_node.query(\"CREATE TABLE startup.rmt (n int) ENGINE=ReplicatedMergeTree order by n\")\n-    main_node.query(\"CREATE TABLE startup.rmt (n int) ENGINE=MergeTree order by n\")\n+    main_node.query(\n+        \"CREATE TABLE startup.rmt (n int) ENGINE=ReplicatedMergeTree order by n\"\n+    )\n+\n     main_node.query(\"INSERT INTO startup.rmt VALUES (42)\")\n     with PartitionManager() as pm:\n         pm.drop_instance_zk_connections(main_node)\n-        main_node.restart_clickhouse(stop_start_wait_sec=30)\n+        main_node.restart_clickhouse(stop_start_wait_sec=60)\n         assert main_node.query(\"SELECT (*,).1 FROM startup.rmt\") == \"42\\n\"\n \n-    for _ in range(10):\n-        try:\n-            main_node.query(\"CREATE TABLE startup.m (n int) ENGINE=Memory\")\n-            break\n-        except:\n-            time.sleep(1)\n+    # we need to wait until the table is not readonly\n+    main_node.query_with_retry(\"INSERT INTO startup.rmt VALUES(42)\")\n+\n+    main_node.query_with_retry(\"CREATE TABLE startup.m (n int) ENGINE=Memory\")\n \n     main_node.query(\"EXCHANGE TABLES startup.rmt AND startup.m\")\n     assert main_node.query(\"SELECT (*,).1 FROM startup.m\") == \"42\\n\"\n+\n     main_node.query(\"DROP DATABASE startup SYNC\")\n \n \n",
  "problem_statement": "ReplicatedMergeTree tables should be able to start up if ZooKeeper is configured but unavailable\n**Describe the solution you'd like**\r\n\r\nWe already can run clickhouse-server without configured ZooKeeper (the tables will start in read-only mode)\r\nand with configured ZooKeeper, if it starts to be inaccessible at runtime (the tables will be read-only until the session is re-established).\r\n\r\nLet's also make it possible to start up when ZooKeeper is unavailable.\r\n\r\n**Additional context**\r\n\r\nIf that will be very hard to implement, we can consider other options:\r\n- lazy attaching of the tables after sever start-up;\r\n- a maintenance mode when the tables are not attached at start-up and require manual ATTACH queries.\n",
  "hints_text": "The biggest problem is the checks being done during the startup.\r\n\r\nWhat if it fails in the background while we try to attach a table (a check that notices some inconsistencies with the ZK data)?\r\nIs it okay to leave it just as readonly?\n> What if it fails in the background while we try to attach a table (a check that notices some inconsistencies with the ZK data)?\r\nIs it okay to leave it just as readonly?\r\n\r\nYes, I think it's totally fine to leave table readonly in this case (it's even ok to just keep retrying to initialize replication and infinitely print exceptions into logs)",
  "created_at": "2022-08-12T09:37:14Z",
  "modified_files": [
    "src/Common/ErrorCodes.cpp",
    "src/Storages/MergeTree/MergeTreeSettings.h",
    "b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp",
    "b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h",
    "src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "tests/integration/test_replicated_database/configs/config.xml",
    "tests/integration/test_replicated_database/test.py"
  ]
}