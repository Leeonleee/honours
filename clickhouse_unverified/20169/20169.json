{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 20169,
  "instance_id": "ClickHouse__ClickHouse-20169",
  "issue_numbers": [
    "13052"
  ],
  "base_commit": "be831d09f77a63bb559b8ecbb67495ef4d4135b9",
  "patch": "diff --git a/src/Core/ExternalTable.cpp b/src/Core/ExternalTable.cpp\nindex 722bc5705c3c..767ed9599509 100644\n--- a/src/Core/ExternalTable.cpp\n+++ b/src/Core/ExternalTable.cpp\n@@ -1,18 +1,24 @@\n #include <boost/program_options.hpp>\n+#include <DataStreams/IBlockOutputStream.h>\n #include <DataStreams/AsynchronousBlockInputStream.h>\n #include <DataTypes/DataTypeFactory.h>\n+#include <Storages/IStorage.h>\n+#include <Storages/ColumnsDescription.h>\n+#include <Storages/ConstraintsDescription.h>\n #include <Interpreters/Context.h>\n-#include <IO/copyData.h>\n+#include <Interpreters/DatabaseCatalog.h>\n #include <IO/ReadBufferFromIStream.h>\n #include <IO/ReadBufferFromFile.h>\n #include <IO/LimitReadBuffer.h>\n-#include <Storages/StorageMemory.h>\n-#include <Processors/Sources/SourceFromInputStream.h>\n+\n #include <Processors/Pipe.h>\n #include <Processors/Sources/SinkToOutputStream.h>\n #include <Processors/Executors/PipelineExecutor.h>\n+#include <Processors/Sources/SourceFromInputStream.h>\n+\n #include <Core/ExternalTable.h>\n #include <Poco/Net/MessageHeader.h>\n+#include <Formats/FormatFactory.h>\n #include <common/find_symbols.h>\n \n \n@@ -39,7 +45,7 @@ ExternalTableDataPtr BaseExternalTable::getData(const Context & context)\n     return data;\n }\n \n-void BaseExternalTable::clean()\n+void BaseExternalTable::clear()\n {\n     name.clear();\n     file.clear();\n@@ -49,17 +55,6 @@ void BaseExternalTable::clean()\n     read_buffer.reset();\n }\n \n-/// Function for debugging information output\n-void BaseExternalTable::write()\n-{\n-    std::cerr << \"file \" << file << std::endl;\n-    std::cerr << \"name \" << name << std::endl;\n-    std::cerr << \"format \" << format << std::endl;\n-    std::cerr << \"structure: \\n\";\n-    for (const auto & elem : structure)\n-        std::cerr << '\\t' << elem.first << ' ' << elem.second << std::endl;\n-}\n-\n void BaseExternalTable::parseStructureFromStructureField(const std::string & argument)\n {\n     std::vector<std::string> vals;\n@@ -182,7 +177,7 @@ void ExternalTablesHandler::handlePart(const Poco::Net::MessageHeader & header,\n     executor->execute(/*num_threads = */ 1);\n \n     /// We are ready to receive the next file, for this we clear all the information received\n-    clean();\n+    clear();\n }\n \n }\ndiff --git a/src/Core/ExternalTable.h b/src/Core/ExternalTable.h\nindex f26af1cc6ca4..0d8e0aaf8acd 100644\n--- a/src/Core/ExternalTable.h\n+++ b/src/Core/ExternalTable.h\n@@ -61,10 +61,7 @@ class BaseExternalTable\n \n protected:\n     /// Clear all accumulated information\n-    void clean();\n-\n-    /// Function for debugging information output\n-    void write();\n+    void clear();\n \n     /// Construct the `structure` vector from the text field `structure`\n     virtual void parseStructureFromStructureField(const std::string & argument);\ndiff --git a/src/Formats/FormatFactory.cpp b/src/Formats/FormatFactory.cpp\nindex 86cf12fbf68a..f7f32cf9b6f5 100644\n--- a/src/Formats/FormatFactory.cpp\n+++ b/src/Formats/FormatFactory.cpp\n@@ -5,7 +5,6 @@\n #include <Interpreters/Context.h>\n #include <Core/Settings.h>\n #include <DataStreams/MaterializingBlockOutputStream.h>\n-#include <DataStreams/SquashingBlockOutputStream.h>\n #include <DataStreams/NativeBlockInputStream.h>\n #include <Formats/FormatSettings.h>\n #include <Processors/Formats/IRowInputFormat.h>\ndiff --git a/src/Functions/array/arrayDifference.cpp b/src/Functions/array/arrayDifference.cpp\nindex c02533c25648..2c71c58867f6 100644\n--- a/src/Functions/array/arrayDifference.cpp\n+++ b/src/Functions/array/arrayDifference.cpp\n@@ -83,9 +83,9 @@ struct ArrayDifferenceImpl\n         }\n         res_ptr = ColumnArray::create(std::move(res_nested), array.getOffsetsPtr());\n         return true;\n-\n     }\n \n+\n     static ColumnPtr execute(const ColumnArray & array, ColumnPtr mapped)\n     {\n         ColumnPtr res;\n@@ -107,7 +107,6 @@ struct ArrayDifferenceImpl\n         else\n             throw Exception(\"Unexpected column for arrayDifference: \" + mapped->getName(), ErrorCodes::ILLEGAL_COLUMN);\n     }\n-\n };\n \n struct NameArrayDifference { static constexpr auto name = \"arrayDifference\"; };\ndiff --git a/src/Interpreters/GlobalSubqueriesVisitor.h b/src/Interpreters/GlobalSubqueriesVisitor.h\nindex cde59d1e6c99..80d133ebea61 100644\n--- a/src/Interpreters/GlobalSubqueriesVisitor.h\n+++ b/src/Interpreters/GlobalSubqueriesVisitor.h\n@@ -12,7 +12,6 @@\n #include <Core/Block.h>\n #include <Core/NamesAndTypes.h>\n #include <Databases/IDatabase.h>\n-#include <Storages/StorageMemory.h>\n #include <IO/WriteHelpers.h>\n #include <Interpreters/InDepthNodeVisitor.h>\n #include <Interpreters/IdentifierSemantic.h>\ndiff --git a/src/Interpreters/InterpreterInsertQuery.cpp b/src/Interpreters/InterpreterInsertQuery.cpp\nindex 55c4d19206fb..67444d49f866 100644\n--- a/src/Interpreters/InterpreterInsertQuery.cpp\n+++ b/src/Interpreters/InterpreterInsertQuery.cpp\n@@ -166,7 +166,7 @@ BlockIO InterpreterInsertQuery::execute()\n     BlockIO res;\n \n     StoragePtr table = getTable(query);\n-    auto table_lock = table->lockForShare(context.getInitialQueryId(), context.getSettingsRef().lock_acquire_timeout);\n+    auto table_lock = table->lockForShare(context.getInitialQueryId(), settings.lock_acquire_timeout);\n     auto metadata_snapshot = table->getInMemoryMetadataPtr();\n \n     auto query_sample_block = getSampleBlock(query, table, metadata_snapshot);\n@@ -289,7 +289,7 @@ BlockIO InterpreterInsertQuery::execute()\n \n                 new_settings.max_threads = std::max<UInt64>(1, settings.max_insert_threads);\n \n-                if (settings.min_insert_block_size_rows)\n+                if (settings.min_insert_block_size_rows && table->prefersLargeBlocks())\n                     new_settings.max_block_size = settings.min_insert_block_size_rows;\n \n                 Context new_context = context;\n@@ -348,13 +348,15 @@ BlockIO InterpreterInsertQuery::execute()\n \n             /// Do not squash blocks if it is a sync INSERT into Distributed, since it lead to double bufferization on client and server side.\n             /// Client-side bufferization might cause excessive timeouts (especially in case of big blocks).\n-            if (!(context.getSettingsRef().insert_distributed_sync && table->isRemote()) && !no_squash && !query.watch)\n+            if (!(settings.insert_distributed_sync && table->isRemote()) && !no_squash && !query.watch)\n             {\n+                bool table_prefers_large_blocks = table->prefersLargeBlocks();\n+\n                 out = std::make_shared<SquashingBlockOutputStream>(\n                     out,\n                     out->getHeader(),\n-                    context.getSettingsRef().min_insert_block_size_rows,\n-                    context.getSettingsRef().min_insert_block_size_bytes);\n+                    table_prefers_large_blocks ? settings.min_insert_block_size_rows : settings.max_block_size,\n+                    table_prefers_large_blocks ? settings.min_insert_block_size_bytes : 0);\n             }\n \n             auto out_wrapper = std::make_shared<CountingBlockOutputStream>(out);\ndiff --git a/src/Server/HTTPHandler.cpp b/src/Server/HTTPHandler.cpp\nindex 5e0d1f0ac66a..eb4d6119c6fe 100644\n--- a/src/Server/HTTPHandler.cpp\n+++ b/src/Server/HTTPHandler.cpp\n@@ -800,7 +800,6 @@ bool DynamicQueryHandler::customizeQueryParam(Context & context, const std::stri\n \n std::string DynamicQueryHandler::getQuery(Poco::Net::HTTPServerRequest & request, HTMLForm & params, Context & context)\n {\n-\n     if (likely(!startsWith(request.getContentType(), \"multipart/form-data\")))\n     {\n         /// Part of the query can be passed in the 'query' parameter and the rest in the request body\ndiff --git a/src/Server/TCPHandler.cpp b/src/Server/TCPHandler.cpp\nindex fa213dcdc551..d66639ef1110 100644\n--- a/src/Server/TCPHandler.cpp\n+++ b/src/Server/TCPHandler.cpp\n@@ -22,7 +22,6 @@\n #include <Interpreters/TablesStatus.h>\n #include <Interpreters/InternalTextLogsQueue.h>\n #include <Interpreters/OpenTelemetrySpanLog.h>\n-#include <Storages/StorageMemory.h>\n #include <Storages/StorageReplicatedMergeTree.h>\n #include <Storages/MergeTree/MergeTreeDataPartUUID.h>\n #include <Core/ExternalTable.h>\n@@ -1181,33 +1180,44 @@ bool TCPHandler::receiveData(bool scalar)\n     if (block)\n     {\n         if (scalar)\n+        {\n+            /// Scalar value\n             query_context->addScalar(temporary_id.table_name, block);\n-        else\n+        }\n+        else if (!state.need_receive_data_for_insert && !state.need_receive_data_for_input)\n         {\n-            /// If there is an insert request, then the data should be written directly to `state.io.out`.\n-            /// Otherwise, we write the blocks in the temporary `external_table_name` table.\n-            if (!state.need_receive_data_for_insert && !state.need_receive_data_for_input)\n+            /// Data for external tables\n+\n+            auto resolved = query_context->tryResolveStorageID(temporary_id, Context::ResolveExternal);\n+            StoragePtr storage;\n+            /// If such a table does not exist, create it.\n+            if (resolved)\n             {\n-                auto resolved = query_context->tryResolveStorageID(temporary_id, Context::ResolveExternal);\n-                StoragePtr storage;\n-                /// If such a table does not exist, create it.\n-                if (resolved)\n-                    storage = DatabaseCatalog::instance().getTable(resolved, *query_context);\n-                else\n-                {\n-                    NamesAndTypesList columns = block.getNamesAndTypesList();\n-                    auto temporary_table = TemporaryTableHolder(*query_context, ColumnsDescription{columns}, {});\n-                    storage = temporary_table.getTable();\n-                    query_context->addExternalTable(temporary_id.table_name, std::move(temporary_table));\n-                }\n-                auto metadata_snapshot = storage->getInMemoryMetadataPtr();\n-                /// The data will be written directly to the table.\n-                state.io.out = storage->write(ASTPtr(), metadata_snapshot, *query_context);\n+                storage = DatabaseCatalog::instance().getTable(resolved, *query_context);\n             }\n-            if (state.need_receive_data_for_input)\n-                state.block_for_input = block;\n             else\n-                state.io.out->write(block);\n+            {\n+                NamesAndTypesList columns = block.getNamesAndTypesList();\n+                auto temporary_table = TemporaryTableHolder(*query_context, ColumnsDescription{columns}, {});\n+                storage = temporary_table.getTable();\n+                query_context->addExternalTable(temporary_id.table_name, std::move(temporary_table));\n+            }\n+            auto metadata_snapshot = storage->getInMemoryMetadataPtr();\n+            /// The data will be written directly to the table.\n+            auto temporary_table_out = storage->write(ASTPtr(), metadata_snapshot, *query_context);\n+            temporary_table_out->write(block);\n+            temporary_table_out->writeSuffix();\n+\n+        }\n+        else if (state.need_receive_data_for_input)\n+        {\n+            /// 'input' table function.\n+            state.block_for_input = block;\n+        }\n+        else\n+        {\n+            /// INSERT query.\n+            state.io.out->write(block);\n         }\n         return true;\n     }\ndiff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h\nindex 1c0149ac261d..651688f41bb9 100644\n--- a/src/Storages/IStorage.h\n+++ b/src/Storages/IStorage.h\n@@ -131,6 +131,10 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo\n     /// Returns true if the storage supports reading of subcolumns of complex types.\n     virtual bool supportsSubcolumns() const { return false; }\n \n+    /// Requires squashing small blocks to large for optimal storage.\n+    /// This is true for most storages that store data on disk.\n+    virtual bool prefersLargeBlocks() const { return true; }\n+\n \n     /// Optional size information of each physical column.\n     /// Currently it's only used by the MergeTree family for query optimizations.\ndiff --git a/src/Storages/StorageMemory.cpp b/src/Storages/StorageMemory.cpp\nindex 1474fbcee029..4530d93c274a 100644\n--- a/src/Storages/StorageMemory.cpp\n+++ b/src/Storages/StorageMemory.cpp\n@@ -104,33 +104,46 @@ class MemorySource : public SourceWithProgress\n class MemoryBlockOutputStream : public IBlockOutputStream\n {\n public:\n-    explicit MemoryBlockOutputStream(\n+    MemoryBlockOutputStream(\n         StorageMemory & storage_,\n         const StorageMetadataPtr & metadata_snapshot_)\n         : storage(storage_)\n         , metadata_snapshot(metadata_snapshot_)\n-    {}\n+    {\n+    }\n \n     Block getHeader() const override { return metadata_snapshot->getSampleBlock(); }\n \n     void write(const Block & block) override\n     {\n-        const auto size_bytes_diff = block.allocatedBytes();\n-        const auto size_rows_diff = block.rows();\n-\n         metadata_snapshot->check(block, true);\n-        {\n-            std::lock_guard lock(storage.mutex);\n-            auto new_data = std::make_unique<Blocks>(*(storage.data.get()));\n-            new_data->push_back(block);\n-            storage.data.set(std::move(new_data));\n+        new_blocks.emplace_back(block);\n+    }\n+\n+    void writeSuffix() override\n+    {\n+        size_t inserted_bytes = 0;\n+        size_t inserted_rows = 0;\n \n-            storage.total_size_bytes.fetch_add(size_bytes_diff, std::memory_order_relaxed);\n-            storage.total_size_rows.fetch_add(size_rows_diff, std::memory_order_relaxed);\n+        for (const auto & block : new_blocks)\n+        {\n+            inserted_bytes += block.allocatedBytes();\n+            inserted_rows += block.rows();\n         }\n \n+        std::lock_guard lock(storage.mutex);\n+\n+        auto new_data = std::make_unique<Blocks>(*(storage.data.get()));\n+        new_data->insert(new_data->end(), new_blocks.begin(), new_blocks.end());\n+\n+        storage.data.set(std::move(new_data));\n+        storage.total_size_bytes.fetch_add(inserted_bytes, std::memory_order_relaxed);\n+        storage.total_size_rows.fetch_add(inserted_rows, std::memory_order_relaxed);\n     }\n+\n private:\n+    Blocks new_blocks;\n+\n     StorageMemory & storage;\n     StorageMetadataPtr metadata_snapshot;\n };\ndiff --git a/src/Storages/StorageMemory.h b/src/Storages/StorageMemory.h\nindex 702cb265ea92..dc695427156b 100644\n--- a/src/Storages/StorageMemory.h\n+++ b/src/Storages/StorageMemory.h\n@@ -40,9 +40,11 @@ friend struct ext::shared_ptr_helper<StorageMemory>;\n         unsigned num_streams) override;\n \n     bool supportsParallelInsert() const override { return true; }\n-\n     bool supportsSubcolumns() const override { return true; }\n \n+    /// Smaller blocks (e.g. 64K rows) are better for CPU cache.\n+    bool prefersLargeBlocks() const override { return false; }\n+\n     BlockOutputStreamPtr write(const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, const Context & context) override;\n \n     void drop() override;\n",
  "test_patch": "diff --git a/tests/performance/decimal_aggregates.xml b/tests/performance/decimal_aggregates.xml\nindex 615c3201843a..f7bc2ac18681 100644\n--- a/tests/performance/decimal_aggregates.xml\n+++ b/tests/performance/decimal_aggregates.xml\n@@ -11,7 +11,7 @@\n     <query>SELECT min(d32), max(d32), argMin(x, d32), argMax(x, d32) FROM t</query>\n     <query>SELECT min(d64), max(d64), argMin(x, d64), argMax(x, d64) FROM t</query>\n     <query>SELECT min(d128), max(d128), argMin(x, d128), argMax(x, d128) FROM t</query>\n-    \n+\n     <query>SELECT avg(d32), sum(d32), sumWithOverflow(d32) FROM t</query>\n     <query>SELECT avg(d64), sum(d64), sumWithOverflow(d64) FROM t</query>\n     <query>SELECT avg(d128), sum(d128), sumWithOverflow(d128) FROM t</query>\n@@ -19,11 +19,11 @@\n     <query>SELECT uniq(d32), uniqCombined(d32), uniqExact(d32), uniqHLL12(d32) FROM     (SELECT * FROM t LIMIT 10000000)</query>\n     <query>SELECT uniq(d64), uniqCombined(d64), uniqExact(d64), uniqHLL12(d64) FROM     (SELECT * FROM t LIMIT 10000000)</query>\n     <query>SELECT uniq(d128), uniqCombined(d128), uniqExact(d128), uniqHLL12(d128) FROM (SELECT * FROM t LIMIT 1000000)</query>\n-    \n+\n     <query>SELECT median(d32), medianExact(d32), medianExactWeighted(d32, 2) FROM    (SELECT * FROM t LIMIT 10000000)</query>\n     <query>SELECT median(d64), medianExact(d64), medianExactWeighted(d64, 2) FROM    (SELECT * FROM t LIMIT 1000000)</query>\n     <query>SELECT median(d128), medianExact(d128), medianExactWeighted(d128, 2) FROM (SELECT * FROM t LIMIT 1000000)</query>\n-    \n+\n     <query>SELECT quantile(d32), quantileExact(d32), quantileExactWeighted(d32, 2) FROM    (SELECT * FROM t LIMIT 10000000)</query>\n     <query>SELECT quantile(d64), quantileExact(d64), quantileExactWeighted(d64, 2) FROM    (SELECT * FROM t LIMIT 1000000)</query>\n     <query>SELECT quantile(d128), quantileExact(d128), quantileExactWeighted(d128, 2) FROM (SELECT * FROM t LIMIT 1000000)</query>\n@@ -31,8 +31,8 @@\n     <query>SELECT quantilesExact(0.1, 0.9)(d32), quantilesExactWeighted(0.1, 0.9)(d32, 2) FROM   (SELECT * FROM t LIMIT 10000000)</query>\n     <query>SELECT quantilesExact(0.1, 0.9)(d64), quantilesExactWeighted(0.1, 0.9)(d64, 2) FROM   (SELECT * FROM t LIMIT 1000000)</query>\n     <query>SELECT quantilesExact(0.1, 0.9)(d128), quantilesExactWeighted(0.1, 0.9)(d128, 2) FROM (SELECT * FROM t LIMIT 1000000)</query>\n-    \n+\n     <query>SELECT varPop(d32), varSamp(d32), stddevPop(d32) FROM t</query>\n-    <query>SELECT varPop(d64), varSamp(d64), stddevPop(d64) FROM    (SELECT * FROM t LIMIT 1000000)</query>\n-    <query>SELECT varPop(d128), varSamp(d128), stddevPop(d128) FROM (SELECT * FROM t LIMIT 1000000)</query>\n+    <query>SELECT varPop(d64), varSamp(d64), stddevPop(d64) FROM    (SELECT * FROM t LIMIT 10000000)</query>\n+    <query>SELECT varPop(d128), varSamp(d128), stddevPop(d128) FROM (SELECT * FROM t LIMIT 10000000)</query>\n </test>\ndiff --git a/tests/performance/memory_cache_friendliness.xml b/tests/performance/memory_cache_friendliness.xml\nnew file mode 100644\nindex 000000000000..92b796615405\n--- /dev/null\n+++ b/tests/performance/memory_cache_friendliness.xml\n@@ -0,0 +1,8 @@\n+<test>\n+    <create_query>CREATE TABLE test_memory (x UInt64) ENGINE Memory</create_query>\n+    <fill_query>INSERT INTO test_memory SELECT 1 FROM numbers(1000000000)</fill_query>\n+\n+    <query>SELECT sum(x * x + x) FROM test_memory</query>\n+\n+    <drop_query>DROP TABLE IF EXISTS test_memory</drop_query>\n+</test>\ndiff --git a/tests/queries/0_stateless/00341_squashing_insert_select2.sql b/tests/queries/0_stateless/00341_squashing_insert_select2.sql\nindex 469fdaaa64ae..3eb5a2682e03 100644\n--- a/tests/queries/0_stateless/00341_squashing_insert_select2.sql\n+++ b/tests/queries/0_stateless/00341_squashing_insert_select2.sql\n@@ -1,5 +1,5 @@\n DROP TABLE IF EXISTS numbers_squashed;\n-CREATE TABLE numbers_squashed (number UInt8) ENGINE = Memory;\n+CREATE TABLE numbers_squashed (number UInt8) ENGINE = StripeLog;\n \n SET min_insert_block_size_rows = 100;\n SET min_insert_block_size_bytes = 0;\ndiff --git a/tests/queries/0_stateless/00979_live_view_watch_live_moving_avg.py b/tests/queries/0_stateless/00979_live_view_watch_live_moving_avg.py.disabled\nsimilarity index 100%\nrename from tests/queries/0_stateless/00979_live_view_watch_live_moving_avg.py\nrename to tests/queries/0_stateless/00979_live_view_watch_live_moving_avg.py.disabled\ndiff --git a/tests/queries/0_stateless/01455_optimize_trivial_insert_select.sql b/tests/queries/0_stateless/01455_optimize_trivial_insert_select.sql\nindex de470fe6a572..5b59bc065dda 100644\n--- a/tests/queries/0_stateless/01455_optimize_trivial_insert_select.sql\n+++ b/tests/queries/0_stateless/01455_optimize_trivial_insert_select.sql\n@@ -1,7 +1,9 @@\n SET max_insert_threads = 1, max_threads = 100, min_insert_block_size_rows = 1048576, max_block_size = 65536;\n-CREATE TEMPORARY TABLE t (x UInt64);\n+DROP TABLE IF EXISTS t;\n+CREATE TABLE t (x UInt64) ENGINE = StripeLog;\n -- For trivial INSERT SELECT, max_threads is lowered to max_insert_threads and max_block_size is changed to min_insert_block_size_rows.\n INSERT INTO t SELECT * FROM numbers_mt(1000000);\n SET max_threads = 1;\n -- If data was inserted by more threads, we will probably see data out of order.\n SELECT DISTINCT blockSize(), runningDifference(x) FROM t;\n+DROP TABLE t;\n",
  "problem_statement": "Do not squash blocks too much on INSERT SELECT if inserting into Memory table.\nIt's more efficient to store blocks in Memory table in the size that is optimal for processing (e.g. 64K rows).\r\nBut by default they are squashed to 1M rows (min_insert_block_size_rows) or 256MB (min_insert_block_size_bytes).\r\n\r\nIt can lead to 3x performance drop.\n",
  "hints_text": "We can add `optimalWriteBlockSize` method to `IStorage`.",
  "created_at": "2021-02-07T01:58:25Z"
}