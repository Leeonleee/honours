{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 5080,
  "instance_id": "ClickHouse__ClickHouse-5080",
  "issue_numbers": [
    "4736"
  ],
  "base_commit": "81259c0b522267805c803ccbc53ec47ddc36efd6",
  "patch": "diff --git a/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp b/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp\nindex c511a1053b35..2cabb9c431f7 100644\n--- a/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp\n+++ b/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp\n@@ -24,36 +24,21 @@ KafkaBlockInputStream::~KafkaBlockInputStream()\n         return;\n \n     if (broken)\n-    {\n-        LOG_TRACE(storage.log, \"Re-joining claimed consumer after failure\");\n-        consumer->unsubscribe();\n-    }\n+        buffer->subBufferAs<ReadBufferFromKafkaConsumer>()->unsubscribe();\n \n-    storage.pushConsumer(consumer);\n+    storage.pushBuffer(buffer);\n }\n \n void KafkaBlockInputStream::readPrefixImpl()\n {\n-    consumer = storage.tryClaimConsumer(context.getSettingsRef().queue_max_wait_ms.totalMilliseconds());\n-    claimed = !!consumer;\n+    buffer = storage.tryClaimBuffer(context.getSettingsRef().queue_max_wait_ms.totalMilliseconds());\n+    claimed = !!buffer;\n \n-    if (!consumer)\n-        consumer = std::make_shared<cppkafka::Consumer>(storage.createConsumerConfiguration());\n+    if (!buffer)\n+        buffer = storage.createBuffer();\n \n-    // While we wait for an assignment after subscribtion, we'll poll zero messages anyway.\n-    // If we're doing a manual select then it's better to get something after a wait, then immediate nothing.\n-    if (consumer->get_subscription().empty())\n-    {\n-        using namespace std::chrono_literals;\n+    buffer->subBufferAs<ReadBufferFromKafkaConsumer>()->subscribe(storage.topics);\n \n-        consumer->pause(); // don't accidentally read any messages\n-        consumer->subscribe(storage.topics);\n-        consumer->poll(5s);\n-        consumer->resume();\n-    }\n-\n-    buffer = std::make_unique<DelimitedReadBuffer>(\n-        new ReadBufferFromKafkaConsumer(consumer, storage.log, max_block_size), storage.row_delimiter);\n     addChild(FormatFactory::instance().getInput(storage.format_name, *buffer, storage.getSampleBlock(), context, max_block_size));\n \n     broken = true;\n@@ -66,4 +51,4 @@ void KafkaBlockInputStream::readSuffixImpl()\n     broken = false;\n }\n \n-} // namespace DB\n+}\ndiff --git a/dbms/src/Storages/Kafka/KafkaBlockInputStream.h b/dbms/src/Storages/Kafka/KafkaBlockInputStream.h\nindex 29d013c183d8..1b6c8b8ae256 100644\n--- a/dbms/src/Storages/Kafka/KafkaBlockInputStream.h\n+++ b/dbms/src/Storages/Kafka/KafkaBlockInputStream.h\n@@ -1,7 +1,6 @@\n #pragma once\n \n #include <DataStreams/IBlockInputStream.h>\n-#include <IO/DelimitedReadBuffer.h>\n #include <Interpreters/Context.h>\n \n #include <Storages/Kafka/StorageKafka.h>\n@@ -27,8 +26,7 @@ class KafkaBlockInputStream : public IBlockInputStream\n     Context context;\n     UInt64 max_block_size;\n \n-    ConsumerPtr consumer;\n-    std::unique_ptr<DelimitedReadBuffer> buffer;\n+    BufferPtr buffer;\n     bool broken = true, claimed = false;\n };\n \ndiff --git a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\nindex 948662a9f933..fa5eb453e926 100644\n--- a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n+++ b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n@@ -2,10 +2,11 @@\n \n namespace DB\n {\n+\n namespace\n {\n     const auto READ_POLL_MS = 500; /// How long to wait for a batch of messages.\n-} // namespace\n+}\n \n void ReadBufferFromKafkaConsumer::commit()\n {\n@@ -13,10 +14,32 @@ void ReadBufferFromKafkaConsumer::commit()\n         return;\n \n     auto & previous = *std::prev(current);\n+\n     LOG_TRACE(log, \"Committing message with offset \" << previous.get_offset());\n     consumer->async_commit(previous);\n }\n \n+void ReadBufferFromKafkaConsumer::subscribe(const Names & topics)\n+{\n+    // While we wait for an assignment after subscribtion, we'll poll zero messages anyway.\n+    // If we're doing a manual select then it's better to get something after a wait, then immediate nothing.\n+    if (consumer->get_subscription().empty())\n+    {\n+        using namespace std::chrono_literals;\n+\n+        consumer->pause(); // don't accidentally read any messages\n+        consumer->subscribe(topics);\n+        consumer->poll(5s);\n+        consumer->resume();\n+    }\n+}\n+\n+void ReadBufferFromKafkaConsumer::unsubscribe()\n+{\n+    LOG_TRACE(log, \"Re-joining claimed consumer after failure\");\n+    consumer->unsubscribe();\n+}\n+\n /// Do commit messages implicitly after we processed the previous batch.\n bool ReadBufferFromKafkaConsumer::nextImpl()\n {\n@@ -50,4 +73,4 @@ bool ReadBufferFromKafkaConsumer::nextImpl()\n     return true;\n }\n \n-} // namespace DB\n+}\ndiff --git a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\nindex 8a3ebcbc2ef1..f7b49d955de2 100644\n--- a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n+++ b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n@@ -1,14 +1,18 @@\n #pragma once\n \n-#include <IO/ReadBuffer.h>\n+#include <Core/Names.h>\n+#include <IO/DelimitedReadBuffer.h>\n #include <common/logger_useful.h>\n \n #include <cppkafka/cppkafka.h>\n \n namespace DB\n {\n+\n+using BufferPtr = std::shared_ptr<DelimitedReadBuffer>;\n using ConsumerPtr = std::shared_ptr<cppkafka::Consumer>;\n \n+\n class ReadBufferFromKafkaConsumer : public ReadBuffer\n {\n public:\n@@ -17,8 +21,9 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n     {\n     }\n \n-    // Commit all processed messages.\n-    void commit();\n+    void commit(); // Commit all processed messages.\n+    void subscribe(const Names & topics); // Subscribe internal consumer to topics.\n+    void unsubscribe(); // Unsubscribe internal consumer in case of failure.\n \n private:\n     using Messages = std::vector<cppkafka::Message>;\n@@ -33,4 +38,4 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n     bool nextImpl() override;\n };\n \n-} // namespace DB\n+}\ndiff --git a/dbms/src/Storages/Kafka/StorageKafka.cpp b/dbms/src/Storages/Kafka/StorageKafka.cpp\nindex 5785e65d4d92..1e1ee6bb6951 100644\n--- a/dbms/src/Storages/Kafka/StorageKafka.cpp\n+++ b/dbms/src/Storages/Kafka/StorageKafka.cpp\n@@ -81,7 +81,7 @@ StorageKafka::StorageKafka(\n     row_delimiter(row_delimiter_),\n     schema_name(global_context.getMacros()->expand(schema_name_)),\n     num_consumers(num_consumers_), max_block_size(max_block_size_), log(&Logger::get(\"StorageKafka (\" + table_name_ + \")\")),\n-    semaphore(0, num_consumers_), mutex(), consumers(),\n+    semaphore(0, num_consumers_),\n     skip_broken(skip_broken_)\n {\n     task = global_context.getSchedulePool().createTask(log->name(), [this]{ streamThread(); });\n@@ -124,12 +124,8 @@ void StorageKafka::startup()\n {\n     for (size_t i = 0; i < num_consumers; ++i)\n     {\n-        // Create a consumer and subscribe to topics\n-        auto consumer = std::make_shared<cppkafka::Consumer>(createConsumerConfiguration());\n-        consumer->subscribe(topics);\n-\n-        // Make consumer available\n-        pushConsumer(consumer);\n+        // Make buffer available\n+        pushBuffer(createBuffer());\n         ++num_created_consumers;\n     }\n \n@@ -146,8 +142,8 @@ void StorageKafka::shutdown()\n     // Close all consumers\n     for (size_t i = 0; i < num_created_consumers; ++i)\n     {\n-        auto consumer = claimConsumer();\n-        // FIXME: not sure if really close consumers here, and if we really need to close them here.\n+        auto buffer = claimBuffer();\n+        // FIXME: not sure if we really close consumers here, and if we really need to close them here.\n     }\n \n     LOG_TRACE(log, \"Waiting for cleanup\");\n@@ -203,14 +199,29 @@ cppkafka::Configuration StorageKafka::createConsumerConfiguration()\n     return conf;\n }\n \n-ConsumerPtr StorageKafka::claimConsumer()\n+BufferPtr StorageKafka::createBuffer()\n {\n-    return tryClaimConsumer(-1L);\n+    // Create a consumer and subscribe to topics\n+    auto consumer = std::make_shared<cppkafka::Consumer>(createConsumerConfiguration());\n+    consumer->subscribe(topics);\n+\n+    // Limit the number of batched messages to allow early cancellations\n+    const Settings & settings = global_context.getSettingsRef();\n+    size_t batch_size = max_block_size;\n+    if (!batch_size)\n+        batch_size = settings.max_block_size.value;\n+\n+    return std::make_shared<DelimitedReadBuffer>(new ReadBufferFromKafkaConsumer(consumer, log, batch_size), row_delimiter);\n }\n \n-ConsumerPtr StorageKafka::tryClaimConsumer(long wait_ms)\n+BufferPtr StorageKafka::claimBuffer()\n {\n-    // Wait for the first free consumer\n+    return tryClaimBuffer(-1L);\n+}\n+\n+BufferPtr StorageKafka::tryClaimBuffer(long wait_ms)\n+{\n+    // Wait for the first free buffer\n     if (wait_ms >= 0)\n     {\n         if (!semaphore.tryWait(wait_ms))\n@@ -219,17 +230,17 @@ ConsumerPtr StorageKafka::tryClaimConsumer(long wait_ms)\n     else\n         semaphore.wait();\n \n-    // Take the first available consumer from the list\n+    // Take the first available buffer from the list\n     std::lock_guard lock(mutex);\n-    auto consumer = consumers.back();\n-    consumers.pop_back();\n-    return consumer;\n+    auto buffer = buffers.back();\n+    buffers.pop_back();\n+    return buffer;\n }\n \n-void StorageKafka::pushConsumer(ConsumerPtr consumer)\n+void StorageKafka::pushBuffer(BufferPtr buffer)\n {\n     std::lock_guard lock(mutex);\n-    consumers.push_back(consumer);\n+    buffers.push_back(buffer);\n     semaphore.set();\n }\n \n@@ -303,7 +314,6 @@ bool StorageKafka::streamToViews()\n     insert->table = table_name;\n     insert->no_destination = true; // Only insert into dependent views\n \n-    // Limit the number of batched messages to allow early cancellations\n     const Settings & settings = global_context.getSettingsRef();\n     size_t block_size = max_block_size;\n     if (block_size == 0)\ndiff --git a/dbms/src/Storages/Kafka/StorageKafka.h b/dbms/src/Storages/Kafka/StorageKafka.h\nindex b8d09bb2da1f..c2de0e011786 100644\n--- a/dbms/src/Storages/Kafka/StorageKafka.h\n+++ b/dbms/src/Storages/Kafka/StorageKafka.h\n@@ -4,6 +4,7 @@\n #include <Core/NamesAndTypes.h>\n #include <DataStreams/IBlockOutputStream.h>\n #include <Storages/IStorage.h>\n+#include <Storages/Kafka/ReadBufferFromKafkaConsumer.h>\n #include <Poco/Event.h>\n #include <Poco/Semaphore.h>\n #include <ext/shared_ptr_helper.h>\n@@ -14,15 +15,13 @@\n namespace DB\n {\n \n-using ConsumerPtr = std::shared_ptr<cppkafka::Consumer>;\n-\n /** Implements a Kafka queue table engine that can be used as a persistent queue / buffer,\n   * or as a basic building block for creating pipelines with a continuous insertion / ETL.\n   */\n class StorageKafka : public ext::shared_ptr_helper<StorageKafka>, public IStorage\n {\n-friend class KafkaBlockInputStream;\n-friend class KafkaBlockOutputStream;\n+    friend class KafkaBlockInputStream;\n+    friend class KafkaBlockOutputStream;\n \n public:\n     std::string getName() const override { return \"Kafka\"; }\n@@ -40,7 +39,7 @@ friend class KafkaBlockOutputStream;\n         size_t max_block_size,\n         unsigned num_streams) override;\n \n-    void rename(const String & /*new_path_to_db*/, const String & new_database_name, const String & new_table_name) override\n+    void rename(const String & /* new_path_to_db */, const String & new_database_name, const String & new_table_name) override\n     {\n         table_name = new_table_name;\n         database_name = new_database_name;\n@@ -74,7 +73,7 @@ friend class KafkaBlockOutputStream;\n     // Consumer list\n     Poco::Semaphore semaphore;\n     std::mutex mutex;\n-    std::vector<ConsumerPtr> consumers; /// Available consumers\n+    std::vector<BufferPtr> buffers; /// available buffers for Kafka consumers\n \n     size_t skip_broken;\n \n@@ -83,9 +82,10 @@ friend class KafkaBlockOutputStream;\n     std::atomic<bool> stream_cancelled{false};\n \n     cppkafka::Configuration createConsumerConfiguration();\n-    ConsumerPtr claimConsumer();\n-    ConsumerPtr tryClaimConsumer(long wait_ms);\n-    void pushConsumer(ConsumerPtr c);\n+    BufferPtr createBuffer();\n+    BufferPtr claimBuffer();\n+    BufferPtr tryClaimBuffer(long wait_ms);\n+    void pushBuffer(BufferPtr buf);\n \n     void streamThread();\n     bool streamToViews();\ndiff --git a/utils/kafka/consume.py b/utils/kafka/consume.py\nnew file mode 100755\nindex 000000000000..ce2a0ef6a96c\n--- /dev/null\n+++ b/utils/kafka/consume.py\n@@ -0,0 +1,39 @@\n+#! /usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+\n+# `pip install \u2026`\n+import kafka  # \u2026 kafka-python\n+\n+import argparse\n+from pprint import pprint\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description='Kafka Producer client')\n+    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',\n+        help='Kafka bootstrap-server address')\n+    parser.add_argument('--port', type=int, metavar='PORT', default=9092,\n+        help='Kafka bootstrap-server port')\n+    parser.add_argument('--client', type=str, default='ch-kafka-python',\n+        help='custom client id for this producer')\n+    parser.add_argument('--topic', type=str, required=True,\n+        help='name of Kafka topic to store in')\n+    parser.add_argument('--group', type=str, required=True,\n+        help='name of the consumer group')\n+\n+    args = parser.parse_args()\n+    config = {\n+        'bootstrap_servers': f'{args.server}:{args.port}',\n+        'client_id': args.client,\n+        'group_id': args.group,\n+    }\n+    client = kafka.KafkaConsumer(**config)\n+\n+    client.subscribe([args.topic])\n+    pprint(client.poll(10000))\n+    client.unsubscribe()\n+    client.close()\n+\n+\n+if __name__ == \"__main__\":\n+    exit(main())\ndiff --git a/utils/kafka/manage.py b/utils/kafka/manage.py\nnew file mode 100755\nindex 000000000000..13bc2fa03883\n--- /dev/null\n+++ b/utils/kafka/manage.py\n@@ -0,0 +1,41 @@\n+#! /usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+\n+# `pip install \u2026`\n+import kafka  # \u2026 kafka-python\n+\n+import argparse\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description='Kafka Topic manager')\n+    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',\n+        help='Kafka bootstrap-server address')\n+    parser.add_argument('--port', type=int, metavar='PORT', default=9092,\n+        help='Kafka bootstrap-server port')\n+    parser.add_argument('--client', type=str, default='ch-kafka-python',\n+        help='custom client id for this producer')\n+\n+    commands = parser.add_mutually_exclusive_group()\n+    commands.add_argument('--create', type=str, metavar='TOPIC', nargs='+',\n+        help='create new topic(s) in the cluster')\n+    commands.add_argument('--delete', type=str, metavar='TOPIC', nargs='+',\n+        help='delete existing topic(s) from the cluster')\n+\n+    args = parser.parse_args()\n+    config = {\n+        'bootstrap_servers': f'{args.server}:{args.port}',\n+        'client_id': args.client,\n+    }\n+\n+    client = kafka.KafkaAdminClient(**config)\n+    if args.create:\n+        print(client.create_topics(args.create))\n+    elif args.delete:\n+        print(client.delete_topics(args.delete))\n+\n+    client.close()\n+\n+\n+if __name__ == \"__main__\":\n+    exit(main())\ndiff --git a/utils/kafka/produce.py b/utils/kafka/produce.py\nnew file mode 100755\nindex 000000000000..f98b2cbbcdba\n--- /dev/null\n+++ b/utils/kafka/produce.py\n@@ -0,0 +1,72 @@\n+#! /usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+\n+# `pip install \u2026`\n+import kafka  # \u2026 kafka-python\n+\n+import argparse\n+from concurrent.futures import ThreadPoolExecutor\n+import enum\n+import multiprocessing\n+import sys\n+\n+\n+class Sync(enum.Enum):\n+    NONE = 'none'\n+    LEAD = 'leader'\n+    ALL = 'all'\n+\n+    def __str__(self):\n+        return self.value\n+\n+    def convert(self):\n+        values = {\n+            str(Sync.NONE): '0',\n+            str(Sync.LEAD): '1',\n+            str(Sync.ALL): 'all',\n+        }\n+        return values[self.value]\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description='Produce a single message taken from input')\n+    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',\n+        help='Kafka bootstrap-server address')\n+    parser.add_argument('--port', type=int, metavar='PORT', default=9092,\n+        help='Kafka bootstrap-server port')\n+    parser.add_argument('--client', type=str, default='ch-kafka-python',\n+        help='custom client id for this producer')\n+    parser.add_argument('--topic', type=str, required=True,\n+        help='name of Kafka topic to store in')\n+    parser.add_argument('--retries', type=int, default=0,\n+        help='number of retries to send on failure')\n+    parser.add_argument('--multiply', type=int, default=1,\n+        help='multiplies incoming string many times')\n+    parser.add_argument('--repeat', type=int, default=1,\n+        help='send same (multiplied) message many times')\n+\n+    args = parser.parse_args()\n+    config = {\n+        'bootstrap_servers': f'{args.server}:{args.port}',\n+        'client_id': args.client,\n+        'retries': args.retries,\n+    }\n+    client = kafka.KafkaProducer(**config)\n+\n+    message = sys.stdin.buffer.read() * args.multiply\n+\n+    def send(num):\n+        client.send(topic=args.topic, value=message)\n+        print(f'iteration {num}: sent a message multiplied {args.multiply} times')\n+\n+    pool = ThreadPoolExecutor(max_workers=multiprocessing.cpu_count())\n+    for num in range(args.repeat):\n+        pool.submit(send, num)\n+    pool.shutdown()\n+\n+    client.flush()\n+    client.close()\n+\n+\n+if __name__ == \"__main__\":\n+    exit(main())\ndiff --git a/utils/kafka/status.py b/utils/kafka/status.py\nnew file mode 100755\nindex 000000000000..8331a056dff9\n--- /dev/null\n+++ b/utils/kafka/status.py\n@@ -0,0 +1,52 @@\n+#! /usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+\n+# `pip install \u2026`\n+import kafka  # \u2026 kafka-python\n+\n+import argparse\n+from pprint import pprint\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description='Kafka client to get groups and topics status')\n+    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',\n+        help='Kafka bootstrap-server address')\n+    parser.add_argument('--port', type=int, metavar='PORT', default=9092,\n+        help='Kafka bootstrap-server port')\n+    parser.add_argument('--client', type=str, default='ch-kafka-python',\n+        help='custom client id for this producer')\n+\n+    args = parser.parse_args()\n+    config = {\n+        'bootstrap_servers': f'{args.server}:{args.port}',\n+        'client_id': args.client,\n+    }\n+\n+    client = kafka.KafkaAdminClient(**config)\n+    consumer = kafka.KafkaConsumer(**config)\n+    cluster = client._client.cluster\n+\n+    topics = cluster.topics()\n+    for topic in topics:\n+        print(f'Topic \"{topic}\":', end='')\n+        for partition in cluster.partitions_for_topic(topic):\n+            tp = kafka.TopicPartition(topic, partition)\n+            print(f' {partition} (begin: {consumer.beginning_offsets([tp])[tp]}, end: {consumer.end_offsets([tp])[tp]})', end='')\n+        print()\n+\n+    groups = client.list_consumer_groups()\n+    for group in groups:\n+        print(f'Group \"{group[0]}\" ({group[1]}):')\n+\n+        consumer = kafka.KafkaConsumer(**config, group_id=group[0])\n+        offsets = client.list_consumer_group_offsets(group[0])\n+        for topic, offset in offsets.items():\n+            print(f'\\t{topic.topic}[{topic.partition}]: {consumer.beginning_offsets([topic])[topic]}, {offset.offset}, {consumer.end_offsets([topic])[topic]}')\n+        consumer.close()\n+\n+    client.close()\n+\n+\n+if __name__ == \"__main__\":\n+    exit(main())\n",
  "test_patch": "diff --git a/dbms/tests/integration/helpers/docker_compose_kafka.yml b/dbms/tests/integration/helpers/docker_compose_kafka.yml\nindex bed537a9760c..8fea4faa2721 100644\n--- a/dbms/tests/integration/helpers/docker_compose_kafka.yml\n+++ b/dbms/tests/integration/helpers/docker_compose_kafka.yml\n@@ -12,7 +12,7 @@ services:\n         - label:disable\n \n   kafka1:\n-    image: confluentinc/cp-kafka:4.1.0\n+    image: confluentinc/cp-kafka:5.2.0\n     hostname: kafka1\n     ports:\n         - \"9092:9092\"\ndiff --git a/dbms/tests/integration/test_storage_kafka/test.py b/dbms/tests/integration/test_storage_kafka/test.py\nindex ed4b6d14d25a..c67b95c1e832 100644\n--- a/dbms/tests/integration/test_storage_kafka/test.py\n+++ b/dbms/tests/integration/test_storage_kafka/test.py\n@@ -7,7 +7,8 @@\n \n import json\n import subprocess\n-from kafka import KafkaProducer\n+import kafka.errors\n+from kafka import KafkaAdminClient, KafkaProducer\n from google.protobuf.internal.encoder import _VarintBytes\n \n \"\"\"\n@@ -62,22 +63,11 @@ def wait_kafka_is_available(max_retries=50):\n \n \n def kafka_produce(topic, messages):\n-    p = subprocess.Popen(('docker',\n-                          'exec',\n-                          '-i',\n-                          kafka_id,\n-                          '/usr/bin/kafka-console-producer',\n-                          '--broker-list',\n-                          'INSIDE://localhost:9092',\n-                          '--topic',\n-                          topic,\n-                          '--sync',\n-                          '--message-send-max-retries',\n-                          '100'),\n-                         stdin=subprocess.PIPE)\n-    p.communicate(messages)\n-    p.stdin.close()\n-    print(\"Produced {} messages for topic {}\".format(len(messages.splitlines()), topic))\n+    producer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n+    for message in messages:\n+        producer.send(topic=topic, value=message)\n+        producer.flush()\n+    print (\"Produced {} messages for topic {}\".format(len(messages), topic))\n \n \n def kafka_produce_protobuf_messages(topic, start_index, num_messages):\n@@ -141,9 +131,9 @@ def test_kafka_settings_old_syntax(kafka_cluster):\n \n     # Don't insert malformed messages since old settings syntax\n     # doesn't support skipping of broken messages.\n-    messages = ''\n+    messages = []\n     for i in range(50):\n-        messages += json.dumps({'key': i, 'value': i}) + '\\n'\n+        messages.append(json.dumps({'key': i, 'value': i}))\n     kafka_produce('old', messages)\n \n     result = ''\n@@ -167,18 +157,18 @@ def test_kafka_settings_new_syntax(kafka_cluster):\n                 kafka_skip_broken_messages = 1;\n         ''')\n \n-    messages = ''\n+    messages = []\n     for i in range(25):\n-        messages += json.dumps({'key': i, 'value': i}) + '\\n'\n+        messages.append(json.dumps({'key': i, 'value': i}))\n     kafka_produce('new', messages)\n \n     # Insert couple of malformed messages.\n-    kafka_produce('new', '}{very_broken_message,\\n')\n-    kafka_produce('new', '}another{very_broken_message,\\n')\n+    kafka_produce('new', ['}{very_broken_message,'])\n+    kafka_produce('new', ['}another{very_broken_message,'])\n \n-    messages = ''\n+    messages = []\n     for i in range(25, 50):\n-        messages += json.dumps({'key': i, 'value': i}) + '\\n'\n+        messages.append(json.dumps({'key': i, 'value': i}))\n     kafka_produce('new', messages)\n \n     result = ''\n@@ -201,9 +191,9 @@ def test_kafka_csv_with_delimiter(kafka_cluster):\n                 kafka_row_delimiter = '\\\\n';\n         ''')\n \n-    messages = ''\n+    messages = []\n     for i in range(50):\n-        messages += '{i}, {i}\\n'.format(i=i)\n+        messages.append('{i}, {i}'.format(i=i))\n     kafka_produce('csv', messages)\n \n     result = ''\n@@ -226,9 +216,9 @@ def test_kafka_tsv_with_delimiter(kafka_cluster):\n                 kafka_row_delimiter = '\\\\n';\n         ''')\n \n-    messages = ''\n+    messages = []\n     for i in range(50):\n-        messages += '{i}\\t{i}\\n'.format(i=i)\n+        messages.append('{i}\\t{i}'.format(i=i))\n     kafka_produce('tsv', messages)\n \n     result = ''\n@@ -239,6 +229,35 @@ def test_kafka_tsv_with_delimiter(kafka_cluster):\n     kafka_check_result(result, True)\n \n \n+def test_kafka_json_without_delimiter(kafka_cluster):\n+    instance.query('''\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS\n+                kafka_broker_list = 'kafka1:19092',\n+                kafka_topic_list = 'json',\n+                kafka_group_name = 'json',\n+                kafka_format = 'JSONEachRow';\n+        ''')\n+\n+    messages = ''\n+    for i in range(25):\n+        messages += json.dumps({'key': i, 'value': i}) + '\\n'\n+    kafka_produce('json', [messages])\n+\n+    messages = ''\n+    for i in range(25, 50):\n+        messages += json.dumps({'key': i, 'value': i}) + '\\n'\n+    kafka_produce('json', [messages])\n+\n+    result = ''\n+    for i in range(50):\n+        result += instance.query('SELECT * FROM test.kafka')\n+        if kafka_check_result(result):\n+            break\n+    kafka_check_result(result, True)\n+\n+\n def test_kafka_protobuf(kafka_cluster):\n     instance.query('''\n         CREATE TABLE test.kafka (key UInt64, value String)\n@@ -282,9 +301,9 @@ def test_kafka_materialized_view(kafka_cluster):\n             SELECT * FROM test.kafka;\n     ''')\n \n-    messages = ''\n+    messages = []\n     for i in range(50):\n-        messages += json.dumps({'key': i, 'value': i}) + '\\n'\n+        messages.append(json.dumps({'key': i, 'value': i}))\n     kafka_produce('json', messages)\n \n     for i in range(20):\n@@ -300,6 +319,52 @@ def test_kafka_materialized_view(kafka_cluster):\n     ''')\n \n \n+def test_kafka_flush_on_big_message(kafka_cluster):\n+    # Create batchs of messages of size ~100Kb\n+    kafka_messages = 10000\n+    batch_messages = 1000\n+    messages = [json.dumps({'key': i, 'value': 'x' * 100}) * batch_messages for i in range(kafka_messages)]\n+    kafka_produce('flush', messages)\n+\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.view;\n+        DROP TABLE IF EXISTS test.consumer;\n+        CREATE TABLE test.kafka (key UInt64, value String)\n+            ENGINE = Kafka\n+            SETTINGS\n+                kafka_broker_list = 'kafka1:19092',\n+                kafka_topic_list = 'flush',\n+                kafka_group_name = 'flush',\n+                kafka_format = 'JSONEachRow',\n+                kafka_max_block_size = 10;\n+        CREATE TABLE test.view (key UInt64, value String)\n+            ENGINE = MergeTree\n+            ORDER BY key;\n+        CREATE MATERIALIZED VIEW test.consumer TO test.view AS\n+            SELECT * FROM test.kafka;\n+    ''')\n+\n+    client = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n+    received = False\n+    while not received:\n+        try:\n+            offsets = client.list_consumer_group_offsets('flush')\n+            for topic, offset in offsets.items():\n+                if topic.topic == 'flush' and offset.offset == kafka_messages:\n+                    received = True\n+                    break\n+        except kafka.errors.GroupCoordinatorNotAvailableError:\n+            continue\n+\n+    for _ in range(20):\n+        time.sleep(1)\n+        result = instance.query('SELECT count() FROM test.view')\n+        if int(result) == kafka_messages*batch_messages:\n+            break\n+\n+    assert int(result) == kafka_messages*batch_messages, 'ClickHouse lost some messages: {}'.format(result)\n+\n+\n if __name__ == '__main__':\n     cluster.start()\n     raw_input(\"Cluster created, press any key to destroy...\")\n",
  "problem_statement": "Data get lost on heavy load via KafkaEngine\nWe have created Yandex tank scenario in order to test the database on heavy load.\r\nWe use **KafkaEngine** to populate the database. The problem is that the database starts loosing data at 100rps.\r\n\r\n**How to reproduce**\r\n* Create kafka group named _labeling__test_shard_localhost_\r\n* Create kafka topic named _labeling__all_\r\n* Create the database schema defined in **DB_SCHEMA.txt**\r\n* Install **Yandex tank** \r\n* Use tank python script and configution files attached to this issue\r\n* Run tank via the following command: \r\n_sudo docker run     -v $(pwd):/var/loadtest     -v $SSH_AUTH_SOCK:/ssh-agent -e SSH_AUTH_SOCK=/ssh-agent     --net host     -it direvius/yandex-tank -c ch_kafka_conf.yaml_\r\n\r\n**Expected behavior**\r\nThe tank executes 100 requests, 30000 records each. So you should end up having 3000000 records via select count(), but you'll get less due to data loss.\r\nTo make sure that Kafka works as expected we used standard Kafka console consumer and it consumes exactly 3000000 messages.\r\n\r\n**Error message and/or stacktrace**\r\nThere is nothing in server log file that corresponds to this issue.\r\n\r\n**Additional context**\r\nWe do use the following versions of SW:\r\n* Kafka 2.11\r\n* Clickhouse 19.3.4 and the latest \r\n\r\n[DB_SCHEMA.txt](https://github.com/yandex/ClickHouse/files/2991319/DB_SCHEMA.txt)\r\n[ya_tank.zip](https://github.com/yandex/ClickHouse/files/2991320/ya_tank.zip)\r\n\r\n\n",
  "hints_text": "I thought @den-crane helped yu to find the root of the problem in telegram chat? \r\nAs I understand the reason of \"missing\" rows was wrong `order by` columns in  VersionedCollapsingMergeTree? Or it was not the only problem?\n`VersionedCollapsingMergeTree` is a very specific table engine. Do you have a reason to use it?\r\nDoes the issue persist if you use `MergeTree`?\n> I thought @den-crane helped yu to find the root of the problem in telegram chat?\r\n> As I understand the reason of \"missing\" rows was wrong `order by` columns in VersionedCollapsingMergeTree? Or it was not the only problem?\r\n\r\nprobably you've confused two different cases. It's another issue. \r\nAnd probably it's not related to VersionedCollapsingMergeTree (though I am not sure, because I don't really know how yaTank generates unique `ORDER BY (gtin, batch, kiz__sign`\r\nI advised to try TSV. @aegorov05 agreed to try.\r\n\n> `VersionedCollapsingMergeTree` is a very specific table engine. Do you have a reason to use it?\r\n> Does the issue persist if you use `MergeTree`?\r\n\r\nChanging engine to standard MergeTree does not make any difference. Instead of 3 million records we only got 2941414.\n> > I thought @den-crane helped yu to find the root of the problem in telegram chat?\r\n> > As I understand the reason of \"missing\" rows was wrong `order by` columns in VersionedCollapsingMergeTree? Or it was not the only problem?\r\n> \r\n> probably you've confused two different cases. It's another issue.\r\n> And probably it's not related to VersionedCollapsingMergeTree (though I am not sure, because I don't really know how yaTank generates unique `ORDER BY (gtin, batch, kiz__sign`\r\n> I advised to try TSV. @aegorov05 agreed to try.\r\n\r\nWe have also tried using TSV format instead of JSONEachRow. Same result.\r\n[TSV Test.zip](https://github.com/yandex/ClickHouse/files/2991471/TSV.Test.zip)\r\n\n> > `VersionedCollapsingMergeTree` is a very specific table engine. Do you have a reason to use it?\r\n> > Does the issue persist if you use `MergeTree`?\r\n> \r\n> Changing engine to standard MergeTree does not make any difference. Instead of 3 million records we only got 2941414.\r\n\r\nSo the difference is 2941414 - 3000000 = 58586  and it's smaller that 65535 (by default kafka_max_block_size = max_block_size = 65536). May be your last block with messages was not flushed yet? Can you recheck the result after 10 seconds of inactivity (by default  stream_flush_interval_ms = 7500 ms)?\nI think kafka engine waits for {kafka_max_block_size} rows with **blocking** read and after that flushes by reaching max_insert_block_size or 7500ms.\r\n\r\n@aegorov05 try to write (3000 * 65536 + 1)  = 196608001 rows\n> > > `VersionedCollapsingMergeTree` is a very specific table engine. Do you have a reason to use it?\r\n> > > Does the issue persist if you use `MergeTree`?\r\n> > \r\n> > \r\n> > Changing engine to standard MergeTree does not make any difference. Instead of 3 million records we only got 2941414.\r\n> \r\n> So the difference is 2941414 - 3000000 = 58586 and it's smaller that 65535 (by default kafka_max_block_size = max_block_size = 65536). May be your last block with messages was not flushed yet? Can you recheck the result after 10 seconds of inactivity (by default stream_flush_interval_ms = 7500 ms)?\r\n\r\nI checked result after 10 second and more time. Count is not changed.\n> @aegorov05 try to write (3000 * 65536 + 1) = 196608001 rows\r\n\r\nWhy so many? :) Any multiple of 65536 would be ok,  so for example 100 requests, 32768 messages each. It would be 3276800 messages in the table which is 50 blocks of size 65536. \r\n\r\n> I checked result after 10 second and more time. Count is not changed. \r\n\r\nSomebody was reporting some issues with flushing by timeout. Need to check the code. Please ensure that after you fill last block to the required block size whole data appears. \n> > I checked result after 10 second and more time. Count is not changed.\r\n> \r\n> Somebody was reporting some issues with flushing by timeout. Need to check the code. Please ensure that after you fill last block to the required block size whole data appears.\r\n\r\nI used default kafka-consumer-groups for check current offset. Offset is committed, LAG = 0.\n@abyss7 Is there any estimate/milestone when this bug is going to be fixed?\nI had similar problems: a small amount of messages was lost.\r\nIt turned out that the messages were recorded in Kafka with incorrect timestamp of messages, instead of milliseconds, seconds were written.\r\nMaybe you also write seconds to timestamps?\r\n\r\nTo check this, use **kafkacat**:\r\n`> kafkacat -b broker:9092 -t topic -o end -c1 -f 'timestamp: %T, message: %s'`\n> I had similar problems: a small amount of messages was lost.\r\n> It turned out that the messages were recorded in Kafka with incorrect timestamp of messages, instead of milliseconds, seconds were written.\r\n> Maybe you also write seconds to timestamps?\r\n\r\nNo, we write ms to timestamp\r\nBut I think CH Kafka Consumer shouldn't rely on message timestamps, because there could be a lot of messages with the same timestamp (ms or even ns) on heavy load.",
  "created_at": "2019-04-22T14:01:28Z",
  "modified_files": [
    "dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp",
    "dbms/src/Storages/Kafka/KafkaBlockInputStream.h",
    "dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp",
    "dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h",
    "dbms/src/Storages/Kafka/StorageKafka.cpp",
    "dbms/src/Storages/Kafka/StorageKafka.h",
    "b/utils/kafka/consume.py",
    "b/utils/kafka/manage.py",
    "b/utils/kafka/produce.py",
    "b/utils/kafka/status.py"
  ],
  "modified_test_files": [
    "dbms/tests/integration/helpers/docker_compose_kafka.yml",
    "dbms/tests/integration/test_storage_kafka/test.py"
  ]
}