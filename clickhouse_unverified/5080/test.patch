diff --git a/dbms/tests/integration/helpers/docker_compose_kafka.yml b/dbms/tests/integration/helpers/docker_compose_kafka.yml
index bed537a9760c..8fea4faa2721 100644
--- a/dbms/tests/integration/helpers/docker_compose_kafka.yml
+++ b/dbms/tests/integration/helpers/docker_compose_kafka.yml
@@ -12,7 +12,7 @@ services:
         - label:disable
 
   kafka1:
-    image: confluentinc/cp-kafka:4.1.0
+    image: confluentinc/cp-kafka:5.2.0
     hostname: kafka1
     ports:
         - "9092:9092"
diff --git a/dbms/tests/integration/test_storage_kafka/test.py b/dbms/tests/integration/test_storage_kafka/test.py
index ed4b6d14d25a..c67b95c1e832 100644
--- a/dbms/tests/integration/test_storage_kafka/test.py
+++ b/dbms/tests/integration/test_storage_kafka/test.py
@@ -7,7 +7,8 @@
 
 import json
 import subprocess
-from kafka import KafkaProducer
+import kafka.errors
+from kafka import KafkaAdminClient, KafkaProducer
 from google.protobuf.internal.encoder import _VarintBytes
 
 """
@@ -62,22 +63,11 @@ def wait_kafka_is_available(max_retries=50):
 
 
 def kafka_produce(topic, messages):
-    p = subprocess.Popen(('docker',
-                          'exec',
-                          '-i',
-                          kafka_id,
-                          '/usr/bin/kafka-console-producer',
-                          '--broker-list',
-                          'INSIDE://localhost:9092',
-                          '--topic',
-                          topic,
-                          '--sync',
-                          '--message-send-max-retries',
-                          '100'),
-                         stdin=subprocess.PIPE)
-    p.communicate(messages)
-    p.stdin.close()
-    print("Produced {} messages for topic {}".format(len(messages.splitlines()), topic))
+    producer = KafkaProducer(bootstrap_servers="localhost:9092")
+    for message in messages:
+        producer.send(topic=topic, value=message)
+        producer.flush()
+    print ("Produced {} messages for topic {}".format(len(messages), topic))
 
 
 def kafka_produce_protobuf_messages(topic, start_index, num_messages):
@@ -141,9 +131,9 @@ def test_kafka_settings_old_syntax(kafka_cluster):
 
     # Don't insert malformed messages since old settings syntax
     # doesn't support skipping of broken messages.
-    messages = ''
+    messages = []
     for i in range(50):
-        messages += json.dumps({'key': i, 'value': i}) + '
'
+        messages.append(json.dumps({'key': i, 'value': i}))
     kafka_produce('old', messages)
 
     result = ''
@@ -167,18 +157,18 @@ def test_kafka_settings_new_syntax(kafka_cluster):
                 kafka_skip_broken_messages = 1;
         ''')
 
-    messages = ''
+    messages = []
     for i in range(25):
-        messages += json.dumps({'key': i, 'value': i}) + '
'
+        messages.append(json.dumps({'key': i, 'value': i}))
     kafka_produce('new', messages)
 
     # Insert couple of malformed messages.
-    kafka_produce('new', '}{very_broken_message,
')
-    kafka_produce('new', '}another{very_broken_message,
')
+    kafka_produce('new', ['}{very_broken_message,'])
+    kafka_produce('new', ['}another{very_broken_message,'])
 
-    messages = ''
+    messages = []
     for i in range(25, 50):
-        messages += json.dumps({'key': i, 'value': i}) + '
'
+        messages.append(json.dumps({'key': i, 'value': i}))
     kafka_produce('new', messages)
 
     result = ''
@@ -201,9 +191,9 @@ def test_kafka_csv_with_delimiter(kafka_cluster):
                 kafka_row_delimiter = '\
';
         ''')
 
-    messages = ''
+    messages = []
     for i in range(50):
-        messages += '{i}, {i}
'.format(i=i)
+        messages.append('{i}, {i}'.format(i=i))
     kafka_produce('csv', messages)
 
     result = ''
@@ -226,9 +216,9 @@ def test_kafka_tsv_with_delimiter(kafka_cluster):
                 kafka_row_delimiter = '\
';
         ''')
 
-    messages = ''
+    messages = []
     for i in range(50):
-        messages += '{i}\t{i}
'.format(i=i)
+        messages.append('{i}\t{i}'.format(i=i))
     kafka_produce('tsv', messages)
 
     result = ''
@@ -239,6 +229,35 @@ def test_kafka_tsv_with_delimiter(kafka_cluster):
     kafka_check_result(result, True)
 
 
+def test_kafka_json_without_delimiter(kafka_cluster):
+    instance.query('''
+        CREATE TABLE test.kafka (key UInt64, value UInt64)
+            ENGINE = Kafka
+            SETTINGS
+                kafka_broker_list = 'kafka1:19092',
+                kafka_topic_list = 'json',
+                kafka_group_name = 'json',
+                kafka_format = 'JSONEachRow';
+        ''')
+
+    messages = ''
+    for i in range(25):
+        messages += json.dumps({'key': i, 'value': i}) + '
'
+    kafka_produce('json', [messages])
+
+    messages = ''
+    for i in range(25, 50):
+        messages += json.dumps({'key': i, 'value': i}) + '
'
+    kafka_produce('json', [messages])
+
+    result = ''
+    for i in range(50):
+        result += instance.query('SELECT * FROM test.kafka')
+        if kafka_check_result(result):
+            break
+    kafka_check_result(result, True)
+
+
 def test_kafka_protobuf(kafka_cluster):
     instance.query('''
         CREATE TABLE test.kafka (key UInt64, value String)
@@ -282,9 +301,9 @@ def test_kafka_materialized_view(kafka_cluster):
             SELECT * FROM test.kafka;
     ''')
 
-    messages = ''
+    messages = []
     for i in range(50):
-        messages += json.dumps({'key': i, 'value': i}) + '
'
+        messages.append(json.dumps({'key': i, 'value': i}))
     kafka_produce('json', messages)
 
     for i in range(20):
@@ -300,6 +319,52 @@ def test_kafka_materialized_view(kafka_cluster):
     ''')
 
 
+def test_kafka_flush_on_big_message(kafka_cluster):
+    # Create batchs of messages of size ~100Kb
+    kafka_messages = 10000
+    batch_messages = 1000
+    messages = [json.dumps({'key': i, 'value': 'x' * 100}) * batch_messages for i in range(kafka_messages)]
+    kafka_produce('flush', messages)
+
+    instance.query('''
+        DROP TABLE IF EXISTS test.view;
+        DROP TABLE IF EXISTS test.consumer;
+        CREATE TABLE test.kafka (key UInt64, value String)
+            ENGINE = Kafka
+            SETTINGS
+                kafka_broker_list = 'kafka1:19092',
+                kafka_topic_list = 'flush',
+                kafka_group_name = 'flush',
+                kafka_format = 'JSONEachRow',
+                kafka_max_block_size = 10;
+        CREATE TABLE test.view (key UInt64, value String)
+            ENGINE = MergeTree
+            ORDER BY key;
+        CREATE MATERIALIZED VIEW test.consumer TO test.view AS
+            SELECT * FROM test.kafka;
+    ''')
+
+    client = KafkaAdminClient(bootstrap_servers="localhost:9092")
+    received = False
+    while not received:
+        try:
+            offsets = client.list_consumer_group_offsets('flush')
+            for topic, offset in offsets.items():
+                if topic.topic == 'flush' and offset.offset == kafka_messages:
+                    received = True
+                    break
+        except kafka.errors.GroupCoordinatorNotAvailableError:
+            continue
+
+    for _ in range(20):
+        time.sleep(1)
+        result = instance.query('SELECT count() FROM test.view')
+        if int(result) == kafka_messages*batch_messages:
+            break
+
+    assert int(result) == kafka_messages*batch_messages, 'ClickHouse lost some messages: {}'.format(result)
+
+
 if __name__ == '__main__':
     cluster.start()
     raw_input("Cluster created, press any key to destroy...")
