{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 69236,
  "instance_id": "ClickHouse__ClickHouse-69236",
  "issue_numbers": [
    "67768"
  ],
  "base_commit": "28690d639f337fd51f6ced5d6ba399a96dc40c5e",
  "patch": "diff --git a/programs/local/LocalServer.cpp b/programs/local/LocalServer.cpp\nindex df40ce3fa5d0..7e4b458eca6c 100644\n--- a/programs/local/LocalServer.cpp\n+++ b/programs/local/LocalServer.cpp\n@@ -831,6 +831,9 @@ void LocalServer::processConfig()\n     /// Initialize a dummy query cache.\n     global_context->setQueryCache(0, 0, 0, 0);\n \n+    /// Initialize a dummy query condition cache.\n+    global_context->setQueryConditionCache(DEFAULT_QUERY_CONDITION_CACHE_POLICY, 0, 0);\n+\n     /// Initialize allowed tiers\n     global_context->getAccessControl().setAllowTierSettings(server_settings[ServerSetting::allow_feature_tier]);\n \ndiff --git a/programs/server/Server.cpp b/programs/server/Server.cpp\nindex 4fc6e07886d5..c062adfeb190 100644\n--- a/programs/server/Server.cpp\n+++ b/programs/server/Server.cpp\n@@ -279,6 +279,9 @@ namespace ServerSetting\n     extern const ServerSettingsUInt64 page_cache_chunk_size;\n     extern const ServerSettingsUInt64 page_cache_mmap_size;\n     extern const ServerSettingsUInt64 page_cache_size;\n+    extern const ServerSettingsString query_condition_cache_policy;\n+    extern const ServerSettingsUInt64 query_condition_cache_size;\n+    extern const ServerSettingsDouble query_condition_cache_size_ratio;\n     extern const ServerSettingsBool page_cache_use_madv_free;\n     extern const ServerSettingsBool page_cache_use_transparent_huge_pages;\n     extern const ServerSettingsBool prepare_system_log_tables_on_startup;\n@@ -1715,6 +1718,16 @@ try\n     }\n     global_context->setQueryCache(query_cache_max_size_in_bytes, query_cache_max_entries, query_cache_query_cache_max_entry_size_in_bytes, query_cache_max_entry_size_in_rows);\n \n+    String query_condition_cache_policy = server_settings[ServerSetting::query_condition_cache_policy];\n+    size_t query_condition_cache_size = server_settings[ServerSetting::query_condition_cache_size];\n+    double query_condition_cache_size_ratio = server_settings[ServerSetting::query_condition_cache_size_ratio];\n+    if (query_condition_cache_size > max_cache_size)\n+    {\n+        query_condition_cache_size = max_cache_size;\n+        LOG_INFO(log, \"Lowered query condition cache size to {} because the system has limited RAM\", formatReadableSizeWithBinarySuffix(query_condition_cache_size));\n+    }\n+    global_context->setQueryConditionCache(query_condition_cache_policy, query_condition_cache_size, query_condition_cache_size_ratio);\n+\n #if USE_EMBEDDED_COMPILER\n     size_t compiled_expression_cache_max_size_in_bytes = server_settings[ServerSetting::compiled_expression_cache_size];\n     size_t compiled_expression_cache_max_elements = server_settings[ServerSetting::compiled_expression_cache_elements_size];\n@@ -2010,6 +2023,7 @@ try\n             global_context->updateSkippingIndexCacheConfiguration(*config);\n             global_context->updateMMappedFileCacheConfiguration(*config);\n             global_context->updateQueryCacheConfiguration(*config);\n+            global_context->updateQueryConditionCacheConfiguration(*config);\n \n             CompressionCodecEncrypted::Configuration::instance().tryLoad(*config, \"encryption_codecs\");\n #if USE_SSL\ndiff --git a/src/Access/Common/AccessType.h b/src/Access/Common/AccessType.h\nindex 77edd1630fc6..32084eef9f1c 100644\n--- a/src/Access/Common/AccessType.h\n+++ b/src/Access/Common/AccessType.h\n@@ -173,6 +173,7 @@ enum class AccessType : uint8_t\n     M(SYSTEM_DROP_SKIPPING_INDEX_CACHE, \"SYSTEM DROP SKIPPING INDEX CACHE, DROP SKIPPING INDEX CACHE\", GLOBAL, SYSTEM_DROP_CACHE) \\\n     M(SYSTEM_DROP_MMAP_CACHE, \"SYSTEM DROP MMAP, DROP MMAP CACHE, DROP MMAP\", GLOBAL, SYSTEM_DROP_CACHE) \\\n     M(SYSTEM_DROP_QUERY_CACHE, \"SYSTEM DROP QUERY, DROP QUERY CACHE, DROP QUERY\", GLOBAL, SYSTEM_DROP_CACHE) \\\n+    M(SYSTEM_DROP_QUERY_CONDITION_CACHE, \"SYSTEM DROP QUERY CONDITION, DROP QUERY CONDITION CACHE, DROP QUERY CONDITION\", GLOBAL, SYSTEM_DROP_CACHE) \\\n     M(SYSTEM_DROP_COMPILED_EXPRESSION_CACHE, \"SYSTEM DROP COMPILED EXPRESSION, DROP COMPILED EXPRESSION CACHE, DROP COMPILED EXPRESSIONS\", GLOBAL, SYSTEM_DROP_CACHE) \\\n     M(SYSTEM_DROP_FILESYSTEM_CACHE, \"SYSTEM DROP FILESYSTEM CACHE, DROP FILESYSTEM CACHE\", GLOBAL, SYSTEM_DROP_CACHE) \\\n     M(SYSTEM_DROP_DISTRIBUTED_CACHE, \"SYSTEM DROP DISTRIBUTED CACHE, DROP DISTRIBUTED CACHE\", GLOBAL, SYSTEM_DROP_CACHE) \\\ndiff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp\nindex 421793001a48..7ca525285189 100644\n--- a/src/Common/ProfileEvents.cpp\n+++ b/src/Common/ProfileEvents.cpp\n@@ -75,6 +75,8 @@\n     M(SkippingIndexCacheWeightLost, \"Approximate number of bytes evicted from the secondary index cache.\", ValueType::Number) \\\n     M(QueryCacheHits, \"Number of times a query result has been found in the query cache (and query computation was avoided). Only updated for SELECT queries with SETTING use_query_cache = 1.\", ValueType::Number) \\\n     M(QueryCacheMisses, \"Number of times a query result has not been found in the query cache (and required query computation). Only updated for SELECT queries with SETTING use_query_cache = 1.\", ValueType::Number) \\\n+    M(QueryConditionCacheHits, \"Number of times an entry has been found in the query condition cache (and reading of marks can be skipped). Only updated for SELECT queries with SETTING use_query_condition_cache = 1.\", ValueType::Number) \\\n+    M(QueryConditionCacheMisses, \"Number of times an entry has not been found in the query condition cache (and reading of mark cannot be skipped). Only updated for SELECT queries with SETTING use_query_condition_cache = 1.\", ValueType::Number) \\\n     /* Each page cache chunk access increments exactly one of the following 5 PageCacheChunk* counters. */ \\\n     /* Something like hit rate: (PageCacheChunkShared + PageCacheChunkDataHits) / [sum of all 5]. */ \\\n     M(PageCacheChunkMisses, \"Number of times a chunk has not been found in the userspace page cache.\", ValueType::Number) \\\ndiff --git a/src/Core/Defines.h b/src/Core/Defines.h\nindex 140463c12126..583371c1c9e4 100644\n--- a/src/Core/Defines.h\n+++ b/src/Core/Defines.h\n@@ -115,6 +115,9 @@ static constexpr auto DEFAULT_QUERY_CACHE_MAX_SIZE = 1_GiB;\n static constexpr auto DEFAULT_QUERY_CACHE_MAX_ENTRIES = 1024uz;\n static constexpr auto DEFAULT_QUERY_CACHE_MAX_ENTRY_SIZE_IN_BYTES = 1_MiB;\n static constexpr auto DEFAULT_QUERY_CACHE_MAX_ENTRY_SIZE_IN_ROWS = 30'000'000uz;\n+static constexpr auto DEFAULT_QUERY_CONDITION_CACHE_POLICY = \"SLRU\";\n+static constexpr auto DEFAULT_QUERY_CONDITION_CACHE_MAX_SIZE = 100_MiB;\n+static constexpr auto DEFAULT_QUERY_CONDITION_CACHE_SIZE_RATIO = 0.5l;\n \n /// Query profiler cannot work with sanitizers.\n /// Sanitizers are using quick \"frame walking\" stack unwinding (this implies -fno-omit-frame-pointer)\ndiff --git a/src/Core/ServerSettings.cpp b/src/Core/ServerSettings.cpp\nindex 0135131c184b..18c9a6ac4993 100644\n--- a/src/Core/ServerSettings.cpp\n+++ b/src/Core/ServerSettings.cpp\n@@ -511,6 +511,10 @@ namespace DB\n     DECLARE(UInt64, compiled_expression_cache_size, DEFAULT_COMPILED_EXPRESSION_CACHE_MAX_SIZE, R\"(Sets the cache size (in bytes) for [compiled expressions](../../operations/caches.md).)\", 0) \\\n     \\\n     DECLARE(UInt64, compiled_expression_cache_elements_size, DEFAULT_COMPILED_EXPRESSION_CACHE_MAX_ENTRIES, R\"(Sets the cache size (in elements) for [compiled expressions](../../operations/caches.md).)\", 0) \\\n+    DECLARE(String, query_condition_cache_policy, DEFAULT_QUERY_CONDITION_CACHE_POLICY, \"Query condition cache policy name.\", 0) \\\n+    DECLARE(UInt64, query_condition_cache_size, DEFAULT_QUERY_CONDITION_CACHE_MAX_SIZE, \"Size of the query condition cache.\", 0) \\\n+    DECLARE(Double, query_condition_cache_size_ratio, DEFAULT_QUERY_CONDITION_CACHE_SIZE_RATIO, \"The size of the protected queue in the query condition cache relative to the cache's total size.\", 0) \\\n+    \\\n     DECLARE(Bool, disable_internal_dns_cache, false, \"Disable internal DNS caching at all.\", 0) \\\n     DECLARE(UInt64, dns_cache_max_entries, 10000, R\"(Internal DNS cache max entries.)\", 0) \\\n     DECLARE(Int32, dns_cache_update_period, 15, \"Internal DNS cache update period in seconds.\", 0) \\\ndiff --git a/src/Core/Settings.cpp b/src/Core/Settings.cpp\nindex 376505392ff6..8830f0303a49 100644\n--- a/src/Core/Settings.cpp\n+++ b/src/Core/Settings.cpp\n@@ -4377,7 +4377,14 @@ Possible values:\n     DECLARE(Bool, enable_sharing_sets_for_mutations, true, R\"(\n Allow sharing set objects build for IN subqueries between different tasks of the same mutation. This reduces memory usage and CPU consumption\n )\", 0) \\\n-    \\\n+    DECLARE(Bool, use_query_condition_cache, false, R\"(\n+Enable the query condition cache.\n+\n+Possible values:\n+\n+- 0 - Disabled\n+- 1 - Enabled\n+)\", 0) \\\n     DECLARE(Bool, optimize_rewrite_sum_if_to_count_if, true, R\"(\n Rewrite sumIf() and sum(if()) function countIf() function when logically equivalent\n )\", 0) \\\ndiff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp\nindex eeee78a933e3..3b9188643137 100644\n--- a/src/Core/SettingsChangesHistory.cpp\n+++ b/src/Core/SettingsChangesHistory.cpp\n@@ -79,6 +79,7 @@ const VersionToSettingsChangesMap & getSettingsChangesHistory()\n             {\"output_format_parquet_bloom_filter_flush_threshold_bytes\", 128 * 1024 * 1024, 128 * 1024 * 1024, \"New setting.\"},\n             {\"output_format_pretty_max_rows\", 10000, 1000, \"It is better for usability - less amount to scroll.\"},\n             {\"restore_replicated_merge_tree_to_shared_merge_tree\", false, false, \"New setting.\"},\n+            {\"use_query_condition_cache\", false, false, \"New setting.\"},\n             {\"parallel_replicas_only_with_analyzer\", false, true, \"Parallel replicas is supported only with analyzer enabled\"},\n             {\"s3_allow_multipart_copy\", true, true, \"New setting.\"},\n         });\ndiff --git a/src/Interpreters/ActionsDAG.cpp b/src/Interpreters/ActionsDAG.cpp\nindex a7cc531e7499..c37c3773e12f 100644\n--- a/src/Interpreters/ActionsDAG.cpp\n+++ b/src/Interpreters/ActionsDAG.cpp\n@@ -26,6 +26,7 @@\n #include <stack>\n #include <base/sort.h>\n #include <Common/JSONBuilder.h>\n+#include <Common/SipHash.h>\n #include <DataTypes/DataTypeSet.h>\n \n #include <absl/container/flat_hash_map.h>\n@@ -135,6 +136,38 @@ void ActionsDAG::Node::toTree(JSONBuilder::JSONMap & map) const\n         map.add(\"Compiled\", is_function_compiled);\n }\n \n+size_t ActionsDAG::Node::getHash() const\n+{\n+    SipHash hash_state;\n+    updateHash(hash_state);\n+    return hash_state.get64();\n+}\n+\n+void ActionsDAG::Node::updateHash(SipHash & hash_state) const\n+{\n+    hash_state.update(type);\n+\n+    if (!result_name.empty())\n+        hash_state.update(result_name);\n+\n+    if (result_type)\n+        hash_state.update(result_type->getName());\n+\n+    if (function_base)\n+        hash_state.update(function_base->getName());\n+\n+    if (function)\n+        hash_state.update(function->getName());\n+\n+    hash_state.update(is_function_compiled);\n+    hash_state.update(is_deterministic_constant);\n+\n+    if (column)\n+        hash_state.update(column->getName());\n+\n+    for (const auto & child : children)\n+        child->updateHash(hash_state);\n+}\n \n ActionsDAG::ActionsDAG(const NamesAndTypesList & inputs_)\n {\ndiff --git a/src/Interpreters/ActionsDAG.h b/src/Interpreters/ActionsDAG.h\nindex a4981200cc67..ee15438cd4f5 100644\n--- a/src/Interpreters/ActionsDAG.h\n+++ b/src/Interpreters/ActionsDAG.h\n@@ -4,6 +4,7 @@\n #include <Core/ColumnsWithTypeAndName.h>\n #include <Core/NamesAndTypes.h>\n #include <Core/Names.h>\n+#include <Common/SipHash.h>\n #include <Interpreters/Context_fwd.h>\n \n #include \"config.h\"\n@@ -93,6 +94,9 @@ class ActionsDAG\n         /// If result of this not is deterministic. Checks only this node, not a subtree.\n         bool isDeterministic() const;\n         void toTree(JSONBuilder::JSONMap & map) const;\n+        size_t getHash() const;\n+    private:\n+        void updateHash(SipHash & hash_state) const;\n     };\n \n     /// NOTE: std::list is an implementation detail.\ndiff --git a/src/Interpreters/Cache/QueryConditionCache.cpp b/src/Interpreters/Cache/QueryConditionCache.cpp\nnew file mode 100644\nindex 000000000000..3ebccff0f036\n--- /dev/null\n+++ b/src/Interpreters/Cache/QueryConditionCache.cpp\n@@ -0,0 +1,101 @@\n+#include <Interpreters/Cache/QueryConditionCache.h>\n+#include <Storages/MergeTree/MergeTreeData.h>\n+#include \"Interpreters/Cache/FileSegmentInfo.h\"\n+\n+namespace ProfileEvents\n+{\n+    extern const Event QueryConditionCacheHits;\n+    extern const Event QueryConditionCacheMisses;\n+};\n+\n+namespace DB\n+{\n+\n+QueryConditionCache::QueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio)\n+    : cache(cache_policy, max_size_in_bytes, 0, size_ratio)\n+{\n+}\n+\n+std::optional<QueryConditionCache::MatchingMarks> QueryConditionCache::read(const UUID & table_id, const String & part_name, size_t condition_hash)\n+{\n+    Key key = {table_id, part_name, condition_hash};\n+\n+    if (auto entry = cache.get(key))\n+    {\n+        ProfileEvents::increment(ProfileEvents::QueryConditionCacheHits);\n+\n+        std::lock_guard lock(entry->mutex);\n+        return {entry->matching_marks};\n+    }\n+\n+    ProfileEvents::increment(ProfileEvents::QueryConditionCacheMisses);\n+\n+    return std::nullopt;\n+}\n+\n+void QueryConditionCache::write(const UUID & table_id, const String & part_name, size_t condition_hash, const MarkRanges & mark_ranges, size_t marks_count, bool has_final_mark)\n+{\n+    Key key = {table_id, part_name, condition_hash};\n+\n+    auto load_func = [&](){ return std::make_shared<Entry>(marks_count); };\n+    auto [entry, _] = cache.getOrSet(key, load_func);\n+\n+    chassert(marks_count == entry->matching_marks.size());\n+\n+    /// Set MarkRanges to false, so there is no need to read these marks again later.\n+    {\n+        std::lock_guard lock(entry->mutex);\n+        for (const auto & mark_range : mark_ranges)\n+            std::fill(entry->matching_marks.begin() + mark_range.begin, entry->matching_marks.begin() + mark_range.end, false);\n+\n+        if (has_final_mark)\n+            entry->matching_marks[marks_count - 1] = false;\n+\n+        LOG_DEBUG(\n+            logger,\n+            \"table_id: {}, part_name: {}, condition_hash: {}, marks_count: {}, has_final_mark: {}, (ranges: {})\",\n+            table_id,\n+            part_name,\n+            condition_hash,\n+            marks_count,\n+            has_final_mark,\n+            toString(mark_ranges));\n+    }\n+}\n+\n+void QueryConditionCache::clear()\n+{\n+    cache.clear();\n+}\n+\n+void QueryConditionCache::setMaxSizeInBytes(size_t max_size_in_bytes)\n+{\n+    cache.setMaxSizeInBytes(max_size_in_bytes);\n+}\n+\n+bool QueryConditionCache::Key::operator==(const Key & other) const\n+{\n+    return table_id == other.table_id && part_name == other.part_name && condition_hash == other.condition_hash;\n+}\n+\n+QueryConditionCache::Entry::Entry(size_t mark_count)\n+    : matching_marks(mark_count, true) /// by default, all marks potentially are potential matches\n+{\n+}\n+\n+size_t QueryConditionCache::KeyHasher::operator()(const Key & key) const\n+{\n+    SipHash hash;\n+    hash.update(key.table_id);\n+    hash.update(key.part_name);\n+    hash.update(key.condition_hash);\n+    return hash.get64();\n+}\n+\n+size_t QueryConditionCache::QueryConditionCacheEntryWeight::operator()(const Entry & entry) const\n+{\n+    /// Estimate the memory size of `std::vector<bool>`, for bool values, only 1 bit per element.\n+    size_t dynamic_memory = (entry.matching_marks.capacity() + 7) / 8; /// Round up to bytes.\n+    return sizeof(decltype(entry.matching_marks)) + dynamic_memory;\n+}\n+}\ndiff --git a/src/Interpreters/Cache/QueryConditionCache.h b/src/Interpreters/Cache/QueryConditionCache.h\nnew file mode 100644\nindex 000000000000..7fc4d14f0a36\n--- /dev/null\n+++ b/src/Interpreters/Cache/QueryConditionCache.h\n@@ -0,0 +1,66 @@\n+#pragma once\n+\n+#include <Common/CacheBase.h>\n+#include <Storages/MergeTree/MarkRange.h>\n+\n+namespace DB\n+{\n+\n+/// Cache the mark filter corresponding to the query condition,\n+/// which helps to quickly filter out useless Marks and speed up the query when the index is not hit.\n+class QueryConditionCache\n+{\n+public:\n+    /// 0 means none of the rows in the mark match the predicate. We can skip such marks.\n+    /// 1 means at least one row in the mark matches the predicate. We need to read such marks.\n+    using MatchingMarks = std::vector<bool>;\n+\n+    QueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio);\n+\n+    /// Read the filter and return empty if it does not exist.\n+    std::optional<MatchingMarks> read(const UUID & table_id, const String & part_name, size_t condition_hash);\n+\n+    /// Take out the mark filter corresponding to the query condition and set it to false on the corresponding mark.\n+    void write(const UUID & table_id, const String & part_name, size_t condition_hash, const MarkRanges & mark_ranges, size_t marks_count, bool has_final_mark);\n+\n+    void clear();\n+\n+    void setMaxSizeInBytes(size_t max_size_in_bytes);\n+\n+private:\n+    struct Key\n+    {\n+        const UUID table_id;\n+        const String part_name;\n+        const size_t condition_hash;\n+\n+        bool operator==(const Key & other) const;\n+    };\n+\n+    struct Entry\n+    {\n+        MatchingMarks matching_marks;\n+        std::mutex mutex;\n+\n+        explicit Entry(size_t mark_count);\n+    };\n+\n+    struct KeyHasher\n+    {\n+        size_t operator()(const Key & key) const;\n+    };\n+\n+    struct QueryConditionCacheEntryWeight\n+    {\n+        size_t operator()(const Entry & entry) const;\n+    };\n+\n+    using Cache = CacheBase<Key, Entry, KeyHasher, QueryConditionCacheEntryWeight>;\n+    Cache cache;\n+\n+    LoggerPtr logger = getLogger(\"QueryConditionCache\");\n+};\n+\n+using QueryConditionCachePtr = std::shared_ptr<QueryConditionCache>;\n+\n+}\ndiff --git a/src/Interpreters/Context.cpp b/src/Interpreters/Context.cpp\nindex c957482270bc..b7464792d33f 100644\n--- a/src/Interpreters/Context.cpp\n+++ b/src/Interpreters/Context.cpp\n@@ -54,9 +54,10 @@\n #include <Interpreters/ActionLocksManager.h>\n #include <Interpreters/ExternalLoaderXMLConfigRepository.h>\n #include <Interpreters/TemporaryDataOnDisk.h>\n-#include <Interpreters/Cache/QueryCache.h>\n #include <Interpreters/Cache/FileCacheFactory.h>\n #include <Interpreters/Cache/FileCache.h>\n+#include <Interpreters/Cache/QueryCache.h>\n+#include <Interpreters/Cache/QueryConditionCache.h>\n #include <Interpreters/SessionTracker.h>\n #include <Core/ServerSettings.h>\n #include <Interpreters/PreparedSets.h>\n@@ -442,6 +443,7 @@ struct ContextSharedPart : boost::noncopyable\n     mutable QueryCachePtr query_cache TSA_GUARDED_BY(mutex);                          /// Cache of query results.\n     mutable MarkCachePtr index_mark_cache TSA_GUARDED_BY(mutex);                      /// Cache of marks in compressed files of MergeTree indices.\n     mutable MMappedFileCachePtr mmap_cache TSA_GUARDED_BY(mutex);                     /// Cache of mmapped files to avoid frequent open/map/unmap/close and to reuse from several threads.\n+    mutable QueryConditionCachePtr query_condition_cache TSA_GUARDED_BY(mutex);       /// Mark filter for caching query conditions\n     AsynchronousMetrics * asynchronous_metrics TSA_GUARDED_BY(mutex) = nullptr;       /// Points to asynchronous metrics\n     mutable PageCachePtr page_cache TSA_GUARDED_BY(mutex);                            /// Userspace page cache.\n     ProcessList process_list;                                   /// Executing queries at the moment.\n@@ -3618,6 +3620,41 @@ void Context::clearQueryCache(const std::optional<String> & tag) const\n         cache->clear(tag);\n }\n \n+void Context::setQueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio)\n+{\n+    std::lock_guard lock(shared->mutex);\n+\n+    if (shared->query_condition_cache)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Mark filter cache has been already create.\");\n+\n+    shared->query_condition_cache = std::make_shared<QueryConditionCache>(cache_policy, max_size_in_bytes, size_ratio);\n+}\n+\n+QueryConditionCachePtr Context::getQueryConditionCache() const\n+{\n+    SharedLockGuard lock(shared->mutex);\n+    return shared->query_condition_cache;\n+}\n+\n+void Context::updateQueryConditionCacheConfiguration(const Poco::Util::AbstractConfiguration & config)\n+{\n+    std::lock_guard lock(shared->mutex);\n+\n+    if (!shared->query_condition_cache)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Query condition cache was not created yet.\");\n+\n+    size_t max_size_in_bytes = config.getUInt64(\"query_condition_cache_size\", DEFAULT_QUERY_CONDITION_CACHE_MAX_SIZE);\n+    shared->query_condition_cache->setMaxSizeInBytes(max_size_in_bytes);\n+}\n+\n+void Context::clearQueryConditionCache() const\n+{\n+    std::lock_guard lock(shared->mutex);\n+\n+    if (shared->query_condition_cache)\n+        shared->query_condition_cache->clear();\n+}\n+\n void Context::clearCaches() const\n {\n     std::lock_guard lock(shared->mutex);\n@@ -3651,6 +3688,10 @@ void Context::clearCaches() const\n     shared->mmap_cache->clear();\n \n     /// Intentionally not clearing the query cache which is transactionally inconsistent by design.\n+\n+    if (!shared->query_condition_cache)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Query condition cache was not created yet.\");\n+    shared->query_condition_cache->clear();\n }\n \n void Context::setAsynchronousMetrics(AsynchronousMetrics * asynchronous_metrics_)\ndiff --git a/src/Interpreters/Context.h b/src/Interpreters/Context.h\nindex bb86a73aad64..14962fe0791b 100644\n--- a/src/Interpreters/Context.h\n+++ b/src/Interpreters/Context.h\n@@ -106,6 +106,7 @@ struct Progress;\n struct FileProgress;\n class Clusters;\n class QueryCache;\n+class QueryConditionCache;\n class ISystemLog;\n class QueryLog;\n class QueryMetricLog;\n@@ -1128,6 +1129,11 @@ class Context: public ContextData, public std::enable_shared_from_this<Context>\n     std::shared_ptr<QueryCache> getQueryCache() const;\n     void clearQueryCache(const std::optional<String> & tag) const;\n \n+    void setQueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio);\n+    void updateQueryConditionCacheConfiguration(const Poco::Util::AbstractConfiguration & config);\n+    std::shared_ptr<QueryConditionCache> getQueryConditionCache() const;\n+    void clearQueryConditionCache() const;\n+\n     /** Clear the caches of the uncompressed blocks and marks.\n       * This is usually done when renaming tables, changing the type of columns, deleting a table.\n       *  - since caches are linked to file names, and become incorrect.\ndiff --git a/src/Interpreters/InterpreterSystemQuery.cpp b/src/Interpreters/InterpreterSystemQuery.cpp\nindex fc0f9594d0b9..53d3900bfce1 100644\n--- a/src/Interpreters/InterpreterSystemQuery.cpp\n+++ b/src/Interpreters/InterpreterSystemQuery.cpp\n@@ -414,7 +414,12 @@ BlockIO InterpreterSystemQuery::execute()\n             getContext()->clearQueryCache(query.query_cache_tag);\n             break;\n         }\n-\n+        case Type::DROP_QUERY_CONDITION_CACHE:\n+        {\n+            getContext()->checkAccess(AccessType::SYSTEM_DROP_QUERY_CONDITION_CACHE);\n+            getContext()->clearQueryConditionCache();\n+            break;\n+        }\n         case Type::DROP_COMPILED_EXPRESSION_CACHE:\n #if USE_EMBEDDED_COMPILER\n             getContext()->checkAccess(AccessType::SYSTEM_DROP_COMPILED_EXPRESSION_CACHE);\n@@ -1435,6 +1440,7 @@ AccessRightsElements InterpreterSystemQuery::getRequiredAccessForDDLOnCluster()\n         case Type::DROP_PRIMARY_INDEX_CACHE:\n         case Type::DROP_MMAP_CACHE:\n         case Type::DROP_QUERY_CACHE:\n+        case Type::DROP_QUERY_CONDITION_CACHE:\n         case Type::DROP_COMPILED_EXPRESSION_CACHE:\n         case Type::DROP_UNCOMPRESSED_CACHE:\n         case Type::DROP_INDEX_MARK_CACHE:\ndiff --git a/src/Parsers/ASTSystemQuery.cpp b/src/Parsers/ASTSystemQuery.cpp\nindex 43a100fc57ad..60d70cbec446 100644\n--- a/src/Parsers/ASTSystemQuery.cpp\n+++ b/src/Parsers/ASTSystemQuery.cpp\n@@ -423,6 +423,7 @@ void ASTSystemQuery::formatImpl(WriteBuffer & ostr, const FormatSettings & setti\n         case Type::DROP_CONNECTIONS_CACHE:\n         case Type::DROP_MMAP_CACHE:\n         case Type::DROP_QUERY_CACHE:\n+        case Type::DROP_QUERY_CONDITION_CACHE:\n         case Type::DROP_MARK_CACHE:\n         case Type::DROP_PRIMARY_INDEX_CACHE:\n         case Type::DROP_INDEX_MARK_CACHE:\ndiff --git a/src/Parsers/ASTSystemQuery.h b/src/Parsers/ASTSystemQuery.h\nindex fb580bf48194..4dcf7f4b8c54 100644\n--- a/src/Parsers/ASTSystemQuery.h\n+++ b/src/Parsers/ASTSystemQuery.h\n@@ -33,6 +33,7 @@ class ASTSystemQuery : public IAST, public ASTQueryWithOnCluster\n         DROP_SKIPPING_INDEX_CACHE,\n         DROP_MMAP_CACHE,\n         DROP_QUERY_CACHE,\n+        DROP_QUERY_CONDITION_CACHE,\n         DROP_COMPILED_EXPRESSION_CACHE,\n         DROP_FILESYSTEM_CACHE,\n         DROP_DISK_METADATA_CACHE,\ndiff --git a/src/Processors/Chunk.h b/src/Processors/Chunk.h\nindex e5802e3dcccc..e1a4f36f7165 100644\n--- a/src/Processors/Chunk.h\n+++ b/src/Processors/Chunk.h\n@@ -2,6 +2,8 @@\n \n #include <Columns/IColumn_fwd.h>\n #include <Common/CollectionOfDerived.h>\n+#include <Columns/IColumn.h>\n+#include <Storages/MergeTree/MarkRange.h>\n \n #include <memory>\n \n@@ -156,6 +158,24 @@ class AsyncInsertInfo : public ChunkInfoCloneable<AsyncInsertInfo>\n \n using AsyncInsertInfoPtr = std::shared_ptr<AsyncInsertInfo>;\n \n+class IMergeTreeDataPart;\n+\n+/// The query condition cache needs to know the mark ranges of which part the chunk data comes from.\n+class MarkRangesInfo : public ChunkInfoCloneable<MarkRangesInfo>\n+{\n+public:\n+    MarkRangesInfo(std::shared_ptr<const IMergeTreeDataPart> data_part_, MarkRanges mark_ranges_)\n+        : data_part(data_part_)\n+        , mark_ranges(std::move(mark_ranges_))\n+    {}\n+\n+    std::shared_ptr<const IMergeTreeDataPart> getDataPart() const { return data_part; }\n+    const MarkRanges & getMarkRanges() const { return mark_ranges; }\n+private:\n+    std::shared_ptr<const IMergeTreeDataPart> data_part;\n+    MarkRanges mark_ranges;\n+};\n+\n /// Converts all columns to full serialization in chunk.\n /// It's needed, when you have to access to the internals of the column,\n /// or when you need to perform operation with two columns\ndiff --git a/src/Processors/QueryPlan/FilterStep.cpp b/src/Processors/QueryPlan/FilterStep.cpp\nindex fd52a6b14ccf..983fe80c1b0c 100644\n--- a/src/Processors/QueryPlan/FilterStep.cpp\n+++ b/src/Processors/QueryPlan/FilterStep.cpp\n@@ -173,6 +173,10 @@ void FilterStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQ\n     pipeline.addSimpleTransform([&](const Block & header, QueryPipelineBuilder::StreamType stream_type)\n     {\n         bool on_totals = stream_type == QueryPipelineBuilder::StreamType::Totals;\n+        if (condition_hash)\n+            return std::make_shared<FilterTransform>(header, expression, filter_column_name, remove_filter_column,\n+                on_totals, nullptr, condition_hash);\n+\n         return std::make_shared<FilterTransform>(header, expression, filter_column_name, remove_filter_column, on_totals);\n     });\n \n@@ -248,6 +252,10 @@ void FilterStep::updateOutputHeader()\n         return;\n }\n \n+void FilterStep::setQueryConditionKey(size_t condition_hash_)\n+{\n+    condition_hash = condition_hash_;\n+}\n \n bool FilterStep::canUseType(const DataTypePtr & filter_type)\n {\ndiff --git a/src/Processors/QueryPlan/FilterStep.h b/src/Processors/QueryPlan/FilterStep.h\nindex 06d796a407e7..65f35af80770 100644\n--- a/src/Processors/QueryPlan/FilterStep.h\n+++ b/src/Processors/QueryPlan/FilterStep.h\n@@ -1,6 +1,7 @@\n #pragma once\n #include <Processors/QueryPlan/ITransformingStep.h>\n #include <Interpreters/ActionsDAG.h>\n+#include <Interpreters/Cache/QueryConditionCache.h>\n \n namespace DB\n {\n@@ -25,6 +26,7 @@ class FilterStep : public ITransformingStep\n     ActionsDAG & getExpression() { return actions_dag; }\n     const String & getFilterColumnName() const { return filter_column_name; }\n     bool removesFilterColumn() const { return remove_filter_column; }\n+    void setQueryConditionKey(size_t condition_hash_);\n \n     static bool canUseType(const DataTypePtr & type);\n \n@@ -38,6 +40,8 @@ class FilterStep : public ITransformingStep\n     ActionsDAG actions_dag;\n     String filter_column_name;\n     bool remove_filter_column;\n+\n+    std::optional<size_t> condition_hash;\n };\n \n }\ndiff --git a/src/Processors/QueryPlan/Optimizations/Optimizations.h b/src/Processors/QueryPlan/Optimizations/Optimizations.h\nindex 4273143f57dc..71549c5c4c68 100644\n--- a/src/Processors/QueryPlan/Optimizations/Optimizations.h\n+++ b/src/Processors/QueryPlan/Optimizations/Optimizations.h\n@@ -126,6 +126,7 @@ bool optimizeJoinLegacy(QueryPlan::Node & node, QueryPlan::Nodes &, const QueryP\n bool optimizeJoinLogical(QueryPlan::Node & node, QueryPlan::Nodes &, const QueryPlanOptimizationSettings &);\n bool convertLogicalJoinToPhysical(QueryPlan::Node & node, QueryPlan::Nodes &, const QueryPlanOptimizationSettings & optimization_settings);\n void optimizeDistinctInOrder(QueryPlan::Node & node, QueryPlan::Nodes &);\n+void tryUpdateQueryConditionCache(const QueryPlanOptimizationSettings & optimization_settings, const Stack & stack);\n \n /// A separate tree traverse to apply sorting properties after *InOrder optimizations.\n void applyOrder(const QueryPlanOptimizationSettings & optimization_settings, QueryPlan::Node & root);\ndiff --git a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp\nindex b35cb5bf50fa..00feae4f797b 100644\n--- a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp\n@@ -35,6 +35,8 @@ namespace Setting\n     extern const SettingsString force_optimize_projection_name;\n     extern const SettingsUInt64 max_limit_for_ann_queries;\n     extern const SettingsUInt64 query_plan_max_optimizations_to_apply;\n+    extern const SettingsBool use_query_condition_cache;\n+    extern const SettingsBool allow_experimental_analyzer;\n }\n \n QueryPlanOptimizationSettings::QueryPlanOptimizationSettings(const Settings & from)\n@@ -74,6 +76,7 @@ QueryPlanOptimizationSettings::QueryPlanOptimizationSettings(const Settings & fr\n \n     /// These settings comes from EXPLAIN settings not query settings and outside of the scope of this class\n     keep_logical_steps = false;\n+    use_query_condition_cache = from[Setting::use_query_condition_cache] && from[Setting::allow_experimental_analyzer];\n     is_explain = false;\n }\n \ndiff --git a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h\nindex 487adeb7fac2..2bc61ea6e7f9 100644\n--- a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h\n+++ b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h\n@@ -69,6 +69,9 @@ struct QueryPlanOptimizationSettings\n     size_t max_limit_for_ann_queries;\n \n     bool keep_logical_steps;\n+\n+    /// If query condition cache is enabled, the query condition cache needs to be updated in the WHERE stage.\n+    bool use_query_condition_cache = false;\n     bool is_explain;\n };\n \ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp b/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp\nindex 7d51203e71c6..5d3c28540ed3 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp\n@@ -123,6 +123,9 @@ void optimizeTreeSecondPass(const QueryPlanOptimizationSettings & optimization_s\n     while (!stack.empty())\n     {\n         optimizePrimaryKeyConditionAndLimit(stack);\n+\n+        tryUpdateQueryConditionCache(optimization_settings, stack);\n+\n         /// NOTE: optimizePrewhere can modify the stack.\n         /// Prewhere optimization relies on PK optimization (getConditionSelectivityEstimatorByPredicate)\n         if (optimization_settings.optimize_prewhere)\ndiff --git a/src/Processors/QueryPlan/Optimizations/tryUpdateQueryConditionCache.cpp b/src/Processors/QueryPlan/Optimizations/tryUpdateQueryConditionCache.cpp\nnew file mode 100644\nindex 000000000000..74a6fb9924a9\n--- /dev/null\n+++ b/src/Processors/QueryPlan/Optimizations/tryUpdateQueryConditionCache.cpp\n@@ -0,0 +1,39 @@\n+#include <Processors/QueryPlan/Optimizations/Optimizations.h>\n+#include <Processors/QueryPlan/FilterStep.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n+#include <Storages/VirtualColumnUtils.h>\n+\n+namespace DB::QueryPlanOptimizations\n+{\n+\n+void tryUpdateQueryConditionCache(const QueryPlanOptimizationSettings & optimization_settings, const Stack & stack)\n+{\n+    if (!optimization_settings.use_query_condition_cache)\n+        return;\n+\n+    const auto & frame = stack.back();\n+\n+    auto * read_from_merge_tree = dynamic_cast<ReadFromMergeTree *>(frame.node->step.get());\n+    if (!read_from_merge_tree)\n+        return;\n+\n+    const auto & query_info = read_from_merge_tree->getQueryInfo();\n+    const auto & filter_actions_dag = query_info.filter_actions_dag;\n+    if (!filter_actions_dag || query_info.isFinal())\n+        return;\n+\n+    if (!VirtualColumnUtils::isDeterministic(filter_actions_dag->getOutputs().front())) /// TODO check if front() still works for >1 condition\n+        return;\n+\n+    for (auto iter = stack.rbegin() + 1; iter != stack.rend(); ++iter)\n+    {\n+        if (auto * filter_step = typeid_cast<FilterStep *>(iter->node->step.get()))\n+        {\n+            size_t condition_hash = filter_actions_dag->getOutputs().front()->getHash();\n+            filter_step->setQueryConditionKey(condition_hash);\n+            return;\n+        }\n+    }\n+}\n+\n+}\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex 7e6a749ba9a0..f2f2da32fcd9 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -8,6 +8,7 @@\n #include <Interpreters/ExpressionActions.h>\n #include <Interpreters/InterpreterSelectQuery.h>\n #include <Interpreters/TreeRewriter.h>\n+#include <Interpreters/Cache/QueryConditionCache.h>\n #include <Parsers/ASTFunction.h>\n #include <Parsers/ASTIdentifier.h>\n #include <Parsers/ASTSelectQuery.h>\n@@ -180,6 +181,8 @@ namespace Setting\n     extern const SettingsBool query_plan_merge_filters;\n     extern const SettingsUInt64 merge_tree_min_read_task_size;\n     extern const SettingsBool read_in_order_use_virtual_row;\n+    extern const SettingsBool use_query_condition_cache;\n+    extern const SettingsBool allow_experimental_analyzer;\n     extern const SettingsBool merge_tree_use_deserialization_prefixes_cache;\n     extern const SettingsBool merge_tree_use_prefixes_deserialization_thread_pool;\n }\n@@ -212,6 +215,7 @@ static MergeTreeReaderSettings getMergeTreeReaderSettings(\n             && (settings[Setting::max_streams_to_max_threads_ratio] > 1 || settings[Setting::max_streams_for_merge_tree_reading] > 1),\n         .enable_multiple_prewhere_read_steps = settings[Setting::enable_multiple_prewhere_read_steps],\n         .force_short_circuit_execution = settings[Setting::query_plan_merge_filters],\n+        .use_query_condition_cache = settings[Setting::use_query_condition_cache] && settings[Setting::allow_experimental_analyzer],\n         .use_deserialization_prefixes_cache = settings[Setting::merge_tree_use_deserialization_prefixes_cache],\n         .use_prefixes_deserialization_thread_pool = settings[Setting::merge_tree_use_prefixes_deserialization_thread_pool],\n     };\n@@ -854,13 +858,15 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreams(RangesInDataParts && parts_\n         = settings[Setting::merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability];\n     std::bernoulli_distribution fault(read_split_ranges_into_intersecting_and_non_intersecting_injection_probability);\n \n+    /// When query condition cache is enabled, split ranges into intersecting will cause incorrect results and intersecting needs to be avoided.\n     if (read_type != ReadType::ParallelReplicas &&\n         num_streams > 1 &&\n         read_split_ranges_into_intersecting_and_non_intersecting_injection_probability > 0.0 &&\n         fault(thread_local_rng) &&\n         !isQueryWithFinal() &&\n         data.merging_params.is_deleted_column.empty() &&\n-        !prewhere_info)\n+        !prewhere_info &&\n+        !reader_settings.use_query_condition_cache)\n     {\n         NameSet column_names_set(column_names.begin(), column_names.end());\n         Names in_order_column_names_to_read(column_names);\n@@ -1794,6 +1800,8 @@ ReadFromMergeTree::AnalysisResultPtr ReadFromMergeTree::selectRangesToRead(\n             result.index_stats,\n             indexes->use_skip_indexes,\n             find_exact_ranges);\n+\n+        MergeTreeDataSelectExecutor::filterPartsByQueryConditionCache(result.parts_with_ranges, query_info_, context_, log);\n     }\n \n     size_t sum_marks_pk = total_marks_pk;\n@@ -2118,6 +2126,10 @@ void ReadFromMergeTree::initializePipeline(QueryPipelineBuilder & pipeline, cons\n \n     auto query_id_holder = MergeTreeDataSelectExecutor::checkLimits(data, result, context);\n \n+    /// When the query condition cache is enabled, the complete Mark needs to be read every time. When both where and prewhere are null, it is disabled.\n+    if (reader_settings.use_query_condition_cache && !query_info.prewhere_info && !query_info.filter_actions_dag)\n+        reader_settings.use_query_condition_cache = false;\n+\n     if (result.parts_with_ranges.empty())\n     {\n         pipeline.init(Pipe(std::make_shared<NullSource>(getOutputHeader())));\ndiff --git a/src/Processors/Transforms/FilterTransform.cpp b/src/Processors/Transforms/FilterTransform.cpp\nindex 756a42d40885..8786de55456c 100644\n--- a/src/Processors/Transforms/FilterTransform.cpp\n+++ b/src/Processors/Transforms/FilterTransform.cpp\n@@ -1,11 +1,14 @@\n #include <Processors/Transforms/FilterTransform.h>\n \n+#include <Interpreters/Context.h>\n #include <Interpreters/ExpressionActions.h>\n+#include <Interpreters/Cache/QueryConditionCache.h>\n #include <Columns/ColumnsCommon.h>\n #include <Core/Field.h>\n #include <DataTypes/DataTypeNullable.h>\n #include <DataTypes/DataTypeLowCardinality.h>\n #include <Processors/Merges/Algorithms/ReplacingSortedAlgorithm.h>\n+#include <Storages/MergeTree/MergeTreeData.h>\n \n namespace DB\n {\n@@ -43,7 +46,8 @@ FilterTransform::FilterTransform(\n     String filter_column_name_,\n     bool remove_filter_column_,\n     bool on_totals_,\n-    std::shared_ptr<std::atomic<size_t>> rows_filtered_)\n+    std::shared_ptr<std::atomic<size_t>> rows_filtered_,\n+    std::optional<size_t> condition_hash_)\n     : ISimpleTransform(\n             header_,\n             transformHeader(header_, expression_ ? &expression_->getActionsDAG() : nullptr, filter_column_name_, remove_filter_column_),\n@@ -53,6 +57,7 @@ FilterTransform::FilterTransform(\n     , remove_filter_column(remove_filter_column_)\n     , on_totals(on_totals_)\n     , rows_filtered(rows_filtered_)\n+    , condition_hash(condition_hash_)\n {\n     transformed_header = getInputPort().getHeader();\n     if (expression)\n@@ -62,6 +67,9 @@ FilterTransform::FilterTransform(\n     auto & column = transformed_header.getByPosition(filter_column_position).column;\n     if (column)\n         constant_filter_description = ConstantFilterDescription(*column);\n+\n+    if (condition_hash.has_value())\n+        query_condition_cache = Context::getGlobalContextInstance()->getQueryConditionCache();\n }\n \n IProcessor::Status FilterTransform::prepare()\n@@ -108,6 +116,26 @@ void FilterTransform::doTransform(Chunk & chunk)\n     auto columns = chunk.detachColumns();\n     DataTypes types;\n \n+    auto write_into_query_condition_cache = [&]()\n+    {\n+        if (!query_condition_cache)\n+            return;\n+\n+        auto mark_info = chunk.getChunkInfos().get<MarkRangesInfo>();\n+        if (!mark_info)\n+            return;\n+\n+        const auto & data_part = mark_info->getDataPart();\n+        auto storage_id = data_part->storage.getStorageID();\n+        query_condition_cache->write(\n+                storage_id.uuid,\n+                data_part->name,\n+                *condition_hash,\n+                mark_info->getMarkRanges(),\n+                data_part->index_granularity->getMarksCount(),\n+                data_part->index_granularity->hasFinalMark());\n+    };\n+\n     {\n         Block block = getInputPort().getHeader().cloneWithColumns(columns);\n         columns.clear();\n@@ -137,7 +165,10 @@ void FilterTransform::doTransform(Chunk & chunk)\n     constant_filter_description = ConstantFilterDescription(*filter_column);\n \n     if (constant_filter_description.always_false)\n+    {\n+        write_into_query_condition_cache();\n         return; /// Will finish at next prepare call\n+    }\n \n     if (constant_filter_description.always_true)\n     {\n@@ -184,8 +215,11 @@ void FilterTransform::doTransform(Chunk & chunk)\n \n     /// If the current block is completely filtered out, let's move on to the next one.\n     if (num_filtered_rows == 0)\n+    {\n+        write_into_query_condition_cache();\n         /// SimpleTransform will skip it.\n         return;\n+    }\n \n     /// If all the rows pass through the filter.\n     if (num_filtered_rows == num_rows_before_filtration)\ndiff --git a/src/Processors/Transforms/FilterTransform.h b/src/Processors/Transforms/FilterTransform.h\nindex 72d8a367a109..8662af593ad2 100644\n--- a/src/Processors/Transforms/FilterTransform.h\n+++ b/src/Processors/Transforms/FilterTransform.h\n@@ -9,6 +9,7 @@ class ExpressionActions;\n using ExpressionActionsPtr = std::shared_ptr<ExpressionActions>;\n \n class ActionsDAG;\n+class QueryConditionCache;\n \n /** Implements WHERE, HAVING operations.\n   * Takes an expression, which adds to the block one ColumnUInt8 column containing the filtering conditions.\n@@ -20,7 +21,8 @@ class FilterTransform : public ISimpleTransform\n public:\n     FilterTransform(\n         const Block & header_, ExpressionActionsPtr expression_, String filter_column_name_,\n-        bool remove_filter_column_, bool on_totals_ = false, std::shared_ptr<std::atomic<size_t>> rows_filtered_ = nullptr);\n+        bool remove_filter_column_, bool on_totals_ = false, std::shared_ptr<std::atomic<size_t>> rows_filtered_ = nullptr,\n+        std::optional<size_t> condition_hash_ = std::nullopt);\n \n     static Block\n     transformHeader(const Block & header, const ActionsDAG * expression, const String & filter_column_name, bool remove_filter_column);\n@@ -44,6 +46,10 @@ class FilterTransform : public ISimpleTransform\n \n     std::shared_ptr<std::atomic<size_t>> rows_filtered;\n \n+    /// If `condition_hash` is not null, the query condition cache needs to be updated at runtime.\n+    std::optional<size_t> condition_hash;\n+    std::shared_ptr<QueryConditionCache> query_condition_cache;\n+\n     /// Header after expression, but before removing filter column.\n     Block transformed_header;\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeReader.h b/src/Storages/MergeTree/IMergeTreeReader.h\nindex ad40ae01197c..e2b59a0da861 100644\n--- a/src/Storages/MergeTree/IMergeTreeReader.h\n+++ b/src/Storages/MergeTree/IMergeTreeReader.h\n@@ -66,6 +66,8 @@ class IMergeTreeReader : private boost::noncopyable\n \n     virtual void prefetchBeginOfRange(Priority) {}\n \n+    MergeTreeReaderSettings & getMergeTreeReaderSettings() { return settings; }\n+\n protected:\n     /// Returns true if requested column is a subcolumn with offsets of Array which is part of Nested column.\n     bool isSubcolumnOffsetsOfNested(const String & name_in_storage, const String & subcolumn_name) const;\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 0039ded87b11..e6afeedf8f2b 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -58,6 +58,7 @@\n #include <Interpreters/TransactionLog.h>\n #include <Interpreters/TreeRewriter.h>\n #include <Interpreters/inplaceBlockConversions.h>\n+#include <Interpreters/Cache/QueryConditionCache.h>\n #include <Parsers/ASTExpressionList.h>\n #include <Parsers/ASTIndexDeclaration.h>\n #include <Parsers/ASTHelpers.h>\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex a0ecb06fc170..0419318c67b2 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -82,6 +82,8 @@ namespace Setting\n     extern const SettingsUInt64 parallel_replica_offset;\n     extern const SettingsUInt64 parallel_replicas_count;\n     extern const SettingsParallelReplicasMode parallel_replicas_mode;\n+    extern const SettingsBool use_query_condition_cache;\n+    extern const SettingsBool allow_experimental_analyzer;\n     extern const SettingsBool parallel_replicas_local_plan;\n     extern const SettingsBool parallel_replicas_index_analysis_only_on_coordinator;\n }\n@@ -890,6 +892,103 @@ RangesInDataParts MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipInd\n     return parts_with_ranges;\n }\n \n+void MergeTreeDataSelectExecutor::filterPartsByQueryConditionCache(\n+    RangesInDataParts & parts_with_ranges,\n+    const SelectQueryInfo & select_query_info,\n+    const ContextPtr & context,\n+    LoggerPtr log)\n+{\n+    const auto & settings = context->getSettingsRef();\n+    if (!settings[Setting::use_query_condition_cache] || !settings[Setting::allow_experimental_analyzer] ||\n+        (!select_query_info.prewhere_info && !select_query_info.filter_actions_dag))\n+        return;\n+\n+    QueryConditionCachePtr query_condition_cache = context->getQueryConditionCache();\n+\n+    struct Stat\n+    {\n+        size_t total_granules{0};\n+        size_t granules_dropped{0};\n+    };\n+\n+    auto drop_mark_ranges =[&](const ActionsDAG::Node * dag) -> Stat\n+    {\n+        size_t condition_hash = dag->getHash();\n+        Stat stat;\n+        for (auto it = parts_with_ranges.begin(); it != parts_with_ranges.end();)\n+        {\n+            auto & part_with_ranges = *it;\n+            stat.total_granules += part_with_ranges.getMarksCount();\n+\n+            auto & data_part = part_with_ranges.data_part;\n+            auto storage_id = data_part->storage.getStorageID();\n+            auto matching_marks_opt = query_condition_cache->read(storage_id.uuid, data_part->name, condition_hash);\n+            if (!matching_marks_opt)\n+            {\n+                ++it;\n+                continue;\n+            }\n+\n+            auto & matching_marks = *matching_marks_opt;\n+            MarkRanges ranges;\n+            for (auto & mark_range : part_with_ranges.ranges)\n+            {\n+                size_t begin = mark_range.begin;\n+                for (size_t mark_it = begin; mark_it < mark_range.end; ++mark_it)\n+                {\n+                    if (!matching_marks[mark_it])\n+                    {\n+                        ++stat.granules_dropped;\n+                        if (mark_it == begin)\n+                            ++begin;\n+                        else\n+                        {\n+                            ranges.emplace_back(begin, mark_it);\n+                            begin = mark_it + 1;\n+                        }\n+                    }\n+                }\n+\n+                if (begin != mark_range.begin && begin != mark_range.end)\n+                    ranges.emplace_back(begin, mark_range.end);\n+                else if (begin == mark_range.begin)\n+                    ranges.emplace_back(begin, mark_range.end);\n+            }\n+\n+            if (ranges.empty())\n+                it = parts_with_ranges.erase(it);\n+            else\n+            {\n+                part_with_ranges.ranges = ranges;\n+                ++it;\n+            }\n+        }\n+\n+        return stat;\n+    };\n+\n+    if (const auto & prewhere_info = select_query_info.prewhere_info)\n+    {\n+        for (const auto * dag : prewhere_info->prewhere_actions.getOutputs())\n+        {\n+            if (dag->result_name == prewhere_info->prewhere_column_name)\n+            {\n+                auto stat = drop_mark_ranges(dag);\n+                LOG_DEBUG(log, \"PREWHERE contition {} by query condition cache has dropped {}/{} granules.\", prewhere_info->prewhere_column_name, stat.granules_dropped, stat.total_granules);\n+                break;\n+            }\n+        }\n+    }\n+\n+    if (const auto & filter_actions_dag = select_query_info.filter_actions_dag)\n+    {\n+        const auto * dag = filter_actions_dag->getOutputs().front();\n+        auto stat = drop_mark_ranges(dag);\n+        LOG_DEBUG(log, \"WHERE condition {} by query condition cache has dropped {}/{} granules.\", filter_actions_dag->getOutputs().front()->result_name, stat.granules_dropped, stat.total_granules);\n+    }\n+}\n+\n+\n std::shared_ptr<QueryIdHolder> MergeTreeDataSelectExecutor::checkLimits(\n     const MergeTreeData & data,\n     const ReadFromMergeTree::AnalysisResult & result,\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\nindex 5f6d4ddbabcf..182bbc4a1d33 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n@@ -201,6 +201,13 @@ class MergeTreeDataSelectExecutor\n         bool use_skip_indexes,\n         bool find_exact_ranges);\n \n+    /// If WHERE or PREWHERE condition is deterministic, try to use query condition cache to filter parts, delete invalid mark ranges.\n+    static void filterPartsByQueryConditionCache(\n+        RangesInDataParts & parts_with_ranges,\n+        const SelectQueryInfo & select_query_info,\n+        const ContextPtr & context,\n+        LoggerPtr log);\n+\n     /// Create expression for sampling.\n     /// Also, calculate _sample_factor if needed.\n     /// Also, update key condition with selected sampling range.\ndiff --git a/src/Storages/MergeTree/MergeTreeIOSettings.h b/src/Storages/MergeTree/MergeTreeIOSettings.h\nindex 16319234ee86..72d6d1bc3333 100644\n--- a/src/Storages/MergeTree/MergeTreeIOSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeIOSettings.h\n@@ -52,6 +52,8 @@ struct MergeTreeReaderSettings\n     bool adjust_read_buffer_size = true;\n     /// If true, it's allowed to read the whole part without reading marks.\n     bool can_read_part_without_marks = false;\n+    /// Prewhere condition filtered marks is written to the mark filter cache.\n+    bool use_query_condition_cache = false;\n     bool use_deserialization_prefixes_cache = false;\n     bool use_prefixes_deserialization_thread_pool = false;\n };\ndiff --git a/src/Storages/MergeTree/MergeTreeRangeReader.cpp b/src/Storages/MergeTree/MergeTreeRangeReader.cpp\nindex 7b366ac81c15..79dc490db1dd 100644\n--- a/src/Storages/MergeTree/MergeTreeRangeReader.cpp\n+++ b/src/Storages/MergeTree/MergeTreeRangeReader.cpp\n@@ -948,33 +948,40 @@ MergeTreeRangeReader::ReadResult MergeTreeRangeReader::startReadingChain(size_t\n     /// end offsets to properly fill _part_offset column.\n     UInt64 leading_begin_part_offset = 0;\n     UInt64 leading_end_part_offset = 0;\n-\n+    std::optional<size_t> current_mark;\n     if (!stream.isFinished())\n     {\n         leading_begin_part_offset = stream.currentPartOffset();\n         leading_end_part_offset = stream.lastPartOffset();\n+        current_mark = stream.current_mark;\n     }\n \n     /// Stream is lazy. result.num_added_rows is the number of rows added to block which is not equal to\n     /// result.num_rows_read until call to stream.finalize(). Also result.num_added_rows may be less than\n     /// result.num_rows_read if the last granule in range also the last in part (so we have to adjust last granule).\n     {\n+        bool use_query_condition_cache = merge_tree_reader->getMergeTreeReaderSettings().use_query_condition_cache;\n         size_t space_left = max_rows;\n         while (space_left && (!stream.isFinished() || !ranges.empty()))\n         {\n             if (stream.isFinished())\n             {\n                 result.addRows(stream.finalize(result.columns));\n+                if (current_mark && *current_mark < stream.last_mark)\n+                    result.addReadRange(MarkRange{*current_mark, stream.last_mark});\n+\n                 stream = Stream(ranges.front().begin, ranges.front().end, current_task_last_mark, merge_tree_reader);\n                 result.addRange(ranges.front());\n                 ranges.pop_front();\n+                current_mark = stream.current_mark;\n             }\n \n             size_t current_space = space_left;\n \n             /// If reader can't read part of granule, we have to increase number of reading rows\n             ///  to read complete granules and exceed max_rows a bit.\n-            if (!merge_tree_reader->canReadIncompleteGranules())\n+            /// When using query condition cache, you need to ensure that the read Mark is complete.\n+            if (use_query_condition_cache || !merge_tree_reader->canReadIncompleteGranules())\n                 current_space = stream.ceilRowsToCompleteGranules(space_left);\n \n             auto rows_to_read = std::min(current_space, stream.numPendingRowsInCurrentGranule());\n@@ -987,6 +994,9 @@ MergeTreeRangeReader::ReadResult MergeTreeRangeReader::startReadingChain(size_t\n     }\n \n     result.addRows(stream.finalize(result.columns));\n+    size_t last_mark = stream.isFinished() ? stream.last_mark : stream.current_mark;\n+    if (current_mark && current_mark < last_mark)\n+        result.addReadRange(MarkRange{*current_mark, last_mark});\n \n     /// Last granule may be incomplete.\n     if (!result.rows_per_granule.empty())\ndiff --git a/src/Storages/MergeTree/MergeTreeRangeReader.h b/src/Storages/MergeTree/MergeTreeRangeReader.h\nindex 0e0429954058..3c7a03a54c42 100644\n--- a/src/Storages/MergeTree/MergeTreeRangeReader.h\n+++ b/src/Storages/MergeTree/MergeTreeRangeReader.h\n@@ -237,6 +237,9 @@ class MergeTreeRangeReader\n         Columns columns;\n         size_t num_rows = 0;\n \n+        /// All read marks.\n+        MarkRanges read_mark_ranges;\n+\n         /// The number of rows were added to block as a result of reading chain.\n         size_t numReadRows() const { return num_read_rows; }\n         /// The number of bytes read from disk.\n@@ -264,6 +267,7 @@ class MergeTreeRangeReader\n         void adjustLastGranule();\n         void addRows(size_t rows) { num_read_rows += rows; }\n         void addRange(const MarkRange & range) { started_ranges.push_back({rows_per_granule.size(), range}); }\n+        void addReadRange(MarkRange mark_range) { read_mark_ranges.push_back(std::move(mark_range)); }\n \n         /// Add current step filter to the result and then for each granule calculate the number of filtered rows at the end.\n         /// Remove them and update filter.\ndiff --git a/src/Storages/MergeTree/MergeTreeReadTask.cpp b/src/Storages/MergeTree/MergeTreeReadTask.cpp\nindex 827fb8b2e3f5..f67ce7caa295 100644\n--- a/src/Storages/MergeTree/MergeTreeReadTask.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReadTask.cpp\n@@ -204,6 +204,7 @@ MergeTreeReadTask::BlockAndProgress MergeTreeReadTask::read()\n \n     BlockAndProgress res = {\n         .block = std::move(block),\n+        .read_mark_ranges = read_result.read_mark_ranges,\n         .row_count = read_result.num_rows,\n         .num_read_rows = num_read_rows,\n         .num_read_bytes = num_read_bytes };\n@@ -211,4 +212,9 @@ MergeTreeReadTask::BlockAndProgress MergeTreeReadTask::read()\n     return res;\n }\n \n+void MergeTreeReadTask::addPreWhereUnmatchedMarks(MarkRanges & mark_ranges_)\n+{\n+    prewhere_unmatched_marks.insert(prewhere_unmatched_marks.end(), mark_ranges_.begin(), mark_ranges_.end());\n+}\n+\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeReadTask.h b/src/Storages/MergeTree/MergeTreeReadTask.h\nindex 9a481c7ddce7..08b651b4db83 100644\n--- a/src/Storages/MergeTree/MergeTreeReadTask.h\n+++ b/src/Storages/MergeTree/MergeTreeReadTask.h\n@@ -118,6 +118,7 @@ struct MergeTreeReadTask : private boost::noncopyable\n     struct BlockAndProgress\n     {\n         Block block;\n+        MarkRanges read_mark_ranges;\n         size_t row_count = 0;\n         size_t num_read_rows = 0;\n         size_t num_read_bytes = 0;\n@@ -139,6 +140,9 @@ struct MergeTreeReadTask : private boost::noncopyable\n     const MergeTreeReadersChain & getReadersChain() const { return readers_chain; }\n     const IMergeTreeReader & getMainReader() const { return *readers.main; }\n \n+    void addPreWhereUnmatchedMarks(MarkRanges & mark_ranges_);\n+    const MarkRanges & getPreWhereUnmatchedMarks() { return prewhere_unmatched_marks; }\n+\n     Readers releaseReaders() { return std::move(readers); }\n \n     static Readers createReaders(const MergeTreeReadTaskInfoPtr & read_info, const Extras & extras, const MarkRanges & ranges);\n@@ -160,6 +164,9 @@ struct MergeTreeReadTask : private boost::noncopyable\n     /// Ranges to read from data_part\n     MarkRanges mark_ranges;\n \n+    /// There is no mark matching a row of data under the prewhere condition.\n+    MarkRanges prewhere_unmatched_marks;\n+\n     BlockSizeParams block_size_params;\n \n     /// Used to satistfy preferred_block_size_bytes limitation\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\nindex c58265739925..6875484d09a4 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n@@ -15,6 +15,7 @@\n #include <Processors/QueryPlan/SourceStepWithFilter.h>\n #include <Processors/Transforms/AggregatingTransform.h>\n #include <Storages/MergeTree/MergeTreeVirtualColumns.h>\n+#include <Interpreters/Cache/QueryConditionCache.h>\n #include <city.h>\n \n namespace\n@@ -163,7 +164,29 @@ ChunkAndProgress MergeTreeSelectProcessor::read()\n         try\n         {\n             if (!task || algorithm->needNewTask(*task))\n+            {\n+                if (task && prewhere_info && reader_settings.use_query_condition_cache)\n+                {\n+                    for (const auto * dag : prewhere_info->prewhere_actions.getOutputs())\n+                    {\n+                        if (dag->result_name == prewhere_info->prewhere_column_name)\n+                        {\n+                            auto data_part = task->getInfo().data_part;\n+                            auto storage_id = data_part->storage.getStorageID();\n+                            auto query_condition_cache = Context::getGlobalContextInstance()->getQueryConditionCache();\n+                            query_condition_cache->write(storage_id.uuid,\n+                                data_part->name,\n+                                dag->getHash(),\n+                                task->getPreWhereUnmatchedMarks(),\n+                                data_part->index_granularity->getMarksCount(),\n+                                data_part->index_granularity->hasFinalMark());\n+                            break;\n+                        }\n+                    }\n+                }\n+\n                 task = algorithm->getNewTask(*pool, task.get());\n+            }\n \n             if (!task)\n                 break;\n@@ -192,8 +215,20 @@ ChunkAndProgress MergeTreeSelectProcessor::read()\n             }\n \n             auto chunk = Chunk(ordered_columns, res.row_count);\n+            const auto & data_part = task->getInfo().data_part;\n             if (add_part_level)\n-                chunk.getChunkInfos().add(std::make_shared<MergeTreeReadInfo>(task->getInfo().data_part->info.level));\n+                chunk.getChunkInfos().add(std::make_shared<MergeTreeReadInfo>(data_part->info.level));\n+\n+            if (reader_settings.use_query_condition_cache)\n+            {\n+                chunk.getChunkInfos().add(std::make_shared<MarkRangesInfo>(data_part, res.read_mark_ranges));\n+                LOG_DEBUG(\n+                    log,\n+                    \"Chunk mark ranges info, part_name: {}, num_read_rows: {}, ranges: {}\",\n+                    data_part->name,\n+                    res.num_read_rows,\n+                    toString(res.read_mark_ranges));\n+            }\n \n             return ChunkAndProgress{\n                 .chunk = std::move(chunk),\n@@ -201,6 +236,8 @@ ChunkAndProgress MergeTreeSelectProcessor::read()\n                 .num_read_bytes = res.num_read_bytes,\n                 .is_finished = false};\n         }\n+        if (reader_settings.use_query_condition_cache && prewhere_info)\n+            task->addPreWhereUnmatchedMarks(res.read_mark_ranges);\n \n         return {Chunk(), res.num_read_rows, res.num_read_bytes, false};\n     }\ndiff --git a/src/Storages/VirtualColumnUtils.cpp b/src/Storages/VirtualColumnUtils.cpp\nindex dd03f200d81c..ce353204e1d3 100644\n--- a/src/Storages/VirtualColumnUtils.cpp\n+++ b/src/Storages/VirtualColumnUtils.cpp\n@@ -358,6 +358,23 @@ static bool canEvaluateSubtree(const ActionsDAG::Node * node, const Block * allo\n     return true;\n }\n \n+bool isDeterministic(const ActionsDAG::Node * node)\n+{\n+    for (const auto * child : node->children)\n+    {\n+        if (!isDeterministic(child))\n+            return false;\n+    }\n+\n+    if (node->type != ActionsDAG::ActionType::FUNCTION)\n+        return true;\n+\n+    if (!node->function_base->isDeterministic())\n+        return false;\n+\n+    return true;\n+}\n+\n bool isDeterministicInScopeOfQuery(const ActionsDAG::Node * node)\n {\n     for (const auto * child : node->children)\ndiff --git a/src/Storages/VirtualColumnUtils.h b/src/Storages/VirtualColumnUtils.h\nindex 0196b246a17b..67db9cf42acc 100644\n--- a/src/Storages/VirtualColumnUtils.h\n+++ b/src/Storages/VirtualColumnUtils.h\n@@ -43,7 +43,10 @@ void filterBlockWithExpression(const ExpressionActionsPtr & actions, Block & blo\n /// Builds sets used by ActionsDAG inplace.\n void buildSetsForDAG(const ActionsDAG & dag, const ContextPtr & context);\n \n-/// Recursively checks if all functions used in DAG are deterministic in scope of query.\n+/// Checks if all functions used in DAG are deterministic.\n+bool isDeterministic(const ActionsDAG::Node * node);\n+\n+/// Checks recursively if all functions used in DAG are deterministic in scope of query.\n bool isDeterministicInScopeOfQuery(const ActionsDAG::Node * node);\n \n /// Extract a part of predicate that can be evaluated using only columns from input_names.\n",
  "test_patch": "diff --git a/tests/clickhouse-test b/tests/clickhouse-test\nindex 8a4b867b9626..7bf0a6f5e13d 100755\n--- a/tests/clickhouse-test\n+++ b/tests/clickhouse-test\n@@ -1086,6 +1086,7 @@ class SettingsRandomizer:\n         \"query_plan_join_swap_table\": lambda: random.choice([\"auto\", \"false\", \"true\"]),\n         \"enable_vertical_final\": lambda: random.randint(0, 1),\n         \"optimize_extract_common_expressions\": lambda: random.randint(0, 1),\n+        \"use_query_condition_cache\": lambda: random.randint(0, 1),\n         **randomize_external_sort_group_by(),\n     }\n \ndiff --git a/tests/queries/0_stateless/00837_minmax_index.sh b/tests/queries/0_stateless/00837_minmax_index.sh\nindex ff487f50ee04..d11cd960c4c5 100755\n--- a/tests/queries/0_stateless/00837_minmax_index.sh\n+++ b/tests/queries/0_stateless/00837_minmax_index.sh\n@@ -41,11 +41,11 @@ $CLICKHOUSE_CLIENT --query=\"INSERT INTO minmax_idx VALUES\n (12, 5, 4.7, 6.5, 'cba', 'b', '2015-01-01')\"\n \n # simple select\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n # select with hole made by primary key\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt SETTINGS use_query_condition_cache = 0\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n $CLICKHOUSE_CLIENT --query=\"DROP TABLE minmax_idx\"\ndiff --git a/tests/queries/0_stateless/00838_unique_index.sh b/tests/queries/0_stateless/00838_unique_index.sh\nindex a3aba4f26b6c..edec41b5711f 100755\n--- a/tests/queries/0_stateless/00838_unique_index.sh\n+++ b/tests/queries/0_stateless/00838_unique_index.sh\n@@ -39,18 +39,18 @@ $CLICKHOUSE_CLIENT --query=\"INSERT INTO set_idx VALUES\n (13, 5, 4.7, 6.5, 'cba', 'b', '2015-01-01')\"\n \n # simple select\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE NOT (i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba'))  ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE NOT (i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba')) ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE NOT (i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba'))  ORDER BY dt SETTINGS use_query_condition_cache = 0\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE NOT (i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba')) ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n # select with hole made by primary key\n $CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE (u64 < 2 OR u64 > 10) AND s != 'cba' ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE (u64 < 2 OR u64 > 10) AND s != 'cba' ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE (u64 < 2 OR u64 > 10) AND s != 'cba' ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE (u64 < 2 OR NOT u64 > 10) AND NOT (s = 'cba') ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE (u64 < 2 OR NOT u64 > 10) AND NOT (s = 'cba') ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE (u64 < 2 OR NOT u64 > 10) AND NOT (s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM set_idx WHERE (u64 < 2 OR NOT u64 > 10) AND NOT (s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n \n $CLICKHOUSE_CLIENT --query=\"DROP TABLE set_idx;\"\ndiff --git a/tests/queries/0_stateless/01055_minmax_index_compact_parts.sh b/tests/queries/0_stateless/01055_minmax_index_compact_parts.sh\nindex 29ce4da02edd..211d86e2cdd9 100755\n--- a/tests/queries/0_stateless/01055_minmax_index_compact_parts.sh\n+++ b/tests/queries/0_stateless/01055_minmax_index_compact_parts.sh\n@@ -43,11 +43,11 @@ $CLICKHOUSE_CLIENT --query=\"INSERT INTO minmax_idx VALUES\n (12, 5, 4.7, 6.5, 'cba', 'b', '2015-01-01')\"\n \n # simple select\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE i32 = 5 AND i32 + f64 < 12 AND 3 < d AND d < 7 AND (s = 'bac' OR s = 'cba') ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n # select with hole made by primary key\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt\"\n-$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt FORMAT JSON\" | grep \"rows_read\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt SETTINGS use_query_condition_cache = 0\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM minmax_idx WHERE (u64 < 2 OR u64 > 10) AND e != 'b' ORDER BY dt SETTINGS use_query_condition_cache = 0 FORMAT JSON\" | grep \"rows_read\"\n \n $CLICKHOUSE_CLIENT --query=\"DROP TABLE minmax_idx\"\ndiff --git a/tests/queries/0_stateless/01271_show_privileges.reference b/tests/queries/0_stateless/01271_show_privileges.reference\nindex ba91e23e4ce7..9d7c297828d4 100644\n--- a/tests/queries/0_stateless/01271_show_privileges.reference\n+++ b/tests/queries/0_stateless/01271_show_privileges.reference\n@@ -122,6 +122,7 @@ SYSTEM DROP UNCOMPRESSED CACHE\t['SYSTEM DROP UNCOMPRESSED','DROP UNCOMPRESSED CA\n SYSTEM DROP SKIPPING INDEX CACHE\t['SYSTEM DROP SKIPPING INDEX CACHE','DROP SKIPPING INDEX CACHE']\tGLOBAL\tSYSTEM DROP CACHE\n SYSTEM DROP MMAP CACHE\t['SYSTEM DROP MMAP','DROP MMAP CACHE','DROP MMAP']\tGLOBAL\tSYSTEM DROP CACHE\n SYSTEM DROP QUERY CACHE\t['SYSTEM DROP QUERY','DROP QUERY CACHE','DROP QUERY']\tGLOBAL\tSYSTEM DROP CACHE\n+SYSTEM DROP QUERY CONDITION CACHE\t['SYSTEM DROP QUERY CONDITION','DROP QUERY CONDITION CACHE','DROP QUERY CONDITION']\tGLOBAL\tSYSTEM DROP CACHE\n SYSTEM DROP COMPILED EXPRESSION CACHE\t['SYSTEM DROP COMPILED EXPRESSION','DROP COMPILED EXPRESSION CACHE','DROP COMPILED EXPRESSIONS']\tGLOBAL\tSYSTEM DROP CACHE\n SYSTEM DROP FILESYSTEM CACHE\t['SYSTEM DROP FILESYSTEM CACHE','DROP FILESYSTEM CACHE']\tGLOBAL\tSYSTEM DROP CACHE\n SYSTEM DROP DISTRIBUTED CACHE\t['SYSTEM DROP DISTRIBUTED CACHE','DROP DISTRIBUTED CACHE']\tGLOBAL\tSYSTEM DROP CACHE\ndiff --git a/tests/queries/0_stateless/02784_parallel_replicas_automatic_decision.sh b/tests/queries/0_stateless/02784_parallel_replicas_automatic_decision.sh\nindex f25619437478..77a52da6c0f3 100755\n--- a/tests/queries/0_stateless/02784_parallel_replicas_automatic_decision.sh\n+++ b/tests/queries/0_stateless/02784_parallel_replicas_automatic_decision.sh\n@@ -61,36 +61,36 @@ function run_query_with_pure_parallel_replicas () {\n query_id_base=\"02784_automatic_parallel_replicas-$CLICKHOUSE_DATABASE\"\n \n #### Reading 10M rows without filters\n-whole_table_query=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count format Null\"\n+whole_table_query=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count SETTINGS use_query_condition_cache = 0 format Null\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_whole_table_10M\" 10000000 \"$whole_table_query\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_whole_table_6M\" 6000000 \"$whole_table_query\" # 1.6 replicas -> 1 replica -> No parallel replicas\n run_query_with_pure_parallel_replicas \"${query_id_base}_whole_table_5M\" 5000000 \"$whole_table_query\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_whole_table_1M\" 1000000 \"$whole_table_query\"\n \n ##### Reading 2M rows without filters as partition (p=3) is pruned completely\n-query_with_partition_pruning=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count WHERE p != 3 format Null\"\n+query_with_partition_pruning=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count WHERE p != 3 SETTINGS use_query_condition_cache = 0 format Null\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_pruning_10M\" 10000000 \"$query_with_partition_pruning\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_pruning_1M\" 1000000 \"$query_with_partition_pruning\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_pruning_500k\" 500000 \"$query_with_partition_pruning\"\n \n #### Reading ~500k rows as index filter should prune granules from partition=1 and partition=2, and drop p3 completely\n-query_with_index=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count WHERE number < 500_000 format Null\"\n+query_with_index=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count WHERE number < 500_000 SETTINGS use_query_condition_cache = 0 format Null\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_index_1M\" 1000000 \"$query_with_index\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_index_200k\" 200000 \"$query_with_index\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_index_100k\" 100000 \"$query_with_index\"\n \n #### Reading 1M (because of LIMIT)\n-limit_table_query=\"SELECT sum(number) FROM (SELECT number FROM test_parallel_replicas_automatic_count LIMIT 1_000_000) format Null\"\n+limit_table_query=\"SELECT sum(number) FROM (SELECT number FROM test_parallel_replicas_automatic_count LIMIT 1_000_000) SETTINGS use_query_condition_cache = 0 format Null\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_limit_10M\" 10000000 \"$limit_table_query\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_limit_1M\" 1000000 \"$limit_table_query\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_limit_500k\" 500000 \"$limit_table_query\"\n \n #### Reading 10M (because of LIMIT is applied after aggregations)\n-limit_agg_table_query=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count LIMIT 1_000_000 format Null\"\n+limit_agg_table_query=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count LIMIT 1_000_000 SETTINGS use_query_condition_cache = 0 format Null\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_useless_limit_500k\" 500000 \"$limit_agg_table_query\"\n \n #### If the filter does not help, it shouldn't disable parallel replicas. Table has 10M rows, filter removes all rows\n-helpless_filter_query=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count WHERE number + p = 42 format Null\"\n+helpless_filter_query=\"SELECT sum(number) FROM test_parallel_replicas_automatic_count WHERE number + p = 42 SETTINGS use_query_condition_cache = 0 format Null\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_helpless_filter_10M\" 10000000 \"$helpless_filter_query\"\n run_query_with_pure_parallel_replicas \"${query_id_base}_helpless_filter_5M\" 5000000 \"$helpless_filter_query\"\n \ndiff --git a/tests/queries/0_stateless/03031_read_in_order_optimization_with_virtual_row.sql b/tests/queries/0_stateless/03031_read_in_order_optimization_with_virtual_row.sql\nindex 0f1002878155..8ad8a88a4163 100644\n--- a/tests/queries/0_stateless/03031_read_in_order_optimization_with_virtual_row.sql\n+++ b/tests/queries/0_stateless/03031_read_in_order_optimization_with_virtual_row.sql\n@@ -1,5 +1,6 @@\n \n SET read_in_order_use_virtual_row = 1;\n+SET use_query_condition_cache = 0;\n \n DROP TABLE IF EXISTS t;\n \ndiff --git a/tests/queries/0_stateless/03229_query_condition_cache.reference b/tests/queries/0_stateless/03229_query_condition_cache.reference\nnew file mode 100644\nindex 000000000000..079d63832267\n--- /dev/null\n+++ b/tests/queries/0_stateless/03229_query_condition_cache.reference\n@@ -0,0 +1,8 @@\n+1\n+0\t1\t0\n+10000\t10000\n+1\t1\t1\n+1\n+0\t1\t0\n+10000\t10000\n+1\t0\t1\ndiff --git a/tests/queries/0_stateless/03229_query_condition_cache.sql b/tests/queries/0_stateless/03229_query_condition_cache.sql\nnew file mode 100644\nindex 000000000000..5a3068d085f6\n--- /dev/null\n+++ b/tests/queries/0_stateless/03229_query_condition_cache.sql\n@@ -0,0 +1,76 @@\n+-- Tags: no-parallel, no-parallel-replicas\n+\n+SET allow_experimental_analyzer = 1;\n+\n+SYSTEM DROP QUERY CONDITION CACHE;\n+\n+DROP TABLE IF EXISTS tab;\n+\n+CREATE TABLE tab (a Int64, b Int64) ENGINE = MergeTree ORDER BY a;\n+INSERT INTO tab SELECT number, number FROM numbers(1000000);\n+\n+SELECT count(*) FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true;\n+\n+SYSTEM FLUSH LOGS;\n+SELECT\n+    ProfileEvents['QueryConditionCacheHits'],\n+    ProfileEvents['QueryConditionCacheMisses'],\n+    toInt32(ProfileEvents['SelectedMarks']) < toInt32(ProfileEvents['SelectedMarksTotal'])\n+FROM system.query_log\n+WHERE\n+    type = 'QueryFinish'\n+    AND current_database = currentDatabase()\n+    AND query = 'SELECT count(*) FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true;'\n+ORDER BY\n+    event_time_microseconds;\n+\n+SELECT * FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true;\n+\n+SYSTEM FLUSH LOGS;\n+SELECT\n+    ProfileEvents['QueryConditionCacheHits'],\n+    ProfileEvents['QueryConditionCacheMisses'],\n+    toInt32(ProfileEvents['SelectedMarks']) < toInt32(ProfileEvents['SelectedMarksTotal'])\n+FROM system.query_log\n+WHERE\n+    type = 'QueryFinish'\n+    AND current_database = currentDatabase()\n+    AND query = 'SELECT * FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true;'\n+ORDER BY\n+    event_time_microseconds;\n+\n+-- Now test without move to PREWHERE\n+\n+SYSTEM DROP QUERY CONDITION CACHE;\n+\n+SELECT count(*) FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true, optimize_move_to_prewhere = false;\n+\n+SYSTEM FLUSH LOGS;\n+SELECT\n+    ProfileEvents['QueryConditionCacheHits'],\n+    ProfileEvents['QueryConditionCacheMisses'],\n+    toInt32(ProfileEvents['SelectedMarks']) < toInt32(ProfileEvents['SelectedMarksTotal'])\n+FROM system.query_log\n+WHERE\n+    type = 'QueryFinish'\n+    AND current_database = currentDatabase()\n+    AND query = 'SELECT count(*) FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true, optimize_move_to_prewhere = false;'\n+ORDER BY\n+    event_time_microseconds;\n+\n+SELECT * FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true, optimize_move_to_prewhere = false;\n+\n+SYSTEM FLUSH LOGS;\n+SELECT\n+    ProfileEvents['QueryConditionCacheHits'],\n+    ProfileEvents['QueryConditionCacheMisses'],\n+    toInt32(ProfileEvents['SelectedMarks']) < toInt32(ProfileEvents['SelectedMarksTotal'])\n+FROM system.query_log\n+WHERE\n+    type = 'QueryFinish'\n+    AND current_database = currentDatabase()\n+    AND query = 'SELECT * FROM tab WHERE b = 10000 SETTINGS use_query_condition_cache = true, optimize_move_to_prewhere = false;'\n+ORDER BY\n+    event_time_microseconds;\n+\n+DROP TABLE tab;\ndiff --git a/tests/queries/0_stateless/03252_merge_tree_min_bytes_to_seek.sql b/tests/queries/0_stateless/03252_merge_tree_min_bytes_to_seek.sql\nindex cbfadbb723fb..445871283df6 100644\n--- a/tests/queries/0_stateless/03252_merge_tree_min_bytes_to_seek.sql\n+++ b/tests/queries/0_stateless/03252_merge_tree_min_bytes_to_seek.sql\n@@ -1,3 +1,7 @@\n+\n+-- Disable query condition cache because it affects the `SelectedRanges` metric.\n+SET use_query_condition_cache = 0;\n+\n DROP TABLE IF EXISTS t_min_bytes_to_seek;\n \n CREATE TABLE t_min_bytes_to_seek (id UInt64)\n",
  "problem_statement": "Cache For Filters\n**Basic Idea**\r\n\r\nLet's suppose a query has a highly selective WHERE condition, but it does not benefit from the index. Subsequent queries use the same condition.\r\n\r\nLet's remember the ranges in data parts where that condition was satisfied and not, in the form of the ephemeral index in memory. Subsequent queries will use this index.\r\n\r\n**Implementation Proposal**\r\n\r\nAdd `ChunkInfo` with the information about the table, data part, and marks range, where this chunk came from.\r\n\r\nThis is applicable only for `MergeTree` tables but will also naturally work for `Merge` tables. It only works for Atomic and Replicated databases (databases that have table UUIDs).\r\n\r\nMaintain an index data structure in memory in the form of\r\ntable -> data part -> marks range -> condition -> 0 or 1\r\n\r\nWhen calculating a WHERE or PREWHERE condition, check if it is deterministic, look at the chunk info, and update the cache.\r\n\r\nWhen running a query, use the cache around `MergeTreeDataSelectExecutor`.\r\n\r\nAdd settings to control cache usage on query analysis and cache update on processing.\r\nAdd a SYSTEM command to flush this cache.\r\nAdd a server setting to control the maximum size of this cache in the number of cells.\r\n\r\nThe cache is shared between users, and we don't mind side-channel information leakage (about whether another user has run a similar query recently).\r\n\r\n**Additional Context**\r\n\r\nIt could work for external data, like S3 table functions, if we learn to use `etag`.\n",
  "hints_text": "I would like to try this.\n@zhongyuankai, this will be amazing, thank you!",
  "created_at": "2024-09-04T03:33:28Z",
  "modified_files": [
    "programs/local/LocalServer.cpp",
    "programs/server/Server.cpp",
    "src/Access/Common/AccessType.h",
    "src/Common/ProfileEvents.cpp",
    "src/Core/Defines.h",
    "src/Core/ServerSettings.cpp",
    "src/Core/Settings.cpp",
    "src/Core/SettingsChangesHistory.cpp",
    "src/Interpreters/ActionsDAG.cpp",
    "src/Interpreters/ActionsDAG.h",
    "b/src/Interpreters/Cache/QueryConditionCache.cpp",
    "b/src/Interpreters/Cache/QueryConditionCache.h",
    "src/Interpreters/Context.cpp",
    "src/Interpreters/Context.h",
    "src/Interpreters/InterpreterSystemQuery.cpp",
    "src/Parsers/ASTSystemQuery.cpp",
    "src/Parsers/ASTSystemQuery.h",
    "src/Processors/Chunk.h",
    "src/Processors/QueryPlan/FilterStep.cpp",
    "src/Processors/QueryPlan/FilterStep.h",
    "src/Processors/QueryPlan/Optimizations/Optimizations.h",
    "src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp",
    "src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h",
    "src/Processors/QueryPlan/Optimizations/optimizeTree.cpp",
    "b/src/Processors/QueryPlan/Optimizations/tryUpdateQueryConditionCache.cpp",
    "src/Processors/QueryPlan/ReadFromMergeTree.cpp",
    "src/Processors/Transforms/FilterTransform.cpp",
    "src/Processors/Transforms/FilterTransform.h",
    "src/Storages/MergeTree/IMergeTreeReader.h",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp",
    "src/Storages/MergeTree/MergeTreeDataSelectExecutor.h",
    "src/Storages/MergeTree/MergeTreeIOSettings.h",
    "src/Storages/MergeTree/MergeTreeRangeReader.cpp",
    "src/Storages/MergeTree/MergeTreeRangeReader.h",
    "src/Storages/MergeTree/MergeTreeReadTask.cpp",
    "src/Storages/MergeTree/MergeTreeReadTask.h",
    "src/Storages/MergeTree/MergeTreeSelectProcessor.cpp",
    "src/Storages/VirtualColumnUtils.cpp",
    "src/Storages/VirtualColumnUtils.h"
  ],
  "modified_test_files": [
    "tests/clickhouse-test",
    "tests/queries/0_stateless/00837_minmax_index.sh",
    "tests/queries/0_stateless/00838_unique_index.sh",
    "tests/queries/0_stateless/01055_minmax_index_compact_parts.sh",
    "tests/queries/0_stateless/01271_show_privileges.reference",
    "tests/queries/0_stateless/02784_parallel_replicas_automatic_decision.sh",
    "tests/queries/0_stateless/03031_read_in_order_optimization_with_virtual_row.sql",
    "b/tests/queries/0_stateless/03229_query_condition_cache.reference",
    "b/tests/queries/0_stateless/03229_query_condition_cache.sql",
    "tests/queries/0_stateless/03252_merge_tree_min_bytes_to_seek.sql"
  ]
}