{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 41215,
  "instance_id": "ClickHouse__ClickHouse-41215",
  "issue_numbers": [
    "41076"
  ],
  "base_commit": "49e0b1316df5e7ddff014af19b67bcb21f01f2d0",
  "patch": "diff --git a/src/Coordination/CoordinationSettings.h b/src/Coordination/CoordinationSettings.h\nindex 5247f5d7ec88..c436c1b66357 100644\n--- a/src/Coordination/CoordinationSettings.h\n+++ b/src/Coordination/CoordinationSettings.h\n@@ -30,6 +30,7 @@ struct Settings;\n     M(UInt64, snapshot_distance, 100000, \"How many log items we have to collect to write new snapshot\", 0) \\\n     M(Bool, auto_forwarding, true, \"Allow to forward write requests from followers to leader\", 0) \\\n     M(Milliseconds, shutdown_timeout, 5000, \"How much time we will wait until RAFT shutdown\", 0) \\\n+    M(Milliseconds, session_shutdown_timeout, 10000, \"How much time we will wait until sessions are closed during shutdown\", 0) \\\n     M(Milliseconds, startup_timeout, 180000, \"How much time we will wait until RAFT to start.\", 0) \\\n     M(LogsLevel, raft_logs_level, LogsLevel::information, \"Log internal RAFT logs into main server log level. Valid values: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'\", 0) \\\n     M(UInt64, rotate_log_storage_interval, 100000, \"How many records will be stored in one log storage file\", 0) \\\ndiff --git a/src/Coordination/KeeperDispatcher.cpp b/src/Coordination/KeeperDispatcher.cpp\nindex 5b376a03b025..261e43d80e4d 100644\n--- a/src/Coordination/KeeperDispatcher.cpp\n+++ b/src/Coordination/KeeperDispatcher.cpp\n@@ -354,9 +354,6 @@ void KeeperDispatcher::shutdown()\n                 update_configuration_thread.join();\n         }\n \n-        if (server)\n-            server->shutdown();\n-\n         KeeperStorage::RequestForSession request_for_session;\n \n         /// Set session expired for all pending requests\n@@ -368,10 +365,58 @@ void KeeperDispatcher::shutdown()\n             setResponse(request_for_session.session_id, response);\n         }\n \n-        /// Clear all registered sessions\n-        std::lock_guard lock(session_to_response_callback_mutex);\n-        session_to_response_callback.clear();\n+        KeeperStorage::RequestsForSessions close_requests;\n+        {\n+            /// Clear all registered sessions\n+            std::lock_guard lock(session_to_response_callback_mutex);\n+\n+            if (hasLeader())\n+            {\n+                close_requests.reserve(session_to_response_callback.size());\n+                // send to leader CLOSE requests for active sessions\n+                for (const auto & [session, response] : session_to_response_callback)\n+                {\n+                    auto request = Coordination::ZooKeeperRequestFactory::instance().get(Coordination::OpNum::Close);\n+                    request->xid = Coordination::CLOSE_XID;\n+                    using namespace std::chrono;\n+                    KeeperStorage::RequestForSession request_info\n+                    {\n+                        .session_id = session,\n+                        .time = duration_cast<milliseconds>(system_clock::now().time_since_epoch()).count(),\n+                        .request = std::move(request),\n+                    };\n+\n+                    close_requests.push_back(std::move(request_info));\n+                }\n+            }\n+\n+            session_to_response_callback.clear();\n+        }\n+\n+        // if there is no leader, there is no reason to do CLOSE because it's a write request\n+        if (hasLeader() && !close_requests.empty())\n+        {\n+            LOG_INFO(log, \"Trying to close {} session(s)\", close_requests.size());\n+            const auto raft_result = server->putRequestBatch(close_requests);\n+            auto sessions_closing_done_promise = std::make_shared<std::promise<void>>();\n+            auto sessions_closing_done = sessions_closing_done_promise->get_future();\n+            raft_result->when_ready([sessions_closing_done_promise = std::move(sessions_closing_done_promise)](\n+                                        nuraft::cmd_result<nuraft::ptr<nuraft::buffer>> & /*result*/,\n+                                        nuraft::ptr<std::exception> & /*exception*/) { sessions_closing_done_promise->set_value(); });\n+\n+            auto session_shutdown_timeout = configuration_and_settings->coordination_settings->session_shutdown_timeout.totalMilliseconds();\n+            if (sessions_closing_done.wait_for(std::chrono::milliseconds(session_shutdown_timeout)) != std::future_status::ready)\n+                LOG_WARNING(\n+                    log,\n+                    \"Failed to close sessions in {}ms. If they are not closed, they will be closed after session timeout.\",\n+                    session_shutdown_timeout);\n+        }\n+\n+        if (server)\n+            server->shutdown();\n+\n         CurrentMetrics::set(CurrentMetrics::KeeperAliveConnections, 0);\n+\n     }\n     catch (...)\n     {\n@@ -418,13 +463,15 @@ void KeeperDispatcher::sessionCleanerTask()\n                     LOG_INFO(log, \"Found dead session {}, will try to close it\", dead_session);\n \n                     /// Close session == send close request to raft server\n-                    Coordination::ZooKeeperRequestPtr request = Coordination::ZooKeeperRequestFactory::instance().get(Coordination::OpNum::Close);\n+                    auto request = Coordination::ZooKeeperRequestFactory::instance().get(Coordination::OpNum::Close);\n                     request->xid = Coordination::CLOSE_XID;\n-                    KeeperStorage::RequestForSession request_info;\n-                    request_info.request = request;\n                     using namespace std::chrono;\n-                    request_info.time = duration_cast<milliseconds>(system_clock::now().time_since_epoch()).count();\n-                    request_info.session_id = dead_session;\n+                    KeeperStorage::RequestForSession request_info\n+                    {\n+                        .session_id = dead_session,\n+                        .time = duration_cast<milliseconds>(system_clock::now().time_since_epoch()).count(),\n+                        .request = std::move(request),\n+                    };\n                     {\n                         std::lock_guard lock(push_request_mutex);\n                         if (!requests_queue->push(std::move(request_info)))\n",
  "test_patch": "diff --git a/tests/integration/test_keeper_session/configs/keeper_config.xml b/tests/integration/test_keeper_session/configs/keeper_config1.xml\nsimilarity index 67%\nrename from tests/integration/test_keeper_session/configs/keeper_config.xml\nrename to tests/integration/test_keeper_session/configs/keeper_config1.xml\nindex ed0bb52bd518..fd308fe8a2fc 100644\n--- a/tests/integration/test_keeper_session/configs/keeper_config.xml\n+++ b/tests/integration/test_keeper_session/configs/keeper_config1.xml\n@@ -1,4 +1,4 @@\n-<yandex>\n+<clickhouse>\n     <keeper_server>\n         <tcp_port>9181</tcp_port>\n         <server_id>1</server_id>\n@@ -19,9 +19,19 @@\n                 <id>1</id>\n                 <hostname>node1</hostname>\n                 <port>9234</port>\n-                <can_become_leader>true</can_become_leader>\n-                <priority>3</priority>\n+            </server>\n+            <server>\n+                <id>2</id>\n+                <hostname>node2</hostname>\n+                <port>9234</port>\n+                <start_as_follower>true</start_as_follower>\n+            </server>\n+            <server>\n+                <id>3</id>\n+                <hostname>node3</hostname>\n+                <port>9234</port>\n+                <start_as_follower>true</start_as_follower>\n             </server>\n         </raft_configuration>\n     </keeper_server>\n-</yandex>\n+</clickhouse>\ndiff --git a/tests/integration/test_keeper_session/configs/keeper_config2.xml b/tests/integration/test_keeper_session/configs/keeper_config2.xml\nnew file mode 100644\nindex 000000000000..ad558fbccade\n--- /dev/null\n+++ b/tests/integration/test_keeper_session/configs/keeper_config2.xml\n@@ -0,0 +1,37 @@\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>2</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+        <four_letter_word_white_list>*</four_letter_word_white_list>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <min_session_timeout_ms>5000</min_session_timeout_ms>\n+            <snapshot_distance>75</snapshot_distance>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>node1</hostname>\n+                <port>9234</port>\n+            </server>\n+            <server>\n+                <id>2</id>\n+                <hostname>node2</hostname>\n+                <port>9234</port>\n+                <start_as_follower>true</start_as_follower>\n+            </server>\n+            <server>\n+                <id>3</id>\n+                <hostname>node3</hostname>\n+                <port>9234</port>\n+                <start_as_follower>true</start_as_follower>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\ndiff --git a/tests/integration/test_keeper_session/configs/keeper_config3.xml b/tests/integration/test_keeper_session/configs/keeper_config3.xml\nnew file mode 100644\nindex 000000000000..2a21f959816c\n--- /dev/null\n+++ b/tests/integration/test_keeper_session/configs/keeper_config3.xml\n@@ -0,0 +1,37 @@\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>3</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+        <four_letter_word_white_list>*</four_letter_word_white_list>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <min_session_timeout_ms>5000</min_session_timeout_ms>\n+            <snapshot_distance>75</snapshot_distance>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>node1</hostname>\n+                <port>9234</port>\n+            </server>\n+            <server>\n+                <id>2</id>\n+                <hostname>node2</hostname>\n+                <port>9234</port>\n+                <start_as_follower>true</start_as_follower>\n+            </server>\n+            <server>\n+                <id>3</id>\n+                <hostname>node3</hostname>\n+                <port>9234</port>\n+                <start_as_follower>true</start_as_follower>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\ndiff --git a/tests/integration/test_keeper_session/test.py b/tests/integration/test_keeper_session/test.py\nindex 30db4d9548c4..4b3aa7e3fdfd 100644\n--- a/tests/integration/test_keeper_session/test.py\n+++ b/tests/integration/test_keeper_session/test.py\n@@ -10,7 +10,15 @@\n \n cluster = ClickHouseCluster(__file__)\n node1 = cluster.add_instance(\n-    \"node1\", main_configs=[\"configs/keeper_config.xml\"], stay_alive=True\n+    \"node1\", main_configs=[\"configs/keeper_config1.xml\"], stay_alive=True\n+)\n+\n+node2 = cluster.add_instance(\n+    \"node2\", main_configs=[\"configs/keeper_config2.xml\"], stay_alive=True\n+)\n+\n+node3 = cluster.add_instance(\n+    \"node3\", main_configs=[\"configs/keeper_config3.xml\"], stay_alive=True\n )\n \n bool_struct = struct.Struct(\"B\")\n@@ -61,7 +69,7 @@ def wait_node(node):\n \n \n def wait_nodes():\n-    for n in [node1]:\n+    for n in [node1, node2, node3]:\n         wait_node(n)\n \n \n@@ -165,3 +173,21 @@ def test_session_timeout(started_cluster):\n \n     negotiated_timeout, _ = handshake(node1.name, session_timeout=20000, session_id=0)\n     assert negotiated_timeout == 10000\n+\n+\n+def test_session_close_shutdown(started_cluster):\n+    wait_nodes()\n+\n+    node1_zk = get_fake_zk(node1.name)\n+    node2_zk = get_fake_zk(node2.name)\n+\n+    eph_node = \"/test_node\"\n+    node2_zk.create(eph_node, ephemeral=True)\n+    assert node1_zk.exists(eph_node) != None\n+\n+    # shutdown while session is active\n+    node2.stop_clickhouse()\n+\n+    assert node1_zk.exists(eph_node) == None\n+\n+    node2.start_clickhouse()\n",
  "problem_statement": "Remove all connected sessions during keeper shutdown\nSessions in clickhouse-keeper are local entity for each quorum participant but some information about session are global for the state of keeper: session_ids and corresponding ephemeral nodes. It make sense to send close requests to RAFT during server shutdown so all connected sessions will be closed properly and corresponding ephemeral nodes will be removed from state.\r\n\n",
  "hints_text": "",
  "created_at": "2022-09-12T12:27:07Z",
  "modified_files": [
    "src/Coordination/CoordinationSettings.h",
    "src/Coordination/KeeperDispatcher.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_keeper_session/configs/keeper_config.xml",
    "b/tests/integration/test_keeper_session/configs/keeper_config2.xml",
    "b/tests/integration/test_keeper_session/configs/keeper_config3.xml",
    "tests/integration/test_keeper_session/test.py"
  ]
}