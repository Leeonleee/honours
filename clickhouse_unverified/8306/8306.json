{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 8306,
  "instance_id": "ClickHouse__ClickHouse-8306",
  "issue_numbers": [
    "7868"
  ],
  "base_commit": "e83eeef7313c1b3ac58cbb57b63d2d8a1ceb5133",
  "patch": "diff --git a/dbms/src/Interpreters/Cluster.cpp b/dbms/src/Interpreters/Cluster.cpp\nindex 12ba58507507..2c75bd821fe8 100644\n--- a/dbms/src/Interpreters/Cluster.cpp\n+++ b/dbms/src/Interpreters/Cluster.cpp\n@@ -83,7 +83,8 @@ Cluster::Address::Address(const Poco::Util::AbstractConfiguration & config, cons\n     default_database = config.getString(config_prefix + \".default_database\", \"\");\n     secure = config.getBool(config_prefix + \".secure\", false) ? Protocol::Secure::Enable : Protocol::Secure::Disable;\n     compression = config.getBool(config_prefix + \".compression\", true) ? Protocol::Compression::Enable : Protocol::Compression::Disable;\n-    is_local = isLocal(config.getInt(\"tcp_port\", 0));\n+    const char * port_type = secure == Protocol::Secure::Enable ? \"tcp_port_secure\" : \"tcp_port\";\n+    is_local = isLocal(config.getInt(port_type, 0));\n }\n \n \ndiff --git a/dbms/src/Interpreters/DDLWorker.cpp b/dbms/src/Interpreters/DDLWorker.cpp\nindex afa4fca79f85..861a6b5ff03c 100644\n--- a/dbms/src/Interpreters/DDLWorker.cpp\n+++ b/dbms/src/Interpreters/DDLWorker.cpp\n@@ -645,7 +645,8 @@ void DDLWorker::processTask(DDLTask & task, const ZooKeeperPtr & zookeeper)\n         }\n         catch (...)\n         {\n-            task.execution_status = ExecutionStatus::fromCurrentException(\"An error occured before execution\");\n+            tryLogCurrentException(log, \"An error occurred before execution of DDL task: \");\n+            task.execution_status = ExecutionStatus::fromCurrentException(\"An error occurred before execution\");\n         }\n \n         /// We need to distinguish ZK errors occured before and after query executing\ndiff --git a/dbms/src/Storages/Distributed/DirectoryMonitor.cpp b/dbms/src/Storages/Distributed/DirectoryMonitor.cpp\nindex 2befe19308a3..111d1ff7aab3 100644\n--- a/dbms/src/Storages/Distributed/DirectoryMonitor.cpp\n+++ b/dbms/src/Storages/Distributed/DirectoryMonitor.cpp\n@@ -81,7 +81,10 @@ namespace\n \n StorageDistributedDirectoryMonitor::StorageDistributedDirectoryMonitor(\n     StorageDistributed & storage_, const std::string & name_, const ConnectionPoolPtr & pool_, ActionBlocker & monitor_blocker_)\n-    : storage(storage_), pool{pool_}, path{storage.path + name_ + '/'}\n+    : storage(storage_)\n+    , pool{pool_}\n+    , name{name_}\n+    , path{storage.path + name + '/'}\n     , current_batch_file_path{path + \"current_batch.txt\"}\n     , default_sleep_time{storage.global_context.getSettingsRef().distributed_directory_monitor_sleep_time_ms.totalMilliseconds()}\n     , sleep_time{default_sleep_time}\n@@ -642,4 +645,11 @@ std::string StorageDistributedDirectoryMonitor::getLoggerName() const\n     return storage.table_name + '.' + storage.getName() + \".DirectoryMonitor\";\n }\n \n+void StorageDistributedDirectoryMonitor::updatePath()\n+{\n+    std::lock_guard lock{mutex};\n+    path = storage.path + name + '/';\n+    current_batch_file_path = path + \"current_batch.txt\";\n+}\n+\n }\ndiff --git a/dbms/src/Storages/Distributed/DirectoryMonitor.h b/dbms/src/Storages/Distributed/DirectoryMonitor.h\nindex be613aec6e08..7e8f6a298f74 100644\n--- a/dbms/src/Storages/Distributed/DirectoryMonitor.h\n+++ b/dbms/src/Storages/Distributed/DirectoryMonitor.h\n@@ -26,6 +26,8 @@ class StorageDistributedDirectoryMonitor\n \n     static ConnectionPoolPtr createPool(const std::string & name, const StorageDistributed & storage);\n \n+    void updatePath();\n+\n     void flushAllData();\n \n     void shutdownAndDropAllData();\n@@ -43,6 +45,7 @@ class StorageDistributedDirectoryMonitor\n \n     StorageDistributed & storage;\n     ConnectionPoolPtr pool;\n+    std::string name;\n     std::string path;\n \n     bool should_batch_inserts = false;\ndiff --git a/dbms/src/Storages/StorageDistributed.cpp b/dbms/src/Storages/StorageDistributed.cpp\nindex 152712a72f44..f93f014e0795 100644\n--- a/dbms/src/Storages/StorageDistributed.cpp\n+++ b/dbms/src/Storages/StorageDistributed.cpp\n@@ -596,6 +596,21 @@ void StorageDistributed::flushClusterNodesAllData()\n         it->second.flushAllData();\n }\n \n+void StorageDistributed::rename(const String & new_path_to_db, const String & new_database_name, const String & new_table_name,\n+                                TableStructureWriteLockHolder &)\n+{\n+    table_name = new_table_name;\n+    database_name = new_database_name;\n+    if (!path.empty())\n+    {\n+        Poco::File(path).renameTo(new_path_to_db + escapeForFileName(new_table_name));\n+        path = new_path_to_db + escapeForFileName(new_table_name) + '/';\n+        std::lock_guard lock(cluster_nodes_mutex);\n+        for (auto & node : cluster_nodes_data)\n+            node.second.directory_monitor->updatePath();\n+    }\n+}\n+\n \n void registerStorageDistributed(StorageFactory & factory)\n {\ndiff --git a/dbms/src/Storages/StorageDistributed.h b/dbms/src/Storages/StorageDistributed.h\nindex e2409fe136ec..7d4bda94ef3e 100644\n--- a/dbms/src/Storages/StorageDistributed.h\n+++ b/dbms/src/Storages/StorageDistributed.h\n@@ -82,11 +82,7 @@ class StorageDistributed : public ext::shared_ptr_helper<StorageDistributed>, pu\n     /// Removes temporary data in local filesystem.\n     void truncate(const ASTPtr &, const Context &, TableStructureWriteLockHolder &) override;\n \n-    void rename(const String & /*new_path_to_db*/, const String & new_database_name, const String & new_table_name, TableStructureWriteLockHolder &) override\n-    {\n-        table_name = new_table_name;\n-        database_name = new_database_name;\n-    }\n+    void rename(const String & new_path_to_db, const String & new_database_name, const String & new_table_name, TableStructureWriteLockHolder &) override;\n \n     /// in the sub-tables, you need to manually add and delete columns\n     /// the structure of the sub-table is not checked\n",
  "test_patch": "diff --git a/dbms/tests/integration/test_distributed_ddl/cluster.py b/dbms/tests/integration/test_distributed_ddl/cluster.py\nindex fed672d22745..280713815861 100644\n--- a/dbms/tests/integration/test_distributed_ddl/cluster.py\n+++ b/dbms/tests/integration/test_distributed_ddl/cluster.py\n@@ -106,4 +106,4 @@ def insert_reliable(instance, query_insert):\n                 if not (s.find('Unknown status, client must retry') >= 0 or s.find('zkutil::KeeperException')):\n                     raise e\n \n-        raise last_exception\n\\ No newline at end of file\n+        raise last_exception\ndiff --git a/dbms/tests/integration/test_distributed_ddl/test.py b/dbms/tests/integration/test_distributed_ddl/test.py\nindex 6e57ed1b2df9..e30880e6ea45 100755\n--- a/dbms/tests/integration/test_distributed_ddl/test.py\n+++ b/dbms/tests/integration/test_distributed_ddl/test.py\n@@ -245,6 +245,51 @@ def test_create_reserved(test_cluster):\n     test_cluster.ddl_check_query(instance, \"DROP TABLE IF EXISTS test_as_reserved ON CLUSTER cluster\")\n \n \n+def test_rename(test_cluster):\n+    instance = test_cluster.instances['ch1']\n+    rules = test_cluster.pm_random_drops.pop_rules()\n+    test_cluster.ddl_check_query(instance, \"CREATE TABLE rename_shard ON CLUSTER cluster (id Int64, sid String DEFAULT concat('old', toString(id))) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/staging/test_shard', '{replica}') ORDER BY (id)\")\n+    test_cluster.ddl_check_query(instance, \"CREATE TABLE rename_new ON CLUSTER cluster AS rename_shard ENGINE = Distributed(cluster, default, rename_shard, id % 2)\")\n+    test_cluster.ddl_check_query(instance, \"RENAME TABLE rename_new TO rename ON CLUSTER cluster;\")\n+\n+\n+    for i in range(10):\n+        instance.query(\"insert into rename (id) values ({})\".format(i))\n+\n+    # FIXME ddl_check_query doesnt work for replicated DDDL if replace_hostnames_with_ips=True\n+    # because replicas use wrong host name of leader (and wrong path in zk) to check if it has executed query\n+    # so ddl query will always fail on some replicas even if query was actually executed by leader\n+    # Also such inconsistency in cluster configuration may lead to query duplication if leader suddenly changed\n+    # because path of lock in zk contains shard name, which is list of host names of replicas\n+    instance.query(\"ALTER TABLE rename_shard ON CLUSTER cluster MODIFY COLUMN sid String DEFAULT concat('new', toString(id))\", ignore_error=True)\n+    time.sleep(1)\n+\n+    test_cluster.ddl_check_query(instance, \"CREATE TABLE rename_new ON CLUSTER cluster AS rename_shard ENGINE = Distributed(cluster, default, rename_shard, id % 2)\")\n+\n+    instance.query(\"system stop distributed sends rename\")\n+\n+    for i in range(10, 20):\n+        instance.query(\"insert into rename (id) values ({})\".format(i))\n+\n+    test_cluster.ddl_check_query(instance, \"RENAME TABLE rename TO rename_old, rename_new TO rename ON CLUSTER cluster\")\n+\n+    for i in range(20, 30):\n+        instance.query(\"insert into rename (id) values ({})\".format(i))\n+\n+    instance.query(\"system flush distributed rename\")\n+    for name in ['ch1', 'ch2', 'ch3', 'ch4']:\n+        test_cluster.instances[name].query(\"system sync replica rename_shard\")\n+\n+    # system stop distributed sends does not affect inserts into local shard,\n+    # so some ids in range (10, 20) will be inserted into rename_shard\n+    assert instance.query(\"select count(id), sum(id) from rename\").rstrip() == \"25\\t360\"\n+    #assert instance.query(\"select count(id), sum(id) from rename\").rstrip() == \"20\\t290\"\n+    assert instance.query(\"select count(id), sum(id) from rename where sid like 'old%'\").rstrip() == \"15\\t115\"\n+    #assert instance.query(\"select count(id), sum(id) from rename where sid like 'old%'\").rstrip() == \"10\\t45\"\n+    assert instance.query(\"select count(id), sum(id) from rename where sid like 'new%'\").rstrip() == \"10\\t245\"\n+    test_cluster.pm_random_drops.push_rules(rules)\n+\n+\n if __name__ == '__main__':\n     with contextmanager(test_cluster)() as ctx_cluster:\n        for name, instance in ctx_cluster.instances.items():\n",
  "problem_statement": "Rename on Distributed Engine does not move data\n(you don't have to strictly follow this form)\r\n\r\n**Describe the bug or unexpected behaviour**\r\nRename query on Distributed table does not rename data directory. All data will be putted to old directory which lead to some side effects.\r\n\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n19.16.4.12\r\n\r\n```sql\r\nCREATE TABLE staging.test_shard ON CLUSTER cluster_name\r\n(\r\n  id String,\r\n  sid String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/staging/test_shard', '{replica}')\r\nPARTITION BY tuple()\r\nORDER BY (id);\r\n\r\nCREATE TABLE staging.test_new ON CLUSTER cluster_name\r\n  AS staging.test_shard\r\nENGINE = Distributed(cluster_name, staging, test_shard, rand());\r\n\r\nRENAME TABLE staging.test_new TO staging.test ON CLUSTER cluster_name;\r\n\r\ninsert into test (id) values ('1');\r\n\r\nALTER TABLE staging.test_shard ON CLUSTER cluster_name\r\nDROP COLUMN sid;\r\nALTER TABLE staging.test_shard ON CLUSTER cluster_name\r\n\tADD COLUMN sid MATERIALIZED id;\r\n\r\n-- need to trigger temporary storage in distributed table\r\ninsert into test (id) values ('2');\r\ninsert into test (id) values ('3');\r\n\r\n-- we usually use this queries to atomic replace distributed table on whole cluster\r\n-- with same structure as shard\r\nCREATE TABLE staging.test_new ON CLUSTER cluster_name\r\nAS staging.test_shard\r\nENGINE = Distributed(cluster_name, staging, test_shard, rand());\r\nRENAME TABLE staging.test TO staging.test_old, staging.test_new TO staging.test ON CLUSTER cluster_name;\r\n\r\nselect count(*) from staging.test\r\n\r\ninsert into test (id) values ('4');\r\ninsert into test (id) values ('5');\r\nselect count(*) from staging.test\r\nDROP TABLE staging.test_old ON CLUSTER cluster_name;\r\n```\r\nAfter this:\r\nin data directory:\r\n`test_new` exists; `test` does not exists.\r\nNo new data in table `test_shard`.\r\n\r\nOnly restart solve situation.\r\n\r\n**Expected behavior**\r\nData directory should be renamed. All tem\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2019.11.20 18:21:59.809605 [ 167 ] {} <Error> test.Distributed.DirectoryMonitor: Code: 44, e.displayText() = DB::Exception: Received from clickhouse-03.infra.whi.sk:9440. DB::Exception: Cannot insert column sid, because it is MATERIALIZED column.. Stack trace:\r\n\r\n0. 0x55818beaaed0 StackTrace::StackTrace() /usr/bin/clickhouse\r\n1. 0x55818beaaca5 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse\r\n2. 0x55818bbd98e6 ? /usr/bin/clickhouse\r\n3. 0x55818f2038d1 DB::InterpreterInsertQuery::execute() /usr/bin/clickhouse\r\n4. 0x55818f34d946 ? /usr/bin/clickhouse\r\n5. 0x55818f34eb0e DB::executeQuery(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, bool) /usr/bin/clickhouse\r\n6. 0x55818bf330a4 DB::TCPHandler::runImpl() /usr/bin/clickhouse\r\n7. 0x55818bf33adb DB::TCPHandler::run() /usr/bin/clickhouse\r\n8. 0x55818fdcb240 Poco::Net::TCPServerConnection::start() /usr/bin/clickhouse\r\n9. 0x55818fdcb95d Poco::Net::TCPServerDispatcher::run() /usr/bin/clickhouse\r\n10. 0x5581914a1b31 Poco::PooledThread::run() /usr/bin/clickhouse\r\n11. 0x55819149f8dc Poco::ThreadImpl::runnableEntry(void*) /usr/bin/clickhouse\r\n12. 0x558191c11460 ? /usr/bin/clickhouse\r\n13. 0x7fea927066db start_thread /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n14. 0x7fea9202388f clone /lib/x86_64-linux-gnu/libc-2.27.so\r\n, Stack trace:\r\n```\r\n\n",
  "hints_text": "",
  "created_at": "2019-12-19T19:55:04Z"
}