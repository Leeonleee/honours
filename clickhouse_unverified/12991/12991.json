{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 12991,
  "instance_id": "ClickHouse__ClickHouse-12991",
  "issue_numbers": [
    "12990"
  ],
  "base_commit": "e9b5687e6851317212336b68e17e933db7ef4f12",
  "patch": "diff --git a/.gitmodules b/.gitmodules\nindex 32ce4983bb01..fefaa0eb5f88 100644\n--- a/.gitmodules\n+++ b/.gitmodules\n@@ -10,7 +10,7 @@\n \turl = https://github.com/lz4/lz4.git\n [submodule \"contrib/librdkafka\"]\n \tpath = contrib/librdkafka\n-\turl = https://github.com/edenhill/librdkafka.git\n+\turl = https://github.com/ClickHouse-Extras/librdkafka.git\n [submodule \"contrib/cctz\"]\n \tpath = contrib/cctz\n \turl = https://github.com/ClickHouse-Extras/cctz.git\ndiff --git a/contrib/librdkafka b/contrib/librdkafka\nindex b0d91bd74abb..2090cbf56b71 160000\n--- a/contrib/librdkafka\n+++ b/contrib/librdkafka\n@@ -1,1 +1,1 @@\n-Subproject commit b0d91bd74abb5f0e1ee972d326a317ad610f6300\n+Subproject commit 2090cbf56b715247ec2be7f768707a7ab1bf7ede\ndiff --git a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\nindex bbd93bc5aad0..2498b7196483 100644\n--- a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n+++ b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n@@ -384,7 +384,8 @@ bool ReadBufferFromKafkaConsumer::poll()\n         {\n             messages = std::move(new_messages);\n             current = messages.begin();\n-            LOG_TRACE(log, \"Polled batch of {} messages. Offset position: {}\", messages.size(), consumer->get_offsets_position(consumer->get_assignment()));\n+            LOG_TRACE(log, \"Polled batch of {} messages. Offsets position: {}\",\n+                messages.size(), consumer->get_offsets_position(consumer->get_assignment()));\n             break;\n         }\n     }\n@@ -416,7 +417,7 @@ size_t ReadBufferFromKafkaConsumer::filterMessageErrors()\n         return false;\n     });\n \n-    size_t skipped = std::distance(messages.end(), new_end);\n+    size_t skipped = std::distance(new_end, messages.end());\n     if (skipped)\n     {\n         LOG_ERROR(log, \"There were {} messages with an error\", skipped);\n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex 75270512fc46..5d23a9cfa40b 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -664,7 +664,8 @@ def test_kafka_issue11308(kafka_cluster):\n         FROM test.kafka;\n         ''')\n \n-    time.sleep(9)\n+    while int(instance.query('SELECT count() FROM test.persistent_kafka')) < 3:\n+        time.sleep(1)\n \n     result = instance.query('SELECT * FROM test.persistent_kafka ORDER BY time;')\n \n@@ -1431,7 +1432,8 @@ def test_kafka_produce_key_timestamp(kafka_cluster):\n     instance.query(\"INSERT INTO test.kafka_writer VALUES ({},{},'{}',toDateTime({})),({},{},'{}',toDateTime({}))\".format(3,3,'k3',1577836803,4,4,'k4',1577836804))\n     instance.query(\"INSERT INTO test.kafka_writer VALUES ({},{},'{}',toDateTime({}))\".format(5,5,'k5',1577836805))\n \n-    time.sleep(10)\n+    while int(instance.query(\"SELECT count() FROM test.view\")) < 5:\n+        time.sleep(1)\n \n     result = instance.query(\"SELECT * FROM test.view ORDER BY value\", ignore_error=True)\n \n@@ -1535,7 +1537,9 @@ def test_kafka_flush_by_block_size(kafka_cluster):\n         messages.append(json.dumps({'key': 0, 'value': 0}))\n     kafka_produce('flush_by_block_size', messages)\n \n-    time.sleep(1)\n+    # Wait for Kafka engine to consume this data\n+    while 1 != int(instance.query(\"SELECT count() FROM system.parts WHERE database = 'test' AND table = 'view' AND name = 'all_1_1_0'\")):\n+        time.sleep(1)\n \n     # TODO: due to https://github.com/ClickHouse/ClickHouse/issues/11216\n     # second flush happens earlier than expected, so we have 2 parts here instead of one\n",
  "problem_statement": "MemorySanitizer: use-of-uninitialized-value test_storage_kafka\n**How to reproduce**\r\n* 20.7(master)\r\n* Memory sanitizer\r\n* Integration test test_storage_kafka\r\n```\r\nUninitialized bytes in __interceptor_getaddrinfo at offset 20 inside [0x7f7b39a07300, 48)\r\n==1==WARNING: MemorySanitizer: use-of-uninitialized-value\r\n==1==WARNING: invalid path to external symbolizer!\r\n==1==WARNING: Failed to use and restart external symbolizer!\r\n    #0 0x3259c51f  (/usr/bin/clickhouse+0x3259c51f)\r\n    #1 0x32560452  (/usr/bin/clickhouse+0x32560452)\r\n    #2 0x3288b54c  (/usr/bin/clickhouse+0x3288b54c)\r\n    #3 0x7f7c16816668  (/lib/x86_64-linux-gnu/libpthread.so.0+0x9668)\r\n    #4 0x7f7c1672d322  (/lib/x86_64-linux-gnu/libc.so.6+0x122322)\r\n\r\n  Uninitialized value was created by an allocation of 'hints' in the stack frame of function 'rd_getaddrinfo'\r\n    #0 0x3259c150  (/usr/bin/clickhouse+0x3259c150)\r\n\r\nSUMMARY: MemorySanitizer: use-of-uninitialized-value (/usr/bin/clickhouse+0x3259c51f)\r\n\r\naddr2line -afiCe ~/memory-ch/output/binary/clickhouse-server 0x3259c51f 0x32560452 0x3288b54c 0x9668 0x122322 0x3259c150 0x3259c51f\r\n0x000000003259c51f\r\nrd_getaddrinfo\r\n/build/obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdaddr.c:168\r\n0x0000000032560452\r\nrd_kafka_broker_resolve\r\n/build/obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_broker.c:840\r\nrd_kafka_broker_connect\r\n/build/obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_broker.c:1890\r\nrd_kafka_broker_thread_main\r\n/build/obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_broker.c:4914\r\n0x000000003288b54c\r\n_thrd_wrapper_function\r\n/build/obj-x86_64-linux-gnu/../contrib/librdkafka/src/tinycthread.c:576\r\n0x0000000000009668\r\n??\r\n??:0\r\n0x0000000000122322\r\n??\r\n??:0\r\n0x000000003259c150\r\nrd_getaddrinfo\r\n/build/obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdaddr.c:149\r\n0x000000003259c51f\r\nrd_getaddrinfo\r\n/build/obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdaddr.c:168\r\n```\n",
  "hints_text": "Minor defect in `rdkafka` library.\nI'll fix it.",
  "created_at": "2020-07-28T11:14:41Z"
}