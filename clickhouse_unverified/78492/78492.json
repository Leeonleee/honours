{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 78492,
  "instance_id": "ClickHouse__ClickHouse-78492",
  "issue_numbers": [
    "78550"
  ],
  "base_commit": "a5b66c6b833f801e51a21c9660cab0612d40b0f4",
  "patch": "diff --git a/.github/workflows/backport_branches.yml b/.github/workflows/backport_branches.yml\nindex 1f3f219946f7..a774aa2c6162 100644\n--- a/.github/workflows/backport_branches.yml\n+++ b/.github/workflows/backport_branches.yml\n@@ -701,11 +701,11 @@ jobs:\n             python3 -m praktika run 'Stateless tests (asan, 2/2)' --workflow \"BackportPR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_tsan:\n+  stress_test_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKHRzYW4p') }}\n-    name: \"Stress test (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF90c2FuKQ==') }}\n+    name: \"Stress test (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -734,9 +734,9 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"BackportPR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"BackportPR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"BackportPR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"BackportPR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n   integration_tests_asan_old_analyzer_1_6:\ndiff --git a/.github/workflows/master.yml b/.github/workflows/master.yml\nindex 3c56ba97594b..4b1d5ed6725d 100644\n--- a/.github/workflows/master.yml\n+++ b/.github/workflows/master.yml\n@@ -509,11 +509,11 @@ jobs:\n             python3 -m praktika run 'Build (arm_asan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  build_amd_coverage:\n-    runs-on: [self-hosted, builder]\n+  build_arm_coverage:\n+    runs-on: [self-hosted, builder-aarch64]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tidy, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFtZF9jb3ZlcmFnZSk=') }}\n-    name: \"Build (amd_coverage)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFybV9jb3ZlcmFnZSk=') }}\n+    name: \"Build (arm_coverage)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -542,9 +542,9 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Build (amd_coverage)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Build (arm_coverage)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Build (amd_coverage)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Build (arm_coverage)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n   build_arm_binary:\n@@ -662,7 +662,7 @@ jobs:\n           fi\n \n   build_arm_v80compat:\n-    runs-on: [self-hosted, builder]\n+    runs-on: [self-hosted, builder-aarch64]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tidy, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFybV92ODBjb21wYXQp') }}\n     name: \"Build (arm_v80compat)\"\n@@ -776,7 +776,7 @@ jobs:\n           fi\n \n   build_amd_compat:\n-    runs-on: [self-hosted, builder]\n+    runs-on: [self-hosted, builder-aarch64]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tidy, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFtZF9jb21wYXQp') }}\n     name: \"Build (amd_compat)\"\n@@ -2143,11 +2143,11 @@ jobs:\n             python3 -m praktika run 'Stateless tests (aarch64)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stateless_tests_azure_asan_1_3:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChhenVyZSwgYXNhbiwgMS8zKQ==') }}\n-    name: \"Stateless tests (azure, asan, 1/3)\"\n+  stateless_tests_azure_arm_asan_1_3:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChhenVyZSwgYXJtX2FzYW4sIDEvMyk=') }}\n+    name: \"Stateless tests (azure, arm_asan, 1/3)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -2176,16 +2176,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stateless tests (azure, asan, 1/3)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stateless tests (azure, arm_asan, 1/3)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stateless tests (azure, asan, 1/3)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stateless tests (azure, arm_asan, 1/3)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stateless_tests_azure_asan_2_3:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChhenVyZSwgYXNhbiwgMi8zKQ==') }}\n-    name: \"Stateless tests (azure, asan, 2/3)\"\n+  stateless_tests_azure_arm_asan_2_3:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChhenVyZSwgYXJtX2FzYW4sIDIvMyk=') }}\n+    name: \"Stateless tests (azure, arm_asan, 2/3)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -2214,16 +2214,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stateless tests (azure, asan, 2/3)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stateless tests (azure, arm_asan, 2/3)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stateless tests (azure, asan, 2/3)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stateless tests (azure, arm_asan, 2/3)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stateless_tests_azure_asan_3_3:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChhenVyZSwgYXNhbiwgMy8zKQ==') }}\n-    name: \"Stateless tests (azure, asan, 3/3)\"\n+  stateless_tests_azure_arm_asan_3_3:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChhenVyZSwgYXJtX2FzYW4sIDMvMyk=') }}\n+    name: \"Stateless tests (azure, arm_asan, 3/3)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -2252,9 +2252,9 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stateless tests (azure, asan, 3/3)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stateless tests (azure, arm_asan, 3/3)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stateless tests (azure, asan, 3/3)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stateless tests (azure, arm_asan, 3/3)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n   integration_tests_asan_old_analyzer_1_6:\n@@ -3018,8 +3018,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_1_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgMS82KQ==') }}\n     name: \"Stateless tests (coverage, 1/6)\"\n     outputs:\n@@ -3056,8 +3056,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_2_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgMi82KQ==') }}\n     name: \"Stateless tests (coverage, 2/6)\"\n     outputs:\n@@ -3094,8 +3094,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_3_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgMy82KQ==') }}\n     name: \"Stateless tests (coverage, 3/6)\"\n     outputs:\n@@ -3132,8 +3132,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_4_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgNC82KQ==') }}\n     name: \"Stateless tests (coverage, 4/6)\"\n     outputs:\n@@ -3170,8 +3170,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_5_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgNS82KQ==') }}\n     name: \"Stateless tests (coverage, 5/6)\"\n     outputs:\n@@ -3208,8 +3208,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_6_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgNi82KQ==') }}\n     name: \"Stateless tests (coverage, 6/6)\"\n     outputs:\n@@ -3245,11 +3245,11 @@ jobs:\n             python3 -m praktika run 'Stateless tests (coverage, 6/6)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_debug:\n+  stress_test_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGRlYnVnKQ==') }}\n-    name: \"Stress test (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF9kZWJ1Zyk=') }}\n+    name: \"Stress test (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3278,16 +3278,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (debug)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_debug)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (debug)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_debug)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_tsan:\n+  stress_test_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKHRzYW4p') }}\n-    name: \"Stress test (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF90c2FuKQ==') }}\n+    name: \"Stress test (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3316,16 +3316,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFzYW4p') }}\n-    name: \"Stress test (asan)\"\n+  stress_test_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFybV9hc2FuKQ==') }}\n+    name: \"Stress test (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3354,16 +3354,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (asan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (arm_asan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (asan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (arm_asan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_ubsan:\n+  stress_test_amd_ubsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_ubsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKHVic2FuKQ==') }}\n-    name: \"Stress test (ubsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF91YnNhbik=') }}\n+    name: \"Stress test (amd_ubsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3392,16 +3392,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (ubsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_ubsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (ubsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_ubsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_msan:\n+  stress_test_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKG1zYW4p') }}\n-    name: \"Stress test (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF9tc2FuKQ==') }}\n+    name: \"Stress test (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3430,9 +3430,9 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (msan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_msan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (msan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_msan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n   stress_test_azure_tsan:\n@@ -3511,11 +3511,11 @@ jobs:\n             python3 -m praktika run 'Stress test (azure, msan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_debug:\n+  ast_fuzzer_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoZGVidWcp') }}\n-    name: \"AST fuzzer (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX2RlYnVnKQ==') }}\n+    name: \"AST fuzzer (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3544,16 +3544,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (debug)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_debug)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (debug)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_debug)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYXNhbik=') }}\n-    name: \"AST fuzzer (asan)\"\n+  ast_fuzzer_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYXJtX2FzYW4p') }}\n+    name: \"AST fuzzer (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3582,16 +3582,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (asan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (arm_asan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (asan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (arm_asan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_tsan:\n+  ast_fuzzer_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAodHNhbik=') }}\n-    name: \"AST fuzzer (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX3RzYW4p') }}\n+    name: \"AST fuzzer (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3620,16 +3620,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (tsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_tsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (tsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_tsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_msan:\n+  ast_fuzzer_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAobXNhbik=') }}\n-    name: \"AST fuzzer (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX21zYW4p') }}\n+    name: \"AST fuzzer (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3658,16 +3658,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (msan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_msan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (msan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_msan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_ubsan:\n+  ast_fuzzer_amd_ubsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_ubsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAodWJzYW4p') }}\n-    name: \"AST fuzzer (ubsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX3Vic2FuKQ==') }}\n+    name: \"AST fuzzer (amd_ubsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3696,16 +3696,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (ubsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_ubsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (ubsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_ubsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_debug:\n+  buzzhouse_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChkZWJ1Zyk=') }}\n-    name: \"BuzzHouse (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfZGVidWcp') }}\n+    name: \"BuzzHouse (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3734,16 +3734,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (debug)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_debug)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (debug)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_debug)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhc2FuKQ==') }}\n-    name: \"BuzzHouse (asan)\"\n+  buzzhouse_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhcm1fYXNhbik=') }}\n+    name: \"BuzzHouse (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3772,16 +3772,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (asan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (arm_asan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (asan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (arm_asan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_tsan:\n+  buzzhouse_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlICh0c2FuKQ==') }}\n-    name: \"BuzzHouse (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfdHNhbik=') }}\n+    name: \"BuzzHouse (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3810,16 +3810,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (tsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_tsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (tsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_tsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_msan:\n+  buzzhouse_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChtc2FuKQ==') }}\n-    name: \"BuzzHouse (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfbXNhbik=') }}\n+    name: \"BuzzHouse (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3848,16 +3848,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (msan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_msan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (msan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_msan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_ubsan:\n+  buzzhouse_amd_ubsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_ubsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlICh1YnNhbik=') }}\n-    name: \"BuzzHouse (ubsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfdWJzYW4p') }}\n+    name: \"BuzzHouse (amd_ubsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3886,9 +3886,9 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (ubsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_ubsan)' --workflow \"MasterCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (ubsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_ubsan)' --workflow \"MasterCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n   performance_comparison_amd_release_master_head_1_3:\ndiff --git a/.github/workflows/pull_request.yml b/.github/workflows/pull_request.yml\nindex 65c890b7d93e..e312416be126 100644\n--- a/.github/workflows/pull_request.yml\n+++ b/.github/workflows/pull_request.yml\n@@ -283,6 +283,44 @@ jobs:\n             python3 -m praktika run 'Build (amd_tidy)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n+  build_arm_tidy:\n+    runs-on: [self-hosted, builder-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tidy]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFybV90aWR5KQ==') }}\n+    name: \"Build (arm_tidy)\"\n+    outputs:\n+      data: ${{ steps.run.outputs.DATA }}\n+    steps:\n+      - name: Checkout code\n+        uses: actions/checkout@v4\n+        with:\n+          ref: ${{ env.CHECKOUT_REF }}\n+\n+      - name: Prepare env script\n+        run: |\n+          rm -rf ./ci/tmp ./ci/tmp ./ci/tmp\n+          mkdir -p ./ci/tmp ./ci/tmp ./ci/tmp\n+          cat > ./ci/tmp/praktika_setup_env.sh << 'ENV_SETUP_SCRIPT_EOF'\n+          export PYTHONPATH=./ci:.:\n+          cat > ./ci/tmp/workflow_config_pr.json << 'EOF'\n+          ${{ needs.config_workflow.outputs.data }}\n+          EOF\n+          cat > ./ci/tmp/workflow_status.json << 'EOF'\n+          ${{ toJson(needs) }}\n+          EOF\n+          ENV_SETUP_SCRIPT_EOF\n+\n+      - name: Run\n+        id: run\n+        run: |\n+          . ./ci/tmp/praktika_setup_env.sh\n+          set -o pipefail\n+          if command -v ts &> /dev/null; then\n+            python3 -m praktika run 'Build (arm_tidy)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+          else\n+            python3 -m praktika run 'Build (arm_tidy)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+          fi\n+\n   build_amd_debug:\n     runs-on: [self-hosted, builder]\n     needs: [config_workflow, dockers_build_amd_and_merge, style_check, fast_test, build_amd_tidy]\n@@ -625,11 +663,11 @@ jobs:\n             python3 -m praktika run 'Build (arm_asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  build_amd_coverage:\n-    runs-on: [self-hosted, builder]\n+  build_arm_coverage:\n+    runs-on: [self-hosted, builder-aarch64]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFtZF9jb3ZlcmFnZSk=') }}\n-    name: \"Build (amd_coverage)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFybV9jb3ZlcmFnZSk=') }}\n+    name: \"Build (arm_coverage)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -658,9 +696,9 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Build (amd_coverage)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Build (arm_coverage)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Build (amd_coverage)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Build (arm_coverage)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n   build_arm_binary:\n@@ -778,7 +816,7 @@ jobs:\n           fi\n \n   build_arm_v80compat:\n-    runs-on: [self-hosted, builder]\n+    runs-on: [self-hosted, builder-aarch64]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFybV92ODBjb21wYXQp') }}\n     name: \"Build (arm_v80compat)\"\n@@ -892,7 +930,7 @@ jobs:\n           fi\n \n   build_amd_compat:\n-    runs-on: [self-hosted, builder]\n+    runs-on: [self-hosted, builder-aarch64]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnVpbGQgKGFtZF9jb21wYXQp') }}\n     name: \"Build (amd_compat)\"\n@@ -2260,8 +2298,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_1_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgMS82KQ==') }}\n     name: \"Stateless tests (coverage, 1/6)\"\n     outputs:\n@@ -2298,8 +2336,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_2_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgMi82KQ==') }}\n     name: \"Stateless tests (coverage, 2/6)\"\n     outputs:\n@@ -2336,8 +2374,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_3_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgMy82KQ==') }}\n     name: \"Stateless tests (coverage, 3/6)\"\n     outputs:\n@@ -2374,8 +2412,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_4_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgNC82KQ==') }}\n     name: \"Stateless tests (coverage, 4/6)\"\n     outputs:\n@@ -2412,8 +2450,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_5_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgNS82KQ==') }}\n     name: \"Stateless tests (coverage, 5/6)\"\n     outputs:\n@@ -2450,8 +2488,8 @@ jobs:\n           fi\n \n   stateless_tests_coverage_6_6:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_coverage, stateless_tests_asan_1_2, stateless_tests_asan_2_2]\n     if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RhdGVsZXNzIHRlc3RzIChjb3ZlcmFnZSwgNi82KQ==') }}\n     name: \"Stateless tests (coverage, 6/6)\"\n     outputs:\n@@ -3361,11 +3399,11 @@ jobs:\n             python3 -m praktika run 'Integration tests (asan, flaky check)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_debug:\n+  stress_test_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGRlYnVnKQ==') }}\n-    name: \"Stress test (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF9kZWJ1Zyk=') }}\n+    name: \"Stress test (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3394,16 +3432,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_tsan:\n+  stress_test_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKHRzYW4p') }}\n-    name: \"Stress test (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF90c2FuKQ==') }}\n+    name: \"Stress test (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3432,16 +3470,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFzYW4p') }}\n-    name: \"Stress test (asan)\"\n+  stress_test_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFybV9hc2FuKQ==') }}\n+    name: \"Stress test (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3470,16 +3508,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (arm_asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (arm_asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_ubsan:\n+  stress_test_amd_ubsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_ubsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKHVic2FuKQ==') }}\n-    name: \"Stress test (ubsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF91YnNhbik=') }}\n+    name: \"Stress test (amd_ubsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3508,16 +3546,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (ubsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_ubsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (ubsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_ubsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_msan:\n+  stress_test_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKG1zYW4p') }}\n-    name: \"Stress test (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF9tc2FuKQ==') }}\n+    name: \"Stress test (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3546,16 +3584,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  upgrade_check_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAoYXNhbik=') }}\n-    name: \"Upgrade check (asan)\"\n+  upgrade_check_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAoYXJtX2FzYW4p') }}\n+    name: \"Upgrade check (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3584,16 +3622,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Upgrade check (asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (arm_asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Upgrade check (asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (arm_asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  upgrade_check_tsan:\n+  upgrade_check_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAodHNhbik=') }}\n-    name: \"Upgrade check (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAoYW1kX3RzYW4p') }}\n+    name: \"Upgrade check (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3622,16 +3660,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Upgrade check (tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (amd_tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Upgrade check (tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (amd_tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  upgrade_check_msan:\n+  upgrade_check_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAobXNhbik=') }}\n-    name: \"Upgrade check (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAoYW1kX21zYW4p') }}\n+    name: \"Upgrade check (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3660,16 +3698,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Upgrade check (msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (amd_msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Upgrade check (msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (amd_msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  upgrade_check_debug:\n+  upgrade_check_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAoZGVidWcp') }}\n-    name: \"Upgrade check (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'VXBncmFkZSBjaGVjayAoYW1kX2RlYnVnKQ==') }}\n+    name: \"Upgrade check (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3698,16 +3736,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Upgrade check (debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (amd_debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Upgrade check (debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Upgrade check (amd_debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_debug:\n+  ast_fuzzer_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoZGVidWcp') }}\n-    name: \"AST fuzzer (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX2RlYnVnKQ==') }}\n+    name: \"AST fuzzer (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3736,16 +3774,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYXNhbik=') }}\n-    name: \"AST fuzzer (asan)\"\n+  ast_fuzzer_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYXJtX2FzYW4p') }}\n+    name: \"AST fuzzer (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3774,16 +3812,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (arm_asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (arm_asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_tsan:\n+  ast_fuzzer_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAodHNhbik=') }}\n-    name: \"AST fuzzer (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX3RzYW4p') }}\n+    name: \"AST fuzzer (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3812,16 +3850,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_msan:\n+  ast_fuzzer_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAobXNhbik=') }}\n-    name: \"AST fuzzer (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX21zYW4p') }}\n+    name: \"AST fuzzer (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3850,16 +3888,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  ast_fuzzer_ubsan:\n+  ast_fuzzer_amd_ubsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_ubsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAodWJzYW4p') }}\n-    name: \"AST fuzzer (ubsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QVNUIGZ1enplciAoYW1kX3Vic2FuKQ==') }}\n+    name: \"AST fuzzer (amd_ubsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3888,16 +3926,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'AST fuzzer (ubsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_ubsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'AST fuzzer (ubsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'AST fuzzer (amd_ubsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_debug:\n+  buzzhouse_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChkZWJ1Zyk=') }}\n-    name: \"BuzzHouse (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfZGVidWcp') }}\n+    name: \"BuzzHouse (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3926,16 +3964,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_debug)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_debug)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhc2FuKQ==') }}\n-    name: \"BuzzHouse (asan)\"\n+  buzzhouse_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhcm1fYXNhbik=') }}\n+    name: \"BuzzHouse (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -3964,16 +4002,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (arm_asan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (arm_asan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_tsan:\n+  buzzhouse_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlICh0c2FuKQ==') }}\n-    name: \"BuzzHouse (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfdHNhbik=') }}\n+    name: \"BuzzHouse (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -4002,16 +4040,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_tsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_tsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_msan:\n+  buzzhouse_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChtc2FuKQ==') }}\n-    name: \"BuzzHouse (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfbXNhbik=') }}\n+    name: \"BuzzHouse (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -4040,16 +4078,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_msan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_msan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  buzzhouse_ubsan:\n+  buzzhouse_amd_ubsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_ubsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlICh1YnNhbik=') }}\n-    name: \"BuzzHouse (ubsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'QnV6ekhvdXNlIChhbWRfdWJzYW4p') }}\n+    name: \"BuzzHouse (amd_ubsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -4078,9 +4116,9 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'BuzzHouse (ubsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_ubsan)' --workflow \"PR\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'BuzzHouse (ubsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'BuzzHouse (amd_ubsan)' --workflow \"PR\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n   performance_comparison_amd_release_master_head_1_3:\n@@ -4313,7 +4351,7 @@ jobs:\n \n   finish_workflow:\n     runs-on: [self-hosted, style-checker-aarch64]\n-    needs: [config_workflow, dockers_build_arm, dockers_build_amd_and_merge, style_check, docs_check, fast_test, build_amd_tidy, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan, build_amd_coverage, build_arm_binary, build_amd_darwin, build_arm_darwin, build_arm_v80compat, build_amd_freebsd, build_ppc64le, build_amd_compat, build_amd_musl, build_riscv64, build_s390x, build_loongarch64, build_fuzzers, unit_tests_asan, unit_tests_tsan, unit_tests_msan, unit_tests_ubsan, docker_server_image, docker_keeper_image, install_packages_release, install_packages_aarch64, compatibility_check_release, compatibility_check_aarch64, stateless_tests_asan_1_2, stateless_tests_asan_2_2, stateless_tests_release, stateless_tests_release_old_analyzer_s3_databasereplicated_1_2, stateless_tests_release_old_analyzer_s3_databasereplicated_2_2, stateless_tests_release_parallelreplicas_s3_storage, stateless_tests_debug, stateless_tests_tsan_1_3, stateless_tests_tsan_2_3, stateless_tests_tsan_3_3, stateless_tests_msan_1_4, stateless_tests_msan_2_4, stateless_tests_msan_3_4, stateless_tests_msan_4_4, stateless_tests_ubsan, stateless_tests_debug_s3_storage, stateless_tests_tsan_s3_storage_1_3, stateless_tests_tsan_s3_storage_2_3, stateless_tests_tsan_s3_storage_3_3, stateless_tests_aarch64, stateless_tests_coverage_1_6, stateless_tests_coverage_2_6, stateless_tests_coverage_3_6, stateless_tests_coverage_4_6, stateless_tests_coverage_5_6, stateless_tests_coverage_6_6, bugfix_validation, stateless_tests_asan_flaky_check, integration_tests_asan_old_analyzer_1_6, integration_tests_asan_old_analyzer_2_6, integration_tests_asan_old_analyzer_3_6, integration_tests_asan_old_analyzer_4_6, integration_tests_asan_old_analyzer_5_6, integration_tests_asan_old_analyzer_6_6, integration_tests_release_1_4, integration_tests_release_2_4, integration_tests_release_3_4, integration_tests_release_4_4, integration_tests_aarch64_1_4, integration_tests_aarch64_2_4, integration_tests_aarch64_3_4, integration_tests_aarch64_4_4, integration_tests_tsan_1_6, integration_tests_tsan_2_6, integration_tests_tsan_3_6, integration_tests_tsan_4_6, integration_tests_tsan_5_6, integration_tests_tsan_6_6, integration_tests_asan_flaky_check, stress_test_debug, stress_test_tsan, stress_test_asan, stress_test_ubsan, stress_test_msan, upgrade_check_asan, upgrade_check_tsan, upgrade_check_msan, upgrade_check_debug, ast_fuzzer_debug, ast_fuzzer_asan, ast_fuzzer_tsan, ast_fuzzer_msan, ast_fuzzer_ubsan, buzzhouse_debug, buzzhouse_asan, buzzhouse_tsan, buzzhouse_msan, buzzhouse_ubsan, performance_comparison_amd_release_master_head_1_3, performance_comparison_amd_release_master_head_2_3, performance_comparison_amd_release_master_head_3_3, performance_comparison_arm_release_master_head_1_3, performance_comparison_arm_release_master_head_2_3, performance_comparison_arm_release_master_head_3_3]\n+    needs: [config_workflow, dockers_build_arm, dockers_build_amd_and_merge, style_check, docs_check, fast_test, build_amd_tidy, build_arm_tidy, build_amd_debug, build_amd_release, build_amd_asan, build_amd_tsan, build_amd_msan, build_amd_ubsan, build_amd_binary, build_arm_release, build_arm_asan, build_arm_coverage, build_arm_binary, build_amd_darwin, build_arm_darwin, build_arm_v80compat, build_amd_freebsd, build_ppc64le, build_amd_compat, build_amd_musl, build_riscv64, build_s390x, build_loongarch64, build_fuzzers, unit_tests_asan, unit_tests_tsan, unit_tests_msan, unit_tests_ubsan, docker_server_image, docker_keeper_image, install_packages_release, install_packages_aarch64, compatibility_check_release, compatibility_check_aarch64, stateless_tests_asan_1_2, stateless_tests_asan_2_2, stateless_tests_release, stateless_tests_release_old_analyzer_s3_databasereplicated_1_2, stateless_tests_release_old_analyzer_s3_databasereplicated_2_2, stateless_tests_release_parallelreplicas_s3_storage, stateless_tests_debug, stateless_tests_tsan_1_3, stateless_tests_tsan_2_3, stateless_tests_tsan_3_3, stateless_tests_msan_1_4, stateless_tests_msan_2_4, stateless_tests_msan_3_4, stateless_tests_msan_4_4, stateless_tests_ubsan, stateless_tests_debug_s3_storage, stateless_tests_tsan_s3_storage_1_3, stateless_tests_tsan_s3_storage_2_3, stateless_tests_tsan_s3_storage_3_3, stateless_tests_aarch64, stateless_tests_coverage_1_6, stateless_tests_coverage_2_6, stateless_tests_coverage_3_6, stateless_tests_coverage_4_6, stateless_tests_coverage_5_6, stateless_tests_coverage_6_6, bugfix_validation, stateless_tests_asan_flaky_check, integration_tests_asan_old_analyzer_1_6, integration_tests_asan_old_analyzer_2_6, integration_tests_asan_old_analyzer_3_6, integration_tests_asan_old_analyzer_4_6, integration_tests_asan_old_analyzer_5_6, integration_tests_asan_old_analyzer_6_6, integration_tests_release_1_4, integration_tests_release_2_4, integration_tests_release_3_4, integration_tests_release_4_4, integration_tests_aarch64_1_4, integration_tests_aarch64_2_4, integration_tests_aarch64_3_4, integration_tests_aarch64_4_4, integration_tests_tsan_1_6, integration_tests_tsan_2_6, integration_tests_tsan_3_6, integration_tests_tsan_4_6, integration_tests_tsan_5_6, integration_tests_tsan_6_6, integration_tests_asan_flaky_check, stress_test_amd_debug, stress_test_amd_tsan, stress_test_arm_asan, stress_test_amd_ubsan, stress_test_amd_msan, upgrade_check_arm_asan, upgrade_check_amd_tsan, upgrade_check_amd_msan, upgrade_check_amd_debug, ast_fuzzer_amd_debug, ast_fuzzer_arm_asan, ast_fuzzer_amd_tsan, ast_fuzzer_amd_msan, ast_fuzzer_amd_ubsan, buzzhouse_amd_debug, buzzhouse_arm_asan, buzzhouse_amd_tsan, buzzhouse_amd_msan, buzzhouse_amd_ubsan, performance_comparison_amd_release_master_head_1_3, performance_comparison_amd_release_master_head_2_3, performance_comparison_amd_release_master_head_3_3, performance_comparison_arm_release_master_head_1_3, performance_comparison_arm_release_master_head_2_3, performance_comparison_arm_release_master_head_3_3]\n     if: ${{ !cancelled() }}\n     name: \"Finish Workflow\"\n     outputs:\ndiff --git a/.github/workflows/release_branches.yml b/.github/workflows/release_branches.yml\nindex c404ea3ae4cc..9575d6220027 100644\n--- a/.github/workflows/release_branches.yml\n+++ b/.github/workflows/release_branches.yml\n@@ -1459,11 +1459,11 @@ jobs:\n             python3 -m praktika run 'Integration tests (tsan, 6/6)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_debug:\n+  stress_test_amd_debug:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_debug]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGRlYnVnKQ==') }}\n-    name: \"Stress test (debug)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF9kZWJ1Zyk=') }}\n+    name: \"Stress test (amd_debug)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -1492,16 +1492,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (debug)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_debug)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (debug)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_debug)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_tsan:\n+  stress_test_amd_tsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_tsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKHRzYW4p') }}\n-    name: \"Stress test (tsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF90c2FuKQ==') }}\n+    name: \"Stress test (amd_tsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -1530,16 +1530,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (tsan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_tsan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_asan:\n-    runs-on: [self-hosted, func-tester]\n-    needs: [config_workflow, dockers_build_amd_and_merge, build_amd_asan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFzYW4p') }}\n-    name: \"Stress test (asan)\"\n+  stress_test_arm_asan:\n+    runs-on: [self-hosted, func-tester-aarch64]\n+    needs: [config_workflow, dockers_build_amd_and_merge, build_arm_asan]\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFybV9hc2FuKQ==') }}\n+    name: \"Stress test (arm_asan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -1568,16 +1568,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (asan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (arm_asan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (asan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (arm_asan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_ubsan:\n+  stress_test_amd_ubsan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_ubsan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKHVic2FuKQ==') }}\n-    name: \"Stress test (ubsan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF91YnNhbik=') }}\n+    name: \"Stress test (amd_ubsan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -1606,16 +1606,16 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (ubsan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_ubsan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (ubsan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_ubsan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n           fi\n \n-  stress_test_msan:\n+  stress_test_amd_msan:\n     runs-on: [self-hosted, func-tester]\n     needs: [config_workflow, dockers_build_amd_and_merge, build_amd_msan]\n-    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKG1zYW4p') }}\n-    name: \"Stress test (msan)\"\n+    if: ${{ !failure() && !cancelled() && !contains(fromJson(needs.config_workflow.outputs.data).cache_success_base64, 'U3RyZXNzIHRlc3QgKGFtZF9tc2FuKQ==') }}\n+    name: \"Stress test (amd_msan)\"\n     outputs:\n       data: ${{ steps.run.outputs.DATA }}\n     steps:\n@@ -1644,7 +1644,7 @@ jobs:\n           . ./ci/tmp/praktika_setup_env.sh\n           set -o pipefail\n           if command -v ts &> /dev/null; then\n-            python3 -m praktika run 'Stress test (msan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_msan)' --workflow \"ReleaseBranchCI\" --ci |& ts '[%Y-%m-%d %H:%M:%S]' | tee ./ci/tmp/job.log\n           else\n-            python3 -m praktika run 'Stress test (msan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n+            python3 -m praktika run 'Stress test (amd_msan)' --workflow \"ReleaseBranchCI\" --ci |& tee ./ci/tmp/job.log\n           fi\ndiff --git a/ci/defs/defs.py b/ci/defs/defs.py\nindex 466d182bc44e..91d22f791cbc 100644\n--- a/ci/defs/defs.py\n+++ b/ci/defs/defs.py\n@@ -283,9 +283,10 @@ class BuildTypes(metaclass=MetaClasses.WithIter):\n     ARM_RELEASE = \"arm_release\"\n     ARM_ASAN = \"arm_asan\"\n \n-    AMD_COVERAGE = \"amd_coverage\"\n+    ARM_COVERAGE = \"arm_coverage\"\n     ARM_BINARY = \"arm_binary\"\n     AMD_TIDY = \"amd_tidy\"\n+    ARM_TIDY = \"arm_tidy\"\n     AMD_DARWIN = \"amd_darwin\"\n     ARM_DARWIN = \"arm_darwin\"\n     ARM_V80COMPAT = \"arm_v80compat\"\ndiff --git a/ci/defs/job_configs.py b/ci/defs/job_configs.py\nindex 5ba7a1a69820..e8fbe81bc463 100644\n--- a/ci/defs/job_configs.py\n+++ b/ci/defs/job_configs.py\n@@ -87,6 +87,36 @@ class JobConfigs:\n             RunnerLabels.BUILDER_AMD,\n         ],\n     )\n+    tidy_arm_build_jobs = Job.Config(\n+        name=JobNames.BUILD,\n+        runs_on=[\"...from params...\"],\n+        requires=[\"Build (amd_tidy)\"],\n+        command=\"python3 ./ci/jobs/build_clickhouse.py --build-type {PARAMETER}\",\n+        run_in_docker=\"clickhouse/binary-builder+--network=host\",\n+        timeout=3600 * 4,\n+        allow_merge_on_failure=True,\n+        digest_config=Job.CacheDigestConfig(\n+            include_paths=[\n+                \"./src\",\n+                \"./contrib/\",\n+                \"./CMakeLists.txt\",\n+                \"./PreLoad.cmake\",\n+                \"./cmake\",\n+                \"./base\",\n+                \"./programs\",\n+                \"./rust\",\n+                \"./ci/jobs/build_clickhouse.py\",\n+            ],\n+            with_git_submodules=True,\n+        ),\n+    ).parametrize(\n+        parameter=[\n+            BuildTypes.ARM_TIDY,\n+        ],\n+        runs_on=[\n+            RunnerLabels.BUILDER_ARM,\n+        ],\n+    )\n     build_jobs = Job.Config(\n         name=JobNames.BUILD,\n         runs_on=[\"...from params...\"],\n@@ -202,7 +232,7 @@ class JobConfigs:\n         post_hooks=[\"python3 ./ci/jobs/scripts/job_hooks/build_post_hook.py\"],\n     ).parametrize(\n         parameter=[\n-            BuildTypes.AMD_COVERAGE,\n+            BuildTypes.ARM_COVERAGE,\n             BuildTypes.ARM_BINARY,\n             BuildTypes.AMD_DARWIN,\n             BuildTypes.ARM_DARWIN,\n@@ -232,14 +262,14 @@ class JobConfigs:\n             [],  # no need for fuzzers artifacts in normal pr run [ArtifactNames.FUZZERS, ArtifactNames.FUZZERS_CORPUS],\n         ],\n         runs_on=[\n-            RunnerLabels.BUILDER_AMD,  # BuildTypes.AMD_COVERAGE\n+            RunnerLabels.BUILDER_ARM,  # BuildTypes.ARM_COVERAGE\n             RunnerLabels.BUILDER_ARM,  # BuildTypes.ARM_BINARY\n             RunnerLabels.BUILDER_AMD,  # BuildTypes.AMD_DARWIN,\n             RunnerLabels.BUILDER_ARM,  # BuildTypes.ARM_DARWIN,\n-            RunnerLabels.BUILDER_AMD,  # BuildTypes.ARM_V80COMPAT,\n+            RunnerLabels.BUILDER_ARM,  # BuildTypes.ARM_V80COMPAT,\n             RunnerLabels.BUILDER_AMD,  # BuildTypes.AMD_FREEBSD,\n             RunnerLabels.BUILDER_ARM,  # BuildTypes.PPC64LE,\n-            RunnerLabels.BUILDER_AMD,  # BuildTypes.AMD_COMPAT,\n+            RunnerLabels.BUILDER_ARM,  # BuildTypes.AMD_COMPAT,\n             RunnerLabels.BUILDER_AMD,  # BuildTypes.AMD_MUSL,\n             RunnerLabels.BUILDER_ARM,  # BuildTypes.RISCV64,\n             RunnerLabels.BUILDER_AMD,  # BuildTypes.S390X,\n@@ -354,8 +384,8 @@ class JobConfigs:\n         allow_merge_on_failure=True,\n     ).parametrize(\n         parameter=[f\"coverage, {i}/6\" for i in range(1, 7)],\n-        runs_on=[RunnerLabels.FUNC_TESTER_AMD for _ in range(6)],\n-        requires=[[\"Build (amd_coverage)\"] for _ in range(6)],\n+        runs_on=[RunnerLabels.FUNC_TESTER_ARM for _ in range(6)],\n+        requires=[[\"Build (arm_coverage)\"] for _ in range(6)],\n     )\n     functional_tests_jobs_non_required = Job.Config(\n         name=JobNames.STATELESS,\n@@ -425,7 +455,7 @@ class JobConfigs:\n     )\n     functional_tests_jobs_azure_master_only = Job.Config(\n         name=JobNames.STATELESS,\n-        runs_on=RunnerLabels.FUNC_TESTER_AMD,\n+        runs_on=RunnerLabels.FUNC_TESTER_ARM,\n         command=\"cd ./tests/ci && python3 ci.py --run-from-praktika\",\n         digest_config=Job.CacheDigestConfig(\n             include_paths=[\n@@ -441,19 +471,14 @@ class JobConfigs:\n         allow_merge_on_failure=True,\n     ).parametrize(\n         parameter=[\n-            \"azure, asan, 1/3\",\n-            \"azure, asan, 2/3\",\n-            \"azure, asan, 3/3\",\n-        ],\n-        runs_on=[\n-            RunnerLabels.FUNC_TESTER_AMD,\n-            RunnerLabels.FUNC_TESTER_AMD,\n-            RunnerLabels.FUNC_TESTER_AMD,\n+            \"azure, arm_asan, 1/3\",\n+            \"azure, arm_asan, 2/3\",\n+            \"azure, arm_asan, 3/3\",\n         ],\n         requires=[\n-            [\"Build (amd_asan)\"],  # azure asan 1\n-            [\"Build (amd_asan)\"],  # azure asan 2\n-            [\"Build (amd_asan)\"],  # azure asan 3\n+            [\"Build (arm_asan)\"],  # azure asan 1\n+            [\"Build (arm_asan)\"],  # azure asan 2\n+            [\"Build (arm_asan)\"],  # azure asan 3\n         ],\n     )\n     bugfix_validation_job = Job.Config(\n@@ -508,23 +533,23 @@ class JobConfigs:\n         allow_merge_on_failure=True,\n     ).parametrize(\n         parameter=[\n-            \"debug\",\n-            \"tsan\",\n-            \"asan\",\n-            \"ubsan\",\n-            \"msan\",\n+            \"amd_debug\",\n+            \"amd_tsan\",\n+            \"arm_asan\",\n+            \"amd_ubsan\",\n+            \"amd_msan\",\n         ],\n         runs_on=[\n             RunnerLabels.FUNC_TESTER_AMD,\n             RunnerLabels.FUNC_TESTER_AMD,\n-            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_ARM,\n             RunnerLabels.FUNC_TESTER_AMD,\n             RunnerLabels.FUNC_TESTER_AMD,\n         ],\n         requires=[\n             [\"Build (amd_debug)\"],\n             [\"Build (amd_tsan)\"],\n-            [\"Build (amd_asan)\"],\n+            [\"Build (arm_asan)\"],\n             [\"Build (amd_ubsan)\"],\n             [\"Build (amd_msan)\"],\n         ],\n@@ -573,19 +598,19 @@ class JobConfigs:\n         allow_merge_on_failure=True,\n     ).parametrize(\n         parameter=[\n-            \"asan\",\n-            \"tsan\",\n-            \"msan\",\n-            \"debug\",\n+            \"arm_asan\",\n+            \"amd_tsan\",\n+            \"amd_msan\",\n+            \"amd_debug\",\n         ],\n         runs_on=[\n-            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_ARM,\n             RunnerLabels.FUNC_TESTER_AMD,\n             RunnerLabels.FUNC_TESTER_AMD,\n             RunnerLabels.FUNC_TESTER_AMD,\n         ],\n         requires=[\n-            [\"Build (amd_asan)\"],\n+            [\"Build (arm_asan)\"],\n             [\"Build (amd_tsan)\"],\n             [\"Build (amd_msan)\"],\n             [\"Build (amd_debug)\"],\n@@ -712,16 +737,22 @@ class JobConfigs:\n         allow_merge_on_failure=True,\n     ).parametrize(\n         parameter=[\n-            \"debug\",\n-            \"asan\",\n-            \"tsan\",\n-            \"msan\",\n-            \"ubsan\",\n+            \"amd_debug\",\n+            \"arm_asan\",\n+            \"amd_tsan\",\n+            \"amd_msan\",\n+            \"amd_ubsan\",\n+        ],\n+        runs_on=[\n+            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_ARM,\n+            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_AMD,\n         ],\n-        runs_on=[RunnerLabels.FUNC_TESTER_AMD for _ in range(5)],\n         requires=[\n             [\"Build (amd_debug)\"],\n-            [\"Build (amd_asan)\"],\n+            [\"Build (arm_asan)\"],\n             [\"Build (amd_tsan)\"],\n             [\"Build (amd_msan)\"],\n             [\"Build (amd_ubsan)\"],\n@@ -734,16 +765,22 @@ class JobConfigs:\n         allow_merge_on_failure=True,\n     ).parametrize(\n         parameter=[\n-            \"debug\",\n-            \"asan\",\n-            \"tsan\",\n-            \"msan\",\n-            \"ubsan\",\n+            \"amd_debug\",\n+            \"arm_asan\",\n+            \"amd_tsan\",\n+            \"amd_msan\",\n+            \"amd_ubsan\",\n+        ],\n+        runs_on=[\n+            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_ARM,\n+            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_AMD,\n+            RunnerLabels.FUNC_TESTER_AMD,\n         ],\n-        runs_on=[RunnerLabels.FUNC_TESTER_AMD for _ in range(5)],\n         requires=[\n             [\"Build (amd_debug)\"],\n-            [\"Build (amd_asan)\"],\n+            [\"Build (arm_asan)\"],\n             [\"Build (amd_tsan)\"],\n             [\"Build (amd_msan)\"],\n             [\"Build (amd_ubsan)\"],\ndiff --git a/ci/jobs/build_clickhouse.py b/ci/jobs/build_clickhouse.py\nindex 32ae6c3d237b..3539f5683ca5 100644\n--- a/ci/jobs/build_clickhouse.py\n+++ b/ci/jobs/build_clickhouse.py\n@@ -22,9 +22,10 @@\n     BuildTypes.AMD_UBSAN: f\"    cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=0 -DSANITIZE=undefined -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=1 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_INSTALL_LOCALSTATEDIR=/var -DCMAKE_SKIP_INSTALL_ALL_DEPENDENCY=ON\",\n     BuildTypes.ARM_RELEASE: f\"  cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=1 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=0 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_INSTALL_LOCALSTATEDIR=/var -DCMAKE_SKIP_INSTALL_ALL_DEPENDENCY=ON -DSPLIT_DEBUG_SYMBOLS=ON -DBUILD_STANDALONE_KEEPER=1\",\n     BuildTypes.ARM_ASAN: f\"     cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=0 -DSANITIZE=address   -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=1 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_INSTALL_LOCALSTATEDIR=/var -DCMAKE_SKIP_INSTALL_ALL_DEPENDENCY=ON -DCMAKE_TOOLCHAIN_FILE={current_directory}/cmake/linux/toolchain-aarch64.cmake\",\n-    BuildTypes.AMD_COVERAGE: f\" cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=0 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=0 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_INSTALL_LOCALSTATEDIR=/var -DCMAKE_SKIP_INSTALL_ALL_DEPENDENCY=ON -DSANITIZE_COVERAGE=1\",\n+    BuildTypes.ARM_COVERAGE: f\" cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=0 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=0 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_INSTALL_LOCALSTATEDIR=/var -DCMAKE_SKIP_INSTALL_ALL_DEPENDENCY=ON -DSANITIZE_COVERAGE=1\",\n     BuildTypes.ARM_BINARY: f\"   cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=0 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=0 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON\",\n     BuildTypes.AMD_TIDY: f\"     cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=Debug -DENABLE_THINLTO=0 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=0 -DENABLE_TESTS=1 -DENABLE_UTILS=1 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DENABLE_CLANG_TIDY=1 -DENABLE_EXAMPLES=1 -DENABLE_BUZZHOUSE=1 -DENABLE_RUST=0\",\n+    BuildTypes.ARM_TIDY: f\"     cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=Debug -DENABLE_THINLTO=0 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=0 -DENABLE_TESTS=1 -DENABLE_UTILS=1 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DENABLE_CLANG_TIDY=1 -DENABLE_EXAMPLES=1 -DENABLE_BUZZHOUSE=1 -DENABLE_RUST=0\",\n     BuildTypes.AMD_DARWIN: f\"   cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=0 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=0 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_TOOLCHAIN_FILE={current_directory}/cmake/darwin/toolchain-x86_64.cmake -DCMAKE_AR:FILEPATH=/cctools/bin/x86_64-apple-darwin-ar -DCMAKE_INSTALL_NAME_TOOL=/cctools/bin/x86_64-apple-darwin-install_name_tool -DCMAKE_RANLIB:FILEPATH=/cctools/bin/x86_64-apple-darwin-ranlib -DLINKER_NAME=/cctools/bin/x86_64-apple-darwin-ld\",\n     BuildTypes.ARM_DARWIN: f\"   cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=0 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=0 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_TOOLCHAIN_FILE={current_directory}/cmake/darwin/toolchain-aarch64.cmake -DCMAKE_AR:FILEPATH=/cctools/bin/aarch64-apple-darwin-ar -DCMAKE_INSTALL_NAME_TOOL=/cctools/bin/aarch64-apple-darwin-install_name_tool -DCMAKE_RANLIB:FILEPATH=/cctools/bin/aarch64-apple-darwin-ranlib -DLINKER_NAME=/cctools/bin/aarch64-apple-darwin-ld\",\n     BuildTypes.ARM_V80COMPAT: f\"cmake --debug-trycompile -DCMAKE_VERBOSE_MAKEFILE=1 -LA -DCMAKE_BUILD_TYPE=None  -DENABLE_THINLTO=1 -DSANITIZE=          -DENABLE_CHECK_HEAVY_BUILDS=1 -DENABLE_CLICKHOUSE_SELF_EXTRACTING=1 -DCMAKE_C_COMPILER={ToolSet.COMPILER_C} -DCMAKE_CXX_COMPILER={ToolSet.COMPILER_CPP} -DCOMPILER_CACHE=sccache -DENABLE_BUILD_PROFILING=1 -DENABLE_TESTS=0 -DENABLE_UTILS=0 -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_TOOLCHAIN_FILE={current_directory}/cmake/linux/toolchain-aarch64.cmake -DNO_ARMV81_OR_HIGHER=1\",\n@@ -48,7 +49,7 @@\n     BuildTypes.AMD_MSAN: \"msan\",\n     BuildTypes.AMD_UBSAN: \"ubsan\",\n     BuildTypes.AMD_TSAN: \"tsan\",\n-    BuildTypes.AMD_COVERAGE: \"release\",\n+    BuildTypes.ARM_COVERAGE: \"release\",\n }\n \n \n@@ -162,13 +163,13 @@ def main():\n \n     if res and JobStages.BUILD in stages:\n         run_shell(\"sccache stats\", \"sccache --show-stats\")\n-        run_shell(\"clang-tidy-cache stats\", \"clang-tidy-cache --show-stats\")\n         if build_type in BUILD_TYPE_TO_DEB_PACKAGE_TYPE:\n             targets = \"clickhouse-bundle\"\n         elif build_type == BuildTypes.FUZZERS:\n             targets = \"fuzzers\"\n-        elif build_type == BuildTypes.AMD_TIDY:\n+        elif build_type in (BuildTypes.AMD_TIDY, BuildTypes.ARM_TIDY):\n             targets = \"-k0 all\"\n+            run_shell(\"clang-tidy-cache stats\", \"clang-tidy-cache --show-stats\")\n         else:\n             targets = \"clickhouse-bundle\"\n         results.append(\n@@ -180,7 +181,8 @@ def main():\n             )\n         )\n         run_shell(\"sccache stats\", \"sccache --show-stats\")\n-        run_shell(\"clang-tidy-cache stats\", \"clang-tidy-cache --show-stats\")\n+        if build_type in (BuildTypes.AMD_TIDY, BuildTypes.ARM_TIDY):\n+            run_shell(\"clang-tidy-cache stats\", \"clang-tidy-cache --show-stats\")\n         run_shell(\"Output programs\", f\"ls -l {build_dir}/programs/\", verbose=True)\n         Shell.check(\"pwd\")\n         res = results[-1].is_ok()\ndiff --git a/ci/workflows/pull_request.py b/ci/workflows/pull_request.py\nindex 2b522c397b3c..e3a5440b3feb 100644\n--- a/ci/workflows/pull_request.py\n+++ b/ci/workflows/pull_request.py\n@@ -19,6 +19,7 @@\n         JobConfigs.docs_job,\n         JobConfigs.fast_test,\n         *JobConfigs.tidy_build_jobs,\n+        *JobConfigs.tidy_arm_build_jobs,\n         *[\n             job.set_dependency(\n                 [\ndiff --git a/clang_tidy_cache b/clang_tidy_cache\nnew file mode 100755\nindex 000000000000..fa85185b333c\n--- /dev/null\n+++ b/clang_tidy_cache\n@@ -0,0 +1,1449 @@\n+#!/usr/bin/env python3\n+# coding: UTF-8\n+# Copyright (c) 2019-2024 Matus Chochlik\n+# Distributed under the Boost Software License, Version 1.0.\n+# See accompanying file LICENSE_1_0.txt or copy at\n+#  http://www.boost.org/LICENSE_1_0.txt\n+\n+import os\n+import re\n+import sys\n+import errno\n+import getpass\n+import logging\n+import hashlib\n+import tempfile\n+import subprocess\n+import json\n+import shlex\n+import time\n+import traceback\n+import typing as tp\n+\n+try:\n+    import redis\n+except ImportError:\n+    redis = None\n+\n+# ------------------------------------------------------------------------------\n+def getenv_boolean_flag(name):\n+    return os.getenv(name, \"0\").lower() in [\"true\", \"1\", \"yes\", \"y\", \"on\"]\n+\n+# ------------------------------------------------------------------------------\n+def mkdir_p(path):\n+    try:\n+        os.makedirs(path)\n+    except OSError as os_error:\n+        if os_error.errno == errno.EEXIST and os.path.isdir(path):\n+            pass\n+        else:\n+            raise\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyCacheOpts(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, args):\n+        self._log = log\n+\n+        if len(args) < 1:\n+            self._log.error(\"Missing arguments\")\n+\n+        self._original_args = args\n+        self._clang_tidy_args = []\n+        self._compiler_args = []\n+        self._cache_dir = None\n+        self._compile_commands_db = None\n+\n+        self._strip_list = os.getenv(\"CTCACHE_STRIP\", \"\").split(os.pathsep)\n+\n+        args = self._split_compiler_clang_tidy_args(args)\n+        self._adjust_compiler_args(args)\n+\n+    # --------------------------------------------------------------------------\n+    def __repr__(self):\n+        return \\\n+            f\"ClangTidyCacheOpts(\" \\\n+                f\"clang_tidy_args:{self._clang_tidy_args},\" \\\n+                f\"compiler_args:{self._compiler_args},\" \\\n+                f\"original_args:{self._original_args}\" \\\n+            f\")\"\n+\n+    # --------------------------------------------------------------------------\n+    def running_on_msvc(self):\n+        if self._compiler_args:\n+            return os.path.basename(self._compiler_args[0]) == \"cl.exe\"\n+        return False\n+\n+    # --------------------------------------------------------------------------\n+    def running_on_clang_cl(self):\n+        if self._compiler_args:\n+            return os.path.basename(self._compiler_args[0]) == \"clang-cl.exe\"\n+        return False\n+\n+    # --------------------------------------------------------------------------\n+    def _split_compiler_clang_tidy_args(self, args):\n+        # splits arguments starting with - on the first =\n+        args = [arg.split('=', 1) if arg.startswith('-p') else [arg] for arg in args]\n+        args = [arg for sub in args for arg in sub]\n+\n+        if args.count(\"--\") == 1:\n+            # Invoked with compiler args on the actual command line\n+            i = args.index(\"--\")\n+            self._clang_tidy_args = args[:i]\n+            self._compiler_args = args[i+1:]\n+        elif args.count(\"-p\") == 1:\n+            # Invoked with compiler args in a compile commands json db\n+            i = args.index(\"-p\")\n+            self._clang_tidy_args = args\n+\n+            i += 1\n+            if i >= len(args):\n+                return\n+\n+            cdb_path = args[i]\n+            if os.path.isdir(cdb_path):\n+                cdb_path = os.path.join(cdb_path, \"compile_commands.json\")\n+            self._load_compile_command_db(cdb_path)\n+\n+            i += 1\n+            if i >= len(args):\n+                return\n+\n+            # This assumes that the filename occurs after the -p <cdb path>\n+            # and that there is only one of them\n+            filenames = [arg for arg in args[i:] if not arg.startswith(\"-\")]\n+            if len(filenames) > 0:\n+                self._compiler_args = self._compiler_args_for(filenames[0])\n+        else:\n+            # Invoked as pure clang-tidy command\n+            self._clang_tidy_args = args[1:]\n+        return args\n+\n+    # --------------------------------------------------------------------------\n+    def _adjust_compiler_args(self, args):\n+        if self._compiler_args:\n+            pos = next((pos for pos, arg in enumerate(self._compiler_args) if arg.startswith('-D')), 1)\n+            self._compiler_args.insert(pos, \"-D__clang_analyzer__=1\")\n+            for i in range(1, len(self._compiler_args)):\n+                if self._compiler_args[i-1] in [\"-o\", \"--output\"]:\n+                    self._compiler_args[i] = \"-\"\n+                if self._compiler_args[i-1] in [\"-c\"]:\n+                    self._compiler_args[i-1] = \"-E\"\n+            for i in range(1, len(self._compiler_args)):\n+                if self._compiler_args[i-1] in [\"-E\"]:\n+                    if self.running_on_msvc():\n+                        self._compiler_args[i-1] = \"-EP\"\n+                    else:\n+                        self._compiler_args.insert(i, \"-P\")\n+                    if self.keep_comments():\n+                        self._compiler_args.insert(i, \"-C\")\n+\n+    # --------------------------------------------------------------------------\n+    def _load_compile_command_db(self, filename):\n+        try:\n+            with open(filename) as f:\n+                cdb = f.read()\n+                try:\n+                    js = cdb.replace(r'\\\\\\\"', \"'\").replace(\"\\\\\", \"\\\\\\\\\")\n+                    self._compile_commands_db = json.loads(js)\n+                except JSONDecodeError:\n+                    self._compile_commands_db = json.loads(cdb)\n+\n+        except Exception as err:\n+            self._log.error(\"Loading compile command DB failed: {0}\".format(repr(err)))\n+            return False\n+\n+    # --------------------------------------------------------------------------\n+    def _compiler_args_for(self, filename):\n+        if self._compile_commands_db is None:\n+            return []\n+\n+        filename = os.path.expanduser(filename)\n+        filename = os.path.realpath(filename)\n+\n+        for command in self._compile_commands_db:\n+            db_filename = command[\"file\"]\n+            try:\n+                if os.path.samefile(filename, db_filename):\n+                    try:\n+                        return shlex.split(command[\"command\"])\n+                    except KeyError:\n+                        try:\n+                            return shlex.split(command[\"arguments\"][0])\n+                        except:\n+                            return \"clang-tidy\"\n+            except FileNotFoundError:\n+                continue\n+\n+        return []\n+\n+    # --------------------------------------------------------------------------\n+    def should_print_dir(self):\n+        try:\n+            return self._original_args[0] == \"--cache-dir\"\n+        except IndexError:\n+            return False\n+\n+    # --------------------------------------------------------------------------\n+    def should_print_stats(self):\n+        try:\n+            return self._original_args[0] == \"--show-stats\"\n+        except IndexError:\n+            return False\n+\n+    # --------------------------------------------------------------------------\n+    def should_print_stats_raw(self):\n+        try:\n+            return self._original_args[0] == \"--print-stats\"\n+        except IndexError:\n+            return False\n+\n+    # --------------------------------------------------------------------------\n+    def should_remove_dir(self):\n+        try:\n+            return self._original_args[0] == \"--clean\"\n+        except IndexError:\n+            return False\n+\n+    # --------------------------------------------------------------------------\n+    def should_zero_stats(self):\n+        try:\n+            return self._original_args[0] == \"--zero-stats\"\n+        except IndexError:\n+            return False\n+\n+    # --------------------------------------------------------------------------\n+    def should_print_usage(self):\n+        return len(self.original_args()) < 1\n+\n+    # --------------------------------------------------------------------------\n+    def original_args(self):\n+        return self._original_args\n+\n+    # --------------------------------------------------------------------------\n+    def clang_tidy_args(self):\n+        return self._clang_tidy_args\n+\n+    # --------------------------------------------------------------------------\n+    def compiler_args(self):\n+        return self._compiler_args\n+\n+    # --------------------------------------------------------------------------\n+    @property\n+    def cache_dir(self):\n+        if self._cache_dir:\n+            return self._cache_dir\n+\n+        try:\n+            user = getpass.getuser()\n+        except KeyError:\n+            user = \"unknown\"\n+        self._cache_dir = os.getenv(\n+            \"CTCACHE_DIR\",\n+            os.path.join(\n+                tempfile.tempdir if tempfile.tempdir else \"/tmp\", \"ctcache-\" + user\n+            ),\n+        )\n+        return self._cache_dir\n+\n+     # --------------------------------------------------------------------------\n+    def strip_paths(self, input):\n+        for item in self._strip_list:\n+            input = re.sub(item, '', input)\n+        return input\n+\n+    # --------------------------------------------------------------------------\n+    def adjust_chunk(self, x):\n+        x = x.strip()\n+        r = str().encode(\"utf8\")\n+        if not x.startswith(\"# \"):\n+            for w in x.split():\n+                w = w.strip('\"')\n+                if os.path.exists(w):\n+                    w = os.path.realpath(w)\n+                w = self.strip_paths(w)\n+                w.strip()\n+                if w:\n+                    r += w.encode(\"utf8\")\n+        return r\n+\n+    # --------------------------------------------------------------------------\n+    def has_s3(self):\n+        return \"CTCACHE_S3_BUCKET\" in os.environ\n+\n+    # --------------------------------------------------------------------------\n+    def s3_bucket(self):\n+        return os.getenv(\"CTCACHE_S3_BUCKET\")\n+\n+    # --------------------------------------------------------------------------\n+    def s3_bucket_folder(self):\n+        return os.getenv(\"CTCACHE_S3_FOLDER\", 'clang-tidy-cache')\n+\n+    # --------------------------------------------------------------------------\n+    def s3_no_credentials(self):\n+        return os.getenv(\"CTCACHE_S3_NO_CREDENTIALS\", \"\")\n+\n+    # --------------------------------------------------------------------------\n+    def s3_read_only(self):\n+        return getenv_boolean_flag(\"CTCACHE_S3_READ_ONLY\")\n+\n+    # --------------------------------------------------------------------------\n+    def has_gcs(self):\n+        return \"CTCACHE_GCS_BUCKET\" in os.environ\n+\n+    # --------------------------------------------------------------------------\n+    def gcs_bucket(self):\n+        return os.getenv(\"CTCACHE_GCS_BUCKET\")\n+\n+    # --------------------------------------------------------------------------\n+    def gcs_bucket_folder(self):\n+        return os.getenv(\"CTCACHE_GCS_FOLDER\", 'clang-tidy-cache')\n+\n+    # --------------------------------------------------------------------------\n+    def gcs_no_credentials(self):\n+        return os.getenv(\"CTCACHE_GCS_NO_CREDENTIALS\", None)\n+\n+    # --------------------------------------------------------------------------\n+    def gcs_read_only(self):\n+        return getenv_boolean_flag(\"CTCACHE_GCS_READ_ONLY\")\n+\n+    # --------------------------------------------------------------------------\n+    def cache_locally(self):\n+        return getenv_boolean_flag(\"CTCACHE_LOCAL\")\n+\n+    # --------------------------------------------------------------------------\n+    def no_local_stats(self):\n+        return getenv_boolean_flag(\"CTCACHE_NO_LOCAL_STATS\")\n+\n+    # --------------------------------------------------------------------------\n+    def no_local_writeback(self):\n+        return getenv_boolean_flag(\"CTCACHE_NO_LOCAL_WRITEBACK\")\n+\n+    # --------------------------------------------------------------------------\n+    def has_host(self):\n+        return os.getenv(\"CTCACHE_HOST\") is not None\n+\n+    # --------------------------------------------------------------------------\n+    def rest_host(self):\n+        return os.getenv(\"CTCACHE_HOST\", \"localhost\")\n+\n+    # --------------------------------------------------------------------------\n+    def rest_proto(self):\n+        return os.getenv(\"CTCACHE_PROTO\", \"http\")\n+\n+    # --------------------------------------------------------------------------\n+    def rest_port(self):\n+        return int(os.getenv(\"CTCACHE_PORT\", 5000))\n+\n+    # --------------------------------------------------------------------------\n+    def rest_host_read_only(self):\n+        return getenv_boolean_flag(\"CTCACHE_HOST_READ_ONLY\")\n+\n+    # --------------------------------------------------------------------------\n+    def save_output(self) -> bool:\n+        return getenv_boolean_flag(\"CTCACHE_SAVE_OUTPUT\")\n+\n+    # --------------------------------------------------------------------------\n+    def ignore_output(self) -> bool:\n+        return self.save_output() or \"CTCACHE_IGNORE_OUTPUT\" in os.environ\n+\n+    # --------------------------------------------------------------------------\n+    def save_all(self) -> bool:\n+        return self.save_output() or \"CTCACHE_SAVE_ALL\" in os.environ\n+\n+    # --------------------------------------------------------------------------\n+    def debug_enabled(self):\n+        return getenv_boolean_flag(\"CTCACHE_DEBUG\")\n+\n+    # --------------------------------------------------------------------------\n+    def dump_enabled(self):\n+        return getenv_boolean_flag(\"CTCACHE_DUMP\")\n+\n+    # --------------------------------------------------------------------------\n+    def dump_dir(self):\n+        return os.getenv(\"CTCACHE_DUMP_DIR\", tempfile.gettempdir())\n+\n+    # --------------------------------------------------------------------------\n+    def strip_src(self):\n+        return getenv_boolean_flag(\"CTCACHE_STRIP_SRC\")\n+\n+    # --------------------------------------------------------------------------\n+    def keep_comments(self):\n+        return getenv_boolean_flag(\"CTCACHE_KEEP_COMMENTS\")\n+\n+    # --------------------------------------------------------------------------\n+    def exclude_hash_regex(self):\n+        return os.getenv(\"CTCACHE_EXCLUDE_HASH_REGEX\")\n+\n+    # --------------------------------------------------------------------------\n+    def exclude_hash(self, chunk):\n+        return self.exclude_hash_regex() is not None and \\\n+            re.match(self.exclude_hash_regex(), chunk.decode(\"utf8\"))\n+\n+    # --------------------------------------------------------------------------\n+    def exclude_user_config(self):\n+        return getenv_boolean_flag(\"CTCACHE_EXCLUDE_USER_CONFIG\")\n+\n+    # --------------------------------------------------------------------------\n+    def has_redis_host(self) -> bool:\n+        return \"CTCACHE_REDIS_HOST\" in os.environ\n+\n+    # --------------------------------------------------------------------------\n+    def redis_host(self) -> str:\n+        return os.getenv(\"CTCACHE_REDIS_HOST\", \"\")\n+\n+    # --------------------------------------------------------------------------\n+    def redis_port(self) -> int:\n+        return int(os.getenv(\"CTCACHE_REDIS_PORT\", \"6379\"))\n+\n+    # --------------------------------------------------------------------------\n+    def redis_db(self) -> int:\n+        return int(os.getenv(\"CTCACHE_REDIS_DB\", \"0\"))\n+\n+    # --------------------------------------------------------------------------\n+    def redis_username(self) -> str:\n+        return os.getenv(\"CTCACHE_REDIS_USERNAME\", \"\")\n+\n+    # --------------------------------------------------------------------------\n+    def redis_password(self) -> str:\n+        return os.getenv(\"CTCACHE_REDIS_PASSWORD\", \"\")\n+\n+    # --------------------------------------------------------------------------\n+    def redis_connect_timeout(self) -> float:\n+        return float(os.getenv(\"CTCACHE_REDIS_CONNECT_TIMEOUT\", \"0.1\"))\n+\n+    # --------------------------------------------------------------------------\n+    def redis_socket_timeout(self) -> float:\n+        return float(os.getenv(\"CTCACHE_REDIS_OPERATION_TIMEOUT\", \"10.0\"))\n+\n+    # --------------------------------------------------------------------------\n+    def redis_cache_ttl(self) -> float:\n+        ttl = int(os.getenv(\"CTCACHE_REDIS_CACHE_TTL\", \"-1\"))\n+        if ttl < 0:\n+            return None\n+        return ttl\n+\n+    # --------------------------------------------------------------------------\n+    def redis_namespace(self) -> str:\n+        return os.getenv(\"CTCACHE_REDIS_NAMESPACE\", \"ctcache/\")\n+\n+    # --------------------------------------------------------------------------\n+    def redis_read_only(self):\n+        return getenv_boolean_flag(\"CTCACHE_REDIS_READ_ONLY\")\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyCacheHash(object):\n+    # --------------------------------------------------------------------------\n+    def _opendump(self, opts):\n+        return open(os.path.join(opts.dump_dir(), \"ctcache.dump\"), \"ab\")\n+\n+    # --------------------------------------------------------------------------\n+    def __init__(self, opts):\n+        self._hash = hashlib.sha1()\n+        if opts.dump_enabled():\n+            self._dump = self._opendump(opts)\n+        else:\n+            self._dump = None\n+        assert self._dump or not opts.dump_enabled()\n+\n+    # --------------------------------------------------------------------------\n+    def __del__(self):\n+        if self._dump:\n+            self._dump.close()\n+\n+    # --------------------------------------------------------------------------\n+    def update(self, content):\n+        if content:\n+            self._hash.update(content)\n+            if self._dump:\n+                self._dump.write(content)\n+\n+    # --------------------------------------------------------------------------\n+    def hexdigest(self):\n+        return self._hash.hexdigest()\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyServerCache(object):\n+    def __init__(self, log, opts):\n+        import requests\n+        self._requests = requests\n+        self._log = log\n+        self._opts = opts\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest):\n+        try:\n+            query = self._requests.get(self._make_query_url(digest), timeout=3)\n+            if query.status_code == 200:\n+                if query.json() is True:\n+                    return True\n+                elif query.json() is False:\n+                    return False\n+                else:\n+                    self._log.error(\"is_cached: Can't connect to server {0}, error {1}\".format(\n+                        self._opts.rest_host(), query.status_code))\n+        except:\n+            pass\n+\n+        return False\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        try:\n+            query = self._requests.get(self._make_data_url(digest), timeout=3)\n+            if query.status_code == 200:\n+                return query.text.encode('UTF-8')\n+        except:\n+            pass\n+\n+        return None\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        self.store_in_cache_with_data(digest, bytes())\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        if self._opts.rest_host_read_only():\n+            return\n+        try:\n+            query = self._requests.put(self._make_data_url(digest), data={'data': data}, timeout=3)\n+            if query.status_code != 200:\n+                self._log.error(\"store_in_cache: Can't store data in server {0}, error {1}\".format(\n+                    self._opts.rest_host(), query.status_code))\n+        except:\n+            pass\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        try:\n+            query = self._requests.get(self._make_stats_url(), timeout=3)\n+            if query.status_code == 200:\n+                return query.json()\n+            else:\n+                self._log.error(\"query_stats: Can't connect to server {0}, error {1}\".format(\n+                    self._opts.rest_host(), query.status_code))\n+        except:\n+            pass\n+        return None\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        # Not implemented\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def _make_query_url(self, digest):\n+        return \"%(proto)s://%(host)s:%(port)d/is_cached/%(digest)s\" % {\n+            \"proto\": self._opts.rest_proto(),\n+            \"host\": self._opts.rest_host(),\n+            \"port\": self._opts.rest_port(),\n+            \"digest\": digest\n+        }\n+\n+    # --------------------------------------------------------------------------\n+    def _make_data_url(self, digest):\n+        return \"%(proto)s://%(host)s:%(port)d/cache/%(digest)s\" % {\n+            \"proto\": self._opts.rest_proto(),\n+            \"host\": self._opts.rest_host(),\n+            \"port\": self._opts.rest_port(),\n+            \"digest\": digest\n+        }\n+\n+    # --------------------------------------------------------------------------\n+    def _make_stats_url(self):\n+        return \"%(proto)s://%(host)s:%(port)d/stats\" % {\n+            \"proto\": self._opts.rest_proto(),\n+            \"host\": self._opts.rest_host(),\n+            \"port\": self._opts.rest_port()\n+        }\n+\n+# ------------------------------------------------------------------------------\n+class MultiprocessLock:\n+    # --------------------------------------------------------------------------\n+    def __init__(self, lock_path, timeout=3): # timeout 3 seconds\n+        self._lock_path = os.path.abspath(os.path.expanduser(lock_path))\n+        self._timeout = timeout\n+        self._lock_handle = None\n+\n+    # --------------------------------------------------------------------------\n+    def acquire(self):\n+        start_time = time.time()\n+        while True:\n+            try:\n+                # Attempt to create the lock file exclusively\n+                self._lock_handle = os.open(self._lock_path, os.O_CREAT | os.O_EXCL)\n+                return self\n+            except FileExistsError:\n+                # File is locked, check if the timeout has been exceeded\n+                if time.time() - start_time > self._timeout:\n+                    msg = f\"Timeout ({self._timeout} seconds) exceeded while acquiring lock.\"\n+                    raise RuntimeError(msg)\n+                # Wait and try again\n+                time.sleep(0.1)\n+            except FileNotFoundError:\n+                # The path to the lock file doesn't exist, create it and retry\n+                os.makedirs(os.path.dirname(self._lock_path), exist_ok=True)\n+\n+    # --------------------------------------------------------------------------\n+    def release(self):\n+        if self._lock_handle is not None:\n+            try:\n+                os.close(self._lock_handle)\n+                os.unlink(self._lock_path)  # Remove the lock file upon release\n+            except OSError:\n+                pass  # Ignore errors if the file doesn't exist or has already been released\n+            finally:\n+                self._lock_handle = None\n+\n+    # --------------------------------------------------------------------------\n+    def __enter__(self):\n+        return self.acquire()\n+\n+    # --------------------------------------------------------------------------\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        self.release()\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyCacheStats(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, opts, name):\n+        self._log = log\n+        self._opts = opts\n+        self._name = name\n+\n+    # --------------------------------------------------------------------------\n+    def stats_file(self, digest):\n+        return os.path.join(self._opts.cache_dir, digest[:2], self._name)\n+\n+    # --------------------------------------------------------------------------\n+    def read(self):\n+        hits, misses = 0, 0\n+        for i in range(0, 256):\n+            digest = f'{i:x}'\n+            file = self.stats_file(digest)\n+            if os.path.isfile(file):\n+                h, m = self._read(file)\n+                hits += h\n+                misses += m\n+        return hits, misses\n+\n+    # --------------------------------------------------------------------------\n+    def _read(self, file):\n+        with MultiprocessLock(file + \".lock\") as _:\n+            if os.path.isfile(file):\n+                with open(file, 'r') as f:\n+                    return self.read_from_file(f)\n+            return 0,0\n+\n+    # --------------------------------------------------------------------------\n+    def read_from_file(self, f):\n+        content = f.read().split()\n+        if len(content) == 2:\n+            return int(content[0]), int(content[1])\n+        else:\n+            self._log.error(f\"Invalid stats content in: {f.name}\")\n+        return 0,0\n+\n+    # --------------------------------------------------------------------------\n+    def write_to_file(self, f, hits, misses, hit):\n+        if hit:\n+            hits += 1\n+        else:\n+            misses += 1\n+        f.write(f\"{hits} {misses}\\n\")\n+\n+    # --------------------------------------------------------------------------\n+    def update(self, digest, hit):\n+        try:\n+            file = self.stats_file(digest)\n+            mkdir_p(os.path.dirname(file))\n+            with MultiprocessLock(file + \".lock\") as _:\n+                try:\n+                    if os.path.isfile(file):\n+                        with open(file, 'r+') as fh:\n+                            hits, misses = self.read_from_file(fh)\n+                            fh.seek(0)\n+                            self.write_to_file(fh, hits, misses, hit)\n+                            fh.truncate()\n+                    else:\n+                        with open(file, 'w') as fh:\n+                            self.write_to_file(fh, 0, 0, hit)\n+                except IOError as e:\n+                    self._log.error(f\"Error writing to file: {e}\")\n+        except Exception as e:\n+            traceback.print_exc(file=sys.stdout)\n+            raise\n+\n+    # --------------------------------------------------------------------------\n+    def clear(self):\n+        for i in range(0, 256):\n+            digest = f'{i:x}'\n+            file = self.stats_file(digest)\n+            if os.path.isfile(file):\n+                os.unlink(file)\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyLocalCache(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, opts):\n+        self._log = log\n+        self._opts = opts\n+        self._hash_regex = re.compile(r'^[0-9a-f]{38}$')\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest):\n+        path = self._make_path(digest)\n+        if os.path.isfile(path):\n+            os.utime(path, None)\n+            return True\n+\n+        return False\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        path = self._make_path(digest)\n+        if os.path.isfile(path):\n+            os.utime(path, None)\n+            with open(path, \"rb\") as stream:\n+                return stream.read()\n+        else:\n+            return None\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        p = self._make_path(digest)\n+        mkdir_p(os.path.dirname(p))\n+        open(p, \"w\").close()\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        p = self._make_path(digest)\n+        mkdir_p(os.path.dirname(p))\n+        with open(p, \"wb\") as stream:\n+            stream.write(data)\n+\n+    # --------------------------------------------------------------------------\n+    def _list_cached_files(self, options, prefix):\n+        for root, dirs, files in os.walk(prefix):\n+            for prefix in dirs:\n+                for filename in self._list_cached_files(options, prefix):\n+                    if self._hash_regex.match(filename):\n+                        yield root, prefix, filename\n+            for filename in files:\n+                if self._hash_regex.match(filename):\n+                    yield root, prefix, filename\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        hash_count = sum(1 for x in self._list_cached_files(options, options.cache_dir))\n+        return {\"cached_count\": hash_count}\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def _make_path(self, digest):\n+        return os.path.join(self._opts.cache_dir, digest[:2], digest[2:])\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyRedisCache(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, opts: ClangTidyCacheOpts):\n+        self._log = log\n+        self._opts = opts\n+        assert redis\n+        self._cli = redis.Redis(\n+            host=opts.redis_host(),\n+            port=opts.redis_port(),\n+            db=opts.redis_db(),\n+            username=opts.redis_username(),\n+            password=opts.redis_password(),\n+            socket_connect_timeout=opts.redis_connect_timeout(),\n+            socket_timeout=opts.redis_socket_timeout(),\n+            # the two settings below are used to avoid sending any commands to the Redis\n+            # server other than AUTH, GET, and SET (to let the ctcache operate with a\n+            # server configuration giving only minimal permissions to the given user)\n+            lib_name=None,\n+            lib_version=None)\n+        self._namespace = opts.redis_namespace()\n+\n+    # --------------------------------------------------------------------------\n+    def _get_key_from_digest(self, digest) -> str:\n+        return self._namespace + digest\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest) -> bool:\n+        n_digest = self._get_key_from_digest(digest)\n+        return self._cli.get(n_digest) is not None\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        n_digest = self._get_key_from_digest(digest)\n+        data = self._cli.get(n_digest)\n+        ttl = self._opts.redis_cache_ttl()\n+        if not self._opts.redis_read_only() and data is not None and ttl is not None:\n+            # try to extend TTL on cache hits\n+            self._cli.expire(n_digest, ttl, xx=True)\n+        return data\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        self.store_in_cache_with_data(digest, bytes())\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        if self._opts.redis_read_only():\n+            return\n+        n_digest = self._get_key_from_digest(digest)\n+        self._cli.set(n_digest, data, ex=self._opts.redis_cache_ttl())\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        # TODO\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        # TODO\n+        pass\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyS3Cache(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, opts):\n+        from boto3 import client\n+        from botocore.exceptions import ClientError\n+        from botocore.config import Config\n+        from botocore.session import UNSIGNED\n+\n+        self._ClientError = ClientError\n+        self._log = log\n+        self._opts = opts\n+        if self._opts.s3_no_credentials():\n+            self._client = client(\"s3\", config=Config(signature_version=UNSIGNED))\n+        else:\n+            self._client = client(\"s3\")\n+        self._bucket = opts.s3_bucket()\n+        self._bucket_folder = opts.s3_bucket_folder()\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest):\n+        try:\n+            path = self._make_path(digest)\n+            self._client.get_object(Bucket=self._bucket, Key=path)\n+        except self._ClientError as e:\n+            if e.response['Error']['Code'] == \"NoSuchKey\":\n+                return False\n+            else:\n+                self._log.error(\n+                    \"Error calling S3:get_object {}\".format(str(e)))\n+                raise\n+\n+        return True\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        # TODO\n+        return None\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        if self._opts.s3_no_credentials() or self._opts.s3_read_only():\n+            return\n+        try:\n+            path = self._make_path(digest)\n+            self._client.put_object(Bucket=self._bucket, Key=path, Body=digest)\n+        except self._ClientError as e:\n+            self._log.error(\"Error calling S3:put_object {}\".format(str(e)))\n+            raise\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        # TODO\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        # TODO\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        # TODO\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def _make_path(self, digest):\n+        return os.path.join(self._bucket_folder, digest[:2], digest[2:])\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyGcsCache(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, opts):\n+        import google.cloud.storage as gcs\n+\n+        self._log = log\n+        self._opts = opts\n+        if self._opts.gcs_no_credentials():\n+            self._client = gcs.Client.create_anonymous_client()\n+        else:\n+            self._client = gcs.Client()\n+        self._bucket = self._client.bucket(opts.gcs_bucket())\n+        self._bucket_folder = opts.gcs_bucket_folder()\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest):\n+        try:\n+            path = self._make_path(digest)\n+            blob = self._bucket.blob(path)\n+            t = blob.exists()\n+            return t\n+        except Exception as e:\n+            self._log.error(\"Error calling GCS:blob.exists {}\".format(str(e)))\n+            raise\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        try:\n+            path = self._make_path(digest)\n+            blob = self._bucket.blob(path)\n+            return blob.download_as_bytes()\n+        except Exception as e:\n+            return None\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        if self._opts.gcs_no_credentials() or self._opts.gcs_read_only():\n+            return\n+        try:\n+            path = self._make_path(digest)\n+            blob = self._bucket.blob(path)\n+            blob.upload_from_string(digest)\n+        except Exception as e:\n+            self._log.error(\n+                \"Error calling GCS:blob.upload_from_string {}\".format(str(e)))\n+            raise\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        if self._opts.gcs_no_credentials() or self._opts.gcs_read_only():\n+            return\n+        try:\n+            path = self._make_path(digest)\n+            blob = self._bucket.blob(path)\n+            blob.upload_from_string(data, content_type=\"application/octet-stream\")\n+        except Exception as e:\n+            self._log.error(\n+                \"Error calling GCS:blob.upload_from_string {}\".format(str(e)))\n+            raise\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        # TODO\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        # TODO\n+        pass\n+\n+    # --------------------------------------------------------------------------\n+    def _make_path(self, digest):\n+        return os.path.join(self._bucket_folder, digest[:2], digest[2:])\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyMultiCache(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, caches):\n+        self._log = log\n+        self._caches = caches\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest):\n+        for cache in self._caches:\n+            if cache.is_cached(digest):\n+                return True\n+\n+        return False\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        for cache in self._caches:\n+            data = cache.get_cache_data(digest)\n+            if data is not None:\n+                return data\n+\n+        return None\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        for cache in self._caches:\n+            cache.store_in_cache(digest)\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        for cache in self._caches:\n+            cache.store_in_cache_with_data(digest, data)\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        for cache in self._caches:\n+            stats = cache.query_stats(options)\n+            if stats:\n+                return stats\n+\n+        return {}\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        for cache in self._caches:\n+            cache.clear_stats(options)\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyCacheWithStats(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, opts, cache, stats):\n+        self._log = log\n+        self._opts = opts\n+        self._cache = cache\n+        self._stats = stats\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest):\n+        res = self._cache.is_cached(digest)\n+        if self._stats:\n+            self._stats.update(digest, res)\n+        return res\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        res = self._cache.get_cache_data(digest)\n+        if self._stats:\n+            self._stats.update(digest, res is not None)\n+        return res\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        self._cache.store_in_cache(digest)\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        self._cache.store_in_cache_with_data(digest, data)\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        stats = self._cache.query_stats(options)\n+        if stats is None:\n+            stats = {}\n+\n+        if self._stats:\n+            hits, misses = self._stats.read()\n+            total = hits + misses\n+            stats[\"hit_count\"] = hits\n+            stats[\"miss_count\"] = misses\n+            stats[\"hit_rate\"] = hits/total if total else 0\n+            stats[\"miss_rate\"] = misses/total if total else 0\n+\n+        return stats\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        self._cache.clear_stats(options)\n+        if self._stats:\n+            self._stats.clear()\n+\n+# ------------------------------------------------------------------------------\n+class ClangTidyCache(object):\n+    # --------------------------------------------------------------------------\n+    def __init__(self, log, opts: ClangTidyCacheOpts):\n+        self._log = log\n+        self._opts = opts\n+        self._local = None\n+        self._remote = None\n+\n+        caches = []\n+\n+        if opts.has_host():\n+            caches.append(ClangTidyServerCache(log, opts))\n+\n+        if opts.has_redis_host() and redis:\n+            caches.append(ClangTidyRedisCache(log, opts))\n+\n+        if opts.has_s3():\n+            caches.append(ClangTidyS3Cache(log, opts))\n+\n+        if opts.has_gcs():\n+            caches.append(ClangTidyGcsCache(log, opts))\n+\n+        if not caches or opts.cache_locally():\n+            local = ClangTidyLocalCache(log, opts)\n+            self._local = self._wrap_with_stats(local, \"stats\")\n+\n+        if caches:\n+            remote = ClangTidyMultiCache(log, caches)\n+            self._remote = self._wrap_with_stats(remote, \"remote_stats\")\n+\n+    # --------------------------------------------------------------------------\n+    def _wrap_with_stats(self, cache, name):\n+        if not self._opts.no_local_stats():\n+            stats = ClangTidyCacheStats(self._log, self._opts, name)\n+            return ClangTidyCacheWithStats(self._log, self._opts, cache, stats)\n+        return cache\n+\n+    # --------------------------------------------------------------------------\n+    def is_cached(self, digest):\n+        if self._local:\n+            if self._local.is_cached(digest):\n+                return True\n+\n+        if self._remote:\n+            if self._remote.is_cached(digest):\n+                if self.should_writeback():\n+                    self._local.store_in_cache(digest)\n+                return True\n+\n+        return False\n+\n+    # --------------------------------------------------------------------------\n+    def get_cache_data(self, digest) -> tp.Optional[bytes]:\n+        if self._local:\n+            data = self._local.get_cache_data(digest)\n+            if data is not None:\n+                return data\n+\n+        if self._remote:\n+            data = self._remote.get_cache_data(digest)\n+            if data is not None:\n+                if self.should_writeback():\n+                    self._local.store_in_cache_with_data(digest, data)\n+                return data\n+\n+        return None\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache(self, digest):\n+        if self._local:\n+            self._local.store_in_cache(digest)\n+\n+        if self._remote:\n+            self._remote.store_in_cache(digest)\n+\n+    # --------------------------------------------------------------------------\n+    def store_in_cache_with_data(self, digest, data: bytes):\n+        if self._local:\n+            self._local.store_in_cache_with_data(digest, data)\n+\n+        if self._remote:\n+            self._remote.store_in_cache_with_data(digest, data)\n+\n+    # --------------------------------------------------------------------------\n+    def query_stats(self, options):\n+        stats = {}\n+\n+        if self._local:\n+            stats[\"local\"] = self._local.query_stats(options)\n+\n+        if self._remote:\n+            stats[\"remote\"] = self._remote.query_stats(options)\n+\n+        return stats\n+\n+    # --------------------------------------------------------------------------\n+    def clear_stats(self, options):\n+        if self._local:\n+            self._local.clear_stats(options)\n+\n+        if self._remote:\n+            self._remote.clear_stats(options)\n+\n+    # --------------------------------------------------------------------------\n+    def should_writeback(self):\n+        return self._local is not None and not self._opts.no_local_writeback()\n+\n+# ------------------------------------------------------------------------------\n+source_file_change_re = re.compile(r'#\\s+\\d+\\s+\"([^\"]+)\".*')\n+\n+def source_file_changed(cpp_line):\n+    found = source_file_change_re.match(cpp_line)\n+    if found:\n+        found_path = found.group(1)\n+        if os.path.isfile(found_path):\n+            return os.path.realpath(os.path.dirname(found_path))\n+\n+# ------------------------------------------------------------------------------\n+def remove_matching_line(byte_stream, pattern):\n+    text = byte_stream.decode(\"utf-8\")\n+    lines = text.split(\"\\n\")\n+    regex = re.compile(pattern)\n+    filtered_lines = [line for line in lines if not regex.search(line)]\n+    filtered_text = \"\\n\".join(filtered_lines)\n+    return filtered_text.encode(\"utf-8\")\n+\n+# ------------------------------------------------------------------------------\n+def hash_inputs(log, opts):\n+    ct_args = opts.clang_tidy_args()\n+    co_args = opts.compiler_args()\n+\n+    if not ct_args and not co_args:\n+        return None\n+\n+    def _is_src_ext(s):\n+        exts = [\".cppm\", \".cpp\", \".c\", \".cc\", \".h\", \".hpp\", \".cxx\"]\n+        return any(s.lower().endswith(ext) for ext in exts)\n+\n+    result = ClangTidyCacheHash(opts)\n+\n+    # --- Source file content (potentially pre-processed)\n+    source_file = \"unknown\"\n+    if len(co_args) == 0:\n+        for arg in ct_args[1:]:\n+            if os.path.exists(arg) and _is_src_ext(arg):\n+                source_file = arg\n+                with open(arg, \"rb\") as srcfd:\n+                    src_data_binary = srcfd.read()\n+                    if opts.strip_src():\n+                        src_data = src_data_binary.decode(encoding=\"utf-8\")\n+                        src_data = opts.strip_paths(src_data)\n+                        src_data_binary = src_data.encode(\"utf-8\")\n+                    result.update(src_data_binary)\n+    else:\n+        # Execute the compiler command defined by the compiler arguments. At this\n+        # point if we have compiler arguments with expect that it defines a valid\n+        # command to get the pre-processed output.\n+        # If we have a valid output this gets added to the hash.\n+        proc = subprocess.Popen(\n+            co_args,\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE\n+        )\n+        stdout, stderr = proc.communicate()\n+        if opts.running_on_msvc() or opts.running_on_clang_cl():\n+            if proc.returncode != 0:\n+                return None\n+        else:\n+            if stderr:\n+                log.error(f\"Error executing compile command: #{co_args}.\\n#{stderr}\")\n+                return None\n+\n+        if opts.strip_src():\n+            stdout_str = stdout.decode(encoding=\"utf-8\")\n+            stdout_str = opts.strip_paths(stdout_str)\n+            stdout = stdout_str.encode(\"utf-8\")\n+\n+        result.update(stdout)\n+\n+    # --- Config Contents ------------------------------------------------------\n+    # (as obtained by running clang-tidy with --dump-config flag)\n+\n+    ct_args_flags = [ ct_args[0] ]\n+    source_files = []\n+\n+    for arg in ct_args[1:]:\n+        if os.path.exists(arg) and _is_src_ext(arg):\n+            source_files.append(os.path.normpath(os.path.realpath(arg)))\n+        else:\n+            ct_args_flags.append(arg)\n+\n+    for source_file in sorted(source_files):\n+        ct_dump_cfg_source_file = ct_args_flags + [ \"--dump-config\",  source_file ]\n+        proc = subprocess.Popen(\n+            ct_dump_cfg_source_file,\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE\n+        )\n+        stdout, stderr = proc.communicate()\n+        if opts.exclude_user_config():\n+            stdout = remove_matching_line(stdout, \"User:.*$\")\n+            stdout = remove_matching_line(stdout, \"HeaderFilterRegex:.*$\")\n+\n+        if (proc.returncode == 0) and (len(stdout) > 0):\n+            result.update(stdout)\n+        else:\n+            msg = f\"Failed dumping the clang-tidy config with <{' '.join(ct_dump_cfg_source_file)}>\"\n+            raise RuntimeError(msg)\n+\n+    # --- Clang-Tidy and Compiler Args -----------------------------------------\n+\n+    def _omit_after(args, excl):\n+        omit_next = False\n+        for arg in args:\n+            omit_this = arg in excl\n+            if not omit_this and not omit_next:\n+                yield arg\n+            omit_next = omit_this\n+\n+    ct_args = list(_omit_after(ct_args, [\"-export-fixes\"]))\n+\n+    for chunk in sorted(set([opts.adjust_chunk(arg) for arg in ct_args[1:]])):\n+        if not opts.exclude_hash(chunk):\n+            result.update(chunk)\n+\n+    for chunk in sorted(set([opts.adjust_chunk(arg) for arg in co_args[1:]])):\n+        if not opts.exclude_hash(chunk):\n+            result.update(chunk)\n+\n+    print(f\"PMO: Hash for log {source_file}: {result.hexdigest()}\")\n+\n+    return result.hexdigest()\n+\n+# ------------------------------------------------------------------------------\n+def print_usage(log):\n+    print(\"Usage: clang-tidy-cache /path/to/real/clang-tidy [[cache-options] --] <clang-tidy-options>\")\n+# ------------------------------------------------------------------------------\n+def print_stats(log, opts, raw):\n+    def _format_bytes(s):\n+        if s < 10000:\n+            return \"%d B\" % (s)\n+        if s < 10000000:\n+            return \"%d kB\" % (s / 1000)\n+        return \"%d MB\" % (s / 1000000)\n+\n+    def _format_time(s):\n+        if s < 60:\n+            return \"%d seconds\" % (s)\n+        if s < 3600:\n+            return \"%d minutes %d seconds\" % (s / 60, s % 60)\n+        if s < 86400:\n+            return \"%d hours %d minutes\" % (s / 3600, (s / 60) % 60)\n+        if s < 604800:\n+            return \"%d days %d hours\" % (s / 86400, (s / 3600) % 24)\n+        if int(s / 86400) % 7 == 0:\n+            return \"%d weeks\" % (s / 604800)\n+        return \"%d weeks %d days\" % (s / 604800, (s / 86400) % 7)\n+\n+    cache = ClangTidyCache(log, opts)\n+    stats = cache.query_stats(opts)\n+\n+    if raw:\n+        print(json.dumps(stats))\n+        return\n+\n+    entries = [\n+        (\"Server host\", lambda o, s: o.rest_host()),\n+        (\"Server port\", lambda o, s: \"%d\" % o.rest_port()),\n+        (\"Long-term hit rate\", lambda o, s: \"%.1f %%\" % (s[\"remote\"][\"total_hit_rate\"] * 100.0)),\n+        (\"Hit rate\", lambda o, s: \"%.1f %%\" % (s[\"remote\"][\"hit_rate\"] * 100.0)),\n+        (\"Hit count\", lambda o, s: \"%d\" % s[\"remote\"][\"hit_count\"]),\n+        (\"Miss count\", lambda o, s: \"%d\" % s[\"remote\"][\"miss_count\"]),\n+        (\"Miss rate\", lambda o, s: \"%.1f %%\" % (s[\"remote\"][\"miss_rate\"] * 100.0)),\n+        (\"Max hash age\", lambda o, s: \"%d days\" % max(int(k) for k in s[\"remote\"][\"age_days_histogram\"])),\n+        (\"Max hash hits\", lambda o, s: \"%d\" % max(int(k) for k in s[\"remote\"][\"hit_count_histogram\"])),\n+        (\"Cache size\", lambda o, s: _format_bytes(s[\"remote\"][\"saved_size_bytes\"])),\n+        (\"Cached hashes\", lambda o, s: \"%d\" % s[\"remote\"][\"cached_count\"]),\n+        (\"Cleaned hashes\", lambda o, s: \"%d\" % s[\"remote\"][\"cleaned_count\"]),\n+        (\"Cleaned ago\", lambda o, s: _format_time(s[\"remote\"][\"cleaned_seconds_ago\"])),\n+        (\"Saved ago\", lambda o, s: _format_time(s[\"remote\"][\"saved_seconds_ago\"])),\n+        (\"Uptime\", lambda o, s: _format_time(s[\"remote\"][\"uptime_seconds\"])),\n+        (\"Hit rate (local)\", lambda o, s: \"%.1f %%\" % (s[\"local\"][\"hit_rate\"] * 100.0)),\n+        (\"Hit count (local)\", lambda o, s: \"%d\" % s[\"local\"][\"hit_count\"]),\n+        (\"Miss count (local)\", lambda o, s: \"%d\" % s[\"local\"][\"miss_count\"]),\n+        (\"Miss rate (local)\", lambda o, s: \"%.1f %%\" % (s[\"local\"][\"miss_rate\"] * 100.0)),\n+        (\"Cached hashes (local)\", lambda o, s: \"%d\" % s[\"local\"][\"cached_count\"])\n+    ]\n+\n+    max_len = max(len(e[0]) for e in entries)\n+    for label, fmtfunc in entries:\n+        padding = \" \" * (max_len-len(label))\n+        try:\n+            print(label+\":\", padding, fmtfunc(opts, stats))\n+        except:\n+            print(label+\":\", padding, \"N/A\")\n+\n+# ------------------------------------------------------------------------------\n+def clear_stats(log, opts):\n+    cache = ClangTidyCache(log, opts)\n+    cache.clear_stats(opts)\n+\n+# ------------------------------------------------------------------------------\n+def run_clang_tidy_cached(log, opts):\n+    cache = ClangTidyCache(log, opts)\n+    digest = None\n+    try:\n+        digest = hash_inputs(log, opts)\n+        if digest and opts.save_output():\n+            data = cache.get_cache_data(digest)\n+            if data is not None:\n+                returncode = int(data[0])\n+                sys.stdout.write(data[1:].decode(\"utf8\"))\n+                return returncode\n+        elif digest and cache.is_cached(digest):\n+            return 0\n+        else:\n+            pass\n+    except Exception as error:\n+        log.error(str(error))\n+\n+    proc = subprocess.Popen(\n+        opts.original_args(),\n+        stdout=subprocess.PIPE,\n+        stderr=subprocess.PIPE\n+    )\n+    stdout, stderr = proc.communicate()\n+    sys.stdout.write(stdout.decode(\"utf8\"))\n+    sys.stderr.write(stderr.decode(\"utf8\"))\n+\n+    tidy_success = True\n+    if proc.returncode != 0:\n+        tidy_success = False\n+\n+    if stdout and not opts.ignore_output():\n+        tidy_success = False\n+\n+    # saving the result even in case clang-tidy wasn't successful is only meaningful\n+    # if the output is actually stored. Only then the exit code can be retained\n+    # (as the first byte in the corresponding key's value)\n+    save_even_without_success = opts.save_all() and opts.save_output()\n+\n+    if (tidy_success or save_even_without_success) and digest:\n+        try:\n+            if opts.save_output():\n+                returncode_and_ct_output = bytes([proc.returncode]) + stdout\n+                cache.store_in_cache_with_data(digest, returncode_and_ct_output)\n+            else:\n+                cache.store_in_cache(digest)\n+        except Exception as error:\n+            log.error(str(error))\n+\n+    return proc.returncode\n+\n+# ------------------------------------------------------------------------------\n+def main():\n+    log = logging.getLogger(os.path.basename(__file__))\n+    log.setLevel(logging.WARNING)\n+    debug = False\n+    opts = None\n+    try:\n+        opts = ClangTidyCacheOpts(log, sys.argv[1:])\n+        debug = opts.debug_enabled()\n+        if opts.should_print_usage():\n+            print_usage(log)\n+        elif opts.should_print_dir():\n+            print(opts.cache_dir)\n+        elif opts.should_remove_dir():\n+            import shutil\n+            try:\n+                shutil.rmtree(opts.cache_dir)\n+            except FileNotFoundError:\n+                pass\n+        elif opts.should_print_stats():\n+            print_stats(log, opts, False)\n+        elif opts.should_print_stats_raw():\n+            print_stats(log, opts, True)\n+        elif opts.should_zero_stats():\n+            clear_stats(log, opts)\n+        else:\n+            return run_clang_tidy_cached(log, opts)\n+        return 0\n+    except Exception as error:\n+        if debug:\n+            log.error(\"Options: %s\" % (repr(opts),))\n+            raise\n+        else:\n+            log.error(\"%s: %s\" % (str(type(error)), repr(error)))\n+        return 1\n+\n+# ------------------------------------------------------------------------------\n+if __name__ == \"__main__\":\n+    sys.exit(main())\ndiff --git a/src/Common/IPv6ToBinary.cpp b/src/Common/IPv6ToBinary.cpp\nindex 8d335d893536..2d6f2421ba0b 100644\n--- a/src/Common/IPv6ToBinary.cpp\n+++ b/src/Common/IPv6ToBinary.cpp\n@@ -105,8 +105,7 @@ bool matchIPv6Subnet(const uint8_t * addr, const uint8_t * cidr_addr, UInt8 pref\n \n bool matchIPv6Subnet(const uint8_t * addr, const uint8_t * cidr_addr, UInt8 prefix)\n {\n-    if (prefix > IPV6_BINARY_LENGTH * 8U)\n-        prefix = IPV6_BINARY_LENGTH * 8U;\n+    prefix = std::min<size_t>(prefix, IPV6_BINARY_LENGTH * 8U);\n \n     size_t i = 0;\n     for (; prefix >= 8; ++i, prefix -= 8)\ndiff --git a/src/Interpreters/Aggregator.cpp b/src/Interpreters/Aggregator.cpp\nindex a417020c2d32..db3ecd7bac46 100644\n--- a/src/Interpreters/Aggregator.cpp\n+++ b/src/Interpreters/Aggregator.cpp\n@@ -2850,7 +2850,11 @@ void NO_INLINE Aggregator::mergeStreamsImplCase(\n     {\n         for (size_t i = row_begin; i < row_end; i++)\n         {\n-            auto emplace_result = state.emplaceKey(data, i, *arena_for_keys);\n+            /// clang-tidy complains wrongly about this one when running the analysis from an ARM host.\n+            /// The same thing does not fail when cross-compiling from a x86_64 host.\n+            /// Furthermore, arena_for_keys is set to be a pointer to the last member of aggregates_pools,\n+            /// which is always initialized to have at least 1 arena.\n+            auto emplace_result = state.emplaceKey(data, i, *arena_for_keys); /// NOLINT(clang-analyzer-core.NonNullParamChecker)\n             if (!emplace_result.isInserted())\n                 places[i] = emplace_result.getMapped();\n             else\n",
  "test_patch": "diff --git a/tests/ci/build_download_helper.py b/tests/ci/build_download_helper.py\nindex f9b4d0feab05..51b17a9c4e46 100644\n--- a/tests/ci/build_download_helper.py\n+++ b/tests/ci/build_download_helper.py\n@@ -132,7 +132,7 @@ def set_auth_header():\n     BuildNames.PACKAGE_DEBUG: \"artifact_report_build_amd_debug.json\",\n     BuildNames.PACKAGE_AARCH64: \"artifact_report_build_arm_release.json\",\n     BuildNames.PACKAGE_AARCH64_ASAN: \"artifact_report_build_arm_asan.json\",\n-    BuildNames.PACKAGE_RELEASE_COVERAGE: \"artifact_report_build_amd_coverage.json\",\n+    BuildNames.PACKAGE_RELEASE_COVERAGE: \"artifact_report_build_arm_coverage.json\",\n     BuildNames.BINARY_RELEASE: \"artifact_report_build_amd_binary.json\",\n     BuildNames.BINARY_TIDY: \"artifact_report_build_amd_tidy.json\",\n     BuildNames.BINARY_DARWIN: \"artifact_report_build_amd_darwin.json\",\ndiff --git a/tests/ci/ci_config.py b/tests/ci/ci_config.py\nindex 88a35cfa0bd7..c72939a186ca 100644\n--- a/tests/ci/ci_config.py\n+++ b/tests/ci/ci_config.py\n@@ -326,7 +326,7 @@ class CI:\n             required_builds=[BuildNames.PACKAGE_DEBUG], num_batches=1\n         ),\n         JobNames.STATELESS_TEST_AZURE_ASAN: CommonJobConfigs.STATELESS_TEST.with_properties(\n-            required_builds=[BuildNames.PACKAGE_ASAN], num_batches=3, release_only=True\n+            required_builds=[BuildNames.PACKAGE_AARCH64_ASAN], num_batches=3, release_only=True\n         ),\n         JobNames.STATELESS_TEST_S3_TSAN: CommonJobConfigs.STATELESS_TEST.with_properties(\n             required_builds=[BuildNames.PACKAGE_TSAN],\n@@ -339,7 +339,7 @@ class CI:\n             required_builds=[BuildNames.PACKAGE_TSAN],\n         ),\n         JobNames.STRESS_TEST_ASAN: CommonJobConfigs.STRESS_TEST.with_properties(\n-            required_builds=[BuildNames.PACKAGE_ASAN],\n+            required_builds=[BuildNames.PACKAGE_AARCH64_ASAN],\n             random_bucket=\"stress_with_sanitizer\",\n         ),\n         JobNames.STRESS_TEST_UBSAN: CommonJobConfigs.STRESS_TEST.with_properties(\n@@ -357,7 +357,7 @@ class CI:\n             required_builds=[BuildNames.PACKAGE_MSAN], release_only=True\n         ),\n         JobNames.UPGRADE_TEST_ASAN: CommonJobConfigs.UPGRADE_TEST.with_properties(\n-            required_builds=[BuildNames.PACKAGE_ASAN],\n+            required_builds=[BuildNames.PACKAGE_AARCH64_ASAN],\n             random_bucket=\"upgrade_with_sanitizer\",\n             pr_only=True,\n         ),\n@@ -434,7 +434,7 @@ class CI:\n             required_builds=[BuildNames.PACKAGE_DEBUG],\n         ),\n         JobNames.AST_FUZZER_TEST_ASAN: CommonJobConfigs.ASTFUZZER_TEST.with_properties(\n-            required_builds=[BuildNames.PACKAGE_ASAN],\n+            required_builds=[BuildNames.PACKAGE_AARCH64_ASAN],\n         ),\n         JobNames.AST_FUZZER_TEST_MSAN: CommonJobConfigs.ASTFUZZER_TEST.with_properties(\n             required_builds=[BuildNames.PACKAGE_MSAN],\n@@ -449,7 +449,7 @@ class CI:\n             required_builds=[BuildNames.PACKAGE_DEBUG],\n         ),\n         JobNames.BUZZHOUSE_TEST_ASAN: CommonJobConfigs.BUZZHOUSE_TEST.with_properties(\n-            required_builds=[BuildNames.PACKAGE_ASAN],\n+            required_builds=[BuildNames.PACKAGE_AARCH64_ASAN],\n         ),\n         JobNames.BUZZHOUSE_TEST_MSAN: CommonJobConfigs.BUZZHOUSE_TEST.with_properties(\n             required_builds=[BuildNames.PACKAGE_MSAN],\n@@ -616,6 +616,7 @@ def get_job_ci_stage(cls, job_name: str, non_blocking_ci: bool = False) -> str:\n     @classmethod\n     def get_job_config(cls, check_name: str) -> JobConfig:\n         # remove job batch if it exists in check name (hack for migration to praktika)\n+        check_name = check_name.replace(\"arm_\", \"\").replace(\"amd_\", \"\")  # hack for new names in praktika\n         check_name = re.sub(r\",\\s*\\d+/\\d+\\)\", \")\", check_name)\n         return cls.JOB_CONFIGS[check_name]\n \n@@ -746,7 +747,7 @@ def is_workflow_ok(cls) -> bool:\n     \"Build (amd_ubsan)\": BuildNames.PACKAGE_UBSAN,\n     \"Build (arm_release)\": BuildNames.PACKAGE_AARCH64,\n     \"Build (arm_asan)\": BuildNames.PACKAGE_AARCH64_ASAN,\n-    \"Build (amd_coverage)\": BuildNames.PACKAGE_RELEASE_COVERAGE,\n+    \"Build (arm_coverage)\": BuildNames.PACKAGE_RELEASE_COVERAGE,\n     \"Build (arm_binary)\": BuildNames.BINARY_AARCH64,\n     \"Build (amd_tidy)\": BuildNames.BINARY_TIDY,\n     \"Build (amd_darwin)\": BuildNames.BINARY_DARWIN,\n",
  "problem_statement": "clang-tidy finds issues on arm\n### Describe the bug\n\n```\n`/home/ubuntu/actions-runner/_work/ClickHouse/ClickHouse/src/Interpreters/Aggregator.cpp:2853:35: error: Forming reference to null pointer [clang-analyzer-core.NonNullParamChecker,-warnings-as-errors]\n 2853 |             auto emplace_result = state.emplaceKey(data, i, *arena_for_keys);`\n\n`/home/ubuntu/actions-runner/_work/ClickHouse/ClickHouse/src/Common/IPv6ToBinary.cpp:108:5: error: use `std::min` instead of `>` [readability-use-std-min-max,-warnings-as-errors]\n    7 |     if (prefix > IPV6_BINARY_LENGTH * 8U)\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      |     prefix = std::min<size_t>(prefix, IPV6_BINARY_LENGTH * 8U);\n    8 |         prefix = IPV6_BINARY_LENGTH * 8U;\n      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`\n```\n\n[full log](https://s3.amazonaws.com/clickhouse-test-reports/PRs/78492/7d895f2564a65d5780508c3034395dd45f8619a8//build_amd_tidy/job.log)\n\n### How to reproduce\n\nrun clang-tidy build on arm machine\n\n### Error message and/or stacktrace\n\n_No response_\n",
  "hints_text": "",
  "created_at": "2025-03-31T19:28:16Z",
  "modified_files": [
    ".github/workflows/backport_branches.yml",
    ".github/workflows/master.yml",
    ".github/workflows/pull_request.yml",
    ".github/workflows/release_branches.yml",
    "ci/defs/defs.py",
    "ci/defs/job_configs.py",
    "ci/jobs/build_clickhouse.py",
    "ci/workflows/pull_request.py",
    "b/clang_tidy_cache",
    "src/Common/IPv6ToBinary.cpp",
    "src/Interpreters/Aggregator.cpp"
  ],
  "modified_test_files": [
    "tests/ci/build_download_helper.py",
    "tests/ci/ci_config.py"
  ]
}