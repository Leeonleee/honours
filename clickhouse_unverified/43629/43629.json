{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 43629,
  "instance_id": "ClickHouse__ClickHouse-43629",
  "issue_numbers": [
    "43534"
  ],
  "base_commit": "6531ff765d0910f732b80a4a650f0644622306ad",
  "patch": "diff --git a/src/Storages/StorageS3Cluster.cpp b/src/Storages/StorageS3Cluster.cpp\nindex ec970654b6e4..b10f3c65ebff 100644\n--- a/src/Storages/StorageS3Cluster.cpp\n+++ b/src/Storages/StorageS3Cluster.cpp\n@@ -14,6 +14,8 @@\n #include <Interpreters/getHeaderForProcessingStage.h>\n #include <Interpreters/SelectQueryOptions.h>\n #include <Interpreters/InterpreterSelectQuery.h>\n+#include <Interpreters/AddDefaultDatabaseVisitor.h>\n+#include <Interpreters/TranslateQualifiedNamesVisitor.h>\n #include <Processors/Transforms/AddingDefaultsTransform.h>\n #include <QueryPipeline/narrowPipe.h>\n #include <QueryPipeline/Pipe.h>\n@@ -103,8 +105,7 @@ Pipe StorageS3Cluster::read(\n     auto callback = std::make_shared<StorageS3Source::IteratorWrapper>([iterator]() mutable -> String { return iterator->next(); });\n \n     /// Calculate the header. This is significant, because some columns could be thrown away in some cases like query with count(*)\n-    Block header =\n-        InterpreterSelectQuery(query_info.query, context, SelectQueryOptions(processed_stage).analyze()).getSampleBlock();\n+    auto interpreter = InterpreterSelectQuery(query_info.query, context, SelectQueryOptions(processed_stage).analyze());\n \n     const Scalars & scalars = context->hasQueryContext() ? context->getQueryContext()->getScalars() : Scalars{};\n \n@@ -112,11 +113,21 @@ Pipe StorageS3Cluster::read(\n \n     const bool add_agg_info = processed_stage == QueryProcessingStage::WithMergeableState;\n \n-    ASTPtr query_to_send = query_info.original_query->clone();\n+    ASTPtr query_to_send = interpreter.getQueryInfo().query->clone();\n     if (add_columns_structure_to_query)\n         addColumnsStructureToQueryWithClusterEngine(\n             query_to_send, StorageDictionary::generateNamesAndTypesDescription(storage_snapshot->metadata->getColumns().getAll()), 5, getName());\n \n+    RestoreQualifiedNamesVisitor::Data data;\n+    data.distributed_table = DatabaseAndTableWithAlias(*getTableExpression(query_info.query->as<ASTSelectQuery &>(), 0));\n+    data.remote_table.database = context->getCurrentDatabase();\n+    data.remote_table.table = getName();\n+    RestoreQualifiedNamesVisitor(data).visit(query_to_send);\n+    AddDefaultDatabaseVisitor visitor(context, context->getCurrentDatabase(),\n+        /* only_replace_current_database_function_= */false,\n+        /* only_replace_in_join_= */true);\n+    visitor.visit(query_to_send);\n+\n     const auto & current_settings = context->getSettingsRef();\n     auto timeouts = ConnectionTimeouts::getTCPTimeoutsWithFailover(current_settings);\n     for (const auto & shard_info : cluster->getShardsInfo())\n@@ -128,7 +139,7 @@ Pipe StorageS3Cluster::read(\n                     shard_info.pool,\n                     std::vector<IConnectionPool::Entry>{try_result},\n                     queryToString(query_to_send),\n-                    header,\n+                    interpreter.getSampleBlock(),\n                     context,\n                     /*throttler=*/nullptr,\n                     scalars,\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01801_s3_cluster_count.reference b/tests/queries/0_stateless/01801_s3_cluster_count.reference\nnew file mode 100644\nindex 000000000000..c094c553f81a\n--- /dev/null\n+++ b/tests/queries/0_stateless/01801_s3_cluster_count.reference\n@@ -0,0 +1,2 @@\n+12\n+12\ndiff --git a/tests/queries/0_stateless/01801_s3_cluster_count.sql b/tests/queries/0_stateless/01801_s3_cluster_count.sql\nnew file mode 100644\nindex 000000000000..8a4fb8049679\n--- /dev/null\n+++ b/tests/queries/0_stateless/01801_s3_cluster_count.sql\n@@ -0,0 +1,5 @@\n+-- Tags: no-fasttest\n+-- Tag no-fasttest: Depends on AWS\n+\n+select COUNT() from s3Cluster('test_cluster_two_shards_localhost', 'http://localhost:11111/test/{a,b,c}.tsv');\n+select COUNT(*) from s3Cluster('test_cluster_two_shards_localhost', 'http://localhost:11111/test/{a,b,c}.tsv');\n",
  "problem_statement": "s3Cluster function returns NOT_FOUND_COLUMN_IN_BLOCK error\n(you don't have to strictly follow this form)\r\n\r\n**Describe the unexpected behaviour**\r\nWe are seeing errors like \r\n```\r\nReceived exception from server (version 22.10.2):\r\nCode: 10. DB::Exception: Received from localhost:9000. DB::Exception: Not found column count() in block. There are only columns: COUNT(): While executing Remote. (NOT_FOUND_COLUMN_IN_BLOCK)\r\n```\r\n\r\nwhen trying to use **s3Cluster** function to query data using queries like: ```SELECT COUNT(*) FROM s3Cluster('cluster', 'https://...', ...)```\r\n\r\nHowever **s3** function works fine.\r\n \r\n* Which ClickHouse server version to use\r\n22.11 and 22.10 are seeing this issue. \r\n\r\n**Expected behavior**\r\nExpected s3Cluster function runs successfully.  \r\n\r\n**Error message and/or stacktrace**\r\nSee description. \r\n\r\n**Additional context**\r\nN/A\n",
  "hints_text": "Related discussion in slack: https://clickhousedb.slack.com/archives/CU478UEQZ/p1667549291289289",
  "created_at": "2022-11-24T14:02:01Z",
  "modified_files": [
    "src/Storages/StorageS3Cluster.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/01801_s3_cluster_count.reference",
    "b/tests/queries/0_stateless/01801_s3_cluster_count.sql"
  ]
}