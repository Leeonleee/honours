{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 37693,
  "instance_id": "ClickHouse__ClickHouse-37693",
  "issue_numbers": [
    "34437"
  ],
  "base_commit": "4d146b05a959e52c004df3ef5da986408d19adb4",
  "patch": "diff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex 60bf8d6a15c7..89c8aef8528c 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -340,7 +340,7 @@ struct PartRangesReadInfo\n             sum_marks_in_parts[i] = parts[i].getMarksCount();\n             sum_marks += sum_marks_in_parts[i];\n \n-            if (parts[i].data_part->index_granularity_info.is_adaptive)\n+            if (parts[i].data_part->index_granularity_info.mark_type.adaptive)\n                 ++adaptive_parts;\n         }\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex e9d900c6d54d..bafbb31f3a22 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -67,6 +67,7 @@ namespace ErrorCodes\n     extern const int NO_SUCH_COLUMN_IN_TABLE;\n }\n \n+\n void IMergeTreeDataPart::MinMaxIndex::load(const MergeTreeData & data, const PartMetadataManagerPtr & manager)\n {\n     auto metadata_snapshot = data.getInMemoryMetadataPtr();\n@@ -319,6 +320,7 @@ IMergeTreeDataPart::IMergeTreeDataPart(\n \n     minmax_idx = std::make_shared<MinMaxIndex>();\n \n+    initializeIndexGranularityInfo();\n     initializePartMetadataManager();\n }\n \n@@ -345,6 +347,7 @@ IMergeTreeDataPart::IMergeTreeDataPart(\n \n     minmax_idx = std::make_shared<MinMaxIndex>();\n \n+    initializeIndexGranularityInfo();\n     initializePartMetadataManager();\n }\n \n@@ -738,7 +741,7 @@ void IMergeTreeDataPart::loadIndex()\n             loaded_index[i]->reserve(index_granularity.getMarksCount());\n         }\n \n-        String index_name = \"primary.idx\";\n+        String index_name = \"primary\" + getIndexExtensionFromFilesystem(data_part_storage).value();\n         String index_path = fs::path(data_part_storage->getRelativePath()) / index_name;\n         auto index_file = metadata_manager->read(index_name);\n         size_t marks_count = index_granularity.getMarksCount();\n@@ -777,7 +780,10 @@ void IMergeTreeDataPart::appendFilesOfIndex(Strings & files) const\n         return;\n \n     if (metadata_snapshot->hasPrimaryKey())\n-        files.push_back(\"primary.idx\");\n+    {\n+        String index_name = \"primary\" + getIndexExtensionFromFilesystem(data_part_storage).value();\n+        files.push_back(index_name);\n+    }\n }\n \n NameSet IMergeTreeDataPart::getFileNamesWithoutChecksums() const\n@@ -1428,6 +1434,15 @@ void IMergeTreeDataPart::initializePartMetadataManager()\n #endif\n }\n \n+void IMergeTreeDataPart::initializeIndexGranularityInfo()\n+{\n+    auto mrk_ext = MergeTreeIndexGranularityInfo::getMarksExtensionFromFilesystem(data_part_storage);\n+    if (mrk_ext)\n+        index_granularity_info = MergeTreeIndexGranularityInfo(storage, MarkType{*mrk_ext});\n+    else\n+        index_granularity_info = MergeTreeIndexGranularityInfo(storage, part_type);\n+}\n+\n void IMergeTreeDataPart::remove() const\n {\n     assert(assertHasValidVersionMetadata());\n@@ -1531,8 +1546,11 @@ void IMergeTreeDataPart::checkConsistencyBase() const\n     const auto & partition_key = metadata_snapshot->getPartitionKey();\n     if (!checksums.empty())\n     {\n-        if (!pk.column_names.empty() && !checksums.files.contains(\"primary.idx\"))\n-            throw Exception(\"No checksum for primary.idx\", ErrorCodes::NO_FILE_IN_DATA_PART);\n+        if (!pk.column_names.empty()\n+            && (!checksums.files.contains(\"primary\" + getIndexExtension(false))\n+                && !checksums.files.contains(\"primary\" + getIndexExtension(true))))\n+            throw Exception(\"No checksum for \" + toString(\"primary\" + getIndexExtension(false)) + \" or \" + toString(\"primary\" + getIndexExtension(true)),\n+                            ErrorCodes::NO_FILE_IN_DATA_PART);\n \n         if (storage.format_version >= MERGE_TREE_DATA_MIN_FORMAT_VERSION_WITH_CUSTOM_PARTITIONING)\n         {\n@@ -1570,7 +1588,10 @@ void IMergeTreeDataPart::checkConsistencyBase() const\n \n         /// Check that the primary key index is not empty.\n         if (!pk.column_names.empty())\n-            check_file_not_empty(\"primary.idx\");\n+        {\n+            String index_name = \"primary\" + getIndexExtensionFromFilesystem(data_part_storage).value();\n+            check_file_not_empty(index_name);\n+        }\n \n         if (storage.format_version >= MERGE_TREE_DATA_MIN_FORMAT_VERSION_WITH_CUSTOM_PARTITIONING)\n         {\n@@ -1623,7 +1644,7 @@ void IMergeTreeDataPart::calculateSecondaryIndicesSizesOnDisk()\n         auto index_name_escaped = escapeForFileName(index_name);\n \n         auto index_file_name = index_name_escaped + index_ptr->getSerializedFileExtension();\n-        auto index_marks_file_name = index_name_escaped + index_granularity_info.marks_file_extension;\n+        auto index_marks_file_name = index_name_escaped + getMarksFileExtension();\n \n         /// If part does not contain index\n         auto bin_checksum = checksums.files.find(index_file_name);\n@@ -1783,4 +1804,24 @@ bool isInMemoryPart(const MergeTreeDataPartPtr & data_part)\n     return (data_part && data_part->getType() == MergeTreeDataPartType::InMemory);\n }\n \n+std::optional<std::string> getIndexExtensionFromFilesystem(const DataPartStoragePtr & data_part_storage)\n+{\n+    if (data_part_storage->exists())\n+    {\n+        for (auto it = data_part_storage->iterate(); it->isValid(); it->next())\n+        {\n+            const auto & extension = fs::path(it->name()).extension();\n+            if (extension == getIndexExtension(false)\n+                    || extension == getIndexExtension(true))\n+                return extension;\n+        }\n+    }\n+    return {\".idx\"};\n+}\n+\n+bool isCompressedFromIndexExtension(const String & index_extension)\n+{\n+    return index_extension == getIndexExtension(true);\n+}\n+\n }\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex 1e23886fb21f..32afa2a482db 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -23,6 +23,7 @@\n \n #include <shared_mutex>\n \n+\n namespace zkutil\n {\n     class ZooKeeper;\n@@ -158,7 +159,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     void loadColumnsChecksumsIndexes(bool require_columns_checksums, bool check_consistency);\n     void appendFilesOfColumnsChecksumsIndexes(Strings & files, bool include_projection = false) const;\n \n-    String getMarksFileExtension() const { return index_granularity_info.marks_file_extension; }\n+    String getMarksFileExtension() const { return index_granularity_info.mark_type.getFileExtension(); }\n \n     /// Generate the new name for this part according to `new_part_info` and min/max dates from the old name.\n     /// This is useful when you want to change e.g. block numbers or the mutation version of the part.\n@@ -500,6 +501,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     void initializePartMetadataManager();\n \n+    void initializeIndexGranularityInfo();\n \n private:\n     /// In compact parts order of columns is necessary\n@@ -582,5 +584,8 @@ using MergeTreeMutableDataPartPtr = std::shared_ptr<IMergeTreeDataPart>;\n bool isCompactPart(const MergeTreeDataPartPtr & data_part);\n bool isWidePart(const MergeTreeDataPartPtr & data_part);\n bool isInMemoryPart(const MergeTreeDataPartPtr & data_part);\n+inline String getIndexExtension(bool is_compressed_primary_key) { return is_compressed_primary_key ? \".cidx\" : \".idx\"; }\n+std::optional<String> getIndexExtensionFromFilesystem(const DataPartStoragePtr & data_part_storage);\n+bool isCompressedFromIndexExtension(const String & index_extension);\n \n }\ndiff --git a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\nindex 9be49a9bba4d..5af9bbd3ed85 100644\n--- a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n@@ -106,6 +106,7 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n         if (remove_it != columns.end())\n             columns.erase(remove_it);\n     }\n+\n     return remove_files;\n }\n \ndiff --git a/src/Storages/MergeTree/IPartMetadataManager.cpp b/src/Storages/MergeTree/IPartMetadataManager.cpp\nindex d09fc9d42440..03fa3e3309ee 100644\n--- a/src/Storages/MergeTree/IPartMetadataManager.cpp\n+++ b/src/Storages/MergeTree/IPartMetadataManager.cpp\n@@ -3,9 +3,19 @@\n #include <Disks/IVolume.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n \n+\n namespace DB\n {\n+\n IPartMetadataManager::IPartMetadataManager(const IMergeTreeDataPart * part_) : part(part_)\n {\n }\n+\n+bool IPartMetadataManager::isCompressedFromFileName(const String & file_name)\n+{\n+    std::string extension = fs::path(file_name).extension();\n+    return (MarkType::isMarkFileExtension(extension) && MarkType(extension).compressed)\n+        || isCompressedFromIndexExtension(extension);\n+}\n+\n }\ndiff --git a/src/Storages/MergeTree/IPartMetadataManager.h b/src/Storages/MergeTree/IPartMetadataManager.h\nindex c1bf3b15805c..d9e97d91518b 100644\n--- a/src/Storages/MergeTree/IPartMetadataManager.h\n+++ b/src/Storages/MergeTree/IPartMetadataManager.h\n@@ -9,6 +9,7 @@ namespace DB\n \n class IMergeTreeDataPart;\n \n+class ReadBuffer;\n class SeekableReadBuffer;\n \n class IDisk;\n@@ -29,8 +30,8 @@ class IPartMetadataManager\n \n     virtual ~IPartMetadataManager() = default;\n \n-    /// Read metadata content and return SeekableReadBuffer object.\n-    virtual std::unique_ptr<SeekableReadBuffer> read(const String & file_name) const = 0;\n+    /// Read metadata content and return ReadBuffer object.\n+    virtual std::unique_ptr<ReadBuffer> read(const String & file_name) const = 0;\n \n     /// Return true if metadata exists in part.\n     virtual bool exists(const String & file_name) const = 0;\n@@ -50,6 +51,9 @@ class IPartMetadataManager\n     /// Check all metadatas in part.\n     virtual std::unordered_map<String, uint128> check() const = 0;\n \n+    /// Determine whether to compress by file extension\n+    static bool isCompressedFromFileName(const String & file_name);\n+\n protected:\n     const IMergeTreeDataPart * part;\n };\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 7c508568fe81..73da9d362b84 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -155,6 +155,7 @@ namespace ErrorCodes\n     extern const int CANNOT_RESTORE_TABLE;\n }\n \n+\n static void checkSampleExpression(const StorageInMemoryMetadata & metadata, bool allow_sampling_expression_not_in_primary_key, bool check_sample_column_is_correct)\n {\n     if (metadata.sampling_key.column_names.empty())\n@@ -1086,7 +1087,7 @@ void MergeTreeData::loadDataPartsFromDisk(\n                 suspicious_broken_parts_bytes += *size_of_part;\n             return;\n         }\n-        if (!part->index_granularity_info.is_adaptive)\n+        if (!part->index_granularity_info.mark_type.adaptive)\n             has_non_adaptive_parts.store(true, std::memory_order_relaxed);\n         else\n             has_adaptive_parts.store(true, std::memory_order_relaxed);\n@@ -2699,18 +2700,6 @@ MergeTreeData::MutableDataPartPtr MergeTreeData::createPart(const String & name,\n         throw Exception(\"Unknown type of part \" + data_part_storage->getRelativePath(), ErrorCodes::UNKNOWN_PART_TYPE);\n }\n \n-static MergeTreeDataPartType getPartTypeFromMarkExtension(const String & mrk_ext)\n-{\n-    if (mrk_ext == getNonAdaptiveMrkExtension())\n-        return MergeTreeDataPartType::Wide;\n-    if (mrk_ext == getAdaptiveMrkExtension(MergeTreeDataPartType::Wide))\n-        return MergeTreeDataPartType::Wide;\n-    if (mrk_ext == getAdaptiveMrkExtension(MergeTreeDataPartType::Compact))\n-        return MergeTreeDataPartType::Compact;\n-\n-    throw Exception(\"Can't determine part type, because of unknown mark extension \" + mrk_ext, ErrorCodes::UNKNOWN_PART_TYPE);\n-}\n-\n MergeTreeData::MutableDataPartPtr MergeTreeData::createPart(\n     const String & name, const DataPartStoragePtr & data_part_storage, const IMergeTreeDataPart * parent_part) const\n {\n@@ -2725,7 +2714,9 @@ MergeTreeData::MutableDataPartPtr MergeTreeData::createPart(\n     auto mrk_ext = MergeTreeIndexGranularityInfo::getMarksExtensionFromFilesystem(data_part_storage);\n \n     if (mrk_ext)\n-        type = getPartTypeFromMarkExtension(*mrk_ext);\n+    {\n+        type = MarkType(*mrk_ext).part_type;\n+    }\n     else\n     {\n         /// Didn't find any mark file, suppose that part is empty.\n@@ -6435,9 +6426,9 @@ bool MergeTreeData::canReplacePartition(const DataPartPtr & src_part) const\n \n     if (!settings->enable_mixed_granularity_parts || settings->index_granularity_bytes == 0)\n     {\n-        if (!canUseAdaptiveGranularity() && src_part->index_granularity_info.is_adaptive)\n+        if (!canUseAdaptiveGranularity() && src_part->index_granularity_info.mark_type.adaptive)\n             return false;\n-        if (canUseAdaptiveGranularity() && !src_part->index_granularity_info.is_adaptive)\n+        if (canUseAdaptiveGranularity() && !src_part->index_granularity_info.mark_type.adaptive)\n             return false;\n     }\n     return true;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\nindex 4a21ae1592a5..5c5fc0cd8f4a 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\n@@ -75,7 +75,7 @@ IMergeTreeDataPart::MergeTreeWriterPtr MergeTreeDataPartCompact::getWriter(\n \n     return std::make_unique<MergeTreeDataPartWriterCompact>(\n         shared_from_this(), std::move(data_part_storage_builder), ordered_columns_list, metadata_snapshot,\n-        indices_to_recalc, index_granularity_info.marks_file_extension,\n+        indices_to_recalc, getMarksFileExtension(),\n         default_codec_, writer_settings, computed_index_granularity);\n }\n \n@@ -89,7 +89,7 @@ void MergeTreeDataPartCompact::calculateEachColumnSizes(ColumnSizeByName & /*eac\n         total_size.data_uncompressed += bin_checksum->second.uncompressed_size;\n     }\n \n-    auto mrk_checksum = checksums.files.find(DATA_FILE_NAME + index_granularity_info.marks_file_extension);\n+    auto mrk_checksum = checksums.files.find(DATA_FILE_NAME + getMarksFileExtension());\n     if (mrk_checksum != checksums.files.end())\n         total_size.marks += mrk_checksum->second.file_size;\n }\n@@ -98,7 +98,7 @@ void MergeTreeDataPartCompact::loadIndexGranularityImpl(\n     MergeTreeIndexGranularity & index_granularity_, const MergeTreeIndexGranularityInfo & index_granularity_info_,\n     size_t columns_count, const DataPartStoragePtr & data_part_storage_)\n {\n-    if (!index_granularity_info_.is_adaptive)\n+    if (!index_granularity_info_.mark_type.adaptive)\n         throw Exception(\"MergeTreeDataPartCompact cannot be created with non-adaptive granulary.\", ErrorCodes::NOT_IMPLEMENTED);\n \n     auto marks_file_path = index_granularity_info_.getMarksFilePath(\"data\");\n@@ -140,7 +140,7 @@ bool MergeTreeDataPartCompact::hasColumnFiles(const NameAndTypePair & column) co\n         return false;\n \n     auto bin_checksum = checksums.files.find(DATA_FILE_NAME_WITH_EXTENSION);\n-    auto mrk_checksum = checksums.files.find(DATA_FILE_NAME + index_granularity_info.marks_file_extension);\n+    auto mrk_checksum = checksums.files.find(DATA_FILE_NAME + getMarksFileExtension());\n \n     return (bin_checksum != checksums.files.end() && mrk_checksum != checksums.files.end());\n }\n@@ -148,7 +148,7 @@ bool MergeTreeDataPartCompact::hasColumnFiles(const NameAndTypePair & column) co\n void MergeTreeDataPartCompact::checkConsistency(bool require_part_metadata) const\n {\n     checkConsistencyBase();\n-    String mrk_file_name = DATA_FILE_NAME + index_granularity_info.marks_file_extension;\n+    String mrk_file_name = DATA_FILE_NAME + getMarksFileExtension();\n \n     if (!checksums.empty())\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\nindex 58a0e48caab5..170d1b1d7038 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n@@ -12,7 +12,6 @@ namespace DB\n \n namespace ErrorCodes\n {\n-    extern const int CANNOT_READ_ALL_DATA;\n     extern const int NO_FILE_IN_DATA_PART;\n     extern const int BAD_SIZE_OF_FILE_IN_DATA_PART;\n     extern const int LOGICAL_ERROR;\n@@ -68,7 +67,7 @@ IMergeTreeDataPart::MergeTreeWriterPtr MergeTreeDataPartWide::getWriter(\n     return std::make_unique<MergeTreeDataPartWriterWide>(\n         shared_from_this(), data_part_storage_builder,\n         columns_list, metadata_snapshot, indices_to_recalc,\n-        index_granularity_info.marks_file_extension,\n+        getMarksFileExtension(),\n         default_codec_, writer_settings, computed_index_granularity);\n }\n \n@@ -96,7 +95,7 @@ ColumnSize MergeTreeDataPartWide::getColumnSizeImpl(\n             size.data_uncompressed += bin_checksum->second.uncompressed_size;\n         }\n \n-        auto mrk_checksum = checksums.files.find(file_name + index_granularity_info.marks_file_extension);\n+        auto mrk_checksum = checksums.files.find(file_name + getMarksFileExtension());\n         if (mrk_checksum != checksums.files.end())\n             size.marks += mrk_checksum->second.file_size;\n     });\n@@ -119,26 +118,41 @@ void MergeTreeDataPartWide::loadIndexGranularityImpl(\n \n     size_t marks_file_size = data_part_storage_->getFileSize(marks_file_path);\n \n-    if (!index_granularity_info_.is_adaptive)\n+    if (!index_granularity_info_.mark_type.adaptive && !index_granularity_info_.mark_type.compressed)\n     {\n+        /// The most easy way - no need to read the file, everything is known from its size.\n         size_t marks_count = marks_file_size / index_granularity_info_.getMarkSizeInBytes();\n         index_granularity_.resizeWithFixedGranularity(marks_count, index_granularity_info_.fixed_index_granularity); /// all the same\n     }\n     else\n     {\n-        auto buffer = data_part_storage_->readFile(marks_file_path, ReadSettings().adjustBufferSize(marks_file_size), marks_file_size, std::nullopt);\n-        while (!buffer->eof())\n+        auto marks_file = data_part_storage_->readFile(marks_file_path, ReadSettings().adjustBufferSize(marks_file_size), marks_file_size, std::nullopt);\n+\n+        std::unique_ptr<ReadBuffer> marks_reader;\n+        if (!index_granularity_info_.mark_type.compressed)\n+            marks_reader = std::move(marks_file);\n+        else\n+            marks_reader = std::make_unique<CompressedReadBufferFromFile>(std::move(marks_file));\n+\n+        size_t marks_count = 0;\n+        while (!marks_reader->eof())\n         {\n-            buffer->seek(sizeof(size_t) * 2, SEEK_CUR); /// skip offset_in_compressed file and offset_in_decompressed_block\n+            MarkInCompressedFile mark;\n             size_t granularity;\n-            readIntBinary(granularity, *buffer);\n-            index_granularity_.appendMark(granularity);\n+\n+            readBinary(mark.offset_in_compressed_file, *marks_reader);\n+            readBinary(mark.offset_in_decompressed_block, *marks_reader);\n+            ++marks_count;\n+\n+            if (index_granularity_info_.mark_type.adaptive)\n+            {\n+                readIntBinary(granularity, *marks_reader);\n+                index_granularity_.appendMark(granularity);\n+            }\n         }\n \n-        if (index_granularity_.getMarksCount() * index_granularity_info_.getMarkSizeInBytes() != marks_file_size)\n-            throw Exception(\n-                ErrorCodes::CANNOT_READ_ALL_DATA, \"Cannot read all marks from file {}\",\n-                std::string(fs::path(data_part_storage_->getFullPath()) / marks_file_path));\n+        if (!index_granularity_info_.mark_type.adaptive)\n+            index_granularity_.resizeWithFixedGranularity(marks_count, index_granularity_info_.fixed_index_granularity); /// all the same\n     }\n \n     index_granularity_.setInitialized();\n@@ -152,6 +166,7 @@ void MergeTreeDataPartWide::loadIndexGranularity()\n     loadIndexGranularityImpl(index_granularity, index_granularity_info, data_part_storage, getFileNameForColumn(columns.front()));\n }\n \n+\n bool MergeTreeDataPartWide::isStoredOnRemoteDisk() const\n {\n     return data_part_storage->isStoredOnRemoteDisk();\n@@ -170,7 +185,7 @@ MergeTreeDataPartWide::~MergeTreeDataPartWide()\n void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n {\n     checkConsistencyBase();\n-    //String path = getRelativePath();\n+    std::string marks_file_extension = index_granularity_info.mark_type.getFileExtension();\n \n     if (!checksums.empty())\n     {\n@@ -181,7 +196,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n                 getSerialization(name_type.name)->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n                 {\n                     String file_name = ISerialization::getFileNameForStream(name_type, substream_path);\n-                    String mrk_file_name = file_name + index_granularity_info.marks_file_extension;\n+                    String mrk_file_name = file_name + marks_file_extension;\n                     String bin_file_name = file_name + DATA_FILE_EXTENSION;\n \n                     if (!checksums.files.contains(mrk_file_name))\n@@ -207,7 +222,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n         {\n             getSerialization(name_type.name)->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n             {\n-                auto file_path = ISerialization::getFileNameForStream(name_type, substream_path) + index_granularity_info.marks_file_extension;\n+                auto file_path = ISerialization::getFileNameForStream(name_type, substream_path) + marks_file_extension;\n \n                 /// Missing file is Ok for case when new column was added.\n                 if (data_part_storage->exists(file_path))\n@@ -235,10 +250,11 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n \n bool MergeTreeDataPartWide::hasColumnFiles(const NameAndTypePair & column) const\n {\n-    auto check_stream_exists = [this](const String & stream_name)\n+    std::string marks_file_extension = index_granularity_info.mark_type.getFileExtension();\n+    auto check_stream_exists = [this, &marks_file_extension](const String & stream_name)\n     {\n         auto bin_checksum = checksums.files.find(stream_name + DATA_FILE_EXTENSION);\n-        auto mrk_checksum = checksums.files.find(stream_name + index_granularity_info.marks_file_extension);\n+        auto mrk_checksum = checksums.files.find(stream_name + marks_file_extension);\n \n         return bin_checksum != checksums.files.end() && mrk_checksum != checksums.files.end();\n     };\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex 44fe50815daf..457aad550233 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -27,12 +27,24 @@ MergeTreeDataPartWriterCompact::MergeTreeDataPartWriterCompact(\n             settings.max_compress_block_size,\n             settings_.query_write_settings))\n     , plain_hashing(*plain_file)\n-    , marks_file(data_part_storage_builder->writeFile(\n+{\n+    marks_file = data_part_storage_builder->writeFile(\n             MergeTreeDataPartCompact::DATA_FILE_NAME + marks_file_extension_,\n             4096,\n-            settings_.query_write_settings))\n-    , marks(*marks_file)\n-{\n+            settings_.query_write_settings);\n+\n+    marks_file_hashing = std::make_unique<HashingWriteBuffer>(*marks_file);\n+\n+    if (data_part_->index_granularity_info.mark_type.compressed)\n+    {\n+        marks_compressor = std::make_unique<CompressedWriteBuffer>(\n+            *marks_file_hashing,\n+            settings_.getMarksCompressionCodec(),\n+            settings_.marks_compress_block_size);\n+\n+        marks_source_hashing = std::make_unique<HashingWriteBuffer>(*marks_compressor);\n+    }\n+\n     const auto & storage_columns = metadata_snapshot->getColumns();\n     for (const auto & column : columns_list)\n         addStreams(column, storage_columns.getCodecDescOrDefault(column.name, default_codec));\n@@ -172,6 +184,8 @@ void MergeTreeDataPartWriterCompact::writeDataBlockPrimaryIndexAndSkipIndices(co\n \n void MergeTreeDataPartWriterCompact::writeDataBlock(const Block & block, const Granules & granules)\n {\n+    WriteBuffer & marks_out = marks_source_hashing ? *marks_source_hashing : *marks_file_hashing;\n+\n     for (const auto & granule : granules)\n     {\n         data_written = true;\n@@ -203,8 +217,8 @@ void MergeTreeDataPartWriterCompact::writeDataBlock(const Block & block, const G\n             };\n \n \n-            writeIntBinary(plain_hashing.count(), marks);\n-            writeIntBinary(static_cast<UInt64>(0), marks);\n+            writeIntBinary(plain_hashing.count(), marks_out);\n+            writeIntBinary(static_cast<UInt64>(0), marks_out);\n \n             writeColumnSingleGranule(\n                 block.getByName(name_and_type->name), data_part->getSerialization(name_and_type->name),\n@@ -214,7 +228,7 @@ void MergeTreeDataPartWriterCompact::writeDataBlock(const Block & block, const G\n             prev_stream->hashing_buf.next(); //-V522\n         }\n \n-        writeIntBinary(granule.rows_to_write, marks);\n+        writeIntBinary(granule.rows_to_write, marks_out);\n     }\n }\n \n@@ -239,18 +253,26 @@ void MergeTreeDataPartWriterCompact::fillDataChecksums(IMergeTreeDataPart::Check\n         assert(stream->hashing_buf.offset() == 0);\n #endif\n \n+    WriteBuffer & marks_out = marks_source_hashing ? *marks_source_hashing : *marks_file_hashing;\n+\n     if (with_final_mark && data_written)\n     {\n         for (size_t i = 0; i < columns_list.size(); ++i)\n         {\n-            writeIntBinary(plain_hashing.count(), marks);\n-            writeIntBinary(static_cast<UInt64>(0), marks);\n+            writeIntBinary(plain_hashing.count(), marks_out);\n+            writeIntBinary(static_cast<UInt64>(0), marks_out);\n         }\n-        writeIntBinary(static_cast<UInt64>(0), marks);\n+        writeIntBinary(static_cast<UInt64>(0), marks_out);\n     }\n \n     plain_file->next();\n-    marks.next();\n+\n+    if (marks_source_hashing)\n+        marks_source_hashing->next();\n+    if (marks_compressor)\n+        marks_compressor->next();\n+\n+    marks_file_hashing->next();\n     addToChecksums(checksums);\n \n     plain_file->preFinalize();\n@@ -261,6 +283,7 @@ void MergeTreeDataPartWriterCompact::finishDataSerialization(bool sync)\n {\n     plain_file->finalize();\n     marks_file->finalize();\n+\n     if (sync)\n     {\n         plain_file->sync();\n@@ -332,8 +355,15 @@ void MergeTreeDataPartWriterCompact::addToChecksums(MergeTreeDataPartChecksums &\n     checksums.files[data_file_name].file_size = plain_hashing.count();\n     checksums.files[data_file_name].file_hash = plain_hashing.getHash();\n \n-    checksums.files[marks_file_name].file_size = marks.count();\n-    checksums.files[marks_file_name].file_hash = marks.getHash();\n+    if (marks_compressor)\n+    {\n+        checksums.files[marks_file_name].is_compressed = true;\n+        checksums.files[marks_file_name].uncompressed_size = marks_source_hashing->count();\n+        checksums.files[marks_file_name].uncompressed_hash = marks_source_hashing->getHash();\n+    }\n+\n+    checksums.files[marks_file_name].file_size = marks_file_hashing->count();\n+    checksums.files[marks_file_name].file_hash = marks_file_hashing->getHash();\n }\n \n void MergeTreeDataPartWriterCompact::ColumnsBuffer::add(MutableColumns && columns)\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\nindex dd098b125cd7..7b68f61925fe 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n@@ -1,6 +1,8 @@\n #pragma once\n+\n #include <Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h>\n \n+\n namespace DB\n {\n \n@@ -84,9 +86,16 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n     /// Stream for each column's substreams path (look at addStreams).\n     std::unordered_map<String, CompressedStreamPtr> compressed_streams;\n \n-    /// marks -> marks_file\n+    /// If marks are uncompressed, the data is written to 'marks_file_hashing' for hash calculation and then to the 'marks_file'.\n     std::unique_ptr<WriteBufferFromFileBase> marks_file;\n-    HashingWriteBuffer marks;\n+    std::unique_ptr<HashingWriteBuffer> marks_file_hashing;\n+\n+    /// If marks are compressed, the data is written to 'marks_source_hashing' for hash calculation,\n+    /// then to 'marks_compressor' for compression,\n+    /// then to 'marks_file_hashing' for calculation of hash of compressed data,\n+    /// then finally to 'marks_file'.\n+    std::unique_ptr<CompressedWriteBuffer> marks_compressor;\n+    std::unique_ptr<HashingWriteBuffer> marks_source_hashing;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\nindex 56ebadc082cc..1d2b095330eb 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n@@ -13,10 +13,17 @@ namespace ErrorCodes\n \n void MergeTreeDataPartWriterOnDisk::Stream::preFinalize()\n {\n-    compressed.next();\n-    /// 'compressed_buf' doesn't call next() on underlying buffer ('plain_hashing'). We should do it manually.\n+    compressed_hashing.next();\n+    compressor.next();\n     plain_hashing.next();\n-    marks.next();\n+\n+    if (compress_marks)\n+    {\n+        marks_compressed_hashing.next();\n+        marks_compressor.next();\n+    }\n+\n+    marks_hashing.next();\n \n     plain_file->preFinalize();\n     marks_file->preFinalize();\n@@ -48,15 +55,21 @@ MergeTreeDataPartWriterOnDisk::Stream::Stream(\n     const std::string & marks_file_extension_,\n     const CompressionCodecPtr & compression_codec_,\n     size_t max_compress_block_size_,\n+    const CompressionCodecPtr & marks_compression_codec_,\n+    size_t marks_compress_block_size_,\n     const WriteSettings & query_write_settings) :\n     escaped_column_name(escaped_column_name_),\n     data_file_extension{data_file_extension_},\n     marks_file_extension{marks_file_extension_},\n     plain_file(data_part_storage_builder->writeFile(data_path_ + data_file_extension, max_compress_block_size_, query_write_settings)),\n     plain_hashing(*plain_file),\n-    compressed_buf(plain_hashing, compression_codec_, max_compress_block_size_),\n-    compressed(compressed_buf),\n-    marks_file(data_part_storage_builder->writeFile(marks_path_ + marks_file_extension, 4096, query_write_settings)), marks(*marks_file)\n+    compressor(plain_hashing, compression_codec_, max_compress_block_size_),\n+    compressed_hashing(compressor),\n+    marks_file(data_part_storage_builder->writeFile(marks_path_ + marks_file_extension, 4096, query_write_settings)),\n+    marks_hashing(*marks_file),\n+    marks_compressor(marks_hashing, marks_compression_codec_, marks_compress_block_size_),\n+    marks_compressed_hashing(marks_compressor),\n+    compress_marks(MarkType(marks_file_extension).compressed)\n {\n }\n \n@@ -65,13 +78,20 @@ void MergeTreeDataPartWriterOnDisk::Stream::addToChecksums(MergeTreeData::DataPa\n     String name = escaped_column_name;\n \n     checksums.files[name + data_file_extension].is_compressed = true;\n-    checksums.files[name + data_file_extension].uncompressed_size = compressed.count();\n-    checksums.files[name + data_file_extension].uncompressed_hash = compressed.getHash();\n+    checksums.files[name + data_file_extension].uncompressed_size = compressed_hashing.count();\n+    checksums.files[name + data_file_extension].uncompressed_hash = compressed_hashing.getHash();\n     checksums.files[name + data_file_extension].file_size = plain_hashing.count();\n     checksums.files[name + data_file_extension].file_hash = plain_hashing.getHash();\n \n-    checksums.files[name + marks_file_extension].file_size = marks.count();\n-    checksums.files[name + marks_file_extension].file_hash = marks.getHash();\n+    if (compress_marks)\n+    {\n+        checksums.files[name + marks_file_extension].is_compressed = true;\n+        checksums.files[name + marks_file_extension].uncompressed_size = marks_compressed_hashing.count();\n+        checksums.files[name + marks_file_extension].uncompressed_hash = marks_compressed_hashing.getHash();\n+    }\n+\n+    checksums.files[name + marks_file_extension].file_size = marks_hashing.count();\n+    checksums.files[name + marks_file_extension].file_hash = marks_hashing.getHash();\n }\n \n \n@@ -91,6 +111,7 @@ MergeTreeDataPartWriterOnDisk::MergeTreeDataPartWriterOnDisk(\n     , marks_file_extension(marks_file_extension_)\n     , default_codec(default_codec_)\n     , compute_granularity(index_granularity.empty())\n+    , compress_primary_key(settings.compress_primary_key)\n {\n     if (settings.blocks_are_granules_size && !index_granularity.empty())\n         throw Exception(\"Can't take information about index granularity from blocks, when non empty index_granularity array specified\", ErrorCodes::LOGICAL_ERROR);\n@@ -156,13 +177,27 @@ void MergeTreeDataPartWriterOnDisk::initPrimaryIndex()\n {\n     if (metadata_snapshot->hasPrimaryKey())\n     {\n-        index_file_stream = data_part_storage_builder->writeFile(\"primary.idx\", DBMS_DEFAULT_BUFFER_SIZE, settings.query_write_settings);\n-        index_stream = std::make_unique<HashingWriteBuffer>(*index_file_stream);\n+        String index_name = \"primary\" + getIndexExtension(compress_primary_key);\n+        index_file_stream = data_part_storage_builder->writeFile(index_name, DBMS_DEFAULT_BUFFER_SIZE, settings.query_write_settings);\n+        index_file_hashing_stream = std::make_unique<HashingWriteBuffer>(*index_file_stream);\n+\n+        if (compress_primary_key)\n+        {\n+            ParserCodec codec_parser;\n+            auto ast = parseQuery(codec_parser, \"(\" + Poco::toUpper(settings.primary_key_compression_codec) + \")\", 0, DBMS_DEFAULT_MAX_PARSER_DEPTH);\n+            CompressionCodecPtr primary_key_compression_codec = CompressionCodecFactory::instance().get(ast, nullptr);\n+            index_compressor_stream = std::make_unique<CompressedWriteBuffer>(*index_file_hashing_stream, primary_key_compression_codec, settings.primary_key_compress_block_size);\n+            index_source_hashing_stream = std::make_unique<HashingWriteBuffer>(*index_compressor_stream);\n+        }\n     }\n }\n \n void MergeTreeDataPartWriterOnDisk::initSkipIndices()\n {\n+    ParserCodec codec_parser;\n+    auto ast = parseQuery(codec_parser, \"(\" + Poco::toUpper(settings.marks_compression_codec) + \")\", 0, DBMS_DEFAULT_MAX_PARSER_DEPTH);\n+    CompressionCodecPtr marks_compression_codec = CompressionCodecFactory::instance().get(ast, nullptr);\n+\n     for (const auto & index_helper : skip_indices)\n     {\n         String stream_name = index_helper->getFileName();\n@@ -172,7 +207,9 @@ void MergeTreeDataPartWriterOnDisk::initSkipIndices()\n                         data_part_storage_builder,\n                         stream_name, index_helper->getSerializedFileExtension(),\n                         stream_name, marks_file_extension,\n-                        default_codec, settings.max_compress_block_size, settings.query_write_settings));\n+                        default_codec, settings.max_compress_block_size,\n+                        marks_compression_codec, settings.marks_compress_block_size,\n+                        settings.query_write_settings));\n         skip_indices_aggregators.push_back(index_helper->createIndexAggregator());\n         skip_index_accumulated_marks.push_back(0);\n     }\n@@ -208,7 +245,8 @@ void MergeTreeDataPartWriterOnDisk::calculateAndSerializePrimaryIndex(const Bloc\n                 {\n                     const auto & primary_column = primary_index_block.getByPosition(j);\n                     index_columns[j]->insertFrom(*primary_column.column, granule.start_row);\n-                    primary_column.type->getDefaultSerialization()->serializeBinary(*primary_column.column, granule.start_row, *index_stream);\n+                    primary_column.type->getDefaultSerialization()->serializeBinary(\n+                        *primary_column.column, granule.start_row, compress_primary_key ? *index_source_hashing_stream : *index_file_hashing_stream);\n                 }\n             }\n         }\n@@ -226,11 +264,13 @@ void MergeTreeDataPartWriterOnDisk::calculateAndSerializeSkipIndices(const Block\n     {\n         const auto index_helper = skip_indices[i];\n         auto & stream = *skip_indices_streams[i];\n+        WriteBuffer & marks_out = stream.compress_marks ? stream.marks_compressed_hashing : stream.marks_hashing;\n+\n         for (const auto & granule : granules_to_write)\n         {\n             if (skip_index_accumulated_marks[i] == index_helper->index.granularity)\n             {\n-                skip_indices_aggregators[i]->getGranuleAndReset()->serializeBinary(stream.compressed);\n+                skip_indices_aggregators[i]->getGranuleAndReset()->serializeBinary(stream.compressed_hashing);\n                 skip_index_accumulated_marks[i] = 0;\n             }\n \n@@ -238,15 +278,16 @@ void MergeTreeDataPartWriterOnDisk::calculateAndSerializeSkipIndices(const Block\n             {\n                 skip_indices_aggregators[i] = index_helper->createIndexAggregator();\n \n-                if (stream.compressed.offset() >= settings.min_compress_block_size)\n-                    stream.compressed.next();\n+                if (stream.compressed_hashing.offset() >= settings.min_compress_block_size)\n+                    stream.compressed_hashing.next();\n+\n+                writeIntBinary(stream.plain_hashing.count(), marks_out);\n+                writeIntBinary(stream.compressed_hashing.offset(), marks_out);\n \n-                writeIntBinary(stream.plain_hashing.count(), stream.marks);\n-                writeIntBinary(stream.compressed.offset(), stream.marks);\n                 /// Actually this numbers is redundant, but we have to store them\n-                /// to be compatible with normal .mrk2 file format\n+                /// to be compatible with the normal .mrk2 file format\n                 if (settings.can_use_adaptive_granularity)\n-                    writeIntBinary(1UL, stream.marks);\n+                    writeIntBinary(1UL, marks_out);\n             }\n \n             size_t pos = granule.start_row;\n@@ -263,7 +304,7 @@ void MergeTreeDataPartWriterOnDisk::fillPrimaryIndexChecksums(MergeTreeData::Dat\n     if (write_final_mark && compute_granularity)\n         index_granularity.appendMark(0);\n \n-    if (index_stream)\n+    if (index_file_hashing_stream)\n     {\n         if (write_final_mark)\n         {\n@@ -272,26 +313,44 @@ void MergeTreeDataPartWriterOnDisk::fillPrimaryIndexChecksums(MergeTreeData::Dat\n                 const auto & column = *last_block_index_columns[j];\n                 size_t last_row_number = column.size() - 1;\n                 index_columns[j]->insertFrom(column, last_row_number);\n-                index_types[j]->getDefaultSerialization()->serializeBinary(column, last_row_number, *index_stream);\n+                index_types[j]->getDefaultSerialization()->serializeBinary(\n+                    column, last_row_number, compress_primary_key ? *index_source_hashing_stream : *index_file_hashing_stream);\n             }\n             last_block_index_columns.clear();\n         }\n \n-        index_stream->next();\n-        checksums.files[\"primary.idx\"].file_size = index_stream->count();\n-        checksums.files[\"primary.idx\"].file_hash = index_stream->getHash();\n+        if (compress_primary_key)\n+            index_source_hashing_stream->next();\n+\n+        index_file_hashing_stream->next();\n+\n+        String index_name = \"primary\" + getIndexExtension(compress_primary_key);\n+        if (compress_primary_key)\n+        {\n+            checksums.files[index_name].is_compressed = true;\n+            checksums.files[index_name].uncompressed_size = index_source_hashing_stream->count();\n+            checksums.files[index_name].uncompressed_hash = index_source_hashing_stream->getHash();\n+        }\n+        checksums.files[index_name].file_size = index_file_hashing_stream->count();\n+        checksums.files[index_name].file_hash = index_file_hashing_stream->getHash();\n         index_file_stream->preFinalize();\n     }\n }\n \n void MergeTreeDataPartWriterOnDisk::finishPrimaryIndexSerialization(bool sync)\n {\n-    if (index_stream)\n+    if (index_file_hashing_stream)\n     {\n         index_file_stream->finalize();\n         if (sync)\n             index_file_stream->sync();\n-        index_stream = nullptr;\n+\n+        if (compress_primary_key)\n+        {\n+            index_source_hashing_stream = nullptr;\n+            index_compressor_stream = nullptr;\n+        }\n+        index_file_hashing_stream = nullptr;\n     }\n }\n \n@@ -301,7 +360,7 @@ void MergeTreeDataPartWriterOnDisk::fillSkipIndicesChecksums(MergeTreeData::Data\n     {\n         auto & stream = *skip_indices_streams[i];\n         if (!skip_indices_aggregators[i]->empty())\n-            skip_indices_aggregators[i]->getGranuleAndReset()->serializeBinary(stream.compressed);\n+            skip_indices_aggregators[i]->getGranuleAndReset()->serializeBinary(stream.compressed_hashing);\n     }\n \n     for (auto & stream : skip_indices_streams)\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\nindex 7cc53db80661..4b58224de78b 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\n@@ -8,7 +8,8 @@\n #include <Storages/MergeTree/MergeTreeData.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n #include <Disks/IDisk.h>\n-\n+#include <Parsers/ExpressionElementParsers.h>\n+#include <Parsers/parseQuery.h>\n \n namespace DB\n {\n@@ -56,21 +57,26 @@ class MergeTreeDataPartWriterOnDisk : public IMergeTreeDataPartWriter\n             const std::string & marks_file_extension_,\n             const CompressionCodecPtr & compression_codec_,\n             size_t max_compress_block_size_,\n+            const CompressionCodecPtr & marks_compression_codec_,\n+            size_t marks_compress_block_size_,\n             const WriteSettings & query_write_settings);\n \n         String escaped_column_name;\n         std::string data_file_extension;\n         std::string marks_file_extension;\n \n-        /// compressed -> compressed_buf -> plain_hashing -> plain_file\n+        /// compressed_hashing -> compressor -> plain_hashing -> plain_file\n         std::unique_ptr<WriteBufferFromFileBase> plain_file;\n         HashingWriteBuffer plain_hashing;\n-        CompressedWriteBuffer compressed_buf;\n-        HashingWriteBuffer compressed;\n+        CompressedWriteBuffer compressor;\n+        HashingWriteBuffer compressed_hashing;\n \n-        /// marks -> marks_file\n+        /// marks_compressed_hashing -> marks_compressor -> marks_hashing -> marks_file\n         std::unique_ptr<WriteBufferFromFileBase> marks_file;\n-        HashingWriteBuffer marks;\n+        HashingWriteBuffer marks_hashing;\n+        CompressedWriteBuffer marks_compressor;\n+        HashingWriteBuffer marks_compressed_hashing;\n+        bool compress_marks;\n \n         bool is_prefinalized = false;\n \n@@ -139,7 +145,11 @@ class MergeTreeDataPartWriterOnDisk : public IMergeTreeDataPartWriter\n     std::vector<size_t> skip_index_accumulated_marks;\n \n     std::unique_ptr<WriteBufferFromFileBase> index_file_stream;\n-    std::unique_ptr<HashingWriteBuffer> index_stream;\n+    std::unique_ptr<HashingWriteBuffer> index_file_hashing_stream;\n+    std::unique_ptr<CompressedWriteBuffer> index_compressor_stream;\n+    std::unique_ptr<HashingWriteBuffer> index_source_hashing_stream;\n+    bool compress_primary_key;\n+\n     DataTypes index_types;\n     /// Index columns from the last block\n     /// It's written to index file in the `writeSuffixAndFinalizePart` method\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex 99bf188f03c1..70654f521a14 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -30,6 +30,7 @@ Granules getGranulesToWrite(const MergeTreeIndexGranularity & index_granularity,\n \n     Granules result;\n     size_t current_row = 0;\n+\n     /// When our last mark is not finished yet and we have to write rows into it\n     if (rows_written_in_last_mark > 0)\n     {\n@@ -43,7 +44,7 @@ Granules getGranulesToWrite(const MergeTreeIndexGranularity & index_granularity,\n             .is_complete = (rows_left_in_block >= rows_left_in_last_mark),\n         });\n         current_row += result.back().rows_to_write;\n-        current_mark++;\n+        ++current_mark;\n     }\n \n     /// Calculating normal granules for block\n@@ -61,7 +62,7 @@ Granules getGranulesToWrite(const MergeTreeIndexGranularity & index_granularity,\n             .is_complete = (rows_left_in_block >= expected_rows_in_mark),\n         });\n         current_row += result.back().rows_to_write;\n-        current_mark++;\n+        ++current_mark;\n     }\n \n     return result;\n@@ -110,6 +111,10 @@ void MergeTreeDataPartWriterWide::addStreams(\n         else /// otherwise return only generic codecs and don't use info about the` data_type\n             compression_codec = CompressionCodecFactory::instance().get(effective_codec_desc, nullptr, default_codec, true);\n \n+        ParserCodec codec_parser;\n+        auto ast = parseQuery(codec_parser, \"(\" + Poco::toUpper(settings.marks_compression_codec) + \")\", 0, DBMS_DEFAULT_MAX_PARSER_DEPTH);\n+        CompressionCodecPtr marks_compression_codec = CompressionCodecFactory::instance().get(ast, nullptr);\n+\n         column_streams[stream_name] = std::make_unique<Stream>(\n             stream_name,\n             data_part_storage_builder,\n@@ -117,6 +122,8 @@ void MergeTreeDataPartWriterWide::addStreams(\n             stream_name, marks_file_extension,\n             compression_codec,\n             settings.max_compress_block_size,\n+            marks_compression_codec,\n+            settings.marks_compress_block_size,\n             settings.query_write_settings);\n     };\n \n@@ -138,7 +145,7 @@ ISerialization::OutputStreamGetter MergeTreeDataPartWriterWide::createStreamGett\n         if (is_offsets && offset_columns.contains(stream_name))\n             return nullptr;\n \n-        return &column_streams.at(stream_name)->compressed;\n+        return &column_streams.at(stream_name)->compressed_hashing;\n     };\n }\n \n@@ -265,10 +272,12 @@ void MergeTreeDataPartWriterWide::writeSingleMark(\n void MergeTreeDataPartWriterWide::flushMarkToFile(const StreamNameAndMark & stream_with_mark, size_t rows_in_mark)\n {\n     Stream & stream = *column_streams[stream_with_mark.stream_name];\n-    writeIntBinary(stream_with_mark.mark.offset_in_compressed_file, stream.marks);\n-    writeIntBinary(stream_with_mark.mark.offset_in_decompressed_block, stream.marks);\n+    WriteBuffer & marks_out = stream.compress_marks ? stream.marks_compressed_hashing : stream.marks_hashing;\n+\n+    writeIntBinary(stream_with_mark.mark.offset_in_compressed_file, marks_out);\n+    writeIntBinary(stream_with_mark.mark.offset_in_decompressed_block, marks_out);\n     if (settings.can_use_adaptive_granularity)\n-        writeIntBinary(rows_in_mark, stream.marks);\n+        writeIntBinary(rows_in_mark, marks_out);\n }\n \n StreamsWithMarks MergeTreeDataPartWriterWide::getCurrentMarksForColumn(\n@@ -289,13 +298,13 @@ StreamsWithMarks MergeTreeDataPartWriterWide::getCurrentMarksForColumn(\n         Stream & stream = *column_streams[stream_name];\n \n         /// There could already be enough data to compress into the new block.\n-        if (stream.compressed.offset() >= settings.min_compress_block_size)\n-            stream.compressed.next();\n+        if (stream.compressed_hashing.offset() >= settings.min_compress_block_size)\n+            stream.compressed_hashing.next();\n \n         StreamNameAndMark stream_with_mark;\n         stream_with_mark.stream_name = stream_name;\n         stream_with_mark.mark.offset_in_compressed_file = stream.plain_hashing.count();\n-        stream_with_mark.mark.offset_in_decompressed_block = stream.compressed.offset();\n+        stream_with_mark.mark.offset_in_decompressed_block = stream.compressed_hashing.offset();\n \n         result.push_back(stream_with_mark);\n     });\n@@ -325,7 +334,7 @@ void MergeTreeDataPartWriterWide::writeSingleGranule(\n         if (is_offsets && offset_columns.contains(stream_name))\n             return;\n \n-        column_streams[stream_name]->compressed.nextIfAtEnd();\n+        column_streams[stream_name]->compressed_hashing.nextIfAtEnd();\n     });\n }\n \n@@ -418,7 +427,13 @@ void MergeTreeDataPartWriterWide::validateColumnOfFixedSize(const NameAndTypePai\n     if (!data_part_storage->exists(mrk_path))\n         return;\n \n-    auto mrk_in = data_part_storage->readFile(mrk_path, {}, std::nullopt, std::nullopt);\n+    auto mrk_file_in = data_part_storage->readFile(mrk_path, {}, std::nullopt, std::nullopt);\n+    std::unique_ptr<ReadBuffer> mrk_in;\n+    if (data_part->index_granularity_info.mark_type.compressed)\n+        mrk_in = std::make_unique<CompressedReadBufferFromFile>(std::move(mrk_file_in));\n+    else\n+        mrk_in = std::move(mrk_file_in);\n+\n     DB::CompressedReadBufferFromFile bin_in(data_part_storage->readFile(bin_path, {}, std::nullopt, std::nullopt));\n     bool must_be_last = false;\n     UInt64 offset_in_compressed_file = 0;\ndiff --git a/src/Storages/MergeTree/MergeTreeIOSettings.h b/src/Storages/MergeTree/MergeTreeIOSettings.h\nindex 55848e09434c..d5d2c68b190c 100644\n--- a/src/Storages/MergeTree/MergeTreeIOSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeIOSettings.h\n@@ -3,6 +3,8 @@\n #include <Core/Settings.h>\n #include <Storages/MergeTree/MergeTreeSettings.h>\n #include <IO/WriteSettings.h>\n+#include <Parsers/ExpressionElementParsers.h>\n+#include <Parsers/parseQuery.h>\n \n \n namespace DB\n@@ -43,6 +45,11 @@ struct MergeTreeWriterSettings\n         , max_compress_block_size(\n               storage_settings->max_compress_block_size ? storage_settings->max_compress_block_size\n                                                         : global_settings.max_compress_block_size)\n+        , marks_compression_codec(storage_settings->marks_compression_codec)\n+        , marks_compress_block_size(storage_settings->marks_compress_block_size)\n+        , compress_primary_key(storage_settings->compress_primary_key)\n+        , primary_key_compression_codec(storage_settings->primary_key_compression_codec)\n+        , primary_key_compress_block_size(storage_settings->primary_key_compress_block_size)\n         , can_use_adaptive_granularity(can_use_adaptive_granularity_)\n         , rewrite_primary_key(rewrite_primary_key_)\n         , blocks_are_granules_size(blocks_are_granules_size_)\n@@ -50,8 +57,23 @@ struct MergeTreeWriterSettings\n     {\n     }\n \n+    CompressionCodecPtr getMarksCompressionCodec() const\n+    {\n+        ParserCodec codec_parser;\n+        auto ast = parseQuery(codec_parser, \"(\" + Poco::toUpper(marks_compression_codec) + \")\", 0, DBMS_DEFAULT_MAX_PARSER_DEPTH);\n+        return CompressionCodecFactory::instance().get(ast, nullptr);\n+    }\n+\n     size_t min_compress_block_size;\n     size_t max_compress_block_size;\n+\n+    String marks_compression_codec;\n+    size_t marks_compress_block_size;\n+\n+    bool compress_primary_key;\n+    String primary_key_compression_codec;\n+    size_t primary_key_compress_block_size;\n+\n     bool can_use_adaptive_granularity;\n     bool rewrite_primary_key;\n     bool blocks_are_granules_size;\ndiff --git a/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.cpp b/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.cpp\nindex 6ae58dc45847..9c154f786f70 100644\n--- a/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.cpp\n+++ b/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.cpp\n@@ -9,71 +9,124 @@ namespace DB\n \n namespace ErrorCodes\n {\n-    extern const int NOT_IMPLEMENTED;\n+    extern const int LOGICAL_ERROR;\n     extern const int UNKNOWN_PART_TYPE;\n+    extern const int INCORRECT_FILE_NAME;\n }\n \n-std::optional<std::string> MergeTreeIndexGranularityInfo::getMarksExtensionFromFilesystem(const DataPartStoragePtr & data_part_storage)\n+\n+MarkType::MarkType(std::string_view extension)\n {\n-    if (data_part_storage->exists())\n+    if (extension.starts_with('.'))\n+        extension = extension.substr(1);\n+\n+    if (extension.starts_with('c'))\n     {\n-        for (auto it = data_part_storage->iterate(); it->isValid(); it->next())\n-        {\n-            const auto & ext = fs::path(it->name()).extension();\n-            if (ext == getNonAdaptiveMrkExtension()\n-                || ext == getAdaptiveMrkExtension(MergeTreeDataPartType::Wide)\n-                || ext == getAdaptiveMrkExtension(MergeTreeDataPartType::Compact))\n-                return ext;\n-        }\n+        compressed = true;\n+        extension = extension.substr(1);\n     }\n-    return {};\n+\n+    if (!extension.starts_with(\"mrk\"))\n+        throw Exception(ErrorCodes::INCORRECT_FILE_NAME, \"Mark file extension does not start with .mrk or .cmrk: {}\", extension);\n+\n+    extension = extension.substr(strlen(\"mrk\"));\n+\n+    if (extension.empty())\n+    {\n+        adaptive = false;\n+        part_type = MergeTreeDataPartType::Wide;\n+    }\n+    else if (extension == \"2\")\n+    {\n+        adaptive = true;\n+        part_type = MergeTreeDataPartType::Wide;\n+    }\n+    else if (extension == \"3\")\n+    {\n+        adaptive = true;\n+        part_type = MergeTreeDataPartType::Compact;\n+    }\n+    else\n+        throw Exception(ErrorCodes::INCORRECT_FILE_NAME, \"Unknown mark file extension: '{}'\", extension);\n }\n \n-MergeTreeIndexGranularityInfo::MergeTreeIndexGranularityInfo(const MergeTreeData & storage, MergeTreeDataPartType type_)\n-    : type(type_)\n+MarkType::MarkType(bool adaptive_, bool compressed_, MergeTreeDataPartType::Value part_type_)\n+    : adaptive(adaptive_), compressed(compressed_), part_type(part_type_)\n+{\n+    if (!adaptive && part_type != MergeTreeDataPartType::Wide)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Logical error: non-Wide data part type with non-adaptive granularity\");\n+    if (part_type == MergeTreeDataPartType::Unknown)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Logical error: unknown data part type\");\n+}\n+\n+bool MarkType::isMarkFileExtension(std::string_view extension)\n+{\n+    return extension.find(\"mrk\") != std::string_view::npos;\n+}\n+\n+std::string MarkType::getFileExtension() const\n {\n-    const auto storage_settings = storage.getSettings();\n-    fixed_index_granularity = storage_settings->index_granularity;\n+    std::string res = compressed ? \".cmrk\" : \".mrk\";\n \n-    /// Granularity is fixed\n-    if (!storage.canUseAdaptiveGranularity())\n+    if (!adaptive)\n     {\n-        if (type != MergeTreeDataPartType::Wide)\n-            throw Exception(\"Only Wide parts can be used with non-adaptive granularity.\", ErrorCodes::NOT_IMPLEMENTED);\n-        setNonAdaptive();\n+        if (part_type != MergeTreeDataPartType::Wide)\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Logical error: non-Wide data part type with non-adaptive granularity\");\n+        return res;\n+    }\n+\n+    switch (part_type)\n+    {\n+        case MergeTreeDataPartType::Wide:\n+            return res + \"2\";\n+        case MergeTreeDataPartType::Compact:\n+            return res + \"3\";\n+        case MergeTreeDataPartType::InMemory:\n+            return \"\";\n+        case MergeTreeDataPartType::Unknown:\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Logical error: unknown data part type\");\n     }\n-    else\n-        setAdaptive(storage_settings->index_granularity_bytes);\n }\n \n-void MergeTreeIndexGranularityInfo::changeGranularityIfRequired(const DataPartStoragePtr & data_part_storage)\n+\n+std::optional<std::string> MergeTreeIndexGranularityInfo::getMarksExtensionFromFilesystem(const DataPartStoragePtr & data_part_storage)\n {\n-    auto mrk_ext = getMarksExtensionFromFilesystem(data_part_storage);\n-    if (mrk_ext && *mrk_ext == getNonAdaptiveMrkExtension())\n-        setNonAdaptive();\n+    if (data_part_storage->exists())\n+        for (auto it = data_part_storage->iterate(); it->isValid(); it->next())\n+            if (it->isFile())\n+                if (std::string ext = fs::path(it->name()).extension(); MarkType::isMarkFileExtension(ext))\n+                    return ext;\n+    return {};\n }\n \n-void MergeTreeIndexGranularityInfo::setAdaptive(size_t index_granularity_bytes_)\n+MergeTreeIndexGranularityInfo::MergeTreeIndexGranularityInfo(const MergeTreeData & storage, MergeTreeDataPartType type_)\n+    : MergeTreeIndexGranularityInfo(storage, {storage.canUseAdaptiveGranularity(), storage.getSettings()->compress_marks, type_.getValue()})\n {\n-    is_adaptive = true;\n-    marks_file_extension = getAdaptiveMrkExtension(type);\n-    index_granularity_bytes = index_granularity_bytes_;\n }\n \n-void MergeTreeIndexGranularityInfo::setNonAdaptive()\n+MergeTreeIndexGranularityInfo::MergeTreeIndexGranularityInfo(const MergeTreeData & storage, MarkType mark_type_)\n+    : mark_type(mark_type_)\n {\n-    is_adaptive = false;\n-    marks_file_extension = getNonAdaptiveMrkExtension();\n-    index_granularity_bytes = 0;\n+    fixed_index_granularity = storage.getSettings()->index_granularity;\n+}\n+\n+void MergeTreeIndexGranularityInfo::changeGranularityIfRequired(const DataPartStoragePtr & data_part_storage)\n+{\n+    auto mrk_ext = getMarksExtensionFromFilesystem(data_part_storage);\n+    if (mrk_ext && !MarkType(*mrk_ext).adaptive)\n+    {\n+        mark_type.adaptive = false;\n+        index_granularity_bytes = 0;\n+    }\n }\n \n size_t MergeTreeIndexGranularityInfo::getMarkSizeInBytes(size_t columns_num) const\n {\n-    if (type == MergeTreeDataPartType::Wide)\n-        return is_adaptive ? getAdaptiveMrkSizeWide() : getNonAdaptiveMrkSizeWide();\n-    else if (type == MergeTreeDataPartType::Compact)\n+    if (mark_type.part_type == MergeTreeDataPartType::Wide)\n+        return mark_type.adaptive ? getAdaptiveMrkSizeWide() : getNonAdaptiveMrkSizeWide();\n+    else if (mark_type.part_type == MergeTreeDataPartType::Compact)\n         return getAdaptiveMrkSizeCompact(columns_num);\n-    else if (type == MergeTreeDataPartType::InMemory)\n+    else if (mark_type.part_type == MergeTreeDataPartType::InMemory)\n         return 0;\n     else\n         throw Exception(\"Unknown part type\", ErrorCodes::UNKNOWN_PART_TYPE);\n@@ -85,16 +138,4 @@ size_t getAdaptiveMrkSizeCompact(size_t columns_num)\n     return sizeof(UInt64) * (columns_num * 2 + 1);\n }\n \n-std::string getAdaptiveMrkExtension(MergeTreeDataPartType part_type)\n-{\n-    if (part_type == MergeTreeDataPartType::Wide)\n-        return \".mrk2\";\n-    else if (part_type == MergeTreeDataPartType::Compact)\n-        return \".mrk3\";\n-    else if (part_type == MergeTreeDataPartType::InMemory)\n-        return \"\";\n-    else\n-        throw Exception(\"Unknown part type\", ErrorCodes::UNKNOWN_PART_TYPE);\n-}\n-\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.h b/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.h\nindex a5adc919f4fe..883fe3c899e0 100644\n--- a/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.h\n+++ b/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.h\n@@ -11,15 +11,30 @@ namespace DB\n \n class MergeTreeData;\n \n+\n+/** Various types of mark files are stored in files with various extensions:\n+  * .mrk, .mrk2, .mrk3, .cmrk, .cmrk2, .cmrk3.\n+  * This helper allows to obtain mark type from file extension and vice versa.\n+  */\n+struct MarkType\n+{\n+    MarkType(std::string_view extension);\n+    MarkType(bool adaptive_, bool compressed_, MergeTreeDataPartType::Value part_type_);\n+\n+    static bool isMarkFileExtension(std::string_view extension);\n+    std::string getFileExtension() const;\n+\n+    bool adaptive = false;\n+    bool compressed = false;\n+    MergeTreeDataPartType::Value part_type = MergeTreeDataPartType::Unknown;\n+};\n+\n+\n /// Meta information about index granularity\n struct MergeTreeIndexGranularityInfo\n {\n public:\n-    /// Marks file extension '.mrk' or '.mrk2'\n-    String marks_file_extension;\n-\n-    /// Is stride in rows between marks non fixed?\n-    bool is_adaptive = false;\n+    MarkType mark_type;\n \n     /// Fixed size in rows of one granule if index_granularity_bytes is zero\n     size_t fixed_index_granularity = 0;\n@@ -29,29 +44,24 @@ struct MergeTreeIndexGranularityInfo\n \n     MergeTreeIndexGranularityInfo(const MergeTreeData & storage, MergeTreeDataPartType type_);\n \n+    MergeTreeIndexGranularityInfo(const MergeTreeData & storage, MarkType mark_type_);\n+\n     MergeTreeIndexGranularityInfo(MergeTreeDataPartType type_, bool is_adaptive_, size_t index_granularity_, size_t index_granularity_bytes_);\n \n     void changeGranularityIfRequired(const DataPartStoragePtr & data_part_storage);\n \n     String getMarksFilePath(const String & path_prefix) const\n     {\n-        return path_prefix + marks_file_extension;\n+        return path_prefix + mark_type.getFileExtension();\n     }\n \n     size_t getMarkSizeInBytes(size_t columns_num = 1) const;\n \n     static std::optional<std::string> getMarksExtensionFromFilesystem(const DataPartStoragePtr & data_part_storage);\n-\n-private:\n-    MergeTreeDataPartType type;\n-    void setAdaptive(size_t index_granularity_bytes_);\n-    void setNonAdaptive();\n };\n \n-constexpr inline auto getNonAdaptiveMrkExtension() { return \".mrk\"; }\n constexpr inline auto getNonAdaptiveMrkSizeWide() { return sizeof(UInt64) * 2; }\n constexpr inline auto getAdaptiveMrkSizeWide() { return sizeof(UInt64) * 3; }\n inline size_t getAdaptiveMrkSizeCompact(size_t columns_num);\n-std::string getAdaptiveMrkExtension(MergeTreeDataPartType part_type);\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\nindex ad5d828c4315..397a9d826556 100644\n--- a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n+++ b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n@@ -2,6 +2,7 @@\n #include <Storages/MergeTree/MergeTreeData.h>\n #include <Common/MemoryTrackerBlockerInThread.h>\n #include <IO/ReadBufferFromFile.h>\n+#include <Compression/CompressedReadBufferFromFile.h>\n #include <Common/setThreadName.h>\n #include <Common/scope_guard_safe.h>\n #include <Common/CurrentMetrics.h>\n@@ -59,6 +60,7 @@ MergeTreeMarksLoader::~MergeTreeMarksLoader()\n     }\n }\n \n+\n const MarkInCompressedFile & MergeTreeMarksLoader::getMark(size_t row_index, size_t column_index)\n {\n     if (!marks)\n@@ -87,6 +89,7 @@ const MarkInCompressedFile & MergeTreeMarksLoader::getMark(size_t row_index, siz\n     return (*marks)[row_index * columns_in_mark + column_index];\n }\n \n+\n MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n {\n     /// Memory for marks must not be accounted as memory usage for query, because they are stored in shared cache.\n@@ -94,42 +97,49 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n \n     size_t file_size = data_part_storage->getFileSize(mrk_path);\n     size_t mark_size = index_granularity_info.getMarkSizeInBytes(columns_in_mark);\n-    size_t expected_file_size = mark_size * marks_count;\n+    size_t expected_uncompressed_size = mark_size * marks_count;\n+\n+    auto res = std::make_shared<MarksInCompressedFile>(marks_count * columns_in_mark);\n \n-    if (expected_file_size != file_size)\n+    if (!index_granularity_info.mark_type.compressed && expected_uncompressed_size != file_size)\n         throw Exception(\n             ErrorCodes::CORRUPTED_DATA,\n             \"Bad size of marks file '{}': {}, must be: {}\",\n             std::string(fs::path(data_part_storage->getFullPath()) / mrk_path),\n-            std::to_string(file_size), std::to_string(expected_file_size));\n+            std::to_string(file_size), std::to_string(expected_uncompressed_size));\n \n-    auto res = std::make_shared<MarksInCompressedFile>(marks_count * columns_in_mark);\n+    auto buffer = data_part_storage->readFile(mrk_path, read_settings.adjustBufferSize(file_size), file_size, std::nullopt);\n+    std::unique_ptr<ReadBuffer> reader;\n+    if (!index_granularity_info.mark_type.compressed)\n+        reader = std::move(buffer);\n+    else\n+        reader = std::make_unique<CompressedReadBufferFromFile>(std::move(buffer));\n \n-    if (!index_granularity_info.is_adaptive)\n+    if (!index_granularity_info.mark_type.adaptive)\n     {\n         /// Read directly to marks.\n-        auto buffer = data_part_storage->readFile(mrk_path, read_settings.adjustBufferSize(file_size), file_size, std::nullopt);\n-        buffer->readStrict(reinterpret_cast<char *>(res->data()), file_size);\n+        reader->readStrict(reinterpret_cast<char *>(res->data()), expected_uncompressed_size);\n \n-        if (!buffer->eof())\n+        if (!reader->eof())\n             throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA,\n                 \"Cannot read all marks from file {}, is eof: {}, buffer size: {}, file size: {}\",\n-                mrk_path, buffer->eof(), buffer->buffer().size(), file_size);\n+                mrk_path, reader->eof(), reader->buffer().size(), file_size);\n     }\n     else\n     {\n-        auto buffer = data_part_storage->readFile(mrk_path, read_settings.adjustBufferSize(file_size), file_size, std::nullopt);\n         size_t i = 0;\n-        while (!buffer->eof())\n+        size_t granularity;\n+        while (!reader->eof())\n         {\n-            res->read(*buffer, i * columns_in_mark, columns_in_mark);\n-            buffer->seek(sizeof(size_t), SEEK_CUR);\n+            res->read(*reader, i * columns_in_mark, columns_in_mark);\n+            readIntBinary(granularity, *reader);\n             ++i;\n         }\n \n-        if (i * mark_size != file_size)\n+        if (i * mark_size != expected_uncompressed_size)\n             throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Cannot read all marks from file {}\", mrk_path);\n     }\n+\n     res->protect();\n     return res;\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeMarksLoader.h b/src/Storages/MergeTree/MergeTreeMarksLoader.h\nindex 60ccc953e9bd..4497339d7673 100644\n--- a/src/Storages/MergeTree/MergeTreeMarksLoader.h\n+++ b/src/Storages/MergeTree/MergeTreeMarksLoader.h\n@@ -1,9 +1,11 @@\n #pragma once\n+\n #include <Storages/MergeTree/IDataPartStorage.h>\n #include <Storages/MarkCache.h>\n #include <IO/ReadSettings.h>\n #include <Common/ThreadPool.h>\n \n+\n namespace DB\n {\n \ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 07659b1c9dc3..98bd9c1d34f6 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -145,6 +145,14 @@ struct Settings;\n     M(String, remote_fs_zero_copy_zookeeper_path, \"/clickhouse/zero_copy\", \"ZooKeeper path for Zero-copy table-independet info.\", 0) \\\n     M(Bool, remote_fs_zero_copy_path_compatible_mode, false, \"Run zero-copy in compatible mode during conversion process.\", 0) \\\n     \\\n+    /** Compress marks and primary key. */ \\\n+    M(Bool, compress_marks, false, \"Marks support compression, reduce mark file size and speed up network transmission.\", 0) \\\n+    M(Bool, compress_primary_key, false, \"Primary key support compression, reduce primary key file size and speed up network transmission.\", 0) \\\n+    M(String, marks_compression_codec, \"ZSTD(3)\", \"Compression encoding used by marks, marks are small enough and cached, so the default compression is ZSTD(3).\", 0) \\\n+    M(String, primary_key_compression_codec, \"ZSTD(3)\", \"Compression encoding used by primary, primary key is small enough and cached, so the default compression is ZSTD(3).\", 0) \\\n+    M(UInt64, marks_compress_block_size, 65536, \"Mark compress block size, the actual size of the block to compress.\", 0) \\\n+    M(UInt64, primary_key_compress_block_size, 65536, \"Primary compress block size, the actual size of the block to compress.\", 0) \\\n+    \\\n     /** Obsolete settings. Kept for backward compatibility only. */ \\\n     M(UInt64, min_relative_delay_to_yield_leadership, 120, \"Obsolete setting, does nothing.\", 0) \\\n     M(UInt64, check_delay_period, 60, \"Obsolete setting, does nothing.\", 0) \\\n@@ -153,7 +161,7 @@ struct Settings;\n     M(UInt64, replicated_max_parallel_sends_for_table, 0, \"Obsolete setting, does nothing.\", 0) \\\n     M(UInt64, replicated_max_parallel_fetches, 0, \"Obsolete setting, does nothing.\", 0) \\\n     M(UInt64, replicated_max_parallel_fetches_for_table, 0, \"Obsolete setting, does nothing.\", 0) \\\n-    M(Bool, write_final_mark, true, \"Obsolete setting, does nothing.\", 0)\n+    M(Bool, write_final_mark, true, \"Obsolete setting, does nothing.\", 0) \\\n     /// Settings that should not change after the creation of a table.\n     /// NOLINTNEXTLINE\n #define APPLY_FOR_IMMUTABLE_MERGE_TREE_SETTINGS(M) \\\ndiff --git a/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\nindex a5bc189e42f7..269a78977ad7 100644\n--- a/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n@@ -33,7 +33,7 @@ MergedBlockOutputStream::MergedBlockOutputStream(\n         storage.getContext()->getSettings(),\n         write_settings,\n         storage.getSettings(),\n-        data_part->index_granularity_info.is_adaptive,\n+        data_part->index_granularity_info.mark_type.adaptive,\n         /* rewrite_primary_key = */ true,\n         blocks_are_granules_size);\n \ndiff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\nindex 21e01223e2ce..dd75cddd3801 100644\n--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n@@ -30,8 +30,8 @@ MergedColumnOnlyOutputStream::MergedColumnOnlyOutputStream(\n         global_settings,\n         data_part->storage.getContext()->getWriteSettings(),\n         storage_settings,\n-        index_granularity_info ? index_granularity_info->is_adaptive : data_part->storage.canUseAdaptiveGranularity(),\n-        /* rewrite_primary_key = */false);\n+        index_granularity_info ? index_granularity_info->mark_type.adaptive : data_part->storage.canUseAdaptiveGranularity(),\n+        /* rewrite_primary_key = */ false);\n \n     writer = data_part->getWriter(\n         data_part_storage_builder,\ndiff --git a/src/Storages/MergeTree/MutateTask.cpp b/src/Storages/MergeTree/MutateTask.cpp\nindex 254bcd9f7f90..1c3ad0eb1a43 100644\n--- a/src/Storages/MergeTree/MutateTask.cpp\n+++ b/src/Storages/MergeTree/MutateTask.cpp\n@@ -1570,8 +1570,7 @@ bool MutateTask::prepare()\n     ctx->new_data_part->partition.assign(ctx->source_part->partition);\n \n     /// Don't change granularity type while mutating subset of columns\n-    ctx->mrk_extension = ctx->source_part->index_granularity_info.is_adaptive ? getAdaptiveMrkExtension(ctx->new_data_part->getType())\n-                                                                         : getNonAdaptiveMrkExtension();\n+    ctx->mrk_extension = ctx->source_part->index_granularity_info.mark_type.getFileExtension();\n \n     const auto data_settings = ctx->data->getSettings();\n     ctx->need_sync = needSyncPart(ctx->source_part->rows_count, ctx->source_part->getBytesOnDisk(), *data_settings);\ndiff --git a/src/Storages/MergeTree/PartMetadataManagerOrdinary.cpp b/src/Storages/MergeTree/PartMetadataManagerOrdinary.cpp\nindex da147ff1f0e6..7eb868f77548 100644\n--- a/src/Storages/MergeTree/PartMetadataManagerOrdinary.cpp\n+++ b/src/Storages/MergeTree/PartMetadataManagerOrdinary.cpp\n@@ -1,6 +1,7 @@\n #include \"PartMetadataManagerOrdinary.h\"\n \n #include <IO/ReadBufferFromFileBase.h>\n+#include <Compression/CompressedReadBufferFromFile.h>\n #include <Disks/IDisk.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n \n@@ -18,9 +19,14 @@ PartMetadataManagerOrdinary::PartMetadataManagerOrdinary(const IMergeTreeDataPar\n }\n \n \n-std::unique_ptr<SeekableReadBuffer> PartMetadataManagerOrdinary::read(const String & file_name) const\n+std::unique_ptr<ReadBuffer> PartMetadataManagerOrdinary::read(const String & file_name) const\n {\n-    return openForReading(part->data_part_storage, file_name);\n+    auto res = openForReading(part->data_part_storage, file_name);\n+\n+    if (isCompressedFromFileName(file_name))\n+        return std::make_unique<CompressedReadBufferFromFile>(std::move(res));\n+\n+    return res;\n }\n \n bool PartMetadataManagerOrdinary::exists(const String & file_name) const\ndiff --git a/src/Storages/MergeTree/PartMetadataManagerOrdinary.h b/src/Storages/MergeTree/PartMetadataManagerOrdinary.h\nindex a655431296a9..d86d5c54c007 100644\n--- a/src/Storages/MergeTree/PartMetadataManagerOrdinary.h\n+++ b/src/Storages/MergeTree/PartMetadataManagerOrdinary.h\n@@ -12,7 +12,7 @@ class PartMetadataManagerOrdinary : public IPartMetadataManager\n \n     ~PartMetadataManagerOrdinary() override = default;\n \n-    std::unique_ptr<SeekableReadBuffer> read(const String & file_name) const override;\n+    std::unique_ptr<ReadBuffer> read(const String & file_name) const override;\n \n     bool exists(const String & file_name) const override;\n \ndiff --git a/src/Storages/MergeTree/PartMetadataManagerWithCache.cpp b/src/Storages/MergeTree/PartMetadataManagerWithCache.cpp\nindex 5a291373e6c8..ee0970984f99 100644\n--- a/src/Storages/MergeTree/PartMetadataManagerWithCache.cpp\n+++ b/src/Storages/MergeTree/PartMetadataManagerWithCache.cpp\n@@ -5,6 +5,7 @@\n #include <Common/ErrorCodes.h>\n #include <IO/HashingReadBuffer.h>\n #include <IO/ReadBufferFromString.h>\n+#include <Compression/CompressedReadBufferFromFile.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n \n namespace ProfileEvents\n@@ -38,7 +39,7 @@ String PartMetadataManagerWithCache::getFilePathFromKey(const String & key) cons\n     return key.substr(part->data_part_storage->getDiskName().size() + 1);\n }\n \n-std::unique_ptr<SeekableReadBuffer> PartMetadataManagerWithCache::read(const String & file_name) const\n+std::unique_ptr<ReadBuffer> PartMetadataManagerWithCache::read(const String & file_name) const\n {\n     String file_path = fs::path(part->data_part_storage->getRelativePath()) / file_name;\n     String key = getKeyFromFilePath(file_path);\n@@ -48,7 +49,13 @@ std::unique_ptr<SeekableReadBuffer> PartMetadataManagerWithCache::read(const Str\n     {\n         ProfileEvents::increment(ProfileEvents::MergeTreeMetadataCacheMiss);\n         auto in = part->data_part_storage->readFile(file_name, {}, std::nullopt, std::nullopt);\n-        readStringUntilEOF(value, *in);\n+        std::unique_ptr<ReadBuffer> reader;\n+        if (!isCompressedFromFileName(file_name))\n+            reader = std::move(in);\n+        else\n+            reader = std::make_unique<CompressedReadBufferFromFile>(std::move(in));\n+\n+        readStringUntilEOF(value, *reader);\n         cache->put(key, value);\n     }\n     else\ndiff --git a/src/Storages/MergeTree/PartMetadataManagerWithCache.h b/src/Storages/MergeTree/PartMetadataManagerWithCache.h\nindex 06e7a85ba2b5..791681ee5bbc 100644\n--- a/src/Storages/MergeTree/PartMetadataManagerWithCache.h\n+++ b/src/Storages/MergeTree/PartMetadataManagerWithCache.h\n@@ -19,7 +19,7 @@ class PartMetadataManagerWithCache : public IPartMetadataManager\n     ~PartMetadataManagerWithCache() override = default;\n \n     /// First read the metadata from RocksDB cache, then from disk.\n-    std::unique_ptr<SeekableReadBuffer> read(const String & file_name) const override;\n+    std::unique_ptr<ReadBuffer> read(const String & file_name) const override;\n \n     /// First judge existence of the metadata in RocksDB cache, then in disk.\n     bool exists(const String & file_name) const override;\n@@ -48,7 +48,6 @@ class PartMetadataManagerWithCache : public IPartMetadataManager\n     /// Get cache keys and checksums of corresponding metadata in a part(including projection parts)\n     void getKeysAndCheckSums(Strings & keys, std::vector<uint128> & checksums) const;\n \n-\n     MergeTreeMetadataCachePtr cache;\n };\n \ndiff --git a/src/Storages/System/StorageSystemPartsColumns.cpp b/src/Storages/System/StorageSystemPartsColumns.cpp\nindex 8837c11970da..cc6e69b160f4 100644\n--- a/src/Storages/System/StorageSystemPartsColumns.cpp\n+++ b/src/Storages/System/StorageSystemPartsColumns.cpp\n@@ -261,7 +261,7 @@ void StorageSystemPartsColumns::processNextStorage(\n                     size.data_uncompressed += bin_checksum->second.uncompressed_size;\n                 }\n \n-                auto mrk_checksum = part->checksums.files.find(file_name + part->index_granularity_info.marks_file_extension);\n+                auto mrk_checksum = part->checksums.files.find(file_name + part->index_granularity_info.mark_type.getFileExtension());\n                 if (mrk_checksum != part->checksums.files.end())\n                     size.marks += mrk_checksum->second.file_size;\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02381_compress_marks_and_primary_key.reference b/tests/queries/0_stateless/02381_compress_marks_and_primary_key.reference\nnew file mode 100644\nindex 000000000000..53bddb77b845\n--- /dev/null\n+++ b/tests/queries/0_stateless/02381_compress_marks_and_primary_key.reference\n@@ -0,0 +1,14 @@\n+1000\t10000\n+1000\t10000\n+test_02381\t2000000\t16112790\t11904\t16100886\n+test_02381_compress\t2000000\t16099626\t1658\t16097968\n+10000\t100000\n+10000\t100000\n+10000\t100000\n+10000\t100000\n+test_02381\t4000000\t28098334\t2946\t28095388\n+test_02381_compress\t4000000\t28125412\t23616\t28101796\n+1\tHello\n+2\tWorld\n+1\tHello\n+2\tWorld\ndiff --git a/tests/queries/0_stateless/02381_compress_marks_and_primary_key.sql b/tests/queries/0_stateless/02381_compress_marks_and_primary_key.sql\nnew file mode 100644\nindex 000000000000..b929c9a3a2d7\n--- /dev/null\n+++ b/tests/queries/0_stateless/02381_compress_marks_and_primary_key.sql\n@@ -0,0 +1,50 @@\n+drop table if exists test_02381;\n+create table test_02381(a UInt64, b UInt64) ENGINE = MergeTree order by (a, b);\n+insert into test_02381 select number, number * 10 from system.numbers limit 1000000;\n+\n+drop table if exists test_02381_compress;\n+create table test_02381_compress(a UInt64, b UInt64) ENGINE = MergeTree order by (a, b)\n+    SETTINGS compress_marks=true, compress_primary_key=true, marks_compression_codec='ZSTD(3)', primary_key_compression_codec='ZSTD(3)', marks_compress_block_size=65536, primary_key_compress_block_size=65536;\n+insert into test_02381_compress select number, number * 10 from system.numbers limit 1000000;\n+\n+select * from test_02381_compress where a = 1000 limit 1;\n+optimize table test_02381_compress final;\n+select * from test_02381_compress where a = 1000 limit 1;\n+\n+-- Compare the size of marks on disk\n+select table, sum(rows), sum(bytes_on_disk) sum_bytes, sum(marks_bytes) sum_marks_bytes, (sum_bytes - sum_marks_bytes) exclude_marks from system.parts_columns where active and database = currentDatabase() and table like 'test_02381%' group by table order by table;\n+\n+-- Switch to compressed and uncompressed\n+-- Test wide part\n+alter table test_02381 modify setting compress_marks=true, compress_primary_key=true;\n+insert into test_02381 select number, number * 10 from system.numbers limit 1000000;\n+\n+alter table test_02381_compress modify setting compress_marks=false, compress_primary_key=false;\n+insert into test_02381_compress select number, number * 10 from system.numbers limit 1000000;\n+\n+select * from test_02381_compress where a = 10000 limit 1;\n+optimize table test_02381_compress final;\n+select * from test_02381_compress where a = 10000 limit 1;\n+\n+select * from test_02381 where a = 10000 limit 1;\n+optimize table test_02381 final;\n+select * from test_02381 where a = 10000 limit 1;\n+\n+select table, sum(rows), sum(bytes_on_disk) sum_bytes, sum(marks_bytes) sum_marks_bytes, (sum_bytes - sum_marks_bytes) exclude_marks  from system.parts_columns where active and  database = currentDatabase() and table like 'test_02381%' group by table order by table;\n+\n+drop table if exists test_02381;\n+drop table if exists test_02381_compress;\n+\n+-- Test compact part\n+drop table if exists test_02381_compact;\n+create table test_02381_compact (a UInt64, b String) ENGINE = MergeTree order by (a, b);\n+\n+insert into test_02381_compact values (1, 'Hello');\n+alter table test_02381_compact modify setting compress_marks = true, compress_primary_key = true;\n+insert into test_02381_compact values (2, 'World');\n+\n+select * from test_02381_compact order by a;\n+optimize table test_02381_compact final;\n+select * from test_02381_compact order by a;\n+\n+drop table if exists test_02381_compact;\n",
  "problem_statement": "It makes sense to compress marks and primary.idx\n**Describe the issue**\r\n```\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name *.mrk2 | xargs wc -c\r\n7602201768 total\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name *.mrk2 | xargs cat | clickhouse-compressor | wc -c\r\n1367956184\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name *.mrk2 | xargs cat | clickhouse-compressor --codec 'ZSTD' | wc -c\r\n455141209\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name *.mrk2 | xargs cat | clickhouse-compressor --codec 'ZSTD(3)' | wc -c\r\n364667373\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name primary.idx | xargs wc -c\r\n1184498877 total\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name primary.idx | xargs cat | clickhouse-compressor | wc -c\r\n756816544\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name primary.idx | xargs cat | clickhouse-compressor --codec 'ZSTD' | wc -c\r\n602538200\r\nroot@ip-172-31-22-38:/opt/clickhouse/store/308/30855e33-a73f-481b-942c-04710e028fe9# find . -name primary.idx | xargs cat | clickhouse-compressor --codec 'ZSTD(3)' | wc -c\r\n552119618\r\n```\r\n\r\nIn this example, marks compressed 20 times and primary key - 2 times.\r\nThat's a huge difference if they are loaded over slow network (example: using MergeTree over `web` disk plugged on my desktop).\r\n\n",
  "hints_text": "Proposal:\r\n\r\nAdd the following to `MergeTreeSettings`:\r\n- `compress_marks`: 0/1; disabled by default to allow downgrades and will be enabled by default in several months after introducing this option;\r\n- `compress_primary_key`: 0/1, similarly;\r\n- `marks_compression_codec` - `ZSTD(3)` by default. Motivation: marks are small enough and cached, so heavy compression by default is ok.\r\n- `primary_key_compression_codec` - similarly;\r\n- `marks_compress_block_size` - 64K by default;\r\n- `primary_key_compress_block_size` - similarly.\r\n\r\nIf compression of marks or index is enabled, they will be written to files with `.cmrk`, `.cmrk2`, `.cmrk3`, `.cidx` files instead of `.mrk`, `.mrk2`, `.mrk3`, `.idx`.\r\n\r\nWhen data parts are loaded, we will check the existence of compressed files regardless if compression is enabled or not and support reading both types of files.\r\n\r\nData is compressed and decompressed simply by using `CompressedReadBuffer`/`CompressedWriteBuffer`.",
  "created_at": "2022-05-31T12:50:39Z",
  "modified_files": [
    "src/Processors/QueryPlan/ReadFromMergeTree.cpp",
    "src/Storages/MergeTree/IMergeTreeDataPart.cpp",
    "src/Storages/MergeTree/IMergeTreeDataPart.h",
    "src/Storages/MergeTree/IMergedBlockOutputStream.cpp",
    "src/Storages/MergeTree/IPartMetadataManager.cpp",
    "src/Storages/MergeTree/IPartMetadataManager.h",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartCompact.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWide.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h",
    "src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h",
    "src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp",
    "src/Storages/MergeTree/MergeTreeIOSettings.h",
    "src/Storages/MergeTree/MergeTreeIndexGranularityInfo.cpp",
    "src/Storages/MergeTree/MergeTreeIndexGranularityInfo.h",
    "src/Storages/MergeTree/MergeTreeMarksLoader.cpp",
    "src/Storages/MergeTree/MergeTreeMarksLoader.h",
    "src/Storages/MergeTree/MergeTreeSettings.h",
    "src/Storages/MergeTree/MergedBlockOutputStream.cpp",
    "src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp",
    "src/Storages/MergeTree/MutateTask.cpp",
    "src/Storages/MergeTree/PartMetadataManagerOrdinary.cpp",
    "src/Storages/MergeTree/PartMetadataManagerOrdinary.h",
    "src/Storages/MergeTree/PartMetadataManagerWithCache.cpp",
    "src/Storages/MergeTree/PartMetadataManagerWithCache.h",
    "src/Storages/System/StorageSystemPartsColumns.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/02381_compress_marks_and_primary_key.reference",
    "b/tests/queries/0_stateless/02381_compress_marks_and_primary_key.sql"
  ]
}