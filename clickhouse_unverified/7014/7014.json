{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 7014,
  "instance_id": "ClickHouse__ClickHouse-7014",
  "issue_numbers": [
    "6923"
  ],
  "base_commit": "ef75a45fefef671b7cc1b206bd7f5ba01dbb0834",
  "patch": "diff --git a/dbms/src/IO/HTTPCommon.cpp b/dbms/src/IO/HTTPCommon.cpp\nindex ca5b5ab700ba..0a7c7e7af66b 100644\n--- a/dbms/src/IO/HTTPCommon.cpp\n+++ b/dbms/src/IO/HTTPCommon.cpp\n@@ -45,7 +45,7 @@ namespace ErrorCodes\n \n namespace\n {\n-void setTimeouts(Poco::Net::HTTPClientSession & session, const ConnectionTimeouts & timeouts)\n+    void setTimeouts(Poco::Net::HTTPClientSession & session, const ConnectionTimeouts & timeouts)\n     {\n #if defined(POCO_CLICKHOUSE_PATCH) || POCO_VERSION >= 0x02000000\n         session.setTimeout(timeouts.connection_timeout, timeouts.send_timeout, timeouts.receive_timeout);\n@@ -216,20 +216,25 @@ PooledHTTPSessionPtr makePooledHTTPSession(const Poco::URI & uri, const Connecti\n std::istream * receiveResponse(\n     Poco::Net::HTTPClientSession & session, const Poco::Net::HTTPRequest & request, Poco::Net::HTTPResponse & response)\n {\n-    auto istr = &session.receiveResponse(response);\n+    auto & istr = session.receiveResponse(response);\n+    assertResponseIsOk(request, response, istr);\n+    return &istr;\n+}\n+\n+void assertResponseIsOk(const Poco::Net::HTTPRequest & request, Poco::Net::HTTPResponse & response, std::istream & istr)\n+{\n     auto status = response.getStatus();\n \n     if (status != Poco::Net::HTTPResponse::HTTP_OK)\n     {\n         std::stringstream error_message;\n         error_message << \"Received error from remote server \" << request.getURI() << \". HTTP status code: \" << status << \" \"\n-                      << response.getReason() << \", body: \" << istr->rdbuf();\n+                      << response.getReason() << \", body: \" << istr.rdbuf();\n \n         throw Exception(error_message.str(),\n             status == HTTP_TOO_MANY_REQUESTS ? ErrorCodes::RECEIVED_ERROR_TOO_MANY_REQUESTS\n                                              : ErrorCodes::RECEIVED_ERROR_FROM_REMOTE_IO_SERVER);\n     }\n-    return istr;\n }\n \n }\ndiff --git a/dbms/src/IO/HTTPCommon.h b/dbms/src/IO/HTTPCommon.h\nindex 6dc669c248e0..412429e59d19 100644\n--- a/dbms/src/IO/HTTPCommon.h\n+++ b/dbms/src/IO/HTTPCommon.h\n@@ -57,4 +57,6 @@ PooledHTTPSessionPtr makePooledHTTPSession(const Poco::URI & uri, const Connecti\n   */\n std::istream * receiveResponse(\n     Poco::Net::HTTPClientSession & session, const Poco::Net::HTTPRequest & request, Poco::Net::HTTPResponse & response);\n+void assertResponseIsOk(const Poco::Net::HTTPRequest & request, Poco::Net::HTTPResponse & response, std::istream & istr);\n+\n }\ndiff --git a/dbms/src/IO/ReadBufferFromS3.cpp b/dbms/src/IO/ReadBufferFromS3.cpp\nnew file mode 100644\nindex 000000000000..ae09f0fb1898\n--- /dev/null\n+++ b/dbms/src/IO/ReadBufferFromS3.cpp\n@@ -0,0 +1,70 @@\n+#include <IO/ReadBufferFromS3.h>\n+\n+#include <IO/ReadBufferFromIStream.h>\n+\n+#include <common/logger_useful.h>\n+\n+\n+namespace DB\n+{\n+\n+const int DEFAULT_S3_MAX_FOLLOW_GET_REDIRECT = 2;\n+\n+ReadBufferFromS3::ReadBufferFromS3(Poco::URI uri_,\n+    const ConnectionTimeouts & timeouts,\n+    const Poco::Net::HTTPBasicCredentials & credentials,\n+    size_t buffer_size_)\n+    : ReadBuffer(nullptr, 0)\n+    , uri {uri_}\n+    , method {Poco::Net::HTTPRequest::HTTP_GET}\n+    , session {makeHTTPSession(uri_, timeouts)}\n+{\n+    Poco::Net::HTTPResponse response;\n+    std::unique_ptr<Poco::Net::HTTPRequest> request;\n+\n+    for (int i = 0; i < DEFAULT_S3_MAX_FOLLOW_GET_REDIRECT; ++i)\n+    {\n+        // With empty path poco will send \"POST  HTTP/1.1\" its bug.\n+        if (uri.getPath().empty())\n+            uri.setPath(\"/\");\n+\n+        request = std::make_unique<Poco::Net::HTTPRequest>(method, uri.getPathAndQuery(), Poco::Net::HTTPRequest::HTTP_1_1);\n+        request->setHost(uri.getHost()); // use original, not resolved host name in header\n+\n+        if (!credentials.getUsername().empty())\n+            credentials.authenticate(*request);\n+\n+        LOG_TRACE((&Logger::get(\"ReadBufferFromS3\")), \"Sending request to \" << uri.toString());\n+\n+        session->sendRequest(*request);\n+\n+        istr = &session->receiveResponse(response);\n+\n+        // Handle 307 Temporary Redirect in order to allow request redirection\n+        // See https://docs.aws.amazon.com/AmazonS3/latest/dev/Redirects.html\n+        if (response.getStatus() != Poco::Net::HTTPResponse::HTTP_TEMPORARY_REDIRECT)\n+            break;\n+\n+        auto location_iterator = response.find(\"Location\");\n+        if (location_iterator == response.end())\n+            break;\n+\n+        uri = location_iterator->second;\n+        session = makeHTTPSession(uri, timeouts);\n+    }\n+\n+    assertResponseIsOk(*request, response, *istr);\n+    impl = std::make_unique<ReadBufferFromIStream>(*istr, buffer_size_);\n+}\n+\n+\n+bool ReadBufferFromS3::nextImpl()\n+{\n+    if (!impl->next())\n+        return false;\n+    internal_buffer = impl->buffer();\n+    working_buffer = internal_buffer;\n+    return true;\n+}\n+\n+}\ndiff --git a/dbms/src/IO/ReadBufferFromS3.h b/dbms/src/IO/ReadBufferFromS3.h\nnew file mode 100644\nindex 000000000000..ffc0c5c0ab1f\n--- /dev/null\n+++ b/dbms/src/IO/ReadBufferFromS3.h\n@@ -0,0 +1,35 @@\n+#pragma once\n+\n+#include <memory>\n+\n+#include <IO/ConnectionTimeouts.h>\n+#include <IO/HTTPCommon.h>\n+#include <IO/ReadBuffer.h>\n+#include <Poco/Net/HTTPBasicCredentials.h>\n+#include <Poco/URI.h>\n+\n+\n+namespace DB\n+{\n+/** Perform S3 HTTP GET request and provide response to read.\n+  */\n+class ReadBufferFromS3 : public ReadBuffer\n+{\n+protected:\n+    Poco::URI uri;\n+    std::string method;\n+\n+    HTTPSessionPtr session;\n+    std::istream * istr; /// owned by session\n+    std::unique_ptr<ReadBuffer> impl;\n+\n+public:\n+    explicit ReadBufferFromS3(Poco::URI uri_,\n+        const ConnectionTimeouts & timeouts = {},\n+        const Poco::Net::HTTPBasicCredentials & credentials = {},\n+        size_t buffer_size_ = DBMS_DEFAULT_BUFFER_SIZE);\n+\n+    bool nextImpl() override;\n+};\n+\n+}\ndiff --git a/dbms/src/IO/WriteBufferFromS3.cpp b/dbms/src/IO/WriteBufferFromS3.cpp\nnew file mode 100644\nindex 000000000000..1ef6f3b19a06\n--- /dev/null\n+++ b/dbms/src/IO/WriteBufferFromS3.cpp\n@@ -0,0 +1,286 @@\n+#include <IO/WriteBufferFromS3.h>\n+\n+#include <IO/WriteHelpers.h>\n+\n+#include <Poco/DOM/AutoPtr.h>\n+#include <Poco/DOM/DOMParser.h>\n+#include <Poco/DOM/Document.h>\n+#include <Poco/DOM/NodeList.h>\n+#include <Poco/SAX/InputSource.h>\n+\n+#include <common/logger_useful.h>\n+\n+\n+namespace DB\n+{\n+\n+const int DEFAULT_S3_MAX_FOLLOW_PUT_REDIRECT = 2;\n+const int S3_WARN_MAX_PARTS = 10000;\n+\n+\n+namespace ErrorCodes\n+{\n+    extern const int INCORRECT_DATA;\n+}\n+\n+\n+WriteBufferFromS3::WriteBufferFromS3(\n+    const Poco::URI & uri_,\n+    size_t minimum_upload_part_size_,\n+    const ConnectionTimeouts & timeouts_,\n+    const Poco::Net::HTTPBasicCredentials & credentials, size_t buffer_size_\n+)\n+    : BufferWithOwnMemory<WriteBuffer>(buffer_size_, nullptr, 0)\n+    , uri {uri_}\n+    , minimum_upload_part_size {minimum_upload_part_size_}\n+    , timeouts {timeouts_}\n+    , auth_request {Poco::Net::HTTPRequest::HTTP_PUT, uri.getPathAndQuery(), Poco::Net::HTTPRequest::HTTP_1_1}\n+    , temporary_buffer {std::make_unique<WriteBufferFromString>(buffer_string)}\n+    , last_part_size {0}\n+{\n+    if (!credentials.getUsername().empty())\n+        credentials.authenticate(auth_request);\n+\n+    initiate();\n+}\n+\n+\n+void WriteBufferFromS3::nextImpl()\n+{\n+    if (!offset())\n+        return;\n+\n+    temporary_buffer->write(working_buffer.begin(), offset());\n+\n+    last_part_size += offset();\n+\n+    if (last_part_size > minimum_upload_part_size)\n+    {\n+        temporary_buffer->finish();\n+        writePart(buffer_string);\n+        last_part_size = 0;\n+        temporary_buffer = std::make_unique<WriteBufferFromString>(buffer_string);\n+    }\n+}\n+\n+\n+void WriteBufferFromS3::finalize()\n+{\n+    temporary_buffer->finish();\n+    if (!buffer_string.empty())\n+    {\n+        writePart(buffer_string);\n+    }\n+\n+    complete();\n+}\n+\n+\n+WriteBufferFromS3::~WriteBufferFromS3()\n+{\n+    try\n+    {\n+        next();\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(__PRETTY_FUNCTION__);\n+    }\n+}\n+\n+\n+void WriteBufferFromS3::initiate()\n+{\n+    // See https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html\n+    Poco::Net::HTTPResponse response;\n+    std::unique_ptr<Poco::Net::HTTPRequest> request_ptr;\n+    HTTPSessionPtr session;\n+    std::istream * istr = nullptr; /// owned by session\n+    Poco::URI initiate_uri = uri;\n+    initiate_uri.setRawQuery(\"uploads\");\n+    for (auto & param: uri.getQueryParameters())\n+    {\n+        initiate_uri.addQueryParameter(param.first, param.second);\n+    }\n+\n+    for (int i = 0; i < DEFAULT_S3_MAX_FOLLOW_PUT_REDIRECT; ++i)\n+    {\n+        session = makeHTTPSession(initiate_uri, timeouts);\n+        request_ptr = std::make_unique<Poco::Net::HTTPRequest>(Poco::Net::HTTPRequest::HTTP_POST, initiate_uri.getPathAndQuery(), Poco::Net::HTTPRequest::HTTP_1_1);\n+        request_ptr->setHost(initiate_uri.getHost()); // use original, not resolved host name in header\n+\n+        if (auth_request.hasCredentials())\n+        {\n+            Poco::Net::HTTPBasicCredentials credentials(auth_request);\n+            credentials.authenticate(*request_ptr);\n+        }\n+\n+        request_ptr->setContentLength(0);\n+\n+        LOG_TRACE((&Logger::get(\"WriteBufferFromS3\")), \"Sending request to \" << initiate_uri.toString());\n+\n+        session->sendRequest(*request_ptr);\n+\n+        istr = &session->receiveResponse(response);\n+\n+        // Handle 307 Temporary Redirect in order to allow request redirection\n+        // See https://docs.aws.amazon.com/AmazonS3/latest/dev/Redirects.html\n+        if (response.getStatus() != Poco::Net::HTTPResponse::HTTP_TEMPORARY_REDIRECT)\n+            break;\n+\n+        auto location_iterator = response.find(\"Location\");\n+        if (location_iterator == response.end())\n+            break;\n+\n+        initiate_uri = location_iterator->second;\n+    }\n+    assertResponseIsOk(*request_ptr, response, *istr);\n+\n+    Poco::XML::InputSource src(*istr);\n+    Poco::XML::DOMParser parser;\n+    Poco::AutoPtr<Poco::XML::Document> document = parser.parse(&src);\n+    Poco::AutoPtr<Poco::XML::NodeList> nodes = document->getElementsByTagName(\"UploadId\");\n+    if (nodes->length() != 1)\n+    {\n+        throw Exception(\"Incorrect XML in response, no upload id\", ErrorCodes::INCORRECT_DATA);\n+    }\n+    upload_id = nodes->item(0)->innerText();\n+    if (upload_id.empty())\n+    {\n+        throw Exception(\"Incorrect XML in response, empty upload id\", ErrorCodes::INCORRECT_DATA);\n+    }\n+}\n+\n+\n+void WriteBufferFromS3::writePart(const String & data)\n+{\n+    // See https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html\n+    Poco::Net::HTTPResponse response;\n+    std::unique_ptr<Poco::Net::HTTPRequest> request_ptr;\n+    HTTPSessionPtr session;\n+    std::istream * istr = nullptr; /// owned by session\n+    Poco::URI part_uri = uri;\n+    part_uri.addQueryParameter(\"partNumber\", std::to_string(part_tags.size() + 1));\n+    part_uri.addQueryParameter(\"uploadId\", upload_id);\n+\n+    if (part_tags.size() == S3_WARN_MAX_PARTS)\n+    {\n+        // Don't throw exception here by ourselves but leave the decision to take by S3 server.\n+        LOG_WARNING(&Logger::get(\"WriteBufferFromS3\"), \"Maximum part number in S3 protocol has reached (too much parts). Server may not accept this whole upload.\");\n+    }\n+\n+    for (int i = 0; i < DEFAULT_S3_MAX_FOLLOW_PUT_REDIRECT; ++i)\n+    {\n+        session = makeHTTPSession(part_uri, timeouts);\n+        request_ptr = std::make_unique<Poco::Net::HTTPRequest>(Poco::Net::HTTPRequest::HTTP_PUT, part_uri.getPathAndQuery(), Poco::Net::HTTPRequest::HTTP_1_1);\n+        request_ptr->setHost(part_uri.getHost()); // use original, not resolved host name in header\n+\n+        if (auth_request.hasCredentials())\n+        {\n+            Poco::Net::HTTPBasicCredentials credentials(auth_request);\n+            credentials.authenticate(*request_ptr);\n+        }\n+\n+        request_ptr->setExpectContinue(true);\n+\n+        request_ptr->setContentLength(data.size());\n+\n+        LOG_TRACE((&Logger::get(\"WriteBufferFromS3\")), \"Sending request to \" << part_uri.toString());\n+\n+        std::ostream & ostr = session->sendRequest(*request_ptr);\n+        if (session->peekResponse(response))\n+        {\n+            // Received 100-continue.\n+            ostr << data;\n+        }\n+\n+        istr = &session->receiveResponse(response);\n+\n+        // Handle 307 Temporary Redirect in order to allow request redirection\n+        // See https://docs.aws.amazon.com/AmazonS3/latest/dev/Redirects.html\n+        if (response.getStatus() != Poco::Net::HTTPResponse::HTTP_TEMPORARY_REDIRECT)\n+            break;\n+\n+        auto location_iterator = response.find(\"Location\");\n+        if (location_iterator == response.end())\n+            break;\n+\n+        part_uri = location_iterator->second;\n+    }\n+    assertResponseIsOk(*request_ptr, response, *istr);\n+\n+    auto etag_iterator = response.find(\"ETag\");\n+    if (etag_iterator == response.end())\n+    {\n+        throw Exception(\"Incorrect response, no ETag\", ErrorCodes::INCORRECT_DATA);\n+    }\n+    part_tags.push_back(etag_iterator->second);\n+}\n+\n+\n+void WriteBufferFromS3::complete()\n+{\n+    // See https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html\n+    Poco::Net::HTTPResponse response;\n+    std::unique_ptr<Poco::Net::HTTPRequest> request_ptr;\n+    HTTPSessionPtr session;\n+    std::istream * istr = nullptr; /// owned by session\n+    Poco::URI complete_uri = uri;\n+    complete_uri.addQueryParameter(\"uploadId\", upload_id);\n+\n+    String data;\n+    WriteBufferFromString buffer(data);\n+    writeString(\"<CompleteMultipartUpload>\", buffer);\n+    for (size_t i = 0; i < part_tags.size(); ++i)\n+    {\n+        writeString(\"<Part><PartNumber>\", buffer);\n+        writeIntText(i + 1, buffer);\n+        writeString(\"</PartNumber><ETag>\", buffer);\n+        writeString(part_tags[i], buffer);\n+        writeString(\"</ETag></Part>\", buffer);\n+    }\n+    writeString(\"</CompleteMultipartUpload>\", buffer);\n+    buffer.finish();\n+\n+    for (int i = 0; i < DEFAULT_S3_MAX_FOLLOW_PUT_REDIRECT; ++i)\n+    {\n+        session = makeHTTPSession(complete_uri, timeouts);\n+        request_ptr = std::make_unique<Poco::Net::HTTPRequest>(Poco::Net::HTTPRequest::HTTP_POST, complete_uri.getPathAndQuery(), Poco::Net::HTTPRequest::HTTP_1_1);\n+        request_ptr->setHost(complete_uri.getHost()); // use original, not resolved host name in header\n+\n+        if (auth_request.hasCredentials())\n+        {\n+            Poco::Net::HTTPBasicCredentials credentials(auth_request);\n+            credentials.authenticate(*request_ptr);\n+        }\n+\n+        request_ptr->setExpectContinue(true);\n+\n+        request_ptr->setContentLength(data.size());\n+\n+        LOG_TRACE((&Logger::get(\"WriteBufferFromS3\")), \"Sending request to \" << complete_uri.toString());\n+\n+        std::ostream & ostr = session->sendRequest(*request_ptr);\n+        if (session->peekResponse(response))\n+        {\n+            // Received 100-continue.\n+            ostr << data;\n+        }\n+\n+        istr = &session->receiveResponse(response);\n+\n+        // Handle 307 Temporary Redirect in order to allow request redirection\n+        // See https://docs.aws.amazon.com/AmazonS3/latest/dev/Redirects.html\n+        if (response.getStatus() != Poco::Net::HTTPResponse::HTTP_TEMPORARY_REDIRECT)\n+            break;\n+\n+        auto location_iterator = response.find(\"Location\");\n+        if (location_iterator == response.end())\n+            break;\n+\n+        complete_uri = location_iterator->second;\n+    }\n+    assertResponseIsOk(*request_ptr, response, *istr);\n+}\n+\n+}\ndiff --git a/dbms/src/IO/WriteBufferFromS3.h b/dbms/src/IO/WriteBufferFromS3.h\nnew file mode 100644\nindex 000000000000..0eb689e468fc\n--- /dev/null\n+++ b/dbms/src/IO/WriteBufferFromS3.h\n@@ -0,0 +1,62 @@\n+#pragma once\n+\n+#include <functional>\n+#include <memory>\n+#include <vector>\n+#include <Core/Types.h>\n+#include <IO/ConnectionTimeouts.h>\n+#include <IO/HTTPCommon.h>\n+#include <IO/BufferWithOwnMemory.h>\n+#include <IO/ReadBuffer.h>\n+#include <IO/ReadBufferFromIStream.h>\n+#include <IO/WriteBuffer.h>\n+#include <IO/WriteBufferFromString.h>\n+#include <Poco/Net/HTTPBasicCredentials.h>\n+#include <Poco/Net/HTTPClientSession.h>\n+#include <Poco/Net/HTTPRequest.h>\n+#include <Poco/Net/HTTPResponse.h>\n+#include <Poco/URI.h>\n+#include <Poco/Version.h>\n+#include <Common/DNSResolver.h>\n+#include <Common/config.h>\n+#include <common/logger_useful.h>\n+\n+\n+namespace DB\n+{\n+/* Perform S3 HTTP PUT request.\n+ */\n+class WriteBufferFromS3 : public BufferWithOwnMemory<WriteBuffer>\n+{\n+private:\n+    Poco::URI uri;\n+    size_t minimum_upload_part_size;\n+    ConnectionTimeouts timeouts;\n+    Poco::Net::HTTPRequest auth_request;\n+    String buffer_string;\n+    std::unique_ptr<WriteBufferFromString> temporary_buffer;\n+    size_t last_part_size;\n+    String upload_id;\n+    std::vector<String> part_tags;\n+\n+public:\n+    explicit WriteBufferFromS3(const Poco::URI & uri,\n+        size_t minimum_upload_part_size_,\n+        const ConnectionTimeouts & timeouts = {},\n+        const Poco::Net::HTTPBasicCredentials & credentials = {},\n+        size_t buffer_size_ = DBMS_DEFAULT_BUFFER_SIZE);\n+\n+    void nextImpl() override;\n+\n+    /// Receives response from the server after sending all data.\n+    void finalize();\n+\n+    ~WriteBufferFromS3() override;\n+\n+private:\n+    void initiate();\n+    void writePart(const String & data);\n+    void complete();\n+};\n+\n+}\ndiff --git a/dbms/src/Storages/StorageS3.cpp b/dbms/src/Storages/StorageS3.cpp\nnew file mode 100644\nindex 000000000000..59b2ef589a9f\n--- /dev/null\n+++ b/dbms/src/Storages/StorageS3.cpp\n@@ -0,0 +1,177 @@\n+#include <Storages/StorageFactory.h>\n+#include <Storages/StorageS3.h>\n+\n+#include <Interpreters/Context.h>\n+#include <Interpreters/evaluateConstantExpression.h>\n+#include <Parsers/ASTLiteral.h>\n+\n+#include <IO/ReadBufferFromS3.h>\n+#include <IO/WriteBufferFromS3.h>\n+\n+#include <Formats/FormatFactory.h>\n+\n+#include <DataStreams/IBlockOutputStream.h>\n+#include <DataStreams/IBlockInputStream.h>\n+#include <DataStreams/AddingDefaultsBlockInputStream.h>\n+\n+#include <Poco/Net/HTTPRequest.h>\n+\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n+}\n+\n+namespace\n+{\n+    class StorageS3BlockInputStream : public IBlockInputStream\n+    {\n+    public:\n+        StorageS3BlockInputStream(const Poco::URI & uri,\n+            const String & format,\n+            const String & name_,\n+            const Block & sample_block,\n+            const Context & context,\n+            UInt64 max_block_size,\n+            const ConnectionTimeouts & timeouts)\n+            : name(name_)\n+        {\n+            read_buf = std::make_unique<ReadBufferFromS3>(uri, timeouts);\n+\n+            reader = FormatFactory::instance().getInput(format, *read_buf, sample_block, context, max_block_size);\n+        }\n+\n+        String getName() const override\n+        {\n+            return name;\n+        }\n+\n+        Block readImpl() override\n+        {\n+            return reader->read();\n+        }\n+\n+        Block getHeader() const override\n+        {\n+            return reader->getHeader();\n+        }\n+\n+        void readPrefixImpl() override\n+        {\n+            reader->readPrefix();\n+        }\n+\n+        void readSuffixImpl() override\n+        {\n+            reader->readSuffix();\n+        }\n+\n+    private:\n+        String name;\n+        std::unique_ptr<ReadBufferFromS3> read_buf;\n+        BlockInputStreamPtr reader;\n+    };\n+\n+    class StorageS3BlockOutputStream : public IBlockOutputStream\n+    {\n+    public:\n+        StorageS3BlockOutputStream(const Poco::URI & uri,\n+            const String & format,\n+            const Block & sample_block_,\n+            const Context & context,\n+            const ConnectionTimeouts & timeouts)\n+            : sample_block(sample_block_)\n+        {\n+            auto minimum_upload_part_size = context.getConfigRef().getUInt64(\"s3_minimum_upload_part_size\", 512'000'000);\n+            write_buf = std::make_unique<WriteBufferFromS3>(uri, minimum_upload_part_size, timeouts);\n+            writer = FormatFactory::instance().getOutput(format, *write_buf, sample_block, context);\n+        }\n+\n+        Block getHeader() const override\n+        {\n+            return sample_block;\n+        }\n+\n+        void write(const Block & block) override\n+        {\n+            writer->write(block);\n+        }\n+\n+        void writePrefix() override\n+        {\n+            writer->writePrefix();\n+        }\n+\n+        void writeSuffix() override\n+        {\n+            writer->writeSuffix();\n+            writer->flush();\n+            write_buf->finalize();\n+        }\n+\n+    private:\n+        Block sample_block;\n+        std::unique_ptr<WriteBufferFromS3> write_buf;\n+        BlockOutputStreamPtr writer;\n+    };\n+}\n+\n+\n+BlockInputStreams StorageS3::read(const Names & column_names,\n+    const SelectQueryInfo & /*query_info*/,\n+    const Context & context,\n+    QueryProcessingStage::Enum /*processed_stage*/,\n+    size_t max_block_size,\n+    unsigned /*num_streams*/)\n+{\n+    BlockInputStreamPtr block_input = std::make_shared<StorageS3BlockInputStream>(uri,\n+        format_name,\n+        getName(),\n+        getHeaderBlock(column_names),\n+        context,\n+        max_block_size,\n+        ConnectionTimeouts::getHTTPTimeouts(context));\n+\n+    auto column_defaults = getColumns().getDefaults();\n+    if (column_defaults.empty())\n+        return {block_input};\n+    return {std::make_shared<AddingDefaultsBlockInputStream>(block_input, column_defaults, context)};\n+}\n+\n+void StorageS3::rename(const String & /*new_path_to_db*/, const String & new_database_name, const String & new_table_name, TableStructureWriteLockHolder &)\n+{\n+    table_name = new_table_name;\n+    database_name = new_database_name;\n+}\n+\n+BlockOutputStreamPtr StorageS3::write(const ASTPtr & /*query*/, const Context & /*context*/)\n+{\n+    return std::make_shared<StorageS3BlockOutputStream>(\n+        uri, format_name, getSampleBlock(), context_global, ConnectionTimeouts::getHTTPTimeouts(context_global));\n+}\n+\n+void registerStorageS3(StorageFactory & factory)\n+{\n+    factory.registerStorage(\"S3\", [](const StorageFactory::Arguments & args)\n+    {\n+        ASTs & engine_args = args.engine_args;\n+\n+        if (!(engine_args.size() == 1 || engine_args.size() == 2))\n+            throw Exception(\n+                \"Storage S3 requires exactly 2 arguments: url and name of used format.\", ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);\n+\n+        engine_args[0] = evaluateConstantExpressionOrIdentifierAsLiteral(engine_args[0], args.local_context);\n+\n+        String url = engine_args[0]->as<ASTLiteral &>().value.safeGet<String>();\n+        Poco::URI uri(url);\n+\n+        engine_args[1] = evaluateConstantExpressionOrIdentifierAsLiteral(engine_args[1], args.local_context);\n+\n+        String format_name = engine_args[1]->as<ASTLiteral &>().value.safeGet<String>();\n+\n+        return StorageS3::create(uri, args.database_name, args.table_name, format_name, args.columns, args.context);\n+    });\n+}\n+}\ndiff --git a/dbms/src/Storages/StorageS3.h b/dbms/src/Storages/StorageS3.h\nnew file mode 100644\nindex 000000000000..ad073aaa14cb\n--- /dev/null\n+++ b/dbms/src/Storages/StorageS3.h\n@@ -0,0 +1,71 @@\n+#pragma once\n+\n+#include <Storages/IStorage.h>\n+#include <Poco/URI.h>\n+#include <common/logger_useful.h>\n+#include <ext/shared_ptr_helper.h>\n+\n+namespace DB\n+{\n+/**\n+ * This class represents table engine for external S3 urls.\n+ * It sends HTTP GET to server when select is called and\n+ * HTTP PUT when insert is called.\n+ */\n+class StorageS3 : public ext::shared_ptr_helper<StorageS3>, public IStorage\n+{\n+public:\n+    StorageS3(const Poco::URI & uri_,\n+        const std::string & database_name_,\n+        const std::string & table_name_,\n+        const String & format_name_,\n+        const ColumnsDescription & columns_,\n+        Context & context_\n+    )\n+        : IStorage(columns_)\n+        , uri(uri_)\n+        , context_global(context_)\n+        , format_name(format_name_)\n+        , database_name(database_name_)\n+        , table_name(table_name_)\n+    {\n+        setColumns(columns_);\n+    }\n+\n+    String getName() const override\n+    {\n+        return \"S3\";\n+    }\n+\n+    Block getHeaderBlock(const Names & /*column_names*/) const\n+    {\n+        return getSampleBlock();\n+    }\n+\n+    String getTableName() const override\n+    {\n+        return table_name;\n+    }\n+\n+    BlockInputStreams read(const Names & column_names,\n+        const SelectQueryInfo & query_info,\n+        const Context & context,\n+        QueryProcessingStage::Enum processed_stage,\n+        size_t max_block_size,\n+        unsigned num_streams) override;\n+\n+    BlockOutputStreamPtr write(const ASTPtr & query, const Context & context) override;\n+\n+    void rename(const String & new_path_to_db, const String & new_database_name, const String & new_table_name, TableStructureWriteLockHolder &) override;\n+\n+protected:\n+    Poco::URI uri;\n+    const Context & context_global;\n+\n+private:\n+    String format_name;\n+    String database_name;\n+    String table_name;\n+};\n+\n+}\ndiff --git a/dbms/src/Storages/registerStorages.cpp b/dbms/src/Storages/registerStorages.cpp\nindex c21156ea44db..4c29884dfcfc 100644\n--- a/dbms/src/Storages/registerStorages.cpp\n+++ b/dbms/src/Storages/registerStorages.cpp\n@@ -19,6 +19,7 @@ void registerStorageDistributed(StorageFactory & factory);\n void registerStorageMemory(StorageFactory & factory);\n void registerStorageFile(StorageFactory & factory);\n void registerStorageURL(StorageFactory & factory);\n+void registerStorageS3(StorageFactory & factory);\n void registerStorageDictionary(StorageFactory & factory);\n void registerStorageSet(StorageFactory & factory);\n void registerStorageJoin(StorageFactory & factory);\n@@ -60,6 +61,7 @@ void registerStorages()\n     registerStorageMemory(factory);\n     registerStorageFile(factory);\n     registerStorageURL(factory);\n+    registerStorageS3(factory);\n     registerStorageDictionary(factory);\n     registerStorageSet(factory);\n     registerStorageJoin(factory);\ndiff --git a/dbms/src/TableFunctions/TableFunctionS3.cpp b/dbms/src/TableFunctions/TableFunctionS3.cpp\nnew file mode 100644\nindex 000000000000..38ca0830e5b8\n--- /dev/null\n+++ b/dbms/src/TableFunctions/TableFunctionS3.cpp\n@@ -0,0 +1,19 @@\n+#include <Storages/StorageS3.h>\n+#include <TableFunctions/TableFunctionFactory.h>\n+#include <TableFunctions/TableFunctionS3.h>\n+#include <Poco/URI.h>\n+\n+namespace DB\n+{\n+StoragePtr TableFunctionS3::getStorage(\n+    const String & source, const String & format, const ColumnsDescription & columns, Context & global_context, const std::string & table_name) const\n+{\n+    Poco::URI uri(source);\n+    return StorageS3::create(uri, getDatabaseName(), table_name, format, columns, global_context);\n+}\n+\n+void registerTableFunctionS3(TableFunctionFactory & factory)\n+{\n+    factory.registerFunction<TableFunctionS3>();\n+}\n+}\ndiff --git a/dbms/src/TableFunctions/TableFunctionS3.h b/dbms/src/TableFunctions/TableFunctionS3.h\nnew file mode 100644\nindex 000000000000..a4966be13c79\n--- /dev/null\n+++ b/dbms/src/TableFunctions/TableFunctionS3.h\n@@ -0,0 +1,25 @@\n+#pragma once\n+\n+#include <TableFunctions/ITableFunctionFileLike.h>\n+#include <Interpreters/Context.h>\n+#include <Core/Block.h>\n+\n+\n+namespace DB\n+{\n+/* s3(source, format, structure) - creates a temporary storage for a file in S3\n+ */\n+class TableFunctionS3 : public ITableFunctionFileLike\n+{\n+public:\n+    static constexpr auto name = \"s3\";\n+    std::string getName() const override\n+    {\n+        return name;\n+    }\n+\n+private:\n+    StoragePtr getStorage(\n+        const String & source, const String & format, const ColumnsDescription & columns, Context & global_context, const std::string & table_name) const override;\n+};\n+}\ndiff --git a/dbms/src/TableFunctions/registerTableFunctions.cpp b/dbms/src/TableFunctions/registerTableFunctions.cpp\nindex 61d0ec23f7d7..aad5eebe9353 100644\n--- a/dbms/src/TableFunctions/registerTableFunctions.cpp\n+++ b/dbms/src/TableFunctions/registerTableFunctions.cpp\n@@ -11,6 +11,7 @@ void registerTableFunctionMerge(TableFunctionFactory & factory);\n void registerTableFunctionRemote(TableFunctionFactory & factory);\n void registerTableFunctionNumbers(TableFunctionFactory & factory);\n void registerTableFunctionFile(TableFunctionFactory & factory);\n+void registerTableFunctionS3(TableFunctionFactory & factory);\n void registerTableFunctionURL(TableFunctionFactory & factory);\n void registerTableFunctionValues(TableFunctionFactory & factory);\n \n@@ -37,6 +38,7 @@ void registerTableFunctions()\n     registerTableFunctionRemote(factory);\n     registerTableFunctionNumbers(factory);\n     registerTableFunctionFile(factory);\n+    registerTableFunctionS3(factory);\n     registerTableFunctionURL(factory);\n     registerTableFunctionValues(factory);\n \n",
  "test_patch": "diff --git a/dbms/tests/integration/helpers/cluster.py b/dbms/tests/integration/helpers/cluster.py\nindex aadd2e70a529..4131ee086532 100644\n--- a/dbms/tests/integration/helpers/cluster.py\n+++ b/dbms/tests/integration/helpers/cluster.py\n@@ -225,12 +225,12 @@ def _replace(self, path, what, to):\n     def restart_instance_with_ip_change(self, node, new_ip):\n         if '::' in new_ip:\n             if node.ipv6_address is None:\n-                raise Exception(\"You shoud specity ipv6_address in add_node method\")\n+                raise Exception(\"You should specity ipv6_address in add_node method\")\n             self._replace(node.docker_compose_path, node.ipv6_address, new_ip)\n             node.ipv6_address = new_ip\n         else:\n             if node.ipv4_address is None:\n-                raise Exception(\"You shoud specity ipv4_address in add_node method\")\n+                raise Exception(\"You should specity ipv4_address in add_node method\")\n             self._replace(node.docker_compose_path, node.ipv4_address, new_ip)\n             node.ipv4_address = new_ip\n         subprocess.check_call(self.base_cmd + [\"stop\", node.name])\ndiff --git a/dbms/tests/integration/runner b/dbms/tests/integration/runner\nindex 0d0ec929b960..071df8b1fd02 100755\n--- a/dbms/tests/integration/runner\n+++ b/dbms/tests/integration/runner\n@@ -107,4 +107,4 @@ if __name__ == \"__main__\":\n     )\n \n     #print(cmd)\n-    subprocess.check_call(cmd, shell=True)\n\\ No newline at end of file\n+    subprocess.check_call(cmd, shell=True)\ndiff --git a/dbms/tests/integration/test_storage_s3/__init__.py b/dbms/tests/integration/test_storage_s3/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/dbms/tests/integration/test_storage_s3/configs/min_chunk_size.xml b/dbms/tests/integration/test_storage_s3/configs/min_chunk_size.xml\nnew file mode 100644\nindex 000000000000..f61fcd2c5c91\n--- /dev/null\n+++ b/dbms/tests/integration/test_storage_s3/configs/min_chunk_size.xml\n@@ -0,0 +1,3 @@\n+<yandex>\n+    <s3_minimum_upload_part_size>1000000</s3_minimum_upload_part_size>\n+</yandex>\ndiff --git a/dbms/tests/integration/test_storage_s3/test.py b/dbms/tests/integration/test_storage_s3/test.py\nnew file mode 100644\nindex 000000000000..84f6bf72f60f\n--- /dev/null\n+++ b/dbms/tests/integration/test_storage_s3/test.py\n@@ -0,0 +1,159 @@\n+import httplib\n+import json\n+import logging\n+import os\n+import time\n+import traceback\n+\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+\n+logging.getLogger().setLevel(logging.INFO)\n+logging.getLogger().addHandler(logging.StreamHandler())\n+\n+\n+def get_communication_data(started_cluster):\n+    conn = httplib.HTTPConnection(started_cluster.instances[\"dummy\"].ip_address, started_cluster.communication_port)\n+    conn.request(\"GET\", \"/\")\n+    r = conn.getresponse()\n+    raw_data = r.read()\n+    conn.close()\n+    return json.loads(raw_data)\n+\n+\n+def put_communication_data(started_cluster, body):\n+    conn = httplib.HTTPConnection(started_cluster.instances[\"dummy\"].ip_address, started_cluster.communication_port)\n+    conn.request(\"PUT\", \"/\", body)\n+    r = conn.getresponse()\n+    conn.close()\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster = ClickHouseCluster(__file__)\n+        instance = cluster.add_instance(\"dummy\", config_dir=\"configs\", main_configs=[\"configs/min_chunk_size.xml\"])\n+        cluster.start()\n+\n+        cluster.communication_port = 10000\n+        instance.copy_file_to_container(os.path.join(os.path.dirname(__file__), \"test_server.py\"), \"test_server.py\")\n+        cluster.bucket = \"abc\"\n+        instance.exec_in_container([\"python\", \"test_server.py\", str(cluster.communication_port), cluster.bucket], detach=True)\n+        cluster.mock_host = instance.ip_address\n+\n+        for i in range(10):\n+            try:\n+                data = get_communication_data(cluster)\n+                cluster.redirecting_to_http_port = data[\"redirecting_to_http_port\"]\n+                cluster.preserving_data_port = data[\"preserving_data_port\"]\n+                cluster.multipart_preserving_data_port = data[\"multipart_preserving_data_port\"]\n+                cluster.redirecting_preserving_data_port = data[\"redirecting_preserving_data_port\"]\n+            except:\n+                logging.error(traceback.format_exc())\n+                time.sleep(0.5)\n+            else:\n+                break\n+        else:\n+            assert False, \"Could not initialize mock server\"\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def run_query(instance, query, stdin=None):\n+    logging.info(\"Running query '{}'...\".format(query))\n+    result = instance.query(query, stdin=stdin)\n+    logging.info(\"Query finished\")\n+    return result\n+\n+\n+def test_get_with_redirect(started_cluster):\n+    instance = started_cluster.instances[\"dummy\"]\n+    format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+\n+    put_communication_data(started_cluster, \"=== Get with redirect test ===\")\n+    query = \"select *, column1*column2*column3 from s3('http://{}:{}/', 'CSV', '{}')\".format(started_cluster.mock_host, started_cluster.redirecting_to_http_port, format)\n+    stdout = run_query(instance, query)\n+    data = get_communication_data(started_cluster)\n+    expected = [ [str(row[0]), str(row[1]), str(row[2]), str(row[0]*row[1]*row[2])] for row in data[\"redirect_csv_data\"] ]\n+    assert list(map(str.split, stdout.splitlines())) == expected\n+    \n+\n+def test_put(started_cluster):\n+    instance = started_cluster.instances[\"dummy\"]\n+    format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+\n+    logging.info(\"Phase 3\")\n+    put_communication_data(started_cluster, \"=== Put test ===\")\n+    values = \"(1, 2, 3), (3, 2, 1), (78, 43, 45)\"\n+    put_query = \"insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') values {}\".format(started_cluster.mock_host, started_cluster.preserving_data_port, started_cluster.bucket, format, values)\n+    run_query(instance, put_query)\n+    data = get_communication_data(started_cluster)\n+    received_data_completed = data[\"received_data_completed\"]\n+    received_data = data[\"received_data\"]\n+    finalize_data = data[\"finalize_data\"]\n+    finalize_data_query = data[\"finalize_data_query\"]\n+    assert received_data[-1].decode() == \"1,2,3\\n3,2,1\\n78,43,45\\n\"\n+    assert received_data_completed\n+    assert finalize_data == \"<CompleteMultipartUpload><Part><PartNumber>1</PartNumber><ETag>hello-etag</ETag></Part></CompleteMultipartUpload>\"\n+    assert finalize_data_query == \"uploadId=TEST\"\n+\n+    \n+def test_put_csv(started_cluster):\n+    instance = started_cluster.instances[\"dummy\"]\n+    format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+\n+    put_communication_data(started_cluster, \"=== Put test CSV ===\")\n+    put_query = \"insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') format CSV\".format(started_cluster.mock_host, started_cluster.preserving_data_port, started_cluster.bucket, format)\n+    csv_data = \"8,9,16\\n11,18,13\\n22,14,2\\n\"\n+    run_query(instance, put_query, stdin=csv_data)\n+    data = get_communication_data(started_cluster)\n+    received_data_completed = data[\"received_data_completed\"]\n+    received_data = data[\"received_data\"]\n+    finalize_data = data[\"finalize_data\"]\n+    finalize_data_query = data[\"finalize_data_query\"]\n+    assert received_data[-1].decode() == csv_data\n+    assert received_data_completed\n+    assert finalize_data == \"<CompleteMultipartUpload><Part><PartNumber>1</PartNumber><ETag>hello-etag</ETag></Part></CompleteMultipartUpload>\"\n+    assert finalize_data_query == \"uploadId=TEST\"\n+\n+    \n+def test_put_with_redirect(started_cluster):\n+    instance = started_cluster.instances[\"dummy\"]\n+    format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+\n+    put_communication_data(started_cluster, \"=== Put with redirect test ===\")\n+    other_values = \"(1, 1, 1), (1, 1, 1), (11, 11, 11)\"\n+    query = \"insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') values {}\".format(started_cluster.mock_host, started_cluster.redirecting_preserving_data_port, started_cluster.bucket, format, other_values)\n+    run_query(instance, query)\n+\n+    query = \"select *, column1*column2*column3 from s3('http://{}:{}/{}/test.csv', 'CSV', '{}')\".format(started_cluster.mock_host, started_cluster.preserving_data_port, started_cluster.bucket, format)\n+    stdout = run_query(instance, query)\n+    assert list(map(str.split, stdout.splitlines())) == [\n+        [\"1\", \"1\", \"1\", \"1\"],\n+        [\"1\", \"1\", \"1\", \"1\"],\n+        [\"11\", \"11\", \"11\", \"1331\"],\n+    ]\n+    data = get_communication_data(started_cluster)\n+    received_data = data[\"received_data\"]\n+    assert received_data[-1].decode() == \"1,1,1\\n1,1,1\\n11,11,11\\n\"\n+\n+\n+def test_multipart_put(started_cluster):\n+    instance = started_cluster.instances[\"dummy\"]\n+    format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+\n+    put_communication_data(started_cluster, \"=== Multipart test ===\")\n+    long_data = [[i, i+1, i+2] for i in range(100000)]\n+    long_values = \"\".join([ \"{},{},{}\\n\".format(x,y,z) for x, y, z in long_data ])\n+    put_query = \"insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') format CSV\".format(started_cluster.mock_host, started_cluster.multipart_preserving_data_port, started_cluster.bucket, format)\n+    run_query(instance, put_query, stdin=long_values)\n+    data = get_communication_data(started_cluster)\n+    assert \"multipart_received_data\" in data\n+    received_data = data[\"multipart_received_data\"]\n+    assert received_data[-1].decode() == \"\".join([ \"{},{},{}\\n\".format(x, y, z) for x, y, z in long_data ])\n+    assert 1 < data[\"multipart_parts\"] < 10000\ndiff --git a/dbms/tests/integration/test_storage_s3/test_server.py b/dbms/tests/integration/test_storage_s3/test_server.py\nnew file mode 100644\nindex 000000000000..8896af9c23e7\n--- /dev/null\n+++ b/dbms/tests/integration/test_storage_s3/test_server.py\n@@ -0,0 +1,365 @@\n+try:\n+    from BaseHTTPServer import BaseHTTPRequestHandler\n+except ImportError:\n+    from http.server import BaseHTTPRequestHandler\n+\n+try:\n+    from BaseHTTPServer import HTTPServer\n+except ImportError:\n+    from http.server import HTTPServer\n+\n+try:\n+    import urllib.parse as urlparse\n+except ImportError:\n+    import urlparse\n+\n+import json\n+import logging\n+import os\n+import socket\n+import sys\n+import threading\n+import time\n+import uuid\n+import xml.etree.ElementTree\n+\n+\n+logging.getLogger().setLevel(logging.INFO)\n+file_handler = logging.FileHandler(\"/var/log/clickhouse-server/test-server.log\", \"a\", encoding=\"utf-8\")\n+file_handler.setFormatter(logging.Formatter(\"%(asctime)s %(message)s\"))\n+logging.getLogger().addHandler(file_handler)\n+logging.getLogger().addHandler(logging.StreamHandler())\n+\n+communication_port = int(sys.argv[1])\n+bucket = sys.argv[2]\n+\n+def GetFreeTCPPortsAndIP(n):\n+    result = []\n+    sockets = []\n+    for i in range(n):\n+        tcp = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        tcp.bind((socket.gethostname(), 0))\n+        addr, port = tcp.getsockname()\n+        result.append(port)\n+        sockets.append(tcp)\n+    [ s.close() for s in sockets ]\n+    return result, addr\n+\n+(\n+    redirecting_to_http_port,\n+    simple_server_port,\n+    preserving_data_port,\n+    multipart_preserving_data_port,\n+    redirecting_preserving_data_port\n+), localhost = GetFreeTCPPortsAndIP(5)\n+\n+data = {\n+    \"redirecting_to_http_port\": redirecting_to_http_port,\n+    \"preserving_data_port\": preserving_data_port,\n+    \"multipart_preserving_data_port\": multipart_preserving_data_port,\n+    \"redirecting_preserving_data_port\": redirecting_preserving_data_port,\n+}\n+\n+\n+class SimpleHTTPServerHandler(BaseHTTPRequestHandler):\n+    def do_GET(self):\n+        logging.info(\"GET {}\".format(self.path))\n+        if self.path == \"/milovidov/test.csv\":\n+             self.send_response(200)\n+             self.send_header(\"Content-type\", \"text/plain\")\n+             self.end_headers()\n+             data[\"redirect_csv_data\"] = [[42, 87, 44], [55, 33, 81], [1, 0, 9]]\n+             self.wfile.write(\"\".join([ \"{},{},{}\\n\".format(*row) for row in data[\"redirect_csv_data\"]]))\n+        else:\n+             self.send_response(404)\n+             self.end_headers()\n+        self.finish()\n+\n+\n+class RedirectingToHTTPHandler(BaseHTTPRequestHandler):\n+    def do_GET(self):\n+        self.send_response(307)\n+        self.send_header(\"Content-type\", \"text/xml\")\n+        self.send_header(\"Location\", \"http://{}:{}/milovidov/test.csv\".format(localhost, simple_server_port))\n+        self.end_headers()\n+        self.wfile.write(r\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<Error>\n+  <Code>TemporaryRedirect</Code>\n+  <Message>Please re-send this request to the specified temporary endpoint.\n+  Continue to use the original request endpoint for future requests.</Message>\n+  <Endpoint>storage.yandexcloud.net</Endpoint>\n+</Error>\"\"\".encode())\n+        self.finish()\n+\n+\n+class PreservingDataHandler(BaseHTTPRequestHandler):\n+    protocol_version = \"HTTP/1.1\"\n+\n+    def parse_request(self):\n+        result = BaseHTTPRequestHandler.parse_request(self)\n+        # Adaptation to Python 3.\n+        if sys.version_info.major == 2 and result == True:\n+            expect = self.headers.get(\"Expect\", \"\")\n+            if (expect.lower() == \"100-continue\" and self.protocol_version >= \"HTTP/1.1\" and self.request_version >= \"HTTP/1.1\"):\n+                if not self.handle_expect_100():\n+                    return False\n+        return result\n+\n+    def send_response_only(self, code, message=None):\n+        if message is None:\n+            if code in self.responses:\n+                message = self.responses[code][0]\n+            else:\n+                message = \"\"\n+        if self.request_version != \"HTTP/0.9\":\n+            self.wfile.write(\"%s %d %s\\r\\n\" % (self.protocol_version, code, message))\n+\n+    def handle_expect_100(self):\n+        logging.info(\"Received Expect-100\")\n+        self.send_response_only(100)\n+        self.end_headers()\n+        return True\n+\n+    def do_POST(self):\n+        self.send_response(200)\n+        query = urlparse.urlparse(self.path).query\n+        logging.info(\"PreservingDataHandler POST ?\" + query)\n+        if query == \"uploads\":\n+            post_data = r\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<hi><UploadId>TEST</UploadId></hi>\"\"\".encode()\n+            self.send_header(\"Content-length\", str(len(post_data)))\n+            self.send_header(\"Content-type\", \"text/plain\")\n+            self.end_headers()\n+            self.wfile.write(post_data)\n+        else:\n+            post_data = self.rfile.read(int(self.headers.get(\"Content-Length\")))\n+            self.send_header(\"Content-type\", \"text/plain\")\n+            self.end_headers()\n+            data[\"received_data_completed\"] = True\n+            data[\"finalize_data\"] = post_data\n+            data[\"finalize_data_query\"] = query\n+        self.finish()\n+ \n+    def do_PUT(self):\n+        self.send_response(200)\n+        self.send_header(\"Content-type\", \"text/plain\")\n+        self.send_header(\"ETag\", \"hello-etag\")\n+        self.end_headers()\n+        query = urlparse.urlparse(self.path).query\n+        path = urlparse.urlparse(self.path).path\n+        logging.info(\"Content-Length = \" + self.headers.get(\"Content-Length\"))\n+        logging.info(\"PUT \" + query)\n+        assert self.headers.get(\"Content-Length\")\n+        assert self.headers[\"Expect\"] == \"100-continue\"\n+        put_data = self.rfile.read()\n+        data.setdefault(\"received_data\", []).append(put_data)\n+        logging.info(\"PUT to {}\".format(path))\n+        self.server.storage[path] = put_data\n+        self.finish()\n+\n+    def do_GET(self):\n+        path = urlparse.urlparse(self.path).path\n+        if path in self.server.storage:\n+            self.send_response(200)\n+            self.send_header(\"Content-type\", \"text/plain\")\n+            self.send_header(\"Content-length\", str(len(self.server.storage[path])))\n+            self.end_headers()\n+            self.wfile.write(self.server.storage[path])\n+        else:\n+            self.send_response(404)\n+            self.end_headers()\n+        self.finish()\n+\n+\n+class MultipartPreservingDataHandler(BaseHTTPRequestHandler):\n+    protocol_version = \"HTTP/1.1\"\n+\n+    def parse_request(self):\n+        result = BaseHTTPRequestHandler.parse_request(self)\n+        # Adaptation to Python 3.\n+        if sys.version_info.major == 2 and result == True:\n+            expect = self.headers.get(\"Expect\", \"\")\n+            if (expect.lower() == \"100-continue\" and self.protocol_version >= \"HTTP/1.1\" and self.request_version >= \"HTTP/1.1\"):\n+                if not self.handle_expect_100():\n+                    return False\n+        return result\n+\n+    def send_response_only(self, code, message=None):\n+        if message is None:\n+            if code in self.responses:\n+                message = self.responses[code][0]\n+            else:\n+                message = \"\"\n+        if self.request_version != \"HTTP/0.9\":\n+            self.wfile.write(\"%s %d %s\\r\\n\" % (self.protocol_version, code, message))\n+\n+    def handle_expect_100(self):\n+        logging.info(\"Received Expect-100\")\n+        self.send_response_only(100)\n+        self.end_headers()\n+        return True\n+\n+    def do_POST(self):\n+        query = urlparse.urlparse(self.path).query\n+        logging.info(\"MultipartPreservingDataHandler POST ?\" + query)\n+        if query == \"uploads\":\n+            self.send_response(200)\n+            post_data = r\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<hi><UploadId>TEST</UploadId></hi>\"\"\".encode()\n+            self.send_header(\"Content-length\", str(len(post_data)))\n+            self.send_header(\"Content-type\", \"text/plain\")\n+            self.end_headers()\n+            self.wfile.write(post_data)\n+        else:\n+            try:\n+                assert query == \"uploadId=TEST\"\n+                logging.info(\"Content-Length = \" + self.headers.get(\"Content-Length\"))\n+                post_data = self.rfile.read(int(self.headers.get(\"Content-Length\")))\n+                root = xml.etree.ElementTree.fromstring(post_data)\n+                assert root.tag == \"CompleteMultipartUpload\"\n+                assert len(root) > 1\n+                content = \"\"\n+                for i, part in enumerate(root):\n+                    assert part.tag == \"Part\"\n+                    assert len(part) == 2\n+                    assert part[0].tag == \"PartNumber\"\n+                    assert part[1].tag == \"ETag\"\n+                    assert int(part[0].text) == i + 1\n+                    content += self.server.storage[\"@\"+part[1].text]\n+                data.setdefault(\"multipart_received_data\", []).append(content)\n+                data[\"multipart_parts\"] = len(root)\n+                self.send_response(200)\n+                self.send_header(\"Content-type\", \"text/plain\")\n+                self.end_headers()\n+                logging.info(\"Sending 200\")\n+            except:\n+                logging.error(\"Sending 500\")\n+                self.send_response(500)\n+        self.finish()\n+ \n+    def do_PUT(self):\n+        uid = uuid.uuid4()\n+        self.send_response(200)\n+        self.send_header(\"Content-type\", \"text/plain\")\n+        self.send_header(\"ETag\", str(uid))\n+        self.end_headers()\n+        query = urlparse.urlparse(self.path).query\n+        path = urlparse.urlparse(self.path).path\n+        logging.info(\"Content-Length = \" + self.headers.get(\"Content-Length\"))\n+        logging.info(\"PUT \" + query)\n+        assert self.headers.get(\"Content-Length\")\n+        assert self.headers[\"Expect\"] == \"100-continue\"\n+        put_data = self.rfile.read()\n+        data.setdefault(\"received_data\", []).append(put_data)\n+        logging.info(\"PUT to {}\".format(path))\n+        self.server.storage[\"@\"+str(uid)] = put_data\n+        self.finish()\n+\n+    def do_GET(self):\n+        path = urlparse.urlparse(self.path).path\n+        if path in self.server.storage:\n+            self.send_response(200)\n+            self.send_header(\"Content-type\", \"text/plain\")\n+            self.send_header(\"Content-length\", str(len(self.server.storage[path])))\n+            self.end_headers()\n+            self.wfile.write(self.server.storage[path])\n+        else:\n+            self.send_response(404)\n+            self.end_headers()\n+        self.finish()\n+\n+\n+class RedirectingPreservingDataHandler(BaseHTTPRequestHandler):\n+    protocol_version = \"HTTP/1.1\"\n+\n+    def parse_request(self):\n+        result = BaseHTTPRequestHandler.parse_request(self)\n+        # Adaptation to Python 3.\n+        if sys.version_info.major == 2 and result == True:\n+            expect = self.headers.get(\"Expect\", \"\")\n+            if (expect.lower() == \"100-continue\" and self.protocol_version >= \"HTTP/1.1\" and self.request_version >= \"HTTP/1.1\"):\n+                if not self.handle_expect_100():\n+                    return False\n+        return result\n+\n+    def send_response_only(self, code, message=None):\n+        if message is None:\n+            if code in self.responses:\n+                message = self.responses[code][0]\n+            else:\n+                message = \"\"\n+        if self.request_version != \"HTTP/0.9\":\n+            self.wfile.write(\"%s %d %s\\r\\n\" % (self.protocol_version, code, message))\n+\n+    def handle_expect_100(self):\n+        logging.info(\"Received Expect-100\")\n+        return True\n+\n+    def do_POST(self):\n+        query = urlparse.urlparse(self.path).query\n+        if query:\n+            query = \"?{}\".format(query)\n+        self.send_response(307)\n+        self.send_header(\"Content-type\", \"text/xml\")\n+        self.send_header(\"Location\", \"http://{host}:{port}/{bucket}/test.csv{query}\".format(host=localhost, port=preserving_data_port, bucket=bucket, query=query))\n+        self.end_headers()\n+        self.wfile.write(r\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<Error>\n+  <Code>TemporaryRedirect</Code>\n+  <Message>Please re-send this request to the specified temporary endpoint.\n+  Continue to use the original request endpoint for future requests.</Message>\n+  <Endpoint>{host}:{port}</Endpoint>\n+</Error>\"\"\".format(host=localhost, port=preserving_data_port).encode())\n+        self.finish()\n+\n+    def do_PUT(self):\n+        query = urlparse.urlparse(self.path).query\n+        if query:\n+            query = \"?{}\".format(query)\n+        self.send_response(307)\n+        self.send_header(\"Content-type\", \"text/xml\")\n+        self.send_header(\"Location\", \"http://{host}:{port}/{bucket}/test.csv{query}\".format(host=localhost, port=preserving_data_port, bucket=bucket, query=query))\n+        self.end_headers()\n+        self.wfile.write(r\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<Error>\n+  <Code>TemporaryRedirect</Code>\n+  <Message>Please re-send this request to the specified temporary endpoint.\n+  Continue to use the original request endpoint for future requests.</Message>\n+  <Endpoint>{host}:{port}</Endpoint>\n+</Error>\"\"\".format(host=localhost, port=preserving_data_port).encode())\n+        self.finish()\n+\n+\n+class CommunicationServerHandler(BaseHTTPRequestHandler):\n+    def do_GET(self):\n+        self.send_response(200)\n+        self.end_headers()\n+        self.wfile.write(json.dumps(data))\n+        self.finish()\n+\n+    def do_PUT(self):\n+        self.send_response(200)\n+        self.end_headers()\n+        logging.info(self.rfile.read())\n+        self.finish()\n+\n+\n+servers = []\n+servers.append(HTTPServer((localhost, communication_port), CommunicationServerHandler))\n+servers.append(HTTPServer((localhost, redirecting_to_http_port), RedirectingToHTTPHandler))\n+servers.append(HTTPServer((localhost, preserving_data_port), PreservingDataHandler))\n+servers[-1].storage = {}\n+servers.append(HTTPServer((localhost, multipart_preserving_data_port), MultipartPreservingDataHandler))\n+servers[-1].storage = {}\n+servers.append(HTTPServer((localhost, simple_server_port), SimpleHTTPServerHandler))\n+servers.append(HTTPServer((localhost, redirecting_preserving_data_port), RedirectingPreservingDataHandler))\n+jobs = [ threading.Thread(target=server.serve_forever) for server in servers ]\n+[ job.start() for job in jobs ]\n+\n+time.sleep(60) # Timeout\n+\n+logging.info(\"Shutting down\")\n+[ server.shutdown() for server in servers ]\n+logging.info(\"Joining threads\")\n+[ job.join() for job in jobs ]\n+logging.info(\"Done\")\n",
  "problem_statement": "Implement passing auth data in requests to S3\n\n",
  "hints_text": "",
  "created_at": "2019-09-20T10:39:17Z"
}