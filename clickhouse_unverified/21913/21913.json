{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 21913,
  "instance_id": "ClickHouse__ClickHouse-21913",
  "issue_numbers": [
    "21388"
  ],
  "base_commit": "61dee8c05660531dfc16fed3d2dff63f354852df",
  "patch": "diff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp\nindex d1af86e7b11c..f8bcbf02ab48 100644\n--- a/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -260,7 +260,8 @@ BlockIO InterpreterCreateQuery::createDatabase(ASTCreateQuery & create)\n             renamed = true;\n         }\n \n-        database->loadStoredObjects(context, has_force_restore_data_flag, create.attach && force_attach);\n+        /// We use global context here, because storages lifetime is bigger than query context lifetime\n+        database->loadStoredObjects(context.getGlobalContext(), has_force_restore_data_flag, create.attach && force_attach);\n     }\n     catch (...)\n     {\n@@ -970,7 +971,8 @@ bool InterpreterCreateQuery::doCreateTable(ASTCreateQuery & create,\n     if (create.as_table_function)\n     {\n         const auto & factory = TableFunctionFactory::instance();\n-        res = factory.get(create.as_table_function, context)->execute(create.as_table_function, context, create.table, properties.columns);\n+        auto table_func = factory.get(create.as_table_function, context);\n+        res = table_func->execute(create.as_table_function, context, create.table, properties.columns);\n         res->renameInMemory({create.database, create.table, create.uuid});\n     }\n     else\ndiff --git a/src/Storages/StorageFactory.cpp b/src/Storages/StorageFactory.cpp\nindex 85f3bea9e0c8..7aaec9b7e764 100644\n--- a/src/Storages/StorageFactory.cpp\n+++ b/src/Storages/StorageFactory.cpp\n@@ -179,6 +179,7 @@ StoragePtr StorageFactory::get(\n         .attach = query.attach,\n         .has_force_restore_data_flag = has_force_restore_data_flag\n     };\n+    assert(&arguments.context == &arguments.context.getGlobalContext());\n \n     auto res = storages.at(name).creator_fn(arguments);\n     if (!empty_engine_args.empty())\ndiff --git a/src/Storages/StorageURL.cpp b/src/Storages/StorageURL.cpp\nindex ca984f9ece9e..2d3879340dcd 100644\n--- a/src/Storages/StorageURL.cpp\n+++ b/src/Storages/StorageURL.cpp\n@@ -33,7 +33,7 @@ namespace ErrorCodes\n \n IStorageURLBase::IStorageURLBase(\n     const Poco::URI & uri_,\n-    const Context & context_,\n+    const Context & /*context_*/,\n     const StorageID & table_id_,\n     const String & format_name_,\n     const std::optional<FormatSettings> & format_settings_,\n@@ -42,13 +42,10 @@ IStorageURLBase::IStorageURLBase(\n     const String & compression_method_)\n     : IStorage(table_id_)\n     , uri(uri_)\n-    , context_global(context_)\n     , compression_method(compression_method_)\n     , format_name(format_name_)\n     , format_settings(format_settings_)\n {\n-    context_global.getRemoteHostFilter().checkURL(uri);\n-\n     StorageInMemoryMetadata storage_metadata;\n     storage_metadata.setColumns(columns_);\n     storage_metadata.setConstraints(constraints_);\n@@ -237,14 +234,28 @@ Pipe IStorageURLBase::read(\n         chooseCompressionMethod(request_uri.getPath(), compression_method)));\n }\n \n-BlockOutputStreamPtr IStorageURLBase::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & /*context*/)\n+BlockOutputStreamPtr IStorageURLBase::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & context)\n {\n     return std::make_shared<StorageURLBlockOutputStream>(uri, format_name,\n-        format_settings, metadata_snapshot->getSampleBlock(), context_global,\n-        ConnectionTimeouts::getHTTPTimeouts(context_global),\n+        format_settings, metadata_snapshot->getSampleBlock(), context,\n+        ConnectionTimeouts::getHTTPTimeouts(context),\n         chooseCompressionMethod(uri.toString(), compression_method));\n }\n \n+StorageURL::StorageURL(const Poco::URI & uri_,\n+           const StorageID & table_id_,\n+           const String & format_name_,\n+           const std::optional<FormatSettings> & format_settings_,\n+           const ColumnsDescription & columns_,\n+           const ConstraintsDescription & constraints_,\n+           Context & context_,\n+           const String & compression_method_)\n+    : IStorageURLBase(uri_, context_, table_id_, format_name_,\n+                      format_settings_, columns_, constraints_, compression_method_)\n+{\n+    context_.getRemoteHostFilter().checkURL(uri);\n+}\n+\n void registerStorageURL(StorageFactory & factory)\n {\n     factory.registerStorage(\"URL\", [](const StorageFactory::Arguments & args)\ndiff --git a/src/Storages/StorageURL.h b/src/Storages/StorageURL.h\nindex 21b2e3e27a16..2b2384b1043b 100644\n--- a/src/Storages/StorageURL.h\n+++ b/src/Storages/StorageURL.h\n@@ -45,7 +45,6 @@ class IStorageURLBase : public IStorage\n         const String & compression_method_);\n \n     Poco::URI uri;\n-    const Context & context_global;\n     String compression_method;\n     String format_name;\n     // For URL engine, we use format settings from server context + `SETTINGS`\n@@ -114,11 +113,7 @@ class StorageURL final : public ext::shared_ptr_helper<StorageURL>, public IStor\n             const ColumnsDescription & columns_,\n             const ConstraintsDescription & constraints_,\n             Context & context_,\n-            const String & compression_method_)\n-        : IStorageURLBase(uri_, context_, table_id_, format_name_,\n-            format_settings_, columns_, constraints_, compression_method_)\n-    {\n-    }\n+            const String & compression_method_);\n \n     String getName() const override\n     {\ndiff --git a/src/TableFunctions/ITableFunction.cpp b/src/TableFunctions/ITableFunction.cpp\nindex 804a5b232ecb..b637838c6dae 100644\n--- a/src/TableFunctions/ITableFunction.cpp\n+++ b/src/TableFunctions/ITableFunction.cpp\n@@ -20,12 +20,20 @@ StoragePtr ITableFunction::execute(const ASTPtr & ast_function, const Context &\n     ProfileEvents::increment(ProfileEvents::TableFunctionExecute);\n     context.checkAccess(AccessType::CREATE_TEMPORARY_TABLE | StorageFactory::instance().getSourceAccessType(getStorageTypeName()));\n \n-    if (cached_columns.empty() || (hasStaticStructure() && cached_columns == getActualTableStructure(context)))\n+    if (cached_columns.empty())\n         return executeImpl(ast_function, context, table_name, std::move(cached_columns));\n \n-    auto get_storage = [=, tf = shared_from_this()]() -> StoragePtr\n+    /// We have table structure, so it's CREATE AS table_function().\n+    /// We should use global context here because there will be no query context on server startup\n+    /// and because storage lifetime is bigger than query context lifetime.\n+    const Context & global_context = context.getGlobalContext();\n+    if (hasStaticStructure() && cached_columns == getActualTableStructure(context))\n+        return executeImpl(ast_function, global_context, table_name, std::move(cached_columns));\n+\n+    auto this_table_function = shared_from_this();\n+    auto get_storage = [=, &global_context]() -> StoragePtr\n     {\n-        return tf->executeImpl(ast_function, context, table_name, cached_columns);\n+        return this_table_function->executeImpl(ast_function, global_context, table_name, cached_columns);\n     };\n \n     /// It will request actual table structure and create underlying storage lazily\ndiff --git a/src/TableFunctions/ITableFunctionXDBC.cpp b/src/TableFunctions/ITableFunctionXDBC.cpp\nindex e04a86b5abf9..21c78d199dbf 100644\n--- a/src/TableFunctions/ITableFunctionXDBC.cpp\n+++ b/src/TableFunctions/ITableFunctionXDBC.cpp\n@@ -55,15 +55,21 @@ void ITableFunctionXDBC::parseArguments(const ASTPtr & ast_function, const Conte\n         connection_string = args[0]->as<ASTLiteral &>().value.safeGet<String>();\n         remote_table_name = args[1]->as<ASTLiteral &>().value.safeGet<String>();\n     }\n+}\n \n-    /// Have to const_cast, because bridges store their commands inside context\n-    helper = createBridgeHelper(const_cast<Context &>(context), context.getSettingsRef().http_receive_timeout.value, connection_string);\n-    helper->startBridgeSync();\n+void ITableFunctionXDBC::startBridgeIfNot(const Context & context) const\n+{\n+    if (!helper)\n+    {\n+        /// Have to const_cast, because bridges store their commands inside context\n+        helper = createBridgeHelper(const_cast<Context &>(context), context.getSettingsRef().http_receive_timeout.value, connection_string);\n+        helper->startBridgeSync();\n+    }\n }\n \n ColumnsDescription ITableFunctionXDBC::getActualTableStructure(const Context & context) const\n {\n-    assert(helper);\n+    startBridgeIfNot(context);\n \n     /* Infer external table structure */\n     Poco::URI columns_info_uri = helper->getColumnsInfoURI();\n@@ -87,7 +93,7 @@ ColumnsDescription ITableFunctionXDBC::getActualTableStructure(const Context & c\n \n StoragePtr ITableFunctionXDBC::executeImpl(const ASTPtr & /*ast_function*/, const Context & context, const std::string & table_name, ColumnsDescription /*cached_columns*/) const\n {\n-    assert(helper);\n+    startBridgeIfNot(context);\n     auto columns = getActualTableStructure(context);\n     auto result = std::make_shared<StorageXDBC>(StorageID(getDatabaseName(), table_name), schema_name, remote_table_name, columns, context, helper);\n     result->startup();\ndiff --git a/src/TableFunctions/ITableFunctionXDBC.h b/src/TableFunctions/ITableFunctionXDBC.h\nindex fb0a0fd1185a..f3ff64c2f2d3 100644\n--- a/src/TableFunctions/ITableFunctionXDBC.h\n+++ b/src/TableFunctions/ITableFunctionXDBC.h\n@@ -29,10 +29,12 @@ class ITableFunctionXDBC : public ITableFunction\n \n     void parseArguments(const ASTPtr & ast_function, const Context & context) override;\n \n+    void startBridgeIfNot(const Context & context) const;\n+\n     String connection_string;\n     String schema_name;\n     String remote_table_name;\n-    BridgeHelperPtr helper;\n+    mutable BridgeHelperPtr helper;\n };\n \n class TableFunctionJDBC : public ITableFunctionXDBC\n",
  "test_patch": "diff --git a/tests/integration/test_odbc_interaction/test.py b/tests/integration/test_odbc_interaction/test.py\nindex 6bb6a6ee7776..6232168f2e6e 100644\n--- a/tests/integration/test_odbc_interaction/test.py\n+++ b/tests/integration/test_odbc_interaction/test.py\n@@ -74,6 +74,9 @@ def started_cluster():\n         node1.exec_in_container(\n             [\"bash\", \"-c\", \"echo 'CREATE TABLE t4(X INTEGER PRIMARY KEY ASC, Y, Z);' | sqlite3 {}\".format(sqlite_db)],\n             privileged=True, user='root')\n+        node1.exec_in_container(\n+            [\"bash\", \"-c\", \"echo 'CREATE TABLE tf1(x INTEGER PRIMARY KEY ASC, y, z);' | sqlite3 {}\".format(sqlite_db)],\n+            privileged=True, user='root')\n         print(\"sqlite tables created\")\n         mysql_conn = get_mysql_conn()\n         print(\"mysql connection received\")\n@@ -177,6 +180,21 @@ def test_sqlite_simple_select_function_works(started_cluster):\n     assert node1.query(\n         \"select count(), sum(x) from odbc('DSN={}', '{}') group by x\".format(sqlite_setup[\"DSN\"], 't1')) == \"1\\t1\\n\"\n \n+def test_sqlite_table_function(started_cluster):\n+    sqlite_setup = node1.odbc_drivers[\"SQLite3\"]\n+    sqlite_db = sqlite_setup[\"Database\"]\n+\n+    node1.exec_in_container([\"bash\", \"-c\", \"echo 'INSERT INTO tf1 values(1, 2, 3);' | sqlite3 {}\".format(sqlite_db)],\n+                            privileged=True, user='root')\n+    node1.query(\"create table odbc_tf as odbc('DSN={}', '{}')\".format(sqlite_setup[\"DSN\"], 'tf1'))\n+    assert node1.query(\"select * from odbc_tf\") == \"1\\t2\\t3\\n\"\n+\n+    assert node1.query(\"select y from odbc_tf\") == \"2\\n\"\n+    assert node1.query(\"select z from odbc_tf\") == \"3\\n\"\n+    assert node1.query(\"select x from odbc_tf\") == \"1\\n\"\n+    assert node1.query(\"select x, y from odbc_tf\") == \"1\\t2\\n\"\n+    assert node1.query(\"select z, x, y from odbc_tf\") == \"3\\t1\\t2\\n\"\n+    assert node1.query(\"select count(), sum(x) from odbc_tf group by x\") == \"1\\t1\\n\"\n \n def test_sqlite_simple_select_storage_works(started_cluster):\n     sqlite_setup = node1.odbc_drivers[\"SQLite3\"]\n",
  "problem_statement": "21.2.3: range_hashed dictionary with ClickHouse source over a VIEW with odbc/jdbc function - possible dangling reference to Context\nThis issue happens on production of one user that enabled Sentry:\r\n\r\n![Screenshot_20210303_002028](https://user-images.githubusercontent.com/18581488/109716945-7f9a1200-7bb6-11eb-8cf4-1080ad535d18.png)\r\n\r\nBuild id: 49938CCB63EE2A4E16CF0528307EE15F08469219.\n",
  "hints_text": "Need to construct a reproducer.\nThe fix is probably just to obtain a global context in dictionaries.",
  "created_at": "2021-03-19T14:04:50Z"
}