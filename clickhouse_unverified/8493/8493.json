{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 8493,
  "instance_id": "ClickHouse__ClickHouse-8493",
  "issue_numbers": [
    "7262"
  ],
  "base_commit": "8140b2f75af50d80e54abfff6e3bc91fb057db92",
  "patch": "diff --git a/dbms/src/Interpreters/Cluster.cpp b/dbms/src/Interpreters/Cluster.cpp\nindex 2c75bd821fe8..aaa6b31ab36a 100644\n--- a/dbms/src/Interpreters/Cluster.cpp\n+++ b/dbms/src/Interpreters/Cluster.cpp\n@@ -10,6 +10,7 @@\n #include <IO/ReadHelpers.h>\n #include <Poco/Util/AbstractConfiguration.h>\n #include <Poco/Util/Application.h>\n+#include <ext/range.h>\n \n namespace DB\n {\n@@ -449,6 +450,10 @@ void Cluster::initMisc()\n     }\n }\n \n+std::unique_ptr<Cluster> Cluster::getClusterWithReplicasAsShards(const Settings & settings) const\n+{\n+    return std::unique_ptr<Cluster>{ new Cluster(settings, *this)};\n+}\n \n std::unique_ptr<Cluster> Cluster::getClusterWithSingleShard(size_t index) const\n {\n@@ -460,6 +465,55 @@ std::unique_ptr<Cluster> Cluster::getClusterWithMultipleShards(const std::vector\n     return std::unique_ptr<Cluster>{ new Cluster(*this, indices) };\n }\n \n+Cluster::Cluster(const Settings & settings, const Cluster & from) : shards_info{}, addresses_with_failover{}\n+{\n+    std::set<std::tuple<String, int>> hosts;\n+    if (!from.addresses_with_failover.empty())\n+    {\n+        for (size_t shard_index : ext::range(0, from.shards_info.size()))\n+        {\n+            const auto & replicas = from.addresses_with_failover[shard_index];\n+            for (size_t replica_index : ext::range(0, replicas.size()))\n+            {\n+                ShardInfo info;\n+                Address address = replicas[replica_index];\n+                auto position = find_if(hosts.begin(), hosts.end(), [=](auto item) {return std::get<0>(item) == address.host_name && std::get<1>(item) == address.port;});\n+                if (position == hosts.end())\n+                {\n+                    if (address.is_local)\n+                        info.local_addresses.push_back(replicas[replica_index]);\n+                    hosts.insert(std::tuple<String, int>(address.host_name, address.port));\n+                    ConnectionPoolPtr pool = std::make_shared<ConnectionPool>(\n+                        settings.distributed_connections_pool_size,\n+                        address.host_name,\n+                        address.port,\n+                        address.default_database,\n+                        address.user,\n+                        address.password,\n+                        \"server\",\n+                        address.compression,\n+                        address.secure);\n+\n+                    info.pool = std::make_shared<ConnectionPoolWithFailover>(ConnectionPoolPtrs{pool}, settings.load_balancing);\n+                    info.per_replica_pools = {std::move(pool)};\n+                    std ::vector<Cluster::Address> newAddress = {address};\n+                    addresses_with_failover.emplace_back(newAddress);\n+                    shards_info.emplace_back(std::move(info));\n+                }\n+                else\n+                {\n+                    continue;\n+                }\n+            }\n+        }\n+    }\n+    else\n+    {\n+        throw Exception(\"There must be either 'node' or 'shard' elements in the cluster\", ErrorCodes::EXCESSIVE_ELEMENT_IN_CONFIG);\n+    }\n+    initMisc();\n+}\n+\n Cluster::Cluster(const Cluster & from, const std::vector<size_t> & indices)\n     : shards_info{}\n {\ndiff --git a/dbms/src/Interpreters/Cluster.h b/dbms/src/Interpreters/Cluster.h\nindex e778c9bcf6f4..8b95b1ce986c 100644\n--- a/dbms/src/Interpreters/Cluster.h\n+++ b/dbms/src/Interpreters/Cluster.h\n@@ -26,7 +26,7 @@ class Cluster\n             const String & username, const String & password,\n             UInt16 clickhouse_port, bool treat_local_as_remote, bool secure = false);\n \n-    Cluster(const Cluster &) = delete;\n+    Cluster(const Cluster &)= delete;\n     Cluster & operator=(const Cluster &) = delete;\n \n     /// is used to set a limit on the size of the timeout\n@@ -148,6 +148,9 @@ class Cluster\n     /// Get a subcluster consisting of one or multiple shards - indexes by count (from 0) of the shard of this cluster.\n     std::unique_ptr<Cluster> getClusterWithMultipleShards(const std::vector<size_t> & indices) const;\n \n+    /// Get a new Cluster From the existing cluster\n+    std::unique_ptr<Cluster> getClusterWithReplicasAsShards(const Settings & settings) const;\n+\n private:\n     using SlotToShard = std::vector<UInt64>;\n     SlotToShard slot_to_shard;\n@@ -161,6 +164,9 @@ class Cluster\n     /// For getClusterWithMultipleShards implementation.\n     Cluster(const Cluster & from, const std::vector<size_t> & indices);\n \n+    /// For getClusterWithReplicasAsShards implementation\n+    Cluster(const Settings & settings, const Cluster &);\n+\n     String hash_of_addresses;\n     /// Description of the cluster shards.\n     ShardsInfo shards_info;\ndiff --git a/dbms/src/TableFunctions/TableFunctionRemote.cpp b/dbms/src/TableFunctions/TableFunctionRemote.cpp\nindex 87c8989cbe29..033839009bb4 100644\n--- a/dbms/src/TableFunctions/TableFunctionRemote.cpp\n+++ b/dbms/src/TableFunctions/TableFunctionRemote.cpp\n@@ -14,6 +14,7 @@\n #include <Common/parseRemoteDescription.h>\n #include <TableFunctions/TableFunctionFactory.h>\n #include <Core/Defines.h>\n+#include <ext/range.h>\n #include \"registerTableFunctions.h\"\n \n \n@@ -140,7 +141,10 @@ StoragePtr TableFunctionRemote::executeImpl(const ASTPtr & ast_function, const C\n     if (!cluster_name.empty())\n     {\n         /// Use an existing cluster from the main config\n-        cluster = context.getCluster(cluster_name);\n+        if (name != \"clusterAllReplicas\")\n+            cluster = context.getCluster(cluster_name);\n+        else\n+            cluster = context.getCluster(cluster_name)->getClusterWithReplicasAsShards(context.getSettings());\n     }\n     else\n     {\n@@ -164,13 +168,22 @@ StoragePtr TableFunctionRemote::executeImpl(const ASTPtr & ast_function, const C\n             {\n                 size_t colon = host.find(':');\n                 if (colon == String::npos)\n-                    context.getRemoteHostFilter().checkHostAndPort(host, toString((secure ? (maybe_secure_port ? *maybe_secure_port : DBMS_DEFAULT_SECURE_PORT) : context.getTCPPort())));\n+                    context.getRemoteHostFilter().checkHostAndPort(\n+                        host,\n+                        toString((secure ? (maybe_secure_port ? *maybe_secure_port : DBMS_DEFAULT_SECURE_PORT) : context.getTCPPort())));\n                 else\n                     context.getRemoteHostFilter().checkHostAndPort(host.substr(0, colon), host.substr(colon + 1));\n             }\n         }\n \n-        cluster = std::make_shared<Cluster>(context.getSettings(), names, username, password, (secure ? (maybe_secure_port ? *maybe_secure_port : DBMS_DEFAULT_SECURE_PORT) : context.getTCPPort()), false, secure);\n+        cluster = std::make_shared<Cluster>(\n+            context.getSettings(),\n+            names,\n+            username,\n+            password,\n+            (secure ? (maybe_secure_port ? *maybe_secure_port : DBMS_DEFAULT_SECURE_PORT) : context.getTCPPort()),\n+            false,\n+            secure);\n     }\n \n     auto structure_remote_table = getStructureOfRemoteTable(*cluster, remote_database, remote_table, context, remote_table_function_ptr);\n@@ -198,7 +211,7 @@ StoragePtr TableFunctionRemote::executeImpl(const ASTPtr & ast_function, const C\n TableFunctionRemote::TableFunctionRemote(const std::string & name_, bool secure_)\n     : name{name_}, secure{secure_}\n {\n-    is_cluster_function = name == \"cluster\";\n+    is_cluster_function = (name == \"cluster\" || name == \"clusterAllReplicas\");\n \n     std::stringstream ss;\n     ss << \"Table function '\" << name + \"' requires from 2 to \" << (is_cluster_function ? 3 : 5) << \" parameters\"\n@@ -213,6 +226,7 @@ void registerTableFunctionRemote(TableFunctionFactory & factory)\n     factory.registerFunction(\"remote\", [] () -> TableFunctionPtr { return std::make_shared<TableFunctionRemote>(\"remote\"); });\n     factory.registerFunction(\"remoteSecure\", [] () -> TableFunctionPtr { return std::make_shared<TableFunctionRemote>(\"remote\", /* secure = */ true); });\n     factory.registerFunction(\"cluster\", [] () -> TableFunctionPtr { return std::make_shared<TableFunctionRemote>(\"cluster\"); });\n+    factory.registerFunction(\"clusterAllReplicas\", [] () -> TableFunctionPtr { return std::make_shared<TableFunctionRemote>(\"clusterAllReplicas\"); });\n }\n \n }\n",
  "test_patch": "diff --git a/dbms/tests/integration/test_cluster_all_replicas/__init__.py b/dbms/tests/integration/test_cluster_all_replicas/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/dbms/tests/integration/test_cluster_all_replicas/configs/remote_servers.xml b/dbms/tests/integration/test_cluster_all_replicas/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..68dcfcc14607\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_all_replicas/configs/remote_servers.xml\n@@ -0,0 +1,16 @@\n+<yandex>\n+    <remote_servers>\n+        <two_shards>\n+            <shard>\n+                <replica>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node2</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </two_shards>\n+    </remote_servers>\n+</yandex>\ndiff --git a/dbms/tests/integration/test_cluster_all_replicas/test.py b/dbms/tests/integration/test_cluster_all_replicas/test.py\nnew file mode 100644\nindex 000000000000..0af5693fc75d\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_all_replicas/test.py\n@@ -0,0 +1,21 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', main_configs=['configs/remote_servers.xml'], with_zookeeper=True)\n+node2 = cluster.add_instance('node2', main_configs=['configs/remote_servers.xml'], with_zookeeper=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_remote(start_cluster):\n+    assert node1.query('''SELECT hostName() FROM clusterAllReplicas(\"two_shards\", system.one)''') == 'node1\\nnode2\\n'\n+    assert node1.query('''SELECT hostName() FROM cluster(\"two_shards\", system.one)''') == 'node1\\n'\n",
  "problem_statement": "Table function nodes or cluster_nodes that queries all the nodes in the cluster\n**Use case**\r\n\r\n```sql\r\n-- See all nodes in the cluster that are up.\r\nSELECT hostName() from nodes('cluster-name', system.one)\r\n\r\n-- Query logs on all nodes.\r\nSELECT ... from nodes('cluster-name', system.query_log) SETTINGS skip_unavailable_shards=1\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nCreate extra toplogy for `cluster-name` named `cluster-name-nodes` and use `cluster` table function.\n",
  "hints_text": "I also have this use case quite frequently. I'd like if it will be named `cluster_all_replicas`.\nHi, I want to take this issue.",
  "created_at": "2020-01-01T08:22:07Z"
}