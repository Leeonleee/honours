{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 22322,
  "instance_id": "ClickHouse__ClickHouse-22322",
  "issue_numbers": [
    "21907"
  ],
  "base_commit": "8f23d39f2604e788c417148136bbc24aeaa313be",
  "patch": "diff --git a/src/IO/HTTPChunkedReadBuffer.cpp b/src/IO/HTTPChunkedReadBuffer.cpp\nindex bd9bbba4c6c4..374e04031d07 100644\n--- a/src/IO/HTTPChunkedReadBuffer.cpp\n+++ b/src/IO/HTTPChunkedReadBuffer.cpp\n@@ -14,7 +14,6 @@ namespace ErrorCodes\n     extern const int ARGUMENT_OUT_OF_BOUND;\n     extern const int UNEXPECTED_END_OF_FILE;\n     extern const int CORRUPTED_DATA;\n-    extern const int TOO_MANY_BYTES;\n }\n \n size_t HTTPChunkedReadBuffer::readChunkHeader()\n@@ -40,9 +39,6 @@ size_t HTTPChunkedReadBuffer::readChunkHeader()\n     if (in->eof())\n         throw Exception(\"Unexpected end of file while reading chunk header of HTTP chunked data\", ErrorCodes::UNEXPECTED_END_OF_FILE);\n \n-    if (res > max_size)\n-        throw Exception(\"Chunk size is too large\", ErrorCodes::TOO_MANY_BYTES);\n-\n     assertString(\"\\n\", *in);\n     return res;\n }\ndiff --git a/src/IO/HTTPChunkedReadBuffer.h b/src/IO/HTTPChunkedReadBuffer.h\nindex 0ccebc69d08e..378835cafc0c 100644\n--- a/src/IO/HTTPChunkedReadBuffer.h\n+++ b/src/IO/HTTPChunkedReadBuffer.h\n@@ -10,11 +10,10 @@ namespace DB\n class HTTPChunkedReadBuffer : public BufferWithOwnMemory<ReadBuffer>\n {\n public:\n-    HTTPChunkedReadBuffer(std::unique_ptr<ReadBuffer> in_, size_t max_chunk_size) : in(std::move(in_)), max_size(max_chunk_size) {}\n+    explicit HTTPChunkedReadBuffer(std::unique_ptr<ReadBuffer> in_) : in(std::move(in_)) {}\n \n private:\n     std::unique_ptr<ReadBuffer> in;\n-    const size_t max_size;\n \n     size_t readChunkHeader();\n     void readChunkFooter();\ndiff --git a/src/Server/HTTP/HTTPServerRequest.cpp b/src/Server/HTTP/HTTPServerRequest.cpp\nindex bdba6a51d91a..2a765f36fd7b 100644\n--- a/src/Server/HTTP/HTTPServerRequest.cpp\n+++ b/src/Server/HTTP/HTTPServerRequest.cpp\n@@ -26,7 +26,6 @@ HTTPServerRequest::HTTPServerRequest(const Context & context, HTTPServerResponse\n \n     auto receive_timeout = context.getSettingsRef().http_receive_timeout;\n     auto send_timeout = context.getSettingsRef().http_send_timeout;\n-    auto max_query_size = context.getSettingsRef().max_query_size;\n \n     session.socket().setReceiveTimeout(receive_timeout);\n     session.socket().setSendTimeout(send_timeout);\n@@ -37,7 +36,7 @@ HTTPServerRequest::HTTPServerRequest(const Context & context, HTTPServerResponse\n     readRequest(*in);  /// Try parse according to RFC7230\n \n     if (getChunkedTransferEncoding())\n-        stream = std::make_unique<HTTPChunkedReadBuffer>(std::move(in), max_query_size);\n+        stream = std::make_unique<HTTPChunkedReadBuffer>(std::move(in));\n     else if (hasContentLength())\n         stream = std::make_unique<LimitReadBuffer>(std::move(in), getContentLength(), false);\n     else if (getMethod() != HTTPRequest::HTTP_GET && getMethod() != HTTPRequest::HTTP_HEAD && getMethod() != HTTPRequest::HTTP_DELETE)\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01783_http_chunk_size.reference b/tests/queries/0_stateless/01783_http_chunk_size.reference\nnew file mode 100644\nindex 000000000000..e454a00607ce\n--- /dev/null\n+++ b/tests/queries/0_stateless/01783_http_chunk_size.reference\n@@ -0,0 +1,1 @@\n+1234567890 1234567890 1234567890 1234567890\ndiff --git a/tests/queries/0_stateless/01783_http_chunk_size.sh b/tests/queries/0_stateless/01783_http_chunk_size.sh\nnew file mode 100755\nindex 000000000000..66ac4dfa975f\n--- /dev/null\n+++ b/tests/queries/0_stateless/01783_http_chunk_size.sh\n@@ -0,0 +1,17 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+URL=\"${CLICKHOUSE_URL}&session_id=id_${CLICKHOUSE_DATABASE}\"\n+\n+echo \"DROP TABLE IF EXISTS table\" | ${CLICKHOUSE_CURL} -sSg \"${URL}\" -d @-\n+echo \"CREATE TABLE table (a String) ENGINE Memory()\" | ${CLICKHOUSE_CURL} -sSg \"${URL}\" -d @-\n+\n+# NOTE: suppose that curl sends everything in a single chunk - there are no options to force the chunk-size.\n+echo \"SET max_query_size=44\" | ${CLICKHOUSE_CURL} -sSg \"${URL}\" -d @-\n+echo -ne \"INSERT INTO TABLE table FORMAT TabSeparated 1234567890 1234567890 1234567890 1234567890\\n\" | ${CLICKHOUSE_CURL} -H \"Transfer-Encoding: chunked\" -sS \"${URL}\" --data-binary @-\n+\n+echo \"SELECT * from table\" | ${CLICKHOUSE_CURL} -sSg \"${URL}\" -d @-\n+echo \"DROP TABLE table\" | ${CLICKHOUSE_CURL} -sSg \"${URL}\" -d @-\ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex df2090325a33..534b6ae54336 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -164,7 +164,8 @@\n         \"00062_replicated_merge_tree_alter_zookeeper\",\n         /// Does not support renaming of multiple tables in single query\n         \"00634_rename_view\",\n-        \"00140_rename\"\n+        \"00140_rename\",\n+        \"01783_http_chunk_size\"\n     ],\n     \"polymorphic-parts\": [\n         \"01508_partition_pruning_long\", /// bug, shoud be fixed\n",
  "problem_statement": "JDBC driver / specific HTTP clients: Chunk size is too large error when executing an \"INSERT INTO\" with HTTP client compression\nI recently doing some tests for updating from clickhouse 20.3 to 21.3.2.5.\r\n\r\nWith this new build, when I am executing  large insertion over the HTTP connection, with compression activated on the client side\r\nLike this query\r\nPOST /?query=INSERT+INTO+%60avc%60.%60topconversationdetails%60+FORMAT+RowBinary&database=avc&compress=1&decompress=1 HTTP/1.1\r\nWhere data are inside the content on the HTTP request in compressed row binary format\r\n\r\nI sometimes see this errors/stack trace in the clickhouse log file :\r\n\r\n```\r\n2021.03.19 11:54:42.764099 [ 41063 ] {eba46f57-0491-4ffb-b571-ed9d1618da75} <Error> DynamicQueryHandler: Code: 307, e.displayText() = DB::Exception: Chunk size is too large, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::HTTPChunkedReadBuffer::readChunkHeader() @ 0xf8d5e5b in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n1. DB::HTTPChunkedReadBuffer::nextImpl() @ 0xf8d616b in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n2. DB::wrapReadBufferReference(DB::ReadBuffer&)::ReadBufferWrapper::nextImpl() @ 0xe7183fc in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n3. DB::CompressedReadBufferBase::readCompressedData(unsigned long&, unsigned long&, bool) @ 0xe7d88cc in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n4. DB::CompressedReadBuffer::nextImpl() @ 0xe7d83e7 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n5. DB::ConcatReadBuffer::nextImpl() @ 0xe966e1e in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n6. DB::LimitReadBuffer::nextImpl() @ 0x86a7fcc in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n7. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>) @ 0xf1306cb in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n8. DB::HTTPHandler::processQuery(DB::Context&, DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) @ 0xf84675a in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n9. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) @ 0xf84a8ee in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n10. DB::HTTPServerConnection::run() @ 0xf8d35ff in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n11. Poco::Net::TCPServerConnection::start() @ 0x11f7ccbf in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n12. Poco::Net::TCPServerDispatcher::run() @ 0x11f7e6d1 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n13. Poco::PooledThread::run() @ 0x120b4df9 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n14. Poco::ThreadImpl::runnableEntry(void*) @ 0x120b0c5a in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n15. start_thread @ 0x7dd5 in /usr/lib64/libpthread-2.17.so\r\n16. __clone @ 0xfdead in /usr/lib64/libc-2.17.so\r\n (version 21.3.2.5 (official build))\r\n2021.03.19 11:54:42.764302 [ 41063 ] {eba46f57-0491-4ffb-b571-ed9d1618da75} <Error> DynamicQueryHandler: Cannot send exception to client: Code: 246, e.displayText() = DB::Exception: Unexpected data instead of HTTP chunk header, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::HTTPChunkedReadBuffer::readChunkHeader() @ 0xf8d5e02 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n1. DB::HTTPChunkedReadBuffer::nextImpl() @ 0xf8d616b in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n2. DB::HTTPHandler::trySendExceptionToClient(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, DB::HTTPServerRequest&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&) @ 0xf849ca2 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n3. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) @ 0xf84b015 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n4. DB::HTTPServerConnection::run() @ 0xf8d35ff in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n5. Poco::Net::TCPServerConnection::start() @ 0x11f7ccbf in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n6. Poco::Net::TCPServerDispatcher::run() @ 0x11f7e6d1 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n7. Poco::PooledThread::run() @ 0x120b4df9 in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n8. Poco::ThreadImpl::runnableEntry(void*) @ 0x120b0c5a in /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n9. start_thread @ 0x7dd5 in /usr/lib64/libpthread-2.17.so\r\n10. __clone @ 0xfdead in /usr/lib64/libc-2.17.so\r\n (version 21.3.2.5 (official build))\r\n\r\n```\r\n\r\nIt does not seems to happen if I disable the client compression (but my requests are larger)\r\n\r\n\n",
  "hints_text": "Can you  show `create table topconversationdetails`\nCould not reproduce\r\n\r\n```\r\n21.4.1.6248\r\n\r\nclickhouse-client -q \"select arrayStringConcat(arrayMap(x->toString (cityHash64(x)) , range(1000)),' ') from numbers(100000) Format RowBinary\" |gzip >test.bin.gz\r\n\r\nclickhouse-client -q \"CREATE TABLE log (row String) ENGINE = Null\"\r\n\r\nfor i in `seq 1 50`; do curl -s -w 'Total: %{time_total}s HTTP: %{http_code}\\n' -H 'Content-Encoding: gzip'  \"http://localhost:8123/?compress=1&query=INSERT%20INTO%20log%20format%20RowBinary\" --data-binary @test.bin.gz ; done;\r\nTotal: 2.770964s HTTP: 200\r\nTotal: 2.765195s HTTP: 200\r\nTotal: 2.835844s HTTP: 200\r\nTotal: 2.766895s HTTP: 200\r\nTotal: 2.607872s HTTP: 200\r\nTotal: 2.635509s HTTP: 200\r\nTotal: 2.533113s HTTP: 200\r\nTotal: 2.619424s HTTP: 200\r\nTotal: 2.670999s HTTP: 200\r\nTotal: 2.696852s HTTP: 200\r\nTotal: 2.804179s HTTP: 200\r\nTotal: 2.750693s HTTP: 200\r\nTotal: 2.739884s HTTP: 200\r\nTotal: 2.724064s HTTP: 200\r\n...\r\n```\nHi,\r\n\r\nIn your test case, you are trying to use the standard http compression.\r\nHere, I got issue with the internal data compression algorithm.\r\nYou have to specify decompress=1 if the data sent by the client is compressed, and compress=1 if you want that the server compresses the data:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/21.3/src/Server/HTTPHandler.cpp#L467\r\nhttps://clickhouse.tech/docs/en/interfaces/http/#compression\r\n\r\nI am trying to find a reproducible test case.\r\n\nOk, it seems to happens when you use Transfer-Encoding: chunked, and the Content-Length is not specified.\r\n\r\nIt is difficult to reproduce with curl (I am still working on it), but it is what happens with the clickhouse-jdbc driver from java code.\nhttps://github.com/ClickHouse/ClickHouse/issues/17073\r\nhttps://github.com/ClickHouse/ClickHouse/issues/17446\nThanks for the links.\r\nAfter digging inside several network traces, it seems that the jdbc driver, when sending a rowbinary format, with decompress=1, is generating only one HTTP Data chunk split over several TCP packets.\r\nIt used to be accepted by the clickhouse server, but it is no more the case (I doubt that this way to do that is valid from the HTTP specs.)\r\n\r\nI will dig more in the jdbc driver.\nOk, so I troubleshooted at more deeper level:\r\n\r\nWhen the clickhouse-jdbc driver is used with decompress=1 and with native or rowbinary format for insertion, the apache http library (a quite standard library for using http) is generating a Tranfer-Encoding: chunked communication where chunks are bigger than a TCP packets. (so we receive the chunk size on the first tcp packet and some part of the chunk, and then, the remaining content of the chunk is split into several tcp packets).\r\n\r\nA cannot reproduce with curl or wget, but it should be possible to write a java test case.\r\n\r\nThis mode seems to be called by apache team \"large chunk\", it is not forbidden by the RFC: https://tools.ietf.org/html/rfc7230#section-4.1\r\nAnd was previously supported by clickhouse server.\r\n\r\nSo finally, it seems to be a clickhouse server bug. What do you think ?\nI confirm - the code around limiting the chunk size is incorrect.\r\n\r\nHTTPServerRequest.cpp:40\r\n> stream = std::make_unique<HTTPChunkedReadBuffer>(std::move(in), max_query_size);\r\n\r\n`max_query_size` should not apply to the size of INSERTed data.\n@abyss7 Better not to limit at all or limit with some large value (will be implicitly limited by max_memory_usage).\r\n\r\nBTW, why do we need\r\n```\r\n/// Chunk is not completely in buffer, copy it to scratch space.\r\nmemory.resize(chunk_size);\r\n```\r\n\r\nMaybe it's possible to read data by parts that will fit in buffer and count the number of bytes remaining to the end of chunk?\nLet's also make the error message more descriptive. It should say about the chunk of HTTP Transfer-Encoding: chunked.",
  "created_at": "2021-03-29T17:50:59Z"
}