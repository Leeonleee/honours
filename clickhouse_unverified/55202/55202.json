{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 55202,
  "instance_id": "ClickHouse__ClickHouse-55202",
  "issue_numbers": [
    "52652",
    "53858"
  ],
  "base_commit": "70c3f319265ceb450aa4a33b4ef131c257c76b38",
  "patch": "diff --git a/src/Interpreters/MutationsInterpreter.cpp b/src/Interpreters/MutationsInterpreter.cpp\nindex 4b0cbec4f9fb..2db4fce81f0d 100644\n--- a/src/Interpreters/MutationsInterpreter.cpp\n+++ b/src/Interpreters/MutationsInterpreter.cpp\n@@ -40,6 +40,7 @@\n #include <Interpreters/InterpreterSelectQueryAnalyzer.h>\n #include <Parsers/makeASTForLogicalFunction.h>\n #include <Common/logger_useful.h>\n+#include <Storages/MergeTree/MergeTreeDataPartType.h>\n \n namespace DB\n {\n@@ -304,6 +305,11 @@ bool MutationsInterpreter::Source::hasProjection(const String & name) const\n     return part && part->hasProjection(name);\n }\n \n+bool MutationsInterpreter::Source::isCompactPart() const\n+{\n+    return part && part->getType() == MergeTreeDataPartType::Compact;\n+}\n+\n static Names getAvailableColumnsWithVirtuals(StorageMetadataPtr metadata_snapshot, const IStorage & storage)\n {\n     auto all_columns = metadata_snapshot->getColumns().getNamesOfPhysical();\n@@ -562,7 +568,8 @@ void MutationsInterpreter::prepare(bool dry_run)\n     if (settings.recalculate_dependencies_of_updated_columns)\n         dependencies = getAllColumnDependencies(metadata_snapshot, updated_columns, has_dependency);\n \n-    bool has_alter_delete = false;\n+    bool need_rebuild_indexes = false;\n+    bool need_rebuild_projections = false;\n     std::vector<String> read_columns;\n \n     /// First, break a sequence of commands into stages.\n@@ -583,7 +590,9 @@ void MutationsInterpreter::prepare(bool dry_run)\n                 predicate = makeASTFunction(\"isZeroOrNull\", predicate);\n \n             stages.back().filters.push_back(predicate);\n-            has_alter_delete = true;\n+            /// ALTER DELETE can changes number of rows in the part, so we need to rebuild indexes and projection\n+            need_rebuild_indexes = true;\n+            need_rebuild_projections = true;\n         }\n         else if (command.type == MutationCommand::UPDATE)\n         {\n@@ -687,6 +696,11 @@ void MutationsInterpreter::prepare(bool dry_run)\n                     }\n                 }\n             }\n+\n+            /// If the part is compact and adaptive index granularity is enabled, modify data in one column via ALTER UPDATE can change\n+            /// the part granularity, so we need to rebuild indexes\n+            if (source.isCompactPart() && source.getMergeTreeData() && source.getMergeTreeData()->getSettings()->index_granularity_bytes > 0)\n+                need_rebuild_indexes = true;\n         }\n         else if (command.type == MutationCommand::MATERIALIZE_COLUMN)\n         {\n@@ -892,7 +906,7 @@ void MutationsInterpreter::prepare(bool dry_run)\n         if (!source.hasSecondaryIndex(index.name))\n             continue;\n \n-        if (has_alter_delete)\n+        if (need_rebuild_indexes)\n         {\n             materialized_indices.insert(index.name);\n             continue;\n@@ -913,7 +927,7 @@ void MutationsInterpreter::prepare(bool dry_run)\n         if (!source.hasProjection(projection.name))\n             continue;\n \n-        if (has_alter_delete)\n+        if (need_rebuild_projections)\n         {\n             materialized_projections.insert(projection.name);\n             continue;\ndiff --git a/src/Interpreters/MutationsInterpreter.h b/src/Interpreters/MutationsInterpreter.h\nindex 9b4caaae2313..c53b86ddb5ec 100644\n--- a/src/Interpreters/MutationsInterpreter.h\n+++ b/src/Interpreters/MutationsInterpreter.h\n@@ -122,6 +122,7 @@ class MutationsInterpreter\n         bool materializeTTLRecalculateOnly() const;\n         bool hasSecondaryIndex(const String & name) const;\n         bool hasProjection(const String & name) const;\n+        bool isCompactPart() const;\n \n         void read(\n             Stage & first_stage,\ndiff --git a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\nindex c9b22c8a03e2..7531c03a011e 100644\n--- a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n+++ b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n@@ -107,13 +107,14 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n     // We first read the marks into a temporary simple array, then compress them into a more compact\n     // representation.\n     PODArray<MarkInCompressedFile> plain_marks(marks_count * columns_in_mark); // temporary\n+    auto full_mark_path = std::string(fs::path(data_part_storage->getFullPath()) / mrk_path);\n \n     if (file_size == 0 && marks_count != 0)\n     {\n         throw Exception(\n             ErrorCodes::CORRUPTED_DATA,\n             \"Empty marks file '{}': {}, must be: {}\",\n-            std::string(fs::path(data_part_storage->getFullPath()) / mrk_path),\n+            full_mark_path,\n             file_size, expected_uncompressed_size);\n     }\n \n@@ -121,7 +122,7 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n         throw Exception(\n             ErrorCodes::CORRUPTED_DATA,\n             \"Bad size of marks file '{}': {}, must be: {}\",\n-            std::string(fs::path(data_part_storage->getFullPath()) / mrk_path),\n+            full_mark_path,\n             file_size,\n             expected_uncompressed_size);\n \n@@ -142,7 +143,7 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n             throw Exception(\n                 ErrorCodes::CANNOT_READ_ALL_DATA,\n                 \"Cannot read all marks from file {}, is eof: {}, buffer size: {}, file size: {}\",\n-                mrk_path,\n+                full_mark_path,\n                 reader->eof(),\n                 reader->buffer().size(),\n                 file_size);\n@@ -155,7 +156,7 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n                 throw Exception(\n                     ErrorCodes::CANNOT_READ_ALL_DATA,\n                     \"Cannot read all marks from file {}, marks expected {} (bytes size {}), marks read {} (bytes size {})\",\n-                    mrk_path, marks_count, expected_uncompressed_size, i, reader->count());\n+                    full_mark_path, marks_count, expected_uncompressed_size, i, reader->count());\n \n             size_t granularity;\n             reader->readStrict(\n@@ -167,7 +168,7 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n             throw Exception(\n                 ErrorCodes::CANNOT_READ_ALL_DATA,\n                 \"Too many marks in file {}, marks expected {} (bytes size {})\",\n-                mrk_path, marks_count, expected_uncompressed_size);\n+                full_mark_path, marks_count, expected_uncompressed_size);\n     }\n \n #if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02891_alter_update_adaptive_granularity.reference b/tests/queries/0_stateless/02891_alter_update_adaptive_granularity.reference\nnew file mode 100644\nindex 000000000000..4b58ede56992\n--- /dev/null\n+++ b/tests/queries/0_stateless/02891_alter_update_adaptive_granularity.reference\n@@ -0,0 +1,1 @@\n+342\t442\tThe Containers library is a generic collection of class templates and algorithms that allow programmers to easily implement common data structures like queues, lists and stacks\ndiff --git a/tests/queries/0_stateless/02891_alter_update_adaptive_granularity.sql b/tests/queries/0_stateless/02891_alter_update_adaptive_granularity.sql\nnew file mode 100644\nindex 000000000000..09dfd6d8c3ea\n--- /dev/null\n+++ b/tests/queries/0_stateless/02891_alter_update_adaptive_granularity.sql\n@@ -0,0 +1,24 @@\n+CREATE TABLE kv\n+(\n+    `key` UInt64,\n+    `value` UInt64,\n+    `s` String,\n+    INDEX value_idx value TYPE minmax GRANULARITY 1\n+)\n+ENGINE = ReplacingMergeTree\n+ORDER BY key\n+SETTINGS index_granularity = 32, index_granularity_bytes = 1024;\n+\n+INSERT INTO kv SELECT\n+    number,\n+    number + 100,\n+    toString(number)\n+FROM numbers(2048);\n+\n+ALTER TABLE kv\n+    UPDATE s = 'The Containers library is a generic collection of class templates and algorithms that allow programmers to easily implement common data structures like queues, lists and stacks' WHERE 1\n+SETTINGS mutations_sync = 2;\n+\n+SELECT *\n+FROM kv\n+WHERE value = 442;\n",
  "problem_statement": "Code: 33. DB::Exception: Cannot read all marks from file\n```sql\r\n2023.07.27 10:54:01.785213 [ 1720 ] {9e777690-69a0-4208-aafc-675961364fbf} <Error> executeQuery: Code: 33. DB::Exception: Cannot read all marks from file skp_idx_regionId_index.cmrk3, marks expected 2 (bytes size 48), marks read 3 (bytes size 72). (CANNOT_READ_ALL_DATA) (version 23.5.4.10) (from 10.244.125.151:44596) (in query: SELECT count() FROM ...), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e0c3d95 in /usr/bin/clickhouse\r\n1. ? @ 0x00000000146daa67 in /usr/bin/clickhouse\r\n2. DB::MergeTreeMarksLoader::loadMarksImpl() @ 0x00000000146d9f22 in /usr/bin/clickhouse\r\n3. DB::MergeTreeMarksLoader::loadMarks() @ 0x00000000146d80ea in /usr/bin/clickhouse\r\n4. DB::MergeTreeMarksLoader::getMark(unsigned long, unsigned long) @ 0x00000000146d7dd8 in /usr/bin/clickhouse\r\n5. DB::MergeTreeReaderStream::init() @ 0x00000000146ff601 in /usr/bin/clickhouse\r\n6. DB::MergeTreeIndexReader::MergeTreeIndexReader(std::shared_ptr<DB::IMergeTreeIndex const>, std::shared_ptr<DB::IMergeTreeDataPart const>, unsigned long, DB::MarkRanges const&, DB::MarkCache*, DB::UncompressedCache*, DB::MergeTreeReaderSettings) @ 0x0000000014684e6e in /usr/bin/clickhouse\r\n7. DB::MergeTreeDataSelectExecutor::filterMarksUsingIndex(std::shared_ptr<DB::IMergeTreeIndex const>, std::shared_ptr<DB::IMergeTreeIndexCondition>, std::shared_ptr<DB::IMergeTreeDataPart const>, DB::MarkRanges const&, DB::Settings const&, DB::MergeTreeReaderSettings const&, unsigned long&, DB::MarkCache*, DB::UncompressedCache*, Poco::Logger*) @ 0x000000001466a088 in /usr/bin/clickhouse\r\n8. ? @ 0x0000000014666f9a in /usr/bin/clickhouse\r\n9. ? @ 0x0000000014671bfb in /usr/bin/clickhouse\r\n10. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x000000000e19be23 in /usr/bin/clickhouse\r\n11. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000e19e615 in /usr/bin/clickhouse\r\n12. ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000000e197cb4 in /usr/bin/clickhouse\r\n13. ? @ 0x000000000e19d4a1 in /usr/bin/clickhouse\r\n14. ? @ 0x00007fe65ed00609 in ?\r\n15. clone @ 0x00007fe65ec25133 in ?\r\n```\r\n\r\n\r\n<details>\r\n<summary>addr2line</summary>\r\n\r\n```\r\n# llvm-addr2line-16 -pafiCse ./clickhouse 0x000000000e0c3d95  0x00000000146daa67  0x00000000146d9f22  0x00000000146d80ea  0x00000000146d7dd8 0x00000000146ff601  0x0000000014684e6e 0x000000001466a088  0x0000000014666f9a\r\n0xe0c3d95: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) at Exception.cpp:92\r\n0x146daa67: std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>::__is_long[abi:v15000]() const at string:1499\r\n (inlined by) std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>::~basic_string() at string:2333\r\n (inlined by) DB::Exception::MessageMasked::~MessageMasked() at Exception.h:42\r\n (inlined by) DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&&, int, bool) at Exception.h:54\r\n (inlined by) DB::Exception::Exception<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&, unsigned long&, unsigned long&, unsigned long&, unsigned long>(int, FormatStringHelperImpl<std::__1::type_identity<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&>::type, std::__1::type_identity<unsigned long&>::type, std::__1::type_identity<unsigned long&>::type, std::__1::type_identity<unsigned long&>::type, std::__1::type_identity<unsigned long>::type>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&, unsigned long&, unsigned long&, unsigned long&, unsigned long&&) at Exception.h:81\r\n0x146d9f22: DB::MergeTreeMarksLoader::loadMarksImpl() at MergeTreeMarksLoader.cpp:163\r\n0x146d80ea: DB::MergeTreeMarksLoader::loadMarks() at MergeTreeMarksLoader.cpp:200\r\n0x146d7dd8: std::__1::enable_if<is_move_constructible<DB::MarksInCompressedFile*>::value && is_move_assignable<DB::MarksInCompressedFile*>::value, void>::type std::__1::swap[abi:v15000]<DB::MarksInCompressedFile*>(DB::MarksInCompressedFile*&, DB::MarksInCompressedFile*&) at swap.h:37\r\n (inlined by) std::__1::shared_ptr<DB::MarksInCompressedFile>::swap[abi:v15000](std::__1::shared_ptr<DB::MarksInCompressedFile>&) at shared_ptr.h:761\r\n (inlined by) std::__1::shared_ptr<DB::MarksInCompressedFile>::operator=[abi:v15000](std::__1::shared_ptr<DB::MarksInCompressedFile>&&) at shared_ptr.h:723\r\n (inlined by) DB::MergeTreeMarksLoader::getMark(unsigned long, unsigned long) at MergeTreeMarksLoader.cpp:80\r\n0x146ff601: DB::MergeTreeReaderStream::init() at MergeTreeReaderStream.cpp:72\r\n0x14684e6e: DB::MergeTreeReaderStream::adjustRightMark(unsigned long) at MergeTreeReaderStream.cpp:276\r\n (inlined by) DB::MergeTreeIndexReader::MergeTreeIndexReader(std::__1::shared_ptr<DB::IMergeTreeIndex const>, std::__1::shared_ptr<DB::IMergeTreeDataPart const>, unsigned long, DB::MarkRanges const&, DB::MarkCache*, DB::UncompressedCache*, DB::MergeTreeReaderSettings) at MergeTreeIndexReader.cpp:61\r\n0x1466a088: std::__1::__deque_base<DB::MarkRange, std::__1::allocator<DB::MarkRange>>::__deque_base() at deque:1188\r\n (inlined by) std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange>>::deque[abi:v15000]() at deque:1313\r\n (inlined by) DB::MarkRanges::MarkRanges() at MarkRange.h:34\r\n (inlined by) DB::MergeTreeDataSelectExecutor::filterMarksUsingIndex(std::__1::shared_ptr<DB::IMergeTreeIndex const>, std::__1::shared_ptr<DB::IMergeTreeIndexCondition>, std::__1::shared_ptr<DB::IMergeTreeDataPart const>, DB::MarkRanges const&, DB::Settings const&, DB::MergeTreeReaderSettings const&, unsigned long&, DB::MarkCache*, DB::UncompressedCache*, Poco::Logger*) at MergeTreeDataSelectExecutor.cpp:1694\r\n0x14666f9a: DB::MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipIndexes(std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const>>>&&, std::__1::vector<std::__1::shared_ptr<DB::AlterConversions const>, std::__1::allocator<std::__1::shared_ptr<DB::AlterConversions const>>>&&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const>, DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const> const&, DB::KeyCondition const&, DB::MergeTreeReaderSettings const&, Poco::Logger*, unsigned long, std::__1::vector<DB::ReadFromMergeTree::IndexStat, std::__1::allocator<DB::ReadFromMergeTree::IndexStat>>&, bool)::$_1::operator()(unsigned long) const at MergeTreeDataSelectExecutor.cpp:1071\r\n\r\n```\r\n\r\n</details>\nDB::Exception: Too many marks in file skip_idx_sindex_visitorid.cmrk3, marks expected 3 (bytes size 72)\nHi,\r\n\r\nI'm try to use the latest version 23.7.4.5 (official build) and skip index, but after I insert data and try to query via skipe index, I got this error, can you please tell me why?\r\n\r\nI get started with clickhouse but stuck at first step here\r\n\r\n`error: HttpCode:500 ;  ;Code: 33. DB::Exception: Too many marks in file skp_idx_sindex_visitorid.cmrk3, marks expected 3 (bytes size 72). (CANNOT_READ_ALL_DATA) (version 23.7.4.5 (official build))`\r\n\r\nThank you very much!\n",
  "hints_text": "Sorry for being annoying lately, if any.\nRelated #52433, may be fixed in #52530\n@jasonbigl can I ask what's the issue you found?\n> @jasonbigl can I ask what's the issue you found?\r\n\r\nHi, thank you for reply, its just like the issue described, we use skip index and we remove some data periodically. but we found every time we setup with v23.7.4.5, after a few hours, CH start throw that error when we try to query by skip index.\r\n\r\nSince its urgent in our side so we fall back to v22.9\r\n\r\nI'm gussing this is related to https://github.com/ClickHouse/ClickHouse/issues/52652, it seems its fixed, but at the time I post this issue, the fix is not merged and released, so I closed this issue. we never have time to test it before, but now we are going to test the issue",
  "created_at": "2023-10-02T07:41:55Z"
}