{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 12045,
  "instance_id": "ClickHouse__ClickHouse-12045",
  "issue_numbers": [
    "11943"
  ],
  "base_commit": "c1d2d2d7f759536cdac991077110ea40023030c2",
  "patch": "diff --git a/src/Databases/DatabaseOnDisk.cpp b/src/Databases/DatabaseOnDisk.cpp\nindex 0a16b6eacff9..2ad7af9c7037 100644\n--- a/src/Databases/DatabaseOnDisk.cpp\n+++ b/src/Databases/DatabaseOnDisk.cpp\n@@ -389,6 +389,9 @@ void DatabaseOnDisk::iterateMetadataFiles(const Context & context, const Iterati\n         }\n     };\n \n+    /// Metadata files to load: name and flag for .tmp_drop files\n+    std::set<std::pair<String, bool>> metadata_files;\n+\n     Poco::DirectoryIterator dir_end;\n     for (Poco::DirectoryIterator dir_it(getMetadataPath()); dir_it != dir_end; ++dir_it)\n     {\n@@ -404,7 +407,7 @@ void DatabaseOnDisk::iterateMetadataFiles(const Context & context, const Iterati\n         if (endsWith(dir_it.name(), tmp_drop_ext))\n         {\n             /// There are files that we tried to delete previously\n-            process_tmp_drop_metadata_file(dir_it.name());\n+            metadata_files.emplace(dir_it.name(), false);\n         }\n         else if (endsWith(dir_it.name(), \".sql.tmp\"))\n         {\n@@ -415,12 +418,26 @@ void DatabaseOnDisk::iterateMetadataFiles(const Context & context, const Iterati\n         else if (endsWith(dir_it.name(), \".sql\"))\n         {\n             /// The required files have names like `table_name.sql`\n-            process_metadata_file(dir_it.name());\n+            metadata_files.emplace(dir_it.name(), true);\n         }\n         else\n             throw Exception(\"Incorrect file extension: \" + dir_it.name() + \" in metadata directory \" + getMetadataPath(),\n                 ErrorCodes::INCORRECT_FILE_NAME);\n     }\n+\n+    /// Read and parse metadata in parallel\n+    ThreadPool pool(SettingMaxThreads().getAutoValue());\n+    for (const auto & file : metadata_files)\n+    {\n+        pool.scheduleOrThrowOnError([&]()\n+        {\n+            if (file.second)\n+                process_metadata_file(file.first);\n+            else\n+                process_tmp_drop_metadata_file(file.first);\n+        });\n+    }\n+    pool.wait();\n }\n \n ASTPtr DatabaseOnDisk::parseQueryFromMetadata(Poco::Logger * loger, const Context & context, const String & metadata_file_path, bool throw_on_error /*= true*/, bool remove_empty /*= false*/)\ndiff --git a/src/Databases/DatabaseOrdinary.cpp b/src/Databases/DatabaseOrdinary.cpp\nindex eec58ed9b334..277588b59f02 100644\n--- a/src/Databases/DatabaseOrdinary.cpp\n+++ b/src/Databases/DatabaseOrdinary.cpp\n@@ -121,11 +121,12 @@ void DatabaseOrdinary::loadStoredObjects(\n       *  which does not correspond to order tables creation and does not correspond to order of their location on disk.\n       */\n     using FileNames = std::map<std::string, ASTPtr>;\n+    std::mutex file_names_mutex;\n     FileNames file_names;\n \n     size_t total_dictionaries = 0;\n \n-    auto process_metadata = [&context, &file_names, &total_dictionaries, this](const String & file_name)\n+    auto process_metadata = [&context, &file_names, &total_dictionaries, &file_names_mutex, this](const String & file_name)\n     {\n         String full_path = getMetadataPath() + file_name;\n         try\n@@ -134,6 +135,7 @@ void DatabaseOrdinary::loadStoredObjects(\n             if (ast)\n             {\n                 auto * create_query = ast->as<ASTCreateQuery>();\n+                std::lock_guard lock{file_names_mutex};\n                 file_names[file_name] = ast;\n                 total_dictionaries += create_query->is_dictionary;\n             }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01193_metadata_loading.reference b/tests/queries/0_stateless/01193_metadata_loading.reference\nnew file mode 100644\nindex 000000000000..8ff246325ac7\n--- /dev/null\n+++ b/tests/queries/0_stateless/01193_metadata_loading.reference\n@@ -0,0 +1,5 @@\n+10000\t0\t2020-06-25\thello\t[1,2]\t[3,4]\n+10000\t1\t2020-06-26\tword\t[10,20]\t[30,40]\n+ok\n+8000\t0\t2020-06-25\thello\t[1,2]\t[3,4]\n+8000\t1\t2020-06-26\tword\t[10,20]\t[30,40]\ndiff --git a/tests/queries/0_stateless/01193_metadata_loading.sh b/tests/queries/0_stateless/01193_metadata_loading.sh\nnew file mode 100755\nindex 000000000000..de74b3ec1af6\n--- /dev/null\n+++ b/tests/queries/0_stateless/01193_metadata_loading.sh\n@@ -0,0 +1,59 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+. $CURDIR/../shell_config.sh\n+\n+# it is the worst way of making performance test, nevertheless it can detect significant slowdown and some other issues, that usually found by stress test\n+\n+db=\"test_01193_$RANDOM\"\n+\n+declare -A engines\n+engines[0]=\"Memory\"\n+engines[1]=\"File(CSV)\"\n+engines[2]=\"Log\"\n+engines[3]=\"StripeLog\"\n+engines[4]=\"MergeTree ORDER BY i\"\n+\n+tables=1000\n+threads=10\n+count_multiplier=1\n+max_time_ms=1000\n+\n+debug_or_sanitizer_build=`$CLICKHOUSE_CLIENT -q \"WITH ((SELECT value FROM system.build_options WHERE name='BUILD_TYPE') AS build, (SELECT value FROM system.build_options WHERE name='CXX_FLAGS') as flags) SELECT build='Debug' OR flags LIKE '%fsanitize%'\"`\n+\n+if [[ debug_or_sanitizer_build -eq 1 ]]; then tables=100; count_multiplier=10; max_time_ms=1500; fi\n+\n+create_tables() {\n+  for i in $(seq 1 $tables); do\n+    engine=${engines[$((i % ${#engines[@]}))]}\n+    $CLICKHOUSE_CLIENT -q \"CREATE TABLE $db.table_$1_$i (i UInt64, d Date, s String, n Nested(i UInt8, f Float32)) ENGINE=$engine\"\n+    $CLICKHOUSE_CLIENT -q \"INSERT INTO $db.table_$1_$i VALUES (0, '2020-06-25', 'hello', [1, 2], [3, 4]), (1, '2020-06-26', 'word', [10, 20], [30, 40])\"\n+  done\n+}\n+\n+$CLICKHOUSE_CLIENT -q \"CREATE DATABASE $db\"\n+\n+for i in $(seq 1 $threads); do\n+  create_tables $i &\n+done\n+wait\n+\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE $db.table_merge (i UInt64, d Date, s String, n Nested(i UInt8, f Float32)) ENGINE=Merge('$db', '^table_')\"\n+#FIXME the following query leads to segfault\n+#$CLICKHOUSE_CLIENT -q \"SELECT count() * $count_multiplier, i, d, s, n.i, n.f FROM $db.table_merge GROUP BY i, d, s, n.i, n.f ORDER BY i\"\n+$CLICKHOUSE_CLIENT -q \"SELECT 10000, i, d, s, n.i, n.f FROM $db.table_1_1 GROUP BY i, d, s, n.i, n.f ORDER BY i\"\n+\n+db_engine=`$CLICKHOUSE_CLIENT -q \"SELECT engine FROM system.databases WHERE name='$db'\"`\n+\n+$CLICKHOUSE_CLIENT -q \"DETACH DATABASE $db\"\n+\n+# get real time, grep seconds, remove point, remove leading zeros\n+elapsed_ms=`{ time $CLICKHOUSE_CLIENT -q \"ATTACH DATABASE $db ENGINE=$db_engine\"; } 2>&1 | grep real | grep -Po \"0m\\K[0-9\\.]*\" | tr -d '.' | sed \"s/^0*//\"`\n+$CLICKHOUSE_CLIENT -q \"SELECT '01193_metadata_loading', $elapsed_ms FORMAT Null\" # it will be printed to server log\n+\n+if [[ $elapsed_ms -le $max_time_ms ]]; then echo ok; fi\n+\n+#$CLICKHOUSE_CLIENT -q \"SELECT count() * $count_multiplier, i, d, s, n.i, n.f FROM $db.table_merge GROUP BY i, d, s, n.i, n.f ORDER BY i\"\n+$CLICKHOUSE_CLIENT -q \"SELECT 8000, i, d, s, n.i, n.f FROM $db.table_1_1 GROUP BY i, d, s, n.i, n.f ORDER BY i\"\n+\n+$CLICKHOUSE_CLIENT -q \"DROP DATABASE $db\"\n",
  "problem_statement": "Loading of tables is not parallel in version 20.1+\nIt hurts if server has millions of tables.\r\n\r\nSee https://github.com/ClickHouse/ClickHouse/pull/3398\r\nPlease add some tests to ensure it won't be broken in future.\n",
  "hints_text": "",
  "created_at": "2020-06-30T00:13:50Z"
}