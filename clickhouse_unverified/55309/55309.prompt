You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
S3 disk garbage collection?
Hi,

If we use S3 as a disk for a table, how does Clickhouse make sure there are no orphan files left in the S3 bucket?

In my recent test, I have encountered a lot of S3 rate limiting errors (HTTP 503, "Slow Down") when bulk inserted lots of data.

Then I truncated the table. This seems have left many files in the S3 bucket undeleted, although the table is empty.

This bucket is used only in this test and only for this table, I expect the S3 bucket to be empty when truncated. I also waited some time for Clickhouse to perform some sort of "garbage collection" but it seems not happening.

This will be an issue if we use S3 disks for production and the 'garbage' size keeps growing (storage costs $$). 

Does Clickhouse have any mechanism to detect and collect garbage (unused S3 objects)?

Thanks for any insights if I missed something.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
