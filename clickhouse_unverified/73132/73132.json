{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 73132,
  "instance_id": "ClickHouse__ClickHouse-73132",
  "issue_numbers": [
    "72710"
  ],
  "base_commit": "c11dc6b03e5ba8fdeae755c7f0cb2c98953ab276",
  "patch": "diff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 98d83972fd9a..f5a5d8524f3f 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -2425,10 +2425,10 @@ UInt128 IMergeTreeDataPart::getPartBlockIDHash() const\n     return hash.get128();\n }\n \n-String IMergeTreeDataPart::getZeroLevelPartBlockID(std::string_view token) const\n+String IMergeTreeDataPart::getNewPartBlockID(std::string_view token) const\n {\n-    if (info.level != 0)\n-        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Trying to get block id for non zero level part {}\", name);\n+    if (info.min_block != info.max_block)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Trying to get block id for part {} that contains more than one block\", name);\n \n     if (token.empty())\n     {\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex ec949699236e..b269deb34078 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -227,7 +227,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     /// Compute part block id for zero level part. Otherwise throws an exception.\n     /// If token is not empty, block id is calculated based on it instead of block data\n     UInt128 getPartBlockIDHash() const;\n-    String getZeroLevelPartBlockID(std::string_view token) const;\n+    String getNewPartBlockID(std::string_view token) const;\n \n     void setName(const String & new_name);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 27ba87a488bd..4187a01da8c4 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -507,7 +507,11 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n \n     MergeTreePartition partition(block_with_partition.partition);\n \n-    MergeTreePartInfo new_part_info(partition.getID(metadata_snapshot->getPartitionKey().sample_block), block_number, block_number, 0);\n+    bool optimize_on_insert = context->getSettingsRef()[Setting::optimize_on_insert] && data.merging_params.mode != MergeTreeData::MergingParams::Ordinary;\n+    UInt32 new_part_level = optimize_on_insert ? 1 : 0;\n+\n+    MergeTreePartInfo new_part_info(partition.getID(metadata_snapshot->getPartitionKey().sample_block), block_number, block_number, new_part_level);\n+\n     String part_name;\n     if (data.format_version < MERGE_TREE_DATA_MIN_FORMAT_VERSION_WITH_CUSTOM_PARTITIONING)\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeSink.cpp b/src/Storages/MergeTree/MergeTreeSink.cpp\nindex dfddcb311f4b..141128e86d38 100644\n--- a/src/Storages/MergeTree/MergeTreeSink.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSink.cpp\n@@ -224,7 +224,7 @@ void MergeTreeSink::finishDelayedChunk()\n \n             if (settings[Setting::insert_deduplicate] && deduplication_log)\n             {\n-                const String block_id = part->getZeroLevelPartBlockID(partition.block_dedup_token);\n+                const String block_id = part->getNewPartBlockID(partition.block_dedup_token);\n                 auto res = deduplication_log->addPart(block_id, part->info);\n                 if (!res.second)\n                 {\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\nindex 8d3f4e8812ec..691b89ab14ae 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n@@ -396,7 +396,7 @@ void ReplicatedMergeTreeSinkImpl<async_insert>::consume(Chunk & chunk)\n             {\n                 /// We add the hash from the data and partition identifier to deduplication ID.\n                 /// That is, do not insert the same data to the same partition twice.\n-                block_id = temp_part.part->getZeroLevelPartBlockID(block_dedup_token);\n+                block_id = temp_part.part->getNewPartBlockID(block_dedup_token);\n                 LOG_DEBUG(log, \"Wrote block with ID '{}', {} rows{}\", block_id, current_block.block.rows(), quorumLogMessage(replicas_num));\n             }\n             else\n@@ -607,6 +607,10 @@ bool ReplicatedMergeTreeSinkImpl<false>::writeExistingPart(MergeTreeData::Mutabl\n \n     try\n     {\n+        bool keep_non_zero_level = storage.merging_params.mode != MergeTreeData::MergingParams::Ordinary;\n+        part->info.level = (keep_non_zero_level && part->info.level > 0) ? 1 : 0;\n+        part->info.mutation = 0;\n+\n         part->version.setCreationTID(Tx::PrehistoricTID, nullptr);\n         String block_id = deduplicate ? fmt::format(\"{}_{}\", part->info.partition_id, part->checksums.getTotalChecksumHex()) : \"\";\n         bool deduplicated = commitPart(zookeeper, part, block_id, replicas_num).second;\n@@ -920,8 +924,6 @@ std::pair<std::vector<String>, bool> ReplicatedMergeTreeSinkImpl<async_insert>::\n         /// Set part attributes according to part_number.\n         part->info.min_block = block_number;\n         part->info.max_block = block_number;\n-        part->info.level = 0;\n-        part->info.mutation = 0;\n \n         part->setName(part->getNewName(part->info));\n         retry_context.actual_part_name = part->name;\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 408106751804..d3a0ac94481f 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -2730,7 +2730,9 @@ void StorageMergeTree::fillNewPartNameAndResetLevel(MutableDataPartPtr & part, D\n {\n     part->info.min_block = part->info.max_block = increment.get();\n     part->info.mutation = 0;\n-    part->info.level = 0;\n+\n+    bool keep_non_zero_level = merging_params.mode != MergeTreeData::MergingParams::Ordinary;\n+    part->info.level = (keep_non_zero_level && part->info.level > 0) ? 1 : 0;\n     part->setName(part->getNewName(part->info));\n }\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01861_explain_pipeline.sql b/tests/queries/0_stateless/01861_explain_pipeline.sql\nindex 99ea52ebfa4e..cd42e4bc5a17 100644\n--- a/tests/queries/0_stateless/01861_explain_pipeline.sql\n+++ b/tests/queries/0_stateless/01861_explain_pipeline.sql\n@@ -1,5 +1,6 @@\n DROP TABLE IF EXISTS test;\n CREATE TABLE test(a Int, b Int) Engine=ReplacingMergeTree order by a SETTINGS index_granularity = 8192, index_granularity_bytes = '10Mi';\n+set optimize_on_insert = 0;\n INSERT INTO test select number, number from numbers(5);\n INSERT INTO test select number, number from numbers(5,2);\n set max_threads =1;\ndiff --git a/tests/queries/0_stateless/02780_final_streams_data_skipping_index.sql b/tests/queries/0_stateless/02780_final_streams_data_skipping_index.sql\nindex 7de7a58e2e10..56bc073ecc9e 100644\n--- a/tests/queries/0_stateless/02780_final_streams_data_skipping_index.sql\n+++ b/tests/queries/0_stateless/02780_final_streams_data_skipping_index.sql\n@@ -12,6 +12,7 @@ ORDER BY key\n SETTINGS index_granularity=8192, min_bytes_for_wide_part=0, min_rows_for_wide_part=0;\n \n SYSTEM STOP MERGES data;\n+SET optimize_on_insert = 0;\n \n -- generate 50% of marks that cannot be skipped with v1_index\n -- this will create a gap in marks\ndiff --git a/tests/queries/0_stateless/03283_optimize_on_insert_level.reference b/tests/queries/0_stateless/03283_optimize_on_insert_level.reference\nnew file mode 100644\nindex 000000000000..3cac4df7b40b\n--- /dev/null\n+++ b/tests/queries/0_stateless/03283_optimize_on_insert_level.reference\n@@ -0,0 +1,14 @@\n+all_1_1_1\t1\t2\n+all_1_1_1\t2\t3\n+all_2_2_1\t4\t3\n+all_2_2_1\t5\t4\n+0\n+all_3_3_1\n+all_4_4_1\n+all_0_0_1\t1\t2\n+all_0_0_1\t2\t3\n+all_1_1_1\t4\t3\n+all_1_1_1\t5\t4\n+0\n+all_3_3_1\n+all_4_4_1\ndiff --git a/tests/queries/0_stateless/03283_optimize_on_insert_level.sql b/tests/queries/0_stateless/03283_optimize_on_insert_level.sql\nnew file mode 100644\nindex 000000000000..70b758df0768\n--- /dev/null\n+++ b/tests/queries/0_stateless/03283_optimize_on_insert_level.sql\n@@ -0,0 +1,42 @@\n+SET insert_keeper_fault_injection_probability = 0;\n+SET max_threads = 4;\n+\n+DROP TABLE IF EXISTS t_optimize_level;\n+\n+CREATE TABLE t_optimize_level (a UInt64, b UInt64)\n+ENGINE = ReplacingMergeTree ORDER BY a\n+SETTINGS index_granularity = 1;\n+\n+SYSTEM STOP MERGES t_optimize_level;\n+\n+INSERT INTO t_optimize_level VALUES (1, 1) (1, 2) (2, 3);\n+INSERT INTO t_optimize_level VALUES (4, 3) (5, 4);\n+\n+SELECT _part, a, b FROM t_optimize_level ORDER BY a;\n+SELECT count() FROM (EXPLAIN PIPELINE SELECT a, b FROM t_optimize_level FINAL) WHERE explain LIKE '%Replacing%';\n+\n+ALTER TABLE t_optimize_level DETACH PARTITION tuple();\n+ALTER TABLE t_optimize_level ATTACH PARTITION tuple();\n+\n+SELECT name FROM system.parts WHERE database = currentDatabase() AND table = 't_optimize_level' AND active;\n+\n+DROP TABLE t_optimize_level;\n+\n+CREATE TABLE t_optimize_level (a UInt64, b UInt64)\n+ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{database}/03283_optimize_on_insert_level', '1') ORDER BY a\n+SETTINGS index_granularity = 1;\n+\n+SYSTEM STOP MERGES t_optimize_level;\n+\n+INSERT INTO t_optimize_level VALUES (1, 1) (1, 2) (2, 3);\n+INSERT INTO t_optimize_level VALUES (4, 3) (5, 4);\n+\n+SELECT _part, a, b FROM t_optimize_level ORDER BY a;\n+SELECT count() FROM (EXPLAIN PIPELINE SELECT a, b FROM t_optimize_level FINAL) WHERE explain LIKE '%Replacing%';\n+\n+ALTER TABLE t_optimize_level DETACH PARTITION tuple();\n+ALTER TABLE t_optimize_level ATTACH PARTITION tuple();\n+\n+SELECT name FROM system.parts WHERE database = currentDatabase() AND table = 't_optimize_level' AND active;\n+\n+DROP TABLE t_optimize_level;\n",
  "problem_statement": "Create part with level 1 if `optimize_on_insert` is enabled\n**Use case**\r\n\r\nUse optimizations of merges and queries with `FINAL` that have an assumption that keys are unique in parts with levels >= 1 (e.g. `split_parts_ranges_into_intersecting_and_non_intersecting_final`).\r\n\r\n**Describe the solution you'd like**\r\n\r\nIf `optimize_on_insert` is enabled write part with level 1 instead of 0 because it doesn't contain duplicates. Otherwise the work for optimizing data which is done on insert is kind of useless.\r\n\n",
  "hints_text": "",
  "created_at": "2024-12-11T14:30:03Z"
}