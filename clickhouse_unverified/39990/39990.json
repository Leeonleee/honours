{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 39990,
  "instance_id": "ClickHouse__ClickHouse-39990",
  "issue_numbers": [
    "39915"
  ],
  "base_commit": "2a62910fb59fd8bcc21e183d4aab208ba6661003",
  "patch": "diff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex 4bf44689ba2a..637756fed869 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -719,7 +719,7 @@ void Fetcher::downloadBaseOrProjectionPartToDisk(\n                 \"This may happen if we are trying to download part from malicious replica or logical error.\",\n                 absolute_file_path, data_part_storage_builder->getRelativePath());\n \n-        auto file_out = data_part_storage_builder->writeFile(file_name, file_size, {});\n+        auto file_out = data_part_storage_builder->writeFile(file_name, std::min<UInt64>(file_size, DBMS_DEFAULT_BUFFER_SIZE), {});\n         HashingWriteBuffer hashing_out(*file_out);\n         copyDataWithThrottler(in, hashing_out, file_size, blocker.getCounter(), throttler);\n \n",
  "test_patch": "diff --git a/tests/integration/test_fetch_memory_usage/__init__.py b/tests/integration/test_fetch_memory_usage/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_fetch_memory_usage/configs/config.xml b/tests/integration/test_fetch_memory_usage/configs/config.xml\nnew file mode 100644\nindex 000000000000..8cfdcf973f4a\n--- /dev/null\n+++ b/tests/integration/test_fetch_memory_usage/configs/config.xml\n@@ -0,0 +1,3 @@\n+<clickhouse>\n+    <max_server_memory_usage>1000000000</max_server_memory_usage>\n+</clickhouse>\ndiff --git a/tests/integration/test_fetch_memory_usage/test.py b/tests/integration/test_fetch_memory_usage/test.py\nnew file mode 100644\nindex 000000000000..a43711401507\n--- /dev/null\n+++ b/tests/integration/test_fetch_memory_usage/test.py\n@@ -0,0 +1,59 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node = cluster.add_instance(\n+    \"node\", main_configs=[\"configs/config.xml\"], with_zookeeper=True\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_huge_column(started_cluster):\n+\n+    if (\n+        node.is_built_with_thread_sanitizer()\n+        or node.is_built_with_memory_sanitizer()\n+        or node.is_built_with_address_sanitizer()\n+    ):\n+        pytest.skip(\"sanitizer build has higher memory consumption; also it is slow\")\n+\n+    # max_server_memory_usage is set to 1GB\n+    # Added column should be 1e6 * 2000 ~= 2GB\n+    # ALTER ADD COLUMN + MATRIALIZE COLUMN should not cause big memory consumption\n+    node.query(\n+        \"\"\"\n+        create table test_fetch (x UInt64) engine = ReplicatedMergeTree('/clickhouse/tables/test_fetch', 'r1') order by x settings index_granularity=1024;\n+        insert into test_fetch select number from numbers(1e6);\n+\n+        set mutations_sync=1;\n+        alter table test_fetch add column y String default repeat(' ', 2000) CODEC(NONE);\n+        alter table test_fetch materialize column y;\n+\n+        create table test_fetch2 (x UInt64, y String default repeat(' ', 2000) CODEC(NONE)) engine = ReplicatedMergeTree('/clickhouse/tables/test_fetch', 'r2') order by x settings index_granularity=1024;\n+    \"\"\"\n+    )\n+\n+    # Here we just check that fetch has started.\n+    node.query(\n+        \"\"\"\n+        set receive_timeout=1;\n+        system sync replica test_fetch2;\n+    \"\"\",\n+        ignore_error=True,\n+    )\n+\n+    # Here we check that fetch did not use too much memory.\n+    # See https://github.com/ClickHouse/ClickHouse/issues/39915\n+    maybe_exception = node.query(\n+        \"select last_exception from system.replication_queue where last_exception like '%Memory limit%';\"\n+    )\n+    assert maybe_exception == \"\"\n",
  "problem_statement": "Do replicated fetches use up RAM and how to prevent it?\nWe have a big replicated table (~15TB/server compressed) with two replicas. When I tried to add two more replicas on new servers, they started to restart because of OOM:\r\n![image](https://user-images.githubusercontent.com/979624/183004927-32fbbf77-854b-4bc0-805e-59d8832e7922.png)\r\n\r\nI tried to set the `max_replicated_fetches_network_bandwidth` setting on the new replicas but it didn't help; the RAM usage continues to grow, although more slowly, until it eventually blows up again:\r\n![image](https://user-images.githubusercontent.com/979624/183005247-27a1e354-42cc-465e-8b7c-f2d302e65735.png)\r\n\r\nI was looking for what's causing it and it appears Clickhouse is fetching the parts into memory first, as the numbers of the below query correlate with the RAM usage reported by the system:\r\n```\r\nSELECT\r\n    database,\r\n    table,\r\n    formatReadableSize(sum(total_size_bytes_compressed)) AS total,\r\n    formatReadableSize(sum(bytes_read_compressed)) AS read\r\nFROM system.replicated_fetches\r\nGROUP BY\r\n    database,\r\n    table\r\n\r\nQuery id: f7d90b30-04a8-441f-a809-ed8c9f8fdf69\r\n\r\n\u250c\u2500database\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500total\u2500\u2500\u2500\u2500\u252c\u2500read\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 xxxxxxxxxx_20210401 \u2502 xxxxxxxxxxxxxxxx \u2502 2.29 TiB \u2502 1.17 TiB \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.004 sec. \r\n```\r\nI can't see any setting that would limit the RAM usage by replicated fetches, and it's also quite unexpected that RAM is used instead of writing the replicated parts straight to the disk. Am I missing something?\n",
  "hints_text": "As far as I recall, the data parts are not buffered in memory while being fetched.\r\n\r\nAlthough there can be unusual scenarios when fetching the data part will lead to the growth of memory consumption. Maybe you're using a very small index granularity? Or do you have a very large primary key? Or is there an unusual case of projections usage?\r\n\r\nPlease tell more details about the tables and the usage scenario...\n```\r\nCREATE TABLE IF NOT EXISTS xxxxxxxxxxxxxxxxxxx.xxxxxxxxxxxxxxxx \r\n      (\r\n        `c1` String CODEC(ZSTD(1)),\r\n        `c2` UInt64 CODEC(ZSTD(1)),\r\n        `c3` Array(UInt64) CODEC(ZSTD(1)),\r\n        `c4` Enum8('e0' = 0, 'e1' = 1, 'e2' = 2, 'e3' = 3, 'e4' = 4, 'e5' = 5, 'e6' = 6, 'e7' = 7, 'e8' = 8, 'e9' = 9) CODEC(ZSTD(1)),\r\n        `c5` Enum8('e0' = 0, 'e1' = 1) CODEC(ZSTD(1)),\r\n        `c6` Enum8('e0' = 0, 'e1' = 1, 'e2' = 2, 'e3' = 3, 'e4' = 4, 'e5' = 5, 'e6' = 6, 'e7' = 7, 'e8' = 8, 'e9' = 9, 'e10' = 10, 'e11' = 11, 'e12' = 12, 'e13' = 13, 'e14' = 14, 'e15' = 15, 'e16' = 16, 'e17' = 17, 'e18' = 18, 'e19' = 19, 'e20' = 20, 'e21' = 21, 'e22' = 22, 'e23' = 23, 'e24' = 24, 'e25' = 25, 'e26' = 26, 'e27' = 27, 'e28' = 28, 'e29' = 29, 'e30' = 30, 'e31' = 31, 'e32' = 32, 'e33' = 33, 'e34' = 34, 'e35' = 35, 'e36' = 36, 'e37' = 37, 'e38' = 38, 'e39' = 39, 'e40' = 40, 'e41' = 41, 'e42' = 42, 'e43' = 43, 'e44' = 44, 'e45' = 45, 'e46' = 46, 'e47' = 47, 'e48' = 48, 'e49' = 49, 'e50' = 50, 'e51' = 51, 'e52' = 52, 'e53' = 53, 'e54' = 54, 'e55' = 55, 'e56' = 56, 'e57' = 57, 'e58' = 58, 'e59' = 59, 'e60' = 60, 'e61' = 61, 'e62' = 62, 'e63' = 63, 'e64' = 64, 'e65' = 65, 'e66' = 66, 'e67' = 67, 'e68' = 68, 'e69' = 69, 'e70' = 70, 'e71' = 71, 'e72' = 72, 'e73' = 73, 'e74' = 74, 'e75' = 75, 'e76' = 76, 'e77' = 77, 'e78' = 78, 'e79' = 79, 'e80' = 80, 'e81' = 81, 'e82' = 82, 'e83' = 83, 'e84' = 84, 'e85' = 85, 'e86' = 86, 'e87' = 87, 'e88' = 88, 'e89' = 89, 'e90' = 90, 'e91' = 91, 'e92' = 92, 'e93' = 93, 'e94' = 94, 'e95' = 95, 'e96' = 96, 'e97' = 97, 'e98' = 98, 'e99' = 99, 'e100' = 100, 'e101' = 101, 'e102' = 102, 'e103' = 103, 'e104' = 104, 'e105' = 105, 'e106' = 106, 'e107' = 107, 'e108' = 108, 'e109' = 109, 'e110' = 110, 'e111' = 111, 'e112' = 112, 'e113' = 113, 'e114' = 114, 'e115' = 115, 'e116' = 116, 'e117' = 117, 'e118' = 118, 'e119' = 119, 'e120' = 120, 'e121' = 121, 'e122' = 122, 'e123' = 123, 'e124' = 124, 'e125' = 125, 'e126' = 126, 'e127' = 127, 'e-128' = -128, 'e-127' = -127, 'e-126' = -126, 'e-125' = -125, 'e-124' = -124, 'e-123' = -123, 'e-122' = -122, 'e-121' = -121, 'e-120' = -120, 'e-119' = -119, 'e-118' = -118, 'e-117' = -117, 'e-116' = -116, 'e-115' = -115, 'e-114' = -114, 'e-113' = -113, 'e-112' = -112, 'e-111' = -111, 'e-110' = -110, 'e-109' = -109, 'e-108' = -108, 'e-107' = -107, 'e-106' = -106, 'e-105' = -105, 'e-104' = -104, 'e-103' = -103, 'e-102' = -102, 'e-101' = -101, 'e-100' = -100, 'e-99' = -99, 'e-98' = -98, 'e-97' = -97, 'e-96' = -96, 'e-95' = -95, 'e-94' = -94, 'e-93' = -93, 'e-92' = -92, 'e-91' = -91, 'e-90' = -90, 'e-89' = -89, 'e-88' = -88, 'e-87' = -87, 'e-86' = -86, 'e-85' = -85, 'e-84' = -84, 'e-83' = -83, 'e-82' = -82, 'e-81' = -81, 'e-80' = -80, 'e-79' = -79, 'e-78' = -78, 'e-77' = -77, 'e-76' = -76, 'e-75' = -75, 'e-74' = -74, 'e-73' = -73, 'e-72' = -72, 'e-71' = -71, 'e-70' = -70, 'e-69' = -69, 'e-68' = -68, 'e-67' = -67, 'e-66' = -66, 'e-65' = -65, 'e-64' = -64, 'e-63' = -63, 'e-62' = -62, 'e-61' = -61, 'e-60' = -60, 'e-59' = -59, 'e-58' = -58, 'e-57' = -57, 'e-56' = -56, 'e-55' = -55, 'e-54' = -54, 'e-53' = -53, 'e-52' = -52, 'e-51' = -51, 'e-50' = -50, 'e-49' = -49, 'e-48' = -48, 'e-47' = -47, 'e-46' = -46, 'e-45' = -45, 'e-44' = -44, 'e-43' = -43, 'e-42' = -42, 'e-41' = -41, 'e-40' = -40, 'e-39' = -39, 'e-38' = -38, 'e-37' = -37, 'e-36' = -36, 'e-35' = -35, 'e-34' = -34, 'e-33' = -33, 'e-32' = -32, 'e-31' = -31, 'e-30' = -30, 'e-29' = -29, 'e-28' = -28, 'e-27' = -27, 'e-26' = -26, 'e-25' = -25, 'e-24' = -24, 'e-23' = -23, 'e-22' = -22, 'e-21' = -21, 'e-20' = -20, 'e-19' = -19, 'e-18' = -18, 'e-17' = -17, 'e-16' = -16, 'e-15' = -15, 'e-14' = -14, 'e-13' = -13, 'e-12' = -12, 'e-11' = -11, 'e-10' = -10, 'e-9' = -9, 'e-8' = -8, 'e-7' = -7) CODEC(ZSTD(1)),\r\n        `c7` FixedString(6) CODEC(ZSTD(1)),\r\n        `c8` DateTime CODEC(ZSTD(1)),\r\n        `c9` String CODEC(ZSTD(9,24)),\r\n        `c10` Nullable(UInt32) CODEC(ZSTD(1)),\r\n        `c11` Nullable(String) CODEC(ZSTD(1)),\r\n        `c12` Nullable(String) CODEC(ZSTD(1)),\r\n        `c13` UInt8 CODEC(ZSTD(1)))\r\n      ENGINE ReplicatedReplacingMergeTree('{zk}:/clickhouse/tables/xxxxxxxxxxxxxxxxxxx/xxxxxxxxxx/{replica}-{shard}', '3')\r\n      PARTITION BY (c13, c4, c5) \r\n      ORDER BY (c2, c1, c6, c7, c8)\r\n      SETTINGS\r\n                     storage_policy='jbod_policy_1',\r\n                     index_granularity=128,\r\n                     min_compress_block_size=67108864,\r\n                     max_compress_block_size=67108864,\r\n                     merge_max_block_size=128,\r\n                     max_replicated_fetches_network_bandwidth = 67108864;\r\n```\n> Maybe you're using a very small index granularity?\r\n\r\nIs 128 too small?\r\n\r\n> Or do you have a very large primary key?\r\n\r\nI checked stats and PK is 40 bytes on average.\r\n\r\n> Or is there an unusual case of projections usage?\r\n\r\nWe don't use projections AFAIK, and certainly not with this table.\r\n\r\n> Please tell more details about the tables and the usage scenario...\r\n\r\nThis table holds raw data in the column `c9`; its main purpose is to have the archive of the raw source data to be able to re-process it later if needed, and it's also used sometimes for point lookups to check the data for a particular key.\nI checked one part reported by `system.replicated_fetches` and the `result_part_path` does not even exist on disk so I assume it's held in memory:\r\n```\r\nSELECT * EXCEPT (database, table, source_replica_path, source_replica_hostname, source_replica_port, URI)\r\nFROM system.replicated_fetches\r\nORDER BY progress DESC\r\nLIMIT 1\r\n\r\nQuery id: ef1362c7-e963-4c03-9c06-b30e726cf6de\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nelapsed:                     3329.285516327\r\nprogress:                    0.18444289146471013\r\nresult_part_name:            32-4-0_763_56845_13_67141\r\nresult_part_path:            /disk17/clickhouse/store/317/317ee90a-beef-4549-87d3-202e1551506e/32-4-0_763_56845_13_67141/\r\npartition_id:                32-4-0\r\ntotal_size_bytes_compressed: 53348964039\r\nbytes_read_compressed:       9839837184\r\ninterserver_scheme:          http\r\nto_detached:                 0\r\nthread_id:                   15672\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n```\r\n\r\n```\r\n$ ls -l /disk17/clickhouse/store/317/317ee90a-beef-4549-87d3-\r\n202e1551506e/32-4-0_763_56845_13_67141/\r\nls: cannot access '/disk17/clickhouse/store/317/317ee90a-beef-4549-87d3-202e1551506e/32-4-0_763_56845_13_67141/': No such file or directory\r\n```\n128 can be too granular, with 40-byte per key it will be 0.3 bytes per record; for a trillion-row table, it will be 300 GB of RAM.\nThe parts are downloaded into the `tmp_fetch_*` directory, then renamed.\nThank you for the pointers!\r\nI checked the `tmp_fetch_*` directory and it seems Clickhouse does not flush the BIN file for the `` `c9` String CODEC(ZSTD(9,24)) `` column (the biggest one with the actual raw data) until it's finished fetching it.\r\n```\r\nlsof /disk17/clickhouse/store/317/317ee90a-beef-4549-87d3-2\r\n02e1551506e/tmp-fetch_32-4-0_763_56845_13_67141/c9.bin\r\nCOMMAND     PID       USER   FD   TYPE DEVICE SIZE/OFF      NODE NAME\r\nclickhous 15455 clickhouse 3195w   REG 259,21        0 324601015 /disk17/clickhouse/store/317/317ee90a-beef-4549-87d3-202e1551506e/tmp-fetch_32-4-0_763_56845_13_67141/c9.bin\r\n```\nhttps://github.com/ClickHouse/ClickHouse/blob/0492ef63a40673b457e72892cfc3226245a41f65/src/Storages/MergeTree/DataPartsExchange.cpp#L722\r\nDoes this mean the buffer size = the file size?\nI figured out the second parameter is indeed the buffer size:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/42ea79c266850239c5640a3f4ea34531b92ba62c/src/Storages/MergeTree/DataPartStorageOnDisk.cpp#L800-L803\r\nSo passing the file size (which in our case is 50-100GB) as the buffer size will lead to the described behaviour. It's a bug.\n@cyberhuman so the issue is on a receiver only? No issue at a sender? What is the exact Clickhouse version?\nAlso please share the result from OOMed server of\r\n\r\n```sql\r\nSELECT\r\n    event_date,\r\n    formatReadableSize(maxIf(value, metric = 'MemoryResident')) AS maxMemoryResident,\r\n    formatReadableSize(avgIf(value, metric = 'MemoryResident')) AS avgMemoryResident,\r\n    formatReadableSize(maxIf(value, metric = 'MemoryVirtual')) AS maxMemoryVirtual,\r\n    formatReadableSize(avgIf(value, metric = 'MemoryVirtual')) AS avgMemoryVirtual\r\nFROM asynchronous_metric_log\r\nWHERE (metric IN ('MemoryResident', 'MemoryVirtual')) AND (event_date >= (today() - 3))\r\nGROUP BY event_date\r\nORDER BY event_date ASC\r\n```\r\n\nBTW, I don't see anything like this with 21.8. \r\nI am adding 25TB replica right now, and my CH fetches files \r\n\r\n```sql\r\nSELECT formatReadableSize(column_bytes_on_disk)\r\nFROM system.parts_columns\r\nORDER BY column_bytes_on_disk DESC\r\nLIMIT 5\r\n\u250c\u2500formatReadableSize(column_bytes_on_disk)\u2500\u2510\r\n\u2502 44.05 GiB                                \u2502\r\n\u2502 34.19 GiB                                \u2502\r\n\u2502 29.84 GiB                                \u2502\r\n\u2502 27.80 GiB                                \u2502\r\n\u2502 24.81 GiB                                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd memory usage of CH process is about 800MB (I see this in top)\r\n\r\n\n> @cyberhuman so the issue is on a receiver only?\r\n\r\nCurrently yes.\r\n\r\n> No issue at a sender?\r\n\r\nI can't say for sure because one day ago there was some issue initially but because it was affecting a live system unfortunately I did not have time to investigate and had to remove the replica. So I would tackle the issue with the receiver first.\r\n\r\n> What is the exact Clickhouse version?\r\n\r\n22.7.1\r\n\n```\r\nSELECT\r\n...\r\nFROM asynchronous_metric_log\r\n```\r\nunfortunately we have `asynchronous_metric_log` disabled in production.\n> BTW, I don't see anything like this with 21.8.\r\n\r\nObviously, because it is only broken since 22.7.1: 9133e398b8caa8af019dc3603a019e092085a336\nI mean the graphs from your first messages show 2.5TB, what is 2.5TB ? Virtual or Resident memory ?\nIt's total resident memory by all processes. The server has 2TiB of RAM.\n> It's total resident memory by all processes. The server has 2TiB of RAM.\r\n\r\nI see. That is quite unexpected and your graphs don't have a legend.\nrepro\r\n\r\n```sql\r\n-- Tags: long, replica, no-replicated-database\r\n\r\nDROP TABLE IF EXISTS fetch_test_huge;\r\nDROP TABLE IF EXISTS fetch_test_huge1;\r\n\r\ncreate table fetch_test_huge (A Int64, huge_column String CODEC(NONE)) Engine=ReplicatedMergeTree('/clickhouse/tables/{database}/fetch_test_huge', 'r1') order by A;\r\ninsert into fetch_test_huge select number, repeat(' ', 2000) from numbers(1e7) settings max_block_size=8192;\r\nsystem sync replica fetch_test_huge;\r\noptimize table fetch_test_huge final;\r\ncreate table fetch_test_huge1(A Int64, huge_column String CODEC(NONE)) Engine=ReplicatedMergeTree('/clickhouse/tables/{database}/fetch_test_huge', 'r2') order by A settings max_replicated_fetches_network_bandwidth =100000000;\r\nsystem sync replica fetch_test_huge;\r\nsystem sync replica fetch_test_huge1;\r\nselect count() from fetch_test_huge;\r\nselect count() from fetch_test_huge1;\r\nDROP TABLE IF EXISTS fetch_test_huge;\r\nDROP TABLE IF EXISTS fetch_test_huge1;\r\n```\n> https://github.com/ClickHouse/ClickHouse/blob/0492ef63a40673b457e72892cfc3226245a41f65/src/Storages/MergeTree/DataPartsExchange.cpp#L722\r\n> \r\n> \r\n> Does this mean the buffer size = the file size?\r\n\r\nThanks for investigation! That's indeed a bug. Do you want to make a contribution?\n@CurtizJ thank you for the confirmation!\r\nI'm verry sorry but I'm afraid I don't have time for a **proper** contribution. We applied and tested the following quickfix though:\r\n\r\n```diff\r\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\r\nindex 3609a65bc7..bca7e8e3a6 100644\r\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\r\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\r\n@@ -719,7 +719,7 @@ void Fetcher::downloadBaseOrProjectionPartToDisk(\r\n                 \"This may happen if we are trying to download part from malicious replica or logical error.\",\r\n                 absolute_file_path, data_part_storage_builder->getRelativePath());\r\n \r\n-        auto file_out = data_part_storage_builder->writeFile(file_name, file_size, {});\r\n+        auto file_out = data_part_storage_builder->writeFile(file_name, std::min(file_size, 16*1024*1024UL), {});\r\n         HashingWriteBuffer hashing_out(*file_out);\r\n         copyDataWithThrottler(in, hashing_out, file_size, blocker.getCounter(), throttler);\r\n ```",
  "created_at": "2022-08-08T13:34:09Z"
}