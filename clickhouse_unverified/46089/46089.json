{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 46089,
  "instance_id": "ClickHouse__ClickHouse-46089",
  "issue_numbers": [
    "45710"
  ],
  "base_commit": "d85ede23a37a92445f8dc87881d3516003e1c205",
  "patch": "diff --git a/docs/en/operations/server-configuration-parameters/settings.md b/docs/en/operations/server-configuration-parameters/settings.md\nindex 7c97d0ab6409..a9f0cc276ff5 100644\n--- a/docs/en/operations/server-configuration-parameters/settings.md\n+++ b/docs/en/operations/server-configuration-parameters/settings.md\n@@ -1045,7 +1045,7 @@ Default value: `0`.\n \n ## background_pool_size {#background_pool_size}\n \n-Sets the number of threads performing background merges and mutations for tables with MergeTree engines. This setting is also could be applied  at server startup from the `default` profile configuration for backward compatibility at the ClickHouse server start. You can only increase the number of threads at runtime. To lower the number of threads you have to restart the server. By adjusting this setting, you manage CPU and disk load. Smaller pool size utilizes less CPU and disk resources, but background processes advance slower which might eventually impact query performance.\n+Sets the number of threads performing background merges and mutations for tables with MergeTree engines. This setting is also could be applied at server startup from the `default` profile configuration for backward compatibility at the ClickHouse server start. You can only increase the number of threads at runtime. To lower the number of threads you have to restart the server. By adjusting this setting, you manage CPU and disk load. Smaller pool size utilizes less CPU and disk resources, but background processes advance slower which might eventually impact query performance.\n \n Before changing it, please also take a look at related MergeTree settings, such as [number_of_free_entries_in_pool_to_lower_max_size_of_merge](../../operations/settings/merge-tree-settings.md#number-of-free-entries-in-pool-to-lower-max-size-of-merge) and [number_of_free_entries_in_pool_to_execute_mutation](../../operations/settings/merge-tree-settings.md#number-of-free-entries-in-pool-to-execute-mutation).\n \n@@ -1063,8 +1063,8 @@ Default value: 16.\n \n ## background_merges_mutations_concurrency_ratio {#background_merges_mutations_concurrency_ratio}\n \n-Sets a ratio between the number of threads and the number of background merges and mutations that can be executed concurrently. For example if the ratio equals to 2 and\n-`background_pool_size` is set to 16 then ClickHouse can execute 32 background merges concurrently. This is possible, because background operation could be suspended and postponed. This is needed to give small merges more execution priority. You can only increase this ratio at runtime. To lower it you have to restart the server.\n+Sets a ratio between the number of threads and the number of background merges and mutations that can be executed concurrently. For example, if the ratio equals to 2 and\n+`background_pool_size` is set to 16 then ClickHouse can execute 32 background merges concurrently. This is possible, because background operations could be suspended and postponed. This is needed to give small merges more execution priority. You can only increase this ratio at runtime. To lower it you have to restart the server.\n The same as for `background_pool_size` setting `background_merges_mutations_concurrency_ratio` could be applied from the `default` profile for backward compatibility.\n \n Possible values:\n@@ -1079,6 +1079,33 @@ Default value: 2.\n <background_merges_mutations_concurrency_ratio>3</background_merges_mutations_concurrency_ratio>\n ```\n \n+## merges_mutations_memory_usage_soft_limit {#merges_mutations_memory_usage_soft_limit}\n+\n+Sets the limit on how much RAM is allowed to use for performing merge and mutation operations.\n+Zero means unlimited.\n+If ClickHouse reaches this limit, it won't schedule any new background merge or mutation operations but will continue to execute already scheduled tasks.\n+\n+Possible values:\n+\n+-   Any positive integer.\n+\n+**Example**\n+\n+```xml\n+<merges_mutations_memory_usage_soft_limit>0</merges_mutations_memory_usage_soft_limit>\n+```\n+\n+## merges_mutations_memory_usage_to_ram_ratio {#merges_mutations_memory_usage_to_ram_ratio}\n+\n+The default `merges_mutations_memory_usage_soft_limit` value is calculated as `memory_amount * merges_mutations_memory_usage_to_ram_ratio`.\n+\n+Default value: `0.5`.\n+\n+**See also**\n+\n+-   [max_memory_usage](../../operations/settings/query-complexity.md#settings_max_memory_usage)\n+-   [merges_mutations_memory_usage_soft_limit](#merges_mutations_memory_usage_soft_limit)\n+\n ## background_merges_mutations_scheduling_policy {#background_merges_mutations_scheduling_policy}\n \n Algorithm used to select next merge or mutation to be executed by background thread pool. Policy may be changed at runtime without server restart.\ndiff --git a/programs/server/Server.cpp b/programs/server/Server.cpp\nindex 164e1ce14e56..35e557fc9bc8 100644\n--- a/programs/server/Server.cpp\n+++ b/programs/server/Server.cpp\n@@ -135,6 +135,7 @@ namespace CurrentMetrics\n     extern const Metric Revision;\n     extern const Metric VersionInteger;\n     extern const Metric MemoryTracking;\n+    extern const Metric MergesMutationsMemoryTracking;\n     extern const Metric MaxDDLEntryID;\n     extern const Metric MaxPushedDDLEntryID;\n }\n@@ -1225,6 +1226,25 @@ try\n             total_memory_tracker.setDescription(\"(total)\");\n             total_memory_tracker.setMetric(CurrentMetrics::MemoryTracking);\n \n+            size_t merges_mutations_memory_usage_soft_limit = server_settings.merges_mutations_memory_usage_soft_limit;\n+\n+            size_t default_merges_mutations_server_memory_usage = static_cast<size_t>(memory_amount * server_settings.merges_mutations_memory_usage_to_ram_ratio);\n+            if (merges_mutations_memory_usage_soft_limit == 0 || merges_mutations_memory_usage_soft_limit > default_merges_mutations_server_memory_usage)\n+            {\n+                merges_mutations_memory_usage_soft_limit = default_merges_mutations_server_memory_usage;\n+                LOG_WARNING(log, \"Setting merges_mutations_memory_usage_soft_limit was set to {}\"\n+                    \" ({} available * {:.2f} merges_mutations_memory_usage_to_ram_ratio)\",\n+                    formatReadableSizeWithBinarySuffix(merges_mutations_memory_usage_soft_limit),\n+                    formatReadableSizeWithBinarySuffix(memory_amount),\n+                    server_settings.merges_mutations_memory_usage_to_ram_ratio);\n+            }\n+\n+            LOG_INFO(log, \"Merges and mutations memory limit is set to {}\",\n+                formatReadableSizeWithBinarySuffix(merges_mutations_memory_usage_soft_limit));\n+            background_memory_tracker.setSoftLimit(merges_mutations_memory_usage_soft_limit);\n+            background_memory_tracker.setDescription(\"(background)\");\n+            background_memory_tracker.setMetric(CurrentMetrics::MergesMutationsMemoryTracking);\n+\n             total_memory_tracker.setAllowUseJemallocMemory(server_settings.allow_use_jemalloc_memory);\n \n             auto * global_overcommit_tracker = global_context->getGlobalOvercommitTracker();\ndiff --git a/src/Common/CurrentMetrics.cpp b/src/Common/CurrentMetrics.cpp\nindex cfe9f41befeb..550fcd1ba771 100644\n--- a/src/Common/CurrentMetrics.cpp\n+++ b/src/Common/CurrentMetrics.cpp\n@@ -53,6 +53,7 @@\n     M(QueryThread, \"Number of query processing threads\") \\\n     M(ReadonlyReplica, \"Number of Replicated tables that are currently in readonly state due to re-initialization after ZooKeeper session loss or due to startup without ZooKeeper configured.\") \\\n     M(MemoryTracking, \"Total amount of memory (bytes) allocated by the server.\") \\\n+    M(MergesMutationsMemoryTracking, \"Total amount of memory (bytes) allocated by background tasks (merges and mutations).\") \\\n     M(EphemeralNode, \"Number of ephemeral nodes hold in ZooKeeper.\") \\\n     M(ZooKeeperSession, \"Number of sessions (connections) to ZooKeeper. Should be no more than one, because using more than one connection to ZooKeeper may lead to bugs due to lack of linearizability (stale reads) that ZooKeeper consistency model allows.\") \\\n     M(ZooKeeperWatch, \"Number of watches (event subscriptions) in ZooKeeper.\") \\\ndiff --git a/src/Common/MemoryTracker.cpp b/src/Common/MemoryTracker.cpp\nindex 674d8d469af4..9bff365483d3 100644\n--- a/src/Common/MemoryTracker.cpp\n+++ b/src/Common/MemoryTracker.cpp\n@@ -96,6 +96,7 @@ using namespace std::chrono_literals;\n static constexpr size_t log_peak_memory_usage_every = 1ULL << 30;\n \n MemoryTracker total_memory_tracker(nullptr, VariableContext::Global);\n+MemoryTracker background_memory_tracker(&total_memory_tracker, VariableContext::User);\n \n std::atomic<Int64> MemoryTracker::free_memory_in_allocator_arenas;\n \n@@ -528,3 +529,10 @@ void MemoryTracker::setOrRaiseProfilerLimit(Int64 value)\n     while ((value == 0 || old_value < value) && !profiler_limit.compare_exchange_weak(old_value, value))\n         ;\n }\n+\n+bool canEnqueueBackgroundTask()\n+{\n+    auto limit = background_memory_tracker.getSoftLimit();\n+    auto amount = background_memory_tracker.get();\n+    return limit == 0 || amount < limit;\n+}\ndiff --git a/src/Common/MemoryTracker.h b/src/Common/MemoryTracker.h\nindex 0d7748856bd7..260005fd5366 100644\n--- a/src/Common/MemoryTracker.h\n+++ b/src/Common/MemoryTracker.h\n@@ -110,6 +110,22 @@ class MemoryTracker\n         return amount.load(std::memory_order_relaxed);\n     }\n \n+    // Merges and mutations may pass memory ownership to other threads thus in the end of execution\n+    // MemoryTracker for background task may have a non-zero counter.\n+    // This method is intended to fix the counter inside of background_memory_tracker.\n+    // NOTE: We can't use alloc/free methods to do it, because they also will change the value inside\n+    // of total_memory_tracker.\n+    void adjustOnBackgroundTaskEnd(const MemoryTracker * child)\n+    {\n+        auto background_memory_consumption = child->amount.load(std::memory_order_relaxed);\n+        amount.fetch_sub(background_memory_consumption, std::memory_order_relaxed);\n+\n+        // Also fix CurrentMetrics::MergesMutationsMemoryTracking\n+        auto metric_loaded = metric.load(std::memory_order_relaxed);\n+        if (metric_loaded != CurrentMetrics::end())\n+            CurrentMetrics::sub(metric_loaded, background_memory_consumption);\n+    }\n+\n     Int64 getPeak() const\n     {\n         return peak.load(std::memory_order_relaxed);\n@@ -220,3 +236,6 @@ class MemoryTracker\n };\n \n extern MemoryTracker total_memory_tracker;\n+extern MemoryTracker background_memory_tracker;\n+\n+bool canEnqueueBackgroundTask();\ndiff --git a/src/Core/ServerSettings.h b/src/Core/ServerSettings.h\nindex aabc89cc6d75..2d8c37783db8 100644\n--- a/src/Core/ServerSettings.h\n+++ b/src/Core/ServerSettings.h\n@@ -40,6 +40,8 @@ namespace DB\n     M(String, temporary_data_in_cache, \"\", \"Cache disk name for temporary data.\", 0) \\\n     M(UInt64, max_server_memory_usage, 0, \"Limit on total memory usage. Zero means Unlimited.\", 0) \\\n     M(Double, max_server_memory_usage_to_ram_ratio, 0.9, \"Same as max_server_memory_usage but in to ram ratio. Allows to lower max memory on low-memory systems.\", 0) \\\n+    M(UInt64, merges_mutations_memory_usage_soft_limit, 0, \"Limit on total memory usage for merges and mutations. Zero means Unlimited.\", 0) \\\n+    M(Double, merges_mutations_memory_usage_to_ram_ratio, 0.5, \"Same as merges_mutations_memory_usage_soft_limit but in to ram ratio. Allows to lower memory limit on low-memory systems.\", 0) \\\n     M(Bool, allow_use_jemalloc_memory, true, \"Allows to use jemalloc memory.\", 0) \\\n     \\\n     M(UInt64, max_concurrent_queries, 0, \"Limit on total number of concurrently executed queries. Zero means Unlimited.\", 0) \\\ndiff --git a/src/Interpreters/ThreadStatusExt.cpp b/src/Interpreters/ThreadStatusExt.cpp\nindex fd4a6b5e9966..559652fe56c0 100644\n--- a/src/Interpreters/ThreadStatusExt.cpp\n+++ b/src/Interpreters/ThreadStatusExt.cpp\n@@ -84,6 +84,7 @@ ThreadGroupPtr ThreadGroup::createForBackgroundProcess(ContextPtr storage_contex\n     group->memory_tracker.setProfilerStep(settings.memory_profiler_step);\n     group->memory_tracker.setSampleProbability(settings.memory_profiler_sample_probability);\n     group->memory_tracker.setSoftLimit(settings.memory_overcommit_ratio_denominator);\n+    group->memory_tracker.setParent(&background_memory_tracker);\n     if (settings.memory_tracker_fault_probability > 0.0)\n         group->memory_tracker.setFaultProbability(settings.memory_tracker_fault_probability);\n \ndiff --git a/src/Storages/MergeTree/MergeList.cpp b/src/Storages/MergeTree/MergeList.cpp\nindex 0bf662921ad3..d54079bc7a53 100644\n--- a/src/Storages/MergeTree/MergeList.cpp\n+++ b/src/Storages/MergeTree/MergeList.cpp\n@@ -78,4 +78,9 @@ MergeInfo MergeListElement::getInfo() const\n     return res;\n }\n \n+MergeListElement::~MergeListElement()\n+{\n+    background_memory_tracker.adjustOnBackgroundTaskEnd(&getMemoryTracker());\n+}\n+\n }\ndiff --git a/src/Storages/MergeTree/MergeList.h b/src/Storages/MergeTree/MergeList.h\nindex 9c8c2ebd1e4e..d8271a66b45b 100644\n--- a/src/Storages/MergeTree/MergeList.h\n+++ b/src/Storages/MergeTree/MergeList.h\n@@ -113,6 +113,8 @@ struct MergeListElement : boost::noncopyable\n     MergeListElement * ptr() { return this; }\n \n     MergeListElement & ref() { return *this; }\n+\n+    ~MergeListElement();\n };\n \n /** Maintains a list of currently running merges.\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 71a826fbc229..d3549e1d8c24 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -7,6 +7,7 @@\n #include <base/sort.h>\n #include <Backups/BackupEntriesCollector.h>\n #include <Databases/IDatabase.h>\n+#include <Common/MemoryTracker.h>\n #include <Common/escapeForFileName.h>\n #include <Common/ProfileEventsScope.h>\n #include <Common/typeid_cast.h>\n@@ -41,6 +42,7 @@\n #include <Processors/QueryPlan/QueryPlan.h>\n #include <Processors/QueryPlan/BuildQueryPipelineSettings.h>\n #include <Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h>\n+#include <fmt/core.h>\n \n namespace DB\n {\n@@ -900,7 +902,14 @@ MergeMutateSelectedEntryPtr StorageMergeTree::selectPartsToMerge(\n \n     SelectPartsDecision select_decision = SelectPartsDecision::CANNOT_SELECT;\n \n-    if (partition_id.empty())\n+    if (!canEnqueueBackgroundTask())\n+    {\n+        if (out_disable_reason)\n+            *out_disable_reason = fmt::format(\"Current background tasks memory usage ({}) is more than the limit ({})\",\n+                formatReadableSizeWithBinarySuffix(background_memory_tracker.get()),\n+                formatReadableSizeWithBinarySuffix(background_memory_tracker.getSoftLimit()));\n+    }\n+    else if (partition_id.empty())\n     {\n         UInt64 max_source_parts_size = merger_mutator.getMaxSourcePartsSizeForMerge();\n         bool merge_with_ttl_allowed = getTotalMergesWithTTLInMergeList() < data_settings->max_number_of_merges_with_ttl_in_pool;\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 5e99426ba7b2..2175a5f2709d 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -5,6 +5,7 @@\n \n #include <base/hex.h>\n #include <Common/Macros.h>\n+#include <Common/MemoryTracker.h>\n #include <Common/ProfileEventsScope.h>\n #include <Common/StringUtils/StringUtils.h>\n #include <Common/ZooKeeper/KeeperException.h>\n@@ -3219,7 +3220,14 @@ void StorageReplicatedMergeTree::mergeSelectingTask()\n \n         auto merges_and_mutations_queued = queue.countMergesAndPartMutations();\n         size_t merges_and_mutations_sum = merges_and_mutations_queued.merges + merges_and_mutations_queued.mutations;\n-        if (merges_and_mutations_sum >= storage_settings_ptr->max_replicated_merges_in_queue)\n+        if (!canEnqueueBackgroundTask())\n+        {\n+            LOG_TRACE(log, \"Reached memory limit for the background tasks ({}), so won't select new parts to merge or mutate.\"\n+                \"Current background tasks memory usage: {}.\",\n+                formatReadableSizeWithBinarySuffix(background_memory_tracker.getSoftLimit()),\n+                formatReadableSizeWithBinarySuffix(background_memory_tracker.get()));\n+        }\n+        else if (merges_and_mutations_sum >= storage_settings_ptr->max_replicated_merges_in_queue)\n         {\n             LOG_TRACE(log, \"Number of queued merges ({}) and part mutations ({})\"\n                 \" is greater than max_replicated_merges_in_queue ({}), so won't select new parts to merge or mutate.\",\n",
  "test_patch": "diff --git a/tests/integration/test_merges_memory_limit/__init__.py b/tests/integration/test_merges_memory_limit/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_merges_memory_limit/test.py b/tests/integration/test_merges_memory_limit/test.py\nnew file mode 100644\nindex 000000000000..04729f3a01c0\n--- /dev/null\n+++ b/tests/integration/test_merges_memory_limit/test.py\n@@ -0,0 +1,38 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node = cluster.add_instance(\"node\")\n+\n+\n+@pytest.fixture(scope=\"module\", autouse=True)\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_memory_limit_success():\n+    node.query(\n+        \"CREATE TABLE test_merge_oom ENGINE=AggregatingMergeTree ORDER BY id EMPTY AS SELECT number%1024 AS id, arrayReduce( 'groupArrayState', arrayMap( x-> randomPrintableASCII(100), range(8192))) fat_state FROM numbers(20000)\"\n+    )\n+    node.query(\"SYSTEM STOP MERGES test_merge_oom\")\n+    node.query(\n+        \"INSERT INTO test_merge_oom SELECT number%1024 AS id, arrayReduce( 'groupArrayState', arrayMap( x-> randomPrintableASCII(100), range(8192))) fat_state FROM numbers(10000)\"\n+    )\n+    node.query(\n+        \"INSERT INTO test_merge_oom SELECT number%1024 AS id, arrayReduce( 'groupArrayState', arrayMap( x-> randomPrintableASCII(100), range(8192))) fat_state FROM numbers(10000)\"\n+    )\n+    node.query(\n+        \"INSERT INTO test_merge_oom SELECT number%1024 AS id, arrayReduce( 'groupArrayState', arrayMap( x-> randomPrintableASCII(100), range(8192))) fat_state FROM numbers(10000)\"\n+    )\n+    _, error = node.query_and_get_answer_with_error(\n+        \"SYSTEM START MERGES test_merge_oom;SET optimize_throw_if_noop=1;OPTIMIZE TABLE test_merge_oom FINAL\"\n+    )\n+\n+    assert not error\n+    node.query(\"DROP TABLE test_merge_oom\")\n",
  "problem_statement": "Soft memory limits for merges and mutations.\n**Use case**\r\n\r\nA user can specify too large `background_pool_size`, and merges can constantly fail and make no progress due to memory exhaustion.\r\n\r\nHorizontal merges of wide tables with many `LowCardinality` or `AggregateFunction` columns can have high memory usage.\r\n\r\nMutations can have high memory usage if they use complex expressions for updating or deleting.\r\n\r\nWe should not let too many merges be processed at once if the memory usage is high.\r\n\r\n\r\n**Implementation proposal**\r\n\r\nAdd two global configuration options:\r\n\r\n`merges_memory_usage_soft_limit` and `merges_memory_usage_soft_limit_to_ram_ratio`\r\n\r\nsimilarly to `max_server_memory_usage` and `max_server_memory_usage_to_ram_ratio`. The latter setting should be 0.5 by default.\r\n\r\nTrack the total memory usage by merges.\r\n\r\nCaveat: it cannot be done with MemoryTracker. Because a merge operation can free some memory (primary keys) that were allocated outside of the context of the merge, the resulting memory usage at the end of the merge can be negative. We cannot reset it to zero at the end of the merge operation because the MemoryTracker of the merge is attached to the server's global MemoryTracker and should be consistent with it. The only reasonable way is to track the sum of memory usage by active merges separately.\r\n\r\nIf the current memory usage by merges is higher than the soft limit, do not start new merges.\n",
  "hints_text": "",
  "created_at": "2023-02-06T17:34:46Z"
}