{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 50612,
  "instance_id": "ClickHouse__ClickHouse-50612",
  "issue_numbers": [
    "38554",
    "37119"
  ],
  "base_commit": "0629b8679b934e38477e67bd07d319498cd496d1",
  "patch": "diff --git a/docs/en/operations/settings/merge-tree-settings.md b/docs/en/operations/settings/merge-tree-settings.md\nindex 483dcf2e61cf..6e285f4dc7e4 100644\n--- a/docs/en/operations/settings/merge-tree-settings.md\n+++ b/docs/en/operations/settings/merge-tree-settings.md\n@@ -555,7 +555,7 @@ Merge reads rows from parts in blocks of `merge_max_block_size` rows, then merge\n \n ## number_of_free_entries_in_pool_to_lower_max_size_of_merge {#number-of-free-entries-in-pool-to-lower-max-size-of-merge}\n \n-When there is less than specified number of free entries in pool (or replicated queue), start to lower maximum size of merge to process (or to put in queue). \n+When there is less than specified number of free entries in pool (or replicated queue), start to lower maximum size of merge to process (or to put in queue).\n This is to allow small merges to process - not filling the pool with long running merges.\n \n Possible values:\n@@ -566,7 +566,7 @@ Default value: 8\n \n ## number_of_free_entries_in_pool_to_execute_mutation {#number-of-free-entries-in-pool-to-execute-mutation}\n \n-When there is less than specified number of free entries in pool, do not execute part mutations. \n+When there is less than specified number of free entries in pool, do not execute part mutations.\n This is to leave free threads for regular merges and avoid \"Too many parts\".\n \n Possible values:\n@@ -845,6 +845,13 @@ You can see which parts of `s` were stored using the sparse serialization:\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n ```\n \n+## replace_long_file_name_to_hash {#replace_long_file_name_to_hash}\n+If the file name for column is too long (more than `max_file_name_length` bytes) replace it to SipHash128. Default value: `false`.\n+\n+## max_file_name_length {#max_file_name_length}\n+\n+The maximal length of the file name to keep it as is without hashing. Takes effect only if setting `replace_long_file_name_to_hash` is enabled. The value of this setting does not include the length of file extension. So, it is recommended to set it below the maximum filename length (usually 255 bytes) with some gap to avoid filesystem errors. Default value: 127.\n+\n ## clean_deleted_rows\n \n Enable/disable automatic deletion of rows flagged as `is_deleted` when perform `OPTIMIZE ... FINAL` on a table using the ReplacingMergeTree engine. When disabled, the `CLEANUP` keyword has to be added to the `OPTIMIZE ... FINAL` to have the same behaviour.\ndiff --git a/src/Common/SipHash.h b/src/Common/SipHash.h\nindex e87dcb40b2e4..22cff7a49428 100644\n--- a/src/Common/SipHash.h\n+++ b/src/Common/SipHash.h\n@@ -22,6 +22,7 @@\n #include <base/extended_types.h>\n #include <base/types.h>\n #include <base/unaligned.h>\n+#include <base/hex.h>\n #include <Common/Exception.h>\n \n #include <city.h>\n@@ -257,6 +258,16 @@ inline UInt128 sipHash128(const char * data, const size_t size)\n     return sipHash128Keyed(0, 0, data, size);\n }\n \n+inline String sipHash128String(const char * data, const size_t size)\n+{\n+    return getHexUIntLowercase(sipHash128(data, size));\n+}\n+\n+inline String sipHash128String(const String & str)\n+{\n+    return sipHash128String(str.data(), str.size());\n+}\n+\n inline UInt128 sipHash128ReferenceKeyed(UInt64 key0, UInt64 key1, const char * data, const size_t size)\n {\n     SipHash hash(key0, key1, true);\ndiff --git a/src/DataTypes/NestedUtils.cpp b/src/DataTypes/NestedUtils.cpp\nindex 9ee803c42356..efac2454a034 100644\n--- a/src/DataTypes/NestedUtils.cpp\n+++ b/src/DataTypes/NestedUtils.cpp\n@@ -164,7 +164,7 @@ NameToDataType getSubcolumnsOfNested(const NamesAndTypesList & names_and_types)\n     std::unordered_map<String, NamesAndTypesList> nested;\n     for (const auto & name_type : names_and_types)\n     {\n-        const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(name_type.type.get());\n+        const auto * type_arr = typeid_cast<const DataTypeArray *>(name_type.type.get());\n \n         /// Ignore true Nested type, but try to unite flatten arrays to Nested type.\n         if (!isNested(name_type.type) && type_arr)\n@@ -191,8 +191,11 @@ NamesAndTypesList collect(const NamesAndTypesList & names_and_types)\n     auto nested_types = getSubcolumnsOfNested(names_and_types);\n \n     for (const auto & name_type : names_and_types)\n-        if (!isArray(name_type.type) || !nested_types.contains(splitName(name_type.name).first))\n+    {\n+        auto split = splitName(name_type.name);\n+        if (!isArray(name_type.type) || split.second.empty() || !nested_types.contains(split.first))\n             res.push_back(name_type);\n+    }\n \n     for (const auto & name_type : nested_types)\n         res.emplace_back(name_type.first, name_type.second);\ndiff --git a/src/DataTypes/Serializations/ISerialization.h b/src/DataTypes/Serializations/ISerialization.h\nindex ed1ad037ea03..030c3c6d81ed 100644\n--- a/src/DataTypes/Serializations/ISerialization.h\n+++ b/src/DataTypes/Serializations/ISerialization.h\n@@ -370,6 +370,7 @@ class ISerialization : private boost::noncopyable, public std::enable_shared_fro\n \n     static String getFileNameForStream(const NameAndTypePair & column, const SubstreamPath & path);\n     static String getFileNameForStream(const String & name_in_storage, const SubstreamPath & path);\n+\n     static String getSubcolumnNameForStream(const SubstreamPath & path);\n     static String getSubcolumnNameForStream(const SubstreamPath & path, size_t prefix_len);\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex dc3874963713..e98cf4f6e32b 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -1032,11 +1032,14 @@ CompressionCodecPtr IMergeTreeDataPart::detectDefaultCompressionCodec() const\n             {\n                 if (path_to_data_file.empty())\n                 {\n-                    String candidate_path = /*fs::path(getRelativePath()) */ (ISerialization::getFileNameForStream(part_column, substream_path) + \".bin\");\n+                    auto stream_name = getStreamNameForColumn(part_column, substream_path, \".bin\", getDataPartStorage());\n+                    if (!stream_name)\n+                        return;\n \n+                    auto file_name = *stream_name + \".bin\";\n                     /// We can have existing, but empty .bin files. Example: LowCardinality(Nullable(...)) columns and column_name.dict.null.bin file.\n-                    if (getDataPartStorage().exists(candidate_path) && getDataPartStorage().getFileSize(candidate_path) != 0)\n-                        path_to_data_file = candidate_path;\n+                    if (getDataPartStorage().getFileSize(file_name) != 0)\n+                        path_to_data_file = file_name;\n                 }\n             });\n \n@@ -1321,8 +1324,8 @@ void IMergeTreeDataPart::loadColumns(bool require)\n     auto metadata_snapshot = storage.getInMemoryMetadataPtr();\n     if (parent_part)\n         metadata_snapshot = metadata_snapshot->projections.get(name).metadata;\n-    NamesAndTypesList loaded_columns;\n \n+    NamesAndTypesList loaded_columns;\n     bool is_readonly_storage = getDataPartStorage().isReadonly();\n \n     if (!metadata_manager->exists(\"columns.txt\"))\n@@ -1334,7 +1337,7 @@ void IMergeTreeDataPart::loadColumns(bool require)\n \n         /// If there is no file with a list of columns, write it down.\n         for (const NameAndTypePair & column : metadata_snapshot->getColumns().getAllPhysical())\n-            if (getDataPartStorage().exists(getFileNameForColumn(column) + \".bin\"))\n+            if (getFileNameForColumn(column))\n                 loaded_columns.push_back(column);\n \n         if (columns.empty())\n@@ -2064,6 +2067,73 @@ String IMergeTreeDataPart::getZeroLevelPartBlockID(std::string_view token) const\n     return info.partition_id + \"_\" + toString(hash_value.items[0]) + \"_\" + toString(hash_value.items[1]);\n }\n \n+std::optional<String> IMergeTreeDataPart::getStreamNameOrHash(\n+    const String & stream_name,\n+    const Checksums & checksums_)\n+{\n+    if (checksums_.files.contains(stream_name + \".bin\"))\n+        return stream_name;\n+\n+    auto hash = sipHash128String(stream_name);\n+    if (checksums_.files.contains(hash + \".bin\"))\n+        return hash;\n+\n+    return {};\n+}\n+\n+std::optional<String> IMergeTreeDataPart::getStreamNameOrHash(\n+    const String & stream_name,\n+    const String & extension,\n+    const IDataPartStorage & storage_)\n+{\n+    if (storage_.exists(stream_name + extension))\n+        return stream_name;\n+\n+    auto hash = sipHash128String(stream_name);\n+    if (storage_.exists(hash + extension))\n+        return hash;\n+\n+    return {};\n+}\n+\n+std::optional<String> IMergeTreeDataPart::getStreamNameForColumn(\n+    const String & column_name,\n+    const ISerialization::SubstreamPath & substream_path,\n+    const Checksums & checksums_)\n+{\n+    auto stream_name = ISerialization::getFileNameForStream(column_name, substream_path);\n+    return getStreamNameOrHash(stream_name, checksums_);\n+}\n+\n+std::optional<String> IMergeTreeDataPart::getStreamNameForColumn(\n+    const NameAndTypePair & column,\n+    const ISerialization::SubstreamPath & substream_path,\n+    const Checksums & checksums_)\n+{\n+    auto stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+    return getStreamNameOrHash(stream_name, checksums_);\n+}\n+\n+std::optional<String> IMergeTreeDataPart::getStreamNameForColumn(\n+    const String & column_name,\n+    const ISerialization::SubstreamPath & substream_path,\n+    const String & extension,\n+    const IDataPartStorage & storage_)\n+{\n+    auto stream_name = ISerialization::getFileNameForStream(column_name, substream_path);\n+    return getStreamNameOrHash(stream_name, extension, storage_);\n+}\n+\n+std::optional<String> IMergeTreeDataPart::getStreamNameForColumn(\n+    const NameAndTypePair & column,\n+    const ISerialization::SubstreamPath & substream_path,\n+    const String & extension,\n+    const IDataPartStorage & storage_)\n+{\n+    auto stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+    return getStreamNameOrHash(stream_name, extension, storage_);\n+}\n+\n bool isCompactPart(const MergeTreeDataPartPtr & data_part)\n {\n     return (data_part && data_part->getType() == MergeTreeDataPartType::Compact);\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex c30accbc1ba9..30c9b19fcbcf 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -131,7 +131,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     /// Return information about secondary indexes size on disk for all indexes in part\n     IndexSize getTotalSeconaryIndicesSize() const { return total_secondary_indices_size; }\n \n-    virtual String getFileNameForColumn(const NameAndTypePair & column) const = 0;\n+    virtual std::optional<String> getFileNameForColumn(const NameAndTypePair & column) const = 0;\n \n     virtual ~IMergeTreeDataPart();\n \n@@ -501,6 +501,37 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     /// This one is about removing file with version of part's metadata (columns, pk and so on)\n     void removeMetadataVersion();\n \n+    static std::optional<String> getStreamNameOrHash(\n+        const String & name,\n+        const IMergeTreeDataPart::Checksums & checksums);\n+\n+    static std::optional<String> getStreamNameOrHash(\n+        const String & name,\n+        const String & extension,\n+        const IDataPartStorage & storage_);\n+\n+    static std::optional<String> getStreamNameForColumn(\n+        const String & column_name,\n+        const ISerialization::SubstreamPath & substream_path,\n+        const Checksums & checksums_);\n+\n+    static std::optional<String> getStreamNameForColumn(\n+        const NameAndTypePair & column,\n+        const ISerialization::SubstreamPath & substream_path,\n+        const Checksums & checksums_);\n+\n+    static std::optional<String> getStreamNameForColumn(\n+        const String & column_name,\n+        const ISerialization::SubstreamPath & substream_path,\n+        const String & extension,\n+        const IDataPartStorage & storage_);\n+\n+    static std::optional<String> getStreamNameForColumn(\n+        const NameAndTypePair & column,\n+        const ISerialization::SubstreamPath & substream_path,\n+        const String & extension,\n+        const IDataPartStorage & storage_);\n+\n     mutable std::atomic<DataPartRemovalState> removal_state = DataPartRemovalState::NOT_ATTEMPTED;\n \n     mutable std::atomic<time_t> last_removal_attempt_time = 0;\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\nindex fa3c675f7da2..3f359904ddd8 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n@@ -32,7 +32,7 @@ class IMergeTreeDataPartWriter : private boost::noncopyable\n \n     virtual void write(const Block & block, const IColumn::Permutation * permutation) = 0;\n \n-    virtual void fillChecksums(IMergeTreeDataPart::Checksums & checksums) = 0;\n+    virtual void fillChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & checksums_to_remove) = 0;\n \n     virtual void finish(bool sync) = 0;\n \ndiff --git a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\nindex 21bead2864a1..c8d6aa0ba657 100644\n--- a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n@@ -51,7 +51,9 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n         data_part->getSerialization(column.name)->enumerateStreams(\n             [&](const ISerialization::SubstreamPath & substream_path)\n             {\n-                ++stream_counts[ISerialization::getFileNameForStream(column.name, substream_path)];\n+                auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(column, substream_path, checksums);\n+                if (stream_name)\n+                    ++stream_counts[*stream_name];\n             });\n     }\n \n@@ -65,12 +67,13 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n \n         ISerialization::StreamCallback callback = [&](const ISerialization::SubstreamPath & substream_path)\n         {\n-            String stream_name = ISerialization::getFileNameForStream(column_name, substream_path);\n+            auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(column_name, substream_path, checksums);\n+\n             /// Delete files if they are no longer shared with another column.\n-            if (--stream_counts[stream_name] == 0)\n+            if (stream_name && --stream_counts[*stream_name] == 0)\n             {\n-                remove_files.emplace(stream_name + \".bin\");\n-                remove_files.emplace(stream_name + mrk_extension);\n+                remove_files.emplace(*stream_name + \".bin\");\n+                remove_files.emplace(*stream_name + mrk_extension);\n             }\n         };\n \ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 5187744868e7..165d3dc331fe 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -399,6 +399,7 @@ MergeTreeData::MergeTreeData(\n                               settings->check_sample_column_is_correct && !attach);\n     }\n \n+    checkColumnFilenamesForCollision(metadata_.getColumns(), *settings, !attach);\n     checkTTLExpressions(metadata_, metadata_);\n \n     String reason;\n@@ -3351,6 +3352,7 @@ void MergeTreeData::checkAlterIsPossible(const AlterCommands & commands, Context\n         }\n     }\n \n+    checkColumnFilenamesForCollision(new_metadata, /*throw_on_error=*/ true);\n     checkProperties(new_metadata, old_metadata, false, false, allow_nullable_key, local_context);\n     checkTTLExpressions(new_metadata, old_metadata);\n \n@@ -7445,6 +7447,73 @@ bool MergeTreeData::canUseParallelReplicasBasedOnPKAnalysis(\n     return decision;\n }\n \n+void MergeTreeData::checkColumnFilenamesForCollision(const StorageInMemoryMetadata & metadata, bool throw_on_error) const\n+{\n+    auto settings = getDefaultSettings();\n+    if (metadata.settings_changes)\n+    {\n+        const auto & changes = metadata.settings_changes->as<const ASTSetQuery &>().changes;\n+        settings->applyChanges(changes);\n+    }\n+\n+    checkColumnFilenamesForCollision(metadata.getColumns(), *settings, throw_on_error);\n+}\n+\n+void MergeTreeData::checkColumnFilenamesForCollision(const ColumnsDescription & columns, const MergeTreeSettings & settings, bool throw_on_error) const\n+{\n+    std::unordered_map<String, std::pair<String, String>> stream_name_to_full_name;\n+    auto columns_list = Nested::collect(columns.getAllPhysical());\n+\n+    for (const auto & column : columns_list)\n+    {\n+        std::unordered_map<String, String> column_streams;\n+\n+        auto callback = [&](const auto & substream_path)\n+        {\n+            String stream_name;\n+            auto full_stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+\n+            if (settings.replace_long_file_name_to_hash && full_stream_name.size() > settings.max_file_name_length)\n+                stream_name = sipHash128String(full_stream_name);\n+            else\n+                stream_name = full_stream_name;\n+\n+            column_streams.emplace(stream_name, full_stream_name);\n+        };\n+\n+        auto serialization = column.type->getDefaultSerialization();\n+        serialization->enumerateStreams(callback);\n+\n+        if (column.type->supportsSparseSerialization() && settings.ratio_of_defaults_for_sparse_serialization < 1.0)\n+        {\n+            auto sparse_serialization = column.type->getSparseSerialization();\n+            sparse_serialization->enumerateStreams(callback);\n+        }\n+\n+        for (const auto & [stream_name, full_stream_name] : column_streams)\n+        {\n+            auto [it, inserted] = stream_name_to_full_name.emplace(stream_name, std::pair{full_stream_name, column.name});\n+            if (!inserted)\n+            {\n+                const auto & [other_full_name, other_column_name] = it->second;\n+                auto other_type = columns.getPhysical(other_column_name).type;\n+\n+                auto message = fmt::format(\n+                    \"Columns '{} {}' and '{} {}' have streams ({} and {}) with collision in file name {}\",\n+                    column.name, column.type->getName(), other_column_name, other_type->getName(), full_stream_name, other_full_name, stream_name);\n+\n+                if (settings.replace_long_file_name_to_hash)\n+                    message += \". It may be a collision between a filename for one column and a hash of filename for another column (see setting 'replace_long_file_name_to_hash')\";\n+\n+                if (throw_on_error)\n+                    throw Exception(ErrorCodes::BAD_ARGUMENTS, \"{}\", message);\n+\n+                LOG_ERROR(log, \"Table definition is incorrect. {}. It may lead to corruption of data or crashes. You need to resolve it manually\", message);\n+                return;\n+            }\n+        }\n+    }\n+}\n \n MergeTreeData & MergeTreeData::checkStructureAndGetMergeTreeData(IStorage & source_table, const StorageMetadataPtr & src_snapshot, const StorageMetadataPtr & my_snapshot) const\n {\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 59b421dc7d1c..e309cd44bc9b 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -1585,6 +1585,9 @@ class MergeTreeData : public IStorage, public WithMutableContext\n         ContextPtr query_context,\n         const StorageSnapshotPtr & storage_snapshot,\n         SelectQueryInfo & query_info) const;\n+\n+    void checkColumnFilenamesForCollision(const StorageInMemoryMetadata & metadata, bool throw_on_error) const;\n+    void checkColumnFilenamesForCollision(const ColumnsDescription & columns, const MergeTreeSettings & settings, bool throw_on_error) const;\n };\n \n /// RAII struct to record big parts that are submerging or emerging.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartChecksum.cpp b/src/Storages/MergeTree/MergeTreeDataPartChecksum.cpp\nindex b4d405312e02..ed2202fcb194 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartChecksum.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartChecksum.cpp\n@@ -8,6 +8,7 @@\n #include <Compression/CompressedReadBuffer.h>\n #include <Compression/CompressedWriteBuffer.h>\n #include <Storages/MergeTree/IDataPartStorage.h>\n+#include <optional>\n \n \n namespace DB\n@@ -67,44 +68,35 @@ void MergeTreeDataPartChecksum::checkSize(const IDataPartStorage & storage, cons\n \n void MergeTreeDataPartChecksums::checkEqual(const MergeTreeDataPartChecksums & rhs, bool have_uncompressed) const\n {\n-    for (const auto & it : rhs.files)\n-    {\n-        const String & name = it.first;\n-\n+    for (const auto & [name, _] : rhs.files)\n         if (!files.contains(name))\n             throw Exception(ErrorCodes::UNEXPECTED_FILE_IN_DATA_PART, \"Unexpected file {} in data part\", name);\n-    }\n \n-    for (const auto & it : files)\n+    for (const auto & [name, checksum] : files)\n     {\n-        const String & name = it.first;\n-\n         /// Exclude files written by inverted index from check. No correct checksums are available for them currently.\n         if (name.ends_with(\".gin_dict\") || name.ends_with(\".gin_post\") || name.ends_with(\".gin_seg\") || name.ends_with(\".gin_sid\"))\n             continue;\n \n-        auto jt = rhs.files.find(name);\n-        if (jt == rhs.files.end())\n+        auto it = rhs.files.find(name);\n+        if (it == rhs.files.end())\n             throw Exception(ErrorCodes::NO_FILE_IN_DATA_PART, \"No file {} in data part\", name);\n \n-        it.second.checkEqual(jt->second, have_uncompressed, name);\n+        checksum.checkEqual(it->second, have_uncompressed, name);\n     }\n }\n \n void MergeTreeDataPartChecksums::checkSizes(const IDataPartStorage & storage) const\n {\n-    for (const auto & it : files)\n-    {\n-        const String & name = it.first;\n-        it.second.checkSize(storage, name);\n-    }\n+    for (const auto & [name, checksum] : files)\n+        checksum.checkSize(storage, name);\n }\n \n UInt64 MergeTreeDataPartChecksums::getTotalSizeOnDisk() const\n {\n     UInt64 res = 0;\n-    for (const auto & it : files)\n-        res += it.second.file_size;\n+    for (const auto & [_, checksum] : files)\n+        res += checksum.file_size;\n     return res;\n }\n \n@@ -218,11 +210,8 @@ void MergeTreeDataPartChecksums::write(WriteBuffer & to) const\n \n     writeVarUInt(files.size(), out);\n \n-    for (const auto & it : files)\n+    for (const auto & [name, sum] : files)\n     {\n-        const String & name = it.first;\n-        const Checksum & sum = it.second;\n-\n         writeStringBinary(name, out);\n         writeVarUInt(sum.file_size, out);\n         writeBinaryLittleEndian(sum.file_hash, out);\n@@ -255,11 +244,8 @@ void MergeTreeDataPartChecksums::add(MergeTreeDataPartChecksums && rhs_checksums\n void MergeTreeDataPartChecksums::computeTotalChecksumDataOnly(SipHash & hash) const\n {\n     /// We use fact that iteration is in deterministic (lexicographical) order.\n-    for (const auto & it : files)\n+    for (const auto & [name, sum] : files)\n     {\n-        const String & name = it.first;\n-        const Checksum & sum = it.second;\n-\n         if (!endsWith(name, \".bin\"))\n             continue;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartChecksum.h b/src/Storages/MergeTree/MergeTreeDataPartChecksum.h\nindex db110043b743..8e5e8c8c4484 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartChecksum.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartChecksum.h\n@@ -90,7 +90,6 @@ struct MergeTreeDataPartChecksums\n     UInt64 getTotalSizeOnDisk() const;\n };\n \n-\n /// A kind of MergeTreeDataPartChecksums intended to be stored in ZooKeeper (to save its RAM)\n /// MinimalisticDataPartChecksums and MergeTreeDataPartChecksums have the same serialization format\n ///  for versions less than MINIMAL_VERSION_WITH_MINIMALISTIC_CHECKSUMS.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.h b/src/Storages/MergeTree/MergeTreeDataPartCompact.h\nindex 2bbac766c8ef..7850e7c976c1 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.h\n@@ -57,7 +57,7 @@ class MergeTreeDataPartCompact : public IMergeTreeDataPart\n \n     std::optional<time_t> getColumnModificationTime(const String & column_name) const override;\n \n-    String getFileNameForColumn(const NameAndTypePair & /* column */) const override { return DATA_FILE_NAME; }\n+    std::optional<String> getFileNameForColumn(const NameAndTypePair & /* column */) const override { return DATA_FILE_NAME; }\n \n     ~MergeTreeDataPartCompact() override;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\nindex 95f7b796f9ad..c7b7dde50a6f 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\n@@ -40,7 +40,7 @@ class MergeTreeDataPartInMemory : public IMergeTreeDataPart\n     bool isStoredOnRemoteDisk() const override { return false; }\n     bool isStoredOnRemoteDiskWithZeroCopySupport() const override { return false; }\n     bool hasColumnFiles(const NameAndTypePair & column) const override { return !!getColumnPosition(column.getNameInStorage()); }\n-    String getFileNameForColumn(const NameAndTypePair & /* column */) const override { return \"\"; }\n+    std::optional<String> getFileNameForColumn(const NameAndTypePair & /* column */) const override { return \"\"; }\n     void renameTo(const String & new_relative_path, bool remove_new_dir_if_exists) override;\n     DataPartStoragePtr makeCloneInDetached(const String & prefix, const StorageMetadataPtr & metadata_snapshot,\n                                            const DiskTransactionPtr & disk_transaction) const override;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\nindex 20600909ce42..9b71c8df3a30 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n@@ -73,19 +73,22 @@ ColumnSize MergeTreeDataPartWide::getColumnSizeImpl(\n \n     getSerialization(column.name)->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n     {\n-        String file_name = ISerialization::getFileNameForStream(column, substream_path);\n+        auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(column, substream_path, checksums);\n \n-        if (processed_substreams && !processed_substreams->insert(file_name).second)\n+        if (!stream_name)\n             return;\n \n-        auto bin_checksum = checksums.files.find(file_name + \".bin\");\n+        if (processed_substreams && !processed_substreams->insert(*stream_name).second)\n+            return;\n+\n+        auto bin_checksum = checksums.files.find(*stream_name + \".bin\");\n         if (bin_checksum != checksums.files.end())\n         {\n             size.data_compressed += bin_checksum->second.file_size;\n             size.data_uncompressed += bin_checksum->second.uncompressed_size;\n         }\n \n-        auto mrk_checksum = checksums.files.find(file_name + getMarksFileExtension());\n+        auto mrk_checksum = checksums.files.find(*stream_name + getMarksFileExtension());\n         if (mrk_checksum != checksums.files.end())\n             size.marks += mrk_checksum->second.file_size;\n     });\n@@ -153,7 +156,13 @@ void MergeTreeDataPartWide::loadIndexGranularity()\n     if (columns.empty())\n         throw Exception(ErrorCodes::NO_FILE_IN_DATA_PART, \"No columns in part {}\", name);\n \n-    loadIndexGranularityImpl(index_granularity, index_granularity_info, getDataPartStorage(), getFileNameForColumn(columns.front()));\n+    auto any_column_filename = getFileNameForColumn(columns.front());\n+    if (!any_column_filename)\n+        throw Exception(ErrorCodes::NO_FILE_IN_DATA_PART,\n+            \"There are no files for column {} in part {}\",\n+            columns.front().name, getDataPartStorage().getFullPath());\n+\n+    loadIndexGranularityImpl(index_granularity, index_granularity_info, getDataPartStorage(), *any_column_filename);\n }\n \n \n@@ -185,21 +194,19 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n             {\n                 getSerialization(name_type.name)->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n                 {\n-                    String file_name = ISerialization::getFileNameForStream(name_type, substream_path);\n-                    String mrk_file_name = file_name + marks_file_extension;\n-                    String bin_file_name = file_name + DATA_FILE_EXTENSION;\n+                    auto stream_name = getStreamNameForColumn(name_type, substream_path, checksums);\n+                    if (!stream_name)\n+                        throw Exception(\n+                            ErrorCodes::NO_FILE_IN_DATA_PART,\n+                            \"No {}.{} file checksum for column {} in part {}\",\n+                            *stream_name, DATA_FILE_EXTENSION, name_type.name, getDataPartStorage().getFullPath());\n \n+                    auto mrk_file_name = *stream_name + marks_file_extension;\n                     if (!checksums.files.contains(mrk_file_name))\n                         throw Exception(\n                             ErrorCodes::NO_FILE_IN_DATA_PART,\n                             \"No {} file checksum for column {} in part {} \",\n                             mrk_file_name, name_type.name, getDataPartStorage().getFullPath());\n-\n-                    if (!checksums.files.contains(bin_file_name))\n-                        throw Exception(\n-                            ErrorCodes::NO_FILE_IN_DATA_PART,\n-                            \"No {} file checksum for column {} in part {}\",\n-                            bin_file_name, name_type.name, getDataPartStorage().getFullPath());\n                 });\n             }\n         }\n@@ -212,27 +219,28 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n         {\n             getSerialization(name_type.name)->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n             {\n-                auto file_path = ISerialization::getFileNameForStream(name_type, substream_path) + marks_file_extension;\n+                auto stream_name = getStreamNameForColumn(name_type, substream_path, marks_file_extension, getDataPartStorage());\n \n                 /// Missing file is Ok for case when new column was added.\n-                if (getDataPartStorage().exists(file_path))\n-                {\n-                    UInt64 file_size = getDataPartStorage().getFileSize(file_path);\n-\n-                    if (!file_size)\n-                        throw Exception(\n-                            ErrorCodes::BAD_SIZE_OF_FILE_IN_DATA_PART,\n-                            \"Part {} is broken: {} is empty.\",\n-                            getDataPartStorage().getFullPath(),\n-                            std::string(fs::path(getDataPartStorage().getFullPath()) / file_path));\n-\n-                    if (!marks_size)\n-                        marks_size = file_size;\n-                    else if (file_size != *marks_size)\n-                        throw Exception(\n-                            ErrorCodes::BAD_SIZE_OF_FILE_IN_DATA_PART,\n-                            \"Part {} is broken: marks have different sizes.\", getDataPartStorage().getFullPath());\n-                }\n+                if (!stream_name)\n+                    return;\n+\n+                auto file_path = *stream_name + marks_file_extension;\n+                UInt64 file_size = getDataPartStorage().getFileSize(file_path);\n+\n+                if (!file_size)\n+                    throw Exception(\n+                        ErrorCodes::BAD_SIZE_OF_FILE_IN_DATA_PART,\n+                        \"Part {} is broken: {} is empty.\",\n+                        getDataPartStorage().getFullPath(),\n+                        std::string(fs::path(getDataPartStorage().getFullPath()) / file_path));\n+\n+                if (!marks_size)\n+                    marks_size = file_size;\n+                else if (file_size != *marks_size)\n+                    throw Exception(\n+                        ErrorCodes::BAD_SIZE_OF_FILE_IN_DATA_PART,\n+                        \"Part {} is broken: marks have different sizes.\", getDataPartStorage().getFullPath());\n             });\n         }\n     }\n@@ -240,20 +248,13 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n \n bool MergeTreeDataPartWide::hasColumnFiles(const NameAndTypePair & column) const\n {\n-    std::string marks_file_extension = index_granularity_info.mark_type.getFileExtension();\n-    auto check_stream_exists = [this, &marks_file_extension](const String & stream_name)\n-    {\n-        auto bin_checksum = checksums.files.find(stream_name + DATA_FILE_EXTENSION);\n-        auto mrk_checksum = checksums.files.find(stream_name + marks_file_extension);\n-\n-        return bin_checksum != checksums.files.end() && mrk_checksum != checksums.files.end();\n-    };\n+    auto marks_file_extension = index_granularity_info.mark_type.getFileExtension();\n \n     bool res = true;\n     getSerialization(column.name)->enumerateStreams([&](const auto & substream_path)\n     {\n-        String file_name = ISerialization::getFileNameForStream(column, substream_path);\n-        if (!check_stream_exists(file_name))\n+        auto stream_name = getStreamNameForColumn(column, substream_path, checksums);\n+        if (!stream_name || !checksums.files.contains(*stream_name + marks_file_extension))\n             res = false;\n     });\n \n@@ -264,7 +265,11 @@ std::optional<time_t> MergeTreeDataPartWide::getColumnModificationTime(const Str\n {\n     try\n     {\n-        return getDataPartStorage().getFileLastModified(column_name + DATA_FILE_EXTENSION).epochTime();\n+        auto stream_name = getStreamNameOrHash(column_name, checksums);\n+        if (!stream_name)\n+            return {};\n+\n+        return getDataPartStorage().getFileLastModified(*stream_name + DATA_FILE_EXTENSION).epochTime();\n     }\n     catch (const fs::filesystem_error &)\n     {\n@@ -272,13 +277,19 @@ std::optional<time_t> MergeTreeDataPartWide::getColumnModificationTime(const Str\n     }\n }\n \n-String MergeTreeDataPartWide::getFileNameForColumn(const NameAndTypePair & column) const\n+std::optional<String> MergeTreeDataPartWide::getFileNameForColumn(const NameAndTypePair & column) const\n {\n-    String filename;\n+    std::optional<String> filename;\n     getSerialization(column.name)->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n     {\n-        if (filename.empty())\n-            filename = ISerialization::getFileNameForStream(column, substream_path);\n+        if (!filename.has_value())\n+        {\n+            /// This method may be called when checksums are not initialized yet.\n+            if (!checksums.empty())\n+                filename = getStreamNameForColumn(column, substream_path, checksums);\n+            else\n+                filename = getStreamNameForColumn(column, substream_path, DATA_FILE_EXTENSION, getDataPartStorage());\n+        }\n     });\n     return filename;\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.h b/src/Storages/MergeTree/MergeTreeDataPartWide.h\nindex 2076a1ec0282..bcf70426fa6d 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.h\n@@ -48,7 +48,7 @@ class MergeTreeDataPartWide : public IMergeTreeDataPart\n \n     bool isStoredOnRemoteDiskWithZeroCopySupport() const override;\n \n-    String getFileNameForColumn(const NameAndTypePair & column) const override;\n+    std::optional<String> getFileNameForColumn(const NameAndTypePair & column) const override;\n \n     ~MergeTreeDataPartWide() override;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex d2a9632d4e58..236616348886 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -423,7 +423,7 @@ size_t MergeTreeDataPartWriterCompact::ColumnsBuffer::size() const\n     return accumulated_columns.at(0)->size();\n }\n \n-void MergeTreeDataPartWriterCompact::fillChecksums(IMergeTreeDataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterCompact::fillChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & /*checksums_to_remove*/)\n {\n     // If we don't have anything to write, skip finalization.\n     if (!columns_list.empty())\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\nindex 06f8122393fd..b1cfefd2d8fa 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n@@ -22,7 +22,7 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n \n     void write(const Block & block, const IColumn::Permutation * permutation) override;\n \n-    void fillChecksums(IMergeTreeDataPart::Checksums & checksums) override;\n+    void fillChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & checksums_to_remove) override;\n     void finish(bool sync) override;\n \n private:\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp\nindex 9afa7a1e80d6..048339b58c94 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp\n@@ -76,7 +76,7 @@ void MergeTreeDataPartWriterInMemory::calculateAndSerializePrimaryIndex(const Bl\n     }\n }\n \n-void MergeTreeDataPartWriterInMemory::fillChecksums(IMergeTreeDataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterInMemory::fillChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & /*checksums_to_remove*/)\n {\n     /// If part is empty we still need to initialize block by empty columns.\n     if (!part_in_memory->block)\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h\nindex 9e1e868beacf..2d3338226520 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h\n@@ -18,7 +18,7 @@ class MergeTreeDataPartWriterInMemory : public IMergeTreeDataPartWriter\n     /// You can write only one block. In-memory part can be written only at INSERT.\n     void write(const Block & block, const IColumn::Permutation * permutation) override;\n \n-    void fillChecksums(IMergeTreeDataPart::Checksums & checksums) override;\n+    void fillChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & checksums_to_remove) override;\n     void finish(bool /*sync*/) override {}\n \n private:\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex f3e60e224aa7..135cafbca210 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -15,6 +15,7 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n+    extern const int INCORRECT_FILE_NAME;\n }\n \n namespace\n@@ -107,7 +108,22 @@ void MergeTreeDataPartWriterWide::addStreams(\n     ISerialization::StreamCallback callback = [&](const auto & substream_path)\n     {\n         assert(!substream_path.empty());\n-        String stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+\n+        auto storage_settings = storage.getSettings();\n+        auto full_stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+\n+        String stream_name;\n+        if (storage_settings->replace_long_file_name_to_hash && full_stream_name.size() > storage_settings->max_file_name_length)\n+            stream_name = sipHash128String(full_stream_name);\n+        else\n+            stream_name = full_stream_name;\n+\n+        auto it = stream_name_to_full_name.find(stream_name);\n+        if (it != stream_name_to_full_name.end() && it->second != full_stream_name)\n+            throw Exception(ErrorCodes::INCORRECT_FILE_NAME,\n+                \"Stream with name {} already created (full stream name: {}). Current full stream name: {}.\"\n+                \" It is a collision between a filename for one column and a hash of filename for another column or a bug\",\n+                stream_name, it->second, full_stream_name);\n \n         /// Shared offsets for Nested type.\n         if (column_streams.contains(stream_name))\n@@ -136,12 +152,22 @@ void MergeTreeDataPartWriterWide::addStreams(\n             marks_compression_codec,\n             settings.marks_compress_block_size,\n             settings.query_write_settings);\n+\n+        full_name_to_stream_name.emplace(full_stream_name, stream_name);\n+        stream_name_to_full_name.emplace(stream_name, full_stream_name);\n     };\n \n     ISerialization::SubstreamPath path;\n     data_part->getSerialization(column.name)->enumerateStreams(callback, column.type);\n }\n \n+const String & MergeTreeDataPartWriterWide::getStreamName(\n+    const NameAndTypePair & column,\n+    const ISerialization::SubstreamPath & substream_path) const\n+{\n+    auto full_stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+    return full_name_to_stream_name.at(full_stream_name);\n+}\n \n ISerialization::OutputStreamGetter MergeTreeDataPartWriterWide::createStreamGetter(\n         const NameAndTypePair & column, WrittenOffsetColumns & offset_columns) const\n@@ -149,8 +175,7 @@ ISerialization::OutputStreamGetter MergeTreeDataPartWriterWide::createStreamGett\n     return [&, this] (const ISerialization::SubstreamPath & substream_path) -> WriteBuffer *\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == ISerialization::Substream::ArraySizes;\n-\n-        String stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+        auto stream_name = getStreamName(column, substream_path);\n \n         /// Don't write offsets more than one time for Nested type.\n         if (is_offsets && offset_columns.contains(stream_name))\n@@ -299,8 +324,7 @@ StreamsWithMarks MergeTreeDataPartWriterWide::getCurrentMarksForColumn(\n     data_part->getSerialization(column.name)->enumerateStreams([&] (const ISerialization::SubstreamPath & substream_path)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == ISerialization::Substream::ArraySizes;\n-\n-        String stream_name = ISerialization::getFileNameForStream(column, substream_path);\n+        auto stream_name = getStreamName(column, substream_path);\n \n         /// Don't write offsets more than one time for Nested type.\n         if (is_offsets && offset_columns.contains(stream_name))\n@@ -338,14 +362,13 @@ void MergeTreeDataPartWriterWide::writeSingleGranule(\n     serialization->enumerateStreams([&] (const ISerialization::SubstreamPath & substream_path)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == ISerialization::Substream::ArraySizes;\n-\n-        String stream_name = ISerialization::getFileNameForStream(name_and_type, substream_path);\n+        auto stream_name = getStreamName(name_and_type, substream_path);\n \n         /// Don't write offsets more than one time for Nested type.\n         if (is_offsets && offset_columns.contains(stream_name))\n             return;\n \n-        column_streams[stream_name]->compressed_hashing.nextIfAtEnd();\n+        column_streams.at(stream_name)->compressed_hashing.nextIfAtEnd();\n     });\n }\n \n@@ -416,10 +439,7 @@ void MergeTreeDataPartWriterWide::writeColumn(\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == ISerialization::Substream::ArraySizes;\n         if (is_offsets)\n-        {\n-            String stream_name = ISerialization::getFileNameForStream(name_and_type, substream_path);\n-            offset_columns.insert(stream_name);\n-        }\n+            offset_columns.insert(getStreamName(name_and_type, substream_path));\n     });\n }\n \n@@ -561,7 +581,7 @@ void MergeTreeDataPartWriterWide::validateColumnOfFixedSize(const NameAndTypePai\n \n }\n \n-void MergeTreeDataPartWriterWide::fillDataChecksums(IMergeTreeDataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterWide::fillDataChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & checksums_to_remove)\n {\n     const auto & global_settings = storage.getContext()->getSettingsRef();\n     ISerialization::SerializeBinaryBulkSettings serialize_settings;\n@@ -597,10 +617,19 @@ void MergeTreeDataPartWriterWide::fillDataChecksums(IMergeTreeDataPart::Checksum\n         }\n     }\n \n-    for (auto & stream : column_streams)\n+    for (auto & [stream_name, stream] : column_streams)\n     {\n-        stream.second->preFinalize();\n-        stream.second->addToChecksums(checksums);\n+        /// Remove checksums for old stream name if file was\n+        /// renamed due to replacing the name to the hash of name.\n+        const auto & full_stream_name = stream_name_to_full_name.at(stream_name);\n+        if (stream_name != full_stream_name)\n+        {\n+            checksums_to_remove.insert(full_stream_name + stream->data_file_extension);\n+            checksums_to_remove.insert(full_stream_name + stream->marks_file_extension);\n+        }\n+\n+        stream->preFinalize();\n+        stream->addToChecksums(checksums);\n     }\n }\n \n@@ -632,11 +661,11 @@ void MergeTreeDataPartWriterWide::finishDataSerialization(bool sync)\n \n }\n \n-void MergeTreeDataPartWriterWide::fillChecksums(IMergeTreeDataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterWide::fillChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & checksums_to_remove)\n {\n     // If we don't have anything to write, skip finalization.\n     if (!columns_list.empty())\n-        fillDataChecksums(checksums);\n+        fillDataChecksums(checksums, checksums_to_remove);\n \n     if (settings.rewrite_primary_key)\n         fillPrimaryIndexChecksums(checksums);\n@@ -666,10 +695,7 @@ void MergeTreeDataPartWriterWide::writeFinalMark(\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == ISerialization::Substream::ArraySizes;\n         if (is_offsets)\n-        {\n-            String stream_name = ISerialization::getFileNameForStream(column, substream_path);\n-            offset_columns.insert(stream_name);\n-        }\n+            offset_columns.insert(getStreamName(column, substream_path));\n     });\n }\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\nindex 633b5119474c..c274fc9807c0 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n@@ -29,14 +29,14 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n \n     void write(const Block & block, const IColumn::Permutation * permutation) override;\n \n-    void fillChecksums(IMergeTreeDataPart::Checksums & checksums) final;\n+    void fillChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & checksums_to_remove) final;\n \n     void finish(bool sync) final;\n \n private:\n     /// Finish serialization of data: write final mark if required and compute checksums\n     /// Also validate written data in debug mode\n-    void fillDataChecksums(IMergeTreeDataPart::Checksums & checksums);\n+    void fillDataChecksums(IMergeTreeDataPart::Checksums & checksums, NameSet & checksums_to_remove);\n     void finishDataSerialization(bool sync);\n \n     /// Write data of one column.\n@@ -101,6 +101,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n     void adjustLastMarkIfNeedAndFlushToDisk(size_t new_rows_in_last_mark);\n \n     ISerialization::OutputStreamGetter createStreamGetter(const NameAndTypePair & column, WrittenOffsetColumns & offset_columns) const;\n+    const String & getStreamName(const NameAndTypePair & column, const ISerialization::SubstreamPath & substream_path) const;\n \n     using SerializationState = ISerialization::SerializeBinaryBulkStatePtr;\n     using SerializationStates = std::unordered_map<String, SerializationState>;\n@@ -110,6 +111,12 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n     using ColumnStreams = std::map<String, StreamPtr>;\n     ColumnStreams column_streams;\n \n+    /// Some long column names may be replaced to hashes.\n+    /// Below are mapping from original stream name to actual\n+    /// stream name (probably hash of the stream) and vice versa.\n+    std::unordered_map<String, String> full_name_to_stream_name;\n+    std::unordered_map<String, String> stream_name_to_full_name;\n+\n     /// Non written marks to disk (for each column). Waiting until all rows for\n     /// this marks will be written to disk.\n     using MarksForColumns = std::unordered_map<String, StreamsWithMarks>;\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderWide.cpp b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\nindex 29924d06f685..640432ef755a 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n@@ -206,35 +206,33 @@ void MergeTreeReaderWide::addStreams(\n \n     ISerialization::StreamCallback callback = [&] (const ISerialization::SubstreamPath & substream_path)\n     {\n-        String stream_name = ISerialization::getFileNameForStream(name_and_type, substream_path);\n-\n-        if (streams.contains(stream_name))\n-        {\n-            has_any_stream = true;\n-            return;\n-        }\n-\n-        bool data_file_exists = data_part_info_for_read->getChecksums().files.contains(stream_name + DATA_FILE_EXTENSION);\n+        auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(name_and_type, substream_path, data_part_info_for_read->getChecksums());\n \n         /** If data file is missing then we will not try to open it.\n           * It is necessary since it allows to add new column to structure of the table without creating new files for old parts.\n           */\n-        if (!data_file_exists)\n+        if (!stream_name)\n         {\n             has_all_streams = false;\n             return;\n         }\n \n+        if (streams.contains(*stream_name))\n+        {\n+            has_any_stream = true;\n+            return;\n+        }\n+\n         has_any_stream = true;\n         bool is_lc_dict = substream_path.size() > 1 && substream_path[substream_path.size() - 2].type == ISerialization::Substream::Type::DictionaryKeys;\n \n         auto context = data_part_info_for_read->getContext();\n         auto * load_marks_threadpool = settings.read_settings.load_marks_asynchronously ? &context->getLoadMarksThreadpool() : nullptr;\n \n-        streams.emplace(stream_name, std::make_unique<MergeTreeReaderStream>(\n-            data_part_info_for_read, stream_name, DATA_FILE_EXTENSION,\n+        streams.emplace(*stream_name, std::make_unique<MergeTreeReaderStream>(\n+            data_part_info_for_read, *stream_name, DATA_FILE_EXTENSION,\n             data_part_info_for_read->getMarksCount(), all_mark_ranges, settings, mark_cache,\n-            uncompressed_cache, data_part_info_for_read->getFileSizeOrZero(stream_name + DATA_FILE_EXTENSION),\n+            uncompressed_cache, data_part_info_for_read->getFileSizeOrZero(*stream_name + DATA_FILE_EXTENSION),\n             &data_part_info_for_read->getIndexGranularityInfo(),\n             profile_callback, clock_type, is_lc_dict, load_marks_threadpool));\n     };\n@@ -245,13 +243,14 @@ void MergeTreeReaderWide::addStreams(\n         partially_read_columns.insert(name_and_type.name);\n }\n \n-\n static ReadBuffer * getStream(\n     bool seek_to_start,\n     const ISerialization::SubstreamPath & substream_path,\n+    const MergeTreeDataPartChecksums & checksums,\n     MergeTreeReaderWide::FileStreams & streams,\n     const NameAndTypePair & name_and_type,\n-    size_t from_mark, bool seek_to_mark,\n+    size_t from_mark,\n+    bool seek_to_mark,\n     size_t current_task_last_mark,\n     ISerialization::SubstreamsCache & cache)\n {\n@@ -259,9 +258,11 @@ static ReadBuffer * getStream(\n     if (cache.contains(ISerialization::getSubcolumnNameForStream(substream_path)))\n         return nullptr;\n \n-    String stream_name = ISerialization::getFileNameForStream(name_and_type, substream_path);\n+    auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(name_and_type, substream_path, checksums);\n+    if (!stream_name)\n+        return nullptr;\n \n-    auto it = streams.find(stream_name);\n+    auto it = streams.find(*stream_name);\n     if (it == streams.end())\n         return nullptr;\n \n@@ -288,7 +289,7 @@ void MergeTreeReaderWide::deserializePrefix(\n         ISerialization::DeserializeBinaryBulkSettings deserialize_settings;\n         deserialize_settings.getter = [&](const ISerialization::SubstreamPath & substream_path)\n         {\n-            return getStream(/* seek_to_start = */true, substream_path, streams, name_and_type, 0, /* seek_to_mark = */false, current_task_last_mark, cache);\n+            return getStream(/* seek_to_start = */true, substream_path, data_part_info_for_read->getChecksums(), streams, name_and_type, 0, /* seek_to_mark = */false, current_task_last_mark, cache);\n         };\n         serialization->deserializeBinaryBulkStatePrefix(deserialize_settings, deserialize_binary_bulk_state_map[name]);\n     }\n@@ -307,15 +308,15 @@ void MergeTreeReaderWide::prefetchForColumn(\n \n     serialization->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n     {\n-        String stream_name = ISerialization::getFileNameForStream(name_and_type, substream_path);\n+        auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(name_and_type, substream_path, data_part_info_for_read->getChecksums());\n \n-        if (!prefetched_streams.contains(stream_name))\n+        if (stream_name && !prefetched_streams.contains(*stream_name))\n         {\n             bool seek_to_mark = !continue_reading;\n-            if (ReadBuffer * buf = getStream(false, substream_path, streams, name_and_type, from_mark, seek_to_mark, current_task_last_mark, cache))\n+            if (ReadBuffer * buf = getStream(false, substream_path, data_part_info_for_read->getChecksums(), streams, name_and_type, from_mark, seek_to_mark, current_task_last_mark, cache))\n             {\n                 buf->prefetch(priority);\n-                prefetched_streams.insert(stream_name);\n+                prefetched_streams.insert(*stream_name);\n             }\n         }\n     });\n@@ -338,8 +339,9 @@ void MergeTreeReaderWide::readData(\n         bool seek_to_mark = !was_prefetched && !continue_reading;\n \n         return getStream(\n-            /* seek_to_start = */false, substream_path, streams, name_and_type, from_mark,\n-            seek_to_mark, current_task_last_mark, cache);\n+            /* seek_to_start = */false, substream_path,\n+            data_part_info_for_read->getChecksums(), streams,\n+            name_and_type, from_mark, seek_to_mark, current_task_last_mark, cache);\n     };\n \n     deserialize_settings.continuous_reading = continue_reading;\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex fe1ef7031756..8660813fbaa5 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -34,7 +34,8 @@ struct Settings;\n     M(UInt64, min_bytes_for_wide_part, 10485760, \"Minimal uncompressed size in bytes to create part in wide format instead of compact\", 0) \\\n     M(UInt64, min_rows_for_wide_part, 0, \"Minimal number of rows to create part in wide format instead of compact\", 0) \\\n     M(Float, ratio_of_defaults_for_sparse_serialization, 0.9375f, \"Minimal ratio of number of default values to number of all values in column to store it in sparse serializations. If >= 1, columns will be always written in full serialization.\", 0) \\\n-    \\\n+    M(Bool, replace_long_file_name_to_hash, false, \"If the file name for column is too long (more than 'max_file_name_length' bytes) replace it to SipHash128\", 0) \\\n+    M(UInt64, max_file_name_length, 127, \"The maximal length of the file name to keep it as is without hashing\", 0) \\\n     /** Merge settings. */ \\\n     M(UInt64, merge_max_block_size, 8192, \"How many rows in blocks should be formed for merge operations. By default has the same value as `index_granularity`.\", 0) \\\n     M(UInt64, merge_max_block_size_bytes, 10 * 1024 * 1024, \"How many bytes in blocks should be formed for merge operations. By default has the same value as `index_granularity_bytes`.\", 0) \\\ndiff --git a/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\nindex bfd9e92b4eb6..adea78429c4c 100644\n--- a/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n@@ -142,12 +142,16 @@ MergedBlockOutputStream::Finalizer MergedBlockOutputStream::finalizePartAsync(\n {\n     /// Finish write and get checksums.\n     MergeTreeData::DataPart::Checksums checksums;\n+    NameSet checksums_to_remove;\n \n     if (additional_column_checksums)\n         checksums = std::move(*additional_column_checksums);\n \n     /// Finish columns serialization.\n-    writer->fillChecksums(checksums);\n+    writer->fillChecksums(checksums, checksums_to_remove);\n+\n+    for (const auto & name : checksums_to_remove)\n+        checksums.files.erase(name);\n \n     LOG_TRACE(&Poco::Logger::get(\"MergedBlockOutputStream\"), \"filled checksums {}\", new_part->getNameWithState());\n \ndiff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\nindex 3b2eb96f2d4e..108f364fc2dd 100644\n--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n@@ -63,7 +63,11 @@ MergedColumnOnlyOutputStream::fillChecksums(\n {\n     /// Finish columns serialization.\n     MergeTreeData::DataPart::Checksums checksums;\n-    writer->fillChecksums(checksums);\n+    NameSet checksums_to_remove;\n+    writer->fillChecksums(checksums, checksums_to_remove);\n+\n+    for (const auto & filename : checksums_to_remove)\n+        all_checksums.files.erase(filename);\n \n     for (const auto & [projection_name, projection_part] : new_part->getProjectionParts())\n         checksums.addFile(\n@@ -80,9 +84,7 @@ MergedColumnOnlyOutputStream::fillChecksums(\n     for (const String & removed_file : removed_files)\n     {\n         new_part->getDataPartStorage().removeFileIfExists(removed_file);\n-\n-        if (all_checksums.files.contains(removed_file))\n-            all_checksums.files.erase(removed_file);\n+        all_checksums.files.erase(removed_file);\n     }\n \n     new_part->setColumns(columns, serialization_infos, metadata_snapshot->getMetadataVersion());\ndiff --git a/src/Storages/MergeTree/MutateTask.cpp b/src/Storages/MergeTree/MutateTask.cpp\nindex 15ca2b657319..238a7de78214 100644\n--- a/src/Storages/MergeTree/MutateTask.cpp\n+++ b/src/Storages/MergeTree/MutateTask.cpp\n@@ -1,6 +1,5 @@\n #include <Storages/MergeTree/MutateTask.h>\n \n-#include \"Common/Priority.h\"\n #include <Common/logger_useful.h>\n #include <Common/escapeForFileName.h>\n #include <Storages/MergeTree/DataPartStorageOnDiskFull.h>\n@@ -523,7 +522,9 @@ static std::set<ProjectionDescriptionRawPtr> getProjectionsToRecalculate(\n }\n \n static std::unordered_map<String, size_t> getStreamCounts(\n-    const MergeTreeDataPartPtr & data_part, const Names & column_names)\n+    const MergeTreeDataPartPtr & data_part,\n+    const MergeTreeDataPartChecksums & source_part_checksums,\n+    const Names & column_names)\n {\n     std::unordered_map<String, size_t> stream_counts;\n \n@@ -533,8 +534,9 @@ static std::unordered_map<String, size_t> getStreamCounts(\n         {\n             auto callback = [&](const ISerialization::SubstreamPath & substream_path)\n             {\n-                auto stream_name = ISerialization::getFileNameForStream(column_name, substream_path);\n-                ++stream_counts[stream_name];\n+                auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(column_name, substream_path, source_part_checksums);\n+                if (stream_name)\n+                    ++stream_counts[*stream_name];\n             };\n \n             serialization->enumerateStreams(callback);\n@@ -544,7 +546,6 @@ static std::unordered_map<String, size_t> getStreamCounts(\n     return stream_counts;\n }\n \n-\n /// Files, that we don't need to remove and don't need to hardlink, for example columns.txt and checksums.txt.\n /// Because we will generate new versions of them after we perform mutation.\n static NameSet collectFilesToSkip(\n@@ -572,9 +573,10 @@ static NameSet collectFilesToSkip(\n \n     if (isWidePart(source_part))\n     {\n-        auto new_stream_counts = getStreamCounts(new_part, new_part->getColumns().getNames());\n-        auto source_updated_stream_counts = getStreamCounts(source_part, updated_header.getNames());\n-        auto new_updated_stream_counts = getStreamCounts(new_part, updated_header.getNames());\n+        auto new_stream_counts = getStreamCounts(new_part, source_part->checksums, new_part->getColumns().getNames());\n+        auto source_updated_stream_counts = getStreamCounts(source_part, source_part->checksums, updated_header.getNames());\n+        auto new_updated_stream_counts = getStreamCounts(new_part, source_part->checksums, updated_header.getNames());\n+\n \n         /// Skip all modified files in new part.\n         for (const auto & [stream_name, _] : new_updated_stream_counts)\n@@ -615,7 +617,7 @@ static NameToNameVector collectFilesForRenames(\n     const String & mrk_extension)\n {\n     /// Collect counts for shared streams of different columns. As an example, Nested columns have shared stream with array sizes.\n-    auto stream_counts = getStreamCounts(source_part, source_part->getColumns().getNames());\n+    auto stream_counts = getStreamCounts(source_part, source_part->checksums, source_part->getColumns().getNames());\n     NameToNameVector rename_vector;\n     NameSet collected_names;\n \n@@ -652,12 +654,13 @@ static NameToNameVector collectFilesForRenames(\n             {\n                 ISerialization::StreamCallback callback = [&](const ISerialization::SubstreamPath & substream_path)\n                 {\n-                    String stream_name = ISerialization::getFileNameForStream({command.column_name, command.data_type}, substream_path);\n+                    auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(command.column_name, substream_path, source_part->checksums);\n+\n                     /// Delete files if they are no longer shared with another column.\n-                    if (--stream_counts[stream_name] == 0)\n+                    if (stream_name && --stream_counts[*stream_name] == 0)\n                     {\n-                        add_rename(stream_name + \".bin\", \"\");\n-                        add_rename(stream_name + mrk_extension, \"\");\n+                        add_rename(*stream_name + \".bin\", \"\");\n+                        add_rename(*stream_name + mrk_extension, \"\");\n                     }\n                 };\n \n@@ -671,13 +674,25 @@ static NameToNameVector collectFilesForRenames(\n \n                 ISerialization::StreamCallback callback = [&](const ISerialization::SubstreamPath & substream_path)\n                 {\n-                    String stream_from = ISerialization::getFileNameForStream(command.column_name, substream_path);\n-                    String stream_to = boost::replace_first_copy(stream_from, escaped_name_from, escaped_name_to);\n+                    String full_stream_from = ISerialization::getFileNameForStream(command.column_name, substream_path);\n+                    String full_stream_to = boost::replace_first_copy(full_stream_from, escaped_name_from, escaped_name_to);\n+\n+                    auto stream_from = IMergeTreeDataPart::getStreamNameOrHash(full_stream_from, source_part->checksums);\n+                    if (!stream_from)\n+                        return;\n+\n+                    String stream_to;\n+                    auto storage_settings = source_part->storage.getSettings();\n+\n+                    if (storage_settings->replace_long_file_name_to_hash && full_stream_to.size() > storage_settings->max_file_name_length)\n+                        stream_to = sipHash128String(full_stream_to);\n+                    else\n+                        stream_to = full_stream_to;\n \n                     if (stream_from != stream_to)\n                     {\n-                        add_rename(stream_from + \".bin\", stream_to + \".bin\");\n-                        add_rename(stream_from + mrk_extension, stream_to + mrk_extension);\n+                        add_rename(*stream_from + \".bin\", stream_to + \".bin\");\n+                        add_rename(*stream_from + mrk_extension, stream_to + mrk_extension);\n                     }\n                 };\n \n@@ -690,8 +705,8 @@ static NameToNameVector collectFilesForRenames(\n                 /// but were removed in new_part by MODIFY COLUMN from\n                 /// type with higher number of streams (e.g. LowCardinality -> String).\n \n-                auto old_streams = getStreamCounts(source_part, source_part->getColumns().getNames());\n-                auto new_streams = getStreamCounts(new_part, source_part->getColumns().getNames());\n+                auto old_streams = getStreamCounts(source_part, source_part->checksums, source_part->getColumns().getNames());\n+                auto new_streams = getStreamCounts(new_part, source_part->checksums, source_part->getColumns().getNames());\n \n                 for (const auto & [old_stream, _] : old_streams)\n                 {\ndiff --git a/src/Storages/MergeTree/checkDataPart.cpp b/src/Storages/MergeTree/checkDataPart.cpp\nindex 6e67bfbcdf65..fd379f16f3cc 100644\n--- a/src/Storages/MergeTree/checkDataPart.cpp\n+++ b/src/Storages/MergeTree/checkDataPart.cpp\n@@ -15,6 +15,7 @@\n #include <IO/HashingReadBuffer.h>\n #include <IO/S3Common.h>\n #include <Common/CurrentMetrics.h>\n+#include <Common/SipHash.h>\n #include <Poco/Net/NetException.h>\n \n #if USE_AZURE_BLOB_STORAGE\n@@ -38,6 +39,7 @@ namespace ErrorCodes\n     extern const int CANNOT_MUNMAP;\n     extern const int CANNOT_MREMAP;\n     extern const int UNEXPECTED_FILE_IN_DATA_PART;\n+    extern const int NO_FILE_IN_DATA_PART;\n     extern const int NETWORK_ERROR;\n     extern const int SOCKET_TIMEOUT;\n }\n@@ -200,7 +202,14 @@ static IMergeTreeDataPart::Checksums checkDataPart(\n         {\n             get_serialization(column)->enumerateStreams([&](const ISerialization::SubstreamPath & substream_path)\n             {\n-                String file_name = ISerialization::getFileNameForStream(column, substream_path) + \".bin\";\n+                auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(column, substream_path, \".bin\", data_part_storage);\n+\n+                if (!stream_name)\n+                    throw Exception(ErrorCodes::NO_FILE_IN_DATA_PART,\n+                        \"There is no file for column '{}' in data part '{}'\",\n+                        column.name, data_part->name);\n+\n+                auto file_name = *stream_name + \".bin\";\n                 checksums_data.files[file_name] = checksum_compressed_file(data_part_storage, file_name);\n             });\n         }\ndiff --git a/src/Storages/System/StorageSystemPartsColumns.cpp b/src/Storages/System/StorageSystemPartsColumns.cpp\nindex 67c8d06e4327..275d56c3da59 100644\n--- a/src/Storages/System/StorageSystemPartsColumns.cpp\n+++ b/src/Storages/System/StorageSystemPartsColumns.cpp\n@@ -271,19 +271,22 @@ void StorageSystemPartsColumns::processNextStorage(\n \n                 ColumnSize size;\n                 NameAndTypePair subcolumn(column.name, name, column.type, data.type);\n-                String file_name = ISerialization::getFileNameForStream(subcolumn, subpath);\n \n-                auto bin_checksum = part->checksums.files.find(file_name + \".bin\");\n-                if (bin_checksum != part->checksums.files.end())\n+                auto stream_name = IMergeTreeDataPart::getStreamNameForColumn(subcolumn, subpath, part->checksums);\n+                if (stream_name)\n                 {\n-                    size.data_compressed += bin_checksum->second.file_size;\n-                    size.data_uncompressed += bin_checksum->second.uncompressed_size;\n+                    auto bin_checksum = part->checksums.files.find(*stream_name + \".bin\");\n+                    if (bin_checksum != part->checksums.files.end())\n+                    {\n+                        size.data_compressed += bin_checksum->second.file_size;\n+                        size.data_uncompressed += bin_checksum->second.uncompressed_size;\n+                    }\n+\n+                    auto mrk_checksum = part->checksums.files.find(*stream_name + part->index_granularity_info.mark_type.getFileExtension());\n+                    if (mrk_checksum != part->checksums.files.end())\n+                        size.marks += mrk_checksum->second.file_size;\n                 }\n \n-                auto mrk_checksum = part->checksums.files.find(file_name + part->index_granularity_info.mark_type.getFileExtension());\n-                if (mrk_checksum != part->checksums.files.end())\n-                    size.marks += mrk_checksum->second.file_size;\n-\n                 subcolumn_bytes_on_disk.push_back(size.data_compressed + size.marks);\n                 subcolumn_data_compressed_bytes.push_back(size.data_compressed);\n                 subcolumn_data_uncompressed_bytes.push_back(size.data_uncompressed);\n",
  "test_patch": "diff --git a/tests/clickhouse-test b/tests/clickhouse-test\nindex b9e2f4ddbe6d..cab7d7e79ff9 100755\n--- a/tests/clickhouse-test\n+++ b/tests/clickhouse-test\n@@ -671,6 +671,8 @@ class MergeTreeSettingsRandomizer:\n         \"compress_primary_key\": lambda: random.randint(0, 1),\n         \"marks_compress_block_size\": lambda: random.randint(8000, 100000),\n         \"primary_key_compress_block_size\": lambda: random.randint(8000, 100000),\n+        \"replace_long_file_name_to_hash\": lambda: random.randint(0, 1),\n+        \"max_file_name_length\": threshold_generator(0.3, 0.3, 0, 128),\n     }\n \n     @staticmethod\ndiff --git a/tests/integration/test_backward_compatibility/configs/wide_parts_only.xml b/tests/integration/test_backward_compatibility/configs/wide_parts_only.xml\nindex e9cf053f1c59..674ffff6c935 100644\n--- a/tests/integration/test_backward_compatibility/configs/wide_parts_only.xml\n+++ b/tests/integration/test_backward_compatibility/configs/wide_parts_only.xml\n@@ -1,5 +1,6 @@\n <clickhouse>\n     <merge_tree>\n         <min_bytes_for_wide_part>0</min_bytes_for_wide_part>\n+        <replace_long_file_name_to_hash>0</replace_long_file_name_to_hash>\n     </merge_tree>\n </clickhouse>\ndiff --git a/tests/integration/test_compression_nested_columns/test.py b/tests/integration/test_compression_nested_columns/test.py\nindex 55d881742876..3541a9f6061e 100644\n--- a/tests/integration/test_compression_nested_columns/test.py\n+++ b/tests/integration/test_compression_nested_columns/test.py\n@@ -48,7 +48,7 @@ def test_nested_compression_codec(start_cluster):\n             column_array Array(Array(UInt64)) CODEC(T64, LZ4),\n             column_bad LowCardinality(Int64) CODEC(Delta)\n         ) ENGINE = ReplicatedMergeTree('/t', '{}') ORDER BY tuple() PARTITION BY key\n-        SETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0;\n+        SETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0, replace_long_file_name_to_hash = 0;\n         \"\"\".format(\n                 i\n             ),\ndiff --git a/tests/integration/test_default_compression_codec/configs/long_names.xml b/tests/integration/test_default_compression_codec/configs/long_names.xml\nnew file mode 100644\nindex 000000000000..1dc241dbf051\n--- /dev/null\n+++ b/tests/integration/test_default_compression_codec/configs/long_names.xml\n@@ -0,0 +1,5 @@\n+<clickhouse>\n+    <merge_tree>\n+        <replace_long_file_name_to_hash>0</replace_long_file_name_to_hash>\n+    </merge_tree>\n+</clickhouse>\ndiff --git a/tests/integration/test_default_compression_codec/test.py b/tests/integration/test_default_compression_codec/test.py\nindex c8b75ea9751d..82d5eb04d2aa 100644\n--- a/tests/integration/test_default_compression_codec/test.py\n+++ b/tests/integration/test_default_compression_codec/test.py\n@@ -9,12 +9,20 @@\n \n node1 = cluster.add_instance(\n     \"node1\",\n-    main_configs=[\"configs/default_compression.xml\", \"configs/wide_parts_only.xml\"],\n+    main_configs=[\n+        \"configs/default_compression.xml\",\n+        \"configs/wide_parts_only.xml\",\n+        \"configs/long_names.xml\",\n+    ],\n     with_zookeeper=True,\n )\n node2 = cluster.add_instance(\n     \"node2\",\n-    main_configs=[\"configs/default_compression.xml\", \"configs/wide_parts_only.xml\"],\n+    main_configs=[\n+        \"configs/default_compression.xml\",\n+        \"configs/wide_parts_only.xml\",\n+        \"configs/long_names.xml\",\n+    ],\n     with_zookeeper=True,\n )\n node3 = cluster.add_instance(\ndiff --git a/tests/integration/test_filesystem_layout/test.py b/tests/integration/test_filesystem_layout/test.py\nindex 2be478f95d08..4e719aa0fe94 100644\n--- a/tests/integration/test_filesystem_layout/test.py\n+++ b/tests/integration/test_filesystem_layout/test.py\n@@ -23,7 +23,7 @@ def test_file_path_escaping(started_cluster):\n     node.query(\n         \"\"\"\n         CREATE TABLE test.`T.a_b,l-e!` (`~Id` UInt32)\n-        ENGINE = MergeTree() PARTITION BY `~Id` ORDER BY `~Id` SETTINGS min_bytes_for_wide_part = 0;\n+        ENGINE = MergeTree() PARTITION BY `~Id` ORDER BY `~Id` SETTINGS min_bytes_for_wide_part = 0, replace_long_file_name_to_hash = 0;\n         \"\"\"\n     )\n     node.query(\"\"\"INSERT INTO test.`T.a_b,l-e!` VALUES (1);\"\"\")\n@@ -48,7 +48,7 @@ def test_file_path_escaping(started_cluster):\n     node.query(\n         \"\"\"\n         CREATE TABLE `test 2`.`T.a_b,l-e!` UUID '12345678-1000-4000-8000-000000000001' (`~Id` UInt32)\n-        ENGINE = MergeTree() PARTITION BY `~Id` ORDER BY `~Id` SETTINGS min_bytes_for_wide_part = 0;\n+        ENGINE = MergeTree() PARTITION BY `~Id` ORDER BY `~Id` SETTINGS min_bytes_for_wide_part = 0, replace_long_file_name_to_hash = 0;\n         \"\"\"\n     )\n     node.query(\"\"\"INSERT INTO `test 2`.`T.a_b,l-e!` VALUES (1);\"\"\")\ndiff --git a/tests/integration/test_mutations_hardlinks/configs/wide_parts_only.xml b/tests/integration/test_mutations_hardlinks/configs/wide_parts_only.xml\nindex 10b9edef36dd..4d1a33577995 100644\n--- a/tests/integration/test_mutations_hardlinks/configs/wide_parts_only.xml\n+++ b/tests/integration/test_mutations_hardlinks/configs/wide_parts_only.xml\n@@ -2,5 +2,6 @@\n     <merge_tree>\n         <min_rows_for_wide_part>0</min_rows_for_wide_part>\n         <min_bytes_for_wide_part>0</min_bytes_for_wide_part>\n+        <replace_long_file_name_to_hash>0</replace_long_file_name_to_hash>\n     </merge_tree>\n </clickhouse>\ndiff --git a/tests/integration/test_partition/test.py b/tests/integration/test_partition/test.py\nindex 97d21d14184f..054418a8ba90 100644\n--- a/tests/integration/test_partition/test.py\n+++ b/tests/integration/test_partition/test.py\n@@ -150,7 +150,7 @@ def partition_table_complex(started_cluster):\n     q(\"DROP TABLE IF EXISTS test.partition_complex\")\n     q(\n         \"CREATE TABLE test.partition_complex (p Date, k Int8, v1 Int8 MATERIALIZED k + 1) \"\n-        \"ENGINE = MergeTree PARTITION BY p ORDER BY k SETTINGS index_granularity=1, index_granularity_bytes=0, compress_marks=false, compress_primary_key=false, ratio_of_defaults_for_sparse_serialization=1\"\n+        \"ENGINE = MergeTree PARTITION BY p ORDER BY k SETTINGS index_granularity=1, index_granularity_bytes=0, compress_marks=false, compress_primary_key=false, ratio_of_defaults_for_sparse_serialization=1, replace_long_file_name_to_hash=false\"\n     )\n     q(\"INSERT INTO test.partition_complex (p, k) VALUES(toDate(31), 1)\")\n     q(\"INSERT INTO test.partition_complex (p, k) VALUES(toDate(1), 2)\")\ndiff --git a/tests/queries/0_stateless/00961_checksums_in_system_parts_columns_table.sql b/tests/queries/0_stateless/00961_checksums_in_system_parts_columns_table.sql\nindex 8df7d728560d..70db45e4f368 100644\n--- a/tests/queries/0_stateless/00961_checksums_in_system_parts_columns_table.sql\n+++ b/tests/queries/0_stateless/00961_checksums_in_system_parts_columns_table.sql\n@@ -4,7 +4,13 @@ DROP TABLE IF EXISTS test_00961;\n \n CREATE TABLE test_00961 (d Date, a String, b UInt8, x String, y Int8, z UInt32)\n     ENGINE = MergeTree PARTITION BY d ORDER BY (a, b)\n-    SETTINGS index_granularity = 111, min_bytes_for_wide_part = 0, compress_marks = 0, compress_primary_key = 0, index_granularity_bytes = '10Mi', ratio_of_defaults_for_sparse_serialization = 1;\n+    SETTINGS index_granularity = 111,\n+    min_bytes_for_wide_part = 0,\n+    compress_marks = 0,\n+    compress_primary_key = 0,\n+    index_granularity_bytes = '10Mi',\n+    ratio_of_defaults_for_sparse_serialization = 1,\n+    replace_long_file_name_to_hash = 0;\n \n INSERT INTO test_00961 VALUES ('2000-01-01', 'Hello, world!', 123, 'xxx yyy', -123, 123456789);\n \ndiff --git a/tests/queries/0_stateless/02253_empty_part_checksums.sh b/tests/queries/0_stateless/02253_empty_part_checksums.sh\nindex 5d4e750f46de..371c0768e3d0 100755\n--- a/tests/queries/0_stateless/02253_empty_part_checksums.sh\n+++ b/tests/queries/0_stateless/02253_empty_part_checksums.sh\n@@ -10,7 +10,7 @@ $CLICKHOUSE_CLIENT -q \"drop table if exists rmt sync;\"\n $CLICKHOUSE_CLIENT -q \"CREATE TABLE rmt (a UInt8, b Int16, c Float32, d String, e Array(UInt8), f Nullable(UUID), g Tuple(UInt8, UInt16))\n ENGINE = ReplicatedMergeTree('/test/02253/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX/rmt', '1') ORDER BY a PARTITION BY b % 10\n SETTINGS old_parts_lifetime = 1, cleanup_delay_period = 0, cleanup_delay_period_random_add = 0,\n-cleanup_thread_preferred_points_per_iteration=0, min_bytes_for_wide_part=0, remove_empty_parts=0\"\n+cleanup_thread_preferred_points_per_iteration=0, min_bytes_for_wide_part=0, remove_empty_parts=0, replace_long_file_name_to_hash=0\"\n \n $CLICKHOUSE_CLIENT --insert_keeper_fault_injection_probability=0 -q \"INSERT INTO rmt SELECT rand(1), 0, 1 / rand(3), toString(rand(4)), [rand(5), rand(6)], rand(7) % 2 ? NULL : generateUUIDv4(), (rand(8), rand(9)) FROM numbers(1000);\"\n \ndiff --git a/tests/queries/0_stateless/02869_insert_filenames_collisions.reference b/tests/queries/0_stateless/02869_insert_filenames_collisions.reference\nnew file mode 100644\nindex 000000000000..d835614ae165\n--- /dev/null\n+++ b/tests/queries/0_stateless/02869_insert_filenames_collisions.reference\n@@ -0,0 +1,3 @@\n+e798545eefc8b7a1c2c81ff00c064ad8\n+1\t1\n+2\t2\ndiff --git a/tests/queries/0_stateless/02869_insert_filenames_collisions.sql b/tests/queries/0_stateless/02869_insert_filenames_collisions.sql\nnew file mode 100644\nindex 000000000000..441355930059\n--- /dev/null\n+++ b/tests/queries/0_stateless/02869_insert_filenames_collisions.sql\n@@ -0,0 +1,73 @@\n+DROP TABLE IF EXISTS t_collisions;\n+\n+SELECT lower(hex(reverse(CAST(sipHash128('very_very_long_column_name_that_will_be_replaced_with_hash'), 'FixedString(16)'))));\n+\n+CREATE TABLE t_collisions\n+(\n+    `very_very_long_column_name_that_will_be_replaced_with_hash` Int32,\n+    `e798545eefc8b7a1c2c81ff00c064ad8` Int32\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple()\n+SETTINGS replace_long_file_name_to_hash = 1, max_file_name_length = 42; -- { serverError BAD_ARGUMENTS }\n+\n+DROP TABLE IF EXISTS t_collisions;\n+\n+CREATE TABLE t_collisions\n+(\n+    `col1` Int32,\n+    `e798545eefc8b7a1c2c81ff00c064ad8` Int32\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple()\n+SETTINGS replace_long_file_name_to_hash = 1, max_file_name_length = 42;\n+\n+ALTER TABLE t_collisions ADD COLUMN very_very_long_column_name_that_will_be_replaced_with_hash Int32;  -- { serverError BAD_ARGUMENTS }\n+ALTER TABLE t_collisions RENAME COLUMN col1 TO very_very_long_column_name_that_will_be_replaced_with_hash;  -- { serverError BAD_ARGUMENTS }\n+\n+DROP TABLE IF EXISTS t_collisions;\n+\n+CREATE TABLE t_collisions\n+(\n+    `very_very_long_column_name_that_will_be_replaced_with_hash` Int32,\n+    `e798545eefc8b7a1c2c81ff00c064ad8` Int32\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple()\n+SETTINGS replace_long_file_name_to_hash = 0;\n+\n+INSERT INTO t_collisions VALUES (1, 1);\n+\n+ALTER TABLE t_collisions MODIFY SETTING replace_long_file_name_to_hash = 1, max_file_name_length = 42; -- { serverError BAD_ARGUMENTS }\n+\n+INSERT INTO t_collisions VALUES (2, 2);\n+\n+SELECT * FROM t_collisions ORDER BY e798545eefc8b7a1c2c81ff00c064ad8;\n+\n+DROP TABLE IF EXISTS t_collisions;\n+\n+CREATE TABLE t_collisions\n+(\n+    `id` Int,\n+    `col` Array(String),\n+    `col.s` Array(LowCardinality(String)),\n+    `col.u` Array(LowCardinality(String))\n+)\n+ENGINE = MergeTree\n+ORDER BY id; -- { serverError BAD_ARGUMENTS }\n+\n+DROP TABLE IF EXISTS t_collisions;\n+\n+CREATE TABLE t_collisions\n+(\n+    `id` Int,\n+    `col` String,\n+    `col.s` Array(LowCardinality(String)),\n+    `col.u` Array(LowCardinality(String))\n+)\n+ENGINE = MergeTree\n+ORDER BY id;\n+\n+ALTER TABLE t_collisions MODIFY COLUMN col Array(String); -- { serverError BAD_ARGUMENTS }\n+\n+DROP TABLE IF EXISTS t_collisions;\n",
  "problem_statement": "Object('json'): File name too long. (CANNOT_OPEN_FILE) long key\n**Describe what's wrong**\r\n\r\nWhen cloning a bunch of data over to a new Object('json') column, I got the following error.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\n\r\nTested on 22.6.1 revision 54455\r\n\r\n**How to reproduce**\r\n\r\nThis occurred to me when copying ~1M records from an old table to a new one. I did manage to eventually reproduce this:\r\n\r\n```sql\r\n\r\nSET allow_experimental_object_type = 1;\r\nCREATE TABLE t_json(id UInt64, obj JSON) ENGINE = MergeTree ORDER BY id;\r\n\r\nINSERT INTO t_json (id, obj) SELECT\r\n    number,\r\n    concat('{\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', toString(number), '\":1}')\r\nFROM system.numbers\r\nLIMIT 10000;\r\n```\r\n\r\n**Expected behavior**\r\n\r\nINSERT succeeds or long object keys are explicitly forbidden.\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\n2022.06.29 09:04:37.920717 [ 18087 ] {44e0edf4-9939-4ab4-a0c3-48300e794fb5} <Error> TCPHandler: Code: 76. DB::ErrnoException: Cannot open file /var/lib/clickhouse/store/d71/d712ff2b-a7fe-4ad6-bdee-80acea7e7087/t\r\nmp_insert_all_16_16_0/obj%2Eaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0.bin, errno: 36, strerror: File name too long. (CANNOT_OPEN_FILE), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xb8a147a in /usr/bin/clickhouse\r\n1. DB::throwFromErrnoWithPath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int) @ 0xb8a252a in /usr/bin/clickhouse\r\n2. DB::WriteBufferFromFile::WriteBufferFromFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, int, unsigned int, char*, unsigned long) @ 0xb9ea23a in /usr/bin/clickhouse\r\n3. DB::DiskLocal::writeFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, DB::WriteMode, DB::WriteSettings const&) @ 0x159734b7 in /usr/bin/clickhouse\r\n4. DB::MergeTreeDataPartWriterOnDisk::Stream::Stream(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::IDisk>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::ICompressionCodec> const&, unsigned long, DB::WriteSettings const&) @ 0x16c1e02b in /usr/bin/clickhouse\r\n5. ? @ 0x16c27db8 in /usr/bin/clickhouse\r\n6. DB::ISerialization::enumerateStreams(DB::ISerialization::SubstreamPath&, std::__1::function<void (DB::ISerialization::SubstreamPath const&)> const&, DB::ISerialization::SubstreamData const&) const @ 0x15840511 in /usr/bin/clickhouse\r\n7. DB::SerializationNamed::enumerateStreams(DB::ISerialization::SubstreamPath&, std::__1::function<void (DB::ISerialization::SubstreamPath const&)> const&, DB::ISerialization::SubstreamData const&) const @ 0x15865bfb in /usr/bin/clickhouse\r\n8. DB::SerializationTuple::enumerateStreams(DB::ISerialization::SubstreamPath&, std::__1::function<void (DB::ISerialization::SubstreamPath const&)> const&, DB::ISerialization::SubstreamData const&) const @ 0x158874c6 in /usr/bin/clickhouse\r\n9. DB::ISerialization::enumerateStreams(DB::ISerialization::SubstreamPath&, std::__1::function<void (DB::ISerialization::SubstreamPath const&)> const&, std::__1::shared_ptr<DB::IDataType const> const&) const @ 0x158408f5 in /usr/bin/clickhouse\r\n10. DB::MergeTreeDataPartWriterWide::addStreams(DB::NameAndTypePair const&, std::__1::shared_ptr<DB::IAST> const&) @ 0x16c2196c in /usr/bin/clickhouse\r\n11. DB::MergeTreeDataPartWriterWide::MergeTreeDataPartWriterWide(std::__1::shared_ptr<DB::IMergeTreeDataPart const> const&, DB::NamesAndTypesList const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeIndex const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeIndex const> > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::ICompressionCodec> const&, DB::MergeTreeWriterSettings const&, DB::MergeTreeIndexGranularity const&) @ 0x16c2178a in /usr/bin/clickhouse\r\n12. DB::MergeTreeDataPartWide::getWriter(DB::NamesAndTypesList const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeIndex const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeIndex const> > > const&, std::__1::shared_ptr<DB::ICompressionCodec> const&, DB::MergeTreeWriterSettings const&, DB::MergeTreeIndexGranularity const&) const @ 0x16c11162 in /usr/bin/clickhouse\r\n13. DB::MergedBlockOutputStream::MergedBlockOutputStream(std::__1::shared_ptr<DB::IMergeTreeDataPart const> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::NamesAndTypesList const&, std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeIndex const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeIndex const> > > const&, std::__1::shared_ptr<DB::ICompressionCodec>, std::__1::shared_ptr<DB::MergeTreeTransaction> const&, bool, bool, DB::WriteSettings const&) @ 0x16d0573e in /usr/bin/clickhouse\r\n14. DB::MergeTreeDataWriter::writeTempPart(DB::BlockWithPartition&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::Context const>) @ 0x16cf31aa in /usr/bin/clickhouse\r\n15. DB::MergeTreeSink::consume(DB::Chunk) @ 0x16ce8284 in /usr/bin/clickhouse\r\n16. DB::SinkToStorage::onConsume(DB::Chunk) @ 0x17407b40 in /usr/bin/clickhouse\r\n17. ? @ 0x17373ff7 in /usr/bin/clickhouse\r\n18. ? @ 0x17373cd6 in /usr/bin/clickhouse\r\n19. DB::ExceptionKeepingTransform::work() @ 0x1737354a in /usr/bin/clickhouse\r\n20. DB::ExecutionThreadContext::executeTask() @ 0x171af9fa in /usr/bin/clickhouse\r\n21. DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x171a491e in /usr/bin/clickhouse\r\n22. DB::PipelineExecutor::executeImpl(unsigned long) @ 0x171a3921 in /usr/bin/clickhouse\r\n23. DB::PipelineExecutor::execute(unsigned long) @ 0x171a36b8 in /usr/bin/clickhouse\r\n24. ? @ 0x171a2a2b in /usr/bin/clickhouse\r\n25. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xb94d0b7 in /usr/bin/clickhouse\r\n26. ? @ 0xb9504dd in /usr/bin/clickhouse\r\n27. ? @ 0x7fae6e91db43 in ?\r\n28. ? @ 0x7fae6e9afa00 in ?\r\n```\r\n\r\n**Additional context**\r\n\r\nThis only seems to occur when the table switches to the wide format, so the setup above might need to be run a few times.\nAdding Checks on the column name to avoid: strerror: File name too long.\nClickHouse should have checks on the column name to avoid long column names that could lead to the following error:\r\n\r\n> <Error> MergeFromLogEntryTask: virtual bool DB::ReplicatedMergeMutateTaskBase::executeStep(): Code: 76. DB::ErrnoException: Cannot open file /var/lib/clickhouse/data/yc%2Dosquery/shard_syslog_shell/tmp_merge_20220422_46793_50514_2474/sudo__command_continued__cr%2Ecorp_yc%2Dcr%2Dinternal_container%2Dregistry%2Dmigrator_cr%2D2022%2D04%2D21%2Dcloud%2D98762%2Dpeasa%2D25602%2D58a09ee3f3_java_%2Djar_maven_container%2Dregistry%2Dmigrator%2Dcr%2D2022%2D04%2D21%2Dcloud%2D98762%2Dpeasa%2D25602%2D58a09ee3f3%2Ejar__migration.bin, errno: 36, strerror: File name too long.\r\n\r\nThis kind of issue might happen if you have an automated process of creating tables and columns. It would be nice if there was a hard/soft limit implemented to make sure that column names do not exceed a specific threshold.\r\n\r\nOr maybe we could find a different mechanism to create the name of the file.\r\n\n",
  "hints_text": "Related #37119\nI'm running into the same problem. My json data has some pretty long URLs as keys (can't change the data, unfortunately).\nrelated https://github.com/ClickHouse/ClickHouse/issues/36485\r\n\nIs this feature completed now?\nI think we can introduce user-level settings like:\r\n`max_table_name_bytes_length`\r\n`max_column_name_bytes_length`\r\n`max_table_and_column_name_bytes_length`",
  "created_at": "2023-06-06T00:49:01Z"
}