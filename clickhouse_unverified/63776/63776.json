{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 63776,
  "instance_id": "ClickHouse__ClickHouse-63776",
  "issue_numbers": [
    "62820"
  ],
  "base_commit": "cf1f1f56e44d861c49326c5ee67fbcd59da3011f",
  "patch": "diff --git a/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp b/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp\nindex 232d3118612b..8b92cc45cee6 100644\n--- a/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp\n@@ -64,36 +64,61 @@ namespace\n         return non_const_columns;\n     }\n \n+    /// build actions DAG from stack of steps\n+    ActionsDAGPtr buildActionsForPlanPath(std::vector<ActionsDAGPtr> & dag_stack)\n+    {\n+        if (dag_stack.empty())\n+            return nullptr;\n+\n+        ActionsDAGPtr path_actions = dag_stack.back()->clone();\n+        dag_stack.pop_back();\n+        while (!dag_stack.empty())\n+        {\n+            ActionsDAGPtr clone = dag_stack.back()->clone();\n+            logActionsDAG(\"DAG to merge\", clone);\n+            dag_stack.pop_back();\n+            path_actions->mergeInplace(std::move(*clone));\n+        }\n+        return path_actions;\n+    }\n+\n     bool compareAggregationKeysWithDistinctColumns(\n-        const Names & aggregation_keys, const DistinctColumns & distinct_columns, const ActionsDAGPtr & path_actions)\n+        const Names & aggregation_keys, const DistinctColumns & distinct_columns, std::vector<std::vector<ActionsDAGPtr>> actions_chain)\n     {\n         logDebug(\"aggregation_keys\", aggregation_keys);\n         logDebug(\"aggregation_keys size\", aggregation_keys.size());\n         logDebug(\"distinct_columns size\", distinct_columns.size());\n \n-        std::set<std::string_view> original_distinct_columns;\n-        FindOriginalNodeForOutputName original_node_finder(path_actions);\n-        for (const auto & column : distinct_columns)\n+        std::set<String> current_columns(begin(distinct_columns), end(distinct_columns));\n+        std::set<String> source_columns;\n+        for (auto & actions : actions_chain)\n         {\n-            logDebug(\"distinct column name\", column);\n-            const auto * alias_node = original_node_finder.find(String(column));\n-            if (!alias_node)\n+            FindOriginalNodeForOutputName original_node_finder(buildActionsForPlanPath(actions));\n+            for (const auto & column : current_columns)\n             {\n-                logDebug(\"original name for alias is not found\", column);\n-                original_distinct_columns.insert(column);\n-            }\n-            else\n-            {\n-                logDebug(\"alias result name\", alias_node->result_name);\n-                original_distinct_columns.insert(alias_node->result_name);\n+                logDebug(\"distinct column name\", column);\n+                const auto * alias_node = original_node_finder.find(String(column));\n+                if (!alias_node)\n+                {\n+                    logDebug(\"original name for alias is not found\", column);\n+                    source_columns.insert(String(column));\n+                }\n+                else\n+                {\n+                    logDebug(\"alias result name\", alias_node->result_name);\n+                    source_columns.insert(alias_node->result_name);\n+                }\n             }\n+\n+            current_columns = std::move(source_columns);\n+            source_columns.clear();\n         }\n         /// if aggregation keys are part of distinct columns then rows already distinct\n         for (const auto & key : aggregation_keys)\n         {\n-            if (!original_distinct_columns.contains(key))\n+            if (!current_columns.contains(key))\n             {\n-                logDebug(\"aggregation key NOT found: {}\", key);\n+                logDebug(\"aggregation key NOT found\", key);\n                 return false;\n             }\n         }\n@@ -122,30 +147,13 @@ namespace\n         return false;\n     }\n \n-    /// build actions DAG from stack of steps\n-    ActionsDAGPtr buildActionsForPlanPath(std::vector<ActionsDAGPtr> & dag_stack)\n-    {\n-        if (dag_stack.empty())\n-            return nullptr;\n-\n-        ActionsDAGPtr path_actions = dag_stack.back()->clone();\n-        dag_stack.pop_back();\n-        while (!dag_stack.empty())\n-        {\n-            ActionsDAGPtr clone = dag_stack.back()->clone();\n-            logActionsDAG(\"DAG to merge\", clone);\n-            dag_stack.pop_back();\n-            path_actions->mergeInplace(std::move(*clone));\n-        }\n-        return path_actions;\n-    }\n-\n     bool passTillAggregation(const QueryPlan::Node * distinct_node)\n     {\n         const DistinctStep * distinct_step = typeid_cast<DistinctStep *>(distinct_node->step.get());\n         chassert(distinct_step);\n \n         std::vector<ActionsDAGPtr> dag_stack;\n+        std::vector<std::vector<ActionsDAGPtr>> actions_chain;\n         const DistinctStep * inner_distinct_step = nullptr;\n         const IQueryPlanStep * aggregation_before_distinct = nullptr;\n         const QueryPlan::Node * node = distinct_node;\n@@ -163,6 +171,12 @@ namespace\n                 break;\n             }\n \n+            if (typeid_cast<const WindowStep *>(current_step))\n+            {\n+                actions_chain.push_back(std::move(dag_stack));\n+                dag_stack.clear();\n+            }\n+\n             if (const auto * const expr = typeid_cast<const ExpressionStep *>(current_step); expr)\n                 dag_stack.push_back(expr->getExpression());\n             else if (const auto * const filter = typeid_cast<const FilterStep *>(current_step); filter)\n@@ -177,16 +191,22 @@ namespace\n \n         if (aggregation_before_distinct)\n         {\n-            ActionsDAGPtr actions = buildActionsForPlanPath(dag_stack);\n-            logActionsDAG(\"aggregation pass: merged DAG\", actions);\n+            if (actions_chain.empty())\n+                actions_chain.push_back(std::move(dag_stack));\n \n             const auto distinct_columns = getDistinctColumns(distinct_step);\n \n             if (const auto * aggregating_step = typeid_cast<const AggregatingStep *>(aggregation_before_distinct); aggregating_step)\n-                return compareAggregationKeysWithDistinctColumns(aggregating_step->getParams().keys, distinct_columns, actions);\n+            {\n+                return compareAggregationKeysWithDistinctColumns(\n+                    aggregating_step->getParams().keys, distinct_columns, std::move(actions_chain));\n+            }\n             else if (const auto * merging_aggregated_step = typeid_cast<const MergingAggregatedStep *>(aggregation_before_distinct);\n                      merging_aggregated_step)\n-                return compareAggregationKeysWithDistinctColumns(merging_aggregated_step->getParams().keys, distinct_columns, actions);\n+            {\n+                return compareAggregationKeysWithDistinctColumns(\n+                    merging_aggregated_step->getParams().keys, distinct_columns, std::move(actions_chain));\n+            }\n         }\n \n         return false;\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03151_redundant_distinct_with_window.reference b/tests/queries/0_stateless/03151_redundant_distinct_with_window.reference\nnew file mode 100644\nindex 000000000000..e321055f1e29\n--- /dev/null\n+++ b/tests/queries/0_stateless/03151_redundant_distinct_with_window.reference\n@@ -0,0 +1,7 @@\n+1\n+2\n+3\n+--------\n+1\t2023-01-14 00:00:00\n+2\t2023-01-14 00:00:00\n+3\t2023-01-14 00:00:00\ndiff --git a/tests/queries/0_stateless/03151_redundant_distinct_with_window.sql b/tests/queries/0_stateless/03151_redundant_distinct_with_window.sql\nnew file mode 100644\nindex 000000000000..79e0074e91bb\n--- /dev/null\n+++ b/tests/queries/0_stateless/03151_redundant_distinct_with_window.sql\n@@ -0,0 +1,21 @@\n+DROP TABLE IF EXISTS tab;\n+DROP TABLE IF EXISTS tab_v;\n+\n+CREATE TABLE tab (id Int32, val Nullable(Float64), dt Nullable(DateTime64(6)), type Nullable(Int32)) ENGINE = MergeTree ORDER BY id;\n+\n+insert into tab values (1,10,'2023-01-14 00:00:00',1),(2,20,'2023-01-14 00:00:00',1),(3,20,'2023-01-14 00:00:00',2),(4,40,'2023-01-14 00:00:00',3),(5,50,'2023-01-14 00:00:00',3);\n+\n+CREATE VIEW tab_v AS SELECT\n+    t1.type AS type,\n+    sum(t1.val) AS sval,\n+    toStartOfDay(t1.dt) AS sday,\n+    anyLast(sval) OVER w AS lval\n+FROM tab AS t1\n+GROUP BY\n+    type,\n+    sday\n+WINDOW w AS (PARTITION BY type);\n+\n+select distinct type from tab_v order by type;\n+select '--------';\n+select distinct type, sday from tab_v order by type, sday;\n",
  "problem_statement": "Exception Cannot find column if make select distinct from view and view contains window function and group by\nIf the view contains a grouping and a windowing function, querying with distinct results in an error.\r\n\r\nCREATE TABLE default.t\r\n(\r\n    `id` Int32,\r\n    `val` Nullable(Float64),\r\n    `dt` Nullable(DateTime64(6)),\r\n    `type` Nullable(Int32)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id\r\n\r\n\r\nCREATE VIEW default.t_v\r\nAS SELECT\r\n    t1.type AS type,\r\n    sum(t1.val) AS sval,\r\n    toStartOfDay(t1.dt) AS sday,\r\n    anyLast(sum(t1.val)) OVER w\r\nFROM default.t AS t1\r\nGROUP BY\r\n    type,\r\n    t1.dt\r\nWINDOW w AS (PARTITION BY type ORDER BY dt ASC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)\r\n\r\nSELECT DISTINCT type\r\nFROM t_v\r\n\r\n\r\nReceived exception from server (version 24.3.2):\r\nCode: 47. DB::Exception: Received from 84.201.141.233:9000. DB::Exception: Cannot find column anyLast(sum(__table1.val)) OVER (PARTITION BY __table1.type ORDER BY __table1.dt ASC ROWS BETWEEN UNBOUNDED PRECEDING AND 1_UInt8 PRECEDING) in ActionsDAG result. (UNKNOWN_IDENTIFIER)\r\n\r\nExample in fiddle\r\nhttps://fiddle.clickhouse.com/f3f2dac4-183d-4748-85ae-9b25452684c9\r\n\r\nThe problem is also reproduced on new installations and in 24.1.8.22\r\n\n",
  "hints_text": "the problem is not reproduced in Yandex Managed Servese for Clickhouse same version\neverything is work at version 23.8.12\nThe error is related to the new analyzer. It is resolved by setting SET allow_experimental_analyzer=0;",
  "created_at": "2024-05-14T14:38:08Z"
}