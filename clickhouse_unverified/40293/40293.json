{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 40293,
  "instance_id": "ClickHouse__ClickHouse-40293",
  "issue_numbers": [
    "32401"
  ],
  "base_commit": "a3a124cc349adf95fff5016eac176bf33d2d3c67",
  "patch": "diff --git a/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp b/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp\nindex 36f4536d9188..a6ca5bcf9164 100644\n--- a/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp\n+++ b/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp\n@@ -98,6 +98,7 @@ static ColumnWithTypeAndName readColumnWithNumericData(std::shared_ptr<arrow::Ch\n /// Inserts chars and offsets right into internal column data to reduce an overhead.\n /// Internal offsets are shifted by one to the right in comparison with Arrow ones. So the last offset should map to the end of all chars.\n /// Also internal strings are null terminated.\n+template <typename ArrowArray>\n static ColumnWithTypeAndName readColumnWithStringData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, const String & column_name)\n {\n     auto internal_type = std::make_shared<DataTypeString>();\n@@ -108,7 +109,7 @@ static ColumnWithTypeAndName readColumnWithStringData(std::shared_ptr<arrow::Chu\n     size_t chars_t_size = 0;\n     for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n     {\n-        arrow::BinaryArray & chunk = dynamic_cast<arrow::BinaryArray &>(*(arrow_column->chunk(chunk_i)));\n+        ArrowArray & chunk = dynamic_cast<ArrowArray &>(*(arrow_column->chunk(chunk_i)));\n         const size_t chunk_length = chunk.length();\n \n         if (chunk_length > 0)\n@@ -123,7 +124,7 @@ static ColumnWithTypeAndName readColumnWithStringData(std::shared_ptr<arrow::Chu\n \n     for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n     {\n-        arrow::BinaryArray & chunk = dynamic_cast<arrow::BinaryArray &>(*(arrow_column->chunk(chunk_i)));\n+        ArrowArray & chunk = dynamic_cast<ArrowArray &>(*(arrow_column->chunk(chunk_i)));\n         std::shared_ptr<arrow::Buffer> buffer = chunk.value_data();\n         const size_t chunk_length = chunk.length();\n \n@@ -418,7 +419,10 @@ static ColumnWithTypeAndName readColumnFromArrowColumn(\n         case arrow::Type::STRING:\n         case arrow::Type::BINARY:\n             //case arrow::Type::FIXED_SIZE_BINARY:\n-            return readColumnWithStringData(arrow_column, column_name);\n+            return readColumnWithStringData<arrow::BinaryArray>(arrow_column, column_name);\n+        case arrow::Type::LARGE_BINARY:\n+        case arrow::Type::LARGE_STRING:\n+            return readColumnWithStringData<arrow::LargeBinaryArray>(arrow_column, column_name);\n         case arrow::Type::BOOL:\n             return readColumnWithBooleanData(arrow_column, column_name);\n         case arrow::Type::DATE32:\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02403_arrow_large_string.reference b/tests/queries/0_stateless/02403_arrow_large_string.reference\nnew file mode 100644\nindex 000000000000..9a761064d687\n--- /dev/null\n+++ b/tests/queries/0_stateless/02403_arrow_large_string.reference\n@@ -0,0 +1,5 @@\n+a\n+Nullable(String)\n+00000\n+00001\n+00002\ndiff --git a/tests/queries/0_stateless/02403_arrow_large_string.sh b/tests/queries/0_stateless/02403_arrow_large_string.sh\nnew file mode 100755\nindex 000000000000..14f2b1f6e6a6\n--- /dev/null\n+++ b/tests/queries/0_stateless/02403_arrow_large_string.sh\n@@ -0,0 +1,28 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# ## generate arrow file with python\n+# import pyarrow as pa\n+# schema = pa.schema([ pa.field('a', pa.large_utf8()) ])\n+# a = pa.array([\"00000\", \"00001\", \"00002\"])\n+# with pa.OSFile('arraydata.arrow', 'wb') as sink:\n+#    with pa.ipc.new_file(sink, schema=schema) as writer:\n+#        batch = pa.record_batch([a], schema=schema)\n+#        writer.write(batch)\n+\n+# cat arraydata.arrow | base64\n+\n+cat <<EOF | base64 --decode |  $CLICKHOUSE_LOCAL --query='SELECT * FROM table FORMAT TSVWithNamesAndTypes' --input-format=Arrow\n+QVJST1cxAAD/////cAAAABAAAAAAAAoADAAGAAUACAAKAAAAAAEEAAwAAAAIAAgAAAAEAAgAAAAE\n+AAAAAQAAABQAAAAQABQACAAGAAcADAAAABAAEAAAAAAAARQQAAAAGAAAAAQAAAAAAAAAAQAAAGEA\n+AAAEAAQABAAAAAAAAAD/////mAAAABQAAAAAAAAADAAWAAYABQAIAAwADAAAAAADBAAYAAAAMAAA\n+AAAAAAAAAAoAGAAMAAQACAAKAAAATAAAABAAAAADAAAAAAAAAAAAAAADAAAAAAAAAAAAAAAAAAAA\n+AAAAAAAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAAA8AAAAAAAAAAAAAAAEAAAADAAAAAAAAAAAAAAAA\n+AAAAAAAAAAAAAAAFAAAAAAAAAAoAAAAAAAAADwAAAAAAAAAwMDAwMDAwMDAxMDAwMDIA/////wAA\n+AAAQAAAADAAUAAYACAAMABAADAAAAAAABAA4AAAAKAAAAAQAAAABAAAAgAAAAAAAAACgAAAAAAAA\n+ADAAAAAAAAAAAAAAAAAAAAAIAAgAAAAEAAgAAAAEAAAAAQAAABQAAAAQABQACAAGAAcADAAAABAA\n+EAAAAAAAARQQAAAAGAAAAAQAAAAAAAAAAQAAAGEAAAAEAAQABAAAAJgAAABBUlJPVzE=\n+EOF\n",
  "problem_statement": "Support large_utf8 column format on parquet\n**Describe the unexpected behaviour**\r\nClickhouse does not support parquet with 'large_utf8' columns (only 'utf8'). From my understanding, the only difference is offsets are u64 in 'large_utf8' instead of u32\r\n\r\n**How to reproduce**\r\n\r\n```\r\nimport pyarrow\r\nimport pyarrow.parquet\r\nimport subprocess\r\n\r\n\r\nfor schema in (None,pyarrow.schema({\"a\": pyarrow.large_utf8()})):\r\n    a = pyarrow.table({\"a\": [\"00000\"]}, schema=schema)\r\n    pyarrow.parquet.write_table(a, \"test.parquet\")\r\n    subprocess.run(\r\n        \"\"\"cat test.parquet | clickhouse local \\\r\n            --input-format \"Parquet\" \\\r\n            --structure \"a String\" \\\r\n            --query \"select * from table\"\\\r\n        \"\"\",\r\n        shell=True,\r\n        check=True,\r\n    )\r\n\r\n```\r\ngives\r\n```\r\n00000\r\nCode: 50. DB::Exception: Unsupported Parquet type 'large_utf8' of an input column 'a'.: While executing ParquetBlockInputFormat: While executing File. (UNKNOWN_TYPE)\r\n```\r\n\r\n\r\n\n",
  "hints_text": "Actually, ARROW is using signed i32 or i64 to store the offsets (I don't know why it did not support u32 or u64).\r\nRefer: https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-layout\r\n\r\nBut your issue can be supported by adding match case in:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/7a8fe9a9ad169272a1b6f004ad47f1780769bbc1/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp#L344-L347\r\n\r\n\nI tried that, but then a dynamical_dispatch fail at runtime in another function. It seems that the arrow column is reinterpreted as a clickhouse column, but i'm out of my depth there !",
  "created_at": "2022-08-17T02:34:27Z"
}