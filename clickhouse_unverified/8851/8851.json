{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 8851,
  "instance_id": "ClickHouse__ClickHouse-8851",
  "issue_numbers": [
    "7612"
  ],
  "base_commit": "0734fd7f7d50c2a95ce4be46540b0c8a8c0ade04",
  "patch": "diff --git a/dbms/src/Storages/StorageS3.cpp b/dbms/src/Storages/StorageS3.cpp\nindex 0798820104b2..4cdea957b3df 100644\n--- a/dbms/src/Storages/StorageS3.cpp\n+++ b/dbms/src/Storages/StorageS3.cpp\n@@ -20,8 +20,15 @@\n #include <DataStreams/IBlockOutputStream.h>\n #include <DataStreams/IBlockInputStream.h>\n #include <DataStreams/AddingDefaultsBlockInputStream.h>\n+#include <DataStreams/narrowBlockInputStreams.h>\n+\n+#include <DataTypes/DataTypeString.h>\n \n #include <aws/s3/S3Client.h>\n+#include <aws/s3/model/ListObjectsRequest.h>\n+\n+#include <Common/parseGlobs.h>\n+#include <re2/re2.h>\n \n \n namespace DB\n@@ -29,6 +36,8 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n+    extern const int UNEXPECTED_EXPRESSION;\n+    extern const int S3_ERROR;\n }\n \n \n@@ -38,6 +47,8 @@ namespace\n     {\n     public:\n         StorageS3BlockInputStream(\n+            bool need_path,\n+            bool need_file,\n             const String & format,\n             const String & name_,\n             const Block & sample_block,\n@@ -48,6 +59,9 @@ namespace\n             const String & bucket,\n             const String & key)\n             : name(name_)\n+            , with_file_column(need_file)\n+            , with_path_column(need_path)\n+            , file_path(bucket + \"/\" + key)\n         {\n             read_buf = wrapReadBufferWithCompressionMethod(std::make_unique<ReadBufferFromS3>(client, bucket, key), compression_method);\n             reader = FormatFactory::instance().getInput(format, *read_buf, sample_block, context, max_block_size);\n@@ -60,12 +74,34 @@ namespace\n \n         Block readImpl() override\n         {\n-            return reader->read();\n+            auto res = reader->read();\n+            if (res)\n+            {\n+                if (with_path_column)\n+                    res.insert({DataTypeString().createColumnConst(res.rows(), file_path)->convertToFullColumnIfConst(), std::make_shared<DataTypeString>(),\n+                            \"_path\"});  /// construction with const is for probably generating less code\n+                if (with_file_column)\n+                {\n+                    size_t last_slash_pos = file_path.find_last_of('/');\n+                    res.insert({DataTypeString().createColumnConst(res.rows(), file_path.substr(\n+                            last_slash_pos + 1))->convertToFullColumnIfConst(), std::make_shared<DataTypeString>(),\n+                                \"_file\"});\n+                }\n+            }\n+            return res;\n         }\n \n         Block getHeader() const override\n         {\n-            return reader->getHeader();\n+            auto res = reader->getHeader();\n+            if (res)\n+            {\n+                if (with_path_column)\n+                    res.insert({DataTypeString().createColumn(), std::make_shared<DataTypeString>(), \"_path\"});\n+                if (with_file_column)\n+                    res.insert({DataTypeString().createColumn(), std::make_shared<DataTypeString>(), \"_file\"});\n+            }\n+            return res;\n         }\n \n         void readPrefixImpl() override\n@@ -82,6 +118,9 @@ namespace\n         String name;\n         std::unique_ptr<ReadBuffer> read_buf;\n         BlockInputStreamPtr reader;\n+        bool with_file_column = false;\n+        bool with_path_column = false;\n+        String file_path;\n     };\n \n     class StorageS3BlockOutputStream : public IBlockOutputStream\n@@ -144,7 +183,10 @@ StorageS3::StorageS3(\n     const ConstraintsDescription & constraints_,\n     Context & context_,\n     const String & compression_method_ = \"\")\n-    : IStorage(table_id_, columns_)\n+    : IStorage(table_id_, ColumnsDescription({\n+            {\"_path\", std::make_shared<DataTypeString>()},\n+            {\"_file\", std::make_shared<DataTypeString>()}\n+        }, true))\n     , uri(uri_)\n     , context_global(context_)\n     , format_name(format_name_)\n@@ -158,29 +200,103 @@ StorageS3::StorageS3(\n }\n \n \n+namespace\n+{\n+\n+/* \"Recursive\" directory listing with matched paths as a result.\n+ * Have the same method in StorageFile.\n+ */\n+Strings listFilesWithRegexpMatching(Aws::S3::S3Client & client, const S3::URI & globbed_uri)\n+{\n+    if (globbed_uri.bucket.find_first_of(\"*?{\") != globbed_uri.bucket.npos)\n+    {\n+        throw Exception(\"Expression can not have wildcards inside bucket name\", ErrorCodes::UNEXPECTED_EXPRESSION);\n+    }\n+\n+    const String key_prefix = globbed_uri.key.substr(0, globbed_uri.key.find_first_of(\"*?{\"));\n+    if (key_prefix.size() == globbed_uri.key.size())\n+    {\n+        return {globbed_uri.key};\n+    }\n+\n+    Aws::S3::Model::ListObjectsRequest request;\n+    request.SetBucket(globbed_uri.bucket);\n+    request.SetPrefix(key_prefix);\n+\n+    re2::RE2 matcher(makeRegexpPatternFromGlobs(globbed_uri.key));\n+    Strings result;\n+    Aws::S3::Model::ListObjectsOutcome outcome;\n+    int page = 0;\n+    do\n+    {\n+        ++page;\n+        outcome = client.ListObjects(request);\n+        if (!outcome.IsSuccess())\n+        {\n+            throw Exception(\"Could not list objects in bucket \" + quoteString(request.GetBucket())\n+                    + \" with prefix \" + quoteString(request.GetPrefix())\n+                    + \", page \" + std::to_string(page), ErrorCodes::S3_ERROR);\n+        }\n+\n+        for (const auto & row : outcome.GetResult().GetContents())\n+        {\n+            String key = row.GetKey();\n+            if (re2::RE2::FullMatch(key, matcher))\n+                result.emplace_back(std::move(key));\n+        }\n+\n+        request.SetMarker(outcome.GetResult().GetNextMarker());\n+    }\n+    while (outcome.GetResult().GetIsTruncated());\n+\n+    return result;\n+}\n+\n+}\n+\n+\n BlockInputStreams StorageS3::read(\n     const Names & column_names,\n     const SelectQueryInfo & /*query_info*/,\n     const Context & context,\n     QueryProcessingStage::Enum /*processed_stage*/,\n     size_t max_block_size,\n-    unsigned /*num_streams*/)\n+    unsigned num_streams)\n {\n-    BlockInputStreamPtr block_input = std::make_shared<StorageS3BlockInputStream>(\n-        format_name,\n-        getName(),\n-        getHeaderBlock(column_names),\n-        context,\n-        max_block_size,\n-        chooseCompressionMethod(uri.endpoint, compression_method),\n-        client,\n-        uri.bucket,\n-        uri.key);\n-\n-    auto column_defaults = getColumns().getDefaults();\n-    if (column_defaults.empty())\n-        return {block_input};\n-    return {std::make_shared<AddingDefaultsBlockInputStream>(block_input, column_defaults, context)};\n+    BlockInputStreams result;\n+    bool need_path_column = false;\n+    bool need_file_column = false;\n+    for (const auto & column : column_names)\n+    {\n+        if (column == \"_path\")\n+            need_path_column = true;\n+        if (column == \"_file\")\n+            need_file_column = true;\n+    }\n+\n+    for (const String & key : listFilesWithRegexpMatching(*client, uri))\n+    {\n+        BlockInputStreamPtr block_input = std::make_shared<StorageS3BlockInputStream>(\n+            need_path_column,\n+            need_file_column,\n+            format_name,\n+            getName(),\n+            getHeaderBlock(column_names),\n+            context,\n+            max_block_size,\n+            chooseCompressionMethod(uri.endpoint, compression_method),\n+            client,\n+            uri.bucket,\n+            key);\n+\n+        auto column_defaults = getColumns().getDefaults();\n+        if (column_defaults.empty())\n+            result.emplace_back(std::move(block_input));\n+        else\n+            result.emplace_back(std::make_shared<AddingDefaultsBlockInputStream>(block_input, column_defaults, context));\n+    }\n+\n+    return narrowBlockInputStreams(result, num_streams);\n }\n \n BlockOutputStreamPtr StorageS3::write(const ASTPtr & /*query*/, const Context & /*context*/)\n",
  "test_patch": "diff --git a/dbms/src/Common/tests/gtest_makeRegexpPatternFromGlobs.cpp b/dbms/src/Common/tests/gtest_makeRegexpPatternFromGlobs.cpp\nindex db695b965a14..42777d0bbbab 100644\n--- a/dbms/src/Common/tests/gtest_makeRegexpPatternFromGlobs.cpp\n+++ b/dbms/src/Common/tests/gtest_makeRegexpPatternFromGlobs.cpp\n@@ -8,6 +8,11 @@ using namespace DB;\n \n TEST(Common, makeRegexpPatternFromGlobs)\n {\n+    EXPECT_EQ(makeRegexpPatternFromGlobs(\"?\"), \"[^/]\");\n+    EXPECT_EQ(makeRegexpPatternFromGlobs(\"*\"), \"[^/]*\");\n+    EXPECT_EQ(makeRegexpPatternFromGlobs(\"/?\"), \"/[^/]\");\n+    EXPECT_EQ(makeRegexpPatternFromGlobs(\"/*\"), \"/[^/]*\");\n+    EXPECT_EQ(makeRegexpPatternFromGlobs(\"*_{{a,b,c,d}}/?.csv\"), \"[^/]*_\\\\{(a|b|c|d)\\\\}/[^/]\\\\.csv\");\n     EXPECT_EQ(makeRegexpPatternFromGlobs(\"f{01..09}\"), \"f(1|2|3|4|5|6|7|8|9)\");\n     EXPECT_EQ(makeRegexpPatternFromGlobs(\"f{01..9}\"), \"f(1|2|3|4|5|6|7|8|9)\");\n     EXPECT_EQ(makeRegexpPatternFromGlobs(\"f{0001..0000009}\"), \"f(1|2|3|4|5|6|7|8|9)\");\ndiff --git a/dbms/tests/integration/test_storage_s3/test.py b/dbms/tests/integration/test_storage_s3/test.py\nindex a651df9eb70d..b92fbba058cf 100644\n--- a/dbms/tests/integration/test_storage_s3/test.py\n+++ b/dbms/tests/integration/test_storage_s3/test.py\n@@ -1,5 +1,6 @@\n import json\n import logging\n+import random\n \n import pytest\n \n@@ -192,6 +193,27 @@ def test_put_get_with_redirect(cluster):\n     ]\n \n \n+def test_put_get_with_globs(cluster):\n+    # type: (ClickHouseCluster) -> None\n+\n+    bucket = cluster.minio_bucket\n+    instance = cluster.instances[\"dummy\"]  # type: ClickHouseInstance\n+    table_format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+    max_path = \"\"\n+    for i in range(10):\n+        for j in range(10):\n+            path = \"{}_{}/{}.csv\".format(i, random.choice(['a', 'b', 'c', 'd']), j)\n+            max_path = max(path, max_path)\n+            values = \"({},{},{})\".format(i, j, i+j)\n+            query = \"insert into table function s3('http://{}:{}/{}/{}', 'CSV', '{}') values {}\".format(\n+                cluster.minio_host, cluster.minio_port, bucket, path, table_format, values)\n+            run_query(instance, query)\n+\n+    query = \"select sum(column1), sum(column2), sum(column3), min(_file), max(_path) from s3('http://{}:{}/{}/*_{{a,b,c,d}}/%3f.csv', 'CSV', '{}')\".format(\n+        cluster.minio_redirect_host, cluster.minio_redirect_port, bucket, table_format)\n+    assert run_query(instance, query).splitlines() == [\"450\\t450\\t900\\t0.csv\\t{bucket}/{max_path}\".format(bucket=bucket, max_path=max_path)]\n+\n+\n # Test multipart put.\n @pytest.mark.parametrize(\"maybe_auth,positive\", [\n     (\"\", True),\n",
  "problem_statement": "Import all csv files from a s3 directory\nMake sure to check documentation https://clickhouse.yandex/docs/en/ first. If the question is concise and probably has a short answer, asking it in Telegram chat https://telegram.me/clickhouse_en is probably the fastest way to find the answer. For more complicated questions, consider asking them on StackOverflow with \"clickhouse\" tag https://stackoverflow.com/questions/tagged/clickhouse \r\n\r\nIf you still prefer GitHub issues, remove all this text and ask your question here.\r\n\n",
  "hints_text": "Is there a out of the box solution for importing all the csv files in a s3 directory?\nThere is no out of the box solution right now (engine URL does not support auth, engine S3 is not released). \r\nYou should use local files. \nIs this will be supported in future?\r\nOn Nov 4, 2019, 8:59 AM -0500, Denis Zhuravlev <notifications@github.com>, wrote:\r\n> There is no out of the box solution right now (engine URL does not support auth, engine S3 is not released).\r\n> You should use local files.\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub, or unsubscribe.\r\n\nNot sure, probably it works in test releases\r\n\r\nlike this\r\n```\r\ninsert into mytable\r\nselect * from s3('http://s3auth:s3auth@host/bucket/test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')\r\n```\r\n\nIs S3 import/export function released ? \nYes.\n@alexey-milovidov thanks for the confirmation. Can you please point me towards documentation or an example of inserting all csv files from s3 directory.\r\n\nI forget that:\r\n- s3 table function is implemented and released;\r\n- support for wildcards in file, hdfs, url is implemented and released;\r\n- support for wildcards in s3 is *not implemented*.\nSorry for incomplete info.\r\n\r\nThis task is assigned to @stavrolia \n@alexey-milovidov Thanks you!\r\n\r\nIf you have an Working example of importing a file from S3, can you please give me. \r\n\r\nWhat version of clickhouse has this feature.\n> @alexey-milovidov Thanks you!\r\n> \r\n> If you have an Working example of importing a file from S3, can you please give me.\r\n> \r\n> What version of clickhouse has this feature.\r\n\r\nYou can import a file with `select * from s3('URL', 'format', 'columns')` - where `URL` means URL, `format` means format of your file and `columns` means the columns you need.\nThanks, @stavrolia !!\r\n\r\nSince I am doing insert into click house should I do  insert into tablename select * from s3('URL', 'format', 'columns')\r\n\r\nwhere should i give secretkey and accesskey?\n> Not sure, probably it works in test releases\r\n> \r\n> like this\r\n> \r\n> ```\r\n> insert into mytable\r\n> select * from s3('http://s3auth:s3auth@host/bucket/test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')\r\n> ```\r\n\r\nThat is not correct. One shall use this syntax:\r\n\r\n```\r\nselect * from s3('http://host/bucket/test.csv', 'access_key_id'. 'secret_access_key'. 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')\r\n```",
  "created_at": "2020-01-27T10:24:07Z"
}