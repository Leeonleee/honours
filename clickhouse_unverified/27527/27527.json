{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 27527,
  "instance_id": "ClickHouse__ClickHouse-27527",
  "issue_numbers": [
    "16609"
  ],
  "base_commit": "531079c452e638ef1475dcb9f1a0d69377008028",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 74600bd5095d..40e69ce11522 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -176,6 +176,7 @@ class IColumn;\n     M(LogQueriesType, log_queries_min_type, QueryLogElementType::QUERY_START, \"Minimal type in query_log to log, possible values (from low to high): QUERY_START, QUERY_FINISH, EXCEPTION_BEFORE_START, EXCEPTION_WHILE_PROCESSING.\", 0) \\\n     M(Milliseconds, log_queries_min_query_duration_ms, 0, \"Minimal time for the query to run, to get to the query_log/query_thread_log/query_views_log.\", 0) \\\n     M(UInt64, log_queries_cut_to_length, 100000, \"If query length is greater than specified threshold (in bytes), then cut query when writing to query log. Also limit length of printed query in ordinary text log.\", 0) \\\n+    M(Float, log_queries_probability, 1., \"Log queries with the specified probabality.\", 0) \\\n     \\\n     M(DistributedProductMode, distributed_product_mode, DistributedProductMode::DENY, \"How are distributed subqueries performed inside IN or JOIN sections?\", IMPORTANT) \\\n     \\\ndiff --git a/src/Interpreters/executeQuery.cpp b/src/Interpreters/executeQuery.cpp\nindex 23fb35deee32..6f8caf743229 100644\n--- a/src/Interpreters/executeQuery.cpp\n+++ b/src/Interpreters/executeQuery.cpp\n@@ -56,6 +56,8 @@\n #include <Processors/Formats/IOutputFormat.h>\n #include <Processors/Sources/SinkToOutputStream.h>\n \n+#include <random>\n+\n \n namespace ProfileEvents\n {\n@@ -463,6 +465,18 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n \n     setQuerySpecificSettings(ast, context);\n \n+    /// There is an option of probabilistic logging of queries.\n+    /// If it is used - do the random sampling and \"collapse\" the settings.\n+    /// It allows to consistently log queries with all the subqueries in distributed query processing\n+    /// (subqueries on remote nodes will receive these \"collapsed\" settings)\n+    if (!internal && settings.log_queries && settings.log_queries_probability < 1.0)\n+    {\n+        std::bernoulli_distribution should_write_log{settings.log_queries_probability};\n+\n+        context->setSetting(\"log_queries\", should_write_log(thread_local_rng));\n+        context->setSetting(\"log_queries_probability\", 1.0);\n+    }\n+\n     /// Copy query into string. It will be written to log and presented in processlist. If an INSERT query, string will not include data to insertion.\n     String query(begin, query_end);\n     BlockIO res;\n",
  "test_patch": "diff --git a/tests/integration/test_log_query_probability/__init__.py b/tests/integration/test_log_query_probability/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_log_query_probability/test.py b/tests/integration/test_log_query_probability/test.py\nnew file mode 100644\nindex 000000000000..d1e19974e758\n--- /dev/null\n+++ b/tests/integration/test_log_query_probability/test.py\n@@ -0,0 +1,46 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__, name=\"log_quries_probability\")\n+node1 = cluster.add_instance('node1', with_zookeeper=False)\n+node2 = cluster.add_instance('node2', with_zookeeper=False)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_log_quries_probability_one(start_cluster):\n+    for i in range(100):\n+        node1.query(\"SELECT 12345\", settings={\"log_queries_probability\":0.5})\n+\n+    node1.query(\"SYSTEM FLUSH LOGS\")\n+\n+    assert node1.query(\"SELECT count() < (2 * 100) FROM system.query_log WHERE query LIKE '%12345%' AND query NOT LIKE '%system.query_log%'\") == \"1\\n\"\n+    assert node1.query(\"SELECT count() > 0         FROM system.query_log WHERE query LIKE '%12345%' AND query NOT LIKE '%system.query_log%'\") == \"1\\n\"\n+    assert node1.query(\"SELECT count() % 2         FROM system.query_log WHERE query LIKE '%12345%' AND query NOT LIKE '%system.query_log%'\") == \"0\\n\"\n+\n+    node1.query(\"TRUNCATE TABLE system.query_log\")\n+\n+\n+def test_log_quries_probability_two(start_cluster):\n+    for i in range(100):\n+        node1.query(\"SELECT 12345 FROM remote('node2', system, one)\", settings={\"log_queries_probability\":0.5})\n+\n+        node1.query(\"SYSTEM FLUSH LOGS\")\n+        node2.query(\"SYSTEM FLUSH LOGS\")\n+\n+        ans1 = node1.query(\"SELECT count() FROM system.query_log WHERE query LIKE '%12345%' AND query NOT LIKE '%system.query_log%'\")\n+        ans2 = node2.query(\"SELECT count() FROM system.query_log WHERE query LIKE '%12345%' AND query NOT LIKE '%system.query_log%'\")\n+\n+        assert ans1 == ans2\n+\n+        node1.query(\"TRUNCATE TABLE system.query_log\")\n+        node2.query(\"TRUNCATE TABLE system.query_log\")\n",
  "problem_statement": "Support logging a random sample of queries to query_log\nWith a large volume of queries per second logging all queries gets expensive, it would be nice to be able to log only a configurable fraction of the queries, selected at random.\n",
  "hints_text": "I am going to resolve this issue.\nThis task is available.",
  "created_at": "2021-08-10T13:46:42Z"
}