{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 11639,
  "instance_id": "ClickHouse__ClickHouse-11639",
  "issue_numbers": [
    "10367"
  ],
  "base_commit": "ceb8775bc1a60b35c0e1b47fd412afd06c32fe57",
  "patch": "diff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp\nindex 8393ea851123..a0eb7a5fb481 100644\n--- a/src/Common/ProfileEvents.cpp\n+++ b/src/Common/ProfileEvents.cpp\n@@ -203,6 +203,11 @@\n     \\\n     M(CannotWriteToWriteBufferDiscard, \"Number of stack traces dropped by query profiler or signal handler because pipe is full or cannot write to pipe.\") \\\n     M(QueryProfilerSignalOverruns, \"Number of times we drop processing of a signal due to overrun plus the number of signals that OS has not delivered due to overrun.\") \\\n+    \\\n+    M(CreatedLogEntryForMerge, \"Successfully created log entry to merge parts in ReplicatedMergeTree.\") \\\n+    M(NotCreatedLogEntryForMerge, \"Log entry to merge parts in ReplicatedMergeTree is not created due to concurrent log update by another replica.\") \\\n+    M(CreatedLogEntryForMutation, \"Successfully created log entry to mutate parts in ReplicatedMergeTree.\") \\\n+    M(NotCreatedLogEntryForMutation, \"Log entry to mutate parts in ReplicatedMergeTree is not created due to concurrent log update by another replica.\") \\\n \n namespace ProfileEvents\n {\ndiff --git a/src/Interpreters/ClientInfo.h b/src/Interpreters/ClientInfo.h\nindex 7a4df63c17a6..704fba3b3efa 100644\n--- a/src/Interpreters/ClientInfo.h\n+++ b/src/Interpreters/ClientInfo.h\n@@ -47,7 +47,8 @@ class ClientInfo\n     String current_user;\n     String current_query_id;\n     Poco::Net::SocketAddress current_address;\n-    /// Use current user and password when sending query to replica leader\n+\n+    /// This field is only used in foreign \"Arcadia\" build.\n     String current_password;\n \n     /// When query_kind == INITIAL_QUERY, these values are equal to current.\ndiff --git a/src/Interpreters/Context.cpp b/src/Interpreters/Context.cpp\nindex cb780443e03a..02060534aefb 100644\n--- a/src/Interpreters/Context.cpp\n+++ b/src/Interpreters/Context.cpp\n@@ -660,9 +660,13 @@ void Context::setUser(const String & name, const String & password, const Poco::\n     auto lock = getLock();\n \n     client_info.current_user = name;\n-    client_info.current_password = password;\n     client_info.current_address = address;\n \n+#if defined(ARCADIA_BUILD)\n+    /// This is harmful field that is used only in foreign \"Arcadia\" build.\n+    client_info.current_password = password;\n+#endif\n+\n     auto new_user_id = getAccessControlManager().find<User>(name);\n     std::shared_ptr<const ContextAccess> new_access;\n     if (new_user_id)\ndiff --git a/src/Interpreters/SystemLog.cpp b/src/Interpreters/SystemLog.cpp\nindex d79edde70526..b432cd8803be 100644\n--- a/src/Interpreters/SystemLog.cpp\n+++ b/src/Interpreters/SystemLog.cpp\n@@ -95,7 +95,6 @@ SystemLogs::SystemLogs(Context & global_context, const Poco::Util::AbstractConfi\n     if (asynchronous_metric_log)\n         logs.emplace_back(asynchronous_metric_log.get());\n \n-\n     try\n     {\n         for (auto & log : logs)\ndiff --git a/src/Common/ZooKeeper/LeaderElection.h b/src/Storages/MergeTree/LeaderElection.h\nsimilarity index 65%\nrename from src/Common/ZooKeeper/LeaderElection.h\nrename to src/Storages/MergeTree/LeaderElection.h\nindex f8a4d56dc76a..4d3a533d1391 100644\n--- a/src/Common/ZooKeeper/LeaderElection.h\n+++ b/src/Storages/MergeTree/LeaderElection.h\n@@ -1,11 +1,11 @@\n #pragma once\n \n-#include \"ZooKeeper.h\"\n-#include \"KeeperException.h\"\n #include <functional>\n #include <memory>\n #include <common/logger_useful.h>\n #include <Common/CurrentMetrics.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n+#include <Common/ZooKeeper/KeeperException.h>\n #include <Core/BackgroundSchedulePool.h>\n \n \n@@ -23,7 +23,18 @@ namespace CurrentMetrics\n namespace zkutil\n {\n \n-/** Implements leader election algorithm described here: http://zookeeper.apache.org/doc/r3.4.5/recipes.html#sc_leaderElection\n+/** Initially was used to implement leader election algorithm described here:\n+  * http://zookeeper.apache.org/doc/r3.4.5/recipes.html#sc_leaderElection\n+  *\n+  * But then we decided to get rid of leader election, so every replica can become leader.\n+  * For now, every replica can become leader if there is no leader among replicas with old version.\n+  *\n+  * It's tempting to remove this class at all, but we have to maintain it,\n+  *  to maintain compatibility when replicas with different versions work on the same cluster\n+  *  (this is allowed for short time period during cluster update).\n+  *\n+  * Replicas with new versions creates ephemeral sequential nodes with values like \"replica_name (multiple leaders Ok)\".\n+  * If the first node belongs to a replica with new version, then all replicas with new versions become leaders.\n   */\n class LeaderElection\n {\n@@ -42,7 +53,7 @@ class LeaderElection\n         ZooKeeper & zookeeper_,\n         LeadershipHandler handler_,\n         const std::string & identifier_)\n-        : pool(pool_), path(path_), zookeeper(zookeeper_), handler(handler_), identifier(identifier_)\n+        : pool(pool_), path(path_), zookeeper(zookeeper_), handler(handler_), identifier(identifier_ + suffix)\n         , log_name(\"LeaderElection (\" + path + \")\")\n         , log(&Poco::Logger::get(log_name))\n     {\n@@ -65,6 +76,7 @@ class LeaderElection\n     }\n \n private:\n+    static inline constexpr auto suffix = \" (multiple leaders Ok)\";\n     DB::BackgroundSchedulePool & pool;\n     DB::BackgroundSchedulePool::TaskHolder task;\n     std::string path;\n@@ -106,18 +118,27 @@ class LeaderElection\n         {\n             Strings children = zookeeper.getChildren(path);\n             std::sort(children.begin(), children.end());\n-            auto it = std::lower_bound(children.begin(), children.end(), node_name);\n-            if (it == children.end() || *it != node_name)\n+\n+            auto my_node_it = std::lower_bound(children.begin(), children.end(), node_name);\n+            if (my_node_it == children.end() || *my_node_it != node_name)\n                 throw Poco::Exception(\"Assertion failed in LeaderElection\");\n \n-            if (it == children.begin())\n+            String value = zookeeper.get(path + \"/\" + children.front());\n+\n+#if !defined(ARCADIA_BUILD) /// C++20; Replicated tables are unused in Arcadia.\n+            if (value.ends_with(suffix))\n             {\n                 ProfileEvents::increment(ProfileEvents::LeaderElectionAcquiredLeadership);\n                 handler();\n                 return;\n             }\n+#endif\n+            if (my_node_it == children.begin())\n+                throw Poco::Exception(\"Assertion failed in LeaderElection\");\n \n-            if (!zookeeper.existsWatch(path + \"/\" + *(it - 1), nullptr, task->getWatchCallback()))\n+            /// Watch for the node in front of us.\n+            --my_node_it;\n+            if (!zookeeper.existsWatch(path + \"/\" + *my_node_it, nullptr, task->getWatchCallback()))\n                 task->schedule();\n \n             success = true;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex d861173d8a04..f867a39581fb 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -298,7 +298,7 @@ bool MergeTreeDataMergerMutator::selectPartsToMerge(\n     if (parts_to_merge.empty())\n     {\n         if (out_disable_reason)\n-            *out_disable_reason = \"There are no need to merge parts according to merge selector algorithm\";\n+            *out_disable_reason = \"There is no need to merge parts according to merge selector algorithm\";\n         return false;\n     }\n \ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\nindex 1bc132eaba46..0870c0fdf724 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\n@@ -4,6 +4,7 @@\n #include <Interpreters/Context.h>\n \n #include <random>\n+#include <pcg_random.hpp>\n #include <unordered_set>\n \n \n@@ -85,8 +86,14 @@ void ReplicatedMergeTreeCleanupThread::clearOldLogs()\n \n     int children_count = stat.numChildren;\n \n-    /// We will wait for 1.1 times more records to accumulate than necessary.\n-    if (static_cast<double>(children_count) < storage_settings->min_replicated_logs_to_keep * 1.1)\n+    /// We will wait for 1.05 to 1.15 times more records to accumulate than necessary.\n+    /// Randomization is needed to spread the time when multiple replicas come here.\n+    /// Numbers are arbitrary.\n+    std::uniform_real_distribution<double> distr(1.05, 1.15);\n+    double ratio = distr(rng);\n+    size_t min_replicated_logs_to_keep = storage_settings->min_replicated_logs_to_keep * ratio;\n+\n+    if (static_cast<double>(children_count) < min_replicated_logs_to_keep)\n         return;\n \n     Strings replicas = zookeeper->getChildren(storage.zookeeper_path + \"/replicas\", &stat);\n@@ -214,10 +221,15 @@ void ReplicatedMergeTreeCleanupThread::clearOldLogs()\n     if (entries.empty())\n         return;\n \n-    markLostReplicas(host_versions_lost_replicas, log_pointers_candidate_lost_replicas, replicas.size() - num_replicas_were_marked_is_lost, zookeeper);\n+    markLostReplicas(\n+        host_versions_lost_replicas,\n+        log_pointers_candidate_lost_replicas,\n+        replicas.size() - num_replicas_were_marked_is_lost,\n+        zookeeper);\n \n     Coordination::Requests ops;\n-    for (size_t i = 0; i < entries.size(); ++i)\n+    size_t i = 0;\n+    for (; i < entries.size(); ++i)\n     {\n         ops.emplace_back(zkutil::makeRemoveRequest(storage.zookeeper_path + \"/log/\" + entries[i], -1));\n \n@@ -229,12 +241,25 @@ void ReplicatedMergeTreeCleanupThread::clearOldLogs()\n \n             /// Simultaneously with clearing the log, we check to see if replica was added since we received replicas list.\n             ops.emplace_back(zkutil::makeCheckRequest(storage.zookeeper_path + \"/replicas\", stat.version));\n-            zookeeper->multi(ops);\n+\n+            try\n+            {\n+                zookeeper->multi(ops);\n+            }\n+            catch (const zkutil::KeeperMultiException & e)\n+            {\n+                /// Another replica already deleted the same node concurrently.\n+                if (e.code == Coordination::Error::ZNONODE)\n+                    break;\n+\n+                throw;\n+            }\n             ops.clear();\n         }\n     }\n \n-    LOG_DEBUG(log, \"Removed {} old log entries: {} - {}\", entries.size(), entries.front(), entries.back());\n+    if (i != 0)\n+        LOG_DEBUG(log, \"Removed {} old log entries: {} - {}\", i, entries[0], entries[i - 1]);\n }\n \n \n@@ -250,8 +275,10 @@ void ReplicatedMergeTreeCleanupThread::markLostReplicas(const std::unordered_map\n         String replica = pair.first;\n         Coordination::Requests ops;\n         /// If host changed version we can not mark replicas, because replica started to be active.\n-        ops.emplace_back(zkutil::makeCheckRequest(storage.zookeeper_path + \"/replicas/\" + replica + \"/host\", host_versions_lost_replicas.at(replica)));\n-        ops.emplace_back(zkutil::makeSetRequest(storage.zookeeper_path + \"/replicas/\" + replica + \"/is_lost\", \"1\", -1));\n+        ops.emplace_back(zkutil::makeCheckRequest(\n+            storage.zookeeper_path + \"/replicas/\" + replica + \"/host\", host_versions_lost_replicas.at(replica)));\n+        ops.emplace_back(zkutil::makeSetRequest(\n+            storage.zookeeper_path + \"/replicas/\" + replica + \"/is_lost\", \"1\", -1));\n         candidate_lost_replicas.push_back(replica);\n         requests.push_back(ops);\n     }\n@@ -299,14 +326,17 @@ void ReplicatedMergeTreeCleanupThread::clearOldBlocks()\n \n     /// Use ZooKeeper's first node (last according to time) timestamp as \"current\" time.\n     Int64 current_time = timed_blocks.front().ctime;\n-    Int64 time_threshold = std::max(static_cast<Int64>(0), current_time - static_cast<Int64>(1000 * storage_settings->replicated_deduplication_window_seconds));\n+    Int64 time_threshold = std::max(\n+        static_cast<Int64>(0),\n+        current_time - static_cast<Int64>(1000 * storage_settings->replicated_deduplication_window_seconds));\n \n     /// Virtual node, all nodes that are \"greater\" than this one will be deleted\n     NodeWithStat block_threshold{{}, time_threshold};\n \n     size_t current_deduplication_window = std::min<size_t>(timed_blocks.size(), storage_settings->replicated_deduplication_window);\n     auto first_outdated_block_fixed_threshold = timed_blocks.begin() + current_deduplication_window;\n-    auto first_outdated_block_time_threshold = std::upper_bound(timed_blocks.begin(), timed_blocks.end(), block_threshold, NodeWithStat::greaterByTime);\n+    auto first_outdated_block_time_threshold = std::upper_bound(\n+        timed_blocks.begin(), timed_blocks.end(), block_threshold, NodeWithStat::greaterByTime);\n     auto first_outdated_block = std::min(first_outdated_block_fixed_threshold, first_outdated_block_time_threshold);\n \n     zkutil::AsyncResponses<Coordination::RemoveResponse> try_remove_futures;\n@@ -326,13 +356,16 @@ void ReplicatedMergeTreeCleanupThread::clearOldBlocks()\n             zookeeper->removeRecursive(path);\n             cached_block_stats.erase(first_outdated_block->node);\n         }\n-        else if (rc != Coordination::Error::ZOK)\n-            LOG_WARNING(log, \"Error while deleting ZooKeeper path `{}`: {}, ignoring.\", path, Coordination::errorMessage(rc));\n-        else\n+        else if (rc == Coordination::Error::ZOK || rc == Coordination::Error::ZNONODE)\n         {\n+            /// No node is Ok. Another replica is removing nodes concurrently.\n             /// Successfully removed blocks have to be removed from cache\n             cached_block_stats.erase(first_outdated_block->node);\n         }\n+        else\n+        {\n+            LOG_WARNING(log, \"Error while deleting ZooKeeper path `{}`: {}, ignoring.\", path, Coordination::errorMessage(rc));\n+        }\n         first_outdated_block++;\n     }\n \n@@ -453,8 +486,20 @@ void ReplicatedMergeTreeCleanupThread::clearOldMutations()\n         {\n             /// Simultaneously with clearing the log, we check to see if replica was added since we received replicas list.\n             ops.emplace_back(zkutil::makeCheckRequest(storage.zookeeper_path + \"/replicas\", replicas_stat.version));\n-            zookeeper->multi(ops);\n-            LOG_DEBUG(log, \"Removed {} old mutation entries: {} - {}\", (i + 1 - batch_start_i), entries[batch_start_i], entries[i]);\n+            try\n+            {\n+                zookeeper->multi(ops);\n+            }\n+            catch (const zkutil::KeeperMultiException & e)\n+            {\n+                /// Another replica already deleted the same node concurrently.\n+                if (e.code == Coordination::Error::ZNONODE)\n+                    break;\n+\n+                throw;\n+            }\n+            LOG_DEBUG(log, \"Removed {} old mutation entries: {} - {}\",\n+                i + 1 - batch_start_i, entries[batch_start_i], entries[i]);\n             batch_start_i = i + 1;\n             ops.clear();\n         }\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\nindex a787f99d9078..f4191482d642 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\n@@ -4,6 +4,7 @@\n #include <Common/ZooKeeper/Types.h>\n #include <Common/ZooKeeper/ZooKeeper.h>\n #include <common/logger_useful.h>\n+#include <Common/randomSeed.h>\n #include <Core/BackgroundSchedulePool.h>\n #include <thread>\n \n@@ -36,7 +37,7 @@ class ReplicatedMergeTreeCleanupThread\n     String log_name;\n     Poco::Logger * log;\n     BackgroundSchedulePool::TaskHolder task;\n-    pcg64 rng;\n+    pcg64 rng{randomSeed()};\n \n     void run();\n     void iterate();\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 5931bca17ead..ac762b3e05f3 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -71,6 +71,10 @@ namespace ProfileEvents\n     extern const Event ReplicatedPartFetches;\n     extern const Event DataAfterMergeDiffersFromReplica;\n     extern const Event DataAfterMutationDiffersFromReplica;\n+    extern const Event CreatedLogEntryForMerge;\n+    extern const Event NotCreatedLogEntryForMerge;\n+    extern const Event CreatedLogEntryForMutation;\n+    extern const Event NotCreatedLogEntryForMutation;\n }\n \n namespace CurrentMetrics\n@@ -2585,10 +2589,12 @@ StorageReplicatedMergeTree::CreateMergeEntryResult StorageReplicatedMergeTree::c\n         String path_created = dynamic_cast<const Coordination::CreateResponse &>(*responses.front()).path_created;\n         entry.znode_name = path_created.substr(path_created.find_last_of('/') + 1);\n \n+        ProfileEvents::increment(ProfileEvents::CreatedLogEntryForMerge);\n         LOG_TRACE(log, \"Created log entry {} for merge {}\", path_created, merged_name);\n     }\n     else if (code == Coordination::Error::ZBADVERSION)\n     {\n+        ProfileEvents::increment(ProfileEvents::NotCreatedLogEntryForMerge);\n         LOG_TRACE(log, \"Log entry is not created for merge {} because log was updated\", merged_name);\n         return CreateMergeEntryResult::LogUpdated;\n     }\n@@ -2649,12 +2655,14 @@ StorageReplicatedMergeTree::CreateMergeEntryResult StorageReplicatedMergeTree::c\n \n     if (code == Coordination::Error::ZBADVERSION)\n     {\n+        ProfileEvents::increment(ProfileEvents::NotCreatedLogEntryForMutation);\n         LOG_TRACE(log, \"Log entry is not created for mutation {} because log was updated\", new_part_name);\n         return CreateMergeEntryResult::LogUpdated;\n     }\n \n     zkutil::KeeperMultiException::check(code, ops, responses);\n \n+    ProfileEvents::increment(ProfileEvents::CreatedLogEntryForMutation);\n     LOG_TRACE(log, \"Created log entry for mutation {}\", new_part_name);\n     return CreateMergeEntryResult::Ok;\n }\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 52ce1aada08f..b2bd546b4788 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -19,12 +19,12 @@\n #include <Storages/MergeTree/BackgroundProcessingPool.h>\n #include <Storages/MergeTree/DataPartsExchange.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeAddress.h>\n+#include <Storages/MergeTree/LeaderElection.h>\n #include <DataTypes/DataTypesNumber.h>\n #include <Interpreters/Cluster.h>\n #include <Interpreters/PartLog.h>\n #include <Common/randomSeed.h>\n #include <Common/ZooKeeper/ZooKeeper.h>\n-#include <Common/ZooKeeper/LeaderElection.h>\n #include <Core/BackgroundSchedulePool.h>\n #include <Processors/Pipe.h>\n \n@@ -222,6 +222,7 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n     zkutil::EphemeralNodeHolderPtr replica_is_active_node;\n \n     /** Is this replica \"leading\". The leader replica selects the parts to merge.\n+      * It can be false only when old ClickHouse versions are working on the same cluster, because now we allow multiple leaders.\n       */\n     std::atomic<bool> is_leader {false};\n     zkutil::LeaderElectionPtr leader_election;\n@@ -497,6 +498,7 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n     bool waitForReplicaToProcessLogEntry(const String & replica_name, const ReplicatedMergeTreeLogEntryData & entry, bool wait_for_non_active = true);\n \n     /// Choose leader replica, send requst to it and wait.\n+    /// Only makes sense when old ClickHouse versions are working on the same cluster, because now we allow multiple leaders.\n     void sendRequestToLeaderReplica(const ASTPtr & query, const Context & query_context);\n \n     /// Throw an exception if the table is readonly.\n",
  "test_patch": "diff --git a/tests/integration/test_quorum_inserts/test.py b/tests/integration/test_quorum_inserts/test.py\nindex 279018426920..f490c13ca278 100644\n--- a/tests/integration/test_quorum_inserts/test.py\n+++ b/tests/integration/test_quorum_inserts/test.py\n@@ -256,7 +256,7 @@ def test_insert_quorum_with_ttl(started_cluster):\n                    \"(a Int8, d Date) \" \\\n                    \"Engine = ReplicatedMergeTree('/clickhouse/tables/{table}', '{replica}') \" \\\n                    \"PARTITION BY d ORDER BY a \" \\\n-                   \"TTL d + INTERVAL 5 second \" \\\n+                   \"TTL d + INTERVAL 5 second DELETE WHERE toYear(d) = 2011 \" \\\n                    \"SETTINGS merge_with_ttl_timeout=2 \"\n \n     print(\"Create Replicated table with two replicas\")\n@@ -284,11 +284,14 @@ def test_insert_quorum_with_ttl(started_cluster):\n     zero.query(\"INSERT INTO test_insert_quorum_with_ttl(a,d) VALUES(1, '2011-01-01')\",\n                                               settings={'insert_quorum_timeout' : 5000})\n \n-\n-    assert TSV(\"1\\t2011-01-01\\n\") == TSV(first.query(\"SELECT * FROM test_insert_quorum_with_ttl\", settings={'select_sequential_consistency' : 0}))\n-    assert TSV(\"1\\t2011-01-01\\n\") == TSV(first.query(\"SELECT * FROM test_insert_quorum_with_ttl\", settings={'select_sequential_consistency' : 1}))\n-\n     print(\"Inserts should resume.\")\n     zero.query(\"INSERT INTO test_insert_quorum_with_ttl(a, d) VALUES(2, '2012-02-02')\")\n \n+    first.query(\"OPTIMIZE TABLE test_insert_quorum_with_ttl\")\n+    first.query(\"SYSTEM SYNC REPLICA test_insert_quorum_with_ttl\")\n+    zero.query(\"SYSTEM SYNC REPLICA test_insert_quorum_with_ttl\")\n+\n+    assert TSV(\"2\\t2012-02-02\\n\") == TSV(first.query(\"SELECT * FROM test_insert_quorum_with_ttl\", settings={'select_sequential_consistency' : 0}))\n+    assert TSV(\"2\\t2012-02-02\\n\") == TSV(first.query(\"SELECT * FROM test_insert_quorum_with_ttl\", settings={'select_sequential_consistency' : 1}))\n+\n     execute_on_all_cluster(\"DROP TABLE IF EXISTS test_insert_quorum_with_ttl\")\ndiff --git a/tests/queries/0_stateless/00620_optimize_on_nonleader_replica_zookeeper.sql b/tests/queries/0_stateless/00620_optimize_on_nonleader_replica_zookeeper.sql\nindex 9622a5bd3c2a..f488502b13b0 100644\n--- a/tests/queries/0_stateless/00620_optimize_on_nonleader_replica_zookeeper.sql\n+++ b/tests/queries/0_stateless/00620_optimize_on_nonleader_replica_zookeeper.sql\n@@ -1,3 +1,5 @@\n+-- The test is mostly outdated as now every replica is leader and can do OPTIMIZE locally.\n+\n DROP TABLE IF EXISTS rename1;\n DROP TABLE IF EXISTS rename2;\n DROP TABLE IF EXISTS rename3;\n@@ -14,7 +16,9 @@ SELECT * FROM rename1;\n RENAME TABLE rename2 TO rename3;\n \n INSERT INTO rename1 VALUES (0, 1, 2);\n+SYSTEM SYNC REPLICA rename3; -- Make \"rename3\" to see all data parts.\n OPTIMIZE TABLE rename3;\n+SYSTEM SYNC REPLICA rename1; -- Make \"rename1\" to see and process all scheduled merges.\n SELECT * FROM rename1;\n \n DROP TABLE IF EXISTS rename1;\ndiff --git a/tests/queries/0_stateless/01307_multiple_leaders.sh b/tests/queries/0_stateless/01307_multiple_leaders.sh\nindex e19a10bcecb6..a43aa074c438 100755\n--- a/tests/queries/0_stateless/01307_multiple_leaders.sh\n+++ b/tests/queries/0_stateless/01307_multiple_leaders.sh\n@@ -5,35 +5,28 @@ CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n \n set -e\n \n-$CLICKHOUSE_CLIENT -n --query \"\n-DROP TABLE IF EXISTS r0;\n-DROP TABLE IF EXISTS r1;\n+NUM_REPLICAS=2\n+DATA_SIZE=200\n \n-CREATE TABLE r0 (x UInt64) ENGINE = ReplicatedMergeTree('/test/table', 'r0') ORDER BY x SETTINGS min_bytes_for_wide_part = '10M';\n-CREATE TABLE r1 (x UInt64) ENGINE = ReplicatedMergeTree('/test/table', 'r1') ORDER BY x SETTINGS min_bytes_for_wide_part = '10M';\n-\"\n+SEQ=$(seq 0 $(($NUM_REPLICAS - 1)))\n+\n+for REPLICA in $SEQ; do $CLICKHOUSE_CLIENT -n --query \"DROP TABLE IF EXISTS r$REPLICA\"; done\n+for REPLICA in $SEQ; do $CLICKHOUSE_CLIENT -n --query \"CREATE TABLE r$REPLICA (x UInt64) ENGINE = ReplicatedMergeTree('/test/table', 'r$REPLICA') ORDER BY x SETTINGS min_bytes_for_wide_part = '10M';\"; done\n \n function thread()\n {\n     REPLICA=$1\n     ITERATIONS=$2\n \n-    $CLICKHOUSE_CLIENT --max_block_size 1 --min_insert_block_size_rows 0 --min_insert_block_size_bytes 0 --query \"INSERT INTO r$REPLICA SELECT number * 2 + $REPLICA FROM numbers($ITERATIONS)\"\n+    $CLICKHOUSE_CLIENT --max_block_size 1 --min_insert_block_size_rows 0 --min_insert_block_size_bytes 0 --query \"INSERT INTO r$REPLICA SELECT number * $NUM_REPLICAS + $REPLICA FROM numbers($ITERATIONS)\"\n }\n \n-\n-thread 0 200 &\n-thread 1 200 &\n+for REPLICA in $SEQ; do\n+    thread $REPLICA $DATA_SIZE &\n+done\n \n wait\n \n-$CLICKHOUSE_CLIENT -n --query \"\n-SYSTEM SYNC REPLICA r0;\n-SYSTEM SYNC REPLICA r1;\n-\n-SELECT count(), sum(x) FROM r0;\n-SELECT count(), sum(x) FROM r1;\n-\n-DROP TABLE r0;\n-DROP TABLE r1;\n-\"\n+for REPLICA in $SEQ; do $CLICKHOUSE_CLIENT -n --query \"SYSTEM SYNC REPLICA r$REPLICA\"; done\n+for REPLICA in $SEQ; do $CLICKHOUSE_CLIENT -n --query \"SELECT count(), sum(x) FROM r$REPLICA\"; done\n+for REPLICA in $SEQ; do $CLICKHOUSE_CLIENT -n --query \"DROP TABLE r$REPLICA\"; done\n",
  "problem_statement": "Get rid of leader election in ReplicatedMergeTree tables.\nhttps://clickhouse.tech/docs/ru/whats-new/extended-roadmap/#odnovremennyi-vybor-kuskov-dlia-sliianiia-mnogimi-replikami-otkaz-ot-leader-election-v-zk\r\n\r\nThe way of implementation is discussed verbally.\n",
  "hints_text": "",
  "created_at": "2020-06-12T20:26:19Z"
}