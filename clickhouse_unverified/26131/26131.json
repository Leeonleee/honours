{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 26131,
  "instance_id": "ClickHouse__ClickHouse-26131",
  "issue_numbers": [
    "23941"
  ],
  "base_commit": "38dfe1fc0a5951e92eca2f877fa9459682c621c1",
  "patch": "diff --git a/src/Interpreters/InterpreterExplainQuery.cpp b/src/Interpreters/InterpreterExplainQuery.cpp\nindex b4a91170bc47..37650f5caa7e 100644\n--- a/src/Interpreters/InterpreterExplainQuery.cpp\n+++ b/src/Interpreters/InterpreterExplainQuery.cpp\n@@ -78,17 +78,35 @@ BlockIO InterpreterExplainQuery::execute()\n }\n \n \n-Block InterpreterExplainQuery::getSampleBlock()\n+Block InterpreterExplainQuery::getSampleBlock(const ASTExplainQuery::ExplainKind kind)\n {\n-    Block block;\n-\n-    ColumnWithTypeAndName col;\n-    col.name = \"explain\";\n-    col.type = std::make_shared<DataTypeString>();\n-    col.column = col.type->createColumn();\n-    block.insert(col);\n-\n-    return block;\n+    if (kind == ASTExplainQuery::ExplainKind::QueryEstimates)\n+    {\n+        auto cols = NamesAndTypes{\n+            {\"database\", std::make_shared<DataTypeString>()},\n+            {\"table\", std::make_shared<DataTypeString>()},\n+            {\"parts\", std::make_shared<DataTypeUInt64>()},\n+            {\"rows\", std::make_shared<DataTypeUInt64>()},\n+            {\"marks\", std::make_shared<DataTypeUInt64>()},\n+        };\n+        return Block({\n+            {cols[0].type->createColumn(), cols[0].type, cols[0].name},\n+            {cols[1].type->createColumn(), cols[1].type, cols[1].name},\n+            {cols[2].type->createColumn(), cols[2].type, cols[2].name},\n+            {cols[3].type->createColumn(), cols[3].type, cols[3].name},\n+            {cols[4].type->createColumn(), cols[4].type, cols[4].name},\n+        });\n+    }\n+    else\n+    {\n+        Block res;\n+        ColumnWithTypeAndName col;\n+        col.name = \"explain\";\n+        col.type = std::make_shared<DataTypeString>();\n+        col.column = col.type->createColumn();\n+        res.insert(col);\n+        return res;\n+    }\n }\n \n /// Split str by line feed and write as separate row to ColumnString.\n@@ -223,9 +241,9 @@ ExplainSettings<Settings> checkAndGetSettings(const ASTPtr & ast_settings)\n \n BlockInputStreamPtr InterpreterExplainQuery::executeImpl()\n {\n-    const auto & ast = query->as<ASTExplainQuery &>();\n+    const auto & ast = query->as<const ASTExplainQuery &>();\n \n-    Block sample_block = getSampleBlock();\n+    Block sample_block = getSampleBlock(ast.getKind());\n     MutableColumns res_columns = sample_block.cloneEmptyColumns();\n \n     WriteBufferFromOwnString buf;\n@@ -313,11 +331,32 @@ BlockInputStreamPtr InterpreterExplainQuery::executeImpl()\n             plan.explainPipeline(buf, settings.query_pipeline_options);\n         }\n     }\n+    else if (ast.getKind() == ASTExplainQuery::QueryEstimates)\n+    {\n+        if (!dynamic_cast<const ASTSelectWithUnionQuery *>(ast.getExplainedQuery().get()))\n+            throw Exception(\"Only SELECT is supported for EXPLAIN ESTIMATE query\", ErrorCodes::INCORRECT_QUERY);\n \n-    if (single_line)\n-        res_columns[0]->insertData(buf.str().data(), buf.str().size());\n-    else\n-        fillColumn(*res_columns[0], buf.str());\n+        auto settings = checkAndGetSettings<QueryPlanSettings>(ast.getSettings());\n+        QueryPlan plan;\n+\n+        InterpreterSelectWithUnionQuery interpreter(ast.getExplainedQuery(), getContext(), SelectQueryOptions());\n+        interpreter.buildQueryPlan(plan);\n+        // collect the selected marks, rows, parts during build query pipeline.\n+        plan.buildQueryPipeline(\n+            QueryPlanOptimizationSettings::fromContext(getContext()),\n+            BuildQueryPipelineSettings::fromContext(getContext()));\n+\n+        if (settings.optimize)\n+            plan.optimize(QueryPlanOptimizationSettings::fromContext(getContext()));\n+        plan.explainEstimate(res_columns);\n+    }\n+    if (ast.getKind() != ASTExplainQuery::QueryEstimates)\n+    {\n+        if (single_line)\n+            res_columns[0]->insertData(buf.str().data(), buf.str().size());\n+        else\n+            fillColumn(*res_columns[0], buf.str());\n+    }\n \n     return std::make_shared<OneBlockInputStream>(sample_block.cloneWithColumns(std::move(res_columns)));\n }\ndiff --git a/src/Interpreters/InterpreterExplainQuery.h b/src/Interpreters/InterpreterExplainQuery.h\nindex f16b1a8f69d2..a7f54a10e3e1 100644\n--- a/src/Interpreters/InterpreterExplainQuery.h\n+++ b/src/Interpreters/InterpreterExplainQuery.h\n@@ -2,7 +2,7 @@\n \n #include <Interpreters/IInterpreter.h>\n #include <Parsers/IAST_fwd.h>\n-\n+#include <Parsers/ASTExplainQuery.h>\n \n namespace DB\n {\n@@ -15,7 +15,7 @@ class InterpreterExplainQuery : public IInterpreter, WithContext\n \n     BlockIO execute() override;\n \n-    static Block getSampleBlock();\n+    static Block getSampleBlock(const ASTExplainQuery::ExplainKind kind);\n \n private:\n     ASTPtr query;\ndiff --git a/src/Parsers/ASTExplainQuery.h b/src/Parsers/ASTExplainQuery.h\nindex 95a3a362030b..5c50a8cd82e0 100644\n--- a/src/Parsers/ASTExplainQuery.h\n+++ b/src/Parsers/ASTExplainQuery.h\n@@ -17,6 +17,7 @@ class ASTExplainQuery : public ASTQueryWithOutput\n         AnalyzedSyntax, /// 'EXPLAIN SYNTAX SELECT ...'\n         QueryPlan, /// 'EXPLAIN SELECT ...'\n         QueryPipeline, /// 'EXPLAIN PIPELINE ...'\n+        QueryEstimates, /// 'EXPLAIN ESTIMATE ...'\n     };\n \n     explicit ASTExplainQuery(ExplainKind kind_) : kind(kind_) {}\n@@ -76,6 +77,7 @@ class ASTExplainQuery : public ASTQueryWithOutput\n             case AnalyzedSyntax: return \"EXPLAIN SYNTAX\";\n             case QueryPlan: return \"EXPLAIN\";\n             case QueryPipeline: return \"EXPLAIN PIPELINE\";\n+            case QueryEstimates: return \"EXPLAIN ESTIMATE\";\n         }\n \n         __builtin_unreachable();\ndiff --git a/src/Parsers/ParserExplainQuery.cpp b/src/Parsers/ParserExplainQuery.cpp\nindex dc5481641570..b4ba0523239c 100644\n--- a/src/Parsers/ParserExplainQuery.cpp\n+++ b/src/Parsers/ParserExplainQuery.cpp\n@@ -19,6 +19,7 @@ bool ParserExplainQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected\n     ParserKeyword s_syntax(\"SYNTAX\");\n     ParserKeyword s_pipeline(\"PIPELINE\");\n     ParserKeyword s_plan(\"PLAN\");\n+    ParserKeyword s_estimates(\"ESTIMATE\");\n \n     if (s_explain.ignore(pos, expected))\n     {\n@@ -32,6 +33,8 @@ bool ParserExplainQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected\n             kind = ASTExplainQuery::ExplainKind::QueryPipeline;\n         else if (s_plan.ignore(pos, expected))\n             kind = ASTExplainQuery::ExplainKind::QueryPlan; //-V1048\n+        else if (s_estimates.ignore(pos, expected))\n+            kind = ASTExplainQuery::ExplainKind::QueryEstimates; //-V1048\n     }\n     else\n         return false;\ndiff --git a/src/Processors/QueryPlan/QueryPlan.cpp b/src/Processors/QueryPlan/QueryPlan.cpp\nindex 44c5c48975c5..bc3b8458531e 100644\n--- a/src/Processors/QueryPlan/QueryPlan.cpp\n+++ b/src/Processors/QueryPlan/QueryPlan.cpp\n@@ -9,6 +9,7 @@\n #include <Processors/QueryPlan/Optimizations/Optimizations.h>\n #include <Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h>\n #include <Processors/QueryPlan/BuildQueryPipelineSettings.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n #include <Common/JSONBuilder.h>\n \n namespace DB\n@@ -434,4 +435,59 @@ void QueryPlan::optimize(const QueryPlanOptimizationSettings & optimization_sett\n     QueryPlanOptimizations::optimizeTree(optimization_settings, *root, nodes);\n }\n \n+void QueryPlan::explainEstimate(MutableColumns & columns)\n+{\n+    checkInitialized();\n+\n+    struct EstimateCounters\n+    {\n+        std::string database_name;\n+        std::string table_name;\n+        UInt64 parts = 0;\n+        UInt64 rows = 0;\n+        UInt64 marks = 0;\n+\n+        EstimateCounters(const std::string & database, const std::string & table) : database_name(database), table_name(table)\n+        {\n+        }\n+    };\n+\n+    using CountersPtr = std::shared_ptr<EstimateCounters>;\n+    std::unordered_map<std::string, CountersPtr> counters;\n+    using processNodeFuncType = std::function<void(const Node * node)>;\n+    processNodeFuncType process_node = [&counters, &process_node] (const Node * node)\n+    {\n+        if (!node)\n+            return;\n+        if (const auto * step = dynamic_cast<ReadFromMergeTree*>(node->step.get()))\n+        {\n+            const auto & id = step->getStorageID();\n+            auto key = id.database_name + \".\" + id.table_name;\n+            auto it = counters.find(key);\n+            if (it == counters.end())\n+            {\n+                it = counters.insert({key, std::make_shared<EstimateCounters>(id.database_name, id.table_name)}).first;\n+            }\n+            it->second->parts += step->getSelectedParts();\n+            it->second->rows += step->getSelectedRows();\n+            it->second->marks += step->getSelectedMarks();\n+        }\n+        for (const auto * child : node->children)\n+            process_node(child);\n+    };\n+    process_node(root);\n+\n+    for (const auto & counter : counters)\n+    {\n+        size_t index = 0;\n+        const auto & database_name = counter.second->database_name;\n+        const auto & table_name = counter.second->table_name;\n+        columns[index++]->insertData(database_name.c_str(), database_name.size());\n+        columns[index++]->insertData(table_name.c_str(), table_name.size());\n+        columns[index++]->insert(counter.second->parts);\n+        columns[index++]->insert(counter.second->rows);\n+        columns[index++]->insert(counter.second->marks);\n+    }\n+}\n+\n }\ndiff --git a/src/Processors/QueryPlan/QueryPlan.h b/src/Processors/QueryPlan/QueryPlan.h\nindex 4c75f00cf4d1..95034d34c9c1 100644\n--- a/src/Processors/QueryPlan/QueryPlan.h\n+++ b/src/Processors/QueryPlan/QueryPlan.h\n@@ -2,6 +2,7 @@\n \n #include <Core/Names.h>\n #include <Interpreters/Context_fwd.h>\n+#include <Columns/IColumn.h>\n \n #include <list>\n #include <memory>\n@@ -85,6 +86,7 @@ class QueryPlan\n     JSONBuilder::ItemPtr explainPlan(const ExplainPlanOptions & options);\n     void explainPlan(WriteBuffer & buffer, const ExplainPlanOptions & options);\n     void explainPipeline(WriteBuffer & buffer, const ExplainPipelineOptions & options);\n+    void explainEstimate(MutableColumns & columns);\n \n     /// Set upper limit for the recommend number of threads. Will be applied to the newly-created pipelines.\n     /// TODO: make it in a better way.\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex 3cf1bdcb4d86..2983663d0ce3 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -47,6 +47,9 @@ struct ReadFromMergeTree::AnalysisResult\n     IndexStats index_stats;\n     Names column_names_to_read;\n     ReadFromMergeTree::ReadType read_type = ReadFromMergeTree::ReadType::Default;\n+    UInt64 selected_rows = 0;\n+    UInt64 selected_marks = 0;\n+    UInt64 selected_parts = 0;\n };\n \n static MergeTreeReaderSettings getMergeTreeReaderSettings(const ContextPtr & context)\n@@ -839,13 +842,17 @@ ReadFromMergeTree::AnalysisResult ReadFromMergeTree::selectRangesToRead(MergeTre\n \n     size_t sum_marks = 0;\n     size_t sum_ranges = 0;\n+    size_t sum_rows = 0;\n \n     for (const auto & part : result.parts_with_ranges)\n     {\n         sum_ranges += part.ranges.size();\n         sum_marks += part.getMarksCount();\n+        sum_rows += part.getRowsCount();\n     }\n-\n+    result.selected_parts = result.parts_with_ranges.size();\n+    result.selected_marks = sum_marks;\n+    result.selected_rows  = sum_rows;\n     LOG_DEBUG(\n         log,\n         \"Selected {}/{} parts by partition key, {} parts by primary key, {}/{} marks by primary key, {} marks to read from {} ranges\",\n@@ -883,6 +890,9 @@ void ReadFromMergeTree::initializePipeline(QueryPipeline & pipeline, const Build\n         return;\n     }\n \n+    selected_marks = result.selected_marks;\n+    selected_rows = result.selected_rows;\n+    selected_parts = result.selected_parts;\n     /// Projection, that needed to drop columns, which have appeared by execution\n     /// of some extra expressions, and to allow execute the same expressions later.\n     /// NOTE: It may lead to double computation of expressions.\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.h b/src/Processors/QueryPlan/ReadFromMergeTree.h\nindex a5184d285934..b82e027420bb 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.h\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.h\n@@ -80,6 +80,10 @@ class ReadFromMergeTree final : public ISourceStep\n     void describeActions(JSONBuilder::JSONMap & map) const override;\n     void describeIndexes(JSONBuilder::JSONMap & map) const override;\n \n+    const StorageID getStorageID() const { return data.getStorageID(); }\n+    UInt64 getSelectedParts() const { return selected_parts; }\n+    UInt64 getSelectedRows() const { return selected_rows; }\n+    UInt64 getSelectedMarks() const { return selected_marks; }\n private:\n     const MergeTreeReaderSettings reader_settings;\n \n@@ -106,6 +110,9 @@ class ReadFromMergeTree final : public ISourceStep\n     std::shared_ptr<PartitionIdToMaxBlock> max_block_numbers_to_read;\n \n     Poco::Logger * log;\n+    UInt64 selected_parts = 0;\n+    UInt64 selected_rows = 0;\n+    UInt64 selected_marks = 0;\n \n     Pipe read(RangesInDataParts parts_with_range, Names required_columns, ReadType read_type, size_t max_streams, size_t min_marks_for_concurrent_read, bool use_uncompressed_cache);\n     Pipe readFromPool(RangesInDataParts parts_with_ranges, Names required_columns, size_t max_streams, size_t min_marks_for_concurrent_read, bool use_uncompressed_cache);\n",
  "test_patch": "diff --git a/tests/integration/test_explain_estimates/__init__.py b/tests/integration/test_explain_estimates/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_explain_estimates/test.py b/tests/integration/test_explain_estimates/test.py\nnew file mode 100644\nindex 000000000000..a2b65564dbcd\n--- /dev/null\n+++ b/tests/integration/test_explain_estimates/test.py\n@@ -0,0 +1,24 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('instance')\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_explain_estimates(start_cluster):\n+    node1.query(\"CREATE TABLE test (i Int64) ENGINE = MergeTree() ORDER BY i SETTINGS index_granularity = 16, write_final_mark = 0\")\n+    node1.query(\"INSERT INTO test SELECT number FROM numbers(128)\")\n+    node1.query(\"OPTIMIZE TABLE test\")\n+    system_parts_result = node1.query(\"SELECT any(database), any(table), count() as parts, sum(rows) as rows, sum(marks) as marks FROM system.parts WHERE database = 'default' AND table = 'test' and active = 1 GROUP BY (database, table)\")\n+    explain_estimates_result = node1.query(\"EXPLAIN ESTIMATE SELECT * FROM test\")\n+    assert(system_parts_result == explain_estimates_result)\ndiff --git a/tests/queries/1_stateful/00166_explain_estimate.reference b/tests/queries/1_stateful/00166_explain_estimate.reference\nnew file mode 100644\nindex 000000000000..71ddd6815812\n--- /dev/null\n+++ b/tests/queries/1_stateful/00166_explain_estimate.reference\n@@ -0,0 +1,5 @@\n+test\thits\t1\t57344\t7\n+test\thits\t1\t8839168\t1079\n+test\thits\t1\t835584\t102\n+test\thits\t1\t8003584\t977\n+test\thits\t2\t581632\t71\ndiff --git a/tests/queries/1_stateful/00166_explain_estimate.sql b/tests/queries/1_stateful/00166_explain_estimate.sql\nnew file mode 100644\nindex 000000000000..06725ff7f9f9\n--- /dev/null\n+++ b/tests/queries/1_stateful/00166_explain_estimate.sql\n@@ -0,0 +1,5 @@\n+EXPLAIN ESTIMATE SELECT count() FROM test.hits WHERE CounterID = 29103473;\n+EXPLAIN ESTIMATE SELECT count() FROM test.hits WHERE CounterID != 29103473;\n+EXPLAIN ESTIMATE SELECT count() FROM test.hits WHERE CounterID > 29103473;\n+EXPLAIN ESTIMATE SELECT count() FROM test.hits WHERE CounterID < 29103473;\n+EXPLAIN ESTIMATE SELECT count() FROM test.hits WHERE CounterID = 29103473 UNION ALL SELECT count() FROM test.hits WHERE CounterID = 1704509;\n",
  "problem_statement": "show estimates (estimated query complexity / parts / marks / rows) \nClickhouse knows before a query execution \r\n```\r\nSelected 4/4 parts by partition key, 4 parts by primary key, \r\n126/126 marks by primary key, 126 marks to read from 4 ranges\r\nReading approx. 1032192 rows with 7 streams\r\n```\r\n\r\nI need ability to get these numbers without query execution:\r\n\r\n```\r\nshow estimates for select count() from table;\r\n\r\n\u250c\u2500parts\u2500\u252c\u2500marks\u2500\u252c\u2500\u2500\u2500\u2500rows\u2500\u2510\r\n\u2502   343 \u2502  1126 \u2502 1032192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nalso it seems they will be helpful in system.processes & system.query_log\n",
  "hints_text": "Is it ok like following?\r\n```\r\nshow estimates  database_name.table_name;\r\n\r\n\u250c\u2500parts\u2500\u252c\u2500marks\u2500\u252c\u2500\u2500\u2500\u2500rows\u2500\u2510\r\n\u2502   343 \u2502  1126 \u2502 1032192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n>Is it ok like following?\r\n\r\nNo. For a query.\r\n\r\n```\r\ncreate table x (a Int64, b Int64, s String) Engine=MergeTree partition by a order by b;\r\ninsert into x select number%5, number%999, toString(number) from numbers(100000);\r\n\r\nshow estimates for select uniq(s), a from x where a=1 and b > 42 group by a;\r\n\u250c\u2500parts\u2500\u252c\u2500marks\u2500\u252c\u2500\u2500rows\u2500\u2510\r\n\u2502     1 \u2502     3 \u2502 24576 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\nIt can be a mode of EXPLAIN query.\nOk, I'll try it.\n> Ok, I'll try it.\r\n\r\nPlease also take distributed queries into account.\n> Please also take distributed queries into account.\r\n\r\nSure.\nWhat about queries affecting non-merge tree? several merge tree tables (i.e. joins) ? \n@filimonov \r\n\r\nTwo options exist:\r\n- sum the values across all tables;\r\n- display the values corresponding to every table in separate rows.\n> @filimonov\r\n> \r\n> Two options exist:\r\n> \r\n> * sum the values across all tables;\r\n> * display the values corresponding to every table in separate rows.\r\n\r\nI'm voting for the second one\nI also would like to see count of ranges. (Probably even marks_id?)\r\nAnd how much each index/partition by/skip index contributed to amount of read rows.\r\n\r\nAnd if you have some nested query like\r\n\r\n```\r\nSELECT * FROM table WHERE id IN (SELECT id FROM table_2 WHERE x = 1)\r\n```\r\n\r\nIn order to get estimates for each level of sub query, you actually need to execute at least some stages of this query.\r\nProbably you can have some setting like this:\r\n\r\n```\r\nEXPLAIN ESTIMATES analyze=1 SELECT ...\r\n```\r\n\r\n\r\nBTW, we already have quite a lot of explaining columns in query_log (column names / functions) and total amount of read rows.\r\nProbably we can also include rows per table in it.",
  "created_at": "2021-07-09T14:29:28Z"
}