You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Setup to implement "archive" merges that merges high level parts that are old
The current merge algorithm follows a tree approach. It works well, but this means that some data will not be merged for a very long period of time. 

To force merging old data, there is a setting, min_age_to_force_merge_seconds that can be applied per table. On paper this setting works perfectly, but it is also a timebomb. As i understand, force merges means "queue merge immediatly as soon as the condition fires". There is no way to limit concurrency of force merges, so any spike of ingest can trigger a bomb months later where suddently "archive" merges will starve normal merges, and there is no way to smooth that. 

Does this usecase make sense ? Or is it because mergeSelecting settings are not tuned well enough for the data ? I think something like max_number_of_merges_force_in_pool would solve this. 

Consider adding a separated task for historical merges
**Use case**

The merged algorithm in ClickHouse was designed to favour smaller and more recent parts. This is obvious because we need to minimise number of parts.

However, there's a case when merging old parts also important. For example, when we have a `ReplacingMergeTree` and the `SELECT ... FINAL` spans across several partitions. Thanks to `do_not_merge_across_partitions_select_final`, if a partition only has 1 part, we don't need to apply `FINAL` logic on that partition, which significantly improve the query performance. Therefore we want that at some point, the "old" partition (last week, last month, etc...) will only contain 1 part.

**What we have now**

I think the use case has been raised in #35836, and there's already a solution with `min_age_to_force_merge_seconds` in #42423. With `min_age_to_force_merge_seconds`, once the parts stay long enough,  they will be allowed to merged without respecting the criteria (total size, age...).

However, this setting can lead to merge starving for recent parts, because: historical merges are usually big (not to say super big), and number of concurrent merges are limited.

```
-- Future merge tasks, mixing between small (recent parts) merges and big (old parts) merges
@@@@@@@@@
@
@@@@@@@@@@@
@
@@
@@@@@@@@@@@@@@
@@
-- Current merge tasks queue - running in round robin manner for each @
@
@@
@@@@
@@@@@
@@@@@@
@@@@@@
@@@@@@@@@@

-- Current merge tasks at some points
@@@@@@@@
@@@@@@@@@
@@@@@@@@@@@
@@@@@@
@@@@@@
@@@@@@
@@@@@@@
``` 

If we change `background_merges_mutations_scheduling_policy` to `shortest_task_first`, then we face the same old issue again: small merges are alway preferred, while big merges can stuck in merge queue forever (ever if they're scheduled).

**Describe the solution you'd like**

Each table having a separated task to optimize old partitions. The task will get thread from common schedule pool, and it will scan from oldest partition -> most recent partition to find which partition it can merge to single part. Whether to active this task or not will be controlled by a table setting. Merging old partition will takes some times, but eventually we will reach a point where every old partitions only have 1 part.

I think this is the original idea implemented in #35836, but then changed to current solution.

I don't know if there's a better solution, appreciate any comments [THANKS]!
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
