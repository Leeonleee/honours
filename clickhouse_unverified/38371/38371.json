{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 38371,
  "instance_id": "ClickHouse__ClickHouse-38371",
  "issue_numbers": [
    "38282"
  ],
  "base_commit": "25100ad5e52ee7845ec54ddaed1185a9cf2d333d",
  "patch": "diff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex ec7c3878b064..454457c7b93b 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -1242,8 +1242,12 @@ void InterpreterSelectQuery::executeImpl(QueryPlan & query_plan, std::optional<P\n                 if (expressions.has_order_by)\n                     executeOrder(query_plan, input_order_info_for_order);\n \n-                if (expressions.has_order_by && query.limitLength())\n-                    executeDistinct(query_plan, false, expressions.selected_columns, true);\n+                /// pre_distinct = false, because if we have limit and distinct,\n+                /// we need to merge streams to one and calculate overall distinct.\n+                /// Otherwise we can take several equal values from different streams\n+                /// according to limit and skip some distinct values.\n+                if (query.limitLength())\n+                    executeDistinct(query_plan, false, expressions.selected_columns, false);\n \n                 if (expressions.hasLimitBy())\n                 {\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02344_distinct_limit_distiributed.reference b/tests/queries/0_stateless/02344_distinct_limit_distiributed.reference\nnew file mode 100644\nindex 000000000000..091c57076d27\n--- /dev/null\n+++ b/tests/queries/0_stateless/02344_distinct_limit_distiributed.reference\n@@ -0,0 +1,6 @@\n+-1\n+1\n+11\n+12\n+13\n+14\ndiff --git a/tests/queries/0_stateless/02344_distinct_limit_distiributed.sql b/tests/queries/0_stateless/02344_distinct_limit_distiributed.sql\nnew file mode 100644\nindex 000000000000..d0d9b130b7ed\n--- /dev/null\n+++ b/tests/queries/0_stateless/02344_distinct_limit_distiributed.sql\n@@ -0,0 +1,26 @@\n+drop table if exists t_distinct_limit;\n+\n+create table t_distinct_limit (d Date, id Int64)\n+engine = MergeTree partition by toYYYYMM(d) order by d;\n+\n+set max_threads = 10;\n+\n+insert into t_distinct_limit select '2021-12-15', -1 from numbers(1e6);\n+insert into t_distinct_limit select '2021-12-15', -1 from numbers(1e6);\n+insert into t_distinct_limit select '2021-12-15', -1 from numbers(1e6);\n+insert into t_distinct_limit select '2022-12-15', 1 from numbers(1e6);\n+insert into t_distinct_limit select '2022-12-15', 1 from numbers(1e6);\n+insert into t_distinct_limit select '2022-12-16', 11 from numbers(1);\n+insert into t_distinct_limit select '2023-12-16', 12 from numbers(1);\n+insert into t_distinct_limit select '2023-12-16', 13 from numbers(1);\n+insert into t_distinct_limit select '2023-12-16', 14 from numbers(1);\n+\n+set max_block_size = 1024;\n+\n+select id from\n+(\n+    select distinct id from remote('127.0.0.1,127.0.0.2', currentDatabase(),t_distinct_limit) limit 10\n+)\n+order by id;\n+\n+drop table if exists t_distinct_limit;\n",
  "problem_statement": "incorrect result: distinct + distributed + limit \n21.8.13.6, 22.3.7.28, 22.5.1.2079, 22.6.1.1985\r\n\r\ndistinct + limit over distributed tables stops to read rows from the table before reaching the limit.\r\n\r\n```sql\r\ncat xy.sql\r\ndrop table  if exists test;\r\ncreate table test  (d Date, id Int64 ) Engine = MergeTree partition by toYYYYMM(d) order by d;\r\n\r\ninsert into test select '2021-12-15', -1 from numbers(1e7);\r\ninsert into test select '2021-12-15', -1 from numbers(1e7);\r\ninsert into test select '2021-12-15', -1 from numbers(1e7);\r\ninsert into test select '2021-12-15', -1 from numbers(1e7);\r\ninsert into test select '2022-12-15', 1 from numbers(1e7);\r\ninsert into test select '2022-12-16', 11 from numbers(1);\r\ninsert into test select '2023-12-16', 12 from numbers(1);\r\ninsert into test select '2023-12-16', 13 from numbers(1);\r\ninsert into test select '2023-12-16', 14 from numbers(1);\r\n\r\nselect distinct id from remote('127.0.0.2,127.0.0.1', currentDatabase(),test) limit 10;\r\n\r\nselect '-----';\r\n\r\nselect distinct id from remote('127.0.0.2,127.0.0.1', currentDatabase(),test) ;\r\n```\r\n\r\n```\r\n$ cat xy.sql |clickhouse-client -mn\r\n1\r\n-1\r\n-----\r\n-1\r\n1\r\n11\r\n12\r\n13\r\n\r\n$ cat xy.sql |clickhouse-client -mn\r\n1\r\n-1\r\n11\r\n-----\r\n1\r\n-1\r\n11\r\n12\r\n13\r\n14\r\n\r\n$ cat xy.sql |clickhouse-client -mn\r\n-1\r\n1\r\n-----\r\n-1\r\n1\r\n11\r\n12\r\n13\r\n14\r\n\r\n$ cat xy.sql |clickhouse-client -mn\r\n1\r\n-1\r\n11\r\n-----\r\n-1\r\n1\r\n11\r\n12\r\n13\r\n14\r\n\r\n$ cat xy.sql |clickhouse-client -mn\r\n1\r\n-1\r\n-----\r\n-1\r\n1\r\n11\r\n12\r\n13\r\n14\r\n```\r\n\r\n------\r\n\r\nin real prod it looks like this\r\n\r\n```sql\r\n-- very fast, but incorrect\r\n\r\nselect distinct ext_io_id from csp_ad_fact_event where access_day > '2021-12-15' limit 10;\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502        -1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.062 sec. Processed 17.30 million rows, 172.98 MB (278.38 million rows/s., 2.78 GB/s.)\r\n\r\n\r\n-- without limit \r\n\r\nselect distinct ext_io_id from csp_ad_fact_event where access_day > '2021-12-15';\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502        -1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502     23120 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502   2704949 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 25.510 sec. Processed 376.27 billion rows, 3.76 TB (14.75 billion rows/s., 147.50 GB/s.)\r\n\r\n\r\n-- limit 100\r\n\r\nselect distinct ext_io_id from csp_ad_fact_event where access_day > '2021-12-15' limit 100;\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502        -1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502     23120 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502   2704949 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 26.331 sec. Processed 376.33 billion rows, 3.76 TB (14.29 billion rows/s., 142.92 GB/s.)\r\n\r\n\r\n-- with order by\r\n\r\nselect distinct ext_io_id from csp_ad_fact_event where access_day > '2021-12-15' order by  ext_io_id limit 10;\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502        -1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502     23120 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500ext_io_id\u2500\u2510\r\n\u2502   2704949 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 26.103 sec. Processed 376.36 billion rows, 3.76 TB (14.42 billion rows/s., 144.18 GB/s.)\r\n```\r\n\r\nProbably related to the optimization with small limits.\n",
  "hints_text": "",
  "created_at": "2022-06-24T02:31:24Z"
}