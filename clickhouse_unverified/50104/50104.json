{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 50104,
  "instance_id": "ClickHouse__ClickHouse-50104",
  "issue_numbers": [
    "43107",
    "47549"
  ],
  "base_commit": "4fd674a3b23f9bed6b3e1cf1ca84c704c003597e",
  "patch": "diff --git a/src/Interpreters/MutationsInterpreter.cpp b/src/Interpreters/MutationsInterpreter.cpp\nindex 713ebade1d54..25c52ad89257 100644\n--- a/src/Interpreters/MutationsInterpreter.cpp\n+++ b/src/Interpreters/MutationsInterpreter.cpp\n@@ -38,6 +38,7 @@\n #include <Analyzer/TableNode.h>\n #include <Interpreters/InterpreterSelectQueryAnalyzer.h>\n #include <Parsers/makeASTForLogicalFunction.h>\n+#include <Common/logger_useful.h>\n \n \n namespace DB\n@@ -109,13 +110,16 @@ QueryTreeNodePtr prepareQueryAffectedQueryTree(const std::vector<MutationCommand\n     return query_tree;\n }\n \n-ColumnDependencies getAllColumnDependencies(const StorageMetadataPtr & metadata_snapshot, const NameSet & updated_columns)\n+ColumnDependencies getAllColumnDependencies(\n+    const StorageMetadataPtr & metadata_snapshot,\n+    const NameSet & updated_columns,\n+    const std::function<bool(const String & file_name)> & has_index_or_projection)\n {\n     NameSet new_updated_columns = updated_columns;\n     ColumnDependencies dependencies;\n     while (!new_updated_columns.empty())\n     {\n-        auto new_dependencies = metadata_snapshot->getColumnDependencies(new_updated_columns, true);\n+        auto new_dependencies = metadata_snapshot->getColumnDependencies(new_updated_columns, true, has_index_or_projection);\n         new_updated_columns.clear();\n         for (const auto & dependency : new_dependencies)\n         {\n@@ -288,6 +292,11 @@ bool MutationsInterpreter::Source::materializeTTLRecalculateOnly() const\n     return data && data->getSettings()->materialize_ttl_recalculate_only;\n }\n \n+bool MutationsInterpreter::Source::hasIndexOrProjection(const String & file_name) const\n+{\n+    return part && part->checksums.has(file_name);\n+}\n+\n static Names getAvailableColumnsWithVirtuals(StorageMetadataPtr metadata_snapshot, const IStorage & storage)\n {\n     auto all_columns = metadata_snapshot->getColumns().getNamesOfPhysical();\n@@ -524,8 +533,11 @@ void MutationsInterpreter::prepare(bool dry_run)\n         validateUpdateColumns(source, metadata_snapshot, updated_columns, column_to_affected_materialized);\n     }\n \n+    std::function<bool(const String & file_name)> has_index_or_projection\n+        = [&](const String & file_name) { return source.hasIndexOrProjection(file_name); };\n+\n     if (settings.recalculate_dependencies_of_updated_columns)\n-        dependencies = getAllColumnDependencies(metadata_snapshot, updated_columns);\n+        dependencies = getAllColumnDependencies(metadata_snapshot, updated_columns, has_index_or_projection);\n \n     std::vector<String> read_columns;\n     /// First, break a sequence of commands into stages.\n@@ -680,20 +692,27 @@ void MutationsInterpreter::prepare(bool dry_run)\n             if (it == std::cend(indices_desc))\n                 throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Unknown index: {}\", command.index_name);\n \n-            auto query = (*it).expression_list_ast->clone();\n-            auto syntax_result = TreeRewriter(context).analyze(query, all_columns);\n-            const auto required_columns = syntax_result->requiredSourceColumns();\n-            for (const auto & column : required_columns)\n-                dependencies.emplace(column, ColumnDependency::SKIP_INDEX);\n-            materialized_indices.emplace(command.index_name);\n+            if (!source.hasIndexOrProjection(\"skp_idx_\" + it->name + \".idx\")\n+                && !source.hasIndexOrProjection(\"skp_idx_\" + it->name + \".idx2\"))\n+            {\n+                auto query = (*it).expression_list_ast->clone();\n+                auto syntax_result = TreeRewriter(context).analyze(query, all_columns);\n+                const auto required_columns = syntax_result->requiredSourceColumns();\n+                for (const auto & column : required_columns)\n+                    dependencies.emplace(column, ColumnDependency::SKIP_INDEX);\n+                materialized_indices.emplace(command.index_name);\n+            }\n         }\n         else if (command.type == MutationCommand::MATERIALIZE_PROJECTION)\n         {\n             mutation_kind.set(MutationKind::MUTATE_INDEX_PROJECTION);\n             const auto & projection = projections_desc.get(command.projection_name);\n-            for (const auto & column : projection.required_columns)\n-                dependencies.emplace(column, ColumnDependency::PROJECTION);\n-            materialized_projections.emplace(command.projection_name);\n+            if (!source.hasIndexOrProjection(projection.getDirectoryName()))\n+            {\n+                for (const auto & column : projection.required_columns)\n+                    dependencies.emplace(column, ColumnDependency::PROJECTION);\n+                materialized_projections.emplace(command.projection_name);\n+            }\n         }\n         else if (command.type == MutationCommand::DROP_INDEX)\n         {\n@@ -712,7 +731,8 @@ void MutationsInterpreter::prepare(bool dry_run)\n             {\n                 // just recalculate ttl_infos without remove expired data\n                 auto all_columns_vec = all_columns.getNames();\n-                auto new_dependencies = metadata_snapshot->getColumnDependencies(NameSet(all_columns_vec.begin(), all_columns_vec.end()), false);\n+                auto new_dependencies = metadata_snapshot->getColumnDependencies(\n+                    NameSet(all_columns_vec.begin(), all_columns_vec.end()), false, has_index_or_projection);\n                 for (const auto & dependency : new_dependencies)\n                 {\n                     if (dependency.kind == ColumnDependency::TTL_EXPRESSION)\n@@ -737,7 +757,8 @@ void MutationsInterpreter::prepare(bool dry_run)\n                 }\n \n                 auto all_columns_vec = all_columns.getNames();\n-                auto all_dependencies = getAllColumnDependencies(metadata_snapshot, NameSet(all_columns_vec.begin(), all_columns_vec.end()));\n+                auto all_dependencies = getAllColumnDependencies(\n+                    metadata_snapshot, NameSet(all_columns_vec.begin(), all_columns_vec.end()), has_index_or_projection);\n \n                 for (const auto & dependency : all_dependencies)\n                 {\n@@ -746,7 +767,7 @@ void MutationsInterpreter::prepare(bool dry_run)\n                 }\n \n                 /// Recalc only skip indices and projections of columns which could be updated by TTL.\n-                auto new_dependencies = metadata_snapshot->getColumnDependencies(new_updated_columns, true);\n+                auto new_dependencies = metadata_snapshot->getColumnDependencies(new_updated_columns, true, has_index_or_projection);\n                 for (const auto & dependency : new_dependencies)\n                 {\n                     if (dependency.kind == ColumnDependency::SKIP_INDEX || dependency.kind == ColumnDependency::PROJECTION)\n@@ -784,10 +805,10 @@ void MutationsInterpreter::prepare(bool dry_run)\n     /// We care about affected indices and projections because we also need to rewrite them\n     /// when one of index columns updated or filtered with delete.\n     /// The same about columns, that are needed for calculation of TTL expressions.\n+    NameSet changed_columns;\n+    NameSet unchanged_columns;\n     if (!dependencies.empty())\n     {\n-        NameSet changed_columns;\n-        NameSet unchanged_columns;\n         for (const auto & dependency : dependencies)\n         {\n             if (dependency.isReadOnly())\n@@ -838,6 +859,39 @@ void MutationsInterpreter::prepare(bool dry_run)\n         }\n     }\n \n+    for (const auto & index : metadata_snapshot->getSecondaryIndices())\n+    {\n+        if (source.hasIndexOrProjection(\"skp_idx_\" + index.name + \".idx\") || source.hasIndexOrProjection(\"skp_idx_\" + index.name + \".idx2\"))\n+        {\n+            const auto & index_cols = index.expression->getRequiredColumns();\n+            bool changed = std::any_of(\n+                index_cols.begin(),\n+                index_cols.end(),\n+                [&](const auto & col) { return updated_columns.contains(col) || changed_columns.contains(col); });\n+            if (changed)\n+                materialized_indices.insert(index.name);\n+        }\n+    }\n+\n+    for (const auto & projection : metadata_snapshot->getProjections())\n+    {\n+        if (source.hasIndexOrProjection(projection.getDirectoryName()))\n+        {\n+            const auto & projection_cols = projection.required_columns;\n+            bool changed = std::any_of(\n+                projection_cols.begin(),\n+                projection_cols.end(),\n+                [&](const auto & col) { return updated_columns.contains(col) || changed_columns.contains(col); });\n+            if (changed)\n+                materialized_projections.insert(projection.name);\n+        }\n+    }\n+\n+    /// Stages might be empty when we materialize skip indices or projections which don't add any\n+    /// column dependencies.\n+    if (stages.empty())\n+        stages.emplace_back(context);\n+\n     is_prepared = true;\n     prepareMutationStages(stages, dry_run);\n }\ndiff --git a/src/Interpreters/MutationsInterpreter.h b/src/Interpreters/MutationsInterpreter.h\nindex 49ba07641d9c..d783b503531c 100644\n--- a/src/Interpreters/MutationsInterpreter.h\n+++ b/src/Interpreters/MutationsInterpreter.h\n@@ -120,6 +120,7 @@ class MutationsInterpreter\n         bool supportsLightweightDelete() const;\n         bool hasLightweightDeleteMask() const;\n         bool materializeTTLRecalculateOnly() const;\n+        bool hasIndexOrProjection(const String & file_name) const;\n \n         void read(\n             Stage & first_stage,\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex d27b03fff44d..ca814a2afd58 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -632,7 +632,7 @@ void IMergeTreeDataPart::loadColumnsChecksumsIndexes(bool require_columns_checks\n         if (!parent_part)\n         {\n             loadTTLInfos();\n-            loadProjections(require_columns_checksums, check_consistency);\n+            loadProjections(require_columns_checksums, check_consistency, false /* if_not_loaded */);\n         }\n \n         if (check_consistency)\n@@ -690,13 +690,13 @@ void IMergeTreeDataPart::addProjectionPart(\n     const String & projection_name,\n     std::shared_ptr<IMergeTreeDataPart> && projection_part)\n {\n-    /// Here should be a check that projection we are trying to add\n-    /// does not exist, but unfortunately this check fails in tests.\n-    /// TODO: fix.\n+    if (hasProjection(projection_name))\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Projection part {} in part {} is already loaded. This is a bug\", projection_name, name);\n+\n     projection_parts[projection_name] = std::move(projection_part);\n }\n \n-void IMergeTreeDataPart::loadProjections(bool require_columns_checksums, bool check_consistency)\n+void IMergeTreeDataPart::loadProjections(bool require_columns_checksums, bool check_consistency, bool if_not_loaded)\n {\n     auto metadata_snapshot = storage.getInMemoryMetadataPtr();\n     for (const auto & projection : metadata_snapshot->projections)\n@@ -704,9 +704,18 @@ void IMergeTreeDataPart::loadProjections(bool require_columns_checksums, bool ch\n         auto path = projection.name + \".proj\";\n         if (getDataPartStorage().exists(path))\n         {\n-            auto part = getProjectionPartBuilder(projection.name).withPartFormatFromDisk().build();\n-            part->loadColumnsChecksumsIndexes(require_columns_checksums, check_consistency);\n-            addProjectionPart(projection.name, std::move(part));\n+            if (hasProjection(projection.name))\n+            {\n+                if (!if_not_loaded)\n+                    throw Exception(\n+                        ErrorCodes::LOGICAL_ERROR, \"Projection part {} in part {} is already loaded. This is a bug\", projection.name, name);\n+            }\n+            else\n+            {\n+                auto part = getProjectionPartBuilder(projection.name).withPartFormatFromDisk().build();\n+                part->loadColumnsChecksumsIndexes(require_columns_checksums, check_consistency);\n+                addProjectionPart(projection.name, std::move(part));\n+            }\n         }\n     }\n }\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex 411de3af9828..b6b6d8c66939 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -388,7 +388,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     bool hasProjection(const String & projection_name) const { return projection_parts.contains(projection_name); }\n \n-    void loadProjections(bool require_columns_checksums, bool check_consistency);\n+    void loadProjections(bool require_columns_checksums, bool check_consistency, bool if_not_loaded = false);\n \n     /// Return set of metadata file names without checksums. For example,\n     /// columns.txt or checksums.txt itself.\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 32665429051d..0115ce07b2c3 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -605,14 +605,14 @@ namespace\n \n ExpressionActionsPtr getCombinedIndicesExpression(\n     const KeyDescription & key,\n-    const IndicesDescription & indices,\n+    const MergeTreeIndices & indices,\n     const ColumnsDescription & columns,\n     ContextPtr context)\n {\n     ASTPtr combined_expr_list = key.expression_list_ast->clone();\n \n     for (const auto & index : indices)\n-        for (const auto & index_expr : index.expression_list_ast->children)\n+        for (const auto & index_expr : index->index.expression_list_ast->children)\n             combined_expr_list->children.push_back(index_expr->clone());\n \n     auto syntax_result = TreeRewriter(context).analyze(combined_expr_list, columns.getAllPhysical());\n@@ -644,14 +644,16 @@ DataTypes MergeTreeData::getMinMaxColumnsTypes(const KeyDescription & partition_\n     return {};\n }\n \n-ExpressionActionsPtr MergeTreeData::getPrimaryKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot) const\n+ExpressionActionsPtr\n+MergeTreeData::getPrimaryKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot, const MergeTreeIndices & indices) const\n {\n-    return getCombinedIndicesExpression(metadata_snapshot->getPrimaryKey(), metadata_snapshot->getSecondaryIndices(), metadata_snapshot->getColumns(), getContext());\n+    return getCombinedIndicesExpression(metadata_snapshot->getPrimaryKey(), indices, metadata_snapshot->getColumns(), getContext());\n }\n \n-ExpressionActionsPtr MergeTreeData::getSortingKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot) const\n+ExpressionActionsPtr\n+MergeTreeData::getSortingKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot, const MergeTreeIndices & indices) const\n {\n-    return getCombinedIndicesExpression(metadata_snapshot->getSortingKey(), metadata_snapshot->getSecondaryIndices(), metadata_snapshot->getColumns(), getContext());\n+    return getCombinedIndicesExpression(metadata_snapshot->getSortingKey(), indices, metadata_snapshot->getColumns(), getContext());\n }\n \n \ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 1c41de6fa194..6fd9d223f32f 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -956,8 +956,10 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Get column types required for partition key\n     static DataTypes getMinMaxColumnsTypes(const KeyDescription & partition_key);\n \n-    ExpressionActionsPtr getPrimaryKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot) const;\n-    ExpressionActionsPtr getSortingKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot) const;\n+    ExpressionActionsPtr\n+    getPrimaryKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot, const MergeTreeIndices & indices) const;\n+    ExpressionActionsPtr\n+    getSortingKeyAndSkipIndicesExpression(const StorageMetadataPtr & metadata_snapshot, const MergeTreeIndices & indices) const;\n \n     /// Get compression codec for part according to TTL rules and <compression>\n     /// section from config.xml.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex db486b163ebe..6ff4d6be870a 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -398,9 +398,11 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n \n     temp_part.temporary_directory_lock = data.getTemporaryPartDirectoryHolder(part_dir);\n \n+    auto indices = MergeTreeIndexFactory::instance().getMany(metadata_snapshot->getSecondaryIndices());\n+\n     /// If we need to calculate some columns to sort.\n     if (metadata_snapshot->hasSortingKey() || metadata_snapshot->hasSecondaryIndices())\n-        data.getSortingKeyAndSkipIndicesExpression(metadata_snapshot)->execute(block);\n+        data.getSortingKeyAndSkipIndicesExpression(metadata_snapshot, indices)->execute(block);\n \n     Names sort_columns = metadata_snapshot->getSortingKeyColumns();\n     SortDescription sort_description;\n@@ -517,10 +519,16 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     ///  either default lz4 or compression method with zero thresholds on absolute and relative part size.\n     auto compression_codec = data.getContext()->chooseCompressionCodec(0, 0);\n \n-    const auto & index_factory = MergeTreeIndexFactory::instance();\n-    auto out = std::make_unique<MergedBlockOutputStream>(new_data_part, metadata_snapshot, columns,\n-        index_factory.getMany(metadata_snapshot->getSecondaryIndices()), compression_codec,\n-        context->getCurrentTransaction(), false, false, context->getWriteSettings());\n+    auto out = std::make_unique<MergedBlockOutputStream>(\n+        new_data_part,\n+        metadata_snapshot,\n+        columns,\n+        indices,\n+        compression_codec,\n+        context->getCurrentTransaction(),\n+        false,\n+        false,\n+        context->getWriteSettings());\n \n     out->writeWithPermutation(block, perm_ptr);\n \n@@ -606,7 +614,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n \n     /// If we need to calculate some columns to sort.\n     if (metadata_snapshot->hasSortingKey() || metadata_snapshot->hasSecondaryIndices())\n-        data.getSortingKeyAndSkipIndicesExpression(metadata_snapshot)->execute(block);\n+        data.getSortingKeyAndSkipIndicesExpression(metadata_snapshot, {})->execute(block);\n \n     Names sort_columns = metadata_snapshot->getSortingKeyColumns();\n     SortDescription sort_description;\ndiff --git a/src/Storages/MergeTree/MutateTask.cpp b/src/Storages/MergeTree/MutateTask.cpp\nindex 76096d00641c..7031027002d2 100644\n--- a/src/Storages/MergeTree/MutateTask.cpp\n+++ b/src/Storages/MergeTree/MutateTask.cpp\n@@ -200,8 +200,7 @@ static void splitAndModifyMutationCommands(\n             {\n                 for_file_renames.push_back(command);\n             }\n-            /// If we don't have this column in source part, than we don't need\n-            /// to materialize it\n+            /// If we don't have this column in source part, we don't need to materialize it.\n             else if (part_columns.has(command.column_name))\n             {\n                 if (command.type == MutationCommand::Type::READ_COLUMN)\n@@ -438,51 +437,13 @@ static ExecuteTTLType shouldExecuteTTL(const StorageMetadataPtr & metadata_snaps\n }\n \n \n-/// Get skip indices, that should exists in the resulting data part.\n-static MergeTreeIndices getIndicesForNewDataPart(\n-    const IndicesDescription & all_indices,\n-    const MutationCommands & commands_for_removes)\n-{\n-    NameSet removed_indices;\n-    for (const auto & command : commands_for_removes)\n-        if (command.type == MutationCommand::DROP_INDEX)\n-            removed_indices.insert(command.column_name);\n-\n-    MergeTreeIndices new_indices;\n-    for (const auto & index : all_indices)\n-        if (!removed_indices.contains(index.name))\n-            new_indices.push_back(MergeTreeIndexFactory::instance().get(index));\n-\n-    return new_indices;\n-}\n-\n-static std::vector<ProjectionDescriptionRawPtr> getProjectionsForNewDataPart(\n-    const ProjectionsDescription & all_projections,\n-    const MutationCommands & commands_for_removes)\n-{\n-    NameSet removed_projections;\n-    for (const auto & command : commands_for_removes)\n-        if (command.type == MutationCommand::DROP_PROJECTION)\n-            removed_projections.insert(command.column_name);\n-\n-    std::vector<ProjectionDescriptionRawPtr> new_projections;\n-    for (const auto & projection : all_projections)\n-        if (!removed_projections.contains(projection.name))\n-            new_projections.push_back(&projection);\n-\n-    return new_projections;\n-}\n-\n-\n /// Return set of indices which should be recalculated during mutation also\n /// wraps input stream into additional expression stream\n static std::set<MergeTreeIndexPtr> getIndicesToRecalculate(\n     QueryPipelineBuilder & builder,\n-    const NameSet & updated_columns,\n     const StorageMetadataPtr & metadata_snapshot,\n     ContextPtr context,\n-    const NameSet & materialized_indices,\n-    const MergeTreeData::DataPartPtr & source_part)\n+    const NameSet & materialized_indices)\n {\n     /// Checks if columns used in skipping indexes modified.\n     const auto & index_factory = MergeTreeIndexFactory::instance();\n@@ -492,11 +453,7 @@ static std::set<MergeTreeIndexPtr> getIndicesToRecalculate(\n \n     for (const auto & index : indices)\n     {\n-        bool has_index =\n-            source_part->checksums.has(INDEX_FILE_PREFIX + index.name + \".idx\") ||\n-            source_part->checksums.has(INDEX_FILE_PREFIX + index.name + \".idx2\");\n-        // If we ask to materialize and it already exists\n-        if (!has_index && materialized_indices.contains(index.name))\n+        if (materialized_indices.contains(index.name))\n         {\n             if (indices_to_recalc.insert(index_factory.get(index)).second)\n             {\n@@ -505,26 +462,6 @@ static std::set<MergeTreeIndexPtr> getIndicesToRecalculate(\n                     indices_recalc_expr_list->children.push_back(expr->clone());\n             }\n         }\n-        // If some dependent columns gets mutated\n-        else\n-        {\n-            bool mutate = false;\n-            const auto & index_cols = index.expression->getRequiredColumns();\n-            for (const auto & col : index_cols)\n-            {\n-                if (updated_columns.contains(col))\n-                {\n-                    mutate = true;\n-                    break;\n-                }\n-            }\n-            if (mutate && indices_to_recalc.insert(index_factory.get(index)).second)\n-            {\n-                ASTPtr expr_list = index.expression_list_ast->clone();\n-                for (const auto & expr : expr_list->children)\n-                    indices_recalc_expr_list->children.push_back(expr->clone());\n-            }\n-        }\n     }\n \n     if (!indices_to_recalc.empty() && builder.initialized())\n@@ -545,37 +482,15 @@ static std::set<MergeTreeIndexPtr> getIndicesToRecalculate(\n     return indices_to_recalc;\n }\n \n-std::set<ProjectionDescriptionRawPtr> getProjectionsToRecalculate(\n-    const NameSet & updated_columns,\n+static std::set<ProjectionDescriptionRawPtr> getProjectionsToRecalculate(\n     const StorageMetadataPtr & metadata_snapshot,\n-    const NameSet & materialized_projections,\n-    const MergeTreeData::DataPartPtr & source_part)\n+    const NameSet & materialized_projections)\n {\n-    /// Checks if columns used in projections modified.\n     std::set<ProjectionDescriptionRawPtr> projections_to_recalc;\n     for (const auto & projection : metadata_snapshot->getProjections())\n     {\n-        // If we ask to materialize and it doesn't exist\n-        if (!source_part->checksums.has(projection.name + \".proj\") && materialized_projections.contains(projection.name))\n-        {\n+        if (materialized_projections.contains(projection.name))\n             projections_to_recalc.insert(&projection);\n-        }\n-        else\n-        {\n-            // If some dependent columns gets mutated\n-            bool mutate = false;\n-            const auto & projection_cols = projection.required_columns;\n-            for (const auto & col : projection_cols)\n-            {\n-                if (updated_columns.contains(col))\n-                {\n-                    mutate = true;\n-                    break;\n-                }\n-            }\n-            if (mutate)\n-                projections_to_recalc.insert(&projection);\n-        }\n     }\n     return projections_to_recalc;\n }\n@@ -618,42 +533,45 @@ static NameSet collectFilesToSkip(\n     /// Do not hardlink this file because it's always rewritten at the end of mutation.\n     files_to_skip.insert(IMergeTreeDataPart::SERIALIZATION_FILE_NAME);\n \n-    auto new_stream_counts = getStreamCounts(new_part, new_part->getColumns().getNames());\n-    auto source_updated_stream_counts = getStreamCounts(source_part, updated_header.getNames());\n-    auto new_updated_stream_counts = getStreamCounts(new_part, updated_header.getNames());\n-\n-    /// Skip all modified files in new part.\n-    for (const auto & [stream_name, _] : new_updated_stream_counts)\n+    for (const auto & index : indices_to_recalc)\n     {\n-        files_to_skip.insert(stream_name + \".bin\");\n-        files_to_skip.insert(stream_name + mrk_extension);\n+        /// Since MinMax index has .idx2 extension, we need to add correct extension.\n+        files_to_skip.insert(index->getFileName() + index->getSerializedFileExtension());\n+        files_to_skip.insert(index->getFileName() + mrk_extension);\n     }\n \n-    /// Skip files that we read from source part and do not write in new part.\n-    /// E.g. ALTER MODIFY from LowCardinality(String) to String.\n-    for (const auto & [stream_name, _] : source_updated_stream_counts)\n+    for (const auto & projection : projections_to_recalc)\n+        files_to_skip.insert(projection->getDirectoryName());\n+\n+    if (isWidePart(source_part))\n     {\n-        /// If we read shared stream and do not write it\n-        /// (e.g. while ALTER MODIFY COLUMN from array of Nested type to String),\n-        /// we need to hardlink its files, because they will be lost otherwise.\n-        bool need_hardlink = new_updated_stream_counts[stream_name] == 0 && new_stream_counts[stream_name] != 0;\n+        auto new_stream_counts = getStreamCounts(new_part, new_part->getColumns().getNames());\n+        auto source_updated_stream_counts = getStreamCounts(source_part, updated_header.getNames());\n+        auto new_updated_stream_counts = getStreamCounts(new_part, updated_header.getNames());\n \n-        if (!need_hardlink)\n+        /// Skip all modified files in new part.\n+        for (const auto & [stream_name, _] : new_updated_stream_counts)\n         {\n             files_to_skip.insert(stream_name + \".bin\");\n             files_to_skip.insert(stream_name + mrk_extension);\n         }\n-    }\n \n-    for (const auto & index : indices_to_recalc)\n-    {\n-        /// Since MinMax index has .idx2 extension, we need to add correct extension.\n-        files_to_skip.insert(index->getFileName() + index->getSerializedFileExtension());\n-        files_to_skip.insert(index->getFileName() + mrk_extension);\n-    }\n+        /// Skip files that we read from source part and do not write in new part.\n+        /// E.g. ALTER MODIFY from LowCardinality(String) to String.\n+        for (const auto & [stream_name, _] : source_updated_stream_counts)\n+        {\n+            /// If we read shared stream and do not write it\n+            /// (e.g. while ALTER MODIFY COLUMN from array of Nested type to String),\n+            /// we need to hardlink its files, because they will be lost otherwise.\n+            bool need_hardlink = new_updated_stream_counts[stream_name] == 0 && new_stream_counts[stream_name] != 0;\n \n-    for (const auto & projection : projections_to_recalc)\n-        files_to_skip.insert(projection->getDirectoryName());\n+            if (!need_hardlink)\n+            {\n+                files_to_skip.insert(stream_name + \".bin\");\n+                files_to_skip.insert(stream_name + mrk_extension);\n+            }\n+        }\n+    }\n \n     return files_to_skip;\n }\n@@ -701,57 +619,60 @@ static NameToNameVector collectFilesForRenames(\n             if (source_part->checksums.has(command.column_name + \".proj\"))\n                 add_rename(command.column_name + \".proj\", \"\");\n         }\n-        else if (command.type == MutationCommand::Type::DROP_COLUMN)\n+        else if (isWidePart(source_part))\n         {\n-            ISerialization::StreamCallback callback = [&](const ISerialization::SubstreamPath & substream_path)\n+            if (command.type == MutationCommand::Type::DROP_COLUMN)\n             {\n-                String stream_name = ISerialization::getFileNameForStream({command.column_name, command.data_type}, substream_path);\n-                /// Delete files if they are no longer shared with another column.\n-                if (--stream_counts[stream_name] == 0)\n+                ISerialization::StreamCallback callback = [&](const ISerialization::SubstreamPath & substream_path)\n                 {\n-                    add_rename(stream_name + \".bin\", \"\");\n-                    add_rename(stream_name + mrk_extension, \"\");\n-                }\n-            };\n-\n-            if (auto serialization = source_part->tryGetSerialization(command.column_name))\n-                serialization->enumerateStreams(callback);\n-        }\n-        else if (command.type == MutationCommand::Type::RENAME_COLUMN)\n-        {\n-            String escaped_name_from = escapeForFileName(command.column_name);\n-            String escaped_name_to = escapeForFileName(command.rename_to);\n+                    String stream_name = ISerialization::getFileNameForStream({command.column_name, command.data_type}, substream_path);\n+                    /// Delete files if they are no longer shared with another column.\n+                    if (--stream_counts[stream_name] == 0)\n+                    {\n+                        add_rename(stream_name + \".bin\", \"\");\n+                        add_rename(stream_name + mrk_extension, \"\");\n+                    }\n+                };\n \n-            ISerialization::StreamCallback callback = [&](const ISerialization::SubstreamPath & substream_path)\n+                if (auto serialization = source_part->tryGetSerialization(command.column_name))\n+                    serialization->enumerateStreams(callback);\n+            }\n+            else if (command.type == MutationCommand::Type::RENAME_COLUMN)\n             {\n-                String stream_from = ISerialization::getFileNameForStream(command.column_name, substream_path);\n-                String stream_to = boost::replace_first_copy(stream_from, escaped_name_from, escaped_name_to);\n+                String escaped_name_from = escapeForFileName(command.column_name);\n+                String escaped_name_to = escapeForFileName(command.rename_to);\n \n-                if (stream_from != stream_to)\n+                ISerialization::StreamCallback callback = [&](const ISerialization::SubstreamPath & substream_path)\n                 {\n-                    add_rename(stream_from + \".bin\", stream_to + \".bin\");\n-                    add_rename(stream_from + mrk_extension, stream_to + mrk_extension);\n-                }\n-            };\n-\n-            if (auto serialization = source_part->tryGetSerialization(command.column_name))\n-                serialization->enumerateStreams(callback);\n-        }\n-        else if (command.type == MutationCommand::Type::READ_COLUMN)\n-        {\n-            /// Remove files for streams that exist in source_part,\n-            /// but were removed in new_part by MODIFY COLUMN from\n-            /// type with higher number of streams (e.g. LowCardinality -> String).\n+                    String stream_from = ISerialization::getFileNameForStream(command.column_name, substream_path);\n+                    String stream_to = boost::replace_first_copy(stream_from, escaped_name_from, escaped_name_to);\n \n-            auto old_streams = getStreamCounts(source_part, source_part->getColumns().getNames());\n-            auto new_streams = getStreamCounts(new_part, source_part->getColumns().getNames());\n+                    if (stream_from != stream_to)\n+                    {\n+                        add_rename(stream_from + \".bin\", stream_to + \".bin\");\n+                        add_rename(stream_from + mrk_extension, stream_to + mrk_extension);\n+                    }\n+                };\n \n-            for (const auto & [old_stream, _] : old_streams)\n+                if (auto serialization = source_part->tryGetSerialization(command.column_name))\n+                    serialization->enumerateStreams(callback);\n+            }\n+            else if (command.type == MutationCommand::Type::READ_COLUMN)\n             {\n-                if (!new_streams.contains(old_stream) && --stream_counts[old_stream] == 0)\n+                /// Remove files for streams that exist in source_part,\n+                /// but were removed in new_part by MODIFY COLUMN from\n+                /// type with higher number of streams (e.g. LowCardinality -> String).\n+\n+                auto old_streams = getStreamCounts(source_part, source_part->getColumns().getNames());\n+                auto new_streams = getStreamCounts(new_part, source_part->getColumns().getNames());\n+\n+                for (const auto & [old_stream, _] : old_streams)\n                 {\n-                    add_rename(old_stream + \".bin\", \"\");\n-                    add_rename(old_stream + mrk_extension, \"\");\n+                    if (!new_streams.contains(old_stream) && --stream_counts[old_stream] == 0)\n+                    {\n+                        add_rename(old_stream + \".bin\", \"\");\n+                        add_rename(old_stream + mrk_extension, \"\");\n+                    }\n                 }\n             }\n         }\n@@ -851,11 +772,8 @@ void finalizeMutatedPart(\n     new_data_part->minmax_idx = source_part->minmax_idx;\n     new_data_part->modification_time = time(nullptr);\n \n-    /// This line should not be here because at that moment\n-    /// of executing of mutation all projections should be loaded.\n-    /// But unfortunately without it some tests fail.\n-    /// TODO: fix.\n-    new_data_part->loadProjections(false, false);\n+    /// Load rest projections which are hardlinked\n+    new_data_part->loadProjections(false, false, true /* if_not_loaded */);\n \n     /// All information about sizes is stored in checksums.\n     /// It doesn't make sense to touch filesystem for sizes.\n@@ -917,9 +835,9 @@ struct MutationContext\n     std::vector<ProjectionDescriptionRawPtr> projections_to_build;\n     IMergeTreeDataPart::MinMaxIndexPtr minmax_idx{nullptr};\n \n-    NameSet updated_columns;\n     std::set<MergeTreeIndexPtr> indices_to_recalc;\n     std::set<ProjectionDescriptionRawPtr> projections_to_recalc;\n+    MergeTreeData::DataPart::Checksums existing_indices_checksums;\n     NameSet files_to_skip;\n     NameToNameVector files_to_rename;\n \n@@ -1331,10 +1249,107 @@ class MutateAllPartColumnsTask : public IExecutableTask\n         /// (which is locked in shared mode when input streams are created) and when inserting new data\n         /// the order is reverse. This annoys TSan even though one lock is locked in shared mode and thus\n         /// deadlock is impossible.\n-        ctx->compression_codec = ctx->data->getCompressionCodecForPart(ctx->source_part->getBytesOnDisk(), ctx->source_part->ttl_infos, ctx->time_of_mutation);\n+        ctx->compression_codec\n+            = ctx->data->getCompressionCodecForPart(ctx->source_part->getBytesOnDisk(), ctx->source_part->ttl_infos, ctx->time_of_mutation);\n+\n+        NameSet entries_to_hardlink;\n+\n+        NameSet removed_indices;\n+        for (const auto & command : ctx->for_file_renames)\n+        {\n+            if (command.type == MutationCommand::DROP_INDEX)\n+                removed_indices.insert(command.column_name);\n+        }\n+\n+        const auto & indices = ctx->metadata_snapshot->getSecondaryIndices();\n+        MergeTreeIndices skip_indices;\n+        for (const auto & idx : indices)\n+        {\n+            if (removed_indices.contains(idx.name))\n+                continue;\n+\n+            if (ctx->materialized_indices.contains(idx.name))\n+            {\n+                skip_indices.push_back(MergeTreeIndexFactory::instance().get(idx));\n+            }\n+            else\n+            {\n+                auto prefix = fmt::format(\"{}{}.\", INDEX_FILE_PREFIX, idx.name);\n+                auto it = ctx->source_part->checksums.files.upper_bound(prefix);\n+                while (it != ctx->source_part->checksums.files.end())\n+                {\n+                    if (!startsWith(it->first, prefix))\n+                        break;\n+\n+                    entries_to_hardlink.insert(it->first);\n+                    ctx->existing_indices_checksums.addFile(it->first, it->second.file_size, it->second.file_hash);\n+                    ++it;\n+                }\n+            }\n+        }\n+\n+        NameSet removed_projections;\n+        for (const auto & command : ctx->for_file_renames)\n+        {\n+            if (command.type == MutationCommand::DROP_PROJECTION)\n+                removed_projections.insert(command.column_name);\n+        }\n \n-        auto skip_part_indices = MutationHelpers::getIndicesForNewDataPart(ctx->metadata_snapshot->getSecondaryIndices(), ctx->for_file_renames);\n-        ctx->projections_to_build = MutationHelpers::getProjectionsForNewDataPart(ctx->metadata_snapshot->getProjections(), ctx->for_file_renames);\n+        const auto & projections = ctx->metadata_snapshot->getProjections();\n+        for (const auto & projection : projections)\n+        {\n+            if (removed_projections.contains(projection.name))\n+                continue;\n+\n+            if (ctx->materialized_projections.contains(projection.name))\n+            {\n+                ctx->projections_to_build.push_back(&projection);\n+            }\n+            else\n+            {\n+                if (ctx->source_part->checksums.has(projection.getDirectoryName()))\n+                    entries_to_hardlink.insert(projection.getDirectoryName());\n+            }\n+        }\n+\n+        NameSet hardlinked_files;\n+        /// Create hardlinks for unchanged files\n+        for (auto it = ctx->source_part->getDataPartStorage().iterate(); it->isValid(); it->next())\n+        {\n+            if (!entries_to_hardlink.contains(it->name()))\n+                continue;\n+\n+            if (it->isFile())\n+            {\n+                ctx->new_data_part->getDataPartStorage().createHardLinkFrom(\n+                    ctx->source_part->getDataPartStorage(), it->name(), it->name());\n+                hardlinked_files.insert(it->name());\n+            }\n+            else\n+            {\n+                // it's a projection part directory\n+                ctx->new_data_part->getDataPartStorage().createProjection(it->name());\n+\n+                auto projection_data_part_storage_src = ctx->source_part->getDataPartStorage().getProjection(it->name());\n+                auto projection_data_part_storage_dst = ctx->new_data_part->getDataPartStorage().getProjection(it->name());\n+\n+                for (auto p_it = projection_data_part_storage_src->iterate(); p_it->isValid(); p_it->next())\n+                {\n+                    projection_data_part_storage_dst->createHardLinkFrom(\n+                        *projection_data_part_storage_src, p_it->name(), p_it->name());\n+\n+                    auto file_name_with_projection_prefix = fs::path(projection_data_part_storage_src->getPartDirectory()) / p_it->name();\n+                    hardlinked_files.insert(file_name_with_projection_prefix);\n+                }\n+            }\n+        }\n+\n+        /// Tracking of hardlinked files required for zero-copy replication.\n+        /// We don't remove them when we delete last copy of source part because\n+        /// new part can use them.\n+        ctx->hardlinked_files.source_table_shared_id = ctx->source_part->storage.getTableSharedID();\n+        ctx->hardlinked_files.source_part_name = ctx->source_part->name;\n+        ctx->hardlinked_files.hardlinks_from_source_part = std::move(hardlinked_files);\n \n         if (!ctx->mutating_pipeline_builder.initialized())\n             throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot mutate part columns with uninitialized mutations stream. It's a bug\");\n@@ -1343,8 +1358,8 @@ class MutateAllPartColumnsTask : public IExecutableTask\n \n         if (ctx->metadata_snapshot->hasPrimaryKey() || ctx->metadata_snapshot->hasSecondaryIndices())\n         {\n-            builder.addTransform(\n-                std::make_shared<ExpressionTransform>(builder.getHeader(), ctx->data->getPrimaryKeyAndSkipIndicesExpression(ctx->metadata_snapshot)));\n+            builder.addTransform(std::make_shared<ExpressionTransform>(\n+                builder.getHeader(), ctx->data->getPrimaryKeyAndSkipIndicesExpression(ctx->metadata_snapshot, skip_indices)));\n \n             builder.addTransform(std::make_shared<MaterializingTransform>(builder.getHeader()));\n         }\n@@ -1361,7 +1376,7 @@ class MutateAllPartColumnsTask : public IExecutableTask\n             ctx->new_data_part,\n             ctx->metadata_snapshot,\n             ctx->new_data_part->getColumns(),\n-            skip_part_indices,\n+            skip_indices,\n             ctx->compression_codec,\n             ctx->txn,\n             /*reset_columns=*/ true,\n@@ -1381,10 +1396,12 @@ class MutateAllPartColumnsTask : public IExecutableTask\n     void finalize()\n     {\n         ctx->new_data_part->minmax_idx = std::move(ctx->minmax_idx);\n+        ctx->new_data_part->loadProjections(false, false, true /* if_not_loaded */);\n         ctx->mutating_executor.reset();\n         ctx->mutating_pipeline.reset();\n \n-        static_pointer_cast<MergedBlockOutputStream>(ctx->out)->finalizePart(ctx->new_data_part, ctx->need_sync);\n+        static_pointer_cast<MergedBlockOutputStream>(ctx->out)->finalizePart(\n+            ctx->new_data_part, ctx->need_sync, nullptr, &ctx->existing_indices_checksums);\n         ctx->out.reset();\n     }\n \n@@ -1530,7 +1547,7 @@ class MutateSomePartColumnsTask : public IExecutableTask\n         /// new part can use them.\n         ctx->hardlinked_files.source_table_shared_id = ctx->source_part->storage.getTableSharedID();\n         ctx->hardlinked_files.source_part_name = ctx->source_part->name;\n-        ctx->hardlinked_files.hardlinks_from_source_part = hardlinked_files;\n+        ctx->hardlinked_files.hardlinks_from_source_part = std::move(hardlinked_files);\n \n         (*ctx->mutate_entry)->columns_written = ctx->storage_columns.size() - ctx->updated_header.columns();\n \n@@ -1878,14 +1895,10 @@ bool MutateTask::prepare()\n     }\n     else /// TODO: check that we modify only non-key columns in this case.\n     {\n-        /// We will modify only some of the columns. Other columns and key values can be copied as-is.\n-        for (const auto & name_type : ctx->updated_header.getNamesAndTypesList())\n-            ctx->updated_columns.emplace(name_type.name);\n-\n         ctx->indices_to_recalc = MutationHelpers::getIndicesToRecalculate(\n-            ctx->mutating_pipeline_builder, ctx->updated_columns, ctx->metadata_snapshot, ctx->context, ctx->materialized_indices, ctx->source_part);\n-        ctx->projections_to_recalc = MutationHelpers::getProjectionsToRecalculate(\n-            ctx->updated_columns, ctx->metadata_snapshot, ctx->materialized_projections, ctx->source_part);\n+            ctx->mutating_pipeline_builder, ctx->metadata_snapshot, ctx->context, ctx->materialized_indices);\n+\n+        ctx->projections_to_recalc = MutationHelpers::getProjectionsToRecalculate(ctx->metadata_snapshot, ctx->materialized_projections);\n \n         ctx->files_to_skip = MutationHelpers::collectFilesToSkip(\n             ctx->source_part,\ndiff --git a/src/Storages/StorageInMemoryMetadata.cpp b/src/Storages/StorageInMemoryMetadata.cpp\nindex 45abd4bebef7..afe753498644 100644\n--- a/src/Storages/StorageInMemoryMetadata.cpp\n+++ b/src/Storages/StorageInMemoryMetadata.cpp\n@@ -236,7 +236,10 @@ bool StorageInMemoryMetadata::hasAnyGroupByTTL() const\n     return !table_ttl.group_by_ttl.empty();\n }\n \n-ColumnDependencies StorageInMemoryMetadata::getColumnDependencies(const NameSet & updated_columns, bool include_ttl_target) const\n+ColumnDependencies StorageInMemoryMetadata::getColumnDependencies(\n+    const NameSet & updated_columns,\n+    bool include_ttl_target,\n+    const std::function<bool(const String & file_name)> & has_indice_or_projection) const\n {\n     if (updated_columns.empty())\n         return {};\n@@ -264,10 +267,16 @@ ColumnDependencies StorageInMemoryMetadata::getColumnDependencies(const NameSet\n     };\n \n     for (const auto & index : getSecondaryIndices())\n-        add_dependent_columns(index.expression, indices_columns);\n+    {\n+        if (has_indice_or_projection(\"skp_idx_\" + index.name + \".idx\") || has_indice_or_projection(\"skp_idx_\" + index.name + \".idx2\"))\n+            add_dependent_columns(index.expression, indices_columns);\n+    }\n \n     for (const auto & projection : getProjections())\n-        add_dependent_columns(&projection, projections_columns);\n+    {\n+        if (has_indice_or_projection(projection.getDirectoryName()))\n+            add_dependent_columns(&projection, projections_columns);\n+    }\n \n     auto add_for_rows_ttl = [&](const auto & expression, auto & to_set)\n     {\n@@ -312,7 +321,6 @@ ColumnDependencies StorageInMemoryMetadata::getColumnDependencies(const NameSet\n         res.emplace(column, ColumnDependency::TTL_TARGET);\n \n     return res;\n-\n }\n \n Block StorageInMemoryMetadata::getSampleBlockInsertable() const\ndiff --git a/src/Storages/StorageInMemoryMetadata.h b/src/Storages/StorageInMemoryMetadata.h\nindex 25618c5b03fd..4ed7eb8bf295 100644\n--- a/src/Storages/StorageInMemoryMetadata.h\n+++ b/src/Storages/StorageInMemoryMetadata.h\n@@ -147,9 +147,12 @@ struct StorageInMemoryMetadata\n     TTLDescriptions getGroupByTTLs() const;\n     bool hasAnyGroupByTTL() const;\n \n-    /// Returns columns, which will be needed to calculate dependencies (skip\n-    /// indices, TTL expressions) if we update @updated_columns set of columns.\n-    ColumnDependencies getColumnDependencies(const NameSet & updated_columns, bool include_ttl_target) const;\n+    /// Returns columns, which will be needed to calculate dependencies (skip indices, projections,\n+    /// TTL expressions) if we update @updated_columns set of columns.\n+    ColumnDependencies getColumnDependencies(\n+        const NameSet & updated_columns,\n+        bool include_ttl_target,\n+        const std::function<bool(const String & file_name)> & has_indice_or_projection) const;\n \n     /// Block with ordinary + materialized columns.\n     Block getSampleBlock() const;\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02763_mutate_compact_part_with_skip_indices_and_projections.reference b/tests/queries/0_stateless/02763_mutate_compact_part_with_skip_indices_and_projections.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02763_mutate_compact_part_with_skip_indices_and_projections.sql b/tests/queries/0_stateless/02763_mutate_compact_part_with_skip_indices_and_projections.sql\nnew file mode 100644\nindex 000000000000..bb9825fe5a0a\n--- /dev/null\n+++ b/tests/queries/0_stateless/02763_mutate_compact_part_with_skip_indices_and_projections.sql\n@@ -0,0 +1,31 @@\n+DROP TABLE IF EXISTS test;\n+\n+CREATE TABLE test ( col1 Int64, dt Date ) ENGINE = MergeTree PARTITION BY dt ORDER BY tuple();\n+\n+INSERT INTO test FORMAT Values (1, today());\n+\n+ALTER TABLE test ADD COLUMN col2 String;\n+\n+ALTER TABLE test ADD INDEX i1 (col1, col2) TYPE set(100) GRANULARITY 1;\n+\n+ALTER TABLE test MATERIALIZE INDEX i1;\n+\n+ALTER TABLE test ADD COLUMN col3 String;\n+\n+ALTER TABLE test DROP COLUMN col3;\n+\n+DROP TABLE IF EXISTS test;\n+\n+CREATE TABLE test ( col1 Int64, dt Date ) ENGINE = MergeTree PARTITION BY dt ORDER BY tuple();\n+\n+INSERT INTO test FORMAT Values (1, today());\n+\n+ALTER TABLE test ADD COLUMN col2 String;\n+\n+ALTER TABLE test ADD PROJECTION p1 ( SELECT col2, sum(col1) GROUP BY col2 );\n+\n+ALTER TABLE test MATERIALIZE PROJECTION p1;\n+\n+ALTER TABLE test ADD COLUMN col3 String;\n+\n+ALTER TABLE test DROP COLUMN col3;\n",
  "problem_statement": "Drop column lead to 'Missing columns' bug with projections\nClickHouse Version 22.11.1.1\r\n\r\n`CREATE TABLE test\r\n(\r\n    col1 Int64,\r\n    dt Date\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY dt\r\nORDER BY tuple()`\r\n\r\nOk.\r\n0 rows in set. Elapsed: 0.055 sec.\r\n\r\n\r\n`INSERT INTO test FORMAT Values (1, today())`\r\n\r\nOk.\r\n1 row in set. Elapsed: 0.033 sec.\r\n\r\n\r\n`ALTER TABLE test ADD COLUMN col2 String`\r\n\r\nOk.\r\n0 rows in set. Elapsed: 0.007 sec.\r\n\r\n\r\n`ALTER TABLE test\r\n    ADD PROJECTION p1\r\n    (\r\n        SELECT\r\n            col2,\r\n            sum(col1)\r\n        GROUP BY col2\r\n    )`\r\n\r\nOk.\r\n0 rows in set. Elapsed: 0.017 sec.\r\n\r\n\r\n`ALTER TABLE test\r\n    ADD COLUMN col3 String`\r\n\r\nOk.\r\n0 rows in set. Elapsed: 0.010 sec.\r\n\r\n\r\n`ALTER TABLE test\r\n    DROP COLUMN col3`\r\n\r\n0 rows in set. Elapsed: 0.044 sec.\r\nReceived exception from server (version 22.11.1):\r\nCode: 341. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Exception happened during execution of mutation 'mutation_2.txt' with part '20221110_1_1_0' reason: 'Code: 47. DB::Exception: Missing columns: 'col2' while processing query: 'SELECT col2, sum(col1) GROUP BY col2', required columns: 'col2' 'col1' 'col2' 'col1'. (UNKNOWN_IDENTIFIER) (version 22.11.1.1)'. This error maybe retryable or not. In case of unretryable error, mutation can be killed with KILL MUTATION query. (UNFINISHED)\nNOT_FOUND_COLUMN_IN_BLOCK error in mutation adding data skipping index to a default column\n> A clear and concise description of what works not as it is supposed to.\r\n\r\nData skipping indexes on DEFAULT and MATERIALIZED columns can cause `NOT_FOUND_COLUMN_IN_BLOCK` errors on subsequent mutations.\r\n\r\nReproducer fiddle: https://fiddle.clickhouse.com/ddfd1acb-af96-4597-b78d-fad02f8aa666\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nTested under 22.8 and 23.2.\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\n22.8\r\n\r\n```sql\r\nCREATE TABLE IF NOT EXISTS test (a Int64) ENGINE = MergeTree() ORDER BY a\r\nINSERT INTO test (a) VALUES (1)\r\n\r\nALTER TABLE test ADD COLUMN default_1 Int64 DEFAULT a + 1\r\n\r\nALTER TABLE test ADD COLUMN default_2 Int64 DEFAULT a + 2\r\nALTER TABLE test ADD INDEX default_2_index default_2 TYPE minmax GRANULARITY 1\r\n\r\nALTER TABLE test UPDATE default_1 = default_1 WHERE 1=1\r\n```\r\n\r\n**Expected behavior**\r\n\r\nMutation succeeds without errors\r\n\r\n**Error message and/or stacktrace**\r\n\r\n`system.mutations` reports:\r\n\r\n```\r\nSELECT *\r\nFROM system.mutations\r\n\r\nQuery id: b8c046eb-3414-455e-9cf7-a8c1afd2944a\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\ndatabase:                   default\r\ntable:                      test\r\nmutation_id:                mutation_2.txt\r\ncommand:                    UPDATE default_1 = default_1 WHERE 1 = 1\r\ncreate_time:                2023-03-14 07:05:14\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [2]\r\nparts_to_do_names:          ['all_1_1_0']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         all_1_1_0\r\nlatest_fail_time:           2023-03-14 07:07:12\r\nlatest_fail_reason:         Code: 10. DB::Exception: Not found column default_2 in block. There are only columns: a, default_1. (NOT_FOUND_COLUMN_IN_BLOCK) (version 22.8.9.24 (official build))\r\n```\r\n\r\nError logs:\r\n\r\n```\r\n2023.03.14 07:07:42.953529 [ 355 ] {5129bccd-1e5a-4f5e-b34f-b5c951a4f810::all_1_1_0_2} <Error> MutatePlainMergeTreeTask: Code: 10. DB::Exception: Not found column default_2 in block. There are only columns: a, default_1. (NOT_FOUND_COLUMN_IN_BLOCK) (version 22.8.9.24 (official build))\r\n2023.03.14 07:07:42.953835 [ 355 ] {5129bccd-1e5a-4f5e-b34f-b5c951a4f810::all_1_1_0_2} <Error> virtual bool DB::MutatePlainMergeTreeTask::executeStep(): Code: 10. DB::Exception: Not found column default_2 in block. There are only columns: a, default_1. (NOT_FOUND_COLUMN_IN_BLOCK), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa3efd5a in /usr/bin/clickhouse\r\n1. DB::Block::getByName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool) const @ 0x13ef15b2 in /usr/bin/clickhouse\r\n2. DB::getBlockAndPermute(DB::Block const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, DB::PODArray<unsigned long, 4096ul, Allocator<false, false>, 15ul, 16ul> const*) @ 0x158dce2f in /usr/bin/clickhouse\r\n3. DB::MergeTreeDataPartWriterCompact::writeDataBlockPrimaryIndexAndSkipIndices(DB::Block const&, std::__1::vector<DB::Granule, std::__1::allocator<DB::Granule> > const&) @ 0x158d7cee in /usr/bin/clickhouse\r\n4. DB::MergeTreeDataPartWriterCompact::fillDataChecksums(DB::MergeTreeDataPartChecksums&) @ 0x158d9082 in /usr/bin/clickhouse\r\n5. DB::MergeTreeDataPartWriterCompact::fillChecksums(DB::MergeTreeDataPartChecksums&) @ 0x158d993c in /usr/bin/clickhouse\r\n6. DB::MergedBlockOutputStream::finalizePartAsync(std::__1::shared_ptr<DB::IMergeTreeDataPart>&, bool, DB::NamesAndTypesList const*, DB::MergeTreeDataPartChecksums*) @ 0x159ca896 in /usr/bin/clickhouse\r\n7. DB::MutateAllPartColumnsTask::finalize() @ 0x159efee5 in /usr/bin/clickhouse\r\n8. DB::MutateAllPartColumnsTask::executeStep() @ 0x159ee4ec in /usr/bin/clickhouse\r\n9. DB::MutatePlainMergeTreeTask::executeStep() @ 0x159d6b2e in /usr/bin/clickhouse\r\n10. DB::MergeTreeBackgroundExecutor<DB::MergeMutateRuntimeQueue>::routine(std::__1::shared_ptr<DB::TaskRuntimeData>) @ 0xa3ba51b in /usr/bin/clickhouse\r\n11. DB::MergeTreeBackgroundExecutor<DB::MergeMutateRuntimeQueue>::threadFunction() @ 0xa3b9f50 in /usr/bin/clickhouse\r\n12. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0xa4b3f86 in /usr/bin/clickhouse\r\n13. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0xa4b58d7 in /usr/bin/clickhouse\r\n14. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xa4b18a8 in /usr/bin/clickhouse\r\n15. ? @ 0xa4b4abd in /usr/bin/clickhouse\r\n16. ? @ 0x7fe2bde62609 in ?\r\n17. __clone @ 0x7fe2bdd87133 in ?\r\n (version 22.8.9.24 (official build))\r\n```\r\n\r\n**Additional context**\r\n\r\nThis seems to make the clickhouse-server run reasonably 'hot' (high CPU usage) as it's attempting the mutation in a loop. The `system.errors` counter also seems to increment really rapidly.\r\n\r\nOriginal issue: https://github.com/PostHog/posthog/issues/14200\n",
  "hints_text": "This only affects Compact parts. Any mutation will materialize all indices/projections in Compact part, which breaks certain assumption of projection calculation. Will investigate.\nThis is a common bug among both skip indices and projections. \r\n\r\n```\r\nDROP TABLE IF EXISTS test;\r\n\r\nCREATE TABLE test ( col1 Int64, dt Date ) ENGINE = MergeTree PARTITION BY dt ORDER BY tuple();\r\n\r\nINSERT INTO test FORMAT Values (1, today());\r\n\r\nALTER TABLE test ADD COLUMN col2 String;\r\n\r\nALTER TABLE test ADD INDEX i1 (col1, col2) TYPE set(100) GRANULARITY 1;\r\n\r\nALTER TABLE test MATERIALIZE INDEX i1;\r\n\r\nALTER TABLE test ADD COLUMN col3 String;\r\n\r\nALTER TABLE test DROP COLUMN col3;\r\n```\r\n\r\nThe root cause is that the mutation interpreter doesn't add column dependencies for `DROP_COLUMN` action, but when mutating compact parts, we'll always re-materialize all indices and projections. Will try to fix it soon.\nrelated https://github.com/ClickHouse/ClickHouse/issues/47549\nBad workaround:\r\n\r\nhttps://fiddle.clickhouse.com/f128dae5-be50-4251-b97d-3bb4c791dc41\r\n\r\n```\r\nselect 'optimize table test partition id \\''|| arrayJoin(part) ||'\\' final;'\r\nfrom (\r\nselect \r\n   arrayDistinct( \r\n    arrayFilter(i-> i.2 <> max(cnt), groupArray((p, cnt))).1) part\r\n   \r\nfrom (\r\nselect column, \r\n       count() cnt,\r\n       partition_id p\r\nfrom system.parts_columns \r\nwhere active and table = 'test'\r\ngroup by column, p)) format TSVRaw;\r\n```\r\n\r\nPS: this query cannot find columns with 0 partitions, need to join with system.columns, but I guess it's good enough without such join.",
  "created_at": "2023-05-22T15:32:43Z"
}