{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 80740,
  "instance_id": "ClickHouse__ClickHouse-80740",
  "issue_numbers": [
    "79535",
    "79875"
  ],
  "base_commit": "bb07aa563cc26cbabeb37a69424658f397f2ca10",
  "patch": "diff --git a/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.cpp b/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.cpp\nindex fd015e060ea2..797899cc2ac4 100644\n--- a/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.cpp\n@@ -676,7 +676,7 @@ DataTypePtr DeltaLakeMetadata::getSimpleTypeByName(const String & type_name)\n         return DataTypeFactory::instance().get(\"Bool\");\n     if (type_name == \"date\")\n         return std::make_shared<DataTypeDate32>();\n-    if (type_name == \"timestamp\")\n+    if (type_name == \"timestamp\" || type_name == \"timestamp_ntz\")\n         return std::make_shared<DataTypeDateTime64>(6);\n     if (type_name.starts_with(\"decimal(\") && type_name.ends_with(')'))\n     {\n",
  "test_patch": "diff --git a/tests/integration/test_database_delta/test.py b/tests/integration/test_database_delta/test.py\nindex 50c6441d552a..d73992a10868 100644\n--- a/tests/integration/test_database_delta/test.py\n+++ b/tests/integration/test_database_delta/test.py\n@@ -233,3 +233,61 @@ def test_complex_table_schema(started_cluster, use_delta_kernel):\n \n     if use_delta_kernel == \"1\":\n         assert node1.contains_in_log(f\"DeltaLakeMetadata: Initializing snapshot\")\n+\n+@pytest.mark.parametrize(\"use_delta_kernel\", [\"1\", \"0\"])\n+def test_timestamp_ntz(started_cluster, use_delta_kernel):\n+    node1 = started_cluster.instances[\"node1\"]\n+    node1.query(\"drop database if exists ntz_schema\")\n+\n+    schema_name = f\"schema_with_timetstamp_ntz_{use_delta_kernel}\"\n+    execute_spark_query(\n+        node1, f\"CREATE SCHEMA {schema_name}\", ignore_exit_code=True\n+    )\n+    table_name = f\"table_with_timestamp_{use_delta_kernel}\"\n+    schema = \"event_date DATE, event_time TIMESTAMP, event_time_ntz TIMESTAMP_NTZ\"\n+    create_query = f\"CREATE TABLE {schema_name}.{table_name} ({schema}) using Delta location '/tmp/ntz_schema/{table_name}'\"\n+    execute_spark_query(node1, create_query, ignore_exit_code=True)\n+    execute_spark_query(\n+        node1,\n+        f\"insert into {schema_name}.{table_name} SELECT to_date('2024-10-01', 'yyyy-MM-dd'), to_timestamp('2024-10-01 00:12:00'), to_timestamp_ntz('2024-10-01 00:12:00')\",\n+        ignore_exit_code=True,\n+    )\n+\n+    node1.query(\n+        f\"\"\"\n+drop database if exists ntz_schema;\n+create database ntz_schema\n+engine DataLakeCatalog('http://localhost:8080/api/2.1/unity-catalog')\n+settings warehouse = 'unity', catalog_type='unity', vended_credentials=false, allow_experimental_delta_kernel_rs={use_delta_kernel}\n+        \"\"\",\n+        settings={\"allow_experimental_database_unity_catalog\": \"1\"},\n+    )\n+\n+    ntz_tables = list(\n+        sorted(\n+            node1.query(\n+                f\"SHOW TABLES FROM ntz_schema LIKE '{schema_name}%'\",\n+                settings={\"use_hive_partitioning\": \"0\"},\n+            )\n+            .strip()\n+            .split(\"\\n\")\n+        )\n+    )\n+\n+    assert len(ntz_tables) == 1\n+\n+    ntz_data = (\n+        node1.query(\n+            f\"SELECT * FROM ntz_schema.`{schema_name}.{table_name}`\", settings={\"allow_experimental_delta_kernel_rs\": use_delta_kernel}\n+        )\n+        .strip()\n+        .split(\"\\t\")\n+    )\n+    print(ntz_data)\n+    assert ntz_data[0] == \"2024-10-01\"\n+    if use_delta_kernel == \"1\":\n+        assert ntz_data[1] == \"2024-10-01 00:12:00.000\" #FIXME\n+        assert ntz_data[2] == \"2024-10-01 00:12:00.000\" #FIXME\n+    else:\n+        assert ntz_data[1] == \"2024-10-01 00:12:00.000000\"\n+        assert ntz_data[2] == \"2024-10-01 00:12:00.000000\"\n",
  "problem_statement": "Unity catalog is unusable if a table contains an unsupported feature\n### Company or project name\n\n_No response_\n\n### Describe the unexpected behaviour\n\nI created a db using the DataLakeCatalog db engine pointing to a dbx Unity catalog. I created a Delta table in dbx which contained a column of type 'timestamp_ntz' which a public preview feature. \n\nIf I then run `SHOW TABLES;` in ClickHouse on my DataLakeCatalog DB, it fails:\n\n```\nUnsupported DeltaLake type: timestamp_ntz: while parsing JSON: ...etc...\n```\n\nI cannot perform any actions on the db or its tables due to this error.\n\nI would expect that an unsupported type would prevent me from using the specific table, but not prevent me from using the catalog & other tables.\n\n### How to reproduce\n\nHave a Unity catalog with a table with an unsupported feature (e.g. a datatype like `timestamp_ntz`)\n\nCREATE DATABASE unity_test ENGINE = DataLakeCatalog(...)\n\nUSE unity_test;\nSHOW TABLES;\n\n### Expected behavior\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\nDeltaLake timestamp_ntz data type unsupported\n### Company or project name\n\nI am writing telemetry data to a delta lake platform and I would like to use ClickHouse to explore this data.  \n\n\n### Describe the unexpected behaviour\n\nWhen I am trying to query my DeltaLake data, I encounter the following error: \n\n```\nCode 36 : BAD_ARGUMENTS\nUnsupported DeltaLake type: timestamp_ntz.\n```\n\nHere is the query I am using: \n```\nSELECT\n    log_id,\n    level\nFROM deltaLake('http://s3.mnm.st/data-lake/telemetry_logs/', '2z...', 'yA...')\nLIMIT 2\n```\n\n### How to reproduce\n\nI created my table with the following queries, using [Arroyo](https://www.arroyo.dev): \n\n```\nCREATE TABLE telemetry_logs (\n    log_id TEXT,\n    level TEXT,\n    logger_name TEXT,\n    message TEXT,\n    schema_version TEXT,\n    timestamp TIMESTAMP,\n    ingested_at TIMESTAMP,\n    ...\n\n) WITH (\n    -- https://doc.arroyo.dev/connectors/filesystem\n    -- https://doc.arroyo.dev/connectors/delta\n    connector = 'delta',\n    path = '...',\n    format = 'parquet',\n    parquet_compression = 'zstd',\n    'filename.strategy' = 'uuid',\n    time_partition_pattern = '%Y/%m/%d/%H'\n);\n```\n\nAnd \n\n\n```\nINSERT INTO telemetry_logs\nSELECT\n    -- Extract top-level fields using ->>\n    CAST(value ->> 'id' AS TEXT) AS log_id,\n    CAST(value ->> 'level' AS TEXT) AS level,\n    CAST(value ->> 'logger_name' AS TEXT) AS logger_name,\n    CAST(value ->> 'message' AS TEXT) AS message,\n    CAST(value ->> 'schema_version' AS TEXT) AS schema_version,\n    TO_TIMESTAMP(CAST(value ->> 'timestamp' AS DOUBLE)) AS timestamp,\n    NOW() as ingested_at,\n    ...\n\nFROM telemetry_events\nWHERE\n    CAST(value->'type' AS TEXT) = 'log';\n```\n\n### Expected behavior\n\nIdeally I would like to be able to read that type into Clickhouse. Alternatively, I would like the column to be ignored.\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\n",
  "hints_text": "Problem is not related to the unity integration but to the way we are reading delta table. \nWe have a similar issue here: https://github.com/ClickHouse/ClickHouse/issues/79535",
  "created_at": "2025-05-23T15:34:52Z",
  "modified_files": [
    "src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_database_delta/test.py"
  ]
}