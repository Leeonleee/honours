{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 43072,
  "instance_id": "ClickHouse__ClickHouse-43072",
  "issue_numbers": [
    "4510"
  ],
  "base_commit": "b30a0656e10a0d8277b4241e9aa416cf0c65025d",
  "patch": "diff --git a/src/AggregateFunctions/AggregateFunctionUniq.cpp b/src/AggregateFunctions/AggregateFunctionUniq.cpp\nindex 0d1c831c8399..1c90767131c8 100644\n--- a/src/AggregateFunctions/AggregateFunctionUniq.cpp\n+++ b/src/AggregateFunctions/AggregateFunctionUniq.cpp\n@@ -9,6 +9,7 @@\n #include <DataTypes/DataTypeTuple.h>\n #include <DataTypes/DataTypeUUID.h>\n \n+#include <Core/Settings.h>\n \n namespace DB\n {\n@@ -28,8 +29,9 @@ namespace\n /** `DataForVariadic` is a data structure that will be used for `uniq` aggregate function of multiple arguments.\n   * It differs, for example, in that it uses a trivial hash function, since `uniq` of many arguments first hashes them out itself.\n   */\n-template <typename Data, typename DataForVariadic>\n-AggregateFunctionPtr createAggregateFunctionUniq(const std::string & name, const DataTypes & argument_types, const Array & params, const Settings *)\n+template <typename Data, template <bool, bool> typename DataForVariadic>\n+AggregateFunctionPtr\n+createAggregateFunctionUniq(const std::string & name, const DataTypes & argument_types, const Array & params, const Settings *)\n {\n     assertNoParameters(name, params);\n \n@@ -61,21 +63,22 @@ AggregateFunctionPtr createAggregateFunctionUniq(const std::string & name, const\n         else if (which.isTuple())\n         {\n             if (use_exact_hash_function)\n-                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, true, true>>(argument_types);\n+                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<true, true>>>(argument_types);\n             else\n-                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, false, true>>(argument_types);\n+                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<false, true>>>(argument_types);\n         }\n     }\n \n     /// \"Variadic\" method also works as a fallback generic case for single argument.\n     if (use_exact_hash_function)\n-        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, true, false>>(argument_types);\n+        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<true, false>>>(argument_types);\n     else\n-        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, false, false>>(argument_types);\n+        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<false, false>>>(argument_types);\n }\n \n-template <bool is_exact, template <typename> class Data, typename DataForVariadic>\n-AggregateFunctionPtr createAggregateFunctionUniq(const std::string & name, const DataTypes & argument_types, const Array & params, const Settings *)\n+template <bool is_exact, template <typename, bool> typename Data, template <bool, bool, bool> typename DataForVariadic, bool is_able_to_parallelize_merge>\n+AggregateFunctionPtr\n+createAggregateFunctionUniq(const std::string & name, const DataTypes & argument_types, const Array & params, const Settings *)\n {\n     assertNoParameters(name, params);\n \n@@ -91,35 +94,35 @@ AggregateFunctionPtr createAggregateFunctionUniq(const std::string & name, const\n     {\n         const IDataType & argument_type = *argument_types[0];\n \n-        AggregateFunctionPtr res(createWithNumericType<AggregateFunctionUniq, Data>(*argument_types[0], argument_types));\n+        AggregateFunctionPtr res(createWithNumericType<AggregateFunctionUniq, Data, is_able_to_parallelize_merge>(*argument_types[0], argument_types));\n \n         WhichDataType which(argument_type);\n         if (res)\n             return res;\n         else if (which.isDate())\n-            return std::make_shared<AggregateFunctionUniq<DataTypeDate::FieldType, Data<DataTypeDate::FieldType>>>(argument_types);\n+            return std::make_shared<AggregateFunctionUniq<DataTypeDate::FieldType, Data<DataTypeDate::FieldType, is_able_to_parallelize_merge>>>(argument_types);\n         else if (which.isDate32())\n-            return std::make_shared<AggregateFunctionUniq<DataTypeDate32::FieldType, Data<DataTypeDate32::FieldType>>>(argument_types);\n+            return std::make_shared<AggregateFunctionUniq<DataTypeDate32::FieldType, Data<DataTypeDate32::FieldType, is_able_to_parallelize_merge>>>(argument_types);\n         else if (which.isDateTime())\n-            return std::make_shared<AggregateFunctionUniq<DataTypeDateTime::FieldType, Data<DataTypeDateTime::FieldType>>>(argument_types);\n+            return std::make_shared<AggregateFunctionUniq<DataTypeDateTime::FieldType, Data<DataTypeDateTime::FieldType, is_able_to_parallelize_merge>>>(argument_types);\n         else if (which.isStringOrFixedString())\n-            return std::make_shared<AggregateFunctionUniq<String, Data<String>>>(argument_types);\n+            return std::make_shared<AggregateFunctionUniq<String, Data<String, is_able_to_parallelize_merge>>>(argument_types);\n         else if (which.isUUID())\n-            return std::make_shared<AggregateFunctionUniq<DataTypeUUID::FieldType, Data<DataTypeUUID::FieldType>>>(argument_types);\n+            return std::make_shared<AggregateFunctionUniq<DataTypeUUID::FieldType, Data<DataTypeUUID::FieldType, is_able_to_parallelize_merge>>>(argument_types);\n         else if (which.isTuple())\n         {\n             if (use_exact_hash_function)\n-                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, true, true>>(argument_types);\n+                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<true, true, is_able_to_parallelize_merge>>>(argument_types);\n             else\n-                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, false, true>>(argument_types);\n+                return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<false, true, is_able_to_parallelize_merge>>>(argument_types);\n         }\n     }\n \n     /// \"Variadic\" method also works as a fallback generic case for single argument.\n     if (use_exact_hash_function)\n-        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, true, false>>(argument_types);\n+        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<true, false, is_able_to_parallelize_merge>>>(argument_types);\n     else\n-        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic, false, false>>(argument_types);\n+        return std::make_shared<AggregateFunctionUniqVariadic<DataForVariadic<false, false, is_able_to_parallelize_merge>>>(argument_types);\n }\n \n }\n@@ -132,14 +135,23 @@ void registerAggregateFunctionsUniq(AggregateFunctionFactory & factory)\n         {createAggregateFunctionUniq<AggregateFunctionUniqUniquesHashSetData, AggregateFunctionUniqUniquesHashSetDataForVariadic>, properties});\n \n     factory.registerFunction(\"uniqHLL12\",\n-        {createAggregateFunctionUniq<false, AggregateFunctionUniqHLL12Data, AggregateFunctionUniqHLL12DataForVariadic>, properties});\n+        {createAggregateFunctionUniq<false, AggregateFunctionUniqHLL12Data, AggregateFunctionUniqHLL12DataForVariadic, false /* is_able_to_parallelize_merge */>, properties});\n \n-    factory.registerFunction(\"uniqExact\",\n-        {createAggregateFunctionUniq<true, AggregateFunctionUniqExactData, AggregateFunctionUniqExactData<String>>, properties});\n+    auto assign_bool_param = [](const std::string & name, const DataTypes & argument_types, const Array & params, const Settings * settings)\n+    {\n+        /// Using two level hash set if we wouldn't be able to merge in parallel can cause ~10% slowdown.\n+        if (settings && settings->max_threads > 1)\n+            return createAggregateFunctionUniq<\n+                true, AggregateFunctionUniqExactData, AggregateFunctionUniqExactDataForVariadic, true /* is_able_to_parallelize_merge */>(name, argument_types, params, settings);\n+        else\n+            return createAggregateFunctionUniq<\n+                true, AggregateFunctionUniqExactData, AggregateFunctionUniqExactDataForVariadic, false /* is_able_to_parallelize_merge */>(name, argument_types, params, settings);\n+    };\n+    factory.registerFunction(\"uniqExact\", {assign_bool_param, properties});\n \n #if USE_DATASKETCHES\n     factory.registerFunction(\"uniqTheta\",\n-        {createAggregateFunctionUniq<AggregateFunctionUniqThetaData, AggregateFunctionUniqThetaData>, properties});\n+        {createAggregateFunctionUniq<AggregateFunctionUniqThetaData, AggregateFunctionUniqThetaDataForVariadic>, properties});\n #endif\n \n }\ndiff --git a/src/AggregateFunctions/AggregateFunctionUniq.h b/src/AggregateFunctions/AggregateFunctionUniq.h\nindex fe2530800cce..1a98bfc8456c 100644\n--- a/src/AggregateFunctions/AggregateFunctionUniq.h\n+++ b/src/AggregateFunctions/AggregateFunctionUniq.h\n@@ -1,7 +1,10 @@\n #pragma once\n \n-#include <city.h>\n+#include <atomic>\n+#include <memory>\n #include <type_traits>\n+#include <utility>\n+#include <city.h>\n \n #include <base/bit_cast.h>\n \n@@ -13,17 +16,18 @@\n \n #include <Interpreters/AggregationCommon.h>\n \n+#include <Common/CombinedCardinalityEstimator.h>\n #include <Common/HashTable/Hash.h>\n #include <Common/HashTable/HashSet.h>\n #include <Common/HyperLogLogWithSmallSetOptimization.h>\n-#include <Common/CombinedCardinalityEstimator.h>\n-#include <Common/typeid_cast.h>\n #include <Common/assert_cast.h>\n+#include <Common/typeid_cast.h>\n \n-#include <AggregateFunctions/UniquesHashSet.h>\n #include <AggregateFunctions/IAggregateFunction.h>\n #include <AggregateFunctions/ThetaSketchData.h>\n+#include <AggregateFunctions/UniqExactSet.h>\n #include <AggregateFunctions/UniqVariadicHash.h>\n+#include <AggregateFunctions/UniquesHashSet.h>\n \n \n namespace DB\n@@ -37,94 +41,128 @@ struct AggregateFunctionUniqUniquesHashSetData\n     using Set = UniquesHashSet<DefaultHash<UInt64>>;\n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = false;\n+    constexpr static bool is_variadic = false;\n+\n     static String getName() { return \"uniq\"; }\n };\n \n /// For a function that takes multiple arguments. Such a function pre-hashes them in advance, so TrivialHash is used here.\n+template <bool is_exact_, bool argument_is_tuple_>\n struct AggregateFunctionUniqUniquesHashSetDataForVariadic\n {\n     using Set = UniquesHashSet<TrivialHash>;\n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = false;\n+    constexpr static bool is_variadic = true;\n+    constexpr static bool is_exact = is_exact_;\n+    constexpr static bool argument_is_tuple = argument_is_tuple_;\n+\n     static String getName() { return \"uniq\"; }\n };\n \n \n /// uniqHLL12\n \n-template <typename T>\n+template <typename T, bool is_able_to_parallelize_merge_>\n struct AggregateFunctionUniqHLL12Data\n {\n     using Set = HyperLogLogWithSmallSetOptimization<T, 16, 12>;\n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = is_able_to_parallelize_merge_;\n+    constexpr static bool is_variadic = false;\n+\n     static String getName() { return \"uniqHLL12\"; }\n };\n \n template <>\n-struct AggregateFunctionUniqHLL12Data<String>\n+struct AggregateFunctionUniqHLL12Data<String, false>\n {\n     using Set = HyperLogLogWithSmallSetOptimization<UInt64, 16, 12>;\n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = false;\n+    constexpr static bool is_variadic = false;\n+\n     static String getName() { return \"uniqHLL12\"; }\n };\n \n template <>\n-struct AggregateFunctionUniqHLL12Data<UUID>\n+struct AggregateFunctionUniqHLL12Data<UUID, false>\n {\n     using Set = HyperLogLogWithSmallSetOptimization<UInt64, 16, 12>;\n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = false;\n+    constexpr static bool is_variadic = false;\n+\n     static String getName() { return \"uniqHLL12\"; }\n };\n \n+template <bool is_exact_, bool argument_is_tuple_, bool is_able_to_parallelize_merge_>\n struct AggregateFunctionUniqHLL12DataForVariadic\n {\n     using Set = HyperLogLogWithSmallSetOptimization<UInt64, 16, 12, TrivialHash>;\n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = is_able_to_parallelize_merge_;\n+    constexpr static bool is_variadic = true;\n+    constexpr static bool is_exact = is_exact_;\n+    constexpr static bool argument_is_tuple = argument_is_tuple_;\n+\n     static String getName() { return \"uniqHLL12\"; }\n };\n \n \n /// uniqExact\n \n-template <typename T>\n+template <typename T, bool is_able_to_parallelize_merge_>\n struct AggregateFunctionUniqExactData\n {\n     using Key = T;\n \n     /// When creating, the hash table must be small.\n-    using Set = HashSet<\n-        Key,\n-        HashCRC32<Key>,\n-        HashTableGrower<4>,\n-        HashTableAllocatorWithStackMemory<sizeof(Key) * (1 << 4)>>;\n+    using SingleLevelSet = HashSet<Key, HashCRC32<Key>, HashTableGrower<4>, HashTableAllocatorWithStackMemory<sizeof(Key) * (1 << 4)>>;\n+    using TwoLevelSet = TwoLevelHashSet<Key, HashCRC32<Key>>;\n+    using Set = UniqExactSet<SingleLevelSet, TwoLevelSet>;\n \n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = is_able_to_parallelize_merge_;\n+    constexpr static bool is_variadic = false;\n+\n     static String getName() { return \"uniqExact\"; }\n };\n \n /// For rows, we put the SipHash values (128 bits) into the hash table.\n-template <>\n-struct AggregateFunctionUniqExactData<String>\n+template <bool is_able_to_parallelize_merge_>\n+struct AggregateFunctionUniqExactData<String, is_able_to_parallelize_merge_>\n {\n     using Key = UInt128;\n \n     /// When creating, the hash table must be small.\n-    using Set = HashSet<\n-        Key,\n-        UInt128TrivialHash,\n-        HashTableGrower<3>,\n-        HashTableAllocatorWithStackMemory<sizeof(Key) * (1 << 3)>>;\n+    using SingleLevelSet = HashSet<Key, UInt128TrivialHash, HashTableGrower<3>, HashTableAllocatorWithStackMemory<sizeof(Key) * (1 << 3)>>;\n+    using TwoLevelSet = TwoLevelHashSet<Key, UInt128TrivialHash>;\n+    using Set = UniqExactSet<SingleLevelSet, TwoLevelSet>;\n \n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = is_able_to_parallelize_merge_;\n+    constexpr static bool is_variadic = false;\n+\n     static String getName() { return \"uniqExact\"; }\n };\n \n+template <bool is_exact_, bool argument_is_tuple_, bool is_able_to_parallelize_merge_>\n+struct AggregateFunctionUniqExactDataForVariadic : AggregateFunctionUniqExactData<String, is_able_to_parallelize_merge_>\n+{\n+    constexpr static bool is_able_to_parallelize_merge = is_able_to_parallelize_merge_;\n+    constexpr static bool is_variadic = true;\n+    constexpr static bool is_exact = is_exact_;\n+    constexpr static bool argument_is_tuple = argument_is_tuple_;\n+};\n \n /// uniqTheta\n #if USE_DATASKETCHES\n@@ -134,14 +172,37 @@ struct AggregateFunctionUniqThetaData\n     using Set = ThetaSketchData<UInt64>;\n     Set set;\n \n+    constexpr static bool is_able_to_parallelize_merge = false;\n+    constexpr static bool is_variadic = false;\n+\n     static String getName() { return \"uniqTheta\"; }\n };\n \n+template <bool is_exact_, bool argument_is_tuple_>\n+struct AggregateFunctionUniqThetaDataForVariadic : AggregateFunctionUniqThetaData\n+{\n+    constexpr static bool is_able_to_parallelize_merge = false;\n+    constexpr static bool is_variadic = true;\n+    constexpr static bool is_exact = is_exact_;\n+    constexpr static bool argument_is_tuple = argument_is_tuple_;\n+};\n+\n #endif\n \n namespace detail\n {\n \n+template <typename T>\n+struct IsUniqExactSet : std::false_type\n+{\n+};\n+\n+template <typename T1, typename T2>\n+struct IsUniqExactSet<UniqExactSet<T1, T2>> : std::true_type\n+{\n+};\n+\n+\n /** Hash function for uniq.\n   */\n template <typename T> struct AggregateFunctionUniqTraits\n@@ -162,17 +223,31 @@ template <typename T> struct AggregateFunctionUniqTraits\n };\n \n \n-/** The structure for the delegation work to add one element to the `uniq` aggregate functions.\n+/** The structure for the delegation work to add elements to the `uniq` aggregate functions.\n   * Used for partial specialization to add strings.\n   */\n template <typename T, typename Data>\n-struct OneAdder\n+struct Adder\n {\n-    static void ALWAYS_INLINE add(Data & data, const IColumn & column, size_t row_num)\n+    /// We have to introduce this template parameter (and a bunch of ugly code dealing with it), because we cannot\n+    /// add runtime branches in whatever_hash_set::insert - it will immediately pop up in the perf top.\n+    template <bool use_single_level_hash_table = true>\n+    static void ALWAYS_INLINE add(Data & data, const IColumn ** columns, size_t num_args, size_t row_num)\n     {\n-        if constexpr (std::is_same_v<Data, AggregateFunctionUniqUniquesHashSetData>\n-            || std::is_same_v<Data, AggregateFunctionUniqHLL12Data<T>>)\n+        if constexpr (Data::is_variadic)\n+        {\n+            if constexpr (IsUniqExactSet<typename Data::Set>::value)\n+                data.set.template insert<T, use_single_level_hash_table>(\n+                    UniqVariadicHash<Data::is_exact, Data::argument_is_tuple>::apply(num_args, columns, row_num));\n+            else\n+                data.set.insert(T{UniqVariadicHash<Data::is_exact, Data::argument_is_tuple>::apply(num_args, columns, row_num)});\n+        }\n+        else if constexpr (\n+            std::is_same_v<\n+                Data,\n+                AggregateFunctionUniqUniquesHashSetData> || std::is_same_v<Data, AggregateFunctionUniqHLL12Data<T, Data::is_able_to_parallelize_merge>>)\n         {\n+            const auto & column = *columns[0];\n             if constexpr (!std::is_same_v<T, String>)\n             {\n                 using ValueType = typename decltype(data.set)::value_type;\n@@ -185,11 +260,13 @@ struct OneAdder\n                 data.set.insert(CityHash_v1_0_2::CityHash64(value.data, value.size));\n             }\n         }\n-        else if constexpr (std::is_same_v<Data, AggregateFunctionUniqExactData<T>>)\n+        else if constexpr (std::is_same_v<Data, AggregateFunctionUniqExactData<T, Data::is_able_to_parallelize_merge>>)\n         {\n+            const auto & column = *columns[0];\n             if constexpr (!std::is_same_v<T, String>)\n             {\n-                data.set.insert(assert_cast<const ColumnVector<T> &>(column).getData()[row_num]);\n+                data.set.template insert<const T &, use_single_level_hash_table>(\n+                    assert_cast<const ColumnVector<T> &>(column).getData()[row_num]);\n             }\n             else\n             {\n@@ -200,16 +277,72 @@ struct OneAdder\n                 hash.update(value.data, value.size);\n                 hash.get128(key);\n \n-                data.set.insert(key);\n+                data.set.template insert<const UInt128 &, use_single_level_hash_table>(key);\n             }\n         }\n #if USE_DATASKETCHES\n         else if constexpr (std::is_same_v<Data, AggregateFunctionUniqThetaData>)\n         {\n+            const auto & column = *columns[0];\n             data.set.insertOriginal(column.getDataAt(row_num));\n         }\n #endif\n     }\n+\n+    static void ALWAYS_INLINE\n+    add(Data & data, const IColumn ** columns, size_t num_args, size_t row_begin, size_t row_end, const char8_t * flags, const UInt8 * null_map)\n+    {\n+        bool use_single_level_hash_table = true;\n+        if constexpr (Data::is_able_to_parallelize_merge)\n+            use_single_level_hash_table = data.set.isSingleLevel();\n+\n+        if (use_single_level_hash_table)\n+            addImpl<true>(data, columns, num_args, row_begin, row_end, flags, null_map);\n+        else\n+            addImpl<false>(data, columns, num_args, row_begin, row_end, flags, null_map);\n+\n+        if constexpr (Data::is_able_to_parallelize_merge)\n+        {\n+            if (data.set.isSingleLevel() && data.set.size() > 100'000)\n+                data.set.convertToTwoLevel();\n+        }\n+    }\n+\n+private:\n+    template <bool use_single_level_hash_table>\n+    static void ALWAYS_INLINE\n+    addImpl(Data & data, const IColumn ** columns, size_t num_args, size_t row_begin, size_t row_end, const char8_t * flags, const UInt8 * null_map)\n+    {\n+        if (!flags)\n+        {\n+            if (!null_map)\n+            {\n+                for (size_t row = row_begin; row < row_end; ++row)\n+                    add<use_single_level_hash_table>(data, columns, num_args, row);\n+            }\n+            else\n+            {\n+                for (size_t row = row_begin; row < row_end; ++row)\n+                    if (!null_map[row])\n+                        add<use_single_level_hash_table>(data, columns, num_args, row);\n+            }\n+        }\n+        else\n+        {\n+            if (!null_map)\n+            {\n+                for (size_t row = row_begin; row < row_end; ++row)\n+                    if (flags[row])\n+                        add<use_single_level_hash_table>(data, columns, num_args, row);\n+            }\n+            else\n+            {\n+                for (size_t row = row_begin; row < row_end; ++row)\n+                    if (!null_map[row] && flags[row])\n+                        add<use_single_level_hash_table>(data, columns, num_args, row);\n+            }\n+        }\n+    }\n };\n \n }\n@@ -219,9 +352,15 @@ struct OneAdder\n template <typename T, typename Data>\n class AggregateFunctionUniq final : public IAggregateFunctionDataHelper<Data, AggregateFunctionUniq<T, Data>>\n {\n+private:\n+    static constexpr size_t num_args = 1;\n+    static constexpr bool is_able_to_parallelize_merge = Data::is_able_to_parallelize_merge;\n+\n public:\n-    AggregateFunctionUniq(const DataTypes & argument_types_)\n-        : IAggregateFunctionDataHelper<Data, AggregateFunctionUniq<T, Data>>(argument_types_, {}) {}\n+    explicit AggregateFunctionUniq(const DataTypes & argument_types_)\n+        : IAggregateFunctionDataHelper<Data, AggregateFunctionUniq<T, Data>>(argument_types_, {})\n+    {\n+    }\n \n     String getName() const override { return Data::getName(); }\n \n@@ -235,7 +374,18 @@ class AggregateFunctionUniq final : public IAggregateFunctionDataHelper<Data, Ag\n     /// ALWAYS_INLINE is required to have better code layout for uniqHLL12 function\n     void ALWAYS_INLINE add(AggregateDataPtr __restrict place, const IColumn ** columns, size_t row_num, Arena *) const override\n     {\n-        detail::OneAdder<T, Data>::add(this->data(place), *columns[0], row_num);\n+        detail::Adder<T, Data>::add(this->data(place), columns, num_args, row_num);\n+    }\n+\n+    void ALWAYS_INLINE addBatchSinglePlace(\n+        size_t row_begin, size_t row_end, AggregateDataPtr __restrict place, const IColumn ** columns, Arena *, ssize_t if_argument_pos)\n+        const override\n+    {\n+        const char8_t * flags = nullptr;\n+        if (if_argument_pos >= 0)\n+            flags = assert_cast<const ColumnUInt8 &>(*columns[if_argument_pos]).getData().data();\n+\n+        detail::Adder<T, Data>::add(this->data(place), columns, num_args, row_begin, row_end, flags, nullptr /* null_map */);\n     }\n \n     void addManyDefaults(\n@@ -244,7 +394,23 @@ class AggregateFunctionUniq final : public IAggregateFunctionDataHelper<Data, Ag\n         size_t /*length*/,\n         Arena * /*arena*/) const override\n     {\n-        detail::OneAdder<T, Data>::add(this->data(place), *columns[0], 0);\n+        detail::Adder<T, Data>::add(this->data(place), columns, num_args, 0);\n+    }\n+\n+    void addBatchSinglePlaceNotNull(\n+        size_t row_begin,\n+        size_t row_end,\n+        AggregateDataPtr __restrict place,\n+        const IColumn ** columns,\n+        const UInt8 * null_map,\n+        Arena *,\n+        ssize_t if_argument_pos) const override\n+    {\n+        const char8_t * flags = nullptr;\n+        if (if_argument_pos >= 0)\n+            flags = assert_cast<const ColumnUInt8 &>(*columns[if_argument_pos]).getData().data();\n+\n+        detail::Adder<T, Data>::add(this->data(place), columns, num_args, row_begin, row_end, flags, null_map);\n     }\n \n     void merge(AggregateDataPtr __restrict place, ConstAggregateDataPtr rhs, Arena *) const override\n@@ -252,6 +418,16 @@ class AggregateFunctionUniq final : public IAggregateFunctionDataHelper<Data, Ag\n         this->data(place).set.merge(this->data(rhs).set);\n     }\n \n+    bool isAbleToParallelizeMerge() const override { return is_able_to_parallelize_merge; }\n+\n+    void merge(AggregateDataPtr __restrict place, ConstAggregateDataPtr rhs, ThreadPool & thread_pool, Arena *) const override\n+    {\n+        if constexpr (is_able_to_parallelize_merge)\n+            this->data(place).set.merge(this->data(rhs).set, &thread_pool);\n+        else\n+            this->data(place).set.merge(this->data(rhs).set);\n+    }\n+\n     void serialize(ConstAggregateDataPtr __restrict place, WriteBuffer & buf, std::optional<size_t> /* version */) const override\n     {\n         this->data(place).set.write(buf);\n@@ -273,15 +449,20 @@ class AggregateFunctionUniq final : public IAggregateFunctionDataHelper<Data, Ag\n   * You can pass multiple arguments as is; You can also pass one argument - a tuple.\n   * But (for the possibility of efficient implementation), you can not pass several arguments, among which there are tuples.\n   */\n-template <typename Data, bool is_exact, bool argument_is_tuple>\n-class AggregateFunctionUniqVariadic final : public IAggregateFunctionDataHelper<Data, AggregateFunctionUniqVariadic<Data, is_exact, argument_is_tuple>>\n+template <typename Data>\n+class AggregateFunctionUniqVariadic final : public IAggregateFunctionDataHelper<Data, AggregateFunctionUniqVariadic<Data>>\n {\n private:\n+    using T = typename Data::Set::value_type;\n+\n+    static constexpr size_t is_able_to_parallelize_merge = Data::is_able_to_parallelize_merge;\n+    static constexpr size_t argument_is_tuple = Data::argument_is_tuple;\n+\n     size_t num_args = 0;\n \n public:\n-    AggregateFunctionUniqVariadic(const DataTypes & arguments)\n-        : IAggregateFunctionDataHelper<Data, AggregateFunctionUniqVariadic<Data, is_exact, argument_is_tuple>>(arguments, {})\n+    explicit AggregateFunctionUniqVariadic(const DataTypes & arguments)\n+        : IAggregateFunctionDataHelper<Data, AggregateFunctionUniqVariadic<Data>>(arguments, {})\n     {\n         if (argument_is_tuple)\n             num_args = typeid_cast<const DataTypeTuple &>(*arguments[0]).getElements().size();\n@@ -300,8 +481,34 @@ class AggregateFunctionUniqVariadic final : public IAggregateFunctionDataHelper<\n \n     void add(AggregateDataPtr __restrict place, const IColumn ** columns, size_t row_num, Arena *) const override\n     {\n-        this->data(place).set.insert(typename Data::Set::value_type(\n-            UniqVariadicHash<is_exact, argument_is_tuple>::apply(num_args, columns, row_num)));\n+        detail::Adder<T, Data>::add(this->data(place), columns, num_args, row_num);\n+    }\n+\n+    void addBatchSinglePlace(\n+        size_t row_begin, size_t row_end, AggregateDataPtr __restrict place, const IColumn ** columns, Arena *, ssize_t if_argument_pos)\n+        const override\n+    {\n+        const char8_t * flags = nullptr;\n+        if (if_argument_pos >= 0)\n+            flags = assert_cast<const ColumnUInt8 &>(*columns[if_argument_pos]).getData().data();\n+\n+        detail::Adder<T, Data>::add(this->data(place), columns, num_args, row_begin, row_end, flags, nullptr /* null_map */);\n+    }\n+\n+    void addBatchSinglePlaceNotNull(\n+        size_t row_begin,\n+        size_t row_end,\n+        AggregateDataPtr __restrict place,\n+        const IColumn ** columns,\n+        const UInt8 * null_map,\n+        Arena *,\n+        ssize_t if_argument_pos) const override\n+    {\n+        const char8_t * flags = nullptr;\n+        if (if_argument_pos >= 0)\n+            flags = assert_cast<const ColumnUInt8 &>(*columns[if_argument_pos]).getData().data();\n+\n+        detail::Adder<T, Data>::add(this->data(place), columns, num_args, row_begin, row_end, flags, null_map);\n     }\n \n     void merge(AggregateDataPtr __restrict place, ConstAggregateDataPtr rhs, Arena *) const override\n@@ -309,6 +516,16 @@ class AggregateFunctionUniqVariadic final : public IAggregateFunctionDataHelper<\n         this->data(place).set.merge(this->data(rhs).set);\n     }\n \n+    bool isAbleToParallelizeMerge() const override { return is_able_to_parallelize_merge; }\n+\n+    void merge(AggregateDataPtr __restrict place, ConstAggregateDataPtr rhs, ThreadPool & thread_pool, Arena *) const override\n+    {\n+        if constexpr (is_able_to_parallelize_merge)\n+            this->data(place).set.merge(this->data(rhs).set, &thread_pool);\n+        else\n+            this->data(place).set.merge(this->data(rhs).set);\n+    }\n+\n     void serialize(ConstAggregateDataPtr __restrict place, WriteBuffer & buf, std::optional<size_t> /* version */) const override\n     {\n         this->data(place).set.write(buf);\ndiff --git a/src/AggregateFunctions/Helpers.h b/src/AggregateFunctions/Helpers.h\nindex 6e140f4b9cf5..c97733571a37 100644\n--- a/src/AggregateFunctions/Helpers.h\n+++ b/src/AggregateFunctions/Helpers.h\n@@ -74,6 +74,19 @@ static IAggregateFunction * createWithNumericType(const IDataType & argument_typ\n     return nullptr;\n }\n \n+template <template <typename, typename> class AggregateFunctionTemplate, template <typename, bool> class Data, bool bool_param, typename... TArgs>\n+static IAggregateFunction * createWithNumericType(const IDataType & argument_type, TArgs && ... args)\n+{\n+    WhichDataType which(argument_type);\n+#define DISPATCH(TYPE) \\\n+    if (which.idx == TypeIndex::TYPE) return new AggregateFunctionTemplate<TYPE, Data<TYPE, bool_param>>(std::forward<TArgs>(args)...); /// NOLINT\n+    FOR_NUMERIC_TYPES(DISPATCH)\n+#undef DISPATCH\n+    if (which.idx == TypeIndex::Enum8) return new AggregateFunctionTemplate<Int8, Data<Int8, bool_param>>(std::forward<TArgs>(args)...);\n+    if (which.idx == TypeIndex::Enum16) return new AggregateFunctionTemplate<Int16, Data<Int16, bool_param>>(std::forward<TArgs>(args)...);\n+    return nullptr;\n+}\n+\n template <template <typename, typename> class AggregateFunctionTemplate, template <typename> class Data, typename... TArgs>\n static IAggregateFunction * createWithUnsignedIntegerType(const IDataType & argument_type, TArgs && ... args)\n {\ndiff --git a/src/AggregateFunctions/IAggregateFunction.h b/src/AggregateFunctions/IAggregateFunction.h\nindex b3fd055b28db..ada00791e699 100644\n--- a/src/AggregateFunctions/IAggregateFunction.h\n+++ b/src/AggregateFunctions/IAggregateFunction.h\n@@ -1,14 +1,15 @@\n #pragma once\n \n+#include <Columns/ColumnSparse.h>\n #include <Columns/ColumnTuple.h>\n #include <Columns/ColumnsNumber.h>\n-#include <Columns/ColumnSparse.h>\n #include <Core/Block.h>\n #include <Core/ColumnNumbers.h>\n #include <Core/Field.h>\n #include <Interpreters/Context_fwd.h>\n-#include <Common/Exception.h>\n #include <base/types.h>\n+#include <Common/Exception.h>\n+#include <Common/ThreadPool.h>\n \n #include \"config.h\"\n \n@@ -147,6 +148,16 @@ class IAggregateFunction : public std::enable_shared_from_this<IAggregateFunctio\n     /// Merges state (on which place points to) with other state of current aggregation function.\n     virtual void merge(AggregateDataPtr __restrict place, ConstAggregateDataPtr rhs, Arena * arena) const = 0;\n \n+    /// Tells if merge() with thread pool parameter could be used.\n+    virtual bool isAbleToParallelizeMerge() const { return false; }\n+\n+    /// Should be used only if isAbleToParallelizeMerge() returned true.\n+    virtual void\n+    merge(AggregateDataPtr __restrict /*place*/, ConstAggregateDataPtr /*rhs*/, ThreadPool & /*thread_pool*/, Arena * /*arena*/) const\n+    {\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"merge() with thread pool parameter isn't implemented for {} \", getName());\n+    }\n+\n     /// Serializes state (to transmit it over the network, for example).\n     virtual void serialize(ConstAggregateDataPtr __restrict place, WriteBuffer & buf, std::optional<size_t> version = std::nullopt) const = 0; /// NOLINT\n \ndiff --git a/src/AggregateFunctions/UniqExactSet.h b/src/AggregateFunctions/UniqExactSet.h\nnew file mode 100644\nindex 000000000000..cf05d54e5414\n--- /dev/null\n+++ b/src/AggregateFunctions/UniqExactSet.h\n@@ -0,0 +1,112 @@\n+#pragma once\n+\n+#include <Common/CurrentThread.h>\n+#include <Common/HashTable/HashSet.h>\n+#include <Common/ThreadPool.h>\n+#include <Common/setThreadName.h>\n+\n+\n+namespace DB\n+{\n+\n+template <typename SingleLevelSet, typename TwoLevelSet>\n+class UniqExactSet\n+{\n+    static_assert(std::is_same_v<typename SingleLevelSet::value_type, typename TwoLevelSet::value_type>);\n+\n+public:\n+    using value_type = typename SingleLevelSet::value_type;\n+\n+    template <typename Arg, bool use_single_level_hash_table = true>\n+    auto ALWAYS_INLINE insert(Arg && arg)\n+    {\n+        if constexpr (use_single_level_hash_table)\n+            asSingleLevel().insert(std::forward<Arg>(arg));\n+        else\n+            asTwoLevel().insert(std::forward<Arg>(arg));\n+    }\n+\n+    auto merge(const UniqExactSet & other, ThreadPool * thread_pool = nullptr)\n+    {\n+        if (isSingleLevel() && other.isTwoLevel())\n+            convertToTwoLevel();\n+\n+        if (isSingleLevel())\n+        {\n+            asSingleLevel().merge(other.asSingleLevel());\n+        }\n+        else\n+        {\n+            auto & lhs = asTwoLevel();\n+            const auto rhs_ptr = other.getTwoLevelSet();\n+            const auto & rhs = *rhs_ptr;\n+            if (!thread_pool)\n+            {\n+                for (size_t i = 0; i < rhs.NUM_BUCKETS; ++i)\n+                    lhs.impls[i].merge(rhs.impls[i]);\n+            }\n+            else\n+            {\n+                auto next_bucket_to_merge = std::make_shared<std::atomic_uint32_t>(0);\n+\n+                auto thread_func = [&lhs, &rhs, next_bucket_to_merge, thread_group = CurrentThread::getGroup()]()\n+                {\n+                    if (thread_group)\n+                        CurrentThread::attachToIfDetached(thread_group);\n+                    setThreadName(\"UniqExactMerger\");\n+\n+                    while (true)\n+                    {\n+                        const auto bucket = next_bucket_to_merge->fetch_add(1);\n+                        if (bucket >= rhs.NUM_BUCKETS)\n+                            return;\n+                        lhs.impls[bucket].merge(rhs.impls[bucket]);\n+                    }\n+                };\n+\n+                for (size_t i = 0; i < std::min<size_t>(thread_pool->getMaxThreads(), rhs.NUM_BUCKETS); ++i)\n+                    thread_pool->scheduleOrThrowOnError(thread_func);\n+                thread_pool->wait();\n+            }\n+        }\n+    }\n+\n+    void read(ReadBuffer & in) { asSingleLevel().read(in); }\n+\n+    void write(WriteBuffer & out) const\n+    {\n+        if (isSingleLevel())\n+            asSingleLevel().write(out);\n+        else\n+            /// We have to preserve compatibility with the old implementation that used only single level hash sets.\n+            asTwoLevel().writeAsSingleLevel(out);\n+    }\n+\n+    size_t size() const { return isSingleLevel() ? asSingleLevel().size() : asTwoLevel().size(); }\n+\n+    /// To convert set to two level before merging (we cannot just call convertToTwoLevel() on right hand side set, because it is declared const).\n+    std::shared_ptr<TwoLevelSet> getTwoLevelSet() const\n+    {\n+        return two_level_set ? two_level_set : std::make_shared<TwoLevelSet>(asSingleLevel());\n+    }\n+\n+    void convertToTwoLevel()\n+    {\n+        two_level_set = getTwoLevelSet();\n+        single_level_set.clear();\n+    }\n+\n+    bool isSingleLevel() const { return !two_level_set; }\n+    bool isTwoLevel() const { return !!two_level_set; }\n+\n+private:\n+    SingleLevelSet & asSingleLevel() { return single_level_set; }\n+    const SingleLevelSet & asSingleLevel() const { return single_level_set; }\n+\n+    TwoLevelSet & asTwoLevel() { return *two_level_set; }\n+    const TwoLevelSet & asTwoLevel() const { return *two_level_set; }\n+\n+    SingleLevelSet single_level_set;\n+    std::shared_ptr<TwoLevelSet> two_level_set;\n+};\n+}\ndiff --git a/src/AggregateFunctions/UniquesHashSet.h b/src/AggregateFunctions/UniquesHashSet.h\nindex 777ec0edc7e8..075b0897c3ac 100644\n--- a/src/AggregateFunctions/UniquesHashSet.h\n+++ b/src/AggregateFunctions/UniquesHashSet.h\n@@ -329,7 +329,7 @@ class UniquesHashSet : private HashTableAllocatorWithStackMemory<(1ULL << UNIQUE\n         free();\n     }\n \n-    void insert(Value x)\n+    void ALWAYS_INLINE insert(Value x)\n     {\n         HashValue hash_value = hash(x);\n         if (!good(hash_value))\ndiff --git a/src/Common/HashTable/HashSet.h b/src/Common/HashTable/HashSet.h\nindex 279ab1673476..be4be078ee8e 100644\n--- a/src/Common/HashTable/HashSet.h\n+++ b/src/Common/HashTable/HashSet.h\n@@ -3,6 +3,7 @@\n #include <Common/HashTable/Hash.h>\n #include <Common/HashTable/HashTable.h>\n #include <Common/HashTable/HashTableAllocator.h>\n+#include <Common/HashTable/TwoLevelHashTable.h>\n \n #include <IO/WriteBuffer.h>\n #include <IO/WriteHelpers.h>\n@@ -10,6 +11,14 @@\n #include <IO/ReadHelpers.h>\n #include <IO/VarInt.h>\n \n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+}\n+\n /** NOTE HashSet could only be used for memmoveable (position independent) types.\n   * Example: std::string is not position independent in libstdc++ with C++11 ABI or in libc++.\n   * Also, key must be of type, that zero bytes is compared equals to zero key.\n@@ -64,6 +73,47 @@ class HashSetTable : public HashTable<Key, TCell, Hash, Grower, Allocator>\n };\n \n \n+template <\n+    typename Key,\n+    typename TCell, /// Supposed to have no state (HashTableNoState)\n+    typename Hash = DefaultHash<Key>,\n+    typename Grower = TwoLevelHashTableGrower<>,\n+    typename Allocator = HashTableAllocator>\n+class TwoLevelHashSetTable\n+    : public TwoLevelHashTable<Key, TCell, Hash, Grower, Allocator, HashSetTable<Key, TCell, Hash, Grower, Allocator>>\n+{\n+public:\n+    using Self = TwoLevelHashSetTable;\n+    using Base = TwoLevelHashTable<Key, TCell, Hash, Grower, Allocator, HashSetTable<Key, TCell, Hash, Grower, Allocator>>;\n+\n+    using Base::Base;\n+\n+    /// Writes its content in a way that it will be correctly read by HashSetTable.\n+    /// Used by uniqExact to preserve backward compatibility.\n+    void writeAsSingleLevel(DB::WriteBuffer & wb) const\n+    {\n+        DB::writeVarUInt(this->size(), wb);\n+\n+        bool zero_written = false;\n+        for (size_t i = 0; i < Base::NUM_BUCKETS; ++i)\n+        {\n+            if (this->impls[i].hasZero())\n+            {\n+                if (zero_written)\n+                    throw DB::Exception(DB::ErrorCodes::LOGICAL_ERROR, \"No more than one zero value expected\");\n+                this->impls[i].zeroValue()->write(wb);\n+                zero_written = true;\n+            }\n+        }\n+\n+        static constexpr HashTableNoState state;\n+        for (auto ptr = this->begin(); ptr != this->end(); ++ptr)\n+            if (!ptr.getPtr()->isZero(state))\n+                ptr.getPtr()->write(wb);\n+    }\n+};\n+\n+\n template <typename Key, typename Hash, typename TState = HashTableNoState>\n struct HashSetCellWithSavedHash : public HashTableCell<Key, Hash, TState>\n {\n@@ -89,6 +139,13 @@ template <\n     typename Allocator = HashTableAllocator>\n using HashSet = HashSetTable<Key, HashTableCell<Key, Hash>, Hash, Grower, Allocator>;\n \n+template <\n+    typename Key,\n+    typename Hash = DefaultHash<Key>,\n+    typename Grower = TwoLevelHashTableGrower<>,\n+    typename Allocator = HashTableAllocator>\n+using TwoLevelHashSet = TwoLevelHashSetTable<Key, HashTableCell<Key, Hash>, Hash, Grower, Allocator>;\n+\n template <typename Key, typename Hash, size_t initial_size_degree>\n using HashSetWithStackMemory = HashSet<\n     Key,\ndiff --git a/src/Common/HashTable/HashTable.h b/src/Common/HashTable/HashTable.h\nindex 7aa375cfa79b..837647cb8bd3 100644\n--- a/src/Common/HashTable/HashTable.h\n+++ b/src/Common/HashTable/HashTable.h\n@@ -432,20 +432,12 @@ struct AllocatorBufferDeleter<true, Allocator, Cell>\n \n \n // The HashTable\n-template\n-<\n-    typename Key,\n-    typename Cell,\n-    typename Hash,\n-    typename Grower,\n-    typename Allocator\n->\n-class HashTable :\n-    private boost::noncopyable,\n-    protected Hash,\n-    protected Allocator,\n-    protected Cell::State,\n-    protected ZeroValueStorage<Cell::need_zero_value_storage, Cell>     /// empty base optimization\n+template <typename Key, typename Cell, typename Hash, typename Grower, typename Allocator>\n+class HashTable : private boost::noncopyable,\n+                  protected Hash,\n+                  protected Allocator,\n+                  protected Cell::State,\n+                  public ZeroValueStorage<Cell::need_zero_value_storage, Cell> /// empty base optimization\n {\n public:\n     // If we use an allocator with inline memory, check that the initial\ndiff --git a/src/Common/HashTable/TwoLevelHashTable.h b/src/Common/HashTable/TwoLevelHashTable.h\nindex 5acc8b191952..bd4c4c366f22 100644\n--- a/src/Common/HashTable/TwoLevelHashTable.h\n+++ b/src/Common/HashTable/TwoLevelHashTable.h\n@@ -159,14 +159,16 @@ class TwoLevelHashTable :\n \n     class const_iterator /// NOLINT\n     {\n-        Self * container{};\n+        const Self * container{};\n         size_t bucket{};\n         typename Impl::const_iterator current_it{};\n \n         friend class TwoLevelHashTable;\n \n-        const_iterator(Self * container_, size_t bucket_, typename Impl::const_iterator current_it_)\n-            : container(container_), bucket(bucket_), current_it(current_it_) {}\n+        const_iterator(const Self * container_, size_t bucket_, typename Impl::const_iterator current_it_)\n+            : container(container_), bucket(bucket_), current_it(current_it_)\n+        {\n+        }\n \n     public:\n         const_iterator() = default;\ndiff --git a/src/Common/examples/small_table.cpp b/src/Common/examples/small_table.cpp\nindex ca38516d09a5..0e8a419e13da 100644\n--- a/src/Common/examples/small_table.cpp\n+++ b/src/Common/examples/small_table.cpp\n@@ -27,7 +27,7 @@ int main(int, char **)\n             std::cerr << x.getValue() << std::endl;\n \n         DB::WriteBufferFromOwnString wb;\n-        cont.writeText(wb);\n+        cont.write(wb);\n \n         std::cerr << \"dump: \" << wb.str() << std::endl;\n     }\ndiff --git a/src/Interpreters/Aggregator.cpp b/src/Interpreters/Aggregator.cpp\nindex c38006af9751..95fa5ed543d1 100644\n--- a/src/Interpreters/Aggregator.cpp\n+++ b/src/Interpreters/Aggregator.cpp\n@@ -2508,6 +2508,8 @@ void NO_INLINE Aggregator::mergeDataOnlyExistingKeysImpl(\n void NO_INLINE Aggregator::mergeWithoutKeyDataImpl(\n     ManyAggregatedDataVariants & non_empty_data) const\n {\n+    ThreadPool thread_pool{params.max_threads};\n+\n     AggregatedDataVariantsPtr & res = non_empty_data[0];\n \n     /// We merge all aggregation results to the first.\n@@ -2517,7 +2519,15 @@ void NO_INLINE Aggregator::mergeWithoutKeyDataImpl(\n         AggregatedDataWithoutKey & current_data = non_empty_data[result_num]->without_key;\n \n         for (size_t i = 0; i < params.aggregates_size; ++i)\n-            aggregate_functions[i]->merge(res_data + offsets_of_aggregate_states[i], current_data + offsets_of_aggregate_states[i], res->aggregates_pool);\n+            if (aggregate_functions[i]->isAbleToParallelizeMerge())\n+                aggregate_functions[i]->merge(\n+                    res_data + offsets_of_aggregate_states[i],\n+                    current_data + offsets_of_aggregate_states[i],\n+                    thread_pool,\n+                    res->aggregates_pool);\n+            else\n+                aggregate_functions[i]->merge(\n+                    res_data + offsets_of_aggregate_states[i], current_data + offsets_of_aggregate_states[i], res->aggregates_pool);\n \n         for (size_t i = 0; i < params.aggregates_size; ++i)\n             aggregate_functions[i]->destroy(current_data + offsets_of_aggregate_states[i]);\n",
  "test_patch": "diff --git a/src/Common/tests/gtest_hash_table.cpp b/src/Common/tests/gtest_hash_table.cpp\nindex fd0b2495fdee..0221a682577b 100644\n--- a/src/Common/tests/gtest_hash_table.cpp\n+++ b/src/Common/tests/gtest_hash_table.cpp\n@@ -15,6 +15,17 @@\n \n using namespace DB;\n \n+namespace\n+{\n+std::vector<UInt64> getVectorWithNumbersUpToN(size_t n)\n+{\n+    std::vector<UInt64> res(n);\n+    std::iota(res.begin(), res.end(), 0);\n+    return res;\n+}\n+\n+}\n+\n \n /// To test dump functionality without using other hashes that can change\n template <typename T>\n@@ -371,3 +382,48 @@ TEST(HashTable, Resize)\n         ASSERT_EQ(actual, expected);\n     }\n }\n+\n+\n+using HashSetContent = std::vector<UInt64>;\n+\n+class TwoLevelHashSetFixture : public ::testing::TestWithParam<HashSetContent>\n+{\n+};\n+\n+\n+TEST_P(TwoLevelHashSetFixture, WriteAsSingleLevel)\n+{\n+    using Key = UInt64;\n+\n+    {\n+        const auto & hash_set_content = GetParam();\n+\n+        TwoLevelHashSet<Key, HashCRC32<Key>> two_level;\n+        for (const auto & elem : hash_set_content)\n+            two_level.insert(elem);\n+\n+        WriteBufferFromOwnString wb;\n+        two_level.writeAsSingleLevel(wb);\n+\n+        ReadBufferFromString rb(wb.str());\n+        HashSet<Key, HashCRC32<Key>> single_level;\n+        single_level.read(rb);\n+\n+        EXPECT_EQ(single_level.size(), hash_set_content.size());\n+        for (const auto & elem : hash_set_content)\n+            EXPECT_NE(single_level.find(elem), nullptr);\n+    }\n+}\n+\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    TwoLevelHashSetTests,\n+    TwoLevelHashSetFixture,\n+    ::testing::Values(\n+        HashSetContent{},\n+        getVectorWithNumbersUpToN(1),\n+        getVectorWithNumbersUpToN(100),\n+        getVectorWithNumbersUpToN(1000),\n+        getVectorWithNumbersUpToN(10000),\n+        getVectorWithNumbersUpToN(100000),\n+        getVectorWithNumbersUpToN(1000000)));\ndiff --git a/tests/integration/test_backward_compatibility/test_aggregate_function_state.py b/tests/integration/test_backward_compatibility/test_aggregate_function_state.py\nnew file mode 100644\nindex 000000000000..1f6d405603a6\n--- /dev/null\n+++ b/tests/integration/test_backward_compatibility/test_aggregate_function_state.py\n@@ -0,0 +1,228 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance(\n+    \"node1\",\n+    with_zookeeper=False,\n+    image=\"yandex/clickhouse-server\",\n+    tag=\"19.16.9.37\",\n+    stay_alive=True,\n+    with_installed_binary=True,\n+)\n+node2 = cluster.add_instance(\n+    \"node2\",\n+    with_zookeeper=False,\n+    image=\"yandex/clickhouse-server\",\n+    tag=\"19.16.9.37\",\n+    stay_alive=True,\n+    with_installed_binary=True,\n+)\n+node3 = cluster.add_instance(\"node3\", with_zookeeper=False)\n+node4 = cluster.add_instance(\"node4\", with_zookeeper=False)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+# We will test that serialization of internal state of \"avg\" function is compatible between different versions.\n+# TODO Implement versioning of serialization format for aggregate function states.\n+# NOTE This test is too ad-hoc.\n+\n+\n+def test_backward_compatability_for_avg(start_cluster):\n+    node1.query(\"create table tab (x UInt64) engine = Memory\")\n+    node2.query(\"create table tab (x UInt64) engine = Memory\")\n+    node3.query(\"create table tab (x UInt64) engine = Memory\")\n+    node4.query(\"create table tab (x UInt64) engine = Memory\")\n+\n+    node1.query(\"INSERT INTO tab VALUES (1)\")\n+    node2.query(\"INSERT INTO tab VALUES (2)\")\n+    node3.query(\"INSERT INTO tab VALUES (3)\")\n+    node4.query(\"INSERT INTO tab VALUES (4)\")\n+\n+    assert (\n+        node1.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n+    )\n+    assert (\n+        node2.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n+    )\n+    assert (\n+        node3.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n+    )\n+    assert (\n+        node4.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n+    )\n+\n+    # Also check with persisted aggregate function state\n+\n+    node1.query(\"create table state (x AggregateFunction(avg, UInt64)) engine = Log\")\n+    node1.query(\n+        \"INSERT INTO state SELECT avgState(arrayJoin(CAST([1, 2, 3, 4] AS Array(UInt64))))\"\n+    )\n+\n+    assert node1.query(\"SELECT avgMerge(x) FROM state\") == \"2.5\\n\"\n+\n+    node1.restart_with_latest_version(fix_metadata=True)\n+\n+    assert node1.query(\"SELECT avgMerge(x) FROM state\") == \"2.5\\n\"\n+\n+    node1.query(\"drop table tab\")\n+    node1.query(\"drop table state\")\n+    node2.query(\"drop table tab\")\n+    node3.query(\"drop table tab\")\n+    node4.query(\"drop table tab\")\n+\n+\n+@pytest.mark.parametrize(\"uniq_keys\", [1000, 500000])\n+def test_backward_compatability_for_uniq_exact(start_cluster, uniq_keys):\n+    node1.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64) Engine = Memory\")\n+    node2.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64) Engine = Memory\")\n+    node3.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64) Engine = Memory\")\n+    node4.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64) Engine = Memory\")\n+\n+    node1.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number FROM numbers_mt(0, {uniq_keys})\"\n+    )\n+    node2.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number FROM numbers_mt(1, {uniq_keys})\"\n+    )\n+    node3.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number FROM numbers_mt(2, {uniq_keys})\"\n+    )\n+    node4.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number FROM numbers_mt(3, {uniq_keys})\"\n+    )\n+\n+    assert (\n+        node1.query(\n+            f\"SELECT uniqExact(x) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+    assert (\n+        node2.query(\n+            f\"SELECT uniqExact(x) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+    assert (\n+        node3.query(\n+            f\"SELECT uniqExact(x) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+    assert (\n+        node4.query(\n+            f\"SELECT uniqExact(x) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+\n+    # Also check with persisted aggregate function state\n+\n+    node1.query(\n+        f\"CREATE TABLE state_{uniq_keys} (x AggregateFunction(uniqExact, UInt64)) Engine = Log\"\n+    )\n+    node1.query(\n+        f\"INSERT INTO state_{uniq_keys} SELECT uniqExactState(number) FROM numbers_mt({uniq_keys})\"\n+    )\n+\n+    assert (\n+        node1.query(f\"SELECT uniqExactMerge(x) FROM state_{uniq_keys}\")\n+        == f\"{uniq_keys}\\n\"\n+    )\n+\n+    node1.restart_with_latest_version()\n+\n+    assert (\n+        node1.query(f\"SELECT uniqExactMerge(x) FROM state_{uniq_keys}\")\n+        == f\"{uniq_keys}\\n\"\n+    )\n+\n+    node1.query(f\"DROP TABLE state_{uniq_keys}\")\n+    node1.query(f\"DROP TABLE tab_{uniq_keys}\")\n+    node2.query(f\"DROP TABLE tab_{uniq_keys}\")\n+    node3.query(f\"DROP TABLE tab_{uniq_keys}\")\n+    node4.query(f\"DROP TABLE tab_{uniq_keys}\")\n+\n+\n+@pytest.mark.parametrize(\"uniq_keys\", [1000, 500000])\n+def test_backward_compatability_for_uniq_exact_variadic(start_cluster, uniq_keys):\n+    node1.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64, y UInt64) Engine = Memory\")\n+    node2.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64, y UInt64) Engine = Memory\")\n+    node3.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64, y UInt64) Engine = Memory\")\n+    node4.query(f\"CREATE TABLE tab_{uniq_keys} (x UInt64, y UInt64) Engine = Memory\")\n+\n+    node1.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number, number/2 FROM numbers_mt(0, {uniq_keys})\"\n+    )\n+    node2.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number, number/2 FROM numbers_mt(1, {uniq_keys})\"\n+    )\n+    node3.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number, number/2 FROM numbers_mt(2, {uniq_keys})\"\n+    )\n+    node4.query(\n+        f\"INSERT INTO tab_{uniq_keys} SELECT number, number/2 FROM numbers_mt(3, {uniq_keys})\"\n+    )\n+\n+    assert (\n+        node1.query(\n+            f\"SELECT uniqExact(x, y) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+    assert (\n+        node2.query(\n+            f\"SELECT uniqExact(x, y) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+    assert (\n+        node3.query(\n+            f\"SELECT uniqExact(x, y) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+    assert (\n+        node4.query(\n+            f\"SELECT uniqExact(x, y) FROM remote('node{{1..4}}', default, tab_{uniq_keys})\"\n+        )\n+        == f\"{uniq_keys + 3}\\n\"\n+    )\n+\n+    # Also check with persisted aggregate function state\n+\n+    node1.query(\n+        f\"CREATE TABLE state_{uniq_keys} (x AggregateFunction(uniqExact, UInt64, UInt64)) Engine = Log\"\n+    )\n+    node1.query(\n+        f\"INSERT INTO state_{uniq_keys} SELECT uniqExactState(number, intDiv(number,2)) FROM numbers_mt({uniq_keys})\"\n+    )\n+\n+    assert (\n+        node1.query(f\"SELECT uniqExactMerge(x) FROM state_{uniq_keys}\")\n+        == f\"{uniq_keys}\\n\"\n+    )\n+\n+    node1.restart_with_latest_version()\n+\n+    assert (\n+        node1.query(f\"SELECT uniqExactMerge(x) FROM state_{uniq_keys}\")\n+        == f\"{uniq_keys}\\n\"\n+    )\n+\n+    node1.query(f\"DROP TABLE state_{uniq_keys}\")\n+    node1.query(f\"DROP TABLE tab_{uniq_keys}\")\n+    node2.query(f\"DROP TABLE tab_{uniq_keys}\")\n+    node3.query(f\"DROP TABLE tab_{uniq_keys}\")\n+    node4.query(f\"DROP TABLE tab_{uniq_keys}\")\ndiff --git a/tests/integration/test_backward_compatibility/test_aggregate_function_state_avg.py b/tests/integration/test_backward_compatibility/test_aggregate_function_state_avg.py\ndeleted file mode 100644\nindex 1e54e6220d7a..000000000000\n--- a/tests/integration/test_backward_compatibility/test_aggregate_function_state_avg.py\n+++ /dev/null\n@@ -1,82 +0,0 @@\n-import pytest\n-\n-from helpers.cluster import ClickHouseCluster\n-\n-cluster = ClickHouseCluster(__file__)\n-node1 = cluster.add_instance(\n-    \"node1\",\n-    with_zookeeper=False,\n-    image=\"yandex/clickhouse-server\",\n-    tag=\"19.16.9.37\",\n-    stay_alive=True,\n-    with_installed_binary=True,\n-)\n-node2 = cluster.add_instance(\n-    \"node2\",\n-    with_zookeeper=False,\n-    image=\"yandex/clickhouse-server\",\n-    tag=\"19.16.9.37\",\n-    stay_alive=True,\n-    with_installed_binary=True,\n-)\n-node3 = cluster.add_instance(\"node3\", with_zookeeper=False)\n-node4 = cluster.add_instance(\"node4\", with_zookeeper=False)\n-\n-\n-@pytest.fixture(scope=\"module\")\n-def start_cluster():\n-    try:\n-        cluster.start()\n-        yield cluster\n-\n-    finally:\n-        cluster.shutdown()\n-\n-\n-# We will test that serialization of internal state of \"avg\" function is compatible between different versions.\n-# TODO Implement versioning of serialization format for aggregate function states.\n-# NOTE This test is too ad-hoc.\n-\n-\n-def test_backward_compatability(start_cluster):\n-    node1.query(\"create table tab (x UInt64) engine = Memory\")\n-    node2.query(\"create table tab (x UInt64) engine = Memory\")\n-    node3.query(\"create table tab (x UInt64) engine = Memory\")\n-    node4.query(\"create table tab (x UInt64) engine = Memory\")\n-\n-    node1.query(\"INSERT INTO tab VALUES (1)\")\n-    node2.query(\"INSERT INTO tab VALUES (2)\")\n-    node3.query(\"INSERT INTO tab VALUES (3)\")\n-    node4.query(\"INSERT INTO tab VALUES (4)\")\n-\n-    assert (\n-        node1.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n-    )\n-    assert (\n-        node2.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n-    )\n-    assert (\n-        node3.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n-    )\n-    assert (\n-        node4.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == \"2.5\\n\"\n-    )\n-\n-    # Also check with persisted aggregate function state\n-\n-    node1.query(\"create table state (x AggregateFunction(avg, UInt64)) engine = Log\")\n-    node1.query(\n-        \"INSERT INTO state SELECT avgState(arrayJoin(CAST([1, 2, 3, 4] AS Array(UInt64))))\"\n-    )\n-\n-    assert node1.query(\"SELECT avgMerge(x) FROM state\") == \"2.5\\n\"\n-\n-    node1.restart_with_latest_version(fix_metadata=True)\n-\n-    assert node1.query(\"SELECT avgMerge(x) FROM state\") == \"2.5\\n\"\n-\n-    node1.query(\"drop table tab\")\n-    node1.query(\"drop table state\")\n-    node2.query(\"drop table tab\")\n-    node3.query(\"drop table tab\")\n-    node4.query(\"drop table tab\")\ndiff --git a/tests/performance/uniq_without_key.xml b/tests/performance/uniq_without_key.xml\nnew file mode 100644\nindex 000000000000..4394aef7889e\n--- /dev/null\n+++ b/tests/performance/uniq_without_key.xml\n@@ -0,0 +1,33 @@\n+<test>\n+    <substitutions>\n+        <substitution>\n+           <name>uniq_keys</name>\n+           <values>\n+               <value>10000</value>\n+               <value>50000</value>\n+               <value>100000</value>\n+               <value>250000</value>\n+               <value>500000</value>\n+               <value>1000000</value>\n+           </values>\n+        </substitution>\n+    </substitutions>\n+\n+    <create_query>create table t_{uniq_keys}(a UInt64) engine=MergeTree order by tuple()</create_query>\n+\n+    <fill_query>insert into t_{uniq_keys} select number % {uniq_keys} from numbers_mt(5e7)</fill_query>\n+\n+    <query>SELECT count(distinct a) FROM t_{uniq_keys} GROUP BY a FORMAT Null</query>\n+    <query>SELECT uniqExact(a) FROM t_{uniq_keys} GROUP BY a FORMAT Null</query>\n+\n+    <query>SELECT count(distinct a) FROM t_{uniq_keys}</query>\n+    <query>SELECT uniqExact(a) FROM t_{uniq_keys}</query>\n+\n+    <query>SELECT uniqExact(number) from numbers_mt(1e7)</query>\n+    <query>SELECT uniqExact(number) from numbers_mt(5e7)</query>\n+\n+    <query>SELECT uniqExact(number, number) from numbers_mt(5e6)</query>\n+    <query>SELECT uniqExact(number, number) from numbers_mt(1e7)</query>\n+\n+    <drop_query>drop table t_{uniq_keys}</drop_query>\n+</test>\n",
  "problem_statement": "`uniqExact` (= `distinct count`) is slow\nI'm comparing ClickHouse with a commercial columnar database. It's amazing that in some cases ClickHouse beats the commercial product even with less resources(4 cores vs. 20 cores in my case). However, I noticed uniqExact(or distinct count) is unreasonably slow.\r\n\r\nHere is my findings(using dockerized ClickHouse 19.3.4) and hopefully they can be of use:\r\n* assume we have a table with a few hundred millions rows(mine: ~600,000,000 rows) like below\r\n```sql\r\ncreate table test_distinct_count(a String, b String, b Int32, d String, ...)\r\nEngine=MergeTree() order by (a, b, c, d) settings index_granularity = 8192;\r\n```\r\n\r\n* we'll likely get the following test results on a VM with 4 cores(max_threads=4)\r\n```sql\r\nselect a from test_distinct_count group by a -- 1,100+ rows returned in 5.252s\r\nselect distinct a from test_distinct_count -- 1,100+ rows returned in 5.912s\r\nselect uniq(a) from test_distinct_count -- 1 row returned in 7.483s\r\nselect uniqExact(a) from test_distinct_count -- 1 row returned in 12.308s\r\nselect count(1) from (select distinct a from test_distinct_count) x -- 1 row returned in 5.821s\r\nselect count(1) from (select a from test_distinct_count group by a) x -- 1 row returned in 4.937s\r\n```\r\n\r\nIt looks like using `group by` and avoiding `distinct` bring us better performance, but it's really inconvenient to replace all `distinct count` with the trick...\r\n\r\nUpdate: same issue on 19.3.5\n",
  "hints_text": "Hi,\r\nOne accidental run on complex installation means nothing. Could you make reliable test, please?\r\n\r\n1. Run each query 100-1000 times and place the time into texts file (with one column of time in milliseconds)\r\n2. Use ministat tool to compare the distributions.\nuniqExact is always the worst.\r\n\r\nConnected to ClickHouse server version 19.3.4\r\n\r\ncase1. create table u_perf(a Int64, v String) Engine=MergeTree order by a;\r\ncase2. create table u_perf(a Int64, v String) Engine=MergeTree order by v;\r\ncase3. create table u_perf(a Int64, v LowCardinality(String)) Engine=MergeTree order by a;\r\ncase4. create table u_perf(a Int64, v LowCardinality(String)) Engine=MergeTree order by v;\r\n\r\ninsert into u_perf select number, toString(number % 21277) from numbers(100000000);\r\nsleep(1);\r\noptimize table u_perf final;\r\n\r\n\r\n\r\nsql | case1 | case2 (sorted) | case3 (LC) | case4 LC(sorted)\r\n-- | -- | -- | -- | --\r\nselect count() from (select v from u_perf group by v); | 1.092 sec.  1.096 sec. | 0.362 sec.  0.354 sec. | 1.694 sec.  1.640 sec. | 0.090 sec.  0.105 sec.\r\nselect count() from (select distinct v from u_perf); | 0.651 sec.  0.761 sec. | 0.400 sec.  0.356 sec. | 2.250 sec.  2.164 sec. | 0.879 sec.  0.746 sec.\r\nselect uniq(v) from u_perf; | 0.541 sec.  0.595 sec. | 0.473 sec.  0.552 sec. | 1.851 sec.  1.747 sec. | 0.451 sec.  0.392 sec.\r\nselect uniqExact(v) from u_perf; | 1.325 sec.  1.304 sec. | 0.759 sec.  0.867 sec. | 2.341 sec.  2.661 sec. | 0.678 sec.  0.817 sec.\r\n\r\n\r\n\nWhen you calculate a single value of aggregate function, the merging phase is not parallelized. (It is parallelized for different aggregate function states.)\r\n\r\nE.g. if you calculate multiple small-medium uniqExacts in a single query (with GROUP BY), it will be fast, but when you calculate just a single uniqExact, it is slow.\r\n\r\nThis issue is yet to be addressed.",
  "created_at": "2022-11-09T00:03:27Z",
  "modified_files": [
    "src/AggregateFunctions/AggregateFunctionUniq.cpp",
    "src/AggregateFunctions/AggregateFunctionUniq.h",
    "src/AggregateFunctions/Helpers.h",
    "src/AggregateFunctions/IAggregateFunction.h",
    "b/src/AggregateFunctions/UniqExactSet.h",
    "src/AggregateFunctions/UniquesHashSet.h",
    "src/Common/HashTable/HashSet.h",
    "src/Common/HashTable/HashTable.h",
    "src/Common/HashTable/TwoLevelHashTable.h",
    "src/Common/examples/small_table.cpp",
    "src/Interpreters/Aggregator.cpp"
  ],
  "modified_test_files": [
    "src/Common/tests/gtest_hash_table.cpp",
    "b/tests/integration/test_backward_compatibility/test_aggregate_function_state.py",
    "tests/integration/test_backward_compatibility/test_aggregate_function_state_avg.py",
    "b/tests/performance/uniq_without_key.xml"
  ]
}