{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 6523,
  "instance_id": "ClickHouse__ClickHouse-6523",
  "issue_numbers": [
    "6522"
  ],
  "base_commit": "e4dda4332ef2a9ed7e87b6c681dd2cdd74c534a4",
  "patch": "diff --git a/dbms/src/Storages/StorageReplicatedMergeTree.cpp b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\nindex 8b32cc327049..3781b9efea50 100644\n--- a/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -1956,10 +1956,37 @@ void StorageReplicatedMergeTree::cloneReplica(const String & source_replica, Coo\n     }\n \n     /// Add to the queue jobs to receive all the active parts that the reference/master replica has.\n-    Strings parts = zookeeper->getChildren(source_path + \"/parts\");\n-    ActiveDataPartSet active_parts_set(format_version, parts);\n+    Strings source_replica_parts = zookeeper->getChildren(source_path + \"/parts\");\n+    ActiveDataPartSet active_parts_set(format_version, source_replica_parts);\n \n     Strings active_parts = active_parts_set.getParts();\n+\n+    /// Remove local parts if source replica does not have them, because such parts will never be fetched by other replicas.\n+    Strings local_parts_in_zk = zookeeper->getChildren(replica_path + \"/parts\");\n+    Strings parts_to_remove_from_zk;\n+    for (const auto & part : local_parts_in_zk)\n+    {\n+        if (active_parts_set.getContainingPart(part).empty())\n+        {\n+            queue.remove(zookeeper, part);\n+            parts_to_remove_from_zk.emplace_back(part);\n+            LOG_WARNING(log, \"Source replica does not have part \" << part << \". Removing it from ZooKeeper.\");\n+        }\n+    }\n+    tryRemovePartsFromZooKeeperWithRetries(parts_to_remove_from_zk);\n+\n+    auto local_active_parts = getDataParts();\n+    DataPartsVector parts_to_remove_from_working_set;\n+    for (const auto & part : local_active_parts)\n+    {\n+        if (active_parts_set.getContainingPart(part->name).empty())\n+        {\n+            parts_to_remove_from_working_set.emplace_back(part);\n+            LOG_WARNING(log, \"Source replica does not have part \" << part->name << \". Removing it from working set.\");\n+        }\n+    }\n+    removePartsFromWorkingSet(parts_to_remove_from_working_set, true);\n+\n     for (const String & name : active_parts)\n     {\n         LogEntry log_entry;\n",
  "test_patch": "diff --git a/dbms/tests/integration/test_consistent_parts_after_clone_replica/__init__.py b/dbms/tests/integration/test_consistent_parts_after_clone_replica/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/dbms/tests/integration/test_consistent_parts_after_clone_replica/configs/remote_servers.xml b/dbms/tests/integration/test_consistent_parts_after_clone_replica/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..a6e80ce2b08a\n--- /dev/null\n+++ b/dbms/tests/integration/test_consistent_parts_after_clone_replica/configs/remote_servers.xml\n@@ -0,0 +1,19 @@\n+<yandex>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <default_database>shard_0</default_database>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <default_database>shard_0</default_database>\n+                    <host>node2</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+</yandex>\ndiff --git a/dbms/tests/integration/test_consistent_parts_after_clone_replica/test.py b/dbms/tests/integration/test_consistent_parts_after_clone_replica/test.py\nnew file mode 100644\nindex 000000000000..b8a58242ad11\n--- /dev/null\n+++ b/dbms/tests/integration/test_consistent_parts_after_clone_replica/test.py\n@@ -0,0 +1,60 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.network import PartitionManager\n+from helpers.test_tools import assert_eq_with_retry\n+\n+\n+def fill_nodes(nodes, shard):\n+    for node in nodes:\n+        node.query(\n+        '''\n+        CREATE DATABASE test;\n+        CREATE TABLE test_table(date Date, id UInt32)\n+        ENGINE = ReplicatedMergeTree('/clickhouse/tables/test{shard}/replicated', '{replica}')\n+        ORDER BY id PARTITION BY toYYYYMM(date) \n+        SETTINGS min_replicated_logs_to_keep=3, max_replicated_logs_to_keep=5, cleanup_delay_period=0, cleanup_delay_period_random_add=0;\n+        '''.format(shard=shard, replica=node.name))\n+\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('node1', main_configs=['configs/remote_servers.xml'], with_zookeeper=True)\n+node2 = cluster.add_instance('node2', main_configs=['configs/remote_servers.xml'], with_zookeeper=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        fill_nodes([node1, node2], 1)\n+        yield cluster\n+    except Exception as ex:\n+        print ex\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_inconsistent_parts_if_drop_while_replica_not_active(start_cluster):\n+    with PartitionManager() as pm:\n+        # insert into all replicas\n+        for i in range(50):\n+            node1.query(\"INSERT INTO test_table VALUES ('2019-08-16', {})\".format(i))\n+        assert_eq_with_retry(node2, \"SELECT count(*) FROM test_table\", node1.query(\"SELECT count(*) FROM test_table\"))\n+\n+        # disable network on the first replica\n+        pm.partition_instances(node1, node2)\n+        pm.drop_instance_zk_connections(node1)\n+\n+        # drop all parts on the second replica\n+        node2.query_with_retry(\"ALTER TABLE test_table DROP PARTITION 201908\")\n+        assert_eq_with_retry(node2, \"SELECT count(*) FROM test_table\", \"0\")\n+\n+        # insert into the second replica\n+        # DROP_RANGE will be removed from the replication log and the first replica will be lost\n+        for i in range(50):\n+            node2.query(\"INSERT INTO test_table VALUES ('2019-08-16', {})\".format(50 + i))\n+\n+        # the first replica will be cloned from the second\n+        pm.heal_all()\n+        assert_eq_with_retry(node1, \"SELECT count(*) FROM test_table\", node2.query(\"SELECT count(*) FROM test_table\"))\n+\n+\n",
  "problem_statement": "Inconsistent parts after DROP PARTITION and outdated replica restore\n**Describe the bug**\r\nIf active replicas have executed `DROP PARTITION` while some replica was lost, the set of data parts will be inconsistent after recovery of lost replica.\r\n\r\n**How to reproduce**\r\nSee test in the related PR.\r\n\r\n**Expected behavior**\r\nConsistent set of parts on all replicas.\r\n\n",
  "hints_text": "",
  "created_at": "2019-08-16T16:27:01Z"
}