{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 77318,
  "instance_id": "ClickHouse__ClickHouse-77318",
  "issue_numbers": [
    "47412"
  ],
  "base_commit": "60b7ab4d9840994badc9e0771a9ffb0db9a43d30",
  "patch": "diff --git a/src/Databases/Iceberg/DatabaseIceberg.cpp b/src/Databases/Iceberg/DatabaseIceberg.cpp\nindex 9906ace90dd7..eea667f4ade6 100644\n--- a/src/Databases/Iceberg/DatabaseIceberg.cpp\n+++ b/src/Databases/Iceberg/DatabaseIceberg.cpp\n@@ -225,11 +225,11 @@ StoragePtr DatabaseIceberg::tryGetTable(const String & name, ContextPtr context_\n         storage_type = table_metadata.getStorageType();\n \n     const auto configuration = getConfiguration(storage_type);\n-    auto storage_settings = std::make_unique<StorageObjectStorageSettings>();\n+    auto storage_settings = std::make_shared<StorageObjectStorageSettings>();\n \n     /// with_table_structure = false: because there will be\n     /// no table structure in table definition AST.\n-    StorageObjectStorage::Configuration::initialize(*configuration, args, context_, /* with_table_structure */false, storage_settings.get());\n+    StorageObjectStorage::Configuration::initialize(*configuration, args, context_, /* with_table_structure */false, storage_settings);\n \n     return std::make_shared<StorageObjectStorage>(\n         configuration,\ndiff --git a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\nindex 253ae9102726..15360bf7d0db 100644\n--- a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\n+++ b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\n@@ -10,6 +10,7 @@\n #include <Storages/ObjectStorage/Local/Configuration.h>\n #include <Storages/ObjectStorage/S3/Configuration.h>\n #include <Storages/ObjectStorage/StorageObjectStorage.h>\n+#include <Storages/ObjectStorage/StorageObjectStorageSettings.h>\n #include <Storages/StorageFactory.h>\n #include <Common/logger_useful.h>\n #include \"Storages/ColumnsDescription.h\"\n@@ -31,6 +32,12 @@ namespace ErrorCodes\n extern const int FORMAT_VERSION_TOO_OLD;\n }\n \n+namespace StorageObjectStorageSetting\n+{\n+extern const StorageObjectStorageSettingsBool allow_dynamic_metadata_for_data_lakes;\n+}\n+\n+\n template <typename T>\n concept StorageConfiguration = std::derived_from<T, StorageObjectStorage::Configuration>;\n \n@@ -98,7 +105,8 @@ class DataLakeConfiguration : public BaseStorageConfiguration, public std::enabl\n \n     bool hasExternalDynamicMetadata() override\n     {\n-        return StorageObjectStorage::Configuration::allow_dynamic_metadata_for_data_lakes && current_metadata\n+        return BaseStorageConfiguration::getSettingsRef()[StorageObjectStorageSetting::allow_dynamic_metadata_for_data_lakes]\n+            && current_metadata\n             && current_metadata->supportsExternalMetadataChange();\n     }\n \n@@ -160,7 +168,7 @@ class DataLakeConfiguration : public BaseStorageConfiguration, public std::enabl\n             current_metadata = DataLakeMetadata::create(\n                 object_storage,\n                 weak_from_this(),\n-                local_context, BaseStorageConfiguration::allow_experimental_delta_kernel_rs);\n+                local_context);\n         }\n         auto read_schema = current_metadata->getReadSchema();\n         if (!read_schema.empty())\n@@ -219,8 +227,7 @@ class DataLakeConfiguration : public BaseStorageConfiguration, public std::enabl\n             current_metadata = DataLakeMetadata::create(\n                 object_storage,\n                 weak_from_this(),\n-                context,\n-                BaseStorageConfiguration::allow_experimental_delta_kernel_rs);\n+                context);\n             return true;\n         }\n \n@@ -232,8 +239,7 @@ class DataLakeConfiguration : public BaseStorageConfiguration, public std::enabl\n         auto new_metadata = DataLakeMetadata::create(\n             object_storage,\n             weak_from_this(),\n-            context,\n-            BaseStorageConfiguration::allow_experimental_delta_kernel_rs);\n+            context);\n \n         if (*current_metadata != *new_metadata)\n         {\ndiff --git a/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.h b/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.h\nindex b2f519781e9d..cf811df2eca3 100644\n--- a/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.h\n+++ b/src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.h\n@@ -56,11 +56,11 @@ class DeltaLakeMetadata final : public IDataLakeMetadata\n     static DataLakeMetadataPtr create(\n         ObjectStoragePtr object_storage,\n         ConfigurationObserverPtr configuration,\n-        ContextPtr local_context,\n-        [[maybe_unused]] bool allow_experimental_delta_kernel_rs)\n+        ContextPtr local_context)\n     {\n #if USE_DELTA_KERNEL_RS\n-        if (allow_experimental_delta_kernel_rs)\n+        auto configuration_ptr = configuration.lock();\n+        if (configuration_ptr->getSettingsRef()[StorageObjectStorageSetting::allow_experimental_delta_kernel_rs])\n             return std::make_unique<DeltaLakeMetadataDeltaKernel>(object_storage, configuration, local_context);\n         else\n             return std::make_unique<DeltaLakeMetadata>(object_storage, configuration, local_context);\ndiff --git a/src/Storages/ObjectStorage/DataLakes/HudiMetadata.h b/src/Storages/ObjectStorage/DataLakes/HudiMetadata.h\nindex c5c81b583026..46291d9e6d96 100644\n--- a/src/Storages/ObjectStorage/DataLakes/HudiMetadata.h\n+++ b/src/Storages/ObjectStorage/DataLakes/HudiMetadata.h\n@@ -34,8 +34,7 @@ class HudiMetadata final : public IDataLakeMetadata, private WithContext\n     static DataLakeMetadataPtr create(\n         ObjectStoragePtr object_storage,\n         ConfigurationObserverPtr configuration,\n-        ContextPtr local_context,\n-        bool)\n+        ContextPtr local_context)\n     {\n         return std::make_unique<HudiMetadata>(object_storage, configuration, local_context);\n     }\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\nindex 91cf6d57ede2..c48ab9f077bd 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\n@@ -16,14 +16,15 @@\n #include <Processors/Formats/Impl/AvroRowInputFormat.h>\n #include <Storages/ObjectStorage/DataLakes/Common.h>\n #include <Storages/ObjectStorage/StorageObjectStorageSource.h>\n+#include <Storages/ObjectStorage/StorageObjectStorageSettings.h>\n #include <Common/logger_useful.h>\n #include <Interpreters/ExpressionActions.h>\n \n-#include \"Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h\"\n-#include \"Storages/ObjectStorage/DataLakes/Iceberg/Utils.h\"\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n \n-#include \"Storages/ObjectStorage/DataLakes/Iceberg/ManifestFileImpl.h\"\n-#include \"Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h\"\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFileImpl.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h>\n \n #include <Common/ProfileEvents.h>\n \n@@ -35,6 +36,11 @@ extern const Event IcebergPartitionPrunnedFiles;\n namespace DB\n {\n \n+namespace StorageObjectStorageSetting\n+{\n+extern const StorageObjectStorageSettingsString iceberg_metadata_file_path;\n+}\n+\n namespace ErrorCodes\n {\n extern const int FILE_DOESNT_EXIST;\n@@ -186,14 +192,32 @@ Int32 IcebergMetadata::parseTableSchema(\n     }\n }\n \n+static std::pair<Int32, String> getMetadataFileAndVersion(const std::string & path)\n+{\n+    String file_name(path.begin() + path.find_last_of('/') + 1, path.end());\n+    String version_str;\n+    /// v<V>.metadata.json\n+    if (file_name.starts_with('v'))\n+        version_str = String(file_name.begin() + 1, file_name.begin() + file_name.find_first_of('.'));\n+    /// <V>-<random-uuid>.metadata.json\n+    else\n+        version_str = String(file_name.begin(), file_name.begin() + file_name.find_first_of('-'));\n+\n+    if (!std::all_of(version_str.begin(), version_str.end(), isdigit))\n+        throw Exception(\n+            ErrorCodes::BAD_ARGUMENTS, \"Bad metadata file name: {}. Expected vN.metadata.json where N is a number\", file_name);\n+\n+    return std::make_pair(std::stoi(version_str), path);\n+}\n+\n /**\n  * Each version of table metadata is stored in a `metadata` directory and\n  * has one of 2 formats:\n  *   1) v<V>.metadata.json, where V - metadata version.\n  *   2) <V>-<random-uuid>.metadata.json, where V - metadata version\n  */\n-std::pair<Int32, String>\n-getMetadataFileAndVersion(const ObjectStoragePtr & object_storage, const StorageObjectStorage::Configuration & configuration)\n+static std::pair<Int32, String>\n+getLatestMetadataFileAndVersion(const ObjectStoragePtr & object_storage, const StorageObjectStorage::Configuration & configuration)\n {\n     const auto metadata_files = listFiles(*object_storage, configuration, \"metadata\", \".metadata.json\");\n     if (metadata_files.empty())\n@@ -201,30 +225,37 @@ getMetadataFileAndVersion(const ObjectStoragePtr & object_storage, const Storage\n         throw Exception(\n             ErrorCodes::FILE_DOESNT_EXIST, \"The metadata file for Iceberg table with path {} doesn't exist\", configuration.getPath());\n     }\n-\n     std::vector<std::pair<UInt32, String>> metadata_files_with_versions;\n     metadata_files_with_versions.reserve(metadata_files.size());\n     for (const auto & path : metadata_files)\n     {\n-        String file_name(path.begin() + path.find_last_of('/') + 1, path.end());\n-        String version_str;\n-        /// v<V>.metadata.json\n-        if (file_name.starts_with('v'))\n-            version_str = String(file_name.begin() + 1, file_name.begin() + file_name.find_first_of('.'));\n-        /// <V>-<random-uuid>.metadata.json\n-        else\n-            version_str = String(file_name.begin(), file_name.begin() + file_name.find_first_of('-'));\n-\n-        if (!std::all_of(version_str.begin(), version_str.end(), isdigit))\n-            throw Exception(\n-                ErrorCodes::BAD_ARGUMENTS, \"Bad metadata file name: {}. Expected vN.metadata.json where N is a number\", file_name);\n-        metadata_files_with_versions.emplace_back(std::stoi(version_str), path);\n+        metadata_files_with_versions.emplace_back(getMetadataFileAndVersion(path));\n     }\n \n     /// Get the latest version of metadata file: v<V>.metadata.json\n     return *std::max_element(metadata_files_with_versions.begin(), metadata_files_with_versions.end());\n }\n \n+static std::pair<Int32, String> getLatestOrExplicitMetadataFileAndVersion(const ObjectStoragePtr & object_storage, const StorageObjectStorage::Configuration & configuration)\n+{\n+    auto explicit_metadata_path = configuration.getSettingsRef()[StorageObjectStorageSetting::iceberg_metadata_file_path].value;\n+    std::pair<Int32, String> result;\n+    if (!explicit_metadata_path.empty())\n+    {\n+        auto prefix_storage_path = configuration.getPath();\n+        if (!explicit_metadata_path.starts_with(prefix_storage_path))\n+            explicit_metadata_path = std::filesystem::path(prefix_storage_path) / explicit_metadata_path;\n+        result = getMetadataFileAndVersion(explicit_metadata_path);\n+    }\n+    else\n+    {\n+        result = getLatestMetadataFileAndVersion(object_storage, configuration);\n+    }\n+\n+    return result;\n+}\n+\n+\n Poco::JSON::Object::Ptr IcebergMetadata::readJSON(const String & metadata_file_path, const ContextPtr & local_context) const\n {\n     ObjectInfo object_info(metadata_file_path);\n@@ -242,7 +273,7 @@ bool IcebergMetadata::update(const ContextPtr & local_context)\n {\n     auto configuration_ptr = configuration.lock();\n \n-    const auto [metadata_version, metadata_file_path] = getMetadataFileAndVersion(object_storage, *configuration_ptr);\n+    const auto [metadata_version, metadata_file_path] = getLatestOrExplicitMetadataFileAndVersion(object_storage, *configuration_ptr);\n \n     if (metadata_version == current_metadata_version)\n         return false;\n@@ -304,12 +335,11 @@ std::optional<Int32> IcebergMetadata::getSchemaVersionByFileIfOutdated(String da\n DataLakeMetadataPtr IcebergMetadata::create(\n     const ObjectStoragePtr & object_storage,\n     const ConfigurationObserverPtr & configuration,\n-    const ContextPtr & local_context,\n-    bool)\n+    const ContextPtr & local_context)\n {\n     auto configuration_ptr = configuration.lock();\n \n-    const auto [metadata_version, metadata_file_path] = getMetadataFileAndVersion(object_storage, *configuration_ptr);\n+    const auto [metadata_version, metadata_file_path] = getLatestOrExplicitMetadataFileAndVersion(object_storage, *configuration_ptr);\n \n     auto log = getLogger(\"IcebergMetadata\");\n \ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h\nindex d82d0817d900..c252ad636fa0 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h\n@@ -56,8 +56,7 @@ class IcebergMetadata : public IDataLakeMetadata, private WithContext\n     static DataLakeMetadataPtr create(\n         const ObjectStoragePtr & object_storage,\n         const ConfigurationObserverPtr & configuration,\n-        const ContextPtr & local_context,\n-        bool allow_experimental_delta_kernel_rs);\n+        const ContextPtr & local_context);\n \n     size_t getVersion() const { return current_metadata_version; }\n \ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorage.cpp b/src/Storages/ObjectStorage/StorageObjectStorage.cpp\nindex d5d75c23902a..229a6e47b545 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorage.cpp\n+++ b/src/Storages/ObjectStorage/StorageObjectStorage.cpp\n@@ -47,12 +47,6 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n }\n \n-namespace StorageObjectStorageSetting\n-{\n-extern const StorageObjectStorageSettingsBool allow_dynamic_metadata_for_data_lakes;\n-extern const StorageObjectStorageSettingsBool allow_experimental_delta_kernel_rs;\n-}\n-\n String StorageObjectStorage::getPathSample(ContextPtr context)\n {\n     auto query_settings = configuration->getQuerySettings(context);\n@@ -585,42 +579,41 @@ SchemaCache & StorageObjectStorage::getSchemaCache(const ContextPtr & context, c\n }\n \n void StorageObjectStorage::Configuration::initialize(\n-    Configuration & configuration,\n+    Configuration & configuration_to_initialize,\n     ASTs & engine_args,\n     ContextPtr local_context,\n     bool with_table_structure,\n-    StorageObjectStorageSettings * settings)\n+    StorageObjectStorageSettingsPtr settings)\n {\n     if (auto named_collection = tryGetNamedCollectionWithOverrides(engine_args, local_context))\n-        configuration.fromNamedCollection(*named_collection, local_context);\n+        configuration_to_initialize.fromNamedCollection(*named_collection, local_context);\n     else\n-        configuration.fromAST(engine_args, local_context, with_table_structure);\n+        configuration_to_initialize.fromAST(engine_args, local_context, with_table_structure);\n \n-    if (configuration.format == \"auto\")\n+    if (configuration_to_initialize.format == \"auto\")\n     {\n-        if (configuration.isDataLakeConfiguration())\n+        if (configuration_to_initialize.isDataLakeConfiguration())\n         {\n-            configuration.format = \"Parquet\";\n+            configuration_to_initialize.format = \"Parquet\";\n         }\n         else\n         {\n-            configuration.format\n+            configuration_to_initialize.format\n                 = FormatFactory::instance()\n-                      .tryGetFormatFromFileName(configuration.isArchive() ? configuration.getPathInArchive() : configuration.getPath())\n+                      .tryGetFormatFromFileName(configuration_to_initialize.isArchive() ? configuration_to_initialize.getPathInArchive() : configuration_to_initialize.getPath())\n                       .value_or(\"auto\");\n         }\n     }\n     else\n-        FormatFactory::instance().checkFormatName(configuration.format);\n+        FormatFactory::instance().checkFormatName(configuration_to_initialize.format);\n \n-    if (settings)\n-    {\n-        configuration.allow_dynamic_metadata_for_data_lakes\n-            = (*settings)[StorageObjectStorageSetting::allow_dynamic_metadata_for_data_lakes];\n-        configuration.allow_experimental_delta_kernel_rs\n-            = (*settings)[StorageObjectStorageSetting::allow_experimental_delta_kernel_rs];\n-    }\n-    configuration.initialized = true;\n+    configuration_to_initialize.storage_settings = settings;\n+    configuration_to_initialize.initialized = true;\n+}\n+\n+const StorageObjectStorageSettings & StorageObjectStorage::Configuration::getSettingsRef() const\n+{\n+    return *storage_settings;\n }\n \n void StorageObjectStorage::Configuration::check(ContextPtr) const\ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorage.h b/src/Storages/ObjectStorage/StorageObjectStorage.h\nindex 5332d5d7d6ef..440774d30d5c 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorage.h\n+++ b/src/Storages/ObjectStorage/StorageObjectStorage.h\n@@ -18,6 +18,7 @@ class ReadBufferIterator;\n class SchemaCache;\n class NamedCollection;\n struct StorageObjectStorageSettings;\n+using StorageObjectStorageSettingsPtr = std::shared_ptr<StorageObjectStorageSettings>;\n \n namespace ErrorCodes\n {\n@@ -168,11 +169,11 @@ class StorageObjectStorage::Configuration\n     using Paths = std::vector<Path>;\n \n     static void initialize(\n-        Configuration & configuration,\n+        Configuration & configuration_to_initialize,\n         ASTs & engine_args,\n         ContextPtr local_context,\n         bool with_table_structure,\n-        StorageObjectStorageSettings * settings);\n+        StorageObjectStorageSettingsPtr settings);\n \n     /// Storage type: s3, hdfs, azure, local.\n     virtual ObjectStorageType getType() const = 0;\n@@ -253,6 +254,7 @@ class StorageObjectStorage::Configuration\n \n     virtual void update(ObjectStoragePtr object_storage, ContextPtr local_context);\n \n+    const StorageObjectStorageSettings & getSettingsRef() const;\n \n protected:\n     virtual void fromNamedCollection(const NamedCollection & collection, ContextPtr context) = 0;\n@@ -262,8 +264,7 @@ class StorageObjectStorage::Configuration\n \n     bool initialized = false;\n \n-    bool allow_dynamic_metadata_for_data_lakes = false;\n-    bool allow_experimental_delta_kernel_rs = false;\n+    StorageObjectStorageSettingsPtr storage_settings;\n };\n \n }\ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorageSettings.cpp b/src/Storages/ObjectStorage/StorageObjectStorageSettings.cpp\nindex 3edf1efccdf9..f59914cae3f3 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorageSettings.cpp\n+++ b/src/Storages/ObjectStorage/StorageObjectStorageSettings.cpp\n@@ -19,6 +19,9 @@ If enabled, indicates that metadata is taken from iceberg specification that is\n )\", 0) \\\n     DECLARE(Bool, allow_experimental_delta_kernel_rs, false, R\"(\n If enabled, the engine would use delta-kernel-rs for DeltaLake metadata parsing\n+)\", 0) \\\n+    DECLARE(String, iceberg_metadata_file_path, \"\", R\"(\n+Explicit path to desired Iceberg metadata file, should be relative to path in object storage. Make sense for table function use case only.\n )\", 0) \\\n \n // clang-format on\ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorageSettings.h b/src/Storages/ObjectStorage/StorageObjectStorageSettings.h\nindex 0fad318a9ac7..d3f72aa2f8a3 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorageSettings.h\n+++ b/src/Storages/ObjectStorage/StorageObjectStorageSettings.h\n@@ -64,4 +64,6 @@ struct StorageObjectStorageSettings\n     std::unique_ptr<StorageObjectStorageSettingsImpl> impl;\n };\n \n+using StorageObjectStorageSettingsPtr = std::shared_ptr<StorageObjectStorageSettings>;\n+\n }\ndiff --git a/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp b/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp\nindex 60fc17d9b6d4..f62b9cae37f0 100644\n--- a/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp\n+++ b/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp\n@@ -34,12 +34,12 @@ createStorageObjectStorage(const StorageFactory::Arguments & args, StorageObject\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"External data source must have arguments\");\n \n     const auto context = args.getLocalContext();\n-    auto queue_settings = std::make_unique<StorageObjectStorageSettings>();\n+    auto storage_settings = std::make_shared<StorageObjectStorageSettings>();\n \n     if (args.storage_def->settings)\n-        queue_settings->loadFromQuery(*args.storage_def->settings);\n+        storage_settings->loadFromQuery(*args.storage_def->settings);\n \n-    StorageObjectStorage::Configuration::initialize(*configuration, args.engine_args, context, false, queue_settings.get());\n+    StorageObjectStorage::Configuration::initialize(*configuration, args.engine_args, context, false, storage_settings);\n \n     // Use format settings from global server context + settings from\n     // the SETTINGS clause of the create query. Settings from current\ndiff --git a/src/TableFunctions/TableFunctionObjectStorage.cpp b/src/TableFunctions/TableFunctionObjectStorage.cpp\nindex d2db40285338..04e1aa585d77 100644\n--- a/src/TableFunctions/TableFunctionObjectStorage.cpp\n+++ b/src/TableFunctions/TableFunctionObjectStorage.cpp\n@@ -73,13 +73,15 @@ void TableFunctionObjectStorage<Definition, Configuration>::parseArguments(const\n     if (args_func.size() != 1)\n         throw Exception(ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH, \"Table function '{}' must have arguments.\", getName());\n \n+    settings = std::make_shared<StorageObjectStorageSettings>();\n+\n     auto & args = args_func.at(0)->children;\n     for (auto * it = args.begin(); it != args.end(); ++it)\n     {\n         ASTSetQuery * settings_ast = (*it)->as<ASTSetQuery>();\n         if (settings_ast)\n         {\n-            settings.loadFromQuery(*settings_ast);\n+            settings->loadFromQuery(*settings_ast);\n             args.erase(it);\n             break;\n         }\ndiff --git a/src/TableFunctions/TableFunctionObjectStorage.h b/src/TableFunctions/TableFunctionObjectStorage.h\nindex 413cb5360c31..4e3158d78c33 100644\n--- a/src/TableFunctions/TableFunctionObjectStorage.h\n+++ b/src/TableFunctions/TableFunctionObjectStorage.h\n@@ -131,7 +131,7 @@ class TableFunctionObjectStorage : public ITableFunction\n \n     virtual void parseArgumentsImpl(ASTs & args, const ContextPtr & context)\n     {\n-        StorageObjectStorage::Configuration::initialize(*getConfiguration(), args, context, true, &settings);\n+        StorageObjectStorage::Configuration::initialize(*getConfiguration(), args, context, true, settings);\n     }\n \n     static void updateStructureAndFormatArgumentsIfNeeded(\n@@ -164,7 +164,7 @@ class TableFunctionObjectStorage : public ITableFunction\n     mutable ConfigurationPtr configuration;\n     mutable ObjectStoragePtr object_storage;\n     ColumnsDescription structure_hint;\n-    StorageObjectStorageSettings settings;\n+    std::shared_ptr<StorageObjectStorageSettings> settings;\n \n     std::vector<size_t> skipAnalysisForArguments(const QueryTreeNodePtr & query_node_table_function, ContextPtr context) const override;\n };\n",
  "test_patch": "diff --git a/tests/integration/test_storage_iceberg/test.py b/tests/integration/test_storage_iceberg/test.py\nindex 440f83adb38d..29a29c7694f1 100644\n--- a/tests/integration/test_storage_iceberg/test.py\n+++ b/tests/integration/test_storage_iceberg/test.py\n@@ -201,13 +201,21 @@ def get_creation_expression(\n     table_function=False,\n     allow_dynamic_metadata_for_data_lakes=False,\n     run_on_cluster=False,\n+    explicit_metadata_path=\"\",\n     **kwargs,\n ):\n-    allow_dynamic_metadata_for_datalakes_suffix = (\n-        \" SETTINGS allow_dynamic_metadata_for_data_lakes = 1\"\n-        if allow_dynamic_metadata_for_data_lakes\n-        else \"\"\n-    )\n+\n+    settings_array = []\n+    if allow_dynamic_metadata_for_data_lakes:\n+        settings_array.append(\"allow_dynamic_metadata_for_data_lakes = 1\")\n+\n+    if explicit_metadata_path:\n+        settings_array.append(f\"iceberg_metadata_file_path = '{explicit_metadata_path}'\")\n+\n+    if settings_array:\n+        settings_expression = \" SETTINGS \" + \",\".join(settings_array)\n+    else:\n+        settings_expression = \"\"\n \n     if storage_type == \"s3\":\n         if \"bucket\" in kwargs:\n@@ -227,7 +235,7 @@ def get_creation_expression(\n                     DROP TABLE IF EXISTS {table_name};\n                     CREATE TABLE {table_name}\n                     ENGINE=IcebergS3(s3, filename = 'iceberg_data/default/{table_name}/', format={format}, url = 'http://minio1:9001/{bucket}/')\"\"\"\n-                    + allow_dynamic_metadata_for_datalakes_suffix\n+                    + settings_expression\n                 )\n \n     elif storage_type == \"azure\":\n@@ -247,7 +255,7 @@ def get_creation_expression(\n                     DROP TABLE IF EXISTS {table_name};\n                     CREATE TABLE {table_name}\n                     ENGINE=IcebergAzure(azure, container = {cluster.azure_container_name}, storage_account_url = '{cluster.env_variables[\"AZURITE_STORAGE_ACCOUNT_URL\"]}', blob_path = '/iceberg_data/default/{table_name}/', format={format})\"\"\"\n-                    + allow_dynamic_metadata_for_datalakes_suffix\n+                    + settings_expression\n                 )\n \n     elif storage_type == \"local\":\n@@ -263,7 +271,7 @@ def get_creation_expression(\n                 DROP TABLE IF EXISTS {table_name};\n                 CREATE TABLE {table_name}\n                 ENGINE=IcebergLocal(local, path = '/iceberg_data/default/{table_name}/', format={format})\"\"\"\n-                + allow_dynamic_metadata_for_datalakes_suffix\n+                + settings_expression\n             )\n \n     else:\n@@ -2061,3 +2069,42 @@ def check_validity_and_get_prunned_files(select_expression):\n         )\n         == 1\n     )\n+\n+@pytest.mark.parametrize(\"storage_type\", [\"s3\", \"azure\", \"local\"])\n+def test_explicit_metadata_file(started_cluster, storage_type):\n+    instance = started_cluster.instances[\"node1\"]\n+    spark = started_cluster.spark_session\n+    TABLE_NAME = (\n+        \"test_explicit_metadata_file_\"\n+        + storage_type\n+        + \"_\"\n+        + get_uuid_str()\n+    )\n+\n+    spark.sql(\n+        f\"CREATE TABLE {TABLE_NAME} (id bigint, data string) USING iceberg TBLPROPERTIES ('format-version' = '2', 'write.update.mode'='merge-on-read', 'write.delete.mode'='merge-on-read', 'write.merge.mode'='merge-on-read')\"\n+    )\n+\n+    for i in range(50):\n+        spark.sql(\n+            f\"INSERT INTO {TABLE_NAME} select id, char(id + ascii('a')) from range(10)\"\n+        )\n+\n+    default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+    )\n+\n+    create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, explicit_metadata_path=\"\")\n+\n+    assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 500\n+\n+    create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, explicit_metadata_path=\"metadata/v31.metadata.json\")\n+\n+    assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 300\n+\n+    create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, explicit_metadata_path=\"metadata/v11.metadata.json\")\n+\n+    assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 100\n",
  "problem_statement": "Allow Iceberg table engine to point to a specific metadata file\nThe current implementation of the Iceberg table engine takes a URL with the path to an existing Iceberg table. The example given in the documentation is `http://test.s3.amazonaws.com/clickhouse-bucket/`. The engine calls `S3DataLakeMetadataReadHelper::listFilesMatchSuffix` to list all `*metadata.json` files in the `metadata` directory (this directory name is hard-coded) under this base path, and then picks the metadata file that sorts last in lexicographical order.\r\n\r\nThere are a few problems with this implementation:\r\n\r\n* The name of the metadata directory is unnecessarily hard-coded. It would be better if the `url` parameter directly specified the path to the metadata directory, not its parent directory. Unfortunately, changing this would break compatibility for current users of the Iceberg table engine.\r\n* Listing the metadata files involves one or more S3 `ListObjectsV2` calls. This is slow when the `metadata` directory contains many files, which is often the case for Iceberg tables in the real world.\r\n* The user is not given the option to choose a specific metadata file, which specifies the `current-snapshot-id` for an immutable snapshot of the table. It is not possible to `CREATE TABLE` from a specific version of an Iceberg table that is not the latest version. \n",
  "hints_text": "To add on, Iceberg can have separate metadata storage and data storage. i.e. store metadata on HDFS with a HDFS catalog and actual data on S3. Therefore I think that Iceberg storage engine can be enhanced with different implementations of metadata storage (from HMS/HDFS/Glue). \r\nFYI Doris https://doris.apache.org/docs/dev/lakehouse/multi-catalog/iceberg\nTo add on:\r\n> The engine calls S3DataLakeMetadataReadHelper::listFilesMatchSuffix to list all *metadata.json files in the metadata directory \r\n\r\nIn our case, metadata is avro format instead of json. So this hard coded suffix can't fetch the metadata. \nAnd one more point: metadata files are written before the actually commit in iceberg.\r\nIt's possible that latest metadata file is pointing to uncommitted state and transaction that created this file will be rolled back.\nVery much looking forward to this feature as it would decouple catalog and the data of the table. That way we can easily implement Glue catalog integration (or any other), but just querying it explicitly ourselves for metadata file location and just pass it as arg to the `iceberg()` function.\nSo to summarize the thread:\n\nWe should soon have a new setting, that will be able to read from a specific snapshot ID: https://github.com/ClickHouse/ClickHouse/pull/77032\n\nIn addition to the above the catalog integration, should address the problem of when the data are stored in a different place than the metadata. \n\nSomething probably missing is a function to list metadata from an Iceberg table to help with usability.  It should list the metadata of your tables, like snapshots, manifest files, etc... \n\nSo most of this issue should be covered soon. Am I missing something? \n\n@melvynator It's a different way of \"time travel\" implementation. Explicit Snapshot ID is highlevel way to do it (engine will have to use latest metadata file and find appropriate data file), and explicit metadata file path is more low level \"please use this metadata file and latest snapshot ID\" ",
  "created_at": "2025-03-07T16:43:20Z",
  "modified_files": [
    "src/Databases/Iceberg/DatabaseIceberg.cpp",
    "src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h",
    "src/Storages/ObjectStorage/DataLakes/DeltaLakeMetadata.h",
    "src/Storages/ObjectStorage/DataLakes/HudiMetadata.h",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h",
    "src/Storages/ObjectStorage/StorageObjectStorage.cpp",
    "src/Storages/ObjectStorage/StorageObjectStorage.h",
    "src/Storages/ObjectStorage/StorageObjectStorageSettings.cpp",
    "src/Storages/ObjectStorage/StorageObjectStorageSettings.h",
    "src/Storages/ObjectStorage/registerStorageObjectStorage.cpp",
    "src/TableFunctions/TableFunctionObjectStorage.cpp",
    "src/TableFunctions/TableFunctionObjectStorage.h"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_iceberg/test.py"
  ]
}