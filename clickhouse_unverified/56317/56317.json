{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 56317,
  "instance_id": "ClickHouse__ClickHouse-56317",
  "issue_numbers": [
    "56015"
  ],
  "base_commit": "946defa8a67880e7a4f47985b53d9e954e64453a",
  "patch": "diff --git a/.github/actions/common_setup/action.yml b/.github/actions/common_setup/action.yml\nindex b02413adc44c..e492fa97816d 100644\n--- a/.github/actions/common_setup/action.yml\n+++ b/.github/actions/common_setup/action.yml\n@@ -18,9 +18,6 @@ runs:\n           echo \"Setup the common ENV variables\"\n           cat >> \"$GITHUB_ENV\" << 'EOF'\n           TEMP_PATH=${{runner.temp}}/${{inputs.job_type}}\n-          REPO_COPY=${{runner.temp}}/${{inputs.job_type}}/git-repo-copy\n-          IMAGES_PATH=${{runner.temp}}/images_path\n-          REPORTS_PATH=${{runner.temp}}/reports_dir\n           EOF\n           if [ -z \"${{env.GITHUB_JOB_OVERRIDDEN}}\" ] && [ \"true\" == \"${{inputs.nested_job}}\" ]; then\n             echo \"The GITHUB_JOB_OVERRIDDEN ENV is unset, and must be set for the nested jobs\"\n@@ -30,6 +27,4 @@ runs:\n       shell: bash\n       run: |\n           # to remove every leftovers\n-          sudo rm -fr \"$TEMP_PATH\"\n-          mkdir -p \"$REPO_COPY\"\n-          cp -a \"$GITHUB_WORKSPACE\"/. \"$REPO_COPY\"/\n+          sudo rm -fr \"$TEMP_PATH\" && mkdir -p \"$TEMP_PATH\"\ndiff --git a/.github/workflows/backport_branches.yml b/.github/workflows/backport_branches.yml\nindex f6af4778cf13..ceb62083f78f 100644\n--- a/.github/workflows/backport_branches.yml\n+++ b/.github/workflows/backport_branches.yml\n@@ -10,27 +10,19 @@ on: # yamllint disable-line rule:truthy\n     branches:\n       - 'backport/**'\n jobs:\n-  CheckLabels:\n+  RunConfig:\n     runs-on: [self-hosted, style-checker]\n-    # Run the first check always, even if the CI is cancelled\n-    if: ${{ always() }}\n+    outputs:\n+      data: ${{ steps.runconfig.outputs.CI_DATA }}\n     steps:\n       - name: Check out repository code\n         uses: ClickHouse/checkout@v1\n         with:\n-          clear-repository: true\n+          clear-repository: true # to ensure correct digests\n       - name: Labels check\n         run: |\n           cd \"$GITHUB_WORKSPACE/tests/ci\"\n           python3 run_check.py\n-  PythonUnitTests:\n-    runs-on: [self-hosted, style-checker]\n-    needs: CheckLabels\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n       - name: Python unit tests\n         run: |\n           cd \"$GITHUB_WORKSPACE/tests/ci\"\n@@ -40,273 +32,235 @@ jobs:\n             echo \"Testing $dir\"\n             python3 -m unittest discover -s \"$dir\" -p 'test_*.py'\n           done\n-  DockerHubPushAarch64:\n-    runs-on: [self-hosted, style-checker-aarch64]\n-    needs: CheckLabels\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n+      - name: PrepareRunConfig\n+        id: runconfig\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_aarch64.json\n-  DockerHubPushAmd64:\n-    runs-on: [self-hosted, style-checker]\n-    needs: CheckLabels\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix amd64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_amd64.json\n-  DockerHubPush:\n-    needs: [DockerHubPushAmd64, DockerHubPushAarch64, PythonUnitTests]\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # to find ancestor merge commits necessary for finding proper docker tags\n-          filter: tree:0\n-      - name: Download changed aarch64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}\n-      - name: Download changed amd64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}\n-      - name: Images check\n+            echo \"::group::configure CI run\"\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --configure --outfile ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+\n+            echo \"::group::CI run configure results\"\n+            python3 -m json.tool ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+\n+            {\n+              echo 'CI_DATA<<EOF'\n+              cat  ${{ runner.temp }}/ci_run_data.json\n+              echo 'EOF'\n+            } >> \"$GITHUB_OUTPUT\"\n+      - name: Re-create GH statuses for skipped jobs if any\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images\n-          path: ${{ runner.temp }}/changed_images.json\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ runner.temp }}/ci_run_data.json --update-gh-statuses\n+  BuildDockers:\n+    needs: [RunConfig]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_docker.yml\n+    with:\n+      data: ${{ needs.RunConfig.outputs.data }}\n   CompatibilityCheckX86:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (amd64)\" --check-glibc --check-distributions\n   CompatibilityCheckAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (aarch64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (aarch64)\" --check-glibc\n #########################################################################################\n #################################### ORDINARY BUILDS ####################################\n #########################################################################################\n   BuilderDebRelease:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_release\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAarch64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_aarch64\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_asan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebTsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_tsan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebDebug:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_debug\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinDarwin:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinDarwinAarch64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin_aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n ############################################################################################\n ##################################### Docker images  #######################################\n ############################################################################################\n   DockerServerImages:\n-    needs:\n-      - BuilderDebRelease\n-      - BuilderDebAarch64\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # It MUST BE THE SAME for all dependencies and the job itself\n-          filter: tree:0\n-      - name: Check docker clickhouse/clickhouse-server building\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_server.py --release-type head --no-push \\\n-            --image-repo clickhouse/clickhouse-server --image-path docker/server\n-          python3 docker_server.py --release-type head --no-push \\\n-            --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper\n-      - name: Cleanup\n-        if: always()\n-        run: |\n-          docker ps --quiet | xargs --no-run-if-empty docker kill ||:\n-          docker ps --all --quiet | xargs --no-run-if-empty docker rm -f ||:\n-          sudo rm -fr \"$TEMP_PATH\"\n+    needs: [RunConfig, BuilderDebRelease, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_test.yml\n+    with:\n+      test_name: Docker server and keeper images\n+      runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n+      checkout_depth: 0  # It MUST BE THE SAME for all dependencies and the job itself\n+      run_command: |\n+        cd \"$GITHUB_WORKSPACE/tests/ci\"\n+        python3 docker_server.py --release-type head --no-push \\\n+          --image-repo clickhouse/clickhouse-server --image-path docker/server --allow-build-reuse\n+        python3 docker_server.py --release-type head --no-push \\\n+          --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper --allow-build-reuse\n ############################################################################################\n ##################################### BUILD REPORTER #######################################\n ############################################################################################\n   BuilderReport:\n     if: ${{ success() || failure() }}\n     needs:\n-      - BuilderDebRelease\n+      - RunConfig\n       - BuilderDebAarch64\n       - BuilderDebAsan\n-      - BuilderDebTsan\n       - BuilderDebDebug\n+      - BuilderDebRelease\n+      - BuilderDebTsan\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickHouse build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n   BuilderSpecialReport:\n-    if: ${{ success() || failure() }}\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n+      - RunConfig\n       - BuilderBinDarwin\n       - BuilderBinDarwinAarch64\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickHouse special build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n ############################################################################################\n #################################### INSTALL PACKAGES ######################################\n ############################################################################################\n   InstallPackagesTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n   InstallPackagesTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (arm64)\n       runner_type: style-checker-aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n ##############################################################################################\n ########################### FUNCTIONAl STATELESS TESTS #######################################\n ##############################################################################################\n   FunctionalStatelessTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (asan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ############################ FUNCTIONAl STATEFUL TESTS #######################################\n ##############################################################################################\n   FunctionalStatefulTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (debug)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ######################################### STRESS TESTS #######################################\n ##############################################################################################\n   StressTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (tsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n ############################# INTEGRATION TESTS #############################################\n #############################################################################################\n   IntegrationTestsRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (release)\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FinishCheck:\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n-      - DockerHubPush\n-      - DockerServerImages\n       - BuilderReport\n       - BuilderSpecialReport\n       - FunctionalStatelessTestAsan\ndiff --git a/.github/workflows/docs_check.yml b/.github/workflows/docs_check.yml\ndeleted file mode 100644\nindex 6d449e74f304..000000000000\n--- a/.github/workflows/docs_check.yml\n+++ /dev/null\n@@ -1,138 +0,0 @@\n-name: DocsCheck\n-\n-env:\n-  # Force the stdout and stderr streams to be unbuffered\n-  PYTHONUNBUFFERED: 1\n-\n-on:  # yamllint disable-line rule:truthy\n-  pull_request:\n-    types:\n-      - synchronize\n-      - reopened\n-      - opened\n-    branches:\n-      - master\n-    paths:\n-      - '**.md'\n-      - 'docker/docs/**'\n-      - 'docs/**'\n-      - 'utils/check-style/aspell-ignore/**'\n-      - 'tests/ci/docs_check.py'\n-      - '.github/workflows/docs_check.yml'\n-jobs:\n-  CheckLabels:\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Labels check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 run_check.py\n-  DockerHubPushAarch64:\n-    needs: CheckLabels\n-    runs-on: [self-hosted, style-checker-aarch64]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_aarch64.json\n-  DockerHubPushAmd64:\n-    needs: CheckLabels\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix amd64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_amd64.json\n-  DockerHubPush:\n-    needs: [DockerHubPushAmd64, DockerHubPushAarch64]\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # to find ancestor merge commits necessary for finding proper docker tags\n-          filter: tree:0\n-      - name: Download changed aarch64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}\n-      - name: Download changed amd64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images\n-          path: ${{ runner.temp }}/changed_images.json\n-  StyleCheck:\n-    needs: DockerHubPush\n-    # We need additional `&& ! cancelled()` to have the job being able to cancel\n-    if: ${{ success() || failure() || ( always() && ! cancelled() ) }}\n-    uses: ./.github/workflows/reusable_test.yml\n-    with:\n-      test_name: Style check\n-      runner_type: style-checker\n-      run_command: |\n-          cd \"$REPO_COPY/tests/ci\"\n-          python3 style_check.py\n-    secrets:\n-      secret_envs: |\n-        ROBOT_CLICKHOUSE_SSH_KEY<<RCSK\n-        ${{secrets.ROBOT_CLICKHOUSE_SSH_KEY}}\n-        RCSK\n-  DocsCheck:\n-    needs: DockerHubPush\n-    uses: ./.github/workflows/reusable_test.yml\n-    with:\n-      test_name: Docs check\n-      runner_type: func-tester-aarch64\n-      additional_envs: |\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 docs_check.py\n-  FinishCheck:\n-    needs:\n-      - StyleCheck\n-      - DockerHubPush\n-      - DocsCheck\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Finish label\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 finish_check.py\n-          python3 merge_pr.py --check-approved\ndiff --git a/.github/workflows/jepsen.yml b/.github/workflows/jepsen.yml\nindex 163de7769af5..50118eaf6221 100644\n--- a/.github/workflows/jepsen.yml\n+++ b/.github/workflows/jepsen.yml\n@@ -11,16 +11,14 @@ on: # yamllint disable-line rule:truthy\n   workflow_call:\n jobs:\n   KeeperJepsenRelease:\n-    uses: ./.github/workflows/reusable_test.yml\n+    uses: ./.github/workflows/reusable_simple_job.yml\n     with:\n       test_name: Jepsen keeper check\n       runner_type: style-checker\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 jepsen_check.py keeper\n   # ServerJepsenRelease:\n-  #   runs-on: [self-hosted, style-checker]\n-  #   uses: ./.github/workflows/reusable_test.yml\n+  #   uses: ./.github/workflows/reusable_simple_job.yml\n   #   with:\n   #     test_name: Jepsen server check\n   #     runner_type: style-checker\ndiff --git a/.github/workflows/libfuzzer.yml b/.github/workflows/libfuzzer.yml\nindex 1ca637c0d844..062ba99a8936 100644\n--- a/.github/workflows/libfuzzer.yml\n+++ b/.github/workflows/libfuzzer.yml\n@@ -8,19 +8,26 @@ on: # yamllint disable-line rule:truthy\n   #  schedule:\n   #    - cron: '0 0 2 31 1' # never for now\n   workflow_call:\n+    inputs:\n+      data:\n+        description: json ci data\n+        type: string\n+        required: true\n+\n jobs:\n   BuilderFuzzers:\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: fuzzers\n+      data: ${{ inputs.data }}\n   libFuzzerTest:\n     needs: [BuilderFuzzers]\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: libFuzzer tests\n       runner_type: func-tester\n+      data: ${{ inputs.data }}\n       additional_envs: |\n         KILL_TIMEOUT=10800\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 libfuzzer_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\ndiff --git a/.github/workflows/master.yml b/.github/workflows/master.yml\nindex 771de46be133..7947f082fc90 100644\n--- a/.github/workflows/master.yml\n+++ b/.github/workflows/master.yml\n@@ -10,13 +10,15 @@ on: # yamllint disable-line rule:truthy\n     branches:\n       - 'master'\n jobs:\n-  PythonUnitTests:\n+  RunConfig:\n     runs-on: [self-hosted, style-checker]\n+    outputs:\n+      data: ${{ steps.runconfig.outputs.CI_DATA }}\n     steps:\n       - name: Check out repository code\n         uses: ClickHouse/checkout@v1\n         with:\n-          clear-repository: true\n+          clear-repository: true # to ensure correct digests\n       - name: Python unit tests\n         run: |\n           cd \"$GITHUB_WORKSPACE/tests/ci\"\n@@ -26,243 +28,240 @@ jobs:\n             echo \"Testing $dir\"\n             python3 -m unittest discover -s \"$dir\" -p 'test_*.py'\n           done\n-  DockerHubPushAarch64:\n-    runs-on: [self-hosted, style-checker-aarch64]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n+      - name: PrepareRunConfig\n+        id: runconfig\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_aarch64.json\n-  DockerHubPushAmd64:\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix amd64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_amd64.json\n-  DockerHubPush:\n-    needs: [DockerHubPushAmd64, DockerHubPushAarch64, PythonUnitTests]\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # to find ancestor merge commits necessary for finding proper docker tags\n-          filter: tree:0\n-      - name: Download changed aarch64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}\n-      - name: Download changed amd64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}\n-      - name: Images check\n+            echo \"::group::configure CI run\"\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --configure --rebuild-all-binaries --outfile ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+\n+            echo \"::group::CI run configure results\"\n+            python3 -m json.tool ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+\n+            {\n+              echo 'CI_DATA<<EOF'\n+              cat  ${{ runner.temp }}/ci_run_data.json\n+              echo 'EOF'\n+            } >> \"$GITHUB_OUTPUT\"\n+      - name: Re-create GH statuses for skipped jobs if any\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images\n-          path: ${{ runner.temp }}/changed_images.json\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ runner.temp }}/ci_run_data.json --update-gh-statuses\n+  BuildDockers:\n+    needs: [RunConfig]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_docker.yml\n+    with:\n+      data: ${{ needs.RunConfig.outputs.data }}\n+      set_latest: true\n   StyleCheck:\n-    needs: DockerHubPush\n-    if: ${{ success() || failure() }}\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Style check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-          cd \"$REPO_COPY/tests/ci\"\n           python3 style_check.py --no-push\n   CompatibilityCheckX86:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (amd64)\" --check-glibc --check-distributions\n   CompatibilityCheckAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (aarch64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (aarch64)\" --check-glibc\n #########################################################################################\n #################################### ORDINARY BUILDS ####################################\n #########################################################################################\n+# TODO: never skip builds!\n   BuilderDebRelease:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n-      checkout_depth: 0\n       build_name: package_release\n+      checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAarch64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n-      checkout_depth: 0\n       build_name: package_aarch64\n+      checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinRelease:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n-      checkout_depth: 0\n       build_name: binary_release\n+      checkout_depth: 0 # otherwise we will have no info about contributors\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_asan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebUBsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_ubsan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebTsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_tsan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebMsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_msan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebDebug:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_debug\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##########################################################################################\n ##################################### SPECIAL BUILDS #####################################\n ##########################################################################################\n   BuilderBinClangTidy:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_tidy\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinDarwin:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinAarch64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinFreeBSD:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_freebsd\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinDarwinAarch64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin_aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinPPC64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_ppc64le\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinAmd64Compat:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_amd64_compat\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinAmd64Musl:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_amd64_musl\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinAarch64V80Compat:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_aarch64_v80compat\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinRISCV64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_riscv64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n   BuilderBinS390X:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_s390x\n+      data: ${{ needs.RunConfig.outputs.data }}\n       checkout_depth: 0\n ############################################################################################\n ##################################### Docker images  #######################################\n ############################################################################################\n   DockerServerImages:\n-    needs:\n-      - BuilderDebRelease\n-      - BuilderDebAarch64\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # It MUST BE THE SAME for all dependencies and the job itself\n-          filter: tree:0\n-      - name: Check docker clickhouse/clickhouse-server building\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_server.py --release-type head \\\n-            --image-repo clickhouse/clickhouse-server --image-path docker/server\n-          python3 docker_server.py --release-type head \\\n-            --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper\n-      - name: Cleanup\n-        if: always()\n-        run: |\n-          docker ps --quiet | xargs --no-run-if-empty docker kill ||:\n-          docker ps --all --quiet | xargs --no-run-if-empty docker rm -f ||:\n-          sudo rm -fr \"$TEMP_PATH\"\n+    needs: [RunConfig, BuilderDebRelease, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_test.yml\n+    with:\n+      test_name: Docker server and keeper images\n+      runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n+      # FIXME: avoid using 0 checkout\n+      checkout_depth: 0  # It MUST BE THE SAME for all dependencies and the job itself\n+      run_command: |\n+        cd \"$GITHUB_WORKSPACE/tests/ci\"\n+        python3 docker_server.py --release-type head \\\n+          --image-repo clickhouse/clickhouse-server --image-path docker/server\n+        python3 docker_server.py --release-type head \\\n+          --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper\n ############################################################################################\n ##################################### BUILD REPORTER #######################################\n ############################################################################################\n   BuilderReport:\n-    if: ${{ success() || failure() }}\n     needs:\n+      - RunConfig\n       - BuilderBinRelease\n       - BuilderDebAarch64\n       - BuilderDebAsan\n@@ -271,20 +270,22 @@ jobs:\n       - BuilderDebRelease\n       - BuilderDebTsan\n       - BuilderDebUBsan\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickHouse build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n   BuilderSpecialReport:\n-    if: ${{ success() || failure() }}\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n+      - RunConfig\n       - BuilderBinAarch64\n       - BuilderBinDarwin\n       - BuilderBinDarwinAarch64\n@@ -295,16 +296,17 @@ jobs:\n       - BuilderBinAmd64Compat\n       - BuilderBinAarch64V80Compat\n       - BuilderBinClangTidy\n+      - BuilderBinAmd64Musl\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickHouse special build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n   MarkReleaseReady:\n     needs:\n@@ -326,491 +328,404 @@ jobs:\n #################################### INSTALL PACKAGES ######################################\n ############################################################################################\n   InstallPackagesTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n   InstallPackagesTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (arm64)\n       runner_type: style-checker-aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n ##############################################################################################\n ########################### FUNCTIONAl STATELESS TESTS #######################################\n ##############################################################################################\n   FunctionalStatelessTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestReleaseDatabaseOrdinary:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release, DatabaseOrdinary)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestReleaseDatabaseReplicated:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release, DatabaseReplicated)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n-  FunctionalStatelessTestReleaseS3:\n-    needs: [BuilderDebRelease]\n+      data: ${{ needs.RunConfig.outputs.data }}\n+  FunctionalStatelessTestReleaseAnalyzer:\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Stateless tests (release, s3 storage)\n+      test_name: Stateless tests (release, analyzer)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 2\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n-  FunctionalStatelessTestReleaseAnalyzer:\n-    needs: [BuilderDebRelease]\n+      data: ${{ needs.RunConfig.outputs.data }}\n+  FunctionalStatelessTestReleaseS3:\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Stateless tests (release, analyzer)\n+      test_name: Stateless tests (release, s3 storage)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (aarch64)\n       runner_type: func-tester-aarch64\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (asan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n+\n   FunctionalStatelessTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (tsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 5\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n-  FunctionalStatelessTestUBsan:\n-    needs: [BuilderDebUBsan]\n+      data: ${{ needs.RunConfig.outputs.data }}\n+  FunctionalStatelessTestMsan:\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Stateless tests (ubsan)\n+      test_name: Stateless tests (msan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 2\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n-  FunctionalStatelessTestMsan:\n-    needs: [BuilderDebMsan]\n+      data: ${{ needs.RunConfig.outputs.data }}\n+  FunctionalStatelessTestUBsan:\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Stateless tests (msan)\n+      test_name: Stateless tests (ubsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (debug)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 5\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ############################ FUNCTIONAl STATEFUL TESTS #######################################\n ##############################################################################################\n   FunctionalStatefulTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (release)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (aarch64)\n       runner_type: func-tester-aarch64\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (asan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (tsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (msan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (ubsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (debug)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ########################### ClickBench #######################################################\n ##############################################################################################\n   ClickBenchAMD64:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickBench (amd64)\n       runner_type: func-tester\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 clickbench.py \"$CHECK_NAME\"\n   ClickBenchAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickBench (aarch64)\n       runner_type: func-tester-aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 clickbench.py \"$CHECK_NAME\"\n ##############################################################################################\n ######################################### STRESS TESTS #######################################\n ##############################################################################################\n   StressTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (asan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (tsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (msan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (ubsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (debug)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n ############################# INTEGRATION TESTS #############################################\n #############################################################################################\n   IntegrationTestsAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (asan)\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsAnalyzerAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (asan, analyzer)\n       runner_type: stress-tester\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (tsan)\n       runner_type: stress-tester\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (release)\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ##################################### AST FUZZERS ############################################\n ##############################################################################################\n   ASTFuzzerTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (asan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (tsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestUBSan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (ubsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestMSan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (msan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (debug)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n #################################### UNIT TESTS #############################################\n #############################################################################################\n   UnitTestsAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (asan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsReleaseClang:\n-    needs: [BuilderBinRelease]\n+    needs: [RunConfig, BuilderBinRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (release)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (tsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (msan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (ubsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n #################################### PERFORMANCE TESTS ######################################\n #############################################################################################\n   PerformanceComparisonX86:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Performance Comparison\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 performance_comparison_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   PerformanceComparisonAarch:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Performance Comparison Aarch64\n       runner_type: func-tester-aarch64\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 performance_comparison_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ###################################### SQLANCER FUZZERS ######################################\n ##############################################################################################\n   SQLancerTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: SQLancer (release)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 sqlancer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   SQLancerTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: SQLancer (debug)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 sqlancer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FinishCheck:\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n-      - DockerHubPush\n-      - BuilderReport\n-      - BuilderSpecialReport\n       - MarkReleaseReady\n       - FunctionalStatelessTestDebug\n       - FunctionalStatelessTestRelease\ndiff --git a/.github/workflows/nightly.yml b/.github/workflows/nightly.yml\nindex 1e94f70b9e67..458f59dfd384 100644\n--- a/.github/workflows/nightly.yml\n+++ b/.github/workflows/nightly.yml\n@@ -13,67 +13,36 @@ jobs:\n   Debug:\n     # The task for having a preserved ENV and event.json for later investigation\n     uses: ./.github/workflows/debug.yml\n-  DockerHubPushAarch64:\n-    runs-on: [self-hosted, style-checker-aarch64]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix aarch64 --all\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_aarch64.json\n-  DockerHubPushAmd64:\n+  RunConfig:\n     runs-on: [self-hosted, style-checker]\n+    outputs:\n+      data: ${{ steps.runconfig.outputs.CI_DATA }}\n     steps:\n       - name: Check out repository code\n         uses: ClickHouse/checkout@v1\n         with:\n-          clear-repository: true\n-      - name: Images check\n+          clear-repository: true # to ensure correct digests\n+      - name: PrepareRunConfig\n+        id: runconfig\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix amd64 --all\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_amd64.json\n-  DockerHubPush:\n-    needs: [DockerHubPushAmd64, DockerHubPushAarch64]\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # to find ancestor merge commits necessary for finding proper docker tags\n-          filter: tree:0\n-      - name: Download changed aarch64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}\n-      - name: Download changed amd64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images\n-          path: ${{ runner.temp }}/changed_images.json\n+            echo \"::group::configure CI run\"\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --configure --skip-jobs --rebuild-all-docker --outfile ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+\n+            echo \"::group::CI run configure results\"\n+            python3 -m json.tool ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+            {\n+              echo 'CI_DATA<<EOF'\n+              cat  ${{ runner.temp }}/ci_run_data.json\n+              echo 'EOF'\n+            } >> \"$GITHUB_OUTPUT\"\n+  BuildDockers:\n+    needs: [RunConfig]\n+    uses: ./.github/workflows/reusable_docker.yml\n+    with:\n+      data: \"${{ needs.RunConfig.outputs.data }}\"\n+      set_latest: true\n   SonarCloud:\n     runs-on: [self-hosted, builder]\n     env:\ndiff --git a/.github/workflows/pull_request.yml b/.github/workflows/pull_request.yml\nindex 0be703e11967..9f4636f73a4b 100644\n--- a/.github/workflows/pull_request.yml\n+++ b/.github/workflows/pull_request.yml\n@@ -24,26 +24,21 @@ on:  # yamllint disable-line rule:truthy\n ##################################### SMALL CHECKS #######################################\n ##########################################################################################\n jobs:\n-  CheckLabels:\n+  RunConfig:\n     runs-on: [self-hosted, style-checker]\n-    # Run the first check always, even if the CI is cancelled\n-    if: ${{ always() }}\n+    outputs:\n+      data: ${{ steps.runconfig.outputs.CI_DATA }}\n     steps:\n       - name: Check out repository code\n         uses: ClickHouse/checkout@v1\n         with:\n-          clear-repository: true\n+          clear-repository: true # to ensure correct digests\n+          fetch-depth: 0 # to get version\n+          filter: tree:0\n       - name: Labels check\n         run: |\n           cd \"$GITHUB_WORKSPACE/tests/ci\"\n           python3 run_check.py\n-  PythonUnitTests:\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n       - name: Python unit tests\n         run: |\n           cd \"$GITHUB_WORKSPACE/tests/ci\"\n@@ -53,249 +48,258 @@ jobs:\n             echo \"Testing $dir\"\n             python3 -m unittest discover -s \"$dir\" -p 'test_*.py'\n           done\n-  DockerHubPushAarch64:\n-    needs: CheckLabels\n-    runs-on: [self-hosted, style-checker-aarch64]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n+      - name: PrepareRunConfig\n+        id: runconfig\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_aarch64.json\n-  DockerHubPushAmd64:\n-    needs: CheckLabels\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n+            echo \"::group::configure CI run\"\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --configure --outfile ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+\n+            echo \"::group::CI run configure results\"\n+            python3 -m json.tool ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+\n+            {\n+              echo 'CI_DATA<<EOF'\n+              cat  ${{ runner.temp }}/ci_run_data.json\n+              echo 'EOF'\n+            } >> \"$GITHUB_OUTPUT\"\n+      - name: Re-create GH statuses for skipped jobs if any\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix amd64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_amd64.json\n-  DockerHubPush:\n-    needs: [DockerHubPushAmd64, DockerHubPushAarch64, PythonUnitTests]\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # to find ancestor merge commits necessary for finding proper docker tags\n-          filter: tree:0\n-      - name: Download changed aarch64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}\n-      - name: Download changed amd64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}\n-      - name: Images check\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ runner.temp }}/ci_run_data.json --update-gh-statuses\n+      - name: Style check early\n+        # hack to run style check before the docker build job if possible (style-check image not changed)\n+        if: contains(fromJson(steps.runconfig.outputs.CI_DATA).jobs_data.jobs_to_do, 'Style check early')\n         run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images\n-          path: ${{ runner.temp }}/changed_images.json\n+          DOCKER_TAG=$(echo '${{ toJson(fromJson(steps.runconfig.outputs.CI_DATA).docker_data.images) }}' | tr -d '\\n')\n+          export DOCKER_TAG=$DOCKER_TAG\n+          python3 ./tests/ci/style_check.py --no-push\n+  BuildDockers:\n+    needs: [RunConfig]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_docker.yml\n+    with:\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StyleCheck:\n-    needs: DockerHubPush\n-    # We need additional `&& ! cancelled()` to have the job being able to cancel\n-    if: ${{ success() || failure() || ( always() && ! cancelled() ) }}\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Style check\n       runner_type: style-checker\n       run_command: |\n-          cd \"$REPO_COPY/tests/ci\"\n           python3 style_check.py\n+      data: ${{ needs.RunConfig.outputs.data }}\n     secrets:\n       secret_envs: |\n         ROBOT_CLICKHOUSE_SSH_KEY<<RCSK\n         ${{secrets.ROBOT_CLICKHOUSE_SSH_KEY}}\n         RCSK\n+  DocsCheck:\n+    needs: [RunConfig, StyleCheck]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_test.yml\n+    with:\n+      test_name: Docs check\n+      runner_type: func-tester-aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n+      run_command: |\n+        python3 docs_check.py\n   FastTest:\n-    needs: DockerHubPush\n+    needs: [RunConfig, StyleCheck]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Fast tests\n       runner_type: builder\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-          cd \"$REPO_COPY/tests/ci\"\n           python3 fast_test_check.py\n   CompatibilityCheckX86:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (amd64)\" --check-glibc --check-distributions\n   CompatibilityCheckAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (aarch64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (aarch64)\" --check-glibc\n #########################################################################################\n #################################### ORDINARY BUILDS ####################################\n #########################################################################################\n+  BuilderDebDebug:\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_build.yml\n+    with:\n+      build_name: package_debug\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebRelease:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_release\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAarch64:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_aarch64\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinRelease:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_release\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAsan:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_asan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebUBsan:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_ubsan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebTsan:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_tsan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebMsan:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_msan\n-  BuilderDebDebug:\n-    needs: [FastTest, StyleCheck]\n-    uses: ./.github/workflows/reusable_build.yml\n-    with:\n-      build_name: package_debug\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##########################################################################################\n ##################################### SPECIAL BUILDS #####################################\n ##########################################################################################\n   BuilderBinClangTidy:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_tidy\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinDarwin:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinAarch64:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinFreeBSD:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_freebsd\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinDarwinAarch64:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin_aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinPPC64:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_ppc64le\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinAmd64Compat:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_amd64_compat\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinAmd64Musl:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_amd64_musl\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinAarch64V80Compat:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_aarch64_v80compat\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinRISCV64:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_riscv64\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinS390X:\n-    needs: [FastTest, StyleCheck]\n+    needs: [RunConfig, FastTest]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_s390x\n+      data: ${{ needs.RunConfig.outputs.data }}\n ############################################################################################\n ##################################### Docker images  #######################################\n ############################################################################################\n   DockerServerImages:\n-    needs:\n-      - BuilderDebRelease\n-      - BuilderDebAarch64\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # It MUST BE THE SAME for all dependencies and the job itself\n-          filter: tree:0\n-      - name: Check docker clickhouse/clickhouse-server building\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_server.py --release-type head --no-push \\\n-            --image-repo clickhouse/clickhouse-server --image-path docker/server\n-          python3 docker_server.py --release-type head --no-push \\\n-            --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper\n-      - name: Cleanup\n-        if: always()\n-        run: |\n-          docker ps --quiet | xargs --no-run-if-empty docker kill ||:\n-          docker ps --all --quiet | xargs --no-run-if-empty docker rm -f ||:\n-          sudo rm -fr \"$TEMP_PATH\"\n+    needs: [RunConfig, BuilderDebRelease, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_test.yml\n+    with:\n+      test_name: Docker server and keeper images\n+      runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n+      checkout_depth: 0  # It MUST BE THE SAME for all dependencies and the job itself\n+      run_command: |\n+        cd \"$GITHUB_WORKSPACE/tests/ci\"\n+        python3 docker_server.py --release-type head --no-push \\\n+          --image-repo clickhouse/clickhouse-server --image-path docker/server --allow-build-reuse\n+        python3 docker_server.py --release-type head --no-push \\\n+          --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper --allow-build-reuse\n ############################################################################################\n ##################################### BUILD REPORTER #######################################\n ############################################################################################\n   BuilderReport:\n-    if: ${{ success() || failure() }}\n     needs:\n+      - RunConfig\n       - BuilderBinRelease\n       - BuilderDebAarch64\n       - BuilderDebAsan\n@@ -304,20 +308,22 @@ jobs:\n       - BuilderDebRelease\n       - BuilderDebTsan\n       - BuilderDebUBsan\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickHouse build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n   BuilderSpecialReport:\n-    if: ${{ success() || failure() }}\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n+      - RunConfig\n       - BuilderBinAarch64\n       - BuilderBinDarwin\n       - BuilderBinDarwinAarch64\n@@ -332,223 +338,159 @@ jobs:\n     with:\n       test_name: ClickHouse special build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n ############################################################################################\n #################################### INSTALL PACKAGES ######################################\n ############################################################################################\n   InstallPackagesTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n   InstallPackagesTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (arm64)\n       runner_type: style-checker-aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n ##############################################################################################\n ########################### FUNCTIONAl STATELESS TESTS #######################################\n ##############################################################################################\n   FunctionalStatelessTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestReleaseDatabaseReplicated:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release, DatabaseReplicated)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n-  FunctionalStatelessTestReleaseWideParts:\n-    needs: [BuilderDebRelease]\n-    uses: ./.github/workflows/reusable_test.yml\n-    with:\n-      test_name: Stateless tests (release, wide parts enabled)\n-      runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestReleaseAnalyzer:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release, analyzer)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestReleaseS3:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release, s3 storage)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 2\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestS3Debug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (debug, s3 storage)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestS3Tsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (tsan, s3 storage)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 5\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (aarch64)\n       runner_type: func-tester-aarch64\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (asan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (tsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 5\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (msan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (ubsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 2\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (debug)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 5\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestFlakyCheck:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests flaky check (asan)\n+      data: ${{ needs.RunConfig.outputs.data }}\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n   TestsBugfixCheck:\n-    needs: [CheckLabels, StyleCheck]\n+    needs: [RunConfig, StyleCheck]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: tests bugfix validate check\n       runner_type: func-tester\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         KILL_TIMEOUT=3600\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-\n-        mkdir -p \"${REPORTS_PATH}/integration\"\n-        mkdir -p \"${REPORTS_PATH}/stateless\"\n-        cp -r ${REPORTS_PATH}/changed_images* ${REPORTS_PATH}/integration\n-        cp -r ${REPORTS_PATH}/changed_images* ${REPORTS_PATH}/stateless\n-\n         TEMP_PATH=\"${TEMP_PATH}/integration\" \\\n-          REPORTS_PATH=\"${REPORTS_PATH}/integration\" \\\n           python3 integration_test_check.py \"Integration $CHECK_NAME\" \\\n             --validate-bugfix --post-commit-status=file || echo 'ignore exit code'\n \n         TEMP_PATH=\"${TEMP_PATH}/stateless\" \\\n-          REPORTS_PATH=\"${REPORTS_PATH}/stateless\" \\\n           python3 functional_test_check.py \"Stateless $CHECK_NAME\" \"$KILL_TIMEOUT\" \\\n             --validate-bugfix --post-commit-status=file || echo 'ignore exit code'\n \n@@ -557,462 +499,388 @@ jobs:\n ############################ FUNCTIONAl STATEFUL TESTS #######################################\n ##############################################################################################\n   FunctionalStatefulTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (release)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (aarch64)\n       runner_type: func-tester-aarch64\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (asan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (tsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (msan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (ubsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (debug)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   # Parallel replicas\n   FunctionalStatefulTestDebugParallelReplicas:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (debug, ParallelReplicas)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestUBsanParallelReplicas:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (ubsan, ParallelReplicas)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestMsanParallelReplicas:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (msan, ParallelReplicas)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestTsanParallelReplicas:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (tsan, ParallelReplicas)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestAsanParallelReplicas:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (asan, ParallelReplicas)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestReleaseParallelReplicas:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (release, ParallelReplicas)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ########################### ClickBench #######################################################\n ##############################################################################################\n   ClickBenchAMD64:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickBench (amd64)\n       runner_type: func-tester\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 clickbench.py \"$CHECK_NAME\"\n   ClickBenchAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickBench (aarch64)\n       runner_type: func-tester-aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 clickbench.py \"$CHECK_NAME\"\n ##############################################################################################\n ######################################### STRESS TESTS #######################################\n ##############################################################################################\n   StressTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (asan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (tsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (msan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (ubsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (debug)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ######################################### UPGRADE CHECK ######################################\n ##############################################################################################\n   UpgradeCheckAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Upgrade check (asan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 upgrade_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UpgradeCheckTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Upgrade check (tsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 upgrade_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UpgradeCheckMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Upgrade check (msan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 upgrade_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UpgradeCheckDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Upgrade check (debug)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 upgrade_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ##################################### AST FUZZERS ############################################\n ##############################################################################################\n   ASTFuzzerTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (asan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (tsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestUBSan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (ubsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestMSan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (msan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   ASTFuzzerTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: AST fuzzer (debug)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 ast_fuzzer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n ############################# INTEGRATION TESTS #############################################\n #############################################################################################\n   IntegrationTestsAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (asan)\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsAnalyzerAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (asan, analyzer)\n       runner_type: stress-tester\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (tsan)\n       runner_type: stress-tester\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (release)\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsFlakyCheck:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests flaky check (asan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n #################################### UNIT TESTS #############################################\n #############################################################################################\n   UnitTestsAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (asan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsReleaseClang:\n-    needs: [BuilderBinRelease]\n+    needs: [RunConfig, BuilderBinRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (release)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (tsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (msan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   UnitTestsUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Unit tests (ubsan)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 unit_tests_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n #################################### PERFORMANCE TESTS ######################################\n #############################################################################################\n   PerformanceComparisonX86:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Performance Comparison\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 performance_comparison_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   PerformanceComparisonAarch:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Performance Comparison Aarch64\n       runner_type: func-tester-aarch64\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 performance_comparison_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ###################################### SQLANCER FUZZERS ######################################\n ##############################################################################################\n   SQLancerTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: SQLancer (release)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 sqlancer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   SQLancerTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: SQLancer (debug)\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 sqlancer_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FinishCheck:\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n-      - StyleCheck\n-      - DockerHubPush\n-      - DockerServerImages\n-      - CheckLabels\n       - BuilderReport\n       - BuilderSpecialReport\n+      - DocsCheck\n       - FastTest\n       - FunctionalStatelessTestDebug\n       - FunctionalStatelessTestRelease\n       - FunctionalStatelessTestReleaseDatabaseReplicated\n-      - FunctionalStatelessTestReleaseWideParts\n       - FunctionalStatelessTestReleaseAnalyzer\n       - FunctionalStatelessTestAarch64\n       - FunctionalStatelessTestAsan\n@@ -1080,28 +948,24 @@ jobs:\n ############################ SQLLOGIC TEST ###################################################\n ##############################################################################################\n   SQLLogicTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Sqllogic test (release)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 sqllogic_test.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ##################################### SQL TEST ###############################################\n ##############################################################################################\n   SQLTest:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: SQLTest\n       runner_type: fuzzer-unit-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 sqltest.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n ###################################### NOT IN FINISH ########################################\n #############################################################################################\n@@ -1111,13 +975,15 @@ jobs:\n     # This is special test NOT INCLUDED in FinishCheck\n     # When it's skipped, all dependent tasks will be skipped too.\n     # DO NOT add it there\n-    if: contains(github.event.pull_request.labels.*.name, 'jepsen-test')\n-    needs: [BuilderBinRelease]\n+    if: ${{ !failure() && !cancelled() && contains(github.event.pull_request.labels.*.name, 'jepsen-test') }}\n+    needs: [RunConfig, BuilderBinRelease]\n     uses: ./.github/workflows/jepsen.yml\n #############################################################################################\n ####################################### libFuzzer ###########################################\n #############################################################################################\n   libFuzzer:\n-    if: contains(github.event.pull_request.labels.*.name, 'libFuzzer')\n-    needs: [DockerHubPush, StyleCheck]\n+    if: ${{ !failure() && !cancelled() && contains(github.event.pull_request.labels.*.name, 'libFuzzer') }}\n+    needs: [RunConfig, StyleCheck]\n     uses: ./.github/workflows/libfuzzer.yml\n+    with:\n+      data: ${{ needs.RunConfig.outputs.data }}\ndiff --git a/.github/workflows/release_branches.yml b/.github/workflows/release_branches.yml\nindex b5771fa87ab9..4fecc5acf58b 100644\n--- a/.github/workflows/release_branches.yml\n+++ b/.github/workflows/release_branches.yml\n@@ -13,171 +13,165 @@ on: # yamllint disable-line rule:truthy\n       - '2[1-9].[1-9]'\n \n jobs:\n-  DockerHubPushAarch64:\n-    runs-on: [self-hosted, style-checker-aarch64]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-      - name: Images check\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_aarch64.json\n-  DockerHubPushAmd64:\n+  RunConfig:\n     runs-on: [self-hosted, style-checker]\n+    outputs:\n+      data: ${{ steps.runconfig.outputs.CI_DATA }}\n     steps:\n       - name: Check out repository code\n         uses: ClickHouse/checkout@v1\n         with:\n-          clear-repository: true\n-      - name: Images check\n+          clear-repository: true # to ensure correct digests\n+      - name: Labels check\n         run: |\n           cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_images_check.py --suffix amd64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}/docker_images_check/changed_images_amd64.json\n-  DockerHubPush:\n-    needs: [DockerHubPushAmd64, DockerHubPushAarch64]\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # to find ancestor merge commits necessary for finding proper docker tags\n-          filter: tree:0\n-      - name: Download changed aarch64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_aarch64\n-          path: ${{ runner.temp }}\n-      - name: Download changed amd64 images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images_amd64\n-          path: ${{ runner.temp }}\n-      - name: Images check\n+          python3 run_check.py\n+      - name: Python unit tests\n         run: |\n           cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64\n-      - name: Upload images files to artifacts\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: changed_images\n-          path: ${{ runner.temp }}/changed_images.json\n+          echo \"Testing the main ci directory\"\n+          python3 -m unittest discover -s . -p 'test_*.py'\n+          for dir in *_lambda/; do\n+            echo \"Testing $dir\"\n+            python3 -m unittest discover -s \"$dir\" -p 'test_*.py'\n+          done\n+      - name: PrepareRunConfig\n+        id: runconfig\n+        run: |\n+            echo \"::group::configure CI run\"\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --configure --rebuild-all-binaries --outfile ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+            echo \"::group::CI run configure results\"\n+            python3 -m json.tool ${{ runner.temp }}/ci_run_data.json\n+            echo \"::endgroup::\"\n+            {\n+              echo 'CI_DATA<<EOF'\n+              cat  ${{ runner.temp }}/ci_run_data.json\n+              echo 'EOF'\n+            } >> \"$GITHUB_OUTPUT\"\n+      - name: Re-create GH statuses for skipped jobs if any\n+        run: |\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ runner.temp }}/ci_run_data.json --update-gh-statuses\n+  BuildDockers:\n+    needs: [RunConfig]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_docker.yml\n+    with:\n+      data: ${{ needs.RunConfig.outputs.data }}\n   CompatibilityCheckX86:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (amd64)\" --check-glibc --check-distributions\n   CompatibilityCheckAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Compatibility check X86\n+      test_name: Compatibility check (aarch64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 compatibility_check.py --check-name \"Compatibility check (aarch64)\" --check-glibc\n #########################################################################################\n #################################### ORDINARY BUILDS ####################################\n #########################################################################################\n   BuilderDebRelease:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_release\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAarch64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_aarch64\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebAsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_asan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebUBsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_ubsan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebTsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_tsan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebMsan:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_msan\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderDebDebug:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: package_debug\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinDarwin:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n   BuilderBinDarwinAarch64:\n-    needs: [DockerHubPush]\n+    needs: [RunConfig, BuildDockers]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_build.yml\n     with:\n       build_name: binary_darwin_aarch64\n       checkout_depth: 0\n+      data: ${{ needs.RunConfig.outputs.data }}\n ############################################################################################\n ##################################### Docker images  #######################################\n ############################################################################################\n   DockerServerImages:\n-    needs:\n-      - BuilderDebRelease\n-      - BuilderDebAarch64\n-    runs-on: [self-hosted, style-checker]\n-    steps:\n-      - name: Check out repository code\n-        uses: ClickHouse/checkout@v1\n-        with:\n-          clear-repository: true\n-          fetch-depth: 0  # It MUST BE THE SAME for all dependencies and the job itself\n-          filter: tree:0\n-      - name: Check docker clickhouse/clickhouse-server building\n-        run: |\n-          cd \"$GITHUB_WORKSPACE/tests/ci\"\n-          python3 docker_server.py --release-type head --no-push \\\n-            --image-repo clickhouse/clickhouse-server --image-path docker/server\n-          python3 docker_server.py --release-type head --no-push \\\n-            --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper\n-      - name: Cleanup\n-        if: always()\n-        run: |\n-          docker ps --quiet | xargs --no-run-if-empty docker kill ||:\n-          docker ps --all --quiet | xargs --no-run-if-empty docker rm -f ||:\n-          sudo rm -fr \"$TEMP_PATH\"\n+    needs: [RunConfig, BuilderDebRelease, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n+    uses: ./.github/workflows/reusable_test.yml\n+    with:\n+      test_name: Docker server and keeper images\n+      runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n+      checkout_depth: 0\n+      run_command: |\n+        cd \"$GITHUB_WORKSPACE/tests/ci\"\n+        python3 docker_server.py --release-type head --no-push \\\n+          --image-repo clickhouse/clickhouse-server --image-path docker/server --allow-build-reuse\n+        python3 docker_server.py --release-type head --no-push \\\n+          --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper --allow-build-reuse\n ############################################################################################\n ##################################### BUILD REPORTER #######################################\n ############################################################################################\n   BuilderReport:\n-    if: ${{ success() || failure() }}\n     needs:\n+      - RunConfig\n       - BuilderDebRelease\n       - BuilderDebAarch64\n       - BuilderDebAsan\n@@ -185,32 +179,39 @@ jobs:\n       - BuilderDebUBsan\n       - BuilderDebMsan\n       - BuilderDebDebug\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickHouse build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n   BuilderSpecialReport:\n-    if: ${{ success() || failure() }}\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n-      - BuilderBinDarwin\n-      - BuilderBinDarwinAarch64\n+      - RunConfig\n+      - BuilderDebRelease\n+      - BuilderDebAarch64\n+      - BuilderDebAsan\n+      - BuilderDebTsan\n+      - BuilderDebUBsan\n+      - BuilderDebMsan\n+      - BuilderDebDebug\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: ClickHouse special build check\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       additional_envs: |\n         NEEDS_DATA<<NDENV\n         ${{ toJSON(needs) }}\n         NDENV\n       run_command: |\n-        cd \"$GITHUB_WORKSPACE/tests/ci\"\n         python3 build_report_check.py \"$CHECK_NAME\"\n   MarkReleaseReady:\n     needs:\n@@ -232,282 +233,224 @@ jobs:\n #################################### INSTALL PACKAGES ######################################\n ############################################################################################\n   InstallPackagesTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (amd64)\n       runner_type: style-checker\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n   InstallPackagesTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Install packages (arm64)\n       runner_type: style-checker-aarch64\n+      data: ${{ needs.RunConfig.outputs.data }}\n       run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n         python3 install_check.py \"$CHECK_NAME\"\n ##############################################################################################\n ########################### FUNCTIONAl STATELESS TESTS #######################################\n ##############################################################################################\n   FunctionalStatelessTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (release)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (aarch64)\n       runner_type: func-tester-aarch64\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (asan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (tsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 5\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n-  FunctionalStatelessTestUBsan:\n-    needs: [BuilderDebUBsan]\n+      data: ${{ needs.RunConfig.outputs.data }}\n+  FunctionalStatelessTestMsan:\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Stateless tests (ubsan)\n+      test_name: Stateless tests (msan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 2\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n-  FunctionalStatelessTestMsan:\n-    needs: [BuilderDebMsan]\n+      data: ${{ needs.RunConfig.outputs.data }}\n+  FunctionalStatelessTestUBsan:\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n-      test_name: Stateless tests (msan)\n+      test_name: Stateless tests (ubsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatelessTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateless tests (debug)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=10800\n-      batches: 5\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ############################ FUNCTIONAl STATEFUL TESTS #######################################\n ##############################################################################################\n   FunctionalStatefulTestRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (release)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestAarch64:\n-    needs: [BuilderDebAarch64]\n+    needs: [RunConfig, BuilderDebAarch64]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (aarch64)\n       runner_type: func-tester-aarch64\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (asan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (tsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (msan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (ubsan)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FunctionalStatefulTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stateful tests (debug)\n       runner_type: func-tester\n-      additional_envs: |\n-        KILL_TIMEOUT=3600\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 functional_test_check.py \"$CHECK_NAME\" \"$KILL_TIMEOUT\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n ##############################################################################################\n ######################################### STRESS TESTS #######################################\n ##############################################################################################\n   StressTestAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (asan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (tsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestMsan:\n-    needs: [BuilderDebMsan]\n+    needs: [RunConfig, BuilderDebMsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (msan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestUBsan:\n-    needs: [BuilderDebUBsan]\n+    needs: [RunConfig, BuilderDebUBsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (ubsan)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   StressTestDebug:\n-    needs: [BuilderDebDebug]\n+    needs: [RunConfig, BuilderDebDebug]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Stress test (debug)\n       runner_type: stress-tester\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 stress_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n #############################################################################################\n ############################# INTEGRATION TESTS #############################################\n #############################################################################################\n   IntegrationTestsAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (asan)\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsAnalyzerAsan:\n-    needs: [BuilderDebAsan]\n+    needs: [RunConfig, BuilderDebAsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (asan, analyzer)\n       runner_type: stress-tester\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsTsan:\n-    needs: [BuilderDebTsan]\n+    needs: [RunConfig, BuilderDebTsan]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (tsan)\n       runner_type: stress-tester\n-      batches: 6\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   IntegrationTestsRelease:\n-    needs: [BuilderDebRelease]\n+    needs: [RunConfig, BuilderDebRelease]\n+    if: ${{ !failure() && !cancelled() }}\n     uses: ./.github/workflows/reusable_test.yml\n     with:\n       test_name: Integration tests (release)\n       runner_type: stress-tester\n-      batches: 4\n-      run_command: |\n-        cd \"$REPO_COPY/tests/ci\"\n-        python3 integration_test_check.py \"$CHECK_NAME\"\n+      data: ${{ needs.RunConfig.outputs.data }}\n   FinishCheck:\n+    if: ${{ !failure() && !cancelled() }}\n     needs:\n-      - DockerHubPush\n       - DockerServerImages\n       - BuilderReport\n       - BuilderSpecialReport\ndiff --git a/.github/workflows/reusable_build.yml b/.github/workflows/reusable_build.yml\nindex f36b93bea588..b1bc64c1f692 100644\n--- a/.github/workflows/reusable_build.yml\n+++ b/.github/workflows/reusable_build.yml\n@@ -22,6 +22,10 @@ name: Build ClickHouse\n         description: the label of runner to use\n         default: builder\n         type: string\n+      data:\n+        description: json ci data\n+        type: string\n+        required: true\n       additional_envs:\n         description: additional ENV variables to setup the job\n         type: string\n@@ -29,6 +33,7 @@ name: Build ClickHouse\n jobs:\n   Build:\n     name: Build-${{inputs.build_name}}\n+    if: contains(fromJson(inputs.data).jobs_data.jobs_to_do, inputs.build_name)\n     env:\n       GITHUB_JOB_OVERRIDDEN: Build-${{inputs.build_name}}\n     runs-on: [self-hosted, '${{inputs.runner_type}}']\n@@ -37,6 +42,7 @@ jobs:\n         uses: ClickHouse/checkout@v1\n         with:\n           clear-repository: true\n+          ref: ${{ fromJson(inputs.data).git_ref }}\n           submodules: true\n           fetch-depth: ${{inputs.checkout_depth}}\n           filter: tree:0\n@@ -44,6 +50,9 @@ jobs:\n         run: |\n           cat >> \"$GITHUB_ENV\" << 'EOF'\n           ${{inputs.additional_envs}}\n+          DOCKER_TAG<<DOCKER_JSON\n+          ${{ toJson(fromJson(inputs.data).docker_data.images) }}\n+          DOCKER_JSON\n           EOF\n           python3 \"$GITHUB_WORKSPACE\"/tests/ci/ci_config.py --build-name \"${{inputs.build_name}}\" >> \"$GITHUB_ENV\"\n       - name: Apply sparse checkout for contrib # in order to check that it doesn't break build\n@@ -60,20 +69,18 @@ jobs:\n         uses: ./.github/actions/common_setup\n         with:\n           job_type: build_check\n-      - name: Download changed images\n-        uses: actions/download-artifact@v3\n-        with:\n-          name: changed_images\n-          path: ${{ env.IMAGES_PATH }}\n+      - name: Pre\n+        run: |\n+          python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ toJson(inputs.data) }} --pre --job-name '${{inputs.build_name}}'\n       - name: Build\n         run: |\n-          cd \"$REPO_COPY/tests/ci\" && python3 build_check.py \"$BUILD_NAME\"\n-      - name: Upload build URLs to artifacts\n-        if: ${{ success() || failure() }}\n-        uses: actions/upload-artifact@v3\n-        with:\n-          name: ${{ env.BUILD_URLS }}\n-          path: ${{ env.TEMP_PATH }}/${{ env.BUILD_URLS }}.json\n+          python3 \"$GITHUB_WORKSPACE/tests/ci/build_check.py\" \"$BUILD_NAME\"\n+      - name: Post\n+        run: |\n+          python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ toJson(inputs.data) }} --post --job-name '${{inputs.build_name}}'\n+      - name: Mark as done\n+        run: |\n+          python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ toJson(inputs.data) }} --mark-success --job-name '${{inputs.build_name}}'\n       - name: Clean\n         if: always()\n         uses: ./.github/actions/clean\ndiff --git a/.github/workflows/reusable_docker.yml b/.github/workflows/reusable_docker.yml\nnew file mode 100644\nindex 000000000000..08a5740e7e06\n--- /dev/null\n+++ b/.github/workflows/reusable_docker.yml\n@@ -0,0 +1,68 @@\n+name: Build docker images\n+'on':\n+  workflow_call:\n+    inputs:\n+      data:\n+        description: json with ci data from todo job\n+        required: true\n+        type: string\n+      set_latest:\n+        description: set latest tag for resulting multiarch manifest\n+        required: false\n+        type: boolean\n+        default: false\n+jobs:\n+  DockerBuildAarch64:\n+    runs-on: [self-hosted, style-checker-aarch64]\n+    if: |\n+      !failure() && !cancelled() && toJson(fromJson(inputs.data).docker_data.missing_aarch64) != '[]'\n+    steps:\n+      - name: Check out repository code\n+        uses: ClickHouse/checkout@v1\n+        with:\n+          ref: ${{ fromJson(inputs.data).git_ref }}\n+      - name: Build images\n+        run: |\n+          python3 \"${GITHUB_WORKSPACE}/tests/ci/docker_images_check.py\" \\\n+            --suffix aarch64 \\\n+            --image-tags '${{ toJson(fromJson(inputs.data).docker_data.images) }}' \\\n+            --missing-images '${{ toJson(fromJson(inputs.data).docker_data.missing_aarch64) }}'\n+  DockerBuildAmd64:\n+    runs-on: [self-hosted, style-checker]\n+    if: |\n+      !failure() && !cancelled() && toJson(fromJson(inputs.data).docker_data.missing_amd64) != '[]'\n+    steps:\n+      - name: Check out repository code\n+        uses: ClickHouse/checkout@v1\n+        with:\n+          ref: ${{ fromJson(inputs.data).git_ref }}\n+      - name: Build images\n+        run: |\n+          python3 \"${GITHUB_WORKSPACE}/tests/ci/docker_images_check.py\" \\\n+            --suffix amd64 \\\n+            --image-tags '${{ toJson(fromJson(inputs.data).docker_data.images) }}' \\\n+            --missing-images '${{ toJson(fromJson(inputs.data).docker_data.missing_amd64) }}'\n+  DockerMultiArchManifest:\n+    needs: [DockerBuildAmd64, DockerBuildAarch64]\n+    runs-on: [self-hosted, style-checker]\n+    if: |\n+      !failure() && !cancelled() && toJson(fromJson(inputs.data).docker_data.missing_multi) != '[]'\n+    steps:\n+      - name: Check out repository code\n+        uses: ClickHouse/checkout@v1\n+        with:\n+          ref: ${{ fromJson(inputs.data).git_ref }}\n+      - name: Build images\n+        run: |\n+          cd \"$GITHUB_WORKSPACE/tests/ci\"\n+          if [ \"${{ inputs.set_latest }}\" == \"true\" ]; then\n+            echo \"latest tag will be set for resulting manifests\"\n+            python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64 \\\n+              --image-tags '${{ toJson(fromJson(inputs.data).docker_data.images) }}' \\\n+              --missing-images '${{ toJson(fromJson(inputs.data).docker_data.missing_multi) }}' \\\n+              --set-latest\n+          else\n+            python3 docker_manifests_merge.py --suffix amd64 --suffix aarch64 \\\n+              --image-tags '${{ toJson(fromJson(inputs.data).docker_data.images) }}' \\\n+              --missing-images '${{ toJson(fromJson(inputs.data).docker_data.missing_multi) }}'\n+          fi\ndiff --git a/.github/workflows/reusable_simple_job.yml b/.github/workflows/reusable_simple_job.yml\nnew file mode 100644\nindex 000000000000..ea196a326647\n--- /dev/null\n+++ b/.github/workflows/reusable_simple_job.yml\n@@ -0,0 +1,90 @@\n+### For the pure soul wishes to move it to another place\n+# https://github.com/orgs/community/discussions/9050\n+\n+name: Simple job\n+'on':\n+  workflow_call:\n+    inputs:\n+      test_name:\n+        description: the value of test type from tests/ci/ci_config.py, ends up as $CHECK_NAME ENV\n+        required: true\n+        type: string\n+      runner_type:\n+        description: the label of runner to use\n+        required: true\n+        type: string\n+      run_command:\n+        description: the command to launch the check\n+        default: \"\"\n+        required: false\n+        type: string\n+      checkout_depth:\n+        description: the value of the git shallow checkout\n+        required: false\n+        type: number\n+        default: 1\n+      submodules:\n+        description: if the submodules should be checked out\n+        required: false\n+        type: boolean\n+        default: false\n+      additional_envs:\n+        description: additional ENV variables to setup the job\n+        type: string\n+      working-directory:\n+        description: sets custom working directory\n+        type: string\n+        default: \"\"\n+      git_ref:\n+        description: commit to use, merge commit for pr or head\n+        required: false\n+        type: string\n+        default: ${{ github.event.after }} # no merge commit\n+    secrets:\n+      secret_envs:\n+        description: if given, it's passed to the environments\n+        required: false\n+\n+\n+env:\n+  # Force the stdout and stderr streams to be unbuffered\n+  PYTHONUNBUFFERED: 1\n+  CHECK_NAME: ${{inputs.test_name}}\n+\n+jobs:\n+  Test:\n+    runs-on: [self-hosted, '${{inputs.runner_type}}']\n+    name: ${{inputs.test_name}}\n+    env:\n+      GITHUB_JOB_OVERRIDDEN: ${{inputs.test_name}}\n+    steps:\n+      - name: Check out repository code\n+        uses: ClickHouse/checkout@v1\n+        with:\n+          clear-repository: true\n+          ref: ${{ inputs.git_ref }}\n+          submodules: ${{inputs.submodules}}\n+          fetch-depth: ${{inputs.checkout_depth}}\n+          filter: tree:0\n+      - name: Set build envs\n+        run: |\n+          cat >> \"$GITHUB_ENV\" << 'EOF'\n+          CHECK_NAME=${{ inputs.test_name }}\n+          ${{inputs.additional_envs}}\n+          ${{secrets.secret_envs}}\n+          EOF\n+      - name: Common setup\n+        uses: ./.github/actions/common_setup\n+        with:\n+          job_type: test\n+      - name: Run\n+        run: |\n+          if [ -n '${{ inputs.working-directory }}' ]; then\n+            cd \"${{ inputs.working-directory }}\"\n+          else\n+            cd \"$GITHUB_WORKSPACE/tests/ci\"\n+          fi\n+          ${{ inputs.run_command }}\n+      - name: Clean\n+        if: always()\n+        uses: ./.github/actions/clean\ndiff --git a/docker/keeper/Dockerfile b/docker/keeper/Dockerfile\nindex a238a9851d9f..06bb3f2cdda8 100644\n--- a/docker/keeper/Dockerfile\n+++ b/docker/keeper/Dockerfile\n@@ -36,6 +36,7 @@ ARG REPO_CHANNEL=\"stable\"\n ARG REPOSITORY=\"https://packages.clickhouse.com/tgz/${REPO_CHANNEL}\"\n ARG VERSION=\"23.11.2.11\"\n ARG PACKAGES=\"clickhouse-keeper\"\n+ARG DIRECT_DOWNLOAD_URLS=\"\"\n \n # user/group precreated explicitly with fixed uid/gid on purpose.\n # It is especially important for rootless containers: in that case entrypoint\n@@ -47,15 +48,27 @@ ARG PACKAGES=\"clickhouse-keeper\"\n \n ARG TARGETARCH\n RUN arch=${TARGETARCH:-amd64} \\\n-    && for package in ${PACKAGES}; do \\\n-    ( \\\n-        cd /tmp \\\n-        && echo \"Get ${REPOSITORY}/${package}-${VERSION}-${arch}.tgz\" \\\n+    && cd /tmp && rm -f /tmp/*tgz && rm -f /tmp/*tgz.sha512 |: \\\n+    && if [ -n \"${DIRECT_DOWNLOAD_URLS}\" ]; then \\\n+        echo \"installing from provided urls with tgz packages: ${DIRECT_DOWNLOAD_URLS}\" \\\n+        && for url in $DIRECT_DOWNLOAD_URLS; do \\\n+            echo \"Get ${url}\" \\\n+            && wget -c -q \"$url\" \\\n+        ; done \\\n+    else \\\n+        for package in ${PACKAGES}; do \\\n+            cd /tmp \\\n+            && echo \"Get ${REPOSITORY}/${package}-${VERSION}-${arch}.tgz\" \\\n             && wget -c -q \"${REPOSITORY}/${package}-${VERSION}-${arch}.tgz\" \\\n             && wget -c -q \"${REPOSITORY}/${package}-${VERSION}-${arch}.tgz.sha512\" \\\n-            && sed 's:/output/:/tmp/:' < \"${package}-${VERSION}-${arch}.tgz.sha512\" | sha512sum -c \\\n-            && tar xvzf \"${package}-${VERSION}-${arch}.tgz\" --strip-components=1 -C / \\\n-    ) \\\n+        ; done \\\n+    fi \\\n+    && cat *.tgz.sha512 | sha512sum -c \\\n+    && for file in *.tgz; do \\\n+        if [ -f \"$file\" ]; then \\\n+            echo \"Unpacking $file\"; \\\n+            tar xvzf \"$file\" --strip-components=1 -C /; \\\n+        fi \\\n     ; done \\\n     && rm /tmp/*.tgz /install -r \\\n     && addgroup -S -g 101 clickhouse \\\ndiff --git a/docker/packager/binary/build.sh b/docker/packager/binary/build.sh\nindex fd9bfcaabb2e..b63643419fe9 100755\n--- a/docker/packager/binary/build.sh\n+++ b/docker/packager/binary/build.sh\n@@ -149,7 +149,7 @@ then\n     mkdir -p \"$PERF_OUTPUT\"\n     cp -r ../tests/performance \"$PERF_OUTPUT\"\n     cp -r ../tests/config/top_level_domains  \"$PERF_OUTPUT\"\n-    cp -r ../docker/test/performance-comparison/config \"$PERF_OUTPUT\" ||:\n+    cp -r ../tests/performance/scripts/config \"$PERF_OUTPUT\" ||:\n     for SRC in /output/clickhouse*; do\n         # Copy all clickhouse* files except packages and bridges\n         [[ \"$SRC\" != *.* ]] && [[ \"$SRC\" != *-bridge ]] && \\\n@@ -160,7 +160,7 @@ then\n         ln -sf clickhouse \"$PERF_OUTPUT\"/clickhouse-keeper\n     fi\n \n-    cp -r ../docker/test/performance-comparison \"$PERF_OUTPUT\"/scripts ||:\n+    cp -r ../tests/performance/scripts \"$PERF_OUTPUT\"/scripts ||:\n     prepare_combined_output \"$PERF_OUTPUT\"\n \n     # We have to know the revision that corresponds to this binary build.\ndiff --git a/docker/server/Dockerfile.alpine b/docker/server/Dockerfile.alpine\nindex 31dbc38708f4..e7b0d4e15e53 100644\n--- a/docker/server/Dockerfile.alpine\n+++ b/docker/server/Dockerfile.alpine\n@@ -34,6 +34,7 @@ ARG REPO_CHANNEL=\"stable\"\n ARG REPOSITORY=\"https://packages.clickhouse.com/tgz/${REPO_CHANNEL}\"\n ARG VERSION=\"23.11.2.11\"\n ARG PACKAGES=\"clickhouse-client clickhouse-server clickhouse-common-static\"\n+ARG DIRECT_DOWNLOAD_URLS=\"\"\n \n # user/group precreated explicitly with fixed uid/gid on purpose.\n # It is especially important for rootless containers: in that case entrypoint\n@@ -43,15 +44,26 @@ ARG PACKAGES=\"clickhouse-client clickhouse-server clickhouse-common-static\"\n # The same uid / gid (101) is used both for alpine and ubuntu.\n \n RUN arch=${TARGETARCH:-amd64} \\\n-    && for package in ${PACKAGES}; do \\\n-    ( \\\n-        cd /tmp \\\n-        && echo \"Get ${REPOSITORY}/${package}-${VERSION}-${arch}.tgz\" \\\n+    && cd /tmp \\\n+    && if [ -n \"${DIRECT_DOWNLOAD_URLS}\" ]; then \\\n+        echo \"installing from provided urls with tgz packages: ${DIRECT_DOWNLOAD_URLS}\" \\\n+        && for url in $DIRECT_DOWNLOAD_URLS; do \\\n+            echo \"Get ${url}\" \\\n+            && wget -c -q \"$url\" \\\n+        ; done \\\n+    else \\\n+        for package in ${PACKAGES}; do \\\n+            echo \"Get ${REPOSITORY}/${package}-${VERSION}-${arch}.tgz\" \\\n             && wget -c -q \"${REPOSITORY}/${package}-${VERSION}-${arch}.tgz\" \\\n             && wget -c -q \"${REPOSITORY}/${package}-${VERSION}-${arch}.tgz.sha512\" \\\n-            && sed 's:/output/:/tmp/:' < \"${package}-${VERSION}-${arch}.tgz.sha512\" | sha512sum -c \\\n-            && tar xvzf \"${package}-${VERSION}-${arch}.tgz\" --strip-components=1 -C / \\\n-    ) \\\n+        ; done \\\n+    fi \\\n+    && cat *.tgz.sha512 | sed 's:/output/:/tmp/:' | sha512sum -c \\\n+    && for file in *.tgz; do \\\n+        if [ -f \"$file\" ]; then \\\n+            echo \"Unpacking $file\"; \\\n+            tar xvzf \"$file\" --strip-components=1 -C /; \\\n+        fi \\\n     ; done \\\n     && rm /tmp/*.tgz /install -r \\\n     && addgroup -S -g 101 clickhouse \\\ndiff --git a/docker/server/Dockerfile.ubuntu b/docker/server/Dockerfile.ubuntu\nindex 6bbec625300f..8cb4bf94ac93 100644\n--- a/docker/server/Dockerfile.ubuntu\n+++ b/docker/server/Dockerfile.ubuntu\n@@ -37,6 +37,7 @@ ARG PACKAGES=\"clickhouse-client clickhouse-server clickhouse-common-static\"\n # from debs created by CI build, for example:\n # docker build . --network host --build-arg version=\"21.4.1.6282\" --build-arg deb_location_url=\"https://...\" -t ...\n ARG deb_location_url=\"\"\n+ARG DIRECT_DOWNLOAD_URLS=\"\"\n \n # set non-empty single_binary_location_url to create docker image\n # from a single binary url (useful for non-standard builds - with sanitizers, for arm64).\n@@ -44,6 +45,18 @@ ARG single_binary_location_url=\"\"\n \n ARG TARGETARCH\n \n+# install from direct URL\n+RUN if [ -n \"${DIRECT_DOWNLOAD_URLS}\" ]; then \\\n+        echo \"installing from custom predefined urls with deb packages: ${DIRECT_DOWNLOAD_URLS}\" \\\n+        && rm -rf /tmp/clickhouse_debs \\\n+        && mkdir -p /tmp/clickhouse_debs \\\n+        && for url in $DIRECT_DOWNLOAD_URLS; do \\\n+            wget --progress=bar:force:noscroll \"$url\" -P /tmp/clickhouse_debs || exit 1 \\\n+        ; done \\\n+        && dpkg -i /tmp/clickhouse_debs/*.deb \\\n+        && rm -rf /tmp/* ; \\\n+    fi\n+\n # install from a web location with deb packages\n RUN arch=\"${TARGETARCH:-amd64}\" \\\n     && if [ -n \"${deb_location_url}\" ]; then \\\ndiff --git a/utils/check-style/check-typos b/utils/check-style/check-typos\nindex 9194a9464a78..0486efb37b1b 100755\n--- a/utils/check-style/check-typos\n+++ b/utils/check-style/check-typos\n@@ -4,6 +4,7 @@\n \n ROOT_PATH=$(git rev-parse --show-toplevel)\n \n+#FIXME: check all (or almost all) repo\n codespell \\\n     --skip \"*generated*,*gperf*,*.bin,*.mrk*,*.idx,checksums.txt,*.dat,*.pyc,*.kate-swp,*obfuscateQueries.cpp,d3-*.js,*.min.js,*.sum,${ROOT_PATH}/utils/check-style/aspell-ignore\" \\\n     --ignore-words \"${ROOT_PATH}/utils/check-style/codespell-ignore-words.list\" \\\n",
  "test_patch": "diff --git a/.github/workflows/reusable_test.yml b/.github/workflows/reusable_test.yml\nindex e82d2d515963..09177ad887a9 100644\n--- a/.github/workflows/reusable_test.yml\n+++ b/.github/workflows/reusable_test.yml\n@@ -14,13 +14,10 @@ name: Testing workflow\n         required: true\n         type: string\n       run_command:\n-        description: the command to launch the check. Usually starts with `cd '$REPO_COPY/tests/ci'`\n-        required: true\n+        description: the command to launch the check\n+        default: \"\"\n+        required: false\n         type: string\n-      batches:\n-        description: how many batches for the test will be launched\n-        default: 1\n-        type: number\n       checkout_depth:\n         description: the value of the git shallow checkout\n         required: false\n@@ -34,80 +31,89 @@ name: Testing workflow\n       additional_envs:\n         description: additional ENV variables to setup the job\n         type: string\n+      data:\n+        description: ci data\n+        type: string\n+        required: true\n+      working-directory:\n+        description: sets custom working directory\n+        type: string\n+        default: \"\"\n     secrets:\n       secret_envs:\n         description: if given, it's passed to the environments\n         required: false\n \n+\n env:\n   # Force the stdout and stderr streams to be unbuffered\n   PYTHONUNBUFFERED: 1\n   CHECK_NAME: ${{inputs.test_name}}\n \n jobs:\n-  PrepareStrategy:\n-    # batches < 1 is misconfiguration,\n-    # and we need this step only for batches > 1\n-    if: ${{ inputs.batches > 1 }}\n-    runs-on: [self-hosted, style-checker-aarch64]\n-    outputs:\n-      batches: ${{steps.batches.outputs.batches}}\n-    steps:\n-      - name: Calculate batches\n-        id: batches\n-        run: |\n-          batches_output=$(python3 -c 'import json; print(json.dumps(list(range(${{inputs.batches}}))))')\n-          echo \"batches=${batches_output}\" >> \"$GITHUB_OUTPUT\"\n   Test:\n-    # If PrepareStrategy is skipped for batches == 1,\n-    # we still need to launch the test.\n-    # `! failure()` is mandatory here to launch on skipped Job\n-    # `&& !cancelled()` to allow the be cancelable\n-    if: ${{ ( !failure() && !cancelled() ) && inputs.batches > 0 }}\n-    # Do not add `-0` to the end, if there's only one batch\n-    name: ${{inputs.test_name}}${{ inputs.batches > 1 && format('-{0}',matrix.batch) || '' }}\n-    env:\n-      GITHUB_JOB_OVERRIDDEN: ${{inputs.test_name}}${{ inputs.batches > 1 && format('-{0}',matrix.batch) || '' }}\n     runs-on: [self-hosted, '${{inputs.runner_type}}']\n-    needs: [PrepareStrategy]\n+    if: ${{ !failure() && !cancelled() && contains(fromJson(inputs.data).jobs_data.jobs_to_do, inputs.test_name) }}\n+    name: ${{inputs.test_name}}${{ fromJson(inputs.data).jobs_data.jobs_params[inputs.test_name].num_batches > 1 && format('-{0}',matrix.batch) || '' }}\n+    env:\n+      GITHUB_JOB_OVERRIDDEN: ${{inputs.test_name}}${{ fromJson(inputs.data).jobs_data.jobs_params[inputs.test_name].num_batches > 1 && format('-{0}',matrix.batch) || '' }}\n     strategy:\n       fail-fast: false  # we always wait for entire matrix\n       matrix:\n-        # if PrepareStrategy does not have batches, we use 0\n-        batch: ${{ needs.PrepareStrategy.outputs.batches\n-          && fromJson(needs.PrepareStrategy.outputs.batches)\n-          || fromJson('[0]')}}\n+        batch: ${{ fromJson(inputs.data).jobs_data.jobs_params[inputs.test_name].batches }}\n     steps:\n       - name: Check out repository code\n         uses: ClickHouse/checkout@v1\n         with:\n           clear-repository: true\n+          ref: ${{ fromJson(inputs.data).git_ref }}\n           submodules: ${{inputs.submodules}}\n           fetch-depth: ${{inputs.checkout_depth}}\n           filter: tree:0\n       - name: Set build envs\n         run: |\n           cat >> \"$GITHUB_ENV\" << 'EOF'\n+          CHECK_NAME=${{ inputs.test_name }}\n           ${{inputs.additional_envs}}\n           ${{secrets.secret_envs}}\n+          DOCKER_TAG<<DOCKER_JSON\n+          ${{ toJson(fromJson(inputs.data).docker_data.images) }}\n+          DOCKER_JSON\n           EOF\n       - name: Common setup\n         uses: ./.github/actions/common_setup\n         with:\n           job_type: test\n-      - name: Download json reports\n-        uses: actions/download-artifact@v3\n-        with:\n-          path: ${{ env.REPORTS_PATH }}\n       - name: Setup batch\n-        if: ${{ inputs.batches > 1}}\n+        if: ${{ fromJson(inputs.data).jobs_data.jobs_params[inputs.test_name].num_batches > 1 }}\n         run: |\n           cat >> \"$GITHUB_ENV\" << 'EOF'\n           RUN_BY_HASH_NUM=${{matrix.batch}}\n-          RUN_BY_HASH_TOTAL=${{inputs.batches}}\n+          RUN_BY_HASH_TOTAL=${{ fromJson(inputs.data).jobs_data.jobs_params[inputs.test_name].num_batches }}\n           EOF\n-      - name: Run test\n-        run: ${{inputs.run_command}}\n+      - name: Pre run\n+        run: |\n+          python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ toJson(inputs.data) }} --pre --job-name '${{inputs.test_name}}'\n+      - name: Run\n+        run: |\n+          if [ -n \"${{ inputs.working-directory }}\" ]; then\n+            cd \"${{ inputs.working-directory }}\"\n+          else\n+            cd \"$GITHUB_WORKSPACE/tests/ci\"\n+          fi\n+          if [ -n \"$(echo '${{ inputs.run_command }}' | tr -d '\\n')\" ]; then\n+            echo \"Running command from workflow input\"\n+            ${{ inputs.run_command }}\n+          else\n+            echo \"Running command from job config\"\n+            python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ toJson(inputs.data) }} --run --job-name '${{inputs.test_name}}'\n+          fi\n+      - name: Post run\n+        run: |\n+          python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ toJson(inputs.data) }} --post --job-name '${{inputs.test_name}}'\n+      - name: Mark as done\n+        run: |\n+          python3 \"$GITHUB_WORKSPACE/tests/ci/ci.py\" --infile ${{ toJson(inputs.data) }} --mark-success --job-name '${{inputs.test_name}}' --batch ${{matrix.batch}}\n       - name: Clean\n         if: always()\n         uses: ./.github/actions/clean\ndiff --git a/docker/test/performance-comparison/Dockerfile b/docker/test/performance-comparison/Dockerfile\nindex d31663f90711..e4ced104445d 100644\n--- a/docker/test/performance-comparison/Dockerfile\n+++ b/docker/test/performance-comparison/Dockerfile\n@@ -39,18 +39,8 @@ RUN apt-get update \\\n     && apt-get clean \\\n     && rm -rf /var/lib/apt/lists/*\n \n-COPY * /\n+COPY run.sh /\n \n-# Bind everything to one NUMA node, if there's more than one. Theoretically the\n-# node #0 should be less stable because of system interruptions. We bind\n-# randomly to node 1 or 0 to gather some statistics on that. We have to bind\n-# both servers and the tmpfs on which the database is stored. How to do it\n-# is unclear, but by default tmpfs uses\n-# 'process allocation policy', not sure which process but hopefully the one that\n-# writes to it, so just bind the downloader script as well.\n-# https://www.kernel.org/doc/Documentation/filesystems/tmpfs.txt\n-# Double-escaped backslashes are a tribute to the engineering wonder of docker --\n-# it gives '/bin/sh: 1: [bash,: not found' otherwise.\n-CMD [\"bash\", \"-c\", \"node=$((RANDOM % $(numactl --hardware | sed -n 's/^.*available:\\\\(.*\\\\)nodes.*$/\\\\1/p'))); echo Will bind to NUMA node $node; numactl --cpunodebind=$node --membind=$node /entrypoint.sh\"]\n+CMD [\"bash\", \"/run.sh\"]\n \n # docker run --network=host --volume <workspace>:/workspace --volume=<output>:/output -e PR_TO_TEST=<> -e SHA_TO_TEST=<> clickhouse/performance-comparison\ndiff --git a/docker/test/performance-comparison/run.sh b/docker/test/performance-comparison/run.sh\nnew file mode 100644\nindex 000000000000..7afb5da59b18\n--- /dev/null\n+++ b/docker/test/performance-comparison/run.sh\n@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+entry=\"/usr/share/clickhouse-test/performance/scripts/entrypoint.sh\"\n+[ ! -e \"$entry\" ] && echo \"ERROR: test scripts are not found\" && exit 1\n+\n+# Bind everything to one NUMA node, if there's more than one. Theoretically the\n+# node #0 should be less stable because of system interruptions. We bind\n+# randomly to node 1 or 0 to gather some statistics on that. We have to bind\n+# both servers and the tmpfs on which the database is stored. How to do it\n+# is unclear, but by default tmpfs uses\n+# 'process allocation policy', not sure which process but hopefully the one that\n+# writes to it, so just bind the downloader script as well.\n+# https://www.kernel.org/doc/Documentation/filesystems/tmpfs.txt\n+# Double-escaped backslashes are a tribute to the engineering wonder of docker --\n+# it gives '/bin/sh: 1: [bash,: not found' otherwise.\n+node=$(( RANDOM % $(numactl --hardware | sed -n 's/^.*available:\\(.*\\)nodes.*$/\\1/p') ));\n+echo Will bind to NUMA node $node;\n+numactl --cpunodebind=$node --membind=$node $entry\ndiff --git a/tests/ci/artifacts_helper.py b/tests/ci/artifacts_helper.py\nindex 0d1ecafbaa53..a9f3385585bd 100644\n--- a/tests/ci/artifacts_helper.py\n+++ b/tests/ci/artifacts_helper.py\n@@ -123,9 +123,7 @@ def ignore(key: str) -> bool:\n                 return fnmatch(key, glob)\n             return True\n \n-        results = filter(\n-            ignore, self.s3_helper.list_prefix(self.s3_prefix, S3_BUILDS_BUCKET)\n-        )\n+        results = filter(ignore, self.s3_helper.list_prefix(self.s3_prefix))\n         return list(results)\n \n     @staticmethod\ndiff --git a/tests/ci/ast_fuzzer_check.py b/tests/ci/ast_fuzzer_check.py\nindex 620462991efc..68004eec2bbe 100644\n--- a/tests/ci/ast_fuzzer_check.py\n+++ b/tests/ci/ast_fuzzer_check.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n \n import logging\n+import os\n import subprocess\n import sys\n from pathlib import Path\n@@ -19,11 +20,8 @@\n     get_commit,\n     post_commit_status,\n )\n-from docker_pull_helper import DockerImage, get_image_with_version\n-from env_helper import (\n-    REPORTS_PATH,\n-    TEMP_PATH,\n-)\n+from docker_images_helper import DockerImage, get_docker_image, pull_image\n+from env_helper import REPORT_PATH, TEMP_PATH\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import TestResult\n@@ -69,10 +67,13 @@ def main():\n     stopwatch = Stopwatch()\n \n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n-    reports_path = Path(REPORTS_PATH)\n \n-    check_name = sys.argv[1]\n+    check_name = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n \n     pr_info = PRInfo()\n \n@@ -84,7 +85,7 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(reports_path, IMAGE_NAME)\n+    docker_image = pull_image(get_docker_image(IMAGE_NAME))\n \n     build_name = get_build_name_for_check(check_name)\n     urls = read_build_urls(build_name, reports_path)\n@@ -208,7 +209,9 @@ def main():\n \n     logging.info(\"Result: '%s', '%s', '%s'\", status, description, report_url)\n     print(f\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, status, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, status, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n \n \n if __name__ == \"__main__\":\ndiff --git a/tests/ci/bugfix_validate_check.py b/tests/ci/bugfix_validate_check.py\nindex 574b119fda3f..adb798bd3926 100644\n--- a/tests/ci/bugfix_validate_check.py\n+++ b/tests/ci/bugfix_validate_check.py\n@@ -82,19 +82,21 @@ def main():\n \n     is_ok, test_results = process_all_results(status_files)\n \n+    pr_info = PRInfo()\n     if not test_results:\n+        description = \"No results to upload\"\n+        report_url = \"\"\n         logging.info(\"No results to upload\")\n-        return\n-\n-    pr_info = PRInfo()\n-    report_url = upload_results(\n-        S3Helper(),\n-        pr_info.number,\n-        pr_info.sha,\n-        test_results,\n-        status_files,\n-        check_name_with_group,\n-    )\n+    else:\n+        description = \"\" if is_ok else \"Changed tests don't reproduce the bug\"\n+        report_url = upload_results(\n+            S3Helper(),\n+            pr_info.number,\n+            pr_info.sha,\n+            test_results,\n+            status_files,\n+            check_name_with_group,\n+        )\n \n     gh = Github(get_best_robot_token(), per_page=100)\n     commit = get_commit(gh, pr_info.sha)\n@@ -102,9 +104,10 @@ def main():\n         commit,\n         \"success\" if is_ok else \"error\",\n         report_url,\n-        \"\" if is_ok else \"Changed tests don't reproduce the bug\",\n+        description,\n         check_name_with_group,\n         pr_info,\n+        dump_to_file=True,\n     )\n \n \ndiff --git a/tests/ci/build_check.py b/tests/ci/build_check.py\nindex adbd67bd95a5..f5181f4c843d 100644\n--- a/tests/ci/build_check.py\n+++ b/tests/ci/build_check.py\n@@ -1,5 +1,6 @@\n #!/usr/bin/env python3\n \n+import argparse\n from pathlib import Path\n from typing import Tuple\n import subprocess\n@@ -9,10 +10,9 @@\n \n from ci_config import CI_CONFIG, BuildConfig\n from ccache_utils import CargoCache\n-from docker_pull_helper import get_image_with_version\n+\n from env_helper import (\n     GITHUB_JOB_API_URL,\n-    IMAGES_PATH,\n     REPO_COPY,\n     S3_BUILDS_BUCKET,\n     S3_DOWNLOAD,\n@@ -23,6 +23,7 @@\n from report import BuildResult, FAILURE, StatusType, SUCCESS\n from s3_helper import S3Helper\n from tee_popen import TeePopen\n+import docker_images_helper\n from version_helper import (\n     ClickHouseVersion,\n     get_version_from_repo,\n@@ -223,11 +224,22 @@ def upload_master_static_binaries(\n     print(f\"::notice ::Binary static URL (compact): {url_compact}\")\n \n \n+def parse_args() -> argparse.Namespace:\n+    parser = argparse.ArgumentParser(\"Clickhouse builder script\")\n+    parser.add_argument(\n+        \"build_name\",\n+        help=\"build name\",\n+    )\n+    return parser.parse_args()\n+\n+\n def main():\n     logging.basicConfig(level=logging.INFO)\n \n+    args = parse_args()\n+\n     stopwatch = Stopwatch()\n-    build_name = sys.argv[1]\n+    build_name = args.build_name\n \n     build_config = CI_CONFIG.build_config[build_name]\n \n@@ -250,15 +262,13 @@ def main():\n         (performance_pr, pr_info.sha, build_name, \"performance.tar.zst\")\n     )\n \n+    # FIXME: to be removed in favor of \"skip by job digest\"\n     # If this is rerun, then we try to find already created artifacts and just\n     # put them as github actions artifact (result)\n     # The s3_path_prefix has additional \"/\" in the end to prevent finding\n     # e.g. `binary_darwin_aarch64/clickhouse` for `binary_darwin`\n     check_for_success_run(s3_helper, f\"{s3_path_prefix}/\", build_name, version)\n \n-    docker_image = get_image_with_version(IMAGES_PATH, IMAGE_NAME)\n-    image_version = docker_image.version\n-\n     logging.info(\"Got version from repo %s\", version.string)\n \n     official_flag = pr_info.number == 0\n@@ -281,13 +291,17 @@ def main():\n     )\n     cargo_cache.download()\n \n+    docker_image = docker_images_helper.pull_image(\n+        docker_images_helper.get_docker_image(IMAGE_NAME)\n+    )\n+\n     packager_cmd = get_packager_cmd(\n         build_config,\n         repo_path / \"docker\" / \"packager\",\n         build_output_path,\n         cargo_cache.directory,\n         version.string,\n-        image_version,\n+        docker_image.version,\n         official_flag,\n     )\n \ndiff --git a/tests/ci/build_report_check.py b/tests/ci/build_report_check.py\nindex d6368d7d3b33..755217f89b50 100644\n--- a/tests/ci/build_report_check.py\n+++ b/tests/ci/build_report_check.py\n@@ -6,6 +6,7 @@\n import sys\n import atexit\n from pathlib import Path\n+from typing import List\n \n from github import Github\n \n@@ -13,8 +14,8 @@\n     GITHUB_JOB_URL,\n     GITHUB_REPOSITORY,\n     GITHUB_SERVER_URL,\n-    REPORTS_PATH,\n     TEMP_PATH,\n+    REPORT_PATH,\n )\n from report import (\n     BuildResult,\n@@ -26,7 +27,7 @@\n )\n from s3_helper import S3Helper\n from get_robot_token import get_best_robot_token\n-from pr_info import NeedsDataType, PRInfo\n+from pr_info import PRInfo\n from commit_status_helper import (\n     RerunHelper,\n     format_description,\n@@ -46,32 +47,32 @@\n def main():\n     logging.basicConfig(level=logging.INFO)\n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n \n-    logging.info(\"Reports path %s\", REPORTS_PATH)\n-    reports_path = Path(REPORTS_PATH)\n     logging.info(\n         \"Reports found:\\n %s\",\n         \"\\n \".join(p.as_posix() for p in reports_path.rglob(\"*.json\")),\n     )\n \n     build_check_name = sys.argv[1]\n-    needs_data = {}  # type: NeedsDataType\n+    needs_data: List[str] = []\n     required_builds = 0\n-    if os.path.exists(NEEDS_DATA_PATH):\n-        with open(NEEDS_DATA_PATH, \"rb\") as file_handler:\n-            needs_data = json.load(file_handler)\n \n     if NEEDS_DATA:\n         needs_data = json.loads(NEEDS_DATA)\n+        # drop non build jobs if any\n+        needs_data = [d for d in needs_data if \"Build\" in d]\n+    elif os.path.exists(NEEDS_DATA_PATH):\n+        with open(NEEDS_DATA_PATH, \"rb\") as file_handler:\n+            needs_data = list(json.load(file_handler).keys())\n+    else:\n+        assert False, \"NEEDS_DATA env var required\"\n \n     required_builds = len(needs_data)\n \n     if needs_data:\n         logging.info(\"The next builds are required: %s\", \", \".join(needs_data))\n-        if all(i[\"result\"] == \"skipped\" for i in needs_data.values()):\n-            logging.info(\"All builds are skipped, exiting\")\n-            sys.exit(0)\n \n     gh = Github(get_best_robot_token(), per_page=100)\n     pr_info = PRInfo()\n@@ -84,14 +85,13 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    builds_for_check = CI_CONFIG.builds_report_config[build_check_name]\n+    builds_for_check = CI_CONFIG.get_builds_for_report(build_check_name)\n     required_builds = required_builds or len(builds_for_check)\n \n     # Collect reports from json artifacts\n     build_results = []\n     for build_name in builds_for_check:\n-        report_name = BuildResult.get_report_name(build_name).stem\n-        build_result = BuildResult.read_json(reports_path / report_name, build_name)\n+        build_result = BuildResult.read_json(reports_path, build_name)\n         if build_result.is_missing:\n             logging.warning(\"Build results for %s are missing\", build_name)\n             continue\n@@ -179,7 +179,13 @@ def main():\n     )\n \n     post_commit_status(\n-        commit, summary_status, url, description, build_check_name, pr_info\n+        commit,\n+        summary_status,\n+        url,\n+        description,\n+        build_check_name,\n+        pr_info,\n+        dump_to_file=True,\n     )\n \n     if summary_status == ERROR:\ndiff --git a/tests/ci/ci.py b/tests/ci/ci.py\nnew file mode 100644\nindex 000000000000..4cbb4ef353ea\n--- /dev/null\n+++ b/tests/ci/ci.py\n@@ -0,0 +1,738 @@\n+import argparse\n+import json\n+import os\n+import concurrent.futures\n+from pathlib import Path\n+import re\n+import subprocess\n+import sys\n+from typing import Any, Dict, Iterable, List, Optional\n+\n+from github import Github\n+from s3_helper import S3Helper\n+from digest_helper import DockerDigester, JobDigester\n+import docker_images_helper\n+from env_helper import (\n+    CI,\n+    ROOT_DIR,\n+    S3_BUILDS_BUCKET,\n+    TEMP_PATH,\n+    REPORT_PATH,\n+)\n+from commit_status_helper import CommitStatusData, get_commit, set_status_comment\n+from get_robot_token import get_best_robot_token\n+from pr_info import PRInfo\n+from ci_config import CI_CONFIG\n+from git_helper import Git, Runner as GitRunner, GIT_PREFIX\n+from report import BuildResult\n+from version_helper import get_version_from_repo\n+\n+\n+def get_check_name(check_name: str, batch: int, num_batches: int) -> str:\n+    res = check_name\n+    if num_batches > 1:\n+        res = f\"{check_name} [{batch+1}/{num_batches}]\"\n+    return res\n+\n+\n+def normalize_check_name(check_name: str) -> str:\n+    res = check_name.lower()\n+    for r in ((\" \", \"_\"), (\"(\", \"_\"), (\")\", \"_\"), (\",\", \"_\"), (\"/\", \"_\")):\n+        res = res.replace(*r)\n+    return res\n+\n+\n+def is_build_job(job: str) -> bool:\n+    if \"package_\" in job or \"binary_\" in job or job == \"fuzzers\":\n+        return True\n+    return False\n+\n+\n+def is_test_job(job: str) -> bool:\n+    return not is_build_job(job) and not \"Style\" in job and not \"Docs check\" in job\n+\n+\n+def is_docs_job(job: str) -> bool:\n+    return \"Docs check\" in job\n+\n+\n+def parse_args(parser: argparse.ArgumentParser) -> argparse.Namespace:\n+    # FIXME: consider switching to sub_parser for configure, pre, run, post actions\n+    parser.add_argument(\n+        \"--configure\",\n+        action=\"store_true\",\n+        help=\"Action that configures ci run. Calculates digests, checks job to be executed, generates json output\",\n+    )\n+    parser.add_argument(\n+        \"--update-gh-statuses\",\n+        action=\"store_true\",\n+        help=\"Action that recreate success GH statuses for jobs that finished successfully in past and will be skipped this time\",\n+    )\n+    parser.add_argument(\n+        \"--pre\",\n+        action=\"store_true\",\n+        help=\"Action that executes prerequesetes for the job provided in --job-name\",\n+    )\n+    parser.add_argument(\n+        \"--run\",\n+        action=\"store_true\",\n+        help=\"Action that executes run action for specified --job-name. run_command must be configured for a given job name.\",\n+    )\n+    parser.add_argument(\n+        \"--post\",\n+        action=\"store_true\",\n+        help=\"Action that executes post actions for the job provided in --job-name\",\n+    )\n+    parser.add_argument(\n+        \"--mark-success\",\n+        action=\"store_true\",\n+        help=\"Action that marks job provided in --job-name (with batch provided in --batch) as successfull\",\n+    )\n+    parser.add_argument(\n+        \"--job-name\",\n+        default=\"\",\n+        type=str,\n+        help=\"Job name as in config\",\n+    )\n+    parser.add_argument(\n+        \"--batch\",\n+        default=-1,\n+        type=int,\n+        help=\"Current batch number (required for --mark-success), -1 or omit for single-batch job\",\n+    )\n+    parser.add_argument(\n+        \"--infile\",\n+        default=\"\",\n+        type=str,\n+        help=\"Input json file or json string with ci run config\",\n+    )\n+    parser.add_argument(\n+        \"--outfile\",\n+        default=\"\",\n+        type=str,\n+        required=False,\n+        help=\"otput file to write json result to, if not set - stdout\",\n+    )\n+    parser.add_argument(\n+        \"--pretty\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"makes json output pretty formated\",\n+    )\n+    parser.add_argument(\n+        \"--skip-docker\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"skip fetching docker data from dockerhub, used in --configure action (for debugging)\",\n+    )\n+    parser.add_argument(\n+        \"--docker-digest-or-latest\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"temporary hack to fallback to latest if image with digest as a tag is not on docker hub\",\n+    )\n+    parser.add_argument(\n+        \"--skip-jobs\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"skip fetching data about job runs, used in --configure action (for debugging)\",\n+    )\n+    parser.add_argument(\n+        \"--rebuild-all-docker\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"will create run config for rebuilding all dockers, used in --configure action (for nightly docker job)\",\n+    )\n+    parser.add_argument(\n+        \"--rebuild-all-binaries\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"will create run config without skipping build jobs in any case, used in --configure action (for release branches)\",\n+    )\n+    return parser.parse_args()\n+\n+\n+def get_file_flag_name(\n+    job_name: str, digest: str, batch: int = 0, num_batches: int = 1\n+) -> str:\n+    if num_batches < 2:\n+        return f\"job_{job_name}_{digest}.ci\"\n+    else:\n+        return f\"job_{job_name}_{digest}_{batch}_{num_batches}.ci\"\n+\n+\n+def get_s3_path(build_digest: str) -> str:\n+    return f\"CI_data/BUILD-{build_digest}/\"\n+\n+\n+def get_s3_path_docs(digest: str) -> str:\n+    return f\"CI_data/DOCS-{digest}/\"\n+\n+\n+def check_missing_images_on_dockerhub(\n+    image_name_tag: Dict[str, str], arch: Optional[str] = None\n+) -> Dict[str, str]:\n+    \"\"\"\n+    Checks missing images on dockerhub.\n+    Works concurrently for all given images.\n+    Docker must be logged in.\n+    \"\"\"\n+\n+    def run_docker_command(\n+        image: str, image_digest: str, arch: Optional[str] = None\n+    ) -> Dict:\n+        \"\"\"\n+        aux command for fetching single docker manifest\n+        \"\"\"\n+        command = [\n+            \"docker\",\n+            \"manifest\",\n+            \"inspect\",\n+            f\"{image}:{image_digest}\" if not arch else f\"{image}:{image_digest}-{arch}\",\n+        ]\n+\n+        process = subprocess.run(\n+            command,\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE,\n+            text=True,\n+            check=False,\n+        )\n+\n+        return {\n+            \"image\": image,\n+            \"image_digest\": image_digest,\n+            \"arch\": arch,\n+            \"stdout\": process.stdout,\n+            \"stderr\": process.stderr,\n+            \"return_code\": process.returncode,\n+        }\n+\n+    result: Dict[str, str] = {}\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        futures = [\n+            executor.submit(run_docker_command, image, tag, arch)\n+            for image, tag in image_name_tag.items()\n+        ]\n+\n+        responses = [\n+            future.result() for future in concurrent.futures.as_completed(futures)\n+        ]\n+        for resp in responses:\n+            name, stdout, stderr, digest, arch = (\n+                resp[\"image\"],\n+                resp[\"stdout\"],\n+                resp[\"stderr\"],\n+                resp[\"image_digest\"],\n+                resp[\"arch\"],\n+            )\n+            if stderr:\n+                if stderr.startswith(\"no such manifest\"):\n+                    result[name] = digest\n+                else:\n+                    print(f\"Error: Unknown error: {stderr}, {name}, {arch}\")\n+            elif stdout:\n+                if \"mediaType\" in stdout:\n+                    pass\n+                else:\n+                    print(f\"Error: Unknown response: {stdout}\")\n+                    assert False, \"FIXME\"\n+            else:\n+                print(f\"Error: No response for {name}, {digest}, {arch}\")\n+                assert False, \"FIXME\"\n+    return result\n+\n+\n+def _check_and_update_for_early_style_check(run_config: dict) -> None:\n+    \"\"\"\n+    This is temporary hack to start style check before docker build if possible\n+    FIXME: need better solution to do style check as soon as possible and as fast as possible w/o dependency on docker job\n+    \"\"\"\n+    jobs_to_do = run_config.get(\"jobs_data\", {}).get(\"jobs_to_do\", [])\n+    docker_to_build = run_config.get(\"docker_data\", {}).get(\"missing_multi\", [])\n+    if (\n+        \"Style check\" in jobs_to_do\n+        and docker_to_build\n+        and \"clickhouse/style-test\" not in docker_to_build\n+    ):\n+        index = jobs_to_do.index(\"Style check\")\n+        jobs_to_do[index] = \"Style check early\"\n+\n+\n+def _configure_docker_jobs(\n+    rebuild_all_dockers: bool, docker_digest_or_latest: bool = False\n+) -> Dict:\n+    # generate docker jobs data\n+    docker_digester = DockerDigester()\n+    imagename_digest_dict = (\n+        docker_digester.get_all_digests()\n+    )  # 'image name - digest' mapping\n+    images_info = docker_images_helper.get_images_info()\n+\n+    # a. check missing images\n+    print(\"Start checking missing images in dockerhub\")\n+    # FIXME: we need login as docker manifest inspect goes directly to one of the *.docker.com hosts instead of \"registry-mirrors\" : [\"http://dockerhub-proxy.dockerhub-proxy-zone:5000\"]\n+    #         find if it's possible to use the setting of /etc/docker/daemon.json\n+    docker_images_helper.docker_login()\n+    if not rebuild_all_dockers:\n+        missing_multi_dict = check_missing_images_on_dockerhub(imagename_digest_dict)\n+        missing_multi = list(missing_multi_dict)\n+        missing_amd64 = []\n+        missing_aarch64 = []\n+        if not docker_digest_or_latest:\n+            # look for missing arm and amd images only among missing multiarch manifests @missing_multi_dict\n+            # to avoid extra dockerhub api calls\n+            missing_amd64 = list(\n+                check_missing_images_on_dockerhub(missing_multi_dict, \"amd64\")\n+            )\n+            # FIXME: WA until full arm support: skip not supported arm images\n+            missing_aarch64 = list(\n+                check_missing_images_on_dockerhub(\n+                    {\n+                        im: digest\n+                        for im, digest in missing_multi_dict.items()\n+                        if not images_info[im][\"only_amd64\"]\n+                    },\n+                    \"aarch64\",\n+                )\n+            )\n+    else:\n+        # add all images to missing\n+        missing_multi = list(imagename_digest_dict)\n+        missing_amd64 = missing_multi\n+        # FIXME: WA until full arm support: skip not supported arm images\n+        missing_aarch64 = [\n+            name\n+            for name in imagename_digest_dict\n+            if not images_info[name][\"only_amd64\"]\n+        ]\n+    # FIXME: temporary hack, remove after transition to docker digest as tag\n+    if docker_digest_or_latest:\n+        if missing_multi:\n+            print(\n+                f\"WARNING: Missing images {list(missing_multi)} - fallback to latest tag\"\n+            )\n+            for image in missing_multi:\n+                imagename_digest_dict[image] = \"latest\"\n+\n+    print(\"...checking missing images in dockerhub - done\")\n+    return {\n+        \"images\": imagename_digest_dict,\n+        \"missing_aarch64\": missing_aarch64,\n+        \"missing_amd64\": missing_amd64,\n+        \"missing_multi\": missing_multi,\n+    }\n+\n+\n+def _configure_jobs(\n+    build_digest: str,\n+    docs_digest: str,\n+    job_digester: JobDigester,\n+    s3: S3Helper,\n+    rebuild_all_binaries: bool,\n+    pr_labels: Iterable[str],\n+    commit_tokens: List[str],\n+) -> Dict:\n+    # a. digest each item from the config\n+    job_digester = JobDigester()\n+    jobs_params: Dict[str, Dict] = {}\n+    jobs_to_do: List[str] = []\n+    jobs_to_skip: List[str] = []\n+    digests: Dict[str, str] = {}\n+    print(\"Calculating job digests - start\")\n+    for job in CI_CONFIG.job_generator():\n+        digest = job_digester.get_job_digest(CI_CONFIG.get_digest_config(job))\n+        digests[job] = digest\n+        print(f\"    job [{job.rjust(50)}] has digest [{digest}]\")\n+    print(\"Calculating job digests - done\")\n+\n+    # b. check if we have something done\n+    path = get_s3_path(build_digest)\n+    done_files = s3.list_prefix(path)\n+    done_files = [file.split(\"/\")[-1] for file in done_files]\n+    print(f\"S3 CI files for the build [{build_digest}]: {done_files}\")\n+    docs_path = get_s3_path_docs(docs_digest)\n+    done_files_docs = s3.list_prefix(docs_path)\n+    done_files_docs = [file.split(\"/\")[-1] for file in done_files_docs]\n+    print(f\"S3 CI files for the docs [{docs_digest}]: {done_files_docs}\")\n+    done_files += done_files_docs\n+    for job in digests:\n+        digest = digests[job]\n+        job_config = CI_CONFIG.get_job_config(job)\n+        num_batches: int = job_config.num_batches\n+        batches_to_do: List[int] = []\n+\n+        if job_config.run_by_label:\n+            # this job controled by label, add to todo if it's labe is set in pr\n+            if job_config.run_by_label in pr_labels:\n+                for batch in range(num_batches):  # type: ignore\n+                    batches_to_do.append(batch)\n+        else:\n+            # this job controled by digest, add to todo if it's not successfully done before\n+            for batch in range(num_batches):  # type: ignore\n+                success_flag_name = get_file_flag_name(job, digest, batch, num_batches)\n+                if success_flag_name not in done_files or (\n+                    rebuild_all_binaries and is_build_job(job)\n+                ):\n+                    batches_to_do.append(batch)\n+\n+        if batches_to_do:\n+            jobs_to_do.append(job)\n+            jobs_params[job] = {\n+                \"batches\": batches_to_do,\n+                \"num_batches\": num_batches,\n+            }\n+        else:\n+            jobs_to_skip += (job,)\n+\n+    if commit_tokens:\n+        requested_jobs = [\n+            token[len(\"#job_\") :]\n+            for token in commit_tokens\n+            if token.startswith(\"#job_\")\n+        ]\n+        assert any(\n+            len(x) > 1 for x in requested_jobs\n+        ), f\"Invalid job names requested [{requested_jobs}]\"\n+        if requested_jobs:\n+            jobs_to_do_requested = []\n+            for job in requested_jobs:\n+                job_with_parents = CI_CONFIG.get_job_with_parents(job)\n+                # always add requested job itself, even if it could be skipped\n+                jobs_to_do_requested.append(job_with_parents[0])\n+                for parent in job_with_parents[1:]:\n+                    if parent in jobs_to_do and parent not in jobs_to_do_requested:\n+                        jobs_to_do_requested.append(parent)\n+            print(\n+                f\"NOTE: Only specific job(s) were requested: [{jobs_to_do_requested}]\"\n+            )\n+            jobs_to_do = jobs_to_do_requested\n+\n+    return {\n+        \"digests\": digests,\n+        \"jobs_to_do\": jobs_to_do,\n+        \"jobs_to_skip\": jobs_to_skip,\n+        \"jobs_params\": jobs_params,\n+    }\n+\n+\n+def _update_gh_statuses(indata: Dict, s3: S3Helper) -> None:\n+    # This action is required to re-create all GH statuses for skiped jobs, so that ci report can be generated afterwards\n+    temp_path = Path(TEMP_PATH)\n+    if not temp_path.exists():\n+        temp_path.mkdir(parents=True, exist_ok=True)\n+\n+    # clean up before start\n+    for file in temp_path.glob(\"*.ci\"):\n+        file.unlink()\n+\n+    # download all metadata files\n+    path = get_s3_path(indata[\"build\"])\n+    files = s3.download_files(  # type: ignore\n+        bucket=S3_BUILDS_BUCKET,\n+        s3_path=path,\n+        file_suffix=\".ci\",\n+        local_directory=temp_path,\n+    )\n+    print(f\"CI metadata files [{files}]\")\n+    path = get_s3_path_docs(indata[\"docs\"])\n+    files_docs = s3.download_files(  # type: ignore\n+        bucket=S3_BUILDS_BUCKET,\n+        s3_path=path,\n+        file_suffix=\".ci\",\n+        local_directory=temp_path,\n+    )\n+    print(f\"CI docs metadata files [{files_docs}]\")\n+    files += files_docs\n+\n+    # parse CI metadata\n+    job_digests = indata[\"jobs_data\"][\"digests\"]\n+    # create GH status\n+    pr_info = PRInfo()\n+    commit = get_commit(Github(get_best_robot_token(), per_page=100), pr_info.sha)\n+\n+    def run_create_status(job, digest, batch, num_batches):\n+        success_flag_name = get_file_flag_name(job, digest, batch, num_batches)\n+        if success_flag_name in files:\n+            print(f\"Going to re-create GH status for job [{job}] sha [{pr_info.sha}]\")\n+            job_status = CommitStatusData.load_from_file(\n+                f\"{TEMP_PATH}/{success_flag_name}\"\n+            )  # type: CommitStatusData\n+            assert job_status.status == \"success\", \"BUG!\"\n+            commit.create_status(\n+                state=job_status.status,\n+                target_url=job_status.report_url,\n+                description=f\"Reused from [{job_status.pr_num}-{job_status.sha[0:8]}]: {job_status.description}\",\n+                context=get_check_name(job, batch=batch, num_batches=num_batches),\n+            )\n+            print(f\"GH status re-created from file [{success_flag_name}]\")\n+\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        futures = []\n+        for job in job_digests:\n+            if is_build_job(job):\n+                # no GH status for build jobs\n+                continue\n+            digest = job_digests[job]\n+            num_batches = CI_CONFIG.get_job_config(job).num_batches\n+            for batch in range(num_batches):\n+                future = executor.submit(\n+                    run_create_status, job, digest, batch, num_batches\n+                )\n+                futures.append(future)\n+        done, _ = concurrent.futures.wait(futures)\n+        for future in done:\n+            try:\n+                _ = future.result()\n+            except Exception as e:\n+                raise e\n+    print(\"Going to update overall CI report\")\n+    set_status_comment(commit, pr_info)\n+    print(\"... CI report update - done\")\n+\n+    # clean up\n+    ci_files = list(temp_path.glob(\"*.ci\"))\n+    for file in ci_files:\n+        file.unlink()\n+\n+\n+def _fetch_commit_tokens(message: str) -> List[str]:\n+    pattern = r\"#[\\w-]+\"\n+    matches = re.findall(pattern, message)\n+    return matches\n+\n+\n+def main() -> int:\n+    exit_code = 0\n+    parser = argparse.ArgumentParser(\n+        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n+    )\n+    args = parse_args(parser)\n+\n+    if args.mark_success or args.pre or args.post or args.run:\n+        assert args.infile, \"Run config must be provided via --infile\"\n+        assert args.job_name, \"Job name must be provided via --job-name\"\n+\n+    indata: Optional[Dict[str, Any]] = None\n+    if args.infile:\n+        indata = (\n+            json.loads(args.infile)\n+            if not os.path.isfile(args.infile)\n+            else json.load(open(args.infile))\n+        )\n+        assert indata and isinstance(indata, dict), \"Invalid --infile json\"\n+\n+    result: Dict[str, Any] = {}\n+    s3 = S3Helper()\n+\n+    if args.configure:\n+        GR = GitRunner()\n+        pr_info = PRInfo()\n+\n+        docker_data = {}\n+        git_ref = GR.run(f\"{GIT_PREFIX} rev-parse HEAD\")\n+\n+        # if '#no-merge-commit' is set in commit message - set git ref to PR branch head to avoid merge-commit\n+        tokens = []\n+        if pr_info.number != 0:\n+            message = GR.run(f\"{GIT_PREFIX} log {pr_info.sha} --format=%B -n 1\")\n+            tokens = _fetch_commit_tokens(message)\n+            print(f\"Found commit message tokens: [{tokens}]\")\n+            if \"#no-merge-commit\" in tokens and CI:\n+                GR.run(f\"{GIT_PREFIX} checkout {pr_info.sha}\")\n+                git_ref = GR.run(f\"{GIT_PREFIX} rev-parse HEAD\")\n+                print(\n+                    \"#no-merge-commit is set in commit message - Setting git ref to PR branch HEAD to not use merge commit\"\n+                )\n+\n+        # let's get CH version\n+        version = get_version_from_repo(git=Git(True)).string\n+        print(f\"Got CH version for this commit: [{version}]\")\n+\n+        docker_data = (\n+            _configure_docker_jobs(\n+                args.rebuild_all_docker, args.docker_digest_or_latest\n+            )\n+            if not args.skip_docker\n+            else {}\n+        )\n+\n+        job_digester = JobDigester()\n+        build_digest = job_digester.get_job_digest(\n+            CI_CONFIG.get_digest_config(\"package_release\")\n+        )\n+        docs_digest = job_digester.get_job_digest(\n+            CI_CONFIG.get_digest_config(\"Docs check\")\n+        )\n+        jobs_data = (\n+            _configure_jobs(\n+                build_digest,\n+                docs_digest,\n+                job_digester,\n+                s3,\n+                args.rebuild_all_binaries,\n+                pr_info.labels,\n+                tokens,\n+            )\n+            if not args.skip_jobs\n+            else {}\n+        )\n+\n+        # conclude results\n+        result[\"git_ref\"] = git_ref\n+        result[\"version\"] = version\n+        result[\"build\"] = build_digest\n+        result[\"docs\"] = docs_digest\n+        result[\"jobs_data\"] = jobs_data\n+        result[\"docker_data\"] = docker_data\n+        if not args.docker_digest_or_latest:\n+            _check_and_update_for_early_style_check(result)\n+\n+    elif args.update_gh_statuses:\n+        assert indata, \"Run config must be provided via --infile\"\n+        _update_gh_statuses(indata=indata, s3=s3)\n+\n+    elif args.pre:\n+        # remove job status file if any\n+        CommitStatusData.cleanup()\n+\n+        if is_test_job(args.job_name):\n+            assert indata, \"Run config must be provided via --infile\"\n+            report_path = Path(REPORT_PATH)\n+            report_path.mkdir(exist_ok=True, parents=True)\n+            path = get_s3_path(indata[\"build\"])\n+            files = s3.download_files(  # type: ignore\n+                bucket=S3_BUILDS_BUCKET,\n+                s3_path=path,\n+                file_suffix=\".json\",\n+                local_directory=report_path,\n+            )\n+            print(\n+                f\"Pre action done. Report files [{files}] have been downloaded from [{path}] to [{report_path}]\"\n+            )\n+        else:\n+            print(\"Pre action done. Nothing to do for [{args.job_name}]\")\n+\n+    elif args.run:\n+        assert CI_CONFIG.get_job_config(\n+            args.job_name\n+        ).run_command, f\"Run command must be configured in CI_CONFIG for [{args.job_name}] or in GH workflow\"\n+        if CI_CONFIG.get_job_config(args.job_name).timeout:\n+            os.environ[\"KILL_TIMEOUT\"] = str(\n+                CI_CONFIG.get_job_config(args.job_name).timeout\n+            )\n+        os.environ[\"CHECK_NAME\"] = args.job_name\n+        run_command = (\n+            \"./tests/ci/\" + CI_CONFIG.get_job_config(args.job_name).run_command\n+        )\n+        if \".py\" in run_command:\n+            run_command = \"python3 \" + run_command\n+        print(f\"Going to start run command [{run_command}]\")\n+        process = subprocess.run(\n+            run_command,\n+            stdout=sys.stdout,\n+            stderr=sys.stderr,\n+            text=True,\n+            check=False,\n+            shell=True,\n+        )\n+        if process.returncode == 0:\n+            print(f\"Run action done for: [{args.job_name}]\")\n+        else:\n+            print(\n+                f\"Run action failed for: [{args.job_name}] with exit code [{process.returncode}]\"\n+            )\n+            exit_code = process.returncode\n+\n+    elif args.post:\n+        if is_build_job(args.job_name):\n+            report_path = Path(TEMP_PATH)  # build-check.py stores report in TEMP_PATH\n+            assert report_path.is_dir(), f\"File [{report_path}] is not a dir\"\n+            files = list(report_path.glob(f\"*{args.job_name}.json\"))  # type: ignore[arg-type]\n+            assert len(files) == 1, f\"Which is the report file: {files}?\"\n+            local_report = f\"{files[0]}\"\n+            report_name = BuildResult.get_report_name(args.job_name)\n+            assert indata\n+            s3_path = Path(get_s3_path(indata[\"build\"])) / report_name\n+            report_url = s3.upload_file(\n+                bucket=S3_BUILDS_BUCKET, file_path=local_report, s3_path=s3_path\n+            )\n+            print(\n+                f\"Post action done. Report file [{local_report}] has been uploaded to [{report_url}]\"\n+            )\n+        else:\n+            print(f\"Post action done. Nothing to do for [{args.job_name}]\")\n+\n+    elif args.mark_success:\n+        assert indata, \"Run config must be provided via --infile\"\n+        job = args.job_name\n+        num_batches = CI_CONFIG.get_job_config(job).num_batches\n+        assert (\n+            num_batches <= 1 or 0 <= args.batch < num_batches\n+        ), f\"--batch must be provided and in range [0, {num_batches}) for {job}\"\n+\n+        # FIXME: find generic design for propagating and handling job status (e.g. stop using statuses in GH api)\n+        #   now job ca be build job w/o status data, any other job that exit with 0 with or w/o status data\n+        if is_build_job(job):\n+            # there is no status for build jobs\n+            # create dummy success to mark it as done\n+            job_status = CommitStatusData(\n+                status=\"success\", description=\"dummy status\", report_url=\"dummy_url\"\n+            )\n+        else:\n+            if not CommitStatusData.is_present():\n+                # apperently exit after rerun-helper check\n+                # do nothing, exit without failure\n+                print(\"ERROR: no status file for job [{job}]\")\n+                job_status = CommitStatusData(\n+                    status=\"dummy failure\",\n+                    description=\"dummy status\",\n+                    report_url=\"dummy_url\",\n+                )\n+            else:\n+                # normal case\n+                job_status = CommitStatusData.load_status()\n+\n+        # Storing job data (report_url) to restore OK GH status on job results reuse\n+        if job_status.is_ok():\n+            success_flag_name = get_file_flag_name(\n+                job, indata[\"jobs_data\"][\"digests\"][job], args.batch, num_batches\n+            )\n+            if not is_docs_job(job):\n+                path = get_s3_path(indata[\"build\"]) + success_flag_name\n+            else:\n+                path = get_s3_path_docs(indata[\"docs\"]) + success_flag_name\n+            job_status.dump_to_file(success_flag_name)\n+            _ = s3.upload_file(\n+                bucket=S3_BUILDS_BUCKET, file_path=success_flag_name, s3_path=path\n+            )\n+            os.remove(success_flag_name)\n+            print(\n+                f\"Job [{job}] with digest [{indata['jobs_data']['digests'][job]}] {f'and batch {args.batch}/{num_batches}' if num_batches > 1 else ''} marked as successful. path: [{path}]\"\n+            )\n+        else:\n+            print(f\"Job [{job}] is not ok, status [{job_status.status}]\")\n+\n+    # print results\n+    if args.outfile:\n+        with open(args.outfile, \"w\") as f:\n+            if isinstance(result, str):\n+                print(result, file=f)\n+            elif isinstance(result, dict):\n+                print(json.dumps(result, indent=2 if args.pretty else None), file=f)\n+            else:\n+                raise AssertionError(f\"Unexpected type for 'res': {type(result)}\")\n+    else:\n+        if isinstance(result, str):\n+            print(result)\n+        elif isinstance(result, dict):\n+            print(json.dumps(result, indent=2 if args.pretty else None))\n+        else:\n+            raise AssertionError(f\"Unexpected type for 'res': {type(result)}\")\n+\n+    return exit_code\n+\n+\n+if __name__ == \"__main__\":\n+    os.chdir(ROOT_DIR)\n+    sys.exit(main())\ndiff --git a/tests/ci/ci_config.py b/tests/ci/ci_config.py\nindex de2ba3dc1cea..f76aedac80b7 100644\n--- a/tests/ci/ci_config.py\n+++ b/tests/ci/ci_config.py\n@@ -3,8 +3,40 @@\n import logging\n \n from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n-from dataclasses import dataclass\n-from typing import Callable, Dict, List, Literal, Union\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+from typing import Callable, Dict, Iterable, List, Literal, Optional, Union\n+\n+\n+@dataclass\n+class DigestConfig:\n+    # all files, dirs to include into digest, glob supported\n+    include_paths: List[Union[str, Path]] = field(default_factory=list)\n+    # file suffixes to exclude from digest\n+    exclude_files: List[str] = field(default_factory=list)\n+    # directories to exlude from digest\n+    exclude_dirs: List[Union[str, Path]] = field(default_factory=list)\n+    # docker names to include into digest\n+    docker: List[str] = field(default_factory=list)\n+    # git submodules digest\n+    git_submodules: bool = False\n+\n+\n+@dataclass\n+class JobConfig:\n+    \"\"\"\n+    contains config parameter relevant for job execution in CI workflow\n+    @digest - configures digest calculation for the job\n+    @run_command - will be triggered for the job if omited in CI workflow yml\n+    @timeout\n+    @num_batches - sets number of batches for multi-batch job\n+    \"\"\"\n+\n+    digest: DigestConfig = DigestConfig()\n+    run_command: str = \"\"\n+    timeout: Optional[int] = None\n+    num_batches: int = 1\n+    run_by_label: str = \"\"\n \n \n @dataclass\n@@ -19,6 +51,21 @@ class BuildConfig:\n     sparse_checkout: bool = False\n     comment: str = \"\"\n     static_binary_name: str = \"\"\n+    job_config: JobConfig = JobConfig(\n+        digest=DigestConfig(\n+            include_paths=[\n+                \"./src\",\n+                \"./contrib/*-cmake\",\n+                \"./cmake\",\n+                \"./base\",\n+                \"./programs\",\n+                \"./packages\",\n+            ],\n+            exclude_files=[\".md\"],\n+            docker=[\"clickhouse/binary-builder\"],\n+            git_submodules=True,\n+        ),\n+    )\n \n     def export_env(self, export: bool = False) -> str:\n         def process(field_name: str, field: Union[bool, str]) -> str:\n@@ -31,29 +78,292 @@ def process(field_name: str, field: Union[bool, str]) -> str:\n         return \"\\n\".join(process(k, v) for k, v in self.__dict__.items())\n \n \n+@dataclass\n+class BuildReportConfig:\n+    builds: List[str]\n+    job_config: JobConfig = JobConfig()\n+\n+\n @dataclass\n class TestConfig:\n     required_build: str\n     force_tests: bool = False\n+    job_config: JobConfig = JobConfig()\n \n \n BuildConfigs = Dict[str, BuildConfig]\n-BuildsReportConfig = Dict[str, List[str]]\n+BuildsReportConfig = Dict[str, BuildReportConfig]\n TestConfigs = Dict[str, TestConfig]\n \n \n+# common digests configs\n+compatibility_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/compatibility_check.py\"],\n+    docker=[\"clickhouse/test-old-ubuntu\", \"clickhouse/test-old-centos\"],\n+)\n+install_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/install_check.py\"],\n+    docker=[\"clickhouse/install-deb-test\", \"clickhouse/install-rpm-test\"],\n+)\n+statless_check_digest = DigestConfig(\n+    include_paths=[\"./tests/queries/0_stateless/\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/stateless-test\"],\n+)\n+stateful_check_digest = DigestConfig(\n+    include_paths=[\"./tests/queries/1_stateful/\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/stateful-test\"],\n+)\n+# FIXME: which tests are stresstest? stateless?\n+stress_check_digest = DigestConfig(\n+    include_paths=[\"./tests/queries/0_stateless/\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/stress-test\"],\n+)\n+# FIXME: which tests are upgrade? just python?\n+upgrade_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/upgrade_check.py\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/upgrade-check\"],\n+)\n+integration_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/integration_test_check.py\", \"./tests/integration\"],\n+    exclude_files=[\".md\"],\n+    docker=[\n+        \"clickhouse/dotnet-client\",\n+        \"clickhouse/integration-helper\",\n+        \"clickhouse/integration-test\",\n+        \"clickhouse/integration-tests-runner\",\n+        \"clickhouse/kerberized-hadoop\",\n+        \"clickhouse/kerberos-kdc\",\n+        \"clickhouse/mysql-golang-client\",\n+        \"clickhouse/mysql-java-client\",\n+        \"clickhouse/mysql-js-client\",\n+        \"clickhouse/mysql-php-client\",\n+        \"clickhouse/nginx-dav\",\n+        \"clickhouse/postgresql-java-client\",\n+    ],\n+)\n+# FIXME: which tests are AST_FUZZER_TEST? just python?\n+# FIXME: should ast fuzzer test be non-skipable?\n+ast_fuzzer_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/ast_fuzzer_check.py\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/fuzzer\"],\n+)\n+unit_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/unit_tests_check.py\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/unit-test\"],\n+)\n+perf_check_digest = DigestConfig(\n+    include_paths=[\n+        \"./tests/ci/performance_comparison_check.py\",\n+        \"./tests/performance/\",\n+    ],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/performance-comparison\"],\n+)\n+sqllancer_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/sqlancer_check.py\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/sqlancer-test\"],\n+)\n+sqllogic_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/sqllogic_test.py\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/sqllogic-test\"],\n+)\n+sqltest_check_digest = DigestConfig(\n+    include_paths=[\"./tests/ci/sqltest.py\"],\n+    exclude_files=[\".md\"],\n+    docker=[\"clickhouse/sqltest\"],\n+)\n+bugfix_validate_check = DigestConfig(\n+    include_paths=[\n+        \"./tests/queries/0_stateless/\",\n+        \"./tests/ci/integration_test_check.py\",\n+        \"./tests/ci/functional_test_check.py\",\n+        \"./tests/ci/bugfix_validate_check.py\",\n+    ],\n+    exclude_files=[\".md\"],\n+    docker=[\n+        \"clickhouse/stateless-test\",\n+        \"clickhouse/dotnet-client\",\n+        \"clickhouse/integration-helper\",\n+        \"clickhouse/integration-test\",\n+        \"clickhouse/integration-tests-runner\",\n+        \"clickhouse/kerberized-hadoop\",\n+        \"clickhouse/kerberos-kdc\",\n+        \"clickhouse/mysql-golang-client\",\n+        \"clickhouse/mysql-java-client\",\n+        \"clickhouse/mysql-js-client\",\n+        \"clickhouse/mysql-php-client\",\n+        \"clickhouse/nginx-dav\",\n+        \"clickhouse/postgresql-java-client\",\n+    ],\n+)\n+# common test params\n+statless_test_common_params = {\n+    \"digest\": statless_check_digest,\n+    \"run_command\": 'functional_test_check.py \"$CHECK_NAME\" $KILL_TIMEOUT',\n+    \"timeout\": 10800,\n+}\n+stateful_test_common_params = {\n+    \"digest\": stateful_check_digest,\n+    \"run_command\": 'functional_test_check.py \"$CHECK_NAME\" $KILL_TIMEOUT',\n+    \"timeout\": 3600,\n+}\n+stress_test_common_params = {\n+    \"digest\": stress_check_digest,\n+    \"run_command\": \"stress_check.py\",\n+}\n+upgrade_test_common_params = {\n+    \"digest\": upgrade_check_digest,\n+    \"run_command\": \"upgrade_check.py\",\n+}\n+astfuzzer_test_common_params = {\n+    \"digest\": ast_fuzzer_check_digest,\n+    \"run_command\": \"ast_fuzzer_check.py\",\n+}\n+integration_test_common_params = {\n+    \"digest\": integration_check_digest,\n+    \"run_command\": 'integration_test_check.py \"$CHECK_NAME\"',\n+}\n+unit_test_common_params = {\n+    \"digest\": unit_check_digest,\n+    \"run_command\": \"unit_tests_check.py\",\n+}\n+perf_test_common_params = {\n+    \"digest\": perf_check_digest,\n+    \"run_command\": \"performance_comparison_check.py\",\n+}\n+sqllancer_test_common_params = {\n+    \"digest\": sqllancer_check_digest,\n+    \"run_command\": \"sqlancer_check.py\",\n+}\n+sqllogic_test_params = {\n+    \"digest\": sqllogic_check_digest,\n+    \"run_command\": \"sqllogic_test.py\",\n+    \"timeout\": 10800,\n+}\n+sql_test_params = {\n+    \"digest\": sqltest_check_digest,\n+    \"run_command\": \"sqltest.py\",\n+    \"timeout\": 10800,\n+}\n+\n+\n @dataclass\n class CiConfig:\n+    \"\"\"\n+    Contains configs for ALL jobs in CI pipeline\n+    each config item in the below dicts should be an instance of JobConfig class or inherited from it\n+    \"\"\"\n+\n     build_config: BuildConfigs\n     builds_report_config: BuildsReportConfig\n     test_configs: TestConfigs\n+    other_jobs_configs: TestConfigs\n+\n+    def get_job_config(self, check_name: str) -> JobConfig:\n+        res = None\n+        for config in (\n+            self.build_config,\n+            self.builds_report_config,\n+            self.test_configs,\n+            self.other_jobs_configs,\n+        ):\n+            if check_name in config:  # type: ignore\n+                res = config[check_name].job_config  # type: ignore\n+                break\n+        assert (\n+            res is not None\n+        ), f\"Invalid check_name or CI_CONFIG outdated, config not found for [{check_name}]\"\n+        return res  # type: ignore\n+\n+    def get_job_with_parents(self, check_name: str) -> List[str]:\n+        def _normalize_string(input_string: str) -> str:\n+            lowercase_string = input_string.lower()\n+            normalized_string = (\n+                lowercase_string.replace(\" \", \"_\")\n+                .replace(\"-\", \"_\")\n+                .replace(\"(\", \"\")\n+                .replace(\")\", \"\")\n+                .replace(\",\", \"\")\n+            )\n+            return normalized_string\n+\n+        res = []\n+        check_name = _normalize_string(check_name)\n+\n+        for config in (\n+            self.build_config,\n+            self.builds_report_config,\n+            self.test_configs,\n+            self.other_jobs_configs,\n+        ):\n+            for job_name in config:  # type: ignore\n+                if check_name == _normalize_string(job_name):\n+                    res.append(job_name)\n+                    if isinstance(config[job_name], TestConfig):  # type: ignore\n+                        assert config[\n+                            job_name\n+                        ].required_build, f\"Error: Experimantal feature... Not supported job [{job_name}]\"  # type: ignore\n+                        res.append(config[job_name].required_build)  # type: ignore\n+                        res.append(\"Fast tests\")\n+                        res.append(\"Style check\")\n+                    elif isinstance(config[job_name], BuildConfig):  # type: ignore\n+                        res.append(\"Fast tests\")\n+                        res.append(\"Style check\")\n+                    else:\n+                        assert (\n+                            False\n+                        ), f\"check commit message tags or FIXME: request for job [{check_name}] not yet supported\"\n+                    break\n+        assert (\n+            res\n+        ), f\"Error: Experimantal feature... Invlid request or not supported job [{check_name}]\"\n+        return res\n+\n+    def get_digest_config(self, check_name: str) -> DigestConfig:\n+        res = None\n+        for config in (\n+            self.other_jobs_configs,\n+            self.build_config,\n+            self.builds_report_config,\n+            self.test_configs,\n+        ):\n+            if check_name in config:  # type: ignore\n+                res = config[check_name].job_config.digest  # type: ignore\n+        assert (\n+            res\n+        ), f\"Invalid check_name or CI_CONFIG outdated, config not found for [{check_name}]\"\n+        return res  # type: ignore\n+\n+    def job_generator(self) -> Iterable[str]:\n+        \"\"\"\n+        traverses all check names in CI pipeline\n+        \"\"\"\n+        for config in (\n+            self.other_jobs_configs,\n+            self.build_config,\n+            self.builds_report_config,\n+            self.test_configs,\n+        ):\n+            for check_name in config:  # type: ignore\n+                yield check_name\n+\n+    def get_builds_for_report(self, report_name: str) -> List[str]:\n+        return self.builds_report_config[report_name].builds\n \n     def validate(self) -> None:\n         errors = []\n         for name, build_config in self.build_config.items():\n             build_in_reports = False\n-            for report_config in self.builds_report_config.values():\n-                if name in report_config:\n+            for _, report_config in self.builds_report_config.items():\n+                if name in report_config.builds:\n                     build_in_reports = True\n                     break\n             # All build configs must belong to build_report_config\n@@ -71,7 +381,8 @@ def validate(self) -> None:\n                     f\"Build name {name} does not match 'name' value '{build_config.name}'\"\n                 )\n         # All build_report_config values should be in build_config.keys()\n-        for build_report_name, build_names in self.builds_report_config.items():\n+        for build_report_name, build_report_config in self.builds_report_config.items():\n+            build_names = build_report_config.builds\n             missed_names = [\n                 name for name in build_names if name not in self.build_config.keys()\n             ]\n@@ -234,104 +545,301 @@ def validate(self) -> None:\n         ),\n     },\n     builds_report_config={\n-        \"ClickHouse build check\": [\n-            \"package_release\",\n-            \"package_aarch64\",\n-            \"package_asan\",\n-            \"package_ubsan\",\n-            \"package_tsan\",\n-            \"package_msan\",\n-            \"package_debug\",\n-            \"binary_release\",\n-            \"fuzzers\",\n-        ],\n-        \"ClickHouse special build check\": [\n-            \"binary_tidy\",\n-            \"binary_darwin\",\n-            \"binary_aarch64\",\n-            \"binary_aarch64_v80compat\",\n-            \"binary_freebsd\",\n-            \"binary_darwin_aarch64\",\n-            \"binary_ppc64le\",\n-            \"binary_riscv64\",\n-            \"binary_s390x\",\n-            \"binary_amd64_compat\",\n-            \"binary_amd64_musl\",\n-        ],\n+        \"ClickHouse build check\": BuildReportConfig(\n+            builds=[\n+                \"package_release\",\n+                \"package_aarch64\",\n+                \"package_asan\",\n+                \"package_ubsan\",\n+                \"package_tsan\",\n+                \"package_msan\",\n+                \"package_debug\",\n+                \"binary_release\",\n+                \"fuzzers\",\n+            ]\n+        ),\n+        \"ClickHouse special build check\": BuildReportConfig(\n+            builds=[\n+                \"binary_tidy\",\n+                \"binary_darwin\",\n+                \"binary_aarch64\",\n+                \"binary_aarch64_v80compat\",\n+                \"binary_freebsd\",\n+                \"binary_darwin_aarch64\",\n+                \"binary_ppc64le\",\n+                \"binary_riscv64\",\n+                \"binary_s390x\",\n+                \"binary_amd64_compat\",\n+                \"binary_amd64_musl\",\n+            ]\n+        ),\n+    },\n+    other_jobs_configs={\n+        \"Docker server and keeper images\": TestConfig(\n+            \"\",\n+            job_config=JobConfig(\n+                digest=DigestConfig(\n+                    include_paths=[\n+                        \"tests/ci/docker_server.py\",\n+                        \"./docker/server\",\n+                        \"./docker/keeper\",\n+                    ]\n+                )\n+            ),\n+        ),\n+        \"Docs check\": TestConfig(\n+            \"\",\n+            job_config=JobConfig(\n+                digest=DigestConfig(\n+                    include_paths=[\"**/*.md\", \"./docs\", \"tests/ci/docs_check.py\"],\n+                    docker=[\"clickhouse/docs-builder\"],\n+                ),\n+            ),\n+        ),\n+        \"Fast tests\": TestConfig(\n+            \"\",\n+            job_config=JobConfig(\n+                digest=DigestConfig(\n+                    include_paths=[\"./tests/queries/0_stateless/\"],\n+                    exclude_files=[\".md\"],\n+                    docker=[\"clickhouse/fasttest\"],\n+                )\n+            ),\n+        ),\n+        \"Style check\": TestConfig(\n+            \"\",\n+            job_config=JobConfig(\n+                digest=DigestConfig(\n+                    include_paths=[\".\"], exclude_dirs=[\".git\", \"__pycache__\"]\n+                )\n+            ),\n+        ),\n+        \"tests bugfix validate check\": TestConfig(\n+            \"\",\n+            # we run this check by label - no digest required\n+            job_config=JobConfig(run_by_label=\"pr-bugfix\"),\n+        ),\n     },\n     test_configs={\n-        \"Install packages (amd64)\": TestConfig(\"package_release\"),\n-        \"Install packages (arm64)\": TestConfig(\"package_aarch64\"),\n-        \"Stateful tests (asan)\": TestConfig(\"package_asan\"),\n-        \"Stateful tests (tsan)\": TestConfig(\"package_tsan\"),\n-        \"Stateful tests (msan)\": TestConfig(\"package_msan\"),\n-        \"Stateful tests (ubsan)\": TestConfig(\"package_ubsan\"),\n-        \"Stateful tests (debug)\": TestConfig(\"package_debug\"),\n-        \"Stateful tests (release)\": TestConfig(\"package_release\"),\n-        \"Stateful tests (aarch64)\": TestConfig(\"package_aarch64\"),\n-        \"Stateful tests (release, DatabaseOrdinary)\": TestConfig(\"package_release\"),\n-        \"Stateful tests (release, DatabaseReplicated)\": TestConfig(\"package_release\"),\n+        \"Install packages (amd64)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(digest=install_check_digest)\n+        ),\n+        \"Install packages (arm64)\": TestConfig(\n+            \"package_aarch64\", job_config=JobConfig(digest=install_check_digest)\n+        ),\n+        \"Stateful tests (asan)\": TestConfig(\n+            \"package_asan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (tsan)\": TestConfig(\n+            \"package_tsan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (msan)\": TestConfig(\n+            \"package_msan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (ubsan)\": TestConfig(\n+            \"package_ubsan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (debug)\": TestConfig(\n+            \"package_debug\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (release)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (aarch64)\": TestConfig(\n+            \"package_aarch64\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (release, DatabaseOrdinary)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        # \"Stateful tests (release, DatabaseReplicated)\": TestConfig(\n+        #     \"package_release\", job_config=JobConfig(**stateful_test_common_params) # type: ignore\n+        # ),\n         # Stateful tests for parallel replicas\n-        \"Stateful tests (release, ParallelReplicas)\": TestConfig(\"package_release\"),\n-        \"Stateful tests (debug, ParallelReplicas)\": TestConfig(\"package_debug\"),\n-        \"Stateful tests (asan, ParallelReplicas)\": TestConfig(\"package_asan\"),\n-        \"Stateful tests (msan, ParallelReplicas)\": TestConfig(\"package_msan\"),\n-        \"Stateful tests (ubsan, ParallelReplicas)\": TestConfig(\"package_ubsan\"),\n-        \"Stateful tests (tsan, ParallelReplicas)\": TestConfig(\"package_tsan\"),\n+        \"Stateful tests (release, ParallelReplicas)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (debug, ParallelReplicas)\": TestConfig(\n+            \"package_debug\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (asan, ParallelReplicas)\": TestConfig(\n+            \"package_asan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (msan, ParallelReplicas)\": TestConfig(\n+            \"package_msan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (ubsan, ParallelReplicas)\": TestConfig(\n+            \"package_ubsan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n+        \"Stateful tests (tsan, ParallelReplicas)\": TestConfig(\n+            \"package_tsan\", job_config=JobConfig(**stateful_test_common_params)  # type: ignore\n+        ),\n         # End stateful tests for parallel replicas\n-        \"Stateless tests (asan)\": TestConfig(\"package_asan\"),\n-        \"Stateless tests (tsan)\": TestConfig(\"package_tsan\"),\n-        \"Stateless tests (msan)\": TestConfig(\"package_msan\"),\n-        \"Stateless tests (ubsan)\": TestConfig(\"package_ubsan\"),\n-        \"Stateless tests (debug)\": TestConfig(\"package_debug\"),\n-        \"Stateless tests (release)\": TestConfig(\"package_release\"),\n-        \"Stateless tests (aarch64)\": TestConfig(\"package_aarch64\"),\n-        \"Stateless tests (release, wide parts enabled)\": TestConfig(\"package_release\"),\n-        \"Stateless tests (release, analyzer)\": TestConfig(\"package_release\"),\n-        \"Stateless tests (release, DatabaseOrdinary)\": TestConfig(\"package_release\"),\n-        \"Stateless tests (release, DatabaseReplicated)\": TestConfig(\"package_release\"),\n-        \"Stateless tests (release, s3 storage)\": TestConfig(\"package_release\"),\n-        \"Stateless tests (debug, s3 storage)\": TestConfig(\"package_debug\"),\n-        \"Stateless tests (tsan, s3 storage)\": TestConfig(\"package_tsan\"),\n-        \"Stress test (asan)\": TestConfig(\"package_asan\"),\n-        \"Stress test (tsan)\": TestConfig(\"package_tsan\"),\n-        \"Stress test (ubsan)\": TestConfig(\"package_ubsan\"),\n-        \"Stress test (msan)\": TestConfig(\"package_msan\"),\n-        \"Stress test (debug)\": TestConfig(\"package_debug\"),\n-        \"Upgrade check (asan)\": TestConfig(\"package_asan\"),\n-        \"Upgrade check (tsan)\": TestConfig(\"package_tsan\"),\n-        \"Upgrade check (msan)\": TestConfig(\"package_msan\"),\n-        \"Upgrade check (debug)\": TestConfig(\"package_debug\"),\n-        \"Integration tests (asan)\": TestConfig(\"package_asan\"),\n-        \"Integration tests (asan, analyzer)\": TestConfig(\"package_asan\"),\n-        \"Integration tests (tsan)\": TestConfig(\"package_tsan\"),\n-        \"Integration tests (release)\": TestConfig(\"package_release\"),\n-        \"Integration tests (msan)\": TestConfig(\"package_msan\"),\n-        \"Integration tests flaky check (asan)\": TestConfig(\"package_asan\"),\n-        \"Compatibility check (amd64)\": TestConfig(\"package_release\"),\n-        \"Compatibility check (aarch64)\": TestConfig(\"package_aarch64\"),\n-        \"Unit tests (release)\": TestConfig(\"binary_release\"),\n-        \"Unit tests (asan)\": TestConfig(\"package_asan\"),\n-        \"Unit tests (msan)\": TestConfig(\"package_msan\"),\n-        \"Unit tests (tsan)\": TestConfig(\"package_tsan\"),\n-        \"Unit tests (ubsan)\": TestConfig(\"package_ubsan\"),\n-        \"AST fuzzer (debug)\": TestConfig(\"package_debug\"),\n-        \"AST fuzzer (asan)\": TestConfig(\"package_asan\"),\n-        \"AST fuzzer (msan)\": TestConfig(\"package_msan\"),\n-        \"AST fuzzer (tsan)\": TestConfig(\"package_tsan\"),\n-        \"AST fuzzer (ubsan)\": TestConfig(\"package_ubsan\"),\n-        \"Stateless tests flaky check (asan)\": TestConfig(\"package_asan\"),\n+        \"Stateless tests (asan)\": TestConfig(\n+            \"package_asan\",\n+            job_config=JobConfig(num_batches=4, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (tsan)\": TestConfig(\n+            \"package_tsan\",\n+            job_config=JobConfig(num_batches=5, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (msan)\": TestConfig(\n+            \"package_msan\",\n+            job_config=JobConfig(num_batches=6, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (ubsan)\": TestConfig(\n+            \"package_ubsan\",\n+            job_config=JobConfig(num_batches=2, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (debug)\": TestConfig(\n+            \"package_debug\",\n+            job_config=JobConfig(num_batches=5, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (release)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**statless_test_common_params)  # type: ignore\n+        ),\n+        \"Stateless tests (aarch64)\": TestConfig(\n+            \"package_aarch64\", job_config=JobConfig(**statless_test_common_params)  # type: ignore\n+        ),\n+        \"Stateless tests (release, analyzer)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**statless_test_common_params)  # type: ignore\n+        ),\n+        \"Stateless tests (release, DatabaseOrdinary)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**statless_test_common_params)  # type: ignore\n+        ),\n+        \"Stateless tests (release, DatabaseReplicated)\": TestConfig(\n+            \"package_release\",\n+            job_config=JobConfig(num_batches=4, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (release, s3 storage)\": TestConfig(\n+            \"package_release\",\n+            job_config=JobConfig(num_batches=2, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (debug, s3 storage)\": TestConfig(\n+            \"package_debug\",\n+            job_config=JobConfig(num_batches=6, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stateless tests (tsan, s3 storage)\": TestConfig(\n+            \"package_tsan\",\n+            job_config=JobConfig(num_batches=5, **statless_test_common_params),  # type: ignore\n+        ),\n+        \"Stress test (asan)\": TestConfig(\n+            \"package_asan\", job_config=JobConfig(**stress_test_common_params)  # type: ignore\n+        ),\n+        \"Stress test (tsan)\": TestConfig(\n+            \"package_tsan\", job_config=JobConfig(**stress_test_common_params)  # type: ignore\n+        ),\n+        \"Stress test (ubsan)\": TestConfig(\n+            \"package_ubsan\", job_config=JobConfig(**stress_test_common_params)  # type: ignore\n+        ),\n+        \"Stress test (msan)\": TestConfig(\n+            \"package_msan\", job_config=JobConfig(**stress_test_common_params)  # type: ignore\n+        ),\n+        \"Stress test (debug)\": TestConfig(\n+            \"package_debug\", job_config=JobConfig(**stress_test_common_params)  # type: ignore\n+        ),\n+        \"Upgrade check (asan)\": TestConfig(\n+            \"package_asan\", job_config=JobConfig(**upgrade_test_common_params)  # type: ignore\n+        ),\n+        \"Upgrade check (tsan)\": TestConfig(\n+            \"package_tsan\", job_config=JobConfig(**upgrade_test_common_params)  # type: ignore\n+        ),\n+        \"Upgrade check (msan)\": TestConfig(\n+            \"package_msan\", job_config=JobConfig(**upgrade_test_common_params)  # type: ignore\n+        ),\n+        \"Upgrade check (debug)\": TestConfig(\n+            \"package_debug\", job_config=JobConfig(**upgrade_test_common_params)  # type: ignore\n+        ),\n+        \"Integration tests (asan)\": TestConfig(\n+            \"package_asan\",\n+            job_config=JobConfig(num_batches=4, **integration_test_common_params),  # type: ignore\n+        ),\n+        \"Integration tests (asan, analyzer)\": TestConfig(\n+            \"package_asan\",\n+            job_config=JobConfig(num_batches=6, **integration_test_common_params),  # type: ignore\n+        ),\n+        \"Integration tests (tsan)\": TestConfig(\n+            \"package_tsan\",\n+            job_config=JobConfig(num_batches=6, **integration_test_common_params),  # type: ignore\n+        ),\n+        # FIXME: currently no wf has this job. Try to enable\n+        # \"Integration tests (msan)\": TestConfig(\"package_msan\", job_config=JobConfig(num_batches=6, **integration_test_common_params) # type: ignore\n+        # ),\n+        \"Integration tests (release)\": TestConfig(\n+            \"package_release\",\n+            job_config=JobConfig(num_batches=4, **integration_test_common_params),  # type: ignore\n+        ),\n+        \"Integration tests flaky check (asan)\": TestConfig(\n+            \"package_asan\", job_config=JobConfig(**integration_test_common_params)  # type: ignore\n+        ),\n+        \"Compatibility check (amd64)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(digest=compatibility_check_digest)\n+        ),\n+        \"Compatibility check (aarch64)\": TestConfig(\n+            \"package_aarch64\", job_config=JobConfig(digest=compatibility_check_digest)\n+        ),\n+        \"Unit tests (release)\": TestConfig(\n+            \"binary_release\", job_config=JobConfig(**unit_test_common_params)  # type: ignore\n+        ),\n+        \"Unit tests (asan)\": TestConfig(\n+            \"package_asan\", job_config=JobConfig(**unit_test_common_params)  # type: ignore\n+        ),\n+        \"Unit tests (msan)\": TestConfig(\n+            \"package_msan\", job_config=JobConfig(**unit_test_common_params)  # type: ignore\n+        ),\n+        \"Unit tests (tsan)\": TestConfig(\n+            \"package_tsan\", job_config=JobConfig(**unit_test_common_params)  # type: ignore\n+        ),\n+        \"Unit tests (ubsan)\": TestConfig(\n+            \"package_ubsan\", job_config=JobConfig(**unit_test_common_params)  # type: ignore\n+        ),\n+        \"AST fuzzer (debug)\": TestConfig(\n+            \"package_debug\", job_config=JobConfig(**astfuzzer_test_common_params)  # type: ignore\n+        ),\n+        \"AST fuzzer (asan)\": TestConfig(\n+            \"package_asan\", job_config=JobConfig(**astfuzzer_test_common_params)  # type: ignore\n+        ),\n+        \"AST fuzzer (msan)\": TestConfig(\n+            \"package_msan\", job_config=JobConfig(**astfuzzer_test_common_params)  # type: ignore\n+        ),\n+        \"AST fuzzer (tsan)\": TestConfig(\n+            \"package_tsan\", job_config=JobConfig(**astfuzzer_test_common_params)  # type: ignore\n+        ),\n+        \"AST fuzzer (ubsan)\": TestConfig(\n+            \"package_ubsan\", job_config=JobConfig(**astfuzzer_test_common_params)  # type: ignore\n+        ),\n+        \"Stateless tests flaky check (asan)\": TestConfig(\n+            # replace to non-default\n+            \"package_asan\",\n+            job_config=JobConfig(**{**statless_test_common_params, \"timeout\": 3600}),  # type: ignore\n+        ),\n+        # FIXME: add digest and params\n         \"ClickHouse Keeper Jepsen\": TestConfig(\"binary_release\"),\n+        # FIXME: add digest and params\n         \"ClickHouse Server Jepsen\": TestConfig(\"binary_release\"),\n-        \"Performance Comparison\": TestConfig(\"package_release\"),\n-        \"Performance Comparison Aarch64\": TestConfig(\"package_aarch64\"),\n-        \"SQLancer (release)\": TestConfig(\"package_release\"),\n-        \"SQLancer (debug)\": TestConfig(\"package_debug\"),\n-        \"Sqllogic test (release)\": TestConfig(\"package_release\"),\n-        \"SQLTest\": TestConfig(\"package_release\"),\n+        \"Performance Comparison\": TestConfig(\n+            \"package_release\",\n+            job_config=JobConfig(num_batches=4, **perf_test_common_params),  # type: ignore\n+        ),\n+        \"Performance Comparison Aarch64\": TestConfig(\n+            \"package_aarch64\",\n+            job_config=JobConfig(num_batches=4, run_by_label=\"pr-performance\", **perf_test_common_params),  # type: ignore\n+        ),\n+        \"SQLancer (release)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**sqllancer_test_common_params)  # type: ignore\n+        ),\n+        \"SQLancer (debug)\": TestConfig(\n+            \"package_debug\", job_config=JobConfig(**sqllancer_test_common_params)  # type: ignore\n+        ),\n+        \"Sqllogic test (release)\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**sqllogic_test_params)  # type: ignore\n+        ),\n+        \"SQLTest\": TestConfig(\n+            \"package_release\", job_config=JobConfig(**sql_test_params)  # type: ignore\n+        ),\n         \"ClickBench (amd64)\": TestConfig(\"package_release\"),\n         \"ClickBench (aarch64)\": TestConfig(\"package_aarch64\"),\n-        \"libFuzzer tests\": TestConfig(\"fuzzers\"),\n+        # FIXME: add digest and params\n+        \"libFuzzer tests\": TestConfig(\"fuzzers\"),  # type: ignore\n     },\n )\n CI_CONFIG.validate()\ndiff --git a/tests/ci/clickbench.py b/tests/ci/clickbench.py\nindex 2ea5e39ce8e3..26a826a19ad9 100644\n--- a/tests/ci/clickbench.py\n+++ b/tests/ci/clickbench.py\n@@ -25,8 +25,8 @@\n     post_commit_status,\n     update_mergeable_check,\n )\n-from docker_pull_helper import DockerImage, get_image_with_version\n-from env_helper import TEMP_PATH, REPORTS_PATH\n+from docker_images_helper import get_docker_image, pull_image, DockerImage\n+from env_helper import TEMP_PATH, REPORT_PATH\n from get_robot_token import get_best_robot_token\n from pr_info import FORCE_TESTS_LABEL, PRInfo\n from s3_helper import S3Helper\n@@ -123,7 +123,7 @@ def main():\n     temp_path = Path(TEMP_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n \n-    reports_path = Path(REPORTS_PATH)\n+    reports_path = Path(REPORT_PATH)\n \n     args = parse_args()\n     check_name = args.check_name\n@@ -141,7 +141,7 @@ def main():\n         sys.exit(0)\n \n     image_name = get_image_name()\n-    docker_image = get_image_with_version(reports_path, image_name)\n+    docker_image = pull_image(get_docker_image(image_name))\n \n     packages_path = temp_path / \"packages\"\n     packages_path.mkdir(parents=True, exist_ok=True)\n@@ -205,7 +205,9 @@ def main():\n     )\n \n     print(f\"::notice:: {check_name} Report url: {report_url}\")\n-    post_commit_status(commit, state, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, state, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/commit_status_helper.py b/tests/ci/commit_status_helper.py\nindex 09e3478b3fcd..2eac974858c5 100644\n--- a/tests/ci/commit_status_helper.py\n+++ b/tests/ci/commit_status_helper.py\n@@ -1,23 +1,25 @@\n #!/usr/bin/env python3\n \n from collections import defaultdict\n+import json\n from pathlib import Path\n from typing import Dict, List, Optional, Union\n import csv\n import logging\n import time\n+from dataclasses import asdict, dataclass\n \n from github import Github\n from github.Commit import Commit\n from github.CommitStatus import CommitStatus\n from github.GithubException import GithubException\n-from github.GithubObject import _NotSetType, NotSet as NotSet\n+from github.GithubObject import NotSet\n from github.IssueComment import IssueComment\n from github.PullRequest import PullRequest\n from github.Repository import Repository\n \n from ci_config import CI_CONFIG, REQUIRED_CHECKS, CHECK_DESCRIPTIONS, CheckDescription\n-from env_helper import GITHUB_REPOSITORY, GITHUB_RUN_URL\n+from env_helper import GITHUB_REPOSITORY, GITHUB_RUN_URL, TEMP_PATH\n from pr_info import PRInfo, SKIP_MERGEABLE_CHECK_LABEL\n from report import (\n     ERROR,\n@@ -37,6 +39,7 @@\n MERGEABLE_NAME = \"Mergeable Check\"\n GH_REPO = None  # type: Optional[Repository]\n CI_STATUS_NAME = \"CI running\"\n+STATUS_FILE_PATH = Path(TEMP_PATH) / \"status.json\"\n \n \n class RerunHelper:\n@@ -92,10 +95,11 @@ def get_commit(gh: Github, commit_sha: str, retry_count: int = RETRY) -> Commit:\n def post_commit_status(\n     commit: Commit,\n     state: str,\n-    report_url: Union[_NotSetType, str] = NotSet,\n-    description: Union[_NotSetType, str] = NotSet,\n-    check_name: Union[_NotSetType, str] = NotSet,\n+    report_url: Optional[str] = None,\n+    description: Optional[str] = None,\n+    check_name: Optional[str] = None,\n     pr_info: Optional[PRInfo] = None,\n+    dump_to_file: bool = False,\n ) -> None:\n     \"\"\"The parameters are given in the same order as for commit.create_status,\n     if an optional parameter `pr_info` is given, the `set_status_comment` functions\n@@ -104,9 +108,9 @@ def post_commit_status(\n         try:\n             commit.create_status(\n                 state=state,\n-                target_url=report_url,\n-                description=description,\n-                context=check_name,\n+                target_url=report_url if report_url is not None else NotSet,\n+                description=description if description is not None else NotSet,\n+                context=check_name if check_name is not None else NotSet,\n             )\n             break\n         except Exception as ex:\n@@ -129,6 +133,15 @@ def post_commit_status(\n \n         if not status_updated:\n             logging.error(\"Failed to update the status comment, continue anyway\")\n+    if dump_to_file:\n+        assert pr_info\n+        CommitStatusData(\n+            status=state,\n+            description=description or \"\",\n+            report_url=report_url or \"\",\n+            sha=pr_info.sha,\n+            pr_num=pr_info.number,\n+        ).dump_status()\n \n \n STATUS_ICON_MAP = defaultdict(\n@@ -309,6 +322,55 @@ def post_commit_status_to_file(\n         out.writerow([state, report_url, description])\n \n \n+@dataclass\n+class CommitStatusData:\n+    \"\"\"\n+    if u about to add/remove fields in this class be causious that it dumps/loads to/from files (see it's method)\n+    - you might want to add default values for new fields so that it won't break with old files\n+    \"\"\"\n+\n+    status: str\n+    report_url: str\n+    description: str\n+    sha: str = \"deadbeaf\"\n+    pr_num: int = -1\n+\n+    @classmethod\n+    def _filter_dict(cls, data: dict) -> Dict:\n+        return {k: v for k, v in data.items() if k in cls.__annotations__.keys()}\n+\n+    @classmethod\n+    def load_from_file(cls, file_path: Union[Path, str]):  # type: ignore\n+        res = {}\n+        with open(file_path, \"r\") as json_file:\n+            res = json.load(json_file)\n+        return CommitStatusData(**cls._filter_dict(res))\n+\n+    @classmethod\n+    def load_status(cls):  # type: ignore\n+        return cls.load_from_file(STATUS_FILE_PATH)\n+\n+    @classmethod\n+    def is_present(cls) -> bool:\n+        return STATUS_FILE_PATH.is_file()\n+\n+    def dump_status(self) -> None:\n+        STATUS_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)\n+        self.dump_to_file(STATUS_FILE_PATH)\n+\n+    def dump_to_file(self, file_path: Union[Path, str]) -> None:\n+        file_path = Path(file_path) or STATUS_FILE_PATH\n+        with open(file_path, \"w\") as json_file:\n+            json.dump(asdict(self), json_file)\n+\n+    def is_ok(self):\n+        return self.status == SUCCESS\n+\n+    @staticmethod\n+    def cleanup():\n+        STATUS_FILE_PATH.unlink(missing_ok=True)\n+\n+\n def get_commit_filtered_statuses(commit: Commit) -> CommitStatuses:\n     \"\"\"\n     Squash statuses to latest state\ndiff --git a/tests/ci/compatibility_check.py b/tests/ci/compatibility_check.py\nindex 8f6d4917efe6..e3da81a54ad0 100644\n--- a/tests/ci/compatibility_check.py\n+++ b/tests/ci/compatibility_check.py\n@@ -16,8 +16,8 @@\n     prepare_tests_results_for_clickhouse,\n )\n from commit_status_helper import RerunHelper, get_commit, post_commit_status\n-from docker_pull_helper import get_images_with_versions, DockerImage\n-from env_helper import TEMP_PATH, REPORTS_PATH\n+from docker_images_helper import DockerImage, get_docker_image, pull_image\n+from env_helper import TEMP_PATH, REPORT_PATH\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import TestResults, TestResult\n@@ -145,8 +145,9 @@ def main():\n     stopwatch = Stopwatch()\n \n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n-    reports_path = Path(REPORTS_PATH)\n+    reports_path.mkdir(parents=True, exist_ok=True)\n \n     pr_info = PRInfo()\n \n@@ -187,15 +188,14 @@ def url_filter(url):\n         run_commands.extend(check_glibc_commands)\n \n     if args.check_distributions:\n-        docker_images = get_images_with_versions(\n-            reports_path, [IMAGE_CENTOS, IMAGE_UBUNTU]\n-        )\n+        centos_image = pull_image(get_docker_image(IMAGE_CENTOS))\n+        ubuntu_image = pull_image(get_docker_image(IMAGE_UBUNTU))\n         check_distributions_commands = get_run_commands_distributions(\n             packages_path,\n             result_path,\n             server_log_path,\n-            docker_images[0],\n-            docker_images[1],\n+            centos_image,\n+            ubuntu_image,\n         )\n         run_commands.extend(check_distributions_commands)\n \n@@ -239,7 +239,15 @@ def url_filter(url):\n         args.check_name,\n     )\n     print(f\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, state, report_url, description, args.check_name, pr_info)\n+    post_commit_status(\n+        commit,\n+        state,\n+        report_url,\n+        description,\n+        args.check_name,\n+        pr_info,\n+        dump_to_file=True,\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/digest_helper.py b/tests/ci/digest_helper.py\nindex 543de51e46b2..c0cbae13a45d 100644\n--- a/tests/ci/digest_helper.py\n+++ b/tests/ci/digest_helper.py\n@@ -1,11 +1,20 @@\n #!/usr/bin/env python3\n \n+import bisect\n+from dataclasses import asdict\n from hashlib import md5\n from logging import getLogger\n from pathlib import Path\n-from typing import TYPE_CHECKING, Iterable, Optional\n+from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Union\n from sys import modules\n \n+from docker_images_helper import get_images_info\n+from ci_config import DigestConfig\n+from git_helper import Runner\n+\n+DOCKER_DIGEST_LEN = 12\n+JOB_DIGEST_LEN = 10\n+\n if TYPE_CHECKING:\n     from hashlib import (  # pylint:disable=no-name-in-module,ungrouped-imports\n         _Hash as HASH,\n@@ -23,46 +32,55 @@ def _digest_file(file: Path, hash_object: HASH) -> None:\n             hash_object.update(chunk)\n \n \n-def _digest_directory(directory: Path, hash_object: HASH) -> None:\n-    assert directory.is_dir()\n-    for p in sorted(directory.rglob(\"*\")):\n-        if p.is_symlink() and p.is_dir():\n-            # The symlink directory is not listed recursively, so we process it manually\n-            (_digest_directory(p, hash_object))\n-        if p.is_file():\n-            (_digest_file(p, hash_object))\n-\n-\n-def digest_path(path: Path, hash_object: Optional[HASH] = None) -> HASH:\n+def digest_path(\n+    path: Union[Path, str],\n+    hash_object: Optional[HASH] = None,\n+    exclude_files: Optional[Iterable[str]] = None,\n+    exclude_dirs: Optional[Iterable[Union[Path, str]]] = None,\n+) -> HASH:\n     \"\"\"Calculates md5 (or updates existing hash_object) hash of the path, either it's\n-    directory or file\"\"\"\n+    directory or file\n+    @exclude_files - file extension(s) or any filename suffix(es) that you want to exclude from digest\n+    @exclude_dirs - dir names that you want to exclude from digest\n+    \"\"\"\n+    path = Path(path)\n     hash_object = hash_object or md5()\n-    if path.is_dir():\n-        _digest_directory(path, hash_object)\n-    elif path.is_file():\n-        _digest_file(path, hash_object)\n+    if path.is_file():\n+        if not exclude_files or not any(path.name.endswith(x) for x in exclude_files):\n+            _digest_file(path, hash_object)\n+    elif path.is_dir():\n+        if not exclude_dirs or not any(path.name == x for x in exclude_dirs):\n+            for p in sorted(path.iterdir()):\n+                digest_path(p, hash_object, exclude_files, exclude_dirs)\n+    else:\n+        pass  # broken symlink\n     return hash_object\n \n \n-def digest_paths(paths: Iterable[Path], hash_object: Optional[HASH] = None) -> HASH:\n+def digest_paths(\n+    paths: Iterable[Union[Path, str]],\n+    hash_object: Optional[HASH] = None,\n+    exclude_files: Optional[Iterable[str]] = None,\n+    exclude_dirs: Optional[Iterable[Union[Path, str]]] = None,\n+) -> HASH:\n     \"\"\"Calculates aggregated md5 (or updates existing hash_object) hash of passed paths.\n     The order is processed as given\"\"\"\n     hash_object = hash_object or md5()\n-    for path in paths:\n+    paths_all: List[Path] = []\n+    for p in paths:\n+        if isinstance(p, str) and \"*\" in p:\n+            for path in Path(\".\").glob(p):\n+                bisect.insort(paths_all, path.absolute())  # type: ignore[misc]\n+        else:\n+            bisect.insort(paths_all, Path(p).absolute())  # type: ignore[misc]\n+    for path in paths_all:  # type: ignore\n         if path.exists():\n-            digest_path(path, hash_object)\n+            digest_path(path, hash_object, exclude_files, exclude_dirs)\n+        else:\n+            raise AssertionError(f\"Invalid path: {path}\")\n     return hash_object\n \n \n-def digest_consistent_paths(\n-    paths: Iterable[Path], hash_object: Optional[HASH] = None\n-) -> HASH:\n-    \"\"\"Calculates aggregated md5 (or updates existing hash_object) hash of passed paths.\n-    The order doesn't matter, paths are converted to `absolute` and ordered before\n-    calculation\"\"\"\n-    return digest_paths(sorted(p.absolute() for p in paths), hash_object)\n-\n-\n def digest_script(path_str: str) -> HASH:\n     \"\"\"Accepts value of the __file__ executed script and calculates the md5 hash for it\"\"\"\n     path = Path(path_str)\n@@ -78,3 +96,85 @@ def digest_script(path_str: str) -> HASH:\n         logger.warning(\"The modules size has changed, retry calculating digest\")\n         return digest_script(path_str)\n     return md5_hash\n+\n+\n+def digest_string(string: str) -> str:\n+    hash_object = md5()\n+    hash_object.update(string.encode(\"utf-8\"))\n+    return hash_object.hexdigest()\n+\n+\n+class DockerDigester:\n+    EXCLUDE_FILES = [\".md\"]\n+\n+    def __init__(self):\n+        self.images_info = get_images_info()\n+        assert self.images_info, \"Fetch image info error\"\n+\n+    def get_image_digest(self, name: str) -> str:\n+        assert isinstance(name, str)\n+        deps = [name]\n+        digest = None\n+        while deps:\n+            dep_name = deps.pop(0)\n+            digest = digest_path(\n+                self.images_info[dep_name][\"path\"],\n+                digest,\n+                exclude_files=self.EXCLUDE_FILES,\n+            )\n+            deps += self.images_info[dep_name][\"deps\"]\n+        assert digest\n+        return digest.hexdigest()[0:DOCKER_DIGEST_LEN]\n+\n+    def get_all_digests(self) -> Dict:\n+        res = {}\n+        for image_name in self.images_info:\n+            res[image_name] = self.get_image_digest(image_name)\n+        return res\n+\n+\n+class JobDigester:\n+    def __init__(self):\n+        self.dd = DockerDigester()\n+        self.cache: Dict[str, str] = {}\n+\n+    @staticmethod\n+    def _get_config_hash(digest_config: DigestConfig) -> str:\n+        data_dict = asdict(digest_config)\n+        hash_obj = md5()\n+        hash_obj.update(str(data_dict).encode())\n+        hash_string = hash_obj.hexdigest()\n+        return hash_string\n+\n+    def get_job_digest(self, digest_config: DigestConfig) -> str:\n+        if not digest_config.include_paths:\n+            # job is not for digest\n+            return \"f\" * JOB_DIGEST_LEN\n+\n+        cache_key = self._get_config_hash(digest_config)\n+        if cache_key in self.cache:\n+            return self.cache[cache_key]\n+\n+        digest_str: List[str] = []\n+        if digest_config.include_paths:\n+            digest = digest_paths(\n+                digest_config.include_paths,\n+                hash_object=None,\n+                exclude_files=digest_config.exclude_files,\n+                exclude_dirs=digest_config.exclude_dirs,\n+            )\n+            digest_str += (digest.hexdigest(),)\n+        if digest_config.docker:\n+            for image_name in digest_config.docker:\n+                image_digest = self.dd.get_image_digest(image_name)\n+                digest_str += (image_digest,)\n+        if digest_config.git_submodules:\n+            submodules_sha = Runner().run(\n+                \"git submodule | awk '{print $1}' | sed 's/^[+-]//'\"\n+            )\n+            assert submodules_sha and len(submodules_sha) > 10\n+            submodules_digest = digest_string(\"-\".join(submodules_sha))\n+            digest_str += (submodules_digest,)\n+        res = digest_string(\"-\".join(digest_str))[0:JOB_DIGEST_LEN]\n+        self.cache[cache_key] = res\n+        return res\ndiff --git a/tests/ci/docker_images_check.py b/tests/ci/docker_images_check.py\nindex 274d0d1d1dfb..ae6e81c4c9f8 100644\n--- a/tests/ci/docker_images_check.py\n+++ b/tests/ci/docker_images_check.py\n@@ -2,210 +2,43 @@\n import argparse\n import json\n import logging\n-import platform\n-import subprocess\n+import os\n import time\n import sys\n from pathlib import Path\n-from typing import Any, List, Optional, Set, Tuple, Union\n+from typing import List, Optional, Tuple\n \n from github import Github\n \n from clickhouse_helper import ClickHouseHelper, prepare_tests_results_for_clickhouse\n from commit_status_helper import format_description, get_commit, post_commit_status\n-from env_helper import REPO_COPY, RUNNER_TEMP, GITHUB_RUN_URL\n-from get_robot_token import get_best_robot_token, get_parameter_from_ssm\n+from env_helper import ROOT_DIR, RUNNER_TEMP, GITHUB_RUN_URL\n+from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import TestResults, TestResult\n from s3_helper import S3Helper\n from stopwatch import Stopwatch\n from tee_popen import TeePopen\n from upload_result_helper import upload_results\n-from docker_images_helper import ImagesDict, IMAGES_FILE_PATH, get_images_dict\n+from docker_images_helper import DockerImageData, docker_login, get_images_oredered_list\n \n NAME = \"Push to Dockerhub\"\n TEMP_PATH = Path(RUNNER_TEMP) / \"docker_images_check\"\n TEMP_PATH.mkdir(parents=True, exist_ok=True)\n \n \n-class DockerImage:\n-    def __init__(\n-        self,\n-        path: str,\n-        repo: str,\n-        only_amd64: bool,\n-        parent: Optional[\"DockerImage\"] = None,\n-        gh_repo: str = REPO_COPY,\n-    ):\n-        assert not path.startswith(\"/\")\n-        self.path = path\n-        self.full_path = Path(gh_repo) / path\n-        self.repo = repo\n-        self.only_amd64 = only_amd64\n-        self.parent = parent\n-        self.built = False\n-\n-    def __eq__(self, other) -> bool:  # type: ignore\n-        \"\"\"Is used to check if DockerImage is in a set or not\"\"\"\n-        return (\n-            self.path == other.path\n-            and self.repo == self.repo\n-            and self.only_amd64 == other.only_amd64\n-        )\n-\n-    def __lt__(self, other: Any) -> bool:\n-        if not isinstance(other, DockerImage):\n-            return False\n-        if self.parent and not other.parent:\n-            return False\n-        if not self.parent and other.parent:\n-            return True\n-        if self.path < other.path:\n-            return True\n-        if self.repo < other.repo:\n-            return True\n-        return False\n-\n-    def __hash__(self):\n-        return hash(self.path)\n-\n-    def __str__(self):\n-        return self.repo\n-\n-    def __repr__(self):\n-        return f\"DockerImage(path={self.path},repo={self.repo},parent={self.parent})\"\n-\n-\n-def get_changed_docker_images(\n-    pr_info: PRInfo, images_dict: ImagesDict\n-) -> Set[DockerImage]:\n-    if not images_dict:\n-        return set()\n-\n-    files_changed = pr_info.changed_files\n-\n-    logging.info(\n-        \"Changed files for PR %s @ %s: %s\",\n-        pr_info.number,\n-        pr_info.sha,\n-        str(files_changed),\n-    )\n-\n-    changed_images = []\n-\n-    for dockerfile_dir, image_description in images_dict.items():\n-        for f in files_changed:\n-            if f.startswith(dockerfile_dir):\n-                name = image_description[\"name\"]\n-                only_amd64 = image_description.get(\"only_amd64\", False)\n-                logging.info(\n-                    \"Found changed file '%s' which affects \"\n-                    \"docker image '%s' with path '%s'\",\n-                    f,\n-                    name,\n-                    dockerfile_dir,\n-                )\n-                changed_images.append(DockerImage(dockerfile_dir, name, only_amd64))\n-                break\n-\n-    # The order is important: dependents should go later than bases, so that\n-    # they are built with updated base versions.\n-    index = 0\n-    while index < len(changed_images):\n-        image = changed_images[index]\n-        for dependent in images_dict[image.path][\"dependent\"]:\n-            logging.info(\n-                \"Marking docker image '%s' as changed because it \"\n-                \"depends on changed docker image '%s'\",\n-                dependent,\n-                image,\n-            )\n-            name = images_dict[dependent][\"name\"]\n-            only_amd64 = images_dict[dependent].get(\"only_amd64\", False)\n-            changed_images.append(DockerImage(dependent, name, only_amd64, image))\n-        index += 1\n-        if index > 5 * len(images_dict):\n-            # Sanity check to prevent infinite loop.\n-            raise RuntimeError(\n-                f\"Too many changed docker images, this is a bug. {changed_images}\"\n-            )\n-\n-    # With reversed changed_images set will use images with parents first, and\n-    # images without parents then\n-    result = set(reversed(changed_images))\n-    logging.info(\n-        \"Changed docker images for PR %s @ %s: '%s'\",\n-        pr_info.number,\n-        pr_info.sha,\n-        result,\n-    )\n-    return result\n-\n-\n-def gen_versions(\n-    pr_info: PRInfo, suffix: Optional[str]\n-) -> Tuple[List[str], Union[str, List[str]]]:\n-    pr_commit_version = str(pr_info.number) + \"-\" + pr_info.sha\n-    # The order is important, PR number is used as cache during the build\n-    versions = [str(pr_info.number), pr_commit_version]\n-    result_version = pr_commit_version  # type: Union[str, List[str]]\n-    if pr_info.number == 0 and pr_info.base_ref == \"master\":\n-        # First get the latest for cache\n-        versions.insert(0, \"latest\")\n-\n-    if suffix:\n-        # We should build architecture specific images separately and merge a\n-        # manifest lately in a different script\n-        versions = [f\"{v}-{suffix}\" for v in versions]\n-        # changed_images_{suffix}.json should contain all changed images\n-        result_version = versions\n-\n-    return versions, result_version\n-\n-\n-def build_and_push_dummy_image(\n-    image: DockerImage,\n-    version_string: str,\n-    push: bool,\n-) -> Tuple[bool, Path]:\n-    dummy_source = \"ubuntu:20.04\"\n-    logging.info(\"Building docker image %s as %s\", image.repo, dummy_source)\n-    build_log = (\n-        Path(TEMP_PATH)\n-        / f\"build_and_push_log_{image.repo.replace('/', '_')}_{version_string}.log\"\n-    )\n-    cmd = (\n-        f\"docker pull {dummy_source}; \"\n-        f\"docker tag {dummy_source} {image.repo}:{version_string}; \"\n-    )\n-    if push:\n-        cmd += f\"docker push {image.repo}:{version_string}\"\n-\n-    logging.info(\"Docker command to run: %s\", cmd)\n-    with TeePopen(cmd, build_log) as proc:\n-        retcode = proc.wait()\n-\n-    if retcode != 0:\n-        return False, build_log\n-\n-    logging.info(\"Processing of %s successfully finished\", image.repo)\n-    return True, build_log\n-\n-\n def build_and_push_one_image(\n-    image: DockerImage,\n+    image: DockerImageData,\n     version_string: str,\n     additional_cache: List[str],\n     push: bool,\n-    child: bool,\n+    from_tag: Optional[str] = None,\n ) -> Tuple[bool, Path]:\n-    if image.only_amd64 and platform.machine() not in [\"amd64\", \"x86_64\"]:\n-        return build_and_push_dummy_image(image, version_string, push)\n     logging.info(\n         \"Building docker image %s with version %s from path %s\",\n         image.repo,\n         version_string,\n-        image.full_path,\n+        image.path,\n     )\n     build_log = (\n         Path(TEMP_PATH)\n@@ -216,8 +49,8 @@ def build_and_push_one_image(\n         push_arg = \"--push \"\n \n     from_tag_arg = \"\"\n-    if child:\n-        from_tag_arg = f\"--build-arg FROM_TAG={version_string} \"\n+    if from_tag:\n+        from_tag_arg = f\"--build-arg FROM_TAG={from_tag} \"\n \n     cache_from = (\n         f\"--cache-from type=registry,ref={image.repo}:{version_string} \"\n@@ -237,7 +70,7 @@ def build_and_push_one_image(\n         f\"{cache_from} \"\n         f\"--cache-to type=inline,mode=max \"\n         f\"{push_arg}\"\n-        f\"--progress plain {image.full_path}\"\n+        f\"--progress plain {image.path}\"\n     )\n     logging.info(\"Docker command to run: %s\", cmd)\n     with TeePopen(cmd, build_log) as proc:\n@@ -251,11 +84,11 @@ def build_and_push_one_image(\n \n \n def process_single_image(\n-    image: DockerImage,\n+    image: DockerImageData,\n     versions: List[str],\n     additional_cache: List[str],\n     push: bool,\n-    child: bool,\n+    from_tag: Optional[str] = None,\n ) -> TestResults:\n     logging.info(\"Image will be pushed with versions %s\", \", \".join(versions))\n     results = []  # type: TestResults\n@@ -263,7 +96,7 @@ def process_single_image(\n         stopwatch = Stopwatch()\n         for i in range(5):\n             success, build_log = build_and_push_one_image(\n-                image, ver, additional_cache, push, child\n+                image, ver, additional_cache, push, from_tag\n             )\n             if success:\n                 results.append(\n@@ -294,27 +127,6 @@ def process_single_image(\n     return results\n \n \n-def process_image_with_parents(\n-    image: DockerImage,\n-    versions: List[str],\n-    additional_cache: List[str],\n-    push: bool,\n-    child: bool = False,\n-) -> TestResults:\n-    results = []  # type: TestResults\n-    if image.built:\n-        return results\n-\n-    if image.parent is not None:\n-        results += process_image_with_parents(\n-            image.parent, versions, additional_cache, push, False\n-        )\n-        child = True\n-\n-    results += process_single_image(image, versions, additional_cache, push, child)\n-    return results\n-\n-\n def parse_args() -> argparse.Namespace:\n     parser = argparse.ArgumentParser(\n         formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n@@ -324,30 +136,18 @@ def parse_args() -> argparse.Namespace:\n         \"--image-path docker/packager/binary\",\n     )\n \n+    parser.add_argument(\"--suffix\", type=str, required=True, help=\"arch suffix\")\n     parser.add_argument(\n-        \"--suffix\",\n+        \"--missing-images\",\n         type=str,\n-        help=\"suffix for all built images tags and resulting json file; the parameter \"\n-        \"significantly changes the script behavior, e.g. changed_images.json is called \"\n-        \"changed_images_{suffix}.json and contains list of all tags\",\n-    )\n-    parser.add_argument(\n-        \"--repo\",\n-        type=str,\n-        default=\"clickhouse\",\n-        help=\"docker hub repository prefix\",\n-    )\n-    parser.add_argument(\n-        \"--all\",\n-        action=\"store_true\",\n-        help=\"rebuild all images\",\n+        required=True,\n+        help=\"json string or json file with images to build {IMAGE: TAG} or type all to build all\",\n     )\n     parser.add_argument(\n-        \"--image-path\",\n+        \"--image-tags\",\n         type=str,\n-        nargs=\"*\",\n-        help=\"list of image paths to build instead of using pr_info + diff URL, \"\n-        \"e.g. 'docker/packager/binary'\",\n+        required=True,\n+        help=\"json string or json file with all images and their tags {IMAGE: TAG}\",\n     )\n     parser.add_argument(\"--reports\", default=True, help=argparse.SUPPRESS)\n     parser.add_argument(\n@@ -370,82 +170,81 @@ def parse_args() -> argparse.Namespace:\n \n \n def main():\n+    # to be always aligned with docker paths from image.json\n+    os.chdir(ROOT_DIR)\n     logging.basicConfig(level=logging.INFO)\n     stopwatch = Stopwatch()\n \n     args = parse_args()\n-    if args.suffix:\n-        global NAME\n-        NAME += f\" {args.suffix}\"\n-        changed_json = TEMP_PATH / f\"changed_images_{args.suffix}.json\"\n-    else:\n-        changed_json = TEMP_PATH / \"changed_images.json\"\n-\n     if args.push:\n-        subprocess.check_output(  # pylint: disable=unexpected-keyword-arg\n-            \"docker login --username 'robotclickhouse' --password-stdin\",\n-            input=get_parameter_from_ssm(\"dockerhub_robot_password\"),\n-            encoding=\"utf-8\",\n-            shell=True,\n-        )\n-\n-    images_dict = get_images_dict(Path(REPO_COPY), IMAGES_FILE_PATH)\n-\n-    pr_info = PRInfo()\n-    if args.all:\n-        pr_info.changed_files = set(images_dict.keys())\n-    elif args.image_path:\n-        pr_info.changed_files = set(i for i in args.image_path)\n-    else:\n-        try:\n-            pr_info.fetch_changed_files()\n-        except TypeError:\n-            # If the event does not contain diff, nothing will be built\n-            pass\n-\n-    changed_images = get_changed_docker_images(pr_info, images_dict)\n-    if changed_images:\n-        logging.info(\n-            \"Has changed images: %s\", \", \".join([im.path for im in changed_images])\n-        )\n+        logging.info(\"login to docker hub\")\n+        docker_login()\n \n-    image_versions, result_version = gen_versions(pr_info, args.suffix)\n-\n-    result_images = {}\n     test_results = []  # type: TestResults\n     additional_cache = []  # type: List[str]\n-    if pr_info.release_pr:\n-        logging.info(\"Use %s as additional cache tag\", pr_info.release_pr)\n-        additional_cache.append(str(pr_info.release_pr))\n-    if pr_info.merged_pr:\n-        logging.info(\"Use %s as additional cache tag\", pr_info.merged_pr)\n-        additional_cache.append(str(pr_info.merged_pr))\n-\n-    for image in changed_images:\n-        # If we are in backport PR, then pr_info.release_pr is defined\n-        # We use it as tag to reduce rebuilding time\n-        test_results += process_image_with_parents(\n-            image, image_versions, additional_cache, args.push\n-        )\n-        result_images[image.repo] = result_version\n+    # FIXME: add all tags taht we need. latest on master!\n+    # if pr_info.release_pr:\n+    #     logging.info(\"Use %s as additional cache tag\", pr_info.release_pr)\n+    #     additional_cache.append(str(pr_info.release_pr))\n+    # if pr_info.merged_pr:\n+    #     logging.info(\"Use %s as additional cache tag\", pr_info.merged_pr)\n+    #     additional_cache.append(str(pr_info.merged_pr))\n+\n+    ok_cnt = 0\n+    status = \"success\"\n+    image_tags = (\n+        json.loads(args.image_tags)\n+        if not os.path.isfile(args.image_tags)\n+        else json.load(open(args.image_tags))\n+    )\n+    missing_images = (\n+        image_tags\n+        if args.missing_images == \"all\"\n+        else json.loads(args.missing_images)\n+        if not os.path.isfile(args.missing_images)\n+        else json.load(open(args.missing_images))\n+    )\n+    images_build_list = get_images_oredered_list()\n \n-    if changed_images:\n-        description = \"Updated \" + \",\".join([im.repo for im in changed_images])\n-    else:\n-        description = \"Nothing to update\"\n+    for image in images_build_list:\n+        if image.repo not in missing_images:\n+            continue\n+        logging.info(\"Start building image: %s\", image)\n \n-    description = format_description(description)\n+        image_versions = (\n+            [image_tags[image.repo]]\n+            if not args.suffix\n+            else [f\"{image_tags[image.repo]}-{args.suffix}\"]\n+        )\n+        parent_version = (\n+            None\n+            if not image.parent\n+            else image_tags[image.parent]\n+            if not args.suffix\n+            else f\"{image_tags[image.parent]}-{args.suffix}\"\n+        )\n \n-    with open(changed_json, \"w\", encoding=\"utf-8\") as images_file:\n-        logging.info(\"Saving changed images file %s\", changed_json)\n-        json.dump(result_images, images_file)\n+        res = process_single_image(\n+            image,\n+            image_versions,\n+            additional_cache,\n+            args.push,\n+            from_tag=parent_version,\n+        )\n+        test_results += res\n+        if all(x.status == \"OK\" for x in res):\n+            ok_cnt += 1\n+        else:\n+            status = \"failure\"\n+            break  # No need to continue with next images\n \n-    s3_helper = S3Helper()\n+    description = format_description(\n+        f\"Images build done. built {ok_cnt} out of {len(missing_images)} images.\"\n+    )\n \n-    status = \"success\"\n-    if [r for r in test_results if r.status != \"OK\"]:\n-        status = \"failure\"\n+    s3_helper = S3Helper()\n \n+    pr_info = PRInfo()\n     url = upload_results(s3_helper, pr_info.number, pr_info.sha, test_results, [], NAME)\n \n     print(f\"::notice ::Report url: {url}\")\n@@ -455,7 +254,9 @@ def main():\n \n     gh = Github(get_best_robot_token(), per_page=100)\n     commit = get_commit(gh, pr_info.sha)\n-    post_commit_status(commit, status, url, description, NAME, pr_info)\n+    post_commit_status(\n+        commit, status, url, description, NAME, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/docker_images_helper.py b/tests/ci/docker_images_helper.py\nindex b12f7fe037e7..6ea679e05973 100644\n--- a/tests/ci/docker_images_helper.py\n+++ b/tests/ci/docker_images_helper.py\n@@ -2,19 +2,136 @@\n \n import json\n import logging\n+import os\n+import subprocess\n from pathlib import Path\n-from typing import Dict, List\n+from typing import Any, Dict, List, Optional\n+\n+from env_helper import ROOT_DIR, DOCKER_TAG\n+from get_robot_token import get_parameter_from_ssm\n \n IMAGES_FILE_PATH = Path(\"docker/images.json\")\n \n ImagesDict = Dict[str, dict]\n \n \n-def get_images_dict(repo_path: Path, images_file_path: Path) -> ImagesDict:\n+def docker_login(relogin: bool = True) -> None:\n+    if (\n+        relogin\n+        or subprocess.run(  # pylint: disable=unexpected-keyword-arg\n+            \"docker system info | grep --quiet -E 'Username|Registry'\",\n+            shell=True,\n+            check=False,\n+        ).returncode\n+        == 1\n+    ):\n+        subprocess.check_output(  # pylint: disable=unexpected-keyword-arg\n+            \"docker login --username 'robotclickhouse' --password-stdin\",\n+            input=get_parameter_from_ssm(\"dockerhub_robot_password\"),\n+            encoding=\"utf-8\",\n+            shell=True,\n+        )\n+\n+\n+class DockerImage:\n+    def __init__(self, name: str, version: Optional[str] = None):\n+        self.name = name\n+        if version is None:\n+            self.version = \"latest\"\n+        else:\n+            self.version = version\n+\n+    def __str__(self):\n+        return f\"{self.name}:{self.version}\"\n+\n+\n+def pull_image(image: DockerImage) -> DockerImage:\n+    try:\n+        logging.info(\"Pulling image %s - start\", image)\n+        subprocess.check_output(\n+            f\"docker pull {image}\",\n+            stderr=subprocess.STDOUT,\n+            shell=True,\n+        )\n+        logging.info(\"Pulling image %s - done\", image)\n+    except Exception as ex:\n+        logging.info(\"Got execption pulling docker %s\", ex)\n+        raise ex\n+    return image\n+\n+\n+def get_docker_image(image_name: str) -> DockerImage:\n+    assert DOCKER_TAG and isinstance(DOCKER_TAG, str), \"DOCKER_TAG env must be provided\"\n+    if \"{\" in DOCKER_TAG:\n+        tags_map = json.loads(DOCKER_TAG)\n+        assert (\n+            image_name in tags_map\n+        ), \"Image name does not exist in provided DOCKER_TAG json string\"\n+        return DockerImage(image_name, tags_map[image_name])\n+    else:\n+        # DOCKER_TAG is a tag itself\n+        return DockerImage(image_name, DOCKER_TAG)\n+\n+\n+class DockerImageData:\n+    def __init__(\n+        self,\n+        path: str,\n+        repo: str,\n+        only_amd64: bool,\n+        parent: Optional[\"DockerImageData\"] = None,\n+    ):\n+        assert not path.startswith(\"/\")\n+        self.path = Path(ROOT_DIR) / path\n+        self.repo = repo\n+        self.only_amd64 = only_amd64\n+        self.parent = parent\n+        self.built = False\n+\n+    def __eq__(self, other) -> bool:  # type: ignore\n+        \"\"\"Is used to check if DockerImageData is in a set or not\"\"\"\n+        return (\n+            self.path == other.path\n+            and self.repo == self.repo\n+            and self.only_amd64 == other.only_amd64\n+        )\n+\n+    def __lt__(self, other: Any) -> bool:\n+        if not isinstance(other, DockerImageData):\n+            return False\n+        if self.parent and not other.parent:\n+            return False\n+        if not self.parent and other.parent:\n+            return True\n+        if self.path < other.path:\n+            return True\n+        if self.repo < other.repo:\n+            return True\n+        return False\n+\n+    def __hash__(self):\n+        return hash(self.path)\n+\n+    def __str__(self):\n+        return self.repo\n+\n+    def __repr__(self):\n+        return (\n+            f\"DockerImageData(path={self.path},repo={self.repo},parent={self.parent})\"\n+        )\n+\n+\n+def get_images_dict(\n+    repo_path: Optional[Path] = None, images_file_path: Optional[Path] = None\n+) -> ImagesDict:\n     \"\"\"Return images suppose to build on the current architecture host\"\"\"\n     images_dict = {}\n+    images_file_path = images_file_path if images_file_path else IMAGES_FILE_PATH\n     assert not images_file_path.is_absolute()\n-    path_to_images_file = repo_path / images_file_path\n+    cur_dir = os.path.dirname(__file__)\n+    path_to_images_file = (\n+        repo_path if repo_path else Path(f\"{cur_dir}/../..\") / images_file_path\n+    )\n     if path_to_images_file.exists():\n         with open(path_to_images_file, \"rb\") as dict_file:\n             images_dict = json.load(dict_file)\n@@ -26,6 +143,56 @@ def get_images_dict(repo_path: Path, images_file_path: Path) -> ImagesDict:\n     return images_dict\n \n \n-def get_image_names(repo_path: Path, images_file_path: Path) -> List[str]:\n+def get_image_names(\n+    repo_path: Optional[Path] = None, images_file_path: Optional[Path] = None\n+) -> List[str]:\n     images_dict = get_images_dict(repo_path, images_file_path)\n     return [info[\"name\"] for (_, info) in images_dict.items()]\n+\n+\n+def get_images_info() -> Dict[str, dict]:\n+    \"\"\"\n+    get docker info from images.json in format \"image name\" : image_info\n+    \"\"\"\n+    images_dict = get_images_dict()\n+    images_info: dict = {info[\"name\"]: {\"deps\": []} for _, info in images_dict.items()}\n+    for path, image_info_reversed in images_dict.items():\n+        name = image_info_reversed[\"name\"]\n+        dependents = image_info_reversed[\"dependent\"]\n+        only_amd64 = \"only_amd64\" in image_info_reversed\n+        images_info[name][\"path\"] = path\n+        images_info[name][\"only_amd64\"] = only_amd64\n+        for dep_path in dependents:\n+            name_dep = images_dict[dep_path][\"name\"]\n+            images_info[name_dep][\"deps\"] += [name]\n+    assert len(images_dict) == len(images_info), \"BUG!\"\n+    return images_info\n+\n+\n+def get_images_oredered_list() -> List[DockerImageData]:\n+    \"\"\"\n+    returns images in a sorted list so that dependents follow their dependees\n+    \"\"\"\n+    images_info = get_images_info()\n+\n+    ordered_images: List[DockerImageData] = []\n+    ordered_names: List[str] = []\n+    while len(ordered_names) < len(images_info):\n+        for name, info in images_info.items():\n+            if name in ordered_names:\n+                continue\n+            if all(dep in ordered_names for dep in info[\"deps\"]):\n+                ordered_names += [name]\n+                parents = info[\"deps\"]\n+                assert (\n+                    len(parents) < 2\n+                ), \"FIXME: Multistage docker images are not supported in CI\"\n+                ordered_images += [\n+                    DockerImageData(\n+                        path=info[\"path\"],\n+                        repo=name,\n+                        only_amd64=info[\"only_amd64\"],\n+                        parent=parents[0] if parents else None,\n+                    )\n+                ]\n+    return ordered_images\ndiff --git a/tests/ci/docker_manifests_merge.py b/tests/ci/docker_manifests_merge.py\nindex 1be2a1f2e7a7..d1801c2328bc 100644\n--- a/tests/ci/docker_manifests_merge.py\n+++ b/tests/ci/docker_manifests_merge.py\n@@ -6,30 +6,26 @@\n import os\n import subprocess\n \n-from pathlib import Path\n-from typing import List, Dict, Tuple\n+import sys\n+from typing import List, Tuple\n \n from github import Github\n \n from clickhouse_helper import (\n     ClickHouseHelper,\n     prepare_tests_results_for_clickhouse,\n-    CHException,\n )\n from commit_status_helper import format_description, get_commit, post_commit_status\n-from docker_images_helper import IMAGES_FILE_PATH, get_image_names\n-from env_helper import RUNNER_TEMP, REPO_COPY\n-from get_robot_token import get_best_robot_token, get_parameter_from_ssm\n-from git_helper import Runner\n+from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n-from report import TestResults, TestResult\n+from report import TestResult\n from s3_helper import S3Helper\n from stopwatch import Stopwatch\n+from env_helper import ROOT_DIR\n from upload_result_helper import upload_results\n+from docker_images_helper import docker_login, get_images_oredered_list\n \n NAME = \"Push multi-arch images to Dockerhub\"\n-CHANGED_IMAGES = \"changed_images_{}.json\"\n-Images = Dict[str, List[str]]\n \n \n def parse_args() -> argparse.Namespace:\n@@ -48,10 +44,21 @@ def parse_args() -> argparse.Namespace:\n         help=\"suffixes for existing images' tags. More than two should be given\",\n     )\n     parser.add_argument(\n-        \"--path\",\n-        type=Path,\n-        default=RUNNER_TEMP,\n-        help=\"path to changed_images_*.json files\",\n+        \"--missing-images\",\n+        type=str,\n+        required=True,\n+        help=\"json string or json file with images to build {IMAGE: TAG} or type all to build all\",\n+    )\n+    parser.add_argument(\n+        \"--image-tags\",\n+        type=str,\n+        required=True,\n+        help=\"json string or json file with all images and their tags {IMAGE: TAG}\",\n+    )\n+    parser.add_argument(\n+        \"--set-latest\",\n+        type=str,\n+        help=\"add latest tag\",\n     )\n     parser.add_argument(\"--reports\", default=True, help=argparse.SUPPRESS)\n     parser.add_argument(\n@@ -77,70 +84,13 @@ def parse_args() -> argparse.Namespace:\n     return args\n \n \n-def load_images(path: Path, suffix: str) -> Images:\n-    with open(path / CHANGED_IMAGES.format(suffix), \"rb\") as images:\n-        return json.load(images)  # type: ignore\n-\n-\n-def strip_suffix(suffix: str, images: Images) -> Images:\n-    result = {}\n-    for image, versions in images.items():\n-        for v in versions:\n-            if not v.endswith(f\"-{suffix}\"):\n-                raise ValueError(\n-                    f\"version {image}:{v} does not contain suffix {suffix}\"\n-                )\n-        result[image] = [v[: -len(suffix) - 1] for v in versions]\n-\n-    return result\n-\n-\n-def check_sources(to_merge: Dict[str, Images]) -> Images:\n-    \"\"\"get a dict {arch1: Images, arch2: Images}\"\"\"\n-    result = {}  # type: Images\n-    first_suffix = \"\"\n-    for suffix, images in to_merge.items():\n-        if not result:\n-            first_suffix = suffix\n-            result = strip_suffix(suffix, images)\n-            continue\n-        if not result == strip_suffix(suffix, images):\n-            raise ValueError(\n-                f\"images in {images} are not equal to {to_merge[first_suffix]}\"\n-            )\n-\n-    return result\n-\n-\n-def get_changed_images(images: Images) -> Dict[str, str]:\n-    \"\"\"The original json format is {\"image\": \"tag\"}, so the output artifact is\n-    produced here. The latest version is {PR_NUMBER}-{SHA1}\n-    \"\"\"\n-    return {k: v[-1] for k, v in images.items()}\n-\n-\n-def merge_images(to_merge: Dict[str, Images]) -> Dict[str, List[List[str]]]:\n-    \"\"\"The function merges image-name:version-suffix1 and image-name:version-suffix2\n-    into image-name:version\"\"\"\n-    suffixes = to_merge.keys()\n-    result_images = check_sources(to_merge)\n-    merge = {}  # type: Dict[str, List[List[str]]]\n-\n-    for image, versions in result_images.items():\n-        merge[image] = []\n-        for i, v in enumerate(versions):\n-            merged_v = [v]  # type: List[str]\n-            for suf in suffixes:\n-                merged_v.append(to_merge[suf][image][i])\n-            merge[image].append(merged_v)\n-\n-    return merge\n-\n-\n-def create_manifest(image: str, tags: List[str], push: bool) -> Tuple[str, str]:\n-    tag = tags[0]\n-    manifest = f\"{image}:{tag}\"\n-    cmd = \"docker manifest create --amend \" + \" \".join((f\"{image}:{t}\" for t in tags))\n+def create_manifest(\n+    image: str, result_tag: str, tags: List[str], push: bool\n+) -> Tuple[str, str]:\n+    manifest = f\"{image}:{result_tag}\"\n+    cmd = \"docker manifest create --amend \" + \" \".join(\n+        (f\"{image}:{t}\" for t in [result_tag] + tags)\n+    )\n     logging.info(\"running: %s\", cmd)\n     with subprocess.Popen(\n         cmd,\n@@ -175,114 +125,51 @@ def create_manifest(image: str, tags: List[str], push: bool) -> Tuple[str, str]:\n     return manifest, \"OK\"\n \n \n-def enrich_images(changed_images: Dict[str, str]) -> None:\n-    all_image_names = get_image_names(Path(REPO_COPY), IMAGES_FILE_PATH)\n-\n-    images_to_find_tags_for = [\n-        image for image in all_image_names if image not in changed_images\n-    ]\n-    images_to_find_tags_for.sort()\n-\n-    logging.info(\n-        \"Trying to find versions for images:\\n %s\", \"\\n \".join(images_to_find_tags_for)\n-    )\n-\n-    COMMIT_SHA_BATCH_SIZE = 100\n-    MAX_COMMIT_BATCHES_TO_CHECK = 10\n-    # Gets the sha of the last COMMIT_SHA_BATCH_SIZE commits after skipping some commits (see below)\n-    LAST_N_ANCESTOR_SHA_COMMAND = f\"git log --format=format:'%H' --max-count={COMMIT_SHA_BATCH_SIZE} --skip={{}} --merges\"\n-    git_runner = Runner()\n-\n-    GET_COMMIT_SHAS_QUERY = \"\"\"\n-        WITH {commit_shas:Array(String)} AS commit_shas,\n-             {images:Array(String)} AS images\n-        SELECT\n-            splitByChar(':', test_name)[1] AS image_name,\n-            argMax(splitByChar(':', test_name)[2], check_start_time) AS tag\n-        FROM checks\n-            WHERE\n-                check_name == 'Push multi-arch images to Dockerhub'\n-                AND position(test_name, checks.commit_sha)\n-                AND checks.commit_sha IN commit_shas\n-                AND image_name IN images\n-        GROUP BY image_name\n-        \"\"\"\n-\n-    batch_count = 0\n-    # We use always publicly available DB here intentionally\n-    ch_helper = ClickHouseHelper(\n-        \"https://play.clickhouse.com\", {\"X-ClickHouse-User\": \"play\"}\n-    )\n-\n-    while (\n-        batch_count <= MAX_COMMIT_BATCHES_TO_CHECK and len(images_to_find_tags_for) != 0\n-    ):\n-        commit_shas = git_runner(\n-            LAST_N_ANCESTOR_SHA_COMMAND.format(batch_count * COMMIT_SHA_BATCH_SIZE)\n-        ).split(\"\\n\")\n-\n-        result = ch_helper.select_json_each_row(\n-            \"default\",\n-            GET_COMMIT_SHAS_QUERY,\n-            {\"commit_shas\": commit_shas, \"images\": images_to_find_tags_for},\n-        )\n-        result.sort(key=lambda x: x[\"image_name\"])\n-\n-        logging.info(\n-            \"Found images for commits %s..%s:\\n %s\",\n-            commit_shas[0],\n-            commit_shas[-1],\n-            \"\\n \".join(f\"{im['image_name']}:{im['tag']}\" for im in result),\n-        )\n-\n-        for row in result:\n-            image_name = row[\"image_name\"]\n-            changed_images[image_name] = row[\"tag\"]\n-            images_to_find_tags_for.remove(image_name)\n-\n-        batch_count += 1\n-\n-\n def main():\n+    # to be aligned with docker paths from image.json\n+    os.chdir(ROOT_DIR)\n     logging.basicConfig(level=logging.INFO)\n     stopwatch = Stopwatch()\n \n     args = parse_args()\n-    if args.push:\n-        subprocess.check_output(  # pylint: disable=unexpected-keyword-arg\n-            \"docker login --username 'robotclickhouse' --password-stdin\",\n-            input=get_parameter_from_ssm(\"dockerhub_robot_password\"),\n-            encoding=\"utf-8\",\n-            shell=True,\n-        )\n \n-    to_merge = {}\n-    for suf in args.suffixes:\n-        to_merge[suf] = load_images(args.path, suf)\n+    if args.push:\n+        docker_login()\n \n-    changed_images = get_changed_images(check_sources(to_merge))\n+    archs = args.suffixes\n+    assert len(archs) > 1, \"arch suffix input param is invalid\"\n \n-    os.environ[\"DOCKER_CLI_EXPERIMENTAL\"] = \"enabled\"\n-    merged = merge_images(to_merge)\n+    image_tags = (\n+        json.loads(args.image_tags)\n+        if not os.path.isfile(args.image_tags)\n+        else json.load(open(args.image_tags))\n+    )\n \n+    test_results = []\n     status = \"success\"\n-    test_results = []  # type: TestResults\n-    for image, versions in merged.items():\n-        for tags in versions:\n-            manifest, test_result = create_manifest(image, tags, args.push)\n-            test_results.append(TestResult(manifest, test_result))\n-            if test_result != \"OK\":\n-                status = \"failure\"\n \n-    enriched_images = changed_images.copy()\n-    try:\n-        # changed_images now contains all the images that are changed in this PR. Let's find the latest tag for the images that are not changed.\n-        enrich_images(enriched_images)\n-    except CHException as ex:\n-        logging.warning(\"Couldn't get proper tags for not changed images: %s\", ex)\n+    ok_cnt, fail_cnt = 0, 0\n+    images = get_images_oredered_list()\n+    for image_obj in images:\n+        tag = image_tags[image_obj.repo]\n+        if image_obj.only_amd64:\n+            # FIXME: WA until full arm support\n+            tags = [f\"{tag}-{arch}\" for arch in archs if arch != \"aarch64\"]\n+        else:\n+            tags = [f\"{tag}-{arch}\" for arch in archs]\n+        manifest, test_result = create_manifest(image_obj.repo, tag, tags, args.push)\n+        test_results.append(TestResult(manifest, test_result))\n+        if args.set_latest:\n+            manifest, test_result = create_manifest(\n+                image_obj.repo, \"latest\", tags, args.push\n+            )\n+            test_results.append(TestResult(manifest, test_result))\n \n-    with open(args.path / \"changed_images.json\", \"w\", encoding=\"utf-8\") as ci:\n-        json.dump(enriched_images, ci)\n+        if test_result != \"OK\":\n+            status = \"failure\"\n+            fail_cnt += 1\n+        else:\n+            ok_cnt += 1\n \n     pr_info = PRInfo()\n     s3_helper = S3Helper()\n@@ -294,16 +181,15 @@ def main():\n     if not args.reports:\n         return\n \n-    if changed_images:\n-        description = \"Updated \" + \", \".join(changed_images.keys())\n-    else:\n-        description = \"Nothing to update\"\n-\n-    description = format_description(description)\n+    description = format_description(\n+        f\"Multiarch images created [ok: {ok_cnt}, failed: {fail_cnt}]\"\n+    )\n \n     gh = Github(get_best_robot_token(), per_page=100)\n     commit = get_commit(gh, pr_info.sha)\n-    post_commit_status(commit, status, url, description, NAME, pr_info)\n+    post_commit_status(\n+        commit, status, url, description, NAME, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\n@@ -316,6 +202,8 @@ def main():\n     )\n     ch_helper = ClickHouseHelper()\n     ch_helper.insert_events_into(db=\"default\", table=\"checks\", events=prepared_events)\n+    if status == \"failure\":\n+        sys.exit(1)\n \n \n if __name__ == \"__main__\":\ndiff --git a/tests/ci/docker_pull_helper.py b/tests/ci/docker_pull_helper.py\ndeleted file mode 100644\nindex e1327f505a07..000000000000\n--- a/tests/ci/docker_pull_helper.py\n+++ /dev/null\n@@ -1,90 +0,0 @@\n-#!/usr/bin/env python3\n-\n-import os\n-import json\n-import time\n-import subprocess\n-import logging\n-\n-from pathlib import Path\n-from typing import List, Optional, Union\n-\n-\n-class DockerImage:\n-    def __init__(self, name: str, version: Optional[str] = None):\n-        self.name = name\n-        if version is None:\n-            self.version = \"latest\"\n-        else:\n-            self.version = version\n-\n-    def __str__(self):\n-        return f\"{self.name}:{self.version}\"\n-\n-\n-def get_images_with_versions(\n-    reports_path: Union[Path, str],\n-    required_images: List[str],\n-    pull: bool = True,\n-    version: Optional[str] = None,\n-) -> List[DockerImage]:\n-    images_path = None\n-    for root, _, files in os.walk(reports_path):\n-        for f in files:\n-            if f == \"changed_images.json\":\n-                images_path = os.path.join(root, \"changed_images.json\")\n-                break\n-\n-    if not images_path:\n-        logging.info(\"Images file not found\")\n-    else:\n-        logging.info(\"Images file path %s\", images_path)\n-\n-    if images_path is not None and os.path.exists(images_path):\n-        logging.info(\"Images file exists\")\n-        with open(images_path, \"r\", encoding=\"utf-8\") as images_fd:\n-            images = json.load(images_fd)\n-            logging.info(\"Got images %s\", images)\n-    else:\n-        images = {}\n-\n-    docker_images = []\n-    for image_name in required_images:\n-        docker_image = DockerImage(image_name, version)\n-        if image_name in images:\n-            docker_image.version = images[image_name]\n-        docker_images.append(docker_image)\n-\n-    latest_error = Exception(\"predefined to avoid access before created\")\n-    if pull:\n-        for docker_image in docker_images:\n-            for i in range(10):\n-                try:\n-                    logging.info(\"Pulling image %s\", docker_image)\n-                    subprocess.check_output(\n-                        f\"docker pull {docker_image}\",\n-                        stderr=subprocess.STDOUT,\n-                        shell=True,\n-                    )\n-                    break\n-                except Exception as ex:\n-                    latest_error = ex\n-                    time.sleep(i * 3)\n-                    logging.info(\"Got execption pulling docker %s\", ex)\n-            else:\n-                raise Exception(\n-                    \"Cannot pull dockerhub for image docker pull \"\n-                    f\"{docker_image} because of {latest_error}\"\n-                )\n-\n-    return docker_images\n-\n-\n-def get_image_with_version(\n-    reports_path: Union[Path, str],\n-    image: str,\n-    pull: bool = True,\n-    version: Optional[str] = None,\n-) -> DockerImage:\n-    logging.info(\"Looking for images file in %s\", reports_path)\n-    return get_images_with_versions(reports_path, [image], pull, version=version)[0]\ndiff --git a/tests/ci/docker_server.py b/tests/ci/docker_server.py\nindex 55bd2983ea48..b75808890bdc 100644\n--- a/tests/ci/docker_server.py\n+++ b/tests/ci/docker_server.py\n@@ -4,27 +4,33 @@\n import argparse\n import json\n import logging\n-import subprocess\n import sys\n import time\n from pathlib import Path\n from os import path as p, makedirs\n-from typing import List\n+from typing import Dict, List\n \n from github import Github\n \n from build_check import get_release_or_pr\n from clickhouse_helper import ClickHouseHelper, prepare_tests_results_for_clickhouse\n from commit_status_helper import format_description, get_commit, post_commit_status\n-from docker_images_check import DockerImage\n-from env_helper import CI, GITHUB_RUN_URL, RUNNER_TEMP, S3_BUILDS_BUCKET, S3_DOWNLOAD\n-from get_robot_token import get_best_robot_token, get_parameter_from_ssm\n+from docker_images_helper import DockerImageData, docker_login\n+from env_helper import (\n+    GITHUB_RUN_URL,\n+    REPORT_PATH,\n+    TEMP_PATH,\n+    S3_BUILDS_BUCKET,\n+    S3_DOWNLOAD,\n+)\n+from get_robot_token import get_best_robot_token\n from git_helper import Git\n from pr_info import PRInfo\n from report import TestResults, TestResult\n from s3_helper import S3Helper\n from stopwatch import Stopwatch\n from tee_popen import TeePopen\n+from build_download_helper import read_build_urls\n from upload_result_helper import upload_results\n from version_helper import (\n     ClickHouseVersion,\n@@ -33,10 +39,10 @@\n     version_arg,\n )\n \n-TEMP_PATH = p.join(RUNNER_TEMP, \"docker_images_check\")\n-BUCKETS = {\"amd64\": \"package_release\", \"arm64\": \"package_aarch64\"}\n git = Git(ignore_no_tags=True)\n \n+ARCH = (\"amd64\", \"arm64\")\n+\n \n class DelOS(argparse.Action):\n     def __call__(self, _, namespace, __, option_string=None):\n@@ -115,6 +121,11 @@ def parse_args() -> argparse.Namespace:\n         default=argparse.SUPPRESS,\n         help=\"don't build alpine image\",\n     )\n+    parser.add_argument(\n+        \"--allow-build-reuse\",\n+        action=\"store_true\",\n+        help=\"allows binaries built on different branch if source digest matches current repo state\",\n+    )\n \n     return parser.parse_args()\n \n@@ -214,26 +225,29 @@ def gen_tags(version: ClickHouseVersion, release_type: str) -> List[str]:\n     return tags\n \n \n-def buildx_args(bucket_prefix: str, arch: str) -> List[str]:\n+def buildx_args(urls: Dict[str, str], arch: str, direct_urls: List[str]) -> List[str]:\n     args = [\n         f\"--platform=linux/{arch}\",\n         f\"--label=build-url={GITHUB_RUN_URL}\",\n         f\"--label=com.clickhouse.build.githash={git.sha}\",\n     ]\n-    if bucket_prefix:\n-        url = p.join(bucket_prefix, BUCKETS[arch])  # to prevent a double //\n+    if direct_urls:\n+        args.append(f\"--build-arg=DIRECT_DOWNLOAD_URLS='{' '.join(direct_urls)}'\")\n+    elif urls:\n+        url = urls[arch]\n         args.append(f\"--build-arg=REPOSITORY='{url}'\")\n         args.append(f\"--build-arg=deb_location_url='{url}'\")\n     return args\n \n \n def build_and_push_image(\n-    image: DockerImage,\n+    image: DockerImageData,\n     push: bool,\n-    bucket_prefix: str,\n+    repo_urls: dict[str, str],\n     os: str,\n     tag: str,\n     version: ClickHouseVersion,\n+    direct_urls: Dict[str, List[str]],\n ) -> TestResults:\n     result = []  # type: TestResults\n     if os != \"ubuntu\":\n@@ -250,13 +264,19 @@ def build_and_push_image(\n     # images must be built separately and merged together with `docker manifest`\n     digests = []\n     multiplatform_sw = Stopwatch()\n-    for arch in BUCKETS:\n+    for arch in ARCH:\n         single_sw = Stopwatch()\n         arch_tag = f\"{tag}-{arch}\"\n         metadata_path = p.join(TEMP_PATH, arch_tag)\n-        dockerfile = p.join(image.full_path, f\"Dockerfile.{os}\")\n+        dockerfile = p.join(image.path, f\"Dockerfile.{os}\")\n         cmd_args = list(init_args)\n-        cmd_args.extend(buildx_args(bucket_prefix, arch))\n+        urls = []\n+        if direct_urls:\n+            if os == \"ubuntu\" and \"clickhouse-server\" in image.repo:\n+                urls = [url for url in direct_urls[arch] if \".deb\" in url]\n+            else:\n+                urls = [url for url in direct_urls[arch] if \".tgz\" in url]\n+        cmd_args.extend(buildx_args(repo_urls, arch, direct_urls=urls))\n         if not push:\n             cmd_args.append(f\"--tag={image.repo}:{arch_tag}\")\n         cmd_args.extend(\n@@ -265,7 +285,7 @@ def build_and_push_image(\n                 f\"--build-arg=VERSION='{version.string}'\",\n                 \"--progress=plain\",\n                 f\"--file={dockerfile}\",\n-                image.full_path.as_posix(),\n+                image.path.as_posix(),\n             ]\n         )\n         cmd = \" \".join(cmd_args)\n@@ -323,25 +343,47 @@ def main():\n     makedirs(TEMP_PATH, exist_ok=True)\n \n     args = parse_args()\n-    image = DockerImage(args.image_path, args.image_repo, False)\n+    image = DockerImageData(args.image_path, args.image_repo, False)\n     args.release_type = auto_release_type(args.version, args.release_type)\n     tags = gen_tags(args.version, args.release_type)\n     NAME = f\"Docker image {image.repo} building check\"\n     pr_info = None\n-    if CI:\n-        pr_info = PRInfo()\n-        release_or_pr, _ = get_release_or_pr(pr_info, args.version)\n-        args.bucket_prefix = (\n-            f\"{S3_DOWNLOAD}/{S3_BUILDS_BUCKET}/{release_or_pr}/{pr_info.sha}\"\n-        )\n+    repo_urls = dict()\n+    direct_urls: Dict[str, List[str]] = dict()\n+    pr_info = PRInfo()\n+    release_or_pr, _ = get_release_or_pr(pr_info, args.version)\n+\n+    for arch, build_name in zip(ARCH, (\"package_release\", \"package_aarch64\")):\n+        if not args.bucket_prefix:\n+            repo_urls[\n+                arch\n+            ] = f\"{S3_DOWNLOAD}/{S3_BUILDS_BUCKET}/{release_or_pr}/{pr_info.sha}/{build_name}\"\n+        else:\n+            repo_urls[arch] = f\"{args.bucket_prefix}/{build_name}\"\n+        if args.allow_build_reuse:\n+            # read s3 urls from pre-downloaded build reports\n+            if \"clickhouse-server\" in args.image_repo:\n+                PACKAGES = [\n+                    \"clickhouse-client\",\n+                    \"clickhouse-server\",\n+                    \"clickhouse-common-static\",\n+                ]\n+            elif \"clickhouse-keeper\" in args.image_repo:\n+                PACKAGES = [\"clickhouse-keeper\"]\n+            else:\n+                assert False, \"BUG\"\n+            urls = read_build_urls(build_name, Path(REPORT_PATH))\n+            assert (\n+                urls\n+            ), f\"URLS has not been read from build report, report path[{REPORT_PATH}], build [{build_name}]\"\n+            direct_urls[arch] = [\n+                url\n+                for url in urls\n+                if any(package in url for package in PACKAGES) and \"-dbg\" not in url\n+            ]\n \n     if args.push:\n-        subprocess.check_output(  # pylint: disable=unexpected-keyword-arg\n-            \"docker login --username 'robotclickhouse' --password-stdin\",\n-            input=get_parameter_from_ssm(\"dockerhub_robot_password\"),\n-            encoding=\"utf-8\",\n-            shell=True,\n-        )\n+        docker_login()\n         NAME = f\"Docker image {image.repo} build and push\"\n \n     logging.info(\"Following tags will be created: %s\", \", \".join(tags))\n@@ -351,7 +393,7 @@ def main():\n         for tag in tags:\n             test_results.extend(\n                 build_and_push_image(\n-                    image, args.push, args.bucket_prefix, os, tag, args.version\n+                    image, args.push, repo_urls, os, tag, args.version, direct_urls\n                 )\n             )\n             if test_results[-1].status != \"OK\":\n@@ -373,7 +415,9 @@ def main():\n \n     gh = Github(get_best_robot_token(), per_page=100)\n     commit = get_commit(gh, pr_info.sha)\n-    post_commit_status(commit, status, url, description, NAME, pr_info)\n+    post_commit_status(\n+        commit, status, url, description, NAME, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/docs_check.py b/tests/ci/docs_check.py\nindex 650ed93aa710..6f68918e63cd 100644\n--- a/tests/ci/docs_check.py\n+++ b/tests/ci/docs_check.py\n@@ -10,14 +10,13 @@\n \n from clickhouse_helper import ClickHouseHelper, prepare_tests_results_for_clickhouse\n from commit_status_helper import (\n-    NotSet,\n     RerunHelper,\n     get_commit,\n     post_commit_status,\n     update_mergeable_check,\n )\n-from docker_pull_helper import get_image_with_version\n-from env_helper import TEMP_PATH, REPO_COPY, REPORTS_PATH\n+from docker_images_helper import get_docker_image, pull_image\n+from env_helper import TEMP_PATH, REPO_COPY\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import TestResults, TestResult\n@@ -57,8 +56,6 @@ def main():\n \n     temp_path = Path(TEMP_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n-    reports_path = Path(REPORTS_PATH)\n-    reports_path.mkdir(parents=True, exist_ok=True)\n     repo_path = Path(REPO_COPY)\n \n     pr_info = PRInfo(need_changed_files=True)\n@@ -75,7 +72,13 @@ def main():\n     if not pr_info.has_changes_in_documentation() and not args.force:\n         logging.info(\"No changes in documentation\")\n         post_commit_status(\n-            commit, \"success\", NotSet, \"No changes in docs\", NAME, pr_info\n+            commit,\n+            \"success\",\n+            \"\",\n+            \"No changes in docs\",\n+            NAME,\n+            pr_info,\n+            dump_to_file=True,\n         )\n         sys.exit(0)\n \n@@ -84,7 +87,7 @@ def main():\n     elif args.force:\n         logging.info(\"Check the docs because of force flag\")\n \n-    docker_image = get_image_with_version(reports_path, \"clickhouse/docs-builder\")\n+    docker_image = pull_image(get_docker_image(\"clickhouse/docs-builder\"))\n \n     test_output = temp_path / \"docs_check_log\"\n     test_output.mkdir(parents=True, exist_ok=True)\n@@ -138,7 +141,9 @@ def main():\n         s3_helper, pr_info.number, pr_info.sha, test_results, additional_files, NAME\n     )\n     print(\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, status, report_url, description, NAME, pr_info)\n+    post_commit_status(\n+        commit, status, report_url, description, NAME, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/env_helper.py b/tests/ci/env_helper.py\nindex 6364ea0ff7c5..fa09d0731779 100644\n--- a/tests/ci/env_helper.py\n+++ b/tests/ci/env_helper.py\n@@ -9,10 +9,12 @@\n \n module_dir = p.abspath(p.dirname(__file__))\n git_root = p.abspath(p.join(module_dir, \"..\", \"..\"))\n-\n+ROOT_DIR = git_root\n CI = bool(os.getenv(\"CI\"))\n TEMP_PATH = os.getenv(\"TEMP_PATH\", p.abspath(p.join(module_dir, \"./tmp\")))\n-\n+REPORT_PATH = f\"{TEMP_PATH}/reports\"\n+# FIXME: latest should not be used in CI, set temporary for transition to \"docker with digest as a tag\"\n+DOCKER_TAG = os.getenv(\"DOCKER_TAG\", \"latest\")\n CACHES_PATH = os.getenv(\"CACHES_PATH\", TEMP_PATH)\n CLOUDFLARE_TOKEN = os.getenv(\"CLOUDFLARE_TOKEN\")\n GITHUB_EVENT_PATH = os.getenv(\"GITHUB_EVENT_PATH\", \"\")\n@@ -23,7 +25,6 @@\n GITHUB_WORKSPACE = os.getenv(\"GITHUB_WORKSPACE\", git_root)\n GITHUB_RUN_URL = f\"{GITHUB_SERVER_URL}/{GITHUB_REPOSITORY}/actions/runs/{GITHUB_RUN_ID}\"\n IMAGES_PATH = os.getenv(\"IMAGES_PATH\", TEMP_PATH)\n-REPORTS_PATH = os.getenv(\"REPORTS_PATH\", p.abspath(p.join(module_dir, \"./reports\")))\n REPO_COPY = os.getenv(\"REPO_COPY\", GITHUB_WORKSPACE)\n RUNNER_TEMP = os.getenv(\"RUNNER_TEMP\", p.abspath(p.join(module_dir, \"./tmp\")))\n S3_BUILDS_BUCKET = os.getenv(\"S3_BUILDS_BUCKET\", \"clickhouse-builds\")\ndiff --git a/tests/ci/fast_test_check.py b/tests/ci/fast_test_check.py\nindex 265fc81ccb3d..1ce6ab617ec1 100644\n--- a/tests/ci/fast_test_check.py\n+++ b/tests/ci/fast_test_check.py\n@@ -7,8 +7,7 @@\n import sys\n import atexit\n from pathlib import Path\n-from typing import List, Tuple\n-\n+from typing import Tuple\n from github import Github\n \n from build_check import get_release_or_pr\n@@ -23,8 +22,9 @@\n     update_mergeable_check,\n     format_description,\n )\n-from docker_pull_helper import get_image_with_version, DockerImage\n-from env_helper import S3_BUILDS_BUCKET, TEMP_PATH, REPO_COPY, REPORTS_PATH\n+\n+from docker_images_helper import DockerImage, get_docker_image, pull_image\n+from env_helper import S3_BUILDS_BUCKET, TEMP_PATH, REPO_COPY\n from get_robot_token import get_best_robot_token\n from pr_info import FORCE_TESTS_LABEL, PRInfo\n from report import TestResult, TestResults, read_test_results\n@@ -118,8 +118,6 @@ def main():\n \n     temp_path = Path(TEMP_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n-    reports_path = Path(REPORTS_PATH)\n-    reports_path.mkdir(parents=True, exist_ok=True)\n \n     pr_info = PRInfo()\n \n@@ -136,7 +134,7 @@ def main():\n             sys.exit(1)\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(reports_path, \"clickhouse/fasttest\")\n+    docker_image = pull_image(get_docker_image(\"clickhouse/fasttest\"))\n \n     s3_helper = S3Helper()\n \n@@ -233,7 +231,9 @@ def main():\n         build_urls,\n     )\n     print(f\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, state, report_url, description, NAME, pr_info)\n+    post_commit_status(\n+        commit, state, report_url, description, NAME, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/finish_check.py b/tests/ci/finish_check.py\nindex 74392947b82a..92d2fddef0fa 100644\n--- a/tests/ci/finish_check.py\n+++ b/tests/ci/finish_check.py\n@@ -4,7 +4,6 @@\n \n from commit_status_helper import (\n     CI_STATUS_NAME,\n-    NotSet,\n     get_commit,\n     get_commit_filtered_statuses,\n     post_commit_status,\n@@ -36,10 +35,11 @@ def main():\n         post_commit_status(\n             commit,\n             \"success\",\n-            status.target_url or NotSet,\n+            status.target_url,\n             \"All checks finished\",\n             CI_STATUS_NAME,\n             pr_info,\n+            dump_to_file=True,\n         )\n \n \ndiff --git a/tests/ci/functional_test_check.py b/tests/ci/functional_test_check.py\nindex 0dea2c5476fd..c7ae91d88b21 100644\n--- a/tests/ci/functional_test_check.py\n+++ b/tests/ci/functional_test_check.py\n@@ -20,7 +20,6 @@\n     prepare_tests_results_for_clickhouse,\n )\n from commit_status_helper import (\n-    NotSet,\n     RerunHelper,\n     get_commit,\n     override_status,\n@@ -28,9 +27,9 @@\n     post_commit_status_to_file,\n     update_mergeable_check,\n )\n-from docker_pull_helper import DockerImage, get_image_with_version\n+from docker_images_helper import DockerImage, pull_image, get_docker_image\n from download_release_packages import download_last_release\n-from env_helper import TEMP_PATH, REPO_COPY, REPORTS_PATH\n+from env_helper import REPORT_PATH, TEMP_PATH, REPO_COPY\n from get_robot_token import get_best_robot_token\n from pr_info import FORCE_TESTS_LABEL, PRInfo\n from report import TestResults, read_test_results\n@@ -225,16 +224,24 @@ def main():\n     stopwatch = Stopwatch()\n \n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n+    reports_path.mkdir(parents=True, exist_ok=True)\n \n     repo_path = Path(REPO_COPY)\n-    reports_path = Path(REPORTS_PATH)\n     post_commit_path = temp_path / \"functional_commit_status.tsv\"\n \n     args = parse_args()\n-    check_name = args.check_name\n-    kill_timeout = args.kill_timeout\n+    check_name = args.check_name or os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n+    kill_timeout = args.kill_timeout or int(os.getenv(\"KILL_TIMEOUT\", \"0\"))\n+    assert (\n+        kill_timeout > 0\n+    ), \"kill timeout must be provided as an input arg or in KILL_TIMEOUT env\"\n     validate_bugfix_check = args.validate_bugfix\n+    print(f\"Runnin check [{check_name}] with timeout [{kill_timeout}]\")\n \n     flaky_check = \"flaky\" in check_name.lower()\n \n@@ -285,10 +292,11 @@ def main():\n                 post_commit_status(\n                     commit,\n                     state,\n-                    NotSet,\n+                    \"\",\n                     NO_CHANGES_MSG,\n                     check_name_with_group,\n                     pr_info,\n+                    dump_to_file=True,\n                 )\n             elif args.post_commit_status == \"file\":\n                 post_commit_status_to_file(\n@@ -300,7 +308,8 @@ def main():\n             sys.exit(0)\n \n     image_name = get_image_name(check_name)\n-    docker_image = get_image_with_version(reports_path, image_name)\n+\n+    docker_image = pull_image(get_docker_image(image_name))\n \n     packages_path = temp_path / \"packages\"\n     packages_path.mkdir(parents=True, exist_ok=True)\n@@ -379,7 +388,13 @@ def main():\n     print(f\"::notice:: {check_name} Report url: {report_url}\")\n     if args.post_commit_status == \"commit_status\":\n         post_commit_status(\n-            commit, state, report_url, description, check_name_with_group, pr_info\n+            commit,\n+            state,\n+            report_url,\n+            description,\n+            check_name_with_group,\n+            pr_info,\n+            dump_to_file=True,\n         )\n     elif args.post_commit_status == \"file\":\n         post_commit_status_to_file(\ndiff --git a/tests/ci/git_helper.py b/tests/ci/git_helper.py\nindex 9927d5a42489..598ffbafb517 100644\n--- a/tests/ci/git_helper.py\n+++ b/tests/ci/git_helper.py\n@@ -19,6 +19,13 @@\n CWD = p.dirname(p.realpath(__file__))\n TWEAK = 1\n \n+GIT_PREFIX = (  # All commits to remote are done as robot-clickhouse\n+    \"git -c user.email=robot-clickhouse@users.noreply.github.com \"\n+    \"-c user.name=robot-clickhouse -c commit.gpgsign=false \"\n+    \"-c core.sshCommand=\"\n+    \"'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'\"\n+)\n+\n \n # Py 3.8 removeprefix and removesuffix\n def removeprefix(string: str, prefix: str) -> str:\ndiff --git a/tests/ci/install_check.py b/tests/ci/install_check.py\nindex 9971d0c236c6..b8cfa0acd3d8 100644\n--- a/tests/ci/install_check.py\n+++ b/tests/ci/install_check.py\n@@ -25,8 +25,8 @@\n     update_mergeable_check,\n )\n from compress_files import compress_fast\n-from docker_pull_helper import get_image_with_version, DockerImage\n-from env_helper import CI, TEMP_PATH as TEMP, REPORTS_PATH\n+from docker_images_helper import DockerImage, pull_image, get_docker_image\n+from env_helper import CI, REPORT_PATH, TEMP_PATH as TEMP\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import TestResults, TestResult, FAILURE, FAIL, OK, SUCCESS\n@@ -151,7 +151,7 @@ def test_install_tgz(image: DockerImage) -> TestResults:\n     # FIXME: I couldn't find why Type=notify is broken in centos:8\n     # systemd just ignores the watchdog completely\n     tests = {\n-        f\"Install server tgz in {image.name}\": r\"\"\"#!/bin/bash -ex\n+        f\"Install server tgz in {image}\": r\"\"\"#!/bin/bash -ex\n [ -f /etc/debian_version ] && CONFIGURE=configure || CONFIGURE=\n for pkg in /packages/clickhouse-{common,client,server}*tgz; do\n     package=${pkg%-*}\n@@ -161,7 +161,7 @@ def test_install_tgz(image: DockerImage) -> TestResults:\n done\n [ -f /etc/yum.conf ] && echo CLICKHOUSE_WATCHDOG_ENABLE=0 > /etc/default/clickhouse-server\n bash -ex /packages/server_test.sh\"\"\",\n-        f\"Install keeper tgz in {image.name}\": r\"\"\"#!/bin/bash -ex\n+        f\"Install keeper tgz in {image}\": r\"\"\"#!/bin/bash -ex\n [ -f /etc/debian_version ] && CONFIGURE=configure || CONFIGURE=\n for pkg in /packages/clickhouse-keeper*tgz; do\n     package=${pkg%-*}\n@@ -224,7 +224,6 @@ def parse_args() -> argparse.Namespace:\n         formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n         description=\"The script to check if the packages are able to install\",\n     )\n-\n     parser.add_argument(\n         \"check_name\",\n         help=\"check name, used to download the packages\",\n@@ -289,10 +288,9 @@ def main():\n             )\n             sys.exit(0)\n \n-    docker_images = {\n-        name: get_image_with_version(REPORTS_PATH, name, args.download)\n-        for name in (RPM_IMAGE, DEB_IMAGE)\n-    }\n+    deb_image = pull_image(get_docker_image(DEB_IMAGE))\n+    rpm_image = pull_image(get_docker_image(RPM_IMAGE))\n+\n     prepare_test_scripts()\n \n     if args.download:\n@@ -312,7 +310,7 @@ def filter_artifacts(path: str) -> bool:\n             return is_match\n \n         download_builds_filter(\n-            args.check_name, REPORTS_PATH, TEMP_PATH, filter_artifacts\n+            args.check_name, REPORT_PATH, TEMP_PATH, filter_artifacts\n         )\n \n     test_results = []  # type: TestResults\n@@ -325,12 +323,12 @@ def filter_artifacts(path: str) -> bool:\n         subprocess.check_output(f\"{ch_copy.absolute()} local -q 'SELECT 1'\", shell=True)\n \n     if args.deb:\n-        test_results.extend(test_install_deb(docker_images[DEB_IMAGE]))\n+        test_results.extend(test_install_deb(deb_image))\n     if args.rpm:\n-        test_results.extend(test_install_rpm(docker_images[RPM_IMAGE]))\n+        test_results.extend(test_install_rpm(rpm_image))\n     if args.tgz:\n-        test_results.extend(test_install_tgz(docker_images[DEB_IMAGE]))\n-        test_results.extend(test_install_tgz(docker_images[RPM_IMAGE]))\n+        test_results.extend(test_install_tgz(deb_image))\n+        test_results.extend(test_install_tgz(rpm_image))\n \n     state = SUCCESS\n     test_status = OK\n@@ -360,7 +358,15 @@ def filter_artifacts(path: str) -> bool:\n \n     description = format_description(description)\n \n-    post_commit_status(commit, state, report_url, description, args.check_name, pr_info)\n+    post_commit_status(\n+        commit,\n+        state,\n+        report_url,\n+        description,\n+        args.check_name,\n+        pr_info,\n+        dump_to_file=True,\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/integration_test_check.py b/tests/ci/integration_test_check.py\nindex e49cec6d694f..0a8f166e53e6 100644\n--- a/tests/ci/integration_test_check.py\n+++ b/tests/ci/integration_test_check.py\n@@ -24,9 +24,9 @@\n     post_commit_status,\n     post_commit_status_to_file,\n )\n-from docker_pull_helper import get_images_with_versions, DockerImage\n+from docker_images_helper import DockerImage, pull_image, get_docker_image\n from download_release_packages import download_last_release\n-from env_helper import TEMP_PATH, REPO_COPY, REPORTS_PATH\n+from env_helper import REPORT_PATH, TEMP_PATH, REPO_COPY\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import ERROR, TestResult, TestResults, read_test_results\n@@ -166,14 +166,17 @@ def main():\n     stopwatch = Stopwatch()\n \n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n \n     post_commit_path = temp_path / \"integration_commit_status.tsv\"\n     repo_path = Path(REPO_COPY)\n-    reports_path = Path(REPORTS_PATH)\n \n     args = parse_args()\n-    check_name = args.check_name\n+    check_name = args.check_name or os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided in --check-name input option or in CHECK_NAME env\"\n     validate_bugfix_check = args.validate_bugfix\n \n     if \"RUN_BY_HASH_NUM\" in os.environ:\n@@ -215,7 +218,7 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    images = get_images_with_versions(reports_path, IMAGES)\n+    images = [pull_image(get_docker_image(i)) for i in IMAGES]\n     result_path = temp_path / \"output_dir\"\n     result_path.mkdir(parents=True, exist_ok=True)\n \n@@ -310,7 +313,13 @@ def main():\n     print(f\"::notice:: {check_name} Report url: {report_url}\")\n     if args.post_commit_status == \"commit_status\":\n         post_commit_status(\n-            commit, state, report_url, description, check_name_with_group, pr_info\n+            commit,\n+            state,\n+            report_url,\n+            description,\n+            check_name_with_group,\n+            pr_info,\n+            dump_to_file=True,\n         )\n     elif args.post_commit_status == \"file\":\n         post_commit_status_to_file(post_commit_path, description, state, report_url)\ndiff --git a/tests/ci/jepsen_check.py b/tests/ci/jepsen_check.py\nindex 94ec8f937900..73ae231e7b73 100644\n--- a/tests/ci/jepsen_check.py\n+++ b/tests/ci/jepsen_check.py\n@@ -292,7 +292,9 @@ def main():\n     )\n \n     print(f\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, status, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, status, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n \n     ch_helper = ClickHouseHelper()\n     prepared_events = prepare_tests_results_for_clickhouse(\ndiff --git a/tests/ci/libfuzzer_test_check.py b/tests/ci/libfuzzer_test_check.py\nindex e768b7f1b4e7..58e78d571c53 100644\n--- a/tests/ci/libfuzzer_test_check.py\n+++ b/tests/ci/libfuzzer_test_check.py\n@@ -20,9 +20,9 @@\n     get_commit,\n     update_mergeable_check,\n )\n-from docker_pull_helper import DockerImage, get_image_with_version\n+from docker_images_helper import DockerImage, pull_image, get_docker_image\n \n-from env_helper import TEMP_PATH, REPO_COPY, REPORTS_PATH\n+from env_helper import REPORT_PATH, TEMP_PATH, REPO_COPY\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import TestResults\n@@ -107,8 +107,9 @@ def main():\n     stopwatch = Stopwatch()\n \n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n+    temp_path.mkdir(parents=True, exist_ok=True)\n     repo_path = Path(REPO_COPY)\n-    reports_path = REPORTS_PATH\n \n     args = parse_args()\n     check_name = args.check_name\n@@ -137,7 +138,7 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(reports_path, \"clickhouse/libfuzzer\")\n+    docker_image = pull_image(get_docker_image(\"clickhouse/libfuzzer\"))\n \n     fuzzers_path = temp_path / \"fuzzers\"\n     fuzzers_path.mkdir(parents=True, exist_ok=True)\ndiff --git a/tests/ci/mark_release_ready.py b/tests/ci/mark_release_ready.py\nindex 4501d40e4d32..0ad4b2bd2ed5 100755\n--- a/tests/ci/mark_release_ready.py\n+++ b/tests/ci/mark_release_ready.py\n@@ -4,7 +4,7 @@\n import logging\n import os\n \n-from commit_status_helper import NotSet, get_commit, post_commit_status\n+from commit_status_helper import get_commit, post_commit_status\n from env_helper import GITHUB_JOB_URL\n from get_robot_token import get_best_robot_token\n from github_helper import GitHub\n@@ -49,7 +49,13 @@ def main():\n     commit = get_commit(gh, args.commit)\n     gh.get_rate_limit()\n     post_commit_status(\n-        commit, \"success\", url or NotSet, description, RELEASE_READY_STATUS, pr_info\n+        commit,\n+        \"success\",\n+        url,\n+        description,\n+        RELEASE_READY_STATUS,\n+        pr_info,\n+        dump_to_file=True,\n     )\n \n \ndiff --git a/tests/ci/performance_comparison_check.py b/tests/ci/performance_comparison_check.py\nindex 81096d5d6029..75f40ae7febe 100644\n--- a/tests/ci/performance_comparison_check.py\n+++ b/tests/ci/performance_comparison_check.py\n@@ -14,15 +14,15 @@\n \n from commit_status_helper import RerunHelper, get_commit, post_commit_status\n from ci_config import CI_CONFIG\n-from docker_pull_helper import get_image_with_version\n+from docker_images_helper import pull_image, get_docker_image\n from env_helper import (\n     GITHUB_EVENT_PATH,\n     GITHUB_RUN_URL,\n     REPO_COPY,\n-    REPORTS_PATH,\n     S3_BUILDS_BUCKET,\n     S3_DOWNLOAD,\n     TEMP_PATH,\n+    REPORT_PATH,\n )\n from get_robot_token import get_best_robot_token, get_parameter_from_ssm\n from pr_info import PRInfo\n@@ -30,6 +30,7 @@\n from tee_popen import TeePopen\n from clickhouse_helper import get_instance_type, get_instance_id\n from stopwatch import Stopwatch\n+from build_download_helper import download_builds_filter\n \n IMAGE_NAME = \"clickhouse/performance-comparison\"\n \n@@ -63,6 +64,7 @@ def get_run_command(\n         f\"docker run --privileged --volume={workspace}:/workspace \"\n         f\"--volume={result_path}:/output \"\n         f\"--volume={repo_tests_path}:/usr/share/clickhouse-test \"\n+        f\"--volume={TEMP_PATH}:/artifacts \"\n         f\"--cap-add syslog --cap-add sys_admin --cap-add sys_rawio \"\n         f\"{env_str} {additional_env} \"\n         f\"{image}\"\n@@ -77,9 +79,11 @@ def main():\n     temp_path = Path(TEMP_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n     repo_tests_path = Path(REPO_COPY, \"tests\")\n-    reports_path = Path(REPORTS_PATH)\n \n-    check_name = sys.argv[1]\n+    check_name = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n     required_build = CI_CONFIG.test_configs[check_name].required_build\n \n     with open(GITHUB_EVENT_PATH, \"r\", encoding=\"utf-8\") as event_file:\n@@ -123,7 +127,13 @@ def main():\n         message = \"Skipped, not labeled with 'pr-performance'\"\n         report_url = GITHUB_RUN_URL\n         post_commit_status(\n-            commit, status, report_url, message, check_name_with_group, pr_info\n+            commit,\n+            status,\n+            report_url,\n+            message,\n+            check_name_with_group,\n+            pr_info,\n+            dump_to_file=True,\n         )\n         sys.exit(0)\n \n@@ -141,7 +151,7 @@ def main():\n         .replace(\"/\", \"_\")\n     )\n \n-    docker_image = get_image_with_version(reports_path, IMAGE_NAME)\n+    docker_image = pull_image(get_docker_image(IMAGE_NAME))\n \n     result_path = temp_path / \"result\"\n     result_path.mkdir(parents=True, exist_ok=True)\n@@ -158,6 +168,11 @@ def main():\n         \"CLICKHOUSE_PERFORMANCE_COMPARISON_CHECK_NAME_PREFIX\": check_name_prefix,\n     }\n \n+    download_builds_filter(\n+        check_name, REPORT_PATH, TEMP_PATH, lambda url: \"performance.tar.zst\" in url\n+    )\n+    assert os.path.exists(f\"{TEMP_PATH}/performance.tar.zst\"), \"Perf artifact not found\"\n+\n     docker_env += \"\".join([f\" -e {name}\" for name in env_extra])\n \n     run_command = get_run_command(\n@@ -264,7 +279,13 @@ def too_many_slow(msg):\n     )\n \n     post_commit_status(\n-        commit, status, report_url, message, check_name_with_group, pr_info\n+        commit,\n+        status,\n+        report_url,\n+        message,\n+        check_name_with_group,\n+        pr_info,\n+        dump_to_file=True,\n     )\n \n     if status == \"error\":\ndiff --git a/tests/ci/report.py b/tests/ci/report.py\nindex ba2377faa363..bf400aebd4b7 100644\n--- a/tests/ci/report.py\n+++ b/tests/ci/report.py\n@@ -415,10 +415,13 @@ def job_name(self, job_name: str) -> None:\n     def _set_properties(self) -> None:\n         if all(p is not None for p in (self._job_name, self._job_html_url)):\n             return\n-        try:\n-            job_data = get_gh_api(self.job_api_url).json()\n-        except Exception:\n-            job_data = {}\n+        job_data = {}\n+        # quick check @self.job_api_url is valid url before request. it's set to \"missing\" for dummy BuildResult\n+        if \"http\" in self.job_api_url:\n+            try:\n+                job_data = get_gh_api(self.job_api_url).json()\n+            except Exception:\n+                pass\n         # job_name can be set manually\n         self._job_name = self._job_name or job_data.get(\"name\", \"unknown\")\n         self._job_html_url = job_data.get(\"html_url\", \"\")\ndiff --git a/tests/ci/run_check.py b/tests/ci/run_check.py\nindex 231e2617a3f1..d1949a4da0b7 100644\n--- a/tests/ci/run_check.py\n+++ b/tests/ci/run_check.py\n@@ -7,7 +7,6 @@\n \n from commit_status_helper import (\n     CI_STATUS_NAME,\n-    NotSet,\n     create_ci_report,\n     format_description,\n     get_commit,\n@@ -137,6 +136,7 @@ def main():\n     if pr_labels_to_remove:\n         remove_labels(gh, pr_info, pr_labels_to_remove)\n \n+    # FIXME: it should rather be in finish check. no reason to stop ci run.\n     if FEATURE_LABEL in pr_info.labels and not pr_info.has_changes_in_documentation():\n         print(\n             f\"The '{FEATURE_LABEL}' in the labels, \"\n@@ -145,7 +145,7 @@ def main():\n         post_commit_status(  # do not pass pr_info here intentionally\n             commit,\n             \"failure\",\n-            NotSet,\n+            \"\",\n             f\"expect adding docs for {FEATURE_LABEL}\",\n             DOCS_NAME,\n             pr_info,\n@@ -181,13 +181,23 @@ def main():\n     if not can_run:\n         print(\"::notice ::Cannot run\")\n         post_commit_status(\n-            commit, labels_state, ci_report_url, description, CI_STATUS_NAME, pr_info\n+            commit,\n+            labels_state,\n+            ci_report_url,\n+            description,\n+            CI_STATUS_NAME,\n+            pr_info,\n         )\n         sys.exit(1)\n     else:\n         print(\"::notice ::Can run\")\n         post_commit_status(\n-            commit, \"pending\", ci_report_url, description, CI_STATUS_NAME, pr_info\n+            commit,\n+            \"pending\",\n+            ci_report_url,\n+            description,\n+            CI_STATUS_NAME,\n+            pr_info,\n         )\n \n \ndiff --git a/tests/ci/s3_helper.py b/tests/ci/s3_helper.py\nindex f94f7f60bb66..bc403aa50154 100644\n--- a/tests/ci/s3_helper.py\n+++ b/tests/ci/s3_helper.py\n@@ -117,6 +117,40 @@ def upload_build_file_to_s3(self, file_path: Path, s3_path: str) -> str:\n \n         return S3Helper.copy_file_to_local(S3_BUILDS_BUCKET, file_path, s3_path)\n \n+    def upload_file(\n+        self, bucket: str, file_path: Union[Path, str], s3_path: Union[Path, str]\n+    ) -> str:\n+        return self._upload_file_to_s3(bucket, Path(file_path), str(s3_path))\n+\n+    def download_file(\n+        self, bucket: str, s3_path: str, local_file_path: Union[Path, str]\n+    ) -> None:\n+        if Path(local_file_path).is_dir():\n+            local_file_path = Path(local_file_path) / s3_path.split(\"/\")[-1]\n+        try:\n+            self.client.download_file(bucket, s3_path, local_file_path)\n+        except botocore.exceptions.ClientError as e:\n+            if e.response and e.response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 404:\n+                assert False, f\"No such object [s3://{S3_BUILDS_BUCKET}/{s3_path}]\"\n+\n+    def download_files(\n+        self,\n+        bucket: str,\n+        s3_path: str,\n+        file_suffix: str,\n+        local_directory: Union[Path, str],\n+    ) -> List[str]:\n+        local_directory = Path(local_directory)\n+        local_directory.mkdir(parents=True, exist_ok=True)\n+        objects = self.list_prefix_non_recursive(s3_path)\n+        res = []\n+        for obj in objects:\n+            if obj.endswith(file_suffix):\n+                local_file_path = local_directory\n+                self.download_file(bucket, obj, local_file_path)\n+                res.append(obj.split(\"/\")[-1])\n+        return res\n+\n     def fast_parallel_upload_dir(\n         self, dir_path: Path, s3_dir_path: str, bucket_name: str\n     ) -> List[str]:\n@@ -278,6 +312,18 @@ def list_prefix(\n \n         return result\n \n+    def list_prefix_non_recursive(\n+        self, s3_prefix_path: str, bucket: str = S3_BUILDS_BUCKET\n+    ) -> List[str]:\n+        objects = self.client.list_objects_v2(Bucket=bucket, Prefix=s3_prefix_path)\n+        result = []\n+        if \"Contents\" in objects:\n+            for obj in objects[\"Contents\"]:\n+                if \"/\" not in obj[\"Key\"][len(s3_prefix_path) + 1 :]:\n+                    result.append(obj[\"Key\"])\n+\n+        return result\n+\n     def url_if_exists(self, key: str, bucket: str = S3_BUILDS_BUCKET) -> str:\n         if not CI:\n             local_path = self.local_path(bucket, key)\ndiff --git a/tests/ci/sqlancer_check.py b/tests/ci/sqlancer_check.py\nindex 47bc3b2c1e8c..35641ba04556 100644\n--- a/tests/ci/sqlancer_check.py\n+++ b/tests/ci/sqlancer_check.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n \n import logging\n+import os\n import subprocess\n import sys\n from pathlib import Path\n@@ -15,10 +16,10 @@\n     get_commit,\n     post_commit_status,\n )\n-from docker_pull_helper import get_image_with_version, DockerImage\n+from docker_images_helper import DockerImage, pull_image, get_docker_image\n from env_helper import (\n     GITHUB_RUN_URL,\n-    REPORTS_PATH,\n+    REPORT_PATH,\n     TEMP_PATH,\n )\n from get_robot_token import get_best_robot_token\n@@ -50,10 +51,12 @@ def main():\n \n     temp_path = Path(TEMP_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n+    reports_path = Path(REPORT_PATH)\n \n-    reports_path = Path(REPORTS_PATH)\n-\n-    check_name = sys.argv[1]\n+    check_name = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n \n     pr_info = PRInfo()\n \n@@ -65,7 +68,7 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(reports_path, IMAGE_NAME)\n+    docker_image = pull_image(get_docker_image(IMAGE_NAME))\n \n     build_name = get_build_name_for_check(check_name)\n     urls = read_build_urls(build_name, reports_path)\n@@ -147,7 +150,9 @@ def main():\n         check_name,\n     )\n \n-    post_commit_status(commit, status, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, status, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n     print(f\"::notice:: {check_name} Report url: {report_url}\")\n \n     ch_helper = ClickHouseHelper()\ndiff --git a/tests/ci/sqllogic_test.py b/tests/ci/sqllogic_test.py\nindex 7650a4afa40a..96b4de4517fa 100755\n--- a/tests/ci/sqllogic_test.py\n+++ b/tests/ci/sqllogic_test.py\n@@ -18,8 +18,8 @@\n     override_status,\n     post_commit_status,\n )\n-from docker_pull_helper import get_image_with_version, DockerImage\n-from env_helper import TEMP_PATH, REPO_COPY, REPORTS_PATH\n+from docker_images_helper import DockerImage, pull_image, get_docker_image\n+from env_helper import REPORT_PATH, TEMP_PATH, REPO_COPY\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import OK, FAIL, ERROR, SUCCESS, TestResults, TestResult, read_test_results\n@@ -70,8 +70,16 @@ def read_check_status(result_folder: Path) -> Tuple[str, str]:\n \n def parse_args() -> argparse.Namespace:\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\"check_name\")\n-    parser.add_argument(\"kill_timeout\", type=int)\n+    parser.add_argument(\n+        \"--check-name\",\n+        required=False,\n+        default=\"\",\n+    )\n+    parser.add_argument(\n+        \"--kill-timeout\",\n+        required=False,\n+        default=0,\n+    )\n     return parser.parse_args()\n \n \n@@ -81,12 +89,20 @@ def main():\n     stopwatch = Stopwatch()\n \n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n     repo_path = Path(REPO_COPY)\n-    reports_path = Path(REPORTS_PATH)\n \n     args = parse_args()\n     check_name = args.check_name\n+    check_name = args.check_name or os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n+    kill_timeout = args.kill_timeout or int(os.getenv(\"KILL_TIMEOUT\", \"0\"))\n+    assert (\n+        kill_timeout > 0\n+    ), \"kill timeout must be provided as an input arg or in KILL_TIMEOUT env\"\n \n     pr_info = PRInfo()\n     gh = Github(get_best_robot_token(), per_page=100)\n@@ -97,7 +113,7 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(reports_path, IMAGE_NAME)\n+    docker_image = pull_image(get_docker_image(IMAGE_NAME))\n \n     repo_tests_path = repo_path / \"tests\"\n \n@@ -123,7 +139,7 @@ def main():\n     )\n     logging.info(\"Going to run func tests: %s\", run_command)\n \n-    with TeePopen(run_command, run_log_path, timeout=args.kill_timeout) as process:\n+    with TeePopen(run_command, run_log_path, timeout=kill_timeout) as process:\n         retcode = process.wait()\n         if retcode == 0:\n             logging.info(\"Run successfully\")\n@@ -190,7 +206,9 @@ def main():\n     assert description is not None\n     # FIXME: force SUCCESS until all cases are fixed\n     status = SUCCESS\n-    post_commit_status(commit, status, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, status, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n \n \n if __name__ == \"__main__\":\ndiff --git a/tests/ci/sqltest.py b/tests/ci/sqltest.py\nindex a4eb1b23349d..edb64d9f1066 100644\n--- a/tests/ci/sqltest.py\n+++ b/tests/ci/sqltest.py\n@@ -16,10 +16,10 @@\n     get_commit,\n     post_commit_status,\n )\n-from docker_pull_helper import get_image_with_version\n+from docker_images_helper import pull_image, get_docker_image\n from env_helper import (\n     GITHUB_RUN_URL,\n-    REPORTS_PATH,\n+    REPORT_PATH,\n     TEMP_PATH,\n )\n from get_robot_token import get_best_robot_token\n@@ -50,9 +50,13 @@ def main():\n     stopwatch = Stopwatch()\n \n     temp_path = Path(TEMP_PATH)\n-    reports_path = Path(REPORTS_PATH)\n+    reports_path = Path(REPORT_PATH)\n+    temp_path.mkdir(parents=True, exist_ok=True)\n \n-    check_name = sys.argv[1]\n+    check_name = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n \n     temp_path.mkdir(parents=True, exist_ok=True)\n \n@@ -66,7 +70,7 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(reports_path, IMAGE_NAME)\n+    docker_image = pull_image(get_docker_image(IMAGE_NAME))\n \n     build_name = get_build_name_for_check(check_name)\n     print(build_name)\n@@ -150,7 +154,9 @@ def main():\n \n     logging.info(\"Result: '%s', '%s', '%s'\", status, description, report_url)\n     print(f\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, status, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, status, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n \n \n if __name__ == \"__main__\":\ndiff --git a/tests/ci/stress_check.py b/tests/ci/stress_check.py\nindex 56c96dc2338a..05250c14fd11 100644\n--- a/tests/ci/stress_check.py\n+++ b/tests/ci/stress_check.py\n@@ -2,6 +2,7 @@\n \n import csv\n import logging\n+import os\n import subprocess\n import sys\n from pathlib import Path\n@@ -21,8 +22,8 @@\n     post_commit_status,\n     format_description,\n )\n-from docker_pull_helper import DockerImage, get_image_with_version\n-from env_helper import TEMP_PATH, REPO_COPY, REPORTS_PATH\n+from docker_images_helper import DockerImage, pull_image, get_docker_image\n+from env_helper import REPORT_PATH, TEMP_PATH, REPO_COPY\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import TestResult, TestResults, read_test_results\n@@ -126,12 +127,15 @@ def run_stress_test(docker_image_name: str) -> None:\n \n     stopwatch = Stopwatch()\n     temp_path = Path(TEMP_PATH)\n+    reports_path = Path(REPORT_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n     repo_path = Path(REPO_COPY)\n     repo_tests_path = repo_path / \"tests\"\n-    reports_path = Path(REPORTS_PATH)\n \n-    check_name = sys.argv[1]\n+    check_name = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n \n     pr_info = PRInfo()\n \n@@ -143,7 +147,7 @@ def run_stress_test(docker_image_name: str) -> None:\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(reports_path, docker_image_name)\n+    docker_image = pull_image(get_docker_image(docker_image_name))\n \n     packages_path = temp_path / \"packages\"\n     packages_path.mkdir(parents=True, exist_ok=True)\n@@ -212,7 +216,9 @@ def run_stress_test(docker_image_name: str) -> None:\n     )\n     print(f\"::notice ::Report url: {report_url}\")\n \n-    post_commit_status(commit, state, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, state, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/style_check.py b/tests/ci/style_check.py\nindex a006e01ff6bb..642fc00e4b63 100644\n--- a/tests/ci/style_check.py\n+++ b/tests/ci/style_check.py\n@@ -9,7 +9,6 @@\n from pathlib import Path\n from typing import List, Tuple\n \n-\n from clickhouse_helper import (\n     ClickHouseHelper,\n     prepare_tests_results_for_clickhouse,\n@@ -20,27 +19,21 @@\n     post_commit_status,\n     update_mergeable_check,\n )\n-from docker_pull_helper import get_image_with_version\n-from env_helper import REPO_COPY, REPORTS_PATH, TEMP_PATH\n+\n+from env_helper import REPO_COPY, TEMP_PATH\n from get_robot_token import get_best_robot_token\n from github_helper import GitHub\n-from git_helper import git_runner\n+from git_helper import GIT_PREFIX, git_runner\n from pr_info import PRInfo\n from report import TestResults, read_test_results\n from s3_helper import S3Helper\n from ssh import SSHKey\n from stopwatch import Stopwatch\n+from docker_images_helper import get_docker_image, pull_image\n from upload_result_helper import upload_results\n \n NAME = \"Style Check\"\n \n-GIT_PREFIX = (  # All commits to remote are done as robot-clickhouse\n-    \"git -c user.email=robot-clickhouse@users.noreply.github.com \"\n-    \"-c user.name=robot-clickhouse -c commit.gpgsign=false \"\n-    \"-c core.sshCommand=\"\n-    \"'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'\"\n-)\n-\n \n def process_result(\n     result_directory: Path,\n@@ -142,15 +135,12 @@ def main():\n     repo_path = Path(REPO_COPY)\n     temp_path = Path(TEMP_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n-    reports_path = Path(REPORTS_PATH)\n-    reports_path.mkdir(parents=True, exist_ok=True)\n \n     pr_info = PRInfo()\n-    if args.push:\n-        checkout_head(pr_info)\n-\n     gh = GitHub(get_best_robot_token(), create_cache_dir=False)\n     commit = get_commit(gh, pr_info.sha)\n+    if args.push:\n+        checkout_head(pr_info)\n \n     atexit.register(update_mergeable_check, gh, pr_info, NAME)\n \n@@ -163,13 +153,14 @@ def main():\n         code = int(state != \"success\")\n         sys.exit(code)\n \n-    docker_image = get_image_with_version(reports_path, \"clickhouse/style-test\")\n     s3_helper = S3Helper()\n \n+    IMAGE_NAME = \"clickhouse/style-test\"\n+    image = pull_image(get_docker_image(IMAGE_NAME))\n     cmd = (\n         f\"docker run -u $(id -u ${{USER}}):$(id -g ${{USER}}) --cap-add=SYS_PTRACE \"\n         f\"--volume={repo_path}:/ClickHouse --volume={temp_path}:/test_output \"\n-        f\"{docker_image}\"\n+        f\"{image}\"\n     )\n \n     logging.info(\"Is going to run the command: %s\", cmd)\n@@ -188,7 +179,9 @@ def main():\n         s3_helper, pr_info.number, pr_info.sha, test_results, additional_files, NAME\n     )\n     print(f\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, state, report_url, description, NAME, pr_info)\n+    post_commit_status(\n+        commit, state, report_url, description, NAME, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/tests/ci/test_digest.py b/tests/ci/test_digest.py\nindex 278b27129171..86ba16ec525f 100644\n--- a/tests/ci/test_digest.py\n+++ b/tests/ci/test_digest.py\n@@ -38,35 +38,6 @@ def test__digest_file(self):\n         dh._digest_file(self.tests_dir / \"symlink-12\", hash_tested)\n         self.assertEqual(hash_expected.digest(), hash_tested.digest())\n \n-    def test__digest_directory(self):\n-        hash_tested = md5()\n-        with self.assertRaises(\n-            AssertionError, msg=\"_digest_directory shouldn't work with files\"\n-        ):\n-            dh._digest_directory(self.tests_dir / \"12\", hash_tested)\n-        with self.assertRaises(\n-            AssertionError, msg=\"_digest_directory shouldn't work with broken links\"\n-        ):\n-            dh._digest_file(self.broken_link, hash_tested)\n-\n-        # dir1\n-        hash_expected = md5()\n-        hash_expected.update(_12 + _14)\n-        dh._digest_directory(self.tests_dir / \"dir1\", hash_tested)\n-        self.assertEqual(hash_expected.digest(), hash_tested.digest())\n-\n-        # dir2 contains 12 and 13\n-        hash_expected = md5()\n-        hash_expected.update(_12 + _13)\n-        hash_tested = md5()\n-        dh._digest_directory(self.tests_dir / \"dir2\", hash_tested)\n-        self.assertEqual(hash_expected.digest(), hash_tested.digest())\n-\n-        # dir3 is symlink to dir2\n-        hash_tested = md5()\n-        dh._digest_directory(self.tests_dir / \"dir3\", hash_tested)\n-        self.assertEqual(hash_expected.digest(), hash_tested.digest())\n-\n     def test_digest_path(self):\n         # test broken link does nothing\n         self.assertEqual(\n@@ -105,7 +76,7 @@ def test_digest_path(self):\n         hash_expected = md5()\n         hash_expected.update(_12 * 2 + _14 + (_12 + _13) * 2 + _12)\n         self.assertEqual(\n-            hash_expected.digest(), dh.digest_path(self.tests_dir).digest()\n+            hash_expected.hexdigest(), dh.digest_path(self.tests_dir).hexdigest()\n         )\n \n     def test_digest_paths(self):\n@@ -119,19 +90,9 @@ def test_digest_paths(self):\n         hash_unordered = dh.digest_paths(\n             (self.tests_dir / d for d in (\"dir3\", \"dir1\", \"dir2\"))\n         )\n-        self.assertNotEqual(hash_ordered.digest(), hash_unordered.digest())\n-        self.assertNotEqual(hash_ordered.digest(), hash_reversed.digest())\n-        self.assertNotEqual(hash_unordered.digest(), hash_reversed.digest())\n-\n-    def test_digest_consistent_paths(self):\n-        # test paths order does not matter\n-        hash_ordered = dh.digest_consistent_paths(\n-            (self.tests_dir / d for d in (\"dir1\", \"dir2\", \"dir3\"))\n-        )\n-        hash_reversed = dh.digest_consistent_paths(\n-            (self.tests_dir / d for d in (\"dir3\", \"dir2\", \"dir1\"))\n-        )\n+        self.assertEqual(hash_ordered.digest(), hash_unordered.digest())\n         self.assertEqual(hash_ordered.digest(), hash_reversed.digest())\n+        self.assertEqual(hash_unordered.digest(), hash_reversed.digest())\n \n     @classmethod\n     def setUpClass(cls):\ndiff --git a/tests/ci/test_docker.py b/tests/ci/test_docker.py\nindex 8aab50ed0825..662143bfd9b4 100644\n--- a/tests/ci/test_docker.py\n+++ b/tests/ci/test_docker.py\n@@ -2,13 +2,6 @@\n \n import unittest\n from unittest.mock import patch, MagicMock\n-from pathlib import Path\n-\n-from env_helper import GITHUB_RUN_URL\n-from pr_info import PRInfo\n-from report import TestResult\n-import docker_images_check as di\n-from docker_images_helper import get_images_dict\n \n from version_helper import get_version_from_string\n import docker_server as ds\n@@ -16,257 +9,6 @@\n # di.logging.basicConfig(level=di.logging.INFO)\n \n \n-class TestDockerImageCheck(unittest.TestCase):\n-    def test_get_changed_docker_images(self):\n-        pr_info = PRInfo(PRInfo.default_event.copy())\n-        pr_info.changed_files = {\n-            \"docker/test/stateless\",\n-            \"docker/test/base\",\n-            \"docker/docs/builder\",\n-        }\n-        images = sorted(\n-            list(\n-                di.get_changed_docker_images(\n-                    pr_info,\n-                    get_images_dict(\n-                        Path(__file__).parent,\n-                        Path(\"tests/docker_images_for_tests.json\"),\n-                    ),\n-                )\n-            )\n-        )\n-        self.maxDiff = None\n-        expected = sorted(\n-            [\n-                di.DockerImage(\"docker/test/base\", \"clickhouse/test-base\", False),\n-                di.DockerImage(\"docker/docs/builder\", \"clickhouse/docs-builder\", True),\n-                di.DockerImage(\n-                    \"docker/test/sqltest\",\n-                    \"clickhouse/sqltest\",\n-                    False,\n-                    \"clickhouse/test-base\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/test/stateless\",\n-                    \"clickhouse/stateless-test\",\n-                    False,\n-                    \"clickhouse/test-base\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/test/integration/base\",\n-                    \"clickhouse/integration-test\",\n-                    False,\n-                    \"clickhouse/test-base\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/test/fuzzer\",\n-                    \"clickhouse/fuzzer\",\n-                    False,\n-                    \"clickhouse/test-base\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/test/keeper-jepsen\",\n-                    \"clickhouse/keeper-jepsen-test\",\n-                    False,\n-                    \"clickhouse/test-base\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/docs/check\",\n-                    \"clickhouse/docs-check\",\n-                    False,\n-                    \"clickhouse/docs-builder\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/docs/release\",\n-                    \"clickhouse/docs-release\",\n-                    False,\n-                    \"clickhouse/docs-builder\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/test/stateful\",\n-                    \"clickhouse/stateful-test\",\n-                    False,\n-                    \"clickhouse/stateless-test\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/test/unit\",\n-                    \"clickhouse/unit-test\",\n-                    False,\n-                    \"clickhouse/stateless-test\",  # type: ignore\n-                ),\n-                di.DockerImage(\n-                    \"docker/test/stress\",\n-                    \"clickhouse/stress-test\",\n-                    False,\n-                    \"clickhouse/stateful-test\",  # type: ignore\n-                ),\n-            ]\n-        )\n-        self.assertEqual(images, expected)\n-\n-    def test_gen_version(self):\n-        pr_info = PRInfo(PRInfo.default_event.copy())\n-        pr_info.base_ref = \"anything-else\"\n-        versions, result_version = di.gen_versions(pr_info, None)\n-        self.assertEqual(versions, [\"0\", \"0-HEAD\"])\n-        self.assertEqual(result_version, \"0-HEAD\")\n-        pr_info.base_ref = \"master\"\n-        versions, result_version = di.gen_versions(pr_info, None)\n-        self.assertEqual(versions, [\"latest\", \"0\", \"0-HEAD\"])\n-        self.assertEqual(result_version, \"0-HEAD\")\n-        versions, result_version = di.gen_versions(pr_info, \"suffix\")\n-        self.assertEqual(versions, [\"latest-suffix\", \"0-suffix\", \"0-HEAD-suffix\"])\n-        self.assertEqual(result_version, versions)\n-        pr_info.number = 1\n-        versions, result_version = di.gen_versions(pr_info, None)\n-        self.assertEqual(versions, [\"1\", \"1-HEAD\"])\n-        self.assertEqual(result_version, \"1-HEAD\")\n-\n-    @patch(\"docker_images_check.TeePopen\")\n-    @patch(\"platform.machine\")\n-    def test_build_and_push_one_image(self, mock_machine, mock_popen):\n-        mock_popen.return_value.__enter__.return_value.wait.return_value = 0\n-        image = di.DockerImage(\"path\", \"name\", False, gh_repo=\"\")\n-\n-        result, _ = di.build_and_push_one_image(image, \"version\", [], True, True)\n-        mock_popen.assert_called_once()\n-        mock_machine.assert_not_called()\n-        self.assertIn(\n-            f\"docker buildx build --builder default --label build-url={GITHUB_RUN_URL} \"\n-            \"--build-arg FROM_TAG=version \"\n-            f\"--build-arg CACHE_INVALIDATOR={GITHUB_RUN_URL} \"\n-            \"--tag name:version --cache-from type=registry,ref=name:version \"\n-            \"--cache-from type=registry,ref=name:latest \"\n-            \"--cache-to type=inline,mode=max --push --progress plain path\",\n-            mock_popen.call_args.args,\n-        )\n-        self.assertTrue(result)\n-        mock_popen.reset_mock()\n-        mock_machine.reset_mock()\n-\n-        mock_popen.return_value.__enter__.return_value.wait.return_value = 0\n-        result, _ = di.build_and_push_one_image(image, \"version2\", [], False, True)\n-        mock_popen.assert_called_once()\n-        mock_machine.assert_not_called()\n-        self.assertIn(\n-            f\"docker buildx build --builder default --label build-url={GITHUB_RUN_URL} \"\n-            \"--build-arg FROM_TAG=version2 \"\n-            f\"--build-arg CACHE_INVALIDATOR={GITHUB_RUN_URL} \"\n-            \"--tag name:version2 --cache-from type=registry,ref=name:version2 \"\n-            \"--cache-from type=registry,ref=name:latest \"\n-            \"--cache-to type=inline,mode=max --progress plain path\",\n-            mock_popen.call_args.args,\n-        )\n-        self.assertTrue(result)\n-\n-        mock_popen.reset_mock()\n-        mock_machine.reset_mock()\n-        mock_popen.return_value.__enter__.return_value.wait.return_value = 1\n-        result, _ = di.build_and_push_one_image(image, \"version2\", [], False, False)\n-        mock_popen.assert_called_once()\n-        mock_machine.assert_not_called()\n-        self.assertIn(\n-            f\"docker buildx build --builder default --label build-url={GITHUB_RUN_URL} \"\n-            f\"--build-arg CACHE_INVALIDATOR={GITHUB_RUN_URL} \"\n-            \"--tag name:version2 --cache-from type=registry,ref=name:version2 \"\n-            \"--cache-from type=registry,ref=name:latest \"\n-            \"--cache-to type=inline,mode=max --progress plain path\",\n-            mock_popen.call_args.args,\n-        )\n-        self.assertFalse(result)\n-\n-        mock_popen.reset_mock()\n-        mock_machine.reset_mock()\n-        mock_popen.return_value.__enter__.return_value.wait.return_value = 1\n-        result, _ = di.build_and_push_one_image(\n-            image, \"version2\", [\"cached-version\", \"another-cached\"], False, False\n-        )\n-        mock_popen.assert_called_once()\n-        mock_machine.assert_not_called()\n-        self.assertIn(\n-            f\"docker buildx build --builder default --label build-url={GITHUB_RUN_URL} \"\n-            f\"--build-arg CACHE_INVALIDATOR={GITHUB_RUN_URL} \"\n-            \"--tag name:version2 --cache-from type=registry,ref=name:version2 \"\n-            \"--cache-from type=registry,ref=name:latest \"\n-            \"--cache-from type=registry,ref=name:cached-version \"\n-            \"--cache-from type=registry,ref=name:another-cached \"\n-            \"--cache-to type=inline,mode=max --progress plain path\",\n-            mock_popen.call_args.args,\n-        )\n-        self.assertFalse(result)\n-\n-        mock_popen.reset_mock()\n-        mock_machine.reset_mock()\n-        only_amd64_image = di.DockerImage(\"path\", \"name\", True)\n-        mock_popen.return_value.__enter__.return_value.wait.return_value = 0\n-\n-        result, _ = di.build_and_push_one_image(\n-            only_amd64_image, \"version\", [], True, True\n-        )\n-        mock_popen.assert_called_once()\n-        mock_machine.assert_called_once()\n-        self.assertIn(\n-            \"docker pull ubuntu:20.04; docker tag ubuntu:20.04 name:version; \"\n-            \"docker push name:version\",\n-            mock_popen.call_args.args,\n-        )\n-        self.assertTrue(result)\n-        result, _ = di.build_and_push_one_image(\n-            only_amd64_image, \"version\", [], False, True\n-        )\n-        self.assertIn(\n-            \"docker pull ubuntu:20.04; docker tag ubuntu:20.04 name:version; \",\n-            mock_popen.call_args.args,\n-        )\n-        with self.assertRaises(AssertionError):\n-            result, _ = di.build_and_push_one_image(image, \"version\", [\"\"], False, True)\n-\n-    @patch(\"docker_images_check.build_and_push_one_image\")\n-    def test_process_image_with_parents(self, mock_build):\n-        mock_build.side_effect = lambda v, w, x, y, z: (True, Path(f\"{v.repo}_{w}.log\"))\n-        im1 = di.DockerImage(\"path1\", \"repo1\", False)\n-        im2 = di.DockerImage(\"path2\", \"repo2\", False, im1)\n-        im3 = di.DockerImage(\"path3\", \"repo3\", False, im2)\n-        im4 = di.DockerImage(\"path4\", \"repo4\", False, im1)\n-        # We use list to have determined order of image builgings\n-        images = [im4, im1, im3, im2, im1]\n-        test_results = [\n-            di.process_image_with_parents(im, [\"v1\", \"v2\", \"latest\"], [], True)\n-            for im in images\n-        ]\n-        # The time is random, so we check it's not None and greater than 0,\n-        # and then set to 1\n-        for results in test_results:\n-            for result in results:\n-                self.assertIsNotNone(result.time)\n-                self.assertGreater(result.time, 0)  # type: ignore\n-                result.time = 1\n-\n-        self.maxDiff = None\n-        expected = [\n-            [  # repo4 -> repo1\n-                TestResult(\"repo1:v1\", \"OK\", 1, [Path(\"repo1_v1.log\")]),\n-                TestResult(\"repo1:v2\", \"OK\", 1, [Path(\"repo1_v2.log\")]),\n-                TestResult(\"repo1:latest\", \"OK\", 1, [Path(\"repo1_latest.log\")]),\n-                TestResult(\"repo4:v1\", \"OK\", 1, [Path(\"repo4_v1.log\")]),\n-                TestResult(\"repo4:v2\", \"OK\", 1, [Path(\"repo4_v2.log\")]),\n-                TestResult(\"repo4:latest\", \"OK\", 1, [Path(\"repo4_latest.log\")]),\n-            ],\n-            [],  # repo1 is built\n-            [  # repo3 -> repo2 -> repo1\n-                TestResult(\"repo2:v1\", \"OK\", 1, [Path(\"repo2_v1.log\")]),\n-                TestResult(\"repo2:v2\", \"OK\", 1, [Path(\"repo2_v2.log\")]),\n-                TestResult(\"repo2:latest\", \"OK\", 1, [Path(\"repo2_latest.log\")]),\n-                TestResult(\"repo3:v1\", \"OK\", 1, [Path(\"repo3_v1.log\")]),\n-                TestResult(\"repo3:v2\", \"OK\", 1, [Path(\"repo3_v2.log\")]),\n-                TestResult(\"repo3:latest\", \"OK\", 1, [Path(\"repo3_latest.log\")]),\n-            ],\n-            [],  # repo2 -> repo1 are built\n-            [],  # repo1 is built\n-        ]\n-        self.assertEqual(test_results, expected)\n-\n-\n class TestDockerServer(unittest.TestCase):\n     def test_gen_tags(self):\n         version = get_version_from_string(\"22.2.2.2\")\ndiff --git a/tests/ci/unit_tests_check.py b/tests/ci/unit_tests_check.py\nindex b9fdddfe74b2..d6767cf8b7fa 100644\n--- a/tests/ci/unit_tests_check.py\n+++ b/tests/ci/unit_tests_check.py\n@@ -22,8 +22,8 @@\n     post_commit_status,\n     update_mergeable_check,\n )\n-from docker_pull_helper import get_image_with_version\n-from env_helper import TEMP_PATH, REPORTS_PATH\n+from docker_images_helper import pull_image, get_docker_image\n+from env_helper import REPORT_PATH, TEMP_PATH\n from get_robot_token import get_best_robot_token\n from pr_info import PRInfo\n from report import ERROR, FAILURE, FAIL, OK, SUCCESS, TestResults, TestResult\n@@ -174,7 +174,10 @@ def main():\n \n     stopwatch = Stopwatch()\n \n-    check_name = sys.argv[1]\n+    check_name = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"CHECK_NAME\")\n+    assert (\n+        check_name\n+    ), \"Check name must be provided as an input arg or in CHECK_NAME env\"\n \n     temp_path = Path(TEMP_PATH)\n     temp_path.mkdir(parents=True, exist_ok=True)\n@@ -191,9 +194,9 @@ def main():\n         logging.info(\"Check is already finished according to github status, exiting\")\n         sys.exit(0)\n \n-    docker_image = get_image_with_version(REPORTS_PATH, IMAGE_NAME)\n+    docker_image = pull_image(get_docker_image(IMAGE_NAME))\n \n-    download_unit_tests(check_name, REPORTS_PATH, TEMP_PATH)\n+    download_unit_tests(check_name, REPORT_PATH, TEMP_PATH)\n \n     tests_binary = temp_path / \"unit_tests_dbms\"\n     os.chmod(tests_binary, 0o777)\n@@ -233,7 +236,9 @@ def main():\n         check_name,\n     )\n     print(f\"::notice ::Report url: {report_url}\")\n-    post_commit_status(commit, state, report_url, description, check_name, pr_info)\n+    post_commit_status(\n+        commit, state, report_url, description, check_name, pr_info, dump_to_file=True\n+    )\n \n     prepared_events = prepare_tests_results_for_clickhouse(\n         pr_info,\ndiff --git a/docker/test/performance-comparison/README.md b/tests/performance/scripts/README.md\nsimilarity index 93%\nrename from docker/test/performance-comparison/README.md\nrename to tests/performance/scripts/README.md\nindex fd9001e23c75..0a0580c62a00 100644\n--- a/docker/test/performance-comparison/README.md\n+++ b/tests/performance/scripts/README.md\n@@ -25,7 +25,7 @@ The check status summarizes the report in a short text message like `1 faster, 1\n * `1 unstable` -- how many queries have unstable results,\n * `1 errors` -- how many errors there are in total. Action is required for every error, this number must be zero. The number of errors includes slower tests, tests that are too long, errors while running the tests and building reports, etc. Please look at the main report page to investigate these errors.\n \n-The report page itself constists of a several tables. Some of them always signify errors, e.g. \"Run errors\" -- the very presence of this table indicates that there were errors during the test, that are not normal and must be fixed. Some tables are mostly informational, e.g. \"Test times\" -- they reflect normal test results. But if a cell in such table is marked in red, this also means an error, e.g., a test is taking too long to run.\n+The report page itself consists of a several tables. Some of them always signify errors, e.g. \"Run errors\" -- the very presence of this table indicates that there were errors during the test, that are not normal and must be fixed. Some tables are mostly informational, e.g. \"Test times\" -- they reflect normal test results. But if a cell in such table is marked in red, this also means an error, e.g., a test is taking too long to run.\n \n #### Tested Commits\n Informational, no action required. Log messages for the commits that are tested. Note that for the right commit, we show nominal tested commit `pull/*/head` and real tested commit `pull/*/merge`, which is generated by GitHub by merging latest master to the `pull/*/head` and which we actually build and test in CI.\n@@ -33,12 +33,12 @@ Informational, no action required. Log messages for the commits that are tested.\n #### Error Summary\n Action required for every item.\n \n-This table summarizes all errors that ocurred during the test. Click the links to go to the description of a particular error.\n+This table summarizes all errors that occurred during the test. Click the links to go to the description of a particular error.\n \n #### Run Errors\n Action required for every item -- these are errors that must be fixed.\n \n-The errors that ocurred when running some test queries. For more information about the error, download test output archive and see `test-name-err.log`. To reproduce, see 'How to run' below.\n+The errors that occurred when running some test queries. For more information about the error, download test output archive and see `test-name-err.log`. To reproduce, see 'How to run' below.\n \n #### Slow on Client\n Action required for every item -- these are errors that must be fixed.\n@@ -65,7 +65,7 @@ You can find flame graphs for queries with performance changes in the test outpu\n #### Unstable Queries\n Action required for the cells marked in red.\n \n-These are the queries for which we did not observe a statistically significant change in performance, but for which the variance in query performance is very high. This means that we are likely to observe big changes in performance even in the absence of real changes, e.g. when comparing the server to itself. Such queries are going to have bad sensitivity as performance tests -- if a query has, say, 50% expected variability, this means we are going to see changes in performance up to 50%, even when there were no real changes in the code. And because of this, we won't be able to detect changes less than 50% with such a query, which is pretty bad. The reasons for the high variability must be investigated and fixed; ideally, the variability should be brought under 5-10%. \n+These are the queries for which we did not observe a statistically significant change in performance, but for which the variance in query performance is very high. This means that we are likely to observe big changes in performance even in the absence of real changes, e.g. when comparing the server to itself. Such queries are going to have bad sensitivity as performance tests -- if a query has, say, 50% expected variability, this means we are going to see changes in performance up to 50%, even when there were no real changes in the code. And because of this, we won't be able to detect changes less than 50% with such a query, which is pretty bad. The reasons for the high variability must be investigated and fixed; ideally, the variability should be brought under 5-10%.\n \n The most frequent reason for instability is that the query is just too short -- e.g. below 0.1 seconds. Bringing query time to 0.2 seconds or above usually helps.\n Other reasons may include:\n@@ -88,7 +88,7 @@ This table summarizes the changes in performance of queries in each test -- how\n Action required for the cells marked in red.\n \n This table shows the run times for all the tests. You may have to fix two kinds of errors in this table:\n-1) Average query run time is too long -- probalby means that the preparatory steps such as creating the table and filling them with data are taking too long. Try to make them faster.\n+1) Average query run time is too long -- probably means that the preparatory steps such as creating the table and filling them with data are taking too long. Try to make them faster.\n 2) Longest query run time is too long -- some particular queries are taking too long, try to make them faster. The ideal query run time is between 0.1 and 1 s.\n \n #### Metric Changes\n@@ -186,4 +186,4 @@ analytically, but I don't know enough math to do it. It would be something\n close to Wilcoxon test distribution.\n \n ### References\n-1\\. Box, Hunter, Hunter \"Statictics for exprerimenters\", p. 78: \"A Randomized Design Used in the Comparison of Standard and Modified Fertilizer Mixtures for Tomato Plants.\"\n+1\\. Box, Hunter, Hunter \"Statistics for exprerimenters\", p. 78: \"A Randomized Design Used in the Comparison of Standard and Modified Fertilizer Mixtures for Tomato Plants.\"\ndiff --git a/docker/test/performance-comparison/compare-releases.sh b/tests/performance/scripts/compare-releases.sh\nsimilarity index 99%\nrename from docker/test/performance-comparison/compare-releases.sh\nrename to tests/performance/scripts/compare-releases.sh\nindex dc7681815d47..6e982168fb17 100755\n--- a/docker/test/performance-comparison/compare-releases.sh\n+++ b/tests/performance/scripts/compare-releases.sh\n@@ -79,4 +79,3 @@ run\n \n rm output.7z\n 7z a output.7z ./*.{log,tsv,html,txt,rep,svg} {right,left}/{performance,db/preprocessed_configs}\n-\ndiff --git a/docker/test/performance-comparison/compare.sh b/tests/performance/scripts/compare.sh\nsimilarity index 99%\nrename from docker/test/performance-comparison/compare.sh\nrename to tests/performance/scripts/compare.sh\nindex f10236b7135b..454b8903e5a3 100755\n--- a/docker/test/performance-comparison/compare.sh\n+++ b/tests/performance/scripts/compare.sh\n@@ -236,7 +236,7 @@ function run_tests\n         fi\n     fi\n \n-    # For PRs w/o changes in test definitons, test only a subset of queries,\n+    # For PRs w/o changes in test definitions, test only a subset of queries,\n     # and run them less times. If the corresponding environment variables are\n     # already set, keep those values.\n     #\ndiff --git a/docker/test/performance-comparison/config/client_config.xml b/tests/performance/scripts/config/client_config.xml\nsimilarity index 100%\nrename from docker/test/performance-comparison/config/client_config.xml\nrename to tests/performance/scripts/config/client_config.xml\ndiff --git a/docker/test/performance-comparison/config/config.d/top_level_domains_lists.xml b/tests/performance/scripts/config/config.d/top_level_domains_lists.xml\nsimilarity index 100%\nrename from docker/test/performance-comparison/config/config.d/top_level_domains_lists.xml\nrename to tests/performance/scripts/config/config.d/top_level_domains_lists.xml\ndiff --git a/docker/test/performance-comparison/config/config.d/user_files.xml b/tests/performance/scripts/config/config.d/user_files.xml\nsimilarity index 100%\nrename from docker/test/performance-comparison/config/config.d/user_files.xml\nrename to tests/performance/scripts/config/config.d/user_files.xml\ndiff --git a/docker/test/performance-comparison/config/config.d/zzz-perf-comparison-tweaks-config.xml b/tests/performance/scripts/config/config.d/zzz-perf-comparison-tweaks-config.xml\nsimilarity index 100%\nrename from docker/test/performance-comparison/config/config.d/zzz-perf-comparison-tweaks-config.xml\nrename to tests/performance/scripts/config/config.d/zzz-perf-comparison-tweaks-config.xml\ndiff --git a/docker/test/performance-comparison/config/users.d/perf-comparison-tweaks-users.xml b/tests/performance/scripts/config/users.d/perf-comparison-tweaks-users.xml\nsimilarity index 100%\nrename from docker/test/performance-comparison/config/users.d/perf-comparison-tweaks-users.xml\nrename to tests/performance/scripts/config/users.d/perf-comparison-tweaks-users.xml\ndiff --git a/docker/test/performance-comparison/download.sh b/tests/performance/scripts/download.sh\nsimilarity index 100%\nrename from docker/test/performance-comparison/download.sh\nrename to tests/performance/scripts/download.sh\ndiff --git a/docker/test/performance-comparison/entrypoint.sh b/tests/performance/scripts/entrypoint.sh\nsimilarity index 89%\nrename from docker/test/performance-comparison/entrypoint.sh\nrename to tests/performance/scripts/entrypoint.sh\nindex fb5e6bd2a7af..95ffe44b6549 100755\n--- a/docker/test/performance-comparison/entrypoint.sh\n+++ b/tests/performance/scripts/entrypoint.sh\n@@ -7,8 +7,9 @@ export CHPC_CHECK_START_TIMESTAMP\n S3_URL=${S3_URL:=\"https://clickhouse-builds.s3.amazonaws.com\"}\n BUILD_NAME=${BUILD_NAME:-package_release}\n export S3_URL BUILD_NAME\n+SCRIPT_DIR=\"$(dirname \"$(readlink -f \"$0\")\")\"\n \n-# Sometimes AWS responde with DNS error and it's impossible to retry it with\n+# Sometimes AWS responds with DNS error and it's impossible to retry it with\n # current curl version options.\n function curl_with_retry\n {\n@@ -88,19 +89,9 @@ chmod 777 workspace output\n \n cd workspace\n \n-# Download the package for the version we are going to test.\n-# A temporary solution for migrating into PRs directory\n-for prefix in \"$S3_URL/PRs\" \"$S3_URL\";\n-do\n-    if curl_with_retry \"$prefix/$PR_TO_TEST/$SHA_TO_TEST/$BUILD_NAME/performance.tar.zst\"\n-    then\n-        right_path=\"$prefix/$PR_TO_TEST/$SHA_TO_TEST/$BUILD_NAME/performance.tar.zst\"\n-        break\n-    fi\n-done\n-\n-mkdir right\n-wget -nv -nd -c \"$right_path\" -O- | tar -C right --no-same-owner --strip-components=1 --zstd --extract --verbose\n+[ ! -e \"/artifacts/performance.tar.zst\" ] && echo \"ERROR: performance.tar.zst not found\" && exit 1\n+mkdir -p right\n+tar -xf \"/artifacts/performance.tar.zst\" -C right --no-same-owner --strip-components=1 --zstd --extract --verbose\n \n # Find reference revision if not specified explicitly\n if [ \"$REF_SHA\" == \"\" ]; then find_reference_sha; fi\n@@ -158,7 +149,7 @@ cat /proc/sys/kernel/core_pattern\n \n # Start the main comparison script.\n {\n-    time ../download.sh \"$REF_PR\" \"$REF_SHA\" \"$PR_TO_TEST\" \"$SHA_TO_TEST\" && \\\n+    time $SCRIPT_DIR/download.sh \"$REF_PR\" \"$REF_SHA\" \"$PR_TO_TEST\" \"$SHA_TO_TEST\" && \\\n     time stage=configure \"$script_path\"/compare.sh ; \\\n } 2>&1 | ts \"$(printf '%%Y-%%m-%%d %%H:%%M:%%S\\t')\" | tee compare.log\n \ndiff --git a/docker/test/performance-comparison/eqmed.sql b/tests/performance/scripts/eqmed.sql\nsimilarity index 97%\nrename from docker/test/performance-comparison/eqmed.sql\nrename to tests/performance/scripts/eqmed.sql\nindex d0111550ee6a..94e6733a3d71 100644\n--- a/docker/test/performance-comparison/eqmed.sql\n+++ b/tests/performance/scripts/eqmed.sql\n@@ -12,7 +12,7 @@ from\n       -- quantiles of randomization distributions\n       -- note that for small number of runs, the exact quantile might not make\n       -- sense, because the last possible value of randomization distribution\n-      -- might take a larger percentage of distirbution (i.e. the distribution\n+      -- might take a larger percentage of distribution (i.e. the distribution\n       -- actually has discrete values, and the last step can be large).\n       select quantileExactForEach(0.99)(\n         arrayMap(x, y -> abs(x - y), metrics_by_label[1], metrics_by_label[2]) as d\n@@ -44,7 +44,7 @@ from\n                               -- for each virtual run, randomly reorder measurements\n                               order by virtual_run, rand()\n                            ) virtual_runs\n-                     ) relabeled \n+                     ) relabeled\n                   group by virtual_run, random_label\n                ) virtual_medians\n             group by virtual_run -- aggregate by random_label\ndiff --git a/docker/test/performance-comparison/manual-run.sh b/tests/performance/scripts/manual-run.sh\nsimilarity index 99%\nrename from docker/test/performance-comparison/manual-run.sh\nrename to tests/performance/scripts/manual-run.sh\nindex 2cc40bf4648a..82609489d726 100755\n--- a/docker/test/performance-comparison/manual-run.sh\n+++ b/tests/performance/scripts/manual-run.sh\n@@ -51,4 +51,3 @@ run\n \n rm output.7z\n 7z a output.7z ./*.{log,tsv,html,txt,rep,svg} {right,left}/{performance,db/preprocessed_configs}\n-\ndiff --git a/docker/test/performance-comparison/perf.py b/tests/performance/scripts/perf.py\nsimilarity index 99%\nrename from docker/test/performance-comparison/perf.py\nrename to tests/performance/scripts/perf.py\nindex d23a9ac61c19..e98c158249a8 100755\n--- a/docker/test/performance-comparison/perf.py\n+++ b/tests/performance/scripts/perf.py\n@@ -357,7 +357,7 @@ def do_create(connection, index, queries):\n             prewarm_id = f\"{query_prefix}.prewarm0\"\n \n             try:\n-                # During the warmup runs, we will also:\n+                # During the warm-up runs, we will also:\n                 # * detect queries that are exceedingly long, to fail fast,\n                 # * collect profiler traces, which might be helpful for analyzing\n                 #   test coverage. We disable profiler for normal runs because\n@@ -390,7 +390,7 @@ def do_create(connection, index, queries):\n             query_error_on_connection[conn_index] = traceback.format_exc()\n             continue\n \n-    # Report all errors that ocurred during prewarm and decide what to do next.\n+    # Report all errors that occurred during prewarm and decide what to do next.\n     # If prewarm fails for the query on all servers -- skip the query and\n     # continue testing the next query.\n     # If prewarm fails on one of the servers, run the query on the rest of them.\ndiff --git a/docker/test/performance-comparison/report.py b/tests/performance/scripts/report.py\nsimilarity index 100%\nrename from docker/test/performance-comparison/report.py\nrename to tests/performance/scripts/report.py\n",
  "problem_statement": "[RFC] Boost up CI with source files Digest\nThis proposes a new source digesting technique in the CI pipeline aimed at improving overall efficiency, including computational resource utilization, reliability, pipeline execution time, and artifact/job reusability. It addresses the CI execution flow and does not cover the content of tests/checks running in the pipeline.\r\n\r\nBy source files digesting, we mean a technique that calculates a hash value over a specific part of the CH source code to facilitate the reuse of build artifacts, skip jobs if already run, and more.\r\n\r\n### Three Ways to Improve CI with source files Digest:\r\n\r\n1. **Digest to Reuse Build Artifacts:**\r\n   - To reuse build artifacts whenever possible, the digest is calculated over CH source files `./src`, `./base`, `./cmake`, `./programs`, git submodules, and builder docker image(s). This is simplified bash commands to give an idea how digest is calculated for a build:\r\n```bash\r\nSOURCE_DIGEST=$(find ./src -type f | xargs md5sum | sort | md5sum)\r\nMODULES_DIGEST=$(git submodule status | md5sum)\r\nDOCKER_BUILDER_DIGEST=$(find ./docker/packager/ -type f | xargs md5sum | sort | md5sum)\r\nBUILD_DIGEST=$(echo $SOURCE_DIGEST-$MODULES_DIGEST-$DOCKER_BUILDER_DIGEST | md5sum)\r\n```\r\n   - CI build jobs should calculate this digest before the build and check if the desired build with the resulting digest is already uploaded on S3. If uploaded, the job is skipped. The digest is part of the S3 URL path, e.g.: `https://s3.console.aws.amazon.com/s3/buckets/clickhouse-builds?prefix=PRs/storage/<digest>/package_release/clickhouse`.\r\n\r\n>    **Benefits:**\r\n>    - Can be done on a shallow cloned repo without git history or GH API usage.\r\n>    - Artifact reuse across any git branches.\r\n>    - Locally Reproducible.\r\n> \r\n>    **Possible Obstacles:**\r\n>    - GitHub's merge from master before testing may modify sources and digest value, decreasing the artifact reuse rate. Possible solution: Disable merge from master.\r\n>    - Automatic build versioning may modify sources. Possible solution: Count digest before autogeneration.\r\n\r\n2. **Digest to tag docker images:**\r\n   - Instead of relying on GH API, git history, and CH CI database, propose calculating the digest for a docker image and using this digest as the image tag.\r\n   - The digest is calculated based on docker files and dependent-on docker images. Shallow clone is enough to calculate docker digest. If a docker image with the resulting tag exists, CI reuses it; otherwise, it builds it.\r\n   - This works automatically on any repo branch: master, release, or PR and avoids rebuilding docker image whenever possible.\r\n\r\n3. **Tests Digest:**\r\n   - A digest could be calculated against each test category (Functional, Integration, etc.). If this digest is written to the CI checks database, it provides a means to skip certain Test Cases for a specific build and test digests match. This could be beneficial in case of job re-run due to AWS spot instance restarts.\r\n\n",
  "hints_text": "Nice approach.\r\n\r\nSome comments:\r\n* `SOURCE_DIGEST=$(find ./src -type f | xargs md5sum | sort | md5sum)`  is missing other source folders (base/ cmake/ programs/ at least) which is worrisome because somebody needs to keep this updated with any folder changes or you'll get old/invalid artifacts and won't notice a thing.\r\n* Any estimation / verification on which percentage of runs would be improved by this?\r\n* Based on the above, what is the reason we are rebuilding the same thing multiple times? Would a different approach help there? \n> Nice approach.\r\n> \r\n> Some comments:\r\n> \r\n> * `SOURCE_DIGEST=$(find ./src -type f | xargs md5sum | sort | md5sum)`  is missing other source folders (base/ cmake/ programs/ at least) which is worrisome because somebody needs to keep this updated with any folder changes or you'll get old/invalid artifacts and won't notice a thing.\r\n> * Any estimation / verification on which percentage of runs would be improved by this?\r\n> * Based on the above, what is the reason we are rebuilding the same thing multiple times? Would a different approach help there?\r\n\r\nThanks for the comment. I updated source paths in the text for the build digest. \r\n* It's difficult to give some numbers on how many runs could be improved by build digest usage, but given the fact that some commits address only tests, docs, ci scripts without touching the CH binary, it's expected to be beneficial.\r\n* Docker images digesting: We do not change dockers too often (at least for master or PRs) and main benefit here, I think, is making CI scripts more generic and simple when it comes to maintaining docker tags.\r\n* Test digests: this is the most tricky thing too implement. From the CI _checks_ DB I would estimate number of failed jobs  due to spot instance going down (number of completed job steps <= 5) as ~2-3%. There are also other cases when it can help to skip some unnecessary work (e.g. binary and certain test category has not been changed in the commit). I don't expect it will take place very often but anyway can help get faster results for a developer.\r\n\r\n* Why do we rebuild the same thing? Because we have nothing to control it at the moment and this improvements suggest to change this.  \n> Why do we rebuild the same thing? Because we have nothing to control it at the moment and this improvements suggest to change this.\r\n\r\nHave we thought of any way to allow retry on certain tasks only? Many of the reruns I've seen are from people retrying because of flaky CI (flaky tests or instances), and what has happened is people rebase / merge master to retrigger a full CI run. If a limited number of people could re-run a single task then we'd avoid rerunning the other 100+ tasks unnecessarily.\nWe already use ccache / sccache to reuse object files compiled from unchanged source files between builds. Are digests orthogonal to that?\n> > Why do we rebuild the same thing? Because we have nothing to control it at the moment and this improvements suggest to change this.\r\n> \r\n> Have we thought of any way to allow retry on certain tasks only? Many of the reruns I've seen are from people retrying because of flaky CI (flaky tests or instances), and what has happened is people rebase / merge master to retrigger a full CI run. If a limited number of people could re-run a single task then we'd avoid rerunning the other 100+ tasks unnecessarily.\r\n\r\nYes, this is a good point where we can improve. First of all, flaky tests are so annoying and there shouldn't be flaky tests in CI, I believe. Regarding re-run failed job, actually, it's possible now, though it will require manual work-around to reset failed job status via GH API before rerun. I think we can improve here but this topic is not in scope now. However, described approach with digesting can help too in case of pipeline re-run as it allows to skip builds (1), skip docker builds if any (2), skip successful test jobs (3).\n> We already use ccache / sccache to reuse object files compiled from unchanged source files between builds. Are digests orthogonal to that?\r\n\r\nsccache speeds up our builds significantly, but still loads, compiles, links, uploads, etc. _Build Digest_ lets you skip the build step entirely when possible, so I'd say it's orthogonal\n> The digest is calculated based on docker files and dependent-on docker images. Shallow clone is enough to calculate docker digest. If a docker image with the resulting tag exists, CI reuses it; otherwise, it builds it.\r\n\r\n@Felixoid please correct me if I am wrong, but I think two docker images from the same docker files could be different depending on when they were built, e.g.: `sudo apt install` can install different versions of package. I am not sure currently how do we ensure the freshness of packages (especially questionable with [enrich_images](https://github.com/ClickHouse/ClickHouse/blob/1d5f75fd716c36f474a94ca4d67bf7598ab1cd4b/tests/ci/docker_manifests_merge.py#L178-L243), we kind of reusing old images), but we should think about this also.\r\n\r\n~~Speaking of `enrich_images`, doesn't that skip the building of docker images?~~\r\n\r\nPS.: I found the answer for my second point:\r\n> Instead of relying on GH API, git history, and CH CI database, propose calculating the digest for a docker image and using this digest as the image tag.\r\n\r\n`enrich_images` uses the git history/GH API/etc to figure out images.\n> I think two docker images from the same docker files could be different depending on when they were built, e.g.: `sudo apt install` can install different versions of package.\r\n\r\nYou are right. There is nightly job that continuously update latest docker images, images(tags) built for previous commits are not being updated. I thinks it's appropriate and it should work the same way with digest as a tag.\n> sccache speeds up our builds significantly, but still loads, compiles, links, uploads, etc. Build Digest lets you skip the build step entirely when possible, so I'd say it's orthogonal\r\n\r\nThanks. Very generally speaking, the effort that any CI will do falls into two categories:\r\n- work that cannot be avoided because it has never been done before\r\n- work that can be avoided because it was done before.\r\n\r\nI wonder how big the relation between both categories is for ClickHouse's CI. I'll give an example.\r\n\r\nThink about functional/integration/other tests: Their output depends on the tested ClickHouse binary which will be different between different PRs. So the only chance to save work will be \"within\" a PR. For example, I often run into the situation that I change the C++ code and the tests. This then goes into one or multiple commit. CI then often finds issues (e.g. flaky tests) which I then fix by adjusting tests while the code remains unchanged (example [here](https://github.com/ClickHouse/ClickHouse/pull/55983, commit \"Try to stabilize test\"). Builds currently start regardless and because of sccache they will be \"fast\" (compared to a scratch build). The question is however how much *not* starting the build jobs (due to digests) would save us in relation to the unavoidable costs of CI jobs for such a commit. The unavoidable costs are dominated by the cost of the test jobs.\r\n\r\nBut maybe I miss something as I am not an expert in CI and other are other situations where digests are useful.\n@rschu1ze\r\nI see following cases where we could save work with build digest:\r\n1. commits within a PR with no CH source files touched \r\n2. pipeline reruns\r\n3. whole PR could touch only tests (for instance), so build could be taken from base branch\r\n\r\nI think you make a good point that relation of avoidable builds against all jobs is small and  I don't expect _build skip by the digest_ to be more than a small share in total CI execution time, given that build jobs itself are small part of pipeline duration, though I do expect important benefits of this:\r\n1. we still avoid wasting many VM-hours and storage in absolute numbers, as we have many parallel builds for each commit and  it comes at zero cost: easy to calculate, code should be simple and generic and work for any branch or for re-runs\r\n2. it opens up door for next improvement: skipping tests if say _build and test digest_ pair already done which might save way more time (item 3 - tests digest)\r\n\r\n\nHuh, it's a lot to follow up on the very first day after the vacation, so I've postponed it. So far I'll address only the most controversial topic of docker images.\r\n\r\nThe digest as the tags IMHO won't work for us. For many reasons:\r\n\r\n- The directory with a Dockerfile itself won't provide the full picture of the image. First, because we have `FROM` (in details below), and second because we don't pin every software version there in apt and pipy. As @antaljanosbenjamin said. And I am against the idea of global pinning.\r\n- We'll lose the `docker buildx --cache-from` feature because we won't know the previous digest. This is a game-breaker.\r\n- Then the script to determine each `FROM_TAG` will become much more complex, and will work in the [build_and_push_one_image](https://github.com/ClickHouse/ClickHouse/blob/1e4fe038f562029fc24a0e7a33e5d428ea0474f9/tests/ci/docker_images_check.py#L213) stage. The whole dependency tree building will look redundant and broken at the same time.\r\n- Now we can run `docker run ... clickhouse/binary-builder:56015 bash` and investigate it without additional steps to identify the image tag. Losing it is a slight inconvenience.\r\n- Think of the cloud repo, where directories could be different `just because`. So, the whole system will break, because the tag does not exist?\r\n- Altogether, it will probably cause the reworking of the whole docker building system.\r\n\r\nGiven it, what's the flaw of the current system? Using clickhouse service for getting the last built images? We are heavily bound to this service. We don't use anything else, and the full checkout after the recent `filter:tree=0` is very fast now.\r\n\r\nJFYI, there's a ticket to improve the mentioned places a little bit https://github.com/ClickHouse/ClickHouse/issues/41568, but it's about a completely orthogonal problem.\r\n\r\n### Update\r\n\r\nWe've spoken in a huddle. The digest would work together with the mentioned ticket, as an additional image. This image will be used everywhere and automatically calculated from the repository, so we won't need the `changed_images.json` anymore at all.\r\n\r\nLet's try!\nNow, regarding the first, the most promising part. Digest to reuse the build artifacts. I'll describe it in python code, the place where it would be mostly implementer is near the cargo cache in [ccache_utils](https://github.com/ClickHouse/ClickHouse/blob/c5b1aa4/tests/ci/ccache_utils.py#L101) (side notes - let's rename it to cache_helper? ccache is dead, long live the ccache)\r\n\r\n- It should respect `force tests` label.\r\n- The full list of files to get the digest from the repo is `./programs ./src ./contrib/*-cmake ./base ./cmake ./rust + **/CMakeLists.txt`\r\n- Instead of long running `git submodule status` we should calculate the instant `.git/modules/contrib/*/HEAD`. It mostly will cross with `**/CMakeLists.txt`, but well, it shouldn't make things worse.\r\n- `DOCKER_BUILDER_DIGEST` should be calculated out from `docker/packager/packager` + `docker_builder_tag`. Simple as is\r\n- Here is a tricky part. We don't actually know currently the state of the CI script. Sure, we can check the script itself, but it's unreliable. I think of something like https://github.com/thebjorn/pydeps to build the dependency tree. **5 minutes later** we'll calculate this nice thing `[k.__file__ for k in sys.modules.values() if hasattr(k, '__file__') and 'tests/ci' in k.__file__]`, but instead of `tests/ci` we should use a proper files' directory\r\n- The digest should also include the build_type. Like `package_release` etc.\r\n- And the main discussed thing. We won't change the scheme or build artifacts, but instead save the build report in `ccache/build-reports`. Because it will allow to have a lightweight \"symlink\" to the build results.\r\n\r\nI think, that's it",
  "created_at": "2023-11-03T20:23:02Z"
}