{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 9612,
  "instance_id": "ClickHouse__ClickHouse-9612",
  "issue_numbers": [
    "9132"
  ],
  "base_commit": "b4cf1410eddd28b849f096b5e9c5f512515e516b",
  "patch": "diff --git a/dbms/src/Storages/MergeTree/MergeTreeRangeReader.cpp b/dbms/src/Storages/MergeTree/MergeTreeRangeReader.cpp\nindex f5cf7ae622f7..0d032faa8199 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeRangeReader.cpp\n+++ b/dbms/src/Storages/MergeTree/MergeTreeRangeReader.cpp\n@@ -317,6 +317,9 @@ void MergeTreeRangeReader::ReadResult::optimize(bool can_read_incomplete_granule\n         }\n         num_rows_to_skip_in_last_granule += rows_per_granule_original.back() - rows_per_granule.back();\n \n+        filter_original = filter;\n+        filter_holder_original = std::move(filter_holder);\n+\n         /// Check if const 1 after shrink\n         if (countBytesInResultFilter(filter->getData()) + total_zero_rows_in_tails == total_rows_per_granule)\n         {\n@@ -333,12 +336,10 @@ void MergeTreeRangeReader::ReadResult::optimize(bool can_read_incomplete_granule\n             collapseZeroTails(filter->getData(), new_data);\n             total_rows_per_granule = new_filter->size();\n             num_rows = total_rows_per_granule;\n-            filter_original = filter;\n-            filter_holder_original = std::move(filter_holder);\n             filter = new_filter.get();\n             filter_holder = std::move(new_filter);\n-            need_filter = true;\n         }\n+        need_filter = true;\n     }\n     /// Another guess, if it's worth filtering at PREWHERE\n     else if (countBytesInResultFilter(filter->getData()) < 0.6 * filter->size())\n@@ -631,7 +632,7 @@ MergeTreeRangeReader::ReadResult MergeTreeRangeReader::read(size_t max_rows, Mar\n                 if (read_result.need_filter)\n                 {\n                     auto old_columns = block_before_prewhere.getColumns();\n-                    filterColumns(old_columns, read_result.getFilter()->getData());\n+                    filterColumns(old_columns, read_result.getFilterOriginal()->getData());\n                     block_before_prewhere.setColumns(std::move(old_columns));\n                 }\n \n@@ -875,7 +876,7 @@ void MergeTreeRangeReader::executePrewhereActionsAndFilterColumns(ReadResult & r\n         if (result.getFilter())\n         {\n             /// filter might be shrinked while columns not\n-            auto result_filter = result.getFilterOriginal() ? result.getFilterOriginal() : result.getFilter();\n+            auto result_filter = result.getFilterOriginal();\n             filterColumns(result.columns, result_filter->getData());\n             result.need_filter = true;\n \ndiff --git a/dbms/src/Storages/MergeTree/MergeTreeRangeReader.h b/dbms/src/Storages/MergeTree/MergeTreeRangeReader.h\nindex 4309076931da..affdb00147a4 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeRangeReader.h\n+++ b/dbms/src/Storages/MergeTree/MergeTreeRangeReader.h\n@@ -144,7 +144,7 @@ class MergeTreeRangeReader\n         /// The number of bytes read from disk.\n         size_t numBytesRead() const { return num_bytes_read; }\n         /// Filter you need to apply to newly-read columns in order to add them to block.\n-        const ColumnUInt8 * getFilterOriginal() const { return filter_original; }\n+        const ColumnUInt8 * getFilterOriginal() const { return filter_original ? filter_original : filter; }\n         const ColumnUInt8 * getFilter() const { return filter; }\n         ColumnPtr & getFilterHolder() { return filter_holder; }\n \n",
  "test_patch": "diff --git a/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.reference b/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.reference\nnew file mode 100644\nindex 000000000000..b4dfe343bbe6\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.reference\n@@ -0,0 +1,3 @@\n+foo\n+foo\n+foo\ndiff --git a/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.sql b/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.sql\nnew file mode 100644\nindex 000000000000..53eab0dc52ec\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.sql\n@@ -0,0 +1,17 @@\n+drop table if exists t;\n+\n+create table t (id UInt32, a Int) engine = MergeTree order by id;\n+\n+insert into t values (1, 0) (2, 1) (3, 0) (4, 0) (5, 0);\n+alter table t add column s String default 'foo';\n+select s from t prewhere a = 1;\n+\n+drop table t;\n+\n+create table t (id UInt32, a Int) engine = MergeTree order by id;\n+\n+insert into t values (1, 1) (2, 1) (3, 0) (4, 0) (5, 0);\n+alter table t add column s String default 'foo';\n+select s from t prewhere a = 1;\n+\n+drop table t;\n",
  "problem_statement": "20.1 Exception when aggregating a column that does not exist in every part\n20.1.4.14\r\nThe 'rcount' column was added to the table in November:\r\n```\r\nselect partition, name from system.parts_columns where column like 'rcount'\r\npartition | name          \r\n----------+---------------\r\n201911    | 201911_10_10_0\r\n201912    | 201912_4_4_0  \r\n202002    | 202002_0_0_0  \r\n202002    | 202002_1_1_0  \r\n```\r\nWhen CH needs to process older and newer parts together I get an exception or a segfault.\r\nA segfault example:\r\n```\r\n2020.02.14 18:31:19.455661 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Debug> executeQuery: (from [::ffff:10.253.59.13]:58671, user: default) SELECT sum(rcount)  FROM dsp_fact_event_shard PREWHERE (account_id IN (2276648,-1)) WHERE access_day >= '2019-02-02' FORMAT TabSeparatedWithNamesAndTypes;\r\n2020.02.14 18:31:19.456758 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Debug> dw.dsp_fact_event_shard (SelectExecutor): Key condition: unknown, unknown, and, unknown, and\r\n2020.02.14 18:31:19.456775 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Debug> dw.dsp_fact_event_shard (SelectExecutor): MinMax index condition: unknown, (column 0 in [17929, +inf)), and, unknown, and\r\n2020.02.14 18:31:19.456785 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Debug> dw.dsp_fact_event_shard (SelectExecutor): Selected 11 parts by date, 11 parts by key, 231 marks to read from 11 ranges\r\n2020.02.14 18:31:19.457860 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Trace> dw.dsp_fact_event_shard (SelectExecutor): Reading approx. 1892352 rows with 4 streams\r\n2020.02.14 18:31:19.458055 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.02.14 18:31:19.458146 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Debug> executeQuery: Query pipeline:\r\nExpression\r\n Expression\r\n  ParallelAggregating\r\n   Expression <C3><97> 4\r\n    Filter\r\n     MergeTreeThread\r\n\r\n2020.02.14 18:31:19.458182 [ 56 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Trace> ParallelAggregatingBlockInputStream: Aggregating\r\n2020.02.14 18:31:19.458726 [ 54 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Trace> Aggregator: Aggregation method: without_key\r\n2020.02.14 18:31:19.458752 [ 47 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Trace> Aggregator: Aggregation method: without_key\r\n2020.02.14 18:31:19.458774 [ 44 ] {a3f4a1cb-e769-4e2f-abeb-258e08b7556a} <Trace> Aggregator: Aggregation method: without_key\r\n2020.02.14 18:31:19.458935 [ 58 ] {} <Fatal> BaseDaemon: ########################################\r\n2020.02.14 18:31:19.458978 [ 58 ] {} <Fatal> BaseDaemon: (version 20.1.4.14 (official build)) (from thread 23) (query_id: a3f4a1cb-e769-4e2f-abeb-258e08b7556a) Received signal Segmentation fault (11).\r\n2020.02.14 18:31:19.458996 [ 58 ] {} <Fatal> BaseDaemon: Address: 0x18 Access: read. Address not mapped to object.\r\n2020.02.14 18:31:19.459013 [ 58 ] {} <Fatal> BaseDaemon: Stack trace: 0x8e549ae 0x97a6605 0x97a9d7f 0x976c2d6 0x976dad3 0x91a7ef7 0x91da0f4 0x91da313 0x8a6f42f 0x94a9a11 0x8a6f42f 0x949f368 0x8a6f42f 0x94ec720 0x94ecca5 0x4fa4fd7 0x4fa358f 0x7f5e7e871494 0x7f5e7e1a7acf\r\n2020.02.14 18:31:19.459067 [ 58 ] {} <Fatal> BaseDaemon: 3. 0x8e549ae DB::ColumnVector<int>::filter(DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, long) const  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459089 [ 58 ] {} <Fatal> BaseDaemon: 4. 0x97a6605 DB::MergeTreeRangeReader::filterColumns(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&, DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&) const  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459103 [ 58 ] {} <Fatal> BaseDaemon: 5. 0x97a9d7f DB::MergeTreeRangeReader::read(unsigned long, std::__1::vector<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459118 [ 58 ] {} <Fatal> BaseDaemon: 6. 0x976c2d6 DB::MergeTreeBaseSelectProcessor::readFromPartImpl()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459130 [ 58 ] {} <Fatal> BaseDaemon: 7. 0x976dad3 DB::MergeTreeBaseSelectProcessor::generate()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459142 [ 58 ] {} <Fatal> BaseDaemon: 8. 0x91a7ef7 DB::ISource::work()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459154 [ 58 ] {} <Fatal> BaseDaemon: 9. 0x91da0f4 DB::TreeExecutorBlockInputStream::execute()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459172 [ 58 ] {} <Fatal> BaseDaemon: 10. 0x91da313 DB::TreeExecutorBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459186 [ 58 ] {} <Fatal> BaseDaemon: 11. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459197 [ 58 ] {} <Fatal> BaseDaemon: 12. 0x94a9a11 DB::FilterBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459208 [ 58 ] {} <Fatal> BaseDaemon: 13. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459219 [ 58 ] {} <Fatal> BaseDaemon: 14. 0x949f368 DB::ExpressionBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459229 [ 58 ] {} <Fatal> BaseDaemon: 15. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459238 [ 58 ] {} <Fatal> BaseDaemon: 16. 0x94ec720 DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long)  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459257 [ 58 ] {} <Fatal> BaseDaemon: 17. 0x94ecca5 ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*&&)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*&&, std::__1::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::'lambda'()::operator()() const  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459275 [ 58 ] {} <Fatal> BaseDaemon: 18. 0x4fa4fd7 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459282 [ 58 ] {} <Fatal> BaseDaemon: 19. 0x4fa358f ?  in /usr/bin/clickhouse\r\n2020.02.14 18:31:19.459295 [ 58 ] {} <Fatal> BaseDaemon: 20. 0x7494 start_thread  in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n2020.02.14 18:31:19.459306 [ 58 ] {} <Fatal> BaseDaemon: 21. 0xe8acf clone  in /lib/x86_64-linux-gnu/libc-2.24.so\r\n```\r\nAn exception example:\r\n```\r\n2020.02.14 18:30:54.284219 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Debug> executeQuery: (from [::ffff:10.253.59.13]:58650, user: default) SELECT sum(rcount)  FROM dsp_fact_event_shard PREWHERE (account_id IN (2276648,-1)) WHERE access_day = '2019-02-02' FORMAT TabSeparatedWithNamesAndTypes;\r\n2020.02.14 18:30:54.285674 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Debug> dw.dsp_fact_event_shard (SelectExecutor): Key condition: unknown, unknown, and, unknown, and\r\n2020.02.14 18:30:54.285693 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Debug> dw.dsp_fact_event_shard (SelectExecutor): MinMax index condition: unknown, (column 0 in [17929, 17929]), and, unknown, and\r\n2020.02.14 18:30:54.285702 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Debug> dw.dsp_fact_event_shard (SelectExecutor): Selected 1 parts by date, 1 parts by key, 208 marks to read from 1 ranges\r\n2020.02.14 18:30:54.285918 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Trace> dw.dsp_fact_event_shard (SelectExecutor): Reading approx. 1703936 rows with 4 streams\r\n2020.02.14 18:30:54.286104 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.02.14 18:30:54.286196 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Debug> executeQuery: Query pipeline:\r\nExpression\r\n Expression\r\n  ParallelAggregating\r\n   Expression <C3><97> 4\r\n    Filter\r\n     MergeTreeThread\r\n\r\n2020.02.14 18:30:54.286232 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Trace> ParallelAggregatingBlockInputStream: Aggregating\r\n2020.02.14 18:30:54.286928 [ 44 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Trace> Aggregator: Aggregation method: without_key\r\n2020.02.14 18:30:54.287829 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Error> executeQuery: Code: 9, e.displayText() = DB::Exception: Size of filter doesn't match size of column. (version 20.1.4.14 (official build)) (from [::ffff:10.253.59.13]:58650) (in query: SELECT sum(rcount)  FROM dsp_fact_event_shard PREWHERE (account_id IN (2276648,-1)) WHERE access_day = '2019-02-02' FORMAT TabSeparatedWithNamesAndTypes;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. 0xbc34d6c Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n1. 0x4f6e129 DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n2. 0x8e54f4e DB::ColumnVector<int>::filter(DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, long) const  in /usr/bin/clickhouse\r\n3. 0x97a6605 DB::MergeTreeRangeReader::filterColumns(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&, DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&) const  in /usr/bin/clickhouse\r\n4. 0x97a9d7f DB::MergeTreeRangeReader::read(unsigned long, std::__1::vector<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)  in /usr/bin/clickhouse\r\n5. 0x976c2d6 DB::MergeTreeBaseSelectProcessor::readFromPartImpl()  in /usr/bin/clickhouse\r\n6. 0x976dad3 DB::MergeTreeBaseSelectProcessor::generate()  in /usr/bin/clickhouse\r\n7. 0x91a7ef7 DB::ISource::work()  in /usr/bin/clickhouse\r\n8. 0x91da0f4 DB::TreeExecutorBlockInputStream::execute()  in /usr/bin/clickhouse\r\n9. 0x91da313 DB::TreeExecutorBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n10. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n11. 0x94a9a11 DB::FilterBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n12. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n13. 0x949f368 DB::ExpressionBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n14. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n15. 0x94ec720 DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long)  in /usr/bin/clickhouse\r\n16. 0x94ecca5 ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*&&)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*&&, std::__1::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::'lambda'()::operator()() const  in /usr/bin/clickhouse\r\n17. 0x4fa4fd7 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)  in /usr/bin/clickhouse\r\n18. 0x4fa358f ?  in /usr/bin/clickhouse\r\n19. 0x7494 start_thread  in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n20. 0xe8acf clone  in /lib/x86_64-linux-gnu/libc-2.24.so\r\n\r\n2020.02.14 18:30:54.287965 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Debug> MemoryTracker: Peak memory usage (total): 649.07 KiB.\r\n2020.02.14 18:30:54.287992 [ 56 ] {97c6b0d3-c459-41f6-b7bd-f75caaa743bc} <Debug> MemoryTracker: Peak memory usage (for query): 4.64 MiB.\r\n2020.02.14 18:30:54.288018 [ 56 ] {} <Error> HTTPHandler: Code: 9, e.displayText() = DB::Exception: Size of filter doesn't match size of column., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. 0xbc34d6c Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n1. 0x4f6e129 DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n2. 0x8e54f4e DB::ColumnVector<int>::filter(DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, long) const  in /usr/bin/clickhouse\r\n3. 0x97a6605 DB::MergeTreeRangeReader::filterColumns(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&, DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&) const  in /usr/bin/clickhouse\r\n4. 0x97a9d7f DB::MergeTreeRangeReader::read(unsigned long, std::__1::vector<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)  in /usr/bin/clickhouse\r\n5. 0x976c2d6 DB::MergeTreeBaseSelectProcessor::readFromPartImpl()  in /usr/bin/clickhouse\r\n6. 0x976dad3 DB::MergeTreeBaseSelectProcessor::generate()  in /usr/bin/clickhouse\r\n7. 0x91a7ef7 DB::ISource::work()  in /usr/bin/clickhouse\r\n8. 0x91da0f4 DB::TreeExecutorBlockInputStream::execute()  in /usr/bin/clickhouse\r\n9. 0x91da313 DB::TreeExecutorBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n10. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n11. 0x94a9a11 DB::FilterBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n12. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n13. 0x949f368 DB::ExpressionBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n14. 0x8a6f42f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n15. 0x94ec720 DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long)  in /usr/bin/clickhouse\r\n16. 0x94ecca5 ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*&&)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*&&, std::__1::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::'lambda'()::operator()() const  in /usr/bin/clickhouse\r\n17. 0x4fa4fd7 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)  in /usr/bin/clickhouse\r\n18. 0x4fa358f ?  in /usr/bin/clickhouse\r\n19. 0x7494 start_thread  in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n20. 0xe8acf clone  in /lib/x86_64-linux-gnu/libc-2.24.so\r\n (version 20.1.4.14 (official build))\r\n```\r\nThere is no problem with this in 19.16.\n",
  "hints_text": "Same as #9064. Fixed in master\nThe segfault is fixed in 20.1.6.30, but the exception is still there:\r\n```\r\n2020.03.10 14:46:27.997915 [ 59 ] {} <Error> HTTPHandler: Code: 9, e.displayText() = DB::Exception: Size of filter doesn't match size of column., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. 0x100ac1bc Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n1. 0x8e74849 DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n2. 0xd35391e DB::ColumnVector<int>::filter(DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, long) const  in /usr/bin/clickhouse\r\n3. 0xd72ec75 DB::MergeTreeRangeReader::filterColumns(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&, DB::PODArray<unsigned char, 4096ul, Allocator<false, false>, 15ul, 16ul> const&) const  in /usr/bin/clickhouse\r\n4. 0xd7324ff DB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)  in /usr/bin/clickhouse\r\n5. 0xd7289c9 DB::MergeTreeBaseSelectProcessor::readFromPartImpl()  in /usr/bin/clickhouse\r\n6. 0xd72a243 DB::MergeTreeBaseSelectProcessor::generate()  in /usr/bin/clickhouse\r\n7. 0xd8688b7 DB::ISource::work()  in /usr/bin/clickhouse\r\n8. 0xdb59250 DB::SourceWithProgress::work()  in /usr/bin/clickhouse\r\n9. 0xd8b0f94 DB::TreeExecutorBlockInputStream::execute()  in /usr/bin/clickhouse\r\n10. 0xd8b11d3 DB::TreeExecutorBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n11. 0xcc1e47f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n12. 0xd029951 DB::FilterBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n13. 0xcc1e47f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n14. 0xd025ac8 DB::ExpressionBlockInputStream::readImpl()  in /usr/bin/clickhouse\r\n15. 0xcc1e47f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n16. 0xd0aa830 DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long)  in /usr/bin/clickhouse\r\n17. 0xd0aadb5 ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*&&)(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*&&, std::__1::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::'lambda'()::operator()() const  in /usr/bin/clickhouse\r\n18. 0x8e97347 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)  in /usr/bin/clickhouse\r\n19. 0x8e9580f ?  in /usr/bin/clickhouse\r\n20. 0x7494 start_thread  in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n21. 0xe8acf clone  in /lib/x86_64-linux-gnu/libc-2.24.so\r\n (version 20.1.6.30 (official build))\r\n```\n@CurtizJ I can provide the parts if needed\n> I can provide the parts if needed\r\n\r\nYes, it will be helpful.\n@CurtizJ Here are the parts\r\n[i9132.tar.gz](https://github.com/ClickHouse/ClickHouse/files/4320828/i9132.tar.gz)\r\n```\r\nATTACH TABLE test.i9132 \r\n(\r\n  `access_time`   DateTime,\r\n  `access_day`    Date DEFAULT toDate (access_time),\r\n  `time_key`      Int32,\r\n  `seller_key`    Int32,\r\n  `cmp_key`       Int32,\r\n  `rcount`        Int32 DEFAULT toInt32 (1)\r\n)\r\nENGINE = MergeTree () PARTITION BY toYYYYMM (access_day)\r\nORDER BY (time_key,seller_key,cmp_key) SETTINGS index_granularity_bytes = 0;\r\n\r\nSELECT sum(rcount)\r\nFROM test.i9132\r\nPREWHERE seller_key IN (2350462);\r\n\r\n\u250c\u2500sum(rcount)\u2500\u2510\r\n\u2502      955714 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec. Processed 955.73 thousand rows, 5.54 MB (316.78 million rows/s., 1.84 GB/s.)\r\n\r\nSELECT sum(rcount)\r\nFROM test.i9132\r\nPREWHERE seller_key IN (-1);\r\n\r\nReceived exception from server (version 20.1.6):\r\nCode: 9. DB::Exception: Received from localhost:9000. DB::Exception: Size of filter doesn't match size of column..\r\n```\r\nSometimes I also get another exception, but I couldn't reproduce it on these parts:\r\nDB::Exception: Invalid number of rows in Chunk column Int32: expected 13, got 65536\n> Sometimes I also get another exception, but I couldn't reproduce it on these parts:\r\n> DB::Exception: Invalid number of rows in Chunk column Int32: expected 13, got 65536\r\n\r\n#9580",
  "created_at": "2020-03-12T00:45:06Z",
  "modified_files": [
    "dbms/src/Storages/MergeTree/MergeTreeRangeReader.cpp",
    "dbms/src/Storages/MergeTree/MergeTreeRangeReader.h"
  ],
  "modified_test_files": [
    "b/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.reference",
    "b/dbms/tests/queries/0_stateless/01097_one_more_range_reader_test.sql"
  ]
}