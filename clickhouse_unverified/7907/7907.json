{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 7907,
  "instance_id": "ClickHouse__ClickHouse-7907",
  "issue_numbers": [
    "5186"
  ],
  "base_commit": "7c59fd4795397439aa8ff47cbe5fabd36f425ea8",
  "patch": "diff --git a/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex 25d2c0ac2dc9..39e8c3fe1cda 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -388,18 +388,18 @@ Pipes MergeTreeDataSelectExecutor::readFromParts(\n             used_sample_factor = 1.0 / boost::rational_cast<Float64>(relative_sample_size);\n \n         RelativeSize size_of_universum = 0;\n-        DataTypePtr type = data.primary_key_sample.getByName(data.sampling_expr_column_name).type;\n+        DataTypePtr sampling_column_type = data.primary_key_sample.getByName(data.sampling_expr_column_name).type;\n \n-        if (typeid_cast<const DataTypeUInt64 *>(type.get()))\n+        if (typeid_cast<const DataTypeUInt64 *>(sampling_column_type.get()))\n             size_of_universum = RelativeSize(std::numeric_limits<UInt64>::max()) + RelativeSize(1);\n-        else if (typeid_cast<const DataTypeUInt32 *>(type.get()))\n+        else if (typeid_cast<const DataTypeUInt32 *>(sampling_column_type.get()))\n             size_of_universum = RelativeSize(std::numeric_limits<UInt32>::max()) + RelativeSize(1);\n-        else if (typeid_cast<const DataTypeUInt16 *>(type.get()))\n+        else if (typeid_cast<const DataTypeUInt16 *>(sampling_column_type.get()))\n             size_of_universum = RelativeSize(std::numeric_limits<UInt16>::max()) + RelativeSize(1);\n-        else if (typeid_cast<const DataTypeUInt8 *>(type.get()))\n+        else if (typeid_cast<const DataTypeUInt8 *>(sampling_column_type.get()))\n             size_of_universum = RelativeSize(std::numeric_limits<UInt8>::max()) + RelativeSize(1);\n         else\n-            throw Exception(\"Invalid sampling column type in storage parameters: \" + type->getName() + \". Must be unsigned integer type.\",\n+            throw Exception(\"Invalid sampling column type in storage parameters: \" + sampling_column_type->getName() + \". Must be unsigned integer type.\",\n                 ErrorCodes::ILLEGAL_TYPE_OF_COLUMN_FOR_FILTER);\n \n         if (settings.parallel_replicas_count > 1)\n@@ -453,13 +453,25 @@ Pipes MergeTreeDataSelectExecutor::readFromParts(\n             std::shared_ptr<ASTFunction> lower_function;\n             std::shared_ptr<ASTFunction> upper_function;\n \n+            /// If sample and final are used together no need to calculate sampling expression twice.\n+            /// The first time it was calculated for final, because sample key is a part of the PK.\n+            /// So, assume that we already have calculated column.\n+            ASTPtr sampling_key_ast = data.getSamplingKeyAST();\n+            if (select.final())\n+            {\n+                sampling_key_ast = std::make_shared<ASTIdentifier>(data.sampling_expr_column_name);\n+\n+                /// We do spoil available_real_columns here, but it is not used later.\n+                available_real_columns.emplace_back(data.sampling_expr_column_name, std::move(sampling_column_type));\n+            }\n+\n             if (has_lower_limit)\n             {\n                 if (!key_condition.addCondition(data.sampling_expr_column_name, Range::createLeftBounded(lower, true)))\n                     throw Exception(\"Sampling column not in primary key\", ErrorCodes::ILLEGAL_COLUMN);\n \n                 ASTPtr args = std::make_shared<ASTExpressionList>();\n-                args->children.push_back(data.getSamplingKeyAST());\n+                args->children.push_back(sampling_key_ast);\n                 args->children.push_back(std::make_shared<ASTLiteral>(lower));\n \n                 lower_function = std::make_shared<ASTFunction>();\n@@ -476,7 +488,7 @@ Pipes MergeTreeDataSelectExecutor::readFromParts(\n                     throw Exception(\"Sampling column not in primary key\", ErrorCodes::ILLEGAL_COLUMN);\n \n                 ASTPtr args = std::make_shared<ASTExpressionList>();\n-                args->children.push_back(data.getSamplingKeyAST());\n+                args->children.push_back(sampling_key_ast);\n                 args->children.push_back(std::make_shared<ASTLiteral>(upper));\n \n                 upper_function = std::make_shared<ASTFunction>();\n@@ -503,11 +515,16 @@ Pipes MergeTreeDataSelectExecutor::readFromParts(\n             auto syntax_result = SyntaxAnalyzer(context).analyze(query, available_real_columns);\n             filter_expression = ExpressionAnalyzer(filter_function, syntax_result, context).getActions(false);\n \n-            /// Add columns needed for `sample_by_ast` to `column_names_to_read`.\n-            std::vector<String> add_columns = filter_expression->getRequiredColumns();\n-            column_names_to_read.insert(column_names_to_read.end(), add_columns.begin(), add_columns.end());\n-            std::sort(column_names_to_read.begin(), column_names_to_read.end());\n-            column_names_to_read.erase(std::unique(column_names_to_read.begin(), column_names_to_read.end()), column_names_to_read.end());\n+            if (!select.final())\n+            {\n+                /// Add columns needed for `sample_by_ast` to `column_names_to_read`.\n+                /// Skip this if final was used, because such columns were already added from PK.\n+                std::vector<String> add_columns = filter_expression->getRequiredColumns();\n+                column_names_to_read.insert(column_names_to_read.end(), add_columns.begin(), add_columns.end());\n+                std::sort(column_names_to_read.begin(), column_names_to_read.end());\n+                column_names_to_read.erase(std::unique(column_names_to_read.begin(), column_names_to_read.end()),\n+                                           column_names_to_read.end());\n+            }\n         }\n     }\n \n",
  "test_patch": "diff --git a/dbms/tests/queries/0_stateless/01034_sample_final.reference b/dbms/tests/queries/0_stateless/01034_sample_final.reference\nnew file mode 100644\nindex 000000000000..bbb327295f3e\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01034_sample_final.reference\n@@ -0,0 +1,10 @@\n+count\n+1000000\n+count final\n+666667\n+count sample\n+557632\n+count sample final\n+371758\n+count final max_parallel_replicas\n+666667\ndiff --git a/dbms/tests/queries/0_stateless/01034_sample_final.sql b/dbms/tests/queries/0_stateless/01034_sample_final.sql\nnew file mode 100644\nindex 000000000000..ca03daebe122\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01034_sample_final.sql\n@@ -0,0 +1,17 @@\n+drop table if exists sample_final;\n+create table sample_final (CounterID UInt32, EventDate Date, EventTime DateTime, UserID UInt64, Sign Int8) engine = CollapsingMergeTree(Sign) order by (CounterID, EventDate, intHash32(UserID), EventTime) sample by intHash32(UserID);\n+insert into sample_final select number / (8192 * 4), toDate('2019-01-01'), toDateTime('2019-01-01 00:00:01') + number, number / (8192 * 2), number % 3 = 1 ? -1 : 1 from numbers(1000000);\n+\n+select 'count';\n+select count() from sample_final;\n+select 'count final';\n+select count() from sample_final final;\n+select 'count sample';\n+select count() from sample_final sample 1/2;\n+select 'count sample final';\n+select count() from sample_final final sample 1/2;\n+select 'count final max_parallel_replicas';\n+set max_parallel_replicas=2;\n+select count() from remote('127.0.0.{2|3}', currentDatabase(), sample_final) final;\n+\n+drop table if exists sample_final;\n",
  "problem_statement": "Column already exists with FINAL queries to distributed tables with max_parallel_replicas > 1\nLet's say that I have a cluster with one shard and two replicas. The setting `max_parallel_replicas` is set to 2.\r\n\r\nOn each of the replicas I create the following tables:\r\n\r\n```sql\r\nCREATE TABLE test (\r\n  sequenceId  String,\r\n  id          String,\r\n  dateTime    DateTime('UTC'),\r\n  version     UInt64\r\n) ENGINE ReplicatedReplacingMergeTree('/shard/{shard}', '{replica}', version)\r\nPARTITION BY toYYYYMM(dateTime)\r\nORDER BY (toDate(dateTime), cityHash64(sequenceId), id)\r\nPRIMARY KEY (toDate(dateTime), cityHash64(sequenceId))\r\nSAMPLE BY cityHash64(sequenceId);\r\n\r\nCREATE TABLE IF NOT EXISTS test_d AS test ENGINE = Distributed('test-cluster', 'default', test, sipHash64(sequenceId));\r\n```\r\n\r\nand insert some values:\r\n\r\n```sql\r\nINSERT INTO test_d (sequenceId, id, dateTime, version) VALUES (\r\n  'a3892197-3ad7-37d4-a1cf-2053b33fc287',\r\n  'f0d88bf6-b258-4a37-b15e-c68a7c1ec258',\r\n  toDateTime('2019-01-01 00:00:00'),\r\n  1\r\n), (\r\n  'a3892197-3ad7-37d4-a1cf-2053b33fc287',\r\n  'f0d88bf6-b258-4a37-b15e-c68a7c1ec258',\r\n  toDateTime('2019-01-01 00:00:00'),\r\n  2\r\n)\r\n```\r\n\r\nThen I try to query counts from the local table and this works fine:\r\n```sql\r\n:) select count() from test\r\n                                                                                                                                              \r\nSELECT count()                                                                                                                                \r\nFROM test\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.208 sec.\r\n```\r\n```sql\r\n:) select count() from test final\r\n\r\nSELECT count()\r\nFROM test\r\nFINAL\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.213 sec.\r\n```\r\nHowever, when I try to do the same with the distributed table, the `FINAL` query doesn't work:\r\n```sql\r\n :) select count() from test_d\r\n\r\nSELECT count()\r\nFROM test_d\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.238 sec.\r\n```\r\n```sql\r\n :) select count() from test_d final\r\n\r\nSELECT count()\r\nFROM test_d\r\nFINAL\r\n\r\nReceived exception from server (version 19.1.6):\r\nCode: 15. DB::Exception: Received from clickhouse-2:9000, 192.168.0.251. DB::Exception: Received from clickhouse-2:9000, 192.168.0.251. DB::Exception: Column 'cityHash64(sequenceId)' already exists.\r\n\r\n0 rows in set. Elapsed: 0.208 sec. \r\n```\r\n\r\nHowever, when I set `max_parallel_replicas` to 1 it works fine, but doesn't use both replicas for processing.\n",
  "hints_text": "Be aware that `FROM FINAL` is not usable with the real FACT table (which needs max_parallel_replicas ). In case of billions rows FINAL is super slow.",
  "created_at": "2019-11-25T13:16:32Z",
  "modified_files": [
    "dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp"
  ],
  "modified_test_files": [
    "b/dbms/tests/queries/0_stateless/01034_sample_final.reference",
    "b/dbms/tests/queries/0_stateless/01034_sample_final.sql"
  ]
}