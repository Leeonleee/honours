{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 69828,
  "instance_id": "ClickHouse__ClickHouse-69828",
  "issue_numbers": [
    "65397"
  ],
  "base_commit": "95f0ca28d4cde06e57f9bb4d3d619aa586837ca6",
  "patch": "diff --git a/src/Core/Block.cpp b/src/Core/Block.cpp\nindex d560cb2c1053..c7e0e9b7b377 100644\n--- a/src/Core/Block.cpp\n+++ b/src/Core/Block.cpp\n@@ -818,6 +818,23 @@ Serializations Block::getSerializations() const\n     return res;\n }\n \n+Serializations Block::getSerializations(const SerializationInfoByName & hints) const\n+{\n+    Serializations res;\n+    res.reserve(data.size());\n+\n+    for (const auto & column : data)\n+    {\n+        auto it = hints.find(column.name);\n+        if (it == hints.end())\n+            res.push_back(column.type->getDefaultSerialization());\n+        else\n+            res.push_back(column.type->getSerialization(*it->second));\n+    }\n+\n+    return res;\n+}\n+\n void convertToFullIfSparse(Block & block)\n {\n     for (auto & column : block)\ndiff --git a/src/Core/Block.h b/src/Core/Block.h\nindex d998581a50fe..841fb3fb6639 100644\n--- a/src/Core/Block.h\n+++ b/src/Core/Block.h\n@@ -10,6 +10,7 @@\n #include <set>\n #include <vector>\n #include <sparsehash/dense_hash_map>\n+#include <DataTypes/Serializations/SerializationInfo.h>\n \n \n namespace DB\n@@ -99,6 +100,7 @@ class Block\n     NameMap getNamesToIndexesMap() const;\n \n     Serializations getSerializations() const;\n+    Serializations getSerializations(const SerializationInfoByName & hints) const;\n \n     /// Returns number of rows from first column in block, not equal to nullptr. If no columns, returns 0.\n     size_t rows() const;\ndiff --git a/src/Core/Settings.cpp b/src/Core/Settings.cpp\nindex a09a0b8375fe..35579a520f63 100644\n--- a/src/Core/Settings.cpp\n+++ b/src/Core/Settings.cpp\n@@ -242,6 +242,7 @@ namespace ErrorCodes\n     M(Bool, output_format_parallel_formatting, true, \"Enable parallel formatting for some data formats.\", 0) \\\n     M(UInt64, output_format_compression_level, 3, \"Default compression level if query output is compressed. The setting is applied when `SELECT` query has `INTO OUTFILE` or when inserting to table function `file`, `url`, `hdfs`, `s3`, and `azureBlobStorage`.\", 0) \\\n     M(UInt64, output_format_compression_zstd_window_log, 0, \"Can be used when the output compression method is `zstd`. If greater than `0`, this setting explicitly sets compression window size (power of `2`) and enables a long-range mode for zstd compression.\", 0) \\\n+    M(Bool, enable_parsing_to_custom_serialization, true, \"If true then data can be parsed directly to columns with custom serialization (e.g. Sparse) according to hints for serialization got from the table.\", 0) \\\n     \\\n     M(UInt64, merge_tree_min_rows_for_concurrent_read, (20 * 8192), \"If at least as many lines are read from one file, the reading can be parallelized.\", 0) \\\n     M(UInt64, merge_tree_min_bytes_for_concurrent_read, (24 * 10 * 1024 * 1024), \"If at least as many bytes are read from one file, the reading can be parallelized.\", 0) \\\ndiff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp\nindex 8b9dfc91e3e6..6408a989ee14 100644\n--- a/src/Core/SettingsChangesHistory.cpp\n+++ b/src/Core/SettingsChangesHistory.cpp\n@@ -67,6 +67,7 @@ static std::initializer_list<std::pair<ClickHouseVersion, SettingsChangesHistory\n     },\n     {\"24.10\",\n         {\n+            {\"enable_parsing_to_custom_serialization\", false, true, \"New setting\"},\n             {\"mongodb_throw_on_unsupported_query\", false, true, \"New setting.\"},\n         }\n     },\ndiff --git a/src/DataTypes/Serializations/SerializationInfo.cpp b/src/DataTypes/Serializations/SerializationInfo.cpp\nindex 7d5c456af7f3..df9d27d4ca28 100644\n--- a/src/DataTypes/Serializations/SerializationInfo.cpp\n+++ b/src/DataTypes/Serializations/SerializationInfo.cpp\n@@ -47,6 +47,12 @@ void SerializationInfo::Data::add(const Data & other)\n     num_defaults += other.num_defaults;\n }\n \n+void SerializationInfo::Data::remove(const Data & other)\n+{\n+    num_rows -= other.num_rows;\n+    num_defaults -= other.num_defaults;\n+}\n+\n void SerializationInfo::Data::addDefaults(size_t length)\n {\n     num_rows += length;\n@@ -80,6 +86,14 @@ void SerializationInfo::add(const SerializationInfo & other)\n         kind = chooseKind(data, settings);\n }\n \n+void SerializationInfo::remove(const SerializationInfo & other)\n+{\n+    data.remove(other.data);\n+    if (settings.choose_kind)\n+        kind = chooseKind(data, settings);\n+}\n+\n+\n void SerializationInfo::addDefaults(size_t length)\n {\n     data.addDefaults(length);\n@@ -202,13 +216,37 @@ void SerializationInfoByName::add(const Block & block)\n void SerializationInfoByName::add(const SerializationInfoByName & other)\n {\n     for (const auto & [name, info] : other)\n-    {\n-        auto it = find(name);\n-        if (it == end())\n-            continue;\n+        add(name, *info);\n+}\n \n-        it->second->add(*info);\n-    }\n+void SerializationInfoByName::add(const String & name, const SerializationInfo & info)\n+{\n+    if (auto it = find(name); it != end())\n+        it->second->add(info);\n+}\n+\n+void SerializationInfoByName::remove(const SerializationInfoByName & other)\n+{\n+    for (const auto & [name, info] : other)\n+        remove(name, *info);\n+}\n+\n+void SerializationInfoByName::remove(const String & name, const SerializationInfo & info)\n+{\n+    if (auto it = find(name); it != end())\n+        it->second->remove(info);\n+}\n+\n+SerializationInfoPtr SerializationInfoByName::tryGet(const String & name) const\n+{\n+    auto it = find(name);\n+    return it == end() ? nullptr : it->second;\n+}\n+\n+MutableSerializationInfoPtr SerializationInfoByName::tryGet(const String & name)\n+{\n+    auto it = find(name);\n+    return it == end() ? nullptr : it->second;\n }\n \n void SerializationInfoByName::replaceData(const SerializationInfoByName & other)\n@@ -224,6 +262,12 @@ void SerializationInfoByName::replaceData(const SerializationInfoByName & other)\n     }\n }\n \n+ISerialization::Kind SerializationInfoByName::getKind(const String & column_name) const\n+{\n+    auto it = find(column_name);\n+    return it != end() ? it->second->getKind() : ISerialization::Kind::DEFAULT;\n+}\n+\n void SerializationInfoByName::writeJSON(WriteBuffer & out) const\n {\n     Poco::JSON::Object object;\ndiff --git a/src/DataTypes/Serializations/SerializationInfo.h b/src/DataTypes/Serializations/SerializationInfo.h\nindex 5a900a5521cd..c30e50ab12c8 100644\n--- a/src/DataTypes/Serializations/SerializationInfo.h\n+++ b/src/DataTypes/Serializations/SerializationInfo.h\n@@ -39,6 +39,7 @@ class SerializationInfo\n \n         void add(const IColumn & column);\n         void add(const Data & other);\n+        void remove(const Data & other);\n         void addDefaults(size_t length);\n     };\n \n@@ -52,6 +53,7 @@ class SerializationInfo\n \n     virtual void add(const IColumn & column);\n     virtual void add(const SerializationInfo & other);\n+    virtual void remove(const SerializationInfo & other);\n     virtual void addDefaults(size_t length);\n     virtual void replaceData(const SerializationInfo & other);\n \n@@ -99,6 +101,14 @@ class SerializationInfoByName : public std::map<String, MutableSerializationInfo\n \n     void add(const Block & block);\n     void add(const SerializationInfoByName & other);\n+    void add(const String & name, const SerializationInfo & info);\n+\n+    void remove(const SerializationInfoByName & other);\n+    void remove(const String & name, const SerializationInfo & info);\n+\n+    SerializationInfoPtr tryGet(const String & name) const;\n+    MutableSerializationInfoPtr tryGet(const String & name);\n+    ISerialization::Kind getKind(const String & column_name) const;\n \n     /// Takes data from @other, but keeps current serialization kinds.\n     /// If column exists in @other infos, but not in current infos,\ndiff --git a/src/DataTypes/Serializations/SerializationInfoTuple.cpp b/src/DataTypes/Serializations/SerializationInfoTuple.cpp\nindex cd65b865248a..b7449be3cc53 100644\n--- a/src/DataTypes/Serializations/SerializationInfoTuple.cpp\n+++ b/src/DataTypes/Serializations/SerializationInfoTuple.cpp\n@@ -10,6 +10,7 @@ namespace ErrorCodes\n {\n     extern const int CORRUPTED_DATA;\n     extern const int THERE_IS_NO_COLUMN;\n+    extern const int NOT_IMPLEMENTED;\n }\n \n SerializationInfoTuple::SerializationInfoTuple(\n@@ -68,6 +69,19 @@ void SerializationInfoTuple::add(const SerializationInfo & other)\n     }\n }\n \n+void SerializationInfoTuple::remove(const SerializationInfo & other)\n+{\n+    if (!structureEquals(other))\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Cannot remove from serialization info different structure\");\n+\n+    SerializationInfo::remove(other);\n+    const auto & other_elems = assert_cast<const SerializationInfoTuple &>(other).elems;\n+    chassert(elems.size() == other_elems.size());\n+\n+    for (size_t i = 0; i < elems.size(); ++i)\n+        elems[i]->remove(*other_elems[i]);\n+}\n+\n void SerializationInfoTuple::addDefaults(size_t length)\n {\n     SerializationInfo::addDefaults(length);\ndiff --git a/src/DataTypes/Serializations/SerializationInfoTuple.h b/src/DataTypes/Serializations/SerializationInfoTuple.h\nindex a9f3bdb6c6e7..a6b9c89166fb 100644\n--- a/src/DataTypes/Serializations/SerializationInfoTuple.h\n+++ b/src/DataTypes/Serializations/SerializationInfoTuple.h\n@@ -15,6 +15,7 @@ class SerializationInfoTuple : public SerializationInfo\n \n     void add(const IColumn & column) override;\n     void add(const SerializationInfo & other) override;\n+    void remove(const SerializationInfo & other) override;\n     void addDefaults(size_t length) override;\n     void replaceData(const SerializationInfo & other) override;\n \ndiff --git a/src/DataTypes/Serializations/SerializationSparse.cpp b/src/DataTypes/Serializations/SerializationSparse.cpp\nindex 73488d308bb3..327d1f23ccaf 100644\n--- a/src/DataTypes/Serializations/SerializationSparse.cpp\n+++ b/src/DataTypes/Serializations/SerializationSparse.cpp\n@@ -13,7 +13,6 @@ namespace DB\n \n namespace ErrorCodes\n {\n-    extern const int NOT_IMPLEMENTED;\n     extern const int LOGICAL_ERROR;\n }\n \n@@ -313,15 +312,35 @@ void SerializationSparse::deserializeBinary(Field & field, ReadBuffer & istr, co\n     nested->deserializeBinary(field, istr, settings);\n }\n \n+template <typename Reader>\n+void SerializationSparse::deserialize(IColumn & column, Reader && reader) const\n+{\n+    auto & column_sparse = assert_cast<ColumnSparse &>(column);\n+    auto & values = column_sparse.getValuesColumn();\n+    size_t old_size = column_sparse.size();\n+\n+    /// It just increments the size of column.\n+    column_sparse.insertDefault();\n+    reader(column_sparse.getValuesColumn());\n+\n+    if (values.isDefaultAt(values.size() - 1))\n+        values.popBack(1);\n+    else\n+        column_sparse.getOffsetsData().push_back(old_size);\n+}\n+\n void SerializationSparse::serializeBinary(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\n {\n     const auto & column_sparse = assert_cast<const ColumnSparse &>(column);\n     nested->serializeBinary(column_sparse.getValuesColumn(), column_sparse.getValueIndex(row_num), ostr, settings);\n }\n \n-void SerializationSparse::deserializeBinary(IColumn &, ReadBuffer &, const FormatSettings &) const\n+void SerializationSparse::deserializeBinary(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const\n {\n-    throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'deserializeBinary' is not implemented for SerializationSparse\");\n+    deserialize(column, [&](auto & nested_column)\n+    {\n+        nested->deserializeBinary(nested_column, istr, settings);\n+    });\n }\n \n void SerializationSparse::serializeTextEscaped(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\n@@ -330,9 +349,12 @@ void SerializationSparse::serializeTextEscaped(const IColumn & column, size_t ro\n     nested->serializeTextEscaped(column_sparse.getValuesColumn(), column_sparse.getValueIndex(row_num), ostr, settings);\n }\n \n-void SerializationSparse::deserializeTextEscaped(IColumn &, ReadBuffer &, const FormatSettings &) const\n+void SerializationSparse::deserializeTextEscaped(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const\n {\n-    throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'deserializeTextEscaped' is not implemented for SerializationSparse\");\n+    deserialize(column, [&](auto & nested_column)\n+    {\n+        nested->deserializeTextEscaped(nested_column, istr, settings);\n+    });\n }\n \n void SerializationSparse::serializeTextQuoted(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\n@@ -341,9 +363,12 @@ void SerializationSparse::serializeTextQuoted(const IColumn & column, size_t row\n     nested->serializeTextQuoted(column_sparse.getValuesColumn(), column_sparse.getValueIndex(row_num), ostr, settings);\n }\n \n-void SerializationSparse::deserializeTextQuoted(IColumn &, ReadBuffer &, const FormatSettings &) const\n+void SerializationSparse::deserializeTextQuoted(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const\n {\n-    throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'deserializeTextQuoted' is not implemented for SerializationSparse\");\n+    deserialize(column, [&](auto & nested_column)\n+    {\n+        nested->deserializeTextQuoted(nested_column, istr, settings);\n+    });\n }\n \n void SerializationSparse::serializeTextCSV(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\n@@ -352,9 +377,12 @@ void SerializationSparse::serializeTextCSV(const IColumn & column, size_t row_nu\n     nested->serializeTextCSV(column_sparse.getValuesColumn(), column_sparse.getValueIndex(row_num), ostr, settings);\n }\n \n-void SerializationSparse::deserializeTextCSV(IColumn &, ReadBuffer &, const FormatSettings &) const\n+void SerializationSparse::deserializeTextCSV(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const\n {\n-    throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'deserializeTextCSV' is not implemented for SerializationSparse\");\n+    deserialize(column, [&](auto & nested_column)\n+    {\n+        nested->deserializeTextCSV(nested_column, istr, settings);\n+    });\n }\n \n void SerializationSparse::serializeText(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\n@@ -363,9 +391,12 @@ void SerializationSparse::serializeText(const IColumn & column, size_t row_num,\n     nested->serializeText(column_sparse.getValuesColumn(), column_sparse.getValueIndex(row_num), ostr, settings);\n }\n \n-void SerializationSparse::deserializeWholeText(IColumn &, ReadBuffer &, const FormatSettings &) const\n+void SerializationSparse::deserializeWholeText(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const\n {\n-    throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'deserializeWholeText' is not implemented for SerializationSparse\");\n+    deserialize(column, [&](auto & nested_column)\n+    {\n+        nested->deserializeWholeText(nested_column, istr, settings);\n+    });\n }\n \n void SerializationSparse::serializeTextJSON(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\n@@ -374,9 +405,12 @@ void SerializationSparse::serializeTextJSON(const IColumn & column, size_t row_n\n     nested->serializeTextJSON(column_sparse.getValuesColumn(), column_sparse.getValueIndex(row_num), ostr, settings);\n }\n \n-void SerializationSparse::deserializeTextJSON(IColumn &, ReadBuffer &, const FormatSettings &) const\n+void SerializationSparse::deserializeTextJSON(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const\n {\n-    throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'deserializeTextJSON' is not implemented for SerializationSparse\");\n+    deserialize(column, [&](auto & nested_column)\n+    {\n+        nested->deserializeTextJSON(nested_column, istr, settings);\n+    });\n }\n \n void SerializationSparse::serializeTextXML(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\ndiff --git a/src/DataTypes/Serializations/SerializationSparse.h b/src/DataTypes/Serializations/SerializationSparse.h\nindex a55856bacf0e..b31a006639b8 100644\n--- a/src/DataTypes/Serializations/SerializationSparse.h\n+++ b/src/DataTypes/Serializations/SerializationSparse.h\n@@ -99,6 +99,9 @@ class SerializationSparse final : public ISerialization\n         ColumnPtr create(const ColumnPtr & prev) const override;\n     };\n \n+    template <typename Reader>\n+    void deserialize(IColumn & column, Reader && reader) const;\n+\n     SerializationPtr nested;\n };\n \ndiff --git a/src/Functions/materialize.cpp b/src/Functions/materialize.cpp\nindex 5cef610b60ab..e8a43dfc820f 100644\n--- a/src/Functions/materialize.cpp\n+++ b/src/Functions/materialize.cpp\n@@ -7,7 +7,7 @@ namespace DB\n \n REGISTER_FUNCTION(Materialize)\n {\n-    factory.registerFunction<FunctionMaterialize>();\n+    factory.registerFunction<FunctionMaterialize<true>>();\n }\n \n }\ndiff --git a/src/Functions/materialize.h b/src/Functions/materialize.h\nindex 571391faba79..ac4a01d875e4 100644\n--- a/src/Functions/materialize.h\n+++ b/src/Functions/materialize.h\n@@ -9,13 +9,14 @@ namespace DB\n \n /** materialize(x) - materialize the constant\n   */\n+template <bool remove_sparse>\n class FunctionMaterialize : public IFunction\n {\n public:\n     static constexpr auto name = \"materialize\";\n     static FunctionPtr create(ContextPtr)\n     {\n-        return std::make_shared<FunctionMaterialize>();\n+        return std::make_shared<FunctionMaterialize<remove_sparse>>();\n     }\n \n     /// Get the function name.\n@@ -55,7 +56,10 @@ class FunctionMaterialize : public IFunction\n \n     ColumnPtr executeImpl(const ColumnsWithTypeAndName & arguments, const DataTypePtr &, size_t /*input_rows_count*/) const override\n     {\n-        return recursiveRemoveSparse(arguments[0].column->convertToFullColumnIfConst());\n+        auto res = arguments[0].column->convertToFullColumnIfConst();\n+        if constexpr (remove_sparse)\n+            res = recursiveRemoveSparse(res);\n+        return res;\n     }\n \n     bool hasInformationAboutMonotonicity() const override { return true; }\ndiff --git a/src/Interpreters/ActionsDAG.cpp b/src/Interpreters/ActionsDAG.cpp\nindex 44862344df00..5d99a63e2c4a 100644\n--- a/src/Interpreters/ActionsDAG.cpp\n+++ b/src/Interpreters/ActionsDAG.cpp\n@@ -1433,16 +1433,21 @@ bool ActionsDAG::hasNonDeterministic() const\n     return false;\n }\n \n-void ActionsDAG::addMaterializingOutputActions()\n+void ActionsDAG::addMaterializingOutputActions(bool materialize_sparse)\n {\n     for (auto & output_node : outputs)\n-        output_node = &materializeNode(*output_node);\n+        output_node = &materializeNode(*output_node, materialize_sparse);\n }\n \n-const ActionsDAG::Node & ActionsDAG::materializeNode(const Node & node)\n+const ActionsDAG::Node & ActionsDAG::materializeNode(const Node & node, bool materialize_sparse)\n {\n-    FunctionOverloadResolverPtr func_builder_materialize\n-        = std::make_unique<FunctionToOverloadResolverAdaptor>(std::make_shared<FunctionMaterialize>());\n+    FunctionPtr func_materialize;\n+    if (materialize_sparse)\n+        func_materialize = std::make_shared<FunctionMaterialize<true>>();\n+    else\n+        func_materialize = std::make_shared<FunctionMaterialize<false>>();\n+\n+    FunctionOverloadResolverPtr func_builder_materialize = std::make_unique<FunctionToOverloadResolverAdaptor>(std::move(func_materialize));\n \n     const auto & name = node.result_name;\n     const auto * func = &addFunction(func_builder_materialize, {&node}, {});\n@@ -1469,7 +1474,7 @@ ActionsDAG ActionsDAG::makeConvertingActions(\n     ActionsDAG actions_dag(source);\n     NodeRawConstPtrs projection(num_result_columns);\n \n-    FunctionOverloadResolverPtr func_builder_materialize = std::make_unique<FunctionToOverloadResolverAdaptor>(std::make_shared<FunctionMaterialize>());\n+    FunctionOverloadResolverPtr func_builder_materialize = std::make_unique<FunctionToOverloadResolverAdaptor>(std::make_shared<FunctionMaterialize<false>>());\n \n     std::unordered_map<std::string_view, std::list<size_t>> inputs;\n     if (mode == MatchColumnsMode::Name)\n@@ -1596,7 +1601,7 @@ ActionsDAG ActionsDAG::makeAddingColumnActions(ColumnWithTypeAndName column)\n {\n     ActionsDAG adding_column_action;\n     FunctionOverloadResolverPtr func_builder_materialize\n-        = std::make_unique<FunctionToOverloadResolverAdaptor>(std::make_shared<FunctionMaterialize>());\n+        = std::make_unique<FunctionToOverloadResolverAdaptor>(std::make_shared<FunctionMaterialize<true>>());\n \n     auto column_name = column.name;\n     const auto * column_node = &adding_column_action.addColumn(std::move(column));\ndiff --git a/src/Interpreters/ActionsDAG.h b/src/Interpreters/ActionsDAG.h\nindex 0d6b1ce0e047..746d12f6884b 100644\n--- a/src/Interpreters/ActionsDAG.h\n+++ b/src/Interpreters/ActionsDAG.h\n@@ -282,14 +282,13 @@ class ActionsDAG\n \n     /// For apply materialize() function for every output.\n     /// Also add aliases so the result names remain unchanged.\n-    void addMaterializingOutputActions();\n+    void addMaterializingOutputActions(bool materialize_sparse);\n \n     /// Apply materialize() function to node. Result node has the same name.\n-    const Node & materializeNode(const Node & node);\n+    const Node & materializeNode(const Node & node, bool materialize_sparse = true);\n \n     enum class MatchColumnsMode : uint8_t\n     {\n-        /// Require same number of columns in source and result. Match columns by corresponding positions, regardless to names.\n         Position,\n         /// Find columns in source by their names. Allow excessive columns in source.\n         Name,\ndiff --git a/src/Interpreters/BloomFilterHash.h b/src/Interpreters/BloomFilterHash.h\nindex 8248e9e44694..49450b5932b2 100644\n--- a/src/Interpreters/BloomFilterHash.h\n+++ b/src/Interpreters/BloomFilterHash.h\n@@ -171,7 +171,7 @@ struct BloomFilterHash\n         const auto * index_column = typeid_cast<const ColumnVector<Type> *>(column);\n \n         if (unlikely(!index_column))\n-            throw Exception(ErrorCodes::ILLEGAL_COLUMN, \"Illegal column type was passed to the bloom filter index.\");\n+            throw Exception(ErrorCodes::ILLEGAL_COLUMN, \"Illegal column {} was passed to the bloom filter index\", column->getName());\n \n         const typename ColumnVector<Type>::Container & vec_from = index_column->getData();\n \ndiff --git a/src/Interpreters/InterpreterInsertQuery.cpp b/src/Interpreters/InterpreterInsertQuery.cpp\nindex d7f9c338ab1c..1a2213bf4640 100644\n--- a/src/Interpreters/InterpreterInsertQuery.cpp\n+++ b/src/Interpreters/InterpreterInsertQuery.cpp\n@@ -71,6 +71,7 @@ namespace Setting\n     extern const SettingsBool use_concurrency_control;\n     extern const SettingsSeconds lock_acquire_timeout;\n     extern const SettingsUInt64 parallel_distributed_insert_select;\n+    extern const SettingsBool enable_parsing_to_custom_serialization;\n }\n \n namespace ErrorCodes\n@@ -563,11 +564,10 @@ QueryPipeline InterpreterInsertQuery::buildInsertSelectPipeline(ASTInsertQuery &\n         return std::make_shared<ExpressionTransform>(in_header, actions);\n     });\n \n-    /// We need to convert Sparse columns to full, because it's destination storage\n-    /// may not support it or may have different settings for applying Sparse serialization.\n+    /// We need to convert Sparse columns to full if the destination storage doesn't support them.\n     pipeline.addSimpleTransform([&](const Block & in_header) -> ProcessorPtr\n     {\n-        return std::make_shared<MaterializingTransform>(in_header);\n+        return std::make_shared<MaterializingTransform>(in_header, !table->supportsSparseSerialization());\n     });\n \n     pipeline.addSimpleTransform([&](const Block & in_header) -> ProcessorPtr\n@@ -737,11 +737,14 @@ QueryPipeline InterpreterInsertQuery::buildInsertPipeline(ASTInsertQuery & query\n \n     if (query.hasInlinedData() && !async_insert)\n     {\n-        /// can execute without additional data\n         auto format = getInputFormatFromASTInsertQuery(query_ptr, true, query_sample_block, getContext(), nullptr);\n-        for (auto && buffer : owned_buffers)\n+\n+        for (auto & buffer : owned_buffers)\n             format->addBuffer(std::move(buffer));\n \n+        if (settings[Setting::enable_parsing_to_custom_serialization])\n+            format->setSerializationHints(table->getSerializationHints());\n+\n         auto pipe = getSourceFromInputFormat(query_ptr, std::move(format), getContext(), nullptr);\n         pipeline.complete(std::move(pipe));\n     }\ndiff --git a/src/Interpreters/Squashing.cpp b/src/Interpreters/Squashing.cpp\nindex c656a1a797b0..8122800f882f 100644\n--- a/src/Interpreters/Squashing.cpp\n+++ b/src/Interpreters/Squashing.cpp\n@@ -1,8 +1,9 @@\n #include <vector>\n #include <Interpreters/Squashing.h>\n-#include \"Common/Logger.h\"\n-#include \"Common/logger_useful.h\"\n #include <Common/CurrentThread.h>\n+#include <Common/Logger.h>\n+#include <Common/logger_useful.h>\n+#include <Columns/ColumnSparse.h>\n #include <base/defines.h>\n \n namespace DB\n@@ -116,7 +117,7 @@ Chunk Squashing::squash(std::vector<Chunk> && input_chunks, Chunk::ChunkInfoColl\n         return result;\n     }\n \n-    std::vector<IColumn::MutablePtr> mutable_columns = {};\n+    std::vector<IColumn::MutablePtr> mutable_columns;\n     size_t rows = 0;\n     for (const Chunk & chunk : input_chunks)\n         rows += chunk.getNumRows();\n@@ -130,8 +131,11 @@ Chunk Squashing::squash(std::vector<Chunk> && input_chunks, Chunk::ChunkInfoColl\n     }\n \n     size_t num_columns = mutable_columns.size();\n+\n     /// Collect the list of source columns for each column.\n-    std::vector<Columns> source_columns_list(num_columns, Columns{});\n+    std::vector<Columns> source_columns_list(num_columns);\n+    std::vector<UInt8> have_same_serialization(num_columns, true);\n+\n     for (size_t i = 0; i != num_columns; ++i)\n         source_columns_list[i].reserve(input_chunks.size() - 1);\n \n@@ -139,11 +143,21 @@ Chunk Squashing::squash(std::vector<Chunk> && input_chunks, Chunk::ChunkInfoColl\n     {\n         auto columns = input_chunks[i].detachColumns();\n         for (size_t j = 0; j != num_columns; ++j)\n+        {\n+            have_same_serialization[j] &= ISerialization::getKind(*columns[j]) == ISerialization::getKind(*mutable_columns[j]);\n             source_columns_list[j].emplace_back(std::move(columns[j]));\n+        }\n     }\n \n     for (size_t i = 0; i != num_columns; ++i)\n     {\n+        if (!have_same_serialization[i])\n+        {\n+            mutable_columns[i] = recursiveRemoveSparse(std::move(mutable_columns[i]))->assumeMutable();\n+            for (auto & column : source_columns_list[i])\n+                column = recursiveRemoveSparse(column);\n+        }\n+\n         /// We know all the data we will insert in advance and can make all necessary pre-allocations.\n         mutable_columns[i]->prepareForSquashing(source_columns_list[i]);\n         for (auto & source_column : source_columns_list[i])\ndiff --git a/src/Interpreters/addMissingDefaults.cpp b/src/Interpreters/addMissingDefaults.cpp\nindex 27d79e86622f..173478332f3d 100644\n--- a/src/Interpreters/addMissingDefaults.cpp\n+++ b/src/Interpreters/addMissingDefaults.cpp\n@@ -85,7 +85,7 @@ ActionsDAG addMissingDefaults(\n \n     /// Removes unused columns and reorders result.\n     actions.removeUnusedActions(required_columns.getNames(), false);\n-    actions.addMaterializingOutputActions();\n+    actions.addMaterializingOutputActions(/*materialize_sparse=*/ false);\n \n     return actions;\n }\ndiff --git a/src/Interpreters/executeQuery.cpp b/src/Interpreters/executeQuery.cpp\nindex a96350c7ca34..14a67521a100 100644\n--- a/src/Interpreters/executeQuery.cpp\n+++ b/src/Interpreters/executeQuery.cpp\n@@ -1247,7 +1247,6 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n                 {\n                     if (!interpreter->supportsTransactions())\n                         throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Transactions are not supported for this type of query ({})\", ast->getID());\n-\n                 }\n \n                 // InterpreterSelectQueryAnalyzer does not build QueryPlan in the constructor.\ndiff --git a/src/Processors/Formats/IInputFormat.h b/src/Processors/Formats/IInputFormat.h\nindex 713c1089d289..64b289170d2d 100644\n--- a/src/Processors/Formats/IInputFormat.h\n+++ b/src/Processors/Formats/IInputFormat.h\n@@ -58,6 +58,10 @@ class IInputFormat : public SourceWithKeyCondition\n     /// parallel parsing before creating this parser.\n     virtual void setRowsReadBefore(size_t /*rows*/) {}\n \n+    /// Sets the serialization hints for the columns. It allows to create columns\n+    /// in custom serializations (e.g. Sparse) for parsing and avoid extra conversion.\n+    virtual void setSerializationHints(const SerializationInfoByName & /*hints*/) {}\n+\n     void addBuffer(std::unique_ptr<ReadBuffer> buffer) { owned_buffers.emplace_back(std::move(buffer)); }\n \n     void setErrorsLogger(const InputFormatErrorsLoggerPtr & errors_logger_) { errors_logger = errors_logger_; }\ndiff --git a/src/Processors/Formats/IRowInputFormat.cpp b/src/Processors/Formats/IRowInputFormat.cpp\nindex 0b6c81923dbe..b8e8822e6485 100644\n--- a/src/Processors/Formats/IRowInputFormat.cpp\n+++ b/src/Processors/Formats/IRowInputFormat.cpp\n@@ -103,7 +103,10 @@ Chunk IRowInputFormat::read()\n     const Block & header = getPort().getHeader();\n \n     size_t num_columns = header.columns();\n-    MutableColumns columns = header.cloneEmptyColumns();\n+    MutableColumns columns(num_columns);\n+\n+    for (size_t i = 0; i < num_columns; ++i)\n+        columns[i] = header.getByPosition(i).type->createColumn(*serializations[i]);\n \n     block_missing_values.clear();\n \n@@ -266,5 +269,10 @@ size_t IRowInputFormat::countRows(size_t)\n     throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method countRows is not implemented for input format {}\", getName());\n }\n \n+void IRowInputFormat::setSerializationHints(const SerializationInfoByName & hints)\n+{\n+    serializations = getPort().getHeader().getSerializations(hints);\n+}\n+\n \n }\ndiff --git a/src/Processors/Formats/IRowInputFormat.h b/src/Processors/Formats/IRowInputFormat.h\nindex f8796df86049..c6786f45ecb2 100644\n--- a/src/Processors/Formats/IRowInputFormat.h\n+++ b/src/Processors/Formats/IRowInputFormat.h\n@@ -5,6 +5,7 @@\n #include <Processors/Formats/IInputFormat.h>\n #include <QueryPipeline/SizeLimits.h>\n #include <Poco/Timespan.h>\n+#include <DataTypes/Serializations/SerializationInfo.h>\n \n class Stopwatch;\n \n@@ -84,6 +85,7 @@ class IRowInputFormat : public IInputFormat\n     size_t getApproxBytesReadForChunk() const override { return approx_bytes_read_for_chunk; }\n \n     void setRowsReadBefore(size_t rows) override { total_rows = rows; }\n+    void setSerializationHints(const SerializationInfoByName & hints) override;\n \n     Serializations serializations;\n \ndiff --git a/src/Processors/Formats/Impl/ParallelParsingInputFormat.cpp b/src/Processors/Formats/Impl/ParallelParsingInputFormat.cpp\nindex 447adb1ed48f..faf6bf81869b 100644\n--- a/src/Processors/Formats/Impl/ParallelParsingInputFormat.cpp\n+++ b/src/Processors/Formats/Impl/ParallelParsingInputFormat.cpp\n@@ -92,6 +92,7 @@ void ParallelParsingInputFormat::parserThreadFunction(ThreadGroupPtr thread_grou\n         InputFormatPtr input_format = internal_parser_creator(read_buffer);\n         input_format->setRowsReadBefore(unit.offset);\n         input_format->setErrorsLogger(errors_logger);\n+        input_format->setSerializationHints(serialization_hints);\n         InternalParser parser(input_format);\n \n         unit.chunk_ext.chunk.clear();\ndiff --git a/src/Processors/Formats/Impl/ParallelParsingInputFormat.h b/src/Processors/Formats/Impl/ParallelParsingInputFormat.h\nindex b97bf5213e60..e3753385ae8b 100644\n--- a/src/Processors/Formats/Impl/ParallelParsingInputFormat.h\n+++ b/src/Processors/Formats/Impl/ParallelParsingInputFormat.h\n@@ -129,6 +129,11 @@ class ParallelParsingInputFormat : public IInputFormat\n         return last_block_missing_values;\n     }\n \n+    void setSerializationHints(const SerializationInfoByName & hints) override\n+    {\n+        serialization_hints = hints;\n+    }\n+\n     size_t getApproxBytesReadForChunk() const override { return last_approx_bytes_read_for_chunk; }\n \n     String getName() const final { return \"ParallelParsingBlockInputFormat\"; }\n@@ -207,6 +212,7 @@ class ParallelParsingInputFormat : public IInputFormat\n \n     BlockMissingValues last_block_missing_values;\n     size_t last_approx_bytes_read_for_chunk = 0;\n+    SerializationInfoByName serialization_hints;\n \n     /// Non-atomic because it is used in one thread.\n     std::optional<size_t> next_block_in_current_unit;\ndiff --git a/src/Processors/Transforms/MaterializingTransform.cpp b/src/Processors/Transforms/MaterializingTransform.cpp\nindex 9ae80e21a686..771718e5cede 100644\n--- a/src/Processors/Transforms/MaterializingTransform.cpp\n+++ b/src/Processors/Transforms/MaterializingTransform.cpp\n@@ -5,8 +5,11 @@\n namespace DB\n {\n \n-MaterializingTransform::MaterializingTransform(const Block & header)\n-    : ISimpleTransform(header, materializeBlock(header), false) {}\n+MaterializingTransform::MaterializingTransform(const Block & header, bool remove_sparse_)\n+    : ISimpleTransform(header, materializeBlock(header), false)\n+    , remove_sparse(remove_sparse_)\n+{\n+}\n \n void MaterializingTransform::transform(Chunk & chunk)\n {\n@@ -14,7 +17,11 @@ void MaterializingTransform::transform(Chunk & chunk)\n     auto columns = chunk.detachColumns();\n \n     for (auto & col : columns)\n-        col = recursiveRemoveSparse(col->convertToFullColumnIfConst());\n+    {\n+        col = col->convertToFullColumnIfConst();\n+        if (remove_sparse)\n+            col = recursiveRemoveSparse(col);\n+    }\n \n     chunk.setColumns(std::move(columns), num_rows);\n }\ndiff --git a/src/Processors/Transforms/MaterializingTransform.h b/src/Processors/Transforms/MaterializingTransform.h\nindex 5ecd85224268..d384083a50d5 100644\n--- a/src/Processors/Transforms/MaterializingTransform.h\n+++ b/src/Processors/Transforms/MaterializingTransform.h\n@@ -8,12 +8,13 @@ namespace DB\n class MaterializingTransform : public ISimpleTransform\n {\n public:\n-    explicit MaterializingTransform(const Block & header);\n+    explicit MaterializingTransform(const Block & header, bool remove_sparse_ = true);\n \n     String getName() const override { return \"MaterializingTransform\"; }\n \n protected:\n     void transform(Chunk & chunk) override;\n+    bool remove_sparse;\n };\n \n }\ndiff --git a/src/Processors/Transforms/getSourceFromASTInsertQuery.cpp b/src/Processors/Transforms/getSourceFromASTInsertQuery.cpp\nindex 4bb3b88886e2..1f6474da7d00 100644\n--- a/src/Processors/Transforms/getSourceFromASTInsertQuery.cpp\n+++ b/src/Processors/Transforms/getSourceFromASTInsertQuery.cpp\n@@ -66,8 +66,7 @@ InputFormatPtr getInputFormatFromASTInsertQuery(\n         : std::make_unique<EmptyReadBuffer>();\n \n     /// Create a source from input buffer using format from query\n-    auto source\n-        = context->getInputFormat(ast_insert_query->format, *input_buffer, header, context->getSettingsRef()[Setting::max_insert_block_size]);\n+    auto source = context->getInputFormat(ast_insert_query->format, *input_buffer, header, context->getSettingsRef()[Setting::max_insert_block_size]);\n     source->addBuffer(std::move(input_buffer));\n     return source;\n }\ndiff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h\nindex 07058dfb5dff..0dc486342821 100644\n--- a/src/Storages/IStorage.h\n+++ b/src/Storages/IStorage.h\n@@ -18,6 +18,7 @@\n #include <Common/Exception.h>\n #include <Common/RWLock.h>\n #include <Common/TypePromotion.h>\n+#include <DataTypes/Serializations/SerializationInfo.h>\n \n #include <optional>\n \n@@ -269,6 +270,9 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo\n     /// because those are internally translated into 'ALTER UDPATE' mutations.\n     virtual bool supportsDelete() const { return false; }\n \n+    /// Returns true if storage can store columns in sparse serialization.\n+    virtual bool supportsSparseSerialization() const { return false; }\n+\n     /// Return true if the trivial count query could be optimized without reading the data at all\n     /// in totalRows() or totalRowsByPartitionPredicate() methods or with optimized reading in read() method.\n     /// 'storage_snapshot' may be nullptr.\n@@ -277,6 +281,9 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo\n         return false;\n     }\n \n+    /// Returns hints for serialization of columns accorsing to statistics accumulated by storage.\n+    virtual SerializationInfoByName getSerializationHints() const { return {}; }\n+\n private:\n     StorageID storage_id;\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp b/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp\nindex c87f66b64f33..4f42a7e91226 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp\n@@ -1,5 +1,6 @@\n #include <Storages/MergeTree/IMergeTreeDataPartWriter.h>\n #include <Common/MemoryTrackerBlockerInThread.h>\n+#include <Columns/ColumnSparse.h>\n \n namespace DB\n {\n@@ -11,13 +12,14 @@ namespace ErrorCodes\n }\n \n \n-Block getBlockAndPermute(const Block & block, const Names & names, const IColumn::Permutation * permutation)\n+Block getIndexBlockAndPermute(const Block & block, const Names & names, const IColumn::Permutation * permutation)\n {\n     Block result;\n     for (size_t i = 0, size = names.size(); i < size; ++i)\n     {\n-        const auto & name = names[i];\n-        result.insert(i, block.getByName(name));\n+        auto src_column = block.getByName(names[i]);\n+        src_column.column = recursiveRemoveSparse(src_column.column);\n+        result.insert(i, src_column);\n \n         /// Reorder primary key columns in advance and add them to `primary_key_columns`.\n         if (permutation)\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\nindex 2fdb0794789a..eb51a1b29224 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n@@ -16,7 +16,7 @@ namespace DB\n struct MergeTreeSettings;\n using MergeTreeSettingsPtr = std::shared_ptr<const MergeTreeSettings>;\n \n-Block getBlockAndPermute(const Block & block, const Names & names, const IColumn::Permutation * permutation);\n+Block getIndexBlockAndPermute(const Block & block, const Names & names, const IColumn::Permutation * permutation);\n \n Block permuteBlockIfNeeded(const Block & block, const IColumn::Permutation * permutation);\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeReader.cpp b/src/Storages/MergeTree/IMergeTreeReader.cpp\nindex 1f46d6b8e1b2..b2f18f08f416 100644\n--- a/src/Storages/MergeTree/IMergeTreeReader.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeReader.cpp\n@@ -172,7 +172,7 @@ void IMergeTreeReader::evaluateMissingDefaults(Block additional_columns, Columns\n \n         if (dag)\n         {\n-            dag->addMaterializingOutputActions();\n+            dag->addMaterializingOutputActions(/*materialize_sparse=*/ false);\n             auto actions = std::make_shared<ExpressionActions>(\n                 std::move(*dag),\n                 ExpressionActionsSettings::fromSettings(data_part_info_for_read->getContext()->getSettingsRef()));\ndiff --git a/src/Storages/MergeTree/MergeTask.cpp b/src/Storages/MergeTree/MergeTask.cpp\nindex 4dd8400ec962..0be1e5087032 100644\n--- a/src/Storages/MergeTree/MergeTask.cpp\n+++ b/src/Storages/MergeTree/MergeTask.cpp\n@@ -995,7 +995,7 @@ MergeTask::VerticalMergeRuntimeContext::PreparedColumnPipeline MergeTask::Vertic\n             indexes_to_recalc = MergeTreeIndexFactory::instance().getMany(indexes_it->second);\n \n             auto indices_expression_dag = indexes_it->second.getSingleExpressionForIndices(global_ctx->metadata_snapshot->getColumns(), global_ctx->data->getContext())->getActionsDAG().clone();\n-            indices_expression_dag.addMaterializingOutputActions(); /// Const columns cannot be written without materialization.\n+            indices_expression_dag.addMaterializingOutputActions(/*materialize_sparse=*/ true); /// Const columns cannot be written without materialization.\n             auto calculate_indices_expression_step = std::make_unique<ExpressionStep>(\n                 merge_column_query_plan.getCurrentDataStream(),\n                 std::move(indices_expression_dag));\n@@ -1719,7 +1719,7 @@ void MergeTask::ExecuteAndFinalizeHorizontalPart::createMergedStream() const\n     if (!global_ctx->merging_skip_indexes.empty())\n     {\n         auto indices_expression_dag = global_ctx->merging_skip_indexes.getSingleExpressionForIndices(global_ctx->metadata_snapshot->getColumns(), global_ctx->data->getContext())->getActionsDAG().clone();\n-        indices_expression_dag.addMaterializingOutputActions(); /// Const columns cannot be written without materialization.\n+        indices_expression_dag.addMaterializingOutputActions(/*materialize_sparse=*/ true); /// Const columns cannot be written without materialization.\n         auto calculate_indices_expression_step = std::make_unique<ExpressionStep>(\n             merge_parts_query_plan.getCurrentDataStream(),\n             std::move(indices_expression_dag));\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex c728ffd63d49..040b696af296 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -1904,6 +1904,7 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks, std::optional<std::un\n     if (num_parts == 0 && unexpected_parts_to_load.empty())\n     {\n         resetObjectColumnsFromActiveParts(part_lock);\n+        resetSerializationHints(part_lock);\n         LOG_DEBUG(log, \"There are no data parts\");\n         return;\n     }\n@@ -1950,6 +1951,7 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks, std::optional<std::un\n             part->renameToDetached(\"broken-on-start\"); /// detached parts must not have '_' in prefixes\n \n     resetObjectColumnsFromActiveParts(part_lock);\n+    resetSerializationHints(part_lock);\n     calculateColumnAndSecondaryIndexSizesImpl();\n \n     PartLoadingTreeNodes unloaded_parts;\n@@ -6908,6 +6910,8 @@ MergeTreeData::DataPartsVector MergeTreeData::Transaction::commit(DataPartsLock\n                 }\n             }\n \n+            data.updateSerializationHints(precommitted_parts, total_covered_parts, parts_lock);\n+\n             if (reduce_parts == 0)\n             {\n                 for (const auto & part : precommitted_parts)\n@@ -8571,6 +8575,66 @@ void MergeTreeData::updateObjectColumns(const DataPartPtr & part, const DataPart\n     DB::updateObjectColumns(object_columns, columns, part->getColumns());\n }\n \n+template <typename DataPartPtr>\n+static void updateSerializationHintsForPart(const DataPartPtr & part, const ColumnsDescription & storage_columns, SerializationInfoByName & hints, bool remove)\n+{\n+    const auto & part_columns = part->getColumnsDescription();\n+    for (const auto & [name, info] : part->getSerializationInfos())\n+    {\n+        auto new_hint = hints.tryGet(name);\n+        if (!new_hint)\n+            continue;\n+\n+        /// Structure may change after alter. Do not add info for such items.\n+        /// Instead it will be updated on commit of the result part of alter.\n+        if (part_columns.tryGetPhysical(name) != storage_columns.tryGetPhysical(name))\n+            continue;\n+\n+        chassert(new_hint->structureEquals(*info));\n+        if (remove)\n+            new_hint->remove(*info);\n+        else\n+            new_hint->add(*info);\n+    }\n+}\n+\n+void MergeTreeData::resetSerializationHints(const DataPartsLock & /*lock*/)\n+{\n+    SerializationInfo::Settings settings =\n+    {\n+        .ratio_of_defaults_for_sparse = getSettings()->ratio_of_defaults_for_sparse_serialization,\n+        .choose_kind = true,\n+    };\n+\n+    const auto & storage_columns = getInMemoryMetadataPtr()->getColumns();\n+    serialization_hints = SerializationInfoByName(storage_columns.getAllPhysical(), settings);\n+    auto range = getDataPartsStateRange(DataPartState::Active);\n+\n+    for (const auto & part : range)\n+        updateSerializationHintsForPart(part, storage_columns, serialization_hints, false);\n+}\n+\n+template <typename AddedParts, typename RemovedParts>\n+void MergeTreeData::updateSerializationHints(const AddedParts & added_parts, const RemovedParts & removed_parts, const DataPartsLock & /*lock*/)\n+{\n+    const auto & storage_columns = getInMemoryMetadataPtr()->getColumns();\n+\n+    for (const auto & part : added_parts)\n+        updateSerializationHintsForPart(part, storage_columns, serialization_hints, false);\n+\n+    for (const auto & part : removed_parts)\n+        updateSerializationHintsForPart(part, storage_columns, serialization_hints, true);\n+}\n+\n+SerializationInfoByName MergeTreeData::getSerializationHints() const\n+{\n+    auto lock = lockParts();\n+    SerializationInfoByName res;\n+    for (const auto & [name, info] : serialization_hints)\n+        res.emplace(name, info->clone());\n+    return res;\n+}\n+\n bool MergeTreeData::supportsTrivialCountOptimization(const StorageSnapshotPtr & storage_snapshot, ContextPtr query_context) const\n {\n     if (hasLightweightDeletedMask())\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 5edd24db40d5..7a9730e8627c 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -441,6 +441,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n \n     bool supportsDynamicSubcolumnsDeprecated() const override { return true; }\n     bool supportsDynamicSubcolumns() const override { return true; }\n+    bool supportsSparseSerialization() const override { return true; }\n \n     bool supportsLightweightDelete() const override;\n \n@@ -1242,6 +1243,11 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// protected by @data_parts_mutex.\n     ColumnsDescription object_columns;\n \n+    /// Serialization info accumulated among all active parts.\n+    /// It changes only when set of parts is changed and is\n+    /// protected by @data_parts_mutex.\n+    SerializationInfoByName serialization_hints;\n+\n     MergeTreePartsMover parts_mover;\n \n     /// Executors are common for both ReplicatedMergeTree and plain MergeTree\n@@ -1530,6 +1536,13 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     void resetObjectColumnsFromActiveParts(const DataPartsLock & lock);\n     void updateObjectColumns(const DataPartPtr & part, const DataPartsLock & lock);\n \n+    void resetSerializationHints(const DataPartsLock & lock);\n+\n+    template <typename AddedParts, typename RemovedParts>\n+    void updateSerializationHints(const AddedParts & added_parts, const RemovedParts & removed_parts, const DataPartsLock & lock);\n+\n+    SerializationInfoByName getSerializationHints() const override;\n+\n     /** A structure that explicitly represents a \"merge tree\" of parts\n      *  which is implicitly presented by min-max block numbers and levels of parts.\n      *  The children of node are parts which are covered by parent part.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex f4be7619fc83..a859172023fa 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -213,11 +213,11 @@ void MergeTreeDataPartWriterCompact::writeDataBlockPrimaryIndexAndSkipIndices(co\n \n     if (settings.rewrite_primary_key)\n     {\n-        Block primary_key_block = getBlockAndPermute(block, metadata_snapshot->getPrimaryKeyColumns(), nullptr);\n+        Block primary_key_block = getIndexBlockAndPermute(block, metadata_snapshot->getPrimaryKeyColumns(), nullptr);\n         calculateAndSerializePrimaryIndex(primary_key_block, granules_to_write);\n     }\n \n-    Block skip_indices_block = getBlockAndPermute(block, getSkipIndicesColumns(), nullptr);\n+    Block skip_indices_block = getIndexBlockAndPermute(block, getSkipIndicesColumns(), nullptr);\n     calculateAndSerializeSkipIndices(skip_indices_block, granules_to_write);\n }\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex f050accd7a1f..04e07a0588a7 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -296,9 +296,9 @@ void MergeTreeDataPartWriterWide::write(const Block & block, const IColumn::Perm\n     auto offset_columns = written_offset_columns ? *written_offset_columns : WrittenOffsetColumns{};\n     Block primary_key_block;\n     if (settings.rewrite_primary_key)\n-        primary_key_block = getBlockAndPermute(block, metadata_snapshot->getPrimaryKeyColumns(), permutation);\n+        primary_key_block = getIndexBlockAndPermute(block, metadata_snapshot->getPrimaryKeyColumns(), permutation);\n \n-    Block skip_indexes_block = getBlockAndPermute(block, getSkipIndicesColumns(), permutation);\n+    Block skip_indexes_block = getIndexBlockAndPermute(block, getSkipIndicesColumns(), permutation);\n \n     auto it = columns_list.begin();\n     for (size_t i = 0; i < columns_list.size(); ++i, ++it)\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex ed68200041bf..130d9ca8f6a2 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -577,6 +577,13 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     SerializationInfoByName infos(columns, settings);\n     infos.add(block);\n \n+    for (const auto & [column_name, _] : columns)\n+    {\n+        auto & column = block.getByName(column_name);\n+        if (column.column->isSparse() && infos.getKind(column_name) != ISerialization::Kind::SPARSE)\n+            column.column = recursiveRemoveSparse(column.column);\n+    }\n+\n     new_data_part->setColumns(columns, infos, metadata_snapshot->getMetadataVersion());\n     new_data_part->rows_count = block.rows();\n     new_data_part->existing_rows_count = block.rows();\ndiff --git a/src/Storages/MergeTree/MergeTreeReadTask.h b/src/Storages/MergeTree/MergeTreeReadTask.h\nindex e90a07e0b558..748babb5b4c7 100644\n--- a/src/Storages/MergeTree/MergeTreeReadTask.h\n+++ b/src/Storages/MergeTree/MergeTreeReadTask.h\n@@ -66,7 +66,7 @@ struct MergeTreeReadTaskInfo\n     MergeTreeReadTaskColumns task_columns;\n     /// Shared initialized size predictor. It is copied for each new task.\n     MergeTreeBlockSizePredictorPtr shared_size_predictor;\n-    /// TODO: comment\n+    /// Shared constant fields for virtual columns.\n     VirtualFields const_virtual_fields;\n     /// The amount of data to read per task based on size of the queried columns.\n     size_t min_marks_per_task = 0;\ndiff --git a/src/Storages/ObjectStorage/DataLakes/IStorageDataLake.h b/src/Storages/ObjectStorage/DataLakes/IStorageDataLake.h\nindex a17fd1632536..6dff60aeaa9c 100644\n--- a/src/Storages/ObjectStorage/DataLakes/IStorageDataLake.h\n+++ b/src/Storages/ObjectStorage/DataLakes/IStorageDataLake.h\n@@ -144,7 +144,7 @@ class IStorageDataLake final : public StorageObjectStorage\n         bool supports_subset_of_columns,\n         ContextPtr local_context) override\n     {\n-        auto info = DB::prepareReadingFromFormat(requested_columns, storage_snapshot, supports_subset_of_columns);\n+        auto info = DB::prepareReadingFromFormat(requested_columns, storage_snapshot, local_context, supports_subset_of_columns);\n         if (!current_metadata)\n         {\n             Storage::updateConfiguration(local_context);\ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorage.cpp b/src/Storages/ObjectStorage/StorageObjectStorage.cpp\nindex bc27820707cc..040ce8db51de 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorage.cpp\n+++ b/src/Storages/ObjectStorage/StorageObjectStorage.cpp\n@@ -247,9 +247,9 @@ ReadFromFormatInfo StorageObjectStorage::prepareReadingFromFormat(\n     const Strings & requested_columns,\n     const StorageSnapshotPtr & storage_snapshot,\n     bool supports_subset_of_columns,\n-    ContextPtr /* local_context */)\n+    ContextPtr local_context)\n {\n-    return DB::prepareReadingFromFormat(requested_columns, storage_snapshot, supports_subset_of_columns);\n+    return DB::prepareReadingFromFormat(requested_columns, storage_snapshot, local_context, supports_subset_of_columns);\n }\n \n void StorageObjectStorage::read(\ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp b/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp\nindex 641b43e57d6d..0b7106de9498 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp\n+++ b/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp\n@@ -377,6 +377,8 @@ StorageObjectStorageSource::ReaderHolder StorageObjectStorageSource::createReade\n             compression_method,\n             need_only_count);\n \n+        input_format->setSerializationHints(read_from_format_info.serialization_hints);\n+\n         if (key_condition_)\n             input_format->setKeyCondition(key_condition_);\n \ndiff --git a/src/Storages/ObjectStorageQueue/StorageObjectStorageQueue.cpp b/src/Storages/ObjectStorageQueue/StorageObjectStorageQueue.cpp\nindex 250a0deec4fe..94ca07eec166 100644\n--- a/src/Storages/ObjectStorageQueue/StorageObjectStorageQueue.cpp\n+++ b/src/Storages/ObjectStorageQueue/StorageObjectStorageQueue.cpp\n@@ -299,7 +299,7 @@ void StorageObjectStorageQueue::read(\n     }\n \n     auto this_ptr = std::static_pointer_cast<StorageObjectStorageQueue>(shared_from_this());\n-    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, supportsSubsetOfColumns(local_context));\n+    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, local_context, supportsSubsetOfColumns(local_context));\n \n     auto reading = std::make_unique<ReadFromObjectStorageQueue>(\n         column_names,\n@@ -460,6 +460,7 @@ bool StorageObjectStorageQueue::streamToViews()\n         auto read_from_format_info = prepareReadingFromFormat(\n             block_io.pipeline.getHeader().getNames(),\n             storage_snapshot,\n+            queue_context,\n             supportsSubsetOfColumns(queue_context));\n \n         Pipes pipes;\ndiff --git a/src/Storages/StorageFile.cpp b/src/Storages/StorageFile.cpp\nindex 1d846b6bb0f5..46f4800b4977 100644\n--- a/src/Storages/StorageFile.cpp\n+++ b/src/Storages/StorageFile.cpp\n@@ -99,6 +99,7 @@ namespace Setting\n     extern const SettingsLocalFSReadMethod storage_file_read_method;\n     extern const SettingsBool use_cache_for_count_from_files;\n     extern const SettingsInt64 zstd_window_log_max;\n+    extern const SettingsBool enable_parsing_to_custom_serialization;\n }\n \n namespace ErrorCodes\n@@ -1136,7 +1137,6 @@ void StorageFile::setStorageMetadata(CommonArguments args)\n     setInMemoryMetadata(storage_metadata);\n }\n \n-\n static std::chrono::seconds getLockTimeout(const ContextPtr & context)\n {\n     const Settings & settings = context->getSettingsRef();\n@@ -1209,6 +1209,7 @@ StorageFileSource::StorageFileSource(\n     , requested_columns(info.requested_columns)\n     , requested_virtual_columns(info.requested_virtual_columns)\n     , block_for_format(info.format_header)\n+    , serialization_hints(info.serialization_hints)\n     , max_block_size(max_block_size_)\n     , need_only_count(need_only_count_)\n {\n@@ -1439,6 +1440,8 @@ Chunk StorageFileSource::generate()\n                 storage->format_name, *read_buf, block_for_format, getContext(), max_block_size, storage->format_settings,\n                 max_parsing_threads, std::nullopt, /*is_remote_fs*/ false, CompressionMethod::None, need_only_count);\n \n+            input_format->setSerializationHints(serialization_hints);\n+\n             if (key_condition)\n                 input_format->setKeyCondition(key_condition);\n \n@@ -1630,7 +1633,7 @@ void StorageFile::read(\n \n     auto this_ptr = std::static_pointer_cast<StorageFile>(shared_from_this());\n \n-    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, supportsSubsetOfColumns(context));\n+    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, context, supportsSubsetOfColumns(context));\n     bool need_only_count = (query_info.optimize_trivial_count || read_from_format_info.requested_columns.empty())\n         && context->getSettingsRef()[Setting::optimize_count_from_files];\n \ndiff --git a/src/Storages/StorageFile.h b/src/Storages/StorageFile.h\nindex bb969c1877c8..6b21353f1614 100644\n--- a/src/Storages/StorageFile.h\n+++ b/src/Storages/StorageFile.h\n@@ -296,6 +296,7 @@ class StorageFileSource : public SourceWithKeyCondition, WithContext\n     NamesAndTypesList requested_columns;\n     NamesAndTypesList requested_virtual_columns;\n     Block block_for_format;\n+    SerializationInfoByName serialization_hints;\n \n     UInt64 max_block_size;\n \ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex c921adaf2fd7..e5fc8e1a3b94 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -400,17 +400,18 @@ void StorageMergeTree::alter(\n \n             DatabaseCatalog::instance().getDatabase(table_id.database_name)->alterTable(local_context, table_id, new_metadata);\n \n+            {\n+                /// Reset Object columns, because column of type\n+                /// Object may be added or dropped by alter.\n+                auto parts_lock = lockParts();\n+                resetObjectColumnsFromActiveParts(parts_lock);\n+                resetSerializationHints(parts_lock);\n+            }\n+\n             if (!maybe_mutation_commands.empty())\n                 mutation_version = startMutation(maybe_mutation_commands, local_context);\n         }\n \n-        {\n-            /// Reset Object columns, because column of type\n-            /// Object may be added or dropped by alter.\n-            auto parts_lock = lockParts();\n-            resetObjectColumnsFromActiveParts(parts_lock);\n-        }\n-\n         if (!maybe_mutation_commands.empty() && query_settings[Setting::alter_sync] > 0)\n             waitForMutation(mutation_version, false);\n     }\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 67924fb29130..093ba2b97a34 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -6072,6 +6072,7 @@ bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMer\n         /// Object may be added or dropped by alter.\n         auto parts_lock = lockParts();\n         resetObjectColumnsFromActiveParts(parts_lock);\n+        resetSerializationHints(parts_lock);\n     }\n \n     return true;\ndiff --git a/src/Storages/StorageURL.cpp b/src/Storages/StorageURL.cpp\nindex 42648ad73e65..80c07658055f 100644\n--- a/src/Storages/StorageURL.cpp\n+++ b/src/Storages/StorageURL.cpp\n@@ -408,6 +408,8 @@ StorageURLSource::StorageURLSource(\n                 compression_method,\n                 need_only_count);\n \n+            input_format->setSerializationHints(info.serialization_hints);\n+\n             if (key_condition)\n                 input_format->setKeyCondition(key_condition);\n \n@@ -1127,7 +1129,7 @@ void IStorageURLBase::read(\n     size_t num_streams)\n {\n     auto params = getReadURIParams(column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size);\n-    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, supportsSubsetOfColumns(local_context));\n+    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, local_context, supportsSubsetOfColumns(local_context));\n \n     bool need_only_count = (query_info.optimize_trivial_count || read_from_format_info.requested_columns.empty())\n         && local_context->getSettingsRef()[Setting::optimize_count_from_files];\n@@ -1297,7 +1299,7 @@ void StorageURLWithFailover::read(\n     size_t num_streams)\n {\n     auto params = getReadURIParams(column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size);\n-    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, supportsSubsetOfColumns(local_context));\n+    auto read_from_format_info = prepareReadingFromFormat(column_names, storage_snapshot, local_context, supportsSubsetOfColumns(local_context));\n \n     bool need_only_count = (query_info.optimize_trivial_count || read_from_format_info.requested_columns.empty())\n         && local_context->getSettingsRef()[Setting::optimize_count_from_files];\ndiff --git a/src/Storages/StorageView.cpp b/src/Storages/StorageView.cpp\nindex cb438e0efa68..bcbcd4f66c8e 100644\n--- a/src/Storages/StorageView.cpp\n+++ b/src/Storages/StorageView.cpp\n@@ -187,7 +187,7 @@ void StorageView::read(\n     /// It's expected that the columns read from storage are not constant.\n     /// Because method 'getSampleBlockForColumns' is used to obtain a structure of result in InterpreterSelectQuery.\n     ActionsDAG materializing_actions(query_plan.getCurrentDataStream().header.getColumnsWithTypeAndName());\n-    materializing_actions.addMaterializingOutputActions();\n+    materializing_actions.addMaterializingOutputActions(/*materialize_sparse=*/ true);\n \n     auto materializing = std::make_unique<ExpressionStep>(query_plan.getCurrentDataStream(), std::move(materializing_actions));\n     materializing->setStepDescription(\"Materialize constants after VIEW subquery\");\ndiff --git a/src/Storages/prepareReadingFromFormat.cpp b/src/Storages/prepareReadingFromFormat.cpp\nindex 406b7f379f90..b87af449dc50 100644\n--- a/src/Storages/prepareReadingFromFormat.cpp\n+++ b/src/Storages/prepareReadingFromFormat.cpp\n@@ -1,10 +1,19 @@\n #include <Storages/prepareReadingFromFormat.h>\n #include <Formats/FormatFactory.h>\n+#include <Core/Settings.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/DatabaseCatalog.h>\n+#include <Storages/IStorage.h>\n \n namespace DB\n {\n \n-ReadFromFormatInfo prepareReadingFromFormat(const Strings & requested_columns, const StorageSnapshotPtr & storage_snapshot, bool supports_subset_of_columns)\n+namespace Setting\n+{\n+    extern const SettingsBool enable_parsing_to_custom_serialization;\n+}\n+\n+ReadFromFormatInfo prepareReadingFromFormat(const Strings & requested_columns, const StorageSnapshotPtr & storage_snapshot, const ContextPtr & context, bool supports_subset_of_columns)\n {\n     ReadFromFormatInfo info;\n     /// Collect requested virtual columns and remove them from requested columns.\n@@ -72,7 +81,35 @@ ReadFromFormatInfo prepareReadingFromFormat(const Strings & requested_columns, c\n \n     /// Create header for InputFormat with columns that will be read from the data.\n     info.format_header = storage_snapshot->getSampleBlockForColumns(info.columns_description.getNamesOfPhysical());\n+    info.serialization_hints = getSerializationHintsForFileLikeStorage(storage_snapshot->metadata, context);\n     return info;\n }\n \n+SerializationInfoByName getSerializationHintsForFileLikeStorage(const StorageMetadataPtr & metadata_snapshot, const ContextPtr & context)\n+{\n+    if (!context->getSettingsRef()[Setting::enable_parsing_to_custom_serialization])\n+        return {};\n+\n+    auto insertion_table = context->getInsertionTable();\n+    if (!insertion_table)\n+        return {};\n+\n+    auto storage_ptr = DatabaseCatalog::instance().tryGetTable(insertion_table, context);\n+    if (!storage_ptr)\n+        return {};\n+\n+    const auto & our_columns = metadata_snapshot->getColumns();\n+    const auto & storage_columns = storage_ptr->getInMemoryMetadataPtr()->getColumns();\n+    auto storage_hints = storage_ptr->getSerializationHints();\n+    SerializationInfoByName res;\n+\n+    for (const auto & hint : storage_hints)\n+    {\n+        if (our_columns.tryGetPhysical(hint.first) == storage_columns.tryGetPhysical(hint.first))\n+            res.insert(hint);\n+    }\n+\n+    return res;\n+}\n+\n }\ndiff --git a/src/Storages/prepareReadingFromFormat.h b/src/Storages/prepareReadingFromFormat.h\nindex e4d62c29ec6d..02e42056d0cc 100644\n--- a/src/Storages/prepareReadingFromFormat.h\n+++ b/src/Storages/prepareReadingFromFormat.h\n@@ -1,6 +1,8 @@\n #pragma once\n #include <Core/Block.h>\n #include <Storages/StorageSnapshot.h>\n+#include <DataTypes/Serializations/SerializationInfo.h>\n+#include <Interpreters/Context_fwd.h>\n \n namespace DB\n {\n@@ -19,8 +21,14 @@ namespace DB\n         NamesAndTypesList requested_columns;\n         /// The list of requested virtual columns.\n         NamesAndTypesList requested_virtual_columns;\n+        /// Hints for the serialization of columns.\n+        /// For example can be retrieved from the destination table in INSERT SELECT query.\n+        SerializationInfoByName serialization_hints;\n     };\n \n     /// Get all needed information for reading from data in some input format.\n-    ReadFromFormatInfo prepareReadingFromFormat(const Strings & requested_columns, const StorageSnapshotPtr & storage_snapshot, bool supports_subset_of_columns);\n+    ReadFromFormatInfo prepareReadingFromFormat(const Strings & requested_columns, const StorageSnapshotPtr & storage_snapshot, const ContextPtr & context, bool supports_subset_of_columns);\n+\n+    /// Returns the serialization hints from the insertion table (if it's set in the Context).\n+    SerializationInfoByName getSerializationHintsForFileLikeStorage(const StorageMetadataPtr & metadata_snapshot, const ContextPtr & context);\n }\n",
  "test_patch": "diff --git a/tests/performance/insert_sparse_column.xml b/tests/performance/insert_sparse_column.xml\nnew file mode 100644\nindex 000000000000..0f6cdcec332c\n--- /dev/null\n+++ b/tests/performance/insert_sparse_column.xml\n@@ -0,0 +1,17 @@\n+<test>\n+    <create_query>CREATE TABLE t_insert_sparse (id UInt64, c0 String, c1 String, c2 String, c3 String, c4 String, c5 String, c6 String, c7 String, c8 String, c9 String, c10 String, c11 String, c12 String, c13 String, c14 String, c15 String, c16 String, c17 String, c18 String, c19 String, c20 String, c21 String, c22 String, c23 String, c24 String, c25 String, c26 String, c27 String, c28 String, c29 String, c30 String, c31 String, c32 String, c33 String, c34 String, c35 String, c36 String, c37 String, c38 String, c39 String, c40 String, c41 String, c42 String, c43 String, c44 String, c45 String, c46 String, c47 String, c48 String, c49 String, c50 String, c51 String, c52 String, c53 String, c54 String, c55 String, c56 String, c57 String, c58 String, c59 String, c60 String, c61 String, c62 String, c63 String, c64 String, c65 String, c66 String, c67 String, c68 String, c69 String, c70 String, c71 String, c72 String, c73 String, c74 String, c75 String, c76 String, c77 String, c78 String, c79 String, c80 String, c81 String, c82 String, c83 String, c84 String, c85 String, c86 String, c87 String, c88 String, c89 String, c90 String, c91 String, c92 String, c93 String, c94 String, c95 String, c96 String, c97 String, c98 String, c99 String) ENGINE = MergeTree ORDER BY id</create_query>\n+    <!-- Prepare JSON data -->\n+    <fill_query>SYSTEM STOP MERGES t_insert_sparse</fill_query>\n+    <!-- Prepare JSON data -->\n+    <fill_query>\n+        INSERT INTO FUNCTION file('test_data_sparse.json', LineAsString)\n+        SELECT '{{\"id\": ' || number || ', \"c' || number % 50 || '\": \"' || hex(rand()) || '\"}}'\n+        FROM numbers(100000) SETTINGS engine_file_truncate_on_insert = 1\n+    </fill_query>\n+    <!-- Insert one batch to create statistics about serializations -->\n+    <fill_query>INSERT INTO t_insert_sparse SELECT * FROM file('test_data_sparse.json', JSONEachRow)</fill_query>\n+\n+    <query>INSERT INTO t_insert_sparse SELECT * FROM file('test_data_sparse.json', JSONEachRow)</query>\n+\n+    <drop_query>DROP TABLE IF EXISTS t_insert_sparse</drop_query>\n+</test>\ndiff --git a/tests/queries/0_stateless/02423_insert_stats_behaviour.sh b/tests/queries/0_stateless/02423_insert_stats_behaviour.sh\nindex b85ca3111016..5680af7da718 100755\n--- a/tests/queries/0_stateless/02423_insert_stats_behaviour.sh\n+++ b/tests/queries/0_stateless/02423_insert_stats_behaviour.sh\n@@ -4,9 +4,9 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n # shellcheck source=../shell_config.sh\n . \"$CUR_DIR\"/../shell_config.sh\n \n-$CLICKHOUSE_CLIENT -q \"CREATE TABLE floats (v Float64) Engine=MergeTree() ORDER BY tuple();\"\n-$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_1 (v Float64) Engine=MergeTree() ORDER BY tuple();\"\n-$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_2 (v Float64) Engine=MergeTree() ORDER BY tuple();\"\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE floats (v Float64) Engine=MergeTree() ORDER BY tuple() SETTINGS ratio_of_defaults_for_sparse_serialization = 1.0\"\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_1 (v Float64) Engine=MergeTree() ORDER BY tuple() SETTINGS ratio_of_defaults_for_sparse_serialization = 1.0;\"\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_2 (v Float64) Engine=MergeTree() ORDER BY tuple() SETTINGS ratio_of_defaults_for_sparse_serialization = 1.0;\"\n $CLICKHOUSE_CLIENT -q \"CREATE MATERIALIZED VIEW floats_to_target TO target_1 AS SELECT * FROM floats\"\n $CLICKHOUSE_CLIENT -q \"CREATE MATERIALIZED VIEW floats_to_target_2 TO target_2 AS SELECT * FROM floats, numbers(2) n\"\n \ndiff --git a/tests/queries/0_stateless/02423_insert_summary_behaviour.sh b/tests/queries/0_stateless/02423_insert_summary_behaviour.sh\nindex b184d9ccf47f..cb28724ab58e 100755\n--- a/tests/queries/0_stateless/02423_insert_summary_behaviour.sh\n+++ b/tests/queries/0_stateless/02423_insert_summary_behaviour.sh\n@@ -4,9 +4,9 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n # shellcheck source=../shell_config.sh\n . \"$CUR_DIR\"/../shell_config.sh\n \n-$CLICKHOUSE_CLIENT -q \"CREATE TABLE floats (v Float64) Engine=MergeTree() ORDER BY tuple();\"\n-$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_1 (v Float64) Engine=MergeTree() ORDER BY tuple();\"\n-$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_2 (v Float64) Engine=MergeTree() ORDER BY tuple();\"\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE floats (v Float64) Engine=MergeTree() ORDER BY tuple() SETTINGS ratio_of_defaults_for_sparse_serialization = 1.0;\"\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_1 (v Float64) Engine=MergeTree() ORDER BY tuple() SETTINGS ratio_of_defaults_for_sparse_serialization = 1.0;\"\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE target_2 (v Float64) Engine=MergeTree() ORDER BY tuple() SETTINGS ratio_of_defaults_for_sparse_serialization = 1.0;\"\n $CLICKHOUSE_CLIENT -q \"CREATE MATERIALIZED VIEW floats_to_target TO target_1 AS SELECT * FROM floats\"\n $CLICKHOUSE_CLIENT -q \"CREATE MATERIALIZED VIEW floats_to_target_2 TO target_2 AS SELECT * FROM floats, numbers(2) n\"\n \ndiff --git a/tests/queries/0_stateless/03237_insert_sparse_columns.reference b/tests/queries/0_stateless/03237_insert_sparse_columns.reference\nnew file mode 100644\nindex 000000000000..592fcff9b258\n--- /dev/null\n+++ b/tests/queries/0_stateless/03237_insert_sparse_columns.reference\n@@ -0,0 +1,21 @@\n+1\t0\n+2\t0\n+3\t0\n+4\t0\n+5\t0\n+6\t0\n+7\t0\n+8\t0\n+9\t0\n+10\t0\n+11\t100\n+12\t200\n+13\t300\n+14\t400\n+15\t500\n+all_1_1_0\tid\tDefault\n+all_1_1_0\tv\tSparse\n+all_2_2_0\tid\tDefault\n+all_2_2_0\tv\tSparse\n+all_3_3_0\tid\tDefault\n+all_3_3_0\tv\tDefault\ndiff --git a/tests/queries/0_stateless/03237_insert_sparse_columns.sh b/tests/queries/0_stateless/03237_insert_sparse_columns.sh\nnew file mode 100755\nindex 000000000000..a4d53a36b87e\n--- /dev/null\n+++ b/tests/queries/0_stateless/03237_insert_sparse_columns.sh\n@@ -0,0 +1,25 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT --query \"\n+    DROP TABLE IF EXISTS t_insert_sparse_columns;\n+    CREATE TABLE t_insert_sparse_columns (id UInt64, v UInt64) ENGINE = MergeTree ORDER BY id SETTINGS ratio_of_defaults_for_sparse_serialization = 0.5;\n+    SYSTEM STOP MERGES t_insert_sparse_columns;\n+\"\n+\n+${CLICKHOUSE_CURL} -sS ${CLICKHOUSE_URL} -d 'INSERT INTO t_insert_sparse_columns FORMAT JSONEachRow {\"id\": 1} {\"id\": 2} {\"id\": 3} {\"id\": 4} {\"id\": 5}'\n+${CLICKHOUSE_CURL} -sS ${CLICKHOUSE_URL} -d 'INSERT INTO t_insert_sparse_columns FORMAT JSONEachRow {\"id\": 6} {\"id\": 7} {\"id\": 8} {\"id\": 9} {\"id\": 10}'\n+${CLICKHOUSE_CURL} -sS ${CLICKHOUSE_URL} -d 'INSERT INTO t_insert_sparse_columns FORMAT JSONEachRow {\"id\": 11, \"v\": 100} {\"id\": 12, \"v\": 200} {\"id\": 13, \"v\": 300} {\"id\": 14, \"v\": 400} {\"id\": 15, \"v\": 500}'\n+\n+$CLICKHOUSE_CLIENT --query \"\n+    SELECT * FROM t_insert_sparse_columns ORDER BY id;\n+\n+    SELECT name, column, serialization_kind FROM system.parts_columns\n+    WHERE table = 't_insert_sparse_columns' AND database = currentDatabase() AND active\n+    ORDER BY name, column;\n+\n+    DROP TABLE t_insert_sparse_columns;\n+\"\ndiff --git a/tests/queries/0_stateless/03237_insert_sparse_columns_mem.reference b/tests/queries/0_stateless/03237_insert_sparse_columns_mem.reference\nnew file mode 100644\nindex 000000000000..09ef3399bad7\n--- /dev/null\n+++ b/tests/queries/0_stateless/03237_insert_sparse_columns_mem.reference\n@@ -0,0 +1,9 @@\n+120000\n+435170936075214220\n+435170936075214220\n+Default\t4\n+Sparse\t1000\n+0\n+1\n+1\n+1\ndiff --git a/tests/queries/0_stateless/03237_insert_sparse_columns_mem.sh b/tests/queries/0_stateless/03237_insert_sparse_columns_mem.sh\nnew file mode 100755\nindex 000000000000..ac682a0f574b\n--- /dev/null\n+++ b/tests/queries/0_stateless/03237_insert_sparse_columns_mem.sh\n@@ -0,0 +1,62 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest, long\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+table_structure=\"id UInt64\"\n+\n+for i in {1..250}; do\n+    table_structure+=\", c$i String\"\n+done\n+\n+$CLICKHOUSE_CLIENT --query \"\n+    DROP TABLE IF EXISTS t_insert_mem;\n+    DROP TABLE IF EXISTS t_reference;\n+\n+    CREATE TABLE t_insert_mem ($table_structure) ENGINE = MergeTree ORDER BY id SETTINGS ratio_of_defaults_for_sparse_serialization = 0.9;\n+    CREATE TABLE t_reference ($table_structure) ENGINE = Log;\n+\n+    SYSTEM STOP MERGES t_insert_mem;\n+\"\n+\n+filename=\"test_data_sparse_$CLICKHOUSE_DATABASE.json\"\n+\n+$CLICKHOUSE_CLIENT --query \"\n+    INSERT INTO FUNCTION file('$filename', LineAsString)\n+    SELECT format('{{ \\\"id\\\": {}, \\\"c{}\\\": \\\"{}\\\" }}', number, number % 250, hex(number * 1000000)) FROM numbers(30000)\n+    SETTINGS engine_file_truncate_on_insert = 1;\n+\n+    INSERT INTO FUNCTION s3(s3_conn, filename='$filename', format='LineAsString')\n+    SELECT * FROM file('$filename', LineAsString)\n+    SETTINGS s3_truncate_on_insert = 1;\n+\"\n+\n+for _ in {1..4}; do\n+    $CLICKHOUSE_CLIENT --query \"INSERT INTO t_reference SELECT * FROM file('$filename', JSONEachRow)\"\n+done;\n+\n+$CLICKHOUSE_CLIENT --enable_parsing_to_custom_serialization 1 --query \"INSERT INTO t_insert_mem SELECT * FROM file('$filename', JSONEachRow)\"\n+$CLICKHOUSE_CLIENT --enable_parsing_to_custom_serialization 1 --query \"INSERT INTO t_insert_mem SELECT * FROM file('$filename', JSONEachRow)\"\n+$CLICKHOUSE_CLIENT --enable_parsing_to_custom_serialization 1 --query \"INSERT INTO t_insert_mem SELECT * FROM s3(s3_conn, filename='$filename', format='JSONEachRow')\"\n+$CLICKHOUSE_CLIENT --query \"SELECT * FROM file('$filename', LineAsString) FORMAT LineAsString\" | ${CLICKHOUSE_CURL} -sS \"${CLICKHOUSE_URL}&query=INSERT+INTO+t_insert_mem+FORMAT+JSONEachRow&enable_parsing_to_custom_serialization=1\" --data-binary @-\n+\n+$CLICKHOUSE_CLIENT --query \"\n+    SELECT count() FROM t_insert_mem;\n+    SELECT sum(sipHash64(*)) FROM t_insert_mem;\n+    SELECT sum(sipHash64(*)) FROM t_reference;\n+\n+    SELECT serialization_kind, count() FROM system.parts_columns\n+    WHERE table = 't_insert_mem' AND database = '$CLICKHOUSE_DATABASE'\n+    GROUP BY serialization_kind ORDER BY serialization_kind;\n+\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT written_bytes <= 3000000 FROM system.query_log\n+    WHERE query LIKE 'INSERT INTO t_insert_mem%' AND current_database = '$CLICKHOUSE_DATABASE' AND type = 'QueryFinish'\n+    ORDER BY event_time_microseconds;\n+\n+    DROP TABLE IF EXISTS t_insert_mem;\n+    DROP TABLE IF EXISTS t_reference;\n+\"\n",
  "problem_statement": "Allow to parse data directly into sparse columns\n**Use case**\r\n\r\nCreate `Sparse` column for parsing if it will be most likely written in sparse serialization according to columns' statistics. It will lower memory consumption for inserts and will probably increase the performance of inserts in case of data with high amount of default values. Especially it's interesting for formats in which some values may be missed in data (JSON), because now in that case size of uncompressed binary data in memory may grow a lot from the size of inserted text data for tables with large number of columns.\r\n\n",
  "hints_text": "",
  "created_at": "2024-09-20T15:09:48Z"
}