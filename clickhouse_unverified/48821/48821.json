{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 48821,
  "instance_id": "ClickHouse__ClickHouse-48821",
  "issue_numbers": [
    "33810"
  ],
  "base_commit": "c08d11c97b4f0f1b5ec6f1576baa3625a94d01a4",
  "patch": "diff --git a/programs/local/LocalServer.cpp b/programs/local/LocalServer.cpp\nindex caca7cfb50d2..2afcd48dafb1 100644\n--- a/programs/local/LocalServer.cpp\n+++ b/programs/local/LocalServer.cpp\n@@ -8,7 +8,9 @@\n #include <Poco/Logger.h>\n #include <Poco/NullChannel.h>\n #include <Poco/SimpleFileChannel.h>\n+#include <Databases/DatabaseFilesystem.h>\n #include <Databases/DatabaseMemory.h>\n+#include <Databases/DatabasesOverlay.h>\n #include <Storages/System/attachSystemTables.h>\n #include <Storages/System/attachInformationSchemaTables.h>\n #include <Interpreters/DatabaseCatalog.h>\n@@ -50,6 +52,8 @@\n #include <base/argsToConfig.h>\n #include <filesystem>\n \n+#include \"config.h\"\n+\n #if defined(FUZZING_MODE)\n     #include <Functions/getFuzzerData.h>\n #endif\n@@ -170,6 +174,13 @@ static DatabasePtr createMemoryDatabaseIfNotExists(ContextPtr context, const Str\n     return system_database;\n }\n \n+static DatabasePtr createClickHouseLocalDatabaseOverlay(const String & name_, ContextPtr context_)\n+{\n+    auto databaseCombiner = std::make_shared<DatabasesOverlay>(name_, context_);\n+    databaseCombiner->registerNextDatabase(std::make_shared<DatabaseFilesystem>(name_, \"\", context_));\n+    databaseCombiner->registerNextDatabase(std::make_shared<DatabaseMemory>(name_, context_));\n+    return databaseCombiner;\n+}\n \n /// If path is specified and not empty, will try to setup server environment and load existing metadata\n void LocalServer::tryInitPath()\n@@ -669,7 +680,7 @@ void LocalServer::processConfig()\n       *  if such tables will not be dropped, clickhouse-server will not be able to load them due to security reasons.\n       */\n     std::string default_database = config().getString(\"default_database\", \"_local\");\n-    DatabaseCatalog::instance().attachDatabase(default_database, std::make_shared<DatabaseMemory>(default_database, global_context));\n+    DatabaseCatalog::instance().attachDatabase(default_database, createClickHouseLocalDatabaseOverlay(default_database, global_context));\n     global_context->setCurrentDatabase(default_database);\n     applyCmdOptions(global_context);\n \ndiff --git a/src/Databases/DatabaseFactory.cpp b/src/Databases/DatabaseFactory.cpp\nindex e1c8afa52c0a..9d90c61bb41b 100644\n--- a/src/Databases/DatabaseFactory.cpp\n+++ b/src/Databases/DatabaseFactory.cpp\n@@ -3,6 +3,7 @@\n #include <filesystem>\n #include <Databases/DatabaseAtomic.h>\n #include <Databases/DatabaseDictionary.h>\n+#include <Databases/DatabaseFilesystem.h>\n #include <Databases/DatabaseLazy.h>\n #include <Databases/DatabaseMemory.h>\n #include <Databases/DatabaseOrdinary.h>\n@@ -47,6 +48,14 @@\n #include <Databases/SQLite/DatabaseSQLite.h>\n #endif\n \n+#if USE_AWS_S3\n+#include <Databases/DatabaseS3.h>\n+#endif\n+\n+#if USE_HDFS\n+#include <Databases/DatabaseHDFS.h>\n+#endif\n+\n namespace fs = std::filesystem;\n \n namespace DB\n@@ -131,13 +140,13 @@ DatabasePtr DatabaseFactory::getImpl(const ASTCreateQuery & create, const String\n \n     static const std::unordered_set<std::string_view> database_engines{\"Ordinary\", \"Atomic\", \"Memory\",\n         \"Dictionary\", \"Lazy\", \"Replicated\", \"MySQL\", \"MaterializeMySQL\", \"MaterializedMySQL\",\n-        \"PostgreSQL\", \"MaterializedPostgreSQL\", \"SQLite\"};\n+        \"PostgreSQL\", \"MaterializedPostgreSQL\", \"SQLite\", \"Filesystem\", \"S3\", \"HDFS\"};\n \n     if (!database_engines.contains(engine_name))\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Database engine name `{}` does not exist\", engine_name);\n \n     static const std::unordered_set<std::string_view> engines_with_arguments{\"MySQL\", \"MaterializeMySQL\", \"MaterializedMySQL\",\n-        \"Lazy\", \"Replicated\", \"PostgreSQL\", \"MaterializedPostgreSQL\", \"SQLite\"};\n+        \"Lazy\", \"Replicated\", \"PostgreSQL\", \"MaterializedPostgreSQL\", \"SQLite\", \"Filesystem\", \"S3\", \"HDFS\"};\n \n     static const std::unordered_set<std::string_view> engines_with_table_overrides{\"MaterializeMySQL\", \"MaterializedMySQL\", \"MaterializedPostgreSQL\"};\n     bool engine_may_have_arguments = engines_with_arguments.contains(engine_name);\n@@ -432,6 +441,63 @@ DatabasePtr DatabaseFactory::getImpl(const ASTCreateQuery & create, const String\n     }\n #endif\n \n+    else if (engine_name == \"Filesystem\")\n+    {\n+        const ASTFunction * engine = engine_define->engine;\n+\n+        /// If init_path is empty, then the current path will be used\n+        std::string init_path;\n+\n+        if (engine->arguments && !engine->arguments->children.empty())\n+        {\n+            if (engine->arguments->children.size() != 1)\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Filesystem database requires at most 1 argument: filesystem_path\");\n+\n+            const auto & arguments = engine->arguments->children;\n+            init_path = safeGetLiteralValue<String>(arguments[0], engine_name);\n+        }\n+\n+        return std::make_shared<DatabaseFilesystem>(database_name, init_path, context);\n+    }\n+\n+#if USE_AWS_S3\n+    else if (engine_name == \"S3\")\n+    {\n+        const ASTFunction * engine = engine_define->engine;\n+\n+        DatabaseS3::Configuration config;\n+\n+        if (engine->arguments && !engine->arguments->children.empty())\n+        {\n+            ASTs & engine_args = engine->arguments->children;\n+            config = DatabaseS3::parseArguments(engine_args, context);\n+        }\n+\n+        return std::make_shared<DatabaseS3>(database_name, config, context);\n+    }\n+#endif\n+\n+#if USE_HDFS\n+    else if (engine_name == \"HDFS\")\n+    {\n+        const ASTFunction * engine = engine_define->engine;\n+\n+        /// If source_url is empty, then table name must contain full url\n+        std::string source_url;\n+\n+        if (engine->arguments && !engine->arguments->children.empty())\n+        {\n+            if (engine->arguments->children.size() != 1)\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, \"HDFS database requires at most 1 argument: source_url\");\n+\n+            const auto & arguments = engine->arguments->children;\n+            source_url = safeGetLiteralValue<String>(arguments[0], engine_name);\n+        }\n+\n+        return std::make_shared<DatabaseHDFS>(database_name, source_url, context);\n+    }\n+#endif\n+\n     throw Exception(ErrorCodes::UNKNOWN_DATABASE_ENGINE, \"Unknown database engine: {}\", engine_name);\n }\n \ndiff --git a/src/Databases/DatabaseFilesystem.cpp b/src/Databases/DatabaseFilesystem.cpp\nnew file mode 100644\nindex 000000000000..001aa1f9ef61\n--- /dev/null\n+++ b/src/Databases/DatabaseFilesystem.cpp\n@@ -0,0 +1,247 @@\n+#include <Databases/DatabaseFilesystem.h>\n+\n+#include <IO/Operators.h>\n+#include <IO/WriteBufferFromString.h>\n+#include <Interpreters/Context.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Parsers/ParserCreateQuery.h>\n+#include <Parsers/parseQuery.h>\n+#include <Storages/IStorage.h>\n+#include <TableFunctions/TableFunctionFactory.h>\n+#include <Common/filesystemHelpers.h>\n+\n+#include <filesystem>\n+\n+namespace fs = std::filesystem;\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int UNKNOWN_TABLE;\n+    extern const int DATABASE_ACCESS_DENIED;\n+    extern const int BAD_ARGUMENTS;\n+    extern const int FILE_DOESNT_EXIST;\n+}\n+\n+DatabaseFilesystem::DatabaseFilesystem(const String & name_, const String & path_, ContextPtr context_)\n+    : IDatabase(name_), WithContext(context_->getGlobalContext()), path(path_), log(&Poco::Logger::get(\"DatabaseFileSystem(\" + name_ + \")\"))\n+{\n+    bool is_local = context_->getApplicationType() == Context::ApplicationType::LOCAL;\n+    fs::path user_files_path = is_local ? \"\" : fs::canonical(getContext()->getUserFilesPath());\n+\n+    if (fs::path(path).is_relative())\n+    {\n+        path = user_files_path / path;\n+    }\n+    else if (!is_local && !pathStartsWith(fs::path(path), user_files_path))\n+    {\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+                        \"Path must be inside user-files path: {}\", user_files_path.string());\n+    }\n+\n+    path = fs::absolute(path).lexically_normal();\n+    if (!fs::exists(path))\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Path does not exist: {}\", path);\n+}\n+\n+std::string DatabaseFilesystem::getTablePath(const std::string & table_name) const\n+{\n+    fs::path table_path = fs::path(path) / table_name;\n+    return table_path.lexically_normal().string();\n+}\n+\n+void DatabaseFilesystem::addTable(const std::string & table_name, StoragePtr table_storage) const\n+{\n+    std::lock_guard lock(mutex);\n+    auto [_, inserted] = loaded_tables.emplace(table_name, table_storage);\n+    if (!inserted)\n+        throw Exception(\n+            ErrorCodes::LOGICAL_ERROR,\n+            \"Table with name `{}` already exists in database `{}` (engine {})\",\n+            table_name, getDatabaseName(), getEngineName());\n+}\n+\n+bool DatabaseFilesystem::checkTableFilePath(const std::string & table_path, ContextPtr context_, bool throw_on_error) const\n+{\n+    /// If run in Local mode, no need for path checking.\n+    bool check_path = context_->getApplicationType() != Context::ApplicationType::LOCAL;\n+    const auto & user_files_path = context_->getUserFilesPath();\n+\n+    /// Check access for file before checking its existence.\n+    if (check_path && !fileOrSymlinkPathStartsWith(table_path, user_files_path))\n+    {\n+        if (throw_on_error)\n+            throw Exception(ErrorCodes::DATABASE_ACCESS_DENIED, \"File is not inside {}\", user_files_path);\n+        else\n+            return false;\n+    }\n+\n+    /// Check if the corresponding file exists.\n+    if (!fs::exists(table_path))\n+    {\n+        if (throw_on_error)\n+            throw Exception(ErrorCodes::FILE_DOESNT_EXIST, \"File does not exist: {}\", table_path);\n+        else\n+            return false;\n+    }\n+\n+    if (!fs::is_regular_file(table_path))\n+    {\n+        if (throw_on_error)\n+            throw Exception(ErrorCodes::FILE_DOESNT_EXIST,\n+                            \"File is directory, but expected a file: {}\", table_path);\n+        else\n+            return false;\n+    }\n+\n+    return true;\n+}\n+\n+StoragePtr DatabaseFilesystem::tryGetTableFromCache(const std::string & name) const\n+{\n+    StoragePtr table = nullptr;\n+    {\n+        std::lock_guard lock(mutex);\n+        auto it = loaded_tables.find(name);\n+        if (it != loaded_tables.end())\n+            table = it->second;\n+    }\n+\n+    /// Invalidate cache if file no longer exists.\n+    if (table && !fs::exists(getTablePath(name)))\n+    {\n+        std::lock_guard lock(mutex);\n+        loaded_tables.erase(name);\n+        return nullptr;\n+    }\n+\n+    return table;\n+}\n+\n+bool DatabaseFilesystem::isTableExist(const String & name, ContextPtr context_) const\n+{\n+    if (tryGetTableFromCache(name))\n+        return true;\n+\n+    return checkTableFilePath(getTablePath(name), context_, /* throw_on_error */false);\n+}\n+\n+StoragePtr DatabaseFilesystem::getTableImpl(const String & name, ContextPtr context_) const\n+{\n+    /// Check if table exists in loaded tables map.\n+    if (auto table = tryGetTableFromCache(name))\n+        return table;\n+\n+    auto table_path = getTablePath(name);\n+    checkTableFilePath(table_path, context_, /* throw_on_error */true);\n+\n+    /// If the file exists, create a new table using TableFunctionFile and return it.\n+    auto args = makeASTFunction(\"file\", std::make_shared<ASTLiteral>(table_path));\n+\n+    auto table_function = TableFunctionFactory::instance().get(args, context_);\n+    if (!table_function)\n+        return nullptr;\n+\n+    /// TableFunctionFile throws exceptions, if table cannot be created.\n+    auto table_storage = table_function->execute(args, context_, name);\n+    if (table_storage)\n+        addTable(name, table_storage);\n+\n+    return table_storage;\n+}\n+\n+StoragePtr DatabaseFilesystem::getTable(const String & name, ContextPtr context_) const\n+{\n+    /// getTableImpl can throw exceptions, do not catch them to show correct error to user.\n+    if (auto storage = getTableImpl(name, context_))\n+        return storage;\n+\n+    throw Exception(ErrorCodes::UNKNOWN_TABLE, \"Table {}.{} doesn't exist\",\n+                    backQuoteIfNeed(getDatabaseName()), backQuoteIfNeed(name));\n+}\n+\n+StoragePtr DatabaseFilesystem::tryGetTable(const String & name, ContextPtr context_) const\n+{\n+    try\n+    {\n+        return getTableImpl(name, context_);\n+    }\n+    catch (const Exception & e)\n+    {\n+        /// Ignore exceptions thrown by TableFunctionFile, which indicate that there is no table\n+        /// see tests/02722_database_filesystem.sh for more details.\n+        if (e.code() == ErrorCodes::BAD_ARGUMENTS\n+            || e.code() == ErrorCodes::DATABASE_ACCESS_DENIED\n+            || e.code() == ErrorCodes::FILE_DOESNT_EXIST)\n+        {\n+            return nullptr;\n+        }\n+        throw;\n+    }\n+}\n+\n+bool DatabaseFilesystem::empty() const\n+{\n+    std::lock_guard lock(mutex);\n+    return loaded_tables.empty();\n+}\n+\n+ASTPtr DatabaseFilesystem::getCreateDatabaseQuery() const\n+{\n+    const auto & settings = getContext()->getSettingsRef();\n+    const String query = fmt::format(\"CREATE DATABASE {} ENGINE = Filesystem('{}')\", backQuoteIfNeed(getDatabaseName()), path);\n+\n+    ParserCreateQuery parser;\n+    ASTPtr ast = parseQuery(parser, query.data(), query.data() + query.size(), \"\", 0, settings.max_parser_depth);\n+\n+    if (const auto database_comment = getDatabaseComment(); !database_comment.empty())\n+    {\n+        auto & ast_create_query = ast->as<ASTCreateQuery &>();\n+        ast_create_query.set(ast_create_query.comment, std::make_shared<ASTLiteral>(database_comment));\n+    }\n+\n+    return ast;\n+}\n+\n+void DatabaseFilesystem::shutdown()\n+{\n+    Tables tables_snapshot;\n+    {\n+        std::lock_guard lock(mutex);\n+        tables_snapshot = loaded_tables;\n+    }\n+\n+    for (const auto & kv : tables_snapshot)\n+    {\n+        auto table_id = kv.second->getStorageID();\n+        kv.second->flushAndShutdown();\n+    }\n+\n+    std::lock_guard lock(mutex);\n+    loaded_tables.clear();\n+}\n+\n+/**\n+ * Returns an empty vector because the database is read-only and no tables can be backed up\n+ */\n+std::vector<std::pair<ASTPtr, StoragePtr>> DatabaseFilesystem::getTablesForBackup(const FilterByNameFunction &, const ContextPtr &) const\n+{\n+    return {};\n+}\n+\n+/**\n+ *\n+ * Returns an empty iterator because the database does not have its own tables\n+ * But only caches them for quick access\n+ */\n+DatabaseTablesIteratorPtr DatabaseFilesystem::getTablesIterator(ContextPtr, const FilterByNameFunction &) const\n+{\n+    return std::make_unique<DatabaseTablesSnapshotIterator>(Tables{}, getDatabaseName());\n+}\n+\n+}\ndiff --git a/src/Databases/DatabaseFilesystem.h b/src/Databases/DatabaseFilesystem.h\nnew file mode 100644\nindex 000000000000..7fe620401dc6\n--- /dev/null\n+++ b/src/Databases/DatabaseFilesystem.h\n@@ -0,0 +1,67 @@\n+#pragma once\n+\n+#include <mutex>\n+#include <Databases/IDatabase.h>\n+#include <Parsers/IAST.h>\n+#include <Storages/IStorage_fwd.h>\n+#include <base/types.h>\n+\n+namespace DB\n+{\n+\n+class Context;\n+\n+/**\n+  * DatabaseFilesystem allows to interact with files stored on the local filesystem.\n+  * Uses TableFunctionFile to implicitly load file when a user requests the table,\n+  * and provides a read-only access to the data in the file.\n+  * Tables are cached inside the database for quick access\n+  *\n+  * Used in clickhouse-local to access local files.\n+  * For clickhouse-server requires allows to access file only from user_files directory.\n+  */\n+class DatabaseFilesystem : public IDatabase, protected WithContext\n+{\n+public:\n+    DatabaseFilesystem(const String & name, const String & path, ContextPtr context);\n+\n+    String getEngineName() const override { return \"Filesystem\"; }\n+\n+    bool isTableExist(const String & name, ContextPtr context) const override;\n+\n+    StoragePtr getTable(const String & name, ContextPtr context) const override;\n+\n+    StoragePtr tryGetTable(const String & name, ContextPtr context) const override;\n+\n+    bool shouldBeEmptyOnDetach() const override { return false; } /// Contains only temporary tables.\n+\n+    bool empty() const override;\n+\n+    bool isReadOnly() const override { return true; }\n+\n+    ASTPtr getCreateDatabaseQuery() const override;\n+\n+    void shutdown() override;\n+\n+    std::vector<std::pair<ASTPtr, StoragePtr>> getTablesForBackup(const FilterByNameFunction &, const ContextPtr &) const override;\n+\n+    DatabaseTablesIteratorPtr getTablesIterator(ContextPtr, const FilterByNameFunction &) const override;\n+\n+protected:\n+    StoragePtr getTableImpl(const String & name, ContextPtr context) const;\n+\n+    StoragePtr tryGetTableFromCache(const std::string & name) const;\n+\n+    std::string getTablePath(const std::string & table_name) const;\n+\n+    void addTable(const std::string & table_name, StoragePtr table_storage) const;\n+\n+    bool checkTableFilePath(const std::string & table_path, ContextPtr context_, bool throw_on_error) const;\n+\n+private:\n+    String path;\n+    mutable Tables loaded_tables TSA_GUARDED_BY(mutex);\n+    Poco::Logger * log;\n+};\n+\n+}\ndiff --git a/src/Databases/DatabaseHDFS.cpp b/src/Databases/DatabaseHDFS.cpp\nnew file mode 100644\nindex 000000000000..1a0145b90152\n--- /dev/null\n+++ b/src/Databases/DatabaseHDFS.cpp\n@@ -0,0 +1,234 @@\n+#include \"config.h\"\n+\n+#if USE_HDFS\n+\n+#include <Databases/DatabaseHDFS.h>\n+\n+#include <Interpreters/Context.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Parsers/parseQuery.h>\n+#include <Parsers/ParserCreateQuery.h>\n+#include <Storages/HDFS/HDFSCommon.h>\n+#include <Storages/IStorage.h>\n+#include <TableFunctions/TableFunctionFactory.h>\n+\n+#include <Poco/URI.h>\n+#include <re2/re2.h>\n+\n+#include <filesystem>\n+\n+namespace fs = std::filesystem;\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int UNKNOWN_TABLE;\n+    extern const int BAD_ARGUMENTS;\n+    extern const int FILE_DOESNT_EXIST;\n+    extern const int UNACCEPTABLE_URL;\n+    extern const int ACCESS_DENIED;\n+    extern const int DATABASE_ACCESS_DENIED;\n+    extern const int HDFS_ERROR;\n+    extern const int CANNOT_EXTRACT_TABLE_STRUCTURE;\n+}\n+\n+static constexpr std::string_view HDFS_HOST_REGEXP = \"^hdfs://[^/]*\";\n+\n+\n+DatabaseHDFS::DatabaseHDFS(const String & name_, const String & source_url, ContextPtr context_)\n+    : IDatabase(name_)\n+    , WithContext(context_->getGlobalContext())\n+    , source(source_url)\n+    , log(&Poco::Logger::get(\"DatabaseHDFS(\" + name_ + \")\"))\n+{\n+    if (!source.empty())\n+    {\n+        if (!re2::RE2::FullMatch(source, std::string(HDFS_HOST_REGEXP)))\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Bad hdfs host: {}. \"\n+                            \"It should have structure 'hdfs://<host_name>:<port>'\", source);\n+\n+        context_->getGlobalContext()->getRemoteHostFilter().checkURL(Poco::URI(source));\n+    }\n+}\n+\n+void DatabaseHDFS::addTable(const std::string & table_name, StoragePtr table_storage) const\n+{\n+    std::lock_guard lock(mutex);\n+    auto [_, inserted] = loaded_tables.emplace(table_name, table_storage);\n+    if (!inserted)\n+        throw Exception(\n+            ErrorCodes::LOGICAL_ERROR,\n+            \"Table with name `{}` already exists in database `{}` (engine {})\",\n+            table_name, getDatabaseName(), getEngineName());\n+}\n+\n+std::string DatabaseHDFS::getTablePath(const std::string & table_name) const\n+{\n+    if (table_name.starts_with(\"hdfs://\"))\n+        return table_name;\n+\n+    if (source.empty())\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Bad hdfs url: {}. \"\n+                        \"It should have structure 'hdfs://<host_name>:<port>/path'\", table_name);\n+\n+    return fs::path(source) / table_name;\n+}\n+\n+bool DatabaseHDFS::checkUrl(const std::string & url, ContextPtr context_, bool throw_on_error) const\n+{\n+    try\n+    {\n+        checkHDFSURL(url);\n+        context_->getGlobalContext()->getRemoteHostFilter().checkURL(Poco::URI(url));\n+    }\n+    catch (...)\n+    {\n+        if (throw_on_error)\n+            throw;\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n+bool DatabaseHDFS::isTableExist(const String & name, ContextPtr context_) const\n+{\n+    std::lock_guard lock(mutex);\n+    if (loaded_tables.find(name) != loaded_tables.end())\n+        return true;\n+\n+    return checkUrl(name, context_, false);\n+}\n+\n+StoragePtr DatabaseHDFS::getTableImpl(const String & name, ContextPtr context_) const\n+{\n+    /// Check if the table exists in the loaded tables map.\n+    {\n+        std::lock_guard lock(mutex);\n+        auto it = loaded_tables.find(name);\n+        if (it != loaded_tables.end())\n+            return it->second;\n+    }\n+\n+    auto url = getTablePath(name);\n+\n+    checkUrl(url, context_, true);\n+\n+    auto args = makeASTFunction(\"hdfs\", std::make_shared<ASTLiteral>(url));\n+\n+    auto table_function = TableFunctionFactory::instance().get(args, context_);\n+    if (!table_function)\n+        return nullptr;\n+\n+    /// TableFunctionHDFS throws exceptions, if table cannot be created.\n+    auto table_storage = table_function->execute(args, context_, name);\n+    if (table_storage)\n+        addTable(name, table_storage);\n+\n+    return table_storage;\n+}\n+\n+StoragePtr DatabaseHDFS::getTable(const String & name, ContextPtr context_) const\n+{\n+    /// Rethrow all exceptions from TableFunctionHDFS to show correct error to user.\n+    if (auto storage = getTableImpl(name, context_))\n+        return storage;\n+\n+    throw Exception(ErrorCodes::UNKNOWN_TABLE, \"Table {}.{} doesn't exist\",\n+                    backQuoteIfNeed(getDatabaseName()), backQuoteIfNeed(name));\n+}\n+\n+StoragePtr DatabaseHDFS::tryGetTable(const String & name, ContextPtr context_) const\n+{\n+    try\n+    {\n+        return getTableImpl(name, context_);\n+    }\n+    catch (const Exception & e)\n+    {\n+        // Ignore exceptions thrown by TableFunctionHDFS, which indicate that there is no table\n+        if (e.code() == ErrorCodes::BAD_ARGUMENTS\n+            || e.code() == ErrorCodes::ACCESS_DENIED\n+            || e.code() == ErrorCodes::DATABASE_ACCESS_DENIED\n+            || e.code() == ErrorCodes::FILE_DOESNT_EXIST\n+            || e.code() == ErrorCodes::UNACCEPTABLE_URL\n+            || e.code() == ErrorCodes::HDFS_ERROR\n+            || e.code() == ErrorCodes::CANNOT_EXTRACT_TABLE_STRUCTURE)\n+        {\n+            return nullptr;\n+        }\n+        throw;\n+    }\n+    catch (const Poco::URISyntaxException &)\n+    {\n+        return nullptr;\n+    }\n+}\n+\n+bool DatabaseHDFS::empty() const\n+{\n+    std::lock_guard lock(mutex);\n+    return loaded_tables.empty();\n+}\n+\n+ASTPtr DatabaseHDFS::getCreateDatabaseQuery() const\n+{\n+    const auto & settings = getContext()->getSettingsRef();\n+    ParserCreateQuery parser;\n+\n+    const String query = fmt::format(\"CREATE DATABASE {} ENGINE = HDFS('{}')\", backQuoteIfNeed(getDatabaseName()), source);\n+    ASTPtr ast = parseQuery(parser, query.data(), query.data() + query.size(), \"\", 0, settings.max_parser_depth);\n+\n+    if (const auto database_comment = getDatabaseComment(); !database_comment.empty())\n+    {\n+        auto & ast_create_query = ast->as<ASTCreateQuery &>();\n+        ast_create_query.set(ast_create_query.comment, std::make_shared<ASTLiteral>(database_comment));\n+    }\n+\n+    return ast;\n+}\n+\n+void DatabaseHDFS::shutdown()\n+{\n+    Tables tables_snapshot;\n+    {\n+        std::lock_guard lock(mutex);\n+        tables_snapshot = loaded_tables;\n+    }\n+\n+    for (const auto & kv : tables_snapshot)\n+    {\n+        auto table_id = kv.second->getStorageID();\n+        kv.second->flushAndShutdown();\n+    }\n+\n+    std::lock_guard lock(mutex);\n+    loaded_tables.clear();\n+}\n+\n+/**\n+ * Returns an empty vector because the database is read-only and no tables can be backed up\n+ */\n+std::vector<std::pair<ASTPtr, StoragePtr>> DatabaseHDFS::getTablesForBackup(const FilterByNameFunction &, const ContextPtr &) const\n+{\n+    return {};\n+}\n+\n+/**\n+ *\n+ * Returns an empty iterator because the database does not have its own tables\n+ * But only caches them for quick access\n+ */\n+DatabaseTablesIteratorPtr DatabaseHDFS::getTablesIterator(ContextPtr, const FilterByNameFunction &) const\n+{\n+    return std::make_unique<DatabaseTablesSnapshotIterator>(Tables{}, getDatabaseName());\n+}\n+\n+} // DB\n+\n+#endif\ndiff --git a/src/Databases/DatabaseHDFS.h b/src/Databases/DatabaseHDFS.h\nnew file mode 100644\nindex 000000000000..957b2080135c\n--- /dev/null\n+++ b/src/Databases/DatabaseHDFS.h\n@@ -0,0 +1,68 @@\n+#pragma once\n+\n+#include \"config.h\"\n+\n+#if USE_HDFS\n+\n+#include <mutex>\n+#include <Databases/IDatabase.h>\n+#include <Parsers/IAST.h>\n+#include <Storages/IStorage_fwd.h>\n+#include <base/types.h>\n+\n+namespace DB\n+{\n+\n+class Context;\n+\n+/**\n+  * DatabaseHDFS allows to interact with files stored on the file system.\n+  * Uses TableFunctionHDFS to implicitly load file when a user requests the table,\n+  * and provides read-only access to the data in the file.\n+  * Tables are cached inside the database for quick access.\n+  */\n+class DatabaseHDFS : public IDatabase, protected WithContext\n+{\n+public:\n+    DatabaseHDFS(const String & name, const String & source_url, ContextPtr context);\n+\n+    String getEngineName() const override { return \"S3\"; }\n+\n+    bool isTableExist(const String & name, ContextPtr context) const override;\n+\n+    StoragePtr getTable(const String & name, ContextPtr context) const override;\n+\n+    StoragePtr tryGetTable(const String & name, ContextPtr context) const override;\n+\n+    bool shouldBeEmptyOnDetach() const override { return false; } /// Contains only temporary tables.\n+\n+    bool empty() const override;\n+\n+    bool isReadOnly() const override { return true; }\n+\n+    ASTPtr getCreateDatabaseQuery() const override;\n+\n+    void shutdown() override;\n+\n+    std::vector<std::pair<ASTPtr, StoragePtr>> getTablesForBackup(const FilterByNameFunction &, const ContextPtr &) const override;\n+    DatabaseTablesIteratorPtr getTablesIterator(ContextPtr, const FilterByNameFunction &) const override;\n+\n+protected:\n+    StoragePtr getTableImpl(const String & name, ContextPtr context) const;\n+\n+    void addTable(const std::string & table_name, StoragePtr table_storage) const;\n+\n+    bool checkUrl(const std::string & url, ContextPtr context_, bool throw_on_error) const;\n+\n+    std::string getTablePath(const std::string & table_name) const;\n+\n+private:\n+    const String source;\n+\n+    mutable Tables loaded_tables TSA_GUARDED_BY(mutex);\n+    Poco::Logger * log;\n+};\n+\n+}\n+\n+#endif\ndiff --git a/src/Databases/DatabaseS3.cpp b/src/Databases/DatabaseS3.cpp\nnew file mode 100644\nindex 000000000000..11655f5f100f\n--- /dev/null\n+++ b/src/Databases/DatabaseS3.cpp\n@@ -0,0 +1,312 @@\n+#include \"config.h\"\n+\n+#if USE_AWS_S3\n+\n+#include <Databases/DatabaseS3.h>\n+\n+#include <Interpreters/Context.h>\n+#include <Interpreters/evaluateConstantExpression.h>\n+#include <IO/S3/URI.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Parsers/parseQuery.h>\n+#include <Parsers/ParserCreateQuery.h>\n+#include <Storages/checkAndGetLiteralArgument.h>\n+#include <Storages/IStorage.h>\n+#include <Storages/NamedCollectionsHelpers.h>\n+#include <TableFunctions/TableFunctionFactory.h>\n+\n+#include <boost/algorithm/string.hpp>\n+#include <filesystem>\n+\n+namespace fs = std::filesystem;\n+\n+namespace DB\n+{\n+\n+static const std::unordered_set<std::string_view> optional_configuration_keys = {\n+    \"url\",\n+    \"access_key_id\",\n+    \"secret_access_key\",\n+    \"no_sign_request\"\n+};\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int UNKNOWN_TABLE;\n+    extern const int BAD_ARGUMENTS;\n+    extern const int FILE_DOESNT_EXIST;\n+    extern const int UNACCEPTABLE_URL;\n+    extern const int S3_ERROR;\n+\n+    extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n+}\n+\n+DatabaseS3::DatabaseS3(const String & name_, const Configuration& config_, ContextPtr context_)\n+    : IDatabase(name_)\n+    , WithContext(context_->getGlobalContext())\n+    , config(config_)\n+    , log(&Poco::Logger::get(\"DatabaseS3(\" + name_ + \")\"))\n+{\n+}\n+\n+void DatabaseS3::addTable(const std::string & table_name, StoragePtr table_storage) const\n+{\n+    std::lock_guard lock(mutex);\n+    auto [_, inserted] = loaded_tables.emplace(table_name, table_storage);\n+    if (!inserted)\n+        throw Exception(\n+            ErrorCodes::LOGICAL_ERROR,\n+            \"Table with name `{}` already exists in database `{}` (engine {})\",\n+            table_name, getDatabaseName(), getEngineName());\n+}\n+\n+std::string DatabaseS3::getFullUrl(const std::string & name) const\n+{\n+    if (!config.url_prefix.empty())\n+        return fs::path(config.url_prefix) / name;\n+\n+    return name;\n+}\n+\n+bool DatabaseS3::checkUrl(const std::string & url, ContextPtr context_, bool throw_on_error) const\n+{\n+    try\n+    {\n+        S3::URI uri(url);\n+        context_->getGlobalContext()->getRemoteHostFilter().checkURL(uri.uri);\n+    }\n+    catch (...)\n+    {\n+        if (throw_on_error)\n+            throw;\n+        return false;\n+    }\n+    return true;\n+}\n+\n+bool DatabaseS3::isTableExist(const String & name, ContextPtr context_) const\n+{\n+    std::lock_guard lock(mutex);\n+    if (loaded_tables.find(name) != loaded_tables.end())\n+        return true;\n+\n+    return checkUrl(getFullUrl(name), context_, false);\n+}\n+\n+StoragePtr DatabaseS3::getTableImpl(const String & name, ContextPtr context_) const\n+{\n+    /// Check if the table exists in the loaded tables map.\n+    {\n+        std::lock_guard lock(mutex);\n+        auto it = loaded_tables.find(name);\n+        if (it != loaded_tables.end())\n+            return it->second;\n+    }\n+\n+    auto url = getFullUrl(name);\n+    checkUrl(url, context_, /* throw_on_error */true);\n+\n+    auto function = std::make_shared<ASTFunction>();\n+    function->name = \"s3\";\n+    function->arguments = std::make_shared<ASTExpressionList>();\n+    function->children.push_back(function->arguments);\n+\n+    function->arguments->children.push_back(std::make_shared<ASTLiteral>(url));\n+    if (config.no_sign_request)\n+    {\n+        function->arguments->children.push_back(std::make_shared<ASTLiteral>(\"NOSIGN\"));\n+    }\n+    else if (config.access_key_id.has_value() && config.secret_access_key.has_value())\n+    {\n+        function->arguments->children.push_back(std::make_shared<ASTLiteral>(config.access_key_id.value()));\n+        function->arguments->children.push_back(std::make_shared<ASTLiteral>(config.secret_access_key.value()));\n+    }\n+\n+    auto table_function = TableFunctionFactory::instance().get(function, context_);\n+    if (!table_function)\n+        return nullptr;\n+\n+    /// TableFunctionS3 throws exceptions, if table cannot be created.\n+    auto table_storage = table_function->execute(function, context_, name);\n+    if (table_storage)\n+        addTable(name, table_storage);\n+\n+    return table_storage;\n+}\n+\n+StoragePtr DatabaseS3::getTable(const String & name, ContextPtr context_) const\n+{\n+    /// Rethrow all exceptions from TableFunctionS3 to show correct error to user.\n+    if (auto storage = getTableImpl(name, context_))\n+        return storage;\n+\n+    throw Exception(ErrorCodes::UNKNOWN_TABLE, \"Table {}.{} doesn't exist\",\n+                    backQuoteIfNeed(getDatabaseName()), backQuoteIfNeed(name));\n+}\n+\n+StoragePtr DatabaseS3::tryGetTable(const String & name, ContextPtr context_) const\n+{\n+    try\n+    {\n+        return getTableImpl(name, context_);\n+    }\n+    catch (const Exception & e)\n+    {\n+        /// Ignore exceptions thrown by TableFunctionS3, which indicate that there is no table.\n+        if (e.code() == ErrorCodes::BAD_ARGUMENTS\n+            || e.code() == ErrorCodes::S3_ERROR\n+            || e.code() == ErrorCodes::FILE_DOESNT_EXIST\n+            || e.code() == ErrorCodes::UNACCEPTABLE_URL)\n+        {\n+            return nullptr;\n+        }\n+        throw;\n+    }\n+    catch (const Poco::URISyntaxException &)\n+    {\n+        return nullptr;\n+    }\n+}\n+\n+bool DatabaseS3::empty() const\n+{\n+    std::lock_guard lock(mutex);\n+    return loaded_tables.empty();\n+}\n+\n+ASTPtr DatabaseS3::getCreateDatabaseQuery() const\n+{\n+    const auto & settings = getContext()->getSettingsRef();\n+    ParserCreateQuery parser;\n+\n+    std::string creation_args;\n+    creation_args += fmt::format(\"'{}'\", config.url_prefix);\n+    if (config.no_sign_request)\n+        creation_args += \", 'NOSIGN'\";\n+    else if (config.access_key_id.has_value() && config.secret_access_key.has_value())\n+        creation_args += fmt::format(\", '{}', '{}'\", config.access_key_id.value(), config.secret_access_key.value());\n+\n+    const String query = fmt::format(\"CREATE DATABASE {} ENGINE = S3({})\", backQuoteIfNeed(getDatabaseName()), creation_args);\n+    ASTPtr ast = parseQuery(parser, query.data(), query.data() + query.size(), \"\", 0, settings.max_parser_depth);\n+\n+    if (const auto database_comment = getDatabaseComment(); !database_comment.empty())\n+    {\n+        auto & ast_create_query = ast->as<ASTCreateQuery &>();\n+        ast_create_query.set(ast_create_query.comment, std::make_shared<ASTLiteral>(database_comment));\n+    }\n+\n+    return ast;\n+}\n+\n+void DatabaseS3::shutdown()\n+{\n+    Tables tables_snapshot;\n+    {\n+        std::lock_guard lock(mutex);\n+        tables_snapshot = loaded_tables;\n+    }\n+\n+    for (const auto & kv : tables_snapshot)\n+    {\n+        auto table_id = kv.second->getStorageID();\n+        kv.second->flushAndShutdown();\n+    }\n+\n+    std::lock_guard lock(mutex);\n+    loaded_tables.clear();\n+}\n+\n+DatabaseS3::Configuration DatabaseS3::parseArguments(ASTs engine_args, ContextPtr context_)\n+{\n+    Configuration result;\n+\n+    if (auto named_collection = tryGetNamedCollectionWithOverrides(engine_args, context_))\n+    {\n+        auto & collection = *named_collection;\n+\n+        validateNamedCollection(collection, {}, optional_configuration_keys);\n+\n+        result.url_prefix = collection.getOrDefault<String>(\"url\", \"\");\n+        result.no_sign_request = collection.getOrDefault<bool>(\"no_sign_request\", false);\n+\n+        auto key_id = collection.getOrDefault<String>(\"access_key_id\", \"\");\n+        auto secret_key = collection.getOrDefault<String>(\"secret_access_key\", \"\");\n+\n+        if (!key_id.empty())\n+            result.access_key_id = key_id;\n+\n+        if (!secret_key.empty())\n+            result.secret_access_key = secret_key;\n+    }\n+    else\n+    {\n+        const std::string supported_signature =\n+            \" - S3()\\n\"\n+            \" - S3('url')\\n\"\n+            \" - S3('url', 'NOSIGN')\\n\"\n+            \" - S3('url', 'access_key_id', 'secret_access_key')\\n\";\n+        const auto error_message =\n+            fmt::format(\"Engine DatabaseS3 must have the following arguments signature\\n{}\", supported_signature);\n+\n+        for (auto & arg : engine_args)\n+            arg = evaluateConstantExpressionOrIdentifierAsLiteral(arg, context_);\n+\n+        if (engine_args.size() > 3)\n+            throw Exception(ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH, error_message.c_str());\n+\n+        if (engine_args.empty())\n+            return result;\n+\n+        result.url_prefix = checkAndGetLiteralArgument<String>(engine_args[0], \"url\");\n+\n+        // url, NOSIGN\n+        if (engine_args.size() == 2)\n+        {\n+            auto second_arg = checkAndGetLiteralArgument<String>(engine_args[1], \"NOSIGN\");\n+            if (boost::iequals(second_arg, \"NOSIGN\"))\n+                result.no_sign_request = true;\n+            else\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, error_message.c_str());\n+        }\n+\n+        // url, access_key_id, secret_access_key\n+        if (engine_args.size() == 3)\n+        {\n+            auto key_id = checkAndGetLiteralArgument<String>(engine_args[1], \"access_key_id\");\n+            auto secret_key = checkAndGetLiteralArgument<String>(engine_args[2], \"secret_access_key\");\n+\n+            if (key_id.empty() || secret_key.empty() || boost::iequals(key_id, \"NOSIGN\"))\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, error_message.c_str());\n+\n+            result.access_key_id = key_id;\n+            result.secret_access_key = secret_key;\n+        }\n+    }\n+\n+    return result;\n+}\n+\n+/**\n+ * Returns an empty vector because the database is read-only and no tables can be backed up\n+ */\n+std::vector<std::pair<ASTPtr, StoragePtr>> DatabaseS3::getTablesForBackup(const FilterByNameFunction &, const ContextPtr &) const\n+{\n+    return {};\n+}\n+\n+/**\n+ *\n+ * Returns an empty iterator because the database does not have its own tables\n+ * But only caches them for quick access\n+ */\n+DatabaseTablesIteratorPtr DatabaseS3::getTablesIterator(ContextPtr, const FilterByNameFunction &) const\n+{\n+    return std::make_unique<DatabaseTablesSnapshotIterator>(Tables{}, getDatabaseName());\n+}\n+\n+}\n+\n+#endif\ndiff --git a/src/Databases/DatabaseS3.h b/src/Databases/DatabaseS3.h\nnew file mode 100644\nindex 000000000000..8297ae4e02d6\n--- /dev/null\n+++ b/src/Databases/DatabaseS3.h\n@@ -0,0 +1,81 @@\n+#pragma once\n+\n+#include \"config.h\"\n+\n+#if USE_AWS_S3\n+\n+#include <mutex>\n+#include <Databases/IDatabase.h>\n+#include <Parsers/IAST.h>\n+#include <Storages/IStorage_fwd.h>\n+#include <base/types.h>\n+\n+namespace DB\n+{\n+\n+class Context;\n+\n+/**\n+  * DatabaseS3 provides access to data stored in S3.\n+  * Uses TableFunctionS3 to implicitly load file when a user requests the table,\n+  * and provides read-only access to the data in the file.\n+  * Tables are cached inside the database for quick access.\n+  */\n+class DatabaseS3 : public IDatabase, protected WithContext\n+{\n+public:\n+    struct Configuration\n+    {\n+        std::string url_prefix;\n+\n+        bool no_sign_request = false;\n+\n+        std::optional<std::string> access_key_id;\n+        std::optional<std::string> secret_access_key;\n+    };\n+\n+    DatabaseS3(const String & name, const Configuration& config, ContextPtr context);\n+\n+    String getEngineName() const override { return \"S3\"; }\n+\n+    bool isTableExist(const String & name, ContextPtr context) const override;\n+\n+    StoragePtr getTable(const String & name, ContextPtr context) const override;\n+\n+    StoragePtr tryGetTable(const String & name, ContextPtr context) const override;\n+\n+    // Contains only temporary tables\n+    bool shouldBeEmptyOnDetach() const override { return false; }\n+\n+    bool empty() const override;\n+\n+    bool isReadOnly() const override { return true; }\n+\n+    ASTPtr getCreateDatabaseQuery() const override;\n+\n+    void shutdown() override;\n+\n+    std::vector<std::pair<ASTPtr, StoragePtr>> getTablesForBackup(const FilterByNameFunction &, const ContextPtr &) const override;\n+    DatabaseTablesIteratorPtr getTablesIterator(ContextPtr, const FilterByNameFunction &) const override;\n+\n+    static Configuration parseArguments(ASTs engine_args, ContextPtr context);\n+\n+protected:\n+    StoragePtr getTableImpl(const String & name, ContextPtr context) const;\n+\n+    void addTable(const std::string & table_name, StoragePtr table_storage) const;\n+\n+    bool checkUrl(const std::string & url, ContextPtr context_, bool throw_on_error) const;\n+\n+    std::string getFullUrl(const std::string & name) const;\n+\n+private:\n+    const Configuration config;\n+\n+    mutable Tables loaded_tables TSA_GUARDED_BY(mutex);\n+    Poco::Logger * log;\n+};\n+\n+}\n+\n+#endif\ndiff --git a/src/Databases/DatabasesOverlay.cpp b/src/Databases/DatabasesOverlay.cpp\nnew file mode 100644\nindex 000000000000..b44a97980725\n--- /dev/null\n+++ b/src/Databases/DatabasesOverlay.cpp\n@@ -0,0 +1,266 @@\n+#include <Databases/DatabasesOverlay.h>\n+\n+#include <Common/typeid_cast.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/InterpreterCreateQuery.h>\n+#include <Parsers/ASTCreateQuery.h>\n+\n+#include <Storages/IStorage_fwd.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int CANNOT_GET_CREATE_TABLE_QUERY;\n+}\n+\n+DatabasesOverlay::DatabasesOverlay(const String & name_, ContextPtr context_)\n+    : IDatabase(name_), WithContext(context_->getGlobalContext()), log(&Poco::Logger::get(\"DatabaseOverlay(\" + name_ + \")\"))\n+{\n+}\n+\n+DatabasesOverlay & DatabasesOverlay::registerNextDatabase(DatabasePtr database)\n+{\n+    databases.push_back(std::move(database));\n+    return *this;\n+}\n+\n+bool DatabasesOverlay::isTableExist(const String & table_name, ContextPtr context_) const\n+{\n+    for (const auto & db : databases)\n+    {\n+        if (db->isTableExist(table_name, context_))\n+            return true;\n+    }\n+    return false;\n+}\n+\n+StoragePtr DatabasesOverlay::tryGetTable(const String & table_name, ContextPtr context_) const\n+{\n+    StoragePtr result = nullptr;\n+    for (const auto & db : databases)\n+    {\n+        result = db->tryGetTable(table_name, context_);\n+        if (result)\n+            break;\n+    }\n+    return result;\n+}\n+\n+void DatabasesOverlay::createTable(ContextPtr context_, const String & table_name, const StoragePtr & table, const ASTPtr & query)\n+{\n+    for (auto & db : databases)\n+    {\n+        if (!db->isReadOnly())\n+        {\n+            db->createTable(context_, table_name, table, query);\n+            return;\n+        }\n+    }\n+    throw Exception(\n+        ErrorCodes::LOGICAL_ERROR,\n+        \"There is no databases for CREATE TABLE `{}` query in database `{}` (engine {})\",\n+        table_name,\n+        getDatabaseName(),\n+        getEngineName());\n+}\n+\n+void DatabasesOverlay::dropTable(ContextPtr context_, const String & table_name, bool sync)\n+{\n+    for (auto & db : databases)\n+    {\n+        if (db->isTableExist(table_name, context_))\n+        {\n+            db->dropTable(context_, table_name, sync);\n+            return;\n+        }\n+    }\n+    throw Exception(\n+        ErrorCodes::LOGICAL_ERROR,\n+        \"There is no databases for DROP TABLE `{}` query in database `{}` (engine {})\",\n+        table_name,\n+        getDatabaseName(),\n+        getEngineName());\n+}\n+\n+void DatabasesOverlay::attachTable(\n+    ContextPtr context_, const String & table_name, const StoragePtr & table, const String & relative_table_path)\n+{\n+    for (auto & db : databases)\n+    {\n+        try\n+        {\n+            db->attachTable(context_, table_name, table, relative_table_path);\n+            return;\n+        }\n+        catch (...)\n+        {\n+            continue;\n+        }\n+    }\n+    throw Exception(\n+        ErrorCodes::LOGICAL_ERROR,\n+        \"There is no databases for ATTACH TABLE `{}` query in database `{}` (engine {})\",\n+        table_name,\n+        getDatabaseName(),\n+        getEngineName());\n+}\n+\n+StoragePtr DatabasesOverlay::detachTable(ContextPtr context_, const String & table_name)\n+{\n+    StoragePtr result = nullptr;\n+    for (auto & db : databases)\n+    {\n+        if (db->isTableExist(table_name, context_))\n+            return db->detachTable(context_, table_name);\n+    }\n+    throw Exception(\n+        ErrorCodes::LOGICAL_ERROR,\n+        \"There is no databases for DETACH TABLE `{}` query in database `{}` (engine {})\",\n+        table_name,\n+        getDatabaseName(),\n+        getEngineName());\n+}\n+\n+ASTPtr DatabasesOverlay::getCreateTableQueryImpl(const String & name, ContextPtr context_, bool throw_on_error) const\n+{\n+    ASTPtr result = nullptr;\n+    for (const auto & db : databases)\n+    {\n+        result = db->tryGetCreateTableQuery(name, context_);\n+        if (result)\n+            break;\n+    }\n+    if (!result && throw_on_error)\n+        throw Exception(\n+            ErrorCodes::CANNOT_GET_CREATE_TABLE_QUERY,\n+            \"There is no metadata of table `{}` in database `{}` (engine {})\",\n+            name,\n+            getDatabaseName(),\n+            getEngineName());\n+    return result;\n+}\n+\n+/*\n+ * DatabaseOverlay cannot be constructed by \"CREATE DATABASE\" query, as it is not a traditional ClickHouse database\n+ * To use DatabaseOverlay, it must be constructed programmatically in code\n+ */\n+ASTPtr DatabasesOverlay::getCreateDatabaseQuery() const\n+{\n+    return std::make_shared<ASTCreateQuery>();\n+}\n+\n+String DatabasesOverlay::getTableDataPath(const String & table_name) const\n+{\n+    String result;\n+    for (const auto & db : databases)\n+    {\n+        result = db->getTableDataPath(table_name);\n+        if (!result.empty())\n+            break;\n+    }\n+    return result;\n+}\n+\n+String DatabasesOverlay::getTableDataPath(const ASTCreateQuery & query) const\n+{\n+    String result;\n+    for (const auto & db : databases)\n+    {\n+        result = db->getTableDataPath(query);\n+        if (!result.empty())\n+            break;\n+    }\n+    return result;\n+}\n+\n+UUID DatabasesOverlay::tryGetTableUUID(const String & table_name) const\n+{\n+    UUID result = UUIDHelpers::Nil;\n+    for (const auto & db : databases)\n+    {\n+        result = db->tryGetTableUUID(table_name);\n+        if (result != UUIDHelpers::Nil)\n+            break;\n+    }\n+    return result;\n+}\n+\n+void DatabasesOverlay::drop(ContextPtr context_)\n+{\n+    for (auto & db : databases)\n+        db->drop(context_);\n+}\n+\n+void DatabasesOverlay::alterTable(ContextPtr local_context, const StorageID & table_id, const StorageInMemoryMetadata & metadata)\n+{\n+    for (auto & db : databases)\n+    {\n+        if (!db->isReadOnly() && db->isTableExist(table_id.table_name, local_context))\n+        {\n+            db->alterTable(local_context, table_id, metadata);\n+            return;\n+        }\n+    }\n+    throw Exception(\n+        ErrorCodes::LOGICAL_ERROR,\n+        \"There is no databases for ALTER TABLE `{}` query in database `{}` (engine {})\",\n+        table_id.table_name,\n+        getDatabaseName(),\n+        getEngineName());\n+}\n+\n+std::vector<std::pair<ASTPtr, StoragePtr>>\n+DatabasesOverlay::getTablesForBackup(const FilterByNameFunction & filter, const ContextPtr & local_context) const\n+{\n+    std::vector<std::pair<ASTPtr, StoragePtr>> result;\n+    for (const auto & db : databases)\n+    {\n+        auto db_backup = db->getTablesForBackup(filter, local_context);\n+        result.insert(result.end(), std::make_move_iterator(db_backup.begin()), std::make_move_iterator(db_backup.end()));\n+    }\n+    return result;\n+}\n+\n+void DatabasesOverlay::createTableRestoredFromBackup(\n+    const ASTPtr & create_table_query,\n+    ContextMutablePtr local_context,\n+    std::shared_ptr<IRestoreCoordination> /*restore_coordination*/,\n+    UInt64 /*timeout_ms*/)\n+{\n+    /// Creates a tables by executing a \"CREATE TABLE\" query.\n+    InterpreterCreateQuery interpreter{create_table_query, local_context};\n+    interpreter.setInternal(true);\n+    interpreter.execute();\n+}\n+\n+bool DatabasesOverlay::empty() const\n+{\n+    for (const auto & db : databases)\n+    {\n+        if (!db->empty())\n+            return false;\n+    }\n+    return true;\n+}\n+\n+void DatabasesOverlay::shutdown()\n+{\n+    for (auto & db : databases)\n+        db->shutdown();\n+}\n+\n+DatabaseTablesIteratorPtr DatabasesOverlay::getTablesIterator(ContextPtr context_, const FilterByNameFunction & filter_by_table_name) const\n+{\n+    Tables tables;\n+    for (const auto & db : databases)\n+    {\n+        for (auto table_it = db->getTablesIterator(context_, filter_by_table_name); table_it->isValid(); table_it->next())\n+            tables.insert({table_it->name(), table_it->table()});\n+    }\n+    return std::make_unique<DatabaseTablesSnapshotIterator>(std::move(tables), getDatabaseName());\n+}\n+\n+}\ndiff --git a/src/Databases/DatabasesOverlay.h b/src/Databases/DatabasesOverlay.h\nnew file mode 100644\nindex 000000000000..0f31bbd6a47f\n--- /dev/null\n+++ b/src/Databases/DatabasesOverlay.h\n@@ -0,0 +1,66 @@\n+#pragma once\n+\n+#include <Storages/IStorage_fwd.h>\n+#include <Databases/IDatabase.h>\n+\n+namespace DB\n+{\n+\n+/**\n+ * Implements the IDatabase interface and combines multiple other databases\n+ * Searches for tables in each database in order until found, and delegates operations to the appropriate database\n+ * Useful for combining databases\n+ *\n+ * Used in clickhouse-local to combine DatabaseFileSystem and DatabaseMemory\n+ */\n+class DatabasesOverlay : public IDatabase, protected WithContext\n+{\n+public:\n+    DatabasesOverlay(const String & name_, ContextPtr context_);\n+\n+    /// Not thread-safe. Use only as factory to initialize database\n+    DatabasesOverlay & registerNextDatabase(DatabasePtr database);\n+\n+    String getEngineName() const override { return \"Overlay\"; }\n+\n+public:\n+    bool isTableExist(const String & table_name, ContextPtr context) const override;\n+\n+    StoragePtr tryGetTable(const String & table_name, ContextPtr context) const override;\n+\n+    void createTable(ContextPtr context, const String & table_name, const StoragePtr & table, const ASTPtr & query) override;\n+\n+    void dropTable(ContextPtr context, const String & table_name, bool sync) override;\n+\n+    void attachTable(ContextPtr context, const String & table_name, const StoragePtr & table, const String & relative_table_path) override;\n+\n+    StoragePtr detachTable(ContextPtr context, const String & table_name) override;\n+\n+    ASTPtr getCreateTableQueryImpl(const String & name, ContextPtr context, bool throw_on_error) const override;\n+    ASTPtr getCreateDatabaseQuery() const override;\n+\n+    String getTableDataPath(const String & table_name) const override;\n+    String getTableDataPath(const ASTCreateQuery & query) const override;\n+\n+    UUID tryGetTableUUID(const String & table_name) const override;\n+\n+    void drop(ContextPtr context) override;\n+\n+    void alterTable(ContextPtr local_context, const StorageID & table_id, const StorageInMemoryMetadata & metadata) override;\n+\n+    std::vector<std::pair<ASTPtr, StoragePtr>> getTablesForBackup(const FilterByNameFunction & filter, const ContextPtr & local_context) const override;\n+\n+    void createTableRestoredFromBackup(const ASTPtr & create_table_query, ContextMutablePtr local_context, std::shared_ptr<IRestoreCoordination> restore_coordination, UInt64 timeout_ms) override;\n+\n+    DatabaseTablesIteratorPtr getTablesIterator(ContextPtr context, const FilterByNameFunction & filter_by_table_name) const override;\n+\n+    bool empty() const override;\n+\n+    void shutdown() override;\n+\n+protected:\n+    std::vector<DatabasePtr> databases;\n+    Poco::Logger * log;\n+};\n+\n+}\ndiff --git a/src/Databases/IDatabase.h b/src/Databases/IDatabase.h\nindex 53a2f3728149..6508e2ce0600 100644\n--- a/src/Databases/IDatabase.h\n+++ b/src/Databases/IDatabase.h\n@@ -170,7 +170,7 @@ class IDatabase : public std::enable_shared_from_this<IDatabase>\n     /// Get the table for work. Return nullptr if there is no table.\n     virtual StoragePtr tryGetTable(const String & name, ContextPtr context) const = 0;\n \n-    StoragePtr getTable(const String & name, ContextPtr context) const;\n+    virtual StoragePtr getTable(const String & name, ContextPtr context) const;\n \n     virtual UUID tryGetTableUUID(const String & /*table_name*/) const { return UUIDHelpers::Nil; }\n \n@@ -183,6 +183,8 @@ class IDatabase : public std::enable_shared_from_this<IDatabase>\n     /// Is the database empty.\n     virtual bool empty() const = 0;\n \n+    virtual bool isReadOnly() const { return false; }\n+\n     /// Add the table to the database. Record its presence in the metadata.\n     virtual void createTable(\n         ContextPtr /*context*/,\ndiff --git a/src/Interpreters/DatabaseCatalog.cpp b/src/Interpreters/DatabaseCatalog.cpp\nindex 8d3fa91a7fec..129323cd6b32 100644\n--- a/src/Interpreters/DatabaseCatalog.cpp\n+++ b/src/Interpreters/DatabaseCatalog.cpp\n@@ -338,9 +338,17 @@ DatabaseAndTable DatabaseCatalog::getTableImpl(\n         database = it->second;\n     }\n \n-    auto table = database->tryGetTable(table_id.table_name, context_);\n-    if (!table && exception)\n-            exception->emplace(Exception(ErrorCodes::UNKNOWN_TABLE, \"Table {} doesn't exist\", table_id.getNameForLogs()));\n+    StoragePtr table = nullptr;\n+    try\n+    {\n+        table = database->getTable(table_id.table_name, context_);\n+    }\n+    catch (const Exception & e)\n+    {\n+        if (exception)\n+            exception->emplace(e);\n+    }\n+\n     if (!table)\n         database = nullptr;\n \n",
  "test_patch": "diff --git a/tests/config/config.d/named_collection.xml b/tests/config/config.d/named_collection.xml\nindex 2e49c0c596f8..5b716a7b8da5 100644\n--- a/tests/config/config.d/named_collection.xml\n+++ b/tests/config/config.d/named_collection.xml\n@@ -32,5 +32,10 @@\n             <secret_access_key>testtest</secret_access_key>\n             <structure>auto</structure>\n         </s3_conn>\n+        <s3_conn_db>\n+            <url>http://localhost:11111/test/</url>\n+            <access_key_id>test</access_key_id>\n+            <secret_access_key>testtest</secret_access_key>\n+        </s3_conn_db>\n     </named_collections>\n </clickhouse>\ndiff --git a/tests/queries/0_stateless/02707_clickhouse_local_implicit_file_table_function.reference b/tests/queries/0_stateless/02707_clickhouse_local_implicit_file_table_function.reference\nnew file mode 100644\nindex 000000000000..ccc02ad4f345\n--- /dev/null\n+++ b/tests/queries/0_stateless/02707_clickhouse_local_implicit_file_table_function.reference\n@@ -0,0 +1,9 @@\n+Test 1: check explicit and implicit call of the file table function\n+explicit:\n+4\n+implicit:\n+4\n+Test 2: check Filesystem database\n+4\n+Test 3: check show database with Filesystem\n+test02707\ndiff --git a/tests/queries/0_stateless/02707_clickhouse_local_implicit_file_table_function.sh b/tests/queries/0_stateless/02707_clickhouse_local_implicit_file_table_function.sh\nnew file mode 100755\nindex 000000000000..7c9095b3d8b3\n--- /dev/null\n+++ b/tests/queries/0_stateless/02707_clickhouse_local_implicit_file_table_function.sh\n@@ -0,0 +1,45 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+dir=${CLICKHOUSE_TEST_UNIQUE_NAME}\n+[[ -d $dir ]] && rm -rd $dir\n+mkdir $dir\n+\n+# Create temporary csv file for tests\n+echo '\"id\",\"str\",\"int\",\"text\"' > $dir/tmp.csv\n+echo '1,\"abc\",123,\"abacaba\"' >> $dir/tmp.csv\n+echo '2,\"def\",456,\"bacabaa\"' >> $dir/tmp.csv\n+echo '3,\"story\",78912,\"acabaab\"' >> $dir/tmp.csv\n+echo '4,\"history\",21321321,\"cabaaba\"' >> $dir/tmp.csv\n+\n+#################\n+echo \"Test 1: check explicit and implicit call of the file table function\"\n+\n+echo \"explicit:\"\n+$CLICKHOUSE_LOCAL -q \"SELECT COUNT(*) FROM file('${dir}/tmp.csv')\"\n+echo \"implicit:\"\n+$CLICKHOUSE_LOCAL -q \"SELECT COUNT(*) FROM \\\"${dir}/tmp.csv\\\"\"\n+\n+#################\n+echo \"Test 2: check Filesystem database\"\n+$CLICKHOUSE_LOCAL --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test;\n+CREATE DATABASE test ENGINE = Filesystem('${dir}');\n+SELECT COUNT(*) FROM test.\\`tmp.csv\\`;\n+DROP DATABASE test;\n+\"\"\"\n+\n+#################\n+echo \"Test 3: check show database with Filesystem\"\n+$CLICKHOUSE_LOCAL --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test02707;\n+CREATE DATABASE test02707 ENGINE = Filesystem('${dir}');\n+SHOW DATABASES;\n+DROP DATABASE test02707;\n+\"\"\" | grep \"test02707\"\n+\n+# Remove temporary dir with files\n+rm -rd $dir\ndiff --git a/tests/queries/0_stateless/02722_database_filesystem.reference b/tests/queries/0_stateless/02722_database_filesystem.reference\nnew file mode 100644\nindex 000000000000..c65dda7933a5\n--- /dev/null\n+++ b/tests/queries/0_stateless/02722_database_filesystem.reference\n@@ -0,0 +1,15 @@\n+Test 1: create filesystem database and check implicit calls\n+0\n+test1\n+4\n+4\n+4\n+Test 2: check DatabaseFilesystem access rights and errors handling on server\n+OK\n+OK\n+OK\n+OK\n+OK\n+OK\n+OK\n+OK\ndiff --git a/tests/queries/0_stateless/02722_database_filesystem.sh b/tests/queries/0_stateless/02722_database_filesystem.sh\nnew file mode 100755\nindex 000000000000..4ff659ee7468\n--- /dev/null\n+++ b/tests/queries/0_stateless/02722_database_filesystem.sh\n@@ -0,0 +1,72 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# see 01658_read_file_to_stringcolumn.sh\n+CLICKHOUSE_USER_FILES_PATH=$(clickhouse-client --query \"select _path, _file from file('nonexist.txt', 'CSV', 'val1 char')\" 2>&1 | grep Exception | awk '{gsub(\"/nonexist.txt\",\"\",$9); print $9}')\n+\n+# Prepare data\n+unique_name=${CLICKHOUSE_TEST_UNIQUE_NAME}\n+user_files_tmp_dir=${CLICKHOUSE_USER_FILES_PATH}/${unique_name}\n+mkdir -p ${user_files_tmp_dir}/tmp/\n+echo '\"id\",\"str\",\"int\",\"text\"' > ${user_files_tmp_dir}/tmp.csv\n+echo '1,\"abc\",123,\"abacaba\"' >> ${user_files_tmp_dir}/tmp.csv\n+echo '2,\"def\",456,\"bacabaa\"' >> ${user_files_tmp_dir}/tmp.csv\n+echo '3,\"story\",78912,\"acabaab\"' >> ${user_files_tmp_dir}/tmp.csv\n+echo '4,\"history\",21321321,\"cabaaba\"' >> ${user_files_tmp_dir}/tmp.csv\n+\n+tmp_dir=${CLICKHOUSE_TEST_UNIQUE_NAME}\n+[[ -d $tmp_dir ]] && rm -rd $tmp_dir\n+mkdir $tmp_dir\n+cp ${user_files_tmp_dir}/tmp.csv ${tmp_dir}/tmp.csv\n+cp ${user_files_tmp_dir}/tmp.csv ${user_files_tmp_dir}/tmp/tmp.csv\n+cp ${user_files_tmp_dir}/tmp.csv ${user_files_tmp_dir}/tmp.myext\n+\n+#################\n+echo \"Test 1: create filesystem database and check implicit calls\"\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test1;\n+CREATE DATABASE test1 ENGINE = Filesystem;\n+\"\"\"\n+echo $?\n+${CLICKHOUSE_CLIENT} --query \"SHOW DATABASES\" | grep \"test1\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT COUNT(*) FROM test1.\\`${unique_name}/tmp.csv\\`;\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT COUNT(*) FROM test1.\\`${unique_name}/tmp/tmp.csv\\`;\"\n+${CLICKHOUSE_LOCAL} -q \"SELECT COUNT(*) FROM \\\"${tmp_dir}/tmp.csv\\\"\"\n+\n+#################\n+echo \"Test 2: check DatabaseFilesystem access rights and errors handling on server\"\n+# DATABASE_ACCESS_DENIED: Allows list files only inside user_files\n+${CLICKHOUSE_CLIENT} --query \"SELECT COUNT(*) FROM test1.\\`../tmp.csv\\`;\" 2>&1| grep -F \"Code: 291\" > /dev/null && echo \"OK\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT COUNT(*) FROM test1.\\`/tmp/tmp.csv\\`;\" 2>&1| grep -F \"Code: 291\" > /dev/null && echo \"OK\"\n+${CLICKHOUSE_CLIENT} --multiline --multiquery --query \"\"\"\n+USE test1;\n+SELECT COUNT(*) FROM \\\"../${tmp_dir}/tmp.csv\\\";\n+\"\"\" 2>&1| grep -F \"Code: 291\" > /dev/null && echo \"OK\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT COUNT(*) FROM test1.\\`../../../../../../tmp.csv\\`;\" 2>&1| grep -F \"Code: 291\" > /dev/null && echo \"OK\"\n+\n+# BAD_ARGUMENTS: path should be inside user_files\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test2;\n+CREATE DATABASE test2 ENGINE = Filesystem('/tmp');\n+\"\"\" 2>&1| grep -F \"Code: 36\" > /dev/null && echo \"OK\"\n+\n+# BAD_ARGUMENTS: .../user_files/relative_unknown_dir does not exists\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test2;\n+CREATE DATABASE test2 ENGINE = Filesystem('relative_unknown_dir');\n+\"\"\" 2>&1| grep -F \"Code: 36\" > /dev/null && echo \"OK\"\n+\n+# FILE_DOESNT_EXIST: unknown file\n+${CLICKHOUSE_CLIENT} --query \"SELECT COUNT(*) FROM test1.\\`tmp2.csv\\`;\" 2>&1| grep -F \"Code: 107\" > /dev/null && echo \"OK\"\n+\n+# BAD_ARGUMENTS: Cannot determine the file format by it's extension\n+${CLICKHOUSE_CLIENT} --query \"SELECT COUNT(*) FROM test1.\\`${unique_name}/tmp.myext\\`;\" 2>&1| grep -F \"Code: 36\" > /dev/null && echo \"OK\"\n+\n+# Clean\n+${CLICKHOUSE_CLIENT} --query \"DROP DATABASE test1;\"\n+rm -rd $tmp_dir\n+rm -rd $user_files_tmp_dir\ndiff --git a/tests/queries/0_stateless/02724_database_s3.reference b/tests/queries/0_stateless/02724_database_s3.reference\nnew file mode 100644\nindex 000000000000..425cca6a077a\n--- /dev/null\n+++ b/tests/queries/0_stateless/02724_database_s3.reference\n@@ -0,0 +1,21 @@\n+Test 1: select from s3\n+1\t2\t3\n+4\t5\t6\n+7\t8\t9\n+0\t0\t0\n+test1\n+10\t11\t12\n+13\t14\t15\n+16\t17\t18\n+0\t0\t0\n+10\t11\t12\n+13\t14\t15\n+16\t17\t18\n+0\t0\t0\n+10\t11\t12\n+13\t14\t15\n+16\t17\t18\n+0\t0\t0\n+Test 2: check exceptions\n+OK\n+OK\ndiff --git a/tests/queries/0_stateless/02724_database_s3.sh b/tests/queries/0_stateless/02724_database_s3.sh\nnew file mode 100755\nindex 000000000000..79199b435712\n--- /dev/null\n+++ b/tests/queries/0_stateless/02724_database_s3.sh\n@@ -0,0 +1,63 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest, no-parallel\n+# Tag no-fasttest: Depends on AWS\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+#################\n+echo \"Test 1: select from s3\"\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test1;\n+CREATE DATABASE test1 ENGINE = S3;\n+USE test1;\n+SELECT * FROM \\\"http://localhost:11111/test/a.tsv\\\"\n+\"\"\"\n+${CLICKHOUSE_CLIENT} -q \"SHOW DATABASES;\" | grep test1\n+\n+# check credentials with absolute path\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test2;\n+CREATE DATABASE test2 ENGINE = S3('', 'test', 'testtest');\n+USE test2;\n+SELECT * FROM \\\"http://localhost:11111/test/b.tsv\\\"\n+\"\"\"\n+\n+# check credentials with relative path\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test4;\n+CREATE DATABASE test4 ENGINE = S3('http://localhost:11111/test', 'test', 'testtest');\n+USE test4;\n+SELECT * FROM \\\"b.tsv\\\"\n+\"\"\"\n+\n+# Check named collection loading\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test5;\n+CREATE DATABASE test5 ENGINE = S3(s3_conn_db);\n+SELECT * FROM test5.\\`b.tsv\\`\n+\"\"\"\n+\n+#################\n+echo \"Test 2: check exceptions\"\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test3;\n+CREATE DATABASE test3 ENGINE = S3;\n+USE test3;\n+SELECT * FROM \\\"http://localhost:11111/test/a.myext\\\"\n+\"\"\" 2>&1| grep -F \"BAD_ARGUMENTS\" > /dev/null && echo \"OK\"\n+\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+USE test3;\n+SELECT * FROM \\\"abacaba\\\"\n+\"\"\" 2>&1| grep -F \"BAD_ARGUMENTS\" > /dev/null && echo \"OK\"\n+\n+# Cleanup\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test1;\n+DROP DATABASE IF EXISTS test2;\n+DROP DATABASE IF EXISTS test3;\n+DROP DATABASE IF EXISTS test4;\n+DROP DATABASE IF EXISTS test5;\n+\"\"\"\ndiff --git a/tests/queries/0_stateless/02725_database_hdfs.reference b/tests/queries/0_stateless/02725_database_hdfs.reference\nnew file mode 100644\nindex 000000000000..ef8adae2bbc9\n--- /dev/null\n+++ b/tests/queries/0_stateless/02725_database_hdfs.reference\n@@ -0,0 +1,12 @@\n+Test 1: select from hdfs database\n+1\t2\t3\n+test1\n+1\t2\t3\n+test2\n+Test 2: check exceptions\n+OK0\n+OK1\n+OK2\n+OK3\n+OK4\n+OK5\ndiff --git a/tests/queries/0_stateless/02725_database_hdfs.sh b/tests/queries/0_stateless/02725_database_hdfs.sh\nnew file mode 100755\nindex 000000000000..a78f3e6bbdc1\n--- /dev/null\n+++ b/tests/queries/0_stateless/02725_database_hdfs.sh\n@@ -0,0 +1,60 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest, use-hdfs, no-parallel\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# Prepare data\n+${CLICKHOUSE_CLIENT} -q \"insert into table function hdfs('hdfs://localhost:12222/test_02725_1.tsv', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32') select 1, 2, 3 settings hdfs_truncate_on_insert=1;\"\n+${CLICKHOUSE_CLIENT} -q \"insert into table function hdfs('hdfs://localhost:12222/test_02725_2.tsv', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32') select 4, 5, 6 settings hdfs_truncate_on_insert=1;\"\n+\n+#################\n+echo \"Test 1: select from hdfs database\"\n+\n+# Database without specific host\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test1;\n+CREATE DATABASE test1 ENGINE = HDFS;\n+USE test1;\n+SELECT * FROM \\\"hdfs://localhost:12222/test_02725_1.tsv\\\"\n+\"\"\"\n+${CLICKHOUSE_CLIENT} -q \"SHOW DATABASES;\" | grep test1\n+\n+# Database with host\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test2;\n+CREATE DATABASE test2 ENGINE = HDFS('hdfs://localhost:12222');\n+USE test2;\n+SELECT * FROM \\\"test_02725_1.tsv\\\"\n+\"\"\"\n+${CLICKHOUSE_CLIENT} -q \"SHOW DATABASES;\" | grep test2\n+\n+#################\n+echo \"Test 2: check exceptions\"\n+\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test3;\n+CREATE DATABASE test3 ENGINE = HDFS('abacaba');\n+\"\"\" 2>&1| grep -F \"BAD_ARGUMENTS\" > /dev/null && echo \"OK0\"\n+\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test4;\n+CREATE DATABASE test4 ENGINE = HDFS;\n+USE test4;\n+SELECT * FROM \\\"abacaba/file.tsv\\\"\n+\"\"\" 2>&1| grep -F \"BAD_ARGUMENTS\" > /dev/null && echo \"OK1\"\n+\n+${CLICKHOUSE_CLIENT} -q \"SELECT * FROM test4.\\`http://localhost:11111/test/a.tsv\\`\" 2>&1| grep -F \"BAD_ARGUMENTS\" > /dev/null && echo \"OK2\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT * FROM test4.\\`hdfs://localhost:12222/file.myext\\`\" 2>&1| grep -F \"BAD_ARGUMENTS\" > /dev/null && echo \"OK3\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT * FROM test4.\\`hdfs://localhost:12222/test_02725_3.tsv\\`\" 2>&1| grep -F \"CANNOT_EXTRACT_TABLE_STRUCTURE\" > /dev/null && echo \"OK4\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT * FROM test4.\\`hdfs://localhost:12222\\`\" 2>&1| grep -F \"BAD_ARGUMENTS\" > /dev/null && echo \"OK5\"\n+\n+\n+# Cleanup\n+${CLICKHOUSE_CLIENT} --multiline --multiquery -q \"\"\"\n+DROP DATABASE IF EXISTS test1;\n+DROP DATABASE IF EXISTS test2;\n+DROP DATABASE IF EXISTS test3;\n+DROP DATABASE IF EXISTS test4;\n+\"\"\"\n\\ No newline at end of file\n",
  "problem_statement": "File/S3/HDFS as Database engine\n**Use case**\r\n\r\nProcess a bunch of files in the most intuitive way.\r\n\r\n**Describe the solution you'd like**\r\n\r\nAfter #32455 is implemented, the format and structure of remote datasets is autodetected.\r\nSo, we can write `SELECT * FROM url('https://datasets.clickhouse.com/github_events_v2.native.xz')` without specifying data format or data structure.\r\n\r\nNow let's look at the directory with files as a database.\r\nIt will allow to write something like:\r\n\r\n```\r\nSELECT * FROM \"data.parquet\"\r\n```\r\n\r\nThe database engine should do lazy listing of the files.\r\nThe table structure should also be obtained lazily to avoid slow SHOW TABLES.\r\n\r\nDatabase engine can take the following parameters:\r\n- prefix for files;\r\n- filter for filenames / file paths, optional (a question - should it be regexp or glob style);\r\n- should it do recursive directory traversal or look at the immediate directory only (the latter by default);\r\n- maybe a rule to glue some files together by pattern to represent them as a single table;\r\n- maybe a regexp to transform names;\r\n- default format if it is not autodetected, optional (skip unknown by default);\r\n\r\n**Caveats**\r\n\r\n`SELECT * FROM system.tables` without filters will become slower if these types of databases exist.\r\n\r\n**Additional ideas**\r\n\r\nWe can introduce the notion of overlay databases (one database that can shadow tables from another database) and add File database to `clickhouse-local` to be available under default database.\n",
  "hints_text": "@alexey-milovidov If this feature gets implemented do you think we'll be able to query S3 like this:\r\n\r\n```sql\r\nSELECT *\r\nFROM \"datasets.clickhouse.com'\"\r\n\r\n\u250c\u2500Key\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500LastModified\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500ETag\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500Size\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500Owner\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500StorageClass\u2500\u2510\r\n\u2502 cell_towers.csv.xz                            \u2502 2022-02-07T02:00:24.000Z \u2502 \"f67958765b5e3cf3979347ec4b83f252-87\"  \u2502 729138772   \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/anhoahjpkvdwymlmpzoxkhqtskmmdqck \u2502 2022-03-17T02:45:11.000Z \u2502 \"3517079828fb04b3942e636d9e92e91c\"     \u2502 11          \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/apkqizfzczhsecyxxfjppamnupnccpqo \u2502 2022-02-09T05:52:48.000Z \u2502 \"54e96b6cca2d02ca4cde05de41408a2a\"     \u2502 62808       \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/atnfsdxzgqngwfrpxwlaucxrgbuyymme \u2502 2022-03-17T01:59:48.000Z \u2502 \"68f0a2df225a27fef3be1fb085067be3\"     \u2502 14926       \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/awbyetkiyqaxolidftzzxtchefvbxztr \u2502 2022-03-17T02:06:10.000Z \u2502 \"7456d8d924953c92bdf81827799034fe-8\"   \u2502 147597936   \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/awjjgaqfxlrgaexafwrsdyvolcsfqzbv \u2502 2022-03-17T02:15:20.000Z \u2502 \"62fa530ba3e99aaab9426d653e89ed1b-566\" \u2502 10098292833 \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/awxjnezqatcyvbizohcgdmuscpeuqrwh \u2502 2022-03-17T02:05:06.000Z \u2502 \"32f079641830b95d951673f1f8103990\"     \u2502 150         \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/ayvuctobazdjlzoxmyyxhxmbzufnewfs \u2502 2022-03-17T02:03:33.000Z \u2502 \"e5c67fede4385a9547444e8a8e784b62-374\" \u2502 6670229982  \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/baxskruadpfwqzkjkgddfpvunallvowv \u2502 2022-03-17T01:59:49.000Z \u2502 \"3c055689adde409f55dd2571778ed9ad-3\"   \u2502 68211072    \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2502 disk-s3-test/bhlarfpzxpxfjjowqiqizbjqecbtthut \u2502 2022-03-17T02:19:10.000Z \u2502 \"8e00d713e032a6f18bddf23b26832cd1-13\"  \u2502 236123088   \u2502 ad0f8138e7b6cb179e4dbef43a90d2304805936a92b9960987136016599a3592 \u2502 STANDARD     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\n",
  "created_at": "2023-04-16T13:14:02Z"
}