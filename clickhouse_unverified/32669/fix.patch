diff --git a/src/Interpreters/InterpreterInsertQuery.cpp b/src/Interpreters/InterpreterInsertQuery.cpp
index b7edf12e23fd..d1b8a0560533 100644
--- a/src/Interpreters/InterpreterInsertQuery.cpp
+++ b/src/Interpreters/InterpreterInsertQuery.cpp
@@ -196,6 +196,9 @@ Chain InterpreterInsertQuery::buildChainImpl(
     /// We create a pipeline of several streams, into which we will write data.
     Chain out;
 
+    /// Keep a reference to the context to make sure it stays alive until the chain is executed and destroyed
+    out.addInterpreterContext(context_ptr);
+
     /// NOTE: we explicitly ignore bound materialized views when inserting into Kafka Storage.
     ///       Otherwise we'll get duplicates when MV reads same rows again from Kafka.
     if (table->noPushingToViews() && !no_destination)
diff --git a/src/QueryPipeline/Chain.h b/src/QueryPipeline/Chain.h
index c5fdc34cecfd..60dbad101311 100644
--- a/src/QueryPipeline/Chain.h
+++ b/src/QueryPipeline/Chain.h
@@ -1,5 +1,6 @@
 #pragma once
 
+#include <Interpreters/Context_fwd.h>
 #include <Processors/IProcessor.h>
 #include <QueryPipeline/PipelineResourcesHolder.h>
 
@@ -42,6 +43,7 @@ class Chain
     void addTableLock(TableLockHolder lock) { holder.table_locks.emplace_back(std::move(lock)); }
     void addStorageHolder(StoragePtr storage) { holder.storage_holders.emplace_back(std::move(storage)); }
     void attachResources(PipelineResourcesHolder holder_) { holder = std::move(holder_); }
+    void addInterpreterContext(ContextPtr context) { holder.interpreter_context.emplace_back(std::move(context)); }
     PipelineResourcesHolder detachResources() { return std::move(holder); }
 
     void reset();
