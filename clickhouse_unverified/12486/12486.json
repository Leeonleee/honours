{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 12486,
  "instance_id": "ClickHouse__ClickHouse-12486",
  "issue_numbers": [
    "12342"
  ],
  "base_commit": "0fbad9dbec097075913812095e897fec97f7ee38",
  "patch": "diff --git a/src/AggregateFunctions/AggregateFunctionAvg.h b/src/AggregateFunctions/AggregateFunctionAvg.h\nindex 95b4836c336f..f7bc228bcdd5 100644\n--- a/src/AggregateFunctions/AggregateFunctionAvg.h\n+++ b/src/AggregateFunctions/AggregateFunctionAvg.h\n@@ -20,6 +20,7 @@ template <typename T, typename Denominator>\n struct AggregateFunctionAvgData\n {\n     using NumeratorType = T;\n+    using DenominatorType = Denominator;\n \n     T numerator = 0;\n     Denominator denominator = 0;\n@@ -73,13 +74,21 @@ class AggregateFunctionAvgBase : public IAggregateFunctionDataHelper<Data, Deriv\n     void serialize(ConstAggregateDataPtr place, WriteBuffer & buf) const override\n     {\n         writeBinary(this->data(place).numerator, buf);\n-        writeBinary(this->data(place).denominator, buf);\n+\n+        if constexpr (std::is_unsigned_v<typename Data::DenominatorType>)\n+            writeVarUInt(this->data(place).denominator, buf);\n+        else /// Floating point denominator type can be used\n+            writeBinary(this->data(place).denominator, buf);\n     }\n \n     void deserialize(AggregateDataPtr place, ReadBuffer & buf, Arena *) const override\n     {\n         readBinary(this->data(place).numerator, buf);\n-        readBinary(this->data(place).denominator, buf);\n+\n+        if constexpr (std::is_unsigned_v<typename Data::DenominatorType>)\n+            readVarUInt(this->data(place).denominator, buf);\n+        else /// Floating point denominator type can be used\n+            readBinary(this->data(place).denominator, buf);\n     }\n \n     void insertResultInto(AggregateDataPtr place, IColumn & to, Arena *) const override\n",
  "test_patch": "diff --git a/tests/integration/test_backward_compatability/test_string_aggregation.py b/tests/integration/test_backward_compatability/test_string_aggregation.py\ndeleted file mode 100644\nindex e69de29bb2d1..000000000000\ndiff --git a/tests/integration/test_backward_compatability/__init__.py b/tests/integration/test_backward_compatibility/__init__.py\nsimilarity index 100%\nrename from tests/integration/test_backward_compatability/__init__.py\nrename to tests/integration/test_backward_compatibility/__init__.py\ndiff --git a/tests/integration/test_backward_compatability/test.py b/tests/integration/test_backward_compatibility/test.py\nsimilarity index 100%\nrename from tests/integration/test_backward_compatability/test.py\nrename to tests/integration/test_backward_compatibility/test.py\ndiff --git a/tests/integration/test_backward_compatibility/test_aggregate_function_state_avg.py b/tests/integration/test_backward_compatibility/test_aggregate_function_state_avg.py\nnew file mode 100644\nindex 000000000000..c9f3acc2e2ea\n--- /dev/null\n+++ b/tests/integration/test_backward_compatibility/test_aggregate_function_state_avg.py\n@@ -0,0 +1,52 @@\n+import pytest\n+\n+import helpers.client as client\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('node1',\n+    with_zookeeper=False, image='yandex/clickhouse-server:19.16.9.37', stay_alive=True, with_installed_binary=True)\n+node2 = cluster.add_instance('node2',\n+    with_zookeeper=False, image='yandex/clickhouse-server:19.16.9.37', stay_alive=True, with_installed_binary=True)\n+node3 = cluster.add_instance('node3', with_zookeeper=False)\n+node4 = cluster.add_instance('node4', with_zookeeper=False)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+# We will test that serialization of internal state of \"avg\" function is compatible between different versions.\n+# TODO Implement versioning of serialization format for aggregate function states.\n+# NOTE This test is too ad-hoc.\n+\n+def test_backward_compatability(start_cluster):\n+    node1.query(\"create table tab (x UInt64) engine = Memory\")\n+    node2.query(\"create table tab (x UInt64) engine = Memory\")\n+    node3.query(\"create table tab (x UInt64) engine = Memory\")\n+    node4.query(\"create table tab (x UInt64) engine = Memory\")\n+\n+    node1.query(\"INSERT INTO tab VALUES (1)\")\n+    node2.query(\"INSERT INTO tab VALUES (2)\")\n+    node3.query(\"INSERT INTO tab VALUES (3)\")\n+    node4.query(\"INSERT INTO tab VALUES (4)\")\n+\n+    assert(node1.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == '2.5\\n')\n+    assert(node2.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == '2.5\\n')\n+    assert(node3.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == '2.5\\n')\n+    assert(node4.query(\"SELECT avg(x) FROM remote('node{1..4}', default, tab)\") == '2.5\\n')\n+\n+    # Also check with persisted aggregate function state\n+\n+    node1.query(\"create table state (x AggregateFunction(avg, UInt64)) engine = Log\")\n+    node1.query(\"INSERT INTO state SELECT avgState(arrayJoin(CAST([1, 2, 3, 4] AS Array(UInt64))))\")\n+\n+    assert(node1.query(\"SELECT avgMerge(x) FROM state\") == '2.5\\n')\n+\n+    node1.restart_with_latest_version()\n+\n+    assert(node1.query(\"SELECT avgMerge(x) FROM state\") == '2.5\\n')\ndiff --git a/tests/integration/test_backward_compatability/test_short_strings_aggregation.py b/tests/integration/test_backward_compatibility/test_short_strings_aggregation.py\nsimilarity index 100%\nrename from tests/integration/test_backward_compatability/test_short_strings_aggregation.py\nrename to tests/integration/test_backward_compatibility/test_short_strings_aggregation.py\n",
  "problem_statement": "Cannot read all data during merge from AggregateFunctionAvgBase\nWe were testing an upgrade from version 20.3.7 to 20.5.2 on one set of four replicas.  There were many stack traces like on columns that use the `AggregateFunction(avg, UInt32)` data type.  This exception happened only on that data type, but it appears to affect all of our tables that have it.\r\n\r\nIs this possibly an issue with inconsistent parts?  We are using zstd compression and are seeing the \"checksum does not match\" errors as well.  Those errors continued even after all four replicas were upgraded, which was not expected.\r\n\r\nUnfortunately this is a show stopping bug for us.\r\n\r\nPossibly relevant updated server settings:\r\n```\r\nindex_granularity_bytes                                   \u2502 10485760   \r\nenable_mixed_granularity_parts                            \u2502 1      \r\n```\r\n\r\nExample table definition (from zookeeper):\r\n\r\n```columns format version: 1\r\n16 columns:\r\n`datetime` DateTime(\\'Etc/UTC\\')\r\n`svc_type` String\r\n`svc` String\r\n`host` String\r\n`pssc` UInt16\r\n`crc` String\r\n`sssc` UInt16\r\n`pqsn` String\r\n`event_count` UInt64\r\n`served_bytes` UInt64\r\n`parent_bytes` UInt64\r\n`ttms_avg` AggregateFunction(avg, UInt32)\r\n`ttms_quants` AggregateFunction(quantilesTDigest(0.99, 0.95, 0.9), UInt32)\r\n`chi_count` AggregateFunction(uniq, FixedString(16))\r\n`manifest_count` UInt64\r\n`fragment_count` UInt64\r\n\r\nmetadata format version: 1\r\ndate column: \r\nsampling expression: \r\nindex granularity: 8192\r\nmode: 2\r\nsign column: \r\nprimary key: datetime, svc_type, svc, host, pssc, crc, sssc, pqsn\r\ndata format version: 1\r\npartition key: toDate(datetime)\r\ngranularity bytes: 10485760\r\n\r\n```\r\n\r\n\r\nStack trace:\r\n\r\n```\r\n2020.07.09 16:34:59.968849 [ 397374 ] {} <Error> rogers.atsmid_svc_host_1m_v2: DB::StorageReplicatedMergeTree::queueTask()::<lambda(DB::StorageReplicatedMergeTree::LogEntry\r\nPtr&)>: Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 5. Bytes expected: 8.: (while reading column ttms_avg): (while reading from part /opt/d\r\nata/clickhouse/data/rogers/atsmid_svc_host_1m_v2/20200709_3390_3390_0/ from mark 0 with max_rows_to_read = 8192), Stack trace (when copying this message, always include the\r\n lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x10ed0da0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x95c923d in /usr/bin/clickhouse\r\n2. DB::ReadBuffer::readStrict(char*, unsigned long) @ 0x960c90f in /usr/bin/clickhouse\r\n3. DB::AggregateFunctionAvgBase<unsigned int, DB::AggregateFunctionAvgData<unsigned long, unsigned long>, DB::AggregateFunctionAvg<unsigned int, DB::AggregateFunctionAvgDat\r\na<unsigned long, unsigned long> > >::deserialize(char*, DB::ReadBuffer&, DB::Arena*) const @ 0x97705bd in /usr/bin/clickhouse\r\n4. DB::DataTypeAggregateFunction::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0xd9bdaa5 in /usr/bin/clickhouse\r\n5. DB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsig\r\nned long, bool, unsigned long, bool) @ 0xe51d774 in /usr/bin/clickhouse\r\n6. DB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>\r\n::immutable_ptr<DB::IColumn> > >&) @ 0xe51dbcc in /usr/bin/clickhouse\r\n7. DB::MergeTreeSequentialSource::generate() @ 0xe54519b in /usr/bin/clickhouse\r\n8. DB::ISource::work() @ 0xe6dd3ab in /usr/bin/clickhouse\r\n9. DB::SourceWithProgress::work() @ 0xe91f337 in /usr/bin/clickhouse\r\n10. DB::TreeExecutorBlockInputStream::execute(bool, bool) @ 0xe71fb0e in /usr/bin/clickhouse\r\n11. DB::TreeExecutorBlockInputStream::readImpl() @ 0xe71fe7f in /usr/bin/clickhouse\r\n12. DB::IBlockInputStream::read() @ 0xd999b1d in /usr/bin/clickhouse\r\n13. DB::MergeTreeDataMergerMutator::mergePartsToTemporaryPart(DB::FutureMergedMutatedPart const&, DB::MergeListEntry&, DB::TableStructureReadLockHolder&, long, std::__1::un\r\nique_ptr<DB::IReservation, std::__1::default_delete<DB::IReservation> > const&, bool, bool) @ 0xe42d37c in /usr/bin/clickhouse\r\n14. DB::StorageReplicatedMergeTree::tryExecuteMerge(DB::ReplicatedMergeTreeLogEntry const&) @ 0xe28c04a in /usr/bin/clickhouse\r\n15. DB::StorageReplicatedMergeTree::executeLogEntry(DB::ReplicatedMergeTreeLogEntry&) @ 0xe2bb57b in /usr/bin/clickhouse\r\n16. ? @ 0xe2bb97d in /usr/bin/clickhouse\r\n17. DB::ReplicatedMergeTreeQueue::processEntry(std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()>, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&, std::\r\n__1::function<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>) @ 0xe5a99b2 in /usr/bin/clickhouse\r\n18. DB::StorageReplicatedMergeTree::queueTask() @ 0xe26e52e in /usr/bin/clickhouse\r\n19. DB::BackgroundProcessingPool::workLoopFunc() @ 0xe39f763 in /usr/bin/clickhouse\r\n20. ? @ 0xe3a0092 in /usr/bin/clickhouse\r\n21. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x95f6e97 in /usr/bin/clickhouse\r\n22. ? @ 0x95f5383 in /usr/bin/clickhouse\r\n23. start_thread @ 0x7dd5 in /usr/lib64/libpthread-2.17.so\r\n24. clone @ 0xfdead in /usr/lib64/libc-2.17.so\r\n```\r\n\r\n\n",
  "hints_text": "A couple of other points:\r\nThe only tables that display this issue are aggregate tables that are populated with materialized views\r\nOne other column appeared to have an issue -- a FixedString(16) column used as a key\r\nThe parts created that showed this issue were basically corrupted and unusable.  This is a critical bug that can cause data loss.\nWe need to exclude the probability that it happens due to some data corruption or hw issues. Can you try to create some reproducable example? Do check table return 1? Does downgrade helps with the issue? \nDowngrading fixed the issue in the sense that new parts created by the materialized views were not corrupted and merged cleanly.   However, the ttms_avg columns that were created under v20.5 were not readable under 20.3 and caused the same merge exceptions/stack traces as the example above.  Hardware issues are unlikely as the new parts were being created by inserts on two different nodes to 4 different tables and the `AggregateFunction(avg, UInt32)` column in each of the 8 materialized view tables was unreadable for every single part.  I will see if we can create a reproducible example and look for common elements in the materialized view queries.\nA somewhat related issue -- we only upgrade a single shard on a multi shard cluster, and a distributed query generated the following error.  Does that mean all shards must be upgraded to version 20.5 simultaneously?\r\n\r\n```\r\n2020.07.09 17:39:28.440083 [ 398558 ] {} <Error> DynamicQueryHandler: Code: 271, e.displayText() = DB::Exception: Data compressed with different methods, given method byte 0x80, previous method byte 0x82: while receiving packet from cdn-maple-as-011.prod.comcast.net:9000: While executing Remote, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x10ed0da0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x95c923d in /usr/bin/clickhouse\r\n2. ? @ 0xd946e9d in /usr/bin/clickhouse\r\n3. DB::CompressedReadBuffer::nextImpl() @ 0xd941c2e in /usr/bin/clickhouse\r\n4. DB::ReadBuffer::readStrict(char*, unsigned long) @ 0x960c995 in /usr/bin/clickhouse\r\n5. DB::DataTypeAggregateFunction::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0xd9bdaa5 in /usr/bin/clickhouse\r\n6. DB::NativeBlockInputStream::readData(DB::IDataType const&, DB::IColumn&, DB::ReadBuffer&, unsigned long, double) @ 0xdf658e8 in /usr/bin/clickhouse\r\n7. DB::NativeBlockInputStream::readImpl() @ 0xdf6667f in /usr/bin/clickhouse\r\n8. DB::IBlockInputStream::read() @ 0xd999b1d in /usr/bin/clickhouse\r\n9. DB::Connection::receiveDataImpl(std::__1::shared_ptr<DB::IBlockInputStream>&) @ 0xe5fc17e in /usr/bin/clickhouse\r\n10. DB::Connection::receivePacket() @ 0xe5ff844 in /usr/bin/clickhouse\r\n11. DB::MultiplexedConnections::receivePacket() @ 0xe61f9da in /usr/bin/clickhouse\r\n12. DB::RemoteQueryExecutor::read() @ 0xd9ac874 in /usr/bin/clickhouse\r\n13. DB::RemoteSource::generate() @ 0xe9187bf in /usr/bin/clickhouse\r\n14. DB::ISource::work() @ 0xe6dd3ab in /usr/bin/clickhouse\r\n15. DB::SourceWithProgress::work() @ 0xe91f337 in /usr/bin/clickhouse\r\n16. ? @ 0xe70aa21 in /usr/bin/clickhouse\r\n17. ? @ 0xe70f1a6 in /usr/bin/clickhouse\r\n18. ? @ 0xe70f7f2 in /usr/bin/clickhouse\r\n19. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x95f6e97 in /usr/bin/clickhouse\r\n20. ? @ 0x95f5383 in /usr/bin/clickhouse\r\n21. start_thread @ 0x7dd5 in /usr/lib64/libpthread-2.17.so\r\n22. clone @ 0xfdead in /usr/lib64/libc-2.17.so\r\n (version 20.5.2.7 (official build))\r\n```\nOne more possibly relevant setting:  `parallel_view_processing         \u2502 1`\r\n\r\nStill working on trying to create a reproducible example\nIt was broken here: https://github.com/ClickHouse/ClickHouse/pull/10758\r\nSerialization format was changed for `avgWeighted` but in fact is was also changed for `avg` (I have missed that).\r\n\r\nDo not upgrade before we will come up with the solution.\nIt's first appeared in 20.5, it means that we can safely rollback the change.",
  "created_at": "2020-07-14T08:58:48Z"
}