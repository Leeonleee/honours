{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 48726,
  "instance_id": "ClickHouse__ClickHouse-48726",
  "issue_numbers": [
    "45486"
  ],
  "base_commit": "15ebbd2ed637f79d0ff1fb5b2698d0929e2d5a05",
  "patch": "diff --git a/src/Backups/BackupCoordinationRemote.cpp b/src/Backups/BackupCoordinationRemote.cpp\nindex 9a2fdf5dd6bb..4cd06061e1ba 100644\n--- a/src/Backups/BackupCoordinationRemote.cpp\n+++ b/src/Backups/BackupCoordinationRemote.cpp\n@@ -256,7 +256,10 @@ void BackupCoordinationRemote::removeAllNodes()\n \n void BackupCoordinationRemote::setStage(const String & new_stage, const String & message)\n {\n-    stage_sync->set(current_host, new_stage, message);\n+    if (is_internal)\n+        stage_sync->set(current_host, new_stage, message);\n+    else\n+        stage_sync->set(current_host, new_stage, /* message */ \"\", /* all_hosts */ true);\n }\n \n void BackupCoordinationRemote::setError(const Exception & exception)\n@@ -779,8 +782,8 @@ bool BackupCoordinationRemote::hasConcurrentBackups(const std::atomic<size_t> &)\n                 String status;\n                 if (zk->tryGet(root_zookeeper_path + \"/\" + existing_backup_path + \"/stage\", status))\n                 {\n-                    /// If status is not COMPLETED it could be because the backup failed, check if 'error' exists\n-                    if (status != Stage::COMPLETED && !zk->exists(root_zookeeper_path + \"/\" + existing_backup_path + \"/error\"))\n+                    /// Check if some other backup is in progress\n+                    if (status == Stage::SCHEDULED_TO_START)\n                     {\n                         LOG_WARNING(log, \"Found a concurrent backup: {}, current backup: {}\", existing_backup_uuid, toString(backup_uuid));\n                         result = true;\ndiff --git a/src/Backups/BackupCoordinationStage.h b/src/Backups/BackupCoordinationStage.h\nindex 40a4b262caaa..41cd66346a21 100644\n--- a/src/Backups/BackupCoordinationStage.h\n+++ b/src/Backups/BackupCoordinationStage.h\n@@ -43,6 +43,10 @@ namespace BackupCoordinationStage\n \n     /// Coordination stage meaning that a host finished its work.\n     constexpr const char * COMPLETED = \"completed\";\n+\n+    /// Coordination stage meaning that backup/restore has failed due to an error\n+    /// Check '/error' for the error message\n+    constexpr const char * ERROR = \"error\";\n }\n \n }\ndiff --git a/src/Backups/BackupCoordinationStageSync.cpp b/src/Backups/BackupCoordinationStageSync.cpp\nindex effb00085c3d..3d8c283f0847 100644\n--- a/src/Backups/BackupCoordinationStageSync.cpp\n+++ b/src/Backups/BackupCoordinationStageSync.cpp\n@@ -8,11 +8,13 @@\n #include <IO/ReadHelpers.h>\n #include <IO/WriteBufferFromString.h>\n #include <IO/WriteHelpers.h>\n-\n+#include <Backups/BackupCoordinationStage.h>\n \n namespace DB\n {\n \n+namespace Stage = BackupCoordinationStage;\n+\n namespace ErrorCodes\n {\n     extern const int FAILED_TO_SYNC_BACKUP_OR_RESTORE;\n@@ -42,7 +44,7 @@ void BackupCoordinationStageSync::createRootNodes()\n     });\n }\n \n-void BackupCoordinationStageSync::set(const String & current_host, const String & new_stage, const String & message)\n+void BackupCoordinationStageSync::set(const String & current_host, const String & new_stage, const String & message, const bool & all_hosts)\n {\n     auto holder = with_retries.createRetriesControlHolder(\"set\");\n     holder.retries_ctl.retryLoop(\n@@ -50,14 +52,23 @@ void BackupCoordinationStageSync::set(const String & current_host, const String\n     {\n         with_retries.renewZooKeeper(zookeeper);\n \n-        /// Make an ephemeral node so the initiator can track if the current host is still working.\n-        String alive_node_path = zookeeper_path + \"/alive|\" + current_host;\n-        auto code = zookeeper->tryCreate(alive_node_path, \"\", zkutil::CreateMode::Ephemeral);\n-        if (code != Coordination::Error::ZOK && code != Coordination::Error::ZNODEEXISTS)\n-            throw zkutil::KeeperException(code, alive_node_path);\n-\n-        zookeeper->createIfNotExists(zookeeper_path + \"/started|\" + current_host, \"\");\n-        zookeeper->createIfNotExists(zookeeper_path + \"/current|\" + current_host + \"|\" + new_stage, message);\n+        if (all_hosts)\n+        {\n+            auto code = zookeeper->trySet(zookeeper_path, new_stage);\n+            if (code != Coordination::Error::ZOK)\n+                throw zkutil::KeeperException(code, zookeeper_path);\n+        }\n+        else\n+        {\n+            /// Make an ephemeral node so the initiator can track if the current host is still working.\n+            String alive_node_path = zookeeper_path + \"/alive|\" + current_host;\n+            auto code = zookeeper->tryCreate(alive_node_path, \"\", zkutil::CreateMode::Ephemeral);\n+            if (code != Coordination::Error::ZOK && code != Coordination::Error::ZNODEEXISTS)\n+                throw zkutil::KeeperException(code, alive_node_path);\n+\n+            zookeeper->createIfNotExists(zookeeper_path + \"/started|\" + current_host, \"\");\n+            zookeeper->createIfNotExists(zookeeper_path + \"/current|\" + current_host + \"|\" + new_stage, message);\n+        }\n     });\n }\n \n@@ -73,6 +84,10 @@ void BackupCoordinationStageSync::setError(const String & current_host, const Ex\n         writeStringBinary(current_host, buf);\n         writeException(exception, buf, true);\n         zookeeper->createIfNotExists(zookeeper_path + \"/error\", buf.str());\n+\n+        auto code = zookeeper->trySet(zookeeper_path, Stage::ERROR);\n+        if (code != Coordination::Error::ZOK)\n+            throw zkutil::KeeperException(code, zookeeper_path);\n     });\n }\n \ndiff --git a/src/Backups/BackupCoordinationStageSync.h b/src/Backups/BackupCoordinationStageSync.h\nindex 56081f8779cd..2efaec46b3a8 100644\n--- a/src/Backups/BackupCoordinationStageSync.h\n+++ b/src/Backups/BackupCoordinationStageSync.h\n@@ -15,7 +15,7 @@ class BackupCoordinationStageSync\n         Poco::Logger * log_);\n \n     /// Sets the stage of the current host and signal other hosts if there were other hosts waiting for that.\n-    void set(const String & current_host, const String & new_stage, const String & message);\n+    void set(const String & current_host, const String & new_stage, const String & message, const bool & all_hosts = false);\n     void setError(const String & current_host, const Exception & exception);\n \n     /// Sets the stage of the current host and waits until all hosts come to the same stage.\ndiff --git a/src/Backups/BackupsWorker.cpp b/src/Backups/BackupsWorker.cpp\nindex 4b17174a8de0..0a6482fb7deb 100644\n--- a/src/Backups/BackupsWorker.cpp\n+++ b/src/Backups/BackupsWorker.cpp\n@@ -368,6 +368,7 @@ void BackupsWorker::doBackup(\n \n             /// Wait until all the hosts have written their backup entries.\n             backup_coordination->waitForStage(Stage::COMPLETED);\n+            backup_coordination->setStage(Stage::COMPLETED,\"\");\n         }\n         else\n         {\n@@ -385,7 +386,7 @@ void BackupsWorker::doBackup(\n             writeBackupEntries(backup, std::move(backup_entries), backup_id, backup_coordination, backup_settings.internal);\n \n             /// We have written our backup entries, we need to tell other hosts (they could be waiting for it).\n-            backup_coordination->setStage(Stage::COMPLETED, \"\");\n+            backup_coordination->setStage(Stage::COMPLETED,\"\");\n         }\n \n         size_t num_files = 0;\n@@ -654,12 +655,26 @@ void BackupsWorker::doRestore(\n         /// (If this isn't ON CLUSTER query RestorerFromBackup will check access rights later.)\n         ClusterPtr cluster;\n         bool on_cluster = !restore_query->cluster.empty();\n+\n         if (on_cluster)\n         {\n             restore_query->cluster = context->getMacros()->expand(restore_query->cluster);\n             cluster = context->getCluster(restore_query->cluster);\n             restore_settings.cluster_host_ids = cluster->getHostIDs();\n+        }\n+\n+        /// Make a restore coordination.\n+        if (!restore_coordination)\n+            restore_coordination = makeRestoreCoordination(context, restore_settings, /* remote= */ on_cluster);\n+\n+        if (!allow_concurrent_restores && restore_coordination->hasConcurrentRestores(std::ref(num_active_restores)))\n+            throw Exception(\n+                ErrorCodes::CONCURRENT_ACCESS_NOT_SUPPORTED,\n+                \"Concurrent restores not supported, turn on setting 'allow_concurrent_restores'\");\n+\n \n+        if (on_cluster)\n+        {\n             /// We cannot just use access checking provided by the function executeDDLQueryOnCluster(): it would be incorrect\n             /// because different replicas can contain different set of tables and so the required access rights can differ too.\n             /// So the right way is pass through the entire cluster and check access for each host.\n@@ -676,15 +691,6 @@ void BackupsWorker::doRestore(\n             }\n         }\n \n-        /// Make a restore coordination.\n-        if (!restore_coordination)\n-            restore_coordination = makeRestoreCoordination(context, restore_settings, /* remote= */ on_cluster);\n-\n-        if (!allow_concurrent_restores && restore_coordination->hasConcurrentRestores(std::ref(num_active_restores)))\n-            throw Exception(\n-                ErrorCodes::CONCURRENT_ACCESS_NOT_SUPPORTED,\n-                \"Concurrent restores not supported, turn on setting 'allow_concurrent_restores'\");\n-\n         /// Do RESTORE.\n         if (on_cluster)\n         {\n@@ -703,6 +709,7 @@ void BackupsWorker::doRestore(\n \n             /// Wait until all the hosts have written their backup entries.\n             restore_coordination->waitForStage(Stage::COMPLETED);\n+            restore_coordination->setStage(Stage::COMPLETED,\"\");\n         }\n         else\n         {\ndiff --git a/src/Backups/RestoreCoordinationRemote.cpp b/src/Backups/RestoreCoordinationRemote.cpp\nindex 860443665cfa..f95969b52a16 100644\n--- a/src/Backups/RestoreCoordinationRemote.cpp\n+++ b/src/Backups/RestoreCoordinationRemote.cpp\n@@ -93,7 +93,10 @@ void RestoreCoordinationRemote::createRootNodes()\n \n void RestoreCoordinationRemote::setStage(const String & new_stage, const String & message)\n {\n-    stage_sync->set(current_host, new_stage, message);\n+    if (is_internal)\n+        stage_sync->set(current_host, new_stage, message);\n+    else\n+        stage_sync->set(current_host, new_stage, /* message */ \"\", /* all_hosts */ true);\n }\n \n void RestoreCoordinationRemote::setError(const Exception & exception)\n@@ -283,8 +286,8 @@ bool RestoreCoordinationRemote::hasConcurrentRestores(const std::atomic<size_t>\n                     String status;\n                     if (zk->tryGet(root_zookeeper_path + \"/\" + existing_restore_path + \"/stage\", status))\n                     {\n-                        /// If status is not COMPLETED it could be because the restore failed, check if 'error' exists\n-                        if (status != Stage::COMPLETED && !zk->exists(root_zookeeper_path + \"/\" + existing_restore_path + \"/error\"))\n+                        /// Check if some other restore is in progress\n+                        if (status == Stage::SCHEDULED_TO_START)\n                         {\n                             LOG_WARNING(log, \"Found a concurrent restore: {}, current restore: {}\", existing_restore_uuid, toString(restore_uuid));\n                             result = true;\n",
  "test_patch": "diff --git a/tests/integration/test_backup_restore_on_cluster/test_disallow_concurrency.py b/tests/integration/test_backup_restore_on_cluster/test_disallow_concurrency.py\nindex e5cd9ade68b5..d0ce2e030169 100644\n--- a/tests/integration/test_backup_restore_on_cluster/test_disallow_concurrency.py\n+++ b/tests/integration/test_backup_restore_on_cluster/test_disallow_concurrency.py\n@@ -6,7 +6,6 @@\n from helpers.cluster import ClickHouseCluster\n from helpers.test_tools import TSV, assert_eq_with_retry\n \n-\n cluster = ClickHouseCluster(__file__)\n \n num_nodes = 10\n",
  "problem_statement": "Flaky test_backup_restore_on_cluster/test_disallow_concurrency.py\nThis test is to check if disallowing of concurrent backups & restores works as expected. \r\nWhen one backup/restore fails, the status is not updated properly, thus we need to set the status of backup/restore so that other nodes in cluster are aware of failures.\r\n\r\nSome occurences : \r\nhttps://play.clickhouse.com/play?user=play#c2VsZWN0IAp0b1N0YXJ0T2ZIb3VyKGNoZWNrX3N0YXJ0X3RpbWUpIGFzIGQsCmNvdW50KCksICBncm91cFVuaXFBcnJheShwdWxsX3JlcXVlc3RfbnVtYmVyKSwgIGFueShyZXBvcnRfdXJsKQpmcm9tIGNoZWNrcyB3aGVyZSAnMjAyMi0wNi0wMScgPD0gY2hlY2tfc3RhcnRfdGltZSBhbmQgdGVzdF9uYW1lIGxpa2UgJyV0ZXN0X2JhY2t1cF9yZXN0b3JlX29uX2NsdXN0ZXIvdGVzdF9kaXNhbGxvd19jb25jdXJyZW5jeS5weSUnIGFuZCB0ZXN0X3N0YXR1cyBpbiAoJ0ZBSUwnLCAnRkxBS1knKSBncm91cCBieSBkIG9yZGVyIGJ5IGQgZGVzYw==\n",
  "hints_text": "`test_backup_restore_on_cluster` also fails with ASan asserts:\r\n\r\nhttps://play.clickhouse.com/play?user=play#c2VsZWN0IApjaGVja19zdGFydF90aW1lLAogICAgcmVwb3J0X3VybCwgY291bnQoKQpGUk9NIGNoZWNrcwpXSEVSRSB0ZXN0X25hbWUgbGlrZSAnJXRlc3RfYmFja3VwX3Jlc3RvcmVfb25fY2x1c3RlciUnIApBTkQgdGVzdF9zdGF0dXMgaW4gKCdGQUlMJywgJ0ZMQUtZJykKQU5EIHB1bGxfcmVxdWVzdF9udW1iZXIgPT0gMApHUk9VUCBCWSBjaGVja19uYW1lLCByZXBvcnRfdXJsLCBjaGVja19zdGFydF90aW1lCm9yZGVyIGJ5IGNoZWNrX3N0YXJ0X3RpbWUgZGVzYwpMSU1JVCAyMA==\n@SmitaRKulkarni This issue popped up in tests of #46075 ([here](https://s3.amazonaws.com/clickhouse-test-reports/46075/3259353bd3f41a59b73b9072dc0e7a862930c00b/integration_tests__release__[3/4].html)), error message:\r\n```\r\n>       assert \"Concurrent backups not supported\" in nodes[2].query_and_get_error(\r\n            f\"BACKUP TABLE tbl ON CLUSTER 'cluster' TO {backup_name}\r\n```\r\n\r\nWould you like to check if this is a new issue or this one? Thanks!\n@rschu1ze  : Looks like it will be resolved after this #45982. will keep an eye on the integration test fails\nIt's still flaky \r\nhttps://s3.amazonaws.com/clickhouse-test-reports/47863/73ffdd893dccfe29e8b04a464bba33bf72b6d95e/integration_tests__asan__[3/6].html",
  "created_at": "2023-04-12T18:28:13Z"
}