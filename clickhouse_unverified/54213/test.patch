diff --git a/tests/analyzer_integration_broken_tests.txt b/tests/analyzer_integration_broken_tests.txt
index 20ea31efa70f..080cd3f2677b 100644
--- a/tests/analyzer_integration_broken_tests.txt
+++ b/tests/analyzer_integration_broken_tests.txt
@@ -95,3 +95,4 @@ test_odbc_interaction/test.py::test_postgres_insert
 test_zookeeper_config/test.py::test_chroot_with_different_root
 test_zookeeper_config/test.py::test_chroot_with_same_root
 test_merge_tree_azure_blob_storage/test.py::test_table_manipulations
+test_parallel_replicas_skip_shards/test.py::test_skip_unavailable_shards
diff --git a/tests/integration/test_parallel_replicas_distributed_skip_shards/__init__.py b/tests/integration/test_parallel_replicas_distributed_skip_shards/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/tests/integration/test_parallel_replicas_distributed_skip_shards/configs/remote_servers.xml b/tests/integration/test_parallel_replicas_distributed_skip_shards/configs/remote_servers.xml
new file mode 100644
index 000000000000..48c1587eae3a
--- /dev/null
+++ b/tests/integration/test_parallel_replicas_distributed_skip_shards/configs/remote_servers.xml
@@ -0,0 +1,54 @@
+<clickhouse>
+    <remote_servers>
+        <test_multiple_shards_multiple_replicas>
+            <shard>
+                <internal_replication>true</internal_replication>
+                <replica>
+                    <host>n1</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>n2</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>n3</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+            <shard>
+                <internal_replication>true</internal_replication>
+                <replica>
+                    <host>n4</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>n5</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>n6</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </test_multiple_shards_multiple_replicas>
+        <test_single_shard_multiple_replicas>
+            <shard>
+                <internal_replication>true</internal_replication>
+                <replica>
+                    <host>n1</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>n2</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>n3</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </test_single_shard_multiple_replicas>
+    </remote_servers>
+</clickhouse>
+
diff --git a/tests/integration/test_parallel_replicas_distributed_skip_shards/test.py b/tests/integration/test_parallel_replicas_distributed_skip_shards/test.py
new file mode 100644
index 000000000000..315a9781c8b4
--- /dev/null
+++ b/tests/integration/test_parallel_replicas_distributed_skip_shards/test.py
@@ -0,0 +1,164 @@
+import pytest
+from helpers.cluster import ClickHouseCluster
+from helpers.client import QueryRuntimeException
+
+cluster = ClickHouseCluster(__file__)
+
+# create only 2 nodes out of 3 nodes in cluster with 1 shard
+# and out of 6 nodes in first shard in cluster with 2 shards
+node1 = cluster.add_instance(
+    "n1", main_configs=["configs/remote_servers.xml"], with_zookeeper=True
+)
+node2 = cluster.add_instance(
+    "n2", main_configs=["configs/remote_servers.xml"], with_zookeeper=True
+)
+
+
+@pytest.fixture(scope="module", autouse=True)
+def start_cluster():
+    try:
+        cluster.start()
+        yield cluster
+    finally:
+        cluster.shutdown()
+
+
+def create_tables(cluster, table_name):
+    # create replicated tables
+    node1.query(f"DROP TABLE IF EXISTS {table_name} SYNC")
+    node2.query(f"DROP TABLE IF EXISTS {table_name} SYNC")
+
+    node1.query(
+        f"CREATE TABLE IF NOT EXISTS {table_name} (key Int64, value String) Engine=ReplicatedMergeTree('/test_parallel_replicas/shard1/{table_name}', 'r1') ORDER BY (key)"
+    )
+    node2.query(
+        f"CREATE TABLE IF NOT EXISTS {table_name} (key Int64, value String) Engine=ReplicatedMergeTree('/test_parallel_replicas/shard1/{table_name}', 'r2') ORDER BY (key)"
+    )
+
+    # create distributed table
+    node1.query(f"DROP TABLE IF EXISTS {table_name}_d SYNC")
+    node1.query(
+        f"""
+            CREATE TABLE {table_name}_d AS {table_name}
+            Engine=Distributed(
+                {cluster},
+                currentDatabase(),
+                {table_name},
+                key
+            )
+            """
+    )
+
+    # populate data
+    node1.query(f"INSERT INTO {table_name} SELECT number, number FROM numbers(1000)")
+    node2.query(f"INSERT INTO {table_name} SELECT -number, -number FROM numbers(1000)")
+    node1.query(f"INSERT INTO {table_name} SELECT number, number FROM numbers(3)")
+
+
+@pytest.mark.parametrize(
+    "prefer_localhost_replica",
+    [
+        pytest.param(0),
+        pytest.param(1),
+    ],
+)
+def test_skip_unavailable_shards(start_cluster, prefer_localhost_replica):
+    cluster = "test_multiple_shards_multiple_replicas"
+    table_name = "test_table"
+    create_tables(cluster, table_name)
+
+    expected_result = f"2003\t-999\t999\t3
"
+
+    # w/o parallel replicas
+    assert (
+        node1.query(
+            f"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d settings skip_unavailable_shards=1"
+        )
+        == expected_result
+    )
+
+    # parallel replicas
+    assert (
+        node1.query(
+            f"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d",
+            settings={
+                "allow_experimental_parallel_reading_from_replicas": 2,
+                "max_parallel_replicas": 3,
+                "use_hedged_requests": 0,
+                "prefer_localhost_replica": prefer_localhost_replica,
+                "skip_unavailable_shards": 1,
+                "connections_with_failover_max_tries": 0,  # just don't wait for unavailable replicas
+            },
+        )
+        == expected_result
+    )
+
+
+@pytest.mark.parametrize(
+    "prefer_localhost_replica",
+    [
+        pytest.param(0),
+        pytest.param(1),
+    ],
+)
+def test_error_on_unavailable_shards(start_cluster, prefer_localhost_replica):
+    cluster = "test_multiple_shards_multiple_replicas"
+    table_name = "test_table"
+    create_tables(cluster, table_name)
+
+    # w/o parallel replicas
+    with pytest.raises(QueryRuntimeException):
+        node1.query(
+            f"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d settings skip_unavailable_shards=0"
+        )
+
+    # parallel replicas
+    with pytest.raises(QueryRuntimeException):
+        node1.query(
+            f"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d",
+            settings={
+                "allow_experimental_parallel_reading_from_replicas": 2,
+                "max_parallel_replicas": 3,
+                "use_hedged_requests": 0,
+                "prefer_localhost_replica": prefer_localhost_replica,
+                "skip_unavailable_shards": 0,
+            },
+        )
+
+
+@pytest.mark.parametrize(
+    "skip_unavailable_shards",
+    [
+        pytest.param(0),
+        pytest.param(1),
+    ],
+)
+def test_no_unavailable_shards(start_cluster, skip_unavailable_shards):
+    cluster = "test_single_shard_multiple_replicas"
+    table_name = "test_table"
+    create_tables(cluster, table_name)
+
+    expected_result = f"2003\t-999\t999\t3
"
+
+    # w/o parallel replicas
+    assert (
+        node1.query(
+            f"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d settings skip_unavailable_shards={skip_unavailable_shards}"
+        )
+        == expected_result
+    )
+
+    # parallel replicas
+    assert (
+        node1.query(
+            f"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d",
+            settings={
+                "allow_experimental_parallel_reading_from_replicas": 2,
+                "max_parallel_replicas": 3,
+                "use_hedged_requests": 0,
+                "prefer_localhost_replica": 0,
+                "skip_unavailable_shards": skip_unavailable_shards,
+            },
+        )
+        == expected_result
+    )
diff --git a/tests/integration/test_parallel_replicas_skip_shards/__init__.py b/tests/integration/test_parallel_replicas_skip_shards/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/tests/integration/test_parallel_replicas_skip_shards/configs/remote_servers.xml b/tests/integration/test_parallel_replicas_skip_shards/configs/remote_servers.xml
new file mode 100644
index 000000000000..7caa44d4df17
--- /dev/null
+++ b/tests/integration/test_parallel_replicas_skip_shards/configs/remote_servers.xml
@@ -0,0 +1,34 @@
+<clickhouse>
+    <remote_servers>
+        <two_shards>
+            <shard>
+                <replica>
+                    <host>node1</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>node2</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>node3</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+            <shard>
+                <replica>
+                    <host>node4</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>node5</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>node6</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </two_shards>
+    </remote_servers>
+</clickhouse>
diff --git a/tests/integration/test_parallel_replicas_skip_shards/test.py b/tests/integration/test_parallel_replicas_skip_shards/test.py
new file mode 100644
index 000000000000..3df80ba061e8
--- /dev/null
+++ b/tests/integration/test_parallel_replicas_skip_shards/test.py
@@ -0,0 +1,71 @@
+import pytest
+
+from helpers.cluster import ClickHouseCluster
+from helpers.client import QueryRuntimeException
+
+cluster = ClickHouseCluster(__file__)
+
+node1 = cluster.add_instance("node1", main_configs=["configs/remote_servers.xml"])
+node2 = cluster.add_instance("node2", main_configs=["configs/remote_servers.xml"])
+node3 = cluster.add_instance("node3", main_configs=["configs/remote_servers.xml"])
+
+
+@pytest.fixture(scope="module")
+def start_cluster():
+    try:
+        cluster.start()
+        yield cluster
+    finally:
+        cluster.shutdown()
+
+
+def test_skip_unavailable_shards(start_cluster):
+    expected = "node1
node2
node3
"
+    assert (
+        node1.query(
+            "SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h",
+            settings={
+                "allow_experimental_parallel_reading_from_replicas": 0,
+                "skip_unavailable_shards": 1,
+            },
+        )
+        == expected
+    )
+
+    assert (
+        node1.query(
+            "SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h",
+            settings={
+                "allow_experimental_parallel_reading_from_replicas": 2,
+                "max_parallel_replicas": 3,
+                "use_hedged_requests": 0,
+                "skip_unavailable_shards": 1,
+                # "async_socket_for_remote" : 0,
+                # "async_query_sending_for_remote" : 0,
+                # "connections_with_failover_max_tries": 0,
+            },
+        )
+        == expected
+    )
+
+
+def test_error_on_unavailable_shards(start_cluster):
+    with pytest.raises(QueryRuntimeException):
+        node1.query(
+            "SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h",
+            settings={
+                "allow_experimental_parallel_reading_from_replicas": 0,
+                "skip_unavailable_shards": 0,
+            },
+        )
+
+    with pytest.raises(QueryRuntimeException):
+        node1.query(
+            "SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h",
+            settings={
+                "allow_experimental_parallel_reading_from_replicas": 2,
+                "max_parallel_replicas": 3,
+                "use_hedged_requests": 0,
+                "skip_unavailable_shards": 0,
+            },
+        )
diff --git a/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql b/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql
index ecc243b9c895..020a429c1091 100644
--- a/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql
+++ b/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql
@@ -4,7 +4,7 @@ INSERT INTO test_parallel_replicas_unavailable_shards SELECT * FROM numbers(10);
 
 SYSTEM FLUSH LOGS;
 
-SET skip_unavailable_shards=1, allow_experimental_parallel_reading_from_replicas=1, max_parallel_replicas=11, use_hedged_requests=0, cluster_for_parallel_replicas='parallel_replicas', parallel_replicas_for_non_replicated_merge_tree=1;
+SET allow_experimental_parallel_reading_from_replicas=2, max_parallel_replicas=11, use_hedged_requests=0, cluster_for_parallel_replicas='parallel_replicas', parallel_replicas_for_non_replicated_merge_tree=1;
 SET send_logs_level='error';
 SELECT count() FROM test_parallel_replicas_unavailable_shards WHERE NOT ignore(*);
 
