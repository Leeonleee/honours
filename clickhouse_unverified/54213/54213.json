{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 54213,
  "instance_id": "ClickHouse__ClickHouse-54213",
  "issue_numbers": [
    "53594"
  ],
  "base_commit": "86223699be977ab9fbd0abf850dd40303d23d6b8",
  "patch": "diff --git a/src/Client/ConnectionPoolWithFailover.cpp b/src/Client/ConnectionPoolWithFailover.cpp\nindex feb4c01c3748..bc93d1a44e1b 100644\n--- a/src/Client/ConnectionPoolWithFailover.cpp\n+++ b/src/Client/ConnectionPoolWithFailover.cpp\n@@ -113,14 +113,15 @@ ConnectionPoolWithFailover::Status ConnectionPoolWithFailover::getStatus() const\n std::vector<IConnectionPool::Entry> ConnectionPoolWithFailover::getMany(const ConnectionTimeouts & timeouts,\n                                                                         const Settings * settings,\n                                                                         PoolMode pool_mode,\n-                                                                        AsyncCallback async_callback)\n+                                                                        AsyncCallback async_callback,\n+                                                                        std::optional<bool> skip_unavailable_endpoints)\n {\n     TryGetEntryFunc try_get_entry = [&](NestedPool & pool, std::string & fail_message)\n     {\n         return tryGetEntry(pool, timeouts, fail_message, settings, nullptr, async_callback);\n     };\n \n-    std::vector<TryResult> results = getManyImpl(settings, pool_mode, try_get_entry);\n+    std::vector<TryResult> results = getManyImpl(settings, pool_mode, try_get_entry, skip_unavailable_endpoints);\n \n     std::vector<Entry> entries;\n     entries.reserve(results.size());\n@@ -146,14 +147,15 @@ std::vector<ConnectionPoolWithFailover::TryResult> ConnectionPoolWithFailover::g\n     const ConnectionTimeouts & timeouts,\n     const Settings * settings, PoolMode pool_mode,\n     const QualifiedTableName & table_to_check,\n-    AsyncCallback async_callback)\n+    AsyncCallback async_callback,\n+    std::optional<bool> skip_unavailable_endpoints)\n {\n     TryGetEntryFunc try_get_entry = [&](NestedPool & pool, std::string & fail_message)\n     {\n         return tryGetEntry(pool, timeouts, fail_message, settings, &table_to_check, async_callback);\n     };\n \n-    return getManyImpl(settings, pool_mode, try_get_entry);\n+    return getManyImpl(settings, pool_mode, try_get_entry, skip_unavailable_endpoints);\n }\n \n ConnectionPoolWithFailover::Base::GetPriorityFunc ConnectionPoolWithFailover::makeGetPriorityFunc(const Settings * settings)\n@@ -172,13 +174,18 @@ ConnectionPoolWithFailover::Base::GetPriorityFunc ConnectionPoolWithFailover::ma\n std::vector<ConnectionPoolWithFailover::TryResult> ConnectionPoolWithFailover::getManyImpl(\n         const Settings * settings,\n         PoolMode pool_mode,\n-        const TryGetEntryFunc & try_get_entry)\n+        const TryGetEntryFunc & try_get_entry,\n+        std::optional<bool> skip_unavailable_endpoints)\n {\n     if (nested_pools.empty())\n         throw DB::Exception(DB::ErrorCodes::ALL_CONNECTION_TRIES_FAILED,\n                             \"Cannot get connection from ConnectionPoolWithFailover cause nested pools are empty\");\n \n-    size_t min_entries = (settings && settings->skip_unavailable_shards) ? 0 : 1;\n+    if (!skip_unavailable_endpoints.has_value())\n+        skip_unavailable_endpoints = (settings && settings->skip_unavailable_shards);\n+\n+    size_t min_entries = skip_unavailable_endpoints.value() ? 0 : 1;\n+\n     size_t max_tries = (settings ?\n         size_t{settings->connections_with_failover_max_tries} :\n         size_t{DBMS_CONNECTION_POOL_WITH_FAILOVER_DEFAULT_MAX_TRIES});\ndiff --git a/src/Client/ConnectionPoolWithFailover.h b/src/Client/ConnectionPoolWithFailover.h\nindex 75a0dafd9772..72a441fe3d6b 100644\n--- a/src/Client/ConnectionPoolWithFailover.h\n+++ b/src/Client/ConnectionPoolWithFailover.h\n@@ -55,7 +55,8 @@ class ConnectionPoolWithFailover : public IConnectionPool, private PoolWithFailo\n       */\n     std::vector<Entry> getMany(const ConnectionTimeouts & timeouts,\n                                const Settings * settings, PoolMode pool_mode,\n-                               AsyncCallback async_callback = {});\n+                               AsyncCallback async_callback = {},\n+                               std::optional<bool> skip_unavailable_endpoints = std::nullopt);\n \n     /// The same as getMany(), but return std::vector<TryResult>.\n     std::vector<TryResult> getManyForTableFunction(const ConnectionTimeouts & timeouts,\n@@ -71,7 +72,8 @@ class ConnectionPoolWithFailover : public IConnectionPool, private PoolWithFailo\n             const Settings * settings,\n             PoolMode pool_mode,\n             const QualifiedTableName & table_to_check,\n-            AsyncCallback async_callback = {});\n+            AsyncCallback async_callback = {},\n+            std::optional<bool> skip_unavailable_endpoints = std::nullopt);\n \n     struct NestedPoolStatus\n     {\n@@ -98,7 +100,8 @@ class ConnectionPoolWithFailover : public IConnectionPool, private PoolWithFailo\n     std::vector<TryResult> getManyImpl(\n             const Settings * settings,\n             PoolMode pool_mode,\n-            const TryGetEntryFunc & try_get_entry);\n+            const TryGetEntryFunc & try_get_entry,\n+            std::optional<bool> skip_unavailable_endpoints = std::nullopt);\n \n     /// Try to get a connection from the pool and check that it is good.\n     /// If table_to_check is not null and the check is enabled in settings, check that replication delay\ndiff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 5dd6af3f6f72..de2d34162a8c 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -463,12 +463,6 @@ InterpreterSelectQuery::InterpreterSelectQuery(\n         }\n     }\n \n-    /// Set skip_unavailable_shards to true only if it wasn't disabled explicitly\n-    if (settings.allow_experimental_parallel_reading_from_replicas > 0 && !settings.skip_unavailable_shards && !settings.isChanged(\"skip_unavailable_shards\"))\n-    {\n-        context->setSetting(\"skip_unavailable_shards\", true);\n-    }\n-\n     /// Check support for JOIN for parallel replicas with custom key\n     if (joined_tables.tablesCount() > 1 && !settings.parallel_replicas_custom_key.value.empty())\n     {\ndiff --git a/src/QueryPipeline/RemoteQueryExecutor.cpp b/src/QueryPipeline/RemoteQueryExecutor.cpp\nindex 198c3265a840..eebe97970512 100644\n--- a/src/QueryPipeline/RemoteQueryExecutor.cpp\n+++ b/src/QueryPipeline/RemoteQueryExecutor.cpp\n@@ -108,7 +108,7 @@ RemoteQueryExecutor::RemoteQueryExecutor(\n     , scalars(scalars_), external_tables(external_tables_), stage(stage_)\n     , extension(extension_)\n {\n-    create_connections = [this, pool, throttler, extension_](AsyncCallback async_callback)->std::unique_ptr<IConnections>\n+    create_connections = [this, pool, throttler](AsyncCallback async_callback)->std::unique_ptr<IConnections>\n     {\n         const Settings & current_settings = context->getSettingsRef();\n         auto timeouts = ConnectionTimeouts::getTCPTimeoutsWithFailover(current_settings);\n@@ -121,26 +121,32 @@ RemoteQueryExecutor::RemoteQueryExecutor(\n                 table_to_check = std::make_shared<QualifiedTableName>(main_table.getQualifiedName());\n \n             auto res = std::make_unique<HedgedConnections>(pool, context, timeouts, throttler, pool_mode, table_to_check, std::move(async_callback));\n-            if (extension_ && extension_->replica_info)\n-                res->setReplicaInfo(*extension_->replica_info);\n+            if (extension && extension->replica_info)\n+                res->setReplicaInfo(*extension->replica_info);\n             return res;\n         }\n #endif\n \n         std::vector<IConnectionPool::Entry> connection_entries;\n+        std::optional<bool> skip_unavailable_endpoints;\n+        if (extension && extension->parallel_reading_coordinator)\n+            skip_unavailable_endpoints = true;\n+\n         if (main_table)\n         {\n-            auto try_results = pool->getManyChecked(timeouts, &current_settings, pool_mode, main_table.getQualifiedName(), std::move(async_callback));\n+            auto try_results = pool->getManyChecked(timeouts, &current_settings, pool_mode, main_table.getQualifiedName(), std::move(async_callback), skip_unavailable_endpoints);\n             connection_entries.reserve(try_results.size());\n             for (auto & try_result : try_results)\n                 connection_entries.emplace_back(std::move(try_result.entry));\n         }\n         else\n-            connection_entries = pool->getMany(timeouts, &current_settings, pool_mode, std::move(async_callback));\n+        {\n+            connection_entries = pool->getMany(timeouts, &current_settings, pool_mode, std::move(async_callback), skip_unavailable_endpoints);\n+        }\n \n         auto res = std::make_unique<MultiplexedConnections>(std::move(connection_entries), current_settings, throttler);\n-        if (extension_ && extension_->replica_info)\n-            res->setReplicaInfo(*extension_->replica_info);\n+        if (extension && extension->replica_info)\n+            res->setReplicaInfo(*extension->replica_info);\n         return res;\n     };\n }\n@@ -237,7 +243,7 @@ void RemoteQueryExecutor::sendQueryUnlocked(ClientInfo::QueryKind query_kind, As\n     AsyncCallbackSetter async_callback_setter(connections.get(), async_callback);\n \n     const auto & settings = context->getSettingsRef();\n-    if (needToSkipUnavailableShard())\n+    if (isReplicaUnavailable() || needToSkipUnavailableShard())\n     {\n         /// To avoid sending the query again in the read(), we need to update the following flags:\n         was_cancelled = true;\n@@ -363,7 +369,7 @@ RemoteQueryExecutor::ReadResult RemoteQueryExecutor::readAsync()\n \n         read_context->resume();\n \n-        if (needToSkipUnavailableShard())\n+        if (isReplicaUnavailable() || needToSkipUnavailableShard())\n         {\n             /// We need to tell the coordinator not to wait for this replica.\n             /// But at this point it may lead to an incomplete result set, because\ndiff --git a/src/QueryPipeline/RemoteQueryExecutor.h b/src/QueryPipeline/RemoteQueryExecutor.h\nindex fb3baf4f983d..e5094b4705db 100644\n--- a/src/QueryPipeline/RemoteQueryExecutor.h\n+++ b/src/QueryPipeline/RemoteQueryExecutor.h\n@@ -186,6 +186,8 @@ class RemoteQueryExecutor\n \n     bool needToSkipUnavailableShard() const { return context->getSettingsRef().skip_unavailable_shards && (0 == connections->size()); }\n \n+    bool isReplicaUnavailable() const { return extension && extension->parallel_reading_coordinator && connections->size() == 0; }\n+\n private:\n     RemoteQueryExecutor(\n         const String & query_, const Block & header_, ContextPtr context_,\n",
  "test_patch": "diff --git a/tests/analyzer_integration_broken_tests.txt b/tests/analyzer_integration_broken_tests.txt\nindex 20ea31efa70f..080cd3f2677b 100644\n--- a/tests/analyzer_integration_broken_tests.txt\n+++ b/tests/analyzer_integration_broken_tests.txt\n@@ -95,3 +95,4 @@ test_odbc_interaction/test.py::test_postgres_insert\n test_zookeeper_config/test.py::test_chroot_with_different_root\n test_zookeeper_config/test.py::test_chroot_with_same_root\n test_merge_tree_azure_blob_storage/test.py::test_table_manipulations\n+test_parallel_replicas_skip_shards/test.py::test_skip_unavailable_shards\ndiff --git a/tests/integration/test_parallel_replicas_distributed_skip_shards/__init__.py b/tests/integration/test_parallel_replicas_distributed_skip_shards/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_parallel_replicas_distributed_skip_shards/configs/remote_servers.xml b/tests/integration/test_parallel_replicas_distributed_skip_shards/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..48c1587eae3a\n--- /dev/null\n+++ b/tests/integration/test_parallel_replicas_distributed_skip_shards/configs/remote_servers.xml\n@@ -0,0 +1,54 @@\n+<clickhouse>\n+    <remote_servers>\n+        <test_multiple_shards_multiple_replicas>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>n1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>n2</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>n3</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>n4</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>n5</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>n6</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_multiple_shards_multiple_replicas>\n+        <test_single_shard_multiple_replicas>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>n1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>n2</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>n3</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_single_shard_multiple_replicas>\n+    </remote_servers>\n+</clickhouse>\n+\ndiff --git a/tests/integration/test_parallel_replicas_distributed_skip_shards/test.py b/tests/integration/test_parallel_replicas_distributed_skip_shards/test.py\nnew file mode 100644\nindex 000000000000..315a9781c8b4\n--- /dev/null\n+++ b/tests/integration/test_parallel_replicas_distributed_skip_shards/test.py\n@@ -0,0 +1,164 @@\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+from helpers.client import QueryRuntimeException\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+# create only 2 nodes out of 3 nodes in cluster with 1 shard\n+# and out of 6 nodes in first shard in cluster with 2 shards\n+node1 = cluster.add_instance(\n+    \"n1\", main_configs=[\"configs/remote_servers.xml\"], with_zookeeper=True\n+)\n+node2 = cluster.add_instance(\n+    \"n2\", main_configs=[\"configs/remote_servers.xml\"], with_zookeeper=True\n+)\n+\n+\n+@pytest.fixture(scope=\"module\", autouse=True)\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def create_tables(cluster, table_name):\n+    # create replicated tables\n+    node1.query(f\"DROP TABLE IF EXISTS {table_name} SYNC\")\n+    node2.query(f\"DROP TABLE IF EXISTS {table_name} SYNC\")\n+\n+    node1.query(\n+        f\"CREATE TABLE IF NOT EXISTS {table_name} (key Int64, value String) Engine=ReplicatedMergeTree('/test_parallel_replicas/shard1/{table_name}', 'r1') ORDER BY (key)\"\n+    )\n+    node2.query(\n+        f\"CREATE TABLE IF NOT EXISTS {table_name} (key Int64, value String) Engine=ReplicatedMergeTree('/test_parallel_replicas/shard1/{table_name}', 'r2') ORDER BY (key)\"\n+    )\n+\n+    # create distributed table\n+    node1.query(f\"DROP TABLE IF EXISTS {table_name}_d SYNC\")\n+    node1.query(\n+        f\"\"\"\n+            CREATE TABLE {table_name}_d AS {table_name}\n+            Engine=Distributed(\n+                {cluster},\n+                currentDatabase(),\n+                {table_name},\n+                key\n+            )\n+            \"\"\"\n+    )\n+\n+    # populate data\n+    node1.query(f\"INSERT INTO {table_name} SELECT number, number FROM numbers(1000)\")\n+    node2.query(f\"INSERT INTO {table_name} SELECT -number, -number FROM numbers(1000)\")\n+    node1.query(f\"INSERT INTO {table_name} SELECT number, number FROM numbers(3)\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"prefer_localhost_replica\",\n+    [\n+        pytest.param(0),\n+        pytest.param(1),\n+    ],\n+)\n+def test_skip_unavailable_shards(start_cluster, prefer_localhost_replica):\n+    cluster = \"test_multiple_shards_multiple_replicas\"\n+    table_name = \"test_table\"\n+    create_tables(cluster, table_name)\n+\n+    expected_result = f\"2003\\t-999\\t999\\t3\\n\"\n+\n+    # w/o parallel replicas\n+    assert (\n+        node1.query(\n+            f\"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d settings skip_unavailable_shards=1\"\n+        )\n+        == expected_result\n+    )\n+\n+    # parallel replicas\n+    assert (\n+        node1.query(\n+            f\"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d\",\n+            settings={\n+                \"allow_experimental_parallel_reading_from_replicas\": 2,\n+                \"max_parallel_replicas\": 3,\n+                \"use_hedged_requests\": 0,\n+                \"prefer_localhost_replica\": prefer_localhost_replica,\n+                \"skip_unavailable_shards\": 1,\n+                \"connections_with_failover_max_tries\": 0,  # just don't wait for unavailable replicas\n+            },\n+        )\n+        == expected_result\n+    )\n+\n+\n+@pytest.mark.parametrize(\n+    \"prefer_localhost_replica\",\n+    [\n+        pytest.param(0),\n+        pytest.param(1),\n+    ],\n+)\n+def test_error_on_unavailable_shards(start_cluster, prefer_localhost_replica):\n+    cluster = \"test_multiple_shards_multiple_replicas\"\n+    table_name = \"test_table\"\n+    create_tables(cluster, table_name)\n+\n+    # w/o parallel replicas\n+    with pytest.raises(QueryRuntimeException):\n+        node1.query(\n+            f\"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d settings skip_unavailable_shards=0\"\n+        )\n+\n+    # parallel replicas\n+    with pytest.raises(QueryRuntimeException):\n+        node1.query(\n+            f\"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d\",\n+            settings={\n+                \"allow_experimental_parallel_reading_from_replicas\": 2,\n+                \"max_parallel_replicas\": 3,\n+                \"use_hedged_requests\": 0,\n+                \"prefer_localhost_replica\": prefer_localhost_replica,\n+                \"skip_unavailable_shards\": 0,\n+            },\n+        )\n+\n+\n+@pytest.mark.parametrize(\n+    \"skip_unavailable_shards\",\n+    [\n+        pytest.param(0),\n+        pytest.param(1),\n+    ],\n+)\n+def test_no_unavailable_shards(start_cluster, skip_unavailable_shards):\n+    cluster = \"test_single_shard_multiple_replicas\"\n+    table_name = \"test_table\"\n+    create_tables(cluster, table_name)\n+\n+    expected_result = f\"2003\\t-999\\t999\\t3\\n\"\n+\n+    # w/o parallel replicas\n+    assert (\n+        node1.query(\n+            f\"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d settings skip_unavailable_shards={skip_unavailable_shards}\"\n+        )\n+        == expected_result\n+    )\n+\n+    # parallel replicas\n+    assert (\n+        node1.query(\n+            f\"SELECT count(), min(key), max(key), sum(key) FROM {table_name}_d\",\n+            settings={\n+                \"allow_experimental_parallel_reading_from_replicas\": 2,\n+                \"max_parallel_replicas\": 3,\n+                \"use_hedged_requests\": 0,\n+                \"prefer_localhost_replica\": 0,\n+                \"skip_unavailable_shards\": skip_unavailable_shards,\n+            },\n+        )\n+        == expected_result\n+    )\ndiff --git a/tests/integration/test_parallel_replicas_skip_shards/__init__.py b/tests/integration/test_parallel_replicas_skip_shards/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_parallel_replicas_skip_shards/configs/remote_servers.xml b/tests/integration/test_parallel_replicas_skip_shards/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..7caa44d4df17\n--- /dev/null\n+++ b/tests/integration/test_parallel_replicas_skip_shards/configs/remote_servers.xml\n@@ -0,0 +1,34 @@\n+<clickhouse>\n+    <remote_servers>\n+        <two_shards>\n+            <shard>\n+                <replica>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node2</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node3</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+            <shard>\n+                <replica>\n+                    <host>node4</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node5</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node6</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </two_shards>\n+    </remote_servers>\n+</clickhouse>\ndiff --git a/tests/integration/test_parallel_replicas_skip_shards/test.py b/tests/integration/test_parallel_replicas_skip_shards/test.py\nnew file mode 100644\nindex 000000000000..3df80ba061e8\n--- /dev/null\n+++ b/tests/integration/test_parallel_replicas_skip_shards/test.py\n@@ -0,0 +1,71 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.client import QueryRuntimeException\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance(\"node1\", main_configs=[\"configs/remote_servers.xml\"])\n+node2 = cluster.add_instance(\"node2\", main_configs=[\"configs/remote_servers.xml\"])\n+node3 = cluster.add_instance(\"node3\", main_configs=[\"configs/remote_servers.xml\"])\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_skip_unavailable_shards(start_cluster):\n+    expected = \"node1\\nnode2\\nnode3\\n\"\n+    assert (\n+        node1.query(\n+            \"SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h\",\n+            settings={\n+                \"allow_experimental_parallel_reading_from_replicas\": 0,\n+                \"skip_unavailable_shards\": 1,\n+            },\n+        )\n+        == expected\n+    )\n+\n+    assert (\n+        node1.query(\n+            \"SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h\",\n+            settings={\n+                \"allow_experimental_parallel_reading_from_replicas\": 2,\n+                \"max_parallel_replicas\": 3,\n+                \"use_hedged_requests\": 0,\n+                \"skip_unavailable_shards\": 1,\n+                # \"async_socket_for_remote\" : 0,\n+                # \"async_query_sending_for_remote\" : 0,\n+                # \"connections_with_failover_max_tries\": 0,\n+            },\n+        )\n+        == expected\n+    )\n+\n+\n+def test_error_on_unavailable_shards(start_cluster):\n+    with pytest.raises(QueryRuntimeException):\n+        node1.query(\n+            \"SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h\",\n+            settings={\n+                \"allow_experimental_parallel_reading_from_replicas\": 0,\n+                \"skip_unavailable_shards\": 0,\n+            },\n+        )\n+\n+    with pytest.raises(QueryRuntimeException):\n+        node1.query(\n+            \"SELECT hostName() as h FROM clusterAllReplicas('two_shards', system.one) order by h\",\n+            settings={\n+                \"allow_experimental_parallel_reading_from_replicas\": 2,\n+                \"max_parallel_replicas\": 3,\n+                \"use_hedged_requests\": 0,\n+                \"skip_unavailable_shards\": 0,\n+            },\n+        )\ndiff --git a/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql b/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql\nindex ecc243b9c895..020a429c1091 100644\n--- a/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql\n+++ b/tests/queries/0_stateless/02769_parallel_replicas_unavailable_shards.sql\n@@ -4,7 +4,7 @@ INSERT INTO test_parallel_replicas_unavailable_shards SELECT * FROM numbers(10);\n \n SYSTEM FLUSH LOGS;\n \n-SET skip_unavailable_shards=1, allow_experimental_parallel_reading_from_replicas=1, max_parallel_replicas=11, use_hedged_requests=0, cluster_for_parallel_replicas='parallel_replicas', parallel_replicas_for_non_replicated_merge_tree=1;\n+SET allow_experimental_parallel_reading_from_replicas=2, max_parallel_replicas=11, use_hedged_requests=0, cluster_for_parallel_replicas='parallel_replicas', parallel_replicas_for_non_replicated_merge_tree=1;\n SET send_logs_level='error';\n SELECT count() FROM test_parallel_replicas_unavailable_shards WHERE NOT ignore(*);\n \n",
  "problem_statement": "Parallel replicas with distributed queries mistakenly skipping unavailable shards.\nWhen you run a query with `allow_experimental_parallel_reading_from_replicas`, it should silently skip unavailable replicas, but shouldn't skip unavailable shards (when no replicas for a shard is available), unless it is explicitly turned on by the `skip_unavailable_shards` setting.\r\n\r\nFor example, a cluster with multiple shards, each of one replica appears when you query `FROM clusterAllReplicas(...)`\n",
  "hints_text": "",
  "created_at": "2023-09-02T21:55:00Z"
}