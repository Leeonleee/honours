{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 75270,
  "instance_id": "ClickHouse__ClickHouse-75270",
  "issue_numbers": [
    "62741",
    "56640",
    "74472"
  ],
  "base_commit": "af9631d0d986ed8abcaf205be5de729935d00fb8",
  "patch": "diff --git a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp\nindex 4b5c682700d9..b4700b38ae1b 100644\n--- a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp\n+++ b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp\n@@ -303,7 +303,11 @@ MergeTreeReadTaskColumns getReadTaskColumns(\n             if (!columns_from_previous_steps.contains(name))\n                 step_column_names.push_back(name);\n \n-        if (!step_column_names.empty())\n+        const bool has_adaptive_granularity = data_part_info_for_reader.getIndexGranularityInfo().mark_type.adaptive;\n+\n+        /// If part has non-adaptive granularity we always have to read at least one column\n+        /// because we cannot determine the correct size of the last granule without reading data.\n+        if (!step_column_names.empty() || !has_adaptive_granularity)\n             injectRequiredColumns(\n                 data_part_info_for_reader, storage_snapshot,\n                 with_subcolumns, step_column_names);\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03225_const_prewhere_non_ataptive.reference b/tests/queries/0_stateless/03225_const_prewhere_non_ataptive.reference\nnew file mode 100644\nindex 000000000000..647ee5072145\n--- /dev/null\n+++ b/tests/queries/0_stateless/03225_const_prewhere_non_ataptive.reference\n@@ -0,0 +1,4 @@\n+1\t1\n+1\t1\n+1\t1\n+1\t1\ndiff --git a/tests/queries/0_stateless/03225_const_prewhere_non_ataptive.sql b/tests/queries/0_stateless/03225_const_prewhere_non_ataptive.sql\nnew file mode 100644\nindex 000000000000..66546177cf4d\n--- /dev/null\n+++ b/tests/queries/0_stateless/03225_const_prewhere_non_ataptive.sql\n@@ -0,0 +1,23 @@\n+DROP TABLE IF EXISTS t_const_prewhere;\n+\n+CREATE TABLE t_const_prewhere (id Int16, row_ver UInt64)\n+ENGINE = MergeTree ORDER BY id\n+SETTINGS index_granularity_bytes = 0, index_granularity = 42, min_bytes_for_wide_part = 0, min_bytes_for_full_part_storage = 0;\n+\n+INSERT INTO t_const_prewhere FORMAT VALUES (1, 1);\n+\n+SELECT * FROM t_const_prewhere PREWHERE 1;\n+SELECT * FROM t_const_prewhere PREWHERE materialize(1);\n+\n+DROP TABLE IF EXISTS t_const_prewhere;\n+\n+CREATE TABLE t_const_prewhere (id Int16, row_ver UInt64)\n+ENGINE = MergeTree() ORDER BY id\n+SETTINGS index_granularity_bytes = '10M', index_granularity = 42;\n+\n+INSERT INTO t_const_prewhere FORMAT VALUES (1, 1);\n+\n+SELECT * FROM t_const_prewhere PREWHERE 1;\n+SELECT * FROM t_const_prewhere PREWHERE materialize(1);\n+\n+DROP TABLE IF EXISTS t_const_prewhere;\n",
  "problem_statement": "LOGICAL_ERROR when reading from merge tree table without adaptive index granularity (RangeReader read N rows, but M expected)\n> Please make sure that the version you're using is still supported (you can find the list [here](https://github.com/ClickHouse/ClickHouse/blob/master/SECURITY.md#scope-and-supported-versions)).\r\n\r\n> You have to provide the following information whenever possible.\r\n\r\n**Describe what's wrong**\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\n```\r\nReceived exception from server (version 24.4.1):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: RangeReader read 803 rows, but 1024 expected.: While executing MergeTreeSelect(pool: ReadPool, algorithm: Thread). (LOGICAL_ERROR)\r\n(query: SELECT   29, id = 11338881281426660955, * FROM account_test__fuzz_36 PREWHERE materialize(29);)\r\n```\r\n\r\n> A link to reproducer in [https://fiddle.clickhouse.com/](https://fiddle.clickhouse.com/).\r\n\r\nhttps://fiddle.clickhouse.com/3db4eb06-f421-46e9-a115-d01bf462fcf0\r\n\r\n**Does it reproduce on the most recent release?**\r\n\r\n[The list of releases](https://github.com/ClickHouse/ClickHouse/blob/master/utils/list-versions/version_date.tsv)\r\n\r\nYes, on HEAD\nLogical error: 'RangeReader read 576 rows, but 8192 expected.'\nhttps://s3.amazonaws.com/clickhouse-test-reports/56516/857b47de2eaffef8995217f20c8f348c5510fbff/fuzzer_astfuzzertsan/report.html\r\n\r\n```\r\n2023.11.11 11:54:41.536045 [ 676 ] {9f4b4aaa-956f-4065-be8f-500452cecacc} <Fatal> : Logical error: 'RangeReader read 576 rows, but 8192 expected.'.\r\n2023.11.11 11:54:41.537064 [ 908 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2023.11.11 11:54:41.537136 [ 908 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 696598418FD730A935D80DF7AB04D7B4E875F36D, git hash: 72e7c28ef4bd490451062cab14879261eaf11181) (from thread 676) Received signal 6\r\n2023.11.11 11:54:41.537209 [ 908 ] {} <Fatal> BaseDaemon: Signal description: Aborted\r\n2023.11.11 11:54:41.537246 [ 908 ] {} <Fatal> BaseDaemon: \r\n2023.11.11 11:54:41.537321 [ 908 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fca136ed9fc 0x00007fca13699476 0x00007fca1367f7f3 0x0000559f58cf4573 0x0000559f60db254b 0x0000559f60db2f54 0x0000559f5a09c3c6 0x0000559f6c7e9feb 0x0000559f6c7e7bb8 0x0000559f6c7e7b86 0x0000559f6c7f228b 0x0000559f6d3671aa 0x0000559f6c7d9af4 0x0000559f6d36a46a 0x0000559f6cce3dcb 0x0000559f6cd06c55 0x0000559f6ccf9c51 0x0000559f6ccfac91 0x0000559f60efdcf2 0x0000559f60f02b9d 0x0000559f60f02b02 0x0000559f60ef9967 0x0000559f60effbd2 0x0000559f58ceda4f 0x00007fca136ebac3 0x00007fca1377da40\r\n2023.11.11 11:54:41.537415 [ 908 ] {} <Fatal> BaseDaemon: ########################################\r\n2023.11.11 11:54:41.537648 [ 908 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 696598418FD730A935D80DF7AB04D7B4E875F36D, git hash: 72e7c28ef4bd490451062cab14879261eaf11181) (from thread 676) (query_id: 9f4b4aaa-956f-4065-be8f-500452cecacc) (query: SELECT count(ignore(*)) FROM data_02051__fuzz_3 PREWHERE 1023 AND ignore(10, 1024 AND 1025 AND ignore(ignore(1048577, (NULL AND NULL) AND NULL, 1025, ignore(-2147483649 AND 1048576 AND NULL AND 1, *), NULL AND NULL), ignore(100000000000000000000., (NULL AND NULL) AND 2147483646, -2147483647, NULL AND NULL, 1023 AND ignore(ignore(NULL, (NULL AND NULL) AND -2147483648, 10, ignore(0.0001, (NULL AND NULL) AND 1048577, 9223372036854775807, 65536 AND NULL, 0 AND 10), 65537 AND 65535), *) AND 0 AND 65535), *) AND NULL AND (NULL AND NULL), -9223372036854775807 AND (NULL AND NULL AND NULL) AND -2147483648, 7, 10 AND 65537 AND -1) AND 1025 AND NULL AND (NULL AND NULL) SETTINGS min_bytes_to_use_direct_io = 0, local_filesystem_read_method = 'mmap', local_filesystem_read_prefetch = 0, read_priority = 0, max_read_buffer_size = 1048576) Received signal Aborted (6)\r\n2023.11.11 11:54:41.537799 [ 908 ] {} <Fatal> BaseDaemon: \r\n2023.11.11 11:54:41.537924 [ 908 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fca136ed9fc 0x00007fca13699476 0x00007fca1367f7f3 0x0000559f58cf4573 0x0000559f60db254b 0x0000559f60db2f54 0x0000559f5a09c3c6 0x0000559f6c7e9feb 0x0000559f6c7e7bb8 0x0000559f6c7e7b86 0x0000559f6c7f228b 0x0000559f6d3671aa 0x0000559f6c7d9af4 0x0000559f6d36a46a 0x0000559f6cce3dcb 0x0000559f6cd06c55 0x0000559f6ccf9c51 0x0000559f6ccfac91 0x0000559f60efdcf2 0x0000559f60f02b9d 0x0000559f60f02b02 0x0000559f60ef9967 0x0000559f60effbd2 0x0000559f58ceda4f 0x00007fca136ebac3 0x00007fca1377da40\r\n2023.11.11 11:54:41.538108 [ 908 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007fca136ed9fc in ?\r\n2023.11.11 11:54:41.538236 [ 908 ] {} <Fatal> BaseDaemon: 6. ? @ 0x00007fca13699476 in ?\r\n2023.11.11 11:54:41.538335 [ 908 ] {} <Fatal> BaseDaemon: 7. ? @ 0x00007fca1367f7f3 in ?\r\n2023.11.11 11:54:53.333418 [ 908 ] {} <Fatal> BaseDaemon: 8. __interceptor_abort @ 0x0000000007e99573 in /workspace/clickhouse\r\n2023.11.11 11:54:53.661430 [ 908 ] {} <Fatal> BaseDaemon: 9. ./build_docker/./src/Common/Exception.cpp:0: DB::abortOnFailedAssertion(String const&) @ 0x000000000ff5754b in /workspace/clickhouse\r\n2023.11.11 11:54:53.988327 [ 908 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Common/Exception.cpp:0: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000ff57f54 in /workspace/clickhouse\r\n2023.11.11 11:55:05.114904 [ 908 ] {} <Fatal> BaseDaemon: 11. DB::Exception::Exception<unsigned long&, unsigned long const&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type, std::type_identity<unsigned long const&>::type>, unsigned long&, unsigned long const&) @ 0x00000000092413c6 in /workspace/clickhouse\r\n2023.11.11 11:55:05.513819 [ 908 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:1242: DB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult const&, unsigned long&) @ 0x000000001b98efeb in /workspace/clickhouse\r\n2023.11.11 11:55:05.796150 [ 908 ] {} <Fatal> BaseDaemon: 13. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:987: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x000000001b98cbb8 in /workspace/clickhouse\r\n2023.11.11 11:55:06.079903 [ 908 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:0: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x000000001b98cb86 in /workspace/clickhouse\r\n2023.11.11 11:55:06.316604 [ 908 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./src/Storages/MergeTree/MergeTreeReadTask.cpp:163: DB::MergeTreeReadTask::read(DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001b99728b in /workspace/clickhouse\r\n2023.11.11 11:55:08.352510 [ 908 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectAlgorithms.h:38: DB::MergeTreeThreadSelectAlgorithm::readFromTask(DB::MergeTreeReadTask&, DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001c50c1aa in /workspace/clickhouse\r\n2023.11.11 11:55:08.783246 [ 908 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:162: DB::MergeTreeSelectProcessor::read() @ 0x000000001b97eaf4 in /workspace/clickhouse\r\n2023.11.11 11:55:08.965197 [ 908 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:181: DB::MergeTreeSource::processReadResult(DB::ChunkAndProgress)\r\n2023.11.11 11:55:08.965410 [ 908 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:226: DB::MergeTreeSource::tryGenerate() @ 0x000000001c50f46a in /workspace/clickhouse\r\n2023.11.11 11:55:09.116249 [ 908 ] {} <Fatal> BaseDaemon: 19.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/optional:344: std::__optional_storage_base<DB::Chunk, false>::has_value[abi:v15000]() const\r\n2023.11.11 11:55:09.116481 [ 908 ] {} <Fatal> BaseDaemon: 19.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/optional:998: std::optional<DB::Chunk>::operator bool[abi:v15000]() const\r\n2023.11.11 11:55:09.116600 [ 908 ] {} <Fatal> BaseDaemon: 19. ./build_docker/./src/Processors/ISource.cpp:108: DB::ISource::work() @ 0x000000001be88dcb in /workspace/clickhouse\r\n2023.11.11 11:55:09.166808 [ 908 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:588: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::__sz[abi:v15000]() const\r\n2023.11.11 11:55:09.167023 [ 908 ] {} <Fatal> BaseDaemon: 20.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:616: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.11.11 11:55:09.167216 [ 908 ] {} <Fatal> BaseDaemon: 20.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:918: std::list<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.11.11 11:55:09.167368 [ 908 ] {} <Fatal> BaseDaemon: 20.4. inlined from ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:50: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*)\r\n2023.11.11 11:55:09.167477 [ 908 ] {} <Fatal> BaseDaemon: 20. ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:95: DB::ExecutionThreadContext::executeTask() @ 0x000000001beabc55 in /workspace/clickhouse\r\n2023.11.11 11:55:09.372272 [ 908 ] {} <Fatal> BaseDaemon: 21. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:272: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x000000001be9ec51 in /workspace/clickhouse\r\n2023.11.11 11:55:09.613200 [ 908 ] {} <Fatal> BaseDaemon: 22.1. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:370: operator()\r\n2023.11.11 11:55:09.613497 [ 908 ] {} <Fatal> BaseDaemon: 22.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: decltype(std::declval<DB::PipelineExecutor::spawnThreads()::$_0&>()()) std::__invoke[abi:v15000]<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)\r\n2023.11.11 11:55:09.613688 [ 908 ] {} <Fatal> BaseDaemon: 22.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: void std::__invoke_void_return_wrapper<void, true>::__call<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)\r\n2023.11.11 11:55:09.613844 [ 908 ] {} <Fatal> BaseDaemon: 22.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:235: std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>::operator()[abi:v15000]()\r\n2023.11.11 11:55:09.613964 [ 908 ] {} <Fatal> BaseDaemon: 22. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:716: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x000000001be9fc91 in /workspace/clickhouse\r\n2023.11.11 11:55:09.825954 [ 908 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:09.826179 [ 908 ] {} <Fatal> BaseDaemon: 23.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:09.826316 [ 908 ] {} <Fatal> BaseDaemon: 23.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.11.11 11:55:09.826443 [ 908 ] {} <Fatal> BaseDaemon: 23.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.11.11 11:55:09.826564 [ 908 ] {} <Fatal> BaseDaemon: 23. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x00000000100a2cf2 in /workspace/clickhouse\r\n2023.11.11 11:55:09.983893 [ 908 ] {} <Fatal> BaseDaemon: 24. ./build_docker/./src/Common/ThreadPool.cpp:0: ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'()::operator()() @ 0x00000000100a7b9d in /workspace/clickhouse\r\n2023.11.11 11:55:10.222417 [ 908 ] {} <Fatal> BaseDaemon: 25. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:717: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x00000000100a7b02 in /workspace/clickhouse\r\n2023.11.11 11:55:10.401473 [ 908 ] {} <Fatal> BaseDaemon: 26.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:10.401724 [ 908 ] {} <Fatal> BaseDaemon: 26.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.11.11 11:55:10.401879 [ 908 ] {} <Fatal> BaseDaemon: 26.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.11.11 11:55:10.402008 [ 908 ] {} <Fatal> BaseDaemon: 26.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.11.11 11:55:10.402125 [ 908 ] {} <Fatal> BaseDaemon: 26. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000001009e967 in /workspace/clickhouse\r\n2023.11.11 11:55:10.642451 [ 908 ] {} <Fatal> BaseDaemon: 27.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>::reset[abi:v15000](std::__thread_struct*)\r\n2023.11.11 11:55:10.642651 [ 908 ] {} <Fatal> BaseDaemon: 27.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr\r\n2023.11.11 11:55:10.642800 [ 908 ] {} <Fatal> BaseDaemon: 27.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:265: ~__tuple_leaf\r\n2023.11.11 11:55:10.642931 [ 908 ] {} <Fatal> BaseDaemon: 27.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:538: ~tuple\r\n2023.11.11 11:55:10.643170 [ 908 ] {} <Fatal> BaseDaemon: 27.5. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:48: std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>::operator()[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*) const\r\n2023.11.11 11:55:10.643439 [ 908 ] {} <Fatal> BaseDaemon: 27.6. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:305: std::unique_ptr<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>, std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>>::reset[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*)\r\n2023.11.11 11:55:10.643564 [ 908 ] {} <Fatal> BaseDaemon: 27.7. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr\r\n2023.11.11 11:55:10.643679 [ 908 ] {} <Fatal> BaseDaemon: 27. ./build_docker/./contrib/llvm-project/libcxx/include/thread:297: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x00000000100a4bd2 in /workspace/clickhouse\r\n2023.11.11 11:55:22.746228 [ 908 ] {} <Fatal> BaseDaemon: 28. __tsan_thread_start_func @ 0x0000000007e92a4f in /workspace/clickhouse\r\n2023.11.11 11:55:22.746457 [ 908 ] {} <Fatal> BaseDaemon: 29. ? @ 0x00007fca136ebac3 in ?\r\n2023.11.11 11:55:22.746564 [ 908 ] {} <Fatal> BaseDaemon: 30. ? @ 0x00007fca1377da40 in ?\r\n```\r\n\nExposing 'RangeReader read N rows, but M expected' error\nRelated to #62741 #56640\r\n\r\nThe issue is happened to parts with constant index granularity and triggered by having `PREWHERE` in the query.\r\nSo, having a MergeTree table with `index_granularity_bytes = 0, index_granularity = 42` settings and `1` row, and reading it, will trigger the following error:\r\n`RangeReader read 1 rows, but 42 expected`\r\n\r\nTechnically, `MergeTreeIndexGranularityInfo::fixed_index_granularity` \r\nhttps://github.com/ClickHouse/ClickHouse/blob/39a25b1d3995618d7f20fcfd13f5a3bc229f5f87/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.h#L41-L42\r\n\r\nis used to initialize `MergeTreeIndexGranularityConstant::last_mark_granularity` (it's `42` in our example)\r\nWhich in turn used to get the number of rows in the part range (one only in our example, i.e. last as well) in `MergeTreeIndexGranularityConstant::getRowsCountInRange()`\r\nhttps://github.com/ClickHouse/ClickHouse/blob/39a25b1d3995618d7f20fcfd13f5a3bc229f5f87/src/Storages/MergeTree/MergeTreeIndexGranularityConstant.cpp#L99\r\nWhich will return not a number of rows in the granule but its granularity\r\n\r\nCC @CurtizJ, fill free to use this PR for the fix if applicable\r\n\r\n### Changelog category (leave one):\r\n- Not for changelog (changelog entry is not required)\r\n\r\n\r\n> Information about CI checks: https://clickhouse.com/docs/en/development/continuous-integration/\r\n\r\n#### CI Settings (Only check the boxes if you know what you are doing)\r\n\r\nAll builds in Builds_1 and Builds_2 stages are always mandatory\r\nand will run independently of the checks below:\r\n\r\n- [ ] <!---ci_set_required--> Allow: All Required Checks\r\n- [ ] <!---ci_include_stateless--> Allow: Stateless tests\r\n- [ ] <!---ci_include_stateful--> Allow: Stateful tests\r\n- [ ] <!---ci_include_integration--> Allow: Integration Tests\r\n- [ ] <!---ci_include_performance--> Allow: Performance tests\r\n- [ ] <!---ci_set_builds--> Allow: All Builds\r\n- [ ] <!---batch_0_1--> Allow: batch 1, 2 for multi-batch jobs\r\n- [ ] <!---batch_2_3--> Allow: batch 3, 4, 5, 6 for multi-batch jobs\r\n---\r\n- [ ] <!---ci_exclude_style--> Exclude: Style check\r\n- [ ] <!---ci_exclude_fast--> Exclude: Fast test\r\n- [ ] <!---ci_exclude_asan--> Exclude: All with ASAN\r\n- [ ] <!---ci_exclude_tsan|msan|ubsan|coverage--> Exclude: All with TSAN, MSAN, UBSAN, Coverage\r\n- [ ] <!---ci_exclude_aarch64|release|debug--> Exclude: All with aarch64\r\n- [ ] <!---ci_exclude_release--> Exclude: All with release\r\n- [ ] <!---ci_exclude_debug--> Exclude: All with debug\r\n---\r\n- [ ] <!---ci_include_uzz--> Run only fuzzers related jobs (libFuzzer fuzzers, AST fuzzers, BuzzHouse, etc.)\r\n- [ ] <!---ci_exclude_ast--> Exclude: AST fuzzers\r\n---\r\n- [ ] <!---do_not_test--> Do not test\r\n- [ ] <!---woolen_wolfdog--> Woolen Wolfdog\r\n- [ ] <!---upload_all--> Upload binaries for special builds\r\n- [ ] <!---no_merge_commit--> Disable merge-commit\r\n- [ ] <!---no_ci_cache--> Disable CI cache\r\n\n",
  "hints_text": "Also https://github.com/ClickHouse/ClickHouse/issues/56640\nStarts failing in 23.8. 23.7 runs the reproducer fine\nSimple repro:\r\n```\r\nCREATE TABLE data_02051__fuzz_24 (`key` Int16, `value` String) ENGINE = MergeTree ORDER BY key SETTINGS index_granularity_bytes = 0, min_bytes_for_wide_part = 0 AS SELECT number, repeat(toString(number), 5) FROM numbers(1000000.)\r\n\r\n\r\nSELECT\r\n    concat(toNullable(3)),\r\n    count(ignore(concat(concat(concat(concat(38), toLowCardinality(toNullable(3)))), 3), *)) IGNORE NULLS\r\nFROM data_02051__fuzz_24\r\nPREWHERE materialize(38)\r\nGROUP BY\r\n    concat(concat(toNullable(3))),\r\n    ignore(concat(concat(38, ignore(concat(3), concat(position(concat(38), 'ca', 2), 3), 1, *), 3), 38, 3), *),\r\n    concat('ca', 2, 2, toUInt128(2))\r\nSETTINGS min_bytes_to_use_direct_io = 2147483648, local_filesystem_read_method = 'io_uring', local_filesystem_read_prefetch = 1, read_priority = 0, max_read_buffer_size = 256\r\n\r\nQuery id: 04401ab0-7571-404e-b433-636f618a0b65\r\n\r\n[alesapin-workstation] 2024.07.01 12:18:52.282219 [ 3316628 ] {04401ab0-7571-404e-b433-636f618a0b65} <Fatal> : Logical error: 'RangeReader read 16960 rows, but 24576 expected.'.\r\n[alesapin-workstation] 2024.07.01 12:18:52.283737 [ 3316654 ] <Fatal> BaseDaemon: ########################################\r\n[alesapin-workstation] 2024.07.01 12:18:52.284062 [ 3316654 ] <Fatal> BaseDaemon: (version 24.7.1.1, build id: FABBEFA5512100A51CF5EF5CFBB4359ECE68A8E2, git hash: 3294deacee38248322b537cdd7d3bc1547a933d7) (from thread 3316628) (query_id: 04401ab0-7571-404e-b433-636f618a0b65) (query: SELECT concat(toNullable(3)), count(ignore(concat(concat(concat(concat(38), toLowCardinality(toNullable(3)))), 3), *)) IGNORE NULLS FROM data_02051__fuzz_24 PREWHERE materialize(38) GROUP BY concat(concat(toNullable(3))), ignore(concat(concat(38, ignore(concat(3), concat(position(concat(38), 'ca', 2), 3), 1, *), 3), 38, 3), *), concat('ca', 2, 2, toUInt128(2)) SETTINGS min_bytes_to_use_direct_io = 2147483648, local_filesystem_read_method = 'io_uring', local_filesystem_read_prefetch = 1, read_priority = 0, max_read_buffer_size = 256) Received signal Aborted (6)\r\n[alesapin-workstation] 2024.07.01 12:18:52.284332 [ 3316654 ] <Fatal> BaseDaemon: \r\n[alesapin-workstation] 2024.07.01 12:18:52.284523 [ 3316654 ] <Fatal> BaseDaemon: Stack trace: 0x000000001328136a 0x0000000013281260 0x00000000136ec531 0x00007f20b70dbae0 0x00007f20b7133e45 0x00007f20b70dba30 0x00007f20b70c34c3 0x0000000013220602 0x0000000013220775 0x0000000013220d20 0x000000000acd720a 0x000000000acd49e9 0x000000000b910f6d 0x000000001d7b9f84 0x000000001d7b83f1 0x000000001d7cb589 0x000000001e87ed8c 0x000000001d7ec043 0x000000001e8d4c70 0x000000001e008f96 0x000000001e0452c3 0x000000001e045000 0x000000001e027141 0x000000001e0274cb 0x000000001e0282b8 0x000000001e028215 0x000000001e0281f5 0x000000001e0281d5 0x000000001e0281a0 0x0000000011ba7316 0x0000000011ba7015 0x0000000013351712 0x000000001335b684 0x000000001335b655 0x000000001335b639 0x000000001335b4dd 0x000000001335b3cc 0x000000001335b295 0x000000001335b275 0x000000001335b255 0x000000001335b220 0x0000000011ba7316 0x0000000011ba7015 0x000000001334e212 0x0000000013355ce4\r\n[alesapin-workstation] 2024.07.01 12:18:52.367935 [ 3316654 ] <Fatal> BaseDaemon: 0. ./build/./src/Common/StackTrace.cpp:349: StackTrace::tryCapture() @ 0x000000001328136a\r\n[alesapin-workstation] 2024.07.01 12:18:52.459122 [ 3316654 ] <Fatal> BaseDaemon: 1. ./build/./src/Common/StackTrace.cpp:323: StackTrace::StackTrace(ucontext_t const&) @ 0x0000000013281260\r\n[alesapin-workstation] 2024.07.01 12:18:52.633860 [ 3316654 ] <Fatal> BaseDaemon: 2. ./build/./src/Daemon/BaseDaemon.cpp:158: signalHandler(int, siginfo_t*, void*) @ 0x00000000136ec531\r\n[alesapin-workstation] 2024.07.01 12:18:52.634107 [ 3316654 ] <Fatal> BaseDaemon: 3. ? @ 0x00007f20b70dbae0\r\n[alesapin-workstation] 2024.07.01 12:18:52.634332 [ 3316654 ] <Fatal> BaseDaemon: 4. ? @ 0x00007f20b7133e45\r\n[alesapin-workstation] 2024.07.01 12:18:52.634499 [ 3316654 ] <Fatal> BaseDaemon: 5. ? @ 0x00007f20b70dba30\r\n[alesapin-workstation] 2024.07.01 12:18:52.634679 [ 3316654 ] <Fatal> BaseDaemon: 6. ? @ 0x00007f20b70c34c3\r\n[alesapin-workstation] 2024.07.01 12:18:52.743441 [ 3316654 ] <Fatal> BaseDaemon: 7. ./build/./src/Common/Exception.cpp:0: DB::abortOnFailedAssertion(String const&) @ 0x0000000013220602\r\n[alesapin-workstation] 2024.07.01 12:18:52.846894 [ 3316654 ] <Fatal> BaseDaemon: 8. ./build/./src/Common/Exception.cpp:65: DB::handle_error_code(String const&, int, bool, std::vector<void*, std::allocator<void*>> const&) @ 0x0000000013220775\r\n[alesapin-workstation] 2024.07.01 12:18:52.930615 [ 3316654 ] <Fatal> BaseDaemon: 9. ./build/./src/Common/Exception.cpp:105: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000013220d20\r\n[alesapin-workstation] 2024.07.01 12:18:53.921442 [ 3316654 ] <Fatal> BaseDaemon: 10. DB::Exception::Exception(String&&, int, bool) @ 0x000000000acd720a\r\n[alesapin-workstation] 2024.07.01 12:18:54.846033 [ 3316654 ] <Fatal> BaseDaemon: 11. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x000000000acd49e9\r\n[alesapin-workstation] 2024.07.01 12:18:55.779771 [ 3316654 ] <Fatal> BaseDaemon: 12. DB::Exception::Exception<unsigned long&, unsigned long const&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type, std::type_identity<unsigned long const&>::type>, unsigned long&, unsigned long const&) @ 0x000000000b910f6d\r\n[alesapin-workstation] 2024.07.01 12:18:55.899690 [ 3316654 ] <Fatal> BaseDaemon: 13. ./build/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:1270: DB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult const&, unsigned long&) @ 0x000000001d7b9f84\r\n[alesapin-workstation] 2024.07.01 12:18:56.012668 [ 3316654 ] <Fatal> BaseDaemon: 14. ./build/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:981: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x000000001d7b83f1\r\n[alesapin-workstation] 2024.07.01 12:18:56.113231 [ 3316654 ] <Fatal> BaseDaemon: 15. ./build/./src/Storages/MergeTree/MergeTreeReadTask.cpp:165: DB::MergeTreeReadTask::read(DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001d7cb589\r\n[alesapin-workstation] 2024.07.01 12:18:56.745287 [ 3316654 ] <Fatal> BaseDaemon: 16. ./src/Storages/MergeTree/MergeTreeSelectAlgorithms.h:38: DB::MergeTreeThreadSelectAlgorithm::readFromTask(DB::MergeTreeReadTask&, DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001e87ed8c\r\n[alesapin-workstation] 2024.07.01 12:18:56.844550 [ 3316654 ] <Fatal> BaseDaemon: 17. ./build/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:137: DB::MergeTreeSelectProcessor::read() @ 0x000000001d7ec043\r\n[alesapin-workstation] 2024.07.01 12:18:56.935969 [ 3316654 ] <Fatal> BaseDaemon: 18. ./build/./src/Storages/MergeTree/MergeTreeSource.cpp:226: DB::MergeTreeSource::tryGenerate() @ 0x000000001e8d4c70\r\n[alesapin-workstation] 2024.07.01 12:18:56.998336 [ 3316654 ] <Fatal> BaseDaemon: 19. ./build/./src/Processors/ISource.cpp:108: DB::ISource::work() @ 0x000000001e008f96\r\n[alesapin-workstation] 2024.07.01 12:18:57.040630 [ 3316654 ] <Fatal> BaseDaemon: 20. ./build/./src/Processors/Executors/ExecutionThreadContext.cpp:47: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*) @ 0x000000001e0452c3\r\n[alesapin-workstation] 2024.07.01 12:18:57.074019 [ 3316654 ] <Fatal> BaseDaemon: 21. ./build/./src/Processors/Executors/ExecutionThreadContext.cpp:96: DB::ExecutionThreadContext::executeTask() @ 0x000000001e045000\r\n[alesapin-workstation] 2024.07.01 12:18:57.172389 [ 3316654 ] <Fatal> BaseDaemon: 22. ./build/./src/Processors/Executors/PipelineExecutor.cpp:272: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x000000001e027141\r\n[alesapin-workstation] 2024.07.01 12:18:57.271744 [ 3316654 ] <Fatal> BaseDaemon: 23. ./build/./src/Processors/Executors/PipelineExecutor.cpp:238: DB::PipelineExecutor::executeSingleThread(unsigned long) @ 0x000000001e0274cb\r\n[alesapin-workstation] 2024.07.01 12:18:57.368321 [ 3316654 ] <Fatal> BaseDaemon: 24. ./build/./src/Processors/Executors/PipelineExecutor.cpp:372: DB::PipelineExecutor::spawnThreads()::$_0::operator()() const @ 0x000000001e0282b8\r\n[alesapin-workstation] 2024.07.01 12:18:57.487157 [ 3316654 ] <Fatal> BaseDaemon: 25. ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ? @ 0x000000001e028215\r\n[alesapin-workstation] 2024.07.01 12:18:57.598431 [ 3316654 ] <Fatal> BaseDaemon: 26. ./contrib/llvm-project/libcxx/include/__functional/invoke.h:480: ? @ 0x000000001e0281f5\r\n[alesapin-workstation] 2024.07.01 12:18:57.710208 [ 3316654 ] <Fatal> BaseDaemon: 27. ./contrib/llvm-project/libcxx/include/__functional/function.h:235: ? @ 0x000000001e0281d5\r\n[alesapin-workstation] 2024.07.01 12:18:57.822321 [ 3316654 ] <Fatal> BaseDaemon: 28. ./contrib/llvm-project/libcxx/include/__functional/function.h:716: ? @ 0x000000001e0281a0\r\n[alesapin-workstation] 2024.07.01 12:18:58.755238 [ 3316654 ] <Fatal> BaseDaemon: 29. std::__function::__policy_func<void ()>::operator()[abi:v15000]() const @ 0x0000000011ba7316\r\n[alesapin-workstation] 2024.07.01 12:18:59.703523 [ 3316654 ] <Fatal> BaseDaemon: 30. std::function<void ()>::operator()() const @ 0x0000000011ba7015\r\n[alesapin-workstation] 2024.07.01 12:18:59.779865 [ 3316654 ] <Fatal> BaseDaemon: 31. ./build/./src/Common/ThreadPool.cpp:462: ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false, true>, void*>) @ 0x0000000013351712\r\n[alesapin-workstation] 2024.07.01 12:18:59.876169 [ 3316654 ] <Fatal> BaseDaemon: 32. ./build/./src/Common/ThreadPool.cpp:219: void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()::operator()() const @ 0x000000001335b684\r\n[alesapin-workstation] 2024.07.01 12:18:59.980030 [ 3316654 ] <Fatal> BaseDaemon: 33. ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ? @ 0x000000001335b655\r\n[alesapin-workstation] 2024.07.01 12:19:00.084011 [ 3316654 ] <Fatal> BaseDaemon: 34. ./contrib/llvm-project/libcxx/include/tuple:1789: decltype(auto) std::__apply_tuple_impl[abi:v15000]<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()&, std::tuple<>&>(void&&, std::tuple<>&, std::__tuple_indices<>) @ 0x000000001335b639\r\n[alesapin-workstation] 2024.07.01 12:19:00.188562 [ 3316654 ] <Fatal> BaseDaemon: 35. ./contrib/llvm-project/libcxx/include/tuple:1798: decltype(auto) std::apply[abi:v15000]<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()&, std::tuple<>&>(void&&, std::tuple<>&) @ 0x000000001335b4dd\r\n[alesapin-workstation] 2024.07.01 12:19:00.267531 [ 3316654 ] <Fatal> BaseDaemon: 36. ./src/Common/ThreadPool.h:251: ThreadFromGlobalPoolImpl<false, true>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'()::operator()() @ 0x000000001335b3cc\r\n[alesapin-workstation] 2024.07.01 12:19:00.372310 [ 3316654 ] <Fatal> BaseDaemon: 37. ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ? @ 0x000000001335b295\r\n[alesapin-workstation] 2024.07.01 12:19:00.468808 [ 3316654 ] <Fatal> BaseDaemon: 38. ./contrib/llvm-project/libcxx/include/__functional/invoke.h:480: ? @ 0x000000001335b275\r\n[alesapin-workstation] 2024.07.01 12:19:00.565298 [ 3316654 ] <Fatal> BaseDaemon: 39. ./contrib/llvm-project/libcxx/include/__functional/function.h:235: ? @ 0x000000001335b255\r\n[alesapin-workstation] 2024.07.01 12:19:00.662297 [ 3316654 ] <Fatal> BaseDaemon: 40. ./contrib/llvm-project/libcxx/include/__functional/function.h:716: ? @ 0x000000001335b220\r\n[alesapin-workstation] 2024.07.01 12:19:01.604796 [ 3316654 ] <Fatal> BaseDaemon: 41. std::__function::__policy_func<void ()>::operator()[abi:v15000]() const @ 0x0000000011ba7316\r\n[alesapin-workstation] 2024.07.01 12:19:02.571943 [ 3316654 ] <Fatal> BaseDaemon: 42. std::function<void ()>::operator()() const @ 0x0000000011ba7015\r\n[alesapin-workstation] 2024.07.01 12:19:02.647439 [ 3316654 ] <Fatal> BaseDaemon: 43. ./build/./src/Common/ThreadPool.cpp:462: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000001334e212\r\n[alesapin-workstation] 2024.07.01 12:19:02.737973 [ 3316654 ] <Fatal> BaseDaemon: 44. ./build/./src/Common/ThreadPool.cpp:219: void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()::operator()() const @ 0x0000000013355ce4\r\n[alesapin-workstation] 2024.07.01 12:19:02.738152 [ 3316654 ] <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.\r\n[alesapin-workstation] 2024.07.01 12:19:12.996956 [ 3316654 ] <Fatal> BaseDaemon: This ClickHouse version is not official and should be upgraded to the official build.\r\n[alesapin-workstation] 2024.07.01 12:19:12.997466 [ 3316654 ] <Fatal> BaseDaemon: Changed settings: max_read_buffer_size = 256, connect_timeout_with_failover_ms = 2000, connect_timeout_with_failover_secure_ms = 3000, idle_connection_timeout = 36000, s3_check_objects_after_upload = true, stream_like_engine_allow_direct_select = true, replication_wait_for_inactive_replica_timeout = 30, load_balancing = 'random', min_bytes_to_use_direct_io = 2147483648, log_queries = true, insert_quorum_timeout = 60000, fsync_metadata = false, distributed_ddl_task_timeout = 120, http_send_timeout = 60., http_receive_timeout = 60., opentelemetry_start_trace_probability = 0.10000000149011612, max_memory_usage = 10000000000, max_untracked_memory = 1048576, memory_profiler_step = 1048576, trace_profile_events = true, database_atomic_wait_for_drop_and_detach_synchronously = true, database_replicated_initial_query_timeout_sec = 120, database_replicated_always_detach_permanently = true, distributed_ddl_output_mode = 'none', distributed_ddl_entry_format_version = 2, local_filesystem_read_method = 'io_uring', local_filesystem_read_prefetch = true, read_priority = 0, merge_tree_compact_parts_min_granules_to_multibuffer_read = 1, async_insert_busy_timeout_max_ms = 5000, enable_filesystem_cache = true, enable_filesystem_cache_on_write_operations = true, load_marks_asynchronously = true, allow_experimental_shared_merge_tree = true, allow_experimental_database_replicated = true, default_database_engine = 'Ordinary'\r\n\u2199 Progress: 983.04 thousand rows, 39.76 MB (40.72 thousand rows/s., 1.65 MB/s.)  96%Exception on client:\r\nCode: 32. DB::Exception: Attempt to read after eof: while receiving packet from localhost:9000. (ATTEMPT_TO_READ_AFTER_EOF)\r\n\r\n\r\n```\nThere is some more investigation here\r\nhttps://github.com/ClickHouse/ClickHouse/issues/56640#issuecomment-1900945712\r\nThe issue is that when we read no columns at the first PREWHERE step (e.g. `... PREWHERE materialize(1)`)  and the table has fixed index granularity, the reader incorrectly sees that the last granule has number of rows equal to the fixed_index_granularity\nEven simpler:\r\n```\r\nCREATE TABLE data_02051__fuzz_24 (`key` Int16, `value` String) ENGINE = MergeTree ORDER BY key SETTINGS index_granularity_bytes = 0 AS SELECT number, repeat(toString(number), 5) FROM numbers(1000000.)\r\n\r\nSELECT  count(ignore(*)) FROM data_02051__fuzz_24 PREWHERE 1 GROUP BY ignore(*);\r\n```\nhttps://s3.amazonaws.com/clickhouse-test-reports/69845/bf5e68f334f6128e1e440b047ef90ef311e22861/ast_fuzzer__debug_.html\none more:\r\nhttps://s3.amazonaws.com/clickhouse-test-reports/71625/78e56e7e3da58a9aeba292624254fa4d6eb3795a/ast_fuzzer__msan_.html\ncc @davenger \nduplicate of #56202?\n> duplicate of https://github.com/ClickHouse/ClickHouse/issues/56202?\r\n\r\n\r\nNo, the error message and the stacktrace are different.\nhttps://s3.amazonaws.com/clickhouse-test-reports/57205/82270627ed1c74ce96c1eb8100430de3b81b3efe/fuzzer_astfuzzermsan/report.html\nhttps://s3.amazonaws.com/clickhouse-test-reports/57467/434c2113d207b4507cac7f7cac88dbf553cc3474/fuzzer_astfuzzermsan/report.html\r\n\r\n```\r\n2023.12.04 15:11:49.999517 [ 1182 ] {49219680-1cd9-43de-af98-fc8eeceeaa2a} <Fatal> : Logical error: 'RangeReader read 10000 rows, but 16384 expected.'.\r\n2023.12.04 15:11:50.000383 [ 1255 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2023.12.04 15:11:50.000463 [ 1255 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 30C754DCB0A11F170684E63F74476BC05C09FECE, git hash: e0ff7b34220657b70e39d83d78a780d5675fc396) (from thread 1182) Received signal 6\r\n2023.12.04 15:11:50.000524 [ 1255 ] {} <Fatal> BaseDaemon: Signal description: Aborted\r\n2023.12.04 15:11:50.000583 [ 1255 ] {} <Fatal> BaseDaemon: \r\n2023.12.04 15:11:50.000658 [ 1255 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007f8bd45419fc 0x00007f8bd44ed476 0x00007f8bd44d37f3 0x000055bffb5460ca 0x000055bffb547404 0x000055bfe7526a6c 0x000055c01613b442 0x000055c0161353b7 0x000055c016154ee1 0x000055c017f7ba94 0x000055c01610d8ea 0x000055c017f17f00 0x000055c016e2b36c 0x000055c016e89097 0x000055c016e5ea71 0x000055c016e5a5bc 0x000055c016e5a028 0x000055c016e94d86 0x000055bffb84e26b 0x000055bffb85c8eb 0x00007f8bd453fac3 0x00007f8bd45d1a40\r\n2023.12.04 15:11:50.000727 [ 1255 ] {} <Fatal> BaseDaemon: ########################################\r\n2023.12.04 15:11:50.000877 [ 1255 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 30C754DCB0A11F170684E63F74476BC05C09FECE, git hash: e0ff7b34220657b70e39d83d78a780d5675fc396) (from thread 1182) (query_id: 49219680-1cd9-43de-af98-fc8eeceeaa2a) (query: WITH [1.1920928955078125e-7, 1.] AS reference_vec SELECT vec, [0.0001, 10000000000.], 65535, -2147483647, L2Distance(vec, reference_vec) FROM tab__fuzz_0 PREWHERE 3 ORDER BY 1000.0001 DESC NULLS FIRST, -inf DESC NULLS LAST LIMIT 9223372036854775807) Received signal Aborted (6)\r\n2023.12.04 15:11:50.001031 [ 1255 ] {} <Fatal> BaseDaemon: \r\n2023.12.04 15:11:50.001161 [ 1255 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007f8bd45419fc 0x00007f8bd44ed476 0x00007f8bd44d37f3 0x000055bffb5460ca 0x000055bffb547404 0x000055bfe7526a6c 0x000055c01613b442 0x000055c0161353b7 0x000055c016154ee1 0x000055c017f7ba94 0x000055c01610d8ea 0x000055c017f17f00 0x000055c016e2b36c 0x000055c016e89097 0x000055c016e5ea71 0x000055c016e5a5bc 0x000055c016e5a028 0x000055c016e94d86 0x000055bffb84e26b 0x000055bffb85c8eb 0x00007f8bd453fac3 0x00007f8bd45d1a40\r\n2023.12.04 15:11:50.001300 [ 1255 ] {} <Fatal> BaseDaemon: 4. ? @ 0x00007f8bd45419fc in ?\r\n2023.12.04 15:11:50.001425 [ 1255 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007f8bd44ed476 in ?\r\n2023.12.04 15:11:50.001545 [ 1255 ] {} <Fatal> BaseDaemon: 6. ? @ 0x00007f8bd44d37f3 in ?\r\n2023.12.04 15:11:50.175753 [ 1255 ] {} <Fatal> BaseDaemon: 7. ./build_docker/./src/Common/Exception.cpp:0: DB::abortOnFailedAssertion(String const&) @ 0x000000001f3c50ca in /workspace/clickhouse\r\n2023.12.04 15:11:50.333573 [ 1255 ] {} <Fatal> BaseDaemon: 8.1. inlined from ./build_docker/./src/Common/Exception.cpp:194: DB::Exception::getStackFramePointers() const\r\n2023.12.04 15:11:50.333790 [ 1255 ] {} <Fatal> BaseDaemon: 8. ./build_docker/./src/Common/Exception.cpp:101: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000001f3c6404 in /workspace/clickhouse\r\n2023.12.04 15:11:54.377579 [ 1255 ] {} <Fatal> BaseDaemon: 9. DB::Exception::Exception<unsigned long&, unsigned long const&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type, std::type_identity<unsigned long const&>::type>, unsigned long&, unsigned long const&) @ 0x000000000b3a5a6c in /workspace/clickhouse\r\n2023.12.04 15:11:54.609886 [ 1255 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:1246: DB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult const&, unsigned long&) @ 0x0000000039fba442 in /workspace/clickhouse\r\n2023.12.04 15:11:54.773283 [ 1255 ] {} <Fatal> BaseDaemon: 11. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:991: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x0000000039fb43b7 in /workspace/clickhouse\r\n2023.12.04 15:11:54.902637 [ 1255 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Storages/MergeTree/MergeTreeReadTask.cpp:163: DB::MergeTreeReadTask::read(DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x0000000039fd3ee1 in /workspace/clickhouse\r\n2023.12.04 15:11:54.988551 [ 1255 ] {} <Fatal> BaseDaemon: 13. ./src/Storages/MergeTree/MergeTreeSelectAlgorithms.h:53: DB::MergeTreeInOrderSelectAlgorithm::readFromTask(DB::MergeTreeReadTask&, DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000003bdfaa94 in /workspace/clickhouse\r\n2023.12.04 15:11:55.203020 [ 1255 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:162: DB::MergeTreeSelectProcessor::read() @ 0x0000000039f8c8ea in /workspace/clickhouse\r\n2023.12.04 15:11:55.303242 [ 1255 ] {} <Fatal> BaseDaemon: 15.1. inlined from ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:181: DB::MergeTreeSource::processReadResult(DB::ChunkAndProgress)\r\n2023.12.04 15:11:55.303353 [ 1255 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:226: DB::MergeTreeSource::tryGenerate() @ 0x000000003bd96f00 in /workspace/clickhouse\r\n2023.12.04 15:11:55.380176 [ 1255 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./contrib/llvm-project/libcxx/include/optional:344: std::__optional_storage_base<DB::Chunk, false>::has_value[abi:v15000]() const\r\n2023.12.04 15:11:55.380298 [ 1255 ] {} <Fatal> BaseDaemon: 16.2. inlined from ./contrib/llvm-project/libcxx/include/optional:998: std::optional<DB::Chunk>::operator bool[abi:v15000]() const\r\n2023.12.04 15:11:55.380347 [ 1255 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Processors/ISource.cpp:108: DB::ISource::work() @ 0x000000003acaa36c in /workspace/clickhouse\r\n2023.12.04 15:11:55.407046 [ 1255 ] {} <Fatal> BaseDaemon: 17.1. inlined from ./contrib/llvm-project/libcxx/include/list:588: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::__sz[abi:v15000]() const\r\n2023.12.04 15:11:55.407162 [ 1255 ] {} <Fatal> BaseDaemon: 17.2. inlined from ./contrib/llvm-project/libcxx/include/list:616: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.12.04 15:11:55.407205 [ 1255 ] {} <Fatal> BaseDaemon: 17.3. inlined from ./contrib/llvm-project/libcxx/include/list:918: std::list<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const\r\n2023.12.04 15:11:55.407251 [ 1255 ] {} <Fatal> BaseDaemon: 17.4. inlined from ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:50: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*)\r\n2023.12.04 15:11:55.407285 [ 1255 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:95: DB::ExecutionThreadContext::executeTask() @ 0x000000003ad08097 in /workspace/clickhouse\r\n2023.12.04 15:11:55.518238 [ 1255 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:273: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*)\r\n2023.12.04 15:11:55.518348 [ 1255 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:239: DB::PipelineExecutor::executeSingleThread(unsigned long) @ 0x000000003acdda71 in /workspace/clickhouse\r\n2023.12.04 15:11:55.617272 [ 1255 ] {} <Fatal> BaseDaemon: 19. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:409: DB::PipelineExecutor::executeImpl(unsigned long, bool) @ 0x000000003acd95bc in /workspace/clickhouse\r\n2023.12.04 15:11:55.725568 [ 1255 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:274: std::unique_ptr<DB::ExecutingGraph, std::default_delete<DB::ExecutingGraph>>::operator->[abi:v15000]() const\r\n2023.12.04 15:11:55.725689 [ 1255 ] {} <Fatal> BaseDaemon: 20. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:114: DB::PipelineExecutor::execute(unsigned long, bool) @ 0x000000003acd9028 in /workspace/clickhouse\r\n2023.12.04 15:11:55.833436 [ 1255 ] {} <Fatal> BaseDaemon: 21.1. inlined from ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:96: DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::shared_ptr<DB::ThreadGroup>, unsigned long, bool)\r\n2023.12.04 15:11:55.833553 [ 1255 ] {} <Fatal> BaseDaemon: 21.2. inlined from ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112: operator()\r\n2023.12.04 15:11:55.833588 [ 1255 ] {} <Fatal> BaseDaemon: 21.3. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ?\r\n2023.12.04 15:11:55.833700 [ 1255 ] {} <Fatal> BaseDaemon: 21.4. inlined from ./contrib/llvm-project/libcxx/include/tuple:1789: decltype(auto) std::__apply_tuple_impl[abi:v15000]<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&, std::__tuple_indices<>)\r\n2023.12.04 15:11:55.833754 [ 1255 ] {} <Fatal> BaseDaemon: 21.5. inlined from ./contrib/llvm-project/libcxx/include/tuple:1798: decltype(auto) std::apply[abi:v15000]<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::tuple<>&)\r\n2023.12.04 15:11:55.833791 [ 1255 ] {} <Fatal> BaseDaemon: 21.6. inlined from ./src/Common/ThreadPool.h:250: operator()\r\n2023.12.04 15:11:55.833832 [ 1255 ] {} <Fatal> BaseDaemon: 21.7. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ?\r\n2023.12.04 15:11:55.833862 [ 1255 ] {} <Fatal> BaseDaemon: 21.8. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: ?\r\n2023.12.04 15:11:55.833893 [ 1255 ] {} <Fatal> BaseDaemon: 21.9. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:235: ?\r\n2023.12.04 15:11:55.833925 [ 1255 ] {} <Fatal> BaseDaemon: 21. ./contrib/llvm-project/libcxx/include/__functional/function.h:716: ? @ 0x000000003ad13d86 in /workspace/clickhouse\r\n2023.12.04 15:11:55.928560 [ 1255 ] {} <Fatal> BaseDaemon: 22.1. inlined from ./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.12.04 15:11:55.928675 [ 1255 ] {} <Fatal> BaseDaemon: 22.2. inlined from ./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.12.04 15:11:55.928723 [ 1255 ] {} <Fatal> BaseDaemon: 22.3. inlined from ./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.12.04 15:11:55.928764 [ 1255 ] {} <Fatal> BaseDaemon: 22.4. inlined from ./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.12.04 15:11:55.928798 [ 1255 ] {} <Fatal> BaseDaemon: 22. ./build_docker/./src/Common/ThreadPool.cpp:423: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000001f6cd26b in /workspace/clickhouse\r\n2023.12.04 15:11:56.054154 [ 1255 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:303: std::unique_ptr<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>, std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>>::reset[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*)\r\n2023.12.04 15:11:56.054260 [ 1255 ] {} <Fatal> BaseDaemon: 23.2. inlined from ./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr\r\n2023.12.04 15:11:56.054296 [ 1255 ] {} <Fatal> BaseDaemon: 23. ./contrib/llvm-project/libcxx/include/thread:297: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000001f6db8eb in /workspace/clickhouse\r\n2023.12.04 15:11:56.054357 [ 1255 ] {} <Fatal> BaseDaemon: 24. ? @ 0x00007f8bd453fac3 in ?\r\n2023.12.04 15:11:56.054388 [ 1255 ] {} <Fatal> BaseDaemon: 25. ? @ 0x00007f8bd45d1a40 in ?\r\n2023.12.04 15:11:56.054439 [ 1255 ] {} <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.\r\n2023.12.04 15:12:01.827812 [ 1255 ] {} <Fatal> BaseDaemon: This ClickHouse version is not official and should be upgraded to the official build.\r\n2023.12.04 15:12:01.828389 [ 1255 ] {} <Fatal> BaseDaemon: Changed settings: receive_timeout = 10., receive_data_timeout_ms = 10000, allow_suspicious_low_cardinality_types = true, log_queries = true, table_function_remote_max_addresses = 200, allow_experimental_analyzer = true, max_execution_time = 10., max_memory_usage = 10000000000, log_comment = '/workspace/ch/tests/queries/0_stateless/02798_substring_index.sql', send_logs_level = 'fatal', allow_introspection_functions = true, allow_experimental_annoy_index = true, allow_experimental_usearch_index = true\r\n2023.12.04 15:12:13.407295 [ 158 ] {} <Fatal> Application: Child process was terminated by signal 6\r\n```\nReproducer with the analyzer:\r\n\r\n```\r\nDROP TABLE IF EXISTS tab__fuzz_7;\r\nCREATE TABLE tab__fuzz_7 (`id` UInt16, `vec` Array(Float32)) ENGINE = MergeTree\r\nORDER BY id\r\nSETTINGS index_granularity_bytes = 0, min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0, index_granularity = 8192;\r\n\r\nINSERT INTO tab__fuzz_7 values (0, [2.2, 2.3]) (1, [3.1, 3.2]);\r\nINSERT INTO tab__fuzz_7 values (2, [2.2, 2.3, 2.4]) (3, [3.1, 3.2, 3.3]);\r\n\r\n\r\nWITH NULL AS reference_vec SELECT (NULL, 3.4028234663852886e38, NULL, NULL), tuple('9223372036854775807'), 1023, -2147483647, (NULL, NULL), NULL FROM tab__fuzz_7 PREWHERE 8 GROUP BY GROUPING SETS ((('0.0000001024', '104857.5')), ((NULL, NULL, NULL, NULL, NULL))) ORDER BY tuple('102.4') DESC NULLS FIRST, tuple(NULL) ASC LIMIT 9223372036854775806 SETTINGS allow_experimental_analyzer = true;\r\n```\r\n\r\n```\r\nCode: 49. DB::Exception: Received from clickhouse-01:49000. DB::Exception: RangeReader read 2 rows, but 8192 expected.: While executing MergeTreeSelect(pool: ReadPool, algorithm: Thread). (LOGICAL_ERROR)\r\n(query: WITH NULL AS reference_vec SELECT (NULL, 3.4028234663852886e38, NULL, NULL), tuple('9223372036854775807'), 1023, -2147483647, (NULL, NULL), NULL FROM tab__fuzz_7 PREWHERE 8 GROUP BY GROUPING SETS ((('0.0000001024', '104857.5')), ((NULL, NULL, NULL, NULL, NULL))) ORDER BY tuple('102.4') DESC NULLS FIRST, tuple(NULL) ASC LIMIT 9223372036854775806 SETTINGS allow_experimental_analyzer = true;)\r\n```\nIt's unrelated to the analyzer. The problem is about the execution of PREWHERE clause. Sometimes PrewhereInfo is incorrect.\r\n\r\nReproduction with old analyzer:\r\n\r\n```sql\r\nCREATE TABLE data_02051__fuzz_33\r\n(\r\n  `key` Date,\r\n  `value` UInt16\r\n)\r\nENGINE = MergeTree\r\nORDER BY key\r\nSETTINGS\r\n  index_granularity_bytes = 0\r\nAS SELECT\r\n  number,\r\n  repeat(toString(number), 5)\r\nFROM numbers(1000000.);\r\n\r\nSET allow_experimental_analyzer = 0;\r\n-- SET optimize_move_to_prewhere = 0;\r\n-- set send_logs_level = 'debug';\r\n\r\nSELECT\r\n  count(ignore(*))\r\nFROM data_02051__fuzz_33\r\nPREWHERE CAST(ignore() + 1 as UInt8)\r\nGROUP BY \r\n  ignore(65535, *),\r\n  ignore(255, 256, *)\r\n```\r\n\r\nTest for new analyzer:\r\n```sql\r\nCREATE TABLE data_02051__fuzz_33\r\n(\r\n  `key` Date,\r\n  `value` UInt16\r\n)\r\nENGINE = MergeTree\r\nORDER BY key\r\nSETTINGS\r\n  index_granularity_bytes = 0\r\nAS SELECT\r\n  number,\r\n  repeat(toString(number), 5)\r\nFROM numbers(1000000.);\r\n\r\nSET allow_experimental_analyzer = 1;\r\n\r\nSELECT\r\n  count(ignore(*))\r\nFROM data_02051__fuzz_33\r\nPREWHERE 255\r\nGROUP BY \r\n  ignore(65535, *),\r\n  ignore(255, 256, *)\r\n```\nWorks on 23.6 https://fiddle.clickhouse.com/1dca2559-27f5-4c0b-a63d-ad58def955ac\r\nFails on 23.7 https://fiddle.clickhouse.com/f5f1d9e4-4532-457a-a765-14231ca2d4df\r\n\r\nThe difference is that in 23.6 the first reader in the chain of read steps reads 1 column (`key`)\r\n```\r\nMergeTreeRangeReader: First reader returned: num_rows: 10000, columns: 1, total_rows_per_granule: 10000, no filter, column[0]:  UInt16(size = 10000), requested columns: key\r\n```\r\nbut in 23.7 it somehow ends up reading not columns so it returns row count of 2 full granules  \r\n```\r\nMergeTreeRangeReader: First reader returned: num_rows: 16384, columns: 0, total_rows_per_granule: 16384, no filter, requested columns: \r\n```\r\n\nLooks like this started failing after https://github.com/ClickHouse/ClickHouse/pull/52689 Before that we would add a real column to the read step if doesn't read anything.\r\nBut the real problem here seems to be that with fixed index granularity of let's say 8192 rows and a part with 10k rows we have index_granularity structure like this:\r\n```\r\np index_granularity\r\n(DB::MergeTreeIndexGranularity &) 0x000051b0002c0150: {\r\n  marks_rows_partial_sums = size=2 {\r\n    [0] = 8192\r\n    [1] = 16384\r\n  }\r\n  initialized = true\r\n}\r\n```\r\nthe last granule size is rounded up 2*8192 while it only has 10000-8192 rows\nhttps://s3.amazonaws.com/clickhouse-test-reports/59027/838f0f4d08c49b04053f7cdc2ae02bbc579bd4e1/fuzzer_astfuzzerasan/report.html \nhttps://s3.amazonaws.com/clickhouse-test-reports/59176/c47efd186790b4e3e6ef14cb3c123ed87c6272b7/ast_fuzzer__msan_.html\nhttps://s3.amazonaws.com/clickhouse-test-reports/59908/58e5d7876b40879dedecc34d0bcef3f435508b1a/ast_fuzzer__debug_.html\nAnother reproducer (https://s3.amazonaws.com/clickhouse-test-reports/60211/1fa2e0ce3f1779258e450c054d1a13769dc25d6e/ast_fuzzer__ubsan_.html):\r\n\r\n```\r\nSET allow_experimental_usearch_index=1;\r\n\r\nCREATE TABLE tab__fuzz_30 (`id` Nullable(Int16), `vec` Array(Float32), INDEX idx vec TYPE usearch GRANULARITY 100000000) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity_bytes = 0, min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0, index_granularity = 8192, allow_nullable_key=1;\r\n\r\nINSERT INTO tab__fuzz_30 SELECT number, [toFloat32(number), 0.] FROM numbers(10000);\r\n\r\nWITH [1., 0.] AS reference_vec\r\nSELECT\r\n    id,\r\n    vec,\r\n    L2Distance(vec, reference_vec)\r\nFROM tab__fuzz_30\r\nPREWHERE 34\r\nORDER BY\r\n    toDateTime(toFixedString(toFixedString(toFixedString(toFixedString(toFixedString(toFixedString('2024-02-21 12:00:00', 19), toNullable(19)), 19), 19), 19), 19)) ASC NULLS LAST,\r\n    L2Distance(vec, reference_vec) ASC NULLS FIRST\r\nLIMIT 65537 SETTINGS allow_experimental_analyzer = true, allow_experimental_usearch_index = true;\r\n```\r\n\r\nRemoving prewhere or analyzer fixes it, but it looks more like prewhere is the problem.\r\n\r\nAside from the pending fix, why isn't `PREWHERE 34` just ignored? It's not reading any column and it's always true.\nAnother reproducer:\r\n```\r\nCREATE TABLE old_school_table__fuzz_0 (`key` UInt64, `value` String) ENGINE = MergeTree ORDER BY key SETTINGS index_granularity_bytes = 0;\r\nINSERT INTO old_school_table VALUES (1, '1');\r\nINSERT INTO old_school_table VALUES (2, '2');\r\nSELECT * FROM old_school_table__fuzz_0 PREWHERE materialize(1)\r\n```\r\n\r\nWithout `index_granularity_bytes=0` it works ok\nhttps://s3.amazonaws.com/clickhouse-test-reports/64312/553fcb5e0618858cfa7659522208be49320b0e48/ast_fuzzer__debug_.html \nIt seems that logical error appeared again:\r\nhttps://s3.amazonaws.com/clickhouse-test-reports/69611/a461d20af92278fb973ba7a86ec84fb58946ce74/ast_fuzzer__msan_.html\r\n```\r\nLogical error: 'RangeReader read 1 rows, but 8192 expected.'.\r\n```\r\n\r\nThere are other failures too:\r\nhttps://play.clickhouse.com/play?user=play#c2VsZWN0IAp0ZXN0X25hbWUsIGNoZWNrX3N0YXJ0X3RpbWUsIHB1bGxfcmVxdWVzdF9udW1iZXIsIHJlcG9ydF91cmwKZnJvbSBjaGVja3Mgd2hlcmUgJzIwMjQtMDktMDEnIDw9IGNoZWNrX3N0YXJ0X3RpbWUgYW5kIHRlc3RfbmFtZSBsaWtlICclUmFuZ2VSZWFkZXIgcmVhZCAxIHJvd3MsIGJ1dCA4MTkyIGV4cGVjdGVkJScgYW5kIHRlc3Rfc3RhdHVzIGluICgnRkFJTCcsICdGTEFLWScpIG9yZGVyIGJ5IGNoZWNrX3N0YXJ0X3RpbWU=\nAnother one: https://s3.amazonaws.com/clickhouse-test-reports/70122/61976c986a7777178af45dc690f6c242be4b3323/ast_fuzzer__debug_.html\r\n\r\nThis time: `Logical error: 'RangeReader read 10000 rows, but 16384 expected.'`\nAnother one with `Logical error: 'RangeReader read 1 rows, but 1000 expected.`: https://s3.amazonaws.com/clickhouse-test-reports/73091/e935b2ef5439be2ce037e8a54ae5fdfd9c568dab/ast_fuzzer__debug_.html\r\nRepro: https://fiddle.clickhouse.com/add8a712-7da3-4b86-bfcf-f882ce142aed\n<!-- automatic status comment for PR #74472 from ClickHouse/ClickHouse:reader_read_error -->\n*This is an automated comment for commit c0055e116c0a26b8ed22d3b7403450116e5f01de with description of existing statuses. It's updated for the latest CI running*\n\n[\u274c Click here](https://s3.amazonaws.com/clickhouse-test-reports/74472/c0055e116c0a26b8ed22d3b7403450116e5f01de/ci_running.html) to open a full report in a separate page\n\n<table>\n<thead><tr><th>Check name</th><th>Description</th><th>Status</th></tr></thead>\n<tbody>\n<tr><td>Fast test</td><td>Normally this is the first check that is ran for a PR. It builds ClickHouse and runs most of <a href=\"https://clickhouse.com/docs/en/development/tests#functional-tests\">stateless functional tests</a>, omitting some. If it fails, further checks are not started until it is fixed. Look at the report to see which tests fail, then reproduce the failure locally as described <a href=\"https://clickhouse.com/docs/en/development/tests#functional-test-locally\">here</a></td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/74472/c0055e116c0a26b8ed22d3b7403450116e5f01de/fast_test.html\">\u274c failure</a></td></tr>\n<tbody>\n</table>\n<details><summary>Successful checks</summary>\n<table>\n<thead><tr><th>Check name</th><th>Description</th><th>Status</th></tr></thead>\n<tbody>\n<tr><td>Docs check</td><td>Builds and tests the documentation</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/74454/64a78170b7f6ed84fd887fede8cce9cc3e519fdc/docs_check.html\">\u2705 success</a></td></tr>\n<tr><td>Style check</td><td>Runs a set of checks to keep the code style clean. If some of tests failed, see the related log from the report</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/74472/c0055e116c0a26b8ed22d3b7403450116e5f01de/style_check.html\">\u2705 success</a></td></tr>\n<tbody>\n</table>\n</details>\n",
  "created_at": "2025-01-30T16:00:34Z"
}