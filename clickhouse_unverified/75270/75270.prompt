You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
LOGICAL_ERROR when reading from merge tree table without adaptive index granularity (RangeReader read N rows, but M expected)
> Please make sure that the version you're using is still supported (you can find the list [here](https://github.com/ClickHouse/ClickHouse/blob/master/SECURITY.md#scope-and-supported-versions)).

> You have to provide the following information whenever possible.

**Describe what's wrong**

> A clear and concise description of what works not as it is supposed to.

```
Received exception from server (version 24.4.1):
Code: 49. DB::Exception: Received from localhost:9000. DB::Exception: RangeReader read 803 rows, but 1024 expected.: While executing MergeTreeSelect(pool: ReadPool, algorithm: Thread). (LOGICAL_ERROR)
(query: SELECT   29, id = 11338881281426660955, * FROM account_test__fuzz_36 PREWHERE materialize(29);)
```

> A link to reproducer in [https://fiddle.clickhouse.com/](https://fiddle.clickhouse.com/).

https://fiddle.clickhouse.com/3db4eb06-f421-46e9-a115-d01bf462fcf0

**Does it reproduce on the most recent release?**

[The list of releases](https://github.com/ClickHouse/ClickHouse/blob/master/utils/list-versions/version_date.tsv)

Yes, on HEAD
Logical error: 'RangeReader read 576 rows, but 8192 expected.'
https://s3.amazonaws.com/clickhouse-test-reports/56516/857b47de2eaffef8995217f20c8f348c5510fbff/fuzzer_astfuzzertsan/report.html

```
2023.11.11 11:54:41.536045 [ 676 ] {9f4b4aaa-956f-4065-be8f-500452cecacc} <Fatal> : Logical error: 'RangeReader read 576 rows, but 8192 expected.'.
2023.11.11 11:54:41.537064 [ 908 ] {} <Fatal> BaseDaemon: ########## Short fault info ############
2023.11.11 11:54:41.537136 [ 908 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 696598418FD730A935D80DF7AB04D7B4E875F36D, git hash: 72e7c28ef4bd490451062cab14879261eaf11181) (from thread 676) Received signal 6
2023.11.11 11:54:41.537209 [ 908 ] {} <Fatal> BaseDaemon: Signal description: Aborted
2023.11.11 11:54:41.537246 [ 908 ] {} <Fatal> BaseDaemon: 
2023.11.11 11:54:41.537321 [ 908 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fca136ed9fc 0x00007fca13699476 0x00007fca1367f7f3 0x0000559f58cf4573 0x0000559f60db254b 0x0000559f60db2f54 0x0000559f5a09c3c6 0x0000559f6c7e9feb 0x0000559f6c7e7bb8 0x0000559f6c7e7b86 0x0000559f6c7f228b 0x0000559f6d3671aa 0x0000559f6c7d9af4 0x0000559f6d36a46a 0x0000559f6cce3dcb 0x0000559f6cd06c55 0x0000559f6ccf9c51 0x0000559f6ccfac91 0x0000559f60efdcf2 0x0000559f60f02b9d 0x0000559f60f02b02 0x0000559f60ef9967 0x0000559f60effbd2 0x0000559f58ceda4f 0x00007fca136ebac3 0x00007fca1377da40
2023.11.11 11:54:41.537415 [ 908 ] {} <Fatal> BaseDaemon: ########################################
2023.11.11 11:54:41.537648 [ 908 ] {} <Fatal> BaseDaemon: (version 23.11.1.1, build id: 696598418FD730A935D80DF7AB04D7B4E875F36D, git hash: 72e7c28ef4bd490451062cab14879261eaf11181) (from thread 676) (query_id: 9f4b4aaa-956f-4065-be8f-500452cecacc) (query: SELECT count(ignore(*)) FROM data_02051__fuzz_3 PREWHERE 1023 AND ignore(10, 1024 AND 1025 AND ignore(ignore(1048577, (NULL AND NULL) AND NULL, 1025, ignore(-2147483649 AND 1048576 AND NULL AND 1, *), NULL AND NULL), ignore(100000000000000000000., (NULL AND NULL) AND 2147483646, -2147483647, NULL AND NULL, 1023 AND ignore(ignore(NULL, (NULL AND NULL) AND -2147483648, 10, ignore(0.0001, (NULL AND NULL) AND 1048577, 9223372036854775807, 65536 AND NULL, 0 AND 10), 65537 AND 65535), *) AND 0 AND 65535), *) AND NULL AND (NULL AND NULL), -9223372036854775807 AND (NULL AND NULL AND NULL) AND -2147483648, 7, 10 AND 65537 AND -1) AND 1025 AND NULL AND (NULL AND NULL) SETTINGS min_bytes_to_use_direct_io = 0, local_filesystem_read_method = 'mmap', local_filesystem_read_prefetch = 0, read_priority = 0, max_read_buffer_size = 1048576) Received signal Aborted (6)
2023.11.11 11:54:41.537799 [ 908 ] {} <Fatal> BaseDaemon: 
2023.11.11 11:54:41.537924 [ 908 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fca136ed9fc 0x00007fca13699476 0x00007fca1367f7f3 0x0000559f58cf4573 0x0000559f60db254b 0x0000559f60db2f54 0x0000559f5a09c3c6 0x0000559f6c7e9feb 0x0000559f6c7e7bb8 0x0000559f6c7e7b86 0x0000559f6c7f228b 0x0000559f6d3671aa 0x0000559f6c7d9af4 0x0000559f6d36a46a 0x0000559f6cce3dcb 0x0000559f6cd06c55 0x0000559f6ccf9c51 0x0000559f6ccfac91 0x0000559f60efdcf2 0x0000559f60f02b9d 0x0000559f60f02b02 0x0000559f60ef9967 0x0000559f60effbd2 0x0000559f58ceda4f 0x00007fca136ebac3 0x00007fca1377da40
2023.11.11 11:54:41.538108 [ 908 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007fca136ed9fc in ?
2023.11.11 11:54:41.538236 [ 908 ] {} <Fatal> BaseDaemon: 6. ? @ 0x00007fca13699476 in ?
2023.11.11 11:54:41.538335 [ 908 ] {} <Fatal> BaseDaemon: 7. ? @ 0x00007fca1367f7f3 in ?
2023.11.11 11:54:53.333418 [ 908 ] {} <Fatal> BaseDaemon: 8. __interceptor_abort @ 0x0000000007e99573 in /workspace/clickhouse
2023.11.11 11:54:53.661430 [ 908 ] {} <Fatal> BaseDaemon: 9. ./build_docker/./src/Common/Exception.cpp:0: DB::abortOnFailedAssertion(String const&) @ 0x000000000ff5754b in /workspace/clickhouse
2023.11.11 11:54:53.988327 [ 908 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Common/Exception.cpp:0: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000ff57f54 in /workspace/clickhouse
2023.11.11 11:55:05.114904 [ 908 ] {} <Fatal> BaseDaemon: 11. DB::Exception::Exception<unsigned long&, unsigned long const&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type, std::type_identity<unsigned long const&>::type>, unsigned long&, unsigned long const&) @ 0x00000000092413c6 in /workspace/clickhouse
2023.11.11 11:55:05.513819 [ 908 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:1242: DB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult const&, unsigned long&) @ 0x000000001b98efeb in /workspace/clickhouse
2023.11.11 11:55:05.796150 [ 908 ] {} <Fatal> BaseDaemon: 13. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:987: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x000000001b98cbb8 in /workspace/clickhouse
2023.11.11 11:55:06.079903 [ 908 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Storages/MergeTree/MergeTreeRangeReader.cpp:0: DB::MergeTreeRangeReader::read(unsigned long, DB::MarkRanges&) @ 0x000000001b98cb86 in /workspace/clickhouse
2023.11.11 11:55:06.316604 [ 908 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./src/Storages/MergeTree/MergeTreeReadTask.cpp:163: DB::MergeTreeReadTask::read(DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001b99728b in /workspace/clickhouse
2023.11.11 11:55:08.352510 [ 908 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectAlgorithms.h:38: DB::MergeTreeThreadSelectAlgorithm::readFromTask(DB::MergeTreeReadTask&, DB::MergeTreeReadTask::BlockSizeParams const&) @ 0x000000001c50c1aa in /workspace/clickhouse
2023.11.11 11:55:08.783246 [ 908 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:162: DB::MergeTreeSelectProcessor::read() @ 0x000000001b97eaf4 in /workspace/clickhouse
2023.11.11 11:55:08.965197 [ 908 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:181: DB::MergeTreeSource::processReadResult(DB::ChunkAndProgress)
2023.11.11 11:55:08.965410 [ 908 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:226: DB::MergeTreeSource::tryGenerate() @ 0x000000001c50f46a in /workspace/clickhouse
2023.11.11 11:55:09.116249 [ 908 ] {} <Fatal> BaseDaemon: 19.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/optional:344: std::__optional_storage_base<DB::Chunk, false>::has_value[abi:v15000]() const
2023.11.11 11:55:09.116481 [ 908 ] {} <Fatal> BaseDaemon: 19.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/optional:998: std::optional<DB::Chunk>::operator bool[abi:v15000]() const
2023.11.11 11:55:09.116600 [ 908 ] {} <Fatal> BaseDaemon: 19. ./build_docker/./src/Processors/ISource.cpp:108: DB::ISource::work() @ 0x000000001be88dcb in /workspace/clickhouse
2023.11.11 11:55:09.166808 [ 908 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:588: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::__sz[abi:v15000]() const
2023.11.11 11:55:09.167023 [ 908 ] {} <Fatal> BaseDaemon: 20.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:616: std::__list_imp<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const
2023.11.11 11:55:09.167216 [ 908 ] {} <Fatal> BaseDaemon: 20.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/list:918: std::list<DB::ExecutingGraph::Edge, std::allocator<DB::ExecutingGraph::Edge>>::empty[abi:v15000]() const
2023.11.11 11:55:09.167368 [ 908 ] {} <Fatal> BaseDaemon: 20.4. inlined from ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:50: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*)
2023.11.11 11:55:09.167477 [ 908 ] {} <Fatal> BaseDaemon: 20. ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:95: DB::ExecutionThreadContext::executeTask() @ 0x000000001beabc55 in /workspace/clickhouse
2023.11.11 11:55:09.372272 [ 908 ] {} <Fatal> BaseDaemon: 21. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:272: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x000000001be9ec51 in /workspace/clickhouse
2023.11.11 11:55:09.613200 [ 908 ] {} <Fatal> BaseDaemon: 22.1. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:370: operator()
2023.11.11 11:55:09.613497 [ 908 ] {} <Fatal> BaseDaemon: 22.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: decltype(std::declval<DB::PipelineExecutor::spawnThreads()::$_0&>()()) std::__invoke[abi:v15000]<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)
2023.11.11 11:55:09.613688 [ 908 ] {} <Fatal> BaseDaemon: 22.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: void std::__invoke_void_return_wrapper<void, true>::__call<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)
2023.11.11 11:55:09.613844 [ 908 ] {} <Fatal> BaseDaemon: 22.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:235: std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>::operator()[abi:v15000]()
2023.11.11 11:55:09.613964 [ 908 ] {} <Fatal> BaseDaemon: 22. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:716: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x000000001be9fc91 in /workspace/clickhouse
2023.11.11 11:55:09.825954 [ 908 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)
2023.11.11 11:55:09.826179 [ 908 ] {} <Fatal> BaseDaemon: 23.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)
2023.11.11 11:55:09.826316 [ 908 ] {} <Fatal> BaseDaemon: 23.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const
2023.11.11 11:55:09.826443 [ 908 ] {} <Fatal> BaseDaemon: 23.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const
2023.11.11 11:55:09.826564 [ 908 ] {} <Fatal> BaseDaemon: 23. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x00000000100a2cf2 in /workspace/clickhouse
2023.11.11 11:55:09.983893 [ 908 ] {} <Fatal> BaseDaemon: 24. ./build_docker/./src/Common/ThreadPool.cpp:0: ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'()::operator()() @ 0x00000000100a7b9d in /workspace/clickhouse
2023.11.11 11:55:10.222417 [ 908 ] {} <Fatal> BaseDaemon: 25. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:717: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x00000000100a7b02 in /workspace/clickhouse
2023.11.11 11:55:10.401473 [ 908 ] {} <Fatal> BaseDaemon: 26.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)
2023.11.11 11:55:10.401724 [ 908 ] {} <Fatal> BaseDaemon: 26.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)
2023.11.11 11:55:10.401879 [ 908 ] {} <Fatal> BaseDaemon: 26.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const
2023.11.11 11:55:10.402008 [ 908 ] {} <Fatal> BaseDaemon: 26.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const
2023.11.11 11:55:10.402125 [ 908 ] {} <Fatal> BaseDaemon: 26. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000001009e967 in /workspace/clickhouse
2023.11.11 11:55:10.642451 [ 908 ] {} <Fatal> BaseDaemon: 27.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>::reset[abi:v15000](std::__thread_struct*)
2023.11.11 11:55:10.642651 [ 908 ] {} <Fatal> BaseDaemon: 27.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr
2023.11.11 11:55:10.642800 [ 908 ] {} <Fatal> BaseDaemon: 27.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:265: ~__tuple_leaf
2023.11.11 11:55:10.642931 [ 908 ] {} <Fatal> BaseDaemon: 27.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:538: ~tuple
2023.11.11 11:55:10.643170 [ 908 ] {} <Fatal> BaseDaemon: 27.5. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:48: std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>::operator()[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*) const
2023.11.11 11:55:10.643439 [ 908 ] {} <Fatal> BaseDaemon: 27.6. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:305: std::unique_ptr<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>, std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>>::reset[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*)
2023.11.11 11:55:10.643564 [ 908 ] {} <Fatal> BaseDaemon: 27.7. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr
2023.11.11 11:55:10.643679 [ 908 ] {} <Fatal> BaseDaemon: 27. ./build_docker/./contrib/llvm-project/libcxx/include/thread:297: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x00000000100a4bd2 in /workspace/clickhouse
2023.11.11 11:55:22.746228 [ 908 ] {} <Fatal> BaseDaemon: 28. __tsan_thread_start_func @ 0x0000000007e92a4f in /workspace/clickhouse
2023.11.11 11:55:22.746457 [ 908 ] {} <Fatal> BaseDaemon: 29. ? @ 0x00007fca136ebac3 in ?
2023.11.11 11:55:22.746564 [ 908 ] {} <Fatal> BaseDaemon: 30. ? @ 0x00007fca1377da40 in ?
```

Exposing 'RangeReader read N rows, but M expected' error
Related to #62741 #56640

The issue is happened to parts with constant index granularity and triggered by having `PREWHERE` in the query.
So, having a MergeTree table with `index_granularity_bytes = 0, index_granularity = 42` settings and `1` row, and reading it, will trigger the following error:
`RangeReader read 1 rows, but 42 expected`

Technically, `MergeTreeIndexGranularityInfo::fixed_index_granularity` 
https://github.com/ClickHouse/ClickHouse/blob/39a25b1d3995618d7f20fcfd13f5a3bc229f5f87/src/Storages/MergeTree/MergeTreeIndexGranularityInfo.h#L41-L42

is used to initialize `MergeTreeIndexGranularityConstant::last_mark_granularity` (it's `42` in our example)
Which in turn used to get the number of rows in the part range (one only in our example, i.e. last as well) in `MergeTreeIndexGranularityConstant::getRowsCountInRange()`
https://github.com/ClickHouse/ClickHouse/blob/39a25b1d3995618d7f20fcfd13f5a3bc229f5f87/src/Storages/MergeTree/MergeTreeIndexGranularityConstant.cpp#L99
Which will return not a number of rows in the granule but its granularity

CC @CurtizJ, fill free to use this PR for the fix if applicable

### Changelog category (leave one):
- Not for changelog (changelog entry is not required)


> Information about CI checks: https://clickhouse.com/docs/en/development/continuous-integration/

#### CI Settings (Only check the boxes if you know what you are doing)

All builds in Builds_1 and Builds_2 stages are always mandatory
and will run independently of the checks below:

- [ ] <!---ci_set_required--> Allow: All Required Checks
- [ ] <!---ci_include_stateless--> Allow: Stateless tests
- [ ] <!---ci_include_stateful--> Allow: Stateful tests
- [ ] <!---ci_include_integration--> Allow: Integration Tests
- [ ] <!---ci_include_performance--> Allow: Performance tests
- [ ] <!---ci_set_builds--> Allow: All Builds
- [ ] <!---batch_0_1--> Allow: batch 1, 2 for multi-batch jobs
- [ ] <!---batch_2_3--> Allow: batch 3, 4, 5, 6 for multi-batch jobs
---
- [ ] <!---ci_exclude_style--> Exclude: Style check
- [ ] <!---ci_exclude_fast--> Exclude: Fast test
- [ ] <!---ci_exclude_asan--> Exclude: All with ASAN
- [ ] <!---ci_exclude_tsan|msan|ubsan|coverage--> Exclude: All with TSAN, MSAN, UBSAN, Coverage
- [ ] <!---ci_exclude_aarch64|release|debug--> Exclude: All with aarch64
- [ ] <!---ci_exclude_release--> Exclude: All with release
- [ ] <!---ci_exclude_debug--> Exclude: All with debug
---
- [ ] <!---ci_include_uzz--> Run only fuzzers related jobs (libFuzzer fuzzers, AST fuzzers, BuzzHouse, etc.)
- [ ] <!---ci_exclude_ast--> Exclude: AST fuzzers
---
- [ ] <!---do_not_test--> Do not test
- [ ] <!---woolen_wolfdog--> Woolen Wolfdog
- [ ] <!---upload_all--> Upload binaries for special builds
- [ ] <!---no_merge_commit--> Disable merge-commit
- [ ] <!---no_ci_cache--> Disable CI cache
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
