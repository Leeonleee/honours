{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 33642,
  "instance_id": "ClickHouse__ClickHouse-33642",
  "issue_numbers": [
    "33643"
  ],
  "base_commit": "bb620a93afe98d68be2aa8b2901fe56cce532572",
  "patch": "diff --git a/src/Common/ZooKeeper/ZooKeeperLock.cpp b/src/Common/ZooKeeper/ZooKeeperLock.cpp\nnew file mode 100644\nindex 000000000000..1200dcdb533a\n--- /dev/null\n+++ b/src/Common/ZooKeeper/ZooKeeperLock.cpp\n@@ -0,0 +1,93 @@\n+#include <Common/ZooKeeper/ZooKeeperLock.h>\n+#include <filesystem>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+}\n+\n+namespace fs = std::filesystem;\n+\n+namespace zkutil\n+{\n+\n+ZooKeeperLock::ZooKeeperLock(\n+    const ZooKeeperPtr & zookeeper_,\n+    const std::string & lock_prefix_,\n+    const std::string & lock_name_,\n+    const std::string & lock_message_)\n+    : zookeeper(zookeeper_)\n+    , lock_path(fs::path(lock_prefix_) / lock_name_)\n+    , lock_message(lock_message_)\n+    , log(&Poco::Logger::get(\"zkutil::Lock\"))\n+{\n+    zookeeper->createIfNotExists(lock_prefix_, \"\");\n+}\n+\n+ZooKeeperLock::~ZooKeeperLock()\n+{\n+    try\n+    {\n+        unlock();\n+    }\n+    catch (...)\n+    {\n+        DB::tryLogCurrentException(__PRETTY_FUNCTION__);\n+    }\n+}\n+\n+void ZooKeeperLock::unlock()\n+{\n+    if (!locked)\n+        return;\n+\n+    locked = false;\n+\n+    if (zookeeper->expired())\n+    {\n+        LOG_WARNING(log, \"Lock is lost, because session was expired. Path: {}, message: {}\", lock_path, lock_message);\n+        return;\n+    }\n+\n+    Coordination::Stat stat;\n+    /// NOTE It will throw if session expired after we checked it above\n+    bool result = zookeeper->exists(lock_path, &stat);\n+\n+    if (result && stat.ephemeralOwner == zookeeper->getClientID())\n+        zookeeper->remove(lock_path, -1);\n+    else if (result)\n+        throw DB::Exception(DB::ErrorCodes::LOGICAL_ERROR, \"Lock is lost, it has another owner. Path: {}, message: {}, owner: {}, our id: {}\",\n+                        lock_path, lock_message, stat.ephemeralOwner, zookeeper->getClientID());\n+    else\n+        throw DB::Exception(DB::ErrorCodes::LOGICAL_ERROR, \"Lock is lost, node does not exist. Path: {}, message: {}\", lock_path, lock_message);\n+}\n+\n+bool ZooKeeperLock::tryLock()\n+{\n+    Coordination::Error code = zookeeper->tryCreate(lock_path, lock_message, zkutil::CreateMode::Ephemeral);\n+\n+    if (code == Coordination::Error::ZOK)\n+    {\n+        locked = true;\n+    }\n+    else if (code != Coordination::Error::ZNODEEXISTS)\n+    {\n+        throw Coordination::Exception(code);\n+    }\n+\n+    return locked;\n+}\n+\n+std::unique_ptr<ZooKeeperLock> createSimpleZooKeeperLock(\n+    const ZooKeeperPtr & zookeeper, const String & lock_prefix, const String & lock_name, const String & lock_message)\n+{\n+    return std::make_unique<ZooKeeperLock>(zookeeper, lock_prefix, lock_name, lock_message);\n+}\n+\n+\n+}\ndiff --git a/src/Common/ZooKeeper/ZooKeeperLock.h b/src/Common/ZooKeeper/ZooKeeperLock.h\nnew file mode 100644\nindex 000000000000..218f14ef1329\n--- /dev/null\n+++ b/src/Common/ZooKeeper/ZooKeeperLock.h\n@@ -0,0 +1,54 @@\n+#pragma once\n+#include <Common/ZooKeeper/ZooKeeper.h>\n+#include <Common/ZooKeeper/KeeperException.h>\n+#include <memory>\n+#include <string>\n+#include <base/logger_useful.h>\n+\n+namespace zkutil\n+{\n+\n+/** Caveats: usage of locks in ZooKeeper is incorrect in 99% of cases,\n+  *  and highlights your poor understanding of distributed systems.\n+  *\n+  * It's only correct if all the operations that are performed under lock\n+  *  are atomically checking that the lock still holds\n+  *  or if we ensure that these operations will be undone if lock is lost\n+  *  (due to ZooKeeper session loss) that's very difficult to achieve.\n+  *\n+  * It's Ok if every operation that we perform under lock is actually operation in ZooKeeper.\n+  *\n+  * In 1% of cases when you can correctly use Lock, the logic is complex enough, so you don't need this class.\n+  *\n+  * TLDR: Don't use this code if you are not sure. We only have a few cases of it's usage.\n+  */\n+class ZooKeeperLock\n+{\n+public:\n+    /// lock_prefix - path where the ephemeral lock node will be created\n+    /// lock_name - the name of the ephemeral lock node\n+    ZooKeeperLock(\n+        const ZooKeeperPtr & zookeeper_,\n+        const std::string & lock_prefix_,\n+        const std::string & lock_name_,\n+        const std::string & lock_message_ = \"\");\n+\n+    ~ZooKeeperLock();\n+\n+    void unlock();\n+    bool tryLock();\n+\n+private:\n+    zkutil::ZooKeeperPtr zookeeper;\n+\n+    std::string lock_path;\n+    std::string lock_message;\n+    Poco::Logger * log;\n+    bool locked = false;\n+\n+};\n+\n+std::unique_ptr<ZooKeeperLock> createSimpleZooKeeperLock(\n+    const ZooKeeperPtr & zookeeper, const String & lock_prefix, const String & lock_name, const String & lock_message);\n+\n+}\ndiff --git a/src/Interpreters/DDLWorker.cpp b/src/Interpreters/DDLWorker.cpp\nindex ee5dc4deebbb..3eeb817cbab1 100644\n--- a/src/Interpreters/DDLWorker.cpp\n+++ b/src/Interpreters/DDLWorker.cpp\n@@ -21,6 +21,7 @@\n #include <Common/randomSeed.h>\n #include <Common/ZooKeeper/ZooKeeper.h>\n #include <Common/ZooKeeper/KeeperException.h>\n+#include <Common/ZooKeeper/ZooKeeperLock.h>\n #include <Common/isLocalAddress.h>\n #include <Storages/StorageReplicatedMergeTree.h>\n #include <Poco/Timestamp.h>\n@@ -54,113 +55,6 @@ namespace ErrorCodes\n constexpr const char * TASK_PROCESSED_OUT_REASON = \"Task has been already processed\";\n \n \n-/** Caveats: usage of locks in ZooKeeper is incorrect in 99% of cases,\n-  *  and highlights your poor understanding of distributed systems.\n-  *\n-  * It's only correct if all the operations that are performed under lock\n-  *  are atomically checking that the lock still holds\n-  *  or if we ensure that these operations will be undone if lock is lost\n-  *  (due to ZooKeeper session loss) that's very difficult to achieve.\n-  *\n-  * It's Ok if every operation that we perform under lock is actually operation in ZooKeeper.\n-  *\n-  * In 1% of cases when you can correctly use Lock, the logic is complex enough, so you don't need this class.\n-  *\n-  * TLDR: Don't use this code.\n-  * We only have a few cases of it's usage and it will be removed.\n-  */\n-class ZooKeeperLock\n-{\n-public:\n-    /// lock_prefix - path where the ephemeral lock node will be created\n-    /// lock_name - the name of the ephemeral lock node\n-    ZooKeeperLock(\n-        const zkutil::ZooKeeperPtr & zookeeper_,\n-        const std::string & lock_prefix_,\n-        const std::string & lock_name_,\n-        const std::string & lock_message_ = \"\")\n-    :\n-        zookeeper(zookeeper_),\n-        lock_path(fs::path(lock_prefix_) / lock_name_),\n-        lock_message(lock_message_),\n-        log(&Poco::Logger::get(\"zkutil::Lock\"))\n-    {\n-        zookeeper->createIfNotExists(lock_prefix_, \"\");\n-    }\n-\n-    ~ZooKeeperLock()\n-    {\n-        try\n-        {\n-            unlock();\n-        }\n-        catch (...)\n-        {\n-            DB::tryLogCurrentException(__PRETTY_FUNCTION__);\n-        }\n-    }\n-\n-    void unlock()\n-    {\n-        if (!locked)\n-            return;\n-\n-        locked = false;\n-\n-        if (zookeeper->expired())\n-        {\n-            LOG_WARNING(log, \"Lock is lost, because session was expired. Path: {}, message: {}\", lock_path, lock_message);\n-            return;\n-        }\n-\n-        Coordination::Stat stat;\n-        std::string dummy;\n-        /// NOTE It will throw if session expired after we checked it above\n-        bool result = zookeeper->tryGet(lock_path, dummy, &stat);\n-\n-        if (result && stat.ephemeralOwner == zookeeper->getClientID())\n-            zookeeper->remove(lock_path, -1);\n-        else if (result)\n-            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Lock is lost, it has another owner. Path: {}, message: {}, owner: {}, our id: {}\",\n-                            lock_path, lock_message, stat.ephemeralOwner, zookeeper->getClientID());\n-        else\n-            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Lock is lost, node does not exist. Path: {}, message: {}\", lock_path, lock_message);\n-    }\n-\n-    bool tryLock()\n-    {\n-        std::string dummy;\n-        Coordination::Error code = zookeeper->tryCreate(lock_path, lock_message, zkutil::CreateMode::Ephemeral, dummy);\n-\n-        if (code == Coordination::Error::ZOK)\n-        {\n-            locked = true;\n-        }\n-        else if (code != Coordination::Error::ZNODEEXISTS)\n-        {\n-            throw Coordination::Exception(code);\n-        }\n-\n-        return locked;\n-    }\n-\n-private:\n-    zkutil::ZooKeeperPtr zookeeper;\n-\n-    std::string lock_path;\n-    std::string lock_message;\n-    Poco::Logger * log;\n-    bool locked = false;\n-\n-};\n-\n-std::unique_ptr<ZooKeeperLock> createSimpleZooKeeperLock(\n-    const zkutil::ZooKeeperPtr & zookeeper, const String & lock_prefix, const String & lock_name, const String & lock_message)\n-{\n-    return std::make_unique<ZooKeeperLock>(zookeeper, lock_prefix, lock_name, lock_message);\n-}\n-\n-\n DDLWorker::DDLWorker(\n     int pool_size_,\n     const std::string & zk_root_dir,\n@@ -656,7 +550,7 @@ void DDLWorker::processTask(DDLTaskBase & task, const ZooKeeperPtr & zookeeper)\n \n     /// We must hold the lock until task execution status is committed to ZooKeeper,\n     /// otherwise another replica may try to execute query again.\n-    std::unique_ptr<ZooKeeperLock> execute_on_leader_lock;\n+    std::unique_ptr<zkutil::ZooKeeperLock> execute_on_leader_lock;\n \n     /// Step 2: Execute query from the task.\n     if (!task.was_executed)\n@@ -776,7 +670,7 @@ bool DDLWorker::tryExecuteQueryOnLeaderReplica(\n     const String & rewritten_query,\n     const String & /*node_path*/,\n     const ZooKeeperPtr & zookeeper,\n-    std::unique_ptr<ZooKeeperLock> & execute_on_leader_lock)\n+    std::unique_ptr<zkutil::ZooKeeperLock> & execute_on_leader_lock)\n {\n     StorageReplicatedMergeTree * replicated_storage = dynamic_cast<StorageReplicatedMergeTree *>(storage.get());\n \ndiff --git a/src/Interpreters/DDLWorker.h b/src/Interpreters/DDLWorker.h\nindex 0b8b0a4a4d8c..dbdf0e94f06d 100644\n--- a/src/Interpreters/DDLWorker.h\n+++ b/src/Interpreters/DDLWorker.h\n@@ -30,6 +30,11 @@ namespace Coordination\n     struct Stat;\n }\n \n+namespace zkutil\n+{\n+    class ZooKeeperLock;\n+}\n+\n namespace DB\n {\n class ASTAlterQuery;\n@@ -38,7 +43,6 @@ struct DDLTaskBase;\n using DDLTaskPtr = std::unique_ptr<DDLTaskBase>;\n using ZooKeeperPtr = std::shared_ptr<zkutil::ZooKeeper>;\n class AccessRightsElements;\n-class ZooKeeperLock;\n \n class DDLWorker\n {\n@@ -95,7 +99,7 @@ class DDLWorker\n         const String & rewritten_query,\n         const String & node_path,\n         const ZooKeeperPtr & zookeeper,\n-        std::unique_ptr<ZooKeeperLock> & execute_on_leader_lock);\n+        std::unique_ptr<zkutil::ZooKeeperLock> & execute_on_leader_lock);\n \n     bool tryExecuteQuery(const String & query, DDLTaskBase & task, const ZooKeeperPtr & zookeeper);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex b38a0112116e..54705a3c405f 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -5516,6 +5516,9 @@ bool MergeTreeData::moveParts(const CurrentlyMovingPartsTaggerPtr & moving_tagge\n {\n     LOG_INFO(log, \"Got {} parts to move.\", moving_tagger->parts_to_move.size());\n \n+    const auto settings = getSettings();\n+\n+    bool result = true;\n     for (const auto & moving_part : moving_tagger->parts_to_move)\n     {\n         Stopwatch stopwatch;\n@@ -5535,8 +5538,41 @@ bool MergeTreeData::moveParts(const CurrentlyMovingPartsTaggerPtr & moving_tagge\n \n         try\n         {\n-            cloned_part = parts_mover.clonePart(moving_part);\n-            parts_mover.swapClonedPart(cloned_part);\n+            /// If zero-copy replication enabled than replicas shouldn't try to\n+            /// move parts to another disk simultaneously. For this purpose we\n+            /// use shared lock across replicas. NOTE: it's not 100% reliable,\n+            /// because we are not checking lock while finishing part move.\n+            /// However it's not dangerous at all, we will just have very rare\n+            /// copies of some part.\n+            ///\n+            /// FIXME: this code is related to Replicated merge tree, and not\n+            /// common for ordinary merge tree. So it's a bad design and should\n+            /// be fixed.\n+            auto disk = moving_part.reserved_space->getDisk();\n+            if (supportsReplication() && disk->supportZeroCopyReplication() && settings->allow_remote_fs_zero_copy_replication)\n+            {\n+                /// If we acuqired lock than let's try to move. After one\n+                /// replica will actually move the part from disk to some\n+                /// zero-copy storage other replicas will just fetch\n+                /// metainformation.\n+                if (auto lock = tryCreateZeroCopyExclusiveLock(moving_part.part, disk); lock)\n+                {\n+                    cloned_part = parts_mover.clonePart(moving_part);\n+                    parts_mover.swapClonedPart(cloned_part);\n+                }\n+                else\n+                {\n+                    /// Move will be retried but with backoff.\n+                    LOG_DEBUG(log, \"Move of part {} postponed, because zero copy mode enabled and someone other moving this part right now\", moving_part.part->name);\n+                    result = false;\n+                    continue;\n+                }\n+            }\n+            else /// Ordinary move as it should be\n+            {\n+                cloned_part = parts_mover.clonePart(moving_part);\n+                parts_mover.swapClonedPart(cloned_part);\n+            }\n             write_part_log({});\n         }\n         catch (...)\n@@ -5548,7 +5584,7 @@ bool MergeTreeData::moveParts(const CurrentlyMovingPartsTaggerPtr & moving_tagge\n             throw;\n         }\n     }\n-    return true;\n+    return result;\n }\n \n bool MergeTreeData::partsContainSameProjections(const DataPartPtr & left, const DataPartPtr & right)\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex f1d0abffc7aa..4c58a53f3687 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -27,6 +27,8 @@\n #include <Interpreters/Aggregator.h>\n #include <Storages/extractKeyExpressionList.h>\n #include <Storages/PartitionCommands.h>\n+#include <Storages/MergeTree/ZeroCopyLock.h>\n+\n \n #include <boost/multi_index_container.hpp>\n #include <boost/multi_index/ordered_index.hpp>\n@@ -43,6 +45,7 @@ class MergeTreeDataMergerMutator;\n class MutationCommands;\n class Context;\n struct JobAndPool;\n+struct ZeroCopyLock;\n \n /// Auxiliary struct holding information about the future merged or mutated part.\n struct EmergingPartInfo\n@@ -1189,6 +1192,10 @@ class MergeTreeData : public IStorage, public WithMutableContext\n         DataPartsVector & duplicate_parts_to_remove,\n         MutableDataPartsVector & parts_from_wal,\n         DataPartsLock & part_lock);\n+\n+    /// Create zero-copy exclusive lock for part and disk. Useful for coordination of\n+    /// distributed operations which can lead to data duplication. Implemented only in ReplicatedMergeTree.\n+    virtual std::optional<ZeroCopyLock> tryCreateZeroCopyExclusiveLock(const DataPartPtr &, const DiskPtr &) { return std::nullopt; }\n };\n \n /// RAII struct to record big parts that are submerging or emerging.\ndiff --git a/src/Storages/MergeTree/MergeTreePartsMover.cpp b/src/Storages/MergeTree/MergeTreePartsMover.cpp\nindex 5a889ea5e8b9..190fc0d30a0c 100644\n--- a/src/Storages/MergeTree/MergeTreePartsMover.cpp\n+++ b/src/Storages/MergeTree/MergeTreePartsMover.cpp\n@@ -200,7 +200,7 @@ MergeTreeData::DataPartPtr MergeTreePartsMover::clonePart(const MergeTreeMoveEnt\n     auto settings = data->getSettings();\n     auto part = moving_part.part;\n     auto disk = moving_part.reserved_space->getDisk();\n-    LOG_DEBUG(log, \"Cloning part {} from {} to {}\", part->name, part->volume->getDisk()->getName(), disk->getName());\n+    LOG_DEBUG(log, \"Cloning part {} from '{}' to '{}'\", part->name, part->volume->getDisk()->getName(), disk->getName());\n \n     const String directory_to_move = \"moving\";\n     if (disk->supportZeroCopyReplication() && settings->allow_remote_fs_zero_copy_replication)\ndiff --git a/src/Storages/MergeTree/ZeroCopyLock.cpp b/src/Storages/MergeTree/ZeroCopyLock.cpp\nnew file mode 100644\nindex 000000000000..dbb12d0d6104\n--- /dev/null\n+++ b/src/Storages/MergeTree/ZeroCopyLock.cpp\n@@ -0,0 +1,9 @@\n+#include \"ZeroCopyLock.h\"\n+\n+namespace DB\n+{\n+    ZeroCopyLock::ZeroCopyLock(const zkutil::ZooKeeperPtr & zookeeper, const std::string & lock_path)\n+        : lock(zkutil::createSimpleZooKeeperLock(zookeeper, lock_path, \"part_exclusive_lock\", \"\"))\n+    {\n+    }\n+}\ndiff --git a/src/Storages/MergeTree/ZeroCopyLock.h b/src/Storages/MergeTree/ZeroCopyLock.h\nnew file mode 100644\nindex 000000000000..96709fb01c90\n--- /dev/null\n+++ b/src/Storages/MergeTree/ZeroCopyLock.h\n@@ -0,0 +1,21 @@\n+#pragma once\n+#include <Core/Types.h>\n+#include <optional>\n+#include <memory>\n+#include <Common/ZooKeeper/ZooKeeperLock.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n+\n+namespace DB\n+{\n+\n+/// Very simple wrapper for zookeeper ephemeral lock. It's better to have it\n+/// because due to bad abstraction we use it in MergeTreeData.\n+struct ZeroCopyLock\n+{\n+    ZeroCopyLock(const zkutil::ZooKeeperPtr & zookeeper, const std::string & lock_path);\n+\n+    /// Actual lock\n+    std::unique_ptr<zkutil::ZooKeeperLock> lock;\n+};\n+\n+}\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 91a9c8567bad..8d2e15357a0b 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -31,6 +31,7 @@\n #include <Storages/VirtualColumnUtils.h>\n #include <Storages/MergeTree/MergeTreeReaderCompact.h>\n #include <Storages/MergeTree/LeaderElection.h>\n+#include <Storages/MergeTree/ZeroCopyLock.h>\n \n \n #include <Databases/DatabaseOnDisk.h>\n@@ -7128,11 +7129,11 @@ bool StorageReplicatedMergeTree::unlockSharedData(const IMergeTreeDataPart & par\n }\n \n \n-bool StorageReplicatedMergeTree::unlockSharedDataByID(String id, const String & table_uuid, const String & part_name,\n+bool StorageReplicatedMergeTree::unlockSharedDataByID(String part_id, const String & table_uuid, const String & part_name,\n         const String & replica_name_, DiskPtr disk, zkutil::ZooKeeperPtr zookeeper_ptr, const MergeTreeSettings & settings,\n         Poco::Logger * logger, const String & zookeeper_path_old)\n {\n-    boost::replace_all(id, \"/\", \"_\");\n+    boost::replace_all(part_id, \"/\", \"_\");\n \n     Strings zc_zookeeper_paths = getZeroCopyPartPath(settings, disk->getType(), table_uuid, part_name, zookeeper_path_old);\n \n@@ -7140,13 +7141,16 @@ bool StorageReplicatedMergeTree::unlockSharedDataByID(String id, const String &\n \n     for (const auto & zc_zookeeper_path : zc_zookeeper_paths)\n     {\n-        String zookeeper_part_uniq_node = fs::path(zc_zookeeper_path) / id;\n-        String zookeeper_node = fs::path(zookeeper_part_uniq_node) / replica_name_;\n+        String zookeeper_part_uniq_node = fs::path(zc_zookeeper_path) / part_id;\n+\n+        /// Delete our replica node for part from zookeeper (we are not interested in it anymore)\n+        String zookeeper_part_replica_node = fs::path(zookeeper_part_uniq_node) / replica_name_;\n \n-        LOG_TRACE(logger, \"Remove zookeeper lock {}\", zookeeper_node);\n+        LOG_TRACE(logger, \"Remove zookeeper lock {}\", zookeeper_part_replica_node);\n \n-        zookeeper_ptr->tryRemove(zookeeper_node);\n+        zookeeper_ptr->tryRemove(zookeeper_part_replica_node);\n \n+        /// Check, maybe we were the last replica and can remove part forever\n         Strings children;\n         zookeeper_ptr->tryGetChildren(zookeeper_part_uniq_node, children);\n \n@@ -7157,9 +7161,9 @@ bool StorageReplicatedMergeTree::unlockSharedDataByID(String id, const String &\n             continue;\n         }\n \n-        auto e = zookeeper_ptr->tryRemove(zookeeper_part_uniq_node);\n+        auto error_code = zookeeper_ptr->tryRemove(zookeeper_part_uniq_node);\n \n-        LOG_TRACE(logger, \"Remove parent zookeeper lock {} : {}\", zookeeper_part_uniq_node, e != Coordination::Error::ZNOTEMPTY);\n+        LOG_TRACE(logger, \"Remove parent zookeeper lock {} : {}\", zookeeper_part_uniq_node, error_code != Coordination::Error::ZNOTEMPTY);\n \n         /// Even when we have lock with same part name, but with different uniq, we can remove files on S3\n         children.clear();\n@@ -7168,9 +7172,9 @@ bool StorageReplicatedMergeTree::unlockSharedDataByID(String id, const String &\n         if (children.empty())\n         {\n             /// Cleanup after last uniq removing\n-            e = zookeeper_ptr->tryRemove(zookeeper_part_node);\n+            error_code = zookeeper_ptr->tryRemove(zookeeper_part_node);\n \n-            LOG_TRACE(logger, \"Remove parent zookeeper lock {} : {}\", zookeeper_part_node, e != Coordination::Error::ZNOTEMPTY);\n+            LOG_TRACE(logger, \"Remove parent zookeeper lock {} : {}\", zookeeper_part_node, error_code != Coordination::Error::ZNOTEMPTY);\n         }\n         else\n         {\n@@ -7213,7 +7217,7 @@ String StorageReplicatedMergeTree::getSharedDataReplica(\n \n     zkutil::ZooKeeperPtr zookeeper = tryGetZooKeeper();\n     if (!zookeeper)\n-        return best_replica;\n+        return \"\";\n \n     Strings zc_zookeeper_paths = getZeroCopyPartPath(*getSettings(), disk_type, getTableSharedID(), part.name,\n             zookeeper_path);\n@@ -7251,7 +7255,7 @@ String StorageReplicatedMergeTree::getSharedDataReplica(\n     LOG_TRACE(log, \"Found zookeper active replicas for part {}: {}\", part.name, active_replicas.size());\n \n     if (active_replicas.empty())\n-        return best_replica;\n+        return \"\";\n \n     /** You must select the best (most relevant) replica.\n     * This is a replica with the maximum `log_pointer`, then with the minimum `queue` size.\n@@ -7305,6 +7309,30 @@ Strings StorageReplicatedMergeTree::getZeroCopyPartPath(const MergeTreeSettings\n }\n \n \n+std::optional<ZeroCopyLock> StorageReplicatedMergeTree::tryCreateZeroCopyExclusiveLock(const DataPartPtr & part, const DiskPtr & disk)\n+{\n+    if (!disk || !disk->supportZeroCopyReplication())\n+        return std::nullopt;\n+\n+    zkutil::ZooKeeperPtr zookeeper = tryGetZooKeeper();\n+    if (!zookeeper)\n+        return std::nullopt;\n+\n+    String zc_zookeeper_path = getZeroCopyPartPath(*getSettings(), disk->getType(), getTableSharedID(),\n+        part->name, zookeeper_path)[0];\n+\n+    /// Just recursively create ancestors for lock\n+    zookeeper->createAncestors(zc_zookeeper_path);\n+    zookeeper->createIfNotExists(zc_zookeeper_path, \"\");\n+\n+    /// Create actual lock\n+    ZeroCopyLock lock(zookeeper, zc_zookeeper_path);\n+    if (lock.lock->tryLock())\n+        return lock;\n+    else\n+        return std::nullopt;\n+}\n+\n String StorageReplicatedMergeTree::findReplicaHavingPart(\n     const String & part_name, const String & zookeeper_path_, zkutil::ZooKeeper::Ptr zookeeper_ptr)\n {\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex e390a0bcea45..c91152ca0f33 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -243,7 +243,7 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica\n     /// Unlock shared data part in zookeeper by part id\n     /// Return true if data unlocked\n     /// Return false if data is still used by another node\n-    static bool unlockSharedDataByID(String id, const String & table_uuid, const String & part_name, const String & replica_name_,\n+    static bool unlockSharedDataByID(String part_id, const String & table_uuid, const String & part_name, const String & replica_name_,\n         DiskPtr disk, zkutil::ZooKeeperPtr zookeeper_, const MergeTreeSettings & settings, Poco::Logger * logger,\n         const String & zookeeper_path_old);\n \n@@ -758,6 +758,10 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica\n     // Create table id if needed\n     void createTableSharedID();\n \n+    /// Create ephemeral lock in zookeeper for part and disk which support zero copy replication.\n+    /// If somebody already holding the lock -- return std::nullopt.\n+    std::optional<ZeroCopyLock> tryCreateZeroCopyExclusiveLock(const DataPartPtr & part, const DiskPtr & disk) override;\n+\n protected:\n     /** If not 'attach', either creates a new table in ZK, or adds a replica to an existing table.\n       */\n",
  "test_patch": "diff --git a/tests/integration/test_s3_zero_copy_ttl/__init__.py b/tests/integration/test_s3_zero_copy_ttl/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_s3_zero_copy_ttl/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_s3_zero_copy_ttl/configs/s3.xml b/tests/integration/test_s3_zero_copy_ttl/configs/s3.xml\nnew file mode 100644\nindex 000000000000..c4889186e38e\n--- /dev/null\n+++ b/tests/integration/test_s3_zero_copy_ttl/configs/s3.xml\n@@ -0,0 +1,26 @@\n+<clickhouse>\n+  <storage_configuration>\n+    <disks>\n+        <s3_disk>\n+            <type>s3</type>\n+            <endpoint>http://minio1:9001/root/data/</endpoint>\n+            <access_key_id>minio</access_key_id>\n+            <secret_access_key>minio123</secret_access_key>\n+        </s3_disk>\n+    </disks>\n+\n+    <policies>\n+        <s3_and_default>\n+            <volumes>\n+                <main>\n+                    <disk>default</disk>\n+                </main>\n+                <external>\n+                    <disk>s3_disk</disk>\n+                </external>\n+            </volumes>\n+        </s3_and_default>\n+    </policies>\n+\n+  </storage_configuration>\n+</clickhouse>\ndiff --git a/tests/integration/test_s3_zero_copy_ttl/test.py b/tests/integration/test_s3_zero_copy_ttl/test.py\nnew file mode 100644\nindex 000000000000..5f63bfbfdff4\n--- /dev/null\n+++ b/tests/integration/test_s3_zero_copy_ttl/test.py\n@@ -0,0 +1,69 @@\n+#!/usr/bin/env python3\n+import time\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance(\"node1\", main_configs=[\"configs/s3.xml\"], with_minio=True, with_zookeeper=True)\n+node2 = cluster.add_instance(\"node2\", main_configs=[\"configs/s3.xml\"], with_minio=True, with_zookeeper=True)\n+node3 = cluster.add_instance(\"node3\", main_configs=[\"configs/s3.xml\"], with_minio=True, with_zookeeper=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+def test_ttl_move_and_s3(started_cluster):\n+    for i, node in enumerate([node1, node2, node3]):\n+        node.query(\n+            \"\"\"\n+            CREATE TABLE s3_test_with_ttl (date DateTime, id UInt32, value String)\n+            ENGINE=ReplicatedMergeTree('/clickhouse/tables/s3_test', '{}')\n+            ORDER BY id\n+            PARTITION BY id\n+            TTL date TO DISK 's3_disk'\n+            SETTINGS storage_policy='s3_and_default'\n+            \"\"\".format(i))\n+\n+    node1.query(\"SYSTEM STOP MOVES s3_test_with_ttl\")\n+\n+    node2.query(\"SYSTEM STOP MOVES s3_test_with_ttl\")\n+\n+    for i in range(30):\n+        if i % 2 == 0:\n+            node = node1\n+        else:\n+            node = node2\n+\n+        node.query(f\"INSERT INTO s3_test_with_ttl SELECT now() + 5, {i}, randomPrintableASCII(1048570)\")\n+\n+    node1.query(\"SYSTEM SYNC REPLICA s3_test_with_ttl\")\n+    node2.query(\"SYSTEM SYNC REPLICA s3_test_with_ttl\")\n+    node3.query(\"SYSTEM SYNC REPLICA s3_test_with_ttl\")\n+\n+    assert node1.query(\"SELECT COUNT() FROM s3_test_with_ttl\") == \"30\\n\"\n+    assert node2.query(\"SELECT COUNT() FROM s3_test_with_ttl\") == \"30\\n\"\n+\n+    node1.query(\"SYSTEM START MOVES s3_test_with_ttl\")\n+    node2.query(\"SYSTEM START MOVES s3_test_with_ttl\")\n+\n+    assert node1.query(\"SELECT COUNT() FROM s3_test_with_ttl\") == \"30\\n\"\n+    assert node2.query(\"SELECT COUNT() FROM s3_test_with_ttl\") == \"30\\n\"\n+\n+    time.sleep(5)\n+\n+    print(node1.query(\"SELECT * FROM system.parts WHERE table = 's3_test_with_ttl' FORMAT Vertical\"))\n+\n+    minio = cluster.minio_client\n+    objects = minio.list_objects(cluster.minio_bucket, 'data/', recursive=True)\n+    counter = 0\n+    for obj in objects:\n+        print(\"Objectname:\", obj.object_name, \"metadata:\", obj.metadata)\n+        counter += 1\n+    print(\"Total objects\", counter)\n+    assert counter == 300\n",
  "problem_statement": "Zero copy replication can lead to data duplication in case of background moves \nI've added a test that reproduces the error https://github.com/ClickHouse/ClickHouse/pull/33642.\n",
  "hints_text": "",
  "created_at": "2022-01-14T15:45:30Z",
  "modified_files": [
    "b/src/Common/ZooKeeper/ZooKeeperLock.cpp",
    "b/src/Common/ZooKeeper/ZooKeeperLock.h",
    "src/Interpreters/DDLWorker.cpp",
    "src/Interpreters/DDLWorker.h",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeData.h",
    "src/Storages/MergeTree/MergeTreePartsMover.cpp",
    "b/src/Storages/MergeTree/ZeroCopyLock.cpp",
    "b/src/Storages/MergeTree/ZeroCopyLock.h",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_s3_zero_copy_ttl/__init__.py",
    "b/tests/integration/test_s3_zero_copy_ttl/configs/s3.xml",
    "b/tests/integration/test_s3_zero_copy_ttl/test.py"
  ]
}