diff --git a/.gitattributes b/.gitattributes
index bcc7d57b9049..a23f027122b6 100644
--- a/.gitattributes
+++ b/.gitattributes
@@ -1,2 +1,3 @@
 contrib/* linguist-vendored
 *.h linguist-language=C++
+tests/queries/0_stateless/data_json/* binary
diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index 22fe1f2ffff8..b24181625d3f 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -569,6 +569,14 @@ if (ENABLE_TESTS)
         clickhouse_common_zookeeper
         string_utils)
 
+    if (TARGET ch_contrib::simdjson)
+        target_link_libraries(unit_tests_dbms PRIVATE ch_contrib::simdjson)
+    endif()
+
+    if(TARGET ch_contrib::rapidjson)
+        target_include_directories(unit_tests_dbms PRIVATE ch_contrib::rapidjson)
+    endif()
+
     if (TARGET ch_contrib::yaml_cpp)
         target_link_libraries(unit_tests_dbms PRIVATE ch_contrib::yaml_cpp)
     endif()
diff --git a/src/Client/ClientBase.cpp b/src/Client/ClientBase.cpp
index c575cd37a5f5..4f1c1f4539eb 100644
--- a/src/Client/ClientBase.cpp
+++ b/src/Client/ClientBase.cpp
@@ -1092,10 +1092,11 @@ void ClientBase::sendData(Block & sample, const ColumnsDescription & columns_des
 
         try
         {
+            auto metadata = storage->getInMemoryMetadataPtr();
             sendDataFromPipe(
                 storage->read(
                         sample.getNames(),
-                        storage->getInMemoryMetadataPtr(),
+                        storage->getStorageSnapshot(metadata),
                         query_info,
                         global_context,
                         {},
diff --git a/src/Columns/ColumnAggregateFunction.cpp b/src/Columns/ColumnAggregateFunction.cpp
index f27e103b3049..59d56c6e437c 100644
--- a/src/Columns/ColumnAggregateFunction.cpp
+++ b/src/Columns/ColumnAggregateFunction.cpp
@@ -297,7 +297,7 @@ ColumnPtr ColumnAggregateFunction::filter(const Filter & filter, ssize_t result_
 {
     size_t size = data.size();
     if (size != filter.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filter.size(), size);
 
     if (size == 0)
         return cloneEmpty();
diff --git a/src/Columns/ColumnArray.cpp b/src/Columns/ColumnArray.cpp
index be5d90652818..24da96443355 100644
--- a/src/Columns/ColumnArray.cpp
+++ b/src/Columns/ColumnArray.cpp
@@ -608,7 +608,7 @@ ColumnPtr ColumnArray::filterString(const Filter & filt, ssize_t result_size_hin
 {
     size_t col_size = getOffsets().size();
     if (col_size != filt.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), col_size);
 
     if (0 == col_size)
         return ColumnArray::create(data);
@@ -676,7 +676,7 @@ ColumnPtr ColumnArray::filterGeneric(const Filter & filt, ssize_t result_size_hi
 {
     size_t size = getOffsets().size();
     if (size != filt.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), size);
 
     if (size == 0)
         return ColumnArray::create(data);
@@ -1189,4 +1189,12 @@ void ColumnArray::gather(ColumnGathererStream & gatherer)
     gatherer.gather(*this);
 }
 
+size_t ColumnArray::getNumberOfDimensions() const
+{
+    const auto * nested_array = checkAndGetColumn<ColumnArray>(*data);
+    if (!nested_array)
+        return 1;
+    return 1 + nested_array->getNumberOfDimensions();   /// Every modern C++ compiler optimizes tail recursion.
+}
+
 }
diff --git a/src/Columns/ColumnArray.h b/src/Columns/ColumnArray.h
index cc80d1300cec..3f41ae9cd8a1 100644
--- a/src/Columns/ColumnArray.h
+++ b/src/Columns/ColumnArray.h
@@ -169,6 +169,8 @@ class ColumnArray final : public COWHelper<IColumn, ColumnArray>
 
     bool isCollationSupported() const override { return getData().isCollationSupported(); }
 
+    size_t getNumberOfDimensions() const;
+
 private:
     WrappedPtr data;
     WrappedPtr offsets;
diff --git a/src/Columns/ColumnDecimal.cpp b/src/Columns/ColumnDecimal.cpp
index f9feb8f10b91..4290a7a4cb16 100644
--- a/src/Columns/ColumnDecimal.cpp
+++ b/src/Columns/ColumnDecimal.cpp
@@ -266,7 +266,7 @@ ColumnPtr ColumnDecimal<T>::filter(const IColumn::Filter & filt, ssize_t result_
 {
     size_t size = data.size();
     if (size != filt.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), size);
 
     auto res = this->create(0, scale);
     Container & res_data = res->getData();
diff --git a/src/Columns/ColumnFixedString.cpp b/src/Columns/ColumnFixedString.cpp
index d0a735a55804..de6324ca7ceb 100644
--- a/src/Columns/ColumnFixedString.cpp
+++ b/src/Columns/ColumnFixedString.cpp
@@ -207,7 +207,7 @@ ColumnPtr ColumnFixedString::filter(const IColumn::Filter & filt, ssize_t result
 {
     size_t col_size = size();
     if (col_size != filt.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), col_size);
 
     auto res = ColumnFixedString::create(n);
 
diff --git a/src/Columns/ColumnNullable.h b/src/Columns/ColumnNullable.h
index 41ad099818eb..52e5e43fa484 100644
--- a/src/Columns/ColumnNullable.h
+++ b/src/Columns/ColumnNullable.h
@@ -144,15 +144,15 @@ class ColumnNullable final : public COWHelper<IColumn, ColumnNullable>
 
     double getRatioOfDefaultRows(double sample_ratio) const override
     {
-        return null_map->getRatioOfDefaultRows(sample_ratio);
+        return getRatioOfDefaultRowsImpl<ColumnNullable>(sample_ratio);
     }
 
     void getIndicesOfNonDefaultRows(Offsets & indices, size_t from, size_t limit) const override
     {
-        null_map->getIndicesOfNonDefaultRows(indices, from, limit);
+        getIndicesOfNonDefaultRowsImpl<ColumnNullable>(indices, from, limit);
     }
 
-    ColumnPtr createWithOffsets(const IColumn::Offsets & offsets, const Field & default_field, size_t total_rows, size_t shift) const override;
+    ColumnPtr createWithOffsets(const Offsets & offsets, const Field & default_field, size_t total_rows, size_t shift) const override;
 
     bool isNullable() const override { return true; }
     bool isFixedAndContiguous() const override { return false; }
diff --git a/src/Columns/ColumnObject.cpp b/src/Columns/ColumnObject.cpp
new file mode 100644
index 000000000000..bfa8ffe63580
--- /dev/null
+++ b/src/Columns/ColumnObject.cpp
@@ -0,0 +1,780 @@
+#include <Core/Field.h>
+#include <Columns/ColumnObject.h>
+#include <Columns/ColumnsNumber.h>
+#include <Columns/ColumnArray.h>
+#include <DataTypes/ObjectUtils.h>
+#include <DataTypes/getLeastSupertype.h>
+#include <DataTypes/DataTypeNothing.h>
+#include <DataTypes/DataTypeNullable.h>
+#include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/NestedUtils.h>
+#include <Interpreters/castColumn.h>
+#include <Interpreters/convertFieldToType.h>
+#include <Common/HashTable/HashSet.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int LOGICAL_ERROR;
+    extern const int ILLEGAL_COLUMN;
+    extern const int DUPLICATE_COLUMN;
+    extern const int NUMBER_OF_DIMENSIONS_MISMATHED;
+    extern const int NOT_IMPLEMENTED;
+    extern const int SIZES_OF_COLUMNS_DOESNT_MATCH;
+}
+
+namespace
+{
+
+/// Recreates column with default scalar values and keeps sizes of arrays.
+ColumnPtr recreateColumnWithDefaultValues(
+    const ColumnPtr & column, const DataTypePtr & scalar_type, size_t num_dimensions)
+{
+    const auto * column_array = checkAndGetColumn<ColumnArray>(column.get());
+    if (column_array && num_dimensions)
+    {
+        return ColumnArray::create(
+            recreateColumnWithDefaultValues(
+                column_array->getDataPtr(), scalar_type, num_dimensions - 1),
+                IColumn::mutate(column_array->getOffsetsPtr()));
+    }
+
+    return createArrayOfType(scalar_type, num_dimensions)->createColumn()->cloneResized(column->size());
+}
+
+/// Replaces NULL fields to given field or empty array.
+class FieldVisitorReplaceNull : public StaticVisitor<Field>
+{
+public:
+    explicit FieldVisitorReplaceNull(
+        const Field & replacement_, size_t num_dimensions_)
+        : replacement(replacement_)
+        , num_dimensions(num_dimensions_)
+    {
+    }
+
+    Field operator()(const Null &) const
+    {
+        return num_dimensions
+            ? createEmptyArrayField(num_dimensions)
+            : replacement;
+    }
+
+    Field operator()(const Array & x) const
+    {
+        assert(num_dimensions > 0);
+        const size_t size = x.size();
+        Array res(size);
+        for (size_t i = 0; i < size; ++i)
+            res[i] = applyVisitor(FieldVisitorReplaceNull(replacement, num_dimensions - 1), x[i]);
+        return res;
+    }
+
+    template <typename T>
+    Field operator()(const T & x) const { return x; }
+
+private:
+    const Field & replacement;
+    size_t num_dimensions;
+};
+
+/// Calculates number of dimensions in array field.
+/// Returns 0 for scalar fields.
+class FieldVisitorToNumberOfDimensions : public StaticVisitor<size_t>
+{
+public:
+    size_t operator()(const Array & x) const
+    {
+        const size_t size = x.size();
+        std::optional<size_t> dimensions;
+
+        for (size_t i = 0; i < size; ++i)
+        {
+            /// Do not count Nulls, because they will be replaced by default
+            /// values with proper number of dimensions.
+            if (x[i].isNull())
+                continue;
+
+            size_t current_dimensions = applyVisitor(*this, x[i]);
+            if (!dimensions)
+                dimensions = current_dimensions;
+            else if (current_dimensions != *dimensions)
+                throw Exception(ErrorCodes::NUMBER_OF_DIMENSIONS_MISMATHED,
+                    "Number of dimensions mismatched among array elements");
+        }
+
+        return 1 + dimensions.value_or(0);
+    }
+
+    template <typename T>
+    size_t operator()(const T &) const { return 0; }
+};
+
+/// Visitor that allows to get type of scalar field
+/// or least common type of scalars in array.
+/// More optimized version of FieldToDataType.
+class FieldVisitorToScalarType : public StaticVisitor<>
+{
+public:
+    using FieldType = Field::Types::Which;
+
+    void operator()(const Array & x)
+    {
+        size_t size = x.size();
+        for (size_t i = 0; i < size; ++i)
+            applyVisitor(*this, x[i]);
+    }
+
+    void operator()(const UInt64 & x)
+    {
+        field_types.insert(FieldType::UInt64);
+        if (x <= std::numeric_limits<UInt8>::max())
+            type_indexes.insert(TypeIndex::UInt8);
+        else if (x <= std::numeric_limits<UInt16>::max())
+            type_indexes.insert(TypeIndex::UInt16);
+        else if (x <= std::numeric_limits<UInt32>::max())
+            type_indexes.insert(TypeIndex::UInt32);
+        else
+            type_indexes.insert(TypeIndex::UInt64);
+    }
+
+    void operator()(const Int64 & x)
+    {
+        field_types.insert(FieldType::Int64);
+        if (x <= std::numeric_limits<Int8>::max() && x >= std::numeric_limits<Int8>::min())
+            type_indexes.insert(TypeIndex::Int8);
+        else if (x <= std::numeric_limits<Int16>::max() && x >= std::numeric_limits<Int16>::min())
+            type_indexes.insert(TypeIndex::Int16);
+        else if (x <= std::numeric_limits<Int32>::max() && x >= std::numeric_limits<Int32>::min())
+            type_indexes.insert(TypeIndex::Int32);
+        else
+            type_indexes.insert(TypeIndex::Int64);
+    }
+
+    void operator()(const Null &)
+    {
+        have_nulls = true;
+    }
+
+    template <typename T>
+    void operator()(const T &)
+    {
+        field_types.insert(Field::TypeToEnum<NearestFieldType<T>>::value);
+        type_indexes.insert(TypeToTypeIndex<NearestFieldType<T>>);
+    }
+
+    DataTypePtr getScalarType() const { return getLeastSupertype(type_indexes, true); }
+    bool haveNulls() const { return have_nulls; }
+    bool needConvertField() const { return field_types.size() > 1; }
+
+private:
+    TypeIndexSet type_indexes;
+    std::unordered_set<FieldType> field_types;
+    bool have_nulls = false;
+};
+
+}
+
+FieldInfo getFieldInfo(const Field & field)
+{
+    FieldVisitorToScalarType to_scalar_type_visitor;
+    applyVisitor(to_scalar_type_visitor, field);
+
+    return
+    {
+        to_scalar_type_visitor.getScalarType(),
+        to_scalar_type_visitor.haveNulls(),
+        to_scalar_type_visitor.needConvertField(),
+        applyVisitor(FieldVisitorToNumberOfDimensions(), field),
+    };
+}
+
+ColumnObject::Subcolumn::Subcolumn(MutableColumnPtr && data_, bool is_nullable_)
+    : least_common_type(getDataTypeByColumn(*data_))
+    , is_nullable(is_nullable_)
+{
+    data.push_back(std::move(data_));
+}
+
+ColumnObject::Subcolumn::Subcolumn(
+    size_t size_, bool is_nullable_)
+    : least_common_type(std::make_shared<DataTypeNothing>())
+    , is_nullable(is_nullable_)
+    , num_of_defaults_in_prefix(size_)
+{
+}
+
+size_t ColumnObject::Subcolumn::Subcolumn::size() const
+{
+    size_t res = num_of_defaults_in_prefix;
+    for (const auto & part : data)
+        res += part->size();
+    return res;
+}
+
+size_t ColumnObject::Subcolumn::Subcolumn::byteSize() const
+{
+    size_t res = 0;
+    for (const auto & part : data)
+        res += part->byteSize();
+    return res;
+}
+
+size_t ColumnObject::Subcolumn::Subcolumn::allocatedBytes() const
+{
+    size_t res = 0;
+    for (const auto & part : data)
+        res += part->allocatedBytes();
+    return res;
+}
+
+void ColumnObject::Subcolumn::checkTypes() const
+{
+    DataTypes prefix_types;
+    prefix_types.reserve(data.size());
+    for (size_t i = 0; i < data.size(); ++i)
+    {
+        auto current_type = getDataTypeByColumn(*data[i]);
+        prefix_types.push_back(current_type);
+        auto prefix_common_type = getLeastSupertype(prefix_types);
+        if (!prefix_common_type->equals(*current_type))
+            throw Exception(ErrorCodes::LOGICAL_ERROR,
+                "Data type {} of column at position {} cannot represent all columns from i-th prefix",
+                current_type->getName(), i);
+    }
+}
+
+void ColumnObject::Subcolumn::insert(Field field)
+{
+    auto info = getFieldInfo(field);
+    insert(std::move(field), std::move(info));
+}
+
+void ColumnObject::Subcolumn::insert(Field field, FieldInfo info)
+{
+    auto base_type = info.scalar_type;
+
+    if (isNothing(base_type) && info.num_dimensions == 0)
+    {
+        insertDefault();
+        return;
+    }
+
+    auto column_dim = getNumberOfDimensions(*least_common_type);
+    auto value_dim = info.num_dimensions;
+
+    if (isNothing(least_common_type))
+        column_dim = value_dim;
+
+    if (field.isNull())
+        value_dim = column_dim;
+
+    if (value_dim != column_dim)
+        throw Exception(ErrorCodes::NUMBER_OF_DIMENSIONS_MISMATHED,
+            "Dimension of types mismatched between inserted value and column. "
+            "Dimension of value: {}. Dimension of column: {}",
+             value_dim, column_dim);
+
+    if (is_nullable)
+        base_type = makeNullable(base_type);
+
+    if (!is_nullable && info.have_nulls)
+        field = applyVisitor(FieldVisitorReplaceNull(base_type->getDefault(), value_dim), std::move(field));
+
+    auto value_type = createArrayOfType(base_type, value_dim);
+    bool type_changed = false;
+
+    if (data.empty())
+    {
+        data.push_back(value_type->createColumn());
+        least_common_type = value_type;
+    }
+    else if (!least_common_type->equals(*value_type))
+    {
+        value_type = getLeastSupertype(DataTypes{value_type, least_common_type}, true);
+        type_changed = true;
+        if (!least_common_type->equals(*value_type))
+        {
+            data.push_back(value_type->createColumn());
+            least_common_type = value_type;
+        }
+    }
+
+    if (type_changed || info.need_convert)
+        field = convertFieldToTypeOrThrow(field, *value_type);
+
+    data.back()->insert(field);
+}
+
+void ColumnObject::Subcolumn::insertRangeFrom(const Subcolumn & src, size_t start, size_t length)
+{
+    assert(src.isFinalized());
+
+    const auto & src_column = src.data.back();
+    const auto & src_type = src.least_common_type;
+
+    if (data.empty())
+    {
+        least_common_type = src_type;
+        data.push_back(src_type->createColumn());
+        data.back()->insertRangeFrom(*src_column, start, length);
+    }
+    else if (least_common_type->equals(*src_type))
+    {
+        data.back()->insertRangeFrom(*src_column, start, length);
+    }
+    else
+    {
+        auto new_least_common_type = getLeastSupertype(DataTypes{least_common_type, src_type}, true);
+        auto casted_column = castColumn({src_column, src_type, ""}, new_least_common_type);
+
+        if (!least_common_type->equals(*new_least_common_type))
+        {
+            least_common_type = new_least_common_type;
+            data.push_back(least_common_type->createColumn());
+        }
+
+        data.back()->insertRangeFrom(*casted_column, start, length);
+    }
+}
+
+void ColumnObject::Subcolumn::finalize()
+{
+    if (isFinalized() || data.empty())
+        return;
+
+    const auto & to_type = least_common_type;
+    auto result_column = to_type->createColumn();
+
+    if (num_of_defaults_in_prefix)
+        result_column->insertManyDefaults(num_of_defaults_in_prefix);
+
+    for (auto & part : data)
+    {
+        auto from_type = getDataTypeByColumn(*part);
+        size_t part_size = part->size();
+
+        if (!from_type->equals(*to_type))
+        {
+            auto offsets = ColumnUInt64::create();
+            auto & offsets_data = offsets->getData();
+
+            /// We need to convert only non-default values and then recreate column
+            /// with default value of new type, because default values (which represents misses in data)
+            /// may be inconsistent between types (e.g "0" in UInt64 and empty string in String).
+
+            part->getIndicesOfNonDefaultRows(offsets_data, 0, part_size);
+
+            if (offsets->size() == part_size)
+            {
+                part = castColumn({part, from_type, ""}, to_type);
+            }
+            else
+            {
+                auto values = part->index(*offsets, offsets->size());
+                values = castColumn({values, from_type, ""}, to_type);
+                part = values->createWithOffsets(offsets_data, to_type->getDefault(), part_size, /*shift=*/ 0);
+            }
+        }
+
+        result_column->insertRangeFrom(*part, 0, part_size);
+    }
+
+    data = { std::move(result_column) };
+    num_of_defaults_in_prefix = 0;
+}
+
+void ColumnObject::Subcolumn::insertDefault()
+{
+    if (data.empty())
+        ++num_of_defaults_in_prefix;
+    else
+        data.back()->insertDefault();
+}
+
+void ColumnObject::Subcolumn::insertManyDefaults(size_t length)
+{
+    if (data.empty())
+        num_of_defaults_in_prefix += length;
+    else
+        data.back()->insertManyDefaults(length);
+}
+
+void ColumnObject::Subcolumn::popBack(size_t n)
+{
+    assert(n <= size());
+
+    size_t num_removed = 0;
+    for (auto it = data.rbegin(); it != data.rend(); ++it)
+    {
+        if (n == 0)
+            break;
+
+        auto & column = *it;
+        if (n < column->size())
+        {
+            column->popBack(n);
+            n = 0;
+        }
+        else
+        {
+            ++num_removed;
+            n -= column->size();
+        }
+    }
+
+    data.resize(data.size() - num_removed);
+    num_of_defaults_in_prefix -= n;
+}
+
+Field ColumnObject::Subcolumn::getLastField() const
+{
+    if (data.empty())
+        return Field();
+
+    const auto & last_part = data.back();
+    assert(!last_part->empty());
+    return (*last_part)[last_part->size() - 1];
+}
+
+ColumnObject::Subcolumn ColumnObject::Subcolumn::recreateWithDefaultValues(const FieldInfo & field_info) const
+{
+    auto scalar_type = field_info.scalar_type;
+    if (is_nullable)
+        scalar_type = makeNullable(scalar_type);
+
+    Subcolumn new_subcolumn;
+    new_subcolumn.least_common_type = createArrayOfType(scalar_type, field_info.num_dimensions);
+    new_subcolumn.is_nullable = is_nullable;
+    new_subcolumn.num_of_defaults_in_prefix = num_of_defaults_in_prefix;
+    new_subcolumn.data.reserve(data.size());
+
+    for (const auto & part : data)
+        new_subcolumn.data.push_back(recreateColumnWithDefaultValues(
+            part, scalar_type, field_info.num_dimensions));
+
+    return new_subcolumn;
+}
+
+IColumn & ColumnObject::Subcolumn::getFinalizedColumn()
+{
+    assert(isFinalized());
+    return *data[0];
+}
+
+const IColumn & ColumnObject::Subcolumn::getFinalizedColumn() const
+{
+    assert(isFinalized());
+    return *data[0];
+}
+
+const ColumnPtr & ColumnObject::Subcolumn::getFinalizedColumnPtr() const
+{
+    assert(isFinalized());
+    return data[0];
+}
+
+ColumnObject::ColumnObject(bool is_nullable_)
+    : is_nullable(is_nullable_)
+    , num_rows(0)
+{
+}
+
+ColumnObject::ColumnObject(SubcolumnsTree && subcolumns_, bool is_nullable_)
+    : is_nullable(is_nullable_)
+    , subcolumns(std::move(subcolumns_))
+    , num_rows(subcolumns.empty() ? 0 : (*subcolumns.begin())->data.size())
+
+{
+    checkConsistency();
+}
+
+void ColumnObject::checkConsistency() const
+{
+    if (subcolumns.empty())
+        return;
+
+    for (const auto & leaf : subcolumns)
+    {
+        if (num_rows != leaf->data.size())
+        {
+            throw Exception(ErrorCodes::LOGICAL_ERROR, "Sizes of subcolumns are inconsistent in ColumnObject."
+                " Subcolumn '{}' has {} rows, but expected size is {}",
+                leaf->path.getPath(), leaf->data.size(), num_rows);
+        }
+    }
+}
+
+size_t ColumnObject::size() const
+{
+#ifndef NDEBUG
+    checkConsistency();
+#endif
+    return num_rows;
+}
+
+MutableColumnPtr ColumnObject::cloneResized(size_t new_size) const
+{
+    /// cloneResized with new_size == 0 is used for cloneEmpty().
+    if (new_size != 0)
+        throw Exception(ErrorCodes::NOT_IMPLEMENTED,
+            "ColumnObject doesn't support resize to non-zero length");
+
+    return ColumnObject::create(is_nullable);
+}
+
+size_t ColumnObject::byteSize() const
+{
+    size_t res = 0;
+    for (const auto & entry : subcolumns)
+        res += entry->data.byteSize();
+    return res;
+}
+
+size_t ColumnObject::allocatedBytes() const
+{
+    size_t res = 0;
+    for (const auto & entry : subcolumns)
+        res += entry->data.allocatedBytes();
+    return res;
+}
+
+void ColumnObject::forEachSubcolumn(ColumnCallback callback)
+{
+    if (!isFinalized())
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot iterate over non-finalized ColumnObject");
+
+    for (auto & entry : subcolumns)
+        callback(entry->data.data.back());
+}
+
+void ColumnObject::insert(const Field & field)
+{
+    const auto & object = field.get<const Object &>();
+
+    HashSet<StringRef, StringRefHash> inserted;
+    size_t old_size = size();
+    for (const auto & [key_str, value] : object)
+    {
+        PathInData key(key_str);
+        inserted.insert(key_str);
+        if (!hasSubcolumn(key))
+            addSubcolumn(key, old_size);
+
+        auto & subcolumn = getSubcolumn(key);
+        subcolumn.insert(value);
+    }
+
+    for (auto & entry : subcolumns)
+        if (!inserted.has(entry->path.getPath()))
+            entry->data.insertDefault();
+
+    ++num_rows;
+}
+
+void ColumnObject::insertDefault()
+{
+    for (auto & entry : subcolumns)
+        entry->data.insertDefault();
+
+    ++num_rows;
+}
+
+Field ColumnObject::operator[](size_t n) const
+{
+    if (!isFinalized())
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot get Field from non-finalized ColumnObject");
+
+    Object object;
+    for (const auto & entry : subcolumns)
+        object[entry->path.getPath()] = (*entry->data.data.back())[n];
+
+    return object;
+}
+
+void ColumnObject::get(size_t n, Field & res) const
+{
+    if (!isFinalized())
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot get Field from non-finalized ColumnObject");
+
+    auto & object = res.get<Object &>();
+    for (const auto & entry : subcolumns)
+    {
+        auto it = object.try_emplace(entry->path.getPath()).first;
+        entry->data.data.back()->get(n, it->second);
+    }
+}
+
+void ColumnObject::insertRangeFrom(const IColumn & src, size_t start, size_t length)
+{
+    const auto & src_object = assert_cast<const ColumnObject &>(src);
+
+    for (auto & entry : subcolumns)
+    {
+        if (src_object.hasSubcolumn(entry->path))
+            entry->data.insertRangeFrom(src_object.getSubcolumn(entry->path), start, length);
+        else
+            entry->data.insertManyDefaults(length);
+    }
+
+    num_rows += length;
+    finalize();
+}
+
+ColumnPtr ColumnObject::replicate(const Offsets & offsets) const
+{
+    if (!isFinalized())
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot replicate non-finalized ColumnObject");
+
+    auto res_column = ColumnObject::create(is_nullable);
+    for (const auto & entry : subcolumns)
+    {
+        auto replicated_data = entry->data.data.back()->replicate(offsets)->assumeMutable();
+        res_column->addSubcolumn(entry->path, std::move(replicated_data));
+    }
+
+    return res_column;
+}
+
+void ColumnObject::popBack(size_t length)
+{
+    for (auto & entry : subcolumns)
+        entry->data.popBack(length);
+
+    num_rows -= length;
+}
+
+const ColumnObject::Subcolumn & ColumnObject::getSubcolumn(const PathInData & key) const
+{
+    if (const auto * node = subcolumns.findLeaf(key))
+        return node->data;
+
+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, "There is no subcolumn {} in ColumnObject", key.getPath());
+}
+
+ColumnObject::Subcolumn & ColumnObject::getSubcolumn(const PathInData & key)
+{
+    if (const auto * node = subcolumns.findLeaf(key))
+        return const_cast<SubcolumnsTree::Node *>(node)->data;
+
+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, "There is no subcolumn {} in ColumnObject", key.getPath());
+}
+
+bool ColumnObject::hasSubcolumn(const PathInData & key) const
+{
+    return subcolumns.findLeaf(key) != nullptr;
+}
+
+void ColumnObject::addSubcolumn(const PathInData & key, MutableColumnPtr && subcolumn)
+{
+    size_t new_size = subcolumn->size();
+    bool inserted = subcolumns.add(key, Subcolumn(std::move(subcolumn), is_nullable));
+
+    if (!inserted)
+        throw Exception(ErrorCodes::DUPLICATE_COLUMN, "Subcolumn '{}' already exists", key.getPath());
+
+    if (num_rows == 0)
+        num_rows = new_size;
+    else if (new_size != num_rows)
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH,
+            "Size of subcolumn {} ({}) is inconsistent with column size ({})",
+            key.getPath(), new_size, num_rows);
+}
+
+void ColumnObject::addSubcolumn(const PathInData & key, size_t new_size)
+{
+    bool inserted = subcolumns.add(key, Subcolumn(new_size, is_nullable));
+    if (!inserted)
+        throw Exception(ErrorCodes::DUPLICATE_COLUMN, "Subcolumn '{}' already exists", key.getPath());
+
+    if (num_rows == 0)
+        num_rows = new_size;
+    else if (new_size != num_rows)
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH,
+            "Required size of subcolumn {} ({}) is inconsistent with column size ({})",
+            key.getPath(), new_size, num_rows);
+}
+
+void ColumnObject::addNestedSubcolumn(const PathInData & key, const FieldInfo & field_info, size_t new_size)
+{
+    if (!key.hasNested())
+        throw Exception(ErrorCodes::LOGICAL_ERROR,
+            "Cannot add Nested subcolumn, because path doesn't contain Nested");
+
+    bool inserted = false;
+    /// We find node that represents the same Nested type as @key.
+    const auto * nested_node = subcolumns.findBestMatch(key);
+
+    if (nested_node)
+    {
+        /// Find any leaf of Nested subcolumn.
+        const auto * leaf = subcolumns.findLeaf(nested_node, [&](const auto &) { return true; });
+        assert(leaf);
+
+        /// Recreate subcolumn with default values and the same sizes of arrays.
+        auto new_subcolumn = leaf->data.recreateWithDefaultValues(field_info);
+
+        /// It's possible that we have already inserted value from current row
+        /// to this subcolumn. So, adjust size to expected.
+        if (new_subcolumn.size() > new_size)
+            new_subcolumn.popBack(new_subcolumn.size() - new_size);
+
+        assert(new_subcolumn.size() == new_size);
+        inserted = subcolumns.add(key, new_subcolumn);
+    }
+    else
+    {
+        /// If node was not found just add subcolumn with empty arrays.
+        inserted = subcolumns.add(key, Subcolumn(new_size, is_nullable));
+    }
+
+    if (!inserted)
+        throw Exception(ErrorCodes::DUPLICATE_COLUMN, "Subcolumn '{}' already exists", key.getPath());
+
+    if (num_rows == 0)
+        num_rows = new_size;
+}
+
+PathsInData ColumnObject::getKeys() const
+{
+    PathsInData keys;
+    keys.reserve(subcolumns.size());
+    for (const auto & entry : subcolumns)
+        keys.emplace_back(entry->path);
+    return keys;
+}
+
+bool ColumnObject::isFinalized() const
+{
+    return std::all_of(subcolumns.begin(), subcolumns.end(),
+        [](const auto & entry) { return entry->data.isFinalized(); });
+}
+
+void ColumnObject::finalize()
+{
+    size_t old_size = size();
+    SubcolumnsTree new_subcolumns;
+    for (auto && entry : subcolumns)
+    {
+        const auto & least_common_type = entry->data.getLeastCommonType();
+
+        /// Do not add subcolumns, which consists only from NULLs.
+        if (isNothing(getBaseTypeOfArray(least_common_type)))
+            continue;
+
+        entry->data.finalize();
+        new_subcolumns.add(entry->path, entry->data);
+    }
+
+    /// If all subcolumns were skipped add a dummy subcolumn,
+    /// because Tuple type must have at least one element.
+    if (new_subcolumns.empty())
+        new_subcolumns.add(PathInData{COLUMN_NAME_DUMMY}, Subcolumn{ColumnUInt8::create(old_size, 0), is_nullable});
+
+    std::swap(subcolumns, new_subcolumns);
+    checkObjectHasNoAmbiguosPaths(getKeys());
+}
+
+}
diff --git a/src/Columns/ColumnObject.h b/src/Columns/ColumnObject.h
new file mode 100644
index 000000000000..06d946f2ea82
--- /dev/null
+++ b/src/Columns/ColumnObject.h
@@ -0,0 +1,219 @@
+#pragma once
+
+#include <Core/Field.h>
+#include <Core/Names.h>
+#include <Columns/IColumn.h>
+#include <Common/PODArray.h>
+#include <Common/HashTable/HashMap.h>
+#include <DataTypes/Serializations/JSONDataParser.h>
+#include <DataTypes/Serializations/SubcolumnsTree.h>
+
+#include <DataTypes/IDataType.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int LOGICAL_ERROR;
+}
+
+/// Info that represents a scalar or array field in a decomposed view.
+/// It allows to recreate field with different number
+/// of dimensions or nullability.
+struct FieldInfo
+{
+    /// The common type of of all scalars in field.
+    DataTypePtr scalar_type;
+
+    /// Do we have NULL scalar in field.
+    bool have_nulls;
+
+    /// If true then we have scalars with different types in array and
+    /// we need to convert scalars to the common type.
+    bool need_convert;
+
+    /// Number of dimension in array. 0 if field is scalar.
+    size_t num_dimensions;
+};
+
+FieldInfo getFieldInfo(const Field & field);
+
+/** A column that represents object with dynamic set of subcolumns.
+ *  Subcolumns are identified by paths in document and are stored in
+ *  a trie-like structure. ColumnObject is not suitable for writing into tables
+ *  and it should be converted to Tuple with fixed set of subcolumns before that.
+ */
+class ColumnObject final : public COWHelper<IColumn, ColumnObject>
+{
+public:
+    /** Class that represents one subcolumn.
+     * It stores values in several parts of column
+     * and keeps current common type of all parts.
+     * We add a new column part with a new type, when we insert a field,
+     * which can't be converted to the current common type.
+     * After insertion of all values subcolumn should be finalized
+     * for writing and other operations.
+     */
+    class Subcolumn
+    {
+    public:
+        Subcolumn() = default;
+        Subcolumn(size_t size_, bool is_nullable_);
+        Subcolumn(MutableColumnPtr && data_, bool is_nullable_);
+
+        size_t size() const;
+        size_t byteSize() const;
+        size_t allocatedBytes() const;
+
+        bool isFinalized() const { return data.size() == 1 && num_of_defaults_in_prefix == 0; }
+        const DataTypePtr & getLeastCommonType() const { return least_common_type; }
+
+        /// Checks the consistency of column's parts stored in @data.
+        void checkTypes() const;
+
+        /// Inserts a field, which scalars can be arbitrary, but number of
+        /// dimensions should be consistent with current common type.
+        void insert(Field field);
+        void insert(Field field, FieldInfo info);
+
+        void insertDefault();
+        void insertManyDefaults(size_t length);
+        void insertRangeFrom(const Subcolumn & src, size_t start, size_t length);
+        void popBack(size_t n);
+
+        /// Converts all column's parts to the common type and
+        /// creates a single column that stores all values.
+        void finalize();
+
+        /// Returns last inserted field.
+        Field getLastField() const;
+
+        /// Recreates subcolumn with default scalar values and keeps sizes of arrays.
+        /// Used to create columns of type Nested with consistent array sizes.
+        Subcolumn recreateWithDefaultValues(const FieldInfo & field_info) const;
+
+        /// Returns single column if subcolumn in finalizes.
+        /// Otherwise -- undefined behaviour.
+        IColumn & getFinalizedColumn();
+        const IColumn & getFinalizedColumn() const;
+        const ColumnPtr & getFinalizedColumnPtr() const;
+
+        friend class ColumnObject;
+
+    private:
+        /// Current least common type of all values inserted to this subcolumn.
+        DataTypePtr least_common_type;
+
+        /// If true then common type type of subcolumn is Nullable
+        /// and default values are NULLs.
+        bool is_nullable = false;
+
+        /// Parts of column. Parts should be in increasing order in terms of subtypes/supertypes.
+        /// That means that the least common type for i-th prefix is the type of i-th part
+        /// and it's the supertype for all type of column from 0 to i-1.
+        std::vector<WrappedPtr> data;
+
+        /// Until we insert any non-default field we don't know further
+        /// least common type and we count number of defaults in prefix,
+        /// which will be converted to the default type of final common type.
+        size_t num_of_defaults_in_prefix = 0;
+    };
+
+    using SubcolumnsTree = SubcolumnsTree<Subcolumn>;
+
+private:
+    /// If true then all subcolumns are nullable.
+    const bool is_nullable;
+
+    SubcolumnsTree subcolumns;
+    size_t num_rows;
+
+public:
+    static constexpr auto COLUMN_NAME_DUMMY = "_dummy";
+
+    explicit ColumnObject(bool is_nullable_);
+    ColumnObject(SubcolumnsTree && subcolumns_, bool is_nullable_);
+
+    /// Checks that all subcolumns have consistent sizes.
+    void checkConsistency() const;
+
+    bool hasSubcolumn(const PathInData & key) const;
+
+    const Subcolumn & getSubcolumn(const PathInData & key) const;
+    Subcolumn & getSubcolumn(const PathInData & key);
+
+    void incrementNumRows() { ++num_rows; }
+
+    /// Adds a subcolumn from existing IColumn.
+    void addSubcolumn(const PathInData & key, MutableColumnPtr && subcolumn);
+
+    /// Adds a subcolumn of specific size with default values.
+    void addSubcolumn(const PathInData & key, size_t new_size);
+
+    /// Adds a subcolumn of type Nested of specific size with default values.
+    /// It cares about consistency of sizes of Nested arrays.
+    void addNestedSubcolumn(const PathInData & key, const FieldInfo & field_info, size_t new_size);
+
+    const SubcolumnsTree & getSubcolumns() const { return subcolumns; }
+    SubcolumnsTree & getSubcolumns() { return subcolumns; }
+    PathsInData getKeys() const;
+
+    /// Finalizes all subcolumns.
+    void finalize();
+    bool isFinalized() const;
+
+    /// Part of interface
+
+    const char * getFamilyName() const override { return "Object"; }
+    TypeIndex getDataType() const override { return TypeIndex::Object; }
+
+    size_t size() const override;
+    MutableColumnPtr cloneResized(size_t new_size) const override;
+    size_t byteSize() const override;
+    size_t allocatedBytes() const override;
+    void forEachSubcolumn(ColumnCallback callback) override;
+    void insert(const Field & field) override;
+    void insertDefault() override;
+    void insertRangeFrom(const IColumn & src, size_t start, size_t length) override;
+    ColumnPtr replicate(const Offsets & offsets) const override;
+    void popBack(size_t length) override;
+    Field operator[](size_t n) const override;
+    void get(size_t n, Field & res) const override;
+
+    /// All other methods throw exception.
+
+    ColumnPtr decompress() const override { throwMustBeConcrete(); }
+    StringRef getDataAt(size_t) const override { throwMustBeConcrete(); }
+    bool isDefaultAt(size_t) const override { throwMustBeConcrete(); }
+    void insertData(const char *, size_t) override { throwMustBeConcrete(); }
+    StringRef serializeValueIntoArena(size_t, Arena &, char const *&) const override { throwMustBeConcrete(); }
+    const char * deserializeAndInsertFromArena(const char *) override { throwMustBeConcrete(); }
+    const char * skipSerializedInArena(const char *) const override { throwMustBeConcrete(); }
+    void updateHashWithValue(size_t, SipHash &) const override { throwMustBeConcrete(); }
+    void updateWeakHash32(WeakHash32 &) const override { throwMustBeConcrete(); }
+    void updateHashFast(SipHash &) const override { throwMustBeConcrete(); }
+    ColumnPtr filter(const Filter &, ssize_t) const override { throwMustBeConcrete(); }
+    void expand(const Filter &, bool) override { throwMustBeConcrete(); }
+    ColumnPtr permute(const Permutation &, size_t) const override { throwMustBeConcrete(); }
+    ColumnPtr index(const IColumn &, size_t) const override { throwMustBeConcrete(); }
+    int compareAt(size_t, size_t, const IColumn &, int) const override { throwMustBeConcrete(); }
+    void compareColumn(const IColumn &, size_t, PaddedPODArray<UInt64> *, PaddedPODArray<Int8> &, int, int) const override { throwMustBeConcrete(); }
+    bool hasEqualValues() const override { throwMustBeConcrete(); }
+    void getPermutation(PermutationSortDirection, PermutationSortStability, size_t, int, Permutation &) const override { throwMustBeConcrete(); }
+    void updatePermutation(PermutationSortDirection, PermutationSortStability, size_t, int, Permutation &, EqualRanges &) const override { throwMustBeConcrete(); }
+    MutableColumns scatter(ColumnIndex, const Selector &) const override { throwMustBeConcrete(); }
+    void gather(ColumnGathererStream &) override { throwMustBeConcrete(); }
+    void getExtremes(Field &, Field &) const override { throwMustBeConcrete(); }
+    size_t byteSizeAt(size_t) const override { throwMustBeConcrete(); }
+    double getRatioOfDefaultRows(double) const override { throwMustBeConcrete(); }
+    void getIndicesOfNonDefaultRows(Offsets &, size_t, size_t) const override { throwMustBeConcrete(); }
+
+private:
+    [[noreturn]] static void throwMustBeConcrete()
+    {
+        throw Exception("ColumnObject must be converted to ColumnTuple before use", ErrorCodes::LOGICAL_ERROR);
+    }
+};
+
+}
diff --git a/src/Columns/ColumnSparse.cpp b/src/Columns/ColumnSparse.cpp
index 79ec06d78820..d2cc8223a91b 100644
--- a/src/Columns/ColumnSparse.cpp
+++ b/src/Columns/ColumnSparse.cpp
@@ -288,7 +288,7 @@ void ColumnSparse::popBack(size_t n)
 ColumnPtr ColumnSparse::filter(const Filter & filt, ssize_t) const
 {
     if (_size != filt.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), _size);
 
     if (offsets->empty())
     {
diff --git a/src/Columns/ColumnVector.cpp b/src/Columns/ColumnVector.cpp
index 7431637ff0eb..dded5ff6c990 100644
--- a/src/Columns/ColumnVector.cpp
+++ b/src/Columns/ColumnVector.cpp
@@ -381,7 +381,7 @@ ColumnPtr ColumnVector<T>::filter(const IColumn::Filter & filt, ssize_t result_s
 {
     size_t size = data.size();
     if (size != filt.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), size);
 
     auto res = this->create();
     Container & res_data = res->getData();
@@ -450,7 +450,7 @@ void ColumnVector<T>::applyZeroMap(const IColumn::Filter & filt, bool inverted)
 {
     size_t size = data.size();
     if (size != filt.size())
-        throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+        throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), size);
 
     const UInt8 * filt_pos = filt.data();
     const UInt8 * filt_end = filt_pos + size;
diff --git a/src/Columns/ColumnsCommon.cpp b/src/Columns/ColumnsCommon.cpp
index 701b888fb254..0a9201f75432 100644
--- a/src/Columns/ColumnsCommon.cpp
+++ b/src/Columns/ColumnsCommon.cpp
@@ -192,7 +192,7 @@ namespace
     {
         const size_t size = src_offsets.size();
         if (size != filt.size())
-            throw Exception("Size of filter doesn't match size of column.", ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH);
+            throw Exception(ErrorCodes::SIZES_OF_COLUMNS_DOESNT_MATCH, "Size of filter ({}) doesn't match size of column ({})", filt.size(), size);
 
         ResultOffsetsBuilder result_offsets_builder(res_offsets);
 
diff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp
index 09167ea4849f..3ed13ac04eaa 100644
--- a/src/Common/ErrorCodes.cpp
+++ b/src/Common/ErrorCodes.cpp
@@ -613,6 +613,7 @@
     M(642, CANNOT_PACK_ARCHIVE) \
     M(643, CANNOT_UNPACK_ARCHIVE) \
     M(644, REMOTE_FS_OBJECT_CACHE_ERROR) \
+    M(645, NUMBER_OF_DIMENSIONS_MISMATHED) \
     \
     M(999, KEEPER_EXCEPTION) \
     M(1000, POCO_EXCEPTION) \
diff --git a/src/Common/FieldVisitorConvertToNumber.h b/src/Common/FieldVisitorConvertToNumber.h
index 7bbb7f0708ac..ba647e3cf147 100644
--- a/src/Common/FieldVisitorConvertToNumber.h
+++ b/src/Common/FieldVisitorConvertToNumber.h
@@ -46,6 +46,11 @@ class FieldVisitorConvertToNumber : public StaticVisitor<T>
         throw Exception("Cannot convert Map to " + demangle(typeid(T).name()), ErrorCodes::CANNOT_CONVERT_TYPE);
     }
 
+    T operator() (const Object &) const
+    {
+        throw Exception("Cannot convert Object to " + demangle(typeid(T).name()), ErrorCodes::CANNOT_CONVERT_TYPE);
+    }
+
     T operator() (const UInt64 & x) const { return T(x); }
     T operator() (const Int64 & x) const { return T(x); }
     T operator() (const Int128 & x) const { return T(x); }
diff --git a/src/Common/FieldVisitorDump.cpp b/src/Common/FieldVisitorDump.cpp
index 6c869e05fd4c..fc3d56c3503c 100644
--- a/src/Common/FieldVisitorDump.cpp
+++ b/src/Common/FieldVisitorDump.cpp
@@ -95,6 +95,23 @@ String FieldVisitorDump::operator() (const Map & x) const
     return wb.str();
 }
 
+String FieldVisitorDump::operator() (const Object & x) const
+{
+    WriteBufferFromOwnString wb;
+
+    wb << "Object_(";
+    for (auto it = x.begin(); it != x.end(); ++it)
+    {
+        if (it != x.begin())
+            wb << ", ";
+        wb << "(" << it->first << ", " << applyVisitor(*this, it->second) << ")";
+    }
+    wb << ')';
+
+    return wb.str();
+
+}
+
 String FieldVisitorDump::operator() (const AggregateFunctionStateData & x) const
 {
     WriteBufferFromOwnString wb;
diff --git a/src/Common/FieldVisitorDump.h b/src/Common/FieldVisitorDump.h
index 0b1b311999e2..dc67ccf7da33 100644
--- a/src/Common/FieldVisitorDump.h
+++ b/src/Common/FieldVisitorDump.h
@@ -22,6 +22,7 @@ class FieldVisitorDump : public StaticVisitor<String>
     String operator() (const Array & x) const;
     String operator() (const Tuple & x) const;
     String operator() (const Map & x) const;
+    String operator() (const Object & x) const;
     String operator() (const DecimalField<Decimal32> & x) const;
     String operator() (const DecimalField<Decimal64> & x) const;
     String operator() (const DecimalField<Decimal128> & x) const;
diff --git a/src/Common/FieldVisitorHash.cpp b/src/Common/FieldVisitorHash.cpp
index 09b8b7908f3d..b6750fdcd03a 100644
--- a/src/Common/FieldVisitorHash.cpp
+++ b/src/Common/FieldVisitorHash.cpp
@@ -94,6 +94,19 @@ void FieldVisitorHash::operator() (const Array & x) const
         applyVisitor(*this, elem);
 }
 
+void FieldVisitorHash::operator() (const Object & x) const
+{
+    UInt8 type = Field::Types::Object;
+    hash.update(type);
+    hash.update(x.size());
+
+    for (const auto & [key, value]: x)
+    {
+        hash.update(key);
+        applyVisitor(*this, value);
+    }
+}
+
 void FieldVisitorHash::operator() (const DecimalField<Decimal32> & x) const
 {
     UInt8 type = Field::Types::Decimal32;
diff --git a/src/Common/FieldVisitorHash.h b/src/Common/FieldVisitorHash.h
index 82e831b961e0..e574b0456eba 100644
--- a/src/Common/FieldVisitorHash.h
+++ b/src/Common/FieldVisitorHash.h
@@ -28,6 +28,7 @@ class FieldVisitorHash : public StaticVisitor<>
     void operator() (const Array & x) const;
     void operator() (const Tuple & x) const;
     void operator() (const Map & x) const;
+    void operator() (const Object & x) const;
     void operator() (const DecimalField<Decimal32> & x) const;
     void operator() (const DecimalField<Decimal64> & x) const;
     void operator() (const DecimalField<Decimal128> & x) const;
diff --git a/src/Common/FieldVisitorSum.cpp b/src/Common/FieldVisitorSum.cpp
index c3d7f4f84626..a8cb694d9304 100644
--- a/src/Common/FieldVisitorSum.cpp
+++ b/src/Common/FieldVisitorSum.cpp
@@ -26,6 +26,7 @@ bool FieldVisitorSum::operator() (String &) const { throw Exception("Cannot sum
 bool FieldVisitorSum::operator() (Array &) const { throw Exception("Cannot sum Arrays", ErrorCodes::LOGICAL_ERROR); }
 bool FieldVisitorSum::operator() (Tuple &) const { throw Exception("Cannot sum Tuples", ErrorCodes::LOGICAL_ERROR); }
 bool FieldVisitorSum::operator() (Map &) const { throw Exception("Cannot sum Maps", ErrorCodes::LOGICAL_ERROR); }
+bool FieldVisitorSum::operator() (Object &) const { throw Exception("Cannot sum Objects", ErrorCodes::LOGICAL_ERROR); }
 bool FieldVisitorSum::operator() (UUID &) const { throw Exception("Cannot sum UUIDs", ErrorCodes::LOGICAL_ERROR); }
 
 bool FieldVisitorSum::operator() (AggregateFunctionStateData &) const
diff --git a/src/Common/FieldVisitorSum.h b/src/Common/FieldVisitorSum.h
index 3e868e46f714..609336870093 100644
--- a/src/Common/FieldVisitorSum.h
+++ b/src/Common/FieldVisitorSum.h
@@ -25,6 +25,7 @@ class FieldVisitorSum : public StaticVisitor<bool>
     bool operator() (Array &) const;
     bool operator() (Tuple &) const;
     bool operator() (Map &) const;
+    bool operator() (Object &) const;
     bool operator() (UUID &) const;
     bool operator() (AggregateFunctionStateData &) const;
     bool operator() (bool &) const;
diff --git a/src/Common/FieldVisitorToString.cpp b/src/Common/FieldVisitorToString.cpp
index 6cc83f32a529..7d93cfba78fc 100644
--- a/src/Common/FieldVisitorToString.cpp
+++ b/src/Common/FieldVisitorToString.cpp
@@ -126,5 +126,24 @@ String FieldVisitorToString::operator() (const Map & x) const
     return wb.str();
 }
 
+String FieldVisitorToString::operator() (const Object & x) const
+{
+    WriteBufferFromOwnString wb;
+
+    wb << '{';
+    for (auto it = x.begin(); it != x.end(); ++it)
+    {
+        if (it != x.begin())
+            wb << ", ";
+
+        writeDoubleQuoted(it->first, wb);
+        wb << ": " << applyVisitor(*this, it->second);
+    }
+    wb << '}';
+
+    return wb.str();
+
+}
+
 }
 
diff --git a/src/Common/FieldVisitorToString.h b/src/Common/FieldVisitorToString.h
index 991f7b4b2d7d..324a4aa73d52 100644
--- a/src/Common/FieldVisitorToString.h
+++ b/src/Common/FieldVisitorToString.h
@@ -22,6 +22,7 @@ class FieldVisitorToString : public StaticVisitor<String>
     String operator() (const Array & x) const;
     String operator() (const Tuple & x) const;
     String operator() (const Map & x) const;
+    String operator() (const Object & x) const;
     String operator() (const DecimalField<Decimal32> & x) const;
     String operator() (const DecimalField<Decimal64> & x) const;
     String operator() (const DecimalField<Decimal128> & x) const;
diff --git a/src/Common/FieldVisitorWriteBinary.cpp b/src/Common/FieldVisitorWriteBinary.cpp
index fc17b58b3349..edabd26fd3a0 100644
--- a/src/Common/FieldVisitorWriteBinary.cpp
+++ b/src/Common/FieldVisitorWriteBinary.cpp
@@ -66,6 +66,20 @@ void FieldVisitorWriteBinary::operator() (const Map & x, WriteBuffer & buf) cons
     }
 }
 
+void FieldVisitorWriteBinary::operator() (const Object & x, WriteBuffer & buf) const
+{
+    const size_t size = x.size();
+    writeBinary(size, buf);
+
+    for (const auto & [key, value] : x)
+    {
+        const UInt8 type = value.getType();
+        writeBinary(type, buf);
+        writeBinary(key, buf);
+        Field::dispatch([&buf] (const auto & val) { FieldVisitorWriteBinary()(val, buf); }, value);
+    }
+}
+
 void FieldVisitorWriteBinary::operator()(const bool & x, WriteBuffer & buf) const
 {
     writeBinary(UInt8(x), buf);
diff --git a/src/Common/FieldVisitorWriteBinary.h b/src/Common/FieldVisitorWriteBinary.h
index 155cf0e10507..ff2740383f7e 100644
--- a/src/Common/FieldVisitorWriteBinary.h
+++ b/src/Common/FieldVisitorWriteBinary.h
@@ -21,6 +21,7 @@ class FieldVisitorWriteBinary
     void operator() (const Array & x, WriteBuffer & buf) const;
     void operator() (const Tuple & x, WriteBuffer & buf) const;
     void operator() (const Map & x, WriteBuffer & buf) const;
+    void operator() (const Object & x, WriteBuffer & buf) const;
     void operator() (const DecimalField<Decimal32> & x, WriteBuffer & buf) const;
     void operator() (const DecimalField<Decimal64> & x, WriteBuffer & buf) const;
     void operator() (const DecimalField<Decimal128> & x, WriteBuffer & buf) const;
diff --git a/src/Functions/DummyJSONParser.h b/src/Common/JSONParsers/DummyJSONParser.h
similarity index 100%
rename from src/Functions/DummyJSONParser.h
rename to src/Common/JSONParsers/DummyJSONParser.h
diff --git a/src/Functions/RapidJSONParser.h b/src/Common/JSONParsers/RapidJSONParser.h
similarity index 100%
rename from src/Functions/RapidJSONParser.h
rename to src/Common/JSONParsers/RapidJSONParser.h
diff --git a/src/Functions/SimdJSONParser.h b/src/Common/JSONParsers/SimdJSONParser.h
similarity index 100%
rename from src/Functions/SimdJSONParser.h
rename to src/Common/JSONParsers/SimdJSONParser.h
diff --git a/src/Common/config.h.in b/src/Common/config.h.in
index edade4ce2be6..d8d308c59bd0 100644
--- a/src/Common/config.h.in
+++ b/src/Common/config.h.in
@@ -13,6 +13,9 @@
 #cmakedefine01 USE_CASSANDRA
 #cmakedefine01 USE_SENTRY
 #cmakedefine01 USE_GRPC
+#cmakedefine01 USE_SIMDJSON
+#cmakedefine01 USE_RAPIDJSON
+
 #cmakedefine01 USE_DATASKETCHES
 #cmakedefine01 USE_YAML_CPP
 #cmakedefine01 CLICKHOUSE_SPLIT_BINARY
diff --git a/src/Core/Block.cpp b/src/Core/Block.cpp
index f7d6761124f3..5c93d6719fae 100644
--- a/src/Core/Block.cpp
+++ b/src/Core/Block.cpp
@@ -726,18 +726,6 @@ void convertToFullIfSparse(Block & block)
         column.column = recursiveRemoveSparse(column.column);
 }
 
-ColumnPtr getColumnFromBlock(const Block & block, const NameAndTypePair & column)
-{
-    auto current_column = block.getByName(column.getNameInStorage()).column;
-    current_column = current_column->decompress();
-
-    if (column.isSubcolumn())
-        return column.getTypeInStorage()->getSubcolumn(column.getSubcolumnName(), current_column);
-
-    return current_column;
-}
-
-
 Block materializeBlock(const Block & block)
 {
     if (!block)
diff --git a/src/Core/Block.h b/src/Core/Block.h
index d7b02f44f13b..66e16b70f47d 100644
--- a/src/Core/Block.h
+++ b/src/Core/Block.h
@@ -196,10 +196,6 @@ void getBlocksDifference(const Block & lhs, const Block & rhs, std::string & out
 
 void convertToFullIfSparse(Block & block);
 
-/// Helps in-memory storages to extract columns from block.
-/// Properly handles cases, when column is a subcolumn and when it is compressed.
-ColumnPtr getColumnFromBlock(const Block & block, const NameAndTypePair & column);
-
 /// Converts columns-constants to full columns ("materializes" them).
 Block materializeBlock(const Block & block);
 void materializeBlockInplace(Block & block);
diff --git a/src/Core/Field.cpp b/src/Core/Field.cpp
index 70a1458c9f03..2f37d2ea9517 100644
--- a/src/Core/Field.cpp
+++ b/src/Core/Field.cpp
@@ -99,6 +99,12 @@ inline Field getBinaryValue(UInt8 type, ReadBuffer & buf)
             readBinary(value, buf);
             return value;
         }
+        case Field::Types::Object:
+        {
+            Object value;
+            readBinary(value, buf);
+            return value;
+        }
         case Field::Types::AggregateFunctionState:
         {
             AggregateFunctionStateData value;
@@ -208,6 +214,40 @@ void writeText(const Map & x, WriteBuffer & buf)
     writeFieldText(Field(x), buf);
 }
 
+void readBinary(Object & x, ReadBuffer & buf)
+{
+    size_t size;
+    readBinary(size, buf);
+
+    for (size_t index = 0; index < size; ++index)
+    {
+        UInt8 type;
+        String key;
+        readBinary(type, buf);
+        readBinary(key, buf);
+        x[key] = getBinaryValue(type, buf);
+    }
+}
+
+void writeBinary(const Object & x, WriteBuffer & buf)
+{
+    const size_t size = x.size();
+    writeBinary(size, buf);
+
+    for (const auto & [key, value] : x)
+    {
+        const UInt8 type = value.getType();
+        writeBinary(type, buf);
+        writeBinary(key, buf);
+        Field::dispatch([&buf] (const auto & val) { FieldVisitorWriteBinary()(val, buf); }, value);
+    }
+}
+
+void writeText(const Object & x, WriteBuffer & buf)
+{
+    writeFieldText(Field(x), buf);
+}
+
 template <typename T>
 void readQuoted(DecimalField<T> & x, ReadBuffer & buf)
 {
diff --git a/src/Core/Field.h b/src/Core/Field.h
index 9b830771c5f4..2f4e648d379f 100644
--- a/src/Core/Field.h
+++ b/src/Core/Field.h
@@ -3,6 +3,7 @@
 #include <cassert>
 #include <vector>
 #include <algorithm>
+#include <map>
 #include <type_traits>
 #include <functional>
 
@@ -49,10 +50,22 @@ DEFINE_FIELD_VECTOR(Array);
 DEFINE_FIELD_VECTOR(Tuple);
 
 /// An array with the following structure: [(key1, value1), (key2, value2), ...]
-DEFINE_FIELD_VECTOR(Map);
+DEFINE_FIELD_VECTOR(Map); /// TODO: use map instead of vector.
 
 #undef DEFINE_FIELD_VECTOR
 
+using FieldMap = std::map<String, Field, std::less<String>, AllocatorWithMemoryTracking<std::pair<const String, Field>>>;
+
+#define DEFINE_FIELD_MAP(X) \
+struct X : public FieldMap \
+{ \
+    using FieldMap::FieldMap; \
+}
+
+DEFINE_FIELD_MAP(Object);
+
+#undef DEFINE_FIELD_MAP
+
 struct AggregateFunctionStateData
 {
     String name; /// Name with arguments.
@@ -219,6 +232,7 @@ template <> struct NearestFieldTypeImpl<String> { using Type = String; };
 template <> struct NearestFieldTypeImpl<Array> { using Type = Array; };
 template <> struct NearestFieldTypeImpl<Tuple> { using Type = Tuple; };
 template <> struct NearestFieldTypeImpl<Map> { using Type = Map; };
+template <> struct NearestFieldTypeImpl<Object> { using Type = Object; };
 template <> struct NearestFieldTypeImpl<bool> { using Type = UInt64; };
 template <> struct NearestFieldTypeImpl<Null> { using Type = Null; };
 
@@ -283,6 +297,7 @@ class Field
             Map = 26,
             UUID = 27,
             Bool = 28,
+            Object = 29,
         };
     };
 
@@ -472,6 +487,7 @@ class Field
             case Types::Array:   return get<Array>()   < rhs.get<Array>();
             case Types::Tuple:   return get<Tuple>()   < rhs.get<Tuple>();
             case Types::Map:     return get<Map>()     < rhs.get<Map>();
+            case Types::Object:  return get<Object>()  < rhs.get<Object>();
             case Types::Decimal32:  return get<DecimalField<Decimal32>>()  < rhs.get<DecimalField<Decimal32>>();
             case Types::Decimal64:  return get<DecimalField<Decimal64>>()  < rhs.get<DecimalField<Decimal64>>();
             case Types::Decimal128: return get<DecimalField<Decimal128>>() < rhs.get<DecimalField<Decimal128>>();
@@ -510,6 +526,7 @@ class Field
             case Types::Array:   return get<Array>()   <= rhs.get<Array>();
             case Types::Tuple:   return get<Tuple>()   <= rhs.get<Tuple>();
             case Types::Map:     return get<Map>()     <= rhs.get<Map>();
+            case Types::Object:  return get<Object>()  <= rhs.get<Object>();
             case Types::Decimal32:  return get<DecimalField<Decimal32>>()  <= rhs.get<DecimalField<Decimal32>>();
             case Types::Decimal64:  return get<DecimalField<Decimal64>>()  <= rhs.get<DecimalField<Decimal64>>();
             case Types::Decimal128: return get<DecimalField<Decimal128>>() <= rhs.get<DecimalField<Decimal128>>();
@@ -548,6 +565,7 @@ class Field
             case Types::Array:   return get<Array>()   == rhs.get<Array>();
             case Types::Tuple:   return get<Tuple>()   == rhs.get<Tuple>();
             case Types::Map:     return get<Map>()     == rhs.get<Map>();
+            case Types::Object:  return get<Object>()  == rhs.get<Object>();
             case Types::UInt128: return get<UInt128>() == rhs.get<UInt128>();
             case Types::UInt256: return get<UInt256>() == rhs.get<UInt256>();
             case Types::Int128:  return get<Int128>()  == rhs.get<Int128>();
@@ -597,6 +615,7 @@ class Field
                 bool value = bool(field.template get<UInt64>());
                 return f(value);
             }
+            case Types::Object:     return f(field.template get<Object>());
             case Types::Decimal32:  return f(field.template get<DecimalField<Decimal32>>());
             case Types::Decimal64:  return f(field.template get<DecimalField<Decimal64>>());
             case Types::Decimal128: return f(field.template get<DecimalField<Decimal128>>());
@@ -713,6 +732,9 @@ class Field
             case Types::Map:
                 destroy<Map>();
                 break;
+            case Types::Object:
+                destroy<Object>();
+                break;
             case Types::AggregateFunctionState:
                 destroy<AggregateFunctionStateData>();
                 break;
@@ -737,26 +759,27 @@ class Field
 using Row = std::vector<Field>;
 
 
-template <> struct Field::TypeToEnum<Null> { static const Types::Which value = Types::Null; };
-template <> struct Field::TypeToEnum<UInt64>  { static const Types::Which value = Types::UInt64; };
-template <> struct Field::TypeToEnum<UInt128> { static const Types::Which value = Types::UInt128; };
-template <> struct Field::TypeToEnum<UInt256> { static const Types::Which value = Types::UInt256; };
-template <> struct Field::TypeToEnum<Int64>   { static const Types::Which value = Types::Int64; };
-template <> struct Field::TypeToEnum<Int128>  { static const Types::Which value = Types::Int128; };
-template <> struct Field::TypeToEnum<Int256>  { static const Types::Which value = Types::Int256; };
-template <> struct Field::TypeToEnum<UUID>    { static const Types::Which value = Types::UUID; };
-template <> struct Field::TypeToEnum<Float64> { static const Types::Which value = Types::Float64; };
-template <> struct Field::TypeToEnum<String>  { static const Types::Which value = Types::String; };
-template <> struct Field::TypeToEnum<Array>   { static const Types::Which value = Types::Array; };
-template <> struct Field::TypeToEnum<Tuple>   { static const Types::Which value = Types::Tuple; };
-template <> struct Field::TypeToEnum<Map>     { static const Types::Which value = Types::Map; };
-template <> struct Field::TypeToEnum<DecimalField<Decimal32>>{ static const Types::Which value = Types::Decimal32; };
-template <> struct Field::TypeToEnum<DecimalField<Decimal64>>{ static const Types::Which value = Types::Decimal64; };
-template <> struct Field::TypeToEnum<DecimalField<Decimal128>>{ static const Types::Which value = Types::Decimal128; };
-template <> struct Field::TypeToEnum<DecimalField<Decimal256>>{ static const Types::Which value = Types::Decimal256; };
-template <> struct Field::TypeToEnum<DecimalField<DateTime64>>{ static const Types::Which value = Types::Decimal64; };
-template <> struct Field::TypeToEnum<AggregateFunctionStateData>{ static const Types::Which value = Types::AggregateFunctionState; };
-template <> struct Field::TypeToEnum<bool>{ static const Types::Which value = Types::Bool; };
+template <> struct Field::TypeToEnum<Null> { static constexpr Types::Which value = Types::Null; };
+template <> struct Field::TypeToEnum<UInt64>  { static constexpr Types::Which value = Types::UInt64; };
+template <> struct Field::TypeToEnum<UInt128> { static constexpr Types::Which value = Types::UInt128; };
+template <> struct Field::TypeToEnum<UInt256> { static constexpr Types::Which value = Types::UInt256; };
+template <> struct Field::TypeToEnum<Int64>   { static constexpr Types::Which value = Types::Int64; };
+template <> struct Field::TypeToEnum<Int128>  { static constexpr Types::Which value = Types::Int128; };
+template <> struct Field::TypeToEnum<Int256>  { static constexpr Types::Which value = Types::Int256; };
+template <> struct Field::TypeToEnum<UUID>    { static constexpr Types::Which value = Types::UUID; };
+template <> struct Field::TypeToEnum<Float64> { static constexpr Types::Which value = Types::Float64; };
+template <> struct Field::TypeToEnum<String>  { static constexpr Types::Which value = Types::String; };
+template <> struct Field::TypeToEnum<Array>   { static constexpr Types::Which value = Types::Array; };
+template <> struct Field::TypeToEnum<Tuple>   { static constexpr Types::Which value = Types::Tuple; };
+template <> struct Field::TypeToEnum<Map>     { static constexpr Types::Which value = Types::Map; };
+template <> struct Field::TypeToEnum<Object>  { static constexpr Types::Which value = Types::Object; };
+template <> struct Field::TypeToEnum<DecimalField<Decimal32>>{ static constexpr Types::Which value = Types::Decimal32; };
+template <> struct Field::TypeToEnum<DecimalField<Decimal64>>{ static constexpr Types::Which value = Types::Decimal64; };
+template <> struct Field::TypeToEnum<DecimalField<Decimal128>>{ static constexpr Types::Which value = Types::Decimal128; };
+template <> struct Field::TypeToEnum<DecimalField<Decimal256>>{ static constexpr Types::Which value = Types::Decimal256; };
+template <> struct Field::TypeToEnum<DecimalField<DateTime64>>{ static constexpr Types::Which value = Types::Decimal64; };
+template <> struct Field::TypeToEnum<AggregateFunctionStateData>{ static constexpr Types::Which value = Types::AggregateFunctionState; };
+template <> struct Field::TypeToEnum<bool>{ static constexpr Types::Which value = Types::Bool; };
 
 template <> struct Field::EnumToType<Field::Types::Null>    { using Type = Null; };
 template <> struct Field::EnumToType<Field::Types::UInt64>  { using Type = UInt64; };
@@ -771,6 +794,7 @@ template <> struct Field::EnumToType<Field::Types::String>  { using Type = Strin
 template <> struct Field::EnumToType<Field::Types::Array>   { using Type = Array; };
 template <> struct Field::EnumToType<Field::Types::Tuple>   { using Type = Tuple; };
 template <> struct Field::EnumToType<Field::Types::Map>     { using Type = Map; };
+template <> struct Field::EnumToType<Field::Types::Object>  { using Type = Object; };
 template <> struct Field::EnumToType<Field::Types::Decimal32> { using Type = DecimalField<Decimal32>; };
 template <> struct Field::EnumToType<Field::Types::Decimal64> { using Type = DecimalField<Decimal64>; };
 template <> struct Field::EnumToType<Field::Types::Decimal128> { using Type = DecimalField<Decimal128>; };
@@ -931,34 +955,39 @@ class WriteBuffer;
 
 /// It is assumed that all elements of the array have the same type.
 void readBinary(Array & x, ReadBuffer & buf);
-
 [[noreturn]] inline void readText(Array &, ReadBuffer &) { throw Exception("Cannot read Array.", ErrorCodes::NOT_IMPLEMENTED); }
 [[noreturn]] inline void readQuoted(Array &, ReadBuffer &) { throw Exception("Cannot read Array.", ErrorCodes::NOT_IMPLEMENTED); }
 
 /// It is assumed that all elements of the array have the same type.
 /// Also write size and type into buf. UInt64 and Int64 is written in variadic size form
 void writeBinary(const Array & x, WriteBuffer & buf);
-
 void writeText(const Array & x, WriteBuffer & buf);
-
 [[noreturn]] inline void writeQuoted(const Array &, WriteBuffer &) { throw Exception("Cannot write Array quoted.", ErrorCodes::NOT_IMPLEMENTED); }
 
 void readBinary(Tuple & x, ReadBuffer & buf);
-
 [[noreturn]] inline void readText(Tuple &, ReadBuffer &) { throw Exception("Cannot read Tuple.", ErrorCodes::NOT_IMPLEMENTED); }
 [[noreturn]] inline void readQuoted(Tuple &, ReadBuffer &) { throw Exception("Cannot read Tuple.", ErrorCodes::NOT_IMPLEMENTED); }
 
 void writeBinary(const Tuple & x, WriteBuffer & buf);
-
 void writeText(const Tuple & x, WriteBuffer & buf);
+[[noreturn]] inline void writeQuoted(const Tuple &, WriteBuffer &) { throw Exception("Cannot write Tuple quoted.", ErrorCodes::NOT_IMPLEMENTED); }
 
 void readBinary(Map & x, ReadBuffer & buf);
 [[noreturn]] inline void readText(Map &, ReadBuffer &) { throw Exception("Cannot read Map.", ErrorCodes::NOT_IMPLEMENTED); }
 [[noreturn]] inline void readQuoted(Map &, ReadBuffer &) { throw Exception("Cannot read Map.", ErrorCodes::NOT_IMPLEMENTED); }
+
 void writeBinary(const Map & x, WriteBuffer & buf);
 void writeText(const Map & x, WriteBuffer & buf);
 [[noreturn]] inline void writeQuoted(const Map &, WriteBuffer &) { throw Exception("Cannot write Map quoted.", ErrorCodes::NOT_IMPLEMENTED); }
 
+void readBinary(Object & x, ReadBuffer & buf);
+[[noreturn]] inline void readText(Object &, ReadBuffer &) { throw Exception("Cannot read Object.", ErrorCodes::NOT_IMPLEMENTED); }
+[[noreturn]] inline void readQuoted(Object &, ReadBuffer &) { throw Exception("Cannot read Object.", ErrorCodes::NOT_IMPLEMENTED); }
+
+void writeBinary(const Object & x, WriteBuffer & buf);
+void writeText(const Object & x, WriteBuffer & buf);
+[[noreturn]] inline void writeQuoted(const Object &, WriteBuffer &) { throw Exception("Cannot write Object quoted.", ErrorCodes::NOT_IMPLEMENTED); }
+
 __attribute__ ((noreturn)) inline void writeText(const AggregateFunctionStateData &, WriteBuffer &)
 {
     // This probably doesn't make any sense, but we have to have it for
@@ -977,8 +1006,6 @@ void readQuoted(DecimalField<T> & x, ReadBuffer & buf);
 
 void writeFieldText(const Field & x, WriteBuffer & buf);
 
-[[noreturn]] inline void writeQuoted(const Tuple &, WriteBuffer &) { throw Exception("Cannot write Tuple quoted.", ErrorCodes::NOT_IMPLEMENTED); }
-
 String toString(const Field & x);
 
 }
diff --git a/src/Core/Settings.h b/src/Core/Settings.h
index 513effabffa9..3ff61379e39e 100644
--- a/src/Core/Settings.h
+++ b/src/Core/Settings.h
@@ -490,6 +490,7 @@ class IColumn;
     M(Bool, force_optimize_projection, false, "If projection optimization is enabled, SELECT queries need to use projection", 0) \
     M(Bool, async_socket_for_remote, true, "Asynchronously read from socket executing remote query", 0) \
     M(Bool, insert_null_as_default, true, "Insert DEFAULT values instead of NULL in INSERT SELECT (UNION ALL)", 0) \
+    M(Bool, describe_extend_object_types, false, "Deduce concrete type of columns of type Object in DESCRIBE query", 0) \
     M(Bool, describe_include_subcolumns, false, "If true, subcolumns of all table columns will be included into result of DESCRIBE query", 0) \
     \
     M(Bool, optimize_rewrite_sum_if_to_count_if, true, "Rewrite sumIf() and sum(if()) function countIf() function when logically equivalent", 0) \
@@ -566,6 +567,7 @@ class IColumn;
     /** Experimental functions */ \
     M(Bool, allow_experimental_funnel_functions, false, "Enable experimental functions for funnel analysis.", 0) \
     M(Bool, allow_experimental_nlp_functions, false, "Enable experimental functions for natural language processing.", 0) \
+    M(Bool, allow_experimental_object_type, false, "Allow Object and JSON data types", 0) \
     M(String, insert_deduplication_token, "", "If not empty, used for duplicate detection instead of data digest", 0) \
 // End of COMMON_SETTINGS
 // Please add settings related to formats into the FORMAT_FACTORY_SETTINGS and move obsolete settings to OBSOLETE_SETTINGS.
diff --git a/src/Core/Types.h b/src/Core/Types.h
index 9d3ff15d29cc..92546d7d07a6 100644
--- a/src/Core/Types.h
+++ b/src/Core/Types.h
@@ -87,6 +87,7 @@ enum class TypeIndex
     AggregateFunction,
     LowCardinality,
     Map,
+    Object,
 };
 #if !defined(__clang__)
 #pragma GCC diagnostic pop
diff --git a/src/Core/config_core.h.in b/src/Core/config_core.h.in
index 5d37f8cf361e..3fc2503aaa55 100644
--- a/src/Core/config_core.h.in
+++ b/src/Core/config_core.h.in
@@ -15,6 +15,8 @@
 #cmakedefine01 USE_NURAFT
 #cmakedefine01 USE_NLP
 #cmakedefine01 USE_KRB5
+#cmakedefine01 USE_SIMDJSON
+#cmakedefine01 USE_RAPIDJSON
 #cmakedefine01 USE_FILELOG
 #cmakedefine01 USE_ODBC
 #cmakedefine01 USE_REPLXX
diff --git a/src/DataTypes/CMakeLists.txt b/src/DataTypes/CMakeLists.txt
index a6176efc7f33..4a60d6c54cff 100644
--- a/src/DataTypes/CMakeLists.txt
+++ b/src/DataTypes/CMakeLists.txt
@@ -1,3 +1,5 @@
+add_subdirectory (Serializations)
+
 if (ENABLE_EXAMPLES)
-    add_subdirectory(examples)
+    add_subdirectory (examples)
 endif ()
diff --git a/src/DataTypes/DataTypeFactory.cpp b/src/DataTypes/DataTypeFactory.cpp
index 582b42accd99..ce501f4333dd 100644
--- a/src/DataTypes/DataTypeFactory.cpp
+++ b/src/DataTypes/DataTypeFactory.cpp
@@ -213,6 +213,7 @@ DataTypeFactory::DataTypeFactory()
     registerDataTypeDomainSimpleAggregateFunction(*this);
     registerDataTypeDomainGeo(*this);
     registerDataTypeMap(*this);
+    registerDataTypeObject(*this);
 }
 
 DataTypeFactory & DataTypeFactory::instance()
diff --git a/src/DataTypes/DataTypeFactory.h b/src/DataTypes/DataTypeFactory.h
index e7b638b6d7ba..704d8926bf0d 100644
--- a/src/DataTypes/DataTypeFactory.h
+++ b/src/DataTypes/DataTypeFactory.h
@@ -87,5 +87,6 @@ void registerDataTypeDomainIPv4AndIPv6(DataTypeFactory & factory);
 void registerDataTypeDomainBool(DataTypeFactory & factory);
 void registerDataTypeDomainSimpleAggregateFunction(DataTypeFactory & factory);
 void registerDataTypeDomainGeo(DataTypeFactory & factory);
+void registerDataTypeObject(DataTypeFactory & factory);
 
 }
diff --git a/src/DataTypes/DataTypeObject.cpp b/src/DataTypes/DataTypeObject.cpp
new file mode 100644
index 000000000000..9203c6764eae
--- /dev/null
+++ b/src/DataTypes/DataTypeObject.cpp
@@ -0,0 +1,83 @@
+#include <DataTypes/DataTypeObject.h>
+#include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/Serializations/SerializationObject.h>
+
+#include <Parsers/IAST.h>
+#include <Parsers/ASTLiteral.h>
+#include <Parsers/ASTFunction.h>
+#include <IO/Operators.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
+    extern const int UNEXPECTED_AST_STRUCTURE;
+}
+
+DataTypeObject::DataTypeObject(const String & schema_format_, bool is_nullable_)
+    : schema_format(Poco::toLower(schema_format_))
+    , is_nullable(is_nullable_)
+    , default_serialization(getObjectSerialization(schema_format))
+{
+}
+
+bool DataTypeObject::equals(const IDataType & rhs) const
+{
+    if (const auto * object = typeid_cast<const DataTypeObject *>(&rhs))
+        return schema_format == object->schema_format && is_nullable == object->is_nullable;
+    return false;
+}
+
+SerializationPtr DataTypeObject::doGetDefaultSerialization() const
+{
+    return default_serialization;
+}
+
+String DataTypeObject::doGetName() const
+{
+    WriteBufferFromOwnString out;
+    if (is_nullable)
+        out << "Object(Nullable(" << quote << schema_format << "))";
+    else
+        out << "Object(" << quote << schema_format << ")";
+    return out.str();
+}
+
+static DataTypePtr create(const ASTPtr & arguments)
+{
+    if (!arguments || arguments->children.size() != 1)
+        throw Exception(ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH,
+            "Object data type family must have one argument - name of schema format");
+
+    ASTPtr schema_argument = arguments->children[0];
+    bool is_nullable = false;
+
+    if (const auto * func = schema_argument->as<ASTFunction>())
+    {
+        if (func->name != "Nullable" || func->arguments->children.size() != 1)
+            throw Exception(ErrorCodes::UNEXPECTED_AST_STRUCTURE,
+                "Expected 'Nullable(<schema_name>)' as parameter for type Object", func->name);
+
+        schema_argument = func->arguments->children[0];
+        is_nullable = true;
+    }
+
+    const auto * literal = schema_argument->as<ASTLiteral>();
+    if (!literal || literal->value.getType() != Field::Types::String)
+        throw Exception(ErrorCodes::UNEXPECTED_AST_STRUCTURE,
+            "Object data type family must have a const string as its schema name parameter");
+
+    return std::make_shared<DataTypeObject>(literal->value.get<const String &>(), is_nullable);
+}
+
+void registerDataTypeObject(DataTypeFactory & factory)
+{
+    factory.registerDataType("Object", create);
+    factory.registerSimpleDataType("JSON",
+        [] { return std::make_shared<DataTypeObject>("JSON", false); },
+        DataTypeFactory::CaseInsensitive);
+}
+
+}
diff --git a/src/DataTypes/DataTypeObject.h b/src/DataTypes/DataTypeObject.h
new file mode 100644
index 000000000000..b4b31f0b8ead
--- /dev/null
+++ b/src/DataTypes/DataTypeObject.h
@@ -0,0 +1,46 @@
+#pragma once
+
+#include <DataTypes/IDataType.h>
+#include <Core/Field.h>
+#include <Columns/ColumnObject.h>
+
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int NOT_IMPLEMENTED;
+}
+
+class DataTypeObject : public IDataType
+{
+private:
+    String schema_format;
+    bool is_nullable;
+    SerializationPtr default_serialization;
+
+public:
+    DataTypeObject(const String & schema_format_, bool is_nullable_);
+
+    const char * getFamilyName() const override { return "Object"; }
+    String doGetName() const override;
+    TypeIndex getTypeId() const override { return TypeIndex::Object; }
+
+    MutableColumnPtr createColumn() const override { return ColumnObject::create(is_nullable); }
+
+    Field getDefault() const override
+    {
+        throw Exception("Method getDefault() is not implemented for data type " + getName(), ErrorCodes::NOT_IMPLEMENTED);
+    }
+
+    bool haveSubtypes() const override { return false; }
+    bool equals(const IDataType & rhs) const override;
+    bool isParametric() const override { return true; }
+
+    SerializationPtr doGetDefaultSerialization() const override;
+
+    bool hasNullableSubcolumns() const { return is_nullable; }
+};
+
+}
diff --git a/src/DataTypes/FieldToDataType.cpp b/src/DataTypes/FieldToDataType.cpp
index 8ca5ffac7c5b..283d1b1e41a6 100644
--- a/src/DataTypes/FieldToDataType.cpp
+++ b/src/DataTypes/FieldToDataType.cpp
@@ -1,6 +1,7 @@
 #include <DataTypes/FieldToDataType.h>
 #include <DataTypes/DataTypeTuple.h>
 #include <DataTypes/DataTypeMap.h>
+#include <DataTypes/DataTypeObject.h>
 #include <DataTypes/DataTypesNumber.h>
 #include <DataTypes/DataTypesDecimal.h>
 #include <DataTypes/DataTypeString.h>
@@ -108,12 +109,11 @@ DataTypePtr FieldToDataType::operator() (const Array & x) const
     element_types.reserve(x.size());
 
     for (const Field & elem : x)
-        element_types.emplace_back(applyVisitor(FieldToDataType(), elem));
+        element_types.emplace_back(applyVisitor(FieldToDataType(allow_convertion_to_string), elem));
 
-    return std::make_shared<DataTypeArray>(getLeastSupertype(element_types));
+    return std::make_shared<DataTypeArray>(getLeastSupertype(element_types, allow_convertion_to_string));
 }
 
-
 DataTypePtr FieldToDataType::operator() (const Tuple & tuple) const
 {
     if (tuple.empty())
@@ -123,7 +123,7 @@ DataTypePtr FieldToDataType::operator() (const Tuple & tuple) const
     element_types.reserve(tuple.size());
 
     for (const auto & element : tuple)
-        element_types.push_back(applyVisitor(FieldToDataType(), element));
+        element_types.push_back(applyVisitor(FieldToDataType(allow_convertion_to_string), element));
 
     return std::make_shared<DataTypeTuple>(element_types);
 }
@@ -139,11 +139,19 @@ DataTypePtr FieldToDataType::operator() (const Map & map) const
     {
         const auto & tuple = elem.safeGet<const Tuple &>();
         assert(tuple.size() == 2);
-        key_types.push_back(applyVisitor(FieldToDataType(), tuple[0]));
-        value_types.push_back(applyVisitor(FieldToDataType(), tuple[1]));
+        key_types.push_back(applyVisitor(FieldToDataType(allow_convertion_to_string), tuple[0]));
+        value_types.push_back(applyVisitor(FieldToDataType(allow_convertion_to_string), tuple[1]));
     }
 
-    return std::make_shared<DataTypeMap>(getLeastSupertype(key_types), getLeastSupertype(value_types));
+    return std::make_shared<DataTypeMap>(
+        getLeastSupertype(key_types, allow_convertion_to_string),
+        getLeastSupertype(value_types, allow_convertion_to_string));
+}
+
+DataTypePtr FieldToDataType::operator() (const Object &) const
+{
+    /// TODO: Do we need different parameters for type Object?
+    return std::make_shared<DataTypeObject>("json", false);
 }
 
 DataTypePtr FieldToDataType::operator() (const AggregateFunctionStateData & x) const
diff --git a/src/DataTypes/FieldToDataType.h b/src/DataTypes/FieldToDataType.h
index 72575c070f57..1922ac8b7467 100644
--- a/src/DataTypes/FieldToDataType.h
+++ b/src/DataTypes/FieldToDataType.h
@@ -20,26 +20,34 @@ using DataTypePtr = std::shared_ptr<const IDataType>;
 class FieldToDataType : public StaticVisitor<DataTypePtr>
 {
 public:
+    FieldToDataType(bool allow_convertion_to_string_ = false)
+      : allow_convertion_to_string(allow_convertion_to_string_)
+    {
+    }
+
     DataTypePtr operator() (const Null & x) const;
     DataTypePtr operator() (const UInt64 & x) const;
     DataTypePtr operator() (const UInt128 & x) const;
-    DataTypePtr operator() (const UInt256 & x) const;
     DataTypePtr operator() (const Int64 & x) const;
     DataTypePtr operator() (const Int128 & x) const;
-    DataTypePtr operator() (const Int256 & x) const;
     DataTypePtr operator() (const UUID & x) const;
     DataTypePtr operator() (const Float64 & x) const;
     DataTypePtr operator() (const String & x) const;
     DataTypePtr operator() (const Array & x) const;
     DataTypePtr operator() (const Tuple & tuple) const;
     DataTypePtr operator() (const Map & map) const;
+    DataTypePtr operator() (const Object & map) const;
     DataTypePtr operator() (const DecimalField<Decimal32> & x) const;
     DataTypePtr operator() (const DecimalField<Decimal64> & x) const;
     DataTypePtr operator() (const DecimalField<Decimal128> & x) const;
     DataTypePtr operator() (const DecimalField<Decimal256> & x) const;
     DataTypePtr operator() (const AggregateFunctionStateData & x) const;
+    DataTypePtr operator() (const UInt256 & x) const;
+    DataTypePtr operator() (const Int256 & x) const;
     DataTypePtr operator() (const bool & x) const;
+
+private:
+    bool allow_convertion_to_string;
 };
 
 }
-
diff --git a/src/DataTypes/IDataType.cpp b/src/DataTypes/IDataType.cpp
index edc9e4159f4c..0976233c031f 100644
--- a/src/DataTypes/IDataType.cpp
+++ b/src/DataTypes/IDataType.cpp
@@ -126,19 +126,25 @@ DataTypePtr IDataType::tryGetSubcolumnType(const String & subcolumn_name) const
 DataTypePtr IDataType::getSubcolumnType(const String & subcolumn_name) const
 {
     SubstreamData data = { getDefaultSerialization(), getPtr(), nullptr, nullptr };
-    return getForSubcolumn<DataTypePtr>(subcolumn_name, data, &SubstreamData::type);
+    return getForSubcolumn<DataTypePtr>(subcolumn_name, data, &SubstreamData::type, true);
 }
 
-SerializationPtr IDataType::getSubcolumnSerialization(const String & subcolumn_name, const SerializationPtr & serialization) const
+ColumnPtr IDataType::tryGetSubcolumn(const String & subcolumn_name, const ColumnPtr & column) const
 {
-    SubstreamData data = { serialization, nullptr, nullptr, nullptr };
-    return getForSubcolumn<SerializationPtr>(subcolumn_name, data, &SubstreamData::serialization);
+    SubstreamData data = { getDefaultSerialization(), nullptr, column, nullptr };
+    return getForSubcolumn<ColumnPtr>(subcolumn_name, data, &SubstreamData::column, false);
 }
 
 ColumnPtr IDataType::getSubcolumn(const String & subcolumn_name, const ColumnPtr & column) const
 {
     SubstreamData data = { getDefaultSerialization(), nullptr, column, nullptr };
-    return getForSubcolumn<ColumnPtr>(subcolumn_name, data, &SubstreamData::column);
+    return getForSubcolumn<ColumnPtr>(subcolumn_name, data, &SubstreamData::column, true);
+}
+
+SerializationPtr IDataType::getSubcolumnSerialization(const String & subcolumn_name, const SerializationPtr & serialization) const
+{
+    SubstreamData data = { serialization, nullptr, nullptr, nullptr };
+    return getForSubcolumn<SerializationPtr>(subcolumn_name, data, &SubstreamData::serialization, true);
 }
 
 Names IDataType::getSubcolumnNames() const
diff --git a/src/DataTypes/IDataType.h b/src/DataTypes/IDataType.h
index 36e1ce8ddd5a..fc9e50dc55ba 100644
--- a/src/DataTypes/IDataType.h
+++ b/src/DataTypes/IDataType.h
@@ -82,9 +82,11 @@ class IDataType : private boost::noncopyable, public std::enable_shared_from_thi
     DataTypePtr tryGetSubcolumnType(const String & subcolumn_name) const;
     DataTypePtr getSubcolumnType(const String & subcolumn_name) const;
 
-    SerializationPtr getSubcolumnSerialization(const String & subcolumn_name, const SerializationPtr & serialization) const;
+    ColumnPtr tryGetSubcolumn(const String & subcolumn_name, const ColumnPtr & column) const;
     ColumnPtr getSubcolumn(const String & subcolumn_name, const ColumnPtr & column) const;
 
+    SerializationPtr getSubcolumnSerialization(const String & subcolumn_name, const SerializationPtr & serialization) const;
+
     using SubstreamData = ISerialization::SubstreamData;
     using SubstreamPath = ISerialization::SubstreamPath;
 
@@ -309,7 +311,7 @@ class IDataType : private boost::noncopyable, public std::enable_shared_from_thi
         const String & subcolumn_name,
         const SubstreamData & data,
         Ptr SubstreamData::*member,
-        bool throw_if_null = true) const;
+        bool throw_if_null) const;
 };
 
 
@@ -373,11 +375,13 @@ struct WhichDataType
     constexpr bool isMap() const {return idx == TypeIndex::Map; }
     constexpr bool isSet() const { return idx == TypeIndex::Set; }
     constexpr bool isInterval() const { return idx == TypeIndex::Interval; }
+    constexpr bool isObject() const { return idx == TypeIndex::Object; }
 
     constexpr bool isNothing() const { return idx == TypeIndex::Nothing; }
     constexpr bool isNullable() const { return idx == TypeIndex::Nullable; }
     constexpr bool isFunction() const { return idx == TypeIndex::Function; }
     constexpr bool isAggregateFunction() const { return idx == TypeIndex::AggregateFunction; }
+    constexpr bool isSimple() const  { return isInt() || isUInt() || isFloat() || isString(); }
 
     constexpr bool isLowCarnality() const { return idx == TypeIndex::LowCardinality; }
 };
@@ -399,10 +403,16 @@ inline bool isEnum(const DataTypePtr & data_type) { return WhichDataType(data_ty
 inline bool isDecimal(const DataTypePtr & data_type) { return WhichDataType(data_type).isDecimal(); }
 inline bool isTuple(const DataTypePtr & data_type) { return WhichDataType(data_type).isTuple(); }
 inline bool isArray(const DataTypePtr & data_type) { return WhichDataType(data_type).isArray(); }
-inline bool isMap(const DataTypePtr & data_type) { return WhichDataType(data_type).isMap(); }
+inline bool isMap(const DataTypePtr & data_type) {return WhichDataType(data_type).isMap(); }
 inline bool isNothing(const DataTypePtr & data_type) { return WhichDataType(data_type).isNothing(); }
 inline bool isUUID(const DataTypePtr & data_type) { return WhichDataType(data_type).isUUID(); }
 
+template <typename T>
+inline bool isObject(const T & data_type)
+{
+    return WhichDataType(data_type).isObject();
+}
+
 template <typename T>
 inline bool isUInt8(const T & data_type)
 {
diff --git a/src/DataTypes/NestedUtils.cpp b/src/DataTypes/NestedUtils.cpp
index b35a07135191..df504bc34a83 100644
--- a/src/DataTypes/NestedUtils.cpp
+++ b/src/DataTypes/NestedUtils.cpp
@@ -30,6 +30,12 @@ namespace Nested
 
 std::string concatenateName(const std::string & nested_table_name, const std::string & nested_field_name)
 {
+    if (nested_table_name.empty())
+        return nested_field_name;
+
+    if (nested_field_name.empty())
+        return nested_table_name;
+
     return nested_table_name + "." + nested_field_name;
 }
 
diff --git a/src/DataTypes/ObjectUtils.cpp b/src/DataTypes/ObjectUtils.cpp
new file mode 100644
index 000000000000..9004a5296e02
--- /dev/null
+++ b/src/DataTypes/ObjectUtils.cpp
@@ -0,0 +1,703 @@
+#include <DataTypes/ObjectUtils.h>
+#include <DataTypes/DataTypeObject.h>
+#include <DataTypes/DataTypeNothing.h>
+#include <DataTypes/DataTypeArray.h>
+#include <DataTypes/DataTypeNullable.h>
+#include <DataTypes/DataTypesNumber.h>
+#include <DataTypes/DataTypeNested.h>
+#include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/getLeastSupertype.h>
+#include <DataTypes/NestedUtils.h>
+#include <Columns/ColumnObject.h>
+#include <Columns/ColumnTuple.h>
+#include <Columns/ColumnArray.h>
+#include <Columns/ColumnNullable.h>
+#include <Parsers/ASTSelectQuery.h>
+#include <Parsers/ASTExpressionList.h>
+#include <Parsers/ASTLiteral.h>
+#include <Parsers/ASTFunction.h>
+#include <IO/Operators.h>
+
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int TYPE_MISMATCH;
+    extern const int LOGICAL_ERROR;
+    extern const int DUPLICATE_COLUMN;
+}
+
+size_t getNumberOfDimensions(const IDataType & type)
+{
+    if (const auto * type_array = typeid_cast<const DataTypeArray *>(&type))
+        return type_array->getNumberOfDimensions();
+    return 0;
+}
+
+size_t getNumberOfDimensions(const IColumn & column)
+{
+    if (const auto * column_array = checkAndGetColumn<ColumnArray>(column))
+        return column_array->getNumberOfDimensions();
+    return 0;
+}
+
+DataTypePtr getBaseTypeOfArray(const DataTypePtr & type)
+{
+    /// Get raw pointers to avoid extra copying of type pointers.
+    const DataTypeArray * last_array = nullptr;
+    const auto * current_type = type.get();
+    while (const auto * type_array = typeid_cast<const DataTypeArray *>(current_type))
+    {
+        current_type = type_array->getNestedType().get();
+        last_array = type_array;
+    }
+
+    return last_array ? last_array->getNestedType() : type;
+}
+
+ColumnPtr getBaseColumnOfArray(const ColumnPtr & column)
+{
+    /// Get raw pointers to avoid extra copying of column pointers.
+    const ColumnArray * last_array = nullptr;
+    const auto * current_column = column.get();
+    while (const auto * column_array = checkAndGetColumn<ColumnArray>(current_column))
+    {
+        current_column = &column_array->getData();
+        last_array = column_array;
+    }
+
+    return last_array ? last_array->getDataPtr() : column;
+}
+
+DataTypePtr createArrayOfType(DataTypePtr type, size_t num_dimensions)
+{
+    for (size_t i = 0; i < num_dimensions; ++i)
+        type = std::make_shared<DataTypeArray>(std::move(type));
+    return type;
+}
+
+ColumnPtr createArrayOfColumn(ColumnPtr column, size_t num_dimensions)
+{
+    for (size_t i = 0; i < num_dimensions; ++i)
+        column = ColumnArray::create(column);
+    return column;
+}
+
+Array createEmptyArrayField(size_t num_dimensions)
+{
+    if (num_dimensions == 0)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot create array field with 0 dimensions");
+
+    Array array;
+    Array * current_array = &array;
+    for (size_t i = 1; i < num_dimensions; ++i)
+    {
+        current_array->push_back(Array());
+        current_array = &current_array->back().get<Array &>();
+    }
+
+    return array;
+}
+
+DataTypePtr getDataTypeByColumn(const IColumn & column)
+{
+    auto idx = column.getDataType();
+    if (WhichDataType(idx).isSimple())
+        return DataTypeFactory::instance().get(String(magic_enum::enum_name(idx)));
+
+    if (const auto * column_array = checkAndGetColumn<ColumnArray>(&column))
+        return std::make_shared<DataTypeArray>(getDataTypeByColumn(column_array->getData()));
+
+    if (const auto * column_nullable = checkAndGetColumn<ColumnNullable>(&column))
+        return makeNullable(getDataTypeByColumn(column_nullable->getNestedColumn()));
+
+    /// TODO: add more types.
+    throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot get data type of column {}", column.getFamilyName());
+}
+
+template <size_t I, typename Tuple>
+static auto extractVector(const std::vector<Tuple> & vec)
+{
+    static_assert(I < std::tuple_size_v<Tuple>);
+    std::vector<std::tuple_element_t<I, Tuple>> res;
+    res.reserve(vec.size());
+    for (const auto & elem : vec)
+        res.emplace_back(std::get<I>(elem));
+    return res;
+}
+
+void convertObjectsToTuples(NamesAndTypesList & columns_list, Block & block, const NamesAndTypesList & extended_storage_columns)
+{
+    std::unordered_map<String, DataTypePtr> storage_columns_map;
+    for (const auto & [name, type] : extended_storage_columns)
+        storage_columns_map[name] = type;
+
+    for (auto & name_type : columns_list)
+    {
+        if (!isObject(name_type.type))
+            continue;
+
+        auto & column = block.getByName(name_type.name);
+        if (!isObject(column.type))
+            throw Exception(ErrorCodes::TYPE_MISMATCH,
+                "Type for column '{}' mismatch in columns list and in block. In list: {}, in block: {}",
+                name_type.name, name_type.type->getName(), column.type->getName());
+
+        const auto & column_object = assert_cast<const ColumnObject &>(*column.column);
+        const auto & subcolumns = column_object.getSubcolumns();
+
+        if (!column_object.isFinalized())
+            throw Exception(ErrorCodes::LOGICAL_ERROR,
+                "Cannot convert to tuple column '{}' from type {}. Column should be finalized first",
+                name_type.name, name_type.type->getName());
+
+        PathsInData tuple_paths;
+        DataTypes tuple_types;
+        Columns tuple_columns;
+
+        for (const auto & entry : subcolumns)
+        {
+            tuple_paths.emplace_back(entry->path);
+            tuple_types.emplace_back(entry->data.getLeastCommonType());
+            tuple_columns.emplace_back(entry->data.getFinalizedColumnPtr());
+        }
+
+        auto it = storage_columns_map.find(name_type.name);
+        if (it == storage_columns_map.end())
+            throw Exception(ErrorCodes::LOGICAL_ERROR, "Column '{}' not found in storage", name_type.name);
+
+        std::tie(column.column, column.type) = unflattenTuple(tuple_paths, tuple_types, tuple_columns);
+        name_type.type = column.type;
+
+        /// Check that constructed Tuple type and type in storage are compatible.
+        getLeastCommonTypeForObject({column.type, it->second}, true);
+    }
+}
+
+static bool isPrefix(const PathInData::Parts & prefix, const PathInData::Parts & parts)
+{
+    if (prefix.size() > parts.size())
+        return false;
+
+    for (size_t i = 0; i < prefix.size(); ++i)
+        if (prefix[i].key != parts[i].key)
+            return false;
+    return true;
+}
+
+void checkObjectHasNoAmbiguosPaths(const PathsInData & paths)
+{
+    size_t size = paths.size();
+    for (size_t i = 0; i < size; ++i)
+    {
+        for (size_t j = 0; j < i; ++j)
+        {
+            if (isPrefix(paths[i].getParts(), paths[j].getParts())
+                || isPrefix(paths[j].getParts(), paths[i].getParts()))
+                throw Exception(ErrorCodes::DUPLICATE_COLUMN,
+                    "Data in Object has ambiguous paths: '{}' and '{}'",
+                    paths[i].getPath(), paths[j].getPath());
+        }
+    }
+}
+
+DataTypePtr getLeastCommonTypeForObject(const DataTypes & types, bool check_ambiguos_paths)
+{
+    if (types.empty())
+        return nullptr;
+
+    bool all_equal = true;
+    for (size_t i = 1; i < types.size(); ++i)
+    {
+        if (!types[i]->equals(*types[0]))
+        {
+            all_equal = false;
+            break;
+        }
+    }
+
+    if (all_equal)
+        return types[0];
+
+    /// Types of subcolumns by path from all tuples.
+    std::unordered_map<PathInData, DataTypes, PathInData::Hash> subcolumns_types;
+
+    /// First we flatten tuples, then get common type for paths
+    /// and finally unflatten paths and create new tuple type.
+    for (const auto & type : types)
+    {
+        const auto * type_tuple = typeid_cast<const DataTypeTuple *>(type.get());
+        if (!type_tuple)
+            throw Exception(ErrorCodes::LOGICAL_ERROR,
+                "Least common type for object can be deduced only from tuples, but {} given", type->getName());
+
+        auto [tuple_paths, tuple_types] = flattenTuple(type);
+        assert(tuple_paths.size() == tuple_types.size());
+
+        for (size_t i = 0; i < tuple_paths.size(); ++i)
+            subcolumns_types[tuple_paths[i]].push_back(tuple_types[i]);
+    }
+
+    PathsInData tuple_paths;
+    DataTypes tuple_types;
+
+    /// Get the least common type for all paths.
+    for (const auto & [key, subtypes] : subcolumns_types)
+    {
+        assert(!subtypes.empty());
+        if (key.getPath() == ColumnObject::COLUMN_NAME_DUMMY)
+            continue;
+
+        size_t first_dim = getNumberOfDimensions(*subtypes[0]);
+        for (size_t i = 1; i < subtypes.size(); ++i)
+            if (first_dim != getNumberOfDimensions(*subtypes[i]))
+                throw Exception(ErrorCodes::TYPE_MISMATCH,
+                    "Uncompatible types of subcolumn '{}': {} and {}",
+                    key.getPath(), subtypes[0]->getName(), subtypes[i]->getName());
+
+        tuple_paths.emplace_back(key);
+        tuple_types.emplace_back(getLeastSupertype(subtypes, /*allow_conversion_to_string=*/ true));
+    }
+
+    if (tuple_paths.empty())
+    {
+        tuple_paths.emplace_back(ColumnObject::COLUMN_NAME_DUMMY);
+        tuple_types.emplace_back(std::make_shared<DataTypeUInt8>());
+    }
+
+    if (check_ambiguos_paths)
+        checkObjectHasNoAmbiguosPaths(tuple_paths);
+
+    return unflattenTuple(tuple_paths, tuple_types);
+}
+
+NameSet getNamesOfObjectColumns(const NamesAndTypesList & columns_list)
+{
+    NameSet res;
+    for (const auto & [name, type] : columns_list)
+        if (isObject(type))
+            res.insert(name);
+
+    return res;
+}
+
+bool hasObjectColumns(const ColumnsDescription & columns)
+{
+    return std::any_of(columns.begin(), columns.end(), [](const auto & column) { return isObject(column.type); });
+}
+
+void extendObjectColumns(NamesAndTypesList & columns_list, const ColumnsDescription & object_columns, bool with_subcolumns)
+{
+    NamesAndTypesList subcolumns_list;
+    for (auto & column : columns_list)
+    {
+        auto object_column = object_columns.tryGetColumn(GetColumnsOptions::All, column.name);
+        if (object_column)
+        {
+            column.type = object_column->type;
+
+            if (with_subcolumns)
+                subcolumns_list.splice(subcolumns_list.end(), object_columns.getSubcolumns(column.name));
+        }
+    }
+
+    columns_list.splice(columns_list.end(), std::move(subcolumns_list));
+}
+
+void updateObjectColumns(ColumnsDescription & object_columns, const NamesAndTypesList & new_columns)
+{
+    for (const auto & new_column : new_columns)
+    {
+        auto object_column = object_columns.tryGetColumn(GetColumnsOptions::All, new_column.name);
+        if (object_column && !object_column->type->equals(*new_column.type))
+        {
+            object_columns.modify(new_column.name, [&](auto & column)
+            {
+                column.type = getLeastCommonTypeForObject({object_column->type, new_column.type});
+            });
+        }
+    }
+}
+
+namespace
+{
+
+void flattenTupleImpl(
+    PathInDataBuilder & builder,
+    DataTypePtr type,
+    std::vector<PathInData::Parts> & new_paths,
+    DataTypes & new_types)
+{
+    if (const auto * type_tuple = typeid_cast<const DataTypeTuple *>(type.get()))
+    {
+        const auto & tuple_names = type_tuple->getElementNames();
+        const auto & tuple_types = type_tuple->getElements();
+
+        for (size_t i = 0; i < tuple_names.size(); ++i)
+        {
+            builder.append(tuple_names[i], false);
+            flattenTupleImpl(builder, tuple_types[i], new_paths, new_types);
+            builder.popBack();
+        }
+    }
+    else if (const auto * type_array = typeid_cast<const DataTypeArray *>(type.get()))
+    {
+        PathInDataBuilder element_builder;
+        std::vector<PathInData::Parts> element_paths;
+        DataTypes element_types;
+
+        flattenTupleImpl(element_builder, type_array->getNestedType(), element_paths, element_types);
+        assert(element_paths.size() == element_types.size());
+
+        for (size_t i = 0; i < element_paths.size(); ++i)
+        {
+            builder.append(element_paths[i], true);
+            new_paths.emplace_back(builder.getParts());
+            new_types.emplace_back(std::make_shared<DataTypeArray>(element_types[i]));
+            builder.popBack(element_paths[i].size());
+        }
+    }
+    else
+    {
+        new_paths.emplace_back(builder.getParts());
+        new_types.emplace_back(type);
+    }
+}
+
+/// @offsets_columns are used as stack of array offsets and allows to recreate Array columns.
+void flattenTupleImpl(const ColumnPtr & column, Columns & new_columns, Columns & offsets_columns)
+{
+    if (const auto * column_tuple = checkAndGetColumn<ColumnTuple>(column.get()))
+    {
+        const auto & subcolumns = column_tuple->getColumns();
+        for (const auto & subcolumn : subcolumns)
+            flattenTupleImpl(subcolumn, new_columns, offsets_columns);
+    }
+    else if (const auto * column_array = checkAndGetColumn<ColumnArray>(column.get()))
+    {
+        offsets_columns.push_back(column_array->getOffsetsPtr());
+        flattenTupleImpl(column_array->getDataPtr(), new_columns, offsets_columns);
+        offsets_columns.pop_back();
+    }
+    else
+    {
+        if (!offsets_columns.empty())
+        {
+            auto new_column = ColumnArray::create(column, offsets_columns.back());
+            for (auto it = offsets_columns.rbegin() + 1; it != offsets_columns.rend(); ++it)
+                new_column = ColumnArray::create(new_column, *it);
+
+            new_columns.push_back(std::move(new_column));
+        }
+        else
+        {
+            new_columns.push_back(column);
+        }
+    }
+}
+
+DataTypePtr reduceNumberOfDimensions(DataTypePtr type, size_t dimensions_to_reduce)
+{
+    while (dimensions_to_reduce--)
+    {
+        const auto * type_array = typeid_cast<const DataTypeArray *>(type.get());
+        if (!type_array)
+            throw Exception(ErrorCodes::LOGICAL_ERROR, "Not enough dimensions to reduce");
+
+        type = type_array->getNestedType();
+    }
+
+    return type;
+}
+
+ColumnPtr reduceNumberOfDimensions(ColumnPtr column, size_t dimensions_to_reduce)
+{
+    while (dimensions_to_reduce--)
+    {
+        const auto * column_array = typeid_cast<const ColumnArray *>(column.get());
+        if (!column_array)
+            throw Exception(ErrorCodes::LOGICAL_ERROR, "Not enough dimensions to reduce");
+
+        column = column_array->getDataPtr();
+    }
+
+    return column;
+}
+
+/// We save intermediate column, type and number of array
+/// dimensions for each intermediate node in path in subcolumns tree.
+struct ColumnWithTypeAndDimensions
+{
+    ColumnPtr column;
+    DataTypePtr type;
+    size_t array_dimensions;
+};
+
+using SubcolumnsTreeWithColumns = SubcolumnsTree<ColumnWithTypeAndDimensions>;
+using Node = SubcolumnsTreeWithColumns::Node;
+
+/// Creates data type and column from tree of subcolumns.
+ColumnWithTypeAndDimensions createTypeFromNode(const Node * node)
+{
+    auto collect_tuple_elemets = [](const auto & children)
+    {
+        std::vector<std::tuple<String, ColumnWithTypeAndDimensions>> tuple_elements;
+        tuple_elements.reserve(children.size());
+        for (const auto & [name, child] : children)
+        {
+            auto column = createTypeFromNode(child.get());
+            tuple_elements.emplace_back(name, std::move(column));
+        }
+
+        /// Sort to always create the same type for the same set of subcolumns.
+        std::sort(tuple_elements.begin(), tuple_elements.end(),
+            [](const auto & lhs, const auto & rhs) { return std::get<0>(lhs) < std::get<0>(rhs); });
+
+        auto tuple_names = extractVector<0>(tuple_elements);
+        auto tuple_columns = extractVector<1>(tuple_elements);
+
+        return std::make_tuple(std::move(tuple_names), std::move(tuple_columns));
+    };
+
+    if (node->kind == Node::SCALAR)
+    {
+        return node->data;
+    }
+    else if (node->kind == Node::NESTED)
+    {
+        auto [tuple_names, tuple_columns] = collect_tuple_elemets(node->children);
+
+        Columns offsets_columns;
+        offsets_columns.reserve(tuple_columns[0].array_dimensions + 1);
+
+        /// If we have a Nested node and child node with anonymous array levels
+        /// we need to push a Nested type through all array levels.
+        /// Example: { "k1": [[{"k2": 1, "k3": 2}] } should be parsed as
+        /// `k1 Array(Nested(k2 Int, k3 Int))` and k1 is marked as Nested
+        /// and `k2` and `k3` has anonymous_array_level = 1 in that case.
+
+        const auto & current_array = assert_cast<const ColumnArray &>(*node->data.column);
+        offsets_columns.push_back(current_array.getOffsetsPtr());
+
+        auto first_column = tuple_columns[0].column;
+        for (size_t i = 0; i < tuple_columns[0].array_dimensions; ++i)
+        {
+            const auto & column_array = assert_cast<const ColumnArray &>(*first_column);
+            offsets_columns.push_back(column_array.getOffsetsPtr());
+            first_column = column_array.getDataPtr();
+        }
+
+        size_t num_elements = tuple_columns.size();
+        Columns tuple_elements_columns(num_elements);
+        DataTypes tuple_elements_types(num_elements);
+
+        /// Reduce extra array dimensions to get columns and types of Nested elements.
+        for (size_t i = 0; i < num_elements; ++i)
+        {
+            assert(tuple_columns[i].array_dimensions == tuple_columns[0].array_dimensions);
+            tuple_elements_columns[i] = reduceNumberOfDimensions(tuple_columns[i].column, tuple_columns[i].array_dimensions);
+            tuple_elements_types[i] = reduceNumberOfDimensions(tuple_columns[i].type, tuple_columns[i].array_dimensions);
+        }
+
+        auto result_column = ColumnArray::create(ColumnTuple::create(tuple_elements_columns), offsets_columns.back());
+        auto result_type = createNested(tuple_elements_types, tuple_names);
+
+        /// Recreate result Array type and Array column.
+        for (auto it = offsets_columns.rbegin() + 1; it != offsets_columns.rend(); ++it)
+        {
+            result_column = ColumnArray::create(result_column, *it);
+            result_type = std::make_shared<DataTypeArray>(result_type);
+        }
+
+        return {result_column, result_type, tuple_columns[0].array_dimensions};
+    }
+    else
+    {
+        auto [tuple_names, tuple_columns] = collect_tuple_elemets(node->children);
+
+        size_t num_elements = tuple_columns.size();
+        Columns tuple_elements_columns(num_elements);
+        DataTypes tuple_elements_types(num_elements);
+
+        for (size_t i = 0; i < tuple_columns.size(); ++i)
+        {
+            assert(tuple_columns[i].array_dimensions == tuple_columns[0].array_dimensions);
+            tuple_elements_columns[i] = tuple_columns[i].column;
+            tuple_elements_types[i] = tuple_columns[i].type;
+        }
+
+        auto result_column = ColumnTuple::create(tuple_elements_columns);
+        auto result_type = std::make_shared<DataTypeTuple>(tuple_elements_types, tuple_names);
+
+        return {result_column, result_type, tuple_columns[0].array_dimensions};
+    }
+}
+
+}
+
+std::pair<PathsInData, DataTypes> flattenTuple(const DataTypePtr & type)
+{
+    std::vector<PathInData::Parts> new_path_parts;
+    DataTypes new_types;
+    PathInDataBuilder builder;
+
+    flattenTupleImpl(builder, type, new_path_parts, new_types);
+
+    PathsInData new_paths(new_path_parts.begin(), new_path_parts.end());
+    return {new_paths, new_types};
+}
+
+ColumnPtr flattenTuple(const ColumnPtr & column)
+{
+    Columns new_columns;
+    Columns offsets_columns;
+
+    flattenTupleImpl(column, new_columns, offsets_columns);
+    return ColumnTuple::create(new_columns);
+}
+
+DataTypePtr unflattenTuple(const PathsInData & paths, const DataTypes & tuple_types)
+{
+    assert(paths.size() == tuple_types.size());
+    Columns tuple_columns;
+    tuple_columns.reserve(tuple_types.size());
+    for (const auto & type : tuple_types)
+        tuple_columns.emplace_back(type->createColumn());
+
+    return unflattenTuple(paths, tuple_types, tuple_columns).second;
+}
+
+std::pair<ColumnPtr, DataTypePtr> unflattenTuple(
+    const PathsInData & paths,
+    const DataTypes & tuple_types,
+    const Columns & tuple_columns)
+{
+    assert(paths.size() == tuple_types.size());
+    assert(paths.size() == tuple_columns.size());
+
+    /// We add all paths to the subcolumn tree and then create a type from it.
+    /// The tree stores column, type and number of array dimensions
+    /// for each intermediate node.
+    SubcolumnsTreeWithColumns tree;
+
+    for (size_t i = 0; i < paths.size(); ++i)
+    {
+        auto column = tuple_columns[i];
+        auto type = tuple_types[i];
+
+        const auto & parts = paths[i].getParts();
+        size_t num_parts = parts.size();
+
+        size_t pos = 0;
+        tree.add(paths[i], [&](Node::Kind kind, bool exists) -> std::shared_ptr<Node>
+            {
+                if (pos >= num_parts)
+                    throw Exception(ErrorCodes::LOGICAL_ERROR,
+                        "Not enough name parts for path {}. Expected at least {}, got {}",
+                            paths[i].getPath(), pos + 1, num_parts);
+
+                size_t array_dimensions = kind == Node::NESTED ? 1 : parts[pos].anonymous_array_level;
+                ColumnWithTypeAndDimensions current_column{column, type, array_dimensions};
+
+                /// Get type and column for next node.
+                if (array_dimensions)
+                {
+                    type = reduceNumberOfDimensions(type, array_dimensions);
+                    column = reduceNumberOfDimensions(column, array_dimensions);
+                }
+
+                ++pos;
+                if (exists)
+                    return nullptr;
+
+                return kind == Node::SCALAR
+                    ? std::make_shared<Node>(kind, current_column, paths[i])
+                    : std::make_shared<Node>(kind, current_column);
+            });
+    }
+
+    auto [column, type, _] = createTypeFromNode(tree.getRoot());
+    return std::make_pair(std::move(column), std::move(type));
+}
+
+static void addConstantToWithClause(const ASTPtr & query, const String & column_name, const DataTypePtr & data_type)
+{
+    auto & select = query->as<ASTSelectQuery &>();
+    if (!select.with())
+        select.setExpression(ASTSelectQuery::Expression::WITH, std::make_shared<ASTExpressionList>());
+
+    /// TODO: avoid materialize
+    auto node = makeASTFunction("materialize",
+        makeASTFunction("CAST",
+            std::make_shared<ASTLiteral>(data_type->getDefault()),
+            std::make_shared<ASTLiteral>(data_type->getName())));
+
+    node->alias = column_name;
+    node->prefer_alias_to_column_name = true;
+    select.with()->children.push_back(std::move(node));
+}
+
+/// @expected_columns and @available_columns contain descriptions
+/// of extended Object columns.
+void replaceMissedSubcolumnsByConstants(
+    const ColumnsDescription & expected_columns,
+    const ColumnsDescription & available_columns,
+    ASTPtr query)
+{
+    NamesAndTypes missed_names_types;
+
+    /// Find all subcolumns that are in @expected_columns, but not in @available_columns.
+    for (const auto & column : available_columns)
+    {
+        auto expected_column = expected_columns.getColumn(GetColumnsOptions::All, column.name);
+
+        /// Extract all paths from both descriptions to easily check existence of subcolumns.
+        auto [available_paths, available_types] = flattenTuple(column.type);
+        auto [expected_paths, expected_types] = flattenTuple(expected_column.type);
+
+        auto extract_names_and_types = [&column](const auto & paths, const auto & types)
+        {
+            NamesAndTypes res;
+            res.reserve(paths.size());
+            for (size_t i = 0; i < paths.size(); ++i)
+            {
+                auto full_name = Nested::concatenateName(column.name, paths[i].getPath());
+                res.emplace_back(full_name, types[i]);
+            }
+
+            std::sort(res.begin(), res.end());
+            return res;
+        };
+
+        auto available_names_types = extract_names_and_types(available_paths, available_types);
+        auto expected_names_types = extract_names_and_types(expected_paths, expected_types);
+
+        std::set_difference(
+            expected_names_types.begin(), expected_names_types.end(),
+            available_names_types.begin(), available_names_types.end(),
+            std::back_inserter(missed_names_types),
+            [](const auto & lhs, const auto & rhs) { return lhs.name < rhs.name; });
+    }
+
+    if (missed_names_types.empty())
+        return;
+
+    IdentifierNameSet identifiers;
+    query->collectIdentifierNames(identifiers);
+
+    /// Replace missed subcolumns to default literals of theirs type.
+    for (const auto & [name, type] : missed_names_types)
+        if (identifiers.count(name))
+            addConstantToWithClause(query, name, type);
+}
+
+void finalizeObjectColumns(MutableColumns & columns)
+{
+    for (auto & column : columns)
+        if (auto * column_object = typeid_cast<ColumnObject *>(column.get()))
+            column_object->finalize();
+}
+
+}
diff --git a/src/DataTypes/ObjectUtils.h b/src/DataTypes/ObjectUtils.h
new file mode 100644
index 000000000000..199a048c8cd6
--- /dev/null
+++ b/src/DataTypes/ObjectUtils.h
@@ -0,0 +1,140 @@
+#pragma once
+
+#include <Core/Block.h>
+#include <Core/NamesAndTypes.h>
+#include <Common/FieldVisitors.h>
+#include <Storages/ColumnsDescription.h>
+#include <DataTypes/DataTypeTuple.h>
+#include <DataTypes/Serializations/JSONDataParser.h>
+#include <DataTypes/DataTypesNumber.h>
+#include <Columns/ColumnObject.h>
+
+namespace DB
+{
+
+/// Returns number of dimensions in Array type. 0 if type is not array.
+size_t getNumberOfDimensions(const IDataType & type);
+
+/// Returns number of dimensions in Array column. 0 if column is not array.
+size_t getNumberOfDimensions(const IColumn & column);
+
+/// Returns type of scalars of Array of arbitrary dimensions.
+DataTypePtr getBaseTypeOfArray(const DataTypePtr & type);
+
+/// Returns Array type with requested scalar type and number of dimensions.
+DataTypePtr createArrayOfType(DataTypePtr type, size_t num_dimensions);
+
+/// Returns column of scalars of Array of arbitrary dimensions.
+ColumnPtr getBaseColumnOfArray(const ColumnPtr & column);
+
+/// Returns empty Array column with requested scalar column and number of dimensions.
+ColumnPtr createArrayOfColumn(const ColumnPtr & column, size_t num_dimensions);
+
+/// Returns Array with requested number of dimensions and no scalars.
+Array createEmptyArrayField(size_t num_dimensions);
+
+/// Tries to get data type by column. Only limited subset of types is supported
+DataTypePtr getDataTypeByColumn(const IColumn & column);
+
+/// Converts Object types and columns to Tuples in @columns_list and @block
+/// and checks that types are consistent with types in @extended_storage_columns.
+void convertObjectsToTuples(NamesAndTypesList & columns_list, Block & block, const NamesAndTypesList & extended_storage_columns);
+
+/// Checks that each path is not the prefix of any other path.
+void checkObjectHasNoAmbiguosPaths(const PathsInData & paths);
+
+/// Receives several Tuple types and deduces the least common type among them.
+DataTypePtr getLeastCommonTypeForObject(const DataTypes & types, bool check_ambiguos_paths = false);
+
+/// Converts types of object columns to tuples in @columns_list
+/// according to @object_columns and adds all tuple's subcolumns if needed.
+void extendObjectColumns(NamesAndTypesList & columns_list, const ColumnsDescription & object_columns, bool with_subcolumns);
+
+NameSet getNamesOfObjectColumns(const NamesAndTypesList & columns_list);
+bool hasObjectColumns(const ColumnsDescription & columns);
+void finalizeObjectColumns(MutableColumns & columns);
+
+/// Updates types of objects in @object_columns inplace
+/// according to types in new_columns.
+void updateObjectColumns(ColumnsDescription & object_columns, const NamesAndTypesList & new_columns);
+
+using DataTypeTuplePtr = std::shared_ptr<DataTypeTuple>;
+
+/// Flattens nested Tuple to plain Tuple. I.e extracts all paths and types from tuple.
+/// E.g. Tuple(t Tuple(c1 UInt32, c2 String), c3 UInt64) -> Tuple(t.c1 UInt32, t.c2 String, c3 UInt32)
+std::pair<PathsInData, DataTypes> flattenTuple(const DataTypePtr & type);
+
+/// Flattens nested Tuple column to plain Tuple column.
+ColumnPtr flattenTuple(const ColumnPtr & column);
+
+/// The reverse operation to 'flattenTuple'.
+/// Creates nested Tuple from all paths and types.
+/// E.g. Tuple(t.c1 UInt32, t.c2 String, c3 UInt32) -> Tuple(t Tuple(c1 UInt32, c2 String), c3 UInt64)
+DataTypePtr unflattenTuple(
+    const PathsInData & paths,
+    const DataTypes & tuple_types);
+
+std::pair<ColumnPtr, DataTypePtr> unflattenTuple(
+    const PathsInData & paths,
+    const DataTypes & tuple_types,
+    const Columns & tuple_columns);
+
+/// For all columns which exist in @expected_columns and
+/// don't exist in @available_columns adds to WITH clause
+/// an alias with column name to literal of default value of column type.
+void replaceMissedSubcolumnsByConstants(
+    const ColumnsDescription & expected_columns,
+    const ColumnsDescription & available_columns,
+    ASTPtr query);
+
+/// Receives range of objects, which contains collections
+/// of columns-like objects (e.g. ColumnsDescription or NamesAndTypesList)
+/// and deduces the common types of object columns for all entries.
+/// @entry_columns_getter should extract reference to collection of
+/// columns-like objects from entry to which Iterator points.
+/// columns-like object should have fields "name" and "type".
+template <typename Iterator, typename EntryColumnsGetter>
+ColumnsDescription getObjectColumns(
+    Iterator begin, Iterator end,
+    const ColumnsDescription & storage_columns,
+    EntryColumnsGetter && entry_columns_getter)
+{
+    ColumnsDescription res;
+
+    if (begin == end)
+    {
+        for (const auto & column : storage_columns)
+        {
+            if (isObject(column.type))
+            {
+                auto tuple_type = std::make_shared<DataTypeTuple>(
+                    DataTypes{std::make_shared<DataTypeUInt8>()},
+                    Names{ColumnObject::COLUMN_NAME_DUMMY});
+
+                res.add({column.name, std::move(tuple_type)});
+            }
+        }
+
+        return res;
+    }
+
+    std::unordered_map<String, DataTypes> types_in_entries;
+
+    for (auto it = begin; it != end; ++it)
+    {
+        const auto & entry_columns = entry_columns_getter(*it);
+        for (const auto & column : entry_columns)
+        {
+            auto storage_column = storage_columns.tryGetPhysical(column.name);
+            if (storage_column && isObject(storage_column->type))
+                types_in_entries[column.name].push_back(column.type);
+        }
+    }
+
+    for (const auto & [name, types] : types_in_entries)
+        res.add({name, getLeastCommonTypeForObject(types)});
+
+    return res;
+}
+
+}
diff --git a/src/DataTypes/Serializations/CMakeLists.txt b/src/DataTypes/Serializations/CMakeLists.txt
new file mode 100644
index 000000000000..651723566451
--- /dev/null
+++ b/src/DataTypes/Serializations/CMakeLists.txt
@@ -0,0 +1,3 @@
+if (ENABLE_TESTS)
+    add_subdirectory (tests)
+endif ()
diff --git a/src/DataTypes/Serializations/ISerialization.cpp b/src/DataTypes/Serializations/ISerialization.cpp
index 7df4a956c1a2..512653ecb13a 100644
--- a/src/DataTypes/Serializations/ISerialization.cpp
+++ b/src/DataTypes/Serializations/ISerialization.cpp
@@ -172,6 +172,10 @@ String getNameForSubstreamPath(
             else
                 stream_name += "." + it->tuple_element_name;
         }
+        else if (it->type == Substream::ObjectElement)
+        {
+            stream_name += escapeForFileName(".") + escapeForFileName(it->object_key_name);
+        }
     }
 
     return stream_name;
diff --git a/src/DataTypes/Serializations/ISerialization.h b/src/DataTypes/Serializations/ISerialization.h
index 86d4eab289a1..6c6b64f2416e 100644
--- a/src/DataTypes/Serializations/ISerialization.h
+++ b/src/DataTypes/Serializations/ISerialization.h
@@ -125,6 +125,9 @@ class ISerialization : private boost::noncopyable, public std::enable_shared_fro
             SparseElements,
             SparseOffsets,
 
+            ObjectStructure,
+            ObjectElement,
+
             Regular,
         };
 
@@ -133,6 +136,9 @@ class ISerialization : private boost::noncopyable, public std::enable_shared_fro
         /// Index of tuple element, starting at 1 or name.
         String tuple_element_name;
 
+        /// Name of subcolumn of object column.
+        String object_key_name;
+
         /// Do we need to escape a dot in filenames for tuple elements.
         bool escape_tuple_delimiter = true;
 
diff --git a/src/DataTypes/Serializations/JSONDataParser.h b/src/DataTypes/Serializations/JSONDataParser.h
new file mode 100644
index 000000000000..76974b269fd6
--- /dev/null
+++ b/src/DataTypes/Serializations/JSONDataParser.h
@@ -0,0 +1,183 @@
+#pragma once
+
+#include <IO/ReadHelpers.h>
+#include <Common/HashTable/HashMap.h>
+#include <Common/checkStackSize.h>
+#include <DataTypes/Serializations/PathInData.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int LOGICAL_ERROR;
+}
+
+class ReadBuffer;
+class WriteBuffer;
+
+template <typename Element>
+static Field getValueAsField(const Element & element)
+{
+    if (element.isBool())   return element.getBool();
+    if (element.isInt64())  return element.getInt64();
+    if (element.isUInt64()) return element.getUInt64();
+    if (element.isDouble()) return element.getDouble();
+    if (element.isString()) return element.getString();
+    if (element.isNull())   return Field();
+
+    throw Exception(ErrorCodes::LOGICAL_ERROR, "Unsupported type of JSON field");
+}
+
+template <typename ParserImpl>
+class JSONDataParser
+{
+public:
+    using Element = typename ParserImpl::Element;
+
+    void readJSON(String & s, ReadBuffer & buf)
+    {
+        readJSONObjectPossiblyInvalid(s, buf);
+    }
+
+    std::optional<ParseResult> parse(const char * begin, size_t length)
+    {
+        std::string_view json{begin, length};
+        Element document;
+        if (!parser.parse(json, document))
+            return {};
+
+        ParseResult result;
+        PathInDataBuilder builder;
+        std::vector<PathInData::Parts> paths;
+
+        traverse(document, builder, paths, result.values);
+
+        result.paths.reserve(paths.size());
+        for (auto && path : paths)
+            result.paths.emplace_back(std::move(path));
+
+        return result;
+    }
+
+private:
+    void traverse(
+        const Element & element,
+        PathInDataBuilder & builder,
+        std::vector<PathInData::Parts> & paths,
+        std::vector<Field> & values)
+    {
+        checkStackSize();
+
+        if (element.isObject())
+        {
+            auto object = element.getObject();
+
+            paths.reserve(paths.size() + object.size());
+            values.reserve(values.size() + object.size());
+
+            for (auto it = object.begin(); it != object.end(); ++it)
+            {
+                const auto & [key, value] = *it;
+                traverse(value, builder.append(key, false), paths, values);
+                builder.popBack();
+            }
+        }
+        else if (element.isArray())
+        {
+            auto array = element.getArray();
+
+            using PathPartsWithArray = std::pair<PathInData::Parts, Array>;
+            using PathToArray = HashMapWithStackMemory<UInt128, PathPartsWithArray, UInt128TrivialHash, 5>;
+
+            /// Traverse elements of array and collect an array
+            /// of fields by each path.
+
+            PathToArray arrays_by_path;
+            Arena strings_pool;
+
+            size_t current_size = 0;
+            for (auto it = array.begin(); it != array.end(); ++it)
+            {
+                std::vector<PathInData::Parts> element_paths;
+                std::vector<Field> element_values;
+                PathInDataBuilder element_builder;
+
+                traverse(*it, element_builder, element_paths, element_values);
+                size_t size = element_paths.size();
+                size_t keys_to_update = arrays_by_path.size();
+
+                for (size_t i = 0; i < size; ++i)
+                {
+                    UInt128 hash = PathInData::getPartsHash(element_paths[i]);
+                    if (auto * found = arrays_by_path.find(hash))
+                    {
+                        auto & path_array = found->getMapped().second;
+
+                        assert(path_array.size() == current_size);
+                        path_array.push_back(std::move(element_values[i]));
+                        --keys_to_update;
+                    }
+                    else
+                    {
+                        /// We found a new key. Add and empty array with current size.
+                        Array path_array;
+                        path_array.reserve(array.size());
+                        path_array.resize(current_size);
+                        path_array.push_back(std::move(element_values[i]));
+
+                        auto & elem = arrays_by_path[hash];
+                        elem.first = std::move(element_paths[i]);
+                        elem.second = std::move(path_array);
+                    }
+                }
+
+                /// If some of the keys are missed in current element,
+                /// add default values for them.
+                if (keys_to_update)
+                {
+                    for (auto & [_, value] : arrays_by_path)
+                    {
+                        auto & path_array = value.second;
+                        assert(path_array.size() == current_size || path_array.size() == current_size + 1);
+                        if (path_array.size() == current_size)
+                            path_array.push_back(Field());
+                    }
+                }
+
+                ++current_size;
+            }
+
+            if (arrays_by_path.empty())
+            {
+                paths.push_back(builder.getParts());
+                values.push_back(Array());
+            }
+            else
+            {
+                paths.reserve(paths.size() + arrays_by_path.size());
+                values.reserve(values.size() + arrays_by_path.size());
+
+                for (auto && [_, value] : arrays_by_path)
+                {
+                    auto && [path, path_array] = value;
+
+                    /// Merge prefix path and path of array element.
+                    paths.push_back(builder.append(path, true).getParts());
+                    values.push_back(std::move(path_array));
+
+                    builder.popBack(path.size());
+                }
+            }
+        }
+        else
+        {
+            paths.push_back(builder.getParts());
+            values.push_back(getValueAsField(element));
+        }
+    }
+
+    ParserImpl parser;
+};
+
+}
diff --git a/src/DataTypes/Serializations/PathInData.cpp b/src/DataTypes/Serializations/PathInData.cpp
new file mode 100644
index 000000000000..9631138dce9b
--- /dev/null
+++ b/src/DataTypes/Serializations/PathInData.cpp
@@ -0,0 +1,199 @@
+#include <DataTypes/Serializations/PathInData.h>
+#include <DataTypes/NestedUtils.h>
+#include <DataTypes/DataTypeTuple.h>
+#include <DataTypes/DataTypeArray.h>
+#include <Columns/ColumnTuple.h>
+#include <Columns/ColumnArray.h>
+#include <Common/SipHash.h>
+
+#include <IO/ReadHelpers.h>
+#include <IO/WriteHelpers.h>
+
+#include <boost/algorithm/string/split.hpp>
+#include <boost/algorithm/string.hpp>
+
+namespace DB
+{
+
+PathInData::PathInData(std::string_view path_)
+    : path(path_)
+{
+    const char * begin = path.data();
+    const char * end = path.data() + path.size();
+
+    for (const char * it = path.data(); it != end; ++it)
+    {
+        if (*it == '.')
+        {
+            size_t size = static_cast<size_t>(it - begin);
+            parts.emplace_back(std::string_view{begin, size}, false, 0);
+            begin = it + 1;
+        }
+    }
+
+    size_t size = static_cast<size_t>(end - begin);
+    parts.emplace_back(std::string_view{begin, size}, false, 0.);
+}
+
+PathInData::PathInData(const Parts & parts_)
+    : path(buildPath(parts_))
+    , parts(buildParts(path, parts_))
+{
+}
+
+PathInData::PathInData(const PathInData & other)
+    : path(other.path)
+    , parts(buildParts(path, other.getParts()))
+{
+}
+
+PathInData & PathInData::operator=(const PathInData & other)
+{
+    if (this != &other)
+    {
+        path = other.path;
+        parts = buildParts(path, other.parts);
+    }
+    return *this;
+}
+
+UInt128 PathInData::getPartsHash(const Parts & parts_)
+{
+    SipHash hash;
+    hash.update(parts_.size());
+    for (const auto & part : parts_)
+    {
+        hash.update(part.key.data(), part.key.length());
+        hash.update(part.is_nested);
+        hash.update(part.anonymous_array_level);
+    }
+
+    UInt128 res;
+    hash.get128(res);
+    return res;
+}
+
+void PathInData::writeBinary(WriteBuffer & out) const
+{
+    writeVarUInt(parts.size(), out);
+    for (const auto & part : parts)
+    {
+        writeStringBinary(part.key, out);
+        writeVarUInt(part.is_nested, out);
+        writeVarUInt(part.anonymous_array_level, out);
+    }
+}
+
+void PathInData::readBinary(ReadBuffer & in)
+{
+    size_t num_parts;
+    readVarUInt(num_parts, in);
+
+    Arena arena;
+    Parts temp_parts;
+    temp_parts.reserve(num_parts);
+
+    for (size_t i = 0; i < num_parts; ++i)
+    {
+        bool is_nested;
+        UInt8 anonymous_array_level;
+
+        auto ref = readStringBinaryInto(arena, in);
+        readVarUInt(is_nested, in);
+        readVarUInt(anonymous_array_level, in);
+
+        temp_parts.emplace_back(static_cast<std::string_view>(ref), is_nested, anonymous_array_level);
+    }
+
+    /// Recreate path and parts.
+    path = buildPath(temp_parts);
+    parts = buildParts(path, temp_parts);
+}
+
+String PathInData::buildPath(const Parts & other_parts)
+{
+    if (other_parts.empty())
+        return "";
+
+    String res;
+    auto it = other_parts.begin();
+    res += it->key;
+    ++it;
+    for (; it != other_parts.end(); ++it)
+    {
+        res += ".";
+        res += it->key;
+    }
+
+    return res;
+}
+
+PathInData::Parts PathInData::buildParts(const String & other_path, const Parts & other_parts)
+{
+    if (other_parts.empty())
+        return {};
+
+    Parts res;
+    const char * begin = other_path.data();
+    for (const auto & part : other_parts)
+    {
+        res.emplace_back(std::string_view{begin, part.key.length()}, part.is_nested, part.anonymous_array_level);
+        begin += part.key.length() + 1;
+    }
+    return res;
+}
+
+size_t PathInData::Hash::operator()(const PathInData & value) const
+{
+    auto hash = getPartsHash(value.parts);
+    return hash.items[0] ^ hash.items[1];
+}
+
+PathInDataBuilder & PathInDataBuilder::append(std::string_view key, bool is_array)
+{
+    if (parts.empty())
+        current_anonymous_array_level += is_array;
+
+    if (!key.empty())
+    {
+        if (!parts.empty())
+            parts.back().is_nested = is_array;
+
+        parts.emplace_back(key, false, current_anonymous_array_level);
+        current_anonymous_array_level = 0;
+    }
+
+    return *this;
+}
+
+PathInDataBuilder & PathInDataBuilder::append(const PathInData::Parts & path, bool is_array)
+{
+    if (parts.empty())
+        current_anonymous_array_level += is_array;
+
+    if (!path.empty())
+    {
+        if (!parts.empty())
+            parts.back().is_nested = is_array;
+
+        auto it = parts.insert(parts.end(), path.begin(), path.end());
+        for (; it != parts.end(); ++it)
+            it->anonymous_array_level += current_anonymous_array_level;
+        current_anonymous_array_level = 0;
+    }
+
+    return *this;
+}
+
+void PathInDataBuilder::popBack()
+{
+    parts.pop_back();
+}
+
+void PathInDataBuilder::popBack(size_t n)
+{
+    assert(n <= parts.size());
+    parts.resize(parts.size() - n);
+}
+
+}
diff --git a/src/DataTypes/Serializations/PathInData.h b/src/DataTypes/Serializations/PathInData.h
new file mode 100644
index 000000000000..35f6d10438d7
--- /dev/null
+++ b/src/DataTypes/Serializations/PathInData.h
@@ -0,0 +1,112 @@
+#pragma once
+
+#include <Core/Types.h>
+#include <Core/Field.h>
+#include <bitset>
+
+namespace DB
+{
+
+class ReadBuffer;
+class WriteBuffer;
+
+/// Class that represents path in document, e.g. JSON.
+class PathInData
+{
+public:
+    struct Part
+    {
+        Part() = default;
+        Part(std::string_view key_, bool is_nested_, UInt8 anonymous_array_level_)
+            : key(key_), is_nested(is_nested_), anonymous_array_level(anonymous_array_level_)
+        {
+        }
+
+        /// Name of part of path.
+        std::string_view key;
+
+        /// If this part is Nested, i.e. element
+        /// related to this key is the array of objects.
+        bool is_nested = false;
+
+        /// Number of array levels between current key and previous key.
+        /// E.g. in JSON {"k1": [[[{"k2": 1, "k3": 2}]]]}
+        /// "k1" is nested and has anonymous_array_level = 0.
+        /// "k2" and "k3" are not nested and have anonymous_array_level = 2.
+        UInt8 anonymous_array_level = 0;
+
+        bool operator==(const Part & other) const = default;
+    };
+
+    using Parts = std::vector<Part>;
+
+    PathInData() = default;
+    explicit PathInData(std::string_view path_);
+    explicit PathInData(const Parts & parts_);
+
+    PathInData(const PathInData & other);
+    PathInData & operator=(const PathInData & other);
+
+    static UInt128 getPartsHash(const Parts & parts_);
+
+    bool empty() const { return parts.empty(); }
+
+    const String & getPath() const { return path; }
+    const Parts & getParts() const  { return parts; }
+
+    bool isNested(size_t i) const { return parts[i].is_nested; }
+    bool hasNested() const { return std::any_of(parts.begin(), parts.end(), [](const auto & part) { return part.is_nested; }); }
+
+    void writeBinary(WriteBuffer & out) const;
+    void readBinary(ReadBuffer & in);
+
+    bool operator==(const PathInData & other) const { return parts == other.parts; }
+    struct Hash { size_t operator()(const PathInData & value) const; };
+
+private:
+    /// Creates full path from parts.
+    static String buildPath(const Parts & other_parts);
+
+    /// Creates new parts full from full path with correct string pointers.
+    static Parts buildParts(const String & other_path, const Parts & other_parts);
+
+    /// The full path. Parts are separated by dots.
+    String path;
+
+    /// Parts of the path. All string_view-s in parts must point to the @path.
+    Parts parts;
+};
+
+class PathInDataBuilder
+{
+public:
+    const PathInData::Parts & getParts() const { return parts; }
+
+    PathInDataBuilder & append(std::string_view key, bool is_array);
+    PathInDataBuilder & append(const PathInData::Parts & path, bool is_array);
+
+    void popBack();
+    void popBack(size_t n);
+
+private:
+    PathInData::Parts parts;
+
+    /// Number of array levels without key to which
+    /// next non-empty key will be nested.
+    /// Example: for JSON { "k1": [[{"k2": 1, "k3": 2}] }
+    // `k2` and `k3` has anonymous_array_level = 1 in that case.
+    size_t current_anonymous_array_level = 0;
+};
+
+using PathsInData = std::vector<PathInData>;
+
+/// Result of parsing of a document.
+/// Contains all paths extracted from document
+/// and values which are related to them.
+struct ParseResult
+{
+    std::vector<PathInData> paths;
+    std::vector<Field> values;
+};
+
+}
diff --git a/src/DataTypes/Serializations/SerializationObject.cpp b/src/DataTypes/Serializations/SerializationObject.cpp
new file mode 100644
index 000000000000..f826478958ce
--- /dev/null
+++ b/src/DataTypes/Serializations/SerializationObject.cpp
@@ -0,0 +1,460 @@
+#include <DataTypes/Serializations/SerializationObject.h>
+#include <DataTypes/Serializations/JSONDataParser.h>
+#include <DataTypes/DataTypeString.h>
+#include <DataTypes/DataTypeNullable.h>
+#include <DataTypes/ObjectUtils.h>
+#include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/NestedUtils.h>
+#include <Common/JSONParsers/SimdJSONParser.h>
+#include <Common/JSONParsers/RapidJSONParser.h>
+#include <Common/HashTable/HashSet.h>
+#include <Columns/ColumnObject.h>
+
+#include <Common/FieldVisitorToString.h>
+
+#include <IO/ReadHelpers.h>
+#include <IO/WriteHelpers.h>
+#include <IO/VarInt.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int NOT_IMPLEMENTED;
+    extern const int INCORRECT_DATA;
+    extern const int CANNOT_READ_ALL_DATA;
+    extern const int LOGICAL_ERROR;
+}
+
+namespace
+{
+
+/// Visitor that keeps @num_dimensions_to_keep dimensions in arrays
+/// and replaces all scalars or nested arrays to @replacement at that level.
+class FieldVisitorReplaceScalars : public StaticVisitor<Field>
+{
+public:
+    FieldVisitorReplaceScalars(const Field & replacement_, size_t num_dimensions_to_keep_)
+        : replacement(replacement_), num_dimensions_to_keep(num_dimensions_to_keep_)
+    {
+    }
+
+    template <typename T>
+    Field operator()(const T & x) const
+    {
+        if constexpr (std::is_same_v<T, Array>)
+        {
+            if (num_dimensions_to_keep == 0)
+                return replacement;
+
+            const size_t size = x.size();
+            Array res(size);
+            for (size_t i = 0; i < size; ++i)
+                res[i] = applyVisitor(FieldVisitorReplaceScalars(replacement, num_dimensions_to_keep - 1), x[i]);
+            return res;
+        }
+        else
+            return replacement;
+    }
+
+private:
+    const Field & replacement;
+    size_t num_dimensions_to_keep;
+};
+
+using Node = typename ColumnObject::SubcolumnsTree::Node;
+
+/// Finds a subcolumn from the same Nested type as @entry and inserts
+/// an array with default values with consistent sizes as in Nested type.
+bool tryInsertDefaultFromNested(
+    std::shared_ptr<Node> entry, const ColumnObject::SubcolumnsTree & subcolumns)
+{
+    if (!entry->path.hasNested())
+        return false;
+
+    const Node * current_node = subcolumns.findLeaf(entry->path);
+    const Node * leaf = nullptr;
+    size_t num_skipped_nested = 0;
+
+    while (current_node)
+    {
+        /// Try to find the first Nested up to the current node.
+        const auto * node_nested = subcolumns.findParent(current_node,
+            [](const auto & candidate) { return candidate.isNested(); });
+
+        if (!node_nested)
+            break;
+
+        /// If there are no leaves, skip current node and find
+        /// the next node up to the current.
+        leaf = subcolumns.findLeaf(node_nested,
+            [&](const auto & candidate)
+            {
+                return candidate.data.size() == entry->data.size() + 1;
+            });
+
+        if (leaf)
+            break;
+
+        current_node = node_nested->parent;
+        ++num_skipped_nested;
+    }
+
+    if (!leaf)
+        return false;
+
+    auto last_field = leaf->data.getLastField();
+    if (last_field.isNull())
+        return false;
+
+    const auto & least_common_type = entry->data.getLeastCommonType();
+    size_t num_dimensions = getNumberOfDimensions(*least_common_type);
+    assert(num_skipped_nested < num_dimensions);
+
+    /// Replace scalars to default values with consistent array sizes.
+    size_t num_dimensions_to_keep = num_dimensions - num_skipped_nested;
+    auto default_scalar = num_skipped_nested
+        ? createEmptyArrayField(num_skipped_nested)
+        : getBaseTypeOfArray(least_common_type)->getDefault();
+
+    auto default_field = applyVisitor(FieldVisitorReplaceScalars(default_scalar, num_dimensions_to_keep), last_field);
+    entry->data.insert(std::move(default_field));
+
+    return true;
+}
+
+}
+
+template <typename Parser>
+template <typename Reader>
+void SerializationObject<Parser>::deserializeTextImpl(IColumn & column, Reader && reader) const
+{
+    auto & column_object = assert_cast<ColumnObject &>(column);
+
+    String buf;
+    reader(buf);
+
+    auto result = parser.parse(buf.data(), buf.size());
+    if (!result)
+        throw Exception(ErrorCodes::INCORRECT_DATA, "Cannot parse object");
+
+    auto & [paths, values] = *result;
+    assert(paths.size() == values.size());
+
+    HashSet<StringRef, StringRefHash> paths_set;
+    size_t column_size = column_object.size();
+
+    for (size_t i = 0; i < paths.size(); ++i)
+    {
+        auto field_info = getFieldInfo(values[i]);
+        if (isNothing(field_info.scalar_type))
+            continue;
+
+        if (!paths_set.insert(paths[i].getPath()).second)
+            throw Exception(ErrorCodes::INCORRECT_DATA,
+                "Object has ambiguous path: {}", paths[i].getPath());
+
+        if (!column_object.hasSubcolumn(paths[i]))
+        {
+            if (paths[i].hasNested())
+                column_object.addNestedSubcolumn(paths[i], field_info, column_size);
+            else
+                column_object.addSubcolumn(paths[i], column_size);
+        }
+
+        auto & subcolumn = column_object.getSubcolumn(paths[i]);
+        assert(subcolumn.size() == column_size);
+
+        subcolumn.insert(std::move(values[i]), std::move(field_info));
+    }
+
+    /// Insert default values to missed subcolumns.
+    const auto & subcolumns = column_object.getSubcolumns();
+    for (const auto & entry : subcolumns)
+    {
+        if (!paths_set.has(entry->path.getPath()))
+        {
+            bool inserted = tryInsertDefaultFromNested(entry, subcolumns);
+            if (!inserted)
+                entry->data.insertDefault();
+        }
+    }
+
+    column_object.incrementNumRows();
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeWholeText(IColumn & column, ReadBuffer & istr, const FormatSettings &) const
+{
+    deserializeTextImpl(column, [&](String & s) { readStringInto(s, istr); });
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeTextEscaped(IColumn & column, ReadBuffer & istr, const FormatSettings &) const
+{
+    deserializeTextImpl(column, [&](String & s) { readEscapedStringInto(s, istr); });
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeTextQuoted(IColumn & column, ReadBuffer & istr, const FormatSettings &) const
+{
+    deserializeTextImpl(column, [&](String & s) { readQuotedStringInto<true>(s, istr); });
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeTextJSON(IColumn & column, ReadBuffer & istr, const FormatSettings &) const
+{
+    deserializeTextImpl(column, [&](String & s) { parser.readJSON(s, istr); });
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeTextCSV(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const
+{
+    deserializeTextImpl(column, [&](String & s) { readCSVStringInto(s, istr, settings.csv); });
+}
+
+template <typename Parser>
+template <typename TSettings, typename TStatePtr>
+void SerializationObject<Parser>::checkSerializationIsSupported(const TSettings & settings, const TStatePtr & state) const
+{
+    if (settings.position_independent_encoding)
+        throw Exception(ErrorCodes::NOT_IMPLEMENTED,
+            "DataTypeObject doesn't support serialization with position independent encoding");
+
+    if (state)
+        throw Exception(ErrorCodes::NOT_IMPLEMENTED,
+            "DataTypeObject doesn't support serialization with non-trivial state");
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeBinaryBulkStatePrefix(
+    SerializeBinaryBulkSettings & settings,
+    SerializeBinaryBulkStatePtr & state) const
+{
+    checkSerializationIsSupported(settings, state);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeBinaryBulkStateSuffix(
+    SerializeBinaryBulkSettings & settings,
+    SerializeBinaryBulkStatePtr & state) const
+{
+    checkSerializationIsSupported(settings, state);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeBinaryBulkStatePrefix(
+    DeserializeBinaryBulkSettings & settings,
+    DeserializeBinaryBulkStatePtr & state) const
+{
+    checkSerializationIsSupported(settings, state);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeBinaryBulkWithMultipleStreams(
+    const IColumn & column,
+    size_t offset,
+    size_t limit,
+    SerializeBinaryBulkSettings & settings,
+    SerializeBinaryBulkStatePtr & state) const
+{
+    checkSerializationIsSupported(settings, state);
+    const auto & column_object = assert_cast<const ColumnObject &>(column);
+
+    if (!column_object.isFinalized())
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot write non-finalized ColumnObject");
+
+    settings.path.push_back(Substream::ObjectStructure);
+    if (auto * stream = settings.getter(settings.path))
+        writeVarUInt(column_object.getSubcolumns().size(), *stream);
+
+    const auto & subcolumns = column_object.getSubcolumns();
+    for (const auto & entry : subcolumns)
+    {
+        settings.path.back() = Substream::ObjectStructure;
+        settings.path.back().object_key_name = entry->path.getPath();
+
+        const auto & type = entry->data.getLeastCommonType();
+        if (auto * stream = settings.getter(settings.path))
+        {
+            entry->path.writeBinary(*stream);
+            writeStringBinary(type->getName(), *stream);
+        }
+
+        settings.path.back() = Substream::ObjectElement;
+        if (auto * stream = settings.getter(settings.path))
+        {
+            auto serialization = type->getDefaultSerialization();
+            serialization->serializeBinaryBulkWithMultipleStreams(
+                entry->data.getFinalizedColumn(), offset, limit, settings, state);
+        }
+    }
+
+    settings.path.pop_back();
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeBinaryBulkWithMultipleStreams(
+    ColumnPtr & column,
+    size_t limit,
+    DeserializeBinaryBulkSettings & settings,
+    DeserializeBinaryBulkStatePtr & state,
+    SubstreamsCache * cache) const
+{
+    checkSerializationIsSupported(settings, state);
+    if (!column->empty())
+        throw Exception(ErrorCodes::NOT_IMPLEMENTED,
+            "DataTypeObject cannot be deserialized to non-empty column");
+
+    auto mutable_column = column->assumeMutable();
+    auto & column_object = typeid_cast<ColumnObject &>(*mutable_column);
+
+    size_t num_subcolumns = 0;
+    settings.path.push_back(Substream::ObjectStructure);
+    if (auto * stream = settings.getter(settings.path))
+        readVarUInt(num_subcolumns, *stream);
+
+    settings.path.back() = Substream::ObjectElement;
+    for (size_t i = 0; i < num_subcolumns; ++i)
+    {
+        PathInData key;
+        String type_name;
+
+        settings.path.back() = Substream::ObjectStructure;
+        if (auto * stream = settings.getter(settings.path))
+        {
+            key.readBinary(*stream);
+            readStringBinary(type_name, *stream);
+        }
+        else
+        {
+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA,
+                "Cannot read structure of DataTypeObject, because its stream is missing");
+        }
+
+        settings.path.back() = Substream::ObjectElement;
+        settings.path.back().object_key_name = key.getPath();
+
+        if (auto * stream = settings.getter(settings.path))
+        {
+            auto type = DataTypeFactory::instance().get(type_name);
+            auto serialization = type->getDefaultSerialization();
+            ColumnPtr subcolumn_data = type->createColumn();
+            serialization->deserializeBinaryBulkWithMultipleStreams(subcolumn_data, limit, settings, state, cache);
+            column_object.addSubcolumn(key, subcolumn_data->assumeMutable());
+        }
+        else
+        {
+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA,
+                "Cannot read subcolumn '{}' of DataTypeObject, because its stream is missing", key.getPath());
+        }
+    }
+
+    settings.path.pop_back();
+    column_object.checkConsistency();
+    column_object.finalize();
+    column = std::move(mutable_column);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeBinary(const Field &, WriteBuffer &) const
+{
+    throw Exception(ErrorCodes::NOT_IMPLEMENTED, "Not implemented for SerializationObject");
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeBinary(Field &, ReadBuffer &) const
+{
+    throw Exception(ErrorCodes::NOT_IMPLEMENTED, "Not implemented for SerializationObject");
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeBinary(const IColumn &, size_t, WriteBuffer &) const
+{
+    throw Exception(ErrorCodes::NOT_IMPLEMENTED, "Not implemented for SerializationObject");
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::deserializeBinary(IColumn &, ReadBuffer &) const
+{
+    throw Exception(ErrorCodes::NOT_IMPLEMENTED, "Not implemented for SerializationObject");
+}
+
+/// TODO: use format different of JSON in serializations.
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeTextImpl(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const
+{
+    const auto & column_object = assert_cast<const ColumnObject &>(column);
+    const auto & subcolumns = column_object.getSubcolumns();
+
+    writeChar('{', ostr);
+    for (auto it = subcolumns.begin(); it != subcolumns.end(); ++it)
+    {
+        if (it != subcolumns.begin())
+            writeCString(",", ostr);
+
+        writeDoubleQuoted((*it)->path.getPath(), ostr);
+        writeChar(':', ostr);
+
+        auto serialization = (*it)->data.getLeastCommonType()->getDefaultSerialization();
+        serialization->serializeTextJSON((*it)->data.getFinalizedColumn(), row_num, ostr, settings);
+    }
+    writeChar('}', ostr);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeText(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const
+{
+    serializeTextImpl(column, row_num, ostr, settings);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeTextEscaped(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const
+{
+    WriteBufferFromOwnString ostr_str;
+    serializeTextImpl(column, row_num, ostr_str, settings);
+    writeEscapedString(ostr_str.str(), ostr);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeTextQuoted(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const
+{
+    WriteBufferFromOwnString ostr_str;
+    serializeTextImpl(column, row_num, ostr_str, settings);
+    writeQuotedString(ostr_str.str(), ostr);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeTextJSON(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const
+{
+    serializeTextImpl(column, row_num, ostr, settings);
+}
+
+template <typename Parser>
+void SerializationObject<Parser>::serializeTextCSV(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const
+{
+    WriteBufferFromOwnString ostr_str;
+    serializeTextImpl(column, row_num, ostr_str, settings);
+    writeCSVString(ostr_str.str(), ostr);
+}
+
+SerializationPtr getObjectSerialization(const String & schema_format)
+{
+    if (schema_format == "json")
+    {
+#if USE_SIMDJSON
+        return std::make_shared<SerializationObject<JSONDataParser<SimdJSONParser>>>();
+#elif USE_RAPIDJSON
+        return std::make_shared<SerializationObject<JSONDataParser<RapidJSONParser>>>();
+#else
+        throw Exception(ErrorCodes::NOT_IMPLEMENTED,
+            "To use data type Object with JSON format ClickHouse should be built with Simdjson or Rapidjson");
+#endif
+    }
+
+    throw Exception(ErrorCodes::NOT_IMPLEMENTED, "Unknown schema format '{}'", schema_format);
+}
+
+}
diff --git a/src/DataTypes/Serializations/SerializationObject.h b/src/DataTypes/Serializations/SerializationObject.h
new file mode 100644
index 000000000000..c91d467d5e1f
--- /dev/null
+++ b/src/DataTypes/Serializations/SerializationObject.h
@@ -0,0 +1,73 @@
+#pragma once
+
+#include <DataTypes/Serializations/SimpleTextSerialization.h>
+
+namespace DB
+{
+
+/// Serialization for data type Object.
+/// Supported only test serialization/deserialization.
+/// and binary bulk serialization/deserialization without position independent
+/// encoding, i.e. serialization/deserialization into Native format.
+template <typename Parser>
+class SerializationObject : public ISerialization
+{
+public:
+    void serializeBinaryBulkStatePrefix(
+        SerializeBinaryBulkSettings & settings,
+        SerializeBinaryBulkStatePtr & state) const override;
+
+    void serializeBinaryBulkStateSuffix(
+        SerializeBinaryBulkSettings & settings,
+        SerializeBinaryBulkStatePtr & state) const override;
+
+    void deserializeBinaryBulkStatePrefix(
+        DeserializeBinaryBulkSettings & settings,
+        DeserializeBinaryBulkStatePtr & state) const override;
+
+    void serializeBinaryBulkWithMultipleStreams(
+        const IColumn & column,
+        size_t offset,
+        size_t limit,
+        SerializeBinaryBulkSettings & settings,
+        SerializeBinaryBulkStatePtr & state) const override;
+
+    void deserializeBinaryBulkWithMultipleStreams(
+        ColumnPtr & column,
+        size_t limit,
+        DeserializeBinaryBulkSettings & settings,
+        DeserializeBinaryBulkStatePtr & state,
+        SubstreamsCache * cache) const override;
+
+    void serializeBinary(const Field & field, WriteBuffer & ostr) const override;
+    void deserializeBinary(Field & field, ReadBuffer & istr) const override;
+    void serializeBinary(const IColumn & column, size_t row_num, WriteBuffer & ostr) const override;
+    void deserializeBinary(IColumn & column, ReadBuffer & istr) const override;
+
+    void serializeText(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const override;
+    void serializeTextEscaped(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const override;
+    void serializeTextQuoted(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const override;
+    void serializeTextJSON(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const override;
+    void serializeTextCSV(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const override;
+
+    void deserializeWholeText(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const override;
+    void deserializeTextEscaped(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const override;
+    void deserializeTextQuoted(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const override;
+    void deserializeTextJSON(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const override;
+    void deserializeTextCSV(IColumn & column, ReadBuffer & istr, const FormatSettings & settings) const override;
+
+private:
+    template <typename TSettings, typename TStatePtr>
+    void checkSerializationIsSupported(const TSettings & settings, const TStatePtr & state) const;
+
+    template <typename Reader>
+    void deserializeTextImpl(IColumn & column, Reader && reader) const;
+
+    void serializeTextImpl(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const;
+
+    mutable Parser parser;
+};
+
+SerializationPtr getObjectSerialization(const String & schema_format);
+
+}
diff --git a/src/DataTypes/Serializations/SubcolumnsTree.h b/src/DataTypes/Serializations/SubcolumnsTree.h
new file mode 100644
index 000000000000..64fc14ba8343
--- /dev/null
+++ b/src/DataTypes/Serializations/SubcolumnsTree.h
@@ -0,0 +1,209 @@
+#pragma once
+
+#include <DataTypes/Serializations/PathInData.h>
+#include <DataTypes/IDataType.h>
+#include <Columns/IColumn.h>
+#include <unordered_map>
+
+namespace DB
+{
+
+/// Tree that represents paths in document
+/// with additional data in nodes.
+template <typename NodeData>
+class SubcolumnsTree
+{
+public:
+    struct Node
+    {
+        enum Kind
+        {
+            TUPLE,
+            NESTED,
+            SCALAR,
+        };
+
+        explicit Node(Kind kind_) : kind(kind_) {}
+        Node(Kind kind_, const NodeData & data_) : kind(kind_), data(data_) {}
+        Node(Kind kind_, const NodeData & data_, const PathInData & path_)
+            : kind(kind_), data(data_), path(path_) {}
+
+        Kind kind = TUPLE;
+        const Node * parent = nullptr;
+
+        std::map<String, std::shared_ptr<Node>, std::less<>> children;
+
+        NodeData data;
+        PathInData path;
+
+        bool isNested() const { return kind == NESTED; }
+        bool isScalar() const { return kind == SCALAR; }
+
+        void addChild(const String & key, std::shared_ptr<Node> next_node)
+        {
+            next_node->parent = this;
+            children[key] = std::move(next_node);
+        }
+    };
+
+    using NodeKind = typename Node::Kind;
+    using NodePtr = std::shared_ptr<Node>;
+
+    /// Add a leaf without any data in other nodes.
+    bool add(const PathInData & path, const NodeData & leaf_data)
+    {
+        return add(path, [&](NodeKind kind, bool exists) -> NodePtr
+        {
+            if (exists)
+                return nullptr;
+
+            if (kind == Node::SCALAR)
+                return std::make_shared<Node>(kind, leaf_data, path);
+
+            return std::make_shared<Node>(kind);
+        });
+    }
+
+    /// Callback for creation of node. Receives kind of node and
+    /// flag, which is true if node already exists.
+    using NodeCreator = std::function<NodePtr(NodeKind, bool)>;
+
+    bool add(const PathInData & path, const NodeCreator & node_creator)
+    {
+        const auto & parts = path.getParts();
+
+        if (parts.empty())
+            return false;
+
+        if (!root)
+            root = std::make_shared<Node>(Node::TUPLE);
+
+        Node * current_node = root.get();
+        for (size_t i = 0; i < parts.size() - 1; ++i)
+        {
+            assert(current_node->kind != Node::SCALAR);
+
+            auto it = current_node->children.find(parts[i].key);
+            if (it != current_node->children.end())
+            {
+                current_node = it->second.get();
+                node_creator(current_node->kind, true);
+
+                if (current_node->isNested() != parts[i].is_nested)
+                    return false;
+            }
+            else
+            {
+                auto next_kind = parts[i].is_nested ? Node::NESTED : Node::TUPLE;
+                auto next_node = node_creator(next_kind, false);
+                current_node->addChild(String(parts[i].key), next_node);
+                current_node = next_node.get();
+            }
+        }
+
+        auto it = current_node->children.find(parts.back().key);
+        if (it != current_node->children.end())
+            return false;
+
+        auto next_node = node_creator(Node::SCALAR, false);
+        current_node->addChild(String(parts.back().key), next_node);
+        leaves.push_back(std::move(next_node));
+
+        return true;
+    }
+
+    /// Find node that matches the path the best.
+    const Node * findBestMatch(const PathInData & path) const
+    {
+        return findImpl(path, false);
+    }
+
+    /// Find node that matches the path exactly.
+    const Node * findExact(const PathInData & path) const
+    {
+        return findImpl(path, true);
+    }
+
+    /// Find leaf by path.
+    const Node * findLeaf(const PathInData & path) const
+    {
+        const auto * candidate = findExact(path);
+        if (!candidate || !candidate->isScalar())
+            return nullptr;
+        return candidate;
+    }
+
+    using NodePredicate = std::function<bool(const Node &)>;
+
+    /// Finds leaf that satisfies the predicate.
+    const Node * findLeaf(const NodePredicate & predicate)
+    {
+        return findLeaf(root.get(), predicate);
+    }
+
+    static const Node * findLeaf(const Node * node, const NodePredicate & predicate)
+    {
+        if (!node)
+            return nullptr;
+
+        if (node->isScalar())
+            return predicate(*node) ? node : nullptr;
+
+        for (const auto & [_, child] : node->children)
+            if (const auto * leaf = findLeaf(child.get(), predicate))
+                return leaf;
+
+        return nullptr;
+    }
+
+    /// Find first parent node that satisfies the predicate.
+    static const Node * findParent(const Node * node, const NodePredicate & predicate)
+    {
+        while (node && !predicate(*node))
+            node = node->parent;
+        return node;
+    }
+
+    bool empty() const { return root == nullptr; }
+    size_t size() const { return leaves.size(); }
+
+    using Nodes = std::vector<NodePtr>;
+
+    const Nodes & getLeaves() const { return leaves; }
+    const Node * getRoot() const { return root.get(); }
+
+    using iterator = typename Nodes::iterator;
+    using const_iterator = typename Nodes::const_iterator;
+
+    iterator begin() { return leaves.begin(); }
+    iterator end() { return leaves.end(); }
+
+    const_iterator begin() const { return leaves.begin(); }
+    const_iterator end() const { return leaves.end(); }
+
+private:
+    const Node * findImpl(const PathInData & path, bool find_exact) const
+    {
+        if (!root)
+            return nullptr;
+
+        const auto & parts = path.getParts();
+        const Node * current_node = root.get();
+
+        for (const auto & part : parts)
+        {
+            auto it = current_node->children.find(part.key);
+            if (it == current_node->children.end())
+                return find_exact ? nullptr : current_node;
+
+            current_node = it->second.get();
+        }
+
+        return current_node;
+    }
+
+    NodePtr root;
+    Nodes leaves;
+};
+
+}
diff --git a/src/DataTypes/getLeastSupertype.cpp b/src/DataTypes/getLeastSupertype.cpp
index 22f6a0775040..3fcb3fef25b3 100644
--- a/src/DataTypes/getLeastSupertype.cpp
+++ b/src/DataTypes/getLeastSupertype.cpp
@@ -18,6 +18,8 @@
 #include <DataTypes/DataTypeEnum.h>
 #include <DataTypes/DataTypesNumber.h>
 #include <DataTypes/DataTypesDecimal.h>
+#include <DataTypes/DataTypeFactory.h>
+#include <base/EnumReflection.h>
 
 
 namespace DB
@@ -30,28 +32,181 @@ namespace ErrorCodes
 
 namespace
 {
-    String getExceptionMessagePrefix(const DataTypes & types)
+
+String typeToString(const DataTypePtr & type) { return type->getName(); }
+String typeToString(const TypeIndex & type) { return String(magic_enum::enum_name(type)); }
+
+template <typename DataTypes>
+String getExceptionMessagePrefix(const DataTypes & types)
+{
+    WriteBufferFromOwnString res;
+    res << "There is no supertype for types ";
+
+    bool first = true;
+    for (const auto & type : types)
     {
-        WriteBufferFromOwnString res;
-        res << "There is no supertype for types ";
+        if (!first)
+            res << ", ";
+        first = false;
 
-        bool first = true;
-        for (const auto & type : types)
+        res << typeToString(type);
+    }
+
+    return res.str();
+}
+
+DataTypePtr getNumericType(const TypeIndexSet & types, bool allow_conversion_to_string)
+{
+    auto throw_or_return = [&](std::string_view message, int error_code)
+    {
+        if (allow_conversion_to_string)
+            return std::make_shared<DataTypeString>();
+
+        throw Exception(String(message), error_code);
+    };
+
+    bool all_numbers = true;
+
+    size_t max_bits_of_signed_integer = 0;
+    size_t max_bits_of_unsigned_integer = 0;
+    size_t max_mantissa_bits_of_floating = 0;
+
+    auto maximize = [](size_t & what, size_t value)
+    {
+        if (value > what)
+            what = value;
+    };
+
+    for (const auto & type : types)
+    {
+        if (type == TypeIndex::UInt8)
+            maximize(max_bits_of_unsigned_integer, 8);
+        else if (type == TypeIndex::UInt16)
+            maximize(max_bits_of_unsigned_integer, 16);
+        else if (type == TypeIndex::UInt32)
+            maximize(max_bits_of_unsigned_integer, 32);
+        else if (type == TypeIndex::UInt64)
+            maximize(max_bits_of_unsigned_integer, 64);
+        else if (type == TypeIndex::UInt128)
+            maximize(max_bits_of_unsigned_integer, 128);
+        else if (type == TypeIndex::UInt256)
+            maximize(max_bits_of_unsigned_integer, 256);
+        else if (type == TypeIndex::Int8 || type == TypeIndex::Enum8)
+            maximize(max_bits_of_signed_integer, 8);
+        else if (type == TypeIndex::Int16 || type == TypeIndex::Enum16)
+            maximize(max_bits_of_signed_integer, 16);
+        else if (type == TypeIndex::Int32)
+            maximize(max_bits_of_signed_integer, 32);
+        else if (type == TypeIndex::Int64)
+            maximize(max_bits_of_signed_integer, 64);
+        else if (type == TypeIndex::Int128)
+            maximize(max_bits_of_signed_integer, 128);
+        else if (type == TypeIndex::Int256)
+            maximize(max_bits_of_signed_integer, 256);
+        else if (type == TypeIndex::Float32)
+            maximize(max_mantissa_bits_of_floating, 24);
+        else if (type == TypeIndex::Float64)
+            maximize(max_mantissa_bits_of_floating, 53);
+        else
+            all_numbers = false;
+    }
+
+    if (max_bits_of_signed_integer || max_bits_of_unsigned_integer || max_mantissa_bits_of_floating)
+    {
+        if (!all_numbers)
+            return throw_or_return(getExceptionMessagePrefix(types) + " because some of them are numbers and some of them are not", ErrorCodes::NO_COMMON_TYPE);
+
+        /// If there are signed and unsigned types of same bit-width, the result must be signed number with at least one more bit.
+        /// Example, common of Int32, UInt32 = Int64.
+
+        size_t min_bit_width_of_integer = std::max(max_bits_of_signed_integer, max_bits_of_unsigned_integer);
+
+        /// If unsigned is not covered by signed.
+        if (max_bits_of_signed_integer && max_bits_of_unsigned_integer >= max_bits_of_signed_integer) //-V1051
         {
-            if (!first)
-                res << ", ";
-            first = false;
+            // Because 128 and 256 bit integers are significantly slower, we should not promote to them.
+            // But if we already have wide numbers, promotion is necessary.
+            if (min_bit_width_of_integer != 64)
+                ++min_bit_width_of_integer;
+            else
+                return throw_or_return(
+                    getExceptionMessagePrefix(types)
+                        + " because some of them are signed integers and some are unsigned integers,"
+                            " but there is no signed integer type, that can exactly represent all required unsigned integer values",
+                    ErrorCodes::NO_COMMON_TYPE);
+        }
 
-            res << type->getName();
+        /// If the result must be floating.
+        if (max_mantissa_bits_of_floating)
+        {
+            size_t min_mantissa_bits = std::max(min_bit_width_of_integer, max_mantissa_bits_of_floating);
+            if (min_mantissa_bits <= 24)
+                return std::make_shared<DataTypeFloat32>();
+            else if (min_mantissa_bits <= 53)
+                return std::make_shared<DataTypeFloat64>();
+            else
+                return throw_or_return(getExceptionMessagePrefix(types)
+                    + " because some of them are integers and some are floating point,"
+                    " but there is no floating point type, that can exactly represent all required integers", ErrorCodes::NO_COMMON_TYPE);
         }
 
-        return res.str();
+        /// If the result must be signed integer.
+        if (max_bits_of_signed_integer)
+        {
+            if (min_bit_width_of_integer <= 8)
+                return std::make_shared<DataTypeInt8>();
+            else if (min_bit_width_of_integer <= 16)
+                return std::make_shared<DataTypeInt16>();
+            else if (min_bit_width_of_integer <= 32)
+                return std::make_shared<DataTypeInt32>();
+            else if (min_bit_width_of_integer <= 64)
+                return std::make_shared<DataTypeInt64>();
+            else if (min_bit_width_of_integer <= 128)
+                return std::make_shared<DataTypeInt128>();
+            else if (min_bit_width_of_integer <= 256)
+                return std::make_shared<DataTypeInt256>();
+            else
+                return throw_or_return(getExceptionMessagePrefix(types)
+                    + " because some of them are signed integers and some are unsigned integers,"
+                    " but there is no signed integer type, that can exactly represent all required unsigned integer values", ErrorCodes::NO_COMMON_TYPE);
+        }
+
+        /// All unsigned.
+        {
+            if (min_bit_width_of_integer <= 8)
+                return std::make_shared<DataTypeUInt8>();
+            else if (min_bit_width_of_integer <= 16)
+                return std::make_shared<DataTypeUInt16>();
+            else if (min_bit_width_of_integer <= 32)
+                return std::make_shared<DataTypeUInt32>();
+            else if (min_bit_width_of_integer <= 64)
+                return std::make_shared<DataTypeUInt64>();
+            else if (min_bit_width_of_integer <= 128)
+                return std::make_shared<DataTypeUInt128>();
+            else if (min_bit_width_of_integer <= 256)
+                return std::make_shared<DataTypeUInt256>();
+            else
+                return throw_or_return("Logical error: " + getExceptionMessagePrefix(types)
+                    + " but as all data types are unsigned integers, we must have found maximum unsigned integer type", ErrorCodes::NO_COMMON_TYPE);
+
+        }
     }
+
+    return {};
 }
 
+}
 
-DataTypePtr getLeastSupertype(const DataTypes & types)
+DataTypePtr getLeastSupertype(const DataTypes & types, bool allow_conversion_to_string)
 {
+    auto throw_or_return = [&](std::string_view message, int error_code)
+    {
+        if (allow_conversion_to_string)
+            return std::make_shared<DataTypeString>();
+
+        throw Exception(String(message), error_code);
+    };
+
     /// Trivial cases
 
     if (types.empty())
@@ -88,7 +243,7 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
                 non_nothing_types.emplace_back(type);
 
         if (non_nothing_types.size() < types.size())
-            return getLeastSupertype(non_nothing_types);
+            return getLeastSupertype(non_nothing_types, allow_conversion_to_string);
     }
 
     /// For Arrays
@@ -113,9 +268,9 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
         if (have_array)
         {
             if (!all_arrays)
-                throw Exception(getExceptionMessagePrefix(types) + " because some of them are Array and some of them are not", ErrorCodes::NO_COMMON_TYPE);
+                return throw_or_return(getExceptionMessagePrefix(types) + " because some of them are Array and some of them are not", ErrorCodes::NO_COMMON_TYPE);
 
-            return std::make_shared<DataTypeArray>(getLeastSupertype(nested_types));
+            return std::make_shared<DataTypeArray>(getLeastSupertype(nested_types, allow_conversion_to_string));
         }
     }
 
@@ -139,7 +294,7 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
                         nested_types[elem_idx].reserve(types.size());
                 }
                 else if (tuple_size != type_tuple->getElements().size())
-                    throw Exception(getExceptionMessagePrefix(types) + " because Tuples have different sizes", ErrorCodes::NO_COMMON_TYPE);
+                    return throw_or_return(getExceptionMessagePrefix(types) + " because Tuples have different sizes", ErrorCodes::NO_COMMON_TYPE);
 
                 have_tuple = true;
 
@@ -153,11 +308,11 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
         if (have_tuple)
         {
             if (!all_tuples)
-                throw Exception(getExceptionMessagePrefix(types) + " because some of them are Tuple and some of them are not", ErrorCodes::NO_COMMON_TYPE);
+                return throw_or_return(getExceptionMessagePrefix(types) + " because some of them are Tuple and some of them are not", ErrorCodes::NO_COMMON_TYPE);
 
             DataTypes common_tuple_types(tuple_size);
             for (size_t elem_idx = 0; elem_idx < tuple_size; ++elem_idx)
-                common_tuple_types[elem_idx] = getLeastSupertype(nested_types[elem_idx]);
+                common_tuple_types[elem_idx] = getLeastSupertype(nested_types[elem_idx], allow_conversion_to_string);
 
             return std::make_shared<DataTypeTuple>(common_tuple_types);
         }
@@ -187,9 +342,11 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
         if (have_maps)
         {
             if (!all_maps)
-                throw Exception(getExceptionMessagePrefix(types) + " because some of them are Maps and some of them are not", ErrorCodes::NO_COMMON_TYPE);
+                return throw_or_return(getExceptionMessagePrefix(types) + " because some of them are Maps and some of them are not", ErrorCodes::NO_COMMON_TYPE);
 
-            return std::make_shared<DataTypeMap>(getLeastSupertype(key_types), getLeastSupertype(value_types));
+            return std::make_shared<DataTypeMap>(
+                getLeastSupertype(key_types, allow_conversion_to_string),
+                getLeastSupertype(value_types, allow_conversion_to_string));
         }
     }
 
@@ -220,9 +377,9 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
         if (have_low_cardinality)
         {
             if (have_not_low_cardinality)
-                return getLeastSupertype(nested_types);
+                return getLeastSupertype(nested_types, allow_conversion_to_string);
             else
-                return std::make_shared<DataTypeLowCardinality>(getLeastSupertype(nested_types));
+                return std::make_shared<DataTypeLowCardinality>(getLeastSupertype(nested_types, allow_conversion_to_string));
         }
     }
 
@@ -248,13 +405,13 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
 
         if (have_nullable)
         {
-            return std::make_shared<DataTypeNullable>(getLeastSupertype(nested_types));
+            return std::make_shared<DataTypeNullable>(getLeastSupertype(nested_types, allow_conversion_to_string));
         }
     }
 
     /// Non-recursive rules
 
-    std::unordered_set<TypeIndex> type_ids;
+    TypeIndexSet type_ids;
     for (const auto & type : types)
         type_ids.insert(type->getTypeId());
 
@@ -268,7 +425,7 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
         {
             bool all_strings = type_ids.size() == (have_string + have_fixed_string);
             if (!all_strings)
-                throw Exception(getExceptionMessagePrefix(types) + " because some of them are String/FixedString and some of them are not", ErrorCodes::NO_COMMON_TYPE);
+                return throw_or_return(getExceptionMessagePrefix(types) + " because some of them are String/FixedString and some of them are not", ErrorCodes::NO_COMMON_TYPE);
 
             return std::make_shared<DataTypeString>();
         }
@@ -285,7 +442,8 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
         {
             bool all_date_or_datetime = type_ids.size() == (have_date + have_date32 + have_datetime + have_datetime64);
             if (!all_date_or_datetime)
-                throw Exception(getExceptionMessagePrefix(types) + " because some of them are Date/Date32/DateTime/DateTime64 and some of them are not",
+                return throw_or_return(getExceptionMessagePrefix(types)
+                    + " because some of them are Date/Date32/DateTime/DateTime64 and some of them are not",
                     ErrorCodes::NO_COMMON_TYPE);
 
             if (have_datetime64 == 0 && have_date32 == 0)
@@ -362,7 +520,7 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
             }
 
             if (num_supported != type_ids.size())
-                throw Exception(getExceptionMessagePrefix(types) + " because some of them have no lossless conversion to Decimal",
+                return throw_or_return(getExceptionMessagePrefix(types) + " because some of them have no lossless conversion to Decimal",
                                 ErrorCodes::NO_COMMON_TYPE);
 
             UInt32 max_scale = 0;
@@ -385,7 +543,7 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
             }
 
             if (min_precision > DataTypeDecimal<Decimal128>::maxPrecision())
-                throw Exception(getExceptionMessagePrefix(types) + " because the least supertype is Decimal("
+                return throw_or_return(getExceptionMessagePrefix(types) + " because the least supertype is Decimal("
                                 + toString(min_precision) + ',' + toString(max_scale) + ')',
                                 ErrorCodes::NO_COMMON_TYPE);
 
@@ -399,135 +557,56 @@ DataTypePtr getLeastSupertype(const DataTypes & types)
 
     /// For numeric types, the most complicated part.
     {
-        bool all_numbers = true;
-
-        size_t max_bits_of_signed_integer = 0;
-        size_t max_bits_of_unsigned_integer = 0;
-        size_t max_mantissa_bits_of_floating = 0;
+        auto numeric_type = getNumericType(type_ids, allow_conversion_to_string);
+        if (numeric_type)
+            return numeric_type;
+    }
 
-        auto maximize = [](size_t & what, size_t value)
-        {
-            if (value > what)
-                what = value;
-        };
+    /// All other data types (UUID, AggregateFunction, Enum...) are compatible only if they are the same (checked in trivial cases).
+    return throw_or_return(getExceptionMessagePrefix(types), ErrorCodes::NO_COMMON_TYPE);
+}
 
-        for (const auto & type : types)
-        {
-            if (typeid_cast<const DataTypeUInt8 *>(type.get()))
-                maximize(max_bits_of_unsigned_integer, 8);
-            else if (typeid_cast<const DataTypeUInt16 *>(type.get()))
-                maximize(max_bits_of_unsigned_integer, 16);
-            else if (typeid_cast<const DataTypeUInt32 *>(type.get()))
-                maximize(max_bits_of_unsigned_integer, 32);
-            else if (typeid_cast<const DataTypeUInt64 *>(type.get()))
-                maximize(max_bits_of_unsigned_integer, 64);
-            else if (typeid_cast<const DataTypeUInt128 *>(type.get()))
-                maximize(max_bits_of_unsigned_integer, 128);
-            else if (typeid_cast<const DataTypeUInt256 *>(type.get()))
-                maximize(max_bits_of_unsigned_integer, 256);
-            else if (typeid_cast<const DataTypeInt8 *>(type.get()) || typeid_cast<const DataTypeEnum8 *>(type.get()))
-                maximize(max_bits_of_signed_integer, 8);
-            else if (typeid_cast<const DataTypeInt16 *>(type.get()) || typeid_cast<const DataTypeEnum16 *>(type.get()))
-                maximize(max_bits_of_signed_integer, 16);
-            else if (typeid_cast<const DataTypeInt32 *>(type.get()))
-                maximize(max_bits_of_signed_integer, 32);
-            else if (typeid_cast<const DataTypeInt64 *>(type.get()))
-                maximize(max_bits_of_signed_integer, 64);
-            else if (typeid_cast<const DataTypeInt128 *>(type.get()))
-                maximize(max_bits_of_signed_integer, 128);
-            else if (typeid_cast<const DataTypeInt256 *>(type.get()))
-                maximize(max_bits_of_signed_integer, 256);
-            else if (typeid_cast<const DataTypeFloat32 *>(type.get()))
-                maximize(max_mantissa_bits_of_floating, 24);
-            else if (typeid_cast<const DataTypeFloat64 *>(type.get()))
-                maximize(max_mantissa_bits_of_floating, 53);
-            else
-                all_numbers = false;
-        }
+DataTypePtr getLeastSupertype(const TypeIndexSet & types, bool allow_conversion_to_string)
+{
+    auto throw_or_return = [&](std::string_view message, int error_code)
+    {
+        if (allow_conversion_to_string)
+            return std::make_shared<DataTypeString>();
 
-        if (max_bits_of_signed_integer || max_bits_of_unsigned_integer || max_mantissa_bits_of_floating)
-        {
-            if (!all_numbers)
-                throw Exception(getExceptionMessagePrefix(types) + " because some of them are numbers and some of them are not", ErrorCodes::NO_COMMON_TYPE);
+        throw Exception(String(message), error_code);
+    };
 
-            /// If there are signed and unsigned types of same bit-width, the result must be signed number with at least one more bit.
-            /// Example, common of Int32, UInt32 = Int64.
+    TypeIndexSet types_set;
+    for (const auto & type : types)
+    {
+        if (WhichDataType(type).isNothing())
+            continue;
 
-            size_t min_bit_width_of_integer = std::max(max_bits_of_signed_integer, max_bits_of_unsigned_integer);
+        if (!WhichDataType(type).isSimple())
+            throw Exception(ErrorCodes::NO_COMMON_TYPE,
+                "Cannot get common type by type ids with parametric type {}", typeToString(type));
 
-            /// If unsigned is not covered by signed.
-            if (max_bits_of_signed_integer && max_bits_of_unsigned_integer >= max_bits_of_signed_integer) //-V1051
-            {
-                // Because 128 and 256 bit integers are significantly slower, we should not promote to them.
-                // But if we already have wide numbers, promotion is necessary.
-                if (min_bit_width_of_integer != 64)
-                    ++min_bit_width_of_integer;
-                else
-                    throw Exception(
-                        getExceptionMessagePrefix(types)
-                            + " because some of them are signed integers and some are unsigned integers,"
-                              " but there is no signed integer type, that can exactly represent all required unsigned integer values",
-                        ErrorCodes::NO_COMMON_TYPE);
-            }
+        types_set.insert(type);
+    }
 
-            /// If the result must be floating.
-            if (max_mantissa_bits_of_floating)
-            {
-                size_t min_mantissa_bits = std::max(min_bit_width_of_integer, max_mantissa_bits_of_floating);
-                if (min_mantissa_bits <= 24)
-                    return std::make_shared<DataTypeFloat32>();
-                else if (min_mantissa_bits <= 53)
-                    return std::make_shared<DataTypeFloat64>();
-                else
-                    throw Exception(getExceptionMessagePrefix(types)
-                        + " because some of them are integers and some are floating point,"
-                        " but there is no floating point type, that can exactly represent all required integers", ErrorCodes::NO_COMMON_TYPE);
-            }
+    if (types_set.empty())
+        return std::make_shared<DataTypeNothing>();
 
-            /// If the result must be signed integer.
-            if (max_bits_of_signed_integer)
-            {
-                if (min_bit_width_of_integer <= 8)
-                    return std::make_shared<DataTypeInt8>();
-                else if (min_bit_width_of_integer <= 16)
-                    return std::make_shared<DataTypeInt16>();
-                else if (min_bit_width_of_integer <= 32)
-                    return std::make_shared<DataTypeInt32>();
-                else if (min_bit_width_of_integer <= 64)
-                    return std::make_shared<DataTypeInt64>();
-                else if (min_bit_width_of_integer <= 128)
-                    return std::make_shared<DataTypeInt128>();
-                else if (min_bit_width_of_integer <= 256)
-                    return std::make_shared<DataTypeInt256>();
-                else
-                    throw Exception(getExceptionMessagePrefix(types)
-                        + " because some of them are signed integers and some are unsigned integers,"
-                        " but there is no signed integer type, that can exactly represent all required unsigned integer values", ErrorCodes::NO_COMMON_TYPE);
-            }
+    if (types.count(TypeIndex::String))
+    {
+        if (types.size() != 1)
+            return throw_or_return(getExceptionMessagePrefix(types) + " because some of them are String and some of them are not", ErrorCodes::NO_COMMON_TYPE);
 
-            /// All unsigned.
-            {
-                if (min_bit_width_of_integer <= 8)
-                    return std::make_shared<DataTypeUInt8>();
-                else if (min_bit_width_of_integer <= 16)
-                    return std::make_shared<DataTypeUInt16>();
-                else if (min_bit_width_of_integer <= 32)
-                    return std::make_shared<DataTypeUInt32>();
-                else if (min_bit_width_of_integer <= 64)
-                    return std::make_shared<DataTypeUInt64>();
-                else if (min_bit_width_of_integer <= 128)
-                    return std::make_shared<DataTypeUInt128>();
-                else if (min_bit_width_of_integer <= 256)
-                    return std::make_shared<DataTypeUInt256>();
-                else
-                    throw Exception("Logical error: " + getExceptionMessagePrefix(types)
-                        + " but as all data types are unsigned integers, we must have found maximum unsigned integer type", ErrorCodes::NO_COMMON_TYPE);
-            }
-        }
+        return std::make_shared<DataTypeString>();
     }
 
+    /// For numeric types, the most complicated part.
+    auto numeric_type = getNumericType(types, allow_conversion_to_string);
+    if (numeric_type)
+        return numeric_type;
+
     /// All other data types (UUID, AggregateFunction, Enum...) are compatible only if they are the same (checked in trivial cases).
-    throw Exception(getExceptionMessagePrefix(types), ErrorCodes::NO_COMMON_TYPE);
+    return throw_or_return(getExceptionMessagePrefix(types), ErrorCodes::NO_COMMON_TYPE);
 }
 
 DataTypePtr tryGetLeastSupertype(const DataTypes & types)
diff --git a/src/DataTypes/getLeastSupertype.h b/src/DataTypes/getLeastSupertype.h
index c35ec7d722c9..5444bb34d063 100644
--- a/src/DataTypes/getLeastSupertype.h
+++ b/src/DataTypes/getLeastSupertype.h
@@ -7,12 +7,16 @@ namespace DB
 {
 
 /** Get data type that covers all possible values of passed data types.
-  * If there is no such data type, throws an exception.
+  * If there is no such data type, throws an exception
+  * or if 'allow_conversion_to_string' is true returns String as common type.
   *
   * Examples: least common supertype for UInt8, Int8 - Int16.
   * Examples: there is no least common supertype for Array(UInt8), Int8.
   */
-DataTypePtr getLeastSupertype(const DataTypes & types);
+DataTypePtr getLeastSupertype(const DataTypes & types, bool allow_conversion_to_string = false);
+
+using TypeIndexSet = std::unordered_set<TypeIndex>;
+DataTypePtr getLeastSupertype(const TypeIndexSet & types, bool allow_conversion_to_string = false);
 
 /// Same as above but return nullptr instead of throwing exception.
 DataTypePtr tryGetLeastSupertype(const DataTypes & types);
diff --git a/src/Formats/JSONEachRowUtils.cpp b/src/Formats/JSONEachRowUtils.cpp
index 8189164c98b0..66e0538fef11 100644
--- a/src/Formats/JSONEachRowUtils.cpp
+++ b/src/Formats/JSONEachRowUtils.cpp
@@ -9,9 +9,9 @@
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/DataTypeTuple.h>
 #include <DataTypes/DataTypeMap.h>
-#include <Functions/SimdJSONParser.h>
-#include <Functions/RapidJSONParser.h>
-#include <Functions/DummyJSONParser.h>
+#include <Common/JSONParsers/SimdJSONParser.h>
+#include <Common/JSONParsers/RapidJSONParser.h>
+#include <Common/JSONParsers/DummyJSONParser.h>
 
 #include <base/find_symbols.h>
 
diff --git a/src/Formats/registerFormats.cpp b/src/Formats/registerFormats.cpp
index b7b3b51cd7bb..78851e5ebb0b 100644
--- a/src/Formats/registerFormats.cpp
+++ b/src/Formats/registerFormats.cpp
@@ -74,6 +74,7 @@ void registerOutputFormatCapnProto(FormatFactory & factory);
 
 void registerInputFormatRegexp(FormatFactory & factory);
 void registerInputFormatJSONAsString(FormatFactory & factory);
+void registerInputFormatJSONAsObject(FormatFactory & factory);
 void registerInputFormatLineAsString(FormatFactory & factory);
 void registerInputFormatCapnProto(FormatFactory & factory);
 
@@ -84,6 +85,7 @@ void registerInputFormatHiveText(FormatFactory & factory);
 /// Non trivial prefix and suffix checkers for disabling parallel parsing.
 void registerNonTrivialPrefixAndSuffixCheckerJSONEachRow(FormatFactory & factory);
 void registerNonTrivialPrefixAndSuffixCheckerJSONAsString(FormatFactory & factory);
+void registerNonTrivialPrefixAndSuffixCheckerJSONAsObject(FormatFactory & factory);
 
 void registerArrowSchemaReader(FormatFactory & factory);
 void registerParquetSchemaReader(FormatFactory & factory);
@@ -175,6 +177,7 @@ void registerFormats()
     registerInputFormatRegexp(factory);
     registerInputFormatJSONAsString(factory);
     registerInputFormatLineAsString(factory);
+    registerInputFormatJSONAsObject(factory);
 #if USE_HIVE
     registerInputFormatHiveText(factory);
 #endif
@@ -183,6 +186,7 @@ void registerFormats()
 
     registerNonTrivialPrefixAndSuffixCheckerJSONEachRow(factory);
     registerNonTrivialPrefixAndSuffixCheckerJSONAsString(factory);
+    registerNonTrivialPrefixAndSuffixCheckerJSONAsObject(factory);
 
     registerArrowSchemaReader(factory);
     registerParquetSchemaReader(factory);
diff --git a/src/Functions/FunctionSQLJSON.h b/src/Functions/FunctionSQLJSON.h
index 56d29e0c776e..e45951e3ec55 100644
--- a/src/Functions/FunctionSQLJSON.h
+++ b/src/Functions/FunctionSQLJSON.h
@@ -8,13 +8,13 @@
 #include <Core/Settings.h>
 #include <DataTypes/DataTypeString.h>
 #include <DataTypes/DataTypesNumber.h>
-#include <Functions/DummyJSONParser.h>
+#include <Common/JSONParsers/DummyJSONParser.h>
 #include <Functions/IFunction.h>
 #include <Functions/JSONPath/ASTs/ASTJSONPath.h>
 #include <Functions/JSONPath/Generator/GeneratorJSONPath.h>
 #include <Functions/JSONPath/Parsers/ParserJSONPath.h>
-#include <Functions/RapidJSONParser.h>
-#include <Functions/SimdJSONParser.h>
+#include <Common/JSONParsers/RapidJSONParser.h>
+#include <Common/JSONParsers/SimdJSONParser.h>
 #include <Interpreters/Context.h>
 #include <Parsers/IParser.h>
 #include <Parsers/Lexer.h>
diff --git a/src/Functions/FunctionsComparison.h b/src/Functions/FunctionsComparison.h
index 0d0195eb2d7b..48170d6f5645 100644
--- a/src/Functions/FunctionsComparison.h
+++ b/src/Functions/FunctionsComparison.h
@@ -1055,7 +1055,7 @@ class FunctionComparison : public IFunction
 
     ColumnPtr executeGeneric(const ColumnWithTypeAndName & c0, const ColumnWithTypeAndName & c1) const
     {
-        DataTypePtr common_type = getLeastSupertype({c0.type, c1.type});
+        DataTypePtr common_type = getLeastSupertype(DataTypes{c0.type, c1.type});
 
         ColumnPtr c0_converted = castColumn(c0, common_type);
         ColumnPtr c1_converted = castColumn(c1, common_type);
@@ -1228,7 +1228,7 @@ class FunctionComparison : public IFunction
             // Comparing Date/Date32 and DateTime64 requires implicit conversion,
             if (date_and_datetime && (isDateOrDate32(left_type) || isDateOrDate32(right_type)))
             {
-                DataTypePtr common_type = getLeastSupertype({left_type, right_type});
+                DataTypePtr common_type = getLeastSupertype(DataTypes{left_type, right_type});
                 ColumnPtr c0_converted = castColumn(col_with_type_and_name_left, common_type);
                 ColumnPtr c1_converted = castColumn(col_with_type_and_name_right, common_type);
                 return executeDecimal({c0_converted, common_type, "left"}, {c1_converted, common_type, "right"});
@@ -1258,7 +1258,7 @@ class FunctionComparison : public IFunction
         }
         else if (date_and_datetime)
         {
-            DataTypePtr common_type = getLeastSupertype({left_type, right_type});
+            DataTypePtr common_type = getLeastSupertype(DataTypes{left_type, right_type});
             ColumnPtr c0_converted = castColumn(col_with_type_and_name_left, common_type);
             ColumnPtr c1_converted = castColumn(col_with_type_and_name_right, common_type);
             if (!((res = executeNumLeftType<UInt32>(c0_converted.get(), c1_converted.get()))
diff --git a/src/Functions/FunctionsConversion.h b/src/Functions/FunctionsConversion.h
index 5e11cab7e79d..f964558cd97c 100644
--- a/src/Functions/FunctionsConversion.h
+++ b/src/Functions/FunctionsConversion.h
@@ -25,6 +25,9 @@
 #include <DataTypes/DataTypeUUID.h>
 #include <DataTypes/DataTypeInterval.h>
 #include <DataTypes/DataTypeAggregateFunction.h>
+#include <DataTypes/DataTypeObject.h>
+#include <DataTypes/ObjectUtils.h>
+#include <DataTypes/DataTypeNested.h>
 #include <DataTypes/Serializations/SerializationDecimal.h>
 #include <Formats/FormatSettings.h>
 #include <Columns/ColumnString.h>
@@ -34,6 +37,7 @@
 #include <Columns/ColumnNullable.h>
 #include <Columns/ColumnTuple.h>
 #include <Columns/ColumnMap.h>
+#include <Columns/ColumnObject.h>
 #include <Columns/ColumnsCommon.h>
 #include <Columns/ColumnStringHelpers.h>
 #include <Common/assert_cast.h>
@@ -2934,20 +2938,61 @@ class FunctionCast final : public FunctionCastBase
             throw Exception{"CAST AS Tuple can only be performed between tuple types or from String.
Left type: "
                 + from_type_untyped->getName() + ", right type: " + to_type->getName(), ErrorCodes::TYPE_MISMATCH};
 
-        if (from_type->getElements().size() != to_type->getElements().size())
-            throw Exception{"CAST AS Tuple can only be performed between tuple types with the same number of elements or from String.
"
-                "Left type: " + from_type->getName() + ", right type: " + to_type->getName(), ErrorCodes::TYPE_MISMATCH};
-
         const auto & from_element_types = from_type->getElements();
         const auto & to_element_types = to_type->getElements();
-        auto element_wrappers = getElementWrappers(from_element_types, to_element_types);
 
-        return [element_wrappers, from_element_types, to_element_types]
+        std::vector<WrapperType> element_wrappers;
+        std::vector<std::optional<size_t>> to_reverse_index;
+
+        /// For named tuples allow conversions for tuples with
+        /// different sets of elements. If element exists in @to_type
+        /// and doesn't exist in @to_type it will be filled by default values.
+        if (from_type->haveExplicitNames() && from_type->serializeNames()
+            && to_type->haveExplicitNames() && to_type->serializeNames())
+        {
+            const auto & from_names = from_type->getElementNames();
+            std::unordered_map<String, size_t> from_positions;
+            from_positions.reserve(from_names.size());
+            for (size_t i = 0; i < from_names.size(); ++i)
+                from_positions[from_names[i]] = i;
+
+            const auto & to_names = to_type->getElementNames();
+            element_wrappers.reserve(to_names.size());
+            to_reverse_index.reserve(from_names.size());
+
+            for (size_t i = 0; i < to_names.size(); ++i)
+            {
+                auto it = from_positions.find(to_names[i]);
+                if (it != from_positions.end())
+                {
+                    element_wrappers.emplace_back(prepareUnpackDictionaries(from_element_types[it->second], to_element_types[i]));
+                    to_reverse_index.emplace_back(it->second);
+                }
+                else
+                {
+                    element_wrappers.emplace_back();
+                    to_reverse_index.emplace_back();
+                }
+            }
+        }
+        else
+        {
+            if (from_element_types.size() != to_element_types.size())
+                throw Exception{"CAST AS Tuple can only be performed between tuple types with the same number of elements or from String.
"
+                    "Left type: " + from_type->getName() + ", right type: " + to_type->getName(), ErrorCodes::TYPE_MISMATCH};
+
+            element_wrappers = getElementWrappers(from_element_types, to_element_types);
+            to_reverse_index.reserve(to_element_types.size());
+            for (size_t i = 0; i < to_element_types.size(); ++i)
+                to_reverse_index.emplace_back(i);
+        }
+
+        return [element_wrappers, from_element_types, to_element_types, to_reverse_index]
             (ColumnsWithTypeAndName & arguments, const DataTypePtr &, const ColumnNullable * nullable_source, size_t input_rows_count) -> ColumnPtr
         {
             const auto * col = arguments.front().column.get();
 
-            size_t tuple_size = from_element_types.size();
+            size_t tuple_size = to_element_types.size();
             const ColumnTuple & column_tuple = typeid_cast<const ColumnTuple &>(*col);
 
             Columns converted_columns(tuple_size);
@@ -2955,8 +3000,16 @@ class FunctionCast final : public FunctionCastBase
             /// invoke conversion for each element
             for (size_t i = 0; i < tuple_size; ++i)
             {
-                ColumnsWithTypeAndName element = {{column_tuple.getColumns()[i], from_element_types[i], "" }};
-                converted_columns[i] = element_wrappers[i](element, to_element_types[i], nullable_source, input_rows_count);
+                if (to_reverse_index[i])
+                {
+                    size_t from_idx = *to_reverse_index[i];
+                    ColumnsWithTypeAndName element = {{column_tuple.getColumns()[from_idx], from_element_types[from_idx], "" }};
+                    converted_columns[i] = element_wrappers[i](element, to_element_types[i], nullable_source, input_rows_count);
+                }
+                else
+                {
+                    converted_columns[i] = to_element_types[i]->createColumn()->cloneResized(input_rows_count);
+                }
             }
 
             return ColumnTuple::create(converted_columns);
@@ -3077,6 +3130,68 @@ class FunctionCast final : public FunctionCastBase
         }
     }
 
+    WrapperType createObjectWrapper(const DataTypePtr & from_type, const DataTypeObject * to_type) const
+    {
+        if (const auto * from_tuple = checkAndGetDataType<DataTypeTuple>(from_type.get()))
+        {
+            if (!from_tuple->haveExplicitNames())
+                 throw Exception(ErrorCodes::TYPE_MISMATCH,
+                    "Cast to Object can be performed only from flatten Named Tuple. Got: {}", from_type->getName());
+
+            PathsInData paths;
+            DataTypes from_types;
+
+            std::tie(paths, from_types) = flattenTuple(from_type);
+            auto to_types = from_types;
+
+            for (auto & type : to_types)
+            {
+                if (isTuple(type) || isNested(type))
+                     throw Exception(ErrorCodes::TYPE_MISMATCH,
+                        "Cast to Object can be performed only from flatten Named Tuple. Got: {}", from_type->getName());
+
+                type = recursiveRemoveLowCardinality(type);
+            }
+
+            return [element_wrappers = getElementWrappers(from_types, to_types),
+                has_nullable_subcolumns = to_type->hasNullableSubcolumns(), from_types, to_types, paths]
+                (ColumnsWithTypeAndName & arguments, const DataTypePtr &, const ColumnNullable * nullable_source, size_t input_rows_count)
+            {
+                size_t tuple_size = to_types.size();
+                auto flattened_column = flattenTuple(arguments.front().column);
+                const auto & column_tuple = assert_cast<const ColumnTuple &>(*flattened_column);
+
+                if (tuple_size != column_tuple.getColumns().size())
+                    throw Exception(ErrorCodes::TYPE_MISMATCH,
+                        "Expected tuple with {} subcolumn, but got {} subcolumns",
+                        tuple_size, column_tuple.getColumns().size());
+
+                auto res = ColumnObject::create(has_nullable_subcolumns);
+                for (size_t i = 0; i < tuple_size; ++i)
+                {
+                    ColumnsWithTypeAndName element = {{column_tuple.getColumns()[i], from_types[i], "" }};
+                    auto converted_column = element_wrappers[i](element, to_types[i], nullable_source, input_rows_count);
+                    res->addSubcolumn(paths[i], converted_column->assumeMutable());
+                }
+
+                return res;
+            };
+        }
+        else if (checkAndGetDataType<DataTypeString>(from_type.get()))
+        {
+            return [] (ColumnsWithTypeAndName & arguments, const DataTypePtr & result_type, const ColumnNullable * nullable_source, size_t input_rows_count)
+            {
+                auto res = ConvertImplGenericFromString<ColumnString>::execute(arguments, result_type, nullable_source, input_rows_count);
+                auto & res_object = assert_cast<ColumnObject &>(res->assumeMutableRef());
+                res_object.finalize();
+                return res;
+            };
+        }
+
+        throw Exception(ErrorCodes::TYPE_MISMATCH,
+            "Cast to Object can be performed only from flatten named tuple or string. Got: {}", from_type->getName());
+    }
+
     template <typename FieldType>
     WrapperType createEnumWrapper(const DataTypePtr & from_type, const DataTypeEnum<FieldType> * to_type) const
     {
@@ -3496,6 +3611,8 @@ class FunctionCast final : public FunctionCastBase
                 return createTupleWrapper(from_type, checkAndGetDataType<DataTypeTuple>(to_type.get()));
             case TypeIndex::Map:
                 return createMapWrapper(from_type, checkAndGetDataType<DataTypeMap>(to_type.get()));
+            case TypeIndex::Object:
+                return createObjectWrapper(from_type, checkAndGetDataType<DataTypeObject>(to_type.get()));
             case TypeIndex::AggregateFunction:
                 return createAggregateFunctionWrapper(from_type, checkAndGetDataType<DataTypeAggregateFunction>(to_type.get()));
             default:
diff --git a/src/Functions/FunctionsJSON.cpp b/src/Functions/FunctionsJSON.cpp
index d542f0236251..e8f9f73b805d 100644
--- a/src/Functions/FunctionsJSON.cpp
+++ b/src/Functions/FunctionsJSON.cpp
@@ -35,9 +35,9 @@
 
 #include <Functions/FunctionFactory.h>
 #include <Functions/IFunction.h>
-#include <Functions/DummyJSONParser.h>
-#include <Functions/SimdJSONParser.h>
-#include <Functions/RapidJSONParser.h>
+#include <Common/JSONParsers/DummyJSONParser.h>
+#include <Common/JSONParsers/SimdJSONParser.h>
+#include <Common/JSONParsers/RapidJSONParser.h>
 #include <Functions/FunctionHelpers.h>
 
 #include <Interpreters/Context.h>
diff --git a/src/Functions/FunctionsRound.h b/src/Functions/FunctionsRound.h
index 1a8bf85167f6..518b969d4411 100644
--- a/src/Functions/FunctionsRound.h
+++ b/src/Functions/FunctionsRound.h
@@ -663,7 +663,7 @@ class FunctionRoundDown : public IFunction
             throw Exception{"Elements of array of second argument of function " + getName()
                             + " must be numeric type.", ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};
         }
-        return getLeastSupertype({type_x, type_arr_nested});
+        return getLeastSupertype(DataTypes{type_x, type_arr_nested});
     }
 
     ColumnPtr executeImpl(const ColumnsWithTypeAndName & arguments, const DataTypePtr & result_type, size_t) const override
diff --git a/src/Functions/array/arrayIndex.h b/src/Functions/array/arrayIndex.h
index 35c731dfc781..8b42b99cd69f 100644
--- a/src/Functions/array/arrayIndex.h
+++ b/src/Functions/array/arrayIndex.h
@@ -474,7 +474,7 @@ class FunctionArrayIndex : public IFunction
         auto arg_decayed = removeNullable(removeLowCardinality(arg));
 
         return ((isNativeNumber(inner_type_decayed) || isEnum(inner_type_decayed)) && isNativeNumber(arg_decayed))
-            || getLeastSupertype({inner_type_decayed, arg_decayed});
+            || getLeastSupertype(DataTypes{inner_type_decayed, arg_decayed});
     }
 
     /**
@@ -1045,7 +1045,7 @@ class FunctionArrayIndex : public IFunction
         DataTypePtr array_elements_type = assert_cast<const DataTypeArray &>(*arguments[0].type).getNestedType();
         const DataTypePtr & index_type = arguments[1].type;
 
-        DataTypePtr common_type = getLeastSupertype({array_elements_type, index_type});
+        DataTypePtr common_type = getLeastSupertype(DataTypes{array_elements_type, index_type});
 
         ColumnPtr col_nested = castColumn({ col->getDataPtr(), array_elements_type, "" }, common_type);
 
diff --git a/src/Functions/array/arrayResize.cpp b/src/Functions/array/arrayResize.cpp
index 9d2a29b2fb41..1e6dcfbf0690 100644
--- a/src/Functions/array/arrayResize.cpp
+++ b/src/Functions/array/arrayResize.cpp
@@ -62,7 +62,7 @@ class FunctionArrayResize : public IFunction
         if (number_of_arguments == 2)
             return arguments[0];
         else /* if (number_of_arguments == 3) */
-            return std::make_shared<DataTypeArray>(getLeastSupertype({array_type->getNestedType(), arguments[2]}));
+            return std::make_shared<DataTypeArray>(getLeastSupertype(DataTypes{array_type->getNestedType(), arguments[2]}));
     }
 
     ColumnPtr executeImpl(const ColumnsWithTypeAndName & arguments, const DataTypePtr & return_type, size_t input_rows_count) const override
diff --git a/src/Functions/if.cpp b/src/Functions/if.cpp
index 0b30f404f8ea..730612745efa 100644
--- a/src/Functions/if.cpp
+++ b/src/Functions/if.cpp
@@ -632,7 +632,7 @@ class FunctionIf : public FunctionIfBase
         const ColumnWithTypeAndName & arg1 = arguments[1];
         const ColumnWithTypeAndName & arg2 = arguments[2];
 
-        DataTypePtr common_type = getLeastSupertype({arg1.type, arg2.type});
+        DataTypePtr common_type = getLeastSupertype(DataTypes{arg1.type, arg2.type});
 
         ColumnPtr col_then = castColumn(arg1, common_type);
         ColumnPtr col_else = castColumn(arg2, common_type);
@@ -1022,7 +1022,7 @@ class FunctionIf : public FunctionIfBase
             throw Exception("Illegal type " + arguments[0]->getName() + " of first argument (condition) of function if. Must be UInt8.",
                 ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT);
 
-        return getLeastSupertype({arguments[1], arguments[2]});
+        return getLeastSupertype(DataTypes{arguments[1], arguments[2]});
     }
 
     ColumnPtr executeImpl(const ColumnsWithTypeAndName & args, const DataTypePtr & result_type, size_t input_rows_count) const override
diff --git a/src/Functions/ifNull.cpp b/src/Functions/ifNull.cpp
index 31880b81a41c..ab8e2677d281 100644
--- a/src/Functions/ifNull.cpp
+++ b/src/Functions/ifNull.cpp
@@ -47,7 +47,7 @@ class FunctionIfNull : public IFunction
         if (!arguments[0]->isNullable())
             return arguments[0];
 
-        return getLeastSupertype({removeNullable(arguments[0]), arguments[1]});
+        return getLeastSupertype(DataTypes{removeNullable(arguments[0]), arguments[1]});
     }
 
     ColumnPtr executeImpl(const ColumnsWithTypeAndName & arguments, const DataTypePtr & result_type, size_t input_rows_count) const override
diff --git a/src/Functions/neighbor.cpp b/src/Functions/neighbor.cpp
index a1254446e01f..ab447e61aed2 100644
--- a/src/Functions/neighbor.cpp
+++ b/src/Functions/neighbor.cpp
@@ -78,7 +78,7 @@ class FunctionNeighbor : public IFunction
 
         // check that default value column has supertype with first argument
         if (number_of_arguments == 3)
-            return getLeastSupertype({arguments[0], arguments[2]});
+            return getLeastSupertype(DataTypes{arguments[0], arguments[2]});
 
         return arguments[0];
     }
diff --git a/src/Functions/transform.cpp b/src/Functions/transform.cpp
index b7e1db59c233..de9f1a5ba05c 100644
--- a/src/Functions/transform.cpp
+++ b/src/Functions/transform.cpp
@@ -117,7 +117,7 @@ class FunctionTransform : public IFunction
                     + " has signature: transform(T, Array(T), Array(U), U) -> U; or transform(T, Array(T), Array(T)) -> T; where T and U are types.",
                     ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};
 
-            return getLeastSupertype({type_x, type_arr_to_nested});
+            return getLeastSupertype(DataTypes{type_x, type_arr_to_nested});
         }
         else
         {
@@ -140,7 +140,7 @@ class FunctionTransform : public IFunction
             if (type_arr_to_nested->isValueRepresentedByNumber() && type_default->isValueRepresentedByNumber())
             {
                 /// We take the smallest common type for the elements of the array of values `to` and for `default`.
-                return getLeastSupertype({type_arr_to_nested, type_default});
+                return getLeastSupertype(DataTypes{type_arr_to_nested, type_default});
             }
 
             /// TODO More checks.
diff --git a/src/IO/ReadHelpers.cpp b/src/IO/ReadHelpers.cpp
index a61258181552..e086f16be54f 100644
--- a/src/IO/ReadHelpers.cpp
+++ b/src/IO/ReadHelpers.cpp
@@ -248,7 +248,7 @@ void readString(String & s, ReadBuffer & buf)
 }
 
 template void readStringInto<PaddedPODArray<UInt8>>(PaddedPODArray<UInt8> & s, ReadBuffer & buf);
-
+template void readStringInto<String>(String & s, ReadBuffer & buf);
 
 template <typename Vector>
 void readStringUntilEOFInto(Vector & s, ReadBuffer & buf)
@@ -580,6 +580,7 @@ void readQuotedStringWithSQLStyle(String & s, ReadBuffer & buf)
 
 
 template void readQuotedStringInto<true>(PaddedPODArray<UInt8> & s, ReadBuffer & buf);
+template void readQuotedStringInto<true>(String & s, ReadBuffer & buf);
 template void readDoubleQuotedStringInto<false>(NullOutput & s, ReadBuffer & buf);
 
 void readDoubleQuotedString(String & s, ReadBuffer & buf)
@@ -782,6 +783,68 @@ template bool readJSONStringInto<PaddedPODArray<UInt8>, bool>(PaddedPODArray<UIn
 template void readJSONStringInto<NullOutput>(NullOutput & s, ReadBuffer & buf);
 template void readJSONStringInto<String>(String & s, ReadBuffer & buf);
 
+template <typename Vector, typename ReturnType>
+ReturnType readJSONObjectPossiblyInvalid(Vector & s, ReadBuffer & buf)
+{
+    static constexpr bool throw_exception = std::is_same_v<ReturnType, void>;
+
+    auto error = [](const char * message [[maybe_unused]], int code [[maybe_unused]])
+    {
+        if constexpr (throw_exception)
+            throw ParsingException(message, code);
+        return ReturnType(false);
+    };
+
+    if (buf.eof() || *buf.position() != '{')
+        return error("JSON should start from opening curly bracket", ErrorCodes::INCORRECT_DATA);
+
+    s.push_back(*buf.position());
+    ++buf.position();
+
+    Int64 balance = 1;
+    bool quotes = false;
+
+    while (!buf.eof())
+    {
+        char * next_pos = find_first_symbols<'\\', '{', '}', '"'>(buf.position(), buf.buffer().end());
+        appendToStringOrVector(s, buf, next_pos);
+        buf.position() = next_pos;
+
+        if (!buf.hasPendingData())
+            continue;
+
+        s.push_back(*buf.position());
+
+        if (*buf.position() == '\\')
+        {
+            ++buf.position();
+            if (!buf.eof())
+            {
+                s.push_back(*buf.position());
+                ++buf.position();
+            }
+
+            continue;
+        }
+
+        if (*buf.position() == '"')
+            quotes = !quotes;
+        else if (!quotes) // can be only '{' or '}'
+            balance += *buf.position() == '{' ? 1 : -1;
+
+        ++buf.position();
+
+        if (balance == 0)
+            return ReturnType(true);
+
+        if (balance <    0)
+            break;
+    }
+
+    return error("JSON should have equal number of opening and closing brackets", ErrorCodes::INCORRECT_DATA);
+}
+
+template void readJSONObjectPossiblyInvalid<String>(String & s, ReadBuffer & buf);
 
 template <typename ReturnType>
 ReturnType readDateTextFallback(LocalDate & date, ReadBuffer & buf)
diff --git a/src/IO/ReadHelpers.h b/src/IO/ReadHelpers.h
index fd2c4218aefb..6247ad4bb102 100644
--- a/src/IO/ReadHelpers.h
+++ b/src/IO/ReadHelpers.h
@@ -601,6 +601,12 @@ bool tryReadJSONStringInto(Vector & s, ReadBuffer & buf)
     return readJSONStringInto<Vector, bool>(s, buf);
 }
 
+/// Reads chunk of data between {} in that way,
+/// that it has balanced parentheses sequence of {}.
+/// So, it may form a JSON object, but it can be incorrenct.
+template <typename Vector, typename ReturnType = void>
+ReturnType readJSONObjectPossiblyInvalid(Vector & s, ReadBuffer & buf);
+
 template <typename Vector>
 void readStringUntilWhitespaceInto(Vector & s, ReadBuffer & buf);
 
diff --git a/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp b/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp
index 5d35525aee96..2b92fab15deb 100644
--- a/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp
+++ b/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp
@@ -8,6 +8,7 @@
 #include <TableFunctions/TableFunctionFactory.h>
 #include <IO/ConnectionTimeoutsContext.h>
 #include <Interpreters/RequiredSourceColumnsVisitor.h>
+#include <DataTypes/ObjectUtils.h>
 
 #include <base/logger_useful.h>
 #include <Processors/QueryPlan/QueryPlan.h>
@@ -35,9 +36,13 @@ namespace ClusterProxy
 
 SelectStreamFactory::SelectStreamFactory(
     const Block & header_,
+    const ColumnsDescriptionByShardNum & objects_by_shard_,
+    const StorageSnapshotPtr & storage_snapshot_,
     QueryProcessingStage::Enum processed_stage_)
-    : header(header_)
-    , processed_stage{processed_stage_}
+    : header(header_),
+    objects_by_shard(objects_by_shard_),
+    storage_snapshot(storage_snapshot_),
+    processed_stage(processed_stage_)
 {
 }
 
@@ -100,6 +105,10 @@ void SelectStreamFactory::createForShard(
     Shards & remote_shards,
     UInt32 shard_count)
 {
+    auto it = objects_by_shard.find(shard_info.shard_num);
+    if (it != objects_by_shard.end())
+        replaceMissedSubcolumnsByConstants(storage_snapshot->object_columns, it->second, query_ast);
+
     auto emplace_local_stream = [&]()
     {
         local_plans.emplace_back(createLocalPlan(query_ast, header, context, processed_stage, shard_info.shard_num, shard_count));
diff --git a/src/Interpreters/ClusterProxy/SelectStreamFactory.h b/src/Interpreters/ClusterProxy/SelectStreamFactory.h
index 55e81feee338..731bf3acd103 100644
--- a/src/Interpreters/ClusterProxy/SelectStreamFactory.h
+++ b/src/Interpreters/ClusterProxy/SelectStreamFactory.h
@@ -4,6 +4,7 @@
 #include <Interpreters/ClusterProxy/IStreamFactory.h>
 #include <Interpreters/StorageID.h>
 #include <Storages/IStorage_fwd.h>
+#include <Storages/StorageSnapshot.h>
 
 namespace DB
 {
@@ -11,11 +12,15 @@ namespace DB
 namespace ClusterProxy
 {
 
+using ColumnsDescriptionByShardNum = std::unordered_map<UInt32, ColumnsDescription>;
+
 class SelectStreamFactory final : public IStreamFactory
 {
 public:
     SelectStreamFactory(
         const Block & header_,
+        const ColumnsDescriptionByShardNum & objects_by_shard_,
+        const StorageSnapshotPtr & storage_snapshot_,
         QueryProcessingStage::Enum processed_stage_);
 
     void createForShard(
@@ -30,6 +35,8 @@ class SelectStreamFactory final : public IStreamFactory
 
 private:
     const Block header;
+    const ColumnsDescriptionByShardNum objects_by_shard;
+    const StorageSnapshotPtr storage_snapshot;
     QueryProcessingStage::Enum processed_stage;
 };
 
diff --git a/src/Interpreters/ClusterProxy/executeQuery.cpp b/src/Interpreters/ClusterProxy/executeQuery.cpp
index 884b8445732b..3f1823fb1719 100644
--- a/src/Interpreters/ClusterProxy/executeQuery.cpp
+++ b/src/Interpreters/ClusterProxy/executeQuery.cpp
@@ -1,5 +1,5 @@
 #include <Interpreters/ClusterProxy/executeQuery.h>
-#include <Interpreters/ClusterProxy/IStreamFactory.h>
+#include <Interpreters/ClusterProxy/SelectStreamFactory.h>
 #include <Core/Settings.h>
 #include <Interpreters/Context.h>
 #include <Interpreters/Cluster.h>
diff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp
index ba4063b4c388..f7dbd1c8b65d 100644
--- a/src/Interpreters/InterpreterCreateQuery.cpp
+++ b/src/Interpreters/InterpreterCreateQuery.cpp
@@ -53,6 +53,7 @@
 #include <DataTypes/DataTypeLowCardinality.h>
 #include <DataTypes/DataTypeNullable.h>
 #include <DataTypes/DataTypeAggregateFunction.h>
+#include <DataTypes/ObjectUtils.h>
 
 #include <Databases/DatabaseFactory.h>
 #include <Databases/DatabaseReplicated.h>
@@ -733,11 +734,26 @@ void InterpreterCreateQuery::validateTableStructure(const ASTCreateQuery & creat
             {
                 String message = "Cannot create table with column '" + name_and_type_pair.name + "' which type is '"
                                  + type + "' because experimental geo types are not allowed. "
-                                 + "Set setting allow_experimental_geo_types = 1 in order to allow it.";
+                                 + "Set setting allow_experimental_geo_types = 1 in order to allow it";
                 throw Exception(message, ErrorCodes::ILLEGAL_COLUMN);
             }
         }
     }
+
+    if (!create.attach && !settings.allow_experimental_object_type)
+    {
+        for (const auto & [name, type] : properties.columns.getAllPhysical())
+        {
+            if (isObject(type))
+            {
+                throw Exception(ErrorCodes::ILLEGAL_COLUMN,
+                    "Cannot create table with column '{}' which type is '{}' "
+                    "because experimental Object type is not allowed. "
+                    "Set setting allow_experimental_object_type = 1 in order to allow it",
+                    name, type->getName());
+            }
+        }
+    }
 }
 
 String InterpreterCreateQuery::getTableEngineName(DefaultTableEngine default_table_engine)
@@ -1230,6 +1246,14 @@ bool InterpreterCreateQuery::doCreateTable(ASTCreateQuery & create,
     /// we can safely destroy the object without a call to "shutdown", because there is guarantee
     /// that no background threads/similar resources remain after exception from "startup".
 
+    if (!res->supportsDynamicSubcolumns() && hasObjectColumns(res->getInMemoryMetadataPtr()->getColumns()))
+    {
+        throw Exception(ErrorCodes::ILLEGAL_COLUMN,
+            "Cannot create table with column of type Object, "
+            "because storage {} doesn't support dynamic subcolumns",
+            res->getName());
+    }
+
     res->startup();
     return true;
 }
diff --git a/src/Interpreters/InterpreterDescribeQuery.cpp b/src/Interpreters/InterpreterDescribeQuery.cpp
index 36ea2949b6af..da5fcedd4693 100644
--- a/src/Interpreters/InterpreterDescribeQuery.cpp
+++ b/src/Interpreters/InterpreterDescribeQuery.cpp
@@ -64,9 +64,12 @@ Block InterpreterDescribeQuery::getSampleBlock(bool include_subcolumns)
 BlockIO InterpreterDescribeQuery::execute()
 {
     ColumnsDescription columns;
+    StorageSnapshotPtr storage_snapshot;
 
     const auto & ast = query_ptr->as<ASTDescribeQuery &>();
     const auto & table_expression = ast.table_expression->as<ASTTableExpression &>();
+    const auto & settings = getContext()->getSettingsRef();
+
     if (table_expression.subquery)
     {
         auto names_and_types = InterpreterSelectWithUnionQuery::getSampleBlock(
@@ -83,19 +86,27 @@ BlockIO InterpreterDescribeQuery::execute()
         auto table_id = getContext()->resolveStorageID(table_expression.database_and_table_name);
         getContext()->checkAccess(AccessType::SHOW_COLUMNS, table_id);
         auto table = DatabaseCatalog::instance().getTable(table_id, getContext());
-        auto table_lock = table->lockForShare(getContext()->getInitialQueryId(), getContext()->getSettingsRef().lock_acquire_timeout);
+        auto table_lock = table->lockForShare(getContext()->getInitialQueryId(), settings.lock_acquire_timeout);
+
         auto metadata_snapshot = table->getInMemoryMetadataPtr();
+        storage_snapshot = table->getStorageSnapshot(metadata_snapshot);
         columns = metadata_snapshot->getColumns();
     }
 
-    bool include_subcolumns = getContext()->getSettingsRef().describe_include_subcolumns;
+    bool extend_object_types = settings.describe_extend_object_types && storage_snapshot;
+    bool include_subcolumns = settings.describe_include_subcolumns;
+
     Block sample_block = getSampleBlock(include_subcolumns);
     MutableColumns res_columns = sample_block.cloneEmptyColumns();
 
     for (const auto & column : columns)
     {
         res_columns[0]->insert(column.name);
-        res_columns[1]->insert(column.type->getName());
+
+        if (extend_object_types)
+            res_columns[1]->insert(storage_snapshot->getConcreteType(column.name)->getName());
+        else
+            res_columns[1]->insert(column.type->getName());
 
         if (column.default_desc.expression)
         {
@@ -128,6 +139,8 @@ BlockIO InterpreterDescribeQuery::execute()
     {
         for (const auto & column : columns)
         {
+            auto type = extend_object_types ? storage_snapshot->getConcreteType(column.name) : column.type;
+
             IDataType::forEachSubcolumn([&](const auto & path, const auto & name, const auto & data)
             {
                 res_columns[0]->insert(Nested::concatenateName(column.name, name));
@@ -150,7 +163,7 @@ BlockIO InterpreterDescribeQuery::execute()
                     res_columns[6]->insertDefault();
 
                 res_columns[7]->insert(1u);
-            }, {column.type->getDefaultSerialization(), column.type, nullptr, nullptr});
+            }, { type->getDefaultSerialization(), type, nullptr, nullptr });
         }
     }
 
diff --git a/src/Interpreters/InterpreterOptimizeQuery.cpp b/src/Interpreters/InterpreterOptimizeQuery.cpp
index f9a701a0a77d..d4fe7604ced6 100644
--- a/src/Interpreters/InterpreterOptimizeQuery.cpp
+++ b/src/Interpreters/InterpreterOptimizeQuery.cpp
@@ -32,6 +32,7 @@ BlockIO InterpreterOptimizeQuery::execute()
     auto table_id = getContext()->resolveStorageID(ast, Context::ResolveOrdinary);
     StoragePtr table = DatabaseCatalog::instance().getTable(table_id, getContext());
     auto metadata_snapshot = table->getInMemoryMetadataPtr();
+    auto storage_snapshot = table->getStorageSnapshot(metadata_snapshot);
 
     // Empty list of names means we deduplicate by all columns, but user can explicitly state which columns to use.
     Names column_names;
@@ -46,7 +47,7 @@ BlockIO InterpreterOptimizeQuery::execute()
                 column_names.emplace_back(col->getColumnName());
         }
 
-        metadata_snapshot->check(column_names, NamesAndTypesList{}, table_id);
+        storage_snapshot->check(column_names);
         Names required_columns;
         {
             required_columns = metadata_snapshot->getColumnsRequiredForSortingKey();
diff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp
index f2fc17fbf9aa..ce0929f9c6e8 100644
--- a/src/Interpreters/InterpreterSelectQuery.cpp
+++ b/src/Interpreters/InterpreterSelectQuery.cpp
@@ -138,7 +138,7 @@ String InterpreterSelectQuery::generateFilterActions(ActionsDAGPtr & actions, co
     table_expr->children.push_back(table_expr->database_and_table_name);
 
     /// Using separate expression analyzer to prevent any possible alias injection
-    auto syntax_result = TreeRewriter(context).analyzeSelect(query_ast, TreeRewriterResult({}, storage, metadata_snapshot));
+    auto syntax_result = TreeRewriter(context).analyzeSelect(query_ast, TreeRewriterResult({}, storage, storage_snapshot));
     SelectQueryExpressionAnalyzer analyzer(query_ast, syntax_result, context, metadata_snapshot);
     actions = analyzer.simpleSelectActions();
 
@@ -328,6 +328,8 @@ InterpreterSelectQuery::InterpreterSelectQuery(
         table_id = storage->getStorageID();
         if (!metadata_snapshot)
             metadata_snapshot = storage->getInMemoryMetadataPtr();
+
+        storage_snapshot = storage->getStorageSnapshotForQuery(metadata_snapshot, query_ptr);
     }
 
     if (has_input || !joined_tables.resolveTables())
@@ -395,7 +397,7 @@ InterpreterSelectQuery::InterpreterSelectQuery(
 
         syntax_analyzer_result = TreeRewriter(context).analyzeSelect(
             query_ptr,
-            TreeRewriterResult(source_header.getNamesAndTypesList(), storage, metadata_snapshot),
+            TreeRewriterResult(source_header.getNamesAndTypesList(), storage, storage_snapshot),
             options, joined_tables.tablesWithColumns(), required_result_column_names, table_join);
 
         query_info.syntax_analyzer_result = syntax_analyzer_result;
@@ -516,7 +518,7 @@ InterpreterSelectQuery::InterpreterSelectQuery(
                 }
             }
 
-            source_header = metadata_snapshot->getSampleBlockForColumns(required_columns, storage->getVirtuals(), storage->getStorageID());
+            source_header = storage_snapshot->getSampleBlockForColumns(required_columns);
         }
 
         /// Calculate structure of the result.
@@ -582,6 +584,9 @@ InterpreterSelectQuery::InterpreterSelectQuery(
         analysis_result.required_columns = required_columns;
     }
 
+    if (query_info.projection)
+        storage_snapshot->addProjection(query_info.projection->desc);
+
     /// Blocks used in expression analysis contains size 1 const columns for constant folding and
     ///  null non-const columns to avoid useless memory allocations. However, a valid block sample
     ///  requires all columns to be of size 0, thus we need to sanitize the block here.
@@ -631,10 +636,9 @@ Block InterpreterSelectQuery::getSampleBlockImpl()
         query_analyzer->makeSetsForIndex(query.where());
         query_analyzer->makeSetsForIndex(query.prewhere());
         query_info.sets = query_analyzer->getPreparedSets();
-    }
 
-    if (storage && !options.only_analyze)
-        from_stage = storage->getQueryProcessingStage(context, options.to_stage, metadata_snapshot, query_info);
+        from_stage = storage->getQueryProcessingStage(context, options.to_stage, storage_snapshot, query_info);
+    }
 
     /// Do I need to perform the first part of the pipeline?
     /// Running on remote servers during distributed processing or if query is not distributed.
@@ -1724,7 +1728,7 @@ void InterpreterSelectQuery::addPrewhereAliasActions()
         }
 
         auto syntax_result
-            = TreeRewriter(context).analyze(required_columns_all_expr, required_columns_after_prewhere, storage, metadata_snapshot);
+            = TreeRewriter(context).analyze(required_columns_all_expr, required_columns_after_prewhere, storage, storage_snapshot);
         alias_actions = ExpressionAnalyzer(required_columns_all_expr, syntax_result, context).getActionsDAG(true);
 
         /// The set of required columns could be added as a result of adding an action to calculate ALIAS.
@@ -2000,7 +2004,7 @@ void InterpreterSelectQuery::executeFetchColumns(QueryProcessingStage::Enum proc
             quota = context->getQuota();
 
         query_info.settings_limit_offset_done = options.settings_limit_offset_done;
-        storage->read(query_plan, required_columns, metadata_snapshot, query_info, context, processing_stage, max_block_size, max_streams);
+        storage->read(query_plan, required_columns, storage_snapshot, query_info, context, processing_stage, max_block_size, max_streams);
 
         if (context->hasQueryContext() && !options.is_internal)
         {
@@ -2017,11 +2021,7 @@ void InterpreterSelectQuery::executeFetchColumns(QueryProcessingStage::Enum proc
         /// Create step which reads from empty source if storage has no data.
         if (!query_plan.isInitialized())
         {
-            auto header = query_info.projection
-                ? query_info.projection->desc->metadata->getSampleBlockForColumns(
-                    query_info.projection->required_columns, storage->getVirtuals(), storage->getStorageID())
-                : metadata_snapshot->getSampleBlockForColumns(required_columns, storage->getVirtuals(), storage->getStorageID());
-
+            auto header = storage_snapshot->getSampleBlockForColumns(required_columns);
             addEmptySourceToQueryPlan(query_plan, header, query_info, context);
         }
 
diff --git a/src/Interpreters/InterpreterSelectQuery.h b/src/Interpreters/InterpreterSelectQuery.h
index 4298cbbb7943..6bb12caff7d3 100644
--- a/src/Interpreters/InterpreterSelectQuery.h
+++ b/src/Interpreters/InterpreterSelectQuery.h
@@ -202,6 +202,7 @@ class InterpreterSelectQuery : public IInterpreterUnionOrSelectQuery
 
     Poco::Logger * log;
     StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
 
     /// Reuse already built sets for multiple passes of analysis, possibly across interpreters.
     PreparedSets prepared_sets;
diff --git a/src/Interpreters/MutationsInterpreter.cpp b/src/Interpreters/MutationsInterpreter.cpp
index 1c7b970e7312..5e795c5760a9 100644
--- a/src/Interpreters/MutationsInterpreter.cpp
+++ b/src/Interpreters/MutationsInterpreter.cpp
@@ -802,7 +802,9 @@ ASTPtr MutationsInterpreter::prepareInterpreterSelectQuery(std::vector<Stage> &
         /// e.g. ALTER referencing the same table in scalar subquery
         bool execute_scalar_subqueries = !dry_run;
         auto syntax_result = TreeRewriter(context).analyze(
-            all_asts, all_columns, storage, metadata_snapshot, false, true, execute_scalar_subqueries);
+            all_asts, all_columns, storage, storage->getStorageSnapshot(metadata_snapshot),
+            false, true, execute_scalar_subqueries);
+
         if (execute_scalar_subqueries && context->hasQueryContext())
             for (const auto & it : syntax_result->getScalars())
                 context->getQueryContext()->addScalar(it.first, it.second);
diff --git a/src/Interpreters/TableJoin.cpp b/src/Interpreters/TableJoin.cpp
index 8fda5371f687..7b7ccb689c38 100644
--- a/src/Interpreters/TableJoin.cpp
+++ b/src/Interpreters/TableJoin.cpp
@@ -551,7 +551,7 @@ void TableJoin::inferJoinKeyCommonType(const LeftNamesAndTypes & left, const Rig
         try
         {
             /// TODO(vdimir): use getMostSubtype if possible
-            common_type = DB::getLeastSupertype({ltype->second, rtype->second});
+            common_type = DB::getLeastSupertype(DataTypes{ltype->second, rtype->second});
         }
         catch (DB::Exception & ex)
         {
diff --git a/src/Interpreters/TableOverrideUtils.cpp b/src/Interpreters/TableOverrideUtils.cpp
index 922dd6af25b0..58e885380bf8 100644
--- a/src/Interpreters/TableOverrideUtils.cpp
+++ b/src/Interpreters/TableOverrideUtils.cpp
@@ -96,7 +96,7 @@ void TableOverrideAnalyzer::analyze(const StorageInMemoryMetadata & metadata, Re
         {
             auto * override_column = column_ast->as<ASTColumnDeclaration>();
             auto override_type = DataTypeFactory::instance().get(override_column->type);
-            auto found = metadata.columns.tryGetColumnOrSubcolumn(ColumnsDescription::GetFlags::All, override_column->name);
+            auto found = metadata.columns.tryGetColumnOrSubcolumn(GetColumnsOptions::All, override_column->name);
             std::optional<ColumnDefaultKind> override_default_kind;
             if (!override_column->default_specifier.empty())
                 override_default_kind = columnDefaultKindFromString(override_column->default_specifier);
diff --git a/src/Interpreters/TreeOptimizer.cpp b/src/Interpreters/TreeOptimizer.cpp
index 64b25ca9777c..23938beffc5f 100644
--- a/src/Interpreters/TreeOptimizer.cpp
+++ b/src/Interpreters/TreeOptimizer.cpp
@@ -445,7 +445,7 @@ void optimizeMonotonousFunctionsInOrderBy(ASTSelectQuery * select_query, Context
         }
     }
 
-    auto sorting_key_columns = result.metadata_snapshot ? result.metadata_snapshot->getSortingKeyColumns() : Names{};
+    auto sorting_key_columns = result.storage_snapshot ? result.storage_snapshot->metadata->getSortingKeyColumns() : Names{};
 
     bool is_sorting_key_prefix = true;
     for (size_t i = 0; i < order_by->children.size(); ++i)
@@ -740,9 +740,8 @@ void TreeOptimizer::apply(ASTPtr & query, TreeRewriterResult & result,
     if (!select_query)
         throw Exception("Select analyze for not select asts.", ErrorCodes::LOGICAL_ERROR);
 
-    if (settings.optimize_functions_to_subcolumns && result.storage
-        && result.storage->supportsSubcolumns() && result.metadata_snapshot)
-        optimizeFunctionsToSubcolumns(query, result.metadata_snapshot);
+    if (settings.optimize_functions_to_subcolumns && result.storage_snapshot && result.storage->supportsSubcolumns())
+        optimizeFunctionsToSubcolumns(query, result.storage_snapshot->metadata);
 
     /// Move arithmetic operations out of aggregation functions
     if (settings.optimize_arithmetic_operations_in_aggregate_functions)
@@ -755,11 +754,11 @@ void TreeOptimizer::apply(ASTPtr & query, TreeRewriterResult & result,
     if (converted_to_cnf && settings.optimize_using_constraints)
     {
         optimizeWithConstraints(select_query, result.aliases, result.source_columns_set,
-            tables_with_columns, result.metadata_snapshot, settings.optimize_append_index);
+            tables_with_columns, result.storage_snapshot->metadata, settings.optimize_append_index);
 
         if (settings.optimize_substitute_columns)
             optimizeSubstituteColumn(select_query, result.aliases, result.source_columns_set,
-                tables_with_columns, result.metadata_snapshot, result.storage);
+                tables_with_columns, result.storage_snapshot->metadata, result.storage);
     }
 
     /// GROUP BY injective function elimination.
diff --git a/src/Interpreters/TreeRewriter.cpp b/src/Interpreters/TreeRewriter.cpp
index fc3ef681c2c1..78e7ed33f8f8 100644
--- a/src/Interpreters/TreeRewriter.cpp
+++ b/src/Interpreters/TreeRewriter.cpp
@@ -518,7 +518,7 @@ void getArrayJoinedColumns(ASTPtr & query, TreeRewriterResult & result, const AS
             bool found = false;
             for (const auto & column : source_columns)
             {
-                auto split = Nested::splitName(column.name);
+                auto split = Nested::splitName(column.name, /*reverse=*/ true);
                 if (split.first == source_name && !split.second.empty())
                 {
                     result.array_join_result_to_source[Nested::concatenateName(result_name, split.second)] = column.name;
@@ -831,10 +831,10 @@ using RewriteShardNumVisitor = InDepthNodeVisitor<RewriteShardNum, true>;
 TreeRewriterResult::TreeRewriterResult(
     const NamesAndTypesList & source_columns_,
     ConstStoragePtr storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     bool add_special)
     : storage(storage_)
-    , metadata_snapshot(metadata_snapshot_)
+    , storage_snapshot(storage_snapshot_)
     , source_columns(source_columns_)
 {
     collectSourceColumns(add_special);
@@ -847,13 +847,12 @@ void TreeRewriterResult::collectSourceColumns(bool add_special)
 {
     if (storage)
     {
-        const ColumnsDescription & columns = metadata_snapshot->getColumns();
-
-        NamesAndTypesList columns_from_storage;
+        auto options = GetColumnsOptions(add_special ? GetColumnsOptions::All : GetColumnsOptions::AllPhysical);
+        options.withExtendedObjects();
         if (storage->supportsSubcolumns())
-            columns_from_storage = add_special ? columns.getAllWithSubcolumns() : columns.getAllPhysicalWithSubcolumns();
-        else
-            columns_from_storage = add_special ? columns.getAll() : columns.getAllPhysical();
+            options.withSubcolumns();
+
+        auto columns_from_storage = storage_snapshot->getColumns(options);
 
         if (source_columns.empty())
             source_columns.swap(columns_from_storage);
@@ -960,9 +959,9 @@ void TreeRewriterResult::collectUsedColumns(const ASTPtr & query, bool is_select
             /// If we have no information about columns sizes, choose a column of minimum size of its data type.
             required.insert(ExpressionActions::getSmallestColumn(source_columns));
     }
-    else if (is_select && metadata_snapshot && !columns_context.has_array_join)
+    else if (is_select && storage_snapshot && !columns_context.has_array_join)
     {
-        const auto & partition_desc = metadata_snapshot->getPartitionKey();
+        const auto & partition_desc = storage_snapshot->metadata->getPartitionKey();
         if (partition_desc.expression)
         {
             auto partition_source_columns = partition_desc.expression->getRequiredColumns();
@@ -1018,7 +1017,7 @@ void TreeRewriterResult::collectUsedColumns(const ASTPtr & query, bool is_select
         {
             for (const auto & name_type : storage_virtuals)
             {
-                if (name_type.name == "_shard_num" && storage->isVirtualColumn("_shard_num", metadata_snapshot))
+                if (name_type.name == "_shard_num" && storage->isVirtualColumn("_shard_num", storage_snapshot->getMetadataForQuery()))
                 {
                     has_virtual_shard_num = true;
                     break;
@@ -1190,7 +1189,7 @@ TreeRewriterResultPtr TreeRewriter::analyzeSelect(
 
     /// rewrite filters for select query, must go after getArrayJoinedColumns
     bool is_initiator = getContext()->getClientInfo().distributed_depth == 0;
-    if (settings.optimize_respect_aliases && result.metadata_snapshot && is_initiator)
+    if (settings.optimize_respect_aliases && result.storage_snapshot && is_initiator)
     {
         std::unordered_set<IAST *> excluded_nodes;
         {
@@ -1201,7 +1200,7 @@ TreeRewriterResultPtr TreeRewriter::analyzeSelect(
                 excluded_nodes.insert(table_join_ast->using_expression_list.get());
         }
 
-        bool is_changed = replaceAliasColumnsInQuery(query, result.metadata_snapshot->getColumns(),
+        bool is_changed = replaceAliasColumnsInQuery(query, result.storage_snapshot->metadata->getColumns(),
                                                      result.array_join_result_to_source, getContext(), excluded_nodes);
         /// If query is changed, we need to redo some work to correct name resolution.
         if (is_changed)
@@ -1234,7 +1233,7 @@ TreeRewriterResultPtr TreeRewriter::analyze(
     ASTPtr & query,
     const NamesAndTypesList & source_columns,
     ConstStoragePtr storage,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     bool allow_aggregations,
     bool allow_self_aliases,
     bool execute_scalar_subqueries) const
@@ -1244,7 +1243,7 @@ TreeRewriterResultPtr TreeRewriter::analyze(
 
     const auto & settings = getContext()->getSettingsRef();
 
-    TreeRewriterResult result(source_columns, storage, metadata_snapshot, false);
+    TreeRewriterResult result(source_columns, storage, storage_snapshot, false);
 
     normalize(query, result.aliases, result.source_columns_set, false, settings, allow_self_aliases);
 
diff --git a/src/Interpreters/TreeRewriter.h b/src/Interpreters/TreeRewriter.h
index 45b3a5a00e34..7fbe4e45fb32 100644
--- a/src/Interpreters/TreeRewriter.h
+++ b/src/Interpreters/TreeRewriter.h
@@ -19,11 +19,13 @@ struct SelectQueryOptions;
 using Scalars = std::map<String, Block>;
 struct StorageInMemoryMetadata;
 using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;
+struct StorageSnapshot;
+using StorageSnapshotPtr = std::shared_ptr<const StorageSnapshot>;
 
 struct TreeRewriterResult
 {
     ConstStoragePtr storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     std::shared_ptr<TableJoin> analyzed_join;
     const ASTTablesInSelectQueryElement * ast_join = nullptr;
 
@@ -80,7 +82,7 @@ struct TreeRewriterResult
     explicit TreeRewriterResult(
         const NamesAndTypesList & source_columns_,
         ConstStoragePtr storage_ = {},
-        const StorageMetadataPtr & metadata_snapshot_ = {},
+        const StorageSnapshotPtr & storage_snapshot_ = {},
         bool add_special = true);
 
     void collectSourceColumns(bool add_special);
@@ -112,7 +114,7 @@ class TreeRewriter : WithContext
         ASTPtr & query,
         const NamesAndTypesList & source_columns_,
         ConstStoragePtr storage = {},
-        const StorageMetadataPtr & metadata_snapshot = {},
+        const StorageSnapshotPtr & storage_snapshot = {},
         bool allow_aggregations = false,
         bool allow_self_aliases = true,
         bool execute_scalar_subqueries = true) const;
diff --git a/src/Interpreters/convertFieldToType.cpp b/src/Interpreters/convertFieldToType.cpp
index 5813b8c3926c..7abe8342100c 100644
--- a/src/Interpreters/convertFieldToType.cpp
+++ b/src/Interpreters/convertFieldToType.cpp
@@ -6,6 +6,7 @@
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/DataTypeTuple.h>
 #include <DataTypes/DataTypeMap.h>
+#include <DataTypes/DataTypeObject.h>
 #include <DataTypes/DataTypesNumber.h>
 #include <DataTypes/DataTypesDecimal.h>
 #include <DataTypes/DataTypeString.h>
@@ -21,6 +22,7 @@
 #include <Core/AccurateComparison.h>
 #include <Common/typeid_cast.h>
 #include <Common/NaNUtils.h>
+#include <Common/FieldVisitorToString.h>
 
 #include <Common/DateLUT.h>
 #include <DataTypes/DataTypeAggregateFunction.h>
@@ -246,6 +248,8 @@ Field convertFieldToTypeImpl(const Field & src, const IDataType & type, const ID
             }
             return src;
         }
+
+        return applyVisitor(FieldVisitorToString(), src);
     }
     else if (const DataTypeArray * type_array = typeid_cast<const DataTypeArray *>(&type))
     {
@@ -363,6 +367,46 @@ Field convertFieldToTypeImpl(const Field & src, const IDataType & type, const ID
 
         return src;
     }
+    else if (isObject(type))
+    {
+        const auto * from_type_tuple = typeid_cast<const DataTypeTuple *>(from_type_hint);
+        if (src.getType() == Field::Types::Tuple && from_type_tuple && from_type_tuple->haveExplicitNames())
+        {
+            const auto & names = from_type_tuple->getElementNames();
+            const auto & tuple = src.get<const Tuple &>();
+
+            if (names.size() != tuple.size())
+                throw Exception(ErrorCodes::TYPE_MISMATCH,
+                    "Bad size of tuple in IN or VALUES section (while converting to Object). Expected size: {}, actual size: {}",
+                        names.size(), tuple.size());
+
+            Object object;
+            for (size_t i = 0; i < names.size(); ++i)
+                object[names[i]] = tuple[i];
+
+            return object;
+        }
+
+        if (src.getType() == Field::Types::Map)
+        {
+            Object object;
+            const auto & map = src.get<const Map &>();
+            for (size_t i = 0; i < map.size(); ++i)
+            {
+                const auto & map_entry = map[i].get<Tuple>();
+                const auto & key = map_entry[0];
+                const auto & value = map_entry[1];
+
+                if (key.getType() != Field::Types::String)
+                    throw Exception(ErrorCodes::TYPE_MISMATCH,
+                        "Cannot convert from Map with key of type {} to Object", key.getTypeName());
+
+                object[key.get<const String &>()] = value;
+            }
+
+            return object;
+        }
+    }
 
     /// Conversion from string by parsing.
     if (src.getType() == Field::Types::String)
diff --git a/src/Interpreters/getColumnFromBlock.cpp b/src/Interpreters/getColumnFromBlock.cpp
new file mode 100644
index 000000000000..ce6fa2904db9
--- /dev/null
+++ b/src/Interpreters/getColumnFromBlock.cpp
@@ -0,0 +1,50 @@
+#include <Interpreters/getColumnFromBlock.h>
+#include <Interpreters/castColumn.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int NOT_FOUND_COLUMN_IN_BLOCK;
+}
+
+ColumnPtr tryGetColumnFromBlock(const Block & block, const NameAndTypePair & requested_column)
+{
+    const auto * elem = block.findByName(requested_column.getNameInStorage());
+    if (!elem)
+        return nullptr;
+
+    DataTypePtr elem_type;
+    ColumnPtr elem_column;
+
+    if (requested_column.isSubcolumn())
+    {
+        auto subcolumn_name = requested_column.getSubcolumnName();
+        elem_type = elem->type->tryGetSubcolumnType(subcolumn_name);
+        elem_column = elem->type->tryGetSubcolumn(subcolumn_name, elem->column);
+
+        if (!elem_type || !elem_column)
+            return nullptr;
+    }
+    else
+    {
+        elem_type = elem->type;
+        elem_column = elem->column;
+    }
+
+    return castColumn({elem_column, elem_type, ""}, requested_column.type);
+}
+
+ColumnPtr getColumnFromBlock(const Block & block, const NameAndTypePair & requested_column)
+{
+    auto result_column = tryGetColumnFromBlock(block, requested_column);
+    if (!result_column)
+        throw Exception(ErrorCodes::NOT_FOUND_COLUMN_IN_BLOCK,
+            "Not found column or subcolumn {} in block. There are only columns: {}",
+                requested_column.name, block.dumpNames());
+
+    return result_column;
+}
+
+}
diff --git a/src/Interpreters/getColumnFromBlock.h b/src/Interpreters/getColumnFromBlock.h
new file mode 100644
index 000000000000..26500cfdd17b
--- /dev/null
+++ b/src/Interpreters/getColumnFromBlock.h
@@ -0,0 +1,13 @@
+#pragma once
+#include <Core/Block.h>
+
+namespace DB
+{
+
+/// Helps in-memory storages to extract columns from block.
+/// Properly handles cases, when column is a subcolumn and when it is compressed.
+ColumnPtr getColumnFromBlock(const Block & block, const NameAndTypePair & requested_column);
+
+ColumnPtr tryGetColumnFromBlock(const Block & block, const NameAndTypePair & requested_column);
+
+}
diff --git a/src/Interpreters/getHeaderForProcessingStage.cpp b/src/Interpreters/getHeaderForProcessingStage.cpp
index 319137779023..6f4d7d5c525e 100644
--- a/src/Interpreters/getHeaderForProcessingStage.cpp
+++ b/src/Interpreters/getHeaderForProcessingStage.cpp
@@ -82,9 +82,8 @@ bool removeJoin(ASTSelectQuery & select, TreeRewriterResult & rewriter_result, C
 }
 
 Block getHeaderForProcessingStage(
-        const IStorage & storage,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage)
@@ -93,7 +92,8 @@ Block getHeaderForProcessingStage(
     {
         case QueryProcessingStage::FetchColumns:
         {
-            Block header = metadata_snapshot->getSampleBlockForColumns(column_names, storage.getVirtuals(), storage.getStorageID());
+            Block header = storage_snapshot->getSampleBlockForColumns(column_names);
+
             if (query_info.prewhere_info)
             {
                 auto & prewhere_info = *query_info.prewhere_info;
@@ -123,7 +123,7 @@ Block getHeaderForProcessingStage(
             removeJoin(*query->as<ASTSelectQuery>(), new_rewriter_result, context);
 
             auto pipe = Pipe(std::make_shared<SourceFromSingleChunk>(
-                    metadata_snapshot->getSampleBlockForColumns(column_names, storage.getVirtuals(), storage.getStorageID())));
+                    storage_snapshot->getSampleBlockForColumns(column_names)));
             return InterpreterSelectQuery(query, context, std::move(pipe), SelectQueryOptions(processed_stage).analyze()).getSampleBlock();
         }
     }
diff --git a/src/Interpreters/getHeaderForProcessingStage.h b/src/Interpreters/getHeaderForProcessingStage.h
index 54a1126a3df1..6ada136030e4 100644
--- a/src/Interpreters/getHeaderForProcessingStage.h
+++ b/src/Interpreters/getHeaderForProcessingStage.h
@@ -10,8 +10,8 @@ namespace DB
 {
 
 class IStorage;
-struct StorageInMemoryMetadata;
-using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;
+struct StorageSnapshot;
+using StorageSnapshotPtr = std::shared_ptr<const StorageSnapshot>;
 struct SelectQueryInfo;
 struct TreeRewriterResult;
 class ASTSelectQuery;
@@ -20,9 +20,8 @@ bool hasJoin(const ASTSelectQuery & select);
 bool removeJoin(ASTSelectQuery & select, TreeRewriterResult & rewriter_result, ContextPtr context);
 
 Block getHeaderForProcessingStage(
-        const IStorage & storage,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage);
diff --git a/src/Interpreters/inplaceBlockConversions.cpp b/src/Interpreters/inplaceBlockConversions.cpp
index 5c87950b8212..15dd9229194a 100644
--- a/src/Interpreters/inplaceBlockConversions.cpp
+++ b/src/Interpreters/inplaceBlockConversions.cpp
@@ -15,11 +15,20 @@
 #include <Interpreters/RequiredSourceColumnsVisitor.h>
 #include <Common/checkStackSize.h>
 #include <Storages/ColumnsDescription.h>
+#include <DataTypes/NestedUtils.h>
+#include <Columns/ColumnArray.h>
+#include <DataTypes/DataTypeArray.h>
+#include <Storages/StorageInMemoryMetadata.h>
 
 
 namespace DB
 {
 
+namespace ErrorCode
+{
+    extern const int LOGICAL_ERROR;
+}
+
 namespace
 {
 
@@ -178,4 +187,97 @@ ActionsDAGPtr evaluateMissingDefaults(
     return createExpressions(header, expr_list, save_unneeded_columns, context);
 }
 
+static bool arrayHasNoElementsRead(const IColumn & column)
+{
+    const auto * column_array = typeid_cast<const ColumnArray *>(&column);
+
+    if (!column_array)
+        return false;
+
+    size_t size = column_array->size();
+    if (!size)
+        return false;
+
+    size_t data_size = column_array->getData().size();
+    if (data_size)
+        return false;
+
+    size_t last_offset = column_array->getOffsets()[size - 1];
+    return last_offset != 0;
+}
+
+void fillMissingColumns(
+    Columns & res_columns,
+    size_t num_rows,
+    const NamesAndTypesList & requested_columns,
+    StorageMetadataPtr metadata_snapshot)
+{
+    size_t num_columns = requested_columns.size();
+    if (num_columns != res_columns.size())
+        throw Exception(ErrorCodes::LOGICAL_ERROR,
+            "Invalid number of columns passed to fillMissingColumns. Expected {}, got {}",
+            num_columns, res_columns.size());
+
+    /// For a missing column of a nested data structure we must create not a column of empty
+    /// arrays, but a column of arrays of correct length.
+
+    /// First, collect offset columns for all arrays in the block.
+
+    std::unordered_map<String, ColumnPtr> offset_columns;
+    auto requested_column = requested_columns.begin();
+    for (size_t i = 0; i < num_columns; ++i, ++requested_column)
+    {
+        if (res_columns[i] == nullptr)
+            continue;
+
+        if (const auto * array = typeid_cast<const ColumnArray *>(res_columns[i].get()))
+        {
+            String offsets_name = Nested::extractTableName(requested_column->name);
+            auto & offsets_column = offset_columns[offsets_name];
+
+            /// If for some reason multiple offsets columns are present for the same nested data structure,
+            /// choose the one that is not empty.
+            if (!offsets_column || offsets_column->empty())
+                offsets_column = array->getOffsetsPtr();
+        }
+    }
+
+    /// insert default values only for columns without default expressions
+    requested_column = requested_columns.begin();
+    for (size_t i = 0; i < num_columns; ++i, ++requested_column)
+    {
+        const auto & [name, type] = *requested_column;
+
+        if (res_columns[i] && arrayHasNoElementsRead(*res_columns[i]))
+            res_columns[i] = nullptr;
+
+        if (res_columns[i] == nullptr)
+        {
+            if (metadata_snapshot && metadata_snapshot->getColumns().hasDefault(name))
+                continue;
+
+            String offsets_name = Nested::extractTableName(name);
+            auto offset_it = offset_columns.find(offsets_name);
+            const auto * array_type = typeid_cast<const DataTypeArray *>(type.get());
+            if (offset_it != offset_columns.end() && array_type)
+            {
+                const auto & nested_type = array_type->getNestedType();
+                ColumnPtr offsets_column = offset_it->second;
+                size_t nested_rows = typeid_cast<const ColumnUInt64 &>(*offsets_column).getData().back();
+
+                ColumnPtr nested_column =
+                    nested_type->createColumnConstWithDefaultValue(nested_rows)->convertToFullColumnIfConst();
+
+                res_columns[i] = ColumnArray::create(nested_column, offsets_column);
+            }
+            else
+            {
+                /// We must turn a constant column into a full column because the interpreter could infer
+                /// that it is constant everywhere but in some blocks (from other parts) it can be a full column.
+                res_columns[i] = type->createColumnConstWithDefaultValue(num_rows)->convertToFullColumnIfConst();
+            }
+        }
+    }
+}
+
 }
diff --git a/src/Interpreters/inplaceBlockConversions.h b/src/Interpreters/inplaceBlockConversions.h
index cc8261693f99..b3113ddfa5c2 100644
--- a/src/Interpreters/inplaceBlockConversions.h
+++ b/src/Interpreters/inplaceBlockConversions.h
@@ -1,6 +1,7 @@
 #pragma once
 
 #include <Interpreters/Context_fwd.h>
+#include <Common/COW.h>
 
 #include <memory>
 #include <string>
@@ -14,6 +15,13 @@ class Block;
 class NamesAndTypesList;
 class ColumnsDescription;
 
+class IColumn;
+using ColumnPtr = COW<IColumn>::Ptr;
+using Columns = std::vector<ColumnPtr>;
+
+struct StorageInMemoryMetadata;
+using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;
+
 class ActionsDAG;
 using ActionsDAGPtr = std::shared_ptr<ActionsDAG>;
 
@@ -31,4 +39,10 @@ ActionsDAGPtr evaluateMissingDefaults(
 /// Tries to convert columns in block to required_columns
 void performRequiredConversions(Block & block, const NamesAndTypesList & required_columns, ContextPtr context);
 
+void fillMissingColumns(
+    Columns & res_columns,
+    size_t num_rows,
+    const NamesAndTypesList & requested_columns,
+    StorageMetadataPtr metadata_snapshot);
+
 }
diff --git a/src/Processors/Formats/IRowInputFormat.cpp b/src/Processors/Formats/IRowInputFormat.cpp
index c4857326e6e4..645100dad19f 100644
--- a/src/Processors/Formats/IRowInputFormat.cpp
+++ b/src/Processors/Formats/IRowInputFormat.cpp
@@ -1,4 +1,5 @@
 #include <Processors/Formats/IRowInputFormat.h>
+#include <DataTypes/ObjectUtils.h>
 #include <IO/WriteHelpers.h>    // toString
 #include <base/logger_useful.h>
 
@@ -199,6 +200,7 @@ Chunk IRowInputFormat::generate()
         return {};
     }
 
+    finalizeObjectColumns(columns);
     Chunk chunk(std::move(columns), num_rows);
     //chunk.setChunkInfo(std::move(chunk_missing_values));
     return chunk;
diff --git a/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.cpp b/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.cpp
index 56ba975dea15..914ec27fc464 100644
--- a/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.cpp
+++ b/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.cpp
@@ -14,30 +14,25 @@ namespace ErrorCodes
     extern const int INCORRECT_DATA;
 }
 
-JSONAsStringRowInputFormat::JSONAsStringRowInputFormat(const Block & header_, ReadBuffer & in_, Params params_)
-    : JSONAsStringRowInputFormat(header_, std::make_unique<PeekableReadBuffer>(in_), params_) {}
+JSONAsRowInputFormat::JSONAsRowInputFormat(const Block & header_, ReadBuffer & in_, Params params_)
+    : JSONAsRowInputFormat(header_, std::make_unique<PeekableReadBuffer>(in_), params_) {}
 
-JSONAsStringRowInputFormat::JSONAsStringRowInputFormat(const Block & header_, std::unique_ptr<PeekableReadBuffer> buf_, Params params_) :
+JSONAsRowInputFormat::JSONAsRowInputFormat(const Block & header_, std::unique_ptr<PeekableReadBuffer> buf_, Params params_) :
     IRowInputFormat(header_, *buf_, std::move(params_)), buf(std::move(buf_))
 {
     if (header_.columns() > 1)
         throw Exception(ErrorCodes::BAD_ARGUMENTS,
-            "This input format is only suitable for tables with a single column of type String but the number of columns is {}",
+            "This input format is only suitable for tables with a single column of type String or Object, but the number of columns is {}",
             header_.columns());
-
-    if (!isString(removeNullable(removeLowCardinality(header_.getByPosition(0).type))))
-        throw Exception(ErrorCodes::BAD_ARGUMENTS,
-            "This input format is only suitable for tables with a single column of type String but the column type is {}",
-            header_.getByPosition(0).type->getName());
 }
 
-void JSONAsStringRowInputFormat::resetParser()
+void JSONAsRowInputFormat::resetParser()
 {
     IRowInputFormat::resetParser();
     buf->reset();
 }
 
-void JSONAsStringRowInputFormat::readPrefix()
+void JSONAsRowInputFormat::readPrefix()
 {
     /// In this format, BOM at beginning of stream cannot be confused with value, so it is safe to skip it.
     skipBOMIfExists(*buf);
@@ -50,7 +45,7 @@ void JSONAsStringRowInputFormat::readPrefix()
     }
 }
 
-void JSONAsStringRowInputFormat::readSuffix()
+void JSONAsRowInputFormat::readSuffix()
 {
     skipWhitespaceIfAny(*buf);
     if (data_in_square_brackets)
@@ -66,6 +61,57 @@ void JSONAsStringRowInputFormat::readSuffix()
     assertEOF(*buf);
 }
 
+bool JSONAsRowInputFormat::readRow(MutableColumns & columns, RowReadExtension &)
+{
+    assert(columns.size() == 1);
+    assert(serializations.size() == 1);
+
+    if (!allow_new_rows)
+        return false;
+
+    skipWhitespaceIfAny(*buf);
+    if (!buf->eof())
+    {
+        if (!data_in_square_brackets && *buf->position() == ';')
+        {
+            /// ';' means the end of query, but it cannot be before ']'.
+            return allow_new_rows = false;
+        }
+        else if (data_in_square_brackets && *buf->position() == ']')
+        {
+            /// ']' means the end of query.
+            return allow_new_rows = false;
+        }
+    }
+
+    if (!buf->eof())
+        readJSONObject(*columns[0]);
+
+    skipWhitespaceIfAny(*buf);
+    if (!buf->eof() && *buf->position() == ',')
+        ++buf->position();
+    skipWhitespaceIfAny(*buf);
+
+    return !buf->eof();
+}
+
+void JSONAsRowInputFormat::setReadBuffer(ReadBuffer & in_)
+{
+    buf = std::make_unique<PeekableReadBuffer>(in_);
+    IInputFormat::setReadBuffer(*buf);
+}
+
+
+JSONAsStringRowInputFormat::JSONAsStringRowInputFormat(
+    const Block & header_, ReadBuffer & in_, Params params_)
+    : JSONAsRowInputFormat(header_, in_, params_)
+{
+    if (!isString(removeNullable(removeLowCardinality(header_.getByPosition(0).type))))
+        throw Exception(ErrorCodes::BAD_ARGUMENTS,
+            "This input format is only suitable for tables with a single column of type String but the column type is {}",
+            header_.getByPosition(0).type->getName());
+}
+
 void JSONAsStringRowInputFormat::readJSONObject(IColumn & column)
 {
     PeekableReadBufferCheckpoint checkpoint{*buf};
@@ -143,41 +189,21 @@ void JSONAsStringRowInputFormat::readJSONObject(IColumn & column)
     buf->position() = end;
 }
 
-bool JSONAsStringRowInputFormat::readRow(MutableColumns & columns, RowReadExtension &)
-{
-    if (!allow_new_rows)
-        return false;
-
-    skipWhitespaceIfAny(*buf);
-    if (!buf->eof())
-    {
-        if (!data_in_square_brackets && *buf->position() == ';')
-        {
-            /// ';' means the end of query, but it cannot be before ']'.
-            return allow_new_rows = false;
-        }
-        else if (data_in_square_brackets && *buf->position() == ']')
-        {
-            /// ']' means the end of query.
-            return allow_new_rows = false;
-        }
-    }
-
-    if (!buf->eof())
-        readJSONObject(*columns[0]);
 
-    skipWhitespaceIfAny(*buf);
-    if (!buf->eof() && *buf->position() == ',')
-        ++buf->position();
-    skipWhitespaceIfAny(*buf);
-
-    return !buf->eof();
+JSONAsObjectRowInputFormat::JSONAsObjectRowInputFormat(
+    const Block & header_, ReadBuffer & in_, Params params_, const FormatSettings & format_settings_)
+    : JSONAsRowInputFormat(header_, in_, params_)
+    , format_settings(format_settings_)
+{
+    if (!isObject(header_.getByPosition(0).type))
+        throw Exception(ErrorCodes::BAD_ARGUMENTS,
+            "Input format JSONAsObject is only suitable for tables with a single column of type Object but the column type is {}",
+            header_.getByPosition(0).type->getName());
 }
 
-void JSONAsStringRowInputFormat::setReadBuffer(ReadBuffer & in_)
+void JSONAsObjectRowInputFormat::readJSONObject(IColumn & column)
 {
-    buf = std::make_unique<PeekableReadBuffer>(in_);
-    IInputFormat::setReadBuffer(*buf);
+    serializations[0]->deserializeTextJSON(column, *buf, format_settings);
 }
 
 void registerInputFormatJSONAsString(FormatFactory & factory)
@@ -202,6 +228,23 @@ void registerNonTrivialPrefixAndSuffixCheckerJSONAsString(FormatFactory & factor
     factory.registerNonTrivialPrefixAndSuffixChecker("JSONAsString", nonTrivialPrefixAndSuffixCheckerJSONEachRowImpl);
 }
 
+void registerInputFormatJSONAsObject(FormatFactory & factory)
+{
+    factory.registerInputFormat("JSONAsObject", [](
+        ReadBuffer & buf,
+        const Block & sample,
+        IRowInputFormat::Params params,
+        const FormatSettings & settings)
+    {
+        return std::make_shared<JSONAsObjectRowInputFormat>(sample, buf, std::move(params), settings);
+    });
+}
+
+void registerNonTrivialPrefixAndSuffixCheckerJSONAsObject(FormatFactory & factory)
+{
+    factory.registerNonTrivialPrefixAndSuffixChecker("JSONAsObject", nonTrivialPrefixAndSuffixCheckerJSONEachRowImpl);
+}
+
 void registerJSONAsStringSchemaReader(FormatFactory & factory)
 {
     factory.registerExternalSchemaReader("JSONAsString", [](const FormatSettings &)
diff --git a/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.h b/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.h
index 9979a5d1474d..f7880eac867f 100644
--- a/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.h
+++ b/src/Processors/Formats/Impl/JSONAsStringRowInputFormat.h
@@ -12,35 +12,58 @@ namespace DB
 class ReadBuffer;
 
 /// This format parses a sequence of JSON objects separated by newlines, spaces and/or comma.
-/// Each JSON object is parsed as a whole to string.
-/// This format can only parse a table with single field of type String.
-
-class JSONAsStringRowInputFormat final : public IRowInputFormat
+class JSONAsRowInputFormat : public IRowInputFormat
 {
 public:
-    JSONAsStringRowInputFormat(const Block & header_, ReadBuffer & in_, Params params_);
+    JSONAsRowInputFormat(const Block & header_, ReadBuffer & in_, Params params_);
 
-    String getName() const override { return "JSONAsStringRowInputFormat"; }
     void resetParser() override;
     void setReadBuffer(ReadBuffer & in_) override;
 
 private:
-    JSONAsStringRowInputFormat(const Block & header_, std::unique_ptr<PeekableReadBuffer> buf_, Params params_);
+    JSONAsRowInputFormat(const Block & header_, std::unique_ptr<PeekableReadBuffer> buf_, Params params_);
 
     bool readRow(MutableColumns & columns, RowReadExtension & ext) override;
 
     void readPrefix() override;
     void readSuffix() override;
 
-    void readJSONObject(IColumn & column);
-
+protected:
+    virtual void readJSONObject(IColumn & column) = 0;
     std::unique_ptr<PeekableReadBuffer> buf;
 
+private:
     /// This flag is needed to know if data is in square brackets.
     bool data_in_square_brackets = false;
     bool allow_new_rows = true;
 };
 
+/// Each JSON object is parsed as a whole to string.
+/// This format can only parse a table with single field of type String.
+class JSONAsStringRowInputFormat final : public JSONAsRowInputFormat
+{
+public:
+    JSONAsStringRowInputFormat(const Block & header_, ReadBuffer & in_, Params params_);
+    String getName() const override { return "JSONAsStringRowInputFormat"; }
+
+private:
+    void readJSONObject(IColumn & column) override;
+};
+
+
+/// Each JSON object is parsed as a whole to object.
+/// This format can only parse a table with single field of type Object.
+class JSONAsObjectRowInputFormat final : public JSONAsRowInputFormat
+{
+public:
+    JSONAsObjectRowInputFormat(const Block & header_, ReadBuffer & in_, Params params_, const FormatSettings & format_settings_);
+    String getName() const override { return "JSONAsObjectRowInputFormat"; }
+
+private:
+    void readJSONObject(IColumn & column) override;
+    const FormatSettings format_settings;
+};
+
 class JSONAsStringExternalSchemaReader : public IExternalSchemaReader
 {
 public:
diff --git a/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp b/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp
index c9337929adcf..bf8feb077ed0 100644
--- a/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp
+++ b/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp
@@ -15,6 +15,7 @@
 #include <DataTypes/DataTypeTuple.h>
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/DataTypeMap.h>
+#include <DataTypes/ObjectUtils.h>
 
 #include <base/logger_useful.h>
 
@@ -105,6 +106,7 @@ Chunk ValuesBlockInputFormat::generate()
         return {};
     }
 
+    finalizeObjectColumns(columns);
     size_t rows_in_block = columns[0]->size();
     return Chunk{std::move(columns), rows_in_block};
 }
diff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp
index ad4d1ea86d60..1bfc1ec7306f 100644
--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp
+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp
@@ -68,8 +68,7 @@ ReadFromMergeTree::ReadFromMergeTree(
     Names virt_column_names_,
     const MergeTreeData & data_,
     const SelectQueryInfo & query_info_,
-    StorageMetadataPtr metadata_snapshot_,
-    StorageMetadataPtr metadata_snapshot_base_,
+    StorageSnapshotPtr storage_snapshot_,
     ContextPtr context_,
     size_t max_block_size_,
     size_t num_streams_,
@@ -79,7 +78,7 @@ ReadFromMergeTree::ReadFromMergeTree(
     MergeTreeDataSelectAnalysisResultPtr analyzed_result_ptr_,
     bool enable_parallel_reading)
     : ISourceStep(DataStream{.header = MergeTreeBaseSelectProcessor::transformHeader(
-        metadata_snapshot_->getSampleBlockForColumns(real_column_names_, data_.getVirtuals(), data_.getStorageID()),
+        storage_snapshot_->getSampleBlockForColumns(real_column_names_),
         getPrewhereInfo(query_info_),
         data_.getPartitionValueType(),
         virt_column_names_)})
@@ -91,8 +90,8 @@ ReadFromMergeTree::ReadFromMergeTree(
     , query_info(query_info_)
     , prewhere_info(getPrewhereInfo(query_info))
     , actions_settings(ExpressionActionsSettings::fromContext(context_))
-    , metadata_snapshot(std::move(metadata_snapshot_))
-    , metadata_snapshot_base(std::move(metadata_snapshot_base_))
+    , storage_snapshot(std::move(storage_snapshot_))
+    , metadata_for_reading(storage_snapshot->getMetadataForQuery())
     , context(std::move(context_))
     , max_block_size(max_block_size_)
     , requested_num_streams(num_streams_)
@@ -142,7 +141,7 @@ Pipe ReadFromMergeTree::readFromPool(
         min_marks_for_concurrent_read,
         std::move(parts_with_range),
         data,
-        metadata_snapshot,
+        storage_snapshot,
         prewhere_info,
         required_columns,
         backoff_settings,
@@ -169,7 +168,7 @@ Pipe ReadFromMergeTree::readFromPool(
         auto source = std::make_shared<MergeTreeThreadSelectProcessor>(
             i, pool, min_marks_for_concurrent_read, max_block_size,
             settings.preferred_block_size_bytes, settings.preferred_max_column_in_block_size_bytes,
-            data, metadata_snapshot, use_uncompressed_cache,
+            data, storage_snapshot, use_uncompressed_cache,
             prewhere_info, actions_settings, reader_settings, virt_column_names, std::move(extension));
 
         /// Set the approximate number of rows for the first source only
@@ -205,7 +204,7 @@ ProcessorPtr ReadFromMergeTree::createSource(
         };
     }
     return std::make_shared<TSource>(
-            data, metadata_snapshot, part.data_part, max_block_size, preferred_block_size_bytes,
+            data, storage_snapshot, part.data_part, max_block_size, preferred_block_size_bytes,
             preferred_max_column_in_block_size_bytes, required_columns, part.ranges, use_uncompressed_cache, prewhere_info,
             actions_settings, reader_settings, virt_column_names, part.part_index_in_query, has_limit_below_one_block, std::move(extension));
 }
@@ -511,12 +510,12 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsWithOrder(
         size_t fixed_prefix_size = input_order_info->order_key_fixed_prefix_descr.size();
         size_t prefix_size = fixed_prefix_size + input_order_info->order_key_prefix_descr.size();
 
-        auto order_key_prefix_ast = metadata_snapshot->getSortingKey().expression_list_ast->clone();
+        auto order_key_prefix_ast = metadata_for_reading->getSortingKey().expression_list_ast->clone();
         order_key_prefix_ast->children.resize(prefix_size);
 
-        auto syntax_result = TreeRewriter(context).analyze(order_key_prefix_ast, metadata_snapshot->getColumns().getAllPhysical());
+        auto syntax_result = TreeRewriter(context).analyze(order_key_prefix_ast, metadata_for_reading->getColumns().getAllPhysical());
         auto sorting_key_prefix_expr = ExpressionAnalyzer(order_key_prefix_ast, syntax_result, context).getActionsDAG(false);
-        const auto & sorting_columns = metadata_snapshot->getSortingKey().column_names;
+        const auto & sorting_columns = metadata_for_reading->getSortingKey().column_names;
 
         SortDescription sort_description;
         for (size_t j = 0; j < prefix_size; ++j)
@@ -745,7 +744,7 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsFinal(
         }
 
         auto sorting_expr = std::make_shared<ExpressionActions>(
-            metadata_snapshot->getSortingKey().expression->getActionsDAG().clone());
+            metadata_for_reading->getSortingKey().expression->getActionsDAG().clone());
 
         pipe.addSimpleTransform([sorting_expr](const Block & header)
         {
@@ -762,12 +761,12 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsFinal(
             continue;
         }
 
-        Names sort_columns = metadata_snapshot->getSortingKeyColumns();
+        Names sort_columns = metadata_for_reading->getSortingKeyColumns();
         SortDescription sort_description;
         size_t sort_columns_size = sort_columns.size();
         sort_description.reserve(sort_columns_size);
 
-        Names partition_key_columns = metadata_snapshot->getPartitionKey().column_names;
+        Names partition_key_columns = metadata_for_reading->getPartitionKey().column_names;
 
         const auto & header = pipe.getHeader();
         for (size_t i = 0; i < sort_columns_size; ++i)
@@ -807,7 +806,7 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsFinal(
             out_projection = createProjection(pipe.getHeader());
 
         auto sorting_expr = std::make_shared<ExpressionActions>(
-            metadata_snapshot->getSortingKey().expression->getActionsDAG().clone());
+            metadata_for_reading->getSortingKey().expression->getActionsDAG().clone());
 
         pipe.addSimpleTransform([sorting_expr](const Block & header)
         {
@@ -824,8 +823,8 @@ MergeTreeDataSelectAnalysisResultPtr ReadFromMergeTree::selectRangesToRead(Merge
 {
     return selectRangesToRead(
         std::move(parts),
-        metadata_snapshot_base,
-        metadata_snapshot,
+        storage_snapshot->metadata,
+        storage_snapshot->getMetadataForQuery(),
         query_info,
         context,
         requested_num_streams,
@@ -867,7 +866,7 @@ MergeTreeDataSelectAnalysisResultPtr ReadFromMergeTree::selectRangesToRead(
         result.column_names_to_read.push_back(ExpressionActions::getSmallestColumn(available_real_columns));
     }
 
-    metadata_snapshot->check(result.column_names_to_read, data.getVirtuals(), data.getStorageID());
+    // storage_snapshot->check(result.column_names_to_read);
 
     // Build and check if primary key is used when necessary
     const auto & primary_key = metadata_snapshot->getPrimaryKey();
@@ -1045,7 +1044,7 @@ void ReadFromMergeTree::initializePipeline(QueryPipelineBuilder & pipeline, cons
     if (select.final())
     {
         /// Add columns needed to calculate the sorting expression and the sign.
-        std::vector<String> add_columns = metadata_snapshot->getColumnsRequiredForSortingKey();
+        std::vector<String> add_columns = metadata_for_reading->getColumnsRequiredForSortingKey();
         column_names_to_read.insert(column_names_to_read.end(), add_columns.begin(), add_columns.end());
 
         if (!data.merging_params.sign_column.empty())
diff --git a/src/Processors/QueryPlan/ReadFromMergeTree.h b/src/Processors/QueryPlan/ReadFromMergeTree.h
index 0d07a3e2ea29..685b99a7bdcb 100644
--- a/src/Processors/QueryPlan/ReadFromMergeTree.h
+++ b/src/Processors/QueryPlan/ReadFromMergeTree.h
@@ -89,8 +89,7 @@ class ReadFromMergeTree final : public ISourceStep
         Names virt_column_names_,
         const MergeTreeData & data_,
         const SelectQueryInfo & query_info_,
-        StorageMetadataPtr metadata_snapshot_,
-        StorageMetadataPtr metadata_snapshot_base_,
+        StorageSnapshotPtr storage_snapshot,
         ContextPtr context_,
         size_t max_block_size_,
         size_t num_streams_,
@@ -141,8 +140,8 @@ class ReadFromMergeTree final : public ISourceStep
     PrewhereInfoPtr prewhere_info;
     ExpressionActionsSettings actions_settings;
 
-    StorageMetadataPtr metadata_snapshot;
-    StorageMetadataPtr metadata_snapshot_base;
+    StorageSnapshotPtr storage_snapshot;
+    StorageMetadataPtr metadata_for_reading;
 
     ContextPtr context;
 
diff --git a/src/Processors/Transforms/WindowTransform.cpp b/src/Processors/Transforms/WindowTransform.cpp
index 3c96e12e8694..b81ed099915f 100644
--- a/src/Processors/Transforms/WindowTransform.cpp
+++ b/src/Processors/Transforms/WindowTransform.cpp
@@ -1999,7 +1999,7 @@ struct WindowFunctionLagLeadInFrame final : public WindowFunction
             return;
         }
 
-        const auto supertype = getLeastSupertype({argument_types[0], argument_types[2]});
+        const auto supertype = getLeastSupertype(DataTypes{argument_types[0], argument_types[2]});
         if (!supertype)
         {
             throw Exception(ErrorCodes::BAD_ARGUMENTS,
diff --git a/src/QueryPipeline/RemoteQueryExecutor.cpp b/src/QueryPipeline/RemoteQueryExecutor.cpp
index d1275444b848..110d43082364 100644
--- a/src/QueryPipeline/RemoteQueryExecutor.cpp
+++ b/src/QueryPipeline/RemoteQueryExecutor.cpp
@@ -563,12 +563,13 @@ void RemoteQueryExecutor::sendExternalTables()
                 {
                     SelectQueryInfo query_info;
                     auto metadata_snapshot = cur->getInMemoryMetadataPtr();
+                    auto storage_snapshot = cur->getStorageSnapshot(metadata_snapshot);
                     QueryProcessingStage::Enum read_from_table_stage = cur->getQueryProcessingStage(
-                        context, QueryProcessingStage::Complete, metadata_snapshot, query_info);
+                        context, QueryProcessingStage::Complete, storage_snapshot, query_info);
 
                     Pipe pipe = cur->read(
                         metadata_snapshot->getColumns().getNamesOfPhysical(),
-                        metadata_snapshot, query_info, context,
+                        storage_snapshot, query_info, context,
                         read_from_table_stage, DEFAULT_BLOCK_SIZE, 1);
 
                     if (pipe.empty())
diff --git a/src/Storages/ColumnsDescription.cpp b/src/Storages/ColumnsDescription.cpp
index 8ca3c44bac27..88f38b54f2bd 100644
--- a/src/Storages/ColumnsDescription.cpp
+++ b/src/Storages/ColumnsDescription.cpp
@@ -384,6 +384,56 @@ NamesAndTypesList ColumnsDescription::getAll() const
     return ret;
 }
 
+NamesAndTypesList ColumnsDescription::getSubcolumns(const String & name_in_storage) const
+{
+    auto range = subcolumns.get<1>().equal_range(name_in_storage);
+    return NamesAndTypesList(range.first, range.second);
+}
+
+void ColumnsDescription::addSubcolumnsToList(NamesAndTypesList & source_list) const
+{
+    NamesAndTypesList subcolumns_list;
+    for (const auto & col : source_list)
+    {
+        auto range = subcolumns.get<1>().equal_range(col.name);
+        if (range.first != range.second)
+            subcolumns_list.insert(subcolumns_list.end(), range.first, range.second);
+    }
+
+    source_list.splice(source_list.end(), std::move(subcolumns_list));
+}
+
+NamesAndTypesList ColumnsDescription::get(const GetColumnsOptions & options) const
+{
+    NamesAndTypesList res;
+    switch (options.kind)
+    {
+        case GetColumnsOptions::All:
+            res = getAll();
+            break;
+        case GetColumnsOptions::AllPhysical:
+            res = getAllPhysical();
+            break;
+        case GetColumnsOptions::Ordinary:
+            res = getOrdinary();
+            break;
+        case GetColumnsOptions::Materialized:
+            res = getMaterialized();
+            break;
+        case GetColumnsOptions::Aliases:
+            res = getAliases();
+            break;
+        case GetColumnsOptions::Ephemeral:
+            res = getEphemeral();
+            break;
+    }
+
+    if (options.with_subcolumns)
+        addSubcolumnsToList(res);
+
+    return res;
+}
+
 bool ColumnsDescription::has(const String & column_name) const
 {
     return columns.get<1>().find(column_name) != columns.get<1>().end();
@@ -410,37 +460,37 @@ const ColumnDescription & ColumnsDescription::get(const String & column_name) co
     return *it;
 }
 
-static ColumnsDescription::GetFlags defaultKindToGetFlag(ColumnDefaultKind kind)
+static GetColumnsOptions::Kind defaultKindToGetKind(ColumnDefaultKind kind)
 {
     switch (kind)
     {
         case ColumnDefaultKind::Default:
-            return ColumnsDescription::Ordinary;
+            return GetColumnsOptions::Ordinary;
         case ColumnDefaultKind::Materialized:
-            return ColumnsDescription::Materialized;
+            return GetColumnsOptions::Materialized;
         case ColumnDefaultKind::Alias:
-            return ColumnsDescription::Aliases;
+            return GetColumnsOptions::Aliases;
         case ColumnDefaultKind::Ephemeral:
-            return ColumnsDescription::Ephemeral;
+            return GetColumnsOptions::Ephemeral;
     }
     __builtin_unreachable();
 }
 
-NamesAndTypesList ColumnsDescription::getByNames(GetFlags flags, const Names & names, bool with_subcolumns) const
+NamesAndTypesList ColumnsDescription::getByNames(const GetColumnsOptions & options, const Names & names) const
 {
     NamesAndTypesList res;
     for (const auto & name : names)
     {
         if (auto it = columns.get<1>().find(name); it != columns.get<1>().end())
         {
-            auto kind = defaultKindToGetFlag(it->default_desc.kind);
-            if (flags & kind)
+            auto kind = defaultKindToGetKind(it->default_desc.kind);
+            if (options.kind & kind)
             {
                 res.emplace_back(name, it->type);
                 continue;
             }
         }
-        else if (with_subcolumns)
+        else if (options.with_subcolumns)
         {
             auto jt = subcolumns.get<0>().find(name);
             if (jt != subcolumns.get<0>().end())
@@ -475,22 +525,40 @@ Names ColumnsDescription::getNamesOfPhysical() const
     return ret;
 }
 
-std::optional<NameAndTypePair> ColumnsDescription::tryGetColumnOrSubcolumn(GetFlags flags, const String & column_name) const
+std::optional<NameAndTypePair> ColumnsDescription::tryGetColumn(const GetColumnsOptions & options, const String & column_name) const
 {
     auto it = columns.get<1>().find(column_name);
-    if (it != columns.get<1>().end() && (defaultKindToGetFlag(it->default_desc.kind) & flags))
+    if (it != columns.get<1>().end() && (defaultKindToGetKind(it->default_desc.kind) & options.kind))
         return NameAndTypePair(it->name, it->type);
 
-    auto jt = subcolumns.get<0>().find(column_name);
-    if (jt != subcolumns.get<0>().end())
-        return *jt;
+    if (options.with_subcolumns)
+    {
+        auto jt = subcolumns.get<0>().find(column_name);
+        if (jt != subcolumns.get<0>().end())
+            return *jt;
+    }
 
     return {};
 }
 
-NameAndTypePair ColumnsDescription::getColumnOrSubcolumn(GetFlags flags, const String & column_name) const
+NameAndTypePair ColumnsDescription::getColumn(const GetColumnsOptions & options, const String & column_name) const
+{
+    auto column = tryGetColumn(options, column_name);
+    if (!column)
+        throw Exception(ErrorCodes::NO_SUCH_COLUMN_IN_TABLE,
+            "There is no column {} in table.", column_name);
+
+    return *column;
+}
+
+std::optional<NameAndTypePair> ColumnsDescription::tryGetColumnOrSubcolumn(GetColumnsOptions::Kind kind, const String & column_name) const
 {
-    auto column = tryGetColumnOrSubcolumn(flags, column_name);
+    return tryGetColumn(GetColumnsOptions(kind).withSubcolumns(), column_name);
+}
+
+NameAndTypePair ColumnsDescription::getColumnOrSubcolumn(GetColumnsOptions::Kind kind, const String & column_name) const
+{
+    auto column = tryGetColumnOrSubcolumn(kind, column_name);
     if (!column)
         throw Exception(ErrorCodes::NO_SUCH_COLUMN_IN_TABLE,
             "There is no column or subcolumn {} in table.", column_name);
@@ -500,12 +568,7 @@ NameAndTypePair ColumnsDescription::getColumnOrSubcolumn(GetFlags flags, const S
 
 std::optional<NameAndTypePair> ColumnsDescription::tryGetPhysical(const String & column_name) const
 {
-    auto it = columns.get<1>().find(column_name);
-    if (it == columns.get<1>().end() ||
-            it->default_desc.kind == ColumnDefaultKind::Alias || it->default_desc.kind == ColumnDefaultKind::Ephemeral)
-        return {};
-
-    return NameAndTypePair(it->name, it->type);
+    return tryGetColumn(GetColumnsOptions::AllPhysical, column_name);
 }
 
 NameAndTypePair ColumnsDescription::getPhysical(const String & column_name) const
@@ -525,41 +588,14 @@ bool ColumnsDescription::hasPhysical(const String & column_name) const
         it->default_desc.kind != ColumnDefaultKind::Alias && it->default_desc.kind != ColumnDefaultKind::Ephemeral;
 }
 
-bool ColumnsDescription::hasColumnOrSubcolumn(GetFlags flags, const String & column_name) const
+bool ColumnsDescription::hasColumnOrSubcolumn(GetColumnsOptions::Kind kind, const String & column_name) const
 {
     auto it = columns.get<1>().find(column_name);
     return (it != columns.get<1>().end()
-        && (defaultKindToGetFlag(it->default_desc.kind) & flags))
+        && (defaultKindToGetKind(it->default_desc.kind) & kind))
             || hasSubcolumn(column_name);
 }
 
-void ColumnsDescription::addSubcolumnsToList(NamesAndTypesList & source_list) const
-{
-    NamesAndTypesList subcolumns_list;
-    for (const auto & col : source_list)
-    {
-        auto range = subcolumns.get<1>().equal_range(col.name);
-        if (range.first != range.second)
-            subcolumns_list.insert(subcolumns_list.end(), range.first, range.second);
-    }
-
-    source_list.splice(source_list.end(), std::move(subcolumns_list));
-}
-
-NamesAndTypesList ColumnsDescription::getAllWithSubcolumns() const
-{
-    auto columns_list = getAll();
-    addSubcolumnsToList(columns_list);
-    return columns_list;
-}
-
-NamesAndTypesList ColumnsDescription::getAllPhysicalWithSubcolumns() const
-{
-    auto columns_list = getAllPhysical();
-    addSubcolumnsToList(columns_list);
-    return columns_list;
-}
-
 bool ColumnsDescription::hasDefaults() const
 {
     for (const auto & column : columns)
diff --git a/src/Storages/ColumnsDescription.h b/src/Storages/ColumnsDescription.h
index 9fb03c70be91..4ae1dcfc2cd0 100644
--- a/src/Storages/ColumnsDescription.h
+++ b/src/Storages/ColumnsDescription.h
@@ -28,6 +28,44 @@ namespace ErrorCodes
     extern const int LOGICAL_ERROR;
 }
 
+struct GetColumnsOptions
+{
+    enum Kind : UInt8
+    {
+        Ordinary = 1,
+        Materialized = 2,
+        Aliases = 4,
+        Ephemeral = 8,
+
+        AllPhysical = Ordinary | Materialized,
+        All = AllPhysical | Aliases | Ephemeral,
+    };
+
+    GetColumnsOptions(Kind kind_) : kind(kind_) {}
+
+    GetColumnsOptions & withSubcolumns(bool value = true)
+    {
+        with_subcolumns = value;
+        return *this;
+    }
+
+    GetColumnsOptions & withVirtuals(bool value = true)
+    {
+        with_virtuals = value;
+        return *this;
+    }
+
+    GetColumnsOptions & withExtendedObjects(bool value = true)
+    {
+        with_extended_objects = value;
+        return *this;
+    }
+
+    Kind kind;
+    bool with_subcolumns = false;
+    bool with_virtuals = false;
+    bool with_extended_objects = false;
+};
 
 /// Description of a single table column (in CREATE TABLE for example).
 struct ColumnDescription
@@ -79,18 +117,8 @@ class ColumnsDescription
     auto begin() const { return columns.begin(); }
     auto end() const { return columns.end(); }
 
-    enum GetFlags : UInt8
-    {
-        Ordinary = 1,
-        Materialized = 2,
-        Aliases = 4,
-        Ephemeral = 8,
-
-        AllPhysical = Ordinary | Materialized,
-        All = AllPhysical | Aliases | Ephemeral,
-    };
-
-    NamesAndTypesList getByNames(GetFlags flags, const Names & names, bool with_subcolumns) const;
+    NamesAndTypesList get(const GetColumnsOptions & options) const;
+    NamesAndTypesList getByNames(const GetColumnsOptions & options, const Names & names) const;
 
     NamesAndTypesList getOrdinary() const;
     NamesAndTypesList getMaterialized() const;
@@ -99,8 +127,7 @@ class ColumnsDescription
     NamesAndTypesList getEphemeral() const;
     NamesAndTypesList getAllPhysical() const; /// ordinary + materialized.
     NamesAndTypesList getAll() const; /// ordinary + materialized + aliases + ephemeral
-    NamesAndTypesList getAllWithSubcolumns() const;
-    NamesAndTypesList getAllPhysicalWithSubcolumns() const;
+    NamesAndTypesList getSubcolumns(const String & name_in_storage) const;
 
     using ColumnTTLs = std::unordered_map<String, ASTPtr>;
     ColumnTTLs getColumnTTLs() const;
@@ -123,22 +150,27 @@ class ColumnsDescription
         auto it = columns.get<1>().find(column_name);
         if (it == columns.get<1>().end())
             throw Exception("Cannot find column " + column_name + " in ColumnsDescription", ErrorCodes::LOGICAL_ERROR);
+
+        removeSubcolumns(it->name);
         if (!columns.get<1>().modify(it, std::forward<F>(f)))
             throw Exception("Cannot modify ColumnDescription for column " + column_name + ": column name cannot be changed", ErrorCodes::LOGICAL_ERROR);
 
+        addSubcolumns(it->name, it->type);
         modifyColumnOrder(column_name, after_column, first);
     }
 
     Names getNamesOfPhysical() const;
 
     bool hasPhysical(const String & column_name) const;
-    bool hasColumnOrSubcolumn(GetFlags flags, const String & column_name) const;
+    bool hasColumnOrSubcolumn(GetColumnsOptions::Kind kind, const String & column_name) const;
 
     NameAndTypePair getPhysical(const String & column_name) const;
-    NameAndTypePair getColumnOrSubcolumn(GetFlags flags, const String & column_name) const;
+    NameAndTypePair getColumnOrSubcolumn(GetColumnsOptions::Kind kind, const String & column_name) const;
+    NameAndTypePair getColumn(const GetColumnsOptions & options, const String & column_name) const;
 
     std::optional<NameAndTypePair> tryGetPhysical(const String & column_name) const;
-    std::optional<NameAndTypePair> tryGetColumnOrSubcolumn(GetFlags flags, const String & column_name) const;
+    std::optional<NameAndTypePair> tryGetColumnOrSubcolumn(GetColumnsOptions::Kind kind, const String & column_name) const;
+    std::optional<NameAndTypePair> tryGetColumn(const GetColumnsOptions & options, const String & column_name) const;
 
     ColumnDefaults getDefaults() const; /// TODO: remove
     bool hasDefault(const String & column_name) const;
diff --git a/src/Storages/FileLog/FileLogSource.cpp b/src/Storages/FileLog/FileLogSource.cpp
index 7d4b5ac6fec2..be818b93a4ca 100644
--- a/src/Storages/FileLog/FileLogSource.cpp
+++ b/src/Storages/FileLog/FileLogSource.cpp
@@ -12,25 +12,24 @@ static constexpr auto MAX_FAILED_POLL_ATTEMPTS = 10;
 
 FileLogSource::FileLogSource(
     StorageFileLog & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     const ContextPtr & context_,
     const Names & columns,
     size_t max_block_size_,
     size_t poll_time_out_,
     size_t stream_number_,
     size_t max_streams_number_)
-    : SourceWithProgress(metadata_snapshot_->getSampleBlockForColumns(columns, storage_.getVirtuals(), storage_.getStorageID()))
+    : SourceWithProgress(storage_snapshot_->getSampleBlockForColumns(columns))
     , storage(storage_)
-    , metadata_snapshot(metadata_snapshot_)
+    , storage_snapshot(storage_snapshot_)
     , context(context_)
     , column_names(columns)
     , max_block_size(max_block_size_)
     , poll_time_out(poll_time_out_)
     , stream_number(stream_number_)
     , max_streams_number(max_streams_number_)
-    , non_virtual_header(metadata_snapshot_->getSampleBlockNonMaterialized())
-    , virtual_header(
-          metadata_snapshot->getSampleBlockForColumns(storage.getVirtualColumnNames(), storage.getVirtuals(), storage.getStorageID()))
+    , non_virtual_header(storage_snapshot->metadata->getSampleBlockNonMaterialized())
+    , virtual_header(storage_snapshot->getSampleBlockForColumns(storage.getVirtualColumnNames()))
 {
     buffer = std::make_unique<ReadBufferFromFileLog>(storage, max_block_size, poll_time_out, context, stream_number_, max_streams_number_);
 
diff --git a/src/Storages/FileLog/FileLogSource.h b/src/Storages/FileLog/FileLogSource.h
index f1cc83b4a068..831f4c907a58 100644
--- a/src/Storages/FileLog/FileLogSource.h
+++ b/src/Storages/FileLog/FileLogSource.h
@@ -15,7 +15,7 @@ class FileLogSource : public SourceWithProgress
 public:
     FileLogSource(
         StorageFileLog & storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         const ContextPtr & context_,
         const Names & columns,
         size_t max_block_size_,
@@ -36,7 +36,7 @@ class FileLogSource : public SourceWithProgress
 
 private:
     StorageFileLog & storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     ContextPtr context;
     Names column_names;
     UInt64 max_block_size;
diff --git a/src/Storages/FileLog/StorageFileLog.cpp b/src/Storages/FileLog/StorageFileLog.cpp
index 700b35a5a48f..32ca936f0392 100644
--- a/src/Storages/FileLog/StorageFileLog.cpp
+++ b/src/Storages/FileLog/StorageFileLog.cpp
@@ -313,7 +313,7 @@ UInt64 StorageFileLog::getInode(const String & file_name)
 
 Pipe StorageFileLog::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /* query_info */,
     ContextPtr local_context,
     QueryProcessingStage::Enum /* processed_stage */,
@@ -355,7 +355,7 @@ Pipe StorageFileLog::read(
     {
         pipes.emplace_back(std::make_shared<FileLogSource>(
             *this,
-            metadata_snapshot,
+            storage_snapshot,
             modified_context,
             column_names,
             getMaxBlockSize(),
@@ -677,7 +677,9 @@ bool StorageFileLog::streamToViews()
     auto table = DatabaseCatalog::instance().getTable(table_id, getContext());
     if (!table)
         throw Exception("Engine table " + table_id.getNameForLogs() + " doesn't exist", ErrorCodes::LOGICAL_ERROR);
+
     auto metadata_snapshot = getInMemoryMetadataPtr();
+    auto storage_snapshot = getStorageSnapshot(metadata_snapshot);
 
     auto max_streams_number = std::min<UInt64>(filelog_settings->max_threads.value, file_infos.file_names.size());
     /// No files to parse
@@ -705,7 +707,7 @@ bool StorageFileLog::streamToViews()
     {
         pipes.emplace_back(std::make_shared<FileLogSource>(
             *this,
-            metadata_snapshot,
+            storage_snapshot,
             new_context,
             block_io.pipeline.getHeader().getNames(),
             getPollMaxBatchSize(),
diff --git a/src/Storages/FileLog/StorageFileLog.h b/src/Storages/FileLog/StorageFileLog.h
index 98915f10a05e..ded97ecbd8c7 100644
--- a/src/Storages/FileLog/StorageFileLog.h
+++ b/src/Storages/FileLog/StorageFileLog.h
@@ -43,7 +43,7 @@ class StorageFileLog final : public shared_ptr_helper<StorageFileLog>, public IS
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/HDFS/StorageHDFS.cpp b/src/Storages/HDFS/StorageHDFS.cpp
index 13873aa21abf..74f6937dbae0 100644
--- a/src/Storages/HDFS/StorageHDFS.cpp
+++ b/src/Storages/HDFS/StorageHDFS.cpp
@@ -292,16 +292,15 @@ Block HDFSSource::getHeader(const StorageMetadataPtr & metadata_snapshot, bool n
 
 Block HDFSSource::getBlockForSource(
     const StorageHDFSPtr & storage,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     const ColumnsDescription & columns_description,
     bool need_path_column,
     bool need_file_column)
 {
     if (storage->isColumnOriented())
-        return metadata_snapshot->getSampleBlockForColumns(
-            columns_description.getNamesOfPhysical(), storage->getVirtuals(), storage->getStorageID());
+        return storage_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
     else
-        return getHeader(metadata_snapshot, need_path_column, need_file_column);
+        return getHeader(storage_snapshot->metadata, need_path_column, need_file_column);
 }
 
 HDFSSource::DisclosedGlobIterator::DisclosedGlobIterator(ContextPtr context_, const String & uri)
@@ -324,17 +323,17 @@ String HDFSSource::URISIterator::next()
 
 HDFSSource::HDFSSource(
     StorageHDFSPtr storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     ContextPtr context_,
     UInt64 max_block_size_,
     bool need_path_column_,
     bool need_file_column_,
     std::shared_ptr<IteratorWrapper> file_iterator_,
     ColumnsDescription columns_description_)
-    : SourceWithProgress(getBlockForSource(storage_, metadata_snapshot_, columns_description_, need_path_column_, need_file_column_))
+    : SourceWithProgress(getBlockForSource(storage_, storage_snapshot_, columns_description_, need_path_column_, need_file_column_))
     , WithContext(context_)
     , storage(std::move(storage_))
-    , metadata_snapshot(metadata_snapshot_)
+    , storage_snapshot(storage_snapshot_)
     , max_block_size(max_block_size_)
     , need_path_column(need_path_column_)
     , need_file_column(need_file_column_)
@@ -365,8 +364,8 @@ bool HDFSSource::initialize()
     auto get_block_for_format = [&]() -> Block
     {
         if (storage->isColumnOriented())
-            return metadata_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
-        return metadata_snapshot->getSampleBlock();
+            return storage_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
+        return storage_snapshot->metadata->getSampleBlock();
     };
 
     auto input_format = getContext()->getInputFormat(storage->format_name, *read_buf, get_block_for_format(), max_block_size);
@@ -520,7 +519,7 @@ bool StorageHDFS::isColumnOriented() const
 
 Pipe StorageHDFS::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context_,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -571,15 +570,14 @@ Pipe StorageHDFS::read(
          const auto get_columns_for_format = [&]() -> ColumnsDescription
         {
             if (isColumnOriented())
-                return ColumnsDescription{
-                    metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID()).getNamesAndTypesList()};
+                return ColumnsDescription{storage_snapshot->getSampleBlockForColumns(column_names).getNamesAndTypesList()};
             else
-                return metadata_snapshot->getColumns();
+                return storage_snapshot->metadata->getColumns();
         };
 
         pipes.emplace_back(std::make_shared<HDFSSource>(
             this_ptr,
-            metadata_snapshot,
+            storage_snapshot,
             context_,
             max_block_size,
             need_path_column,
diff --git a/src/Storages/HDFS/StorageHDFS.h b/src/Storages/HDFS/StorageHDFS.h
index 99b5ba95d253..e87564aef322 100644
--- a/src/Storages/HDFS/StorageHDFS.h
+++ b/src/Storages/HDFS/StorageHDFS.h
@@ -24,7 +24,7 @@ class StorageHDFS final : public shared_ptr_helper<StorageHDFS>, public IStorage
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -117,14 +117,14 @@ class HDFSSource : public SourceWithProgress, WithContext
 
     static Block getBlockForSource(
         const StorageHDFSPtr & storage,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot_,
         const ColumnsDescription & columns_description,
         bool need_path_column,
         bool need_file_column);
 
     HDFSSource(
         StorageHDFSPtr storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         ContextPtr context_,
         UInt64 max_block_size_,
         bool need_path_column_,
@@ -140,7 +140,7 @@ class HDFSSource : public SourceWithProgress, WithContext
 
 private:
     StorageHDFSPtr storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     UInt64 max_block_size;
     bool need_path_column;
     bool need_file_column;
diff --git a/src/Storages/HDFS/StorageHDFSCluster.cpp b/src/Storages/HDFS/StorageHDFSCluster.cpp
index dde3329040bc..b039caa43309 100644
--- a/src/Storages/HDFS/StorageHDFSCluster.cpp
+++ b/src/Storages/HDFS/StorageHDFSCluster.cpp
@@ -50,7 +50,7 @@ StorageHDFSCluster::StorageHDFSCluster(
 /// The code executes on initiator
 Pipe StorageHDFSCluster::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
@@ -106,12 +106,12 @@ Pipe StorageHDFSCluster::read(
         }
     }
 
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
     return Pipe::unitePipes(std::move(pipes));
 }
 
 QueryProcessingStage::Enum StorageHDFSCluster::getQueryProcessingStage(
-    ContextPtr context, QueryProcessingStage::Enum to_stage, const StorageMetadataPtr &, SelectQueryInfo &) const
+    ContextPtr context, QueryProcessingStage::Enum to_stage, const StorageSnapshotPtr &, SelectQueryInfo &) const
 {
     /// Initiator executes query on remote node.
     if (context->getClientInfo().query_kind == ClientInfo::QueryKind::INITIAL_QUERY)
diff --git a/src/Storages/HDFS/StorageHDFSCluster.h b/src/Storages/HDFS/StorageHDFSCluster.h
index 0e568a9faf8b..953311de056a 100644
--- a/src/Storages/HDFS/StorageHDFSCluster.h
+++ b/src/Storages/HDFS/StorageHDFSCluster.h
@@ -24,11 +24,11 @@ class StorageHDFSCluster : public shared_ptr_helper<StorageHDFSCluster>, public
 public:
     std::string getName() const override { return "HDFSCluster"; }
 
-    Pipe read(const Names &, const StorageMetadataPtr &, SelectQueryInfo &,
+    Pipe read(const Names &, const StorageSnapshotPtr &, SelectQueryInfo &,
         ContextPtr, QueryProcessingStage::Enum, size_t /*max_block_size*/, unsigned /*num_streams*/) override;
 
     QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const override;
+    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const override;
 
     NamesAndTypesList getVirtuals() const override;
 
diff --git a/src/Storages/Hive/StorageHive.cpp b/src/Storages/Hive/StorageHive.cpp
index 2ae7c30fd5b3..7b6a8db568fc 100644
--- a/src/Storages/Hive/StorageHive.cpp
+++ b/src/Storages/Hive/StorageHive.cpp
@@ -590,7 +590,7 @@ void StorageHive::getActualColumnsToRead(Block & sample_block, const Block & hea
 }
 Pipe StorageHive::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context_,
     QueryProcessingStage::Enum /* processed_stage */,
@@ -654,7 +654,7 @@ Pipe StorageHive::read(
     sources_info->hive_metastore_client = hive_metastore_client;
     sources_info->partition_name_types = partition_name_types;
 
-    const auto & header_block = metadata_snapshot->getSampleBlock();
+    const auto & header_block = storage_snapshot->metadata->getSampleBlock();
     Block sample_block;
     for (const auto & column : column_names)
     {
diff --git a/src/Storages/Hive/StorageHive.h b/src/Storages/Hive/StorageHive.h
index 323293cbbe01..376aab311d03 100644
--- a/src/Storages/Hive/StorageHive.h
+++ b/src/Storages/Hive/StorageHive.h
@@ -42,7 +42,7 @@ class StorageHive final : public shared_ptr_helper<StorageHive>, public IStorage
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/IStorage.cpp b/src/Storages/IStorage.cpp
index a923258b111c..89403a773b37 100644
--- a/src/Storages/IStorage.cpp
+++ b/src/Storages/IStorage.cpp
@@ -90,7 +90,7 @@ TableExclusiveLockHolder IStorage::lockExclusively(const String & query_id, cons
 
 Pipe IStorage::read(
     const Names & /*column_names*/,
-    const StorageMetadataPtr & /*metadata_snapshot*/,
+    const StorageSnapshotPtr & /*storage_snapshot*/,
     SelectQueryInfo & /*query_info*/,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -103,18 +103,17 @@ Pipe IStorage::read(
 void IStorage::read(
     QueryPlan & query_plan,
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
     size_t max_block_size,
     unsigned num_streams)
 {
-    auto pipe = read(column_names, metadata_snapshot, query_info, context, processed_stage, max_block_size, num_streams);
+    auto pipe = read(column_names, storage_snapshot, query_info, context, processed_stage, max_block_size, num_streams);
     if (pipe.empty())
     {
-        auto header = (query_info.projection ? query_info.projection->desc->metadata : metadata_snapshot)
-                          ->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID());
+        auto header = storage_snapshot->getSampleBlockForColumns(column_names);
         InterpreterSelectQuery::addEmptySourceToQueryPlan(query_plan, header, query_info, context);
     }
     else
diff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h
index 1010164f71e7..e9969859d5fb 100644
--- a/src/Storages/IStorage.h
+++ b/src/Storages/IStorage.h
@@ -13,6 +13,7 @@
 #include <Storages/SelectQueryDescription.h>
 #include <Storages/StorageInMemoryMetadata.h>
 #include <Storages/TableLockHolder.h>
+#include <Storages/StorageSnapshot.h>
 #include <Common/ActionLock.h>
 #include <Common/Exception.h>
 #include <Common/RWLock.h>
@@ -158,6 +159,10 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo
     /// Returns true if the storage supports reading of subcolumns of complex types.
     virtual bool supportsSubcolumns() const { return false; }
 
+    /// Returns true if the storage supports storing of dynamic subcolumns.
+    /// For now it makes sense only for data type Object.
+    virtual bool supportsDynamicSubcolumns() const { return false; }
+
     /// Requires squashing small blocks to large for optimal storage.
     /// This is true for most storages that store data on disk.
     virtual bool prefersLargeBlocks() const { return true; }
@@ -269,8 +274,7 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo
       * QueryProcessingStage::Enum required for Distributed over Distributed,
       * since it cannot return Complete for intermediate queries never.
       */
-    virtual QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const
+    virtual QueryProcessingStage::Enum getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const
     {
         return QueryProcessingStage::FetchColumns;
     }
@@ -331,7 +335,7 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo
       */
     virtual Pipe read(
         const Names & /*column_names*/,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & /*storage_snapshot*/,
         SelectQueryInfo & /*query_info*/,
         ContextPtr /*context*/,
         QueryProcessingStage::Enum /*processed_stage*/,
@@ -343,7 +347,7 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo
     virtual void read(
         QueryPlan & query_plan,
         const Names & /*column_names*/,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & /*storage_snapshot*/,
         SelectQueryInfo & /*query_info*/,
         ContextPtr /*context*/,
         QueryProcessingStage::Enum /*processed_stage*/,
@@ -600,6 +604,18 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo
     /// Does not takes underlying Storage (if any) into account.
     virtual std::optional<UInt64> lifetimeBytes() const { return {}; }
 
+    /// Creates a storage snapshot from given metadata.
+    virtual StorageSnapshotPtr getStorageSnapshot(const StorageMetadataPtr & metadata_snapshot) const
+    {
+        return std::make_shared<StorageSnapshot>(*this, metadata_snapshot);
+    }
+
+    /// Creates a storage snapshot from given metadata and columns, which are used in query.
+    virtual StorageSnapshotPtr getStorageSnapshotForQuery(const StorageMetadataPtr & metadata_snapshot, const ASTPtr & /*query*/) const
+    {
+        return getStorageSnapshot(metadata_snapshot);
+    }
+
 private:
     /// Lock required for alter queries (lockForAlter).
     /// Allows to execute only one simultaneous alter query.
diff --git a/src/Storages/Kafka/KafkaSource.cpp b/src/Storages/Kafka/KafkaSource.cpp
index 3e24608a1808..99130f615f59 100644
--- a/src/Storages/Kafka/KafkaSource.cpp
+++ b/src/Storages/Kafka/KafkaSource.cpp
@@ -19,22 +19,22 @@ const auto MAX_FAILED_POLL_ATTEMPTS = 10;
 
 KafkaSource::KafkaSource(
     StorageKafka & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     const ContextPtr & context_,
     const Names & columns,
     Poco::Logger * log_,
     size_t max_block_size_,
     bool commit_in_suffix_)
-    : SourceWithProgress(metadata_snapshot_->getSampleBlockForColumns(columns, storage_.getVirtuals(), storage_.getStorageID()))
+    : SourceWithProgress(storage_snapshot_->getSampleBlockForColumns(columns))
     , storage(storage_)
-    , metadata_snapshot(metadata_snapshot_)
+    , storage_snapshot(storage_snapshot_)
     , context(context_)
     , column_names(columns)
     , log(log_)
     , max_block_size(max_block_size_)
     , commit_in_suffix(commit_in_suffix_)
-    , non_virtual_header(metadata_snapshot->getSampleBlockNonMaterialized())
-    , virtual_header(metadata_snapshot->getSampleBlockForColumns(storage.getVirtualColumnNames(), storage.getVirtuals(), storage.getStorageID()))
+    , non_virtual_header(storage_snapshot->metadata->getSampleBlockNonMaterialized())
+    , virtual_header(storage_snapshot->getSampleBlockForColumns(storage.getVirtualColumnNames()))
     , handle_error_mode(storage.getHandleKafkaErrorMode())
 {
 }
diff --git a/src/Storages/Kafka/KafkaSource.h b/src/Storages/Kafka/KafkaSource.h
index e80edfb9ef45..59b6d370b715 100644
--- a/src/Storages/Kafka/KafkaSource.h
+++ b/src/Storages/Kafka/KafkaSource.h
@@ -18,7 +18,7 @@ class KafkaSource : public SourceWithProgress
 public:
     KafkaSource(
         StorageKafka & storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         const ContextPtr & context_,
         const Names & columns,
         Poco::Logger * log_,
@@ -35,7 +35,7 @@ class KafkaSource : public SourceWithProgress
 
 private:
     StorageKafka & storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     ContextPtr context;
     Names column_names;
     Poco::Logger * log;
diff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp
index ae470cdccc95..4c7465d587d1 100644
--- a/src/Storages/Kafka/StorageKafka.cpp
+++ b/src/Storages/Kafka/StorageKafka.cpp
@@ -263,7 +263,7 @@ String StorageKafka::getDefaultClientId(const StorageID & table_id_)
 
 Pipe StorageKafka::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /* query_info */,
     ContextPtr local_context,
     QueryProcessingStage::Enum /* processed_stage */,
@@ -291,7 +291,7 @@ Pipe StorageKafka::read(
         /// Use block size of 1, otherwise LIMIT won't work properly as it will buffer excess messages in the last block
         /// TODO: probably that leads to awful performance.
         /// FIXME: seems that doesn't help with extra reading and committing unprocessed messages.
-        pipes.emplace_back(std::make_shared<KafkaSource>(*this, metadata_snapshot, modified_context, column_names, log, 1, kafka_settings->kafka_commit_on_select));
+        pipes.emplace_back(std::make_shared<KafkaSource>(*this, storage_snapshot, modified_context, column_names, log, 1, kafka_settings->kafka_commit_on_select));
     }
 
     LOG_DEBUG(log, "Starting reading {} streams", pipes.size());
@@ -614,7 +614,8 @@ bool StorageKafka::streamToViews()
     auto table = DatabaseCatalog::instance().getTable(table_id, getContext());
     if (!table)
         throw Exception("Engine table " + table_id.getNameForLogs() + " doesn't exist.", ErrorCodes::LOGICAL_ERROR);
-    auto metadata_snapshot = getInMemoryMetadataPtr();
+
+    auto storage_snapshot = getStorageSnapshot(getInMemoryMetadataPtr());
 
     // Create an INSERT query for streaming data
     auto insert = std::make_shared<ASTInsertQuery>();
@@ -640,7 +641,7 @@ bool StorageKafka::streamToViews()
     pipes.reserve(stream_count);
     for (size_t i = 0; i < stream_count; ++i)
     {
-        auto source = std::make_shared<KafkaSource>(*this, metadata_snapshot, kafka_context, block_io.pipeline.getHeader().getNames(), log, block_size, false);
+        auto source = std::make_shared<KafkaSource>(*this, storage_snapshot, kafka_context, block_io.pipeline.getHeader().getNames(), log, block_size, false);
         sources.emplace_back(source);
         pipes.emplace_back(source);
 
diff --git a/src/Storages/Kafka/StorageKafka.h b/src/Storages/Kafka/StorageKafka.h
index 03e90b1f6c3d..707db7a798e3 100644
--- a/src/Storages/Kafka/StorageKafka.h
+++ b/src/Storages/Kafka/StorageKafka.h
@@ -43,7 +43,7 @@ class StorageKafka final : public shared_ptr_helper<StorageKafka>, public IStora
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/LiveView/StorageBlocks.h b/src/Storages/LiveView/StorageBlocks.h
index 01293a1e5d7f..bc860a1fa3c6 100644
--- a/src/Storages/LiveView/StorageBlocks.h
+++ b/src/Storages/LiveView/StorageBlocks.h
@@ -34,14 +34,14 @@ class StorageBlocks : public IStorage
     bool supportsFinal() const override { return true; }
 
     QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const override
+    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const override
     {
         return to_stage;
     }
 
     Pipe read(
         const Names & /*column_names*/,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & /*storage_snapshot*/,
         SelectQueryInfo & /*query_info*/,
         ContextPtr /*context*/,
         QueryProcessingStage::Enum /*processed_stage*/,
diff --git a/src/Storages/LiveView/StorageLiveView.cpp b/src/Storages/LiveView/StorageLiveView.cpp
index 83578e3b5b99..8f80f8632cc4 100644
--- a/src/Storages/LiveView/StorageLiveView.cpp
+++ b/src/Storages/LiveView/StorageLiveView.cpp
@@ -540,7 +540,7 @@ void StorageLiveView::refresh(bool grab_lock)
 
 Pipe StorageLiveView::read(
     const Names & /*column_names*/,
-    const StorageMetadataPtr & /*metadata_snapshot*/,
+    const StorageSnapshotPtr & /*storage_snapshot*/,
     SelectQueryInfo & /*query_info*/,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
diff --git a/src/Storages/LiveView/StorageLiveView.h b/src/Storages/LiveView/StorageLiveView.h
index 17e2f50e7ec3..2fb2ec509fa0 100644
--- a/src/Storages/LiveView/StorageLiveView.h
+++ b/src/Storages/LiveView/StorageLiveView.h
@@ -146,7 +146,7 @@ friend class LiveViewSink;
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/MergeTree/IMergeTreeReader.cpp b/src/Storages/MergeTree/IMergeTreeReader.cpp
index 87cda7218626..3a823345ddaf 100644
--- a/src/Storages/MergeTree/IMergeTreeReader.cpp
+++ b/src/Storages/MergeTree/IMergeTreeReader.cpp
@@ -62,101 +62,13 @@ const IMergeTreeReader::ValueSizeMap & IMergeTreeReader::getAvgValueSizeHints()
     return avg_value_size_hints;
 }
 
-
-static bool arrayHasNoElementsRead(const IColumn & column)
-{
-    const auto * column_array = typeid_cast<const ColumnArray *>(&column);
-
-    if (!column_array)
-        return false;
-
-    size_t size = column_array->size();
-    if (!size)
-        return false;
-
-    size_t data_size = column_array->getData().size();
-    if (data_size)
-        return false;
-
-    size_t last_offset = column_array->getOffsets()[size - 1];
-    return last_offset != 0;
-}
-
-void IMergeTreeReader::fillMissingColumns(Columns & res_columns, bool & should_evaluate_missing_defaults, size_t num_rows)
+void IMergeTreeReader::fillMissingColumns(Columns & res_columns, bool & should_evaluate_missing_defaults, size_t num_rows) const
 {
     try
     {
-        size_t num_columns = columns.size();
-
-        if (res_columns.size() != num_columns)
-            throw Exception("invalid number of columns passed to MergeTreeReader::fillMissingColumns. "
-                            "Expected " + toString(num_columns) + ", "
-                            "got " + toString(res_columns.size()), ErrorCodes::LOGICAL_ERROR);
-
-        /// For a missing column of a nested data structure we must create not a column of empty
-        /// arrays, but a column of arrays of correct length.
-
-        /// First, collect offset columns for all arrays in the block.
-        OffsetColumns offset_columns;
-        auto requested_column = columns.begin();
-        for (size_t i = 0; i < num_columns; ++i, ++requested_column)
-        {
-            if (res_columns[i] == nullptr)
-                continue;
-
-            if (const auto * array = typeid_cast<const ColumnArray *>(res_columns[i].get()))
-            {
-                String offsets_name = Nested::extractTableName(requested_column->name);
-                auto & offsets_column = offset_columns[offsets_name];
-
-                /// If for some reason multiple offsets columns are present for the same nested data structure,
-                /// choose the one that is not empty.
-                if (!offsets_column || offsets_column->empty())
-                    offsets_column = array->getOffsetsPtr();
-            }
-        }
-
-        should_evaluate_missing_defaults = false;
-
-        /// insert default values only for columns without default expressions
-        requested_column = columns.begin();
-        for (size_t i = 0; i < num_columns; ++i, ++requested_column)
-        {
-            auto & [name, type] = *requested_column;
-
-            if (res_columns[i] && arrayHasNoElementsRead(*res_columns[i]))
-                res_columns[i] = nullptr;
-
-            if (res_columns[i] == nullptr)
-            {
-                if (metadata_snapshot->getColumns().hasDefault(name))
-                {
-                    should_evaluate_missing_defaults = true;
-                    continue;
-                }
-
-                String offsets_name = Nested::extractTableName(name);
-                auto offset_it = offset_columns.find(offsets_name);
-                const auto * array_type = typeid_cast<const DataTypeArray *>(type.get());
-                if (offset_it != offset_columns.end() && array_type)
-                {
-                    const auto & nested_type = array_type->getNestedType();
-                    ColumnPtr offsets_column = offset_it->second;
-                    size_t nested_rows = typeid_cast<const ColumnUInt64 &>(*offsets_column).getData().back();
-
-                    ColumnPtr nested_column =
-                        nested_type->createColumnConstWithDefaultValue(nested_rows)->convertToFullColumnIfConst();
-
-                    res_columns[i] = ColumnArray::create(nested_column, offsets_column);
-                }
-                else
-                {
-                    /// We must turn a constant column into a full column because the interpreter could infer
-                    /// that it is constant everywhere but in some blocks (from other parts) it can be a full column.
-                    res_columns[i] = type->createColumnConstWithDefaultValue(num_rows)->convertToFullColumnIfConst();
-                }
-            }
-        }
+        DB::fillMissingColumns(res_columns, num_rows, columns, metadata_snapshot);
+        should_evaluate_missing_defaults = std::any_of(
+            res_columns.begin(), res_columns.end(), [](const auto & column) { return column == nullptr; });
     }
     catch (Exception & e)
     {
@@ -166,7 +78,7 @@ void IMergeTreeReader::fillMissingColumns(Columns & res_columns, bool & should_e
     }
 }
 
-void IMergeTreeReader::evaluateMissingDefaults(Block additional_columns, Columns & res_columns)
+void IMergeTreeReader::evaluateMissingDefaults(Block additional_columns, Columns & res_columns) const
 {
     try
     {
@@ -245,7 +157,7 @@ NameAndTypePair IMergeTreeReader::getColumnFromPart(const NameAndTypePair & requ
     return {String(it->getKey()), type};
 }
 
-void IMergeTreeReader::performRequiredConversions(Columns & res_columns)
+void IMergeTreeReader::performRequiredConversions(Columns & res_columns) const
 {
     try
     {
diff --git a/src/Storages/MergeTree/IMergeTreeReader.h b/src/Storages/MergeTree/IMergeTreeReader.h
index 28334b9a8bb8..7c5977b5cb25 100644
--- a/src/Storages/MergeTree/IMergeTreeReader.h
+++ b/src/Storages/MergeTree/IMergeTreeReader.h
@@ -44,13 +44,13 @@ class IMergeTreeReader : private boost::noncopyable
     /// Add columns from ordered_names that are not present in the block.
     /// Missing columns are added in the order specified by ordered_names.
     /// num_rows is needed in case if all res_columns are nullptr.
-    void fillMissingColumns(Columns & res_columns, bool & should_evaluate_missing_defaults, size_t num_rows);
+    void fillMissingColumns(Columns & res_columns, bool & should_evaluate_missing_defaults, size_t num_rows) const;
     /// Evaluate defaulted columns if necessary.
-    void evaluateMissingDefaults(Block additional_columns, Columns & res_columns);
+    void evaluateMissingDefaults(Block additional_columns, Columns & res_columns) const;
 
     /// If part metadata is not equal to storage metadata, than
     /// try to perform conversions of columns.
-    void performRequiredConversions(Columns & res_columns);
+    void performRequiredConversions(Columns & res_columns) const;
 
     const NamesAndTypesList & getColumns() const { return columns; }
     size_t numColumnsInResult() const { return columns.size(); }
diff --git a/src/Storages/MergeTree/MergeTask.cpp b/src/Storages/MergeTree/MergeTask.cpp
index 8b5c2e0dc6e1..935a11ec5fae 100644
--- a/src/Storages/MergeTree/MergeTask.cpp
+++ b/src/Storages/MergeTree/MergeTask.cpp
@@ -6,6 +6,7 @@
 #include <base/logger_useful.h>
 #include <Common/ActionBlocker.h>
 
+#include <DataTypes/ObjectUtils.h>
 #include <DataTypes/Serializations/SerializationInfo.h>
 #include <Storages/MergeTree/MergeTreeData.h>
 #include <Storages/MergeTree/IMergeTreeDataPart.h>
@@ -134,6 +135,10 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare()
     global_ctx->all_column_names = global_ctx->metadata_snapshot->getColumns().getNamesOfPhysical();
     global_ctx->storage_columns = global_ctx->metadata_snapshot->getColumns().getAllPhysical();
 
+    auto object_columns = MergeTreeData::getObjectColumns(global_ctx->future_part->parts, global_ctx->metadata_snapshot->getColumns());
+    global_ctx->storage_snapshot = std::make_shared<StorageSnapshot>(*global_ctx->data, global_ctx->metadata_snapshot, object_columns);
+    extendObjectColumns(global_ctx->storage_columns, object_columns, false);
+
     extractMergingAndGatheringColumns(
         global_ctx->storage_columns,
         global_ctx->metadata_snapshot->getSortingKey().expression,
@@ -414,7 +419,7 @@ void MergeTask::VerticalMergeStage::prepareVerticalMergeForOneColumn() const
     for (size_t part_num = 0; part_num < global_ctx->future_part->parts.size(); ++part_num)
     {
         auto column_part_source = std::make_shared<MergeTreeSequentialSource>(
-            *global_ctx->data, global_ctx->metadata_snapshot, global_ctx->future_part->parts[part_num], column_names, ctx->read_with_direct_io, true);
+            *global_ctx->data, global_ctx->storage_snapshot, global_ctx->future_part->parts[part_num], column_names, ctx->read_with_direct_io, true);
 
         /// Dereference unique_ptr
         column_part_source->setProgressCallback(
@@ -748,7 +753,7 @@ void MergeTask::ExecuteAndFinalizeHorizontalPart::createMergedStream()
     for (const auto & part : global_ctx->future_part->parts)
     {
         auto input = std::make_unique<MergeTreeSequentialSource>(
-            *global_ctx->data, global_ctx->metadata_snapshot, part, global_ctx->merging_column_names, ctx->read_with_direct_io, true);
+            *global_ctx->data, global_ctx->storage_snapshot, part, global_ctx->merging_column_names, ctx->read_with_direct_io, true);
 
         /// Dereference unique_ptr and pass horizontal_stage_progress by reference
         input->setProgressCallback(
diff --git a/src/Storages/MergeTree/MergeTask.h b/src/Storages/MergeTree/MergeTask.h
index d3fc01980eae..04da9ad77c4e 100644
--- a/src/Storages/MergeTree/MergeTask.h
+++ b/src/Storages/MergeTree/MergeTask.h
@@ -127,6 +127,7 @@ class MergeTask
         MergeTreeDataMergerMutator * mutator{nullptr};
         ActionBlocker * merges_blocker{nullptr};
         ActionBlocker * ttl_merges_blocker{nullptr};
+        StorageSnapshotPtr storage_snapshot{nullptr};
         StorageMetadataPtr metadata_snapshot{nullptr};
         FutureMergedMutatedPartPtr future_part{nullptr};
         /// This will be either nullptr or new_data_part, so raw pointer is ok.
diff --git a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp
index 5b69a4e68b65..c656de61bfdc 100644
--- a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp
+++ b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp
@@ -29,7 +29,7 @@ namespace ErrorCodes
 MergeTreeBaseSelectProcessor::MergeTreeBaseSelectProcessor(
     Block header,
     const MergeTreeData & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     const PrewhereInfoPtr & prewhere_info_,
     ExpressionActionsSettings actions_settings,
     UInt64 max_block_size_rows_,
@@ -41,7 +41,7 @@ MergeTreeBaseSelectProcessor::MergeTreeBaseSelectProcessor(
     std::optional<ParallelReadingExtension> extension_)
     : SourceWithProgress(transformHeader(std::move(header), prewhere_info_, storage_.getPartitionValueType(), virt_column_names_))
     , storage(storage_)
-    , metadata_snapshot(metadata_snapshot_)
+    , storage_snapshot(storage_snapshot_)
     , prewhere_info(prewhere_info_)
     , max_block_size_rows(max_block_size_rows_)
     , preferred_block_size_bytes(preferred_block_size_bytes_)
diff --git a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h
index 2e906ecfce03..4b933175ba00 100644
--- a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h
+++ b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h
@@ -35,7 +35,7 @@ class MergeTreeBaseSelectProcessor : public SourceWithProgress
     MergeTreeBaseSelectProcessor(
         Block header,
         const MergeTreeData & storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         const PrewhereInfoPtr & prewhere_info_,
         ExpressionActionsSettings actions_settings,
         UInt64 max_block_size_rows_,
@@ -87,7 +87,7 @@ class MergeTreeBaseSelectProcessor : public SourceWithProgress
     void initializeRangeReaders(MergeTreeReadTask & task);
 
     const MergeTreeData & storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
 
     PrewhereInfoPtr prewhere_info;
     std::unique_ptr<PrewhereExprInfo> prewhere_actions;
diff --git a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp
index dadccd2f9dc3..6e72b843f101 100644
--- a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp
+++ b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp
@@ -1,5 +1,6 @@
 #include <Storages/MergeTree/MergeTreeBlockReadUtils.h>
 #include <Storages/MergeTree/MergeTreeData.h>
+#include <DataTypes/NestedUtils.h>
 #include <Core/NamesAndTypes.h>
 #include <Common/checkStackSize.h>
 #include <Common/typeid_cast.h>
@@ -35,7 +36,7 @@ bool injectRequiredColumnsRecursively(
     /// stages.
     checkStackSize();
 
-    auto column_in_storage = storage_columns.tryGetColumnOrSubcolumn(ColumnsDescription::AllPhysical, column_name);
+    auto column_in_storage = storage_columns.tryGetColumnOrSubcolumn(GetColumnsOptions::AllPhysical, column_name);
     if (column_in_storage)
     {
         auto column_name_in_part = column_in_storage->getNameInStorage();
@@ -92,8 +93,15 @@ NameSet injectRequiredColumns(const MergeTreeData & storage, const StorageMetada
         alter_conversions = storage.getAlterConversionsForPart(part);
     for (size_t i = 0; i < columns.size(); ++i)
     {
+        auto name_in_storage = Nested::extractTableName(columns[i]);
+        if (storage_columns.has(name_in_storage) && isObject(storage_columns.get(name_in_storage).type))
+        {
+            have_at_least_one_physical_column = true;
+            continue;
+        }
+
         /// We are going to fetch only physical columns
-        if (!storage_columns.hasColumnOrSubcolumn(ColumnsDescription::AllPhysical, columns[i]))
+        if (!storage_columns.hasColumnOrSubcolumn(GetColumnsOptions::AllPhysical, columns[i]))
             throw Exception("There is no physical column or subcolumn " + columns[i] + " in table.", ErrorCodes::NO_SUCH_COLUMN_IN_TABLE);
 
         have_at_least_one_physical_column |= injectRequiredColumnsRecursively(
@@ -254,7 +262,7 @@ void MergeTreeBlockSizePredictor::update(const Block & sample_block, const Colum
 
 MergeTreeReadTaskColumns getReadTaskColumns(
     const MergeTreeData & storage,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     const MergeTreeData::DataPartPtr & data_part,
     const Names & required_columns,
     const PrewhereInfoPtr & prewhere_info)
@@ -263,7 +271,7 @@ MergeTreeReadTaskColumns getReadTaskColumns(
     Names pre_column_names;
 
     /// inject columns required for defaults evaluation
-    bool should_reorder = !injectRequiredColumns(storage, metadata_snapshot, data_part, column_names).empty();
+    bool should_reorder = !injectRequiredColumns(storage, storage_snapshot->getMetadataForQuery(), data_part, column_names).empty();
 
     if (prewhere_info)
     {
@@ -288,7 +296,7 @@ MergeTreeReadTaskColumns getReadTaskColumns(
         if (pre_column_names.empty())
             pre_column_names.push_back(column_names[0]);
 
-        const auto injected_pre_columns = injectRequiredColumns(storage, metadata_snapshot, data_part, pre_column_names);
+        const auto injected_pre_columns = injectRequiredColumns(storage, storage_snapshot->getMetadataForQuery(), data_part, pre_column_names);
         if (!injected_pre_columns.empty())
             should_reorder = true;
 
@@ -303,12 +311,12 @@ MergeTreeReadTaskColumns getReadTaskColumns(
     }
 
     MergeTreeReadTaskColumns result;
+    NamesAndTypesList all_columns;
 
-    auto columns = metadata_snapshot->getColumns();
-    result.pre_columns = columns.getByNames(ColumnsDescription::All, pre_column_names, true);
-    result.columns = columns.getByNames(ColumnsDescription::All, column_names, true);
+    auto options = GetColumnsOptions(GetColumnsOptions::All).withSubcolumns().withExtendedObjects();
+    result.pre_columns = storage_snapshot->getColumnsByNames(options, pre_column_names);
+    result.columns = storage_snapshot->getColumnsByNames(options, column_names);
     result.should_reorder = should_reorder;
-
     return result;
 }
 
diff --git a/src/Storages/MergeTree/MergeTreeBlockReadUtils.h b/src/Storages/MergeTree/MergeTreeBlockReadUtils.h
index 1f70ca72f399..2373881f9548 100644
--- a/src/Storages/MergeTree/MergeTreeBlockReadUtils.h
+++ b/src/Storages/MergeTree/MergeTreeBlockReadUtils.h
@@ -73,7 +73,7 @@ struct MergeTreeReadTaskColumns
 
 MergeTreeReadTaskColumns getReadTaskColumns(
     const MergeTreeData & storage,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     const MergeTreeData::DataPartPtr & data_part,
     const Names & required_columns,
     const PrewhereInfoPtr & prewhere_info);
diff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp
index 7f407199e819..b789cefc40ac 100644
--- a/src/Storages/MergeTree/MergeTreeData.cpp
+++ b/src/Storages/MergeTree/MergeTreeData.cpp
@@ -13,6 +13,9 @@
 #include <DataTypes/DataTypeUUID.h>
 #include <DataTypes/DataTypeTuple.h>
 #include <DataTypes/NestedUtils.h>
+#include <DataTypes/DataTypeObject.h>
+#include <DataTypes/ObjectUtils.h>
+#include <Columns/ColumnObject.h>
 #include <DataTypes/hasNullable.h>
 #include <Disks/TemporaryFileOnDisk.h>
 #include <Functions/FunctionFactory.h>
@@ -1292,6 +1295,7 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)
 
     if (num_parts == 0 && parts_from_wal.empty())
     {
+        resetObjectColumnsFromActiveParts(part_lock);
         LOG_DEBUG(log, "There are no data parts");
         return;
     }
@@ -1364,6 +1368,7 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)
         }
     }
 
+    resetObjectColumnsFromActiveParts(part_lock);
     calculateColumnAndSecondaryIndexSizesImpl();
 
     LOG_DEBUG(log, "Loaded data parts ({} items)", data_parts_indexes.size());
@@ -2624,6 +2629,11 @@ bool MergeTreeData::renameTempPartAndReplace(
         modifyPartState(part_it, DataPartState::Active);
         addPartContributionToColumnAndSecondaryIndexSizes(part);
 
+        if (covered_parts.empty())
+            updateObjectColumns(*part_it, lock);
+        else
+            resetObjectColumnsFromActiveParts(lock);
+
         ssize_t diff_bytes = part->getBytesOnDisk() - reduce_bytes;
         ssize_t diff_rows = part->rows_count - reduce_rows;
         ssize_t diff_parts = 1 - reduce_parts;
@@ -2664,9 +2674,10 @@ MergeTreeData::DataPartsVector MergeTreeData::renameTempPartAndReplace(
     return covered_parts;
 }
 
-void MergeTreeData::removePartsFromWorkingSet(const MergeTreeData::DataPartsVector & remove, bool clear_without_timeout, DataPartsLock & /*acquired_lock*/)
+void MergeTreeData::removePartsFromWorkingSet(const MergeTreeData::DataPartsVector & remove, bool clear_without_timeout, DataPartsLock & acquired_lock)
 {
     auto remove_time = clear_without_timeout ? 0 : time(nullptr);
+    bool removed_active_part = false;
 
     for (const DataPartPtr & part : remove)
     {
@@ -2674,6 +2685,7 @@ void MergeTreeData::removePartsFromWorkingSet(const MergeTreeData::DataPartsVect
         {
             removePartContributionToColumnAndSecondaryIndexSizes(part);
             removePartContributionToDataVolume(part);
+            removed_active_part = true;
         }
 
         if (part->getState() == IMergeTreeDataPart::State::Active || clear_without_timeout)
@@ -2685,11 +2697,15 @@ void MergeTreeData::removePartsFromWorkingSet(const MergeTreeData::DataPartsVect
         if (isInMemoryPart(part) && getSettings()->in_memory_parts_enable_wal)
             getWriteAheadLog()->dropPart(part->name);
     }
+
+    if (removed_active_part)
+        resetObjectColumnsFromActiveParts(acquired_lock);
 }
 
 void MergeTreeData::removePartsFromWorkingSetImmediatelyAndSetTemporaryState(const DataPartsVector & remove)
 {
     auto lock = lockParts();
+    bool removed_active_part = false;
 
     for (const auto & part : remove)
     {
@@ -2697,10 +2713,16 @@ void MergeTreeData::removePartsFromWorkingSetImmediatelyAndSetTemporaryState(con
         if (it_part == data_parts_by_info.end())
             throw Exception("Part " + part->getNameWithState() + " not found in data_parts", ErrorCodes::LOGICAL_ERROR);
 
+        if (part->getState() == IMergeTreeDataPart::State::Active)
+            removed_active_part = true;
+
         modifyPartState(part, IMergeTreeDataPart::State::Temporary);
         /// Erase immediately
         data_parts_indexes.erase(it_part);
     }
+
+    if (removed_active_part)
+        resetObjectColumnsFromActiveParts(lock);
 }
 
 void MergeTreeData::removePartsFromWorkingSet(const DataPartsVector & remove, bool clear_without_timeout, DataPartsLock * acquired_lock)
@@ -2793,8 +2815,7 @@ MergeTreeData::DataPartsVector MergeTreeData::removePartsInRangeFromWorkingSet(c
     return parts_to_remove;
 }
 
-void MergeTreeData::forgetPartAndMoveToDetached(const MergeTreeData::DataPartPtr & part_to_detach, const String & prefix, bool
-restore_covered)
+void MergeTreeData::forgetPartAndMoveToDetached(const MergeTreeData::DataPartPtr & part_to_detach, const String & prefix, bool restore_covered)
 {
     if (prefix.empty())
         LOG_INFO(log, "Renaming {} to {} and forgetting it.", part_to_detach->relative_path, part_to_detach->name);
@@ -2802,6 +2823,8 @@ restore_covered)
         LOG_INFO(log, "Renaming {} to {}_{} and forgetting it.", part_to_detach->relative_path, prefix, part_to_detach->name);
 
     auto lock = lockParts();
+    bool removed_active_part = false;
+    bool restored_active_part = false;
 
     auto it_part = data_parts_by_info.find(part_to_detach->info);
     if (it_part == data_parts_by_info.end())
@@ -2814,7 +2837,9 @@ restore_covered)
     {
         removePartContributionToDataVolume(part);
         removePartContributionToColumnAndSecondaryIndexSizes(part);
+        removed_active_part = true;
     }
+
     modifyPartState(it_part, DataPartState::Deleting);
 
     part->renameToDetached(prefix);
@@ -2864,6 +2889,7 @@ restore_covered)
                     addPartContributionToColumnAndSecondaryIndexSizes(*it);
                     addPartContributionToDataVolume(*it);
                     modifyPartState(it, DataPartState::Active); // iterator is not invalidated here
+                    restored_active_part = true;
                 }
 
                 pos = (*it)->info.max_block + 1;
@@ -2895,6 +2921,7 @@ restore_covered)
                 addPartContributionToColumnAndSecondaryIndexSizes(*it);
                 addPartContributionToDataVolume(*it);
                 modifyPartState(it, DataPartState::Active);
+                restored_active_part = true;
             }
 
             pos = (*it)->info.max_block + 1;
@@ -2914,6 +2941,9 @@ restore_covered)
             LOG_ERROR(log, "The set of parts restored in place of {} looks incomplete. There might or might not be a data loss.{}", part->name, (error_parts.empty() ? "" : " Suspicious parts: " + error_parts));
         }
     }
+
+    if (removed_active_part || restored_active_part)
+        resetObjectColumnsFromActiveParts(lock);
 }
 
 
@@ -3841,53 +3871,62 @@ std::set<String> MergeTreeData::getPartitionIdsAffectedByCommands(
 }
 
 
-MergeTreeData::DataPartsVector MergeTreeData::getDataPartsVector(
-    const DataPartStates & affordable_states, DataPartStateVector * out_states, bool require_projection_parts) const
+MergeTreeData::DataPartsVector MergeTreeData::getDataPartsVectorUnlocked(
+    const DataPartStates & affordable_states,
+    const DataPartsLock & /*lock*/,
+    DataPartStateVector * out_states,
+    bool require_projection_parts) const
 {
     DataPartsVector res;
     DataPartsVector buf;
+
+    for (auto state : affordable_states)
     {
-        auto lock = lockParts();
+        auto range = getDataPartsStateRange(state);
 
-        for (auto state : affordable_states)
+        if (require_projection_parts)
         {
-            auto range = getDataPartsStateRange(state);
-
-            if (require_projection_parts)
-            {
-                for (const auto & part : range)
-                {
-                    for (const auto & [_, projection_part] : part->getProjectionParts())
-                        res.push_back(projection_part);
-                }
-            }
-            else
+            for (const auto & part : range)
             {
-                std::swap(buf, res);
-                res.clear();
-                std::merge(range.begin(), range.end(), buf.begin(), buf.end(), std::back_inserter(res), LessDataPart()); //-V783
+                for (const auto & [_, projection_part] : part->getProjectionParts())
+                    res.push_back(projection_part);
             }
         }
+        else
+        {
+            std::swap(buf, res);
+            res.clear();
+            std::merge(range.begin(), range.end(), buf.begin(), buf.end(), std::back_inserter(res), LessDataPart()); //-V783
+        }
+    }
 
-        if (out_states != nullptr)
+    if (out_states != nullptr)
+    {
+        out_states->resize(res.size());
+        if (require_projection_parts)
         {
-            out_states->resize(res.size());
-            if (require_projection_parts)
-            {
-                for (size_t i = 0; i < res.size(); ++i)
-                    (*out_states)[i] = res[i]->getParentPart()->getState();
-            }
-            else
-            {
-                for (size_t i = 0; i < res.size(); ++i)
-                    (*out_states)[i] = res[i]->getState();
-            }
+            for (size_t i = 0; i < res.size(); ++i)
+                (*out_states)[i] = res[i]->getParentPart()->getState();
+        }
+        else
+        {
+            for (size_t i = 0; i < res.size(); ++i)
+                (*out_states)[i] = res[i]->getState();
         }
     }
 
     return res;
 }
 
+MergeTreeData::DataPartsVector MergeTreeData::getDataPartsVector(
+    const DataPartStates & affordable_states,
+    DataPartStateVector * out_states,
+    bool require_projection_parts) const
+{
+    auto lock = lockParts();
+    return getDataPartsVectorUnlocked(affordable_states, lock, out_states, require_projection_parts);
+}
+
 MergeTreeData::DataPartsVector
 MergeTreeData::getAllDataPartsVector(MergeTreeData::DataPartStateVector * out_states, bool require_projection_parts) const
 {
@@ -4337,7 +4376,7 @@ MergeTreeData::DataPartsVector MergeTreeData::Transaction::commit(MergeTreeData:
             else
             {
                 total_covered_parts.insert(total_covered_parts.end(), covered_parts.begin(), covered_parts.end());
-                for (const DataPartPtr & covered_part : covered_parts)
+                for (const auto & covered_part : covered_parts)
                 {
                     covered_part->remove_time.store(current_time, std::memory_order_relaxed);
 
@@ -4347,6 +4386,7 @@ MergeTreeData::DataPartsVector MergeTreeData::Transaction::commit(MergeTreeData:
                     data.modifyPartState(covered_part, DataPartState::Outdated);
                     data.removePartContributionToColumnAndSecondaryIndexSizes(covered_part);
                 }
+
                 reduce_parts += covered_parts.size();
 
                 add_bytes += part->getBytesOnDisk();
@@ -4358,6 +4398,14 @@ MergeTreeData::DataPartsVector MergeTreeData::Transaction::commit(MergeTreeData:
             }
         }
 
+        if (reduce_parts == 0)
+        {
+            for (const auto & part : precommitted_parts)
+                data.updateObjectColumns(part, parts_lock);
+        }
+        else
+            data.resetObjectColumnsFromActiveParts(parts_lock);
+
         ssize_t diff_bytes = add_bytes - reduce_bytes;
         ssize_t diff_rows = add_rows - reduce_rows;
         ssize_t diff_parts  = add_parts - reduce_parts;
@@ -4441,7 +4489,7 @@ using PartitionIdToMaxBlock = std::unordered_map<String, Int64>;
 
 static void selectBestProjection(
     const MergeTreeDataSelectExecutor & reader,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     const SelectQueryInfo & query_info,
     const Names & required_columns,
     ProjectionCandidate & candidate,
@@ -4470,7 +4518,7 @@ static void selectBestProjection(
     auto projection_result_ptr = reader.estimateNumMarksToRead(
         projection_parts,
         candidate.required_columns,
-        metadata_snapshot,
+        storage_snapshot->metadata,
         candidate.desc->metadata,
         query_info,
         query_context,
@@ -4492,9 +4540,9 @@ static void selectBestProjection(
         auto normal_result_ptr = reader.estimateNumMarksToRead(
             normal_parts,
             required_columns,
-            metadata_snapshot,
-            metadata_snapshot,
-            query_info,
+            storage_snapshot->metadata,
+            storage_snapshot->metadata,
+            query_info, // TODO syntax_analysis_result set in index
             query_context,
             settings.max_threads,
             max_added_blocks);
@@ -4697,8 +4745,9 @@ Block MergeTreeData::getMinMaxCountProjectionBlock(
 
 
 std::optional<ProjectionCandidate> MergeTreeData::getQueryProcessingStageWithAggregateProjection(
-    ContextPtr query_context, const StorageMetadataPtr & metadata_snapshot, SelectQueryInfo & query_info) const
+    ContextPtr query_context, const StorageSnapshotPtr & storage_snapshot, SelectQueryInfo & query_info) const
 {
+    const auto & metadata_snapshot = storage_snapshot->metadata;
     const auto & settings = query_context->getSettingsRef();
     if (!settings.allow_experimental_projection_optimization || query_info.ignore_projections || query_info.is_projection_query)
         return std::nullopt;
@@ -5074,7 +5123,7 @@ std::optional<ProjectionCandidate> MergeTreeData::getQueryProcessingStageWithAgg
             {
                 selectBestProjection(
                     reader,
-                    metadata_snapshot,
+                    storage_snapshot,
                     query_info,
                     analysis_result.required_columns,
                     candidate,
@@ -5094,7 +5143,7 @@ std::optional<ProjectionCandidate> MergeTreeData::getQueryProcessingStageWithAgg
             {
                 selectBestProjection(
                     reader,
-                    metadata_snapshot,
+                    storage_snapshot,
                     query_info,
                     analysis_result.required_columns,
                     candidate,
@@ -5134,12 +5183,12 @@ std::optional<ProjectionCandidate> MergeTreeData::getQueryProcessingStageWithAgg
 QueryProcessingStage::Enum MergeTreeData::getQueryProcessingStage(
     ContextPtr query_context,
     QueryProcessingStage::Enum to_stage,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info) const
 {
     if (to_stage >= QueryProcessingStage::Enum::WithMergeableState)
     {
-        if (auto projection = getQueryProcessingStageWithAggregateProjection(query_context, metadata_snapshot, query_info))
+        if (auto projection = getQueryProcessingStageWithAggregateProjection(query_context, storage_snapshot, query_info))
         {
             query_info.projection = std::move(projection);
             if (query_info.projection->desc->type == ProjectionDescription::Type::Aggregate)
@@ -6064,6 +6113,52 @@ ReservationPtr MergeTreeData::balancedReservation(
     return reserved_space;
 }
 
+ColumnsDescription MergeTreeData::getObjectColumns(
+    const DataPartsVector & parts, const ColumnsDescription & storage_columns)
+{
+    return DB::getObjectColumns(
+        parts.begin(), parts.end(),
+        storage_columns, [](const auto & part) -> const auto & { return part->getColumns(); });
+}
+
+ColumnsDescription MergeTreeData::getObjectColumns(
+    boost::iterator_range<DataPartIteratorByStateAndInfo> range, const ColumnsDescription & storage_columns)
+{
+    return DB::getObjectColumns(
+        range.begin(), range.end(),
+        storage_columns, [](const auto & part) -> const auto & { return part->getColumns(); });
+}
+
+void MergeTreeData::resetObjectColumnsFromActiveParts(const DataPartsLock & /*lock*/)
+{
+    auto metadata_snapshot = getInMemoryMetadataPtr();
+    const auto & columns = metadata_snapshot->getColumns();
+    if (!hasObjectColumns(columns))
+        return;
+
+    auto range = getDataPartsStateRange(DataPartState::Active);
+    object_columns = getObjectColumns(range, columns);
+}
+
+void MergeTreeData::updateObjectColumns(const DataPartPtr & part, const DataPartsLock & /*lock*/)
+{
+    auto metadata_snapshot = getInMemoryMetadataPtr();
+    const auto & columns = metadata_snapshot->getColumns();
+    if (!hasObjectColumns(columns))
+        return;
+
+    DB::updateObjectColumns(object_columns, part->getColumns());
+}
+
+StorageSnapshotPtr MergeTreeData::getStorageSnapshot(const StorageMetadataPtr & metadata_snapshot) const
+{
+    auto snapshot_data = std::make_unique<SnapshotData>();
+
+    auto lock = lockParts();
+    snapshot_data->parts = getDataPartsVectorUnlocked({DataPartState::Active}, lock);
+    return std::make_shared<StorageSnapshot>(*this, metadata_snapshot, object_columns, std::move(snapshot_data));
+}
+
 CurrentlySubmergingEmergingTagger::~CurrentlySubmergingEmergingTagger()
 {
     std::lock_guard lock(storage.currently_submerging_emerging_mutex);
diff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h
index d1c48b199851..86f60fd428d2 100644
--- a/src/Storages/MergeTree/MergeTreeData.h
+++ b/src/Storages/MergeTree/MergeTreeData.h
@@ -397,12 +397,12 @@ class MergeTreeData : public IStorage, public WithMutableContext
         ContextPtr query_context) const;
 
     std::optional<ProjectionCandidate> getQueryProcessingStageWithAggregateProjection(
-        ContextPtr query_context, const StorageMetadataPtr & metadata_snapshot, SelectQueryInfo & query_info) const;
+        ContextPtr query_context, const StorageSnapshotPtr & storage_snapshot, SelectQueryInfo & query_info) const;
 
     QueryProcessingStage::Enum getQueryProcessingStage(
         ContextPtr query_context,
         QueryProcessingStage::Enum to_stage,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & info) const override;
 
     ReservationPtr reserveSpace(UInt64 expected_size, VolumePtr & volume) const;
@@ -417,10 +417,21 @@ class MergeTreeData : public IStorage, public WithMutableContext
 
     bool supportsSubcolumns() const override { return true; }
 
+    bool supportsDynamicSubcolumns() const override { return true; }
+
     NamesAndTypesList getVirtuals() const override;
 
     bool mayBenefitFromIndexForIn(const ASTPtr & left_in_operand, ContextPtr, const StorageMetadataPtr & metadata_snapshot) const override;
 
+    /// Snapshot for MergeTree contains the current set of data parts
+    /// at the moment of the start of query.
+    struct SnapshotData : public StorageSnapshot::Data
+    {
+        DataPartsVector parts;
+    };
+
+    StorageSnapshotPtr getStorageSnapshot(const StorageMetadataPtr & metadata_snapshot) const override;
+
     /// Load the set of data parts from disk. Call once - immediately after the object is created.
     void loadDataParts(bool skip_sanity_checks);
 
@@ -431,13 +442,23 @@ class MergeTreeData : public IStorage, public WithMutableContext
     /// Returns a copy of the list so that the caller shouldn't worry about locks.
     DataParts getDataParts(const DataPartStates & affordable_states) const;
 
+    DataPartsVector getDataPartsVectorUnlocked(
+        const DataPartStates & affordable_states,
+        const DataPartsLock & lock,
+        DataPartStateVector * out_states = nullptr,
+        bool require_projection_parts = false) const;
+
     /// Returns sorted list of the parts with specified states
     ///  out_states will contain snapshot of each part state
     DataPartsVector getDataPartsVector(
-        const DataPartStates & affordable_states, DataPartStateVector * out_states = nullptr, bool require_projection_parts = false) const;
+        const DataPartStates & affordable_states,
+        DataPartStateVector * out_states = nullptr,
+        bool require_projection_parts = false) const;
 
     /// Returns absolutely all parts (and snapshot of their states)
-    DataPartsVector getAllDataPartsVector(DataPartStateVector * out_states = nullptr, bool require_projection_parts = false) const;
+    DataPartsVector getAllDataPartsVector(
+        DataPartStateVector * out_states = nullptr,
+        bool require_projection_parts = false) const;
 
     /// Returns all detached parts
     DetachedPartsInfo getDetachedParts() const;
@@ -689,6 +710,12 @@ class MergeTreeData : public IStorage, public WithMutableContext
         return column_sizes;
     }
 
+    const ColumnsDescription & getObjectColumns() const { return object_columns; }
+
+    /// Creates desciprion of columns of data type Object from the range of data parts.
+    static ColumnsDescription getObjectColumns(
+        const DataPartsVector & parts, const ColumnsDescription & storage_columns);
+
     IndexSizeByName getSecondaryIndexSizes() const override
     {
         auto lock = lockParts();
@@ -978,6 +1005,11 @@ class MergeTreeData : public IStorage, public WithMutableContext
     DataPartsIndexes::index<TagByInfo>::type & data_parts_by_info;
     DataPartsIndexes::index<TagByStateAndInfo>::type & data_parts_by_state_and_info;
 
+    /// Current descriprion of columns of data type Object.
+    /// It changes only when set of parts is changed and is
+    /// protected by @data_parts_mutex.
+    ColumnsDescription object_columns;
+
     MergeTreePartsMover parts_mover;
 
     /// Executors are common for both ReplicatedMergeTree and plain MergeTree
@@ -1014,6 +1046,10 @@ class MergeTreeData : public IStorage, public WithMutableContext
         return {begin, end};
     }
 
+    /// Creates desciprion of columns of data type Object from the range of data parts.
+    static ColumnsDescription getObjectColumns(
+        boost::iterator_range<DataPartIteratorByStateAndInfo> range, const ColumnsDescription & storage_columns);
+
     std::optional<UInt64> totalRowsByPartitionPredicateImpl(
         const SelectQueryInfo & query_info, ContextPtr context, const DataPartsVector & parts) const;
 
@@ -1197,6 +1233,9 @@ class MergeTreeData : public IStorage, public WithMutableContext
         MutableDataPartsVector & parts_from_wal,
         DataPartsLock & part_lock);
 
+    void resetObjectColumnsFromActiveParts(const DataPartsLock & lock);
+    void updateObjectColumns(const DataPartPtr & part, const DataPartsLock & lock);
+
     /// Create zero-copy exclusive lock for part and disk. Useful for coordination of
     /// distributed operations which can lead to data duplication. Implemented only in ReplicatedMergeTree.
     virtual std::optional<ZeroCopyLock> tryCreateZeroCopyExclusiveLock(const String &, const DiskPtr &) { return std::nullopt; }
diff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
index 092ca7179642..63d2fe41e484 100644
--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
@@ -117,7 +117,7 @@ static RelativeSize convertAbsoluteSampleSizeToRelative(const ASTPtr & node, siz
 
 QueryPlanPtr MergeTreeDataSelectExecutor::read(
     const Names & column_names_to_return,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     const SelectQueryInfo & query_info,
     ContextPtr context,
     const UInt64 max_block_size,
@@ -130,13 +130,17 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(
         return std::make_unique<QueryPlan>();
 
     const auto & settings = context->getSettingsRef();
+    const auto & metadata_for_reading = storage_snapshot->getMetadataForQuery();
+
+    const auto & snapshot_data = assert_cast<const MergeTreeData::SnapshotData &>(*storage_snapshot->data);
+    const auto & parts = snapshot_data.parts;
+
     if (!query_info.projection)
     {
         auto plan = readFromParts(
-            query_info.merge_tree_select_result_ptr ? MergeTreeData::DataPartsVector{} : data.getDataPartsVector(),
+            query_info.merge_tree_select_result_ptr ? MergeTreeData::DataPartsVector{} : parts,
             column_names_to_return,
-            metadata_snapshot,
-            metadata_snapshot,
+            storage_snapshot,
             query_info,
             context,
             max_block_size,
@@ -146,7 +150,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(
             enable_parallel_reading);
 
         if (plan->isInitialized() && settings.allow_experimental_projection_optimization && settings.force_optimize_projection
-            && !metadata_snapshot->projections.empty())
+            && !metadata_for_reading->projections.empty())
             throw Exception(
                 "No projection is used when allow_experimental_projection_optimization = 1 and force_optimize_projection = 1",
                 ErrorCodes::PROJECTION_NOT_USED);
@@ -178,8 +182,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(
         projection_plan = readFromParts(
             {},
             query_info.projection->required_columns,
-            metadata_snapshot,
-            query_info.projection->desc->metadata,
+            storage_snapshot,
             query_info,
             context,
             max_block_size,
@@ -1204,8 +1207,7 @@ MergeTreeDataSelectAnalysisResultPtr MergeTreeDataSelectExecutor::estimateNumMar
 QueryPlanPtr MergeTreeDataSelectExecutor::readFromParts(
     MergeTreeData::DataPartsVector parts,
     const Names & column_names_to_return,
-    const StorageMetadataPtr & metadata_snapshot_base,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     const SelectQueryInfo & query_info,
     ContextPtr context,
     const UInt64 max_block_size,
@@ -1237,8 +1239,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::readFromParts(
         virt_column_names,
         data,
         query_info,
-        metadata_snapshot,
-        metadata_snapshot_base,
+        storage_snapshot,
         context,
         max_block_size,
         num_streams,
diff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
index 3dde324ce22d..e0647aa1ed20 100644
--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
@@ -28,7 +28,7 @@ class MergeTreeDataSelectExecutor
 
     QueryPlanPtr read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const SelectQueryInfo & query_info,
         ContextPtr context,
         UInt64 max_block_size,
@@ -41,8 +41,7 @@ class MergeTreeDataSelectExecutor
     QueryPlanPtr readFromParts(
         MergeTreeData::DataPartsVector parts,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot_base,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const SelectQueryInfo & query_info,
         ContextPtr context,
         UInt64 max_block_size,
diff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp
index d16b5274a455..4805a273c70f 100644
--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp
@@ -10,6 +10,7 @@
 #include <IO/HashingWriteBuffer.h>
 #include <DataTypes/DataTypeDateTime.h>
 #include <DataTypes/DataTypeDate.h>
+#include <DataTypes/ObjectUtils.h>
 #include <IO/WriteHelpers.h>
 #include <Common/typeid_cast.h>
 #include <Processors/TTL/ITTLAlgorithm.h>
@@ -281,6 +282,16 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPart(
 {
     TemporaryPart temp_part;
     Block & block = block_with_partition.block;
+    auto columns = metadata_snapshot->getColumns().getAllPhysical().filter(block.getNames());
+    auto storage_snapshot = data.getStorageSnapshot(metadata_snapshot);
+
+    if (!storage_snapshot->object_columns.empty())
+    {
+        auto extended_storage_columns = storage_snapshot->getColumns(
+            GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects());
+
+        convertObjectsToTuples(columns, block, extended_storage_columns);
+    }
 
     static const String TMP_PREFIX = "tmp_insert_";
 
@@ -357,7 +368,6 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPart(
     for (const auto & ttl_entry : move_ttl_entries)
         updateTTL(ttl_entry, move_ttl_infos, move_ttl_infos.moves_ttl[ttl_entry.result_column], block, false);
 
-    NamesAndTypesList columns = metadata_snapshot->getColumns().getAllPhysical().filter(block.getNames());
     ReservationPtr reservation = data.reserveSpacePreferringTTLRules(metadata_snapshot, expected_size, move_ttl_infos, time(nullptr), 0, true);
     VolumePtr volume = data.getStoragePolicy()->getVolume(0);
 
@@ -426,7 +436,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPart(
     auto compression_codec = data.getContext()->chooseCompressionCodec(0, 0);
 
     const auto & index_factory = MergeTreeIndexFactory::instance();
-    auto out = std::make_unique<MergedBlockOutputStream>(new_data_part, metadata_snapshot,columns,
+    auto out = std::make_unique<MergedBlockOutputStream>(new_data_part, metadata_snapshot, columns,
         index_factory.getMany(metadata_snapshot->getSecondaryIndices()), compression_codec);
 
     out->writeWithPermutation(block, perm_ptr);
diff --git a/src/Storages/MergeTree/MergeTreePartition.cpp b/src/Storages/MergeTree/MergeTreePartition.cpp
index 706d72771e9b..4edf23bc0fba 100644
--- a/src/Storages/MergeTree/MergeTreePartition.cpp
+++ b/src/Storages/MergeTree/MergeTreePartition.cpp
@@ -124,6 +124,18 @@ namespace
             for (const auto & elem : x)
                 applyVisitor(*this, elem);
         }
+        void operator() (const Object & x) const
+        {
+            UInt8 type = Field::Types::Object;
+            hash.update(type);
+            hash.update(x.size());
+
+            for (const auto & [key, value]: x)
+            {
+                hash.update(key);
+                applyVisitor(*this, value);
+            }
+        }
         void operator() (const DecimalField<Decimal32> & x) const
         {
             UInt8 type = Field::Types::Decimal32;
diff --git a/src/Storages/MergeTree/MergeTreeReadPool.cpp b/src/Storages/MergeTree/MergeTreeReadPool.cpp
index 3c31ffa7c971..87839edc46ff 100644
--- a/src/Storages/MergeTree/MergeTreeReadPool.cpp
+++ b/src/Storages/MergeTree/MergeTreeReadPool.cpp
@@ -23,7 +23,7 @@ MergeTreeReadPool::MergeTreeReadPool(
     size_t min_marks_for_concurrent_read_,
     RangesInDataParts && parts_,
     const MergeTreeData & data_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     const PrewhereInfoPtr & prewhere_info_,
     const Names & column_names_,
     const BackoffSettings & backoff_settings_,
@@ -32,7 +32,7 @@ MergeTreeReadPool::MergeTreeReadPool(
     : backoff_settings{backoff_settings_}
     , backoff_state{threads_}
     , data{data_}
-    , metadata_snapshot{metadata_snapshot_}
+    , storage_snapshot{storage_snapshot_}
     , column_names{column_names_}
     , do_not_steal_tasks{do_not_steal_tasks_}
     , predict_block_size_bytes{preferred_block_size_bytes_ > 0}
@@ -146,7 +146,7 @@ MergeTreeReadTaskPtr MergeTreeReadPool::getTask(size_t min_marks_to_read, size_t
 
 Block MergeTreeReadPool::getHeader() const
 {
-    return metadata_snapshot->getSampleBlockForColumns(column_names, data.getVirtuals(), data.getStorageID());
+    return storage_snapshot->getSampleBlockForColumns(column_names);
 }
 
 void MergeTreeReadPool::profileFeedback(ReadBufferFromFileBase::ProfileInfo info)
@@ -192,7 +192,7 @@ void MergeTreeReadPool::profileFeedback(ReadBufferFromFileBase::ProfileInfo info
 std::vector<size_t> MergeTreeReadPool::fillPerPartInfo(const RangesInDataParts & parts)
 {
     std::vector<size_t> per_part_sum_marks;
-    Block sample_block = metadata_snapshot->getSampleBlock();
+    Block sample_block = storage_snapshot->metadata->getSampleBlock();
     is_part_on_remote_disk.resize(parts.size());
 
     for (const auto i : collections::range(0, parts.size()))
@@ -209,7 +209,7 @@ std::vector<size_t> MergeTreeReadPool::fillPerPartInfo(const RangesInDataParts &
 
         per_part_sum_marks.push_back(sum_marks);
 
-        auto task_columns = getReadTaskColumns(data, metadata_snapshot, part.data_part, column_names, prewhere_info);
+        auto task_columns = getReadTaskColumns(data, storage_snapshot, part.data_part, column_names, prewhere_info);
 
         auto size_predictor = !predict_block_size_bytes ? nullptr
             : MergeTreeBaseSelectProcessor::getSizePredictor(part.data_part, task_columns, sample_block);
diff --git a/src/Storages/MergeTree/MergeTreeReadPool.h b/src/Storages/MergeTree/MergeTreeReadPool.h
index 4ab4393ef5a9..0c93c7014481 100644
--- a/src/Storages/MergeTree/MergeTreeReadPool.h
+++ b/src/Storages/MergeTree/MergeTreeReadPool.h
@@ -71,7 +71,7 @@ class MergeTreeReadPool : private boost::noncopyable
 public:
     MergeTreeReadPool(
         size_t threads_, size_t sum_marks_, size_t min_marks_for_concurrent_read_,
-        RangesInDataParts && parts_, const MergeTreeData & data_, const StorageMetadataPtr & metadata_snapshot_,
+        RangesInDataParts && parts_, const MergeTreeData & data_, const StorageSnapshotPtr & storage_snapshot_,
         const PrewhereInfoPtr & prewhere_info_,
         const Names & column_names_,
         const BackoffSettings & backoff_settings_, size_t preferred_block_size_bytes_,
@@ -95,7 +95,7 @@ class MergeTreeReadPool : private boost::noncopyable
         const RangesInDataParts & parts, size_t min_marks_for_concurrent_read);
 
     const MergeTreeData & data;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     const Names column_names;
     bool do_not_steal_tasks;
     bool predict_block_size_bytes;
diff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp
index 2e17611cd931..b943c3c8718f 100644
--- a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp
+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp
@@ -52,6 +52,15 @@ MergeTreeReaderCompact::MergeTreeReaderCompact(
         auto name_and_type = columns.begin();
         for (size_t i = 0; i < columns_num; ++i, ++name_and_type)
         {
+            if (name_and_type->isSubcolumn())
+            {
+                auto storage_column_from_part = getColumnFromPart(
+                    {name_and_type->getNameInStorage(), name_and_type->getTypeInStorage()});
+
+                if (!storage_column_from_part.type->tryGetSubcolumnType(name_and_type->getSubcolumnName()))
+                    continue;
+            }
+
             auto column_from_part = getColumnFromPart(*name_and_type);
 
             auto position = data_part->getColumnPosition(column_from_part.getNameInStorage());
diff --git a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp
index 9599e3ee82c9..c1b0067dbb0f 100644
--- a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp
+++ b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp
@@ -1,5 +1,6 @@
 #include <Storages/MergeTree/MergeTreeReaderInMemory.h>
 #include <Storages/MergeTree/MergeTreeDataPartInMemory.h>
+#include <Interpreters/getColumnFromBlock.h>
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/NestedUtils.h>
 #include <Columns/ColumnArray.h>
diff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp
index 2d4d3617cee9..3245134c4706 100644
--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp
+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp
@@ -9,7 +9,7 @@ namespace DB
 
 MergeTreeSelectProcessor::MergeTreeSelectProcessor(
     const MergeTreeData & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     const MergeTreeData::DataPartPtr & owned_data_part_,
     UInt64 max_block_size_rows_,
     size_t preferred_block_size_bytes_,
@@ -25,13 +25,13 @@ MergeTreeSelectProcessor::MergeTreeSelectProcessor(
     bool has_limit_below_one_block_,
     std::optional<ParallelReadingExtension> extension_)
     : MergeTreeBaseSelectProcessor{
-        metadata_snapshot_->getSampleBlockForColumns(required_columns_, storage_.getVirtuals(), storage_.getStorageID()),
-        storage_, metadata_snapshot_, prewhere_info_, std::move(actions_settings), max_block_size_rows_,
+        storage_snapshot_->getSampleBlockForColumns(required_columns_),
+        storage_, storage_snapshot_, prewhere_info_, std::move(actions_settings), max_block_size_rows_,
         preferred_block_size_bytes_, preferred_max_column_in_block_size_bytes_,
         reader_settings_, use_uncompressed_cache_, virt_column_names_, extension_},
     required_columns{std::move(required_columns_)},
     data_part{owned_data_part_},
-    sample_block(metadata_snapshot_->getSampleBlock()),
+    sample_block(storage_snapshot_->metadata->getSampleBlock()),
     all_mark_ranges(std::move(mark_ranges_)),
     part_index_in_query(part_index_in_query_),
     has_limit_below_one_block(has_limit_below_one_block_),
@@ -48,7 +48,7 @@ MergeTreeSelectProcessor::MergeTreeSelectProcessor(
 void MergeTreeSelectProcessor::initializeReaders()
 {
     task_columns = getReadTaskColumns(
-        storage, metadata_snapshot, data_part,
+        storage, storage_snapshot, data_part,
         required_columns, prewhere_info);
 
     /// Will be used to distinguish between PREWHERE and WHERE columns when applying filter
@@ -60,12 +60,12 @@ void MergeTreeSelectProcessor::initializeReaders()
 
     owned_mark_cache = storage.getContext()->getMarkCache();
 
-    reader = data_part->getReader(task_columns.columns, metadata_snapshot, all_mark_ranges,
-        owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);
+    reader = data_part->getReader(task_columns.columns, storage_snapshot->getMetadataForQuery(),
+        all_mark_ranges, owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);
 
     if (prewhere_info)
-        pre_reader = data_part->getReader(task_columns.pre_columns, metadata_snapshot, all_mark_ranges,
-            owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);
+        pre_reader = data_part->getReader(task_columns.pre_columns, storage_snapshot->getMetadataForQuery(),
+            all_mark_ranges, owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);
 
 }
 
diff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.h b/src/Storages/MergeTree/MergeTreeSelectProcessor.h
index 2ecdc3b59a8f..4b3a46fc53cb 100644
--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.h
+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.h
@@ -18,7 +18,7 @@ class MergeTreeSelectProcessor : public MergeTreeBaseSelectProcessor
 public:
     MergeTreeSelectProcessor(
         const MergeTreeData & storage,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot_,
         const MergeTreeData::DataPartPtr & owned_data_part,
         UInt64 max_block_size_rows,
         size_t preferred_block_size_bytes,
diff --git a/src/Storages/MergeTree/MergeTreeSequentialSource.cpp b/src/Storages/MergeTree/MergeTreeSequentialSource.cpp
index 37012aa65708..5dbc59ba2d5d 100644
--- a/src/Storages/MergeTree/MergeTreeSequentialSource.cpp
+++ b/src/Storages/MergeTree/MergeTreeSequentialSource.cpp
@@ -11,15 +11,15 @@ namespace ErrorCodes
 
 MergeTreeSequentialSource::MergeTreeSequentialSource(
     const MergeTreeData & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     MergeTreeData::DataPartPtr data_part_,
     Names columns_to_read_,
     bool read_with_direct_io_,
     bool take_column_types_from_storage,
     bool quiet)
-    : SourceWithProgress(metadata_snapshot_->getSampleBlockForColumns(columns_to_read_, storage_.getVirtuals(), storage_.getStorageID()))
+    : SourceWithProgress(storage_snapshot_->getSampleBlockForColumns(columns_to_read_))
     , storage(storage_)
-    , metadata_snapshot(metadata_snapshot_)
+    , storage_snapshot(storage_snapshot_)
     , data_part(std::move(data_part_))
     , columns_to_read(std::move(columns_to_read_))
     , read_with_direct_io(read_with_direct_io_)
@@ -41,11 +41,12 @@ MergeTreeSequentialSource::MergeTreeSequentialSource(
     addTotalRowsApprox(data_part->rows_count);
 
     /// Add columns because we don't want to read empty blocks
-    injectRequiredColumns(storage, metadata_snapshot, data_part, columns_to_read);
+    injectRequiredColumns(storage, storage_snapshot->metadata, data_part, columns_to_read);
     NamesAndTypesList columns_for_reader;
     if (take_column_types_from_storage)
     {
-        columns_for_reader = metadata_snapshot->getColumns().getByNames(ColumnsDescription::AllPhysical, columns_to_read, false);
+        auto options = GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects();
+        columns_for_reader = storage_snapshot->getColumnsByNames(options, columns_to_read);
     }
     else
     {
@@ -63,7 +64,7 @@ MergeTreeSequentialSource::MergeTreeSequentialSource(
         .save_marks_in_cache = false
     };
 
-    reader = data_part->getReader(columns_for_reader, metadata_snapshot,
+    reader = data_part->getReader(columns_for_reader, storage_snapshot->metadata,
         MarkRanges{MarkRange(0, data_part->getMarksCount())},
         /* uncompressed_cache = */ nullptr, mark_cache.get(), reader_settings);
 }
diff --git a/src/Storages/MergeTree/MergeTreeSequentialSource.h b/src/Storages/MergeTree/MergeTreeSequentialSource.h
index a7405140c6d2..962b2035b160 100644
--- a/src/Storages/MergeTree/MergeTreeSequentialSource.h
+++ b/src/Storages/MergeTree/MergeTreeSequentialSource.h
@@ -14,7 +14,7 @@ class MergeTreeSequentialSource : public SourceWithProgress
 public:
     MergeTreeSequentialSource(
         const MergeTreeData & storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         MergeTreeData::DataPartPtr data_part_,
         Names columns_to_read_,
         bool read_with_direct_io_,
@@ -35,7 +35,7 @@ class MergeTreeSequentialSource : public SourceWithProgress
 private:
 
     const MergeTreeData & storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
 
     /// Data part will not be removed if the pointer owns it
     MergeTreeData::DataPartPtr data_part;
diff --git a/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp
index 145d292138aa..063f018b1a40 100644
--- a/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp
+++ b/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp
@@ -20,7 +20,7 @@ MergeTreeThreadSelectProcessor::MergeTreeThreadSelectProcessor(
     size_t preferred_block_size_bytes_,
     size_t preferred_max_column_in_block_size_bytes_,
     const MergeTreeData & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     bool use_uncompressed_cache_,
     const PrewhereInfoPtr & prewhere_info_,
     ExpressionActionsSettings actions_settings,
@@ -29,7 +29,7 @@ MergeTreeThreadSelectProcessor::MergeTreeThreadSelectProcessor(
     std::optional<ParallelReadingExtension> extension_)
     :
     MergeTreeBaseSelectProcessor{
-        pool_->getHeader(), storage_, metadata_snapshot_, prewhere_info_, std::move(actions_settings), max_block_size_rows_,
+        pool_->getHeader(), storage_, storage_snapshot_, prewhere_info_, std::move(actions_settings), max_block_size_rows_,
         preferred_block_size_bytes_, preferred_max_column_in_block_size_bytes_,
         reader_settings_, use_uncompressed_cache_, virt_column_names_, extension_},
     thread{thread_},
@@ -103,6 +103,7 @@ void MergeTreeThreadSelectProcessor::finalizeNewTask()
 
     /// Allows pool to reduce number of threads in case of too slow reads.
     auto profile_callback = [this](ReadBufferFromFileBase::ProfileInfo info_) { pool->profileFeedback(info_); };
+    const auto & metadata_snapshot = storage_snapshot->metadata;
 
     if (!reader)
     {
diff --git a/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.h b/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.h
index ae25ca2a88ae..3bba42bed28d 100644
--- a/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.h
+++ b/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.h
@@ -22,7 +22,7 @@ class MergeTreeThreadSelectProcessor final : public MergeTreeBaseSelectProcessor
         size_t preferred_block_size_bytes_,
         size_t preferred_max_column_in_block_size_bytes_,
         const MergeTreeData & storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         bool use_uncompressed_cache_,
         const PrewhereInfoPtr & prewhere_info_,
         ExpressionActionsSettings actions_settings,
diff --git a/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h b/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h
index 1dc1bd1eca4a..854f070d0e0a 100644
--- a/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h
+++ b/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h
@@ -24,7 +24,7 @@ class StorageFromMergeTreeDataPart final : public shared_ptr_helper<StorageFromM
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum /*processed_stage*/,
@@ -35,8 +35,7 @@ class StorageFromMergeTreeDataPart final : public shared_ptr_helper<StorageFromM
                                               .readFromParts(
                                                   parts,
                                                   column_names,
-                                                  metadata_snapshot,
-                                                  metadata_snapshot,
+                                                  storage_snapshot,
                                                   query_info,
                                                   context,
                                                   max_block_size,
diff --git a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp
index 582a568cb484..d12e91f62e4d 100644
--- a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp
+++ b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp
@@ -270,7 +270,7 @@ bool StorageMaterializedPostgreSQL::needRewriteQueryWithFinal(const Names & colu
 
 Pipe StorageMaterializedPostgreSQL::read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & /*storage_snapshot*/,
         SelectQueryInfo & query_info,
         ContextPtr context_,
         QueryProcessingStage::Enum processed_stage,
@@ -279,7 +279,7 @@ Pipe StorageMaterializedPostgreSQL::read(
 {
     auto nested_table = getNested();
 
-    auto pipe = readFinalFromNestedStorage(nested_table, column_names, metadata_snapshot,
+    auto pipe = readFinalFromNestedStorage(nested_table, column_names,
             query_info, context_, processed_stage, max_block_size, num_streams);
 
     auto lock = lockForShare(context_->getCurrentQueryId(), context_->getSettingsRef().lock_acquire_timeout);
diff --git a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h
index e6ce3bbdf658..e41eb8ee98f2 100644
--- a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h
+++ b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h
@@ -85,7 +85,7 @@ class StorageMaterializedPostgreSQL final : public shared_ptr_helper<StorageMate
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context_,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/RabbitMQ/RabbitMQSource.cpp b/src/Storages/RabbitMQ/RabbitMQSource.cpp
index be2806eb42a8..4946a3537f9c 100644
--- a/src/Storages/RabbitMQ/RabbitMQSource.cpp
+++ b/src/Storages/RabbitMQ/RabbitMQSource.cpp
@@ -8,12 +8,11 @@
 namespace DB
 {
 
-static std::pair<Block, Block> getHeaders(StorageRabbitMQ & storage, const StorageMetadataPtr & metadata_snapshot)
+static std::pair<Block, Block> getHeaders(const StorageSnapshotPtr & storage_snapshot)
 {
-    auto non_virtual_header = metadata_snapshot->getSampleBlockNonMaterialized();
-    auto virtual_header = metadata_snapshot->getSampleBlockForColumns(
-                {"_exchange_name", "_channel_id", "_delivery_tag", "_redelivered", "_message_id", "_timestamp"},
-                storage.getVirtuals(), storage.getStorageID());
+    auto non_virtual_header = storage_snapshot->metadata->getSampleBlockNonMaterialized();
+    auto virtual_header = storage_snapshot->getSampleBlockForColumns(
+                {"_exchange_name", "_channel_id", "_delivery_tag", "_redelivered", "_message_id", "_timestamp"});
 
     return {non_virtual_header, virtual_header};
 }
@@ -29,15 +28,15 @@ static Block getSampleBlock(const Block & non_virtual_header, const Block & virt
 
 RabbitMQSource::RabbitMQSource(
     StorageRabbitMQ & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     ContextPtr context_,
     const Names & columns,
     size_t max_block_size_,
     bool ack_in_suffix_)
     : RabbitMQSource(
         storage_,
-        metadata_snapshot_,
-        getHeaders(storage_, metadata_snapshot_),
+        storage_snapshot_,
+        getHeaders(storage_snapshot_),
         context_,
         columns,
         max_block_size_,
@@ -47,7 +46,7 @@ RabbitMQSource::RabbitMQSource(
 
 RabbitMQSource::RabbitMQSource(
     StorageRabbitMQ & storage_,
-    const StorageMetadataPtr & metadata_snapshot_,
+    const StorageSnapshotPtr & storage_snapshot_,
     std::pair<Block, Block> headers,
     ContextPtr context_,
     const Names & columns,
@@ -55,7 +54,7 @@ RabbitMQSource::RabbitMQSource(
     bool ack_in_suffix_)
     : SourceWithProgress(getSampleBlock(headers.first, headers.second))
     , storage(storage_)
-    , metadata_snapshot(metadata_snapshot_)
+    , storage_snapshot(storage_snapshot_)
     , context(context_)
     , column_names(columns)
     , max_block_size(max_block_size_)
diff --git a/src/Storages/RabbitMQ/RabbitMQSource.h b/src/Storages/RabbitMQ/RabbitMQSource.h
index f3ceac8e1e5f..ff46408db42c 100644
--- a/src/Storages/RabbitMQ/RabbitMQSource.h
+++ b/src/Storages/RabbitMQ/RabbitMQSource.h
@@ -14,7 +14,7 @@ class RabbitMQSource : public SourceWithProgress
 public:
     RabbitMQSource(
             StorageRabbitMQ & storage_,
-            const StorageMetadataPtr & metadata_snapshot_,
+            const StorageSnapshotPtr & storage_snapshot_,
             ContextPtr context_,
             const Names & columns,
             size_t max_block_size_,
@@ -34,7 +34,7 @@ class RabbitMQSource : public SourceWithProgress
 
 private:
     StorageRabbitMQ & storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     ContextPtr context;
     Names column_names;
     const size_t max_block_size;
@@ -48,7 +48,7 @@ class RabbitMQSource : public SourceWithProgress
 
     RabbitMQSource(
         StorageRabbitMQ & storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         std::pair<Block, Block> headers,
         ContextPtr context_,
         const Names & columns,
diff --git a/src/Storages/RabbitMQ/StorageRabbitMQ.cpp b/src/Storages/RabbitMQ/StorageRabbitMQ.cpp
index ac299657ae6a..cadfa85299c0 100644
--- a/src/Storages/RabbitMQ/StorageRabbitMQ.cpp
+++ b/src/Storages/RabbitMQ/StorageRabbitMQ.cpp
@@ -648,7 +648,7 @@ void StorageRabbitMQ::unbindExchange()
 
 Pipe StorageRabbitMQ::read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & /* query_info */,
         ContextPtr local_context,
         QueryProcessingStage::Enum /* processed_stage */,
@@ -669,7 +669,7 @@ Pipe StorageRabbitMQ::read(
 
     std::lock_guard lock(loop_mutex);
 
-    auto sample_block = metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID());
+    auto sample_block = storage_snapshot->getSampleBlockForColumns(column_names);
     auto modified_context = addSettings(local_context);
 
     if (!connection->isConnected())
@@ -688,7 +688,7 @@ Pipe StorageRabbitMQ::read(
     for (size_t i = 0; i < num_created_consumers; ++i)
     {
         auto rabbit_source = std::make_shared<RabbitMQSource>(
-                *this, metadata_snapshot, modified_context, column_names, 1, rabbitmq_settings->rabbitmq_commit_on_select);
+            *this, storage_snapshot, modified_context, column_names, 1, rabbitmq_settings->rabbitmq_commit_on_select);
 
         auto converting_dag = ActionsDAG::makeConvertingActions(
             rabbit_source->getPort().getHeader().getColumnsWithTypeAndName(),
@@ -1024,9 +1024,9 @@ bool StorageRabbitMQ::streamToViews()
     InterpreterInsertQuery interpreter(insert, rabbitmq_context, false, true, true);
     auto block_io = interpreter.execute();
 
-    auto metadata_snapshot = getInMemoryMetadataPtr();
+    auto storage_snapshot = getStorageSnapshot(getInMemoryMetadataPtr());
     auto column_names = block_io.pipeline.getHeader().getNames();
-    auto sample_block = metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID());
+    auto sample_block = storage_snapshot->getSampleBlockForColumns(column_names);
 
     auto block_size = getMaxBlockSize();
 
@@ -1039,7 +1039,7 @@ bool StorageRabbitMQ::streamToViews()
     for (size_t i = 0; i < num_created_consumers; ++i)
     {
         auto source = std::make_shared<RabbitMQSource>(
-                *this, metadata_snapshot, rabbitmq_context, column_names, block_size, false);
+            *this, storage_snapshot, rabbitmq_context, column_names, block_size, false);
         sources.emplace_back(source);
         pipes.emplace_back(source);
 
diff --git a/src/Storages/RabbitMQ/StorageRabbitMQ.h b/src/Storages/RabbitMQ/StorageRabbitMQ.h
index 9633326366d5..394845bbc2f8 100644
--- a/src/Storages/RabbitMQ/StorageRabbitMQ.h
+++ b/src/Storages/RabbitMQ/StorageRabbitMQ.h
@@ -40,7 +40,7 @@ class StorageRabbitMQ final: public shared_ptr_helper<StorageRabbitMQ>, public I
     /// Always return virtual columns in addition to required columns
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/ReadFinalForExternalReplicaStorage.cpp b/src/Storages/ReadFinalForExternalReplicaStorage.cpp
index 58b98aaa4c6c..cf1c5c356296 100644
--- a/src/Storages/ReadFinalForExternalReplicaStorage.cpp
+++ b/src/Storages/ReadFinalForExternalReplicaStorage.cpp
@@ -27,7 +27,6 @@ bool needRewriteQueryWithFinalForStorage(const Names & column_names, const Stora
 Pipe readFinalFromNestedStorage(
     StoragePtr nested_storage,
     const Names & column_names,
-    const StorageMetadataPtr & /*metadata_snapshot*/,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
@@ -36,7 +35,7 @@ Pipe readFinalFromNestedStorage(
 {
     NameSet column_names_set = NameSet(column_names.begin(), column_names.end());
     auto lock = nested_storage->lockForShare(context->getCurrentQueryId(), context->getSettingsRef().lock_acquire_timeout);
-    const StorageMetadataPtr & nested_metadata = nested_storage->getInMemoryMetadataPtr();
+    const auto & nested_metadata = nested_storage->getInMemoryMetadataPtr();
 
     Block nested_header = nested_metadata->getSampleBlock();
     ColumnWithTypeAndName & sign_column = nested_header.getByPosition(nested_header.columns() - 2);
@@ -55,7 +54,8 @@ Pipe readFinalFromNestedStorage(
         filter_column_name = expressions->children.back()->getColumnName();
     }
 
-    Pipe pipe = nested_storage->read(require_columns_name, nested_metadata, query_info, context, processed_stage, max_block_size, num_streams);
+    auto nested_snapshot = nested_storage->getStorageSnapshot(nested_metadata);
+    Pipe pipe = nested_storage->read(require_columns_name, nested_snapshot, query_info, context, processed_stage, max_block_size, num_streams);
     pipe.addTableLock(lock);
     pipe.addStorageHolder(nested_storage);
 
diff --git a/src/Storages/ReadFinalForExternalReplicaStorage.h b/src/Storages/ReadFinalForExternalReplicaStorage.h
index 979945a38c71..f21b396513f9 100644
--- a/src/Storages/ReadFinalForExternalReplicaStorage.h
+++ b/src/Storages/ReadFinalForExternalReplicaStorage.h
@@ -16,7 +16,6 @@ bool needRewriteQueryWithFinalForStorage(const Names & column_names, const Stora
 Pipe readFinalFromNestedStorage(
     StoragePtr nested_storage,
     const Names & column_names,
-    const StorageMetadataPtr & /*metadata_snapshot*/,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp b/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp
index bd525ca9e5a5..05b30bb014e2 100644
--- a/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp
+++ b/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp
@@ -432,19 +432,19 @@ void StorageEmbeddedRocksDB::initDB()
 
 Pipe StorageEmbeddedRocksDB::read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr /*context*/,
         QueryProcessingStage::Enum /*processed_stage*/,
         size_t max_block_size,
         unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     FieldVectorPtr keys;
     bool all_scan = false;
 
-    Block sample_block = metadata_snapshot->getSampleBlock();
+    Block sample_block = storage_snapshot->metadata->getSampleBlock();
     auto primary_key_data_type = sample_block.getByName(primary_key).type;
     std::tie(keys, all_scan) = getFilterKeys(primary_key, primary_key_data_type, query_info);
     if (all_scan)
diff --git a/src/Storages/RocksDB/StorageEmbeddedRocksDB.h b/src/Storages/RocksDB/StorageEmbeddedRocksDB.h
index d45d3dd320c2..52a08cbefd46 100644
--- a/src/Storages/RocksDB/StorageEmbeddedRocksDB.h
+++ b/src/Storages/RocksDB/StorageEmbeddedRocksDB.h
@@ -28,7 +28,7 @@ class StorageEmbeddedRocksDB final : public shared_ptr_helper<StorageEmbeddedRoc
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageBuffer.cpp b/src/Storages/StorageBuffer.cpp
index ead0d6b1260f..801e1b80a20b 100644
--- a/src/Storages/StorageBuffer.cpp
+++ b/src/Storages/StorageBuffer.cpp
@@ -5,6 +5,7 @@
 #include <Interpreters/castColumn.h>
 #include <Interpreters/evaluateConstantExpression.h>
 #include <Interpreters/addMissingDefaults.h>
+#include <Interpreters/getColumnFromBlock.h>
 #include <Storages/StorageBuffer.h>
 #include <Storages/StorageFactory.h>
 #include <Storages/AlterCommands.h>
@@ -145,10 +146,10 @@ StorageBuffer::StorageBuffer(
 class BufferSource : public SourceWithProgress
 {
 public:
-    BufferSource(const Names & column_names_, StorageBuffer::Buffer & buffer_, const StorageBuffer & storage, const StorageMetadataPtr & metadata_snapshot)
-        : SourceWithProgress(
-            metadata_snapshot->getSampleBlockForColumns(column_names_, storage.getVirtuals(), storage.getStorageID()))
-        , column_names_and_types(metadata_snapshot->getColumns().getByNames(ColumnsDescription::All, column_names_, true))
+    BufferSource(const Names & column_names_, StorageBuffer::Buffer & buffer_, const StorageSnapshotPtr & storage_snapshot)
+        : SourceWithProgress(storage_snapshot->getSampleBlockForColumns(column_names_))
+        , column_names_and_types(storage_snapshot->getColumnsByNames(
+            GetColumnsOptions(GetColumnsOptions::All).withSubcolumns(), column_names_))
         , buffer(buffer_) {}
 
     String getName() const override { return "Buffer"; }
@@ -189,7 +190,7 @@ class BufferSource : public SourceWithProgress
 QueryProcessingStage::Enum StorageBuffer::getQueryProcessingStage(
     ContextPtr local_context,
     QueryProcessingStage::Enum to_stage,
-    const StorageMetadataPtr &,
+    const StorageSnapshotPtr &,
     SelectQueryInfo & query_info) const
 {
     if (destination_id)
@@ -201,7 +202,8 @@ QueryProcessingStage::Enum StorageBuffer::getQueryProcessingStage(
 
         /// TODO: Find a way to support projections for StorageBuffer
         query_info.ignore_projections = true;
-        return destination->getQueryProcessingStage(local_context, to_stage, destination->getInMemoryMetadataPtr(), query_info);
+        const auto & destination_metadata = destination->getInMemoryMetadataPtr();
+        return destination->getQueryProcessingStage(local_context, to_stage, destination->getStorageSnapshot(destination_metadata), query_info);
     }
 
     return QueryProcessingStage::FetchColumns;
@@ -210,7 +212,7 @@ QueryProcessingStage::Enum StorageBuffer::getQueryProcessingStage(
 
 Pipe StorageBuffer::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -218,7 +220,7 @@ Pipe StorageBuffer::read(
     const unsigned num_streams)
 {
     QueryPlan plan;
-    read(plan, column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
+    read(plan, column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
     return plan.convertToPipe(
         QueryPlanOptimizationSettings::fromContext(local_context),
         BuildQueryPipelineSettings::fromContext(local_context));
@@ -227,13 +229,15 @@ Pipe StorageBuffer::read(
 void StorageBuffer::read(
     QueryPlan & query_plan,
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
     size_t max_block_size,
     unsigned num_streams)
 {
+    const auto & metadata_snapshot = storage_snapshot->metadata;
+
     if (destination_id)
     {
         auto destination = DatabaseCatalog::instance().getTable(destination_id, local_context);
@@ -244,13 +248,14 @@ void StorageBuffer::read(
         auto destination_lock = destination->lockForShare(local_context->getCurrentQueryId(), local_context->getSettingsRef().lock_acquire_timeout);
 
         auto destination_metadata_snapshot = destination->getInMemoryMetadataPtr();
+        auto destination_snapshot = destination->getStorageSnapshot(destination_metadata_snapshot);
 
         const bool dst_has_same_structure = std::all_of(column_names.begin(), column_names.end(), [metadata_snapshot, destination_metadata_snapshot](const String& column_name)
         {
             const auto & dest_columns = destination_metadata_snapshot->getColumns();
             const auto & our_columns = metadata_snapshot->getColumns();
-            auto dest_columm = dest_columns.tryGetColumnOrSubcolumn(ColumnsDescription::AllPhysical, column_name);
-            return dest_columm && dest_columm->type->equals(*our_columns.getColumnOrSubcolumn(ColumnsDescription::AllPhysical, column_name).type);
+            auto dest_columm = dest_columns.tryGetColumnOrSubcolumn(GetColumnsOptions::AllPhysical, column_name);
+            return dest_columm && dest_columm->type->equals(*our_columns.getColumnOrSubcolumn(GetColumnsOptions::AllPhysical, column_name).type);
         });
 
         if (dst_has_same_structure)
@@ -260,7 +265,7 @@ void StorageBuffer::read(
 
             /// The destination table has the same structure of the requested columns and we can simply read blocks from there.
             destination->read(
-                query_plan, column_names, destination_metadata_snapshot, query_info,
+                query_plan, column_names, destination_snapshot, query_info,
                 local_context, processed_stage, max_block_size, num_streams);
         }
         else
@@ -295,7 +300,7 @@ void StorageBuffer::read(
             else
             {
                 destination->read(
-                        query_plan, columns_intersection, destination_metadata_snapshot, query_info,
+                        query_plan, columns_intersection, destination_snapshot, query_info,
                         local_context, processed_stage, max_block_size, num_streams);
 
                 if (query_plan.isInitialized())
@@ -352,7 +357,7 @@ void StorageBuffer::read(
         Pipes pipes_from_buffers;
         pipes_from_buffers.reserve(num_shards);
         for (auto & buf : buffers)
-            pipes_from_buffers.emplace_back(std::make_shared<BufferSource>(column_names, buf, *this, metadata_snapshot));
+            pipes_from_buffers.emplace_back(std::make_shared<BufferSource>(column_names, buf, storage_snapshot));
 
         pipe_from_buffers = Pipe::unitePipes(std::move(pipes_from_buffers));
     }
diff --git a/src/Storages/StorageBuffer.h b/src/Storages/StorageBuffer.h
index f04619a1d21b..f589560008a2 100644
--- a/src/Storages/StorageBuffer.h
+++ b/src/Storages/StorageBuffer.h
@@ -58,11 +58,11 @@ friend class BufferSink;
     std::string getName() const override { return "Buffer"; }
 
     QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const override;
+    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const override;
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -72,7 +72,7 @@ friend class BufferSink;
     void read(
         QueryPlan & query_plan,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageDictionary.cpp b/src/Storages/StorageDictionary.cpp
index e6d856b80fcd..b3d0c1f17de5 100644
--- a/src/Storages/StorageDictionary.cpp
+++ b/src/Storages/StorageDictionary.cpp
@@ -163,7 +163,7 @@ void StorageDictionary::checkTableCanBeDetached() const
 
 Pipe StorageDictionary::read(
     const Names & column_names,
-    const StorageMetadataPtr & /*metadata_snapshot*/,
+    const StorageSnapshotPtr & /*storage_snapshot*/,
     SelectQueryInfo & /*query_info*/,
     ContextPtr local_context,
     QueryProcessingStage::Enum /*processed_stage*/,
diff --git a/src/Storages/StorageDictionary.h b/src/Storages/StorageDictionary.h
index 855d02b0947f..bf9e68532333 100644
--- a/src/Storages/StorageDictionary.h
+++ b/src/Storages/StorageDictionary.h
@@ -26,7 +26,7 @@ class StorageDictionary final : public shared_ptr_helper<StorageDictionary>, pub
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageDistributed.cpp b/src/Storages/StorageDistributed.cpp
index 5bfb3b4ce458..1a390f784a20 100644
--- a/src/Storages/StorageDistributed.cpp
+++ b/src/Storages/StorageDistributed.cpp
@@ -9,6 +9,8 @@
 #include <DataTypes/DataTypeFactory.h>
 #include <DataTypes/DataTypeUUID.h>
 #include <DataTypes/DataTypesNumber.h>
+#include <DataTypes/ObjectUtils.h>
+#include <DataTypes/NestedUtils.h>
 
 #include <Storages/Distributed/DistributedSink.h>
 #include <Storages/StorageFactory.h>
@@ -55,6 +57,7 @@
 #include <Interpreters/evaluateConstantExpression.h>
 #include <Interpreters/getClusterName.h>
 #include <Interpreters/getTableExpressions.h>
+#include <Interpreters/RequiredSourceColumnsVisitor.h>
 #include <Functions/IFunction.h>
 #include <TableFunctions/TableFunctionView.h>
 #include <TableFunctions/TableFunctionFactory.h>
@@ -277,9 +280,9 @@ void replaceConstantExpressions(
     ContextPtr context,
     const NamesAndTypesList & columns,
     ConstStoragePtr storage,
-    const StorageMetadataPtr & metadata_snapshot)
+    const StorageSnapshotPtr & storage_snapshot)
 {
-    auto syntax_result = TreeRewriter(context).analyze(node, columns, storage, metadata_snapshot);
+    auto syntax_result = TreeRewriter(context).analyze(node, columns, storage, storage_snapshot);
     Block block_with_constants = KeyCondition::getBlockWithConstants(node, syntax_result, context);
 
     InDepthNodeVisitor<ReplacingConstantExpressionsMatcher, true> visitor(block_with_constants);
@@ -423,7 +426,7 @@ StorageDistributed::StorageDistributed(
 QueryProcessingStage::Enum StorageDistributed::getQueryProcessingStage(
     ContextPtr local_context,
     QueryProcessingStage::Enum to_stage,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info) const
 {
     const auto & settings = local_context->getSettingsRef();
@@ -437,7 +440,7 @@ QueryProcessingStage::Enum StorageDistributed::getQueryProcessingStage(
     /// (Anyway it will be calculated in the read())
     if (nodes > 1 && settings.optimize_skip_unused_shards)
     {
-        ClusterPtr optimized_cluster = getOptimizedCluster(local_context, metadata_snapshot, query_info.query);
+        ClusterPtr optimized_cluster = getOptimizedCluster(local_context, storage_snapshot, query_info.query);
         if (optimized_cluster)
         {
             LOG_DEBUG(log, "Skipping irrelevant shards - the query will be sent to the following shards of the cluster (shard numbers): {}",
@@ -590,9 +593,62 @@ std::optional<QueryProcessingStage::Enum> StorageDistributed::getOptimizedQueryP
     return QueryProcessingStage::Complete;
 }
 
+static bool requiresObjectColumns(const ColumnsDescription & all_columns, ASTPtr query)
+{
+    if (!hasObjectColumns(all_columns))
+        return false;
+
+    if (!query)
+        return true;
+
+    RequiredSourceColumnsVisitor::Data columns_context;
+    RequiredSourceColumnsVisitor(columns_context).visit(query);
+
+    auto required_columns = columns_context.requiredColumns();
+    for (const auto & required_column : required_columns)
+    {
+        auto name_in_storage = Nested::splitName(required_column).first;
+        auto column_in_storage = all_columns.tryGetPhysical(name_in_storage);
+
+        if (column_in_storage && isObject(column_in_storage->type))
+            return true;
+    }
+
+    return false;
+}
+
+StorageSnapshotPtr StorageDistributed::getStorageSnapshot(const StorageMetadataPtr & metadata_snapshot) const
+{
+    return getStorageSnapshotForQuery(metadata_snapshot, nullptr);
+}
+
+StorageSnapshotPtr StorageDistributed::getStorageSnapshotForQuery(
+    const StorageMetadataPtr & metadata_snapshot, const ASTPtr & query) const
+{
+    /// If query doesn't use columns of type Object, don't deduce
+    /// concrete types for them, because it required extra round trip.
+    auto snapshot_data = std::make_unique<SnapshotData>();
+    if (!requiresObjectColumns(metadata_snapshot->getColumns(), query))
+        return std::make_shared<StorageSnapshot>(*this, metadata_snapshot, ColumnsDescription{}, std::move(snapshot_data));
+
+    snapshot_data->objects_by_shard = getExtendedObjectsOfRemoteTables(
+        *getCluster(),
+        StorageID{remote_database, remote_table},
+        metadata_snapshot->getColumns(),
+        getContext());
+
+    auto object_columns = DB::getObjectColumns(
+        snapshot_data->objects_by_shard.begin(),
+        snapshot_data->objects_by_shard.end(),
+        metadata_snapshot->getColumns(),
+        [](const auto & shard_num_and_columns) -> const auto & { return shard_num_and_columns.second; });
+
+    return std::make_shared<StorageSnapshot>(*this, metadata_snapshot, object_columns, std::move(snapshot_data));
+}
+
 Pipe StorageDistributed::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -600,7 +656,7 @@ Pipe StorageDistributed::read(
     const unsigned num_streams)
 {
     QueryPlan plan;
-    read(plan, column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
+    read(plan, column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
     return plan.convertToPipe(
         QueryPlanOptimizationSettings::fromContext(local_context),
         BuildQueryPipelineSettings::fromContext(local_context));
@@ -609,7 +665,7 @@ Pipe StorageDistributed::read(
 void StorageDistributed::read(
     QueryPlan & query_plan,
     const Names &,
-    const StorageMetadataPtr &,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -642,9 +698,12 @@ void StorageDistributed::read(
     if (!remote_table_function_ptr)
         main_table = StorageID{remote_database, remote_table};
 
+    const auto & snapshot_data = assert_cast<const SnapshotData &>(*storage_snapshot->data);
     ClusterProxy::SelectStreamFactory select_stream_factory =
         ClusterProxy::SelectStreamFactory(
             header,
+            snapshot_data.objects_by_shard,
+            storage_snapshot,
             processed_stage);
 
     ClusterProxy::executeQuery(
@@ -1077,7 +1136,7 @@ ClusterPtr StorageDistributed::getCluster() const
 }
 
 ClusterPtr StorageDistributed::getOptimizedCluster(
-    ContextPtr local_context, const StorageMetadataPtr & metadata_snapshot, const ASTPtr & query_ptr) const
+    ContextPtr local_context, const StorageSnapshotPtr & storage_snapshot, const ASTPtr & query_ptr) const
 {
     ClusterPtr cluster = getCluster();
     const Settings & settings = local_context->getSettingsRef();
@@ -1086,7 +1145,7 @@ ClusterPtr StorageDistributed::getOptimizedCluster(
 
     if (has_sharding_key && sharding_key_is_usable)
     {
-        ClusterPtr optimized = skipUnusedShards(cluster, query_ptr, metadata_snapshot, local_context);
+        ClusterPtr optimized = skipUnusedShards(cluster, query_ptr, storage_snapshot, local_context);
         if (optimized)
             return optimized;
     }
@@ -1142,7 +1201,7 @@ IColumn::Selector StorageDistributed::createSelector(const ClusterPtr cluster, c
 ClusterPtr StorageDistributed::skipUnusedShards(
     ClusterPtr cluster,
     const ASTPtr & query_ptr,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     ContextPtr local_context) const
 {
     const auto & select = query_ptr->as<ASTSelectQuery &>();
@@ -1162,7 +1221,7 @@ ClusterPtr StorageDistributed::skipUnusedShards(
         condition_ast = select.prewhere() ? select.prewhere()->clone() : select.where()->clone();
     }
 
-    replaceConstantExpressions(condition_ast, local_context, metadata_snapshot->getColumns().getAll(), shared_from_this(), metadata_snapshot);
+    replaceConstantExpressions(condition_ast, local_context, storage_snapshot->metadata->getColumns().getAll(), shared_from_this(), storage_snapshot);
 
     size_t limit = local_context->getSettingsRef().optimize_skip_unused_shards_limit;
     if (!limit || limit > SSIZE_MAX)
@@ -1463,3 +1522,4 @@ void registerStorageDistributed(StorageFactory & factory)
 }
 
 }
+
diff --git a/src/Storages/StorageDistributed.h b/src/Storages/StorageDistributed.h
index 45b1cd640eeb..317463783ee3 100644
--- a/src/Storages/StorageDistributed.h
+++ b/src/Storages/StorageDistributed.h
@@ -5,6 +5,7 @@
 #include <Storages/IStorage.h>
 #include <Storages/Distributed/DirectoryMonitor.h>
 #include <Storages/Distributed/DistributedSettings.h>
+#include <Storages/getStructureOfRemoteTable.h>
 #include <Common/SimpleIncrement.h>
 #include <Client/ConnectionPool.h>
 #include <Client/ConnectionPoolWithFailover.h>
@@ -51,6 +52,7 @@ class StorageDistributed final : public shared_ptr_helper<StorageDistributed>, p
     bool supportsFinal() const override { return true; }
     bool supportsPrewhere() const override { return true; }
     bool supportsSubcolumns() const override { return true; }
+    bool supportsDynamicSubcolumns() const override { return true; }
     StoragePolicyPtr getStoragePolicy() const override;
 
     /// Do not apply moving to PREWHERE optimization for distributed tables,
@@ -59,12 +61,24 @@ class StorageDistributed final : public shared_ptr_helper<StorageDistributed>, p
 
     bool isRemote() const override { return true; }
 
+    /// Snapshot for StorageDistributed contains descriptions
+    /// of columns of type Object for each shard at the moment
+    /// of the start of query.
+    struct SnapshotData : public StorageSnapshot::Data
+    {
+        ColumnsDescriptionByShardNum objects_by_shard;
+    };
+
+    StorageSnapshotPtr getStorageSnapshot(const StorageMetadataPtr & metadata_snapshot) const override;
+    StorageSnapshotPtr getStorageSnapshotForQuery(
+        const StorageMetadataPtr & metadata_snapshot, const ASTPtr & query) const override;
+
     QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const override;
+    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const override;
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -74,7 +88,7 @@ class StorageDistributed final : public shared_ptr_helper<StorageDistributed>, p
     void read(
         QueryPlan & query_plan,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -175,10 +189,10 @@ class StorageDistributed final : public shared_ptr_helper<StorageDistributed>, p
     /// Apply the following settings:
     /// - optimize_skip_unused_shards
     /// - force_optimize_skip_unused_shards
-    ClusterPtr getOptimizedCluster(ContextPtr, const StorageMetadataPtr & metadata_snapshot, const ASTPtr & query_ptr) const;
+    ClusterPtr getOptimizedCluster(ContextPtr, const StorageSnapshotPtr & storage_snapshot, const ASTPtr & query_ptr) const;
 
     ClusterPtr skipUnusedShards(
-        ClusterPtr cluster, const ASTPtr & query_ptr, const StorageMetadataPtr & metadata_snapshot, ContextPtr context) const;
+        ClusterPtr cluster, const ASTPtr & query_ptr, const StorageSnapshotPtr & storage_snapshot, ContextPtr context) const;
 
     /// This method returns optimal query processing stage.
     ///
diff --git a/src/Storages/StorageExecutable.cpp b/src/Storages/StorageExecutable.cpp
index 211434387256..d9e97f98d56a 100644
--- a/src/Storages/StorageExecutable.cpp
+++ b/src/Storages/StorageExecutable.cpp
@@ -105,7 +105,7 @@ StorageExecutable::StorageExecutable(
 
 Pipe StorageExecutable::read(
     const Names & /*column_names*/,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -142,7 +142,7 @@ Pipe StorageExecutable::read(
     if (settings.is_executable_pool)
         transformToSingleBlockSources(inputs);
 
-    auto sample_block = metadata_snapshot->getSampleBlock();
+    auto sample_block = storage_snapshot->metadata->getSampleBlock();
 
     ShellCommandSourceConfiguration configuration;
     configuration.max_block_size = max_block_size;
diff --git a/src/Storages/StorageExecutable.h b/src/Storages/StorageExecutable.h
index b6248abae97e..ede98ea5b476 100644
--- a/src/Storages/StorageExecutable.h
+++ b/src/Storages/StorageExecutable.h
@@ -31,7 +31,7 @@ class StorageExecutable final : public shared_ptr_helper<StorageExecutable>, pub
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & /*storage_snapshot*/,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageExternalDistributed.cpp b/src/Storages/StorageExternalDistributed.cpp
index 40a2ad0b85ed..18b8d4c037a0 100644
--- a/src/Storages/StorageExternalDistributed.cpp
+++ b/src/Storages/StorageExternalDistributed.cpp
@@ -172,7 +172,7 @@ StorageExternalDistributed::StorageExternalDistributed(
 
 Pipe StorageExternalDistributed::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
@@ -184,7 +184,7 @@ Pipe StorageExternalDistributed::read(
     {
         pipes.emplace_back(shard->read(
             column_names,
-            metadata_snapshot,
+            storage_snapshot,
             query_info,
             context,
             processed_stage,
diff --git a/src/Storages/StorageExternalDistributed.h b/src/Storages/StorageExternalDistributed.h
index 33a58d324dad..57767db10b01 100644
--- a/src/Storages/StorageExternalDistributed.h
+++ b/src/Storages/StorageExternalDistributed.h
@@ -32,7 +32,7 @@ class StorageExternalDistributed final : public shared_ptr_helper<StorageExterna
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageFile.cpp b/src/Storages/StorageFile.cpp
index 452eb16ebc51..93cceadaf933 100644
--- a/src/Storages/StorageFile.cpp
+++ b/src/Storages/StorageFile.cpp
@@ -447,28 +447,27 @@ class StorageFileSource : public SourceWithProgress
 
     static Block getBlockForSource(
         const StorageFilePtr & storage,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const ColumnsDescription & columns_description,
         const FilesInfoPtr & files_info)
     {
         if (storage->isColumnOriented())
-            return metadata_snapshot->getSampleBlockForColumns(
-                columns_description.getNamesOfPhysical(), storage->getVirtuals(), storage->getStorageID());
+            return storage_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
         else
-            return getHeader(metadata_snapshot, files_info->need_path_column, files_info->need_file_column);
+            return getHeader(storage_snapshot->metadata, files_info->need_path_column, files_info->need_file_column);
     }
 
     StorageFileSource(
         std::shared_ptr<StorageFile> storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         ContextPtr context_,
         UInt64 max_block_size_,
         FilesInfoPtr files_info_,
         ColumnsDescription columns_description_,
         std::unique_ptr<ReadBuffer> read_buf_)
-        : SourceWithProgress(getBlockForSource(storage_, metadata_snapshot_, columns_description_, files_info_))
+        : SourceWithProgress(getBlockForSource(storage_, storage_snapshot_, columns_description_, files_info_))
         , storage(std::move(storage_))
-        , metadata_snapshot(metadata_snapshot_)
+        , storage_snapshot(storage_snapshot_)
         , files_info(std::move(files_info_))
         , read_buf(std::move(read_buf_))
         , columns_description(std::move(columns_description_))
@@ -518,8 +517,8 @@ class StorageFileSource : public SourceWithProgress
                 auto get_block_for_format = [&]() -> Block
                 {
                     if (storage->isColumnOriented())
-                        return metadata_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
-                    return metadata_snapshot->getSampleBlock();
+                        return storage_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
+                    return storage_snapshot->metadata->getSampleBlock();
                 };
 
                 auto format = context->getInputFormat(
@@ -582,7 +581,7 @@ class StorageFileSource : public SourceWithProgress
 
 private:
     std::shared_ptr<StorageFile> storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     FilesInfoPtr files_info;
     String current_path;
     Block sample_block;
@@ -603,7 +602,7 @@ class StorageFileSource : public SourceWithProgress
 
 Pipe StorageFile::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -617,7 +616,7 @@ Pipe StorageFile::read(
         if (paths.size() == 1 && !fs::exists(paths[0]))
         {
             if (context->getSettingsRef().engine_file_empty_if_not_exists)
-                return Pipe(std::make_shared<NullSource>(metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID())));
+                return Pipe(std::make_shared<NullSource>(storage_snapshot->getSampleBlockForColumns(column_names)));
             else
                 throw Exception("File " + paths[0] + " doesn't exist", ErrorCodes::FILE_DOESNT_EXIST);
         }
@@ -653,9 +652,9 @@ Pipe StorageFile::read(
         {
             if (isColumnOriented())
                 return ColumnsDescription{
-                    metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID()).getNamesAndTypesList()};
+                    storage_snapshot->getSampleBlockForColumns(column_names).getNamesAndTypesList()};
             else
-                return metadata_snapshot->getColumns();
+                return storage_snapshot->metadata->getColumns();
         };
 
         /// In case of reading from fd we have to check whether we have already created
@@ -667,7 +666,7 @@ Pipe StorageFile::read(
             read_buffer = std::move(peekable_read_buffer_from_fd);
 
         pipes.emplace_back(std::make_shared<StorageFileSource>(
-            this_ptr, metadata_snapshot, context, max_block_size, files_info, get_columns_for_format(), std::move(read_buffer)));
+            this_ptr, storage_snapshot, context, max_block_size, files_info, get_columns_for_format(), std::move(read_buffer)));
     }
 
     return Pipe::unitePipes(std::move(pipes));
diff --git a/src/Storages/StorageFile.h b/src/Storages/StorageFile.h
index bc2bd3bc933f..86e75588e149 100644
--- a/src/Storages/StorageFile.h
+++ b/src/Storages/StorageFile.h
@@ -22,7 +22,7 @@ friend class PartitionedStorageFileSink;
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageGenerateRandom.cpp b/src/Storages/StorageGenerateRandom.cpp
index 8934fd0ccbf8..aa7b17191b6b 100644
--- a/src/Storages/StorageGenerateRandom.cpp
+++ b/src/Storages/StorageGenerateRandom.cpp
@@ -486,19 +486,19 @@ void registerStorageGenerateRandom(StorageFactory & factory)
 
 Pipe StorageGenerateRandom::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t max_block_size,
     unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     Pipes pipes;
     pipes.reserve(num_streams);
 
-    const ColumnsDescription & our_columns = metadata_snapshot->getColumns();
+    const ColumnsDescription & our_columns = storage_snapshot->metadata->getColumns();
     Block block_header;
     for (const auto & name : column_names)
     {
diff --git a/src/Storages/StorageGenerateRandom.h b/src/Storages/StorageGenerateRandom.h
index d11a43b1dd6d..2894b17d4097 100644
--- a/src/Storages/StorageGenerateRandom.h
+++ b/src/Storages/StorageGenerateRandom.h
@@ -17,7 +17,7 @@ class StorageGenerateRandom final : public shared_ptr_helper<StorageGenerateRand
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageInMemoryMetadata.cpp b/src/Storages/StorageInMemoryMetadata.cpp
index 8439c721c1fd..15a761a5b84d 100644
--- a/src/Storages/StorageInMemoryMetadata.cpp
+++ b/src/Storages/StorageInMemoryMetadata.cpp
@@ -5,6 +5,7 @@
 #include <Common/quoteString.h>
 #include <Common/StringUtils/StringUtils.h>
 #include <Core/ColumnWithTypeAndName.h>
+#include <DataTypes/NestedUtils.h>
 #include <DataTypes/DataTypeEnum.h>
 #include <IO/ReadBufferFromString.h>
 #include <IO/ReadHelpers.h>
@@ -342,39 +343,6 @@ Block StorageInMemoryMetadata::getSampleBlock() const
     return res;
 }
 
-Block StorageInMemoryMetadata::getSampleBlockForColumns(
-    const Names & column_names, const NamesAndTypesList & virtuals, const StorageID & storage_id) const
-{
-    Block res;
-
-    HashMapWithSavedHash<StringRef, const DataTypePtr *, StringRefHash> virtuals_map;
-
-    /// Virtual columns must be appended after ordinary, because user can
-    /// override them.
-    for (const auto & column : virtuals)
-        virtuals_map[column.name] = &column.type;
-
-    for (const auto & name : column_names)
-    {
-        auto column = getColumns().tryGetColumnOrSubcolumn(ColumnsDescription::All, name);
-        if (column)
-        {
-            res.insert({column->type->createColumn(), column->type, column->name});
-        }
-        else if (auto * it = virtuals_map.find(name); it != virtuals_map.end())
-        {
-            const auto & type = *it->getMapped();
-            res.insert({type->createColumn(), type, name});
-        }
-        else
-            throw Exception(
-                "Column " + backQuote(name) + " not found in table " + (storage_id.empty() ? "" : storage_id.getNameForLogs()),
-                ErrorCodes::NOT_FOUND_COLUMN_IN_BLOCK);
-    }
-
-    return res;
-}
-
 const KeyDescription & StorageInMemoryMetadata::getPartitionKey() const
 {
     return partition_key;
@@ -499,18 +467,6 @@ namespace
     using NamesAndTypesMap = HashMapWithSavedHash<StringRef, const IDataType *, StringRefHash>;
     using UniqueStrings = HashSetWithSavedHash<StringRef, StringRefHash>;
 
-    String listOfColumns(const NamesAndTypesList & available_columns)
-    {
-        WriteBufferFromOwnString ss;
-        for (auto it = available_columns.begin(); it != available_columns.end(); ++it)
-        {
-            if (it != available_columns.begin())
-                ss << ", ";
-            ss << it->name;
-        }
-        return ss.str();
-    }
-
     NamesAndTypesMap getColumnsMap(const NamesAndTypesList & columns)
     {
         NamesAndTypesMap res;
@@ -539,36 +495,16 @@ namespace
     }
 }
 
-void StorageInMemoryMetadata::check(const Names & column_names, const NamesAndTypesList & virtuals, const StorageID & storage_id) const
+String listOfColumns(const NamesAndTypesList & available_columns)
 {
-    if (column_names.empty())
+    WriteBufferFromOwnString ss;
+    for (auto it = available_columns.begin(); it != available_columns.end(); ++it)
     {
-        auto list_of_columns = listOfColumns(getColumns().getAllPhysicalWithSubcolumns());
-        throw Exception(ErrorCodes::EMPTY_LIST_OF_COLUMNS_QUERIED,
-            "Empty list of columns queried. There are columns: {}", list_of_columns);
-    }
-
-    const auto virtuals_map = getColumnsMap(virtuals);
-    UniqueStrings unique_names;
-
-    for (const auto & name : column_names)
-    {
-        bool has_column = getColumns().hasColumnOrSubcolumn(ColumnsDescription::AllPhysical, name)
-            || virtuals_map.find(name) != nullptr;
-
-        if (!has_column)
-        {
-            auto list_of_columns = listOfColumns(getColumns().getAllPhysicalWithSubcolumns());
-            throw Exception(ErrorCodes::NO_SUCH_COLUMN_IN_TABLE,
-                "There is no column with name {} in table {}. There are columns: {}",
-                backQuote(name), storage_id.getNameForLogs(), list_of_columns);
-        }
-
-        if (unique_names.end() != unique_names.find(name))
-            throw Exception(ErrorCodes::COLUMN_QUERIED_MORE_THAN_ONCE, "Column {} queried more than once", name);
-
-        unique_names.insert(name);
+        if (it != available_columns.begin())
+            ss << ", ";
+        ss << it->name;
     }
+    return ss.str();
 }
 
 void StorageInMemoryMetadata::check(const NamesAndTypesList & provided_columns) const
@@ -589,7 +525,10 @@ void StorageInMemoryMetadata::check(const NamesAndTypesList & provided_columns)
                 listOfColumns(available_columns));
 
         const auto * available_type = it->getMapped();
-        if (!column.type->equals(*available_type) && !isCompatibleEnumTypes(available_type, column.type.get()))
+
+        if (!isObject(*available_type)
+            && !column.type->equals(*available_type)
+            && !isCompatibleEnumTypes(available_type, column.type.get()))
             throw Exception(
                 ErrorCodes::TYPE_MISMATCH,
                 "Type mismatch for column {}. Column has type {}, got type {}",
@@ -636,7 +575,9 @@ void StorageInMemoryMetadata::check(const NamesAndTypesList & provided_columns,
         const auto * provided_column_type = it->getMapped();
         const auto * available_column_type = jt->getMapped();
 
-        if (!provided_column_type->equals(*available_column_type) && !isCompatibleEnumTypes(available_column_type, provided_column_type))
+        if (!isObject(*provided_column_type)
+            && !provided_column_type->equals(*available_column_type)
+            && !isCompatibleEnumTypes(available_column_type, provided_column_type))
             throw Exception(
                 ErrorCodes::TYPE_MISMATCH,
                 "Type mismatch for column {}. Column has type {}, got type {}",
@@ -678,7 +619,9 @@ void StorageInMemoryMetadata::check(const Block & block, bool need_all) const
                 listOfColumns(available_columns));
 
         const auto * available_type = it->getMapped();
-        if (!column.type->equals(*available_type) && !isCompatibleEnumTypes(available_type, column.type.get()))
+        if (!isObject(*available_type)
+            && !column.type->equals(*available_type)
+            && !isCompatibleEnumTypes(available_type, column.type.get()))
             throw Exception(
                 ErrorCodes::TYPE_MISMATCH,
                 "Type mismatch for column {}. Column has type {}, got type {}",
diff --git a/src/Storages/StorageInMemoryMetadata.h b/src/Storages/StorageInMemoryMetadata.h
index bdaed8b26246..a9ab96909f49 100644
--- a/src/Storages/StorageInMemoryMetadata.h
+++ b/src/Storages/StorageInMemoryMetadata.h
@@ -165,13 +165,6 @@ struct StorageInMemoryMetadata
     /// Storage metadata.
     Block getSampleBlockWithVirtuals(const NamesAndTypesList & virtuals) const;
 
-
-    /// Block with ordinary + materialized + aliases + virtuals. Virtuals have
-    /// to be explicitly specified, because they are part of Storage type, not
-    /// Storage metadata. StorageID required only for more clear exception
-    /// message.
-    Block getSampleBlockForColumns(
-        const Names & column_names, const NamesAndTypesList & virtuals = {}, const StorageID & storage_id = StorageID::createEmpty()) const;
     /// Returns structure with partition key.
     const KeyDescription & getPartitionKey() const;
     /// Returns ASTExpressionList of partition key expression for storage or nullptr if there is none.
@@ -234,10 +227,6 @@ struct StorageInMemoryMetadata
     const SelectQueryDescription & getSelectQuery() const;
     bool hasSelectQuery() const;
 
-    /// Verify that all the requested names are in the table and are set correctly:
-    /// list of names is not empty and the names do not repeat.
-    void check(const Names & column_names, const NamesAndTypesList & virtuals, const StorageID & storage_id) const;
-
     /// Check that all the requested names are in the table and have the correct types.
     void check(const NamesAndTypesList & columns) const;
 
@@ -253,4 +242,6 @@ struct StorageInMemoryMetadata
 using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;
 using MultiVersionStorageMetadataPtr = MultiVersion<StorageInMemoryMetadata>;
 
+String listOfColumns(const NamesAndTypesList & available_columns);
+
 }
diff --git a/src/Storages/StorageInput.cpp b/src/Storages/StorageInput.cpp
index 2ed7a77b59de..a21a14cc240e 100644
--- a/src/Storages/StorageInput.cpp
+++ b/src/Storages/StorageInput.cpp
@@ -52,7 +52,7 @@ void StorageInput::setPipe(Pipe pipe_)
 
 Pipe StorageInput::read(
     const Names & /*column_names*/,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -66,7 +66,7 @@ Pipe StorageInput::read(
     {
         /// Send structure to the client.
         query_context->initializeInput(shared_from_this());
-        return Pipe(std::make_shared<StorageInputSource>(query_context, metadata_snapshot->getSampleBlock()));
+        return Pipe(std::make_shared<StorageInputSource>(query_context, storage_snapshot->metadata->getSampleBlock()));
     }
 
     if (pipe.empty())
diff --git a/src/Storages/StorageInput.h b/src/Storages/StorageInput.h
index b28bc143bb01..4c44213a06bb 100644
--- a/src/Storages/StorageInput.h
+++ b/src/Storages/StorageInput.h
@@ -20,7 +20,7 @@ class StorageInput final : public shared_ptr_helper<StorageInput>, public IStora
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageJoin.cpp b/src/Storages/StorageJoin.cpp
index 9b3ec6617c02..ecd182457e21 100644
--- a/src/Storages/StorageJoin.cpp
+++ b/src/Storages/StorageJoin.cpp
@@ -579,16 +579,16 @@ class JoinSource : public SourceWithProgress
 // TODO: multiple stream read and index read
 Pipe StorageJoin::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t max_block_size,
     unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
-    Block source_sample_block = metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID());
+    Block source_sample_block = storage_snapshot->getSampleBlockForColumns(column_names);
     RWLockImpl::LockHolder holder = tryLockTimedWithContext(rwlock, RWLockImpl::Read, context);
     return Pipe(std::make_shared<JoinSource>(join, std::move(holder), max_block_size, source_sample_block));
 }
diff --git a/src/Storages/StorageJoin.h b/src/Storages/StorageJoin.h
index ba59bc063343..ea71ff4be8fe 100644
--- a/src/Storages/StorageJoin.h
+++ b/src/Storages/StorageJoin.h
@@ -51,7 +51,7 @@ class StorageJoin final : public shared_ptr_helper<StorageJoin>, public StorageS
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageLog.cpp b/src/Storages/StorageLog.cpp
index d5d1f312bec5..d3923a190a1f 100644
--- a/src/Storages/StorageLog.cpp
+++ b/src/Storages/StorageLog.cpp
@@ -765,14 +765,14 @@ void StorageLog::truncate(const ASTPtr &, const StorageMetadataPtr &, ContextPtr
 
 Pipe StorageLog::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t max_block_size,
     unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     auto lock_timeout = getLockTimeout(context);
     loadMarks(lock_timeout);
@@ -782,7 +782,7 @@ Pipe StorageLog::read(
         throw Exception("Lock timeout exceeded", ErrorCodes::TIMEOUT_EXCEEDED);
 
     if (!num_data_files || !file_checker.getFileSize(data_files[INDEX_WITH_REAL_ROW_COUNT].path))
-        return Pipe(std::make_shared<NullSource>(metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID())));
+        return Pipe(std::make_shared<NullSource>(storage_snapshot->getSampleBlockForColumns(column_names)));
 
     const Marks & marks_with_real_row_count = data_files[INDEX_WITH_REAL_ROW_COUNT].marks;
     size_t num_marks = marks_with_real_row_count.size();
@@ -791,7 +791,8 @@ Pipe StorageLog::read(
     if (num_streams > max_streams)
         num_streams = max_streams;
 
-    auto all_columns = metadata_snapshot->getColumns().getByNames(ColumnsDescription::All, column_names, true);
+    auto options = GetColumnsOptions(GetColumnsOptions::All).withSubcolumns();
+    auto all_columns = storage_snapshot->getColumnsByNames(options, column_names);
     all_columns = Nested::convertToSubcolumns(all_columns);
 
     std::vector<size_t> offsets;
diff --git a/src/Storages/StorageLog.h b/src/Storages/StorageLog.h
index 8b2ef0ccac15..d6f3208f693b 100644
--- a/src/Storages/StorageLog.h
+++ b/src/Storages/StorageLog.h
@@ -31,7 +31,7 @@ class StorageLog final : public shared_ptr_helper<StorageLog>, public IStorage
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageMaterializedMySQL.cpp b/src/Storages/StorageMaterializedMySQL.cpp
index 07817aa634ee..dbc0dd9ae922 100644
--- a/src/Storages/StorageMaterializedMySQL.cpp
+++ b/src/Storages/StorageMaterializedMySQL.cpp
@@ -34,7 +34,7 @@ bool StorageMaterializedMySQL::needRewriteQueryWithFinal(const Names & column_na
 
 Pipe StorageMaterializedMySQL::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & /*storage_snapshot*/,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
@@ -44,7 +44,7 @@ Pipe StorageMaterializedMySQL::read(
     if (const auto * db = typeid_cast<const DatabaseMaterializedMySQL *>(database))
         db->rethrowExceptionIfNeeded();
 
-    return readFinalFromNestedStorage(nested_storage, column_names, metadata_snapshot,
+    return readFinalFromNestedStorage(nested_storage, column_names,
             query_info, context, processed_stage, max_block_size, num_streams);
 }
 
diff --git a/src/Storages/StorageMaterializedMySQL.h b/src/Storages/StorageMaterializedMySQL.h
index ae874649b407..953d83360fdb 100644
--- a/src/Storages/StorageMaterializedMySQL.h
+++ b/src/Storages/StorageMaterializedMySQL.h
@@ -25,7 +25,7 @@ class StorageMaterializedMySQL final : public shared_ptr_helper<StorageMateriali
     bool needRewriteQueryWithFinal(const Names & column_names) const override;
 
     Pipe read(
-        const Names & column_names, const StorageMetadataPtr & metadata_snapshot, SelectQueryInfo & query_info,
+        const Names & column_names, const StorageSnapshotPtr & metadata_snapshot, SelectQueryInfo & query_info,
         ContextPtr context, QueryProcessingStage::Enum processed_stage, size_t max_block_size, unsigned num_streams) override;
 
     SinkToStoragePtr write(const ASTPtr &, const StorageMetadataPtr &, ContextPtr) override { throwNotAllowed(); }
diff --git a/src/Storages/StorageMaterializedView.cpp b/src/Storages/StorageMaterializedView.cpp
index 7c5ef5ac04cb..008b42e32994 100644
--- a/src/Storages/StorageMaterializedView.cpp
+++ b/src/Storages/StorageMaterializedView.cpp
@@ -132,19 +132,20 @@ StorageMaterializedView::StorageMaterializedView(
 QueryProcessingStage::Enum StorageMaterializedView::getQueryProcessingStage(
     ContextPtr local_context,
     QueryProcessingStage::Enum to_stage,
-    const StorageMetadataPtr &,
+    const StorageSnapshotPtr &,
     SelectQueryInfo & query_info) const
 {
     /// TODO: Find a way to support projections for StorageMaterializedView. Why do we use different
     /// metadata for materialized view and target table? If they are the same, we can get rid of all
     /// converting and use it just like a normal view.
     query_info.ignore_projections = true;
-    return getTargetTable()->getQueryProcessingStage(local_context, to_stage, getTargetTable()->getInMemoryMetadataPtr(), query_info);
+    const auto & target_metadata = getTargetTable()->getInMemoryMetadataPtr();
+    return getTargetTable()->getQueryProcessingStage(local_context, to_stage, getTargetTable()->getStorageSnapshot(target_metadata), query_info);
 }
 
 Pipe StorageMaterializedView::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -152,7 +153,7 @@ Pipe StorageMaterializedView::read(
     const unsigned num_streams)
 {
     QueryPlan plan;
-    read(plan, column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
+    read(plan, column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
     return plan.convertToPipe(
         QueryPlanOptimizationSettings::fromContext(local_context),
         BuildQueryPipelineSettings::fromContext(local_context));
@@ -161,7 +162,7 @@ Pipe StorageMaterializedView::read(
 void StorageMaterializedView::read(
     QueryPlan & query_plan,
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -171,15 +172,16 @@ void StorageMaterializedView::read(
     auto storage = getTargetTable();
     auto lock = storage->lockForShare(local_context->getCurrentQueryId(), local_context->getSettingsRef().lock_acquire_timeout);
     auto target_metadata_snapshot = storage->getInMemoryMetadataPtr();
+    auto target_storage_snapshot = storage->getStorageSnapshot(target_metadata_snapshot);
 
     if (query_info.order_optimizer)
         query_info.input_order_info = query_info.order_optimizer->getInputOrder(target_metadata_snapshot, local_context);
 
-    storage->read(query_plan, column_names, target_metadata_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
+    storage->read(query_plan, column_names, target_storage_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
 
     if (query_plan.isInitialized())
     {
-        auto mv_header = getHeaderForProcessingStage(*this, column_names, metadata_snapshot, query_info, local_context, processed_stage);
+        auto mv_header = getHeaderForProcessingStage(column_names, storage_snapshot, query_info, local_context, processed_stage);
         auto target_header = query_plan.getCurrentDataStream().header;
 
         /// No need to convert columns that does not exists in MV
diff --git a/src/Storages/StorageMaterializedView.h b/src/Storages/StorageMaterializedView.h
index 395560c1ca73..838f5278aa9e 100644
--- a/src/Storages/StorageMaterializedView.h
+++ b/src/Storages/StorageMaterializedView.h
@@ -66,7 +66,7 @@ class StorageMaterializedView final : public shared_ptr_helper<StorageMaterializ
     void shutdown() override;
 
     QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const override;
+    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const override;
 
     StoragePtr getTargetTable() const;
     StoragePtr tryGetTargetTable() const;
@@ -78,7 +78,7 @@ class StorageMaterializedView final : public shared_ptr_helper<StorageMaterializ
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -88,7 +88,7 @@ class StorageMaterializedView final : public shared_ptr_helper<StorageMaterializ
     void read(
         QueryPlan & query_plan,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageMemory.cpp b/src/Storages/StorageMemory.cpp
index 72851472b797..c3601b33a045 100644
--- a/src/Storages/StorageMemory.cpp
+++ b/src/Storages/StorageMemory.cpp
@@ -2,9 +2,13 @@
 #include <Common/Exception.h>
 
 #include <Interpreters/MutationsInterpreter.h>
+#include <Interpreters/getColumnFromBlock.h>
+#include <Interpreters/inplaceBlockConversions.h>
 #include <Storages/StorageFactory.h>
 #include <Storages/StorageMemory.h>
 #include <Storages/MemorySettings.h>
+#include <DataTypes/ObjectUtils.h>
+#include <Columns/ColumnObject.h>
 
 #include <IO/WriteHelpers.h>
 #include <Processors/Sources/SourceWithProgress.h>
@@ -30,13 +34,13 @@ class MemorySource : public SourceWithProgress
 
     MemorySource(
         Names column_names_,
-        const StorageMemory & storage,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         std::shared_ptr<const Blocks> data_,
         std::shared_ptr<std::atomic<size_t>> parallel_execution_index_,
         InitializerFunc initializer_func_ = {})
-        : SourceWithProgress(metadata_snapshot->getSampleBlockForColumns(column_names_, storage.getVirtuals(), storage.getStorageID()))
-        , column_names_and_types(metadata_snapshot->getColumns().getByNames(ColumnsDescription::All, column_names_, true))
+        : SourceWithProgress(storage_snapshot->getSampleBlockForColumns(column_names_))
+        , column_names_and_types(storage_snapshot->getColumnsByNames(
+            GetColumnsOptions(GetColumnsOptions::All).withSubcolumns().withExtendedObjects(), column_names_))
         , data(data_)
         , parallel_execution_index(parallel_execution_index_)
         , initializer_func(std::move(initializer_func_))
@@ -62,12 +66,20 @@ class MemorySource : public SourceWithProgress
         }
 
         const Block & src = (*data)[current_index];
+
         Columns columns;
-        columns.reserve(column_names_and_types.size());
+        size_t num_columns = column_names_and_types.size();
+        columns.reserve(num_columns);
+
+        auto name_and_type = column_names_and_types.begin();
+        for (size_t i = 0; i < num_columns; ++i)
+        {
+            columns.emplace_back(tryGetColumnFromBlock(src, *name_and_type));
+            ++name_and_type;
+        }
 
-        /// Add only required columns to `res`.
-        for (const auto & elem : column_names_and_types)
-            columns.emplace_back(getColumnFromBlock(src, elem));
+        fillMissingColumns(columns, src.rows(), column_names_and_types, /*metadata_snapshot=*/ nullptr);
+        assert(std::all_of(columns.begin(), columns.end(), [](const auto & column) { return column != nullptr; }));
 
         return Chunk(std::move(columns), src.rows());
     }
@@ -101,7 +113,7 @@ class MemorySink : public SinkToStorage
         const StorageMetadataPtr & metadata_snapshot_)
         : SinkToStorage(metadata_snapshot_->getSampleBlock())
         , storage(storage_)
-        , metadata_snapshot(metadata_snapshot_)
+        , storage_snapshot(storage_.getStorageSnapshot(metadata_snapshot_))
     {
     }
 
@@ -110,7 +122,15 @@ class MemorySink : public SinkToStorage
     void consume(Chunk chunk) override
     {
         auto block = getHeader().cloneWithColumns(chunk.getColumns());
-        metadata_snapshot->check(block, true);
+        storage_snapshot->metadata->check(block, true);
+        if (!storage_snapshot->object_columns.empty())
+        {
+            auto columns = storage_snapshot->metadata->getColumns().getAllPhysical().filter(block.getNames());
+            auto extended_storage_columns = storage_snapshot->getColumns(
+                GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects());
+
+            convertObjectsToTuples(columns, block, extended_storage_columns);
+        }
 
         if (storage.compress)
         {
@@ -151,7 +171,7 @@ class MemorySink : public SinkToStorage
     Blocks new_blocks;
 
     StorageMemory & storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
 };
 
 
@@ -170,17 +190,36 @@ StorageMemory::StorageMemory(
     setInMemoryMetadata(storage_metadata);
 }
 
+StorageSnapshotPtr StorageMemory::getStorageSnapshot(const StorageMetadataPtr & metadata_snapshot) const
+{
+    auto snapshot_data = std::make_unique<SnapshotData>();
+    snapshot_data->blocks = data.get();
+
+    if (!hasObjectColumns(metadata_snapshot->getColumns()))
+        return std::make_shared<StorageSnapshot>(*this, metadata_snapshot, ColumnsDescription{}, std::move(snapshot_data));
+
+    auto object_columns = getObjectColumns(
+        snapshot_data->blocks->begin(),
+        snapshot_data->blocks->end(),
+        metadata_snapshot->getColumns(),
+        [](const auto & block) -> const auto & { return block.getColumnsWithTypeAndName(); });
+
+    return std::make_shared<StorageSnapshot>(*this, metadata_snapshot, object_columns, std::move(snapshot_data));
+}
 
 Pipe StorageMemory::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t /*max_block_size*/,
     unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
+
+    const auto & snapshot_data = assert_cast<const SnapshotData &>(*storage_snapshot->data);
+    auto current_data = snapshot_data.blocks;
 
     if (delay_read_for_global_subqueries)
     {
@@ -194,17 +233,15 @@ Pipe StorageMemory::read(
 
         return Pipe(std::make_shared<MemorySource>(
             column_names,
-            *this,
-            metadata_snapshot,
+            storage_snapshot,
             nullptr /* data */,
             nullptr /* parallel execution index */,
-            [this](std::shared_ptr<const Blocks> & data_to_initialize)
+            [current_data](std::shared_ptr<const Blocks> & data_to_initialize)
             {
-                data_to_initialize = data.get();
+                data_to_initialize = current_data;
             }));
     }
 
-    auto current_data = data.get();
     size_t size = current_data->size();
 
     if (num_streams > size)
@@ -216,7 +253,7 @@ Pipe StorageMemory::read(
 
     for (size_t stream = 0; stream < num_streams; ++stream)
     {
-        pipes.emplace_back(std::make_shared<MemorySource>(column_names, *this, metadata_snapshot, current_data, parallel_execution_index));
+        pipes.emplace_back(std::make_shared<MemorySource>(column_names, storage_snapshot, current_data, parallel_execution_index));
     }
 
     return Pipe::unitePipes(std::move(pipes));
diff --git a/src/Storages/StorageMemory.h b/src/Storages/StorageMemory.h
index 063802faf1a0..1c4421e51a64 100644
--- a/src/Storages/StorageMemory.h
+++ b/src/Storages/StorageMemory.h
@@ -29,9 +29,18 @@ friend struct shared_ptr_helper<StorageMemory>;
 
     size_t getSize() const { return data.get()->size(); }
 
+    /// Snapshot for StorageMemory contains current set of blocks
+    /// at the moment of the start of query.
+    struct SnapshotData : public StorageSnapshot::Data
+    {
+        std::shared_ptr<const Blocks> blocks;
+    };
+
+    StorageSnapshotPtr getStorageSnapshot(const StorageMetadataPtr & metadata_snapshot) const override;
+
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -40,6 +49,7 @@ friend struct shared_ptr_helper<StorageMemory>;
 
     bool supportsParallelInsert() const override { return true; }
     bool supportsSubcolumns() const override { return true; }
+    bool supportsDynamicSubcolumns() const override { return true; }
 
     /// Smaller blocks (e.g. 64K rows) are better for CPU cache.
     bool prefersLargeBlocks() const override { return false; }
diff --git a/src/Storages/StorageMerge.cpp b/src/Storages/StorageMerge.cpp
index 15e499c6e6c8..96e6070e09ef 100644
--- a/src/Storages/StorageMerge.cpp
+++ b/src/Storages/StorageMerge.cpp
@@ -169,7 +169,7 @@ bool StorageMerge::mayBenefitFromIndexForIn(const ASTPtr & left_in_operand, Cont
 QueryProcessingStage::Enum StorageMerge::getQueryProcessingStage(
     ContextPtr local_context,
     QueryProcessingStage::Enum to_stage,
-    const StorageMetadataPtr &,
+    const StorageSnapshotPtr &,
     SelectQueryInfo & query_info) const
 {
     /// In case of JOIN the first stage (which includes JOIN)
@@ -200,7 +200,8 @@ QueryProcessingStage::Enum StorageMerge::getQueryProcessingStage(
                 ++selected_table_size;
                 stage_in_source_tables = std::max(
                     stage_in_source_tables,
-                    table->getQueryProcessingStage(local_context, to_stage, table->getInMemoryMetadataPtr(), query_info));
+                    table->getQueryProcessingStage(local_context, to_stage,
+                        table->getStorageSnapshot(table->getInMemoryMetadataPtr()), query_info));
             }
 
             iterator->next();
@@ -235,7 +236,7 @@ SelectQueryInfo StorageMerge::getModifiedQueryInfo(
 
 Pipe StorageMerge::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -251,9 +252,9 @@ Pipe StorageMerge::read(
 
     for (const auto & column_name : column_names)
     {
-        if (column_name == "_database" && isVirtualColumn(column_name, metadata_snapshot))
+        if (column_name == "_database" && isVirtualColumn(column_name, storage_snapshot->metadata))
             has_database_virtual_column = true;
-        else if (column_name == "_table" && isVirtualColumn(column_name, metadata_snapshot))
+        else if (column_name == "_table" && isVirtualColumn(column_name, storage_snapshot->metadata))
             has_table_virtual_column = true;
         else
             real_column_names.push_back(column_name);
@@ -266,7 +267,7 @@ Pipe StorageMerge::read(
     modified_context->setSetting("optimize_move_to_prewhere", false);
 
     /// What will be result structure depending on query processed stage in source tables?
-    Block header = getHeaderForProcessingStage(*this, column_names, metadata_snapshot, query_info, local_context, processed_stage);
+    Block header = getHeaderForProcessingStage(column_names, storage_snapshot, query_info, local_context, processed_stage);
 
     /** First we make list of selected tables to find out its size.
       * This is necessary to correctly pass the recommended number of threads to each table.
@@ -337,9 +338,11 @@ Pipe StorageMerge::read(
         Aliases aliases;
         auto storage_metadata_snapshot = storage->getInMemoryMetadataPtr();
         auto storage_columns = storage_metadata_snapshot->getColumns();
+        auto nested_storage_snaphsot = storage->getStorageSnapshot(storage_metadata_snapshot);
 
         auto modified_query_info = getModifiedQueryInfo(query_info, modified_context, storage->getStorageID(), storage->as<StorageMerge>());
-        auto syntax_result = TreeRewriter(local_context).analyzeSelect(modified_query_info.query, TreeRewriterResult({}, storage, storage_metadata_snapshot));
+        auto syntax_result = TreeRewriter(local_context).analyzeSelect(
+            modified_query_info.query, TreeRewriterResult({}, storage, nested_storage_snaphsot));
 
         Names column_names_as_aliases;
         bool with_aliases = processed_stage == QueryProcessingStage::FetchColumns && !storage_columns.getAliases().empty();
@@ -374,7 +377,8 @@ Pipe StorageMerge::read(
             }
 
             syntax_result = TreeRewriter(local_context).analyze(
-                required_columns_expr_list, storage_columns.getAllPhysical(), storage, storage_metadata_snapshot);
+                required_columns_expr_list, storage_columns.getAllPhysical(), storage, storage->getStorageSnapshot(storage_metadata_snapshot));
+
             auto alias_actions = ExpressionAnalyzer(required_columns_expr_list, syntax_result, local_context).getActionsDAG(true);
 
             column_names_as_aliases = alias_actions->getRequiredColumns().getNames();
@@ -383,7 +387,7 @@ Pipe StorageMerge::read(
         }
 
         auto source_pipe = createSources(
-            storage_metadata_snapshot,
+            nested_storage_snaphsot,
             modified_query_info,
             processed_stage,
             max_block_size,
@@ -411,7 +415,7 @@ Pipe StorageMerge::read(
 }
 
 Pipe StorageMerge::createSources(
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & modified_query_info,
     const QueryProcessingStage::Enum & processed_stage,
     const UInt64 max_block_size,
@@ -449,16 +453,16 @@ Pipe StorageMerge::createSources(
     }
 
     auto storage_stage
-        = storage->getQueryProcessingStage(modified_context, QueryProcessingStage::Complete, metadata_snapshot, modified_query_info);
+        = storage->getQueryProcessingStage(modified_context, QueryProcessingStage::Complete, storage_snapshot, modified_query_info);
     if (processed_stage <= storage_stage)
     {
         /// If there are only virtual columns in query, you must request at least one other column.
         if (real_column_names.empty())
-            real_column_names.push_back(ExpressionActions::getSmallestColumn(metadata_snapshot->getColumns().getAllPhysical()));
+            real_column_names.push_back(ExpressionActions::getSmallestColumn(storage_snapshot->metadata->getColumns().getAllPhysical()));
 
         pipe = storage->read(
             real_column_names,
-            metadata_snapshot,
+            storage_snapshot,
             modified_query_info,
             modified_context,
             processed_stage,
@@ -538,7 +542,7 @@ Pipe StorageMerge::createSources(
 
         /// Subordinary tables could have different but convertible types, like numeric types of different width.
         /// We must return streams with structure equals to structure of Merge table.
-        convertingSourceStream(header, metadata_snapshot, aliases, modified_context, modified_query_info.query, pipe, processed_stage);
+        convertingSourceStream(header, storage_snapshot->metadata, aliases, modified_context, modified_query_info.query, pipe, processed_stage);
 
         pipe.addTableLock(struct_lock);
         pipe.addStorageHolder(storage);
diff --git a/src/Storages/StorageMerge.h b/src/Storages/StorageMerge.h
index e0d815313255..b7bdd957164c 100644
--- a/src/Storages/StorageMerge.h
+++ b/src/Storages/StorageMerge.h
@@ -30,11 +30,11 @@ class StorageMerge final : public shared_ptr_helper<StorageMerge>, public IStora
     bool canMoveConditionsToPrewhere() const override;
 
     QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const override;
+    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const override;
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -116,7 +116,7 @@ class StorageMerge final : public shared_ptr_helper<StorageMerge>, public IStora
     using Aliases = std::vector<AliasData>;
 
     Pipe createSources(
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         const QueryProcessingStage::Enum & processed_stage,
         UInt64 max_block_size,
diff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp
index a05ed04a66c0..812e2264adb1 100644
--- a/src/Storages/StorageMergeTree.cpp
+++ b/src/Storages/StorageMergeTree.cpp
@@ -193,7 +193,7 @@ StorageMergeTree::~StorageMergeTree()
 void StorageMergeTree::read(
     QueryPlan & query_plan,
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -207,13 +207,13 @@ void StorageMergeTree::read(
         LOG_TRACE(log, "Parallel reading from replicas enabled {}", enable_parallel_reading);
 
     if (auto plan = reader.read(
-        column_names, metadata_snapshot, query_info, local_context, max_block_size, num_streams, processed_stage, nullptr, enable_parallel_reading))
+        column_names, storage_snapshot, query_info, local_context, max_block_size, num_streams, processed_stage, nullptr, enable_parallel_reading))
         query_plan = std::move(*plan);
 }
 
 Pipe StorageMergeTree::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -221,7 +221,7 @@ Pipe StorageMergeTree::read(
     const unsigned num_streams)
 {
     QueryPlan plan;
-    read(plan, column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
+    read(plan, column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
     return plan.convertToPipe(
         QueryPlanOptimizationSettings::fromContext(local_context),
         BuildQueryPipelineSettings::fromContext(local_context));
diff --git a/src/Storages/StorageMergeTree.h b/src/Storages/StorageMergeTree.h
index d3970449ceb0..abdaf6934d6d 100644
--- a/src/Storages/StorageMergeTree.h
+++ b/src/Storages/StorageMergeTree.h
@@ -43,7 +43,7 @@ class StorageMergeTree final : public shared_ptr_helper<StorageMergeTree>, publi
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -53,7 +53,7 @@ class StorageMergeTree final : public shared_ptr_helper<StorageMergeTree>, publi
     void read(
         QueryPlan & query_plan,
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageMongoDB.cpp b/src/Storages/StorageMongoDB.cpp
index 9b25b44c0e70..dcb3eaa77db8 100644
--- a/src/Storages/StorageMongoDB.cpp
+++ b/src/Storages/StorageMongoDB.cpp
@@ -90,7 +90,7 @@ void StorageMongoDB::connectIfNotConnected()
 
 Pipe StorageMongoDB::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -99,12 +99,12 @@ Pipe StorageMongoDB::read(
 {
     connectIfNotConnected();
 
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     Block sample_block;
     for (const String & column_name : column_names)
     {
-        auto column_data = metadata_snapshot->getColumns().getPhysical(column_name);
+        auto column_data = storage_snapshot->metadata->getColumns().getPhysical(column_name);
         sample_block.insert({ column_data.type, column_data.name });
     }
 
diff --git a/src/Storages/StorageMongoDB.h b/src/Storages/StorageMongoDB.h
index 0edfb5587593..549d444d7bb5 100644
--- a/src/Storages/StorageMongoDB.h
+++ b/src/Storages/StorageMongoDB.h
@@ -36,7 +36,7 @@ class StorageMongoDB final : public shared_ptr_helper<StorageMongoDB>, public IS
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageMySQL.cpp b/src/Storages/StorageMySQL.cpp
index 9dcbec0caae5..5e7c2ae95ae3 100644
--- a/src/Storages/StorageMySQL.cpp
+++ b/src/Storages/StorageMySQL.cpp
@@ -75,17 +75,17 @@ StorageMySQL::StorageMySQL(
 
 Pipe StorageMySQL::read(
     const Names & column_names_,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info_,
     ContextPtr context_,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t /*max_block_size*/,
     unsigned)
 {
-    metadata_snapshot->check(column_names_, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names_);
     String query = transformQueryForExternalDatabase(
         query_info_,
-        metadata_snapshot->getColumns().getOrdinary(),
+        storage_snapshot->metadata->getColumns().getOrdinary(),
         IdentifierQuotingStyle::BackticksMySQL,
         remote_database_name,
         remote_table_name,
@@ -95,7 +95,7 @@ Pipe StorageMySQL::read(
     Block sample_block;
     for (const String & column_name : column_names_)
     {
-        auto column_data = metadata_snapshot->getColumns().getPhysical(column_name);
+        auto column_data = storage_snapshot->metadata->getColumns().getPhysical(column_name);
 
         WhichDataType which(column_data.type);
         /// Convert enum to string.
diff --git a/src/Storages/StorageMySQL.h b/src/Storages/StorageMySQL.h
index fe2ee8439bce..03ebaaf87d71 100644
--- a/src/Storages/StorageMySQL.h
+++ b/src/Storages/StorageMySQL.h
@@ -44,7 +44,7 @@ class StorageMySQL final : public shared_ptr_helper<StorageMySQL>, public IStora
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageNull.h b/src/Storages/StorageNull.h
index 82baa98834d7..c5b2e2bf1617 100644
--- a/src/Storages/StorageNull.h
+++ b/src/Storages/StorageNull.h
@@ -23,7 +23,7 @@ class StorageNull final : public shared_ptr_helper<StorageNull>, public IStorage
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo &,
         ContextPtr /*context*/,
         QueryProcessingStage::Enum /*processing_stage*/,
@@ -31,7 +31,7 @@ class StorageNull final : public shared_ptr_helper<StorageNull>, public IStorage
         unsigned) override
     {
         return Pipe(
-            std::make_shared<NullSource>(metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID())));
+            std::make_shared<NullSource>(storage_snapshot->getSampleBlockForColumns(column_names)));
     }
 
     bool supportsParallelInsert() const override { return true; }
diff --git a/src/Storages/StoragePostgreSQL.cpp b/src/Storages/StoragePostgreSQL.cpp
index aa54663ca105..c50086d2c520 100644
--- a/src/Storages/StoragePostgreSQL.cpp
+++ b/src/Storages/StoragePostgreSQL.cpp
@@ -76,26 +76,26 @@ StoragePostgreSQL::StoragePostgreSQL(
 
 Pipe StoragePostgreSQL::read(
     const Names & column_names_,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info_,
     ContextPtr context_,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t max_block_size_,
     unsigned)
 {
-    metadata_snapshot->check(column_names_, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names_);
 
     /// Connection is already made to the needed database, so it should not be present in the query;
     /// remote_table_schema is empty if it is not specified, will access only table_name.
     String query = transformQueryForExternalDatabase(
-        query_info_, metadata_snapshot->getColumns().getOrdinary(),
+        query_info_, storage_snapshot->metadata->getColumns().getOrdinary(),
         IdentifierQuotingStyle::DoubleQuotes, remote_table_schema, remote_table_name, context_);
     LOG_TRACE(log, "Query: {}", query);
 
     Block sample_block;
     for (const String & column_name : column_names_)
     {
-        auto column_data = metadata_snapshot->getColumns().getPhysical(column_name);
+        auto column_data = storage_snapshot->metadata->getColumns().getPhysical(column_name);
         WhichDataType which(column_data.type);
         if (which.isEnum())
             column_data.type = std::make_shared<DataTypeString>();
diff --git a/src/Storages/StoragePostgreSQL.h b/src/Storages/StoragePostgreSQL.h
index 7d8752c91b9b..ae41a713285e 100644
--- a/src/Storages/StoragePostgreSQL.h
+++ b/src/Storages/StoragePostgreSQL.h
@@ -35,7 +35,7 @@ class StoragePostgreSQL final : public shared_ptr_helper<StoragePostgreSQL>, pub
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageProxy.h b/src/Storages/StorageProxy.h
index 894b470ef22b..d5af81ced3db 100644
--- a/src/Storages/StorageProxy.h
+++ b/src/Storages/StorageProxy.h
@@ -35,12 +35,13 @@ class StorageProxy : public IStorage
     QueryProcessingStage::Enum getQueryProcessingStage(
         ContextPtr context,
         QueryProcessingStage::Enum to_stage,
-        const StorageMetadataPtr &,
+        const StorageSnapshotPtr &,
         SelectQueryInfo & info) const override
     {
         /// TODO: Find a way to support projections for StorageProxy
         info.ignore_projections = true;
-        return getNested()->getQueryProcessingStage(context, to_stage, getNested()->getInMemoryMetadataPtr(), info);
+        const auto & nested_metadata = getNested()->getInMemoryMetadataPtr();
+        return getNested()->getQueryProcessingStage(context, to_stage, getNested()->getStorageSnapshot(nested_metadata), info);
     }
 
     Pipe watch(
@@ -56,14 +57,14 @@ class StorageProxy : public IStorage
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
         size_t max_block_size,
         unsigned num_streams) override
     {
-        return getNested()->read(column_names, metadata_snapshot, query_info, context, processed_stage, max_block_size, num_streams);
+        return getNested()->read(column_names, storage_snapshot, query_info, context, processed_stage, max_block_size, num_streams);
     }
 
     SinkToStoragePtr write(const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, ContextPtr context) override
diff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp
index 9a5e1cfbabd0..508b023bb794 100644
--- a/src/Storages/StorageReplicatedMergeTree.cpp
+++ b/src/Storages/StorageReplicatedMergeTree.cpp
@@ -4216,7 +4216,7 @@ ReplicatedMergeTreeQuorumAddedParts::PartitionIdToMaxBlock StorageReplicatedMerg
 void StorageReplicatedMergeTree::read(
     QueryPlan & query_plan,
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -4235,14 +4235,14 @@ void StorageReplicatedMergeTree::read(
     {
         auto max_added_blocks = std::make_shared<ReplicatedMergeTreeQuorumAddedParts::PartitionIdToMaxBlock>(getMaxAddedBlocks());
         if (auto plan = reader.read(
-                column_names, metadata_snapshot, query_info, local_context,
+                column_names, storage_snapshot, query_info, local_context,
                 max_block_size, num_streams, processed_stage, std::move(max_added_blocks), enable_parallel_reading))
             query_plan = std::move(*plan);
         return;
     }
 
     if (auto plan = reader.read(
-        column_names, metadata_snapshot, query_info, local_context,
+        column_names, storage_snapshot, query_info, local_context,
         max_block_size, num_streams, processed_stage, nullptr, enable_parallel_reading))
     {
         query_plan = std::move(*plan);
@@ -4251,7 +4251,7 @@ void StorageReplicatedMergeTree::read(
 
 Pipe StorageReplicatedMergeTree::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -4259,7 +4259,7 @@ Pipe StorageReplicatedMergeTree::read(
     const unsigned num_streams)
 {
     QueryPlan plan;
-    read(plan, column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
+    read(plan, column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
     return plan.convertToPipe(
         QueryPlanOptimizationSettings::fromContext(local_context),
         BuildQueryPipelineSettings::fromContext(local_context));
diff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h
index 935bd0486033..ff806d7a9b25 100644
--- a/src/Storages/StorageReplicatedMergeTree.h
+++ b/src/Storages/StorageReplicatedMergeTree.h
@@ -98,7 +98,7 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -108,7 +108,7 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica
     void read(
         QueryPlan & query_plan,
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp
index ec506ad0cd02..f319bd1097be 100644
--- a/src/Storages/StorageS3.cpp
+++ b/src/Storages/StorageS3.cpp
@@ -622,7 +622,7 @@ bool StorageS3::isColumnOriented() const
 
 Pipe StorageS3::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr local_context,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -649,13 +649,13 @@ Pipe StorageS3::read(
     if (isColumnOriented())
     {
         columns_description = ColumnsDescription{
-            metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID()).getNamesAndTypesList()};
-        block_for_format = metadata_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
+            storage_snapshot->getSampleBlockForColumns(column_names).getNamesAndTypesList()};
+        block_for_format = storage_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
     }
     else
     {
-        columns_description = metadata_snapshot->getColumns();
-        block_for_format = metadata_snapshot->getSampleBlock();
+        columns_description = storage_snapshot->metadata->getColumns();
+        block_for_format = storage_snapshot->metadata->getSampleBlock();
     }
 
     for (size_t i = 0; i < num_streams; ++i)
diff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h
index b2283687e2b4..300b7becb93f 100644
--- a/src/Storages/StorageS3.h
+++ b/src/Storages/StorageS3.h
@@ -146,7 +146,7 @@ class StorageS3 : public shared_ptr_helper<StorageS3>, public IStorage, WithCont
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageS3Cluster.cpp b/src/Storages/StorageS3Cluster.cpp
index 57220c683476..b5549b32554b 100644
--- a/src/Storages/StorageS3Cluster.cpp
+++ b/src/Storages/StorageS3Cluster.cpp
@@ -73,7 +73,7 @@ StorageS3Cluster::StorageS3Cluster(
 /// The code executes on initiator
 Pipe StorageS3Cluster::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
@@ -132,12 +132,12 @@ Pipe StorageS3Cluster::read(
         }
     }
 
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
     return Pipe::unitePipes(std::move(pipes));
 }
 
 QueryProcessingStage::Enum StorageS3Cluster::getQueryProcessingStage(
-    ContextPtr context, QueryProcessingStage::Enum to_stage, const StorageMetadataPtr &, SelectQueryInfo &) const
+    ContextPtr context, QueryProcessingStage::Enum to_stage, const StorageSnapshotPtr &, SelectQueryInfo &) const
 {
     /// Initiator executes query on remote node.
     if (context->getClientInfo().query_kind == ClientInfo::QueryKind::INITIAL_QUERY)
diff --git a/src/Storages/StorageS3Cluster.h b/src/Storages/StorageS3Cluster.h
index d1e02c5a7308..6d64c56020fb 100644
--- a/src/Storages/StorageS3Cluster.h
+++ b/src/Storages/StorageS3Cluster.h
@@ -25,11 +25,11 @@ class StorageS3Cluster : public shared_ptr_helper<StorageS3Cluster>, public ISto
 public:
     std::string getName() const override { return "S3Cluster"; }
 
-    Pipe read(const Names &, const StorageMetadataPtr &, SelectQueryInfo &,
+    Pipe read(const Names &, const StorageSnapshotPtr &, SelectQueryInfo &,
         ContextPtr, QueryProcessingStage::Enum, size_t /*max_block_size*/, unsigned /*num_streams*/) override;
 
     QueryProcessingStage::Enum
-    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageMetadataPtr &, SelectQueryInfo &) const override;
+    getQueryProcessingStage(ContextPtr, QueryProcessingStage::Enum, const StorageSnapshotPtr &, SelectQueryInfo &) const override;
 
     NamesAndTypesList getVirtuals() const override;
 
diff --git a/src/Storages/StorageSQLite.cpp b/src/Storages/StorageSQLite.cpp
index f93584ab374e..bc4e2b1dfe84 100644
--- a/src/Storages/StorageSQLite.cpp
+++ b/src/Storages/StorageSQLite.cpp
@@ -52,7 +52,7 @@ StorageSQLite::StorageSQLite(
 
 Pipe StorageSQLite::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context_,
     QueryProcessingStage::Enum,
@@ -62,11 +62,11 @@ Pipe StorageSQLite::read(
     if (!sqlite_db)
         sqlite_db = openSQLiteDB(database_path, getContext(), /* throw_on_error */true);
 
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     String query = transformQueryForExternalDatabase(
         query_info,
-        metadata_snapshot->getColumns().getOrdinary(),
+        storage_snapshot->metadata->getColumns().getOrdinary(),
         IdentifierQuotingStyle::DoubleQuotes,
         "",
         remote_table_name,
@@ -76,7 +76,7 @@ Pipe StorageSQLite::read(
     Block sample_block;
     for (const String & column_name : column_names)
     {
-        auto column_data = metadata_snapshot->getColumns().getPhysical(column_name);
+        auto column_data = storage_snapshot->metadata->getColumns().getPhysical(column_name);
         sample_block.insert({column_data.type, column_data.name});
     }
 
diff --git a/src/Storages/StorageSQLite.h b/src/Storages/StorageSQLite.h
index e8fd0771ff4b..367e6ee9e809 100644
--- a/src/Storages/StorageSQLite.h
+++ b/src/Storages/StorageSQLite.h
@@ -36,7 +36,7 @@ friend struct shared_ptr_helper<StorageSQLite>;
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageSnapshot.cpp b/src/Storages/StorageSnapshot.cpp
new file mode 100644
index 000000000000..e214afc6a90c
--- /dev/null
+++ b/src/Storages/StorageSnapshot.cpp
@@ -0,0 +1,175 @@
+#include <Storages/StorageSnapshot.h>
+#include <Storages/IStorage.h>
+#include <DataTypes/ObjectUtils.h>
+#include <DataTypes/NestedUtils.h>
+#include <sparsehash/dense_hash_map>
+#include <sparsehash/dense_hash_set>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int NOT_FOUND_COLUMN_IN_BLOCK;
+    extern const int EMPTY_LIST_OF_COLUMNS_QUERIED;
+    extern const int NO_SUCH_COLUMN_IN_TABLE;
+    extern const int COLUMN_QUERIED_MORE_THAN_ONCE;
+}
+
+void StorageSnapshot::init()
+{
+    for (const auto & [name, type] : storage.getVirtuals())
+        virtual_columns[name] = type;
+}
+
+NamesAndTypesList StorageSnapshot::getColumns(const GetColumnsOptions & options) const
+{
+    auto all_columns = getMetadataForQuery()->getColumns().get(options);
+
+    if (options.with_extended_objects)
+        extendObjectColumns(all_columns, object_columns, options.with_subcolumns);
+
+    if (options.with_virtuals)
+    {
+        /// Virtual columns must be appended after ordinary,
+        /// because user can override them.
+        if (!virtual_columns.empty())
+        {
+            NameSet column_names;
+            for (const auto & column : all_columns)
+                column_names.insert(column.name);
+
+            for (const auto & [name, type] : virtual_columns)
+                if (!column_names.count(name))
+                    all_columns.emplace_back(name, type);
+        }
+    }
+
+    return all_columns;
+}
+
+NamesAndTypesList StorageSnapshot::getColumnsByNames(const GetColumnsOptions & options, const Names & names) const
+{
+    NamesAndTypesList res;
+    const auto & columns = getMetadataForQuery()->getColumns();
+    for (const auto & name : names)
+    {
+        auto column = columns.tryGetColumn(options, name);
+        if (column && !isObject(column->type))
+        {
+            res.emplace_back(std::move(*column));
+            continue;
+        }
+
+        if (options.with_extended_objects)
+        {
+            auto object_column = object_columns.tryGetColumn(options, name);
+            if (object_column)
+            {
+                res.emplace_back(std::move(*object_column));
+                continue;
+            }
+        }
+
+        if (options.with_virtuals)
+        {
+            auto it = virtual_columns.find(name);
+            if (it != virtual_columns.end())
+            {
+                res.emplace_back(name, it->second);
+                continue;
+            }
+        }
+
+        throw Exception(ErrorCodes::NO_SUCH_COLUMN_IN_TABLE, "There is no column {} in table", name);
+    }
+
+    return res;
+}
+
+Block StorageSnapshot::getSampleBlockForColumns(const Names & column_names) const
+{
+    Block res;
+
+    const auto & columns = getMetadataForQuery()->getColumns();
+    for (const auto & name : column_names)
+    {
+        auto column = columns.tryGetColumnOrSubcolumn(GetColumnsOptions::All, name);
+        auto object_column = object_columns.tryGetColumnOrSubcolumn(GetColumnsOptions::All, name);
+
+        if (column && !object_column)
+        {
+            res.insert({column->type->createColumn(), column->type, column->name});
+        }
+        else if (object_column)
+        {
+            res.insert({object_column->type->createColumn(), object_column->type, object_column->name});
+        }
+        else if (auto it = virtual_columns.find(name); it != virtual_columns.end())
+        {
+            /// Virtual columns must be appended after ordinary, because user can
+            /// override them.
+            const auto & type = it->second;
+            res.insert({type->createColumn(), type, name});
+        }
+        else
+        {
+            throw Exception(ErrorCodes::NOT_FOUND_COLUMN_IN_BLOCK,
+                "Column {} not found in table {}", backQuote(name), storage.getStorageID().getNameForLogs());
+        }
+    }
+
+    return res;
+}
+
+namespace
+{
+    using DenseHashSet = google::dense_hash_set<StringRef, StringRefHash>;
+}
+
+void StorageSnapshot::check(const Names & column_names) const
+{
+    const auto & columns = getMetadataForQuery()->getColumns();
+    auto options = GetColumnsOptions(GetColumnsOptions::AllPhysical).withSubcolumns();
+
+    if (column_names.empty())
+    {
+        auto list_of_columns = listOfColumns(columns.get(options));
+        throw Exception(ErrorCodes::EMPTY_LIST_OF_COLUMNS_QUERIED,
+            "Empty list of columns queried. There are columns: {}", list_of_columns);
+    }
+
+    DenseHashSet unique_names;
+    unique_names.set_empty_key(StringRef());
+
+    for (const auto & name : column_names)
+    {
+        bool has_column = columns.hasColumnOrSubcolumn(GetColumnsOptions::AllPhysical, name)
+            || object_columns.hasColumnOrSubcolumn(GetColumnsOptions::AllPhysical, name)
+            || virtual_columns.count(name);
+
+        if (!has_column)
+        {
+            auto list_of_columns = listOfColumns(columns.get(options));
+            throw Exception(ErrorCodes::NO_SUCH_COLUMN_IN_TABLE,
+                "There is no column with name {} in table {}. There are columns: {}",
+                backQuote(name), storage.getStorageID().getNameForLogs(), list_of_columns);
+        }
+
+        if (unique_names.count(name))
+            throw Exception(ErrorCodes::COLUMN_QUERIED_MORE_THAN_ONCE, "Column {} queried more than once", name);
+
+        unique_names.insert(name);
+    }
+}
+
+DataTypePtr StorageSnapshot::getConcreteType(const String & column_name) const
+{
+    auto object_column = object_columns.tryGetColumnOrSubcolumn(GetColumnsOptions::All, column_name);
+    if (object_column)
+        return object_column->type;
+
+    return metadata->getColumns().get(column_name).type;
+}
+
+}
diff --git a/src/Storages/StorageSnapshot.h b/src/Storages/StorageSnapshot.h
new file mode 100644
index 000000000000..46244827f6c1
--- /dev/null
+++ b/src/Storages/StorageSnapshot.h
@@ -0,0 +1,86 @@
+#pragma once
+#include <Storages/StorageInMemoryMetadata.h>
+
+namespace DB
+{
+
+class IStorage;
+
+/// Snapshot of storage that fixes set columns that can be read in query.
+/// There are 3 sources of columns: regular columns from metadata,
+/// dynamic columns from object Types, virtual columns.
+struct StorageSnapshot
+{
+    const IStorage & storage;
+    const StorageMetadataPtr metadata;
+    const ColumnsDescription object_columns;
+
+    /// Additional data, on which set of columns may depend.
+    /// E.g. data parts in MergeTree, list of blocks in Memory, etc.
+    struct Data
+    {
+        virtual ~Data() = default;
+    };
+
+    using DataPtr = std::unique_ptr<const Data>;
+    const DataPtr data;
+
+    /// Projection that is used in query.
+    mutable const ProjectionDescription * projection = nullptr;
+
+    StorageSnapshot(
+        const IStorage & storage_,
+        const StorageMetadataPtr & metadata_)
+        : storage(storage_), metadata(metadata_)
+    {
+        init();
+    }
+
+    StorageSnapshot(
+        const IStorage & storage_,
+        const StorageMetadataPtr & metadata_,
+        const ColumnsDescription & object_columns_)
+        : storage(storage_), metadata(metadata_), object_columns(object_columns_)
+    {
+        init();
+    }
+
+    StorageSnapshot(
+        const IStorage & storage_,
+        const StorageMetadataPtr & metadata_,
+        const ColumnsDescription & object_columns_,
+        DataPtr data_)
+        : storage(storage_), metadata(metadata_), object_columns(object_columns_), data(std::move(data_))
+    {
+        init();
+    }
+
+    /// Get all available columns with types according to options.
+    NamesAndTypesList getColumns(const GetColumnsOptions & options) const;
+
+    /// Get columns with types according to options only for requested names.
+    NamesAndTypesList getColumnsByNames(const GetColumnsOptions & options, const Names & names) const;
+
+    /// Block with ordinary + materialized + aliases + virtuals + subcolumns.
+    Block getSampleBlockForColumns(const Names & column_names) const;
+
+    /// Verify that all the requested names are in the table and are set correctly:
+    /// list of names is not empty and the names do not repeat.
+    void check(const Names & column_names) const;
+
+    DataTypePtr getConcreteType(const String & column_name) const;
+
+    void addProjection(const ProjectionDescription * projection_) const { projection = projection_; }
+
+    /// If we have a projection then we should use its metadata.
+    StorageMetadataPtr getMetadataForQuery() const { return projection ? projection->metadata : metadata; }
+
+private:
+    void init();
+
+    std::unordered_map<String, DataTypePtr> virtual_columns;
+};
+
+using StorageSnapshotPtr = std::shared_ptr<const StorageSnapshot>;
+
+}
diff --git a/src/Storages/StorageStripeLog.cpp b/src/Storages/StorageStripeLog.cpp
index c401d27a8fc3..f1f84a88c36e 100644
--- a/src/Storages/StorageStripeLog.cpp
+++ b/src/Storages/StorageStripeLog.cpp
@@ -63,14 +63,13 @@ class StripeLogSource final : public SourceWithProgress
 {
 public:
     static Block getHeader(
-        const StorageStripeLog & storage,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const Names & column_names,
         IndexForNativeFormat::Blocks::const_iterator index_begin,
         IndexForNativeFormat::Blocks::const_iterator index_end)
     {
         if (index_begin == index_end)
-            return metadata_snapshot->getSampleBlockForColumns(column_names, storage.getVirtuals(), storage.getStorageID());
+            return storage_snapshot->getSampleBlockForColumns(column_names);
 
         /// TODO: check if possible to always return storage.getSampleBlock()
 
@@ -87,16 +86,16 @@ class StripeLogSource final : public SourceWithProgress
 
     StripeLogSource(
         const StorageStripeLog & storage_,
-        const StorageMetadataPtr & metadata_snapshot_,
+        const StorageSnapshotPtr & storage_snapshot_,
         const Names & column_names,
         ReadSettings read_settings_,
         std::shared_ptr<const IndexForNativeFormat> indices_,
         IndexForNativeFormat::Blocks::const_iterator index_begin_,
         IndexForNativeFormat::Blocks::const_iterator index_end_,
         size_t file_size_)
-        : SourceWithProgress(getHeader(storage_, metadata_snapshot_, column_names, index_begin_, index_end_))
+        : SourceWithProgress(getHeader(storage_snapshot_, column_names, index_begin_, index_end_))
         , storage(storage_)
-        , metadata_snapshot(metadata_snapshot_)
+        , storage_snapshot(storage_snapshot_)
         , read_settings(std::move(read_settings_))
         , indices(indices_)
         , index_begin(index_begin_)
@@ -131,7 +130,7 @@ class StripeLogSource final : public SourceWithProgress
 
 private:
     const StorageStripeLog & storage;
-    StorageMetadataPtr metadata_snapshot;
+    StorageSnapshotPtr storage_snapshot;
     ReadSettings read_settings;
 
     std::shared_ptr<const IndexForNativeFormat> indices;
@@ -343,14 +342,14 @@ static std::chrono::seconds getLockTimeout(ContextPtr context)
 
 Pipe StorageStripeLog::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t /*max_block_size*/,
     unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     auto lock_timeout = getLockTimeout(context);
     loadIndices(lock_timeout);
@@ -361,7 +360,7 @@ Pipe StorageStripeLog::read(
 
     size_t data_file_size = file_checker.getFileSize(data_file_path);
     if (!data_file_size)
-        return Pipe(std::make_shared<NullSource>(metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID())));
+        return Pipe(std::make_shared<NullSource>(storage_snapshot->getSampleBlockForColumns(column_names)));
 
     auto indices_for_selected_columns
         = std::make_shared<IndexForNativeFormat>(indices.extractIndexForColumns(NameSet{column_names.begin(), column_names.end()}));
@@ -382,7 +381,7 @@ Pipe StorageStripeLog::read(
         std::advance(end, (stream + 1) * size / num_streams);
 
         pipes.emplace_back(std::make_shared<StripeLogSource>(
-            *this, metadata_snapshot, column_names, read_settings, indices_for_selected_columns, begin, end, data_file_size));
+            *this, storage_snapshot, column_names, read_settings, indices_for_selected_columns, begin, end, data_file_size));
     }
 
     /// We do not keep read lock directly at the time of reading, because we read ranges of data that do not change.
diff --git a/src/Storages/StorageStripeLog.h b/src/Storages/StorageStripeLog.h
index 579e2f991e70..bab5116cfc14 100644
--- a/src/Storages/StorageStripeLog.h
+++ b/src/Storages/StorageStripeLog.h
@@ -32,7 +32,7 @@ class StorageStripeLog final : public shared_ptr_helper<StorageStripeLog>, publi
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageTableFunction.h b/src/Storages/StorageTableFunction.h
index 8054762d3898..4616421b24a9 100644
--- a/src/Storages/StorageTableFunction.h
+++ b/src/Storages/StorageTableFunction.h
@@ -93,7 +93,7 @@ class StorageTableFunctionProxy final : public StorageProxy
 
     Pipe read(
             const Names & column_names,
-            const StorageMetadataPtr & metadata_snapshot,
+            const StorageSnapshotPtr & storage_snapshot,
             SelectQueryInfo & query_info,
             ContextPtr context,
             QueryProcessingStage::Enum processed_stage,
@@ -104,12 +104,12 @@ class StorageTableFunctionProxy final : public StorageProxy
         for (const auto & c : column_names)
             cnames += c + " ";
         auto storage = getNested();
-        auto nested_metadata = storage->getInMemoryMetadataPtr();
-        auto pipe = storage->read(column_names, nested_metadata, query_info, context,
+        auto nested_snapshot = storage->getStorageSnapshot(storage->getInMemoryMetadataPtr());
+        auto pipe = storage->read(column_names, nested_snapshot, query_info, context,
                                   processed_stage, max_block_size, num_streams);
         if (!pipe.empty() && add_conversion)
         {
-            auto to_header = getHeaderForProcessingStage(*this, column_names, metadata_snapshot,
+            auto to_header = getHeaderForProcessingStage(column_names, storage_snapshot,
                                                          query_info, context, processed_stage);
 
             auto convert_actions_dag = ActionsDAG::makeConvertingActions(
diff --git a/src/Storages/StorageURL.cpp b/src/Storages/StorageURL.cpp
index 57fca113a581..f727b8f69528 100644
--- a/src/Storages/StorageURL.cpp
+++ b/src/Storages/StorageURL.cpp
@@ -394,7 +394,7 @@ std::string IStorageURLBase::getReadMethod() const
 
 std::vector<std::pair<std::string, std::string>> IStorageURLBase::getReadURIParams(
     const Names & /*column_names*/,
-    const StorageMetadataPtr & /*metadata_snapshot*/,
+    const StorageSnapshotPtr & /*storage_snapshot*/,
     const SelectQueryInfo & /*query_info*/,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum & /*processed_stage*/,
@@ -489,27 +489,27 @@ bool IStorageURLBase::isColumnOriented() const
 
 Pipe IStorageURLBase::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
     size_t max_block_size,
     unsigned num_streams)
 {
-    auto params = getReadURIParams(column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size);
+    auto params = getReadURIParams(column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size);
 
     ColumnsDescription columns_description;
     Block block_for_format;
     if (isColumnOriented())
     {
         columns_description = ColumnsDescription{
-            metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID()).getNamesAndTypesList()};
-        block_for_format = metadata_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
+            storage_snapshot->getSampleBlockForColumns(column_names).getNamesAndTypesList()};
+        block_for_format = storage_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
     }
     else
     {
-        columns_description = metadata_snapshot->getColumns();
-        block_for_format = metadata_snapshot->getSampleBlock();
+        columns_description = storage_snapshot->metadata->getColumns();
+        block_for_format = storage_snapshot->metadata->getSampleBlock();
     }
 
     if (urlWithGlobs(uri))
@@ -573,7 +573,7 @@ Pipe IStorageURLBase::read(
 
 Pipe StorageURLWithFailover::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
@@ -585,16 +585,16 @@ Pipe StorageURLWithFailover::read(
     if (isColumnOriented())
     {
         columns_description = ColumnsDescription{
-            metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID()).getNamesAndTypesList()};
-        block_for_format = metadata_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
+            storage_snapshot->getSampleBlockForColumns(column_names).getNamesAndTypesList()};
+        block_for_format = storage_snapshot->getSampleBlockForColumns(columns_description.getNamesOfPhysical());
     }
     else
     {
-        columns_description = metadata_snapshot->getColumns();
-        block_for_format = metadata_snapshot->getSampleBlock();
+        columns_description = storage_snapshot->metadata->getColumns();
+        block_for_format = storage_snapshot->metadata->getSampleBlock();
     }
 
-    auto params = getReadURIParams(column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size);
+    auto params = getReadURIParams(column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size);
 
     auto uri_info = std::make_shared<StorageURLSource::URIInfo>();
     uri_info->uri_list_to_read.emplace_back(uri_options);
diff --git a/src/Storages/StorageURL.h b/src/Storages/StorageURL.h
index 79d2489f241a..a035b1bb93da 100644
--- a/src/Storages/StorageURL.h
+++ b/src/Storages/StorageURL.h
@@ -30,7 +30,7 @@ class IStorageURLBase : public IStorage
 public:
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -80,7 +80,7 @@ class IStorageURLBase : public IStorage
 
     virtual std::vector<std::pair<std::string, std::string>> getReadURIParams(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum & processed_stage,
@@ -97,7 +97,7 @@ class IStorageURLBase : public IStorage
     bool isColumnOriented() const override;
 
 private:
-    virtual Block getHeaderBlock(const Names & column_names, const StorageMetadataPtr & metadata_snapshot) const = 0;
+    virtual Block getHeaderBlock(const Names & column_names, const StorageSnapshotPtr & storage_snapshot) const = 0;
 };
 
 class StorageURLSink : public SinkToStorage
@@ -145,9 +145,9 @@ class StorageURL : public shared_ptr_helper<StorageURL>, public IStorageURLBase
         return "URL";
     }
 
-    Block getHeaderBlock(const Names & /*column_names*/, const StorageMetadataPtr & metadata_snapshot) const override
+    Block getHeaderBlock(const Names & /*column_names*/, const StorageSnapshotPtr & storage_snapshot) const override
     {
-        return metadata_snapshot->getSampleBlock();
+        return storage_snapshot->metadata->getSampleBlock();
     }
 
     static FormatSettings getFormatSettingsFromArgs(const StorageFactory::Arguments & args);
@@ -172,7 +172,7 @@ class StorageURLWithFailover final : public StorageURL
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageValues.cpp b/src/Storages/StorageValues.cpp
index 650782afbba0..2a3e1743983e 100644
--- a/src/Storages/StorageValues.cpp
+++ b/src/Storages/StorageValues.cpp
@@ -22,14 +22,14 @@ StorageValues::StorageValues(
 
 Pipe StorageValues::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t /*max_block_size*/,
     unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     /// Get only required columns.
     Block block;
diff --git a/src/Storages/StorageValues.h b/src/Storages/StorageValues.h
index 69b2f757046e..21156ec27cca 100644
--- a/src/Storages/StorageValues.h
+++ b/src/Storages/StorageValues.h
@@ -17,7 +17,7 @@ class StorageValues final : public shared_ptr_helper<StorageValues>, public ISto
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageView.cpp b/src/Storages/StorageView.cpp
index bcf7d7856cfa..68b16de5a803 100644
--- a/src/Storages/StorageView.cpp
+++ b/src/Storages/StorageView.cpp
@@ -107,7 +107,7 @@ StorageView::StorageView(
 
 Pipe StorageView::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum processed_stage,
@@ -115,7 +115,7 @@ Pipe StorageView::read(
     const unsigned num_streams)
 {
     QueryPlan plan;
-    read(plan, column_names, metadata_snapshot, query_info, context, processed_stage, max_block_size, num_streams);
+    read(plan, column_names, storage_snapshot, query_info, context, processed_stage, max_block_size, num_streams);
     return plan.convertToPipe(
         QueryPlanOptimizationSettings::fromContext(context),
         BuildQueryPipelineSettings::fromContext(context));
@@ -124,14 +124,14 @@ Pipe StorageView::read(
 void StorageView::read(
         QueryPlan & query_plan,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum /*processed_stage*/,
         const size_t /*max_block_size*/,
         const unsigned /*num_streams*/)
 {
-    ASTPtr current_inner_query = metadata_snapshot->getSelectQuery().inner_query;
+    ASTPtr current_inner_query = storage_snapshot->metadata->getSelectQuery().inner_query;
 
     if (query_info.view_query)
     {
@@ -154,7 +154,7 @@ void StorageView::read(
     query_plan.addStep(std::move(materializing));
 
     /// And also convert to expected structure.
-    const auto & expected_header = metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID());
+    const auto & expected_header = storage_snapshot->getSampleBlockForColumns(column_names);
     const auto & header = query_plan.getCurrentDataStream().header;
 
     const auto * select_with_union = current_inner_query->as<ASTSelectWithUnionQuery>();
diff --git a/src/Storages/StorageView.h b/src/Storages/StorageView.h
index cd36a10aae7d..f49736afe4a4 100644
--- a/src/Storages/StorageView.h
+++ b/src/Storages/StorageView.h
@@ -23,7 +23,7 @@ class StorageView final : public shared_ptr_helper<StorageView>, public IStorage
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -33,7 +33,7 @@ class StorageView final : public shared_ptr_helper<StorageView>, public IStorage
     void read(
         QueryPlan & query_plan,
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/StorageXDBC.cpp b/src/Storages/StorageXDBC.cpp
index 3cb6c9d03599..d9a2a77515e1 100644
--- a/src/Storages/StorageXDBC.cpp
+++ b/src/Storages/StorageXDBC.cpp
@@ -57,7 +57,7 @@ std::string StorageXDBC::getReadMethod() const
 
 std::vector<std::pair<std::string, std::string>> StorageXDBC::getReadURIParams(
     const Names & /* column_names */,
-    const StorageMetadataPtr & /* metadata_snapshot */,
+    const StorageSnapshotPtr & /*storage_snapshot*/,
     const SelectQueryInfo & /*query_info*/,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum & /*processed_stage*/,
@@ -101,17 +101,17 @@ std::function<void(std::ostream &)> StorageXDBC::getReadPOSTDataCallback(
 
 Pipe StorageXDBC::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr local_context,
     QueryProcessingStage::Enum processed_stage,
     size_t max_block_size,
     unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     bridge_helper->startBridgeSync();
-    return IStorageURLBase::read(column_names, metadata_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
+    return IStorageURLBase::read(column_names, storage_snapshot, query_info, local_context, processed_stage, max_block_size, num_streams);
 }
 
 SinkToStoragePtr StorageXDBC::write(const ASTPtr & /* query */, const StorageMetadataPtr & metadata_snapshot, ContextPtr local_context)
@@ -145,9 +145,9 @@ bool StorageXDBC::isColumnOriented() const
     return true;
 }
 
-Block StorageXDBC::getHeaderBlock(const Names & column_names, const StorageMetadataPtr & metadata_snapshot) const
+Block StorageXDBC::getHeaderBlock(const Names & column_names, const StorageSnapshotPtr & storage_snapshot) const
 {
-    return metadata_snapshot->getSampleBlockForColumns(column_names, getVirtuals(), getStorageID());
+    return storage_snapshot->getSampleBlockForColumns(column_names);
 }
 
 std::string StorageXDBC::getName() const
diff --git a/src/Storages/StorageXDBC.h b/src/Storages/StorageXDBC.h
index d8771c4ed834..514171026fc5 100644
--- a/src/Storages/StorageXDBC.h
+++ b/src/Storages/StorageXDBC.h
@@ -21,7 +21,7 @@ class StorageXDBC : public IStorageURLBase
 public:
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -51,7 +51,7 @@ class StorageXDBC : public IStorageURLBase
 
     std::vector<std::pair<std::string, std::string>> getReadURIParams(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         const SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum & processed_stage,
@@ -65,7 +65,7 @@ class StorageXDBC : public IStorageURLBase
         QueryProcessingStage::Enum & processed_stage,
         size_t max_block_size) const override;
 
-    Block getHeaderBlock(const Names & column_names, const StorageMetadataPtr & metadata_snapshot) const override;
+    Block getHeaderBlock(const Names & column_names, const StorageSnapshotPtr & storage_snapshot) const override;
 
     bool isColumnOriented() const override;
 };
diff --git a/src/Storages/System/IStorageSystemOneBlock.h b/src/Storages/System/IStorageSystemOneBlock.h
index d78c8179a711..7cc94a3f2f6d 100644
--- a/src/Storages/System/IStorageSystemOneBlock.h
+++ b/src/Storages/System/IStorageSystemOneBlock.h
@@ -41,17 +41,16 @@ class IStorageSystemOneBlock : public IStorage
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum /*processed_stage*/,
         size_t /*max_block_size*/,
         unsigned /*num_streams*/) override
     {
-        auto virtuals_names_and_types = getVirtuals();
-        metadata_snapshot->check(column_names, virtuals_names_and_types, getStorageID());
+        storage_snapshot->check(column_names);
 
-        Block sample_block = metadata_snapshot->getSampleBlockWithVirtuals(virtuals_names_and_types);
+        Block sample_block = storage_snapshot->metadata->getSampleBlockWithVirtuals(getVirtuals());
         MutableColumns res_columns = sample_block.cloneEmptyColumns();
         fillData(res_columns, context, query_info);
 
diff --git a/src/Storages/System/StorageSystemColumns.cpp b/src/Storages/System/StorageSystemColumns.cpp
index d847c00846cd..082b46f5a7e8 100644
--- a/src/Storages/System/StorageSystemColumns.cpp
+++ b/src/Storages/System/StorageSystemColumns.cpp
@@ -304,20 +304,20 @@ class ColumnsSource : public SourceWithProgress
 
 Pipe StorageSystemColumns::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t max_block_size,
     const unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     /// Create a mask of what columns are needed in the result.
 
     NameSet names_set(column_names.begin(), column_names.end());
 
-    Block sample_block = metadata_snapshot->getSampleBlock();
+    Block sample_block = storage_snapshot->metadata->getSampleBlock();
     Block header;
 
     std::vector<UInt8> columns_mask(sample_block.columns());
diff --git a/src/Storages/System/StorageSystemColumns.h b/src/Storages/System/StorageSystemColumns.h
index dc184b1ae427..126deef19212 100644
--- a/src/Storages/System/StorageSystemColumns.h
+++ b/src/Storages/System/StorageSystemColumns.h
@@ -19,7 +19,7 @@ class StorageSystemColumns final : public shared_ptr_helper<StorageSystemColumns
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemDataSkippingIndices.cpp b/src/Storages/System/StorageSystemDataSkippingIndices.cpp
index d7fc06da9534..42b214bf1015 100644
--- a/src/Storages/System/StorageSystemDataSkippingIndices.cpp
+++ b/src/Storages/System/StorageSystemDataSkippingIndices.cpp
@@ -165,18 +165,18 @@ class DataSkippingIndicesSource : public SourceWithProgress
 
 Pipe StorageSystemDataSkippingIndices::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum /* processed_stage */,
     size_t max_block_size,
     unsigned int /* num_streams */)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     NameSet names_set(column_names.begin(), column_names.end());
 
-    Block sample_block = metadata_snapshot->getSampleBlock();
+    Block sample_block = storage_snapshot->metadata->getSampleBlock();
     Block header;
 
     std::vector<UInt8> columns_mask(sample_block.columns());
diff --git a/src/Storages/System/StorageSystemDataSkippingIndices.h b/src/Storages/System/StorageSystemDataSkippingIndices.h
index 4af2398a04bd..93511d0d591a 100644
--- a/src/Storages/System/StorageSystemDataSkippingIndices.h
+++ b/src/Storages/System/StorageSystemDataSkippingIndices.h
@@ -16,7 +16,7 @@ class StorageSystemDataSkippingIndices : public shared_ptr_helper<StorageSystemD
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemDetachedParts.cpp b/src/Storages/System/StorageSystemDetachedParts.cpp
index 5a24809d05a2..4797dff2fd19 100644
--- a/src/Storages/System/StorageSystemDetachedParts.cpp
+++ b/src/Storages/System/StorageSystemDetachedParts.cpp
@@ -31,7 +31,7 @@ StorageSystemDetachedParts::StorageSystemDetachedParts(const StorageID & table_i
 
 Pipe StorageSystemDetachedParts::read(
     const Names & /* column_names */,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
@@ -41,7 +41,7 @@ Pipe StorageSystemDetachedParts::read(
     StoragesInfoStream stream(query_info, context);
 
     /// Create the result.
-    Block block = metadata_snapshot->getSampleBlock();
+    Block block = storage_snapshot->metadata->getSampleBlock();
     MutableColumns new_columns = block.cloneEmptyColumns();
 
     while (StoragesInfo info = stream.next())
diff --git a/src/Storages/System/StorageSystemDetachedParts.h b/src/Storages/System/StorageSystemDetachedParts.h
index 51ee93a2f155..8ed11eb306cf 100644
--- a/src/Storages/System/StorageSystemDetachedParts.h
+++ b/src/Storages/System/StorageSystemDetachedParts.h
@@ -25,7 +25,7 @@ class StorageSystemDetachedParts final :
 
     Pipe read(
             const Names & /* column_names */,
-            const StorageMetadataPtr & metadata_snapshot,
+            const StorageSnapshotPtr & storage_snapshot,
             SelectQueryInfo & query_info,
             ContextPtr context,
             QueryProcessingStage::Enum /*processed_stage*/,
diff --git a/src/Storages/System/StorageSystemDisks.cpp b/src/Storages/System/StorageSystemDisks.cpp
index 749717922da3..3841abc2f2dd 100644
--- a/src/Storages/System/StorageSystemDisks.cpp
+++ b/src/Storages/System/StorageSystemDisks.cpp
@@ -28,14 +28,14 @@ StorageSystemDisks::StorageSystemDisks(const StorageID & table_id_)
 
 Pipe StorageSystemDisks::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t /*max_block_size*/,
     const unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     MutableColumnPtr col_name = ColumnString::create();
     MutableColumnPtr col_path = ColumnString::create();
@@ -65,7 +65,7 @@ Pipe StorageSystemDisks::read(
     UInt64 num_rows = res_columns.at(0)->size();
     Chunk chunk(std::move(res_columns), num_rows);
 
-    return Pipe(std::make_shared<SourceFromSingleChunk>(metadata_snapshot->getSampleBlock(), std::move(chunk)));
+    return Pipe(std::make_shared<SourceFromSingleChunk>(storage_snapshot->metadata->getSampleBlock(), std::move(chunk)));
 }
 
 }
diff --git a/src/Storages/System/StorageSystemDisks.h b/src/Storages/System/StorageSystemDisks.h
index 1404d6023d40..2640ab7149b0 100644
--- a/src/Storages/System/StorageSystemDisks.h
+++ b/src/Storages/System/StorageSystemDisks.h
@@ -22,7 +22,7 @@ class StorageSystemDisks final : public shared_ptr_helper<StorageSystemDisks>, p
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemNumbers.cpp b/src/Storages/System/StorageSystemNumbers.cpp
index c09279e65acd..2e48bb857ce2 100644
--- a/src/Storages/System/StorageSystemNumbers.cpp
+++ b/src/Storages/System/StorageSystemNumbers.cpp
@@ -124,14 +124,14 @@ StorageSystemNumbers::StorageSystemNumbers(const StorageID & table_id, bool mult
 
 Pipe StorageSystemNumbers::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo &,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t max_block_size,
     unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     if (limit && *limit < max_block_size)
     {
diff --git a/src/Storages/System/StorageSystemNumbers.h b/src/Storages/System/StorageSystemNumbers.h
index 32105bb055dc..5f3a12c530d6 100644
--- a/src/Storages/System/StorageSystemNumbers.h
+++ b/src/Storages/System/StorageSystemNumbers.h
@@ -31,7 +31,7 @@ class StorageSystemNumbers final : public shared_ptr_helper<StorageSystemNumbers
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemOne.cpp b/src/Storages/System/StorageSystemOne.cpp
index 7558ae0ae925..f262c981b832 100644
--- a/src/Storages/System/StorageSystemOne.cpp
+++ b/src/Storages/System/StorageSystemOne.cpp
@@ -22,14 +22,14 @@ StorageSystemOne::StorageSystemOne(const StorageID & table_id_)
 
 Pipe StorageSystemOne::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo &,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t /*max_block_size*/,
     const unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     Block header{ColumnWithTypeAndName(
             DataTypeUInt8().createColumn(),
diff --git a/src/Storages/System/StorageSystemOne.h b/src/Storages/System/StorageSystemOne.h
index cc1d5e05b75d..b0ca389b76f9 100644
--- a/src/Storages/System/StorageSystemOne.h
+++ b/src/Storages/System/StorageSystemOne.h
@@ -23,7 +23,7 @@ class StorageSystemOne final : public shared_ptr_helper<StorageSystemOne>, publi
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemPartsBase.cpp b/src/Storages/System/StorageSystemPartsBase.cpp
index f4dd9cbd45de..dc2a353de27b 100644
--- a/src/Storages/System/StorageSystemPartsBase.cpp
+++ b/src/Storages/System/StorageSystemPartsBase.cpp
@@ -26,7 +26,7 @@ namespace ErrorCodes
     extern const int TABLE_IS_DROPPED;
 }
 
-bool StorageSystemPartsBase::hasStateColumn(const Names & column_names, const StorageMetadataPtr & metadata_snapshot) const
+bool StorageSystemPartsBase::hasStateColumn(const Names & column_names, const StorageSnapshotPtr & storage_snapshot) const
 {
     bool has_state_column = false;
     Names real_column_names;
@@ -41,7 +41,7 @@ bool StorageSystemPartsBase::hasStateColumn(const Names & column_names, const St
 
     /// Do not check if only _state column is requested
     if (!(has_state_column && real_column_names.empty()))
-        metadata_snapshot->check(real_column_names, {}, getStorageID());
+        storage_snapshot->check(real_column_names);
 
     return has_state_column;
 }
@@ -235,14 +235,14 @@ StoragesInfo StoragesInfoStream::next()
 
 Pipe StorageSystemPartsBase::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t /*max_block_size*/,
     const unsigned /*num_streams*/)
 {
-    bool has_state_column = hasStateColumn(column_names, metadata_snapshot);
+    bool has_state_column = hasStateColumn(column_names, storage_snapshot);
 
     StoragesInfoStream stream(query_info, context);
 
@@ -250,7 +250,7 @@ Pipe StorageSystemPartsBase::read(
 
     NameSet names_set(column_names.begin(), column_names.end());
 
-    Block sample = metadata_snapshot->getSampleBlock();
+    Block sample = storage_snapshot->metadata->getSampleBlock();
     Block header;
 
     std::vector<UInt8> columns_mask(sample.columns());
diff --git a/src/Storages/System/StorageSystemPartsBase.h b/src/Storages/System/StorageSystemPartsBase.h
index bf19771c9403..39b6d0f033aa 100644
--- a/src/Storages/System/StorageSystemPartsBase.h
+++ b/src/Storages/System/StorageSystemPartsBase.h
@@ -58,7 +58,7 @@ class StorageSystemPartsBase : public IStorage
 public:
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & metadata_snapshot,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
@@ -70,7 +70,7 @@ class StorageSystemPartsBase : public IStorage
     bool isSystemStorage() const override { return true; }
 
 private:
-    bool hasStateColumn(const Names & column_names, const StorageMetadataPtr & metadata_snapshot) const;
+    bool hasStateColumn(const Names & column_names, const StorageSnapshotPtr & storage_snapshot) const;
 
 protected:
     const FormatSettings format_settings;
diff --git a/src/Storages/System/StorageSystemReplicas.cpp b/src/Storages/System/StorageSystemReplicas.cpp
index 467226c3b7a7..e018ccc0733b 100644
--- a/src/Storages/System/StorageSystemReplicas.cpp
+++ b/src/Storages/System/StorageSystemReplicas.cpp
@@ -61,14 +61,14 @@ StorageSystemReplicas::StorageSystemReplicas(const StorageID & table_id_)
 
 Pipe StorageSystemReplicas::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t /*max_block_size*/,
     const unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     const auto access = context->getAccess();
     const bool check_access_for_databases = !access->isGranted(AccessType::SHOW_TABLES);
@@ -149,7 +149,7 @@ Pipe StorageSystemReplicas::read(
         col_engine = filtered_block.getByName("engine").column;
     }
 
-    MutableColumns res_columns = metadata_snapshot->getSampleBlock().cloneEmptyColumns();
+    MutableColumns res_columns = storage_snapshot->metadata->getSampleBlock().cloneEmptyColumns();
 
     for (size_t i = 0, size = col_database->size(); i < size; ++i)
     {
@@ -203,8 +203,6 @@ Pipe StorageSystemReplicas::read(
         res_columns[col_num++]->insert(std::move(replica_is_active_values));
     }
 
-    Block header = metadata_snapshot->getSampleBlock();
-
     Columns fin_columns;
     fin_columns.reserve(res_columns.size());
 
@@ -218,7 +216,7 @@ Pipe StorageSystemReplicas::read(
     UInt64 num_rows = fin_columns.at(0)->size();
     Chunk chunk(std::move(fin_columns), num_rows);
 
-    return Pipe(std::make_shared<SourceFromSingleChunk>(metadata_snapshot->getSampleBlock(), std::move(chunk)));
+    return Pipe(std::make_shared<SourceFromSingleChunk>(storage_snapshot->metadata->getSampleBlock(), std::move(chunk)));
 }
 
 
diff --git a/src/Storages/System/StorageSystemReplicas.h b/src/Storages/System/StorageSystemReplicas.h
index 500b4e975465..1b93d10367bf 100644
--- a/src/Storages/System/StorageSystemReplicas.h
+++ b/src/Storages/System/StorageSystemReplicas.h
@@ -20,7 +20,7 @@ class StorageSystemReplicas final : public shared_ptr_helper<StorageSystemReplic
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemStoragePolicies.cpp b/src/Storages/System/StorageSystemStoragePolicies.cpp
index 036e4748e651..04c98e6be9c8 100644
--- a/src/Storages/System/StorageSystemStoragePolicies.cpp
+++ b/src/Storages/System/StorageSystemStoragePolicies.cpp
@@ -38,14 +38,14 @@ StorageSystemStoragePolicies::StorageSystemStoragePolicies(const StorageID & tab
 
 Pipe StorageSystemStoragePolicies::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & /*query_info*/,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t /*max_block_size*/,
     const unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     MutableColumnPtr col_policy_name = ColumnString::create();
     MutableColumnPtr col_volume_name = ColumnString::create();
@@ -89,7 +89,7 @@ Pipe StorageSystemStoragePolicies::read(
     UInt64 num_rows = res_columns.at(0)->size();
     Chunk chunk(std::move(res_columns), num_rows);
 
-    return Pipe(std::make_shared<SourceFromSingleChunk>(metadata_snapshot->getSampleBlock(), std::move(chunk)));
+    return Pipe(std::make_shared<SourceFromSingleChunk>(storage_snapshot->metadata->getSampleBlock(), std::move(chunk)));
 }
 
 }
diff --git a/src/Storages/System/StorageSystemStoragePolicies.h b/src/Storages/System/StorageSystemStoragePolicies.h
index 28730ce33c40..e2890c42897f 100644
--- a/src/Storages/System/StorageSystemStoragePolicies.h
+++ b/src/Storages/System/StorageSystemStoragePolicies.h
@@ -22,7 +22,7 @@ class StorageSystemStoragePolicies final : public shared_ptr_helper<StorageSyste
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemTables.cpp b/src/Storages/System/StorageSystemTables.cpp
index 9332bc6a004e..98a07d0f4c34 100644
--- a/src/Storages/System/StorageSystemTables.cpp
+++ b/src/Storages/System/StorageSystemTables.cpp
@@ -556,20 +556,20 @@ class TablesBlockSource : public SourceWithProgress
 
 Pipe StorageSystemTables::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo & query_info,
     ContextPtr context,
     QueryProcessingStage::Enum /*processed_stage*/,
     const size_t max_block_size,
     const unsigned /*num_streams*/)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     /// Create a mask of what columns are needed in the result.
 
     NameSet names_set(column_names.begin(), column_names.end());
 
-    Block sample_block = metadata_snapshot->getSampleBlock();
+    Block sample_block = storage_snapshot->metadata->getSampleBlock();
     Block res_block;
 
     std::vector<UInt8> columns_mask(sample_block.columns());
diff --git a/src/Storages/System/StorageSystemTables.h b/src/Storages/System/StorageSystemTables.h
index 23f3aedb1648..7f6a099a824d 100644
--- a/src/Storages/System/StorageSystemTables.h
+++ b/src/Storages/System/StorageSystemTables.h
@@ -20,7 +20,7 @@ class StorageSystemTables final : public shared_ptr_helper<StorageSystemTables>,
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/System/StorageSystemZeros.cpp b/src/Storages/System/StorageSystemZeros.cpp
index 624fc54998ce..b6a623c30713 100644
--- a/src/Storages/System/StorageSystemZeros.cpp
+++ b/src/Storages/System/StorageSystemZeros.cpp
@@ -92,14 +92,14 @@ StorageSystemZeros::StorageSystemZeros(const StorageID & table_id_, bool multith
 
 Pipe StorageSystemZeros::read(
     const Names & column_names,
-    const StorageMetadataPtr & metadata_snapshot,
+    const StorageSnapshotPtr & storage_snapshot,
     SelectQueryInfo &,
     ContextPtr /*context*/,
     QueryProcessingStage::Enum /*processed_stage*/,
     size_t max_block_size,
     unsigned num_streams)
 {
-    metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
+    storage_snapshot->check(column_names);
 
     bool use_multiple_streams = multithreaded;
 
diff --git a/src/Storages/System/StorageSystemZeros.h b/src/Storages/System/StorageSystemZeros.h
index f5b2bb431176..bf72352b7be6 100644
--- a/src/Storages/System/StorageSystemZeros.h
+++ b/src/Storages/System/StorageSystemZeros.h
@@ -22,7 +22,7 @@ class StorageSystemZeros final : public shared_ptr_helper<StorageSystemZeros>, p
 
     Pipe read(
         const Names & column_names,
-        const StorageMetadataPtr & /*metadata_snapshot*/,
+        const StorageSnapshotPtr & storage_snapshot,
         SelectQueryInfo & query_info,
         ContextPtr context,
         QueryProcessingStage::Enum processed_stage,
diff --git a/src/Storages/getStructureOfRemoteTable.cpp b/src/Storages/getStructureOfRemoteTable.cpp
index 532abb8e2f36..8fa4d02e8e14 100644
--- a/src/Storages/getStructureOfRemoteTable.cpp
+++ b/src/Storages/getStructureOfRemoteTable.cpp
@@ -58,7 +58,6 @@ ColumnsDescription getStructureOfRemoteTableInShard(
     }
 
     ColumnsDescription res;
-
     auto new_context = ClusterProxy::updateSettingsForCluster(cluster, context, context->getSettingsRef());
 
     /// Expect only needed columns from the result of DESC TABLE. NOTE 'comment' column is ignored for compatibility reasons.
@@ -150,4 +149,69 @@ ColumnsDescription getStructureOfRemoteTable(
         ErrorCodes::NO_REMOTE_SHARD_AVAILABLE);
 }
 
+ColumnsDescriptionByShardNum getExtendedObjectsOfRemoteTables(
+    const Cluster & cluster,
+    const StorageID & remote_table_id,
+    const ColumnsDescription & storage_columns,
+    ContextPtr context)
+{
+    const auto & shards_info = cluster.getShardsInfo();
+    auto query = "DESC TABLE " + remote_table_id.getFullTableName();
+
+    auto new_context = ClusterProxy::updateSettingsForCluster(cluster, context, context->getSettingsRef());
+    new_context->setSetting("describe_extend_object_types", true);
+
+    /// Expect only needed columns from the result of DESC TABLE.
+    Block sample_block
+    {
+        { ColumnString::create(), std::make_shared<DataTypeString>(), "name" },
+        { ColumnString::create(), std::make_shared<DataTypeString>(), "type" },
+    };
+
+    auto execute_query_on_shard = [&](const auto & shard_info)
+    {
+        /// Execute remote query without restrictions (because it's not real user query, but part of implementation)
+        RemoteQueryExecutor executor(shard_info.pool, query, sample_block, new_context);
+
+        executor.setPoolMode(PoolMode::GET_ONE);
+        executor.setMainTable(remote_table_id);
+
+        ColumnsDescription res;
+        while (auto block = executor.read())
+        {
+            const auto & name_col = *block.getByName("name").column;
+            const auto & type_col = *block.getByName("type").column;
+
+            size_t size = name_col.size();
+            for (size_t i = 0; i < size; ++i)
+            {
+                auto name = get<const String &>(name_col[i]);
+                auto type_name = get<const String &>(type_col[i]);
+
+                auto storage_column = storage_columns.tryGetPhysical(name);
+                if (storage_column && isObject(storage_column->type))
+                    res.add(ColumnDescription(std::move(name), DataTypeFactory::instance().get(type_name)));
+            }
+        }
+
+        return res;
+    };
+
+    ColumnsDescriptionByShardNum columns;
+    for (const auto & shard_info : shards_info)
+    {
+        auto res = execute_query_on_shard(shard_info);
+
+        /// Expect at least some columns.
+        /// This is a hack to handle the empty block case returned by Connection when skip_unavailable_shards is set.
+        if (!res.empty())
+            columns.emplace(shard_info.shard_num, std::move(res));
+    }
+
+    if (columns.empty())
+        throw NetException("All attempts to get table structure failed", ErrorCodes::NO_REMOTE_SHARD_AVAILABLE);
+
+    return columns;
+}
+
 }
diff --git a/src/Storages/getStructureOfRemoteTable.h b/src/Storages/getStructureOfRemoteTable.h
index 3f77236c756f..62f93dccf1ab 100644
--- a/src/Storages/getStructureOfRemoteTable.h
+++ b/src/Storages/getStructureOfRemoteTable.h
@@ -8,6 +8,7 @@
 
 namespace DB
 {
+
 class Context;
 struct StorageID;
 
@@ -19,4 +20,14 @@ ColumnsDescription getStructureOfRemoteTable(
     ContextPtr context,
     const ASTPtr & table_func_ptr = nullptr);
 
+
+using ColumnsDescriptionByShardNum = std::unordered_map<UInt32, ColumnsDescription>;
+
+/// Returns descriptions of columns of type Object for each shard.
+ColumnsDescriptionByShardNum getExtendedObjectsOfRemoteTables(
+    const Cluster & cluster,
+    const StorageID & remote_table_id,
+    const ColumnsDescription & storage_columns,
+    ContextPtr context);
+
 }
diff --git a/src/TableFunctions/TableFunctionValues.cpp b/src/TableFunctions/TableFunctionValues.cpp
index 07019d260672..595e8f9cf417 100644
--- a/src/TableFunctions/TableFunctionValues.cpp
+++ b/src/TableFunctions/TableFunctionValues.cpp
@@ -109,7 +109,7 @@ void TableFunctionValues::parseArguments(const ASTPtr & ast_function, ContextPtr
                 "Cannot determine common structure for {} function arguments: the amount of columns is differ for different arguments",
                 getName());
         for (size_t j = 0; j != arg_types.size(); ++j)
-            data_types[j] = getLeastSupertype({data_types[j], arg_types[j]});
+            data_types[j] = getLeastSupertype(DataTypes{data_types[j], arg_types[j]});
     }
 
     NamesAndTypesList names_and_types;
