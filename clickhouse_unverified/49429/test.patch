diff --git a/tests/integration/test_backup_restore/test.py b/tests/integration/test_backup_restore/test.py
index 193e638186ca..b91a7cfe1c83 100644
--- a/tests/integration/test_backup_restore/test.py
+++ b/tests/integration/test_backup_restore/test.py
@@ -171,39 +171,3 @@ def test_replace_partition(started_cluster):
     assert TSV(res) == expected
 
     instance.query("DROP TABLE IF EXISTS test.tbl3")
-
-
-def test_freeze_in_memory(started_cluster):
-    instance.query(
-        "CREATE TABLE test.t_in_memory(a UInt32, s String) ENGINE = MergeTree ORDER BY a SETTINGS min_rows_for_compact_part = 1000"
-    )
-    instance.query("INSERT INTO test.t_in_memory VALUES (1, 'a')")
-    instance.query("ALTER TABLE test.t_in_memory FREEZE")
-
-    fp_backup = get_last_backup_path(
-        started_cluster.instances["node"], "test", "t_in_memory"
-    )
-    part_path = fp_backup + "/all_1_1_0/"
-
-    assert TSV(
-        instance.query(
-            "SELECT part_type, is_frozen FROM system.parts WHERE database = 'test' AND table = 't_in_memory'"
-        )
-    ) == TSV("InMemory\t1
")
-    instance.exec_in_container(["test", "-f", part_path + "/data.bin"])
-    assert instance.exec_in_container(["cat", part_path + "/count.txt"]).strip() == "1"
-
-    instance.query(
-        "CREATE TABLE test.t_in_memory_2(a UInt32, s String) ENGINE = MergeTree ORDER BY a"
-    )
-    copy_backup_to_detached(
-        started_cluster.instances["node"], "test", "t_in_memory", "t_in_memory_2"
-    )
-
-    instance.query("ALTER TABLE test.t_in_memory_2 ATTACH PARTITION ID 'all'")
-    assert TSV(
-        instance.query(
-            "SELECT part_type FROM system.parts WHERE database = 'test' AND table = 't_in_memory_2'"
-        )
-    ) == TSV("Compact
")
-    assert TSV(instance.query("SELECT a, s FROM test.t_in_memory_2")) == TSV("1\ta
")
diff --git a/tests/integration/test_backward_compatibility/test_in_memory_parts_still_read.py b/tests/integration/test_backward_compatibility/test_in_memory_parts_still_read.py
new file mode 100644
index 000000000000..d55f155918e6
--- /dev/null
+++ b/tests/integration/test_backward_compatibility/test_in_memory_parts_still_read.py
@@ -0,0 +1,44 @@
+import pytest
+
+from helpers.cluster import ClickHouseCluster
+
+
+cluster = ClickHouseCluster(__file__)
+
+# Version 23.4 is the latest version to support writing in-memory parts.
+node = cluster.add_instance(
+    "node_old",
+    image="clickhouse/clickhouse-server",
+    tag="23.4",
+    stay_alive=True,
+    with_installed_binary=True,
+)
+
+
+@pytest.fixture(scope="module")
+def start_cluster():
+    try:
+        cluster.start()
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+
+def test_in_memory_parts_still_read(start_cluster):
+    node.query(
+        "CREATE TABLE t (x UInt64, s String, a Array(Tuple(Map(String, LowCardinality(String)), Date32, DateTime64(3)))) ENGINE = MergeTree ORDER BY s SETTINGS min_rows_for_compact_part = 1000000, min_bytes_for_compact_part = '1G', in_memory_parts_enable_wal = 1"
+    )
+    node.query("INSERT INTO t SELECT * FROM generateRandom() LIMIT 100")
+
+    assert node.query("SELECT count() FROM t WHERE NOT ignore(*)") == "100
"
+
+    node.restart_with_latest_version()
+    assert node.query("SELECT count() FROM t WHERE NOT ignore(*)") == "100
"
+
+    node.query("INSERT INTO t SELECT * FROM generateRandom() LIMIT 100")
+
+    assert node.query("SELECT count() FROM t WHERE NOT ignore(*)") == "200
"
+
+    node.restart_with_original_version()
+    assert node.query("SELECT count() FROM t WHERE NOT ignore(*)") == "200
"
diff --git a/tests/integration/test_part_uuid/test.py b/tests/integration/test_part_uuid/test.py
index b30dd884427f..b1eb1280d56c 100644
--- a/tests/integration/test_part_uuid/test.py
+++ b/tests/integration/test_part_uuid/test.py
@@ -108,48 +108,3 @@ def test_part_uuid(started_cluster):
         uuids.add(part_merge_uuid)
         assert part_mutate_uuid not in [uuid_zero, part_merge_uuid]
     assert len(uuids) == 1, "expect the same uuid on all the replicas"
-
-
-def test_part_uuid_wal(started_cluster):
-    uuid_zero = uuid.UUID(bytes=b"\x00" * 16)
-
-    for ix, n in enumerate([node1, node2]):
-        n.query(
-            """
-        CREATE TABLE t_wal(key UInt64, value UInt64)
-        ENGINE ReplicatedMergeTree('/clickhouse/tables/t_wal', '{}')
-        ORDER BY tuple()
-        """.format(
-                ix
-            )
-        )
-
-    node2.query("INSERT INTO t_wal VALUES (1, 1)")
-
-    uuids = set()
-    for node in [node1, node2]:
-        node.query("SYSTEM SYNC REPLICA t_wal")
-        part_initial_uuid = uuid.UUID(
-            node.query(
-                "SELECT uuid FROM system.parts WHERE table = 't_wal' AND active ORDER BY name"
-            ).strip()
-        )
-        assert (
-            "InMemory"
-            == node.query(
-                "SELECT part_type FROM system.parts WHERE table = 't_wal' AND active ORDER BY name"
-            ).strip()
-        )
-        uuids.add(part_initial_uuid)
-        assert uuid_zero != part_initial_uuid
-    assert len(uuids) == 1, "expect the same uuid on all the replicas"
-
-    # Test detach / attach table to trigger WAL processing.
-    for node in [node1, node2]:
-        node.query("DETACH TABLE t_wal; ATTACH TABLE t_wal")
-        part_reattach_uuid = uuid.UUID(
-            node.query(
-                "SELECT uuid FROM system.parts WHERE table = 't_wal' AND active ORDER BY name"
-            ).strip()
-        )
-        assert part_initial_uuid == part_reattach_uuid
diff --git a/tests/integration/test_polymorphic_parts/test.py b/tests/integration/test_polymorphic_parts/test.py
index 361b48557475..fb1f363b8253 100644
--- a/tests/integration/test_polymorphic_parts/test.py
+++ b/tests/integration/test_polymorphic_parts/test.py
@@ -49,9 +49,8 @@ def create_tables(name, nodes, node_settings, shard):
             PARTITION BY toYYYYMM(date)
             ORDER BY id
             SETTINGS index_granularity = 64, index_granularity_bytes = {index_granularity_bytes},
-            min_rows_for_wide_part = {min_rows_for_wide_part}, min_rows_for_compact_part = {min_rows_for_compact_part},
-            min_bytes_for_wide_part = 0, min_bytes_for_compact_part = 0,
-            in_memory_parts_enable_wal = 1
+            min_rows_for_wide_part = {min_rows_for_wide_part},
+            min_bytes_for_wide_part = 0
             """.format(
                 name=name, shard=shard, repl=i, **settings
             )
@@ -87,17 +86,14 @@ def create_tables_old_format(name, nodes, shard):
 settings_default = {
     "index_granularity_bytes": 10485760,
     "min_rows_for_wide_part": 512,
-    "min_rows_for_compact_part": 0,
 }
 settings_compact_only = {
     "index_granularity_bytes": 10485760,
     "min_rows_for_wide_part": 1000000,
-    "min_rows_for_compact_part": 0,
 }
 settings_not_adaptive = {
     "index_granularity_bytes": 0,
     "min_rows_for_wide_part": 512,
-    "min_rows_for_compact_part": 0,
 }
 
 node3 = cluster.add_instance(
@@ -116,12 +112,10 @@ def create_tables_old_format(name, nodes, shard):
 settings_compact = {
     "index_granularity_bytes": 10485760,
     "min_rows_for_wide_part": 512,
-    "min_rows_for_compact_part": 0,
 }
 settings_wide = {
     "index_granularity_bytes": 10485760,
     "min_rows_for_wide_part": 0,
-    "min_rows_for_compact_part": 0,
 }
 
 node5 = cluster.add_instance(
@@ -131,12 +125,6 @@ def create_tables_old_format(name, nodes, shard):
     "node6", main_configs=["configs/compact_parts.xml"], with_zookeeper=True
 )
 
-settings_in_memory = {
-    "index_granularity_bytes": 10485760,
-    "min_rows_for_wide_part": 512,
-    "min_rows_for_compact_part": 256,
-}
-
 node9 = cluster.add_instance("node9", with_zookeeper=True, stay_alive=True)
 node10 = cluster.add_instance("node10", with_zookeeper=True)
 
@@ -190,42 +178,6 @@ def start_cluster():
             "shard2",
         )
         create_tables_old_format("polymorphic_table", [node5, node6], "shard3")
-        create_tables(
-            "in_memory_table",
-            [node9, node10],
-            [settings_in_memory, settings_in_memory],
-            "shard4",
-        )
-        create_tables(
-            "wal_table",
-            [node11, node12],
-            [settings_in_memory, settings_in_memory],
-            "shard4",
-        )
-        create_tables(
-            "restore_table",
-            [node11, node12],
-            [settings_in_memory, settings_in_memory],
-            "shard5",
-        )
-        create_tables(
-            "deduplication_table",
-            [node9, node10],
-            [settings_in_memory, settings_in_memory],
-            "shard5",
-        )
-        create_tables(
-            "sync_table",
-            [node9, node10],
-            [settings_in_memory, settings_in_memory],
-            "shard5",
-        )
-        create_tables(
-            "alters_table",
-            [node9, node10],
-            [settings_in_memory, settings_in_memory],
-            "shard5",
-        )
 
         yield cluster
 
@@ -422,7 +374,6 @@ def test_different_part_types_on_replicas(start_cluster, table, part_type):
 settings8 = {
     "index_granularity_bytes": 10485760,
     "min_rows_for_wide_part": 512,
-    "min_rows_for_compact_part": 0,
 }
 
 
@@ -538,187 +489,6 @@ def test_polymorphic_parts_non_adaptive(start_cluster):
     )
 
 
-def test_in_memory(start_cluster):
-    node9.query("SYSTEM STOP MERGES")
-    node10.query("SYSTEM STOP MERGES")
-
-    for size in [200, 200, 300, 600]:
-        insert_random_data("in_memory_table", node9, size)
-    node10.query("SYSTEM SYNC REPLICA in_memory_table", timeout=20)
-
-    assert node9.query("SELECT count() FROM in_memory_table") == "1300
"
-    assert node10.query("SELECT count() FROM in_memory_table") == "1300
"
-
-    expected = "Compact\t1
InMemory\t2
Wide\t1
"
-
-    assert TSV(
-        node9.query(
-            "SELECT part_type, count() FROM system.parts "
-            "WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type"
-        )
-    ) == TSV(expected)
-    assert TSV(
-        node10.query(
-            "SELECT part_type, count() FROM system.parts "
-            "WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type"
-        )
-    ) == TSV(expected)
-
-    node9.query("SYSTEM START MERGES")
-    node10.query("SYSTEM START MERGES")
-
-    assert_eq_with_retry(
-        node9,
-        "OPTIMIZE TABLE in_memory_table FINAL SETTINGS optimize_throw_if_noop = 1",
-        "",
-    )
-    node10.query("SYSTEM SYNC REPLICA in_memory_table", timeout=20)
-
-    assert node9.query("SELECT count() FROM in_memory_table") == "1300
"
-    assert node10.query("SELECT count() FROM in_memory_table") == "1300
"
-
-    assert TSV(
-        node9.query(
-            "SELECT part_type, count() FROM system.parts "
-            "WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type"
-        )
-    ) == TSV("Wide\t1
")
-    assert TSV(
-        node10.query(
-            "SELECT part_type, count() FROM system.parts "
-            "WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type"
-        )
-    ) == TSV("Wide\t1
")
-
-
-def test_in_memory_wal_rotate(start_cluster):
-    # Write every part to single wal
-    node11.query(
-        "ALTER TABLE restore_table MODIFY SETTING write_ahead_log_max_bytes = 10"
-    )
-    for i in range(5):
-        insert_random_data("restore_table", node11, 50)
-
-    for i in range(5):
-        # Check file exists
-        node11.exec_in_container(
-            [
-                "bash",
-                "-c",
-                "test -f /var/lib/clickhouse/data/default/restore_table/wal_{0}_{0}.bin".format(
-                    i
-                ),
-            ]
-        )
-
-    for node in [node11, node12]:
-        node.query(
-            "ALTER TABLE restore_table MODIFY SETTING number_of_free_entries_in_pool_to_lower_max_size_of_merge = 0"
-        )
-        node.query(
-            "ALTER TABLE restore_table MODIFY SETTING max_bytes_to_merge_at_max_space_in_pool = 10000000"
-        )
-
-    assert_eq_with_retry(
-        node11,
-        "OPTIMIZE TABLE restore_table FINAL SETTINGS optimize_throw_if_noop = 1",
-        "",
-    )
-    # Restart to be sure, that clearing stale logs task was ran
-    node11.restart_clickhouse(kill=True)
-
-    for i in range(5):
-        # check file doesn't exist
-        node11.exec_in_container(
-            [
-                "bash",
-                "-c",
-                "test ! -e /var/lib/clickhouse/data/default/restore_table/wal_{0}_{0}.bin".format(
-                    i
-                ),
-            ]
-        )
-
-    # New wal file was created and ready to write part to it
-    # Check file exists
-    node11.exec_in_container(
-        ["bash", "-c", "test -f /var/lib/clickhouse/data/default/restore_table/wal.bin"]
-    )
-    # Chech file empty
-    node11.exec_in_container(
-        [
-            "bash",
-            "-c",
-            "test ! -s /var/lib/clickhouse/data/default/restore_table/wal.bin",
-        ]
-    )
-
-
-def test_in_memory_deduplication(start_cluster):
-    for i in range(3):
-        # table can be in readonly node
-        exec_query_with_retry(
-            node9,
-            "INSERT INTO deduplication_table (date, id, s) VALUES (toDate('2020-03-03'), 1, 'foo')",
-        )
-        exec_query_with_retry(
-            node10,
-            "INSERT INTO deduplication_table (date, id, s) VALUES (toDate('2020-03-03'), 1, 'foo')",
-        )
-
-    node9.query("SYSTEM SYNC REPLICA deduplication_table", timeout=20)
-    node10.query("SYSTEM SYNC REPLICA deduplication_table", timeout=20)
-
-    assert (
-        node9.query("SELECT date, id, s FROM deduplication_table")
-        == "2020-03-03\t1\tfoo
"
-    )
-    assert (
-        node10.query("SELECT date, id, s FROM deduplication_table")
-        == "2020-03-03\t1\tfoo
"
-    )
-
-
-# Checks that restoring from WAL works after table schema changed
-def test_in_memory_alters(start_cluster):
-    def check_parts_type(parts_num):
-        assert (
-            node9.query(
-                "SELECT part_type, count() FROM system.parts WHERE table = 'alters_table' \
-             AND active GROUP BY part_type"
-            )
-            == "InMemory\t{}
".format(parts_num)
-        )
-
-    node9.query(
-        "INSERT INTO alters_table (date, id, s) VALUES (toDate('2020-10-10'), 1, 'ab'), (toDate('2020-10-10'), 2, 'cd')"
-    )
-    node9.query("ALTER TABLE alters_table ADD COLUMN col1 UInt32")
-    node9.restart_clickhouse(kill=True)
-
-    expected = "1\tab\t0
2\tcd\t0
"
-    assert node9.query("SELECT id, s, col1 FROM alters_table ORDER BY id") == expected
-    check_parts_type(1)
-    node9.query(
-        "INSERT INTO alters_table (date, id, col1) VALUES (toDate('2020-10-10'), 3, 100)"
-    )
-    node9.query("ALTER TABLE alters_table MODIFY COLUMN col1 String")
-    node9.query("ALTER TABLE alters_table DROP COLUMN s")
-    node9.restart_clickhouse(kill=True)
-
-    check_parts_type(2)
-    with pytest.raises(Exception):
-        node9.query("SELECT id, s, col1 FROM alters_table")
-
-    # Values of col1 was not materialized as integers, so they have
-    # default string values after alter
-    expected = "1\t_foo
2\t_foo
3\t100_foo
"
-    assert (
-        node9.query("SELECT id, col1 || '_foo' FROM alters_table ORDER BY id")
-        == expected
-    )
-
-
 def test_polymorphic_parts_index(start_cluster):
     node1.query(
         "CREATE DATABASE test_index ENGINE=Ordinary",
diff --git a/tests/performance/polymorphic_parts_s.xml b/tests/performance/polymorphic_parts_s.xml
index 5021e135bb92..b4dd87a7ae3d 100644
--- a/tests/performance/polymorphic_parts_s.xml
+++ b/tests/performance/polymorphic_parts_s.xml
@@ -13,13 +13,6 @@
         SAMPLE BY intHash32(UserID)
         SETTINGS min_bytes_for_wide_part = '10M'
     </create_query>
-    <create_query>
-        CREATE TABLE hits_memory AS hits_10m_single ENGINE = MergeTree()
-        PARTITION BY toYYYYMM(EventDate)
-        ORDER BY (CounterID, EventDate, intHash32(UserID))
-        SAMPLE BY intHash32(UserID)
-        SETTINGS min_bytes_for_compact_part = '1M', min_bytes_for_wide_part = '10M', in_memory_parts_enable_wal = 1
-    </create_query>
     <create_query>
        CREATE TABLE hits_buffer AS hits_10m_single 
        ENGINE = Buffer(default, hits_wide, 1, 0, 0, 10000, 10000, 0, 0)
@@ -38,6 +31,5 @@
 
     <drop_query>DROP TABLE IF EXISTS hits_wide</drop_query>
     <drop_query>DROP TABLE IF EXISTS hits_compact</drop_query>
-    <drop_query>DROP TABLE IF EXISTS hits_memory</drop_query>
     <drop_query>DROP TABLE IF EXISTS hits_buffer</drop_query>
 </test>
diff --git a/tests/queries/0_stateless/01130_in_memory_parts.reference b/tests/queries/0_stateless/01130_in_memory_parts.reference
deleted file mode 100644
index ad5435abb59b..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts.reference
+++ /dev/null
@@ -1,39 +0,0 @@
-system.parts
-InMemory	2
-1
-1
-Simple selects
-0	0
-1	1
-2	2
-3	0
-4	1
-50	2
-51	0
-52	1
-53	2
-54	0
-34
-0
-20
-10
-Mutations and Alters
-66
-1	1
-2	2
-4	1
-5	2
-7	1
-[1,1]
-[]
-[4,16]
-[]
-[7,49]
-1	1
-2	1
-1	[1,1]
-2	[]
-4	[4,16]
-5	[]
-7	[7,49]
-0
diff --git a/tests/queries/0_stateless/01130_in_memory_parts.sql b/tests/queries/0_stateless/01130_in_memory_parts.sql
deleted file mode 100644
index 2b15ae247638..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts.sql
+++ /dev/null
@@ -1,48 +0,0 @@
--- Tags: no-s3-storage
-
-DROP TABLE IF EXISTS in_memory;
-CREATE TABLE in_memory (a UInt32, b UInt32)
-    ENGINE = MergeTree ORDER BY a
-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_compact_part = 1000;
-
-INSERT INTO in_memory SELECT number, number % 3 FROM numbers(100);
-SELECT 'system.parts';
-SELECT DISTINCT part_type, marks FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory' AND active;
-SELECT DISTINCT data_uncompressed_bytes > 0 FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory' AND active;
-SELECT DISTINCT column_data_uncompressed_bytes > 0 FROM system.parts_columns WHERE database = currentDatabase() AND table = 'in_memory' AND active;
-
-SELECT 'Simple selects';
-
-SELECT * FROM in_memory ORDER BY a LIMIT 5;
-SELECT * FROM in_memory ORDER BY a LIMIT 5 OFFSET 50;
-SELECT count() FROM in_memory WHERE b = 0 SETTINGS max_block_size = 10;
--- Check index
-SELECT count() FROM in_memory WHERE a > 100 SETTINGS max_rows_to_read = 0, force_primary_key = 1;
-SELECT count() FROM in_memory WHERE a >= 10 AND a < 30 SETTINGS force_primary_key = 1;
-SELECT DISTINCT blockSize() FROM in_memory SETTINGS max_block_size = 10;
-
-SELECT 'Mutations and Alters';
-SET mutations_sync = 1;
-
-ALTER TABLE in_memory DELETE WHERE b = 0;
-
-SELECT count() FROM in_memory;
-SELECT * FROM in_memory ORDER BY a LIMIT 5;
-
-ALTER TABLE in_memory ADD COLUMN arr Array(UInt64);
-ALTER TABLE in_memory UPDATE arr = [a, a * a] WHERE b = 1;
-
-SELECT arr FROM in_memory ORDER BY a LIMIT 5;
-
-ALTER TABLE in_memory MODIFY COLUMN b String;
-ALTER TABLE in_memory RENAME COLUMN b to str;
-SELECT DISTINCT str, length(str) FROM in_memory ORDER BY str;
-ALTER TABLE in_memory DROP COLUMN str;
-
-SELECT * FROM in_memory ORDER BY a LIMIT 5;
-
--- in-memory parts works if they're empty.
-ALTER TABLE in_memory DELETE WHERE 1;
-SELECT count() FROM in_memory;
-
-DROP TABLE in_memory;
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_check.reference b/tests/queries/0_stateless/01130_in_memory_parts_check.reference
deleted file mode 100644
index 15f72836ff10..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_check.reference
+++ /dev/null
@@ -1,1 +0,0 @@
-201901_1_1_0	1	
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_check.sql b/tests/queries/0_stateless/01130_in_memory_parts_check.sql
deleted file mode 100644
index c2f5eba59491..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_check.sql
+++ /dev/null
@@ -1,13 +0,0 @@
--- Tags: no-s3-storage
-
--- Part of 00961_check_table test, but with in-memory parts
-
-SET check_query_single_value_result = 0;
-DROP TABLE IF EXISTS mt_table;
-CREATE TABLE mt_table (d Date, key UInt64, data String) ENGINE = MergeTree() PARTITION BY toYYYYMM(d) ORDER BY key
-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_compact_part = 1000;
-
-CHECK TABLE mt_table;
-INSERT INTO mt_table VALUES (toDate('2019-01-02'), 1, 'Hello'), (toDate('2019-01-02'), 2, 'World');
-CHECK TABLE mt_table;
-DROP TABLE mt_table;
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_default.reference b/tests/queries/0_stateless/01130_in_memory_parts_default.reference
deleted file mode 100644
index c51afdb66589..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_default.reference
+++ /dev/null
@@ -1,1 +0,0 @@
-0	bbb_aaa
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_default.sql b/tests/queries/0_stateless/01130_in_memory_parts_default.sql
deleted file mode 100644
index 776d5f89fcf4..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_default.sql
+++ /dev/null
@@ -1,21 +0,0 @@
--- Tags: no-s3-storage
--- Test 01266_default_prewhere_reqq, but with in-memory parts
-DROP TABLE IF EXISTS t1;
-
-CREATE TABLE t1
-(
-    date Date, 
-    s1 String,
-    s2 String
-) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(date) ORDER BY (date, s1)
-SETTINGS index_granularity = 8192, min_rows_for_compact_part = 1000, min_rows_for_wide_part = 1000;
-
-
-set max_threads=1;
-
-insert into t1 (date, s1,s2) values(today()-1,'aaa','bbb');
-alter table t1 add column s3 String DEFAULT concat(s2,'_',s1);
--- insert into t1 (date, s1,s2) values(today(),'aaa2','bbb2');
-select ignore(date), s3 from t1 where  s2='bbb';
-
-DROP TABLE t1;
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_nested.reference b/tests/queries/0_stateless/01130_in_memory_parts_nested.reference
deleted file mode 100644
index 3882abb94676..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_nested.reference
+++ /dev/null
@@ -1,25 +0,0 @@
-[0]
-[0,0,0]
-[0,0,0,0,0]
-[0,0,0,0,0,0,0]
-[0,0,0,0,0,0,0,0,0]
-[0]
-[0,0,0]
-[0,0,0,0,0]
-[0,0,0,0,0,0,0]
-[0,0,0,0,0,0,0,0,0]
-[0]
-[0,0,0]
-[0,0,0,0,0]
-[0,0,0,0,0,0,0]
-[0,0,0,0,0,0,0,0,0]
-[0]
-[0,2,4]
-[0,2,4,6,8]
-[0,2,4,6,8,10,12]
-[0,2,4,6,8,10,12,14,16]
-[0]	[0]
-[0,1,2]	[0,2,4]
-[0,1,2,3,4]	[0,2,4,6,8]
-[0,1,2,3,4,5,6]	[0,2,4,6,8,10,12]
-[0,1,2,3,4,5,6,7,8]	[0,2,4,6,8,10,12,14,16]
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_nested.sql b/tests/queries/0_stateless/01130_in_memory_parts_nested.sql
deleted file mode 100644
index 45e778b9f04f..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_nested.sql
+++ /dev/null
@@ -1,19 +0,0 @@
--- Tags: no-s3-storage
--- Test 00576_nested_and_prewhere, but with in-memory parts.
-DROP TABLE IF EXISTS nested;
-
-CREATE TABLE nested (x UInt64, filter UInt8, n Nested(a UInt64)) ENGINE = MergeTree ORDER BY x
-    SETTINGS min_rows_for_compact_part = 200000, min_rows_for_wide_part = 300000;
-
-INSERT INTO nested SELECT number, number % 2, range(number % 10) FROM system.numbers LIMIT 100000;
-
-ALTER TABLE nested ADD COLUMN n.b Array(UInt64);
-SELECT DISTINCT n.b FROM nested PREWHERE filter;
-SELECT DISTINCT n.b FROM nested PREWHERE filter SETTINGS max_block_size = 123;
-SELECT DISTINCT n.b FROM nested PREWHERE filter SETTINGS max_block_size = 1234;
-
-ALTER TABLE nested ADD COLUMN n.c Array(UInt64) DEFAULT arrayMap(x -> x * 2, n.a);
-SELECT DISTINCT n.c FROM nested PREWHERE filter;
-SELECT DISTINCT n.a, n.c FROM nested PREWHERE filter;
-
-DROP TABLE nested;
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_partitons.reference b/tests/queries/0_stateless/01130_in_memory_parts_partitons.reference
deleted file mode 100644
index 44cbbed3f57f..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_partitons.reference
+++ /dev/null
@@ -1,60 +0,0 @@
-1	2	foo
-1	3	bar
-2	4	aa
-2	5	bb
-2	6	cc
-3	7	qq
-3	8	ww
-3	9	ee
-3	10	rr
-1_1_1_0	InMemory	2
-2_2_2_0	InMemory	3
-3_3_3_0	InMemory	4
-^ init ==================
-2	4	aa
-2	5	bb
-2	6	cc
-3	7	qq
-3	8	ww
-3	9	ee
-3	10	rr
-2_2_2_0	InMemory	3
-3_3_3_0	InMemory	4
-^ drop 1 ==================
-3	7	qq
-3	8	ww
-3	9	ee
-3	10	rr
-3_3_3_0	InMemory	4
-^ detach 2 ==================
-2	4	aa
-2	5	bb
-2	6	cc
-3	7	qq
-3	8	ww
-3	9	ee
-3	10	rr
-2_4_4_0	Compact	3
-3_3_3_0	InMemory	4
-^ attach 2 =================
-2	4	aa
-2	5	bb
-2	6	cc
-3	7	qq
-3	8	ww
-3	9	ee
-3	10	rr
-2_4_4_0	Compact	3
-3_3_3_0	InMemory	4
-^ detach attach ==================
-2	4	aa
-2	5	bb
-2	6	cc
-3	11	tt
-3	12	yy
-t2	2_4_4_0	Compact	3
-t2	3_6_6_0	Compact	2
-t3	3_1_1_0	InMemory	2
-^ replace ==================
-3_1_1_0	InMemory	1	2
-^ freeze ==================
diff --git a/tests/queries/0_stateless/01130_in_memory_parts_partitons.sql b/tests/queries/0_stateless/01130_in_memory_parts_partitons.sql
deleted file mode 100644
index 18da2d2bd30a..000000000000
--- a/tests/queries/0_stateless/01130_in_memory_parts_partitons.sql
+++ /dev/null
@@ -1,59 +0,0 @@
--- Tags: no-parallel, no-s3-storage
-
-DROP TABLE IF EXISTS t2;
-
-CREATE TABLE t2(id UInt32, a UInt64, s String)
-    ENGINE = MergeTree ORDER BY a PARTITION BY id
-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_wide_part = 2000;
-
-SYSTEM STOP MERGES t2;
-
-INSERT INTO t2 VALUES (1, 2, 'foo'), (1, 3, 'bar');
-INSERT INTO t2 VALUES (2, 4, 'aa'), (2, 5, 'bb'), (2, 6, 'cc');
-INSERT INTO t2 VALUES (3, 7, 'qq'), (3, 8, 'ww'), (3, 9, 'ee'), (3, 10, 'rr');
-
-SELECT * FROM t2 ORDER BY a;
-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;
-SELECT '^ init ==================';
-
-ALTER TABLE t2 DROP PARTITION 1;
-SELECT * FROM t2 ORDER BY a;
-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;
-SELECT '^ drop 1 ==================';
-
-ALTER TABLE t2 DETACH PARTITION 2;
-SELECT * FROM t2 ORDER BY a;
-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;
-SELECT '^ detach 2 ==================';
-
-ALTER TABLE t2 ATTACH PARTITION 2;
-SELECT * FROM t2 ORDER BY a;
-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;
-SELECT '^ attach 2 =================';
-
-DETACH TABLE t2;
-ATTACH TABLE t2;
-
-SELECT * FROM t2 ORDER BY a;
-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;
-SELECT '^ detach attach ==================';
-
-DROP TABLE IF EXISTS t3;
-
-CREATE TABLE t3(id UInt32, a UInt64, s String)
-    ENGINE = MergeTree ORDER BY a PARTITION BY id
-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_wide_part = 2000;
-
-INSERT INTO t3 VALUES (3, 11, 'tt'), (3, 12, 'yy');
-ALTER TABLE t2 REPLACE PARTITION 3 FROM t3;
-SELECT * FROM t2 ORDER BY a;
-SELECT table, name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;
-SELECT table, name, part_type, rows FROM system.parts WHERE table = 't3' AND active AND database = currentDatabase() ORDER BY name;
-SELECT '^ replace ==================';
-
-ALTER TABLE t3 FREEZE PARTITION 3;
-SELECT name, part_type, is_frozen, rows FROM system.parts WHERE table = 't3' AND active AND database = currentDatabase() ORDER BY name;
-SELECT '^ freeze ==================';
-
-DROP TABLE t2;
-DROP TABLE t3;
diff --git a/tests/queries/0_stateless/01475_read_subcolumns_storages.reference b/tests/queries/0_stateless/01475_read_subcolumns_storages.reference
index 4e37b751d5aa..ce0b13eabd2c 100644
--- a/tests/queries/0_stateless/01475_read_subcolumns_storages.reference
+++ b/tests/queries/0_stateless/01475_read_subcolumns_storages.reference
@@ -7,9 +7,6 @@ TinyLog
 Memory
 100	[1,2,3]	[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]	[1,NULL,2]	('foo',200)	{'foo':1,'bar':42}
 100	0	[1,2,3]	3	[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]	3	[3,2,1]	[[2,0,1],[2,2],[0]]	[1,NULL,2]	3	[0,1,0]	('foo',200)	foo	200	{'foo':1,'bar':42}	['foo','bar']	[1,42]
-MergeTree ORDER BY tuple() SETTINGS min_bytes_for_compact_part='10M'
-100	[1,2,3]	[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]	[1,NULL,2]	('foo',200)	{'foo':1,'bar':42}
-100	0	[1,2,3]	3	[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]	3	[3,2,1]	[[2,0,1],[2,2],[0]]	[1,NULL,2]	3	[0,1,0]	('foo',200)	foo	200	{'foo':1,'bar':42}	['foo','bar']	[1,42]
 MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part='10M'
 100	[1,2,3]	[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]	[1,NULL,2]	('foo',200)	{'foo':1,'bar':42}
 100	0	[1,2,3]	3	[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]	3	[3,2,1]	[[2,0,1],[2,2],[0]]	[1,NULL,2]	3	[0,1,0]	('foo',200)	foo	200	{'foo':1,'bar':42}	['foo','bar']	[1,42]
diff --git a/tests/queries/0_stateless/01475_read_subcolumns_storages.sh b/tests/queries/0_stateless/01475_read_subcolumns_storages.sh
index d770d5118ace..4a9b9639f78a 100755
--- a/tests/queries/0_stateless/01475_read_subcolumns_storages.sh
+++ b/tests/queries/0_stateless/01475_read_subcolumns_storages.sh
@@ -12,7 +12,6 @@ create_query="CREATE TABLE subcolumns(n Nullable(UInt32), a1 Array(UInt32),\
 
 # "StripeLog"
 declare -a ENGINES=("Log" "TinyLog" "Memory" \
-    "MergeTree ORDER BY tuple() SETTINGS min_bytes_for_compact_part='10M'" \
     "MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part='10M'" \
     "MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0")
 
diff --git a/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh b/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh
index efe24aa3a886..80318ba67fbc 100755
--- a/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh
+++ b/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh
@@ -8,9 +8,7 @@ CURDIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
 $CLICKHOUSE_CLIENT --query "DROP TABLE IF EXISTS table_for_renames0"
 $CLICKHOUSE_CLIENT --query "DROP TABLE IF EXISTS table_for_renames50"
 
-
-$CLICKHOUSE_CLIENT --query "CREATE TABLE table_for_renames0 (value UInt64, data String) ENGINE ReplicatedMergeTree('/clickhouse/tables/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX/concurrent_rename', '1') ORDER BY tuple() SETTINGS cleanup_delay_period = 1, cleanup_delay_period_random_add = 0, min_rows_for_compact_part = 100000, min_rows_for_compact_part = 10000000, write_ahead_log_max_bytes = 1"
-
+$CLICKHOUSE_CLIENT --query "CREATE TABLE table_for_renames0 (value UInt64, data String) ENGINE ReplicatedMergeTree('/clickhouse/tables/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX/concurrent_rename', '1') ORDER BY tuple() SETTINGS cleanup_delay_period = 1, cleanup_delay_period_random_add = 0"
 
 $CLICKHOUSE_CLIENT --query "INSERT INTO table_for_renames0 SELECT number, toString(number) FROM numbers(1000)"
 
diff --git a/tests/queries/0_stateless/01600_parts_types_metrics_long.sh b/tests/queries/0_stateless/01600_parts_types_metrics_long.sh
index 0b9afcf633ef..5f724e810422 100755
--- a/tests/queries/0_stateless/01600_parts_types_metrics_long.sh
+++ b/tests/queries/0_stateless/01600_parts_types_metrics_long.sh
@@ -35,7 +35,7 @@ $CLICKHOUSE_CLIENT --database_atomic_wait_for_drop_and_detach_synchronously=1 --
 # InMemory - [0..5]
 # Compact  - (5..10]
 # Wide     - >10
-$CLICKHOUSE_CLIENT --query="CREATE TABLE data_01600 (part_type String, key Int) ENGINE = MergeTree PARTITION BY part_type ORDER BY key SETTINGS min_bytes_for_wide_part=0, min_bytes_for_compact_part=0, min_rows_for_wide_part=10, min_rows_for_compact_part=5"
+$CLICKHOUSE_CLIENT --query="CREATE TABLE data_01600 (part_type String, key Int) ENGINE = MergeTree PARTITION BY part_type ORDER BY key SETTINGS min_bytes_for_wide_part=0, min_rows_for_wide_part=10"
 
 # InMemory
 $CLICKHOUSE_CLIENT --query="INSERT INTO data_01600 SELECT 'InMemory', number FROM system.numbers LIMIT 1"
diff --git a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference
index 613c455fc590..98bb953263a3 100644
--- a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference
+++ b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference
@@ -8,8 +8,6 @@ wide fsync_after_insert
 1
 wide fsync_after_insert,fsync_part_directory
 1
-memory in_memory_parts_insert_sync
-1
 wide fsync_part_directory,vertical
 1
 2
diff --git a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql
index ad0dfca0db20..dfc761e17649 100644
--- a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql
+++ b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql
@@ -37,13 +37,6 @@ select * from data_01643;
 optimize table data_01643 final;
 drop table data_01643;
 
-select 'memory in_memory_parts_insert_sync';
-create table data_01643 (key Int) engine=MergeTree() order by key settings min_rows_for_compact_part=2, in_memory_parts_insert_sync=1, fsync_after_insert=1, fsync_part_directory=1;
-insert into data_01643 values (1);
-select * from data_01643;
-optimize table data_01643 final;
-drop table data_01643;
-
 select 'wide fsync_part_directory,vertical';
 create table data_01643 (key Int) engine=MergeTree() order by key settings min_bytes_for_wide_part=0, fsync_part_directory=1, enable_vertical_merge_algorithm=1, vertical_merge_algorithm_min_rows_to_activate=0, vertical_merge_algorithm_min_columns_to_activate=0;
 insert into data_01643 values (1);
diff --git a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference
index 613c455fc590..98bb953263a3 100644
--- a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference
+++ b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference
@@ -8,8 +8,6 @@ wide fsync_after_insert
 1
 wide fsync_after_insert,fsync_part_directory
 1
-memory in_memory_parts_insert_sync
-1
 wide fsync_part_directory,vertical
 1
 2
diff --git a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql
index bcce87e11db3..54c30fa2b1ac 100644
--- a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql
+++ b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql
@@ -63,17 +63,6 @@ system sync replica rep_fsync_r2;
 drop table rep_fsync_r1;
 drop table rep_fsync_r2;
 
-select 'memory in_memory_parts_insert_sync';
-create table rep_fsync_r1 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r1') order by key settings min_rows_for_compact_part=2, in_memory_parts_insert_sync=1, fsync_after_insert=1, fsync_part_directory=1;
-create table rep_fsync_r2 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r2') order by key settings min_rows_for_compact_part=2, in_memory_parts_insert_sync=1, fsync_after_insert=1, fsync_part_directory=1;
-insert into rep_fsync_r1 values (1);
-system sync replica rep_fsync_r2;
-select * from rep_fsync_r2;
-optimize table rep_fsync_r1 final;
-system sync replica rep_fsync_r2;
-drop table rep_fsync_r1;
-drop table rep_fsync_r2;
-
 select 'wide fsync_part_directory,vertical';
 create table rep_fsync_r1 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r1') order by key settings min_bytes_for_wide_part=0, fsync_part_directory=1, enable_vertical_merge_algorithm=1, vertical_merge_algorithm_min_rows_to_activate=0, vertical_merge_algorithm_min_columns_to_activate=0;
 create table rep_fsync_r2 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r2') order by key settings min_bytes_for_wide_part=0, fsync_part_directory=1, enable_vertical_merge_algorithm=1, vertical_merge_algorithm_min_rows_to_activate=0, vertical_merge_algorithm_min_columns_to_activate=0;
diff --git a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference
index c66682ca038b..cf3e942adfe1 100644
--- a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference
+++ b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference
@@ -4,7 +4,6 @@ ReplacingMergeTree: OK
 JSONEachRow: OK
 clusterAllReplicas: OK
 SimpleAggregateFunction: OK
-write_ahead_log_interval_ms_to_fsync: OK
 max_concurrent_queries_for_all_users: OK
 test_shard_localhost: OK
 default_path_test: OK
diff --git a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh
index 617148de5a37..42ae5e84f445 100755
--- a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh
+++ b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh
@@ -89,8 +89,6 @@ client_compwords_positive=(
     clusterAllReplicas
     # system.data_type_families
     SimpleAggregateFunction
-    # system.merge_tree_settings
-    write_ahead_log_interval_ms_to_fsync
     # system.settings
     max_concurrent_queries_for_all_users
     # system.clusters
diff --git a/tests/queries/0_stateless/01710_projection_fetch_long.reference b/tests/queries/0_stateless/01710_projection_fetch_long.reference
index abce5410b26d..c7834c75d022 100644
--- a/tests/queries/0_stateless/01710_projection_fetch_long.reference
+++ b/tests/queries/0_stateless/01710_projection_fetch_long.reference
@@ -10,8 +10,8 @@
 3	3
 4	4
 0
-CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    ),
    PROJECTION pp
    (
        SELECT 
            x,
            count()
        GROUP BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192
+CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    ),
    PROJECTION pp
    (
        SELECT 
            x,
            count()
        GROUP BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192
 2
-CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    ),
    PROJECTION pp
    (
        SELECT 
            x,
            count()
        GROUP BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192
-CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    ),
    PROJECTION pp
    (
        SELECT 
            x,
            count()
        GROUP BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192
-CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192
+CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    ),
    PROJECTION pp
    (
        SELECT 
            x,
            count()
        GROUP BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192
+CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    ),
    PROJECTION pp
    (
        SELECT 
            x,
            count()
        GROUP BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192
+CREATE TABLE default.tp_2
(
    `x` Int32,
    `y` Int32,
    PROJECTION p
    (
        SELECT 
            x,
            y
        ORDER BY x
    )
)
ENGINE = ReplicatedMergeTree(\'/clickhouse/tables/{shard}/01710_projection_fetch_default\', \'2_{replica}\')
ORDER BY y
SETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192
diff --git a/tests/queries/0_stateless/01710_projection_fetch_long.sql b/tests/queries/0_stateless/01710_projection_fetch_long.sql
index 6c41c69254e1..13f7a293934e 100644
--- a/tests/queries/0_stateless/01710_projection_fetch_long.sql
+++ b/tests/queries/0_stateless/01710_projection_fetch_long.sql
@@ -3,9 +3,9 @@
 drop table if exists tp_1;
 drop table if exists tp_2;
 
-create table tp_1 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '1_{replica}') order by y settings min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32;
+create table tp_1 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '1_{replica}') order by y settings min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32;
 
-create table tp_2 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '2_{replica}') order by y settings min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32;
+create table tp_2 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '2_{replica}') order by y settings min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32;
 
 insert into tp_1 select number, number from numbers(3);
 
diff --git a/tests/queries/0_stateless/01710_projection_part_check.sql b/tests/queries/0_stateless/01710_projection_part_check.sql
index c889bd323a7b..b15d9d7525ec 100644
--- a/tests/queries/0_stateless/01710_projection_part_check.sql
+++ b/tests/queries/0_stateless/01710_projection_part_check.sql
@@ -1,6 +1,6 @@
 drop table if exists tp;
 
-create table tp (x Int32, y Int32, projection p (select x, y order by x)) engine = MergeTree order by y settings min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32;
+create table tp (x Int32, y Int32, projection p (select x, y order by x)) engine = MergeTree order by y settings min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32;
 
 insert into tp select number, number from numbers(3);
 insert into tp select number, number from numbers(5);
diff --git a/tests/queries/0_stateless/02148_in_memory_part_flush.reference b/tests/queries/0_stateless/02148_in_memory_part_flush.reference
deleted file mode 100644
index 219c5f4b22f6..000000000000
--- a/tests/queries/0_stateless/02148_in_memory_part_flush.reference
+++ /dev/null
@@ -1,4 +0,0 @@
-before DETACH TABLE
-500
-after DETACH TABLE
-500
diff --git a/tests/queries/0_stateless/02148_in_memory_part_flush.sql b/tests/queries/0_stateless/02148_in_memory_part_flush.sql
deleted file mode 100644
index ec20721186e2..000000000000
--- a/tests/queries/0_stateless/02148_in_memory_part_flush.sql
+++ /dev/null
@@ -1,26 +0,0 @@
-DROP TABLE IF EXISTS mem_part_flush;
-
-CREATE TABLE mem_part_flush
-(
-`key` UInt32,
-`ts` DateTime,
-`db_time` DateTime DEFAULT now()
-)
-ENGINE = MergeTree
-ORDER BY (key, ts)
-SETTINGS min_rows_for_compact_part = 1000000, min_bytes_for_compact_part = 200000000, in_memory_parts_enable_wal = 0;
-
-INSERT INTO mem_part_flush(key, ts) SELECT number % 1000, now() + intDiv(number,1000) FROM numbers(500);
-
-SELECT 'before DETACH TABLE';
-SELECT count(*) FROM mem_part_flush;
-
-DETACH TABLE mem_part_flush;
-
-ATTACH TABLE mem_part_flush;
-
-SELECT 'after DETACH TABLE';
-SELECT count(*) FROM mem_part_flush;
-
-
-DROP TABLE mem_part_flush;
diff --git a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.reference b/tests/queries/0_stateless/02410_inmemory_wal_cleanup.reference
deleted file mode 100644
index 6727d83a6f42..000000000000
--- a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.reference
+++ /dev/null
@@ -1,35 +0,0 @@
--- { echo }
-
-DROP TABLE IF EXISTS in_memory;
-CREATE TABLE in_memory (a UInt32) ENGINE = MergeTree ORDER BY a SETTINGS min_rows_for_compact_part = 1000, min_bytes_for_wide_part = 10485760;
-INSERT INTO in_memory VALUES (1);
-INSERT INTO in_memory VALUES (2);
-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';
-all_1_1_0	1	InMemory
-all_2_2_0	1	InMemory
-SELECT * FROM in_memory ORDER BY a;
-1
-2
--- no WAL remove since parts are still in use
-DETACH TABLE in_memory;
-ATTACH TABLE in_memory;
-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';
-all_1_1_0	1	InMemory
-all_2_2_0	1	InMemory
-SELECT * FROM in_memory ORDER BY a;
-1
-2
--- WAL should be removed, since on disk part covers all parts in WAL
-OPTIMIZE TABLE in_memory;
-DETACH TABLE in_memory;
-ATTACH TABLE in_memory;
-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';
-all_1_2_1	1	Compact
--- check that the WAL will be reinitialized after remove
-INSERT INTO in_memory VALUES (3);
-DETACH TABLE in_memory;
-ATTACH TABLE in_memory;
-SELECT * FROM in_memory ORDER BY a;
-1
-2
-3
diff --git a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.sql b/tests/queries/0_stateless/02410_inmemory_wal_cleanup.sql
deleted file mode 100644
index 7f832d980ba2..000000000000
--- a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.sql
+++ /dev/null
@@ -1,29 +0,0 @@
--- Tags: no-s3-storage
-
--- { echo }
-
-DROP TABLE IF EXISTS in_memory;
-
-CREATE TABLE in_memory (a UInt32) ENGINE = MergeTree ORDER BY a SETTINGS min_rows_for_compact_part = 1000, min_bytes_for_wide_part = 10485760;
-INSERT INTO in_memory VALUES (1);
-INSERT INTO in_memory VALUES (2);
-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';
-SELECT * FROM in_memory ORDER BY a;
-
--- no WAL remove since parts are still in use
-DETACH TABLE in_memory;
-ATTACH TABLE in_memory;
-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';
-SELECT * FROM in_memory ORDER BY a;
-
--- WAL should be removed, since on disk part covers all parts in WAL
-OPTIMIZE TABLE in_memory;
-DETACH TABLE in_memory;
-ATTACH TABLE in_memory;
-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';
-
--- check that the WAL will be reinitialized after remove
-INSERT INTO in_memory VALUES (3);
-DETACH TABLE in_memory;
-ATTACH TABLE in_memory;
-SELECT * FROM in_memory ORDER BY a;
diff --git a/tests/queries/0_stateless/02423_drop_memory_parts.reference b/tests/queries/0_stateless/02423_drop_memory_parts.reference
deleted file mode 100644
index d69a5f07a051..000000000000
--- a/tests/queries/0_stateless/02423_drop_memory_parts.reference
+++ /dev/null
@@ -1,14 +0,0 @@
-init state
-30
-0_1_1_0	InMemory	10	1
-1_2_2_0	InMemory	10	1
-2_3_3_0	InMemory	10	1
-drop part 0
-20
-1_2_2_0	InMemory	10	1
-2_3_3_0	InMemory	10	1
-detach table
-attach table
-20
-1_2_2_0	InMemory	10	1
-2_3_3_0	InMemory	10	1
diff --git a/tests/queries/0_stateless/02423_drop_memory_parts.sql b/tests/queries/0_stateless/02423_drop_memory_parts.sql
deleted file mode 100644
index 9326f159b0ce..000000000000
--- a/tests/queries/0_stateless/02423_drop_memory_parts.sql
+++ /dev/null
@@ -1,40 +0,0 @@
--- Tags: no-s3-storage
-
-DROP TABLE IF EXISTS table_in_memory;
-
-CREATE TABLE table_in_memory
-(
-    `id` UInt64,
-    `value` UInt64
-)
-ENGINE = MergeTree
-PARTITION BY id
-ORDER BY value
-SETTINGS min_bytes_for_wide_part=1000, min_bytes_for_compact_part=900;
-
-SELECT 'init state';
-INSERT INTO table_in_memory SELECT intDiv(number, 10), number FROM numbers(30);
-
-SELECT count() FROM table_in_memory;
-SELECT name, part_type, rows, active from system.parts
-WHERE table='table_in_memory' AND database=currentDatabase();
-
-SELECT 'drop part 0';
-ALTER TABLE table_in_memory DROP PARTITION 0;
-
-SELECT count() FROM table_in_memory;
-SELECT name, part_type, rows, active from system.parts
-WHERE table='table_in_memory' AND database=currentDatabase() AND active;
-
-SELECT 'detach table';
-DETACH TABLE table_in_memory;
-
-SELECT name, part_type, rows, active from system.parts
-WHERE table='table_in_memory' AND database=currentDatabase();
-
-SELECT 'attach table';
-ATTACH TABLE table_in_memory;
-
-SELECT count() FROM table_in_memory;
-SELECT name, part_type, rows, active from system.parts
-WHERE table='table_in_memory' AND database=currentDatabase() and active;
diff --git a/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference b/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference
index b0214e0e7c7a..9ed0fb620a4c 100644
--- a/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference
+++ b/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference
@@ -1,6 +1,4 @@
 data_compact	Compact
 [[]]
-data_memory	InMemory
-[[]]
 data_wide	Wide
 [[]]
diff --git a/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql b/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql
index 156af5c7784d..9dcdab82acbe 100644
--- a/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql
+++ b/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql
@@ -10,26 +10,12 @@ CREATE TABLE data_compact
 )
 ENGINE = MergeTree()
 ORDER BY tuple()
-SETTINGS min_rows_for_compact_part=0, min_bytes_for_compact_part=0, min_rows_for_wide_part=100, min_bytes_for_wide_part=1e9;
+SETTINGS min_rows_for_wide_part=100, min_bytes_for_wide_part=1e9;
 INSERT INTO data_compact VALUES ([0]);
 ALTER TABLE data_compact ADD COLUMN root.nested_array Array(Array(UInt8));
 SELECT table, part_type FROM system.parts WHERE table = 'data_compact' AND database = currentDatabase();
 SELECT root.nested_array FROM data_compact;
 
--- memory
-DROP TABLE IF EXISTS data_memory;
-CREATE TABLE data_memory
-(
-    `root.array` Array(UInt8),
-)
-ENGINE = MergeTree()
-ORDER BY tuple()
-SETTINGS min_rows_for_compact_part=100, min_bytes_for_compact_part=1e9, min_rows_for_wide_part=100, min_bytes_for_wide_part=1e9, in_memory_parts_enable_wal=0;
-INSERT INTO data_memory VALUES ([0]);
-ALTER TABLE data_memory ADD COLUMN root.nested_array Array(Array(UInt8));
-SELECT table, part_type FROM system.parts WHERE table = 'data_memory' AND database = currentDatabase();
-SELECT root.nested_array FROM data_memory;
-
 -- wide
 DROP TABLE IF EXISTS data_wide;
 CREATE TABLE data_wide
@@ -38,7 +24,7 @@ CREATE TABLE data_wide
 )
 ENGINE = MergeTree()
 ORDER BY tuple()
-SETTINGS min_rows_for_wide_part=0, min_bytes_for_wide_part=0, min_rows_for_wide_part=0, min_bytes_for_wide_part=0;
+SETTINGS min_rows_for_wide_part=0, min_bytes_for_wide_part=0;
 INSERT INTO data_wide VALUES ([0]);
 ALTER TABLE data_wide ADD COLUMN root.nested_array Array(Array(UInt8));
 SELECT table, part_type FROM system.parts WHERE table = 'data_wide' AND database = currentDatabase();
diff --git a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh
index 4713e4cbe8b4..adc9525ef810 100755
--- a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh
+++ b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh
@@ -32,11 +32,8 @@ DROP TABLE IF EXISTS times;
 CREATE TABLE times (t DateTime) ENGINE MergeTree ORDER BY t
   SETTINGS
     storage_policy='default',
-    min_rows_for_compact_part = 0,
-    min_bytes_for_compact_part = 0,
     min_rows_for_wide_part = 1000000,
     min_bytes_for_wide_part = 1000000,
-    in_memory_parts_enable_wal = 0,
     ratio_of_defaults_for_sparse_serialization=1.0;
 "
 
@@ -74,4 +71,3 @@ AND ( query LIKE '%SELECT % FROM times%' OR query LIKE '%INSERT INTO times%' )
 AND type = 'QueryFinish'
 ORDER BY query_start_time_microseconds ASC, query DESC;
 "
-
diff --git a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.reference b/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.reference
deleted file mode 100644
index 0cfbf08886fc..000000000000
--- a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.reference
+++ /dev/null
@@ -1,1 +0,0 @@
-2
diff --git a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.sql b/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.sql
deleted file mode 100644
index 194ea9bfcc11..000000000000
--- a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.sql
+++ /dev/null
@@ -1,15 +0,0 @@
--- Tags: no-s3-storage
-
-DROP TABLE IF EXISTS inmemory_test;
-
-CREATE TABLE inmemory_test (d Date, id String)
-ENGINE=ReplicatedMergeTree('/clickhouse/tables/{database}/inmemory_test', 'r1')
-PARTITION BY toYYYYMMDD(d) ORDER BY (d, id)
-SETTINGS min_rows_for_compact_part = 10, index_granularity = 8192;
-
-INSERT INTO inmemory_test(d, id) VALUES('2023-01-01', 'abcdefghijklmnopqrstuvwxyz');
-INSERT INTO inmemory_test(d, id) VALUES('2023-01-01', 'a1234567890123456789012345');
-
-SELECT COUNT(1) FROM inmemory_test;
-
-DROP TABLE inmemory_test;
diff --git a/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh b/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh
index bdcdb38846c5..130f3a29ade8 100755
--- a/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh
+++ b/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh
@@ -7,7 +7,7 @@ CUR_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
 
 $CLICKHOUSE_CLIENT -nm -q "
     drop table if exists data;
-    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9, min_bytes_for_compact_part=0;
+    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9;
 "
 
 # reading 1e6*8 bytes with 1M bandwith it should take (8-1)/1=7 seconds
diff --git a/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh b/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh
index 276a15ca6cc3..80713e901693 100755
--- a/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh
+++ b/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh
@@ -7,7 +7,7 @@ CUR_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
 
 $CLICKHOUSE_CLIENT -nm -q "
     drop table if exists data;
-    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9, min_bytes_for_compact_part=0;
+    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9;
 "
 
 query_id=$(random_str 10)
diff --git a/tests/queries/0_stateless/02704_max_backup_bandwidth.sh b/tests/queries/0_stateless/02704_max_backup_bandwidth.sh
index c9ad23031b6a..b5d32d2059d6 100755
--- a/tests/queries/0_stateless/02704_max_backup_bandwidth.sh
+++ b/tests/queries/0_stateless/02704_max_backup_bandwidth.sh
@@ -7,7 +7,7 @@ CUR_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
 
 $CLICKHOUSE_CLIENT -nm -q "
     drop table if exists data;
-    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9, min_bytes_for_compact_part=0;
+    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9;
 "
 
 # reading 1e6*8 bytes with 1M bandwith it should take (8-1)/1=7 seconds
