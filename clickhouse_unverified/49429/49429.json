{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 49429,
  "instance_id": "ClickHouse__ClickHouse-49429",
  "issue_numbers": [
    "45409"
  ],
  "base_commit": "6c02b6b327420d04379df7ccefe1e5c435ce8d2e",
  "patch": "diff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp\nindex 60bad1f75990..82b1f289b9ec 100644\n--- a/src/Common/ProfileEvents.cpp\n+++ b/src/Common/ProfileEvents.cpp\n@@ -191,10 +191,8 @@\n     \\\n     M(InsertedWideParts, \"Number of parts inserted in Wide format.\") \\\n     M(InsertedCompactParts, \"Number of parts inserted in Compact format.\") \\\n-    M(InsertedInMemoryParts, \"Number of parts inserted in InMemory format.\") \\\n     M(MergedIntoWideParts, \"Number of parts merged into Wide format.\") \\\n     M(MergedIntoCompactParts, \"Number of parts merged into Compact format.\") \\\n-    M(MergedIntoInMemoryParts, \"Number of parts in merged into InMemory format.\") \\\n     \\\n     M(MergeTreeDataProjectionWriterRows, \"Number of rows INSERTed to MergeTree tables projection.\") \\\n     M(MergeTreeDataProjectionWriterUncompressedBytes, \"Uncompressed bytes (for columns as they stored in memory) INSERTed to MergeTree tables projection.\") \\\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex d60307cc6c52..a2a3348eb42d 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -118,10 +118,8 @@ namespace ProfileEvents\n     extern const Event DelayedInsertsMilliseconds;\n     extern const Event InsertedWideParts;\n     extern const Event InsertedCompactParts;\n-    extern const Event InsertedInMemoryParts;\n     extern const Event MergedIntoWideParts;\n     extern const Event MergedIntoCompactParts;\n-    extern const Event MergedIntoInMemoryParts;\n     extern const Event RejectedMutations;\n     extern const Event DelayedMutations;\n     extern const Event DelayedMutationsMilliseconds;\n@@ -385,8 +383,7 @@ MergeTreeData::MergeTreeData(\n \n     String reason;\n     if (!canUsePolymorphicParts(*settings, &reason) && !reason.empty())\n-        LOG_WARNING(log, \"{} Settings 'min_rows_for_wide_part', 'min_bytes_for_wide_part', \"\n-            \"'min_rows_for_compact_part' and 'min_bytes_for_compact_part' will be ignored.\", reason);\n+        LOG_WARNING(log, \"{} Settings 'min_rows_for_wide_part'and 'min_bytes_for_wide_part' will be ignored.\", reason);\n \n #if !USE_ROCKSDB\n     if (use_metadata_cache)\n@@ -2354,22 +2351,6 @@ void MergeTreeData::removePartsFinally(const MergeTreeData::DataPartsVector & pa\n     }\n }\n \n-void MergeTreeData::flushAllInMemoryPartsIfNeeded()\n-{\n-    if (getSettings()->in_memory_parts_enable_wal)\n-        return;\n-\n-    auto metadata_snapshot = getInMemoryMetadataPtr();\n-    DataPartsVector parts = getDataPartsVectorForInternalUsage();\n-    for (const auto & part : parts)\n-    {\n-        if (auto part_in_memory = asInMemoryPart(part))\n-        {\n-            part_in_memory->flushToDisk(part_in_memory->getDataPartStorage().getPartDirectory(), metadata_snapshot);\n-        }\n-    }\n-}\n-\n size_t MergeTreeData::clearOldPartsFromFilesystem(bool force)\n {\n     DataPartsVector parts_to_remove = grabOldParts(force);\n@@ -3342,7 +3323,7 @@ void MergeTreeData::checkMutationIsPossible(const MutationCommands & /*commands*\n     /// Some validation will be added\n }\n \n-MergeTreeDataPartFormat MergeTreeData::choosePartFormat(size_t bytes_uncompressed, size_t rows_count, bool only_on_disk) const\n+MergeTreeDataPartFormat MergeTreeData::choosePartFormat(size_t bytes_uncompressed, size_t rows_count) const\n {\n     using PartType = MergeTreeDataPartType;\n     using PartStorageType = MergeTreeDataPartStorageType;\n@@ -3356,9 +3337,6 @@ MergeTreeDataPartFormat MergeTreeData::choosePartFormat(size_t bytes_uncompresse\n         return bytes_uncompressed < min_bytes_for || rows_count < min_rows_for;\n     };\n \n-    if (!only_on_disk && satisfies(settings->min_bytes_for_compact_part, settings->min_rows_for_compact_part))\n-        return {PartType::InMemory, PartStorageType::Full};\n-\n     auto part_type = PartType::Wide;\n     if (satisfies(settings->min_bytes_for_wide_part, settings->min_rows_for_wide_part))\n         part_type = PartType::Compact;\n@@ -3368,7 +3346,7 @@ MergeTreeDataPartFormat MergeTreeData::choosePartFormat(size_t bytes_uncompresse\n \n MergeTreeDataPartFormat MergeTreeData::choosePartFormatOnDisk(size_t bytes_uncompressed, size_t rows_count) const\n {\n-    return choosePartFormat(bytes_uncompressed, rows_count, true);\n+    return choosePartFormat(bytes_uncompressed, rows_count);\n }\n \n MergeTreeDataPartBuilder MergeTreeData::getDataPartBuilder(\n@@ -6146,19 +6124,6 @@ MergeTreeData::DataPartsVector MergeTreeData::Transaction::commit(MergeTreeData:\n             }\n         }\n \n-        MergeTreeData::WriteAheadLogPtr wal;\n-        auto get_inited_wal = [&] ()\n-        {\n-            if (!wal)\n-                wal = data.getWriteAheadLog();\n-            return wal;\n-        };\n-\n-        if (settings->in_memory_parts_enable_wal)\n-            for (const auto & part : precommitted_parts)\n-                if (auto part_in_memory = asInMemoryPart(part))\n-                    get_inited_wal()->addPart(part_in_memory);\n-\n         NOEXCEPT_SCOPE({\n             auto current_time = time(nullptr);\n \n@@ -6202,10 +6167,6 @@ MergeTreeData::DataPartsVector MergeTreeData::Transaction::commit(MergeTreeData:\n \n                         data.modifyPartState(covered_part, DataPartState::Outdated);\n                         data.removePartContributionToColumnAndSecondaryIndexSizes(covered_part);\n-\n-                        if (settings->in_memory_parts_enable_wal)\n-                            if (isInMemoryPart(covered_part))\n-                                get_inited_wal()->dropPart(covered_part->name);\n                     }\n \n                     reduce_parts += covered_parts.size();\n@@ -7886,11 +7847,8 @@ bool MergeTreeData::canUsePolymorphicParts(const MergeTreeSettings & settings, S\n                     \"Table can't create parts with adaptive granularity, but settings\"\n                     \" min_rows_for_wide_part = {}\"\n                     \", min_bytes_for_wide_part = {}\"\n-                    \", min_rows_for_compact_part = {}\"\n-                    \", min_bytes_for_compact_part = {}\"\n                     \". Parts with non-adaptive granularity can be stored only in Wide (default) format.\",\n-                    settings.min_rows_for_wide_part,    settings.min_bytes_for_wide_part,\n-                    settings.min_rows_for_compact_part, settings.min_bytes_for_compact_part);\n+                    settings.min_rows_for_wide_part, settings.min_bytes_for_wide_part);\n         }\n \n         return false;\n@@ -8270,9 +8228,6 @@ void MergeTreeData::incrementInsertedPartsProfileEvent(MergeTreeDataPartType typ\n         case MergeTreeDataPartType::Compact:\n             ProfileEvents::increment(ProfileEvents::InsertedCompactParts);\n             break;\n-        case MergeTreeDataPartType::InMemory:\n-            ProfileEvents::increment(ProfileEvents::InsertedInMemoryParts);\n-            break;\n         default:\n             break;\n     }\n@@ -8288,9 +8243,6 @@ void MergeTreeData::incrementMergedPartsProfileEvent(MergeTreeDataPartType type)\n         case MergeTreeDataPartType::Compact:\n             ProfileEvents::increment(ProfileEvents::MergedIntoCompactParts);\n             break;\n-        case MergeTreeDataPartType::InMemory:\n-            ProfileEvents::increment(ProfileEvents::MergedIntoInMemoryParts);\n-            break;\n         default:\n             break;\n     }\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 76e076413832..ce5c6a730e9b 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -226,7 +226,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     using OperationDataPartsLock = std::unique_lock<std::mutex>;\n     OperationDataPartsLock lockOperationsWithParts() const { return OperationDataPartsLock(operation_with_data_parts_mutex); }\n \n-    MergeTreeDataPartFormat choosePartFormat(size_t bytes_uncompressed, size_t rows_count, bool only_on_disk = false) const;\n+    MergeTreeDataPartFormat choosePartFormat(size_t bytes_uncompressed, size_t rows_count) const;\n     MergeTreeDataPartFormat choosePartFormatOnDisk(size_t bytes_uncompressed, size_t rows_count) const;\n     MergeTreeDataPartBuilder getDataPartBuilder(const String & name, const VolumePtr & volume, const String & part_dir) const;\n \n@@ -661,9 +661,6 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Removes parts from data_parts, they should be in Deleting state\n     void removePartsFinally(const DataPartsVector & parts);\n \n-    /// When WAL is not enabled, the InMemoryParts need to be persistent.\n-    void flushAllInMemoryPartsIfNeeded();\n-\n     /// Delete irrelevant parts from memory and disk.\n     /// If 'force' - don't wait for old_parts_lifetime.\n     size_t clearOldPartsFromFilesystem(bool force = false);\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartType.h b/src/Storages/MergeTree/MergeTreeDataPartType.h\nindex bd2acb9ef657..8b06da5167e6 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartType.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartType.h\n@@ -44,7 +44,7 @@ class MergeTreeDataPartType\n         /// Data of all columns is stored in one file. Marks are also stored in single file.\n         Compact,\n \n-        /// Format with buffering data in RAM.\n+        /// Format with buffering data in RAM. Obsolete - new parts cannot be created in this format.\n         InMemory,\n \n         Unknown,\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 5416b77a97e3..87a5449cb83f 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -33,10 +33,6 @@ struct Settings;\n     /** Data storing format settings. */ \\\n     M(UInt64, min_bytes_for_wide_part, 10485760, \"Minimal uncompressed size in bytes to create part in wide format instead of compact\", 0) \\\n     M(UInt64, min_rows_for_wide_part, 0, \"Minimal number of rows to create part in wide format instead of compact\", 0) \\\n-    M(UInt64, min_bytes_for_compact_part, 0, \"Experimental. Minimal uncompressed size in bytes to create part in compact format instead of saving it in RAM\", 0) \\\n-    M(UInt64, min_rows_for_compact_part, 0, \"Experimental. Minimal number of rows to create part in compact format instead of saving it in RAM\", 0) \\\n-    M(Bool, in_memory_parts_enable_wal, true, \"Whether to write blocks in Native format to write-ahead-log before creation in-memory part\", 0) \\\n-    M(UInt64, write_ahead_log_max_bytes, 1024 * 1024 * 1024, \"Rotate WAL, if it exceeds that amount of bytes\", 0) \\\n     M(Float, ratio_of_defaults_for_sparse_serialization, 1.0, \"Minimal ratio of number of default values to number of all values in column to store it in sparse serializations. If >= 1, columns will be always written in full serialization.\", 0) \\\n     \\\n     /** Merge settings. */ \\\n@@ -59,9 +55,6 @@ struct Settings;\n     M(UInt64, min_compressed_bytes_to_fsync_after_fetch, 0, \"Minimal number of compressed bytes to do fsync for part after fetch (0 - disabled)\", 0) \\\n     M(Bool, fsync_after_insert, false, \"Do fsync for every inserted part. Significantly decreases performance of inserts, not recommended to use with wide parts.\", 0) \\\n     M(Bool, fsync_part_directory, false, \"Do fsync for part directory after all part operations (writes, renames, etc.).\", 0) \\\n-    M(UInt64, write_ahead_log_bytes_to_fsync, 100ULL * 1024 * 1024, \"Amount of bytes, accumulated in WAL to do fsync.\", 0) \\\n-    M(UInt64, write_ahead_log_interval_ms_to_fsync, 100, \"Interval in milliseconds after which fsync for WAL is being done.\", 0) \\\n-    M(Bool, in_memory_parts_insert_sync, false, \"If true insert of part with in-memory format will wait for fsync of WAL\", 0) \\\n     M(UInt64, non_replicated_deduplication_window, 0, \"How many last blocks of hashes should be kept on disk (0 - disabled).\", 0) \\\n     M(UInt64, max_parts_to_merge_at_once, 100, \"Max amount of parts which can be merged at once (0 - disabled). Doesn't affect OPTIMIZE FINAL query.\", 0) \\\n     M(UInt64, merge_selecting_sleep_ms, 5000, \"Sleep time for merge selecting when no part selected, a lower setting will trigger selecting tasks in background_schedule_pool frequently which result in large amount of requests to zookeeper in large-scale clusters\", 0) \\\n@@ -188,7 +181,14 @@ struct Settings;\n     M(UInt64, replicated_max_parallel_sends_for_table, 0, \"Obsolete setting, does nothing.\", 0) \\\n     M(UInt64, replicated_max_parallel_fetches, 0, \"Obsolete setting, does nothing.\", 0) \\\n     M(UInt64, replicated_max_parallel_fetches_for_table, 0, \"Obsolete setting, does nothing.\", 0) \\\n-    M(Bool, write_final_mark, true, \"Obsolete setting, does nothing.\", 0) \\\n+    M(Bool, write_final_mark, true, \"Obsolete setting, does nothing.\", 0)                                                                                                                                                                                            \\\n+    M(UInt64, min_bytes_for_compact_part, 0, \"Obsolete setting, does nothing.\", 0) \\\n+    M(UInt64, min_rows_for_compact_part, 0, \"Obsolete setting, does nothing.\", 0) \\\n+    M(Bool, in_memory_parts_enable_wal, true, \"Obsolete setting, does nothing.\", 0) \\\n+    M(UInt64, write_ahead_log_max_bytes, 1024 * 1024 * 1024, \"Obsolete setting, does nothing.\", 0) \\\n+    M(UInt64, write_ahead_log_bytes_to_fsync, 100ULL * 1024 * 1024, \"Obsolete setting, does nothing.\", 0) \\\n+    M(UInt64, write_ahead_log_interval_ms_to_fsync, 100, \"Obsolete setting, does nothing.\", 0) \\\n+    M(Bool, in_memory_parts_insert_sync, false, \"Obsolete setting, does nothing.\", 0) \\\n     /// Settings that should not change after the creation of a table.\n     /// NOLINTNEXTLINE\n #define APPLY_FOR_IMMUTABLE_MERGE_TREE_SETTINGS(M) \\\n@@ -216,8 +216,7 @@ struct MergeTreeSettings : public BaseSettings<MergeTreeSettingsTraits>\n \n     static bool isPartFormatSetting(const String & name)\n     {\n-        return name == \"min_bytes_for_wide_part\" || name == \"min_rows_for_wide_part\"\n-            || name == \"min_bytes_for_compact_part\" || name == \"min_rows_for_compact_part\";\n+        return name == \"min_bytes_for_wide_part\" || name == \"min_rows_for_wide_part\";\n     }\n \n     /// Check that the values are sane taking also query-level settings into account.\ndiff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\nindex 23832464758b..39c4157a42e9 100644\n--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n@@ -84,31 +84,6 @@ void MergeTreeWriteAheadLog::init()\n     bytes_at_last_sync = 0;\n }\n \n-void MergeTreeWriteAheadLog::addPart(DataPartInMemoryPtr & part)\n-{\n-    std::unique_lock lock(write_mutex);\n-\n-    auto part_info = MergeTreePartInfo::fromPartName(part->name, storage.format_version);\n-    min_block_number = std::min(min_block_number, part_info.min_block);\n-    max_block_number = std::max(max_block_number, part_info.max_block);\n-\n-    writeIntBinary(WAL_VERSION, *out);\n-\n-    ActionMetadata metadata{};\n-    metadata.part_uuid = part->uuid;\n-    metadata.write(*out);\n-\n-    writeIntBinary(static_cast<UInt8>(ActionType::ADD_PART), *out);\n-    writeStringBinary(part->name, *out);\n-    block_out->write(part->block);\n-    block_out->flush();\n-    sync(lock);\n-\n-    auto max_wal_bytes = storage.getSettings()->write_ahead_log_max_bytes;\n-    if (out->count() > max_wal_bytes)\n-        rotate(lock);\n-}\n-\n void MergeTreeWriteAheadLog::dropPart(const String & part_name)\n {\n     std::unique_lock lock(write_mutex);\n@@ -121,7 +96,6 @@ void MergeTreeWriteAheadLog::dropPart(const String & part_name)\n     writeIntBinary(static_cast<UInt8>(ActionType::DROP_PART), *out);\n     writeStringBinary(part_name, *out);\n     out->next();\n-    sync(lock);\n }\n \n void MergeTreeWriteAheadLog::rotate(const std::unique_lock<std::mutex> &)\n@@ -269,27 +243,6 @@ MergeTreeData::MutableDataPartsVector MergeTreeWriteAheadLog::restore(\n     return result;\n }\n \n-void MergeTreeWriteAheadLog::sync(std::unique_lock<std::mutex> & lock)\n-{\n-    size_t bytes_to_sync = storage.getSettings()->write_ahead_log_bytes_to_fsync;\n-    time_t time_to_sync = storage.getSettings()->write_ahead_log_interval_ms_to_fsync;\n-    size_t current_bytes = out->count();\n-\n-    if (bytes_to_sync && current_bytes - bytes_at_last_sync > bytes_to_sync)\n-    {\n-        sync_task->schedule();\n-        bytes_at_last_sync = current_bytes;\n-    }\n-    else if (time_to_sync && !sync_scheduled)\n-    {\n-        sync_task->scheduleAfter(time_to_sync);\n-        sync_scheduled = true;\n-    }\n-\n-    if (storage.getSettings()->in_memory_parts_insert_sync)\n-        sync_cv.wait(lock, [this] { return !sync_scheduled; });\n-}\n-\n void MergeTreeWriteAheadLog::shutdown()\n {\n     {\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 5f482a82400d..971314e31497 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -168,14 +168,6 @@ void StorageMergeTree::startup()\n     }\n }\n \n-void StorageMergeTree::flush()\n-{\n-    if (flush_called.exchange(true))\n-        return;\n-\n-    flushAllInMemoryPartsIfNeeded();\n-}\n-\n void StorageMergeTree::shutdown()\n {\n     if (shutdown_called.exchange(true))\ndiff --git a/src/Storages/StorageMergeTree.h b/src/Storages/StorageMergeTree.h\nindex 66780402a41d..884e2f02825a 100644\n--- a/src/Storages/StorageMergeTree.h\n+++ b/src/Storages/StorageMergeTree.h\n@@ -45,7 +45,6 @@ class StorageMergeTree final : public MergeTreeData\n         bool has_force_restore_data_flag);\n \n     void startup() override;\n-    void flush() override;\n     void shutdown() override;\n \n     ~StorageMergeTree() override;\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex b548b1efd93d..2e9a1b814202 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -4455,14 +4455,6 @@ void StorageReplicatedMergeTree::startupImpl(bool from_attach_thread)\n     }\n }\n \n-void StorageReplicatedMergeTree::flush()\n-{\n-    if (flush_called.exchange(true))\n-        return;\n-\n-    flushAllInMemoryPartsIfNeeded();\n-}\n-\n \n void StorageReplicatedMergeTree::partialShutdown()\n {\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 8c8cc986c4c0..29b6a4d6817d 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -113,7 +113,6 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     void startup() override;\n     void shutdown() override;\n     void partialShutdown();\n-    void flush() override;\n     ~StorageReplicatedMergeTree() override;\n \n     static String getDefaultZooKeeperPath(const Poco::Util::AbstractConfiguration & config);\ndiff --git a/utils/CMakeLists.txt b/utils/CMakeLists.txt\nindex 5519b9bb9837..cbb99f2a02cc 100644\n--- a/utils/CMakeLists.txt\n+++ b/utils/CMakeLists.txt\n@@ -23,7 +23,6 @@ if (NOT DEFINED ENABLE_UTILS OR ENABLE_UTILS)\n     add_subdirectory (wikistat-loader)\n     add_subdirectory (check-marks)\n     add_subdirectory (checksum-for-compressed-block)\n-    add_subdirectory (wal-dump)\n     add_subdirectory (check-mysql-binlog)\n     add_subdirectory (keeper-bench)\n     add_subdirectory (keeper-data-dumper)\ndiff --git a/utils/wal-dump/CMakeLists.txt b/utils/wal-dump/CMakeLists.txt\ndeleted file mode 100644\nindex 3d59e95b4cab..000000000000\n--- a/utils/wal-dump/CMakeLists.txt\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-clickhouse_add_executable (wal-dump main.cpp)\n-target_link_libraries(wal-dump PRIVATE dbms boost::program_options)\ndiff --git a/utils/wal-dump/main.cpp b/utils/wal-dump/main.cpp\ndeleted file mode 100644\nindex 3566936324b7..000000000000\n--- a/utils/wal-dump/main.cpp\n+++ /dev/null\n@@ -1,78 +0,0 @@\n-#include <iostream>\n-\n-#include <boost/program_options.hpp>\n-\n-#include <Compression/CompressedReadBuffer.h>\n-#include <Compression/CompressedReadBufferFromFile.h>\n-#include <Compression/CompressedWriteBuffer.h>\n-#include <Formats/NativeReader.h>\n-#include <IO/Operators.h>\n-#include <IO/ReadBufferFromFile.h>\n-#include <IO/ReadHelpers.h>\n-#include <IO/WriteBufferFromFileDescriptor.h>\n-#include <Storages/MergeTree/MergeTreeWriteAheadLog.h>\n-\n-/*\n- * Dump the Write Ahead Log file, outputs:\n- * Part 0, Version: 0, Action : ADD_PART, Name: 4_1_1_0, Block:\n-   a Int32 Int32(size = 2), b Int32 Int32(size = 2), c Int32 Int32(size = 2)\n- */\n-\n-static void dump(const std::string & bin_path)\n-{\n-    DB::ReadBufferFromFile in(bin_path);\n-    DB::NativeReader block_in(in, 0);\n-    DB::Block block;\n-\n-    DB::WriteBufferFromFileDescriptor out(STDOUT_FILENO);\n-\n-    for (size_t part_num = 0; !in.eof(); ++part_num)\n-    {\n-        UInt8 version;\n-        String part_name;\n-        DB::MergeTreeWriteAheadLog::ActionType action_type;\n-\n-        DB::readIntBinary(version, in);\n-        DB::readIntBinary(action_type, in);\n-        DB::readStringBinary(part_name, in);\n-        block = block_in.read();\n-\n-        out << \"Part \" << part_num << \", Version: \" << version\n-            << \", Action : \" << (action_type == DB::MergeTreeWriteAheadLog::ActionType::ADD_PART ? \"ADD_PART\" : \"DROP_PART\")\n-            << \", Name: \" << part_name << \", Block:\\n\";\n-        out << block.dumpStructure() << \"\\n\";\n-        out << \"\\n\" << DB::flush;\n-    }\n-}\n-\n-\n-int main(int argc, char ** argv)\n-{\n-    boost::program_options::options_description desc(\"Allowed options\");\n-    desc.add_options()(\"help,h\", \"produce help message\");\n-\n-    boost::program_options::variables_map options;\n-    boost::program_options::store(boost::program_options::parse_command_line(argc, argv, desc), options);\n-\n-    if (options.count(\"help\") || argc != 2)\n-    {\n-        std::cout << \"Usage: \" << argv[0] << \" wal.bin\" << std::endl;\n-        std::cout << desc << std::endl;\n-        return 1;\n-    }\n-\n-    try\n-    {\n-        dump(argv[1]);\n-    }\n-    catch (const DB::Exception & e)\n-    {\n-        std::cerr << e.what() << \", \" << e.message() << std::endl\n-                  << std::endl\n-                  << \"Stack trace:\" << std::endl\n-                  << e.getStackTraceString() << std::endl;\n-        throw;\n-    }\n-\n-    return 0;\n-}\n",
  "test_patch": "diff --git a/tests/integration/test_backup_restore/test.py b/tests/integration/test_backup_restore/test.py\nindex 193e638186ca..b91a7cfe1c83 100644\n--- a/tests/integration/test_backup_restore/test.py\n+++ b/tests/integration/test_backup_restore/test.py\n@@ -171,39 +171,3 @@ def test_replace_partition(started_cluster):\n     assert TSV(res) == expected\n \n     instance.query(\"DROP TABLE IF EXISTS test.tbl3\")\n-\n-\n-def test_freeze_in_memory(started_cluster):\n-    instance.query(\n-        \"CREATE TABLE test.t_in_memory(a UInt32, s String) ENGINE = MergeTree ORDER BY a SETTINGS min_rows_for_compact_part = 1000\"\n-    )\n-    instance.query(\"INSERT INTO test.t_in_memory VALUES (1, 'a')\")\n-    instance.query(\"ALTER TABLE test.t_in_memory FREEZE\")\n-\n-    fp_backup = get_last_backup_path(\n-        started_cluster.instances[\"node\"], \"test\", \"t_in_memory\"\n-    )\n-    part_path = fp_backup + \"/all_1_1_0/\"\n-\n-    assert TSV(\n-        instance.query(\n-            \"SELECT part_type, is_frozen FROM system.parts WHERE database = 'test' AND table = 't_in_memory'\"\n-        )\n-    ) == TSV(\"InMemory\\t1\\n\")\n-    instance.exec_in_container([\"test\", \"-f\", part_path + \"/data.bin\"])\n-    assert instance.exec_in_container([\"cat\", part_path + \"/count.txt\"]).strip() == \"1\"\n-\n-    instance.query(\n-        \"CREATE TABLE test.t_in_memory_2(a UInt32, s String) ENGINE = MergeTree ORDER BY a\"\n-    )\n-    copy_backup_to_detached(\n-        started_cluster.instances[\"node\"], \"test\", \"t_in_memory\", \"t_in_memory_2\"\n-    )\n-\n-    instance.query(\"ALTER TABLE test.t_in_memory_2 ATTACH PARTITION ID 'all'\")\n-    assert TSV(\n-        instance.query(\n-            \"SELECT part_type FROM system.parts WHERE database = 'test' AND table = 't_in_memory_2'\"\n-        )\n-    ) == TSV(\"Compact\\n\")\n-    assert TSV(instance.query(\"SELECT a, s FROM test.t_in_memory_2\")) == TSV(\"1\\ta\\n\")\ndiff --git a/tests/integration/test_backward_compatibility/test_in_memory_parts_still_read.py b/tests/integration/test_backward_compatibility/test_in_memory_parts_still_read.py\nnew file mode 100644\nindex 000000000000..d55f155918e6\n--- /dev/null\n+++ b/tests/integration/test_backward_compatibility/test_in_memory_parts_still_read.py\n@@ -0,0 +1,44 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+# Version 23.4 is the latest version to support writing in-memory parts.\n+node = cluster.add_instance(\n+    \"node_old\",\n+    image=\"clickhouse/clickhouse-server\",\n+    tag=\"23.4\",\n+    stay_alive=True,\n+    with_installed_binary=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_in_memory_parts_still_read(start_cluster):\n+    node.query(\n+        \"CREATE TABLE t (x UInt64, s String, a Array(Tuple(Map(String, LowCardinality(String)), Date32, DateTime64(3)))) ENGINE = MergeTree ORDER BY s SETTINGS min_rows_for_compact_part = 1000000, min_bytes_for_compact_part = '1G', in_memory_parts_enable_wal = 1\"\n+    )\n+    node.query(\"INSERT INTO t SELECT * FROM generateRandom() LIMIT 100\")\n+\n+    assert node.query(\"SELECT count() FROM t WHERE NOT ignore(*)\") == \"100\\n\"\n+\n+    node.restart_with_latest_version()\n+    assert node.query(\"SELECT count() FROM t WHERE NOT ignore(*)\") == \"100\\n\"\n+\n+    node.query(\"INSERT INTO t SELECT * FROM generateRandom() LIMIT 100\")\n+\n+    assert node.query(\"SELECT count() FROM t WHERE NOT ignore(*)\") == \"200\\n\"\n+\n+    node.restart_with_original_version()\n+    assert node.query(\"SELECT count() FROM t WHERE NOT ignore(*)\") == \"200\\n\"\ndiff --git a/tests/integration/test_part_uuid/test.py b/tests/integration/test_part_uuid/test.py\nindex b30dd884427f..b1eb1280d56c 100644\n--- a/tests/integration/test_part_uuid/test.py\n+++ b/tests/integration/test_part_uuid/test.py\n@@ -108,48 +108,3 @@ def test_part_uuid(started_cluster):\n         uuids.add(part_merge_uuid)\n         assert part_mutate_uuid not in [uuid_zero, part_merge_uuid]\n     assert len(uuids) == 1, \"expect the same uuid on all the replicas\"\n-\n-\n-def test_part_uuid_wal(started_cluster):\n-    uuid_zero = uuid.UUID(bytes=b\"\\x00\" * 16)\n-\n-    for ix, n in enumerate([node1, node2]):\n-        n.query(\n-            \"\"\"\n-        CREATE TABLE t_wal(key UInt64, value UInt64)\n-        ENGINE ReplicatedMergeTree('/clickhouse/tables/t_wal', '{}')\n-        ORDER BY tuple()\n-        \"\"\".format(\n-                ix\n-            )\n-        )\n-\n-    node2.query(\"INSERT INTO t_wal VALUES (1, 1)\")\n-\n-    uuids = set()\n-    for node in [node1, node2]:\n-        node.query(\"SYSTEM SYNC REPLICA t_wal\")\n-        part_initial_uuid = uuid.UUID(\n-            node.query(\n-                \"SELECT uuid FROM system.parts WHERE table = 't_wal' AND active ORDER BY name\"\n-            ).strip()\n-        )\n-        assert (\n-            \"InMemory\"\n-            == node.query(\n-                \"SELECT part_type FROM system.parts WHERE table = 't_wal' AND active ORDER BY name\"\n-            ).strip()\n-        )\n-        uuids.add(part_initial_uuid)\n-        assert uuid_zero != part_initial_uuid\n-    assert len(uuids) == 1, \"expect the same uuid on all the replicas\"\n-\n-    # Test detach / attach table to trigger WAL processing.\n-    for node in [node1, node2]:\n-        node.query(\"DETACH TABLE t_wal; ATTACH TABLE t_wal\")\n-        part_reattach_uuid = uuid.UUID(\n-            node.query(\n-                \"SELECT uuid FROM system.parts WHERE table = 't_wal' AND active ORDER BY name\"\n-            ).strip()\n-        )\n-        assert part_initial_uuid == part_reattach_uuid\ndiff --git a/tests/integration/test_polymorphic_parts/test.py b/tests/integration/test_polymorphic_parts/test.py\nindex 361b48557475..fb1f363b8253 100644\n--- a/tests/integration/test_polymorphic_parts/test.py\n+++ b/tests/integration/test_polymorphic_parts/test.py\n@@ -49,9 +49,8 @@ def create_tables(name, nodes, node_settings, shard):\n             PARTITION BY toYYYYMM(date)\n             ORDER BY id\n             SETTINGS index_granularity = 64, index_granularity_bytes = {index_granularity_bytes},\n-            min_rows_for_wide_part = {min_rows_for_wide_part}, min_rows_for_compact_part = {min_rows_for_compact_part},\n-            min_bytes_for_wide_part = 0, min_bytes_for_compact_part = 0,\n-            in_memory_parts_enable_wal = 1\n+            min_rows_for_wide_part = {min_rows_for_wide_part},\n+            min_bytes_for_wide_part = 0\n             \"\"\".format(\n                 name=name, shard=shard, repl=i, **settings\n             )\n@@ -87,17 +86,14 @@ def create_tables_old_format(name, nodes, shard):\n settings_default = {\n     \"index_granularity_bytes\": 10485760,\n     \"min_rows_for_wide_part\": 512,\n-    \"min_rows_for_compact_part\": 0,\n }\n settings_compact_only = {\n     \"index_granularity_bytes\": 10485760,\n     \"min_rows_for_wide_part\": 1000000,\n-    \"min_rows_for_compact_part\": 0,\n }\n settings_not_adaptive = {\n     \"index_granularity_bytes\": 0,\n     \"min_rows_for_wide_part\": 512,\n-    \"min_rows_for_compact_part\": 0,\n }\n \n node3 = cluster.add_instance(\n@@ -116,12 +112,10 @@ def create_tables_old_format(name, nodes, shard):\n settings_compact = {\n     \"index_granularity_bytes\": 10485760,\n     \"min_rows_for_wide_part\": 512,\n-    \"min_rows_for_compact_part\": 0,\n }\n settings_wide = {\n     \"index_granularity_bytes\": 10485760,\n     \"min_rows_for_wide_part\": 0,\n-    \"min_rows_for_compact_part\": 0,\n }\n \n node5 = cluster.add_instance(\n@@ -131,12 +125,6 @@ def create_tables_old_format(name, nodes, shard):\n     \"node6\", main_configs=[\"configs/compact_parts.xml\"], with_zookeeper=True\n )\n \n-settings_in_memory = {\n-    \"index_granularity_bytes\": 10485760,\n-    \"min_rows_for_wide_part\": 512,\n-    \"min_rows_for_compact_part\": 256,\n-}\n-\n node9 = cluster.add_instance(\"node9\", with_zookeeper=True, stay_alive=True)\n node10 = cluster.add_instance(\"node10\", with_zookeeper=True)\n \n@@ -190,42 +178,6 @@ def start_cluster():\n             \"shard2\",\n         )\n         create_tables_old_format(\"polymorphic_table\", [node5, node6], \"shard3\")\n-        create_tables(\n-            \"in_memory_table\",\n-            [node9, node10],\n-            [settings_in_memory, settings_in_memory],\n-            \"shard4\",\n-        )\n-        create_tables(\n-            \"wal_table\",\n-            [node11, node12],\n-            [settings_in_memory, settings_in_memory],\n-            \"shard4\",\n-        )\n-        create_tables(\n-            \"restore_table\",\n-            [node11, node12],\n-            [settings_in_memory, settings_in_memory],\n-            \"shard5\",\n-        )\n-        create_tables(\n-            \"deduplication_table\",\n-            [node9, node10],\n-            [settings_in_memory, settings_in_memory],\n-            \"shard5\",\n-        )\n-        create_tables(\n-            \"sync_table\",\n-            [node9, node10],\n-            [settings_in_memory, settings_in_memory],\n-            \"shard5\",\n-        )\n-        create_tables(\n-            \"alters_table\",\n-            [node9, node10],\n-            [settings_in_memory, settings_in_memory],\n-            \"shard5\",\n-        )\n \n         yield cluster\n \n@@ -422,7 +374,6 @@ def test_different_part_types_on_replicas(start_cluster, table, part_type):\n settings8 = {\n     \"index_granularity_bytes\": 10485760,\n     \"min_rows_for_wide_part\": 512,\n-    \"min_rows_for_compact_part\": 0,\n }\n \n \n@@ -538,187 +489,6 @@ def test_polymorphic_parts_non_adaptive(start_cluster):\n     )\n \n \n-def test_in_memory(start_cluster):\n-    node9.query(\"SYSTEM STOP MERGES\")\n-    node10.query(\"SYSTEM STOP MERGES\")\n-\n-    for size in [200, 200, 300, 600]:\n-        insert_random_data(\"in_memory_table\", node9, size)\n-    node10.query(\"SYSTEM SYNC REPLICA in_memory_table\", timeout=20)\n-\n-    assert node9.query(\"SELECT count() FROM in_memory_table\") == \"1300\\n\"\n-    assert node10.query(\"SELECT count() FROM in_memory_table\") == \"1300\\n\"\n-\n-    expected = \"Compact\\t1\\nInMemory\\t2\\nWide\\t1\\n\"\n-\n-    assert TSV(\n-        node9.query(\n-            \"SELECT part_type, count() FROM system.parts \"\n-            \"WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type\"\n-        )\n-    ) == TSV(expected)\n-    assert TSV(\n-        node10.query(\n-            \"SELECT part_type, count() FROM system.parts \"\n-            \"WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type\"\n-        )\n-    ) == TSV(expected)\n-\n-    node9.query(\"SYSTEM START MERGES\")\n-    node10.query(\"SYSTEM START MERGES\")\n-\n-    assert_eq_with_retry(\n-        node9,\n-        \"OPTIMIZE TABLE in_memory_table FINAL SETTINGS optimize_throw_if_noop = 1\",\n-        \"\",\n-    )\n-    node10.query(\"SYSTEM SYNC REPLICA in_memory_table\", timeout=20)\n-\n-    assert node9.query(\"SELECT count() FROM in_memory_table\") == \"1300\\n\"\n-    assert node10.query(\"SELECT count() FROM in_memory_table\") == \"1300\\n\"\n-\n-    assert TSV(\n-        node9.query(\n-            \"SELECT part_type, count() FROM system.parts \"\n-            \"WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type\"\n-        )\n-    ) == TSV(\"Wide\\t1\\n\")\n-    assert TSV(\n-        node10.query(\n-            \"SELECT part_type, count() FROM system.parts \"\n-            \"WHERE table = 'in_memory_table' AND active GROUP BY part_type ORDER BY part_type\"\n-        )\n-    ) == TSV(\"Wide\\t1\\n\")\n-\n-\n-def test_in_memory_wal_rotate(start_cluster):\n-    # Write every part to single wal\n-    node11.query(\n-        \"ALTER TABLE restore_table MODIFY SETTING write_ahead_log_max_bytes = 10\"\n-    )\n-    for i in range(5):\n-        insert_random_data(\"restore_table\", node11, 50)\n-\n-    for i in range(5):\n-        # Check file exists\n-        node11.exec_in_container(\n-            [\n-                \"bash\",\n-                \"-c\",\n-                \"test -f /var/lib/clickhouse/data/default/restore_table/wal_{0}_{0}.bin\".format(\n-                    i\n-                ),\n-            ]\n-        )\n-\n-    for node in [node11, node12]:\n-        node.query(\n-            \"ALTER TABLE restore_table MODIFY SETTING number_of_free_entries_in_pool_to_lower_max_size_of_merge = 0\"\n-        )\n-        node.query(\n-            \"ALTER TABLE restore_table MODIFY SETTING max_bytes_to_merge_at_max_space_in_pool = 10000000\"\n-        )\n-\n-    assert_eq_with_retry(\n-        node11,\n-        \"OPTIMIZE TABLE restore_table FINAL SETTINGS optimize_throw_if_noop = 1\",\n-        \"\",\n-    )\n-    # Restart to be sure, that clearing stale logs task was ran\n-    node11.restart_clickhouse(kill=True)\n-\n-    for i in range(5):\n-        # check file doesn't exist\n-        node11.exec_in_container(\n-            [\n-                \"bash\",\n-                \"-c\",\n-                \"test ! -e /var/lib/clickhouse/data/default/restore_table/wal_{0}_{0}.bin\".format(\n-                    i\n-                ),\n-            ]\n-        )\n-\n-    # New wal file was created and ready to write part to it\n-    # Check file exists\n-    node11.exec_in_container(\n-        [\"bash\", \"-c\", \"test -f /var/lib/clickhouse/data/default/restore_table/wal.bin\"]\n-    )\n-    # Chech file empty\n-    node11.exec_in_container(\n-        [\n-            \"bash\",\n-            \"-c\",\n-            \"test ! -s /var/lib/clickhouse/data/default/restore_table/wal.bin\",\n-        ]\n-    )\n-\n-\n-def test_in_memory_deduplication(start_cluster):\n-    for i in range(3):\n-        # table can be in readonly node\n-        exec_query_with_retry(\n-            node9,\n-            \"INSERT INTO deduplication_table (date, id, s) VALUES (toDate('2020-03-03'), 1, 'foo')\",\n-        )\n-        exec_query_with_retry(\n-            node10,\n-            \"INSERT INTO deduplication_table (date, id, s) VALUES (toDate('2020-03-03'), 1, 'foo')\",\n-        )\n-\n-    node9.query(\"SYSTEM SYNC REPLICA deduplication_table\", timeout=20)\n-    node10.query(\"SYSTEM SYNC REPLICA deduplication_table\", timeout=20)\n-\n-    assert (\n-        node9.query(\"SELECT date, id, s FROM deduplication_table\")\n-        == \"2020-03-03\\t1\\tfoo\\n\"\n-    )\n-    assert (\n-        node10.query(\"SELECT date, id, s FROM deduplication_table\")\n-        == \"2020-03-03\\t1\\tfoo\\n\"\n-    )\n-\n-\n-# Checks that restoring from WAL works after table schema changed\n-def test_in_memory_alters(start_cluster):\n-    def check_parts_type(parts_num):\n-        assert (\n-            node9.query(\n-                \"SELECT part_type, count() FROM system.parts WHERE table = 'alters_table' \\\n-             AND active GROUP BY part_type\"\n-            )\n-            == \"InMemory\\t{}\\n\".format(parts_num)\n-        )\n-\n-    node9.query(\n-        \"INSERT INTO alters_table (date, id, s) VALUES (toDate('2020-10-10'), 1, 'ab'), (toDate('2020-10-10'), 2, 'cd')\"\n-    )\n-    node9.query(\"ALTER TABLE alters_table ADD COLUMN col1 UInt32\")\n-    node9.restart_clickhouse(kill=True)\n-\n-    expected = \"1\\tab\\t0\\n2\\tcd\\t0\\n\"\n-    assert node9.query(\"SELECT id, s, col1 FROM alters_table ORDER BY id\") == expected\n-    check_parts_type(1)\n-    node9.query(\n-        \"INSERT INTO alters_table (date, id, col1) VALUES (toDate('2020-10-10'), 3, 100)\"\n-    )\n-    node9.query(\"ALTER TABLE alters_table MODIFY COLUMN col1 String\")\n-    node9.query(\"ALTER TABLE alters_table DROP COLUMN s\")\n-    node9.restart_clickhouse(kill=True)\n-\n-    check_parts_type(2)\n-    with pytest.raises(Exception):\n-        node9.query(\"SELECT id, s, col1 FROM alters_table\")\n-\n-    # Values of col1 was not materialized as integers, so they have\n-    # default string values after alter\n-    expected = \"1\\t_foo\\n2\\t_foo\\n3\\t100_foo\\n\"\n-    assert (\n-        node9.query(\"SELECT id, col1 || '_foo' FROM alters_table ORDER BY id\")\n-        == expected\n-    )\n-\n-\n def test_polymorphic_parts_index(start_cluster):\n     node1.query(\n         \"CREATE DATABASE test_index ENGINE=Ordinary\",\ndiff --git a/tests/performance/polymorphic_parts_s.xml b/tests/performance/polymorphic_parts_s.xml\nindex 5021e135bb92..b4dd87a7ae3d 100644\n--- a/tests/performance/polymorphic_parts_s.xml\n+++ b/tests/performance/polymorphic_parts_s.xml\n@@ -13,13 +13,6 @@\n         SAMPLE BY intHash32(UserID)\n         SETTINGS min_bytes_for_wide_part = '10M'\n     </create_query>\n-    <create_query>\n-        CREATE TABLE hits_memory AS hits_10m_single ENGINE = MergeTree()\n-        PARTITION BY toYYYYMM(EventDate)\n-        ORDER BY (CounterID, EventDate, intHash32(UserID))\n-        SAMPLE BY intHash32(UserID)\n-        SETTINGS min_bytes_for_compact_part = '1M', min_bytes_for_wide_part = '10M', in_memory_parts_enable_wal = 1\n-    </create_query>\n     <create_query>\n        CREATE TABLE hits_buffer AS hits_10m_single \n        ENGINE = Buffer(default, hits_wide, 1, 0, 0, 10000, 10000, 0, 0)\n@@ -38,6 +31,5 @@\n \n     <drop_query>DROP TABLE IF EXISTS hits_wide</drop_query>\n     <drop_query>DROP TABLE IF EXISTS hits_compact</drop_query>\n-    <drop_query>DROP TABLE IF EXISTS hits_memory</drop_query>\n     <drop_query>DROP TABLE IF EXISTS hits_buffer</drop_query>\n </test>\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts.reference b/tests/queries/0_stateless/01130_in_memory_parts.reference\ndeleted file mode 100644\nindex ad5435abb59b..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts.reference\n+++ /dev/null\n@@ -1,39 +0,0 @@\n-system.parts\n-InMemory\t2\n-1\n-1\n-Simple selects\n-0\t0\n-1\t1\n-2\t2\n-3\t0\n-4\t1\n-50\t2\n-51\t0\n-52\t1\n-53\t2\n-54\t0\n-34\n-0\n-20\n-10\n-Mutations and Alters\n-66\n-1\t1\n-2\t2\n-4\t1\n-5\t2\n-7\t1\n-[1,1]\n-[]\n-[4,16]\n-[]\n-[7,49]\n-1\t1\n-2\t1\n-1\t[1,1]\n-2\t[]\n-4\t[4,16]\n-5\t[]\n-7\t[7,49]\n-0\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts.sql b/tests/queries/0_stateless/01130_in_memory_parts.sql\ndeleted file mode 100644\nindex 2b15ae247638..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts.sql\n+++ /dev/null\n@@ -1,48 +0,0 @@\n--- Tags: no-s3-storage\n-\n-DROP TABLE IF EXISTS in_memory;\n-CREATE TABLE in_memory (a UInt32, b UInt32)\n-    ENGINE = MergeTree ORDER BY a\n-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_compact_part = 1000;\n-\n-INSERT INTO in_memory SELECT number, number % 3 FROM numbers(100);\n-SELECT 'system.parts';\n-SELECT DISTINCT part_type, marks FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory' AND active;\n-SELECT DISTINCT data_uncompressed_bytes > 0 FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory' AND active;\n-SELECT DISTINCT column_data_uncompressed_bytes > 0 FROM system.parts_columns WHERE database = currentDatabase() AND table = 'in_memory' AND active;\n-\n-SELECT 'Simple selects';\n-\n-SELECT * FROM in_memory ORDER BY a LIMIT 5;\n-SELECT * FROM in_memory ORDER BY a LIMIT 5 OFFSET 50;\n-SELECT count() FROM in_memory WHERE b = 0 SETTINGS max_block_size = 10;\n--- Check index\n-SELECT count() FROM in_memory WHERE a > 100 SETTINGS max_rows_to_read = 0, force_primary_key = 1;\n-SELECT count() FROM in_memory WHERE a >= 10 AND a < 30 SETTINGS force_primary_key = 1;\n-SELECT DISTINCT blockSize() FROM in_memory SETTINGS max_block_size = 10;\n-\n-SELECT 'Mutations and Alters';\n-SET mutations_sync = 1;\n-\n-ALTER TABLE in_memory DELETE WHERE b = 0;\n-\n-SELECT count() FROM in_memory;\n-SELECT * FROM in_memory ORDER BY a LIMIT 5;\n-\n-ALTER TABLE in_memory ADD COLUMN arr Array(UInt64);\n-ALTER TABLE in_memory UPDATE arr = [a, a * a] WHERE b = 1;\n-\n-SELECT arr FROM in_memory ORDER BY a LIMIT 5;\n-\n-ALTER TABLE in_memory MODIFY COLUMN b String;\n-ALTER TABLE in_memory RENAME COLUMN b to str;\n-SELECT DISTINCT str, length(str) FROM in_memory ORDER BY str;\n-ALTER TABLE in_memory DROP COLUMN str;\n-\n-SELECT * FROM in_memory ORDER BY a LIMIT 5;\n-\n--- in-memory parts works if they're empty.\n-ALTER TABLE in_memory DELETE WHERE 1;\n-SELECT count() FROM in_memory;\n-\n-DROP TABLE in_memory;\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_check.reference b/tests/queries/0_stateless/01130_in_memory_parts_check.reference\ndeleted file mode 100644\nindex 15f72836ff10..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_check.reference\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-201901_1_1_0\t1\t\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_check.sql b/tests/queries/0_stateless/01130_in_memory_parts_check.sql\ndeleted file mode 100644\nindex c2f5eba59491..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_check.sql\n+++ /dev/null\n@@ -1,13 +0,0 @@\n--- Tags: no-s3-storage\n-\n--- Part of 00961_check_table test, but with in-memory parts\n-\n-SET check_query_single_value_result = 0;\n-DROP TABLE IF EXISTS mt_table;\n-CREATE TABLE mt_table (d Date, key UInt64, data String) ENGINE = MergeTree() PARTITION BY toYYYYMM(d) ORDER BY key\n-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_compact_part = 1000;\n-\n-CHECK TABLE mt_table;\n-INSERT INTO mt_table VALUES (toDate('2019-01-02'), 1, 'Hello'), (toDate('2019-01-02'), 2, 'World');\n-CHECK TABLE mt_table;\n-DROP TABLE mt_table;\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_default.reference b/tests/queries/0_stateless/01130_in_memory_parts_default.reference\ndeleted file mode 100644\nindex c51afdb66589..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_default.reference\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-0\tbbb_aaa\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_default.sql b/tests/queries/0_stateless/01130_in_memory_parts_default.sql\ndeleted file mode 100644\nindex 776d5f89fcf4..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_default.sql\n+++ /dev/null\n@@ -1,21 +0,0 @@\n--- Tags: no-s3-storage\n--- Test 01266_default_prewhere_reqq, but with in-memory parts\n-DROP TABLE IF EXISTS t1;\n-\n-CREATE TABLE t1\n-(\n-    date Date, \n-    s1 String,\n-    s2 String\n-) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(date) ORDER BY (date, s1)\n-SETTINGS index_granularity = 8192, min_rows_for_compact_part = 1000, min_rows_for_wide_part = 1000;\n-\n-\n-set max_threads=1;\n-\n-insert into t1 (date, s1,s2) values(today()-1,'aaa','bbb');\n-alter table t1 add column s3 String DEFAULT concat(s2,'_',s1);\n--- insert into t1 (date, s1,s2) values(today(),'aaa2','bbb2');\n-select ignore(date), s3 from t1 where  s2='bbb';\n-\n-DROP TABLE t1;\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_nested.reference b/tests/queries/0_stateless/01130_in_memory_parts_nested.reference\ndeleted file mode 100644\nindex 3882abb94676..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_nested.reference\n+++ /dev/null\n@@ -1,25 +0,0 @@\n-[0]\n-[0,0,0]\n-[0,0,0,0,0]\n-[0,0,0,0,0,0,0]\n-[0,0,0,0,0,0,0,0,0]\n-[0]\n-[0,0,0]\n-[0,0,0,0,0]\n-[0,0,0,0,0,0,0]\n-[0,0,0,0,0,0,0,0,0]\n-[0]\n-[0,0,0]\n-[0,0,0,0,0]\n-[0,0,0,0,0,0,0]\n-[0,0,0,0,0,0,0,0,0]\n-[0]\n-[0,2,4]\n-[0,2,4,6,8]\n-[0,2,4,6,8,10,12]\n-[0,2,4,6,8,10,12,14,16]\n-[0]\t[0]\n-[0,1,2]\t[0,2,4]\n-[0,1,2,3,4]\t[0,2,4,6,8]\n-[0,1,2,3,4,5,6]\t[0,2,4,6,8,10,12]\n-[0,1,2,3,4,5,6,7,8]\t[0,2,4,6,8,10,12,14,16]\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_nested.sql b/tests/queries/0_stateless/01130_in_memory_parts_nested.sql\ndeleted file mode 100644\nindex 45e778b9f04f..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_nested.sql\n+++ /dev/null\n@@ -1,19 +0,0 @@\n--- Tags: no-s3-storage\n--- Test 00576_nested_and_prewhere, but with in-memory parts.\n-DROP TABLE IF EXISTS nested;\n-\n-CREATE TABLE nested (x UInt64, filter UInt8, n Nested(a UInt64)) ENGINE = MergeTree ORDER BY x\n-    SETTINGS min_rows_for_compact_part = 200000, min_rows_for_wide_part = 300000;\n-\n-INSERT INTO nested SELECT number, number % 2, range(number % 10) FROM system.numbers LIMIT 100000;\n-\n-ALTER TABLE nested ADD COLUMN n.b Array(UInt64);\n-SELECT DISTINCT n.b FROM nested PREWHERE filter;\n-SELECT DISTINCT n.b FROM nested PREWHERE filter SETTINGS max_block_size = 123;\n-SELECT DISTINCT n.b FROM nested PREWHERE filter SETTINGS max_block_size = 1234;\n-\n-ALTER TABLE nested ADD COLUMN n.c Array(UInt64) DEFAULT arrayMap(x -> x * 2, n.a);\n-SELECT DISTINCT n.c FROM nested PREWHERE filter;\n-SELECT DISTINCT n.a, n.c FROM nested PREWHERE filter;\n-\n-DROP TABLE nested;\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_partitons.reference b/tests/queries/0_stateless/01130_in_memory_parts_partitons.reference\ndeleted file mode 100644\nindex 44cbbed3f57f..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_partitons.reference\n+++ /dev/null\n@@ -1,60 +0,0 @@\n-1\t2\tfoo\n-1\t3\tbar\n-2\t4\taa\n-2\t5\tbb\n-2\t6\tcc\n-3\t7\tqq\n-3\t8\tww\n-3\t9\tee\n-3\t10\trr\n-1_1_1_0\tInMemory\t2\n-2_2_2_0\tInMemory\t3\n-3_3_3_0\tInMemory\t4\n-^ init ==================\n-2\t4\taa\n-2\t5\tbb\n-2\t6\tcc\n-3\t7\tqq\n-3\t8\tww\n-3\t9\tee\n-3\t10\trr\n-2_2_2_0\tInMemory\t3\n-3_3_3_0\tInMemory\t4\n-^ drop 1 ==================\n-3\t7\tqq\n-3\t8\tww\n-3\t9\tee\n-3\t10\trr\n-3_3_3_0\tInMemory\t4\n-^ detach 2 ==================\n-2\t4\taa\n-2\t5\tbb\n-2\t6\tcc\n-3\t7\tqq\n-3\t8\tww\n-3\t9\tee\n-3\t10\trr\n-2_4_4_0\tCompact\t3\n-3_3_3_0\tInMemory\t4\n-^ attach 2 =================\n-2\t4\taa\n-2\t5\tbb\n-2\t6\tcc\n-3\t7\tqq\n-3\t8\tww\n-3\t9\tee\n-3\t10\trr\n-2_4_4_0\tCompact\t3\n-3_3_3_0\tInMemory\t4\n-^ detach attach ==================\n-2\t4\taa\n-2\t5\tbb\n-2\t6\tcc\n-3\t11\ttt\n-3\t12\tyy\n-t2\t2_4_4_0\tCompact\t3\n-t2\t3_6_6_0\tCompact\t2\n-t3\t3_1_1_0\tInMemory\t2\n-^ replace ==================\n-3_1_1_0\tInMemory\t1\t2\n-^ freeze ==================\ndiff --git a/tests/queries/0_stateless/01130_in_memory_parts_partitons.sql b/tests/queries/0_stateless/01130_in_memory_parts_partitons.sql\ndeleted file mode 100644\nindex 18da2d2bd30a..000000000000\n--- a/tests/queries/0_stateless/01130_in_memory_parts_partitons.sql\n+++ /dev/null\n@@ -1,59 +0,0 @@\n--- Tags: no-parallel, no-s3-storage\n-\n-DROP TABLE IF EXISTS t2;\n-\n-CREATE TABLE t2(id UInt32, a UInt64, s String)\n-    ENGINE = MergeTree ORDER BY a PARTITION BY id\n-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_wide_part = 2000;\n-\n-SYSTEM STOP MERGES t2;\n-\n-INSERT INTO t2 VALUES (1, 2, 'foo'), (1, 3, 'bar');\n-INSERT INTO t2 VALUES (2, 4, 'aa'), (2, 5, 'bb'), (2, 6, 'cc');\n-INSERT INTO t2 VALUES (3, 7, 'qq'), (3, 8, 'ww'), (3, 9, 'ee'), (3, 10, 'rr');\n-\n-SELECT * FROM t2 ORDER BY a;\n-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT '^ init ==================';\n-\n-ALTER TABLE t2 DROP PARTITION 1;\n-SELECT * FROM t2 ORDER BY a;\n-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT '^ drop 1 ==================';\n-\n-ALTER TABLE t2 DETACH PARTITION 2;\n-SELECT * FROM t2 ORDER BY a;\n-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT '^ detach 2 ==================';\n-\n-ALTER TABLE t2 ATTACH PARTITION 2;\n-SELECT * FROM t2 ORDER BY a;\n-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT '^ attach 2 =================';\n-\n-DETACH TABLE t2;\n-ATTACH TABLE t2;\n-\n-SELECT * FROM t2 ORDER BY a;\n-SELECT name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT '^ detach attach ==================';\n-\n-DROP TABLE IF EXISTS t3;\n-\n-CREATE TABLE t3(id UInt32, a UInt64, s String)\n-    ENGINE = MergeTree ORDER BY a PARTITION BY id\n-    SETTINGS min_rows_for_compact_part = 1000, min_rows_for_wide_part = 2000;\n-\n-INSERT INTO t3 VALUES (3, 11, 'tt'), (3, 12, 'yy');\n-ALTER TABLE t2 REPLACE PARTITION 3 FROM t3;\n-SELECT * FROM t2 ORDER BY a;\n-SELECT table, name, part_type, rows FROM system.parts WHERE table = 't2' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT table, name, part_type, rows FROM system.parts WHERE table = 't3' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT '^ replace ==================';\n-\n-ALTER TABLE t3 FREEZE PARTITION 3;\n-SELECT name, part_type, is_frozen, rows FROM system.parts WHERE table = 't3' AND active AND database = currentDatabase() ORDER BY name;\n-SELECT '^ freeze ==================';\n-\n-DROP TABLE t2;\n-DROP TABLE t3;\ndiff --git a/tests/queries/0_stateless/01475_read_subcolumns_storages.reference b/tests/queries/0_stateless/01475_read_subcolumns_storages.reference\nindex 4e37b751d5aa..ce0b13eabd2c 100644\n--- a/tests/queries/0_stateless/01475_read_subcolumns_storages.reference\n+++ b/tests/queries/0_stateless/01475_read_subcolumns_storages.reference\n@@ -7,9 +7,6 @@ TinyLog\n Memory\n 100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\t{'foo':1,'bar':42}\n 100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\t{'foo':1,'bar':42}\t['foo','bar']\t[1,42]\n-MergeTree ORDER BY tuple() SETTINGS min_bytes_for_compact_part='10M'\n-100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\t{'foo':1,'bar':42}\n-100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\t{'foo':1,'bar':42}\t['foo','bar']\t[1,42]\n MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part='10M'\n 100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\t{'foo':1,'bar':42}\n 100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\t{'foo':1,'bar':42}\t['foo','bar']\t[1,42]\ndiff --git a/tests/queries/0_stateless/01475_read_subcolumns_storages.sh b/tests/queries/0_stateless/01475_read_subcolumns_storages.sh\nindex d770d5118ace..4a9b9639f78a 100755\n--- a/tests/queries/0_stateless/01475_read_subcolumns_storages.sh\n+++ b/tests/queries/0_stateless/01475_read_subcolumns_storages.sh\n@@ -12,7 +12,6 @@ create_query=\"CREATE TABLE subcolumns(n Nullable(UInt32), a1 Array(UInt32),\\\n \n # \"StripeLog\"\n declare -a ENGINES=(\"Log\" \"TinyLog\" \"Memory\" \\\n-    \"MergeTree ORDER BY tuple() SETTINGS min_bytes_for_compact_part='10M'\" \\\n     \"MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part='10M'\" \\\n     \"MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0\")\n \ndiff --git a/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh b/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh\nindex efe24aa3a886..80318ba67fbc 100755\n--- a/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh\n+++ b/tests/queries/0_stateless/01508_race_condition_rename_clear_zookeeper_long.sh\n@@ -8,9 +8,7 @@ CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n $CLICKHOUSE_CLIENT --query \"DROP TABLE IF EXISTS table_for_renames0\"\n $CLICKHOUSE_CLIENT --query \"DROP TABLE IF EXISTS table_for_renames50\"\n \n-\n-$CLICKHOUSE_CLIENT --query \"CREATE TABLE table_for_renames0 (value UInt64, data String) ENGINE ReplicatedMergeTree('/clickhouse/tables/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX/concurrent_rename', '1') ORDER BY tuple() SETTINGS cleanup_delay_period = 1, cleanup_delay_period_random_add = 0, min_rows_for_compact_part = 100000, min_rows_for_compact_part = 10000000, write_ahead_log_max_bytes = 1\"\n-\n+$CLICKHOUSE_CLIENT --query \"CREATE TABLE table_for_renames0 (value UInt64, data String) ENGINE ReplicatedMergeTree('/clickhouse/tables/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX/concurrent_rename', '1') ORDER BY tuple() SETTINGS cleanup_delay_period = 1, cleanup_delay_period_random_add = 0\"\n \n $CLICKHOUSE_CLIENT --query \"INSERT INTO table_for_renames0 SELECT number, toString(number) FROM numbers(1000)\"\n \ndiff --git a/tests/queries/0_stateless/01600_parts_types_metrics_long.sh b/tests/queries/0_stateless/01600_parts_types_metrics_long.sh\nindex 0b9afcf633ef..5f724e810422 100755\n--- a/tests/queries/0_stateless/01600_parts_types_metrics_long.sh\n+++ b/tests/queries/0_stateless/01600_parts_types_metrics_long.sh\n@@ -35,7 +35,7 @@ $CLICKHOUSE_CLIENT --database_atomic_wait_for_drop_and_detach_synchronously=1 --\n # InMemory - [0..5]\n # Compact  - (5..10]\n # Wide     - >10\n-$CLICKHOUSE_CLIENT --query=\"CREATE TABLE data_01600 (part_type String, key Int) ENGINE = MergeTree PARTITION BY part_type ORDER BY key SETTINGS min_bytes_for_wide_part=0, min_bytes_for_compact_part=0, min_rows_for_wide_part=10, min_rows_for_compact_part=5\"\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE data_01600 (part_type String, key Int) ENGINE = MergeTree PARTITION BY part_type ORDER BY key SETTINGS min_bytes_for_wide_part=0, min_rows_for_wide_part=10\"\n \n # InMemory\n $CLICKHOUSE_CLIENT --query=\"INSERT INTO data_01600 SELECT 'InMemory', number FROM system.numbers LIMIT 1\"\ndiff --git a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference\nindex 613c455fc590..98bb953263a3 100644\n--- a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference\n+++ b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.reference\n@@ -8,8 +8,6 @@ wide fsync_after_insert\n 1\n wide fsync_after_insert,fsync_part_directory\n 1\n-memory in_memory_parts_insert_sync\n-1\n wide fsync_part_directory,vertical\n 1\n 2\ndiff --git a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql\nindex ad0dfca0db20..dfc761e17649 100644\n--- a/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql\n+++ b/tests/queries/0_stateless/01643_merge_tree_fsync_smoke.sql\n@@ -37,13 +37,6 @@ select * from data_01643;\n optimize table data_01643 final;\n drop table data_01643;\n \n-select 'memory in_memory_parts_insert_sync';\n-create table data_01643 (key Int) engine=MergeTree() order by key settings min_rows_for_compact_part=2, in_memory_parts_insert_sync=1, fsync_after_insert=1, fsync_part_directory=1;\n-insert into data_01643 values (1);\n-select * from data_01643;\n-optimize table data_01643 final;\n-drop table data_01643;\n-\n select 'wide fsync_part_directory,vertical';\n create table data_01643 (key Int) engine=MergeTree() order by key settings min_bytes_for_wide_part=0, fsync_part_directory=1, enable_vertical_merge_algorithm=1, vertical_merge_algorithm_min_rows_to_activate=0, vertical_merge_algorithm_min_columns_to_activate=0;\n insert into data_01643 values (1);\ndiff --git a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference\nindex 613c455fc590..98bb953263a3 100644\n--- a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference\n+++ b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.reference\n@@ -8,8 +8,6 @@ wide fsync_after_insert\n 1\n wide fsync_after_insert,fsync_part_directory\n 1\n-memory in_memory_parts_insert_sync\n-1\n wide fsync_part_directory,vertical\n 1\n 2\ndiff --git a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql\nindex bcce87e11db3..54c30fa2b1ac 100644\n--- a/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql\n+++ b/tests/queries/0_stateless/01643_replicated_merge_tree_fsync_smoke.sql\n@@ -63,17 +63,6 @@ system sync replica rep_fsync_r2;\n drop table rep_fsync_r1;\n drop table rep_fsync_r2;\n \n-select 'memory in_memory_parts_insert_sync';\n-create table rep_fsync_r1 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r1') order by key settings min_rows_for_compact_part=2, in_memory_parts_insert_sync=1, fsync_after_insert=1, fsync_part_directory=1;\n-create table rep_fsync_r2 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r2') order by key settings min_rows_for_compact_part=2, in_memory_parts_insert_sync=1, fsync_after_insert=1, fsync_part_directory=1;\n-insert into rep_fsync_r1 values (1);\n-system sync replica rep_fsync_r2;\n-select * from rep_fsync_r2;\n-optimize table rep_fsync_r1 final;\n-system sync replica rep_fsync_r2;\n-drop table rep_fsync_r1;\n-drop table rep_fsync_r2;\n-\n select 'wide fsync_part_directory,vertical';\n create table rep_fsync_r1 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r1') order by key settings min_bytes_for_wide_part=0, fsync_part_directory=1, enable_vertical_merge_algorithm=1, vertical_merge_algorithm_min_rows_to_activate=0, vertical_merge_algorithm_min_columns_to_activate=0;\n create table rep_fsync_r2 (key Int) engine=ReplicatedMergeTree('/clickhouse/tables/{database}/rep_fsync', 'r2') order by key settings min_bytes_for_wide_part=0, fsync_part_directory=1, enable_vertical_merge_algorithm=1, vertical_merge_algorithm_min_rows_to_activate=0, vertical_merge_algorithm_min_columns_to_activate=0;\ndiff --git a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference\nindex c66682ca038b..cf3e942adfe1 100644\n--- a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference\n+++ b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.reference\n@@ -4,7 +4,6 @@ ReplacingMergeTree: OK\n JSONEachRow: OK\n clusterAllReplicas: OK\n SimpleAggregateFunction: OK\n-write_ahead_log_interval_ms_to_fsync: OK\n max_concurrent_queries_for_all_users: OK\n test_shard_localhost: OK\n default_path_test: OK\ndiff --git a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh\nindex 617148de5a37..42ae5e84f445 100755\n--- a/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh\n+++ b/tests/queries/0_stateless/01676_clickhouse_client_autocomplete.sh\n@@ -89,8 +89,6 @@ client_compwords_positive=(\n     clusterAllReplicas\n     # system.data_type_families\n     SimpleAggregateFunction\n-    # system.merge_tree_settings\n-    write_ahead_log_interval_ms_to_fsync\n     # system.settings\n     max_concurrent_queries_for_all_users\n     # system.clusters\ndiff --git a/tests/queries/0_stateless/01710_projection_fetch_long.reference b/tests/queries/0_stateless/01710_projection_fetch_long.reference\nindex abce5410b26d..c7834c75d022 100644\n--- a/tests/queries/0_stateless/01710_projection_fetch_long.reference\n+++ b/tests/queries/0_stateless/01710_projection_fetch_long.reference\n@@ -10,8 +10,8 @@\n 3\t3\n 4\t4\n 0\n-CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    ),\\n    PROJECTION pp\\n    (\\n        SELECT \\n            x,\\n            count()\\n        GROUP BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192\n+CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    ),\\n    PROJECTION pp\\n    (\\n        SELECT \\n            x,\\n            count()\\n        GROUP BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192\n 2\n-CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    ),\\n    PROJECTION pp\\n    (\\n        SELECT \\n            x,\\n            count()\\n        GROUP BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192\n-CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    ),\\n    PROJECTION pp\\n    (\\n        SELECT \\n            x,\\n            count()\\n        GROUP BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192\n-CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32, index_granularity = 8192\n+CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    ),\\n    PROJECTION pp\\n    (\\n        SELECT \\n            x,\\n            count()\\n        GROUP BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192\n+CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    ),\\n    PROJECTION pp\\n    (\\n        SELECT \\n            x,\\n            count()\\n        GROUP BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192\n+CREATE TABLE default.tp_2\\n(\\n    `x` Int32,\\n    `y` Int32,\\n    PROJECTION p\\n    (\\n        SELECT \\n            x,\\n            y\\n        ORDER BY x\\n    )\\n)\\nENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{shard}/01710_projection_fetch_default\\', \\'2_{replica}\\')\\nORDER BY y\\nSETTINGS min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32, index_granularity = 8192\ndiff --git a/tests/queries/0_stateless/01710_projection_fetch_long.sql b/tests/queries/0_stateless/01710_projection_fetch_long.sql\nindex 6c41c69254e1..13f7a293934e 100644\n--- a/tests/queries/0_stateless/01710_projection_fetch_long.sql\n+++ b/tests/queries/0_stateless/01710_projection_fetch_long.sql\n@@ -3,9 +3,9 @@\n drop table if exists tp_1;\n drop table if exists tp_2;\n \n-create table tp_1 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '1_{replica}') order by y settings min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32;\n+create table tp_1 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '1_{replica}') order by y settings min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32;\n \n-create table tp_2 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '2_{replica}') order by y settings min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32;\n+create table tp_2 (x Int32, y Int32, projection p (select x, y order by x)) engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/01710_projection_fetch_' || currentDatabase(), '2_{replica}') order by y settings min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32;\n \n insert into tp_1 select number, number from numbers(3);\n \ndiff --git a/tests/queries/0_stateless/01710_projection_part_check.sql b/tests/queries/0_stateless/01710_projection_part_check.sql\nindex c889bd323a7b..b15d9d7525ec 100644\n--- a/tests/queries/0_stateless/01710_projection_part_check.sql\n+++ b/tests/queries/0_stateless/01710_projection_part_check.sql\n@@ -1,6 +1,6 @@\n drop table if exists tp;\n \n-create table tp (x Int32, y Int32, projection p (select x, y order by x)) engine = MergeTree order by y settings min_rows_for_compact_part = 2, min_rows_for_wide_part = 4, min_bytes_for_compact_part = 16, min_bytes_for_wide_part = 32;\n+create table tp (x Int32, y Int32, projection p (select x, y order by x)) engine = MergeTree order by y settings min_rows_for_wide_part = 4, min_bytes_for_wide_part = 32;\n \n insert into tp select number, number from numbers(3);\n insert into tp select number, number from numbers(5);\ndiff --git a/tests/queries/0_stateless/02148_in_memory_part_flush.reference b/tests/queries/0_stateless/02148_in_memory_part_flush.reference\ndeleted file mode 100644\nindex 219c5f4b22f6..000000000000\n--- a/tests/queries/0_stateless/02148_in_memory_part_flush.reference\n+++ /dev/null\n@@ -1,4 +0,0 @@\n-before DETACH TABLE\n-500\n-after DETACH TABLE\n-500\ndiff --git a/tests/queries/0_stateless/02148_in_memory_part_flush.sql b/tests/queries/0_stateless/02148_in_memory_part_flush.sql\ndeleted file mode 100644\nindex ec20721186e2..000000000000\n--- a/tests/queries/0_stateless/02148_in_memory_part_flush.sql\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-DROP TABLE IF EXISTS mem_part_flush;\n-\n-CREATE TABLE mem_part_flush\n-(\n-`key` UInt32,\n-`ts` DateTime,\n-`db_time` DateTime DEFAULT now()\n-)\n-ENGINE = MergeTree\n-ORDER BY (key, ts)\n-SETTINGS min_rows_for_compact_part = 1000000, min_bytes_for_compact_part = 200000000, in_memory_parts_enable_wal = 0;\n-\n-INSERT INTO mem_part_flush(key, ts) SELECT number % 1000, now() + intDiv(number,1000) FROM numbers(500);\n-\n-SELECT 'before DETACH TABLE';\n-SELECT count(*) FROM mem_part_flush;\n-\n-DETACH TABLE mem_part_flush;\n-\n-ATTACH TABLE mem_part_flush;\n-\n-SELECT 'after DETACH TABLE';\n-SELECT count(*) FROM mem_part_flush;\n-\n-\n-DROP TABLE mem_part_flush;\ndiff --git a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.reference b/tests/queries/0_stateless/02410_inmemory_wal_cleanup.reference\ndeleted file mode 100644\nindex 6727d83a6f42..000000000000\n--- a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.reference\n+++ /dev/null\n@@ -1,35 +0,0 @@\n--- { echo }\n-\n-DROP TABLE IF EXISTS in_memory;\n-CREATE TABLE in_memory (a UInt32) ENGINE = MergeTree ORDER BY a SETTINGS min_rows_for_compact_part = 1000, min_bytes_for_wide_part = 10485760;\n-INSERT INTO in_memory VALUES (1);\n-INSERT INTO in_memory VALUES (2);\n-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';\n-all_1_1_0\t1\tInMemory\n-all_2_2_0\t1\tInMemory\n-SELECT * FROM in_memory ORDER BY a;\n-1\n-2\n--- no WAL remove since parts are still in use\n-DETACH TABLE in_memory;\n-ATTACH TABLE in_memory;\n-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';\n-all_1_1_0\t1\tInMemory\n-all_2_2_0\t1\tInMemory\n-SELECT * FROM in_memory ORDER BY a;\n-1\n-2\n--- WAL should be removed, since on disk part covers all parts in WAL\n-OPTIMIZE TABLE in_memory;\n-DETACH TABLE in_memory;\n-ATTACH TABLE in_memory;\n-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';\n-all_1_2_1\t1\tCompact\n--- check that the WAL will be reinitialized after remove\n-INSERT INTO in_memory VALUES (3);\n-DETACH TABLE in_memory;\n-ATTACH TABLE in_memory;\n-SELECT * FROM in_memory ORDER BY a;\n-1\n-2\n-3\ndiff --git a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.sql b/tests/queries/0_stateless/02410_inmemory_wal_cleanup.sql\ndeleted file mode 100644\nindex 7f832d980ba2..000000000000\n--- a/tests/queries/0_stateless/02410_inmemory_wal_cleanup.sql\n+++ /dev/null\n@@ -1,29 +0,0 @@\n--- Tags: no-s3-storage\n-\n--- { echo }\n-\n-DROP TABLE IF EXISTS in_memory;\n-\n-CREATE TABLE in_memory (a UInt32) ENGINE = MergeTree ORDER BY a SETTINGS min_rows_for_compact_part = 1000, min_bytes_for_wide_part = 10485760;\n-INSERT INTO in_memory VALUES (1);\n-INSERT INTO in_memory VALUES (2);\n-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';\n-SELECT * FROM in_memory ORDER BY a;\n-\n--- no WAL remove since parts are still in use\n-DETACH TABLE in_memory;\n-ATTACH TABLE in_memory;\n-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';\n-SELECT * FROM in_memory ORDER BY a;\n-\n--- WAL should be removed, since on disk part covers all parts in WAL\n-OPTIMIZE TABLE in_memory;\n-DETACH TABLE in_memory;\n-ATTACH TABLE in_memory;\n-SELECT name, active, part_type FROM system.parts WHERE database = currentDatabase() AND table = 'in_memory';\n-\n--- check that the WAL will be reinitialized after remove\n-INSERT INTO in_memory VALUES (3);\n-DETACH TABLE in_memory;\n-ATTACH TABLE in_memory;\n-SELECT * FROM in_memory ORDER BY a;\ndiff --git a/tests/queries/0_stateless/02423_drop_memory_parts.reference b/tests/queries/0_stateless/02423_drop_memory_parts.reference\ndeleted file mode 100644\nindex d69a5f07a051..000000000000\n--- a/tests/queries/0_stateless/02423_drop_memory_parts.reference\n+++ /dev/null\n@@ -1,14 +0,0 @@\n-init state\n-30\n-0_1_1_0\tInMemory\t10\t1\n-1_2_2_0\tInMemory\t10\t1\n-2_3_3_0\tInMemory\t10\t1\n-drop part 0\n-20\n-1_2_2_0\tInMemory\t10\t1\n-2_3_3_0\tInMemory\t10\t1\n-detach table\n-attach table\n-20\n-1_2_2_0\tInMemory\t10\t1\n-2_3_3_0\tInMemory\t10\t1\ndiff --git a/tests/queries/0_stateless/02423_drop_memory_parts.sql b/tests/queries/0_stateless/02423_drop_memory_parts.sql\ndeleted file mode 100644\nindex 9326f159b0ce..000000000000\n--- a/tests/queries/0_stateless/02423_drop_memory_parts.sql\n+++ /dev/null\n@@ -1,40 +0,0 @@\n--- Tags: no-s3-storage\n-\n-DROP TABLE IF EXISTS table_in_memory;\n-\n-CREATE TABLE table_in_memory\n-(\n-    `id` UInt64,\n-    `value` UInt64\n-)\n-ENGINE = MergeTree\n-PARTITION BY id\n-ORDER BY value\n-SETTINGS min_bytes_for_wide_part=1000, min_bytes_for_compact_part=900;\n-\n-SELECT 'init state';\n-INSERT INTO table_in_memory SELECT intDiv(number, 10), number FROM numbers(30);\n-\n-SELECT count() FROM table_in_memory;\n-SELECT name, part_type, rows, active from system.parts\n-WHERE table='table_in_memory' AND database=currentDatabase();\n-\n-SELECT 'drop part 0';\n-ALTER TABLE table_in_memory DROP PARTITION 0;\n-\n-SELECT count() FROM table_in_memory;\n-SELECT name, part_type, rows, active from system.parts\n-WHERE table='table_in_memory' AND database=currentDatabase() AND active;\n-\n-SELECT 'detach table';\n-DETACH TABLE table_in_memory;\n-\n-SELECT name, part_type, rows, active from system.parts\n-WHERE table='table_in_memory' AND database=currentDatabase();\n-\n-SELECT 'attach table';\n-ATTACH TABLE table_in_memory;\n-\n-SELECT count() FROM table_in_memory;\n-SELECT name, part_type, rows, active from system.parts\n-WHERE table='table_in_memory' AND database=currentDatabase() and active;\ndiff --git a/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference b/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference\nindex b0214e0e7c7a..9ed0fb620a4c 100644\n--- a/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference\n+++ b/tests/queries/0_stateless/02559_nested_multiple_levels_default.reference\n@@ -1,6 +1,4 @@\n data_compact\tCompact\n [[]]\n-data_memory\tInMemory\n-[[]]\n data_wide\tWide\n [[]]\ndiff --git a/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql b/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql\nindex 156af5c7784d..9dcdab82acbe 100644\n--- a/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql\n+++ b/tests/queries/0_stateless/02559_nested_multiple_levels_default.sql\n@@ -10,26 +10,12 @@ CREATE TABLE data_compact\n )\n ENGINE = MergeTree()\n ORDER BY tuple()\n-SETTINGS min_rows_for_compact_part=0, min_bytes_for_compact_part=0, min_rows_for_wide_part=100, min_bytes_for_wide_part=1e9;\n+SETTINGS min_rows_for_wide_part=100, min_bytes_for_wide_part=1e9;\n INSERT INTO data_compact VALUES ([0]);\n ALTER TABLE data_compact ADD COLUMN root.nested_array Array(Array(UInt8));\n SELECT table, part_type FROM system.parts WHERE table = 'data_compact' AND database = currentDatabase();\n SELECT root.nested_array FROM data_compact;\n \n--- memory\n-DROP TABLE IF EXISTS data_memory;\n-CREATE TABLE data_memory\n-(\n-    `root.array` Array(UInt8),\n-)\n-ENGINE = MergeTree()\n-ORDER BY tuple()\n-SETTINGS min_rows_for_compact_part=100, min_bytes_for_compact_part=1e9, min_rows_for_wide_part=100, min_bytes_for_wide_part=1e9, in_memory_parts_enable_wal=0;\n-INSERT INTO data_memory VALUES ([0]);\n-ALTER TABLE data_memory ADD COLUMN root.nested_array Array(Array(UInt8));\n-SELECT table, part_type FROM system.parts WHERE table = 'data_memory' AND database = currentDatabase();\n-SELECT root.nested_array FROM data_memory;\n-\n -- wide\n DROP TABLE IF EXISTS data_wide;\n CREATE TABLE data_wide\n@@ -38,7 +24,7 @@ CREATE TABLE data_wide\n )\n ENGINE = MergeTree()\n ORDER BY tuple()\n-SETTINGS min_rows_for_wide_part=0, min_bytes_for_wide_part=0, min_rows_for_wide_part=0, min_bytes_for_wide_part=0;\n+SETTINGS min_rows_for_wide_part=0, min_bytes_for_wide_part=0;\n INSERT INTO data_wide VALUES ([0]);\n ALTER TABLE data_wide ADD COLUMN root.nested_array Array(Array(UInt8));\n SELECT table, part_type FROM system.parts WHERE table = 'data_wide' AND database = currentDatabase();\ndiff --git a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh\nindex 4713e4cbe8b4..adc9525ef810 100755\n--- a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh\n+++ b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.sh\n@@ -32,11 +32,8 @@ DROP TABLE IF EXISTS times;\n CREATE TABLE times (t DateTime) ENGINE MergeTree ORDER BY t\n   SETTINGS\n     storage_policy='default',\n-    min_rows_for_compact_part = 0,\n-    min_bytes_for_compact_part = 0,\n     min_rows_for_wide_part = 1000000,\n     min_bytes_for_wide_part = 1000000,\n-    in_memory_parts_enable_wal = 0,\n     ratio_of_defaults_for_sparse_serialization=1.0;\n \"\n \n@@ -74,4 +71,3 @@ AND ( query LIKE '%SELECT % FROM times%' OR query LIKE '%INSERT INTO times%' )\n AND type = 'QueryFinish'\n ORDER BY query_start_time_microseconds ASC, query DESC;\n \"\n-\ndiff --git a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.reference b/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.reference\ndeleted file mode 100644\nindex 0cfbf08886fc..000000000000\n--- a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.reference\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-2\ndiff --git a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.sql b/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.sql\ndeleted file mode 100644\nindex 194ea9bfcc11..000000000000\n--- a/tests/queries/0_stateless/02675_replicated_merge_tree_insert_zookeeper_long.sql\n+++ /dev/null\n@@ -1,15 +0,0 @@\n--- Tags: no-s3-storage\n-\n-DROP TABLE IF EXISTS inmemory_test;\n-\n-CREATE TABLE inmemory_test (d Date, id String)\n-ENGINE=ReplicatedMergeTree('/clickhouse/tables/{database}/inmemory_test', 'r1')\n-PARTITION BY toYYYYMMDD(d) ORDER BY (d, id)\n-SETTINGS min_rows_for_compact_part = 10, index_granularity = 8192;\n-\n-INSERT INTO inmemory_test(d, id) VALUES('2023-01-01', 'abcdefghijklmnopqrstuvwxyz');\n-INSERT INTO inmemory_test(d, id) VALUES('2023-01-01', 'a1234567890123456789012345');\n-\n-SELECT COUNT(1) FROM inmemory_test;\n-\n-DROP TABLE inmemory_test;\ndiff --git a/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh b/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh\nindex bdcdb38846c5..130f3a29ade8 100755\n--- a/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh\n+++ b/tests/queries/0_stateless/02703_max_local_read_bandwidth.sh\n@@ -7,7 +7,7 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n \n $CLICKHOUSE_CLIENT -nm -q \"\n     drop table if exists data;\n-    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9, min_bytes_for_compact_part=0;\n+    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9;\n \"\n \n # reading 1e6*8 bytes with 1M bandwith it should take (8-1)/1=7 seconds\ndiff --git a/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh b/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh\nindex 276a15ca6cc3..80713e901693 100755\n--- a/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh\n+++ b/tests/queries/0_stateless/02703_max_local_write_bandwidth.sh\n@@ -7,7 +7,7 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n \n $CLICKHOUSE_CLIENT -nm -q \"\n     drop table if exists data;\n-    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9, min_bytes_for_compact_part=0;\n+    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9;\n \"\n \n query_id=$(random_str 10)\ndiff --git a/tests/queries/0_stateless/02704_max_backup_bandwidth.sh b/tests/queries/0_stateless/02704_max_backup_bandwidth.sh\nindex c9ad23031b6a..b5d32d2059d6 100755\n--- a/tests/queries/0_stateless/02704_max_backup_bandwidth.sh\n+++ b/tests/queries/0_stateless/02704_max_backup_bandwidth.sh\n@@ -7,7 +7,7 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n \n $CLICKHOUSE_CLIENT -nm -q \"\n     drop table if exists data;\n-    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9, min_bytes_for_compact_part=0;\n+    create table data (key UInt64 CODEC(NONE)) engine=MergeTree() order by tuple() settings min_bytes_for_wide_part=1e9;\n \"\n \n # reading 1e6*8 bytes with 1M bandwith it should take (8-1)/1=7 seconds\n",
  "problem_statement": "MergeTreeWriteAheadLog: Cannot write to finalized buffer when dropping part after failed INSERT\nhttps://s3.amazonaws.com/clickhouse-test-reports/0/9d5ec474a3eb84ef6278525dad994e5e42437f73/stress_test__asan_.html\r\n```\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:20:21.284420 [ 37768 ] {a36c46ee-cf0c-4c53-ab93-874e97221baf} <Fatal> : Logical error: 'Cannot write to finalized buffer'.\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:04.799282 [ 68356 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:04.799455 [ 68356 ] {} <Fatal> BaseDaemon: (version 22.13.1.1 (official build), build id: 7A4961E6EC3A0B497046F3C5387E2E1C05821379) (from thread 37768) (query_id: a36c46ee-cf0c-4c53-ab93-874e97221baf) (query: INSERT INTO table_for_renames0 SELECT number, toString(number) FROM numbers(1000)) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:04.799567 [ 68356 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:04.799711 [ 68356 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fb5ee6e100b 0x7fb5ee6c0859 0x200a6ddb 0x200a74fb 0xe993473 0xe9c5c46 0x34c296ef 0x348054d6 0x348075c1 0x3485dd02 0x34ec65a4 0x34e817f8 0x34e7fe1b 0x34e541a5 0x34e769c4 0x35e6ce9b 0x35c62aea 0x35c625a0 0x35c61330 0x356780e1 0x3565711d 0x35655a86 0x35655675 0x356537ab 0x202ff12d 0x20309340 0x7fb5ee898609 0x7fb5ee7bd133\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:04.799855 [ 68356 ] {} <Fatal> BaseDaemon: 3. raise @ 0x7fb5ee6e100b in ?\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:04.799963 [ 68356 ] {} <Fatal> BaseDaemon: 4. abort @ 0x7fb5ee6c0859 in ?\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:05.006055 [ 68356 ] {} <Fatal> BaseDaemon: 5. ./build_docker/../src/Common/Exception.cpp:41: DB::abortOnFailedAssertion(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) @ 0x200a6ddb in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:05.188519 [ 68356 ] {} <Fatal> BaseDaemon: 6.1. inlined from ./build_docker/../src/Common/Exception.cpp:64: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*>> const&)\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:05.188646 [ 68356 ] {} <Fatal> BaseDaemon: 6. ./build_docker/../src/Common/Exception.cpp:78: DB::Exception::Exception(DB::Exception::MessageMasked const&, int, bool) @ 0x200a74fb in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:21:22.599057 [ 68356 ] {} <Fatal> BaseDaemon: 7. DB::Exception::Exception(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) @ 0xe993473 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.err.log:2023.01.13 18:25:02.004982 [ 1691 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:20:21.284420 [ 37768 ] {a36c46ee-cf0c-4c53-ab93-874e97221baf} <Fatal> : Logical error: 'Cannot write to finalized buffer'.\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:04.799282 [ 68356 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:04.799455 [ 68356 ] {} <Fatal> BaseDaemon: (version 22.13.1.1 (official build), build id: 7A4961E6EC3A0B497046F3C5387E2E1C05821379) (from thread 37768) (query_id: a36c46ee-cf0c-4c53-ab93-874e97221baf) (query: INSERT INTO table_for_renames0 SELECT number, toString(number) FROM numbers(1000)) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:04.799567 [ 68356 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:04.799711 [ 68356 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fb5ee6e100b 0x7fb5ee6c0859 0x200a6ddb 0x200a74fb 0xe993473 0xe9c5c46 0x34c296ef 0x348054d6 0x348075c1 0x3485dd02 0x34ec65a4 0x34e817f8 0x34e7fe1b 0x34e541a5 0x34e769c4 0x35e6ce9b 0x35c62aea 0x35c625a0 0x35c61330 0x356780e1 0x3565711d 0x35655a86 0x35655675 0x356537ab 0x202ff12d 0x20309340 0x7fb5ee898609 0x7fb5ee7bd133\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:04.799855 [ 68356 ] {} <Fatal> BaseDaemon: 3. raise @ 0x7fb5ee6e100b in ?\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:04.799963 [ 68356 ] {} <Fatal> BaseDaemon: 4. abort @ 0x7fb5ee6c0859 in ?\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:05.006055 [ 68356 ] {} <Fatal> BaseDaemon: 5. ./build_docker/../src/Common/Exception.cpp:41: DB::abortOnFailedAssertion(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) @ 0x200a6ddb in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:05.188519 [ 68356 ] {} <Fatal> BaseDaemon: 6.1. inlined from ./build_docker/../src/Common/Exception.cpp:64: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*>> const&)\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:05.188646 [ 68356 ] {} <Fatal> BaseDaemon: 6. ./build_docker/../src/Common/Exception.cpp:78: DB::Exception::Exception(DB::Exception::MessageMasked const&, int, bool) @ 0x200a74fb in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:21:22.599057 [ 68356 ] {} <Fatal> BaseDaemon: 7. DB::Exception::Exception(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) @ 0xe993473 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.stress.log:2023.01.13 18:25:02.004982 [ 1691 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n\r\n```\n",
  "hints_text": "Full stacktrace from gdb.log:\r\n```\r\n023-01-13 18:20:21 Thread 1888 \"QueryCompPipeEx\" received signal SIGABRT, Aborted.\r\n2023-01-13 18:20:21 [Switching to Thread 0x7fa65c04e700 (LWP 37768)]\r\n2023-01-13 18:20:21 0x00007fb5ee6e100b in raise () from /lib/x86_64-linux-gnu/libc.so.6\r\n2023-01-13 18:20:21 #0  0x00007fb5ee6e100b in raise () from /lib/x86_64-linux-gnu/libc.so.6\r\n2023-01-13 18:20:21 No symbol table info available.\r\n2023-01-13 18:20:21 #1  0x00007fb5ee6c0859 in abort () from /lib/x86_64-linux-gnu/libc.so.6\r\n2023-01-13 18:20:22 No symbol table info available.\r\n2023-01-13 18:20:22 #2  0x00000000200a6ddb in DB::abortOnFailedAssertion (description=...) at ../src/Common/Exception.cpp:48\r\n2023-01-13 18:20:22         always_false = false\r\n2023-01-13 18:20:22 #3  0x00000000200a74fb in DB::handle_error_code (msg=..., code=0, remote=false, trace=...) at ../src/Common/Exception.cpp:60\r\n2023-01-13 18:20:22 No locals.\r\n2023-01-13 18:20:22 #4  DB::Exception::Exception (this=0x6150039c9040, msg_masked=..., code=0, remote_=false) at ../src/Common/Exception.cpp:78\r\n2023-01-13 18:20:22 No locals.\r\n2023-01-13 18:20:22 #5  0x000000000e993473 in DB::Exception::Exception(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) ()\r\n2023-01-13 18:20:22 No symbol table info available.\r\n2023-01-13 18:20:22 #6  0x000000000e9c5c46 in DB::WriteBuffer::write(char const*, unsigned long) ()\r\n2023-01-13 18:20:25 No symbol table info available.\r\n2023-01-13 18:20:25 #7  0x0000000034c296ef in DB::writePODBinary<char8_t> (buf=..., x=<optimized out>) at ../src/IO/WriteHelpers.h:85\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #8  DB::writeIntBinary<char8_t> (buf=..., x=<optimized out>) at ../src/IO/WriteHelpers.h:91\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #9  DB::MergeTreeWriteAheadLog::dropPart (this=0x6120039826d8, part_name=...) at ../src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp:115\r\n2023-01-13 18:20:25         lock = {__m_ = 0x612003982798, __owns_ = true}\r\n2023-01-13 18:20:25         metadata = {min_compatible_version = 1 '\\001', part_uuid = {t = {items = {0, 0}}}, static JSON_KEY_PART_UUID = <optimized out>}\r\n2023-01-13 18:20:25 #10 0x00000000348054d6 in DB::MergeTreeData::removePartsFromWorkingSet (this=<optimized out>, txn=0x0, remove=..., clear_without_timeout=<optimized out>, acquired_lock=...) at ../src/Storages/MergeTree/MergeTreeData.cpp:3355\r\n2023-01-13 18:20:25         part = @0x6020009c5a30: {__ptr_ = 0x61a004bc2a98, __cntrl_ = 0x61a004bc2a80}\r\n2023-01-13 18:20:25         __range1 = <optimized out>\r\n2023-01-13 18:20:25         __begin1 = {__i = 0x6020009c5a30}\r\n2023-01-13 18:20:25         __end1 = {__i = 0x6020009c5a40}\r\n2023-01-13 18:20:25         remove_time = 0\r\n2023-01-13 18:20:25         removed_active_part = false\r\n2023-01-13 18:20:25 #11 0x00000000348075c1 in DB::MergeTreeData::removePartsFromWorkingSet (this=<optimized out>, txn=<optimized out>, remove=..., clear_without_timeout=<optimized out>, acquired_lock=0x7fa659e22aa0) at ../src/Storages/MergeTree/MergeTreeData.cpp:3393\r\n2023-01-13 18:20:25         lock = {__m_ = 0x0, __owns_ = <synthetic pointer>}\r\n2023-01-13 18:20:25 #12 0x000000003485dd02 in DB::MergeTreeData::Transaction::rollback (this=0x7fa65a0e1de0) at ../src/Storages/MergeTree/MergeTreeData.cpp:5520\r\n2023-01-13 18:20:25         buf = {<DB::detail::StringHolder> = {value = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x604002c4fcd0 \"Removing parts: all_0_0_0.\", __size_ = 26, __cap_ = 48, __is_long_ = 1}, __s = {__data_ = \"\\320\\374\\304\\002@`\\000\\000\\032\\000\\000\\000\\000\\000\\000\\000\\060\\000\\000\\000\\000\\000\", __padding_ = 0x7fa659e228bf \"\\200\\061\\061\\065\\064\\062\\060\\070\\071\\061\\062 140732003603264 0 0 0all_0_0_0\", __size_ = 0 '\\000', __is_long_ = 1 '\\001'}, __r = {__words = {105828040637648, 26, 9223372036854775856}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}}, <DB::WriteBufferFromVector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<DB::WriteBuffer> = {<DB::BufferBase> = {pos = 0x0, bytes = 0, working_buffer = {begin_pos = 0x0, end_pos = 0x0}, internal_buffer = {begin_pos = 0x0, end_pos = 0x0}, padded = false}, _vptr$WriteBuffer = 0x6e2ca60 <vtable for DB::WriteBufferFromOwnString+16>, finalized = true}, vector = @0x7fa659e228a8, static initial_size = 32, static size_multiplier = 2}, <No data fields>}\r\n2023-01-13 18:20:25         lock = {__m_ = 0x62200298fdd0, __owns_ = true}\r\n2023-01-13 18:20:25 #13 0x0000000034ec65a4 in DB::MergeTreeData::Transaction::~Transaction (this=0x7fa65a0e1de0) at ../src/Storages/MergeTree/MergeTreeData.h:269\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #14 DB::ReplicatedMergeTreeSinkImpl<false>::commitPart(std::__1::shared_ptr<DB::ZooKeeperWithFaultInjection> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool)::{lambda()#1}::operator()() const (this=0x7fa659e06240) at ../src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp:976\r\n2023-01-13 18:20:25         block_id_path = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x6110033e5cc0 \"/clickhouse/tables/01508_race_condition_rename_clear_zookeeper_long_test_gmkivotw/concurrent_rename/blocks/all_10556743547062697475_3052979394108171700\", __size_ = 151, __cap_ = 224, __is_long_ = 1}, __s = {__data_ = \"\\300\\\\>\\003\\020a\\000\\000\\227\\000\\000\\000\\000\\000\\000\\000\\340\\000\\000\\000\\000\\000\", __padding_ = 0x7fa65a0e1147 \"\\200\", __size_ = 0 '\\000', __is_long_ = 1 '\\001'}, __r = {__words = {106721401789632, 151, 9223372036854776032}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}\r\n2023-01-13 18:20:25         block_number_lock = {<std::__1::__optional_move_assign_base<DB::EphemeralLockInZooKeeper, false>> = {<std::__1::__optional_copy_assign_base<DB::EphemeralLockInZooKeeper, false>> = {<std::__1::__optional_move_base<DB::EphemeralLockInZooKeeper, false>> = {<std::__1::__optional_copy_base<DB::EphemeralLockInZooKeeper, false>> = {<std::__1::__optional_storage_base<DB::EphemeralLockInZooKeeper, false>> = {<std::__1::__optional_destruct_base<DB::EphemeralLockInZooKeeper, false>> = {{__null_state_ = 64 '@', __val_ = <incomplete type>}, __engaged_ = true}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <std::__1::__sfinae_ctor_base<false, true>> = {<No data fields>}, <std::__1::__sfinae_assign_base<false, true>> = {<No data fields>}, <No data fields>}\r\n2023-01-13 18:20:25         ops = {__begin_ = 0x606050a6efc0, __end_ = 0x606050a6f000, __end_cap_ = {<std::__1::__compressed_pair_elem<std::__1::shared_ptr<Coordination::Request>*, 0, false>> = {__value_ = 0x606050a6f000}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::shared_ptr<Coordination::Request> >, 1, true>> = {<std::__1::allocator<std::__1::shared_ptr<Coordination::Request> >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::shared_ptr<Coordination::Request> > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}\r\n2023-01-13 18:20:25         existing_part_name = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x0, __size_ = 0, __cap_ = 0, __is_long_ = 0}, __s = {__data_ = '\\000' <repeats 22 times>, __padding_ = 0x7fa65a0e12c7 \"\", __size_ = 0 '\\000', __is_long_ = 0 '\\000'}, __r = {__words = {0, 0, 0}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}\r\n2023-01-13 18:20:25         transaction = {<boost::noncopyable_::noncopyable> = {<boost::noncopyable_::base_token> = {<No data fields>}, <No data fields>}, data = @0x62200298f920, txn = 0x0, precommitted_parts = {__tree_ = {__begin_node_ = 0x604003a93c50, __pair1_ = {<std::__1::__compressed_pair_elem<std::__1::__tree_end_node<std::__1::__tree_node_base<void*>*>, 0, false>> = {__value_ = {__left_ = 0x604003a93c50}}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__tree_node<std::__1::shared_ptr<DB::IMergeTreeDataPart>, void*> >, 1, true>> = {<std::__1::allocator<std::__1::__tree_node<std::__1::shared_ptr<DB::IMergeTreeDataPart>, void*> >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__tree_node<std::__1::shared_ptr<DB::IMergeTreeDataPart>, void*> > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __pair3_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 1}, <std::__1::__compressed_pair_elem<DB::MergeTreeData::LessDataPart, 1, true>> = {<DB::MergeTreeData::LessDataPart> = {<No data fields>}, <No data fields>}, <No data fields>}}}, locked_parts = {__tree_ = {__begin_node_ = 0x7fa65a0e1e10, __pair1_ = {<std::__1::__compressed_pair_elem<std::__1::__tree_end_node<std::__1::__tree_node_base<void*>*>, 0, false>> = {__value_ = {__left_ = 0x0}}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__tree_node<std::__1::shared_ptr<DB::IMergeTreeDataPart>, void*> >, 1, true>> = {<std::__1::allocator<std::__1::__tree_node<std::__1::shared_ptr<DB::IMergeTreeDataPart>, void*> >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__tree_node<std::__1::shared_ptr<DB::IMergeTreeDataPart>, void*> > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __pair3_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 0}, <std::__1::__compressed_pair_elem<DB::MergeTreeData::LessDataPart, 1, true>> = {<DB::MergeTreeData::LessDataPart> = {<No data fields>}, <No data fields>}, <No data fields>}}}}\r\n2023-01-13 18:20:25         responses = {__begin_ = 0x60605bf856c0, __end_ = 0x60605bf856c0, __end_cap_ = {<std::__1::__compressed_pair_elem<std::__1::shared_ptr<Coordination::Response>*, 0, false>> = {__value_ = 0x60605bf85700}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::shared_ptr<Coordination::Response> >, 1, true>> = {<std::__1::allocator<std::__1::shared_ptr<Coordination::Response> >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::shared_ptr<Coordination::Response> > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}\r\n2023-01-13 18:20:25         block_number = <optimized out>\r\n2023-01-13 18:20:25         block_unlock_op_idx = <optimized out>\r\n2023-01-13 18:20:25         renamed = <optimized out>\r\n2023-01-13 18:20:25         rename_part_to_temporary = <optimized out>\r\n2023-01-13 18:20:25         multi_code = <optimized out>\r\n2023-01-13 18:20:25         e = <optimized out>\r\n2023-01-13 18:20:25         deduplicate_block = <optimized out>\r\n2023-01-13 18:20:25 #15 0x0000000034e817f8 in DB::ZooKeeperRetriesControl::retryLoop<DB::ReplicatedMergeTreeSinkImpl<false>::commitPart(std::__1::shared_ptr<DB::ZooKeeperWithFaultInjection> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool)::{lambda()#1}, DB::ReplicatedMergeTreeSinkImpl<false>::commitPart(std::__1::shared_ptr<DB::ZooKeeperWithFaultInjection> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool)::{lambda()#2}>(DB::ReplicatedMergeTreeSinkImpl<false>::commitPart(std::__1::shared_ptr<DB::ZooKeeperWithFaultInjection> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool)::{lambda()#1}&&, DB::ReplicatedMergeTreeSinkImpl<false>::commitPart(std::__1::shared_ptr<DB::ZooKeeperWithFaultInjection> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool)::{lambda()#2}&&) (this=0x7fa659e06130, f=..., iteration_cleanup=...) at ../src/Storages/MergeTree/ZooKeeperRetries.h:53\r\n2023-01-13 18:20:25         e = <optimized out>\r\n2023-01-13 18:20:25 #16 0x0000000034e7fe1b in DB::ReplicatedMergeTreeSinkImpl<false>::commitPart (this=<optimized out>, zookeeper=..., part=..., block_id=..., replicas_num=<optimized out>, writing_existing_part=<optimized out>) at ../src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp:557\r\n2023-01-13 18:20:25         temporary_part_relative_path = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x65736e695f706d74 <error: Cannot access memory at address 0x65736e695f706d74>, __size_ = 3557681443391894642, __cap_ = 1441151881570103647, __is_long_ = 0}, __s = {__data_ = \"tmp_insert_all_1_1_0\\000\\000\", __padding_ = 0x7fa659e06077 \"\\024\", __size_ = 20 '\\024', __is_long_ = 0 '\\000'}, __r = {__words = {7310308019004271988, 3557681443391894642, 1441151881570103647}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}\r\n2023-01-13 18:20:25         conflict_block_ids = {__begin_ = 0x0, __end_ = 0x0, __end_cap_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, 0, false>> = {__value_ = 0x0}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, 1, true>> = {<std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}\r\n2023-01-13 18:20:25         retries_ctl = {name = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x615074696d6d6f63 <error: Cannot access memory at address 0x615074696d6d6f63>, __size_ = 29810, __cap_ = 720716292826185792, __is_long_ = 0}, __s = {__data_ = \"commitPart\\000\\000\\000\\000\\000\\000@\\300\\275Y\\246\\177\", __padding_ = 0x7fa659e06147 \"\\n\\370\\237\\333\\004`a\", __size_ = 10 '\\n', __is_long_ = 0 '\\000'}, __r = {__words = {7012232615972138851, 29810, 720716292826185792}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}, retries_info = @0x616004db9ff8, iteration_count = 1, user_error = {code = 0, message = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x0, __size_ = 0, __cap_ = 0, __is_long_ = 0}, __s = {__data_ = '\\000' <repeats 22 times>, __padding_ = 0x7fa659e06177 \"\", __size_ = 0 '\\000', __is_long_ = 0 '\\000'}, __r = {__words = {0, 0, 0}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}}, keeper_error = {code = Coordination::Error::ZOPERATIONTIMEOUT, message = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x606053d8bac0 \"Fault injection after operation (Operation timeout)\", __size_ = 51, __cap_ = 64, __is_long_ = 1}, __s = {__data_ = \"\\300\\272\\330S``\\000\\000\\063\\000\\000\\000\\000\\000\\000\\000@\\000\\000\\000\\000\\000\", __padding_ = 0x7fa659e06197 \"\\200\", __size_ = 0 '\\000', __is_long_ = 1 '\\001'}, __r = {__words = {105966839839424, 51, 9223372036854775872}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}}, action_after_last_failed_retry = {<std::__1::__function::__maybe_derive_from_unary_function<void ()>> = {<No data fields>}, <std::__1::__function::__maybe_derive_from_binary_function<void ()>> = {<No data fields>}, __f_ = {__buf_ = {__small = '\\000' <repeats 15 times>, __large = 0x0}, __invoker_ = {__call_ = 0x34e86ac0 <std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::ZooKeeperRetriesControl::action_after_last_failed_retry::{lambda()#1}, void ()> >(std::__1::__function::__policy_storage const*)>}, __policy_ = 0x8432860 <std::__1::__function::__policy::__choose_policy[abi:v15000]<std::__1::__function::__default_alloc_func<DB::ZooKeeperRetriesControl::action_after_last_failed_retry::{lambda()#1}, void ()> >(std::__1::integral_constant<bool, true>)::__policy_>}}, unconditional_retry = false, iteration_succeeded = true, stop_retries = false}\r\n2023-01-13 18:20:25         loop_counter = 0\r\n2023-01-13 18:20:25         max_iterations = 10\r\n2023-01-13 18:20:25         is_already_existing_part = false\r\n2023-01-13 18:20:25         part_committed_locally_but_zookeeper = false\r\n2023-01-13 18:20:25         write_part_info_keeper_error = Coordination::Error::ZOK\r\n2023-01-13 18:20:25 #17 0x0000000034e541a5 in DB::ReplicatedMergeTreeSinkImpl<false>::finishDelayedChunk (this=0x616004db9e98, zookeeper=...) at ../src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp:454\r\n2023-01-13 18:20:25         error = <optimized out>\r\n2023-01-13 18:20:25         part = @0x6110010632c0: {__ptr_ = 0x61a004bc2a98, __cntrl_ = 0x61a004bc2a80}\r\n2023-01-13 18:20:25         partition = @0x6110010632c0: {temp_part = {part = {__ptr_ = 0x61a004bc2a98, __cntrl_ = 0x61a004bc2a80}, streams = {__begin_ = 0x602000c17050, __end_ = 0x602000c17060, __end_cap_ = {<std::__1::__compressed_pair_elem<DB::MergeTreeDataWriter::TemporaryPart::Stream*, 0, false>> = {__value_ = 0x602000c17060}, <std::__1::__compressed_pair_elem<std::__1::allocator<DB::MergeTreeDataWriter::TemporaryPart::Stream>, 1, true>> = {<std::__1::allocator<DB::MergeTreeDataWriter::TemporaryPart::Stream>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<DB::MergeTreeDataWriter::TemporaryPart::Stream> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}, temporary_directory_lock = {static is_nullable = <optimized out>, function = {<std::__1::__function::__maybe_derive_from_unary_function<void ()>> = {<No data fields>}, <std::__1::__function::__maybe_derive_from_binary_function<void ()>> = {<No data fields>}, __f_ = {__buf_ = {__small = \"`2^\\001\\060`\\000\\000\\000\\000\\000\\000\\000\\000\\000\", __large = 0x6030015e3260}, __invoker_ = {__call_ = 0x348b5a00 <std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::MergeTreeData::getTemporaryPartDirectoryHolder(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const::$_1, void ()> >(std::__1::__function::__policy_storage const*)>}, __policy_ = 0x83d83a0 <std::__1::__function::__policy::__choose_policy[abi:v15000]<std::__1::__function::__default_alloc_func<DB::MergeTreeData::getTemporaryPartDirectoryHolder(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const::$_1, void ()> >(std::__1::integral_constant<bool, false>)::__policy_>}}}}, elapsed_ns = 3350925, block_id = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x604005993d50 \"all_10556743547062697475_3052979394108171700\", __size_ = 44, __cap_ = 48, __is_long_ = 1}, __s = {__data_ = \"P=\\231\\005@`\\000\\000,\\000\\000\\000\\000\\000\\000\\000\\060\\000\\000\\000\\000\\000\", __padding_ = 0x611001063327 \"\\200\\240Q\\t\\003\\200`\", __size_ = 0 '\\000', __is_long_ = 1 '\\001'}, __r = {__words = {105828088102224, 44, 9223372036854775856}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}, block_with_partition = {block = {data = {__begin_ = 0x6080030951a0, __end_ = 0x608003095200, __end_cap_ = {<std::__1::__compressed_pair_elem<DB::ColumnWithTypeAndName*, 0, false>> = {__value_ = 0x608003095200}, <std::__1::__compressed_pair_elem<std::__1::allocator<DB::ColumnWithTypeAndName>, 1, true>> = {<std::__1::allocator<DB::ColumnWithTypeAndName>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<DB::ColumnWithTypeAndName> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}, index_by_name = {__table_ = {__bucket_list_ = {__ptr_ = {<std::__1::__compressed_pair_elem<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>**, 0, false>> = {__value_ = 0x602000ed2350}, <std::__1::__compressed_pair_elem<std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*> >, 1, false>> = {__value_ = {__data_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 2}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*>, 1, true>> = {<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}}, <No data fields>}}, __p1_ = {<std::__1::__compressed_pair_elem<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>, 0, false>> = {__value_ = {__next_ = 0x604001315810}}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> >, 1, true>> = {<std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __p2_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 2}, <std::__1::__compressed_pair_elem<std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, 1, true>> = {<std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>> = {<std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<std::__1::unary_function<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __p3_ = {<std::__1::__compressed_pair_elem<float, 0, false>> = {__value_ = 1}, <std::__1::__compressed_pair_elem<std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, 1, true>> = {<std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>> = {<std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<std::__1::binary_function<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool>> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}}, info = {is_overflows = false, bucket_num = -1}}, partition = {__begin_ = 0x0, __end_ = 0x0, __end_cap_ = {<std::__1::__compressed_pair_elem<DB::Field*, 0, false>> = {__value_ = 0x0}, <std::__1::__compressed_pair_elem<std::__1::allocator<DB::Field>, 1, true>> = {<std::__1::allocator<DB::Field>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<DB::Field> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}, offsets = {__ptr_ = 0x0, __cntrl_ = 0x0}}, block_id_to_offset_idx = {__table_ = {__bucket_list_ = {__ptr_ = {<std::__1::__compressed_pair_elem<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>**, 0, false>> = {__value_ = 0x0}, <std::__1::__compressed_pair_elem<std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*> >, 1, false>> = {__value_ = {__data_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 0}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*>, 1, true>> = {<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}}, <No data fields>}}, __p1_ = {<std::__1::__compressed_pair_elem<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>, 0, false>> = {__value_ = {__next_ = 0x0}}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> >, 1, true>> = {<std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __p2_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 0}, <std::__1::__compressed_pair_elem<std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, 1, true>> = {<std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>> = {<std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<std::__1::unary_function<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __p3_ = {<std::__1::__compressed_pair_elem<float, 0, false>> = {__value_ = 1}, <std::__1::__compressed_pair_elem<std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, 1, true>> = {<std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>> = {<std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<std::__1::binary_function<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool>> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}}}\r\n2023-01-13 18:20:25         __range1 = <optimized out>\r\n2023-01-13 18:20:25         __begin1 = {__i = 0x6110010632c0}\r\n2023-01-13 18:20:25         __end1 = {__i = 0x6110010633c0}\r\n2023-01-13 18:20:25 #18 0x0000000034e769c4 in DB::ReplicatedMergeTreeSinkImpl<false>::consume (this=0x616004db9e98, chunk=...) at ../src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp:435\r\n2023-01-13 18:20:25         block = {data = {__begin_ = 0x608002c75ba0, __end_ = 0x608002c75c00, __end_cap_ = {<std::__1::__compressed_pair_elem<DB::ColumnWithTypeAndName*, 0, false>> = {__value_ = 0x608002c75c00}, <std::__1::__compressed_pair_elem<std::__1::allocator<DB::ColumnWithTypeAndName>, 1, true>> = {<std::__1::allocator<DB::ColumnWithTypeAndName>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<DB::ColumnWithTypeAndName> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}, index_by_name = {__table_ = {__bucket_list_ = {__ptr_ = {<std::__1::__compressed_pair_elem<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>**, 0, false>> = {__value_ = 0x60200057dfb0}, <std::__1::__compressed_pair_elem<std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*> >, 1, false>> = {__value_ = {__data_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 2}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*>, 1, true>> = {<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>*> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}}, <No data fields>}}, __p1_ = {<std::__1::__compressed_pair_elem<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*>*>, 0, false>> = {__value_ = {__next_ = 0x60400139d9d0}}, <std::__1::__compressed_pair_elem<std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> >, 1, true>> = {<std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> >> = {<std::__1::__non_trivial_if<true, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, void*> > >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __p2_ = {<std::__1::__compressed_pair_elem<unsigned long, 0, false>> = {__value_ = 2}, <std::__1::__compressed_pair_elem<std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, 1, true>> = {<std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>> = {<std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<std::__1::unary_function<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, __p3_ = {<std::__1::__compressed_pair_elem<float, 0, false>> = {__value_ = 1}, <std::__1::__compressed_pair_elem<std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, 1, true>> = {<std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>> = {<std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >> = {<std::__1::binary_function<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool>> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}}, info = {is_overflows = false, bucket_num = -1}}\r\n2023-01-13 18:20:25         zookeeper = {__ptr_ = 0x60c0038bb340, __cntrl_ = 0x603001e136c0}\r\n2023-01-13 18:20:25         quorum_retries_ctl = {name = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x603002375140 \"checkQuorumPrecondition\", __size_ = 23, __cap_ = 32, __is_long_ = 1}, __s = {__data_ = \"@Q7\\002\\060`\\000\\000\\027\\000\\000\\000\\000\\000\\000\\000 \\000\\000\\000\\000\\000\", __padding_ = 0x7fa659ee13a7 \"\\200\\370\\237\\333\\004`a\", __size_ = 0 '\\000', __is_long_ = 1 '\\001'}, __r = {__words = {105759311876416, 23, 9223372036854775840}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}, retries_info = @0x616004db9ff8, iteration_count = 1, user_error = {code = 0, message = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x0, __size_ = 0, __cap_ = 0, __is_long_ = 0}, __s = {__data_ = '\\000' <repeats 22 times>, __padding_ = 0x7fa659ee13d7 \"\", __size_ = 0 '\\000', __is_long_ = 0 '\\000'}, __r = {__words = {0, 0, 0}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}}, keeper_error = {code = Coordination::Error::ZOK, message = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x0, __size_ = 0, __cap_ = 0, __is_long_ = 0}, __s = {__data_ = '\\000' <repeats 22 times>, __padding_ = 0x7fa659ee13f7 \"\", __size_ = 0 '\\000', __is_long_ = 0 '\\000'}, __r = {__words = {0, 0, 0}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}}, action_after_last_failed_retry = {<std::__1::__function::__maybe_derive_from_unary_function<void ()>> = {<No data fields>}, <std::__1::__function::__maybe_derive_from_binary_function<void ()>> = {<No data fields>}, __f_ = {__buf_ = {__small = '\\000' <repeats 15 times>, __large = 0x0}, __invoker_ = {__call_ = 0x34e86ac0 <std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::ZooKeeperRetriesControl::action_after_last_failed_retry::{lambda()#1}, void ()> >(std::__1::__function::__policy_storage const*)>}, __policy_ = 0x8432860 <std::__1::__function::__policy::__choose_policy[abi:v15000]<std::__1::__function::__default_alloc_func<DB::ZooKeeperRetriesControl::action_after_last_failed_retry::{lambda()#1}, void ()> >(std::__1::integral_constant<bool, true>)::__policy_>}}, unconditional_retry = false, iteration_succeeded = true, stop_retries = false}\r\n2023-01-13 18:20:25         chunk_offsets = {__ptr_ = 0x0, __cntrl_ = 0x0}\r\n2023-01-13 18:20:25         part_blocks = {__begin_ = 0x60b003aa0c90, __end_ = 0x60b003aa0d00, __end_cap_ = {<std::__1::__compressed_pair_elem<DB::BlockWithPartition*, 0, false>> = {__value_ = 0x60b003aa0d00}, <std::__1::__compressed_pair_elem<std::__1::allocator<DB::BlockWithPartition>, 1, true>> = {<std::__1::allocator<DB::BlockWithPartition>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<DB::BlockWithPartition> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}\r\n2023-01-13 18:20:25         partitions = {__begin_ = 0x0, __end_ = 0x0, __end_cap_ = {<std::__1::__compressed_pair_elem<DB::ReplicatedMergeTreeSinkImpl<false>::DelayedChunk::Partition*, 0, false>> = {__value_ = 0x0}, <std::__1::__compressed_pair_elem<std::__1::allocator<DB::ReplicatedMergeTreeSinkImpl<false>::DelayedChunk::Partition>, 1, true>> = {<std::__1::allocator<DB::ReplicatedMergeTreeSinkImpl<false>::DelayedChunk::Partition>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<DB::ReplicatedMergeTreeSinkImpl<false>::DelayedChunk::Partition> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}}\r\n2023-01-13 18:20:25         settings = @0x62505ae53bd8: <incomplete type>\r\n2023-01-13 18:20:25         replicas_num = <optimized out>\r\n2023-01-13 18:20:25         streams = <optimized out>\r\n2023-01-13 18:20:25         support_parallel_write = false\r\n2023-01-13 18:20:25 #19 0x0000000035e6ce9b in DB::SinkToStorage::onConsume (this=0x616004db9e98, chunk=...) at ../src/Processors/Sinks/SinkToStorage.cpp:18\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #20 0x0000000035c62aea in DB::ExceptionKeepingTransform::work()::$_1::operator()() const (this=<optimized out>) at ../src/Processors/Transforms/ExceptionKeepingTransform.cpp:151\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #21 std::__1::__invoke[abi:v15000]<DB::ExceptionKeepingTransform::work()::$_1&> (__f=...) at ../contrib/llvm-project/libcxx/include/__functional/invoke.h:394\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #22 std::__1::__invoke_void_return_wrapper<void, true>::__call<DB::ExceptionKeepingTransform::work()::$_1&>(DB::ExceptionKeepingTransform::work()::$_1&) (__args=...) at ../contrib/llvm-project/libcxx/include/__functional/invoke.h:479\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #23 std::__1::__function::__default_alloc_func<DB::ExceptionKeepingTransform::work()::$_1, void ()>::operator()[abi:v15000]() (this=<optimized out>) at ../contrib/llvm-project/libcxx/include/__functional/function.h:235\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #24 std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::ExceptionKeepingTransform::work()::$_1, void ()> >(std::__1::__function::__policy_storage const*) (__buf=<optimized out>) at ../contrib/llvm-project/libcxx/include/__functional/function.h:716\r\n2023-01-13 18:20:25         __f = <optimized out>\r\n2023-01-13 18:20:25 #25 0x0000000035c625a0 in std::__1::__function::__policy_func<void ()>::operator()[abi:v15000]() const (this=0x7fb5ee6e100b <raise+203>) at ../contrib/llvm-project/libcxx/include/__functional/function.h:848\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #26 std::__1::function<void ()>::operator()() const (this=0x7fb5ee6e100b <raise+203>) at ../contrib/llvm-project/libcxx/include/__functional/function.h:1187\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #27 DB::runStep(std::__1::function<void ()>, DB::ThreadStatus*, std::__1::atomic<unsigned long>*) (step=..., thread_status=<optimized out>, elapsed_ms=<optimized out>) at ../src/Processors/Transforms/ExceptionKeepingTransform.cpp:115\r\n2023-01-13 18:20:25         res = {__ptr_ = 0x0}\r\n2023-01-13 18:20:25         watch = {<std::__1::__optional_move_assign_base<Stopwatch, true>> = {<std::__1::__optional_copy_assign_base<Stopwatch, true>> = {<std::__1::__optional_move_base<Stopwatch, true>> = {<std::__1::__optional_copy_base<Stopwatch, true>> = {<std::__1::__optional_storage_base<Stopwatch, false>> = {<std::__1::__optional_destruct_base<Stopwatch, true>> = {{__null_state_ = <optimized out>, __val_ = {start_ns = <optimized out>, stop_ns = <optimized out>, clock_type = <optimized out>, is_running = true}}, __engaged_ = <synthetic pointer>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <std::__1::__sfinae_ctor_base<true, true>> = {<No data fields>}, <std::__1::__sfinae_assign_base<true, true>> = {<No data fields>}, <No data fields>}\r\n2023-01-13 18:20:25         original_thread = 0x7fa659d8f860\r\n2023-01-13 18:20:25         scope_exit101 = <optimized out>\r\n2023-01-13 18:20:25 #28 0x0000000035c61330 in DB::ExceptionKeepingTransform::work (this=0x616004db9e98) at ../src/Processors/Transforms/ExceptionKeepingTransform.cpp:151\r\n2023-01-13 18:20:25         exception = {__ptr_ = 0x0}\r\n2023-01-13 18:20:25 #29 0x00000000356780e1 in DB::executeJob (node=0x611004205bc0, read_progress_callback=0x60e002c04440) at ../src/Processors/Executors/ExecutionThreadContext.cpp:47\r\n2023-01-13 18:20:25         exception = <optimized out>\r\n2023-01-13 18:20:25         is_source = <optimized out>\r\n2023-01-13 18:20:25         read_progress = <optimized out>\r\n2023-01-13 18:20:25 #30 DB::ExecutionThreadContext::executeTask (this=<optimized out>) at ../src/Processors/Executors/ExecutionThreadContext.cpp:92\r\n2023-01-13 18:20:25         span = {__ptr_ = {<std::__1::__compressed_pair_elem<DB::OpenTelemetry::SpanHolder*, 0, false>> = {__value_ = 0x0}, <std::__1::__compressed_pair_elem<std::__1::default_delete<DB::OpenTelemetry::SpanHolder>, 1, true>> = {<std::__1::default_delete<DB::OpenTelemetry::SpanHolder>> = {<No data fields>}, <No data fields>}, <No data fields>}}\r\n2023-01-13 18:20:25         execution_time_watch = {<std::__1::__optional_move_assign_base<Stopwatch, true>> = {<std::__1::__optional_copy_assign_base<Stopwatch, true>> = {<std::__1::__optional_move_base<Stopwatch, true>> = {<std::__1::__optional_copy_base<Stopwatch, true>> = {<std::__1::__optional_storage_base<Stopwatch, false>> = {<std::__1::__optional_destruct_base<Stopwatch, true>> = {{__null_state_ = <optimized out>, __val_ = {start_ns = <optimized out>, stop_ns = <optimized out>, clock_type = <optimized out>, is_running = true}}, __engaged_ = <synthetic pointer>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, <std::__1::__sfinae_ctor_base<true, true>> = {<No data fields>}, <std::__1::__sfinae_assign_base<true, true>> = {<No data fields>}, <No data fields>}\r\n2023-01-13 18:20:25 #31 0x000000003565711d in DB::PipelineExecutor::executeStepImpl (this=0x615000d09618, thread_num=<optimized out>, yield_flag=<optimized out>) at ../src/Processors/Executors/PipelineExecutor.cpp:229\r\n2023-01-13 18:20:25         context = @0x61000260a840: <incomplete type>\r\n2023-01-13 18:20:25         yield = false\r\n2023-01-13 18:20:25 #32 0x0000000035655a86 in DB::PipelineExecutor::executeSingleThread (this=0x615000d09618, thread_num=0) at ../src/Processors/Executors/PipelineExecutor.cpp:195\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #33 DB::PipelineExecutor::executeImpl (this=0x615000d09618, num_threads=<optimized out>) at ../src/Processors/Executors/PipelineExecutor.cpp:372\r\n2023-01-13 18:20:25         slot = {__ptr_ = 0x6020004da610, __cntrl_ = 0x603000694900}\r\n2023-01-13 18:20:25         finished_flag = false\r\n2023-01-13 18:20:25         scope_exit361 = {static is_nullable = <optimized out>, function = {finished_flag = <optimized out>, this = 0x615000d09618}}\r\n2023-01-13 18:20:25 #34 0x0000000035655675 in DB::PipelineExecutor::execute (this=0x615000d09618, num_threads=140352485083408) at ../src/Processors/Executors/PipelineExecutor.cpp:90\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #35 0x00000000356537ab in DB::threadFunction (data=..., thread_group=..., num_threads=1) at ../src/Processors/Executors/CompletedPipelineExecutor.cpp:48\r\n2023-01-13 18:20:25         scope_exit40 = {static is_nullable = false, function = {<No data fields>}}\r\n2023-01-13 18:20:25 #36 DB::CompletedPipelineExecutor::execute()::$_0::operator()() const (this=<optimized out>) at ../src/Processors/Executors/CompletedPipelineExecutor.cpp:84\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #37 std::__1::__invoke[abi:v15000]<DB::CompletedPipelineExecutor::execute()::$_0&> (__f=...) at ../contrib/llvm-project/libcxx/include/__functional/invoke.h:394\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #38 std::__1::__apply_tuple_impl[abi:v15000]<DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&>(DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) (__f=..., __t=...) at ../contrib/llvm-project/libcxx/include/tuple:1789\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #39 std::__1::apply[abi:v15000]<DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&>(DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&) (__f=..., __t=...) at ../contrib/llvm-project/libcxx/include/tuple:1798\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #40 ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}::operator()() (this=0x60605d7f84a0) at ../src/Common/ThreadPool.h:196\r\n2023-01-13 18:20:25         thread_status = <incomplete type>\r\n2023-01-13 18:20:25         scope_exit184 = {static is_nullable = false, function = {<No data fields>}}\r\n2023-01-13 18:20:25         function = {data_ptr = 0x60d000b861a0, num_threads = 1, thread_group = {__ptr_ = <synthetic pointer>, __cntrl_ = <synthetic pointer>}}\r\n2023-01-13 18:20:25         arguments = <optimized out>\r\n2023-01-13 18:20:25 #41 std::__1::__invoke[abi:v15000]<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}&> (__f=...) at ../contrib/llvm-project/libcxx/include/__functional/invoke.h:394\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #42 std::__1::__invoke_void_return_wrapper<void, true>::__call<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}&>(ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}&) (__args=...) at ../contrib/llvm-project/libcxx/include/__functional/invoke.h:479\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #43 std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}, void ()>::operator()[abi:v15000]() (this=0x60605d7f84a0) at ../contrib/llvm-project/libcxx/include/__functional/function.h:235\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #44 std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}, void ()> >(std::__1::__function::__policy_storage const*) (__buf=<optimized out>) at ../contrib/llvm-project/libcxx/include/__functional/function.h:716\r\n2023-01-13 18:20:25         __f = 0x60605d7f84a0\r\n2023-01-13 18:20:25 #45 0x00000000202ff12d in std::__1::__function::__policy_func<void ()>::operator()[abi:v15000]() const (this=0x7fa659ccc030) at ../contrib/llvm-project/libcxx/include/__functional/function.h:848\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #46 std::__1::function<void ()>::operator()() const (this=0x7fa659ccc030) at ../contrib/llvm-project/libcxx/include/__functional/function.h:1187\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #47 ThreadPoolImpl<std::__1::thread>::worker (this=<optimized out>, thread_it=...) at ../src/Common/ThreadPool.cpp:295\r\n2023-01-13 18:20:25         metric_active_threads = {what = 0x46d8b5e8 <CurrentMetrics::values+456>, amount = 1}\r\n2023-01-13 18:20:25         thread_trace_context = {root_span = {trace_id = {t = {items = {0, 0}}}, span_id = 0, parent_span_id = 0, operation_name = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x0, __size_ = 0, __cap_ = 0, __is_long_ = 0}, __s = {__data_ = '\\000' <repeats 22 times>, __padding_ = 0x7fa659ccc137 \"\", __size_ = 0 '\\000', __is_long_ = 0 '\\000'}, __r = {__words = {0, 0, 0}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}, start_time_us = 0, finish_time_us = 0, attributes = {<std::__1::vector<DB::Field, AllocatorWithMemoryTracking<DB::Field> >> = {__begin_ = 0x0, __end_ = 0x0, __end_cap_ = {<std::__1::__compressed_pair_elem<DB::Field*, 0, false>> = {__value_ = 0x0}, <std::__1::__compressed_pair_elem<AllocatorWithMemoryTracking<DB::Field>, 1, true>> = {<AllocatorWithMemoryTracking<DB::Field>> = {<No data fields>}, <No data fields>}, <No data fields>}}, <No data fields>}}, is_context_owner = true}\r\n2023-01-13 18:20:25         job = {<std::__1::__function::__maybe_derive_from_unary_function<void ()>> = {<No data fields>}, <std::__1::__function::__maybe_derive_from_binary_function<void ()>> = {<No data fields>}, __f_ = {__buf_ = {__small = \"\\240\\204\\177]``\\000\\000[::1]:52\", __large = 0x60605d7f84a0}, __invoker_ = {__call_ = 0x35653560 <std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}, void ()> >(std::__1::__function::__policy_storage const*)>}, __policy_ = 0x84e63c0 <std::__1::__function::__policy::__choose_policy[abi:v15000]<std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::{lambda()#1}, void ()> >(std::__1::integral_constant<bool, false>)::__policy_>}}\r\n2023-01-13 18:20:25         parent_thead_trace_context = {<DB::OpenTelemetry::TracingContext> = {trace_id = {t = {items = {0, 0}}}, span_id = 0, tracestate = {static __endian_factor = 1, __r_ = {<std::__1::__compressed_pair_elem<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__rep, 0, false>> = {__value_ = {{__l = {__data_ = 0x0, __size_ = 0, __cap_ = 0, __is_long_ = 0}, __s = {__data_ = '\\000' <repeats 22 times>, __padding_ = 0x7fa659ccc09f \"\", __size_ = 0 '\\000', __is_long_ = 0 '\\000'}, __r = {__words = {0, 0, 0}}}}}, <std::__1::__compressed_pair_elem<std::__1::allocator<char>, 1, true>> = {<std::__1::allocator<char>> = {<std::__1::__non_trivial_if<true, std::__1::allocator<char> >> = {<No data fields>}, <No data fields>}, <No data fields>}, <No data fields>}, static npos = 18446744073709551615}, trace_flags = 0 '\\000'}, span_log = {__ptr_ = 0x0, __cntrl_ = 0x0}}\r\n2023-01-13 18:20:25         need_shutdown = <optimized out>\r\n2023-01-13 18:20:25         metric_all_threads = {what = 0x46d8b5e0 <CurrentMetrics::values+448>, amount = 1}\r\n2023-01-13 18:20:25 #48 0x0000000020309340 in ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::{lambda()#2}::operator()() const (this=<optimized out>) at ../src/Common/ThreadPool.cpp:144\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #49 std::__1::__invoke[abi:v15000]<ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::{lambda()#2}> (__f=...) at ../contrib/llvm-project/libcxx/include/__functional/invoke.h:394\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #50 std::__1::__thread_execute[abi:v15000]<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::{lambda()#2}>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::{lambda()#2}>&, std::__1::__tuple_indices<>) (__t=...) at ../contrib/llvm-project/libcxx/include/thread:284\r\n2023-01-13 18:20:25 No locals.\r\n2023-01-13 18:20:25 #51 std::__1::__thread_proxy[abi:v15000]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::{lambda()#2}> >(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::{lambda()#2}>) (__vp=<optimized out>) at ../contrib/llvm-project/libcxx/include/thread:295\r\n2023-01-13 18:20:25         __p = {__ptr_ = {<std::__1::__compressed_pair_elem<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, (lambda at ../src/Common/ThreadPool.cpp:144:42)> *, 0, false>> = {__value_ = 0x60300132e4a0}, <std::__1::__compressed_pair_elem<std::__1::default_delete<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, (lambda at ../src/Common/ThreadPool.cpp:144:42)> >, 1, true>> = {<std::__1::default_delete<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, (lambda at ../src/Common/ThreadPool.cpp:144:42)> >> = {<No data fields>}, <No data fields>}, <No data fields>}}\r\n2023-01-13 18:20:25 #52 0x00007fb5ee898609 in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n2023-01-13 18:20:25 No symbol table info available.\r\n2023-01-13 18:20:25 #53 0x00007fb5ee7bd133 in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n2023-01-13 18:20:25 No symbol table info available.\r\n```",
  "created_at": "2023-05-02T22:32:51Z"
}