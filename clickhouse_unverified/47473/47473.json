{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 47473,
  "instance_id": "ClickHouse__ClickHouse-47473",
  "issue_numbers": [
    "47182"
  ],
  "base_commit": "45fe39fa9c7d9f2d39fd68b9fce32058e76c9ad3",
  "patch": "diff --git a/src/Interpreters/InterpreterExplainQuery.cpp b/src/Interpreters/InterpreterExplainQuery.cpp\nindex b2172a07e918..3c225522cc47 100644\n--- a/src/Interpreters/InterpreterExplainQuery.cpp\n+++ b/src/Interpreters/InterpreterExplainQuery.cpp\n@@ -504,7 +504,10 @@ QueryPipeline InterpreterExplainQuery::executeImpl()\n                     auto pipe = QueryPipelineBuilder::getPipe(std::move(*pipeline), resources);\n                     const auto & processors = pipe.getProcessors();\n \n-                    printPipeline(processors, buf);\n+                    if (settings.compact)\n+                        printPipelineCompact(processors, buf, settings.query_pipeline_options.header);\n+                    else\n+                        printPipeline(processors, buf);\n                 }\n                 else\n                 {\ndiff --git a/src/Processors/QueryPlan/ISourceStep.cpp b/src/Processors/QueryPlan/ISourceStep.cpp\nindex 0644d9b44ebf..37f56bc7a43a 100644\n--- a/src/Processors/QueryPlan/ISourceStep.cpp\n+++ b/src/Processors/QueryPlan/ISourceStep.cpp\n@@ -12,10 +12,19 @@ ISourceStep::ISourceStep(DataStream output_stream_)\n QueryPipelineBuilderPtr ISourceStep::updatePipeline(QueryPipelineBuilders, const BuildQueryPipelineSettings & settings)\n {\n     auto pipeline = std::make_unique<QueryPipelineBuilder>();\n-    QueryPipelineProcessorsCollector collector(*pipeline, this);\n+\n+    /// For `Source` step, since it's not add new Processors to `pipeline->pipe`\n+    /// in `initializePipeline`, but make an assign with new created Pipe.\n+    /// And Processors for the Step is added here. So we do not need to use\n+    /// `QueryPipelineProcessorsCollector` to collect Processors.\n     initializePipeline(*pipeline, settings);\n-    auto added_processors = collector.detachProcessors();\n-    processors.insert(processors.end(), added_processors.begin(), added_processors.end());\n+\n+    /// But we need to set QueryPlanStep manually for the Processors, which\n+    /// will be used in `EXPLAIN PIPELINE`\n+    for (auto & processor : processors)\n+    {\n+        processor->setQueryPlanStep(this);\n+    }\n     return pipeline;\n }\n \ndiff --git a/src/QueryPipeline/printPipeline.cpp b/src/QueryPipeline/printPipeline.cpp\nnew file mode 100644\nindex 000000000000..40c88502ed09\n--- /dev/null\n+++ b/src/QueryPipeline/printPipeline.cpp\n@@ -0,0 +1,177 @@\n+#include <QueryPipeline/printPipeline.h>\n+#include <Processors/QueryPlan/IQueryPlanStep.h>\n+#include <set>\n+#include <map>\n+\n+namespace DB\n+{\n+\n+void printPipelineCompact(const Processors & processors, WriteBuffer & out, bool with_header)\n+{\n+    struct Node;\n+\n+    /// Group by processors name, QueryPlanStep and group in this step.\n+    struct Key\n+    {\n+        size_t group;\n+        IQueryPlanStep * step;\n+        std::string name;\n+\n+        auto getTuple() const { return std::forward_as_tuple(group, step, name); }\n+\n+        bool operator<(const Key & other) const\n+        {\n+            return getTuple() < other.getTuple();\n+        }\n+    };\n+\n+    /// Group ports by header.\n+    struct EdgeData\n+    {\n+        Block header;\n+        size_t count;\n+    };\n+\n+    using Edge = std::vector<EdgeData>;\n+\n+    struct Node\n+    {\n+        size_t id = 0;\n+        std::map<Node *, Edge> edges = {};\n+        std::vector<const IProcessor *> agents = {};\n+    };\n+\n+    std::map<Key, Node> graph;\n+\n+    auto get_key = [](const IProcessor & processor)\n+    {\n+        return Key{processor.getQueryPlanStepGroup(), processor.getQueryPlanStep(), processor.getName()};\n+    };\n+\n+    /// Fill nodes.\n+    for (const auto & processor : processors)\n+    {\n+        auto res = graph.emplace(get_key(*processor), Node());\n+        auto & node = res.first->second;\n+        node.agents.emplace_back(processor.get());\n+\n+        if (res.second)\n+            node.id = graph.size();\n+    }\n+\n+    Block empty_header;\n+\n+    /// Fill edges.\n+    for (const auto & processor : processors)\n+    {\n+        auto & from =  graph[get_key(*processor)];\n+\n+        for (auto & port : processor->getOutputs())\n+        {\n+            if (!port.isConnected())\n+                continue;\n+\n+            auto & to = graph[get_key(port.getInputPort().getProcessor())];\n+            auto & edge = from.edges[&to];\n+\n+            /// Use empty header for each edge if with_header is false.\n+            const auto & header = with_header ? port.getHeader()\n+                                              : empty_header;\n+\n+            /// Group by header.\n+            bool found = false;\n+            for (auto & item : edge)\n+            {\n+                if (blocksHaveEqualStructure(header, item.header))\n+                {\n+                    found = true;\n+                    ++item.count;\n+                    break;\n+                }\n+            }\n+\n+            if (!found)\n+                edge.emplace_back(EdgeData{header, 1});\n+        }\n+    }\n+\n+    /// Group processors by it's QueryPlanStep.\n+    std::map<IQueryPlanStep *, std::vector<const Node *>> steps_map;\n+\n+    for (const auto & item : graph)\n+        steps_map[item.first.step].emplace_back(&item.second);\n+\n+    out << \"digraph\\n{\\n\";\n+    out << \"  rankdir=\\\"LR\\\";\\n\";\n+    out << \"  { node [shape = rect]\\n\";\n+\n+    /// Nodes // TODO quoting and escaping\n+    size_t next_step = 0;\n+    for (const auto & item : steps_map)\n+    {\n+        /// Use separate clusters for each step.\n+        if (item.first != nullptr)\n+        {\n+            out << \"    subgraph cluster_\" << next_step << \" {\\n\";\n+            out << \"      label =\\\"\" << item.first->getName() << \"\\\";\\n\";\n+            out << \"      style=filled;\\n\";\n+            out << \"      color=lightgrey;\\n\";\n+            out << \"      node [style=filled,color=white];\\n\";\n+            out << \"      { rank = same;\\n\";\n+\n+            ++next_step;\n+        }\n+\n+        for (const auto & node : item.second)\n+        {\n+            const auto & processor = node->agents.front();\n+            out << \"        n\" << node->id << \" [label=\\\"\" << processor->getName();\n+\n+            if (node->agents.size() > 1)\n+                out << \" \u00d7 \" << node->agents.size();\n+\n+            const auto & description = processor->getDescription();\n+            if (!description.empty())\n+                out << ' ' << description;\n+\n+            out << \"\\\"];\\n\";\n+        }\n+\n+        if (item.first != nullptr)\n+        {\n+            out << \"      }\\n\";\n+            out << \"    }\\n\";\n+        }\n+    }\n+\n+    out << \"  }\\n\";\n+\n+    /// Edges\n+    for (const auto & item : graph)\n+    {\n+        for (const auto & edge : item.second.edges)\n+        {\n+            for (const auto & data : edge.second)\n+            {\n+                out << \"  n\" << item.second.id << \" -> \" << \"n\" << edge.first->id << \" [label=\\\"\";\n+\n+                if (data.count > 1)\n+                    out << \"\u00d7 \" << data.count;\n+\n+                if (with_header)\n+                {\n+                    for (const auto & elem : data.header)\n+                    {\n+                        out << \"\\n\";\n+                        elem.dumpStructure(out);\n+                    }\n+                }\n+\n+                out << \"\\\"];\\n\";\n+            }\n+        }\n+    }\n+    out << \"}\\n\";\n+}\n+\n+}\ndiff --git a/src/QueryPipeline/printPipeline.h b/src/QueryPipeline/printPipeline.h\nindex ff3b53300ce6..e91909cb50b6 100644\n--- a/src/QueryPipeline/printPipeline.h\n+++ b/src/QueryPipeline/printPipeline.h\n@@ -64,4 +64,9 @@ void printPipeline(const Processors & processors, WriteBuffer & out)\n     printPipeline(processors, std::vector<IProcessor::Status>(), out);\n }\n \n+/// Prints pipeline in compact representation.\n+/// Group processors by it's name, QueryPlanStep and QueryPlanStepGroup.\n+/// If QueryPlanStep wasn't set for processor, representation may be not correct.\n+/// If with_header is set, prints block header for each edge.\n+void printPipelineCompact(const Processors & processors, WriteBuffer & out, bool with_header);\n }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/25340_grace_hash_limit_race.reference b/tests/queries/0_stateless/02677_grace_hash_limit_race.reference\nsimilarity index 100%\nrename from tests/queries/0_stateless/25340_grace_hash_limit_race.reference\nrename to tests/queries/0_stateless/02677_grace_hash_limit_race.reference\ndiff --git a/tests/queries/0_stateless/25340_grace_hash_limit_race.sql b/tests/queries/0_stateless/02677_grace_hash_limit_race.sql\nsimilarity index 100%\nrename from tests/queries/0_stateless/25340_grace_hash_limit_race.sql\nrename to tests/queries/0_stateless/02677_grace_hash_limit_race.sql\ndiff --git a/tests/queries/0_stateless/02678_explain_pipeline_graph.reference b/tests/queries/0_stateless/02678_explain_pipeline_graph_with_projection.reference\nsimilarity index 100%\nrename from tests/queries/0_stateless/02678_explain_pipeline_graph.reference\nrename to tests/queries/0_stateless/02678_explain_pipeline_graph_with_projection.reference\ndiff --git a/tests/queries/0_stateless/02678_explain_pipeline_graph.sql b/tests/queries/0_stateless/02678_explain_pipeline_graph_with_projection.sql\nsimilarity index 50%\nrename from tests/queries/0_stateless/02678_explain_pipeline_graph.sql\nrename to tests/queries/0_stateless/02678_explain_pipeline_graph_with_projection.sql\nindex 48cfbf2b3493..e8b7405d6024 100644\n--- a/tests/queries/0_stateless/02678_explain_pipeline_graph.sql\n+++ b/tests/queries/0_stateless/02678_explain_pipeline_graph_with_projection.sql\n@@ -1,7 +1,12 @@\n--- The server does not crash after these queries:\n-\n DROP TABLE IF EXISTS t1;\n CREATE TABLE t1(ID UInt64, name String) engine=MergeTree order by ID;\n+\n insert into t1(ID, name) values (1, 'abc'), (2, 'bbb');\n+\n+-- The returned node order is uncertain\n explain pipeline graph=1 select count(ID) from t1 FORMAT Null;\n+explain pipeline graph=1 select sum(1) from t1 FORMAT Null;\n+explain pipeline graph=1 select min(ID) from t1 FORMAT Null;\n+explain pipeline graph=1 select max(ID) from t1 FORMAT Null;\n+\n DROP TABLE t1;\n",
  "problem_statement": "Crash when execute `explain pipeline graph=1 select count(ID) from t1`\n** Version **\r\n22.9\r\nand latest should have the issue too. In latest version, no below error message but there is a mess in the output.\r\n\r\n** Reproduce **\r\nhttps://fiddle.clickhouse.com/8308205c-5098-4363-b566-bc690223fa87\r\n```SQL\r\nCREATE TABLE t1(ID UInt64, name String) engine=MergeTree order by ID;\r\n\r\ninsert into t1(ID, name) values (1, 'abc'), (2, 'bbb');\r\n\r\nexplain pipeline graph=1 select count(ID) from t1;\r\n```\r\n\r\n**Error message**\r\n```\r\n[3b65c6d14127] 2023.03.03 08:14:11.235194 [ 258 ] <Fatal> BaseDaemon: ########################################\r\n[3b65c6d14127] 2023.03.03 08:14:11.235257 [ 258 ] <Fatal> BaseDaemon: (version 22.9.7.34 (official build), build id: EF7468FF8440F1A7E98894D99638C4583A10FB41) (from thread 54) (query_id: 6ff39b5f-c2c2-4469-9b2c-f114edf9f77c) (query: explain pipeline graph=1 select count(ID) from t1;) Received signal Segmentation fault (11)\r\n[3b65c6d14127] 2023.03.03 08:14:11.235286 [ 258 ] <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Unknown si_code.\r\n[3b65c6d14127] 2023.03.03 08:14:11.235313 [ 258 ] <Fatal> BaseDaemon: Stack trace: 0x1207f1a6 0x1207332d 0x12070fad 0x123f552b 0x123f2416 0x12f96a05 0x12faa919 0x15a3da4f 0x15a3fddb 0x15bfb432 0x15bf8bbd 0x7f69933d0609 0x7f69932f5133\r\n[3b65c6d14127] 2023.03.03 08:14:11.235368 [ 258 ] <Fatal> BaseDaemon: 2. DB::printPipelineCompact(std::__1::vector<std::__1::shared_ptr<DB::IProcessor>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > > const&, DB::WriteBuffer&, bool) @ 0x1207f1a6 in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235397 [ 258 ] <Fatal> BaseDaemon: 3. DB::InterpreterExplainQuery::executeImpl() @ 0x1207332d in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235416 [ 258 ] <Fatal> BaseDaemon: 4. DB::InterpreterExplainQuery::execute() @ 0x12070fad in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235445 [ 258 ] <Fatal> BaseDaemon: 5. ? @ 0x123f552b in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235471 [ 258 ] <Fatal> BaseDaemon: 6. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x123f2416 in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235498 [ 258 ] <Fatal> BaseDaemon: 7. DB::TCPHandler::runImpl() @ 0x12f96a05 in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235519 [ 258 ] <Fatal> BaseDaemon: 8. DB::TCPHandler::run() @ 0x12faa919 in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235538 [ 258 ] <Fatal> BaseDaemon: 9. Poco::Net::TCPServerConnection::start() @ 0x15a3da4f in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235578 [ 258 ] <Fatal> BaseDaemon: 10. Poco::Net::TCPServerDispatcher::run() @ 0x15a3fddb in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235607 [ 258 ] <Fatal> BaseDaemon: 11. Poco::PooledThread::run() @ 0x15bfb432 in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235630 [ 258 ] <Fatal> BaseDaemon: 12. Poco::ThreadImpl::runnableEntry(void*) @ 0x15bf8bbd in /usr/bin/clickhouse\r\n[3b65c6d14127] 2023.03.03 08:14:11.235655 [ 258 ] <Fatal> BaseDaemon: 13. ? @ 0x7f69933d0609 in ?\r\n[3b65c6d14127] 2023.03.03 08:14:11.235672 [ 258 ] <Fatal> BaseDaemon: 14. __clone @ 0x7f69932f5133 in ?\r\n[3b65c6d14127] 2023.03.03 08:14:11.349745 [ 258 ] <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: A70D04D997328309562E75C69D55D0D5)\r\nError on processing query: Code: 32. DB::Exception: Attempt to read after eof: while receiving packet from localhost:9000. (ATTEMPT_TO_READ_AFTER_EOF) (version 22.9.7.34 (official build))\r\n(query: explain pipeline graph=1 select count(ID) from t1;)\r\n```\r\n\r\n** Diagnositic \r\n![image](https://user-images.githubusercontent.com/1518453/222668077-fb827a05-51a0-4397-a1be-d4d0986125fb.png)\r\n\r\nIn printPipeline.cpp, line 110 ~ 128, the `item.first` of an item in `steps_map` points to an invalid memory, and `typeid()` does not work either.\n",
  "hints_text": "I cannot see the type information for the bad pointer, but it should be closely related to AggregatingStep. \nI confirm the issue.",
  "created_at": "2023-03-11T15:06:13Z"
}