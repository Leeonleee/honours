{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 39507,
  "instance_id": "ClickHouse__ClickHouse-39507",
  "issue_numbers": [
    "13826"
  ],
  "base_commit": "4d6ef8ed86be58ac131f6183bbd5d9bfeb6003ab",
  "patch": "diff --git a/docs/en/sql-reference/statements/alter/partition.md b/docs/en/sql-reference/statements/alter/partition.md\nindex 114b8d5ffe32..5659a0565c54 100644\n--- a/docs/en/sql-reference/statements/alter/partition.md\n+++ b/docs/en/sql-reference/statements/alter/partition.md\n@@ -112,7 +112,7 @@ Note that:\n For the query to run successfully, the following conditions must be met:\n \n - Both tables must have the same structure.\n-- Both tables must have the same partition key, the same order by key and the same primary key.\n+- Both tables must have the same order by key and the same primary key.\n - Both tables must have the same indices and projections.\n - Both tables must have the same storage policy.\n \ndiff --git a/src/Interpreters/MonotonicityCheckVisitor.h b/src/Interpreters/MonotonicityCheckVisitor.h\nindex cc3868250242..4e71bd568518 100644\n--- a/src/Interpreters/MonotonicityCheckVisitor.h\n+++ b/src/Interpreters/MonotonicityCheckVisitor.h\n@@ -1,13 +1,17 @@\n #pragma once\n \n #include <AggregateFunctions/AggregateFunctionFactory.h>\n+#include <Core/Range.h>\n #include <DataTypes/DataTypeFactory.h>\n+#include <DataTypes/FieldToDataType.h>\n #include <Functions/FunctionFactory.h>\n #include <IO/WriteHelpers.h>\n-#include <Interpreters/InDepthNodeVisitor.h>\n #include <Interpreters/IdentifierSemantic.h>\n+#include <Interpreters/InDepthNodeVisitor.h>\n+#include <Interpreters/applyFunction.h>\n #include <Parsers/ASTFunction.h>\n #include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTLiteral.h>\n #include <Parsers/ASTOrderByElement.h>\n #include <Parsers/ASTTablesInSelectQuery.h>\n #include <Parsers/IAST.h>\n@@ -33,6 +37,8 @@ class MonotonicityCheckMatcher\n         ASTIdentifier * identifier = nullptr;\n         DataTypePtr arg_data_type = {};\n \n+        Range range = Range::createWholeUniverse();\n+\n         void reject() { monotonicity.is_monotonic = false; }\n         bool isRejected() const { return !monotonicity.is_monotonic; }\n \n@@ -97,13 +103,30 @@ class MonotonicityCheckMatcher\n         if (data.isRejected())\n             return;\n \n-        /// TODO: monotonicity for functions of several arguments\n-        if (!ast_function.arguments || ast_function.arguments->children.size() != 1)\n+        /// Monotonicity check only works for functions that contain at most two arguments and one of them must be a constant.\n+        if (!ast_function.arguments)\n         {\n             data.reject();\n             return;\n         }\n \n+        auto arguments_size = ast_function.arguments->children.size();\n+\n+        if (arguments_size == 0 || arguments_size > 2)\n+        {\n+            data.reject();\n+            return;\n+        }\n+        else if (arguments_size == 2)\n+        {\n+            /// If the function has two arguments, then one of them must be a constant.\n+            if (!ast_function.arguments->children[0]->as<ASTLiteral>() && !ast_function.arguments->children[1]->as<ASTLiteral>())\n+            {\n+                data.reject();\n+                return;\n+            }\n+        }\n+\n         if (!data.canOptimize(ast_function))\n         {\n             data.reject();\n@@ -124,14 +147,33 @@ class MonotonicityCheckMatcher\n             return;\n         }\n \n-        ColumnsWithTypeAndName args;\n-        args.emplace_back(data.arg_data_type, \"tmp\");\n-        auto function_base = function->build(args);\n+        auto function_arguments = getFunctionArguments(ast_function, data);\n+\n+        auto function_base = function->build(function_arguments);\n \n         if (function_base && function_base->hasInformationAboutMonotonicity())\n         {\n             bool is_positive = data.monotonicity.is_positive;\n-            data.monotonicity = function_base->getMonotonicityForRange(*data.arg_data_type, Field(), Field());\n+            data.monotonicity = function_base->getMonotonicityForRange(*data.arg_data_type, data.range.left, data.range.right);\n+\n+            auto & key_range = data.range;\n+\n+            /// If we apply function to open interval, we can get empty intervals in result.\n+            /// E.g. for ('2020-01-03', '2020-01-20') after applying 'toYYYYMM' we will get ('202001', '202001').\n+            /// To avoid this we make range left and right included.\n+            /// Any function that treats NULL specially is not monotonic.\n+            /// Thus we can safely use isNull() as an -Inf/+Inf indicator here.\n+            if (!key_range.left.isNull())\n+            {\n+                key_range.left = applyFunction(function_base, data.arg_data_type, key_range.left);\n+                key_range.left_included = true;\n+            }\n+\n+            if (!key_range.right.isNull())\n+            {\n+                key_range.right = applyFunction(function_base, data.arg_data_type, key_range.right);\n+                key_range.right_included = true;\n+            }\n \n             if (!is_positive)\n                 data.monotonicity.is_positive = !data.monotonicity.is_positive;\n@@ -143,13 +185,53 @@ class MonotonicityCheckMatcher\n \n     static bool needChildVisit(const ASTPtr & parent, const ASTPtr &)\n     {\n-        /// Currently we check monotonicity only for single-argument functions.\n-        /// Although, multi-argument functions with all but one constant arguments can also be monotonic.\n+        /// Multi-argument functions with all but one constant arguments can be monotonic.\n         if (const auto * func = typeid_cast<const ASTFunction *>(parent.get()))\n-            return func->arguments->children.size() < 2;\n+            return func->arguments->children.size() <= 2;\n \n         return true;\n     }\n+\n+    static ColumnWithTypeAndName extractLiteralColumnAndTypeFromAstLiteral(const ASTLiteral * literal)\n+    {\n+        ColumnWithTypeAndName result;\n+\n+        result.type = applyVisitor(FieldToDataType(), literal->value);\n+        result.column = result.type->createColumnConst(0, literal->value);\n+\n+        return result;\n+    }\n+\n+    static ColumnsWithTypeAndName getFunctionArguments(const ASTFunction & ast_function, const Data & data)\n+    {\n+        ColumnsWithTypeAndName args;\n+\n+        auto arguments_size = ast_function.arguments->children.size();\n+\n+        chassert(arguments_size == 1 || arguments_size == 2);\n+\n+        if (arguments_size == 2)\n+        {\n+            if (ast_function.arguments->children[0]->as<ASTLiteral>())\n+            {\n+                const auto * literal = ast_function.arguments->children[0]->as<ASTLiteral>();\n+                args.push_back(extractLiteralColumnAndTypeFromAstLiteral(literal));\n+                args.emplace_back(data.arg_data_type, \"tmp\");\n+            }\n+            else\n+            {\n+                const auto * literal = ast_function.arguments->children[1]->as<ASTLiteral>();\n+                args.emplace_back(data.arg_data_type, \"tmp\");\n+                args.push_back(extractLiteralColumnAndTypeFromAstLiteral(literal));\n+            }\n+        }\n+        else\n+        {\n+            args.emplace_back(data.arg_data_type, \"tmp\");\n+        }\n+\n+        return args;\n+    }\n };\n \n using MonotonicityCheckVisitor = ConstInDepthNodeVisitor<MonotonicityCheckMatcher, false>;\ndiff --git a/src/Interpreters/applyFunction.cpp b/src/Interpreters/applyFunction.cpp\nnew file mode 100644\nindex 000000000000..a53f14f0381f\n--- /dev/null\n+++ b/src/Interpreters/applyFunction.cpp\n@@ -0,0 +1,43 @@\n+#include <Interpreters/applyFunction.h>\n+\n+#include <Core/Range.h>\n+#include <Functions/IFunction.h>\n+\n+namespace DB\n+{\n+\n+static Field applyFunctionForField(const FunctionBasePtr & func, const DataTypePtr & arg_type, const Field & arg_value)\n+{\n+    ColumnsWithTypeAndName columns{\n+        {arg_type->createColumnConst(1, arg_value), arg_type, \"x\"},\n+    };\n+\n+    auto col = func->execute(columns, func->getResultType(), 1);\n+    return (*col)[0];\n+}\n+\n+FieldRef applyFunction(const FunctionBasePtr & func, const DataTypePtr & current_type, const FieldRef & field)\n+{\n+    /// Fallback for fields without block reference.\n+    if (field.isExplicit())\n+        return applyFunctionForField(func, current_type, field);\n+\n+    String result_name = \"_\" + func->getName() + \"_\" + toString(field.column_idx);\n+    const auto & columns = field.columns;\n+    size_t result_idx = columns->size();\n+\n+    for (size_t i = 0; i < result_idx; ++i)\n+        if ((*columns)[i].name == result_name)\n+            result_idx = i;\n+\n+    if (result_idx == columns->size())\n+    {\n+        ColumnsWithTypeAndName args{(*columns)[field.column_idx]};\n+        field.columns->emplace_back(ColumnWithTypeAndName{nullptr, func->getResultType(), result_name});\n+        (*columns)[result_idx].column = func->execute(args, (*columns)[result_idx].type, columns->front().column->size());\n+    }\n+\n+    return {field.columns, field.row_idx, result_idx};\n+}\n+\n+}\ndiff --git a/src/Interpreters/applyFunction.h b/src/Interpreters/applyFunction.h\nnew file mode 100644\nindex 000000000000..9b8ae43a53ca\n--- /dev/null\n+++ b/src/Interpreters/applyFunction.h\n@@ -0,0 +1,16 @@\n+#pragma once\n+\n+#include <memory>\n+\n+namespace DB\n+{\n+struct FieldRef;\n+\n+class IFunctionBase;\n+class IDataType;\n+\n+using DataTypePtr = std::shared_ptr<const IDataType>;\n+using FunctionBasePtr = std::shared_ptr<const IFunctionBase>;\n+\n+FieldRef applyFunction(const FunctionBasePtr & func, const DataTypePtr & current_type, const FieldRef & field);\n+}\ndiff --git a/src/Parsers/queryToString.cpp b/src/Parsers/queryToString.cpp\nindex 9721aa1f1289..4a1903393f6f 100644\n--- a/src/Parsers/queryToString.cpp\n+++ b/src/Parsers/queryToString.cpp\n@@ -3,6 +3,11 @@\n \n namespace DB\n {\n+    String queryToStringNullable(const ASTPtr & query)\n+    {\n+        return query ? queryToString(query) : \"\";\n+    }\n+\n     String queryToString(const ASTPtr & query)\n     {\n         return queryToString(*query);\ndiff --git a/src/Parsers/queryToString.h b/src/Parsers/queryToString.h\nindex 873de218293e..3acd560b1e28 100644\n--- a/src/Parsers/queryToString.h\n+++ b/src/Parsers/queryToString.h\n@@ -6,4 +6,5 @@ namespace DB\n {\n     String queryToString(const ASTPtr & query);\n     String queryToString(const IAST & query);\n+    String queryToStringNullable(const ASTPtr & query);\n }\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 87f23b0da2ad..f3057a8254f8 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -81,6 +81,7 @@ void IMergeTreeDataPart::MinMaxIndex::load(const MergeTreeData & data, const Par\n     auto minmax_column_types = data.getMinMaxColumnsTypes(partition_key);\n     size_t minmax_idx_size = minmax_column_types.size();\n \n+    hyperrectangle.clear();\n     hyperrectangle.reserve(minmax_idx_size);\n     for (size_t i = 0; i < minmax_idx_size; ++i)\n     {\n@@ -104,6 +105,39 @@ void IMergeTreeDataPart::MinMaxIndex::load(const MergeTreeData & data, const Par\n     initialized = true;\n }\n \n+Block IMergeTreeDataPart::MinMaxIndex::getBlock(const MergeTreeData & data) const\n+{\n+    if (!initialized)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Attempt to get block from uninitialized MinMax index.\");\n+\n+    Block block;\n+\n+    const auto metadata_snapshot = data.getInMemoryMetadataPtr();\n+    const auto & partition_key = metadata_snapshot->getPartitionKey();\n+\n+    const auto minmax_column_names = data.getMinMaxColumnsNames(partition_key);\n+    const auto minmax_column_types = data.getMinMaxColumnsTypes(partition_key);\n+    const auto minmax_idx_size = minmax_column_types.size();\n+\n+    for (size_t i = 0; i < minmax_idx_size; ++i)\n+    {\n+        const auto & data_type = minmax_column_types[i];\n+        const auto & column_name = minmax_column_names[i];\n+\n+        const auto column = data_type->createColumn();\n+\n+        const auto min_val = hyperrectangle.at(i).left;\n+        const auto max_val = hyperrectangle.at(i).right;\n+\n+        column->insert(min_val);\n+        column->insert(max_val);\n+\n+        block.insert(ColumnWithTypeAndName(column->getPtr(), data_type, column_name));\n+    }\n+\n+    return block;\n+}\n+\n IMergeTreeDataPart::MinMaxIndex::WrittenFiles IMergeTreeDataPart::MinMaxIndex::store(\n     const MergeTreeData & data, IDataPartStorage & part_storage, Checksums & out_checksums) const\n {\n@@ -185,8 +219,7 @@ void IMergeTreeDataPart::MinMaxIndex::merge(const MinMaxIndex & other)\n \n     if (!initialized)\n     {\n-        hyperrectangle = other.hyperrectangle;\n-        initialized = true;\n+        *this = other;\n     }\n     else\n     {\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex 640a1f1d0a34..29f0f54d4192 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -336,6 +336,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n         }\n \n         void load(const MergeTreeData & data, const PartMetadataManagerPtr & manager);\n+        Block getBlock(const MergeTreeData & data) const;\n \n         using WrittenFiles = std::vector<std::unique_ptr<WriteBufferFromFileBase>>;\n \ndiff --git a/src/Storages/MergeTree/KeyCondition.cpp b/src/Storages/MergeTree/KeyCondition.cpp\nindex d5922ae1bc2d..e5bcb11091f5 100644\n--- a/src/Storages/MergeTree/KeyCondition.cpp\n+++ b/src/Storages/MergeTree/KeyCondition.cpp\n@@ -1,36 +1,37 @@\n-#include <Storages/MergeTree/KeyCondition.h>\n-#include <Storages/MergeTree/BoolMask.h>\n-#include <DataTypes/DataTypesNumber.h>\n+#include <Columns/ColumnConst.h>\n+#include <Columns/ColumnSet.h>\n #include <DataTypes/DataTypeLowCardinality.h>\n #include <DataTypes/DataTypeNullable.h>\n #include <DataTypes/DataTypeNothing.h>\n #include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n #include <DataTypes/FieldToDataType.h>\n-#include <DataTypes/getLeastSupertype.h>\n #include <DataTypes/Utils.h>\n-#include <Interpreters/TreeRewriter.h>\n-#include <Interpreters/ExpressionAnalyzer.h>\n-#include <Interpreters/ExpressionActions.h>\n-#include <Interpreters/castColumn.h>\n-#include <Interpreters/misc.h>\n-#include <Functions/FunctionFactory.h>\n-#include <Functions/indexHint.h>\n+#include <DataTypes/getLeastSupertype.h>\n #include <Functions/CastOverloadResolver.h>\n+#include <Functions/FunctionFactory.h>\n #include <Functions/IFunction.h>\n-#include <Common/FieldVisitorToString.h>\n-#include <Common/MortonUtils.h>\n-#include <Common/typeid_cast.h>\n-#include <Columns/ColumnSet.h>\n-#include <Columns/ColumnConst.h>\n-#include <Interpreters/convertFieldToType.h>\n+#include <Functions/indexHint.h>\n+#include <IO/Operators.h>\n+#include <IO/WriteBufferFromString.h>\n+#include <Interpreters/ExpressionActions.h>\n+#include <Interpreters/ExpressionAnalyzer.h>\n #include <Interpreters/Set.h>\n-#include <Parsers/queryToString.h>\n+#include <Interpreters/TreeRewriter.h>\n+#include <Interpreters/applyFunction.h>\n+#include <Interpreters/castColumn.h>\n+#include <Interpreters/convertFieldToType.h>\n+#include <Interpreters/misc.h>\n #include <Parsers/ASTIdentifier.h>\n #include <Parsers/ASTLiteral.h>\n #include <Parsers/ASTSelectQuery.h>\n-#include <IO/WriteBufferFromString.h>\n-#include <IO/Operators.h>\n+#include <Parsers/queryToString.h>\n+#include <Storages/MergeTree/BoolMask.h>\n+#include <Storages/MergeTree/KeyCondition.h>\n #include <Storages/MergeTree/MergeTreeIndexUtils.h>\n+#include <Common/FieldVisitorToString.h>\n+#include <Common/MortonUtils.h>\n+#include <Common/typeid_cast.h>\n \n #include <algorithm>\n #include <cassert>\n@@ -836,21 +837,6 @@ bool KeyCondition::getConstant(const ASTPtr & expr, Block & block_with_constants\n     return node.tryGetConstant(out_value, out_type);\n }\n \n-\n-static Field applyFunctionForField(\n-    const FunctionBasePtr & func,\n-    const DataTypePtr & arg_type,\n-    const Field & arg_value)\n-{\n-    ColumnsWithTypeAndName columns\n-    {\n-        { arg_type->createColumnConst(1, arg_value), arg_type, \"x\" },\n-    };\n-\n-    auto col = func->execute(columns, func->getResultType(), 1);\n-    return (*col)[0];\n-}\n-\n /// The case when arguments may have types different than in the primary key.\n static std::pair<Field, DataTypePtr> applyFunctionForFieldOfUnknownType(\n     const FunctionBasePtr & func,\n@@ -890,33 +876,6 @@ static std::pair<Field, DataTypePtr> applyBinaryFunctionForFieldOfUnknownType(\n     return {std::move(result), std::move(return_type)};\n }\n \n-\n-static FieldRef applyFunction(const FunctionBasePtr & func, const DataTypePtr & current_type, const FieldRef & field)\n-{\n-    /// Fallback for fields without block reference.\n-    if (field.isExplicit())\n-        return applyFunctionForField(func, current_type, field);\n-\n-    String result_name = \"_\" + func->getName() + \"_\" + toString(field.column_idx);\n-    const auto & columns = field.columns;\n-    size_t result_idx = columns->size();\n-\n-    for (size_t i = 0; i < result_idx; ++i)\n-    {\n-        if ((*columns)[i].name == result_name)\n-            result_idx = i;\n-    }\n-\n-    if (result_idx == columns->size())\n-    {\n-        ColumnsWithTypeAndName args{(*columns)[field.column_idx]};\n-        field.columns->emplace_back(ColumnWithTypeAndName {nullptr, func->getResultType(), result_name});\n-        (*columns)[result_idx].column = func->execute(args, (*columns)[result_idx].type, columns->front().column->size());\n-    }\n-\n-    return {field.columns, field.row_idx, result_idx};\n-}\n-\n /** When table's key has expression with these functions from a column,\n   * and when a column in a query is compared with a constant, such as:\n   * CREATE TABLE (x String) ORDER BY toDate(x)\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 61332a4ff384..c3e348a549a5 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -8,21 +8,6 @@\n #include <Backups/BackupEntryWrappedWith.h>\n #include <Backups/IBackup.h>\n #include <Backups/RestorerFromBackup.h>\n-#include <Common/Config/ConfigHelper.h>\n-#include <Common/CurrentMetrics.h>\n-#include <Common/Increment.h>\n-#include <Common/ProfileEventsScope.h>\n-#include <Common/SimpleIncrement.h>\n-#include <Common/Stopwatch.h>\n-#include <Common/StringUtils/StringUtils.h>\n-#include <Common/ThreadFuzzer.h>\n-#include <Common/escapeForFileName.h>\n-#include <Common/getNumberOfPhysicalCPUCores.h>\n-#include <Common/noexcept_scope.h>\n-#include <Common/quoteString.h>\n-#include <Common/scope_guard_safe.h>\n-#include <Common/typeid_cast.h>\n-#include <Storages/MergeTree/RangesInDataPart.h>\n #include <Compression/CompressedReadBuffer.h>\n #include <Core/QueryProcessingStage.h>\n #include <DataTypes/DataTypeEnum.h>\n@@ -43,19 +28,20 @@\n #include <IO/WriteHelpers.h>\n #include <Interpreters/Aggregator.h>\n #include <Interpreters/Context.h>\n-#include <Interpreters/convertFieldToType.h>\n-#include <Interpreters/evaluateConstantExpression.h>\n #include <Interpreters/ExpressionAnalyzer.h>\n #include <Interpreters/InterpreterSelectQuery.h>\n #include <Interpreters/MergeTreeTransaction.h>\n #include <Interpreters/PartLog.h>\n #include <Interpreters/TransactionLog.h>\n #include <Interpreters/TreeRewriter.h>\n+#include <Interpreters/convertFieldToType.h>\n+#include <Interpreters/evaluateConstantExpression.h>\n #include <Interpreters/inplaceBlockConversions.h>\n+#include <Parsers/ASTAlterQuery.h>\n #include <Parsers/ASTExpressionList.h>\n-#include <Parsers/ASTIndexDeclaration.h>\n-#include <Parsers/ASTHelpers.h>\n #include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTHelpers.h>\n+#include <Parsers/ASTIndexDeclaration.h>\n #include <Parsers/ASTLiteral.h>\n #include <Parsers/ASTNameTypePair.h>\n #include <Parsers/ASTPartition.h>\n@@ -64,26 +50,41 @@\n #include <Parsers/ExpressionListParsers.h>\n #include <Parsers/parseQuery.h>\n #include <Parsers/queryToString.h>\n-#include <Parsers/ASTAlterQuery.h>\n #include <Processors/Formats/IInputFormat.h>\n #include <Processors/QueryPlan/QueryIdHolder.h>\n #include <Processors/QueryPlan/ReadFromMergeTree.h>\n #include <Storages/AlterCommands.h>\n #include <Storages/BlockNumberColumn.h>\n #include <Storages/Freeze.h>\n+#include <Storages/MergeTree/ActiveDataPartSet.h>\n #include <Storages/MergeTree/DataPartStorageOnDiskFull.h>\n #include <Storages/MergeTree/MergeTreeDataPartBuilder.h>\n+#include <Storages/MergeTree/MergeTreeDataPartCloner.h>\n #include <Storages/MergeTree/MergeTreeDataPartCompact.h>\n #include <Storages/MergeTree/MergeTreeDataPartInMemory.h>\n #include <Storages/MergeTree/MergeTreeDataPartWide.h>\n #include <Storages/Statistics/Estimator.h>\n #include <Storages/MergeTree/MergeTreeSelectProcessor.h>\n+#include <Storages/MergeTree/RangesInDataPart.h>\n #include <Storages/MergeTree/checkDataPart.h>\n #include <Storages/MutationCommands.h>\n-#include <Storages/MergeTree/ActiveDataPartSet.h>\n #include <Storages/StorageMergeTree.h>\n #include <Storages/StorageReplicatedMergeTree.h>\n #include <Storages/VirtualColumnUtils.h>\n+#include <Common/Config/ConfigHelper.h>\n+#include <Common/CurrentMetrics.h>\n+#include <Common/Increment.h>\n+#include <Common/ProfileEventsScope.h>\n+#include <Common/SimpleIncrement.h>\n+#include <Common/Stopwatch.h>\n+#include <Common/StringUtils/StringUtils.h>\n+#include <Common/ThreadFuzzer.h>\n+#include <Common/escapeForFileName.h>\n+#include <Common/getNumberOfPhysicalCPUCores.h>\n+#include <Common/noexcept_scope.h>\n+#include <Common/quoteString.h>\n+#include <Common/scope_guard_safe.h>\n+#include <Common/typeid_cast.h>\n \n #include <boost/range/algorithm_ext/erase.hpp>\n #include <boost/algorithm/string/join.hpp>\n@@ -197,6 +198,50 @@ namespace ErrorCodes\n     extern const int LIMIT_EXCEEDED;\n }\n \n+static size_t getPartitionAstFieldsCount(const ASTPartition & partition_ast, ASTPtr partition_value_ast)\n+{\n+    if (partition_ast.fields_count.has_value())\n+        return *partition_ast.fields_count;\n+\n+    if (partition_value_ast->as<ASTLiteral>())\n+        return 1;\n+\n+    const auto * tuple_ast = partition_value_ast->as<ASTFunction>();\n+\n+    if (!tuple_ast)\n+    {\n+        throw Exception(\n+            ErrorCodes::INVALID_PARTITION_VALUE, \"Expected literal or tuple for partition key, got {}\", partition_value_ast->getID());\n+    }\n+\n+    if (tuple_ast->name != \"tuple\")\n+    {\n+        if (!isFunctionCast(tuple_ast))\n+            throw Exception(ErrorCodes::INVALID_PARTITION_VALUE, \"Expected tuple for complex partition key, got {}\", tuple_ast->name);\n+\n+        if (tuple_ast->arguments->as<ASTExpressionList>()->children.empty())\n+            throw Exception(ErrorCodes::INVALID_PARTITION_VALUE, \"Expected tuple for complex partition key, got {}\", tuple_ast->name);\n+\n+        auto first_arg = tuple_ast->arguments->as<ASTExpressionList>()->children.at(0);\n+        if (const auto * inner_tuple = first_arg->as<ASTFunction>(); inner_tuple && inner_tuple->name == \"tuple\")\n+        {\n+            const auto * arguments_ast = tuple_ast->arguments->as<ASTExpressionList>();\n+            return arguments_ast ? arguments_ast->children.size() : 0;\n+        }\n+        else if (const auto * inner_literal_tuple = first_arg->as<ASTLiteral>(); inner_literal_tuple)\n+        {\n+            return inner_literal_tuple->value.getType() == Field::Types::Tuple ? inner_literal_tuple->value.safeGet<Tuple>().size() : 1;\n+        }\n+\n+        throw Exception(ErrorCodes::INVALID_PARTITION_VALUE, \"Expected tuple for complex partition key, got {}\", tuple_ast->name);\n+    }\n+    else\n+    {\n+        const auto * arguments_ast = tuple_ast->arguments->as<ASTExpressionList>();\n+        return arguments_ast ? arguments_ast->children.size() : 0;\n+    }\n+}\n+\n static void checkSuspiciousIndices(const ASTFunction * index_function)\n {\n     std::unordered_set<UInt64> unique_index_expression_hashes;\n@@ -4854,7 +4899,7 @@ void MergeTreeData::removePartContributionToColumnAndSecondaryIndexSizes(const D\n }\n \n void MergeTreeData::checkAlterPartitionIsPossible(\n-    const PartitionCommands & commands, const StorageMetadataPtr & /*metadata_snapshot*/, const Settings & settings, ContextPtr local_context) const\n+    const PartitionCommands & commands, const StorageMetadataPtr & /*metadata_snapshot*/, const Settings & settings, ContextPtr) const\n {\n     for (const auto & command : commands)\n     {\n@@ -4882,7 +4927,15 @@ void MergeTreeData::checkAlterPartitionIsPossible(\n                         throw DB::Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"Only support DROP/DETACH PARTITION ALL currently\");\n                 }\n                 else\n-                    getPartitionIDFromQuery(command.partition, local_context);\n+                {\n+                    // The below `getPartitionIDFromQuery` call will not work for attach / replace because it assumes the partition expressions\n+                    // are the same and deliberately uses this storage. Later on, `MergeTreeData::replaceFrom` is called, and it makes the right\n+                    // call to `getPartitionIDFromQuery` using source storage.\n+                    // Note: `PartitionCommand::REPLACE_PARTITION` is used both for `REPLACE PARTITION` and `ATTACH PARTITION FROM` queries.\n+                    // But not for `ATTACH PARTITION` queries.\n+                    if (command.type != PartitionCommand::REPLACE_PARTITION)\n+                        getPartitionIDFromQuery(command.partition, getContext());\n+                }\n             }\n         }\n     }\n@@ -5616,69 +5669,8 @@ String MergeTreeData::getPartitionIDFromQuery(const ASTPtr & ast, ContextPtr loc\n         MergeTreePartInfo::validatePartitionID(partition_ast.id->clone(), format_version);\n         return partition_ast.id->as<ASTLiteral>()->value.safeGet<String>();\n     }\n-    size_t partition_ast_fields_count = 0;\n     ASTPtr partition_value_ast = partition_ast.value->clone();\n-    if (!partition_ast.fields_count.has_value())\n-    {\n-        if (partition_value_ast->as<ASTLiteral>())\n-        {\n-            partition_ast_fields_count = 1;\n-        }\n-        else if (const auto * tuple_ast = partition_value_ast->as<ASTFunction>())\n-        {\n-            if (tuple_ast->name != \"tuple\")\n-            {\n-                if (isFunctionCast(tuple_ast))\n-                {\n-                    if (tuple_ast->arguments->as<ASTExpressionList>()->children.empty())\n-                    {\n-                        throw Exception(\n-                            ErrorCodes::INVALID_PARTITION_VALUE, \"Expected tuple for complex partition key, got {}\", tuple_ast->name);\n-                    }\n-                    auto first_arg = tuple_ast->arguments->as<ASTExpressionList>()->children.at(0);\n-                    if (const auto * inner_tuple = first_arg->as<ASTFunction>(); inner_tuple && inner_tuple->name == \"tuple\")\n-                    {\n-                        const auto * arguments_ast = tuple_ast->arguments->as<ASTExpressionList>();\n-                        if (arguments_ast)\n-                            partition_ast_fields_count = arguments_ast->children.size();\n-                        else\n-                            partition_ast_fields_count = 0;\n-                    }\n-                    else if (const auto * inner_literal_tuple = first_arg->as<ASTLiteral>(); inner_literal_tuple)\n-                    {\n-                        if (inner_literal_tuple->value.getType() == Field::Types::Tuple)\n-                            partition_ast_fields_count = inner_literal_tuple->value.safeGet<Tuple>().size();\n-                        else\n-                            partition_ast_fields_count = 1;\n-                    }\n-                    else\n-                    {\n-                        throw Exception(\n-                            ErrorCodes::INVALID_PARTITION_VALUE, \"Expected tuple for complex partition key, got {}\", tuple_ast->name);\n-                    }\n-                }\n-                else\n-                    throw Exception(ErrorCodes::INVALID_PARTITION_VALUE, \"Expected tuple for complex partition key, got {}\", tuple_ast->name);\n-            }\n-            else\n-            {\n-                const auto * arguments_ast = tuple_ast->arguments->as<ASTExpressionList>();\n-                if (arguments_ast)\n-                    partition_ast_fields_count = arguments_ast->children.size();\n-                else\n-                    partition_ast_fields_count = 0;\n-            }\n-        }\n-        else\n-        {\n-            throw Exception(\n-                ErrorCodes::INVALID_PARTITION_VALUE, \"Expected literal or tuple for partition key, got {}\", partition_value_ast->getID());\n-        }\n-    }\n-    else\n-    {\n-        partition_ast_fields_count = *partition_ast.fields_count;\n-    }\n+    auto partition_ast_fields_count = getPartitionAstFieldsCount(partition_ast, partition_value_ast);\n \n     if (format_version < MERGE_TREE_DATA_MIN_FORMAT_VERSION_WITH_CUSTOM_PARTITIONING)\n     {\n@@ -7014,23 +7006,35 @@ MergeTreeData & MergeTreeData::checkStructureAndGetMergeTreeData(IStorage & sour\n     if (my_snapshot->getColumns().getAllPhysical().sizeOfDifference(src_snapshot->getColumns().getAllPhysical()))\n         throw Exception(ErrorCodes::INCOMPATIBLE_COLUMNS, \"Tables have different structure\");\n \n-    auto query_to_string = [] (const ASTPtr & ast)\n-    {\n-        return ast ? queryToString(ast) : \"\";\n-    };\n-\n-    if (query_to_string(my_snapshot->getSortingKeyAST()) != query_to_string(src_snapshot->getSortingKeyAST()))\n+    if (queryToStringNullable(my_snapshot->getSortingKeyAST()) != queryToStringNullable(src_snapshot->getSortingKeyAST()))\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Tables have different ordering\");\n \n-    if (query_to_string(my_snapshot->getPartitionKeyAST()) != query_to_string(src_snapshot->getPartitionKeyAST()))\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Tables have different partition key\");\n-\n     if (format_version != src_data->format_version)\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Tables have different format_version\");\n \n-    if (query_to_string(my_snapshot->getPrimaryKeyAST()) != query_to_string(src_snapshot->getPrimaryKeyAST()))\n+    if (queryToStringNullable(my_snapshot->getPrimaryKeyAST()) != queryToStringNullable(src_snapshot->getPrimaryKeyAST()))\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Tables have different primary key\");\n \n+    const auto is_a_subset_of = [](const auto & lhs, const auto & rhs)\n+    {\n+        if (lhs.size() > rhs.size())\n+            return false;\n+\n+        const auto rhs_set = NameSet(rhs.begin(), rhs.end());\n+        for (const auto & lhs_element : lhs)\n+            if (!rhs_set.contains(lhs_element))\n+                return false;\n+\n+        return true;\n+    };\n+\n+    if (!is_a_subset_of(my_snapshot->getColumnsRequiredForPartitionKey(), src_snapshot->getColumnsRequiredForPartitionKey()))\n+    {\n+        throw Exception(\n+            ErrorCodes::BAD_ARGUMENTS,\n+            \"Destination table partition expression columns must be a subset of source table partition expression columns\");\n+    }\n+\n     const auto check_definitions = [](const auto & my_descriptions, const auto & src_descriptions)\n     {\n         if (my_descriptions.size() != src_descriptions.size())\n@@ -7071,128 +7075,56 @@ std::pair<MergeTreeData::MutableDataPartPtr, scope_guard> MergeTreeData::cloneAn\n     const ReadSettings & read_settings,\n     const WriteSettings & write_settings)\n {\n-    /// Check that the storage policy contains the disk where the src_part is located.\n-    bool does_storage_policy_allow_same_disk = false;\n-    for (const DiskPtr & disk : getStoragePolicy()->getDisks())\n-    {\n-        if (disk->getName() == src_part->getDataPartStorage().getDiskName())\n-        {\n-            does_storage_policy_allow_same_disk = true;\n-            break;\n-        }\n-    }\n-    if (!does_storage_policy_allow_same_disk)\n-        throw Exception(\n-            ErrorCodes::BAD_ARGUMENTS,\n-            \"Could not clone and load part {} because disk does not belong to storage policy\",\n-            quoteString(src_part->getDataPartStorage().getFullPath()));\n-\n-    String dst_part_name = src_part->getNewName(dst_part_info);\n-    String tmp_dst_part_name = tmp_part_prefix + dst_part_name;\n-    auto temporary_directory_lock = getTemporaryPartDirectoryHolder(tmp_dst_part_name);\n-\n-    /// Why it is needed if we only hardlink files?\n-    auto reservation = src_part->getDataPartStorage().reserve(src_part->getBytesOnDisk());\n-    auto src_part_storage = src_part->getDataPartStoragePtr();\n-\n-    scope_guard src_flushed_tmp_dir_lock;\n-    MergeTreeData::MutableDataPartPtr src_flushed_tmp_part;\n-\n-    /// If source part is in memory, flush it to disk and clone it already in on-disk format\n-    /// Protect tmp dir from removing by cleanup thread with src_flushed_tmp_dir_lock\n-    /// Construct src_flushed_tmp_part in order to delete part with its directory at destructor\n-    if (auto src_part_in_memory = asInMemoryPart(src_part))\n-    {\n-        auto flushed_part_path = *src_part_in_memory->getRelativePathForPrefix(tmp_part_prefix);\n-\n-        auto tmp_src_part_file_name = fs::path(tmp_dst_part_name).filename();\n-        src_flushed_tmp_dir_lock = src_part->storage.getTemporaryPartDirectoryHolder(tmp_src_part_file_name);\n-\n-        auto flushed_part_storage = src_part_in_memory->flushToDisk(flushed_part_path, metadata_snapshot);\n-\n-        src_flushed_tmp_part = MergeTreeDataPartBuilder(*this, src_part->name, flushed_part_storage)\n-            .withPartInfo(src_part->info)\n-            .withPartFormatFromDisk()\n-            .build();\n-\n-        src_flushed_tmp_part->is_temp = true;\n-        src_part_storage = flushed_part_storage;\n-    }\n-\n-    String with_copy;\n-    if (params.copy_instead_of_hardlink)\n-        with_copy = \" (copying data)\";\n-\n-    auto dst_part_storage = src_part_storage->freeze(\n-        relative_data_path,\n-        tmp_dst_part_name,\n-        read_settings,\n-        write_settings,\n-        /* save_metadata_callback= */ {},\n-        params);\n-\n-    if (params.metadata_version_to_write.has_value())\n-    {\n-        chassert(!params.keep_metadata_version);\n-        auto out_metadata = dst_part_storage->writeFile(IMergeTreeDataPart::METADATA_VERSION_FILE_NAME, 4096, getContext()->getWriteSettings());\n-        writeText(metadata_snapshot->getMetadataVersion(), *out_metadata);\n-        out_metadata->finalize();\n-        if (getSettings()->fsync_after_insert)\n-            out_metadata->sync();\n-    }\n-\n-    LOG_DEBUG(log, \"Clone{} part {} to {}{}\",\n-              src_flushed_tmp_part ? \" flushed\" : \"\",\n-              src_part_storage->getFullPath(),\n-              std::string(fs::path(dst_part_storage->getFullRootPath()) / tmp_dst_part_name),\n-              with_copy);\n-\n-    auto dst_data_part = MergeTreeDataPartBuilder(*this, dst_part_name, dst_part_storage)\n-        .withPartFormatFromDisk()\n-        .build();\n+    return MergeTreeDataPartCloner::clone(\n+        this, src_part, metadata_snapshot, dst_part_info, tmp_part_prefix, require_part_metadata, params, read_settings, write_settings);\n+}\n \n-    if (!params.copy_instead_of_hardlink && params.hardlinked_files)\n-    {\n-        params.hardlinked_files->source_part_name = src_part->name;\n-        params.hardlinked_files->source_table_shared_id = src_part->storage.getTableSharedID();\n+std::pair<MergeTreeData::MutableDataPartPtr, scope_guard> MergeTreeData::cloneAndLoadPartOnSameDiskWithDifferentPartitionKey(\n+    const MergeTreeData::DataPartPtr & src_part,\n+    const MergeTreePartition & new_partition,\n+    const String & partition_id,\n+    const IMergeTreeDataPart::MinMaxIndex & min_max_index,\n+    const String & tmp_part_prefix,\n+    const StorageMetadataPtr & my_metadata_snapshot,\n+    const IDataPartStorage::ClonePartParams & clone_params,\n+    ContextPtr local_context,\n+    Int64 min_block,\n+    Int64 max_block\n+)\n+{\n+    MergeTreePartInfo dst_part_info(partition_id, min_block, max_block, src_part->info.level);\n+\n+    return MergeTreeDataPartCloner::cloneWithDistinctPartitionExpression(\n+        this,\n+        src_part,\n+        my_metadata_snapshot,\n+        dst_part_info,\n+        tmp_part_prefix,\n+        local_context->getReadSettings(),\n+        local_context->getWriteSettings(),\n+        new_partition,\n+        min_max_index,\n+        false,\n+        clone_params);\n+}\n+\n+std::pair<MergeTreePartition, IMergeTreeDataPart::MinMaxIndex> MergeTreeData::createPartitionAndMinMaxIndexFromSourcePart(\n+    const MergeTreeData::DataPartPtr & src_part,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    ContextPtr local_context)\n+{\n+    const auto & src_data = src_part->storage;\n \n-        for (auto it = src_part->getDataPartStorage().iterate(); it->isValid(); it->next())\n-        {\n-            if (!params.files_to_copy_instead_of_hardlinks.contains(it->name())\n-                && it->name() != IMergeTreeDataPart::DELETE_ON_DESTROY_MARKER_FILE_NAME_DEPRECATED\n-                && it->name() != IMergeTreeDataPart::TXN_VERSION_METADATA_FILE_NAME)\n-            {\n-                params.hardlinked_files->hardlinks_from_source_part.insert(it->name());\n-            }\n-        }\n+    auto metadata_manager = std::make_shared<PartMetadataManagerOrdinary>(src_part.get());\n+    IMergeTreeDataPart::MinMaxIndex min_max_index;\n \n-        auto projections = src_part->getProjectionParts();\n-        for (const auto & [name, projection_part] : projections)\n-        {\n-            const auto & projection_storage = projection_part->getDataPartStorage();\n-            for (auto it = projection_storage.iterate(); it->isValid(); it->next())\n-            {\n-                auto file_name_with_projection_prefix = fs::path(projection_storage.getPartDirectory()) / it->name();\n-                if (!params.files_to_copy_instead_of_hardlinks.contains(file_name_with_projection_prefix)\n-                    && it->name() != IMergeTreeDataPart::DELETE_ON_DESTROY_MARKER_FILE_NAME_DEPRECATED\n-                    && it->name() != IMergeTreeDataPart::TXN_VERSION_METADATA_FILE_NAME)\n-                {\n-                    params.hardlinked_files->hardlinks_from_source_part.insert(file_name_with_projection_prefix);\n-                }\n-            }\n-        }\n-    }\n+    min_max_index.load(src_data, metadata_manager);\n \n-    /// We should write version metadata on part creation to distinguish it from parts that were created without transaction.\n-    TransactionID tid = params.txn ? params.txn->tid : Tx::PrehistoricTID;\n-    dst_data_part->version.setCreationTID(tid, nullptr);\n-    dst_data_part->storeVersionMetadata();\n+    MergeTreePartition new_partition;\n \n-    dst_data_part->is_temp = true;\n+    new_partition.create(metadata_snapshot, min_max_index.getBlock(src_data), 0u, local_context);\n \n-    dst_data_part->loadColumnsChecksumsIndexes(require_part_metadata, true);\n-    dst_data_part->modification_time = dst_part_storage->getLastModified().epochTime();\n-    return std::make_pair(dst_data_part, std::move(temporary_directory_lock));\n+    return {new_partition, min_max_index};\n }\n \n String MergeTreeData::getFullPathOnDisk(const DiskPtr & disk) const\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex f0dbaf0e307a..9c433e11b848 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -231,6 +231,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n         }\n     };\n \n+\n     using DataParts = std::set<DataPartPtr, LessDataPart>;\n     using MutableDataParts = std::set<MutableDataPartPtr, LessDataPart>;\n     using DataPartsVector = std::vector<DataPartPtr>;\n@@ -848,6 +849,23 @@ class MergeTreeData : public IStorage, public WithMutableContext\n         const ReadSettings & read_settings,\n         const WriteSettings & write_settings);\n \n+    std::pair<MergeTreeData::MutableDataPartPtr, scope_guard> cloneAndLoadPartOnSameDiskWithDifferentPartitionKey(\n+        const MergeTreeData::DataPartPtr & src_part,\n+        const MergeTreePartition & new_partition,\n+        const String & partition_id,\n+        const IMergeTreeDataPart::MinMaxIndex & min_max_index,\n+        const String & tmp_part_prefix,\n+        const StorageMetadataPtr & my_metadata_snapshot,\n+        const IDataPartStorage::ClonePartParams & clone_params,\n+        ContextPtr local_context,\n+        Int64 min_block,\n+        Int64 max_block);\n+\n+    static std::pair<MergeTreePartition, IMergeTreeDataPart::MinMaxIndex> createPartitionAndMinMaxIndexFromSourcePart(\n+        const MergeTreeData::DataPartPtr & src_part,\n+        const StorageMetadataPtr & metadata_snapshot,\n+        ContextPtr local_context);\n+\n     virtual std::vector<MergeTreeMutationStatus> getMutationsStatus() const = 0;\n \n     /// Returns true if table can create new parts with adaptive granularity\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCloner.cpp b/src/Storages/MergeTree/MergeTreeDataPartCloner.cpp\nnew file mode 100644\nindex 000000000000..78cb9aa0624a\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCloner.cpp\n@@ -0,0 +1,320 @@\n+#include <Interpreters/MergeTreeTransaction.h>\n+#include <Storages/MergeTree/MergeTreeData.h>\n+#include <Storages/MergeTree/MergeTreeDataPartBuilder.h>\n+#include <Storages/MergeTree/MergeTreeDataPartCloner.h>\n+#include <Common/escapeForFileName.h>\n+#include <Common/logger_useful.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+extern const int BAD_ARGUMENTS;\n+}\n+\n+static Poco::Logger * log = &Poco::Logger::get(\"MergeTreeDataPartCloner\");\n+\n+namespace DistinctPartitionExpression\n+{\n+std::unique_ptr<WriteBufferFromFileBase> updatePartitionFile(\n+    const MergeTreeData & merge_tree_data,\n+    const MergeTreePartition & partition,\n+    const MergeTreeData::MutableDataPartPtr & dst_part,\n+    IDataPartStorage & storage)\n+{\n+    storage.removeFile(\"partition.dat\");\n+    // Leverage already implemented MergeTreePartition::store to create & store partition.dat.\n+    // Checksum is re-calculated later.\n+    return partition.store(merge_tree_data, storage, dst_part->checksums);\n+}\n+\n+IMergeTreeDataPart::MinMaxIndex::WrittenFiles updateMinMaxFiles(\n+    const MergeTreeData & merge_tree_data,\n+    const MergeTreeData::MutableDataPartPtr & dst_part,\n+    IDataPartStorage & storage,\n+    const StorageMetadataPtr & metadata_snapshot)\n+{\n+    for (const auto & column_name : MergeTreeData::getMinMaxColumnsNames(metadata_snapshot->partition_key))\n+    {\n+        auto file = \"minmax_\" + escapeForFileName(column_name) + \".idx\";\n+        storage.removeFile(file);\n+    }\n+\n+    return dst_part->minmax_idx->store(merge_tree_data, storage, dst_part->checksums);\n+}\n+\n+void finalizeNewFiles(const std::vector<std::unique_ptr<WriteBufferFromFileBase>> & files, bool sync_new_files)\n+{\n+    for (const auto & file : files)\n+    {\n+        file->finalize();\n+        if (sync_new_files)\n+            file->sync();\n+    }\n+}\n+\n+void updateNewPartFiles(\n+    const MergeTreeData & merge_tree_data,\n+    const MergeTreeData::MutableDataPartPtr & dst_part,\n+    const MergeTreePartition & new_partition,\n+    const IMergeTreeDataPart::MinMaxIndex & new_min_max_index,\n+    const StorageMetadataPtr & src_metadata_snapshot,\n+    bool sync_new_files)\n+{\n+    auto & storage = dst_part->getDataPartStorage();\n+\n+    *dst_part->minmax_idx = new_min_max_index;\n+\n+    auto partition_file = updatePartitionFile(merge_tree_data, new_partition, dst_part, storage);\n+\n+    auto min_max_files = updateMinMaxFiles(merge_tree_data, dst_part, storage, src_metadata_snapshot);\n+\n+    IMergeTreeDataPart::MinMaxIndex::WrittenFiles written_files;\n+\n+    if (partition_file)\n+        written_files.emplace_back(std::move(partition_file));\n+\n+    written_files.insert(written_files.end(), std::make_move_iterator(min_max_files.begin()), std::make_move_iterator(min_max_files.end()));\n+\n+    finalizeNewFiles(written_files, sync_new_files);\n+\n+    // MergeTreeDataPartCloner::finalize_part calls IMergeTreeDataPart::loadColumnsChecksumsIndexes, which will re-create\n+    // the checksum file if it doesn't exist. Relying on that is cumbersome, but this refactoring is simply a code extraction\n+    // with small improvements. It can be further improved in the future.\n+    storage.removeFile(\"checksums.txt\");\n+}\n+}\n+\n+namespace\n+{\n+bool doesStoragePolicyAllowSameDisk(MergeTreeData * merge_tree_data, const MergeTreeData::DataPartPtr & src_part)\n+{\n+    for (const DiskPtr & disk : merge_tree_data->getStoragePolicy()->getDisks())\n+        if (disk->getName() == src_part->getDataPartStorage().getDiskName())\n+            return true;\n+    return false;\n+}\n+\n+DataPartStoragePtr flushPartStorageToDiskIfInMemory(\n+    MergeTreeData * merge_tree_data,\n+    const MergeTreeData::DataPartPtr & src_part,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    const String & tmp_part_prefix,\n+    const String & tmp_dst_part_name,\n+    scope_guard & src_flushed_tmp_dir_lock,\n+    MergeTreeData::MutableDataPartPtr src_flushed_tmp_part)\n+{\n+    if (auto src_part_in_memory = asInMemoryPart(src_part))\n+    {\n+        auto flushed_part_path = src_part_in_memory->getRelativePathForPrefix(tmp_part_prefix);\n+        auto tmp_src_part_file_name = fs::path(tmp_dst_part_name).filename();\n+\n+        src_flushed_tmp_dir_lock = src_part->storage.getTemporaryPartDirectoryHolder(tmp_src_part_file_name);\n+\n+        auto flushed_part_storage = src_part_in_memory->flushToDisk(*flushed_part_path, metadata_snapshot);\n+\n+        src_flushed_tmp_part = MergeTreeDataPartBuilder(*merge_tree_data, src_part->name, flushed_part_storage)\n+                                   .withPartInfo(src_part->info)\n+                                   .withPartFormatFromDisk()\n+                                   .build();\n+\n+        src_flushed_tmp_part->is_temp = true;\n+\n+        return flushed_part_storage;\n+    }\n+\n+    return src_part->getDataPartStoragePtr();\n+}\n+\n+std::shared_ptr<IDataPartStorage> hardlinkAllFiles(\n+    MergeTreeData * merge_tree_data,\n+    const DB::ReadSettings & read_settings,\n+    const DB::WriteSettings & write_settings,\n+    const DataPartStoragePtr & storage,\n+    const String & path,\n+    const DB::IDataPartStorage::ClonePartParams & params)\n+{\n+    return storage->freeze(\n+        merge_tree_data->getRelativeDataPath(),\n+        path,\n+        read_settings,\n+        write_settings,\n+        /*save_metadata_callback=*/{},\n+        params);\n+}\n+\n+std::pair<MergeTreeData::MutableDataPartPtr, scope_guard> cloneSourcePart(\n+    MergeTreeData * merge_tree_data,\n+    const MergeTreeData::DataPartPtr & src_part,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    const MergeTreePartInfo & dst_part_info,\n+    const String & tmp_part_prefix,\n+    const ReadSettings & read_settings,\n+    const WriteSettings & write_settings,\n+    const DB::IDataPartStorage::ClonePartParams & params)\n+{\n+    const auto dst_part_name = src_part->getNewName(dst_part_info);\n+\n+    const auto tmp_dst_part_name = tmp_part_prefix + dst_part_name;\n+\n+    auto temporary_directory_lock = merge_tree_data->getTemporaryPartDirectoryHolder(tmp_dst_part_name);\n+\n+    src_part->getDataPartStorage().reserve(src_part->getBytesOnDisk());\n+\n+    scope_guard src_flushed_tmp_dir_lock;\n+    MergeTreeData::MutableDataPartPtr src_flushed_tmp_part;\n+\n+    auto src_part_storage = flushPartStorageToDiskIfInMemory(\n+        merge_tree_data, src_part, metadata_snapshot, tmp_part_prefix, tmp_dst_part_name, src_flushed_tmp_dir_lock, src_flushed_tmp_part);\n+\n+    auto dst_part_storage = hardlinkAllFiles(merge_tree_data, read_settings, write_settings, src_part_storage, tmp_dst_part_name, params);\n+\n+    if (params.metadata_version_to_write.has_value())\n+    {\n+        chassert(!params.keep_metadata_version);\n+        auto out_metadata = dst_part_storage->writeFile(\n+            IMergeTreeDataPart::METADATA_VERSION_FILE_NAME, 4096, merge_tree_data->getContext()->getWriteSettings());\n+        writeText(metadata_snapshot->getMetadataVersion(), *out_metadata);\n+        out_metadata->finalize();\n+        if (merge_tree_data->getSettings()->fsync_after_insert)\n+            out_metadata->sync();\n+    }\n+\n+    LOG_DEBUG(\n+        log,\n+        \"Clone {} part {} to {}{}\",\n+        src_flushed_tmp_part ? \"flushed\" : \"\",\n+        src_part_storage->getFullPath(),\n+        std::string(fs::path(dst_part_storage->getFullRootPath()) / tmp_dst_part_name),\n+        false);\n+\n+\n+    auto part = MergeTreeDataPartBuilder(*merge_tree_data, dst_part_name, dst_part_storage).withPartFormatFromDisk().build();\n+\n+    return std::make_pair(part, std::move(temporary_directory_lock));\n+}\n+\n+void handleHardLinkedParameterFiles(const MergeTreeData::DataPartPtr & src_part, const DB::IDataPartStorage::ClonePartParams & params)\n+{\n+    const auto & hardlinked_files = params.hardlinked_files;\n+\n+    hardlinked_files->source_part_name = src_part->name;\n+    hardlinked_files->source_table_shared_id = src_part->storage.getTableSharedID();\n+\n+    for (auto it = src_part->getDataPartStorage().iterate(); it->isValid(); it->next())\n+    {\n+        if (!params.files_to_copy_instead_of_hardlinks.contains(it->name())\n+            && it->name() != IMergeTreeDataPart::DELETE_ON_DESTROY_MARKER_FILE_NAME_DEPRECATED\n+            && it->name() != IMergeTreeDataPart::TXN_VERSION_METADATA_FILE_NAME)\n+        {\n+            hardlinked_files->hardlinks_from_source_part.insert(it->name());\n+        }\n+    }\n+}\n+\n+void handleProjections(const MergeTreeData::DataPartPtr & src_part, const DB::IDataPartStorage::ClonePartParams & params)\n+{\n+    auto projections = src_part->getProjectionParts();\n+    for (const auto & [name, projection_part] : projections)\n+    {\n+        const auto & projection_storage = projection_part->getDataPartStorage();\n+        for (auto it = projection_storage.iterate(); it->isValid(); it->next())\n+        {\n+            auto file_name_with_projection_prefix = fs::path(projection_storage.getPartDirectory()) / it->name();\n+            if (!params.files_to_copy_instead_of_hardlinks.contains(file_name_with_projection_prefix)\n+                && it->name() != IMergeTreeDataPart::DELETE_ON_DESTROY_MARKER_FILE_NAME_DEPRECATED\n+                && it->name() != IMergeTreeDataPart::TXN_VERSION_METADATA_FILE_NAME)\n+            {\n+                params.hardlinked_files->hardlinks_from_source_part.insert(file_name_with_projection_prefix);\n+            }\n+        }\n+    }\n+}\n+\n+MergeTreeData::MutableDataPartPtr finalizePart(\n+    const MergeTreeData::MutableDataPartPtr & dst_part, const DB::IDataPartStorage::ClonePartParams & params, bool require_part_metadata)\n+{\n+    /// We should write version metadata on part creation to distinguish it from parts that were created without transaction.\n+    TransactionID tid = params.txn ? params.txn->tid : Tx::PrehistoricTID;\n+    dst_part->version.setCreationTID(tid, nullptr);\n+    dst_part->storeVersionMetadata();\n+\n+    dst_part->is_temp = true;\n+\n+    dst_part->loadColumnsChecksumsIndexes(require_part_metadata, true);\n+\n+    dst_part->modification_time = dst_part->getDataPartStorage().getLastModified().epochTime();\n+\n+    return dst_part;\n+}\n+\n+std::pair<MergeTreeDataPartCloner::MutableDataPartPtr, scope_guard> cloneAndHandleHardlinksAndProjections(\n+    MergeTreeData * merge_tree_data,\n+    const DataPartPtr & src_part,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    const MergeTreePartInfo & dst_part_info,\n+    const String & tmp_part_prefix,\n+    const ReadSettings & read_settings,\n+    const WriteSettings & write_settings,\n+    const IDataPartStorage::ClonePartParams & params)\n+{\n+    if (!doesStoragePolicyAllowSameDisk(merge_tree_data, src_part))\n+        throw Exception(\n+            ErrorCodes::BAD_ARGUMENTS,\n+            \"Could not clone and load part {} because disk does not belong to storage policy\",\n+            quoteString(src_part->getDataPartStorage().getFullPath()));\n+\n+    auto [destination_part, temporary_directory_lock] = cloneSourcePart(\n+        merge_tree_data, src_part, metadata_snapshot, dst_part_info, tmp_part_prefix, read_settings, write_settings, params);\n+\n+    if (!params.copy_instead_of_hardlink && params.hardlinked_files)\n+    {\n+        handleHardLinkedParameterFiles(src_part, params);\n+        handleProjections(src_part, params);\n+    }\n+\n+    return std::make_pair(destination_part, std::move(temporary_directory_lock));\n+}\n+}\n+\n+std::pair<MergeTreeDataPartCloner::MutableDataPartPtr, scope_guard> MergeTreeDataPartCloner::clone(\n+    MergeTreeData * merge_tree_data,\n+    const DataPartPtr & src_part,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    const MergeTreePartInfo & dst_part_info,\n+    const String & tmp_part_prefix,\n+    bool require_part_metadata,\n+    const IDataPartStorage::ClonePartParams & params,\n+    const ReadSettings & read_settings,\n+    const WriteSettings & write_settings)\n+{\n+    auto [destination_part, temporary_directory_lock] = cloneAndHandleHardlinksAndProjections(\n+        merge_tree_data, src_part, metadata_snapshot, dst_part_info, tmp_part_prefix, read_settings, write_settings, params);\n+\n+    return std::make_pair(finalizePart(destination_part, params, require_part_metadata), std::move(temporary_directory_lock));\n+}\n+\n+std::pair<MergeTreeDataPartCloner::MutableDataPartPtr, scope_guard> MergeTreeDataPartCloner::cloneWithDistinctPartitionExpression(\n+    MergeTreeData * merge_tree_data,\n+    const DataPartPtr & src_part,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    const MergeTreePartInfo & dst_part_info,\n+    const String & tmp_part_prefix,\n+    const ReadSettings & read_settings,\n+    const WriteSettings & write_settings,\n+    const MergeTreePartition & new_partition,\n+    const IMergeTreeDataPart::MinMaxIndex & new_min_max_index,\n+    bool sync_new_files,\n+    const IDataPartStorage::ClonePartParams & params)\n+{\n+    auto [destination_part, temporary_directory_lock] = cloneAndHandleHardlinksAndProjections(\n+        merge_tree_data, src_part, metadata_snapshot, dst_part_info, tmp_part_prefix, read_settings, write_settings, params);\n+\n+    DistinctPartitionExpression::updateNewPartFiles(\n+        *merge_tree_data, destination_part, new_partition, new_min_max_index, src_part->storage.getInMemoryMetadataPtr(), sync_new_files);\n+\n+    return std::make_pair(finalizePart(destination_part, params, false), std::move(temporary_directory_lock));\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCloner.h b/src/Storages/MergeTree/MergeTreeDataPartCloner.h\nnew file mode 100644\nindex 000000000000..53585f20b7f0\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCloner.h\n@@ -0,0 +1,43 @@\n+#pragma once\n+\n+namespace DB\n+{\n+\n+struct StorageInMemoryMetadata;\n+using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;\n+struct MergeTreePartition;\n+class IMergeTreeDataPart;\n+\n+class MergeTreeDataPartCloner\n+{\n+public:\n+    using DataPart = IMergeTreeDataPart;\n+    using MutableDataPartPtr = std::shared_ptr<DataPart>;\n+    using DataPartPtr = std::shared_ptr<const DataPart>;\n+\n+    static std::pair<MutableDataPartPtr, scope_guard> clone(\n+        MergeTreeData * merge_tree_data,\n+        const DataPartPtr & src_part,\n+        const StorageMetadataPtr & metadata_snapshot,\n+        const MergeTreePartInfo & dst_part_info,\n+        const String & tmp_part_prefix,\n+        bool require_part_metadata,\n+        const IDataPartStorage::ClonePartParams & params,\n+        const ReadSettings & read_settings,\n+        const WriteSettings & write_settings);\n+\n+    static std::pair<MutableDataPartPtr, scope_guard> cloneWithDistinctPartitionExpression(\n+        MergeTreeData * merge_tree_data,\n+        const DataPartPtr & src_part,\n+        const StorageMetadataPtr & metadata_snapshot,\n+        const MergeTreePartInfo & dst_part_info,\n+        const String & tmp_part_prefix,\n+        const ReadSettings & read_settings,\n+        const WriteSettings & write_settings,\n+        const MergeTreePartition & new_partition,\n+        const IMergeTreeDataPart::MinMaxIndex & new_min_max_index,\n+        bool sync_new_files,\n+        const IDataPartStorage::ClonePartParams & params);\n+};\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreePartition.cpp b/src/Storages/MergeTree/MergeTreePartition.cpp\nindex ddeaf69136a3..76ef3be25b3a 100644\n--- a/src/Storages/MergeTree/MergeTreePartition.cpp\n+++ b/src/Storages/MergeTree/MergeTreePartition.cpp\n@@ -467,6 +467,45 @@ void MergeTreePartition::create(const StorageMetadataPtr & metadata_snapshot, Bl\n     }\n }\n \n+void MergeTreePartition::createAndValidateMinMaxPartitionIds(\n+    const StorageMetadataPtr & metadata_snapshot, Block block_with_min_max_partition_ids, ContextPtr context)\n+{\n+    if (!metadata_snapshot->hasPartitionKey())\n+        return;\n+\n+    auto partition_key_names_and_types = executePartitionByExpression(metadata_snapshot, block_with_min_max_partition_ids, context);\n+    value.resize(partition_key_names_and_types.size());\n+\n+    /// Executing partition_by expression adds new columns to passed block according to partition functions.\n+    /// The block is passed by reference and is used afterwards. `moduloLegacy` needs to be substituted back\n+    /// with just `modulo`, because it was a temporary substitution.\n+    static constexpr std::string_view modulo_legacy_function_name = \"moduloLegacy\";\n+\n+    size_t i = 0;\n+    for (const auto & element : partition_key_names_and_types)\n+    {\n+        auto & partition_column = block_with_min_max_partition_ids.getByName(element.name);\n+\n+        if (element.name.starts_with(modulo_legacy_function_name))\n+            partition_column.name.replace(0, modulo_legacy_function_name.size(), \"modulo\");\n+\n+        Field extracted_min_partition_id_field;\n+        Field extracted_max_partition_id_field;\n+\n+        partition_column.column->get(0, extracted_min_partition_id_field);\n+        partition_column.column->get(1, extracted_max_partition_id_field);\n+\n+        if (extracted_min_partition_id_field != extracted_max_partition_id_field)\n+        {\n+            throw Exception(\n+                ErrorCodes::INVALID_PARTITION_VALUE,\n+                \"Can not create the partition. A partition can not contain values that have different partition ids\");\n+        }\n+\n+        partition_column.column->get(0u, value[i++]);\n+    }\n+}\n+\n NamesAndTypesList MergeTreePartition::executePartitionByExpression(const StorageMetadataPtr & metadata_snapshot, Block & block, ContextPtr context)\n {\n     auto adjusted_partition_key = adjustPartitionKey(metadata_snapshot, context);\ndiff --git a/src/Storages/MergeTree/MergeTreePartition.h b/src/Storages/MergeTree/MergeTreePartition.h\nindex 78b141f26ec5..fd7ae02cde4d 100644\n--- a/src/Storages/MergeTree/MergeTreePartition.h\n+++ b/src/Storages/MergeTree/MergeTreePartition.h\n@@ -1,11 +1,12 @@\n #pragma once\n \n-#include <base/types.h>\n+#include <Core/Field.h>\n #include <Disks/IDisk.h>\n #include <IO/WriteBuffer.h>\n #include <Storages/KeyDescription.h>\n #include <Storages/MergeTree/IPartMetadataManager.h>\n-#include <Core/Field.h>\n+#include <Storages/MergeTree/PartMetadataManagerOrdinary.h>\n+#include <base/types.h>\n \n namespace DB\n {\n@@ -51,6 +52,11 @@ struct MergeTreePartition\n \n     void create(const StorageMetadataPtr & metadata_snapshot, Block block, size_t row, ContextPtr context);\n \n+    /// Copy of MergeTreePartition::create, but also validates if min max partition keys are equal. If they are different,\n+    /// it means the partition can't be created because the data doesn't belong to the same partition.\n+    void createAndValidateMinMaxPartitionIds(\n+        const StorageMetadataPtr & metadata_snapshot, Block block_with_min_max_partition_ids, ContextPtr context);\n+\n     static void appendFiles(const MergeTreeData & storage, Strings & files);\n \n     /// Adjust partition key and execute its expression on block. Return sample block according to used expression.\ndiff --git a/src/Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.cpp b/src/Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.cpp\nnew file mode 100644\nindex 000000000000..21bcdb84a960\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.cpp\n@@ -0,0 +1,91 @@\n+#include <Interpreters/MonotonicityCheckVisitor.h>\n+#include <Interpreters/getTableExpressions.h>\n+#include <Storages/MergeTree/MergeTreeData.h>\n+#include <Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.h>\n+#include <Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+extern const int BAD_ARGUMENTS;\n+}\n+\n+namespace\n+{\n+bool isDestinationPartitionExpressionMonotonicallyIncreasing(\n+    const std::vector<Range> & hyperrectangle, const MergeTreeData & destination_storage)\n+{\n+    auto destination_table_metadata = destination_storage.getInMemoryMetadataPtr();\n+\n+    auto key_description = destination_table_metadata->getPartitionKey();\n+    auto definition_ast = key_description.definition_ast->clone();\n+\n+    auto table_identifier = std::make_shared<ASTIdentifier>(destination_storage.getStorageID().getTableName());\n+    auto table_with_columns\n+        = TableWithColumnNamesAndTypes{DatabaseAndTableWithAlias(table_identifier), destination_table_metadata->getColumns().getOrdinary()};\n+\n+    auto expression_list = extractKeyExpressionList(definition_ast);\n+\n+    MonotonicityCheckVisitor::Data data{{table_with_columns}, destination_storage.getContext(), /*group_by_function_hashes*/ {}};\n+\n+    for (auto i = 0u; i < expression_list->children.size(); i++)\n+    {\n+        data.range = hyperrectangle[i];\n+\n+        MonotonicityCheckVisitor(data).visit(expression_list->children[i]);\n+\n+        if (!data.monotonicity.is_monotonic || !data.monotonicity.is_positive)\n+            return false;\n+    }\n+\n+    return true;\n+}\n+\n+bool isExpressionDirectSubsetOf(const ASTPtr source, const ASTPtr destination)\n+{\n+    auto source_expression_list = extractKeyExpressionList(source);\n+    auto destination_expression_list = extractKeyExpressionList(destination);\n+\n+    std::unordered_set<std::string> source_columns;\n+\n+    for (auto i = 0u; i < source_expression_list->children.size(); ++i)\n+        source_columns.insert(source_expression_list->children[i]->getColumnName());\n+\n+    for (auto i = 0u; i < destination_expression_list->children.size(); ++i)\n+        if (!source_columns.contains(destination_expression_list->children[i]->getColumnName()))\n+            return false;\n+\n+    return true;\n+}\n+}\n+\n+void MergeTreePartitionCompatibilityVerifier::verify(\n+    const MergeTreeData & source_storage, const MergeTreeData & destination_storage, const DataPartsVector & source_parts)\n+{\n+    const auto source_metadata = source_storage.getInMemoryMetadataPtr();\n+    const auto destination_metadata = destination_storage.getInMemoryMetadataPtr();\n+\n+    const auto source_partition_key_ast = source_metadata->getPartitionKeyAST();\n+    const auto destination_partition_key_ast = destination_metadata->getPartitionKeyAST();\n+\n+    // If destination partition expression columns are a subset of source partition expression columns,\n+    // there is no need to check for monotonicity.\n+    if (isExpressionDirectSubsetOf(source_partition_key_ast, destination_partition_key_ast))\n+        return;\n+\n+    const auto src_global_min_max_indexes = MergeTreePartitionGlobalMinMaxIdxCalculator::calculate(source_parts, destination_storage);\n+\n+    assert(!src_global_min_max_indexes.hyperrectangle.empty());\n+\n+    if (!isDestinationPartitionExpressionMonotonicallyIncreasing(src_global_min_max_indexes.hyperrectangle, destination_storage))\n+        throw DB::Exception(ErrorCodes::BAD_ARGUMENTS, \"Destination table partition expression is not monotonically increasing\");\n+\n+    MergeTreePartition().createAndValidateMinMaxPartitionIds(\n+        destination_storage.getInMemoryMetadataPtr(),\n+        src_global_min_max_indexes.getBlock(destination_storage),\n+        destination_storage.getContext());\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.h b/src/Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.h\nnew file mode 100644\nindex 000000000000..1682add3ebde\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.h\n@@ -0,0 +1,30 @@\n+#pragma once\n+\n+#include <Core/Field.h>\n+#include <Storages/MergeTree/IMergeTreeDataPart.h>\n+\n+namespace DB\n+{\n+\n+/*\n+ * Verifies that source and destination partitions are compatible.\n+ * To be compatible, one of the following criteria must be met:\n+ * 1. Destination partition expression columns are a subset of source partition columns; or\n+ * 2. Destination partition expression is monotonic on the source global min_max idx Range AND the computer partition id for\n+ * the source global min_max idx range is the same.\n+ *\n+ * If not, an exception is thrown.\n+ * */\n+\n+class MergeTreePartitionCompatibilityVerifier\n+{\n+public:\n+    using DataPart = IMergeTreeDataPart;\n+    using DataPartPtr = std::shared_ptr<const DataPart>;\n+    using DataPartsVector = std::vector<DataPartPtr>;\n+\n+    static void\n+    verify(const MergeTreeData & source_storage, const MergeTreeData & destination_storage, const DataPartsVector & source_parts);\n+};\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.cpp b/src/Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.cpp\nnew file mode 100644\nindex 000000000000..0871efadf0ca\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.cpp\n@@ -0,0 +1,25 @@\n+#include <Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.h>\n+\n+namespace DB\n+{\n+\n+IMergeTreeDataPart::MinMaxIndex\n+MergeTreePartitionGlobalMinMaxIdxCalculator::calculate(const DataPartsVector & parts, const MergeTreeData & storage)\n+{\n+    IMergeTreeDataPart::MinMaxIndex global_min_max_indexes;\n+\n+    for (const auto & part : parts)\n+    {\n+        auto metadata_manager = std::make_shared<PartMetadataManagerOrdinary>(part.get());\n+\n+        auto local_min_max_index = MergeTreeData::DataPart::MinMaxIndex();\n+\n+        local_min_max_index.load(storage, metadata_manager);\n+\n+        global_min_max_indexes.merge(local_min_max_index);\n+    }\n+\n+    return global_min_max_indexes;\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.h b/src/Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.h\nnew file mode 100644\nindex 000000000000..4f2711772469\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreePartitionGlobalMinMaxIdxCalculator.h\n@@ -0,0 +1,24 @@\n+#pragma once\n+\n+#include <utility>\n+\n+#include <Core/Field.h>\n+#include <Storages/MergeTree/MergeTreeData.h>\n+\n+namespace DB\n+{\n+\n+/*\n+ * Calculates global min max indexes for a given set of parts on given storage.\n+ * */\n+class MergeTreePartitionGlobalMinMaxIdxCalculator\n+{\n+    using DataPart = IMergeTreeDataPart;\n+    using DataPartPtr = std::shared_ptr<const DataPart>;\n+    using DataPartsVector = std::vector<DataPartPtr>;\n+\n+public:\n+    static IMergeTreeDataPart::MinMaxIndex calculate(const DataPartsVector & parts, const MergeTreeData & storage);\n+};\n+\n+}\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 4761ccd8b58a..fd5354a00a9e 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -5,9 +5,9 @@\n #include <optional>\n #include <ranges>\n \n-#include <base/sort.h>\n #include <Backups/BackupEntriesCollector.h>\n #include <Databases/IDatabase.h>\n+#include <IO/copyData.h>\n #include \"Common/Exception.h\"\n #include <Common/MemoryTracker.h>\n #include <Common/escapeForFileName.h>\n@@ -20,25 +20,30 @@\n #include <Interpreters/TransactionLog.h>\n #include <Interpreters/ClusterProxy/executeQuery.h>\n #include <Interpreters/ClusterProxy/SelectStreamFactory.h>\n+#include <Interpreters/InterpreterAlterQuery.h>\n #include <Interpreters/InterpreterSelectQueryAnalyzer.h>\n-#include <IO/copyData.h>\n #include <Parsers/ASTCheckQuery.h>\n #include <Parsers/ASTFunction.h>\n #include <Parsers/ASTLiteral.h>\n #include <Parsers/ASTPartition.h>\n #include <Parsers/ASTSetQuery.h>\n-#include <Parsers/queryToString.h>\n #include <Parsers/formatAST.h>\n+#include <Parsers/queryToString.h>\n+#include <Planner/Utils.h>\n #include <Storages/MergeTree/MergeTreeData.h>\n #include <Storages/MergeTree/ActiveDataPartSet.h>\n #include <Storages/AlterCommands.h>\n-#include <Storages/PartitionCommands.h>\n-#include <Storages/MergeTree/MergeTreeSink.h>\n-#include <Storages/MergeTree/MergeTreeDataPartInMemory.h>\n+#include <Storages/MergeTree/MergeList.h>\n #include <Storages/MergeTree/MergePlainMergeTreeTask.h>\n+#include <Storages/MergeTree/MergeTreeDataPartInMemory.h>\n+#include <Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.h>\n+#include <Storages/MergeTree/MergeTreeSink.h>\n+#include <Storages/MergeTree/PartMetadataManagerOrdinary.h>\n #include <Storages/MergeTree/PartitionPruner.h>\n-#include <Storages/MergeTree/MergeList.h>\n #include <Storages/MergeTree/checkDataPart.h>\n+#include <Storages/PartitionCommands.h>\n+#include <base/sort.h>\n+#include <Storages/buildQueryTreeForShard.h>\n #include <QueryPipeline/Pipe.h>\n #include <Processors/QueryPlan/QueryPlan.h>\n #include <Processors/QueryPlan/BuildQueryPipelineSettings.h>\n@@ -2039,41 +2044,73 @@ void StorageMergeTree::replacePartitionFrom(const StoragePtr & source_table, con\n     ProfileEventsScope profile_events_scope;\n \n     MergeTreeData & src_data = checkStructureAndGetMergeTreeData(source_table, source_metadata_snapshot, my_metadata_snapshot);\n-    String partition_id = getPartitionIDFromQuery(partition, local_context);\n+    String partition_id = src_data.getPartitionIDFromQuery(partition, local_context);\n \n     DataPartsVector src_parts = src_data.getVisibleDataPartsVectorInPartition(local_context, partition_id);\n+\n+    bool attach_empty_partition = !replace && src_parts.empty();\n+    if (attach_empty_partition)\n+        return;\n+\n     MutableDataPartsVector dst_parts;\n     std::vector<scope_guard> dst_parts_locks;\n \n     static const String TMP_PREFIX = \"tmp_replace_from_\";\n \n-    for (const DataPartPtr & src_part : src_parts)\n+    const auto my_partition_expression = my_metadata_snapshot->getPartitionKeyAST();\n+    const auto src_partition_expression = source_metadata_snapshot->getPartitionKeyAST();\n+    const auto is_partition_exp_different = queryToStringNullable(my_partition_expression) != queryToStringNullable(src_partition_expression);\n+\n+    if (is_partition_exp_different && !src_parts.empty())\n+        MergeTreePartitionCompatibilityVerifier::verify(src_data, /* destination_storage */ *this, src_parts);\n+\n+    for (DataPartPtr & src_part : src_parts)\n     {\n         if (!canReplacePartition(src_part))\n             throw Exception(ErrorCodes::BAD_ARGUMENTS,\n                             \"Cannot replace partition '{}' because part '{}' has inconsistent granularity with table\",\n                             partition_id, src_part->name);\n \n+        IDataPartStorage::ClonePartParams clone_params{.txn = local_context->getCurrentTransaction()};\n         /// This will generate unique name in scope of current server process.\n-        Int64 temp_index = insert_increment.get();\n-        MergeTreePartInfo dst_part_info(partition_id, temp_index, temp_index, src_part->info.level);\n+        auto index = insert_increment.get();\n \n-        IDataPartStorage::ClonePartParams clone_params{.txn = local_context->getCurrentTransaction()};\n-        auto [dst_part, part_lock] = cloneAndLoadDataPartOnSameDisk(\n-            src_part,\n-            TMP_PREFIX,\n-            dst_part_info,\n-            my_metadata_snapshot,\n-            clone_params,\n-            local_context->getReadSettings(),\n-            local_context->getWriteSettings());\n-        dst_parts.emplace_back(std::move(dst_part));\n-        dst_parts_locks.emplace_back(std::move(part_lock));\n-    }\n+        if (is_partition_exp_different)\n+        {\n+            auto [new_partition, new_min_max_index] = createPartitionAndMinMaxIndexFromSourcePart(\n+                src_part, my_metadata_snapshot, local_context);\n+\n+            auto [dst_part, part_lock] = cloneAndLoadPartOnSameDiskWithDifferentPartitionKey(\n+                src_part,\n+                new_partition,\n+                new_partition.getID(*this),\n+                new_min_max_index,\n+                TMP_PREFIX,\n+                my_metadata_snapshot,\n+                clone_params,\n+                local_context,\n+                index,\n+                index);\n \n-    /// ATTACH empty part set\n-    if (!replace && dst_parts.empty())\n-        return;\n+            dst_parts.emplace_back(std::move(dst_part));\n+            dst_parts_locks.emplace_back(std::move(part_lock));\n+        }\n+        else\n+        {\n+            MergeTreePartInfo dst_part_info(partition_id, index, index, src_part->info.level);\n+\n+            auto [dst_part, part_lock] = cloneAndLoadDataPartOnSameDisk(\n+                src_part,\n+                TMP_PREFIX,\n+                dst_part_info,\n+                my_metadata_snapshot,\n+                clone_params,\n+                local_context->getReadSettings(),\n+                local_context->getWriteSettings());\n+            dst_parts.emplace_back(std::move(dst_part));\n+            dst_parts_locks.emplace_back(std::move(part_lock));\n+        }\n+    }\n \n     MergeTreePartInfo drop_range;\n     if (replace)\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex f7e6783dbc27..512811e39d75 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -26,22 +26,21 @@\n \n #include <base/sort.h>\n \n-#include <Storages/buildQueryTreeForShard.h>\n #include <Storages/AlterCommands.h>\n #include <Storages/ColumnsDescription.h>\n #include <Storages/Freeze.h>\n #include <Storages/MergeTree/AsyncBlockIDsCache.h>\n #include <Storages/MergeTree/DataPartStorageOnDiskFull.h>\n-#include <Storages/MergeTree/extractZkPathFromCreateQuery.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n #include <Storages/MergeTree/LeaderElection.h>\n-#include <Storages/MergeTree/MergedBlockOutputStream.h>\n #include <Storages/MergeTree/MergeFromLogEntryTask.h>\n #include <Storages/MergeTree/MergeList.h>\n #include <Storages/MergeTree/MergeTreeBackgroundExecutor.h>\n #include <Storages/MergeTree/MergeTreeDataFormatVersion.h>\n #include <Storages/MergeTree/MergeTreePartInfo.h>\n+#include <Storages/MergeTree/MergeTreePartitionCompatibilityVerifier.h>\n #include <Storages/MergeTree/MergeTreeReaderCompact.h>\n+#include <Storages/MergeTree/MergedBlockOutputStream.h>\n #include <Storages/MergeTree/MutateFromLogEntryTask.h>\n #include <Storages/MergeTree/PinnedPartUUIDs.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeAddress.h>\n@@ -53,9 +52,11 @@\n #include <Storages/MergeTree/ReplicatedMergeTreeSink.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeTableMetadata.h>\n #include <Storages/MergeTree/ZeroCopyLock.h>\n+#include <Storages/MergeTree/extractZkPathFromCreateQuery.h>\n #include <Storages/PartitionCommands.h>\n #include <Storages/StorageReplicatedMergeTree.h>\n #include <Storages/VirtualColumnUtils.h>\n+#include <Storages/buildQueryTreeForShard.h>\n \n #include <Databases/DatabaseOnDisk.h>\n #include <Databases/DatabaseReplicated.h>\n@@ -2713,16 +2714,48 @@ bool StorageReplicatedMergeTree::executeReplaceRange(LogEntry & entry)\n                 .copy_instead_of_hardlink = storage_settings_ptr->always_use_copy_instead_of_hardlinks || ((our_zero_copy_enabled || source_zero_copy_enabled) && part_desc->src_table_part->isStoredOnRemoteDiskWithZeroCopySupport()),\n                 .metadata_version_to_write = metadata_snapshot->getMetadataVersion()\n             };\n-            auto [res_part, temporary_part_lock] = cloneAndLoadDataPartOnSameDisk(\n-                part_desc->src_table_part,\n-                TMP_PREFIX + \"clone_\",\n-                part_desc->new_part_info,\n-                metadata_snapshot,\n-                clone_params,\n-                getContext()->getReadSettings(),\n-                getContext()->getWriteSettings());\n-            part_desc->res_part = std::move(res_part);\n-            part_desc->temporary_part_lock = std::move(temporary_part_lock);\n+\n+            const auto my_partition_expression = metadata_snapshot->getPartitionKeyAST();\n+            const auto src_partition_expression = source_table->getInMemoryMetadataPtr()->getPartitionKeyAST();\n+\n+            const auto is_partition_exp_different = queryToStringNullable(my_partition_expression) != queryToStringNullable(src_partition_expression);\n+\n+            if (is_partition_exp_different)\n+            {\n+                auto [new_partition, new_min_max_index] = createPartitionAndMinMaxIndexFromSourcePart(\n+                    part_desc->src_table_part, metadata_snapshot, getContext());\n+\n+                auto partition_id = new_partition.getID(*this);\n+\n+                auto [res_part, temporary_part_lock] = cloneAndLoadPartOnSameDiskWithDifferentPartitionKey(\n+                    part_desc->src_table_part,\n+                    new_partition,\n+                    partition_id,\n+                    new_min_max_index,\n+                    TMP_PREFIX + \"clone_\",\n+                    metadata_snapshot,\n+                    clone_params,\n+                    getContext(),\n+                    part_desc->new_part_info.min_block,\n+                    part_desc->new_part_info.max_block);\n+\n+                part_desc->res_part = std::move(res_part);\n+                part_desc->temporary_part_lock = std::move(temporary_part_lock);\n+            }\n+            else\n+            {\n+                auto [res_part, temporary_part_lock] = cloneAndLoadDataPartOnSameDisk(\n+                    part_desc->src_table_part,\n+                    TMP_PREFIX + \"clone_\",\n+                    part_desc->new_part_info,\n+                    metadata_snapshot,\n+                    clone_params,\n+                    getContext()->getReadSettings(),\n+                    getContext()->getWriteSettings());\n+\n+                part_desc->res_part = std::move(res_part);\n+                part_desc->temporary_part_lock = std::move(temporary_part_lock);\n+            }\n         }\n         else if (!part_desc->replica.empty())\n         {\n@@ -7852,11 +7885,22 @@ void StorageReplicatedMergeTree::replacePartitionFrom(\n     ProfileEventsScope profile_events_scope;\n \n     MergeTreeData & src_data = checkStructureAndGetMergeTreeData(source_table, source_metadata_snapshot, metadata_snapshot);\n-    String partition_id = getPartitionIDFromQuery(partition, query_context);\n+    String partition_id = src_data.getPartitionIDFromQuery(partition, query_context);\n \n     /// NOTE: Some covered parts may be missing in src_all_parts if corresponding log entries are not executed yet.\n     DataPartsVector src_all_parts = src_data.getVisibleDataPartsVectorInPartition(query_context, partition_id);\n \n+    bool attach_empty_partition = !replace && src_all_parts.empty();\n+    if (attach_empty_partition)\n+        return;\n+\n+    const auto my_partition_expression = metadata_snapshot->getPartitionKeyAST();\n+    const auto src_partition_expression = source_metadata_snapshot->getPartitionKeyAST();\n+    const auto is_partition_exp_different = queryToStringNullable(my_partition_expression) != queryToStringNullable(src_partition_expression);\n+\n+    if (is_partition_exp_different && !src_all_parts.empty())\n+        MergeTreePartitionCompatibilityVerifier::verify(src_data, /* destination_storage */ *this, src_all_parts);\n+\n     LOG_DEBUG(log, \"Cloning {} parts\", src_all_parts.size());\n \n     static const String TMP_PREFIX = \"tmp_replace_from_\";\n@@ -7911,6 +7955,18 @@ void StorageReplicatedMergeTree::replacePartitionFrom(\n                                 \"Cannot replace partition '{}' because part '{}\"\n                                 \"' has inconsistent granularity with table\", partition_id, src_part->name);\n \n+            IMergeTreeDataPart::MinMaxIndex min_max_index = *src_part->minmax_idx;\n+            MergeTreePartition merge_tree_partition = src_part->partition;\n+\n+            if (is_partition_exp_different)\n+            {\n+                auto [new_partition, new_min_max_index] = createPartitionAndMinMaxIndexFromSourcePart(src_part, metadata_snapshot, query_context);\n+\n+                merge_tree_partition = new_partition;\n+                min_max_index = new_min_max_index;\n+                partition_id = merge_tree_partition.getID(*this);\n+            }\n+\n             String hash_hex = src_part->checksums.getTotalChecksumHex();\n             const bool is_duplicated_part = replaced_parts.contains(hash_hex);\n             replaced_parts.insert(hash_hex);\n@@ -7929,27 +7985,52 @@ void StorageReplicatedMergeTree::replacePartitionFrom(\n                 continue;\n             }\n \n-            UInt64 index = lock->getNumber();\n-            MergeTreePartInfo dst_part_info(partition_id, index, index, src_part->info.level);\n-\n             bool zero_copy_enabled = storage_settings_ptr->allow_remote_fs_zero_copy_replication\n                 || dynamic_cast<const MergeTreeData *>(source_table.get())->getSettings()->allow_remote_fs_zero_copy_replication;\n+\n+            UInt64 index = lock->getNumber();\n+\n             IDataPartStorage::ClonePartParams clone_params\n             {\n                 .copy_instead_of_hardlink = storage_settings_ptr->always_use_copy_instead_of_hardlinks || (zero_copy_enabled && src_part->isStoredOnRemoteDiskWithZeroCopySupport()),\n                 .metadata_version_to_write = metadata_snapshot->getMetadataVersion()\n             };\n-            auto [dst_part, part_lock] = cloneAndLoadDataPartOnSameDisk(\n-                src_part,\n-                TMP_PREFIX,\n-                dst_part_info,\n-                metadata_snapshot,\n-                clone_params,\n-                query_context->getReadSettings(),\n-                query_context->getWriteSettings());\n+\n+            if (is_partition_exp_different)\n+            {\n+                auto [dst_part, part_lock] = cloneAndLoadPartOnSameDiskWithDifferentPartitionKey(\n+                    src_part,\n+                    merge_tree_partition,\n+                    partition_id,\n+                    min_max_index,\n+                    TMP_PREFIX,\n+                    metadata_snapshot,\n+                    clone_params,\n+                    query_context,\n+                    index,\n+                    index);\n+\n+                dst_parts.emplace_back(dst_part);\n+                dst_parts_locks.emplace_back(std::move(part_lock));\n+            }\n+            else\n+            {\n+                MergeTreePartInfo dst_part_info(partition_id, index, index, src_part->info.level);\n+\n+                auto [dst_part, part_lock] = cloneAndLoadDataPartOnSameDisk(\n+                    src_part,\n+                    TMP_PREFIX,\n+                    dst_part_info,\n+                    metadata_snapshot,\n+                    clone_params,\n+                    query_context->getReadSettings(),\n+                    query_context->getWriteSettings());\n+\n+                dst_parts.emplace_back(dst_part);\n+                dst_parts_locks.emplace_back(std::move(part_lock));\n+            }\n+\n             src_parts.emplace_back(src_part);\n-            dst_parts.emplace_back(dst_part);\n-            dst_parts_locks.emplace_back(std::move(part_lock));\n             ephemeral_locks.emplace_back(std::move(*lock));\n             block_id_paths.emplace_back(block_id_path);\n             part_checksums.emplace_back(hash_hex);\n",
  "test_patch": "diff --git a/tests/integration/test_attach_partition_distinct_expression_replicated/__init__.py b/tests/integration/test_attach_partition_distinct_expression_replicated/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_attach_partition_distinct_expression_replicated/configs/remote_servers.xml b/tests/integration/test_attach_partition_distinct_expression_replicated/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..b40730e9f7d5\n--- /dev/null\n+++ b/tests/integration/test_attach_partition_distinct_expression_replicated/configs/remote_servers.xml\n@@ -0,0 +1,17 @@\n+<clickhouse>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>replica1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>replica2</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+</clickhouse>\ndiff --git a/tests/integration/test_attach_partition_distinct_expression_replicated/test.py b/tests/integration/test_attach_partition_distinct_expression_replicated/test.py\nnew file mode 100644\nindex 000000000000..1d8ac4e9e370\n--- /dev/null\n+++ b/tests/integration/test_attach_partition_distinct_expression_replicated/test.py\n@@ -0,0 +1,214 @@\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import assert_eq_with_retry\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+replica1 = cluster.add_instance(\n+    \"replica1\", with_zookeeper=True, main_configs=[\"configs/remote_servers.xml\"]\n+)\n+replica2 = cluster.add_instance(\n+    \"replica2\", with_zookeeper=True, main_configs=[\"configs/remote_servers.xml\"]\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    except Exception as ex:\n+        print(ex)\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def cleanup(nodes):\n+    for node in nodes:\n+        node.query(\"DROP TABLE IF EXISTS source SYNC\")\n+        node.query(\"DROP TABLE IF EXISTS destination SYNC\")\n+\n+\n+def create_table(node, table_name, replicated):\n+    replica = node.name\n+    engine = (\n+        f\"ReplicatedMergeTree('/clickhouse/tables/1/{table_name}', '{replica}')\"\n+        if replicated\n+        else \"MergeTree()\"\n+    )\n+    partition_expression = (\n+        \"toYYYYMMDD(timestamp)\" if table_name == \"source\" else \"toYYYYMM(timestamp)\"\n+    )\n+    node.query_with_retry(\n+        \"\"\"\n+        CREATE TABLE {table_name}(timestamp DateTime)\n+        ENGINE = {engine}\n+        ORDER BY tuple() PARTITION BY {partition_expression}\n+        SETTINGS cleanup_delay_period=1, cleanup_delay_period_random_add=1, max_cleanup_delay_period=1;\n+        \"\"\".format(\n+            table_name=table_name,\n+            engine=engine,\n+            partition_expression=partition_expression,\n+        )\n+    )\n+\n+\n+def test_both_replicated(start_cluster):\n+    for node in [replica1, replica2]:\n+        create_table(node, \"source\", True)\n+        create_table(node, \"destination\", True)\n+\n+    replica1.query(\"INSERT INTO source VALUES ('2010-03-02 02:01:01')\")\n+    replica1.query(\"SYSTEM SYNC REPLICA source\")\n+    replica1.query(\"SYSTEM SYNC REPLICA destination\")\n+    replica1.query(\n+        f\"ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source\"\n+    )\n+\n+    assert_eq_with_retry(\n+        replica1, f\"SELECT * FROM destination\", \"2010-03-02 02:01:01\\n\"\n+    )\n+    assert_eq_with_retry(\n+        replica1,\n+        f\"SELECT * FROM destination\",\n+        replica2.query(f\"SELECT * FROM destination\"),\n+    )\n+\n+    cleanup([replica1, replica2])\n+\n+\n+def test_only_destination_replicated(start_cluster):\n+    create_table(replica1, \"source\", False)\n+    create_table(replica1, \"destination\", True)\n+    create_table(replica2, \"destination\", True)\n+\n+    replica1.query(\"INSERT INTO source VALUES ('2010-03-02 02:01:01')\")\n+    replica1.query(\"SYSTEM SYNC REPLICA destination\")\n+    replica1.query(\n+        f\"ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source\"\n+    )\n+\n+    assert_eq_with_retry(\n+        replica1, f\"SELECT * FROM destination\", \"2010-03-02 02:01:01\\n\"\n+    )\n+    assert_eq_with_retry(\n+        replica1,\n+        f\"SELECT * FROM destination\",\n+        replica2.query(f\"SELECT * FROM destination\"),\n+    )\n+\n+    cleanup([replica1, replica2])\n+\n+\n+def test_both_replicated_partitioned_to_unpartitioned(start_cluster):\n+    def create_tables(nodes):\n+        for node in nodes:\n+            source_engine = (\n+                f\"ReplicatedMergeTree('/clickhouse/tables/1/source', '{node.name}')\"\n+            )\n+            node.query(\n+                \"\"\"\n+                CREATE TABLE source(timestamp DateTime)\n+                ENGINE = {engine}\n+                ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp)\n+                SETTINGS cleanup_delay_period=1, cleanup_delay_period_random_add=1, max_cleanup_delay_period=1;\n+                \"\"\".format(\n+                    engine=source_engine,\n+                )\n+            )\n+\n+            destination_engine = f\"ReplicatedMergeTree('/clickhouse/tables/1/destination', '{node.name}')\"\n+            node.query(\n+                \"\"\"\n+                CREATE TABLE destination(timestamp DateTime)\n+                ENGINE = {engine}\n+                ORDER BY tuple() PARTITION BY tuple()\n+                SETTINGS cleanup_delay_period=1, cleanup_delay_period_random_add=1, max_cleanup_delay_period=1;\n+                \"\"\".format(\n+                    engine=destination_engine,\n+                )\n+            )\n+\n+    create_tables([replica1, replica2])\n+\n+    replica1.query(\"INSERT INTO source VALUES ('2010-03-02 02:01:01')\")\n+    replica1.query(\"INSERT INTO source VALUES ('2010-03-03 02:01:01')\")\n+    replica1.query(\"SYSTEM SYNC REPLICA source\")\n+    replica1.query(\"SYSTEM SYNC REPLICA destination\")\n+\n+    replica1.query(\n+        f\"ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source\"\n+    )\n+    replica1.query(\n+        f\"ALTER TABLE destination ATTACH PARTITION ID '20100303' FROM source\"\n+    )\n+\n+    assert_eq_with_retry(\n+        replica1,\n+        f\"SELECT * FROM destination ORDER BY timestamp\",\n+        \"2010-03-02 02:01:01\\n2010-03-03 02:01:01\\n\",\n+    )\n+    assert_eq_with_retry(\n+        replica1,\n+        f\"SELECT * FROM destination ORDER BY timestamp\",\n+        replica2.query(f\"SELECT * FROM destination ORDER BY timestamp\"),\n+    )\n+\n+    cleanup([replica1, replica2])\n+\n+\n+def test_both_replicated_different_exp_same_id(start_cluster):\n+    def create_tables(nodes):\n+        for node in nodes:\n+            source_engine = (\n+                f\"ReplicatedMergeTree('/clickhouse/tables/1/source', '{node.name}')\"\n+            )\n+            node.query(\n+                \"\"\"\n+                CREATE TABLE source(a UInt16,b UInt16,c UInt16,extra UInt64,Path String,Time DateTime,Value Float64,Timestamp Int64,sign Int8)\n+                ENGINE = {engine}\n+                ORDER BY tuple() PARTITION BY a % 3\n+                SETTINGS cleanup_delay_period=1, cleanup_delay_period_random_add=1, max_cleanup_delay_period=1;\n+                \"\"\".format(\n+                    engine=source_engine,\n+                )\n+            )\n+\n+            destination_engine = f\"ReplicatedMergeTree('/clickhouse/tables/1/destination', '{node.name}')\"\n+            node.query(\n+                \"\"\"\n+                CREATE TABLE destination(a UInt16,b UInt16,c UInt16,extra UInt64,Path String,Time DateTime,Value Float64,Timestamp Int64,sign Int8)\n+                ENGINE = {engine}\n+                ORDER BY tuple() PARTITION BY a\n+                SETTINGS cleanup_delay_period=1, cleanup_delay_period_random_add=1, max_cleanup_delay_period=1;\n+                \"\"\".format(\n+                    engine=destination_engine,\n+                )\n+            )\n+\n+    create_tables([replica1, replica2])\n+\n+    replica1.query(\n+        \"INSERT INTO source (a, b, c, extra, sign) VALUES (1, 5, 9, 1000, 1)\"\n+    )\n+    replica1.query(\n+        \"INSERT INTO source (a, b, c, extra, sign) VALUES (2, 6, 10, 1000, 1)\"\n+    )\n+    replica1.query(\"SYSTEM SYNC REPLICA source\")\n+    replica1.query(\"SYSTEM SYNC REPLICA destination\")\n+\n+    replica1.query(f\"ALTER TABLE destination ATTACH PARTITION 1 FROM source\")\n+    replica1.query(f\"ALTER TABLE destination ATTACH PARTITION 2 FROM source\")\n+\n+    assert_eq_with_retry(\n+        replica1,\n+        f\"SELECT * FROM destination ORDER BY a\",\n+        \"1\\t5\\t9\\t1000\\t\\t1970-01-01 00:00:00\\t0\\t0\\t1\\n2\\t6\\t10\\t1000\\t\\t1970-01-01 00:00:00\\t0\\t0\\t1\\n\",\n+    )\n+    assert_eq_with_retry(\n+        replica1,\n+        f\"SELECT * FROM destination ORDER BY a\",\n+        replica2.query(f\"SELECT * FROM destination ORDER BY a\"),\n+    )\n+\n+    cleanup([replica1, replica2])\ndiff --git a/tests/queries/0_stateless/02456_test_attach_partition_different_partition_exp.reference b/tests/queries/0_stateless/02456_test_attach_partition_different_partition_exp.reference\nnew file mode 100644\nindex 000000000000..f1d036b08bf2\n--- /dev/null\n+++ b/tests/queries/0_stateless/02456_test_attach_partition_different_partition_exp.reference\n@@ -0,0 +1,467 @@\n+-- { echoOn }\n+-- Should be allowed since destination partition expr is monotonically increasing and compatible\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+201003\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '20100302' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+201003\n+-- Should be allowed since destination partition expr is monotonically increasing and compatible. Note that even though\n+-- the destination partition expression is more granular, the data would still fall in the same partition. Thus, it is valid\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+20100302\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '201003' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+20100302\n+-- Should be allowed since destination partition expr is monotonically increasing and compatible for those specific values\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY intDiv(A, 6);\n+CREATE TABLE destination (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY A;\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01', 1), ('2010-03-02 02:01:03', 1);\n+ALTER TABLE destination ATTACH PARTITION ID '0' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\n+2010-03-02 02:01:03\t1\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\n+2010-03-02 02:01:03\t1\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+1\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION 0 FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\n+2010-03-02 02:01:03\t1\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\n+2010-03-02 02:01:03\t1\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+1\n+-- Should be allowed because dst partition exp is monot inc and data is not split\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY cityHash64(category);\n+CREATE TABLE destination (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(category);\n+INSERT INTO TABLE source VALUES ('spaghetti', 'food'), ('mop', 'general');\n+INSERT INTO TABLE source VALUES ('rice', 'food');\n+ALTER TABLE destination ATTACH PARTITION ID '17908065610379824077' from source;\n+SELECT * FROM source ORDER BY productName;\n+mop\tgeneral\n+rice\tfood\n+spaghetti\tfood\n+SELECT * FROM destination ORDER BY productName;\n+rice\tfood\n+spaghetti\tfood\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+59532f3c39a412a413f0f014c7750a9d\n+59532f3c39a412a413f0f014c7750a9d\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '17908065610379824077' from source;\n+SELECT * FROM source ORDER BY productName;\n+mop\tgeneral\n+rice\tfood\n+spaghetti\tfood\n+SELECT * FROM destination ORDER BY productName;\n+rice\tfood\n+spaghetti\tfood\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+59532f3c39a412a413f0f014c7750a9d\n+59532f3c39a412a413f0f014c7750a9d\n+-- Should be allowed, extra test case to validate https://github.com/ClickHouse/ClickHouse/pull/39507#issuecomment-1747574133\n+\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp Int64) engine=MergeTree ORDER BY (timestamp) PARTITION BY intDiv(timestamp, 86400000);\n+CREATE TABLE destination (timestamp Int64) engine=MergeTree ORDER BY (timestamp) PARTITION BY toYear(toDateTime(intDiv(timestamp, 1000)));\n+INSERT INTO TABLE source VALUES (1267495261123);\n+ALTER TABLE destination ATTACH PARTITION ID '14670' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+1267495261123\n+SELECT * FROM destination ORDER BY timestamp;\n+1267495261123\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+2010\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '14670' from source;\n+SELECT * FROM source ORDER BY timestamp;\n+1267495261123\n+SELECT * FROM destination ORDER BY timestamp;\n+1267495261123\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+2010\n+-- Should be allowed, extra test case to validate https://github.com/ClickHouse/ClickHouse/pull/39507#issuecomment-1747511726\n+\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime('UTC'), key Int64, f Float64) engine=MergeTree ORDER BY (key, timestamp) PARTITION BY toYear(timestamp);\n+CREATE TABLE destination (timestamp DateTime('UTC'), key Int64, f Float64) engine=MergeTree ORDER BY (key, timestamp) PARTITION BY (intDiv(toUInt32(timestamp),86400));\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01',1,1),('2010-03-02 02:01:01',1,1),('2011-02-02 02:01:03',1,1);\n+ALTER TABLE destination ATTACH PARTITION ID '2010' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\t1\n+2010-03-02 02:01:01\t1\t1\n+2011-02-02 02:01:03\t1\t1\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\t1\n+2010-03-02 02:01:01\t1\t1\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+14670\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '2010' from source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\t1\n+2010-03-02 02:01:01\t1\t1\n+2011-02-02 02:01:03\t1\t1\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\t1\t1\n+2010-03-02 02:01:01\t1\t1\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+14670\n+-- Should be allowed, partitioned table to unpartitioned. Since the destination is unpartitioned, parts would ultimately\n+-- fall into the same partition.\n+-- Destination partition by expression is omitted, which causes StorageMetadata::getPartitionKeyAST() to be nullptr.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple();\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+all\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '201003' from source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+all\n+-- Same as above, but destination partition by expression is explicitly defined. Test case required to validate that\n+-- partition by tuple() is accepted.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY tuple();\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+all\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '201003' from source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+all\n+-- Should be allowed because the destination partition expression columns are a subset of the source partition expression columns\n+-- Columns in this case refer to the expression elements, not to the actual table columns\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY (a, b, c);\n+CREATE TABLE destination (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY (a, b);\n+INSERT INTO TABLE source VALUES (1, 2, 3), (1, 2, 4);\n+ALTER TABLE destination ATTACH PARTITION ID '1-2-3' FROM source;\n+SELECT * FROM source ORDER BY (a, b, c);\n+1\t2\t3\n+1\t2\t4\n+SELECT * FROM destination ORDER BY (a, b, c);\n+1\t2\t3\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+1-2\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION (1, 2, 3) from source;\n+SELECT * FROM source ORDER BY (a, b, c);\n+1\t2\t3\n+1\t2\t4\n+SELECT * FROM destination ORDER BY (a, b, c);\n+1\t2\t3\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+1-2\n+-- Should be allowed because the destination partition expression columns are a subset of the source partition expression columns\n+-- Columns in this case refer to the expression elements, not to the actual table columns\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY (a, b, c);\n+CREATE TABLE destination (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY a;\n+INSERT INTO TABLE source VALUES (1, 2, 3), (1, 2, 4);\n+ALTER TABLE destination ATTACH PARTITION ID '1-2-3' FROM source;\n+SELECT * FROM source ORDER BY (a, b, c);\n+1\t2\t3\n+1\t2\t4\n+SELECT * FROM destination ORDER BY (a, b, c);\n+1\t2\t3\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+1\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION (1, 2, 3) from source;\n+SELECT * FROM source ORDER BY (a, b, c);\n+1\t2\t3\n+1\t2\t4\n+SELECT * FROM destination ORDER BY (a, b, c);\n+1\t2\t3\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+1\n+-- Should be allowed. Special test case, tricky to explain. First column of source partition expression is\n+-- timestamp, while first column of destination partition expression is `A`. One of the previous implementations\n+-- would not match the columns, which could lead to `timestamp` min max being used to calculate monotonicity of `A`.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (`timestamp` DateTime, `A` Int64) ENGINE = MergeTree PARTITION BY tuple(toYYYYMM(timestamp), intDiv(A, 6)) ORDER BY timestamp;\n+CREATE TABLE destination (`timestamp` DateTime, `A` Int64) ENGINE = MergeTree PARTITION BY A ORDER BY timestamp;\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01', 5);\n+ALTER TABLE destination ATTACH PARTITION ID '201003-0' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\t5\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\t5\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+5\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION (201003, 0) from source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\t5\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\t5\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+5\n+-- Should be allowed. Destination partition expression contains multiple expressions, but all of them are monotonically\n+-- increasing in the source partition min max indexes.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(A, B) ORDER BY tuple();\n+CREATE TABLE destination (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(intDiv(A, 2), intDiv(B, 2)) ORDER BY tuple();\n+INSERT INTO TABLE source VALUES (6, 12);\n+ALTER TABLE destination ATTACH PARTITION ID '6-12' FROM source;\n+SELECT * FROM source ORDER BY A;\n+6\t12\n+SELECT * FROM destination ORDER BY A;\n+6\t12\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+3-6\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION (6, 12) from source;\n+SELECT * FROM source ORDER BY A;\n+6\t12\n+SELECT * FROM destination ORDER BY A;\n+6\t12\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+3-6\n+-- Should be allowed. The same scenario as above, but partition expressions inverted.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(intDiv(A, 2), intDiv(B, 2)) ORDER BY tuple();\n+CREATE TABLE destination (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(A, B) ORDER BY tuple();\n+INSERT INTO TABLE source VALUES (6, 12);\n+ALTER TABLE destination ATTACH PARTITION ID '3-6' FROM source;\n+SELECT * FROM source ORDER BY A;\n+6\t12\n+SELECT * FROM destination ORDER BY A;\n+6\t12\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+6-12\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION (3, 6) from source;\n+SELECT * FROM source ORDER BY A;\n+6\t12\n+SELECT * FROM destination ORDER BY A;\n+6\t12\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+6-12\n+-- Should be allowed, it is a local operation, no different than regular attach. Replicated to replicated.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE\n+    source(timestamp DateTime)\n+    ENGINE = ReplicatedMergeTree('/clickhouse/tables/{database}/test/source_replicated_to_replicated_distinct_expression', '1')\n+        PARTITION BY toYYYYMMDD(timestamp)\n+        ORDER BY tuple();\n+CREATE TABLE\n+    destination(timestamp DateTime)\n+    ENGINE = ReplicatedMergeTree('/clickhouse/tables/{database}/test/destination_replicated_to_replicated_distinct_expression', '1')\n+        PARTITION BY toYYYYMM(timestamp)\n+        ORDER BY tuple();\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+201003\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '20100302' from source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+201003\n+-- Should be allowed, it is a local operation, no different than regular attach. Non replicated to replicated\n+DROP TABLE IF EXISTS source SYNC;\n+DROP TABLE IF EXISTS destination SYNC;\n+CREATE TABLE source(timestamp DateTime) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(timestamp) ORDER BY tuple();\n+CREATE TABLE\n+    destination(timestamp DateTime)\n+    ENGINE = ReplicatedMergeTree('/clickhouse/tables/{database}/test/destination_non_replicated_to_replicated_distinct_expression', '1')\n+        PARTITION BY toYYYYMM(timestamp)\n+        ORDER BY tuple();\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+201003\n+TRUNCATE TABLE destination;\n+ALTER TABLE destination ATTACH PARTITION '20100302' from source;\n+SELECT * FROM source ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT * FROM destination ORDER BY timestamp;\n+2010-03-02 02:01:01\n+2010-03-02 02:01:03\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+201003\n+-- Should not be allowed because data would be split into two different partitions\n+DROP TABLE IF EXISTS source SYNC;\n+DROP TABLE IF EXISTS destination SYNC;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-03 02:01:03');\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source; -- { serverError 248 }\n+ALTER TABLE destination ATTACH PARTITION '201003' from source; -- { serverError 248 }\n+-- Should not be allowed because data would be split into two different partitions\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY intDiv(A, 6);\n+CREATE TABLE destination (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY A;\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01', 1), ('2010-03-02 02:01:03', 2);\n+ALTER TABLE destination ATTACH PARTITION ID '0' FROM source; -- { serverError 248 }\n+ALTER TABLE destination ATTACH PARTITION 0 FROM source; -- { serverError 248 }\n+-- Should not be allowed because dst partition exp takes more than two arguments, so it's not considered monotonically inc\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(category);\n+CREATE TABLE destination (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY substring(category, 1, 2);\n+INSERT INTO TABLE source VALUES ('spaghetti', 'food'), ('mop', 'general');\n+INSERT INTO TABLE source VALUES ('rice', 'food');\n+ALTER TABLE destination ATTACH PARTITION ID '4590ba78048910b74a47d5bfb308abed' from source; -- { serverError 36 }\n+ALTER TABLE destination ATTACH PARTITION 'food' from source; -- { serverError 36 }\n+-- Should not be allowed because dst partition exp depends on a different set of columns\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(category);\n+CREATE TABLE destination (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(productName);\n+INSERT INTO TABLE source VALUES ('spaghetti', 'food'), ('mop', 'general');\n+INSERT INTO TABLE source VALUES ('rice', 'food');\n+ALTER TABLE destination ATTACH PARTITION ID '4590ba78048910b74a47d5bfb308abed' from source; -- { serverError 36 }\n+ALTER TABLE destination ATTACH PARTITION 'food' from source; -- { serverError 36 }\n+-- Should not be allowed because dst partition exp is not monotonically increasing\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (productName String) engine=MergeTree ORDER BY tuple() PARTITION BY left(productName, 2);\n+CREATE TABLE destination (productName String) engine=MergeTree ORDER BY tuple() PARTITION BY cityHash64(productName);\n+INSERT INTO TABLE source VALUES ('bread'), ('mop');\n+INSERT INTO TABLE source VALUES ('broccoli');\n+ALTER TABLE destination ATTACH PARTITION ID '4589453b7ee96ce9de1265bd57674496' from source; -- { serverError 36 }\n+ALTER TABLE destination ATTACH PARTITION 'br' from source; -- { serverError 36 }\n+-- Empty/ non-existent partition, same partition expression. Nothing should happen\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+ALTER TABLE destination ATTACH PARTITION ID '1' FROM source;\n+ALTER TABLE destination ATTACH PARTITION 1 FROM source;\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+-- Empty/ non-existent partition, different partition expression. Nothing should happen\n+-- https://github.com/ClickHouse/ClickHouse/pull/39507#discussion_r1399839045\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+ALTER TABLE destination ATTACH PARTITION ID '1' FROM source;\n+ALTER TABLE destination ATTACH PARTITION 1 FROM source;\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+-- Replace instead of attach. Empty/ non-existent partition, same partition expression. Nothing should happen\n+-- https://github.com/ClickHouse/ClickHouse/pull/39507#discussion_r1399839045\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+ALTER TABLE destination REPLACE PARTITION '1' FROM source;\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+-- Replace instead of attach. Empty/ non-existent partition to non-empty partition, same partition id.\n+-- https://github.com/ClickHouse/ClickHouse/pull/39507#discussion_r1399839045\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (A Int) engine=MergeTree ORDER BY tuple() PARTITION BY A;\n+CREATE TABLE destination (A Int) engine=MergeTree ORDER BY tuple() PARTITION BY A;\n+INSERT INTO TABLE destination VALUES (1);\n+ALTER TABLE destination REPLACE PARTITION '1' FROM source;\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\ndiff --git a/tests/queries/0_stateless/02456_test_attach_partition_different_partition_exp.sql b/tests/queries/0_stateless/02456_test_attach_partition_different_partition_exp.sql\nnew file mode 100644\nindex 000000000000..9547d6ae249c\n--- /dev/null\n+++ b/tests/queries/0_stateless/02456_test_attach_partition_different_partition_exp.sql\n@@ -0,0 +1,485 @@\n+-- { echoOn }\n+-- Should be allowed since destination partition expr is monotonically increasing and compatible\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '20100302' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed since destination partition expr is monotonically increasing and compatible. Note that even though\n+-- the destination partition expression is more granular, the data would still fall in the same partition. Thus, it is valid\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '201003' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed since destination partition expr is monotonically increasing and compatible for those specific values\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY intDiv(A, 6);\n+\n+CREATE TABLE destination (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY A;\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01', 1), ('2010-03-02 02:01:03', 1);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '0' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION 0 FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed because dst partition exp is monot inc and data is not split\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY cityHash64(category);\n+CREATE TABLE destination (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(category);\n+\n+INSERT INTO TABLE source VALUES ('spaghetti', 'food'), ('mop', 'general');\n+INSERT INTO TABLE source VALUES ('rice', 'food');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '17908065610379824077' from source;\n+\n+SELECT * FROM source ORDER BY productName;\n+SELECT * FROM destination ORDER BY productName;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '17908065610379824077' from source;\n+\n+SELECT * FROM source ORDER BY productName;\n+SELECT * FROM destination ORDER BY productName;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed, extra test case to validate https://github.com/ClickHouse/ClickHouse/pull/39507#issuecomment-1747574133\n+\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp Int64) engine=MergeTree ORDER BY (timestamp) PARTITION BY intDiv(timestamp, 86400000);\n+CREATE TABLE destination (timestamp Int64) engine=MergeTree ORDER BY (timestamp) PARTITION BY toYear(toDateTime(intDiv(timestamp, 1000)));\n+\n+INSERT INTO TABLE source VALUES (1267495261123);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '14670' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '14670' from source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed, extra test case to validate https://github.com/ClickHouse/ClickHouse/pull/39507#issuecomment-1747511726\n+\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime('UTC'), key Int64, f Float64) engine=MergeTree ORDER BY (key, timestamp) PARTITION BY toYear(timestamp);\n+CREATE TABLE destination (timestamp DateTime('UTC'), key Int64, f Float64) engine=MergeTree ORDER BY (key, timestamp) PARTITION BY (intDiv(toUInt32(timestamp),86400));\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01',1,1),('2010-03-02 02:01:01',1,1),('2011-02-02 02:01:03',1,1);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '2010' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '2010' from source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed, partitioned table to unpartitioned. Since the destination is unpartitioned, parts would ultimately\n+-- fall into the same partition.\n+-- Destination partition by expression is omitted, which causes StorageMetadata::getPartitionKeyAST() to be nullptr.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple();\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '201003' from source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Same as above, but destination partition by expression is explicitly defined. Test case required to validate that\n+-- partition by tuple() is accepted.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY tuple();\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '201003' from source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed because the destination partition expression columns are a subset of the source partition expression columns\n+-- Columns in this case refer to the expression elements, not to the actual table columns\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY (a, b, c);\n+CREATE TABLE destination (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY (a, b);\n+\n+INSERT INTO TABLE source VALUES (1, 2, 3), (1, 2, 4);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '1-2-3' FROM source;\n+\n+SELECT * FROM source ORDER BY (a, b, c);\n+SELECT * FROM destination ORDER BY (a, b, c);\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION (1, 2, 3) from source;\n+\n+SELECT * FROM source ORDER BY (a, b, c);\n+SELECT * FROM destination ORDER BY (a, b, c);\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed because the destination partition expression columns are a subset of the source partition expression columns\n+-- Columns in this case refer to the expression elements, not to the actual table columns\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE source (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY (a, b, c);\n+CREATE TABLE destination (a Int, b Int, c Int) engine=MergeTree ORDER BY tuple() PARTITION BY a;\n+\n+INSERT INTO TABLE source VALUES (1, 2, 3), (1, 2, 4);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '1-2-3' FROM source;\n+\n+SELECT * FROM source ORDER BY (a, b, c);\n+SELECT * FROM destination ORDER BY (a, b, c);\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION (1, 2, 3) from source;\n+\n+SELECT * FROM source ORDER BY (a, b, c);\n+SELECT * FROM destination ORDER BY (a, b, c);\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed. Special test case, tricky to explain. First column of source partition expression is\n+-- timestamp, while first column of destination partition expression is `A`. One of the previous implementations\n+-- would not match the columns, which could lead to `timestamp` min max being used to calculate monotonicity of `A`.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (`timestamp` DateTime, `A` Int64) ENGINE = MergeTree PARTITION BY tuple(toYYYYMM(timestamp), intDiv(A, 6)) ORDER BY timestamp;\n+CREATE TABLE destination (`timestamp` DateTime, `A` Int64) ENGINE = MergeTree PARTITION BY A ORDER BY timestamp;\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01', 5);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '201003-0' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION (201003, 0) from source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed. Destination partition expression contains multiple expressions, but all of them are monotonically\n+-- increasing in the source partition min max indexes.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(A, B) ORDER BY tuple();\n+CREATE TABLE destination (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(intDiv(A, 2), intDiv(B, 2)) ORDER BY tuple();\n+\n+INSERT INTO TABLE source VALUES (6, 12);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '6-12' FROM source;\n+\n+SELECT * FROM source ORDER BY A;\n+SELECT * FROM destination ORDER BY A;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION (6, 12) from source;\n+\n+SELECT * FROM source ORDER BY A;\n+SELECT * FROM destination ORDER BY A;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed. The same scenario as above, but partition expressions inverted.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(intDiv(A, 2), intDiv(B, 2)) ORDER BY tuple();\n+CREATE TABLE destination (A Int, B Int) ENGINE = MergeTree PARTITION BY tuple(A, B) ORDER BY tuple();\n+\n+INSERT INTO TABLE source VALUES (6, 12);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '3-6' FROM source;\n+\n+SELECT * FROM source ORDER BY A;\n+SELECT * FROM destination ORDER BY A;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION (3, 6) from source;\n+\n+SELECT * FROM source ORDER BY A;\n+SELECT * FROM destination ORDER BY A;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed, it is a local operation, no different than regular attach. Replicated to replicated.\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+CREATE TABLE\n+    source(timestamp DateTime)\n+    ENGINE = ReplicatedMergeTree('/clickhouse/tables/{database}/test/source_replicated_to_replicated_distinct_expression', '1')\n+        PARTITION BY toYYYYMMDD(timestamp)\n+        ORDER BY tuple();\n+\n+CREATE TABLE\n+    destination(timestamp DateTime)\n+    ENGINE = ReplicatedMergeTree('/clickhouse/tables/{database}/test/destination_replicated_to_replicated_distinct_expression', '1')\n+        PARTITION BY toYYYYMM(timestamp)\n+        ORDER BY tuple();\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '20100302' from source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should be allowed, it is a local operation, no different than regular attach. Non replicated to replicated\n+DROP TABLE IF EXISTS source SYNC;\n+DROP TABLE IF EXISTS destination SYNC;\n+CREATE TABLE source(timestamp DateTime) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(timestamp) ORDER BY tuple();\n+\n+CREATE TABLE\n+    destination(timestamp DateTime)\n+    ENGINE = ReplicatedMergeTree('/clickhouse/tables/{database}/test/destination_non_replicated_to_replicated_distinct_expression', '1')\n+        PARTITION BY toYYYYMM(timestamp)\n+        ORDER BY tuple();\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-02 02:01:03');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '20100302' FROM source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+TRUNCATE TABLE destination;\n+\n+ALTER TABLE destination ATTACH PARTITION '20100302' from source;\n+\n+SELECT * FROM source ORDER BY timestamp;\n+SELECT * FROM destination ORDER BY timestamp;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Should not be allowed because data would be split into two different partitions\n+DROP TABLE IF EXISTS source SYNC;\n+DROP TABLE IF EXISTS destination SYNC;\n+\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01'), ('2010-03-03 02:01:03');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '201003' FROM source; -- { serverError 248 }\n+ALTER TABLE destination ATTACH PARTITION '201003' from source; -- { serverError 248 }\n+\n+-- Should not be allowed because data would be split into two different partitions\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY intDiv(A, 6);\n+\n+CREATE TABLE destination (timestamp DateTime, A Int64) engine=MergeTree ORDER BY timestamp PARTITION BY A;\n+\n+INSERT INTO TABLE source VALUES ('2010-03-02 02:01:01', 1), ('2010-03-02 02:01:03', 2);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '0' FROM source; -- { serverError 248 }\n+ALTER TABLE destination ATTACH PARTITION 0 FROM source; -- { serverError 248 }\n+\n+-- Should not be allowed because dst partition exp takes more than two arguments, so it's not considered monotonically inc\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(category);\n+CREATE TABLE destination (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY substring(category, 1, 2);\n+\n+INSERT INTO TABLE source VALUES ('spaghetti', 'food'), ('mop', 'general');\n+INSERT INTO TABLE source VALUES ('rice', 'food');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '4590ba78048910b74a47d5bfb308abed' from source; -- { serverError 36 }\n+ALTER TABLE destination ATTACH PARTITION 'food' from source; -- { serverError 36 }\n+\n+-- Should not be allowed because dst partition exp depends on a different set of columns\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(category);\n+CREATE TABLE destination (productName String, category String) engine=MergeTree ORDER BY tuple() PARTITION BY toString(productName);\n+\n+INSERT INTO TABLE source VALUES ('spaghetti', 'food'), ('mop', 'general');\n+INSERT INTO TABLE source VALUES ('rice', 'food');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '4590ba78048910b74a47d5bfb308abed' from source; -- { serverError 36 }\n+ALTER TABLE destination ATTACH PARTITION 'food' from source; -- { serverError 36 }\n+\n+-- Should not be allowed because dst partition exp is not monotonically increasing\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (productName String) engine=MergeTree ORDER BY tuple() PARTITION BY left(productName, 2);\n+CREATE TABLE destination (productName String) engine=MergeTree ORDER BY tuple() PARTITION BY cityHash64(productName);\n+\n+INSERT INTO TABLE source VALUES ('bread'), ('mop');\n+INSERT INTO TABLE source VALUES ('broccoli');\n+\n+ALTER TABLE destination ATTACH PARTITION ID '4589453b7ee96ce9de1265bd57674496' from source; -- { serverError 36 }\n+ALTER TABLE destination ATTACH PARTITION 'br' from source; -- { serverError 36 }\n+\n+-- Empty/ non-existent partition, same partition expression. Nothing should happen\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '1' FROM source;\n+ALTER TABLE destination ATTACH PARTITION 1 FROM source;\n+\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Empty/ non-existent partition, different partition expression. Nothing should happen\n+-- https://github.com/ClickHouse/ClickHouse/pull/39507#discussion_r1399839045\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+\n+ALTER TABLE destination ATTACH PARTITION ID '1' FROM source;\n+ALTER TABLE destination ATTACH PARTITION 1 FROM source;\n+\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Replace instead of attach. Empty/ non-existent partition, same partition expression. Nothing should happen\n+-- https://github.com/ClickHouse/ClickHouse/pull/39507#discussion_r1399839045\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+CREATE TABLE destination (timestamp DateTime) engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp);\n+\n+ALTER TABLE destination REPLACE PARTITION '1' FROM source;\n+\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n+\n+-- Replace instead of attach. Empty/ non-existent partition to non-empty partition, same partition id.\n+-- https://github.com/ClickHouse/ClickHouse/pull/39507#discussion_r1399839045\n+DROP TABLE IF EXISTS source;\n+DROP TABLE IF EXISTS destination;\n+\n+CREATE TABLE source (A Int) engine=MergeTree ORDER BY tuple() PARTITION BY A;\n+CREATE TABLE destination (A Int) engine=MergeTree ORDER BY tuple() PARTITION BY A;\n+\n+INSERT INTO TABLE destination VALUES (1);\n+\n+ALTER TABLE destination REPLACE PARTITION '1' FROM source;\n+\n+SELECT * FROM destination;\n+SELECT partition_id FROM system.parts where table='destination' AND database = currentDatabase() AND active = 1;\n",
  "problem_statement": "Allow to attach a part/partition from a table with more granular partitioning.\nif there are 2 table which differs only in the partitioning key and the part from source table matches single partition in target table - allow that (may be with setting like 'allow_attaching_compatible_partitioning_key_parts`).\r\n\r\n```\r\ncreate table source engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp) AS SELECT number, toDateTime('2010-01-01 00:00:00') as timestamp from numbers(100);\r\n\r\ncreate table target engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMM(timestamp) AS SELECT number, toDateTime('2010-01-01 00:00:00') as timestamp from numbers(100);\r\n\r\nALTER TABLE target ATTACH PARTITION ID '20100101' FROM source;\r\n```\r\n\r\n```\r\nCode: 36, e.displayText() = DB::Exception: Tables have different partition key (version 20.7.1.4186 (official build)) (from 127.0.0.1:39986) (in query: ALTER TABLE target ATTACH PARTITION ID '20100101' FROM source;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Exception.cpp:27: Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x123b04c0 in /usr/lib/debug/usr/bin/clickhouse\r\n1. /build/obj-x86_64-linux-gnu/../src/Common/Exception.cpp:37: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xa2a02bd in /usr/lib/debug/usr/bin/clickhouse\r\n2. /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/string:2134: DB::MergeTreeData::checkStructureAndGetMergeTreeData(DB::IStorage&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) const (.cold) @ 0xf8a32f9 in /usr/lib/debug/usr/bin/clickhouse\r\n3. /build/obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:1146: DB::StorageMergeTree::replacePartitionFrom(std::__1::shared_ptr<DB::IStorage> const&, std::__1::shared_ptr<DB::IAST> const&, bool, DB::Context const&) @ 0xf6a7100 in /usr/lib/debug/usr/bin/clickhouse\r\n4. /build/obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:1057: DB::StorageMergeTree::alterPartition(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::vector<DB::PartitionCommand, std::__1::allocator<DB::PartitionCommand> > const&, DB::Context const&) @ 0xf6aa4bf in /usr/lib/debug/usr/bin/clickhouse\r\n5. /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:662: DB::InterpreterAlterQuery::execute() @ 0xf169433 in /usr/lib/debug/usr/bin/clickhouse\r\n6. /build/obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:386: DB::executeQueryImpl(char const*, char const*, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) @ 0xf4d1bc2 in /usr/lib/debug/usr/bin/clickhouse\r\n7. /build/obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:655: DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xf4d5405 in /usr/lib/debug/usr/bin/clickhouse\r\n8. /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:253: DB::TCPHandler::runImpl() @ 0xfb39ba3 in /usr/lib/debug/usr/bin/clickhouse\r\n9. /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1215: DB::TCPHandler::run() @ 0xfb3a99e in /usr/lib/debug/usr/bin/clickhouse\r\n10. /build/obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x122ce2eb in /usr/lib/debug/usr/bin/clickhouse\r\n11. /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/atomic:856: Poco::Net::TCPServerDispatcher::run() @ 0x122ce77b in /usr/lib/debug/usr/bin/clickhouse\r\n12. /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/Mutex_POSIX.h:59: Poco::PooledThread::run() @ 0x1244d2a6 in /usr/lib/debug/usr/bin/clickhouse\r\n13. /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/AutoPtr.h:223: Poco::ThreadImpl::runnableEntry(void*) @ 0x124486a0 in /usr/lib/debug/usr/bin/clickhouse\r\n14. start_thread @ 0x9609 in /lib/x86_64-linux-gnu/libpthread-2.31.so\r\n15. /build/glibc-YYA7BZ/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: __clone @ 0x122103 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.31.so\r\n\r\n\r\nReceived exception from server (version 20.7.1):\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: Tables have different partition key. \r\n```\r\n\r\nInternally what should happen:\r\n1) we copy (by hardlinks) the part (as now)\r\n2) we recalculate min/max for the column used in partitioning key if it ends up with different partitions - fail with exception, if it's a single partitions - write new minmax file (see IMergeTreeDataPart::MinMaxIndex)\r\n3) recalc checksum file\r\n4) do rest as usual\r\n\n",
  "hints_text": "@excitoon\nOne more test:\r\n```\r\ncreate table test2_origin ( number UInt64, ts DateTime, d MATERIALIZED toDate(ts) ) Engine=MergeTree PARTITION BY d ORDER BY number;\r\ncreate table test2_target ( number UInt64, ts DateTime, d MATERIALIZED toDate(ts) ) Engine=MergeTree PARTITION BY toDate(ts) ORDER BY number;\r\nINSERT INTO test2_origin VALUES (12345, '2010-01-01 01:01:01');\r\nALTER TABLE test2_target ATTACH PARTITION '2010-01-01' FROM test2_origin;\r\n```\nAnd one more - attach to a non-partitioned table:\r\n\r\n```\r\ncreate table test3_source engine=MergeTree ORDER BY tuple() PARTITION BY toYYYYMMDD(timestamp) AS SELECT number, toDateTime('2010-01-01 00:00:00') as timestamp from numbers(100);\r\n\r\ncreate table test3_target engine=MergeTree ORDER BY tuple()  AS test3_source;\r\n\r\nALTER TABLE test3_target ATTACH PARTITION '2010-01-01' FROM test3_origin;\r\n```\nAlso it would be nice to support attaching 'offline' parts / partitions from detached folder the same way. \r\nFor REPLACE PARTITION it should not be supported (we can't replace month part with day of data, only add days one by one)\n> 1. we copy (by hardlinks) the part (as now)\r\n> 2. we recalculate min/max for the column used in partitioning key if it ends up with different partitions - fail with exception, if it's a single partitions - write new minmax file (see IMergeTreeDataPart::MinMaxIndex)\r\n> 3. recalc checksum file\r\n> 4. do rest as usual\r\n\r\nI am currently looking into this issue and exploring multiple approaches. Looking at this one, I am confused by the following:\r\n\r\nHardlinking the source part doesn't \"re-partition\" it, right? That means the part remains untouched. As far as I understand, min/max indexes represent min and max values of a part. Since the part is untouched, min and max indexes are still the same. You mention \"recalculate min/max for the column\". During my research, I came across the `MinMaxIndex::update` method. It [calls](https://github.com/ClickHouse/ClickHouse/blob/master/src/Storages/MergeTree/IMergeTreeDataPart.cpp#L164) `Column::getExtremes` to get min and max values. Which again, would result in the same values.\r\n\r\nOn the other hand, what is making sense in my head is: load source the part onto memory (all values of all partition columns) and apply the partition expression to all values. If they all end up in the same partition, it's good. Otherwise, fail with exception.\r\n\r\nAm I missing anything here?\n> load source the part onto memory (all values of all partition columns) \r\n\r\npart can be 150Gb size on the node with 8Gb. So reading that in the memory is not an option for sure. \r\n\r\nAs we discussed in chat we can limit the scope of the feature to the following scenarios:\r\n* source partitioning and the target partitioning relies on the same columns, both side use monotonic functions (so minmax not need to be recalculated), and every single part 'fits' into one partition\r\n* target table have no partitioning at all - anything can be attached if the columns set are the same.  \r\n\r\n\nsomeday maybe we can want to support also the option to recalculate minmax during that attach (in a streaming manner, w/o reading everything into ram). But it's not needed for now. \n> One more test:\r\n> \r\n> ```\r\n> create table test2_origin ( number UInt64, ts DateTime, d MATERIALIZED toDate(ts) ) Engine=MergeTree PARTITION BY d ORDER BY number;\r\n> create table test2_target ( number UInt64, ts DateTime, d MATERIALIZED toDate(ts) ) Engine=MergeTree PARTITION BY toDate(ts) ORDER BY number;\r\n> INSERT INTO test2_origin VALUES (12345, '2010-01-01 01:01:01');\r\n> ALTER TABLE test2_target ATTACH PARTITION '2010-01-01' FROM test2_origin;\r\n> ```\r\n\r\nso this one we will not support for now (because partitioning is based on different columns) ",
  "created_at": "2022-07-22T17:14:21Z"
}