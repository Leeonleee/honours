{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 15260,
  "instance_id": "ClickHouse__ClickHouse-15260",
  "issue_numbers": [
    "18691",
    "14570",
    "6802",
    "16812"
  ],
  "base_commit": "2a37f5f6878267444adef030a8d7402ff819354d",
  "patch": "diff --git a/src/Common/FileChecker.cpp b/src/Common/FileChecker.cpp\nindex 6cbec3bda77b..e7fcc8cadb79 100644\n--- a/src/Common/FileChecker.cpp\n+++ b/src/Common/FileChecker.cpp\n@@ -21,18 +21,16 @@ namespace ErrorCodes\n FileChecker::FileChecker(DiskPtr disk_, const String & file_info_path_) : disk(std::move(disk_))\n {\n     setPath(file_info_path_);\n+    load();\n }\n \n void FileChecker::setPath(const String & file_info_path_)\n {\n     files_info_path = file_info_path_;\n-\n-    tmp_files_info_path = parentPath(files_info_path) + \"tmp_\" + fileName(files_info_path);\n }\n \n void FileChecker::update(const String & full_file_path)\n {\n-    initialize();\n     map[fileName(full_file_path)] = disk->getFileSize(full_file_path);\n }\n \n@@ -41,19 +39,19 @@ void FileChecker::setEmpty(const String & full_file_path)\n     map[fileName(full_file_path)] = 0;\n }\n \n+FileChecker::Map FileChecker::getFileSizes() const\n+{\n+    return map;\n+}\n+\n CheckResults FileChecker::check() const\n {\n-    // Read the files again every time you call `check` - so as not to violate the constancy.\n-    // `check` method is rarely called.\n+    if (map.empty())\n+        return {};\n \n     CheckResults results;\n-    Map local_map;\n-    load(local_map, files_info_path);\n-\n-    if (local_map.empty())\n-        return {};\n \n-    for (const auto & name_size : local_map)\n+    for (const auto & name_size : map)\n     {\n         const String & name = name_size.first;\n         String path = parentPath(files_info_path) + name;\n@@ -97,17 +95,10 @@ void FileChecker::repair()\n     }\n }\n \n-void FileChecker::initialize()\n-{\n-    if (initialized)\n-        return;\n-\n-    load(map, files_info_path);\n-    initialized = true;\n-}\n-\n void FileChecker::save() const\n {\n+    std::string tmp_files_info_path = parentPath(files_info_path) + \"tmp_\" + fileName(files_info_path);\n+\n     {\n         std::unique_ptr<WriteBuffer> out = disk->writeFile(tmp_files_info_path);\n \n@@ -134,14 +125,14 @@ void FileChecker::save() const\n     disk->replaceFile(tmp_files_info_path, files_info_path);\n }\n \n-void FileChecker::load(Map & local_map, const String & path) const\n+void FileChecker::load()\n {\n-    local_map.clear();\n+    map.clear();\n \n-    if (!disk->exists(path))\n+    if (!disk->exists(files_info_path))\n         return;\n \n-    std::unique_ptr<ReadBuffer> in = disk->readFile(path);\n+    std::unique_ptr<ReadBuffer> in = disk->readFile(files_info_path);\n     WriteBufferFromOwnString out;\n \n     /// The JSON library does not support whitespace. We delete them. Inefficient.\n@@ -156,7 +147,7 @@ void FileChecker::load(Map & local_map, const String & path) const\n \n     JSON files = json[\"yandex\"];\n     for (const JSON file : files) // NOLINT\n-        local_map[unescapeForFileName(file.getName())] = file.getValue()[\"size\"].toUInt();\n+        map[unescapeForFileName(file.getName())] = file.getValue()[\"size\"].toUInt();\n }\n \n }\ndiff --git a/src/Common/FileChecker.h b/src/Common/FileChecker.h\nindex 015d4cadb079..73e4470f2315 100644\n--- a/src/Common/FileChecker.h\n+++ b/src/Common/FileChecker.h\n@@ -18,6 +18,7 @@ class FileChecker\n     void update(const String & full_file_path);\n     void setEmpty(const String & full_file_path);\n     void save() const;\n+    bool empty() const { return map.empty(); }\n \n     /// Check the files whose parameters are specified in sizes.json\n     CheckResults check() const;\n@@ -27,21 +28,18 @@ class FileChecker\n     /// The purpose of this function is to rollback a group of unfinished writes.\n     void repair();\n \n-private:\n     /// File name -> size.\n     using Map = std::map<String, UInt64>;\n \n-    void initialize();\n-    void updateImpl(const String & file_path);\n-    void load(Map & local_map, const String & path) const;\n+    Map getFileSizes() const;\n+\n+private:\n+    void load();\n \n     DiskPtr disk;\n     String files_info_path;\n-    String tmp_files_info_path;\n \n-    /// The data from the file is read lazily.\n     Map map;\n-    bool initialized = false;\n \n     Poco::Logger * log = &Poco::Logger::get(\"FileChecker\");\n };\ndiff --git a/src/Storages/StorageFile.cpp b/src/Storages/StorageFile.cpp\nindex 85888ee4b6a3..a5935ba3bf49 100644\n--- a/src/Storages/StorageFile.cpp\n+++ b/src/Storages/StorageFile.cpp\n@@ -52,6 +52,7 @@ namespace ErrorCodes\n     extern const int UNKNOWN_IDENTIFIER;\n     extern const int INCORRECT_FILE_NAME;\n     extern const int FILE_DOESNT_EXIST;\n+    extern const int TIMEOUT_EXCEEDED;\n     extern const int INCOMPATIBLE_COLUMNS;\n }\n \n@@ -215,6 +216,17 @@ StorageFile::StorageFile(CommonArguments args)\n     setInMemoryMetadata(storage_metadata);\n }\n \n+\n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n class StorageFileSource : public SourceWithProgress\n {\n public:\n@@ -261,7 +273,9 @@ class StorageFileSource : public SourceWithProgress\n     {\n         if (storage->use_table_fd)\n         {\n-            unique_lock = std::unique_lock(storage->rwlock);\n+            unique_lock = std::unique_lock(storage->rwlock, getLockTimeout(context));\n+            if (!unique_lock)\n+                throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n             /// We could use common ReadBuffer and WriteBuffer in storage to leverage cache\n             ///  and add ability to seek unseekable files, but cache sync isn't supported.\n@@ -280,7 +294,9 @@ class StorageFileSource : public SourceWithProgress\n         }\n         else\n         {\n-            shared_lock = std::shared_lock(storage->rwlock);\n+            shared_lock = std::shared_lock(storage->rwlock, getLockTimeout(context));\n+            if (!shared_lock)\n+                throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n         }\n     }\n \n@@ -391,8 +407,8 @@ class StorageFileSource : public SourceWithProgress\n \n     bool finished_generate = false;\n \n-    std::shared_lock<std::shared_mutex> shared_lock;\n-    std::unique_lock<std::shared_mutex> unique_lock;\n+    std::shared_lock<std::shared_timed_mutex> shared_lock;\n+    std::unique_lock<std::shared_timed_mutex> unique_lock;\n };\n \n \n@@ -450,13 +466,17 @@ class StorageFileBlockOutputStream : public IBlockOutputStream\n     explicit StorageFileBlockOutputStream(\n         StorageFile & storage_,\n         const StorageMetadataPtr & metadata_snapshot_,\n+        std::unique_lock<std::shared_timed_mutex> && lock_,\n         const CompressionMethod compression_method,\n         const Context & context,\n         const std::optional<FormatSettings> & format_settings)\n         : storage(storage_)\n         , metadata_snapshot(metadata_snapshot_)\n-        , lock(storage.rwlock)\n+        , lock(std::move(lock_))\n     {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n         std::unique_ptr<WriteBufferFromFileDescriptor> naked_buffer = nullptr;\n         if (storage.use_table_fd)\n         {\n@@ -512,7 +532,7 @@ class StorageFileBlockOutputStream : public IBlockOutputStream\n private:\n     StorageFile & storage;\n     StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n     std::unique_ptr<WriteBuffer> write_buf;\n     BlockOutputStreamPtr writer;\n     bool prefix_written{false};\n@@ -533,8 +553,12 @@ BlockOutputStreamPtr StorageFile::write(\n         Poco::File(Poco::Path(path).makeParent()).createDirectories();\n     }\n \n-    return std::make_shared<StorageFileBlockOutputStream>(*this, metadata_snapshot,\n-        chooseCompressionMethod(path, compression_method), context,\n+    return std::make_shared<StorageFileBlockOutputStream>(\n+        *this,\n+        metadata_snapshot,\n+        std::unique_lock{rwlock, getLockTimeout(context)},\n+        chooseCompressionMethod(path, compression_method),\n+        context,\n         format_settings);\n }\n \n@@ -562,8 +586,6 @@ void StorageFile::rename(const String & new_path_to_table_data, const StorageID\n     if (path_new == paths[0])\n         return;\n \n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n     Poco::File(Poco::Path(path_new).parent()).createDirectories();\n     Poco::File(paths[0]).renameTo(path_new);\n \n@@ -580,8 +602,6 @@ void StorageFile::truncate(\n     if (paths.size() != 1)\n         throw Exception(\"Can't truncate table '\" + getStorageID().getNameForLogs() + \"' in readonly mode\", ErrorCodes::DATABASE_ACCESS_DENIED);\n \n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n     if (use_table_fd)\n     {\n         if (0 != ::ftruncate(table_fd, 0))\ndiff --git a/src/Storages/StorageFile.h b/src/Storages/StorageFile.h\nindex 92287c98fc95..c316412f808c 100644\n--- a/src/Storages/StorageFile.h\n+++ b/src/Storages/StorageFile.h\n@@ -98,7 +98,7 @@ class StorageFile final : public ext::shared_ptr_helper<StorageFile>, public ISt\n     std::atomic<bool> table_fd_was_used{false}; /// To detect repeating reads from stdin\n     off_t table_fd_init_offset = -1;            /// Initial position of fd, used for repeating reads\n \n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Poco::Logger * log = &Poco::Logger::get(\"StorageFile\");\n };\ndiff --git a/src/Storages/StorageLog.cpp b/src/Storages/StorageLog.cpp\nindex 86cc6afe33fb..06e9bb8a2d6d 100644\n--- a/src/Storages/StorageLog.cpp\n+++ b/src/Storages/StorageLog.cpp\n@@ -39,6 +39,7 @@ namespace DB\n \n namespace ErrorCodes\n {\n+    extern const int TIMEOUT_EXCEEDED;\n     extern const int LOGICAL_ERROR;\n     extern const int DUPLICATE_COLUMN;\n     extern const int SIZES_OF_MARKS_FILES_ARE_INCONSISTENT;\n@@ -50,7 +51,6 @@ namespace ErrorCodes\n class LogSource final : public SourceWithProgress\n {\n public:\n-\n     static Block getHeader(const NamesAndTypesList & columns)\n     {\n         Block res;\n@@ -113,90 +113,6 @@ class LogSource final : public SourceWithProgress\n };\n \n \n-class LogBlockOutputStream final : public IBlockOutputStream\n-{\n-public:\n-    explicit LogBlockOutputStream(StorageLog & storage_, const StorageMetadataPtr & metadata_snapshot_)\n-        : storage(storage_)\n-        , metadata_snapshot(metadata_snapshot_)\n-        , lock(storage.rwlock)\n-        , marks_stream(\n-            storage.disk->writeFile(storage.marks_file_path, 4096, WriteMode::Rewrite))\n-    {\n-    }\n-\n-    ~LogBlockOutputStream() override\n-    {\n-        try\n-        {\n-            if (!done)\n-            {\n-                /// Rollback partial writes.\n-                streams.clear();\n-                storage.file_checker.repair();\n-            }\n-        }\n-        catch (...)\n-        {\n-            tryLogCurrentException(__PRETTY_FUNCTION__);\n-        }\n-    }\n-\n-    Block getHeader() const override { return metadata_snapshot->getSampleBlock(); }\n-    void write(const Block & block) override;\n-    void writeSuffix() override;\n-\n-private:\n-    StorageLog & storage;\n-    StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n-    bool done = false;\n-\n-    struct Stream\n-    {\n-        Stream(const DiskPtr & disk, const String & data_path, CompressionCodecPtr codec, size_t max_compress_block_size) :\n-            plain(disk->writeFile(data_path, max_compress_block_size, WriteMode::Append)),\n-            compressed(*plain, std::move(codec), max_compress_block_size),\n-            plain_offset(disk->getFileSize(data_path))\n-        {\n-        }\n-\n-        std::unique_ptr<WriteBuffer> plain;\n-        CompressedWriteBuffer compressed;\n-\n-        size_t plain_offset;    /// How many bytes were in the file at the time the LogBlockOutputStream was created.\n-\n-        void finalize()\n-        {\n-            compressed.next();\n-            plain->next();\n-        }\n-    };\n-\n-    using Mark = StorageLog::Mark;\n-    using MarksForColumns = std::vector<std::pair<size_t, Mark>>;\n-\n-    using FileStreams = std::map<String, Stream>;\n-    FileStreams streams;\n-\n-    using WrittenStreams = std::set<String>;\n-\n-    std::unique_ptr<WriteBuffer> marks_stream; /// Declared below `lock` to make the file open when rwlock is captured.\n-\n-    using SerializeState = IDataType::SerializeBinaryBulkStatePtr;\n-    using SerializeStates = std::map<String, SerializeState>;\n-    SerializeStates serialize_states;\n-\n-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenStreams & written_streams);\n-\n-    void writeData(const String & name, const IDataType & type, const IColumn & column,\n-        MarksForColumns & out_marks,\n-        WrittenStreams & written_streams);\n-\n-    void writeMarks(MarksForColumns && marks);\n-};\n-\n-\n Chunk LogSource::generate()\n {\n     Block res;\n@@ -204,7 +120,7 @@ Chunk LogSource::generate()\n     if (rows_read == rows_limit)\n         return {};\n \n-    if (storage.disk->isDirectoryEmpty(storage.table_path))\n+    if (storage.file_checker.empty())\n         return {};\n \n     /// How many rows to read for the next block.\n@@ -281,6 +197,101 @@ void LogSource::readData(const String & name, const IDataType & type, IColumn &\n }\n \n \n+class LogBlockOutputStream final : public IBlockOutputStream\n+{\n+public:\n+    explicit LogBlockOutputStream(\n+        StorageLog & storage_, const StorageMetadataPtr & metadata_snapshot_, std::unique_lock<std::shared_timed_mutex> && lock_)\n+        : storage(storage_)\n+        , metadata_snapshot(metadata_snapshot_)\n+        , lock(std::move(lock_))\n+        , marks_stream(\n+            storage.disk->writeFile(storage.marks_file_path, 4096, WriteMode::Rewrite))\n+    {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+        /// If there were no files, add info to rollback in case of error.\n+        if (storage.file_checker.empty())\n+        {\n+            for (const auto & file : storage.files)\n+                storage.file_checker.setEmpty(file.second.data_file_path);\n+            storage.file_checker.save();\n+        }\n+    }\n+\n+    ~LogBlockOutputStream() override\n+    {\n+        try\n+        {\n+            if (!done)\n+            {\n+                /// Rollback partial writes.\n+                streams.clear();\n+                storage.file_checker.repair();\n+            }\n+        }\n+        catch (...)\n+        {\n+            tryLogCurrentException(__PRETTY_FUNCTION__);\n+        }\n+    }\n+\n+    Block getHeader() const override { return metadata_snapshot->getSampleBlock(); }\n+    void write(const Block & block) override;\n+    void writeSuffix() override;\n+\n+private:\n+    StorageLog & storage;\n+    StorageMetadataPtr metadata_snapshot;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n+    bool done = false;\n+\n+    struct Stream\n+    {\n+        Stream(const DiskPtr & disk, const String & data_path, CompressionCodecPtr codec, size_t max_compress_block_size) :\n+            plain(disk->writeFile(data_path, max_compress_block_size, WriteMode::Append)),\n+            compressed(*plain, std::move(codec), max_compress_block_size),\n+            plain_offset(disk->getFileSize(data_path))\n+        {\n+        }\n+\n+        std::unique_ptr<WriteBuffer> plain;\n+        CompressedWriteBuffer compressed;\n+\n+        size_t plain_offset;    /// How many bytes were in the file at the time the LogBlockOutputStream was created.\n+\n+        void finalize()\n+        {\n+            compressed.next();\n+            plain->next();\n+        }\n+    };\n+\n+    using Mark = StorageLog::Mark;\n+    using MarksForColumns = std::vector<std::pair<size_t, Mark>>;\n+\n+    using FileStreams = std::map<String, Stream>;\n+    FileStreams streams;\n+\n+    using WrittenStreams = std::set<String>;\n+\n+    std::unique_ptr<WriteBuffer> marks_stream; /// Declared below `lock` to make the file open when rwlock is captured.\n+\n+    using SerializeState = IDataType::SerializeBinaryBulkStatePtr;\n+    using SerializeStates = std::map<String, SerializeState>;\n+    SerializeStates serialize_states;\n+\n+    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenStreams & written_streams);\n+\n+    void writeData(const String & name, const IDataType & type, const IColumn & column,\n+        MarksForColumns & out_marks,\n+        WrittenStreams & written_streams);\n+\n+    void writeMarks(MarksForColumns && marks);\n+};\n+\n+\n void LogBlockOutputStream::write(const Block & block)\n {\n     metadata_snapshot->check(block, true);\n@@ -474,10 +485,6 @@ StorageLog::StorageLog(\n         addFiles(column.name, *column.type);\n \n     marks_file_path = table_path + DBMS_STORAGE_LOG_MARKS_FILE_NAME;\n-\n-    if (!attach)\n-        for (const auto & file : files)\n-            file_checker.setEmpty(file.second.data_file_path);\n }\n \n \n@@ -507,9 +514,11 @@ void StorageLog::addFiles(const String & column_name, const IDataType & type)\n }\n \n \n-void StorageLog::loadMarks()\n+void StorageLog::loadMarks(std::chrono::seconds lock_timeout)\n {\n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n+    std::unique_lock lock(rwlock, lock_timeout);\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n     if (loaded_marks)\n         return;\n@@ -552,8 +561,6 @@ void StorageLog::rename(const String & new_path_to_table_data, const StorageID &\n {\n     assert(table_path != new_path_to_table_data);\n     {\n-        std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n         disk->moveDirectory(table_path, new_path_to_table_data);\n \n         table_path = new_path_to_table_data;\n@@ -569,8 +576,6 @@ void StorageLog::rename(const String & new_path_to_table_data, const StorageID &\n \n void StorageLog::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n-\n     files.clear();\n     file_count = 0;\n     loaded_marks = false;\n@@ -610,6 +615,17 @@ const StorageLog::Marks & StorageLog::getMarksWithRealRowCount(const StorageMeta\n     return it->second.marks;\n }\n \n+\n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n Pipe StorageLog::read(\n     const Names & column_names,\n     const StorageMetadataPtr & metadata_snapshot,\n@@ -620,11 +636,15 @@ Pipe StorageLog::read(\n     unsigned num_streams)\n {\n     metadata_snapshot->check(column_names, getVirtuals(), getStorageID());\n-    loadMarks();\n+\n+    auto lock_timeout = getLockTimeout(context);\n+    loadMarks(lock_timeout);\n \n     NamesAndTypesList all_columns = Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names));\n \n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, lock_timeout);\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n     Pipes pipes;\n \n@@ -653,18 +673,28 @@ Pipe StorageLog::read(\n             max_read_buffer_size));\n     }\n \n+    /// No need to hold lock while reading because we read fixed range of data that does not change while appending more data.\n     return Pipe::unitePipes(std::move(pipes));\n }\n \n-BlockOutputStreamPtr StorageLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & /*context*/)\n+BlockOutputStreamPtr StorageLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & context)\n {\n-    loadMarks();\n-    return std::make_shared<LogBlockOutputStream>(*this, metadata_snapshot);\n+    auto lock_timeout = getLockTimeout(context);\n+    loadMarks(lock_timeout);\n+\n+    std::unique_lock lock(rwlock, lock_timeout);\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+    return std::make_shared<LogBlockOutputStream>(*this, metadata_snapshot, std::move(lock));\n }\n \n-CheckResults StorageLog::checkData(const ASTPtr & /* query */, const Context & /* context */)\n+CheckResults StorageLog::checkData(const ASTPtr & /* query */, const Context & context)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n     return file_checker.check();\n }\n \ndiff --git a/src/Storages/StorageLog.h b/src/Storages/StorageLog.h\nindex 51fd334d8829..a88b6dfb6ff6 100644\n--- a/src/Storages/StorageLog.h\n+++ b/src/Storages/StorageLog.h\n@@ -84,7 +84,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor\n     DiskPtr disk;\n     String table_path;\n \n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Files files;\n \n@@ -105,7 +105,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor\n     /// Read marks files if they are not already read.\n     /// It is done lazily, so that with a large number of tables, the server starts quickly.\n     /// You can not call with a write locked `rwlock`.\n-    void loadMarks();\n+    void loadMarks(std::chrono::seconds lock_timeout);\n \n     /** For normal columns, the number of rows in the block is specified in the marks.\n       * For array columns and nested structures, there are more than one group of marks that correspond to different files\ndiff --git a/src/Storages/StorageStripeLog.cpp b/src/Storages/StorageStripeLog.cpp\nindex bc6afffddeb4..db4fbff78cd0 100644\n--- a/src/Storages/StorageStripeLog.cpp\n+++ b/src/Storages/StorageStripeLog.cpp\n@@ -44,13 +44,13 @@ namespace ErrorCodes\n {\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n     extern const int INCORRECT_FILE_NAME;\n+    extern const int TIMEOUT_EXCEEDED;\n }\n \n \n class StripeLogSource final : public SourceWithProgress\n {\n public:\n-\n     static Block getHeader(\n         StorageStripeLog & storage,\n         const StorageMetadataPtr & metadata_snapshot,\n@@ -98,6 +98,9 @@ class StripeLogSource final : public SourceWithProgress\n protected:\n     Chunk generate() override\n     {\n+        if (storage.file_checker.empty())\n+            return {};\n+\n         Block res;\n         start();\n \n@@ -154,10 +157,11 @@ class StripeLogSource final : public SourceWithProgress\n class StripeLogBlockOutputStream final : public IBlockOutputStream\n {\n public:\n-    explicit StripeLogBlockOutputStream(StorageStripeLog & storage_, const StorageMetadataPtr & metadata_snapshot_)\n+    explicit StripeLogBlockOutputStream(\n+        StorageStripeLog & storage_, const StorageMetadataPtr & metadata_snapshot_, std::unique_lock<std::shared_timed_mutex> && lock_)\n         : storage(storage_)\n         , metadata_snapshot(metadata_snapshot_)\n-        , lock(storage.rwlock)\n+        , lock(std::move(lock_))\n         , data_out_file(storage.table_path + \"data.bin\")\n         , data_out_compressed(storage.disk->writeFile(data_out_file, DBMS_DEFAULT_BUFFER_SIZE, WriteMode::Append))\n         , data_out(std::make_unique<CompressedWriteBuffer>(\n@@ -167,6 +171,15 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n         , index_out(std::make_unique<CompressedWriteBuffer>(*index_out_compressed))\n         , block_out(*data_out, 0, metadata_snapshot->getSampleBlock(), false, index_out.get(), storage.disk->getFileSize(data_out_file))\n     {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+        if (storage.file_checker.empty())\n+        {\n+            storage.file_checker.setEmpty(storage.table_path + \"data.bin\");\n+            storage.file_checker.setEmpty(storage.table_path + \"index.mrk\");\n+            storage.file_checker.save();\n+        }\n     }\n \n     ~StripeLogBlockOutputStream() override\n@@ -220,7 +233,7 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n private:\n     StorageStripeLog & storage;\n     StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n \n     String data_out_file;\n     std::unique_ptr<WriteBuffer> data_out_compressed;\n@@ -261,9 +274,6 @@ StorageStripeLog::StorageStripeLog(\n     {\n         /// create directories if they do not exist\n         disk->createDirectories(table_path);\n-\n-        file_checker.setEmpty(table_path + \"data.bin\");\n-        file_checker.setEmpty(table_path + \"index.mrk\");\n     }\n     else\n     {\n@@ -283,8 +293,6 @@ void StorageStripeLog::rename(const String & new_path_to_table_data, const Stora\n {\n     assert(table_path != new_path_to_table_data);\n     {\n-        std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n         disk->moveDirectory(table_path, new_path_to_table_data);\n \n         table_path = new_path_to_table_data;\n@@ -294,6 +302,16 @@ void StorageStripeLog::rename(const String & new_path_to_table_data, const Stora\n }\n \n \n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n Pipe StorageStripeLog::read(\n     const Names & column_names,\n     const StorageMetadataPtr & metadata_snapshot,\n@@ -303,7 +321,9 @@ Pipe StorageStripeLog::read(\n     const size_t /*max_block_size*/,\n     unsigned num_streams)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n     metadata_snapshot->check(column_names, getVirtuals(), getStorageID());\n \n@@ -342,24 +362,28 @@ Pipe StorageStripeLog::read(\n }\n \n \n-BlockOutputStreamPtr StorageStripeLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & /*context*/)\n+BlockOutputStreamPtr StorageStripeLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & context)\n {\n-    return std::make_shared<StripeLogBlockOutputStream>(*this, metadata_snapshot);\n+    std::unique_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+    return std::make_shared<StripeLogBlockOutputStream>(*this, metadata_snapshot, std::move(lock));\n }\n \n \n-CheckResults StorageStripeLog::checkData(const ASTPtr & /* query */, const Context & /* context */)\n+CheckResults StorageStripeLog::checkData(const ASTPtr & /* query */, const Context & context)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n     return file_checker.check();\n }\n \n void StorageStripeLog::truncate(const ASTPtr &, const StorageMetadataPtr &, const Context &, TableExclusiveLockHolder &)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n-\n     disk->clearDirectory(table_path);\n-\n     file_checker = FileChecker{disk, table_path + \"sizes.json\"};\n }\n \ndiff --git a/src/Storages/StorageStripeLog.h b/src/Storages/StorageStripeLog.h\nindex 1f30ddc8d8b4..5782e2526d3a 100644\n--- a/src/Storages/StorageStripeLog.h\n+++ b/src/Storages/StorageStripeLog.h\n@@ -68,7 +68,7 @@ class StorageStripeLog final : public ext::shared_ptr_helper<StorageStripeLog>,\n     size_t max_compress_block_size;\n \n     FileChecker file_checker;\n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Poco::Logger * log;\n };\ndiff --git a/src/Storages/StorageTinyLog.cpp b/src/Storages/StorageTinyLog.cpp\nindex 81eec735c8a5..fe8a25ba13bb 100644\n--- a/src/Storages/StorageTinyLog.cpp\n+++ b/src/Storages/StorageTinyLog.cpp\n@@ -13,6 +13,7 @@\n \n #include <IO/ReadBufferFromFileBase.h>\n #include <IO/WriteBufferFromFileBase.h>\n+#include <IO/LimitReadBuffer.h>\n #include <Compression/CompressionFactory.h>\n #include <Compression/CompressedReadBuffer.h>\n #include <Compression/CompressedWriteBuffer.h>\n@@ -46,6 +47,7 @@ namespace DB\n \n namespace ErrorCodes\n {\n+    extern const int TIMEOUT_EXCEEDED;\n     extern const int DUPLICATE_COLUMN;\n     extern const int INCORRECT_FILE_NAME;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n@@ -55,7 +57,6 @@ namespace ErrorCodes\n class TinyLogSource final : public SourceWithProgress\n {\n public:\n-\n     static Block getHeader(const NamesAndTypesList & columns)\n     {\n         Block res;\n@@ -66,10 +67,17 @@ class TinyLogSource final : public SourceWithProgress\n         return Nested::flatten(res);\n     }\n \n-    TinyLogSource(size_t block_size_, const NamesAndTypesList & columns_, StorageTinyLog & storage_, size_t max_read_buffer_size_)\n+    TinyLogSource(\n+        size_t block_size_,\n+        const NamesAndTypesList & columns_,\n+        StorageTinyLog & storage_,\n+        size_t max_read_buffer_size_,\n+        FileChecker::Map file_sizes_)\n         : SourceWithProgress(getHeader(columns_))\n-        , block_size(block_size_), columns(columns_), storage(storage_), lock(storage_.rwlock)\n-        , max_read_buffer_size(max_read_buffer_size_) {}\n+        , block_size(block_size_), columns(columns_), storage(storage_)\n+        , max_read_buffer_size(max_read_buffer_size_), file_sizes(std::move(file_sizes_))\n+    {\n+    }\n \n     String getName() const override { return \"TinyLog\"; }\n \n@@ -80,19 +88,21 @@ class TinyLogSource final : public SourceWithProgress\n     size_t block_size;\n     NamesAndTypesList columns;\n     StorageTinyLog & storage;\n-    std::shared_lock<std::shared_mutex> lock;\n     bool is_finished = false;\n     size_t max_read_buffer_size;\n+    FileChecker::Map file_sizes;\n \n     struct Stream\n     {\n-        Stream(const DiskPtr & disk, const String & data_path, size_t max_read_buffer_size_)\n-            : plain(disk->readFile(data_path, std::min(max_read_buffer_size_, disk->getFileSize(data_path)))),\n-            compressed(*plain)\n+        Stream(const DiskPtr & disk, const String & data_path, size_t max_read_buffer_size_, size_t file_size)\n+            : plain(file_size ? disk->readFile(data_path, std::min(max_read_buffer_size_, file_size)) : std::make_unique<ReadBuffer>(nullptr, 0)),\n+            limited(std::make_unique<LimitReadBuffer>(*plain, file_size, false)),\n+            compressed(*limited)\n         {\n         }\n \n         std::unique_ptr<ReadBuffer> plain;\n+        std::unique_ptr<ReadBuffer> limited;\n         CompressedReadBuffer compressed;\n     };\n \n@@ -107,12 +117,93 @@ class TinyLogSource final : public SourceWithProgress\n };\n \n \n+Chunk TinyLogSource::generate()\n+{\n+    Block res;\n+\n+    if (is_finished || file_sizes.empty() || (!streams.empty() && streams.begin()->second->compressed.eof()))\n+    {\n+        /** Close the files (before destroying the object).\n+          * When many sources are created, but simultaneously reading only a few of them,\n+          * buffers don't waste memory.\n+          */\n+        is_finished = true;\n+        streams.clear();\n+        return {};\n+    }\n+\n+    for (const auto & name_type : columns)\n+    {\n+        MutableColumnPtr column = name_type.type->createColumn();\n+\n+        try\n+        {\n+            readData(name_type.name, *name_type.type, *column, block_size);\n+        }\n+        catch (Exception & e)\n+        {\n+            e.addMessage(\"while reading column \" + name_type.name + \" at \" + fullPath(storage.disk, storage.table_path));\n+            throw;\n+        }\n+\n+        if (!column->empty())\n+            res.insert(ColumnWithTypeAndName(std::move(column), name_type.type, name_type.name));\n+    }\n+\n+    if (!res || streams.begin()->second->compressed.eof())\n+    {\n+        is_finished = true;\n+        streams.clear();\n+    }\n+\n+    auto flatten = Nested::flatten(res);\n+    return Chunk(flatten.getColumns(), flatten.rows());\n+}\n+\n+\n+void TinyLogSource::readData(const String & name, const IDataType & type, IColumn & column, UInt64 limit)\n+{\n+    IDataType::DeserializeBinaryBulkSettings settings; /// TODO Use avg_value_size_hint.\n+    settings.getter = [&] (const IDataType::SubstreamPath & path) -> ReadBuffer *\n+    {\n+        String stream_name = IDataType::getFileNameForStream(name, path);\n+\n+        if (!streams.count(stream_name))\n+        {\n+            String file_path = storage.files[stream_name].data_file_path;\n+            streams[stream_name] = std::make_unique<Stream>(\n+                storage.disk, file_path, max_read_buffer_size, file_sizes[fileName(file_path)]);\n+        }\n+\n+        return &streams[stream_name]->compressed;\n+    };\n+\n+    if (deserialize_states.count(name) == 0)\n+        type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n+\n+    type.deserializeBinaryBulkWithMultipleStreams(column, limit, settings, deserialize_states[name]);\n+}\n+\n+\n class TinyLogBlockOutputStream final : public IBlockOutputStream\n {\n public:\n-    explicit TinyLogBlockOutputStream(StorageTinyLog & storage_, const StorageMetadataPtr & metadata_snapshot_)\n-        : storage(storage_), metadata_snapshot(metadata_snapshot_), lock(storage_.rwlock)\n+    explicit TinyLogBlockOutputStream(\n+        StorageTinyLog & storage_,\n+        const StorageMetadataPtr & metadata_snapshot_,\n+        std::unique_lock<std::shared_timed_mutex> && lock_)\n+        : storage(storage_), metadata_snapshot(metadata_snapshot_), lock(std::move(lock_))\n     {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+        /// If there were no files, add info to rollback in case of error.\n+        if (storage.file_checker.empty())\n+        {\n+            for (const auto & file : storage.files)\n+                storage.file_checker.setEmpty(file.second.data_file_path);\n+            storage.file_checker.save();\n+        }\n     }\n \n     ~TinyLogBlockOutputStream() override\n@@ -122,6 +213,7 @@ class TinyLogBlockOutputStream final : public IBlockOutputStream\n             if (!done)\n             {\n                 /// Rollback partial writes.\n+                LOG_WARNING(storage.log, \"Rollback partial writes\");\n                 streams.clear();\n                 storage.file_checker.repair();\n             }\n@@ -140,7 +232,7 @@ class TinyLogBlockOutputStream final : public IBlockOutputStream\n private:\n     StorageTinyLog & storage;\n     StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n     bool done = false;\n \n     struct Stream\n@@ -175,74 +267,6 @@ class TinyLogBlockOutputStream final : public IBlockOutputStream\n };\n \n \n-Chunk TinyLogSource::generate()\n-{\n-    Block res;\n-\n-    if (is_finished || (!streams.empty() && streams.begin()->second->compressed.eof()))\n-    {\n-        /** Close the files (before destroying the object).\n-          * When many sources are created, but simultaneously reading only a few of them,\n-          * buffers don't waste memory.\n-          */\n-        is_finished = true;\n-        streams.clear();\n-        return {};\n-    }\n-\n-    /// if there are no files in the folder, it means that the table is empty\n-    if (storage.disk->isDirectoryEmpty(storage.table_path))\n-        return {};\n-\n-    for (const auto & name_type : columns)\n-    {\n-        MutableColumnPtr column = name_type.type->createColumn();\n-\n-        try\n-        {\n-            readData(name_type.name, *name_type.type, *column, block_size);\n-        }\n-        catch (Exception & e)\n-        {\n-            e.addMessage(\"while reading column \" + name_type.name + \" at \" + fullPath(storage.disk, storage.table_path));\n-            throw;\n-        }\n-\n-        if (!column->empty())\n-            res.insert(ColumnWithTypeAndName(std::move(column), name_type.type, name_type.name));\n-    }\n-\n-    if (!res || streams.begin()->second->compressed.eof())\n-    {\n-        is_finished = true;\n-        streams.clear();\n-    }\n-\n-    auto flatten = Nested::flatten(res);\n-    return Chunk(flatten.getColumns(), flatten.rows());\n-}\n-\n-\n-void TinyLogSource::readData(const String & name, const IDataType & type, IColumn & column, UInt64 limit)\n-{\n-    IDataType::DeserializeBinaryBulkSettings settings; /// TODO Use avg_value_size_hint.\n-    settings.getter = [&] (const IDataType::SubstreamPath & path) -> ReadBuffer *\n-    {\n-        String stream_name = IDataType::getFileNameForStream(name, path);\n-\n-        if (!streams.count(stream_name))\n-            streams[stream_name] = std::make_unique<Stream>(storage.disk, storage.files[stream_name].data_file_path, max_read_buffer_size);\n-\n-        return &streams[stream_name]->compressed;\n-    };\n-\n-    if (deserialize_states.count(name) == 0)\n-         type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n-\n-    type.deserializeBinaryBulkWithMultipleStreams(column, limit, settings, deserialize_states[name]);\n-}\n-\n-\n IDataType::OutputStreamGetter TinyLogBlockOutputStream::createStreamGetter(\n     const String & name,\n     WrittenStreams & written_streams)\n@@ -311,12 +335,12 @@ void TinyLogBlockOutputStream::writeSuffix()\n     for (auto & pair : streams)\n         column_files.push_back(storage.files[pair.first].data_file_path);\n \n+    streams.clear();\n+    done = true;\n+\n     for (const auto & file : column_files)\n         storage.file_checker.update(file);\n     storage.file_checker.save();\n-\n-    streams.clear();\n-    done = true;\n }\n \n \n@@ -377,10 +401,6 @@ StorageTinyLog::StorageTinyLog(\n \n     for (const auto & col : storage_metadata.getColumns().getAllPhysical())\n         addFiles(col.name, *col.type);\n-\n-    if (!attach)\n-        for (const auto & file : files)\n-            file_checker.setEmpty(file.second.data_file_path);\n }\n \n \n@@ -410,8 +430,6 @@ void StorageTinyLog::rename(const String & new_path_to_table_data, const Storage\n {\n     assert(table_path != new_path_to_table_data);\n     {\n-        std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n         disk->moveDirectory(table_path, new_path_to_table_data);\n \n         table_path = new_path_to_table_data;\n@@ -424,6 +442,16 @@ void StorageTinyLog::rename(const String & new_path_to_table_data, const Storage\n }\n \n \n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n Pipe StorageTinyLog::read(\n     const Names & column_names,\n     const StorageMetadataPtr & metadata_snapshot,\n@@ -437,28 +465,40 @@ Pipe StorageTinyLog::read(\n \n     // When reading, we lock the entire storage, because we only have one file\n     // per column and can't modify it concurrently.\n+    const Settings & settings = context.getSettingsRef();\n+\n+    std::shared_lock lock{rwlock, getLockTimeout(context)};\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+    /// No need to hold lock while reading because we read fixed range of data that does not change while appending more data.\n     return Pipe(std::make_shared<TinyLogSource>(\n-        max_block_size, Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names)), *this, context.getSettingsRef().max_read_buffer_size));\n+        max_block_size,\n+        Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names)),\n+        *this,\n+        settings.max_read_buffer_size,\n+        file_checker.getFileSizes()));\n }\n \n \n-BlockOutputStreamPtr StorageTinyLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & /*context*/)\n+BlockOutputStreamPtr StorageTinyLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & context)\n {\n-    return std::make_shared<TinyLogBlockOutputStream>(*this, metadata_snapshot);\n+    return std::make_shared<TinyLogBlockOutputStream>(*this, metadata_snapshot, std::unique_lock{rwlock, getLockTimeout(context)});\n }\n \n \n-CheckResults StorageTinyLog::checkData(const ASTPtr & /* query */, const Context & /* context */)\n+CheckResults StorageTinyLog::checkData(const ASTPtr & /* query */, const Context & context)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n     return file_checker.check();\n }\n \n void StorageTinyLog::truncate(\n     const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &)\n {\n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n     disk->clearDirectory(table_path);\n \n     files.clear();\n@@ -468,14 +508,6 @@ void StorageTinyLog::truncate(\n         addFiles(column.name, *column.type);\n }\n \n-void StorageTinyLog::drop()\n-{\n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-    if (disk->exists(table_path))\n-        disk->removeRecursive(table_path);\n-    files.clear();\n-}\n-\n \n void registerStorageTinyLog(StorageFactory & factory)\n {\ndiff --git a/src/Storages/StorageTinyLog.h b/src/Storages/StorageTinyLog.h\nindex 7d2b7473a218..1398af24f828 100644\n--- a/src/Storages/StorageTinyLog.h\n+++ b/src/Storages/StorageTinyLog.h\n@@ -44,8 +44,6 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ\n \n     void truncate(const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &) override;\n \n-    void drop() override;\n-\n protected:\n     StorageTinyLog(\n         DiskPtr disk_,\n@@ -71,7 +69,7 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ\n     Files files;\n \n     FileChecker file_checker;\n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Poco::Logger * log;\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01383_log_broken_table.sh b/tests/queries/0_stateless/01383_log_broken_table.sh\nindex 37cd6e239e58..5cc0f24a87f9 100755\n--- a/tests/queries/0_stateless/01383_log_broken_table.sh\n+++ b/tests/queries/0_stateless/01383_log_broken_table.sh\n@@ -25,7 +25,7 @@ function test_func()\n \n         $CLICKHOUSE_CLIENT --query \"SELECT count(), sum(x + y + z) FROM log\" > \"${CLICKHOUSE_TMP}\"/select_result 2>&1;\n \n-        grep -o -F 'File not found' \"${CLICKHOUSE_TMP}\"/select_result || cat \"${CLICKHOUSE_TMP}\"/select_result\n+        cat \"${CLICKHOUSE_TMP}\"/select_result\n \n         [[ $MAX_MEM -gt 200000000 ]] && break;\n     done\n@@ -33,9 +33,9 @@ function test_func()\n     $CLICKHOUSE_CLIENT --query \"DROP TABLE log\";\n }\n \n-test_func TinyLog | grep -v -P '^(Memory limit|0\\t0|File not found|[1-9]000000\\t)'\n-test_func StripeLog | grep -v -P '^(Memory limit|0\\t0|File not found|[1-9]000000\\t)'\n-test_func Log | grep -v -P '^(Memory limit|0\\t0|File not found|[1-9]000000\\t)'\n+test_func TinyLog | grep -v -P '^(Memory limit|0\\t0|[1-9]000000\\t)'\n+test_func StripeLog | grep -v -P '^(Memory limit|0\\t0|[1-9]000000\\t)'\n+test_func Log | grep -v -P '^(Memory limit|0\\t0|[1-9]000000\\t)'\n \n rm \"${CLICKHOUSE_TMP}/insert_result\"\n rm \"${CLICKHOUSE_TMP}/select_result\"\ndiff --git a/tests/queries/0_stateless/01499_log_deadlock.reference b/tests/queries/0_stateless/01499_log_deadlock.reference\nnew file mode 100644\nindex 000000000000..166be640db57\n--- /dev/null\n+++ b/tests/queries/0_stateless/01499_log_deadlock.reference\n@@ -0,0 +1,3 @@\n+6\n+6\n+6\ndiff --git a/tests/queries/0_stateless/01499_log_deadlock.sql b/tests/queries/0_stateless/01499_log_deadlock.sql\nnew file mode 100644\nindex 000000000000..e98b37f24554\n--- /dev/null\n+++ b/tests/queries/0_stateless/01499_log_deadlock.sql\n@@ -0,0 +1,26 @@\n+DROP TABLE IF EXISTS t;\n+CREATE TABLE t (x UInt8) ENGINE = TinyLog;\n+\n+INSERT INTO t VALUES (1), (2), (3);\n+INSERT INTO t SELECT * FROM t;\n+SELECT count() FROM t;\n+\n+DROP TABLE t;\n+\n+\n+CREATE TABLE t (x UInt8) ENGINE = Log;\n+\n+INSERT INTO t VALUES (1), (2), (3);\n+INSERT INTO t SELECT * FROM t;\n+SELECT count() FROM t;\n+\n+DROP TABLE t;\n+\n+\n+CREATE TABLE t (x UInt8) ENGINE = StripeLog;\n+\n+INSERT INTO t VALUES (1), (2), (3);\n+INSERT INTO t SELECT * FROM t;\n+SELECT count() FROM t;\n+\n+DROP TABLE t;\ndiff --git a/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference\nnew file mode 100644\nindex 000000000000..4bf85ae79f3a\n--- /dev/null\n+++ b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference\n@@ -0,0 +1,6 @@\n+Testing TinyLog\n+Done TinyLog\n+Testing StripeLog\n+Done StripeLog\n+Testing Log\n+Done Log\ndiff --git a/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh\nnew file mode 100755\nindex 000000000000..856f4c1516f5\n--- /dev/null\n+++ b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh\n@@ -0,0 +1,86 @@\n+#!/usr/bin/env bash\n+\n+set -e\n+\n+CLICKHOUSE_CLIENT_SERVER_LOGS_LEVEL=fatal\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+\n+function thread_create {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"CREATE TABLE IF NOT EXISTS $1 (x UInt64, s Array(Nullable(String))) ENGINE = $2\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|57)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_drop {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"DROP TABLE IF EXISTS $1\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|57)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_rename {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"RENAME TABLE $1 TO $2\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|57)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_select {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"SELECT * FROM $1 FORMAT Null\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|218)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_insert {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"INSERT INTO $1 SELECT rand64(1), [toString(rand64(2))] FROM numbers($2)\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|218)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_insert_select {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"INSERT INTO $1 SELECT * FROM $2\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|218)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+export -f thread_create\n+export -f thread_drop\n+export -f thread_rename\n+export -f thread_select\n+export -f thread_insert\n+export -f thread_insert_select\n+\n+\n+# Do randomized queries and expect nothing extraordinary happens.\n+\n+function test_with_engine {\n+    echo \"Testing $1\"\n+\n+    timeout 10 bash -c \"thread_create t1 $1\" &\n+    timeout 10 bash -c \"thread_create t2 $1\" &\n+    timeout 10 bash -c 'thread_drop t1' &\n+    timeout 10 bash -c 'thread_drop t2' &\n+    timeout 10 bash -c 'thread_rename t1 t2' &\n+    timeout 10 bash -c 'thread_rename t2 t1' &\n+    timeout 10 bash -c 'thread_select t1' &\n+    timeout 10 bash -c 'thread_select t2' &\n+    timeout 10 bash -c 'thread_insert t1 5' &\n+    timeout 10 bash -c 'thread_insert t2 10' &\n+    timeout 10 bash -c 'thread_insert_select t1 t2' &\n+    timeout 10 bash -c 'thread_insert_select t2 t1' &\n+\n+    wait\n+    echo \"Done $1\"\n+}\n+\n+test_with_engine TinyLog\n+test_with_engine StripeLog\n+test_with_engine Log\ndiff --git a/tests/queries/0_stateless/01505_log_distributed_deadlock.reference b/tests/queries/0_stateless/01505_log_distributed_deadlock.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/01505_log_distributed_deadlock.sql b/tests/queries/0_stateless/01505_log_distributed_deadlock.sql\nnew file mode 100644\nindex 000000000000..2b0b2b97188c\n--- /dev/null\n+++ b/tests/queries/0_stateless/01505_log_distributed_deadlock.sql\n@@ -0,0 +1,12 @@\n+DROP TABLE IF EXISTS t_local;\n+DROP TABLE IF EXISTS t_dist;\n+\n+create table t_local(a int) engine Log;\n+create table t_dist (a int) engine Distributed(test_shard_localhost, currentDatabase(), 't_local', cityHash64(a));\n+\n+set insert_distributed_sync = 1;\n+\n+insert into t_dist values (1);\n+\n+DROP TABLE t_local;\n+DROP TABLE t_dist;\ndiff --git a/tests/queries/0_stateless/01632_tinylog_read_write.reference b/tests/queries/0_stateless/01632_tinylog_read_write.reference\nnew file mode 100644\nindex 000000000000..a965a70ed4ed\n--- /dev/null\n+++ b/tests/queries/0_stateless/01632_tinylog_read_write.reference\n@@ -0,0 +1,1 @@\n+Done\ndiff --git a/tests/queries/0_stateless/01632_tinylog_read_write.sh b/tests/queries/0_stateless/01632_tinylog_read_write.sh\nnew file mode 100755\nindex 000000000000..3f41bcc59248\n--- /dev/null\n+++ b/tests/queries/0_stateless/01632_tinylog_read_write.sh\n@@ -0,0 +1,45 @@\n+#!/usr/bin/env bash\n+\n+set -e\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+\n+$CLICKHOUSE_CLIENT --multiquery --query \"DROP TABLE IF EXISTS test; CREATE TABLE IF NOT EXISTS test (x UInt64, s Array(Nullable(String))) ENGINE = TinyLog;\"\n+\n+function thread_select {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"SELECT * FROM test FORMAT Null\"\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_insert {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"INSERT INTO test VALUES (1, ['Hello'])\"\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+export -f thread_select\n+export -f thread_insert\n+\n+\n+# Do randomized queries and expect nothing extraordinary happens.\n+\n+timeout 10 bash -c 'thread_select' &\n+timeout 10 bash -c 'thread_select' &\n+timeout 10 bash -c 'thread_select' &\n+timeout 10 bash -c 'thread_select' &\n+\n+timeout 10 bash -c 'thread_insert' &\n+timeout 10 bash -c 'thread_insert' &\n+timeout 10 bash -c 'thread_insert' &\n+timeout 10 bash -c 'thread_insert' &\n+\n+wait\n+echo \"Done\"\n+\n+$CLICKHOUSE_CLIENT --multiquery --query \"DROP TABLE IF EXISTS test;\"\ndiff --git a/tests/queries/0_stateless/arcadia_skip_list.txt b/tests/queries/0_stateless/arcadia_skip_list.txt\nindex c1f7c14f58a4..c03fb52de5f4 100644\n--- a/tests/queries/0_stateless/arcadia_skip_list.txt\n+++ b/tests/queries/0_stateless/arcadia_skip_list.txt\n@@ -147,6 +147,7 @@\n 01461_query_start_time_microseconds\n 01455_shard_leaf_max_rows_bytes_to_read\n 01505_distributed_local_type_conversion_enum\n+01505_log_distributed_deadlock\n 00604_show_create_database\n 00609_mv_index_in_in\n 00510_materizlized_view_and_deduplication_zookeeper\n",
  "problem_statement": "Deadlock with TinyLog engine (20.8)\nAfter migrating 19.9 -> 20.8 the query like:\r\n\r\n```\r\nINSERT INTO tinylog \r\nSELECT FROM tab WHERE ( x IN (SELECT FROM tinylog))\r\n```\r\n\r\nLocked ( read- write operating in sub-select )\nDeadlock on sync insert into Distributed over Log\n```\r\ncreate table t_local(a int) engine Log;\r\n\r\ncreate table t_dist (a int) engine Distributed(test_shard_localhost, 'default', 't_local', cityHash64(a));\r\n\r\nset insert_distributed_sync = 1;\r\n\r\ninsert into t_dist values (1);\r\n\r\nReceived exception from server (version 20.9.1):\r\nCode: 209. DB::Exception: Received from localhost:9000. DB::Exception: Timeout exceeded while reading from socket (127.0.0.3:9000): while receiving packet from 127.0.0.3:9000: Insertion status:\r\nWrote 0 blocks and 0 rows on shard 0 replica 0, 127.0.0.2:9000 (average 300005 ms per block)\r\nWrote 0 blocks and 0 rows on shard 1 replica 0, 127.0.0.3:9000 (average 300005 ms per block)\r\nWrote 1 blocks and 0 rows on shard 2 replica 0, 127.0.0.4:9000 (average 6 ms per block)\r\n. Stack trace:\r\n```\r\n\r\nInfo from `system.stack_trace`:\r\n```\r\nWITH arrayJoin(trace) AS addr\r\nSELECT \r\n    thread_id,\r\n    query_id,\r\n    substr(concat(addressToLine(addr), '#', demangle(addressToSymbol(addr))), 1, 100)\r\nFROM system.stack_trace\r\nWHERE query_id IN \r\n(\r\n    SELECT query_id\r\n    FROM system.processes\r\n    WHERE initial_query_id = 'bf7b408d-e84f-4e52-9f30-c73c8d43d0c5'\r\n)\r\n\r\n\u250c\u2500thread_id\u2500\u252c\u2500query_id\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500substr(concat(addressToLine(addr), '#', demangle(addressToSymbol(addr))), 1, 100)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /lib/x86_64-linux-gnu/libpthread-2.31.so#pthread_cond_wait@@GLIBC_2.3.2                              \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/__threading_support:353#std::__1::__libcpp_condvar_wait(pt \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/src/condition_variable.cpp:44#std::__1::condition_variable::wait(s \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/src/shared_mutex.cpp:31#std::__1::__shared_mutex_base::lock()      \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/shared_mutex:190#std::__1::shared_mutex::lock()            \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/__mutex_base:119#std::__1::unique_lock<std::__1::shared_mu \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Storages/StorageLog.cpp:0#DB::StorageLog::loadMarks()                         \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Storages/StorageLog.cpp:657#DB::StorageLog::write(std::__1::shared_ptr<DB::IA \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/DataStreams/PushingToViewsBlockOutputStream.cpp:122#DB::PushingToViewsBlockOu \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/memory:2214#std::__1::__compressed_pair_elem<DB::PushingTo \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/memory:2299#std::__1::__compressed_pair<std::__1::allocato \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/memory:3569#std::__1::__shared_ptr_emplace<DB::PushingToVi \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/memory:4400#std::__1::enable_if<!(is_array<DB::PushingToVi \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Interpreters/InterpreterInsertQuery.cpp:313#DB::InterpreterInsertQuery::execu \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Storages/Distributed/DistributedBlockOutputStream.cpp:340#DB::DistributedBloc \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/type_traits:3519#decltype(std::__1::forward<DB::Distribute \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/__functional_base:349#void std::__1::__invoke_void_return_ \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/functional:1540#std::__1::__function::__alloc_func<DB::Dis \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/functional:1714#std::__1::__function::__func<DB::Distribut \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/functional:1867#std::__1::__function::__value_func<void () \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/functional:2473#std::__1::function<void ()>::operator()()  \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Common/ThreadPool.cpp:235#ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::_ \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Common/ThreadPool.cpp:116#void ThreadPoolImpl<ThreadFromGlobalPool>::schedule \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/type_traits:3525#decltype(std::__1::forward<void>(fp)(std: \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/tuple:1415#decltype(auto) std::__1::__apply_tuple_impl<voi \u2502\r\n\u2502    268738 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/tuple:1424#decltype(auto) std::__1::apply<void ThreadPoolI \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /lib/x86_64-linux-gnu/libpthread-2.31.so#pthread_cond_wait@@GLIBC_2.3.2                              \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/__threading_support:353#std::__1::__libcpp_condvar_wait(pt \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/src/condition_variable.cpp:44#std::__1::condition_variable::wait(s \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/libcxx/include/__mutex_base:408#void std::__1::condition_variable::wait<T \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Common/ThreadPool.cpp:160#ThreadPoolImpl<ThreadFromGlobalPool>::wait()        \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Storages/Distributed/DistributedBlockOutputStream.cpp:226#DB::DistributedBloc \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Storages/Distributed/DistributedBlockOutputStream.cpp:407#DB::DistributedBloc \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Storages/Distributed/DistributedBlockOutputStream.cpp:134#DB::DistributedBloc \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/DataStreams/PushingToViewsBlockOutputStream.cpp:0#DB::PushingToViewsBlockOutp \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/DataStreams/AddingDefaultBlockOutputStream.cpp:10#DB::AddingDefaultBlockOutpu \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/DataStreams/CountingBlockOutputStream.cpp:19#DB::CountingBlockOutputStream::w \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Server/TCPHandler.cpp:979#DB::TCPHandler::receiveData(bool)                   \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Server/TCPHandler.cpp:814#DB::TCPHandler::receivePacket()                     \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Server/TCPHandler.cpp:441#DB::TCPHandler::readDataNext(unsigned long const&,  \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Server/TCPHandler.cpp:472#DB::TCPHandler::readData(DB::Settings const&)       \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Server/TCPHandler.cpp:501#DB::TCPHandler::processInsertQuery(DB::Settings con \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Server/TCPHandler.cpp:263#DB::TCPHandler::runImpl()                           \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/src/Server/TCPHandler.cpp:1217#DB::TCPHandler::run()                              \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/poco/Net/src/TCPServerConnection.cpp:43#Poco::Net::TCPServerConnection::s \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/poco/Net/src/TCPServerDispatcher.cpp:114#Poco::Net::TCPServerDispatcher:: \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/poco/Foundation/src/ThreadPool.cpp:199#Poco::PooledThread::run()          \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/poco/Foundation/src/Thread.cpp:56#Poco::(anonymous namespace)::RunnableHo \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /home/akuzm/ch1/ch/contrib/poco/Foundation/src/Thread_POSIX.cpp:345#Poco::ThreadImpl::runnableEntry( \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /lib/x86_64-linux-gnu/libpthread-2.31.so#start_thread                                                \u2502\r\n\u2502    269092 \u2502 bf7b408d-e84f-4e52-9f30-c73c8d43d0c5 \u2502 /build/glibc-YYA7BZ/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97#clone               \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n51 rows in set. Elapsed: 0.142 sec. \r\n\r\n```\nFix insert select deadlock for log family engine (former PR #5097)\nI hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nFor changelog. Remove if this is non-significant change.\r\n\r\nCategory (leave one):\r\n- Bug Fix\r\n\r\nResuming PR #5097 originally started by @zhang2014 \r\n\n2 MV writing to same target with engine=Log can lead to deadlock\nIt's actually a footgun and rather expected, but still better to try avoid those deadlock when possible. \r\n\r\n```\r\ncreate table src1 engine=Null as system.numbers;\r\ncreate table dst1 engine=Log as system.numbers;\r\ncreate materialized view a1 to dst1 as select * from src1;\r\ncreate materialized view b1 to dst1 as select * from src1;\r\ninsert into src1 values (1);\r\n```\r\n\r\nmaybe we should use that new rw lock with timeouts here? \n",
  "hints_text": "\n\n@Akazz \r\nHow are the questions listed here for https://github.com/yandex/ClickHouse/pull/5097#issuecomment-524422831 considered? Sorry, I haven't replied since I haven't come up with a good idea\n> @Akazz\r\n> How are the questions listed here for [#5097 (comment)](https://github.com/yandex/ClickHouse/pull/5097#issuecomment-524422831) considered?\r\n\r\nWe decided to proceed with your PR because it fixed deadlock scenarios as much as for Log + StripeLog engines. Commits are to follow soon\r\n\r\n> Sorry, I haven't replied since I haven't come up with a good idea\r\n\r\nNo problem. Don't worry about that!\n@Akazz This is still to do. And only a few steps remain.\n@Akazz ?\nI am going to pick this up _asap_!\r\n\r\n> @Akazz This is still to do. And only a few steps remain.\r\n\n\"ASAP\" is going to be nearest days.\n@Akazz It's unclear what you are going to do with this PR.\n",
  "created_at": "2020-09-24T23:30:20Z",
  "modified_files": [
    "src/Common/FileChecker.cpp",
    "src/Common/FileChecker.h",
    "src/Storages/StorageFile.cpp",
    "src/Storages/StorageFile.h",
    "src/Storages/StorageLog.cpp",
    "src/Storages/StorageLog.h",
    "src/Storages/StorageStripeLog.cpp",
    "src/Storages/StorageStripeLog.h",
    "src/Storages/StorageTinyLog.cpp",
    "src/Storages/StorageTinyLog.h"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/01383_log_broken_table.sh",
    "b/tests/queries/0_stateless/01499_log_deadlock.reference",
    "b/tests/queries/0_stateless/01499_log_deadlock.sql",
    "b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference",
    "b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh",
    "b/tests/queries/0_stateless/01505_log_distributed_deadlock.sql",
    "b/tests/queries/0_stateless/01632_tinylog_read_write.reference",
    "b/tests/queries/0_stateless/01632_tinylog_read_write.sh",
    "tests/queries/0_stateless/arcadia_skip_list.txt"
  ]
}