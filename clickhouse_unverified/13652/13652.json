{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 13652,
  "instance_id": "ClickHouse__ClickHouse-13652",
  "issue_numbers": [
    "13458"
  ],
  "base_commit": "031a12ec3f085c77b08e2b8f669b81957b2cdb9f",
  "patch": "diff --git a/docs/en/sql-reference/statements/attach.md b/docs/en/sql-reference/statements/attach.md\nindex 01783e9cb2fe..bebba01980e8 100644\n--- a/docs/en/sql-reference/statements/attach.md\n+++ b/docs/en/sql-reference/statements/attach.md\n@@ -56,4 +56,4 @@ Result:\n ATTACH TABLE name UUID '<uuid>' (col1 Type1, ...)\n ```\n \n-It creates new table with provided structure and attaches data from table with the specified UUID.\n\\ No newline at end of file\n+It creates new table with provided structure and attaches data from table with the specified UUID.\ndiff --git a/docs/en/sql-reference/statements/system.md b/docs/en/sql-reference/statements/system.md\nindex a17c87d23262..1708d594641b 100644\n--- a/docs/en/sql-reference/statements/system.md\n+++ b/docs/en/sql-reference/statements/system.md\n@@ -38,6 +38,7 @@ The list of available `SYSTEM` statements:\n -   [START REPLICATION QUEUES](#query_language-system-start-replication-queues)\n -   [SYNC REPLICA](#query_language-system-sync-replica)\n -   [RESTART REPLICA](#query_language-system-restart-replica)\n+-   [RESTORE REPLICA](#query_language-system-restore-replica)\n -   [RESTART REPLICAS](#query_language-system-restart-replicas)\n \n ## RELOAD EMBEDDED DICTIONARIES {#query_language-system-reload-emdedded-dictionaries}\n@@ -290,13 +291,60 @@ After running this statement the `[db.]replicated_merge_tree_family_table_name`\n \n ### RESTART REPLICA {#query_language-system-restart-replica}\n \n-Provides possibility to reinitialize Zookeeper sessions state for `ReplicatedMergeTree` table, will compare current state with Zookeeper as source of true and add tasks to Zookeeper queue if needed\n-Initialization replication quene based on ZooKeeper date happens in the same way as `ATTACH TABLE` statement. For a short time the table will be unavailable for any operations.\n+Provides possibility to reinitialize Zookeeper sessions state for `ReplicatedMergeTree` table, will compare current state with Zookeeper as source of true and add tasks to Zookeeper queue if needed.\n+Initialization replication queue based on ZooKeeper date happens in the same way as `ATTACH TABLE` statement. For a short time the table will be unavailable for any operations.\n \n ``` sql\n SYSTEM RESTART REPLICA [db.]replicated_merge_tree_family_table_name\n ```\n \n+### RESTORE REPLICA {#query_language-system-restore-replica}\n+\n+Restores a replica if data is [possibly] present but Zookeeper metadata is lost.\n+\n+Works only on readonly `ReplicatedMergeTree` tables.\n+\n+One may execute query after:\n+\n+  - ZooKeeper root `/` loss.\n+  - Replicas path `/replicas` loss.\n+  - Individual replica path `/replicas/replica_name/` loss.\n+\n+Replica attaches locally found parts and sends info about them to Zookeeper.\n+Parts present on replica before metadata loss are not re-fetched from other replicas if not being outdated\n+(so replica restoration does not mean re-downloading all data over the network).\n+\n+Caveat: parts in all states are moved to `detached/` folder. Parts active before data loss (Committed) are attached.\n+\n+#### Syntax\n+\n+```sql\n+SYSTEM RESTORE REPLICA [db.]replicated_merge_tree_family_table_name [ON CLUSTER cluster_name]\n+```\n+\n+Alternative syntax:\n+\n+```sql\n+SYSTEM RESTORE REPLICA [ON CLUSTER cluster_name] [db.]replicated_merge_tree_family_table_name\n+```\n+\n+#### Example\n+\n+```sql\n+-- Creating table on multiple servers\n+\n+CREATE TABLE test(n UInt32)\n+ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/', '{replica}')\n+ORDER BY n PARTITION BY n % 10;\n+\n+INSERT INTO test SELECT * FROM numbers(1000);\n+\n+-- zookeeper_delete_path(\"/clickhouse/tables/test\", recursive=True) <- root loss.\n+\n+SYSTEM RESTART REPLICA test; -- Table will attach as readonly as metadata is missing.\n+SYSTEM RESTORE REPLICA test; -- Need to execute on every replica, another way: RESTORE REPLICA test ON CLUSTER cluster\n+```\n+\n ### RESTART REPLICAS {#query_language-system-restart-replicas}\n \n Provides possibility to reinitialize Zookeeper sessions state for all `ReplicatedMergeTree` tables, will compare current state with Zookeeper as source of true and add tasks to Zookeeper queue if needed\ndiff --git a/programs/CMakeLists.txt b/programs/CMakeLists.txt\nindex 12aec76a303f..7f85a3fc3d72 100644\n--- a/programs/CMakeLists.txt\n+++ b/programs/CMakeLists.txt\n@@ -33,6 +33,7 @@ option (ENABLE_CLICKHOUSE_OBFUSCATOR \"Table data obfuscator (convert real data t\n     ${ENABLE_CLICKHOUSE_ALL})\n \n # https://clickhouse.tech/docs/en/operations/utilities/odbc-bridge/\n+# TODO Also needs NANODBC.\n if (ENABLE_ODBC)\n     option (ENABLE_CLICKHOUSE_ODBC_BRIDGE \"HTTP-server working like a proxy to ODBC driver\"\n         ${ENABLE_CLICKHOUSE_ALL})\ndiff --git a/src/Access/AccessType.h b/src/Access/AccessType.h\nindex cef2de12b302..0e2959853031 100644\n--- a/src/Access/AccessType.h\n+++ b/src/Access/AccessType.h\n@@ -154,6 +154,7 @@ enum class AccessType\n     M(SYSTEM_DROP_REPLICA, \"DROP REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_SYNC_REPLICA, \"SYNC REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_RESTART_REPLICA, \"RESTART REPLICA\", TABLE, SYSTEM) \\\n+    M(SYSTEM_RESTORE_REPLICA, \"RESTORE REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_FLUSH_DISTRIBUTED, \"FLUSH DISTRIBUTED\", TABLE, SYSTEM_FLUSH) \\\n     M(SYSTEM_FLUSH_LOGS, \"FLUSH LOGS\", GLOBAL, SYSTEM_FLUSH) \\\n     M(SYSTEM_FLUSH, \"\", GROUP, SYSTEM) \\\ndiff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp\nindex d840830bf28e..be26997d8ff8 100644\n--- a/src/Common/ErrorCodes.cpp\n+++ b/src/Common/ErrorCodes.cpp\n@@ -554,6 +554,7 @@\n     M(584, PROJECTION_NOT_USED) \\\n     M(585, CANNOT_PARSE_YAML) \\\n     M(586, CANNOT_CREATE_FILE) \\\n+    M(587, CONCURRENT_ACCESS_NOT_SUPPORTED) \\\n     \\\n     M(998, POSTGRESQL_CONNECTION_FAILURE) \\\n     M(999, KEEPER_EXCEPTION) \\\ndiff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp\nindex ee208631c9be..28d88bdd8df9 100644\n--- a/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -831,14 +831,17 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)\n     if (create.attach && !create.storage && !create.columns_list)\n     {\n         auto database = DatabaseCatalog::instance().getDatabase(database_name);\n+\n         if (database->getEngineName() == \"Replicated\")\n         {\n             auto guard = DatabaseCatalog::instance().getDDLGuard(database_name, create.table);\n-            if (typeid_cast<DatabaseReplicated *>(database.get()) && getContext()->getClientInfo().query_kind != ClientInfo::QueryKind::SECONDARY_QUERY)\n+\n+            if (auto* ptr = typeid_cast<DatabaseReplicated *>(database.get());\n+                ptr && getContext()->getClientInfo().query_kind != ClientInfo::QueryKind::SECONDARY_QUERY)\n             {\n                 create.database = database_name;\n                 guard->releaseTableLock();\n-                return typeid_cast<DatabaseReplicated *>(database.get())->tryEnqueueReplicatedDDL(query_ptr, getContext());\n+                return ptr->tryEnqueueReplicatedDDL(query_ptr, getContext());\n             }\n         }\n \n@@ -926,11 +929,13 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)\n     if (need_add_to_database && database->getEngineName() == \"Replicated\")\n     {\n         auto guard = DatabaseCatalog::instance().getDDLGuard(create.database, create.table);\n-        if (typeid_cast<DatabaseReplicated *>(database.get()) && getContext()->getClientInfo().query_kind != ClientInfo::QueryKind::SECONDARY_QUERY)\n+\n+        if (auto * ptr = typeid_cast<DatabaseReplicated *>(database.get());\n+            ptr && getContext()->getClientInfo().query_kind != ClientInfo::QueryKind::SECONDARY_QUERY)\n         {\n             assertOrSetUUID(create, database);\n             guard->releaseTableLock();\n-            return typeid_cast<DatabaseReplicated *>(database.get())->tryEnqueueReplicatedDDL(query_ptr, getContext());\n+            return ptr->tryEnqueueReplicatedDDL(query_ptr, getContext());\n         }\n     }\n \n@@ -992,8 +997,10 @@ bool InterpreterCreateQuery::doCreateTable(ASTCreateQuery & create,\n         }\n \n         data_path = database->getTableDataPath(create);\n+\n         if (!create.attach && !data_path.empty() && fs::exists(fs::path{getContext()->getPath()} / data_path))\n-            throw Exception(storage_already_exists_error_code, \"Directory for {} data {} already exists\", Poco::toLower(storage_name), String(data_path));\n+            throw Exception(storage_already_exists_error_code,\n+                \"Directory for {} data {} already exists\", Poco::toLower(storage_name), String(data_path));\n     }\n     else\n     {\ndiff --git a/src/Interpreters/InterpreterInsertQuery.cpp b/src/Interpreters/InterpreterInsertQuery.cpp\nindex 225bf9ec6514..4d9e293d7629 100644\n--- a/src/Interpreters/InterpreterInsertQuery.cpp\n+++ b/src/Interpreters/InterpreterInsertQuery.cpp\n@@ -189,12 +189,11 @@ BlockIO InterpreterInsertQuery::execute()\n                 const auto & union_modes = select_query.list_of_modes;\n \n                 /// ASTSelectWithUnionQuery is not normalized now, so it may pass some queries which can be Trivial select queries\n-                is_trivial_insert_select\n-                    = std::all_of(\n-                          union_modes.begin(),\n-                          union_modes.end(),\n-                          [](const ASTSelectWithUnionQuery::Mode & mode) { return mode == ASTSelectWithUnionQuery::Mode::ALL; })\n-                    && std::all_of(selects.begin(), selects.end(), [](const ASTPtr & select) { return isTrivialSelect(select); });\n+                const auto mode_is_all = [](const auto & mode) { return mode == ASTSelectWithUnionQuery::Mode::ALL; };\n+\n+                is_trivial_insert_select =\n+                    std::all_of(union_modes.begin(), union_modes.end(), std::move(mode_is_all))\n+                    && std::all_of(selects.begin(), selects.end(), isTrivialSelect);\n             }\n \n             if (is_trivial_insert_select)\ndiff --git a/src/Interpreters/InterpreterSystemQuery.cpp b/src/Interpreters/InterpreterSystemQuery.cpp\nindex f97001883bde..f76d51e765b3 100644\n--- a/src/Interpreters/InterpreterSystemQuery.cpp\n+++ b/src/Interpreters/InterpreterSystemQuery.cpp\n@@ -43,11 +43,8 @@\n #    include \"config_core.h\"\n #endif\n \n-\n namespace DB\n {\n-\n-\n namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n@@ -56,6 +53,7 @@ namespace ErrorCodes\n     extern const int NOT_IMPLEMENTED;\n     extern const int TIMEOUT_EXCEEDED;\n     extern const int TABLE_WAS_NOT_DROPPED;\n+    extern const int NO_ZOOKEEPER;\n }\n \n \n@@ -131,6 +129,8 @@ AccessType getRequiredAccessType(StorageActionBlockType action_type)\n         throw Exception(\"Unknown action type: \" + std::to_string(action_type), ErrorCodes::LOGICAL_ERROR);\n }\n \n+constexpr std::string_view table_is_not_replicated = \"Table {} is not replicated\";\n+\n }\n \n /// Implements SYSTEM [START|STOP] <something action from ActionLocks>\n@@ -212,11 +212,16 @@ BlockIO InterpreterSystemQuery::execute()\n     system_context->setSetting(\"profile\", getContext()->getSystemProfileName());\n \n     /// Make canonical query for simpler processing\n-    if (!query.table.empty())\n+    if (query.type == Type::RELOAD_DICTIONARY)\n+    {\n+        if (!query.database.empty())\n+            query.table = query.database + \".\" + query.table;\n+    }\n+    else if (!query.table.empty())\n+    {\n         table_id = getContext()->resolveStorageID(StorageID(query.database, query.table), Context::ResolveOrdinary);\n+    }\n \n-    if (!query.target_dictionary.empty() && !query.database.empty())\n-        query.target_dictionary = query.database + \".\" + query.target_dictionary;\n \n     volume_ptr = {};\n     if (!query.storage_policy.empty() && !query.volume.empty())\n@@ -286,7 +291,7 @@ BlockIO InterpreterSystemQuery::execute()\n             getContext()->checkAccess(AccessType::SYSTEM_RELOAD_DICTIONARY);\n \n             auto & external_dictionaries_loader = system_context->getExternalDictionariesLoader();\n-            external_dictionaries_loader.reloadDictionary(query.target_dictionary, getContext());\n+            external_dictionaries_loader.reloadDictionary(query.table, getContext());\n \n \n             ExternalDictionariesLoader::resetAll();\n@@ -296,8 +301,8 @@ BlockIO InterpreterSystemQuery::execute()\n         {\n             getContext()->checkAccess(AccessType::SYSTEM_RELOAD_DICTIONARY);\n             executeCommandsAndThrowIfError(\n-                    [&] () { system_context->getExternalDictionariesLoader().reloadAllTriedToLoad(); },\n-                    [&] () { system_context->getEmbeddedDictionaries().reload(); }\n+                [&] { system_context->getExternalDictionariesLoader().reloadAllTriedToLoad(); },\n+                [&] { system_context->getEmbeddedDictionaries().reload(); }\n             );\n             ExternalDictionariesLoader::resetAll();\n             break;\n@@ -392,8 +397,10 @@ BlockIO InterpreterSystemQuery::execute()\n             break;\n         case Type::RESTART_REPLICA:\n             if (!tryRestartReplica(table_id, system_context))\n-                throw Exception(\"There is no \" + query.database + \".\" + query.table + \" replicated table\",\n-                                ErrorCodes::BAD_ARGUMENTS);\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, table_is_not_replicated.data(), table_id.getNameForLogs());\n+            break;\n+        case Type::RESTORE_REPLICA:\n+            restoreReplica();\n             break;\n         case Type::RESTART_DISK:\n             restartDisk(query.disk);\n@@ -402,14 +409,14 @@ BlockIO InterpreterSystemQuery::execute()\n         {\n             getContext()->checkAccess(AccessType::SYSTEM_FLUSH_LOGS);\n             executeCommandsAndThrowIfError(\n-                    [&] () { if (auto query_log = getContext()->getQueryLog()) query_log->flush(true); },\n-                    [&] () { if (auto part_log = getContext()->getPartLog(\"\")) part_log->flush(true); },\n-                    [&] () { if (auto query_thread_log = getContext()->getQueryThreadLog()) query_thread_log->flush(true); },\n-                    [&] () { if (auto trace_log = getContext()->getTraceLog()) trace_log->flush(true); },\n-                    [&] () { if (auto text_log = getContext()->getTextLog()) text_log->flush(true); },\n-                    [&] () { if (auto metric_log = getContext()->getMetricLog()) metric_log->flush(true); },\n-                    [&] () { if (auto asynchronous_metric_log = getContext()->getAsynchronousMetricLog()) asynchronous_metric_log->flush(true); },\n-                    [&] () { if (auto opentelemetry_span_log = getContext()->getOpenTelemetrySpanLog()) opentelemetry_span_log->flush(true); }\n+                [&] { if (auto query_log = getContext()->getQueryLog()) query_log->flush(true); },\n+                [&] { if (auto part_log = getContext()->getPartLog(\"\")) part_log->flush(true); },\n+                [&] { if (auto query_thread_log = getContext()->getQueryThreadLog()) query_thread_log->flush(true); },\n+                [&] { if (auto trace_log = getContext()->getTraceLog()) trace_log->flush(true); },\n+                [&] { if (auto text_log = getContext()->getTextLog()) text_log->flush(true); },\n+                [&] { if (auto metric_log = getContext()->getMetricLog()) metric_log->flush(true); },\n+                [&] { if (auto asynchronous_metric_log = getContext()->getAsynchronousMetricLog()) asynchronous_metric_log->flush(true); },\n+                [&] { if (auto opentelemetry_span_log = getContext()->getOpenTelemetrySpanLog()) opentelemetry_span_log->flush(true); }\n             );\n             break;\n         }\n@@ -423,12 +430,51 @@ BlockIO InterpreterSystemQuery::execute()\n     return BlockIO();\n }\n \n+void InterpreterSystemQuery::restoreReplica()\n+{\n+    getContext()->checkAccess(AccessType::SYSTEM_RESTORE_REPLICA, table_id);\n+\n+    const zkutil::ZooKeeperPtr& zookeeper = getContext()->getZooKeeper();\n+\n+    if (zookeeper->expired())\n+        throw Exception(ErrorCodes::NO_ZOOKEEPER,\n+            \"Cannot restore table metadata because ZooKeeper session has expired\");\n+\n+    const StoragePtr table_ptr = DatabaseCatalog::instance().getTable(table_id, getContext());\n+\n+    auto * const table_replicated_ptr = dynamic_cast<StorageReplicatedMergeTree *>(table_ptr.get());\n+\n+    if (table_replicated_ptr == nullptr)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS, table_is_not_replicated.data(), table_id.getNameForLogs());\n+\n+    auto & table_replicated = *table_replicated_ptr;\n+\n+    StorageReplicatedMergeTree::Status status;\n+    table_replicated.getStatus(status);\n+\n+    if (!status.is_readonly)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Replica must be readonly\");\n+\n+    const String replica_name = table_replicated.getReplicaName();\n+    const String& zk_root_path = status.zookeeper_path;\n+\n+    if (String replica_path = zk_root_path + \"replicas/\" + replica_name; zookeeper->exists(replica_path))\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"Replica path is present at {} -- nothing to restore. \"\n+            \"If you are sure that metadata it lost and replica path contain some garbage, \"\n+            \"then use SYSTEM DROP REPLICA query first.\", replica_path);\n+\n+    table_replicated.restoreMetadataInZooKeeper();\n+}\n \n StoragePtr InterpreterSystemQuery::tryRestartReplica(const StorageID & replica, ContextMutablePtr system_context, bool need_ddl_guard)\n {\n     getContext()->checkAccess(AccessType::SYSTEM_RESTART_REPLICA, replica);\n \n-    auto table_ddl_guard = need_ddl_guard ? DatabaseCatalog::instance().getDDLGuard(replica.getDatabaseName(), replica.getTableName()) : nullptr;\n+    auto table_ddl_guard = need_ddl_guard\n+        ? DatabaseCatalog::instance().getDDLGuard(replica.getDatabaseName(), replica.getTableName())\n+        : nullptr;\n+\n     auto [database, table] = DatabaseCatalog::instance().tryGetDatabaseAndTable(replica, getContext());\n     ASTPtr create_ast;\n \n@@ -475,28 +521,23 @@ void InterpreterSystemQuery::restartReplicas(ContextMutablePtr system_context)\n     auto & catalog = DatabaseCatalog::instance();\n \n     for (auto & elem : catalog.getDatabases())\n-    {\n-        DatabasePtr & database = elem.second;\n-        for (auto iterator = database->getTablesIterator(getContext()); iterator->isValid(); iterator->next())\n-        {\n-            if (auto table = iterator->table())\n-            {\n-                if (dynamic_cast<const StorageReplicatedMergeTree *>(table.get()))\n-                    replica_names.emplace_back(StorageID{iterator->databaseName(), iterator->name()});\n-            }\n-        }\n-    }\n+        for (auto it = elem.second->getTablesIterator(getContext()); it->isValid(); it->next())\n+            if (dynamic_cast<const StorageReplicatedMergeTree *>(it->table().get()))\n+                replica_names.emplace_back(it->databaseName(), it->name());\n \n     if (replica_names.empty())\n         return;\n \n     TableGuards guards;\n+\n     for (const auto & name : replica_names)\n         guards.emplace(UniqueTableName{name.database_name, name.table_name}, nullptr);\n+\n     for (auto & guard : guards)\n         guard.second = catalog.getDDLGuard(guard.first.database_name, guard.first.table_name);\n \n     ThreadPool pool(std::min(size_t(getNumberOfPhysicalCPUCores()), replica_names.size()));\n+\n     for (auto & replica : replica_names)\n     {\n         LOG_TRACE(log, \"Restarting replica on {}\", replica.getNameForLogs());\n@@ -516,7 +557,7 @@ void InterpreterSystemQuery::dropReplica(ASTSystemQuery & query)\n         StoragePtr table = DatabaseCatalog::instance().getTable(table_id, getContext());\n \n         if (!dropReplicaImpl(query, table))\n-            throw Exception(\"Table \" + table_id.getNameForLogs() + \" is not replicated\", ErrorCodes::BAD_ARGUMENTS);\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, table_is_not_replicated.data(), table_id.getNameForLogs());\n     }\n     else if (!query.database.empty())\n     {\n@@ -628,7 +669,7 @@ void InterpreterSystemQuery::syncReplica(ASTSystemQuery &)\n         LOG_TRACE(log, \"SYNC REPLICA {}: OK\", table_id.getNameForLogs());\n     }\n     else\n-        throw Exception(\"Table \" + table_id.getNameForLogs() + \" is not replicated\", ErrorCodes::BAD_ARGUMENTS);\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS, table_is_not_replicated.data(), table_id.getNameForLogs());\n }\n \n void InterpreterSystemQuery::flushDistributed(ASTSystemQuery &)\n@@ -659,6 +700,7 @@ AccessRightsElements InterpreterSystemQuery::getRequiredAccessForDDLOnCluster()\n     const auto & query = query_ptr->as<const ASTSystemQuery &>();\n     using Type = ASTSystemQuery::Type;\n     AccessRightsElements required_access;\n+\n     switch (query.type)\n     {\n         case Type::SHUTDOWN: [[fallthrough]];\n@@ -770,6 +812,11 @@ AccessRightsElements InterpreterSystemQuery::getRequiredAccessForDDLOnCluster()\n             required_access.emplace_back(AccessType::SYSTEM_DROP_REPLICA, query.database, query.table);\n             break;\n         }\n+        case Type::RESTORE_REPLICA:\n+        {\n+            required_access.emplace_back(AccessType::SYSTEM_RESTORE_REPLICA, query.database, query.table);\n+            break;\n+        }\n         case Type::SYNC_REPLICA:\n         {\n             required_access.emplace_back(AccessType::SYSTEM_SYNC_REPLICA, query.database, query.table);\ndiff --git a/src/Interpreters/InterpreterSystemQuery.h b/src/Interpreters/InterpreterSystemQuery.h\nindex 297f7225a92f..6d1ad78a991c 100644\n--- a/src/Interpreters/InterpreterSystemQuery.h\n+++ b/src/Interpreters/InterpreterSystemQuery.h\n@@ -49,6 +49,9 @@ class InterpreterSystemQuery : public IInterpreter, WithMutableContext\n \n     void restartReplicas(ContextMutablePtr system_context);\n     void syncReplica(ASTSystemQuery & query);\n+\n+    void restoreReplica();\n+\n     void dropReplica(ASTSystemQuery & query);\n     bool dropReplicaImpl(ASTSystemQuery & query, const StoragePtr & table);\n     void flushDistributed(ASTSystemQuery & query);\ndiff --git a/src/Interpreters/executeDDLQueryOnCluster.cpp b/src/Interpreters/executeDDLQueryOnCluster.cpp\nindex 8a6abf7714f1..c5dec2cf2142 100644\n--- a/src/Interpreters/executeDDLQueryOnCluster.cpp\n+++ b/src/Interpreters/executeDDLQueryOnCluster.cpp\n@@ -102,12 +102,10 @@ BlockIO executeDDLQueryOnCluster(const ASTPtr & query_ptr_, ContextPtr context,\n \n     /// The current database in a distributed query need to be replaced with either\n     /// the local current database or a shard's default database.\n-    bool need_replace_current_database\n-        = (std::find_if(\n-            query_requires_access.begin(),\n-            query_requires_access.end(),\n-            [](const AccessRightsElement & elem) { return elem.isEmptyDatabase(); })\n-           != query_requires_access.end());\n+    bool need_replace_current_database = std::any_of(\n+        query_requires_access.begin(),\n+        query_requires_access.end(),\n+        [](const AccessRightsElement & elem) { return elem.isEmptyDatabase(); });\n \n     bool use_local_default_database = false;\n     const String & current_database = context->getCurrentDatabase();\ndiff --git a/src/Parsers/ASTSystemQuery.cpp b/src/Parsers/ASTSystemQuery.cpp\nindex bf94df0bf501..5d01e124b0e5 100644\n--- a/src/Parsers/ASTSystemQuery.cpp\n+++ b/src/Parsers/ASTSystemQuery.cpp\n@@ -44,6 +44,8 @@ const char * ASTSystemQuery::typeToString(Type type)\n             return \"RESTART REPLICAS\";\n         case Type::RESTART_REPLICA:\n             return \"RESTART REPLICA\";\n+        case Type::RESTORE_REPLICA:\n+            return \"RESTORE REPLICA\";\n         case Type::DROP_REPLICA:\n             return \"DROP REPLICA\";\n         case Type::SYNC_REPLICA:\n@@ -119,18 +121,6 @@ void ASTSystemQuery::formatImpl(const FormatSettings & settings, FormatState &,\n                       << (settings.hilite ? hilite_none : \"\");\n     };\n \n-    auto print_database_dictionary = [&]\n-    {\n-        settings.ostr << \" \";\n-        if (!database.empty())\n-        {\n-            settings.ostr << (settings.hilite ? hilite_identifier : \"\") << backQuoteIfNeed(database)\n-                          << (settings.hilite ? hilite_none : \"\") << \".\";\n-        }\n-        settings.ostr << (settings.hilite ? hilite_identifier : \"\") << backQuoteIfNeed(target_dictionary)\n-                      << (settings.hilite ? hilite_none : \"\");\n-    };\n-\n     auto print_drop_replica = [&]\n     {\n         settings.ostr << \" \" << quoteString(replica);\n@@ -187,14 +177,14 @@ void ASTSystemQuery::formatImpl(const FormatSettings & settings, FormatState &,\n         else if (!volume.empty())\n             print_on_volume();\n     }\n-    else if (type == Type::RESTART_REPLICA || type == Type::SYNC_REPLICA || type == Type::FLUSH_DISTRIBUTED)\n+    else if (  type == Type::RESTART_REPLICA\n+            || type == Type::RESTORE_REPLICA\n+            || type == Type::SYNC_REPLICA\n+            || type == Type::FLUSH_DISTRIBUTED\n+            || type == Type::RELOAD_DICTIONARY)\n     {\n         print_database_table();\n     }\n-    else if (type == Type::RELOAD_DICTIONARY)\n-    {\n-        print_database_dictionary();\n-    }\n     else if (type == Type::DROP_REPLICA)\n     {\n         print_drop_replica();\ndiff --git a/src/Parsers/ASTSystemQuery.h b/src/Parsers/ASTSystemQuery.h\nindex 6cd1443155f1..cbe82cd936fb 100644\n--- a/src/Parsers/ASTSystemQuery.h\n+++ b/src/Parsers/ASTSystemQuery.h\n@@ -32,6 +32,7 @@ class ASTSystemQuery : public IAST, public ASTQueryWithOnCluster\n         START_LISTEN_QUERIES,\n         RESTART_REPLICAS,\n         RESTART_REPLICA,\n+        RESTORE_REPLICA,\n         DROP_REPLICA,\n         SYNC_REPLICA,\n         RELOAD_DICTIONARY,\n@@ -65,7 +66,6 @@ class ASTSystemQuery : public IAST, public ASTQueryWithOnCluster\n \n     Type type = Type::UNKNOWN;\n \n-    String target_dictionary;\n     String target_model;\n     String database;\n     String table;\ndiff --git a/src/Parsers/New/AST/SystemQuery.cpp b/src/Parsers/New/AST/SystemQuery.cpp\nindex 2be9ff951e03..d2fda6a3fbc2 100644\n--- a/src/Parsers/New/AST/SystemQuery.cpp\n+++ b/src/Parsers/New/AST/SystemQuery.cpp\n@@ -133,7 +133,7 @@ ASTPtr SystemQuery::convertToOld() const\n             {\n                 auto table = std::static_pointer_cast<ASTTableIdentifier>(get(TABLE)->convertToOld());\n                 query->database = table->getDatabaseName();\n-                query->target_dictionary = table->shortName();\n+                query->table = table->getTableId().table_name;\n             }\n             break;\n         case QueryType::REPLICATED_SENDS:\ndiff --git a/src/Parsers/ParserRenameQuery.cpp b/src/Parsers/ParserRenameQuery.cpp\nindex e3b35249cd62..c42a0af88b2c 100644\n--- a/src/Parsers/ParserRenameQuery.cpp\n+++ b/src/Parsers/ParserRenameQuery.cpp\n@@ -95,21 +95,18 @@ bool ParserRenameQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n \n     ASTRenameQuery::Elements elements;\n \n-    auto ignore_delim = [&]()\n-    {\n-        return exchange ? s_and.ignore(pos) : s_to.ignore(pos);\n-    };\n+    const auto ignore_delim = [&] { return exchange ? s_and.ignore(pos) : s_to.ignore(pos); };\n \n     while (true)\n     {\n         if (!elements.empty() && !s_comma.ignore(pos))\n             break;\n \n-        elements.push_back(ASTRenameQuery::Element());\n+        ASTRenameQuery::Element& ref = elements.emplace_back();\n \n-        if (!parseDatabaseAndTable(elements.back().from, pos, expected)\n+        if (!parseDatabaseAndTable(ref.from, pos, expected)\n             || !ignore_delim()\n-            || !parseDatabaseAndTable(elements.back().to, pos, expected))\n+            || !parseDatabaseAndTable(ref.to, pos, expected))\n             return false;\n     }\n \ndiff --git a/src/Parsers/ParserSystemQuery.cpp b/src/Parsers/ParserSystemQuery.cpp\nindex a1487468ab3a..66bd39e02020 100644\n--- a/src/Parsers/ParserSystemQuery.cpp\n+++ b/src/Parsers/ParserSystemQuery.cpp\n@@ -15,6 +15,47 @@ namespace ErrorCodes\n namespace DB\n {\n \n+static bool parseQueryWithOnClusterAndMaybeTable(std::shared_ptr<ASTSystemQuery> & res, IParser::Pos & pos,\n+                                                 Expected & expected, bool require_table, bool allow_string_literal)\n+{\n+    /// Better form for user: SYSTEM <ACTION> table ON CLUSTER cluster\n+    /// Query rewritten form + form while executing on cluster: SYSTEM <ACTION> ON CLUSTER cluster table\n+    /// Need to support both\n+    String cluster;\n+    bool parsed_on_cluster = false;\n+\n+    if (ParserKeyword{\"ON\"}.ignore(pos, expected))\n+    {\n+        if (!ASTQueryWithOnCluster::parse(pos, cluster, expected))\n+            return false;\n+        parsed_on_cluster = true;\n+    }\n+\n+    bool parsed_table = false;\n+    if (allow_string_literal)\n+    {\n+        ASTPtr ast;\n+        if (ParserStringLiteral{}.parse(pos, ast, expected))\n+        {\n+            res->database = {};\n+            res->table = ast->as<ASTLiteral &>().value.safeGet<String>();\n+            parsed_table = true;\n+        }\n+    }\n+\n+    if (!parsed_table)\n+        parsed_table = parseDatabaseAndTableName(pos, expected, res->database, res->table);\n+\n+    if (!parsed_table && require_table)\n+            return false;\n+\n+    if (!parsed_on_cluster && ParserKeyword{\"ON\"}.ignore(pos, expected))\n+        if (!ASTQueryWithOnCluster::parse(pos, cluster, expected))\n+            return false;\n+\n+    res->cluster = cluster;\n+    return true;\n+}\n \n bool ParserSystemQuery::parseImpl(IParser::Pos & pos, ASTPtr & node, Expected & expected)\n {\n@@ -43,17 +84,7 @@ bool ParserSystemQuery::parseImpl(IParser::Pos & pos, ASTPtr & node, Expected &\n     {\n         case Type::RELOAD_DICTIONARY:\n         {\n-            String cluster_str;\n-            if (ParserKeyword{\"ON\"}.ignore(pos, expected))\n-            {\n-                if (!ASTQueryWithOnCluster::parse(pos, cluster_str, expected))\n-                    return false;\n-            }\n-            res->cluster = cluster_str;\n-            ASTPtr ast;\n-            if (ParserStringLiteral{}.parse(pos, ast, expected))\n-                res->target_dictionary = ast->as<ASTLiteral &>().value.safeGet<String>();\n-            else if (!parseDatabaseAndTableName(pos, expected, res->database, res->target_dictionary))\n+            if (!parseQueryWithOnClusterAndMaybeTable(res, pos, expected, /* require table = */ true, /* allow_string_literal = */ true))\n                 return false;\n             break;\n         }\n@@ -145,24 +176,21 @@ bool ParserSystemQuery::parseImpl(IParser::Pos & pos, ASTPtr & node, Expected &\n             break;\n         }\n \n+        /// FLUSH DISTRIBUTED requires table\n+        /// START/STOP DISTRIBUTED SENDS does not require table\n         case Type::STOP_DISTRIBUTED_SENDS:\n         case Type::START_DISTRIBUTED_SENDS:\n+        {\n+            if (!parseQueryWithOnClusterAndMaybeTable(res, pos, expected, /* require table = */ false, /* allow_string_literal = */ false))\n+                return false;\n+            break;\n+        }\n+\n         case Type::FLUSH_DISTRIBUTED:\n+        case Type::RESTORE_REPLICA:\n         {\n-            String cluster_str;\n-            if (ParserKeyword{\"ON\"}.ignore(pos, expected))\n-            {\n-                if (!ASTQueryWithOnCluster::parse(pos, cluster_str, expected))\n-                    return false;\n-            }\n-            res->cluster = cluster_str;\n-            if (!parseDatabaseAndTableName(pos, expected, res->database, res->table))\n-            {\n-                /// FLUSH DISTRIBUTED requires table\n-                /// START/STOP DISTRIBUTED SENDS does not require table\n-                if (res->type == Type::FLUSH_DISTRIBUTED)\n-                    return false;\n-            }\n+            if (!parseQueryWithOnClusterAndMaybeTable(res, pos, expected, /* require table = */ true, /* allow_string_literal = */ false))\n+                return false;\n             break;\n         }\n \ndiff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h\nindex a96748ceb637..0e8c7e0a263b 100644\n--- a/src/Storages/IStorage.h\n+++ b/src/Storages/IStorage.h\n@@ -190,9 +190,10 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo\n     /// Initially reserved virtual column name may be shadowed by real column.\n     bool isVirtualColumn(const String & column_name, const StorageMetadataPtr & metadata_snapshot) const;\n \n-\n private:\n+\n     StorageID storage_id;\n+\n     mutable std::mutex id_mutex;\n \n     /// Multiversion storage metadata. Allows to read/write storage metadata\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex f8ff7fe697a9..54fcfc1adc99 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -223,6 +223,12 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n         DeleteOnDestroy, /// part was moved to another disk and should be deleted in own destructor\n     };\n \n+    static constexpr auto all_part_states =\n+    {\n+        State::Temporary, State::PreCommitted, State::Committed, State::Outdated, State::Deleting,\n+        State::DeleteOnDestroy\n+    };\n+\n     using TTLInfo = MergeTreeDataPartTTLInfo;\n     using TTLInfos = MergeTreeDataPartTTLInfos;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex b0bf0c8e672f..abc37f52ff91 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -2156,8 +2156,7 @@ bool MergeTreeData::renameTempPartAndReplace(\n \n     LOG_TRACE(log, \"Renaming temporary part {} to {}.\", part->relative_path, part_name);\n \n-    auto it_duplicate = data_parts_by_info.find(part_info);\n-    if (it_duplicate != data_parts_by_info.end())\n+    if (auto it_duplicate = data_parts_by_info.find(part_info); it_duplicate != data_parts_by_info.end())\n     {\n         String message = \"Part \" + (*it_duplicate)->getNameWithState() + \" already exists\";\n \ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 4f33aa30bdc9..65d875aa9cf5 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -402,6 +402,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n \n     /// Returns a copy of the list so that the caller shouldn't worry about locks.\n     DataParts getDataParts(const DataPartStates & affordable_states) const;\n+\n     /// Returns sorted list of the parts with specified states\n     ///  out_states will contain snapshot of each part state\n     DataPartsVector getDataPartsVector(\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\nindex 7f167f929e55..4a73658e8a44 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\n@@ -262,8 +262,8 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(\n             {\n                 log_entry.type = StorageReplicatedMergeTree::LogEntry::ATTACH_PART;\n \n-                /// We don't need to involve ZooKeeper to obtain the checksums as by the time we get\n-                /// the MutableDataPartPtr here, we already have the data thus being able to\n+                /// We don't need to involve ZooKeeper to obtain checksums as by the time we get\n+                /// MutableDataPartPtr here, we already have the data thus being able to\n                 /// calculate the checksums.\n                 log_entry.part_checksum = part->checksums.getTotalChecksumHex();\n             }\n@@ -384,6 +384,7 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(\n \n         MergeTreeData::Transaction transaction(storage); /// If you can not add a part to ZK, we'll remove it back from the working set.\n         bool renamed = false;\n+\n         try\n         {\n             renamed = storage.renameTempPartAndAdd(part, nullptr, &transaction);\n@@ -394,6 +395,7 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(\n                 && e.code() != ErrorCodes::PART_IS_TEMPORARILY_LOCKED)\n                 throw;\n         }\n+\n         if (!renamed)\n         {\n             if (is_already_existing_part)\ndiff --git a/src/Storages/MergeTree/registerStorageMergeTree.cpp b/src/Storages/MergeTree/registerStorageMergeTree.cpp\nindex ce7fc38b9041..b3b9ce31ff5a 100644\n--- a/src/Storages/MergeTree/registerStorageMergeTree.cpp\n+++ b/src/Storages/MergeTree/registerStorageMergeTree.cpp\n@@ -257,7 +257,7 @@ If you use the Replicated version of engines, see https://clickhouse.tech/docs/e\n \n static StoragePtr create(const StorageFactory::Arguments & args)\n {\n-    /** [Replicated][|Summing|Collapsing|Aggregating|Replacing|Graphite]MergeTree (2 * 7 combinations) engines\n+    /** [Replicated][|Summing|VersionedCollapsing|Collapsing|Aggregating|Replacing|Graphite]MergeTree (2 * 7 combinations) engines\n         * The argument for the engine should be:\n         *  - (for Replicated) The path to the table in ZooKeeper\n         *  - (for Replicated) Replica name in ZooKeeper\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex c11f1580a2e4..47f6bbd0ccc5 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -1,5 +1,6 @@\n #include <Core/Defines.h>\n \n+#include \"Common/hex.h\"\n #include <Common/Macros.h>\n #include <Common/StringUtils/StringUtils.h>\n #include <Common/ThreadPool.h>\n@@ -63,10 +64,13 @@\n #include <common/scope_guard.h>\n #include <common/scope_guard_safe.h>\n \n+#include <algorithm>\n #include <ctime>\n+#include <filesystem>\n+#include <iterator>\n+#include <numeric>\n #include <thread>\n #include <future>\n-#include <filesystem>\n \n #include <boost/algorithm/string/join.hpp>\n \n@@ -135,6 +139,7 @@ namespace ErrorCodes\n     extern const int INTERSERVER_SCHEME_DOESNT_MATCH;\n     extern const int DUPLICATE_DATA_PART;\n     extern const int BAD_ARGUMENTS;\n+    extern const int CONCURRENT_ACCESS_NOT_SUPPORTED;\n }\n \n namespace ActionLocks\n@@ -153,10 +158,6 @@ static const auto MERGE_SELECTING_SLEEP_MS           = 5 * 1000;\n static const auto MUTATIONS_FINALIZING_SLEEP_MS      = 1 * 1000;\n static const auto MUTATIONS_FINALIZING_IDLE_SLEEP_MS = 5 * 1000;\n \n-\n-std::atomic_uint StorageReplicatedMergeTree::total_fetches {0};\n-\n-\n void StorageReplicatedMergeTree::setZooKeeper()\n {\n     /// Every ReplicatedMergeTree table is using only one ZooKeeper session.\n@@ -376,7 +377,7 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n \n     if (attach && !current_zookeeper->exists(zookeeper_path + \"/metadata\"))\n     {\n-        LOG_WARNING(log, \"No metadata in ZooKeeper: table will be in readonly mode.\");\n+        LOG_WARNING(log, \"No metadata in ZooKeeper for {}: table will be in readonly mode.\", zookeeper_path);\n         is_readonly = true;\n         has_metadata_in_zookeeper = false;\n         return;\n@@ -384,10 +385,20 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n \n     auto metadata_snapshot = getInMemoryMetadataPtr();\n \n+    /// May it be ZK lost not the whole root, so the upper check passed, but only the /replicas/replica\n+    /// folder.\n+    if (attach && !current_zookeeper->exists(replica_path))\n+    {\n+        LOG_WARNING(log, \"No metadata in ZooKeeper for {}: table will be in readonly mode\", replica_path);\n+        is_readonly = true;\n+        has_metadata_in_zookeeper = false;\n+        return;\n+    }\n+\n     if (!attach)\n     {\n         if (!getDataParts().empty())\n-            throw Exception(\"Data directory for table already containing data parts\"\n+            throw Exception(\"Data directory for table already contains data parts\"\n                 \" - probably it was unclean DROP table or manual intervention.\"\n                 \" You must either clear directory by hand or use ATTACH TABLE\"\n                 \" instead of CREATE TABLE if you need to use that parts.\", ErrorCodes::INCORRECT_DATA);\n@@ -433,13 +444,17 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     {\n         /// In old tables this node may missing or be empty\n         String replica_metadata;\n-        bool replica_metadata_exists = current_zookeeper->tryGet(replica_path + \"/metadata\", replica_metadata);\n+        const bool replica_metadata_exists = current_zookeeper->tryGet(replica_path + \"/metadata\", replica_metadata);\n+\n         if (!replica_metadata_exists || replica_metadata.empty())\n         {\n             /// We have to check shared node granularity before we create ours.\n             other_replicas_fixed_granularity = checkFixedGranualrityInZookeeper();\n+\n             ReplicatedMergeTreeTableMetadata current_metadata(*this, metadata_snapshot);\n-            current_zookeeper->createOrUpdate(replica_path + \"/metadata\", current_metadata.toString(), zkutil::CreateMode::Persistent);\n+\n+            current_zookeeper->createOrUpdate(replica_path + \"/metadata\", current_metadata.toString(),\n+                zkutil::CreateMode::Persistent);\n         }\n \n         checkTableStructure(replica_path, metadata_snapshot);\n@@ -460,8 +475,8 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n             current_zookeeper->get(zookeeper_path + \"/metadata\", &metadata_stat);\n             metadata_version = metadata_stat.version;\n         }\n-        /// Temporary directories contain untinalized results of Merges or Fetches (after forced restart)\n-        ///  and don't allow to reinitialize them, so delete each of them immediately\n+        /// Temporary directories contain uninitialized results of Merges or Fetches (after forced restart),\n+        /// don't allow to reinitialize them, delete each of them immediately.\n         clearOldTemporaryDirectories(0);\n         clearOldWriteAheadLogs();\n     }\n@@ -727,12 +742,13 @@ void StorageReplicatedMergeTree::createReplica(const StorageMetadataPtr & metada\n         String replicas_value;\n \n         if (!zookeeper->tryGet(zookeeper_path + \"/replicas\", replicas_value, &replicas_stat))\n-            throw Exception(fmt::format(\"Cannot create a replica of the table {}, because the last replica of the table was dropped right now\",\n-                zookeeper_path), ErrorCodes::ALL_REPLICAS_LOST);\n+            throw Exception(ErrorCodes::ALL_REPLICAS_LOST,\n+                \"Cannot create a replica of the table {}, because the last replica of the table was dropped right now\",\n+                zookeeper_path);\n \n         /// It is not the first replica, we will mark it as \"lost\", to immediately repair (clone) from existing replica.\n         /// By the way, it's possible that the replica will be first, if all previous replicas were removed concurrently.\n-        String is_lost_value = replicas_stat.numChildren ? \"1\" : \"0\";\n+        const String is_lost_value = replicas_stat.numChildren ? \"1\" : \"0\";\n \n         Coordination::Requests ops;\n         ops.emplace_back(zkutil::makeCreateRequest(replica_path, \"\",\n@@ -761,21 +777,18 @@ void StorageReplicatedMergeTree::createReplica(const StorageMetadataPtr & metada\n \n         Coordination::Responses responses;\n         code = zookeeper->tryMulti(ops, responses);\n-        if (code == Coordination::Error::ZNODEEXISTS)\n-        {\n-            throw Exception(\"Replica \" + replica_path + \" already exists.\", ErrorCodes::REPLICA_IS_ALREADY_EXIST);\n-        }\n-        else if (code == Coordination::Error::ZBADVERSION)\n-        {\n-            LOG_ERROR(log, \"Retrying createReplica(), because some other replicas were created at the same time\");\n-        }\n-        else if (code == Coordination::Error::ZNONODE)\n-        {\n-            throw Exception(\"Table \" + zookeeper_path + \" was suddenly removed.\", ErrorCodes::ALL_REPLICAS_LOST);\n-        }\n-        else\n+\n+        switch (code)\n         {\n-            zkutil::KeeperMultiException::check(code, ops, responses);\n+            case Coordination::Error::ZNODEEXISTS:\n+                throw Exception(ErrorCodes::REPLICA_IS_ALREADY_EXIST, \"Replica {} already exists\", replica_path);\n+            case Coordination::Error::ZBADVERSION:\n+                LOG_ERROR(log, \"Retrying createReplica(), because some other replicas were created at the same time\");\n+                break;\n+            case Coordination::Error::ZNONODE:\n+                throw Exception(ErrorCodes::ALL_REPLICAS_LOST, \"Table {} was suddenly removed\", zookeeper_path);\n+            default:\n+                zkutil::KeeperMultiException::check(code, ops, responses);\n         }\n     } while (code == Coordination::Error::ZBADVERSION);\n }\n@@ -1123,6 +1136,7 @@ void StorageReplicatedMergeTree::checkParts(bool skip_sanity_checks)\n     size_t unexpected_parts_nonnew = 0;\n     UInt64 unexpected_parts_nonnew_rows = 0;\n     UInt64 unexpected_parts_rows = 0;\n+\n     for (const auto & part : unexpected_parts)\n     {\n         if (part->info.level > 0)\n@@ -1134,20 +1148,17 @@ void StorageReplicatedMergeTree::checkParts(bool skip_sanity_checks)\n         unexpected_parts_rows += part->rows_count;\n     }\n \n-    /// Additional helpful statistics\n-    auto get_blocks_count_in_data_part = [&] (const String & part_name) -> UInt64\n-    {\n-        MergeTreePartInfo part_info;\n-        if (MergeTreePartInfo::tryParsePartName(part_name, &part_info, format_version))\n-            return part_info.getBlocksCount();\n+    const UInt64 parts_to_fetch_blocks = std::accumulate(parts_to_fetch.cbegin(), parts_to_fetch.cend(), 0,\n+        [&](UInt64 acc, const String& part_name)\n+        {\n+            MergeTreePartInfo part_info;\n \n-        LOG_ERROR(log, \"Unexpected part name: {}\", part_name);\n-        return 0;\n-    };\n+            if (MergeTreePartInfo::tryParsePartName(part_name, &part_info, format_version))\n+                return acc + part_info.getBlocksCount();\n \n-    UInt64 parts_to_fetch_blocks = 0;\n-    for (const String & name : parts_to_fetch)\n-        parts_to_fetch_blocks += get_blocks_count_in_data_part(name);\n+            LOG_ERROR(log, \"Unexpected part name: {}\", part_name);\n+            return acc;\n+        });\n \n     /** We can automatically synchronize data,\n       *  if the ratio of the total number of errors to the total number of parts (minimum - on the local filesystem or in ZK)\n@@ -1499,7 +1510,7 @@ bool StorageReplicatedMergeTree::executeLogEntry(LogEntry & entry)\n     {\n         if (MutableDataPartPtr part = attachPartHelperFoundValidPart(entry); part)\n         {\n-            LOG_TRACE(log, \"Found valid part to attach from local data, preparing the transaction\");\n+            LOG_TRACE(log, \"Found valid local part for {}, preparing the transaction\", part->name);\n \n             Transaction transaction(*this);\n \n@@ -1512,7 +1523,9 @@ bool StorageReplicatedMergeTree::executeLogEntry(LogEntry & entry)\n             return true;\n         }\n \n-        LOG_TRACE(log, \"Didn't find part with the correct checksums, will fetch it from other replica\");\n+        LOG_TRACE(log, \"Didn't find valid local part for {} ({}), will fetch it from other replica\",\n+            entry.new_part_name,\n+            entry.actual_new_part_name);\n     }\n \n     if (is_get_or_attach && entry.source_replica == replica_name)\n@@ -2732,6 +2745,7 @@ void StorageReplicatedMergeTree::cloneReplica(const String & source_replica, Coo\n     /// Remove local parts if source replica does not have them, because such parts will never be fetched by other replicas.\n     Strings local_parts_in_zk = zookeeper->getChildren(fs::path(replica_path) / \"parts\");\n     Strings parts_to_remove_from_zk;\n+\n     for (const auto & part : local_parts_in_zk)\n     {\n         if (active_parts_set.getContainingPart(part).empty())\n@@ -2740,10 +2754,13 @@ void StorageReplicatedMergeTree::cloneReplica(const String & source_replica, Coo\n             LOG_WARNING(log, \"Source replica does not have part {}. Removing it from ZooKeeper.\", part);\n         }\n     }\n+\n     tryRemovePartsFromZooKeeperWithRetries(parts_to_remove_from_zk);\n \n     auto local_active_parts = getDataParts();\n+\n     DataPartsVector parts_to_remove_from_working_set;\n+\n     for (const auto & part : local_active_parts)\n     {\n         if (active_parts_set.getContainingPart(part->name).empty())\n@@ -2756,6 +2773,7 @@ void StorageReplicatedMergeTree::cloneReplica(const String & source_replica, Coo\n     if (getSettings()->detach_old_local_parts_when_cloning_replica)\n     {\n         auto metadata_snapshot = getInMemoryMetadataPtr();\n+\n         for (const auto & part : parts_to_remove_from_working_set)\n         {\n             LOG_INFO(log, \"Detaching {}\", part->relative_path);\n@@ -2768,7 +2786,35 @@ void StorageReplicatedMergeTree::cloneReplica(const String & source_replica, Coo\n     for (const String & name : active_parts)\n     {\n         LogEntry log_entry;\n-        log_entry.type = LogEntry::GET_PART;\n+\n+        if (!are_restoring_replica)\n+            log_entry.type = LogEntry::GET_PART;\n+        else\n+        {\n+            LOG_DEBUG(log, \"Obtaining checksum for path {}\", name);\n+\n+            // The part we want to fetch is probably present in detached/ folder.\n+            // However, we need to get part's checksum to check if it's not corrupt.\n+            log_entry.type = LogEntry::ATTACH_PART;\n+\n+            MinimalisticDataPartChecksums desired_checksums;\n+\n+            const fs::path part_path = fs::path(source_path) / \"parts\" / name;\n+\n+            const String part_znode = zookeeper->get(part_path);\n+\n+            if (!part_znode.empty())\n+                desired_checksums = ReplicatedMergeTreePartHeader::fromString(part_znode).getChecksums();\n+            else\n+            {\n+                String desired_checksums_str = zookeeper->get(part_path / \"checksums\");\n+                desired_checksums = MinimalisticDataPartChecksums::deserializeFrom(desired_checksums_str);\n+            }\n+\n+            const auto [lo, hi] = desired_checksums.hash_of_all_files;\n+            log_entry.part_checksum = getHexUIntUppercase(hi) + getHexUIntUppercase(lo);\n+        }\n+\n         log_entry.source_replica = \"\";\n         log_entry.new_part_name = name;\n         log_entry.create_time = tryGetPartCreateTime(zookeeper, source_path, name);\n@@ -2868,6 +2914,7 @@ void StorageReplicatedMergeTree::cloneReplicaIfNeeded(zkutil::ZooKeeperPtr zooke\n     Coordination::Stat is_lost_stat;\n     bool is_new_replica = true;\n     String res;\n+\n     if (zookeeper->tryGet(fs::path(replica_path) / \"is_lost\", res, &is_lost_stat))\n     {\n         if (res == \"0\")\n@@ -3968,6 +4015,7 @@ bool StorageReplicatedMergeTree::fetchPart(const String & part_name, const Stora\n             MinimalisticDataPartChecksums desired_checksums;\n             String part_path = fs::path(source_replica_path) / \"parts\" / part_name;\n             String part_znode = zookeeper->get(part_path);\n+\n             if (!part_znode.empty())\n                 desired_checksums = ReplicatedMergeTreePartHeader::fromString(part_znode).getChecksums();\n             else\n@@ -5030,6 +5078,59 @@ bool StorageReplicatedMergeTree::getFakePartCoveringAllPartsInPartition(const St\n     return true;\n }\n \n+void StorageReplicatedMergeTree::restoreMetadataInZooKeeper()\n+{\n+    LOG_INFO(log, \"Restoring replica metadata\");\n+\n+    if (!is_readonly || has_metadata_in_zookeeper)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"It's a bug: replica is not readonly\");\n+\n+    if (are_restoring_replica.exchange(true))\n+        throw Exception(ErrorCodes::CONCURRENT_ACCESS_NOT_SUPPORTED, \"Replica restoration in progress\");\n+\n+    auto metadata_snapshot = getInMemoryMetadataPtr();\n+\n+    const DataPartsVector all_parts = getDataPartsVector(IMergeTreeDataPart::all_part_states);\n+    Strings active_parts_names;\n+\n+    /// Why all parts (not only Committed) are moved to detached/:\n+    /// After ZK metadata restoration ZK resets sequential counters (including block number counters), so one may\n+    /// potentially encounter a situation that a part we want to attach already exists.\n+    for (const auto & part : all_parts)\n+    {\n+        if (part->getState() == DataPartState::Committed)\n+            active_parts_names.push_back(part->name);\n+\n+        forgetPartAndMoveToDetached(part);\n+    }\n+\n+    LOG_INFO(log, \"Moved all parts to detached/\");\n+\n+    const bool is_first_replica = createTableIfNotExists(metadata_snapshot);\n+\n+    LOG_INFO(log, \"Created initial ZK nodes, replica is first: {}\", is_first_replica);\n+\n+    if (!is_first_replica)\n+        createReplica(metadata_snapshot);\n+\n+    createNewZooKeeperNodes();\n+\n+    LOG_INFO(log, \"Created ZK nodes for table\");\n+\n+    is_readonly = false;\n+    has_metadata_in_zookeeper = true;\n+\n+    if (is_first_replica)\n+        for (const String& part_name : active_parts_names)\n+            attachPartition(std::make_shared<ASTLiteral>(part_name), metadata_snapshot, true, getContext());\n+\n+    LOG_INFO(log, \"Attached all partitions, starting table\");\n+\n+    startup();\n+\n+    are_restoring_replica.store(false);\n+}\n+\n void StorageReplicatedMergeTree::dropPartNoWaitNoThrow(const String & part_name)\n {\n     assertNotReadonly();\n@@ -6938,8 +7039,10 @@ bool StorageReplicatedMergeTree::dropAllPartsInPartition(\n     zookeeper.get(alter_partition_version_path, &alter_partition_version_stat);\n \n     MergeTreePartInfo drop_range_info;\n-    /// It prevent other replicas from assigning merges which intersect locked block number.\n+\n+    /// It would prevent other replicas from assigning merges which intersect locked block number.\n     std::optional<EphemeralLockInZooKeeper> delimiting_block_lock;\n+\n     if (!getFakePartCoveringAllPartsInPartition(partition_id, drop_range_info, delimiting_block_lock))\n     {\n         LOG_INFO(log, \"Will not drop partition {}, it is empty.\", partition_id);\n@@ -6960,23 +7063,31 @@ bool StorageReplicatedMergeTree::dropAllPartsInPartition(\n     entry.create_time = time(nullptr);\n \n     Coordination::Requests ops;\n-    ops.emplace_back(zkutil::makeCreateRequest(fs::path(zookeeper_path) / \"log/log-\", entry.toString(), zkutil::CreateMode::PersistentSequential));\n+\n+    ops.emplace_back(zkutil::makeCreateRequest(fs::path(zookeeper_path) / \"log/log-\", entry.toString(),\n+        zkutil::CreateMode::PersistentSequential));\n+\n     /// Check and update version to avoid race with REPLACE_RANGE.\n     /// Otherwise new parts covered by drop_range_info may appear after execution of current DROP_RANGE entry\n     /// as a result of execution of concurrently created REPLACE_RANGE entry.\n     ops.emplace_back(zkutil::makeCheckRequest(alter_partition_version_path, alter_partition_version_stat.version));\n     ops.emplace_back(zkutil::makeSetRequest(alter_partition_version_path, \"\", -1));\n+\n     /// Just update version, because merges assignment relies on it\n     ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"log\", \"\", -1));\n     delimiting_block_lock->getUnlockOps(ops);\n+\n     if (auto txn = query_context->getZooKeeperMetadataTransaction())\n         txn->moveOpsTo(ops);\n+\n     Coordination::Responses responses;\n     Coordination::Error code = zookeeper.tryMulti(ops, responses);\n+\n     if (code == Coordination::Error::ZOK)\n         delimiting_block_lock->assumeUnlocked();\n     else if (code == Coordination::Error::ZBADVERSION)\n-        throw Exception(ErrorCodes::CANNOT_ASSIGN_ALTER, \"Cannot assign ALTER PARTITION, because another ALTER PARTITION query was concurrently executed\");\n+        throw Exception(ErrorCodes::CANNOT_ASSIGN_ALTER,\n+            \"Cannot assign ALTER PARTITION because another ALTER PARTITION query was concurrently executed\");\n     else\n         zkutil::KeeperMultiException::check(code, ops, responses);\n \ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 505eb4e87c44..396ec7a17419 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -35,7 +35,7 @@\n namespace DB\n {\n \n-/** The engine that uses the merge tree (see MergeTreeData) and replicated through ZooKeeper.\n+/** The engine that uses the merge tree (see MergeTreeData) and is replicated through ZooKeeper.\n   *\n   * ZooKeeper is used for the following things:\n   * - the structure of the table (/metadata, /columns)\n@@ -57,6 +57,7 @@ namespace DB\n   * Log - a sequence of entries (LogEntry) about what to do.\n   * Each entry is one of:\n   * - normal data insertion (GET),\n+  * - data insertion with a possible attach from local data (ATTACH),\n   * - merge (MERGE),\n   * - delete the partition (DROP).\n   *\n@@ -65,10 +66,8 @@ namespace DB\n   * Despite the name of the \"queue\", execution can be reordered, if necessary (shouldExecuteLogEntry, executeLogEntry).\n   * In addition, the records in the queue can be generated independently (not from the log), in the following cases:\n   * - when creating a new replica, actions are put on GET from other replicas (createReplica);\n-  * - if the part is corrupt (removePartAndEnqueueFetch) or absent during the check (at start - checkParts, while running - searchForMissingPart),\n-  *   actions are put on GET from other replicas;\n-  *\n-  * TODO Update the GET part after rewriting the code (search locally).\n+  * - if the part is corrupt (removePartAndEnqueueFetch) or absent during the check\n+  *   (at start - checkParts, while running - searchForMissingPart), actions are put on GET from other replicas;\n   *\n   * The replica to which INSERT was made in the queue will also have an entry of the GET of this data.\n   * Such an entry is considered to be executed as soon as the queue handler sees it.\n@@ -240,6 +239,13 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica\n     /// Get best replica having this partition on S3\n     String getSharedDataReplica(const IMergeTreeDataPart & part) const;\n \n+    inline String getReplicaName() const { return replica_name; }\n+\n+    /// Restores table metadata if ZooKeeper lost it.\n+    /// Used only on restarted readonly replicas (not checked). All active (Committed) parts are moved to detached/\n+    /// folder and attached. Parts in all other states are just moved to detached/ folder.\n+    void restoreMetadataInZooKeeper();\n+\n     /// Get throttler for replicated fetches\n     ThrottlerPtr getFetchesThrottler() const\n     {\n@@ -253,6 +259,8 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica\n     }\n \n private:\n+    std::atomic_bool are_restoring_replica {false};\n+\n     /// Get a sequential consistent view of current parts.\n     ReplicatedMergeTreeQuorumAddedParts::PartitionIdToMaxBlock getMaxAddedBlocks() const;\n \n@@ -332,7 +340,7 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica\n     Poco::Event partial_shutdown_event {false};     /// Poco::Event::EVENT_MANUALRESET\n \n     /// Limiting parallel fetches per node\n-    static std::atomic_uint total_fetches;\n+    static inline std::atomic_uint total_fetches {0};\n \n     /// Limiting parallel fetches per one table\n     std::atomic_uint current_table_fetches {0};\n@@ -389,8 +397,9 @@ class StorageReplicatedMergeTree final : public shared_ptr_helper<StorageReplica\n       */\n     bool createTableIfNotExists(const StorageMetadataPtr & metadata_snapshot);\n \n-    /** Creates a replica in ZooKeeper and adds to the queue all that it takes to catch up with the rest of the replicas.\n-      */\n+    /**\n+     * Creates a replica in ZooKeeper and adds to the queue all that it takes to catch up with the rest of the replicas.\n+     */\n     void createReplica(const StorageMetadataPtr & metadata_snapshot);\n \n     /** Create nodes in the ZK, which must always be, but which might not exist when older versions of the server are running.\n",
  "test_patch": "diff --git a/tests/integration/test_attach_without_fetching/test.py b/tests/integration/test_attach_without_fetching/test.py\nindex 605ca6a4f51e..874f5b36ddc7 100644\n--- a/tests/integration/test_attach_without_fetching/test.py\n+++ b/tests/integration/test_attach_without_fetching/test.py\n@@ -16,11 +16,10 @@ def fill_node(node):\n     '''.format(replica=node.name))\n \n cluster = ClickHouseCluster(__file__)\n-configs =[\"configs/remote_servers.xml\"]\n \n-node_1 = cluster.add_instance('replica1', with_zookeeper=True, main_configs=configs)\n-node_2 = cluster.add_instance('replica2', with_zookeeper=True, main_configs=configs)\n-node_3 = cluster.add_instance('replica3', with_zookeeper=True, main_configs=configs)\n+node_1 = cluster.add_instance('replica1', with_zookeeper=True)\n+node_2 = cluster.add_instance('replica2', with_zookeeper=True)\n+node_3 = cluster.add_instance('replica3', with_zookeeper=True)\n \n @pytest.fixture(scope=\"module\")\n def start_cluster():\ndiff --git a/tests/integration/test_grant_and_revoke/test.py b/tests/integration/test_grant_and_revoke/test.py\nindex c1be16fe17d1..1124f072a06a 100644\n--- a/tests/integration/test_grant_and_revoke/test.py\n+++ b/tests/integration/test_grant_and_revoke/test.py\n@@ -151,7 +151,7 @@ def test_grant_all_on_table():\n     instance.query(\"GRANT ALL ON test.table TO A WITH GRANT OPTION\")\n     instance.query(\"GRANT ALL ON test.table TO B\", user='A')\n     assert instance.query(\n-        \"SHOW GRANTS FOR B\") == \"GRANT SHOW TABLES, SHOW COLUMNS, SHOW DICTIONARIES, SELECT, INSERT, ALTER, CREATE TABLE, CREATE VIEW, CREATE DICTIONARY, DROP TABLE, DROP VIEW, DROP DICTIONARY, TRUNCATE, OPTIMIZE, SYSTEM MERGES, SYSTEM TTL MERGES, SYSTEM FETCHES, SYSTEM MOVES, SYSTEM SENDS, SYSTEM REPLICATION QUEUES, SYSTEM DROP REPLICA, SYSTEM SYNC REPLICA, SYSTEM RESTART REPLICA, SYSTEM FLUSH DISTRIBUTED, dictGet ON test.table TO B\\n\"\n+        \"SHOW GRANTS FOR B\") == \"GRANT SHOW TABLES, SHOW COLUMNS, SHOW DICTIONARIES, SELECT, INSERT, ALTER, CREATE TABLE, CREATE VIEW, CREATE DICTIONARY, DROP TABLE, DROP VIEW, DROP DICTIONARY, TRUNCATE, OPTIMIZE, SYSTEM MERGES, SYSTEM TTL MERGES, SYSTEM FETCHES, SYSTEM MOVES, SYSTEM SENDS, SYSTEM REPLICATION QUEUES, SYSTEM DROP REPLICA, SYSTEM SYNC REPLICA, SYSTEM RESTART REPLICA, SYSTEM RESTORE REPLICA, SYSTEM FLUSH DISTRIBUTED, dictGet ON test.table TO B\\n\"\n     instance.query(\"REVOKE ALL ON test.table FROM B\", user='A')\n     assert instance.query(\"SHOW GRANTS FOR B\") == \"\"\n \ndiff --git a/tests/integration/test_restore_replica/__init__.py b/tests/integration/test_restore_replica/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_attach_without_fetching/configs/remote_servers.xml b/tests/integration/test_restore_replica/configs/remote_servers.xml\nsimilarity index 79%\nrename from tests/integration/test_attach_without_fetching/configs/remote_servers.xml\nrename to tests/integration/test_restore_replica/configs/remote_servers.xml\nindex 7978f921b2e6..0709f97551a2 100644\n--- a/tests/integration/test_attach_without_fetching/configs/remote_servers.xml\n+++ b/tests/integration/test_restore_replica/configs/remote_servers.xml\n@@ -4,15 +4,15 @@\n             <shard>\n                 <internal_replication>true</internal_replication>\n                 <replica>\n-                    <host>node_1_1</host>\n+                    <host>replica1</host>\n                     <port>9000</port>\n                 </replica>\n                 <replica>\n-                    <host>node_1_2</host>\n+                    <host>replica2</host>\n                     <port>9000</port>\n                 </replica>\n                  <replica>\n-                    <host>node_1_3</host>\n+                    <host>replica3</host>\n                     <port>9000</port>\n                 </replica>\n             </shard>\ndiff --git a/tests/integration/test_restore_replica/test.py b/tests/integration/test_restore_replica/test.py\nnew file mode 100644\nindex 000000000000..4197c0642439\n--- /dev/null\n+++ b/tests/integration/test_restore_replica/test.py\n@@ -0,0 +1,156 @@\n+import time\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.cluster import ClickHouseKiller\n+from helpers.test_tools import assert_eq_with_retry\n+from helpers.network import PartitionManager\n+\n+def fill_nodes(nodes):\n+    for node in nodes:\n+        node.query(\n+        '''\n+            CREATE TABLE test(n UInt32)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/', '{replica}')\n+            ORDER BY n PARTITION BY n % 10;\n+        '''.format(replica=node.name))\n+\n+cluster = ClickHouseCluster(__file__)\n+configs =[\"configs/remote_servers.xml\"]\n+\n+node_1 = cluster.add_instance('replica1', with_zookeeper=True, main_configs=configs)\n+node_2 = cluster.add_instance('replica2', with_zookeeper=True, main_configs=configs)\n+node_3 = cluster.add_instance('replica3', with_zookeeper=True, main_configs=configs)\n+nodes = [node_1, node_2, node_3]\n+\n+def fill_table():\n+    node_1.query(\"TRUNCATE TABLE test\")\n+\n+    for node in nodes:\n+        node.query(\"SYSTEM SYNC REPLICA test\")\n+\n+    check_data(0, 0)\n+\n+    # it will create multiple parts in each partition and probably cause merges\n+    node_1.query(\"INSERT INTO test SELECT number + 0 FROM numbers(200)\")\n+    node_1.query(\"INSERT INTO test SELECT number + 200 FROM numbers(200)\")\n+    node_1.query(\"INSERT INTO test SELECT number + 400 FROM numbers(200)\")\n+    node_1.query(\"INSERT INTO test SELECT number + 600 FROM numbers(200)\")\n+    node_1.query(\"INSERT INTO test SELECT number + 800 FROM numbers(200)\")\n+    check_data(499500, 1000)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        fill_nodes(nodes)\n+        yield cluster\n+\n+    except Exception as ex:\n+        print(ex)\n+\n+    finally:\n+        cluster.shutdown()\n+\n+def check_data(_sum: int, count: int) -> None:\n+    res = \"{}\\t{}\\n\".format(_sum, count)\n+    assert_eq_with_retry(node_1, \"SELECT sum(n), count() FROM test\", res)\n+    assert_eq_with_retry(node_2, \"SELECT sum(n), count() FROM test\", res)\n+    assert_eq_with_retry(node_3, \"SELECT sum(n), count() FROM test\", res)\n+\n+def check_after_restoration():\n+    check_data(1999000, 2000)\n+\n+    for node in nodes:\n+        node.query_and_get_error(\"SYSTEM RESTORE REPLICA test\")\n+\n+def test_restore_replica_invalid_tables(start_cluster):\n+    print(\"Checking the invocation on non-existent and non-replicated tables\")\n+    node_1.query_and_get_error(\"SYSTEM RESTORE REPLICA i_dont_exist_42\")\n+    node_1.query_and_get_error(\"SYSTEM RESTORE REPLICA no_db.i_dont_exist_42\")\n+    node_1.query_and_get_error(\"SYSTEM RESTORE REPLICA system.numbers\")\n+\n+def test_restore_replica_sequential(start_cluster):\n+    zk = cluster.get_kazoo_client('zoo1')\n+    fill_table()\n+\n+    print(\"Deleting root ZK path metadata\")\n+    zk.delete(\"/clickhouse/tables/test\", recursive=True)\n+    assert zk.exists(\"/clickhouse/tables/test\") is None\n+\n+    node_1.query(\"SYSTEM RESTART REPLICA test\")\n+    node_1.query_and_get_error(\"INSERT INTO test SELECT number AS num FROM numbers(1000,2000) WHERE num % 2 = 0\")\n+\n+    print(\"Restoring replica1\")\n+\n+    node_1.query(\"SYSTEM RESTORE REPLICA test\")\n+    assert zk.exists(\"/clickhouse/tables/test\")\n+    check_data(499500, 1000)\n+\n+    node_1.query(\"INSERT INTO test SELECT number + 1000 FROM numbers(1000)\")\n+\n+    print(\"Restoring other replicas\")\n+\n+    node_2.query(\"SYSTEM RESTART REPLICA test\")\n+    node_2.query(\"SYSTEM RESTORE REPLICA test\")\n+\n+    node_3.query(\"SYSTEM RESTART REPLICA test\")\n+    node_3.query(\"SYSTEM RESTORE REPLICA test\")\n+\n+    node_2.query(\"SYSTEM SYNC REPLICA test\")\n+    node_3.query(\"SYSTEM SYNC REPLICA test\")\n+\n+    check_after_restoration()\n+\n+def test_restore_replica_parallel(start_cluster):\n+    zk = cluster.get_kazoo_client('zoo1')\n+    fill_table()\n+\n+    print(\"Deleting root ZK path metadata\")\n+    zk.delete(\"/clickhouse/tables/test\", recursive=True)\n+    assert zk.exists(\"/clickhouse/tables/test\") is None\n+\n+    node_1.query(\"SYSTEM RESTART REPLICA test\")\n+    node_1.query_and_get_error(\"INSERT INTO test SELECT number AS num FROM numbers(1000,2000) WHERE num % 2 = 0\")\n+\n+    print(\"Restoring replicas in parallel\")\n+\n+    node_2.query(\"SYSTEM RESTART REPLICA test\")\n+    node_3.query(\"SYSTEM RESTART REPLICA test\")\n+\n+    node_1.query(\"SYSTEM RESTORE REPLICA test ON CLUSTER test_cluster\")\n+\n+    assert zk.exists(\"/clickhouse/tables/test\")\n+    check_data(499500, 1000)\n+\n+    node_1.query(\"INSERT INTO test SELECT number + 1000 FROM numbers(1000)\")\n+\n+    check_after_restoration()\n+\n+def test_restore_replica_alive_replicas(start_cluster):\n+    zk = cluster.get_kazoo_client('zoo1')\n+    fill_table()\n+\n+    print(\"Deleting replica2 path, trying to restore replica1\")\n+    zk.delete(\"/clickhouse/tables/test/replicas/replica2\", recursive=True)\n+    assert zk.exists(\"/clickhouse/tables/test/replicas/replica2\") is None\n+    node_1.query_and_get_error(\"SYSTEM RESTORE REPLICA test\")\n+\n+    print(\"Deleting replica1 path, trying to restore replica1\")\n+    zk.delete(\"/clickhouse/tables/test/replicas/replica1\", recursive=True)\n+    assert zk.exists(\"/clickhouse/tables/test/replicas/replica1\") is None\n+\n+    node_1.query(\"SYSTEM RESTART REPLICA test\")\n+    node_1.query(\"SYSTEM RESTORE REPLICA test\")\n+\n+    node_2.query(\"SYSTEM RESTART REPLICA test\")\n+    node_2.query(\"SYSTEM RESTORE REPLICA test\")\n+\n+    check_data(499500, 1000)\n+\n+    node_1.query(\"INSERT INTO test SELECT number + 1000 FROM numbers(1000)\")\n+\n+    node_2.query(\"SYSTEM SYNC REPLICA test\")\n+    node_3.query(\"SYSTEM SYNC REPLICA test\")\n+\n+    check_after_restoration()\ndiff --git a/tests/queries/0_stateless/01271_show_privileges.reference b/tests/queries/0_stateless/01271_show_privileges.reference\nindex 0ab0d57ebcfa..343d8ceeca36 100644\n--- a/tests/queries/0_stateless/01271_show_privileges.reference\n+++ b/tests/queries/0_stateless/01271_show_privileges.reference\n@@ -103,6 +103,7 @@ SYSTEM REPLICATION QUEUES\t['SYSTEM STOP REPLICATION QUEUES','SYSTEM START REPLIC\n SYSTEM DROP REPLICA\t['DROP REPLICA']\tTABLE\tSYSTEM\n SYSTEM SYNC REPLICA\t['SYNC REPLICA']\tTABLE\tSYSTEM\n SYSTEM RESTART REPLICA\t['RESTART REPLICA']\tTABLE\tSYSTEM\n+SYSTEM RESTORE REPLICA\t['RESTORE REPLICA']\tTABLE\tSYSTEM\n SYSTEM FLUSH DISTRIBUTED\t['FLUSH DISTRIBUTED']\tTABLE\tSYSTEM FLUSH\n SYSTEM FLUSH LOGS\t['FLUSH LOGS']\tGLOBAL\tSYSTEM FLUSH\n SYSTEM FLUSH\t[]\t\\N\tSYSTEM\n",
  "problem_statement": "SYSTEM RESTORE REPLICA option to restore ZooKeeper's metadata\n**Use case**\r\nThere is a case when the ZooKeeper's replica's metadata is lost (but the data itself is present). Currently there are no simple ways to restore the metadata. \r\n\r\n**Describe the solution you'd like**\r\nI propose the new syntax SYSTEM RESTORE REPLICA (similar to SYSTEM RESTART REPLICA) to automate the needed actions.\r\n\r\nRelated issues: #7972, #4691\r\n\n",
  "hints_text": "",
  "created_at": "2020-08-12T14:43:40Z",
  "modified_files": [
    "docs/en/sql-reference/statements/attach.md",
    "docs/en/sql-reference/statements/system.md",
    "programs/CMakeLists.txt",
    "src/Access/AccessType.h",
    "src/Common/ErrorCodes.cpp",
    "src/Interpreters/InterpreterCreateQuery.cpp",
    "src/Interpreters/InterpreterInsertQuery.cpp",
    "src/Interpreters/InterpreterSystemQuery.cpp",
    "src/Interpreters/InterpreterSystemQuery.h",
    "src/Interpreters/executeDDLQueryOnCluster.cpp",
    "src/Parsers/ASTSystemQuery.cpp",
    "src/Parsers/ASTSystemQuery.h",
    "src/Parsers/New/AST/SystemQuery.cpp",
    "src/Parsers/ParserRenameQuery.cpp",
    "src/Parsers/ParserSystemQuery.cpp",
    "src/Storages/IStorage.h",
    "src/Storages/MergeTree/IMergeTreeDataPart.h",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeData.h",
    "src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp",
    "src/Storages/MergeTree/registerStorageMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "tests/integration/test_attach_without_fetching/test.py",
    "tests/integration/test_grant_and_revoke/test.py",
    "b/tests/integration/test_restore_replica/configs/remote_servers.xml",
    "b/tests/integration/test_restore_replica/test.py",
    "tests/queries/0_stateless/01271_show_privileges.reference"
  ]
}