{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 36572,
  "instance_id": "ClickHouse__ClickHouse-36572",
  "issue_numbers": [
    "35483"
  ],
  "base_commit": "f11cfdca94649ae31f4fe8d5ecb91072c7d06f0c",
  "patch": "diff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp\nindex 09b738a516f3..b59777b0a85b 100644\n--- a/src/Common/ProfileEvents.cpp\n+++ b/src/Common/ProfileEvents.cpp\n@@ -240,18 +240,23 @@\n     M(NotCreatedLogEntryForMutation, \"Log entry to mutate parts in ReplicatedMergeTree is not created due to concurrent log update by another replica.\") \\\n     \\\n     M(S3ReadMicroseconds, \"Time of GET and HEAD requests to S3 storage.\") \\\n-    M(S3ReadBytes, \"Read bytes (incoming) in GET and HEAD requests to S3 storage.\") \\\n     M(S3ReadRequestsCount, \"Number of GET and HEAD requests to S3 storage.\") \\\n     M(S3ReadRequestsErrors, \"Number of non-throttling errors in GET and HEAD requests to S3 storage.\") \\\n     M(S3ReadRequestsThrottling, \"Number of 429 and 503 errors in GET and HEAD requests to S3 storage.\") \\\n     M(S3ReadRequestsRedirects, \"Number of redirects in GET and HEAD requests to S3 storage.\") \\\n     \\\n     M(S3WriteMicroseconds, \"Time of POST, DELETE, PUT and PATCH requests to S3 storage.\") \\\n-    M(S3WriteBytes, \"Write bytes (outgoing) in POST, DELETE, PUT and PATCH requests to S3 storage.\") \\\n     M(S3WriteRequestsCount, \"Number of POST, DELETE, PUT and PATCH requests to S3 storage.\") \\\n     M(S3WriteRequestsErrors, \"Number of non-throttling errors in POST, DELETE, PUT and PATCH requests to S3 storage.\") \\\n     M(S3WriteRequestsThrottling, \"Number of 429 and 503 errors in POST, DELETE, PUT and PATCH requests to S3 storage.\") \\\n     M(S3WriteRequestsRedirects, \"Number of redirects in POST, DELETE, PUT and PATCH requests to S3 storage.\") \\\n+    \\\n+    M(ReadBufferFromS3Microseconds, \"Time spend in reading from S3.\") \\\n+    M(ReadBufferFromS3Bytes, \"Bytes read from S3.\") \\\n+    M(ReadBufferFromS3RequestsErrors, \"Number of exceptions while reading from S3.\") \\\n+    \\\n+    M(WriteBufferFromS3Bytes, \"Bytes written to S3.\") \\\n+    \\\n     M(QueryMemoryLimitExceeded, \"Number of times when memory limit exceeded for query.\") \\\n     \\\n     M(RemoteFSReadMicroseconds, \"Time of reading from remote filesystem.\") \\\ndiff --git a/src/IO/ReadBufferFromS3.cpp b/src/IO/ReadBufferFromS3.cpp\nindex d497da9c713e..c1b2ec7db0f6 100644\n--- a/src/IO/ReadBufferFromS3.cpp\n+++ b/src/IO/ReadBufferFromS3.cpp\n@@ -19,9 +19,9 @@\n \n namespace ProfileEvents\n {\n-    extern const Event S3ReadMicroseconds;\n-    extern const Event S3ReadBytes;\n-    extern const Event S3ReadRequestsErrors;\n+    extern const Event ReadBufferFromS3Microseconds;\n+    extern const Event ReadBufferFromS3Bytes;\n+    extern const Event ReadBufferFromS3RequestsErrors;\n     extern const Event ReadBufferSeekCancelConnection;\n }\n \n@@ -121,14 +121,14 @@ bool ReadBufferFromS3::nextImpl()\n             /// Try to read a next portion of data.\n             next_result = impl->next();\n             watch.stop();\n-            ProfileEvents::increment(ProfileEvents::S3ReadMicroseconds, watch.elapsedMicroseconds());\n+            ProfileEvents::increment(ProfileEvents::ReadBufferFromS3Microseconds, watch.elapsedMicroseconds());\n             break;\n         }\n         catch (const Exception & e)\n         {\n             watch.stop();\n-            ProfileEvents::increment(ProfileEvents::S3ReadMicroseconds, watch.elapsedMicroseconds());\n-            ProfileEvents::increment(ProfileEvents::S3ReadRequestsErrors, 1);\n+            ProfileEvents::increment(ProfileEvents::ReadBufferFromS3Microseconds, watch.elapsedMicroseconds());\n+            ProfileEvents::increment(ProfileEvents::ReadBufferFromS3RequestsErrors, 1);\n \n             LOG_DEBUG(\n                 log,\n@@ -157,7 +157,7 @@ bool ReadBufferFromS3::nextImpl()\n \n     BufferBase::set(impl->buffer().begin(), impl->buffer().size(), impl->offset()); /// use the buffer returned by `impl`\n \n-    ProfileEvents::increment(ProfileEvents::S3ReadBytes, working_buffer.size());\n+    ProfileEvents::increment(ProfileEvents::ReadBufferFromS3Bytes, working_buffer.size());\n     offset += working_buffer.size();\n \n     return true;\ndiff --git a/src/IO/WriteBufferFromS3.cpp b/src/IO/WriteBufferFromS3.cpp\nindex 52a958ec968f..a01c4972ac48 100644\n--- a/src/IO/WriteBufferFromS3.cpp\n+++ b/src/IO/WriteBufferFromS3.cpp\n@@ -20,7 +20,7 @@\n \n namespace ProfileEvents\n {\n-    extern const Event S3WriteBytes;\n+    extern const Event WriteBufferFromS3Bytes;\n     extern const Event RemoteFSCacheDownloadBytes;\n }\n \n@@ -121,7 +121,7 @@ void WriteBufferFromS3::nextImpl()\n         }\n     }\n \n-    ProfileEvents::increment(ProfileEvents::S3WriteBytes, offset());\n+    ProfileEvents::increment(ProfileEvents::WriteBufferFromS3Bytes, offset());\n \n     last_part_size += offset();\n \n",
  "test_patch": "diff --git a/tests/integration/test_profile_events_s3/test.py b/tests/integration/test_profile_events_s3/test.py\nindex 5171ea4ac0ef..aa578a1273a3 100644\n--- a/tests/integration/test_profile_events_s3/test.py\n+++ b/tests/integration/test_profile_events_s3/test.py\n@@ -32,25 +32,27 @@ def cluster():\n \n \n init_list = {\n+    \"ReadBufferFromS3Bytes\": 0,\n     \"S3ReadMicroseconds\": 0,\n-    \"S3ReadBytes\": 0,\n+    \"ReadBufferFromS3Microseconds\": 0,\n+    \"ReadBufferFromS3RequestsErrors\": 0,\n     \"S3ReadRequestsCount\": 0,\n     \"S3ReadRequestsErrorsTotal\": 0,\n     \"S3ReadRequestsErrors503\": 0,\n     \"S3ReadRequestsRedirects\": 0,\n     \"S3WriteMicroseconds\": 0,\n-    \"S3WriteBytes\": 0,\n     \"S3WriteRequestsCount\": 0,\n     \"S3WriteRequestsErrorsTotal\": 0,\n     \"S3WriteRequestsErrors503\": 0,\n     \"S3WriteRequestsRedirects\": 0,\n+    \"WriteBufferFromS3Bytes\": 0,\n }\n \n \n def get_s3_events(instance):\n     result = init_list.copy()\n     events = instance.query(\n-        \"SELECT event,value FROM system.events WHERE event LIKE 'S3%'\"\n+        \"SELECT event,value FROM system.events WHERE event LIKE '%S3%'\"\n     ).split(\"\\n\")\n     for event in events:\n         ev = event.split(\"\\t\")\n@@ -108,7 +110,7 @@ def get_query_stat(instance, hint):\n     for event in events:\n         ev = event.split(\"\\t\")\n         if len(ev) == 2:\n-            if ev[0].startswith(\"S3\"):\n+            if \"S3\" in ev[0]:\n                 result[ev[0]] += int(ev[1])\n     return result\n \n@@ -151,7 +153,9 @@ def test_profile_events(cluster):\n     stat1 = get_query_stat(instance, query1)\n     for metric in stat1:\n         assert stat1[metric] == metrics1[metric] - metrics0[metric]\n-    assert metrics1[\"S3WriteBytes\"] - metrics0[\"S3WriteBytes\"] == size1\n+    assert (\n+        metrics1[\"WriteBufferFromS3Bytes\"] - metrics0[\"WriteBufferFromS3Bytes\"] == size1\n+    )\n \n     query2 = \"INSERT INTO test_s3.test_s3 FORMAT Values\"\n     instance.query(query2 + \" (1,1)\")\n@@ -171,7 +175,10 @@ def test_profile_events(cluster):\n     stat2 = get_query_stat(instance, query2)\n     for metric in stat2:\n         assert stat2[metric] == metrics2[metric] - metrics1[metric]\n-    assert metrics2[\"S3WriteBytes\"] - metrics1[\"S3WriteBytes\"] == size2 - size1\n+    assert (\n+        metrics2[\"WriteBufferFromS3Bytes\"] - metrics1[\"WriteBufferFromS3Bytes\"]\n+        == size2 - size1\n+    )\n \n     query3 = \"SELECT * from test_s3.test_s3\"\n     assert instance.query(query3) == \"1\\t1\\n\"\n",
  "problem_statement": "it looks like S3 profile events numbers are inflated\nProfileEvents S3ReadMicroseconds RealTimeMicroseconds looks inconsistent.\r\n\r\nElapsed: 0.193 sec. / query_duration_ms = 192  \r\nVS   \r\nS3ReadMicroseconds  = 350736 / RealTimeMicroseconds = 383210.\r\n\r\n\r\n```sql\r\nINSERT INTO FUNCTION s3(s3_mydata, url = 'https://s3.us-east-1.amazonaws.com/test..../test_file22222.tsv', structure = 'A Int64', format = 'TSV', compression_method = 'none') SELECT number AS A\r\nFROM numbers(10000)\r\n\r\nset max_threads=1;\r\n\r\n\r\nSELECT count()\r\nFROM s3(s3_mydata, url = 'https://s3.us-east-1.amazonaws.com/test..../test_file22222.tsv', structure = 'A Int64', format = 'TSV', compression_method = 'none')\r\n\r\nQuery id: 2f63b92a-eae3-45c5-a30c-2a8b7681147a\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502   10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.193 sec. Processed 10.00 thousand rows, 80.00 KB (51.72 thousand rows/s., 413.74 KB/s.)\r\n\r\n\r\n\r\nSELECT\r\n    query_duration_ms,\r\n    k,\r\n    v\r\nFROM system.query_log\r\nARRAY JOIN\r\n    ProfileEvents.keys AS k,\r\n    ProfileEvents.values AS v\r\nWHERE (query_id = '2f63b92a-eae3-45c5-a30c-2a8b7681147a') AND (type = 2) AND (event_date = today())\r\n\r\n\u250c\u2500query_duration_ms\u2500\u252c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2510\r\n\u2502               192 \u2502 Query                             \u2502       1 \u2502\r\n\u2502               192 \u2502 SelectQuery                       \u2502       1 \u2502\r\n\u2502               192 \u2502 IOBufferAllocs                    \u2502       1 \u2502\r\n\u2502               192 \u2502 IOBufferAllocBytes                \u2502 1048576 \u2502\r\n\u2502               192 \u2502 ArenaAllocChunks                  \u2502       1 \u2502\r\n\u2502               192 \u2502 ArenaAllocBytes                   \u2502    4096 \u2502\r\n\u2502               192 \u2502 TableFunctionExecute              \u2502       1 \u2502\r\n\u2502               192 \u2502 NetworkReceiveElapsedMicroseconds \u2502       7 \u2502\r\n\u2502               192 \u2502 NetworkSendElapsedMicroseconds    \u2502     214 \u2502\r\n\u2502               192 \u2502 NetworkSendBytes                  \u2502    3427 \u2502\r\n\u2502               192 \u2502 SelectedRows                      \u2502   10000 \u2502\r\n\u2502               192 \u2502 SelectedBytes                     \u2502   80000 \u2502\r\n\u2502               192 \u2502 ContextLock                       \u2502      28 \u2502\r\n\u2502               192 \u2502 RWLockAcquiredReadLocks           \u2502       1 \u2502\r\n\u2502               192 \u2502 RealTimeMicroseconds              \u2502  383210 \u2502  --- ? \r\n\u2502               192 \u2502 UserTimeMicroseconds              \u2502    5004 \u2502\r\n\u2502               192 \u2502 SystemTimeMicroseconds            \u2502     492 \u2502\r\n\u2502               192 \u2502 SoftPageFaults                    \u2502      34 \u2502\r\n\u2502               192 \u2502 OSCPUWaitMicroseconds             \u2502      37 \u2502\r\n\u2502               192 \u2502 OSCPUVirtualTimeMicroseconds      \u2502    5494 \u2502\r\n\u2502               192 \u2502 OSWriteBytes                      \u2502    4096 \u2502\r\n\u2502               192 \u2502 OSReadChars                       \u2502   53248 \u2502\r\n\u2502               192 \u2502 OSWriteChars                      \u2502    2048 \u2502\r\n\u2502               192 \u2502 CreatedHTTPConnections            \u2502       1 \u2502\r\n\u2502               192 \u2502 S3ReadMicroseconds                \u2502  350736 \u2502  --- ?\r\n\u2502               192 \u2502 S3ReadBytes                       \u2502   48890 \u2502\r\n\u2502               192 \u2502 S3ReadRequestsCount               \u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\n",
  "hints_text": "Counters are summarized across multiple threads.\r\nIf two threads have spend 100 ms of real time, the total real time counter will be 200 ms,\r\nbut the query duration can be anywhere between 100 and 200 ms.\nI found strange code by @excitoon and @ianton-ru - they increment the same metric in two different places.\nAlso I found that everything related to the metrics in s3 is a trash.",
  "created_at": "2022-04-24T01:46:18Z"
}