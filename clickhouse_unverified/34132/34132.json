{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 34132,
  "instance_id": "ClickHouse__ClickHouse-34132",
  "issue_numbers": [
    "34091"
  ],
  "base_commit": "aff7bc3e3120c6eb844c4a789aaad68ebac7e50f",
  "patch": "diff --git a/src/QueryPipeline/RemoteInserter.cpp b/src/QueryPipeline/RemoteInserter.cpp\nindex c34c625dc6d3..13d087f0db90 100644\n--- a/src/QueryPipeline/RemoteInserter.cpp\n+++ b/src/QueryPipeline/RemoteInserter.cpp\n@@ -24,7 +24,9 @@ RemoteInserter::RemoteInserter(\n     const String & query_,\n     const Settings & settings_,\n     const ClientInfo & client_info_)\n-    : connection(connection_), query(query_)\n+    : connection(connection_)\n+    , query(query_)\n+    , server_revision(connection.getServerRevision(timeouts))\n {\n     ClientInfo modified_client_info = client_info_;\n     modified_client_info.query_kind = ClientInfo::QueryKind::SECONDARY_QUERY;\ndiff --git a/src/QueryPipeline/RemoteInserter.h b/src/QueryPipeline/RemoteInserter.h\nindex 0688b555825f..5b5de962cc64 100644\n--- a/src/QueryPipeline/RemoteInserter.h\n+++ b/src/QueryPipeline/RemoteInserter.h\n@@ -35,12 +35,14 @@ class RemoteInserter\n     ~RemoteInserter();\n \n     const Block & getHeader() const { return header; }\n+    UInt64 getServerRevision() const { return server_revision; }\n \n private:\n     Connection & connection;\n     String query;\n     Block header;\n     bool finished = false;\n+    UInt64 server_revision;\n };\n \n }\ndiff --git a/src/Storages/Distributed/DirectoryMonitor.cpp b/src/Storages/Distributed/DirectoryMonitor.cpp\nindex 0c41cf713866..d7422b1ddbcc 100644\n--- a/src/Storages/Distributed/DirectoryMonitor.cpp\n+++ b/src/Storages/Distributed/DirectoryMonitor.cpp\n@@ -132,6 +132,7 @@ namespace\n \n     struct DistributedHeader\n     {\n+        UInt64 revision = 0;\n         Settings insert_settings;\n         std::string insert_query;\n         ClientInfo client_info;\n@@ -166,9 +167,8 @@ namespace\n             /// Read the parts of the header.\n             ReadBufferFromString header_buf(header_data);\n \n-            UInt64 initiator_revision;\n-            readVarUInt(initiator_revision, header_buf);\n-            if (DBMS_TCP_PROTOCOL_VERSION < initiator_revision)\n+            readVarUInt(distributed_header.revision, header_buf);\n+            if (DBMS_TCP_PROTOCOL_VERSION < distributed_header.revision)\n             {\n                 LOG_WARNING(log, \"ClickHouse shard version is older than ClickHouse initiator version. It may lack support for new features.\");\n             }\n@@ -177,7 +177,7 @@ namespace\n             distributed_header.insert_settings.read(header_buf);\n \n             if (header_buf.hasPendingData())\n-                distributed_header.client_info.read(header_buf, initiator_revision);\n+                distributed_header.client_info.read(header_buf, distributed_header.revision);\n \n             if (header_buf.hasPendingData())\n             {\n@@ -188,10 +188,12 @@ namespace\n \n             if (header_buf.hasPendingData())\n             {\n-                NativeReader header_block_in(header_buf, DBMS_TCP_PROTOCOL_VERSION);\n+                NativeReader header_block_in(header_buf, distributed_header.revision);\n                 distributed_header.block_header = header_block_in.read();\n                 if (!distributed_header.block_header)\n-                    throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Cannot read header from the {} batch\", in.getFileName());\n+                    throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA,\n+                        \"Cannot read header from the {} batch. Data was written with protocol version {}, current version: {}\",\n+                            in.getFileName(), distributed_header.revision, DBMS_TCP_PROTOCOL_VERSION);\n             }\n \n             /// Add handling new data here, for example:\n@@ -264,10 +266,10 @@ namespace\n         return nullptr;\n     }\n \n-    void writeAndConvert(RemoteInserter & remote, ReadBufferFromFile & in)\n+    void writeAndConvert(RemoteInserter & remote, const DistributedHeader & distributed_header, ReadBufferFromFile & in)\n     {\n         CompressedReadBuffer decompressing_in(in);\n-        NativeReader block_in(decompressing_in, DBMS_TCP_PROTOCOL_VERSION);\n+        NativeReader block_in(decompressing_in, distributed_header.revision);\n \n         while (Block block = block_in.read())\n         {\n@@ -304,7 +306,7 @@ namespace\n         {\n             LOG_TRACE(log, \"Processing batch {} with old format (no header)\", in.getFileName());\n \n-            writeAndConvert(remote, in);\n+            writeAndConvert(remote, distributed_header, in);\n             return;\n         }\n \n@@ -314,14 +316,20 @@ namespace\n                 \"Structure does not match (remote: {}, local: {}), implicit conversion will be done\",\n                 remote.getHeader().dumpStructure(), distributed_header.block_header.dumpStructure());\n \n-            writeAndConvert(remote, in);\n+            writeAndConvert(remote, distributed_header, in);\n             return;\n         }\n \n         /// If connection does not use compression, we have to uncompress the data.\n         if (!compression_expected)\n         {\n-            writeAndConvert(remote, in);\n+            writeAndConvert(remote, distributed_header, in);\n+            return;\n+        }\n+\n+        if (distributed_header.revision != remote.getServerRevision())\n+        {\n+            writeAndConvert(remote, distributed_header, in);\n             return;\n         }\n \n@@ -915,10 +923,10 @@ class DirectoryMonitorSource : public SourceWithProgress\n         {\n             in = std::make_unique<ReadBufferFromFile>(file_name);\n             decompressing_in = std::make_unique<CompressedReadBuffer>(*in);\n-            block_in = std::make_unique<NativeReader>(*decompressing_in, DBMS_TCP_PROTOCOL_VERSION);\n             log = &Poco::Logger::get(\"DirectoryMonitorSource\");\n \n-            readDistributedHeader(*in, log);\n+            auto distributed_header = readDistributedHeader(*in, log);\n+            block_in = std::make_unique<NativeReader>(*decompressing_in, distributed_header.revision);\n \n             first_block = block_in->read();\n         }\n@@ -1040,7 +1048,7 @@ void StorageDistributedDirectoryMonitor::processFilesWithBatching(const std::map\n                 LOG_DEBUG(log, \"Processing batch {} with old format (no header/rows)\", in.getFileName());\n \n                 CompressedReadBuffer decompressing_in(in);\n-                NativeReader block_in(decompressing_in, DBMS_TCP_PROTOCOL_VERSION);\n+                NativeReader block_in(decompressing_in, distributed_header.revision);\n \n                 while (Block block = block_in.read())\n                 {\n",
  "test_patch": "diff --git a/tests/integration/test_distributed_insert_backward_compatibility/__init__.py b/tests/integration/test_distributed_insert_backward_compatibility/__init__.py\nnew file mode 100644\nindex 000000000000..8b137891791f\n--- /dev/null\n+++ b/tests/integration/test_distributed_insert_backward_compatibility/__init__.py\n@@ -0,0 +1,1 @@\n+\ndiff --git a/tests/integration/test_distributed_insert_backward_compatibility/configs/remote_servers.xml b/tests/integration/test_distributed_insert_backward_compatibility/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..9c7f02c190fb\n--- /dev/null\n+++ b/tests/integration/test_distributed_insert_backward_compatibility/configs/remote_servers.xml\n@@ -0,0 +1,12 @@\n+<clickhouse>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <replica>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+</clickhouse>\ndiff --git a/tests/integration/test_distributed_insert_backward_compatibility/test.py b/tests/integration/test_distributed_insert_backward_compatibility/test.py\nnew file mode 100644\nindex 000000000000..ba7d8e0a25d9\n--- /dev/null\n+++ b/tests/integration/test_distributed_insert_backward_compatibility/test.py\n@@ -0,0 +1,39 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.client import QueryRuntimeException\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node_shard = cluster.add_instance('node1', main_configs=['configs/remote_servers.xml'])\n+\n+node_dist = cluster.add_instance('node2', main_configs=['configs/remote_servers.xml'], image='yandex/clickhouse-server',\n+                                tag='21.11.9.1', stay_alive=True, with_installed_binary=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        node_shard.query(\"CREATE TABLE local_table(id UInt32, val String) ENGINE = MergeTree ORDER BY id\")\n+        node_dist.query(\"CREATE TABLE local_table(id UInt32, val String) ENGINE = MergeTree ORDER BY id\")\n+        node_dist.query(\"CREATE TABLE dist_table(id UInt32, val String) ENGINE = Distributed(test_cluster, default, local_table, rand())\")\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_distributed_in_tuple(started_cluster):\n+    node_dist.query(\"SYSTEM STOP DISTRIBUTED SENDS dist_table\")\n+\n+    node_dist.query(\"INSERT INTO dist_table VALUES (1, 'foo')\")\n+    assert node_dist.query(\"SELECT count() FROM dist_table\") == \"0\\n\"\n+    assert node_shard.query(\"SELECT count() FROM local_table\") == \"0\\n\"\n+\n+    node_dist.restart_with_latest_version(signal=9)\n+    node_dist.query(\"SYSTEM FLUSH DISTRIBUTED dist_table\")\n+\n+    assert node_dist.query(\"SELECT count() FROM dist_table\") == \"1\\n\"\n+    assert node_shard.query(\"SELECT count() FROM local_table\") == \"1\\n\"\n",
  "problem_statement": "unknown serialization kind on distributed table\n\r\n**Describe the unexpected behaviour**\r\nAfter upgraded to ClickHouse 22.1.3 revision 54455,when using distributed table to insert data,errors occured in log.\r\n\r\n**How to reproduce**\r\nVersion:ClickHouse 22.1.3.7\r\n\r\n**Expected behavior**\r\nAfter upgraded to ClickHouse 22.1.3 revision 54455,when using distributed table to insert data,errors occured in log.\r\nCluster's ClickHouse version are the same,22.1.3.7.\r\n\r\n**Error message and/or stacktrace**\r\n2022.01.28 18:39:41.098174 [ 80700 ] {} <Error> XXX.YYYY.DirectoryMonitor: Code: 246. DB::Exception: Unknown serialization kind 99: While sending /clickhouse/store/4db/4db5491f-555c-4c9d-8db5-491f555cbc9d/shard7_replica1/1571210.bin. (CORRUPTED_DATA), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa82d07a in /usr/bin/clickhouse\r\n1. DB::SerializationInfo::deserializeFromKindsBinary(DB::ReadBuffer&) @ 0x1326c09c in /usr/bin/clickhouse\r\n2. DB::NativeReader::read() @ 0x14826428 in /usr/bin/clickhouse\r\n3. ? @ 0x14195913 in /usr/bin/clickhouse\r\n4. DB::StorageDistributedDirectoryMonitor::processFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1419391a in /usr/bin/clickhouse\r\n5. DB::StorageDistributedDirectoryMonitor::run() @ 0x1418fa78 in /usr/bin/clickhouse\r\n6. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0x12f8470e in /usr/bin/clickhouse\r\n7. DB::BackgroundSchedulePool::threadFunction() @ 0x12f870a7 in /usr/bin/clickhouse\r\n8. ? @ 0x12f88170 in /usr/bin/clickhouse\r\n9. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xa86f4b7 in /usr/bin/clickhouse\r\n10. ? @ 0xa872ebd in /usr/bin/clickhouse\r\n11. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n12. __clone @ 0xfeb0d in /usr/lib64/libc-2.17.so\r\n (version 22.1.3.7 (official build))\r\n\r\n\n",
  "hints_text": "Please, can you share the schema of tables?\nWe are also facing similar issue after upgrading to 22.1.3.7\ncreate table XXX.YYYY(\r\nBEGINTIME DateTime,\r\nABCD String,\r\nDEFG String,\r\nFlag LowCardinality(String),\r\nQWER String,\r\nTYUI Int32,\r\nFGHJ String,\r\nZXCV String\r\n)\r\nengine =Distributed('example','blah','blah_base',rand());\r\n\r\ncreate table blah.blah_base(\r\nBEGINTIME DateTime,\r\nABCD String,\r\nDEFG String,\r\nFlag LowCardinality(String),\r\nQWER String,\r\nTYUI Int32,\r\nFGHJ String,\r\nZXCV String\r\n)\r\nengine=MergeTree PARTITION BY toYYYYMM(BEGINTIME)\r\nORDER BY (ABCD,DEFG)\r\nSETTINGS index_granularity=8192;\r\n\r\nBesides,unknown serialization kind include 99,85,117.\nI reproduced it with the following scenario: after inserts on old version there should remain files with unflushed to shards inserted data. Then after upgrade data cannot be read back and sent to shards, because version of Native format (in which temporary  data is written) is accounted incorrectly. The Native protocol was changed in 22.1 and its version was incremented. I'll try to fix it soon.\r\n\r\nIf you want to upgrade now, then you can do the following:\r\n1. Rollback to old version. \r\n2. Find directory `broken` in the directory with temporary inserted data of distributed table and move all files (if there exists some) from it back to the directory with temporary inserted data.\r\n3. Stop inserts to distributed table.\r\n4. Run `SYSTEM FLUSH DISTRIBUTED ...` query.\r\n5. Upgrade to 22.1",
  "created_at": "2022-01-29T00:28:16Z"
}