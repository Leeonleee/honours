{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 69376,
  "instance_id": "ClickHouse__ClickHouse-69376",
  "issue_numbers": [
    "65951"
  ],
  "base_commit": "bab574d674d51cfee0c83c904adc6772b4653d4d",
  "patch": "diff --git a/docs/en/operations/settings/merge-tree-settings.md b/docs/en/operations/settings/merge-tree-settings.md\nindex a13aacc76e6d..376c1c66ad56 100644\n--- a/docs/en/operations/settings/merge-tree-settings.md\n+++ b/docs/en/operations/settings/merge-tree-settings.md\n@@ -156,6 +156,26 @@ Default value: 1000.\n \n ClickHouse artificially executes `INSERT` longer (adds \u2018sleep\u2019) so that the background merge process can merge parts faster than they are added.\n \n+## min_free_disk_bytes_to_throw_insert {#min_free_disk_bytes_to_throw_insert}\n+\n+The minimum number of bytes that should be free in disk space in order to insert data. If the number of available free bytes - `keep_free_space_bytes` is less than `min_free_disk_bytes_to_throw_insert` then an exception is thrown and the insert is not executed. Note that this setting does not take into account the amount of data that will be written by the `INSERT` operation.\n+\n+Possible values:\n+\n+- Any positive integer.\n+\n+Default value: 0 bytes.\n+\n+## min_free_disk_ratio_to_throw_insert {#min_free_disk_ratio_to_throw_insert}\n+\n+The minimum free to total disk space ratio to perform an `INSERT`. The free space is calculated by subtracting `keep_free_space_bytes` from the total available space in disk.\n+\n+Possible values:\n+\n+- Float, 0.0 - 1.0\n+\n+Default value: 0.0\n+\n ## inactive_parts_to_throw_insert {#inactive-parts-to-throw-insert}\n \n If the number of inactive parts in a single partition more than the `inactive_parts_to_throw_insert` value, `INSERT` is interrupted with the \"Too many inactive parts (N). Parts cleaning are processing significantly slower than inserts\" exception.\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 1cd977f6725a..7da75cf90a0b 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -343,6 +343,8 @@ class IColumn;\n     M(Int64, distributed_ddl_task_timeout, 180, \"Timeout for DDL query responses from all hosts in cluster. If a ddl request has not been performed on all hosts, a response will contain a timeout error and a request will be executed in an async mode. Negative value means infinite. Zero means async mode.\", 0) \\\n     M(Milliseconds, stream_flush_interval_ms, 7500, \"Timeout for flushing data from streaming storages.\", 0) \\\n     M(Milliseconds, stream_poll_timeout_ms, 500, \"Timeout for polling data from/to streaming storages.\", 0) \\\n+    M(UInt64, min_free_disk_bytes_to_throw_insert, 0, \"Minimum free disk space bytes to throw an insert.\", 0) \\\n+    M(Double, min_free_disk_ratio_to_throw_insert, 0.0, \"Minimum free disk space ratio to throw an insert.\", 0) \\\n     \\\n     M(Bool, final, false, \"Query with the FINAL modifier by default. If the engine does not support final, it does not have any effect. On queries with multiple tables final is applied only on those that support it. It also works on distributed tables\", 0) \\\n     \\\ndiff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp\nindex eb47c221c3d4..609f2aad1d10 100644\n--- a/src/Core/SettingsChangesHistory.cpp\n+++ b/src/Core/SettingsChangesHistory.cpp\n@@ -83,7 +83,9 @@ static std::initializer_list<std::pair<ClickHouseVersion, SettingsChangesHistory\n             {\"parallel_replicas_local_plan\", false, false, \"Use local plan for local replica in a query with parallel replicas\"},\n             {\"join_to_sort_minimum_perkey_rows\", 0, 40, \"The lower limit of per-key average rows in the right table to determine whether to rerange the right table by key in left or inner join. This setting ensures that the optimization is not applied for sparse table keys\"},\n             {\"join_to_sort_maximum_table_rows\", 0, 10000, \"The maximum number of rows in the right table to determine whether to rerange the right table by key in left or inner join\"},\n-            {\"allow_experimental_join_right_table_sorting\", false, false, \"If it is set to true, and the conditions of `join_to_sort_minimum_perkey_rows` and `join_to_sort_maximum_table_rows` are met, rerange the right table by key to improve the performance in left or inner hash join\"}\n+            {\"allow_experimental_join_right_table_sorting\", false, false, \"If it is set to true, and the conditions of `join_to_sort_minimum_perkey_rows` and `join_to_sort_maximum_table_rows` are met, rerange the right table by key to improve the performance in left or inner hash join\"},\n+            {\"min_free_disk_bytes_to_throw_insert\", 0, 0, \"Maintain some free disk space bytes from inserts while still allowing for temporary writing.\"},\n+            {\"min_free_disk_ratio_to_throw_insert\", 0.0, 0.0, \"Maintain some free disk space bytes expressed as ratio to total disk space from inserts while still allowing for temporary writing.\"},\n         }\n     },\n     {\"24.8\",\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex f29d715e791d..e766ae01dfcb 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -60,6 +60,7 @@ namespace ErrorCodes\n     extern const int ABORTED;\n     extern const int LOGICAL_ERROR;\n     extern const int TOO_MANY_PARTS;\n+    extern const int NOT_ENOUGH_SPACE;\n }\n \n namespace\n@@ -553,6 +554,32 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     VolumePtr volume = data.getStoragePolicy()->getVolume(0);\n     VolumePtr data_part_volume = createVolumeFromReservation(reservation, volume);\n \n+    const auto & data_settings = data.getSettings();\n+    const UInt64 min_bytes = data_settings->min_free_disk_bytes_to_throw_insert;\n+    const Float64 min_ratio = data_settings->min_free_disk_ratio_to_throw_insert;\n+\n+    if (min_bytes > 0 || min_ratio > 0.0)\n+    {\n+        const auto disk = data_part_volume->getDisk();\n+        const UInt64 total_disk_bytes = *disk->getTotalSpace();\n+        const UInt64 free_disk_bytes = *disk->getAvailableSpace();\n+\n+        const UInt64 min_bytes_from_ratio = static_cast<UInt64>(min_ratio * total_disk_bytes);\n+        const UInt64 needed_free_bytes = std::max(min_bytes, min_bytes_from_ratio);\n+\n+        if (needed_free_bytes > free_disk_bytes)\n+        {\n+            throw Exception(\n+                ErrorCodes::NOT_ENOUGH_SPACE,\n+                \"Could not perform insert: less than {} free bytes in disk space ({}). \"\n+                \"Configure this limit with user settings {} or {}\",\n+                needed_free_bytes,\n+                free_disk_bytes,\n+                \"min_free_disk_bytes_to_throw_insert\",\n+                \"min_free_disk_ratio_to_throw_insert\");\n+        }\n+    }\n+\n     auto new_data_part = data.getDataPartBuilder(part_name, data_part_volume, part_dir)\n         .withPartFormat(data.choosePartFormat(expected_size, block.rows()))\n         .withPartInfo(new_part_info)\n@@ -564,8 +591,6 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     if (data.storage_settings.get()->assign_part_uuids)\n         new_data_part->uuid = UUIDHelpers::generateV4();\n \n-    const auto & data_settings = data.getSettings();\n-\n     SerializationInfo::Settings settings{data_settings->ratio_of_defaults_for_sparse_serialization, true};\n     SerializationInfoByName infos(columns, settings);\n     infos.add(block);\n@@ -688,6 +713,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n     MergeTreeDataPartType part_type;\n     /// Size of part would not be greater than block.bytes() + epsilon\n     size_t expected_size = block.bytes();\n+\n     // just check if there is enough space on parent volume\n     MergeTreeData::reserveSpace(expected_size, parent_part->getDataPartStorage());\n     part_type = data.choosePartFormatOnDisk(expected_size, block.rows()).part_type;\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex dcb181551147..b2ebfa1dfda7 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -99,6 +99,8 @@ struct Settings;\n     M(Bool, add_implicit_sign_column_constraint_for_collapsing_engine, false, \"If true, add implicit constraint for sign column for CollapsingMergeTree engine.\", 0) \\\n     M(Milliseconds, sleep_before_commit_local_part_in_replicated_table_ms, 0, \"For testing. Do not change it.\", 0) \\\n     M(Bool, optimize_row_order, false, \"Allow reshuffling of rows during part inserts and merges to improve the compressibility of the new part\", 0) \\\n+    M(UInt64, min_free_disk_bytes_to_throw_insert, 0, \"Minimum free disk space bytes to throw an insert.\", 0) \\\n+    M(Double, min_free_disk_ratio_to_throw_insert, 0.0, \"Minimum free disk space ratio to throw an insert.\", 0) \\\n     M(Bool, use_adaptive_write_buffer_for_dynamic_subcolumns, true, \"Allow to use adaptive writer buffers during writing dynamic subcolumns to reduce memory usage\", 0) \\\n     M(UInt64, adaptive_write_buffer_initial_size, 16 * 1024, \"Initial size of an adaptive write buffer\", 0) \\\n     \\\n",
  "test_patch": "diff --git a/tests/integration/test_stop_insert_when_disk_close_to_full/__init__.py b/tests/integration/test_stop_insert_when_disk_close_to_full/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_stop_insert_when_disk_close_to_full/configs/config.d/storage_configuration.xml b/tests/integration/test_stop_insert_when_disk_close_to_full/configs/config.d/storage_configuration.xml\nnew file mode 100644\nindex 000000000000..d4031ff656cd\n--- /dev/null\n+++ b/tests/integration/test_stop_insert_when_disk_close_to_full/configs/config.d/storage_configuration.xml\n@@ -0,0 +1,19 @@\n+<clickhouse>\n+    <storage_configuration>\n+        <disks>\n+            <disk1>\n+                <type>local</type>\n+                <path>/disk1/</path>\n+            </disk1>\n+        </disks>\n+        <policies>\n+            <only_disk1>\n+                <volumes>\n+                    <main>\n+                        <disk>disk1</disk>\n+                    </main>\n+                </volumes>\n+            </only_disk1>\n+        </policies>\n+    </storage_configuration>\n+</clickhouse>\ndiff --git a/tests/integration/test_stop_insert_when_disk_close_to_full/test.py b/tests/integration/test_stop_insert_when_disk_close_to_full/test.py\nnew file mode 100644\nindex 000000000000..328de674de1f\n--- /dev/null\n+++ b/tests/integration/test_stop_insert_when_disk_close_to_full/test.py\n@@ -0,0 +1,61 @@\n+import pytest\n+from helpers.cluster import ClickHouseCluster, ClickHouseInstance\n+from helpers.client import QueryRuntimeException\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node = cluster.add_instance(\n+    \"node\",\n+    main_configs=[\"configs/config.d/storage_configuration.xml\"],\n+    tmpfs=[\"/disk1:size=7M\"],\n+    macros={\"shard\": 0, \"replica\": 1},\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_insert_stops_when_disk_full(start_cluster):\n+    min_free_bytes = 3 * 1024 * 1024  # 3 MiB\n+\n+    node.query(\n+        f\"\"\"\n+        CREATE TABLE test_table (\n+            id UInt32,\n+            data String\n+        ) ENGINE = MergeTree()\n+        ORDER BY id\n+        SETTINGS storage_policy = 'only_disk1', min_free_disk_bytes_to_throw_insert = {min_free_bytes}\n+    \"\"\"\n+    )\n+\n+    count = 0\n+\n+    # Insert data to fill up disk\n+    try:\n+        for _ in range(100000):\n+            node.query(\n+                \"INSERT INTO test_table SELECT number, repeat('a', 1000 * 1000) FROM numbers(1)\"\n+            )\n+            count += 1\n+    except QueryRuntimeException as e:\n+        assert \"Could not perform insert\" in str(e)\n+        assert \"free bytes in disk space\" in str(e)\n+\n+    free_space = int(\n+        node.query(\"SELECT free_space FROM system.disks WHERE name = 'disk1'\").strip()\n+    )\n+    assert (\n+        free_space <= min_free_bytes\n+    ), f\"Free space ({free_space}) is less than min_free_bytes ({min_free_bytes})\"\n+\n+    rows = int(node.query(\"SELECT count() from test_table\").strip())\n+    assert rows == count\n+\n+    node.query(\"DROP TABLE test_table\")\n",
  "problem_statement": "Add setting to stop INSERTs when disk space is close to 100%\n**Use Case**\r\n\r\nWhen ClickHouse fills the disk to 100%, it creates several negative side effects: inability to perform certain SELECT queries (which require temporary data), potential file corruption, inability to write logs, and other cascading issues that complicate recovery. It's preferable to stop writes slightly earlier to avoid these problems.\r\n\r\nThe same logic as `parts_to_throw_insert` can be applied here. Eventually, writes will fail as the disk becomes full; it's better if they fail slightly earlier to prevent cascading problems.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIntroduce user-level settings: `min_free_diskspace_bytes_to_throw_insert ` (default 10GB) and `min_free_diskspace_ratio_to_throw_insert ` (default 0.02, i.e., 2%). When these settings are non-zero, during an INSERT operation, ClickHouse will ensure that the storage policy of the target table has at least `min_free_diskspace_bytes_to_throw_insert` bytes or `min_free_diskspace_ratio_to_throw_insert` free space available. If not, it will throw an exception.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nRelying on user discipline, monitoring, and alerting has proven to be ineffective.\r\n\r\n**Additional context**\r\n\r\nThrottling / delaying / slowing down the inserts in that conditions is more controversial, so I don't think it's really needed.\r\n\r\nMay be we can also make the logic a bit more complex - by checking if the default drive have that amount of free disks space AND there is at least one disk in storage policy of the table with that amount of the free disk space.\r\n\n",
  "hints_text": "> potential file corruption\r\n\r\nThis is probably just a speculation and should not be possible.\nI saw that few times on practice (several months ago last time, so have no extra info currently, if will see again - will try to collect more details if it will be possible).\r\n\r\nI'm not 100% sure about the exact mechanics there, but i suspect it caused by lack of fsync \r\n\r\nAFAIK normally the `write` should take into the account both real free space & size of dirty pages, but i'm not sure if's true for all the cases / filesystems / kernels / modes of write.\r\n\r\nReason for reflection: `ENOSPC` is a valid error code for `fsync`... See [man](https://man7.org/linux/man-pages/man2/fsync.2.html), some relevant discussions: [stackoverflow](https://stackoverflow.com/questions/42434872/writing-programs-to-cope-with-i-o-errors-causing-lost-writes-on-linux), [postgresql wiki](https://wiki.postgresql.org/wiki/ENOSPC), [some study](https://ramalagappan.github.io/pdfs/papers/cuttlefs.pdf), https://news.ycombinator.com/item?id=19238121, fsyncgate etc.\n@filimonov I've actually been looking at this too - thanks for raising it.\r\n\r\nAre you already aware of `keep_free_space_bytes`?\r\nhttps://clickhouse.com/docs/en/operations/system-tables/disks\r\n\r\nThis is a value in bytes on the disk storage policy that somewhat achieves this and results in INSERTs being rejected if you configure it to keep say 2% of disk space free.\r\n\r\nThough one behaviour that I'm less sure about and don't really like is that it affects internal operations like merges, as this setting is evaluated as part of getting free space when scheduling merges.\r\n\r\nI'm curious what your thoughts are on this setting.  \nIndeed, may be we can achieve similar effect with that keep_free_space_bytes...\r\n\r\nThat requires the user / dba to remember to configure that on every disk (not so handy), and may give a bit confusing error messages, and will impact all attemps to reserve space on that disk (not only inserts).\nAlso if it will be a user level setting users will be able to temporary relax it. Or even different limits can be used for different users,\n@filimonov @alexey-milovidov any objections to me taking this and making a PR? \r\n\r\nWe have an intern interested in working with ClickHouse, and would like to work on this with them.",
  "created_at": "2024-09-08T14:46:29Z",
  "modified_files": [
    "docs/en/operations/settings/merge-tree-settings.md",
    "src/Core/Settings.h",
    "src/Core/SettingsChangesHistory.cpp",
    "src/Storages/MergeTree/MergeTreeDataWriter.cpp",
    "src/Storages/MergeTree/MergeTreeSettings.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_stop_insert_when_disk_close_to_full/configs/config.d/storage_configuration.xml",
    "b/tests/integration/test_stop_insert_when_disk_close_to_full/test.py"
  ]
}