{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 8958,
  "instance_id": "ClickHouse__ClickHouse-8958",
  "issue_numbers": [
    "8948"
  ],
  "base_commit": "baf218b4317233dbffcf6683be04d8b970900adb",
  "patch": "diff --git a/dbms/src/DataStreams/ParallelParsingBlockInputStream.cpp b/dbms/src/DataStreams/ParallelParsingBlockInputStream.cpp\nindex 3f6ddbd7a15f..3669e77080b2 100644\n--- a/dbms/src/DataStreams/ParallelParsingBlockInputStream.cpp\n+++ b/dbms/src/DataStreams/ParallelParsingBlockInputStream.cpp\n@@ -34,7 +34,7 @@ void ParallelParsingBlockInputStream::segmentatorThreadFunction()\n \n             unit.is_last = !have_more_data;\n             unit.status = READY_TO_PARSE;\n-            scheduleParserThreadForUnitWithNumber(current_unit_number);\n+            scheduleParserThreadForUnitWithNumber(segmentator_ticket_number);\n             ++segmentator_ticket_number;\n \n             if (!have_more_data)\n@@ -49,12 +49,13 @@ void ParallelParsingBlockInputStream::segmentatorThreadFunction()\n     }\n }\n \n-void ParallelParsingBlockInputStream::parserThreadFunction(size_t current_unit_number)\n+void ParallelParsingBlockInputStream::parserThreadFunction(size_t current_ticket_number)\n {\n     try\n     {\n         setThreadName(\"ChunkParser\");\n \n+        const auto current_unit_number = current_ticket_number % processing_units.size();\n         auto & unit = processing_units[current_unit_number];\n \n         /*\n@@ -64,9 +65,9 @@ void ParallelParsingBlockInputStream::parserThreadFunction(size_t current_unit_n\n          * can use it from multiple threads simultaneously.\n          */\n         ReadBuffer read_buffer(unit.segment.data(), unit.segment.size(), 0);\n-        auto parser = std::make_unique<InputStreamFromInputFormat>(\n-                input_processor_creator(read_buffer, header,\n-                row_input_format_params, format_settings));\n+        auto format = input_processor_creator(read_buffer, header, row_input_format_params, format_settings);\n+        format->setCurrentUnitNumber(current_ticket_number);\n+        auto parser = std::make_unique<InputStreamFromInputFormat>(std::move(format));\n \n         unit.block_ext.block.clear();\n         unit.block_ext.block_missing_values.clear();\ndiff --git a/dbms/src/DataStreams/ParallelParsingBlockInputStream.h b/dbms/src/DataStreams/ParallelParsingBlockInputStream.h\nindex 89a9d7c8926b..1b2bfbd52e2c 100644\n--- a/dbms/src/DataStreams/ParallelParsingBlockInputStream.h\n+++ b/dbms/src/DataStreams/ParallelParsingBlockInputStream.h\n@@ -213,9 +213,9 @@ class ParallelParsingBlockInputStream : public IBlockInputStream\n     std::deque<ProcessingUnit> processing_units;\n \n \n-    void scheduleParserThreadForUnitWithNumber(size_t unit_number)\n+    void scheduleParserThreadForUnitWithNumber(size_t ticket_number)\n     {\n-        pool.scheduleOrThrowOnError(std::bind(&ParallelParsingBlockInputStream::parserThreadFunction, this, unit_number));\n+        pool.scheduleOrThrowOnError(std::bind(&ParallelParsingBlockInputStream::parserThreadFunction, this, ticket_number));\n     }\n \n     void finishAndWait()\ndiff --git a/dbms/src/Formats/FormatFactory.cpp b/dbms/src/Formats/FormatFactory.cpp\nindex b67c659ee898..e6c01d81b816 100644\n--- a/dbms/src/Formats/FormatFactory.cpp\n+++ b/dbms/src/Formats/FormatFactory.cpp\n@@ -144,9 +144,19 @@ BlockInputStreamPtr FormatFactory::getInput(\n \n     // Doesn't make sense to use parallel parsing with less than four threads\n     // (segmentator + two parsers + reader).\n-    if (settings.input_format_parallel_parsing\n-        && file_segmentation_engine\n-        && settings.max_threads >= 4)\n+    bool parallel_parsing = settings.input_format_parallel_parsing && file_segmentation_engine && settings.max_threads >= 4;\n+\n+    if (parallel_parsing && name == \"JSONEachRow\")\n+    {\n+        /// FIXME ParallelParsingBlockInputStream doesn't support formats with non-trivial readPrefix() and readSuffix()\n+\n+        /// For JSONEachRow we can safely skip whitespace characters\n+        skipWhitespaceIfAny(buf);\n+        if (buf.eof() || *buf.position() == '[')\n+            parallel_parsing = false; /// Disable it for JSONEachRow if data is in square brackets (see JSONEachRowRowInputFormat)\n+    }\n+\n+    if (parallel_parsing)\n     {\n         const auto & input_getter = getCreators(name).input_processor_creator;\n         if (!input_getter)\ndiff --git a/dbms/src/Processors/Formats/IInputFormat.h b/dbms/src/Processors/Formats/IInputFormat.h\nindex 00cb38405cf7..e1537aff6c56 100644\n--- a/dbms/src/Processors/Formats/IInputFormat.h\n+++ b/dbms/src/Processors/Formats/IInputFormat.h\n@@ -38,6 +38,13 @@ class IInputFormat : public ISource\n         static const BlockMissingValues none;\n         return none;\n     }\n+\n+    size_t getCurrentUnitNumber() const { return current_unit_number; }\n+    void setCurrentUnitNumber(size_t current_unit_number_) { current_unit_number = current_unit_number_; }\n+\n+private:\n+    /// Number of currently parsed chunk (if parallel parsing is enabled)\n+    size_t current_unit_number = 0;\n };\n \n }\ndiff --git a/dbms/src/Processors/Formats/IRowInputFormat.h b/dbms/src/Processors/Formats/IRowInputFormat.h\nindex 436b358cb053..1931fba2a0dc 100644\n--- a/dbms/src/Processors/Formats/IRowInputFormat.h\n+++ b/dbms/src/Processors/Formats/IRowInputFormat.h\n@@ -77,6 +77,8 @@ class IRowInputFormat : public IInputFormat\n \n     const BlockMissingValues & getMissingValues() const override { return block_missing_values; }\n \n+    size_t getTotalRows() const { return total_rows; }\n+\n private:\n     Params params;\n \ndiff --git a/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.cpp b/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.cpp\nindex e4eea18f25c8..d9cd8a21769e 100644\n--- a/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.cpp\n+++ b/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.cpp\n@@ -216,46 +216,37 @@ void JSONEachRowRowInputFormat::readNestedData(const String & name, MutableColum\n \n bool JSONEachRowRowInputFormat::readRow(MutableColumns & columns, RowReadExtension & ext)\n {\n-    /// Set flag data_in_square_brackets if data starts with '['.\n-    if (!in.eof() && parsing_stage == ParsingStage::START)\n-    {\n-        parsing_stage = ParsingStage::PROCESS;\n-        skipWhitespaceIfAny(in);\n-        if (*in.position() == '[')\n-        {\n-            data_in_square_brackets = true;\n-            ++in.position();\n-        }\n-    }\n-\n+    if (!allow_new_rows)\n+        return false;\n     skipWhitespaceIfAny(in);\n \n-    /// We consume ;, or \\n before scanning a new row, instead scanning to next row at the end.\n+    /// We consume , or \\n before scanning a new row, instead scanning to next row at the end.\n     /// The reason is that if we want an exact number of rows read with LIMIT x\n     /// from a streaming table engine with text data format, like File or Kafka\n     /// then seeking to next ;, or \\n would trigger reading of an extra row at the end.\n \n     /// Semicolon is added for convenience as it could be used at end of INSERT query.\n-    if (!in.eof() && (*in.position() == ',' || *in.position() == ';'))\n-        ++in.position();\n-\n-    /// Finish reading rows if data is in square brackets and ']' received.\n-    skipWhitespaceIfAny(in);\n-    if (!in.eof() && *in.position() == ']' && data_in_square_brackets)\n+    bool is_first_row = getCurrentUnitNumber() == 0 && getTotalRows() == 1;\n+    if (!in.eof())\n     {\n-        data_in_square_brackets = false;\n-        parsing_stage = ParsingStage::FINISH;\n-        ++in.position();\n-        return false;\n+        /// There may be optional ',' (but not before the first row)\n+        if (!is_first_row && *in.position() == ',')\n+            ++in.position();\n+        else if (!data_in_square_brackets && *in.position() == ';')\n+        {\n+            /// ';' means the end of query (but it cannot be before ']')\n+            return allow_new_rows = false;\n+        }\n+        else if (data_in_square_brackets && *in.position() == ']')\n+        {\n+            /// ']' means the end of query\n+            return allow_new_rows = false;\n+        }\n     }\n \n     skipWhitespaceIfAny(in);\n-    if (in.eof() || parsing_stage == ParsingStage::FINISH)\n-    {\n-        if (data_in_square_brackets)\n-            throw Exception(\"Unexpected end of data: received end of stream instead of ']'.\", ErrorCodes::INCORRECT_DATA);\n+    if (in.eof())\n         return false;\n-    }\n \n     size_t num_columns = columns.size();\n \n@@ -291,6 +282,32 @@ void JSONEachRowRowInputFormat::resetParser()\n     prev_positions.clear();\n }\n \n+void JSONEachRowRowInputFormat::readPrefix()\n+{\n+    skipWhitespaceIfAny(in);\n+    if (!in.eof() && *in.position() == '[')\n+    {\n+        ++in.position();\n+        data_in_square_brackets = true;\n+    }\n+}\n+\n+void JSONEachRowRowInputFormat::readSuffix()\n+{\n+    skipWhitespaceIfAny(in);\n+    if (data_in_square_brackets)\n+    {\n+        assertChar(']', in);\n+        skipWhitespaceIfAny(in);\n+    }\n+    if (!in.eof() && *in.position() == ';')\n+    {\n+        ++in.position();\n+        skipWhitespaceIfAny(in);\n+    }\n+    assertEOF(in);\n+}\n+\n \n void registerInputFormatProcessorJSONEachRow(FormatFactory & factory)\n {\ndiff --git a/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.h b/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.h\nindex 10e15e62b694..a0a4b735a3e5 100644\n--- a/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.h\n+++ b/dbms/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.h\n@@ -24,6 +24,9 @@ class JSONEachRowRowInputFormat : public IRowInputFormat\n \n     String getName() const override { return \"JSONEachRowRowInputFormat\"; }\n \n+    void readPrefix() override;\n+    void readSuffix() override;\n+\n     bool readRow(MutableColumns & columns, RowReadExtension & ext) override;\n     bool allowSyncAfterError() const override { return true; }\n     void syncAfterError() override;\n@@ -71,15 +74,7 @@ class JSONEachRowRowInputFormat : public IRowInputFormat\n     /// This flag is needed to know if data is in square brackets.\n     bool data_in_square_brackets = false;\n \n-    /// This is needed to know the stage of parsing.\n-    enum class ParsingStage\n-    {\n-        START,\n-        PROCESS,\n-        FINISH\n-    };\n-\n-    ParsingStage parsing_stage = ParsingStage::START;\n+    bool allow_new_rows = true;\n };\n \n }\n",
  "test_patch": "diff --git a/dbms/tests/queries/0_stateless/01060_avro.sh b/dbms/tests/queries/0_stateless/01060_avro.sh\nindex cb81218cad9f..15e97abfa520 100755\n--- a/dbms/tests/queries/0_stateless/01060_avro.sh\n+++ b/dbms/tests/queries/0_stateless/01060_avro.sh\n@@ -11,36 +11,36 @@ DATA_DIR=$CUR_DIR/data_avro\n echo === input\n echo = primitive\n \n-cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a_bool UInt8, b_int Int32, c_long Int64, d_float Float32, e_double Float64, f_bytes String, g_string String' -q 'select * from table'\n-cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a_bool UInt8, c_long Int64, g_string String' -q 'select * from table'\n-cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'g_string String, c_long Int64, a_bool UInt8' -q 'select * from table'\n-cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'g_string String' -q 'select * from table'\n+cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a_bool UInt8, b_int Int32, c_long Int64, d_float Float32, e_double Float64, f_bytes String, g_string String' -q 'select * from table'\n+cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a_bool UInt8, c_long Int64, g_string String' -q 'select * from table'\n+cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'g_string String, c_long Int64, a_bool UInt8' -q 'select * from table'\n+cat $DATA_DIR/primitive.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'g_string String' -q 'select * from table'\n \n echo = complex\n-cat $DATA_DIR/complex.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"a_enum_to_string String, b_enum_to_enum Enum('t' = 1, 'f' = 0), c_array_string Array(String), d_array_array_string Array(Array(String)), e_union_null_string Nullable(String), f_union_long_null Nullable(Int64), g_fixed FixedString(32)\" -q 'select * from table'\n-cat $DATA_DIR/complex.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"g_fixed FixedString(32)\" -q 'select * from table'\n+cat $DATA_DIR/complex.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"a_enum_to_string String, b_enum_to_enum Enum('t' = 1, 'f' = 0), c_array_string Array(String), d_array_array_string Array(Array(String)), e_union_null_string Nullable(String), f_union_long_null Nullable(Int64), g_fixed FixedString(32)\" -q 'select * from table'\n+cat $DATA_DIR/complex.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"g_fixed FixedString(32)\" -q 'select * from table'\n \n echo = logical_types\n-cat $DATA_DIR/logical_types.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"a_date Date, b_timestamp_millis DateTime64(3, 'UTC'), c_timestamp_micros DateTime64(6, 'UTC')\" -q 'select * from table'\n-cat $DATA_DIR/logical_types.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a_date Int32, b_timestamp_millis Int64, c_timestamp_micros Int64' -q 'select * from table'\n+cat $DATA_DIR/logical_types.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"a_date Date, b_timestamp_millis DateTime64(3, 'UTC'), c_timestamp_micros DateTime64(6, 'UTC')\" -q 'select * from table'\n+cat $DATA_DIR/logical_types.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a_date Int32, b_timestamp_millis Int64, c_timestamp_micros Int64' -q 'select * from table'\n \n echo = references\n-cat $DATA_DIR/references.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"a String, c String\" -q 'select * from table'\n+cat $DATA_DIR/references.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"a String, c String\" -q 'select * from table'\n \n echo = compression\n-cat $DATA_DIR/simple.null.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n-cat $DATA_DIR/simple.deflate.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n+cat $DATA_DIR/simple.null.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n+cat $DATA_DIR/simple.deflate.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n \n #snappy is optional\n-#cat $DATA_DIR/simple.snappy.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n+#cat $DATA_DIR/simple.snappy.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n \n echo = other\n #no data\n-cat $DATA_DIR/empty.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n+cat $DATA_DIR/empty.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a Int64' -q 'select count() from table'\n # type mismatch\n-cat $DATA_DIR/simple.null.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'a Int32' -q 'select count() from table'\n+cat $DATA_DIR/simple.null.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'a Int32' -q 'select count() from table'\n # field not found\n-cat $DATA_DIR/simple.null.avro | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S 'b Int64' -q 'select count() from table' 2>&1 | grep -i 'not found' -o\n+cat $DATA_DIR/simple.null.avro | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S 'b Int64' -q 'select count() from table' 2>&1 | grep -i 'not found' -o\n \n \n \n@@ -52,20 +52,20 @@ echo === output\n \n echo = primitive\n S1=\"a_bool UInt8, b_int Int32, c_long Int64, d_float Float32, e_double Float64, f_bytes String, g_string String\"\n-echo '1,1,2,3.4,5.6,\"b1\",\"s1\"' | ${CLICKHOUSE_LOCAL} --input-format CSV -S \"$S1\" -q \"select * from table  format Avro\" | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"$S1\" -q 'select * from table'\n+echo '1,1,2,3.4,5.6,\"b1\",\"s1\"' | ${CLICKHOUSE_LOCAL} --input-format CSV -S \"$S1\" -q \"select * from table  format Avro\" | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"$S1\" -q 'select * from table'\n \n echo = complex\n S2=\"a_enum_to_string String, b_enum_to_enum Enum('t' = 1, 'f' = 0), c_array_string Array(String), d_array_array_string Array(Array(String)), e_union_null_string Nullable(String), f_union_long_null Nullable(Int64), g_fixed FixedString(32)\"\n-echo \"\\\"A\\\",\\\"t\\\",\\\"['s1','s2']\\\",\\\"[['a1'],['a2']]\\\",\\\"s1\\\",\\N,\\\"79cd909892d7e7ade1987cc7422628ba\\\"\" | ${CLICKHOUSE_LOCAL} --input-format CSV -S \"$S2\" -q \"select * from table  format Avro\" | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"$S2\" -q 'select * from table'\n+echo \"\\\"A\\\",\\\"t\\\",\\\"['s1','s2']\\\",\\\"[['a1'],['a2']]\\\",\\\"s1\\\",\\N,\\\"79cd909892d7e7ade1987cc7422628ba\\\"\" | ${CLICKHOUSE_LOCAL} --input-format CSV -S \"$S2\" -q \"select * from table  format Avro\" | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"$S2\" -q 'select * from table'\n \n echo = logical_types\n S3=\"a_date Date, b_timestamp_millis DateTime64(3, 'UTC'), c_timestamp_micros DateTime64(6, 'UTC')\"\n-echo '\"2019-12-20\",\"2020-01-10 07:31:56.227\",\"2020-01-10 07:31:56.227000\"' | ${CLICKHOUSE_LOCAL} --input-format CSV -S \"$S3\" -q \"select * from table  format Avro\" | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"$S3\" -q 'select * from table'\n+echo '\"2019-12-20\",\"2020-01-10 07:31:56.227\",\"2020-01-10 07:31:56.227000\"' | ${CLICKHOUSE_LOCAL} --input-format CSV -S \"$S3\" -q \"select * from table  format Avro\" | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"$S3\" -q 'select * from table'\n \n echo = other\n S4=\"a Int64\"\n-${CLICKHOUSE_LOCAL} -q \"select toInt64(number) as a from numbers(0)  format Avro\" | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"$S4\" -q 'select count() from table'\n-${CLICKHOUSE_LOCAL} -q \"select toInt64(number) as a from numbers(1000)  format Avro\" | ${CLICKHOUSE_LOCAL}  --input-format Avro --output-format CSV -S \"$S4\" -q 'select count() from table'\n+${CLICKHOUSE_LOCAL} -q \"select toInt64(number) as a from numbers(0)  format Avro\" | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"$S4\" -q 'select count() from table'\n+${CLICKHOUSE_LOCAL} -q \"select toInt64(number) as a from numbers(1000)  format Avro\" | ${CLICKHOUSE_LOCAL} --input-format Avro --output-format CSV -S \"$S4\" -q 'select count() from table'\n \n # type supported via conversion\n ${CLICKHOUSE_LOCAL}  -q \"select toInt16(123) as a format Avro\" | wc -c\n\\ No newline at end of file\ndiff --git a/dbms/tests/queries/0_stateless/01072_json_each_row_data_in_square_brackets.sql b/dbms/tests/queries/0_stateless/01072_json_each_row_data_in_square_brackets.sql\nindex bd83373c570e..0e4e031a9854 100644\n--- a/dbms/tests/queries/0_stateless/01072_json_each_row_data_in_square_brackets.sql\n+++ b/dbms/tests/queries/0_stateless/01072_json_each_row_data_in_square_brackets.sql\n@@ -1,6 +1,9 @@\n DROP TABLE IF EXISTS json_square_brackets;\n CREATE TABLE json_square_brackets (id UInt32, name String) ENGINE = Memory;\n-INSERT INTO json_square_brackets FORMAT JSONEachRow [{\"id\": 1, \"name\": \"name1\"}, {\"id\": 2, \"name\": \"name2\"}]\n+INSERT INTO json_square_brackets FORMAT JSONEachRow [{\"id\": 1, \"name\": \"name1\"}, {\"id\": 2, \"name\": \"name2\"}];\n+INSERT INTO json_square_brackets FORMAT JSONEachRow[];\n+INSERT INTO json_square_brackets FORMAT JSONEachRow [  ]  ;\n+INSERT INTO json_square_brackets FORMAT JSONEachRow ;\n \n SELECT * FROM json_square_brackets ORDER BY id;\n DROP TABLE IF EXISTS json_square_brackets;\ndiff --git a/dbms/tests/queries/0_stateless/01076_json_each_row_array.reference b/dbms/tests/queries/0_stateless/01076_json_each_row_array.reference\nnew file mode 100644\nindex 000000000000..749fce669df1\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01076_json_each_row_array.reference\n@@ -0,0 +1,1 @@\n+1000000\ndiff --git a/dbms/tests/queries/0_stateless/01076_json_each_row_array.sh b/dbms/tests/queries/0_stateless/01076_json_each_row_array.sh\nnew file mode 100755\nindex 000000000000..e6bb74eea4b2\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01076_json_each_row_array.sh\n@@ -0,0 +1,8 @@\n+#!/usr/bin/env bash\n+\n+set -e\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+. $CUR_DIR/../shell_config.sh\n+\n+${CLICKHOUSE_LOCAL} --query \"SELECT '[' || arrayStringConcat(arrayMap(x -> '{\\\"id\\\": 1, \\\"name\\\": \\\"name1\\\"}', range(1000000)), ',') || ']'\" | ${CLICKHOUSE_LOCAL} --query \"SELECT count() FROM table\" --input-format JSONEachRow --structure 'id UInt32, name String'\n",
  "problem_statement": "square brackets in JSONEachRowFormat DB::Exception: Unexpected end of data\nCH 20.2.1.2278\r\n```\r\nCREATE TABLE json_square_brackets (id UInt32, name String) ENGINE = Log;\r\n\r\nclickhouse-client -q \"SELECT '['||arrayStringConcat(arrayMap(x -> '{\\\"id\\\": 1, \\\"name\\\": \\\"name1\\\"}', range(100)), ',')||']'\" > test.json\r\n\r\ncat test.json | clickhouse-client -q \"INSERT INTO json_square_brackets FORMAT JSONEachRow\"\r\nOK\r\n\r\nclickhouse-client -q \"SELECT '['||arrayStringConcat(arrayMap(x -> '{\\\"id\\\": 1, \\\"name\\\": \\\"name1\\\"}', range(1000000)), ',')||']'\" > test.json\r\n\r\ncat test.json | clickhouse-client -q \"INSERT INTO json_square_brackets FORMAT JSONEachRow\"\r\n\r\nCode: 117, e.displayText() = DB::Exception: Unexpected end of data: received end of stream instead of ']'., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xbdaef6c in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x4fe3099 in /usr/bin/clickhouse\r\n2. ? @ 0x4ca91f0 in /usr/bin/clickhouse\r\n3. DB::IRowInputFormat::generate() @ 0x994e179 in /usr/bin/clickhouse\r\n4. DB::ISource::work() @ 0x92cf187 in /usr/bin/clickhouse\r\n5. DB::InputStreamFromInputFormat::readImpl() @ 0x9293a3d in /usr/bin/clickhouse\r\n6. DB::IBlockInputStream::read() @ 0x8b772df in /usr/bin/clickhouse\r\n7. DB::ParallelParsingBlockInputStream::parserThreadFunction(unsigned long) @ 0x964cb32 in /usr/bin/clickhouse\r\n8. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x501ac07 in /usr/bin/clickhouse\r\n9. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const @ 0x501b254 in /usr/bin/clickhouse\r\n10. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x501a127 in /usr/bin/clickhouse\r\n11. ? @ 0x50186df in /usr/bin/clickhouse\r\n12. start_thread @ 0x7fa3 in /lib/x86_64-linux-gnu/libpthread-2.28.so\r\n13. clone @ 0xf94cf in /lib/x86_64-linux-gnu/libc-2.28.so\r\n (version 20.2.1.2278)\r\nCode: 117. DB::Exception: Unexpected end of data: received end of stream instead of ']'.\r\n\r\n```\r\n\n",
  "hints_text": "",
  "created_at": "2020-02-03T00:05:35Z"
}