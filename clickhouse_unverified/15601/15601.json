{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 15601,
  "instance_id": "ClickHouse__ClickHouse-15601",
  "issue_numbers": [
    "15601"
  ],
  "base_commit": "4bc40421e21e3f85696fdf784a3d699f0dfb1c21",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 6eb853828c50..575523fc85c9 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -158,6 +158,7 @@ class IColumn;\n     \\\n     M(UInt64, insert_quorum, 0, \"For INSERT queries in the replicated table, wait writing for the specified number of replicas and linearize the addition of the data. 0 - disabled.\", 0) \\\n     M(Milliseconds, insert_quorum_timeout, 600000, \"\", 0) \\\n+    M(Bool, insert_quorum_parallel, false, \"For quorum INSERT queries - enable to make parallel inserts without linearizability\", 0) \\\n     M(UInt64, select_sequential_consistency, 0, \"For SELECT queries from the replicated table, throw an exception if the replica does not have a chunk written with the quorum; do not read the parts that have not yet been written with the quorum.\", 0) \\\n     M(UInt64, table_function_remote_max_addresses, 1000, \"The maximum number of different shards and the maximum number of replicas of one shard in the `remote` function.\", 0) \\\n     M(Milliseconds, read_backoff_min_latency_ms, 1000, \"Setting to reduce the number of threads in case of slow reads. Pay attention only to reads that took at least that much time.\", 0) \\\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\nindex 5696a9cf890e..c8530943873f 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\n@@ -39,12 +39,14 @@ ReplicatedMergeTreeBlockOutputStream::ReplicatedMergeTreeBlockOutputStream(\n     size_t quorum_,\n     size_t quorum_timeout_ms_,\n     size_t max_parts_per_block_,\n+    bool quorum_parallel_,\n     bool deduplicate_)\n     : storage(storage_)\n     , metadata_snapshot(metadata_snapshot_)\n     , quorum(quorum_)\n     , quorum_timeout_ms(quorum_timeout_ms_)\n     , max_parts_per_block(max_parts_per_block_)\n+    , quorum_parallel(quorum_parallel_)\n     , deduplicate(deduplicate_)\n     , log(&Poco::Logger::get(storage.getLogName() + \" (Replicated OutputStream)\"))\n {\n@@ -75,7 +77,6 @@ void ReplicatedMergeTreeBlockOutputStream::checkQuorumPrecondition(zkutil::ZooKe\n {\n     quorum_info.status_path = storage.zookeeper_path + \"/quorum/status\";\n \n-    std::future<Coordination::GetResponse> quorum_status_future = zookeeper->asyncTryGet(quorum_info.status_path);\n     std::future<Coordination::GetResponse> is_active_future = zookeeper->asyncTryGet(storage.replica_path + \"/is_active\");\n     std::future<Coordination::GetResponse> host_future = zookeeper->asyncTryGet(storage.replica_path + \"/host\");\n \n@@ -97,9 +98,9 @@ void ReplicatedMergeTreeBlockOutputStream::checkQuorumPrecondition(zkutil::ZooKe\n         * If the quorum is reached, then the node is deleted.\n         */\n \n-    auto quorum_status = quorum_status_future.get();\n-    if (quorum_status.error != Coordination::Error::ZNONODE)\n-        throw Exception(\"Quorum for previous write has not been satisfied yet. Status: \" + quorum_status.data,\n+    String quorum_status;\n+    if (!quorum_parallel && zookeeper->tryGet(quorum_info.status_path, quorum_status))\n+        throw Exception(\"Quorum for previous write has not been satisfied yet. Status: \" + quorum_status,\n                         ErrorCodes::UNSATISFIED_QUORUM_FOR_PREVIOUS_WRITE);\n \n     /// Both checks are implicitly made also later (otherwise there would be a race condition).\n@@ -294,6 +295,9 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(\n                     *  which indicates that the quorum has been reached.\n                     */\n \n+                if (quorum_parallel)\n+                    quorum_info.status_path = storage.zookeeper_path + \"/quorum/parallel/\" + part->name;\n+\n                 ops.emplace_back(\n                     zkutil::makeCreateRequest(\n                         quorum_info.status_path,\n@@ -346,7 +350,6 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(\n             /// Used only for exception messages.\n             block_number = part->info.min_block;\n \n-\n             /// Do not check for duplicate on commit to ZK.\n             block_id_path.clear();\n         }\n@@ -466,14 +469,16 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(\n         if (is_already_existing_part)\n         {\n             /// We get duplicate part without fetch\n-            storage.updateQuorum(part->name);\n+            /// Check if this quorum insert is parallel or not\n+            if (zookeeper->exists(storage.zookeeper_path + \"/quorum/parallel/\" + part->name))\n+                storage.updateQuorum(part->name, true);\n+            else if (zookeeper->exists(storage.zookeeper_path + \"/quorum/status\"))\n+                storage.updateQuorum(part->name, false);\n         }\n \n         /// We are waiting for quorum to be satisfied.\n         LOG_TRACE(log, \"Waiting for quorum\");\n \n-        String quorum_status_path = storage.zookeeper_path + \"/quorum/status\";\n-\n         try\n         {\n             while (true)\n@@ -482,7 +487,7 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(\n \n                 std::string value;\n                 /// `get` instead of `exists` so that `watch` does not leak if the node is no longer there.\n-                if (!zookeeper->tryGet(quorum_status_path, value, nullptr, event))\n+                if (!zookeeper->tryGet(quorum_info.status_path, value, nullptr, event))\n                     break;\n \n                 ReplicatedMergeTreeQuorumEntry quorum_entry(value);\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.h b/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.h\nindex fa3ede20c283..97c094c1128e 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.h\n@@ -28,6 +28,7 @@ class ReplicatedMergeTreeBlockOutputStream : public IBlockOutputStream\n         size_t quorum_,\n         size_t quorum_timeout_ms_,\n         size_t max_parts_per_block_,\n+        bool quorum_parallel_,\n         bool deduplicate_);\n \n     Block getHeader() const override;\n@@ -64,6 +65,7 @@ class ReplicatedMergeTreeBlockOutputStream : public IBlockOutputStream\n     size_t quorum_timeout_ms;\n     size_t max_parts_per_block;\n \n+    bool quorum_parallel = false;\n     bool deduplicate = true;\n     bool last_block_is_duplicate = false;\n \ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\nindex 8424da11896a..27e870bda786 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n@@ -226,14 +226,32 @@ void ReplicatedMergeTreeRestartingThread::updateQuorumIfWeHavePart()\n     String quorum_str;\n     if (zookeeper->tryGet(storage.zookeeper_path + \"/quorum/status\", quorum_str))\n     {\n-        ReplicatedMergeTreeQuorumEntry quorum_entry;\n-        quorum_entry.fromString(quorum_str);\n+        ReplicatedMergeTreeQuorumEntry quorum_entry(quorum_str);\n \n         if (!quorum_entry.replicas.count(storage.replica_name)\n-            && zookeeper->exists(storage.replica_path + \"/parts/\" + quorum_entry.part_name))\n+            && storage.getActiveContainingPart(quorum_entry.part_name))\n         {\n             LOG_WARNING(log, \"We have part {} but we is not in quorum. Updating quorum. This shouldn't happen often.\", quorum_entry.part_name);\n-            storage.updateQuorum(quorum_entry.part_name);\n+            storage.updateQuorum(quorum_entry.part_name, false);\n+        }\n+    }\n+\n+    Strings part_names;\n+    String parallel_quorum_parts_path = storage.zookeeper_path + \"/quorum/parallel\";\n+    if (zookeeper->tryGetChildren(parallel_quorum_parts_path, part_names) == Coordination::Error::ZOK)\n+    {\n+        for (auto & part_name : part_names)\n+        {\n+            if (zookeeper->tryGet(parallel_quorum_parts_path + \"/\" + part_name, quorum_str))\n+            {\n+                ReplicatedMergeTreeQuorumEntry quorum_entry(quorum_str);\n+                if (!quorum_entry.replicas.count(storage.replica_name)\n+                    && storage.getActiveContainingPart(part_name))\n+                {\n+                    LOG_WARNING(log, \"We have part {} but we is not in quorum. Updating quorum. This shouldn't happen often.\", part_name);\n+                    storage.updateQuorum(part_name, true);\n+                }\n+            }\n         }\n     }\n }\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 9613bd5111da..e4cae6464582 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -475,6 +475,7 @@ void StorageReplicatedMergeTree::createNewZooKeeperNodes()\n \n     /// Working with quorum.\n     zookeeper->createIfNotExists(zookeeper_path + \"/quorum\", String());\n+    zookeeper->createIfNotExists(zookeeper_path + \"/quorum/parallel\", String());\n     zookeeper->createIfNotExists(zookeeper_path + \"/quorum/last_part\", String());\n     zookeeper->createIfNotExists(zookeeper_path + \"/quorum/failed_parts\", String());\n \n@@ -1702,15 +1703,24 @@ bool StorageReplicatedMergeTree::executeFetch(LogEntry & entry)\n                 if (replica.empty())\n                 {\n                     Coordination::Stat quorum_stat;\n-                    String quorum_path = zookeeper_path + \"/quorum/status\";\n-                    String quorum_str = zookeeper->get(quorum_path, &quorum_stat);\n+                    const String quorum_unparallel_path = zookeeper_path + \"/quorum/status\";\n+                    const String quorum_parallel_path = zookeeper_path + \"/quorum/parallel/\" + entry.new_part_name;\n+                    String quorum_str, quorum_path;\n                     ReplicatedMergeTreeQuorumEntry quorum_entry;\n+\n+                    if (zookeeper->tryGet(quorum_unparallel_path, quorum_str, &quorum_stat))\n+                        quorum_path = quorum_unparallel_path;\n+                    else\n+                    {\n+                        quorum_str = zookeeper->get(quorum_parallel_path, &quorum_stat);\n+                        quorum_path = quorum_parallel_path;\n+                    }\n+\n                     quorum_entry.fromString(quorum_str);\n \n                     if (quorum_entry.part_name == entry.new_part_name)\n                     {\n                         ops.emplace_back(zkutil::makeRemoveRequest(quorum_path, quorum_stat.version));\n-\n                         auto part_info = MergeTreePartInfo::fromPartName(entry.new_part_name, format_version);\n \n                         if (part_info.min_block != part_info.max_block)\n@@ -3030,12 +3040,14 @@ String StorageReplicatedMergeTree::findReplicaHavingCoveringPart(\n \n /** If a quorum is tracked for a part, update information about it in ZK.\n   */\n-void StorageReplicatedMergeTree::updateQuorum(const String & part_name)\n+void StorageReplicatedMergeTree::updateQuorum(const String & part_name, bool is_parallel)\n {\n     auto zookeeper = getZooKeeper();\n \n     /// Information on which replicas a part has been added, if the quorum has not yet been reached.\n-    const String quorum_status_path = zookeeper_path + \"/quorum/status\";\n+    String quorum_status_path = zookeeper_path + \"/quorum/status\";\n+    if (is_parallel)\n+        quorum_status_path = zookeeper_path + \"/quorum/parallel/\" + part_name;\n     /// The name of the previous part for which the quorum was reached.\n     const String quorum_last_part_path = zookeeper_path + \"/quorum/last_part\";\n \n@@ -3045,9 +3057,7 @@ void StorageReplicatedMergeTree::updateQuorum(const String & part_name)\n     /// If there is no node, then all quorum INSERTs have already reached the quorum, and nothing is needed.\n     while (zookeeper->tryGet(quorum_status_path, value, &stat))\n     {\n-        ReplicatedMergeTreeQuorumEntry quorum_entry;\n-        quorum_entry.fromString(value);\n-\n+        ReplicatedMergeTreeQuorumEntry quorum_entry(value);\n         if (quorum_entry.part_name != part_name)\n         {\n             /// The quorum has already been achieved. Moreover, another INSERT with a quorum has already started.\n@@ -3063,22 +3073,28 @@ void StorageReplicatedMergeTree::updateQuorum(const String & part_name)\n             Coordination::Requests ops;\n             Coordination::Responses responses;\n \n-            Coordination::Stat added_parts_stat;\n-            String old_added_parts = zookeeper->get(quorum_last_part_path, &added_parts_stat);\n+            if (!is_parallel)\n+            {\n+                Coordination::Stat added_parts_stat;\n+                String old_added_parts = zookeeper->get(quorum_last_part_path, &added_parts_stat);\n \n-            ReplicatedMergeTreeQuorumAddedParts parts_with_quorum(format_version);\n+                ReplicatedMergeTreeQuorumAddedParts parts_with_quorum(format_version);\n \n-            if (!old_added_parts.empty())\n-                parts_with_quorum.fromString(old_added_parts);\n+                if (!old_added_parts.empty())\n+                    parts_with_quorum.fromString(old_added_parts);\n \n-            auto part_info = MergeTreePartInfo::fromPartName(part_name, format_version);\n-            /// We store one last part which reached quorum for each partition.\n-            parts_with_quorum.added_parts[part_info.partition_id] = part_name;\n+                auto part_info = MergeTreePartInfo::fromPartName(part_name, format_version);\n+                /// We store one last part which reached quorum for each partition.\n+                parts_with_quorum.added_parts[part_info.partition_id] = part_name;\n \n-            String new_added_parts = parts_with_quorum.toString();\n+                String new_added_parts = parts_with_quorum.toString();\n+\n+                ops.emplace_back(zkutil::makeRemoveRequest(quorum_status_path, stat.version));\n+                ops.emplace_back(zkutil::makeSetRequest(quorum_last_part_path, new_added_parts, added_parts_stat.version));\n+            }\n+            else\n+                ops.emplace_back(zkutil::makeRemoveRequest(quorum_status_path, stat.version));\n \n-            ops.emplace_back(zkutil::makeRemoveRequest(quorum_status_path, stat.version));\n-            ops.emplace_back(zkutil::makeSetRequest(quorum_last_part_path, new_added_parts, added_parts_stat.version));\n             auto code = zookeeper->tryMulti(ops, responses);\n \n             if (code == Coordination::Error::ZOK)\n@@ -3307,7 +3323,25 @@ bool StorageReplicatedMergeTree::fetchPart(const String & part_name, const Stora\n               * If you do not have time, in case of losing the session, when you restart the server - see the `ReplicatedMergeTreeRestartingThread::updateQuorumIfWeHavePart` method.\n               */\n             if (quorum)\n-                updateQuorum(part_name);\n+            {\n+                /// Check if this quorum insert is parallel or not\n+                if (zookeeper->exists(zookeeper_path + \"/quorum/parallel/\" + part_name))\n+                    updateQuorum(part_name, true);\n+                else if (zookeeper->exists(zookeeper_path + \"/quorum/status\"))\n+                    updateQuorum(part_name, false);\n+            }\n+\n+            /// merged parts that are still inserted with quorum. if it only contains one block, it hasn't been merged before\n+            if (part_info.level != 0 || part_info.mutation != 0)\n+            {\n+                Strings quorum_parts = zookeeper->getChildren(zookeeper_path + \"/quorum/parallel\");\n+                for (const String & quorum_part : quorum_parts)\n+                {\n+                    auto quorum_part_info = MergeTreePartInfo::fromPartName(quorum_part, format_version);\n+                    if (part_info.contains(quorum_part_info))\n+                        updateQuorum(quorum_part, true);\n+                }\n+            }\n \n             merge_selecting_task->schedule();\n \n@@ -3590,6 +3624,7 @@ BlockOutputStreamPtr StorageReplicatedMergeTree::write(const ASTPtr & /*query*/,\n         *this, metadata_snapshot, query_settings.insert_quorum,\n         query_settings.insert_quorum_timeout.totalMilliseconds(),\n         query_settings.max_partitions_per_insert_block,\n+        query_settings.insert_quorum_parallel,\n         deduplicate);\n }\n \n@@ -4166,7 +4201,7 @@ PartitionCommandsResultInfo StorageReplicatedMergeTree::attachPartition(\n     PartsTemporaryRename renamed_parts(*this, \"detached/\");\n     MutableDataPartsVector loaded_parts = tryLoadPartsToAttach(partition, attach_part, query_context, renamed_parts);\n \n-    ReplicatedMergeTreeBlockOutputStream output(*this, metadata_snapshot, 0, 0, 0, false);   /// TODO Allow to use quorum here.\n+    ReplicatedMergeTreeBlockOutputStream output(*this, metadata_snapshot, 0, 0, 0, false, false);   /// TODO Allow to use quorum here.\n     for (size_t i = 0; i < loaded_parts.size(); ++i)\n     {\n         String old_name = loaded_parts[i]->name;\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex d851082d5c2b..14419c69c6ff 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -497,7 +497,7 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n \n \n     /// With the quorum being tracked, add a replica to the quorum for the part.\n-    void updateQuorum(const String & part_name);\n+    void updateQuorum(const String & part_name, bool is_parallel);\n \n     /// Deletes info from quorum/last_part node for particular partition_id.\n     void cleanLastPartNode(const String & partition_id);\n",
  "test_patch": "diff --git a/tests/integration/helpers/network.py b/tests/integration/helpers/network.py\nindex a237f7d3cc7c..7ad3ceed54b7 100644\n--- a/tests/integration/helpers/network.py\n+++ b/tests/integration/helpers/network.py\n@@ -166,6 +166,17 @@ def _ensure_container(self):\n                 except docker.errors.NotFound:\n                     pass\n \n+            # for some reason docker api may hang if image doesn't exist, so we download it\n+            # before running\n+            for i in range(5):\n+                try:\n+                    subprocess.check_call(\"docker pull yandex/clickhouse-integration-helper\", shell=True)\n+                    break\n+                except:\n+                    time.sleep(i)\n+            else:\n+                raise Exception(\"Cannot pull yandex/clickhouse-integration-helper image\")\n+\n             self._container = self._docker_client.containers.run('yandex/clickhouse-integration-helper',\n                                                                  auto_remove=True,\n                                                                  command=('sleep %s' % self.container_exit_timeout),\ndiff --git a/tests/integration/test_quorum_inserts_parallel/__init__.py b/tests/integration/test_quorum_inserts_parallel/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_quorum_inserts_parallel/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_quorum_inserts_parallel/test.py b/tests/integration/test_quorum_inserts_parallel/test.py\nnew file mode 100644\nindex 000000000000..c89f1a03df73\n--- /dev/null\n+++ b/tests/integration/test_quorum_inserts_parallel/test.py\n@@ -0,0 +1,102 @@\n+#!/usr/bin/env python3\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+from multiprocessing.dummy import Pool\n+from helpers.network import PartitionManager\n+from helpers.client import QueryRuntimeException\n+from helpers.test_tools import assert_eq_with_retry\n+\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance(\"node1\", with_zookeeper=True)\n+node2 = cluster.add_instance(\"node2\", with_zookeeper=True)\n+node3 = cluster.add_instance(\"node3\", with_zookeeper=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    global cluster\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_parallel_quorum_actually_parallel(started_cluster):\n+    settings = {\"insert_quorum\": \"3\", \"insert_quorum_parallel\": \"1\"}\n+    for i, node in enumerate([node1, node2, node3]):\n+        node.query(\"CREATE TABLE r (a UInt64, b String) ENGINE=ReplicatedMergeTree('/test/r', '{num}') ORDER BY tuple()\".format(num=i))\n+\n+    p = Pool(10)\n+\n+    def long_insert(node):\n+        node.query(\"INSERT INTO r SELECT number, toString(number) FROM numbers(5) where sleepEachRow(1) == 0\", settings=settings)\n+\n+    job = p.apply_async(long_insert, (node1,))\n+\n+    node2.query(\"INSERT INTO r VALUES (6, '6')\", settings=settings)\n+    assert node1.query(\"SELECT COUNT() FROM r\") == \"1\\n\"\n+    assert node2.query(\"SELECT COUNT() FROM r\") == \"1\\n\"\n+    assert node3.query(\"SELECT COUNT() FROM r\") == \"1\\n\"\n+\n+    node1.query(\"INSERT INTO r VALUES (7, '7')\", settings=settings)\n+    assert node1.query(\"SELECT COUNT() FROM r\") == \"2\\n\"\n+    assert node2.query(\"SELECT COUNT() FROM r\") == \"2\\n\"\n+    assert node3.query(\"SELECT COUNT() FROM r\") == \"2\\n\"\n+\n+    job.get()\n+\n+    assert node1.query(\"SELECT COUNT() FROM r\") == \"7\\n\"\n+    assert node2.query(\"SELECT COUNT() FROM r\") == \"7\\n\"\n+    assert node3.query(\"SELECT COUNT() FROM r\") == \"7\\n\"\n+    p.close()\n+    p.join()\n+\n+\n+def test_parallel_quorum_actually_quorum(started_cluster):\n+    for i, node in enumerate([node1, node2, node3]):\n+        node.query(\"CREATE TABLE q (a UInt64, b String) ENGINE=ReplicatedMergeTree('/test/q', '{num}') ORDER BY tuple()\".format(num=i))\n+\n+    with PartitionManager() as pm:\n+        pm.partition_instances(node2, node1, port=9009)\n+        pm.partition_instances(node2, node3, port=9009)\n+        with pytest.raises(QueryRuntimeException):\n+            node1.query(\"INSERT INTO q VALUES(1, 'Hello')\", settings={\"insert_quorum\": \"3\", \"insert_quorum_parallel\": \"1\", \"insert_quorum_timeout\": \"3000\"})\n+\n+        assert_eq_with_retry(node1, \"SELECT COUNT() FROM q\", \"1\")\n+        assert_eq_with_retry(node2, \"SELECT COUNT() FROM q\", \"0\")\n+        assert_eq_with_retry(node3, \"SELECT COUNT() FROM q\", \"1\")\n+\n+        node1.query(\"INSERT INTO q VALUES(2, 'wlrd')\", settings={\"insert_quorum\": \"2\", \"insert_quorum_parallel\": \"1\", \"insert_quorum_timeout\": \"3000\"})\n+\n+        assert_eq_with_retry(node1, \"SELECT COUNT() FROM q\", \"2\")\n+        assert_eq_with_retry(node2, \"SELECT COUNT() FROM q\", \"0\")\n+        assert_eq_with_retry(node3, \"SELECT COUNT() FROM q\", \"2\")\n+\n+        def insert_value_to_node(node, settings):\n+            node.query(\"INSERT INTO q VALUES(3, 'Hi')\", settings=settings)\n+\n+        p = Pool(2)\n+        res = p.apply_async(insert_value_to_node, (node1, {\"insert_quorum\": \"3\", \"insert_quorum_parallel\": \"1\", \"insert_quorum_timeout\": \"60000\"}))\n+\n+        assert_eq_with_retry(node1, \"SELECT COUNT() FROM system.parts WHERE table == 'q' and active == 1\", \"3\")\n+        assert_eq_with_retry(node3, \"SELECT COUNT() FROM system.parts WHERE table == 'q' and active == 1\", \"3\")\n+        assert_eq_with_retry(node2, \"SELECT COUNT() FROM system.parts WHERE table == 'q' and active == 1\", \"0\")\n+\n+        # Insert to the second to satisfy quorum\n+        insert_value_to_node(node2, {\"insert_quorum\": \"3\", \"insert_quorum_parallel\": \"1\"})\n+\n+        res.get()\n+\n+        assert_eq_with_retry(node1, \"SELECT COUNT() FROM q\", \"3\")\n+        assert_eq_with_retry(node2, \"SELECT COUNT() FROM q\", \"1\")\n+        assert_eq_with_retry(node3, \"SELECT COUNT() FROM q\", \"3\")\n+\n+        p.close()\n+        p.join()\n+\n+    node2.query(\"SYSTEM SYNC REPLICA q\", timeout=10)\n+    assert_eq_with_retry(node2, \"SELECT COUNT() FROM q\", \"3\")\ndiff --git a/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.reference b/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.reference\nnew file mode 100644\nindex 000000000000..52dea650ebc7\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.reference\n@@ -0,0 +1,10 @@\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\n+100\t0\t99\t4950\ndiff --git a/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.sh b/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.sh\nnew file mode 100755\nindex 000000000000..99dc1c5e8f57\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.sh\n@@ -0,0 +1,36 @@\n+#!/usr/bin/env bash\n+\n+set -e\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+. \"$CURDIR\"/../shell_config.sh\n+\n+NUM_REPLICAS=10\n+\n+for i in $(seq 1 $NUM_REPLICAS); do\n+    $CLICKHOUSE_CLIENT -n -q \"\n+        DROP TABLE IF EXISTS r$i;\n+        CREATE TABLE r$i (x UInt64) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01509/parallel_quorum_many', 'r$i') ORDER BY x;\n+    \"\n+done\n+\n+function thread {\n+    $CLICKHOUSE_CLIENT --insert_quorum 5 --insert_quorum_parallel 1 --query \"INSERT INTO r$1 SELECT $2\"\n+}\n+\n+for i in $(seq 1 $NUM_REPLICAS); do\n+    for j in {0..9}; do\n+        a=$((($i - 1) * 10 + $j))\n+        thread $i $a &\n+    done\n+done\n+\n+wait\n+\n+for i in $(seq 1 $NUM_REPLICAS); do\n+    $CLICKHOUSE_CLIENT -n -q \"\n+        SYSTEM SYNC REPLICA r$i;\n+        SELECT count(), min(x), max(x), sum(x) FROM r$i;\n+        DROP TABLE IF EXISTS r$i;\n+\"\n+done\ndiff --git a/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.reference b/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.reference\nnew file mode 100644\nindex 000000000000..103d5e370943\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.reference\n@@ -0,0 +1,2 @@\n+5\t1\t5\t15\n+5\t1\t5\t15\ndiff --git a/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.sh b/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.sh\nnew file mode 100755\nindex 000000000000..eecb06bda5df\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.sh\n@@ -0,0 +1,37 @@\n+#!/usr/bin/env bash\n+\n+set -e\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+. \"$CURDIR\"/../shell_config.sh\n+\n+NUM_REPLICAS=2\n+NUM_INSERTS=5\n+\n+for i in $(seq 1 $NUM_REPLICAS); do\n+    $CLICKHOUSE_CLIENT -n -q \"\n+        DROP TABLE IF EXISTS r$i;\n+        CREATE TABLE r$i (x UInt64) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01509/parallel_quorum', 'r$i') ORDER BY x;\n+    \"\n+done\n+\n+$CLICKHOUSE_CLIENT -n -q \"SYSTEM STOP REPLICATION QUEUES r2;\"\n+\n+function thread {\n+    $CLICKHOUSE_CLIENT --insert_quorum 2 --insert_quorum_parallel 1 --query \"INSERT INTO r1 SELECT $1\"\n+}\n+\n+for i in $(seq 1 $NUM_INSERTS); do\n+    thread $i &\n+done\n+\n+$CLICKHOUSE_CLIENT -n -q \"SYSTEM START REPLICATION QUEUES r2;\"\n+\n+wait\n+\n+for i in $(seq 1 $NUM_REPLICAS); do\n+    $CLICKHOUSE_CLIENT -n -q \"\n+        SELECT count(), min(x), max(x), sum(x) FROM r$i;\n+        DROP TABLE IF EXISTS r$i;\n+\"\n+done\ndiff --git a/tests/queries/0_stateless/01509_parallel_quorum_and_merge.reference b/tests/queries/0_stateless/01509_parallel_quorum_and_merge.reference\nnew file mode 100644\nindex 000000000000..a5c40a850490\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_parallel_quorum_and_merge.reference\n@@ -0,0 +1,4 @@\n+all_0_1_1\n+DownloadPart\n+2\n+2\ndiff --git a/tests/queries/0_stateless/01509_parallel_quorum_and_merge.sh b/tests/queries/0_stateless/01509_parallel_quorum_and_merge.sh\nnew file mode 100755\nindex 000000000000..214c39a21cce\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_parallel_quorum_and_merge.sh\n@@ -0,0 +1,66 @@\n+#!/usr/bin/env bash\n+\n+set -e\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS parallel_q1\"\n+$CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS parallel_q2\"\n+\n+\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE parallel_q1 (x UInt64) ENGINE=ReplicatedMergeTree('/clickhouse/tables/test_01509/parallel_q', 'r1') ORDER BY tuple() SETTINGS old_parts_lifetime = 1, cleanup_delay_period = 0, cleanup_delay_period_random_add = 0\"\n+\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE parallel_q2 (x UInt64) ENGINE=ReplicatedMergeTree('/clickhouse/tables/test_01509/parallel_q', 'r2') ORDER BY tuple() SETTINGS always_fetch_merged_part = 1\"\n+\n+$CLICKHOUSE_CLIENT -q \"SYSTEM STOP REPLICATION QUEUES parallel_q2\"\n+\n+$CLICKHOUSE_CLIENT -q \"INSERT INTO parallel_q1 VALUES (1)\"\n+\n+$CLICKHOUSE_CLIENT --insert_quorum 2 --insert_quorum_parallel 1 --query=\"INSERT INTO parallel_q1 VALUES (2)\" &\n+\n+part_count=$($CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM system.parts WHERE table='parallel_q1' and database='${CLICKHOUSE_DATABASE}'\")\n+\n+# Check part inserted locally\n+while [[ $part_count != 2 ]]\n+do\n+    sleep 0.1\n+    part_count=$($CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM system.parts WHERE table='parallel_q1' and database='${CLICKHOUSE_DATABASE}'\")\n+done\n+\n+$CLICKHOUSE_CLIENT --replication_alter_partitions_sync 0 -q \"OPTIMIZE TABLE parallel_q1 FINAL\"\n+\n+# check part merged locally\n+has_part=$($CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM system.parts WHERE table='parallel_q1' and database='${CLICKHOUSE_DATABASE}' and name='all_0_1_1'\")\n+\n+while [[ $has_part != 1 ]]\n+do\n+    sleep 0.1\n+    has_part=$($CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM system.parts WHERE table='parallel_q1' and database='${CLICKHOUSE_DATABASE}' and name='all_0_1_1'\")\n+done\n+\n+# check source parts removed locally\n+active_parts_count=$($CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM system.parts WHERE table='parallel_q1' and database='${CLICKHOUSE_DATABASE}' and active=1\")\n+\n+while [[ $active_parts_count != 1 ]]\n+do\n+    sleep 0.1\n+    active_parts_count=$($CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM system.parts WHERE table='parallel_q1' and database='${CLICKHOUSE_DATABASE}'\")\n+done\n+\n+# download merged part\n+$CLICKHOUSE_CLIENT -q \"SYSTEM START REPLICATION QUEUES parallel_q2\"\n+\n+$CLICKHOUSE_CLIENT -q \"SYSTEM SYNC REPLICA parallel_q2\"\n+\n+# quorum satisfied even for merged part\n+wait\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM FLUSH LOGS\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT name FROM system.parts WHERE table='parallel_q2' and database='${CLICKHOUSE_DATABASE}' and active=1 ORDER BY name\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT event_type FROM system.part_log WHERE table='parallel_q2' and database='${CLICKHOUSE_DATABASE}' and part_name='all_0_1_1'\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM parallel_q2\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT COUNT() FROM parallel_q1\"\n+\n+$CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS parallel_q1\"\n+$CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS parallel_q2\"\ndiff --git a/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.reference b/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.reference\nnew file mode 100644\nindex 000000000000..5647890ac1c6\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.reference\n@@ -0,0 +1,12 @@\n+insert to two replicas works\n+1\n+1\n+insert to single replica works\n+3\n+3\n+deduplication works\n+3\n+3\n+insert happened\n+4\n+4\ndiff --git a/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.sql b/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.sql\nnew file mode 100644\nindex 000000000000..7607a4e90125\n--- /dev/null\n+++ b/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.sql\n@@ -0,0 +1,66 @@\n+DROP TABLE IF EXISTS r1;\n+DROP TABLE IF EXISTS r2;\n+\n+CREATE TABLE r1 (\n+    key UInt64, value String\n+)\n+ENGINE = ReplicatedMergeTree('/clickhouse/01509_no_repliacs', '1')\n+ORDER BY tuple();\n+\n+CREATE TABLE r2 (\n+    key UInt64, value String\n+)\n+ENGINE = ReplicatedMergeTree('/clickhouse/01509_no_repliacs', '2')\n+ORDER BY tuple();\n+\n+SET insert_quorum_parallel=1;\n+\n+SET insert_quorum=3;\n+INSERT INTO r1 VALUES(1, '1'); --{serverError 285}\n+\n+SELECT 'insert to two replicas works';\n+SET insert_quorum=2, insert_quorum_parallel=1;\n+INSERT INTO r1 VALUES(1, '1');\n+\n+SELECT COUNT() FROM r1;\n+SELECT COUNT() FROM r2;\n+\n+DETACH TABLE r2;\n+\n+INSERT INTO r1 VALUES(2, '2'); --{serverError 285}\n+\n+SET insert_quorum=1, insert_quorum_parallel=1;\n+SELECT 'insert to single replica works';\n+INSERT INTO r1 VALUES(2, '2');\n+\n+ATTACH TABLE r2;\n+\n+SET insert_quorum=2, insert_quorum_parallel=1;\n+\n+INSERT INTO r1 VALUES(3, '3');\n+\n+SELECT COUNT() FROM r1;\n+SELECT COUNT() FROM r2;\n+\n+SELECT 'deduplication works';\n+INSERT INTO r2 VALUES(3, '3');\n+\n+SELECT COUNT() FROM r1;\n+SELECT COUNT() FROM r2;\n+\n+SYSTEM STOP FETCHES r2;\n+\n+SET insert_quorum_timeout=0;\n+\n+INSERT INTO r1 VALUES (4, '4'); -- { serverError 319 }\n+\n+SYSTEM START FETCHES r2;\n+\n+SYSTEM SYNC REPLICA r2;\n+\n+SELECT 'insert happened';\n+SELECT COUNT() FROM r1;\n+SELECT COUNT() FROM r2;\n+\n+DROP TABLE IF EXISTS r1;\n+DROP TABLE IF EXISTS r2;\n",
  "problem_statement": "Improvement of Quorum Inserts in ClickHouse\nI hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- New Feature\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nAdd parallel quorum inserts. This closes #15601.\r\n\r\nDetailed description / Documentation draft:\r\nto use parallel quorum insert set the value insert_quorum >= 2 and insert_quorum_parallel = 1\r\nThese inserts can be non-linearizable and can't run in parallel with non-parallel inserts. They also can't be used with sequential_consistency\n",
  "hints_text": "",
  "created_at": "2020-10-04T21:40:40Z",
  "modified_files": [
    "src/Core/Settings.h",
    "src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp",
    "src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.h",
    "src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "tests/integration/helpers/network.py",
    "b/tests/integration/test_quorum_inserts_parallel/__init__.py",
    "b/tests/integration/test_quorum_inserts_parallel/test.py",
    "b/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.reference",
    "b/tests/queries/0_stateless/01509_check_many_parallel_quorum_inserts.sh",
    "b/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.reference",
    "b/tests/queries/0_stateless/01509_check_parallel_quorum_inserts.sh",
    "b/tests/queries/0_stateless/01509_parallel_quorum_and_merge.reference",
    "b/tests/queries/0_stateless/01509_parallel_quorum_and_merge.sh",
    "b/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.reference",
    "b/tests/queries/0_stateless/01509_parallel_quorum_insert_no_replicas.sql"
  ]
}