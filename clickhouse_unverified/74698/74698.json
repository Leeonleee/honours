{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 74698,
  "instance_id": "ClickHouse__ClickHouse-74698",
  "issue_numbers": [
    "74419"
  ],
  "base_commit": "780742a31f637697a938242e28ce6156d875b549",
  "patch": "diff --git a/src/DataTypes/DataTypeTuple.cpp b/src/DataTypes/DataTypeTuple.cpp\nindex 1267338acb95..a5f5b6692bfc 100644\n--- a/src/DataTypes/DataTypeTuple.cpp\n+++ b/src/DataTypes/DataTypeTuple.cpp\n@@ -367,7 +367,7 @@ MutableSerializationInfoPtr DataTypeTuple::createSerializationInfo(const Seriali\n     for (const auto & elem : elems)\n         infos.push_back(elem->createSerializationInfo(settings));\n \n-    return std::make_shared<SerializationInfoTuple>(std::move(infos), names, settings);\n+    return std::make_shared<SerializationInfoTuple>(std::move(infos), names);\n }\n \n SerializationInfoPtr DataTypeTuple::getSerializationInfo(const IColumn & column) const\n@@ -387,7 +387,7 @@ SerializationInfoPtr DataTypeTuple::getSerializationInfo(const IColumn & column)\n         infos.push_back(const_pointer_cast<SerializationInfo>(element_info));\n     }\n \n-    return std::make_shared<SerializationInfoTuple>(std::move(infos), names, SerializationInfo::Settings{});\n+    return std::make_shared<SerializationInfoTuple>(std::move(infos), names);\n }\n \n void DataTypeTuple::forEachChild(const ChildCallback & callback) const\ndiff --git a/src/DataTypes/Serializations/SerializationInfoTuple.cpp b/src/DataTypes/Serializations/SerializationInfoTuple.cpp\nindex 7f36ecd64de8..405b8ab6d5a6 100644\n--- a/src/DataTypes/Serializations/SerializationInfoTuple.cpp\n+++ b/src/DataTypes/Serializations/SerializationInfoTuple.cpp\n@@ -15,9 +15,9 @@ namespace ErrorCodes\n     extern const int NOT_IMPLEMENTED;\n }\n \n-SerializationInfoTuple::SerializationInfoTuple(\n-    MutableSerializationInfos elems_, Names names_, const Settings & settings_)\n-    : SerializationInfo(ISerialization::Kind::DEFAULT, settings_)\n+SerializationInfoTuple::SerializationInfoTuple(MutableSerializationInfos elems_, Names names_)\n+    /// Pass default settings because Tuple column cannot be sparse itself.\n+    : SerializationInfo(ISerialization::Kind::DEFAULT, SerializationInfo::Settings{})\n     , elems(std::move(elems_))\n     , names(std::move(names_))\n {\n@@ -112,7 +112,7 @@ MutableSerializationInfoPtr SerializationInfoTuple::clone() const\n     for (const auto & elem : elems)\n         elems_cloned.push_back(elem->clone());\n \n-    auto ret = std::make_shared<SerializationInfoTuple>(std::move(elems_cloned), names, settings);\n+    auto ret = std::make_shared<SerializationInfoTuple>(std::move(elems_cloned), names);\n     ret->data = data;\n     return ret;\n }\n@@ -128,15 +128,15 @@ MutableSerializationInfoPtr SerializationInfoTuple::createWithType(\n     const auto & old_elements = old_tuple.getElements();\n     const auto & new_elements = new_tuple.getElements();\n \n-    assert(elems.size() == old_elements.size());\n-    assert(elems.size() == new_elements.size());\n+    chassert(elems.size() == old_elements.size());\n+    chassert(elems.size() == new_elements.size());\n \n     MutableSerializationInfos infos;\n     infos.reserve(elems.size());\n     for (size_t i = 0; i < elems.size(); ++i)\n         infos.push_back(elems[i]->createWithType(*old_elements[i], *new_elements[i], new_settings));\n \n-    return std::make_shared<SerializationInfoTuple>(std::move(infos), names, new_settings);\n+    return std::make_shared<SerializationInfoTuple>(std::move(infos), names);\n }\n \n void SerializationInfoTuple::serialializeKindBinary(WriteBuffer & out) const\ndiff --git a/src/DataTypes/Serializations/SerializationInfoTuple.h b/src/DataTypes/Serializations/SerializationInfoTuple.h\nindex c0050df9b6e7..2127611364fe 100644\n--- a/src/DataTypes/Serializations/SerializationInfoTuple.h\n+++ b/src/DataTypes/Serializations/SerializationInfoTuple.h\n@@ -8,7 +8,7 @@ namespace DB\n class SerializationInfoTuple : public SerializationInfo\n {\n public:\n-    SerializationInfoTuple(MutableSerializationInfos elems_, Names names_, const Settings & settings_);\n+    SerializationInfoTuple(MutableSerializationInfos elems_, Names names_);\n \n     bool hasCustomSerialization() const override;\n     bool structureEquals(const SerializationInfo & rhs) const override;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex f1376f98adc1..274308bffced 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -681,7 +681,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     for (const auto & [column_name, _] : columns)\n     {\n         auto & column = block.getByName(column_name);\n-        if (column.column->isSparse() && infos.getKind(column_name) != ISerialization::Kind::SPARSE)\n+        if (infos.getKind(column_name) != ISerialization::Kind::SPARSE)\n             column.column = recursiveRemoveSparse(column.column);\n     }\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03312_sparse_column_tuple.reference b/tests/queries/0_stateless/03312_sparse_column_tuple.reference\nnew file mode 100644\nindex 000000000000..b5888a1d0b96\n--- /dev/null\n+++ b/tests/queries/0_stateless/03312_sparse_column_tuple.reference\n@@ -0,0 +1,6 @@\n+1000\n+dst_sparse\tbudget\tDefault\t['currencyCode']\t['Sparse']\n+mytable_sparse\tbudget\tDefault\t['currencyCode']\t['Default']\n+1000\n+dst_sparse\tbudget\tDefault\t['currencyCode']\t['Sparse']\n+mytable_sparse\tbudget\tDefault\t['currencyCode']\t['Sparse']\ndiff --git a/tests/queries/0_stateless/03312_sparse_column_tuple.sql b/tests/queries/0_stateless/03312_sparse_column_tuple.sql\nnew file mode 100644\nindex 000000000000..7b8dc5a8abda\n--- /dev/null\n+++ b/tests/queries/0_stateless/03312_sparse_column_tuple.sql\n@@ -0,0 +1,54 @@\n+DROP TABLE IF EXISTS dst_sparse;\n+DROP TABLE IF EXISTS mytable_sparse;\n+\n+CREATE TABLE dst_sparse (\n+    `id` Int64,\n+    `budget` Tuple(currencyCode String)\n+)\n+ENGINE = MergeTree ORDER BY id\n+SETTINGS ratio_of_defaults_for_sparse_serialization = 0.9\n+AS SELECT number, arrayJoin([tuple('')]) FROM numbers(999);\n+\n+INSERT INTO dst_sparse VALUES (999, tuple('x'));\n+\n+OPTIMIZE TABLE dst_sparse FINAL;\n+\n+CREATE TABLE mytable_sparse ENGINE = MergeTree ORDER BY id\n+SETTINGS ratio_of_defaults_for_sparse_serialization = 1.0\n+AS SELECT id, budget FROM dst_sparse;\n+\n+SELECT count() from mytable_sparse;\n+\n+SELECT DISTINCT table, column, serialization_kind, subcolumns.names, subcolumns.serializations\n+FROM system.parts_columns\n+WHERE database = currentDatabase() AND table IN ('dst_sparse', 'mytable_sparse') AND active AND column = 'budget'\n+ORDER BY table;\n+\n+DROP TABLE IF EXISTS dst_sparse;\n+DROP TABLE IF EXISTS mytable_sparse;\n+\n+CREATE TABLE dst_sparse (\n+    `id` Int64,\n+    `budget` Tuple(currencyCode String)\n+)\n+ENGINE = MergeTree ORDER BY id\n+SETTINGS ratio_of_defaults_for_sparse_serialization = 0.9\n+AS SELECT number, arrayJoin([tuple('')]) FROM numbers(999);\n+\n+INSERT INTO dst_sparse VALUES (999, tuple('x'));\n+\n+OPTIMIZE TABLE dst_sparse FINAL;\n+\n+CREATE TABLE mytable_sparse ENGINE = MergeTree ORDER BY id\n+SETTINGS ratio_of_defaults_for_sparse_serialization = 0.9\n+AS SELECT id, budget FROM dst_sparse;\n+\n+SELECT count() from mytable_sparse;\n+\n+SELECT DISTINCT table, column, serialization_kind, subcolumns.names, subcolumns.serializations\n+FROM system.parts_columns\n+WHERE database = currentDatabase() AND table IN ('dst_sparse', 'mytable_sparse') AND active AND column = 'budget'\n+ORDER BY table;\n+\n+DROP TABLE IF EXISTS dst_sparse;\n+DROP TABLE IF EXISTS mytable_sparse;\n",
  "problem_statement": "Potential bug: LOGICAL_ERROR  Bad cast from type DB::ColumnSparse to ... \n**Describe the unexpected behaviour**\r\nWe are seeing Bad Cast LOGICAL_ERROR exceptions that seem to involve `ColumnSparse` in certain circumstances. Ive attached [a video to the slack thread](https://clickhousedb.slack.com/files/U05F05MTAE7/F087SHED61G/logical_error.mp4) (mentioned below) with an example of the behaviour we are seeing.\r\n\r\nThe issue seems to occur when inserting _certain_ columns from a query into a table. I have a case where there are two columns with exactly the same type, but a different distribution of data, and one of the columns works (allows the insert) and the other fails (causes a bad cast logical error).\r\n\r\nIn general, any queries that contain a problematic column will fail, in the simplest case if `budget` is such a column, like this :\r\n\r\n```\r\n          insert into mytable (budget)   \r\n          select budget from mytable2\r\n\r\n          where somecolumn = 'somevalue' -- does not fail if clause is omitted\r\n```\r\n\r\nInterestingly, the error only occurs when I have a `WHERE` clause (see above).\r\n\r\nAlso, adding an `ORDER BY` clause seems to workaround the issue. This could be a clue...?\r\n\r\n\r\n```\r\ninsert into mytable (id, budget)   \r\nselect id, budget from mytable2\r\norder by id\r\n```\r\n\r\nSimilar issues for `CREATE TABLE ... AS` : \r\n\r\n```\r\ncreate or replace table mytable\r\nENGINE = MergeTree\r\nORDER BY id AS \r\nselect id, budget from mytable2\r\norder by id\r\n```\r\n\r\n\r\nAlso there was a suggestion to turn off the setting to `enable_parsing_to_custom_serialization`. Which I have done, but the problem persists. However, it is not clear to me if this suggested workaround means that I have to repopulate the source table of my query, or if it was meant to fix the problem with querying the already existing table.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\nWe are seeing this after upgrading from `24.3.8.13` to `24.12.1.1614`\r\n\r\n**Expected behavior**\r\nLOGICAL_ERROR should not occur, and especially not inconsistently\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2025.01.06 15:03:24.895077 [ 47842 ] {e65ee33e-769c-4f72-988e-9f10668c6ec7} <Error> DynamicQueryHandler: Code: 49. DB::Exception: Bad cast from type DB::ColumnSparse to DB::ColumnVector<long>. (LOGICAL_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000d74955b\r\n1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x000000000863c82c\r\n2. DB::Exception::Exception<String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&) @ 0x000000000863e66b\r\n3. _Z11typeid_castIRKN2DB12ColumnVectorIlEEKNS0_7IColumnEQsr3stdE14is_reference_vIT_EES7_RT0_ @ 0x0000000008668644\r\n4. DB::SerializationNumber<long>::serializeBinaryBulk(DB::IColumn const&, DB::WriteBuffer&, unsigned long, unsigned long) const @ 0x0000000007ffef96\r\n5. DB::ISerialization::serializeBinaryBulkWithMultipleStreams(DB::IColumn const&, unsigned long, unsigned long, DB::ISerialization::SerializeBinaryBulkSettings&, std::shared_ptr<DB::ISerialization::SerializeBinaryBulkState>&) const @ 0x0000000011352586\r\n6. DB::SerializationTuple::serializeBinaryBulkWithMultipleStreams(DB::IColumn const&, unsigned long, unsigned long, DB::ISerialization::SerializeBinaryBulkSettings&, std::shared_ptr<DB::ISerialization::SerializeBinaryBulkState>&) const @ 0x00000000113dd120\r\n7. DB::SerializationTuple::serializeBinaryBulkWithMultipleStreams(DB::IColumn const&, unsigned long, unsigned long, DB::ISerialization::SerializeBinaryBulkSettings&, std::shared_ptr<DB::ISerialization::SerializeBinaryBulkState>&) const @ 0x00000000113dd120\r\n8. DB::MergeTreeDataPartWriterCompact::writeDataBlockPrimaryIndexAndSkipIndices(DB::Block const&, std::vector<DB::Granule, std::allocator<DB::Granule>> const&) @ 0x00000000130f1e23\r\n9. DB::MergeTreeDataPartWriterCompact::fillChecksums(DB::MergeTreeDataPartChecksums&, std::unordered_set<String, std::hash<String>, std::equal_to<String>, std::allocator<String>>&) @ 0x00000000130f2e1e\r\n10. DB::MergedBlockOutputStream::finalizePartAsync(std::shared_ptr<DB::IMergeTreeDataPart> const&, bool, DB::NamesAndTypesList const*, DB::MergeTreeDataPartChecksums*, std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>>*) @ 0x000000001325ec2c\r\n11. DB::MergeTreeDataWriter::writeTempPartImpl(DB::BlockWithPartition&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::Context const>, long, bool) @ 0x000000001313bf2c\r\n12. DB::ReplicatedMergeTreeSinkImpl<false>::consume(DB::Chunk&) @ 0x000000001337dea1\r\n13. DB::SinkToStorage::onConsume(DB::Chunk) @ 0x0000000013aa9084\r\n14. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::ExceptionKeepingTransform::work()::$_1, void ()>>(std::__function::__policy_storage const*) @ 0x00000000139cc0f8\r\n15. DB::runStep(std::function<void ()>, DB::ThreadStatus*, std::atomic<unsigned long>*) @ 0x00000000139cbdff\r\n16. DB::ExceptionKeepingTransform::work() @ 0x00000000139cb6fa\r\n17. DB::ExecutionThreadContext::executeTask() @ 0x0000000013754be7\r\n18. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000013747ec5\r\n19. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x000000001374a84e\r\n20. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::worker() @ 0x000000000d828c1b\r\n21. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false, true>::ThreadFromGlobalPoolImpl<void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*)(), ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*>(void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*&&)(), ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000d82f6e2\r\n22. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000d825ec2\r\n23. void* std::__thread_proxy[abi:v15007]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000d82d27a\r\n24. ? @ 0x00007d17b692eac3\r\n25. ? @ 0x00007d17b69c0850\r\n (version 24.12.1.1614 (official build))\r\n```\r\n\r\n**Additional context**\r\nLink to slack thread :  https://clickhousedb.slack.com/archives/CU478UEQZ/p1736175100763289\r\n_Possibly_ related PR : https://github.com/ClickHouse/ClickHouse/pull/69828/files#diff-66d9da80aff8e19296846f700016fe6cd3ac954329348c996e913b7239b2ffc9\r\n\r\n\r\n\n",
  "hints_text": ">However, it is not clear to me if this suggested workaround means that I have to repopulate the source table\r\n\r\nI guess `optimize table mytable2 partition id .... final` against related partitions, though it's a pretty heavy query.\r\nOr maybe `alter table mytable2 update somecolumn = somecolumn where 1` \r\n\r\n(make a backup before, even simple `alter table mytable2 freeze`)\nThough, I checked the video. It happens with the new table, so enable_parsing_to_custom_serialization=0 does not help.\ncan you please share \r\n\r\n```\r\nselect name, value from system.settings where changed\r\n```\n> can you please share\r\n> \r\n> ```\r\n> select name, value from system.settings where changed\r\n> ```\r\n\r\n[system_settings.csv](https://github.com/user-attachments/files/18395077/system_settings.csv)\r\n\nIve ingested the data from csv into a fresh table and performs a variety of inserts which eventually result in the error. The basic parts of the script are here : \r\n\r\n```\r\nCREATE or replace TABLE dagster.src (\r\n    `id` Int64,\r\n    `budget` Tuple(currencyCode String, microAmount Int64)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nCREATE or replace TABLE dagster.dst (\r\n    `id` Int64,\r\n    `budget` Tuple(currencyCode String, microAmount Int64)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\ninsert into dagster.src from infile 'bkp.tsv';\r\n \r\ninsert into dagster.dst (budget) select budget from dagster.src where id != 0\r\n```\r\n\r\nIve attached the data file and the system settings for the account that I used to run the script.\r\n\r\nhttps://github.com/user-attachments/assets/7798694e-b283-453a-88c8-e3edb88fe24b\r\n\r\n[settings.csv](https://github.com/user-attachments/files/18397523/settings.csv)\r\n\r\n[bkp.tsv.gz](https://github.com/user-attachments/files/18398432/bkp.tsv.gz)\r\n\r\n\nrepro: https://fiddle.clickhouse.com/7d312685-0b04-457b-a057-78e4ce32402e\nit seems `ratio_of_defaults_for_sparse_serialization = 1` helps https://fiddle.clickhouse.com/b9b315f7-cccd-49bd-a599-808493574a8b\r\n\r\n@the4thamigo-uk you can do \r\n\r\n```\r\ncat /etc/clickhouse-server/config.d/config_substitutes.xml\r\n<?xml version=\"1.0\"?>\r\n<clickhouse>\r\n    <merge_tree>\r\n     <ratio_of_defaults_for_sparse_serialization>1</ratio_of_defaults_for_sparse_serialization>\r\n    </merge_tree>\r\n</clickhouse>\r\n```\r\n\r\nrestart Clichkhouse, and recreate a table.\r\n\r\n@CurtizJ\nrepro with synthetic data https://fiddle.clickhouse.com/73296290-bd75-4454-9dfd-1506e665d4db\r\n\r\nhttps://fiddle.clickhouse.com/37f1d492-60ac-4b3d-b927-8ae4eeb0ad1b\r\n\r\n\r\n```sql\r\nCREATE or replace TABLE dst (\r\n    `id` Int64,\r\n    `budget` Tuple(currencyCode String)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id\r\nas select number, arrayJoin([tuple(''), tuple('x')]) \r\nfrom numbers(1000);\r\n\r\ncreate or replace table mytable ENGINE = MergeTree ORDER BY id  AS select id, budget from dst;\r\n\r\nDB::Exception: Bad cast from type DB::ColumnSparse to DB::ColumnString. (LOGICAL_ERROR)\r\n```",
  "created_at": "2025-01-17T00:15:20Z",
  "modified_files": [
    "src/DataTypes/DataTypeTuple.cpp",
    "src/DataTypes/Serializations/SerializationInfoTuple.cpp",
    "src/DataTypes/Serializations/SerializationInfoTuple.h",
    "src/Storages/MergeTree/MergeTreeDataWriter.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/03312_sparse_column_tuple.reference",
    "b/tests/queries/0_stateless/03312_sparse_column_tuple.sql"
  ]
}