{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 39541,
  "instance_id": "ClickHouse__ClickHouse-39541",
  "issue_numbers": [
    "39534"
  ],
  "base_commit": "6fdcb009ffe3592f15148f25bc504a906ccfea02",
  "patch": "diff --git a/programs/obfuscator/Obfuscator.cpp b/programs/obfuscator/Obfuscator.cpp\nindex a194718db582..056ed85c6703 100644\n--- a/programs/obfuscator/Obfuscator.cpp\n+++ b/programs/obfuscator/Obfuscator.cpp\n@@ -34,6 +34,10 @@\n #include <base/bit_cast.h>\n #include <IO/ReadBufferFromFileDescriptor.h>\n #include <IO/WriteBufferFromFileDescriptor.h>\n+#include <IO/ReadBufferFromFile.h>\n+#include <IO/WriteBufferFromFile.h>\n+#include <Compression/CompressedReadBuffer.h>\n+#include <Compression/CompressedWriteBuffer.h>\n #include <memory>\n #include <cmath>\n #include <unistd.h>\n@@ -95,6 +99,9 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n     extern const int NOT_IMPLEMENTED;\n     extern const int CANNOT_SEEK_THROUGH_FILE;\n+    extern const int UNKNOWN_FORMAT_VERSION;\n+    extern const int INCORRECT_NUMBER_OF_COLUMNS;\n+    extern const int TYPE_MISMATCH;\n }\n \n \n@@ -115,6 +122,12 @@ class IModel\n     /// Deterministically change seed to some other value. This can be used to generate more values than were in source.\n     virtual void updateSeed() = 0;\n \n+    /// Save into file. Binary, platform-dependent, version-dependent serialization.\n+    virtual void serialize(WriteBuffer & out) const = 0;\n+\n+    /// Read from file\n+    virtual void deserialize(ReadBuffer & in) = 0;\n+\n     virtual ~IModel() = default;\n };\n \n@@ -189,6 +202,8 @@ class UnsignedIntegerModel : public IModel\n \n     void train(const IColumn &) override {}\n     void finalize() override {}\n+    void serialize(WriteBuffer &) const override {}\n+    void deserialize(ReadBuffer &) override {}\n \n     ColumnPtr generate(const IColumn & column) override\n     {\n@@ -230,6 +245,8 @@ class SignedIntegerModel : public IModel\n \n     void train(const IColumn &) override {}\n     void finalize() override {}\n+    void serialize(WriteBuffer &) const override {}\n+    void deserialize(ReadBuffer &) override {}\n \n     ColumnPtr generate(const IColumn & column) override\n     {\n@@ -279,6 +296,8 @@ class FloatModel : public IModel\n \n     void train(const IColumn &) override {}\n     void finalize() override {}\n+    void serialize(WriteBuffer &) const override {}\n+    void deserialize(ReadBuffer &) override {}\n \n     ColumnPtr generate(const IColumn & column) override\n     {\n@@ -311,6 +330,8 @@ class IdentityModel : public IModel\n public:\n     void train(const IColumn &) override {}\n     void finalize() override {}\n+    void serialize(WriteBuffer &) const override {}\n+    void deserialize(ReadBuffer &) override {}\n \n     ColumnPtr generate(const IColumn & column) override\n     {\n@@ -395,6 +416,8 @@ class FixedStringModel : public IModel\n \n     void train(const IColumn &) override {}\n     void finalize() override {}\n+    void serialize(WriteBuffer &) const override {}\n+    void deserialize(ReadBuffer &) override {}\n \n     ColumnPtr generate(const IColumn & column) override\n     {\n@@ -431,6 +454,8 @@ class UUIDModel : public IModel\n \n     void train(const IColumn &) override {}\n     void finalize() override {}\n+    void serialize(WriteBuffer &) const override {}\n+    void deserialize(ReadBuffer &) override {}\n \n     ColumnPtr generate(const IColumn & column) override\n     {\n@@ -469,6 +494,8 @@ class DateTimeModel : public IModel\n \n     void train(const IColumn &) override {}\n     void finalize() override {}\n+    void serialize(WriteBuffer &) const override {}\n+    void deserialize(ReadBuffer &) override {}\n \n     ColumnPtr generate(const IColumn & column) override\n     {\n@@ -512,6 +539,26 @@ struct MarkovModelParameters\n     size_t frequency_add;\n     double frequency_desaturate;\n     size_t determinator_sliding_window_size;\n+\n+    void serialize(WriteBuffer & out) const\n+    {\n+        writeBinary(order, out);\n+        writeBinary(frequency_cutoff, out);\n+        writeBinary(num_buckets_cutoff, out);\n+        writeBinary(frequency_add, out);\n+        writeBinary(frequency_desaturate, out);\n+        writeBinary(determinator_sliding_window_size, out);\n+    }\n+\n+    void deserialize(ReadBuffer & in)\n+    {\n+        readBinary(order, in);\n+        readBinary(frequency_cutoff, in);\n+        readBinary(num_buckets_cutoff, in);\n+        readBinary(frequency_add, in);\n+        readBinary(frequency_desaturate, in);\n+        readBinary(determinator_sliding_window_size, in);\n+    }\n };\n \n \n@@ -565,6 +612,39 @@ class MarkovModel\n \n             return END;\n         }\n+\n+        void serialize(WriteBuffer & out) const\n+        {\n+            writeBinary(total, out);\n+            writeBinary(count_end, out);\n+\n+            size_t size = buckets.size();\n+            writeBinary(size, out);\n+\n+            for (const auto & elem : buckets)\n+            {\n+                writeBinary(elem.first, out);\n+                writeBinary(elem.second, out);\n+            }\n+        }\n+\n+        void deserialize(ReadBuffer & in)\n+        {\n+            readBinary(total, in);\n+            readBinary(count_end, in);\n+\n+            size_t size = 0;\n+            readBinary(size, in);\n+\n+            buckets.reserve(size);\n+            for (size_t i = 0; i < size; ++i)\n+            {\n+                Buckets::value_type elem;\n+                readBinary(elem.first, in);\n+                readBinary(elem.second, in);\n+                buckets.emplace(std::move(elem));\n+            }\n+        }\n     };\n \n     using Table = HashMap<NGramHash, Histogram, TrivialHash>;\n@@ -621,6 +701,37 @@ class MarkovModel\n     explicit MarkovModel(MarkovModelParameters params_)\n         : params(std::move(params_)), code_points(params.order, BEGIN) {}\n \n+    void serialize(WriteBuffer & out) const\n+    {\n+        params.serialize(out);\n+\n+        size_t size = table.size();\n+        writeBinary(size, out);\n+\n+        for (const auto & elem : table)\n+        {\n+            writeBinary(elem.getKey(), out);\n+            elem.getMapped().serialize(out);\n+        }\n+    }\n+\n+    void deserialize(ReadBuffer & in)\n+    {\n+        params.deserialize(in);\n+\n+        size_t size = 0;\n+        readBinary(size, in);\n+\n+        table.reserve(size);\n+        for (size_t i = 0; i < size; ++i)\n+        {\n+            NGramHash key{};\n+            readBinary(key, in);\n+            Histogram & histogram = table[key];\n+            histogram.deserialize(in);\n+        }\n+    }\n+\n     void consume(const char * data, size_t size)\n     {\n         /// First 'order' number of code points are pre-filled with BEGIN.\n@@ -655,7 +766,6 @@ class MarkovModel\n         }\n     }\n \n-\n     void finalize()\n     {\n         if (params.num_buckets_cutoff)\n@@ -878,6 +988,16 @@ class StringModel : public IModel\n     {\n         seed = hash(seed);\n     }\n+\n+    void serialize(WriteBuffer & out) const override\n+    {\n+        markov_model.serialize(out);\n+    }\n+\n+    void deserialize(ReadBuffer & in) override\n+    {\n+        markov_model.deserialize(in);\n+    }\n };\n \n \n@@ -916,6 +1036,16 @@ class ArrayModel : public IModel\n     {\n         nested_model->updateSeed();\n     }\n+\n+    void serialize(WriteBuffer & out) const override\n+    {\n+        nested_model->serialize(out);\n+    }\n+\n+    void deserialize(ReadBuffer & in) override\n+    {\n+        nested_model->deserialize(in);\n+    }\n };\n \n \n@@ -954,6 +1084,16 @@ class NullableModel : public IModel\n     {\n         nested_model->updateSeed();\n     }\n+\n+    void serialize(WriteBuffer & out) const override\n+    {\n+        nested_model->serialize(out);\n+    }\n+\n+    void deserialize(ReadBuffer & in) override\n+    {\n+        nested_model->deserialize(in);\n+    }\n };\n \n \n@@ -1046,6 +1186,18 @@ class Obfuscator\n         for (auto & model : models)\n             model->updateSeed();\n     }\n+\n+    void serialize(WriteBuffer & out) const\n+    {\n+        for (const auto & model : models)\n+            model->serialize(out);\n+    }\n+\n+    void deserialize(ReadBuffer & in)\n+    {\n+        for (auto & model : models)\n+            model->deserialize(in);\n+    }\n };\n \n }\n@@ -1068,8 +1220,10 @@ try\n         (\"input-format\", po::value<std::string>(), \"input format of the initial table data\")\n         (\"output-format\", po::value<std::string>(), \"default output format\")\n         (\"seed\", po::value<std::string>(), \"seed (arbitrary string), must be random string with at least 10 bytes length; note that a seed for each column is derived from this seed and a column name: you can obfuscate data for different tables and as long as you use identical seed and identical column names, the data for corresponding non-text columns for different tables will be transformed in the same way, so the data for different tables can be JOINed after obfuscation\")\n-        (\"limit\", po::value<UInt64>(), \"if specified - stop after generating that number of rows\")\n+        (\"limit\", po::value<UInt64>(), \"if specified - stop after generating that number of rows; the limit can be also greater than the number of source dataset - in this case it will process the dataset in a loop more than one time, using different seeds on every iteration, generating result as large as needed\")\n         (\"silent\", po::value<bool>()->default_value(false), \"don't print information messages to stderr\")\n+        (\"save\", po::value<std::string>(), \"save the models after training to the specified file. You can use --limit 0 to skip the generation step. The file is using binary, platform-dependent, opaque serialization format. The model parameters are saved, while the seed is not.\")\n+        (\"load\", po::value<std::string>(), \"load the models instead of training from the specified file. The table structure must match the saved file. The seed should be specified separately, while other model parameters are loaded.\")\n         (\"order\", po::value<UInt64>()->default_value(5), \"order of markov model to generate strings\")\n         (\"frequency-cutoff\", po::value<UInt64>()->default_value(5), \"frequency cutoff for markov model: remove all buckets with count less than specified\")\n         (\"num-buckets-cutoff\", po::value<UInt64>()->default_value(0), \"cutoff for number of different possible continuations for a context: remove all histograms with less than specified number of buckets\")\n@@ -1096,12 +1250,26 @@ try\n         return 0;\n     }\n \n+    if (options.count(\"save\") && options.count(\"load\"))\n+    {\n+        std::cerr << \"The options --save and --load cannot be used together.\\n\";\n+        return 1;\n+    }\n+\n     UInt64 seed = sipHash64(options[\"seed\"].as<std::string>());\n \n     std::string structure = options[\"structure\"].as<std::string>();\n     std::string input_format = options[\"input-format\"].as<std::string>();\n     std::string output_format = options[\"output-format\"].as<std::string>();\n \n+    std::string load_from_file;\n+    std::string save_into_file;\n+\n+    if (options.count(\"load\"))\n+        load_from_file = options[\"load\"].as<std::string>();\n+    else if (options.count(\"save\"))\n+        save_into_file = options[\"save\"].as<std::string>();\n+\n     UInt64 limit = 0;\n     if (options.count(\"limit\"))\n         limit = options[\"limit\"].as<UInt64>();\n@@ -1117,7 +1285,7 @@ try\n     markov_model_params.frequency_desaturate = options[\"frequency-desaturate\"].as<double>();\n     markov_model_params.determinator_sliding_window_size = options[\"determinator-sliding-window-size\"].as<UInt64>();\n \n-    // Create header block\n+    /// Create the header block\n     std::vector<std::string> structure_vals;\n     boost::split(structure_vals, structure, boost::algorithm::is_any_of(\" ,\"), boost::algorithm::token_compress_on);\n \n@@ -1143,6 +1311,7 @@ try\n     ReadBufferFromFileDescriptor file_in(STDIN_FILENO);\n     WriteBufferFromFileDescriptor file_out(STDOUT_FILENO);\n \n+    if (load_from_file.empty())\n     {\n         /// stdin must be seekable\n         auto res = lseek(file_in.getFD(), 0, SEEK_SET);\n@@ -1156,6 +1325,9 @@ try\n \n     /// Train step\n     UInt64 source_rows = 0;\n+\n+    bool rewind_needed = false;\n+    if (load_from_file.empty())\n     {\n         if (!silent)\n             std::cerr << \"Training models\\n\";\n@@ -1173,11 +1345,71 @@ try\n             if (!silent)\n                 std::cerr << \"Processed \" << source_rows << \" rows\\n\";\n         }\n+\n+        obfuscator.finalize();\n+        rewind_needed = true;\n     }\n+    else\n+    {\n+        if (!silent)\n+            std::cerr << \"Loading models\\n\";\n+\n+        ReadBufferFromFile model_file_in(load_from_file);\n+        CompressedReadBuffer model_in(model_file_in);\n+\n+        UInt8 version = 0;\n+        readBinary(version, model_in);\n+        if (version != 0)\n+            throw Exception(\"Unknown version of the model file\", ErrorCodes::UNKNOWN_FORMAT_VERSION);\n+\n+        readBinary(source_rows, model_in);\n+\n+        Names data_types = header.getDataTypeNames();\n+        size_t header_size = 0;\n+        readBinary(header_size, model_in);\n+        if (header_size != data_types.size())\n+            throw Exception(\"The saved model was created for different number of columns\", ErrorCodes::INCORRECT_NUMBER_OF_COLUMNS);\n+\n+        for (size_t i = 0; i < header_size; ++i)\n+        {\n+            String type;\n+            readBinary(type, model_in);\n+            if (type != data_types[i])\n+                throw Exception(\"The saved model was created for different types of columns\", ErrorCodes::TYPE_MISMATCH);\n+        }\n \n-    obfuscator.finalize();\n+        obfuscator.deserialize(model_in);\n+    }\n+\n+    if (!save_into_file.empty())\n+    {\n+        if (!silent)\n+            std::cerr << \"Saving models\\n\";\n+\n+        WriteBufferFromFile model_file_out(save_into_file);\n+        CompressedWriteBuffer model_out(model_file_out, CompressionCodecFactory::instance().get(\"ZSTD\", 1));\n+\n+        /// You can change version on format change, it is currently set to zero.\n+        UInt8 version = 0;\n+        writeBinary(version, model_out);\n+\n+        writeBinary(source_rows, model_out);\n+\n+        /// We are writing the data types for validation, because the models serialization depends on the data types.\n+        Names data_types = header.getDataTypeNames();\n+        size_t header_size = data_types.size();\n+        writeBinary(header_size, model_out);\n+        for (const auto & type : data_types)\n+            writeBinary(type, model_out);\n+\n+        /// Write the models.\n+        obfuscator.serialize(model_out);\n+\n+        model_out.finalize();\n+        model_file_out.finalize();\n+    }\n \n-    if (!limit)\n+    if (!options.count(\"limit\"))\n         limit = source_rows;\n \n     /// Generation step\n@@ -1187,7 +1419,8 @@ try\n         if (!silent)\n             std::cerr << \"Generating data\\n\";\n \n-        file_in.seek(0, SEEK_SET);\n+        if (rewind_needed)\n+            file_in.rewind();\n \n         Pipe pipe(context->getInputFormat(input_format, file_in, header, max_block_size));\n \n@@ -1220,6 +1453,7 @@ try\n         out_executor.finish();\n \n         obfuscator.updateSeed();\n+        rewind_needed = true;\n     }\n \n     return 0;\n",
  "test_patch": "diff --git a/tests/queries/1_stateful/00096_obfuscator_save_load.reference b/tests/queries/1_stateful/00096_obfuscator_save_load.reference\nnew file mode 100644\nindex 000000000000..af0e6eade55f\n--- /dev/null\n+++ b/tests/queries/1_stateful/00096_obfuscator_save_load.reference\n@@ -0,0 +1,3 @@\n+403499\n+1000\t320\t171\t23\n+2500\t569\t354\t13\ndiff --git a/tests/queries/1_stateful/00096_obfuscator_save_load.sh b/tests/queries/1_stateful/00096_obfuscator_save_load.sh\nnew file mode 100755\nindex 000000000000..c90eee1d0f9d\n--- /dev/null\n+++ b/tests/queries/1_stateful/00096_obfuscator_save_load.sh\n@@ -0,0 +1,18 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT --max_threads 1 --query=\"SELECT URL, Title, SearchPhrase FROM test.hits LIMIT 1000\" > \"${CLICKHOUSE_TMP}\"/data.tsv\n+\n+$CLICKHOUSE_OBFUSCATOR --structure \"URL String, Title String, SearchPhrase String\" --input-format TSV --output-format TSV --seed hello --limit 0 --save \"${CLICKHOUSE_TMP}\"/model.bin < \"${CLICKHOUSE_TMP}\"/data.tsv 2>/dev/null\n+wc -c < \"${CLICKHOUSE_TMP}\"/model.bin\n+$CLICKHOUSE_OBFUSCATOR --structure \"URL String, Title String, SearchPhrase String\" --input-format TSV --output-format TSV --seed hello --limit 2500 --load \"${CLICKHOUSE_TMP}\"/model.bin < \"${CLICKHOUSE_TMP}\"/data.tsv > \"${CLICKHOUSE_TMP}\"/data2500.tsv 2>/dev/null\n+rm \"${CLICKHOUSE_TMP}\"/model.bin\n+\n+$CLICKHOUSE_LOCAL --structure \"URL String, Title String, SearchPhrase String\" --input-format TSV --output-format TSV --query \"SELECT count(), uniq(URL), uniq(Title), uniq(SearchPhrase) FROM table\" < \"${CLICKHOUSE_TMP}\"/data.tsv\n+$CLICKHOUSE_LOCAL --structure \"URL String, Title String, SearchPhrase String\" --input-format TSV --output-format TSV --query \"SELECT count(), uniq(URL), uniq(Title), uniq(SearchPhrase) FROM table\" < \"${CLICKHOUSE_TMP}\"/data2500.tsv\n+\n+rm \"${CLICKHOUSE_TMP}\"/data.tsv\n+rm \"${CLICKHOUSE_TMP}\"/data2500.tsv\n",
  "problem_statement": "clickhouse-obfuscator: allow to save and load models.\n**Use case**\r\n\r\nGenerate models once and reuse them.\r\n\r\n**Describe the solution you'd like**\r\n\r\nAdd command line options:\r\n```\r\n--save file - write serialized models on disk after training and before generating the data; the user can set --limit to 0 to only train the models and save them without proceeding to data generation;\r\n--load file - instead of training the models, load the prepared models from the file and start generating the data.\r\n```\r\n\r\nSerialization format: one byte for version (equals to 0), then simple binary serialization of all the models; the whole file is compressed with ZSTD level 1 using CompressedWriteBuffer (no need to allow any compression options to choose).\n",
  "hints_text": "",
  "created_at": "2022-07-25T01:28:46Z"
}