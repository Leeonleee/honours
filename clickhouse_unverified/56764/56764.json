{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 56764,
  "instance_id": "ClickHouse__ClickHouse-56764",
  "issue_numbers": [
    "56256"
  ],
  "base_commit": "b9cc1580e0357a0ca2fa761381797c3737677f55",
  "patch": "diff --git a/src/Processors/Executors/ExecutionThreadContext.h b/src/Processors/Executors/ExecutionThreadContext.h\nindex eb048f8ab091..deacd0380fa1 100644\n--- a/src/Processors/Executors/ExecutionThreadContext.h\n+++ b/src/Processors/Executors/ExecutionThreadContext.h\n@@ -43,6 +43,14 @@ class ExecutionThreadContext\n     const bool profile_processors;\n     const bool trace_processors;\n \n+    /// There is a performance optimization that schedules a task to the current thread, avoiding global task queue.\n+    /// Optimization decreases contention on global task queue but may cause starvation.\n+    /// See 01104_distributed_numbers_test.sql\n+    /// This constant tells us that we should skip the optimization\n+    /// if it was applied more than `max_scheduled_local_tasks` in a row.\n+    constexpr static size_t max_scheduled_local_tasks = 128;\n+    size_t num_scheduled_local_tasks = 0;\n+\n     void wait(std::atomic_bool & finished);\n     void wakeUp();\n \ndiff --git a/src/Processors/Executors/ExecutorTasks.cpp b/src/Processors/Executors/ExecutorTasks.cpp\nindex e61d225a968c..ec1fc539884a 100644\n--- a/src/Processors/Executors/ExecutorTasks.cpp\n+++ b/src/Processors/Executors/ExecutorTasks.cpp\n@@ -53,6 +53,17 @@ void ExecutorTasks::tryGetTask(ExecutionThreadContext & context)\n     {\n         std::unique_lock lock(mutex);\n \n+    #if defined(OS_LINUX)\n+        if (num_threads == 1)\n+        {\n+            if (auto res = async_task_queue.tryGetReadyTask(lock))\n+            {\n+                context.setTask(static_cast<ExecutingGraph::Node *>(res.data));\n+                return;\n+            }\n+        }\n+    #endif\n+\n         /// Try get async task assigned to this thread or any other task from queue.\n         if (auto * async_task = context.tryPopAsyncTask())\n         {\n@@ -109,11 +120,15 @@ void ExecutorTasks::pushTasks(Queue & queue, Queue & async_queue, ExecutionThrea\n     context.setTask(nullptr);\n \n     /// Take local task from queue if has one.\n-    if (!queue.empty() && !context.hasAsyncTasks())\n+    if (!queue.empty() && !context.hasAsyncTasks()\n+        && context.num_scheduled_local_tasks < context.max_scheduled_local_tasks)\n     {\n+        ++context.num_scheduled_local_tasks;\n         context.setTask(queue.front());\n         queue.pop();\n     }\n+    else\n+        context.num_scheduled_local_tasks = 0;\n \n     if (!queue.empty() || !async_queue.empty())\n     {\ndiff --git a/src/Processors/Executors/PollingQueue.cpp b/src/Processors/Executors/PollingQueue.cpp\nindex 40f968621b17..4bc0372dfbe0 100644\n--- a/src/Processors/Executors/PollingQueue.cpp\n+++ b/src/Processors/Executors/PollingQueue.cpp\n@@ -65,7 +65,7 @@ static std::string dumpTasks(const std::unordered_map<std::uintptr_t, PollingQue\n     return res.str();\n }\n \n-PollingQueue::TaskData PollingQueue::wait(std::unique_lock<std::mutex> & lock)\n+PollingQueue::TaskData PollingQueue::getTask(std::unique_lock<std::mutex> & lock, int timeout)\n {\n     if (is_finished)\n         return {};\n@@ -74,10 +74,13 @@ PollingQueue::TaskData PollingQueue::wait(std::unique_lock<std::mutex> & lock)\n \n     epoll_event event;\n     event.data.ptr = nullptr;\n-    epoll.getManyReady(1, &event, -1);\n+    size_t num_events = epoll.getManyReady(1, &event, timeout);\n \n     lock.lock();\n \n+    if (num_events == 0)\n+        return {};\n+\n     if (event.data.ptr == pipe_fd)\n         return {};\n \ndiff --git a/src/Processors/Executors/PollingQueue.h b/src/Processors/Executors/PollingQueue.h\nindex 100d762b7319..d709b7c92019 100644\n--- a/src/Processors/Executors/PollingQueue.h\n+++ b/src/Processors/Executors/PollingQueue.h\n@@ -31,6 +31,8 @@ class PollingQueue\n     std::atomic_bool is_finished = false;\n     std::unordered_map<std::uintptr_t, TaskData> tasks;\n \n+    TaskData getTask(std::unique_lock<std::mutex> & lock, int timeout);\n+\n public:\n     PollingQueue();\n     ~PollingQueue();\n@@ -44,7 +46,12 @@ class PollingQueue\n     /// Wait for any descriptor. If no descriptors in queue, blocks.\n     /// Returns ptr which was inserted into queue or nullptr if finished was called.\n     /// Lock is unlocked during waiting.\n-    TaskData wait(std::unique_lock<std::mutex> & lock);\n+    TaskData wait(std::unique_lock<std::mutex> & lock) { return getTask(lock, -1); }\n+\n+    /// Get any ready descriptor.\n+    /// Returns nullptr if no descriptor is ready or if finished was called.\n+    /// Does not block.\n+    TaskData tryGetReadyTask(std::unique_lock<std::mutex> & lock) { return getTask(lock, 0); }\n \n     /// Interrupt waiting.\n     void finish();\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01104_distributed_numbers_test.reference b/tests/queries/0_stateless/01104_distributed_numbers_test.reference\nindex c5079fa2cfd0..c7e52eb2a2fe 100644\n--- a/tests/queries/0_stateless/01104_distributed_numbers_test.reference\n+++ b/tests/queries/0_stateless/01104_distributed_numbers_test.reference\n@@ -2,3 +2,6 @@\n 100\n 100\n 100\n+300\n+100\n+100\ndiff --git a/tests/queries/0_stateless/01104_distributed_numbers_test.sql b/tests/queries/0_stateless/01104_distributed_numbers_test.sql\nindex bb2db88f23f0..07237223bad4 100644\n--- a/tests/queries/0_stateless/01104_distributed_numbers_test.sql\n+++ b/tests/queries/0_stateless/01104_distributed_numbers_test.sql\n@@ -1,9 +1,25 @@\n -- Tags: distributed\n \n+SELECT *\n+FROM\n+(\n+    SELECT *\n+    FROM system.numbers\n+    WHERE number = 100\n+    UNION ALL\n+    SELECT *\n+    FROM system.numbers\n+    WHERE number = 100\n+)\n+LIMIT 2\n+SETTINGS max_threads = 1 FORMAT Null;\n+\n DROP TABLE IF EXISTS d_numbers;\n CREATE TABLE d_numbers (number UInt32) ENGINE = Distributed(test_cluster_two_shards, system, numbers, rand());\n \n SELECT '100' AS number FROM d_numbers AS n WHERE n.number = 100 LIMIT 2;\n+SELECT '100' AS number FROM d_numbers AS n WHERE n.number = 100 LIMIT 2 SETTINGS max_threads = 1, prefer_localhost_replica=1;\n+SELECT sum(number) FROM (select * from remote('127.0.0.{1,1,1}', system.numbers) AS n WHERE n.number = 100 LIMIT 3) SETTINGS max_threads = 2, prefer_localhost_replica=1;\n \n SET distributed_product_mode = 'local';\n \n",
  "problem_statement": "Queries stuck when `max_threads=1`\n[Report](https://play.clickhouse.com/play?user=play#U0VMRUNUCiAgdGVzdF9uYW1lLAogIHRvU3RhcnRPZkRheShjaGVja19zdGFydF90aW1lKSBhcyB0LAogIGNvdW50KCkgYXMgcnVucywKICBjb25jYXQocm91bmQoMTAwICogKGNvdW50SWYodGVzdF9zdGF0dXMgIT0gJ09LJyBBTkQgdGVzdF9zdGF0dXMgIT0gJ1NLSVBQRUQnKSBBUyBmKSAvIHJ1bnMsIDEpOjpTdHJpbmcsICclJykgYXMgZmFpbHVyZV9wZXJjZW50YWdlLAogIGFueUlmKHJlcG9ydF91cmwsIHRlc3Rfc3RhdHVzICE9ICdPSycgQU5EIHRlc3Rfc3RhdHVzICE9ICdTS0lQUEVEJykgYXMgbGFzdF9yZXBvcnQsCiAgZ3JvdXBBcnJheURpc3RpbmN0SWYoY2hlY2tfbmFtZSwgdGVzdF9zdGF0dXMgIT0gJ09LJyBBTkQgdGVzdF9zdGF0dXMgIT0gJ1NLSVBQRUQnKSBhcyBjaGVja19uYW1lc193aXRoX2ZhaWx1cmVzCkZST00gY2hlY2tzCldIRVJFCiAgICB0ZXN0X25hbWUgTElLRSAnJTAxMTA0X2Rpc3RyaWJ1dGVkX251bWJlcnNfdGVzdCUnCiAgICBBTkQgcHVsbF9yZXF1ZXN0X251bWJlciA9IDAKICAgIEFORCBjaGVja19zdGFydF90aW1lID4gdG9kYXkoKSAtIGludGVydmFsIDMwIGRheQpHUk9VUCBCWSB0ZXN0X25hbWUsIHQKT1JERVIgYnkgdA==)\r\n\r\n```sql\r\nSELECT\r\n  test_name,\r\n  toStartOfDay(check_start_time) as t,\r\n  count() as runs,\r\n  concat(round(100 * (countIf(test_status != 'OK' AND test_status != 'SKIPPED') AS f) / runs, 1)::String, '%') as failure_percentage,\r\n  anyIf(report_url, test_status != 'OK' AND test_status != 'SKIPPED') as last_report,\r\n  groupArrayDistinctIf(check_name, test_status != 'OK' AND test_status != 'SKIPPED') as check_names_with_failures\r\nFROM checks\r\nWHERE\r\n    test_name LIKE '%01104_distributed_numbers_test%'\r\n    AND pull_request_number = 0\r\n    AND check_start_time > today() - interval 30 day\r\nGROUP BY test_name, t\r\nORDER by t\r\n```\r\n\r\nIt doesn't seem to fail often.\r\n\r\nChecking one of the [reports](https://s3.amazonaws.com/clickhouse-test-reports/0/9c79c165b939143312a960081b371598f37232e2/stateless_tests__tsan__[2_5].html):\r\n \r\n```\r\n2023-11-01 06:05:42 Found hung queries in processlist:\r\n2023-11-01 06:05:42 Row 1:\r\n2023-11-01 06:05:42 \u2500\u2500\u2500\u2500\u2500\u2500\r\n2023-11-01 06:05:42 is_initial_query:     1\r\n2023-11-01 06:05:42 user:                 default\r\n2023-11-01 06:05:42 query_id:             4d774025-05ea-42a5-9b36-7f53a7ae7921\r\n2023-11-01 06:05:42 address:              ::1\r\n2023-11-01 06:05:42 port:                 50978\r\n2023-11-01 06:05:42 initial_user:         default\r\n2023-11-01 06:05:42 initial_query_id:     4d774025-05ea-42a5-9b36-7f53a7ae7921\r\n2023-11-01 06:05:42 initial_address:      ::1\r\n2023-11-01 06:05:42 initial_port:         50978\r\n2023-11-01 06:05:42 interface:            1\r\n2023-11-01 06:05:42 os_user:              \r\n2023-11-01 06:05:42 client_hostname:      b9e60a8e9b7c\r\n2023-11-01 06:05:42 client_name:          ClickHouse client\r\n2023-11-01 06:05:42 client_revision:      54466\r\n2023-11-01 06:05:42 client_version_major: 23\r\n2023-11-01 06:05:42 client_version_minor: 10\r\n2023-11-01 06:05:42 client_version_patch: 1\r\n2023-11-01 06:05:42 http_method:          0\r\n2023-11-01 06:05:42 http_user_agent:      \r\n2023-11-01 06:05:42 http_referer:         \r\n2023-11-01 06:05:42 forwarded_for:        \r\n2023-11-01 06:05:42 quota_key:            \r\n2023-11-01 06:05:42 distributed_depth:    0\r\n2023-11-01 06:05:42 elapsed:              1739.511744\r\n2023-11-01 06:05:42 is_cancelled:         0\r\n2023-11-01 06:05:42 is_all_data_sent:     0\r\n2023-11-01 06:05:42 read_rows:            57499517845\r\n2023-11-01 06:05:42 read_bytes:           459996142760\r\n2023-11-01 06:05:42 total_rows_approx:    0\r\n2023-11-01 06:05:42 written_rows:         0\r\n2023-11-01 06:05:42 written_bytes:        0\r\n2023-11-01 06:05:42 memory_usage:         0\r\n2023-11-01 06:05:42 peak_memory_usage:    0\r\n2023-11-01 06:05:42 query:                SELECT '100' AS number FROM d_numbers AS n WHERE n.number = 100 LIMIT 2;\r\n2023-11-01 06:05:42 query_kind:           Select\r\n2023-11-01 06:05:42 thread_ids:           [12594,8108]\r\n2023-11-01 06:05:42 ProfileEvents:        {'Query':1,'SelectQuery':1,'QueriesWithSubqueries':2,'SelectQueriesWithSubqueries':2,'FunctionExecute':1411139,'NetworkReceiveElapsedMicroseconds':104261,'NetworkSendElapsedMicroseconds':1224525,'NetworkSendBytes':17206731,'SelectedRows':57499517845,'SelectedBytes':459996142760,'ContextLock':45,'ContextLockWaitMicroseconds':163,'RWLockAcquiredReadLocks':3,'RealTimeMicroseconds':1739403910,'UserTimeMicroseconds':670498851,'SystemTimeMicroseconds':1000496930,'SoftPageFaults':619545535,'OSCPUWaitMicroseconds':18279244,'OSCPUVirtualTimeMicroseconds':1670924968,'OSReadChars':71256772,'OSWriteChars':232,'LogTrace':5,'LogDebug':1}\r\n2023-11-01 06:05:42 Settings:             {'min_compress_block_size':'206180','max_compress_block_size':'2708203','max_block_size':'40747','max_insert_threads':'4','max_threads':'1','max_read_buffer_size':'758270','connect_timeout_with_failover_ms':'2000','connect_timeout_with_failover_secure_ms':'3000','idle_connection_timeout':'36000','s3_check_objects_after_upload':'1','use_uncompressed_cache':'1','stream_like_engine_allow_direct_select':'1','replication_wait_for_inactive_replica_timeout':'30','min_count_to_compile_aggregate_expression':'0','min_count_to_compile_sort_description':'0','group_by_two_level_threshold':'499798','group_by_two_level_threshold_bytes':'24727943','distributed_aggregation_memory_efficient':'0','enable_memory_bound_merging_of_aggregation_results':'0','allow_nonconst_timezone_arguments':'1','min_chunk_bytes_for_parallel_parsing':'7788527','output_format_parallel_formatting':'0','merge_tree_coarse_index_granularity':'22','min_bytes_to_use_direct_io':'10737418240','min_bytes_to_use_mmap_io':'8853165898','log_queries':'1','insert_quorum_timeout':'60000','http_response_buffer_size':'766804','fsync_metadata':'1','http_send_timeout':'60','http_receive_timeout':'60','opentelemetry_start_trace_probability':'0.1','max_untracked_memory':'1048576','memory_profiler_step':'1048576','log_comment':'01104_distributed_numbers_test.sql','send_logs_level':'warning','optimize_read_in_order':'0','aggregation_in_order_max_block_bytes':'18908436','read_in_order_two_level_merge_threshold':'84','database_atomic_wait_for_drop_and_detach_synchronously':'1','local_filesystem_read_method':'read','remote_filesystem_read_method':'read','local_filesystem_read_prefetch':'1','remote_filesystem_read_prefetch':'0','async_insert_busy_timeout_ms':'5000','enable_filesystem_cache':'1','enable_filesystem_cache_on_write_operations':'1','load_marks_asynchronously':'1','allow_prefetched_read_pool_for_remote_filesystem':'1','allow_prefetched_read_pool_for_local_filesystem':'0','filesystem_prefetch_step_bytes':'104857600','filesystem_prefetch_max_memory_usage':'134217728','filesystem_prefetches_limit':'0','insert_keeper_max_retries':'20','insert_keeper_fault_injection_probability':'0.01','session_timezone':'Asia/Ulaanbaatar'}\r\n2023-11-01 06:05:42 current_database:     test_g067ku0s\r\n2023-11-01 06:05:42 stacktraces:          Thread ID 8108\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan::call_pthread_cancel_with_cleanup(int (*)(void*), void (*)(void*), void*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__interceptor_pthread_cond_timedwait\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__threading_support:341::std::__1::condition_variable::__do_timed_wait(std::__1::unique_lock<std::__1::mutex>&, std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l>>>)\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__mutex_base:460::bool std::__1::condition_variable::wait_until<std::__1::chrono::steady_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l>>, bool ConcurrentBoundedQueue<DB::Chunk>::popImpl<true>(DB::Chunk&, std::__1::optional<unsigned long>)::'lambda'()>(std::__1::unique_lock<std::__1::mutex>&, std::__1::chrono::time_point<std::__1::chrono::steady_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l>>> const&, bool ConcurrentBoundedQueue<DB::Chunk>::popImpl<true>(DB::Chunk&, std::__1::optional<unsigned long>)::'lambda'())\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__mutex_base:0::bool ConcurrentBoundedQueue<DB::Chunk>::popImpl<true>(DB::Chunk&, std::__1::optional<unsigned long>)\r\n2023-11-01 06:05:42 ./build_docker/./src/Common/ConcurrentBoundedQueue.h:0::DB::LazyOutputFormat::getChunk(unsigned long)\r\n2023-11-01 06:05:42 ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:0::DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)\r\n2023-11-01 06:05:42 ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:153::DB::PullingAsyncPipelineExecutor::pull(DB::Block&, unsigned long)\r\n2023-11-01 06:05:42 ./build_docker/./src/Server/TCPHandler.cpp:966::DB::TCPHandler::processOrdinaryQueryWithProcessors()\r\n2023-11-01 06:05:42 ./build_docker/./src/Server/TCPHandler.cpp:0::DB::TCPHandler::runImpl()\r\n2023-11-01 06:05:42 ./build_docker/./src/Server/TCPHandler.cpp:2257::DB::TCPHandler::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Net/src/TCPServerConnection.cpp:57::Poco::Net::TCPServerConnection::start()\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:48::Poco::Net::TCPServerDispatcher::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Foundation/src/ThreadPool.cpp:202::Poco::PooledThread::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Foundation/src/Thread.cpp:46::Poco::(anonymous namespace)::RunnableHolder::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Foundation/include/Poco/SharedPtr.h:231::Poco::ThreadImpl::runnableEntry(void*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan_thread_start_func\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 ::Thread ID 12594\r\n2023-11-01 06:05:42 ./build_docker/./src/Common/StackTrace.cpp:286::StackTrace::StackTrace(ucontext_t const&)\r\n2023-11-01 06:05:42 ./build_docker/./src/Storages/System/StorageSystemStackTrace.cpp:96::DB::(anonymous namespace)::signalHandler(int, siginfo_t*, void*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan::CallUserSignalHandler(__tsan::ThreadState*, bool, bool, int, __sanitizer::__sanitizer_siginfo*, void*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan::ProcessPendingSignalsImpl(__tsan::ThreadState*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan_atomic64_fetch_add\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:173::DB::IExecutableFunction::executeWithoutSparseColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const\r\n2023-11-01 06:05:42 ./build_docker/./src/Functions/IFunction.cpp:0::DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const\r\n2023-11-01 06:05:42 ./build_docker/./contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:117::DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const\r\n2023-11-01 06:05:42 ./build_docker/./src/Processors/Transforms/FilterTransform.cpp:0::DB::FilterTransform::doTransform(DB::Chunk&)\r\n2023-11-01 06:05:42 ./build_docker/./src/Processors/Transforms/FilterTransform.cpp:121::DB::FilterTransform::transform(DB::Chunk&)\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__utility/swap.h:35::DB::ISimpleTransform::transform(DB::Chunk&, DB::Chunk&)\r\n2023-11-01 06:05:42 ./build_docker/./src/Processors/ISimpleTransform.cpp:99::DB::ISimpleTransform::work()\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/list:588::DB::ExecutionThreadContext::executeTask()\r\n2023-11-01 06:05:42 ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:272::DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*)\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:701::DB::PipelineExecutor::executeImpl(unsigned long, bool)\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:274::DB::PipelineExecutor::execute(unsigned long, bool)\r\n2023-11-01 06:05:42 ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:96::void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>>(std::__1::__function::__policy_storage const*)\r\n2023-11-01 06:05:42 ./build_docker/./base/base/../base/wide_integer_impl.h:809::ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302::void* std::__1::__thread_proxy[abi:v15000]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, Priority, std::__1::optional<unsigned long>, bool)::'lambda0'()>>(void*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan_thread_start_func\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 \r\n2023-11-01 06:05:42 Row 2:\r\n2023-11-01 06:05:42 \u2500\u2500\u2500\u2500\u2500\u2500\r\n2023-11-01 06:05:42 is_initial_query:     1\r\n2023-11-01 06:05:42 user:                 default\r\n2023-11-01 06:05:42 query_id:             3b821e81-3d97-4d0a-94ae-4b080a3557bb\r\n2023-11-01 06:05:42 address:              ::1\r\n2023-11-01 06:05:42 port:                 48978\r\n2023-11-01 06:05:42 initial_user:         default\r\n2023-11-01 06:05:42 initial_query_id:     3b821e81-3d97-4d0a-94ae-4b080a3557bb\r\n2023-11-01 06:05:42 initial_address:      ::1\r\n2023-11-01 06:05:42 initial_port:         48978\r\n2023-11-01 06:05:42 interface:            2\r\n2023-11-01 06:05:42 os_user:              \r\n2023-11-01 06:05:42 client_hostname:      \r\n2023-11-01 06:05:42 client_name:          \r\n2023-11-01 06:05:42 client_revision:      0\r\n2023-11-01 06:05:42 client_version_major: 0\r\n2023-11-01 06:05:42 client_version_minor: 0\r\n2023-11-01 06:05:42 client_version_patch: 0\r\n2023-11-01 06:05:42 http_method:          2\r\n2023-11-01 06:05:42 http_user_agent:      \r\n2023-11-01 06:05:42 http_referer:         \r\n2023-11-01 06:05:42 forwarded_for:        \r\n2023-11-01 06:05:42 quota_key:            \r\n2023-11-01 06:05:42 distributed_depth:    0\r\n2023-11-01 06:05:42 elapsed:              1139.953739\r\n2023-11-01 06:05:42 is_cancelled:         0\r\n2023-11-01 06:05:42 is_all_data_sent:     0\r\n2023-11-01 06:05:42 read_rows:            0\r\n2023-11-01 06:05:42 read_bytes:           0\r\n2023-11-01 06:05:42 total_rows_approx:    0\r\n2023-11-01 06:05:42 written_rows:         0\r\n2023-11-01 06:05:42 written_bytes:        0\r\n2023-11-01 06:05:42 memory_usage:         1048671\r\n2023-11-01 06:05:42 peak_memory_usage:    1048671\r\n2023-11-01 06:05:42 query:                DROP DATABASE IF EXISTS test_g067ku0s\r\n2023-11-01 06:05:42 \r\n2023-11-01 06:05:42 query_kind:           Drop\r\n2023-11-01 06:05:42 thread_ids:           [25281]\r\n2023-11-01 06:05:42 ProfileEvents:        {'Query':1,'QueriesWithSubqueries':1,'ContextLock':10,'ContextLockWaitMicroseconds':6,'RWLockAcquiredReadLocks':2,'LogTrace':2,'LogDebug':4}\r\n2023-11-01 06:05:42 Settings:             {'connect_timeout':'20','connect_timeout_with_failover_ms':'2000','connect_timeout_with_failover_secure_ms':'3000','receive_timeout':'20','send_timeout':'20','idle_connection_timeout':'36000','s3_check_objects_after_upload':'1','stream_like_engine_allow_direct_select':'1','replication_wait_for_inactive_replica_timeout':'30','allow_nonconst_timezone_arguments':'1','output_format_parallel_formatting':'0','log_queries':'1','insert_quorum_timeout':'60000','fsync_metadata':'0','http_connection_timeout':'20','http_send_timeout':'20','http_receive_timeout':'20','opentelemetry_start_trace_probability':'0.1','max_untracked_memory':'1048576','memory_profiler_step':'1048576','log_comment':'01104_distributed_numbers_test.sql','database_atomic_wait_for_drop_and_detach_synchronously':'1','async_insert_busy_timeout_ms':'5000','enable_filesystem_cache':'1','enable_filesystem_cache_on_write_operations':'1','load_marks_asynchronously':'1','allow_prefetched_read_pool_for_remote_filesystem':'0','allow_prefetched_read_pool_for_local_filesystem':'0','filesystem_prefetch_max_memory_usage':'1073741824','insert_keeper_max_retries':'20','insert_keeper_fault_injection_probability':'0.01'}\r\n2023-11-01 06:05:42 current_database:     system\r\n2023-11-01 06:05:42 stacktraces:          Thread ID 25281\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan::call_pthread_cancel_with_cleanup(int (*)(void*), void (*)(void*), void*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::___interceptor_pthread_cond_wait\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/src/condition_variable.cpp:47::std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&)\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__mutex_base:0::DB::DatabaseCatalog::waitTableFinallyDropped(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&)\r\n2023-11-01 06:05:42 ./build_docker/./src/Interpreters/InterpreterDropQuery.cpp:91::DB::InterpreterDropQuery::executeToDatabase(DB::ASTDropQuery const&)\r\n2023-11-01 06:05:42 ./build_docker/./src/Interpreters/InterpreterDropQuery.cpp:79::DB::InterpreterDropQuery::execute()\r\n2023-11-01 06:05:42 ./build_docker/./src/Interpreters/executeQuery.cpp:0::DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*)\r\n2023-11-01 06:05:42 ./build_docker/./src/Interpreters/executeQuery.cpp:0::DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::__1::shared_ptr<DB::Context>, std::__1::function<void (DB::QueryResultDetails const&)>, std::__1::optional<DB::FormatSettings> const&, std::__1::function<void (DB::IOutputFormat&)>)\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:818::DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&)\r\n2023-11-01 06:05:42 ./build_docker/./src/Server/HTTPHandler.cpp:1078::DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&)\r\n2023-11-01 06:05:42 ./build_docker/./src/Server/HTTP/HTTPServerConnection.cpp:0::DB::HTTPServerConnection::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Net/src/TCPServerConnection.cpp:57::Poco::Net::TCPServerConnection::start()\r\n2023-11-01 06:05:42 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:48::Poco::Net::TCPServerDispatcher::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Foundation/src/ThreadPool.cpp:202::Poco::PooledThread::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Foundation/src/Thread.cpp:46::Poco::(anonymous namespace)::RunnableHolder::run()\r\n2023-11-01 06:05:42 ./build_docker/./base/poco/Foundation/include/Poco/SharedPtr.h:231::Poco::ThreadImpl::runnableEntry(void*)\r\n2023-11-01 06:05:42 /usr/bin/clickhouse::__tsan_thread_start_func\r\n2023-11-01 06:05:42 ::\r\n2023-11-01 06:05:42 ::1381\r\n2023-11-01 06:05:42 Could not attach to process.  If your uid matches the uid of the target\r\n2023-11-01 06:05:42 process, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\n2023-11-01 06:05:42 again as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\r\n2023-11-01 06:05:42 warning: process 727 is already traced by process 1381\r\n2023-11-01 06:05:42 ptrace: Inappropriate ioctl for device.\r\nQuit\r\n```\r\n\r\n\r\nIn a subsequent print of all the threads we see it's still generating numbers.\r\n```\r\n2023-11-01 06:05:45 Row 506:\r\n2023-11-01 06:05:45 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n2023-11-01 06:05:45 thread_name: QueryPullPipeEx\r\n2023-11-01 06:05:45 thread_id:   12594\r\n2023-11-01 06:05:45 query_id:    4d774025-05ea-42a5-9b36-7f53a7ae7921\r\n2023-11-01 06:05:45 trace:       [94600817468605,94600952125614,94600676062736,94600676062139,94600676377543,94601011426647,94600950724843,94601011428949,94601011427561,94601011570581,94601011517329,94601011513689,94601011512927,94601011590041,94600818500135,94600818525330,94600676029711,139729962605251,139729963199476]\r\n2023-11-01 06:05:45 trace_str:   ./build_docker/./src/Common/StackTrace.cpp:286: StackTrace::StackTrace(ucontext_t const&)\r\n2023-11-01 06:05:45 ./build_docker/./src/Storages/System/StorageSystemStackTrace.cpp:96: DB::(anonymous namespace)::signalHandler(int, siginfo_t*, void*)\r\n2023-11-01 06:05:45 /usr/bin/clickhouse: __tsan::CallUserSignalHandler(__tsan::ThreadState*, bool, bool, int, __sanitizer::__sanitizer_siginfo*, void*)\r\n2023-11-01 06:05:45 /usr/bin/clickhouse: __tsan::ProcessPendingSignalsImpl(__tsan::ThreadState*)\r\n2023-11-01 06:05:45 /usr/bin/clickhouse: __tsan_atomic8_store\r\n2023-11-01 06:05:45 ./build_docker/./src/Processors/ISource.cpp:69: DB::ISource::progress(unsigned long, unsigned long)\r\n2023-11-01 06:05:45 ./build_docker/./src/Storages/System/StorageSystemNumbers.cpp:0: DB::(anonymous namespace)::NumbersSource::generate()\r\n2023-11-01 06:05:45 ./build_docker/./src/Processors/Chunk.h:90: DB::ISource::tryGenerate()\r\n2023-11-01 06:05:45 ./build_docker/./contrib/llvm-project/libcxx/include/optional:344: DB::ISource::work()\r\n2023-11-01 06:05:45 ./build_docker/./contrib/llvm-project/libcxx/include/list:588: DB::ExecutionThreadContext::executeTask()\r\n2023-11-01 06:05:45 ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:272: DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*)\r\n2023-11-01 06:05:45 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:701: DB::PipelineExecutor::executeImpl(unsigned long, bool)\r\n2023-11-01 06:05:45 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:274: DB::PipelineExecutor::execute(unsigned long, bool)\r\n2023-11-01 06:05:45 ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:96: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>>(std::__1::__function::__policy_storage const*)\r\n2023-11-01 06:05:45 ./build_docker/./base/base/../base/wide_integer_impl.h:809: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n2023-11-01 06:05:45 ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: void* std::__1::__thread_proxy[abi:v15000]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, Priority, std::__1::optional<unsigned long>, bool)::'lambda0'()>>(void*)\r\n2023-11-01 06:05:45 /usr/bin/clickhouse: __tsan_thread_start_func\r\n2023-11-01 06:05:45 : \r\n2023-11-01 06:05:45 : \r\n2023-11-01 06:05:45 \r\n```\r\n\r\nQuery logs:\r\n```\r\n2023.11.01 05:36:40.978584 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Debug> executeQuery: (from [::1]:50978) (comment: 01104_distributed_numbers_test.sql) SELECT '100' AS number FROM d_numbers AS n WHERE n.number = 100 LIMIT 2; (stage: Complete)\r\n2023.11.01 05:36:40.986117 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Trace> ContextAccess (default): Access granted: SELECT(number) ON test_g067ku0s.d_numbers\r\n2023.11.01 05:36:40.992734 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Trace> ContextAccess (default): Access granted: SELECT(number) ON test_g067ku0s.d_numbers\r\n2023.11.01 05:36:40.996756 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Trace> ContextAccess (default): Access granted: SELECT(number) ON system.numbers\r\n2023.11.01 05:36:40.997086 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableStateAfterAggregation\r\n2023.11.01 05:36:40.997972 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Trace> InterpreterSelectQuery: WithMergeableStateAfterAggregationAndLimit -> Complete\r\n2023.11.01 06:06:10.583931 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Error> executeQuery: Code: 394. 2023.11.01 06:06:10.583931 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Error> executeQuery: Code: 394. DB::Exception: Query was cancelled. (QUERY_WAS_CANCELLED) (version 23.10.1.1 (official build)) (from [::1]:50978) (comment: 01104_distributed_numbers_test.sql) (in query: SELECT '100' AS number FROM d_numbers AS n WHERE n.number = 100 LIMIT 2;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ./build_docker/./contrib/llvm-project/libcxx/include/exception:134: Poco::Exception::Exception(String const&, int) @ 0x000000002043cf03 in /usr/bin/clickhouse\r\n1. ./build_docker/./src/Common/Exception.cpp:103: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x00000000104d9834 in /usr/bin/clickhouse\r\n2. DB::Exception::Exception<char const (&) [20]>(int, char const (&) [20]) @ 0x00000000089824e6 in /usr/bin/clickhouse\r\n3. ./build_docker/./src/Interpreters/ProcessList.cpp:463: DB::QueryStatus::checkTimeLimit() @ 0x000000001a480936 in /usr/bin/clickhouse\r\n4. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:197: DB::PipelineExecutor::finalizeExecution() @ 0x000000001be33a88 in /usr/bin/clickhouse\r\n5. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:0: DB::PipelineExecutor::execute(unsigned long, bool) @ 0x000000001be336c8 in /usr/bin/clickhouse\r\n6. ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:96: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000001be46399 in /usr/bin/clickhouse\r\n7. ./build_docker/./base/base/../base/wide_integer_impl.h:809: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x0000000010621227 in /usr/bin/clickhouse\r\n8. ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x0000000010627492 in /usr/bin/clickhouse\r\n9. __tsan_thread_start_func @ 0x0000000007e4250f in /usr/bin/clickhouse\r\n10. ? @ 0x00007f156ac91ac3 in ?\r\n11. ? @ 0x00007f156ad22bf4 in ?\r\n\r\n2023.11.01 06:06:10.585447 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Error> TCPHandler: Code: 394. DB::Exception: Query was cancelled. (QUERY_WAS_CANCELLED), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ./build_docker/./contrib/llvm-project/libcxx/include/exception:134: Poco::Exception::Exception(String const&, int) @ 0x000000002043cf03 in /usr/bin/clickhouse\r\n1. ./build_docker/./src/Common/Exception.cpp:103: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x00000000104d9834 in /usr/bin/clickhouse\r\n2. DB::Exception::Exception<char const (&) [20]>(int, char const (&) [20]) @ 0x00000000089824e6 in /usr/bin/clickhouse\r\n3. ./build_docker/./src/Interpreters/ProcessList.cpp:463: DB::QueryStatus::checkTimeLimit() @ 0x000000001a480936 in /usr/bin/clickhouse\r\n4. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:197: DB::PipelineExecutor::finalizeExecution() @ 0x000000001be33a88 in /usr/bin/clickhouse\r\n5. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:0: DB::PipelineExecutor::execute(unsigned long, bool) @ 0x000000001be336c8 in /usr/bin/clickhouse\r\n6. ./build_docker/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:96: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000001be46399 in /usr/bin/clickhouse\r\n7. ./build_docker/./base/base/../base/wide_integer_impl.h:809: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x0000000010621227 in /usr/bin/clickhouse\r\n8. ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x0000000010627492 in /usr/bin/clickhouse\r\n9. __tsan_thread_start_func @ 0x0000000007e4250f in /usr/bin/clickhouse\r\n10. ? @ 0x00007f156ac91ac3 in ?\r\n11. ? @ 0x00007f156ad22bf4 in ?\r\n\r\n2023.11.01 06:06:10.585677 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Debug> TCPHandler: Processed in 1769.582480399 sec.\r\n2023.11.01 06:06:10.586095 [ 8108 ] {4d774025-05ea-42a5-9b36-7f53a7ae7921} <Debug> MemoryTracker: Peak memory usage (for query): 318.46 KiB.\r\n2023.11.01 06:06:10.586277 [ 8108 ] {} <Debug> TCPHandler: Done processing connection.\r\n2023.11.01 06:06:10.586898 [ 8108 ] {} <Debug> TCP-Session: ea53bc87-a5b7-4280-a086-7b736cf53cd7 Logout, user_id: 94309d50-4f52-5250-31bd-74fecac179db\r\n```\r\n\r\nWe are waiting to have at least 2 rows so 1 need to come from each shard. Not sure what I'm missing\r\n\n",
  "hints_text": "How to reproduce:\r\n\r\n```\r\nCREATE TABLE d_numbers (number UInt32) ENGINE = Distributed(test_cluster_two_shards, system, numbers, rand());\r\nSELECT '100' AS number FROM d_numbers AS n WHERE n.number = 100 LIMIT 2 SETTINGS max_threads = 1, prefer_localhost_replica=1;\r\n```\r\n\r\nWorks with any higher value for `max_threads`\n@Avogar reverting this PR locally helped https://github.com/ClickHouse/ClickHouse/pull/53504\r\n\r\nthe problem is when max_threads=1 and localhost replica is used\r\nwe end up with 2 plans but only 1 thread so we infinitely read from one of the storages (I think local)\r\nmaybe we can handle differently this case?\r\n\nThe merged PR is not really the root cause of the problem, if we also set `max_distributed_connections=1`, we will get the same behaviour on previous versions\nNot sure what is the best way to fix this tbh. Maybe @KochetovNicolai will have ideas\n> The merged PR is not really the root cause of the problem, if we also set max_distributed_connections=1, we will get the same behaviour on previous versions\r\n\r\nThat's a good observation\r\n\r\n@KochetovNicolai is already looped in, but I'm also not sure what would be the best way to fix.\nAlso we have this in setting description:\r\n```\r\n    M(UInt64, max_distributed_connections, 1024, \"The maximum number of connections for distributed processing of one query (should be greater than max_threads).\", 0) \\\r\n```\r\n\r\n> should be greater than max_threads\r\n\r\nBut we don't check it in any way\nIt doesn't matter in case we have `max_threads=1`, setting `max_distributed_connections` to any value will not help.",
  "created_at": "2023-11-14T17:42:52Z"
}