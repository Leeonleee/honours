{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 23824,
  "instance_id": "ClickHouse__ClickHouse-23824",
  "issue_numbers": [
    "23800"
  ],
  "base_commit": "9f369a0a27dbad7a083a82dfe6c016defd0e5955",
  "patch": "diff --git a/src/Dictionaries/DictionaryHelpers.h b/src/Dictionaries/DictionaryHelpers.h\nindex e4406c9a22b4..c2071d8136ae 100644\n--- a/src/Dictionaries/DictionaryHelpers.h\n+++ b/src/Dictionaries/DictionaryHelpers.h\n@@ -437,24 +437,36 @@ class DictionaryKeysExtractor\n   */\n template <DictionaryKeyType dictionary_key_type>\n void mergeBlockWithStream(\n-    size_t key_column_size [[maybe_unused]],\n-    Block & block_to_update [[maybe_unused]],\n-    BlockInputStreamPtr & stream [[maybe_unused]])\n+    size_t key_columns_size,\n+    Block & block_to_update,\n+    BlockInputStreamPtr & stream)\n {\n     using KeyType = std::conditional_t<dictionary_key_type == DictionaryKeyType::simple, UInt64, StringRef>;\n     static_assert(dictionary_key_type != DictionaryKeyType::range, \"Range key type is not supported by updatePreviousyLoadedBlockWithStream\");\n \n     Columns saved_block_key_columns;\n-    saved_block_key_columns.reserve(key_column_size);\n+    saved_block_key_columns.reserve(key_columns_size);\n \n     /// Split into keys columns and attribute columns\n-    for (size_t i = 0; i < key_column_size; ++i)\n+    for (size_t i = 0; i < key_columns_size; ++i)\n         saved_block_key_columns.emplace_back(block_to_update.safeGetByPosition(i).column);\n \n     DictionaryKeysArenaHolder<dictionary_key_type> arena_holder;\n     DictionaryKeysExtractor<dictionary_key_type> saved_keys_extractor(saved_block_key_columns, arena_holder.getComplexKeyArena());\n     auto saved_keys_extracted_from_block = saved_keys_extractor.extractAllKeys();\n \n+    /**\n+     * We create filter with our block to update size, because we want to filter out values that have duplicate keys\n+     * if they appear in blocks that we fetch from stream.\n+     * But first we try to filter out duplicate keys from existing block.\n+     * For example if we have block with keys 1, 2, 2, 2, 3, 3\n+     * Our filter will have [1, 0, 0, 1, 0, 1] after first stage.\n+     * We also update saved_key_to_index hash map for keys to point to their latest indexes.\n+     * For example if in blocks from stream we will get keys [4, 2, 3]\n+     * Our filter will be [1, 0, 0, 0, 0, 0].\n+     * After reading all blocks from stream we filter our duplicate keys from block_to_update and insert loaded columns.\n+     */\n+\n     IColumn::Filter filter(saved_keys_extracted_from_block.size(), true);\n \n     HashMap<KeyType, size_t> saved_key_to_index;\n@@ -483,10 +495,10 @@ void mergeBlockWithStream(\n     while (Block block = stream->read())\n     {\n         Columns block_key_columns;\n-        block_key_columns.reserve(key_column_size);\n+        block_key_columns.reserve(key_columns_size);\n \n         /// Split into keys columns and attribute columns\n-        for (size_t i = 0; i < key_column_size; ++i)\n+        for (size_t i = 0; i < key_columns_size; ++i)\n             block_key_columns.emplace_back(block.safeGetByPosition(i).column);\n \n         DictionaryKeysExtractor<dictionary_key_type> update_keys_extractor(block_key_columns, arena_holder.getComplexKeyArena());\ndiff --git a/src/Dictionaries/HashedDictionary.cpp b/src/Dictionaries/HashedDictionary.cpp\nindex 71d703480e8e..64699b1711db 100644\n--- a/src/Dictionaries/HashedDictionary.cpp\n+++ b/src/Dictionaries/HashedDictionary.cpp\n@@ -354,10 +354,10 @@ void HashedDictionary<dictionary_key_type, sparse>::updateData()\n             if (!update_field_loaded_block)\n                 update_field_loaded_block = std::make_shared<DB::Block>(block.cloneEmpty());\n \n-            for (const auto attribute_idx : ext::range(0, attributes.size() + 1))\n+            for (size_t attribute_index = 0; attribute_index < block.columns(); ++attribute_index)\n             {\n-                const IColumn & update_column = *block.getByPosition(attribute_idx).column.get();\n-                MutableColumnPtr saved_column = update_field_loaded_block->getByPosition(attribute_idx).column->assumeMutable();\n+                const IColumn & update_column = *block.getByPosition(attribute_index).column.get();\n+                MutableColumnPtr saved_column = update_field_loaded_block->getByPosition(attribute_index).column->assumeMutable();\n                 saved_column->insertRangeFrom(update_column, 0, update_column.size());\n             }\n         }\n",
  "test_patch": "diff --git a/tests/integration/test_dictionaries_update_field/__init__.py b/tests/integration/test_dictionaries_update_field/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_dictionaries_update_field/configs/config.xml b/tests/integration/test_dictionaries_update_field/configs/config.xml\nnew file mode 100644\nindex 000000000000..a1518083be35\n--- /dev/null\n+++ b/tests/integration/test_dictionaries_update_field/configs/config.xml\n@@ -0,0 +1,30 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+<logger>\n+\t<level>trace</level>\n+\t<log>/var/log/clickhouse-server/clickhouse-server.log</log>\n+\t<errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n+\t<size>1000M</size>\n+\t<count>10</count>\n+\t</logger>\n+\n+    <tcp_port>9000</tcp_port>\n+    <listen_host>127.0.0.1</listen_host>\n+\n+    <openSSL>\n+        <client>\n+            <cacheSessions>true</cacheSessions>\n+            <verificationMode>none</verificationMode>\n+            <invalidCertificateHandler>\n+                <name>AcceptCertificateHandler</name>\n+            </invalidCertificateHandler>\n+        </client>\n+    </openSSL>\n+\n+    <max_concurrent_queries>500</max_concurrent_queries>\n+    <mark_cache_size>5368709120</mark_cache_size>\n+    <path>./clickhouse/</path>\n+    <users_config>users.xml</users_config>\n+\n+    <dictionaries_config>/etc/clickhouse-server/config.d/*.xml</dictionaries_config>\n+</yandex>\ndiff --git a/tests/integration/test_dictionaries_update_field/configs/users.xml b/tests/integration/test_dictionaries_update_field/configs/users.xml\nnew file mode 100644\nindex 000000000000..6061af8e33d7\n--- /dev/null\n+++ b/tests/integration/test_dictionaries_update_field/configs/users.xml\n@@ -0,0 +1,23 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <profiles>\n+        <default>\n+        </default>\n+    </profiles>\n+\n+    <users>\n+        <default>\n+            <password></password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </default>\n+    </users>\n+\n+    <quotas>\n+        <default>\n+        </default>\n+    </quotas>\n+</yandex>\ndiff --git a/tests/integration/test_dictionaries_update_field/test.py b/tests/integration/test_dictionaries_update_field/test.py\nnew file mode 100644\nindex 000000000000..c52c836b4f7b\n--- /dev/null\n+++ b/tests/integration/test_dictionaries_update_field/test.py\n@@ -0,0 +1,77 @@\n+## sudo -H pip install PyMySQL\n+import time\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+from helpers.cluster import ClickHouseKiller\n+from helpers.network import PartitionManager\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node = cluster.add_instance('main_node', main_configs=[])\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        node.query(\n+            \"\"\"\n+            CREATE TABLE table_for_update_field_dictionary\n+            (\n+                key UInt64,\n+                value String,\n+                last_insert_time DateTime\n+            )\n+            ENGINE = TinyLog;\n+            \"\"\"\n+        )\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+@pytest.mark.parametrize(\"dictionary_name,dictionary_type\", [\n+    (\"flat_update_field_dictionary\", \"FLAT\"),\n+    (\"simple_key_hashed_update_field_dictionary\", \"HASHED\"),\n+    (\"complex_key_hashed_update_field_dictionary\", \"HASHED\")\n+])\n+def test_update_field(started_cluster, dictionary_name, dictionary_type):\n+    create_dictionary_query = \"\"\"\n+        CREATE DICTIONARY {dictionary_name}\n+        (\n+            key UInt64,\n+            value String,\n+            last_insert_time DateTime\n+        )\n+        PRIMARY KEY key\n+        SOURCE(CLICKHOUSE(table 'table_for_update_field_dictionary' update_field 'last_insert_time'))\n+        LAYOUT({dictionary_type}())\n+        LIFETIME(1);\n+        \"\"\".format(dictionary_name=dictionary_name, dictionary_type=dictionary_type)\n+\n+    node.query(create_dictionary_query)\n+\n+    node.query(\"INSERT INTO table_for_update_field_dictionary VALUES (1, 'First', now());\")\n+    query_result = node.query(\"SELECT key, value FROM {dictionary_name} ORDER BY key ASC\".format(dictionary_name=dictionary_name))\n+    assert query_result == '1\\tFirst\\n'\n+\n+    node.query(\"INSERT INTO table_for_update_field_dictionary VALUES (2, 'Second', now());\")\n+    time.sleep(10)\n+\n+    query_result = node.query(\"SELECT key, value FROM {dictionary_name} ORDER BY key ASC\".format(dictionary_name=dictionary_name))\n+\n+    assert query_result == '1\\tFirst\\n2\\tSecond\\n'\n+\n+    node.query(\"INSERT INTO table_for_update_field_dictionary VALUES (2, 'SecondUpdated', now());\")\n+    node.query(\"INSERT INTO table_for_update_field_dictionary VALUES (3, 'Third', now());\")\n+\n+    time.sleep(10)\n+\n+    query_result = node.query(\"SELECT key, value FROM {dictionary_name} ORDER BY key ASC\".format(dictionary_name=dictionary_name))\n+\n+    assert query_result == '1\\tFirst\\n2\\tSecondUpdated\\n3\\tThird\\n'\n+\n+    node.query(\"TRUNCATE TABLE table_for_update_field_dictionary\")\n+    node.query(\"DROP DICTIONARY {dictionary_name}\".format(dictionary_name=dictionary_name))\n",
  "problem_statement": "21.4.4 complex_key_hashed dictionaries with update_field cannot load\nI upgraded from 21.4.3 to 21.4.4 and my `complex_key_hashed` dictionaries with `update_field` cannot load anymore\r\n```\r\n2021.04.30 13:13:01.913021 [ 7023 ] {} <Error> ExternalDictionariesLoader: Could not load external dictionary 'complex_key_hashed_dict', next update is scheduled at 2021-04-30 13:14:06: std::exception. Code: 1001, type: std::length_error, e.what() = basic_string, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x891bc99 in /usr/bin/clickhouse\r\n1. ? @ 0x891bc60 in /usr/bin/clickhouse\r\n2. ? @ 0x1435753b in ?\r\n3. ? @ 0x1435871c in ?\r\n4. DB::Field::operator=(std::__1::basic_string_view<char, std::__1::char_traits<char> > const&) @ 0xf997726 in /usr/bin/clickhouse\r\n5. DB::ColumnString::get(unsigned long, DB::Field&) const @ 0xfa7d1d2 in /usr/bin/clickhouse\r\n6. auto DB::HashedDictionary<(DB::DictionaryKeyType)1, false>::blockToAttributes(DB::Block const&)::'lambda'(auto&)::operator()<HashMapTable<StringRef, HashMapCellWithSavedHash<StringRef, StringRef, DefaultHash<StringRef, void>, HashTableNoState>, DefaultHash<StringRef, void>, HashTableGrower<8ul>, Allocator<true, true> > >(auto&) const @ 0xda21c0d in /usr/bin/clickhouse\r\n7. DB::HashedDictionary<(DB::DictionaryKeyType)1, false>::blockToAttributes(DB::Block const&) @ 0xd91f299 in /usr/bin/clickhouse\r\n8. DB::HashedDictionary<(DB::DictionaryKeyType)1, false>::updateData() @ 0xd9211a4 in /usr/bin/clickhouse\r\n9. DB::HashedDictionary<(DB::DictionaryKeyType)1, false>::loadData() @ 0xd91dd5a in /usr/bin/clickhouse\r\n10. DB::HashedDictionary<(DB::DictionaryKeyType)1, false>::HashedDictionary(DB::StorageID const&, DB::DictionaryStructure const&, std::__1::unique_ptr<DB::IDictionarySource, std::__1::default_delete<DB::IDictionarySource> >, DB::ExternalLoadableLifetime, bool, std::__1::shared_ptr<DB::Block>) @ 0xd91dad9 in /usr/bin/clickhouse\r\n11. ? @ 0xda26b8d in /usr/bin/clickhouse\r\n12. ? @ 0xda26ecc in /usr/bin/clickhouse\r\n13. DB::DictionaryFactory::create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Util::AbstractConfiguration const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context const&, bool) const @ 0xee13c28 in /usr/bin/clickhouse\r\n14. DB::ExternalDictionariesLoader::create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Util::AbstractConfiguration const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0xf40bea1 in /usr/bin/clickhouse\r\n15. DB::ExternalLoader::LoadingDispatcher::loadSingleObject(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const>) @ 0xf417de7 in /usr/bin/clickhouse\r\n16. DB::ExternalLoader::LoadingDispatcher::doLoading(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool, unsigned long, bool) @ 0xf415249 in /usr/bin/clickhouse\r\n17. ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ExternalLoader::LoadingDispatcher::*)(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool, unsigned long, bool), DB::ExternalLoader::LoadingDispatcher*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, unsigned long&, bool&, unsigned long&, bool>(void (DB::ExternalLoader::LoadingDispatcher::*&&)(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool, unsigned long, bool), DB::ExternalLoader::LoadingDispatcher*&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, unsigned long&, bool&, unsigned long&, bool&&)::'lambda'()::operator()() @ 0xf41a421 in /usr/bin/clickhouse\r\n18. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8954fef in /usr/bin/clickhouse\r\n19. ? @ 0x8958a83 in /usr/bin/clickhouse\r\n20. start_thread @ 0x74a4 in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n21. __clone @ 0xe8d0f in /lib/x86_64-linux-gnu/libc-2.24.so\r\n (version 21.4.4.30 (official build))\r\n```\n",
  "hints_text": "The dictionary source is MySQL and the expression for `update_field` looks like this:\r\n```\r\n<update_field>date_add(updated_date, interval 15 minute)</update_field>\r\n```\r\nI'm trying to create a test case, so far I found that `update_field` as an expression works really weird with ClickHouse as a source...\n@SaltTan if you remove ```update field``` dictionary will load successfully ?\n> @SaltTan if you remove `update field` dictionary will load successfully ?\r\n\r\nYes.\r\n\r\nHere is a test that demonstrates that `update_field` acts like `where`:\r\n```\r\ncreate dictionary system.complex_key_hashed_dict \r\n(\r\nname\tString,\r\ncode\tInt32,\r\nvalue\tUInt64,\r\nlast_error_time\tDateTime,\r\nlast_error_message\tString,\r\nremote\tUInt8\r\n)\r\nPRIMARY KEY name\r\nSOURCE(CLICKHOUSE(db 'system' table 'errors' update_field 'date_add(minute, 1, last_error_time)'))\r\nLAYOUT(complex_key_hashed())\r\nlifetime(10);\r\n\r\n21.4.3\r\nselect name, code, value, last_error_time, date_add(minute, 1, last_error_time) as update_field, toString(now()) as now \r\nfrom system.errors order by last_error_time;\r\n\r\nname               | code | value | last_error_time     | update_field        | now                \r\n-------------------+------+-------+---------------------+---------------------+--------------------\r\nUNKNOWN_DICTIONARY |  488 |     1 | 2021-04-30 11:13:55 | 2021-04-30 14:14:55 | 2021-04-30 18:19:47\r\nUNKNOWN_TABLE      |   60 |     1 | 2021-04-30 11:14:15 | 2021-04-30 14:15:15 | 2021-04-30 18:19:47\r\nTHERE_IS_NO_COLUMN |    8 |     1 | 2021-04-30 11:19:25 | 2021-04-30 14:20:25 | 2021-04-30 18:19:47\r\nKEEPER_EXCEPTION   |  999 |    10 | 2021-04-30 14:43:17 | 2021-04-30 17:44:17 | 2021-04-30 18:19:47\r\nSYNTAX_ERROR       |   62 |     1 | 2021-04-30 18:13:41 | 2021-04-30 18:14:41 | 2021-04-30 18:19:47\r\n\r\n\r\nselect name, code, value, last_error_time \r\nfrom system.complex_key_hashed_dict order by last_error_time;\r\n\r\nname               | code | value | last_error_time    \r\n-------------------+------+-------+--------------------\r\nUNKNOWN_DICTIONARY |  488 |     1 | 2021-04-30 11:13:55\r\nUNKNOWN_TABLE      |   60 |     1 | 2021-04-30 11:14:15\r\nTHERE_IS_NO_COLUMN |    8 |     1 | 2021-04-30 11:19:25\r\nKEEPER_EXCEPTION   |  999 |    10 | 2021-04-30 14:43:17\r\nSYNTAX_ERROR       |   62 |     1 | 2021-04-30 18:13:41\r\n\r\n\r\n21.4.5\r\nselect name, code, value, last_error_time, date_add(minute, 1, last_error_time) as update_field, toString(now()) as now \r\nfrom system.errors order by last_error_time;\r\n\r\nname               | code | value | last_error_time     | update_field        | now                \r\n-------------------+------+-------+---------------------+---------------------+--------------------\r\nUNKNOWN_TABLE      |   60 |     1 | 2021-04-30 18:11:04 | 2021-04-30 18:12:04 | 2021-04-30 18:20:44\r\nSYNTAX_ERROR       |   62 |     2 | 2021-04-30 18:13:51 | 2021-04-30 18:14:51 | 2021-04-30 18:20:44\r\nUNKNOWN_IDENTIFIER |   47 |     1 | 2021-04-30 18:15:32 | 2021-04-30 18:16:32 | 2021-04-30 18:20:44\r\n\r\nselect name, code, value, last_error_time \r\nfrom system.complex_key_hashed_dict order by last_error_time;\r\n\r\nname               | code | value | last_error_time    \r\n-------------------+------+-------+--------------------\r\nUNKNOWN_IDENTIFIER |   47 |     1 | 2021-04-30 18:15:32\r\n\r\n```\nAnother test\r\n```\r\ndrop table if exists table_for_update_field_dictionary;\r\ncreate table table_for_update_field_dictionary \r\n(\r\n  flat_key UInt64,\r\n  complex_key Int32 default flat_key,\r\n  value String,\r\n  last_insert_time DateTime default now()\r\n) engine=Log;\r\n\r\ninsert into table_for_update_field_dictionary (flat_key) values (1);\r\n\r\ncreate dictionary if not exists complex_key_hashed_dict \r\n(\r\n  flat_key UInt64,\r\n  complex_key Int32,\r\n  value String,\r\n  last_insert_time DateTime\r\n)\r\nPRIMARY KEY complex_key\r\nSOURCE(CLICKHOUSE(db 'dw' table 'table_for_update_field_dictionary' update_field 'date_add(minute, 1, last_insert_time)'))\r\nLAYOUT(complex_key_hashed())\r\nlifetime(10);\r\n\r\ncreate dictionary if not exists flat_dict \r\n(\r\n  flat_key UInt64,\r\n  complex_key Int32,\r\n  value String,\r\n  last_insert_time DateTime\r\n)\r\nPRIMARY KEY flat_key\r\nSOURCE(CLICKHOUSE(db 'dw' table 'table_for_update_field_dictionary' update_field 'date_add(minute, 1, last_insert_time)'))\r\nLAYOUT(flat())\r\nlifetime(10);\r\n\r\nsystem reload dictionary flat_dict;\r\nsystem reload dictionary complex_key_hashed_dict;\r\n\r\n-- wait for 1 minute\r\ninsert into table_for_update_field_dictionary (flat_key) values (2);\r\n\r\n-- wait for 10 seconds\r\nselect * from flat_dict;\r\n\r\nflat_key | complex_key | value | last_insert_time   \r\n---------+-------------+-------+--------------------\r\n       1 |           1 |       | 2021-04-30 16:56:24\r\n       2 |           2 |       | 2021-04-30 16:59:17\r\n\r\n\r\nselect * from complex_key_hashed_dict;\r\n\r\ncomplex_key | flat_key | value | last_insert_time   \r\n------------+----------+-------+--------------------\r\n          2 |        2 |       | 2021-04-30 16:59:17\r\n\r\n```\nMarking major (regression)",
  "created_at": "2021-04-30T22:02:22Z"
}