{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 56502,
  "instance_id": "ClickHouse__ClickHouse-56502",
  "issue_numbers": [
    "56481"
  ],
  "base_commit": "0d2277b6c11f949b621f85a361fce0bd5dd33526",
  "patch": "diff --git a/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp b/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\nindex b0f3ac700422..c5e42e76653c 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\n@@ -411,7 +411,6 @@ struct MinMaxProjectionCandidate\n {\n     AggregateProjectionCandidate candidate;\n     Block block;\n-    MergeTreeData::DataPartsVector normal_parts;\n };\n \n struct AggregateProjectionCandidates\n@@ -477,7 +476,6 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n         {\n             // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection analyzed DAG {}\", proj_dag->dumpDAG());\n             AggregateProjectionCandidate candidate{.info = std::move(info), .dag = std::move(proj_dag)};\n-            MergeTreeData::DataPartsVector minmax_projection_normal_parts;\n \n             // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection sample block {}\", sample_block.dumpStructure());\n             auto block = reading.getMergeTreeData().getMinMaxCountProjectionBlock(\n@@ -486,13 +484,12 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n                 dag.filter_node != nullptr,\n                 query_info,\n                 parts,\n-                minmax_projection_normal_parts,\n                 max_added_blocks.get(),\n                 context);\n \n             // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection sample block 2 {}\", block.dumpStructure());\n \n-            // minmax_count_projection cannot be used used when there is no data to process, because\n+            // minmax_count_projection cannot be used when there is no data to process, because\n             // it will produce incorrect result during constant aggregation.\n             // See https://github.com/ClickHouse/ClickHouse/issues/36728\n             if (block)\n@@ -500,7 +497,6 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n                 MinMaxProjectionCandidate minmax;\n                 minmax.candidate = std::move(candidate);\n                 minmax.block = std::move(block);\n-                minmax.normal_parts = std::move(minmax_projection_normal_parts);\n                 minmax.candidate.projection = projection;\n                 candidates.minmax_projection.emplace(std::move(minmax));\n             }\n@@ -509,6 +505,18 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n \n     if (!candidates.minmax_projection)\n     {\n+        auto it = std::find_if(agg_projections.begin(), agg_projections.end(), [&](const auto * projection)\n+        {\n+            return projection->name == context->getSettings().preferred_optimize_projection_name.value;\n+        });\n+\n+        if (it != agg_projections.end())\n+        {\n+            const ProjectionDescription * preferred_projection = *it;\n+            agg_projections.clear();\n+            agg_projections.push_back(preferred_projection);\n+        }\n+\n         candidates.real.reserve(agg_projections.size());\n         for (const auto * projection : agg_projections)\n         {\n@@ -570,57 +578,74 @@ bool optimizeUseAggregateProjections(QueryPlan::Node & node, QueryPlan::Nodes &\n \n     auto candidates = getAggregateProjectionCandidates(node, *aggregating, *reading, max_added_blocks, allow_implicit_projections);\n \n-    AggregateProjectionCandidate * best_candidate = nullptr;\n-    if (candidates.minmax_projection)\n-        best_candidate = &candidates.minmax_projection->candidate;\n-    else if (candidates.real.empty())\n-        return false;\n-\n     const auto & parts = reading->getParts();\n+    const auto & alter_conversions = reading->getAlterConvertionsForParts();\n     const auto & query_info = reading->getQueryInfo();\n     const auto metadata = reading->getStorageMetadata();\n     ContextPtr context = reading->getContext();\n     MergeTreeDataSelectExecutor reader(reading->getMergeTreeData());\n+    AggregateProjectionCandidate * best_candidate = nullptr;\n+    if (candidates.minmax_projection)\n+    {\n+        best_candidate = &candidates.minmax_projection->candidate;\n+    }\n+    else if (!candidates.real.empty())\n+    {\n+        auto ordinary_reading_select_result = reading->selectRangesToRead(parts, alter_conversions);\n+        size_t ordinary_reading_marks = ordinary_reading_select_result->marks();\n \n-    auto ordinary_reading_select_result = reading->selectRangesToRead(parts, /* alter_conversions = */ {});\n-    size_t ordinary_reading_marks = ordinary_reading_select_result->marks();\n+        /// Nothing to read. Ignore projections.\n+        if (ordinary_reading_marks == 0)\n+        {\n+            reading->setAnalyzedResult(std::move(ordinary_reading_select_result));\n+            return false;\n+        }\n \n-    const auto & proj_name_from_settings = context->getSettings().preferred_optimize_projection_name.value;\n-    bool found_best_candidate = false;\n+        const auto & parts_with_ranges = ordinary_reading_select_result->partsWithRanges();\n \n-    /// Selecting best candidate.\n-    for (auto & candidate : candidates.real)\n-    {\n-        auto required_column_names = candidate.dag->getRequiredColumnsNames();\n-        ActionDAGNodes added_filter_nodes;\n-        if (candidates.has_filter)\n-            added_filter_nodes.nodes.push_back(candidate.dag->getOutputs().front());\n+        /// Selecting best candidate.\n+        for (auto & candidate : candidates.real)\n+        {\n+            auto required_column_names = candidate.dag->getRequiredColumnsNames();\n+            ActionDAGNodes added_filter_nodes;\n+            if (candidates.has_filter)\n+                added_filter_nodes.nodes.push_back(candidate.dag->getOutputs().front());\n+\n+            bool analyzed = analyzeProjectionCandidate(\n+                candidate,\n+                *reading,\n+                reader,\n+                required_column_names,\n+                parts_with_ranges,\n+                metadata,\n+                query_info,\n+                context,\n+                max_added_blocks,\n+                added_filter_nodes);\n \n-        bool analyzed = analyzeProjectionCandidate(\n-            candidate, *reading, reader, required_column_names, parts,\n-            metadata, query_info, context, max_added_blocks, added_filter_nodes);\n+            if (!analyzed)\n+                continue;\n \n-        if (!analyzed)\n-            continue;\n+            if (candidate.sum_marks > ordinary_reading_marks)\n+                continue;\n \n-        if (candidate.sum_marks > ordinary_reading_marks)\n-            continue;\n+            if (best_candidate == nullptr || best_candidate->sum_marks > candidate.sum_marks)\n+                best_candidate = &candidate;\n+        }\n \n-        if ((best_candidate == nullptr || best_candidate->sum_marks > candidate.sum_marks) && !found_best_candidate)\n-            best_candidate = &candidate;\n-        if (!proj_name_from_settings.empty() && candidate.projection->name == proj_name_from_settings)\n+        if (!best_candidate)\n         {\n-            best_candidate = &candidate;\n-            found_best_candidate = true;\n+            reading->setAnalyzedResult(std::move(ordinary_reading_select_result));\n+            return false;\n         }\n     }\n-\n-    if (!best_candidate)\n+    else\n     {\n-        reading->setAnalyzedResult(std::move(ordinary_reading_select_result));\n         return false;\n     }\n \n+    chassert(best_candidate != nullptr);\n+\n     QueryPlanStepPtr projection_reading;\n     bool has_ordinary_parts;\n \n@@ -641,9 +666,7 @@ bool optimizeUseAggregateProjections(QueryPlan::Node & node, QueryPlan::Nodes &\n                       .storage_id = reading->getMergeTreeData().getStorageID(),\n                       .projection_name = candidates.minmax_projection->candidate.projection->name,\n                   });\n-        has_ordinary_parts = !candidates.minmax_projection->normal_parts.empty();\n-        if (has_ordinary_parts)\n-            reading->resetParts(std::move(candidates.minmax_projection->normal_parts));\n+        has_ordinary_parts = false;\n     }\n     else\n     {\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp b/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp\nindex 1d14b7b432a6..6880d21facb4 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp\n@@ -10,7 +10,7 @@\n #include <Storages/ProjectionsDescription.h>\n #include <Storages/SelectQueryInfo.h>\n #include <Storages/MergeTree/MergeTreeDataSelectExecutor.h>\n-#include <stack>\n+#include <algorithm>\n \n namespace DB::QueryPlanOptimizations\n {\n@@ -109,6 +109,19 @@ bool optimizeUseNormalProjections(Stack & stack, QueryPlan::Nodes & nodes)\n     if (normal_projections.empty())\n         return false;\n \n+    ContextPtr context = reading->getContext();\n+    auto it = std::find_if(normal_projections.begin(), normal_projections.end(), [&](const auto * projection)\n+    {\n+        return projection->name == context->getSettings().preferred_optimize_projection_name.value;\n+    });\n+\n+    if (it != normal_projections.end())\n+    {\n+        const ProjectionDescription * preferred_projection = *it;\n+        normal_projections.clear();\n+        normal_projections.push_back(preferred_projection);\n+    }\n+\n     QueryDAG query;\n     {\n         auto & child = iter->node->children[iter->next_child - 1];\n@@ -124,30 +137,24 @@ bool optimizeUseNormalProjections(Stack & stack, QueryPlan::Nodes & nodes)\n \n     const Names & required_columns = reading->getRealColumnNames();\n     const auto & parts = reading->getParts();\n+    const auto & alter_conversions = reading->getAlterConvertionsForParts();\n     const auto & query_info = reading->getQueryInfo();\n-    ContextPtr context = reading->getContext();\n     MergeTreeDataSelectExecutor reader(reading->getMergeTreeData());\n \n-    auto ordinary_reading_select_result = reading->selectRangesToRead(parts, /* alter_conversions = */ {});\n+    auto ordinary_reading_select_result = reading->selectRangesToRead(parts, alter_conversions);\n     size_t ordinary_reading_marks = ordinary_reading_select_result->marks();\n \n-    std::shared_ptr<PartitionIdToMaxBlock> max_added_blocks = getMaxAddedBlocks(reading);\n-\n-    // Here we iterate over the projections and check if we have the same projections as we specified in preferred_projection_name\n-    bool is_projection_found = false;\n-    const auto & proj_name_from_settings = context->getSettings().preferred_optimize_projection_name.value;\n-    if (!proj_name_from_settings.empty())\n+    /// Nothing to read. Ignore projections.\n+    if (ordinary_reading_marks == 0)\n     {\n-        for (const auto * projection : normal_projections)\n-        {\n-            if (projection->name == proj_name_from_settings)\n-            {\n-                is_projection_found = true;\n-                break;\n-            }\n-        }\n+        reading->setAnalyzedResult(std::move(ordinary_reading_select_result));\n+        return false;\n     }\n \n+    const auto & parts_with_ranges = ordinary_reading_select_result->partsWithRanges();\n+\n+    std::shared_ptr<PartitionIdToMaxBlock> max_added_blocks = getMaxAddedBlocks(reading);\n+\n     for (const auto * projection : normal_projections)\n     {\n         if (!hasAllRequiredColumns(projection, required_columns))\n@@ -161,8 +168,16 @@ bool optimizeUseNormalProjections(Stack & stack, QueryPlan::Nodes & nodes)\n             added_filter_nodes.nodes.push_back(query.filter_node);\n \n         bool analyzed = analyzeProjectionCandidate(\n-            candidate, *reading, reader, required_columns, parts,\n-            metadata, query_info, context, max_added_blocks, added_filter_nodes);\n+            candidate,\n+            *reading,\n+            reader,\n+            required_columns,\n+            parts_with_ranges,\n+            metadata,\n+            query_info,\n+            context,\n+            max_added_blocks,\n+            added_filter_nodes);\n \n         if (!analyzed)\n             continue;\n@@ -170,9 +185,7 @@ bool optimizeUseNormalProjections(Stack & stack, QueryPlan::Nodes & nodes)\n         if (candidate.sum_marks >= ordinary_reading_marks)\n             continue;\n \n-        if (!is_projection_found && (best_candidate == nullptr || candidate.sum_marks < best_candidate->sum_marks))\n-            best_candidate = &candidate;\n-        else if (is_projection_found && projection->name == proj_name_from_settings)\n+        if (best_candidate == nullptr || candidate.sum_marks < best_candidate->sum_marks)\n             best_candidate = &candidate;\n     }\n \ndiff --git a/src/Processors/QueryPlan/Optimizations/projectionsCommon.cpp b/src/Processors/QueryPlan/Optimizations/projectionsCommon.cpp\nindex 7ddda29cad43..c3b3449857b0 100644\n--- a/src/Processors/QueryPlan/Optimizations/projectionsCommon.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/projectionsCommon.cpp\n@@ -210,7 +210,7 @@ bool analyzeProjectionCandidate(\n     const ReadFromMergeTree & reading,\n     const MergeTreeDataSelectExecutor & reader,\n     const Names & required_column_names,\n-    const MergeTreeData::DataPartsVector & parts,\n+    const RangesInDataParts & parts_with_ranges,\n     const StorageMetadataPtr & metadata,\n     const SelectQueryInfo & query_info,\n     const ContextPtr & context,\n@@ -219,14 +219,20 @@ bool analyzeProjectionCandidate(\n {\n     MergeTreeData::DataPartsVector projection_parts;\n     MergeTreeData::DataPartsVector normal_parts;\n-    for (const auto & part : parts)\n+    std::vector<AlterConversionsPtr> alter_conversions;\n+    for (const auto & part_with_ranges : parts_with_ranges)\n     {\n-        const auto & created_projections = part->getProjectionParts();\n+        const auto & created_projections = part_with_ranges.data_part->getProjectionParts();\n         auto it = created_projections.find(candidate.projection->name);\n         if (it != created_projections.end())\n+        {\n             projection_parts.push_back(it->second);\n+        }\n         else\n-            normal_parts.push_back(part);\n+        {\n+            normal_parts.push_back(part_with_ranges.data_part);\n+            alter_conversions.push_back(part_with_ranges.alter_conversions);\n+        }\n     }\n \n     if (projection_parts.empty())\n@@ -252,7 +258,8 @@ bool analyzeProjectionCandidate(\n \n     if (!normal_parts.empty())\n     {\n-        auto normal_result_ptr = reading.selectRangesToRead(std::move(normal_parts), /* alter_conversions = */ {});\n+        /// TODO: We can reuse existing analysis_result by filtering out projection parts\n+        auto normal_result_ptr = reading.selectRangesToRead(std::move(normal_parts), std::move(alter_conversions));\n \n         if (normal_result_ptr->error())\n             return false;\ndiff --git a/src/Processors/QueryPlan/Optimizations/projectionsCommon.h b/src/Processors/QueryPlan/Optimizations/projectionsCommon.h\nindex 35daccad1154..055ca5d40848 100644\n--- a/src/Processors/QueryPlan/Optimizations/projectionsCommon.h\n+++ b/src/Processors/QueryPlan/Optimizations/projectionsCommon.h\n@@ -19,6 +19,7 @@ using MergeTreeDataSelectAnalysisResultPtr = std::shared_ptr<MergeTreeDataSelect\n class IMergeTreeDataPart;\n using DataPartPtr = std::shared_ptr<const IMergeTreeDataPart>;\n using DataPartsVector = std::vector<DataPartPtr>;\n+struct RangesInDataParts;\n \n struct StorageInMemoryMetadata;\n using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;\n@@ -71,7 +72,7 @@ bool analyzeProjectionCandidate(\n     const ReadFromMergeTree & reading,\n     const MergeTreeDataSelectExecutor & reader,\n     const Names & required_column_names,\n-    const DataPartsVector & parts,\n+    const RangesInDataParts & parts_with_ranges,\n     const StorageMetadataPtr & metadata,\n     const SelectQueryInfo & query_info,\n     const ContextPtr & context,\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex 5ee6a5da94b6..617de8c85300 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -2258,10 +2258,7 @@ size_t MergeTreeDataSelectAnalysisResult::marks() const\n     if (std::holds_alternative<std::exception_ptr>(result))\n         std::rethrow_exception(std::get<std::exception_ptr>(result));\n \n-    const auto & index_stats = std::get<ReadFromMergeTree::AnalysisResult>(result).index_stats;\n-    if (index_stats.empty())\n-        return 0;\n-    return index_stats.back().num_granules_after;\n+    return std::get<ReadFromMergeTree::AnalysisResult>(result).selected_marks;\n }\n \n UInt64 MergeTreeDataSelectAnalysisResult::rows() const\n@@ -2269,9 +2266,15 @@ UInt64 MergeTreeDataSelectAnalysisResult::rows() const\n     if (std::holds_alternative<std::exception_ptr>(result))\n         std::rethrow_exception(std::get<std::exception_ptr>(result));\n \n-    const auto & index_stats = std::get<ReadFromMergeTree::AnalysisResult>(result).index_stats;\n-    if (index_stats.empty())\n-        return 0;\n     return std::get<ReadFromMergeTree::AnalysisResult>(result).selected_rows;\n }\n+\n+const RangesInDataParts & MergeTreeDataSelectAnalysisResult::partsWithRanges() const\n+{\n+    if (std::holds_alternative<std::exception_ptr>(result))\n+        std::rethrow_exception(std::get<std::exception_ptr>(result));\n+\n+    return std::get<ReadFromMergeTree::AnalysisResult>(result).parts_with_ranges;\n+}\n+\n }\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.h b/src/Processors/QueryPlan/ReadFromMergeTree.h\nindex d5948ddd9bf7..35310e14416c 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.h\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.h\n@@ -197,13 +197,9 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n     bool hasAnalyzedResult() const { return analyzed_result_ptr != nullptr; }\n     void setAnalyzedResult(MergeTreeDataSelectAnalysisResultPtr analyzed_result_ptr_) { analyzed_result_ptr = std::move(analyzed_result_ptr_); }\n \n-    void resetParts(MergeTreeData::DataPartsVector parts)\n-    {\n-        prepared_parts = std::move(parts);\n-        alter_conversions_for_parts = {};\n-    }\n-\n     const MergeTreeData::DataPartsVector & getParts() const { return prepared_parts; }\n+    const std::vector<AlterConversionsPtr> & getAlterConvertionsForParts() const { return alter_conversions_for_parts; }\n+\n     const MergeTreeData & getMergeTreeData() const { return data; }\n     size_t getMaxBlockSize() const { return block_size.max_block_size_rows; }\n     size_t getNumStreams() const { return requested_num_streams; }\n@@ -310,6 +306,7 @@ struct MergeTreeDataSelectAnalysisResult\n     bool error() const;\n     size_t marks() const;\n     UInt64 rows() const;\n+    const RangesInDataParts & partsWithRanges() const;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex e8a0b290dc96..1c0f9208fef7 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -6498,7 +6498,6 @@ Block MergeTreeData::getMinMaxCountProjectionBlock(\n     bool has_filter,\n     const SelectQueryInfo & query_info,\n     const DataPartsVector & parts,\n-    DataPartsVector & normal_parts,\n     const PartitionIdToMaxBlock * max_block_numbers_to_read,\n     ContextPtr query_context) const\n {\n@@ -6623,11 +6622,11 @@ Block MergeTreeData::getMinMaxCountProjectionBlock(\n                 continue;\n         }\n \n+        /// It's extremely rare that some parts have final marks while others don't. To make it\n+        /// straightforward, disable minmax_count projection when `max(pk)' encounters any part with\n+        /// no final mark.\n         if (need_primary_key_max_column && !part->index_granularity.hasFinalMark())\n-        {\n-            normal_parts.push_back(part);\n-            continue;\n-        }\n+            return {};\n \n         real_parts.push_back(part);\n         filter_column_data.back() = 1;\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 07de33aaf58a..54104849fe48 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -401,17 +401,12 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// query_info - used to filter unneeded parts\n     ///\n     /// parts - part set to filter\n-    ///\n-    /// normal_parts - collects parts that don't have all the needed values to form the block.\n-    /// Specifically, this is when a part doesn't contain a final mark and the related max value is\n-    /// required.\n     Block getMinMaxCountProjectionBlock(\n         const StorageMetadataPtr & metadata_snapshot,\n         const Names & required_columns,\n         bool has_filter,\n         const SelectQueryInfo & query_info,\n         const DataPartsVector & parts,\n-        DataPartsVector & normal_parts,\n         const PartitionIdToMaxBlock * max_block_numbers_to_read,\n         ContextPtr query_context) const;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex be5e7c5a938f..e521491c2d51 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -828,8 +828,8 @@ std::optional<std::unordered_set<String>> MergeTreeDataSelectExecutor::filterPar\n }\n \n void MergeTreeDataSelectExecutor::filterPartsByPartition(\n-    std::optional<PartitionPruner> & partition_pruner,\n-    std::optional<KeyCondition> & minmax_idx_condition,\n+    const std::optional<PartitionPruner> & partition_pruner,\n+    const std::optional<KeyCondition> & minmax_idx_condition,\n     MergeTreeData::DataPartsVector & parts,\n     std::vector<AlterConversionsPtr> & alter_conversions,\n     const std::optional<std::unordered_set<String>> & part_values,\n@@ -1288,6 +1288,8 @@ MergeTreeDataSelectAnalysisResultPtr MergeTreeDataSelectExecutor::estimateNumMar\n     selectColumnNames(column_names_to_return, data, real_column_names, virt_column_names, sample_factor_column_queried);\n \n     std::optional<ReadFromMergeTree::Indexes> indexes;\n+    /// NOTE: We don't need alter_conversions because the returned analysis_result is only used for:\n+    /// 1. estimate the number of rows to read; 2. projection reading, which doesn't have alter_conversions.\n     return ReadFromMergeTree::selectRangesToRead(\n         std::move(parts),\n         /*alter_conversions=*/ {},\n@@ -1824,7 +1826,7 @@ void MergeTreeDataSelectExecutor::selectPartsToRead(\n     const std::optional<std::unordered_set<String>> & part_values,\n     const std::optional<KeyCondition> & minmax_idx_condition,\n     const DataTypes & minmax_columns_types,\n-    std::optional<PartitionPruner> & partition_pruner,\n+    const std::optional<PartitionPruner> & partition_pruner,\n     const PartitionIdToMaxBlock * max_block_numbers_to_read,\n     PartFilterCounters & counters)\n {\n@@ -1886,7 +1888,7 @@ void MergeTreeDataSelectExecutor::selectPartsToReadWithUUIDFilter(\n     MergeTreeData::PinnedPartUUIDsPtr pinned_part_uuids,\n     const std::optional<KeyCondition> & minmax_idx_condition,\n     const DataTypes & minmax_columns_types,\n-    std::optional<PartitionPruner> & partition_pruner,\n+    const std::optional<PartitionPruner> & partition_pruner,\n     const PartitionIdToMaxBlock * max_block_numbers_to_read,\n     ContextPtr query_context,\n     PartFilterCounters & counters,\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\nindex d5d8107db485..01c2da9dd633 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n@@ -126,7 +126,7 @@ class MergeTreeDataSelectExecutor\n         const std::optional<std::unordered_set<String>> & part_values,\n         const std::optional<KeyCondition> & minmax_idx_condition,\n         const DataTypes & minmax_columns_types,\n-        std::optional<PartitionPruner> & partition_pruner,\n+        const std::optional<PartitionPruner> & partition_pruner,\n         const PartitionIdToMaxBlock * max_block_numbers_to_read,\n         PartFilterCounters & counters);\n \n@@ -138,7 +138,7 @@ class MergeTreeDataSelectExecutor\n         MergeTreeData::PinnedPartUUIDsPtr pinned_part_uuids,\n         const std::optional<KeyCondition> & minmax_idx_condition,\n         const DataTypes & minmax_columns_types,\n-        std::optional<PartitionPruner> & partition_pruner,\n+        const std::optional<PartitionPruner> & partition_pruner,\n         const PartitionIdToMaxBlock * max_block_numbers_to_read,\n         ContextPtr query_context,\n         PartFilterCounters & counters,\n@@ -178,8 +178,8 @@ class MergeTreeDataSelectExecutor\n \n     /// Filter parts using minmax index and partition key.\n     static void filterPartsByPartition(\n-        std::optional<PartitionPruner> & partition_pruner,\n-        std::optional<KeyCondition> & minmax_idx_condition,\n+        const std::optional<PartitionPruner> & partition_pruner,\n+        const std::optional<KeyCondition> & minmax_idx_condition,\n         MergeTreeData::DataPartsVector & parts,\n         std::vector<AlterConversionsPtr> & alter_conversions,\n         const std::optional<std::unordered_set<String>> & part_values,\ndiff --git a/src/Storages/MergeTree/PartitionPruner.cpp b/src/Storages/MergeTree/PartitionPruner.cpp\nindex 97bb9f3b4d43..a5df08e3df96 100644\n--- a/src/Storages/MergeTree/PartitionPruner.cpp\n+++ b/src/Storages/MergeTree/PartitionPruner.cpp\n@@ -31,7 +31,7 @@ PartitionPruner::PartitionPruner(const StorageMetadataPtr & metadata, ActionsDAG\n {\n }\n \n-bool PartitionPruner::canBePruned(const IMergeTreeDataPart & part)\n+bool PartitionPruner::canBePruned(const IMergeTreeDataPart & part) const\n {\n     if (part.isEmpty())\n         return true;\ndiff --git a/src/Storages/MergeTree/PartitionPruner.h b/src/Storages/MergeTree/PartitionPruner.h\nindex 7f1b74795c4c..e8a740b15245 100644\n--- a/src/Storages/MergeTree/PartitionPruner.h\n+++ b/src/Storages/MergeTree/PartitionPruner.h\n@@ -16,14 +16,15 @@ class PartitionPruner\n     PartitionPruner(const StorageMetadataPtr & metadata, const SelectQueryInfo & query_info, ContextPtr context, bool strict);\n     PartitionPruner(const StorageMetadataPtr & metadata, ActionsDAGPtr filter_actions_dag, ContextPtr context, bool strict);\n \n-    bool canBePruned(const IMergeTreeDataPart & part);\n+    bool canBePruned(const IMergeTreeDataPart & part) const;\n \n     bool isUseless() const { return useless; }\n \n     const KeyCondition & getKeyCondition() const { return partition_condition; }\n \n private:\n-    std::unordered_map<String, bool> partition_filter_map;\n+    /// Cache already analyzed partitions.\n+    mutable std::unordered_map<String, bool> partition_filter_map;\n \n     /// partition_key is adjusted here (with substitution from modulo to moduloLegacy).\n     KeyDescription partition_key;\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 474171ba1b16..b8e82e2bfa9f 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -341,6 +341,8 @@ void StorageMergeTree::alter(\n                     prev_mutation = it->first;\n             }\n \n+            /// Always wait previous mutations synchronously, because alters\n+            /// should be executed in sequential order.\n             if (prev_mutation != 0)\n             {\n                 LOG_DEBUG(log, \"Cannot change metadata with barrier alter query, will wait for mutation {}\", prev_mutation);\n@@ -368,9 +370,7 @@ void StorageMergeTree::alter(\n             resetObjectColumnsFromActiveParts(parts_lock);\n         }\n \n-        /// Always execute required mutations synchronously, because alters\n-        /// should be executed in sequential order.\n-        if (!maybe_mutation_commands.empty())\n+        if (!maybe_mutation_commands.empty() && local_context->getSettingsRef().alter_sync > 0)\n             waitForMutation(mutation_version, false);\n     }\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01710_projection_analysis_reuse_partition.reference b/tests/queries/0_stateless/01710_projection_analysis_reuse_partition.reference\nnew file mode 100644\nindex 000000000000..47b07da250f1\n--- /dev/null\n+++ b/tests/queries/0_stateless/01710_projection_analysis_reuse_partition.reference\n@@ -0,0 +1,1 @@\n+Selected 2/2 parts by partition key, 1 parts by primary key, 1/2 marks by primary key, 1 marks to read from 1 ranges\ndiff --git a/tests/queries/0_stateless/01710_projection_analysis_reuse_partition.sh b/tests/queries/0_stateless/01710_projection_analysis_reuse_partition.sh\nnew file mode 100755\nindex 000000000000..ba8b3818ba38\n--- /dev/null\n+++ b/tests/queries/0_stateless/01710_projection_analysis_reuse_partition.sh\n@@ -0,0 +1,16 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+${CLICKHOUSE_CLIENT} -q \"drop table if exists t\"\n+${CLICKHOUSE_CLIENT} -q \"create table t(s LowCardinality(String), e DateTime64(3), projection p1 (select * order by s, e)) engine MergeTree partition by toYYYYMM(e) order by tuple() settings index_granularity = 8192, index_granularity_bytes = '100M'\"\n+${CLICKHOUSE_CLIENT} -q \"insert into t select 'AAP', toDateTime('2023-07-01') + 360 * number from numbers(50000)\"\n+${CLICKHOUSE_CLIENT} -q \"insert into t select 'AAPL', toDateTime('2023-07-01') + 360 * number from numbers(50000)\"\n+\n+CLICKHOUSE_CLIENT_DEBUG_LOG=$(echo ${CLICKHOUSE_CLIENT} | sed 's/'\"--send_logs_level=${CLICKHOUSE_CLIENT_SERVER_LOGS_LEVEL}\"'/--send_logs_level=debug/g')\n+\n+${CLICKHOUSE_CLIENT_DEBUG_LOG} -q \"select count() from t where e >= '2023-11-08 00:00:00.000' and e < '2023-11-09 00:00:00.000' and s in ('AAPL') format Null\" 2>&1 | grep -oh \"Selected .* parts by partition key, *. parts by primary key, .* marks by primary key, .* marks to read from .* ranges.*$\"\n+\n+${CLICKHOUSE_CLIENT} -q \"drop table t\"\ndiff --git a/tests/queries/0_stateless/01710_projection_with_alter_conversions.reference b/tests/queries/0_stateless/01710_projection_with_alter_conversions.reference\nnew file mode 100644\nindex 000000000000..9874d6464ab7\n--- /dev/null\n+++ b/tests/queries/0_stateless/01710_projection_with_alter_conversions.reference\n@@ -0,0 +1,1 @@\n+1\t2\ndiff --git a/tests/queries/0_stateless/01710_projection_with_alter_conversions.sql b/tests/queries/0_stateless/01710_projection_with_alter_conversions.sql\nnew file mode 100644\nindex 000000000000..649a07b9b5f1\n--- /dev/null\n+++ b/tests/queries/0_stateless/01710_projection_with_alter_conversions.sql\n@@ -0,0 +1,15 @@\n+drop table if exists t;\n+\n+create table t (i int, j int, projection p (select i order by i)) engine MergeTree order by tuple();\n+\n+insert into t values (1, 2);\n+\n+system stop merges t;\n+\n+set alter_sync = 0;\n+\n+alter table t rename column j to k;\n+\n+select * from t;\n+\n+drop table t;\ndiff --git a/tests/queries/0_stateless/01710_projections.sql b/tests/queries/0_stateless/01710_projections.sql\nindex a96339e30fac..7c45792847e7 100644\n--- a/tests/queries/0_stateless/01710_projections.sql\n+++ b/tests/queries/0_stateless/01710_projections.sql\n@@ -1,6 +1,6 @@\n drop table if exists projection_test;\n \n-create table projection_test (`sum(block_count)` UInt64, domain_alias UInt64 alias length(domain), datetime DateTime, domain LowCardinality(String), x_id String, y_id String, block_count Int64, retry_count Int64, duration Int64, kbytes Int64, buffer_time Int64, first_time Int64, total_bytes Nullable(UInt64), valid_bytes Nullable(UInt64), completed_bytes Nullable(UInt64), fixed_bytes Nullable(UInt64), force_bytes Nullable(UInt64), projection p (select toStartOfMinute(datetime) dt_m, countIf(first_time = 0) / count(), avg((kbytes * 8) / duration), count(), sum(block_count) / sum(duration), avg(block_count / duration), sum(buffer_time) / sum(duration), avg(buffer_time / duration), sum(valid_bytes) / sum(total_bytes), sum(completed_bytes) / sum(total_bytes), sum(fixed_bytes) / sum(total_bytes), sum(force_bytes) / sum(total_bytes), sum(valid_bytes) / sum(total_bytes), sum(retry_count) / sum(duration), avg(retry_count / duration), countIf(block_count > 0) / count(), countIf(first_time = 0) / count(), uniqHLL12(x_id), uniqHLL12(y_id) group by dt_m, domain)) engine MergeTree partition by toDate(datetime) order by (toStartOfTenMinutes(datetime), domain) settings index_granularity_bytes = 10000000;\n+create table projection_test (`sum(block_count)` UInt64, domain_alias UInt64 alias length(domain), datetime DateTime, domain LowCardinality(String), x_id String, y_id String, block_count Int64, retry_count Int64, duration Int64, kbytes Int64, buffer_time Int64, first_time Int64, total_bytes Nullable(UInt64), valid_bytes Nullable(UInt64), completed_bytes Nullable(UInt64), fixed_bytes Nullable(UInt64), force_bytes Nullable(UInt64), projection p (select toStartOfMinute(datetime) dt_m, countIf(first_time = 0) / count(), avg((kbytes * 8) / duration), count(), sum(block_count) / sum(duration), avg(block_count / duration), sum(buffer_time) / sum(duration), avg(buffer_time / duration), sum(valid_bytes) / sum(total_bytes), sum(completed_bytes) / sum(total_bytes), sum(fixed_bytes) / sum(total_bytes), sum(force_bytes) / sum(total_bytes), sum(valid_bytes) / sum(total_bytes), sum(retry_count) / sum(duration), avg(retry_count / duration), countIf(block_count > 0) / count(), countIf(first_time = 0) / count(), uniqHLL12(x_id), uniqHLL12(y_id) group by dt_m, domain)) engine MergeTree partition by toDate(datetime) order by toStartOfTenMinutes(datetime) settings index_granularity_bytes = 10000000;\n \n insert into projection_test with rowNumberInAllBlocks() as id select 1, toDateTime('2020-10-24 00:00:00') + (id / 20), toString(id % 100), * from generateRandom('x_id String, y_id String, block_count Int64, retry_count Int64, duration Int64, kbytes Int64, buffer_time Int64, first_time Int64, total_bytes Nullable(UInt64), valid_bytes Nullable(UInt64), completed_bytes Nullable(UInt64), fixed_bytes Nullable(UInt64), force_bytes Nullable(UInt64)', 10, 10, 1) limit 1000 settings max_threads = 1;\n \n",
  "problem_statement": "Partition pruning does not work because of projection.\n```sql\r\nCREATE TABLE t(  \r\n    s LowCardinality(String),\r\n    e DateTime64(3),\r\n    PROJECTION p1 (SELECT * ORDER BY s, e) )\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(e)\r\nORDER BY tuple();\r\n\r\ninsert into t select 'AAP', toDateTime('2023-07-01') + 360*number from numbers(50000);\r\ninsert into t select 'AAPL', toDateTime('2023-07-01') + 360*number from numbers(50000);\r\n\r\nset send_logs_level = 'debug';\r\n\r\nselect count() FROM t WHERE e >= '2023-11-08 00:00:00.000' \r\n  AND e < '2023-11-09 00:00:00.000' \r\n  AND s IN ('AAPL') format Null;\r\n\r\n>> Selected 14/14 parts by partition key <<\r\n\r\nalter table t drop projection p1;\r\n\r\nselect count() FROM t WHERE e >= '2023-11-08 00:00:00.000' \r\n  AND e < '2023-11-09 00:00:00.000' \r\n  AND s IN ('AAPL') format Null;\r\n\r\n>> Selected 2/14 parts by partition key <<\r\n```\r\n\r\nhttps://fiddle.clickhouse.com/67b11426-57fd-4269-9408-94a98b37ec29\r\n\r\nhttps://clickhousedb.slack.com/archives/CU478UEQZ/p1699457746606999 user complains that Clickhouse reads data from S3 disk (projection data ? min_max indexes?) though partition on S3 should be prunned, but it's not.\n",
  "hints_text": "",
  "created_at": "2023-11-09T10:11:43Z"
}