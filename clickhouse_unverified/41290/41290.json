{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 41290,
  "instance_id": "ClickHouse__ClickHouse-41290",
  "issue_numbers": [
    "38886"
  ],
  "base_commit": "32ad082f57c4b27eb8e4bc8b72faa4770610b2dd",
  "patch": "diff --git a/src/DataTypes/ObjectUtils.cpp b/src/DataTypes/ObjectUtils.cpp\nindex 3cf557ec5bfb..c14b9b579eac 100644\n--- a/src/DataTypes/ObjectUtils.cpp\n+++ b/src/DataTypes/ObjectUtils.cpp\n@@ -1,3 +1,4 @@\n+#include <Storages/StorageSnapshot.h>\n #include <DataTypes/ObjectUtils.h>\n #include <DataTypes/DataTypeObject.h>\n #include <DataTypes/DataTypeNothing.h>\n@@ -159,6 +160,16 @@ void convertObjectsToTuples(Block & block, const NamesAndTypesList & extended_st\n     }\n }\n \n+void deduceTypesOfObjectColumns(const StorageSnapshotPtr & storage_snapshot, Block & block)\n+{\n+    if (!storage_snapshot->object_columns.empty())\n+    {\n+        auto options = GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects();\n+        auto storage_columns = storage_snapshot->getColumns(options);\n+        convertObjectsToTuples(block, storage_columns);\n+    }\n+}\n+\n static bool isPrefix(const PathInData::Parts & prefix, const PathInData::Parts & parts)\n {\n     if (prefix.size() > parts.size())\ndiff --git a/src/DataTypes/ObjectUtils.h b/src/DataTypes/ObjectUtils.h\nindex 2dde0ed3e652..c60d5bec208c 100644\n--- a/src/DataTypes/ObjectUtils.h\n+++ b/src/DataTypes/ObjectUtils.h\n@@ -11,6 +11,9 @@\n namespace DB\n {\n \n+struct StorageSnapshot;\n+using StorageSnapshotPtr = std::shared_ptr<StorageSnapshot>;\n+\n /// Returns number of dimensions in Array type. 0 if type is not array.\n size_t getNumberOfDimensions(const IDataType & type);\n \n@@ -38,6 +41,7 @@ DataTypePtr getDataTypeByColumn(const IColumn & column);\n /// Converts Object types and columns to Tuples in @columns_list and @block\n /// and checks that types are consistent with types in @extended_storage_columns.\n void convertObjectsToTuples(Block & block, const NamesAndTypesList & extended_storage_columns);\n+void deduceTypesOfObjectColumns(const StorageSnapshotPtr & storage_snapshot, Block & block);\n \n /// Checks that each path is not the prefix of any other path.\n void checkObjectHasNoAmbiguosPaths(const PathsInData & paths);\n@@ -164,27 +168,24 @@ ColumnsDescription getObjectColumns(\n     const ColumnsDescription & storage_columns,\n     EntryColumnsGetter && entry_columns_getter)\n {\n-    ColumnsDescription res;\n+    std::unordered_map<String, DataTypes> types_in_entries;\n \n-    if (begin == end)\n+    /// Add dummy column for all Object columns\n+    /// to not lose any column if it's missing\n+    /// in all entries. If it exists in any entry\n+    /// dummy column will be removed.\n+    for (const auto & column : storage_columns)\n     {\n-        for (const auto & column : storage_columns)\n+        if (isObject(column.type))\n         {\n-            if (isObject(column.type))\n-            {\n-                auto tuple_type = std::make_shared<DataTypeTuple>(\n-                    DataTypes{std::make_shared<DataTypeUInt8>()},\n-                    Names{ColumnObject::COLUMN_NAME_DUMMY});\n-\n-                res.add({column.name, std::move(tuple_type)});\n-            }\n-        }\n+            auto tuple_type = std::make_shared<DataTypeTuple>(\n+                DataTypes{std::make_shared<DataTypeUInt8>()},\n+                Names{ColumnObject::COLUMN_NAME_DUMMY});\n \n-        return res;\n+            types_in_entries[column.name].push_back(std::move(tuple_type));\n+        }\n     }\n \n-    std::unordered_map<String, DataTypes> types_in_entries;\n-\n     for (auto it = begin; it != end; ++it)\n     {\n         const auto & entry_columns = entry_columns_getter(*it);\n@@ -196,6 +197,7 @@ ColumnsDescription getObjectColumns(\n         }\n     }\n \n+    ColumnsDescription res;\n     for (const auto & [name, types] : types_in_entries)\n         res.add({name, getLeastCommonTypeForObject(types)});\n \ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 4158517fc233..0eae622d9fb6 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -1242,6 +1242,9 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Attaches restored parts to the storage.\n     virtual void attachRestoredParts(MutableDataPartsVector && parts) = 0;\n \n+    void resetObjectColumnsFromActiveParts(const DataPartsLock & lock);\n+    void updateObjectColumns(const DataPartPtr & part, const DataPartsLock & lock);\n+\n     static void incrementInsertedPartsProfileEvent(MergeTreeDataPartType type);\n     static void incrementMergedPartsProfileEvent(MergeTreeDataPartType type);\n \n@@ -1329,9 +1332,6 @@ class MergeTreeData : public IStorage, public WithMutableContext\n         DataPartsVector & duplicate_parts_to_remove,\n         MutableDataPartsVector & parts_from_wal);\n \n-    void resetObjectColumnsFromActiveParts(const DataPartsLock & lock);\n-    void updateObjectColumns(const DataPartPtr & part, const DataPartsLock & lock);\n-\n     /// Create zero-copy exclusive lock for part and disk. Useful for coordination of\n     /// distributed operations which can lead to data duplication. Implemented only in ReplicatedMergeTree.\n     virtual std::optional<ZeroCopyLock> tryCreateZeroCopyExclusiveLock(const String &, const DiskPtr &) { return std::nullopt; }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 97900eef22bc..95faef6aac79 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -483,16 +483,6 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPart(\n     return temp_part;\n }\n \n-void MergeTreeDataWriter::deduceTypesOfObjectColumns(const StorageSnapshotPtr & storage_snapshot, Block & block)\n-{\n-    if (!storage_snapshot->object_columns.empty())\n-    {\n-        auto options = GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects();\n-        auto storage_columns = storage_snapshot->getColumns(options);\n-        convertObjectsToTuples(block, storage_columns);\n-    }\n-}\n-\n MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n     const String & part_name,\n     MergeTreeDataPartType part_type,\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.h b/src/Storages/MergeTree/MergeTreeDataWriter.h\nindex 2f9ab1aae8bf..00438a29fa1d 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.h\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.h\n@@ -45,8 +45,6 @@ class MergeTreeDataWriter\n       */\n     static BlocksWithPartition splitBlockIntoParts(const Block & block, size_t max_parts, const StorageMetadataPtr & metadata_snapshot, ContextPtr context);\n \n-    static void deduceTypesOfObjectColumns(const StorageSnapshotPtr & storage_snapshot, Block & block);\n-\n     /// This structure contains not completely written temporary part.\n     /// Some writes may happen asynchronously, e.g. for blob storages.\n     /// You should call finalize() to wait until all data is written.\ndiff --git a/src/Storages/MergeTree/MergeTreeSink.cpp b/src/Storages/MergeTree/MergeTreeSink.cpp\nindex 5eaa8ec80044..5d00db861a8f 100644\n--- a/src/Storages/MergeTree/MergeTreeSink.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSink.cpp\n@@ -1,6 +1,7 @@\n #include <Storages/MergeTree/MergeTreeSink.h>\n #include <Storages/MergeTree/MergeTreeDataPartInMemory.h>\n #include <Storages/StorageMergeTree.h>\n+#include <DataTypes/ObjectUtils.h>\n #include <Interpreters/PartLog.h>\n \n namespace ProfileEvents\n@@ -56,7 +57,7 @@ void MergeTreeSink::consume(Chunk chunk)\n {\n     auto block = getHeader().cloneWithColumns(chunk.detachColumns());\n \n-    storage.writer.deduceTypesOfObjectColumns(storage_snapshot, block);\n+    deduceTypesOfObjectColumns(storage_snapshot, block);\n     auto part_blocks = storage.writer.splitBlockIntoParts(block, max_parts_per_block, metadata_snapshot, context);\n \n     using DelayedPartitions = std::vector<MergeTreeSink::DelayedChunk::Partition>;\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\nindex 6c7fbcb52d8c..89a34a39c138 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n@@ -1,6 +1,7 @@\n #include <Storages/StorageReplicatedMergeTree.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeQuorumEntry.h>\n #include <Storages/MergeTree/ReplicatedMergeTreeSink.h>\n+#include <DataTypes/ObjectUtils.h>\n #include <Interpreters/PartLog.h>\n #include <Common/SipHash.h>\n #include <Common/ZooKeeper/KeeperException.h>\n@@ -161,7 +162,7 @@ void ReplicatedMergeTreeSink::consume(Chunk chunk)\n       */\n     size_t replicas_num = checkQuorumPrecondition(zookeeper);\n \n-    storage.writer.deduceTypesOfObjectColumns(storage_snapshot, block);\n+    deduceTypesOfObjectColumns(storage_snapshot, block);\n     auto part_blocks = storage.writer.splitBlockIntoParts(block, max_parts_per_block, metadata_snapshot, context);\n \n     using DelayedPartitions = std::vector<ReplicatedMergeTreeSink::DelayedChunk::Partition>;\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 5adc1974257d..e4062734352d 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -335,6 +335,13 @@ void StorageMergeTree::alter(\n                 mutation_version = startMutation(maybe_mutation_commands, local_context);\n         }\n \n+        {\n+            /// Reset Object columns, because column of type\n+            /// Object may be added or dropped by alter.\n+            auto parts_lock = lockParts();\n+            resetObjectColumnsFromActiveParts(parts_lock);\n+        }\n+\n         /// Always execute required mutations synchronously, because alters\n         /// should be executed in sequential order.\n         if (!maybe_mutation_commands.empty())\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex a2d10e57f8fd..20567846924e 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -4649,6 +4649,13 @@ bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMer\n         LOG_INFO(log, \"Applied changes to the metadata of the table. Current metadata version: {}\", metadata_version);\n     }\n \n+    {\n+        /// Reset Object columns, because column of type\n+        /// Object may be added or dropped by alter.\n+        auto parts_lock = lockParts();\n+        resetObjectColumnsFromActiveParts(parts_lock);\n+    }\n+\n     /// This transaction may not happen, but it's OK, because on the next retry we will eventually create/update this node\n     /// TODO Maybe do in in one transaction for Replicated database?\n     zookeeper->createOrUpdate(fs::path(replica_path) / \"metadata_version\", std::to_string(metadata_version), zkutil::CreateMode::Persistent);\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01825_type_json_add_column.reference.j2 b/tests/queries/0_stateless/01825_type_json_add_column.reference.j2\nnew file mode 100644\nindex 000000000000..da724aef01a4\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_add_column.reference.j2\n@@ -0,0 +1,6 @@\n+{% for storage in [\"MergeTree\", \"ReplicatedMergeTree('/clickhouse/tables/{database}/test_01825_add_column/', 'r1')\"] -%}\n+{\"id\":\"1\",\"s\":{\"k1\":0}}\n+{\"id\":\"2\",\"s\":{\"k1\":100}}\n+{\"id\":\"1\"}\n+{\"id\":\"2\"}\n+{% endfor -%}\ndiff --git a/tests/queries/0_stateless/01825_type_json_add_column.sql.j2 b/tests/queries/0_stateless/01825_type_json_add_column.sql.j2\nnew file mode 100644\nindex 000000000000..87c76c042a6b\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_add_column.sql.j2\n@@ -0,0 +1,23 @@\n+-- Tags: no-fasttest\n+\n+{% for storage in [\"MergeTree\", \"ReplicatedMergeTree('/clickhouse/tables/{database}/test_01825_add_column/', 'r1')\"] -%}\n+\n+DROP TABLE IF EXISTS t_json_add_column;\n+SET allow_experimental_object_type = 1;\n+\n+CREATE TABLE t_json_add_column (id UInt64) ENGINE = {{ storage }} ORDER BY tuple();\n+\n+INSERT INTO t_json_add_column VALUES (1);\n+ALTER TABLE t_json_add_column ADD COLUMN s JSON;\n+\n+INSERT INTO t_json_add_column VALUES(2, '{\"k1\": 100}');\n+\n+SELECT * FROM t_json_add_column ORDER BY id FORMAT JSONEachRow;\n+\n+ALTER TABLE t_json_add_column DROP COLUMN s;\n+\n+SELECT * FROM t_json_add_column ORDER BY id FORMAT JSONEachRow;\n+\n+DROP TABLE t_json_add_column;\n+\n+{% endfor -%}\n",
  "problem_statement": "Possible issue with JSON fields preventing altering tables.\nJSON field added via alteration does not infer types - throws LOGICAL_ERROR. This prevents records to be written after the alteration.\r\n\r\nI would expect that I can still insert data into the pre-alteration and post-alteration JSON fields regardless of their schema.\r\n\r\n**Clickhouse version**\r\n\r\n```\r\nSELECT version()\r\n\r\nQuery id: 061064c4-5cb3-4833-8acb-8ea79709ca93\r\n\r\n\u250c\u2500version()\u2500\u2500\u2510\r\n\u2502 22.7.1.163 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n```\r\n\r\n**How to reproduce**\r\n\r\nCreate the db:\r\n\r\n```\r\ncreate database json_tests;\r\n```\r\n\r\nEnable experimental object type:\r\n\r\n```\r\nSET allow_experimental_object_type = 1;\r\n```\r\n\r\nCreate the first table:\r\n\r\n```\r\ncreate table if not exists json_tests.testA\r\n(\r\n    ts DateTime64(0, 'UTC'),\r\n    id UUID,\r\n    data JSON\r\n)\r\nengine = ReplacingMergeTree()\r\npartition by toYYYYMM(ts)\r\norder by (ts, id)\r\nttl toDateTime(ts) + interval 5 day;\r\n```\r\n\r\nInsert some data:\r\n\r\n```\r\ninsert into json_tests.testA format JSONEachRow {\"ts\": 1657036800, \"id\": \"a2bd6255-6474-42c9-8137-8f83af49754b\", \"data\": {\"a\": \"1\", \"b\": \"2\", \"c\": [1,2], \"d\": {\"a\": 1}}};\r\n```\r\n\r\nSo far so good, now alter:\r\n```\r\nalter table json_tests.testA\r\nadd column metrics JSON\r\nafter data;\r\n```\r\n\r\nInsert some data:\r\n```\r\ninsert into json_tests.test format JSONEachRow {\"ts\": 1657036800, \"id\": \"a2bd6255-6474-42c9-8137-8f83af49754b\", \"data\": {\"a\": \"1\", \"b\": \"2\", \"c\": [1,2], \"d\": {\"a\": 1}}, \"metrics\": {\"x\": 1, \"y\": {\"test\": \"this\"}}};\r\n```\r\n\r\nYou will see this error:\r\n\r\n```\r\nReceived exception from server (version 22.7.1):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: Least common type for object can be deduced only from tuples, but Object('json') given. (LOGICAL_ERROR)\r\n```\r\n\r\nNow, create another table but this time add all the fields upfront, without altering:\r\n\r\n```\r\ncreate table if not exists json_tests.testB\r\n(\r\n    ts DateTime64(0, 'UTC'),\r\n    id UUID,\r\n    data JSON,\r\n    metrics JSON\r\n)\r\nengine = ReplacingMergeTree()\r\npartition by toYYYYMM(ts)\r\norder by (ts, id)\r\nttl toDateTime(ts) + interval 5 day;\r\n```\r\n\r\nInsert some data:\r\n\r\n```\r\ninsert into json_tests.testB format JSONEachRow {\"ts\": 1657036800, \"id\": \"a2bd6255-6474-42c9-8137-8f83af49754b\", \"data\": {\"a\": \"1\", \"b\": \"2\", \"c\": [1,2], \"d\": {\"a\": 1}}, \"metrics\": {\"x\": 1, \"y\": {\"test\": \"this\"}}};\r\n```\r\n\r\nThis is as expected:\r\n\r\n```\r\nOk.\r\n1 row in set. Elapsed: 0.006 sec.\r\n```\r\n\r\nI would assume that both methods should yield a success and not a LOGICAL_ERROR.\n",
  "hints_text": "",
  "created_at": "2022-09-13T22:48:01Z"
}