{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 11048,
  "instance_id": "ClickHouse__ClickHouse-11048",
  "issue_numbers": [
    "11034"
  ],
  "base_commit": "f18eedc5e785bcaa3aa65aa40a936cbe4fefd26e",
  "patch": "diff --git a/src/Storages/Kafka/KafkaBlockInputStream.cpp b/src/Storages/Kafka/KafkaBlockInputStream.cpp\nindex 55ff86109410..a2403e66c504 100644\n--- a/src/Storages/Kafka/KafkaBlockInputStream.cpp\n+++ b/src/Storages/Kafka/KafkaBlockInputStream.cpp\n@@ -172,7 +172,7 @@ Block KafkaBlockInputStream::readImpl()\n         }\n     }\n \n-    if (buffer->rebalanceHappened() || total_rows == 0)\n+    if (buffer->polledDataUnusable() || total_rows == 0)\n         return Block();\n \n     /// MATERIALIZED columns can be added here, but I think\ndiff --git a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\nindex eff4161ffb63..ad9d660a989b 100644\n--- a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n+++ b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n@@ -250,12 +250,24 @@ void ReadBufferFromKafkaConsumer::resetToLastCommitted(const char * msg)\n /// Do commit messages implicitly after we processed the previous batch.\n bool ReadBufferFromKafkaConsumer::nextImpl()\n {\n+\n     /// NOTE: ReadBuffer was implemented with an immutable underlying contents in mind.\n     ///       If we failed to poll any message once - don't try again.\n     ///       Otherwise, the |poll_timeout| expectations get flawn.\n-    if (stalled || stopped || !allowed || rebalance_happened)\n+\n+    // we can react on stop only during fetching data\n+    // after block is formed (i.e. during copying data to MV / commiting)  we ignore stop attempts\n+    if (stopped)\n+    {\n+        was_stopped = true;\n+        offsets_stored = 0;\n+        return false;\n+    }\n+\n+    if (stalled || was_stopped || !allowed || rebalance_happened)\n         return false;\n \n+\n     if (current == messages.end())\n     {\n         if (intermediate_commit)\n@@ -267,7 +279,13 @@ bool ReadBufferFromKafkaConsumer::nextImpl()\n             /// Don't drop old messages immediately, since we may need them for virtual columns.\n             auto new_messages = consumer->poll_batch(batch_size, std::chrono::milliseconds(poll_timeout));\n \n-            if (rebalance_happened)\n+            if (stopped)\n+            {\n+                was_stopped = true;\n+                offsets_stored = 0;\n+                return false;\n+            }\n+            else if (rebalance_happened)\n             {\n                 if (!new_messages.empty())\n                 {\n@@ -338,7 +356,7 @@ bool ReadBufferFromKafkaConsumer::nextImpl()\n \n void ReadBufferFromKafkaConsumer::storeLastReadMessageOffset()\n {\n-    if (!stalled && !rebalance_happened)\n+    if (!stalled && !was_stopped && !rebalance_happened)\n     {\n         consumer->store_offset(*(current - 1));\n         ++offsets_stored;\ndiff --git a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\nindex c5b72ed6d7c6..46dace827d0e 100644\n--- a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n+++ b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n@@ -37,7 +37,7 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n     auto pollTimeout() const { return poll_timeout; }\n \n     bool hasMorePolledMessages() const;\n-    auto rebalanceHappened() const { return rebalance_happened; }\n+    bool polledDataUnusable() const { return (was_stopped || rebalance_happened); }\n \n     void storeLastReadMessageOffset();\n     void resetToLastCommitted(const char * msg);\n@@ -69,6 +69,8 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n \n     bool rebalance_happened = false;\n \n+    bool was_stopped = false;\n+\n     // order is important, need to be destructed before consumer\n     cppkafka::TopicPartitionList assignment;\n     const Names topics;\ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex 5c4657403b70..793a9a29676b 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -454,7 +454,10 @@ bool StorageKafka::streamToViews()\n     else\n         in = streams[0];\n \n-    copyData(*in, *block_io.out, &stream_cancelled);\n+    // We can't cancel during copyData, as it's not aware of commits and other kafka-related stuff.\n+    // It will be cancelled on underlying layer (kafka buffer)\n+    std::atomic<bool> stub = {false};\n+    copyData(*in, *block_io.out, &stub);\n     for (auto & stream : streams)\n         stream->as<KafkaBlockInputStream>()->commit();\n \n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex cbe96df3c293..9154ad67c059 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -1241,6 +1241,98 @@ def test_exception_from_destructor(kafka_cluster):\n     assert TSV(instance.query('SELECT 1')) == TSV('1')\n \n \n+@pytest.mark.timeout(120)\n+def test_commits_of_unprocessed_messages_on_drop(kafka_cluster):\n+    messages = [json.dumps({'key': j+1, 'value': j+1}) for j in range(1)]\n+    kafka_produce('commits_of_unprocessed_messages_on_drop', messages)\n+\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.destination;\n+        CREATE TABLE test.destination (\n+            key UInt64,\n+            value UInt64,\n+            _topic String,\n+            _key String,\n+            _offset UInt64,\n+            _partition UInt64,\n+            _timestamp Nullable(DateTime),\n+            _consumed_by LowCardinality(String)\n+        )\n+        ENGINE = MergeTree()\n+        ORDER BY key;\n+\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                    kafka_topic_list = 'commits_of_unprocessed_messages_on_drop',\n+                    kafka_group_name = 'commits_of_unprocessed_messages_on_drop_test_group',\n+                    kafka_format = 'JSONEachRow',\n+                    kafka_max_block_size = 1000;\n+\n+        CREATE MATERIALIZED VIEW test.kafka_consumer TO test.destination AS\n+            SELECT\n+            key,\n+            value,\n+            _topic,\n+            _key,\n+            _offset,\n+            _partition,\n+            _timestamp\n+        FROM test.kafka;\n+    ''')\n+\n+    while int(instance.query(\"SELECT count() FROM test.destination\")) == 0:\n+        print(\"Waiting for test.kafka_consumer to start consume\")\n+        time.sleep(1)\n+\n+    cancel = threading.Event()\n+\n+    i = [2]\n+    def produce():\n+        while not cancel.is_set():\n+            messages = []\n+            for _ in range(113):\n+                messages.append(json.dumps({'key': i[0], 'value': i[0]}))\n+                i[0] += 1\n+            kafka_produce('commits_of_unprocessed_messages_on_drop', messages)\n+            time.sleep(1)\n+\n+    kafka_thread = threading.Thread(target=produce)\n+    kafka_thread.start()\n+    time.sleep(12)\n+\n+    instance.query('''\n+        DROP TABLE test.kafka;\n+    ''')\n+\n+    instance.query('''\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                    kafka_topic_list = 'commits_of_unprocessed_messages_on_drop',\n+                    kafka_group_name = 'commits_of_unprocessed_messages_on_drop_test_group',\n+                    kafka_format = 'JSONEachRow',\n+                    kafka_max_block_size = 10000;\n+    ''')\n+\n+    cancel.set()\n+    time.sleep(15)\n+\n+    #kafka_cluster.open_bash_shell('instance')\n+    # SELECT key, _timestamp, _offset FROM test.destination where runningDifference(key) <> 1 ORDER BY key;\n+\n+    result = instance.query('SELECT count(), uniqExact(key), max(key) FROM test.destination')\n+    print(result)\n+\n+    instance.query('''\n+        DROP TABLE test.kafka_consumer;\n+        DROP TABLE test.destination;\n+    ''')\n+\n+    kafka_thread.join()\n+    assert TSV(result) == TSV('{0}\\t{0}\\t{0}'.format(i[0]-1)), 'Missing data!'\n+\n+\n @pytest.mark.timeout(1200)\n def test_kafka_duplicates_when_commit_failed(kafka_cluster):\n     messages = [json.dumps({'key': j+1, 'value': 'x' * 300}) for j in range(22)]\n",
  "problem_statement": "During table drop Engine=Kafka can commit unprocessed messages\nTest catched missing data:\r\n\r\n```\r\nSELECT \r\n    toUInt64(20) AS _partition, \r\n    number + 1 AS _offset\r\nFROM numbers(119)\r\nLEFT JOIN test.destination USING (_partition, _offset)\r\nWHERE test.destination.key = 0\r\nORDER BY _offset ASC\r\n\r\n\u250c\u2500_partition\u2500\u252c\u2500_offset\u2500\u2510\r\n\u2502         20 \u2502     104 \u2502\r\n\u2502         20 \u2502     105 \u2502\r\n\u2502         20 \u2502     106 \u2502\r\n\u2502         20 \u2502     107 \u2502\r\n\u2502         20 \u2502     108 \u2502\r\n\u2502         20 \u2502     109 \u2502\r\n\u2502         20 \u2502     110 \u2502\r\n\u2502         20 \u2502     111 \u2502\r\n\u2502         20 \u2502     112 \u2502\r\n\u2502         20 \u2502     113 \u2502\r\n\u2502         20 \u2502     114 \u2502\r\n\u2502         20 \u2502     115 \u2502\r\n\u2502         20 \u2502     116 \u2502\r\n\u2502         20 \u2502     117 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nLog \r\n```\r\n2020.05.19 11:11:57.480807 [ 29 ] {} <Debug> StorageKafka (kafka_consumer0): Started streaming to 1 attached views\r\n2020.05.19 11:11:57.485702 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Already subscribed to topics: [ topic_with_multiple_partitions ]\r\n2020.05.19 11:11:57.485765 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Already assigned to : [  ]\r\n2020.05.19 11:11:57.486071 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Topics/partitions assigned: [ topic_with_multiple_partitions[20:#], topic_with_multiple_partitions[21:#] ]\r\n2020.05.19 11:11:57.987310 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Polled batch of 24 messages. Offset position: [ topic_with_multiple_partitions[20:118], topic_with_multiple_partitions[21:113] ]\r\n\r\n2020.05.19 11:11:58.376121 [ 36 ] {b6b2459d-3729-448e-ba52-b207bc34a9e0} <Debug> executeQuery: (from 172.22.0.1:34506) DROP TABLE IF EXISTS test.kafka_consumer0\r\n2020.05.19 11:11:58.376277 [ 36 ] {b6b2459d-3729-448e-ba52-b207bc34a9e0} <Trace> ContextAccess (default): Access granted: DROP TABLE ON test.kafka_consumer0\r\n2020.05.19 11:11:58.376332 [ 36 ] {b6b2459d-3729-448e-ba52-b207bc34a9e0} <Trace> StorageKafka (kafka_consumer0): Waiting for cleanup\r\n\r\n2020.05.19 11:11:58.493658 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Polled batch of 3 messages. Offset position: [ topic_with_multiple_partitions[20:120], topic_with_multiple_partitions[21:114] ]\r\n2020.05.19 11:11:58.497287 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Polled offset 120 (topic: topic_with_multiple_partitions, partition: 20)\r\n2020.05.19 11:11:58.497567 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Polled offset 114 (topic: topic_with_multiple_partitions, partition: 21)\r\n2020.05.19 11:11:58.497862 [ 29 ] {} <Warning> StorageKafka (kafka_consumer0): Logical error. Non all polled messages were processed.\r\n\r\n2020.05.19 11:11:58.498330 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: Group \"rebalance_test_group\" received op OFFSET_COMMIT (v0) in state up (join state started, v9 vs 0)\r\n2020.05.19 11:11:58.498479 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: Topic topic_with_multiple_partitions [20]: stored offset 118, committed offset 104: setting stored offset 118 for commit\r\n2020.05.19 11:11:58.498620 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: Topic topic_with_multiple_partitions [21]: stored offset 114, committed offset 103: setting stored offset 114 for commit\r\n2020.05.19 11:11:58.498745 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: Committing offsets for 2 partition(s): manual\r\n2020.05.19 11:11:58.498888 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: Enqueue OffsetCommitRequest(v6, 2/2 partition(s))): manual\r\n2020.05.19 11:11:58.499206 [ 68 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:GroupCoordinator]: GroupCoordinator/1: Sent OffsetCommitRequest (v6, 193 bytes @ 0, CorrId 35)\r\n2020.05.19 11:11:58.500496 [ 68 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:GroupCoordinator]: GroupCoordinator/1: Received OffsetCommitResponse (v6, 56 bytes, CorrId 35, rtt 1.29ms)\r\n2020.05.19 11:11:58.500721 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: OffsetCommit for 2 partition(s): manual: returned: Success\r\n2020.05.19 11:11:58.500967 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: Group \"rebalance_test_group\" received op GET_ASSIGNMENT (v0) in state up (join state started, v9 vs 0)\r\n2020.05.19 11:11:58.501272 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: Group \"rebalance_test_group\" received op OFFSET_FETCH (v0) in state up (join state started, v9 vs 0)\r\n2020.05.19 11:11:58.501418 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: OffsetFetchRequest(v1) for 2/2 partition(s)\r\n2020.05.19 11:11:58.501532 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: Fetch committed offsets for 2/2 partition(s)\r\n2020.05.19 11:11:58.501793 [ 68 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:GroupCoordinator]: GroupCoordinator/1: Sent OffsetFetchRequest (v1, 103 bytes @ 0, CorrId 36)\r\n2020.05.19 11:11:58.502260 [ 68 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:GroupCoordinator]: GroupCoordinator/1: Received OffsetFetchResponse (v1, 72 bytes, CorrId 36, rtt 0.47ms)\r\n2020.05.19 11:11:58.502456 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: Topic topic_with_multiple_partitions [20]: setting default offset INVALID\r\n2020.05.19 11:11:58.502565 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: Topic topic_with_multiple_partitions [21]: setting default offset INVALID\r\n2020.05.19 11:11:58.502673 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: OffsetFetchResponse: topic_with_multiple_partitions [20] offset 118, metadata 0 byte(s)\r\n2020.05.19 11:11:58.502777 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: OffsetFetchResponse: topic_with_multiple_partitions [21] offset 114, metadata 0 byte(s)\r\n2020.05.19 11:11:58.502872 [ 69 ] {} <Debug> StorageKafka (kafka_consumer0): [thrd:main]: GroupCoordinator/1: OffsetFetch for 2/2 partition(s) returned Success\r\n2020.05.19 11:11:58.503088 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Committed offset 118 (topic: topic_with_multiple_partitions, partition: 20)\r\n2020.05.19 11:11:58.503207 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Committed offset 114 (topic: topic_with_multiple_partitions, partition: 21)\r\n2020.05.19 11:11:58.503990 [ 29 ] {} <Trace> StorageKafka (kafka_consumer0): Execution took 1023 ms.\r\n\r\n2020.05.19 11:11:58.792052 [ 29 ] {} <Debug> StorageKafka (kafka_consumer16): Started streaming to 1 attached views\r\n2020.05.19 11:11:58.794984 [ 29 ] {} <Trace> StorageKafka (kafka_consumer16): Already subscribed to topics: [ topic_with_multiple_partitions ]\r\n2020.05.19 11:11:58.795033 [ 29 ] {} <Trace> StorageKafka (kafka_consumer16): Already assigned to : [ topic_with_multiple_partitions[18:#], topic_with_multiple_partitions[19:#] ]\r\n2020.05.19 11:11:59.295823 [ 29 ] {} <Trace> StorageKafka (kafka_consumer16): Polled batch of 3 messages. Offset position: [ topic_with_multiple_partitions[18:117], topic_with_multiple_partitions[19:148] ]\r\n2020.05.19 11:11:59.796602 [ 29 ] {} <Trace> StorageKafka (kafka_consumer16): Stalled\r\n```\n",
  "hints_text": "",
  "created_at": "2020-05-19T16:57:08Z"
}