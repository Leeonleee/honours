diff --git a/src/IO/WriteBufferFromS3.cpp b/src/IO/WriteBufferFromS3.cpp
index 37bc8c78cf4d..ec77fc44de63 100644
--- a/src/IO/WriteBufferFromS3.cpp
+++ b/src/IO/WriteBufferFromS3.cpp
@@ -18,6 +18,7 @@
 #include <aws/s3/model/PutObjectRequest.h>
 #include <aws/s3/model/UploadPartRequest.h>
 #include <aws/s3/model/HeadObjectRequest.h>
+#include <aws/s3/model/StorageClass.h>
 
 #include <utility>
 
@@ -473,6 +474,8 @@ void WriteBufferFromS3::fillPutRequest(Aws::S3::Model::PutObjectRequest & req)
     req.SetBody(temporary_buffer);
     if (object_metadata.has_value())
         req.SetMetadata(object_metadata.value());
+    if (!settings.storage_class_name.empty())
+        req.SetStorageClass(Aws::S3::Model::StorageClassMapper::GetStorageClassForName(settings.storage_class_name));
 
     /// If we don't do it, AWS SDK can mistakenly set it to application/xml, see https://github.com/aws/aws-sdk-cpp/issues/1840
     req.SetContentType("binary/octet-stream");
diff --git a/src/Storages/StorageS3Settings.cpp b/src/Storages/StorageS3Settings.cpp
index 8c1974527b63..9ae08ef41596 100644
--- a/src/Storages/StorageS3Settings.cpp
+++ b/src/Storages/StorageS3Settings.cpp
@@ -40,6 +40,10 @@ S3Settings::RequestSettings::PartUploadSettings::PartUploadSettings(
     max_single_part_upload_size = config.getUInt64(key + "max_single_part_upload_size", max_single_part_upload_size);
     max_single_operation_copy_size = config.getUInt64(key + "max_single_operation_copy_size", max_single_operation_copy_size);
 
+    /// This configuration is only applicable to s3. Other types of object storage are not applicable or have different meanings.
+    storage_class_name = config.getString(config_prefix + ".s3_storage_class", storage_class_name);
+    storage_class_name = Poco::toUpperInPlace(storage_class_name);
+
     validate();
 }
 
@@ -50,6 +54,10 @@ S3Settings::RequestSettings::PartUploadSettings::PartUploadSettings(const NamedC
     upload_part_size_multiply_parts_count_threshold = collection.getOrDefault<UInt64>("upload_part_size_multiply_parts_count_threshold", upload_part_size_multiply_parts_count_threshold);
     max_single_part_upload_size = collection.getOrDefault<UInt64>("max_single_part_upload_size", max_single_part_upload_size);
 
+    /// This configuration is only applicable to s3. Other types of object storage are not applicable or have different meanings.
+    storage_class_name = collection.getOrDefault<String>("s3_storage_class", storage_class_name);
+    storage_class_name = Poco::toUpperInPlace(storage_class_name);
+
     validate();
 }
 
@@ -137,6 +145,13 @@ void S3Settings::RequestSettings::PartUploadSettings::validate()
             "Setting upload_part_size_multiply_factor is too big ({}). Multiplication to max_upload_part_size ({}) will cause integer overflow",
             ReadableSize(max_part_number), ReadableSize(max_part_number_limit));
 
+    std::unordered_set<String> storage_class_names {"STANDARD", "INTELLIGENT_TIERING"};
+    if (!storage_class_name.empty() && !storage_class_names.contains(storage_class_name))
+        throw Exception(
+            ErrorCodes::INVALID_SETTING_VALUE,
+            "Setting storage_class has invalid value {} which only supports STANDARD and INTELLIGENT_TIERING",
+            storage_class_name);
+
     /// TODO: it's possible to set too small limits. We can check that max possible object size is not too small.
 }
 
diff --git a/src/Storages/StorageS3Settings.h b/src/Storages/StorageS3Settings.h
index 368fcfaf469c..6fa74f4f0d01 100644
--- a/src/Storages/StorageS3Settings.h
+++ b/src/Storages/StorageS3Settings.h
@@ -36,6 +36,7 @@ struct S3Settings
             size_t max_part_number = 10000;
             size_t max_single_part_upload_size = 32 * 1024 * 1024;
             size_t max_single_operation_copy_size = 5ULL * 1024 * 1024 * 1024;
+            String storage_class_name;
 
             void updateFromSettings(const Settings & settings) { updateFromSettingsImpl(settings, true); }
             void validate();
