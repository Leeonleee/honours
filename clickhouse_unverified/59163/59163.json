{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 59163,
  "instance_id": "ClickHouse__ClickHouse-59163",
  "issue_numbers": [
    "59128",
    "58102"
  ],
  "base_commit": "0a01a92bf868fc369a5377f67b5f9a300048aebe",
  "patch": "diff --git a/docs/en/engines/table-engines/integrations/embedded-rocksdb.md b/docs/en/engines/table-engines/integrations/embedded-rocksdb.md\nindex 44febe78c772..1958250ed73e 100644\n--- a/docs/en/engines/table-engines/integrations/embedded-rocksdb.md\n+++ b/docs/en/engines/table-engines/integrations/embedded-rocksdb.md\n@@ -17,6 +17,7 @@ CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n     name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n     ...\n ) ENGINE = EmbeddedRocksDB([ttl, rocksdb_dir, read_only]) PRIMARY KEY(primary_key_name)\n+[ SETTINGS name=value, ... ]\n ```\n \n Engine parameters:\n@@ -29,6 +30,11 @@ Engine parameters:\n - columns other than the primary key will be serialized in binary as `rocksdb` value in corresponding order.\n - queries with key `equals` or `in` filtering will be optimized to multi keys lookup from `rocksdb`.\n \n+Engine settings:\n+\n+- `optimize_for_bulk_insert` \u2013 Table is optimized for bulk insertions (insert pipeline will create SST files and import to rocksdb database instead of writing to memtables); default value: `1`.\n+- `bulk_insert_block_size` - Minimum size of SST files (in term of rows) created by bulk insertion; default value: `1048449`.\n+\n Example:\n \n ``` sql\ndiff --git a/src/Interpreters/IKeyValueEntity.h b/src/Interpreters/IKeyValueEntity.h\nindex 856ce28bae7e..f9287e4793c1 100644\n--- a/src/Interpreters/IKeyValueEntity.h\n+++ b/src/Interpreters/IKeyValueEntity.h\n@@ -3,6 +3,7 @@\n #include <Core/Block.h>\n #include <Core/Names.h>\n #include <Processors/Chunk.h>\n+#include <Core/ColumnsWithTypeAndName.h>\n \n namespace DB\n {\ndiff --git a/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.cpp b/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.cpp\nnew file mode 100644\nindex 000000000000..1aca0edc2237\n--- /dev/null\n+++ b/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.cpp\n@@ -0,0 +1,240 @@\n+#include <atomic>\n+#include <cstddef>\n+#include <cstdlib>\n+#include <filesystem>\n+#include <memory>\n+#include <optional>\n+#include <random>\n+#include <stdatomic.h>\n+#include <IO/WriteBufferFromString.h>\n+#include <Storages/RocksDB/EmbeddedRocksDBBulkSink.h>\n+#include <Storages/RocksDB/StorageEmbeddedRocksDB.h>\n+\n+#include <Columns/ColumnString.h>\n+#include <Core/SortDescription.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <IO/WriteHelpers.h>\n+#include <Interpreters/Context.h>\n+#include <rocksdb/options.h>\n+#include <rocksdb/slice.h>\n+#include <rocksdb/status.h>\n+#include <rocksdb/utilities/db_ttl.h>\n+#include <Common/SipHash.h>\n+#include <Common/getRandomASCIIString.h>\n+#include <Common/CurrentThread.h>\n+#include <Common/MemoryTrackerBlockerInThread.h>\n+#include <Common/logger_useful.h>\n+#include <Common/scope_guard_safe.h>\n+#include <Common/setThreadName.h>\n+\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int ROCKSDB_ERROR;\n+}\n+\n+static const IColumn::Permutation & getAscendingPermutation(const IColumn & column, IColumn::Permutation & perm)\n+{\n+    column.getPermutation(IColumn::PermutationSortDirection::Ascending, IColumn::PermutationSortStability::Stable, 0, 1, perm);\n+    return perm;\n+}\n+\n+/// Build SST file from key-value pairs\n+static rocksdb::Status buildSSTFile(const String & path, const ColumnString & keys, const ColumnString & values, const std::optional<IColumn::Permutation> & perm_ = {})\n+{\n+    /// rocksdb::SstFileWriter requires keys to be sorted in ascending order\n+    IColumn::Permutation calculated_perm;\n+    const IColumn::Permutation & perm = perm_ ? *perm_ : getAscendingPermutation(keys, calculated_perm);\n+\n+    rocksdb::SstFileWriter sst_file_writer(rocksdb::EnvOptions{}, rocksdb::Options{});\n+    auto status = sst_file_writer.Open(path);\n+    if (!status.ok())\n+        return status;\n+\n+    auto rows = perm.size();\n+    for (size_t idx = 0; idx < rows;)\n+    {\n+        /// We will write the last row of the same key\n+        size_t next_idx = idx + 1;\n+        while (next_idx < rows && keys.compareAt(perm[idx], perm[next_idx], keys, 1) == 0)\n+            ++next_idx;\n+\n+        auto row = perm[next_idx - 1];\n+        status = sst_file_writer.Put(keys.getDataAt(row).toView(), values.getDataAt(row).toView());\n+        if (!status.ok())\n+            return status;\n+\n+        idx = next_idx;\n+    }\n+\n+    return sst_file_writer.Finish();\n+}\n+\n+EmbeddedRocksDBBulkSink::EmbeddedRocksDBBulkSink(\n+    ContextPtr context_, StorageEmbeddedRocksDB & storage_, const StorageMetadataPtr & metadata_snapshot_)\n+    : SinkToStorage(metadata_snapshot_->getSampleBlock()), WithContext(context_), storage(storage_), metadata_snapshot(metadata_snapshot_)\n+{\n+    for (const auto & elem : getHeader())\n+    {\n+        if (elem.name == storage.primary_key)\n+            break;\n+        ++primary_key_pos;\n+    }\n+\n+    serializations = getHeader().getSerializations();\n+    min_block_size_rows = std::max(storage.getSettings().bulk_insert_block_size, getContext()->getSettingsRef().min_insert_block_size_rows);\n+\n+    /// If max_insert_threads > 1 we may have multiple EmbeddedRocksDBBulkSink and getContext()->getCurrentQueryId() is not guarantee to\n+    /// to have a distinct path. Also we cannot use query id as directory name here, because it could be defined by user and not suitable\n+    /// for directory name\n+    auto base_directory_name = TMP_INSERT_PREFIX + sipHash128String(getContext()->getCurrentQueryId());\n+    insert_directory_queue = fs::path(storage.getDataPaths()[0]) / (base_directory_name + \"-\" + getRandomASCIIString(8));\n+    fs::create_directory(insert_directory_queue);\n+}\n+\n+EmbeddedRocksDBBulkSink::~EmbeddedRocksDBBulkSink()\n+{\n+    try\n+    {\n+        if (fs::exists(insert_directory_queue))\n+            fs::remove_all(insert_directory_queue);\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(__PRETTY_FUNCTION__, fmt::format(\"Error while removing temporary directory {}:\", insert_directory_queue));\n+    }\n+}\n+\n+std::vector<Chunk> EmbeddedRocksDBBulkSink::squash(Chunk chunk)\n+{\n+    /// End of input stream\n+    if (chunk.getNumRows() == 0)\n+    {\n+        return std::move(chunks);\n+    }\n+\n+    /// Just read block is already enough.\n+    if (isEnoughSize(chunk))\n+    {\n+        /// If no accumulated data, return just read block.\n+        if (chunks.empty())\n+        {\n+            chunks.emplace_back(std::move(chunk));\n+            return {};\n+        }\n+\n+        /// Return accumulated data (maybe it has small size) and place new block to accumulated data.\n+        std::vector<Chunk> to_return;\n+        std::swap(to_return, chunks);\n+        chunks.emplace_back(std::move(chunk));\n+        return to_return;\n+    }\n+\n+    /// Accumulated block is already enough.\n+    if (isEnoughSize(chunks))\n+    {\n+        /// Return accumulated data and place new block to accumulated data.\n+        std::vector<Chunk> to_return;\n+        std::swap(to_return, chunks);\n+        chunks.emplace_back(std::move(chunk));\n+        return to_return;\n+    }\n+\n+    chunks.emplace_back(std::move(chunk));\n+    if (isEnoughSize(chunks))\n+    {\n+        std::vector<Chunk> to_return;\n+        std::swap(to_return, chunks);\n+        return to_return;\n+    }\n+\n+    /// Squashed block is not ready.\n+    return {};\n+}\n+\n+std::pair<ColumnString::Ptr, ColumnString::Ptr> EmbeddedRocksDBBulkSink::serializeChunks(const std::vector<Chunk> & input_chunks) const\n+{\n+    auto serialized_key_column = ColumnString::create();\n+    auto serialized_value_column = ColumnString::create();\n+\n+    {\n+        auto & serialized_key_data = serialized_key_column->getChars();\n+        auto & serialized_key_offsets = serialized_key_column->getOffsets();\n+        auto & serialized_value_data = serialized_value_column->getChars();\n+        auto & serialized_value_offsets = serialized_value_column->getOffsets();\n+        WriteBufferFromVector<ColumnString::Chars> writer_key(serialized_key_data);\n+        WriteBufferFromVector<ColumnString::Chars> writer_value(serialized_value_data);\n+\n+        for (const auto & chunk : input_chunks)\n+        {\n+            const auto & columns = chunk.getColumns();\n+            auto rows = chunk.getNumRows();\n+            for (size_t i = 0; i < rows; ++i)\n+            {\n+                for (size_t idx = 0; idx < columns.size(); ++idx)\n+                    serializations[idx]->serializeBinary(*columns[idx], i, idx == primary_key_pos ? writer_key : writer_value, {});\n+                /// String in ColumnString must be null-terminated\n+                writeChar('\\0', writer_key);\n+                writeChar('\\0', writer_value);\n+                serialized_key_offsets.emplace_back(writer_key.count());\n+                serialized_value_offsets.emplace_back(writer_value.count());\n+            }\n+        }\n+\n+        writer_key.finalize();\n+        writer_value.finalize();\n+    }\n+\n+    return {std::move(serialized_key_column), std::move(serialized_value_column)};\n+}\n+\n+void EmbeddedRocksDBBulkSink::consume(Chunk chunk_)\n+{\n+    std::vector<Chunk> to_written = squash(std::move(chunk_));\n+\n+    if (to_written.empty())\n+        return;\n+\n+    auto [serialized_key_column, serialized_value_column] = serializeChunks(to_written);\n+    auto sst_file_path = getTemporarySSTFilePath();\n+    if (auto status = buildSSTFile(sst_file_path, *serialized_key_column, *serialized_value_column); !status.ok())\n+        throw Exception(ErrorCodes::ROCKSDB_ERROR, \"RocksDB write error: {}\", status.ToString());\n+\n+    /// Ingest the SST file\n+    static rocksdb::IngestExternalFileOptions ingest_options;\n+    ingest_options.move_files = true; /// The temporary file is on the same disk, so move (or hardlink) file will be faster than copy\n+    if (auto status = storage.rocksdb_ptr->IngestExternalFile({sst_file_path}, ingest_options); !status.ok())\n+        throw Exception(ErrorCodes::ROCKSDB_ERROR, \"RocksDB write error: {}\", status.ToString());\n+\n+    if (fs::exists(sst_file_path))\n+        fs::remove(sst_file_path);\n+}\n+\n+void EmbeddedRocksDBBulkSink::onFinish()\n+{\n+    /// If there is any data left, write it.\n+    if (!chunks.empty())\n+        consume({});\n+}\n+\n+String EmbeddedRocksDBBulkSink::getTemporarySSTFilePath()\n+{\n+    return fs::path(insert_directory_queue) / (toString(file_counter++) + \".sst\");\n+}\n+\n+bool EmbeddedRocksDBBulkSink::isEnoughSize(const std::vector<Chunk> & input_chunks) const\n+{\n+    size_t total_rows = 0;\n+    for (const auto & chunk : input_chunks)\n+        total_rows += chunk.getNumRows();\n+    return total_rows >= min_block_size_rows;\n+}\n+\n+bool EmbeddedRocksDBBulkSink::isEnoughSize(const Chunk & chunk) const\n+{\n+    return chunk.getNumRows() >= min_block_size_rows;\n+}\n+}\ndiff --git a/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.h b/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.h\nnew file mode 100644\nindex 000000000000..19ce1e3b83e4\n--- /dev/null\n+++ b/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.h\n@@ -0,0 +1,69 @@\n+#pragma once\n+\n+#include <condition_variable>\n+#include <stdatomic.h>\n+#include <Processors/Sinks/SinkToStorage.h>\n+#include <rocksdb/db.h>\n+#include <rocksdb/status.h>\n+#include <Common/CurrentThread.h>\n+#include <Common/ThreadStatus.h>\n+#include <Common/ThreadPool.h>\n+#include <Columns/ColumnString.h>\n+#include <Processors/Chunk.h>\n+\n+\n+namespace DB\n+{\n+namespace fs = std::filesystem;\n+\n+class StorageEmbeddedRocksDB;\n+class EmbeddedRocksDBBulkSink;\n+struct StorageInMemoryMetadata;\n+using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;\n+\n+/// Optimized for bulk importing into StorageEmbeddedRocksDB:\n+/// 1. No mem-table: an SST file is built from chunk, then import to rocksdb\n+/// 2. Squash chunks to reduce the number of SST files\n+class EmbeddedRocksDBBulkSink : public SinkToStorage, public WithContext\n+{\n+public:\n+    EmbeddedRocksDBBulkSink(\n+        ContextPtr context_,\n+        StorageEmbeddedRocksDB & storage_,\n+        const StorageMetadataPtr & metadata_snapshot_);\n+\n+    ~EmbeddedRocksDBBulkSink() override;\n+\n+    void consume(Chunk chunk) override;\n+\n+    void onFinish() override;\n+\n+    String getName() const override { return \"EmbeddedRocksDBBulkSink\"; }\n+\n+private:\n+    /// Get a unique path to write temporary SST file\n+    String getTemporarySSTFilePath();\n+\n+    /// Squash chunks to a minimum size\n+    std::vector<Chunk> squash(Chunk chunk);\n+    bool isEnoughSize(const std::vector<Chunk> & input_chunks) const;\n+    bool isEnoughSize(const Chunk & chunk) const;\n+    /// Serialize chunks to rocksdb key-value pairs\n+    std::pair<ColumnString::Ptr, ColumnString::Ptr> serializeChunks(const std::vector<Chunk> & input_chunks) const;\n+\n+    StorageEmbeddedRocksDB & storage;\n+    StorageMetadataPtr metadata_snapshot;\n+    size_t primary_key_pos = 0;\n+    Serializations serializations;\n+\n+    /// For squashing chunks\n+    std::vector<Chunk> chunks;\n+    size_t min_block_size_rows = 0;\n+\n+    /// For writing SST files\n+    size_t file_counter = 0;\n+    static constexpr auto TMP_INSERT_PREFIX = \"tmp_insert_\";\n+    String insert_directory_queue;\n+};\n+\n+}\ndiff --git a/src/Storages/RocksDB/RocksDBSettings.cpp b/src/Storages/RocksDB/RocksDBSettings.cpp\nnew file mode 100644\nindex 000000000000..7de2077eb471\n--- /dev/null\n+++ b/src/Storages/RocksDB/RocksDBSettings.cpp\n@@ -0,0 +1,41 @@\n+#include \"RocksDBSettings.h\"\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Parsers/ASTFunction.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int UNKNOWN_SETTING;\n+}\n+\n+IMPLEMENT_SETTINGS_TRAITS(RockDBSettingsTraits, LIST_OF_ROCKSDB_SETTINGS)\n+\n+\n+void RocksDBSettings::loadFromQuery(ASTStorage & storage_def, ContextPtr /*context*/)\n+{\n+    if (storage_def.settings)\n+    {\n+        try\n+        {\n+            auto changes = storage_def.settings->changes;\n+            applyChanges(changes);\n+        }\n+        catch (Exception & e)\n+        {\n+            if (e.code() == ErrorCodes::UNKNOWN_SETTING)\n+                e.addMessage(\"for storage \" + storage_def.engine->name);\n+            throw;\n+        }\n+    }\n+}\n+\n+std::vector<String> RocksDBSettings::getAllRegisteredNames() const\n+{\n+    std::vector<String> all_settings;\n+    for (const auto & setting_field : all())\n+        all_settings.push_back(setting_field.getName());\n+    return all_settings;\n+}\n+}\ndiff --git a/src/Storages/RocksDB/RocksDBSettings.h b/src/Storages/RocksDB/RocksDBSettings.h\nnew file mode 100644\nindex 000000000000..1b168c56d89e\n--- /dev/null\n+++ b/src/Storages/RocksDB/RocksDBSettings.h\n@@ -0,0 +1,39 @@\n+#pragma once\n+\n+#include <Core/BaseSettings.h>\n+#include <Core/Defines.h>\n+#include <Interpreters/Context_fwd.h>\n+#include <base/unit.h>\n+#include <Common/NamePrompter.h>\n+\n+\n+namespace Poco::Util\n+{\n+class AbstractConfiguration;\n+}\n+\n+\n+namespace DB\n+{\n+class ASTStorage;\n+struct Settings;\n+\n+\n+/** StorageEmbeddedRocksdb table settings\n+  */\n+\n+#define ROCKSDB_SETTINGS(M, ALIAS) \\\n+    M(Bool, optimize_for_bulk_insert, true, \"Table is optimized for bulk insertions (insert pipeline will create SST files and import to rocksdb database instead of writing to memtables)\", 0) \\\n+    M(UInt64, bulk_insert_block_size, DEFAULT_INSERT_BLOCK_SIZE, \"Size of block for bulk insert, if it's smaller than query setting min_insert_block_size_rows then it will be overridden by min_insert_block_size_rows\", 0) \\\n+\n+#define LIST_OF_ROCKSDB_SETTINGS(M, ALIAS) ROCKSDB_SETTINGS(M, ALIAS)\n+\n+DECLARE_SETTINGS_TRAITS(RockDBSettingsTraits, LIST_OF_ROCKSDB_SETTINGS)\n+\n+struct RocksDBSettings : public BaseSettings<RockDBSettingsTraits>, public IHints<2>\n+{\n+    void loadFromQuery(ASTStorage & storage_def, ContextPtr context);\n+    std::vector<String> getAllRegisteredNames() const override;\n+};\n+\n+}\ndiff --git a/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp b/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp\nindex 7c4581025e58..ad185f8ec2b7 100644\n--- a/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp\n+++ b/src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp\n@@ -1,6 +1,5 @@\n #include <Storages/checkAndGetLiteralArgument.h>\n #include <Storages/RocksDB/StorageEmbeddedRocksDB.h>\n-#include <Storages/RocksDB/EmbeddedRocksDBSink.h>\n #include <Storages/MutationCommands.h>\n \n #include <DataTypes/DataTypesNumber.h>\n@@ -28,8 +27,15 @@\n #include <Poco/Util/AbstractConfiguration.h>\n #include <Common/logger_useful.h>\n #include <Common/Exception.h>\n+#include <Storages/AlterCommands.h>\n+#include <Storages/RocksDB/RocksDBSettings.h>\n+#include <IO/SharedThreadPools.h>\n+#include <Disks/DiskLocal.h>\n #include <base/sort.h>\n \n+#include <rocksdb/advanced_options.h>\n+#include <rocksdb/env.h>\n+#include <rocksdb/options.h>\n #include <rocksdb/table.h>\n #include <rocksdb/convenience.h>\n #include <rocksdb/utilities/db_ttl.h>\n@@ -39,8 +45,6 @@\n #include <utility>\n \n \n-namespace fs = std::filesystem;\n-\n namespace DB\n {\n \n@@ -174,6 +178,7 @@ StorageEmbeddedRocksDB::StorageEmbeddedRocksDB(const StorageID & table_id_,\n         const StorageInMemoryMetadata & metadata_,\n         LoadingStrictnessLevel mode,\n         ContextPtr context_,\n+        std::unique_ptr<RocksDBSettings> settings_,\n         const String & primary_key_,\n         Int32 ttl_,\n         String rocksdb_dir_,\n@@ -186,6 +191,7 @@ StorageEmbeddedRocksDB::StorageEmbeddedRocksDB(const StorageID & table_id_,\n     , read_only(read_only_)\n {\n     setInMemoryMetadata(metadata_);\n+    setSettings(std::move(settings_));\n     if (rocksdb_dir.empty())\n     {\n         rocksdb_dir = context_->getPath() + relative_data_path_;\n@@ -236,22 +242,20 @@ void StorageEmbeddedRocksDB::mutate(const MutationCommands & commands, ContextPt\n \n     if (commands.front().type == MutationCommand::Type::DELETE)\n     {\n-        MutationsInterpreter::Settings settings(true);\n-        settings.return_all_columns = true;\n-        settings.return_mutated_rows = true;\n+        MutationsInterpreter::Settings mutation_settings(true);\n+        mutation_settings.return_all_columns = true;\n+        mutation_settings.return_mutated_rows = true;\n \n         auto interpreter = std::make_unique<MutationsInterpreter>(\n             storage_ptr,\n             metadata_snapshot,\n             commands,\n             context_,\n-            settings);\n+            mutation_settings);\n \n         auto pipeline = QueryPipelineBuilder::getPipeline(interpreter->execute());\n         PullingPipelineExecutor executor(pipeline);\n \n-        auto sink = std::make_shared<EmbeddedRocksDBSink>(*this, metadata_snapshot);\n-\n         auto header = interpreter->getUpdatedHeader();\n         auto primary_key_pos = header.getPositionByName(primary_key);\n \n@@ -287,16 +291,16 @@ void StorageEmbeddedRocksDB::mutate(const MutationCommands & commands, ContextPt\n     if (commands.front().column_to_update_expression.contains(primary_key))\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Primary key cannot be updated (cannot update column {})\", primary_key);\n \n-    MutationsInterpreter::Settings settings(true);\n-    settings.return_all_columns = true;\n-    settings.return_mutated_rows = true;\n+    MutationsInterpreter::Settings mutation_settings(true);\n+    mutation_settings.return_all_columns = true;\n+    mutation_settings.return_mutated_rows = true;\n \n     auto interpreter = std::make_unique<MutationsInterpreter>(\n         storage_ptr,\n         metadata_snapshot,\n         commands,\n         context_,\n-        settings);\n+        mutation_settings);\n \n     auto pipeline = QueryPipelineBuilder::getPipeline(interpreter->execute());\n     PullingPipelineExecutor executor(pipeline);\n@@ -352,7 +356,6 @@ void StorageEmbeddedRocksDB::initDB()\n     rocksdb::Options base;\n \n     base.create_if_missing = true;\n-    base.compression = rocksdb::CompressionType::kZSTD;\n     base.statistics = rocksdb::CreateDBStatistics();\n     /// It is too verbose by default, and in fact we don't care about rocksdb logs at all.\n     base.info_log_level = rocksdb::ERROR_LEVEL;\n@@ -582,8 +585,11 @@ void ReadFromEmbeddedRocksDB::applyFilters(ActionDAGNodes added_filter_nodes)\n }\n \n SinkToStoragePtr StorageEmbeddedRocksDB::write(\n-    const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, ContextPtr /*context*/, bool /*async_insert*/)\n+    const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, ContextPtr  query_context, bool /*async_insert*/)\n {\n+    if (getSettings().optimize_for_bulk_insert)\n+        return std::make_shared<EmbeddedRocksDBBulkSink>(query_context, *this, metadata_snapshot);\n+\n     return std::make_shared<EmbeddedRocksDBSink>(*this, metadata_snapshot);\n }\n \n@@ -622,7 +628,21 @@ static StoragePtr create(const StorageFactory::Arguments & args)\n     {\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"StorageEmbeddedRocksDB must require one column in primary key\");\n     }\n-    return std::make_shared<StorageEmbeddedRocksDB>(args.table_id, args.relative_data_path, metadata, args.mode, args.getContext(), primary_key_names[0], ttl, std::move(rocksdb_dir), read_only);\n+    auto settings = std::make_unique<RocksDBSettings>();\n+    settings->loadFromQuery(*args.storage_def, args.getContext());\n+    if (args.storage_def->settings)\n+        metadata.settings_changes = args.storage_def->settings->ptr();\n+    else\n+    {\n+        /// A workaround because embedded rocksdb doesn't have default immutable settings\n+        /// But InterpreterAlterQuery requires settings_changes to be set to run ALTER MODIFY\n+        /// SETTING queries. So we just add a setting with its default value.\n+        auto settings_changes = std::make_shared<ASTSetQuery>();\n+        settings_changes->is_standalone = false;\n+        settings_changes->changes.insertSetting(\"optimize_for_bulk_insert\", settings->optimize_for_bulk_insert.value);\n+        metadata.settings_changes = settings_changes;\n+    }\n+    return std::make_shared<StorageEmbeddedRocksDB>(args.table_id, args.relative_data_path, metadata, args.mode, args.getContext(), std::move(settings), primary_key_names[0], ttl, std::move(rocksdb_dir), read_only);\n }\n \n std::shared_ptr<rocksdb::Statistics> StorageEmbeddedRocksDB::getRocksDBStatistics() const\n@@ -713,9 +733,9 @@ Chunk StorageEmbeddedRocksDB::getBySerializedKeys(\n     return Chunk(std::move(columns), num_rows);\n }\n \n-std::optional<UInt64> StorageEmbeddedRocksDB::totalRows(const Settings & settings) const\n+std::optional<UInt64> StorageEmbeddedRocksDB::totalRows(const Settings & query_settings) const\n {\n-    if (!settings.optimize_trivial_approximate_count_query)\n+    if (!query_settings.optimize_trivial_approximate_count_query)\n         return {};\n     std::shared_lock lock(rocksdb_ptr_mx);\n     if (!rocksdb_ptr)\n@@ -737,9 +757,26 @@ std::optional<UInt64> StorageEmbeddedRocksDB::totalBytes(const Settings & /*sett\n     return estimated_bytes;\n }\n \n+void StorageEmbeddedRocksDB::alter(\n+    const AlterCommands & params,\n+    ContextPtr query_context,\n+    AlterLockHolder & holder)\n+{\n+    IStorage::alter(params, query_context, holder);\n+    auto new_metadata = getInMemoryMetadataPtr();\n+    if (new_metadata->settings_changes)\n+    {\n+        const auto & settings_changes = new_metadata->settings_changes->as<const ASTSetQuery &>();\n+        auto new_settings = std::make_unique<RocksDBSettings>();\n+        new_settings->applyChanges(settings_changes.changes);\n+        setSettings(std::move(new_settings));\n+    }\n+}\n+\n void registerStorageEmbeddedRocksDB(StorageFactory & factory)\n {\n     StorageFactory::StorageFeatures features{\n+        .supports_settings = true,\n         .supports_sort_order = true,\n         .supports_ttl = true,\n         .supports_parallel_insert = true,\n@@ -747,4 +784,12 @@ void registerStorageEmbeddedRocksDB(StorageFactory & factory)\n \n     factory.registerStorage(\"EmbeddedRocksDB\", create, features);\n }\n+\n+void StorageEmbeddedRocksDB::checkAlterIsPossible(const AlterCommands & commands, ContextPtr /* context */) const\n+{\n+    for (const auto & command : commands)\n+        if (!command.isCommentAlter() && !command.isSettingsAlter())\n+            throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Alter of type '{}' is not supported by storage {}\", command.type, getName());\n+}\n+\n }\ndiff --git a/src/Storages/RocksDB/StorageEmbeddedRocksDB.h b/src/Storages/RocksDB/StorageEmbeddedRocksDB.h\nindex 230464a161f0..9fc58ea6b381 100644\n--- a/src/Storages/RocksDB/StorageEmbeddedRocksDB.h\n+++ b/src/Storages/RocksDB/StorageEmbeddedRocksDB.h\n@@ -1,11 +1,14 @@\n #pragma once\n \n #include <memory>\n+#include <Common/MultiVersion.h>\n #include <Common/SharedMutex.h>\n-#include <Storages/IStorage.h>\n #include <Interpreters/IKeyValueEntity.h>\n #include <rocksdb/status.h>\n+#include <Storages/IStorage.h>\n #include <Storages/RocksDB/EmbeddedRocksDBSink.h>\n+#include <Storages/RocksDB/EmbeddedRocksDBBulkSink.h>\n+#include <Storages/RocksDB/RocksDBSettings.h>\n \n \n namespace rocksdb\n@@ -27,6 +30,7 @@ class Context;\n class StorageEmbeddedRocksDB final : public IStorage, public IKeyValueEntity, WithContext\n {\n     friend class EmbeddedRocksDBSink;\n+    friend class EmbeddedRocksDBBulkSink;\n     friend class ReadFromEmbeddedRocksDB;\n public:\n     StorageEmbeddedRocksDB(const StorageID & table_id_,\n@@ -34,6 +38,7 @@ class StorageEmbeddedRocksDB final : public IStorage, public IKeyValueEntity, Wi\n         const StorageInMemoryMetadata & metadata,\n         LoadingStrictnessLevel mode,\n         ContextPtr context_,\n+        std::unique_ptr<RocksDBSettings> settings_,\n         const String & primary_key_,\n         Int32 ttl_ = 0,\n         String rocksdb_dir_ = \"\",\n@@ -59,6 +64,7 @@ class StorageEmbeddedRocksDB final : public IStorage, public IKeyValueEntity, Wi\n     void checkMutationIsPossible(const MutationCommands & commands, const Settings & settings) const override;\n     void mutate(const MutationCommands &, ContextPtr) override;\n     void drop() override;\n+    void alter(const AlterCommands & params, ContextPtr query_context, AlterLockHolder &) override;\n \n     bool optimize(\n         const ASTPtr & query,\n@@ -99,7 +105,16 @@ class StorageEmbeddedRocksDB final : public IStorage, public IKeyValueEntity, Wi\n \n     std::optional<UInt64> totalBytes(const Settings & settings) const override;\n \n+    void checkAlterIsPossible(const AlterCommands & commands, ContextPtr /* context */) const override;\n+\n+    const RocksDBSettings & getSettings() const { return *storage_settings.get(); }\n+\n+    void setSettings(std::unique_ptr<RocksDBSettings> && settings_) { storage_settings.set(std::move(settings_)); }\n+\n private:\n+    SinkToStoragePtr getSink(ContextPtr context, const StorageMetadataPtr & metadata_snapshot);\n+\n+    MultiVersion<RocksDBSettings> storage_settings;\n     const String primary_key;\n     using RocksDBPtr = std::unique_ptr<rocksdb::DB>;\n     RocksDBPtr rocksdb_ptr;\ndiff --git a/utils/check-style/aspell-ignore/en/aspell-dict.txt b/utils/check-style/aspell-ignore/en/aspell-dict.txt\nindex 0fcc5dd49547..3e72754c604e 100644\n--- a/utils/check-style/aspell-ignore/en/aspell-dict.txt\n+++ b/utils/check-style/aspell-ignore/en/aspell-dict.txt\n@@ -1942,6 +1942,8 @@ mdadm\n meanZTest\n meanztest\n mebibytes\n+memtable\n+memtables\n mergeTreeIndex\n mergeable\n mergetree\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01686_rocksdb.sql b/tests/queries/0_stateless/01686_rocksdb.sql\nindex f3177ce140e1..3ff218bf3987 100644\n--- a/tests/queries/0_stateless/01686_rocksdb.sql\n+++ b/tests/queries/0_stateless/01686_rocksdb.sql\n@@ -4,7 +4,7 @@\n \n DROP TABLE IF EXISTS 01686_test;\n \n-CREATE TABLE 01686_test (key UInt64, value String) Engine=EmbeddedRocksDB PRIMARY KEY(key);\n+CREATE TABLE 01686_test (key UInt64, value String) Engine=EmbeddedRocksDB PRIMARY KEY(key) SETTINGS optimize_for_bulk_insert = 0;\n \n SELECT value FROM system.rocksdb WHERE database = currentDatabase() and table = '01686_test' and name = 'number.keys.written';\n INSERT INTO 01686_test SELECT number, format('Hello, world ({})', toString(number)) FROM numbers(10000);\ndiff --git a/tests/queries/0_stateless/02956_rocksdb_bulk_sink.reference b/tests/queries/0_stateless/02956_rocksdb_bulk_sink.reference\nnew file mode 100644\nindex 000000000000..74c71827e6ef\n--- /dev/null\n+++ b/tests/queries/0_stateless/02956_rocksdb_bulk_sink.reference\n@@ -0,0 +1,10 @@\n+0\n+1000\n+1000\n+1\n+1000\n+2\n+1000000\n+1000\n+0\t999001\n+1000000\ndiff --git a/tests/queries/0_stateless/02956_rocksdb_bulk_sink.sh b/tests/queries/0_stateless/02956_rocksdb_bulk_sink.sh\nnew file mode 100755\nindex 000000000000..8acc83fc86c1\n--- /dev/null\n+++ b/tests/queries/0_stateless/02956_rocksdb_bulk_sink.sh\n@@ -0,0 +1,48 @@\n+#!/usr/bin/env bash\n+# Tags: no-ordinary-database, use-rocksdb\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# Normal importing, as we only insert 1000 rows, so it should be in memtable\n+${CLICKHOUSE_CLIENT} --query \"CREATE TABLE IF NOT EXISTS rocksdb_worm (key UInt64, value UInt64) ENGINE = EmbeddedRocksDB() PRIMARY KEY key SETTINGS optimize_for_bulk_insert = 0;\"\n+${CLICKHOUSE_CLIENT} --query \"INSERT INTO rocksdb_worm SELECT number, number+1 FROM numbers(1000);\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT sum(value) FROM system.rocksdb WHERE database = currentDatabase() AND table = 'rocksdb_worm' AND name = 'no.file.opens';\" # should be 0 because all data is still in memtable\n+${CLICKHOUSE_CLIENT} --query \"SELECT count() FROM rocksdb_worm;\"\n+\n+# Enabling bulk insertion\n+${CLICKHOUSE_CLIENT} --query \"ALTER TABLE rocksdb_worm MODIFY SETTING optimize_for_bulk_insert = 1;\"\n+\n+# Testing that key serialization is identical w. and w/o bulk sink\n+${CLICKHOUSE_CLIENT} --query \"TRUNCATE TABLE rocksdb_worm;\"\n+${CLICKHOUSE_CLIENT} --query \"INSERT INTO rocksdb_worm SELECT number, number+2 FROM numbers(1000);\" # should override previous keys\n+${CLICKHOUSE_CLIENT} --query \"SELECT count() FROM rocksdb_worm WHERE value = key + 2;\"\n+\n+# With bulk insertion, there is no memtable, so a small insert should create a new file\n+${CLICKHOUSE_CLIENT} --query \"TRUNCATE TABLE rocksdb_worm;\"\n+${CLICKHOUSE_CLIENT} --query \"INSERT INTO rocksdb_worm SELECT number, number+1 FROM numbers(1000);\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT sum(value) FROM system.rocksdb WHERE database = currentDatabase() AND table = 'rocksdb_worm' AND name = 'no.file.opens';\" # should be 1\n+${CLICKHOUSE_CLIENT} --query \"SELECT count() FROM rocksdb_worm;\"\n+\n+# Testing insert with multiple sinks and fixed block size\n+${CLICKHOUSE_CLIENT} --query \"TRUNCATE TABLE rocksdb_worm;\"\n+${CLICKHOUSE_CLIENT} --query \"ALTER TABLE rocksdb_worm MODIFY SETTING bulk_insert_block_size = 500000;\"\n+${CLICKHOUSE_CLIENT} --query \"INSERT INTO rocksdb_worm SELECT number, number+1 FROM numbers_mt(1000000) SETTINGS max_insert_threads = 2, max_block_size = 100000;\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT sum(value) FROM system.rocksdb WHERE database = currentDatabase() AND table = 'rocksdb_worm' AND name = 'no.file.opens';\" # should be 2 as max_block_size is set to 500000\n+${CLICKHOUSE_CLIENT} --query \"SELECT count() FROM rocksdb_worm;\"\n+\n+# Testing insert with duplicated keys\n+${CLICKHOUSE_CLIENT} --query \"TRUNCATE TABLE rocksdb_worm;\"\n+${CLICKHOUSE_CLIENT} --query \"INSERT INTO rocksdb_worm SELECT number % 1000, number+1 FROM numbers_mt(1000000) SETTINGS max_block_size = 100000, max_insert_threads = 1;\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT count() FROM rocksdb_worm;\"\n+${CLICKHOUSE_CLIENT} --query \"SELECT * FROM rocksdb_worm WHERE key = 0;\" # should be the latest value - 999001\n+\n+\n+# Testing insert with multiple threads\n+${CLICKHOUSE_CLIENT} --query \"TRUNCATE TABLE rocksdb_worm;\"\n+${CLICKHOUSE_CLIENT} --query \"INSERT INTO rocksdb_worm SELECT number, number+1 FROM numbers_mt(1000000)\" &\n+${CLICKHOUSE_CLIENT} --query \"INSERT INTO rocksdb_worm SELECT number, number+1 FROM numbers_mt(1000000)\" &\n+wait\n+${CLICKHOUSE_CLIENT} --query \"SELECT count() FROM rocksdb_worm;\"\n+\n",
  "problem_statement": "[RFC] Bulk importing to EmbeddedRocksDB tables\n> (you don't have to strictly follow this form)\r\n\r\n**Use case**\r\n\r\nIn our case, EmbeddedRocksDB tables are usually used as dictionaries: we write a large amount of data once, then read many times, and only write again after a few days (or even weeks). I think many others are also using EmbeddedRocksDB this way.\r\n\r\nWe've faced multiple issues when running a long `INSERT SELECT` to EmbeddedRocksDB tables. Some worth mentioning:\r\n\r\n1. ClickHouse cannot track insert query memory properly\r\n\r\nInsertions first go to memtables. When a memtable is full, it becomes immutable and a rocksdb thread will flush immutable memtables to disk later. This means memtables are created and allocated memory by a ClickHouse thread, but then destroyed by a rocksdb thread --> memory tracker cannot track deallocation --> for a long insertion, ClickHouse memory tracker keeps growing and the query will be OOM. The issue can be reproduced easily by a long insertion (e.g. 1B rows) to any EmbeddedRocksDB table.\r\n\r\n2. Write stalls\r\n\r\nInserting is fast at first, but then becomes slower due to write stalls (flush/compaction cannot keep up). Parallel insertion also doesn't increase the insert query speed because write stalls. Although we can tune rocksdb options, TBH I think it's not trivial for normal users.\r\n\r\n**Describe the solution you'd like**\r\n\r\nPropose to have a new sink type: `EmbeddedRocksDBBulkSink`. It will not directly write to rocksdb but will convert the incoming chunk to an SST file in a temporary directory, and then import the file to rocksdb. The advantages:\r\n\r\n- Completely bypass memtables --> less overhead, more control over memory usage\r\n- With a big enough insertion block size, we don't need to worry about write stalls\r\n- Parallel insertion can scale linearly\r\n\r\nBulk loading can be enabled/disabled by a query setting.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nTune rocksdb options, but I think bulk loading will benefit more people. Rocksdb also recommends this approach for bulk loading (https://rocksdb.org/blog/2017/02/17/bulkoad-ingest-sst-file.html).\r\n\r\n**Additional context**\r\n\r\n1. I have an implementation and it works well, can see a visible gain in long-running `INSERT SELECT`\r\n2. This is a warm-up task for a bigger objective: to make `EmbeddedRocksDB` not just a wrap-around of rocksdb API but can control SST files and compactions. Then we can have `ReplicatedEmbeddedRocksDB`.\r\n\r\n\r\nAppreciate for comments @sundy-li @hanfei1991 @azat \nStorageEmbeddedRocksDB table settings\n> (you don't have to strictly follow this form)\r\n\r\n**Use case**\r\n\r\nTuning some options for read/write/flush/compact to table (e.g, WAL) \r\n\r\n**Describe the solution you'd like**\r\n\r\nSimilar to MergeTree, each `StorageEmbeddedRocksDB` has a `StorageEmbeddedRocksDBSettings` object. Settings values can be declared in `CREATE` query and modified with `ALTER` query.\r\n\r\nList of possible settings can be taken from internal rocksdb options: `rocksdb::ReadOptions`, `rocksdb::WriteOptions`, `rocksdb::CompactOptions`, etc...\r\n\r\nRead/Write/Flush/Compact will create rocksdb options from table settings for rocksdb operations.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nA global options in configure file, but these values should be specific to each table.\r\n\r\n**Additional context**\r\n\r\n> Add any other context or screenshots about the feature request here.\r\n\n",
  "hints_text": "\n",
  "created_at": "2024-01-24T14:14:59Z",
  "modified_files": [
    "docs/en/engines/table-engines/integrations/embedded-rocksdb.md",
    "src/Interpreters/IKeyValueEntity.h",
    "b/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.cpp",
    "b/src/Storages/RocksDB/EmbeddedRocksDBBulkSink.h",
    "b/src/Storages/RocksDB/RocksDBSettings.cpp",
    "b/src/Storages/RocksDB/RocksDBSettings.h",
    "src/Storages/RocksDB/StorageEmbeddedRocksDB.cpp",
    "src/Storages/RocksDB/StorageEmbeddedRocksDB.h",
    "utils/check-style/aspell-ignore/en/aspell-dict.txt"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/01686_rocksdb.sql",
    "b/tests/queries/0_stateless/02956_rocksdb_bulk_sink.reference",
    "b/tests/queries/0_stateless/02956_rocksdb_bulk_sink.sh"
  ]
}