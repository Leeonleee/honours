You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
[RFC] Bulk importing to EmbeddedRocksDB tables
> (you don't have to strictly follow this form)

**Use case**

In our case, EmbeddedRocksDB tables are usually used as dictionaries: we write a large amount of data once, then read many times, and only write again after a few days (or even weeks). I think many others are also using EmbeddedRocksDB this way.

We've faced multiple issues when running a long `INSERT SELECT` to EmbeddedRocksDB tables. Some worth mentioning:

1. ClickHouse cannot track insert query memory properly

Insertions first go to memtables. When a memtable is full, it becomes immutable and a rocksdb thread will flush immutable memtables to disk later. This means memtables are created and allocated memory by a ClickHouse thread, but then destroyed by a rocksdb thread --> memory tracker cannot track deallocation --> for a long insertion, ClickHouse memory tracker keeps growing and the query will be OOM. The issue can be reproduced easily by a long insertion (e.g. 1B rows) to any EmbeddedRocksDB table.

2. Write stalls

Inserting is fast at first, but then becomes slower due to write stalls (flush/compaction cannot keep up). Parallel insertion also doesn't increase the insert query speed because write stalls. Although we can tune rocksdb options, TBH I think it's not trivial for normal users.

**Describe the solution you'd like**

Propose to have a new sink type: `EmbeddedRocksDBBulkSink`. It will not directly write to rocksdb but will convert the incoming chunk to an SST file in a temporary directory, and then import the file to rocksdb. The advantages:

- Completely bypass memtables --> less overhead, more control over memory usage
- With a big enough insertion block size, we don't need to worry about write stalls
- Parallel insertion can scale linearly

Bulk loading can be enabled/disabled by a query setting.

**Describe alternatives you've considered**

Tune rocksdb options, but I think bulk loading will benefit more people. Rocksdb also recommends this approach for bulk loading (https://rocksdb.org/blog/2017/02/17/bulkoad-ingest-sst-file.html).

**Additional context**

1. I have an implementation and it works well, can see a visible gain in long-running `INSERT SELECT`
2. This is a warm-up task for a bigger objective: to make `EmbeddedRocksDB` not just a wrap-around of rocksdb API but can control SST files and compactions. Then we can have `ReplicatedEmbeddedRocksDB`.


Appreciate for comments @sundy-li @hanfei1991 @azat 
StorageEmbeddedRocksDB table settings
> (you don't have to strictly follow this form)

**Use case**

Tuning some options for read/write/flush/compact to table (e.g, WAL) 

**Describe the solution you'd like**

Similar to MergeTree, each `StorageEmbeddedRocksDB` has a `StorageEmbeddedRocksDBSettings` object. Settings values can be declared in `CREATE` query and modified with `ALTER` query.

List of possible settings can be taken from internal rocksdb options: `rocksdb::ReadOptions`, `rocksdb::WriteOptions`, `rocksdb::CompactOptions`, etc...

Read/Write/Flush/Compact will create rocksdb options from table settings for rocksdb operations.

**Describe alternatives you've considered**

A global options in configure file, but these values should be specific to each table.

**Additional context**

> Add any other context or screenshots about the feature request here.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
