{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 19793,
  "instance_id": "ClickHouse__ClickHouse-19793",
  "issue_numbers": [
    "18754"
  ],
  "base_commit": "10160e5adf77e0ec0182e0c46a6688780abb5824",
  "patch": "diff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp\nindex 0af115dc0b53..cafd552978ec 100644\n--- a/src/Storages/StorageS3.cpp\n+++ b/src/Storages/StorageS3.cpp\n@@ -329,7 +329,7 @@ Pipe StorageS3::read(\n             context,\n             metadata_snapshot->getColumns(),\n             max_block_size,\n-            chooseCompressionMethod(uri.endpoint, compression_method),\n+            chooseCompressionMethod(uri.key, compression_method),\n             client,\n             uri.bucket,\n             key));\n@@ -347,7 +347,7 @@ BlockOutputStreamPtr StorageS3::write(const ASTPtr & /*query*/, const StorageMet\n         format_name,\n         metadata_snapshot->getSampleBlock(),\n         global_context,\n-        chooseCompressionMethod(uri.endpoint, compression_method),\n+        chooseCompressionMethod(uri.key, compression_method),\n         client,\n         uri.bucket,\n         uri.key,\n",
  "test_patch": "diff --git a/tests/integration/test_storage_s3/test.py b/tests/integration/test_storage_s3/test.py\nindex a97493b5def0..1f445feb5a0e 100644\n--- a/tests/integration/test_storage_s3/test.py\n+++ b/tests/integration/test_storage_s3/test.py\n@@ -443,10 +443,14 @@ def test_infinite_redirect(cluster):\n         assert exception_raised\n \n \n-def test_storage_s3_get_gzip(cluster):\n+@pytest.mark.parametrize(\"extension,method\", [\n+    (\"bin\", \"gzip\"),\n+    (\"gz\", \"auto\")\n+])\n+def test_storage_s3_get_gzip(cluster, extension, method):\n     bucket = cluster.minio_bucket\n     instance = cluster.instances[\"dummy\"]\n-    filename = \"test_get_gzip.bin\"\n+    filename = f\"test_get_gzip.{extension}\"\n     name = \"test_get_gzip\"\n     data = [\n         \"Sophia Intrieri,55\",\n@@ -473,13 +477,15 @@ def test_storage_s3_get_gzip(cluster):\n     put_s3_file_content(cluster, bucket, filename, buf.getvalue())\n \n     try:\n-        run_query(instance, \"CREATE TABLE {} (name String, id UInt32) ENGINE = S3('http://{}:{}/{}/{}', 'CSV', 'gzip')\".format(\n-            name, cluster.minio_host, cluster.minio_port, bucket, filename))\n+        run_query(instance, f\"\"\"CREATE TABLE {name} (name String, id UInt32) ENGINE = S3(\n+                                    'http://{cluster.minio_host}:{cluster.minio_port}/{bucket}/{filename}',\n+                                    'CSV',\n+                                    '{method}')\"\"\")\n \n         run_query(instance, \"SELECT sum(id) FROM {}\".format(name)).splitlines() == [\"565\"]\n \n     finally:\n-        run_query(instance, \"DROP TABLE {}\".format(name))\n+        run_query(instance, f\"DROP TABLE {name}\")\n \n \n def test_storage_s3_put_uncompressed(cluster):\n@@ -515,13 +521,17 @@ def test_storage_s3_put_uncompressed(cluster):\n         uncompressed_content = get_s3_file_content(cluster, bucket, filename)\n         assert sum([ int(i.split(',')[1]) for i in uncompressed_content.splitlines() ]) == 753\n     finally:\n-        run_query(instance, \"DROP TABLE {}\".format(name))\n+        run_query(instance, f\"DROP TABLE {name}\")\n \n \n-def test_storage_s3_put_gzip(cluster):\n+@pytest.mark.parametrize(\"extension,method\", [\n+    (\"bin\", \"gzip\"),\n+    (\"gz\", \"auto\")\n+])\n+def test_storage_s3_put_gzip(cluster, extension, method):\n     bucket = cluster.minio_bucket\n     instance = cluster.instances[\"dummy\"]\n-    filename = \"test_put_gzip.bin\"\n+    filename = f\"test_put_gzip.{extension}\"\n     name = \"test_put_gzip\"\n     data = [\n         \"'Joseph Tomlinson',5\",\n@@ -541,8 +551,10 @@ def test_storage_s3_put_gzip(cluster):\n         \"'Yolanda Joseph',89\"\n     ]\n     try:\n-        run_query(instance, \"CREATE TABLE {} (name String, id UInt32) ENGINE = S3('http://{}:{}/{}/{}', 'CSV', 'gzip')\".format(\n-            name, cluster.minio_host, cluster.minio_port, bucket, filename))\n+        run_query(instance, f\"\"\"CREATE TABLE {name} (name String, id UInt32) ENGINE = S3(\n+                                    'http://{cluster.minio_host}:{cluster.minio_port}/{bucket}/{filename}',\n+                                    'CSV',\n+                                    '{method}')\"\"\")\n \n         run_query(instance, \"INSERT INTO {} VALUES ({})\".format(name, \"),(\".join(data)))\n \n@@ -553,4 +565,4 @@ def test_storage_s3_put_gzip(cluster):\n         uncompressed_content = f.read().decode()\n         assert sum([ int(i.split(',')[1]) for i in uncompressed_content.splitlines() ]) == 708\n     finally:\n-        run_query(instance, \"DROP TABLE {}\".format(name))\n+        run_query(instance, f\"DROP TABLE {name}\")\n",
  "problem_statement": "s3 table function auto compression doesn't work\n**How to reproduce**\r\nClickhouse Server 20.13\r\n```\r\nSELECT *\r\nFROM url('https://storage.yandexcloud.net/my-test-bucket-768/data.csv.gz', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')\r\nLIMIT 2\r\n\r\nQuery id: 31d85b1f-cc2a-41c4-978d-1fb09840d7b7\r\n\r\n\u250c\u2500column1\u2500\u252c\u2500column2\u2500\u252c\u2500column3\u2500\u2510\r\n\u2502       1 \u2502       2 \u2502       3 \u2502\r\n\u2502       3 \u2502       2 \u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT *\r\nFROM url('https://storage.yandexcloud.net/my-test-bucket-768/data.csv.gz', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32', auto)\r\nLIMIT 2\r\n\r\nQuery id: 34742713-7a41-49ba-a2dd-713e6ddbbc00\r\n\r\n\u250c\u2500column1\u2500\u252c\u2500column2\u2500\u252c\u2500column3\u2500\u2510\r\n\u2502       1 \u2502       2 \u2502       3 \u2502\r\n\u2502       3 \u2502       2 \u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT *\r\nFROM s3('https://storage.yandexcloud.net/my-test-bucket-768/data.csv.gz', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32', auto)\r\nLIMIT 2\r\n\r\nQuery id: 6d3975a2-07d6-48fc-8c22-5293eb42c380\r\n\r\n\r\nReceived exception from server (version 20.13.1):\r\nCode: 27. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse input: expected ',' before: '\u001f\ufffd\\b\\bpg\ufffd_\\0\u0003data.csv\\03\ufffd1\ufffd1\ufffd2\u0006\ufffd\ufffd\\\\f:@\ufffde\ufffd\u0003\ufffd\\\\\\0\ufffdV>\ufffd\u0018\\0\\0\\0'{}:\r\nRow 1:\r\nColumn 0,   name: column1, type: UInt32, ERROR: text \"<0x1F>\ufffd<BACKSPACE><BACKSPACE>pg\ufffd_<ASCII NUL><0x03>\" is not like UInt32\r\n\r\n: While executing S3.\r\n\r\n\r\nSELECT *\r\nFROM s3('https://storage.yandexcloud.net/my-test-bucket-768/data.csv.gz', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')\r\nLIMIT 2\r\n\r\nQuery id: 5fbb23a1-9893-4b1a-a4b0-ffa36225e8bb\r\n\r\n\r\nReceived exception from server (version 20.13.1):\r\nCode: 27. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse input: expected ',' before: '\u001f\ufffd\\b\\bpg\ufffd_\\0\u0003data.csv\\03\ufffd1\ufffd1\ufffd2\u0006\ufffd\ufffd\\\\f:@\ufffde\ufffd\u0003\ufffd\\\\\\0\ufffdV>\ufffd\u0018\\0\\0\\0'{}:\r\nRow 1:\r\nColumn 0,   name: column1, type: UInt32, ERROR: text \"<0x1F>\ufffd<BACKSPACE><BACKSPACE>pg\ufffd_<ASCII NUL><0x03>\" is not like UInt32\r\n\r\n: While executing S3.\r\n\r\n\r\nSELECT *\r\nFROM s3('https://storage.yandexcloud.net/my-test-bucket-768/data.csv.gz', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32', gz)\r\nLIMIT 2\r\n\r\nQuery id: 7ce193ce-fd55-4369-be85-d59c83b0dfa8\r\n\r\n\u250c\u2500column1\u2500\u252c\u2500column2\u2500\u252c\u2500column3\u2500\u2510\r\n\u2502       1 \u2502       2 \u2502       3 \u2502\r\n\u2502       3 \u2502       2 \u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\n**Expected behavior**\r\ns3 table function would recognize compression from file extension.\r\n\r\n**Additional context**\r\nhttps://github.com/ClickHouse/ClickHouse/pull/7840#issuecomment-570837458\r\n\n",
  "hints_text": "CC @apbodrov",
  "created_at": "2021-01-29T04:55:49Z",
  "modified_files": [
    "src/Storages/StorageS3.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_s3/test.py"
  ]
}