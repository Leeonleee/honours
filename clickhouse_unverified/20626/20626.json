{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 20626,
  "instance_id": "ClickHouse__ClickHouse-20626",
  "issue_numbers": [
    "18943"
  ],
  "base_commit": "e49d90405cac621c35698443d69b8a2de887a9da",
  "patch": "diff --git a/programs/server/Server.cpp b/programs/server/Server.cpp\nindex f501e182cb7f..8fa535ab179b 100644\n--- a/programs/server/Server.cpp\n+++ b/programs/server/Server.cpp\n@@ -704,7 +704,7 @@ int Server::main(const std::vector<std::string> & /*args*/)\n         config().getString(\"path\", \"\"),\n         std::move(main_config_zk_node_cache),\n         main_config_zk_changed_event,\n-        [&](ConfigurationPtr config)\n+        [&](ConfigurationPtr config, bool initial_loading)\n         {\n             Settings::checkNoSettingNamesAtTopLevel(*config, config_path);\n \n@@ -754,14 +754,19 @@ int Server::main(const std::vector<std::string> & /*args*/)\n             if (config->has(\"max_partition_size_to_drop\"))\n                 global_context->setMaxPartitionSizeToDrop(config->getUInt64(\"max_partition_size_to_drop\"));\n \n-            if (config->has(\"zookeeper\"))\n-                global_context->reloadZooKeeperIfChanged(config);\n+            if (!initial_loading)\n+            {\n+                /// We do not load ZooKeeper configuration on the first config loading\n+                /// because TestKeeper server is not started yet.\n+                if (config->has(\"zookeeper\"))\n+                    global_context->reloadZooKeeperIfChanged(config);\n \n-            global_context->reloadAuxiliaryZooKeepersConfigIfChanged(config);\n+                global_context->reloadAuxiliaryZooKeepersConfigIfChanged(config);\n+            }\n \n             global_context->updateStorageConfiguration(*config);\n         },\n-        /* already_loaded = */ true);\n+        /* already_loaded = */ false);  /// Reload it right now (initial loading)\n \n     auto & access_control = global_context->getAccessControlManager();\n     if (config().has(\"custom_settings_prefixes\"))\ndiff --git a/src/Access/UsersConfigAccessStorage.cpp b/src/Access/UsersConfigAccessStorage.cpp\nindex b3f151c30308..33efd71d0d0c 100644\n--- a/src/Access/UsersConfigAccessStorage.cpp\n+++ b/src/Access/UsersConfigAccessStorage.cpp\n@@ -518,7 +518,7 @@ void UsersConfigAccessStorage::load(\n         preprocessed_dir,\n         zkutil::ZooKeeperNodeCache(get_zookeeper_function),\n         std::make_shared<Poco::Event>(),\n-        [&](Poco::AutoPtr<Poco::Util::AbstractConfiguration> new_config)\n+        [&](Poco::AutoPtr<Poco::Util::AbstractConfiguration> new_config, bool /*initial_loading*/)\n         {\n             parseFromConfig(*new_config);\n             Settings::checkNoSettingNamesAtTopLevel(*new_config, users_config_path);\ndiff --git a/src/Common/Config/ConfigReloader.cpp b/src/Common/Config/ConfigReloader.cpp\nindex 677448e03ae4..afff08e82bb0 100644\n--- a/src/Common/Config/ConfigReloader.cpp\n+++ b/src/Common/Config/ConfigReloader.cpp\n@@ -27,7 +27,7 @@ ConfigReloader::ConfigReloader(\n     , updater(std::move(updater_))\n {\n     if (!already_loaded)\n-        reloadIfNewer(/* force = */ true, /* throw_on_error = */ true, /* fallback_to_preprocessed = */ true);\n+        reloadIfNewer(/* force = */ true, /* throw_on_error = */ true, /* fallback_to_preprocessed = */ true, /* initial_loading = */ true);\n }\n \n \n@@ -66,7 +66,7 @@ void ConfigReloader::run()\n             if (quit)\n                 return;\n \n-            reloadIfNewer(zk_changed, /* throw_on_error = */ false, /* fallback_to_preprocessed = */ false);\n+            reloadIfNewer(zk_changed, /* throw_on_error = */ false, /* fallback_to_preprocessed = */ false, /* initial_loading = */ false);\n         }\n         catch (...)\n         {\n@@ -76,7 +76,7 @@ void ConfigReloader::run()\n     }\n }\n \n-void ConfigReloader::reloadIfNewer(bool force, bool throw_on_error, bool fallback_to_preprocessed)\n+void ConfigReloader::reloadIfNewer(bool force, bool throw_on_error, bool fallback_to_preprocessed, bool initial_loading)\n {\n     std::lock_guard lock(reload_mutex);\n \n@@ -131,7 +131,7 @@ void ConfigReloader::reloadIfNewer(bool force, bool throw_on_error, bool fallbac\n \n         try\n         {\n-            updater(loaded_config.configuration);\n+            updater(loaded_config.configuration, initial_loading);\n         }\n         catch (...)\n         {\ndiff --git a/src/Common/Config/ConfigReloader.h b/src/Common/Config/ConfigReloader.h\nindex 489f062e2fec..2e4399d3c4eb 100644\n--- a/src/Common/Config/ConfigReloader.h\n+++ b/src/Common/Config/ConfigReloader.h\n@@ -27,7 +27,7 @@ class Context;\n class ConfigReloader\n {\n public:\n-    using Updater = std::function<void(ConfigurationPtr)>;\n+    using Updater = std::function<void(ConfigurationPtr, bool)>;\n \n     /** include_from_path is usually /etc/metrika.xml (i.e. value of <include_from> tag)\n       */\n@@ -46,12 +46,12 @@ class ConfigReloader\n     void start();\n \n     /// Reload immediately. For SYSTEM RELOAD CONFIG query.\n-    void reload() { reloadIfNewer(/* force */ true, /* throw_on_error */ true, /* fallback_to_preprocessed */ false); }\n+    void reload() { reloadIfNewer(/* force */ true, /* throw_on_error */ true, /* fallback_to_preprocessed */ false, /* initial_loading = */ false); }\n \n private:\n     void run();\n \n-    void reloadIfNewer(bool force, bool throw_on_error, bool fallback_to_preprocessed);\n+    void reloadIfNewer(bool force, bool throw_on_error, bool fallback_to_preprocessed, bool initial_loading);\n \n     struct FileWithTimestamp;\n \ndiff --git a/src/Databases/DatabaseAtomic.cpp b/src/Databases/DatabaseAtomic.cpp\nindex 71e0effb2d2f..e0078da57b7a 100644\n--- a/src/Databases/DatabaseAtomic.cpp\n+++ b/src/Databases/DatabaseAtomic.cpp\n@@ -108,6 +108,13 @@ StoragePtr DatabaseAtomic::detachTable(const String & name)\n \n void DatabaseAtomic::dropTable(const Context & context, const String & table_name, bool no_delay)\n {\n+    if (auto * mv = dynamic_cast<StorageMaterializedView *>(tryGetTable(table_name, context).get()))\n+    {\n+        /// Remove the inner table (if any) to avoid deadlock\n+        /// (due to attempt to execute DROP from the worker thread)\n+        mv->dropInnerTable(no_delay, context);\n+    }\n+\n     String table_metadata_path = getObjectMetadataPath(table_name);\n     String table_metadata_path_drop;\n     StoragePtr table;\n@@ -131,10 +138,7 @@ void DatabaseAtomic::dropTable(const Context & context, const String & table_nam\n     }\n     if (table->storesDataOnDisk())\n         tryRemoveSymlink(table_name);\n-    /// Remove the inner table (if any) to avoid deadlock\n-    /// (due to attempt to execute DROP from the worker thread)\n-    if (auto * mv = dynamic_cast<StorageMaterializedView *>(table.get()))\n-        mv->dropInnerTable(no_delay, context);\n+\n     /// Notify DatabaseCatalog that table was dropped. It will remove table data in background.\n     /// Cleanup is performed outside of database to allow easily DROP DATABASE without waiting for cleanup to complete.\n     DatabaseCatalog::instance().enqueueDroppedTableCleanup(table->getStorageID(), table, table_metadata_path_drop, no_delay);\ndiff --git a/src/Storages/StorageMaterializedView.cpp b/src/Storages/StorageMaterializedView.cpp\nindex 325bf3d2f742..24fce6ca8d98 100644\n--- a/src/Storages/StorageMaterializedView.cpp\n+++ b/src/Storages/StorageMaterializedView.cpp\n@@ -402,32 +402,6 @@ Strings StorageMaterializedView::getDataPaths() const\n     return {};\n }\n \n-void StorageMaterializedView::checkTableCanBeDropped() const\n-{\n-    /// Don't drop the target table if it was created manually via 'TO inner_table' statement\n-    if (!has_inner_table)\n-        return;\n-\n-    auto target_table = tryGetTargetTable();\n-    if (!target_table)\n-        return;\n-\n-    target_table->checkTableCanBeDropped();\n-}\n-\n-void StorageMaterializedView::checkPartitionCanBeDropped(const ASTPtr & partition)\n-{\n-    /// Don't drop the partition in target table if it was created manually via 'TO inner_table' statement\n-    if (!has_inner_table)\n-        return;\n-\n-    auto target_table = tryGetTargetTable();\n-    if (!target_table)\n-        return;\n-\n-    target_table->checkPartitionCanBeDropped(partition);\n-}\n-\n ActionLock StorageMaterializedView::getActionLock(StorageActionBlockType type)\n {\n     return has_inner_table ? getTargetTable()->getActionLock(type) : ActionLock{};\ndiff --git a/src/Storages/StorageMaterializedView.h b/src/Storages/StorageMaterializedView.h\nindex 94e4295cd343..5dc53a3c68f0 100644\n--- a/src/Storages/StorageMaterializedView.h\n+++ b/src/Storages/StorageMaterializedView.h\n@@ -64,9 +64,6 @@ class StorageMaterializedView final : public ext::shared_ptr_helper<StorageMater\n \n     void shutdown() override;\n \n-    void checkTableCanBeDropped() const override;\n-    void checkPartitionCanBeDropped(const ASTPtr & partition) override;\n-\n     QueryProcessingStage::Enum getQueryProcessingStage(const Context &, QueryProcessingStage::Enum /*to_stage*/, SelectQueryInfo &) const override;\n \n     StoragePtr getTargetTable() const;\n",
  "test_patch": "diff --git a/tests/integration/test_force_drop_table/__init__.py b/tests/integration/test_force_drop_table/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_force_drop_table/configs/config.xml b/tests/integration/test_force_drop_table/configs/config.xml\nnew file mode 100644\nindex 000000000000..e5f133953a6d\n--- /dev/null\n+++ b/tests/integration/test_force_drop_table/configs/config.xml\n@@ -0,0 +1,4 @@\n+<yandex>\n+    <max_table_size_to_drop>1</max_table_size_to_drop>\n+    <max_partition_size_to_drop>1</max_partition_size_to_drop>\n+</yandex>\ndiff --git a/tests/integration/test_force_drop_table/test.py b/tests/integration/test_force_drop_table/test.py\nnew file mode 100644\nindex 000000000000..ad8316493e4a\n--- /dev/null\n+++ b/tests/integration/test_force_drop_table/test.py\n@@ -0,0 +1,49 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node = cluster.add_instance('node', main_configs=[\"configs/config.xml\"], with_zookeeper=True)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+def create_force_drop_flag(node):\n+    force_drop_flag_path = \"/var/lib/clickhouse/flags/force_drop_table\"\n+    node.exec_in_container([\"bash\", \"-c\", \"touch {} && chmod a=rw {}\".format(force_drop_flag_path, force_drop_flag_path)], user=\"root\")\n+\n+@pytest.mark.parametrize(\"engine\", ['Ordinary', 'Atomic'])\n+def test_drop_materialized_view(started_cluster, engine):\n+    node.query(\"CREATE DATABASE d ENGINE={}\".format(engine))\n+    node.query(\"CREATE TABLE d.rmt (n UInt64) ENGINE=ReplicatedMergeTree('/test/rmt', 'r1') ORDER BY n PARTITION BY n % 2\")\n+    node.query(\"CREATE MATERIALIZED VIEW d.mv (n UInt64, s String) ENGINE=MergeTree ORDER BY n PARTITION BY n % 2 AS SELECT n, toString(n) AS s FROM d.rmt\")\n+    node.query(\"INSERT INTO d.rmt VALUES (1), (2)\")\n+    assert \"is greater than max\" in node.query_and_get_error(\"DROP TABLE d.rmt\")\n+    assert \"is greater than max\" in node.query_and_get_error(\"DROP TABLE d.mv\")\n+    assert \"is greater than max\" in node.query_and_get_error(\"TRUNCATE TABLE d.rmt\")\n+    assert \"is greater than max\" in node.query_and_get_error(\"TRUNCATE TABLE d.mv\")\n+    assert \"is greater than max\" in node.query_and_get_error(\"ALTER TABLE d.rmt DROP PARTITION '0'\")\n+    assert node.query(\"SELECT * FROM d.rmt ORDER BY n\") == \"1\\n2\\n\"\n+    assert node.query(\"SELECT * FROM d.mv ORDER BY n\") == \"1\\t1\\n2\\t2\\n\"\n+\n+    create_force_drop_flag(node)\n+    node.query(\"ALTER TABLE d.rmt DROP PARTITION '0'\")\n+    assert node.query(\"SELECT * FROM d.rmt ORDER BY n\") == \"1\\n\"\n+    assert \"is greater than max\" in node.query_and_get_error(\"ALTER TABLE d.mv DROP PARTITION '0'\")\n+    create_force_drop_flag(node)\n+    node.query(\"ALTER TABLE d.mv DROP PARTITION '0'\")\n+    assert node.query(\"SELECT * FROM d.mv ORDER BY n\") == \"1\\t1\\n\"\n+    assert \"is greater than max\" in node.query_and_get_error(\"DROP TABLE d.rmt SYNC\")\n+    create_force_drop_flag(node)\n+    node.query(\"DROP TABLE d.rmt SYNC\")\n+    assert \"is greater than max\" in node.query_and_get_error(\"DROP TABLE d.mv SYNC\")\n+    create_force_drop_flag(node)\n+    node.query(\"DROP TABLE d.mv SYNC\")\n+    node.query(\"DROP DATABASE d\")\n+\ndiff --git a/tests/integration/test_reload_auxiliary_zookeepers/test.py b/tests/integration/test_reload_auxiliary_zookeepers/test.py\nindex 92c66c890fc3..1b14408bc12a 100644\n--- a/tests/integration/test_reload_auxiliary_zookeepers/test.py\n+++ b/tests/integration/test_reload_auxiliary_zookeepers/test.py\n@@ -62,8 +62,7 @@ def test_reload_auxiliary_zookeepers(start_cluster):\n </yandex>\"\"\"\n     node.replace_config(\"/etc/clickhouse-server/conf.d/zookeeper.xml\", new_config)\n \n-    # Hopefully it has finished the configuration reload\n-    time.sleep(2)\n+    node.query(\"SYSTEM RELOAD CONFIG\")\n \n     node.query(\n         \"ALTER TABLE simple2 FETCH PARTITION '2020-08-27' FROM 'zookeeper2:/clickhouse/tables/0/simple';\"\n@@ -81,7 +80,7 @@ def test_reload_auxiliary_zookeepers(start_cluster):\n     </zookeeper>\n </yandex>\"\"\"\n     node.replace_config(\"/etc/clickhouse-server/conf.d/zookeeper.xml\", new_config)\n-    time.sleep(2)\n+    node.query(\"SYSTEM RELOAD CONFIG\")\n     with pytest.raises(QueryRuntimeException):\n         node.query(\n             \"ALTER TABLE simple2 FETCH PARTITION '2020-08-27' FROM 'zookeeper2:/clickhouse/tables/0/simple';\"\n",
  "problem_statement": "force_drop_table does not work with MATERIALIZED VIEW\nmaster (21.2.1.1)\r\n\r\n#### Repro:\r\n\r\n1. Set in `config.xml`\r\n```xml\r\n    <max_table_size_to_drop>1</max_table_size_to_drop>\r\n    <max_partition_size_to_drop>1</max_partition_size_to_drop>\r\n```\r\n\r\n2. Create table and MV:\r\n```sql\r\ncreate table t (x UInt64) engine = Null;\r\nCREATE MATERIALIZED VIEW t_mv engine = SummingMergeTree order by y as select bitAnd(x, 15) as y, sum(x) as s from t group by y;\r\ninsert into t select number from numbers(100);\r\n```\r\n\r\nDrop table should fail (it's ok):\r\n```sql\r\n:) drop table t_mv\r\n\r\nDROP TABLE t_mv\r\n\r\nQuery id: 2a6ade12-9813-458e-8436-02e3d93714f6\r\n\r\n\r\nReceived exception from server (version 21.2.1):\r\nCode: 359. DB::Exception: Received from localhost:9000. DB::Exception: Table or Partition in default.`.inner.t_mv` was not dropped.\r\nReason:\r\n1. Size (288.00 B) is greater than max_[table/partition]_size_to_drop (1.00 B)\r\n2. File '/home/nik-kochetov/test/clickhouse/flags/force_drop_table' intended to force DROP doesn't exist\r\nHow to fix this:\r\n1. Either increase (or set to zero) max_[table/partition]_size_to_drop in server config\r\n2. Either create forcing file /home/nik-kochetov/test/clickhouse/flags/force_drop_table and make sure that ClickHouse has write permission for it.\r\nExample:\r\nsudo touch '/home/nik-kochetov/test/clickhouse/flags/force_drop_table' && sudo chmod 666 '/home/nik-kochetov/test/clickhouse/flags/force_drop_table'. \r\n\r\n0 rows in set. Elapsed: 0.012 sec. \r\n```\r\n\r\n3. Create `force_drop_table`\r\n```sh\r\n$ sudo touch '/home/nik-kochetov/test/clickhouse/flags/force_drop_table' && sudo chmod 666 '/home/nik-kochetov/test/clickhouse/flags/force_drop_table'\r\n$ ls -lah /home/nik-kochetov/test/clickhouse/flags/force_drop_table\r\n-rw-rw-rw- 1 root root 0 Jan 11 17:20 /home/nik-kochetov/test/clickhouse/flags/force_drop_table\r\n```\r\n\r\n4. Drop for MV:\r\n```sql\r\n:) drop table t_mv\r\n\r\nDROP TABLE t_mv\r\n\r\nQuery id: a9e01d6e-d0fc-46bc-b80c-170d809d3869\r\n\r\n\r\nReceived exception from server (version 21.2.1):\r\nCode: 359. DB::Exception: Received from localhost:9000. DB::Exception: Table or Partition in default.`.inner.t_mv` was not dropped.\r\nReason:\r\n1. Size (288.00 B) is greater than max_[table/partition]_size_to_drop (1.00 B)\r\n2. File '/home/nik-kochetov/test/clickhouse/flags/force_drop_table' intended to force DROP doesn't exist\r\nHow to fix this:\r\n1. Either increase (or set to zero) max_[table/partition]_size_to_drop in server config\r\n2. Either create forcing file /home/nik-kochetov/test/clickhouse/flags/force_drop_table and make sure that ClickHouse has write permission for it.\r\nExample:\r\nsudo touch '/home/nik-kochetov/test/clickhouse/flags/force_drop_table' && sudo chmod 666 '/home/nik-kochetov/test/clickhouse/flags/force_drop_table'. \r\n\r\n0 rows in set. Elapsed: 0.015 sec. \r\n```\r\n\r\nResult: `force_drop_table` file is removed, MV is not.\r\n\r\n#### Workaround\r\n\r\n1. Create `force_drop_table` again\r\n2. Drop inner table\r\n```sql\r\n:) drop table `.inner.t_mv`\r\n\r\nDROP TABLE `.inner.t_mv`\r\n\r\nQuery id: 59fba49d-a71e-41b0-87a5-8db03618372f\r\n\r\nOk.\r\n```\r\n3. Drop MV\r\n```sql\r\n:) drop table t_mv\r\n\r\nDROP TABLE t_mv\r\n\r\nQuery id: eaff07df-20bf-4031-bcac-088ab403474a\r\n\r\nOk.\r\n```\n",
  "hints_text": "",
  "created_at": "2021-02-17T14:36:58Z",
  "modified_files": [
    "programs/server/Server.cpp",
    "src/Access/UsersConfigAccessStorage.cpp",
    "src/Common/Config/ConfigReloader.cpp",
    "src/Common/Config/ConfigReloader.h",
    "src/Databases/DatabaseAtomic.cpp",
    "src/Storages/StorageMaterializedView.cpp",
    "src/Storages/StorageMaterializedView.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_force_drop_table/configs/config.xml",
    "b/tests/integration/test_force_drop_table/test.py",
    "tests/integration/test_reload_auxiliary_zookeepers/test.py"
  ]
}