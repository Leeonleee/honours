{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 45681,
  "instance_id": "ClickHouse__ClickHouse-45681",
  "issue_numbers": [
    "46084"
  ],
  "base_commit": "f10e82355e2e44f858baf692599a20674b0af0d9",
  "patch": "diff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex 9d0252bd6258..ea1fd209a20b 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -110,8 +110,6 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     virtual bool isStoredOnRemoteDiskWithZeroCopySupport() const = 0;\n \n-    virtual bool supportsVerticalMerge() const { return false; }\n-\n     /// NOTE: Returns zeros if column files are not found in checksums.\n     /// Otherwise return information about column size on disk.\n     ColumnSize getColumnSize(const String & column_name) const;\ndiff --git a/src/Storages/MergeTree/MergeTask.cpp b/src/Storages/MergeTree/MergeTask.cpp\nindex 5874c257ad0b..02165d244ff4 100644\n--- a/src/Storages/MergeTree/MergeTask.cpp\n+++ b/src/Storages/MergeTree/MergeTask.cpp\n@@ -953,10 +953,10 @@ MergeAlgorithm MergeTask::ExecuteAndFinalizeHorizontalPart::chooseMergeAlgorithm\n         return MergeAlgorithm::Horizontal;\n     if (ctx->need_remove_expired_values)\n         return MergeAlgorithm::Horizontal;\n-\n-    for (const auto & part : global_ctx->future_part->parts)\n-        if (!part->supportsVerticalMerge() || !isFullPartStorage(part->getDataPartStorage()))\n-            return MergeAlgorithm::Horizontal;\n+    if (global_ctx->future_part->part_format.part_type != MergeTreeDataPartType::Wide)\n+        return MergeAlgorithm::Horizontal;\n+    if (global_ctx->future_part->part_format.storage_type != MergeTreeDataPartStorageType::Full)\n+        return MergeAlgorithm::Horizontal;\n \n     bool is_supported_storage =\n         ctx->merging_params.mode == MergeTreeData::MergingParams::Ordinary ||\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.h b/src/Storages/MergeTree/MergeTreeDataPartWide.h\nindex 0d68334a6233..3f0d52d831eb 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.h\n@@ -47,8 +47,6 @@ class MergeTreeDataPartWide : public IMergeTreeDataPart\n \n     bool isStoredOnRemoteDiskWithZeroCopySupport() const override;\n \n-    bool supportsVerticalMerge() const override { return true; }\n-\n     String getFileNameForColumn(const NameAndTypePair & column) const override;\n \n     ~MergeTreeDataPartWide() override;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\nindex 1f40177d0fa8..1dec7c2cd7c6 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n@@ -135,6 +135,7 @@ static size_t computeIndexGranularityImpl(\n {\n     size_t rows_in_block = block.rows();\n     size_t index_granularity_for_block;\n+\n     if (!can_use_adaptive_index_granularity)\n     {\n         index_granularity_for_block = fixed_index_granularity_rows;\n@@ -143,7 +144,9 @@ static size_t computeIndexGranularityImpl(\n     {\n         size_t block_size_in_memory = block.bytes();\n         if (blocks_are_granules)\n+        {\n             index_granularity_for_block = rows_in_block;\n+        }\n         else if (block_size_in_memory >= index_granularity_bytes)\n         {\n             size_t granules_in_block = block_size_in_memory / index_granularity_bytes;\n@@ -155,10 +158,14 @@ static size_t computeIndexGranularityImpl(\n             index_granularity_for_block = index_granularity_bytes / size_of_row_in_bytes;\n         }\n     }\n-    /// We should be less or equal than fixed index granularity\n-    index_granularity_for_block = std::min(fixed_index_granularity_rows, index_granularity_for_block);\n \n-    /// very rare case when index granularity bytes less then single row\n+    /// We should be less or equal than fixed index granularity.\n+    /// But if block size is a granule size then do not adjust it.\n+    /// Granularity greater than fixed granularity might come from compact part.\n+    if (!blocks_are_granules)\n+        index_granularity_for_block = std::min(fixed_index_granularity_rows, index_granularity_for_block);\n+\n+    /// Very rare case when index granularity bytes less than single row.\n     if (index_granularity_for_block == 0)\n         index_granularity_for_block = 1;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex cce459c1ba87..f9fe6f2c8ab2 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -484,15 +484,8 @@ void MergeTreeDataPartWriterWide::validateColumnOfFixedSize(const NameAndTypePai\n                             column->size(), mark_num, index_granularity.getMarksCount(), index_granularity_rows);\n         }\n \n-        if (index_granularity_rows > data_part->index_granularity_info.fixed_index_granularity)\n-        {\n-            throw Exception(ErrorCodes::LOGICAL_ERROR,\n-                            \"Mark #{} has {} rows, but max fixed granularity is {}, index granularity size {}\",\n-                            mark_num, index_granularity_rows, data_part->index_granularity_info.fixed_index_granularity,\n-                            index_granularity.getMarksCount());\n-        }\n-\n         if (index_granularity_rows != index_granularity.getMarkRows(mark_num))\n+        {\n             throw Exception(\n                             ErrorCodes::LOGICAL_ERROR,\n                             \"Incorrect mark rows for part {} for mark #{}\"\n@@ -501,6 +494,7 @@ void MergeTreeDataPartWriterWide::validateColumnOfFixedSize(const NameAndTypePai\n                             mark_num, offset_in_compressed_file, offset_in_decompressed_block,\n                             index_granularity.getMarkRows(mark_num), index_granularity_rows,\n                             index_granularity.getMarksCount());\n+        }\n \n         auto column = type->createColumn();\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02539_vertical_merge_compact_parts.reference b/tests/queries/0_stateless/02539_vertical_merge_compact_parts.reference\nnew file mode 100644\nindex 000000000000..685d3f3140d1\n--- /dev/null\n+++ b/tests/queries/0_stateless/02539_vertical_merge_compact_parts.reference\n@@ -0,0 +1,2 @@\n+1\t2\tMergeParts\tHorizontal\tCompact\n+1\t3\tMergeParts\tVertical\tWide\ndiff --git a/tests/queries/0_stateless/02539_vertical_merge_compact_parts.sql b/tests/queries/0_stateless/02539_vertical_merge_compact_parts.sql\nnew file mode 100644\nindex 000000000000..4140a73b8bc5\n--- /dev/null\n+++ b/tests/queries/0_stateless/02539_vertical_merge_compact_parts.sql\n@@ -0,0 +1,41 @@\n+DROP TABLE IF EXISTS t_compact_vertical_merge;\n+\n+CREATE TABLE t_compact_vertical_merge (id UInt64, s LowCardinality(String), arr Array(UInt64))\n+ENGINE MergeTree ORDER BY id\n+SETTINGS\n+    index_granularity = 16,\n+    min_bytes_for_wide_part = 0,\n+    min_rows_for_wide_part = 100,\n+    vertical_merge_algorithm_min_rows_to_activate = 1,\n+    vertical_merge_algorithm_min_columns_to_activate = 1;\n+\n+INSERT INTO t_compact_vertical_merge SELECT number, toString(number), range(number % 10) FROM numbers(40);\n+INSERT INTO t_compact_vertical_merge SELECT number, toString(number), range(number % 10) FROM numbers(40);\n+\n+OPTIMIZE TABLE t_compact_vertical_merge FINAL;\n+SYSTEM FLUSH LOGS;\n+\n+WITH splitByChar('_', part_name) AS name_parts,\n+    name_parts[2]::UInt64 AS min_block,\n+    name_parts[3]::UInt64 AS max_block\n+SELECT min_block, max_block, event_type, merge_algorithm, part_type FROM system.part_log\n+WHERE\n+    database = currentDatabase() AND\n+    table = 't_compact_vertical_merge' AND\n+    min_block = 1 AND max_block = 2;\n+\n+INSERT INTO t_compact_vertical_merge SELECT number, toString(number), range(number % 10) FROM numbers(40);\n+\n+OPTIMIZE TABLE t_compact_vertical_merge FINAL;\n+SYSTEM FLUSH LOGS;\n+\n+WITH splitByChar('_', part_name) AS name_parts,\n+    name_parts[2]::UInt64 AS min_block,\n+    name_parts[3]::UInt64 AS max_block\n+SELECT min_block, max_block, event_type, merge_algorithm, part_type FROM system.part_log\n+WHERE\n+    database = currentDatabase() AND\n+    table = 't_compact_vertical_merge' AND\n+    min_block = 1 AND max_block = 3;\n+\n+DROP TABLE t_compact_vertical_merge;\n",
  "problem_statement": "Make it possible to use Vertical merge algorithm together with parts of Compact type\n**Use case**\r\n\r\nThere is a special algorithm inside ClickHouse for executing background merges called Vertical. It merges only PRIMARY KEY columns first and computes a permutation mask (in what order rows from all source parts should be fetched and written to the result part to get the right result). This algorithm allows ClickHouse server to consume a lot less memory in comparison with default and naive algorithm called Horizontal.  \r\n\r\nUnfortunately, ClickHouse may want to assign a merge operation for parts in a Compact format together with parts in a Wide format. In this case this background operation will be executed using Horizonal algorithm and thus may consume a lot of memory.\r\n\r\n**Describe the solution you'd like**\r\n\r\nAllow using Vertical merge algorithm with parts in Compact format.\r\n\r\n\r\n\n",
  "hints_text": "",
  "created_at": "2023-01-26T22:19:04Z",
  "modified_files": [
    "src/Storages/MergeTree/IMergeTreeDataPart.h",
    "src/Storages/MergeTree/MergeTask.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWide.h",
    "src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/02539_vertical_merge_compact_parts.reference",
    "b/tests/queries/0_stateless/02539_vertical_merge_compact_parts.sql"
  ]
}