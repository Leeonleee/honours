{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 1438,
  "instance_id": "ClickHouse__ClickHouse-1438",
  "issue_numbers": [
    "1404"
  ],
  "base_commit": "9cf14d2c61c6ac74ec6d8761855a41cadc5b909e",
  "patch": "diff --git a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp\nindex a6824354c06c..194b4dbfc391 100644\n--- a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp\n+++ b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp\n@@ -359,17 +359,32 @@ void DistributedBlockOutputStream::writeSplitAsync(const Block & block)\n void DistributedBlockOutputStream::writeAsyncImpl(const Block & block, const size_t shard_id)\n {\n     const auto & shard_info = cluster->getShardsInfo()[shard_id];\n-    if (shard_info.getLocalNodeCount() > 0)\n-        writeToLocal(block, shard_info.getLocalNodeCount());\n \n     if (shard_info.hasInternalReplication())\n-        writeToShard(block, {shard_info.dir_name_for_internal_replication});\n+    {\n+        if (shard_info.getLocalNodeCount() > 0)\n+        {\n+            /// Prefer insert into current instance directly\n+            writeToLocal(block, shard_info.getLocalNodeCount());\n+        }\n+        else\n+        {\n+            if (shard_info.dir_name_for_internal_replication.empty())\n+                throw Exception(\"Directory name for async inserts is empty, table \" + storage.getTableName(), ErrorCodes::LOGICAL_ERROR);\n+\n+            writeToShard(block, {shard_info.dir_name_for_internal_replication});\n+        }\n+    }\n     else\n     {\n+        if (shard_info.getLocalNodeCount() > 0)\n+            writeToLocal(block, shard_info.getLocalNodeCount());\n+\n         std::vector<std::string> dir_names;\n         for (const auto & address : cluster->getShardsAddresses()[shard_id])\n             if (!address.is_local)\n                 dir_names.push_back(address.toStringFull());\n+\n         if (!dir_names.empty())\n             writeToShard(block, dir_names);\n     }\n",
  "test_patch": "diff --git a/dbms/tests/integration/helpers/client.py b/dbms/tests/integration/helpers/client.py\nindex 1d63765f3247..1fa2b7c76439 100644\n--- a/dbms/tests/integration/helpers/client.py\n+++ b/dbms/tests/integration/helpers/client.py\n@@ -11,11 +11,11 @@ def __init__(self, host, port=9000, command='/usr/bin/clickhouse-client'):\n         self.command = [command, '--host', self.host, '--port', str(self.port), '--stacktrace']\n \n \n-    def query(self, sql, stdin=None, timeout=None):\n-        return self.get_query_request(sql, stdin, timeout).get_answer()\n+    def query(self, sql, stdin=None, timeout=None, settings=None):\n+        return self.get_query_request(sql, stdin=stdin, timeout=timeout, settings=settings).get_answer()\n \n \n-    def get_query_request(self, sql, stdin=None, timeout=None):\n+    def get_query_request(self, sql, stdin=None, timeout=None, settings=None):\n         command = self.command[:]\n \n         if stdin is None:\n@@ -24,6 +24,10 @@ def get_query_request(self, sql, stdin=None, timeout=None):\n         else:\n             command += ['--query', sql]\n \n+        if settings is not None:\n+            for setting, value in settings.iteritems():\n+                command += ['--' + setting, str(value)]\n+\n         return CommandRequest(command, stdin, timeout)\n \n \ndiff --git a/dbms/tests/integration/test_distributed_ddl/test.py b/dbms/tests/integration/test_distributed_ddl/test.py\nindex 0b529bb0a442..4ae27782d393 100755\n--- a/dbms/tests/integration/test_distributed_ddl/test.py\n+++ b/dbms/tests/integration/test_distributed_ddl/test.py\n@@ -32,8 +32,8 @@ def ddl_check_query(instance, query, num_hosts=None):\n     return contents\n \n def ddl_check_there_are_no_dublicates(instance):\n-    rows = instance.query(\"SELECT max(c), argMax(q, c) FROM (SELECT lower(query) AS q, count() AS c FROM system.query_log WHERE type=2 AND q LIKE '/*ddl_entry=query-%' GROUP BY query)\")\n-    assert len(rows) == 0 or rows[0][0] == \"1\", \"dublicates on {} {}, query {}\".format(instance.name, instance.ip_address)\n+    rows = instance.query(\"SELECT max(c), argMax(q, c) FROM (SELECT lower(query) AS q, count() AS c FROM system.query_log WHERE type=2 AND q LIKE '/* ddl_entry=query-%' GROUP BY query)\")\n+    assert len(rows) > 0 and rows[0][0] == \"1\", \"dublicates on {} {}, query {}\".format(instance.name, instance.ip_address)\n \n # Make retries in case of UNKNOWN_STATUS_OF_INSERT or zkutil::KeeperException errors\n def insert_reliable(instance, query_insert):\ndiff --git a/dbms/tests/integration/test_sync_insert_into_distributed/__init__.py b/dbms/tests/integration/test_insert_into_distributed_sync_async/__init__.py\nsimilarity index 100%\nrename from dbms/tests/integration/test_sync_insert_into_distributed/__init__.py\nrename to dbms/tests/integration/test_insert_into_distributed_sync_async/__init__.py\ndiff --git a/dbms/tests/integration/test_sync_insert_into_distributed/configs/remote_servers.xml b/dbms/tests/integration/test_insert_into_distributed_sync_async/configs/remote_servers.xml\nsimilarity index 54%\nrename from dbms/tests/integration/test_sync_insert_into_distributed/configs/remote_servers.xml\nrename to dbms/tests/integration/test_insert_into_distributed_sync_async/configs/remote_servers.xml\nindex 3593cbd7f36a..ab0899c0d79a 100644\n--- a/dbms/tests/integration/test_sync_insert_into_distributed/configs/remote_servers.xml\n+++ b/dbms/tests/integration/test_insert_into_distributed_sync_async/configs/remote_servers.xml\n@@ -1,5 +1,6 @@\n <yandex>\n     <remote_servers>\n+\n         <test_cluster>\n             <shard>\n                 <replica>\n@@ -12,5 +13,16 @@\n                 </replica>\n             </shard>\n         </test_cluster>\n+\n+        <local_shard_with_internal_replication>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>127.0.0.1</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </local_shard_with_internal_replication>\n+\n     </remote_servers>\n </yandex>\ndiff --git a/dbms/tests/integration/test_sync_insert_into_distributed/test.py b/dbms/tests/integration/test_insert_into_distributed_sync_async/test.py\nsimilarity index 79%\nrename from dbms/tests/integration/test_sync_insert_into_distributed/test.py\nrename to dbms/tests/integration/test_insert_into_distributed_sync_async/test.py\nindex f686dd0b68e6..5b4a76bd3b1d 100644\n--- a/dbms/tests/integration/test_sync_insert_into_distributed/test.py\n+++ b/dbms/tests/integration/test_insert_into_distributed_sync_async/test.py\n@@ -1,5 +1,6 @@\n from contextlib import contextmanager\n from helpers.network import PartitionManager\n+from helpers.test_tools import TSV\n \n import pytest\n \n@@ -77,6 +78,20 @@ def test_insertion_sync_with_disabled_timeout(started_cluster):\n         INSERT INTO distributed_table SELECT today() as date, number as val FROM system.numbers''', timeout=1)\n \n \n+def test_async_inserts_into_local_shard(started_cluster):\n+    node1.query('''CREATE TABLE shard_local (i Int64) ENGINE = Memory''')\n+    node1.query('''CREATE TABLE shard_distributed (i Int64) ENGINE = Distributed(local_shard_with_internal_replication, default, shard_local)''')\n+    node1.query('''INSERT INTO shard_distributed VALUES (1)''', settings={ \"insert_distributed_sync\" : 0 })\n+\n+    assert TSV(node1.query('''SELECT count() FROM shard_distributed''')) == TSV(\"1\\n\")\n+    node1.query('''DETACH TABLE shard_distributed''')\n+    node1.query('''ATTACH TABLE shard_distributed''')\n+    assert TSV(node1.query('''SELECT count() FROM shard_distributed''')) == TSV(\"1\\n\")\n+\n+    node1.query('''DROP TABLE shard_distributed''')\n+    node1.query('''DROP TABLE shard_local''')\n+\n+\n if __name__ == '__main__':\n     with contextmanager(started_cluster)() as cluster:\n         for name, instance in cluster.instances.items():\n",
  "problem_statement": "strange error after each update on stable branch\nHello,\r\n\r\nEvery time when I do an update on stable branch I get the following error when I start the new version of the server:\r\n\r\n```sh\r\n2017.10.25 13:36:19.208621 [ 1 ] <Warning> Application: Logging to console\r\n2017.10.25 13:36:19.220839 [ 1 ] <Information> : Starting daemon with revision 54304\r\n2017.10.25 13:36:19.220996 [ 1 ] <Information> Application: starting up\r\n2017.10.25 13:36:19.301727 [ 1 ] <Warning> ConfigProcessor: Include not found: networks\r\n2017.10.25 13:36:19.301771 [ 1 ] <Warning> ConfigProcessor: Include not found: networks\r\n2017.10.25 13:36:19.302847 [ 1 ] <Information> Application: Loading metadata.\r\n2017.10.25 13:36:19.303249 [ 1 ] <Information> DatabaseOrdinary (system): Total 1 tables.\r\n2017.10.25 13:36:19.305420 [ 1 ] <Information> BackgroundProcessingPool: Create BackgroundProcessingPool with 16 threads\r\n2017.10.25 13:36:19.309087 [ 1 ] <Information> DatabaseOrdinary (system): Starting up tables.\r\n2017.10.25 13:36:19.309797 [ 1 ] <Information> DatabaseOrdinary (default): Total 0 tables.\r\n2017.10.25 13:36:19.309821 [ 1 ] <Information> DatabaseOrdinary (default): Starting up tables.\r\n2017.10.25 13:36:19.310006 [ 1 ] <Information> DatabaseOrdinary (f5c00ace4777417f96b336d50a8c40f1): Total 2 tables.\r\n2017.10.25 13:36:19.402237 [ 1 ] <Information> DatabaseOrdinary (f5c00ace4777417f96b336d50a8c40f1): Starting up tables.\r\n2017.10.25 13:36:20.278295 [ 1 ] <Information> ~ZooKeeper: Closing ZooKeeper session\r\n2017.10.25 13:36:20.278636 [ 1 ] <Information> ~ZooKeeper: Removing 0 watches\r\n2017.10.25 13:36:20.278682 [ 1 ] <Information> ~ZooKeeper: Removed watches\r\n2017.10.25 13:36:20.279654 [ 1 ] <Error> Application: DB::Exception: Shard address 'tmp' does not match to 'user[:password]@host:port#default_database' pattern\r\n2017.10.25 13:36:20.279701 [ 1 ] <Information> Application: shutting down\r\n2017.10.25 13:36:20.279775 [ 3 ] <Information> BaseDaemon: Stop SignalListener thread\r\n```\r\n\r\nIn order to be able to start the node again I have to delete the database and all related files. When I go in production unfortunately this is no longer a solution.\r\n\r\nSilviu\n",
  "hints_text": "<img width=\"1024\" alt=\"ckhouse\" src=\"https://user-images.githubusercontent.com/4814731/31995567-a2c5c862-b98d-11e7-82c5-066af6084f07.png\">\r\n\r\nProblem seems related to the tmp folder inside the distributed table. Once deleted node is starting properly.\r\n\r\nTable definition is as follow:\r\n\r\n```sql\r\nCREATE TABLE f5c00ace4777417f96b336d50a8c40f1.test_nested2 (\r\n  array_boolean Array(Nullable(UInt8)),\r\n  array_float Array(Nullable(Float64)),\r\n  array_int Array(Nullable(Int64)),\r\n  array_string Array(Nullable(String)),\r\n  created_at DateTime,  \r\n  partition Date,  \r\n  timestamp DateTime) ENGINE = Distributed(cluster_local, 'f5c00ace4777417f96b336d50a8c40f1', 'test_nested2_src', rand())\r\n```\r\nSilviu\nHi!\r\nSomebody has already reported the same problem.\r\nUnfortunately, in that case, we were not able to investigate the problem due to lack of data, so your information will be helpful for the investigation.\r\n\r\nCould you show the creation time of `tmp` directory?\r\nIs this time similar to the shutdown or startup time of the server?\r\nCould you provide server logs at the creation time?\r\n\r\n\r\n\nHello @ludv1x  I will try to come up with more details and debug closely . It's 3rd time when happens so for sure I can grab the data you need.\r\n\r\nSilviu\nHere you can find the steps to replicate (the tmp folder is created at first insert in the distributed table) and never deleted. Once server is restarted is never starting until you go and delete that folder:\r\n\r\n```sql\r\nCREATE database f5c00ace4777417f96b336d50a8c40f1;\r\n\r\nCREATE TABLE test_nested5_src(\r\n\t`created_at` DateTime,\r\n\t`partition` Date,\r\n\t`timestamp` DateTime,\r\n\t`float_pos` Nullable(Float64),\r\n\t`integer_neg` Nullable(Int64),\r\n\t`integer_pos` Nullable(Int64),\r\n\t`ip_address` Nullable(String),\r\n\t`null_field` Nullable(String),\r\n\t`string` Nullable(String)\r\n) ENGINE = ReplicatedMergeTree('/var/lib/clickhouse/tables/{shard}/f5c00ace4777417f96b336d50a8c40f1/test_nested5_src', '{replica}', `partition`, `timestamp`, 8192);\r\n\r\nCREATE TABLE test_nested5 AS test_nested5_src ENGINE = Distributed(cluster_local, f5c00ace4777417f96b336d50a8c40f1, test_nested5_src, rand());\r\n\r\nINSERT INTO f5c00ace4777417f96b336d50a8c40f1.test_nested5(`string`, `null_field`, `ip_address`, `integer_pos`, `integer_neg`, `float_pos`, `timestamp`, `partition`,`created_at`) \r\nVALUES ('string',NULL,'91.231.138.140',1234,-345,1345.23,1509017026,17465,1509017026);\r\n ```\r\n\r\nLet me know if you can replicate with my steps. If no I will provide you also the configs\r\n\r\nSilviu\nI can't reproduce the problem. I do the following steps:\r\n```\r\nCREATE database IF NOT EXISTS f5c00ace4777417f96b336d50a8c40f1;\r\n\r\nUSE f5c00ace4777417f96b336d50a8c40f1;\r\nDROP TABLE IF EXISTS test_nested5_src;\r\nDROP TABLE IF EXISTS test_nested5;\r\n\r\nCREATE TABLE test_nested5_src(\r\n\t`created_at` DateTime,\r\n\t`partition` Date,\r\n\t`timestamp` DateTime,\r\n\t`float_pos` Nullable(Float64),\r\n\t`integer_neg` Nullable(Int64),\r\n\t`integer_pos` Nullable(Int64),\r\n\t`ip_address` Nullable(String),\r\n\t`null_field` Nullable(String),\r\n\t`string` Nullable(String)\r\n) ENGINE = ReplicatedMergeTree('/var/lib/clickhouse/tables/{shard}/f5c00ace4777417f96b336d50a8c40f1/test_nested5_src', '{replica}', `partition`, `timestamp`, 8192);\r\n\r\nCREATE TABLE test_nested5 AS test_nested5_src ENGINE = Distributed(me, f5c00ace4777417f96b336d50a8c40f1, test_nested5_src, rand());\r\n\r\nINSERT INTO f5c00ace4777417f96b336d50a8c40f1.test_nested5(`string`, `null_field`, `ip_address`, `integer_pos`, `integer_neg`, `float_pos`, `timestamp`, `partition`,`created_at`) VALUES ('string',NULL,'91.231.138.140',1234,-345,1345.23,1509017026,17465,1509017026);\r\n```\r\n\r\nWhere `me` is simples cluster defined as follows:\r\n```\r\n    <me>\r\n        <shard>\r\n            <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>127.0.0.2</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n        </shard>\r\n    </me>\r\n```\r\n\r\nThe data is inserted successfully and data dir looks correctly (there is no `tmp` dir):\r\n```\r\nls /var/lib/clickhouse/data/f5c00ace4777417f96b336d50a8c40f1/test_nested5/\r\ndefault@127%2E0%2E0%2E2:9000\r\n```\r\n\r\nServer restarts successfully.\nCan you try also with <internal_replication>true</internal_replication> (this is what my cluster it's using)\r\n\r\nLet me know if using this you can replicate if not I will provide you my full config.\r\n\r\nMy cluster is defined as follow:\r\n\r\n```xml\r\n\t<clickhouse_remote_servers>\r\n\t\t<cluster_local>\r\n\t\t\t<shard>\r\n\t\t\t\t<internal_replication>true</internal_replication>\r\n\t\t\t\t<replica>\r\n\t\t\t\t\t<host>127.0.0.1</host>\r\n\t\t\t\t\t<port>9000</port>\r\n\t\t\t\t</replica>\r\n\t\t\t</shard>\r\n\t\t</cluster_local>\r\n\t</clickhouse_remote_servers>\r\n\r\n```\r\n\r\nSilviu\nHello @ludv1x ,\r\n\r\nHave you tested using internal_replication = true ? \r\n\r\nSilviu\nYes, it is reproduced if you use `127.0.0.1` and `<internal_replication>true</internal_replication>`!\nGreat let me know if you need other tests !\r\nSilviu\nMore generally, it is reproduced if all replicas of the shard are local replicas.\nThank you! We will fix it in the next release.",
  "created_at": "2017-11-02T14:26:07Z"
}