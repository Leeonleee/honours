{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 38105,
  "instance_id": "ClickHouse__ClickHouse-38105",
  "issue_numbers": [
    "27651"
  ],
  "base_commit": "7fce1d54fe1089fc575161056f0b6b84a793fb19",
  "patch": "diff --git a/docs/en/engines/table-engines/integrations/hdfs.md b/docs/en/engines/table-engines/integrations/hdfs.md\nindex 503bd779abfc..146d1fcb3a61 100644\n--- a/docs/en/engines/table-engines/integrations/hdfs.md\n+++ b/docs/en/engines/table-engines/integrations/hdfs.md\n@@ -186,7 +186,6 @@ Similar to GraphiteMergeTree, the HDFS engine supports extended configuration us\n | -                                                     | -                       |\n |hadoop\\_kerberos\\_keytab                               | \"\"                      |\n |hadoop\\_kerberos\\_principal                            | \"\"                      |\n-|hadoop\\_kerberos\\_kinit\\_command                       | kinit                   |\n |libhdfs3\\_conf                                         | \"\"                      |\n \n ### Limitations {#limitations}\n@@ -200,8 +199,7 @@ Note that due to libhdfs3 limitations only old-fashioned approach is supported,\n datanode communications are not secured by SASL (`HADOOP_SECURE_DN_USER` is a reliable indicator of such\n security approach). Use `tests/integration/test_storage_kerberized_hdfs/hdfs_configs/bootstrap.sh` for reference.\n \n-If `hadoop_kerberos_keytab`, `hadoop_kerberos_principal` or `hadoop_kerberos_kinit_command` is specified, `kinit` will be invoked. `hadoop_kerberos_keytab` and `hadoop_kerberos_principal` are mandatory in this case. `kinit` tool and krb5 configuration files are required.\n-\n+If `hadoop_kerberos_keytab`, `hadoop_kerberos_principal` or `hadoop_security_kerberos_ticket_cache_path` are specified, Kerberos authentication will be used. `hadoop_kerberos_keytab` and `hadoop_kerberos_principal` are mandatory in this case.\n ## HDFS Namenode HA support {#namenode-ha}\n \n libhdfs3 support HDFS namenode HA.\ndiff --git a/docs/en/engines/table-engines/integrations/kafka.md b/docs/en/engines/table-engines/integrations/kafka.md\nindex a9d13194a59d..94fcbab51adb 100644\n--- a/docs/en/engines/table-engines/integrations/kafka.md\n+++ b/docs/en/engines/table-engines/integrations/kafka.md\n@@ -168,7 +168,7 @@ For a list of possible configuration options, see the [librdkafka configuration\n ### Kerberos support {#kafka-kerberos-support}\n \n To deal with Kerberos-aware Kafka, add `security_protocol` child element with `sasl_plaintext` value. It is enough if Kerberos ticket-granting ticket is obtained and cached by OS facilities.\n-ClickHouse is able to maintain Kerberos credentials using a keytab file. Consider `sasl_kerberos_service_name`, `sasl_kerberos_keytab`, `sasl_kerberos_principal` and `sasl.kerberos.kinit.cmd` child elements.\n+ClickHouse is able to maintain Kerberos credentials using a keytab file. Consider `sasl_kerberos_service_name`, `sasl_kerberos_keytab` and `sasl_kerberos_principal` child elements.\n \n Example:\n \ndiff --git a/docs/ru/engines/table-engines/integrations/hdfs.md b/docs/ru/engines/table-engines/integrations/hdfs.md\nindex 0857359e9873..84f31c0afcc8 100644\n--- a/docs/ru/engines/table-engines/integrations/hdfs.md\n+++ b/docs/ru/engines/table-engines/integrations/hdfs.md\n@@ -183,7 +183,6 @@ CREATE TABLE big_table (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9\n | -                                                     | -                       |\n |hadoop\\_kerberos\\_keytab                               | \"\"                      |\n |hadoop\\_kerberos\\_principal                            | \"\"                      |\n-|hadoop\\_kerberos\\_kinit\\_command                       | kinit                   |\n \n ### \u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f {#limitations}\n   * `hadoop_security_kerberos_ticket_cache_path` \u0438 `libhdfs3_conf` \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u043c, \u0430 \u043d\u0435 \u043d\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u043c \u0443\u0440\u043e\u0432\u043d\u0435\n@@ -196,7 +195,7 @@ CREATE TABLE big_table (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9\n \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u044f \u0441 \u0443\u0437\u043b\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435 \u0437\u0430\u0449\u0438\u0449\u0435\u043d\u0430 SASL (`HADOOP_SECURE_DN_USER` \u043d\u0430\u0434\u0435\u0436\u043d\u044b\u0439 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c \u0442\u0430\u043a\u043e\u0433\u043e\n \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043a \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438). \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 `tests/integration/test_storage_kerberized_hdfs/hdfs_configs/bootstrap.sh` \u0434\u043b\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u0430 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a.\n \n-\u0415\u0441\u043b\u0438 `hadoop_kerberos_keytab`, `hadoop_kerberos_principal` \u0438\u043b\u0438 `hadoop_kerberos_kinit_command` \u0443\u043a\u0430\u0437\u0430\u043d\u044b \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445, `kinit` \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0437\u0432\u0430\u043d. `hadoop_kerberos_keytab` \u0438 `hadoop_kerberos_principal` \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b \u0432 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0442\u0430\u043a\u0436\u0435 \u0431\u0443\u0434\u0435\u0442 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c `kinit` \u0438 \u0444\u0430\u0439\u043b\u044b \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438 krb5.\n+\u0415\u0441\u043b\u0438 `hadoop_kerberos_keytab`, `hadoop_kerberos_principal` \u0438\u043b\u0438 `hadoop_security_kerberos_ticket_cache_path` \u0443\u043a\u0430\u0437\u0430\u043d\u044b \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445, \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0430\u0443\u0442\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e Kerberos. `hadoop_kerberos_keytab` \u0438 `hadoop_kerberos_principal` \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b \u0432 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435.\n \n ## \u0412\u0438\u0440\u0442\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b {#virtual-columns}\n \ndiff --git a/docs/ru/engines/table-engines/integrations/kafka.md b/docs/ru/engines/table-engines/integrations/kafka.md\nindex b24a096015d8..b51a01133022 100644\n--- a/docs/ru/engines/table-engines/integrations/kafka.md\n+++ b/docs/ru/engines/table-engines/integrations/kafka.md\n@@ -167,7 +167,7 @@ Kafka(kafka_broker_list, kafka_topic_list, kafka_group_name, kafka_format\n ### \u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 Kerberos {#kafka-kerberos-support}\n \n \u0427\u0442\u043e\u0431\u044b \u043d\u0430\u0447\u0430\u0442\u044c \u0440\u0430\u0431\u043e\u0442\u0443 \u0441 Kafka \u0441 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u043e\u0439 Kerberos, \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u0434\u043e\u0447\u0435\u0440\u043d\u0438\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 `security_protocol` \u0441\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c `sasl_plaintext`. \u042d\u0442\u043e\u0433\u043e \u0431\u0443\u0434\u0435\u0442 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e, \u0435\u0441\u043b\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d \u0442\u0438\u043a\u0435\u0442 \u043d\u0430 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u0438\u043a\u0435\u0442\u0430 (ticket-granting ticket) Kerberos \u0438 \u043e\u043d \u043a\u044d\u0448\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430\u043c\u0438 \u041e\u0421.\n-ClickHouse \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c \u0443\u0447\u0435\u0442\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 Kerberos \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0430\u0439\u043b\u0430 keytab. \u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0434\u043e\u0447\u0435\u0440\u043d\u0438\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b `sasl_kerberos_service_name`, `sasl_kerberos_keytab`, `sasl_kerberos_principal` \u0438 `sasl.kerberos.kinit.cmd`.\n+ClickHouse \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c \u0443\u0447\u0435\u0442\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 Kerberos \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0430\u0439\u043b\u0430 keytab. \u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0434\u043e\u0447\u0435\u0440\u043d\u0438\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b `sasl_kerberos_service_name`, `sasl_kerberos_keytab` \u0438 `sasl_kerberos_principal`.\n \n \u041f\u0440\u0438\u043c\u0435\u0440:\n \ndiff --git a/src/Access/CMakeLists.txt b/src/Access/CMakeLists.txt\nindex e69de29bb2d1..83bbe4182465 100644\n--- a/src/Access/CMakeLists.txt\n+++ b/src/Access/CMakeLists.txt\n@@ -0,0 +1,3 @@\n+if (ENABLE_EXAMPLES)\n+    add_subdirectory(examples)\n+endif()\ndiff --git a/src/Access/KerberosInit.cpp b/src/Access/KerberosInit.cpp\nnew file mode 100644\nindex 000000000000..ace03a5e0b5a\n--- /dev/null\n+++ b/src/Access/KerberosInit.cpp\n@@ -0,0 +1,230 @@\n+#include <Access/KerberosInit.h>\n+#include <Common/Exception.h>\n+#include <Common/logger_useful.h>\n+#include <Poco/Logger.h>\n+#include <Loggers/Loggers.h>\n+#include <filesystem>\n+#include <boost/core/noncopyable.hpp>\n+#include <fmt/format.h>\n+#if USE_KRB5\n+#include <krb5.h>\n+#include <mutex>\n+\n+using namespace DB;\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int KERBEROS_ERROR;\n+}\n+}\n+\n+namespace\n+{\n+struct K5Data\n+{\n+    krb5_context ctx;\n+    krb5_ccache out_cc;\n+    krb5_principal me;\n+    char * name;\n+    krb5_boolean switch_to_cache;\n+};\n+\n+/**\n+ * This class implements programmatic implementation of kinit.\n+ */\n+class KerberosInit : boost::noncopyable\n+{\n+public:\n+    void init(const String & keytab_file, const String & principal, const String & cache_name = \"\");\n+    ~KerberosInit();\n+private:\n+    struct K5Data k5 {};\n+    krb5_ccache defcache = nullptr;\n+    krb5_get_init_creds_opt * options = nullptr;\n+    // Credentials structure including ticket, session key, and lifetime info.\n+    krb5_creds my_creds;\n+    krb5_keytab keytab = nullptr;\n+    krb5_principal defcache_princ = nullptr;\n+    String fmtError(krb5_error_code code) const;\n+};\n+}\n+\n+\n+String KerberosInit::fmtError(krb5_error_code code) const\n+{\n+    const char *msg;\n+    msg = krb5_get_error_message(k5.ctx, code);\n+    String fmt_error = fmt::format(\" ({}, {})\", code, msg);\n+    krb5_free_error_message(k5.ctx, msg);\n+    return fmt_error;\n+}\n+\n+void KerberosInit::init(const String & keytab_file, const String & principal, const String & cache_name)\n+{\n+    auto * log = &Poco::Logger::get(\"KerberosInit\");\n+    LOG_TRACE(log,\"Trying to authenticate with Kerberos v5\");\n+\n+    krb5_error_code ret;\n+\n+    const char *deftype = nullptr;\n+\n+    if (!std::filesystem::exists(keytab_file))\n+        throw Exception(\"Keytab file does not exist\", ErrorCodes::KERBEROS_ERROR);\n+\n+    ret = krb5_init_context(&k5.ctx);\n+    if (ret)\n+        throw Exception(ErrorCodes::KERBEROS_ERROR, \"Error while initializing Kerberos 5 library ({})\", ret);\n+\n+    if (!cache_name.empty())\n+    {\n+        ret = krb5_cc_resolve(k5.ctx, cache_name.c_str(), &k5.out_cc);\n+        if (ret)\n+            throw Exception(\"Error in resolving cache\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+        LOG_TRACE(log,\"Resolved cache\");\n+    }\n+    else\n+    {\n+        // Resolve the default cache and get its type and default principal (if it is initialized).\n+        ret = krb5_cc_default(k5.ctx, &defcache);\n+        if (ret)\n+            throw Exception(\"Error while getting default cache\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+        LOG_TRACE(log,\"Resolved default cache\");\n+        deftype = krb5_cc_get_type(k5.ctx, defcache);\n+        if (krb5_cc_get_principal(k5.ctx, defcache, &defcache_princ) != 0)\n+            defcache_princ = nullptr;\n+    }\n+\n+    // Use the specified principal name.\n+    ret = krb5_parse_name_flags(k5.ctx, principal.c_str(), 0, &k5.me);\n+    if (ret)\n+        throw Exception(\"Error when parsing principal name \" + principal + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+\n+    // Cache related commands\n+    if (k5.out_cc == nullptr && krb5_cc_support_switch(k5.ctx, deftype))\n+    {\n+        // Use an existing cache for the client principal if we can.\n+        ret = krb5_cc_cache_match(k5.ctx, k5.me, &k5.out_cc);\n+        if (ret && ret != KRB5_CC_NOTFOUND)\n+            throw Exception(\"Error while searching for cache for \" + principal + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+        if (0 == ret)\n+        {\n+            LOG_TRACE(log,\"Using default cache: {}\", krb5_cc_get_name(k5.ctx, k5.out_cc));\n+            k5.switch_to_cache = 1;\n+        }\n+        else if (defcache_princ != nullptr)\n+        {\n+            // Create a new cache to avoid overwriting the initialized default cache.\n+            ret = krb5_cc_new_unique(k5.ctx, deftype, nullptr, &k5.out_cc);\n+            if (ret)\n+                throw Exception(\"Error while generating new cache\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+            LOG_TRACE(log,\"Using default cache: {}\", krb5_cc_get_name(k5.ctx, k5.out_cc));\n+            k5.switch_to_cache = 1;\n+        }\n+    }\n+\n+    // Use the default cache if we haven't picked one yet.\n+    if (k5.out_cc == nullptr)\n+    {\n+        k5.out_cc = defcache;\n+        defcache = nullptr;\n+        LOG_TRACE(log,\"Using default cache: {}\", krb5_cc_get_name(k5.ctx, k5.out_cc));\n+    }\n+\n+    ret = krb5_unparse_name(k5.ctx, k5.me, &k5.name);\n+    if (ret)\n+        throw Exception(\"Error when unparsing name\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+    LOG_TRACE(log,\"Using principal: {}\", k5.name);\n+\n+    // Allocate a new initial credential options structure.\n+    ret = krb5_get_init_creds_opt_alloc(k5.ctx, &options);\n+    if (ret)\n+        throw Exception(\"Error in options allocation\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+\n+    // Resolve keytab\n+    ret = krb5_kt_resolve(k5.ctx, keytab_file.c_str(), &keytab);\n+    if (ret)\n+        throw Exception(\"Error in resolving keytab \"+keytab_file + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+    LOG_TRACE(log,\"Using keytab: {}\", keytab_file);\n+\n+    // Set an output credential cache in initial credential options.\n+    ret = krb5_get_init_creds_opt_set_out_ccache(k5.ctx, options, k5.out_cc);\n+    if (ret)\n+        throw Exception(\"Error in setting output credential cache\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+\n+    // Action: init or renew\n+    LOG_TRACE(log,\"Trying to renew credentials\");\n+    memset(&my_creds, 0, sizeof(my_creds));\n+    // Get renewed credential from KDC using an existing credential from output cache.\n+    ret = krb5_get_renewed_creds(k5.ctx, &my_creds, k5.me, k5.out_cc, nullptr);\n+    if (ret)\n+    {\n+        LOG_TRACE(log,\"Renew failed {}\", fmtError(ret));\n+        LOG_TRACE(log,\"Trying to get initial credentials\");\n+        // Request KDC for an initial credentials using keytab.\n+        ret = krb5_get_init_creds_keytab(k5.ctx, &my_creds, k5.me, keytab, 0, nullptr, options);\n+        if (ret)\n+            throw Exception(\"Error in getting initial credentials\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+        else\n+            LOG_TRACE(log,\"Got initial credentials\");\n+    }\n+    else\n+    {\n+        LOG_TRACE(log,\"Successful renewal\");\n+        // Initialize a credential cache. Destroy any existing contents of cache and initialize it for the default principal.\n+        ret = krb5_cc_initialize(k5.ctx, k5.out_cc, k5.me);\n+        if (ret)\n+            throw Exception(\"Error when initializing cache\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+        LOG_TRACE(log,\"Initialized cache\");\n+        // Store credentials in a credential cache.\n+        ret = krb5_cc_store_cred(k5.ctx, k5.out_cc, &my_creds);\n+        if (ret)\n+            LOG_TRACE(log,\"Error while storing credentials\");\n+        LOG_TRACE(log,\"Stored credentials\");\n+    }\n+\n+    if (k5.switch_to_cache)\n+    {\n+        // Make a credential cache the primary cache for its collection.\n+        ret = krb5_cc_switch(k5.ctx, k5.out_cc);\n+        if (ret)\n+            throw Exception(\"Error while switching to new cache\" + fmtError(ret), ErrorCodes::KERBEROS_ERROR);\n+    }\n+\n+    LOG_TRACE(log,\"Authenticated to Kerberos v5\");\n+}\n+\n+KerberosInit::~KerberosInit()\n+{\n+    if (k5.ctx)\n+    {\n+        if (defcache)\n+            krb5_cc_close(k5.ctx, defcache);\n+        krb5_free_principal(k5.ctx, defcache_princ);\n+\n+        if (options)\n+            krb5_get_init_creds_opt_free(k5.ctx, options);\n+        if (my_creds.client == k5.me)\n+            my_creds.client = nullptr;\n+        krb5_free_cred_contents(k5.ctx, &my_creds);\n+        if (keytab)\n+            krb5_kt_close(k5.ctx, keytab);\n+\n+        krb5_free_unparsed_name(k5.ctx, k5.name);\n+        krb5_free_principal(k5.ctx, k5.me);\n+        if (k5.out_cc != nullptr)\n+            krb5_cc_close(k5.ctx, k5.out_cc);\n+        krb5_free_context(k5.ctx);\n+    }\n+}\n+\n+void kerberosInit(const String & keytab_file, const String & principal, const String & cache_name)\n+{\n+    // Using mutex to prevent cache file corruptions\n+    static std::mutex kinit_mtx;\n+    std::unique_lock<std::mutex> lck(kinit_mtx);\n+    KerberosInit k_init;\n+    k_init.init(keytab_file, principal, cache_name);\n+}\n+#endif // USE_KRB5\ndiff --git a/src/Access/KerberosInit.h b/src/Access/KerberosInit.h\nnew file mode 100644\nindex 000000000000..5a11a275529f\n--- /dev/null\n+++ b/src/Access/KerberosInit.h\n@@ -0,0 +1,11 @@\n+#pragma once\n+\n+#include \"config_core.h\"\n+\n+#include <base/types.h>\n+\n+#if USE_KRB5\n+\n+void kerberosInit(const String & keytab_file, const String & principal, const String & cache_name = \"\");\n+\n+#endif // USE_KRB5\ndiff --git a/src/Access/examples/CMakeLists.txt b/src/Access/examples/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..07f75ca0b474\n--- /dev/null\n+++ b/src/Access/examples/CMakeLists.txt\n@@ -0,0 +1,4 @@\n+if (TARGET ch_contrib::krb5)\n+    add_executable (kerberos_init kerberos_init.cpp)\n+    target_link_libraries (kerberos_init PRIVATE dbms ch_contrib::krb5)\n+endif()\ndiff --git a/src/Access/examples/kerberos_init.cpp b/src/Access/examples/kerberos_init.cpp\nnew file mode 100644\nindex 000000000000..5dbe92a5b57b\n--- /dev/null\n+++ b/src/Access/examples/kerberos_init.cpp\n@@ -0,0 +1,47 @@\n+#include <iostream>\n+#include <Poco/ConsoleChannel.h>\n+#include <Poco/Logger.h>\n+#include <Poco/AutoPtr.h>\n+#include <Common/Exception.h>\n+#include <Access/KerberosInit.h>\n+\n+/** The example demonstrates using of kerberosInit function to obtain and cache Kerberos ticket-granting ticket.\n+  * The first argument specifies keytab file. The second argument specifies principal name.\n+  * The third argument is optional. It specifies credentials cache location.\n+  * After successful run of kerberos_init it is useful to call klist command to list cached Kerberos tickets.\n+  * It is also useful to run kdestroy to destroy Kerberos tickets if needed.\n+  */\n+\n+using namespace DB;\n+\n+int main(int argc, char ** argv)\n+{\n+    std::cout << \"Kerberos Init\" << \"\\n\";\n+\n+    if (argc < 3)\n+    {\n+        std::cout << \"kerberos_init obtains and caches an initial ticket-granting ticket for principal.\" << \"\\n\\n\";\n+        std::cout << \"Usage:\" << \"\\n\" << \"    kerberos_init keytab principal [cache]\" << \"\\n\";\n+        return 0;\n+    }\n+\n+    String cache_name = \"\";\n+    if (argc == 4)\n+        cache_name = argv[3];\n+\n+    Poco::AutoPtr<Poco::ConsoleChannel> app_channel(new Poco::ConsoleChannel(std::cerr));\n+    Poco::Logger::root().setChannel(app_channel);\n+    Poco::Logger::root().setLevel(\"trace\");\n+\n+    try\n+    {\n+        kerberosInit(argv[1], argv[2], cache_name);\n+    }\n+    catch (const Exception & e)\n+    {\n+        std::cout << \"KerberosInit failure: \" << getExceptionMessage(e, false) << \"\\n\";\n+        return -1;\n+    }\n+    std::cout << \"Done\" << \"\\n\";\n+    return 0;\n+}\ndiff --git a/src/Storages/HDFS/HDFSCommon.cpp b/src/Storages/HDFS/HDFSCommon.cpp\nindex 2f7b03790eeb..3ac9f50231dc 100644\n--- a/src/Storages/HDFS/HDFSCommon.cpp\n+++ b/src/Storages/HDFS/HDFSCommon.cpp\n@@ -10,7 +10,9 @@\n #include <IO/WriteBufferFromString.h>\n #include <IO/Operators.h>\n #include <Common/logger_useful.h>\n-\n+#if USE_KRB5\n+#include <Access/KerberosInit.h>\n+#endif // USE_KRB5\n \n namespace DB\n {\n@@ -18,8 +20,10 @@ namespace ErrorCodes\n {\n     extern const int BAD_ARGUMENTS;\n     extern const int NETWORK_ERROR;\n+    #if USE_KRB5\n     extern const int EXCESSIVE_ELEMENT_IN_CONFIG;\n-    extern const int NO_ELEMENTS_IN_CONFIG;\n+    extern const int KERBEROS_ERROR;\n+    #endif // USE_KRB5\n }\n \n const String HDFSBuilderWrapper::CONFIG_PREFIX = \"hdfs\";\n@@ -40,25 +44,28 @@ void HDFSBuilderWrapper::loadFromConfig(const Poco::Util::AbstractConfiguration\n         String key_name;\n         if (key == \"hadoop_kerberos_keytab\")\n         {\n+            #if USE_KRB5\n             need_kinit = true;\n             hadoop_kerberos_keytab = config.getString(key_path);\n+            #else // USE_KRB5\n+            LOG_WARNING(&Poco::Logger::get(\"HDFSClient\"), \"hadoop_kerberos_keytab parameter is ignored because ClickHouse was built without support of krb5 library.\");\n+            #endif // USE_KRB5\n             continue;\n         }\n         else if (key == \"hadoop_kerberos_principal\")\n         {\n+            #if USE_KRB5\n             need_kinit = true;\n             hadoop_kerberos_principal = config.getString(key_path);\n             hdfsBuilderSetPrincipal(hdfs_builder, hadoop_kerberos_principal.c_str());\n-            continue;\n-        }\n-        else if (key == \"hadoop_kerberos_kinit_command\")\n-        {\n-            need_kinit = true;\n-            hadoop_kerberos_kinit_command = config.getString(key_path);\n+            #else // USE_KRB5\n+            LOG_WARNING(&Poco::Logger::get(\"HDFSClient\"), \"hadoop_kerberos_principal parameter is ignored because ClickHouse was built without support of krb5 library.\");\n+            #endif // USE_KRB5\n             continue;\n         }\n         else if (key == \"hadoop_security_kerberos_ticket_cache_path\")\n         {\n+            #if USE_KRB5\n             if (isUser)\n             {\n                 throw Exception(\"hadoop.security.kerberos.ticket.cache.path cannot be set per user\",\n@@ -67,6 +74,9 @@ void HDFSBuilderWrapper::loadFromConfig(const Poco::Util::AbstractConfiguration\n \n             hadoop_security_kerberos_ticket_cache_path = config.getString(key_path);\n             // standard param - pass further\n+            #else // USE_KRB5\n+            LOG_WARNING(&Poco::Logger::get(\"HDFSClient\"), \"hadoop.security.kerberos.ticket.cache.path parameter is ignored because ClickHouse was built without support of krb5 library.\");\n+            #endif // USE_KRB5\n         }\n \n         key_name = boost::replace_all_copy(key, \"_\", \".\");\n@@ -76,44 +86,21 @@ void HDFSBuilderWrapper::loadFromConfig(const Poco::Util::AbstractConfiguration\n     }\n }\n \n-String HDFSBuilderWrapper::getKinitCmd()\n+#if USE_KRB5\n+void HDFSBuilderWrapper::runKinit()\n {\n-\n-    if (hadoop_kerberos_keytab.empty() || hadoop_kerberos_principal.empty())\n+    LOG_DEBUG(&Poco::Logger::get(\"HDFSClient\"), \"Running KerberosInit\");\n+    try\n     {\n-        throw Exception(\"Not enough parameters to run kinit\",\n-            ErrorCodes::NO_ELEMENTS_IN_CONFIG);\n+        kerberosInit(hadoop_kerberos_keytab,hadoop_kerberos_principal,hadoop_security_kerberos_ticket_cache_path);\n     }\n-\n-    WriteBufferFromOwnString ss;\n-\n-    String cache_name =  hadoop_security_kerberos_ticket_cache_path.empty() ?\n-        String() :\n-        (String(\" -c \\\"\") + hadoop_security_kerberos_ticket_cache_path + \"\\\"\");\n-\n-    // command to run looks like\n-    // kinit -R -t /keytab_dir/clickhouse.keytab -k somebody@TEST.CLICKHOUSE.TECH || ..\n-    ss << hadoop_kerberos_kinit_command << cache_name <<\n-        \" -R -t \\\"\" << hadoop_kerberos_keytab << \"\\\" -k \" << hadoop_kerberos_principal <<\n-        \"|| \" << hadoop_kerberos_kinit_command << cache_name << \" -t \\\"\" <<\n-        hadoop_kerberos_keytab << \"\\\" -k \" << hadoop_kerberos_principal;\n-    return ss.str();\n-}\n-\n-void HDFSBuilderWrapper::runKinit()\n-{\n-    String cmd = getKinitCmd();\n-    LOG_DEBUG(&Poco::Logger::get(\"HDFSClient\"), \"running kinit: {}\", cmd);\n-\n-    std::unique_lock<std::mutex> lck(kinit_mtx);\n-\n-    auto command = ShellCommand::execute(cmd);\n-    auto status = command->tryWait();\n-    if (status)\n+    catch (const DB::Exception & e)\n     {\n-        throw Exception(\"kinit failure: \" + cmd, ErrorCodes::BAD_ARGUMENTS);\n+        throw Exception(\"KerberosInit failure: \"+ getExceptionMessage(e, false), ErrorCodes::KERBEROS_ERROR);\n     }\n+    LOG_DEBUG(&Poco::Logger::get(\"HDFSClient\"), \"Finished KerberosInit\");\n }\n+#endif // USE_KRB5\n \n HDFSBuilderWrapper createHDFSBuilder(const String & uri_str, const Poco::Util::AbstractConfiguration & config)\n {\n@@ -184,16 +171,16 @@ HDFSBuilderWrapper createHDFSBuilder(const String & uri_str, const Poco::Util::A\n         }\n     }\n \n+    #if USE_KRB5\n     if (builder.need_kinit)\n     {\n         builder.runKinit();\n     }\n+    #endif // USE_KRB5\n \n     return builder;\n }\n \n-std::mutex HDFSBuilderWrapper::kinit_mtx;\n-\n HDFSFSPtr createHDFSFS(hdfsBuilder * builder)\n {\n     HDFSFSPtr fs(hdfsBuilderConnect(builder));\ndiff --git a/src/Storages/HDFS/HDFSCommon.h b/src/Storages/HDFS/HDFSCommon.h\nindex 0523849abe51..9eb2dfd3e467 100644\n--- a/src/Storages/HDFS/HDFSCommon.h\n+++ b/src/Storages/HDFS/HDFSCommon.h\n@@ -9,7 +9,6 @@\n \n #include <hdfs/hdfs.h>\n #include <base/types.h>\n-#include <mutex>\n \n #include <Interpreters/Context.h>\n #include <Poco/Util/AbstractConfiguration.h>\n@@ -69,10 +68,6 @@ static const String CONFIG_PREFIX;\n private:\n     void loadFromConfig(const Poco::Util::AbstractConfiguration & config, const String & prefix, bool isUser = false);\n \n-    String getKinitCmd();\n-\n-    void runKinit();\n-\n     // hdfs builder relies on an external config data storage\n     std::pair<String, String>& keep(const String & k, const String & v)\n     {\n@@ -80,14 +75,15 @@ static const String CONFIG_PREFIX;\n     }\n \n     hdfsBuilder * hdfs_builder;\n+    std::vector<std::pair<String, String>> config_stor;\n+\n+    #if USE_KRB5\n+    void runKinit();\n     String hadoop_kerberos_keytab;\n     String hadoop_kerberos_principal;\n-    String hadoop_kerberos_kinit_command = \"kinit\";\n     String hadoop_security_kerberos_ticket_cache_path;\n-\n-    static std::mutex kinit_mtx;\n-    std::vector<std::pair<String, String>> config_stor;\n     bool need_kinit{false};\n+    #endif // USE_KRB5\n };\n \n using HDFSFSPtr = std::unique_ptr<std::remove_pointer_t<hdfsFS>, detail::HDFSFsDeleter>;\ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex 2409f8dcb6ef..e65ac27b0969 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -43,7 +43,9 @@\n \n #include <Common/CurrentMetrics.h>\n #include <Common/ProfileEvents.h>\n-\n+#if USE_KRB5\n+#include <Access/KerberosInit.h>\n+#endif // USE_KRB5\n \n namespace CurrentMetrics\n {\n@@ -515,6 +517,33 @@ void StorageKafka::updateConfiguration(cppkafka::Configuration & conf)\n     if (config.has(config_prefix))\n         loadFromConfig(conf, config, config_prefix);\n \n+    #if USE_KRB5\n+    if (conf.has_property(\"sasl.kerberos.kinit.cmd\"))\n+        LOG_WARNING(log, \"sasl.kerberos.kinit.cmd configuration parameter is ignored.\");\n+\n+    conf.set(\"sasl.kerberos.kinit.cmd\",\"\");\n+    conf.set(\"sasl.kerberos.min.time.before.relogin\",\"0\");\n+\n+    if (conf.has_property(\"sasl.kerberos.keytab\") && conf.has_property(\"sasl.kerberos.principal\"))\n+    {\n+        String keytab = conf.get(\"sasl.kerberos.keytab\");\n+        String principal = conf.get(\"sasl.kerberos.principal\");\n+        LOG_DEBUG(log, \"Running KerberosInit\");\n+        try\n+        {\n+            kerberosInit(keytab,principal);\n+        }\n+        catch (const Exception & e)\n+        {\n+            LOG_ERROR(log, \"KerberosInit failure: {}\", getExceptionMessage(e, false));\n+        }\n+        LOG_DEBUG(log, \"Finished KerberosInit\");\n+    }\n+    #else // USE_KRB5\n+    if (conf.has_property(\"sasl.kerberos.keytab\") || conf.has_property(\"sasl.kerberos.principal\"))\n+        LOG_WARNING(log, \"Kerberos-related parameters are ignored because ClickHouse was built without support of krb5 library.\");\n+    #endif // USE_KRB5\n+\n     // Update consumer topic-specific configuration\n     for (const auto & topic : topics)\n     {\n",
  "test_patch": "diff --git a/tests/integration/test_storage_kerberized_hdfs/test.py b/tests/integration/test_storage_kerberized_hdfs/test.py\nindex fb00403b952f..5ac8b4670f98 100644\n--- a/tests/integration/test_storage_kerberized_hdfs/test.py\n+++ b/tests/integration/test_storage_kerberized_hdfs/test.py\n@@ -113,7 +113,7 @@ def test_read_table_expired(started_cluster):\n         )\n         assert False, \"Exception have to be thrown\"\n     except Exception as ex:\n-        assert \"DB::Exception: kinit failure:\" in str(ex)\n+        assert \"DB::Exception: KerberosInit failure:\" in str(ex)\n \n     started_cluster.unpause_container(\"hdfskerberos\")\n \ndiff --git a/tests/integration/test_storage_kerberized_kafka/test.py b/tests/integration/test_storage_kerberized_kafka/test.py\nindex 6347ba89c16f..7856361deda3 100644\n--- a/tests/integration/test_storage_kerberized_kafka/test.py\n+++ b/tests/integration/test_storage_kerberized_kafka/test.py\n@@ -128,6 +128,48 @@ def test_kafka_json_as_string(kafka_cluster):\n     )\n \n \n+def test_kafka_json_as_string_request_new_ticket_after_expiration(kafka_cluster):\n+    # Ticket should be expired after the wait time\n+    # On run of SELECT query new ticket should be requested and SELECT query should run fine.\n+\n+    kafka_produce(\n+        kafka_cluster,\n+        \"kafka_json_as_string\",\n+        [\n+            '{\"t\": 123, \"e\": {\"x\": \"woof\"} }',\n+            \"\",\n+            '{\"t\": 124, \"e\": {\"x\": \"test\"} }',\n+            '{\"F1\":\"V1\",\"F2\":{\"F21\":\"V21\",\"F22\":{},\"F23\":\"V23\",\"F24\":\"2019-12-24T16:28:04\"},\"F3\":\"V3\"}',\n+        ],\n+    )\n+\n+    instance.query(\n+        \"\"\"\n+        CREATE TABLE test.kafka (field String)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kerberized_kafka1:19092',\n+                     kafka_topic_list = 'kafka_json_as_string',\n+                     kafka_commit_on_select = 1,\n+                     kafka_group_name = 'kafka_json_as_string',\n+                     kafka_format = 'JSONAsString',\n+                     kafka_flush_interval_ms=1000;\n+        \"\"\"\n+    )\n+\n+    time.sleep(45)  # wait for ticket expiration\n+\n+    result = instance.query(\"SELECT * FROM test.kafka;\")\n+    expected = \"\"\"\\\n+{\"t\": 123, \"e\": {\"x\": \"woof\"} }\n+{\"t\": 124, \"e\": {\"x\": \"test\"} }\n+{\"F1\":\"V1\",\"F2\":{\"F21\":\"V21\",\"F22\":{},\"F23\":\"V23\",\"F24\":\"2019-12-24T16:28:04\"},\"F3\":\"V3\"}\n+\"\"\"\n+    assert TSV(result) == TSV(expected)\n+    assert instance.contains_in_log(\n+        \"Parsing of message (topic: kafka_json_as_string, partition: 0, offset: 1) return no rows\"\n+    )\n+\n+\n def test_kafka_json_as_string_no_kdc(kafka_cluster):\n     # When the test is run alone (not preceded by any other kerberized kafka test),\n     # we need a ticket to\n@@ -182,7 +224,7 @@ def test_kafka_json_as_string_no_kdc(kafka_cluster):\n     assert TSV(result) == TSV(expected)\n     assert instance.contains_in_log(\"StorageKafka (kafka_no_kdc): Nothing to commit\")\n     assert instance.contains_in_log(\"Ticket expired\")\n-    assert instance.contains_in_log(\"Kerberos ticket refresh failed\")\n+    assert instance.contains_in_log(\"KerberosInit failure:\")\n \n \n if __name__ == \"__main__\":\n",
  "problem_statement": "librdkafka use `system` function from libc \nHarmful check fails for librdkafka\r\n```\r\n2021.08.13 09:24:10.499196 [ 179 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.08.13 09:24:10.500177 [ 179 ] {} <Fatal> BaseDaemon: (version 21.9.1.7754 (official build), build id: AA047EDD4FDBABD43D2D23351D89438384B3C4CF) (from thread 10) (query_id: 3a077dac-da31-44f3-a3f2-f27b94603d03) Received signal Illegal instruction (4)\r\n2021.08.13 09:24:10.500753 [ 179 ] {} <Fatal> BaseDaemon: Illegal operand.\r\n2021.08.13 09:24:10.501364 [ 179 ] {} <Fatal> BaseDaemon: Stack trace: 0x136179fd 0x23a09a06 0x23a088b6 0x239c90d3 0x238be36c 0x2386db63 0x1eed6cba 0x1eed6c40 0x1eed6a00 0x1eed6684 0x1eed64b2 0x1eec8091 0x1eebc5d5 0x1eebbdc1 0x1df556dd 0x1df527bb 0x1df56bcc 0x1e992ef0 0x1e991085 0x1f54575c 0x1f552aa8 0x23c03cbc 0x23c04544 0x23d5eb23 0x23d5b3dd 0x23d5a168 0x7effb7910609 0x7effb7826293\r\n2021.08.13 09:24:10.505590 [ 179 ] {} <Fatal> BaseDaemon: 4. ./obj-x86_64-linux-gnu/../base/harmful/harmful.c:220: system @ 0x136179fd in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.580346 [ 179 ] {} <Fatal> BaseDaemon: 5. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_sasl_cyrus.c:226: rd_kafka_sasl_cyrus_kinit_refresh @ 0x23a09a06 in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.652059 [ 179 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_sasl_cyrus.c:572: rd_kafka_sasl_cyrus_init @ 0x23a088b6 in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.723386 [ 179 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_sasl.c:315: rd_kafka_sasl_init @ 0x239c90d3 in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.780824 [ 179 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka.c:2280: rd_kafka_new @ 0x238be36c in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.927985 [ 179 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../contrib/cppkafka/src/consumer.cpp:65: cppkafka::Consumer::Consumer(cppkafka::Configuration) @ 0x2386db63 in /usr/bin/clickhouse\r\n2021.08.13 09:24:11.691891 [ 179 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:886: void std::__1::allocator<cppkafka::Consumer>::construct<cppkafka::Consumer, cppkafka::Configuration&>(cppkafka::Consumer*, cppkafka::Configuration&) @ 0x1eed6cba in /usr/bin/clickhouse\r\n2021.08.13 09:24:12.452286 [ 179 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:521: void std::__1::allocator_traits<std::__1::allocator<cppkafka::Consumer> >::__construct<cppkafka::Consumer, cppkafka::Configuration&>(std::__1::integral_constant<bool, true>, std::__1::allocator<cppkafka::Consumer>&, cppkafka::Consumer*, cppkafka::Configuration&) @ 0x1eed6c40 in /usr/bin/clickhouse\r\n2021.08.13 09:24:13.211423 [ 179 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:482: void std::__1::allocator_traits<std::__1::allocator<cppkafka::Consumer> >::construct<cppkafka::Consumer, cppkafka::Configuration&>(std::__1::allocator<cppkafka::Consumer>&, cppkafka::Consumer*, cppkafka::Configuration&) @ 0x1eed6a00 in /usr/bin/clickhouse\r\n2021.08.13 09:24:13.973190 [ 179 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2594: std::__1::__shared_ptr_emplace<cppkafka::Consumer, std::__1::allocator<cppkafka::Consumer> >::__shared_ptr_emplace<cppkafka::Configuration&>(std::__1::allocator<cppkafka::Consumer>, cppkafka::Configuration&) @ 0x1eed6684 in /usr/bin/clickhouse\r\n2021.08.13 09:24:14.756760 [ 179 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3360: std::__1::shared_ptr<cppkafka::Consumer> std::__1::allocate_shared<cppkafka::Consumer, std::__1::allocator<cppkafka::Consumer>, cppkafka::Configuration&, void>(std::__1::allocator<cppkafka::Consumer> const&, cppkafka::Configuration&) @ 0x1eed64b2 in /usr/bin/clickhouse\r\n2021.08.13 09:24:15.492782 [ 179 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3369: std::__1::shared_ptr<cppkafka::Consumer> std::__1::make_shared<cppkafka::Consumer, cppkafka::Configuration&, void>(cppkafka::Configuration&) @ 0x1eec8091 in /usr/bin/clickhouse\r\n2021.08.13 09:24:16.197138 [ 179 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Storages/Kafka/StorageKafka.cpp:430: DB::StorageKafka::createReadBuffer(unsigned long) @ 0x1eebc5d5 in /usr/bin/clickhouse\r\n2021.08.13 09:24:16.889068 [ 179 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Storages/Kafka/StorageKafka.cpp:309: DB::StorageKafka::startup() @ 0x1eebbdc1 in /usr/bin/clickhouse\r\n2021.08.13 09:24:17.678482 [ 179 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:1077: DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&) @ 0x1df556dd in /usr/bin/clickhouse\r\n2021.08.13 09:24:18.464234 [ 179 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:937: DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x1df527bb in /usr/bin/clickhouse\r\n2021.08.13 09:24:19.255921 [ 179 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:0: DB::InterpreterCreateQuery::execute() @ 0x1df56bcc in /usr/bin/clickhouse\r\n2021.08.13 09:24:19.896872 [ 179 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:565: DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) @ 0x1e992ef0 in /usr/bin/clickhouse\r\n2021.08.13 09:24:20.575817 [ 179 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:926: DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool) @ 0x1e991085 in /usr/bin/clickhouse\r\n2021.08.13 09:24:21.290223 [ 179 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:335: DB::TCPHandler::runImpl() @ 0x1f54575c in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.039072 [ 179 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1670: DB::TCPHandler::run() @ 0x1f552aa8 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.112692 [ 179 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43: Poco::Net::TCPServerConnection::start() @ 0x23c03cbc in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.221010 [ 179 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115: Poco::Net::TCPServerDispatcher::run() @ 0x23c04544 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.322408 [ 179 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199: Poco::PooledThread::run() @ 0x23d5eb23 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.417239 [ 179 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:56: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x23d5b3dd in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.510947 [ 179 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345: Poco::ThreadImpl::runnableEntry(void*) @ 0x23d5a168 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.511385 [ 179 ] {} <Fatal> BaseDaemon: 30. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.08.13 09:24:22.511691 [ 179 ] {} <Fatal> BaseDaemon: 31. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.13 09:24:23.823986 [ 179 ] {} <Fatal> BaseDaemon: Checksum of the binary: 55DDC1145AD182E891203722D68048BB, integrity check passed.\r\n2021.08.13 09:24:32.266474 [ 1 ] {} <Fatal> Application: Child process was terminated by signal 4.\r\n```\n",
  "hints_text": "kinit execution (related to kerberos). \nThe function `system` is really dangerous:\r\n- it depends on the shell that is used in the system;\r\n- parameters should be very carefully escaped;\r\n- it is calling some program from the system and we have to trust this program;\r\n- environment variables will be leaked to this program;\r\n- care should be taken for file descriptors and sockets;\r\n- it is using `fork` that introduces a storm of page faults - major performance degradation;\r\n- calling `fork` is impossible if memory overcommit is set to 2 and the size of virtual address space is large;\r\n- ...\r\n\r\nBut it's unclear how to overcome this issue.\r\nPatching rdkafka library looks like overkill.\n> Patching rdkafka library looks like overkill. \r\n\r\nAFAIR krb5 is very old-schoolish and doesn't have any other API than starting some commands, `kinit` call is needed to refresh the ticket. \r\nSo I don't know if it can be avoided. \r\n\r\n@traceon @ilejn what do you think, guys? You were digging deeper into Kerberos things.\nThat is correct.\r\n\r\nIf the problem is calling _system_, but having this dependency is Ok, it is not necessary to patch librdkafka. It is enough to run kinit explicitly by CH. It is what we do for HDFS (just for lack this functionality in hdfs library).\r\n\r\nIf we consider this as a problem I can handle it.\n> what do you think, guys?\r\n\r\n`kinit`, as an executable, is using calls from `krb5` as far as I remember. So nothing should prevent you from doing the same thing as `kinit` programmatically, without running an external process.\nI have to admit that for an application with massive static linkage it is quite natural to implement kinit internally ;)\r\nI am not a security expert and I cannot say if using kinit provided by OS is required (e.g. it can be customized for monitoring purposes or something).\r\n\r\nHonestly I doubt that it is worth doing.\ni think it's https://web.mit.edu/kerberos/krb5-devel/doc/appdev/refs/api/krb5_get_init_creds_password.html \r\n\r\nhttps://web.mit.edu/kerberos/www/krb5-latest/doc/appdev/init_creds.html\r\n\r\nhttps://krbdev.mit.narkive.com/5sguP6bF/how-to-achieve-what-kinit-does-programmatically\r\n\r\nhttp://web.mit.edu/kaduk/Public/appdev.pdf\nActually we talked on Friday with the team, that it is probably logical to move code like kafka/rabbitmq etc. to separate process like we did with library bridge. It adds overhead for data transfer, but allows to make core process more safe. And we can set up good common interface for such integrations.\n> It adds overhead for data transfer\r\n\r\nPotentially it can also be zero-copy (shared memory)\r\n\r\n> And we can set up good common interface for such integrations\r\n\r\nWorth to check Arrow IPC\r\n\r\n\n@qoega  What is the current vision? Should I put _avoiding kinit usage_ in my TODO list?\r\nAre plans to \"move code ... to separate process\" realistic? Do we have issues or PRs on this?\r\nBTW, what is library bridge, coud you point it out please?\r\n",
  "created_at": "2022-06-15T17:11:33Z",
  "modified_files": [
    "docs/en/engines/table-engines/integrations/hdfs.md",
    "docs/en/engines/table-engines/integrations/kafka.md",
    "docs/ru/engines/table-engines/integrations/hdfs.md",
    "docs/ru/engines/table-engines/integrations/kafka.md",
    "src/Access/CMakeLists.txt",
    "b/src/Access/KerberosInit.cpp",
    "b/src/Access/KerberosInit.h",
    "b/src/Access/examples/CMakeLists.txt",
    "b/src/Access/examples/kerberos_init.cpp",
    "src/Storages/HDFS/HDFSCommon.cpp",
    "src/Storages/HDFS/HDFSCommon.h",
    "src/Storages/Kafka/StorageKafka.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_kerberized_hdfs/test.py",
    "tests/integration/test_storage_kerberized_kafka/test.py"
  ]
}