{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 58831,
  "instance_id": "ClickHouse__ClickHouse-58831",
  "issue_numbers": [
    "58496",
    "57931"
  ],
  "base_commit": "5ba7a78d234b4ddc9de9483be022ff133a4f04ef",
  "patch": "diff --git a/base/poco/Foundation/include/Poco/Logger.h b/base/poco/Foundation/include/Poco/Logger.h\nindex ffe3766dfec7..f91d836f1909 100644\n--- a/base/poco/Foundation/include/Poco/Logger.h\n+++ b/base/poco/Foundation/include/Poco/Logger.h\n@@ -33,7 +33,8 @@ namespace Poco\n \n \n class Exception;\n-\n+class Logger;\n+using LoggerPtr = std::shared_ptr<Logger>;\n \n class Foundation_API Logger : public Channel\n /// Logger is a special Channel that acts as the main\n@@ -870,6 +871,11 @@ class Foundation_API Logger : public Channel\n     /// If the Logger does not yet exist, it is created, based\n     /// on its parent logger.\n \n+    static LoggerPtr getShared(const std::string & name);\n+    /// Returns a shared pointer to the Logger with the given name.\n+    /// If the Logger does not yet exist, it is created, based\n+    /// on its parent logger.\n+\n     static Logger & unsafeGet(const std::string & name);\n     /// Returns a reference to the Logger with the given name.\n     /// If the Logger does not yet exist, it is created, based\n@@ -885,6 +891,11 @@ class Foundation_API Logger : public Channel\n     /// given name. The Logger's Channel and log level as set as\n     /// specified.\n \n+    static LoggerPtr createShared(const std::string & name, Channel * pChannel, int level = Message::PRIO_INFORMATION);\n+    /// Creates and returns a shared pointer to a Logger with the\n+    /// given name. The Logger's Channel and log level as set as\n+    /// specified.\n+\n     static Logger & root();\n     /// Returns a reference to the root logger, which is the ultimate\n     /// ancestor of all Loggers.\n@@ -893,7 +904,7 @@ class Foundation_API Logger : public Channel\n     /// Returns a pointer to the Logger with the given name if it\n     /// exists, or a null pointer otherwise.\n \n-    static void destroy(const std::string & name);\n+    static bool destroy(const std::string & name);\n     /// Destroys the logger with the specified name. Does nothing\n     /// if the logger is not found.\n     ///\n@@ -938,6 +949,7 @@ class Foundation_API Logger : public Channel\n     void log(const std::string & text, Message::Priority prio, const char * file, int line);\n \n     static std::string format(const std::string & fmt, int argc, std::string argv[]);\n+    static Logger & unsafeCreate(const std::string & name, Channel * pChannel, int level = Message::PRIO_INFORMATION);\n     static Logger & parent(const std::string & name);\n     static void add(Logger * pLogger);\n     static Logger * find(const std::string & name);\n@@ -952,7 +964,6 @@ class Foundation_API Logger : public Channel\n     std::atomic_int _level;\n \n     static LoggerMap * _pLoggerMap;\n-    static Mutex _mapMtx;\n };\n \n \ndiff --git a/base/poco/Foundation/include/Poco/RefCountedObject.h b/base/poco/Foundation/include/Poco/RefCountedObject.h\nindex 4ad32e30cad9..db966089e006 100644\n--- a/base/poco/Foundation/include/Poco/RefCountedObject.h\n+++ b/base/poco/Foundation/include/Poco/RefCountedObject.h\n@@ -38,15 +38,15 @@ class Foundation_API RefCountedObject\n     /// Creates the RefCountedObject.\n     /// The initial reference count is one.\n \n-    void duplicate() const;\n-    /// Increments the object's reference count.\n+    size_t duplicate() const;\n+    /// Increments the object's reference count, returns reference count before call.\n \n-    void release() const throw();\n+    size_t release() const throw();\n     /// Decrements the object's reference count\n     /// and deletes the object if the count\n-    /// reaches zero.\n+    /// reaches zero, returns reference count before call.\n \n-    int referenceCount() const;\n+    size_t referenceCount() const;\n     /// Returns the reference count.\n \n protected:\n@@ -57,36 +57,40 @@ class Foundation_API RefCountedObject\n     RefCountedObject(const RefCountedObject &);\n     RefCountedObject & operator=(const RefCountedObject &);\n \n-    mutable AtomicCounter _counter;\n+    mutable std::atomic<size_t> _counter;\n };\n \n \n //\n // inlines\n //\n-inline int RefCountedObject::referenceCount() const\n+inline size_t RefCountedObject::referenceCount() const\n {\n-    return _counter.value();\n+    return _counter.load(std::memory_order_acquire);\n }\n \n \n-inline void RefCountedObject::duplicate() const\n+inline size_t RefCountedObject::duplicate() const\n {\n-    ++_counter;\n+    return _counter.fetch_add(1, std::memory_order_acq_rel);\n }\n \n \n-inline void RefCountedObject::release() const throw()\n+inline size_t RefCountedObject::release() const throw()\n {\n+    size_t reference_count_before = _counter.fetch_sub(1, std::memory_order_acq_rel);\n+\n     try\n     {\n-        if (--_counter == 0)\n+        if (reference_count_before == 1)\n             delete this;\n     }\n     catch (...)\n     {\n         poco_unexpected();\n     }\n+\n+    return reference_count_before;\n }\n \n \ndiff --git a/base/poco/Foundation/src/Logger.cpp b/base/poco/Foundation/src/Logger.cpp\nindex 3d5de585b4f9..7c54116aaa4d 100644\n--- a/base/poco/Foundation/src/Logger.cpp\n+++ b/base/poco/Foundation/src/Logger.cpp\n@@ -20,12 +20,29 @@\n #include \"Poco/NumberParser.h\"\n #include \"Poco/String.h\"\n \n+#include <mutex>\n+\n+namespace\n+{\n+\n+std::mutex & getLoggerMutex()\n+{\n+\tauto get_logger_mutex_placeholder_memory = []()\n+\t{\n+\t\tstatic char buffer[sizeof(std::mutex)]{};\n+\t\treturn buffer;\n+\t};\n+\n+\tstatic std::mutex * logger_mutex = new (get_logger_mutex_placeholder_memory()) std::mutex();\n+\treturn *logger_mutex;\n+}\n+\n+}\n \n namespace Poco {\n \n \n Logger::LoggerMap* Logger::_pLoggerMap = 0;\n-Mutex Logger::_mapMtx;\n const std::string Logger::ROOT;\n \n \n@@ -73,7 +90,7 @@ void Logger::setProperty(const std::string& name, const std::string& value)\n \t\tsetChannel(LoggingRegistry::defaultRegistry().channelForName(value));\n \telse if (name == \"level\")\n \t\tsetLevel(value);\n-\telse \n+\telse\n \t\tChannel::setProperty(name, value);\n }\n \n@@ -112,14 +129,14 @@ void Logger::dump(const std::string& msg, const void* buffer, std::size_t length\n \n void Logger::setLevel(const std::string& name, int level)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \tif (_pLoggerMap)\n \t{\n \t\tstd::string::size_type len = name.length();\n \t\tfor (LoggerMap::iterator it = _pLoggerMap->begin(); it != _pLoggerMap->end(); ++it)\n \t\t{\n-\t\t\tif (len == 0 || \n+\t\t\tif (len == 0 ||\n \t\t\t\t(it->first.compare(0, len, name) == 0 && (it->first.length() == len || it->first[len] == '.')))\n \t\t\t{\n \t\t\t\tit->second->setLevel(level);\n@@ -131,7 +148,7 @@ void Logger::setLevel(const std::string& name, int level)\n \n void Logger::setChannel(const std::string& name, Channel* pChannel)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \tif (_pLoggerMap)\n \t{\n@@ -150,7 +167,7 @@ void Logger::setChannel(const std::string& name, Channel* pChannel)\n \n void Logger::setProperty(const std::string& loggerName, const std::string& propertyName, const std::string& value)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \tif (_pLoggerMap)\n \t{\n@@ -280,13 +297,41 @@ void Logger::formatDump(std::string& message, const void* buffer, std::size_t le\n }\n \n \n+namespace\n+{\n+\n+struct LoggerDeleter\n+{\n+\tvoid operator()(Poco::Logger * logger)\n+\t{\n+\t\tif (Logger::destroy(logger->name()))\n+\t\t\treturn;\n+\n+\t\tlogger->release();\n+\t}\n+};\n+\n+inline LoggerPtr makeLoggerPtr(Logger & logger)\n+{\n+\tlogger.duplicate();\n+\treturn std::shared_ptr<Logger>(&logger, LoggerDeleter());\n+}\n+\n+}\n+\n Logger& Logger::get(const std::string& name)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \treturn unsafeGet(name);\n }\n \n+LoggerPtr Logger::getShared(const std::string & name)\n+{\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n+\n+\treturn makeLoggerPtr(unsafeGet(name));\n+}\n \n Logger& Logger::unsafeGet(const std::string& name)\n {\n@@ -310,18 +355,21 @@ Logger& Logger::unsafeGet(const std::string& name)\n \n Logger& Logger::create(const std::string& name, Channel* pChannel, int level)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n-\tif (find(name)) throw ExistsException();\n-\tLogger* pLogger = new Logger(name, pChannel, level);\n-\tadd(pLogger);\n-\treturn *pLogger;\n+\treturn unsafeCreate(name, pChannel, level);\n }\n \n+LoggerPtr Logger::createShared(const std::string & name, Channel * pChannel, int level)\n+{\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n+\n+\treturn makeLoggerPtr(unsafeCreate(name, pChannel, level));\n+}\n \n Logger& Logger::root()\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \treturn unsafeGet(ROOT);\n }\n@@ -329,7 +377,7 @@ Logger& Logger::root()\n \n Logger* Logger::has(const std::string& name)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \treturn find(name);\n }\n@@ -337,7 +385,7 @@ Logger* Logger::has(const std::string& name)\n \n void Logger::shutdown()\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \tif (_pLoggerMap)\n \t{\n@@ -363,25 +411,29 @@ Logger* Logger::find(const std::string& name)\n }\n \n \n-void Logger::destroy(const std::string& name)\n+bool Logger::destroy(const std::string& name)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \tif (_pLoggerMap)\n \t{\n \t\tLoggerMap::iterator it = _pLoggerMap->find(name);\n \t\tif (it != _pLoggerMap->end())\n \t\t{\n-\t\t\tit->second->release();\n-\t\t\t_pLoggerMap->erase(it);\n+\t\t\tif (it->second->release() == 1)\n+\t\t\t\t_pLoggerMap->erase(it);\n+\n+\t\t\treturn true;\n \t\t}\n \t}\n+\n+\treturn false;\n }\n \n \n void Logger::names(std::vector<std::string>& names)\n {\n-\tMutex::ScopedLock lock(_mapMtx);\n+\tstd::lock_guard<std::mutex> lock(getLoggerMutex());\n \n \tnames.clear();\n \tif (_pLoggerMap)\n@@ -393,6 +445,14 @@ void Logger::names(std::vector<std::string>& names)\n \t}\n }\n \n+Logger& Logger::unsafeCreate(const std::string & name, Channel * pChannel, int level)\n+{\n+\tif (find(name)) throw ExistsException();\n+\tLogger* pLogger = new Logger(name, pChannel, level);\n+\tadd(pLogger);\n+\n+\treturn *pLogger;\n+}\n \n Logger& Logger::parent(const std::string& name)\n {\ndiff --git a/programs/copier/ClusterCopier.h b/programs/copier/ClusterCopier.h\nindex 063b13e90780..01f8b30f5463 100644\n--- a/programs/copier/ClusterCopier.h\n+++ b/programs/copier/ClusterCopier.h\n@@ -20,7 +20,7 @@ class ClusterCopier : WithMutableContext\n                   const String & host_id_,\n                   const String & proxy_database_name_,\n                   ContextMutablePtr context_,\n-                  Poco::Logger * log_)\n+                  LoggerRawPtr log_)\n             : WithMutableContext(context_),\n             task_zookeeper_path(task_path_),\n             host_id(host_id_),\n@@ -230,7 +230,7 @@ class ClusterCopier : WithMutableContext\n \n     bool experimental_use_sample_offset{false};\n \n-    Poco::Logger * log;\n+    LoggerRawPtr log;\n \n     UInt64 max_table_tries = 3;\n     UInt64 max_shard_partition_tries = 3;\ndiff --git a/programs/copier/ZooKeeperStaff.h b/programs/copier/ZooKeeperStaff.h\nindex 36dcfa508425..bbdec230d2df 100644\n--- a/programs/copier/ZooKeeperStaff.h\n+++ b/programs/copier/ZooKeeperStaff.h\n@@ -177,7 +177,7 @@ class CleanStateClock\n         auto watch_callback =\n                 [my_stale = stale] (const Coordination::WatchResponse & rsp)\n                 {\n-                    auto logger = &Poco::Logger::get(\"ClusterCopier\");\n+                    auto logger = getLogger(\"ClusterCopier\");\n                     if (rsp.error == Coordination::Error::ZOK)\n                     {\n                         switch (rsp.type)\ndiff --git a/programs/keeper-client/KeeperClient.cpp b/programs/keeper-client/KeeperClient.cpp\nindex 7ed4499efbd2..fa66a69687c2 100644\n--- a/programs/keeper-client/KeeperClient.cpp\n+++ b/programs/keeper-client/KeeperClient.cpp\n@@ -375,7 +375,7 @@ int KeeperClient::main(const std::vector<String> & /* args */)\n \n     if (!config().has(\"host\") && !config().has(\"port\") && !keys.empty())\n     {\n-        LOG_INFO(&Poco::Logger::get(\"KeeperClient\"), \"Found keeper node in the config.xml, will use it for connection\");\n+        LOG_INFO(getLogger(\"KeeperClient\"), \"Found keeper node in the config.xml, will use it for connection\");\n \n         for (const auto & key : keys)\n         {\ndiff --git a/programs/keeper-converter/KeeperConverter.cpp b/programs/keeper-converter/KeeperConverter.cpp\nindex 20448aafa2f1..2b2759412ab3 100644\n--- a/programs/keeper-converter/KeeperConverter.cpp\n+++ b/programs/keeper-converter/KeeperConverter.cpp\n@@ -28,7 +28,7 @@ int mainEntryClickHouseKeeperConverter(int argc, char ** argv)\n     po::store(po::command_line_parser(argc, argv).options(desc).run(), options);\n     Poco::AutoPtr<Poco::ConsoleChannel> console_channel(new Poco::ConsoleChannel);\n \n-    Poco::Logger * logger = &Poco::Logger::get(\"KeeperConverter\");\n+    LoggerPtr logger = getLogger(\"KeeperConverter\");\n     logger->setChannel(console_channel);\n \n     if (options.count(\"help\"))\ndiff --git a/programs/keeper/Keeper.cpp b/programs/keeper/Keeper.cpp\nindex 109884ec899b..c751702dc6fb 100644\n--- a/programs/keeper/Keeper.cpp\n+++ b/programs/keeper/Keeper.cpp\n@@ -624,7 +624,7 @@ catch (...)\n \n void Keeper::logRevision() const\n {\n-    LOG_INFO(&Poco::Logger::get(\"Application\"),\n+    LOG_INFO(getLogger(\"Application\"),\n         \"Starting ClickHouse Keeper {} (revision: {}, git hash: {}, build id: {}), PID {}\",\n         VERSION_STRING,\n         ClickHouseRevision::getVersionRevision(),\ndiff --git a/programs/library-bridge/CatBoostLibraryHandlerFactory.cpp b/programs/library-bridge/CatBoostLibraryHandlerFactory.cpp\nindex 6ee078f6c5cf..7ce896636e70 100644\n--- a/programs/library-bridge/CatBoostLibraryHandlerFactory.cpp\n+++ b/programs/library-bridge/CatBoostLibraryHandlerFactory.cpp\n@@ -13,7 +13,7 @@ CatBoostLibraryHandlerFactory & CatBoostLibraryHandlerFactory::instance()\n }\n \n CatBoostLibraryHandlerFactory::CatBoostLibraryHandlerFactory()\n-    : log(&Poco::Logger::get(\"CatBoostLibraryHandlerFactory\"))\n+    : log(getLogger(\"CatBoostLibraryHandlerFactory\"))\n {\n }\n \ndiff --git a/programs/library-bridge/CatBoostLibraryHandlerFactory.h b/programs/library-bridge/CatBoostLibraryHandlerFactory.h\nindex 6ba3fe84ec9e..e29834cbe791 100644\n--- a/programs/library-bridge/CatBoostLibraryHandlerFactory.h\n+++ b/programs/library-bridge/CatBoostLibraryHandlerFactory.h\n@@ -31,7 +31,7 @@ class CatBoostLibraryHandlerFactory final : private boost::noncopyable\n     /// map: model path --> catboost library handler\n     std::unordered_map<String, CatBoostLibraryHandlerPtr> library_handlers TSA_GUARDED_BY(mutex);\n     std::mutex mutex;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/programs/library-bridge/ExternalDictionaryLibraryAPI.cpp b/programs/library-bridge/ExternalDictionaryLibraryAPI.cpp\nindex 70cd6fca3751..4fa5c991f0f8 100644\n--- a/programs/library-bridge/ExternalDictionaryLibraryAPI.cpp\n+++ b/programs/library-bridge/ExternalDictionaryLibraryAPI.cpp\n@@ -9,40 +9,40 @@ const char DICT_LOGGER_NAME[] = \"LibraryDictionarySourceExternal\";\n \n void ExternalDictionaryLibraryAPI::log(LogLevel level, CString msg)\n {\n-    auto & logger = Poco::Logger::get(DICT_LOGGER_NAME);\n+    auto logger = getLogger(DICT_LOGGER_NAME);\n     switch (level)\n     {\n         case LogLevel::TRACE:\n-            if (logger.trace())\n-                logger.trace(msg);\n+            if (logger->trace())\n+                logger->trace(msg);\n             break;\n         case LogLevel::DEBUG:\n-            if (logger.debug())\n-                logger.debug(msg);\n+            if (logger->debug())\n+                logger->debug(msg);\n             break;\n         case LogLevel::INFORMATION:\n-            if (logger.information())\n-                logger.information(msg);\n+            if (logger->information())\n+                logger->information(msg);\n             break;\n         case LogLevel::NOTICE:\n-            if (logger.notice())\n-                logger.notice(msg);\n+            if (logger->notice())\n+                logger->notice(msg);\n             break;\n         case LogLevel::WARNING:\n-            if (logger.warning())\n-                logger.warning(msg);\n+            if (logger->warning())\n+                logger->warning(msg);\n             break;\n         case LogLevel::ERROR:\n-            if (logger.error())\n-                logger.error(msg);\n+            if (logger->error())\n+                logger->error(msg);\n             break;\n         case LogLevel::CRITICAL:\n-            if (logger.critical())\n-                logger.critical(msg);\n+            if (logger->critical())\n+                logger->critical(msg);\n             break;\n         case LogLevel::FATAL:\n-            if (logger.fatal())\n-                logger.fatal(msg);\n+            if (logger->fatal())\n+                logger->fatal(msg);\n             break;\n     }\n }\ndiff --git a/programs/library-bridge/ExternalDictionaryLibraryHandlerFactory.cpp b/programs/library-bridge/ExternalDictionaryLibraryHandlerFactory.cpp\nindex 6acd9af20ed8..1b2b57beeb11 100644\n--- a/programs/library-bridge/ExternalDictionaryLibraryHandlerFactory.cpp\n+++ b/programs/library-bridge/ExternalDictionaryLibraryHandlerFactory.cpp\n@@ -26,7 +26,7 @@ void ExternalDictionaryLibraryHandlerFactory::create(\n \n     if (library_handlers.contains(dictionary_id))\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"ExternalDictionaryLibraryHandlerFactory\"), \"Library handler with dictionary id {} already exists\", dictionary_id);\n+        LOG_WARNING(getLogger(\"ExternalDictionaryLibraryHandlerFactory\"), \"Library handler with dictionary id {} already exists\", dictionary_id);\n         return;\n     }\n \ndiff --git a/programs/library-bridge/LibraryBridgeHandlerFactory.cpp b/programs/library-bridge/LibraryBridgeHandlerFactory.cpp\nindex 4af1f8355e80..e5ab22f2d40d 100644\n--- a/programs/library-bridge/LibraryBridgeHandlerFactory.cpp\n+++ b/programs/library-bridge/LibraryBridgeHandlerFactory.cpp\n@@ -12,7 +12,7 @@ LibraryBridgeHandlerFactory::LibraryBridgeHandlerFactory(\n     size_t keep_alive_timeout_,\n     ContextPtr context_)\n     : WithContext(context_)\n-    , log(&Poco::Logger::get(name_))\n+    , log(getLogger(name_))\n     , name(name_)\n     , keep_alive_timeout(keep_alive_timeout_)\n {\ndiff --git a/programs/library-bridge/LibraryBridgeHandlerFactory.h b/programs/library-bridge/LibraryBridgeHandlerFactory.h\nindex 7565052c4cbe..5b0f088bc296 100644\n--- a/programs/library-bridge/LibraryBridgeHandlerFactory.h\n+++ b/programs/library-bridge/LibraryBridgeHandlerFactory.h\n@@ -19,7 +19,7 @@ class LibraryBridgeHandlerFactory : public HTTPRequestHandlerFactory, WithContex\n     std::unique_ptr<HTTPRequestHandler> createRequestHandler(const HTTPServerRequest & request) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const std::string name;\n     const size_t keep_alive_timeout;\n };\ndiff --git a/programs/library-bridge/LibraryBridgeHandlers.cpp b/programs/library-bridge/LibraryBridgeHandlers.cpp\nindex b0b465460e08..ab146f458df9 100644\n--- a/programs/library-bridge/LibraryBridgeHandlers.cpp\n+++ b/programs/library-bridge/LibraryBridgeHandlers.cpp\n@@ -47,7 +47,7 @@ namespace\n         if (!response.sent())\n             *response.send() << message << '\\n';\n \n-        LOG_WARNING(&Poco::Logger::get(\"LibraryBridge\"), fmt::runtime(message));\n+        LOG_WARNING(getLogger(\"LibraryBridge\"), fmt::runtime(message));\n     }\n \n     std::shared_ptr<Block> parseColumns(String && column_string)\n@@ -92,7 +92,7 @@ static void writeData(Block data, OutputFormatPtr format)\n ExternalDictionaryLibraryBridgeRequestHandler::ExternalDictionaryLibraryBridgeRequestHandler(size_t keep_alive_timeout_, ContextPtr context_)\n     : WithContext(context_)\n     , keep_alive_timeout(keep_alive_timeout_)\n-    , log(&Poco::Logger::get(\"ExternalDictionaryLibraryBridgeRequestHandler\"))\n+    , log(getLogger(\"ExternalDictionaryLibraryBridgeRequestHandler\"))\n {\n }\n \n@@ -380,7 +380,7 @@ void ExternalDictionaryLibraryBridgeRequestHandler::handleRequest(HTTPServerRequ\n ExternalDictionaryLibraryBridgeExistsHandler::ExternalDictionaryLibraryBridgeExistsHandler(size_t keep_alive_timeout_, ContextPtr context_)\n     : WithContext(context_)\n     , keep_alive_timeout(keep_alive_timeout_)\n-    , log(&Poco::Logger::get(\"ExternalDictionaryLibraryBridgeExistsHandler\"))\n+    , log(getLogger(\"ExternalDictionaryLibraryBridgeExistsHandler\"))\n {\n }\n \n@@ -419,7 +419,7 @@ CatBoostLibraryBridgeRequestHandler::CatBoostLibraryBridgeRequestHandler(\n     size_t keep_alive_timeout_, ContextPtr context_)\n     : WithContext(context_)\n     , keep_alive_timeout(keep_alive_timeout_)\n-    , log(&Poco::Logger::get(\"CatBoostLibraryBridgeRequestHandler\"))\n+    , log(getLogger(\"CatBoostLibraryBridgeRequestHandler\"))\n {\n }\n \n@@ -623,7 +623,7 @@ void CatBoostLibraryBridgeRequestHandler::handleRequest(HTTPServerRequest & requ\n CatBoostLibraryBridgeExistsHandler::CatBoostLibraryBridgeExistsHandler(size_t keep_alive_timeout_, ContextPtr context_)\n     : WithContext(context_)\n     , keep_alive_timeout(keep_alive_timeout_)\n-    , log(&Poco::Logger::get(\"CatBoostLibraryBridgeExistsHandler\"))\n+    , log(getLogger(\"CatBoostLibraryBridgeExistsHandler\"))\n {\n }\n \ndiff --git a/programs/library-bridge/LibraryBridgeHandlers.h b/programs/library-bridge/LibraryBridgeHandlers.h\nindex 4f08d7a60840..1db71eb24cb8 100644\n--- a/programs/library-bridge/LibraryBridgeHandlers.h\n+++ b/programs/library-bridge/LibraryBridgeHandlers.h\n@@ -26,7 +26,7 @@ class ExternalDictionaryLibraryBridgeRequestHandler : public HTTPRequestHandler,\n     static constexpr inline auto FORMAT = \"RowBinary\";\n \n     const size_t keep_alive_timeout;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \n@@ -40,7 +40,7 @@ class ExternalDictionaryLibraryBridgeExistsHandler : public HTTPRequestHandler,\n \n private:\n     const size_t keep_alive_timeout;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \n@@ -69,7 +69,7 @@ class CatBoostLibraryBridgeRequestHandler : public HTTPRequestHandler, WithConte\n \n private:\n     const size_t keep_alive_timeout;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \n@@ -83,7 +83,7 @@ class CatBoostLibraryBridgeExistsHandler : public HTTPRequestHandler, WithContex\n \n private:\n     const size_t keep_alive_timeout;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/programs/local/LocalServer.cpp b/programs/local/LocalServer.cpp\nindex 4e0b9eeb731a..443d4a52fa3e 100644\n--- a/programs/local/LocalServer.cpp\n+++ b/programs/local/LocalServer.cpp\n@@ -221,7 +221,7 @@ void LocalServer::tryInitPath()\n     {\n         // The path is not provided explicitly - use a unique path in the system temporary directory\n         // (or in the current dir if temporary don't exist)\n-        Poco::Logger * log = &logger();\n+        LoggerRawPtr log = &logger();\n         std::filesystem::path parent_folder;\n         std::filesystem::path default_path;\n \n@@ -631,7 +631,7 @@ void LocalServer::processConfig()\n \n     tryInitPath();\n \n-    Poco::Logger * log = &logger();\n+    LoggerRawPtr log = &logger();\n \n     /// Maybe useless\n     if (config().has(\"macros\"))\ndiff --git a/programs/odbc-bridge/ColumnInfoHandler.h b/programs/odbc-bridge/ColumnInfoHandler.h\nindex e3087701182c..ca7044fdf328 100644\n--- a/programs/odbc-bridge/ColumnInfoHandler.h\n+++ b/programs/odbc-bridge/ColumnInfoHandler.h\n@@ -18,7 +18,7 @@ class ODBCColumnsInfoHandler : public HTTPRequestHandler, WithContext\n public:\n     ODBCColumnsInfoHandler(size_t keep_alive_timeout_, ContextPtr context_)\n         : WithContext(context_)\n-        , log(&Poco::Logger::get(\"ODBCColumnsInfoHandler\"))\n+        , log(getLogger(\"ODBCColumnsInfoHandler\"))\n         , keep_alive_timeout(keep_alive_timeout_)\n     {\n     }\n@@ -26,7 +26,7 @@ class ODBCColumnsInfoHandler : public HTTPRequestHandler, WithContext\n     void handleRequest(HTTPServerRequest & request, HTTPServerResponse & response, const ProfileEvents::Event & write_event) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     size_t keep_alive_timeout;\n };\n \ndiff --git a/programs/odbc-bridge/IdentifierQuoteHandler.h b/programs/odbc-bridge/IdentifierQuoteHandler.h\nindex ff5c02ca07b3..7b78c5b4b93c 100644\n--- a/programs/odbc-bridge/IdentifierQuoteHandler.h\n+++ b/programs/odbc-bridge/IdentifierQuoteHandler.h\n@@ -16,7 +16,7 @@ class IdentifierQuoteHandler : public HTTPRequestHandler, WithContext\n public:\n     IdentifierQuoteHandler(size_t keep_alive_timeout_, ContextPtr context_)\n         : WithContext(context_)\n-        , log(&Poco::Logger::get(\"IdentifierQuoteHandler\"))\n+        , log(getLogger(\"IdentifierQuoteHandler\"))\n         , keep_alive_timeout(keep_alive_timeout_)\n     {\n     }\n@@ -24,7 +24,7 @@ class IdentifierQuoteHandler : public HTTPRequestHandler, WithContext\n     void handleRequest(HTTPServerRequest & request, HTTPServerResponse & response, const ProfileEvents::Event & write_event) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     size_t keep_alive_timeout;\n };\n \ndiff --git a/programs/odbc-bridge/MainHandler.h b/programs/odbc-bridge/MainHandler.h\nindex 7977245ff821..ed0c6b2e28c3 100644\n--- a/programs/odbc-bridge/MainHandler.h\n+++ b/programs/odbc-bridge/MainHandler.h\n@@ -24,7 +24,7 @@ class ODBCHandler : public HTTPRequestHandler, WithContext\n         ContextPtr context_,\n         const String & mode_)\n         : WithContext(context_)\n-        , log(&Poco::Logger::get(\"ODBCHandler\"))\n+        , log(getLogger(\"ODBCHandler\"))\n         , keep_alive_timeout(keep_alive_timeout_)\n         , mode(mode_)\n     {\n@@ -33,7 +33,7 @@ class ODBCHandler : public HTTPRequestHandler, WithContext\n     void handleRequest(HTTPServerRequest & request, HTTPServerResponse & response, const ProfileEvents::Event & write_event) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     size_t keep_alive_timeout;\n     String mode;\ndiff --git a/programs/odbc-bridge/ODBCBlockInputStream.cpp b/programs/odbc-bridge/ODBCBlockInputStream.cpp\nindex 3aa3d9a652b6..c46144c3dc83 100644\n--- a/programs/odbc-bridge/ODBCBlockInputStream.cpp\n+++ b/programs/odbc-bridge/ODBCBlockInputStream.cpp\n@@ -23,7 +23,7 @@ namespace ErrorCodes\n ODBCSource::ODBCSource(\n     nanodbc::ConnectionHolderPtr connection_holder, const std::string & query_str, const Block & sample_block, const UInt64 max_block_size_)\n     : ISource(sample_block)\n-    , log(&Poco::Logger::get(\"ODBCSource\"))\n+    , log(getLogger(\"ODBCSource\"))\n     , max_block_size{max_block_size_}\n     , query(query_str)\n {\ndiff --git a/programs/odbc-bridge/ODBCBlockInputStream.h b/programs/odbc-bridge/ODBCBlockInputStream.h\nindex 79d5816ad014..dedd98f930f5 100644\n--- a/programs/odbc-bridge/ODBCBlockInputStream.h\n+++ b/programs/odbc-bridge/ODBCBlockInputStream.h\n@@ -30,7 +30,7 @@ class ODBCSource final : public ISource\n         column.insertFrom(sample_column, 0);\n     }\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const UInt64 max_block_size;\n     ExternalResultDescription description;\n \ndiff --git a/programs/odbc-bridge/ODBCBlockOutputStream.cpp b/programs/odbc-bridge/ODBCBlockOutputStream.cpp\nindex eb5901ad3e1f..87c09d1e7571 100644\n--- a/programs/odbc-bridge/ODBCBlockOutputStream.cpp\n+++ b/programs/odbc-bridge/ODBCBlockOutputStream.cpp\n@@ -19,7 +19,7 @@ ODBCSink::ODBCSink(\n     ContextPtr local_context_,\n     IdentifierQuotingStyle quoting_)\n     : ISink(sample_block_)\n-    , log(&Poco::Logger::get(\"ODBCSink\"))\n+    , log(getLogger(\"ODBCSink\"))\n     , connection_holder(std::move(connection_holder_))\n     , db_name(remote_database_name_)\n     , table_name(remote_table_name_)\ndiff --git a/programs/odbc-bridge/ODBCBlockOutputStream.h b/programs/odbc-bridge/ODBCBlockOutputStream.h\nindex f5e7b4e3a2d5..06edce92e1a6 100644\n--- a/programs/odbc-bridge/ODBCBlockOutputStream.h\n+++ b/programs/odbc-bridge/ODBCBlockOutputStream.h\n@@ -30,7 +30,7 @@ using ValueType = ExternalResultDescription::ValueType;\n     void consume(Chunk chunk) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     nanodbc::ConnectionHolderPtr connection_holder;\n     std::string db_name;\ndiff --git a/programs/odbc-bridge/ODBCHandlerFactory.cpp b/programs/odbc-bridge/ODBCHandlerFactory.cpp\nindex dd21358df8c3..eebb0c24c7a8 100644\n--- a/programs/odbc-bridge/ODBCHandlerFactory.cpp\n+++ b/programs/odbc-bridge/ODBCHandlerFactory.cpp\n@@ -11,7 +11,7 @@ namespace DB\n \n ODBCBridgeHandlerFactory::ODBCBridgeHandlerFactory(const std::string & name_, size_t keep_alive_timeout_, ContextPtr context_)\n     : WithContext(context_)\n-    , log(&Poco::Logger::get(name_))\n+    , log(getLogger(name_))\n     , name(name_)\n     , keep_alive_timeout(keep_alive_timeout_)\n {\ndiff --git a/programs/odbc-bridge/ODBCHandlerFactory.h b/programs/odbc-bridge/ODBCHandlerFactory.h\nindex 3e3da7c9f246..4aaf1b55453c 100644\n--- a/programs/odbc-bridge/ODBCHandlerFactory.h\n+++ b/programs/odbc-bridge/ODBCHandlerFactory.h\n@@ -22,7 +22,7 @@ class ODBCBridgeHandlerFactory : public HTTPRequestHandlerFactory, WithContext\n     std::unique_ptr<HTTPRequestHandler> createRequestHandler(const HTTPServerRequest & request) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string name;\n     size_t keep_alive_timeout;\n };\ndiff --git a/programs/odbc-bridge/ODBCPooledConnectionFactory.h b/programs/odbc-bridge/ODBCPooledConnectionFactory.h\nindex a10055c66598..b70e45f2b9db 100644\n--- a/programs/odbc-bridge/ODBCPooledConnectionFactory.h\n+++ b/programs/odbc-bridge/ODBCPooledConnectionFactory.h\n@@ -97,7 +97,7 @@ T execute(nanodbc::ConnectionHolderPtr connection_holder, std::function<T(nanodb\n         /// https://docs.microsoft.com/ru-ru/sql/odbc/reference/appendixes/appendix-a-odbc-error-codes?view=sql-server-ver15\n         bool is_retriable = e.state().starts_with(\"08\") || e.state().starts_with(\"24\") || e.state().starts_with(\"25\");\n         LOG_ERROR(\n-            &Poco::Logger::get(\"ODBCConnection\"),\n+            getLogger(\"ODBCConnection\"),\n             \"ODBC query failed with error: {}, state: {}, native code: {}{}\",\n             e.what(), e.state(), e.native(), is_retriable ? \", will retry\" : \"\");\n \ndiff --git a/programs/odbc-bridge/SchemaAllowedHandler.h b/programs/odbc-bridge/SchemaAllowedHandler.h\nindex aa0b04b1d314..8dc725dbb33b 100644\n--- a/programs/odbc-bridge/SchemaAllowedHandler.h\n+++ b/programs/odbc-bridge/SchemaAllowedHandler.h\n@@ -19,7 +19,7 @@ class SchemaAllowedHandler : public HTTPRequestHandler, WithContext\n public:\n     SchemaAllowedHandler(size_t keep_alive_timeout_, ContextPtr context_)\n         : WithContext(context_)\n-        , log(&Poco::Logger::get(\"SchemaAllowedHandler\"))\n+        , log(getLogger(\"SchemaAllowedHandler\"))\n         , keep_alive_timeout(keep_alive_timeout_)\n     {\n     }\n@@ -27,7 +27,7 @@ class SchemaAllowedHandler : public HTTPRequestHandler, WithContext\n     void handleRequest(HTTPServerRequest & request, HTTPServerResponse & response, const ProfileEvents::Event & write_event) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     size_t keep_alive_timeout;\n };\n \ndiff --git a/programs/odbc-bridge/getIdentifierQuote.cpp b/programs/odbc-bridge/getIdentifierQuote.cpp\nindex 793e398363ce..15bd055e6153 100644\n--- a/programs/odbc-bridge/getIdentifierQuote.cpp\n+++ b/programs/odbc-bridge/getIdentifierQuote.cpp\n@@ -26,7 +26,7 @@ std::string getIdentifierQuote(nanodbc::ConnectionHolderPtr connection_holder)\n     }\n     catch (...)\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"ODBCGetIdentifierQuote\"), \"Cannot fetch identifier quote. Default double quote is used. Reason: {}\", getCurrentExceptionMessage(false));\n+        LOG_WARNING(getLogger(\"ODBCGetIdentifierQuote\"), \"Cannot fetch identifier quote. Default double quote is used. Reason: {}\", getCurrentExceptionMessage(false));\n         return \"\\\"\";\n     }\n \ndiff --git a/programs/server/Server.cpp b/programs/server/Server.cpp\nindex 96f3119e0733..8a0357771d27 100644\n--- a/programs/server/Server.cpp\n+++ b/programs/server/Server.cpp\n@@ -365,7 +365,7 @@ void Server::createServer(\n namespace\n {\n \n-void setOOMScore(int value, Poco::Logger * log)\n+void setOOMScore(int value, LoggerRawPtr log)\n {\n     try\n     {\n@@ -450,7 +450,7 @@ void checkForUsersNotInMainConfig(\n     const Poco::Util::AbstractConfiguration & config,\n     const std::string & config_path,\n     const std::string & users_config_path,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     if (config.getBool(\"skip_check_for_incorrect_settings\", false))\n         return;\n@@ -2491,7 +2491,7 @@ void Server::stopServers(\n     const ServerType & server_type\n ) const\n {\n-    Poco::Logger * log = &logger();\n+    LoggerRawPtr log = &logger();\n \n     /// Remove servers once all their connections are closed\n     auto check_server = [&log](const char prefix[], auto & server)\n@@ -2530,7 +2530,7 @@ void Server::updateServers(\n     std::vector<ProtocolServerAdapter> & servers,\n     std::vector<ProtocolServerAdapter> & servers_to_start_before_tables)\n {\n-    Poco::Logger * log = &logger();\n+    LoggerRawPtr log = &logger();\n \n     const auto listen_hosts = getListenHosts(config);\n     const auto interserver_listen_hosts = getInterserverListenHosts(config);\ndiff --git a/src/Access/AccessControl.cpp b/src/Access/AccessControl.cpp\nindex 5de405d9f8fe..71ad219110df 100644\n--- a/src/Access/AccessControl.cpp\n+++ b/src/Access/AccessControl.cpp\n@@ -47,7 +47,7 @@ namespace\n         const Poco::Util::AbstractConfiguration & config,\n         const std::string & config_path,\n         const std::string & users_config_path,\n-        Poco::Logger * log)\n+        LoggerPtr log)\n     {\n         if (config.getBool(\"skip_check_for_incorrect_settings\", false))\n             return;\ndiff --git a/src/Access/AccessRights.cpp b/src/Access/AccessRights.cpp\nindex 520a11bd696c..72cbeca4f114 100644\n--- a/src/Access/AccessRights.cpp\n+++ b/src/Access/AccessRights.cpp\n@@ -443,7 +443,7 @@ struct AccessRights::Node\n             optimizeTree();\n     }\n \n-    void logTree(Poco::Logger * log, const String & title) const\n+    void logTree(LoggerPtr log, const String & title) const\n     {\n         LOG_TRACE(log, \"Tree({}): level={}, name={}, flags={}, min_flags={}, max_flags={}, num_children={}\",\n             title, level, node_name ? *node_name : \"NULL\", flags.toString(),\n@@ -1158,7 +1158,7 @@ AccessRights AccessRights::getFullAccess()\n \n void AccessRights::logTree() const\n {\n-    auto * log = &Poco::Logger::get(\"AccessRights\");\n+    auto log = getLogger(\"AccessRights\");\n     if (root)\n     {\n         root->logTree(log, \"\");\ndiff --git a/src/Access/Common/AllowedClientHosts.cpp b/src/Access/Common/AllowedClientHosts.cpp\nindex 801ccd3748b9..c677465a7a1c 100644\n--- a/src/Access/Common/AllowedClientHosts.cpp\n+++ b/src/Access/Common/AllowedClientHosts.cpp\n@@ -514,7 +514,7 @@ bool AllowedClientHosts::contains(const IPAddress & client_address) const\n                 throw;\n             /// Try to ignore DNS errors: if host cannot be resolved, skip it and try next.\n             LOG_WARNING(\n-                &Poco::Logger::get(\"AddressPatterns\"),\n+                getLogger(\"AddressPatterns\"),\n                 \"Failed to check if the allowed client hosts contain address {}. {}, code = {}\",\n                 client_address.toString(), e.displayText(), e.code());\n             return false;\n@@ -556,7 +556,7 @@ bool AllowedClientHosts::contains(const IPAddress & client_address) const\n                 throw;\n             /// Try to ignore DNS errors: if host cannot be resolved, skip it and try next.\n             LOG_WARNING(\n-                &Poco::Logger::get(\"AddressPatterns\"),\n+                getLogger(\"AddressPatterns\"),\n                 \"Failed to check if the allowed client hosts contain address {}. {}, code = {}\",\n                 client_address.toString(), e.displayText(), e.code());\n             return false;\ndiff --git a/src/Access/ContextAccess.cpp b/src/Access/ContextAccess.cpp\nindex 567b131c00e3..0943e797e3ff 100644\n--- a/src/Access/ContextAccess.cpp\n+++ b/src/Access/ContextAccess.cpp\n@@ -298,7 +298,7 @@ void ContextAccess::setUser(const UserPtr & user_) const\n     }\n \n     user_name = user->getName();\n-    trace_log = &Poco::Logger::get(\"ContextAccess (\" + user_name + \")\");\n+    trace_log = getLogger(\"ContextAccess (\" + user_name + \")\");\n \n     std::vector<UUID> current_roles, current_roles_with_admin_option;\n     if (params.use_default_roles)\ndiff --git a/src/Access/ContextAccess.h b/src/Access/ContextAccess.h\nindex caf903b85bd9..237c423d2618 100644\n--- a/src/Access/ContextAccess.h\n+++ b/src/Access/ContextAccess.h\n@@ -185,9 +185,10 @@ class ContextAccess : public std::enable_shared_from_this<ContextAccess>\n \n     mutable std::atomic<bool> initialized = false; // can be removed after Bug 5504 is resolved\n     mutable std::atomic<bool> user_was_dropped = false;\n-    mutable std::atomic<Poco::Logger *> trace_log = nullptr;\n \n     mutable std::mutex mutex;\n+    /// TODO: Fix race\n+    mutable LoggerPtr trace_log;\n     mutable UserPtr user TSA_GUARDED_BY(mutex);\n     mutable String user_name TSA_GUARDED_BY(mutex);\n     mutable scope_guard subscription_for_user_change TSA_GUARDED_BY(mutex);\ndiff --git a/src/Access/DiskAccessStorage.cpp b/src/Access/DiskAccessStorage.cpp\nindex 190c7567b854..3c20ef3d102e 100644\n--- a/src/Access/DiskAccessStorage.cpp\n+++ b/src/Access/DiskAccessStorage.cpp\n@@ -47,7 +47,7 @@ namespace\n     }\n \n \n-    AccessEntityPtr tryReadEntityFile(const String & file_path, Poco::Logger & log)\n+    AccessEntityPtr tryReadEntityFile(const String & file_path, LoggerPtr log)\n     {\n         try\n         {\n@@ -55,7 +55,7 @@ namespace\n         }\n         catch (...)\n         {\n-            tryLogCurrentException(&log);\n+            tryLogCurrentException(log);\n             return nullptr;\n         }\n     }\n@@ -378,7 +378,7 @@ void DiskAccessStorage::reloadAllAndRebuildLists()\n             continue;\n \n         const auto access_entity_file_path = getEntityFilePath(directory_path, id);\n-        auto entity = tryReadEntityFile(access_entity_file_path, *getLogger());\n+        auto entity = tryReadEntityFile(access_entity_file_path, getLogger());\n         if (!entity)\n             continue;\n \ndiff --git a/src/Access/ExternalAuthenticators.cpp b/src/Access/ExternalAuthenticators.cpp\nindex 351bcb95c737..77812ac5eb5d 100644\n--- a/src/Access/ExternalAuthenticators.cpp\n+++ b/src/Access/ExternalAuthenticators.cpp\n@@ -279,7 +279,7 @@ void ExternalAuthenticators::reset()\n     resetImpl();\n }\n \n-void ExternalAuthenticators::setConfiguration(const Poco::Util::AbstractConfiguration & config, Poco::Logger * log)\n+void ExternalAuthenticators::setConfiguration(const Poco::Util::AbstractConfiguration & config, LoggerPtr log)\n {\n     std::lock_guard lock(mutex);\n     resetImpl();\ndiff --git a/src/Access/ExternalAuthenticators.h b/src/Access/ExternalAuthenticators.h\nindex 46c51f0d2f30..3a710e6df26a 100644\n--- a/src/Access/ExternalAuthenticators.h\n+++ b/src/Access/ExternalAuthenticators.h\n@@ -36,7 +36,7 @@ class ExternalAuthenticators\n {\n public:\n     void reset();\n-    void setConfiguration(const Poco::Util::AbstractConfiguration & config, Poco::Logger * log);\n+    void setConfiguration(const Poco::Util::AbstractConfiguration & config, LoggerPtr log);\n \n     // The name and readiness of the credentials must be verified before calling these.\n     bool checkLDAPCredentials(const String & server, const BasicCredentials & credentials,\ndiff --git a/src/Access/GSSAcceptor.cpp b/src/Access/GSSAcceptor.cpp\nindex 02946f0d74da..cfa1af6a2004 100644\n--- a/src/Access/GSSAcceptor.cpp\n+++ b/src/Access/GSSAcceptor.cpp\n@@ -328,7 +328,7 @@ void GSSAcceptorContext::initHandles()\n     }\n }\n \n-String GSSAcceptorContext::processToken(const String & input_token, Poco::Logger * log)\n+String GSSAcceptorContext::processToken(const String & input_token, LoggerPtr log)\n {\n     std::lock_guard lock(gss_global_mutex);\n \n@@ -455,7 +455,7 @@ void GSSAcceptorContext::initHandles()\n {\n }\n \n-String GSSAcceptorContext::processToken(const String &, Poco::Logger *)\n+String GSSAcceptorContext::processToken(const String &, LoggerPtr)\n {\n     throw Exception(ErrorCodes::FEATURE_IS_NOT_ENABLED_AT_BUILD_TIME, \"ClickHouse was built without GSS-API/Kerberos support\");\n }\ndiff --git a/src/Access/GSSAcceptor.h b/src/Access/GSSAcceptor.h\nindex ba448ae474e8..8d490fb47ae5 100644\n--- a/src/Access/GSSAcceptor.h\n+++ b/src/Access/GSSAcceptor.h\n@@ -3,6 +3,7 @@\n #include \"config.h\"\n \n #include <Access/Credentials.h>\n+#include <Common/Logger.h>\n #include <base/types.h>\n #include <memory>\n \n@@ -42,7 +43,7 @@ class GSSAcceptorContext\n \n     const String & getRealm() const;\n     bool isFailed() const;\n-    MAYBE_NORETURN String processToken(const String & input_token, Poco::Logger * log);\n+    MAYBE_NORETURN String processToken(const String & input_token, LoggerPtr log);\n \n private:\n     void reset();\ndiff --git a/src/Access/IAccessStorage.cpp b/src/Access/IAccessStorage.cpp\nindex 222f38b41b69..fbe9e2310028 100644\n--- a/src/Access/IAccessStorage.cpp\n+++ b/src/Access/IAccessStorage.cpp\n@@ -6,6 +6,7 @@\n #include <Backups/BackupEntriesCollector.h>\n #include <Common/Exception.h>\n #include <Common/quoteString.h>\n+#include <Common/callOnce.h>\n #include <IO/WriteHelpers.h>\n #include <Interpreters/Context.h>\n #include <Poco/UUIDGenerator.h>\n@@ -615,7 +616,7 @@ UUID IAccessStorage::generateRandomID()\n }\n \n \n-void IAccessStorage::clearConflictsInEntitiesList(std::vector<std::pair<UUID, AccessEntityPtr>> & entities, const Poco::Logger * log_)\n+void IAccessStorage::clearConflictsInEntitiesList(std::vector<std::pair<UUID, AccessEntityPtr>> & entities, const LoggerPtr log_)\n {\n     std::unordered_map<UUID, size_t> positions_by_id;\n     std::unordered_map<std::string_view, size_t> positions_by_type_and_name[static_cast<size_t>(AccessEntityType::MAX)];\n@@ -671,12 +672,13 @@ void IAccessStorage::clearConflictsInEntitiesList(std::vector<std::pair<UUID, Ac\n }\n \n \n-Poco::Logger * IAccessStorage::getLogger() const\n+LoggerPtr IAccessStorage::getLogger() const\n {\n-    Poco::Logger * ptr = log.load();\n-    if (!ptr)\n-        log.store(ptr = &Poco::Logger::get(\"Access(\" + storage_name + \")\"), std::memory_order_relaxed);\n-    return ptr;\n+    callOnce(log_initialized, [&] {\n+        log = ::getLogger(\"Access(\" + storage_name + \")\");\n+    });\n+\n+    return log;\n }\n \n \ndiff --git a/src/Access/IAccessStorage.h b/src/Access/IAccessStorage.h\nindex 797318438e11..5ac66fc9b8a0 100644\n--- a/src/Access/IAccessStorage.h\n+++ b/src/Access/IAccessStorage.h\n@@ -6,6 +6,7 @@\n #include <Parsers/IParser.h>\n #include <Parsers/parseIdentifierOrStringLiteral.h>\n #include <Common/SettingsChanges.h>\n+#include <Common/callOnce.h>\n \n #include <atomic>\n #include <functional>\n@@ -225,9 +226,9 @@ class IAccessStorage : public boost::noncopyable\n         SettingsChanges & settings) const;\n     virtual bool isAddressAllowed(const User & user, const Poco::Net::IPAddress & address) const;\n     static UUID generateRandomID();\n-    Poco::Logger * getLogger() const;\n+    LoggerPtr getLogger() const;\n     static String formatEntityTypeWithName(AccessEntityType type, const String & name) { return AccessEntityTypeInfo::get(type).formatEntityNameWithType(name); }\n-    static void clearConflictsInEntitiesList(std::vector<std::pair<UUID, AccessEntityPtr>> & entities, const Poco::Logger * log_);\n+    static void clearConflictsInEntitiesList(std::vector<std::pair<UUID, AccessEntityPtr>> & entities, const LoggerPtr log_);\n     [[noreturn]] void throwNotFound(const UUID & id) const;\n     [[noreturn]] void throwNotFound(AccessEntityType type, const String & name) const;\n     [[noreturn]] static void throwBadCast(const UUID & id, AccessEntityType type, const String & name, AccessEntityType required_type);\n@@ -246,7 +247,9 @@ class IAccessStorage : public boost::noncopyable\n \n private:\n     const String storage_name;\n-    mutable std::atomic<Poco::Logger *> log = nullptr;\n+\n+    mutable OnceFlag log_initialized;\n+    mutable LoggerPtr log = nullptr;\n };\n \n \ndiff --git a/src/Access/KerberosInit.cpp b/src/Access/KerberosInit.cpp\nindex 772938ad9b29..3cda1c8e13c8 100644\n--- a/src/Access/KerberosInit.cpp\n+++ b/src/Access/KerberosInit.cpp\n@@ -63,7 +63,7 @@ String KerberosInit::fmtError(krb5_error_code code) const\n \n void KerberosInit::init(const String & keytab_file, const String & principal, const String & cache_name)\n {\n-    auto * log = &Poco::Logger::get(\"KerberosInit\");\n+    auto log = getLogger(\"KerberosInit\");\n     LOG_TRACE(log,\"Trying to authenticate with Kerberos v5\");\n \n     krb5_error_code ret;\ndiff --git a/src/Access/LDAPClient.cpp b/src/Access/LDAPClient.cpp\nindex 7926298061d5..3a0b82b9a760 100644\n--- a/src/Access/LDAPClient.cpp\n+++ b/src/Access/LDAPClient.cpp\n@@ -532,7 +532,7 @@ LDAPClient::SearchResults LDAPClient::search(const SearchParams & search_params)\n \n                     for (size_t i = 0; referrals[i]; ++i)\n                     {\n-                        LOG_WARNING(&Poco::Logger::get(\"LDAPClient\"), \"Received reference during LDAP search but not following it: {}\", referrals[i]);\n+                        LOG_WARNING(getLogger(\"LDAPClient\"), \"Received reference during LDAP search but not following it: {}\", referrals[i]);\n                     }\n                 }\n \ndiff --git a/src/Access/RowPolicyCache.cpp b/src/Access/RowPolicyCache.cpp\nindex bb9da674477c..13140099a639 100644\n--- a/src/Access/RowPolicyCache.cpp\n+++ b/src/Access/RowPolicyCache.cpp\n@@ -91,7 +91,7 @@ void RowPolicyCache::PolicyInfo::setPolicy(const RowPolicyPtr & policy_)\n         catch (...)\n         {\n             tryLogCurrentException(\n-                &Poco::Logger::get(\"RowPolicy\"),\n+                getLogger(\"RowPolicy\"),\n                 String(\"Could not parse the condition \") + toString(filter_type) + \" of row policy \"\n                     + backQuote(policy->getName()));\n         }\ndiff --git a/src/Access/SettingsAuthResponseParser.cpp b/src/Access/SettingsAuthResponseParser.cpp\nindex 62d15f1dcfc8..a90ae61f93a9 100644\n--- a/src/Access/SettingsAuthResponseParser.cpp\n+++ b/src/Access/SettingsAuthResponseParser.cpp\n@@ -37,7 +37,7 @@ SettingsAuthResponseParser::parse(const Poco::Net::HTTPResponse & response, std:\n     }\n     catch (...)\n     {\n-        LOG_INFO(&Poco::Logger::get(\"HTTPAuthentication\"), \"Failed to parse settings from authentication response. Skip it.\");\n+        LOG_INFO(getLogger(\"HTTPAuthentication\"), \"Failed to parse settings from authentication response. Skip it.\");\n     }\n     return result;\n }\ndiff --git a/src/Backups/BackupCoordinationLocal.cpp b/src/Backups/BackupCoordinationLocal.cpp\nindex fb91bae2303b..9964de2ad6e0 100644\n--- a/src/Backups/BackupCoordinationLocal.cpp\n+++ b/src/Backups/BackupCoordinationLocal.cpp\n@@ -9,7 +9,7 @@ namespace DB\n {\n \n BackupCoordinationLocal::BackupCoordinationLocal(bool plain_backup_)\n-    : log(&Poco::Logger::get(\"BackupCoordinationLocal\")), file_infos(plain_backup_)\n+    : log(getLogger(\"BackupCoordinationLocal\")), file_infos(plain_backup_)\n {\n }\n \ndiff --git a/src/Backups/BackupCoordinationLocal.h b/src/Backups/BackupCoordinationLocal.h\nindex f73cbbe29a89..7719ffd3e52e 100644\n--- a/src/Backups/BackupCoordinationLocal.h\n+++ b/src/Backups/BackupCoordinationLocal.h\n@@ -57,7 +57,7 @@ class BackupCoordinationLocal : public IBackupCoordination\n     bool hasConcurrentBackups(const std::atomic<size_t> & num_active_backups) const override;\n \n private:\n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n \n     BackupCoordinationReplicatedTables TSA_GUARDED_BY(replicated_tables_mutex) replicated_tables;\n     BackupCoordinationReplicatedAccess TSA_GUARDED_BY(replicated_access_mutex) replicated_access;\ndiff --git a/src/Backups/BackupCoordinationRemote.cpp b/src/Backups/BackupCoordinationRemote.cpp\nindex 4662f436aba4..9c509858b2a3 100644\n--- a/src/Backups/BackupCoordinationRemote.cpp\n+++ b/src/Backups/BackupCoordinationRemote.cpp\n@@ -173,7 +173,7 @@ BackupCoordinationRemote::BackupCoordinationRemote(\n     , current_host_index(findCurrentHostIndex(all_hosts, current_host))\n     , plain_backup(plain_backup_)\n     , is_internal(is_internal_)\n-    , log(&Poco::Logger::get(\"BackupCoordinationRemote\"))\n+    , log(getLogger(\"BackupCoordinationRemote\"))\n     , with_retries(\n         log,\n         get_zookeeper_,\ndiff --git a/src/Backups/BackupCoordinationRemote.h b/src/Backups/BackupCoordinationRemote.h\nindex 81980ee56373..ce891699bd29 100644\n--- a/src/Backups/BackupCoordinationRemote.h\n+++ b/src/Backups/BackupCoordinationRemote.h\n@@ -102,7 +102,7 @@ class BackupCoordinationRemote : public IBackupCoordination\n     const size_t current_host_index;\n     const bool plain_backup;\n     const bool is_internal;\n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n \n     /// The order of these two fields matters, because stage_sync holds a reference to with_retries object\n     mutable WithRetries with_retries;\ndiff --git a/src/Backups/BackupCoordinationStageSync.cpp b/src/Backups/BackupCoordinationStageSync.cpp\nindex 2eba3440be93..17ef163ce352 100644\n--- a/src/Backups/BackupCoordinationStageSync.cpp\n+++ b/src/Backups/BackupCoordinationStageSync.cpp\n@@ -24,7 +24,7 @@ namespace ErrorCodes\n BackupCoordinationStageSync::BackupCoordinationStageSync(\n     const String & root_zookeeper_path_,\n     WithRetries & with_retries_,\n-    Poco::Logger * log_)\n+    LoggerPtr log_)\n     : zookeeper_path(root_zookeeper_path_ + \"/stage\")\n     , with_retries(with_retries_)\n     , log(log_)\ndiff --git a/src/Backups/BackupCoordinationStageSync.h b/src/Backups/BackupCoordinationStageSync.h\nindex e34fbcc099b7..a06c5c610416 100644\n--- a/src/Backups/BackupCoordinationStageSync.h\n+++ b/src/Backups/BackupCoordinationStageSync.h\n@@ -12,7 +12,7 @@ class BackupCoordinationStageSync\n     BackupCoordinationStageSync(\n         const String & root_zookeeper_path_,\n         WithRetries & with_retries_,\n-        Poco::Logger * log_);\n+        LoggerPtr log_);\n \n     /// Sets the stage of the current host and signal other hosts if there were other hosts waiting for that.\n     void set(const String & current_host, const String & new_stage, const String & message, const bool & all_hosts = false);\n@@ -36,7 +36,7 @@ class BackupCoordinationStageSync\n     String zookeeper_path;\n     /// A reference to the field of parent object - BackupCoordinationRemote or RestoreCoordinationRemote\n     WithRetries & with_retries;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Backups/BackupEntriesCollector.cpp b/src/Backups/BackupEntriesCollector.cpp\nindex bf9cf50a67ad..5c0b80aa894a 100644\n--- a/src/Backups/BackupEntriesCollector.cpp\n+++ b/src/Backups/BackupEntriesCollector.cpp\n@@ -97,7 +97,7 @@ BackupEntriesCollector::BackupEntriesCollector(\n     , max_sleep_before_next_attempt_to_collect_metadata(\n           context->getConfigRef().getUInt64(\"backups.max_sleep_before_next_attempt_to_collect_metadata\", 5000))\n     , compare_collected_metadata(context->getConfigRef().getBool(\"backups.compare_collected_metadata\", true))\n-    , log(&Poco::Logger::get(\"BackupEntriesCollector\"))\n+    , log(getLogger(\"BackupEntriesCollector\"))\n     , global_zookeeper_retries_info(\n           context->getSettingsRef().backup_restore_keeper_max_retries,\n           context->getSettingsRef().backup_restore_keeper_retry_initial_backoff_ms,\ndiff --git a/src/Backups/BackupEntriesCollector.h b/src/Backups/BackupEntriesCollector.h\nindex fcbc5e5985fa..bad67e494c4c 100644\n--- a/src/Backups/BackupEntriesCollector.h\n+++ b/src/Backups/BackupEntriesCollector.h\n@@ -129,7 +129,7 @@ class BackupEntriesCollector : private boost::noncopyable\n     /// Whether we should collect the metadata after a successful attempt one more time and check that nothing has changed.\n     const bool compare_collected_metadata;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     /// Unfortunately we can use ZooKeeper for collecting information for backup\n     /// and we need to retry...\n     ZooKeeperRetriesInfo global_zookeeper_retries_info;\ndiff --git a/src/Backups/BackupFileInfo.cpp b/src/Backups/BackupFileInfo.cpp\nindex 2a1642f3b84b..f14b955149e3 100644\n--- a/src/Backups/BackupFileInfo.cpp\n+++ b/src/Backups/BackupFileInfo.cpp\n@@ -102,7 +102,7 @@ BackupFileInfo buildFileInfoForBackupEntry(\n     const BackupEntryPtr & backup_entry,\n     const BackupPtr & base_backup,\n     const ReadSettings & read_settings,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     auto adjusted_path = removeLeadingSlash(file_name);\n \n@@ -129,7 +129,7 @@ BackupFileInfo buildFileInfoForBackupEntry(\n     }\n \n     if (!log)\n-        log = &Poco::Logger::get(\"FileInfoFromBackupEntry\");\n+        log = getLogger(\"FileInfoFromBackupEntry\");\n \n     std::optional<SizeAndChecksum> base_backup_file_info = getInfoAboutFileFromBaseBackupIfExists(base_backup, adjusted_path);\n \n@@ -216,7 +216,7 @@ BackupFileInfos buildFileInfosForBackupEntries(const BackupEntries & backup_entr\n     std::exception_ptr exception;\n \n     auto thread_group = CurrentThread::getGroup();\n-    Poco::Logger * log = &Poco::Logger::get(\"FileInfosFromBackupEntries\");\n+    LoggerPtr log = getLogger(\"FileInfosFromBackupEntries\");\n \n     for (size_t i = 0; i != backup_entries.size(); ++i)\n     {\ndiff --git a/src/Backups/BackupFileInfo.h b/src/Backups/BackupFileInfo.h\nindex 15c385950024..a1405a9cafb4 100644\n--- a/src/Backups/BackupFileInfo.h\n+++ b/src/Backups/BackupFileInfo.h\n@@ -2,8 +2,7 @@\n \n #include <Core/Types.h>\n #include <Common/ThreadPool_fwd.h>\n-\n-namespace Poco { class Logger; }\n+#include <Common/Logger.h>\n \n namespace DB\n {\n@@ -77,7 +76,7 @@ struct BackupFileInfo\n using BackupFileInfos = std::vector<BackupFileInfo>;\n \n /// Builds a BackupFileInfo for a specified backup entry.\n-BackupFileInfo buildFileInfoForBackupEntry(const String & file_name, const BackupEntryPtr & backup_entry, const BackupPtr & base_backup, const ReadSettings & read_settings, Poco::Logger * log);\n+BackupFileInfo buildFileInfoForBackupEntry(const String & file_name, const BackupEntryPtr & backup_entry, const BackupPtr & base_backup, const ReadSettings & read_settings, LoggerPtr log);\n \n /// Builds a vector of BackupFileInfos for specified backup entries.\n BackupFileInfos buildFileInfosForBackupEntries(const BackupEntries & backup_entries, const BackupPtr & base_backup, const ReadSettings & read_settings, ThreadPool & thread_pool, QueryStatusPtr process_list_element);\ndiff --git a/src/Backups/BackupIO_Default.cpp b/src/Backups/BackupIO_Default.cpp\nindex 5ac522695ce2..77fd4532c8f2 100644\n--- a/src/Backups/BackupIO_Default.cpp\n+++ b/src/Backups/BackupIO_Default.cpp\n@@ -10,7 +10,7 @@\n namespace DB\n {\n \n-BackupReaderDefault::BackupReaderDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, Poco::Logger * log_)\n+BackupReaderDefault::BackupReaderDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, LoggerPtr log_)\n     : log(log_)\n     , read_settings(read_settings_)\n     , write_settings(write_settings_)\n@@ -36,7 +36,7 @@ void BackupReaderDefault::copyFileToDisk(const String & path_in_backup, size_t f\n     write_buffer->finalize();\n }\n \n-BackupWriterDefault::BackupWriterDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, Poco::Logger * log_)\n+BackupWriterDefault::BackupWriterDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, LoggerPtr log_)\n     : log(log_)\n     , read_settings(read_settings_)\n     , write_settings(write_settings_)\ndiff --git a/src/Backups/BackupIO_Default.h b/src/Backups/BackupIO_Default.h\nindex b4888fecd2fb..639293f22d9f 100644\n--- a/src/Backups/BackupIO_Default.h\n+++ b/src/Backups/BackupIO_Default.h\n@@ -18,7 +18,7 @@ enum class WriteMode;\n class BackupReaderDefault : public IBackupReader\n {\n public:\n-    BackupReaderDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, Poco::Logger * log_);\n+    BackupReaderDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, LoggerPtr log_);\n     ~BackupReaderDefault() override = default;\n \n     /// The function copyFileToDisk() can be much faster than reading the file with readFile() and then writing it to some disk.\n@@ -33,7 +33,7 @@ class BackupReaderDefault : public IBackupReader\n     size_t getWriteBufferSize() const override { return write_buffer_size; }\n \n protected:\n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n     const ReadSettings read_settings;\n \n     /// The write settings are used to write to the source disk in copyFileToDisk().\n@@ -45,7 +45,7 @@ class BackupReaderDefault : public IBackupReader\n class BackupWriterDefault : public IBackupWriter\n {\n public:\n-    BackupWriterDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, Poco::Logger * log_);\n+    BackupWriterDefault(const ReadSettings & read_settings_, const WriteSettings & write_settings_, LoggerPtr log_);\n     ~BackupWriterDefault() override = default;\n \n     bool fileContentsEqual(const String & file_name, const String & expected_file_contents) override;\n@@ -60,7 +60,7 @@ class BackupWriterDefault : public IBackupWriter\n     /// Here readFile() is used only to implement fileContentsEqual().\n     virtual std::unique_ptr<ReadBuffer> readFile(const String & file_name, size_t expected_file_size) = 0;\n \n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n \n     /// The read settings are used to read from the source disk in copyFileFromDisk().\n     const ReadSettings read_settings;\ndiff --git a/src/Backups/BackupIO_Disk.cpp b/src/Backups/BackupIO_Disk.cpp\nindex 91e8b97bc20a..27b594f6bb8d 100644\n--- a/src/Backups/BackupIO_Disk.cpp\n+++ b/src/Backups/BackupIO_Disk.cpp\n@@ -9,7 +9,7 @@ namespace DB\n {\n \n BackupReaderDisk::BackupReaderDisk(const DiskPtr & disk_, const String & root_path_, const ReadSettings & read_settings_, const WriteSettings & write_settings_)\n-    : BackupReaderDefault(read_settings_, write_settings_, &Poco::Logger::get(\"BackupReaderDisk\"))\n+    : BackupReaderDefault(read_settings_, write_settings_, getLogger(\"BackupReaderDisk\"))\n     , disk(disk_)\n     , root_path(root_path_)\n     , data_source_description(disk->getDataSourceDescription())\n@@ -57,7 +57,7 @@ void BackupReaderDisk::copyFileToDisk(const String & path_in_backup, size_t file\n \n \n BackupWriterDisk::BackupWriterDisk(const DiskPtr & disk_, const String & root_path_, const ReadSettings & read_settings_, const WriteSettings & write_settings_)\n-    : BackupWriterDefault(read_settings_, write_settings_, &Poco::Logger::get(\"BackupWriterDisk\"))\n+    : BackupWriterDefault(read_settings_, write_settings_, getLogger(\"BackupWriterDisk\"))\n     , disk(disk_)\n     , root_path(root_path_)\n     , data_source_description(disk->getDataSourceDescription())\ndiff --git a/src/Backups/BackupIO_File.cpp b/src/Backups/BackupIO_File.cpp\nindex 5384637a9693..35544a526f16 100644\n--- a/src/Backups/BackupIO_File.cpp\n+++ b/src/Backups/BackupIO_File.cpp\n@@ -17,7 +17,7 @@ namespace ErrorCodes\n }\n \n BackupReaderFile::BackupReaderFile(const String & root_path_, const ReadSettings & read_settings_, const WriteSettings & write_settings_)\n-    : BackupReaderDefault(read_settings_, write_settings_, &Poco::Logger::get(\"BackupReaderFile\"))\n+    : BackupReaderDefault(read_settings_, write_settings_, getLogger(\"BackupReaderFile\"))\n     , root_path(root_path_)\n     , data_source_description(DiskLocal::getLocalDataSourceDescription(root_path))\n {\n@@ -75,7 +75,7 @@ void BackupReaderFile::copyFileToDisk(const String & path_in_backup, size_t file\n \n \n BackupWriterFile::BackupWriterFile(const String & root_path_, const ReadSettings & read_settings_, const WriteSettings & write_settings_)\n-    : BackupWriterDefault(read_settings_, write_settings_, &Poco::Logger::get(\"BackupWriterFile\"))\n+    : BackupWriterDefault(read_settings_, write_settings_, getLogger(\"BackupWriterFile\"))\n     , root_path(root_path_)\n     , data_source_description(DiskLocal::getLocalDataSourceDescription(root_path))\n {\ndiff --git a/src/Backups/BackupIO_S3.cpp b/src/Backups/BackupIO_S3.cpp\nindex 381c58dd0456..fa4c1af36983 100644\n--- a/src/Backups/BackupIO_S3.cpp\n+++ b/src/Backups/BackupIO_S3.cpp\n@@ -124,7 +124,7 @@ BackupReaderS3::BackupReaderS3(\n     const ReadSettings & read_settings_,\n     const WriteSettings & write_settings_,\n     const ContextPtr & context_)\n-    : BackupReaderDefault(read_settings_, write_settings_, &Poco::Logger::get(\"BackupReaderS3\"))\n+    : BackupReaderDefault(read_settings_, write_settings_, getLogger(\"BackupReaderS3\"))\n     , s3_uri(s3_uri_)\n     , data_source_description{DataSourceType::ObjectStorage, ObjectStorageType::S3, MetadataStorageType::None, s3_uri.endpoint, false, false}\n     , s3_settings(context_->getStorageS3Settings().getSettings(s3_uri.uri.toString()))\n@@ -214,7 +214,7 @@ BackupWriterS3::BackupWriterS3(\n     const ReadSettings & read_settings_,\n     const WriteSettings & write_settings_,\n     const ContextPtr & context_)\n-    : BackupWriterDefault(read_settings_, write_settings_, &Poco::Logger::get(\"BackupWriterS3\"))\n+    : BackupWriterDefault(read_settings_, write_settings_, getLogger(\"BackupWriterS3\"))\n     , s3_uri(s3_uri_)\n     , data_source_description{DataSourceType::ObjectStorage, ObjectStorageType::S3, MetadataStorageType::None, s3_uri.endpoint, false, false}\n     , s3_settings(context_->getStorageS3Settings().getSettings(s3_uri.uri.toString()))\ndiff --git a/src/Backups/BackupImpl.cpp b/src/Backups/BackupImpl.cpp\nindex 9ac68bc24378..0961c867cab6 100644\n--- a/src/Backups/BackupImpl.cpp\n+++ b/src/Backups/BackupImpl.cpp\n@@ -105,7 +105,7 @@ BackupImpl::BackupImpl(\n     , version(INITIAL_BACKUP_VERSION)\n     , base_backup_info(base_backup_info_)\n     , use_same_s3_credentials_for_base_backup(use_same_s3_credentials_for_base_backup_)\n-    , log(&Poco::Logger::get(\"BackupImpl\"))\n+    , log(getLogger(\"BackupImpl\"))\n {\n     open();\n }\n@@ -136,7 +136,7 @@ BackupImpl::BackupImpl(\n     , base_backup_info(base_backup_info_)\n     , deduplicate_files(deduplicate_files_)\n     , use_same_s3_credentials_for_base_backup(use_same_s3_credentials_for_base_backup_)\n-    , log(&Poco::Logger::get(\"BackupImpl\"))\n+    , log(getLogger(\"BackupImpl\"))\n {\n     open();\n }\ndiff --git a/src/Backups/BackupImpl.h b/src/Backups/BackupImpl.h\nindex b369fe001719..e9803b46bb4d 100644\n--- a/src/Backups/BackupImpl.h\n+++ b/src/Backups/BackupImpl.h\n@@ -153,7 +153,7 @@ class BackupImpl : public IBackup\n     bool writing_finalized = false;\n     bool deduplicate_files = true;\n     bool use_same_s3_credentials_for_base_backup = false;\n-    const Poco::Logger * log;\n+    const LoggerPtr log;\n };\n \n }\ndiff --git a/src/Backups/BackupsWorker.cpp b/src/Backups/BackupsWorker.cpp\nindex 9c1b6d8af97e..c19be22c7499 100644\n--- a/src/Backups/BackupsWorker.cpp\n+++ b/src/Backups/BackupsWorker.cpp\n@@ -380,7 +380,7 @@ BackupsWorker::BackupsWorker(ContextMutablePtr global_context, size_t num_backup\n     , allow_concurrent_backups(allow_concurrent_backups_)\n     , allow_concurrent_restores(allow_concurrent_restores_)\n     , test_inject_sleep(test_inject_sleep_)\n-    , log(&Poco::Logger::get(\"BackupsWorker\"))\n+    , log(getLogger(\"BackupsWorker\"))\n     , backup_log(global_context->getBackupLog())\n     , process_list(global_context->getProcessList())\n {\ndiff --git a/src/Backups/BackupsWorker.h b/src/Backups/BackupsWorker.h\nindex 7a514e7032ba..73c8bf194730 100644\n--- a/src/Backups/BackupsWorker.h\n+++ b/src/Backups/BackupsWorker.h\n@@ -127,7 +127,7 @@ class BackupsWorker\n     const bool allow_concurrent_restores;\n     const bool test_inject_sleep;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     struct ExtendedOperationInfo\n     {\ndiff --git a/src/Backups/RestoreCoordinationLocal.cpp b/src/Backups/RestoreCoordinationLocal.cpp\nindex cf606151b53e..4a91b8d95a5e 100644\n--- a/src/Backups/RestoreCoordinationLocal.cpp\n+++ b/src/Backups/RestoreCoordinationLocal.cpp\n@@ -6,7 +6,7 @@\n namespace DB\n {\n \n-RestoreCoordinationLocal::RestoreCoordinationLocal() : log(&Poco::Logger::get(\"RestoreCoordinationLocal\"))\n+RestoreCoordinationLocal::RestoreCoordinationLocal() : log(getLogger(\"RestoreCoordinationLocal\"))\n {\n }\n \ndiff --git a/src/Backups/RestoreCoordinationLocal.h b/src/Backups/RestoreCoordinationLocal.h\nindex 7f6ffe1eeec1..5e51b719d637 100644\n--- a/src/Backups/RestoreCoordinationLocal.h\n+++ b/src/Backups/RestoreCoordinationLocal.h\n@@ -51,7 +51,7 @@ class RestoreCoordinationLocal : public IRestoreCoordination\n     bool hasConcurrentRestores(const std::atomic<size_t> & num_active_restores) const override;\n \n private:\n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n \n     std::set<std::pair<String /* database_zk_path */, String /* table_name */>> acquired_tables_in_replicated_databases;\n     std::unordered_set<String /* table_zk_path */> acquired_data_in_replicated_tables;\ndiff --git a/src/Backups/RestoreCoordinationRemote.cpp b/src/Backups/RestoreCoordinationRemote.cpp\nindex 0d2b3832bad5..84106737fc9d 100644\n--- a/src/Backups/RestoreCoordinationRemote.cpp\n+++ b/src/Backups/RestoreCoordinationRemote.cpp\n@@ -32,7 +32,7 @@ RestoreCoordinationRemote::RestoreCoordinationRemote(\n     , current_host(current_host_)\n     , current_host_index(BackupCoordinationRemote::findCurrentHostIndex(all_hosts, current_host))\n     , is_internal(is_internal_)\n-    , log(&Poco::Logger::get(\"RestoreCoordinationRemote\"))\n+    , log(getLogger(\"RestoreCoordinationRemote\"))\n     , with_retries(\n         log,\n         get_zookeeper_,\ndiff --git a/src/Backups/RestoreCoordinationRemote.h b/src/Backups/RestoreCoordinationRemote.h\nindex f7e678645df5..9c299865cfa7 100644\n--- a/src/Backups/RestoreCoordinationRemote.h\n+++ b/src/Backups/RestoreCoordinationRemote.h\n@@ -73,7 +73,7 @@ class RestoreCoordinationRemote : public IRestoreCoordination\n     const String current_host;\n     const size_t current_host_index;\n     const bool is_internal;\n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n \n     mutable WithRetries with_retries;\n     std::optional<BackupCoordinationStageSync> stage_sync;\ndiff --git a/src/Backups/RestorerFromBackup.cpp b/src/Backups/RestorerFromBackup.cpp\nindex f218410e5993..6f18c070cd73 100644\n--- a/src/Backups/RestorerFromBackup.cpp\n+++ b/src/Backups/RestorerFromBackup.cpp\n@@ -89,7 +89,7 @@ RestorerFromBackup::RestorerFromBackup(\n     , process_list_element(context->getProcessListElement())\n     , on_cluster_first_sync_timeout(context->getConfigRef().getUInt64(\"backups.on_cluster_first_sync_timeout\", 180000))\n     , create_table_timeout(context->getConfigRef().getUInt64(\"backups.create_table_timeout\", 300000))\n-    , log(&Poco::Logger::get(\"RestorerFromBackup\"))\n+    , log(getLogger(\"RestorerFromBackup\"))\n     , tables_dependencies(\"RestorerFromBackup\")\n {\n }\ndiff --git a/src/Backups/RestorerFromBackup.h b/src/Backups/RestorerFromBackup.h\nindex fad79a3a2e6d..5e4ee0c38329 100644\n--- a/src/Backups/RestorerFromBackup.h\n+++ b/src/Backups/RestorerFromBackup.h\n@@ -79,7 +79,7 @@ class RestorerFromBackup : private boost::noncopyable\n     QueryStatusPtr process_list_element;\n     std::chrono::milliseconds on_cluster_first_sync_timeout;\n     std::chrono::milliseconds create_table_timeout;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     Strings all_hosts;\n     DDLRenamingMap renaming_map;\ndiff --git a/src/Backups/WithRetries.cpp b/src/Backups/WithRetries.cpp\nindex db36bc92d92b..66851fa42ce4 100644\n--- a/src/Backups/WithRetries.cpp\n+++ b/src/Backups/WithRetries.cpp\n@@ -21,7 +21,7 @@ WithRetries::KeeperSettings WithRetries::KeeperSettings::fromContext(ContextPtr\n }\n \n WithRetries::WithRetries(\n-    Poco::Logger * log_, zkutil::GetZooKeeper get_zookeeper_, const KeeperSettings & settings_, QueryStatusPtr process_list_element_, RenewerCallback callback_)\n+    LoggerPtr log_, zkutil::GetZooKeeper get_zookeeper_, const KeeperSettings & settings_, QueryStatusPtr process_list_element_, RenewerCallback callback_)\n     : log(log_)\n     , get_zookeeper(get_zookeeper_)\n     , settings(settings_)\ndiff --git a/src/Backups/WithRetries.h b/src/Backups/WithRetries.h\nindex edfccc658d91..3a6e28996b93 100644\n--- a/src/Backups/WithRetries.h\n+++ b/src/Backups/WithRetries.h\n@@ -52,7 +52,7 @@ class WithRetries\n     };\n \n     RetriesControlHolder createRetriesControlHolder(const String & name);\n-    WithRetries(Poco::Logger * log, zkutil::GetZooKeeper get_zookeeper_, const KeeperSettings & settings, QueryStatusPtr process_list_element_, RenewerCallback callback);\n+    WithRetries(LoggerPtr log, zkutil::GetZooKeeper get_zookeeper_, const KeeperSettings & settings, QueryStatusPtr process_list_element_, RenewerCallback callback);\n \n     /// Used to re-establish new connection inside a retry loop.\n     void renewZooKeeper(FaultyKeeper my_faulty_zookeeper) const;\n@@ -62,7 +62,7 @@ class WithRetries\n     /// This will provide a special wrapper which is useful for testing\n     FaultyKeeper getFaultyZooKeeper() const;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     zkutil::GetZooKeeper get_zookeeper;\n     KeeperSettings settings;\n     QueryStatusPtr process_list_element;\ndiff --git a/src/BridgeHelper/IBridgeHelper.h b/src/BridgeHelper/IBridgeHelper.h\nindex 272d97c8a781..6812bd04a030 100644\n--- a/src/BridgeHelper/IBridgeHelper.h\n+++ b/src/BridgeHelper/IBridgeHelper.h\n@@ -51,7 +51,7 @@ class IBridgeHelper: protected WithContext\n \n     virtual const Poco::Util::AbstractConfiguration & getConfig() const = 0;\n \n-    virtual Poco::Logger * getLog() const = 0;\n+    virtual LoggerPtr getLog() const = 0;\n \n     virtual Poco::Timespan getHTTPTimeout() const = 0;\n \ndiff --git a/src/BridgeHelper/LibraryBridgeHelper.cpp b/src/BridgeHelper/LibraryBridgeHelper.cpp\nindex e83707595b9b..84bfe096e796 100644\n--- a/src/BridgeHelper/LibraryBridgeHelper.cpp\n+++ b/src/BridgeHelper/LibraryBridgeHelper.cpp\n@@ -8,7 +8,7 @@ namespace DB\n LibraryBridgeHelper::LibraryBridgeHelper(ContextPtr context_)\n     : IBridgeHelper(context_)\n     , config(context_->getConfigRef())\n-    , log(&Poco::Logger::get(\"LibraryBridgeHelper\"))\n+    , log(getLogger(\"LibraryBridgeHelper\"))\n     , http_timeout(context_->getGlobalContext()->getSettingsRef().http_receive_timeout.value)\n     , bridge_host(config.getString(\"library_bridge.host\", DEFAULT_HOST))\n     , bridge_port(config.getUInt(\"library_bridge.port\", DEFAULT_PORT))\ndiff --git a/src/BridgeHelper/LibraryBridgeHelper.h b/src/BridgeHelper/LibraryBridgeHelper.h\nindex 1723d1f8fb4d..8940f9d1c9ee 100644\n--- a/src/BridgeHelper/LibraryBridgeHelper.h\n+++ b/src/BridgeHelper/LibraryBridgeHelper.h\n@@ -31,7 +31,7 @@ class LibraryBridgeHelper : public IBridgeHelper\n \n     const Poco::Util::AbstractConfiguration & getConfig() const override { return config; }\n \n-    Poco::Logger * getLog() const override { return log; }\n+    LoggerPtr getLog() const override { return log; }\n \n     Poco::Timespan getHTTPTimeout() const override { return http_timeout; }\n \n@@ -40,7 +40,7 @@ class LibraryBridgeHelper : public IBridgeHelper\n     static constexpr inline size_t DEFAULT_PORT = 9012;\n \n     const Poco::Util::AbstractConfiguration & config;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const Poco::Timespan http_timeout;\n     std::string bridge_host;\n     size_t bridge_port;\ndiff --git a/src/BridgeHelper/XDBCBridgeHelper.h b/src/BridgeHelper/XDBCBridgeHelper.h\nindex 060de74b5b16..d208b8ddab07 100644\n--- a/src/BridgeHelper/XDBCBridgeHelper.h\n+++ b/src/BridgeHelper/XDBCBridgeHelper.h\n@@ -65,7 +65,7 @@ class XDBCBridgeHelper : public IXDBCBridgeHelper\n             const std::string & connection_string_,\n             bool use_connection_pooling_)\n         : IXDBCBridgeHelper(context_->getGlobalContext())\n-        , log(&Poco::Logger::get(BridgeHelperMixin::getName() + \"BridgeHelper\"))\n+        , log(getLogger(BridgeHelperMixin::getName() + \"BridgeHelper\"))\n         , connection_string(connection_string_)\n         , use_connection_pooling(use_connection_pooling_)\n         , http_timeout(http_timeout_)\n@@ -123,7 +123,7 @@ class XDBCBridgeHelper : public IXDBCBridgeHelper\n \n     const Poco::Util::AbstractConfiguration & getConfig() const override { return config; }\n \n-    Poco::Logger * getLog() const override { return log; }\n+    LoggerPtr getLog() const override { return log; }\n \n     bool startBridgeManually() const override { return BridgeHelperMixin::startBridgeManually(); }\n \n@@ -146,7 +146,7 @@ class XDBCBridgeHelper : public IXDBCBridgeHelper\n private:\n     using Configuration = Poco::Util::AbstractConfiguration;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string connection_string;\n     bool use_connection_pooling;\n     Poco::Timespan http_timeout;\ndiff --git a/src/Client/Connection.h b/src/Client/Connection.h\nindex 2f209fc92d03..e93a7539d153 100644\n--- a/src/Client/Connection.h\n+++ b/src/Client/Connection.h\n@@ -4,6 +4,7 @@\n #include <Poco/Net/StreamSocket.h>\n \n #include <Common/SSH/Wrappers.h>\n+#include <Common/callOnce.h>\n #include <Client/IServerConnection.h>\n #include <Core/Defines.h>\n \n@@ -244,16 +245,18 @@ class Connection : public IServerConnection\n         {\n         }\n \n-        Poco::Logger * get()\n+        LoggerPtr get()\n         {\n-            if (!log)\n-                log = &Poco::Logger::get(\"Connection (\" + parent.getDescription() + \")\");\n+            callOnce(log_initialized, [&] {\n+                log = getLogger(\"Connection (\" + parent.getDescription() + \")\");\n+            });\n \n             return log;\n         }\n \n     private:\n-        std::atomic<Poco::Logger *> log;\n+        OnceFlag log_initialized;\n+        LoggerPtr log;\n         Connection & parent;\n     };\n \ndiff --git a/src/Client/ConnectionEstablisher.cpp b/src/Client/ConnectionEstablisher.cpp\nindex e5b1347add5b..a9009e5bb25f 100644\n--- a/src/Client/ConnectionEstablisher.cpp\n+++ b/src/Client/ConnectionEstablisher.cpp\n@@ -25,7 +25,7 @@ ConnectionEstablisher::ConnectionEstablisher(\n     IConnectionPool * pool_,\n     const ConnectionTimeouts * timeouts_,\n     const Settings & settings_,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     const QualifiedTableName * table_to_check_)\n     : pool(pool_), timeouts(timeouts_), settings(settings_), log(log_), table_to_check(table_to_check_), is_finished(false)\n {\n@@ -114,7 +114,7 @@ ConnectionEstablisherAsync::ConnectionEstablisherAsync(\n     IConnectionPool * pool_,\n     const ConnectionTimeouts * timeouts_,\n     const Settings & settings_,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     const QualifiedTableName * table_to_check_)\n     : AsyncTaskExecutor(std::make_unique<Task>(*this)), connection_establisher(pool_, timeouts_, settings_, log_, table_to_check_)\n {\ndiff --git a/src/Client/ConnectionEstablisher.h b/src/Client/ConnectionEstablisher.h\nindex 880e44c8a1a5..7ea65708b1d5 100644\n--- a/src/Client/ConnectionEstablisher.h\n+++ b/src/Client/ConnectionEstablisher.h\n@@ -23,7 +23,7 @@ class ConnectionEstablisher\n     ConnectionEstablisher(IConnectionPool * pool_,\n                           const ConnectionTimeouts * timeouts_,\n                           const Settings & settings_,\n-                          Poco::Logger * log,\n+                          LoggerPtr log,\n                           const QualifiedTableName * table_to_check = nullptr);\n \n     /// Establish connection and save it in result, write possible exception message in fail_message.\n@@ -38,7 +38,7 @@ class ConnectionEstablisher\n     IConnectionPool * pool;\n     const ConnectionTimeouts * timeouts;\n     const Settings & settings;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const QualifiedTableName * table_to_check;\n \n     bool is_finished;\n@@ -61,7 +61,7 @@ class ConnectionEstablisherAsync : public AsyncTaskExecutor\n     ConnectionEstablisherAsync(IConnectionPool * pool_,\n                                const ConnectionTimeouts * timeouts_,\n                                const Settings & settings_,\n-                               Poco::Logger * log_,\n+                               LoggerPtr log_,\n                                const QualifiedTableName * table_to_check_ = nullptr);\n \n     /// Get file descriptor that can be added in epoll and be polled,\ndiff --git a/src/Client/ConnectionPool.h b/src/Client/ConnectionPool.h\nindex d663c052abca..1886a0431a54 100644\n--- a/src/Client/ConnectionPool.h\n+++ b/src/Client/ConnectionPool.h\n@@ -64,7 +64,7 @@ class ConnectionPool : public IConnectionPool, private PoolBase<Connection>\n             Protocol::Secure secure_,\n             Priority priority_ = Priority{1})\n        : Base(max_connections_,\n-        &Poco::Logger::get(\"ConnectionPool (\" + host_ + \":\" + toString(port_) + \")\")),\n+        getLogger(\"ConnectionPool (\" + host_ + \":\" + toString(port_) + \")\")),\n         host(host_),\n         port(port_),\n         default_database(default_database_),\ndiff --git a/src/Client/ConnectionPoolWithFailover.cpp b/src/Client/ConnectionPoolWithFailover.cpp\nindex 43166659b186..c05fbb317c81 100644\n--- a/src/Client/ConnectionPoolWithFailover.cpp\n+++ b/src/Client/ConnectionPoolWithFailover.cpp\n@@ -29,7 +29,7 @@ ConnectionPoolWithFailover::ConnectionPoolWithFailover(\n         LoadBalancing load_balancing,\n         time_t decrease_error_period_,\n         size_t max_error_cap_)\n-    : Base(std::move(nested_pools_), decrease_error_period_, max_error_cap_, &Poco::Logger::get(\"ConnectionPoolWithFailover\"))\n+    : Base(std::move(nested_pools_), decrease_error_period_, max_error_cap_, getLogger(\"ConnectionPoolWithFailover\"))\n     , get_priority_load_balancing(load_balancing)\n {\n     const std::string & local_hostname = getFQDNOrHostName();\ndiff --git a/src/Client/HedgedConnectionsFactory.cpp b/src/Client/HedgedConnectionsFactory.cpp\nindex f7b5ceedc965..01f9a32ce75b 100644\n--- a/src/Client/HedgedConnectionsFactory.cpp\n+++ b/src/Client/HedgedConnectionsFactory.cpp\n@@ -34,7 +34,7 @@ HedgedConnectionsFactory::HedgedConnectionsFactory(\n     : pool(pool_)\n     , timeouts(timeouts_)\n     , table_to_check(table_to_check_)\n-    , log(&Poco::Logger::get(\"HedgedConnectionsFactory\"))\n+    , log(getLogger(\"HedgedConnectionsFactory\"))\n     , max_tries(max_tries_)\n     , fallback_to_stale_replicas(fallback_to_stale_replicas_)\n     , max_parallel_replicas(max_parallel_replicas_)\ndiff --git a/src/Client/HedgedConnectionsFactory.h b/src/Client/HedgedConnectionsFactory.h\nindex f187e9b2abb1..ce7b553acdd3 100644\n--- a/src/Client/HedgedConnectionsFactory.h\n+++ b/src/Client/HedgedConnectionsFactory.h\n@@ -133,7 +133,7 @@ class HedgedConnectionsFactory\n     std::shared_ptr<QualifiedTableName> table_to_check;\n     int last_used_index = -1;\n     Epoll epoll;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string fail_messages;\n \n     /// The maximum number of attempts to connect to replicas.\ndiff --git a/src/Common/Allocator.cpp b/src/Common/Allocator.cpp\nindex 24a96c1c85b3..e80c125c2a0b 100644\n--- a/src/Common/Allocator.cpp\n+++ b/src/Common/Allocator.cpp\n@@ -50,7 +50,7 @@ void prefaultPages([[maybe_unused]] void * buf_, [[maybe_unused]] size_t len_)\n     auto [buf, len] = adjustToPageSize(buf_, len_, page_size);\n     if (::madvise(buf, len, MADV_POPULATE_WRITE) < 0)\n         LOG_TRACE(\n-            LogFrequencyLimiter(&Poco::Logger::get(\"Allocator\"), 1),\n+            LogFrequencyLimiter(getLogger(\"Allocator\"), 1),\n             \"Attempt to populate pages failed: {} (EINVAL is expected for kernels < 5.14)\",\n             errnoToString(errno));\n #endif\ndiff --git a/src/Common/AsyncLoader.cpp b/src/Common/AsyncLoader.cpp\nindex 3b7eac3e0d40..e9de95363bc8 100644\n--- a/src/Common/AsyncLoader.cpp\n+++ b/src/Common/AsyncLoader.cpp\n@@ -34,7 +34,7 @@ namespace ErrorCodes\n static constexpr size_t PRINT_MESSAGE_EACH_N_OBJECTS = 256;\n static constexpr size_t PRINT_MESSAGE_EACH_N_SECONDS = 5;\n \n-void logAboutProgress(Poco::Logger * log, size_t processed, size_t total, AtomicStopwatch & watch)\n+void logAboutProgress(LoggerPtr log, size_t processed, size_t total, AtomicStopwatch & watch)\n {\n     if (total && (processed % PRINT_MESSAGE_EACH_N_OBJECTS == 0 || watch.compareAndRestart(PRINT_MESSAGE_EACH_N_SECONDS)))\n     {\n@@ -205,7 +205,7 @@ void LoadTask::detach()\n AsyncLoader::AsyncLoader(std::vector<PoolInitializer> pool_initializers, bool log_failures_, bool log_progress_)\n     : log_failures(log_failures_)\n     , log_progress(log_progress_)\n-    , log(&Poco::Logger::get(\"AsyncLoader\"))\n+    , log(getLogger(\"AsyncLoader\"))\n {\n     pools.reserve(pool_initializers.size());\n     for (auto && init : pool_initializers)\ndiff --git a/src/Common/AsyncLoader.h b/src/Common/AsyncLoader.h\nindex b02bc2ac06a8..c2a9c901f1cc 100644\n--- a/src/Common/AsyncLoader.h\n+++ b/src/Common/AsyncLoader.h\n@@ -15,6 +15,7 @@\n #include <Common/Priority.h>\n #include <Common/Stopwatch.h>\n #include <Common/ThreadPool_fwd.h>\n+#include <Common/Logger.h>\n \n \n namespace Poco { class Logger; }\n@@ -40,7 +41,7 @@ using LoadTaskPtr = std::shared_ptr<LoadTask>;\n using LoadTaskPtrs = std::vector<LoadTaskPtr>;\n class AsyncLoader;\n \n-void logAboutProgress(Poco::Logger * log, size_t processed, size_t total, AtomicStopwatch & watch);\n+void logAboutProgress(LoggerPtr log, size_t processed, size_t total, AtomicStopwatch & watch);\n \n // Execution status of a load job.\n enum class LoadStatus\n@@ -419,7 +420,7 @@ class AsyncLoader : private boost::noncopyable\n     // Logging\n     const bool log_failures; // Worker should log all exceptions caught from job functions.\n     const bool log_progress; // Periodically log total progress\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     mutable std::mutex mutex; // Guards all the fields below.\n     bool is_running = true;\ndiff --git a/src/Common/AsynchronousMetrics.cpp b/src/Common/AsynchronousMetrics.cpp\nindex ec94d17f5908..b24d9bcc301a 100644\n--- a/src/Common/AsynchronousMetrics.cpp\n+++ b/src/Common/AsynchronousMetrics.cpp\n@@ -58,7 +58,7 @@ AsynchronousMetrics::AsynchronousMetrics(\n     int update_period_seconds,\n     const ProtocolServerMetricsFunc & protocol_server_metrics_func_)\n     : update_period(update_period_seconds)\n-    , log(&Poco::Logger::get(\"AsynchronousMetrics\"))\n+    , log(getLogger(\"AsynchronousMetrics\"))\n     , protocol_server_metrics_func(protocol_server_metrics_func_)\n {\n #if defined(OS_LINUX)\n@@ -125,7 +125,7 @@ void AsynchronousMetrics::openSensors() TSA_REQUIRES(data_mutex)\n         catch (const ErrnoException & e)\n         {\n             LOG_WARNING(\n-                &Poco::Logger::get(\"AsynchronousMetrics\"),\n+                getLogger(\"AsynchronousMetrics\"),\n                 \"Thermal monitor '{}' exists but could not be read: {}.\",\n                 thermal_device_index,\n                 errnoToString(e.getErrno()));\n@@ -254,7 +254,7 @@ void AsynchronousMetrics::openSensorsChips() TSA_REQUIRES(data_mutex)\n             catch (const ErrnoException & e)\n             {\n                 LOG_WARNING(\n-                    &Poco::Logger::get(\"AsynchronousMetrics\"),\n+                    getLogger(\"AsynchronousMetrics\"),\n                     \"Hardware monitor '{}', sensor '{}' exists but could not be read: {}.\",\n                     hwmon_name,\n                     sensor_index,\ndiff --git a/src/Common/AsynchronousMetrics.h b/src/Common/AsynchronousMetrics.h\nindex b9a5862dbffe..305e8136b8ac 100644\n--- a/src/Common/AsynchronousMetrics.h\n+++ b/src/Common/AsynchronousMetrics.h\n@@ -82,7 +82,7 @@ class AsynchronousMetrics\n protected:\n     const Duration update_period;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n private:\n     virtual void updateImpl(TimePoint update_time, TimePoint current_time, bool force_update, bool first_run, AsynchronousMetricValues & new_values) = 0;\n     virtual void logImpl(AsynchronousMetricValues &) {}\ndiff --git a/src/Common/Config/ConfigProcessor.cpp b/src/Common/Config/ConfigProcessor.cpp\nindex 92e66fee4896..641e7ddcdaa8 100644\n--- a/src/Common/Config/ConfigProcessor.cpp\n+++ b/src/Common/Config/ConfigProcessor.cpp\n@@ -77,23 +77,17 @@ ConfigProcessor::ConfigProcessor(\n     , name_pool(new Poco::XML::NamePool(65521))\n     , dom_parser(name_pool)\n {\n-    if (log_to_console && !Poco::Logger::has(\"ConfigProcessor\"))\n+    if (log_to_console && !hasLogger(\"ConfigProcessor\"))\n     {\n         channel_ptr = new Poco::ConsoleChannel;\n-        log = &Poco::Logger::create(\"ConfigProcessor\", channel_ptr.get(), Poco::Message::PRIO_TRACE);\n+        log = createLogger(\"ConfigProcessor\", channel_ptr.get(), Poco::Message::PRIO_TRACE);\n     }\n     else\n     {\n-        log = &Poco::Logger::get(\"ConfigProcessor\");\n+        log = getLogger(\"ConfigProcessor\");\n     }\n }\n \n-ConfigProcessor::~ConfigProcessor()\n-{\n-    if (channel_ptr) /// This means we have created a new console logger in the constructor.\n-        Poco::Logger::destroy(\"ConfigProcessor\");\n-}\n-\n static std::unordered_map<std::string, std::string_view> embedded_configs;\n \n void ConfigProcessor::registerEmbeddedConfig(std::string name, std::string_view content)\ndiff --git a/src/Common/Config/ConfigProcessor.h b/src/Common/Config/ConfigProcessor.h\nindex 98592d8846e4..5712c36d7370 100644\n--- a/src/Common/Config/ConfigProcessor.h\n+++ b/src/Common/Config/ConfigProcessor.h\n@@ -7,6 +7,8 @@\n #include <vector>\n #include <memory>\n \n+#include <Common/Logger.h>\n+\n #include <Poco/DOM/Document.h>\n #include <Poco/DOM/DOMParser.h>\n #include <Poco/DOM/DOMWriter.h>\n@@ -44,8 +46,6 @@ class ConfigProcessor\n         bool log_to_console = false,\n         const Substitutions & substitutions = Substitutions());\n \n-    ~ConfigProcessor();\n-\n     /// Perform config includes and substitutions and return the resulting XML-document.\n     ///\n     /// Suppose path is \"/path/file.xml\"\n@@ -125,7 +125,7 @@ class ConfigProcessor\n \n     bool throw_on_bad_incl;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     Poco::AutoPtr<Poco::Channel> channel_ptr;\n \n     Substitutions substitutions;\ndiff --git a/src/Common/Config/ConfigReloader.h b/src/Common/Config/ConfigReloader.h\nindex 2529c7a52369..13a797bad085 100644\n--- a/src/Common/Config/ConfigReloader.h\n+++ b/src/Common/Config/ConfigReloader.h\n@@ -69,7 +69,7 @@ class ConfigReloader\n \n     static constexpr auto reload_interval = std::chrono::seconds(2);\n \n-    Poco::Logger * log = &Poco::Logger::get(\"ConfigReloader\");\n+    LoggerPtr log = getLogger(\"ConfigReloader\");\n \n     std::string config_path;\n     std::vector<std::string> extra_paths;\ndiff --git a/src/Common/DNSResolver.cpp b/src/Common/DNSResolver.cpp\nindex 9cb352da0ba9..fcbbaf6b0be0 100644\n--- a/src/Common/DNSResolver.cpp\n+++ b/src/Common/DNSResolver.cpp\n@@ -104,7 +104,7 @@ DNSResolver::IPAddresses hostByName(const std::string & host)\n     }\n     catch (const Poco::Net::DNSException & e)\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"DNSResolver\"), \"Cannot resolve host ({}), error {}: {}.\", host, e.code(), e.name());\n+        LOG_WARNING(getLogger(\"DNSResolver\"), \"Cannot resolve host ({}), error {}: {}.\", host, e.code(), e.name());\n         addresses.clear();\n     }\n \n@@ -201,7 +201,7 @@ struct DNSResolver::Impl\n };\n \n \n-DNSResolver::DNSResolver() : impl(std::make_unique<DNSResolver::Impl>()), log(&Poco::Logger::get(\"DNSResolver\")) {}\n+DNSResolver::DNSResolver() : impl(std::make_unique<DNSResolver::Impl>()), log(getLogger(\"DNSResolver\")) {}\n \n Poco::Net::IPAddress DNSResolver::resolveHost(const std::string & host)\n {\ndiff --git a/src/Common/DNSResolver.h b/src/Common/DNSResolver.h\nindex 1017607a5bdb..965688f84f24 100644\n--- a/src/Common/DNSResolver.h\n+++ b/src/Common/DNSResolver.h\n@@ -73,7 +73,7 @@ class DNSResolver : private boost::noncopyable\n \n     struct Impl;\n     std::unique_ptr<Impl> impl;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Updates cached value and returns true it has been changed.\n     bool updateHost(const String & host);\ndiff --git a/src/Common/EnvironmentProxyConfigurationResolver.cpp b/src/Common/EnvironmentProxyConfigurationResolver.cpp\nindex 58507904ce99..f2c60afa1a88 100644\n--- a/src/Common/EnvironmentProxyConfigurationResolver.cpp\n+++ b/src/Common/EnvironmentProxyConfigurationResolver.cpp\n@@ -50,7 +50,7 @@ ProxyConfiguration EnvironmentProxyConfigurationResolver::resolve()\n     auto scheme = uri.getScheme();\n     auto port = uri.getPort();\n \n-    LOG_TRACE(&Poco::Logger::get(\"EnvironmentProxyConfigurationResolver\"), \"Use proxy from environment: {}://{}:{}\", scheme, host, port);\n+    LOG_TRACE(getLogger(\"EnvironmentProxyConfigurationResolver\"), \"Use proxy from environment: {}://{}:{}\", scheme, host, port);\n \n     return ProxyConfiguration {\n         host,\ndiff --git a/src/Common/ErrorHandlers.h b/src/Common/ErrorHandlers.h\nindex 301377bff837..a4a7c4683aa7 100644\n--- a/src/Common/ErrorHandlers.h\n+++ b/src/Common/ErrorHandlers.h\n@@ -27,7 +27,7 @@ class ServerErrorHandler : public Poco::ErrorHandler\n     void exception()                        override { logException(); }\n \n private:\n-    Poco::Logger * log = &Poco::Logger::get(\"ServerErrorHandler\");\n+    LoggerPtr log = getLogger(\"ServerErrorHandler\");\n \n     void logException()\n     {\ndiff --git a/src/Common/Exception.cpp b/src/Common/Exception.cpp\nindex e5e8cf2c8187..ff83f6ba8073 100644\n--- a/src/Common/Exception.cpp\n+++ b/src/Common/Exception.cpp\n@@ -235,8 +235,9 @@ void tryLogCurrentException(const char * log_name, const std::string & start_of_\n     /// MemoryTracker until the exception will be logged.\n     LockMemoryExceptionInThread lock_memory_tracker(VariableContext::Global);\n \n-    /// Poco::Logger::get can allocate memory too\n-    tryLogCurrentExceptionImpl(&Poco::Logger::get(log_name), start_of_message);\n+    /// getLogger can allocate memory too\n+    auto logger = getLogger(log_name);\n+    tryLogCurrentExceptionImpl(logger.get(), start_of_message);\n }\n \n void tryLogCurrentException(Poco::Logger * logger, const std::string & start_of_message)\n@@ -251,6 +252,11 @@ void tryLogCurrentException(Poco::Logger * logger, const std::string & start_of_\n     tryLogCurrentExceptionImpl(logger, start_of_message);\n }\n \n+void tryLogCurrentException(LoggerPtr logger, const std::string & start_of_message)\n+{\n+    tryLogCurrentException(logger.get(), start_of_message);\n+}\n+\n static void getNoSpaceLeftInfoMessage(std::filesystem::path path, String & msg)\n {\n     path = std::filesystem::absolute(path);\n@@ -511,7 +517,7 @@ void tryLogException(std::exception_ptr e, const char * log_name, const std::str\n     }\n }\n \n-void tryLogException(std::exception_ptr e, Poco::Logger * logger, const std::string & start_of_message)\n+void tryLogException(std::exception_ptr e, LoggerPtr logger, const std::string & start_of_message)\n {\n     try\n     {\ndiff --git a/src/Common/Exception.h b/src/Common/Exception.h\nindex 6f30fde3876a..8afed6034cfa 100644\n--- a/src/Common/Exception.h\n+++ b/src/Common/Exception.h\n@@ -10,6 +10,7 @@\n #include <base/errnoToString.h>\n #include <base/scope_guard.h>\n #include <Common/LoggingFormatStringHelpers.h>\n+#include <Common/Logger.h>\n #include <Common/StackTrace.h>\n \n #include <fmt/format.h>\n@@ -240,8 +241,10 @@ using Exceptions = std::vector<std::exception_ptr>;\n /** Try to write an exception to the log (and forget about it).\n   * Can be used in destructors in the catch-all block.\n   */\n+/// TODO: Logger leak constexpr overload\n void tryLogCurrentException(const char * log_name, const std::string & start_of_message = \"\");\n void tryLogCurrentException(Poco::Logger * logger, const std::string & start_of_message = \"\");\n+void tryLogCurrentException(LoggerPtr logger, const std::string & start_of_message = \"\");\n \n \n /** Prints current exception in canonical format.\n@@ -284,9 +287,9 @@ struct ExecutionStatus\n     bool tryDeserializeText(const std::string & data);\n };\n \n-\n+/// TODO: Logger leak constexpr overload\n void tryLogException(std::exception_ptr e, const char * log_name, const std::string & start_of_message = \"\");\n-void tryLogException(std::exception_ptr e, Poco::Logger * logger, const std::string & start_of_message = \"\");\n+void tryLogException(std::exception_ptr e, LoggerPtr logger, const std::string & start_of_message = \"\");\n \n std::string getExceptionMessage(const Exception & e, bool with_stacktrace, bool check_embedded_stacktrace = false);\n PreformattedMessage getExceptionMessageAndPattern(const Exception & e, bool with_stacktrace, bool check_embedded_stacktrace = false);\ndiff --git a/src/Common/FileChecker.cpp b/src/Common/FileChecker.cpp\nindex 049dee459a7c..098ea4b1ac46 100644\n--- a/src/Common/FileChecker.cpp\n+++ b/src/Common/FileChecker.cpp\n@@ -29,7 +29,7 @@ FileChecker::FileChecker(const String & file_info_path_) : FileChecker(nullptr,\n \n FileChecker::FileChecker(DiskPtr disk_, const String & file_info_path_)\n     : disk(std::move(disk_))\n-    , log(&Poco::Logger::get(\"FileChecker\"))\n+    , log(getLogger(\"FileChecker\"))\n {\n     setPath(file_info_path_);\n     try\ndiff --git a/src/Common/FileChecker.h b/src/Common/FileChecker.h\nindex b0dd0814edd5..c7ba1b912280 100644\n--- a/src/Common/FileChecker.h\n+++ b/src/Common/FileChecker.h\n@@ -1,6 +1,7 @@\n #pragma once\n \n #include <Storages/CheckResults.h>\n+#include <Common/Logger.h>\n #include <map>\n #include <base/types.h>\n #include <memory>\n@@ -84,7 +85,7 @@ class FileChecker\n     size_t getRealFileSize(const String & path_) const;\n \n     const DiskPtr disk;\n-    const Poco::Logger * log;\n+    const LoggerPtr log;\n \n     String files_info_path;\n     std::map<String, size_t> map;\ndiff --git a/src/Common/FrequencyHolder.cpp b/src/Common/FrequencyHolder.cpp\nindex 7dc1f622aebe..a25486865866 100644\n--- a/src/Common/FrequencyHolder.cpp\n+++ b/src/Common/FrequencyHolder.cpp\n@@ -34,7 +34,7 @@ FrequencyHolder::FrequencyHolder()\n \n void FrequencyHolder::loadEncodingsFrequency()\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"EncodingsFrequency\");\n+    LoggerPtr log = getLogger(\"EncodingsFrequency\");\n \n     LOG_TRACE(log, \"Loading embedded charset frequencies\");\n \n@@ -92,7 +92,7 @@ void FrequencyHolder::loadEncodingsFrequency()\n \n void FrequencyHolder::loadEmotionalDict()\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"EmotionalDict\");\n+    LoggerPtr log = getLogger(\"EmotionalDict\");\n     LOG_TRACE(log, \"Loading embedded emotional dictionary\");\n \n     std::string_view resource(reinterpret_cast<const char *>(gresource_tonality_ru_zstData), gresource_tonality_ru_zstSize);\n@@ -130,7 +130,7 @@ void FrequencyHolder::loadEmotionalDict()\n \n void FrequencyHolder::loadProgrammingFrequency()\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"ProgrammingFrequency\");\n+    LoggerPtr log = getLogger(\"ProgrammingFrequency\");\n \n     LOG_TRACE(log, \"Loading embedded programming languages frequencies loading\");\n \ndiff --git a/src/Common/Jemalloc.cpp b/src/Common/Jemalloc.cpp\nindex a8b9d70e7314..3eb8691a1e18 100644\n--- a/src/Common/Jemalloc.cpp\n+++ b/src/Common/Jemalloc.cpp\n@@ -25,7 +25,7 @@ namespace ErrorCodes\n \n void purgeJemallocArenas()\n {\n-    LOG_TRACE(&Poco::Logger::get(\"SystemJemalloc\"), \"Purging unused memory\");\n+    LOG_TRACE(getLogger(\"SystemJemalloc\"), \"Purging unused memory\");\n     Stopwatch watch;\n     mallctl(\"arena.\" STRINGIFY(MALLCTL_ARENAS_ALL) \".purge\", nullptr, nullptr, nullptr, 0);\n     ProfileEvents::increment(ProfileEvents::MemoryAllocatorPurge);\n@@ -53,12 +53,12 @@ void setJemallocProfileActive(bool value)\n     mallctl(\"prof.active\", &active, &active_size, nullptr, 0);\n     if (active == value)\n     {\n-        LOG_TRACE(&Poco::Logger::get(\"SystemJemalloc\"), \"Profiling is already {}\", active ? \"enabled\" : \"disabled\");\n+        LOG_TRACE(getLogger(\"SystemJemalloc\"), \"Profiling is already {}\", active ? \"enabled\" : \"disabled\");\n         return;\n     }\n \n     mallctl(\"prof.active\", nullptr, nullptr, &value, sizeof(bool));\n-    LOG_TRACE(&Poco::Logger::get(\"SystemJemalloc\"), \"Profiling is {}\", value ? \"enabled\" : \"disabled\");\n+    LOG_TRACE(getLogger(\"SystemJemalloc\"), \"Profiling is {}\", value ? \"enabled\" : \"disabled\");\n }\n \n std::string flushJemallocProfile(const std::string & file_prefix)\n@@ -69,7 +69,7 @@ std::string flushJemallocProfile(const std::string & file_prefix)\n     int n = mallctl(\"opt.prof_prefix\", &prefix_buffer, &prefix_size, nullptr, 0);\n     if (!n && std::string_view(prefix_buffer) != \"jeprof\")\n     {\n-        LOG_TRACE(&Poco::Logger::get(\"SystemJemalloc\"), \"Flushing memory profile with prefix {}\", prefix_buffer);\n+        LOG_TRACE(getLogger(\"SystemJemalloc\"), \"Flushing memory profile with prefix {}\", prefix_buffer);\n         mallctl(\"prof.dump\", nullptr, nullptr, nullptr, 0);\n         return prefix_buffer;\n     }\n@@ -78,7 +78,7 @@ std::string flushJemallocProfile(const std::string & file_prefix)\n     std::string profile_dump_path = fmt::format(\"{}.{}.{}.heap\", file_prefix, getpid(), profile_counter.fetch_add(1));\n     const auto * profile_dump_path_str = profile_dump_path.c_str();\n \n-    LOG_TRACE(&Poco::Logger::get(\"SystemJemalloc\"), \"Flushing memory profile to {}\", profile_dump_path_str);\n+    LOG_TRACE(getLogger(\"SystemJemalloc\"), \"Flushing memory profile to {}\", profile_dump_path_str);\n     mallctl(\"prof.dump\", nullptr, nullptr, &profile_dump_path_str, sizeof(profile_dump_path_str));\n     return profile_dump_path;\n }\ndiff --git a/src/Common/LRUResourceCache.h b/src/Common/LRUResourceCache.h\nindex 1fe3075a2a31..4ccaa272346d 100644\n--- a/src/Common/LRUResourceCache.h\n+++ b/src/Common/LRUResourceCache.h\n@@ -235,7 +235,7 @@ class LRUResourceCache\n                 else\n                 {\n                     // should not reach here\n-                    LOG_ERROR(&Poco::Logger::get(\"LRUResourceCache\"), \"element is in invalid status.\");\n+                    LOG_ERROR(getLogger(\"LRUResourceCache\"), \"element is in invalid status.\");\n                     abort();\n                 }\n             }\n@@ -306,7 +306,7 @@ class LRUResourceCache\n         auto it = cells.find(key);\n         if (it == cells.end() || it->second.reference_count == 0)\n         {\n-            LOG_ERROR(&Poco::Logger::get(\"LRUResourceCache\"), \"try to release an invalid element\");\n+            LOG_ERROR(getLogger(\"LRUResourceCache\"), \"try to release an invalid element\");\n             abort();\n         }\n \n@@ -359,7 +359,7 @@ class LRUResourceCache\n             auto cell_it = cells.find(key);\n             if (cell_it == cells.end())\n             {\n-                LOG_ERROR(&Poco::Logger::get(\"LRUResourceCache\"), \"LRUResourceCache became inconsistent. There must be a bug in it.\");\n+                LOG_ERROR(getLogger(\"LRUResourceCache\"), \"LRUResourceCache became inconsistent. There must be a bug in it.\");\n                 abort();\n             }\n \n@@ -379,7 +379,7 @@ class LRUResourceCache\n \n         if (loss_weight > current_weight + weight)\n         {\n-            LOG_ERROR(&Poco::Logger::get(\"LRUResourceCache\"), \"LRUResourceCache became inconsistent. There must be a bug in it.\");\n+            LOG_ERROR(getLogger(\"LRUResourceCache\"), \"LRUResourceCache became inconsistent. There must be a bug in it.\");\n             abort();\n         }\n \ndiff --git a/src/Common/Logger.cpp b/src/Common/Logger.cpp\nnew file mode 100644\nindex 000000000000..c8d557bc3a3b\n--- /dev/null\n+++ b/src/Common/Logger.cpp\n@@ -0,0 +1,27 @@\n+#include <Poco/Logger.h>\n+#include <Common/Logger.h>\n+\n+LoggerPtr getLogger(const std::string & name)\n+{\n+    return Poco::Logger::getShared(name);\n+}\n+\n+LoggerPtr createLogger(const std::string & name, Poco::Channel * channel, Poco::Message::Priority level)\n+{\n+    return Poco::Logger::createShared(name, channel, level);\n+}\n+\n+LoggerRawPtr getRawLogger(const std::string & name)\n+{\n+    return &Poco::Logger::get(name);\n+}\n+\n+LoggerRawPtr createRawLogger(const std::string & name, Poco::Channel * channel, Poco::Message::Priority level)\n+{\n+    return &Poco::Logger::create(name, channel, level);\n+}\n+\n+bool hasLogger(const std::string & name)\n+{\n+    return Poco::Logger::has(name);\n+}\ndiff --git a/src/Common/Logger.h b/src/Common/Logger.h\nnew file mode 100644\nindex 000000000000..13e1c6bf8f57\n--- /dev/null\n+++ b/src/Common/Logger.h\n@@ -0,0 +1,50 @@\n+#pragma once\n+\n+#include <memory>\n+\n+#include <Poco/Channel.h>\n+#include <Poco/Logger.h>\n+#include <Poco/Message.h>\n+\n+using LoggerPtr = Poco::LoggerPtr;\n+\n+using LoggerRawPtr = Poco::Logger *;\n+\n+/** RAII wrappers around Poco/Logger.h.\n+  *\n+  * You should use this functions in case Logger instance lifetime needs to be properly\n+  * managed, because otherwise it will leak memory.\n+  *\n+  * For example when Logger is created when table is created and Logger contains table name.\n+  * Then it must be destroyed when underlying table is destroyed.\n+  */\n+\n+/** Get Logger with specified name. If the Logger does not exists, it is created.\n+  * Logger is destroyed, when last shared ptr that refers to Logger with specified name is destroyed.\n+  */\n+LoggerPtr getLogger(const std::string & name);\n+\n+/** Create Logger with specified name, channel and logging level.\n+  * If Logger already exists, throws exception.\n+  * Logger is destroyed, when last shared ptr that refers to Logger with specified name is destroyed.\n+  */\n+LoggerPtr createLogger(const std::string & name, Poco::Channel * channel, Poco::Message::Priority level = Poco::Message::PRIO_INFORMATION);\n+\n+/** Create raw Poco::Logger that will not be destroyed before program termination.\n+  * This can be used in cases when specific Logger instance can be singletone.\n+  *\n+  * For example you need to pass Logger into low-level libraries as raw pointer, and using\n+  * RAII wrapper is inconvenient.\n+  *\n+  * Generally you should always use getLogger functions.\n+  */\n+\n+LoggerRawPtr getRawLogger(const std::string & name);\n+\n+LoggerRawPtr createRawLogger(const std::string & name, Poco::Channel * channel, Poco::Message::Priority level = Poco::Message::PRIO_INFORMATION);\n+\n+\n+/** Returns true, if currently Logger with specified name is created.\n+  * Otherwise, returns false.\n+  */\n+bool hasLogger(const std::string & name);\ndiff --git a/src/Common/LoggingFormatStringHelpers.cpp b/src/Common/LoggingFormatStringHelpers.cpp\nindex 074c8dd28039..b3353a590100 100644\n--- a/src/Common/LoggingFormatStringHelpers.cpp\n+++ b/src/Common/LoggingFormatStringHelpers.cpp\n@@ -80,8 +80,8 @@ void LogFrequencyLimiterIml::cleanup(time_t too_old_threshold_s)\n std::mutex LogSeriesLimiter::mutex;\n time_t LogSeriesLimiter::last_cleanup = 0;\n \n-LogSeriesLimiter::LogSeriesLimiter(Poco::Logger * logger_, size_t allowed_count_, time_t interval_s_)\n-    : logger(logger_)\n+LogSeriesLimiter::LogSeriesLimiter(LoggerPtr logger_, size_t allowed_count_, time_t interval_s_)\n+    : logger(std::move(logger_))\n {\n     if (allowed_count_ == 0)\n     {\ndiff --git a/src/Common/LoggingFormatStringHelpers.h b/src/Common/LoggingFormatStringHelpers.h\nindex ef7ec0c6144b..b0f0a5cd7167 100644\n--- a/src/Common/LoggingFormatStringHelpers.h\n+++ b/src/Common/LoggingFormatStringHelpers.h\n@@ -8,7 +8,7 @@\n #include <Poco/Logger.h>\n #include <Poco/Message.h>\n #include <base/EnumReflection.h>\n-\n+#include <Common/Logger.h>\n \n struct PreformattedMessage;\n consteval void formatStringCheckArgsNumImpl(std::string_view str, size_t nargs);\n@@ -203,10 +203,10 @@ class LogFrequencyLimiterIml\n     static time_t last_cleanup;\n     static std::mutex mutex;\n \n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n     time_t min_interval_s;\n public:\n-    LogFrequencyLimiterIml(Poco::Logger * logger_, time_t min_interval_s_) : logger(logger_), min_interval_s(min_interval_s_) {}\n+    LogFrequencyLimiterIml(LoggerPtr logger_, time_t min_interval_s_) : logger(std::move(logger_)), min_interval_s(min_interval_s_) {}\n \n     LogFrequencyLimiterIml & operator -> () { return *this; }\n     bool is(Poco::Message::Priority priority) { return logger->is(priority); }\n@@ -218,7 +218,7 @@ class LogFrequencyLimiterIml\n     /// Clears messages that were logged last time more than too_old_threshold_s seconds ago\n     static void cleanup(time_t too_old_threshold_s = 600);\n \n-    Poco::Logger * getLogger() { return logger; }\n+    LoggerPtr getLogger() { return logger; }\n };\n \n /// This wrapper helps to avoid too noisy log messages from similar objects.\n@@ -240,11 +240,11 @@ class LogSeriesLimiter\n         return records;\n     }\n \n-    Poco::Logger * logger = nullptr;\n+    LoggerPtr logger = nullptr;\n     bool accepted = false;\n     String debug_message;\n public:\n-    LogSeriesLimiter(Poco::Logger * logger_, size_t allowed_count_, time_t interval_s_);\n+    LogSeriesLimiter(LoggerPtr logger_, size_t allowed_count_, time_t interval_s_);\n \n     LogSeriesLimiter & operator -> () { return *this; }\n     bool is(Poco::Message::Priority priority) { return logger->is(priority); }\n@@ -253,18 +253,18 @@ class LogSeriesLimiter\n \n     void log(Poco::Message & message);\n \n-    Poco::Logger * getLogger() { return logger; }\n+    LoggerPtr getLogger() { return logger; }\n };\n \n /// This wrapper is useful to save formatted message into a String before sending it to a logger\n class LogToStrImpl\n {\n     String & out_str;\n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n     std::unique_ptr<LogFrequencyLimiterIml> maybe_nested;\n     bool propagate_to_actual_log = true;\n public:\n-    LogToStrImpl(String & out_str_, Poco::Logger * logger_) : out_str(out_str_), logger(logger_) {}\n+    LogToStrImpl(String & out_str_, LoggerPtr logger_) : out_str(out_str_), logger(std::move(logger_)) {}\n     LogToStrImpl(String & out_str_, std::unique_ptr<LogFrequencyLimiterIml> && maybe_nested_)\n         : out_str(out_str_), logger(maybe_nested_->getLogger()), maybe_nested(std::move(maybe_nested_)) {}\n     LogToStrImpl & operator -> () { return *this; }\ndiff --git a/src/Common/Macros.cpp b/src/Common/Macros.cpp\nindex 0035e7abfe85..9e0977d9bcc2 100644\n--- a/src/Common/Macros.cpp\n+++ b/src/Common/Macros.cpp\n@@ -38,6 +38,10 @@ Macros::Macros(const Poco::Util::AbstractConfiguration & config, const String &\n     }\n }\n \n+Macros::Macros(const Poco::Util::AbstractConfiguration & config, const String & root_key, LoggerPtr log)\n+    : Macros(config, root_key, log.get())\n+{}\n+\n Macros::Macros(std::map<String, String> map)\n {\n     macros = std::move(map);\ndiff --git a/src/Common/Macros.h b/src/Common/Macros.h\nindex 9fe5717effc0..8b9eded7dcb7 100644\n--- a/src/Common/Macros.h\n+++ b/src/Common/Macros.h\n@@ -26,6 +26,7 @@ class Macros\n {\n public:\n     Macros() = default;\n+    Macros(const Poco::Util::AbstractConfiguration & config, const String & key, LoggerPtr log = nullptr);\n     Macros(const Poco::Util::AbstractConfiguration & config, const String & key, Poco::Logger * log = nullptr);\n     explicit Macros(std::map<String, String> map);\n \ndiff --git a/src/Common/MemoryTracker.cpp b/src/Common/MemoryTracker.cpp\nindex 5672bb0ae10d..28cfa98666aa 100644\n--- a/src/Common/MemoryTracker.cpp\n+++ b/src/Common/MemoryTracker.cpp\n@@ -155,14 +155,14 @@ void MemoryTracker::logPeakMemoryUsage()\n     auto peak_bytes = peak.load(std::memory_order::relaxed);\n     if (peak_bytes < 128 * 1024)\n         return;\n-    LOG_DEBUG(&Poco::Logger::get(\"MemoryTracker\"),\n+    LOG_DEBUG(getLogger(\"MemoryTracker\"),\n         \"Peak memory usage{}: {}.\", (description ? \" \" + std::string(description) : \"\"), ReadableSize(peak_bytes));\n }\n \n void MemoryTracker::logMemoryUsage(Int64 current) const\n {\n     const auto * description = description_ptr.load(std::memory_order_relaxed);\n-    LOG_DEBUG(&Poco::Logger::get(\"MemoryTracker\"),\n+    LOG_DEBUG(getLogger(\"MemoryTracker\"),\n         \"Current memory usage{}: {}.\", (description ? \" \" + std::string(description) : \"\"), ReadableSize(current));\n }\n \n@@ -170,7 +170,7 @@ void MemoryTracker::injectFault() const\n {\n     if (!memoryTrackerCanThrow(level, true))\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"MemoryTracker\"),\n+        LOG_WARNING(getLogger(\"MemoryTracker\"),\n                     \"Cannot inject fault at specific point. Uncaught exceptions: {}, stack trace:\\n{}\",\n                     std::uncaught_exceptions(), StackTrace().toString());\n         return;\n@@ -201,7 +201,7 @@ void MemoryTracker::debugLogBigAllocationWithoutCheck(Int64 size [[maybe_unused]\n         return;\n \n     MemoryTrackerBlockerInThread blocker(VariableContext::Global);\n-    LOG_TEST(&Poco::Logger::get(\"MemoryTracker\"), \"Too big allocation ({} bytes) without checking memory limits, \"\n+    LOG_TEST(getLogger(\"MemoryTracker\"), \"Too big allocation ({} bytes) without checking memory limits, \"\n                                                    \"it may lead to OOM. Stack trace: {}\", size, StackTrace().toString());\n #else\n     return;     /// Avoid trash logging in release builds\ndiff --git a/src/Common/NamedCollections/NamedCollectionUtils.cpp b/src/Common/NamedCollections/NamedCollectionUtils.cpp\nindex d62d54b9f375..fe0f42467c78 100644\n--- a/src/Common/NamedCollections/NamedCollectionUtils.cpp\n+++ b/src/Common/NamedCollections/NamedCollectionUtils.cpp\n@@ -135,7 +135,7 @@ class LoadFromSQL : private WithContext\n             else\n             {\n                 LOG_WARNING(\n-                    &Poco::Logger::get(\"NamedCollectionsLoadFromSQL\"),\n+                    getLogger(\"NamedCollectionsLoadFromSQL\"),\n                     \"Unexpected file {} in named collections directory\",\n                     current_path.filename().string());\n             }\n@@ -345,7 +345,7 @@ void loadFromConfigUnlocked(const Poco::Util::AbstractConfiguration & config, st\n {\n     auto named_collections = LoadFromConfig(config).getAll();\n     LOG_TRACE(\n-        &Poco::Logger::get(\"NamedCollectionsUtils\"),\n+        getLogger(\"NamedCollectionsUtils\"),\n         \"Loaded {} collections from config\", named_collections.size());\n \n     NamedCollectionFactory::instance().add(std::move(named_collections));\n@@ -372,7 +372,7 @@ void loadFromSQLUnlocked(ContextPtr context, std::unique_lock<std::mutex> &)\n {\n     auto named_collections = LoadFromSQL(context).getAll();\n     LOG_TRACE(\n-        &Poco::Logger::get(\"NamedCollectionsUtils\"),\n+        getLogger(\"NamedCollectionsUtils\"),\n         \"Loaded {} collections from SQL\", named_collections.size());\n \n     NamedCollectionFactory::instance().add(std::move(named_collections));\ndiff --git a/src/Common/NetlinkMetricsProvider.cpp b/src/Common/NetlinkMetricsProvider.cpp\nindex 23173f316896..6969b5b75429 100644\n--- a/src/Common/NetlinkMetricsProvider.cpp\n+++ b/src/Common/NetlinkMetricsProvider.cpp\n@@ -216,7 +216,7 @@ bool checkPermissionsImpl()\n         {\n             /// This error happens all the time when running inside Docker - consider it ok,\n             /// don't create noise with this error.\n-            LOG_DEBUG(&Poco::Logger::get(__PRETTY_FUNCTION__), getCurrentExceptionMessageAndPattern(/* with_stacktrace */ false));\n+            LOG_DEBUG(getLogger(__PRETTY_FUNCTION__), getCurrentExceptionMessageAndPattern(/* with_stacktrace */ false));\n         }\n         else\n         {\ndiff --git a/src/Common/OptimizedRegularExpression.cpp b/src/Common/OptimizedRegularExpression.cpp\nindex 8a5804692782..b6852964efe5 100644\n--- a/src/Common/OptimizedRegularExpression.cpp\n+++ b/src/Common/OptimizedRegularExpression.cpp\n@@ -463,7 +463,7 @@ catch (...)\n     is_trivial = false;\n     required_substring_is_prefix = false;\n     alternatives.clear();\n-    LOG_ERROR(&Poco::Logger::get(\"OptimizeRegularExpression\"), \"Analyze RegularExpression failed, got error: {}\", DB::getCurrentExceptionMessage(false));\n+    LOG_ERROR(getLogger(\"OptimizeRegularExpression\"), \"Analyze RegularExpression failed, got error: {}\", DB::getCurrentExceptionMessage(false));\n }\n \n OptimizedRegularExpression::OptimizedRegularExpression(const std::string & regexp_, int options)\ndiff --git a/src/Common/PipeFDs.cpp b/src/Common/PipeFDs.cpp\nindex f2a913467a9d..ceadbb2f9833 100644\n--- a/src/Common/PipeFDs.cpp\n+++ b/src/Common/PipeFDs.cpp\n@@ -97,7 +97,7 @@ void LazyPipeFDs::setNonBlockingReadWrite()\n void LazyPipeFDs::tryIncreaseSize(int desired_size)\n {\n #if defined(OS_LINUX)\n-    Poco::Logger * log = &Poco::Logger::get(\"Pipe\");\n+    LoggerPtr log = getLogger(\"Pipe\");\n \n     /** Increase pipe size to avoid slowdown during fine-grained trace collection.\n       */\ndiff --git a/src/Common/PoolBase.h b/src/Common/PoolBase.h\nindex 5575b56f2999..ef35002c45a1 100644\n--- a/src/Common/PoolBase.h\n+++ b/src/Common/PoolBase.h\n@@ -223,9 +223,9 @@ class PoolBase : private boost::noncopyable\n     std::condition_variable available;\n \n protected:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n-    PoolBase(unsigned max_items_, Poco::Logger * log_, BehaviourOnLimit behaviour_on_limit_ = BehaviourOnLimit::Wait)\n+    PoolBase(unsigned max_items_, LoggerPtr log_, BehaviourOnLimit behaviour_on_limit_ = BehaviourOnLimit::Wait)\n         : max_items(max_items_), behaviour_on_limit(behaviour_on_limit_), log(log_)\n     {\n         items.reserve(max_items);\ndiff --git a/src/Common/PoolWithFailoverBase.h b/src/Common/PoolWithFailoverBase.h\nindex f960d551996c..6da4445950ce 100644\n--- a/src/Common/PoolWithFailoverBase.h\n+++ b/src/Common/PoolWithFailoverBase.h\n@@ -58,7 +58,7 @@ class PoolWithFailoverBase : private boost::noncopyable\n             NestedPools nested_pools_,\n             time_t decrease_error_period_,\n             size_t max_error_cap_,\n-            Poco::Logger * log_)\n+            LoggerPtr log_)\n         : nested_pools(std::move(nested_pools_))\n         , decrease_error_period(decrease_error_period_)\n         , max_error_cap(max_error_cap_)\n@@ -159,7 +159,7 @@ class PoolWithFailoverBase : private boost::noncopyable\n     /// The time when error counts were last decreased.\n     time_t last_error_decrease_time = 0;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \ndiff --git a/src/Common/ProxyConfigurationResolverProvider.cpp b/src/Common/ProxyConfigurationResolverProvider.cpp\nindex 96ebb934643e..d15b4d98615d 100644\n--- a/src/Common/ProxyConfigurationResolverProvider.cpp\n+++ b/src/Common/ProxyConfigurationResolverProvider.cpp\n@@ -36,7 +36,7 @@ namespace\n         auto proxy_port = configuration.getUInt(resolver_prefix + \".proxy_port\");\n         auto cache_ttl = configuration.getUInt(resolver_prefix + \".proxy_cache_time\", 10);\n \n-        LOG_DEBUG(&Poco::Logger::get(\"ProxyConfigurationResolverProvider\"), \"Configured remote proxy resolver: {}, Scheme: {}, Port: {}\",\n+        LOG_DEBUG(getLogger(\"ProxyConfigurationResolverProvider\"), \"Configured remote proxy resolver: {}, Scheme: {}, Port: {}\",\n                   endpoint.toString(), proxy_scheme, proxy_port);\n \n         auto server_configuration = RemoteProxyConfigurationResolver::RemoteServerConfiguration {\n@@ -71,7 +71,7 @@ namespace\n \n                 uris.push_back(proxy_uri);\n \n-                LOG_DEBUG(&Poco::Logger::get(\"ProxyConfigurationResolverProvider\"), \"Configured proxy: {}\", proxy_uri.toString());\n+                LOG_DEBUG(getLogger(\"ProxyConfigurationResolverProvider\"), \"Configured proxy: {}\", proxy_uri.toString());\n             }\n         }\n \ndiff --git a/src/Common/ProxyListConfigurationResolver.cpp b/src/Common/ProxyListConfigurationResolver.cpp\nindex 68d676643d62..01a6f52185fc 100644\n--- a/src/Common/ProxyListConfigurationResolver.cpp\n+++ b/src/Common/ProxyListConfigurationResolver.cpp\n@@ -26,7 +26,7 @@ ProxyConfiguration ProxyListConfigurationResolver::resolve()\n \n     auto & proxy = proxies[index];\n \n-    LOG_DEBUG(&Poco::Logger::get(\"ProxyListConfigurationResolver\"), \"Use proxy: {}\", proxies[index].toString());\n+    LOG_DEBUG(getLogger(\"ProxyListConfigurationResolver\"), \"Use proxy: {}\", proxies[index].toString());\n \n     return ProxyConfiguration {\n         proxy.getHost(),\ndiff --git a/src/Common/QueryProfiler.cpp b/src/Common/QueryProfiler.cpp\nindex 16c8d4e223f7..34ffbf6c498f 100644\n--- a/src/Common/QueryProfiler.cpp\n+++ b/src/Common/QueryProfiler.cpp\n@@ -105,7 +105,7 @@ namespace ErrorCodes\n \n #ifndef __APPLE__\n Timer::Timer()\n-    : log(&Poco::Logger::get(\"Timer\"))\n+    : log(getLogger(\"Timer\"))\n {}\n \n void Timer::createIfNecessary(UInt64 thread_id, int clock_type, int pause_signal)\n@@ -211,7 +211,7 @@ void Timer::cleanup()\n \n template <typename ProfilerImpl>\n QueryProfilerBase<ProfilerImpl>::QueryProfilerBase(UInt64 thread_id, int clock_type, UInt32 period, int pause_signal_)\n-    : log(&Poco::Logger::get(\"QueryProfiler\"))\n+    : log(getLogger(\"QueryProfiler\"))\n     , pause_signal(pause_signal_)\n {\n #if defined(SANITIZER)\ndiff --git a/src/Common/QueryProfiler.h b/src/Common/QueryProfiler.h\nindex 87432a4b6998..254b11137ccb 100644\n--- a/src/Common/QueryProfiler.h\n+++ b/src/Common/QueryProfiler.h\n@@ -7,6 +7,8 @@\n \n #include \"config.h\"\n \n+#include <Common/Logger.h>\n+\n \n namespace Poco\n {\n@@ -43,7 +45,7 @@ class Timer\n     void cleanup();\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::optional<timer_t> timer_id;\n };\n #endif\n@@ -58,7 +60,7 @@ class QueryProfilerBase\n private:\n     void cleanup();\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n #ifndef __APPLE__\n     inline static thread_local Timer timer = Timer();\ndiff --git a/src/Common/RemoteProxyConfigurationResolver.cpp b/src/Common/RemoteProxyConfigurationResolver.cpp\nindex 7342933beff5..117c8a34dbb5 100644\n--- a/src/Common/RemoteProxyConfigurationResolver.cpp\n+++ b/src/Common/RemoteProxyConfigurationResolver.cpp\n@@ -27,7 +27,7 @@ RemoteProxyConfigurationResolver::RemoteProxyConfigurationResolver(\n \n ProxyConfiguration RemoteProxyConfigurationResolver::resolve()\n {\n-    auto * logger = &Poco::Logger::get(\"RemoteProxyConfigurationResolver\");\n+    auto logger = getLogger(\"RemoteProxyConfigurationResolver\");\n \n     auto & [endpoint, proxy_protocol, proxy_port, cache_ttl_] = remote_server_configuration;\n \ndiff --git a/src/Common/SensitiveDataMasker.cpp b/src/Common/SensitiveDataMasker.cpp\nindex 33770c3e78af..70346919f65b 100644\n--- a/src/Common/SensitiveDataMasker.cpp\n+++ b/src/Common/SensitiveDataMasker.cpp\n@@ -112,7 +112,7 @@ SensitiveDataMasker::SensitiveDataMasker(const Poco::Util::AbstractConfiguration\n {\n     Poco::Util::AbstractConfiguration::Keys keys;\n     config.keys(config_prefix, keys);\n-    Poco::Logger * logger = &Poco::Logger::get(\"SensitiveDataMaskerConfigRead\");\n+    LoggerPtr logger = getLogger(\"SensitiveDataMaskerConfigRead\");\n \n     std::set<std::string> used_names;\n \ndiff --git a/src/Common/ShellCommand.cpp b/src/Common/ShellCommand.cpp\nindex f4efc9e3526e..98a21b43d763 100644\n--- a/src/Common/ShellCommand.cpp\n+++ b/src/Common/ShellCommand.cpp\n@@ -54,9 +54,9 @@ ShellCommand::ShellCommand(pid_t pid_, int & in_fd_, int & out_fd_, int & err_fd\n {\n }\n \n-Poco::Logger * ShellCommand::getLogger()\n+LoggerPtr ShellCommand::getLogger()\n {\n-    return &Poco::Logger::get(\"ShellCommand\");\n+    return ::getLogger(\"ShellCommand\");\n }\n \n ShellCommand::~ShellCommand()\ndiff --git a/src/Common/ShellCommand.h b/src/Common/ShellCommand.h\nindex da65d2ae494f..5ebc1daefa1c 100644\n--- a/src/Common/ShellCommand.h\n+++ b/src/Common/ShellCommand.h\n@@ -97,7 +97,7 @@ class ShellCommand final\n \n     bool tryWaitProcessWithTimeout(size_t timeout_in_seconds);\n \n-    static Poco::Logger * getLogger();\n+    static LoggerPtr getLogger();\n \n     /// Print command name and the list of arguments to log. NOTE: No escaping of arguments is performed.\n     static void logCommand(const char * filename, char * const argv[]);\ndiff --git a/src/Common/StatusFile.cpp b/src/Common/StatusFile.cpp\nindex 0a9aa2f27395..56eb1d4d0cbc 100644\n--- a/src/Common/StatusFile.cpp\n+++ b/src/Common/StatusFile.cpp\n@@ -56,9 +56,9 @@ StatusFile::StatusFile(std::string path_, FillFunction fill_)\n         }\n \n         if (!contents.empty())\n-            LOG_INFO(&Poco::Logger::get(\"StatusFile\"), \"Status file {} already exists - unclean restart. Contents:\\n{}\", path, contents);\n+            LOG_INFO(getLogger(\"StatusFile\"), \"Status file {} already exists - unclean restart. Contents:\\n{}\", path, contents);\n         else\n-            LOG_INFO(&Poco::Logger::get(\"StatusFile\"), \"Status file {} already exists and is empty - probably unclean hardware restart.\", path);\n+            LOG_INFO(getLogger(\"StatusFile\"), \"Status file {} already exists and is empty - probably unclean hardware restart.\", path);\n     }\n \n     fd = ::open(path.c_str(), O_WRONLY | O_CREAT | O_CLOEXEC, 0666);\n@@ -99,10 +99,10 @@ StatusFile::StatusFile(std::string path_, FillFunction fill_)\n StatusFile::~StatusFile()\n {\n     if (0 != close(fd))\n-        LOG_ERROR(&Poco::Logger::get(\"StatusFile\"), \"Cannot close file {}, {}\", path, errnoToString());\n+        LOG_ERROR(getLogger(\"StatusFile\"), \"Cannot close file {}, {}\", path, errnoToString());\n \n     if (0 != unlink(path.c_str()))\n-        LOG_ERROR(&Poco::Logger::get(\"StatusFile\"), \"Cannot unlink file {}, {}\", path, errnoToString());\n+        LOG_ERROR(getLogger(\"StatusFile\"), \"Cannot unlink file {}, {}\", path, errnoToString());\n }\n \n }\ndiff --git a/src/Common/SystemLogBase.cpp b/src/Common/SystemLogBase.cpp\nindex d82b582fee6b..4dee6d905d99 100644\n--- a/src/Common/SystemLogBase.cpp\n+++ b/src/Common/SystemLogBase.cpp\n@@ -39,7 +39,7 @@ ISystemLog::~ISystemLog() = default;\n \n template <typename LogElement>\n SystemLogQueue<LogElement>::SystemLogQueue(const SystemLogQueueSettings & settings_)\n-    : log(&Poco::Logger::get(\"SystemLogQueue (\" + settings_.database + \".\" +settings_.table + \")\"))\n+    : log(getLogger(\"SystemLogQueue (\" + settings_.database + \".\" +settings_.table + \")\"))\n     , settings(settings_)\n \n {\ndiff --git a/src/Common/SystemLogBase.h b/src/Common/SystemLogBase.h\nindex 1ced313b36ac..a734c70f2852 100644\n--- a/src/Common/SystemLogBase.h\n+++ b/src/Common/SystemLogBase.h\n@@ -121,7 +121,7 @@ class SystemLogQueue\n     /// Data shared between callers of add()/flush()/shutdown(), and the saving thread\n     std::mutex mutex;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     // Queue is bounded. But its size is quite large to not block in all normal cases.\n     std::vector<LogElement> queue;\ndiff --git a/src/Common/TLDListsHolder.cpp b/src/Common/TLDListsHolder.cpp\nindex 623b88f83a54..c3991b869831 100644\n--- a/src/Common/TLDListsHolder.cpp\n+++ b/src/Common/TLDListsHolder.cpp\n@@ -55,7 +55,7 @@ void TLDListsHolder::parseConfig(const std::string & top_level_domains_path, con\n     Poco::Util::AbstractConfiguration::Keys config_keys;\n     config.keys(\"top_level_domains_lists\", config_keys);\n \n-    Poco::Logger * log = &Poco::Logger::get(\"TLDListsHolder\");\n+    LoggerPtr log = getLogger(\"TLDListsHolder\");\n \n     for (const auto & key : config_keys)\n     {\ndiff --git a/src/Common/ThreadProfileEvents.cpp b/src/Common/ThreadProfileEvents.cpp\nindex 256f53df011a..990151d73fff 100644\n--- a/src/Common/ThreadProfileEvents.cpp\n+++ b/src/Common/ThreadProfileEvents.cpp\n@@ -300,7 +300,7 @@ static void enablePerfEvent(int event_fd)\n {\n     if (ioctl(event_fd, PERF_EVENT_IOC_ENABLE, 0))\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+        LOG_WARNING(getLogger(\"PerfEvents\"),\n             \"Can't enable perf event with file descriptor {}: '{}' ({})\",\n             event_fd, errnoToString(), errno);\n     }\n@@ -310,7 +310,7 @@ static void disablePerfEvent(int event_fd)\n {\n     if (ioctl(event_fd, PERF_EVENT_IOC_DISABLE, 0))\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+        LOG_WARNING(getLogger(\"PerfEvents\"),\n             \"Can't disable perf event with file descriptor {}: '{}' ({})\",\n             event_fd, errnoToString(), errno);\n     }\n@@ -320,7 +320,7 @@ static void releasePerfEvent(int event_fd)\n {\n     if (close(event_fd))\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+        LOG_WARNING(getLogger(\"PerfEvents\"),\n             \"Can't close perf event file descriptor {}: {} ({})\",\n             event_fd, errnoToString(), errno);\n     }\n@@ -333,12 +333,12 @@ static bool validatePerfEventDescriptor(int & fd)\n \n     if (errno == EBADF)\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+        LOG_WARNING(getLogger(\"PerfEvents\"),\n             \"Event descriptor {} was closed from the outside; reopening\", fd);\n     }\n     else\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+        LOG_WARNING(getLogger(\"PerfEvents\"),\n             \"Error while checking availability of event descriptor {}: {} ({})\",\n             fd, errnoToString(), errno);\n \n@@ -416,7 +416,7 @@ bool PerfEventsCounters::processThreadLocalChanges(const std::string & needed_ev\n     bool has_cap_sys_admin = hasLinuxCapability(CAP_SYS_ADMIN);\n     if (perf_event_paranoid >= 3 && !has_cap_sys_admin)\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+        LOG_WARNING(getLogger(\"PerfEvents\"),\n             \"Not enough permissions to record perf events: \"\n             \"perf_event_paranoid = {} and CAP_SYS_ADMIN = 0\",\n             perf_event_paranoid);\n@@ -444,7 +444,7 @@ bool PerfEventsCounters::processThreadLocalChanges(const std::string & needed_ev\n             // ENOENT means that the event is not supported. Don't log it, because\n             // this is called for each thread and would be too verbose. Log other\n             // error codes because they might signify an error.\n-            LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+            LOG_WARNING(getLogger(\"PerfEvents\"),\n                 \"Failed to open perf event {} (event_type={}, event_config={}): \"\n                 \"'{}' ({})\", event_info.settings_name, event_info.event_type,\n                 event_info.event_config, errnoToString(), errno);\n@@ -484,7 +484,7 @@ std::vector<size_t> PerfEventsCounters::eventIndicesFromString(const std::string\n         }\n         else\n         {\n-            LOG_ERROR(&Poco::Logger::get(\"PerfEvents\"),\n+            LOG_ERROR(getLogger(\"PerfEvents\"),\n                 \"Unknown perf event name '{}' specified in settings\", event_name);\n         }\n     }\n@@ -531,7 +531,7 @@ void PerfEventsCounters::finalizeProfileEvents(ProfileEvents::Counters & profile\n \n         if (bytes_read != bytes_to_read)\n         {\n-            LOG_WARNING(&Poco::Logger::get(\"PerfEvents\"),\n+            LOG_WARNING(getLogger(\"PerfEvents\"),\n                 \"Can't read event value from file descriptor {}: '{}' ({})\",\n                 fd, errnoToString(), errno);\n             current_values[i] = {};\ndiff --git a/src/Common/ThreadStatus.cpp b/src/Common/ThreadStatus.cpp\nindex c99823b2dfaf..05524a5d6b9b 100644\n--- a/src/Common/ThreadStatus.cpp\n+++ b/src/Common/ThreadStatus.cpp\n@@ -76,7 +76,7 @@ ThreadStatus::ThreadStatus(bool check_current_thread_on_destruction_)\n     last_rusage = std::make_unique<RUsageCounters>();\n \n     memory_tracker.setDescription(\"(for thread)\");\n-    log = &Poco::Logger::get(\"ThreadStatus\");\n+    log = getLogger(\"ThreadStatus\");\n \n     current_thread = this;\n \ndiff --git a/src/Common/ThreadStatus.h b/src/Common/ThreadStatus.h\nindex b8bdebf10edf..f7534c35a982 100644\n--- a/src/Common/ThreadStatus.h\n+++ b/src/Common/ThreadStatus.h\n@@ -236,7 +236,7 @@ class ThreadStatus : public boost::noncopyable\n     using Deleter = std::function<void()>;\n     Deleter deleter;\n \n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n \n     bool check_current_thread_on_destruction;\n \ndiff --git a/src/Common/ZooKeeper/ZooKeeper.cpp b/src/Common/ZooKeeper/ZooKeeper.cpp\nindex 70b8df5cd2cd..8a8465de491e 100644\n--- a/src/Common/ZooKeeper/ZooKeeper.cpp\n+++ b/src/Common/ZooKeeper/ZooKeeper.cpp\n@@ -60,7 +60,7 @@ void ZooKeeper::init(ZooKeeperArgs args_)\n \n {\n     args = std::move(args_);\n-    log = &Poco::Logger::get(\"ZooKeeper\");\n+    log = getLogger(\"ZooKeeper\");\n \n     if (args.implementation == \"zookeeper\")\n     {\n@@ -1455,7 +1455,7 @@ Coordination::RequestPtr makeExistsRequest(const std::string & path)\n     return request;\n }\n \n-std::string normalizeZooKeeperPath(std::string zookeeper_path, bool check_starts_with_slash, Poco::Logger * log)\n+std::string normalizeZooKeeperPath(std::string zookeeper_path, bool check_starts_with_slash, LoggerPtr log)\n {\n     if (!zookeeper_path.empty() && zookeeper_path.back() == '/')\n         zookeeper_path.resize(zookeeper_path.size() - 1);\n@@ -1491,7 +1491,7 @@ String extractZooKeeperName(const String & path)\n     return default_zookeeper_name;\n }\n \n-String extractZooKeeperPath(const String & path, bool check_starts_with_slash, Poco::Logger * log)\n+String extractZooKeeperPath(const String & path, bool check_starts_with_slash, LoggerPtr log)\n {\n     if (path.empty())\n         throw DB::Exception(DB::ErrorCodes::BAD_ARGUMENTS, \"ZooKeeper path should not be empty\");\ndiff --git a/src/Common/ZooKeeper/ZooKeeper.h b/src/Common/ZooKeeper/ZooKeeper.h\nindex 1f29af0797b8..811546fb4b9b 100644\n--- a/src/Common/ZooKeeper/ZooKeeper.h\n+++ b/src/Common/ZooKeeper/ZooKeeper.h\n@@ -650,7 +650,7 @@ class ZooKeeper\n \n     ZooKeeperArgs args;\n \n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n     std::shared_ptr<DB::ZooKeeperLog> zk_log;\n \n     AtomicStopwatch session_uptime;\n@@ -729,7 +729,7 @@ class EphemeralNodeHolder\n             else\n             {\n                 ProfileEvents::increment(ProfileEvents::CannotRemoveEphemeralNode);\n-                LOG_DEBUG(&Poco::Logger::get(\"EphemeralNodeHolder\"), \"Cannot remove {} since session has been expired\", path);\n+                LOG_DEBUG(getLogger(\"EphemeralNodeHolder\"), \"Cannot remove {} since session has been expired\", path);\n             }\n         }\n         catch (...)\n@@ -749,11 +749,11 @@ class EphemeralNodeHolder\n \n using EphemeralNodeHolderPtr = EphemeralNodeHolder::Ptr;\n \n-String normalizeZooKeeperPath(std::string zookeeper_path, bool check_starts_with_slash, Poco::Logger * log = nullptr);\n+String normalizeZooKeeperPath(std::string zookeeper_path, bool check_starts_with_slash, LoggerPtr log = nullptr);\n \n String extractZooKeeperName(const String & path);\n \n-String extractZooKeeperPath(const String & path, bool check_starts_with_slash, Poco::Logger * log = nullptr);\n+String extractZooKeeperPath(const String & path, bool check_starts_with_slash, LoggerPtr log = nullptr);\n \n String getSequentialNodeName(const String & prefix, UInt64 number);\n \ndiff --git a/src/Common/ZooKeeper/ZooKeeperCommon.cpp b/src/Common/ZooKeeper/ZooKeeperCommon.cpp\nindex 592d142e9250..660ae59e81e9 100644\n--- a/src/Common/ZooKeeper/ZooKeeperCommon.cpp\n+++ b/src/Common/ZooKeeper/ZooKeeperCommon.cpp\n@@ -929,7 +929,7 @@ ZooKeeperRequest::~ZooKeeperRequest()\n     constexpr UInt64 max_request_time_ns = 1000000000ULL; /// 1 sec\n     if (max_request_time_ns < elapsed_ns)\n     {\n-        LOG_TEST(&Poco::Logger::get(__PRETTY_FUNCTION__), \"Processing of request xid={} took {} ms\", xid, elapsed_ns / 1000000UL);\n+        LOG_TEST(getLogger(__PRETTY_FUNCTION__), \"Processing of request xid={} took {} ms\", xid, elapsed_ns / 1000000UL);\n     }\n }\n \n@@ -950,7 +950,7 @@ ZooKeeperResponse::~ZooKeeperResponse()\n     constexpr UInt64 max_request_time_ns = 1000000000ULL; /// 1 sec\n     if (max_request_time_ns < elapsed_ns)\n     {\n-        LOG_TEST(&Poco::Logger::get(__PRETTY_FUNCTION__), \"Processing of response xid={} took {} ms\", xid, elapsed_ns / 1000000UL);\n+        LOG_TEST(getLogger(__PRETTY_FUNCTION__), \"Processing of response xid={} took {} ms\", xid, elapsed_ns / 1000000UL);\n     }\n }\n \ndiff --git a/src/Common/ZooKeeper/ZooKeeperImpl.cpp b/src/Common/ZooKeeper/ZooKeeperImpl.cpp\nindex d732b900d37f..1fbadbd7616c 100644\n--- a/src/Common/ZooKeeper/ZooKeeperImpl.cpp\n+++ b/src/Common/ZooKeeper/ZooKeeperImpl.cpp\n@@ -342,7 +342,7 @@ ZooKeeper::ZooKeeper(\n     std::shared_ptr<ZooKeeperLog> zk_log_)\n     : args(args_)\n {\n-    log = &Poco::Logger::get(\"ZooKeeperClient\");\n+    log = getLogger(\"ZooKeeperClient\");\n     std::atomic_store(&zk_log, std::move(zk_log_));\n \n     if (!args.chroot.empty())\ndiff --git a/src/Common/ZooKeeper/ZooKeeperImpl.h b/src/Common/ZooKeeper/ZooKeeperImpl.h\nindex 13e1dc9e3cd8..b63f67bf7a6f 100644\n--- a/src/Common/ZooKeeper/ZooKeeperImpl.h\n+++ b/src/Common/ZooKeeper/ZooKeeperImpl.h\n@@ -308,7 +308,7 @@ class ZooKeeper final : public IKeeper\n     ThreadReference send_thread;\n     ThreadReference receive_thread;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     void connect(\n         const Nodes & node,\ndiff --git a/src/Common/ZooKeeper/ZooKeeperLock.cpp b/src/Common/ZooKeeper/ZooKeeperLock.cpp\nindex 6ee1c380efbb..b90bcfd2b55a 100644\n--- a/src/Common/ZooKeeper/ZooKeeperLock.cpp\n+++ b/src/Common/ZooKeeper/ZooKeeperLock.cpp\n@@ -26,7 +26,7 @@ ZooKeeperLock::ZooKeeperLock(\n     : zookeeper(zookeeper_)\n     , lock_path(fs::path(lock_prefix_) / lock_name_)\n     , lock_message(lock_message_)\n-    , log(&Poco::Logger::get(\"zkutil::Lock\"))\n+    , log(getLogger(\"zkutil::Lock\"))\n {\n     zookeeper->createIfNotExists(lock_prefix_, \"\");\n }\ndiff --git a/src/Common/ZooKeeper/ZooKeeperLock.h b/src/Common/ZooKeeper/ZooKeeperLock.h\nindex 146527c6c943..6271afaf6e24 100644\n--- a/src/Common/ZooKeeper/ZooKeeperLock.h\n+++ b/src/Common/ZooKeeper/ZooKeeperLock.h\n@@ -46,7 +46,7 @@ class ZooKeeperLock\n \n     std::string lock_path;\n     std::string lock_message;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     bool locked = false;\n \n };\ndiff --git a/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.cpp b/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.cpp\nindex 72923ca0487f..694004dee232 100644\n--- a/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.cpp\n+++ b/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.cpp\n@@ -9,7 +9,7 @@ ZooKeeperWithFaultInjection::ZooKeeperWithFaultInjection(\n     double fault_injection_probability,\n     UInt64 fault_injection_seed,\n     std::string name_,\n-    Poco::Logger * logger_)\n+    LoggerPtr logger_)\n     : keeper(keeper_)\n     , fault_policy(std::make_unique<RandomFaultInjection>(fault_injection_probability, fault_injection_seed))\n     , name(std::move(name_))\ndiff --git a/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.h b/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.h\nindex 57e1f0f3b876..2ee456a23b95 100644\n--- a/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.h\n+++ b/src/Common/ZooKeeper/ZooKeeperWithFaultInjection.h\n@@ -62,7 +62,7 @@ class ZooKeeperWithFaultInjection\n \n     std::unique_ptr<RandomFaultInjection> fault_policy;\n     std::string name;\n-    Poco::Logger * logger = nullptr;\n+    LoggerPtr logger = nullptr;\n     const UInt64 seed = 0;\n \n     std::vector<std::string> session_ephemeral_nodes;\n@@ -87,7 +87,7 @@ class ZooKeeperWithFaultInjection\n         double fault_injection_probability,\n         UInt64 fault_injection_seed,\n         std::string name_,\n-        Poco::Logger * logger_);\n+        LoggerPtr logger_);\n \n     explicit ZooKeeperWithFaultInjection(zkutil::ZooKeeper::Ptr const & keeper_) : keeper(keeper_) { }\n     static ZooKeeperWithFaultInjection::Ptr createInstance(\n@@ -95,7 +95,7 @@ class ZooKeeperWithFaultInjection\n         UInt64 fault_injection_seed,\n         zkutil::ZooKeeper::Ptr const & zookeeper,\n         std::string name,\n-        Poco::Logger * logger)\n+        LoggerPtr logger)\n     {\n         /// validate all parameters here, constructor just accept everything\n         if (fault_injection_probability < 0.0)\ndiff --git a/src/Common/logger_useful.h b/src/Common/logger_useful.h\nindex d9fe5ac91908..3899d060b7c6 100644\n--- a/src/Common/logger_useful.h\n+++ b/src/Common/logger_useful.h\n@@ -8,6 +8,7 @@\n #include <Common/CurrentThread.h>\n #include <Common/ProfileEvents.h>\n #include <Common/LoggingFormatStringHelpers.h>\n+#include <Common/Logger.h>\n \n namespace Poco { class Logger; }\n \n@@ -19,11 +20,11 @@ using LogSeriesLimiterPtr = std::shared_ptr<LogSeriesLimiter>;\n \n namespace\n {\n-    [[maybe_unused]] const ::Poco::Logger * getLogger(const ::Poco::Logger * logger) { return logger; }\n-    [[maybe_unused]] const ::Poco::Logger * getLogger(const std::atomic<::Poco::Logger *> & logger) { return logger.load(); }\n-    [[maybe_unused]] std::unique_ptr<LogToStrImpl> getLogger(std::unique_ptr<LogToStrImpl> && logger) { return logger; }\n-    [[maybe_unused]] std::unique_ptr<LogFrequencyLimiterIml> getLogger(std::unique_ptr<LogFrequencyLimiterIml> && logger) { return logger; }\n-    [[maybe_unused]] LogSeriesLimiterPtr getLogger(LogSeriesLimiterPtr & logger) { return logger; }\n+    [[maybe_unused]] const ::Poco::Logger * getLoggerHelper(const LoggerPtr & logger) { return logger.get(); }\n+    [[maybe_unused]] const ::Poco::Logger * getLoggerHelper(const ::Poco::Logger * logger) { return logger; }\n+    [[maybe_unused]] std::unique_ptr<LogToStrImpl> getLoggerHelper(std::unique_ptr<LogToStrImpl> && logger) { return logger; }\n+    [[maybe_unused]] std::unique_ptr<LogFrequencyLimiterIml> getLoggerHelper(std::unique_ptr<LogFrequencyLimiterIml> && logger) { return logger; }\n+    [[maybe_unused]] LogSeriesLimiterPtr getLoggerHelper(LogSeriesLimiterPtr & logger) { return logger; }\n }\n \n #define LOG_IMPL_FIRST_ARG(X, ...) X\n@@ -62,7 +63,7 @@ namespace\n \n #define LOG_IMPL(logger, priority, PRIORITY, ...) do                                                                \\\n {                                                                                                                   \\\n-    auto _logger = ::getLogger(logger);                                                                             \\\n+    auto _logger = ::getLoggerHelper(logger);                                                                             \\\n     const bool _is_clients_log = (DB::CurrentThread::getGroup() != nullptr) &&                                      \\\n         (DB::CurrentThread::get().getClientLogsLevel() >= (priority));                                              \\\n     if (!_is_clients_log && !_logger->is((PRIORITY)))                                                               \\\ndiff --git a/src/Common/makeSocketAddress.cpp b/src/Common/makeSocketAddress.cpp\nindex b5df6a4ef033..ba5bb53cd20f 100644\n--- a/src/Common/makeSocketAddress.cpp\n+++ b/src/Common/makeSocketAddress.cpp\n@@ -33,4 +33,9 @@ Poco::Net::SocketAddress makeSocketAddress(const std::string & host, uint16_t po\n     return socket_address;\n }\n \n+Poco::Net::SocketAddress makeSocketAddress(const std::string & host, uint16_t port, LoggerPtr log)\n+{\n+    return makeSocketAddress(host, port, log.get());\n+}\n+\n }\ndiff --git a/src/Common/makeSocketAddress.h b/src/Common/makeSocketAddress.h\nindex 9c7d10a04718..439a4ef1e9bf 100644\n--- a/src/Common/makeSocketAddress.h\n+++ b/src/Common/makeSocketAddress.h\n@@ -1,5 +1,7 @@\n #pragma once\n+\n #include <Poco/Net/SocketAddress.h>\n+#include <Common/Logger.h>\n \n namespace Poco { class Logger; }\n \n@@ -8,4 +10,6 @@ namespace DB\n \n Poco::Net::SocketAddress makeSocketAddress(const std::string & host, uint16_t port, Poco::Logger * log);\n \n+Poco::Net::SocketAddress makeSocketAddress(const std::string & host, uint16_t port, LoggerPtr log);\n+\n }\ndiff --git a/src/Common/mysqlxx/Query.cpp b/src/Common/mysqlxx/Query.cpp\nindex e30ed2b75c81..babfc8c7c41d 100644\n--- a/src/Common/mysqlxx/Query.cpp\n+++ b/src/Common/mysqlxx/Query.cpp\n@@ -52,7 +52,7 @@ void Query::executeImpl()\n {\n     MYSQL* mysql_driver = conn->getDriver();\n \n-    LOG_TRACE(&Poco::Logger::get(\"mysqlxx::Query\"), \"Running MySQL query using connection {}\", mysql_thread_id(mysql_driver));\n+    LOG_TRACE(getLogger(\"mysqlxx::Query\"), \"Running MySQL query using connection {}\", mysql_thread_id(mysql_driver));\n     if (mysql_real_query(mysql_driver, query.data(), query.size()))\n     {\n         const auto err_no = mysql_errno(mysql_driver);\ndiff --git a/src/Common/mysqlxx/mysqlxx/Pool.h b/src/Common/mysqlxx/mysqlxx/Pool.h\nindex bb4d0cefbdc8..c85295c4dd08 100644\n--- a/src/Common/mysqlxx/mysqlxx/Pool.h\n+++ b/src/Common/mysqlxx/mysqlxx/Pool.h\n@@ -202,7 +202,7 @@ class Pool final\n     void removeConnection(Connection * connection);\n \n protected:\n-    Poco::Logger * log = &Poco::Logger::get(\"mysqlxx::Pool\");\n+    LoggerPtr log = getLogger(\"mysqlxx::Pool\");\n \n     /// Number of MySQL connections which are created at launch.\n     unsigned default_connections;\ndiff --git a/src/Common/parseRemoteDescription.cpp b/src/Common/parseRemoteDescription.cpp\nindex 7b2045b9de15..df3820b11f90 100644\n--- a/src/Common/parseRemoteDescription.cpp\n+++ b/src/Common/parseRemoteDescription.cpp\n@@ -179,7 +179,7 @@ std::vector<std::pair<String, uint16_t>> parseRemoteDescriptionForExternalDataba\n         size_t colon = address.find(':');\n         if (colon == String::npos)\n         {\n-            LOG_WARNING(&Poco::Logger::get(\"ParseRemoteDescription\"), \"Port is not found for host: {}. Using default port {}\", address, default_port);\n+            LOG_WARNING(getLogger(\"ParseRemoteDescription\"), \"Port is not found for host: {}. Using default port {}\", address, default_port);\n             result.emplace_back(std::make_pair(address, default_port));\n         }\n         else\ndiff --git a/src/Compression/CompressionCodecDeflateQpl.cpp b/src/Compression/CompressionCodecDeflateQpl.cpp\nindex ee0356adde5e..631a12cc2522 100644\n--- a/src/Compression/CompressionCodecDeflateQpl.cpp\n+++ b/src/Compression/CompressionCodecDeflateQpl.cpp\n@@ -33,7 +33,7 @@ DeflateQplJobHWPool::DeflateQplJobHWPool()\n     : max_hw_jobs(0)\n     , random_engine(randomSeed())\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"DeflateQplJobHWPool\");\n+    LoggerPtr log = getLogger(\"DeflateQplJobHWPool\");\n     const char * qpl_version = qpl_get_library_version();\n \n     // loop all configured workqueue size to get maximum job number.\n@@ -141,7 +141,7 @@ void DeflateQplJobHWPool::unLockJob(UInt32 index)\n }\n \n HardwareCodecDeflateQpl::HardwareCodecDeflateQpl(SoftwareCodecDeflateQpl & sw_codec_)\n-    : log(&Poco::Logger::get(\"HardwareCodecDeflateQpl\"))\n+    : log(getLogger(\"HardwareCodecDeflateQpl\"))\n     , sw_codec(sw_codec_)\n {\n }\ndiff --git a/src/Compression/CompressionCodecDeflateQpl.h b/src/Compression/CompressionCodecDeflateQpl.h\nindex 3d9a9b13921d..c5978335fe8d 100644\n--- a/src/Compression/CompressionCodecDeflateQpl.h\n+++ b/src/Compression/CompressionCodecDeflateQpl.h\n@@ -88,7 +88,7 @@ class HardwareCodecDeflateQpl\n     /// For each submission, push job ID && job object into this map;\n     /// For flush, pop out job ID && job object from this map. Use job ID to release job lock and use job object to check job status till complete.\n     std::map<UInt32, qpl_job *> decomp_async_job_map;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     /// Provides a fallback in case of errors.\n     SoftwareCodecDeflateQpl & sw_codec;\n };\ndiff --git a/src/Compression/CompressionCodecEncrypted.cpp b/src/Compression/CompressionCodecEncrypted.cpp\nindex 8d945417fc11..3b7f4824069c 100644\n--- a/src/Compression/CompressionCodecEncrypted.cpp\n+++ b/src/Compression/CompressionCodecEncrypted.cpp\n@@ -694,7 +694,7 @@ bool CompressionCodecEncrypted::Configuration::tryLoad(const Poco::Util::Abstrac\n /// if encryption is disabled, print warning about this.\n void CompressionCodecEncrypted::Configuration::load(const Poco::Util::AbstractConfiguration & config [[maybe_unused]], const String & config_prefix [[maybe_unused]])\n {\n-    LOG_WARNING(&Poco::Logger::get(\"CompressionCodecEncrypted\"), \"Server was built without SSL support. Encryption is disabled.\");\n+    LOG_WARNING(getLogger(\"CompressionCodecEncrypted\"), \"Server was built without SSL support. Encryption is disabled.\");\n }\n \n }\ndiff --git a/src/Compression/CompressionCodecZSTDQAT.cpp b/src/Compression/CompressionCodecZSTDQAT.cpp\nindex 4828a71a515c..5a4ef70a30ae 100644\n--- a/src/Compression/CompressionCodecZSTDQAT.cpp\n+++ b/src/Compression/CompressionCodecZSTDQAT.cpp\n@@ -34,7 +34,7 @@ class CompressionCodecZSTDQAT : public CompressionCodecZSTD\n \n private:\n     const int level;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     static std::atomic<int> qat_state; /// Global initialization status of QAT device, we fall back back to software compression if uninitialized\n };\n \n@@ -103,7 +103,7 @@ void registerCodecZSTDQAT(CompressionCodecFactory & factory)\n CompressionCodecZSTDQAT::CompressionCodecZSTDQAT(int level_)\n     : CompressionCodecZSTD(level_)\n     , level(level_)\n-    , log(&Poco::Logger::get(\"CompressionCodecZSTDQAT\"))\n+    , log(getLogger(\"CompressionCodecZSTDQAT\"))\n {\n     setCodecDescription(\"ZSTD_QAT\", {std::make_shared<ASTLiteral>(static_cast<UInt64>(level))});\n }\ndiff --git a/src/Coordination/Changelog.cpp b/src/Coordination/Changelog.cpp\nindex 7f1135eec947..5a58932606e5 100644\n--- a/src/Coordination/Changelog.cpp\n+++ b/src/Coordination/Changelog.cpp\n@@ -116,7 +116,7 @@ class ChangelogWriter\n         : existing_changelogs(existing_changelogs_)\n         , log_file_settings(log_file_settings_)\n         , keeper_context(std::move(keeper_context_))\n-        , log(&Poco::Logger::get(\"Changelog\"))\n+        , log(getLogger(\"Changelog\"))\n     {\n     }\n \n@@ -454,7 +454,7 @@ class ChangelogWriter\n \n     KeeperContextPtr keeper_context;\n \n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n };\n \n struct ChangelogReadResult\n@@ -493,7 +493,7 @@ class ChangelogReader\n     }\n \n     /// start_log_index -- all entries with index < start_log_index will be skipped, but accounted into total_entries_read_from_log\n-    ChangelogReadResult readChangelog(IndexToLogEntry & logs, uint64_t start_log_index, Poco::Logger * log)\n+    ChangelogReadResult readChangelog(IndexToLogEntry & logs, uint64_t start_log_index, LoggerPtr log)\n     {\n         ChangelogReadResult result{};\n         result.compressed_log = compression_method != CompressionMethod::None;\n@@ -592,7 +592,7 @@ class ChangelogReader\n };\n \n Changelog::Changelog(\n-    Poco::Logger * log_, LogFileSettings log_file_settings, FlushSettings flush_settings_, KeeperContextPtr keeper_context_)\n+    LoggerPtr log_, LogFileSettings log_file_settings, FlushSettings flush_settings_, KeeperContextPtr keeper_context_)\n     : changelogs_detached_dir(\"detached\")\n     , rotate_interval(log_file_settings.rotate_interval)\n     , compress_logs(log_file_settings.compress_logs)\ndiff --git a/src/Coordination/Changelog.h b/src/Coordination/Changelog.h\nindex 20f850e3f627..612c68ab7333 100644\n--- a/src/Coordination/Changelog.h\n+++ b/src/Coordination/Changelog.h\n@@ -94,7 +94,7 @@ class Changelog\n {\n public:\n     Changelog(\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         LogFileSettings log_file_settings,\n         FlushSettings flush_settings,\n         KeeperContextPtr keeper_context_);\n@@ -185,7 +185,7 @@ class Changelog\n     const String changelogs_detached_dir;\n     const uint64_t rotate_interval;\n     const bool compress_logs;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::mutex writer_mutex;\n     /// Current writer for changelog file\ndiff --git a/src/Coordination/FourLetterCommand.cpp b/src/Coordination/FourLetterCommand.cpp\nindex af2e4ec5a348..4862acd448f3 100644\n--- a/src/Coordination/FourLetterCommand.cpp\n+++ b/src/Coordination/FourLetterCommand.cpp\n@@ -234,7 +234,7 @@ void FourLetterCommandFactory::initializeAllowList(KeeperDispatcher & keeper_dis\n             }\n             else\n             {\n-                auto * log = &Poco::Logger::get(\"FourLetterCommandFactory\");\n+                auto log = getLogger(\"FourLetterCommandFactory\");\n                 LOG_WARNING(log, \"Find invalid keeper 4lw command {} when initializing, ignore it.\", token);\n             }\n         }\ndiff --git a/src/Coordination/KeeperContext.cpp b/src/Coordination/KeeperContext.cpp\nindex 0d9eb2544a6c..baad8d98e6af 100644\n--- a/src/Coordination/KeeperContext.cpp\n+++ b/src/Coordination/KeeperContext.cpp\n@@ -55,7 +55,7 @@ void KeeperContext::initialize(const Poco::Util::AbstractConfiguration & config,\n         if (!keeper_az.empty())\n         {\n             system_nodes_with_data[keeper_availability_zone_path] = keeper_az;\n-            LOG_INFO(&Poco::Logger::get(\"KeeperContext\"), \"Initialize the KeeperContext with availability zone: '{}'\", keeper_az);\n+            LOG_INFO(getLogger(\"KeeperContext\"), \"Initialize the KeeperContext with availability zone: '{}'\", keeper_az);\n         }\n     }\n \n@@ -88,7 +88,7 @@ bool diskValidator(const Poco::Util::AbstractConfiguration & config, const std::\n             supported_disk_types.end(),\n             [&](const auto supported_type) { return disk_type != supported_type; }))\n     {\n-        LOG_INFO(&Poco::Logger::get(\"KeeperContext\"), \"Disk type '{}' is not supported for Keeper\", disk_type);\n+        LOG_INFO(getLogger(\"KeeperContext\"), \"Disk type '{}' is not supported for Keeper\", disk_type);\n         return false;\n     }\n \n@@ -374,7 +374,7 @@ void KeeperContext::initializeFeatureFlags(const Poco::Util::AbstractConfigurati\n         system_nodes_with_data[keeper_api_feature_flags_path] = feature_flags.getFeatureFlags();\n     }\n \n-    feature_flags.logFlags(&Poco::Logger::get(\"KeeperContext\"));\n+    feature_flags.logFlags(getLogger(\"KeeperContext\"));\n }\n \n void KeeperContext::updateKeeperMemorySoftLimit(const Poco::Util::AbstractConfiguration & config)\ndiff --git a/src/Coordination/KeeperDispatcher.cpp b/src/Coordination/KeeperDispatcher.cpp\nindex 8bf48f23f504..35bc953a705e 100644\n--- a/src/Coordination/KeeperDispatcher.cpp\n+++ b/src/Coordination/KeeperDispatcher.cpp\n@@ -94,7 +94,7 @@ bool checkIfRequestIncreaseMem(const Coordination::ZooKeeperRequestPtr & request\n KeeperDispatcher::KeeperDispatcher()\n     : responses_queue(std::numeric_limits<size_t>::max())\n     , configuration_and_settings(std::make_shared<KeeperConfigurationAndSettings>())\n-    , log(&Poco::Logger::get(\"KeeperDispatcher\"))\n+    , log(getLogger(\"KeeperDispatcher\"))\n {}\n \n void KeeperDispatcher::requestThread()\ndiff --git a/src/Coordination/KeeperDispatcher.h b/src/Coordination/KeeperDispatcher.h\nindex 9c487e7f0f9b..db41fb2ea26b 100644\n--- a/src/Coordination/KeeperDispatcher.h\n+++ b/src/Coordination/KeeperDispatcher.h\n@@ -70,7 +70,7 @@ class KeeperDispatcher\n \n     KeeperConfigurationAndSettingsPtr configuration_and_settings;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Counter for new session_id requests.\n     std::atomic<int64_t> internal_session_id_counter{0};\ndiff --git a/src/Coordination/KeeperFeatureFlags.cpp b/src/Coordination/KeeperFeatureFlags.cpp\nindex d0cd1c86b553..2aad6cbed32e 100644\n--- a/src/Coordination/KeeperFeatureFlags.cpp\n+++ b/src/Coordination/KeeperFeatureFlags.cpp\n@@ -80,7 +80,7 @@ const std::string & KeeperFeatureFlags::getFeatureFlags() const\n     return feature_flags;\n }\n \n-void KeeperFeatureFlags::logFlags(Poco::Logger * log) const\n+void KeeperFeatureFlags::logFlags(LoggerPtr log) const\n {\n     for (const auto & [feature_flag, feature_flag_name] : magic_enum::enum_entries<KeeperFeatureFlag>())\n     {\ndiff --git a/src/Coordination/KeeperFeatureFlags.h b/src/Coordination/KeeperFeatureFlags.h\nindex 4db972fa2a06..4e26ca607361 100644\n--- a/src/Coordination/KeeperFeatureFlags.h\n+++ b/src/Coordination/KeeperFeatureFlags.h\n@@ -32,7 +32,7 @@ class KeeperFeatureFlags\n     void enableFeatureFlag(KeeperFeatureFlag feature);\n     void disableFeatureFlag(KeeperFeatureFlag feature);\n \n-    void logFlags(Poco::Logger * log) const;\n+    void logFlags(LoggerPtr log) const;\n private:\n     std::string feature_flags;\n };\ndiff --git a/src/Coordination/KeeperLogStore.cpp b/src/Coordination/KeeperLogStore.cpp\nindex 8cff3419afcd..ce7c715237e1 100644\n--- a/src/Coordination/KeeperLogStore.cpp\n+++ b/src/Coordination/KeeperLogStore.cpp\n@@ -7,7 +7,7 @@ namespace DB\n {\n \n KeeperLogStore::KeeperLogStore(LogFileSettings log_file_settings, FlushSettings flush_settings, KeeperContextPtr keeper_context)\n-    : log(&Poco::Logger::get(\"KeeperLogStore\")), changelog(log, log_file_settings, flush_settings, keeper_context)\n+    : log(getLogger(\"KeeperLogStore\")), changelog(log, log_file_settings, flush_settings, keeper_context)\n {\n     if (log_file_settings.force_sync)\n         LOG_INFO(log, \"force_sync enabled\");\ndiff --git a/src/Coordination/KeeperLogStore.h b/src/Coordination/KeeperLogStore.h\nindex de9205241bd9..aa277f19d883 100644\n--- a/src/Coordination/KeeperLogStore.h\n+++ b/src/Coordination/KeeperLogStore.h\n@@ -74,7 +74,7 @@ class KeeperLogStore : public nuraft::log_store\n \n private:\n     mutable std::mutex changelog_lock;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     Changelog changelog TSA_GUARDED_BY(changelog_lock);\n };\n \ndiff --git a/src/Coordination/KeeperServer.cpp b/src/Coordination/KeeperServer.cpp\nindex 45619ab38a1b..722b1303cc82 100644\n--- a/src/Coordination/KeeperServer.cpp\n+++ b/src/Coordination/KeeperServer.cpp\n@@ -92,7 +92,7 @@ std::string checkAndGetSuperdigest(const String & user_and_digest)\n     return user_and_digest;\n }\n \n-int32_t getValueOrMaxInt32AndLogWarning(uint64_t value, const std::string & name, Poco::Logger * log)\n+int32_t getValueOrMaxInt32AndLogWarning(uint64_t value, const std::string & name, LoggerPtr log)\n {\n     if (value > std::numeric_limits<int32_t>::max())\n     {\n@@ -120,7 +120,7 @@ KeeperServer::KeeperServer(\n     KeeperStateMachine::CommitCallback commit_callback)\n     : server_id(configuration_and_settings_->server_id)\n     , coordination_settings(configuration_and_settings_->coordination_settings)\n-    , log(&Poco::Logger::get(\"KeeperServer\"))\n+    , log(getLogger(\"KeeperServer\"))\n     , is_recovering(config.getBool(\"keeper_server.force_recovery\", false))\n     , keeper_context{std::move(keeper_context_)}\n     , create_snapshot_on_exit(config.getBool(\"keeper_server.create_snapshot_on_exit\", true))\ndiff --git a/src/Coordination/KeeperServer.h b/src/Coordination/KeeperServer.h\nindex 8c657ab28a70..ef298df3efca 100644\n--- a/src/Coordination/KeeperServer.h\n+++ b/src/Coordination/KeeperServer.h\n@@ -48,7 +48,7 @@ class KeeperServer\n \n     nuraft::ptr<nuraft::cluster_config> last_local_config;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Callback func which is called by NuRaft on all internal events.\n     /// Used to determine the moment when raft is ready to server new requests\ndiff --git a/src/Coordination/KeeperSnapshotManager.cpp b/src/Coordination/KeeperSnapshotManager.cpp\nindex ee5935015e4f..f53b80317121 100644\n--- a/src/Coordination/KeeperSnapshotManager.cpp\n+++ b/src/Coordination/KeeperSnapshotManager.cpp\n@@ -383,7 +383,7 @@ void KeeperStorageSnapshot::deserialize(SnapshotDeserializationResult & deserial\n         {\n             if (keeper_context->ignoreSystemPathOnStartup() || keeper_context->getServerState() != KeeperContext::Phase::INIT)\n             {\n-                LOG_ERROR(&Poco::Logger::get(\"KeeperSnapshotManager\"), \"{}. Ignoring it\", error_msg);\n+                LOG_ERROR(getLogger(\"KeeperSnapshotManager\"), \"{}. Ignoring it\", error_msg);\n                 continue;\n             }\n             else\n@@ -399,7 +399,7 @@ void KeeperStorageSnapshot::deserialize(SnapshotDeserializationResult & deserial\n             {\n                 if (keeper_context->ignoreSystemPathOnStartup() || keeper_context->getServerState() != KeeperContext::Phase::INIT)\n                 {\n-                    LOG_ERROR(&Poco::Logger::get(\"KeeperSnapshotManager\"), \"{}. Ignoring it\", error_msg);\n+                    LOG_ERROR(getLogger(\"KeeperSnapshotManager\"), \"{}. Ignoring it\", error_msg);\n                     node = KeeperStorage::Node{};\n                 }\n                 else\n@@ -437,7 +437,7 @@ void KeeperStorageSnapshot::deserialize(SnapshotDeserializationResult & deserial\n             {\n #ifdef NDEBUG\n                 /// TODO (alesapin) remove this, it should be always CORRUPTED_DATA.\n-                LOG_ERROR(&Poco::Logger::get(\"KeeperSnapshotManager\"), \"Children counter in stat.numChildren {}\"\n+                LOG_ERROR(getLogger(\"KeeperSnapshotManager\"), \"Children counter in stat.numChildren {}\"\n                             \" is different from actual children size {} for node {}\", itr.value.stat.numChildren, itr.value.getChildren().size(), itr.key);\n #else\n                 throw Exception(ErrorCodes::LOGICAL_ERROR, \"Children counter in stat.numChildren {}\"\n@@ -594,7 +594,7 @@ KeeperSnapshotManager::KeeperSnapshotManager(\n \n             if (!inserted)\n                 LOG_WARNING(\n-                    &Poco::Logger::get(\"KeeperSnapshotManager\"),\n+                    getLogger(\"KeeperSnapshotManager\"),\n                     \"Found another snapshots with last log idx {}, will use snapshot from disk {}\",\n                     snapshot_up_to,\n                     disk->getName());\ndiff --git a/src/Coordination/KeeperSnapshotManager.h b/src/Coordination/KeeperSnapshotManager.h\nindex 6096ba318da2..48a66e79cd84 100644\n--- a/src/Coordination/KeeperSnapshotManager.h\n+++ b/src/Coordination/KeeperSnapshotManager.h\n@@ -188,7 +188,7 @@ class KeeperSnapshotManager\n \n     KeeperContextPtr keeper_context;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"KeeperSnapshotManager\");\n+    LoggerPtr log = getLogger(\"KeeperSnapshotManager\");\n };\n \n /// Keeper create snapshots in background thread. KeeperStateMachine just create\ndiff --git a/src/Coordination/KeeperSnapshotManagerS3.cpp b/src/Coordination/KeeperSnapshotManagerS3.cpp\nindex 716184e07d0c..0337a564660e 100644\n--- a/src/Coordination/KeeperSnapshotManagerS3.cpp\n+++ b/src/Coordination/KeeperSnapshotManagerS3.cpp\n@@ -43,7 +43,7 @@ struct KeeperSnapshotManagerS3::S3Configuration\n \n KeeperSnapshotManagerS3::KeeperSnapshotManagerS3()\n     : snapshots_s3_queue(std::numeric_limits<size_t>::max())\n-    , log(&Poco::Logger::get(\"KeeperSnapshotManagerS3\"))\n+    , log(getLogger(\"KeeperSnapshotManagerS3\"))\n     , uuid(UUIDHelpers::generateV4())\n {}\n \ndiff --git a/src/Coordination/KeeperSnapshotManagerS3.h b/src/Coordination/KeeperSnapshotManagerS3.h\nindex e17cf5a1cfbf..d03deb60c1a4 100644\n--- a/src/Coordination/KeeperSnapshotManagerS3.h\n+++ b/src/Coordination/KeeperSnapshotManagerS3.h\n@@ -45,7 +45,7 @@ class KeeperSnapshotManagerS3\n \n     std::atomic<bool> shutdown_called{false};\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     UUID uuid;\n \ndiff --git a/src/Coordination/KeeperStateMachine.cpp b/src/Coordination/KeeperStateMachine.cpp\nindex 39a2347ff802..8d50f0a76b17 100644\n--- a/src/Coordination/KeeperStateMachine.cpp\n+++ b/src/Coordination/KeeperStateMachine.cpp\n@@ -59,7 +59,7 @@ KeeperStateMachine::KeeperStateMachine(\n     , snapshots_queue(snapshots_queue_)\n     , min_request_size_to_cache(coordination_settings_->min_request_size_for_cache)\n     , last_committed_idx(0)\n-    , log(&Poco::Logger::get(\"KeeperStateMachine\"))\n+    , log(getLogger(\"KeeperStateMachine\"))\n     , superdigest(superdigest_)\n     , keeper_context(keeper_context_)\n     , snapshot_manager_s3(snapshot_manager_s3_)\n@@ -144,7 +144,7 @@ void assertDigest(\n     if (!KeeperStorage::checkDigest(first, second))\n     {\n         LOG_FATAL(\n-            &Poco::Logger::get(\"KeeperStateMachine\"),\n+            getLogger(\"KeeperStateMachine\"),\n             \"Digest for nodes is not matching after {} request of type '{}'.\\nExpected digest - {}, actual digest - {} (digest \"\n             \"{}). Keeper will terminate to avoid inconsistencies.\\nExtra information about the request:\\n{}\",\n             committing ? \"committing\" : \"preprocessing\",\n@@ -679,7 +679,7 @@ void KeeperStateMachine::save_logical_snp_obj(\n     }\n }\n \n-static int bufferFromFile(Poco::Logger * log, const std::string & path, nuraft::ptr<nuraft::buffer> & data_out)\n+static int bufferFromFile(LoggerPtr log, const std::string & path, nuraft::ptr<nuraft::buffer> & data_out)\n {\n     if (path.empty() || !std::filesystem::exists(path))\n     {\ndiff --git a/src/Coordination/KeeperStateMachine.h b/src/Coordination/KeeperStateMachine.h\nindex aad5d3aafd4c..b11cd53c00e9 100644\n--- a/src/Coordination/KeeperStateMachine.h\n+++ b/src/Coordination/KeeperStateMachine.h\n@@ -173,7 +173,7 @@ class KeeperStateMachine : public nuraft::state_machine\n     /// Last committed Raft log number.\n     std::atomic<uint64_t> last_committed_idx;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Cluster config for our quorum.\n     /// It's a copy of config stored in StateManager, but here\ndiff --git a/src/Coordination/KeeperStateManager.cpp b/src/Coordination/KeeperStateManager.cpp\nindex efe8a0cb2bd1..4fbb9b52e6e3 100644\n--- a/src/Coordination/KeeperStateManager.cpp\n+++ b/src/Coordination/KeeperStateManager.cpp\n@@ -227,7 +227,7 @@ KeeperStateManager::KeeperStateManager(int server_id_, const std::string & host,\n           keeper_context_))\n     , server_state_file_name(\"state\")\n     , keeper_context(keeper_context_)\n-    , logger(&Poco::Logger::get(\"KeeperStateManager\"))\n+    , logger(getLogger(\"KeeperStateManager\"))\n {\n     auto peer_config = nuraft::cs_new<nuraft::srv_config>(my_server_id, host + \":\" + std::to_string(port));\n     configuration_wrapper.cluster_config = nuraft::cs_new<nuraft::cluster_config>();\n@@ -262,7 +262,7 @@ KeeperStateManager::KeeperStateManager(\n           keeper_context_))\n     , server_state_file_name(server_state_file_name_)\n     , keeper_context(keeper_context_)\n-    , logger(&Poco::Logger::get(\"KeeperStateManager\"))\n+    , logger(getLogger(\"KeeperStateManager\"))\n {\n }\n \n@@ -495,7 +495,7 @@ ClusterUpdateActions KeeperStateManager::getRaftConfigurationDiff(\n             if (old_endpoint != server_config->get_endpoint())\n             {\n                 LOG_WARNING(\n-                    &Poco::Logger::get(\"RaftConfiguration\"),\n+                    getLogger(\"RaftConfiguration\"),\n                     \"Config will be ignored because a server with ID {} is already present in the cluster on a different endpoint ({}). \"\n                     \"The endpoint of the current servers should not be changed. For servers on a new endpoint, please use a new ID.\",\n                     new_id,\ndiff --git a/src/Coordination/KeeperStateManager.h b/src/Coordination/KeeperStateManager.h\nindex fd05261ac6c6..02dd6b2ff53a 100644\n--- a/src/Coordination/KeeperStateManager.h\n+++ b/src/Coordination/KeeperStateManager.h\n@@ -128,7 +128,7 @@ class KeeperStateManager : public nuraft::state_mgr\n \n     KeeperContextPtr keeper_context;\n \n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n \n public:\n     /// Parse configuration from xml config.\ndiff --git a/src/Coordination/KeeperStorage.cpp b/src/Coordination/KeeperStorage.cpp\nindex c128d7c2f98f..992d4ca8a954 100644\n--- a/src/Coordination/KeeperStorage.cpp\n+++ b/src/Coordination/KeeperStorage.cpp\n@@ -609,7 +609,7 @@ namespace\n [[noreturn]] void onStorageInconsistency()\n {\n     LOG_ERROR(\n-        &Poco::Logger::get(\"KeeperStorage\"),\n+        getLogger(\"KeeperStorage\"),\n         \"Inconsistency found between uncommitted and committed data. Keeper will terminate to avoid undefined behaviour.\");\n     std::terminate();\n }\n@@ -887,7 +887,7 @@ void handleSystemNodeModification(const KeeperContext & keeper_context, std::str\n             \"If you still want to ignore it, you can set 'keeper_server.ignore_system_path_on_startup' to true.\",\n             error_msg);\n \n-    LOG_ERROR(&Poco::Logger::get(\"KeeperStorage\"), fmt::runtime(error_msg));\n+    LOG_ERROR(getLogger(\"KeeperStorage\"), fmt::runtime(error_msg));\n }\n \n }\n@@ -2381,7 +2381,7 @@ void KeeperStorage::rollbackRequest(int64_t rollback_zxid, bool allow_missing)\n     }\n     catch (...)\n     {\n-        LOG_FATAL(&Poco::Logger::get(\"KeeperStorage\"), \"Failed to rollback log. Terminating to avoid inconsistencies\");\n+        LOG_FATAL(getLogger(\"KeeperStorage\"), \"Failed to rollback log. Terminating to avoid inconsistencies\");\n         std::terminate();\n     }\n }\ndiff --git a/src/Coordination/LoggerWrapper.h b/src/Coordination/LoggerWrapper.h\nindex d092a8d44407..d08c42b68684 100644\n--- a/src/Coordination/LoggerWrapper.h\n+++ b/src/Coordination/LoggerWrapper.h\n@@ -26,7 +26,7 @@ class LoggerWrapper : public nuraft::logger\n \n public:\n     LoggerWrapper(const std::string & name, LogsLevel level_)\n-        : log(&Poco::Logger::get(name))\n+        : log(getLogger(name))\n         , level(level_)\n     {\n         log->setLevel(static_cast<int>(LEVELS.at(level)));\n@@ -57,7 +57,7 @@ class LoggerWrapper : public nuraft::logger\n     }\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::atomic<LogsLevel> level;\n };\n \ndiff --git a/src/Coordination/ZooKeeperDataReader.cpp b/src/Coordination/ZooKeeperDataReader.cpp\nindex b55ebef327ff..6b9d5f7c8eb1 100644\n--- a/src/Coordination/ZooKeeperDataReader.cpp\n+++ b/src/Coordination/ZooKeeperDataReader.cpp\n@@ -90,7 +90,7 @@ void deserializeACLMap(KeeperStorage & storage, ReadBuffer & in)\n     }\n }\n \n-int64_t deserializeStorageData(KeeperStorage & storage, ReadBuffer & in, Poco::Logger * log)\n+int64_t deserializeStorageData(KeeperStorage & storage, ReadBuffer & in, LoggerPtr log)\n {\n     int64_t max_zxid = 0;\n     std::string path;\n@@ -146,7 +146,7 @@ int64_t deserializeStorageData(KeeperStorage & storage, ReadBuffer & in, Poco::L\n     return max_zxid;\n }\n \n-void deserializeKeeperStorageFromSnapshot(KeeperStorage & storage, const std::string & snapshot_path, Poco::Logger * log)\n+void deserializeKeeperStorageFromSnapshot(KeeperStorage & storage, const std::string & snapshot_path, LoggerPtr log)\n {\n     LOG_INFO(log, \"Deserializing storage snapshot {}\", snapshot_path);\n     int64_t zxid = getZxidFromName(snapshot_path);\n@@ -185,7 +185,7 @@ void deserializeKeeperStorageFromSnapshot(KeeperStorage & storage, const std::st\n     LOG_INFO(log, \"Finished, snapshot ZXID {}\", storage.zxid);\n }\n \n-void deserializeKeeperStorageFromSnapshotsDir(KeeperStorage & storage, const std::string & path, Poco::Logger * log)\n+void deserializeKeeperStorageFromSnapshotsDir(KeeperStorage & storage, const std::string & path, LoggerPtr log)\n {\n     namespace fs = std::filesystem;\n     std::map<int64_t, std::string> existing_snapshots;\n@@ -473,7 +473,7 @@ bool hasErrorsInMultiRequest(Coordination::ZooKeeperRequestPtr request)\n \n }\n \n-bool deserializeTxn(KeeperStorage & storage, ReadBuffer & in, Poco::Logger * /*log*/)\n+bool deserializeTxn(KeeperStorage & storage, ReadBuffer & in, LoggerPtr /*log*/)\n {\n     int64_t checksum;\n     Coordination::read(checksum, in);\n@@ -528,7 +528,7 @@ bool deserializeTxn(KeeperStorage & storage, ReadBuffer & in, Poco::Logger * /*l\n     return true;\n }\n \n-void deserializeLogAndApplyToStorage(KeeperStorage & storage, const std::string & log_path, Poco::Logger * log)\n+void deserializeLogAndApplyToStorage(KeeperStorage & storage, const std::string & log_path, LoggerPtr log)\n {\n     ReadBufferFromFile reader(log_path);\n \n@@ -552,7 +552,7 @@ void deserializeLogAndApplyToStorage(KeeperStorage & storage, const std::string\n     LOG_INFO(log, \"Finished {} deserialization, totally read {} records\", log_path, counter);\n }\n \n-void deserializeLogsAndApplyToStorage(KeeperStorage & storage, const std::string & path, Poco::Logger * log)\n+void deserializeLogsAndApplyToStorage(KeeperStorage & storage, const std::string & path, LoggerPtr log)\n {\n     namespace fs = std::filesystem;\n     std::map<int64_t, std::string> existing_logs;\ndiff --git a/src/Coordination/ZooKeeperDataReader.h b/src/Coordination/ZooKeeperDataReader.h\nindex 8fd86ba99e21..648dc95adcfc 100644\n--- a/src/Coordination/ZooKeeperDataReader.h\n+++ b/src/Coordination/ZooKeeperDataReader.h\n@@ -5,12 +5,12 @@\n namespace DB\n {\n \n-void deserializeKeeperStorageFromSnapshot(KeeperStorage & storage, const std::string & snapshot_path, Poco::Logger * log);\n+void deserializeKeeperStorageFromSnapshot(KeeperStorage & storage, const std::string & snapshot_path, LoggerPtr log);\n \n-void deserializeKeeperStorageFromSnapshotsDir(KeeperStorage & storage, const std::string & path, Poco::Logger * log);\n+void deserializeKeeperStorageFromSnapshotsDir(KeeperStorage & storage, const std::string & path, LoggerPtr log);\n \n-void deserializeLogAndApplyToStorage(KeeperStorage & storage, const std::string & log_path, Poco::Logger * log);\n+void deserializeLogAndApplyToStorage(KeeperStorage & storage, const std::string & log_path, LoggerPtr log);\n \n-void deserializeLogsAndApplyToStorage(KeeperStorage & storage, const std::string & path, Poco::Logger * log);\n+void deserializeLogsAndApplyToStorage(KeeperStorage & storage, const std::string & path, LoggerPtr log);\n \n }\ndiff --git a/src/Core/BackgroundSchedulePool.cpp b/src/Core/BackgroundSchedulePool.cpp\nindex fa892bc3c844..4facdeb4631f 100644\n--- a/src/Core/BackgroundSchedulePool.cpp\n+++ b/src/Core/BackgroundSchedulePool.cpp\n@@ -116,7 +116,7 @@ void BackgroundSchedulePoolTaskInfo::execute()\n     static constexpr UInt64 slow_execution_threshold_ms = 200;\n \n     if (milliseconds >= slow_execution_threshold_ms)\n-        LOG_TRACE(&Poco::Logger::get(log_name), \"Execution took {} ms.\", milliseconds);\n+        LOG_TRACE(getLogger(log_name), \"Execution took {} ms.\", milliseconds);\n \n     {\n         std::lock_guard lock_schedule(schedule_mutex);\n@@ -160,7 +160,7 @@ BackgroundSchedulePool::BackgroundSchedulePool(size_t size_, CurrentMetrics::Met\n     , size_metric(size_metric_, size_)\n     , thread_name(thread_name_)\n {\n-    LOG_INFO(&Poco::Logger::get(\"BackgroundSchedulePool/\" + thread_name), \"Create BackgroundSchedulePool with {} threads\", size_);\n+    LOG_INFO(getLogger(\"BackgroundSchedulePool/\" + thread_name), \"Create BackgroundSchedulePool with {} threads\", size_);\n \n     threads.resize(size_);\n \n@@ -174,7 +174,7 @@ BackgroundSchedulePool::BackgroundSchedulePool(size_t size_, CurrentMetrics::Met\n     catch (...)\n     {\n         LOG_FATAL(\n-            &Poco::Logger::get(\"BackgroundSchedulePool/\" + thread_name),\n+            getLogger(\"BackgroundSchedulePool/\" + thread_name),\n             \"Couldn't get {} threads from global thread pool: {}\",\n             size_,\n             getCurrentExceptionCode() == DB::ErrorCodes::CANNOT_SCHEDULE_TASK\n@@ -192,7 +192,7 @@ void BackgroundSchedulePool::increaseThreadsCount(size_t new_threads_count)\n \n     if (new_threads_count < old_threads_count)\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"BackgroundSchedulePool/\" + thread_name),\n+        LOG_WARNING(getLogger(\"BackgroundSchedulePool/\" + thread_name),\n             \"Tried to increase the number of threads but the new threads count ({}) is not greater than old one ({})\", new_threads_count, old_threads_count);\n         return;\n     }\n@@ -219,7 +219,7 @@ BackgroundSchedulePool::~BackgroundSchedulePool()\n         tasks_cond_var.notify_all();\n         delayed_tasks_cond_var.notify_all();\n \n-        LOG_TRACE(&Poco::Logger::get(\"BackgroundSchedulePool/\" + thread_name), \"Waiting for threads to finish.\");\n+        LOG_TRACE(getLogger(\"BackgroundSchedulePool/\" + thread_name), \"Waiting for threads to finish.\");\n         delayed_thread->join();\n \n         for (auto & thread : threads)\ndiff --git a/src/Core/BaseSettings.cpp b/src/Core/BaseSettings.cpp\nindex 72a8070e6529..a7e1ab99af7e 100644\n--- a/src/Core/BaseSettings.cpp\n+++ b/src/Core/BaseSettings.cpp\n@@ -47,8 +47,7 @@ void BaseSettingsHelpers::throwSettingNotFound(std::string_view name)\n \n void BaseSettingsHelpers::warningSettingNotFound(std::string_view name)\n {\n-    static auto * log = &Poco::Logger::get(\"Settings\");\n-    LOG_WARNING(log, \"Unknown setting {}, skipping\", name);\n+    LOG_WARNING(getLogger(\"Settings\"), \"Unknown setting {}, skipping\", name);\n }\n \n }\ndiff --git a/src/Core/MySQL/Authentication.cpp b/src/Core/MySQL/Authentication.cpp\nindex 2c10bd887228..ac6ed70dbb52 100644\n--- a/src/Core/MySQL/Authentication.cpp\n+++ b/src/Core/MySQL/Authentication.cpp\n@@ -102,7 +102,7 @@ void Native41::authenticate(\n \n #if USE_SSL\n \n-Sha256Password::Sha256Password(RSA & public_key_, RSA & private_key_, Poco::Logger * log_)\n+Sha256Password::Sha256Password(RSA & public_key_, RSA & private_key_, LoggerPtr log_)\n     : public_key(public_key_), private_key(private_key_), log(log_)\n {\n     /** Native authentication sent 20 bytes + '\\0' character = 21 bytes.\ndiff --git a/src/Core/MySQL/Authentication.h b/src/Core/MySQL/Authentication.h\nindex ee6aaac02bcb..3179fa20f593 100644\n--- a/src/Core/MySQL/Authentication.h\n+++ b/src/Core/MySQL/Authentication.h\n@@ -61,7 +61,7 @@ class Native41 : public IPlugin\n class Sha256Password : public IPlugin\n {\n public:\n-    Sha256Password(RSA & public_key_, RSA & private_key_, Poco::Logger * log_);\n+    Sha256Password(RSA & public_key_, RSA & private_key_, LoggerPtr log_);\n \n     String getName() override { return \"sha256_password\"; }\n \n@@ -74,7 +74,7 @@ class Sha256Password : public IPlugin\n private:\n     RSA & public_key;\n     RSA & private_key;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     String scramble;\n };\n #endif\ndiff --git a/src/Core/PostgreSQL/Connection.cpp b/src/Core/PostgreSQL/Connection.cpp\nindex 5a589a80d021..eea24dd69407 100644\n--- a/src/Core/PostgreSQL/Connection.cpp\n+++ b/src/Core/PostgreSQL/Connection.cpp\n@@ -9,7 +9,7 @@ namespace postgres\n \n Connection::Connection(const ConnectionInfo & connection_info_, bool replication_, size_t num_tries_)\n     : connection_info(connection_info_), replication(replication_), num_tries(num_tries_)\n-    , log(&Poco::Logger::get(\"PostgreSQLReplicaConnection\"))\n+    , log(getLogger(\"PostgreSQLReplicaConnection\"))\n {\n     if (replication)\n         connection_info = {fmt::format(\"{} replication=database\", connection_info.connection_string), connection_info.host_port};\n@@ -65,7 +65,7 @@ void Connection::updateConnection()\n     if (replication)\n         connection->set_variable(\"default_transaction_isolation\", \"'repeatable read'\");\n \n-    LOG_DEBUG(&Poco::Logger::get(\"PostgreSQLConnection\"), \"New connection to {}\", connection_info.host_port);\n+    LOG_DEBUG(getLogger(\"PostgreSQLConnection\"), \"New connection to {}\", connection_info.host_port);\n }\n \n void Connection::connect()\ndiff --git a/src/Core/PostgreSQL/Connection.h b/src/Core/PostgreSQL/Connection.h\nindex efc10b6ed20e..5e0aa0983d59 100644\n--- a/src/Core/PostgreSQL/Connection.h\n+++ b/src/Core/PostgreSQL/Connection.h\n@@ -6,6 +6,7 @@\n \n #include <pqxx/pqxx>\n #include <Core/Types.h>\n+#include <Common/Logger.h>\n #include <boost/noncopyable.hpp>\n \n /** Methods to work with PostgreSQL connection object.\n@@ -61,7 +62,7 @@ class Connection : private boost::noncopyable\n     bool replication;\n     size_t num_tries;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n using ConnectionPtr = std::unique_ptr<Connection>;\ndiff --git a/src/Core/PostgreSQL/PoolWithFailover.cpp b/src/Core/PostgreSQL/PoolWithFailover.cpp\nindex 3655681c5158..a034c50094d4 100644\n--- a/src/Core/PostgreSQL/PoolWithFailover.cpp\n+++ b/src/Core/PostgreSQL/PoolWithFailover.cpp\n@@ -32,7 +32,7 @@ PoolWithFailover::PoolWithFailover(\n     , max_tries(max_tries_)\n     , auto_close_connection(auto_close_connection_)\n {\n-    LOG_TRACE(&Poco::Logger::get(\"PostgreSQLConnectionPool\"), \"PostgreSQL connection pool size: {}, connection wait timeout: {}, max failover tries: {}\",\n+    LOG_TRACE(getLogger(\"PostgreSQLConnectionPool\"), \"PostgreSQL connection pool size: {}, connection wait timeout: {}, max failover tries: {}\",\n               pool_size, pool_wait_timeout, max_tries_);\n \n     for (const auto & [priority, configurations] : configurations_by_priority)\n@@ -56,13 +56,13 @@ PoolWithFailover::PoolWithFailover(\n     , max_tries(max_tries_)\n     , auto_close_connection(auto_close_connection_)\n {\n-    LOG_TRACE(&Poco::Logger::get(\"PostgreSQLConnectionPool\"), \"PostgreSQL connection pool size: {}, connection wait timeout: {}, max failover tries: {}\",\n+    LOG_TRACE(getLogger(\"PostgreSQLConnectionPool\"), \"PostgreSQL connection pool size: {}, connection wait timeout: {}, max failover tries: {}\",\n               pool_size, pool_wait_timeout, max_tries_);\n \n     /// Replicas have the same priority, but traversed replicas are moved to the end of the queue.\n     for (const auto & [host, port] : configuration.addresses)\n     {\n-        LOG_DEBUG(&Poco::Logger::get(\"PostgreSQLPoolWithFailover\"), \"Adding address host: {}, port: {} to connection pool\", host, port);\n+        LOG_DEBUG(getLogger(\"PostgreSQLPoolWithFailover\"), \"Adding address host: {}, port: {} to connection pool\", host, port);\n         auto connection_string = formatConnectionString(configuration.database, host, port, configuration.username, configuration.password);\n         replicas_with_priority[0].emplace_back(connection_string, pool_size);\n     }\ndiff --git a/src/Core/PostgreSQL/PoolWithFailover.h b/src/Core/PostgreSQL/PoolWithFailover.h\nindex bf3782afba40..3c538fc3dea8 100644\n--- a/src/Core/PostgreSQL/PoolWithFailover.h\n+++ b/src/Core/PostgreSQL/PoolWithFailover.h\n@@ -62,7 +62,7 @@ using RemoteDescription = std::vector<std::pair<String, uint16_t>>;\n     size_t max_tries;\n     bool auto_close_connection;\n     std::mutex mutex;\n-    Poco::Logger * log = &Poco::Logger::get(\"PostgreSQLConnectionPool\");\n+    LoggerPtr log = getLogger(\"PostgreSQLConnectionPool\");\n };\n \n using PoolWithFailoverPtr = std::shared_ptr<PoolWithFailover>;\ndiff --git a/src/Core/PostgreSQLProtocol.h b/src/Core/PostgreSQLProtocol.h\nindex b0d7646a5f73..7630fbb0b230 100644\n--- a/src/Core/PostgreSQLProtocol.h\n+++ b/src/Core/PostgreSQLProtocol.h\n@@ -872,7 +872,7 @@ class CleartextPasswordAuth : public AuthenticationMethod\n class AuthenticationManager\n {\n private:\n-    Poco::Logger * log = &Poco::Logger::get(\"AuthenticationManager\");\n+    LoggerPtr log = getLogger(\"AuthenticationManager\");\n     std::unordered_map<AuthenticationType, std::shared_ptr<AuthenticationMethod>> type_to_method = {};\n \n public:\ndiff --git a/src/Core/ServerUUID.h b/src/Core/ServerUUID.h\nindex 36bbf0e63153..b5ea17426cb2 100644\n--- a/src/Core/ServerUUID.h\n+++ b/src/Core/ServerUUID.h\n@@ -1,12 +1,10 @@\n #pragma once\n+\n #include <Core/UUID.h>\n+#include <Common/Logger.h>\n #include <filesystem>\n \n namespace fs = std::filesystem;\n-namespace Poco\n-{\n-    class Logger;\n-}\n \n namespace DB\n {\ndiff --git a/src/Core/SettingsQuirks.cpp b/src/Core/SettingsQuirks.cpp\nindex 1a79c23d955a..24dcd43a09cd 100644\n--- a/src/Core/SettingsQuirks.cpp\n+++ b/src/Core/SettingsQuirks.cpp\n@@ -17,7 +17,7 @@ namespace\n ///\n ///   [1]: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=339ddb53d373\n ///   [2]: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=0c54a6a44bf3\n-bool nestedEpollWorks(Poco::Logger * log)\n+bool nestedEpollWorks(LoggerPtr log)\n {\n     if (Poco::Environment::os() != POCO_OS_LINUX)\n         return true;\n@@ -48,7 +48,7 @@ namespace DB\n {\n \n /// Update some settings defaults to avoid some known issues.\n-void applySettingsQuirks(Settings & settings, Poco::Logger * log)\n+void applySettingsQuirks(Settings & settings, LoggerPtr log)\n {\n     if (!nestedEpollWorks(log))\n     {\ndiff --git a/src/Core/SettingsQuirks.h b/src/Core/SettingsQuirks.h\nindex 38def8eebf2b..f6b2a4e33fae 100644\n--- a/src/Core/SettingsQuirks.h\n+++ b/src/Core/SettingsQuirks.h\n@@ -1,9 +1,6 @@\n #pragma once\n \n-namespace Poco\n-{\n-class Logger;\n-}\n+#include <Common/Logger.h>\n \n namespace DB\n {\n@@ -11,6 +8,6 @@ namespace DB\n struct Settings;\n \n /// Update some settings defaults to avoid some known issues.\n-void applySettingsQuirks(Settings & settings, Poco::Logger * log = nullptr);\n+void applySettingsQuirks(Settings & settings, LoggerPtr log = nullptr);\n \n }\ndiff --git a/src/Core/SortDescription.cpp b/src/Core/SortDescription.cpp\nindex 9ba7df8ef24a..9edc79a1ff12 100644\n--- a/src/Core/SortDescription.cpp\n+++ b/src/Core/SortDescription.cpp\n@@ -108,10 +108,9 @@ static std::string getSortDescriptionDump(const SortDescription & description, c\n     return buffer.str();\n }\n \n-static Poco::Logger * getLogger()\n+static LoggerPtr getLogger()\n {\n-    static Poco::Logger & logger = Poco::Logger::get(\"SortDescription\");\n-    return &logger;\n+    return ::getLogger(\"SortDescription\");\n }\n \n void compileSortDescriptionIfNeeded(SortDescription & description, const DataTypes & sort_description_types, bool increase_compile_attempts)\ndiff --git a/src/Daemon/BaseDaemon.cpp b/src/Daemon/BaseDaemon.cpp\nindex b7685159f984..289a41bb75e2 100644\n--- a/src/Daemon/BaseDaemon.cpp\n+++ b/src/Daemon/BaseDaemon.cpp\n@@ -210,7 +210,7 @@ class SignalListener : public Poco::Runnable\n     static constexpr int SanitizerTrap = -3;\n \n     explicit SignalListener(BaseDaemon & daemon_)\n-        : log(&Poco::Logger::get(\"BaseDaemon\"))\n+        : log(getLogger(\"BaseDaemon\"))\n         , daemon(daemon_)\n     {\n     }\n@@ -295,7 +295,7 @@ class SignalListener : public Poco::Runnable\n     }\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     BaseDaemon & daemon;\n \n     void onTerminate(std::string_view message, UInt32 thread_num) const\ndiff --git a/src/Daemon/SentryWriter.cpp b/src/Daemon/SentryWriter.cpp\nindex 2050d5038795..ebfd18abeee8 100644\n--- a/src/Daemon/SentryWriter.cpp\n+++ b/src/Daemon/SentryWriter.cpp\n@@ -68,7 +68,7 @@ void SentryWriter::initialize(Poco::Util::LayeredConfiguration & config)\n {\n     bool enabled = false;\n     bool debug = config.getBool(\"send_crash_reports.debug\", false);\n-    auto * logger = &Poco::Logger::get(\"SentryWriter\");\n+    auto logger = getLogger(\"SentryWriter\");\n \n     if (config.getBool(\"send_crash_reports.enabled\", false))\n     {\n@@ -140,7 +140,7 @@ void SentryWriter::shutdown()\n \n void SentryWriter::onFault(int sig, const std::string & error_message, const StackTrace & stack_trace)\n {\n-    auto * logger = &Poco::Logger::get(\"SentryWriter\");\n+    auto logger = getLogger(\"SentryWriter\");\n     if (initialized)\n     {\n         sentry_value_t event = sentry_value_new_message_event(SENTRY_LEVEL_FATAL, \"fault\", error_message.c_str());\ndiff --git a/src/Databases/DatabaseDictionary.cpp b/src/Databases/DatabaseDictionary.cpp\nindex e2e0d52cd88b..9a65c7a46ef8 100644\n--- a/src/Databases/DatabaseDictionary.cpp\n+++ b/src/Databases/DatabaseDictionary.cpp\n@@ -51,7 +51,7 @@ namespace\n \n DatabaseDictionary::DatabaseDictionary(const String & name_, ContextPtr context_)\n     : IDatabase(name_), WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(\"DatabaseDictionary(\" + database_name + \")\"))\n+    , log(getLogger(\"DatabaseDictionary(\" + database_name + \")\"))\n {\n }\n \ndiff --git a/src/Databases/DatabaseDictionary.h b/src/Databases/DatabaseDictionary.h\nindex 425d048aa65b..469801d183e6 100644\n--- a/src/Databases/DatabaseDictionary.h\n+++ b/src/Databases/DatabaseDictionary.h\n@@ -48,7 +48,7 @@ class DatabaseDictionary final : public IDatabase, WithContext\n     ASTPtr getCreateTableQueryImpl(const String & table_name, ContextPtr context, bool throw_on_error) const override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     Tables listTables(const FilterByNameFunction & filter_by_name) const;\n };\ndiff --git a/src/Databases/DatabaseFilesystem.cpp b/src/Databases/DatabaseFilesystem.cpp\nindex 5564f1d07cf2..55ae60469edc 100644\n--- a/src/Databases/DatabaseFilesystem.cpp\n+++ b/src/Databases/DatabaseFilesystem.cpp\n@@ -32,7 +32,7 @@ namespace ErrorCodes\n }\n \n DatabaseFilesystem::DatabaseFilesystem(const String & name_, const String & path_, ContextPtr context_)\n-    : IDatabase(name_), WithContext(context_->getGlobalContext()), path(path_), log(&Poco::Logger::get(\"DatabaseFileSystem(\" + name_ + \")\"))\n+    : IDatabase(name_), WithContext(context_->getGlobalContext()), path(path_), log(getLogger(\"DatabaseFileSystem(\" + name_ + \")\"))\n {\n     bool is_local = context_->getApplicationType() == Context::ApplicationType::LOCAL;\n     fs::path user_files_path = is_local ? \"\" : fs::canonical(getContext()->getUserFilesPath());\ndiff --git a/src/Databases/DatabaseFilesystem.h b/src/Databases/DatabaseFilesystem.h\nindex b72891b9a5ca..3338aa28c21a 100644\n--- a/src/Databases/DatabaseFilesystem.h\n+++ b/src/Databases/DatabaseFilesystem.h\n@@ -61,7 +61,7 @@ class DatabaseFilesystem : public IDatabase, protected WithContext\n private:\n     String path;\n     mutable Tables loaded_tables TSA_GUARDED_BY(mutex);\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Databases/DatabaseHDFS.cpp b/src/Databases/DatabaseHDFS.cpp\nindex 9d0395e42174..3a1e6b16ccf8 100644\n--- a/src/Databases/DatabaseHDFS.cpp\n+++ b/src/Databases/DatabaseHDFS.cpp\n@@ -45,7 +45,7 @@ DatabaseHDFS::DatabaseHDFS(const String & name_, const String & source_url, Cont\n     : IDatabase(name_)\n     , WithContext(context_->getGlobalContext())\n     , source(source_url)\n-    , log(&Poco::Logger::get(\"DatabaseHDFS(\" + name_ + \")\"))\n+    , log(getLogger(\"DatabaseHDFS(\" + name_ + \")\"))\n {\n     if (!source.empty())\n     {\ndiff --git a/src/Databases/DatabaseHDFS.h b/src/Databases/DatabaseHDFS.h\nindex 957b2080135c..b586a912e163 100644\n--- a/src/Databases/DatabaseHDFS.h\n+++ b/src/Databases/DatabaseHDFS.h\n@@ -60,7 +60,7 @@ class DatabaseHDFS : public IDatabase, protected WithContext\n     const String source;\n \n     mutable Tables loaded_tables TSA_GUARDED_BY(mutex);\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Databases/DatabaseOnDisk.cpp b/src/Databases/DatabaseOnDisk.cpp\nindex 12b0dc077990..67ecd7be66a4 100644\n--- a/src/Databases/DatabaseOnDisk.cpp\n+++ b/src/Databases/DatabaseOnDisk.cpp\n@@ -660,7 +660,7 @@ void DatabaseOnDisk::iterateMetadataFiles(ContextPtr local_context, const Iterat\n }\n \n ASTPtr DatabaseOnDisk::parseQueryFromMetadata(\n-    Poco::Logger * logger,\n+    LoggerPtr logger,\n     ContextPtr local_context,\n     const String & metadata_file_path,\n     bool throw_on_error /*= true*/,\ndiff --git a/src/Databases/DatabaseOnDisk.h b/src/Databases/DatabaseOnDisk.h\nindex 59c2c27068e6..b20b754b727b 100644\n--- a/src/Databases/DatabaseOnDisk.h\n+++ b/src/Databases/DatabaseOnDisk.h\n@@ -68,7 +68,7 @@ class DatabaseOnDisk : public DatabaseWithOwnTablesBase\n     String getTableDataPath(const ASTCreateQuery & query) const override { return getTableDataPath(query.getTable()); }\n     String getMetadataPath() const override { return metadata_path; }\n \n-    static ASTPtr parseQueryFromMetadata(Poco::Logger * log, ContextPtr context, const String & metadata_file_path, bool throw_on_error = true, bool remove_empty = false);\n+    static ASTPtr parseQueryFromMetadata(LoggerPtr log, ContextPtr context, const String & metadata_file_path, bool throw_on_error = true, bool remove_empty = false);\n \n     /// will throw when the table we want to attach already exists (in active / detached / detached permanently form)\n     void checkMetadataFilenameAvailability(const String & to_table_name) const override;\ndiff --git a/src/Databases/DatabaseS3.cpp b/src/Databases/DatabaseS3.cpp\nindex 1721b0e9e97b..d2ca5a05ea4b 100644\n--- a/src/Databases/DatabaseS3.cpp\n+++ b/src/Databases/DatabaseS3.cpp\n@@ -49,7 +49,7 @@ DatabaseS3::DatabaseS3(const String & name_, const Configuration& config_, Conte\n     : IDatabase(name_)\n     , WithContext(context_->getGlobalContext())\n     , config(config_)\n-    , log(&Poco::Logger::get(\"DatabaseS3(\" + name_ + \")\"))\n+    , log(getLogger(\"DatabaseS3(\" + name_ + \")\"))\n {\n }\n \ndiff --git a/src/Databases/DatabaseS3.h b/src/Databases/DatabaseS3.h\nindex 8297ae4e02d6..5e7375dbd58e 100644\n--- a/src/Databases/DatabaseS3.h\n+++ b/src/Databases/DatabaseS3.h\n@@ -73,7 +73,7 @@ class DatabaseS3 : public IDatabase, protected WithContext\n     const Configuration config;\n \n     mutable Tables loaded_tables TSA_GUARDED_BY(mutex);\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Databases/DatabasesCommon.cpp b/src/Databases/DatabasesCommon.cpp\nindex bda487376213..963cf0064df8 100644\n--- a/src/Databases/DatabasesCommon.cpp\n+++ b/src/Databases/DatabasesCommon.cpp\n@@ -197,7 +197,7 @@ void cleanupObjectDefinitionFromTemporaryFlags(ASTCreateQuery & query)\n \n \n DatabaseWithOwnTablesBase::DatabaseWithOwnTablesBase(const String & name_, const String & logger, ContextPtr context_)\n-        : IDatabase(name_), WithContext(context_->getGlobalContext()), log(&Poco::Logger::get(logger))\n+        : IDatabase(name_), WithContext(context_->getGlobalContext()), log(getLogger(logger))\n {\n }\n \ndiff --git a/src/Databases/DatabasesCommon.h b/src/Databases/DatabasesCommon.h\nindex fc67596d3de8..4e9d967c11a9 100644\n--- a/src/Databases/DatabasesCommon.h\n+++ b/src/Databases/DatabasesCommon.h\n@@ -45,7 +45,7 @@ class DatabaseWithOwnTablesBase : public IDatabase, protected WithContext\n \n protected:\n     Tables tables TSA_GUARDED_BY(mutex);\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     DatabaseWithOwnTablesBase(const String & name_, const String & logger, ContextPtr context);\n \ndiff --git a/src/Databases/DatabasesOverlay.cpp b/src/Databases/DatabasesOverlay.cpp\nindex b44a97980725..8cea34416989 100644\n--- a/src/Databases/DatabasesOverlay.cpp\n+++ b/src/Databases/DatabasesOverlay.cpp\n@@ -17,7 +17,7 @@ namespace ErrorCodes\n }\n \n DatabasesOverlay::DatabasesOverlay(const String & name_, ContextPtr context_)\n-    : IDatabase(name_), WithContext(context_->getGlobalContext()), log(&Poco::Logger::get(\"DatabaseOverlay(\" + name_ + \")\"))\n+    : IDatabase(name_), WithContext(context_->getGlobalContext()), log(getLogger(\"DatabaseOverlay(\" + name_ + \")\"))\n {\n }\n \ndiff --git a/src/Databases/DatabasesOverlay.h b/src/Databases/DatabasesOverlay.h\nindex 0f31bbd6a47f..b58df506f709 100644\n--- a/src/Databases/DatabasesOverlay.h\n+++ b/src/Databases/DatabasesOverlay.h\n@@ -60,7 +60,7 @@ class DatabasesOverlay : public IDatabase, protected WithContext\n \n protected:\n     std::vector<DatabasePtr> databases;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp b/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp\nindex 5834fb96dc6a..2656835f912a 100644\n--- a/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp\n+++ b/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp\n@@ -116,7 +116,7 @@ static BlockIO tryToExecuteQuery(const String & query_to_execute, ContextMutable\n     catch (...)\n     {\n         tryLogCurrentException(\n-            &Poco::Logger::get(\"MaterializedMySQLSyncThread(\" + database + \")\"),\n+            getLogger(\"MaterializedMySQLSyncThread(\" + database + \")\"),\n             \"Query \" + query_to_execute + \" wasn't finished successfully\");\n         throw;\n     }\n@@ -255,7 +255,7 @@ MaterializedMySQLSyncThread::MaterializedMySQLSyncThread(\n     const MySQLReplication::BinlogClientPtr & binlog_client_,\n     MaterializedMySQLSettings * settings_)\n     : WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(\"MaterializedMySQLSyncThread\"))\n+    , log(getLogger(\"MaterializedMySQLSyncThread\"))\n     , database_name(database_name_)\n     , mysql_database_name(mysql_database_name_)\n     , pool(std::move(pool_)) /// NOLINT\n@@ -504,7 +504,7 @@ static inline void dumpDataForTables(\n             StreamSettings mysql_input_stream_settings(context->getSettingsRef());\n             String mysql_select_all_query = \"SELECT \" + rewriteMysqlQueryColumn(connection, mysql_database_name, table_name, context->getSettingsRef()) + \" FROM \"\n                     + backQuoteIfNeed(mysql_database_name) + \".\" + backQuoteIfNeed(table_name);\n-            LOG_INFO(&Poco::Logger::get(\"MaterializedMySQLSyncThread(\" + database_name + \")\"), \"mysql_select_all_query is {}\", mysql_select_all_query);\n+            LOG_INFO(getLogger(\"MaterializedMySQLSyncThread(\" + database_name + \")\"), \"mysql_select_all_query is {}\", mysql_select_all_query);\n             auto input = std::make_unique<MySQLSource>(connection, mysql_select_all_query, pipeline.getHeader(), mysql_input_stream_settings);\n             auto counting = std::make_shared<CountingTransform>(pipeline.getHeader());\n             Pipe pipe(std::move(input));\n@@ -516,7 +516,7 @@ static inline void dumpDataForTables(\n             executor.execute();\n \n             const Progress & progress = counting->getProgress();\n-            LOG_INFO(&Poco::Logger::get(\"MaterializedMySQLSyncThread(\" + database_name + \")\"),\n+            LOG_INFO(getLogger(\"MaterializedMySQLSyncThread(\" + database_name + \")\"),\n                 \"Materialize MySQL step 1: dump {}, {} rows, {} in {} sec., {} rows/sec., {}/sec.\"\n                 , table_name, formatReadableQuantity(progress.written_rows), formatReadableSizeWithBinarySuffix(progress.written_bytes)\n                 , watch.elapsedSeconds(), formatReadableQuantity(static_cast<size_t>(progress.written_rows / watch.elapsedSeconds()))\ndiff --git a/src/Databases/MySQL/MaterializedMySQLSyncThread.h b/src/Databases/MySQL/MaterializedMySQLSyncThread.h\nindex 004a4d67d324..03e558bfd68d 100644\n--- a/src/Databases/MySQL/MaterializedMySQLSyncThread.h\n+++ b/src/Databases/MySQL/MaterializedMySQLSyncThread.h\n@@ -56,7 +56,7 @@ class MaterializedMySQLSyncThread : WithContext\n     void assertMySQLAvailable();\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     String database_name;\n     String mysql_database_name;\ndiff --git a/src/Databases/MySQL/MySQLBinlogClient.cpp b/src/Databases/MySQL/MySQLBinlogClient.cpp\nindex e7d707f76ce4..94e01673e88f 100644\n--- a/src/Databases/MySQL/MySQLBinlogClient.cpp\n+++ b/src/Databases/MySQL/MySQLBinlogClient.cpp\n@@ -17,7 +17,7 @@ BinlogClient::BinlogClient(const BinlogFactoryPtr & factory_,\n     , binlog_client_name(name)\n     , max_bytes_in_buffer(max_bytes_in_buffer_)\n     , max_flush_ms(max_flush_ms_)\n-    , logger(&Poco::Logger::get(\"BinlogClient(\" + name + \")\"))\n+    , logger(getLogger(\"BinlogClient(\" + name + \")\"))\n {\n }\n \ndiff --git a/src/Databases/MySQL/MySQLBinlogClient.h b/src/Databases/MySQL/MySQLBinlogClient.h\nindex b76934d08cf6..a45b885d87b3 100644\n--- a/src/Databases/MySQL/MySQLBinlogClient.h\n+++ b/src/Databases/MySQL/MySQLBinlogClient.h\n@@ -48,7 +48,7 @@ class BinlogClient\n     std::vector<BinlogEventsDispatcherPtr> dispatchers;\n     String binlog_checksum;\n     mutable std::mutex mutex;\n-    Poco::Logger * logger = nullptr;\n+    LoggerPtr logger = nullptr;\n     int dispatchers_count = 0;\n };\n \ndiff --git a/src/Databases/MySQL/MySQLBinlogEventsDispatcher.cpp b/src/Databases/MySQL/MySQLBinlogEventsDispatcher.cpp\nindex 4af307f9c0f9..d027d4b21921 100644\n--- a/src/Databases/MySQL/MySQLBinlogEventsDispatcher.cpp\n+++ b/src/Databases/MySQL/MySQLBinlogEventsDispatcher.cpp\n@@ -19,7 +19,7 @@ class BinlogFromDispatcher : public IBinlog\n         , mysql_database_names(mysql_database_names_)\n         , max_bytes(max_bytes_)\n         , max_waiting_ms(max_waiting_ms_)\n-        , logger(&Poco::Logger::get(\"BinlogFromDispatcher(\" + name + \")\"))\n+        , logger(getLogger(\"BinlogFromDispatcher(\" + name + \")\"))\n     {\n     }\n \n@@ -65,7 +65,7 @@ class BinlogFromDispatcher : public IBinlog\n \n     std::condition_variable cv;\n     bool is_cancelled = false;\n-    Poco::Logger * logger = nullptr;\n+    LoggerPtr logger = nullptr;\n     std::exception_ptr exception;\n };\n \n@@ -84,7 +84,7 @@ BinlogEventsDispatcher::BinlogEventsDispatcher(const String & logger_name_, size\n     : logger_name(logger_name_)\n     , max_bytes_in_buffer(max_bytes_in_buffer_)\n     , max_flush_ms(max_flush_ms_)\n-    , logger(&Poco::Logger::get(\"BinlogEventsDispatcher(\" + logger_name + \")\"))\n+    , logger(getLogger(\"BinlogEventsDispatcher(\" + logger_name + \")\"))\n     , dispatching_thread(std::make_unique<ThreadFromGlobalPool>([this]() { dispatchEvents(); }))\n {\n }\ndiff --git a/src/Databases/MySQL/MySQLBinlogEventsDispatcher.h b/src/Databases/MySQL/MySQLBinlogEventsDispatcher.h\nindex 433796970150..324deba3617d 100644\n--- a/src/Databases/MySQL/MySQLBinlogEventsDispatcher.h\n+++ b/src/Databases/MySQL/MySQLBinlogEventsDispatcher.h\n@@ -110,7 +110,7 @@ class BinlogEventsDispatcher final : boost::noncopyable\n     const String logger_name;\n     const size_t max_bytes_in_buffer = 0;\n     const UInt64 max_flush_ms = 0;\n-    Poco::Logger * logger = nullptr;\n+    LoggerPtr logger = nullptr;\n \n     BinlogPtr binlog_read_from;\n \ndiff --git a/src/Databases/PostgreSQL/DatabasePostgreSQL.cpp b/src/Databases/PostgreSQL/DatabasePostgreSQL.cpp\nindex 1fe5c078581a..b07b203f7862 100644\n--- a/src/Databases/PostgreSQL/DatabasePostgreSQL.cpp\n+++ b/src/Databases/PostgreSQL/DatabasePostgreSQL.cpp\n@@ -57,7 +57,7 @@ DatabasePostgreSQL::DatabasePostgreSQL(\n     , configuration(configuration_)\n     , pool(std::move(pool_))\n     , cache_tables(cache_tables_)\n-    , log(&Poco::Logger::get(\"DatabasePostgreSQL(\" + dbname_ + \")\"))\n+    , log(getLogger(\"DatabasePostgreSQL(\" + dbname_ + \")\"))\n {\n     fs::create_directories(metadata_path);\n     cleaner_task = getContext()->getSchedulePool().createTask(\"PostgreSQLCleanerTask\", [this]{ removeOutdatedTables(); });\n@@ -531,7 +531,7 @@ void registerDatabasePostgreSQL(DatabaseFactory & factory)\n                 else\n                 {\n                     use_table_cache = safeGetLiteralValue<UInt8>(engine_args[4], engine_name);\n-                    LOG_WARNING(&Poco::Logger::get(\"DatabaseFactory\"), \"A deprecated syntax of PostgreSQL database engine is used\");\n+                    LOG_WARNING(getLogger(\"DatabaseFactory\"), \"A deprecated syntax of PostgreSQL database engine is used\");\n                     is_deprecated_syntax = true;\n                 }\n             }\ndiff --git a/src/Databases/PostgreSQL/DatabasePostgreSQL.h b/src/Databases/PostgreSQL/DatabasePostgreSQL.h\nindex d731e06649be..3ba7333c98ec 100644\n--- a/src/Databases/PostgreSQL/DatabasePostgreSQL.h\n+++ b/src/Databases/PostgreSQL/DatabasePostgreSQL.h\n@@ -73,7 +73,7 @@ class DatabasePostgreSQL final : public IDatabase, WithContext\n     mutable Tables cached_tables;\n     std::unordered_set<std::string> detached_or_dropped;\n     BackgroundSchedulePool::TaskHolder cleaner_task;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     String getTableNameForLogs(const String & table_name) const;\n \ndiff --git a/src/Databases/SQLite/DatabaseSQLite.cpp b/src/Databases/SQLite/DatabaseSQLite.cpp\nindex 605a354bd7eb..b3d5288cdf77 100644\n--- a/src/Databases/SQLite/DatabaseSQLite.cpp\n+++ b/src/Databases/SQLite/DatabaseSQLite.cpp\n@@ -33,7 +33,7 @@ DatabaseSQLite::DatabaseSQLite(\n     , WithContext(context_->getGlobalContext())\n     , database_engine_define(database_engine_define_->clone())\n     , database_path(database_path_)\n-    , log(&Poco::Logger::get(\"DatabaseSQLite\"))\n+    , log(getLogger(\"DatabaseSQLite\"))\n {\n     sqlite_db = openSQLiteDB(database_path_, context_, !is_attach_);\n }\ndiff --git a/src/Databases/SQLite/DatabaseSQLite.h b/src/Databases/SQLite/DatabaseSQLite.h\nindex a89fbc32c3d7..e5e93bbc8ce3 100644\n--- a/src/Databases/SQLite/DatabaseSQLite.h\n+++ b/src/Databases/SQLite/DatabaseSQLite.h\n@@ -50,7 +50,7 @@ class DatabaseSQLite final : public IDatabase, WithContext\n \n     mutable SQLitePtr sqlite_db;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     bool checkSQLiteTable(const String & table_name) const;\n \ndiff --git a/src/Databases/SQLite/SQLiteUtils.cpp b/src/Databases/SQLite/SQLiteUtils.cpp\nindex 19b8662707b1..eeea04476d3d 100644\n--- a/src/Databases/SQLite/SQLiteUtils.cpp\n+++ b/src/Databases/SQLite/SQLiteUtils.cpp\n@@ -21,7 +21,7 @@ void processSQLiteError(const String & message, bool throw_on_error)\n     if (throw_on_error)\n         throw Exception::createDeprecated(message, ErrorCodes::PATH_ACCESS_DENIED);\n     else\n-        LOG_ERROR(&Poco::Logger::get(\"SQLiteEngine\"), fmt::runtime(message));\n+        LOG_ERROR(getLogger(\"SQLiteEngine\"), fmt::runtime(message));\n }\n \n String validateSQLiteDatabasePath(const String & path, const String & user_files_path, bool need_check, bool throw_on_error)\n@@ -54,7 +54,7 @@ SQLitePtr openSQLiteDB(const String & path, ContextPtr context, bool throw_on_er\n         return nullptr;\n \n     if (!fs::exists(database_path))\n-        LOG_DEBUG(&Poco::Logger::get(\"SQLite\"), \"SQLite database path {} does not exist, will create an empty SQLite database\", database_path);\n+        LOG_DEBUG(getLogger(\"SQLite\"), \"SQLite database path {} does not exist, will create an empty SQLite database\", database_path);\n \n     sqlite3 * tmp_sqlite_db = nullptr;\n     int status;\ndiff --git a/src/Databases/TablesDependencyGraph.cpp b/src/Databases/TablesDependencyGraph.cpp\nindex 16404c6870f5..6b9e202d9005 100644\n--- a/src/Databases/TablesDependencyGraph.cpp\n+++ b/src/Databases/TablesDependencyGraph.cpp\n@@ -720,10 +720,10 @@ void TablesDependencyGraph::log() const\n }\n \n \n-Poco::Logger * TablesDependencyGraph::getLogger() const\n+LoggerPtr TablesDependencyGraph::getLogger() const\n {\n     if (!logger)\n-        logger = &Poco::Logger::get(name_for_logging);\n+        logger = ::getLogger(name_for_logging);\n     return logger;\n }\n \ndiff --git a/src/Databases/TablesDependencyGraph.h b/src/Databases/TablesDependencyGraph.h\nindex 50be3bbf969b..f0553cef3216 100644\n--- a/src/Databases/TablesDependencyGraph.h\n+++ b/src/Databases/TablesDependencyGraph.h\n@@ -163,7 +163,7 @@ class TablesDependencyGraph\n     mutable bool levels_calculated = false;\n \n     const String name_for_logging;\n-    mutable Poco::Logger * logger = nullptr;\n+    mutable LoggerPtr logger = nullptr;\n \n     Node * findNode(const StorageID & table_id) const;\n     Node * addOrUpdateNode(const StorageID & table_id);\n@@ -175,7 +175,7 @@ class TablesDependencyGraph\n     void setNeedRecalculateLevels() const;\n     const NodesSortedByLevel & getNodesSortedByLevel() const;\n \n-    Poco::Logger * getLogger() const;\n+    LoggerPtr getLogger() const;\n };\n \n }\ndiff --git a/src/Databases/TablesLoader.cpp b/src/Databases/TablesLoader.cpp\nindex f1b5c4377fef..48745ff91c2b 100644\n--- a/src/Databases/TablesLoader.cpp\n+++ b/src/Databases/TablesLoader.cpp\n@@ -29,7 +29,7 @@ TablesLoader::TablesLoader(ContextMutablePtr global_context_, Databases database\n     , async_loader(global_context->getAsyncLoader())\n {\n     metadata.default_database = global_context->getCurrentDatabase();\n-    log = &Poco::Logger::get(\"TablesLoader\");\n+    log = getLogger(\"TablesLoader\");\n }\n \n LoadTaskPtrs TablesLoader::loadTablesAsync(LoadJobSet load_after)\ndiff --git a/src/Databases/TablesLoader.h b/src/Databases/TablesLoader.h\nindex 038aa35895f9..26b5777f1a97 100644\n--- a/src/Databases/TablesLoader.h\n+++ b/src/Databases/TablesLoader.h\n@@ -73,7 +73,7 @@ class TablesLoader\n     TablesDependencyGraph referential_dependencies;\n     TablesDependencyGraph loading_dependencies;\n     TablesDependencyGraph all_loading_dependencies;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::atomic<size_t> tables_processed{0};\n     AtomicStopwatch stopwatch;\n \ndiff --git a/src/Dictionaries/CacheDictionary.cpp b/src/Dictionaries/CacheDictionary.cpp\nindex b40a60e09153..000f0ef5b4c6 100644\n--- a/src/Dictionaries/CacheDictionary.cpp\n+++ b/src/Dictionaries/CacheDictionary.cpp\n@@ -63,7 +63,7 @@ CacheDictionary<dictionary_key_type>::CacheDictionary(\n             update(unit_to_update);\n         })\n     , configuration(configuration_)\n-    , log(&Poco::Logger::get(\"ExternalDictionaries\"))\n+    , log(getLogger(\"ExternalDictionaries\"))\n     , rnd_engine(randomSeed())\n {\n     if (!source_ptr->supportsSelectiveLoad())\ndiff --git a/src/Dictionaries/CacheDictionary.h b/src/Dictionaries/CacheDictionary.h\nindex 66efb4a85a51..aae86a83f12a 100644\n--- a/src/Dictionaries/CacheDictionary.h\n+++ b/src/Dictionaries/CacheDictionary.h\n@@ -202,7 +202,7 @@ class CacheDictionary final : public IDictionary\n \n     const CacheDictionaryConfiguration configuration;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     mutable pcg64 rnd_engine;\n \ndiff --git a/src/Dictionaries/CassandraDictionarySource.cpp b/src/Dictionaries/CassandraDictionarySource.cpp\nindex e0cf2483b3d5..b3bf288ef5af 100644\n--- a/src/Dictionaries/CassandraDictionarySource.cpp\n+++ b/src/Dictionaries/CassandraDictionarySource.cpp\n@@ -105,7 +105,7 @@ CassandraDictionarySource::CassandraDictionarySource(\n     const DictionaryStructure & dict_struct_,\n     const Configuration & configuration_,\n     const Block & sample_block_)\n-    : log(&Poco::Logger::get(\"CassandraDictionarySource\"))\n+    : log(getLogger(\"CassandraDictionarySource\"))\n     , dict_struct(dict_struct_)\n     , configuration(configuration_)\n     , sample_block(sample_block_)\ndiff --git a/src/Dictionaries/CassandraDictionarySource.h b/src/Dictionaries/CassandraDictionarySource.h\nindex 2591b33c6388..3700642fc5b5 100644\n--- a/src/Dictionaries/CassandraDictionarySource.h\n+++ b/src/Dictionaries/CassandraDictionarySource.h\n@@ -77,7 +77,7 @@ class CassandraDictionarySource final : public IDictionarySource\n     void maybeAllowFiltering(String & query) const;\n     CassSessionShared getSession();\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const DictionaryStructure dict_struct;\n     const Configuration configuration;\n     Block sample_block;\ndiff --git a/src/Dictionaries/CassandraHelpers.cpp b/src/Dictionaries/CassandraHelpers.cpp\nindex e93b3fe8d496..4c569d00957a 100644\n--- a/src/Dictionaries/CassandraHelpers.cpp\n+++ b/src/Dictionaries/CassandraHelpers.cpp\n@@ -47,7 +47,7 @@ void setupCassandraDriverLibraryLogging(CassLogLevel level)\n {\n     std::call_once(setup_logging_flag, [level]()\n     {\n-        Poco::Logger * logger = &Poco::Logger::get(\"CassandraDriverLibrary\");\n+        Poco::Logger * logger = getRawLogger(\"CassandraDriverLibrary\");\n         cass_log_set_level(level);\n         if (level != CASS_LOG_DISABLED)\n             cass_log_set_callback(cassandraLogCallback, logger);\ndiff --git a/src/Dictionaries/ClickHouseDictionarySource.h b/src/Dictionaries/ClickHouseDictionarySource.h\nindex cfb6a0bcd378..3357514eab2b 100644\n--- a/src/Dictionaries/ClickHouseDictionarySource.h\n+++ b/src/Dictionaries/ClickHouseDictionarySource.h\n@@ -85,7 +85,7 @@ class ClickHouseDictionarySource final : public IDictionarySource\n     ContextMutablePtr context;\n     ConnectionPoolWithFailoverPtr pool;\n     std::string load_all_query;\n-    Poco::Logger * log = &Poco::Logger::get(\"ClickHouseDictionarySource\");\n+    LoggerPtr log = getLogger(\"ClickHouseDictionarySource\");\n \n     /// RegExpTreeDictionary is the only dictionary whose structure of attributions differ from the input block.\n     /// For now we need to modify sample_block in the ctor of RegExpTreeDictionary.\ndiff --git a/src/Dictionaries/DictionaryFactory.cpp b/src/Dictionaries/DictionaryFactory.cpp\nindex f6102d7c6571..a566fb27de40 100644\n--- a/src/Dictionaries/DictionaryFactory.cpp\n+++ b/src/Dictionaries/DictionaryFactory.cpp\n@@ -46,7 +46,7 @@ DictionaryPtr DictionaryFactory::create(\n \n     DictionarySourcePtr source_ptr = DictionarySourceFactory::instance().create(\n         name, config, config_prefix + \".source\", dict_struct, global_context, config.getString(config_prefix + \".database\", \"\"), created_from_ddl);\n-    LOG_TRACE(&Poco::Logger::get(\"DictionaryFactory\"), \"Created dictionary source '{}' for dictionary '{}'\", source_ptr->toString(), name);\n+    LOG_TRACE(getLogger(\"DictionaryFactory\"), \"Created dictionary source '{}' for dictionary '{}'\", source_ptr->toString(), name);\n \n     const auto & layout_type = keys.front();\n \ndiff --git a/src/Dictionaries/DictionarySourceFactory.cpp b/src/Dictionaries/DictionarySourceFactory.cpp\nindex 5ae4bb5a4397..eedf6967c139 100644\n--- a/src/Dictionaries/DictionarySourceFactory.cpp\n+++ b/src/Dictionaries/DictionarySourceFactory.cpp\n@@ -65,7 +65,7 @@ namespace\n }\n \n \n-DictionarySourceFactory::DictionarySourceFactory() : log(&Poco::Logger::get(\"DictionarySourceFactory\"))\n+DictionarySourceFactory::DictionarySourceFactory() : log(getLogger(\"DictionarySourceFactory\"))\n {\n }\n \ndiff --git a/src/Dictionaries/DictionarySourceFactory.h b/src/Dictionaries/DictionarySourceFactory.h\nindex 4c867db4ea1d..a9007230047c 100644\n--- a/src/Dictionaries/DictionarySourceFactory.h\n+++ b/src/Dictionaries/DictionarySourceFactory.h\n@@ -59,7 +59,7 @@ class DictionarySourceFactory : private boost::noncopyable\n     using SourceRegistry = std::unordered_map<std::string, Creator>;\n     SourceRegistry registered_sources;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Dictionaries/DirectDictionary.cpp b/src/Dictionaries/DirectDictionary.cpp\nindex 64c7eb14024f..5bfcb22c23b2 100644\n--- a/src/Dictionaries/DirectDictionary.cpp\n+++ b/src/Dictionaries/DirectDictionary.cpp\n@@ -118,7 +118,7 @@ Columns DirectDictionary<dictionary_key_type>::getColumns(\n         block_key_columns.clear();\n     }\n \n-    LOG_DEBUG(&Poco::Logger::get(\"DirectDictionary\"), \"read {} blocks with {} rows from pipeline in {} ms\",\n+    LOG_DEBUG(getLogger(\"DirectDictionary\"), \"read {} blocks with {} rows from pipeline in {} ms\",\n         block_num, rows_num, watch.elapsedMilliseconds());\n \n     Field value_to_insert;\n@@ -353,7 +353,7 @@ Pipe DirectDictionary<dictionary_key_type>::getSourcePipe(\n             pipe = Pipe(std::make_shared<SourceFromQueryPipeline<PullingPipelineExecutor>>(std::move(pipeline)));\n     }\n \n-    LOG_DEBUG(&Poco::Logger::get(\"DirectDictionary\"), \"building pipeline for loading keys done in {} ms\", watch.elapsedMilliseconds());\n+    LOG_DEBUG(getLogger(\"DirectDictionary\"), \"building pipeline for loading keys done in {} ms\", watch.elapsedMilliseconds());\n     return pipe;\n }\n \ndiff --git a/src/Dictionaries/Embedded/RegionsHierarchies.cpp b/src/Dictionaries/Embedded/RegionsHierarchies.cpp\nindex c3c62bcc83cb..3f1222fff3ff 100644\n--- a/src/Dictionaries/Embedded/RegionsHierarchies.cpp\n+++ b/src/Dictionaries/Embedded/RegionsHierarchies.cpp\n@@ -8,7 +8,7 @@ namespace DB\n \n RegionsHierarchies::RegionsHierarchies(IRegionsHierarchiesDataProviderPtr data_provider)\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"RegionsHierarchies\");\n+    LoggerPtr log = getLogger(\"RegionsHierarchies\");\n \n     LOG_DEBUG(log, \"Adding default regions hierarchy\");\n     data.emplace(\"\", data_provider->getDefaultHierarchySource());\ndiff --git a/src/Dictionaries/Embedded/RegionsHierarchy.cpp b/src/Dictionaries/Embedded/RegionsHierarchy.cpp\nindex 23f4c250a23e..a59f6fcd0e70 100644\n--- a/src/Dictionaries/Embedded/RegionsHierarchy.cpp\n+++ b/src/Dictionaries/Embedded/RegionsHierarchy.cpp\n@@ -23,7 +23,7 @@ RegionsHierarchy::RegionsHierarchy(IRegionsHierarchyDataSourcePtr data_source_)\n \n void RegionsHierarchy::reload()\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"RegionsHierarchy\");\n+    LoggerPtr log = getLogger(\"RegionsHierarchy\");\n \n     if (!data_source->isModified())\n         return;\ndiff --git a/src/Dictionaries/Embedded/RegionsNames.cpp b/src/Dictionaries/Embedded/RegionsNames.cpp\nindex 847dfe99b103..c89bacc7ec91 100644\n--- a/src/Dictionaries/Embedded/RegionsNames.cpp\n+++ b/src/Dictionaries/Embedded/RegionsNames.cpp\n@@ -42,7 +42,7 @@ std::string RegionsNames::dumpSupportedLanguagesNames()\n \n void RegionsNames::reload()\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"RegionsNames\");\n+    LoggerPtr log = getLogger(\"RegionsNames\");\n     LOG_DEBUG(log, \"Reloading regions names\");\n \n     RegionID max_region_id = 0;\ndiff --git a/src/Dictionaries/ExecutableDictionarySource.cpp b/src/Dictionaries/ExecutableDictionarySource.cpp\nindex f1acd6102742..6b9f97a6d5cf 100644\n--- a/src/Dictionaries/ExecutableDictionarySource.cpp\n+++ b/src/Dictionaries/ExecutableDictionarySource.cpp\n@@ -71,7 +71,7 @@ ExecutableDictionarySource::ExecutableDictionarySource(\n     Block & sample_block_,\n     std::shared_ptr<ShellCommandSourceCoordinator> coordinator_,\n     ContextPtr context_)\n-    : log(&Poco::Logger::get(\"ExecutableDictionarySource\"))\n+    : log(getLogger(\"ExecutableDictionarySource\"))\n     , dict_struct(dict_struct_)\n     , configuration(configuration_)\n     , sample_block(sample_block_)\n@@ -93,7 +93,7 @@ ExecutableDictionarySource::ExecutableDictionarySource(\n }\n \n ExecutableDictionarySource::ExecutableDictionarySource(const ExecutableDictionarySource & other)\n-    : log(&Poco::Logger::get(\"ExecutableDictionarySource\"))\n+    : log(getLogger(\"ExecutableDictionarySource\"))\n     , update_time(other.update_time)\n     , dict_struct(other.dict_struct)\n     , configuration(other.configuration)\ndiff --git a/src/Dictionaries/ExecutableDictionarySource.h b/src/Dictionaries/ExecutableDictionarySource.h\nindex c7067a628938..eb936434218a 100644\n--- a/src/Dictionaries/ExecutableDictionarySource.h\n+++ b/src/Dictionaries/ExecutableDictionarySource.h\n@@ -63,7 +63,7 @@ class ExecutableDictionarySource final : public IDictionarySource\n     QueryPipeline getStreamForBlock(const Block & block);\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     time_t update_time = 0;\n     const DictionaryStructure dict_struct;\n     const Configuration configuration;\ndiff --git a/src/Dictionaries/ExecutablePoolDictionarySource.cpp b/src/Dictionaries/ExecutablePoolDictionarySource.cpp\nindex d28c73c9c52b..d8111afdc199 100644\n--- a/src/Dictionaries/ExecutablePoolDictionarySource.cpp\n+++ b/src/Dictionaries/ExecutablePoolDictionarySource.cpp\n@@ -40,7 +40,7 @@ ExecutablePoolDictionarySource::ExecutablePoolDictionarySource(\n     , sample_block(sample_block_)\n     , coordinator(std::move(coordinator_))\n     , context(context_)\n-    , log(&Poco::Logger::get(\"ExecutablePoolDictionarySource\"))\n+    , log(getLogger(\"ExecutablePoolDictionarySource\"))\n {\n     /// Remove keys from sample_block for implicit_key dictionary because\n     /// these columns will not be returned from source\n@@ -64,7 +64,7 @@ ExecutablePoolDictionarySource::ExecutablePoolDictionarySource(const ExecutableP\n     , sample_block(other.sample_block)\n     , coordinator(other.coordinator)\n     , context(Context::createCopy(other.context))\n-    , log(&Poco::Logger::get(\"ExecutablePoolDictionarySource\"))\n+    , log(getLogger(\"ExecutablePoolDictionarySource\"))\n {\n }\n \ndiff --git a/src/Dictionaries/ExecutablePoolDictionarySource.h b/src/Dictionaries/ExecutablePoolDictionarySource.h\nindex e8cc6e834068..752d8ea2757f 100644\n--- a/src/Dictionaries/ExecutablePoolDictionarySource.h\n+++ b/src/Dictionaries/ExecutablePoolDictionarySource.h\n@@ -72,7 +72,7 @@ class ExecutablePoolDictionarySource final : public IDictionarySource\n     Block sample_block;\n     std::shared_ptr<ShellCommandSourceCoordinator> coordinator;\n     ContextPtr context;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Dictionaries/FileDictionarySource.cpp b/src/Dictionaries/FileDictionarySource.cpp\nindex 86287971428e..16a4ecaee754 100644\n--- a/src/Dictionaries/FileDictionarySource.cpp\n+++ b/src/Dictionaries/FileDictionarySource.cpp\n@@ -48,7 +48,7 @@ FileDictionarySource::FileDictionarySource(const FileDictionarySource & other)\n \n QueryPipeline FileDictionarySource::loadAll()\n {\n-    LOG_TRACE(&Poco::Logger::get(\"FileDictionary\"), \"loadAll {}\", toString());\n+    LOG_TRACE(getLogger(\"FileDictionary\"), \"loadAll {}\", toString());\n     auto in_ptr = std::make_unique<ReadBufferFromFile>(filepath);\n     auto source = context->getInputFormat(format, *in_ptr, sample_block, max_block_size);\n     source->addBuffer(std::move(in_ptr));\ndiff --git a/src/Dictionaries/HTTPDictionarySource.cpp b/src/Dictionaries/HTTPDictionarySource.cpp\nindex 689593a969ef..bf42b7931ed9 100644\n--- a/src/Dictionaries/HTTPDictionarySource.cpp\n+++ b/src/Dictionaries/HTTPDictionarySource.cpp\n@@ -32,7 +32,7 @@ HTTPDictionarySource::HTTPDictionarySource(\n     const Poco::Net::HTTPBasicCredentials & credentials_,\n     Block & sample_block_,\n     ContextPtr context_)\n-    : log(&Poco::Logger::get(\"HTTPDictionarySource\"))\n+    : log(getLogger(\"HTTPDictionarySource\"))\n     , update_time(std::chrono::system_clock::from_time_t(0))\n     , dict_struct(dict_struct_)\n     , configuration(configuration_)\n@@ -45,7 +45,7 @@ HTTPDictionarySource::HTTPDictionarySource(\n }\n \n HTTPDictionarySource::HTTPDictionarySource(const HTTPDictionarySource & other)\n-    : log(&Poco::Logger::get(\"HTTPDictionarySource\"))\n+    : log(getLogger(\"HTTPDictionarySource\"))\n     , update_time(other.update_time)\n     , dict_struct(other.dict_struct)\n     , configuration(other.configuration)\ndiff --git a/src/Dictionaries/HTTPDictionarySource.h b/src/Dictionaries/HTTPDictionarySource.h\nindex e22aacd89f1f..414372fe7ac2 100644\n--- a/src/Dictionaries/HTTPDictionarySource.h\n+++ b/src/Dictionaries/HTTPDictionarySource.h\n@@ -66,7 +66,7 @@ class HTTPDictionarySource final : public IDictionarySource\n     // wrap buffer using encoding from made request\n     QueryPipeline createWrappedBuffer(std::unique_ptr<ReadWriteBufferFromHTTP> http_buffer);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     LocalDateTime getLastModification() const;\n \ndiff --git a/src/Dictionaries/HashedArrayDictionary.cpp b/src/Dictionaries/HashedArrayDictionary.cpp\nindex 4c9ff8abe80a..648ecb681fc0 100644\n--- a/src/Dictionaries/HashedArrayDictionary.cpp\n+++ b/src/Dictionaries/HashedArrayDictionary.cpp\n@@ -32,7 +32,7 @@ HashedArrayDictionary<dictionary_key_type, sharded>::HashedArrayDictionary(\n     const HashedArrayDictionaryStorageConfiguration & configuration_,\n     BlockPtr update_field_loaded_block_)\n     : IDictionary(dict_id_)\n-    , log(&Poco::Logger::get(\"HashedArrayDictionary\"))\n+    , log(getLogger(\"HashedArrayDictionary\"))\n     , dict_struct(dict_struct_)\n     , source_ptr(std::move(source_ptr_))\n     , configuration(configuration_)\n@@ -822,7 +822,7 @@ void HashedArrayDictionary<dictionary_key_type, sharded>::loadData()\n         if (parallel_loader)\n             parallel_loader->finish();\n \n-        LOG_DEBUG(&Poco::Logger::get(\"HashedArrayDictionary\"),\n+        LOG_DEBUG(getLogger(\"HashedArrayDictionary\"),\n             \"Finished {}reading {} blocks with {} rows from pipeline in {:.2f} sec and inserted into hashtable in {:.2f} sec\",\n             configuration.use_async_executor ? \"asynchronous \" : \"\",\n             total_blocks, total_rows, pull_time_microseconds / 1000000.0, process_time_microseconds / 1000000.0);\ndiff --git a/src/Dictionaries/HashedArrayDictionary.h b/src/Dictionaries/HashedArrayDictionary.h\nindex 606008ce9210..86b21443e186 100644\n--- a/src/Dictionaries/HashedArrayDictionary.h\n+++ b/src/Dictionaries/HashedArrayDictionary.h\n@@ -244,7 +244,7 @@ class HashedArrayDictionary final : public IDictionary\n \n     void resize(size_t total_rows);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     const DictionaryStructure dict_struct;\n     const DictionarySourcePtr source_ptr;\ndiff --git a/src/Dictionaries/HashedDictionary.h b/src/Dictionaries/HashedDictionary.h\nindex 8009ffab80a3..0b8419dd242c 100644\n--- a/src/Dictionaries/HashedDictionary.h\n+++ b/src/Dictionaries/HashedDictionary.h\n@@ -251,7 +251,7 @@ class HashedDictionary final : public IDictionary\n \n     void resize(size_t added_rows);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     const DictionaryStructure dict_struct;\n     const DictionarySourcePtr source_ptr;\n@@ -293,7 +293,7 @@ HashedDictionary<dictionary_key_type, sparse, sharded>::HashedDictionary(\n     const HashedDictionaryConfiguration & configuration_,\n     BlockPtr update_field_loaded_block_)\n     : IDictionary(dict_id_)\n-    , log(&Poco::Logger::get(\"HashedDictionary\"))\n+    , log(getLogger(\"HashedDictionary\"))\n     , dict_struct(dict_struct_)\n     , source_ptr(std::move(source_ptr_))\n     , configuration(configuration_)\ndiff --git a/src/Dictionaries/IPAddressDictionary.cpp b/src/Dictionaries/IPAddressDictionary.cpp\nindex 2e3c09c67c5d..98ba95f00538 100644\n--- a/src/Dictionaries/IPAddressDictionary.cpp\n+++ b/src/Dictionaries/IPAddressDictionary.cpp\n@@ -205,7 +205,7 @@ IPAddressDictionary::IPAddressDictionary(\n     , source_ptr{std::move(source_ptr_)}\n     , configuration(configuration_)\n     , access_to_key_from_attributes(dict_struct_.access_to_key_from_attributes)\n-    , logger(&Poco::Logger::get(\"IPAddressDictionary\"))\n+    , logger(getLogger(\"IPAddressDictionary\"))\n {\n     createAttributes();\n     loadData();\ndiff --git a/src/Dictionaries/IPAddressDictionary.h b/src/Dictionaries/IPAddressDictionary.h\nindex c5b9287c2b54..d758e23043d6 100644\n--- a/src/Dictionaries/IPAddressDictionary.h\n+++ b/src/Dictionaries/IPAddressDictionary.h\n@@ -234,7 +234,7 @@ class IPAddressDictionary final : public IDictionary\n     mutable std::atomic<size_t> query_count{0};\n     mutable std::atomic<size_t> found_count{0};\n \n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n };\n \n }\ndiff --git a/src/Dictionaries/LibraryDictionarySource.cpp b/src/Dictionaries/LibraryDictionarySource.cpp\nindex 7eb4d803fe8f..f6f104ca11da 100644\n--- a/src/Dictionaries/LibraryDictionarySource.cpp\n+++ b/src/Dictionaries/LibraryDictionarySource.cpp\n@@ -30,7 +30,7 @@ LibraryDictionarySource::LibraryDictionarySource(\n     Block & sample_block_,\n     ContextPtr context_,\n     bool created_from_ddl)\n-    : log(&Poco::Logger::get(\"LibraryDictionarySource\"))\n+    : log(getLogger(\"LibraryDictionarySource\"))\n     , dict_struct{dict_struct_}\n     , config_prefix{config_prefix_}\n     , path{config.getString(config_prefix + \".path\", \"\")}\n@@ -78,7 +78,7 @@ LibraryDictionarySource::~LibraryDictionarySource()\n \n \n LibraryDictionarySource::LibraryDictionarySource(const LibraryDictionarySource & other)\n-    : log(&Poco::Logger::get(\"LibraryDictionarySource\"))\n+    : log(getLogger(\"LibraryDictionarySource\"))\n     , dict_struct{other.dict_struct}\n     , config_prefix{other.config_prefix}\n     , path{other.path}\ndiff --git a/src/Dictionaries/LibraryDictionarySource.h b/src/Dictionaries/LibraryDictionarySource.h\nindex 57ab9976a3b6..04a3d838577c 100644\n--- a/src/Dictionaries/LibraryDictionarySource.h\n+++ b/src/Dictionaries/LibraryDictionarySource.h\n@@ -75,7 +75,7 @@ class LibraryDictionarySource final : public IDictionarySource\n \n     static Field getDictID() { return UUIDHelpers::generateV4(); }\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     const DictionaryStructure dict_struct;\n     const std::string config_prefix;\ndiff --git a/src/Dictionaries/MySQLDictionarySource.cpp b/src/Dictionaries/MySQLDictionarySource.cpp\nindex e61409e2b54b..9a84512fde6b 100644\n--- a/src/Dictionaries/MySQLDictionarySource.cpp\n+++ b/src/Dictionaries/MySQLDictionarySource.cpp\n@@ -173,7 +173,7 @@ MySQLDictionarySource::MySQLDictionarySource(\n     mysqlxx::PoolWithFailoverPtr pool_,\n     const Block & sample_block_,\n     const StreamSettings & settings_)\n-    : log(&Poco::Logger::get(\"MySQLDictionarySource\"))\n+    : log(getLogger(\"MySQLDictionarySource\"))\n     , update_time(std::chrono::system_clock::from_time_t(0))\n     , dict_struct(dict_struct_)\n     , configuration(configuration_)\n@@ -187,7 +187,7 @@ MySQLDictionarySource::MySQLDictionarySource(\n \n /// copy-constructor is provided in order to support cloneability\n MySQLDictionarySource::MySQLDictionarySource(const MySQLDictionarySource & other)\n-    : log(&Poco::Logger::get(\"MySQLDictionarySource\"))\n+    : log(getLogger(\"MySQLDictionarySource\"))\n     , update_time(other.update_time)\n     , dict_struct(other.dict_struct)\n     , configuration(other.configuration)\ndiff --git a/src/Dictionaries/MySQLDictionarySource.h b/src/Dictionaries/MySQLDictionarySource.h\nindex 1d43ebfe2ba6..d9eea3f3e261 100644\n--- a/src/Dictionaries/MySQLDictionarySource.h\n+++ b/src/Dictionaries/MySQLDictionarySource.h\n@@ -82,7 +82,7 @@ class MySQLDictionarySource final : public IDictionarySource\n     // execute invalidate_query. expects single cell in result\n     std::string doInvalidateQuery(const std::string & request) const;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::chrono::time_point<std::chrono::system_clock> update_time;\n     const DictionaryStructure dict_struct;\ndiff --git a/src/Dictionaries/NullDictionarySource.cpp b/src/Dictionaries/NullDictionarySource.cpp\nindex 45dcc77f93d0..2d5656e13359 100644\n--- a/src/Dictionaries/NullDictionarySource.cpp\n+++ b/src/Dictionaries/NullDictionarySource.cpp\n@@ -20,7 +20,7 @@ NullDictionarySource::NullDictionarySource(const NullDictionarySource & other) :\n \n QueryPipeline NullDictionarySource::loadAll()\n {\n-    LOG_TRACE(&Poco::Logger::get(\"NullDictionarySource\"), \"loadAll {}\", toString());\n+    LOG_TRACE(getLogger(\"NullDictionarySource\"), \"loadAll {}\", toString());\n     return QueryPipeline(std::make_shared<NullSource>(sample_block));\n }\n \ndiff --git a/src/Dictionaries/PolygonDictionaryUtils.cpp b/src/Dictionaries/PolygonDictionaryUtils.cpp\nindex 2af97d3fc6fa..8f060fe7b8d0 100644\n--- a/src/Dictionaries/PolygonDictionaryUtils.cpp\n+++ b/src/Dictionaries/PolygonDictionaryUtils.cpp\n@@ -69,7 +69,7 @@ const FinalCellWithSlabs * FinalCellWithSlabs::find(Coord, Coord) const\n \n SlabsPolygonIndex::SlabsPolygonIndex(\n     const std::vector<Polygon> & polygons)\n-    : log(&Poco::Logger::get(\"SlabsPolygonIndex\")),\n+    : log(getLogger(\"SlabsPolygonIndex\")),\n       sorted_x(uniqueX(polygons))\n {\n     indexBuild(polygons);\ndiff --git a/src/Dictionaries/PolygonDictionaryUtils.h b/src/Dictionaries/PolygonDictionaryUtils.h\nindex 5268cb93f780..0acf0d23e5ee 100644\n--- a/src/Dictionaries/PolygonDictionaryUtils.h\n+++ b/src/Dictionaries/PolygonDictionaryUtils.h\n@@ -83,7 +83,7 @@ class SlabsPolygonIndex\n     /** Auxiliary function for adding ring to the index */\n     void indexAddRing(const Ring & ring, size_t polygon_id);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /** Sorted distinct coordinates of all vertices */\n     std::vector<Coord> sorted_x;\ndiff --git a/src/Dictionaries/PostgreSQLDictionarySource.cpp b/src/Dictionaries/PostgreSQLDictionarySource.cpp\nindex 8ec783083927..c7401386e400 100644\n--- a/src/Dictionaries/PostgreSQLDictionarySource.cpp\n+++ b/src/Dictionaries/PostgreSQLDictionarySource.cpp\n@@ -57,7 +57,7 @@ PostgreSQLDictionarySource::PostgreSQLDictionarySource(\n     , configuration(configuration_)\n     , pool(std::move(pool_))\n     , sample_block(sample_block_)\n-    , log(&Poco::Logger::get(\"PostgreSQLDictionarySource\"))\n+    , log(getLogger(\"PostgreSQLDictionarySource\"))\n     , query_builder(makeExternalQueryBuilder(dict_struct, configuration.schema, configuration.table, configuration.query, configuration.where))\n     , load_all_query(query_builder.composeLoadAllQuery())\n {\n@@ -70,7 +70,7 @@ PostgreSQLDictionarySource::PostgreSQLDictionarySource(const PostgreSQLDictionar\n     , configuration(other.configuration)\n     , pool(other.pool)\n     , sample_block(other.sample_block)\n-    , log(&Poco::Logger::get(\"PostgreSQLDictionarySource\"))\n+    , log(getLogger(\"PostgreSQLDictionarySource\"))\n     , query_builder(makeExternalQueryBuilder(dict_struct, configuration.schema, configuration.table, configuration.query, configuration.where))\n     , load_all_query(query_builder.composeLoadAllQuery())\n     , update_time(other.update_time)\ndiff --git a/src/Dictionaries/PostgreSQLDictionarySource.h b/src/Dictionaries/PostgreSQLDictionarySource.h\nindex 1305333458b3..3070184ab3d6 100644\n--- a/src/Dictionaries/PostgreSQLDictionarySource.h\n+++ b/src/Dictionaries/PostgreSQLDictionarySource.h\n@@ -61,7 +61,7 @@ class PostgreSQLDictionarySource final : public IDictionarySource\n     const Configuration configuration;\n     postgres::PoolWithFailoverPtr pool;\n     Block sample_block;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     ExternalQueryBuilder query_builder;\n     const std::string load_all_query;\n     std::chrono::time_point<std::chrono::system_clock> update_time;\ndiff --git a/src/Dictionaries/RegExpTreeDictionary.cpp b/src/Dictionaries/RegExpTreeDictionary.cpp\nindex bbd101d55aaf..a95327372768 100644\n--- a/src/Dictionaries/RegExpTreeDictionary.cpp\n+++ b/src/Dictionaries/RegExpTreeDictionary.cpp\n@@ -139,7 +139,7 @@ struct RegExpTreeDictionary::RegexTreeNode\n     std::unordered_map<String, AttributeValue> attributes;\n };\n \n-std::vector<StringPiece> createStringPieces(const String & value, int num_captures, const String & regex, Poco::Logger * logger)\n+std::vector<StringPiece> createStringPieces(const String & value, int num_captures, const String & regex, LoggerPtr logger)\n {\n     std::vector<StringPiece> result;\n     String literal;\n@@ -401,7 +401,7 @@ RegExpTreeDictionary::RegExpTreeDictionary(\n       use_vectorscan(use_vectorscan_),\n       flag_case_insensitive(flag_case_insensitive_),\n       flag_dotall(flag_dotall_),\n-      logger(&Poco::Logger::get(\"RegExpTreeDictionary\"))\n+      logger(getLogger(\"RegExpTreeDictionary\"))\n {\n     if (auto * ch_source = typeid_cast<ClickHouseDictionarySource *>(source_ptr.get()))\n     {\ndiff --git a/src/Dictionaries/RegExpTreeDictionary.h b/src/Dictionaries/RegExpTreeDictionary.h\nindex 6597584ed45c..78b7f441d349 100644\n--- a/src/Dictionaries/RegExpTreeDictionary.h\n+++ b/src/Dictionaries/RegExpTreeDictionary.h\n@@ -208,7 +208,7 @@ class RegExpTreeDictionary : public IDictionary\n     MultiRegexps::DataBasePtr origin_db;\n     #endif\n \n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n };\n \n }\ndiff --git a/src/Dictionaries/XDBCDictionarySource.cpp b/src/Dictionaries/XDBCDictionarySource.cpp\nindex 080f7db96be7..70fe889a8ead 100644\n--- a/src/Dictionaries/XDBCDictionarySource.cpp\n+++ b/src/Dictionaries/XDBCDictionarySource.cpp\n@@ -67,7 +67,7 @@ XDBCDictionarySource::XDBCDictionarySource(\n     ContextPtr context_,\n     const BridgeHelperPtr bridge_)\n     : WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(bridge_->getName() + \"DictionarySource\"))\n+    , log(getLogger(bridge_->getName() + \"DictionarySource\"))\n     , update_time(std::chrono::system_clock::from_time_t(0))\n     , dict_struct(dict_struct_)\n     , configuration(configuration_)\n@@ -86,7 +86,7 @@ XDBCDictionarySource::XDBCDictionarySource(\n /// copy-constructor is provided in order to support cloneability\n XDBCDictionarySource::XDBCDictionarySource(const XDBCDictionarySource & other)\n     : WithContext(other.getContext())\n-    , log(&Poco::Logger::get(other.bridge_helper->getName() + \"DictionarySource\"))\n+    , log(getLogger(other.bridge_helper->getName() + \"DictionarySource\"))\n     , update_time(other.update_time)\n     , dict_struct(other.dict_struct)\n     , configuration(other.configuration)\ndiff --git a/src/Dictionaries/XDBCDictionarySource.h b/src/Dictionaries/XDBCDictionarySource.h\nindex 8ca2e172aa6f..6011563c5223 100644\n--- a/src/Dictionaries/XDBCDictionarySource.h\n+++ b/src/Dictionaries/XDBCDictionarySource.h\n@@ -76,7 +76,7 @@ class XDBCDictionarySource final : public IDictionarySource, WithContext\n \n     QueryPipeline loadFromQuery(const Poco::URI & url, const Block & required_sample_block, const std::string & query) const;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::chrono::time_point<std::chrono::system_clock> update_time;\n     const DictionaryStructure dict_struct;\ndiff --git a/src/Dictionaries/YAMLRegExpTreeDictionarySource.cpp b/src/Dictionaries/YAMLRegExpTreeDictionarySource.cpp\nindex 118d0f6a0f3c..f1591943a122 100644\n--- a/src/Dictionaries/YAMLRegExpTreeDictionarySource.cpp\n+++ b/src/Dictionaries/YAMLRegExpTreeDictionarySource.cpp\n@@ -284,7 +284,7 @@ Block parseYAMLAsRegExpTree(const YAML::Node & node, const String & key_name, co\n \n YAMLRegExpTreeDictionarySource::YAMLRegExpTreeDictionarySource(\n     const String & filepath_, const DictionaryStructure & dict_struct, ContextPtr context_, bool created_from_ddl)\n-    : filepath(filepath_), structure(dict_struct), context(context_), logger(&Poco::Logger::get(kYAMLRegExpTreeDictionarySource))\n+    : filepath(filepath_), structure(dict_struct), context(context_), logger(getLogger(kYAMLRegExpTreeDictionarySource))\n {\n     key_name = (*structure.key)[0].name;\n \ndiff --git a/src/Dictionaries/YAMLRegExpTreeDictionarySource.h b/src/Dictionaries/YAMLRegExpTreeDictionarySource.h\nindex f5dd9b7d186d..041cbca81c65 100644\n--- a/src/Dictionaries/YAMLRegExpTreeDictionarySource.h\n+++ b/src/Dictionaries/YAMLRegExpTreeDictionarySource.h\n@@ -64,7 +64,7 @@ class YAMLRegExpTreeDictionarySource : public IDictionarySource\n \n     ContextPtr context;\n \n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n     Poco::Timestamp last_modification;\n \n     Poco::Timestamp getLastModification() const;\ndiff --git a/src/Dictionaries/registerHashedDictionary.cpp b/src/Dictionaries/registerHashedDictionary.cpp\nindex f511cad04b0a..303ca8c577e4 100644\n--- a/src/Dictionaries/registerHashedDictionary.cpp\n+++ b/src/Dictionaries/registerHashedDictionary.cpp\n@@ -50,7 +50,7 @@ void registerDictionaryHashed(DictionaryFactory & factory)\n         const std::string dictionary_layout_prefix = \".layout.\" + dictionary_layout_name;\n         const bool preallocate = config.getBool(config_prefix + dictionary_layout_prefix + \".preallocate\", false);\n         if (preallocate)\n-            LOG_WARNING(&Poco::Logger::get(\"HashedDictionary\"), \"'prellocate' attribute is obsolete, consider looking at 'shards'\");\n+            LOG_WARNING(getLogger(\"HashedDictionary\"), \"'prellocate' attribute is obsolete, consider looking at 'shards'\");\n \n         Int64 shards = config.getInt(config_prefix + dictionary_layout_prefix + \".shards\", 1);\n         if (shards <= 0 || shards > 128)\ndiff --git a/src/Disks/DiskLocal.cpp b/src/Disks/DiskLocal.cpp\nindex 07c2beb002d9..f5d67d37b07a 100644\n--- a/src/Disks/DiskLocal.cpp\n+++ b/src/Disks/DiskLocal.cpp\n@@ -106,7 +106,7 @@ class DiskLocalReservation : public IReservation\n             if (disk->reserved_bytes < size)\n             {\n                 disk->reserved_bytes = 0;\n-                LOG_ERROR(&Poco::Logger::get(\"DiskLocal\"), \"Unbalanced reservations size for disk '{}'.\", disk->getName());\n+                LOG_ERROR(getLogger(\"DiskLocal\"), \"Unbalanced reservations size for disk '{}'.\", disk->getName());\n             }\n             else\n             {\n@@ -114,7 +114,7 @@ class DiskLocalReservation : public IReservation\n             }\n \n             if (disk->reservation_count == 0)\n-                LOG_ERROR(&Poco::Logger::get(\"DiskLocal\"), \"Unbalanced reservation count for disk '{}'.\", disk->getName());\n+                LOG_ERROR(getLogger(\"DiskLocal\"), \"Unbalanced reservation count for disk '{}'.\", disk->getName());\n             else\n                 --disk->reservation_count;\n         }\n@@ -475,7 +475,7 @@ DiskLocal::DiskLocal(const String & name_, const String & path_, UInt64 keep_fre\n     : IDisk(name_, config, config_prefix)\n     , disk_path(path_)\n     , keep_free_space_bytes(keep_free_space_bytes_)\n-    , logger(&Poco::Logger::get(\"DiskLocal\"))\n+    , logger(getLogger(\"DiskLocal\"))\n     , data_source_description(getLocalDataSourceDescription(disk_path))\n {\n }\n@@ -494,7 +494,7 @@ DiskLocal::DiskLocal(const String & name_, const String & path_)\n     : IDisk(name_)\n     , disk_path(path_)\n     , keep_free_space_bytes(0)\n-    , logger(&Poco::Logger::get(\"DiskLocal\"))\n+    , logger(getLogger(\"DiskLocal\"))\n     , data_source_description(getLocalDataSourceDescription(disk_path))\n {\n }\ndiff --git a/src/Disks/DiskLocal.h b/src/Disks/DiskLocal.h\nindex affce5a847e3..b9703019c191 100644\n--- a/src/Disks/DiskLocal.h\n+++ b/src/Disks/DiskLocal.h\n@@ -153,7 +153,7 @@ class DiskLocal : public IDisk\n     const String disk_path;\n     const String disk_checker_path = \".disk_checker_file\";\n     std::atomic<UInt64> keep_free_space_bytes;\n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n     DataSourceDescription data_source_description;\n \n     UInt64 reserved_bytes = 0;\ndiff --git a/src/Disks/DiskLocalCheckThread.cpp b/src/Disks/DiskLocalCheckThread.cpp\nindex 87fcc0d1cf5a..e95c614336bf 100644\n--- a/src/Disks/DiskLocalCheckThread.cpp\n+++ b/src/Disks/DiskLocalCheckThread.cpp\n@@ -13,7 +13,7 @@ DiskLocalCheckThread::DiskLocalCheckThread(DiskLocal * disk_, ContextPtr context\n     : WithContext(context_)\n     , disk(std::move(disk_))\n     , check_period_ms(local_disk_check_period_ms)\n-    , log(&Poco::Logger::get(fmt::format(\"DiskLocalCheckThread({})\", disk->getName())))\n+    , log(getLogger(fmt::format(\"DiskLocalCheckThread({})\", disk->getName())))\n {\n     task = getContext()->getSchedulePool().createTask(log->name(), [this] { run(); });\n }\ndiff --git a/src/Disks/DiskLocalCheckThread.h b/src/Disks/DiskLocalCheckThread.h\nindex eb688d599ca0..046b75531360 100644\n--- a/src/Disks/DiskLocalCheckThread.h\n+++ b/src/Disks/DiskLocalCheckThread.h\n@@ -29,7 +29,7 @@ class DiskLocalCheckThread : WithContext\n \n     DiskLocal * disk;\n     size_t check_period_ms;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::atomic<bool> need_stop{false};\n \n     BackgroundSchedulePool::TaskHolder task;\ndiff --git a/src/Disks/DiskSelector.cpp b/src/Disks/DiskSelector.cpp\nindex dad1c7285609..a9260a249ddd 100644\n--- a/src/Disks/DiskSelector.cpp\n+++ b/src/Disks/DiskSelector.cpp\n@@ -124,7 +124,7 @@ DiskSelectorPtr DiskSelector::updateFromConfig(\n         if (num_disks_removed_from_config > 0)\n         {\n             LOG_WARNING(\n-                &Poco::Logger::get(\"DiskSelector\"),\n+                getLogger(\"DiskSelector\"),\n                 \"{} disappeared from configuration, this change will be applied after restart of ClickHouse\",\n                 warning.str());\n         }\ndiff --git a/src/Disks/IDisk.cpp b/src/Disks/IDisk.cpp\nindex 5426f8d09049..066acc250a28 100644\n--- a/src/Disks/IDisk.cpp\n+++ b/src/Disks/IDisk.cpp\n@@ -33,7 +33,7 @@ void IDisk::copyFile( /// NOLINT\n     const std::function<void()> & cancellation_hook\n     )\n {\n-    LOG_DEBUG(&Poco::Logger::get(\"IDisk\"), \"Copying from {} (path: {}) {} to {} (path: {}) {}.\",\n+    LOG_DEBUG(getLogger(\"IDisk\"), \"Copying from {} (path: {}) {} to {} (path: {}) {}.\",\n               getName(), getPath(), from_file_path, to_disk.getName(), to_disk.getPath(), to_file_path);\n \n     auto in = readFile(from_file_path, read_settings);\n@@ -194,7 +194,7 @@ void IDisk::startup(ContextPtr context, bool skip_access_check)\n     {\n         if (isReadOnly())\n         {\n-            LOG_DEBUG(&Poco::Logger::get(\"IDisk\"),\n+            LOG_DEBUG(getLogger(\"IDisk\"),\n                 \"Skip access check for disk {} (read-only disk).\",\n                 getName());\n         }\ndiff --git a/src/Disks/IO/AsynchronousBoundedReadBuffer.cpp b/src/Disks/IO/AsynchronousBoundedReadBuffer.cpp\nindex 1952d8ae253b..8eecd0d99d1c 100644\n--- a/src/Disks/IO/AsynchronousBoundedReadBuffer.cpp\n+++ b/src/Disks/IO/AsynchronousBoundedReadBuffer.cpp\n@@ -55,7 +55,7 @@ AsynchronousBoundedReadBuffer::AsynchronousBoundedReadBuffer(\n     , prefetch_buffer(chooseBufferSizeForRemoteReading(read_settings, impl->getFileSize()))\n     , query_id(CurrentThread::isInitialized() && CurrentThread::get().getQueryContext() != nullptr ? CurrentThread::getQueryId() : \"\")\n     , current_reader_id(getRandomASCIIString(8))\n-    , log(&Poco::Logger::get(\"AsynchronousBoundedReadBuffer\"))\n+    , log(getLogger(\"AsynchronousBoundedReadBuffer\"))\n     , async_read_counters(async_read_counters_)\n     , prefetches_log(prefetches_log_)\n {\ndiff --git a/src/Disks/IO/AsynchronousBoundedReadBuffer.h b/src/Disks/IO/AsynchronousBoundedReadBuffer.h\nindex c43b08ce2b00..e5030f37b1d9 100644\n--- a/src/Disks/IO/AsynchronousBoundedReadBuffer.h\n+++ b/src/Disks/IO/AsynchronousBoundedReadBuffer.h\n@@ -67,7 +67,7 @@ class AsynchronousBoundedReadBuffer : public ReadBufferFromFileBase\n     const std::string query_id;\n     const std::string current_reader_id;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     AsyncReadCountersPtr async_read_counters;\n     FilesystemReadPrefetchesLogPtr prefetches_log;\ndiff --git a/src/Disks/IO/CachedOnDiskReadBufferFromFile.cpp b/src/Disks/IO/CachedOnDiskReadBufferFromFile.cpp\nindex 2b169bd9c7db..7ce3d58dcd8f 100644\n--- a/src/Disks/IO/CachedOnDiskReadBufferFromFile.cpp\n+++ b/src/Disks/IO/CachedOnDiskReadBufferFromFile.cpp\n@@ -60,9 +60,9 @@ CachedOnDiskReadBufferFromFile::CachedOnDiskReadBufferFromFile(\n     std::shared_ptr<FilesystemCacheLog> cache_log_)\n     : ReadBufferFromFileBase(use_external_buffer_ ? 0 : settings_.remote_fs_buffer_size, nullptr, 0, file_size_)\n #ifdef ABORT_ON_LOGICAL_ERROR\n-    , log(&Poco::Logger::get(fmt::format(\"CachedOnDiskReadBufferFromFile({})\", cache_key_)))\n+    , log(getLogger(fmt::format(\"CachedOnDiskReadBufferFromFile({})\", cache_key_)))\n #else\n-    , log(&Poco::Logger::get(\"CachedOnDiskReadBufferFromFile\"))\n+    , log(getLogger(\"CachedOnDiskReadBufferFromFile\"))\n #endif\n     , cache_key(cache_key_)\n     , source_file_path(source_file_path_)\ndiff --git a/src/Disks/IO/CachedOnDiskReadBufferFromFile.h b/src/Disks/IO/CachedOnDiskReadBufferFromFile.h\nindex e2ea5dce3c05..74fb6220af23 100644\n--- a/src/Disks/IO/CachedOnDiskReadBufferFromFile.h\n+++ b/src/Disks/IO/CachedOnDiskReadBufferFromFile.h\n@@ -105,7 +105,7 @@ class CachedOnDiskReadBufferFromFile : public ReadBufferFromFileBase\n \n     bool nextFileSegmentsBatch();\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     FileCacheKey cache_key;\n     String source_file_path;\n \ndiff --git a/src/Disks/IO/CachedOnDiskWriteBufferFromFile.cpp b/src/Disks/IO/CachedOnDiskWriteBufferFromFile.cpp\nindex a97f25e80e5a..25c8ab1c4aea 100644\n--- a/src/Disks/IO/CachedOnDiskWriteBufferFromFile.cpp\n+++ b/src/Disks/IO/CachedOnDiskWriteBufferFromFile.cpp\n@@ -31,7 +31,7 @@ FileSegmentRangeWriter::FileSegmentRangeWriter(\n     const String & source_path_)\n     : cache(cache_)\n     , key(key_)\n-    , log(&Poco::Logger::get(\"FileSegmentRangeWriter\"))\n+    , log(getLogger(\"FileSegmentRangeWriter\"))\n     , cache_log(cache_log_)\n     , query_id(query_id_)\n     , source_path(source_path_)\n@@ -205,7 +205,7 @@ CachedOnDiskWriteBufferFromFile::CachedOnDiskWriteBufferFromFile(\n     const FileCacheUserInfo & user_,\n     std::shared_ptr<FilesystemCacheLog> cache_log_)\n     : WriteBufferFromFileDecorator(std::move(impl_))\n-    , log(&Poco::Logger::get(\"CachedOnDiskWriteBufferFromFile\"))\n+    , log(getLogger(\"CachedOnDiskWriteBufferFromFile\"))\n     , cache(cache_)\n     , source_path(source_path_)\n     , key(key_)\ndiff --git a/src/Disks/IO/CachedOnDiskWriteBufferFromFile.h b/src/Disks/IO/CachedOnDiskWriteBufferFromFile.h\nindex 2a4e78f44c6b..59e0c76ca3d2 100644\n--- a/src/Disks/IO/CachedOnDiskWriteBufferFromFile.h\n+++ b/src/Disks/IO/CachedOnDiskWriteBufferFromFile.h\n@@ -54,7 +54,7 @@ class FileSegmentRangeWriter\n     FileCache * cache;\n     FileSegment::Key key;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::shared_ptr<FilesystemCacheLog> cache_log;\n     String query_id;\n     String source_path;\n@@ -93,7 +93,7 @@ class CachedOnDiskWriteBufferFromFile final : public WriteBufferFromFileDecorato\n private:\n     void cacheData(char * data, size_t size, bool throw_on_error);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     FileCachePtr cache;\n     String source_path;\ndiff --git a/src/Disks/IO/IOUringReader.cpp b/src/Disks/IO/IOUringReader.cpp\nindex 4c9f665093d6..90a4d285ecbe 100644\n--- a/src/Disks/IO/IOUringReader.cpp\n+++ b/src/Disks/IO/IOUringReader.cpp\n@@ -46,7 +46,7 @@ namespace ErrorCodes\n }\n \n IOUringReader::IOUringReader(uint32_t entries_)\n-    : log(&Poco::Logger::get(\"IOUringReader\"))\n+    : log(getLogger(\"IOUringReader\"))\n {\n     struct io_uring_probe * probe = io_uring_get_probe();\n     if (!probe)\ndiff --git a/src/Disks/IO/IOUringReader.h b/src/Disks/IO/IOUringReader.h\nindex b038b3acf7d1..2504dd73a6bc 100644\n--- a/src/Disks/IO/IOUringReader.h\n+++ b/src/Disks/IO/IOUringReader.h\n@@ -73,7 +73,7 @@ class IOUringReader final : public IAsynchronousReader\n         return promise.get_future();\n     }\n \n-    const Poco::Logger * log;\n+    const LoggerPtr log;\n \n public:\n     IOUringReader(uint32_t entries_);\ndiff --git a/src/Disks/IO/ReadBufferFromAzureBlobStorage.h b/src/Disks/IO/ReadBufferFromAzureBlobStorage.h\nindex 4e21f5436536..68058b84a2f6 100644\n--- a/src/Disks/IO/ReadBufferFromAzureBlobStorage.h\n+++ b/src/Disks/IO/ReadBufferFromAzureBlobStorage.h\n@@ -73,7 +73,7 @@ class ReadBufferFromAzureBlobStorage : public ReadBufferFromFileBase\n     char * data_ptr;\n     size_t data_capacity;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"ReadBufferFromAzureBlobStorage\");\n+    LoggerPtr log = getLogger(\"ReadBufferFromAzureBlobStorage\");\n };\n \n }\ndiff --git a/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp b/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp\nindex 63bacaa99e91..923aab5c3431 100644\n--- a/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp\n+++ b/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp\n@@ -56,7 +56,7 @@ ReadBufferFromRemoteFSGather::ReadBufferFromRemoteFSGather(\n     , query_id(CurrentThread::getQueryId())\n     , use_external_buffer(use_external_buffer_)\n     , with_cache(withCache(settings))\n-    , log(&Poco::Logger::get(\"ReadBufferFromRemoteFSGather\"))\n+    , log(getLogger(\"ReadBufferFromRemoteFSGather\"))\n {\n     if (!blobs_to_read.empty())\n         current_object = blobs_to_read.front();\ndiff --git a/src/Disks/IO/ReadBufferFromRemoteFSGather.h b/src/Disks/IO/ReadBufferFromRemoteFSGather.h\nindex c5886dea603c..93ded9fefb33 100644\n--- a/src/Disks/IO/ReadBufferFromRemoteFSGather.h\n+++ b/src/Disks/IO/ReadBufferFromRemoteFSGather.h\n@@ -82,7 +82,7 @@ friend class ReadIndirectBufferFromRemoteFS;\n     size_t current_buf_idx = 0;\n     std::unique_ptr<ReadBufferFromFileBase> current_buf;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n size_t chooseBufferSizeForRemoteReading(const DB::ReadSettings & settings, size_t file_size);\ndiff --git a/src/Disks/IO/ReadBufferFromWebServer.cpp b/src/Disks/IO/ReadBufferFromWebServer.cpp\nindex 7a6028561835..cc8723927383 100644\n--- a/src/Disks/IO/ReadBufferFromWebServer.cpp\n+++ b/src/Disks/IO/ReadBufferFromWebServer.cpp\n@@ -27,7 +27,7 @@ ReadBufferFromWebServer::ReadBufferFromWebServer(\n     bool use_external_buffer_,\n     size_t read_until_position_)\n     : ReadBufferFromFileBase(settings_.remote_fs_buffer_size, nullptr, 0)\n-    , log(&Poco::Logger::get(\"ReadBufferFromWebServer\"))\n+    , log(getLogger(\"ReadBufferFromWebServer\"))\n     , context(context_)\n     , url(url_)\n     , buf_size(settings_.remote_fs_buffer_size)\ndiff --git a/src/Disks/IO/ReadBufferFromWebServer.h b/src/Disks/IO/ReadBufferFromWebServer.h\nindex b4edf16b095e..68ad752bbdb5 100644\n--- a/src/Disks/IO/ReadBufferFromWebServer.h\n+++ b/src/Disks/IO/ReadBufferFromWebServer.h\n@@ -41,7 +41,7 @@ class ReadBufferFromWebServer : public ReadBufferFromFileBase\n private:\n     std::unique_ptr<ReadBuffer> initialize();\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     ContextPtr context;\n \n     const String url;\ndiff --git a/src/Disks/IO/WriteBufferFromAzureBlobStorage.cpp b/src/Disks/IO/WriteBufferFromAzureBlobStorage.cpp\nindex a2e84edf45ff..d281c3dfdc25 100644\n--- a/src/Disks/IO/WriteBufferFromAzureBlobStorage.cpp\n+++ b/src/Disks/IO/WriteBufferFromAzureBlobStorage.cpp\n@@ -27,7 +27,7 @@ WriteBufferFromAzureBlobStorage::WriteBufferFromAzureBlobStorage(\n     size_t buf_size_,\n     const WriteSettings & write_settings_)\n     : WriteBufferFromFileBase(buf_size_, nullptr, 0)\n-    , log(&Poco::Logger::get(\"WriteBufferFromAzureBlobStorage\"))\n+    , log(getLogger(\"WriteBufferFromAzureBlobStorage\"))\n     , max_single_part_upload_size(max_single_part_upload_size_)\n     , blob_path(blob_path_)\n     , write_settings(write_settings_)\ndiff --git a/src/Disks/IO/WriteBufferFromAzureBlobStorage.h b/src/Disks/IO/WriteBufferFromAzureBlobStorage.h\nindex f1be81922e19..5e4f97b0a082 100644\n--- a/src/Disks/IO/WriteBufferFromAzureBlobStorage.h\n+++ b/src/Disks/IO/WriteBufferFromAzureBlobStorage.h\n@@ -45,7 +45,7 @@ class WriteBufferFromAzureBlobStorage : public WriteBufferFromFileBase\n     void execWithRetry(std::function<void()> func, size_t num_tries, size_t cost = 0);\n     void uploadBlock(const char * data, size_t size);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     const size_t max_single_part_upload_size;\n     const std::string blob_path;\ndiff --git a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp\nindex 93fed1e61bf9..05bf2281842f 100644\n--- a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp\n+++ b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp\n@@ -96,7 +96,7 @@ AzureObjectStorage::AzureObjectStorage(\n     : name(name_)\n     , client(std::move(client_))\n     , settings(std::move(settings_))\n-    , log(&Poco::Logger::get(\"AzureObjectStorage\"))\n+    , log(getLogger(\"AzureObjectStorage\"))\n {\n }\n \ndiff --git a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h\nindex 85213a3c24f9..a05eb824b91b 100644\n--- a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h\n+++ b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h\n@@ -134,7 +134,7 @@ class AzureObjectStorage : public IObjectStorage\n     MultiVersion<Azure::Storage::Blobs::BlobContainerClient> client;\n     MultiVersion<AzureObjectStorageSettings> settings;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Disks/ObjectStorages/Cached/CachedObjectStorage.cpp b/src/Disks/ObjectStorages/Cached/CachedObjectStorage.cpp\nindex d4aba197ae41..1444f4c9c76a 100644\n--- a/src/Disks/ObjectStorages/Cached/CachedObjectStorage.cpp\n+++ b/src/Disks/ObjectStorages/Cached/CachedObjectStorage.cpp\n@@ -24,7 +24,7 @@ CachedObjectStorage::CachedObjectStorage(\n     , cache(cache_)\n     , cache_settings(cache_settings_)\n     , cache_config_name(cache_config_name_)\n-    , log(&Poco::Logger::get(getName()))\n+    , log(getLogger(getName()))\n {\n     cache->initialize();\n }\ndiff --git a/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h b/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h\nindex 028b0f7c5104..7b231b688051 100644\n--- a/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h\n+++ b/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h\n@@ -129,7 +129,7 @@ class CachedObjectStorage final : public IObjectStorage\n     FileCachePtr cache;\n     FileCacheSettings cache_settings;\n     std::string cache_config_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Disks/ObjectStorages/Cached/registerDiskCache.cpp b/src/Disks/ObjectStorages/Cached/registerDiskCache.cpp\nindex 5d788f2641a8..6e0453f5f021 100644\n--- a/src/Disks/ObjectStorages/Cached/registerDiskCache.cpp\n+++ b/src/Disks/ObjectStorages/Cached/registerDiskCache.cpp\n@@ -114,7 +114,7 @@ void registerDiskCache(DiskFactory & factory, bool /* global_skip_access_check *\n         disk_object_storage->wrapWithCache(cache, file_cache_settings, name);\n \n         LOG_INFO(\n-            &Poco::Logger::get(\"DiskCache\"),\n+            getLogger(\"DiskCache\"),\n             \"Registered cached disk (`{}`) with structure: {}\",\n             name, assert_cast<DiskObjectStorage *>(disk_object_storage.get())->getStructure());\n \ndiff --git a/src/Disks/ObjectStorages/DiskObjectStorage.cpp b/src/Disks/ObjectStorages/DiskObjectStorage.cpp\nindex 3c39fa2a8ff7..9c4132f433ff 100644\n--- a/src/Disks/ObjectStorages/DiskObjectStorage.cpp\n+++ b/src/Disks/ObjectStorages/DiskObjectStorage.cpp\n@@ -67,7 +67,7 @@ DiskObjectStorage::DiskObjectStorage(\n     const String & config_prefix)\n     : IDisk(name_, config, config_prefix)\n     , object_key_prefix(object_key_prefix_)\n-    , log (&Poco::Logger::get(\"DiskObjectStorage(\" + name + \")\"))\n+    , log(getLogger(\"DiskObjectStorage(\" + name + \")\"))\n     , metadata_storage(std::move(metadata_storage_))\n     , object_storage(std::move(object_storage_))\n     , send_metadata(config.getBool(config_prefix + \".send_metadata\", false))\ndiff --git a/src/Disks/ObjectStorages/DiskObjectStorage.h b/src/Disks/ObjectStorages/DiskObjectStorage.h\nindex 983af483b8a2..e15765097133 100644\n--- a/src/Disks/ObjectStorages/DiskObjectStorage.h\n+++ b/src/Disks/ObjectStorages/DiskObjectStorage.h\n@@ -219,7 +219,7 @@ friend class DiskObjectStorageRemoteMetadataRestoreHelper;\n     String getWriteResourceName() const;\n \n     const String object_key_prefix;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     MetadataStoragePtr metadata_storage;\n     ObjectStoragePtr object_storage;\ndiff --git a/src/Disks/ObjectStorages/DiskObjectStorageMetadata.cpp b/src/Disks/ObjectStorages/DiskObjectStorageMetadata.cpp\nindex 881f7a46c168..19b8b51384fb 100644\n--- a/src/Disks/ObjectStorages/DiskObjectStorageMetadata.cpp\n+++ b/src/Disks/ObjectStorages/DiskObjectStorageMetadata.cpp\n@@ -104,7 +104,7 @@ void DiskObjectStorageMetadata::serialize(WriteBuffer & buf, bool sync) const\n \n     if (version == VERSION_FULL_OBJECT_KEY && !storage_metadata_write_full_object_key)\n     {\n-        Poco::Logger * logger = &Poco::Logger::get(\"DiskObjectStorageMetadata\");\n+        LoggerPtr logger = getLogger(\"DiskObjectStorageMetadata\");\n         LOG_WARNING(\n             logger,\n             \"Metadata file {} is written with VERSION_FULL_OBJECT_KEY version\"\n@@ -192,7 +192,7 @@ void DiskObjectStorageMetadata::addObject(ObjectStorageKey key, size_t size)\n         bool storage_metadata_write_full_object_key = getWriteFullObjectKeySetting();\n         if (!storage_metadata_write_full_object_key)\n         {\n-            Poco::Logger * logger = &Poco::Logger::get(\"DiskObjectStorageMetadata\");\n+            LoggerPtr logger = getLogger(\"DiskObjectStorageMetadata\");\n             LOG_WARNING(\n                 logger,\n                 \"Metadata file {} has at least one key {} without fixed common key prefix.\"\ndiff --git a/src/Disks/ObjectStorages/DiskObjectStorageTransaction.cpp b/src/Disks/ObjectStorages/DiskObjectStorageTransaction.cpp\nindex f39e16828c53..de7a71e8dc14 100644\n--- a/src/Disks/ObjectStorages/DiskObjectStorageTransaction.cpp\n+++ b/src/Disks/ObjectStorages/DiskObjectStorageTransaction.cpp\n@@ -242,7 +242,7 @@ struct RemoveManyObjectStorageOperation final : public IDiskObjectStorageOperati\n                     || e.code() == ErrorCodes::CANNOT_OPEN_FILE)\n                 {\n                     LOG_DEBUG(\n-                        &Poco::Logger::get(\"RemoveManyObjectStorageOperation\"),\n+                        getLogger(\"RemoveManyObjectStorageOperation\"),\n                         \"Can't read metadata because of an exception. Just remove it from the filesystem. Path: {}, exception: {}\",\n                         metadata_storage.getPath() + path,\n                         e.message());\n@@ -276,7 +276,7 @@ struct RemoveManyObjectStorageOperation final : public IDiskObjectStorageOperati\n         if (!keep_all_batch_data)\n         {\n             LOG_DEBUG(\n-                &Poco::Logger::get(\"RemoveManyObjectStorageOperation\"),\n+                getLogger(\"RemoveManyObjectStorageOperation\"),\n                 \"metadata and objects were removed for [{}], \"\n                 \"only metadata were removed for [{}].\",\n                 boost::algorithm::join(paths_removed_with_objects, \", \"),\n@@ -345,7 +345,7 @@ struct RemoveRecursiveObjectStorageOperation final : public IDiskObjectStorageOp\n                     || e.code() == ErrorCodes::CANNOT_PARSE_INPUT_ASSERTION_FAILED)\n                 {\n                     LOG_DEBUG(\n-                        &Poco::Logger::get(\"RemoveRecursiveObjectStorageOperation\"),\n+                        getLogger(\"RemoveRecursiveObjectStorageOperation\"),\n                         \"Can't read metadata because of an exception. Just remove it from the filesystem. Path: {}, exception: {}\",\n                         metadata_storage.getPath() + path_to_remove,\n                         e.message());\n@@ -399,7 +399,7 @@ struct RemoveRecursiveObjectStorageOperation final : public IDiskObjectStorageOp\n             object_storage.removeObjectsIfExist(remove_from_remote);\n \n             LOG_DEBUG(\n-                &Poco::Logger::get(\"RemoveRecursiveObjectStorageOperation\"),\n+                getLogger(\"RemoveRecursiveObjectStorageOperation\"),\n                 \"Recursively remove path {}: \"\n                 \"metadata and objects were removed for [{}], \"\n                 \"only metadata were removed for [{}].\",\n@@ -905,7 +905,7 @@ void DiskObjectStorageTransaction::commit()\n         catch (...)\n         {\n             tryLogCurrentException(\n-                &Poco::Logger::get(\"DiskObjectStorageTransaction\"),\n+                getLogger(\"DiskObjectStorageTransaction\"),\n                 fmt::format(\"An error occurred while executing transaction's operation #{} ({})\", i, operations_to_execute[i]->getInfoForLog()));\n \n             for (int64_t j = i; j >= 0; --j)\n@@ -917,7 +917,7 @@ void DiskObjectStorageTransaction::commit()\n                 catch (...)\n                 {\n                     tryLogCurrentException(\n-                        &Poco::Logger::get(\"DiskObjectStorageTransaction\"),\n+                        getLogger(\"DiskObjectStorageTransaction\"),\n                         fmt::format(\"An error occurred while undoing transaction's operation #{}\", i));\n \n                     throw;\ndiff --git a/src/Disks/ObjectStorages/Local/LocalObjectStorage.cpp b/src/Disks/ObjectStorages/Local/LocalObjectStorage.cpp\nindex b3c1c3202a52..02700b358e01 100644\n--- a/src/Disks/ObjectStorages/Local/LocalObjectStorage.cpp\n+++ b/src/Disks/ObjectStorages/Local/LocalObjectStorage.cpp\n@@ -25,7 +25,7 @@ namespace ErrorCodes\n \n LocalObjectStorage::LocalObjectStorage(String key_prefix_)\n     : key_prefix(std::move(key_prefix_))\n-    , log(&Poco::Logger::get(\"LocalObjectStorage\"))\n+    , log(getLogger(\"LocalObjectStorage\"))\n {\n     if (auto block_device_id = tryGetBlockDeviceId(\"/\"); block_device_id.has_value())\n         description = *block_device_id;\ndiff --git a/src/Disks/ObjectStorages/Local/LocalObjectStorage.h b/src/Disks/ObjectStorages/Local/LocalObjectStorage.h\nindex 522e73b415d3..ed5f8c1f537f 100644\n--- a/src/Disks/ObjectStorages/Local/LocalObjectStorage.h\n+++ b/src/Disks/ObjectStorages/Local/LocalObjectStorage.h\n@@ -90,7 +90,7 @@ class LocalObjectStorage : public IObjectStorage\n \n private:\n     String key_prefix;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string description;\n };\n \ndiff --git a/src/Disks/ObjectStorages/S3/S3ObjectStorage.cpp b/src/Disks/ObjectStorages/S3/S3ObjectStorage.cpp\nindex 499435543845..4cc49288af63 100644\n--- a/src/Disks/ObjectStorages/S3/S3ObjectStorage.cpp\n+++ b/src/Disks/ObjectStorages/S3/S3ObjectStorage.cpp\n@@ -482,7 +482,7 @@ void S3ObjectStorage::copyObjectToAnotherObjectStorage( // NOLINT\n             /// If authentication/permissions error occurs then fallthrough to copy with buffer.\n             if (exc.getS3ErrorCode() != Aws::S3::S3Errors::ACCESS_DENIED)\n                 throw;\n-            LOG_WARNING(&Poco::Logger::get(\"S3ObjectStorage\"),\n+            LOG_WARNING(getLogger(\"S3ObjectStorage\"),\n                 \"S3-server-side copy object from the disk {} to the disk {} can not be performed: {}\\n\",\n                 getName(), dest_s3->getName(), exc.what());\n         }\ndiff --git a/src/Disks/ObjectStorages/S3/S3ObjectStorage.h b/src/Disks/ObjectStorages/S3/S3ObjectStorage.h\nindex 820d4977f98f..ab0fa5bed689 100644\n--- a/src/Disks/ObjectStorages/S3/S3ObjectStorage.h\n+++ b/src/Disks/ObjectStorages/S3/S3ObjectStorage.h\n@@ -59,7 +59,7 @@ class S3ObjectStorage : public IObjectStorage\n         , client(std::move(client_))\n         , s3_settings(std::move(s3_settings_))\n         , s3_capabilities(s3_capabilities_)\n-        , log(&Poco::Logger::get(logger_name))\n+        , log(getLogger(logger_name))\n     {\n     }\n \n@@ -179,7 +179,7 @@ class S3ObjectStorage : public IObjectStorage\n     MultiVersion<S3ObjectStorageSettings> s3_settings;\n     S3Capabilities s3_capabilities;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n /// Do not encode keys, store as-is, and do not require separate disk for metadata.\ndiff --git a/src/Disks/ObjectStorages/Web/WebObjectStorage.cpp b/src/Disks/ObjectStorages/Web/WebObjectStorage.cpp\nindex ff4216a83da2..0223c24973e6 100644\n--- a/src/Disks/ObjectStorages/Web/WebObjectStorage.cpp\n+++ b/src/Disks/ObjectStorages/Web/WebObjectStorage.cpp\n@@ -119,7 +119,7 @@ WebObjectStorage::WebObjectStorage(\n     ContextPtr context_)\n     : WithContext(context_->getGlobalContext())\n     , url(url_)\n-    , log(&Poco::Logger::get(\"WebObjectStorage\"))\n+    , log(getLogger(\"WebObjectStorage\"))\n {\n }\n \n@@ -130,7 +130,7 @@ bool WebObjectStorage::exists(const StoredObject & object) const\n \n bool WebObjectStorage::exists(const std::string & path) const\n {\n-    LOG_TRACE(&Poco::Logger::get(\"DiskWeb\"), \"Checking existence of path: {}\", path);\n+    LOG_TRACE(getLogger(\"DiskWeb\"), \"Checking existence of path: {}\", path);\n     return tryGetFileInfo(path) != nullptr;\n }\n \ndiff --git a/src/Disks/ObjectStorages/Web/WebObjectStorage.h b/src/Disks/ObjectStorages/Web/WebObjectStorage.h\nindex 9688873f0c48..a285742c66d8 100644\n--- a/src/Disks/ObjectStorages/Web/WebObjectStorage.h\n+++ b/src/Disks/ObjectStorages/Web/WebObjectStorage.h\n@@ -158,7 +158,7 @@ class WebObjectStorage : public IObjectStorage, WithContext\n     loadFiles(const String & path, const std::unique_lock<std::shared_mutex> &) const;\n \n     const String url;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     size_t min_bytes_for_seek;\n };\n \ndiff --git a/src/Disks/StoragePolicy.cpp b/src/Disks/StoragePolicy.cpp\nindex 6cf22cbaa1b9..13bd74ceaebc 100644\n--- a/src/Disks/StoragePolicy.cpp\n+++ b/src/Disks/StoragePolicy.cpp\n@@ -41,7 +41,7 @@ StoragePolicy::StoragePolicy(\n     const String & config_prefix,\n     DiskSelectorPtr disks)\n     : name(std::move(name_))\n-    , log(&Poco::Logger::get(\"StoragePolicy (\" + name + \")\"))\n+    , log(getLogger(\"StoragePolicy (\" + name + \")\"))\n {\n     Poco::Util::AbstractConfiguration::Keys keys;\n     String volumes_prefix = config_prefix + \".volumes\";\n@@ -96,7 +96,7 @@ StoragePolicy::StoragePolicy(String name_, Volumes volumes_, double move_factor_\n     : volumes(std::move(volumes_))\n     , name(std::move(name_))\n     , move_factor(move_factor_)\n-    , log(&Poco::Logger::get(\"StoragePolicy (\" + name + \")\"))\n+    , log(getLogger(\"StoragePolicy (\" + name + \")\"))\n {\n     if (volumes.empty())\n         throw Exception(ErrorCodes::NO_ELEMENTS_IN_CONFIG, \"Storage policy {} must contain at least one Volume.\", backQuote(name));\n@@ -418,7 +418,7 @@ StoragePolicySelector::StoragePolicySelector(\n          */\n \n         policies.emplace(name, std::make_shared<StoragePolicy>(name, config, config_prefix + \".\" + name, disks));\n-        LOG_INFO(&Poco::Logger::get(\"StoragePolicySelector\"), \"Storage policy {} loaded\", backQuote(name));\n+        LOG_INFO(getLogger(\"StoragePolicySelector\"), \"Storage policy {} loaded\", backQuote(name));\n     }\n \n     /// Add default policy if it isn't explicitly specified.\ndiff --git a/src/Disks/StoragePolicy.h b/src/Disks/StoragePolicy.h\nindex d210d8c1e2f6..501e033abc38 100644\n--- a/src/Disks/StoragePolicy.h\n+++ b/src/Disks/StoragePolicy.h\n@@ -105,7 +105,7 @@ class StoragePolicy : public IStoragePolicy\n \n     void buildVolumeIndices();\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \ndiff --git a/src/Disks/TemporaryFileOnDisk.cpp b/src/Disks/TemporaryFileOnDisk.cpp\nindex 06d7da4af588..92219a7f25f2 100644\n--- a/src/Disks/TemporaryFileOnDisk.cpp\n+++ b/src/Disks/TemporaryFileOnDisk.cpp\n@@ -59,7 +59,7 @@ TemporaryFileOnDisk::~TemporaryFileOnDisk()\n \n         if (!disk->exists(relative_path))\n         {\n-            LOG_WARNING(&Poco::Logger::get(\"TemporaryFileOnDisk\"), \"Temporary path '{}' does not exist in '{}'\", relative_path, disk->getPath());\n+            LOG_WARNING(getLogger(\"TemporaryFileOnDisk\"), \"Temporary path '{}' does not exist in '{}'\", relative_path, disk->getPath());\n             return;\n         }\n \ndiff --git a/src/Disks/VolumeJBOD.cpp b/src/Disks/VolumeJBOD.cpp\nindex 682a167bf5f0..e437684b8021 100644\n--- a/src/Disks/VolumeJBOD.cpp\n+++ b/src/Disks/VolumeJBOD.cpp\n@@ -21,7 +21,7 @@ VolumeJBOD::VolumeJBOD(\n     : IVolume(name_, config, config_prefix, disk_selector)\n     , disks_by_size(disks.begin(), disks.end())\n {\n-    Poco::Logger * logger = &Poco::Logger::get(\"StorageConfiguration\");\n+    LoggerPtr logger = getLogger(\"StorageConfiguration\");\n \n     auto has_max_bytes = config.has(config_prefix + \".max_data_part_size_bytes\");\n     auto has_max_ratio = config.has(config_prefix + \".max_data_part_size_ratio\");\ndiff --git a/src/Disks/getOrCreateDiskFromAST.cpp b/src/Disks/getOrCreateDiskFromAST.cpp\nindex ab2fb5e7f8b4..7b2762613b6a 100644\n--- a/src/Disks/getOrCreateDiskFromAST.cpp\n+++ b/src/Disks/getOrCreateDiskFromAST.cpp\n@@ -114,7 +114,7 @@ std::string getOrCreateDiskFromDiskAST(const ASTPtr & disk_function, ContextPtr\n     FlattenDiskConfigurationVisitor{data}.visit(ast);\n \n     auto disk_name = assert_cast<const ASTLiteral &>(*ast).value.get<String>();\n-    LOG_TRACE(&Poco::Logger::get(\"getOrCreateDiskFromDiskAST\"), \"Result disk name: {}\", disk_name);\n+    LOG_TRACE(getLogger(\"getOrCreateDiskFromDiskAST\"), \"Result disk name: {}\", disk_name);\n     return disk_name;\n }\n \ndiff --git a/src/Formats/FormatFactory.cpp b/src/Formats/FormatFactory.cpp\nindex 608f9433d6fa..62cbadec4f47 100644\n--- a/src/Formats/FormatFactory.cpp\n+++ b/src/Formats/FormatFactory.cpp\n@@ -395,7 +395,7 @@ std::unique_ptr<ReadBuffer> FormatFactory::wrapReadBufferIfNeeded(\n         {\n             parallel_read = false;\n             LOG_TRACE(\n-                &Poco::Logger::get(\"FormatFactory\"),\n+                getLogger(\"FormatFactory\"),\n                 \"Failed to setup ParallelReadBuffer because of an exception:\\n{}.\\n\"\n                 \"Falling back to the single-threaded buffer\",\n                 e.displayText());\n@@ -405,7 +405,7 @@ std::unique_ptr<ReadBuffer> FormatFactory::wrapReadBufferIfNeeded(\n     if (parallel_read)\n     {\n         LOG_TRACE(\n-            &Poco::Logger::get(\"FormatFactory\"),\n+            getLogger(\"FormatFactory\"),\n             \"Using ParallelReadBuffer with {} workers with chunks of {} bytes\",\n             max_download_threads,\n             settings.max_download_buffer_size);\ndiff --git a/src/Formats/ProtobufSerializer.cpp b/src/Formats/ProtobufSerializer.cpp\nindex dd37c25719c1..c0d0713e2542 100644\n--- a/src/Formats/ProtobufSerializer.cpp\n+++ b/src/Formats/ProtobufSerializer.cpp\n@@ -3045,7 +3045,7 @@ namespace\n             {\n                 *root_serializer_ptr = message_serializer.get();\n #if 0\n-                LOG_INFO(&Poco::Logger::get(\"ProtobufSerializer\"), \"Serialization tree:\\n{}\", get_root_desc_function(0));\n+                LOG_INFO(getLogger(\"ProtobufSerializer\"), \"Serialization tree:\\n{}\", get_root_desc_function(0));\n #endif\n                 return message_serializer;\n             }\n@@ -3054,7 +3054,7 @@ namespace\n                 auto envelope_serializer = std::make_unique<ProtobufSerializerEnvelope>(std::move(message_serializer), reader_or_writer);\n                 *root_serializer_ptr = envelope_serializer.get();\n #if 0\n-                LOG_INFO(&Poco::Logger::get(\"ProtobufSerializer\"), \"Serialization tree:\\n{}\", get_root_desc_function(0));\n+                LOG_INFO(getLogger(\"ProtobufSerializer\"), \"Serialization tree:\\n{}\", get_root_desc_function(0));\n #endif\n                 return envelope_serializer;\n             }\ndiff --git a/src/Functions/UserDefined/ExternalUserDefinedExecutableFunctionsLoader.cpp b/src/Functions/UserDefined/ExternalUserDefinedExecutableFunctionsLoader.cpp\nindex f3c9f511ef64..db98f88e53ba 100644\n--- a/src/Functions/UserDefined/ExternalUserDefinedExecutableFunctionsLoader.cpp\n+++ b/src/Functions/UserDefined/ExternalUserDefinedExecutableFunctionsLoader.cpp\n@@ -95,7 +95,7 @@ namespace\n }\n \n ExternalUserDefinedExecutableFunctionsLoader::ExternalUserDefinedExecutableFunctionsLoader(ContextPtr global_context_)\n-    : ExternalLoader(\"external user defined function\", &Poco::Logger::get(\"ExternalUserDefinedExecutableFunctionsLoader\"))\n+    : ExternalLoader(\"external user defined function\", getLogger(\"ExternalUserDefinedExecutableFunctionsLoader\"))\n     , WithContext(global_context_)\n {\n     setConfigSettings({\"function\", \"name\", \"database\", \"uuid\"});\ndiff --git a/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.cpp b/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.cpp\nindex 271c464e79ae..34946db7d9eb 100644\n--- a/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.cpp\n+++ b/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.cpp\n@@ -54,7 +54,7 @@ namespace\n UserDefinedSQLObjectsDiskStorage::UserDefinedSQLObjectsDiskStorage(const ContextPtr & global_context_, const String & dir_path_)\n     : global_context(global_context_)\n     , dir_path{makeDirectoryPathCanonical(dir_path_)}\n-    , log{&Poco::Logger::get(\"UserDefinedSQLObjectsLoaderFromDisk\")}\n+    , log{getLogger(\"UserDefinedSQLObjectsLoaderFromDisk\")}\n {\n     createDirectory();\n }\ndiff --git a/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.h b/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.h\nindex f0986dbda727..ae0cbd0c5897 100644\n--- a/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.h\n+++ b/src/Functions/UserDefined/UserDefinedSQLObjectsDiskStorage.h\n@@ -44,7 +44,7 @@ class UserDefinedSQLObjectsDiskStorage : public UserDefinedSQLObjectsStorageBase\n \n     ContextPtr global_context;\n     String dir_path;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::atomic<bool> objects_loaded = false;\n };\n \ndiff --git a/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.cpp b/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.cpp\nindex 6e5a53384377..c43b223ffeb6 100644\n--- a/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.cpp\n+++ b/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.cpp\n@@ -53,7 +53,7 @@ UserDefinedSQLObjectsZooKeeperStorage::UserDefinedSQLObjectsZooKeeperStorage(\n     , zookeeper_getter{[global_context_]() { return global_context_->getZooKeeper(); }}\n     , zookeeper_path{zookeeper_path_}\n     , watch_queue{std::make_shared<ConcurrentBoundedQueue<std::pair<UserDefinedSQLObjectType, String>>>(std::numeric_limits<size_t>::max())}\n-    , log{&Poco::Logger::get(\"UserDefinedSQLObjectsLoaderFromZooKeeper\")}\n+    , log{getLogger(\"UserDefinedSQLObjectsLoaderFromZooKeeper\")}\n {\n     if (zookeeper_path.empty())\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"ZooKeeper path must be non-empty\");\ndiff --git a/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.h b/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.h\nindex 9f41763c59ca..61002be2bfd3 100644\n--- a/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.h\n+++ b/src/Functions/UserDefined/UserDefinedSQLObjectsZooKeeperStorage.h\n@@ -80,7 +80,7 @@ class UserDefinedSQLObjectsZooKeeperStorage : public UserDefinedSQLObjectsStorag\n     using UserDefinedSQLObjectTypeAndName = std::pair<UserDefinedSQLObjectType, String>;\n     std::shared_ptr<ConcurrentBoundedQueue<UserDefinedSQLObjectTypeAndName>> watch_queue;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Functions/logTrace.cpp b/src/Functions/logTrace.cpp\nindex 55f387cbfeb2..923ea9fd70ef 100644\n--- a/src/Functions/logTrace.cpp\n+++ b/src/Functions/logTrace.cpp\n@@ -46,7 +46,7 @@ namespace\n                 throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT, \"First argument for function {} must be Constant string\",\n                     getName());\n \n-            static auto * log = &Poco::Logger::get(\"FunctionLogTrace\");\n+            static auto log = getLogger(\"FunctionLogTrace\");\n             LOG_TRACE(log, fmt::runtime(message));\n \n             return DataTypeUInt8().createColumnConst(input_rows_count, 0);\ndiff --git a/src/IO/Archives/ZipArchiveWriter.cpp b/src/IO/Archives/ZipArchiveWriter.cpp\nindex 785a5005f878..8cb4a2e0bd69 100644\n--- a/src/IO/Archives/ZipArchiveWriter.cpp\n+++ b/src/IO/Archives/ZipArchiveWriter.cpp\n@@ -246,7 +246,7 @@ ZipArchiveWriter::~ZipArchiveWriter()\n         /// However it is suspicious to destroy instance without finalization at the green path.\n         if (!std::uncaught_exceptions() && std::current_exception() == nullptr)\n         {\n-            Poco::Logger * log = &Poco::Logger::get(\"ZipArchiveWriter\");\n+            LoggerPtr log = getLogger(\"ZipArchiveWriter\");\n             LOG_ERROR(log,\n                        \"ZipArchiveWriter is not finalized when destructor is called. \"\n                        \"The zip archive might not be written at all or might be truncated. \"\ndiff --git a/src/IO/HTTPCommon.cpp b/src/IO/HTTPCommon.cpp\nindex cce394c67c98..c4468a1b896f 100644\n--- a/src/IO/HTTPCommon.cpp\n+++ b/src/IO/HTTPCommon.cpp\n@@ -70,7 +70,7 @@ namespace\n         static_assert(std::has_virtual_destructor_v<Session>, \"The base class must have a virtual destructor\");\n \n     public:\n-        HTTPSessionAdapter(const std::string & host, UInt16 port) : Session(host, port), log{&Poco::Logger::get(\"HTTPSessionAdapter\")} { }\n+        HTTPSessionAdapter(const std::string & host, UInt16 port) : Session(host, port), log{getLogger(\"HTTPSessionAdapter\")} { }\n         ~HTTPSessionAdapter() override = default;\n \n     protected:\n@@ -132,7 +132,7 @@ namespace\n                 }\n             }\n         }\n-        Poco::Logger * log;\n+        LoggerPtr log;\n     };\n \n     bool isHTTPS(const Poco::URI & uri)\n@@ -223,7 +223,7 @@ namespace\n             bool wait_on_pool_size_limit)\n             : Base(\n                 static_cast<unsigned>(max_pool_size_),\n-                &Poco::Logger::get(\"HTTPSessionPool\"),\n+                getLogger(\"HTTPSessionPool\"),\n                 wait_on_pool_size_limit ? BehaviourOnLimit::Wait : BehaviourOnLimit::AllocateNewBypassingPool)\n             , host(host_)\n             , port(port_)\ndiff --git a/src/IO/ParallelReadBuffer.cpp b/src/IO/ParallelReadBuffer.cpp\nindex 8d73f221748f..cdeb8a186351 100644\n--- a/src/IO/ParallelReadBuffer.cpp\n+++ b/src/IO/ParallelReadBuffer.cpp\n@@ -50,7 +50,7 @@ ParallelReadBuffer::ParallelReadBuffer(\n     , file_size(file_size_)\n     , range_step(std::max(1ul, range_step_))\n {\n-    LOG_TRACE(&Poco::Logger::get(\"ParallelReadBuffer\"), \"Parallel reading is used\");\n+    LOG_TRACE(getLogger(\"ParallelReadBuffer\"), \"Parallel reading is used\");\n \n     try\n     {\ndiff --git a/src/IO/ReadBufferFromS3.h b/src/IO/ReadBufferFromS3.h\nindex 101e25f8b436..f28c23a71d7a 100644\n--- a/src/IO/ReadBufferFromS3.h\n+++ b/src/IO/ReadBufferFromS3.h\n@@ -39,7 +39,7 @@ class ReadBufferFromS3 : public ReadBufferFromFileBase\n     std::optional<Aws::S3::Model::GetObjectResult> read_result;\n     std::unique_ptr<ReadBuffer> impl;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"ReadBufferFromS3\");\n+    LoggerPtr log = getLogger(\"ReadBufferFromS3\");\n \n public:\n     ReadBufferFromS3(\ndiff --git a/src/IO/ReadWriteBufferFromHTTP.cpp b/src/IO/ReadWriteBufferFromHTTP.cpp\nindex b2c4a53bd9b2..bf5c426f8036 100644\n--- a/src/IO/ReadWriteBufferFromHTTP.cpp\n+++ b/src/IO/ReadWriteBufferFromHTTP.cpp\n@@ -265,7 +265,7 @@ ReadWriteBufferFromHTTPBase<UpdatableSessionPtr>::ReadWriteBufferFromHTTPBase(\n     , file_info(file_info_)\n     , http_skip_not_found_url(http_skip_not_found_url_)\n     , settings {settings_}\n-    , log(&Poco::Logger::get(\"ReadWriteBufferFromHTTP\"))\n+    , log(getLogger(\"ReadWriteBufferFromHTTP\"))\n     , proxy_config(proxy_config_)\n {\n     if (settings.http_max_tries <= 0 || settings.http_retry_initial_backoff_ms <= 0\ndiff --git a/src/IO/ReadWriteBufferFromHTTP.h b/src/IO/ReadWriteBufferFromHTTP.h\nindex 29c0804bb280..63ca3e0417cd 100644\n--- a/src/IO/ReadWriteBufferFromHTTP.h\n+++ b/src/IO/ReadWriteBufferFromHTTP.h\n@@ -109,7 +109,7 @@ namespace detail\n         bool http_skip_not_found_url;\n \n         ReadSettings settings;\n-        Poco::Logger * log;\n+        LoggerPtr log;\n \n         ProxyConfiguration proxy_config;\n \ndiff --git a/src/IO/S3/AWSLogger.cpp b/src/IO/S3/AWSLogger.cpp\nindex d6162823aeee..dcdba7753b27 100644\n--- a/src/IO/S3/AWSLogger.cpp\n+++ b/src/IO/S3/AWSLogger.cpp\n@@ -41,7 +41,7 @@ AWSLogger::AWSLogger(bool enable_s3_requests_logging_)\n     : enable_s3_requests_logging(enable_s3_requests_logging_)\n {\n     for (auto [tag, name] : S3_LOGGER_TAG_NAMES)\n-        tag_loggers[tag] = &Poco::Logger::get(name);\n+        tag_loggers[tag] = getLogger(name);\n \n     default_logger = tag_loggers[S3_LOGGER_TAG_NAMES[0][0]];\n }\ndiff --git a/src/IO/S3/AWSLogger.h b/src/IO/S3/AWSLogger.h\nindex fdb6eed1f868..a4987f17c0dd 100644\n--- a/src/IO/S3/AWSLogger.h\n+++ b/src/IO/S3/AWSLogger.h\n@@ -6,6 +6,7 @@\n #include <aws/core/utils/logging/LogSystemInterface.h>\n #include <base/types.h>\n #include <unordered_map>\n+#include <Common/Logger.h>\n \n namespace Poco { class Logger; }\n \n@@ -29,9 +30,9 @@ class AWSLogger final : public Aws::Utils::Logging::LogSystemInterface\n     void Flush() final {}\n \n private:\n-    Poco::Logger * default_logger;\n+    LoggerPtr default_logger;\n     bool enable_s3_requests_logging;\n-    std::unordered_map<String, Poco::Logger *> tag_loggers;\n+    std::unordered_map<String, LoggerPtr> tag_loggers;\n };\n \n }\ndiff --git a/src/IO/S3/Client.cpp b/src/IO/S3/Client.cpp\nindex 64259ce5a76f..7f0ede727408 100644\n--- a/src/IO/S3/Client.cpp\n+++ b/src/IO/S3/Client.cpp\n@@ -184,7 +184,7 @@ Client::Client(\n     , client_settings(client_settings_)\n     , max_redirects(max_redirects_)\n     , sse_kms_config(std::move(sse_kms_config_))\n-    , log(&Poco::Logger::get(\"S3Client\"))\n+    , log(getLogger(\"S3Client\"))\n {\n     auto * endpoint_provider = dynamic_cast<Aws::S3::Endpoint::S3DefaultEpProviderBase *>(accessEndpointProvider().get());\n     endpoint_provider->GetBuiltInParameters().GetParameter(\"Region\").GetString(explicit_region);\n@@ -234,7 +234,7 @@ Client::Client(\n     , provider_type(other.provider_type)\n     , max_redirects(other.max_redirects)\n     , sse_kms_config(other.sse_kms_config)\n-    , log(&Poco::Logger::get(\"S3Client\"))\n+    , log(getLogger(\"S3Client\"))\n {\n     cache = std::make_shared<ClientCache>(*other.cache);\n     ClientCacheRegistry::instance().registerClient(cache);\n@@ -854,7 +854,7 @@ void ClientCacheRegistry::clearCacheForAll()\n         }\n         else\n         {\n-            LOG_INFO(&Poco::Logger::get(\"ClientCacheRegistry\"), \"Deleting leftover S3 client cache\");\n+            LOG_INFO(getLogger(\"ClientCacheRegistry\"), \"Deleting leftover S3 client cache\");\n             it = client_caches.erase(it);\n         }\n     }\ndiff --git a/src/IO/S3/Client.h b/src/IO/S3/Client.h\nindex 677b739fd396..8da21bd2c2c7 100644\n--- a/src/IO/S3/Client.h\n+++ b/src/IO/S3/Client.h\n@@ -281,7 +281,7 @@ class Client : private Aws::S3::S3Client\n \n     const ServerSideEncryptionKMSConfig sse_kms_config;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n class ClientFactory\ndiff --git a/src/IO/S3/Credentials.cpp b/src/IO/S3/Credentials.cpp\nindex b0b33244015f..e64f54b99ad3 100644\n--- a/src/IO/S3/Credentials.cpp\n+++ b/src/IO/S3/Credentials.cpp\n@@ -76,7 +76,7 @@ constexpr int AVAILABILITY_ZONE_REQUEST_TIMEOUT_SECONDS = 3;\n AWSEC2MetadataClient::AWSEC2MetadataClient(const Aws::Client::ClientConfiguration & client_configuration, const char * endpoint_)\n     : Aws::Internal::AWSHttpResourceClient(client_configuration)\n     , endpoint(endpoint_)\n-    , logger(&Poco::Logger::get(\"AWSEC2InstanceProfileConfigLoader\"))\n+    , logger(getLogger(\"AWSEC2InstanceProfileConfigLoader\"))\n {\n }\n \n@@ -200,7 +200,7 @@ Aws::String AWSEC2MetadataClient::getCurrentRegion() const\n \n static Aws::String getAWSMetadataEndpoint()\n {\n-    auto * logger = &Poco::Logger::get(\"AWSEC2InstanceProfileConfigLoader\");\n+    auto logger = getLogger(\"AWSEC2InstanceProfileConfigLoader\");\n     Aws::String ec2_metadata_service_endpoint = Aws::Environment::GetEnv(\"AWS_EC2_METADATA_SERVICE_ENDPOINT\");\n     if (ec2_metadata_service_endpoint.empty())\n     {\n@@ -285,7 +285,7 @@ String getGCPAvailabilityZoneOrException()\n \n String getRunningAvailabilityZone()\n {\n-    LOG_INFO(&Poco::Logger::get(\"Application\"), \"Trying to detect the availability zone.\");\n+    LOG_INFO(getLogger(\"Application\"), \"Trying to detect the availability zone.\");\n     try\n     {\n         return AWSEC2MetadataClient::getAvailabilityZoneOrException();\n@@ -310,7 +310,7 @@ String getRunningAvailabilityZone()\n AWSEC2InstanceProfileConfigLoader::AWSEC2InstanceProfileConfigLoader(const std::shared_ptr<AWSEC2MetadataClient> & client_, bool use_secure_pull_)\n     : client(client_)\n     , use_secure_pull(use_secure_pull_)\n-    , logger(&Poco::Logger::get(\"AWSEC2InstanceProfileConfigLoader\"))\n+    , logger(getLogger(\"AWSEC2InstanceProfileConfigLoader\"))\n {\n }\n \n@@ -352,7 +352,7 @@ bool AWSEC2InstanceProfileConfigLoader::LoadInternal()\n AWSInstanceProfileCredentialsProvider::AWSInstanceProfileCredentialsProvider(const std::shared_ptr<AWSEC2InstanceProfileConfigLoader> & config_loader)\n     : ec2_metadata_config_loader(config_loader)\n     , load_frequency_ms(Aws::Auth::REFRESH_THRESHOLD)\n-    , logger(&Poco::Logger::get(\"AWSInstanceProfileCredentialsProvider\"))\n+    , logger(getLogger(\"AWSInstanceProfileCredentialsProvider\"))\n {\n     LOG_INFO(logger, \"Creating Instance with injected EC2MetadataClient and refresh rate.\");\n }\n@@ -396,7 +396,7 @@ void AWSInstanceProfileCredentialsProvider::refreshIfExpired()\n \n AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider::AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider(\n     DB::S3::PocoHTTPClientConfiguration & aws_client_configuration, uint64_t expiration_window_seconds_)\n-    : logger(&Poco::Logger::get(\"AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider\"))\n+    : logger(getLogger(\"AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider\"))\n     , expiration_window_seconds(expiration_window_seconds_)\n {\n     // check environment variables\n@@ -529,7 +529,7 @@ SSOCredentialsProvider::SSOCredentialsProvider(DB::S3::PocoHTTPClientConfigurati\n     : profile_to_use(Aws::Auth::GetConfigProfileName())\n     , aws_client_configuration(std::move(aws_client_configuration_))\n     , expiration_window_seconds(expiration_window_seconds_)\n-    , logger(&Poco::Logger::get(SSO_CREDENTIALS_PROVIDER_LOG_TAG))\n+    , logger(getLogger(SSO_CREDENTIALS_PROVIDER_LOG_TAG))\n {\n     LOG_TRACE(logger, \"Setting sso credentials provider to read config from {}\", profile_to_use);\n }\n@@ -659,7 +659,7 @@ S3CredentialsProviderChain::S3CredentialsProviderChain(\n         const Aws::Auth::AWSCredentials & credentials,\n         CredentialsConfiguration credentials_configuration)\n {\n-    auto * logger = &Poco::Logger::get(\"S3CredentialsProviderChain\");\n+    auto logger = getLogger(\"S3CredentialsProviderChain\");\n \n     /// we don't provide any credentials to avoid signing\n     if (credentials_configuration.no_sign_request)\ndiff --git a/src/IO/S3/Credentials.h b/src/IO/S3/Credentials.h\nindex 5e83ea307989..34dc0c1d2bd0 100644\n--- a/src/IO/S3/Credentials.h\n+++ b/src/IO/S3/Credentials.h\n@@ -70,7 +70,7 @@ class AWSEC2MetadataClient : public Aws::Internal::AWSHttpResourceClient\n     const Aws::String endpoint;\n     mutable std::recursive_mutex token_mutex;\n     mutable Aws::String token;\n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n };\n \n std::shared_ptr<AWSEC2MetadataClient> InitEC2MetadataClient(const Aws::Client::ClientConfiguration & client_configuration);\n@@ -88,7 +88,7 @@ class AWSEC2InstanceProfileConfigLoader : public Aws::Config::AWSProfileConfigLo\n private:\n     std::shared_ptr<AWSEC2MetadataClient> client;\n     bool use_secure_pull;\n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n };\n \n class AWSInstanceProfileCredentialsProvider : public Aws::Auth::AWSCredentialsProvider\n@@ -107,7 +107,7 @@ class AWSInstanceProfileCredentialsProvider : public Aws::Auth::AWSCredentialsPr\n \n     std::shared_ptr<AWSEC2InstanceProfileConfigLoader> ec2_metadata_config_loader;\n     Int64 load_frequency_ms;\n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n };\n \n class AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider : public Aws::Auth::AWSCredentialsProvider\n@@ -133,7 +133,7 @@ class AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider : public Aws::Auth::AWS\n     Aws::String session_name;\n     Aws::String token;\n     bool initialized = false;\n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n     uint64_t expiration_window_seconds;\n };\n \n@@ -163,7 +163,7 @@ class SSOCredentialsProvider : public Aws::Auth::AWSCredentialsProvider\n \n     DB::S3::PocoHTTPClientConfiguration aws_client_configuration;\n     uint64_t expiration_window_seconds;\n-    Poco::Logger * logger;\n+    LoggerPtr logger;\n \n     void Reload() override;\n     void refreshIfExpired();\ndiff --git a/src/IO/S3/PocoHTTPClient.cpp b/src/IO/S3/PocoHTTPClient.cpp\nindex 946bd74dcb52..21acdfd69f26 100644\n--- a/src/IO/S3/PocoHTTPClient.cpp\n+++ b/src/IO/S3/PocoHTTPClient.cpp\n@@ -345,7 +345,7 @@ void PocoHTTPClient::makeRequestInternalImpl(\n {\n     using SessionPtr = std::conditional_t<pooled, PooledHTTPSessionPtr, HTTPSessionPtr>;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"AWSClient\");\n+    LoggerPtr log = getLogger(\"AWSClient\");\n \n     auto uri = request.GetUri().GetURIString();\n     auto method = getMethod(request);\ndiff --git a/src/IO/S3/copyS3File.cpp b/src/IO/S3/copyS3File.cpp\nindex 830377622ef2..98024e74f8ec 100644\n--- a/src/IO/S3/copyS3File.cpp\n+++ b/src/IO/S3/copyS3File.cpp\n@@ -61,7 +61,7 @@ namespace\n             ThreadPoolCallbackRunner<void> schedule_,\n             bool for_disk_s3_,\n             BlobStorageLogWriterPtr blob_storage_log_,\n-            const Poco::Logger * log_)\n+            const LoggerPtr log_)\n             : client_ptr(client_ptr_)\n             , dest_bucket(dest_bucket_)\n             , dest_key(dest_key_)\n@@ -87,7 +87,7 @@ namespace\n         ThreadPoolCallbackRunner<void> schedule;\n         bool for_disk_s3;\n         BlobStorageLogWriterPtr blob_storage_log;\n-        const Poco::Logger * log;\n+        const LoggerPtr log;\n \n         /// Represents a task uploading a single part.\n         /// Keep this struct small because there can be thousands of parts.\n@@ -475,7 +475,7 @@ namespace\n             ThreadPoolCallbackRunner<void> schedule_,\n             bool for_disk_s3_,\n             BlobStorageLogWriterPtr blob_storage_log_)\n-            : UploadHelper(client_ptr_, dest_bucket_, dest_key_, request_settings_, object_metadata_, schedule_, for_disk_s3_, blob_storage_log_, &Poco::Logger::get(\"copyDataToS3File\"))\n+            : UploadHelper(client_ptr_, dest_bucket_, dest_key_, request_settings_, object_metadata_, schedule_, for_disk_s3_, blob_storage_log_, getLogger(\"copyDataToS3File\"))\n             , create_read_buffer(create_read_buffer_)\n             , offset(offset_)\n             , size(size_)\n@@ -658,7 +658,7 @@ namespace\n             ThreadPoolCallbackRunner<void> schedule_,\n             bool for_disk_s3_,\n             BlobStorageLogWriterPtr blob_storage_log_)\n-            : UploadHelper(client_ptr_, dest_bucket_, dest_key_, request_settings_, object_metadata_, schedule_, for_disk_s3_, blob_storage_log_, &Poco::Logger::get(\"copyS3File\"))\n+            : UploadHelper(client_ptr_, dest_bucket_, dest_key_, request_settings_, object_metadata_, schedule_, for_disk_s3_, blob_storage_log_, getLogger(\"copyS3File\"))\n             , src_bucket(src_bucket_)\n             , src_key(src_key_)\n             , offset(src_offset_)\ndiff --git a/src/IO/WriteBuffer.cpp b/src/IO/WriteBuffer.cpp\nindex 61fdd31e16a3..bcc7445486ea 100644\n--- a/src/IO/WriteBuffer.cpp\n+++ b/src/IO/WriteBuffer.cpp\n@@ -17,7 +17,7 @@ WriteBuffer::~WriteBuffer()\n         /// However it is suspicious to destroy instance without finalization at the green path\n         if (!std::uncaught_exceptions() && std::current_exception() == nullptr)\n         {\n-            Poco::Logger * log = &Poco::Logger::get(\"WriteBuffer\");\n+            LoggerPtr log = getLogger(\"WriteBuffer\");\n             LOG_ERROR(\n                 log,\n                 \"WriteBuffer is not finalized when destructor is called. \"\ndiff --git a/src/IO/WriteBufferFromEncryptedFile.h b/src/IO/WriteBufferFromEncryptedFile.h\nindex c6edcf765337..2b59bb468d13 100644\n--- a/src/IO/WriteBufferFromEncryptedFile.h\n+++ b/src/IO/WriteBufferFromEncryptedFile.h\n@@ -40,7 +40,7 @@ class WriteBufferFromEncryptedFile : public WriteBufferDecorator<WriteBufferFrom\n \n     FileEncryption::Encryptor encryptor;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"WriteBufferFromEncryptedFile\");\n+    LoggerPtr log = getLogger(\"WriteBufferFromEncryptedFile\");\n };\n \n }\ndiff --git a/src/IO/WriteBufferFromHTTP.cpp b/src/IO/WriteBufferFromHTTP.cpp\nindex 3b2721e3bff7..8ddcbc03b84c 100644\n--- a/src/IO/WriteBufferFromHTTP.cpp\n+++ b/src/IO/WriteBufferFromHTTP.cpp\n@@ -33,7 +33,7 @@ WriteBufferFromHTTP::WriteBufferFromHTTP(\n     for (const auto & header: additional_headers)\n         request.add(header.name, header.value);\n \n-    LOG_TRACE((&Poco::Logger::get(\"WriteBufferToHTTP\")), \"Sending request to {}\", uri.toString());\n+    LOG_TRACE((getLogger(\"WriteBufferToHTTP\")), \"Sending request to {}\", uri.toString());\n \n     ostr = &session->sendRequest(request);\n }\ndiff --git a/src/IO/WriteBufferFromS3.h b/src/IO/WriteBufferFromS3.h\nindex 191e522c59a2..230f39b074e4 100644\n--- a/src/IO/WriteBufferFromS3.h\n+++ b/src/IO/WriteBufferFromS3.h\n@@ -91,7 +91,7 @@ class WriteBufferFromS3 final : public WriteBufferFromFileBase\n     const WriteSettings write_settings;\n     const std::shared_ptr<const S3::Client> client_ptr;\n     const std::optional<std::map<String, String>> object_metadata;\n-    Poco::Logger * log = &Poco::Logger::get(\"WriteBufferFromS3\");\n+    LoggerPtr log = getLogger(\"WriteBufferFromS3\");\n     LogSeriesLimiterPtr limitedLog = std::make_shared<LogSeriesLimiter>(log, 1, 5);\n \n     IBufferAllocationPolicyPtr buffer_allocation_policy;\ndiff --git a/src/Interpreters/Aggregator.cpp b/src/Interpreters/Aggregator.cpp\nindex 4171818d3e6a..331cd991ea19 100644\n--- a/src/Interpreters/Aggregator.cpp\n+++ b/src/Interpreters/Aggregator.cpp\n@@ -105,7 +105,7 @@ class HashTablesStatistics\n         if (const auto hint = cache->get(params.key))\n         {\n             LOG_TRACE(\n-                &Poco::Logger::get(\"Aggregator\"),\n+                getLogger(\"Aggregator\"),\n                 \"An entry for key={} found in cache: sum_of_sizes={}, median_size={}\",\n                 params.key,\n                 hint->sum_of_sizes,\n@@ -129,7 +129,7 @@ class HashTablesStatistics\n             || hint->median_size < median_size)\n         {\n             LOG_TRACE(\n-                &Poco::Logger::get(\"Aggregator\"),\n+                getLogger(\"Aggregator\"),\n                 \"Statistics updated for key={}: new sum_of_sizes={}, median_size={}\",\n                 params.key,\n                 sum_of_sizes,\n@@ -229,7 +229,7 @@ void initDataVariantsWithSizeHint(\n                 /// But we will also work with the big (i.e. not so cache friendly) HT from the beginning which may result in a slight slowdown.\n                 /// So let's just do nothing.\n                 LOG_TRACE(\n-                    &Poco::Logger::get(\"Aggregator\"),\n+                    getLogger(\"Aggregator\"),\n                     \"No space were preallocated in hash tables because 'max_size_to_preallocate_for_aggregation' has too small value: {}, \"\n                     \"should be at least {}\",\n                     stats_collecting_params.max_size_to_preallocate_for_aggregation,\ndiff --git a/src/Interpreters/Aggregator.h b/src/Interpreters/Aggregator.h\nindex f4aa78043ca4..109bd0dd939a 100644\n--- a/src/Interpreters/Aggregator.h\n+++ b/src/Interpreters/Aggregator.h\n@@ -1293,7 +1293,7 @@ class Aggregator final\n     /// How many RAM were used to process the query before processing the first block.\n     Int64 memory_usage_before_aggregation = 0;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"Aggregator\");\n+    LoggerPtr log = getLogger(\"Aggregator\");\n \n     /// For external aggregation.\n     TemporaryDataOnDiskPtr tmp_data;\ndiff --git a/src/Interpreters/AsynchronousInsertQueue.cpp b/src/Interpreters/AsynchronousInsertQueue.cpp\nindex 63ee62cdef49..8206c31624ca 100644\n--- a/src/Interpreters/AsynchronousInsertQueue.cpp\n+++ b/src/Interpreters/AsynchronousInsertQueue.cpp\n@@ -515,7 +515,7 @@ try\n \n     SCOPE_EXIT(CurrentMetrics::sub(CurrentMetrics::PendingAsyncInsert, data->entries.size()));\n \n-    const auto * log = &Poco::Logger::get(\"AsynchronousInsertQueue\");\n+    const auto log = getLogger(\"AsynchronousInsertQueue\");\n     const auto & insert_query = assert_cast<const ASTInsertQuery &>(*key.query);\n \n     auto insert_context = Context::createCopy(global_context);\n@@ -732,7 +732,7 @@ Chunk AsynchronousInsertQueue::processEntriesWithParsing(\n     const std::list<InsertData::EntryPtr> & entries,\n     const Block & header,\n     const ContextPtr & insert_context,\n-    const Poco::Logger * logger,\n+    const LoggerPtr logger,\n     LogFunc && add_to_async_insert_log)\n {\n     size_t total_rows = 0;\ndiff --git a/src/Interpreters/AsynchronousInsertQueue.h b/src/Interpreters/AsynchronousInsertQueue.h\nindex 99394d0fb14d..f4bfdbd38a5c 100644\n--- a/src/Interpreters/AsynchronousInsertQueue.h\n+++ b/src/Interpreters/AsynchronousInsertQueue.h\n@@ -214,7 +214,7 @@ class AsynchronousInsertQueue : public WithContext\n     /// Uses async_insert_busy_timeout_ms and processBatchDeadlines()\n     std::vector<ThreadFromGlobalPool> dump_by_first_update_threads;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"AsynchronousInsertQueue\");\n+    LoggerPtr log = getLogger(\"AsynchronousInsertQueue\");\n \n     PushResult pushDataChunk(ASTPtr query, DataChunk chunk, ContextPtr query_context);\n     void preprocessInsertQuery(const ASTPtr & query, const ContextPtr & query_context);\n@@ -230,7 +230,7 @@ class AsynchronousInsertQueue : public WithContext\n         const std::list<InsertData::EntryPtr> & entries,\n         const Block & header,\n         const ContextPtr & insert_context,\n-        const Poco::Logger * logger,\n+        const LoggerPtr logger,\n         LogFunc && add_to_async_insert_log);\n \n     template <typename LogFunc>\ndiff --git a/src/Interpreters/Cache/FileCache.cpp b/src/Interpreters/Cache/FileCache.cpp\nindex 9c29a9c4a47c..d242544f787b 100644\n--- a/src/Interpreters/Cache/FileCache.cpp\n+++ b/src/Interpreters/Cache/FileCache.cpp\n@@ -85,7 +85,7 @@ FileCache::FileCache(const std::string & cache_name, const FileCacheSettings & s\n     , boundary_alignment(settings.boundary_alignment)\n     , load_metadata_threads(settings.load_metadata_threads)\n     , write_cache_per_user_directory(settings.write_cache_per_user_id_directory)\n-    , log(&Poco::Logger::get(\"FileCache(\" + cache_name + \")\"))\n+    , log(getLogger(\"FileCache(\" + cache_name + \")\"))\n     , metadata(settings.base_path, settings.background_download_queue_size_limit, settings.background_download_threads, write_cache_per_user_directory)\n {\n     if (settings.cache_policy == \"LRU\")\ndiff --git a/src/Interpreters/Cache/FileCache.h b/src/Interpreters/Cache/FileCache.h\nindex 64e03b739680..2de2f347999e 100644\n--- a/src/Interpreters/Cache/FileCache.h\n+++ b/src/Interpreters/Cache/FileCache.h\n@@ -193,7 +193,7 @@ class FileCache : private boost::noncopyable\n     size_t load_metadata_threads;\n     const bool write_cache_per_user_directory;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::exception_ptr init_exception;\n     std::atomic<bool> is_initialized = false;\ndiff --git a/src/Interpreters/Cache/FileSegment.cpp b/src/Interpreters/Cache/FileSegment.cpp\nindex 8e5d1fd757fc..9866d95f5efb 100644\n--- a/src/Interpreters/Cache/FileSegment.cpp\n+++ b/src/Interpreters/Cache/FileSegment.cpp\n@@ -65,9 +65,9 @@ FileSegment::FileSegment(\n     , queue_iterator(queue_iterator_)\n     , cache(cache_)\n #ifdef ABORT_ON_LOGICAL_ERROR\n-    , log(&Poco::Logger::get(fmt::format(\"FileSegment({}) : {}\", key_.toString(), range().toString())))\n+    , log(getLogger(fmt::format(\"FileSegment({}) : {}\", key_.toString(), range().toString())))\n #else\n-    , log(&Poco::Logger::get(\"FileSegment\"))\n+    , log(getLogger(\"FileSegment\"))\n #endif\n {\n     /// On creation, file segment state can be EMPTY, DOWNLOADED, DOWNLOADING.\ndiff --git a/src/Interpreters/Cache/FileSegment.h b/src/Interpreters/Cache/FileSegment.h\nindex cb718bcdd2ed..9a2e243131be 100644\n--- a/src/Interpreters/Cache/FileSegment.h\n+++ b/src/Interpreters/Cache/FileSegment.h\n@@ -269,7 +269,7 @@ friend class FileCache; /// Because of reserved_size in tryReserve().\n     FileCache * cache;\n     std::condition_variable cv;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::atomic<size_t> hits_count = 0; /// cache hits.\n     std::atomic<size_t> ref_count = 0; /// Used for getting snapshot state\ndiff --git a/src/Interpreters/Cache/LRUFileCachePriority.h b/src/Interpreters/Cache/LRUFileCachePriority.h\nindex 998b11723d81..a74a4b8b621a 100644\n--- a/src/Interpreters/Cache/LRUFileCachePriority.h\n+++ b/src/Interpreters/Cache/LRUFileCachePriority.h\n@@ -71,7 +71,7 @@ class LRUFileCachePriority final : public IFileCachePriority\n     friend class SLRUFileCachePriority;\n \n     LRUQueue queue;\n-    Poco::Logger * log = &Poco::Logger::get(\"LRUFileCachePriority\");\n+    LoggerPtr log = getLogger(\"LRUFileCachePriority\");\n     StatePtr state;\n \n     void updateElementsCount(int64_t num);\ndiff --git a/src/Interpreters/Cache/Metadata.cpp b/src/Interpreters/Cache/Metadata.cpp\nindex 953413a8ef2c..b6f04807ad82 100644\n--- a/src/Interpreters/Cache/Metadata.cpp\n+++ b/src/Interpreters/Cache/Metadata.cpp\n@@ -153,7 +153,7 @@ std::string KeyMetadata::getFileSegmentPath(const FileSegment & file_segment) co\n     return cache_metadata->getFileSegmentPath(key, file_segment.offset(), file_segment.getKind(), user);\n }\n \n-Poco::Logger * KeyMetadata::logger() const\n+LoggerPtr KeyMetadata::logger() const\n {\n     return cache_metadata->log;\n }\n@@ -167,7 +167,7 @@ CacheMetadata::CacheMetadata(\n     , cleanup_queue(std::make_shared<CleanupQueue>())\n     , download_queue(std::make_shared<DownloadQueue>(background_download_queue_size_limit_))\n     , write_cache_per_user_directory(write_cache_per_user_directory_)\n-    , log(&Poco::Logger::get(\"CacheMetadata\"))\n+    , log(getLogger(\"CacheMetadata\"))\n     , download_threads_num(background_download_threads_)\n {\n }\ndiff --git a/src/Interpreters/Cache/Metadata.h b/src/Interpreters/Cache/Metadata.h\nindex 3003ad74e186..c02127cdef30 100644\n--- a/src/Interpreters/Cache/Metadata.h\n+++ b/src/Interpreters/Cache/Metadata.h\n@@ -99,7 +99,7 @@ struct KeyMetadata : private std::map<size_t, FileSegmentMetadataPtr>,\n     std::atomic<bool> created_base_directory = false;\n \n     LockedKeyPtr lockNoStateCheck();\n-    Poco::Logger * logger() const;\n+    LoggerPtr logger() const;\n     bool addToDownloadQueue(FileSegmentPtr file_segment);\n     void addToCleanupQueue();\n };\n@@ -177,7 +177,7 @@ class CacheMetadata : private boost::noncopyable\n     const DownloadQueuePtr download_queue;\n     const bool write_cache_per_user_directory;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     mutable std::shared_mutex key_prefix_directory_mutex;\n \n     struct MetadataBucket : public std::unordered_map<FileCacheKey, KeyMetadataPtr>\ndiff --git a/src/Interpreters/Cache/QueryCache.h b/src/Interpreters/Cache/QueryCache.h\nindex 2dd4887dd20a..c574f3085e30 100644\n--- a/src/Interpreters/Cache/QueryCache.h\n+++ b/src/Interpreters/Cache/QueryCache.h\n@@ -156,7 +156,7 @@ class QueryCache\n         Cache::MappedPtr query_result TSA_GUARDED_BY(mutex) = std::make_shared<Entry>();\n         std::atomic<bool> skip_insert = false;\n         bool was_finalized = false;\n-        Poco::Logger * logger = &Poco::Logger::get(\"QueryCache\");\n+        LoggerPtr logger = getLogger(\"QueryCache\");\n \n         Writer(Cache & cache_, const Key & key_,\n             size_t max_entry_size_in_bytes_, size_t max_entry_size_in_rows_,\n@@ -183,7 +183,7 @@ class QueryCache\n         std::unique_ptr<SourceFromChunks> source_from_chunks;\n         std::unique_ptr<SourceFromChunks> source_from_chunks_totals;\n         std::unique_ptr<SourceFromChunks> source_from_chunks_extremes;\n-        Poco::Logger * logger = &Poco::Logger::get(\"QueryCache\");\n+        LoggerPtr logger = getLogger(\"QueryCache\");\n         friend class QueryCache; /// for createReader()\n     };\n \ndiff --git a/src/Interpreters/Cache/SLRUFileCachePriority.h b/src/Interpreters/Cache/SLRUFileCachePriority.h\nindex e949979ae611..d97fa80a6c78 100644\n--- a/src/Interpreters/Cache/SLRUFileCachePriority.h\n+++ b/src/Interpreters/Cache/SLRUFileCachePriority.h\n@@ -58,7 +58,7 @@ class SLRUFileCachePriority : public IFileCachePriority\n     double size_ratio;\n     LRUFileCachePriority protected_queue;\n     LRUFileCachePriority probationary_queue;\n-    Poco::Logger * log = &Poco::Logger::get(\"SLRUFileCachePriority\");\n+    LoggerPtr log = getLogger(\"SLRUFileCachePriority\");\n \n     void increasePriority(SLRUIterator & iterator, const CacheGuard::Lock & lock);\n };\ndiff --git a/src/Interpreters/Cache/WriteBufferToFileSegment.cpp b/src/Interpreters/Cache/WriteBufferToFileSegment.cpp\nindex bf5b8712fb73..751e78a0a2d0 100644\n--- a/src/Interpreters/Cache/WriteBufferToFileSegment.cpp\n+++ b/src/Interpreters/Cache/WriteBufferToFileSegment.cpp\n@@ -74,7 +74,7 @@ void WriteBufferToFileSegment::nextImpl()\n     }\n     catch (...)\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"WriteBufferToFileSegment\"), \"Failed to write to the underlying buffer ({})\", file_segment->getInfoForLog());\n+        LOG_WARNING(getLogger(\"WriteBufferToFileSegment\"), \"Failed to write to the underlying buffer ({})\", file_segment->getInfoForLog());\n         throw;\n     }\n \ndiff --git a/src/Interpreters/ClusterDiscovery.cpp b/src/Interpreters/ClusterDiscovery.cpp\nindex d0b00056cb4e..52b74597c4b4 100644\n--- a/src/Interpreters/ClusterDiscovery.cpp\n+++ b/src/Interpreters/ClusterDiscovery.cpp\n@@ -116,7 +116,7 @@ ClusterDiscovery::ClusterDiscovery(\n     const String & config_prefix)\n     : context(Context::createCopy(context_))\n     , current_node_name(toString(ServerUUID::get()))\n-    , log(&Poco::Logger::get(\"ClusterDiscovery\"))\n+    , log(getLogger(\"ClusterDiscovery\"))\n {\n     LOG_DEBUG(log, \"Cluster discovery is enabled\");\n \n@@ -553,7 +553,7 @@ bool ClusterDiscovery::NodeInfo::parse(const String & data, NodeInfo & result)\n         else\n         {\n             LOG_ERROR(\n-                &Poco::Logger::get(\"ClusterDiscovery\"),\n+                getLogger(\"ClusterDiscovery\"),\n                 \"Unsupported version '{}' of data in zk node '{}'\",\n                 ver, data.size() < 1024 ? data : \"[data too long]\");\n         }\n@@ -561,7 +561,7 @@ bool ClusterDiscovery::NodeInfo::parse(const String & data, NodeInfo & result)\n     catch (Poco::Exception & e)\n     {\n         LOG_WARNING(\n-            &Poco::Logger::get(\"ClusterDiscovery\"),\n+            getLogger(\"ClusterDiscovery\"),\n             \"Can't parse '{}' from node: {}\",\n             data.size() < 1024 ? data : \"[data too long]\", e.displayText());\n         return false;\ndiff --git a/src/Interpreters/ClusterDiscovery.h b/src/Interpreters/ClusterDiscovery.h\nindex 8083fb6db413..756ed3d8d9be 100644\n--- a/src/Interpreters/ClusterDiscovery.h\n+++ b/src/Interpreters/ClusterDiscovery.h\n@@ -152,7 +152,7 @@ class ClusterDiscovery\n     bool is_initialized = false;\n     ThreadFromGlobalPool main_thread;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp b/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp\nindex f8a070a6fde1..f0592735cafa 100644\n--- a/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp\n+++ b/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp\n@@ -175,7 +175,7 @@ void SelectStreamFactory::createForShard(\n             ProfileEvents::increment(ProfileEvents::DistributedConnectionMissingTable);\n             if (shard_info.hasRemoteConnections())\n             {\n-                LOG_WARNING(&Poco::Logger::get(\"ClusterProxy::SelectStreamFactory\"),\n+                LOG_WARNING(getLogger(\"ClusterProxy::SelectStreamFactory\"),\n                     \"There is no table {} on local replica of shard {}, will try remote replicas.\",\n                     main_table.getNameForLogs(), shard_info.shard_num);\n                 emplace_remote_stream();\n@@ -213,7 +213,7 @@ void SelectStreamFactory::createForShard(\n \n         /// If we reached this point, local replica is stale.\n         ProfileEvents::increment(ProfileEvents::DistributedConnectionStaleReplica);\n-        LOG_WARNING(&Poco::Logger::get(\"ClusterProxy::SelectStreamFactory\"), \"Local replica of shard {} is stale (delay: {}s.)\", shard_info.shard_num, local_delay);\n+        LOG_WARNING(getLogger(\"ClusterProxy::SelectStreamFactory\"), \"Local replica of shard {} is stale (delay: {}s.)\", shard_info.shard_num, local_delay);\n \n         if (!settings.fallback_to_stale_replicas_for_distributed_queries)\n         {\ndiff --git a/src/Interpreters/ClusterProxy/executeQuery.cpp b/src/Interpreters/ClusterProxy/executeQuery.cpp\nindex 5865e669e47b..35451e1d774c 100644\n--- a/src/Interpreters/ClusterProxy/executeQuery.cpp\n+++ b/src/Interpreters/ClusterProxy/executeQuery.cpp\n@@ -42,7 +42,7 @@ ContextMutablePtr updateSettingsForCluster(const Cluster & cluster,\n     const Settings & settings,\n     const StorageID & main_table,\n     ASTPtr additional_filter_ast,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     const DistributedSettings * distributed_settings)\n {\n     Settings new_settings = settings;\n@@ -202,7 +202,7 @@ void executeQuery(\n     const StorageID & main_table,\n     const ASTPtr & table_func_ptr,\n     SelectStreamFactory & stream_factory,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     const ASTPtr & query_ast,\n     ContextPtr context,\n     const SelectQueryInfo & query_info,\n@@ -347,14 +347,14 @@ void executeQueryWithParallelReplicas(\n         if (settings.use_hedged_requests.changed)\n         {\n             LOG_WARNING(\n-                &Poco::Logger::get(\"executeQueryWithParallelReplicas\"),\n+                getLogger(\"executeQueryWithParallelReplicas\"),\n                 \"Setting 'use_hedged_requests' explicitly with enabled 'allow_experimental_parallel_reading_from_replicas' has no effect. \"\n                 \"Hedged connections are not used for parallel reading from replicas\");\n         }\n         else\n         {\n             LOG_INFO(\n-                &Poco::Logger::get(\"executeQueryWithParallelReplicas\"),\n+                getLogger(\"executeQueryWithParallelReplicas\"),\n                 \"Disabling 'use_hedged_requests' in favor of 'allow_experimental_parallel_reading_from_replicas'. Hedged connections are \"\n                 \"not used for parallel reading from replicas\");\n         }\n@@ -390,7 +390,7 @@ void executeQueryWithParallelReplicas(\n \n         chassert(shard_count == not_optimized_cluster->getShardsAddresses().size());\n \n-        LOG_DEBUG(&Poco::Logger::get(\"executeQueryWithParallelReplicas\"), \"Parallel replicas query in shard scope: shard_num={} cluster={}\",\n+        LOG_DEBUG(getLogger(\"executeQueryWithParallelReplicas\"), \"Parallel replicas query in shard scope: shard_num={} cluster={}\",\n                   shard_num, not_optimized_cluster->getName());\n \n         // get cluster for shard specified by shard_num\n@@ -417,7 +417,7 @@ void executeQueryWithParallelReplicas(\n         getThrottler(new_context),\n         std::move(scalars),\n         std::move(external_tables),\n-        &Poco::Logger::get(\"ReadFromParallelRemoteReplicasStep\"),\n+        getLogger(\"ReadFromParallelRemoteReplicasStep\"),\n         std::move(storage_limits));\n \n     query_plan.addStep(std::move(read_from_remote));\ndiff --git a/src/Interpreters/ClusterProxy/executeQuery.h b/src/Interpreters/ClusterProxy/executeQuery.h\nindex a19ece0bbdcc..bbc3c6c9e49c 100644\n--- a/src/Interpreters/ClusterProxy/executeQuery.h\n+++ b/src/Interpreters/ClusterProxy/executeQuery.h\n@@ -43,7 +43,7 @@ ContextMutablePtr updateSettingsForCluster(const Cluster & cluster,\n     const Settings & settings,\n     const StorageID & main_table,\n     ASTPtr additional_filter_ast = nullptr,\n-    Poco::Logger * log = nullptr,\n+    LoggerPtr log = nullptr,\n     const DistributedSettings * distributed_settings = nullptr);\n \n using AdditionalShardFilterGenerator = std::function<ASTPtr(uint64_t)>;\n@@ -57,7 +57,7 @@ void executeQuery(\n     const StorageID & main_table,\n     const ASTPtr & table_func_ptr,\n     SelectStreamFactory & stream_factory,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     const ASTPtr & query_ast,\n     ContextPtr context,\n     const SelectQueryInfo & query_info,\ndiff --git a/src/Interpreters/Context.cpp b/src/Interpreters/Context.cpp\nindex 9778a02f1fa9..2472fbbe5960 100644\n--- a/src/Interpreters/Context.cpp\n+++ b/src/Interpreters/Context.cpp\n@@ -198,7 +198,7 @@ namespace ErrorCodes\n   */\n struct ContextSharedPart : boost::noncopyable\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"Context\");\n+    LoggerPtr log = getLogger(\"Context\");\n \n     /// For access of most of shared objects.\n     mutable ContextSharedMutex mutex;\n@@ -1011,7 +1011,7 @@ void Context::setFilesystemCacheUser(const String & user)\n     shared->filesystem_cache_user = user;\n }\n \n-static void setupTmpPath(Poco::Logger * log, const std::string & path)\n+static void setupTmpPath(LoggerPtr log, const std::string & path)\n try\n {\n     LOG_DEBUG(log, \"Setting up {} to store temporary data in it\", path);\n@@ -4292,7 +4292,7 @@ void Context::setDefaultProfiles(const Poco::Util::AbstractConfiguration & confi\n     shared->system_profile_name = config.getString(\"system_profile\", shared->default_profile_name);\n     setCurrentProfile(shared->system_profile_name);\n \n-    applySettingsQuirks(settings, &Poco::Logger::get(\"SettingsQuirks\"));\n+    applySettingsQuirks(settings, getLogger(\"SettingsQuirks\"));\n \n     shared->buffer_profile_name = config.getString(\"buffer_profile\", shared->system_profile_name);\n     buffer_context = Context::createCopy(shared_from_this());\ndiff --git a/src/Interpreters/CrossToInnerJoinVisitor.cpp b/src/Interpreters/CrossToInnerJoinVisitor.cpp\nindex 005450c2a2c8..42af164f4ad3 100644\n--- a/src/Interpreters/CrossToInnerJoinVisitor.cpp\n+++ b/src/Interpreters/CrossToInnerJoinVisitor.cpp\n@@ -249,7 +249,7 @@ void CrossToInnerJoinMatcher::visit(ASTSelectQuery & select, ASTPtr &, Data & da\n                 ASTPtr on_expr = makeOnExpression(expr_it->second);\n                 if (rewritten = joined.rewriteCrossToInner(on_expr); rewritten)\n                 {\n-                    LOG_DEBUG(&Poco::Logger::get(\"CrossToInnerJoin\"), \"Rewritten '{}' to '{}'\", query_before, queryToString(*joined.tableJoin()));\n+                    LOG_DEBUG(getLogger(\"CrossToInnerJoin\"), \"Rewritten '{}' to '{}'\", query_before, queryToString(*joined.tableJoin()));\n                 }\n             }\n \ndiff --git a/src/Interpreters/DDLTask.cpp b/src/Interpreters/DDLTask.cpp\nindex d418be51cc59..90eec421abf4 100644\n--- a/src/Interpreters/DDLTask.cpp\n+++ b/src/Interpreters/DDLTask.cpp\n@@ -215,7 +215,7 @@ ContextMutablePtr DDLTaskBase::makeQueryContext(ContextPtr from_context, const Z\n }\n \n \n-bool DDLTask::findCurrentHostID(ContextPtr global_context, Poco::Logger * log, const ZooKeeperPtr & zookeeper, const std::optional<std::string> & config_host_name)\n+bool DDLTask::findCurrentHostID(ContextPtr global_context, LoggerPtr log, const ZooKeeperPtr & zookeeper, const std::optional<std::string> & config_host_name)\n {\n     bool host_in_hostlist = false;\n     std::exception_ptr first_exception = nullptr;\n@@ -312,7 +312,7 @@ bool DDLTask::findCurrentHostID(ContextPtr global_context, Poco::Logger * log, c\n     return host_in_hostlist;\n }\n \n-void DDLTask::setClusterInfo(ContextPtr context, Poco::Logger * log)\n+void DDLTask::setClusterInfo(ContextPtr context, LoggerPtr log)\n {\n     auto * query_on_cluster = dynamic_cast<ASTQueryWithOnCluster *>(query.get());\n     if (!query_on_cluster)\ndiff --git a/src/Interpreters/DDLTask.h b/src/Interpreters/DDLTask.h\nindex bc45b46bf0f2..5a8a5bfb1842 100644\n--- a/src/Interpreters/DDLTask.h\n+++ b/src/Interpreters/DDLTask.h\n@@ -146,9 +146,9 @@ struct DDLTask : public DDLTaskBase\n {\n     DDLTask(const String & name, const String & path) : DDLTaskBase(name, path) {}\n \n-    bool findCurrentHostID(ContextPtr global_context, Poco::Logger * log, const ZooKeeperPtr & zookeeper, const std::optional<std::string> & config_host_name);\n+    bool findCurrentHostID(ContextPtr global_context, LoggerPtr log, const ZooKeeperPtr & zookeeper, const std::optional<std::string> & config_host_name);\n \n-    void setClusterInfo(ContextPtr context, Poco::Logger * log);\n+    void setClusterInfo(ContextPtr context, LoggerPtr log);\n \n     String getShardID() const override;\n \ndiff --git a/src/Interpreters/DDLWorker.cpp b/src/Interpreters/DDLWorker.cpp\nindex c0611dfaf7df..fd807d54eff6 100644\n--- a/src/Interpreters/DDLWorker.cpp\n+++ b/src/Interpreters/DDLWorker.cpp\n@@ -79,7 +79,7 @@ DDLWorker::DDLWorker(\n     const CurrentMetrics::Metric * max_entry_metric_,\n     const CurrentMetrics::Metric * max_pushed_entry_metric_)\n     : context(Context::createCopy(context_))\n-    , log(&Poco::Logger::get(logger_name))\n+    , log(getLogger(logger_name))\n     , pool_size(pool_size_)\n     , max_entry_metric(max_entry_metric_)\n     , max_pushed_entry_metric(max_pushed_entry_metric_)\ndiff --git a/src/Interpreters/DDLWorker.h b/src/Interpreters/DDLWorker.h\nindex adc9a491d815..9eb6606e73cd 100644\n--- a/src/Interpreters/DDLWorker.h\n+++ b/src/Interpreters/DDLWorker.h\n@@ -151,7 +151,7 @@ class DDLWorker\n     void runCleanupThread();\n \n     ContextMutablePtr context;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::optional<std::string> config_host_name; /// host_name from config\n \ndiff --git a/src/Interpreters/DNSCacheUpdater.cpp b/src/Interpreters/DNSCacheUpdater.cpp\nindex c6a38d85acbb..4769395acaab 100644\n--- a/src/Interpreters/DNSCacheUpdater.cpp\n+++ b/src/Interpreters/DNSCacheUpdater.cpp\n@@ -24,7 +24,7 @@ void DNSCacheUpdater::run()\n     /// Reload cluster config if IP of any host has been changed since last update.\n     if (resolver.updateCache(max_consecutive_failures))\n     {\n-        LOG_INFO(&Poco::Logger::get(\"DNSCacheUpdater\"), \"IPs of some hosts have been changed. Will reload cluster config.\");\n+        LOG_INFO(getLogger(\"DNSCacheUpdater\"), \"IPs of some hosts have been changed. Will reload cluster config.\");\n         try\n         {\n             getContext()->reloadClusterConfig();\n@@ -45,7 +45,7 @@ void DNSCacheUpdater::run()\n \n void DNSCacheUpdater::start()\n {\n-    LOG_INFO(&Poco::Logger::get(\"DNSCacheUpdater\"), \"Update period {} seconds\", update_period_seconds);\n+    LOG_INFO(getLogger(\"DNSCacheUpdater\"), \"Update period {} seconds\", update_period_seconds);\n     task_handle->activateAndSchedule();\n }\n \ndiff --git a/src/Interpreters/DatabaseCatalog.cpp b/src/Interpreters/DatabaseCatalog.cpp\nindex 138ec588202f..87985d1d12b5 100644\n--- a/src/Interpreters/DatabaseCatalog.cpp\n+++ b/src/Interpreters/DatabaseCatalog.cpp\n@@ -827,7 +827,7 @@ DatabaseCatalog::DatabaseCatalog(ContextMutablePtr global_context_)\n     , referential_dependencies{\"ReferentialDeps\"}\n     , loading_dependencies{\"LoadingDeps\"}\n     , view_dependencies{\"ViewDeps\"}\n-    , log(&Poco::Logger::get(\"DatabaseCatalog\"))\n+    , log(getLogger(\"DatabaseCatalog\"))\n     , first_async_drop_in_queue(tables_marked_dropped.end())\n {\n }\ndiff --git a/src/Interpreters/DatabaseCatalog.h b/src/Interpreters/DatabaseCatalog.h\nindex 19882b0b8281..beb73e3ef96a 100644\n--- a/src/Interpreters/DatabaseCatalog.h\n+++ b/src/Interpreters/DatabaseCatalog.h\n@@ -318,7 +318,7 @@ class DatabaseCatalog : boost::noncopyable, WithMutableContext\n     /// View dependencies between a source table and its view.\n     TablesDependencyGraph view_dependencies TSA_GUARDED_BY(databases_mutex);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::atomic_bool is_shutting_down = false;\n \ndiff --git a/src/Interpreters/DirectJoin.cpp b/src/Interpreters/DirectJoin.cpp\nindex 431f216436dc..3255b56b3be6 100644\n--- a/src/Interpreters/DirectJoin.cpp\n+++ b/src/Interpreters/DirectJoin.cpp\n@@ -67,7 +67,7 @@ DirectKeyValueJoin::DirectKeyValueJoin(std::shared_ptr<TableJoin> table_join_,\n     : table_join(table_join_)\n     , storage(storage_)\n     , right_sample_block(right_sample_block_)\n-    , log(&Poco::Logger::get(\"DirectKeyValueJoin\"))\n+    , log(getLogger(\"DirectKeyValueJoin\"))\n {\n     if (!table_join->oneDisjunct() ||\n         table_join->getOnlyClause().key_names_left.size() != 1 ||\ndiff --git a/src/Interpreters/DirectJoin.h b/src/Interpreters/DirectJoin.h\nindex 5f6643148181..ef8d12a1b8f1 100644\n--- a/src/Interpreters/DirectJoin.h\n+++ b/src/Interpreters/DirectJoin.h\n@@ -60,7 +60,7 @@ class DirectKeyValueJoin : public IJoin\n     Block right_sample_block;\n     Block right_sample_block_with_storage_column_names;\n     Block sample_block_with_columns_to_add;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n };\n \ndiff --git a/src/Interpreters/EmbeddedDictionaries.cpp b/src/Interpreters/EmbeddedDictionaries.cpp\nindex 6c0ccce66b57..1435d16cb073 100644\n--- a/src/Interpreters/EmbeddedDictionaries.cpp\n+++ b/src/Interpreters/EmbeddedDictionaries.cpp\n@@ -125,7 +125,7 @@ EmbeddedDictionaries::EmbeddedDictionaries(\n     ContextPtr context_,\n     const bool throw_on_error)\n     : WithContext(context_)\n-    , log(&Poco::Logger::get(\"EmbeddedDictionaries\"))\n+    , log(getLogger(\"EmbeddedDictionaries\"))\n     , geo_dictionaries_loader(std::move(geo_dictionaries_loader_))\n     , reload_period(getContext()->getConfigRef().getInt(\"builtin_dictionaries_reload_interval\", 3600))\n {\ndiff --git a/src/Interpreters/EmbeddedDictionaries.h b/src/Interpreters/EmbeddedDictionaries.h\nindex e71098636fe7..b537146e92d7 100644\n--- a/src/Interpreters/EmbeddedDictionaries.h\n+++ b/src/Interpreters/EmbeddedDictionaries.h\n@@ -24,7 +24,7 @@ class GeoDictionariesLoader;\n class EmbeddedDictionaries : WithContext\n {\n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     MultiVersion<RegionsHierarchies> regions_hierarchies;\n     MultiVersion<RegionsNames> regions_names;\ndiff --git a/src/Interpreters/ExpressionAnalyzer.cpp b/src/Interpreters/ExpressionAnalyzer.cpp\nindex 5c628436d609..fefbd67bfc17 100644\n--- a/src/Interpreters/ExpressionAnalyzer.cpp\n+++ b/src/Interpreters/ExpressionAnalyzer.cpp\n@@ -120,7 +120,7 @@ bool allowEarlyConstantFolding(const ActionsDAG & actions, const Settings & sett\n     return true;\n }\n \n-Poco::Logger * getLogger() { return &Poco::Logger::get(\"ExpressionAnalyzer\"); }\n+LoggerPtr getLogger() { return ::getLogger(\"ExpressionAnalyzer\"); }\n \n }\n \ndiff --git a/src/Interpreters/ExpressionJIT.cpp b/src/Interpreters/ExpressionJIT.cpp\nindex 0eacb598fbe9..16275b23053f 100644\n--- a/src/Interpreters/ExpressionJIT.cpp\n+++ b/src/Interpreters/ExpressionJIT.cpp\n@@ -38,10 +38,9 @@ static CHJIT & getJITInstance()\n     return jit;\n }\n \n-static Poco::Logger * getLogger()\n+static LoggerPtr getLogger()\n {\n-    static Poco::Logger & logger = Poco::Logger::get(\"ExpressionJIT\");\n-    return &logger;\n+    return ::getLogger(\"ExpressionJIT\");\n }\n \n class CompiledFunctionHolder : public CompiledExpressionCacheEntry\ndiff --git a/src/Interpreters/ExternalDictionariesLoader.cpp b/src/Interpreters/ExternalDictionariesLoader.cpp\nindex 46171c95cb0f..74984de00643 100644\n--- a/src/Interpreters/ExternalDictionariesLoader.cpp\n+++ b/src/Interpreters/ExternalDictionariesLoader.cpp\n@@ -22,7 +22,7 @@ namespace ErrorCodes\n \n /// Must not acquire Context lock in constructor to avoid possibility of deadlocks.\n ExternalDictionariesLoader::ExternalDictionariesLoader(ContextPtr global_context_)\n-    : ExternalLoader(\"external dictionary\", &Poco::Logger::get(\"ExternalDictionariesLoader\"))\n+    : ExternalLoader(\"external dictionary\", getLogger(\"ExternalDictionariesLoader\"))\n     , WithContext(global_context_)\n {\n     setConfigSettings({\"dictionary\", \"name\", \"database\", \"uuid\"});\ndiff --git a/src/Interpreters/ExternalLoader.cpp b/src/Interpreters/ExternalLoader.cpp\nindex 56d480d87354..36664cbd06fb 100644\n--- a/src/Interpreters/ExternalLoader.cpp\n+++ b/src/Interpreters/ExternalLoader.cpp\n@@ -95,7 +95,7 @@ namespace\n class ExternalLoader::LoadablesConfigReader : private boost::noncopyable\n {\n public:\n-    LoadablesConfigReader(const String & type_name_, Poco::Logger * log_)\n+    LoadablesConfigReader(const String & type_name_, LoggerPtr log_)\n         : type_name(type_name_), log(log_)\n     {\n     }\n@@ -377,7 +377,7 @@ class ExternalLoader::LoadablesConfigReader : private boost::noncopyable\n     }\n \n     const String type_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::mutex mutex;\n     ExternalLoaderConfigSettings settings;\n@@ -401,7 +401,7 @@ class ExternalLoader::LoadingDispatcher : private boost::noncopyable\n     LoadingDispatcher(\n         const CreateObjectFunction & create_object_function_,\n         const String & type_name_,\n-        Poco::Logger * log_)\n+        LoggerPtr log_)\n         : create_object(create_object_function_)\n         , type_name(type_name_)\n         , log(log_)\n@@ -1193,7 +1193,7 @@ class ExternalLoader::LoadingDispatcher : private boost::noncopyable\n \n     const CreateObjectFunction create_object;\n     const String type_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     mutable std::mutex mutex;\n     std::condition_variable event;\n@@ -1273,7 +1273,7 @@ class ExternalLoader::PeriodicUpdater : private boost::noncopyable\n };\n \n \n-ExternalLoader::ExternalLoader(const String & type_name_, Poco::Logger * log_)\n+ExternalLoader::ExternalLoader(const String & type_name_, LoggerPtr log_)\n     : config_files_reader(std::make_unique<LoadablesConfigReader>(type_name_, log_))\n     , loading_dispatcher(std::make_unique<LoadingDispatcher>(\n           [this](auto && a, auto && b, auto && c) { return createObject(a, b, c); },\ndiff --git a/src/Interpreters/ExternalLoader.h b/src/Interpreters/ExternalLoader.h\nindex 49b4ea77e0d3..a5d83bdab50a 100644\n--- a/src/Interpreters/ExternalLoader.h\n+++ b/src/Interpreters/ExternalLoader.h\n@@ -8,6 +8,7 @@\n #include <Interpreters/IExternalLoaderConfigRepository.h>\n #include <base/scope_guard.h>\n #include <Common/ExternalLoaderStatus.h>\n+#include <Common/Logger.h>\n #include <Core/Types.h>\n \n namespace Poco { class Logger; }\n@@ -84,7 +85,7 @@ class ExternalLoader\n     template <typename T>\n     static constexpr bool is_vector_load_result_type = std::is_same_v<T, LoadResults> || std::is_same_v<T, Loadables>;\n \n-    ExternalLoader(const String & type_name_, Poco::Logger * log);\n+    ExternalLoader(const String & type_name_, LoggerPtr log);\n     virtual ~ExternalLoader();\n \n     /// Adds a repository which will be used to read configurations from.\n@@ -230,7 +231,7 @@ class ExternalLoader\n     std::unique_ptr<PeriodicUpdater> periodic_updater;\n \n     const String type_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Interpreters/FullSortingMergeJoin.h b/src/Interpreters/FullSortingMergeJoin.h\nindex 3fc9f8920ed7..7688d44f7a96 100644\n--- a/src/Interpreters/FullSortingMergeJoin.h\n+++ b/src/Interpreters/FullSortingMergeJoin.h\n@@ -25,7 +25,7 @@ class FullSortingMergeJoin : public IJoin\n         : table_join(table_join_)\n         , right_sample_block(right_sample_block_)\n     {\n-        LOG_TRACE(&Poco::Logger::get(\"FullSortingMergeJoin\"), \"Will use full sorting merge join\");\n+        LOG_TRACE(getLogger(\"FullSortingMergeJoin\"), \"Will use full sorting merge join\");\n     }\n \n     std::string getName() const override { return \"FullSortingMergeJoin\"; }\ndiff --git a/src/Interpreters/GraceHashJoin.cpp b/src/Interpreters/GraceHashJoin.cpp\nindex 26d666a89133..5fb92a68a297 100644\n--- a/src/Interpreters/GraceHashJoin.cpp\n+++ b/src/Interpreters/GraceHashJoin.cpp\n@@ -121,7 +121,7 @@ class GraceHashJoin::FileBucket : boost::noncopyable\n public:\n     using BucketLock = std::unique_lock<std::mutex>;\n \n-    explicit FileBucket(size_t bucket_index_, TemporaryFileStream & left_file_, TemporaryFileStream & right_file_, Poco::Logger * log_)\n+    explicit FileBucket(size_t bucket_index_, TemporaryFileStream & left_file_, TemporaryFileStream & right_file_, LoggerPtr log_)\n         : idx{bucket_index_}\n         , left_file{left_file_}\n         , right_file{right_file_}\n@@ -223,7 +223,7 @@ class GraceHashJoin::FileBucket : boost::noncopyable\n \n     std::atomic<State> state;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n namespace\n@@ -261,7 +261,7 @@ GraceHashJoin::GraceHashJoin(\n     const Block & right_sample_block_,\n     TemporaryDataOnDiskScopePtr tmp_data_,\n     bool any_take_last_row_)\n-    : log{&Poco::Logger::get(\"GraceHashJoin\")}\n+    : log{getLogger(\"GraceHashJoin\")}\n     , context{context_}\n     , table_join{std::move(table_join_)}\n     , left_sample_block{left_sample_block_}\n@@ -403,7 +403,7 @@ void GraceHashJoin::addBuckets(const size_t bucket_count)\n         catch (...)\n         {\n             LOG_ERROR(\n-                &Poco::Logger::get(\"GraceHashJoin\"),\n+                getLogger(\"GraceHashJoin\"),\n                 \"Can't create bucket {} due to error: {}\",\n                 current_size + i,\n                 getCurrentExceptionMessage(false));\ndiff --git a/src/Interpreters/GraceHashJoin.h b/src/Interpreters/GraceHashJoin.h\nindex 2cadeee10b97..ff396683230d 100644\n--- a/src/Interpreters/GraceHashJoin.h\n+++ b/src/Interpreters/GraceHashJoin.h\n@@ -120,7 +120,7 @@ class GraceHashJoin final : public IJoin\n     /// Structure block to store in the HashJoin according to sample_block.\n     Block prepareRightBlock(const Block & block);\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     ContextPtr context;\n     std::shared_ptr<TableJoin> table_join;\n     Block left_sample_block;\ndiff --git a/src/Interpreters/HashJoin.cpp b/src/Interpreters/HashJoin.cpp\nindex 467cc4c25319..33dc178ca00c 100644\n--- a/src/Interpreters/HashJoin.cpp\n+++ b/src/Interpreters/HashJoin.cpp\n@@ -245,7 +245,7 @@ HashJoin::HashJoin(std::shared_ptr<TableJoin> table_join_, const Block & right_s\n     , right_sample_block(right_sample_block_)\n     , max_joined_block_rows(table_join->maxJoinedBlockRows())\n     , instance_log_id(!instance_id_.empty() ? \"(\" + instance_id_ + \") \" : \"\")\n-    , log(&Poco::Logger::get(\"HashJoin\"))\n+    , log(getLogger(\"HashJoin\"))\n {\n     LOG_TRACE(log, \"{}Keys: {}, datatype: {}, kind: {}, strictness: {}, right header: {}\",\n         instance_log_id, TableJoin::formatClauses(table_join->getClauses(), true), data->type, kind, strictness, right_sample_block.dumpStructure());\ndiff --git a/src/Interpreters/HashJoin.h b/src/Interpreters/HashJoin.h\nindex 17f003adc4b2..29bb90700092 100644\n--- a/src/Interpreters/HashJoin.h\n+++ b/src/Interpreters/HashJoin.h\n@@ -446,7 +446,7 @@ class HashJoin : public IJoin\n     /// Several instances can be created, for example, in GraceHashJoin to handle different buckets\n     String instance_log_id;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Should be set via setLock to protect hash table from modification from StorageJoin\n     /// If set HashJoin instance is not available for modification (addBlockToJoin)\ndiff --git a/src/Interpreters/InternalTextLogsQueue.cpp b/src/Interpreters/InternalTextLogsQueue.cpp\nindex 3be58a11beba..ca8461937ac3 100644\n--- a/src/Interpreters/InternalTextLogsQueue.cpp\n+++ b/src/Interpreters/InternalTextLogsQueue.cpp\n@@ -43,7 +43,7 @@ void InternalTextLogsQueue::pushBlock(Block && log_block)\n     if (blocksHaveEqualStructure(sample_block, log_block))\n         (void)(emplace(log_block.mutateColumns()));\n     else\n-        LOG_WARNING(&Poco::Logger::get(\"InternalTextLogsQueue\"), \"Log block have different structure\");\n+        LOG_WARNING(getLogger(\"InternalTextLogsQueue\"), \"Log block have different structure\");\n }\n \n std::string_view InternalTextLogsQueue::getPriorityName(int priority)\ndiff --git a/src/Interpreters/InterpreterCheckQuery.cpp b/src/Interpreters/InterpreterCheckQuery.cpp\nindex 0cc4afd62f22..98a281bd5ade 100644\n--- a/src/Interpreters/InterpreterCheckQuery.cpp\n+++ b/src/Interpreters/InterpreterCheckQuery.cpp\n@@ -149,7 +149,7 @@ class TableCheckTask : public ChunkInfo\n class TableCheckSource : public ISource\n {\n public:\n-    TableCheckSource(Strings databases_, ContextPtr context_, Poco::Logger * log_)\n+    TableCheckSource(Strings databases_, ContextPtr context_, LoggerPtr log_)\n         : ISource(getSingleValueBlock(0))\n         , databases(databases_)\n         , context(context_)\n@@ -157,7 +157,7 @@ class TableCheckSource : public ISource\n     {\n     }\n \n-    TableCheckSource(std::shared_ptr<TableCheckTask> table_check_task_, Poco::Logger * log_)\n+    TableCheckSource(std::shared_ptr<TableCheckTask> table_check_task_, LoggerPtr log_)\n         : ISource(getSingleValueBlock(0))\n         , table_check_task(table_check_task_)\n         , log(log_)\n@@ -260,14 +260,14 @@ class TableCheckSource : public ISource\n \n     ContextPtr context;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n /// Receives TableCheckTask and returns CheckResult converted to sinle-row chunk\n class TableCheckWorkerProcessor : public ISimpleTransform\n {\n public:\n-    TableCheckWorkerProcessor(bool with_table_name_, Poco::Logger * log_)\n+    TableCheckWorkerProcessor(bool with_table_name_, LoggerPtr log_)\n         : ISimpleTransform(getSingleValueBlock(0), getHeaderForCheckResult(with_table_name_), true)\n         , with_table_name(with_table_name_)\n         , log(log_)\n@@ -308,7 +308,7 @@ class TableCheckWorkerProcessor : public ISimpleTransform\n     /// If true, then output will contain columns with database and table names\n     bool with_table_name;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n /// Accumulates all results and returns single value\ndiff --git a/src/Interpreters/InterpreterCheckQuery.h b/src/Interpreters/InterpreterCheckQuery.h\nindex 5ffd1d4298f6..4bba3ed780ca 100644\n--- a/src/Interpreters/InterpreterCheckQuery.h\n+++ b/src/Interpreters/InterpreterCheckQuery.h\n@@ -19,7 +19,7 @@ class InterpreterCheckQuery : public IInterpreter, WithContext\n private:\n     ASTPtr query_ptr;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"InterpreterCheckQuery\");\n+    LoggerPtr log = getLogger(\"InterpreterCheckQuery\");\n };\n \n }\ndiff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp\nindex d002cc6d9806..9ce1c8566220 100644\n--- a/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -1214,7 +1214,7 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)\n     }\n     else if (create.attach && !create.attach_short_syntax && getContext()->getClientInfo().query_kind != ClientInfo::QueryKind::SECONDARY_QUERY)\n     {\n-        auto * log = &Poco::Logger::get(\"InterpreterCreateQuery\");\n+        auto log = getLogger(\"InterpreterCreateQuery\");\n         LOG_WARNING(log, \"ATTACH TABLE query with full table definition is not recommended: \"\n                          \"use either ATTACH TABLE {}; to attach existing table \"\n                          \"or CREATE TABLE {} <table definition>; to create new table \"\n@@ -1455,7 +1455,7 @@ bool InterpreterCreateQuery::doCreateTable(ASTCreateQuery & create,\n             /// so the existing directory probably contains some leftovers from previous unsuccessful attempts to create the table\n \n             fs::path trash_path = fs::path{getContext()->getPath()} / \"trash\" / data_path / getHexUIntLowercase(thread_local_rng());\n-            LOG_WARNING(&Poco::Logger::get(\"InterpreterCreateQuery\"), \"Directory for {} data {} already exists. Will move it to {}\",\n+            LOG_WARNING(getLogger(\"InterpreterCreateQuery\"), \"Directory for {} data {} already exists. Will move it to {}\",\n                         Poco::toLower(storage_name), String(data_path), trash_path);\n             fs::create_directories(trash_path.parent_path());\n             renameNoReplace(full_data_path, trash_path);\ndiff --git a/src/Interpreters/InterpreterKillQueryQuery.cpp b/src/Interpreters/InterpreterKillQueryQuery.cpp\nindex 5efffdaa194a..3431cd5e568a 100644\n--- a/src/Interpreters/InterpreterKillQueryQuery.cpp\n+++ b/src/Interpreters/InterpreterKillQueryQuery.cpp\n@@ -161,7 +161,7 @@ class SyncKillQuerySource : public ISource\n                 if (curr_process.processed)\n                     continue;\n \n-                LOG_DEBUG(&Poco::Logger::get(\"KillQuery\"), \"Will kill query {} (synchronously)\", curr_process.query_id);\n+                LOG_DEBUG(getLogger(\"KillQuery\"), \"Will kill query {} (synchronously)\", curr_process.query_id);\n \n                 auto code = process_list.sendCancelToQuery(curr_process.query_id, curr_process.user, true);\n \n@@ -229,7 +229,7 @@ BlockIO InterpreterKillQueryQuery::execute()\n             for (const auto & query_desc : queries_to_stop)\n             {\n                 if (!query.test)\n-                    LOG_DEBUG(&Poco::Logger::get(\"KillQuery\"), \"Will kill query {} (asynchronously)\", query_desc.query_id);\n+                    LOG_DEBUG(getLogger(\"KillQuery\"), \"Will kill query {} (asynchronously)\", query_desc.query_id);\n                 auto code = (query.test) ? CancellationCode::Unknown : process_list.sendCancelToQuery(query_desc.query_id, query_desc.user, true);\n                 insertResultRow(query_desc.source_num, code, processes_block, header, res_columns);\n             }\ndiff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 4e4e25617e4c..2d994483ba83 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -381,7 +381,7 @@ InterpreterSelectQuery::InterpreterSelectQuery(\n     : IInterpreterUnionOrSelectQuery(options_.modify_inplace ? query_ptr_ : query_ptr_->clone(), context_, options_)\n     , storage(storage_)\n     , input_pipe(std::move(input_pipe_))\n-    , log(&Poco::Logger::get(\"InterpreterSelectQuery\"))\n+    , log(getLogger(\"InterpreterSelectQuery\"))\n     , metadata_snapshot(metadata_snapshot_)\n     , prepared_sets(prepared_sets_)\n {\ndiff --git a/src/Interpreters/InterpreterSelectQuery.h b/src/Interpreters/InterpreterSelectQuery.h\nindex fbb53d71755c..c307e457649c 100644\n--- a/src/Interpreters/InterpreterSelectQuery.h\n+++ b/src/Interpreters/InterpreterSelectQuery.h\n@@ -253,7 +253,7 @@ class InterpreterSelectQuery : public IInterpreterUnionOrSelectQuery\n     /// Used when we read from prepared input, not table or subquery.\n     std::optional<Pipe> input_pipe;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     StorageMetadataPtr metadata_snapshot;\n     StorageSnapshotPtr storage_snapshot;\n \ndiff --git a/src/Interpreters/InterpreterSystemQuery.cpp b/src/Interpreters/InterpreterSystemQuery.cpp\nindex 8a242cee213d..9a80553f1493 100644\n--- a/src/Interpreters/InterpreterSystemQuery.cpp\n+++ b/src/Interpreters/InterpreterSystemQuery.cpp\n@@ -221,7 +221,7 @@ void InterpreterSystemQuery::startStopAction(StorageActionBlockType action_type,\n \n void InterpreterSystemQuery::startStopActionInDatabase(StorageActionBlockType action_type, bool start,\n                                                        const String & database_name, const DatabasePtr & database,\n-                                                       const ContextPtr & local_context, Poco::Logger * log)\n+                                                       const ContextPtr & local_context, LoggerPtr log)\n {\n     auto manager = local_context->getActionLocksManager();\n     auto access = local_context->getAccess();\n@@ -251,7 +251,7 @@ void InterpreterSystemQuery::startStopActionInDatabase(StorageActionBlockType ac\n \n \n InterpreterSystemQuery::InterpreterSystemQuery(const ASTPtr & query_ptr_, ContextMutablePtr context_)\n-        : WithMutableContext(context_), query_ptr(query_ptr_->clone()), log(&Poco::Logger::get(\"InterpreterSystemQuery\"))\n+        : WithMutableContext(context_), query_ptr(query_ptr_->clone()), log(getLogger(\"InterpreterSystemQuery\"))\n {\n }\n \ndiff --git a/src/Interpreters/InterpreterSystemQuery.h b/src/Interpreters/InterpreterSystemQuery.h\nindex 89de7402b4d3..1419c430aca2 100644\n--- a/src/Interpreters/InterpreterSystemQuery.h\n+++ b/src/Interpreters/InterpreterSystemQuery.h\n@@ -43,11 +43,11 @@ class InterpreterSystemQuery : public IInterpreter, WithMutableContext\n \n     static void startStopActionInDatabase(StorageActionBlockType action_type, bool start,\n                                           const String & database_name, const DatabasePtr & database,\n-                                          const ContextPtr & local_context, Poco::Logger * log);\n+                                          const ContextPtr & local_context, LoggerPtr log);\n \n private:\n     ASTPtr query_ptr;\n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n     StorageID table_id = StorageID::createEmpty();      /// Will be set up if query contains table name\n     VolumePtr volume_ptr;\n \ndiff --git a/src/Interpreters/InterserverCredentials.cpp b/src/Interpreters/InterserverCredentials.cpp\nindex 094b58789a8d..c344732a2620 100644\n--- a/src/Interpreters/InterserverCredentials.cpp\n+++ b/src/Interpreters/InterserverCredentials.cpp\n@@ -35,7 +35,7 @@ InterserverCredentials::CurrentCredentials InterserverCredentials::parseCredenti\n     const Poco::Util::AbstractConfiguration & config,\n     const std::string & root_tag)\n {\n-    auto * log = &Poco::Logger::get(\"InterserverCredentials\");\n+    auto log = getLogger(\"InterserverCredentials\");\n     CurrentCredentials store;\n     store.emplace_back(current_user_, current_password_);\n     if (config.getBool(root_tag + \".allow_empty\", false))\ndiff --git a/src/Interpreters/JoinedTables.cpp b/src/Interpreters/JoinedTables.cpp\nindex c104af770f06..9be8bf178a19 100644\n--- a/src/Interpreters/JoinedTables.cpp\n+++ b/src/Interpreters/JoinedTables.cpp\n@@ -335,12 +335,12 @@ std::shared_ptr<TableJoin> JoinedTables::makeTableJoin(const ASTSelectQuery & se\n                 auto dictionary = dictionary_helper.getDictionary(dictionary_name);\n                 if (!dictionary)\n                 {\n-                    LOG_TRACE(&Poco::Logger::get(\"JoinedTables\"), \"Can't use dictionary join: dictionary '{}' was not found\", dictionary_name);\n+                    LOG_TRACE(getLogger(\"JoinedTables\"), \"Can't use dictionary join: dictionary '{}' was not found\", dictionary_name);\n                     return nullptr;\n                 }\n                 if (dictionary->getSpecialKeyType() == DictionarySpecialKeyType::Range)\n                 {\n-                    LOG_TRACE(&Poco::Logger::get(\"JoinedTables\"), \"Can't use dictionary join: dictionary '{}' is a range dictionary\", dictionary_name);\n+                    LOG_TRACE(getLogger(\"JoinedTables\"), \"Can't use dictionary join: dictionary '{}' is a range dictionary\", dictionary_name);\n                     return nullptr;\n                 }\n \ndiff --git a/src/Interpreters/MergeJoin.cpp b/src/Interpreters/MergeJoin.cpp\nindex 4a80e1a3c56e..901c82029eec 100644\n--- a/src/Interpreters/MergeJoin.cpp\n+++ b/src/Interpreters/MergeJoin.cpp\n@@ -492,7 +492,7 @@ MergeJoin::MergeJoin(std::shared_ptr<TableJoin> table_join_, const Block & right\n     , max_joined_block_rows(table_join->maxJoinedBlockRows())\n     , max_rows_in_right_block(table_join->maxRowsInRightBlock())\n     , max_files_to_merge(table_join->maxFilesToMerge())\n-    , log(&Poco::Logger::get(\"MergeJoin\"))\n+    , log(getLogger(\"MergeJoin\"))\n {\n     switch (table_join->strictness())\n     {\ndiff --git a/src/Interpreters/MergeJoin.h b/src/Interpreters/MergeJoin.h\nindex 98fae1d419fc..4486c134d518 100644\n--- a/src/Interpreters/MergeJoin.h\n+++ b/src/Interpreters/MergeJoin.h\n@@ -117,7 +117,7 @@ class MergeJoin : public IJoin\n \n     Names lowcard_right_keys;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     void changeLeftColumns(Block & block, MutableColumns && columns) const;\n     void addRightColumns(Block & block, MutableColumns && columns);\ndiff --git a/src/Interpreters/MutationsInterpreter.cpp b/src/Interpreters/MutationsInterpreter.cpp\nindex 86cd2d84fa37..cc447dfef242 100644\n--- a/src/Interpreters/MutationsInterpreter.cpp\n+++ b/src/Interpreters/MutationsInterpreter.cpp\n@@ -1284,7 +1284,7 @@ void MutationsInterpreter::Source::read(\n             plan, *data, storage_snapshot, part,\n             std::move(virtual_columns.columns_to_read),\n             apply_deleted_mask_, filter, context_,\n-            &Poco::Logger::get(\"MutationsInterpreter\"));\n+            getLogger(\"MutationsInterpreter\"));\n \n         virtual_columns.addVirtuals(plan);\n     }\ndiff --git a/src/Interpreters/PartLog.cpp b/src/Interpreters/PartLog.cpp\nindex 9819b8e3ec4e..a7f20a067854 100644\n--- a/src/Interpreters/PartLog.cpp\n+++ b/src/Interpreters/PartLog.cpp\n@@ -271,7 +271,7 @@ bool PartLog::addNewParts(\n     }\n     catch (...)\n     {\n-        tryLogCurrentException(part_log ? part_log->log : &Poco::Logger::get(\"PartLog\"), __PRETTY_FUNCTION__);\n+        tryLogCurrentException(part_log ? part_log->log : getLogger(\"PartLog\"), __PRETTY_FUNCTION__);\n         return false;\n     }\n \ndiff --git a/src/Interpreters/PasteJoin.h b/src/Interpreters/PasteJoin.h\nindex df7bb2f280c1..f87a70215517 100644\n--- a/src/Interpreters/PasteJoin.h\n+++ b/src/Interpreters/PasteJoin.h\n@@ -24,7 +24,7 @@ class PasteJoin : public IJoin\n         : table_join(table_join_)\n         , right_sample_block(right_sample_block_)\n     {\n-        LOG_TRACE(&Poco::Logger::get(\"PasteJoin\"), \"Will use paste join\");\n+        LOG_TRACE(getLogger(\"PasteJoin\"), \"Will use paste join\");\n     }\n \n     std::string getName() const override { return \"PasteJoin\"; }\ndiff --git a/src/Interpreters/ProcessList.cpp b/src/Interpreters/ProcessList.cpp\nindex 2b84b7655b37..5b3b87114ae8 100644\n--- a/src/Interpreters/ProcessList.cpp\n+++ b/src/Interpreters/ProcessList.cpp\n@@ -86,7 +86,7 @@ ProcessList::insert(const String & query_, const IAST * ast, ContextMutablePtr q\n         if (!is_unlimited_query && max_size && processes.size() >= max_size)\n         {\n             if (queue_max_wait_ms)\n-                LOG_WARNING(&Poco::Logger::get(\"ProcessList\"), \"Too many simultaneous queries, will wait {} ms.\", queue_max_wait_ms);\n+                LOG_WARNING(getLogger(\"ProcessList\"), \"Too many simultaneous queries, will wait {} ms.\", queue_max_wait_ms);\n             if (!queue_max_wait_ms || !have_space.wait_for(lock, std::chrono::milliseconds(queue_max_wait_ms), [&]{ return processes.size() < max_size; }))\n                 throw Exception(ErrorCodes::TOO_MANY_SIMULTANEOUS_QUERIES, \"Too many simultaneous queries. Maximum: {}\", max_size);\n         }\n@@ -295,7 +295,7 @@ ProcessListEntry::~ProcessListEntry()\n     auto user_process_list_it = parent.user_to_queries.find(user);\n     if (user_process_list_it == parent.user_to_queries.end())\n     {\n-        LOG_ERROR(&Poco::Logger::get(\"ProcessList\"), \"Logical error: cannot find user in ProcessList\");\n+        LOG_ERROR(getLogger(\"ProcessList\"), \"Logical error: cannot find user in ProcessList\");\n         std::terminate();\n     }\n \n@@ -323,7 +323,7 @@ ProcessListEntry::~ProcessListEntry()\n \n     if (!found)\n     {\n-        LOG_ERROR(&Poco::Logger::get(\"ProcessList\"), \"Logical error: cannot find query by query_id and pointer to ProcessListElement in ProcessListForUser\");\n+        LOG_ERROR(getLogger(\"ProcessList\"), \"Logical error: cannot find query by query_id and pointer to ProcessListElement in ProcessListForUser\");\n         std::terminate();\n     }\n \ndiff --git a/src/Interpreters/Session.cpp b/src/Interpreters/Session.cpp\nindex d2f9fe8b325e..533f33033e38 100644\n--- a/src/Interpreters/Session.cpp\n+++ b/src/Interpreters/Session.cpp\n@@ -265,7 +265,7 @@ class NamedSessionsStorage\n     ThreadFromGlobalPool thread;\n     bool quit = false;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"NamedSessionsStorage\");\n+    LoggerPtr log = getLogger(\"NamedSessionsStorage\");\n };\n \n \n@@ -282,7 +282,7 @@ void Session::shutdownNamedSessions()\n Session::Session(const ContextPtr & global_context_, ClientInfo::Interface interface_, bool is_secure, const std::string & certificate)\n     : auth_id(UUIDHelpers::generateV4()),\n       global_context(global_context_),\n-      log(&Poco::Logger::get(String{magic_enum::enum_name(interface_)} + \"-Session\"))\n+      log(getLogger(String{magic_enum::enum_name(interface_)} + \"-Session\"))\n {\n     prepared_client_info.emplace();\n     prepared_client_info->interface = interface_;\ndiff --git a/src/Interpreters/Session.h b/src/Interpreters/Session.h\nindex 75e1414b8cba..cde000d89fa8 100644\n--- a/src/Interpreters/Session.h\n+++ b/src/Interpreters/Session.h\n@@ -123,7 +123,7 @@ class Session\n     /// to set when creating a session context\n     SettingsChanges settings_from_auth_server;\n \n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n };\n \n }\ndiff --git a/src/Interpreters/Set.h b/src/Interpreters/Set.h\nindex 7136b090c42f..5a65d40d89f2 100644\n--- a/src/Interpreters/Set.h\n+++ b/src/Interpreters/Set.h\n@@ -33,7 +33,7 @@ class Set\n     /// store all set elements in explicit form.\n     /// This is needed for subsequent use for index.\n     Set(const SizeLimits & limits_, size_t max_elements_to_fill_, bool transform_null_in_)\n-        : log(&Poco::Logger::get(\"Set\")),\n+        : log(getLogger(\"Set\")),\n         limits(limits_), max_elements_to_fill(max_elements_to_fill_), transform_null_in(transform_null_in_),\n         cast_cache(std::make_unique<InternalCastFunctionCache>())\n     {}\n@@ -114,7 +114,7 @@ class Set\n     /// Types for set_elements.\n     DataTypes set_elements_types;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Limitations on the maximum size of the set\n     SizeLimits limits;\ndiff --git a/src/Interpreters/SystemLog.cpp b/src/Interpreters/SystemLog.cpp\nindex 954368db3127..2fb782befa16 100644\n--- a/src/Interpreters/SystemLog.cpp\n+++ b/src/Interpreters/SystemLog.cpp\n@@ -125,13 +125,13 @@ std::shared_ptr<TSystemLog> createSystemLog(\n {\n     if (!config.has(config_prefix))\n     {\n-        LOG_DEBUG(&Poco::Logger::get(\"SystemLog\"),\n+        LOG_DEBUG(getLogger(\"SystemLog\"),\n                 \"Not creating {}.{} since corresponding section '{}' is missing from config\",\n                 default_database_name, default_table_name, config_prefix);\n \n         return {};\n     }\n-    LOG_DEBUG(&Poco::Logger::get(\"SystemLog\"),\n+    LOG_DEBUG(getLogger(\"SystemLog\"),\n               \"Creating {}.{} from {}\", default_database_name, default_table_name, config_prefix);\n \n     SystemLogSettings log_settings;\n@@ -143,7 +143,7 @@ std::shared_ptr<TSystemLog> createSystemLog(\n     {\n         /// System tables must be loaded before other tables, but loading order is undefined for all databases except `system`\n         LOG_ERROR(\n-            &Poco::Logger::get(\"SystemLog\"),\n+            getLogger(\"SystemLog\"),\n             \"Custom database name for a system table specified in config.\"\n             \" Table `{}` will be created in `system` database instead of `{}`\",\n             log_settings.queue_settings.table,\n@@ -395,7 +395,7 @@ SystemLog<LogElement>::SystemLog(\n     std::shared_ptr<SystemLogQueue<LogElement>> queue_)\n     : Base(settings_.queue_settings, queue_)\n     , WithContext(context_)\n-    , log(&Poco::Logger::get(\"SystemLog (\" + settings_.queue_settings.database + \".\" + settings_.queue_settings.table + \")\"))\n+    , log(getLogger(\"SystemLog (\" + settings_.queue_settings.database + \".\" + settings_.queue_settings.table + \")\"))\n     , table_id(settings_.queue_settings.database, settings_.queue_settings.table)\n     , storage_def(settings_.engine)\n     , create_query(serializeAST(*getCreateTableQuery()))\ndiff --git a/src/Interpreters/SystemLog.h b/src/Interpreters/SystemLog.h\nindex 8c357e43be98..c296b91e24a7 100644\n--- a/src/Interpreters/SystemLog.h\n+++ b/src/Interpreters/SystemLog.h\n@@ -131,7 +131,7 @@ class SystemLog : public SystemLogBase<LogElement>, private boost::noncopyable,\n     void stopFlushThread() override;\n \n protected:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     using ISystemLog::is_shutdown;\n     using ISystemLog::saving_thread;\ndiff --git a/src/Interpreters/TableJoin.cpp b/src/Interpreters/TableJoin.cpp\nindex efe3fd7f7403..e9fa224df111 100644\n--- a/src/Interpreters/TableJoin.cpp\n+++ b/src/Interpreters/TableJoin.cpp\n@@ -683,7 +683,7 @@ void TableJoin::inferJoinKeyCommonType(const LeftNamesAndTypes & left, const Rig\n     if (!left_type_map.empty() || !right_type_map.empty())\n     {\n         LOG_TRACE(\n-            &Poco::Logger::get(\"TableJoin\"),\n+            getLogger(\"TableJoin\"),\n             \"Infer supertype for joined columns. Left: [{}], Right: [{}]\",\n             formatTypeMap(left_type_map, left_types),\n             formatTypeMap(right_type_map, right_types));\n@@ -876,7 +876,7 @@ static void addJoinConditionWithAnd(ASTPtr & current_cond, const ASTPtr & new_co\n void TableJoin::addJoinCondition(const ASTPtr & ast, bool is_left)\n {\n     auto & cond_ast = is_left ? clauses.back().on_filter_condition_left : clauses.back().on_filter_condition_right;\n-    LOG_TRACE(&Poco::Logger::get(\"TableJoin\"), \"Adding join condition for {} table: {} -> {}\",\n+    LOG_TRACE(getLogger(\"TableJoin\"), \"Adding join condition for {} table: {} -> {}\",\n               (is_left ? \"left\" : \"right\"), ast ? queryToString(ast) : \"NULL\", cond_ast ? queryToString(cond_ast) : \"NULL\");\n     addJoinConditionWithAnd(cond_ast, ast);\n }\ndiff --git a/src/Interpreters/TemporaryDataOnDisk.cpp b/src/Interpreters/TemporaryDataOnDisk.cpp\nindex 3ad72af95f97..a48e7d8e040a 100644\n--- a/src/Interpreters/TemporaryDataOnDisk.cpp\n+++ b/src/Interpreters/TemporaryDataOnDisk.cpp\n@@ -223,7 +223,7 @@ struct TemporaryFileStream::InputReader\n         , in_compressed_buf(in_file_buf)\n         , in_reader(in_compressed_buf, header_, DBMS_TCP_PROTOCOL_VERSION)\n     {\n-        LOG_TEST(&Poco::Logger::get(\"TemporaryFileStream\"), \"Reading {} from {}\", header_.dumpStructure(), path);\n+        LOG_TEST(getLogger(\"TemporaryFileStream\"), \"Reading {} from {}\", header_.dumpStructure(), path);\n     }\n \n     explicit InputReader(const String & path, size_t size = 0)\n@@ -231,7 +231,7 @@ struct TemporaryFileStream::InputReader\n         , in_compressed_buf(in_file_buf)\n         , in_reader(in_compressed_buf, DBMS_TCP_PROTOCOL_VERSION)\n     {\n-        LOG_TEST(&Poco::Logger::get(\"TemporaryFileStream\"), \"Reading from {}\", path);\n+        LOG_TEST(getLogger(\"TemporaryFileStream\"), \"Reading from {}\", path);\n     }\n \n     Block read()\n@@ -250,7 +250,7 @@ TemporaryFileStream::TemporaryFileStream(TemporaryFileOnDiskHolder file_, const\n     , file(std::move(file_))\n     , out_writer(std::make_unique<OutputWriter>(std::make_unique<WriteBufferFromFile>(file->getAbsolutePath()), header))\n {\n-    LOG_TEST(&Poco::Logger::get(\"TemporaryFileStream\"), \"Writing to temporary file {}\", file->getAbsolutePath());\n+    LOG_TEST(getLogger(\"TemporaryFileStream\"), \"Writing to temporary file {}\", file->getAbsolutePath());\n }\n \n TemporaryFileStream::TemporaryFileStream(FileSegmentsHolderPtr segments_, const Block & header_, TemporaryDataOnDisk * parent_)\n@@ -262,7 +262,7 @@ TemporaryFileStream::TemporaryFileStream(FileSegmentsHolderPtr segments_, const\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"TemporaryFileStream can be created only from single segment\");\n     auto out_buf = std::make_unique<WriteBufferToFileSegment>(&segment_holder->front());\n \n-    LOG_TEST(&Poco::Logger::get(\"TemporaryFileStream\"), \"Writing to temporary file {}\", out_buf->getFileName());\n+    LOG_TEST(getLogger(\"TemporaryFileStream\"), \"Writing to temporary file {}\", out_buf->getFileName());\n     out_writer = std::make_unique<OutputWriter>(std::move(out_buf), header);\n }\n \ndiff --git a/src/Interpreters/TraceCollector.cpp b/src/Interpreters/TraceCollector.cpp\nindex 30fbe26d0385..1fe11be60906 100644\n--- a/src/Interpreters/TraceCollector.cpp\n+++ b/src/Interpreters/TraceCollector.cpp\n@@ -65,7 +65,7 @@ TraceCollector::~TraceCollector()\n     if (thread.joinable())\n         thread.join();\n     else\n-        LOG_ERROR(&Poco::Logger::get(\"TraceCollector\"), \"TraceCollector thread is malformed and cannot be joined\");\n+        LOG_ERROR(getLogger(\"TraceCollector\"), \"TraceCollector thread is malformed and cannot be joined\");\n }\n \n \ndiff --git a/src/Interpreters/TransactionLog.cpp b/src/Interpreters/TransactionLog.cpp\nindex a86f6110a840..96c69536c9a5 100644\n--- a/src/Interpreters/TransactionLog.cpp\n+++ b/src/Interpreters/TransactionLog.cpp\n@@ -21,7 +21,7 @@ namespace ErrorCodes\n     extern const int UNKNOWN_STATUS_OF_TRANSACTION;\n }\n \n-static void tryWriteEventToSystemLog(Poco::Logger * log, ContextPtr context,\n+static void tryWriteEventToSystemLog(LoggerPtr log, ContextPtr context,\n                                      TransactionsInfoLogElement::Type type, const TransactionID & tid, CSN csn = Tx::UnknownCSN)\n try\n {\n@@ -44,7 +44,7 @@ catch (...)\n \n TransactionLog::TransactionLog()\n     : global_context(Context::getGlobalContextInstance())\n-    , log(&Poco::Logger::get(\"TransactionLog\"))\n+    , log(getLogger(\"TransactionLog\"))\n     , zookeeper_path(global_context->getConfigRef().getString(\"transaction_log.zookeeper_path\", \"/clickhouse/txn\"))\n     , zookeeper_path_log(zookeeper_path + \"/log\")\n     , fault_probability_before_commit(global_context->getConfigRef().getDouble(\"transaction_log.fault_probability_before_commit\", 0))\ndiff --git a/src/Interpreters/TransactionLog.h b/src/Interpreters/TransactionLog.h\nindex 6e8777d85198..58847553dfda 100644\n--- a/src/Interpreters/TransactionLog.h\n+++ b/src/Interpreters/TransactionLog.h\n@@ -154,7 +154,7 @@ class TransactionLog final : public SingletonHelper<TransactionLog>\n     CSN getCSNImpl(const TIDHash & tid_hash, const std::atomic<CSN> * failback_with_strict_load_csn = nullptr) const;\n \n     const ContextPtr global_context;\n-    Poco::Logger * const log;\n+    LoggerPtr const log;\n \n     /// The newest snapshot available for reading\n     std::atomic<CSN> latest_snapshot;\ndiff --git a/src/Interpreters/TransactionVersionMetadata.cpp b/src/Interpreters/TransactionVersionMetadata.cpp\nindex 01735a798b91..7bedca5d5c75 100644\n--- a/src/Interpreters/TransactionVersionMetadata.cpp\n+++ b/src/Interpreters/TransactionVersionMetadata.cpp\n@@ -23,7 +23,7 @@ namespace ErrorCodes\n VersionMetadata::VersionMetadata()\n {\n     /// It would be better to make it static, but static loggers do not work for some reason (initialization order?)\n-    log = &Poco::Logger::get(\"VersionMetadata\");\n+    log = getLogger(\"VersionMetadata\");\n }\n \n /// It can be used for introspection purposes only\ndiff --git a/src/Interpreters/TransactionVersionMetadata.h b/src/Interpreters/TransactionVersionMetadata.h\nindex 18ac445cc29d..4309975d195b 100644\n--- a/src/Interpreters/TransactionVersionMetadata.h\n+++ b/src/Interpreters/TransactionVersionMetadata.h\n@@ -72,7 +72,7 @@ struct VersionMetadata\n \n     String toString(bool one_line = true) const;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     VersionMetadata();\n };\n \ndiff --git a/src/Interpreters/TransactionsInfoLog.cpp b/src/Interpreters/TransactionsInfoLog.cpp\nindex e893be814ca9..4a413439671e 100644\n--- a/src/Interpreters/TransactionsInfoLog.cpp\n+++ b/src/Interpreters/TransactionsInfoLog.cpp\n@@ -92,7 +92,7 @@ void TransactionsInfoLogElement::appendToBlock(MutableColumns & columns) const\n }\n \n \n-void tryWriteEventToSystemLog(Poco::Logger * log,\n+void tryWriteEventToSystemLog(LoggerPtr log,\n                               TransactionsInfoLogElement::Type type, const TransactionID & tid,\n                               const TransactionInfoContext & context)\n try\ndiff --git a/src/Interpreters/TransactionsInfoLog.h b/src/Interpreters/TransactionsInfoLog.h\nindex 0a607704e741..009d1b67474a 100644\n--- a/src/Interpreters/TransactionsInfoLog.h\n+++ b/src/Interpreters/TransactionsInfoLog.h\n@@ -54,7 +54,7 @@ class TransactionsInfoLog : public SystemLog<TransactionsInfoLogElement>\n };\n \n \n-void tryWriteEventToSystemLog(Poco::Logger * log, TransactionsInfoLogElement::Type type,\n+void tryWriteEventToSystemLog(LoggerPtr log, TransactionsInfoLogElement::Type type,\n                               const TransactionID & tid, const TransactionInfoContext & context);\n \n }\ndiff --git a/src/Interpreters/TreeRewriter.cpp b/src/Interpreters/TreeRewriter.cpp\nindex 6ed3ff2f1e6e..ecd021328e78 100644\n--- a/src/Interpreters/TreeRewriter.cpp\n+++ b/src/Interpreters/TreeRewriter.cpp\n@@ -642,13 +642,13 @@ bool tryJoinOnConst(TableJoin & analyzed_join, const ASTPtr & on_expression, Con\n         if (eval_const_res.value())\n         {\n             /// JOIN ON 1 == 1\n-            LOG_DEBUG(&Poco::Logger::get(\"TreeRewriter\"), \"Join on constant executed as cross join\");\n+            LOG_DEBUG(getLogger(\"TreeRewriter\"), \"Join on constant executed as cross join\");\n             analyzed_join.resetToCross();\n         }\n         else\n         {\n             /// JOIN ON 1 != 1\n-            LOG_DEBUG(&Poco::Logger::get(\"TreeRewriter\"), \"Join on constant executed as empty join\");\n+            LOG_DEBUG(getLogger(\"TreeRewriter\"), \"Join on constant executed as empty join\");\n             analyzed_join.resetKeys();\n         }\n         return true;\ndiff --git a/src/Interpreters/executeDDLQueryOnCluster.cpp b/src/Interpreters/executeDDLQueryOnCluster.cpp\nindex 6b6054fdae3e..6122ec6180a3 100644\n--- a/src/Interpreters/executeDDLQueryOnCluster.cpp\n+++ b/src/Interpreters/executeDDLQueryOnCluster.cpp\n@@ -221,7 +221,7 @@ class DDLQueryStatusSource final : public ISource\n     String node_path;\n     ContextPtr context;\n     Stopwatch watch;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     NameSet waiting_hosts;  /// hosts from task host list\n     NameSet finished_hosts; /// finished hosts from host list\n@@ -309,7 +309,7 @@ DDLQueryStatusSource::DDLQueryStatusSource(\n     , node_path(zk_node_path)\n     , context(context_)\n     , watch(CLOCK_MONOTONIC_COARSE)\n-    , log(&Poco::Logger::get(\"DDLQueryStatusSource\"))\n+    , log(getLogger(\"DDLQueryStatusSource\"))\n {\n     auto output_mode = context->getSettingsRef().distributed_ddl_output_mode;\n     throw_on_timeout = output_mode == DistributedDDLOutputMode::THROW || output_mode == DistributedDDLOutputMode::THROW_ONLY_ACTIVE\n@@ -382,7 +382,7 @@ Chunk DDLQueryStatusSource::generateChunkWithUnfinishedHosts() const\n     return Chunk(std::move(columns), unfinished_hosts.size());\n }\n \n-static NameSet getOfflineHosts(const String & node_path, const NameSet & hosts_to_wait, const ZooKeeperPtr & zookeeper, Poco::Logger * log)\n+static NameSet getOfflineHosts(const String & node_path, const NameSet & hosts_to_wait, const ZooKeeperPtr & zookeeper, LoggerPtr log)\n {\n     fs::path replicas_path;\n     if (node_path.ends_with('/'))\n@@ -470,7 +470,7 @@ Chunk DDLQueryStatusSource::generate()\n \n         {\n             auto retries_ctl = ZooKeeperRetriesControl(\n-                \"executeDDLQueryOnCluster\", &Poco::Logger::get(\"DDLQueryStatusSource\"), getRetriesInfo(), context->getProcessListElement());\n+                \"executeDDLQueryOnCluster\", getLogger(\"DDLQueryStatusSource\"), getRetriesInfo(), context->getProcessListElement());\n             retries_ctl.retryLoop([&]()\n             {\n                 auto zookeeper = context->getZooKeeper();\n@@ -540,7 +540,7 @@ Chunk DDLQueryStatusSource::generate()\n \n                 auto retries_ctl = ZooKeeperRetriesControl(\n                     \"executeDDLQueryOnCluster\",\n-                    &Poco::Logger::get(\"DDLQueryStatusSource\"),\n+                    getLogger(\"DDLQueryStatusSource\"),\n                     getRetriesInfo(),\n                     context->getProcessListElement());\n                 retries_ctl.retryLoop([&]()\ndiff --git a/src/Interpreters/executeQuery.cpp b/src/Interpreters/executeQuery.cpp\nindex 533d58aaa8f2..a377d2e0b971 100644\n--- a/src/Interpreters/executeQuery.cpp\n+++ b/src/Interpreters/executeQuery.cpp\n@@ -119,7 +119,7 @@ static void logQuery(const String & query, ContextPtr context, bool internal, Qu\n {\n     if (internal)\n     {\n-        LOG_DEBUG(&Poco::Logger::get(\"executeQuery\"), \"(internal) {} (stage: {})\", toOneLineQuery(query), QueryProcessingStage::toString(stage));\n+        LOG_DEBUG(getLogger(\"executeQuery\"), \"(internal) {} (stage: {})\", toOneLineQuery(query), QueryProcessingStage::toString(stage));\n     }\n     else\n     {\n@@ -142,7 +142,7 @@ static void logQuery(const String & query, ContextPtr context, bool internal, Qu\n         if (auto txn = context->getCurrentTransaction())\n             transaction_info = fmt::format(\" (TID: {}, TIDH: {})\", txn->tid, txn->tid.getHash());\n \n-        LOG_DEBUG(&Poco::Logger::get(\"executeQuery\"), \"(from {}{}{}){}{} {} (stage: {})\",\n+        LOG_DEBUG(getLogger(\"executeQuery\"), \"(from {}{}{}){}{} {} (stage: {})\",\n             client_info.current_address.toString(),\n             (current_user != \"default\" ? \", user: \" + current_user : \"\"),\n             (!initial_query_id.empty() && current_query_id != initial_query_id ? \", initial_query_id: \" + initial_query_id : std::string()),\n@@ -153,7 +153,7 @@ static void logQuery(const String & query, ContextPtr context, bool internal, Qu\n \n         if (client_info.client_trace_context.trace_id != UUID())\n         {\n-            LOG_TRACE(&Poco::Logger::get(\"executeQuery\"),\n+            LOG_TRACE(getLogger(\"executeQuery\"),\n                 \"OpenTelemetry traceparent '{}'\",\n                 client_info.client_trace_context.composeTraceparentHeader());\n         }\n@@ -207,9 +207,9 @@ static void logException(ContextPtr context, QueryLogElement & elem, bool log_er\n             elem.stack_trace);\n \n     if (log_error)\n-        LOG_ERROR(&Poco::Logger::get(\"executeQuery\"), message);\n+        LOG_ERROR(getLogger(\"executeQuery\"), message);\n     else\n-        LOG_INFO(&Poco::Logger::get(\"executeQuery\"), message);\n+        LOG_INFO(getLogger(\"executeQuery\"), message);\n }\n \n static void\n@@ -396,7 +396,7 @@ void logQueryFinish(\n             double elapsed_seconds = static_cast<double>(info.elapsed_microseconds) / 1000000.0;\n             double rows_per_second = static_cast<double>(elem.read_rows) / elapsed_seconds;\n             LOG_DEBUG(\n-                &Poco::Logger::get(\"executeQuery\"),\n+                getLogger(\"executeQuery\"),\n                 \"Read {} rows, {} in {} sec., {} rows/sec., {}/sec.\",\n                 elem.read_rows,\n                 ReadableSize(elem.read_bytes),\n@@ -660,7 +660,7 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n     /// we still have enough span logs for the execution of external queries.\n     std::shared_ptr<OpenTelemetry::SpanHolder> query_span = internal ? nullptr : std::make_shared<OpenTelemetry::SpanHolder>(\"query\");\n     if (query_span && query_span->trace_id != UUID{})\n-        LOG_TRACE(&Poco::Logger::get(\"executeQuery\"), \"Query span trace_id for opentelemetry log: {}\", query_span->trace_id);\n+        LOG_TRACE(getLogger(\"executeQuery\"), \"Query span trace_id for opentelemetry log: {}\", query_span->trace_id);\n \n     auto query_start_time = std::chrono::system_clock::now();\n \n@@ -925,7 +925,7 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n \n         bool async_insert = false;\n         auto * queue = context->getAsynchronousInsertQueue();\n-        auto * logger = &Poco::Logger::get(\"executeQuery\");\n+        auto logger = getLogger(\"executeQuery\");\n \n         if (insert_query && async_insert_enabled)\n         {\n@@ -1131,7 +1131,7 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n                             const size_t num_query_runs = query_cache->recordQueryRun(key);\n                             if (num_query_runs <= settings.query_cache_min_query_runs)\n                             {\n-                                LOG_TRACE(&Poco::Logger::get(\"QueryCache\"),\n+                                LOG_TRACE(getLogger(\"QueryCache\"),\n                                         \"Skipped insert because the query ran {} times but the minimum required number of query runs to cache the query result is {}\",\n                                         num_query_runs, settings.query_cache_min_query_runs);\n                             }\n@@ -1387,7 +1387,7 @@ void executeQuery(\n             catch (const DB::Exception & e)\n             {\n                 /// Ignore this exception and report the original one\n-                LOG_WARNING(&Poco::Logger::get(\"executeQuery\"), getExceptionMessageAndPattern(e, true));\n+                LOG_WARNING(getLogger(\"executeQuery\"), getExceptionMessageAndPattern(e, true));\n             }\n         }\n     };\ndiff --git a/src/Interpreters/loadMetadata.cpp b/src/Interpreters/loadMetadata.cpp\nindex a2d2c56c710f..0b7a6dc92b07 100644\n--- a/src/Interpreters/loadMetadata.cpp\n+++ b/src/Interpreters/loadMetadata.cpp\n@@ -156,7 +156,7 @@ static void checkIncompleteOrdinaryToAtomicConversion(ContextPtr context, const\n \n LoadTaskPtrs loadMetadata(ContextMutablePtr context, const String & default_database_name, bool async_load_databases)\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"loadMetadata\");\n+    LoggerPtr log = getLogger(\"loadMetadata\");\n \n     String path = context->getPath() + \"metadata\";\n \n@@ -290,7 +290,7 @@ static void loadSystemDatabaseImpl(ContextMutablePtr context, const String & dat\n     }\n }\n \n-static void convertOrdinaryDatabaseToAtomic(Poco::Logger * log, ContextMutablePtr context, const DatabasePtr & database,\n+static void convertOrdinaryDatabaseToAtomic(LoggerPtr log, ContextMutablePtr context, const DatabasePtr & database,\n                                             const String & name, const String tmp_name)\n {\n     /// It's kind of C++ script that creates temporary database with Atomic engine,\n@@ -369,7 +369,7 @@ static void convertOrdinaryDatabaseToAtomic(Poco::Logger * log, ContextMutablePt\n /// Can be called only during server startup when there are no queries from users.\n static void maybeConvertOrdinaryDatabaseToAtomic(ContextMutablePtr context, const String & database_name, LoadTaskPtrs * startup_tasks = nullptr)\n {\n-    Poco::Logger * log = &Poco::Logger::get(\"loadMetadata\");\n+    LoggerPtr log = getLogger(\"loadMetadata\");\n \n     auto database = DatabaseCatalog::instance().getDatabase(database_name);\n     if (!database)\n@@ -482,7 +482,7 @@ void convertDatabasesEnginesIfNeed(const LoadTaskPtrs & load_metadata, ContextMu\n     if (!fs::exists(convert_flag_path))\n         return;\n \n-    LOG_INFO(&Poco::Logger::get(\"loadMetadata\"), \"Found convert_ordinary_to_atomic file in flags directory, \"\n+    LOG_INFO(getLogger(\"loadMetadata\"), \"Found convert_ordinary_to_atomic file in flags directory, \"\n                                                  \"will try to convert all Ordinary databases to Atomic\");\n \n     // Wait for all table to be loaded and started\n@@ -492,7 +492,7 @@ void convertDatabasesEnginesIfNeed(const LoadTaskPtrs & load_metadata, ContextMu\n         if (name != DatabaseCatalog::SYSTEM_DATABASE)\n             maybeConvertOrdinaryDatabaseToAtomic(context, name);\n \n-    LOG_INFO(&Poco::Logger::get(\"loadMetadata\"), \"Conversion finished, removing convert_ordinary_to_atomic flag\");\n+    LOG_INFO(getLogger(\"loadMetadata\"), \"Conversion finished, removing convert_ordinary_to_atomic flag\");\n     fs::remove(convert_flag_path);\n }\n \ndiff --git a/src/Interpreters/removeOnClusterClauseIfNeeded.cpp b/src/Interpreters/removeOnClusterClauseIfNeeded.cpp\nindex f8df03ed8306..44167fe72424 100644\n--- a/src/Interpreters/removeOnClusterClauseIfNeeded.cpp\n+++ b/src/Interpreters/removeOnClusterClauseIfNeeded.cpp\n@@ -52,7 +52,7 @@ ASTPtr removeOnClusterClauseIfNeeded(const ASTPtr & query, ContextPtr context, c\n             && context->getSettings().ignore_on_cluster_for_replicated_access_entities_queries\n             && context->getAccessControl().containsStorage(ReplicatedAccessStorage::STORAGE_TYPE)))\n     {\n-        LOG_DEBUG(&Poco::Logger::get(\"removeOnClusterClauseIfNeeded\"), \"ON CLUSTER clause was ignored for query {}\", query->getID());\n+        LOG_DEBUG(getLogger(\"removeOnClusterClauseIfNeeded\"), \"ON CLUSTER clause was ignored for query {}\", query->getID());\n         return query_on_cluster->getRewrittenASTWithoutOnCluster(params);\n     }\n \ndiff --git a/src/Parsers/DumpASTNode.h b/src/Parsers/DumpASTNode.h\nindex 60fcece55904..5efc0e018f47 100644\n--- a/src/Parsers/DumpASTNode.h\n+++ b/src/Parsers/DumpASTNode.h\n@@ -165,7 +165,7 @@ class DebugASTLog\n         : log(nullptr)\n     {\n         if constexpr (_enable)\n-            log = &Poco::Logger::get(\"AST\");\n+            log = getLogger(\"AST\");\n     }\n \n     ~DebugASTLog()\n@@ -177,7 +177,7 @@ class DebugASTLog\n     WriteBuffer * stream() { return (_enable ? &buf : nullptr); }\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     WriteBufferFromOwnString buf;\n };\n \ndiff --git a/src/Planner/Planner.cpp b/src/Planner/Planner.cpp\nindex f2def5713257..b55f0e440388 100644\n--- a/src/Planner/Planner.cpp\n+++ b/src/Planner/Planner.cpp\n@@ -1196,7 +1196,7 @@ void Planner::buildQueryPlanIfNeeded()\n     if (query_plan.isInitialized())\n         return;\n \n-    LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"Query {} to stage {}{}\",\n+    LOG_TRACE(getLogger(\"Planner\"), \"Query {} to stage {}{}\",\n         query_tree->formatConvertedASTForErrorMessage(),\n         QueryProcessingStage::toString(select_query_options.to_stage),\n         select_query_options.only_analyze ? \" only analyze\" : \"\");\n@@ -1355,7 +1355,7 @@ void Planner::buildPlanForQueryNode()\n \n             auto & mutable_context = planner_context->getMutableQueryContext();\n             mutable_context->setSetting(\"allow_experimental_parallel_reading_from_replicas\", Field(0));\n-            LOG_DEBUG(&Poco::Logger::get(\"Planner\"), \"Disabling parallel replicas to execute a query with IN with subquery\");\n+            LOG_DEBUG(getLogger(\"Planner\"), \"Disabling parallel replicas to execute a query with IN with subquery\");\n         }\n     }\n \n@@ -1382,7 +1382,7 @@ void Planner::buildPlanForQueryNode()\n                 else\n                 {\n                     LOG_DEBUG(\n-                        &Poco::Logger::get(\"Planner\"),\n+                        getLogger(\"Planner\"),\n                         \"FINAL modifier is not supported with parallel replicas. Query will be executed without using them.\");\n                     auto & mutable_context = planner_context->getMutableQueryContext();\n                     mutable_context->setSetting(\"allow_experimental_parallel_reading_from_replicas\", Field(0));\n@@ -1401,7 +1401,7 @@ void Planner::buildPlanForQueryNode()\n             else\n             {\n                 LOG_DEBUG(\n-                    &Poco::Logger::get(\"Planner\"),\n+                    getLogger(\"Planner\"),\n                     \"JOINs are not supported with parallel replicas. Query will be executed without using them.\");\n \n                 auto & mutable_context = planner_context->getMutableQueryContext();\n@@ -1422,7 +1422,7 @@ void Planner::buildPlanForQueryNode()\n     query_plan = std::move(join_tree_query_plan.query_plan);\n     used_row_policies = std::move(join_tree_query_plan.used_row_policies);\n \n-    LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"Query {} from stage {} to stage {}{}\",\n+    LOG_TRACE(getLogger(\"Planner\"), \"Query {} from stage {} to stage {}{}\",\n         query_tree->formatConvertedASTForErrorMessage(),\n         QueryProcessingStage::toString(from_stage),\n         QueryProcessingStage::toString(select_query_options.to_stage),\ndiff --git a/src/Planner/PlannerJoinTree.cpp b/src/Planner/PlannerJoinTree.cpp\nindex 552f25d70358..ab25f6d24234 100644\n--- a/src/Planner/PlannerJoinTree.cpp\n+++ b/src/Planner/PlannerJoinTree.cpp\n@@ -276,7 +276,7 @@ bool applyTrivialCountIfPossible(\n         /// The query could use trivial count if it didn't use parallel replicas, so let's disable it\n         query_context->setSetting(\"allow_experimental_parallel_reading_from_replicas\", Field(0));\n         query_context->setSetting(\"max_parallel_replicas\", UInt64{0});\n-        LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"Disabling parallel replicas to be able to use a trivial count optimization\");\n+        LOG_TRACE(getLogger(\"Planner\"), \"Disabling parallel replicas to be able to use a trivial count optimization\");\n \n     }\n \n@@ -478,7 +478,7 @@ FilterDAGInfo buildCustomKeyFilterIfNeeded(const StoragePtr & storage,\n                 \"(setting 'max_parallel_replcias'), but the table does not have custom_key defined for it \"\n                 \" or it's invalid (setting 'parallel_replicas_custom_key')\");\n \n-    LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"Processing query on a replica using custom_key '{}'\", settings.parallel_replicas_custom_key.value);\n+    LOG_TRACE(getLogger(\"Planner\"), \"Processing query on a replica using custom_key '{}'\", settings.parallel_replicas_custom_key.value);\n \n     auto parallel_replicas_custom_filter_ast = getCustomKeyFilterForParallelReplica(\n             settings.parallel_replicas_count,\n@@ -725,7 +725,7 @@ JoinTreeQueryPlan buildQueryPlanForTableExpression(QueryTreeNodePtr table_expres\n \n                     size_t number_of_replicas_to_use = rows_to_read / settings.parallel_replicas_min_number_of_rows_per_replica;\n                     LOG_TRACE(\n-                        &Poco::Logger::get(\"Planner\"),\n+                        getLogger(\"Planner\"),\n                         \"Estimated {} rows to read. It is enough work for {} parallel replicas\",\n                         rows_to_read,\n                         number_of_replicas_to_use);\n@@ -735,12 +735,12 @@ JoinTreeQueryPlan buildQueryPlanForTableExpression(QueryTreeNodePtr table_expres\n                         planner_context->getMutableQueryContext()->setSetting(\n                             \"allow_experimental_parallel_reading_from_replicas\", Field(0));\n                         planner_context->getMutableQueryContext()->setSetting(\"max_parallel_replicas\", UInt64{0});\n-                        LOG_DEBUG(&Poco::Logger::get(\"Planner\"), \"Disabling parallel replicas because there aren't enough rows to read\");\n+                        LOG_DEBUG(getLogger(\"Planner\"), \"Disabling parallel replicas because there aren't enough rows to read\");\n                     }\n                     else if (number_of_replicas_to_use < settings.max_parallel_replicas)\n                     {\n                         planner_context->getMutableQueryContext()->setSetting(\"max_parallel_replicas\", number_of_replicas_to_use);\n-                        LOG_DEBUG(&Poco::Logger::get(\"Planner\"), \"Reducing the number of replicas to use to {}\", number_of_replicas_to_use);\n+                        LOG_DEBUG(getLogger(\"Planner\"), \"Reducing the number of replicas to use to {}\", number_of_replicas_to_use);\n                     }\n                 }\n \ndiff --git a/src/Planner/PlannerJoins.cpp b/src/Planner/PlannerJoins.cpp\nindex 9b249d21a247..94ee249106a5 100644\n--- a/src/Planner/PlannerJoins.cpp\n+++ b/src/Planner/PlannerJoins.cpp\n@@ -388,8 +388,8 @@ JoinClausesAndActions buildJoinClausesAndActions(//const ColumnsWithTypeAndName\n     ActionsDAGPtr left_join_actions = std::make_shared<ActionsDAG>(left_table_expression_columns);\n     ActionsDAGPtr right_join_actions = std::make_shared<ActionsDAG>(right_table_expression_columns);\n \n-    // LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"buildJoinClausesAndActions cols {} \", left_join_actions->dumpDAG());\n-    // LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"buildJoinClausesAndActions cols {} \", right_join_actions->dumpDAG());\n+    // LOG_TRACE(getLogger(\"Planner\"), \"buildJoinClausesAndActions cols {} \", left_join_actions->dumpDAG());\n+    // LOG_TRACE(getLogger(\"Planner\"), \"buildJoinClausesAndActions cols {} \", right_join_actions->dumpDAG());\n \n     /** In ActionsDAG if input node has constant representation additional constant column is added.\n       * That way we cannot simply check that node has INPUT type during resolution of expression join table side.\n@@ -411,8 +411,8 @@ JoinClausesAndActions buildJoinClausesAndActions(//const ColumnsWithTypeAndName\n       * ON (t1.id = t2.id) AND 1 != 1 AND (t1.value >= t1.value);\n       */\n     auto join_expression = join_node.getJoinExpression();\n-    // LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"buildJoinClausesAndActions expr {} \", join_expression->formatConvertedASTForErrorMessage());\n-    // LOG_TRACE(&Poco::Logger::get(\"Planner\"), \"buildJoinClausesAndActions expr {} \", join_expression->dumpTree());\n+    // LOG_TRACE(getLogger(\"Planner\"), \"buildJoinClausesAndActions expr {} \", join_expression->formatConvertedASTForErrorMessage());\n+    // LOG_TRACE(getLogger(\"Planner\"), \"buildJoinClausesAndActions expr {} \", join_expression->dumpTree());\n \n     auto * constant_join_expression = join_expression->as<ConstantNode>();\n \ndiff --git a/src/Processors/Executors/PipelineExecutor.h b/src/Processors/Executors/PipelineExecutor.h\nindex dee12dad2829..862a460f0ed2 100644\n--- a/src/Processors/Executors/PipelineExecutor.h\n+++ b/src/Processors/Executors/PipelineExecutor.h\n@@ -83,7 +83,7 @@ class PipelineExecutor\n     std::atomic_bool cancelled = false;\n     std::atomic_bool cancelled_reading = false;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"PipelineExecutor\");\n+    LoggerPtr log = getLogger(\"PipelineExecutor\");\n \n     /// Now it's used to check if query was killed.\n     QueryStatusPtr process_list_element;\ndiff --git a/src/Processors/Formats/IRowInputFormat.cpp b/src/Processors/Formats/IRowInputFormat.cpp\nindex 5f27fa78c55d..8c317a34a9d5 100644\n--- a/src/Processors/Formats/IRowInputFormat.cpp\n+++ b/src/Processors/Formats/IRowInputFormat.cpp\n@@ -230,7 +230,7 @@ Chunk IRowInputFormat::read()\n     {\n         if (num_errors && (params.allow_errors_num > 0 || params.allow_errors_ratio > 0))\n         {\n-            Poco::Logger * log = &Poco::Logger::get(\"IRowInputFormat\");\n+            LoggerPtr log = getLogger(\"IRowInputFormat\");\n             LOG_DEBUG(log, \"Skipped {} rows with errors while reading the input stream\", num_errors);\n         }\n \ndiff --git a/src/Processors/Formats/ISchemaReader.cpp b/src/Processors/Formats/ISchemaReader.cpp\nindex 26c632b83dc5..79b7ca17a5af 100644\n--- a/src/Processors/Formats/ISchemaReader.cpp\n+++ b/src/Processors/Formats/ISchemaReader.cpp\n@@ -91,7 +91,7 @@ void IIRowSchemaReader::setContext(ContextPtr & context)\n     }\n     else\n     {\n-        LOG_WARNING(&Poco::Logger::get(\"IIRowSchemaReader\"), \"Couldn't parse schema inference hints: {}. This setting will be ignored\", hints_parsing_error);\n+        LOG_WARNING(getLogger(\"IIRowSchemaReader\"), \"Couldn't parse schema inference hints: {}. This setting will be ignored\", hints_parsing_error);\n     }\n }\n \ndiff --git a/src/Processors/Formats/Impl/AvroRowInputFormat.cpp b/src/Processors/Formats/Impl/AvroRowInputFormat.cpp\nindex 5722c6600717..8dc8fa516dc0 100644\n--- a/src/Processors/Formats/Impl/AvroRowInputFormat.cpp\n+++ b/src/Processors/Formats/Impl/AvroRowInputFormat.cpp\n@@ -984,7 +984,7 @@ class AvroConfluentRowInputFormat::SchemaRegistry\n             try\n             {\n                 Poco::URI url(base_url, base_url.getPath() + \"/schemas/ids/\" + std::to_string(id));\n-                LOG_TRACE((&Poco::Logger::get(\"AvroConfluentRowInputFormat\")), \"Fetching schema id = {} from url {}\", id, url.toString());\n+                LOG_TRACE((getLogger(\"AvroConfluentRowInputFormat\")), \"Fetching schema id = {} from url {}\", id, url.toString());\n \n                 /// One second for connect/send/receive. Just in case.\n                 auto timeouts = ConnectionTimeouts()\n@@ -1029,7 +1029,7 @@ class AvroConfluentRowInputFormat::SchemaRegistry\n                 markSessionForReuse(session);\n \n                 auto schema = json_body->getValue<std::string>(\"schema\");\n-                LOG_TRACE((&Poco::Logger::get(\"AvroConfluentRowInputFormat\")), \"Successfully fetched schema id = {}\\n{}\", id, schema);\n+                LOG_TRACE((getLogger(\"AvroConfluentRowInputFormat\")), \"Successfully fetched schema id = {}\\n{}\", id, schema);\n                 return avro::compileJsonSchemaFromString(schema);\n             }\n             catch (const Exception &)\ndiff --git a/src/Processors/Formats/Impl/DWARFBlockInputFormat.cpp b/src/Processors/Formats/Impl/DWARFBlockInputFormat.cpp\nindex 43ef25210325..b655e892d3b4 100644\n--- a/src/Processors/Formats/Impl/DWARFBlockInputFormat.cpp\n+++ b/src/Processors/Formats/Impl/DWARFBlockInputFormat.cpp\n@@ -198,7 +198,7 @@ void DWARFBlockInputFormat::initializeIfNeeded()\n     if (elf.has_value())\n         return;\n \n-    LOG_DEBUG(&Poco::Logger::get(\"DWARF\"), \"Opening ELF\");\n+    LOG_DEBUG(getLogger(\"DWARF\"), \"Opening ELF\");\n     initELF();\n     if (is_stopped)\n         return;\n@@ -209,7 +209,7 @@ void DWARFBlockInputFormat::initializeIfNeeded()\n     auto abbrev_section = elf->findSectionByName(\".debug_abbrev\");\n     if (!abbrev_section.has_value())\n         throw Exception(ErrorCodes::CANNOT_PARSE_ELF, \"No .debug_abbrev section\");\n-    LOG_DEBUG(&Poco::Logger::get(\"DWARF\"), \".debug_abbrev is {:.3f} MiB, .debug_info is {:.3f} MiB\", abbrev_section->size() * 1. / (1 << 20), info_section->size() * 1. / (1 << 20));\n+    LOG_DEBUG(getLogger(\"DWARF\"), \".debug_abbrev is {:.3f} MiB, .debug_info is {:.3f} MiB\", abbrev_section->size() * 1. / (1 << 20), info_section->size() * 1. / (1 << 20));\n \n     /// (The StringRef points into Elf's mmap of the whole file, or into file_contents.)\n     extractor.emplace(llvm::StringRef(info_section->begin(), info_section->size()), /*IsLittleEndian*/ true, /*AddressSize*/ 8);\n@@ -237,7 +237,7 @@ void DWARFBlockInputFormat::initializeIfNeeded()\n     for (std::unique_ptr<llvm::DWARFUnit> & unit : dwarf_context->info_section_units())\n         units_queue.emplace_back(unit.get());\n \n-    LOG_DEBUG(&Poco::Logger::get(\"DWARF\"), \"{} units, reading in {} threads\", units_queue.size(), num_threads);\n+    LOG_DEBUG(getLogger(\"DWARF\"), \"{} units, reading in {} threads\", units_queue.size(), num_threads);\n \n     pool.emplace(CurrentMetrics::DWARFReaderThreads, CurrentMetrics::DWARFReaderThreadsActive, CurrentMetrics::DWARFReaderThreadsScheduled, num_threads);\n     for (size_t i = 0; i < num_threads; ++i)\n@@ -782,7 +782,7 @@ void DWARFBlockInputFormat::parseFilenameTable(UnitState & unit, uint64_t offset\n     auto error = prologue.parse(*debug_line_extractor, &offset, /*RecoverableErrorHandler*/ [&](auto e)\n         {\n             if (++seen_debug_line_warnings < 10)\n-                LOG_INFO(&Poco::Logger::get(\"DWARF\"), \"Parsing error: {}\", llvm::toString(std::move(e)));\n+                LOG_INFO(getLogger(\"DWARF\"), \"Parsing error: {}\", llvm::toString(std::move(e)));\n         }, *dwarf_context, unit.dwarf_unit);\n \n     if (error)\ndiff --git a/src/Processors/Formats/Impl/ParallelFormattingOutputFormat.h b/src/Processors/Formats/Impl/ParallelFormattingOutputFormat.h\nindex c2f08479730e..50a736463594 100644\n--- a/src/Processors/Formats/Impl/ParallelFormattingOutputFormat.h\n+++ b/src/Processors/Formats/Impl/ParallelFormattingOutputFormat.h\n@@ -84,7 +84,7 @@ class ParallelFormattingOutputFormat : public IOutputFormat\n         , pool(CurrentMetrics::ParallelFormattingOutputFormatThreads, CurrentMetrics::ParallelFormattingOutputFormatThreadsActive, CurrentMetrics::ParallelFormattingOutputFormatThreadsScheduled, params.max_threads_for_parallel_formatting)\n \n     {\n-        LOG_TEST(&Poco::Logger::get(\"ParallelFormattingOutputFormat\"), \"Parallel formatting is being used\");\n+        LOG_TEST(getLogger(\"ParallelFormattingOutputFormat\"), \"Parallel formatting is being used\");\n \n         NullWriteBuffer buf;\n         save_totals_and_extremes_in_statistics = internal_formatter_creator(buf)->areTotalsAndExtremesUsedInFinalize();\ndiff --git a/src/Processors/Formats/Impl/ParallelParsingInputFormat.h b/src/Processors/Formats/Impl/ParallelParsingInputFormat.h\nindex ff97afa83487..c4736ceea3af 100644\n--- a/src/Processors/Formats/Impl/ParallelParsingInputFormat.h\n+++ b/src/Processors/Formats/Impl/ParallelParsingInputFormat.h\n@@ -111,7 +111,7 @@ class ParallelParsingInputFormat : public IInputFormat\n         // bump into reader thread on wraparound.\n         processing_units.resize(params.max_threads + 2);\n \n-        LOG_TRACE(&Poco::Logger::get(\"ParallelParsingInputFormat\"), \"Parallel parsing is used\");\n+        LOG_TRACE(getLogger(\"ParallelParsingInputFormat\"), \"Parallel parsing is used\");\n     }\n \n     ~ParallelParsingInputFormat() override\ndiff --git a/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp b/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp\nindex aa193ffd36a0..3e61bfbc7942 100644\n--- a/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp\n+++ b/src/Processors/Formats/Impl/ValuesBlockInputFormat.cpp\n@@ -492,7 +492,7 @@ bool ValuesBlockInputFormat::parseExpression(IColumn & column, size_t column_idx\n                 &found_in_cache,\n                 delimiter);\n \n-            LOG_TEST(&Poco::Logger::get(\"ValuesBlockInputFormat\"), \"Will use an expression template to parse column {}: {}\",\n+            LOG_TEST(getLogger(\"ValuesBlockInputFormat\"), \"Will use an expression template to parse column {}: {}\",\n                      column_idx, structure->dumpTemplate());\n \n             templates[column_idx].emplace(structure);\ndiff --git a/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.cpp b/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.cpp\nindex 14325223602e..8948cee217ca 100644\n--- a/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.cpp\n+++ b/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.cpp\n@@ -28,7 +28,7 @@ CollapsingSortedAlgorithm::CollapsingSortedAlgorithm(\n     bool only_positive_sign_,\n     size_t max_block_size_rows_,\n     size_t max_block_size_bytes_,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     WriteBuffer * out_row_sources_buf_,\n     bool use_average_block_sizes)\n     : IMergingAlgorithmWithSharedChunks(header_, num_inputs, std::move(description_), out_row_sources_buf_, max_row_refs)\ndiff --git a/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.h b/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.h\nindex 28bb87cb394f..be1a3a3bf33d 100644\n--- a/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.h\n+++ b/src/Processors/Merges/Algorithms/CollapsingSortedAlgorithm.h\n@@ -34,7 +34,7 @@ class CollapsingSortedAlgorithm final : public IMergingAlgorithmWithSharedChunks\n         bool only_positive_sign_, /// For select final. Skip rows with sum(sign) < 0.\n         size_t max_block_size_rows_,\n         size_t max_block_size_bytes_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         WriteBuffer * out_row_sources_buf_ = nullptr,\n         bool use_average_block_sizes = false);\n \n@@ -64,7 +64,7 @@ class CollapsingSortedAlgorithm final : public IMergingAlgorithmWithSharedChunks\n     PODArray<RowSourcePart> current_row_sources;   /// Sources of rows with the current primary key\n \n     size_t count_incorrect_data = 0;    /// To prevent too many error messages from writing to the log.\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     void reportIncorrectData();\n     void insertRow(RowRef & row);\ndiff --git a/src/Processors/Merges/Algorithms/RowRef.h b/src/Processors/Merges/Algorithms/RowRef.h\nindex 81969cd19880..ee64224d44df 100644\n--- a/src/Processors/Merges/Algorithms/RowRef.h\n+++ b/src/Processors/Merges/Algorithms/RowRef.h\n@@ -86,7 +86,7 @@ class SharedChunkAllocator\n     {\n         if (free_chunks.size() != chunks.size())\n         {\n-            LOG_ERROR(&Poco::Logger::get(\"SharedChunkAllocator\"), \"SharedChunkAllocator was destroyed before RowRef was released. StackTrace: {}\", StackTrace().toString());\n+            LOG_ERROR(getLogger(\"SharedChunkAllocator\"), \"SharedChunkAllocator was destroyed before RowRef was released. StackTrace: {}\", StackTrace().toString());\n \n             return;\n         }\n@@ -103,7 +103,7 @@ class SharedChunkAllocator\n             /// This may happen if allocator was removed before chunks.\n             /// Log message and exit, because we don't want to throw exception in destructor.\n \n-            LOG_ERROR(&Poco::Logger::get(\"SharedChunkAllocator\"), \"SharedChunkAllocator was destroyed before RowRef was released. StackTrace: {}\", StackTrace().toString());\n+            LOG_ERROR(getLogger(\"SharedChunkAllocator\"), \"SharedChunkAllocator was destroyed before RowRef was released. StackTrace: {}\", StackTrace().toString());\n \n             return;\n         }\ndiff --git a/src/Processors/Merges/CollapsingSortedTransform.h b/src/Processors/Merges/CollapsingSortedTransform.h\nindex b0cb6bc6d624..4479ac82f66d 100644\n--- a/src/Processors/Merges/CollapsingSortedTransform.h\n+++ b/src/Processors/Merges/CollapsingSortedTransform.h\n@@ -29,7 +29,7 @@ class CollapsingSortedTransform final : public IMergingTransform<CollapsingSorte\n             only_positive_sign,\n             max_block_size_rows,\n             max_block_size_bytes,\n-            &Poco::Logger::get(\"CollapsingSortedTransform\"),\n+            getLogger(\"CollapsingSortedTransform\"),\n             out_row_sources_buf_,\n             use_average_block_sizes)\n     {\ndiff --git a/src/Processors/Merges/MergingSortedTransform.cpp b/src/Processors/Merges/MergingSortedTransform.cpp\nindex 62275f37857f..338b1ff79353 100644\n--- a/src/Processors/Merges/MergingSortedTransform.cpp\n+++ b/src/Processors/Merges/MergingSortedTransform.cpp\n@@ -53,7 +53,7 @@ void MergingSortedTransform::onFinish()\n \n     const auto & merged_data = algorithm.getMergedData();\n \n-    auto * log = &Poco::Logger::get(\"MergingSortedTransform\");\n+    auto log = getLogger(\"MergingSortedTransform\");\n \n     double seconds = total_stopwatch.elapsedSeconds();\n \ndiff --git a/src/Processors/QueryPlan/CreateSetAndFilterOnTheFlyStep.h b/src/Processors/QueryPlan/CreateSetAndFilterOnTheFlyStep.h\nindex 023901dba026..70ae4c6156e1 100644\n--- a/src/Processors/QueryPlan/CreateSetAndFilterOnTheFlyStep.h\n+++ b/src/Processors/QueryPlan/CreateSetAndFilterOnTheFlyStep.h\n@@ -54,7 +54,7 @@ class CreateSetAndFilterOnTheFlyStep : public ITransformingStep\n \n     JoinTableSide position;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"CreateSetAndFilterOnTheFlyStep\");\n+    LoggerPtr log = getLogger(\"CreateSetAndFilterOnTheFlyStep\");\n };\n \n }\ndiff --git a/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp b/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp\nindex 3b31a809f9df..e71bcc5602aa 100644\n--- a/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp\n@@ -383,7 +383,7 @@ size_t tryPushDownFilter(QueryPlan::Node * parent_node, QueryPlan::Nodes & nodes\n             const size_t updated_steps = tryAddNewFilterStep(parent_node, nodes, split_filter, can_remove_filter, child_idx);\n             if (updated_steps > 0)\n             {\n-                LOG_DEBUG(&Poco::Logger::get(\"QueryPlanOptimizations\"), \"Pushed down filter {} to the {} side of join\", split_filter_column_name, kind);\n+                LOG_DEBUG(getLogger(\"QueryPlanOptimizations\"), \"Pushed down filter {} to the {} side of join\", split_filter_column_name, kind);\n             }\n             return updated_steps;\n         };\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizePrewhere.cpp b/src/Processors/QueryPlan/Optimizations/optimizePrewhere.cpp\nindex 5c5171d4296d..7902b36f80ec 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizePrewhere.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizePrewhere.cpp\n@@ -164,7 +164,7 @@ void optimizePrewhere(Stack & stack, QueryPlan::Nodes & nodes)\n         storage.getConditionEstimatorByPredicate(read_from_merge_tree->getQueryInfo(), storage_snapshot, context),\n         queried_columns,\n         storage.supportedPrewhereColumns(),\n-        &Poco::Logger::get(\"QueryPlanOptimizePrewhere\")};\n+        getLogger(\"QueryPlanOptimizePrewhere\")};\n \n     auto optimize_result = where_optimizer.optimize(filter_step->getExpression(),\n         filter_step->getFilterColumnName(),\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp b/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\nindex c3e651154ae7..534716cc60e9 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\n@@ -123,7 +123,7 @@ std::optional<AggregateFunctionMatches> matchAggregateFunctions(\n         if (it == projection_aggregate_functions.end())\n         {\n             // LOG_TRACE(\n-            //     &Poco::Logger::get(\"optimizeUseProjections\"),\n+            //     getLogger(\"optimizeUseProjections\"),\n             //     \"Cannot match agg func {} by name {}\",\n             //     aggregate.column_name, aggregate.function->getName());\n \n@@ -151,7 +151,7 @@ std::optional<AggregateFunctionMatches> matchAggregateFunctions(\n             /// not match.\n             if (!candidate.function->getStateType()->equals(*aggregate.function->getStateType()))\n             {\n-                // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Cannot match agg func {} vs {} by state {} vs {}\",\n+                // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Cannot match agg func {} vs {} by state {} vs {}\",\n                 //     aggregate.column_name, candidate.column_name,\n                 //     candidate.function->getStateType()->getName(), aggregate.function->getStateType()->getName());\n                 continue;\n@@ -194,7 +194,7 @@ std::optional<AggregateFunctionMatches> matchAggregateFunctions(\n                 if (mt == matches.end())\n                 {\n                     // LOG_TRACE(\n-                    //     &Poco::Logger::get(\"optimizeUseProjections\"),\n+                    //     getLogger(\"optimizeUseProjections\"),\n                     //     \"Cannot match agg func {} vs {} : can't match arg {} vs {} : no node in map\",\n                     //     aggregate.column_name, candidate.column_name, query_name, proj_name);\n \n@@ -205,7 +205,7 @@ std::optional<AggregateFunctionMatches> matchAggregateFunctions(\n                 if (node_match.node != proj_node || node_match.monotonicity)\n                 {\n                     // LOG_TRACE(\n-                    //     &Poco::Logger::get(\"optimizeUseProjections\"),\n+                    //     getLogger(\"optimizeUseProjections\"),\n                     //     \"Cannot match agg func {} vs {} : can't match arg {} vs {} : no match or monotonicity\",\n                     //     aggregate.column_name, candidate.column_name, query_name, proj_name);\n \n@@ -285,7 +285,7 @@ ActionsDAGPtr analyzeAggregateProjection(\n \n     // for (const auto & [node, match] : matches)\n     // {\n-    //     LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Match {} {} -> {} {} (with monotonicity : {})\",\n+    //     LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Match {} {} -> {} {} (with monotonicity : {})\",\n     //         static_cast<const void *>(node), node->result_name,\n     //         static_cast<const void *>(match.node), (match.node ? match.node->result_name : \"\"), match.monotonicity != std::nullopt);\n     // }\n@@ -379,7 +379,7 @@ ActionsDAGPtr analyzeAggregateProjection(\n             /// Not a match and there is no matched child.\n             if (frame.node->type == ActionsDAG::ActionType::INPUT)\n             {\n-                // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Cannot find match for {}\", frame.node->result_name);\n+                // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Cannot find match for {}\", frame.node->result_name);\n                 return {};\n             }\n \n@@ -389,7 +389,7 @@ ActionsDAGPtr analyzeAggregateProjection(\n         }\n     }\n \n-    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Folding actions by projection\");\n+    // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Folding actions by projection\");\n \n     auto proj_dag = query.dag->foldActionsByProjection(new_inputs, query_key_nodes);\n     appendAggregateFunctions(*proj_dag, aggregates, *matched_aggregates);\n@@ -453,7 +453,7 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n     if (!can_use_minmax_projection && agg_projections.empty())\n         return candidates;\n \n-    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Has agg projection\");\n+    // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Has agg projection\");\n \n     QueryDAG dag;\n     if (!dag.build(*node.children.front()))\n@@ -461,22 +461,22 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n \n     auto query_index = buildDAGIndex(*dag.dag);\n \n-    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Query DAG: {}\", dag.dag->dumpDAG());\n+    // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Query DAG: {}\", dag.dag->dumpDAG());\n \n     candidates.has_filter = dag.filter_node;\n \n     if (can_use_minmax_projection)\n     {\n         const auto * projection = &*(metadata->minmax_count_projection);\n-        // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Try projection {}\", projection->name);\n+        // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Try projection {}\", projection->name);\n         auto info = getAggregatingProjectionInfo(*projection, context, metadata, key_virtual_columns);\n-        // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection DAG {}\", info.before_aggregation->dumpDAG());\n+        // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Projection DAG {}\", info.before_aggregation->dumpDAG());\n         if (auto proj_dag = analyzeAggregateProjection(info, dag, query_index, keys, aggregates))\n         {\n-            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection analyzed DAG {}\", proj_dag->dumpDAG());\n+            // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Projection analyzed DAG {}\", proj_dag->dumpDAG());\n             AggregateProjectionCandidate candidate{.info = std::move(info), .dag = std::move(proj_dag)};\n \n-            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection sample block {}\", sample_block.dumpStructure());\n+            // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Projection sample block {}\", sample_block.dumpStructure());\n             auto block = reading.getMergeTreeData().getMinMaxCountProjectionBlock(\n                 metadata,\n                 candidate.dag->getRequiredColumnsNames(),\n@@ -485,7 +485,7 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n                 max_added_blocks.get(),\n                 context);\n \n-            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection sample block 2 {}\", block.dumpStructure());\n+            // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Projection sample block 2 {}\", block.dumpStructure());\n \n             // minmax_count_projection cannot be used when there is no data to process, because\n             // it will produce incorrect result during constant aggregation.\n@@ -518,12 +518,12 @@ AggregateProjectionCandidates getAggregateProjectionCandidates(\n         candidates.real.reserve(agg_projections.size());\n         for (const auto * projection : agg_projections)\n         {\n-            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Try projection {}\", projection->name);\n+            // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Try projection {}\", projection->name);\n             auto info = getAggregatingProjectionInfo(*projection, context, metadata, key_virtual_columns);\n-            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection DAG {}\", info.before_aggregation->dumpDAG());\n+            // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Projection DAG {}\", info.before_aggregation->dumpDAG());\n             if (auto proj_dag = analyzeAggregateProjection(info, dag, query_index, keys, aggregates))\n             {\n-                // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection analyzed DAG {}\", proj_dag->dumpDAG());\n+                // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Projection analyzed DAG {}\", proj_dag->dumpDAG());\n                 AggregateProjectionCandidate candidate{.info = std::move(info), .dag = std::move(proj_dag)};\n                 candidate.projection = projection;\n                 candidates.real.emplace_back(std::move(candidate));\n@@ -650,7 +650,7 @@ bool optimizeUseAggregateProjections(QueryPlan::Node & node, QueryPlan::Nodes &\n     /// Add reading from projection step.\n     if (candidates.minmax_projection)\n     {\n-        // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Minmax proj block {}\",\n+        // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Minmax proj block {}\",\n         //           candidates.minmax_projection->block.dumpStructure());\n \n         Pipe pipe(std::make_shared<SourceFromSingleChunk>(std::move(candidates.minmax_projection->block)));\n@@ -712,7 +712,7 @@ bool optimizeUseAggregateProjections(QueryPlan::Node & node, QueryPlan::Nodes &\n         });\n     }\n \n-    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection reading header {}\",\n+    // LOG_TRACE(getLogger(\"optimizeUseProjections\"), \"Projection reading header {}\",\n     //           projection_reading->getOutputStream().header.dumpStructure());\n \n     projection_reading->setStepDescription(best_candidate->projection->name);\ndiff --git a/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp b/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp\nindex a6029d673e32..232d3118612b 100644\n--- a/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/removeRedundantDistinct.cpp\n@@ -39,14 +39,14 @@ namespace\n             else\n                 ss << value;\n \n-            LOG_DEBUG(&Poco::Logger::get(\"redundantDistinct\"), \"{}{}{}\", key, separator, ss.str());\n+            LOG_DEBUG(getLogger(\"redundantDistinct\"), \"{}{}{}\", key, separator, ss.str());\n         }\n     }\n \n     void logActionsDAG(const String & prefix, const ActionsDAGPtr & actions)\n     {\n         if constexpr (debug_logging_enabled)\n-            LOG_DEBUG(&Poco::Logger::get(\"redundantDistinct\"), \"{} :\\n{}\", prefix, actions->dumpDAG());\n+            LOG_DEBUG(getLogger(\"redundantDistinct\"), \"{} :\\n{}\", prefix, actions->dumpDAG());\n     }\n \n     using DistinctColumns = std::set<std::string_view>;\ndiff --git a/src/Processors/QueryPlan/QueryPlanVisitor.h b/src/Processors/QueryPlan/QueryPlanVisitor.h\nindex 0f2652166491..aed1a2b22497 100644\n--- a/src/Processors/QueryPlan/QueryPlanVisitor.h\n+++ b/src/Processors/QueryPlan/QueryPlanVisitor.h\n@@ -99,7 +99,7 @@ class QueryPlanVisitor\n         {\n             const IQueryPlanStep * current_step = node->step.get();\n             LOG_DEBUG(\n-                &Poco::Logger::get(\"QueryPlanVisitor\"),\n+                getLogger(\"QueryPlanVisitor\"),\n                 \"{}: {}: {}\",\n                 prefix,\n                 getStepId(current_step),\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex d02e387afc3d..0465ff54f5a0 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -247,7 +247,7 @@ ReadFromMergeTree::ReadFromMergeTree(\n     size_t num_streams_,\n     bool sample_factor_column_queried_,\n     std::shared_ptr<PartitionIdToMaxBlock> max_block_numbers_to_read_,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     AnalysisResultPtr analyzed_result_ptr_,\n     bool enable_parallel_reading)\n     : SourceStepWithFilter(DataStream{.header = MergeTreeSelectProcessor::transformHeader(\n@@ -274,7 +274,7 @@ ReadFromMergeTree::ReadFromMergeTree(\n     , requested_num_streams(num_streams_)\n     , sample_factor_column_queried(sample_factor_column_queried_)\n     , max_block_numbers_to_read(std::move(max_block_numbers_to_read_))\n-    , log(log_)\n+    , log(std::move(log_))\n     , analyzed_result_ptr(analyzed_result_ptr_)\n     , is_parallel_reading_from_replicas(enable_parallel_reading)\n {\n@@ -1476,7 +1476,7 @@ ReadFromMergeTree::AnalysisResultPtr ReadFromMergeTree::selectRangesToRead(\n     const MergeTreeData & data,\n     const Names & real_column_names,\n     bool sample_factor_column_queried,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     std::optional<Indexes> & indexes)\n {\n     auto updated_query_info_with_filter_dag = query_info;\n@@ -1508,7 +1508,7 @@ ReadFromMergeTree::AnalysisResultPtr ReadFromMergeTree::selectRangesToReadImpl(\n     const MergeTreeData & data,\n     const Names & real_column_names,\n     bool sample_factor_column_queried,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     std::optional<Indexes> & indexes)\n {\n     AnalysisResult result;\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.h b/src/Processors/QueryPlan/ReadFromMergeTree.h\nindex aed2a270ca18..fdeaff57279f 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.h\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.h\n@@ -120,7 +120,7 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n         size_t num_streams_,\n         bool sample_factor_column_queried_,\n         std::shared_ptr<PartitionIdToMaxBlock> max_block_numbers_to_read_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         AnalysisResultPtr analyzed_result_ptr_,\n         bool enable_parallel_reading);\n \n@@ -168,7 +168,7 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n         const MergeTreeData & data,\n         const Names & real_column_names,\n         bool sample_factor_column_queried,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         std::optional<Indexes> & indexes);\n \n     AnalysisResultPtr selectRangesToRead(\n@@ -217,7 +217,7 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n         const MergeTreeData & data,\n         const Names & real_column_names,\n         bool sample_factor_column_queried,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         std::optional<Indexes> & indexes);\n \n     int getSortDirection() const\n@@ -259,7 +259,7 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n     /// Pre-computed value, needed to trigger sets creating for PK\n     mutable std::optional<Indexes> indexes;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     UInt64 selected_parts = 0;\n     UInt64 selected_rows = 0;\n     UInt64 selected_marks = 0;\ndiff --git a/src/Processors/QueryPlan/ReadFromRemote.cpp b/src/Processors/QueryPlan/ReadFromRemote.cpp\nindex 4bbba4cfa304..4dd799039655 100644\n--- a/src/Processors/QueryPlan/ReadFromRemote.cpp\n+++ b/src/Processors/QueryPlan/ReadFromRemote.cpp\n@@ -102,7 +102,7 @@ ReadFromRemote::ReadFromRemote(\n     ThrottlerPtr throttler_,\n     Scalars scalars_,\n     Tables external_tables_,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     UInt32 shard_count_,\n     std::shared_ptr<const StorageLimitsList> storage_limits_,\n     const String & cluster_name_)\n@@ -172,7 +172,7 @@ void ReadFromRemote::addLazyPipe(Pipes & pipes, const ClusterProxy::SelectStream\n         catch (const Exception & ex)\n         {\n             if (ex.code() == ErrorCodes::ALL_CONNECTION_TRIES_FAILED)\n-                LOG_WARNING(&Poco::Logger::get(\"ClusterProxy::SelectStreamFactory\"),\n+                LOG_WARNING(getLogger(\"ClusterProxy::SelectStreamFactory\"),\n                     \"Connections to remote replicas of local shard {} failed, will use stale local replica\", my_shard.shard_info.shard_num);\n             else\n                 throw;\n@@ -361,7 +361,7 @@ ReadFromParallelRemoteReplicasStep::ReadFromParallelRemoteReplicasStep(\n     ThrottlerPtr throttler_,\n     Scalars scalars_,\n     Tables external_tables_,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     std::shared_ptr<const StorageLimitsList> storage_limits_)\n     : ISourceStep(DataStream{.header = std::move(header_)})\n     , cluster(cluster_)\n@@ -402,7 +402,7 @@ void ReadFromParallelRemoteReplicasStep::initializePipeline(QueryPipelineBuilder\n     size_t all_replicas_count = current_settings.max_parallel_replicas;\n     if (all_replicas_count > cluster->getShardsInfo().size())\n     {\n-        LOG_INFO(&Poco::Logger::get(\"ReadFromParallelRemoteReplicasStep\"),\n+        LOG_INFO(getLogger(\"ReadFromParallelRemoteReplicasStep\"),\n             \"The number of replicas requested ({}) is bigger than the real number available in the cluster ({}). \"\\\n             \"Will use the latter number to execute the query.\", current_settings.max_parallel_replicas, cluster->getShardsInfo().size());\n         all_replicas_count = cluster->getShardsInfo().size();\ndiff --git a/src/Processors/QueryPlan/ReadFromRemote.h b/src/Processors/QueryPlan/ReadFromRemote.h\nindex 82ef45d6bbf4..f853a12910b6 100644\n--- a/src/Processors/QueryPlan/ReadFromRemote.h\n+++ b/src/Processors/QueryPlan/ReadFromRemote.h\n@@ -35,7 +35,7 @@ class ReadFromRemote final : public ISourceStep\n         ThrottlerPtr throttler_,\n         Scalars scalars_,\n         Tables external_tables_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         UInt32 shard_count_,\n         std::shared_ptr<const StorageLimitsList> storage_limits_,\n         const String & cluster_name_);\n@@ -57,7 +57,7 @@ class ReadFromRemote final : public ISourceStep\n     Scalars scalars;\n     Tables external_tables;\n     std::shared_ptr<const StorageLimitsList> storage_limits;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     UInt32 shard_count;\n     const String cluster_name;\n     std::optional<GetPriorityForLoadBalancing> priority_func_factory;\n@@ -80,7 +80,7 @@ class ReadFromParallelRemoteReplicasStep : public ISourceStep\n         ThrottlerPtr throttler_,\n         Scalars scalars_,\n         Tables external_tables_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         std::shared_ptr<const StorageLimitsList> storage_limits_);\n \n     String getName() const override { return \"ReadFromRemoteParallelReplicas\"; }\n@@ -103,7 +103,7 @@ class ReadFromParallelRemoteReplicasStep : public ISourceStep\n     Scalars scalars;\n     Tables external_tables;\n     std::shared_ptr<const StorageLimitsList> storage_limits;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Processors/Sources/MySQLSource.cpp b/src/Processors/Sources/MySQLSource.cpp\nindex 81225d1cdf25..be691fd5b2e1 100644\n--- a/src/Processors/Sources/MySQLSource.cpp\n+++ b/src/Processors/Sources/MySQLSource.cpp\n@@ -58,7 +58,7 @@ MySQLSource::MySQLSource(\n     const Block & sample_block,\n     const StreamSettings & settings_)\n     : ISource(sample_block.cloneEmpty())\n-    , log(&Poco::Logger::get(\"MySQLSource\"))\n+    , log(getLogger(\"MySQLSource\"))\n     , connection{std::make_unique<Connection>(entry, query_str)}\n     , settings{std::make_unique<StreamSettings>(settings_)}\n {\n@@ -69,7 +69,7 @@ MySQLSource::MySQLSource(\n /// For descendant MySQLWithFailoverSource\n MySQLSource::MySQLSource(const Block &sample_block_, const StreamSettings & settings_)\n     : ISource(sample_block_.cloneEmpty())\n-    , log(&Poco::Logger::get(\"MySQLSource\"))\n+    , log(getLogger(\"MySQLSource\"))\n     , settings(std::make_unique<StreamSettings>(settings_))\n {\n     description.init(sample_block_);\ndiff --git a/src/Processors/Sources/MySQLSource.h b/src/Processors/Sources/MySQLSource.h\nindex c4d447886c04..fc26ffa3645e 100644\n--- a/src/Processors/Sources/MySQLSource.h\n+++ b/src/Processors/Sources/MySQLSource.h\n@@ -50,7 +50,7 @@ class MySQLSource : public ISource\n         mysqlxx::UseQueryResult result;\n     };\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::unique_ptr<Connection> connection;\n \n     const std::unique_ptr<StreamSettings> settings;\ndiff --git a/src/Processors/Sources/ShellCommandSource.cpp b/src/Processors/Sources/ShellCommandSource.cpp\nindex 1f23292c6b3f..55eaf67eb3b1 100644\n--- a/src/Processors/Sources/ShellCommandSource.cpp\n+++ b/src/Processors/Sources/ShellCommandSource.cpp\n@@ -158,7 +158,7 @@ class TimeoutReadBufferFromFileDescriptor : public BufferWithOwnMemory<ReadBuffe\n                         throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"Executable generates stderr: {}\", str);\n                     else if (stderr_reaction == ExternalCommandStderrReaction::LOG)\n                         LOG_WARNING(\n-                            &::Poco::Logger::get(\"TimeoutReadBufferFromFileDescriptor\"), \"Executable generates stderr: {}\", str);\n+                            getLogger(\"TimeoutReadBufferFromFileDescriptor\"), \"Executable generates stderr: {}\", str);\n                     else if (stderr_reaction == ExternalCommandStderrReaction::LOG_FIRST)\n                     {\n                         res = std::min(ssize_t(stderr_result_buf.reserve()), res);\n@@ -217,7 +217,7 @@ class TimeoutReadBufferFromFileDescriptor : public BufferWithOwnMemory<ReadBuffe\n             stderr_result.reserve(stderr_result_buf.size());\n             stderr_result.append(stderr_result_buf.begin(), stderr_result_buf.end());\n             LOG_WARNING(\n-                &::Poco::Logger::get(\"ShellCommandSource\"),\n+                getLogger(\"ShellCommandSource\"),\n                 \"Executable generates stderr at the {}: {}\",\n                 stderr_reaction == ExternalCommandStderrReaction::LOG_FIRST ? \"beginning\" : \"end\",\n                 stderr_result);\ndiff --git a/src/Processors/Transforms/AggregatingInOrderTransform.h b/src/Processors/Transforms/AggregatingInOrderTransform.h\nindex af63ac61c3c5..5d50e97f5524 100644\n--- a/src/Processors/Transforms/AggregatingInOrderTransform.h\n+++ b/src/Processors/Transforms/AggregatingInOrderTransform.h\n@@ -83,7 +83,7 @@ class AggregatingInOrderTransform : public IProcessor\n     Chunk current_chunk;\n     Chunk to_push_chunk;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"AggregatingInOrderTransform\");\n+    LoggerPtr log = getLogger(\"AggregatingInOrderTransform\");\n };\n \n \ndiff --git a/src/Processors/Transforms/AggregatingTransform.h b/src/Processors/Transforms/AggregatingTransform.h\nindex 91fdf479ffbd..e05528afdc73 100644\n--- a/src/Processors/Transforms/AggregatingTransform.h\n+++ b/src/Processors/Transforms/AggregatingTransform.h\n@@ -180,7 +180,7 @@ class AggregatingTransform : public IProcessor\n     Processors processors;\n \n     AggregatingTransformParamsPtr params;\n-    Poco::Logger * log = &Poco::Logger::get(\"AggregatingTransform\");\n+    LoggerPtr log = getLogger(\"AggregatingTransform\");\n \n     ColumnRawPtrs key_columns;\n     Aggregator::AggregateColumns aggregate_columns;\ndiff --git a/src/Processors/Transforms/ColumnGathererTransform.cpp b/src/Processors/Transforms/ColumnGathererTransform.cpp\nindex 7c2b93faa913..d7f52a538e1b 100644\n--- a/src/Processors/Transforms/ColumnGathererTransform.cpp\n+++ b/src/Processors/Transforms/ColumnGathererTransform.cpp\n@@ -128,7 +128,7 @@ ColumnGathererTransform::ColumnGathererTransform(\n     : IMergingTransform<ColumnGathererStream>(\n         num_inputs, header, header, /*have_all_inputs_=*/ true, /*limit_hint_=*/ 0, /*always_read_till_end_=*/ false,\n         num_inputs, row_sources_buf_, block_preferred_size_)\n-    , log(&Poco::Logger::get(\"ColumnGathererStream\"))\n+    , log(getLogger(\"ColumnGathererStream\"))\n {\n     if (header.columns() != 1)\n         throw Exception(ErrorCodes::INCORRECT_NUMBER_OF_COLUMNS, \"Header should have 1 column, but contains {}\",\ndiff --git a/src/Processors/Transforms/ColumnGathererTransform.h b/src/Processors/Transforms/ColumnGathererTransform.h\nindex b5bbbff9aca1..885cb3f81ba3 100644\n--- a/src/Processors/Transforms/ColumnGathererTransform.h\n+++ b/src/Processors/Transforms/ColumnGathererTransform.h\n@@ -120,7 +120,7 @@ class ColumnGathererTransform final : public IMergingTransform<ColumnGathererStr\n     void onFinish() override;\n     UInt64 elapsed_ns = 0;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \ndiff --git a/src/Processors/Transforms/CreateSetAndFilterOnTheFlyTransform.h b/src/Processors/Transforms/CreateSetAndFilterOnTheFlyTransform.h\nindex d214a310a8c9..0f5dab06fc91 100644\n--- a/src/Processors/Transforms/CreateSetAndFilterOnTheFlyTransform.h\n+++ b/src/Processors/Transforms/CreateSetAndFilterOnTheFlyTransform.h\n@@ -67,7 +67,7 @@ class CreatingSetsOnTheFlyTransform : public ISimpleTransform\n     /// Set to fill\n     SetWithStatePtr set;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"CreatingSetsOnTheFlyTransform\");\n+    LoggerPtr log = getLogger(\"CreatingSetsOnTheFlyTransform\");\n };\n \n /*\n@@ -108,7 +108,7 @@ class FilterBySetOnTheFlyTransform : public ISimpleTransform\n         size_t result_rows = 0;\n     } stat;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"FilterBySetOnTheFlyTransform\");\n+    LoggerPtr log = getLogger(\"FilterBySetOnTheFlyTransform\");\n };\n \n }\ndiff --git a/src/Processors/Transforms/CreatingSetsTransform.h b/src/Processors/Transforms/CreatingSetsTransform.h\nindex d1ec7dcbca74..74dcd829b4da 100644\n--- a/src/Processors/Transforms/CreatingSetsTransform.h\n+++ b/src/Processors/Transforms/CreatingSetsTransform.h\n@@ -63,7 +63,7 @@ class CreatingSetsTransform : public IAccumulatingTransform\n     size_t bytes_to_transfer = 0;\n \n     using Logger = Poco::Logger;\n-    Poco::Logger * log = &Poco::Logger::get(\"CreatingSetsTransform\");\n+    LoggerPtr log = getLogger(\"CreatingSetsTransform\");\n \n     bool is_initialized = false;\n \ndiff --git a/src/Processors/Transforms/FillingTransform.cpp b/src/Processors/Transforms/FillingTransform.cpp\nindex b725c3e1a5fb..aaa98e968031 100644\n--- a/src/Processors/Transforms/FillingTransform.cpp\n+++ b/src/Processors/Transforms/FillingTransform.cpp\n@@ -28,7 +28,7 @@ void logDebug(String key, const T & value, const char * separator = \" : \")\n         else\n             ss << value;\n \n-        LOG_DEBUG(&Poco::Logger::get(\"FillingTransform\"), \"{}{}{}\", key, separator, ss.str());\n+        LOG_DEBUG(getLogger(\"FillingTransform\"), \"{}{}{}\", key, separator, ss.str());\n     }\n }\n \ndiff --git a/src/Processors/Transforms/JoiningTransform.cpp b/src/Processors/Transforms/JoiningTransform.cpp\nindex 4e7868ea1c2d..0c0a86ce270c 100644\n--- a/src/Processors/Transforms/JoiningTransform.cpp\n+++ b/src/Processors/Transforms/JoiningTransform.cpp\n@@ -14,12 +14,12 @@ namespace ErrorCodes\n \n Block JoiningTransform::transformHeader(Block header, const JoinPtr & join)\n {\n-    LOG_DEBUG(&Poco::Logger::get(\"JoiningTransform\"), \"Before join block: '{}'\", header.dumpStructure());\n+    LOG_DEBUG(getLogger(\"JoiningTransform\"), \"Before join block: '{}'\", header.dumpStructure());\n     join->checkTypesOfKeys(header);\n     join->initialize(header);\n     ExtraBlockPtr tmp;\n     join->joinBlock(header, tmp);\n-    LOG_DEBUG(&Poco::Logger::get(\"JoiningTransform\"), \"After join block: '{}'\", header.dumpStructure());\n+    LOG_DEBUG(getLogger(\"JoiningTransform\"), \"After join block: '{}'\", header.dumpStructure());\n     return header;\n }\n \ndiff --git a/src/Processors/Transforms/MergeJoinTransform.cpp b/src/Processors/Transforms/MergeJoinTransform.cpp\nindex 15c88244cbd3..2d313d4ea5c3 100644\n--- a/src/Processors/Transforms/MergeJoinTransform.cpp\n+++ b/src/Processors/Transforms/MergeJoinTransform.cpp\n@@ -273,7 +273,7 @@ MergeJoinAlgorithm::MergeJoinAlgorithm(\n     size_t max_block_size_)\n     : table_join(table_join_)\n     , max_block_size(max_block_size_)\n-    , log(&Poco::Logger::get(\"MergeJoinAlgorithm\"))\n+    , log(getLogger(\"MergeJoinAlgorithm\"))\n {\n     if (input_headers.size() != 2)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"MergeJoinAlgorithm requires exactly two inputs\");\n@@ -860,7 +860,7 @@ MergeJoinTransform::MergeJoinTransform(\n         /* always_read_till_end_= */ false,\n         /* empty_chunk_on_finish_= */ true,\n         table_join, input_headers, max_block_size)\n-    , log(&Poco::Logger::get(\"MergeJoinTransform\"))\n+    , log(getLogger(\"MergeJoinTransform\"))\n {\n     LOG_TRACE(log, \"Use MergeJoinTransform\");\n }\ndiff --git a/src/Processors/Transforms/MergeJoinTransform.h b/src/Processors/Transforms/MergeJoinTransform.h\nindex eb45169a2b07..793de00db406 100644\n--- a/src/Processors/Transforms/MergeJoinTransform.h\n+++ b/src/Processors/Transforms/MergeJoinTransform.h\n@@ -269,7 +269,7 @@ class MergeJoinAlgorithm final : public IMergingAlgorithm\n \n     Statistic stat;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n class MergeJoinTransform final : public IMergingTransform<MergeJoinAlgorithm>\n@@ -289,7 +289,7 @@ class MergeJoinTransform final : public IMergingTransform<MergeJoinAlgorithm>\n protected:\n     void onFinish() override;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Processors/Transforms/MergeSortingTransform.cpp b/src/Processors/Transforms/MergeSortingTransform.cpp\nindex de77711d1294..64d84ea4b007 100644\n--- a/src/Processors/Transforms/MergeSortingTransform.cpp\n+++ b/src/Processors/Transforms/MergeSortingTransform.cpp\n@@ -30,7 +30,7 @@ namespace DB\n class BufferingToFileTransform : public IAccumulatingTransform\n {\n public:\n-    BufferingToFileTransform(const Block & header, TemporaryFileStream & tmp_stream_, Poco::Logger * log_)\n+    BufferingToFileTransform(const Block & header, TemporaryFileStream & tmp_stream_, LoggerPtr log_)\n         : IAccumulatingTransform(header, header)\n         , tmp_stream(tmp_stream_)\n         , log(log_)\n@@ -73,7 +73,7 @@ class BufferingToFileTransform : public IAccumulatingTransform\n private:\n     TemporaryFileStream & tmp_stream;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n MergeSortingTransform::MergeSortingTransform(\ndiff --git a/src/Processors/Transforms/MergeSortingTransform.h b/src/Processors/Transforms/MergeSortingTransform.h\nindex e8c180b69032..4478d5a07e83 100644\n--- a/src/Processors/Transforms/MergeSortingTransform.h\n+++ b/src/Processors/Transforms/MergeSortingTransform.h\n@@ -50,7 +50,7 @@ class MergeSortingTransform : public SortingTransform\n     size_t sum_rows_in_blocks = 0;\n     size_t sum_bytes_in_blocks = 0;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"MergeSortingTransform\");\n+    LoggerPtr log = getLogger(\"MergeSortingTransform\");\n \n     /// If remerge doesn't save memory at least several times, mark it as useless and don't do it anymore.\n     bool remerge_is_useful = true;\ndiff --git a/src/Processors/Transforms/MergingAggregatedTransform.h b/src/Processors/Transforms/MergingAggregatedTransform.h\nindex 73e0d8cd0132..ade76b2f3048 100644\n--- a/src/Processors/Transforms/MergingAggregatedTransform.h\n+++ b/src/Processors/Transforms/MergingAggregatedTransform.h\n@@ -21,7 +21,7 @@ class MergingAggregatedTransform : public IAccumulatingTransform\n \n private:\n     AggregatingTransformParamsPtr params;\n-    Poco::Logger * log = &Poco::Logger::get(\"MergingAggregatedTransform\");\n+    LoggerPtr log = getLogger(\"MergingAggregatedTransform\");\n     size_t max_threads;\n \n     AggregatedDataVariants data_variants;\ndiff --git a/src/Processors/Transforms/PasteJoinTransform.cpp b/src/Processors/Transforms/PasteJoinTransform.cpp\nindex ff3e2fb85e55..d2fa7eed256d 100644\n--- a/src/Processors/Transforms/PasteJoinTransform.cpp\n+++ b/src/Processors/Transforms/PasteJoinTransform.cpp\n@@ -33,7 +33,7 @@ PasteJoinAlgorithm::PasteJoinAlgorithm(\n     size_t max_block_size_)\n     : table_join(table_join_)\n     , max_block_size(max_block_size_)\n-    , log(&Poco::Logger::get(\"PasteJoinAlgorithm\"))\n+    , log(getLogger(\"PasteJoinAlgorithm\"))\n {\n     if (input_headers.size() != 2)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"PasteJoinAlgorithm requires exactly two inputs\");\n@@ -117,7 +117,7 @@ PasteJoinTransform::PasteJoinTransform(\n         /* always_read_till_end_= */ false,\n         /* empty_chunk_on_finish_= */ true,\n         table_join, input_headers, max_block_size)\n-    , log(&Poco::Logger::get(\"PasteJoinTransform\"))\n+    , log(getLogger(\"PasteJoinTransform\"))\n {\n     LOG_TRACE(log, \"Use PasteJoinTransform\");\n }\ndiff --git a/src/Processors/Transforms/PasteJoinTransform.h b/src/Processors/Transforms/PasteJoinTransform.h\nindex 7ecf70e18dc2..04cb5486cd5d 100644\n--- a/src/Processors/Transforms/PasteJoinTransform.h\n+++ b/src/Processors/Transforms/PasteJoinTransform.h\n@@ -61,7 +61,7 @@ class PasteJoinAlgorithm final : public IMergingAlgorithm\n \n     Statistic stat;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     UInt64 last_used_row[2] = {0, 0};\n };\n \n@@ -82,7 +82,7 @@ class PasteJoinTransform final : public IMergingTransform<PasteJoinAlgorithm>\n protected:\n     void onFinish() override;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Processors/Transforms/TTLCalcTransform.cpp b/src/Processors/Transforms/TTLCalcTransform.cpp\nindex 31fb61239ef8..2b4ed96d4e38 100644\n--- a/src/Processors/Transforms/TTLCalcTransform.cpp\n+++ b/src/Processors/Transforms/TTLCalcTransform.cpp\n@@ -13,7 +13,7 @@ TTLCalcTransform::TTLCalcTransform(\n     bool force_)\n     : IAccumulatingTransform(header_, header_)\n     , data_part(data_part_)\n-    , log(&Poco::Logger::get(storage_.getLogName() + \" (TTLCalcTransform)\"))\n+    , log(getLogger(storage_.getLogName() + \" (TTLCalcTransform)\"))\n {\n     auto old_ttl_infos = data_part->ttl_infos;\n \ndiff --git a/src/Processors/Transforms/TTLCalcTransform.h b/src/Processors/Transforms/TTLCalcTransform.h\nindex 495879400dce..baa31c01c526 100644\n--- a/src/Processors/Transforms/TTLCalcTransform.h\n+++ b/src/Processors/Transforms/TTLCalcTransform.h\n@@ -38,7 +38,7 @@ class TTLCalcTransform : public IAccumulatingTransform\n \n     /// ttl_infos and empty_columns are updating while reading\n     const MergeTreeData::MutableDataPartPtr & data_part;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Processors/Transforms/TTLTransform.cpp b/src/Processors/Transforms/TTLTransform.cpp\nindex 7cde86098c7a..db9326f9acf0 100644\n--- a/src/Processors/Transforms/TTLTransform.cpp\n+++ b/src/Processors/Transforms/TTLTransform.cpp\n@@ -25,7 +25,7 @@ TTLTransform::TTLTransform(\n     bool force_)\n     : IAccumulatingTransform(header_, header_)\n     , data_part(data_part_)\n-    , log(&Poco::Logger::get(storage_.getLogName() + \" (TTLTransform)\"))\n+    , log(getLogger(storage_.getLogName() + \" (TTLTransform)\"))\n {\n     auto old_ttl_infos = data_part->ttl_infos;\n \ndiff --git a/src/Processors/Transforms/TTLTransform.h b/src/Processors/Transforms/TTLTransform.h\nindex 3f0dffd19989..3606db7f4c2c 100644\n--- a/src/Processors/Transforms/TTLTransform.h\n+++ b/src/Processors/Transforms/TTLTransform.h\n@@ -42,7 +42,7 @@ class TTLTransform : public IAccumulatingTransform\n \n     /// ttl_infos and empty_columns are updating while reading\n     const MergeTreeData::MutableDataPartPtr & data_part;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Processors/Transforms/buildPushingToViewsChain.cpp b/src/Processors/Transforms/buildPushingToViewsChain.cpp\nindex 71d652e74d07..960cc0190015 100644\n--- a/src/Processors/Transforms/buildPushingToViewsChain.cpp\n+++ b/src/Processors/Transforms/buildPushingToViewsChain.cpp\n@@ -267,7 +267,7 @@ Chain buildPushingToViewsChain(\n         if (view == nullptr)\n         {\n             LOG_WARNING(\n-                &Poco::Logger::get(\"PushingToViews\"), \"Trying to access table {} but it doesn't exist\", view_id.getFullTableName());\n+                getLogger(\"PushingToViews\"), \"Trying to access table {} but it doesn't exist\", view_id.getFullTableName());\n             continue;\n         }\n \n@@ -310,7 +310,7 @@ Chain buildPushingToViewsChain(\n                 // In case the materialized view is dropped/detached at this point, we register a warning and ignore it\n                 assert(materialized_view->is_dropped || materialized_view->is_detached);\n                 LOG_WARNING(\n-                    &Poco::Logger::get(\"PushingToViews\"), \"Trying to access table {} but it doesn't exist\", view_id.getFullTableName());\n+                    getLogger(\"PushingToViews\"), \"Trying to access table {} but it doesn't exist\", view_id.getFullTableName());\n                 continue;\n             }\n \n@@ -341,7 +341,7 @@ Chain buildPushingToViewsChain(\n                 /// It may happen if materialize view query was changed and it doesn't depend on this source table anymore.\n                 /// See setting `allow_experimental_alter_materialized_view_structure`\n                 LOG_DEBUG(\n-                    &Poco::Logger::get(\"PushingToViews\"), \"Table '{}' is not a source for view '{}' anymore, current source is '{}'\",\n+                    getLogger(\"PushingToViews\"), \"Table '{}' is not a source for view '{}' anymore, current source is '{}'\",\n                         select_query.select_table_id.getFullTableName(), view_id.getFullTableName(), table_id);\n                 continue;\n             }\n@@ -835,14 +835,14 @@ void FinalizingViewsTransform::work()\n \n             /// Exception will be ignored, it is saved here for the system.query_views_log\n             if (materialized_views_ignore_errors)\n-                tryLogException(view.exception, &Poco::Logger::get(\"PushingToViews\"), \"Cannot push to the storage, ignoring the error\");\n+                tryLogException(view.exception, getLogger(\"PushingToViews\"), \"Cannot push to the storage, ignoring the error\");\n         }\n         else\n         {\n             view.runtime_stats->setStatus(QueryViewsLogElement::ViewStatus::QUERY_FINISH);\n \n             LOG_TRACE(\n-                &Poco::Logger::get(\"PushingToViews\"),\n+                getLogger(\"PushingToViews\"),\n                 \"Pushing ({}) from {} to {} took {} ms.\",\n                 views_data->max_threads <= 1 ? \"sequentially\" : (\"parallel \" + std::to_string(views_data->max_threads)),\n                 views_data->source_storage_id.getNameForLogs(),\ndiff --git a/src/QueryPipeline/RemoteQueryExecutor.h b/src/QueryPipeline/RemoteQueryExecutor.h\nindex 5a8ccc2592b2..444f1258f3e4 100644\n--- a/src/QueryPipeline/RemoteQueryExecutor.h\n+++ b/src/QueryPipeline/RemoteQueryExecutor.h\n@@ -186,7 +186,7 @@ class RemoteQueryExecutor\n \n     void setMainTable(StorageID main_table_) { main_table = std::move(main_table_); }\n \n-    void setLogger(Poco::Logger * logger) { log = logger; }\n+    void setLogger(LoggerPtr logger) { log = logger; }\n \n     const Block & getHeader() const { return header; }\n \n@@ -283,7 +283,7 @@ class RemoteQueryExecutor\n     PoolMode pool_mode = PoolMode::GET_MANY;\n     StorageID main_table = StorageID::createEmpty();\n \n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n \n     GetPriorityForLoadBalancing::Func priority_func;\n \ndiff --git a/src/Server/CertificateReloader.cpp b/src/Server/CertificateReloader.cpp\nindex 8795d4807de1..c974f450c9a3 100644\n--- a/src/Server/CertificateReloader.cpp\n+++ b/src/Server/CertificateReloader.cpp\n@@ -105,7 +105,7 @@ CertificateReloader::Data::Data(std::string cert_path, std::string key_path, std\n }\n \n \n-bool CertificateReloader::File::changeIfModified(std::string new_path, Poco::Logger * logger)\n+bool CertificateReloader::File::changeIfModified(std::string new_path, LoggerPtr logger)\n {\n     std::error_code ec;\n     std::filesystem::file_time_type new_modification_time = std::filesystem::last_write_time(new_path, ec);\ndiff --git a/src/Server/CertificateReloader.h b/src/Server/CertificateReloader.h\nindex 9f04179b8d6c..028914e682f1 100644\n--- a/src/Server/CertificateReloader.h\n+++ b/src/Server/CertificateReloader.h\n@@ -14,6 +14,7 @@\n #include <Poco/Crypto/RSAKey.h>\n #include <Poco/Crypto/X509Certificate.h>\n #include <Common/MultiVersion.h>\n+#include <Common/Logger.h>\n \n \n namespace DB\n@@ -51,7 +52,7 @@ class CertificateReloader\n private:\n     CertificateReloader() = default;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"CertificateReloader\");\n+    LoggerPtr log = getLogger(\"CertificateReloader\");\n \n     struct File\n     {\n@@ -61,7 +62,7 @@ class CertificateReloader\n         std::string path;\n         std::filesystem::file_time_type modification_time;\n \n-        bool changeIfModified(std::string new_path, Poco::Logger * logger);\n+        bool changeIfModified(std::string new_path, LoggerPtr logger);\n     };\n \n     File cert_file{\"certificate\"};\ndiff --git a/src/Server/GRPCServer.cpp b/src/Server/GRPCServer.cpp\nindex 6bb6ba139adf..f31a8d6feb50 100644\n--- a/src/Server/GRPCServer.cpp\n+++ b/src/Server/GRPCServer.cpp\n@@ -76,7 +76,7 @@ namespace\n         static std::once_flag once_flag;\n         std::call_once(once_flag, [&config]\n         {\n-            static Poco::Logger * logger = &Poco::Logger::get(\"grpc\");\n+            static LoggerPtr logger = getLogger(\"grpc\");\n             gpr_set_log_function([](gpr_log_func_args* args)\n             {\n                 if (args->severity == GPR_LOG_SEVERITY_DEBUG)\n@@ -614,7 +614,7 @@ namespace\n     class Call\n     {\n     public:\n-        Call(CallType call_type_, std::unique_ptr<BaseResponder> responder_, IServer & iserver_, Poco::Logger * log_);\n+        Call(CallType call_type_, std::unique_ptr<BaseResponder> responder_, IServer & iserver_, LoggerPtr log_);\n         ~Call();\n \n         void start(const std::function<void(void)> & on_finish_call_callback);\n@@ -656,7 +656,7 @@ namespace\n         const CallType call_type;\n         std::unique_ptr<BaseResponder> responder;\n         IServer & iserver;\n-        Poco::Logger * log = nullptr;\n+        LoggerPtr log = nullptr;\n \n         std::optional<Session> session;\n         ContextMutablePtr query_context;\n@@ -718,7 +718,7 @@ namespace\n     };\n // NOLINTEND(clang-analyzer-optin.performance.Padding)\n \n-    Call::Call(CallType call_type_, std::unique_ptr<BaseResponder> responder_, IServer & iserver_, Poco::Logger * log_)\n+    Call::Call(CallType call_type_, std::unique_ptr<BaseResponder> responder_, IServer & iserver_, LoggerPtr log_)\n         : call_type(call_type_), responder(std::move(responder_)), iserver(iserver_), log(log_)\n     {\n     }\n@@ -1843,7 +1843,7 @@ class GRPCServer::Runner\n GRPCServer::GRPCServer(IServer & iserver_, const Poco::Net::SocketAddress & address_to_listen_)\n     : iserver(iserver_)\n     , address_to_listen(address_to_listen_)\n-    , log(&Poco::Logger::get(\"GRPCServer\"))\n+    , log(getLogger(\"GRPCServer\"))\n     , runner(std::make_unique<Runner>(*this))\n {}\n \ndiff --git a/src/Server/GRPCServer.h b/src/Server/GRPCServer.h\nindex 359a2506e953..a9c8161298fc 100644\n--- a/src/Server/GRPCServer.h\n+++ b/src/Server/GRPCServer.h\n@@ -5,6 +5,7 @@\n #if USE_GRPC\n #include <Poco/Net/SocketAddress.h>\n #include <base/types.h>\n+#include <Common/Logger.h>\n #include \"clickhouse_grpc.grpc.pb.h\"\n \n namespace Poco { class Logger; }\n@@ -46,7 +47,7 @@ class GRPCServer\n \n     IServer & iserver;\n     const Poco::Net::SocketAddress address_to_listen;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     GRPCService grpc_service;\n     std::unique_ptr<grpc::Server> grpc_server;\n     std::unique_ptr<grpc::ServerCompletionQueue> queue;\ndiff --git a/src/Server/HTTP/HTTPServerRequest.cpp b/src/Server/HTTP/HTTPServerRequest.cpp\nindex 4a6e85ba0fba..9db02eac2206 100644\n--- a/src/Server/HTTP/HTTPServerRequest.cpp\n+++ b/src/Server/HTTP/HTTPServerRequest.cpp\n@@ -65,7 +65,7 @@ HTTPServerRequest::HTTPServerRequest(HTTPContextPtr context, HTTPServerResponse\n     {\n         stream = std::move(in);\n         if (!startsWith(getContentType(), \"multipart/form-data\"))\n-            LOG_WARNING(LogFrequencyLimiter(&Poco::Logger::get(\"HTTPServerRequest\"), 10), \"Got an HTTP request with no content length \"\n+            LOG_WARNING(LogFrequencyLimiter(getLogger(\"HTTPServerRequest\"), 10), \"Got an HTTP request with no content length \"\n                 \"and no chunked/multipart encoding, it may be impossible to distinguish graceful EOF from abnormal connection loss\");\n     }\n     else\ndiff --git a/src/Server/HTTPHandler.cpp b/src/Server/HTTPHandler.cpp\nindex bdc8e7d59c9c..72e7c5552f85 100644\n--- a/src/Server/HTTPHandler.cpp\n+++ b/src/Server/HTTPHandler.cpp\n@@ -137,7 +137,7 @@ bool tryAddHttpOptionHeadersFromConfig(HTTPServerResponse & response, const Poco\n             {\n                 /// If there is empty header name, it will not be processed and message about it will be in logs\n                 if (config.getString(\"http_options_response.\" + config_key + \".name\", \"\").empty())\n-                    LOG_WARNING(&Poco::Logger::get(\"processOptionsRequest\"), \"Empty header was found in config. It will not be processed.\");\n+                    LOG_WARNING(getLogger(\"processOptionsRequest\"), \"Empty header was found in config. It will not be processed.\");\n                 else\n                     response.add(config.getString(\"http_options_response.\" + config_key + \".name\", \"\"),\n                                     config.getString(\"http_options_response.\" + config_key + \".value\", \"\"));\n@@ -328,7 +328,7 @@ void HTTPHandler::pushDelayedResults(Output & used_output)\n \n HTTPHandler::HTTPHandler(IServer & server_, const std::string & name, const std::optional<String> & content_type_override_)\n     : server(server_)\n-    , log(&Poco::Logger::get(name))\n+    , log(getLogger(name))\n     , default_settings(server.context()->getSettingsRef())\n     , content_type_override(content_type_override_)\n {\ndiff --git a/src/Server/HTTPHandler.h b/src/Server/HTTPHandler.h\nindex 0b623fe5f656..fa2d0dae1993 100644\n--- a/src/Server/HTTPHandler.h\n+++ b/src/Server/HTTPHandler.h\n@@ -100,7 +100,7 @@ class HTTPHandler : public HTTPRequestHandler\n     };\n \n     IServer & server;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// It is the name of the server that will be sent in an http-header X-ClickHouse-Server-Display-Name.\n     String server_display_name;\ndiff --git a/src/Server/HTTPRequestHandlerFactoryMain.cpp b/src/Server/HTTPRequestHandlerFactoryMain.cpp\nindex 5481bcd5083c..48c2ab21468a 100644\n--- a/src/Server/HTTPRequestHandlerFactoryMain.cpp\n+++ b/src/Server/HTTPRequestHandlerFactoryMain.cpp\n@@ -7,7 +7,7 @@ namespace DB\n {\n \n HTTPRequestHandlerFactoryMain::HTTPRequestHandlerFactoryMain(const std::string & name_)\n-    : log(&Poco::Logger::get(name_)), name(name_)\n+    : log(getLogger(name_)), name(name_)\n {\n }\n \ndiff --git a/src/Server/HTTPRequestHandlerFactoryMain.h b/src/Server/HTTPRequestHandlerFactoryMain.h\nindex 07b278d831c3..1075b7d1d60d 100644\n--- a/src/Server/HTTPRequestHandlerFactoryMain.h\n+++ b/src/Server/HTTPRequestHandlerFactoryMain.h\n@@ -21,7 +21,7 @@ class HTTPRequestHandlerFactoryMain : public HTTPRequestHandlerFactory\n     std::unique_ptr<HTTPRequestHandler> createRequestHandler(const HTTPServerRequest & request) override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string name;\n     HTTPPathHints hints;\n \ndiff --git a/src/Server/InterserverIOHTTPHandler.h b/src/Server/InterserverIOHTTPHandler.h\nindex 66042ad3d1df..226a06f5a457 100644\n--- a/src/Server/InterserverIOHTTPHandler.h\n+++ b/src/Server/InterserverIOHTTPHandler.h\n@@ -26,7 +26,7 @@ class InterserverIOHTTPHandler : public HTTPRequestHandler\n public:\n     explicit InterserverIOHTTPHandler(IServer & server_)\n         : server(server_)\n-        , log(&Poco::Logger::get(\"InterserverIOHTTPHandler\"))\n+        , log(getLogger(\"InterserverIOHTTPHandler\"))\n     {\n     }\n \n@@ -39,7 +39,7 @@ class InterserverIOHTTPHandler : public HTTPRequestHandler\n     };\n \n     IServer & server;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     CurrentMetrics::Increment metric_increment{CurrentMetrics::InterserverConnection};\n \ndiff --git a/src/Server/KeeperReadinessHandler.cpp b/src/Server/KeeperReadinessHandler.cpp\nindex de6edd199d74..c973be040c8e 100644\n--- a/src/Server/KeeperReadinessHandler.cpp\n+++ b/src/Server/KeeperReadinessHandler.cpp\n@@ -63,7 +63,7 @@ void KeeperReadinessHandler::handleRequest(HTTPServerRequest & /*request*/, HTTP\n         }\n         catch (...)\n         {\n-            LOG_ERROR((&Poco::Logger::get(\"KeeperReadinessHandler\")), \"Cannot send exception to client\");\n+            LOG_ERROR((getLogger(\"KeeperReadinessHandler\")), \"Cannot send exception to client\");\n         }\n     }\n }\ndiff --git a/src/Server/KeeperTCPHandler.cpp b/src/Server/KeeperTCPHandler.cpp\nindex 76b84f0ce6e1..6709cd298e5b 100644\n--- a/src/Server/KeeperTCPHandler.cpp\n+++ b/src/Server/KeeperTCPHandler.cpp\n@@ -220,7 +220,7 @@ KeeperTCPHandler::KeeperTCPHandler(\n     Poco::Timespan send_timeout_,\n     const Poco::Net::StreamSocket & socket_)\n     : Poco::Net::TCPServerConnection(socket_)\n-    , log(&Poco::Logger::get(\"KeeperTCPHandler\"))\n+    , log(getLogger(\"KeeperTCPHandler\"))\n     , keeper_dispatcher(keeper_dispatcher_)\n     , operation_timeout(\n           0,\ndiff --git a/src/Server/KeeperTCPHandler.h b/src/Server/KeeperTCPHandler.h\nindex adb1baa084f9..c1c522eee89d 100644\n--- a/src/Server/KeeperTCPHandler.h\n+++ b/src/Server/KeeperTCPHandler.h\n@@ -63,7 +63,7 @@ class KeeperTCPHandler : public Poco::Net::TCPServerConnection\n     ~KeeperTCPHandler() override;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::shared_ptr<KeeperDispatcher> keeper_dispatcher;\n     Poco::Timespan operation_timeout;\n     Poco::Timespan min_session_timeout;\ndiff --git a/src/Server/KeeperTCPHandlerFactory.h b/src/Server/KeeperTCPHandlerFactory.h\nindex 36f284442c6c..239bf8b55247 100644\n--- a/src/Server/KeeperTCPHandlerFactory.h\n+++ b/src/Server/KeeperTCPHandlerFactory.h\n@@ -17,7 +17,7 @@ class KeeperTCPHandlerFactory : public TCPServerConnectionFactory\n private:\n     ConfigGetter config_getter;\n     std::shared_ptr<KeeperDispatcher> keeper_dispatcher;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     Poco::Timespan receive_timeout;\n     Poco::Timespan send_timeout;\n \n@@ -37,7 +37,7 @@ class KeeperTCPHandlerFactory : public TCPServerConnectionFactory\n         bool secure)\n         : config_getter(config_getter_)\n         , keeper_dispatcher(keeper_dispatcher_)\n-        , log(&Poco::Logger::get(std::string{\"KeeperTCP\"} + (secure ? \"S\" : \"\") + \"HandlerFactory\"))\n+        , log(getLogger(std::string{\"KeeperTCP\"} + (secure ? \"S\" : \"\") + \"HandlerFactory\"))\n         , receive_timeout(/* seconds = */ receive_timeout_seconds, /* microseconds = */ 0)\n         , send_timeout(/* seconds = */ send_timeout_seconds, /* microseconds = */ 0)\n     {\ndiff --git a/src/Server/MySQLHandler.cpp b/src/Server/MySQLHandler.cpp\nindex 969eb24d126c..c159a09c8740 100644\n--- a/src/Server/MySQLHandler.cpp\n+++ b/src/Server/MySQLHandler.cpp\n@@ -78,7 +78,7 @@ MySQLHandler::MySQLHandler(\n     : Poco::Net::TCPServerConnection(socket_)\n     , server(server_)\n     , tcp_server(tcp_server_)\n-    , log(&Poco::Logger::get(\"MySQLHandler\"))\n+    , log(getLogger(\"MySQLHandler\"))\n     , connection_id(connection_id_)\n     , auth_plugin(new MySQLProtocol::Authentication::Native41())\n     , read_event(read_event_)\ndiff --git a/src/Server/MySQLHandler.h b/src/Server/MySQLHandler.h\nindex 36d63ebca849..867a90a6205a 100644\n--- a/src/Server/MySQLHandler.h\n+++ b/src/Server/MySQLHandler.h\n@@ -81,7 +81,7 @@ class MySQLHandler : public Poco::Net::TCPServerConnection\n \n     IServer & server;\n     TCPServer & tcp_server;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     uint32_t connection_id = 0;\n \n     uint32_t server_capabilities = 0;\ndiff --git a/src/Server/MySQLHandlerFactory.cpp b/src/Server/MySQLHandlerFactory.cpp\nindex 79234c647aa6..1dd43e6dab2a 100644\n--- a/src/Server/MySQLHandlerFactory.cpp\n+++ b/src/Server/MySQLHandlerFactory.cpp\n@@ -23,7 +23,7 @@ namespace ErrorCodes\n \n MySQLHandlerFactory::MySQLHandlerFactory(IServer & server_, const ProfileEvents::Event & read_event_, const ProfileEvents::Event & write_event_)\n     : server(server_)\n-    , log(&Poco::Logger::get(\"MySQLHandlerFactory\"))\n+    , log(getLogger(\"MySQLHandlerFactory\"))\n     , read_event(read_event_)\n     , write_event(write_event_)\n {\ndiff --git a/src/Server/MySQLHandlerFactory.h b/src/Server/MySQLHandlerFactory.h\nindex 307ee3b2f0de..4108269d7380 100644\n--- a/src/Server/MySQLHandlerFactory.h\n+++ b/src/Server/MySQLHandlerFactory.h\n@@ -20,7 +20,7 @@ class MySQLHandlerFactory : public TCPServerConnectionFactory\n {\n private:\n     IServer & server;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n #if USE_SSL\n     struct RSADeleter\ndiff --git a/src/Server/PostgreSQLHandler.h b/src/Server/PostgreSQLHandler.h\nindex 57b91a0ad044..1c23d8964152 100644\n--- a/src/Server/PostgreSQLHandler.h\n+++ b/src/Server/PostgreSQLHandler.h\n@@ -40,7 +40,7 @@ class PostgreSQLHandler : public Poco::Net::TCPServerConnection\n     void run() final;\n \n private:\n-    Poco::Logger * log = &Poco::Logger::get(\"PostgreSQLHandler\");\n+    LoggerPtr log = getLogger(\"PostgreSQLHandler\");\n \n     IServer & server;\n     TCPServer & tcp_server;\ndiff --git a/src/Server/PostgreSQLHandlerFactory.cpp b/src/Server/PostgreSQLHandlerFactory.cpp\nindex 096bbbdcda9a..29eb75679760 100644\n--- a/src/Server/PostgreSQLHandlerFactory.cpp\n+++ b/src/Server/PostgreSQLHandlerFactory.cpp\n@@ -7,7 +7,7 @@ namespace DB\n \n PostgreSQLHandlerFactory::PostgreSQLHandlerFactory(IServer & server_, const ProfileEvents::Event & read_event_, const ProfileEvents::Event & write_event_)\n     : server(server_)\n-    , log(&Poco::Logger::get(\"PostgreSQLHandlerFactory\"))\n+    , log(getLogger(\"PostgreSQLHandlerFactory\"))\n     , read_event(read_event_)\n     , write_event(write_event_)\n {\ndiff --git a/src/Server/PostgreSQLHandlerFactory.h b/src/Server/PostgreSQLHandlerFactory.h\nindex e5f762fca6d7..43674306ff66 100644\n--- a/src/Server/PostgreSQLHandlerFactory.h\n+++ b/src/Server/PostgreSQLHandlerFactory.h\n@@ -14,7 +14,7 @@ class PostgreSQLHandlerFactory : public TCPServerConnectionFactory\n {\n private:\n     IServer & server;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     ProfileEvents::Event read_event;\n     ProfileEvents::Event write_event;\n \ndiff --git a/src/Server/ProxyV1Handler.h b/src/Server/ProxyV1Handler.h\nindex b50c2acbc550..a044b9a966b1 100644\n--- a/src/Server/ProxyV1Handler.h\n+++ b/src/Server/ProxyV1Handler.h\n@@ -13,7 +13,7 @@ class ProxyV1Handler : public Poco::Net::TCPServerConnection\n     using StreamSocket = Poco::Net::StreamSocket;\n public:\n     explicit ProxyV1Handler(const StreamSocket & socket, IServer & server_, const std::string & conf_name_, TCPProtocolStackData & stack_data_)\n-        : Poco::Net::TCPServerConnection(socket), log(&Poco::Logger::get(\"ProxyV1Handler\")), server(server_), conf_name(conf_name_), stack_data(stack_data_) {}\n+        : Poco::Net::TCPServerConnection(socket), log(getLogger(\"ProxyV1Handler\")), server(server_), conf_name(conf_name_), stack_data(stack_data_) {}\n \n     void run() override;\n \n@@ -21,7 +21,7 @@ class ProxyV1Handler : public Poco::Net::TCPServerConnection\n     bool readWord(int max_len, std::string & word, bool & eol);\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     IServer & server;\n     std::string conf_name;\n     TCPProtocolStackData & stack_data;\ndiff --git a/src/Server/ProxyV1HandlerFactory.h b/src/Server/ProxyV1HandlerFactory.h\nindex 028596d745d0..0398c8c1ccff 100644\n--- a/src/Server/ProxyV1HandlerFactory.h\n+++ b/src/Server/ProxyV1HandlerFactory.h\n@@ -16,7 +16,7 @@ class ProxyV1HandlerFactory : public TCPServerConnectionFactory\n {\n private:\n     IServer & server;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string conf_name;\n \n     class DummyTCPHandler : public Poco::Net::TCPServerConnection\n@@ -28,7 +28,7 @@ class ProxyV1HandlerFactory : public TCPServerConnectionFactory\n \n public:\n     explicit ProxyV1HandlerFactory(IServer & server_, const std::string & conf_name_)\n-        : server(server_), log(&Poco::Logger::get(\"ProxyV1HandlerFactory\")), conf_name(conf_name_)\n+        : server(server_), log(getLogger(\"ProxyV1HandlerFactory\")), conf_name(conf_name_)\n     {\n     }\n \ndiff --git a/src/Server/ReplicasStatusHandler.cpp b/src/Server/ReplicasStatusHandler.cpp\nindex 07f3b67b6a7e..91c6bd722d32 100644\n--- a/src/Server/ReplicasStatusHandler.cpp\n+++ b/src/Server/ReplicasStatusHandler.cpp\n@@ -118,7 +118,7 @@ void ReplicasStatusHandler::handleRequest(HTTPServerRequest & request, HTTPServe\n         }\n         catch (...)\n         {\n-            LOG_ERROR((&Poco::Logger::get(\"ReplicasStatusHandler\")), \"Cannot send exception to client\");\n+            LOG_ERROR((getLogger(\"ReplicasStatusHandler\")), \"Cannot send exception to client\");\n         }\n     }\n }\ndiff --git a/src/Server/TCPHandler.cpp b/src/Server/TCPHandler.cpp\nindex fa7206eeaac9..ec6b374518d7 100644\n--- a/src/Server/TCPHandler.cpp\n+++ b/src/Server/TCPHandler.cpp\n@@ -189,7 +189,7 @@ TCPHandler::TCPHandler(IServer & server_, TCPServer & tcp_server_, const Poco::N\n     , server(server_)\n     , tcp_server(tcp_server_)\n     , parse_proxy_protocol(parse_proxy_protocol_)\n-    , log(&Poco::Logger::get(\"TCPHandler\"))\n+    , log(getLogger(\"TCPHandler\"))\n     , read_event(read_event_)\n     , write_event(write_event_)\n     , server_display_name(std::move(server_display_name_))\n@@ -200,7 +200,7 @@ TCPHandler::TCPHandler(IServer & server_, TCPServer & tcp_server_, const Poco::N\n : Poco::Net::TCPServerConnection(socket_)\n     , server(server_)\n     , tcp_server(tcp_server_)\n-    , log(&Poco::Logger::get(\"TCPHandler\"))\n+    , log(getLogger(\"TCPHandler\"))\n     , forwarded_for(stack_data.forwarded_for)\n     , certificate(stack_data.certificate)\n     , read_event(read_event_)\ndiff --git a/src/Server/TCPHandler.h b/src/Server/TCPHandler.h\nindex 4eb84ee5eee5..26cecf466629 100644\n--- a/src/Server/TCPHandler.h\n+++ b/src/Server/TCPHandler.h\n@@ -160,7 +160,7 @@ class TCPHandler : public Poco::Net::TCPServerConnection\n     IServer & server;\n     TCPServer & tcp_server;\n     bool parse_proxy_protocol = false;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     String forwarded_for;\n     String certificate;\ndiff --git a/src/Server/TCPHandlerFactory.h b/src/Server/TCPHandlerFactory.h\nindex 3eb032f4250b..d65c9898b23b 100644\n--- a/src/Server/TCPHandlerFactory.h\n+++ b/src/Server/TCPHandlerFactory.h\n@@ -18,7 +18,7 @@ class TCPHandlerFactory : public TCPServerConnectionFactory\n private:\n     IServer & server;\n     bool parse_proxy_protocol = false;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string server_display_name;\n \n     ProfileEvents::Event read_event;\n@@ -38,7 +38,7 @@ class TCPHandlerFactory : public TCPServerConnectionFactory\n       */\n     TCPHandlerFactory(IServer & server_, bool secure_, bool parse_proxy_protocol_, const ProfileEvents::Event & read_event_ = ProfileEvents::end(), const ProfileEvents::Event & write_event_ = ProfileEvents::end())\n         : server(server_), parse_proxy_protocol(parse_proxy_protocol_)\n-        , log(&Poco::Logger::get(std::string(\"TCP\") + (secure_ ? \"S\" : \"\") + \"HandlerFactory\"))\n+        , log(getLogger(std::string(\"TCP\") + (secure_ ? \"S\" : \"\") + \"HandlerFactory\"))\n         , read_event(read_event_)\n         , write_event(write_event_)\n     {\ndiff --git a/src/Server/TCPProtocolStackFactory.h b/src/Server/TCPProtocolStackFactory.h\nindex 7373e6e1c4ea..b76bb8d72fdf 100644\n--- a/src/Server/TCPProtocolStackFactory.h\n+++ b/src/Server/TCPProtocolStackFactory.h\n@@ -23,7 +23,7 @@ class TCPProtocolStackFactory : public TCPServerConnectionFactory\n {\n private:\n     IServer & server [[maybe_unused]];\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string conf_name;\n     std::vector<TCPServerConnectionFactory::Ptr> stack;\n     AllowedClientHosts allowed_client_hosts;\n@@ -38,7 +38,7 @@ class TCPProtocolStackFactory : public TCPServerConnectionFactory\n public:\n     template <typename... T>\n     explicit TCPProtocolStackFactory(IServer & server_, const std::string & conf_name_, T... factory)\n-        : server(server_), log(&Poco::Logger::get(\"TCPProtocolStackFactory\")), conf_name(conf_name_), stack({factory...})\n+        : server(server_), log(getLogger(\"TCPProtocolStackFactory\")), conf_name(conf_name_), stack({factory...})\n     {\n         const auto & config = server.config();\n         /// Fill list of allowed hosts.\ndiff --git a/src/Server/TLSHandlerFactory.h b/src/Server/TLSHandlerFactory.h\nindex 9e3002d29719..19602c7d25e9 100644\n--- a/src/Server/TLSHandlerFactory.h\n+++ b/src/Server/TLSHandlerFactory.h\n@@ -19,7 +19,7 @@ class TLSHandlerFactory : public TCPServerConnectionFactory\n {\n private:\n     IServer & server;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::string conf_name;\n \n     class DummyTCPHandler : public Poco::Net::TCPServerConnection\n@@ -31,7 +31,7 @@ class TLSHandlerFactory : public TCPServerConnectionFactory\n \n public:\n     explicit TLSHandlerFactory(IServer & server_, const std::string & conf_name_)\n-        : server(server_), log(&Poco::Logger::get(\"TLSHandlerFactory\")), conf_name(conf_name_)\n+        : server(server_), log(getLogger(\"TLSHandlerFactory\")), conf_name(conf_name_)\n     {\n     }\n \ndiff --git a/src/Storages/Cache/ExternalDataSourceCache.h b/src/Storages/Cache/ExternalDataSourceCache.h\nindex 937801c47671..a5dea2f63db8 100644\n--- a/src/Storages/Cache/ExternalDataSourceCache.h\n+++ b/src/Storages/Cache/ExternalDataSourceCache.h\n@@ -91,7 +91,7 @@ class ExternalDataSourceCache : private boost::noncopyable\n     std::mutex mutex;\n     std::unique_ptr<RemoteFileCacheType> lru_caches;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"ExternalDataSourceCache\");\n+    LoggerPtr log = getLogger(\"ExternalDataSourceCache\");\n \n     String calculateLocalPath(IRemoteFileMetadataPtr meta) const;\n \ndiff --git a/src/Storages/Cache/RemoteCacheController.cpp b/src/Storages/Cache/RemoteCacheController.cpp\nindex b72f5336ea47..403d0c8e43b9 100644\n--- a/src/Storages/Cache/RemoteCacheController.cpp\n+++ b/src/Storages/Cache/RemoteCacheController.cpp\n@@ -20,7 +20,7 @@ namespace ErrorCodes\n \n std::shared_ptr<RemoteCacheController> RemoteCacheController::recover(const std::filesystem::path & local_path_)\n {\n-    auto * log = &Poco::Logger::get(\"RemoteCacheController\");\n+    auto log = getLogger(\"RemoteCacheController\");\n \n     if (!std::filesystem::exists(local_path_ / \"data.bin\"))\n     {\ndiff --git a/src/Storages/Cache/RemoteCacheController.h b/src/Storages/Cache/RemoteCacheController.h\nindex fafe363bbd48..782a6b895198 100644\n--- a/src/Storages/Cache/RemoteCacheController.h\n+++ b/src/Storages/Cache/RemoteCacheController.h\n@@ -116,7 +116,7 @@ class RemoteCacheController\n     //std::shared_ptr<ReadBuffer> remote_read_buffer;\n     std::unique_ptr<WriteBufferFromFileBase> data_file_writer;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"RemoteCacheController\");\n+    LoggerPtr log = getLogger(\"RemoteCacheController\");\n };\n using RemoteCacheControllerPtr = std::shared_ptr<RemoteCacheController>;\n \ndiff --git a/src/Storages/DataLakes/DeltaLakeMetadataParser.cpp b/src/Storages/DataLakes/DeltaLakeMetadataParser.cpp\nindex b8bffb267e57..3584f137225d 100644\n--- a/src/Storages/DataLakes/DeltaLakeMetadataParser.cpp\n+++ b/src/Storages/DataLakes/DeltaLakeMetadataParser.cpp\n@@ -314,7 +314,7 @@ struct DeltaLakeMetadataParser<Configuration, MetadataReadHelper>::Impl\n         return version;\n     }\n \n-    Poco::Logger * log = &Poco::Logger::get(\"DeltaLakeMetadataParser\");\n+    LoggerPtr log = getLogger(\"DeltaLakeMetadataParser\");\n };\n \n \ndiff --git a/src/Storages/DataLakes/HudiMetadataParser.cpp b/src/Storages/DataLakes/HudiMetadataParser.cpp\nindex 78d69c839891..699dfe8fda02 100644\n--- a/src/Storages/DataLakes/HudiMetadataParser.cpp\n+++ b/src/Storages/DataLakes/HudiMetadataParser.cpp\n@@ -50,7 +50,7 @@ struct HudiMetadataParser<Configuration, MetadataReadHelper>::Impl\n       */\n     Strings processMetadataFiles(const Configuration & configuration)\n     {\n-        auto * log = &Poco::Logger::get(\"HudiMetadataParser\");\n+        auto log = getLogger(\"HudiMetadataParser\");\n \n         const auto keys = MetadataReadHelper::listFiles(configuration, \"\", Poco::toLower(configuration.format));\n \ndiff --git a/src/Storages/DataLakes/IStorageDataLake.h b/src/Storages/DataLakes/IStorageDataLake.h\nindex d87b1272ba68..db3f835494f8 100644\n--- a/src/Storages/DataLakes/IStorageDataLake.h\n+++ b/src/Storages/DataLakes/IStorageDataLake.h\n@@ -25,7 +25,7 @@ class IStorageDataLake : public Storage\n     explicit IStorageDataLake(const Configuration & configuration_, ContextPtr context_, bool attach, Args && ...args)\n         : Storage(getConfigurationForDataRead(configuration_, context_, {}, attach), context_, std::forward<Args>(args)...)\n         , base_configuration(configuration_)\n-        , log(&Poco::Logger::get(getName())) {} // NOLINT(clang-analyzer-optin.cplusplus.VirtualCall)\n+        , log(getLogger(getName())) {} // NOLINT(clang-analyzer-optin.cplusplus.VirtualCall)\n \n     template <class ...Args>\n     static StoragePtr create(const Configuration & configuration_, ContextPtr context_, bool attach, Args && ...args)\n@@ -78,7 +78,7 @@ class IStorageDataLake : public Storage\n                 configuration.keys = keys;\n \n             LOG_TRACE(\n-                &Poco::Logger::get(\"DataLake\"),\n+                getLogger(\"DataLake\"),\n                 \"New configuration path: {}, keys: {}\",\n                 configuration.getPath(), fmt::join(configuration.keys, \", \"));\n \n@@ -112,7 +112,7 @@ class IStorageDataLake : public Storage\n \n     Configuration base_configuration;\n     std::mutex configuration_update_mutex;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \ndiff --git a/src/Storages/DataLakes/Iceberg/IcebergMetadata.cpp b/src/Storages/DataLakes/Iceberg/IcebergMetadata.cpp\nindex e0c7e26a2e12..e01a9a831c09 100644\n--- a/src/Storages/DataLakes/Iceberg/IcebergMetadata.cpp\n+++ b/src/Storages/DataLakes/Iceberg/IcebergMetadata.cpp\n@@ -58,7 +58,7 @@ IcebergMetadata::IcebergMetadata(\n     , manifest_list_file(std::move(manifest_list_file_))\n     , current_schema_id(current_schema_id_)\n     , schema(std::move(schema_))\n-    , log(&Poco::Logger::get(\"IcebergMetadata\"))\n+    , log(getLogger(\"IcebergMetadata\"))\n {\n }\n \n@@ -375,7 +375,7 @@ std::pair<Int32, String> getMetadataFileAndVersion(const StorageS3::Configuratio\n std::unique_ptr<IcebergMetadata> parseIcebergMetadata(const StorageS3::Configuration & configuration, ContextPtr context_)\n {\n     const auto [metadata_version, metadata_file_path] = getMetadataFileAndVersion(configuration);\n-    LOG_DEBUG(&Poco::Logger::get(\"IcebergMetadata\"), \"Parse metadata {}\", metadata_file_path);\n+    LOG_DEBUG(getLogger(\"IcebergMetadata\"), \"Parse metadata {}\", metadata_file_path);\n     auto buf = S3DataLakeMetadataReadHelper::createReadBuffer(metadata_file_path, context_, configuration);\n     String json_str;\n     readJSONObjectPossiblyInvalid(json_str, *buf);\ndiff --git a/src/Storages/DataLakes/Iceberg/IcebergMetadata.h b/src/Storages/DataLakes/Iceberg/IcebergMetadata.h\nindex d42ad84f472e..3e6a2ec34157 100644\n--- a/src/Storages/DataLakes/Iceberg/IcebergMetadata.h\n+++ b/src/Storages/DataLakes/Iceberg/IcebergMetadata.h\n@@ -84,7 +84,7 @@ class IcebergMetadata : WithContext\n     Int32 current_schema_id;\n     NamesAndTypesList schema;\n     Strings data_files;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n };\n \ndiff --git a/src/Storages/DataLakes/S3MetadataReader.cpp b/src/Storages/DataLakes/S3MetadataReader.cpp\nindex ac472c190e44..d66e21550a33 100644\n--- a/src/Storages/DataLakes/S3MetadataReader.cpp\n+++ b/src/Storages/DataLakes/S3MetadataReader.cpp\n@@ -77,7 +77,7 @@ std::vector<String> S3DataLakeMetadataReadHelper::listFiles(\n         is_finished = !outcome.GetResult().GetIsTruncated();\n     }\n \n-    LOG_TRACE(&Poco::Logger::get(\"S3DataLakeMetadataReadHelper\"), \"Listed {} files\", res.size());\n+    LOG_TRACE(getLogger(\"S3DataLakeMetadataReadHelper\"), \"Listed {} files\", res.size());\n \n     return res;\n }\ndiff --git a/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.cpp b/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.cpp\nindex 26fa489a63df..4e01cb2c6cff 100644\n--- a/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.cpp\n+++ b/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.cpp\n@@ -60,7 +60,7 @@ namespace\n {\n \n template <typename PoolFactory>\n-ConnectionPoolPtrs createPoolsForAddresses(const Cluster::Addresses & addresses, PoolFactory && factory, Poco::Logger * log)\n+ConnectionPoolPtrs createPoolsForAddresses(const Cluster::Addresses & addresses, PoolFactory && factory, LoggerPtr log)\n {\n     ConnectionPoolPtrs pools;\n \n@@ -121,7 +121,7 @@ DistributedAsyncInsertDirectoryQueue::DistributedAsyncInsertDirectoryQueue(\n     , default_sleep_time(storage.getDistributedSettingsRef().background_insert_sleep_time_ms.totalMilliseconds())\n     , sleep_time(default_sleep_time)\n     , max_sleep_time(storage.getDistributedSettingsRef().background_insert_max_sleep_time_ms.totalMilliseconds())\n-    , log(&Poco::Logger::get(getLoggerName()))\n+    , log(getLogger(getLoggerName()))\n     , monitor_blocker(monitor_blocker_)\n     , metric_pending_bytes(CurrentMetrics::DistributedBytesToInsert, 0)\n     , metric_pending_files(CurrentMetrics::DistributedFilesToInsert, 0)\ndiff --git a/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.h b/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.h\nindex 8bbd99c786a6..f7d7553851a8 100644\n--- a/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.h\n+++ b/src/Storages/Distributed/DistributedAsyncInsertDirectoryQueue.h\n@@ -145,7 +145,7 @@ class DistributedAsyncInsertDirectoryQueue\n     const std::chrono::milliseconds max_sleep_time;\n     std::chrono::time_point<std::chrono::system_clock> last_decrease_time {std::chrono::system_clock::now()};\n     std::mutex mutex;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     ActionBlocker & monitor_blocker;\n \n     BackgroundSchedulePoolTaskHolder task_handle;\ndiff --git a/src/Storages/Distributed/DistributedAsyncInsertHeader.cpp b/src/Storages/Distributed/DistributedAsyncInsertHeader.cpp\nindex a8ed89e66f12..cfcee4dc8a26 100644\n--- a/src/Storages/Distributed/DistributedAsyncInsertHeader.cpp\n+++ b/src/Storages/Distributed/DistributedAsyncInsertHeader.cpp\n@@ -18,7 +18,7 @@ namespace ErrorCodes\n     extern const int CHECKSUM_DOESNT_MATCH;\n }\n \n-DistributedAsyncInsertHeader DistributedAsyncInsertHeader::read(ReadBufferFromFile & in, Poco::Logger * log)\n+DistributedAsyncInsertHeader DistributedAsyncInsertHeader::read(ReadBufferFromFile & in, LoggerPtr log)\n {\n     DistributedAsyncInsertHeader distributed_header;\n \ndiff --git a/src/Storages/Distributed/DistributedAsyncInsertHeader.h b/src/Storages/Distributed/DistributedAsyncInsertHeader.h\nindex a7330fa5ef1b..fb4b46964637 100644\n--- a/src/Storages/Distributed/DistributedAsyncInsertHeader.h\n+++ b/src/Storages/Distributed/DistributedAsyncInsertHeader.h\n@@ -38,7 +38,7 @@ struct DistributedAsyncInsertHeader\n     std::string block_header_string;\n     Block block_header;\n \n-    static DistributedAsyncInsertHeader read(ReadBufferFromFile & in, Poco::Logger * log);\n+    static DistributedAsyncInsertHeader read(ReadBufferFromFile & in, LoggerPtr log);\n     OpenTelemetry::TracingContextHolderPtr createTracingContextHolder(const char * function, std::shared_ptr<OpenTelemetrySpanLog> open_telemetry_span_log) const;\n };\n \ndiff --git a/src/Storages/Distributed/DistributedAsyncInsertHelpers.cpp b/src/Storages/Distributed/DistributedAsyncInsertHelpers.cpp\nindex 98073ba1e089..a9bdef31711d 100644\n--- a/src/Storages/Distributed/DistributedAsyncInsertHelpers.cpp\n+++ b/src/Storages/Distributed/DistributedAsyncInsertHelpers.cpp\n@@ -72,7 +72,7 @@ void writeRemoteConvert(\n     RemoteInserter & remote,\n     bool compression_expected,\n     ReadBufferFromFile & in,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     if (!remote.getHeader())\n     {\ndiff --git a/src/Storages/Distributed/DistributedAsyncInsertHelpers.h b/src/Storages/Distributed/DistributedAsyncInsertHelpers.h\nindex 9543450418ca..202d9ff6fff8 100644\n--- a/src/Storages/Distributed/DistributedAsyncInsertHelpers.h\n+++ b/src/Storages/Distributed/DistributedAsyncInsertHelpers.h\n@@ -1,9 +1,7 @@\n #pragma once\n \n-namespace Poco\n-{\n-class Logger;\n-}\n+#include <Common/Logger.h>\n+\n \n namespace DB\n {\n@@ -30,6 +28,6 @@ void writeRemoteConvert(\n     RemoteInserter & remote,\n     bool compression_expected,\n     ReadBufferFromFile & in,\n-    Poco::Logger * log);\n+    LoggerPtr log);\n \n }\ndiff --git a/src/Storages/Distributed/DistributedAsyncInsertSource.cpp b/src/Storages/Distributed/DistributedAsyncInsertSource.cpp\nindex 7992636ac112..33e53da2857f 100644\n--- a/src/Storages/Distributed/DistributedAsyncInsertSource.cpp\n+++ b/src/Storages/Distributed/DistributedAsyncInsertSource.cpp\n@@ -10,7 +10,7 @@ namespace DB\n \n struct DistributedAsyncInsertSource::Data\n {\n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n \n     ReadBufferFromFile in;\n     CompressedReadBuffer decompressing_in;\n@@ -19,7 +19,7 @@ struct DistributedAsyncInsertSource::Data\n     Block first_block;\n \n     explicit Data(const String & file_name)\n-        : log(&Poco::Logger::get(\"DistributedAsyncInsertSource\"))\n+        : log(getLogger(\"DistributedAsyncInsertSource\"))\n         , in(file_name)\n         , decompressing_in(in)\n         , block_in(decompressing_in, DistributedAsyncInsertHeader::read(in, log).revision)\ndiff --git a/src/Storages/Distributed/DistributedSink.cpp b/src/Storages/Distributed/DistributedSink.cpp\nindex 650539ef1e92..1efa98d0c13b 100644\n--- a/src/Storages/Distributed/DistributedSink.cpp\n+++ b/src/Storages/Distributed/DistributedSink.cpp\n@@ -62,7 +62,7 @@ namespace ErrorCodes\n     extern const int ABORTED;\n }\n \n-static Block adoptBlock(const Block & header, const Block & block, Poco::Logger * log)\n+static Block adoptBlock(const Block & header, const Block & block, LoggerPtr log)\n {\n     if (blocksHaveEqualStructure(header, block))\n         return block;\n@@ -84,7 +84,7 @@ static Block adoptBlock(const Block & header, const Block & block, Poco::Logger\n }\n \n \n-static void writeBlockConvert(PushingPipelineExecutor & executor, const Block & block, size_t repeats, Poco::Logger * log)\n+static void writeBlockConvert(PushingPipelineExecutor & executor, const Block & block, size_t repeats, LoggerPtr log)\n {\n     Block adopted_block = adoptBlock(executor.getHeader(), block, log);\n     for (size_t i = 0; i < repeats; ++i)\n@@ -126,7 +126,7 @@ DistributedSink::DistributedSink(\n     , insert_timeout(insert_timeout_)\n     , main_table(main_table_)\n     , columns_to_send(columns_to_send_.begin(), columns_to_send_.end())\n-    , log(&Poco::Logger::get(\"DistributedSink\"))\n+    , log(getLogger(\"DistributedSink\"))\n {\n     const auto & settings = context->getSettingsRef();\n     if (settings.max_distributed_depth && context->getClientInfo().distributed_depth >= settings.max_distributed_depth)\ndiff --git a/src/Storages/Distributed/DistributedSink.h b/src/Storages/Distributed/DistributedSink.h\nindex 1bb4419e1a56..654c1db354f3 100644\n--- a/src/Storages/Distributed/DistributedSink.h\n+++ b/src/Storages/Distributed/DistributedSink.h\n@@ -152,7 +152,7 @@ class DistributedSink : public SinkToStorage\n \n     std::atomic<unsigned> finished_jobs_count{0};\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/FileLog/FileLogConsumer.cpp b/src/Storages/FileLog/FileLogConsumer.cpp\nindex bfe081c7bad9..1bd3026ab8c6 100644\n--- a/src/Storages/FileLog/FileLogConsumer.cpp\n+++ b/src/Storages/FileLog/FileLogConsumer.cpp\n@@ -22,7 +22,7 @@ FileLogConsumer::FileLogConsumer(\n     ContextPtr context_,\n     size_t stream_number_,\n     size_t max_streams_number_)\n-    : log(&Poco::Logger::get(\"FileLogConsumer \" + toString(stream_number_)))\n+    : log(getLogger(\"FileLogConsumer \" + toString(stream_number_)))\n     , storage(storage_)\n     , batch_size(max_batch_size)\n     , poll_timeout(poll_timeout_)\ndiff --git a/src/Storages/FileLog/FileLogConsumer.h b/src/Storages/FileLog/FileLogConsumer.h\nindex b19f3a9350ba..e44bfeb18064 100644\n--- a/src/Storages/FileLog/FileLogConsumer.h\n+++ b/src/Storages/FileLog/FileLogConsumer.h\n@@ -42,7 +42,7 @@ class FileLogConsumer\n \n     BufferStatus buffer_status = BufferStatus::INIT;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     StorageFileLog & storage;\n \ndiff --git a/src/Storages/FileLog/FileLogDirectoryWatcher.cpp b/src/Storages/FileLog/FileLogDirectoryWatcher.cpp\nindex 9d488616e851..844b31fd7c91 100644\n--- a/src/Storages/FileLog/FileLogDirectoryWatcher.cpp\n+++ b/src/Storages/FileLog/FileLogDirectoryWatcher.cpp\n@@ -6,7 +6,7 @@ namespace DB\n FileLogDirectoryWatcher::FileLogDirectoryWatcher(const std::string & path_, StorageFileLog & storage_, ContextPtr context_)\n     : path(path_)\n     , storage(storage_)\n-    , log(&Poco::Logger::get(\"FileLogDirectoryWatcher(\" + path + \")\"))\n+    , log(getLogger(\"FileLogDirectoryWatcher(\" + path + \")\"))\n     , dw(std::make_unique<DirectoryWatcherBase>(*this, path, context_))\n {\n }\ndiff --git a/src/Storages/FileLog/FileLogDirectoryWatcher.h b/src/Storages/FileLog/FileLogDirectoryWatcher.h\nindex 9b7afcf8e129..1cf3697c7c07 100644\n--- a/src/Storages/FileLog/FileLogDirectoryWatcher.h\n+++ b/src/Storages/FileLog/FileLogDirectoryWatcher.h\n@@ -65,7 +65,7 @@ class FileLogDirectoryWatcher\n     /// accessed in thread created by dw.\n     Events events;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::mutex mutex;\n \ndiff --git a/src/Storages/FileLog/StorageFileLog.cpp b/src/Storages/FileLog/StorageFileLog.cpp\nindex ef776a3d3137..9c7648ef658b 100644\n--- a/src/Storages/FileLog/StorageFileLog.cpp\n+++ b/src/Storages/FileLog/StorageFileLog.cpp\n@@ -139,7 +139,7 @@ StorageFileLog::StorageFileLog(\n     , path(path_)\n     , metadata_base_path(std::filesystem::path(metadata_base_path_) / \"metadata\")\n     , format_name(format_name_)\n-    , log(&Poco::Logger::get(\"StorageFileLog (\" + table_id_.table_name + \")\"))\n+    , log(getLogger(\"StorageFileLog (\" + table_id_.table_name + \")\"))\n     , disk(getContext()->getStoragePolicy(\"default\")->getDisks().at(0))\n     , milliseconds_to_wait(filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds())\n {\ndiff --git a/src/Storages/FileLog/StorageFileLog.h b/src/Storages/FileLog/StorageFileLog.h\nindex 33442d8b33b5..cc5815a1cef4 100644\n--- a/src/Storages/FileLog/StorageFileLog.h\n+++ b/src/Storages/FileLog/StorageFileLog.h\n@@ -149,7 +149,7 @@ class StorageFileLog final : public IStorage, WithContext\n     FileInfos file_infos;\n \n     const String format_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     DiskPtr disk;\n \ndiff --git a/src/Storages/Freeze.cpp b/src/Storages/Freeze.cpp\nindex b9642ec79078..a5a5a07c9a12 100644\n--- a/src/Storages/Freeze.cpp\n+++ b/src/Storages/Freeze.cpp\n@@ -76,7 +76,7 @@ bool FreezeMetaData::load(DiskPtr data_disk, const String & path)\n     readIntText(version, buffer);\n     if (version < 1 || version > 2)\n     {\n-        LOG_ERROR(&Poco::Logger::get(\"FreezeMetaData\"), \"Unknown frozen metadata version: {}\", version);\n+        LOG_ERROR(getLogger(\"FreezeMetaData\"), \"Unknown frozen metadata version: {}\", version);\n         return false;\n     }\n     DB::assertChar('\\n', buffer);\ndiff --git a/src/Storages/Freeze.h b/src/Storages/Freeze.h\nindex a64be7465dd2..5775653aaeaa 100644\n--- a/src/Storages/Freeze.h\n+++ b/src/Storages/Freeze.h\n@@ -38,7 +38,7 @@ class Unfreezer\n private:\n     ContextPtr local_context;\n     zkutil::ZooKeeperPtr zookeeper;\n-    Poco::Logger * log = &Poco::Logger::get(\"Unfreezer\");\n+    LoggerPtr log = getLogger(\"Unfreezer\");\n     static constexpr std::string_view backup_directory_prefix = \"shadow\";\n     static bool removeFreezedPart(DiskPtr disk, const String & path, const String & part_name, ContextPtr local_context, zkutil::ZooKeeperPtr zookeeper);\n };\ndiff --git a/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.cpp b/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.cpp\nindex 553473fcc9eb..65df2c020ba1 100644\n--- a/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.cpp\n+++ b/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.cpp\n@@ -44,7 +44,7 @@ AsynchronousReadBufferFromHDFS::AsynchronousReadBufferFromHDFS(\n     , prefetch_buffer(settings_.remote_fs_buffer_size)\n     , read_until_position(impl->getFileSize())\n     , use_prefetch(settings_.remote_fs_prefetch)\n-    , log(&Poco::Logger::get(\"AsynchronousReadBufferFromHDFS\"))\n+    , log(getLogger(\"AsynchronousReadBufferFromHDFS\"))\n {\n     ProfileEvents::increment(ProfileEvents::RemoteFSBuffers);\n }\ndiff --git a/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.h b/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.h\nindex 9c01bd6e434e..1d3e8b8e3e98 100644\n--- a/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.h\n+++ b/src/Storages/HDFS/AsynchronousReadBufferFromHDFS.h\n@@ -62,7 +62,7 @@ class AsynchronousReadBufferFromHDFS : public ReadBufferFromFileBase\n     std::optional<size_t> read_until_position;\n     bool use_prefetch;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Metrics to profile prefetch\n     Stopwatch interval_watch;\ndiff --git a/src/Storages/HDFS/HDFSCommon.cpp b/src/Storages/HDFS/HDFSCommon.cpp\nindex 12b32b740deb..f9a55a1285ad 100644\n--- a/src/Storages/HDFS/HDFSCommon.cpp\n+++ b/src/Storages/HDFS/HDFSCommon.cpp\n@@ -55,7 +55,7 @@ void HDFSBuilderWrapper::loadFromConfig(\n             need_kinit = true;\n             hadoop_kerberos_keytab = config.getString(key_path);\n             #else // USE_KRB5\n-            LOG_WARNING(&Poco::Logger::get(\"HDFSClient\"), \"hadoop_kerberos_keytab parameter is ignored because ClickHouse was built without support of krb5 library.\");\n+            LOG_WARNING(getLogger(\"HDFSClient\"), \"hadoop_kerberos_keytab parameter is ignored because ClickHouse was built without support of krb5 library.\");\n             #endif // USE_KRB5\n             continue;\n         }\n@@ -66,7 +66,7 @@ void HDFSBuilderWrapper::loadFromConfig(\n             hadoop_kerberos_principal = config.getString(key_path);\n             hdfsBuilderSetPrincipal(hdfs_builder, hadoop_kerberos_principal.c_str());\n             #else // USE_KRB5\n-            LOG_WARNING(&Poco::Logger::get(\"HDFSClient\"), \"hadoop_kerberos_principal parameter is ignored because ClickHouse was built without support of krb5 library.\");\n+            LOG_WARNING(getLogger(\"HDFSClient\"), \"hadoop_kerberos_principal parameter is ignored because ClickHouse was built without support of krb5 library.\");\n             #endif // USE_KRB5\n             continue;\n         }\n@@ -81,7 +81,7 @@ void HDFSBuilderWrapper::loadFromConfig(\n             hadoop_security_kerberos_ticket_cache_path = config.getString(key_path);\n             // standard param - pass further\n             #else // USE_KRB5\n-            LOG_WARNING(&Poco::Logger::get(\"HDFSClient\"), \"hadoop.security.kerberos.ticket.cache.path parameter is ignored because ClickHouse was built without support of krb5 library.\");\n+            LOG_WARNING(getLogger(\"HDFSClient\"), \"hadoop.security.kerberos.ticket.cache.path parameter is ignored because ClickHouse was built without support of krb5 library.\");\n             #endif // USE_KRB5\n         }\n \n@@ -95,7 +95,7 @@ void HDFSBuilderWrapper::loadFromConfig(\n #if USE_KRB5\n void HDFSBuilderWrapper::runKinit()\n {\n-    LOG_DEBUG(&Poco::Logger::get(\"HDFSClient\"), \"Running KerberosInit\");\n+    LOG_DEBUG(getLogger(\"HDFSClient\"), \"Running KerberosInit\");\n     try\n     {\n         kerberosInit(hadoop_kerberos_keytab,hadoop_kerberos_principal,hadoop_security_kerberos_ticket_cache_path);\n@@ -104,7 +104,7 @@ void HDFSBuilderWrapper::runKinit()\n     {\n         throw Exception(ErrorCodes::KERBEROS_ERROR, \"KerberosInit failure: {}\", getExceptionMessage(e, false));\n     }\n-    LOG_DEBUG(&Poco::Logger::get(\"HDFSClient\"), \"Finished KerberosInit\");\n+    LOG_DEBUG(getLogger(\"HDFSClient\"), \"Finished KerberosInit\");\n }\n #endif // USE_KRB5\n \ndiff --git a/src/Storages/HDFS/StorageHDFS.h b/src/Storages/HDFS/StorageHDFS.h\nindex f1f0019d3e05..7170763c959f 100644\n--- a/src/Storages/HDFS/StorageHDFS.h\n+++ b/src/Storages/HDFS/StorageHDFS.h\n@@ -105,7 +105,7 @@ class StorageHDFS final : public IStorage, WithContext\n     bool is_path_with_globs;\n     NamesAndTypesList virtual_columns;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"StorageHDFS\");\n+    LoggerPtr log = getLogger(\"StorageHDFS\");\n };\n \n class PullingPipelineExecutor;\ndiff --git a/src/Storages/HDFS/StorageHDFSCluster.cpp b/src/Storages/HDFS/StorageHDFSCluster.cpp\nindex 2e8129b9845e..fad294361026 100644\n--- a/src/Storages/HDFS/StorageHDFSCluster.cpp\n+++ b/src/Storages/HDFS/StorageHDFSCluster.cpp\n@@ -45,7 +45,7 @@ StorageHDFSCluster::StorageHDFSCluster(\n     const ConstraintsDescription & constraints_,\n     const String & compression_method_,\n     bool structure_argument_was_provided_)\n-    : IStorageCluster(cluster_name_, table_id_, &Poco::Logger::get(\"StorageHDFSCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n+    : IStorageCluster(cluster_name_, table_id_, getLogger(\"StorageHDFSCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n     , uri(uri_)\n     , format_name(format_name_)\n     , compression_method(compression_method_)\ndiff --git a/src/Storages/Hive/HiveCommon.cpp b/src/Storages/Hive/HiveCommon.cpp\nindex 609adcf65c94..b58302f262ec 100644\n--- a/src/Storages/Hive/HiveCommon.cpp\n+++ b/src/Storages/Hive/HiveCommon.cpp\n@@ -25,7 +25,7 @@ static const int hive_metastore_client_recv_timeout_ms = 10000;\n static const int hive_metastore_client_send_timeout_ms = 10000;\n \n ThriftHiveMetastoreClientPool::ThriftHiveMetastoreClientPool(ThriftHiveMetastoreClientBuilder builder_)\n-    : PoolBase<Object>(max_hive_metastore_client_connections, &Poco::Logger::get(\"ThriftHiveMetastoreClientPool\")), builder(builder_)\n+    : PoolBase<Object>(max_hive_metastore_client_connections, getLogger(\"ThriftHiveMetastoreClientPool\")), builder(builder_)\n {\n }\n \ndiff --git a/src/Storages/Hive/HiveCommon.h b/src/Storages/Hive/HiveCommon.h\nindex e2c19fb1684e..0f9d3364ffd4 100644\n--- a/src/Storages/Hive/HiveCommon.h\n+++ b/src/Storages/Hive/HiveCommon.h\n@@ -115,7 +115,7 @@ class HiveMetastoreClient\n         const bool empty_partition_keys;\n         const HiveFilesCachePtr hive_files_cache;\n \n-        Poco::Logger * log = &Poco::Logger::get(\"HiveMetastoreClient\");\n+        LoggerPtr log = getLogger(\"HiveMetastoreClient\");\n     };\n \n \n@@ -138,7 +138,7 @@ class HiveMetastoreClient\n     CacheBase<String, HiveTableMetadata> table_metadata_cache;\n     ThriftHiveMetastoreClientPool client_pool;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"HiveMetastoreClient\");\n+    LoggerPtr log = getLogger(\"HiveMetastoreClient\");\n };\n \n using HiveMetastoreClientPtr = std::shared_ptr<HiveMetastoreClient>;\ndiff --git a/src/Storages/Hive/StorageHive.cpp b/src/Storages/Hive/StorageHive.cpp\nindex a9347ac4d995..a7ee8ff98916 100644\n--- a/src/Storages/Hive/StorageHive.cpp\n+++ b/src/Storages/Hive/StorageHive.cpp\n@@ -411,7 +411,7 @@ class StorageHiveSource : public ISource, WithContext\n     bool generate_chunk_from_metadata{false};\n     UInt64 current_file_remained_rows = 0;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"StorageHive\");\n+    LoggerPtr log = getLogger(\"StorageHive\");\n };\n \n \n@@ -780,7 +780,7 @@ class ReadFromHive : public SourceStepWithFilter\n         HDFSFSPtr fs_,\n         HiveMetastoreClient::HiveTableMetadataPtr hive_table_metadata_,\n         Block sample_block_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         ContextPtr context_,\n         size_t max_block_size_,\n         size_t num_streams_)\n@@ -805,7 +805,7 @@ class ReadFromHive : public SourceStepWithFilter\n     HDFSFSPtr fs;\n     HiveMetastoreClient::HiveTableMetadataPtr hive_table_metadata;\n     Block sample_block;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     ContextPtr context;\n     size_t max_block_size;\ndiff --git a/src/Storages/Hive/StorageHive.h b/src/Storages/Hive/StorageHive.h\nindex b0ec96604ccc..07440097f7a2 100644\n--- a/src/Storages/Hive/StorageHive.h\n+++ b/src/Storages/Hive/StorageHive.h\n@@ -149,7 +149,7 @@ class StorageHive final : public IStorage, WithContext\n \n     std::shared_ptr<HiveSettings> storage_settings;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"StorageHive\");\n+    LoggerPtr log = getLogger(\"StorageHive\");\n };\n \n }\ndiff --git a/src/Storages/IMessageProducer.cpp b/src/Storages/IMessageProducer.cpp\nindex cf3146960417..c723ec77b700 100644\n--- a/src/Storages/IMessageProducer.cpp\n+++ b/src/Storages/IMessageProducer.cpp\n@@ -4,7 +4,7 @@\n namespace DB\n {\n \n-IMessageProducer::IMessageProducer(Poco::Logger * log_) : log(log_)\n+IMessageProducer::IMessageProducer(LoggerPtr log_) : log(log_)\n {\n }\n \ndiff --git a/src/Storages/IMessageProducer.h b/src/Storages/IMessageProducer.h\nindex 12580d5f94a3..c769c3251916 100644\n--- a/src/Storages/IMessageProducer.h\n+++ b/src/Storages/IMessageProducer.h\n@@ -16,7 +16,7 @@ namespace DB\n class IMessageProducer\n {\n public:\n-    explicit IMessageProducer(Poco::Logger * log_);\n+    explicit IMessageProducer(LoggerPtr log_);\n \n     /// Do some preparations.\n     virtual void start(const ContextPtr & context) = 0;\n@@ -30,14 +30,14 @@ class IMessageProducer\n     virtual ~IMessageProducer() = default;\n \n protected:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n /// Implements interface for concurrent message producing.\n class AsynchronousMessageProducer : public IMessageProducer\n {\n public:\n-    explicit AsynchronousMessageProducer(Poco::Logger * log_) : IMessageProducer(log_) {}\n+    explicit AsynchronousMessageProducer(LoggerPtr log_) : IMessageProducer(log_) {}\n \n     /// Create and schedule task in BackgroundSchedulePool that will produce messages.\n     void start(const ContextPtr & context) override;\ndiff --git a/src/Storages/IStorageCluster.cpp b/src/Storages/IStorageCluster.cpp\nindex 6f42d8f855ca..c9eb07bd9d11 100644\n--- a/src/Storages/IStorageCluster.cpp\n+++ b/src/Storages/IStorageCluster.cpp\n@@ -32,7 +32,7 @@ namespace DB\n IStorageCluster::IStorageCluster(\n     const String & cluster_name_,\n     const StorageID & table_id_,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     bool structure_argument_was_provided_)\n     : IStorage(table_id_)\n     , log(log_)\n@@ -54,7 +54,7 @@ class ReadFromCluster : public SourceStepWithFilter\n         ASTPtr query_to_send_,\n         QueryProcessingStage::Enum processed_stage_,\n         ClusterPtr cluster_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         ContextPtr context_)\n         : SourceStepWithFilter(DataStream{.header = std::move(sample_block)})\n         , storage(std::move(storage_))\n@@ -71,7 +71,7 @@ class ReadFromCluster : public SourceStepWithFilter\n     ASTPtr query_to_send;\n     QueryProcessingStage::Enum processed_stage;\n     ClusterPtr cluster;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     ContextPtr context;\n \n     std::optional<RemoteQueryExecutor::Extension> extension;\ndiff --git a/src/Storages/IStorageCluster.h b/src/Storages/IStorageCluster.h\nindex b233f20103db..8d93e94be9ad 100644\n--- a/src/Storages/IStorageCluster.h\n+++ b/src/Storages/IStorageCluster.h\n@@ -19,7 +19,7 @@ class IStorageCluster : public IStorage\n     IStorageCluster(\n         const String & cluster_name_,\n         const StorageID & table_id_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         bool structure_argument_was_provided_);\n \n     void read(\n@@ -46,7 +46,7 @@ class IStorageCluster : public IStorage\n     virtual void addColumnsStructureToQuery(ASTPtr & query, const String & structure, const ContextPtr & context) = 0;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     String cluster_name;\n     bool structure_argument_was_provided;\n };\ndiff --git a/src/Storages/Kafka/KafkaConsumer.cpp b/src/Storages/Kafka/KafkaConsumer.cpp\nindex 40f2897322de..47167e19a38b 100644\n--- a/src/Storages/Kafka/KafkaConsumer.cpp\n+++ b/src/Storages/Kafka/KafkaConsumer.cpp\n@@ -47,7 +47,7 @@ const auto DRAIN_TIMEOUT_MS = 5000ms;\n \n \n KafkaConsumer::KafkaConsumer(\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     size_t max_batch_size,\n     size_t poll_timeout_,\n     bool intermediate_commit_,\ndiff --git a/src/Storages/Kafka/KafkaConsumer.h b/src/Storages/Kafka/KafkaConsumer.h\nindex c4dfc56312fe..9cc78d428566 100644\n--- a/src/Storages/Kafka/KafkaConsumer.h\n+++ b/src/Storages/Kafka/KafkaConsumer.h\n@@ -62,7 +62,7 @@ class KafkaConsumer\n     };\n \n     KafkaConsumer(\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         size_t max_batch_size,\n         size_t poll_timeout_,\n         bool intermediate_commit_,\n@@ -150,7 +150,7 @@ class KafkaConsumer\n     std::string rdkafka_stat;\n \n     ConsumerPtr consumer;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const size_t batch_size = 1;\n     const size_t poll_timeout = 0;\n     size_t offsets_stored = 0;\ndiff --git a/src/Storages/Kafka/KafkaProducer.cpp b/src/Storages/Kafka/KafkaProducer.cpp\nindex edbfc76ef939..77676fb010b4 100644\n--- a/src/Storages/Kafka/KafkaProducer.cpp\n+++ b/src/Storages/Kafka/KafkaProducer.cpp\n@@ -18,7 +18,7 @@ namespace DB\n \n KafkaProducer::KafkaProducer(\n     ProducerPtr producer_, const std::string & topic_, std::chrono::milliseconds poll_timeout, std::atomic<bool> & shutdown_called_, const Block & header)\n-    : IMessageProducer(&Poco::Logger::get(\"KafkaProducer\"))\n+    : IMessageProducer(getLogger(\"KafkaProducer\"))\n     , producer(producer_)\n     , topic(topic_)\n     , timeout(poll_timeout)\ndiff --git a/src/Storages/Kafka/KafkaSource.cpp b/src/Storages/Kafka/KafkaSource.cpp\nindex 1fbd7e2d705f..dc62c13f6332 100644\n--- a/src/Storages/Kafka/KafkaSource.cpp\n+++ b/src/Storages/Kafka/KafkaSource.cpp\n@@ -33,7 +33,7 @@ KafkaSource::KafkaSource(\n     const StorageSnapshotPtr & storage_snapshot_,\n     const ContextPtr & context_,\n     const Names & columns,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     size_t max_block_size_,\n     bool commit_in_suffix_)\n     : ISource(storage_snapshot_->getSampleBlockForColumns(columns))\ndiff --git a/src/Storages/Kafka/KafkaSource.h b/src/Storages/Kafka/KafkaSource.h\nindex 485a8e55b6a9..a1b94b15a19a 100644\n--- a/src/Storages/Kafka/KafkaSource.h\n+++ b/src/Storages/Kafka/KafkaSource.h\n@@ -22,7 +22,7 @@ class KafkaSource : public ISource\n         const StorageSnapshotPtr & storage_snapshot_,\n         const ContextPtr & context_,\n         const Names & columns,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         size_t max_block_size_,\n         bool commit_in_suffix = false);\n     ~KafkaSource() override;\n@@ -41,7 +41,7 @@ class KafkaSource : public ISource\n     StorageSnapshotPtr storage_snapshot;\n     ContextPtr context;\n     Names column_names;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     UInt64 max_block_size;\n \n     KafkaConsumerPtr consumer;\ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex 522a381700d0..aa347fc719de 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -327,7 +327,7 @@ StorageKafka::StorageKafka(\n     , max_rows_per_message(kafka_settings->kafka_max_rows_per_message.value)\n     , schema_name(getContext()->getMacros()->expand(kafka_settings->kafka_schema.value, macros_info))\n     , num_consumers(kafka_settings->kafka_num_consumers.value)\n-    , log(&Poco::Logger::get(\"StorageKafka (\" + table_id_.table_name + \")\"))\n+    , log(getLogger(\"StorageKafka (\" + table_id_.table_name + \")\"))\n     , intermediate_commit(kafka_settings->kafka_commit_every_batch.value)\n     , settings_adjustments(createSettingsAdjustments())\n     , thread_per_consumer(kafka_settings->kafka_thread_per_consumer.value)\ndiff --git a/src/Storages/Kafka/StorageKafka.h b/src/Storages/Kafka/StorageKafka.h\nindex d370d6018f77..f9a1e3ff6f3a 100644\n--- a/src/Storages/Kafka/StorageKafka.h\n+++ b/src/Storages/Kafka/StorageKafka.h\n@@ -101,7 +101,7 @@ class StorageKafka final : public IStorage, WithContext\n     const size_t max_rows_per_message;\n     const String schema_name;\n     const size_t num_consumers; /// total number of consumers\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const bool intermediate_commit;\n     const SettingsChanges settings_adjustments;\n \ndiff --git a/src/Storages/LiveView/StorageLiveView.cpp b/src/Storages/LiveView/StorageLiveView.cpp\nindex 3c116321083e..f81225bbee32 100644\n--- a/src/Storages/LiveView/StorageLiveView.cpp\n+++ b/src/Storages/LiveView/StorageLiveView.cpp\n@@ -209,7 +209,7 @@ StorageLiveView::StorageLiveView(\n     live_view_context = Context::createCopy(getContext());\n     live_view_context->makeQueryContext();\n \n-    log = &Poco::Logger::get(\"StorageLiveView (\" + table_id_.database_name + \".\" + table_id_.table_name + \")\");\n+    log = getLogger(\"StorageLiveView (\" + table_id_.database_name + \".\" + table_id_.table_name + \")\");\n \n     StorageInMemoryMetadata storage_metadata;\n     storage_metadata.setColumns(columns_);\ndiff --git a/src/Storages/LiveView/StorageLiveView.h b/src/Storages/LiveView/StorageLiveView.h\nindex e0566d586ee1..6b8780cb81b0 100644\n--- a/src/Storages/LiveView/StorageLiveView.h\n+++ b/src/Storages/LiveView/StorageLiveView.h\n@@ -184,7 +184,7 @@ using MilliSeconds = std::chrono::milliseconds;\n \n     ContextMutablePtr live_view_context;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     bool is_periodically_refreshed = false;\n     Seconds periodic_live_view_refresh;\ndiff --git a/src/Storages/MaterializedView/RefreshTask.cpp b/src/Storages/MaterializedView/RefreshTask.cpp\nindex bc26301e3b9e..daf7bd657841 100644\n--- a/src/Storages/MaterializedView/RefreshTask.cpp\n+++ b/src/Storages/MaterializedView/RefreshTask.cpp\n@@ -27,7 +27,7 @@ namespace ErrorCodes\n \n RefreshTask::RefreshTask(\n     const ASTRefreshStrategy & strategy)\n-    : log(&Poco::Logger::get(\"RefreshTask\"))\n+    : log(getLogger(\"RefreshTask\"))\n     , refresh_schedule(strategy)\n {}\n \ndiff --git a/src/Storages/MaterializedView/RefreshTask.h b/src/Storages/MaterializedView/RefreshTask.h\nindex 8a062f6f3591..78599f4f4b41 100644\n--- a/src/Storages/MaterializedView/RefreshTask.h\n+++ b/src/Storages/MaterializedView/RefreshTask.h\n@@ -62,7 +62,7 @@ class RefreshTask : public std::enable_shared_from_this<RefreshTask>\n     void setFakeTime(std::optional<Int64> t);\n \n private:\n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n     std::weak_ptr<IStorage> view_to_refresh;\n \n     /// Protects interrupt_execution and running_executor.\ndiff --git a/src/Storages/MergeTree/AsyncBlockIDsCache.cpp b/src/Storages/MergeTree/AsyncBlockIDsCache.cpp\nindex cc3bc8fc2a8e..9d64592ed64f 100644\n--- a/src/Storages/MergeTree/AsyncBlockIDsCache.cpp\n+++ b/src/Storages/MergeTree/AsyncBlockIDsCache.cpp\n@@ -60,7 +60,7 @@ AsyncBlockIDsCache<TStorage>::AsyncBlockIDsCache(TStorage & storage_)\n     , update_wait(storage.getSettings()->async_block_ids_cache_update_wait_ms)\n     , path(storage.getZooKeeperPath() + \"/async_blocks\")\n     , log_name(storage.getStorageID().getFullTableName() + \" (AsyncBlockIDsCache)\")\n-    , log(&Poco::Logger::get(log_name))\n+    , log(getLogger(log_name))\n {\n     task = storage.getContext()->getSchedulePool().createTask(log_name, [this]{ update(); });\n }\ndiff --git a/src/Storages/MergeTree/AsyncBlockIDsCache.h b/src/Storages/MergeTree/AsyncBlockIDsCache.h\nindex 38c38da0033d..bea012f1d329 100644\n--- a/src/Storages/MergeTree/AsyncBlockIDsCache.h\n+++ b/src/Storages/MergeTree/AsyncBlockIDsCache.h\n@@ -43,7 +43,7 @@ class AsyncBlockIDsCache\n     BackgroundSchedulePool::TaskHolder task;\n \n     const String log_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp b/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\nindex f4b92ff8c577..0cb9eb84bf8d 100644\n--- a/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\n+++ b/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\n@@ -57,7 +57,7 @@ std::string DataPartStorageOnDiskBase::getRelativePath() const\n     return fs::path(root_path) / part_dir / \"\";\n }\n \n-std::optional<String> DataPartStorageOnDiskBase::getRelativePathForPrefix(Poco::Logger * log, const String & prefix, bool detached, bool broken) const\n+std::optional<String> DataPartStorageOnDiskBase::getRelativePathForPrefix(LoggerPtr log, const String & prefix, bool detached, bool broken) const\n {\n     assert(!broken || detached);\n     String res;\n@@ -471,7 +471,7 @@ MutableDataPartStoragePtr DataPartStorageOnDiskBase::clonePart(\n     const DiskPtr & dst_disk,\n     const ReadSettings & read_settings,\n     const WriteSettings & write_settings,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     const std::function<void()> & cancellation_hook) const\n {\n     String path_to_clone = fs::path(to) / dir_path / \"\";\n@@ -505,7 +505,7 @@ MutableDataPartStoragePtr DataPartStorageOnDiskBase::clonePart(\n void DataPartStorageOnDiskBase::rename(\n     std::string new_root_path,\n     std::string new_part_dir,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     bool remove_new_dir_if_exists,\n     bool fsync_part_dir)\n {\n@@ -564,7 +564,7 @@ void DataPartStorageOnDiskBase::remove(\n     const MergeTreeDataPartChecksums & checksums,\n     std::list<ProjectionChecksums> projections,\n     bool is_temp,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     /// NOTE We rename part to delete_tmp_<relative_path> instead of delete_tmp_<name> to avoid race condition\n     /// when we try to remove two parts with the same name, but different relative paths,\n@@ -722,7 +722,7 @@ void DataPartStorageOnDiskBase::clearDirectory(\n     const CanRemoveDescription & can_remove_description,\n     const MergeTreeDataPartChecksums & checksums,\n     bool is_temp,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     auto disk = volume->getDisk();\n     auto [can_remove_shared_data, names_not_to_remove] = can_remove_description;\ndiff --git a/src/Storages/MergeTree/DataPartStorageOnDiskBase.h b/src/Storages/MergeTree/DataPartStorageOnDiskBase.h\nindex 339acce59537..52dc850c7fdd 100644\n--- a/src/Storages/MergeTree/DataPartStorageOnDiskBase.h\n+++ b/src/Storages/MergeTree/DataPartStorageOnDiskBase.h\n@@ -25,7 +25,7 @@ class DataPartStorageOnDiskBase : public IDataPartStorage\n     UInt64 calculateTotalSizeOnDisk() const override;\n \n     /// Returns path to place detached part in or nullopt if we don't need to detach part (if it already exists and has the same content)\n-    std::optional<String> getRelativePathForPrefix(Poco::Logger * log, const String & prefix, bool detached, bool broken) const override;\n+    std::optional<String> getRelativePathForPrefix(LoggerPtr log, const String & prefix, bool detached, bool broken) const override;\n \n     /// Returns true if detached part already exists and has the same content (compares checksums.txt and the list of files)\n     bool looksLikeBrokenDetachedPartHasTheSameContent(const String & detached_part_path, std::optional<String> & original_checksums_content,\n@@ -74,14 +74,14 @@ class DataPartStorageOnDiskBase : public IDataPartStorage\n         const DiskPtr & dst_disk,\n         const ReadSettings & read_settings,\n         const WriteSettings & write_settings,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         const std::function<void()> & cancellation_hook\n         ) const override;\n \n     void rename(\n         std::string new_root_path,\n         std::string new_part_dir,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         bool remove_new_dir_if_exists,\n         bool fsync_part_dir) override;\n \n@@ -90,7 +90,7 @@ class DataPartStorageOnDiskBase : public IDataPartStorage\n         const MergeTreeDataPartChecksums & checksums,\n         std::list<ProjectionChecksums> projections,\n         bool is_temp,\n-        Poco::Logger * log) override;\n+        LoggerPtr log) override;\n \n     void changeRootPath(const std::string & from_root, const std::string & to_root) override;\n     void createDirectories() override;\n@@ -130,7 +130,7 @@ class DataPartStorageOnDiskBase : public IDataPartStorage\n         const CanRemoveDescription & can_remove_description,\n         const MergeTreeDataPartChecksums & checksums,\n         bool is_temp,\n-        Poco::Logger * log);\n+        LoggerPtr log);\n \n     /// For names of expected data part files returns the actual names\n     /// of files in filesystem to which data of these files is written.\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex a59f2a356e88..ce70fbe18e50 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -99,7 +99,7 @@ struct ReplicatedFetchReadCallback\n \n Service::Service(StorageReplicatedMergeTree & data_)\n     : data(data_)\n-    , log(&Poco::Logger::get(data.getStorageID().getNameForLogs() + \" (Replicated PartsService)\"))\n+    , log(getLogger(data.getStorageID().getNameForLogs() + \" (Replicated PartsService)\"))\n {}\n \n std::string Service::getId(const std::string & node_id) const\n@@ -415,7 +415,7 @@ MergeTreeData::DataPartPtr Service::findPart(const String & name)\n \n Fetcher::Fetcher(StorageReplicatedMergeTree & data_)\n     : data(data_)\n-    , log(&Poco::Logger::get(data.getStorageID().getNameForLogs() + \" (Fetcher)\"))\n+    , log(getLogger(data.getStorageID().getNameForLogs() + \" (Fetcher)\"))\n {}\n \n std::pair<MergeTreeData::MutableDataPartPtr, scope_guard> Fetcher::fetchSelectedPart(\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.h b/src/Storages/MergeTree/DataPartsExchange.h\nindex 07939a660a87..8c15dc3cfdb4 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.h\n+++ b/src/Storages/MergeTree/DataPartsExchange.h\n@@ -55,7 +55,7 @@ class Service final : public InterserverIOEndpoint\n     /// StorageReplicatedMergeTree::shutdown() waits for all parts exchange handlers to finish,\n     /// so Service will never access dangling reference to storage\n     StorageReplicatedMergeTree & data;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n /** Client for getting the parts from the table *MergeTree.\n@@ -137,7 +137,7 @@ class Fetcher final : private boost::noncopyable\n        ThrottlerPtr throttler);\n \n     StorageReplicatedMergeTree & data;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp b/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp\nindex 5741e11aa224..1ffb51774302 100644\n--- a/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp\n+++ b/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp\n@@ -64,7 +64,7 @@ std::optional<EphemeralLockInZooKeeper> createEphemeralLockInZooKeeper(\n                 {\n                     const String & failed_op_path = ops[failed_idx]->getPath();\n                     LOG_DEBUG(\n-                        &Poco::Logger::get(\"createEphemeralLockInZooKeeper\"),\n+                        getLogger(\"createEphemeralLockInZooKeeper\"),\n                         \"Deduplication path already exists: deduplication_path={}\",\n                         failed_op_path);\n                     return EphemeralLockInZooKeeper{\"\", nullptr, \"\", failed_op_path};\n@@ -73,7 +73,7 @@ std::optional<EphemeralLockInZooKeeper> createEphemeralLockInZooKeeper(\n             else if (responses[0]->error == Coordination::Error::ZNODEEXISTS)\n             {\n                 LOG_DEBUG(\n-                    &Poco::Logger::get(\"createEphemeralLockInZooKeeper\"),\n+                    getLogger(\"createEphemeralLockInZooKeeper\"),\n                     \"Deduplication path already exists: deduplication_path={}\",\n                     deduplication_path);\n                 return {};\n@@ -119,7 +119,7 @@ EphemeralLockInZooKeeper::~EphemeralLockInZooKeeper()\n     {\n         if (Coordination::isHardwareError(e.code))\n             LOG_DEBUG(\n-                &Poco::Logger::get(\"EphemeralLockInZooKeeper\"),\n+                getLogger(\"EphemeralLockInZooKeeper\"),\n                 \"ZooKeeper communication error during unlock: code={} message='{}'\",\n                 e.code,\n                 e.message());\n@@ -130,7 +130,7 @@ EphemeralLockInZooKeeper::~EphemeralLockInZooKeeper()\n             /// But it's possible that the multi op request can be executed on server side, and client will not get response due to network issue.\n             /// In such case, assumeUnlocked() will not be called, so we'll get ZNONODE error here since the noded is already deleted\n             LOG_DEBUG(\n-                &Poco::Logger::get(\"EphemeralLockInZooKeeper\"),\n+                getLogger(\"EphemeralLockInZooKeeper\"),\n                 \"ZooKeeper node was already deleted: code={} message={}\",\n                 e.code,\n                 e.message());\n@@ -168,7 +168,7 @@ EphemeralLocksInAllPartitions::EphemeralLocksInAllPartitions(\n         Coordination::Error rc = zookeeper->tryMulti(lock_ops, lock_responses);\n         if (rc == Coordination::Error::ZBADVERSION)\n         {\n-            LOG_TRACE(&Poco::Logger::get(\"EphemeralLocksInAllPartitions\"), \"Someone has inserted a block in a new partition while we were creating locks. Retry.\");\n+            LOG_TRACE(getLogger(\"EphemeralLocksInAllPartitions\"), \"Someone has inserted a block in a new partition while we were creating locks. Retry.\");\n             continue;\n         }\n         else if (rc != Coordination::Error::ZOK)\ndiff --git a/src/Storages/MergeTree/IDataPartStorage.h b/src/Storages/MergeTree/IDataPartStorage.h\nindex afbe91a8a6d7..5899ef58cd57 100644\n--- a/src/Storages/MergeTree/IDataPartStorage.h\n+++ b/src/Storages/MergeTree/IDataPartStorage.h\n@@ -151,12 +151,12 @@ class IDataPartStorage : public boost::noncopyable\n         const MergeTreeDataPartChecksums & checksums,\n         std::list<ProjectionChecksums> projections,\n         bool is_temp,\n-        Poco::Logger * log) = 0;\n+        LoggerPtr log) = 0;\n \n     /// Get a name like 'prefix_partdir_tryN' which does not exist in a root dir.\n     /// TODO: remove it.\n     virtual std::optional<String> getRelativePathForPrefix(\n-        Poco::Logger * log, const String & prefix, bool detached, bool broken) const = 0;\n+        LoggerPtr log, const String & prefix, bool detached, bool broken) const = 0;\n \n     /// Reset part directory, used for in-memory parts.\n     /// TODO: remove it.\n@@ -263,7 +263,7 @@ class IDataPartStorage : public boost::noncopyable\n         const DiskPtr & disk,\n         const ReadSettings & read_settings,\n         const WriteSettings & write_settings,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         const std::function<void()> & cancellation_hook\n         ) const = 0;\n \n@@ -314,7 +314,7 @@ class IDataPartStorage : public boost::noncopyable\n     virtual void rename(\n         std::string new_root_path,\n         std::string new_part_dir,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         bool remove_new_dir_if_exists,\n         bool fsync_part_dir) = 0;\n \ndiff --git a/src/Storages/MergeTree/InsertBlockInfo.cpp b/src/Storages/MergeTree/InsertBlockInfo.cpp\nindex ac900f8cf097..2de3ae8996a7 100644\n--- a/src/Storages/MergeTree/InsertBlockInfo.cpp\n+++ b/src/Storages/MergeTree/InsertBlockInfo.cpp\n@@ -9,7 +9,7 @@ namespace ErrorCodes\n }\n \n AsyncInsertBlockInfo::AsyncInsertBlockInfo(\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     std::vector<std::string> && block_id_,\n     BlockWithPartition && block_,\n     std::optional<BlockWithPartition> && unmerged_block_with_partition_)\ndiff --git a/src/Storages/MergeTree/InsertBlockInfo.h b/src/Storages/MergeTree/InsertBlockInfo.h\nindex 3882373c0fa9..7d7ec0c9f292 100644\n--- a/src/Storages/MergeTree/InsertBlockInfo.h\n+++ b/src/Storages/MergeTree/InsertBlockInfo.h\n@@ -8,7 +8,7 @@ namespace DB\n struct SyncInsertBlockInfo\n {\n     SyncInsertBlockInfo(\n-        Poco::Logger * /*log_*/,\n+        LoggerPtr /*log_*/,\n         std::string && block_id_,\n         BlockWithPartition && /*block_*/,\n         std::optional<BlockWithPartition> && /*unmerged_block_with_partition_*/)\n@@ -25,7 +25,7 @@ struct SyncInsertBlockInfo\n \n struct AsyncInsertBlockInfo\n {\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::vector<std::string> block_id;\n     BlockWithPartition block_with_partition;\n     /// Some merging algorithms can mofidy the block which loses the information about the async insert offsets\n@@ -34,7 +34,7 @@ struct AsyncInsertBlockInfo\n     std::unordered_map<String, std::vector<size_t>> block_id_to_offset_idx;\n \n     AsyncInsertBlockInfo(\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         std::vector<std::string> && block_id_,\n         BlockWithPartition && block_,\n         std::optional<BlockWithPartition> && unmerged_block_with_partition_);\ndiff --git a/src/Storages/MergeTree/LeaderElection.h b/src/Storages/MergeTree/LeaderElection.h\nindex 2e48892563be..3bd486fd54a9 100644\n--- a/src/Storages/MergeTree/LeaderElection.h\n+++ b/src/Storages/MergeTree/LeaderElection.h\n@@ -19,7 +19,7 @@ namespace zkutil\n   * For now, every replica can become leader if there is no leader among replicas with old version.\n   */\n \n-void checkNoOldLeaders(Poco::Logger * log, ZooKeeper & zookeeper, const String path)\n+void checkNoOldLeaders(LoggerPtr log, ZooKeeper & zookeeper, const String path)\n {\n     /// Previous versions (before 21.12) used to create ephemeral sequential node path/leader_election-\n     /// Replica with the lexicographically smallest node name becomes leader (before 20.6) or enables multi-leader mode (since 20.6)\ndiff --git a/src/Storages/MergeTree/MergeFromLogEntryTask.cpp b/src/Storages/MergeTree/MergeFromLogEntryTask.cpp\nindex 23037b1ee7ab..ae6e398026d7 100644\n--- a/src/Storages/MergeTree/MergeFromLogEntryTask.cpp\n+++ b/src/Storages/MergeTree/MergeFromLogEntryTask.cpp\n@@ -28,7 +28,7 @@ MergeFromLogEntryTask::MergeFromLogEntryTask(\n     StorageReplicatedMergeTree & storage_,\n     IExecutableTask::TaskResultCallback & task_result_callback_)\n     : ReplicatedMergeMutateTaskBase(\n-        &Poco::Logger::get(\n+        getLogger(\n             storage_.getStorageID().getShortName() + \"::\" + selected_entry_->log_entry->new_part_name + \" (MergeFromLogEntryTask)\"),\n         storage_,\n         selected_entry_,\ndiff --git a/src/Storages/MergeTree/MergeTask.h b/src/Storages/MergeTree/MergeTask.h\nindex b2a5796737d8..6f5336baaad3 100644\n--- a/src/Storages/MergeTree/MergeTask.h\n+++ b/src/Storages/MergeTree/MergeTask.h\n@@ -228,7 +228,7 @@ class MergeTask\n         size_t sum_compressed_bytes_upper_bound{0};\n         bool blocks_are_granules_size{false};\n \n-        Poco::Logger * log{&Poco::Logger::get(\"MergeTask::PrepareStage\")};\n+        LoggerPtr log{getLogger(\"MergeTask::PrepareStage\")};\n \n         /// Dependencies for next stages\n         std::list<DB::NameAndTypePair>::const_iterator it_name_and_type;\n@@ -354,7 +354,7 @@ class MergeTask\n         MergeTasks tasks_for_projections;\n         MergeTasks::iterator projections_iterator;\n \n-        Poco::Logger * log{&Poco::Logger::get(\"MergeTask::MergeProjectionsStage\")};\n+        LoggerPtr log{getLogger(\"MergeTask::MergeProjectionsStage\")};\n     };\n \n     using MergeProjectionsRuntimeContextPtr = std::shared_ptr<MergeProjectionsRuntimeContext>;\ndiff --git a/src/Storages/MergeTree/MergeTreeBackgroundExecutor.cpp b/src/Storages/MergeTree/MergeTreeBackgroundExecutor.cpp\nindex a3f8e02f5eb1..8cb0badc19bf 100644\n--- a/src/Storages/MergeTree/MergeTreeBackgroundExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeBackgroundExecutor.cpp\n@@ -144,7 +144,7 @@ bool MergeTreeBackgroundExecutor<Queue>::trySchedule(ExecutableTaskPtr task)\n     return true;\n }\n \n-void printExceptionWithRespectToAbort(Poco::Logger * log, const String & query_id)\n+void printExceptionWithRespectToAbort(LoggerPtr log, const String & query_id)\n {\n     std::exception_ptr ex = std::current_exception();\n \ndiff --git a/src/Storages/MergeTree/MergeTreeBackgroundExecutor.h b/src/Storages/MergeTree/MergeTreeBackgroundExecutor.h\nindex 63f75ffc8d93..0ed032935894 100644\n--- a/src/Storages/MergeTree/MergeTreeBackgroundExecutor.h\n+++ b/src/Storages/MergeTree/MergeTreeBackgroundExecutor.h\n@@ -307,7 +307,7 @@ class MergeTreeBackgroundExecutor final : boost::noncopyable\n     std::condition_variable has_tasks TSA_GUARDED_BY(mutex);\n     bool shutdown TSA_GUARDED_BY(mutex) = false;\n     std::unique_ptr<ThreadPool> pool;\n-    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeBackgroundExecutor\");\n+    LoggerPtr log = getLogger(\"MergeTreeBackgroundExecutor\");\n };\n \n extern template class MergeTreeBackgroundExecutor<RoundRobinRuntimeQueue>;\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 61332a4ff384..39c113c240eb 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -355,7 +355,7 @@ MergeTreeData::MergeTreeData(\n     , require_part_metadata(require_part_metadata_)\n     , broken_part_callback(broken_part_callback_)\n     , log_name(std::make_shared<String>(table_id_.getNameForLogs()))\n-    , log(&Poco::Logger::get(*log_name))\n+    , log(getLogger(*log_name))\n     , storage_settings(std::move(storage_settings_))\n     , pinned_part_uuids(std::make_shared<PinnedPartUUIDs>())\n     , data_parts_by_info(data_parts_indexes.get<TagByInfo>())\n@@ -1222,7 +1222,7 @@ MergeTreeData::PartLoadingTree::build(PartLoadingInfos nodes)\n }\n \n static std::optional<size_t> calculatePartSizeSafe(\n-    const MergeTreeData::DataPartPtr & part, Poco::Logger * log)\n+    const MergeTreeData::DataPartPtr & part, const LoggerPtr & log)\n {\n     try\n     {\n@@ -2735,7 +2735,7 @@ void MergeTreeData::renameInMemory(const StorageID & new_table_id)\n {\n     IStorage::renameInMemory(new_table_id);\n     std::atomic_store(&log_name, std::make_shared<String>(new_table_id.getNameForLogs()));\n-    log = &Poco::Logger::get(*log_name);\n+    log = getLogger(*log_name);\n }\n \n void MergeTreeData::dropAllData()\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex f0dbaf0e307a..caef247500a8 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -5,6 +5,7 @@\n #include <Common/SimpleIncrement.h>\n #include <Common/SharedMutex.h>\n #include <Common/MultiVersion.h>\n+#include <Common/Logger.h>\n #include <Storages/IStorage.h>\n #include <IO/ReadBufferFromString.h>\n #include <IO/WriteBufferFromFile.h>\n@@ -1117,7 +1118,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// log_name will change during table RENAME. Use atomic_shared_ptr to allow concurrent RW.\n     /// NOTE clang-14 doesn't have atomic_shared_ptr yet. Use std::atomic* operations for now.\n     std::shared_ptr<String> log_name;\n-    std::atomic<Poco::Logger *> log;\n+    LoggerPtr log;\n \n     /// Storage settings.\n     /// Use get and set to receive readonly versions.\n@@ -1601,10 +1602,10 @@ struct CurrentlySubmergingEmergingTagger\n     MergeTreeData & storage;\n     String emerging_part_name;\n     MergeTreeData::DataPartsVector submerging_parts;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     CurrentlySubmergingEmergingTagger(\n-        MergeTreeData & storage_, const String & name_, MergeTreeData::DataPartsVector && parts_, Poco::Logger * log_)\n+        MergeTreeData & storage_, const String & name_, MergeTreeData::DataPartsVector && parts_, LoggerPtr log_)\n         : storage(storage_), emerging_part_name(name_), submerging_parts(std::move(parts_)), log(log_)\n     {\n     }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex 8c03aef6f99f..58fddde7b545 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -66,7 +66,7 @@ static const double DISK_USAGE_COEFFICIENT_TO_SELECT = 2;\n static const double DISK_USAGE_COEFFICIENT_TO_RESERVE = 1.1;\n \n MergeTreeDataMergerMutator::MergeTreeDataMergerMutator(MergeTreeData & data_)\n-    : data(data_), log(&Poco::Logger::get(data.getLogName() + \" (MergerMutator)\"))\n+    : data(data_), log(getLogger(data.getLogName() + \" (MergerMutator)\"))\n {\n }\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\nindex 6eab0ee0c371..f3a3f51b6c3a 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\n@@ -213,7 +213,7 @@ public :\n private:\n     MergeTreeData & data;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// When the last time you wrote to the log that the disk space was running out (not to write about this too often).\n     time_t disk_space_warning_time = 0;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex 66f593bbf331..a76d370d057f 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -74,7 +74,7 @@ namespace ErrorCodes\n \n \n MergeTreeDataSelectExecutor::MergeTreeDataSelectExecutor(const MergeTreeData & data_)\n-    : data(data_), log(&Poco::Logger::get(data.getLogName() + \" (SelectExecutor)\"))\n+    : data(data_), log(getLogger(data.getLogName() + \" (SelectExecutor)\"))\n {\n }\n \n@@ -83,7 +83,7 @@ size_t MergeTreeDataSelectExecutor::getApproximateTotalRowsToRead(\n     const StorageMetadataPtr & metadata_snapshot,\n     const KeyCondition & key_condition,\n     const Settings & settings,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     size_t rows_count = 0;\n \n@@ -167,7 +167,7 @@ MergeTreeDataSelectSamplingData MergeTreeDataSelectExecutor::getSampling(\n     const StorageMetadataPtr & metadata_snapshot,\n     ContextPtr context,\n     bool sample_factor_column_queried,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     const Settings & settings = context->getSettingsRef();\n     /// Sampling.\n@@ -503,7 +503,7 @@ void MergeTreeDataSelectExecutor::filterPartsByPartition(\n     const MergeTreeData & data,\n     const ContextPtr & context,\n     const PartitionIdToMaxBlock * max_block_numbers_to_read,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     ReadFromMergeTree::IndexStats & index_stats)\n {\n     chassert(alter_conversions.empty() || parts.size() == alter_conversions.size());\n@@ -590,7 +590,7 @@ RangesInDataParts MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipInd\n     const std::optional<KeyCondition> & part_offset_condition,\n     const UsefulSkipIndexes & skip_indexes,\n     const MergeTreeReaderSettings & reader_settings,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     size_t num_streams,\n     ReadFromMergeTree::IndexStats & index_stats,\n     bool use_skip_indexes)\n@@ -1082,7 +1082,7 @@ MarkRanges MergeTreeDataSelectExecutor::markRangesFromPKRange(\n     const KeyCondition & key_condition,\n     const std::optional<KeyCondition> & part_offset_condition,\n     const Settings & settings,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     MarkRanges res;\n \n@@ -1322,7 +1322,7 @@ MarkRanges MergeTreeDataSelectExecutor::filterMarksUsingIndex(\n     const MergeTreeReaderSettings & reader_settings,\n     MarkCache * mark_cache,\n     UncompressedCache * uncompressed_cache,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     if (!index_helper->getDeserializedFormat(part->getDataPartStorage(), index_helper->getFileName()))\n     {\n@@ -1440,7 +1440,7 @@ MarkRanges MergeTreeDataSelectExecutor::filterMarksUsingMergedIndex(\n     const MergeTreeReaderSettings & reader_settings,\n     MarkCache * mark_cache,\n     UncompressedCache * uncompressed_cache,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     for (const auto & index_helper : indices)\n     {\n@@ -1596,7 +1596,7 @@ void MergeTreeDataSelectExecutor::selectPartsToReadWithUUIDFilter(\n     const PartitionIdToMaxBlock * max_block_numbers_to_read,\n     ContextPtr query_context,\n     PartFilterCounters & counters,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     /// process_parts prepare parts that have to be read for the query,\n     /// returns false if duplicated parts' UUID have been met\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\nindex ba1f20054f08..17975354187e 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n@@ -71,11 +71,11 @@ class MergeTreeDataSelectExecutor\n         const KeyCondition & key_condition,\n         const std::optional<KeyCondition> & part_offset_condition,\n         const Settings & settings,\n-        Poco::Logger * log);\n+        LoggerPtr log);\n \n private:\n     const MergeTreeData & data;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Get the approximate value (bottom estimate - only by full marks) of the number of rows falling under the index.\n     static size_t getApproximateTotalRowsToRead(\n@@ -83,7 +83,7 @@ class MergeTreeDataSelectExecutor\n         const StorageMetadataPtr & metadata_snapshot,\n         const KeyCondition & key_condition,\n         const Settings & settings,\n-        Poco::Logger * log);\n+        LoggerPtr log);\n \n     static MarkRanges filterMarksUsingIndex(\n         MergeTreeIndexPtr index_helper,\n@@ -94,7 +94,7 @@ class MergeTreeDataSelectExecutor\n         const MergeTreeReaderSettings & reader_settings,\n         MarkCache * mark_cache,\n         UncompressedCache * uncompressed_cache,\n-        Poco::Logger * log);\n+        LoggerPtr log);\n \n     static MarkRanges filterMarksUsingMergedIndex(\n         MergeTreeIndices indices,\n@@ -105,7 +105,7 @@ class MergeTreeDataSelectExecutor\n         const MergeTreeReaderSettings & reader_settings,\n         MarkCache * mark_cache,\n         UncompressedCache * uncompressed_cache,\n-        Poco::Logger * log);\n+        LoggerPtr log);\n \n     struct PartFilterCounters\n     {\n@@ -141,7 +141,7 @@ class MergeTreeDataSelectExecutor\n         const PartitionIdToMaxBlock * max_block_numbers_to_read,\n         ContextPtr query_context,\n         PartFilterCounters & counters,\n-        Poco::Logger * log);\n+        LoggerPtr log);\n \n public:\n     /// For given number rows and bytes, get the number of marks to read.\n@@ -184,7 +184,7 @@ class MergeTreeDataSelectExecutor\n         const MergeTreeData & data,\n         const ContextPtr & context,\n         const PartitionIdToMaxBlock * max_block_numbers_to_read,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         ReadFromMergeTree::IndexStats & index_stats);\n \n     /// Filter parts using primary key and secondary indexes.\n@@ -199,7 +199,7 @@ class MergeTreeDataSelectExecutor\n         const std::optional<KeyCondition> & part_offset_condition,\n         const UsefulSkipIndexes & skip_indexes,\n         const MergeTreeReaderSettings & reader_settings,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         size_t num_streams,\n         ReadFromMergeTree::IndexStats & index_stats,\n         bool use_skip_indexes);\n@@ -216,7 +216,7 @@ class MergeTreeDataSelectExecutor\n         const StorageMetadataPtr & metadata_snapshot,\n         ContextPtr context,\n         bool sample_factor_column_queried,\n-        Poco::Logger * log);\n+        LoggerPtr log);\n \n     /// Check query limits: max_partitions_to_read, max_concurrent_queries.\n     /// Also, return QueryIdHolder. If not null, we should keep it until query finishes.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 3c0b2d2b42e9..ce3015c5dcb7 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -115,7 +115,7 @@ void buildScatterSelector(\n     if (max_parts && partitions_count >= max_parts && !throw_on_limit)\n     {\n         const auto & client_info = context->getClientInfo();\n-        Poco::Logger * log = &Poco::Logger::get(\"MergeTreeDataWriter\");\n+        LoggerPtr log = getLogger(\"MergeTreeDataWriter\");\n \n         LOG_WARNING(log, \"INSERT query from initial_user {} (query ID: {}) inserted a block \"\n                          \"that created parts in {} partitions. This is being logged \"\n@@ -335,7 +335,7 @@ Block MergeTreeDataWriter::mergeBlock(\n             case MergeTreeData::MergingParams::Collapsing:\n                 return std::make_shared<CollapsingSortedAlgorithm>(\n                     block, 1, sort_description, merging_params.sign_column,\n-                    false, block_size + 1, /*block_size_bytes=*/0, &Poco::Logger::get(\"MergeTreeDataWriter\"));\n+                    false, block_size + 1, /*block_size_bytes=*/0, getLogger(\"MergeTreeDataWriter\"));\n             case MergeTreeData::MergingParams::Summing:\n                 return std::make_shared<SummingSortedAlgorithm>(\n                     block, 1, sort_description, merging_params.columns_to_sum,\n@@ -618,7 +618,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n     bool is_temp,\n     IMergeTreeDataPart * parent_part,\n     const MergeTreeData & data,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     Block block,\n     const ProjectionDescription & projection)\n {\n@@ -729,7 +729,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n \n MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPart(\n     const MergeTreeData & data,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     Block block,\n     const ProjectionDescription & projection,\n     IMergeTreeDataPart * parent_part)\n@@ -748,7 +748,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPart(\n /// projection part merges.\n MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempProjectionPart(\n     const MergeTreeData & data,\n-    Poco::Logger * log,\n+    LoggerPtr log,\n     Block block,\n     const ProjectionDescription & projection,\n     IMergeTreeDataPart * parent_part,\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.h b/src/Storages/MergeTree/MergeTreeDataWriter.h\nindex 2fb6b1f22d43..8fb8b82dbe6a 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.h\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.h\n@@ -45,8 +45,9 @@ class MergeTreeDataWriter\n public:\n     explicit MergeTreeDataWriter(MergeTreeData & data_)\n         : data(data_)\n-        , log(&Poco::Logger::get(data.getLogName() + \" (Writer)\"))\n-    {}\n+        , log(getLogger(data.getLogName() + \" (Writer)\"))\n+    {\n+    }\n \n     /** Split the block to blocks, each of them must be written as separate part.\n       *  (split rows by partition)\n@@ -91,7 +92,7 @@ class MergeTreeDataWriter\n     /// For insertion.\n     static TemporaryPart writeProjectionPart(\n         const MergeTreeData & data,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         Block block,\n         const ProjectionDescription & projection,\n         IMergeTreeDataPart * parent_part);\n@@ -99,7 +100,7 @@ class MergeTreeDataWriter\n     /// For mutation: MATERIALIZE PROJECTION.\n     static TemporaryPart writeTempProjectionPart(\n         const MergeTreeData & data,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         Block block,\n         const ProjectionDescription & projection,\n         IMergeTreeDataPart * parent_part,\n@@ -126,12 +127,12 @@ class MergeTreeDataWriter\n         bool is_temp,\n         IMergeTreeDataPart * parent_part,\n         const MergeTreeData & data,\n-        Poco::Logger * log,\n+        LoggerPtr log,\n         Block block,\n         const ProjectionDescription & projection);\n \n     MergeTreeData & data;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreePartsMover.h b/src/Storages/MergeTree/MergeTreePartsMover.h\nindex b9109e51309c..43d8ebdd6d34 100644\n--- a/src/Storages/MergeTree/MergeTreePartsMover.h\n+++ b/src/Storages/MergeTree/MergeTreePartsMover.h\n@@ -48,7 +48,7 @@ class MergeTreePartsMover\n \n     explicit MergeTreePartsMover(MergeTreeData * data_)\n         : data(data_)\n-        , log(&Poco::Logger::get(\"MergeTreePartsMover\"))\n+        , log(getLogger(\"MergeTreePartsMover\"))\n     {\n     }\n \n@@ -81,7 +81,7 @@ class MergeTreePartsMover\n private:\n \n     MergeTreeData * data;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n \ndiff --git a/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp b/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp\nindex 3f9632637b66..47c2fe07bb47 100644\n--- a/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp\n+++ b/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp\n@@ -128,7 +128,7 @@ MergeTreePrefetchedReadPool::MergeTreePrefetchedReadPool(\n         context_)\n     , WithContext(context_)\n     , prefetch_threadpool(getContext()->getPrefetchThreadpool())\n-    , log(&Poco::Logger::get(\"MergeTreePrefetchedReadPool(\" + (parts_ranges.empty() ? \"\" : parts_ranges.front().data_part->storage.getStorageID().getNameForLogs()) + \")\"))\n+    , log(getLogger(\"MergeTreePrefetchedReadPool(\" + (parts_ranges.empty() ? \"\" : parts_ranges.front().data_part->storage.getStorageID().getNameForLogs()) + \")\"))\n {\n     /// Tasks creation might also create a lost of readers - check they do not\n     /// do any time consuming operations in ctor.\ndiff --git a/src/Storages/MergeTree/MergeTreePrefetchedReadPool.h b/src/Storages/MergeTree/MergeTreePrefetchedReadPool.h\nindex 9925d4e2fa4f..378034c5eae5 100644\n--- a/src/Storages/MergeTree/MergeTreePrefetchedReadPool.h\n+++ b/src/Storages/MergeTree/MergeTreePrefetchedReadPool.h\n@@ -122,7 +122,7 @@ class MergeTreePrefetchedReadPool : public MergeTreeReadPoolBase, private WithCo\n     TasksPerThread per_thread_tasks;\n     std::priority_queue<TaskHolder> prefetch_queue; /// the smallest on top\n     bool started_prefetches = false;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// A struct which allows to track max number of tasks which were in the\n     /// threadpool simultaneously (similar to CurrentMetrics, but the result\ndiff --git a/src/Storages/MergeTree/MergeTreeRangeReader.h b/src/Storages/MergeTree/MergeTreeRangeReader.h\nindex 04d421389634..79ed18f4d1f4 100644\n--- a/src/Storages/MergeTree/MergeTreeRangeReader.h\n+++ b/src/Storages/MergeTree/MergeTreeRangeReader.h\n@@ -231,7 +231,7 @@ class MergeTreeRangeReader\n \n         using RangesInfo = std::vector<RangeInfo>;\n \n-        explicit ReadResult(Poco::Logger * log_) : log(log_) {}\n+        explicit ReadResult(LoggerPtr log_) : log(log_) {}\n \n         static size_t getLastMark(const MergeTreeRangeReader::ReadResult::RangesInfo & ranges);\n \n@@ -298,7 +298,7 @@ class MergeTreeRangeReader\n         size_t countZeroTails(const IColumn::Filter & filter, NumRows & zero_tails, bool can_read_incomplete_granules) const;\n         static size_t numZerosInTail(const UInt8 * begin, const UInt8 * end);\n \n-        Poco::Logger * log;\n+        LoggerPtr log;\n     };\n \n     ReadResult read(size_t max_rows, MarkRanges & ranges);\n@@ -325,7 +325,7 @@ class MergeTreeRangeReader\n     bool is_initialized = false;\n     Names non_const_virtual_column_names;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeRangeReader\");\n+    LoggerPtr log = getLogger(\"MergeTreeRangeReader\");\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeReadPool.h b/src/Storages/MergeTree/MergeTreeReadPool.h\nindex 3a1af947cae1..e45ccad912f1 100644\n--- a/src/Storages/MergeTree/MergeTreeReadPool.h\n+++ b/src/Storages/MergeTree/MergeTreeReadPool.h\n@@ -108,7 +108,7 @@ class MergeTreeReadPool : public MergeTreeReadPoolBase\n     std::vector<ThreadTask> threads_tasks;\n     std::set<size_t> remaining_thread_tasks;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeReadPool\");\n+    LoggerPtr log = getLogger(\"MergeTreeReadPool\");\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeReadPoolParallelReplicas.h b/src/Storages/MergeTree/MergeTreeReadPoolParallelReplicas.h\nindex 7579a892b67c..6a548dffe374 100644\n--- a/src/Storages/MergeTree/MergeTreeReadPoolParallelReplicas.h\n+++ b/src/Storages/MergeTree/MergeTreeReadPoolParallelReplicas.h\n@@ -34,7 +34,7 @@ class MergeTreeReadPoolParallelReplicas : public MergeTreeReadPoolBase\n     const CoordinationMode coordination_mode;\n     RangesInDataPartsDescription buffered_ranges;\n     bool no_more_tasks_available{false};\n-    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeReadPoolParallelReplicas\");\n+    LoggerPtr log = getLogger(\"MergeTreeReadPoolParallelReplicas\");\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.h b/src/Storages/MergeTree/MergeTreeSelectProcessor.h\nindex cf1a6313b514..b06ae788e91d 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.h\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.h\n@@ -114,7 +114,7 @@ class MergeTreeSelectProcessor : private boost::noncopyable\n     /// Should we add part level to produced chunk. Part level is useful for next steps if query has FINAL\n     bool add_part_level = false;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeSelectProcessor\");\n+    LoggerPtr log = getLogger(\"MergeTreeSelectProcessor\");\n     std::atomic<bool> is_cancelled{false};\n };\n \ndiff --git a/src/Storages/MergeTree/MergeTreeSequentialSource.cpp b/src/Storages/MergeTree/MergeTreeSequentialSource.cpp\nindex 82e9f8fd2db8..29af7fb4820f 100644\n--- a/src/Storages/MergeTree/MergeTreeSequentialSource.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSequentialSource.cpp\n@@ -68,7 +68,7 @@ class MergeTreeSequentialSource : public ISource\n     /// Should read using direct IO\n     bool read_with_direct_io;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeSequentialSource\");\n+    LoggerPtr log = getLogger(\"MergeTreeSequentialSource\");\n \n     std::optional<MarkRanges> mark_ranges;\n \n@@ -318,7 +318,7 @@ class ReadFromPart final : public ISourceStep\n         bool apply_deleted_mask_,\n         ActionsDAGPtr filter_,\n         ContextPtr context_,\n-        Poco::Logger * log_)\n+        LoggerPtr log_)\n         : ISourceStep(DataStream{.header = storage_snapshot_->getSampleBlockForColumns(columns_to_read_)})\n         , type(type_)\n         , storage(storage_)\n@@ -381,7 +381,7 @@ class ReadFromPart final : public ISourceStep\n     bool apply_deleted_mask;\n     ActionsDAGPtr filter;\n     ContextPtr context;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n void createReadFromPartStep(\n@@ -394,7 +394,7 @@ void createReadFromPartStep(\n     bool apply_deleted_mask,\n     ActionsDAGPtr filter,\n     ContextPtr context,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     auto reading = std::make_unique<ReadFromPart>(type,\n         storage, storage_snapshot, std::move(data_part),\ndiff --git a/src/Storages/MergeTree/MergeTreeSequentialSource.h b/src/Storages/MergeTree/MergeTreeSequentialSource.h\nindex 41def48aab6a..a5e36a7726ff 100644\n--- a/src/Storages/MergeTree/MergeTreeSequentialSource.h\n+++ b/src/Storages/MergeTree/MergeTreeSequentialSource.h\n@@ -41,6 +41,6 @@ void createReadFromPartStep(\n     bool apply_deleted_mask,\n     ActionsDAGPtr filter,\n     ContextPtr context,\n-    Poco::Logger * log);\n+    LoggerPtr log);\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.cpp b/src/Storages/MergeTree/MergeTreeSettings.cpp\nindex 153930b400d3..b42da22239eb 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSettings.cpp\n@@ -65,7 +65,7 @@ void MergeTreeSettings::loadFromQuery(ASTStorage & storage_def, ContextPtr conte\n                         if (ast && isDiskFunction(ast))\n                         {\n                             auto disk_name = getOrCreateDiskFromDiskAST(ast, context, is_attach);\n-                            LOG_TRACE(&Poco::Logger::get(\"MergeTreeSettings\"), \"Created custom disk {}\", disk_name);\n+                            LOG_TRACE(getLogger(\"MergeTreeSettings\"), \"Created custom disk {}\", disk_name);\n                             value = disk_name;\n                         }\n                     }\ndiff --git a/src/Storages/MergeTree/MergeTreeWhereOptimizer.cpp b/src/Storages/MergeTree/MergeTreeWhereOptimizer.cpp\nindex 0cac051bb2cf..3f3dadb3cc51 100644\n--- a/src/Storages/MergeTree/MergeTreeWhereOptimizer.cpp\n+++ b/src/Storages/MergeTree/MergeTreeWhereOptimizer.cpp\n@@ -56,7 +56,7 @@ MergeTreeWhereOptimizer::MergeTreeWhereOptimizer(\n     const ConditionEstimator & estimator_,\n     const Names & queried_columns_,\n     const std::optional<NameSet> & supported_columns_,\n-    Poco::Logger * log_)\n+    LoggerPtr log_)\n     : estimator(estimator_)\n     , table_columns{collections::map<std::unordered_set>(\n         metadata_snapshot->getColumns().getAllPhysical(), [](const NameAndTypePair & col) { return col.name; })}\ndiff --git a/src/Storages/MergeTree/MergeTreeWhereOptimizer.h b/src/Storages/MergeTree/MergeTreeWhereOptimizer.h\nindex 0ef7ac9efff3..7a6651210d02 100644\n--- a/src/Storages/MergeTree/MergeTreeWhereOptimizer.h\n+++ b/src/Storages/MergeTree/MergeTreeWhereOptimizer.h\n@@ -41,7 +41,7 @@ class MergeTreeWhereOptimizer : private boost::noncopyable\n         const ConditionEstimator & estimator_,\n         const Names & queried_columns_,\n         const std::optional<NameSet> & supported_columns_,\n-        Poco::Logger * log_);\n+        LoggerPtr log_);\n \n     void optimize(SelectQueryInfo & select_query_info, const ContextPtr & context) const;\n \n@@ -156,7 +156,7 @@ class MergeTreeWhereOptimizer : private boost::noncopyable\n     const std::optional<NameSet> supported_columns;\n     const NameSet sorting_key_names;\n     const NameToIndexMap primary_key_names_positions;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::unordered_map<std::string, UInt64> column_sizes;\n     UInt64 total_size_of_queried_columns = 0;\n };\ndiff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\nindex a8b3df483ed5..2236c1a93805 100644\n--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n@@ -36,7 +36,7 @@ MergeTreeWriteAheadLog::MergeTreeWriteAheadLog(\n     , name(name_)\n     , path(storage.getRelativeDataPath() + name_)\n     , pool(storage.getContext()->getSchedulePool())\n-    , log(&Poco::Logger::get(storage.getLogName() + \" (WriteAheadLog)\"))\n+    , log(getLogger(storage.getLogName() + \" (WriteAheadLog)\"))\n {\n     init();\n     sync_task = pool.createTask(\"MergeTreeWriteAheadLog::sync\", [this]\ndiff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.h b/src/Storages/MergeTree/MergeTreeWriteAheadLog.h\nindex 5fb9dd907a17..9550fa6ecee5 100644\n--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.h\n+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.h\n@@ -99,7 +99,7 @@ class MergeTreeWriteAheadLog\n \n     mutable std::mutex write_mutex;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\nindex 8b34c221eec6..1d10a1433efd 100644\n--- a/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n@@ -155,7 +155,7 @@ MergedBlockOutputStream::Finalizer MergedBlockOutputStream::finalizePartAsync(\n     for (const auto & name : checksums_to_remove)\n         checksums.files.erase(name);\n \n-    LOG_TRACE(&Poco::Logger::get(\"MergedBlockOutputStream\"), \"filled checksums {}\", new_part->getNameWithState());\n+    LOG_TRACE(getLogger(\"MergedBlockOutputStream\"), \"filled checksums {}\", new_part->getNameWithState());\n \n     for (const auto & [projection_name, projection_part] : new_part->getProjectionParts())\n         checksums.addFile(\ndiff --git a/src/Storages/MergeTree/MutateFromLogEntryTask.h b/src/Storages/MergeTree/MutateFromLogEntryTask.h\nindex 42d8307e948e..68c7f4642148 100644\n--- a/src/Storages/MergeTree/MutateFromLogEntryTask.h\n+++ b/src/Storages/MergeTree/MutateFromLogEntryTask.h\n@@ -23,7 +23,7 @@ class MutateFromLogEntryTask : public ReplicatedMergeMutateTaskBase\n         StorageReplicatedMergeTree & storage_,\n         Callback && task_result_callback_)\n         : ReplicatedMergeMutateTaskBase(\n-            &Poco::Logger::get(storage_.getStorageID().getShortName() + \"::\" + selected_entry_->log_entry->new_part_name + \" (MutateFromLogEntryTask)\"),\n+            getLogger(storage_.getStorageID().getShortName() + \"::\" + selected_entry_->log_entry->new_part_name + \" (MutateFromLogEntryTask)\"),\n             storage_,\n             selected_entry_,\n             task_result_callback_)\ndiff --git a/src/Storages/MergeTree/MutatePlainMergeTreeTask.cpp b/src/Storages/MergeTree/MutatePlainMergeTreeTask.cpp\nindex bf8e879e3d07..0b19aebe36d6 100644\n--- a/src/Storages/MergeTree/MutatePlainMergeTreeTask.cpp\n+++ b/src/Storages/MergeTree/MutatePlainMergeTreeTask.cpp\n@@ -111,7 +111,7 @@ bool MutatePlainMergeTreeTask::executeStep()\n                 if (merge_mutate_entry->txn)\n                     merge_mutate_entry->txn->onException();\n                 PreformattedMessage exception_message = getCurrentExceptionMessageAndPattern(/* with_stacktrace */ false);\n-                LOG_ERROR(&Poco::Logger::get(\"MutatePlainMergeTreeTask\"), exception_message);\n+                LOG_ERROR(getLogger(\"MutatePlainMergeTreeTask\"), exception_message);\n                 storage.updateMutationEntriesErrors(future_part, false, exception_message.text);\n                 write_part_log(ExecutionStatus::fromCurrentException(\"\", true));\n                 tryLogCurrentException(__PRETTY_FUNCTION__);\ndiff --git a/src/Storages/MergeTree/MutateTask.cpp b/src/Storages/MergeTree/MutateTask.cpp\nindex e4070aa82626..6bcdfe34296b 100644\n--- a/src/Storages/MergeTree/MutateTask.cpp\n+++ b/src/Storages/MergeTree/MutateTask.cpp\n@@ -61,7 +61,7 @@ static void splitAndModifyMutationCommands(\n     const MutationCommands & commands,\n     MutationCommands & for_interpreter,\n     MutationCommands & for_file_renames,\n-    Poco::Logger * log)\n+    LoggerPtr log)\n {\n     auto part_columns = part->getColumnsDescription();\n \n@@ -896,7 +896,7 @@ struct MutationContext\n     TableLockHolder * holder;\n     MergeListEntry * mutate_entry;\n \n-    Poco::Logger * log{&Poco::Logger::get(\"MutateTask\")};\n+    LoggerPtr log{getLogger(\"MutateTask\")};\n \n     FutureMergedMutatedPartPtr future_part;\n     MergeTreeData::DataPartPtr source_part;\n@@ -975,7 +975,7 @@ class MergeProjectionPartsTask : public IExecutableTask\n         , projection(projection_)\n         , block_num(block_num_)\n         , ctx(ctx_)\n-        , log(&Poco::Logger::get(\"MergeProjectionPartsTask\"))\n+        , log(getLogger(\"MergeProjectionPartsTask\"))\n         {\n             LOG_DEBUG(log, \"Selected {} projection_parts from {} to {}\", parts.size(), parts.front()->name, parts.back()->name);\n             level_parts[current_level] = std::move(parts);\n@@ -1079,7 +1079,7 @@ class MergeProjectionPartsTask : public IExecutableTask\n     size_t & block_num;\n     MutationContextPtr ctx;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::map<size_t, MergeTreeData::MutableDataPartsVector> level_parts;\n     size_t current_level = 0;\ndiff --git a/src/Storages/MergeTree/ParallelReplicasReadingCoordinator.cpp b/src/Storages/MergeTree/ParallelReplicasReadingCoordinator.cpp\nindex 484a0b37644e..abc51bde3fb8 100644\n--- a/src/Storages/MergeTree/ParallelReplicasReadingCoordinator.cpp\n+++ b/src/Storages/MergeTree/ParallelReplicasReadingCoordinator.cpp\n@@ -245,7 +245,7 @@ class DefaultCoordinator : public ParallelReplicasReadingCoordinator::ImplInterf\n     };\n     std::vector<ReplicaStatus> replica_status;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"DefaultCoordinator\");\n+    LoggerPtr log = getLogger(\"DefaultCoordinator\");\n \n     /// Workflow of a segment:\n     /// 0. `all_parts_to_read` contains all the parts and thus all the segments initially present there (virtually)\n@@ -835,7 +835,7 @@ class InOrderCoordinator : public ParallelReplicasReadingCoordinator::ImplInterf\n     Parts all_parts_to_read;\n     size_t total_rows_to_read = 0;\n \n-    Poco::Logger * log = &Poco::Logger::get(fmt::format(\"{}{}\", magic_enum::enum_name(mode), \"Coordinator\"));\n+    LoggerPtr log = getLogger(fmt::format(\"{}{}\", magic_enum::enum_name(mode), \"Coordinator\"));\n };\n \n template <CoordinationMode mode>\ndiff --git a/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.cpp b/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.cpp\nindex 76b8080f64cd..78fcfabb7044 100644\n--- a/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.cpp\n+++ b/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.cpp\n@@ -20,7 +20,7 @@ PartMovesBetweenShardsOrchestrator::PartMovesBetweenShardsOrchestrator(StorageRe\n     : storage(storage_)\n     , zookeeper_path(storage.zookeeper_path)\n     , logger_name(storage.getStorageID().getFullTableName() + \" (PartMovesBetweenShardsOrchestrator)\")\n-    , log(&Poco::Logger::get(logger_name))\n+    , log(getLogger(logger_name))\n     , entries_znode_path(zookeeper_path + \"/part_moves_shard\")\n {\n     /// Schedule pool is not designed for long-running tasks. TODO replace with a separate thread?\ndiff --git a/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.h b/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.h\nindex af21022953c5..abe259c77ab5 100644\n--- a/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.h\n+++ b/src/Storages/MergeTree/PartMovesBetweenShardsOrchestrator.h\n@@ -176,7 +176,7 @@ class PartMovesBetweenShardsOrchestrator\n \n     String zookeeper_path;\n     String logger_name;\n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n     std::atomic<bool> need_stop{false};\n \n     BackgroundSchedulePool::TaskHolder task;\ndiff --git a/src/Storages/MergeTree/PartitionPruner.cpp b/src/Storages/MergeTree/PartitionPruner.cpp\nindex 668576f90211..eb51d600da3b 100644\n--- a/src/Storages/MergeTree/PartitionPruner.cpp\n+++ b/src/Storages/MergeTree/PartitionPruner.cpp\n@@ -59,7 +59,7 @@ bool PartitionPruner::canBePruned(const IMergeTreeDataPart & part) const\n         {\n             WriteBufferFromOwnString buf;\n             part.partition.serializeText(part.storage, buf, FormatSettings{});\n-            LOG_TRACE(&Poco::Logger::get(\"PartitionPruner\"), \"Partition {} gets pruned\", buf.str());\n+            LOG_TRACE(getLogger(\"PartitionPruner\"), \"Partition {} gets pruned\", buf.str());\n         }\n     }\n \ndiff --git a/src/Storages/MergeTree/ReplicatedMergeMutateTaskBase.h b/src/Storages/MergeTree/ReplicatedMergeMutateTaskBase.h\nindex 18fcacecc9e3..2b1fcec62a83 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeMutateTaskBase.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeMutateTaskBase.h\n@@ -17,7 +17,7 @@ class ReplicatedMergeMutateTaskBase : public IExecutableTask\n {\n public:\n     ReplicatedMergeMutateTaskBase(\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         StorageReplicatedMergeTree & storage_,\n         ReplicatedMergeTreeQueue::SelectedEntryPtr & selected_entry_,\n         IExecutableTask::TaskResultCallback & task_result_callback_)\n@@ -66,7 +66,7 @@ class ReplicatedMergeMutateTaskBase : public IExecutableTask\n     ReplicatedMergeTreeQueue::SelectedEntryPtr selected_entry;\n     ReplicatedMergeTreeLogEntry & entry;\n     MergeList::EntryPtr merge_mutate_entry{nullptr};\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     /// ProfileEvents for current part will be stored here\n     ProfileEvents::Counters profile_counters;\n     ContextMutablePtr task_context;\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\nindex a544ac908a4c..336d19692d43 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\n@@ -19,7 +19,7 @@ namespace ErrorCodes\n ReplicatedMergeTreeAttachThread::ReplicatedMergeTreeAttachThread(StorageReplicatedMergeTree & storage_)\n     : storage(storage_)\n     , log_name(storage.getStorageID().getFullTableName() + \" (ReplicatedMergeTreeAttachThread)\")\n-    , log(&Poco::Logger::get(log_name))\n+    , log(getLogger(log_name))\n {\n     task = storage.getContext()->getSchedulePool().createTask(log_name, [this] { run(); });\n     const auto storage_settings = storage.getSettings();\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h\nindex 222b30b519b1..250a5ed34d1c 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h\n@@ -34,7 +34,7 @@ class ReplicatedMergeTreeAttachThread\n     BackgroundSchedulePool::TaskHolder task;\n \n     std::string log_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::atomic<bool> first_try_done{false};\n \ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\nindex 8daee661c752..67942491ae25 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\n@@ -24,7 +24,7 @@ namespace ErrorCodes\n ReplicatedMergeTreeCleanupThread::ReplicatedMergeTreeCleanupThread(StorageReplicatedMergeTree & storage_)\n     : storage(storage_)\n     , log_name(storage.getStorageID().getFullTableName() + \" (ReplicatedMergeTreeCleanupThread)\")\n-    , log(&Poco::Logger::get(log_name))\n+    , log(getLogger(log_name))\n     , sleep_ms(storage.getSettings()->cleanup_delay_period * 1000)\n {\n     task = storage.getContext()->getSchedulePool().createTask(log_name, [this]{ run(); });\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\nindex ae9aabdb4e7f..5beaef569955 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\n@@ -40,7 +40,7 @@ class ReplicatedMergeTreeCleanupThread\n private:\n     StorageReplicatedMergeTree & storage;\n     String log_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     BackgroundSchedulePool::TaskHolder task;\n     pcg64 rng{randomSeed()};\n \ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp\nindex b1875464725b..156c41563ec0 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp\n@@ -28,7 +28,7 @@ static const auto PART_CHECK_ERROR_SLEEP_MS = 5 * 1000;\n ReplicatedMergeTreePartCheckThread::ReplicatedMergeTreePartCheckThread(StorageReplicatedMergeTree & storage_)\n     : storage(storage_)\n     , log_name(storage.getStorageID().getFullTableName() + \" (ReplicatedMergeTreePartCheckThread)\")\n-    , log(&Poco::Logger::get(log_name))\n+    , log(getLogger(log_name))\n {\n     task = storage.getContext()->getSchedulePool().createTask(log_name, [this] { run(); });\n     task->schedule();\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h\nindex 68dc6ca3d1de..f2e26b3d324f 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h\n@@ -87,7 +87,7 @@ class ReplicatedMergeTreePartCheckThread\n \n     StorageReplicatedMergeTree & storage;\n     String log_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     using StringSet = std::set<String>;\n     struct PartToCheck\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\nindex a3afa8cd88a9..8d921bdcb1c5 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n@@ -36,7 +36,7 @@ ReplicatedMergeTreeQueue::ReplicatedMergeTreeQueue(StorageReplicatedMergeTree &\n     zookeeper_path = storage.zookeeper_path;\n     replica_path = storage.replica_path;\n     logger_name = storage.getStorageID().getFullTableName() + \" (ReplicatedMergeTreeQueue)\";\n-    log = &Poco::Logger::get(logger_name);\n+    log = getLogger(logger_name);\n }\n \n \n@@ -2149,7 +2149,7 @@ LocalMergePredicate::LocalMergePredicate(ReplicatedMergeTreeQueue & queue_)\n \n template<typename VirtualPartsT, typename MutationsStateT>\n CommittingBlocks BaseMergePredicate<VirtualPartsT, MutationsStateT>::getCommittingBlocks(\n-    zkutil::ZooKeeperPtr & zookeeper, const std::string & zookeeper_path, Poco::Logger * log_)\n+    zkutil::ZooKeeperPtr & zookeeper, const std::string & zookeeper_path, LoggerPtr log_)\n {\n     CommittingBlocks committing_blocks;\n \ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\nindex 92201b11d37d..84106565dff0 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n@@ -71,7 +71,7 @@ class ReplicatedMergeTreeQueue\n     String zookeeper_path;\n     String replica_path;\n     String logger_name;\n-    Poco::Logger * log = nullptr;\n+    LoggerPtr log = nullptr;\n \n     /// Protects the queue, future_parts and other queue state variables.\n     mutable std::mutex state_mutex;\n@@ -519,7 +519,7 @@ class BaseMergePredicate\n     /// This predicate is checked for the first part of each range.\n     bool canMergeSinglePart(const MergeTreeData::DataPartPtr & part, String & out_reason) const;\n \n-    CommittingBlocks getCommittingBlocks(zkutil::ZooKeeperPtr & zookeeper, const std::string & zookeeper_path, Poco::Logger * log_);\n+    CommittingBlocks getCommittingBlocks(zkutil::ZooKeeperPtr & zookeeper, const std::string & zookeeper_path, LoggerPtr log_);\n \n protected:\n     /// A list of partitions that can be used in the merge predicate\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\nindex 579592b0b3e8..b79418da7916 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n@@ -33,7 +33,7 @@ static String generateActiveNodeIdentifier()\n ReplicatedMergeTreeRestartingThread::ReplicatedMergeTreeRestartingThread(StorageReplicatedMergeTree & storage_)\n     : storage(storage_)\n     , log_name(storage.getStorageID().getFullTableName() + \" (ReplicatedMergeTreeRestartingThread)\")\n-    , log(&Poco::Logger::get(log_name))\n+    , log(getLogger(log_name))\n     , active_node_identifier(generateActiveNodeIdentifier())\n {\n     const auto storage_settings = storage.getSettings();\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.h b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.h\nindex 02103272a1f5..01071d80e8bf 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.h\n@@ -42,7 +42,7 @@ class ReplicatedMergeTreeRestartingThread\n private:\n     StorageReplicatedMergeTree & storage;\n     String log_name;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::atomic<bool> need_stop {false};\n \n     /// The random data we wrote into `/replicas/me/is_active`.\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\nindex 73ad595ec20e..1fb2393948a6 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n@@ -58,7 +58,7 @@ struct ReplicatedMergeTreeSinkImpl<async_insert>::DelayedChunk\n         ProfileEvents::Counters part_counters;\n \n         Partition() = default;\n-        Partition(Poco::Logger * log_,\n+        Partition(LoggerPtr log_,\n                   MergeTreeDataWriter::TemporaryPart && temp_part_,\n                   UInt64 elapsed_ns_,\n                   BlockIDsType && block_id_,\n@@ -92,7 +92,7 @@ std::vector<Int64> testSelfDeduplicate(std::vector<Int64> data, std::vector<size\n     BlockWithPartition block1(std::move(block), Row(), std::move(offsets), std::move(tokens));\n     ProfileEvents::Counters profile_counters;\n     ReplicatedMergeTreeSinkImpl<true>::DelayedChunk::Partition part(\n-        &Poco::Logger::get(\"testSelfDeduplicate\"), MergeTreeDataWriter::TemporaryPart(), 0, std::move(hashes), std::move(block1), std::nullopt, std::move(profile_counters));\n+        getLogger(\"testSelfDeduplicate\"), MergeTreeDataWriter::TemporaryPart(), 0, std::move(hashes), std::move(block1), std::nullopt, std::move(profile_counters));\n \n     part.filterSelfDuplicate();\n \n@@ -138,7 +138,7 @@ ReplicatedMergeTreeSinkImpl<async_insert>::ReplicatedMergeTreeSinkImpl(\n     , is_attach(is_attach_)\n     , quorum_parallel(quorum_parallel_)\n     , deduplicate(deduplicate_)\n-    , log(&Poco::Logger::get(storage.getLogName() + \" (Replicated OutputStream)\"))\n+    , log(getLogger(storage.getLogName() + \" (Replicated OutputStream)\"))\n     , context(context_)\n     , storage_snapshot(storage.getStorageSnapshotWithoutData(metadata_snapshot, context_))\n {\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeSink.h b/src/Storages/MergeTree/ReplicatedMergeTreeSink.h\nindex 4811d93775b6..bc23204e7d39 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeSink.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeSink.h\n@@ -128,7 +128,7 @@ class ReplicatedMergeTreeSinkImpl : public SinkToStorage\n     bool last_block_is_duplicate = false;\n     UInt64 num_blocks_processed = 0;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     ContextPtr context;\n     StorageSnapshotPtr storage_snapshot;\ndiff --git a/src/Storages/MergeTree/ZooKeeperRetries.h b/src/Storages/MergeTree/ZooKeeperRetries.h\nindex 92faf80e61b4..ecef174c6c77 100644\n--- a/src/Storages/MergeTree/ZooKeeperRetries.h\n+++ b/src/Storages/MergeTree/ZooKeeperRetries.h\n@@ -30,7 +30,7 @@ struct ZooKeeperRetriesInfo\n class ZooKeeperRetriesControl\n {\n public:\n-    ZooKeeperRetriesControl(std::string name_, Poco::Logger * logger_, ZooKeeperRetriesInfo retries_info_, QueryStatusPtr elem)\n+    ZooKeeperRetriesControl(std::string name_, LoggerPtr logger_, ZooKeeperRetriesInfo retries_info_, QueryStatusPtr elem)\n         : name(std::move(name_)), logger(logger_), retries_info(retries_info_), process_list_element(elem)\n     {\n     }\n@@ -160,7 +160,7 @@ class ZooKeeperRetriesControl\n \n     const std::string & getName() const { return name; }\n \n-    Poco::Logger * getLogger() const { return logger; }\n+    LoggerPtr getLogger() const { return logger; }\n \n private:\n     struct KeeperError\n@@ -263,7 +263,7 @@ class ZooKeeperRetriesControl\n \n \n     std::string name;\n-    Poco::Logger * logger = nullptr;\n+    LoggerPtr logger = nullptr;\n     ZooKeeperRetriesInfo retries_info;\n     UInt64 total_failures = 0;\n     UserError user_error;\ndiff --git a/src/Storages/MergeTree/checkDataPart.cpp b/src/Storages/MergeTree/checkDataPart.cpp\nindex 8cf5b6a88945..8ae9b54b6e92 100644\n--- a/src/Storages/MergeTree/checkDataPart.cpp\n+++ b/src/Storages/MergeTree/checkDataPart.cpp\n@@ -338,7 +338,7 @@ IMergeTreeDataPart::Checksums checkDataPart(\n             throw;\n \n         LOG_DEBUG(\n-            &Poco::Logger::get(\"checkDataPart\"),\n+            getLogger(\"checkDataPart\"),\n             \"Will drop cache for data part {} and will check it once again\", data_part->name);\n \n         auto & cache = *FileCacheFactory::instance().getByName(*cache_name)->cache;\ndiff --git a/src/Storages/MessageQueueSink.cpp b/src/Storages/MessageQueueSink.cpp\nindex 1aa19c9ccde5..4fb81d690707 100644\n--- a/src/Storages/MessageQueueSink.cpp\n+++ b/src/Storages/MessageQueueSink.cpp\n@@ -20,7 +20,7 @@ MessageQueueSink::MessageQueueSink(\n void MessageQueueSink::onStart()\n {\n     LOG_TEST(\n-        &Poco::Logger::get(\"MessageQueueSink\"),\n+        getLogger(\"MessageQueueSink\"),\n         \"Executing startup for MessageQueueSink\");\n \n     initialize();\ndiff --git a/src/Storages/NATS/NATSConnection.cpp b/src/Storages/NATS/NATSConnection.cpp\nindex 70b3599aa090..d7ad0cf8219e 100644\n--- a/src/Storages/NATS/NATSConnection.cpp\n+++ b/src/Storages/NATS/NATSConnection.cpp\n@@ -13,7 +13,7 @@ static const auto RETRIES_MAX = 20;\n static const auto CONNECTED_TO_BUFFER_SIZE = 256;\n \n \n-NATSConnectionManager::NATSConnectionManager(const NATSConfiguration & configuration_, Poco::Logger * log_)\n+NATSConnectionManager::NATSConnectionManager(const NATSConfiguration & configuration_, LoggerPtr log_)\n     : configuration(configuration_)\n     , log(log_)\n     , event_handler(loop.getLoop(), log)\n@@ -115,8 +115,8 @@ void NATSConnectionManager::connectImpl()\n     }\n     natsOptions_SetMaxReconnect(options, configuration.max_reconnect);\n     natsOptions_SetReconnectWait(options, configuration.reconnect_wait);\n-    natsOptions_SetDisconnectedCB(options, disconnectedCallback, log);\n-    natsOptions_SetReconnectedCB(options, reconnectedCallback, log);\n+    natsOptions_SetDisconnectedCB(options, disconnectedCallback, log.get());\n+    natsOptions_SetReconnectedCB(options, reconnectedCallback, log.get());\n     natsStatus status;\n     {\n         auto lock = event_handler.setThreadLocalLoop();\ndiff --git a/src/Storages/NATS/NATSConnection.h b/src/Storages/NATS/NATSConnection.h\nindex b49070473b28..c350f395a927 100644\n--- a/src/Storages/NATS/NATSConnection.h\n+++ b/src/Storages/NATS/NATSConnection.h\n@@ -24,7 +24,7 @@ struct NATSConfiguration\n class NATSConnectionManager\n {\n public:\n-    NATSConnectionManager(const NATSConfiguration & configuration_, Poco::Logger * log_);\n+    NATSConnectionManager(const NATSConfiguration & configuration_, LoggerPtr log_);\n     ~NATSConnectionManager();\n \n     bool isConnected();\n@@ -54,7 +54,7 @@ class NATSConnectionManager\n     static void reconnectedCallback(natsConnection * nc, void * log);\n \n     NATSConfiguration configuration;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     UVLoop loop;\n     NATSHandler event_handler;\ndiff --git a/src/Storages/NATS/NATSConsumer.cpp b/src/Storages/NATS/NATSConsumer.cpp\nindex c7b40973b72e..136cb13ddfac 100644\n--- a/src/Storages/NATS/NATSConsumer.cpp\n+++ b/src/Storages/NATS/NATSConsumer.cpp\n@@ -21,7 +21,7 @@ NATSConsumer::NATSConsumer(\n     StorageNATS & storage_,\n     std::vector<String> & subjects_,\n     const String & subscribe_queue_name,\n-    Poco::Logger * log_,\n+    LoggerPtr log_,\n     uint32_t queue_size_,\n     const std::atomic<bool> & stopped_)\n     : connection(connection_)\ndiff --git a/src/Storages/NATS/NATSConsumer.h b/src/Storages/NATS/NATSConsumer.h\nindex a5470433303d..e8d3a849c2a0 100644\n--- a/src/Storages/NATS/NATSConsumer.h\n+++ b/src/Storages/NATS/NATSConsumer.h\n@@ -24,7 +24,7 @@ class NATSConsumer\n         StorageNATS & storage_,\n         std::vector<String> & subjects_,\n         const String & subscribe_queue_name,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         uint32_t queue_size_,\n         const std::atomic<bool> & stopped_);\n \n@@ -58,7 +58,7 @@ class NATSConsumer\n     StorageNATS & storage;\n     std::vector<SubscriptionPtr> subscriptions;\n     std::vector<String> subjects;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     const std::atomic<bool> & stopped;\n \n     bool subscribed = false;\ndiff --git a/src/Storages/NATS/NATSHandler.cpp b/src/Storages/NATS/NATSHandler.cpp\nindex 7006e5633a92..03f1fc1a4955 100644\n--- a/src/Storages/NATS/NATSHandler.cpp\n+++ b/src/Storages/NATS/NATSHandler.cpp\n@@ -12,7 +12,7 @@ namespace DB\n \n static const auto MAX_THREAD_WORK_DURATION_MS = 60000;\n \n-NATSHandler::NATSHandler(uv_loop_t * loop_, Poco::Logger * log_) :\n+NATSHandler::NATSHandler(uv_loop_t * loop_, LoggerPtr log_) :\n     loop(loop_),\n     log(log_),\n     loop_running(false),\ndiff --git a/src/Storages/NATS/NATSHandler.h b/src/Storages/NATS/NATSHandler.h\nindex e3894c888a3c..6f9ec398cfae 100644\n--- a/src/Storages/NATS/NATSHandler.h\n+++ b/src/Storages/NATS/NATSHandler.h\n@@ -6,7 +6,7 @@\n #include <thread>\n #include <nats.h>\n #include <base/types.h>\n-#include <Poco/Logger.h>\n+#include <Common/Logger.h>\n \n namespace DB\n {\n@@ -23,7 +23,7 @@ using LockPtr = std::unique_ptr<std::lock_guard<std::mutex>>;\n class NATSHandler\n {\n public:\n-    NATSHandler(uv_loop_t * loop_, Poco::Logger * log_);\n+    NATSHandler(uv_loop_t * loop_, LoggerPtr log_);\n \n     ~NATSHandler();\n \n@@ -47,7 +47,7 @@ class NATSHandler\n private:\n     uv_loop_t * loop;\n     natsOptions * opts = nullptr;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::atomic<bool> loop_running;\n     std::atomic<UInt8> loop_state;\ndiff --git a/src/Storages/NATS/NATSProducer.cpp b/src/Storages/NATS/NATSProducer.cpp\nindex a8510149baf2..fb8abb016f80 100644\n--- a/src/Storages/NATS/NATSProducer.cpp\n+++ b/src/Storages/NATS/NATSProducer.cpp\n@@ -23,7 +23,7 @@ NATSProducer::NATSProducer(\n     const NATSConfiguration & configuration_,\n     const String & subject_,\n     std::atomic<bool> & shutdown_called_,\n-    Poco::Logger * log_)\n+    LoggerPtr log_)\n     : AsynchronousMessageProducer(log_)\n     , connection(configuration_, log_)\n     , subject(subject_)\ndiff --git a/src/Storages/NATS/NATSProducer.h b/src/Storages/NATS/NATSProducer.h\nindex 0303d05969b2..6923553a551b 100644\n--- a/src/Storages/NATS/NATSProducer.h\n+++ b/src/Storages/NATS/NATSProducer.h\n@@ -20,7 +20,7 @@ class NATSProducer : public AsynchronousMessageProducer\n         const NATSConfiguration & configuration_,\n         const String & subject_,\n         std::atomic<bool> & shutdown_called_,\n-        Poco::Logger * log_);\n+        LoggerPtr log_);\n \n     void produce(const String & message, size_t rows_in_message, const Columns & columns, size_t last_row) override;\n \ndiff --git a/src/Storages/NATS/StorageNATS.cpp b/src/Storages/NATS/StorageNATS.cpp\nindex 9cb1fbd85061..2af9a9f974f9 100644\n--- a/src/Storages/NATS/StorageNATS.cpp\n+++ b/src/Storages/NATS/StorageNATS.cpp\n@@ -59,7 +59,7 @@ StorageNATS::StorageNATS(\n     , schema_name(getContext()->getMacros()->expand(nats_settings->nats_schema))\n     , num_consumers(nats_settings->nats_num_consumers.value)\n     , max_rows_per_message(nats_settings->nats_max_rows_per_message)\n-    , log(&Poco::Logger::get(\"StorageNATS (\" + table_id_.table_name + \")\"))\n+    , log(getLogger(\"StorageNATS (\" + table_id_.table_name + \")\"))\n     , semaphore(0, static_cast<int>(num_consumers))\n     , queue_size(std::max(QUEUE_SIZE, static_cast<uint32_t>(getMaxBlockSize())))\n     , is_attach(is_attach_)\ndiff --git a/src/Storages/NATS/StorageNATS.h b/src/Storages/NATS/StorageNATS.h\nindex 16a162b85008..882119f5cdbc 100644\n--- a/src/Storages/NATS/StorageNATS.h\n+++ b/src/Storages/NATS/StorageNATS.h\n@@ -78,7 +78,7 @@ class StorageNATS final : public IStorage, WithContext\n     size_t num_consumers;\n     size_t max_rows_per_message;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     NATSConnectionManagerPtr connection; /// Connection for all consumers\n     NATSConfiguration configuration;\ndiff --git a/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.cpp b/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.cpp\nindex b24421094098..f99ebf517928 100644\n--- a/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.cpp\n+++ b/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.cpp\n@@ -51,7 +51,7 @@ MaterializedPostgreSQLConsumer::MaterializedPostgreSQLConsumer(\n     bool schema_as_a_part_of_table_name_,\n     StorageInfos storages_info_,\n     const String & name_for_logger)\n-    : log(&Poco::Logger::get(\"PostgreSQLReplicaConsumer(\" + name_for_logger + \")\"))\n+    : log(getLogger(\"PostgreSQLReplicaConsumer(\" + name_for_logger + \")\"))\n     , context(context_)\n     , replication_slot_name(replication_slot_name_)\n     , publication_name(publication_name_)\n@@ -76,7 +76,7 @@ MaterializedPostgreSQLConsumer::MaterializedPostgreSQLConsumer(\n }\n \n \n-MaterializedPostgreSQLConsumer::StorageData::StorageData(const StorageInfo & storage_info, Poco::Logger * log_)\n+MaterializedPostgreSQLConsumer::StorageData::StorageData(const StorageInfo & storage_info, LoggerPtr log_)\n     : storage(storage_info.storage)\n     , table_description(storage_info.storage->getInMemoryMetadataPtr()->getSampleBlock())\n     , columns_attributes(storage_info.attributes)\ndiff --git a/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.h b/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.h\nindex 3e95c1cd7de5..972c03e50d86 100644\n--- a/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.h\n+++ b/src/Storages/PostgreSQL/MaterializedPostgreSQLConsumer.h\n@@ -32,7 +32,7 @@ class MaterializedPostgreSQLConsumer\n private:\n     struct StorageData\n     {\n-        explicit StorageData(const StorageInfo & storage_info, Poco::Logger * log_);\n+        explicit StorageData(const StorageInfo & storage_info, LoggerPtr log_);\n \n         size_t getColumnsNum() const { return table_description.sample_block.columns(); }\n \n@@ -137,7 +137,7 @@ class MaterializedPostgreSQLConsumer\n         return (static_cast<Int64>(upper_half) << 32) + lower_half;\n     }\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     ContextPtr context;\n     const std::string replication_slot_name, publication_name;\n \ndiff --git a/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.cpp b/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.cpp\nindex 43de2069b195..2bb1e2dde0d7 100644\n--- a/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.cpp\n+++ b/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.cpp\n@@ -128,7 +128,7 @@ PostgreSQLReplicationHandler::PostgreSQLReplicationHandler(\n     const MaterializedPostgreSQLSettings & replication_settings,\n     bool is_materialized_postgresql_database_)\n     : WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(\"PostgreSQLReplicationHandler\"))\n+    , log(getLogger(\"PostgreSQLReplicationHandler\"))\n     , is_attach(is_attach_)\n     , postgres_database(postgres_database_)\n     , postgres_schema(replication_settings.materialized_postgresql_schema)\ndiff --git a/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.h b/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.h\nindex 5d426b3c512d..5c519053d844 100644\n--- a/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.h\n+++ b/src/Storages/PostgreSQL/PostgreSQLReplicationHandler.h\n@@ -102,7 +102,7 @@ friend class TemporaryReplicationSlot;\n \n     void assertInitialized() const;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// If it is not attach, i.e. a create query, then if publication already exists - always drop it.\n     bool is_attach;\ndiff --git a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp\nindex 0faf553797ad..f13cb820ec35 100644\n--- a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp\n+++ b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.cpp\n@@ -60,7 +60,7 @@ StorageMaterializedPostgreSQL::StorageMaterializedPostgreSQL(\n     std::unique_ptr<MaterializedPostgreSQLSettings> replication_settings)\n     : IStorage(table_id_)\n     , WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(\"StorageMaterializedPostgreSQL(\" + postgres::formatNameForLogs(remote_database_name, remote_table_name_) + \")\"))\n+    , log(getLogger(\"StorageMaterializedPostgreSQL(\" + postgres::formatNameForLogs(remote_database_name, remote_table_name_) + \")\"))\n     , is_materialized_postgresql_database(false)\n     , has_nested(false)\n     , nested_context(makeNestedTableContext(context_->getGlobalContext()))\n@@ -101,7 +101,7 @@ StorageMaterializedPostgreSQL::StorageMaterializedPostgreSQL(\n         const String & postgres_table_name)\n     : IStorage(table_id_)\n     , WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(\"StorageMaterializedPostgreSQL(\" + postgres::formatNameForLogs(postgres_database_name, postgres_table_name) + \")\"))\n+    , log(getLogger(\"StorageMaterializedPostgreSQL(\" + postgres::formatNameForLogs(postgres_database_name, postgres_table_name) + \")\"))\n     , is_materialized_postgresql_database(true)\n     , has_nested(false)\n     , nested_context(makeNestedTableContext(context_->getGlobalContext()))\n@@ -120,7 +120,7 @@ StorageMaterializedPostgreSQL::StorageMaterializedPostgreSQL(\n         const String & postgres_table_name)\n     : IStorage(StorageID(nested_storage_->getStorageID().database_name, nested_storage_->getStorageID().table_name))\n     , WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(\"StorageMaterializedPostgreSQL(\" + postgres::formatNameForLogs(postgres_database_name, postgres_table_name) + \")\"))\n+    , log(getLogger(\"StorageMaterializedPostgreSQL(\" + postgres::formatNameForLogs(postgres_database_name, postgres_table_name) + \")\"))\n     , is_materialized_postgresql_database(true)\n     , has_nested(true)\n     , nested_context(makeNestedTableContext(context_->getGlobalContext()))\n@@ -141,7 +141,7 @@ StoragePtr StorageMaterializedPostgreSQL::createTemporary() const\n     auto tmp_storage = DatabaseCatalog::instance().tryGetTable(tmp_table_id, nested_context);\n     if (tmp_storage)\n     {\n-        LOG_TRACE(&Poco::Logger::get(\"MaterializedPostgreSQLStorage\"), \"Temporary table {} already exists, dropping\", tmp_table_id.getNameForLogs());\n+        LOG_TRACE(getLogger(\"MaterializedPostgreSQLStorage\"), \"Temporary table {} already exists, dropping\", tmp_table_id.getNameForLogs());\n         InterpreterDropQuery::executeDropQuery(ASTDropQuery::Kind::Drop, getContext(), getContext(), tmp_table_id, /* sync */true);\n     }\n \ndiff --git a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h\nindex bebbb74ddd11..9c9418a8caa9 100644\n--- a/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h\n+++ b/src/Storages/PostgreSQL/StorageMaterializedPostgreSQL.h\n@@ -142,7 +142,7 @@ class StorageMaterializedPostgreSQL final : public IStorage, WithContext\n \n     String getNestedTableName() const;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Not nullptr only for single MaterializedPostgreSQL storage, because for MaterializedPostgreSQL\n     /// database engine there is one replication handler for all tables.\ndiff --git a/src/Storages/RabbitMQ/RabbitMQConnection.cpp b/src/Storages/RabbitMQ/RabbitMQConnection.cpp\nindex 13d065774a27..98ceba42676b 100644\n--- a/src/Storages/RabbitMQ/RabbitMQConnection.cpp\n+++ b/src/Storages/RabbitMQ/RabbitMQConnection.cpp\n@@ -11,7 +11,7 @@ static const auto CONNECT_SLEEP = 200;\n static const auto RETRIES_MAX = 20;\n \n \n-RabbitMQConnection::RabbitMQConnection(const RabbitMQConfiguration & configuration_, Poco::Logger * log_)\n+RabbitMQConnection::RabbitMQConnection(const RabbitMQConfiguration & configuration_, LoggerPtr log_)\n     : configuration(configuration_)\n     , log(log_)\n     , event_handler(loop.getLoop(), log)\ndiff --git a/src/Storages/RabbitMQ/RabbitMQConnection.h b/src/Storages/RabbitMQ/RabbitMQConnection.h\nindex 698230b16f4a..5adb64561948 100644\n--- a/src/Storages/RabbitMQ/RabbitMQConnection.h\n+++ b/src/Storages/RabbitMQ/RabbitMQConnection.h\n@@ -22,7 +22,7 @@ struct RabbitMQConfiguration\n class RabbitMQConnection\n {\n public:\n-    RabbitMQConnection(const RabbitMQConfiguration & configuration_, Poco::Logger * log_);\n+    RabbitMQConnection(const RabbitMQConfiguration & configuration_, LoggerPtr log_);\n \n     bool isConnected();\n \n@@ -51,7 +51,7 @@ class RabbitMQConnection\n     void disconnectImpl(bool immediately = false);\n \n     RabbitMQConfiguration configuration;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     UVLoop loop;\n     /// Preserve order of destruction here:\ndiff --git a/src/Storages/RabbitMQ/RabbitMQConsumer.cpp b/src/Storages/RabbitMQ/RabbitMQConsumer.cpp\nindex f6facc04212e..1843bebe3c7f 100644\n--- a/src/Storages/RabbitMQ/RabbitMQConsumer.cpp\n+++ b/src/Storages/RabbitMQ/RabbitMQConsumer.cpp\n@@ -24,7 +24,7 @@ RabbitMQConsumer::RabbitMQConsumer(\n         std::vector<String> & queues_,\n         size_t channel_id_base_,\n         const String & channel_base_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         uint32_t queue_size_)\n         : event_handler(event_handler_)\n         , queues(queues_)\ndiff --git a/src/Storages/RabbitMQ/RabbitMQConsumer.h b/src/Storages/RabbitMQ/RabbitMQConsumer.h\nindex 89dfa060eecb..c78b33bfc7cc 100644\n--- a/src/Storages/RabbitMQ/RabbitMQConsumer.h\n+++ b/src/Storages/RabbitMQ/RabbitMQConsumer.h\n@@ -32,7 +32,7 @@ class RabbitMQConsumer\n         std::vector<String> & queues_,\n         size_t channel_id_base_,\n         const String & channel_base_,\n-        Poco::Logger * log_,\n+        LoggerPtr log_,\n         uint32_t queue_size_);\n \n     struct CommitInfo\n@@ -88,7 +88,7 @@ class RabbitMQConsumer\n     const String channel_base;\n     const size_t channel_id_base;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::atomic<bool> stopped;\n \n     String channel_id;\ndiff --git a/src/Storages/RabbitMQ/RabbitMQHandler.cpp b/src/Storages/RabbitMQ/RabbitMQHandler.cpp\nindex 745af0d20e3a..be352f26f7be 100644\n--- a/src/Storages/RabbitMQ/RabbitMQHandler.cpp\n+++ b/src/Storages/RabbitMQ/RabbitMQHandler.cpp\n@@ -8,7 +8,7 @@ namespace DB\n /* The object of this class is shared between concurrent consumers (who share the same connection == share the same\n  * event loop and handler).\n  */\n-RabbitMQHandler::RabbitMQHandler(uv_loop_t * loop_, Poco::Logger * log_) :\n+RabbitMQHandler::RabbitMQHandler(uv_loop_t * loop_, LoggerPtr log_) :\n     AMQP::LibUvHandler(loop_),\n     loop(loop_),\n     log(log_),\ndiff --git a/src/Storages/RabbitMQ/RabbitMQHandler.h b/src/Storages/RabbitMQ/RabbitMQHandler.h\nindex 4223732a4a07..244692cf8009 100644\n--- a/src/Storages/RabbitMQ/RabbitMQHandler.h\n+++ b/src/Storages/RabbitMQ/RabbitMQHandler.h\n@@ -24,7 +24,7 @@ class RabbitMQHandler : public AMQP::LibUvHandler\n {\n \n public:\n-    RabbitMQHandler(uv_loop_t * loop_, Poco::Logger * log_);\n+    RabbitMQHandler(uv_loop_t * loop_, LoggerPtr log_);\n \n     void onError(AMQP::TcpConnection * connection, const char * message) override;\n     void onReady(AMQP::TcpConnection * connection) override;\n@@ -50,7 +50,7 @@ class RabbitMQHandler : public AMQP::LibUvHandler\n \n private:\n     uv_loop_t * loop;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::atomic<bool> connection_running, loop_running;\n     std::atomic<UInt8> loop_state;\ndiff --git a/src/Storages/RabbitMQ/RabbitMQProducer.cpp b/src/Storages/RabbitMQ/RabbitMQProducer.cpp\nindex 246569060d00..7ad83213b9b1 100644\n--- a/src/Storages/RabbitMQ/RabbitMQProducer.cpp\n+++ b/src/Storages/RabbitMQ/RabbitMQProducer.cpp\n@@ -31,7 +31,7 @@ RabbitMQProducer::RabbitMQProducer(\n     const size_t channel_id_base_,\n     const bool persistent_,\n     std::atomic<bool> & shutdown_called_,\n-    Poco::Logger * log_)\n+    LoggerPtr log_)\n     : AsynchronousMessageProducer(log_)\n     , connection(configuration_, log_)\n     , routing_keys(routing_keys_)\ndiff --git a/src/Storages/RabbitMQ/RabbitMQProducer.h b/src/Storages/RabbitMQ/RabbitMQProducer.h\nindex 70afbbb9b903..a790eda0d085 100644\n--- a/src/Storages/RabbitMQ/RabbitMQProducer.h\n+++ b/src/Storages/RabbitMQ/RabbitMQProducer.h\n@@ -24,7 +24,7 @@ class RabbitMQProducer : public AsynchronousMessageProducer\n         const size_t channel_id_base_,\n         const bool persistent_,\n         std::atomic<bool> & shutdown_called_,\n-        Poco::Logger * log_);\n+        LoggerPtr log_);\n \n     void produce(const String & message, size_t rows_in_message, const Columns & columns, size_t last_row) override;\n \ndiff --git a/src/Storages/RabbitMQ/RabbitMQSource.cpp b/src/Storages/RabbitMQ/RabbitMQSource.cpp\nindex 793064c10f88..3cec448fc115 100644\n--- a/src/Storages/RabbitMQ/RabbitMQSource.cpp\n+++ b/src/Storages/RabbitMQ/RabbitMQSource.cpp\n@@ -70,7 +70,7 @@ RabbitMQSource::RabbitMQSource(\n     , ack_in_suffix(ack_in_suffix_)\n     , non_virtual_header(std::move(headers.first))\n     , virtual_header(std::move(headers.second))\n-    , log(&Poco::Logger::get(\"RabbitMQSource\"))\n+    , log(getLogger(\"RabbitMQSource\"))\n     , max_execution_time_ms(max_execution_time_)\n {\n     storage.incrementReader();\ndiff --git a/src/Storages/RabbitMQ/RabbitMQSource.h b/src/Storages/RabbitMQ/RabbitMQSource.h\nindex a25b3d502220..21d059bfae2e 100644\n--- a/src/Storages/RabbitMQ/RabbitMQSource.h\n+++ b/src/Storages/RabbitMQ/RabbitMQSource.h\n@@ -47,7 +47,7 @@ class RabbitMQSource : public ISource\n     const Block non_virtual_header;\n     const Block virtual_header;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     RabbitMQConsumerPtr consumer;\n \n     uint64_t max_execution_time_ms = 0;\ndiff --git a/src/Storages/RabbitMQ/StorageRabbitMQ.cpp b/src/Storages/RabbitMQ/StorageRabbitMQ.cpp\nindex fce2d775b157..025f421db592 100644\n--- a/src/Storages/RabbitMQ/StorageRabbitMQ.cpp\n+++ b/src/Storages/RabbitMQ/StorageRabbitMQ.cpp\n@@ -86,7 +86,7 @@ StorageRabbitMQ::StorageRabbitMQ(\n         , persistent(rabbitmq_settings->rabbitmq_persistent.value)\n         , use_user_setup(rabbitmq_settings->rabbitmq_queue_consume.value)\n         , hash_exchange(num_consumers > 1 || num_queues > 1)\n-        , log(&Poco::Logger::get(\"StorageRabbitMQ (\" + table_id_.table_name + \")\"))\n+        , log(getLogger(\"StorageRabbitMQ (\" + table_id_.table_name + \")\"))\n         , semaphore(0, static_cast<int>(num_consumers))\n         , unique_strbase(getRandomName())\n         , queue_size(std::max(QUEUE_SIZE, static_cast<uint32_t>(getMaxBlockSize())))\ndiff --git a/src/Storages/RabbitMQ/StorageRabbitMQ.h b/src/Storages/RabbitMQ/StorageRabbitMQ.h\nindex 120930cf01d1..be46caf97989 100644\n--- a/src/Storages/RabbitMQ/StorageRabbitMQ.h\n+++ b/src/Storages/RabbitMQ/StorageRabbitMQ.h\n@@ -102,7 +102,7 @@ class StorageRabbitMQ final: public IStorage, WithContext\n     bool use_user_setup;\n \n     bool hash_exchange;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     RabbitMQConnectionPtr connection; /// Connection for all consumers\n     RabbitMQConfiguration configuration;\ndiff --git a/src/Storages/S3Queue/S3QueueFilesMetadata.cpp b/src/Storages/S3Queue/S3QueueFilesMetadata.cpp\nindex f49e1d6f25c5..a2b41eb4685e 100644\n--- a/src/Storages/S3Queue/S3QueueFilesMetadata.cpp\n+++ b/src/Storages/S3Queue/S3QueueFilesMetadata.cpp\n@@ -133,7 +133,7 @@ S3QueueFilesMetadata::S3QueueFilesMetadata(const fs::path & zookeeper_path_, con\n     , zookeeper_processed_path(zookeeper_path_ / \"processed\")\n     , zookeeper_failed_path(zookeeper_path_ / \"failed\")\n     , zookeeper_cleanup_lock_path(zookeeper_path_ / \"cleanup_lock\")\n-    , log(&Poco::Logger::get(\"S3QueueFilesMetadata\"))\n+    , log(getLogger(\"S3QueueFilesMetadata\"))\n {\n     if (mode == S3QueueMode::UNORDERED && (max_set_size || max_set_age_sec))\n     {\n@@ -689,7 +689,7 @@ S3QueueFilesMetadata::ProcessingNodeHolder::ProcessingNodeHolder(\n     , path(path_)\n     , zk_node_path(zk_node_path_)\n     , processing_id(processing_id_)\n-    , log(&Poco::Logger::get(\"ProcessingNodeHolder\"))\n+    , log(getLogger(\"ProcessingNodeHolder\"))\n {\n }\n \ndiff --git a/src/Storages/S3Queue/S3QueueFilesMetadata.h b/src/Storages/S3Queue/S3QueueFilesMetadata.h\nindex f3be7c5c3a0d..390cb5a64ab0 100644\n--- a/src/Storages/S3Queue/S3QueueFilesMetadata.h\n+++ b/src/Storages/S3Queue/S3QueueFilesMetadata.h\n@@ -93,7 +93,7 @@ class S3QueueFilesMetadata\n     const fs::path zookeeper_failed_path;\n     const fs::path zookeeper_cleanup_lock_path;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::atomic_bool shutdown = false;\n     BackgroundSchedulePool::TaskHolder task;\n@@ -169,7 +169,7 @@ class S3QueueFilesMetadata::ProcessingNodeHolder\n     std::string zk_node_path;\n     std::string processing_id;\n     bool removed = false;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/S3Queue/S3QueueSource.cpp b/src/Storages/S3Queue/S3QueueSource.cpp\nindex 27bec039f96c..54155ad3ea7d 100644\n--- a/src/Storages/S3Queue/S3QueueSource.cpp\n+++ b/src/Storages/S3Queue/S3QueueSource.cpp\n@@ -60,7 +60,7 @@ StorageS3QueueSource::KeyWithInfoPtr StorageS3QueueSource::FileIterator::next()\n \n         if (shutdown_called)\n         {\n-            LOG_TEST(&Poco::Logger::get(\"StorageS3QueueSource\"), \"Shutdown was called, stopping file iterator\");\n+            LOG_TEST(getLogger(\"StorageS3QueueSource\"), \"Shutdown was called, stopping file iterator\");\n             return {};\n         }\n \n@@ -91,7 +91,7 @@ StorageS3QueueSource::StorageS3QueueSource(\n     const std::atomic<bool> & table_is_being_dropped_,\n     std::shared_ptr<S3QueueLog> s3_queue_log_,\n     const StorageID & storage_id_,\n-    Poco::Logger * log_)\n+    LoggerPtr log_)\n     : ISource(header_)\n     , WithContext(context_)\n     , name(std::move(name_))\ndiff --git a/src/Storages/S3Queue/S3QueueSource.h b/src/Storages/S3Queue/S3QueueSource.h\nindex 542f8e8fd8c7..82e75020efb2 100644\n--- a/src/Storages/S3Queue/S3QueueSource.h\n+++ b/src/Storages/S3Queue/S3QueueSource.h\n@@ -67,7 +67,7 @@ class StorageS3QueueSource : public ISource, WithContext\n         const std::atomic<bool> & table_is_being_dropped_,\n         std::shared_ptr<S3QueueLog> s3_queue_log_,\n         const StorageID & storage_id_,\n-        Poco::Logger * log_);\n+        LoggerPtr log_);\n \n     ~StorageS3QueueSource() override;\n \n@@ -89,7 +89,7 @@ class StorageS3QueueSource : public ISource, WithContext\n     const StorageID storage_id;\n \n     RemoveFileFunc remove_file_func;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     using ReaderHolder = StorageS3Source::ReaderHolder;\n     ReaderHolder reader;\ndiff --git a/src/Storages/S3Queue/StorageS3Queue.cpp b/src/Storages/S3Queue/StorageS3Queue.cpp\nindex bc33e8cf2a90..48c284c022aa 100644\n--- a/src/Storages/S3Queue/StorageS3Queue.cpp\n+++ b/src/Storages/S3Queue/StorageS3Queue.cpp\n@@ -75,7 +75,7 @@ namespace\n         return zkutil::extractZooKeeperPath(result_zk_path, true);\n     }\n \n-    void checkAndAdjustSettings(S3QueueSettings & s3queue_settings, const Settings & settings, Poco::Logger * log)\n+    void checkAndAdjustSettings(S3QueueSettings & s3queue_settings, const Settings & settings, LoggerPtr log)\n     {\n         if (s3queue_settings.mode == S3QueueMode::ORDERED && s3queue_settings.s3queue_processing_threads_num > 1)\n         {\n@@ -119,7 +119,7 @@ StorageS3Queue::StorageS3Queue(\n     , configuration{configuration_}\n     , format_settings(format_settings_)\n     , reschedule_processing_interval_ms(s3queue_settings->s3queue_polling_min_timeout_ms)\n-    , log(&Poco::Logger::get(\"StorageS3Queue (\" + table_id_.table_name + \")\"))\n+    , log(getLogger(\"StorageS3Queue (\" + table_id_.table_name + \")\"))\n {\n     if (configuration.url.key.empty())\n     {\n@@ -600,7 +600,7 @@ void registerStorageS3QueueImpl(const String & name, StorageFactory & factory)\n                     if (user_format_settings.has(change.name))\n                         user_format_settings.set(change.name, change.value);\n                     else\n-                        LOG_TRACE(&Poco::Logger::get(\"StorageS3\"), \"Remove: {}\", change.name);\n+                        LOG_TRACE(getLogger(\"StorageS3\"), \"Remove: {}\", change.name);\n                     args.storage_def->settings->changes.removeSetting(change.name);\n                 }\n \ndiff --git a/src/Storages/S3Queue/StorageS3Queue.h b/src/Storages/S3Queue/StorageS3Queue.h\nindex 3d3594dc2ab3..5d2be610d58a 100644\n--- a/src/Storages/S3Queue/StorageS3Queue.h\n+++ b/src/Storages/S3Queue/StorageS3Queue.h\n@@ -79,7 +79,7 @@ class StorageS3Queue : public IStorage, WithContext\n     std::atomic<bool> shutdown_called = false;\n     std::atomic<bool> table_is_being_dropped = false;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     void startup() override;\n     void shutdown(bool is_drop) override;\ndiff --git a/src/Storages/StorageAzureBlob.h b/src/Storages/StorageAzureBlob.h\nindex 16e5b9edfb69..6fc3c5ce5920 100644\n--- a/src/Storages/StorageAzureBlob.h\n+++ b/src/Storages/StorageAzureBlob.h\n@@ -319,7 +319,7 @@ class StorageAzureBlobSource : public ISource, WithContext\n \n     ReaderHolder reader;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"StorageAzureBlobSource\");\n+    LoggerPtr log = getLogger(\"StorageAzureBlobSource\");\n \n     ThreadPool create_reader_pool;\n     ThreadPoolCallbackRunner<ReaderHolder> create_reader_scheduler;\ndiff --git a/src/Storages/StorageAzureBlobCluster.cpp b/src/Storages/StorageAzureBlobCluster.cpp\nindex a6372577fb05..1d587512f38f 100644\n--- a/src/Storages/StorageAzureBlobCluster.cpp\n+++ b/src/Storages/StorageAzureBlobCluster.cpp\n@@ -38,7 +38,7 @@ StorageAzureBlobCluster::StorageAzureBlobCluster(\n     const ConstraintsDescription & constraints_,\n     ContextPtr context_,\n     bool structure_argument_was_provided_)\n-    : IStorageCluster(cluster_name_, table_id_, &Poco::Logger::get(\"StorageAzureBlobCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n+    : IStorageCluster(cluster_name_, table_id_, getLogger(\"StorageAzureBlobCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n     , configuration{configuration_}\n     , object_storage(std::move(object_storage_))\n {\ndiff --git a/src/Storages/StorageBuffer.cpp b/src/Storages/StorageBuffer.cpp\nindex 6f4b1563a46b..d5c135bb81dc 100644\n--- a/src/Storages/StorageBuffer.cpp\n+++ b/src/Storages/StorageBuffer.cpp\n@@ -137,7 +137,7 @@ StorageBuffer::StorageBuffer(\n     , flush_thresholds(flush_thresholds_)\n     , destination_id(destination_id_)\n     , allow_materialized(allow_materialized_)\n-    , log(&Poco::Logger::get(\"StorageBuffer (\" + table_id_.getFullTableName() + \")\"))\n+    , log(getLogger(\"StorageBuffer (\" + table_id_.getFullTableName() + \")\"))\n     , bg_pool(getContext()->getBufferFlushSchedulePool())\n {\n     StorageInMemoryMetadata storage_metadata;\n@@ -433,7 +433,7 @@ void StorageBuffer::read(\n }\n \n \n-static void appendBlock(Poco::Logger * log, const Block & from, Block & to)\n+static void appendBlock(LoggerPtr log, const Block & from, Block & to)\n {\n     size_t rows = from.rows();\n     size_t old_rows = to.rows();\ndiff --git a/src/Storages/StorageBuffer.h b/src/Storages/StorageBuffer.h\nindex ef646a125483..47f6239b1734 100644\n--- a/src/Storages/StorageBuffer.h\n+++ b/src/Storages/StorageBuffer.h\n@@ -166,7 +166,7 @@ friend class BufferSink;\n     Writes lifetime_writes;\n     Writes total_writes;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     void flushAllBuffers(bool check_thresholds = true);\n     bool flushBuffer(Buffer & buffer, bool check_thresholds, bool locked = false);\ndiff --git a/src/Storages/StorageDistributed.cpp b/src/Storages/StorageDistributed.cpp\nindex 987ea4a4957f..2d05efdd74f6 100644\n--- a/src/Storages/StorageDistributed.cpp\n+++ b/src/Storages/StorageDistributed.cpp\n@@ -329,7 +329,7 @@ StorageDistributed::StorageDistributed(\n     , remote_database(remote_database_)\n     , remote_table(remote_table_)\n     , remote_table_function_ptr(remote_table_function_ptr_)\n-    , log(&Poco::Logger::get(\"StorageDistributed (\" + id_.table_name + \")\"))\n+    , log(getLogger(\"StorageDistributed (\" + id_.table_name + \")\"))\n     , owned_cluster(std::move(owned_cluster_))\n     , cluster_name(getContext()->getMacros()->expand(cluster_name_))\n     , has_sharding_key(sharding_key_)\ndiff --git a/src/Storages/StorageDistributed.h b/src/Storages/StorageDistributed.h\nindex b7ed85e87df3..161a5983f941 100644\n--- a/src/Storages/StorageDistributed.h\n+++ b/src/Storages/StorageDistributed.h\n@@ -238,7 +238,7 @@ class StorageDistributed final : public IStorage, WithContext\n     String remote_table;\n     ASTPtr remote_table_function_ptr;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Used to implement TableFunctionRemote.\n     std::shared_ptr<Cluster> owned_cluster;\ndiff --git a/src/Storages/StorageExecutable.cpp b/src/Storages/StorageExecutable.cpp\nindex 2acbf3f46106..e475211deb39 100644\n--- a/src/Storages/StorageExecutable.cpp\n+++ b/src/Storages/StorageExecutable.cpp\n@@ -80,7 +80,7 @@ StorageExecutable::StorageExecutable(\n     : IStorage(table_id_)\n     , settings(settings_)\n     , input_queries(input_queries_)\n-    , log(settings.is_executable_pool ? &Poco::Logger::get(\"StorageExecutablePool\") : &Poco::Logger::get(\"StorageExecutable\"))\n+    , log(settings.is_executable_pool ? getLogger(\"StorageExecutablePool\") : getLogger(\"StorageExecutable\"))\n {\n     StorageInMemoryMetadata storage_metadata;\n     storage_metadata.setColumns(columns);\ndiff --git a/src/Storages/StorageExecutable.h b/src/Storages/StorageExecutable.h\nindex 37455385675c..2be2a84ab497 100644\n--- a/src/Storages/StorageExecutable.h\n+++ b/src/Storages/StorageExecutable.h\n@@ -45,7 +45,7 @@ class StorageExecutable final : public IStorage\n private:\n     ExecutableSettings settings;\n     std::vector<ASTPtr> input_queries;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n     std::unique_ptr<ShellCommandSourceCoordinator> coordinator;\n };\n \ndiff --git a/src/Storages/StorageFile.cpp b/src/Storages/StorageFile.cpp\nindex 9f864813de9a..8979e068fb53 100644\n--- a/src/Storages/StorageFile.cpp\n+++ b/src/Storages/StorageFile.cpp\n@@ -1040,7 +1040,7 @@ void StorageFileSource::beforeDestroy()\n             catch (const std::exception & e)\n             {\n                 // Cannot throw exception from destructor, will write only error\n-                LOG_ERROR(&Poco::Logger::get(\"~StorageFileSource\"), \"Failed to rename file {}: {}\", file_path_ref, e.what());\n+                LOG_ERROR(getLogger(\"~StorageFileSource\"), \"Failed to rename file {}: {}\", file_path_ref, e.what());\n                 continue;\n             }\n         }\ndiff --git a/src/Storages/StorageFile.h b/src/Storages/StorageFile.h\nindex b74868597a6c..2955eb0f1aac 100644\n--- a/src/Storages/StorageFile.h\n+++ b/src/Storages/StorageFile.h\n@@ -163,7 +163,7 @@ class StorageFile final : public IStorage\n \n     mutable std::shared_timed_mutex rwlock;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"StorageFile\");\n+    LoggerPtr log = getLogger(\"StorageFile\");\n \n     /// Total number of bytes to read (sums for multiple files in case of globs). Needed for progress bar.\n     size_t total_bytes_to_read = 0;\ndiff --git a/src/Storages/StorageFileCluster.cpp b/src/Storages/StorageFileCluster.cpp\nindex c12124f1e07e..0cc961bb464d 100644\n--- a/src/Storages/StorageFileCluster.cpp\n+++ b/src/Storages/StorageFileCluster.cpp\n@@ -34,7 +34,7 @@ StorageFileCluster::StorageFileCluster(\n     const ColumnsDescription & columns_,\n     const ConstraintsDescription & constraints_,\n     bool structure_argument_was_provided_)\n-    : IStorageCluster(cluster_name_, table_id_, &Poco::Logger::get(\"StorageFileCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n+    : IStorageCluster(cluster_name_, table_id_, getLogger(\"StorageFileCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n     , filename(filename_)\n     , format_name(format_name_)\n     , compression_method(compression_method_)\ndiff --git a/src/Storages/StorageJoin.cpp b/src/Storages/StorageJoin.cpp\nindex efe446a8ccd8..b9e082c0b224 100644\n--- a/src/Storages/StorageJoin.cpp\n+++ b/src/Storages/StorageJoin.cpp\n@@ -104,7 +104,7 @@ void StorageJoin::truncate(const ASTPtr &, const StorageMetadataPtr &, ContextPt\n     if (disk->exists(path))\n         disk->removeRecursive(path);\n     else\n-        LOG_INFO(&Poco::Logger::get(\"StorageJoin\"), \"Path {} is already removed from disk {}\", path, disk->getName());\n+        LOG_INFO(getLogger(\"StorageJoin\"), \"Path {} is already removed from disk {}\", path, disk->getName());\n \n     disk->createDirectories(path);\n     disk->createDirectories(fs::path(path) / \"tmp/\");\ndiff --git a/src/Storages/StorageKeeperMap.cpp b/src/Storages/StorageKeeperMap.cpp\nindex 8914838afed3..80abaa3ea2d2 100644\n--- a/src/Storages/StorageKeeperMap.cpp\n+++ b/src/Storages/StorageKeeperMap.cpp\n@@ -321,7 +321,7 @@ StorageKeeperMap::StorageKeeperMap(\n     , primary_key(primary_key_)\n     , zookeeper_name(zkutil::extractZooKeeperName(zk_root_path_))\n     , keys_limit(keys_limit_)\n-    , log(&Poco::Logger::get(fmt::format(\"StorageKeeperMap ({})\", table_id.getNameForLogs())))\n+    , log(getLogger(fmt::format(\"StorageKeeperMap ({})\", table_id.getNameForLogs())))\n {\n     std::string path_prefix = context_->getConfigRef().getString(\"keeper_map_path_prefix\", \"\");\n     if (path_prefix.empty())\n@@ -776,7 +776,7 @@ void StorageKeeperMap::backupData(BackupEntriesCollector & backup_entries_collec\n \n         auto with_retries = std::make_shared<WithRetries>\n         (\n-            &Poco::Logger::get(fmt::format(\"StorageKeeperMapBackup ({})\", getStorageID().getNameForLogs())),\n+            getLogger(fmt::format(\"StorageKeeperMapBackup ({})\", getStorageID().getNameForLogs())),\n             [&] { return getClient(); },\n             WithRetries::KeeperSettings::fromContext(backup_entries_collector.getContext()),\n             backup_entries_collector.getContext()->getProcessListElement(),\n@@ -808,7 +808,7 @@ void StorageKeeperMap::restoreDataFromBackup(RestorerFromBackup & restorer, cons\n \n     auto with_retries = std::make_shared<WithRetries>\n     (\n-        &Poco::Logger::get(fmt::format(\"StorageKeeperMapRestore ({})\", getStorageID().getNameForLogs())),\n+        getLogger(fmt::format(\"StorageKeeperMapRestore ({})\", getStorageID().getNameForLogs())),\n         [&] { return getClient(); },\n         WithRetries::KeeperSettings::fromContext(restorer.getContext()),\n         restorer.getContext()->getProcessListElement(),\ndiff --git a/src/Storages/StorageKeeperMap.h b/src/Storages/StorageKeeperMap.h\nindex aa9687243d88..9dca96a24a36 100644\n--- a/src/Storages/StorageKeeperMap.h\n+++ b/src/Storages/StorageKeeperMap.h\n@@ -146,7 +146,7 @@ class StorageKeeperMap final : public IStorage, public IKeyValueEntity, WithCont\n     mutable std::mutex init_mutex;\n     mutable std::optional<bool> table_is_valid;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/StorageMaterializedView.cpp b/src/Storages/StorageMaterializedView.cpp\nindex 050e76c9205c..bfe75e61bcde 100644\n--- a/src/Storages/StorageMaterializedView.cpp\n+++ b/src/Storages/StorageMaterializedView.cpp\n@@ -523,7 +523,7 @@ void StorageMaterializedView::backupData(BackupEntriesCollector & backup_entries\n         if (auto table = tryGetTargetTable())\n             table->backupData(backup_entries_collector, data_path_in_backup, partitions);\n         else\n-            LOG_WARNING(&Poco::Logger::get(\"StorageMaterializedView\"),\n+            LOG_WARNING(getLogger(\"StorageMaterializedView\"),\n                         \"Inner table does not exist, will not backup any data\");\n     }\n }\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex fbdde15c2af9..7e6c5ca39243 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -74,7 +74,7 @@ namespace ActionLocks\n     extern const StorageActionBlockType PartsMove;\n }\n \n-static MergeTreeTransactionPtr tryGetTransactionForMutation(const MergeTreeMutationEntry & mutation, Poco::Logger * log = nullptr)\n+static MergeTreeTransactionPtr tryGetTransactionForMutation(const MergeTreeMutationEntry & mutation, LoggerPtr log = nullptr)\n {\n     assert(!mutation.tid.isEmpty());\n     if (mutation.tid.isPrehistoric())\ndiff --git a/src/Storages/StorageMySQL.cpp b/src/Storages/StorageMySQL.cpp\nindex 76a439eabafc..da391909dff4 100644\n--- a/src/Storages/StorageMySQL.cpp\n+++ b/src/Storages/StorageMySQL.cpp\n@@ -55,7 +55,7 @@ StorageMySQL::StorageMySQL(\n     , on_duplicate_clause{on_duplicate_clause_}\n     , mysql_settings(mysql_settings_)\n     , pool(std::make_shared<mysqlxx::PoolWithFailover>(pool_))\n-    , log(&Poco::Logger::get(\"StorageMySQL (\" + table_id_.table_name + \")\"))\n+    , log(getLogger(\"StorageMySQL (\" + table_id_.table_name + \")\"))\n {\n     StorageInMemoryMetadata storage_metadata;\n \ndiff --git a/src/Storages/StorageMySQL.h b/src/Storages/StorageMySQL.h\nindex 5303117cf5c8..daabd66a5309 100644\n--- a/src/Storages/StorageMySQL.h\n+++ b/src/Storages/StorageMySQL.h\n@@ -92,7 +92,7 @@ class StorageMySQL final : public IStorage, WithContext\n \n     mysqlxx::PoolWithFailoverPtr pool;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/StoragePostgreSQL.cpp b/src/Storages/StoragePostgreSQL.cpp\nindex 8fe2a161dba4..9379cb5a1c6c 100644\n--- a/src/Storages/StoragePostgreSQL.cpp\n+++ b/src/Storages/StoragePostgreSQL.cpp\n@@ -72,7 +72,7 @@ StoragePostgreSQL::StoragePostgreSQL(\n     , remote_table_schema(remote_table_schema_)\n     , on_conflict(on_conflict_)\n     , pool(std::move(pool_))\n-    , log(&Poco::Logger::get(\"StoragePostgreSQL (\" + table_id_.table_name + \")\"))\n+    , log(getLogger(\"StoragePostgreSQL (\" + table_id_.table_name + \")\"))\n {\n     StorageInMemoryMetadata storage_metadata;\n \ndiff --git a/src/Storages/StoragePostgreSQL.h b/src/Storages/StoragePostgreSQL.h\nindex 725a935aa46c..1ed4f7a7611b 100644\n--- a/src/Storages/StoragePostgreSQL.h\n+++ b/src/Storages/StoragePostgreSQL.h\n@@ -79,7 +79,7 @@ class StoragePostgreSQL final : public IStorage\n     String on_conflict;\n     postgres::PoolWithFailoverPtr pool;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/StorageRedis.cpp b/src/Storages/StorageRedis.cpp\nindex ddb1b62c7b03..83bb3c606c92 100644\n--- a/src/Storages/StorageRedis.cpp\n+++ b/src/Storages/StorageRedis.cpp\n@@ -206,7 +206,7 @@ StorageRedis::StorageRedis(\n     , WithContext(context_->getGlobalContext())\n     , table_id(table_id_)\n     , configuration(configuration_)\n-    , log(&Poco::Logger::get(\"StorageRedis\"))\n+    , log(getLogger(\"StorageRedis\"))\n     , primary_key(primary_key_)\n {\n     pool = std::make_shared<RedisPool>(configuration.pool_size);\ndiff --git a/src/Storages/StorageRedis.h b/src/Storages/StorageRedis.h\nindex a525a4ed7de9..a0eb2bfa580d 100644\n--- a/src/Storages/StorageRedis.h\n+++ b/src/Storages/StorageRedis.h\n@@ -74,7 +74,7 @@ class StorageRedis : public IStorage, public IKeyValueEntity, WithContext\n     StorageID table_id;\n     RedisConfiguration configuration;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n     RedisPoolPtr pool;\n \n     const String primary_key;\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 715cbab9eeae..c82721d2a18f 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -1133,7 +1133,7 @@ void StorageReplicatedMergeTree::drop()\n }\n \n void StorageReplicatedMergeTree::dropReplica(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path, const String & replica,\n-                                             Poco::Logger * logger, MergeTreeSettingsPtr table_settings, std::optional<bool> * has_metadata_out)\n+                                             LoggerPtr logger, MergeTreeSettingsPtr table_settings, std::optional<bool> * has_metadata_out)\n {\n     if (zookeeper->expired())\n         throw Exception(ErrorCodes::TABLE_WAS_NOT_DROPPED, \"Table was not dropped because ZooKeeper session has expired.\");\n@@ -1251,7 +1251,7 @@ void StorageReplicatedMergeTree::dropReplica(zkutil::ZooKeeperPtr zookeeper, con\n     }\n }\n \n-void StorageReplicatedMergeTree::dropReplica(const String & drop_zookeeper_path, const String & drop_replica, Poco::Logger * logger)\n+void StorageReplicatedMergeTree::dropReplica(const String & drop_zookeeper_path, const String & drop_replica, LoggerPtr logger)\n {\n     zkutil::ZooKeeperPtr zookeeper = getZooKeeperIfTableShutDown();\n \n@@ -1266,7 +1266,7 @@ void StorageReplicatedMergeTree::dropReplica(const String & drop_zookeeper_path,\n \n \n bool StorageReplicatedMergeTree::removeTableNodesFromZooKeeper(zkutil::ZooKeeperPtr zookeeper,\n-        const String & zookeeper_path, const zkutil::EphemeralNodeHolder::Ptr & metadata_drop_lock, Poco::Logger * logger)\n+        const String & zookeeper_path, const zkutil::EphemeralNodeHolder::Ptr & metadata_drop_lock, LoggerPtr logger)\n {\n     bool completely_removed = false;\n \n@@ -4316,7 +4316,7 @@ void StorageReplicatedMergeTree::waitForUniquePartsToBeFetchedByOtherReplicas(St\n         LOG_INFO(log, \"Successfully waited all the parts\");\n }\n \n-std::set<MergeTreePartInfo> StorageReplicatedMergeTree::findReplicaUniqueParts(const String & replica_name_, const String & zookeeper_path_, MergeTreeDataFormatVersion format_version_, zkutil::ZooKeeper::Ptr zookeeper_, Poco::Logger * log_)\n+std::set<MergeTreePartInfo> StorageReplicatedMergeTree::findReplicaUniqueParts(const String & replica_name_, const String & zookeeper_path_, MergeTreeDataFormatVersion format_version_, zkutil::ZooKeeper::Ptr zookeeper_, LoggerPtr log_)\n {\n     if (!zookeeper_->exists(fs::path(zookeeper_path_) / \"replicas\" / replica_name_ / \"is_active\"))\n     {\n@@ -9364,7 +9364,7 @@ namespace\n /// But sometimes we need an opposite. When we deleting all_0_0_0_1 it can be non replicated to other replicas, so we are the only owner of this part.\n /// In this case when we will drop all_0_0_0_1 we will drop blobs for all_0_0_0. But it will lead to dataloss. For such case we need to check that other replicas\n /// still need parent part.\n-std::pair<bool, NameSet> getParentLockedBlobs(const ZooKeeperWithFaultInjectionPtr & zookeeper_ptr, const std::string & zero_copy_part_path_prefix, const MergeTreePartInfo & part_info, MergeTreeDataFormatVersion format_version, Poco::Logger * log)\n+std::pair<bool, NameSet> getParentLockedBlobs(const ZooKeeperWithFaultInjectionPtr & zookeeper_ptr, const std::string & zero_copy_part_path_prefix, const MergeTreePartInfo & part_info, MergeTreeDataFormatVersion format_version, LoggerPtr log)\n {\n     NameSet files_not_to_remove;\n \n@@ -9455,7 +9455,7 @@ std::pair<bool, NameSet> getParentLockedBlobs(const ZooKeeperWithFaultInjectionP\n std::pair<bool, NameSet> StorageReplicatedMergeTree::unlockSharedDataByID(\n         String part_id, const String & table_uuid, const MergeTreePartInfo & part_info,\n         const String & replica_name_, const std::string & disk_type, const ZooKeeperWithFaultInjectionPtr & zookeeper_ptr, const MergeTreeSettings & settings,\n-        Poco::Logger * logger, const String & zookeeper_path_old, MergeTreeDataFormatVersion data_format_version)\n+        LoggerPtr logger, const String & zookeeper_path_old, MergeTreeDataFormatVersion data_format_version)\n {\n     boost::replace_all(part_id, \"/\", \"_\");\n \n@@ -10143,7 +10143,7 @@ void StorageReplicatedMergeTree::createZeroCopyLockNode(\n                 size_t failed_op = zkutil::getFailedOpIndex(error, responses);\n                 if (ops[failed_op]->getPath() == zookeeper_node)\n                 {\n-                    LOG_WARNING(&Poco::Logger::get(\"ZeroCopyLocks\"), \"Replacing persistent lock with ephemeral for path {}. It can happen only in case of local part loss\", zookeeper_node);\n+                    LOG_WARNING(getLogger(\"ZeroCopyLocks\"), \"Replacing persistent lock with ephemeral for path {}. It can happen only in case of local part loss\", zookeeper_node);\n                     replace_existing_lock = true;\n                     continue;\n                 }\n@@ -10201,7 +10201,7 @@ bool StorageReplicatedMergeTree::removeSharedDetachedPart(DiskPtr disk, const St\n                 detached_replica_name,\n                 disk->getDataSourceDescription().toString(),\n                 std::make_shared<ZooKeeperWithFaultInjection>(zookeeper), local_context->getReplicatedMergeTreeSettings(),\n-                &Poco::Logger::get(\"StorageReplicatedMergeTree\"),\n+                getLogger(\"StorageReplicatedMergeTree\"),\n                 detached_zookeeper_path,\n                 MERGE_TREE_DATA_MIN_FORMAT_VERSION_WITH_CUSTOM_PARTITIONING);\n \ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 3c3c2f56fe2d..c682b1ec88d0 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -255,13 +255,13 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     /** Remove a specific replica from zookeeper.\n      */\n     static void dropReplica(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path, const String & replica,\n-                            Poco::Logger * logger, MergeTreeSettingsPtr table_settings = nullptr, std::optional<bool> * has_metadata_out = nullptr);\n+                            LoggerPtr logger, MergeTreeSettingsPtr table_settings = nullptr, std::optional<bool> * has_metadata_out = nullptr);\n \n-    void dropReplica(const String & drop_zookeeper_path, const String & drop_replica, Poco::Logger * logger);\n+    void dropReplica(const String & drop_zookeeper_path, const String & drop_replica, LoggerPtr logger);\n \n     /// Removes table from ZooKeeper after the last replica was dropped\n     static bool removeTableNodesFromZooKeeper(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path,\n-                                              const zkutil::EphemeralNodeHolder::Ptr & metadata_drop_lock, Poco::Logger * logger);\n+                                              const zkutil::EphemeralNodeHolder::Ptr & metadata_drop_lock, LoggerPtr logger);\n \n     /// Schedules job to execute in background pool (merge, mutate, drop range and so on)\n     bool scheduleDataProcessingJob(BackgroundJobsAssignee & assignee) override;\n@@ -308,7 +308,7 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n         const std::string & disk_type,\n         const ZooKeeperWithFaultInjectionPtr & zookeeper_,\n         const MergeTreeSettings & settings,\n-        Poco::Logger * logger,\n+        LoggerPtr logger,\n         const String & zookeeper_path_old,\n         MergeTreeDataFormatVersion data_format_version);\n \n@@ -773,7 +773,7 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     String findReplicaHavingCoveringPart(LogEntry & entry, bool active);\n     bool findReplicaHavingCoveringPart(const String & part_name, bool active);\n     String findReplicaHavingCoveringPartImplLowLevel(LogEntry * entry, const String & part_name, String & found_part_name, bool active);\n-    static std::set<MergeTreePartInfo> findReplicaUniqueParts(const String & replica_name_, const String & zookeeper_path_, MergeTreeDataFormatVersion format_version_, zkutil::ZooKeeper::Ptr zookeeper_, Poco::Logger * log_);\n+    static std::set<MergeTreePartInfo> findReplicaUniqueParts(const String & replica_name_, const String & zookeeper_path_, MergeTreeDataFormatVersion format_version_, zkutil::ZooKeeper::Ptr zookeeper_, LoggerPtr log_);\n \n     /** Download the specified part from the specified replica.\n       * If `to_detached`, the part is placed in the `detached` directory.\ndiff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp\nindex c376af5a3d7b..1a7704b4d67a 100644\n--- a/src/Storages/StorageS3.cpp\n+++ b/src/Storages/StorageS3.cpp\n@@ -1194,7 +1194,7 @@ void ReadFromStorageS3Step::initializePipeline(QueryPipelineBuilder & pipeline,\n \n     const size_t max_threads = local_context->getSettingsRef().max_threads;\n     const size_t max_parsing_threads = num_streams >= max_threads ? 1 : (max_threads / std::max(num_streams, 1ul));\n-    LOG_DEBUG(&Poco::Logger::get(\"StorageS3\"), \"Reading in {} streams, {} threads per stream\", num_streams, max_parsing_threads);\n+    LOG_DEBUG(getLogger(\"StorageS3\"), \"Reading in {} streams, {} threads per stream\", num_streams, max_parsing_threads);\n \n     Pipes pipes;\n     pipes.reserve(num_streams);\n@@ -1347,7 +1347,7 @@ void StorageS3::truncate(const ASTPtr & /* query */, const StorageMetadataPtr &,\n     }\n \n     for (const auto & error : response.GetResult().GetErrors())\n-        LOG_WARNING(&Poco::Logger::get(\"StorageS3\"), \"Failed to delete {}, error: {}\", error.GetKey(), error.GetMessage());\n+        LOG_WARNING(getLogger(\"StorageS3\"), \"Failed to delete {}, error: {}\", error.GetKey(), error.GetMessage());\n }\n \n StorageS3::Configuration StorageS3::updateConfigurationAndGetCopy(ContextPtr local_context)\ndiff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h\nindex b90a0d394cb2..8d020c5e9a2d 100644\n--- a/src/Storages/StorageS3.h\n+++ b/src/Storages/StorageS3.h\n@@ -242,7 +242,7 @@ class StorageS3Source : public SourceWithKeyCondition, WithContext\n     size_t max_parsing_threads = 1;\n     bool need_only_count;\n \n-    Poco::Logger * log = &Poco::Logger::get(\"StorageS3Source\");\n+    LoggerPtr log = getLogger(\"StorageS3Source\");\n \n     ThreadPool create_reader_pool;\n     ThreadPoolCallbackRunner<ReaderHolder> create_reader_scheduler;\ndiff --git a/src/Storages/StorageS3Cluster.cpp b/src/Storages/StorageS3Cluster.cpp\nindex e1738056e9d7..25c2b42b766f 100644\n--- a/src/Storages/StorageS3Cluster.cpp\n+++ b/src/Storages/StorageS3Cluster.cpp\n@@ -40,7 +40,7 @@ StorageS3Cluster::StorageS3Cluster(\n     const ConstraintsDescription & constraints_,\n     ContextPtr context_,\n     bool structure_argument_was_provided_)\n-    : IStorageCluster(cluster_name_, table_id_, &Poco::Logger::get(\"StorageS3Cluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n+    : IStorageCluster(cluster_name_, table_id_, getLogger(\"StorageS3Cluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n     , s3_configuration{configuration_}\n {\n     context_->getGlobalContext()->getRemoteHostFilter().checkURL(configuration_.url.uri);\ndiff --git a/src/Storages/StorageSQLite.cpp b/src/Storages/StorageSQLite.cpp\nindex d5db5763da91..85c5e16a1bf9 100644\n--- a/src/Storages/StorageSQLite.cpp\n+++ b/src/Storages/StorageSQLite.cpp\n@@ -42,7 +42,7 @@ StorageSQLite::StorageSQLite(\n     , remote_table_name(remote_table_name_)\n     , database_path(database_path_)\n     , sqlite_db(sqlite_db_)\n-    , log(&Poco::Logger::get(\"StorageSQLite (\" + table_id_.table_name + \")\"))\n+    , log(getLogger(\"StorageSQLite (\" + table_id_.table_name + \")\"))\n {\n     StorageInMemoryMetadata storage_metadata;\n \ndiff --git a/src/Storages/StorageSQLite.h b/src/Storages/StorageSQLite.h\nindex 9da040cbd5c5..baacdfb48997 100644\n--- a/src/Storages/StorageSQLite.h\n+++ b/src/Storages/StorageSQLite.h\n@@ -50,7 +50,7 @@ class StorageSQLite final : public IStorage, public WithContext\n     String remote_table_name;\n     String database_path;\n     SQLitePtr sqlite_db;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/StorageSet.cpp b/src/Storages/StorageSet.cpp\nindex 1b0db1da8005..7d7f3113cdbd 100644\n--- a/src/Storages/StorageSet.cpp\n+++ b/src/Storages/StorageSet.cpp\n@@ -218,7 +218,7 @@ void StorageSet::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_sn\n     if (disk->exists(path))\n         disk->removeRecursive(path);\n     else\n-        LOG_INFO(&Poco::Logger::get(\"StorageSet\"), \"Path {} is already removed from disk {}\", path, disk->getName());\n+        LOG_INFO(getLogger(\"StorageSet\"), \"Path {} is already removed from disk {}\", path, disk->getName());\n \n     disk->createDirectories(path);\n     disk->createDirectories(fs::path(path) / \"tmp/\");\n@@ -284,7 +284,7 @@ void StorageSetOrJoinBase::restoreFromFile(const String & file_path)\n     finishInsert();\n \n     /// TODO Add speed, compressed bytes, data volume in memory, compression ratio ... Generalize all statistics logging in project.\n-    LOG_INFO(&Poco::Logger::get(\"StorageSetOrJoinBase\"), \"Loaded from backup file {}. {} rows, {}. State has {} unique rows.\",\n+    LOG_INFO(getLogger(\"StorageSetOrJoinBase\"), \"Loaded from backup file {}. {} rows, {}. State has {} unique rows.\",\n         file_path, info.rows, ReadableSize(info.bytes), getSize(ctx));\n }\n \ndiff --git a/src/Storages/StorageStripeLog.cpp b/src/Storages/StorageStripeLog.cpp\nindex 91f6246d1018..359f142949f2 100644\n--- a/src/Storages/StorageStripeLog.cpp\n+++ b/src/Storages/StorageStripeLog.cpp\n@@ -277,7 +277,7 @@ StorageStripeLog::StorageStripeLog(\n     , index_file_path(table_path + \"index.mrk\")\n     , file_checker(disk, table_path + \"sizes.json\")\n     , max_compress_block_size(context_->getSettings().max_compress_block_size)\n-    , log(&Poco::Logger::get(\"StorageStripeLog\"))\n+    , log(getLogger(\"StorageStripeLog\"))\n {\n     StorageInMemoryMetadata storage_metadata;\n     storage_metadata.setColumns(columns_);\ndiff --git a/src/Storages/StorageStripeLog.h b/src/Storages/StorageStripeLog.h\nindex a05117a9ad59..c7f3e7e21e6d 100644\n--- a/src/Storages/StorageStripeLog.h\n+++ b/src/Storages/StorageStripeLog.h\n@@ -123,7 +123,7 @@ friend class StripeLogSink;\n \n     mutable std::shared_timed_mutex rwlock;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/StorageURL.cpp b/src/Storages/StorageURL.cpp\nindex 0ba72af6fc08..631c06bd87b2 100644\n--- a/src/Storages/StorageURL.cpp\n+++ b/src/Storages/StorageURL.cpp\n@@ -1284,7 +1284,7 @@ StorageURLWithFailover::StorageURLWithFailover(\n     {\n         Poco::URI poco_uri(uri_option);\n         context_->getRemoteHostFilter().checkURL(poco_uri);\n-        LOG_DEBUG(&Poco::Logger::get(\"StorageURLDistributed\"), \"Adding URL option: {}\", uri_option);\n+        LOG_DEBUG(getLogger(\"StorageURLDistributed\"), \"Adding URL option: {}\", uri_option);\n         uri_options.emplace_back(uri_option);\n     }\n }\ndiff --git a/src/Storages/StorageURLCluster.cpp b/src/Storages/StorageURLCluster.cpp\nindex a0b5fcd6f285..2365887983d7 100644\n--- a/src/Storages/StorageURLCluster.cpp\n+++ b/src/Storages/StorageURLCluster.cpp\n@@ -45,7 +45,7 @@ StorageURLCluster::StorageURLCluster(\n     const ConstraintsDescription & constraints_,\n     const StorageURL::Configuration & configuration_,\n     bool structure_argument_was_provided_)\n-    : IStorageCluster(cluster_name_, table_id_, &Poco::Logger::get(\"StorageURLCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n+    : IStorageCluster(cluster_name_, table_id_, getLogger(\"StorageURLCluster (\" + table_id_.table_name + \")\"), structure_argument_was_provided_)\n     , uri(uri_)\n {\n     context_->getRemoteHostFilter().checkURL(Poco::URI(uri));\ndiff --git a/src/Storages/StorageXDBC.cpp b/src/Storages/StorageXDBC.cpp\nindex a274b1ba4db8..259abefb00fd 100644\n--- a/src/Storages/StorageXDBC.cpp\n+++ b/src/Storages/StorageXDBC.cpp\n@@ -45,7 +45,7 @@ StorageXDBC::StorageXDBC(\n     , bridge_helper(bridge_helper_)\n     , remote_database_name(remote_database_name_)\n     , remote_table_name(remote_table_name_)\n-    , log(&Poco::Logger::get(\"Storage\" + bridge_helper->getName()))\n+    , log(getLogger(\"Storage\" + bridge_helper->getName()))\n {\n     uri = bridge_helper->getMainURI().toString();\n }\ndiff --git a/src/Storages/StorageXDBC.h b/src/Storages/StorageXDBC.h\nindex fe678785dc28..cba15a832267 100644\n--- a/src/Storages/StorageXDBC.h\n+++ b/src/Storages/StorageXDBC.h\n@@ -47,7 +47,7 @@ class StorageXDBC : public IStorageURLBase\n     std::string remote_database_name;\n     std::string remote_table_name;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::string getReadMethod() const override;\n \ndiff --git a/src/Storages/System/StorageSystemDatabases.cpp b/src/Storages/System/StorageSystemDatabases.cpp\nindex 0ffed6c97718..51ecb8f17cae 100644\n--- a/src/Storages/System/StorageSystemDatabases.cpp\n+++ b/src/Storages/System/StorageSystemDatabases.cpp\n@@ -54,7 +54,7 @@ static String getEngineFull(const ContextPtr & ctx, const DatabasePtr & database\n             return {};\n \n         guard.reset();\n-        LOG_TRACE(&Poco::Logger::get(\"StorageSystemDatabases\"), \"Failed to lock database {} ({}), will retry\", name, database->getUUID());\n+        LOG_TRACE(getLogger(\"StorageSystemDatabases\"), \"Failed to lock database {} ({}), will retry\", name, database->getUUID());\n     }\n \n     ASTPtr ast = database->getCreateDatabaseQuery();\ndiff --git a/src/Storages/System/StorageSystemJemalloc.cpp b/src/Storages/System/StorageSystemJemalloc.cpp\nindex 9c3a075b2c15..15543208dd9e 100644\n--- a/src/Storages/System/StorageSystemJemalloc.cpp\n+++ b/src/Storages/System/StorageSystemJemalloc.cpp\n@@ -77,7 +77,7 @@ void fillJemallocBins(MutableColumns & res_columns)\n \n void fillJemallocBins(MutableColumns &)\n {\n-    LOG_INFO(&Poco::Logger::get(\"StorageSystemJemallocBins\"), \"jemalloc is not enabled\");\n+    LOG_INFO(getLogger(\"StorageSystemJemallocBins\"), \"jemalloc is not enabled\");\n }\n \n #endif // USE_JEMALLOC\ndiff --git a/src/Storages/System/StorageSystemReplicas.cpp b/src/Storages/System/StorageSystemReplicas.cpp\nindex d9a120954434..eeb3db342b49 100644\n--- a/src/Storages/System/StorageSystemReplicas.cpp\n+++ b/src/Storages/System/StorageSystemReplicas.cpp\n@@ -56,12 +56,12 @@ class StatusRequestsPool\n     /// Used to assign unique incremental ids to requests.\n     UInt64 request_id TSA_GUARDED_BY(mutex) = 0;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n public:\n     explicit StatusRequestsPool(size_t max_threads)\n         : thread_pool(CurrentMetrics::SystemReplicasThreads, CurrentMetrics::SystemReplicasThreadsActive, CurrentMetrics::SystemReplicasThreadsScheduled, max_threads)\n-        , log(&Poco::Logger::get(\"StatusRequestsPool\"))\n+        , log(getLogger(\"StatusRequestsPool\"))\n     {}\n \n     ~StatusRequestsPool()\ndiff --git a/src/Storages/System/StorageSystemStackTrace.cpp b/src/Storages/System/StorageSystemStackTrace.cpp\nindex e02d4bf1733b..b17d04e98955 100644\n--- a/src/Storages/System/StorageSystemStackTrace.cpp\n+++ b/src/Storages/System/StorageSystemStackTrace.cpp\n@@ -173,7 +173,7 @@ bool wait(int timeout_ms)\n }\n \n using ThreadIdToName = std::unordered_map<UInt64, String, DefaultHash<UInt64>>;\n-ThreadIdToName getFilteredThreadNames(const ActionsDAG::Node * predicate, ContextPtr context, const PaddedPODArray<UInt64> & thread_ids, Poco::Logger * log)\n+ThreadIdToName getFilteredThreadNames(const ActionsDAG::Node * predicate, ContextPtr context, const PaddedPODArray<UInt64> & thread_ids, LoggerPtr log)\n {\n     ThreadIdToName tid_to_name;\n     MutableColumnPtr all_thread_names = ColumnString::create();\n@@ -274,7 +274,7 @@ bool isSignalBlocked(UInt64 tid, int signal)\n class StackTraceSource : public ISource\n {\n public:\n-    StackTraceSource(const Names & column_names, Block header_, ASTPtr && query_, ActionsDAGPtr && filter_dag_, ContextPtr context_, UInt64 max_block_size_, Poco::Logger * log_)\n+    StackTraceSource(const Names & column_names, Block header_, ASTPtr && query_, ActionsDAGPtr && filter_dag_, ContextPtr context_, UInt64 max_block_size_, LoggerPtr log_)\n         : ISource(header_)\n         , context(context_)\n         , header(std::move(header_))\n@@ -426,7 +426,7 @@ class StackTraceSource : public ISource\n     bool send_signal = false;\n     bool read_thread_names = false;\n \n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     std::filesystem::directory_iterator proc_it;\n     std::filesystem::directory_iterator end;\n@@ -481,7 +481,7 @@ class ReadFromSystemStackTrace : public SourceStepWithFilter\n         ASTPtr && query_,\n         ContextPtr context_,\n         size_t max_block_size_,\n-        Poco::Logger * log_)\n+        LoggerPtr log_)\n         : SourceStepWithFilter(DataStream{.header = std::move(sample_block)})\n         , column_names(column_names_)\n         , query(query_)\n@@ -496,7 +496,7 @@ class ReadFromSystemStackTrace : public SourceStepWithFilter\n     ASTPtr query;\n     ContextPtr context;\n     size_t max_block_size;\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\n@@ -504,7 +504,7 @@ class ReadFromSystemStackTrace : public SourceStepWithFilter\n \n StorageSystemStackTrace::StorageSystemStackTrace(const StorageID & table_id_)\n     : IStorage(table_id_)\n-    , log(&Poco::Logger::get(\"StorageSystemStackTrace\"))\n+    , log(getLogger(\"StorageSystemStackTrace\"))\n {\n     StorageInMemoryMetadata storage_metadata;\n     storage_metadata.setColumns(ColumnsDescription({\ndiff --git a/src/Storages/System/StorageSystemStackTrace.h b/src/Storages/System/StorageSystemStackTrace.h\nindex 18216cea1bd3..ce1b7f8ccd2a 100644\n--- a/src/Storages/System/StorageSystemStackTrace.h\n+++ b/src/Storages/System/StorageSystemStackTrace.h\n@@ -38,7 +38,7 @@ class StorageSystemStackTrace final : public IStorage\n     bool isSystemStorage() const override { return true; }\n \n protected:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n };\n \n }\ndiff --git a/src/Storages/UVLoop.h b/src/Storages/UVLoop.h\nindex 4945e1b56fac..dd1d64973d12 100644\n--- a/src/Storages/UVLoop.h\n+++ b/src/Storages/UVLoop.h\n@@ -63,7 +63,7 @@ class UVLoop : public boost::noncopyable\n \n private:\n     std::unique_ptr<uv_loop_t> loop_ptr;\n-    Poco::Logger * log = &Poco::Logger::get(\"UVLoop\");\n+    LoggerPtr log = getLogger(\"UVLoop\");\n \n     static void onUVWalkClosingCallback(uv_handle_t * handle, void *)\n     {\ndiff --git a/src/Storages/WindowView/StorageWindowView.cpp b/src/Storages/WindowView/StorageWindowView.cpp\nindex f9ba8e9717f9..0764685cb07a 100644\n--- a/src/Storages/WindowView/StorageWindowView.cpp\n+++ b/src/Storages/WindowView/StorageWindowView.cpp\n@@ -1160,7 +1160,7 @@ StorageWindowView::StorageWindowView(\n     bool attach_)\n     : IStorage(table_id_)\n     , WithContext(context_->getGlobalContext())\n-    , log(&Poco::Logger::get(fmt::format(\"StorageWindowView({}.{})\", table_id_.database_name, table_id_.table_name)))\n+    , log(getLogger(fmt::format(\"StorageWindowView({}.{})\", table_id_.database_name, table_id_.table_name)))\n     , fire_signal_timeout_s(context_->getSettingsRef().wait_for_window_view_fire_signal_timeout.totalSeconds())\n     , clean_interval_usec(context_->getSettingsRef().window_view_clean_interval.totalMicroseconds())\n {\ndiff --git a/src/Storages/WindowView/StorageWindowView.h b/src/Storages/WindowView/StorageWindowView.h\nindex de8f880c6022..969fda8f78e2 100644\n--- a/src/Storages/WindowView/StorageWindowView.h\n+++ b/src/Storages/WindowView/StorageWindowView.h\n@@ -177,7 +177,7 @@ class StorageWindowView final : public IStorage, WithContext\n     const Block & getOutputHeader() const;\n \n private:\n-    Poco::Logger * log;\n+    LoggerPtr log;\n \n     /// Stored query, e.g. SELECT * FROM * GROUP BY tumble(now(), *)\n     ASTPtr select_query;\ndiff --git a/src/TableFunctions/Hive/TableFunctionHive.cpp b/src/TableFunctions/Hive/TableFunctionHive.cpp\nindex d88850875324..e840d5fc8bee 100644\n--- a/src/TableFunctions/Hive/TableFunctionHive.cpp\n+++ b/src/TableFunctions/Hive/TableFunctionHive.cpp\n@@ -46,7 +46,7 @@ class TableFunctionHive : public ITableFunction\n     void parseArguments(const ASTPtr & ast_function_, ContextPtr context_) override;\n \n private:\n-    Poco::Logger * logger = &Poco::Logger::get(\"TableFunctionHive\");\n+    LoggerPtr logger = getLogger(\"TableFunctionHive\");\n \n     String cluster_name;\n     String hive_metastore_url;\n",
  "test_patch": "diff --git a/src/Common/tests/gtest_log.cpp b/src/Common/tests/gtest_log.cpp\nindex e755c22ba75b..419aac370d6a 100644\n--- a/src/Common/tests/gtest_log.cpp\n+++ b/src/Common/tests/gtest_log.cpp\n@@ -15,7 +15,7 @@ TEST(Logger, Log)\n {\n     Poco::Logger::root().setLevel(\"none\");\n     Poco::Logger::root().setChannel(Poco::AutoPtr<Poco::NullChannel>(new Poco::NullChannel()));\n-    Poco::Logger * log = &Poco::Logger::get(\"Log\");\n+    LoggerPtr log = getLogger(\"Log\");\n \n     /// This test checks that we don't pass this string to fmtlib, because it is the only argument.\n     EXPECT_NO_THROW(LOG_INFO(log, fmt::runtime(\"Hello {} World\")));\n@@ -27,7 +27,7 @@ TEST(Logger, TestLog)\n \n         std::ostringstream oss; // STYLE_CHECK_ALLOW_STD_STRING_STREAM\n         auto my_channel = Poco::AutoPtr<Poco::StreamChannel>(new Poco::StreamChannel(oss));\n-        auto * log = &Poco::Logger::create(\"TestLogger\", my_channel.get());\n+        auto log = createLogger(\"TestLogger\", my_channel.get());\n         log->setLevel(\"test\");\n         LOG_TEST(log, \"Hello World\");\n \n@@ -40,7 +40,7 @@ TEST(Logger, TestLog)\n         {\n             std::ostringstream oss; // STYLE_CHECK_ALLOW_STD_STRING_STREAM\n             auto my_channel = Poco::AutoPtr<Poco::StreamChannel>(new Poco::StreamChannel(oss));\n-            auto * log = &Poco::Logger::create(std::string{level} + \"_Logger\", my_channel.get());\n+            auto log = createLogger(std::string{level} + \"_Logger\", my_channel.get());\n             log->setLevel(level);\n             LOG_TEST(log, \"Hello World\");\n \n@@ -84,7 +84,7 @@ TEST(Logger, SideEffects)\n {\n     std::ostringstream oss; // STYLE_CHECK_ALLOW_STD_STRING_STREAM\n     auto my_channel = Poco::AutoPtr<Poco::StreamChannel>(new Poco::StreamChannel(oss));\n-    auto * log = &Poco::Logger::create(\"Logger\", my_channel.get());\n+    auto log = createLogger(\"Logger\", my_channel.get());\n     log->setLevel(\"trace\");\n \n     /// Ensure that parameters are evaluated only once\ndiff --git a/src/Common/tests/gtest_poolbase.cpp b/src/Common/tests/gtest_poolbase.cpp\nindex 20c3281c9646..879b1b166204 100644\n--- a/src/Common/tests/gtest_poolbase.cpp\n+++ b/src/Common/tests/gtest_poolbase.cpp\n@@ -18,7 +18,7 @@ class MyPoolBase : public PoolBase<PoolObject>\n     using Ptr = PoolBase<PoolObject>::Ptr;\n \n     int last_destroy_value = 0;\n-    MyPoolBase() : PoolBase<PoolObject>(100, &Poco::Logger::get(\"MyPoolBase\")) { }\n+    MyPoolBase() : PoolBase<PoolObject>(100, getLogger(\"MyPoolBase\")) { }\n \n protected:\n     ObjectPtr allocObject() override { return std::make_shared<Object>(); }\ndiff --git a/src/Coordination/tests/gtest_coordination.cpp b/src/Coordination/tests/gtest_coordination.cpp\nindex c981085359e2..59a550177a46 100644\n--- a/src/Coordination/tests/gtest_coordination.cpp\n+++ b/src/Coordination/tests/gtest_coordination.cpp\n@@ -66,7 +66,7 @@ class CoordinationTest : public ::testing::TestWithParam<CompressionParam>\n {\n protected:\n     DB::KeeperContextPtr keeper_context = std::make_shared<DB::KeeperContext>(true);\n-    Poco::Logger * log{&Poco::Logger::get(\"CoordinationTest\")};\n+    LoggerPtr log{getLogger(\"CoordinationTest\")};\n \n     void SetUp() override\n     {\n@@ -1101,7 +1101,7 @@ TEST_P(CoordinationTest, ChangelogTestReadAfterBrokenTruncate2)\n }\n \n /// Truncating only some entries from the end\n-/// For compressed logs we have no reliable way of knowing how many log entries were lost \n+/// For compressed logs we have no reliable way of knowing how many log entries were lost\n /// after we truncate some bytes from the end\n TEST_F(CoordinationTest, ChangelogTestReadAfterBrokenTruncate3)\n {\n@@ -1801,7 +1801,7 @@ void testLogAndStateMachine(\n                 = [&snapshot_created](bool & ret, nuraft::ptr<std::exception> & /*exception*/)\n             {\n                 snapshot_created = ret;\n-                LOG_INFO(&Poco::Logger::get(\"CoordinationTest\"), \"Snapshot finished\");\n+                LOG_INFO(getLogger(\"CoordinationTest\"), \"Snapshot finished\");\n             };\n \n             state_machine->create_snapshot(s, when_done);\n",
  "problem_statement": "Tracking the memory leak problem\nAfter I upgrade clickhouse from 21.8 to 22.8.21.38 lts,I encountered memory leak.The clickhouse process memory keep growing observably about 1GB per day,and release memory after restart server.\r\nI had checked all metrics about clickhouse follow this: [who-ate-my-memory](https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-who-ate-my-memory/)\r\n``` text\r\ncaches\r\nprimary keys in memory\r\nmerge memory\r\nmemory engines\r\n``` \r\nAll those matrics are have low memory useage.Then I found jemalloc memory was similar to clickhouse instance memory,those are three jemalloc metrics:\r\n``` text\r\njemalloc.resident\r\njemalloc.active\r\njemalloc.allocated\r\n```\r\nSo,I try to gather the clickhouse memory using jemalloc profiling tools in my test environment.\r\nTurn on jemalloc profile and start clickhouse like this:\r\n``` bash\r\nexport MALLOC_CONF=background_thread:true,prof:true,prof_leak:true,lg_prof_sample:17,lg_prof_interval:32,prof_final:true,prof_prefix:/data/jemalloc/jeprof\r\nexport CLICKHOUSE_WATCHDOG_ENABLE=0\r\n\r\nnohup /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml &\r\n```\r\n\r\nThen show memory with jeprof tool, the two heap files have 24 hours interval:\r\n``` bash\r\n/usr/local/jemalloc-521/bin/jeprof /usr/bin/clickhouse --base=/data/jemalloc/jeprof.36298.2172.i2172.heap /data/jemalloc/jeprof.36298.15994.i15994.heap\r\n```\r\nthe top command output:\r\n``` text\r\n(jeprof) top\r\nTotal: 272.1 MB\r\n    99.4  36.5%  36.5%    105.3  38.7% std::__1::__tree::__emplace_unique_key_args\r\n    98.0  36.0%  72.6%    198.2  72.8% Poco::Logger::unsafeGet\r\n    19.4   7.1%  79.7%     56.2  20.6% DB::SerializationArray::enumerateStreams\r\n    16.3   6.0%  85.7%    -19.5  -7.2% boost::multi_index::detail::ordered_index_impl::insert_\r\n    13.0   4.8%  90.4%     16.0   5.9% boost::multi_index::detail::hashed_index::insert_\r\n    11.3   4.1%  94.6%     19.2   7.0% std::__1::__hash_table::__emplace_unique_key_args\r\n     7.0   2.6%  97.1%     35.1  12.9% std::__1::__hash_table::__rehash\r\n     5.5   2.0%  99.2%      5.6   2.1% std::__1::vector::__emplace_back_slow_path\r\n     4.9   1.8% 101.0%      6.9   2.5% DB::DataTypeArray::doGetDefaultSerialization\r\n     3.0   1.1% 102.1%      3.0   1.1% boost::multi_index::detail::hashed_index::unchecked_rehash\r\n```\r\nAccording to the output, Is it possible to state that these two lines leak memory?\r\n``` text\r\n    99.4  36.5%  36.5%    105.3  38.7% std::__1::__tree::__emplace_unique_key_args\r\n    98.0  36.0%  72.6%    198.2  72.8% Poco::Logger::unsafeGet\r\n```\r\nAnd how to do it?\r\n\nSuspected memory leak in 23.8\nMigrated from https://github.com/ClickHouse/ClickHouse/issues/54483 as a separate issue, since we are not using Kafka.\r\n\r\n**Describe the unexpected behaviour**\r\n\r\nUsing ClickHouse 23.8.2.7. Memory increases with database usage, apparently a memory leak. After restarting memory is reduced drastically, then starts climbing up again at around 300 MB / day.\r\n\r\n![image](https://github.com/ClickHouse/ClickHouse/assets/876570/80481310-be85-4ddd-b879-cfadd91bbd48)\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: ClickHouse 23.8.2.7\r\n* One particular database has a somewhat non-standard usage: it receives a lot of inserts but they usually last for less than a minute, it is a sort of internal cache. There we see memory usage raising at around 700 MB / day. But we see the same effect in other ClickHouse servers with the same version, less pronounced (300 MB / day). Memory is always released upon restart.\r\n\r\n**Expected behavior**\r\n\r\nSince no new data is being stored we would expect that the database should recycle memory internally without having to restart it.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\r\nAll queries run on a particular instance that is using ~19 GB right now. Merges, as requested by @antaljanosbenjamin:\r\n```\r\nSELECT\r\n    formatReadableSize(memory_usage),\r\n    *\r\nFROM system.merges\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.001 sec. \r\n```\r\n\r\nMemory stats, as requested by @azat:\r\n```\r\nWITH\r\n    (\r\n        SELECT sum(CAST(value, 'UInt64'))\r\n        FROM system.metrics\r\n        WHERE metric = 'MemoryTracking'\r\n    ) AS memory_tracked_,\r\n    (\r\n        SELECT sum(total_bytes)\r\n        FROM system.tables\r\n        WHERE engine IN ('Join', 'Memory', 'Buffer', 'Set')\r\n    ) AS memory_tables_,\r\n    (\r\n        SELECT sum(CAST(value, 'UInt64'))\r\n        FROM system.asynchronous_metrics\r\n        WHERE metric LIKE '%CacheBytes'\r\n    ) AS memory_caches_,\r\n    (\r\n        SELECT sum(CAST(memory_usage, 'UInt64'))\r\n        FROM system.processes\r\n    ) AS memory_processes_,\r\n    (\r\n        SELECT sum(CAST(memory_usage, 'UInt64'))\r\n        FROM system.merges\r\n    ) AS memory_merges_,\r\n    (\r\n        SELECT sum(bytes_allocated)\r\n        FROM system.dictionaries\r\n    ) AS memory_dictionaries_,\r\n    (\r\n        SELECT sum(primary_key_bytes_in_memory_allocated)\r\n        FROM system.parts\r\n    ) AS memory_primary_keys_\r\nSELECT\r\n    assumeNotNull(memory_tracked_) AS memory_tracked,\r\n    assumeNotNull(memory_tables_) AS memory_tables,\r\n    assumeNotNull(memory_caches_) AS memory_caches,\r\n    assumeNotNull(memory_processes_) AS memory_processes,\r\n    assumeNotNull(memory_merges_) AS memory_merges,\r\n    assumeNotNull(memory_dictionaries_) AS memory_dictionaries,\r\n    assumeNotNull(memory_primary_keys_) AS memory_primary_keys\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nmemory_tracked:      19684595772\r\nmemory_tables:       0\r\nmemory_caches:       4538334048\r\nmemory_processes:    175800\r\nmemory_merges:       0\r\nmemory_dictionaries: 0\r\nmemory_primary_keys: 2427744\r\n```\r\n\r\nand more:\r\n```\r\nSELECT\r\n    metric,\r\n    value\r\nFROM system.asynchronous_metrics\r\nWHERE metric LIKE '%jemalloc%'\r\n\r\n\u250c\u2500metric\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500value\u2500\u2510\r\n\u2502 jemalloc.epoch                           \u2502      1507330 \u2502\r\n\u2502 jemalloc.arenas.all.pactive              \u2502      4466147 \u2502\r\n\u2502 jemalloc.background_thread.num_runs      \u2502            0 \u2502\r\n\u2502 jemalloc.active                          \u2502  18293338112 \u2502\r\n\u2502 jemalloc.retained                        \u2502 286359556096 \u2502\r\n\u2502 jemalloc.arenas.all.pmuzzy               \u2502         3687 \u2502\r\n\u2502 jemalloc.allocated                       \u2502  18102287104 \u2502\r\n\u2502 jemalloc.background_thread.num_threads   \u2502            0 \u2502\r\n\u2502 jemalloc.metadata_thp                    \u2502            0 \u2502\r\n\u2502 jemalloc.arenas.all.muzzy_purged         \u2502   6963334478 \u2502\r\n\u2502 jemalloc.arenas.all.dirty_purged         \u2502  11647316965 \u2502\r\n\u2502 jemalloc.mapped                          \u2502  19758575616 \u2502\r\n\u2502 jemalloc.metadata                        \u2502   1241636848 \u2502\r\n\u2502 jemalloc.resident                        \u2502  19578085376 \u2502\r\n\u2502 jemalloc.background_thread.run_intervals \u2502            0 \u2502\r\n\u2502 jemalloc.arenas.all.pdirty               \u2502        14069 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nMemory allocated:\r\n\r\n```\r\nSELECT\r\n    *,\r\n    allocations - deallocations AS active_allocations,\r\n    size * active_allocations AS allocated_bytes\r\nFROM system.jemalloc_bins\r\nWHERE allocated_bytes > 0\r\nORDER BY allocated_bytes DESC\r\n\r\n\u250c\u2500index\u2500\u252c\u2500large\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500size\u2500\u252c\u2500allocations\u2500\u252c\u2500deallocations\u2500\u252c\u2500active_allocations\u2500\u252c\u2500allocated_bytes\u2500\u2510\r\n\u2502    12 \u2502     0 \u2502       256 \u2502  1964164752 \u2502    1934402605 \u2502           29762147 \u2502      7619109632 \u2502\r\n\u2502     4 \u2502     0 \u2502        64 \u2502  8765929776 \u2502    8725048938 \u2502           40880838 \u2502      2616373632 \u2502\r\n\u2502     5 \u2502     0 \u2502        80 \u2502  2965546952 \u2502    2933024532 \u2502           32522420 \u2502      2601793600 \u2502\r\n\u2502    11 \u2502     0 \u2502       224 \u2502   850928924 \u2502     839491604 \u2502           11437320 \u2502      2561959680 \u2502\r\n\u2502     2 \u2502     0 \u2502        32 \u2502  7006726704 \u2502    6965106953 \u2502           41619751 \u2502      1331832032 \u2502\r\n\u2502    91 \u2502     1 \u2502 234881024 \u2502           1 \u2502             0 \u2502                  1 \u2502       234881024 \u2502\r\n\u2502     6 \u2502     0 \u2502        96 \u2502  5682488087 \u2502    5680410878 \u2502            2077209 \u2502       199412064 \u2502\r\n\u2502     7 \u2502     0 \u2502       112 \u2502  1402413732 \u2502    1401220491 \u2502            1193241 \u2502       133642992 \u2502\r\n[...]\r\n```\r\n\r\nPlease let us know what else we can do to diagnose the issue. Thanks!\n",
  "hints_text": "\n 23.8.2.7 is obsolete. Upgrade to 23.8.8.20.\r\n\r\n1 What protocol do you use for inserts and for selects? \r\n2 What format do you use for Inserts, for Selects? JSON/TSV/CSV ? \r\n3 Does it stops to grow if you pause inserts? selects?\r\n4 Do you use mutations? TTL? How do you delete data? \r\n5 Do you use Engine=Buffer?\r\n6 Do you use insertion into Distributed tables?\r\n7 Do you use query cache?\r\n8 Provide\r\n```\r\nselect name, value from system.server_settings where changed;\r\n\r\nselect name, value from system.merge_tree_settings where changed;\r\n\r\nselect name, value from system.settings where changed;\r\n\r\nselect distinct type,  compression_codec from system.columns where database not in ('system','INFORMATION_SCHEMA','information_schema') ;\r\n```\r\n\r\n\n9 What are the engines of the tables?\nYou are on 23.8, so you don't have https://github.com/ClickHouse/ClickHouse/pull/52792, than you also need to provide `MMappedAllocs` and `MMappedAllocBytes` metrics (`system.metrics`)\r\n\r\nAlso please provide the previous version of ClickHouse.\nAlso please provide the following:\r\n- RSS/VIRT of process (`grep Vm /proc/$(pgrep clickhouse-serv)/status`)\r\n- and cgroup stats (`cat /sys/fs/cgroup/$(cut -d: -f3 /proc/$(pgrep clickhouse-serv)/cgroup)/memory.stat`) _I'm assuming that you do have running server via systemd, then you will have separate cgroup_\n> You are on 23.8, so you don't have #52792, than you also need to provide `MMappedAllocs` and `MMappedAllocBytes` metrics (`system.metrics`)\r\n\r\nThere you go:\r\n\r\n```\r\nSELECT *\r\nFROM system.metrics\r\nWHERE metric LIKE 'MMap%'\r\n\r\n\u250c\u2500metric\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500value\u2500\u252c\u2500description\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 MMappedFiles      \u2502         1 \u2502 Total number of mmapped files.      \u2502\r\n\u2502 MMappedFileBytes  \u2502 519114224 \u2502 Sum size of mmapped file regions.   \u2502\r\n\u2502 MMappedAllocs     \u2502         0 \u2502 Total number of mmapped allocations \u2502\r\n\u2502 MMappedAllocBytes \u2502         0 \u2502 Sum bytes of mmapped allocations    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n> Also please provide the previous version of ClickHouse.\r\n\r\nI don't understand, what previous version? We are running 23.8.2.7:\r\n\r\n```\r\nSELECT *\r\nFROM system.metrics\r\nWHERE metric LIKE '%Version%'\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nmetric:      VersionInteger\r\nvalue:       23008002\r\n```\n> Also please provide the following:\r\n\r\nSure!\r\n \r\n> * RSS/VIRT of process (`grep Vm /proc/$(pgrep clickhouse-serv)/status`)\r\n\r\n```\r\n$ grep Vm /proc/$(pgrep clickhouse-serv)/status\r\nVmPeak:\t346043060 kB\r\nVmSize:\t346043060 kB\r\nVmLck:\t  291852 kB\r\nVmPin:\t       0 kB\r\nVmHWM:\t21179412 kB\r\nVmRSS:\t20843156 kB\r\nVmData:\t345133216 kB\r\nVmStk:\t     132 kB\r\nVmExe:\t  291852 kB\r\nVmLib:\t    2320 kB\r\nVmPTE:\t  614420 kB\r\nVmSwap:\t       0 kB\r\n```\r\n\r\n> * and cgroup stats (`cat /sys/fs/cgroup/$(cut -d: -f3 /proc/$(pgrep clickhouse-serv)/cgroup)/memory.stat`) _I'm assuming that you do have running server via systemd, then you will have separate cgroup_\r\n\r\nIndeed:\r\n\r\n```\r\n$ cat /sys/fs/cgroup/$(cut -d: -f3 /proc/$(pgrep clickhouse-serv)/cgroup)/memory.stat\r\nanon 21035593728\r\nfile 6102999040\r\nkernel 1793294336\r\nkernel_stack 8339456\r\npagetables 629751808\r\nsec_pagetables 0\r\npercpu 192\r\nsock 4096\r\nvmalloc 0\r\nshmem 4096\r\nzswap 0\r\nzswapped 0\r\nfile_mapped 9252864\r\nfile_dirty 7569408\r\nfile_writeback 0\r\nswapcached 0\r\nanon_thp 0\r\nfile_thp 0\r\nshmem_thp 0\r\ninactive_anon 18534170624\r\nactive_anon 1193799680\r\ninactive_file 5584982016\r\nactive_file 1825579008\r\nunevictable 0\r\nslab_reclaimable 1150291392\r\nslab_unreclaimable 4869232\r\nslab 1155160624\r\nworkingset_refault_anon 0\r\nworkingset_refault_file 4517315\r\nworkingset_activate_anon 0\r\nworkingset_activate_file 1278646\r\nworkingset_restore_anon 0\r\nworkingset_restore_file 143066\r\nworkingset_nodereclaim 0\r\npgscan 26988711\r\npgsteal 24160345\r\npgscan_kswapd 26978814\r\npgscan_direct 9897\r\npgscan_khugepaged 0\r\npgsteal_kswapd 24150448\r\npgsteal_direct 9897\r\npgsteal_khugepaged 0\r\npgfault 2720763532\r\npgmajfault 791\r\npgrefill 3463429\r\npgactivate 23117682\r\npgdeactivate 2373233\r\npglazyfree 2574598934\r\npglazyfreed 744\r\nzswpin 0\r\nzswpout 0\r\nthp_fault_alloc 0\r\nthp_collapse_alloc 0\r\n```\r\n\r\nHope you can see anything there because to me this output is a big mess :sweat_smile: \n>I don't understand, what previous version? We are running 23.8.2.7:\r\n\r\nYou sad that this issue pops up after upgrade, so the question is, what version you had prio upgrade?\n> 23.8.2.7 is obsolete. Upgrade to 23.8.8.20.\r\n\r\nThis is not possible right now. We have seen the same behavior with 23.10.3.5, is that version OK? We will upgrade to 23.10.3.5 as soon as we can.\r\n\r\n> 1 What protocol do you use for inserts and for selects?\r\n\r\nWe use HTTP for both.\r\n\r\n> 2 What format do you use for Inserts, for Selects? JSON/TSV/CSV ?\r\n\r\nWe use row binary for inserts, JSON for selects.\r\n\r\n> 3 Does it stops to grow if you pause inserts? selects?\r\n\r\nWe have not tried because it would mean pausing the instances for several days. But we do know that we see the same memory growth in the instance where we insert than on the read replica.\r\n\r\n> 4 Do you use mutations? TTL? How do you delete data?\r\n\r\nWe use no mutations or TTL on this instance. We delete data using `drop table`.\r\n\r\n> 5 Do you use Engine=Buffer?\r\n\r\nNo.\r\n\r\n> 6 Do you use insertion into Distributed tables?>\r\n\r\nNo.\r\n\r\n 7 Do you use query cache?\r\n\r\nNo.\r\n\r\n> 8 Provide\r\n> \r\n> select name, value from system.server_settings where changed;\r\n\r\n```\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.server_settings\r\nWHERE changed\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 shutdown_wait_unfinished_queries     \u2502 1          \u2502\r\n\u2502 max_connections                      \u2502 4096       \u2502\r\n\u2502 default_database                     \u2502 default    \u2502\r\n\u2502 max_server_memory_usage_to_ram_ratio \u2502 0.9        \u2502\r\n\u2502 max_concurrent_queries               \u2502 100        \u2502\r\n\u2502 uncompressed_cache_size              \u2502 8589934592 \u2502\r\n\u2502 mark_cache_size                      \u2502 5368709120 \u2502\r\n\u2502 background_pool_size                 \u2502 16         \u2502\r\n\u2502 background_fetches_pool_size         \u2502 8          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n> select name, value from system.merge_tree_settings where changed;\r\n\r\n```\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.merge_tree_settings\r\nWHERE changed\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 ratio_of_defaults_for_sparse_serialization       \u2502 1           \u2502\r\n\u2502 max_bytes_to_merge_at_max_space_in_pool          \u2502 53687091200 \u2502\r\n\u2502 max_suspicious_broken_parts                      \u2502 100         \u2502\r\n\u2502 initialization_retry_period                      \u2502 10          \u2502\r\n\u2502 wait_for_unique_parts_send_before_shutdown_ms    \u2502 10000       \u2502\r\n\u2502 materialize_ttl_recalculate_only                 \u2502 1           \u2502\r\n\u2502 allow_vertical_merges_from_compact_to_wide_parts \u2502 0           \u2502\r\n\u2502 compress_marks                                   \u2502 0           \u2502\r\n\u2502 compress_primary_key                             \u2502 0           \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n> \r\n> select name, value from system.settings where changed;\r\n\r\n```\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.settings\r\nWHERE changed\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_threads                                   \u2502 16                 \u2502\r\n\u2502 use_uncompressed_cache                        \u2502 0                  \u2502\r\n\u2502 move_all_conditions_to_prewhere               \u2502 0                  \u2502\r\n\u2502 enable_multiple_prewhere_read_steps           \u2502 0                  \u2502\r\n\u2502 move_primary_key_columns_to_end_of_prewhere   \u2502 0                  \u2502\r\n\u2502 compile_expressions                           \u2502 0                  \u2502\r\n\u2502 compile_aggregate_expressions                 \u2502 0                  \u2502\r\n\u2502 cluster_for_parallel_replicas                 \u2502 xxx \u2502\r\n\u2502 insert_deduplicate                            \u2502 1                  \u2502\r\n\u2502 http_wait_end_of_query                        \u2502 1                  \u2502\r\n\u2502 http_response_buffer_size                     \u2502 104857600          \u2502\r\n\u2502 joined_subquery_requires_alias                \u2502 0                  \u2502\r\n\u2502 max_bytes_before_external_group_by            \u2502 1442450940         \u2502\r\n\u2502 max_execution_time                            \u2502 0                  \u2502\r\n\u2502 max_expanded_ast_elements                     \u2502 50000              \u2502\r\n\u2502 join_algorithm                                \u2502 hash               \u2502\r\n\u2502 max_memory_usage                              \u2502 31000000000        \u2502\r\n\u2502 memory_usage_overcommit_max_wait_microseconds \u2502 50000              \u2502\r\n\u2502 log_query_threads                             \u2502 0                  \u2502\r\n\u2502 max_partitions_per_insert_block               \u2502 100                \u2502\r\n\u2502 enable_lightweight_delete                     \u2502 0                  \u2502\r\n\u2502 allow_experimental_lightweight_delete         \u2502 0                  \u2502\r\n\u2502 optimize_monotonous_functions_in_order_by     \u2502 0                  \u2502\r\n\u2502 enable_global_with_statement                  \u2502 0                  \u2502\r\n\u2502 optimize_rewrite_sum_if_to_count_if           \u2502 0                  \u2502\r\n\u2502 distributed_ddl_entry_format_version          \u2502 3                  \u2502\r\n\u2502 insert_keeper_max_retries                     \u2502 15                 \u2502\r\n\u2502 insert_keeper_retry_initial_backoff_ms        \u2502 100                \u2502\r\n\u2502 insert_keeper_retry_max_backoff_ms            \u2502 2000               \u2502\r\n\u2502 input_format_null_as_default                  \u2502 0                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n> \r\n> select distinct type,  compression_codec from system.columns where database not in ('system','INFORMATION_SCHEMA','information_schema') ;\r\n\r\n```\r\nSELECT DISTINCT\r\n    type,\r\n    compression_codec\r\nFROM system.columns\r\nWHERE database NOT IN ('system', 'INFORMATION_SCHEMA', 'information_schema')\r\n\r\n\u250c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500compression_codec\u2500\u2510\r\n\u2502 String                 \u2502                   \u2502\r\n\u2502 Nullable(String)       \u2502                   \u2502\r\n\u2502 Int64                  \u2502                   \u2502\r\n\u2502 UInt8                  \u2502                   \u2502\r\n\u2502 DateTime               \u2502                   \u2502\r\n\u2502 Nullable(Float32)      \u2502                   \u2502\r\n\u2502 Array(String)          \u2502                   \u2502\r\n\u2502 LowCardinality(String) \u2502                   \u2502\r\n\u2502 UUID                   \u2502                   \u2502\r\n\u2502 Int32                  \u2502                   \u2502\r\n\u2502 UInt64                 \u2502                   \u2502\r\n\u2502 UInt32                 \u2502                   \u2502\r\n\u2502 Int8                   \u2502                   \u2502\r\n\u2502 UInt16                 \u2502                   \u2502\r\n\u2502 Date                   \u2502                   \u2502\r\n\u2502 Float32                \u2502                   \u2502\r\n\u2502 Nullable(Int16)        \u2502                   \u2502\r\n\u2502 Float64                \u2502                   \u2502\r\n\u2502 Int16                  \u2502                   \u2502\r\n\u2502 Nullable(Int8)         \u2502                   \u2502\r\n\u2502 Nullable(DateTime)     \u2502                   \u2502\r\n\u2502 Nullable(Int32)        \u2502                   \u2502\r\n\u2502 DateTime64(3)          \u2502                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nFrom another message:\r\n> 9 What are the engines of the tables?\r\n\r\nThey are all `MergeTree`.\r\n\r\nHope this helps!\n> > I don't understand, what previous version? We are running 23.8.2.7:\r\n> \r\n> You sad that this issue pops up after upgrade, so the question is, what version you had prio upgrade?\r\n\r\nWe had 22.8 where there was no memory growth, the issue pops up with 23.8 and 23.10.\n>But we do know that we see the same memory growth in the instance where we insert than on the read replica.\r\n>We use no mutations or TTL on this instance. We delete data using drop table.\r\n\r\nSo we can suspect that the leak is due to the insertion of RowBinary+HTTP or due to multiple create/drop table?\r\nWhat API / language do you use for insertion? Are they reuse HTTP/TCP connection for multiple inserts?\r\nDo you use compression for HTTP? content-encoding compression or CH compression?\r\nHow many tables do you create/drop per day (ballpark)?\nAfter I disabled statistics_interval_ms, I restarted ClickHouse 16 hours ago, and everything seemed fine until now. Additionally, I created a new file in **config.d** named **kafka.xml** then i restarted Clickhouse.\r\n\r\n```\r\n<yandex>\r\n  <kafka>\r\n        <statistics_interval_ms>0</statistics_interval_ms>\r\n  </kafka>\r\n</yandex>\r\n```\r\n\r\nI hope this information can assist you.\r\n@alexfernandez \n> After I disabled statistics_interval_ms, I restarted ClickHouse 16 hours ago, and everything seemed fine until now. Additionally, I created a new file in **config.d** named **kafka.xml** then i restarted Clickhouse.\r\n> \r\n> ```\r\n> <yandex>\r\n>   <kafka>\r\n>         <statistics_interval_ms>0</statistics_interval_ms>\r\n>   </kafka>\r\n> </yandex>\r\n> ```\r\n> \r\n> I hope this information can assist you. @alexfernandez\r\n\r\nThanks @aol-59 ! But we are not using Kafka on this particular server, do you think it is worth trying anyway?\n> > After I disabled statistics_interval_ms, I restarted ClickHouse 16 hours ago, and everything seemed fine until now. Additionally, I created a new file in **config.d** named **kafka.xml** then i restarted Clickhouse.\n> \n> > \n> \n> > ```\n> \n> > <yandex>\n> \n> >   <kafka>\n> \n> >         <statistics_interval_ms>0</statistics_interval_ms>\n> \n> >   </kafka>\n> \n> > </yandex>\n> \n> > ```\n> \n> > \n> \n> > I hope this information can assist you. @alexfernandez\n> \n> \n> \n> Thanks @aol-59 ! But we are not using Kafka on this particular server, do you think it is worth trying anyway?\n\nI believe it is enabled by default.\n@den-crane I have been collecting the answers, there you go:\r\n\r\n> So we can suspect that the leak is due to the insertion of RowBinary+HTTP or due to multiple create/drop table?\r\n\r\nWe suspect insertion because it seems to correlate with the leak but we are not sure.\r\n\r\n> What API / language do you use for insertion?\r\n\r\nPython and HTTP API (aiohttp).\r\n\r\n> Are they reuse HTTP/TCP connection for multiple inserts? Yes\r\n\r\n> Do you use compression for HTTP? content-encoding compression or CH compression? No\r\n\r\n> How many tables do you create/drop per day (ballpark)?\r\n\r\nAround 30k tables created and dropped per day.\r\n\r\nThanks!\n@alexfernandez this (https://github.com/ClickHouse/ClickHouse/issues/57931#issuecomment-1858875438) information you need to look/post here when you think that the server eats too much RAM (i.e. it does not correlate with the query from the description), and the more difference the better.\r\n\r\nAnd also you may try the memory profiling that I suggested here - https://github.com/ClickHouse/ClickHouse/issues/54483#issuecomment-1855869079\nHi @azat ,\r\n\r\n> @alexfernandez this ([#57931 (comment)](https://github.com/ClickHouse/ClickHouse/issues/57931#issuecomment-1858875438)) information you need to look/post here when you think that the server eats too much RAM (i.e. it does not correlate with the query from the description), and the more difference the better.\r\n\r\nAlready sent [here](https://github.com/ClickHouse/ClickHouse/issues/57931#issuecomment-1859785429).\r\n\r\n> And also you may try the memory profiling that I suggested here - [#54483 (comment)](https://github.com/ClickHouse/ClickHouse/issues/54483#issuecomment-1855869079)\r\n\r\nI see no `MemoryTracking` entries in the log :thinking: Also I'm wary of using the profiler since this is a pretty heavy duty server. Any other info I may provide?\nI encounter memory leak too.\r\nHow to analyse clickhosue process memory liked Java VM dump? I had checked all metrics table about Memory, but everything looks fine.So I want to dump memory from clickhouse process,is there any way to do it?\n@Jack1007 see https://github.com/ClickHouse/ClickHouse/issues/54483#issuecomment-1855869079 (there is also an ongoing work of moving this into system table - https://github.com/ClickHouse/ClickHouse/issues/57703)\nHi! I faced with Memory Leak after upgrade ClickHouse to 23.8 version too.\r\n\r\nCurrent version: 23.8.9.54\r\nShards: 1\r\nReplicas: 2\r\nUsing kafka engine: No\r\n\r\nI have 2 replicas (node1 and node2) and there is no client queries to node2 at all.\r\nBut after the upgrading to 23.8 version, the both nodes started to leak memory similarly.\r\nMemory leak is going slow, but inevitable: after 30 days the memory on a node is over.\r\n\r\nThis is memory consumption on the node2 (with no client queries):\r\n<img width=\"1831\" alt=\"clickhouse\" src=\"https://github.com/ClickHouse/ClickHouse/assets/41431053/00b81375-c719-4b5c-8e90-2c09297f28fe\">\r\n\r\nPlease, tell me what additional information may I provide you about the issue :pray:\n@fishday53, can you share the version you used before the upgrade? Also, how many create (or drop) table statements do you run per day? We've observed that some memory leak is happening in every create statement, but we had no luck finding the culprit of it.\n@jrdi, Hi! \r\nPrevious version was: 22.2.2.1\r\n\r\nHere results for create and drop tables:\r\n```\r\nSELECT\r\n    event_date,\r\n    count(*)\r\nFROM query_log\r\nWHERE query LIKE '%CREATE TABLE%'\r\nGROUP BY event_date\r\n\r\nQuery id: 2ac4d3aa-10e0-4fb5-8d08-27677a93516a\r\n\r\n\u250c\u2500event_date\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 2023-12-13 \u2502   37863 \u2502\r\n\u2502 2023-12-14 \u2502   38296 \u2502\r\n\u2502 2023-12-15 \u2502   39576 \u2502\r\n\u2502 2023-12-16 \u2502   39592 \u2502\r\n\u2502 2023-12-17 \u2502   39432 \u2502\r\n\u2502 2023-12-18 \u2502   38872 \u2502\r\n\u2502 2023-12-19 \u2502   39432 \u2502\r\n\u2502 2023-12-20 \u2502   38304 \u2502\r\n\u2502 2023-12-21 \u2502   39224 \u2502\r\n\u2502 2023-12-22 \u2502   39352 \u2502\r\n\u2502 2023-12-23 \u2502   39784 \u2502\r\n\u2502 2023-12-24 \u2502   39544 \u2502\r\n\u2502 2023-12-25 \u2502   39232 \u2502\r\n\u2502 2023-12-26 \u2502   38832 \u2502\r\n\u2502 2023-12-27 \u2502   39112 \u2502\r\n\u2502 2023-12-28 \u2502   39424 \u2502\r\n\u2502 2023-12-29 \u2502   39288 \u2502\r\n\u2502 2023-12-30 \u2502   39568 \u2502\r\n\u2502 2023-12-31 \u2502   39272 \u2502\r\n\u2502 2024-01-01 \u2502   39120 \u2502\r\n\u2502 2024-01-02 \u2502   38960 \u2502\r\n\u2502 2024-01-03 \u2502   35300 \u2502\r\n\u2502 2024-01-04 \u2502   39600 \u2502\r\n\u2502 2024-01-05 \u2502   39664 \u2502\r\n\u2502 2024-01-06 \u2502   39672 \u2502\r\n\u2502 2024-01-07 \u2502   39104 \u2502\r\n\u2502 2024-01-08 \u2502   39032 \u2502\r\n\u2502 2024-01-09 \u2502   38640 \u2502\r\n\u2502 2024-01-10 \u2502   39216 \u2502\r\n\u2502 2024-01-11 \u2502   19058 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT\r\n    event_date,\r\n    count(*)\r\nFROM query_log\r\nWHERE query LIKE '%DROP TABLE%'\r\nGROUP BY event_date\r\n\r\nQuery id: 18e4f2e2-c36a-460a-b454-bb213e7f4cc9\r\n\r\n\u250c\u2500event_date\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 2023-12-13 \u2502   64280 \u2502\r\n\u2502 2023-12-14 \u2502   65375 \u2502\r\n\u2502 2023-12-15 \u2502   67920 \u2502\r\n\u2502 2023-12-16 \u2502   67944 \u2502\r\n\u2502 2023-12-17 \u2502   67620 \u2502\r\n\u2502 2023-12-18 \u2502   66584 \u2502\r\n\u2502 2023-12-19 \u2502   67656 \u2502\r\n\u2502 2023-12-20 \u2502   65680 \u2502\r\n\u2502 2023-12-21 \u2502   67244 \u2502\r\n\u2502 2023-12-22 \u2502   67476 \u2502\r\n\u2502 2023-12-23 \u2502   68280 \u2502\r\n\u2502 2023-12-24 \u2502   67872 \u2502\r\n\u2502 2023-12-25 \u2502   67220 \u2502\r\n\u2502 2023-12-26 \u2502   66292 \u2502\r\n\u2502 2023-12-27 \u2502   66936 \u2502\r\n\u2502 2023-12-28 \u2502   67616 \u2502\r\n\u2502 2023-12-29 \u2502   67224 \u2502\r\n\u2502 2023-12-30 \u2502   67852 \u2502\r\n\u2502 2023-12-31 \u2502   67376 \u2502\r\n\u2502 2024-01-01 \u2502   67188 \u2502\r\n\u2502 2024-01-02 \u2502   66776 \u2502\r\n\u2502 2024-01-03 \u2502   59456 \u2502\r\n\u2502 2024-01-04 \u2502   67968 \u2502\r\n\u2502 2024-01-05 \u2502   68076 \u2502\r\n\u2502 2024-01-06 \u2502   68096 \u2502\r\n\u2502 2024-01-07 \u2502   66952 \u2502\r\n\u2502 2024-01-08 \u2502   66764 \u2502\r\n\u2502 2024-01-09 \u2502   65924 \u2502\r\n\u2502 2024-01-10 \u2502   67176 \u2502\r\n\u2502 2024-01-11 \u2502   32554 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\nThanks, @fishday53! Looks like it could be related to what we have observed. Let's see if we can find the culprit.",
  "created_at": "2024-01-15T15:55:32Z"
}