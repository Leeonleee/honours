{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 20448,
  "instance_id": "ClickHouse__ClickHouse-20448",
  "issue_numbers": [
    "20016"
  ],
  "base_commit": "024d8491d6a1a0958de8f8c858a13ff406e197d7",
  "patch": "diff --git a/src/Common/ZooKeeper/ZooKeeper.cpp b/src/Common/ZooKeeper/ZooKeeper.cpp\nindex 4537d5ad8cd5..a1c6eb9b481b 100644\n--- a/src/Common/ZooKeeper/ZooKeeper.cpp\n+++ b/src/Common/ZooKeeper/ZooKeeper.cpp\n@@ -602,7 +602,7 @@ void ZooKeeper::removeChildren(const std::string & path)\n }\n \n \n-void ZooKeeper::removeChildrenRecursive(const std::string & path)\n+void ZooKeeper::removeChildrenRecursive(const std::string & path, const String & keep_child_node)\n {\n     Strings children = getChildren(path);\n     while (!children.empty())\n@@ -611,14 +611,15 @@ void ZooKeeper::removeChildrenRecursive(const std::string & path)\n         for (size_t i = 0; i < MULTI_BATCH_SIZE && !children.empty(); ++i)\n         {\n             removeChildrenRecursive(path + \"/\" + children.back());\n-            ops.emplace_back(makeRemoveRequest(path + \"/\" + children.back(), -1));\n+            if (likely(keep_child_node.empty() || keep_child_node != children.back()))\n+                ops.emplace_back(makeRemoveRequest(path + \"/\" + children.back(), -1));\n             children.pop_back();\n         }\n         multi(ops);\n     }\n }\n \n-void ZooKeeper::tryRemoveChildrenRecursive(const std::string & path)\n+void ZooKeeper::tryRemoveChildrenRecursive(const std::string & path, const String & keep_child_node)\n {\n     Strings children;\n     if (tryGetChildren(path, children) != Coordination::Error::ZOK)\n@@ -629,14 +630,14 @@ void ZooKeeper::tryRemoveChildrenRecursive(const std::string & path)\n         Strings batch;\n         for (size_t i = 0; i < MULTI_BATCH_SIZE && !children.empty(); ++i)\n         {\n-            batch.push_back(path + \"/\" + children.back());\n+            String child_path = path + \"/\" + children.back();\n+            tryRemoveChildrenRecursive(child_path);\n+            if (likely(keep_child_node.empty() || keep_child_node != children.back()))\n+            {\n+                batch.push_back(child_path);\n+                ops.emplace_back(zkutil::makeRemoveRequest(child_path, -1));\n+            }\n             children.pop_back();\n-            tryRemoveChildrenRecursive(batch.back());\n-\n-            Coordination::RemoveRequest request;\n-            request.path = batch.back();\n-\n-            ops.emplace_back(std::make_shared<Coordination::RemoveRequest>(std::move(request)));\n         }\n \n         /// Try to remove the children with a faster method - in bulk. If this fails,\ndiff --git a/src/Common/ZooKeeper/ZooKeeper.h b/src/Common/ZooKeeper/ZooKeeper.h\nindex 0d9dc104c48e..90d15e2ac4ac 100644\n--- a/src/Common/ZooKeeper/ZooKeeper.h\n+++ b/src/Common/ZooKeeper/ZooKeeper.h\n@@ -184,6 +184,12 @@ class ZooKeeper\n     /// result would be the same as for the single call.\n     void tryRemoveRecursive(const std::string & path);\n \n+    /// Similar to removeRecursive(...) and tryRemoveRecursive(...), but does not remove path itself.\n+    /// If keep_child_node is not empty, this method will not remove path/keep_child_node (but will remove its subtree).\n+    /// It can be useful to keep some child node as a flag which indicates that path is currently removing.\n+    void removeChildrenRecursive(const std::string & path, const String & keep_child_node = {});\n+    void tryRemoveChildrenRecursive(const std::string & path, const String & keep_child_node = {});\n+\n     /// Remove all children nodes (non recursive).\n     void removeChildren(const std::string & path);\n \n@@ -246,9 +252,6 @@ class ZooKeeper\n     void init(const std::string & implementation_, const std::string & hosts_, const std::string & identity_,\n               int32_t session_timeout_ms_, int32_t operation_timeout_ms_, const std::string & chroot_);\n \n-    void removeChildrenRecursive(const std::string & path);\n-    void tryRemoveChildrenRecursive(const std::string & path);\n-\n     /// The following methods don't throw exceptions but return error codes.\n     Coordination::Error createImpl(const std::string & path, const std::string & data, int32_t mode, std::string & path_created);\n     Coordination::Error removeImpl(const std::string & path, int32_t version);\n@@ -320,7 +323,7 @@ class EphemeralNodeHolder\n         catch (...)\n         {\n             ProfileEvents::increment(ProfileEvents::CannotRemoveEphemeralNode);\n-            DB::tryLogCurrentException(__PRETTY_FUNCTION__);\n+            DB::tryLogCurrentException(__PRETTY_FUNCTION__, \"Cannot remove \" + path + \": \");\n         }\n     }\n \ndiff --git a/src/Interpreters/DDLWorker.cpp b/src/Interpreters/DDLWorker.cpp\nindex 05370a6a3b72..fc460a5584c6 100644\n--- a/src/Interpreters/DDLWorker.cpp\n+++ b/src/Interpreters/DDLWorker.cpp\n@@ -652,15 +652,10 @@ void DDLWorker::enqueueTask(DDLTaskPtr task_ptr)\n             {\n                 recoverZooKeeper();\n             }\n-            else if (e.code == Coordination::Error::ZNONODE)\n-            {\n-                LOG_ERROR(log, \"ZooKeeper error: {}\", getCurrentExceptionMessage(true));\n-                // TODO: retry?\n-            }\n             else\n             {\n                 LOG_ERROR(log, \"Unexpected ZooKeeper error: {}.\", getCurrentExceptionMessage(true));\n-                return;\n+                throw;\n             }\n         }\n         catch (...)\n@@ -695,25 +690,44 @@ void DDLWorker::processTask(DDLTask & task)\n \n     LOG_DEBUG(log, \"Processing task {} ({})\", task.entry_name, task.entry.query);\n \n-    String dummy;\n     String active_node_path = task.entry_path + \"/active/\" + task.host_id_str;\n     String finished_node_path = task.entry_path + \"/finished/\" + task.host_id_str;\n \n-    auto code = zookeeper->tryCreate(active_node_path, \"\", zkutil::CreateMode::Ephemeral, dummy);\n+    /// It will tryRemove(...) on exception\n+    auto active_node = zkutil::EphemeralNodeHolder::existing(active_node_path, *zookeeper);\n \n-    if (code == Coordination::Error::ZOK || code == Coordination::Error::ZNODEEXISTS)\n+    /// Try fast path\n+    auto create_active_res = zookeeper->tryCreate(active_node_path, {}, zkutil::CreateMode::Ephemeral);\n+    if (create_active_res != Coordination::Error::ZOK)\n     {\n-        // Ok\n-    }\n-    else if (code == Coordination::Error::ZNONODE)\n-    {\n-        /// There is no parent\n-        createStatusDirs(task.entry_path, zookeeper);\n-        if (Coordination::Error::ZOK != zookeeper->tryCreate(active_node_path, \"\", zkutil::CreateMode::Ephemeral, dummy))\n-            throw Coordination::Exception(code, active_node_path);\n+        if (create_active_res != Coordination::Error::ZNONODE && create_active_res != Coordination::Error::ZNODEEXISTS)\n+        {\n+            assert(Coordination::isHardwareError(create_active_res));\n+            throw Coordination::Exception(create_active_res, active_node_path);\n+        }\n+\n+        /// Status dirs were not created in enqueueQuery(...) or someone is removing entry\n+        if (create_active_res == Coordination::Error::ZNONODE)\n+            createStatusDirs(task.entry_path, zookeeper);\n+\n+        if (create_active_res == Coordination::Error::ZNODEEXISTS)\n+        {\n+            /// Connection has been lost and now we are retrying to write query status,\n+            /// but our previous ephemeral node still exists.\n+            assert(task.was_executed);\n+            zkutil::EventPtr eph_node_disappeared = std::make_shared<Poco::Event>();\n+            String dummy;\n+            if (zookeeper->tryGet(active_node_path, dummy, nullptr, eph_node_disappeared))\n+            {\n+                constexpr int timeout_ms = 5000;\n+                if (!eph_node_disappeared->tryWait(timeout_ms))\n+                    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Ephemeral node {} still exists, \"\n+                                    \"probably it's owned by someone else\", active_node_path);\n+            }\n+        }\n+\n+        zookeeper->create(active_node_path, {}, zkutil::CreateMode::Ephemeral);\n     }\n-    else\n-        throw Coordination::Exception(code, active_node_path);\n \n     if (!task.was_executed)\n     {\n@@ -969,7 +983,6 @@ void DDLWorker::cleanupQueue(Int64 current_time_seconds, const ZooKeeperPtr & zo\n \n         String node_name = *it;\n         String node_path = fs::path(queue_dir) / node_name;\n-        String lock_path = fs::path(node_path) / \"lock\";\n \n         Coordination::Stat stat;\n         String dummy;\n@@ -991,19 +1004,14 @@ void DDLWorker::cleanupQueue(Int64 current_time_seconds, const ZooKeeperPtr & zo\n             if (!node_lifetime_is_expired && !node_is_outside_max_window)\n                 continue;\n \n-            /// Skip if there are active nodes (it is weak guard)\n-            if (zookeeper->exists(fs::path(node_path) / \"active\", &stat) && stat.numChildren > 0)\n-            {\n-                LOG_INFO(log, \"Task {} should be deleted, but there are active workers. Skipping it.\", node_name);\n-                continue;\n-            }\n-\n-            /// Usage of the lock is not necessary now (tryRemoveRecursive correctly removes node in a presence of concurrent cleaners)\n-            /// But the lock will be required to implement system.distributed_ddl_queue table\n-            auto lock = createSimpleZooKeeperLock(zookeeper, node_path, \"lock\", host_fqdn_id);\n-            if (!lock->tryLock())\n+            /// At first we remove entry/active node to prevent staled hosts from executing entry concurrently\n+            auto rm_active_res = zookeeper->tryRemove(fs::path(node_path) / \"active\");\n+            if (rm_active_res != Coordination::Error::ZOK && rm_active_res != Coordination::Error::ZNONODE)\n             {\n-                LOG_INFO(log, \"Task {} should be deleted, but it is locked. Skipping it.\", node_name);\n+                if (rm_active_res == Coordination::Error::ZNOTEMPTY)\n+                    LOG_DEBUG(log, \"Task {} should be deleted, but there are active workers. Skipping it.\", node_name);\n+                else\n+                    LOG_WARNING(log, \"Unexpected status code {} on attempt to remove {}/active\", rm_active_res, node_name);\n                 continue;\n             }\n \n@@ -1012,21 +1020,33 @@ void DDLWorker::cleanupQueue(Int64 current_time_seconds, const ZooKeeperPtr & zo\n             else if (node_is_outside_max_window)\n                 LOG_INFO(log, \"Task {} is outdated, deleting it\", node_name);\n \n-            /// Deleting\n+            /// We recursively delete all nodes except node_path/finished to prevent staled hosts from\n+            /// creating node_path/active node (see createStatusDirs(...))\n+            zookeeper->tryRemoveChildrenRecursive(node_path, \"finished\");\n+\n+            /// And then we remove node_path and node_path/finished in a single transaction\n+            Coordination::Requests ops;\n+            Coordination::Responses res;\n+            ops.emplace_back(zkutil::makeCheckRequest(node_path, -1));  /// See a comment below\n+            ops.emplace_back(zkutil::makeRemoveRequest(fs::path(node_path) / \"finished\", -1));\n+            ops.emplace_back(zkutil::makeRemoveRequest(node_path, -1));\n+            auto rm_entry_res = zookeeper->tryMulti(ops, res);\n+            if (rm_entry_res == Coordination::Error::ZNONODE)\n             {\n-                Strings children = zookeeper->getChildren(node_path);\n-                for (const String & child : children)\n-                {\n-                    if (child != \"lock\")\n-                        zookeeper->tryRemoveRecursive(fs::path(node_path) / child);\n-                }\n-\n-                /// Remove the lock node and its parent atomically\n-                Coordination::Requests ops;\n-                ops.emplace_back(zkutil::makeRemoveRequest(lock_path, -1));\n-                ops.emplace_back(zkutil::makeRemoveRequest(node_path, -1));\n-                zookeeper->multi(ops);\n+                /// Most likely both node_path/finished and node_path were removed concurrently.\n+                bool entry_removed_concurrently = res[0]->error == Coordination::Error::ZNONODE;\n+                if (entry_removed_concurrently)\n+                    continue;\n+\n+                /// Possible rare case: initiator node has lost connection after enqueueing entry and failed to create status dirs.\n+                /// No one has started to process the entry, so node_path/active and node_path/finished nodes were never created, node_path has no children.\n+                /// Entry became outdated, but we cannot remove remove it in a transaction with node_path/finished.\n+                assert(res[0]->error == Coordination::Error::ZOK && res[1]->error == Coordination::Error::ZNONODE);\n+                rm_entry_res = zookeeper->tryRemove(node_path);\n+                assert(rm_entry_res != Coordination::Error::ZNOTEMPTY);\n+                continue;\n             }\n+            zkutil::KeeperMultiException::check(rm_entry_res, ops, res);\n         }\n         catch (...)\n         {\n@@ -1040,21 +1060,32 @@ void DDLWorker::cleanupQueue(Int64 current_time_seconds, const ZooKeeperPtr & zo\n void DDLWorker::createStatusDirs(const std::string & node_path, const ZooKeeperPtr & zookeeper)\n {\n     Coordination::Requests ops;\n-    {\n-        Coordination::CreateRequest request;\n-        request.path = fs::path(node_path) / \"active\";\n-        ops.emplace_back(std::make_shared<Coordination::CreateRequest>(std::move(request)));\n-    }\n-    {\n-        Coordination::CreateRequest request;\n-        request.path = fs::path(node_path) / \"finished\";\n-        ops.emplace_back(std::make_shared<Coordination::CreateRequest>(std::move(request)));\n-    }\n+    ops.emplace_back(zkutil::makeCreateRequest(fs::path(node_path) / \"active\", {}, zkutil::CreateMode::Persistent));\n+    ops.emplace_back(zkutil::makeCreateRequest(fs::path(node_path) / \"finished\", {}, zkutil::CreateMode::Persistent));\n+\n     Coordination::Responses responses;\n     Coordination::Error code = zookeeper->tryMulti(ops, responses);\n-    if (code != Coordination::Error::ZOK\n-        && code != Coordination::Error::ZNODEEXISTS)\n-        throw Coordination::Exception(code);\n+\n+    bool both_created = code == Coordination::Error::ZOK;\n+\n+    /// Failed on attempt to create node_path/active because it exists, so node_path/finished must exist too\n+    bool both_already_exists = responses.size() == 2 && responses[0]->error == Coordination::Error::ZNODEEXISTS\n+                                                     && responses[1]->error == Coordination::Error::ZRUNTIMEINCONSISTENCY;\n+    assert(!both_already_exists || (zookeeper->exists(fs::path(node_path) / \"active\") && zookeeper->exists(fs::path(node_path) / \"finished\")));\n+\n+    /// Failed on attempt to create node_path/finished, but node_path/active does not exist\n+    bool is_currently_deleting = responses.size() == 2 && responses[0]->error == Coordination::Error::ZOK\n+                                                       && responses[1]->error == Coordination::Error::ZNODEEXISTS;\n+    if (both_created || both_already_exists)\n+        return;\n+\n+    if (is_currently_deleting)\n+        throw Exception(ErrorCodes::UNFINISHED, \"Cannot create status dirs for {}, \"\n+                        \"most likely because someone is deleting it concurrently\", node_path);\n+\n+    /// Connection lost or entry was removed\n+    assert(Coordination::isHardwareError(code) || code == Coordination::Error::ZNONODE);\n+    zkutil::KeeperMultiException::check(code, ops, responses);\n }\n \n \n@@ -1114,7 +1145,7 @@ void DDLWorker::runMainThread()\n             if (!Coordination::isHardwareError(e.code))\n             {\n                 /// A logical error.\n-                LOG_ERROR(log, \"ZooKeeper error: {}. Failed to start DDLWorker.\",getCurrentExceptionMessage(true));\n+                LOG_ERROR(log, \"ZooKeeper error: {}. Failed to start DDLWorker.\", getCurrentExceptionMessage(true));\n                 reset_state(false);\n                 assert(false);  /// Catch such failures in tests with debug build\n             }\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 097b7679899d..518577c473c4 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -751,7 +751,7 @@ void StorageReplicatedMergeTree::drop()\n         auto zookeeper = global_context.getZooKeeper();\n \n         /// If probably there is metadata in ZooKeeper, we don't allow to drop the table.\n-        if (is_readonly || !zookeeper)\n+        if (!zookeeper)\n             throw Exception(\"Can't drop readonly replicated table (need to drop data in ZooKeeper as well)\", ErrorCodes::TABLE_IS_READ_ONLY);\n \n         shutdown();\n",
  "test_patch": "diff --git a/tests/integration/test_distributed_ddl/cluster.py b/tests/integration/test_distributed_ddl/cluster.py\nindex 811eb94bad43..24f11fec5472 100644\n--- a/tests/integration/test_distributed_ddl/cluster.py\n+++ b/tests/integration/test_distributed_ddl/cluster.py\n@@ -10,8 +10,8 @@\n \n \n class ClickHouseClusterWithDDLHelpers(ClickHouseCluster):\n-    def __init__(self, base_path, config_dir):\n-        ClickHouseCluster.__init__(self, base_path)\n+    def __init__(self, base_path, config_dir, testcase_name):\n+        ClickHouseCluster.__init__(self, base_path, name=testcase_name)\n \n         self.test_config_dir = config_dir\n \n@@ -104,8 +104,8 @@ def replace_domains_to_ip_addresses_in_cluster_config(self, instances_to_replace\n     def ddl_check_there_are_no_dublicates(instance):\n         query = \"SELECT max(c), argMax(q, c) FROM (SELECT lower(query) AS q, count() AS c FROM system.query_log WHERE type=2 AND q LIKE '/* ddl_entry=query-%' GROUP BY query)\"\n         rows = instance.query(query)\n-        assert len(rows) > 0 and rows[0][0] == \"1\", \"dublicates on {} {}, query {}\".format(instance.name,\n-                                                                                           instance.ip_address, query)\n+        assert len(rows) > 0 and rows[0][0] == \"1\", \"dublicates on {} {}: {}\".format(instance.name,\n+                                                                                           instance.ip_address, rows)\n \n     @staticmethod\n     def insert_reliable(instance, query_insert):\ndiff --git a/tests/integration/test_distributed_ddl/test.py b/tests/integration/test_distributed_ddl/test.py\nindex f0e78dfec414..58e1d0d06f79 100755\n--- a/tests/integration/test_distributed_ddl/test.py\n+++ b/tests/integration/test_distributed_ddl/test.py\n@@ -14,7 +14,7 @@\n \n @pytest.fixture(scope=\"module\", params=[\"configs\", \"configs_secure\"])\n def test_cluster(request):\n-    cluster = ClickHouseClusterWithDDLHelpers(__file__, request.param)\n+    cluster = ClickHouseClusterWithDDLHelpers(__file__, request.param, request.param)\n \n     try:\n         cluster.prepare()\ndiff --git a/tests/integration/test_distributed_ddl/test_replicated_alter.py b/tests/integration/test_distributed_ddl/test_replicated_alter.py\nindex bd95f5660b75..148ad5fca5e0 100644\n--- a/tests/integration/test_distributed_ddl/test_replicated_alter.py\n+++ b/tests/integration/test_distributed_ddl/test_replicated_alter.py\n@@ -12,7 +12,7 @@\n \n @pytest.fixture(scope=\"module\", params=[\"configs\", \"configs_secure\"])\n def test_cluster(request):\n-    cluster = ClickHouseClusterWithDDLHelpers(__file__, request.param)\n+    cluster = ClickHouseClusterWithDDLHelpers(__file__, request.param, \"alters_\" + request.param)\n \n     try:\n         # TODO: Fix ON CLUSTER alters when nodes have different configs. Need to canonicalize node identity.\ndiff --git a/tests/queries/0_stateless/01669_columns_declaration_serde.sql b/tests/queries/0_stateless/01669_columns_declaration_serde.sql\nindex 8e3354d63cde..a6bf1184e9f6 100644\n--- a/tests/queries/0_stateless/01669_columns_declaration_serde.sql\n+++ b/tests/queries/0_stateless/01669_columns_declaration_serde.sql\n@@ -22,12 +22,12 @@ DROP TABLE IF EXISTS test_r1;\n DROP TABLE IF EXISTS test_r2;\n \n CREATE TABLE test_r1 (x UInt64, \"\\\\\" String DEFAULT '\\r\\n\\t\\\\' || '\n-') ENGINE = ReplicatedMergeTree('/clickhouse/test', 'r1') ORDER BY \"\\\\\";\n+') ENGINE = ReplicatedMergeTree('/clickhouse/test_01669', 'r1') ORDER BY \"\\\\\";\n \n INSERT INTO test_r1 (\"\\\\\") VALUES ('\\\\');\n \n CREATE TABLE test_r2 (x UInt64, \"\\\\\" String DEFAULT '\\r\\n\\t\\\\' || '\n-') ENGINE = ReplicatedMergeTree('/clickhouse/test', 'r2') ORDER BY \"\\\\\";\n+') ENGINE = ReplicatedMergeTree('/clickhouse/test_01669', 'r2') ORDER BY \"\\\\\";\n \n SYSTEM SYNC REPLICA test_r2;\n \ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex 07250cd9c901..0b4ac2b581bb 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -574,6 +574,7 @@\n         \"01676_dictget_in_default_expression\",\n         \"01715_background_checker_blather_zookeeper\",\n         \"01700_system_zookeeper_path_in\",\n+        \"01669_columns_declaration_serde\",\n         \"attach\",\n         \"ddl_dictionaries\",\n         \"dictionary\",\n",
  "problem_statement": "Distributed ddl worker task loop forever for no znode error\n**Describe the bug**\r\nDistributed DDL task faided besause no znode and the worker will retry the task forever.\r\n\r\n**Does it reproduce on recent release?**\r\nversion 20.12.3.3\r\n\r\n**How to reproduce**\r\n* ClickHouse version 20.12.3.3\r\n* Setup a ClickHouse cluster\r\n* Shutdown one CH node\r\n* Send a Distributed sql to cluster\r\n* Shutdown all nodes when all CH nodes except the showdown one finish the DDL task.\r\n* Wait longer than `task_max_lifetime`  \r\n* Start all CH node at once\r\n* Then the first shutdown node's DDL worker will retry forever for no znode error.\r\n\r\n**Expected behavior**\r\nWhen all nodes startup the first shutdown node's ddl task may failed because ddl-task-cleaner may delete expired task, but the error should accur once but not forever.\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2021.01.29 16:57:35.431774 [ 181362 ] {} <Debug> DDLWorker: Processing task query-0000000088 (CREATE DATABASE woo200 UUID '5758a8fd-1bfb-48b8-bdbf-9c3623a598f3' ON CLUSTER JACKY)\r\n2021.01.29 16:57:35.436581 [ 181362 ] {} <Error> DDLWorker: ZooKeeper error: Code: 999, e.displayText() = Coordination::Exception: No node, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. /project/jd/clickhouse/build/../contrib/libcxx/include/exception:129: std::exception::capture() @ 0x6e23545 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n1. /project/jd/clickhouse/build/../contrib/libcxx/include/exception:109: Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x1b8da876 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n2. /project/jd/clickhouse/build/../src/Common/Exception.cpp:39: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x6e744ae in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n3. /project/jd/clickhouse/build/../src/Common/ZooKeeper/IKeeper.cpp:25: Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error, int) @ 0x175b6cc8 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n4. /project/jd/clickhouse/build/../src/Common/ZooKeeper/IKeeper.cpp:41: Coordination::Exception::Exception(Coordination::Error) @ 0x175b74ea in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n5. /project/jd/clickhouse/build/../src/Interpreters/DDLWorker.cpp:1083: DB::DDLWorker::createStatusDirs(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<zkutil::ZooKeeper> const&) @ 0x1454b3e6 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n6. /project/jd/clickhouse/build/../contrib/libcxx/include/memory:3826: DB::DDLWorker::processTask(DB::DDLTask&) @ 0x1453640c in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n7. /project/jd/clickhouse/build/../src/Interpreters/DDLWorker.cpp:719: DB::DDLWorker::enqueueTask(std::__1::unique_ptr<DB::DDLTask, std::__1::default_delete<DB::DDLTask> >) @ 0x14533565 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n8. /project/jd/clickhouse/build/../src/Interpreters/DDLWorker.cpp:520: DB::DDLWorker::scheduleTasks()::'lambda'()::operator()() const @ 0x14529b68 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n9. /project/jd/clickhouse/build/../contrib/libcxx/include/type_traits:3519: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::DDLWorker::scheduleTasks()::'lambda'()&>(DB::DDLWorker::scheduleTasks()::'lambda'()&) @ 0x14559d97 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n10. /project/jd/clickhouse/build/../contrib/libcxx/include/functional:1541: std::__1::__function::__func<DB::DDLWorker::scheduleTasks()::'lambda'(), std::__1::allocator<DB::DDLWorker::scheduleTasks()::'lambda'()>, void ()>::operator()() @ 0x14559ce3 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n11. /project/jd/clickhouse/build/../contrib/libcxx/include/functional:1867: std::__1::function<void ()>::operator()() const @ 0x6e39852 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n12. /project/jd/clickhouse/build/../src/Common/ThreadPool.cpp:239: ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x6e8c8ba in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n13. /project/jd/clickhouse/build/../src/Common/ThreadPool.cpp:117: void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const @ 0x6e911ea in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n14. /project/jd/clickhouse/build/../contrib/libcxx/include/type_traits:3525: ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x6e979da in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n15. /project/jd/clickhouse/build/../contrib/libcxx/include/type_traits:3519: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()&>(void&&...) @ 0x6ea0d06 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n16. /project/jd/clickhouse/build/../contrib/libcxx/include/functional:1541: std::__1::__function::__func<ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'(), std::__1::allocator<ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()>, void ()>::operator()() @ 0x6ea0b6d in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n17. /project/jd/clickhouse/build/../contrib/libcxx/include/functional:1867: std::__1::function<void ()>::operator()() const @ 0x6e39852 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n18. /project/jd/clickhouse/build/../src/Common/ThreadPool.cpp:239: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x6e8aeea in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n19. /project/jd/clickhouse/build/../src/Common/ThreadPool.cpp:117: void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const @ 0x6e8d90a in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n20. /project/jd/clickhouse/build/../contrib/libcxx/include/type_traits:3519: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x6e998e9 in /data1/home/wujianchao/server/clickhouse/clickhouse\r\n21. start_thread @ 0x7dc5 in /usr/lib64/libpthread-2.17.so\r\n22. clone @ 0xf621d in /usr/lib64/libc-2.17.so\r\n (version 20.12.3.3)\r\n```\r\n\r\n**Additional context**\r\n\r\n\n",
  "hints_text": "Mabye there are two resolutions:\r\n\r\n**1. add a distributed lock to distributed DDL processing**\r\n\r\nThe lock should take 'DDLWorker::runCleanupThread' procedure in consideration. The lock can still employ Zookeeper just like 'ZooKeeperLock', but a read-write-lock is better.\r\n\r\n**2. Adjust distributed DDL queue clean policy**\r\n\r\nIf a distributed DDL task is not finished by all nodes, skip clean it. The 'all nodes' should except the ones who is not in the clusters.\r\n\r\n\r\n\n3. Reinitialize `DDLWorker` state on unexpected errors (#18285), so it will work like server was just restarted. As for now, `ZNONODE` [is not considered as unexpected error](https://github.com/ClickHouse/ClickHouse/blob/3b52b0400863f0d90a0d01ece3a6492d4dfdaf3b/src/Interpreters/DDLWorker.cpp#L669), but it should be.\n@tavplubix Error in this issue is in distributed DDL worker thread but not main thread. So maybe the 3th resolutions is not for this issue.\nDistributed DDL queries used to be executed in the main thread before #14684. Then we added a pool of worker threads to allow parallel execution of queue tasks. However, default pool size (`distributed_ddl.pool_size`) is 1 and it's not recommended to increase number of worker threads, because greater value causes arbitrary reordering of queue entries (see discussion in #14684). I think that there is not much sense to have separate worker pool of size 1, so after #16193 queries will be executed in the main thread again if parallel execution is not enabled. But even for parallel execution we can safely throw exception from `DDLWorker::enqueueTask(...)` (worker thread) on unexpected error and `ThreadPool` will rethrow it from `DDLWorker::scheduleTasks()` (main thread) causing `DDLWorker` to reinitialize its state. The issue is that `DDLWorker::enqueueTask(...)` retries in infinite loop ignoring most errors. \nhi tavplubix, I got what you mean. Because of both DDL worker thread and clean thread will operate the zookeeper queue, I think handle queue competition will make sense.",
  "created_at": "2021-02-12T19:35:43Z",
  "modified_files": [
    "src/Common/ZooKeeper/ZooKeeper.cpp",
    "src/Common/ZooKeeper/ZooKeeper.h",
    "src/Interpreters/DDLWorker.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_distributed_ddl/cluster.py",
    "tests/integration/test_distributed_ddl/test.py",
    "tests/integration/test_distributed_ddl/test_replicated_alter.py",
    "tests/queries/0_stateless/01669_columns_declaration_serde.sql",
    "tests/queries/skip_list.json"
  ]
}