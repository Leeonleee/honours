{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 45072,
  "instance_id": "ClickHouse__ClickHouse-45072",
  "issue_numbers": [
    "43891"
  ],
  "base_commit": "f661dad0e974be9f16d5558edd5f733ccb6ccc13",
  "patch": "diff --git a/src/Backups/BackupSettings.cpp b/src/Backups/BackupSettings.cpp\nindex 8c54b29141ae..747e7bb0c1f0 100644\n--- a/src/Backups/BackupSettings.cpp\n+++ b/src/Backups/BackupSettings.cpp\n@@ -6,7 +6,7 @@\n #include <Parsers/ASTSetQuery.h>\n #include <Parsers/ASTLiteral.h>\n #include <IO/ReadHelpers.h>\n-\n+#include <Backups/SettingsFieldOptionalUUID.h>\n \n namespace DB\n {\n@@ -16,48 +16,6 @@ namespace ErrorCodes\n     extern const int WRONG_BACKUP_SETTINGS;\n }\n \n-\n-namespace\n-{\n-    struct SettingFieldOptionalUUID\n-    {\n-        std::optional<UUID> value;\n-\n-        explicit SettingFieldOptionalUUID(const std::optional<UUID> & value_) : value(value_) {}\n-\n-        explicit SettingFieldOptionalUUID(const Field & field)\n-        {\n-            if (field.getType() == Field::Types::Null)\n-            {\n-                value = std::nullopt;\n-                return;\n-            }\n-\n-            if (field.getType() == Field::Types::String)\n-            {\n-                const String & str = field.get<const String &>();\n-                if (str.empty())\n-                {\n-                    value = std::nullopt;\n-                    return;\n-                }\n-\n-                UUID id;\n-                if (tryParse(id, str))\n-                {\n-                    value = id;\n-                    return;\n-                }\n-            }\n-\n-            throw Exception(ErrorCodes::CANNOT_PARSE_BACKUP_SETTINGS, \"Cannot parse uuid from {}\", field);\n-        }\n-\n-        explicit operator Field() const { return Field(value ? toString(*value) : \"\"); }\n-    };\n-}\n-\n-\n /// List of backup settings except base_backup_name and cluster_host_ids.\n #define LIST_OF_BACKUP_SETTINGS(M) \\\n     M(String, id) \\\ndiff --git a/src/Backups/BackupsWorker.cpp b/src/Backups/BackupsWorker.cpp\nindex 267400ce66d3..53bebaf06d7a 100644\n--- a/src/Backups/BackupsWorker.cpp\n+++ b/src/Backups/BackupsWorker.cpp\n@@ -30,6 +30,7 @@ namespace ErrorCodes\n {\n     extern const int BAD_ARGUMENTS;\n     extern const int LOGICAL_ERROR;\n+    extern const int CONCURRENT_ACCESS_NOT_SUPPORTED;\n }\n \n using OperationID = BackupsWorker::OperationID;\n@@ -121,10 +122,12 @@ namespace\n }\n \n \n-BackupsWorker::BackupsWorker(size_t num_backup_threads, size_t num_restore_threads)\n+BackupsWorker::BackupsWorker(size_t num_backup_threads, size_t num_restore_threads, bool allow_concurrent_backups_, bool allow_concurrent_restores_)\n     : backups_thread_pool(num_backup_threads, /* max_free_threads = */ 0, num_backup_threads)\n     , restores_thread_pool(num_restore_threads, /* max_free_threads = */ 0, num_restore_threads)\n     , log(&Poco::Logger::get(\"BackupsWorker\"))\n+    , allow_concurrent_backups(allow_concurrent_backups_)\n+    , allow_concurrent_restores(allow_concurrent_restores_)\n {\n     /// We set max_free_threads = 0 because we don't want to keep any threads if there is no BACKUP or RESTORE query running right now.\n }\n@@ -157,6 +160,16 @@ OperationID BackupsWorker::startMakingBackup(const ASTPtr & query, const Context\n     else\n         backup_id = toString(*backup_settings.backup_uuid);\n \n+    /// Check if there are no concurrent backups\n+    if (num_active_backups && !allow_concurrent_backups)\n+    {\n+        /// If its an internal backup and we currently have 1 active backup, it could be the original query, validate using backup_uuid\n+        if (!(num_active_backups == 1 && backup_settings.internal && getAllActiveBackupInfos().at(0).id == toString(*backup_settings.backup_uuid)))\n+        {\n+            throw Exception(ErrorCodes::CONCURRENT_ACCESS_NOT_SUPPORTED, \"Concurrent backups not supported, turn on setting 'allow_concurrent_backups'\");\n+        }\n+    }\n+\n     std::shared_ptr<IBackupCoordination> backup_coordination;\n     if (backup_settings.internal)\n     {\n@@ -370,6 +383,9 @@ OperationID BackupsWorker::startRestoring(const ASTPtr & query, ContextMutablePt\n     auto restore_query = std::static_pointer_cast<ASTBackupQuery>(query->clone());\n     auto restore_settings = RestoreSettings::fromRestoreQuery(*restore_query);\n \n+    if (!restore_settings.backup_uuid)\n+        restore_settings.backup_uuid = UUIDHelpers::generateV4();\n+\n     /// `restore_id` will be used as a key to the `infos` map, so it should be unique.\n     OperationID restore_id;\n     if (restore_settings.internal)\n@@ -377,7 +393,17 @@ OperationID BackupsWorker::startRestoring(const ASTPtr & query, ContextMutablePt\n     else if (!restore_settings.id.empty())\n         restore_id = restore_settings.id;\n     else\n-        restore_id = toString(UUIDHelpers::generateV4());\n+        restore_id = toString(*restore_settings.backup_uuid);\n+\n+    /// Check if there are no concurrent restores\n+    if (num_active_restores && !allow_concurrent_restores)\n+    {\n+        /// If its an internal restore and we currently have 1 active restore, it could be the original query, validate using iz\n+        if (!(num_active_restores == 1 && restore_settings.internal && getAllActiveRestoreInfos().at(0).id == toString(*restore_settings.backup_uuid)))\n+        {\n+            throw Exception(ErrorCodes::CONCURRENT_ACCESS_NOT_SUPPORTED, \"Concurrent restores not supported, turn on setting 'allow_concurrent_restores'\");\n+        }\n+    }\n \n     std::shared_ptr<IRestoreCoordination> restore_coordination;\n     if (restore_settings.internal)\n@@ -471,6 +497,7 @@ void BackupsWorker::doRestore(\n         backup_open_params.context = context;\n         backup_open_params.backup_info = backup_info;\n         backup_open_params.base_backup_info = restore_settings.base_backup_info;\n+        backup_open_params.backup_uuid = restore_settings.backup_uuid;\n         backup_open_params.password = restore_settings.password;\n         BackupPtr backup = BackupFactory::instance().createBackup(backup_open_params);\n \n@@ -687,6 +714,30 @@ std::vector<BackupsWorker::Info> BackupsWorker::getAllInfos() const\n     return res_infos;\n }\n \n+std::vector<BackupsWorker::Info> BackupsWorker::getAllActiveBackupInfos() const\n+{\n+    std::vector<Info> res_infos;\n+    std::lock_guard lock{infos_mutex};\n+    for (const auto & info : infos | boost::adaptors::map_values)\n+    {\n+        if (info.status==BackupStatus::CREATING_BACKUP)\n+            res_infos.push_back(info);\n+    }\n+    return res_infos;\n+}\n+\n+std::vector<BackupsWorker::Info> BackupsWorker::getAllActiveRestoreInfos() const\n+{\n+    std::vector<Info> res_infos;\n+    std::lock_guard lock{infos_mutex};\n+    for (const auto & info : infos | boost::adaptors::map_values)\n+    {\n+        if (info.status==BackupStatus::RESTORING)\n+            res_infos.push_back(info);\n+    }\n+    return res_infos;\n+}\n+\n void BackupsWorker::shutdown()\n {\n     bool has_active_backups_and_restores = (num_active_backups || num_active_restores);\ndiff --git a/src/Backups/BackupsWorker.h b/src/Backups/BackupsWorker.h\nindex ab99691c0bc0..b6d9729833e8 100644\n--- a/src/Backups/BackupsWorker.h\n+++ b/src/Backups/BackupsWorker.h\n@@ -23,7 +23,7 @@ class IRestoreCoordination;\n class BackupsWorker\n {\n public:\n-    BackupsWorker(size_t num_backup_threads, size_t num_restore_threads);\n+    BackupsWorker(size_t num_backup_threads, size_t num_restore_threads, bool allow_concurrent_backups_, bool allow_concurrent_restores_);\n \n     /// Waits until all tasks have been completed.\n     void shutdown();\n@@ -103,6 +103,8 @@ class BackupsWorker\n     void setStatus(const OperationID & id, BackupStatus status, bool throw_if_error = true);\n     void setStatusSafe(const String & id, BackupStatus status) { setStatus(id, status, false); }\n     void setNumFilesAndSize(const OperationID & id, size_t num_files, UInt64 uncompressed_size, UInt64 compressed_size);\n+    std::vector<Info> getAllActiveBackupInfos() const;\n+    std::vector<Info> getAllActiveRestoreInfos() const;\n \n     ThreadPool backups_thread_pool;\n     ThreadPool restores_thread_pool;\n@@ -113,6 +115,8 @@ class BackupsWorker\n     std::atomic<size_t> num_active_restores = 0;\n     mutable std::mutex infos_mutex;\n     Poco::Logger * log;\n+    const bool allow_concurrent_backups;\n+    const bool allow_concurrent_restores;\n };\n \n }\ndiff --git a/src/Backups/RestoreSettings.cpp b/src/Backups/RestoreSettings.cpp\nindex 2c06ee907b5e..0ffa48224f7b 100644\n--- a/src/Backups/RestoreSettings.cpp\n+++ b/src/Backups/RestoreSettings.cpp\n@@ -7,6 +7,7 @@\n #include <Parsers/ASTSetQuery.h>\n #include <boost/algorithm/string/predicate.hpp>\n #include <Common/FieldVisitorConvertToNumber.h>\n+#include <Backups/SettingsFieldOptionalUUID.h>\n \n \n namespace DB\n@@ -162,7 +163,9 @@ namespace\n     M(RestoreUDFCreationMode, create_function) \\\n     M(Bool, internal) \\\n     M(String, host_id) \\\n-    M(String, coordination_zk_path)\n+    M(String, coordination_zk_path) \\\n+    M(OptionalUUID, backup_uuid)\n+\n \n RestoreSettings RestoreSettings::fromRestoreQuery(const ASTBackupQuery & query)\n {\ndiff --git a/src/Backups/RestoreSettings.h b/src/Backups/RestoreSettings.h\nindex 713adbe80296..50058d83a25b 100644\n--- a/src/Backups/RestoreSettings.h\n+++ b/src/Backups/RestoreSettings.h\n@@ -122,6 +122,11 @@ struct RestoreSettings\n     /// Path in Zookeeper used to coordinate restoring process while executing by RESTORE ON CLUSTER.\n     String coordination_zk_path;\n \n+    /// Internal, should not be specified by user.\n+    /// UUID of the backup. If it's not set it will be generated randomly.\n+    /// This is used to validate internal restores when allow_concurrent_restores is turned off\n+    std::optional<UUID> backup_uuid;\n+\n     static RestoreSettings fromRestoreQuery(const ASTBackupQuery & query);\n     void copySettingsToQuery(ASTBackupQuery & query) const;\n };\ndiff --git a/src/Backups/SettingsFieldOptionalUUID.cpp b/src/Backups/SettingsFieldOptionalUUID.cpp\nnew file mode 100644\nindex 000000000000..3f14608b206a\n--- /dev/null\n+++ b/src/Backups/SettingsFieldOptionalUUID.cpp\n@@ -0,0 +1,43 @@\n+#include <Backups/SettingsFieldOptionalUUID.h>\n+#include <Common/ErrorCodes.h>\n+#include <Core/SettingsFields.h>\n+#include <IO/ReadHelpers.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int CANNOT_PARSE_BACKUP_SETTINGS;\n+}\n+\n+\n+    SettingFieldOptionalUUID::SettingFieldOptionalUUID(const Field & field)\n+    {\n+        if (field.getType() == Field::Types::Null)\n+        {\n+            value = std::nullopt;\n+            return;\n+        }\n+\n+        if (field.getType() == Field::Types::String)\n+        {\n+            const String & str = field.get<const String &>();\n+            if (str.empty())\n+            {\n+                value = std::nullopt;\n+                return;\n+            }\n+\n+            UUID id;\n+            if (tryParse(id, str))\n+            {\n+                value = id;\n+                return;\n+            }\n+        }\n+\n+        throw Exception(ErrorCodes::CANNOT_PARSE_BACKUP_SETTINGS, \"Cannot parse uuid from {}\", field);\n+    }\n+\n+}\ndiff --git a/src/Backups/SettingsFieldOptionalUUID.h b/src/Backups/SettingsFieldOptionalUUID.h\nnew file mode 100644\nindex 000000000000..d4d0a230f664\n--- /dev/null\n+++ b/src/Backups/SettingsFieldOptionalUUID.h\n@@ -0,0 +1,18 @@\n+#pragma once\n+\n+#include <optional>\n+#include <Core/SettingsFields.h>\n+\n+namespace DB\n+{\n+struct SettingFieldOptionalUUID\n+    {\n+        std::optional<UUID> value;\n+\n+        explicit SettingFieldOptionalUUID(const std::optional<UUID> & value_) : value(value_) {}\n+\n+        explicit SettingFieldOptionalUUID(const Field & field);\n+\n+        explicit operator Field() const { return Field(value ? toString(*value) : \"\"); }\n+    };\n+}\ndiff --git a/src/Interpreters/Context.cpp b/src/Interpreters/Context.cpp\nindex 171803c396d6..0f1126d25028 100644\n--- a/src/Interpreters/Context.cpp\n+++ b/src/Interpreters/Context.cpp\n@@ -1888,8 +1888,11 @@ BackupsWorker & Context::getBackupsWorker() const\n {\n     auto lock = getLock();\n \n+    const bool allow_concurrent_backups = this->getConfigRef().getBool(\"allow_concurrent_backups\", true);\n+    const bool allow_concurrent_restores = this->getConfigRef().getBool(\"allow_concurrent_restores\", true);\n+\n     if (!shared->backups_worker)\n-        shared->backups_worker.emplace(getSettingsRef().backup_threads, getSettingsRef().restore_threads);\n+        shared->backups_worker.emplace(getSettingsRef().backup_threads, getSettingsRef().restore_threads, allow_concurrent_backups, allow_concurrent_restores);\n \n     return *shared->backups_worker;\n }\n",
  "test_patch": "diff --git a/tests/integration/test_backup_restore_on_cluster/configs/disallow_concurrency.xml b/tests/integration/test_backup_restore_on_cluster/configs/disallow_concurrency.xml\nnew file mode 100644\nindex 000000000000..144be77c9f92\n--- /dev/null\n+++ b/tests/integration/test_backup_restore_on_cluster/configs/disallow_concurrency.xml\n@@ -0,0 +1,15 @@\n+<clickhouse>\n+    <storage_configuration>\n+        <disks>\n+            <backups>\n+                <type>local</type>\n+                <path>/backups/</path>\n+            </backups>\n+        </disks>\n+    </storage_configuration>\n+    <backups>\n+        <allowed_disk>backups</allowed_disk>\n+    </backups>\n+    <allow_concurrent_backups>false</allow_concurrent_backups>\n+    <allow_concurrent_restores>false</allow_concurrent_restores>\n+</clickhouse>\ndiff --git a/tests/integration/test_backup_restore_on_cluster/test_disallow_concurrency.py b/tests/integration/test_backup_restore_on_cluster/test_disallow_concurrency.py\nnew file mode 100644\nindex 000000000000..8f514b95d0b0\n--- /dev/null\n+++ b/tests/integration/test_backup_restore_on_cluster/test_disallow_concurrency.py\n@@ -0,0 +1,227 @@\n+from random import randint\n+import pytest\n+import os.path\n+import time\n+import concurrent\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import TSV, assert_eq_with_retry\n+\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+num_nodes = 10\n+\n+\n+def generate_cluster_def():\n+    path = os.path.join(\n+        os.path.dirname(os.path.realpath(__file__)),\n+        \"./_gen/cluster_for_concurrency_test.xml\",\n+    )\n+    os.makedirs(os.path.dirname(path), exist_ok=True)\n+    with open(path, \"w\") as f:\n+        f.write(\n+            \"\"\"\n+        <clickhouse>\n+            <remote_servers>\n+                <cluster>\n+                    <shard>\n+        \"\"\"\n+        )\n+        for i in range(num_nodes):\n+            f.write(\n+                \"\"\"\n+                        <replica>\n+                            <host>node\"\"\"\n+                + str(i)\n+                + \"\"\"</host>\n+                            <port>9000</port>\n+                        </replica>\n+            \"\"\"\n+            )\n+        f.write(\n+            \"\"\"\n+                    </shard>\n+                </cluster>\n+            </remote_servers>\n+        </clickhouse>\n+        \"\"\"\n+        )\n+    return path\n+\n+\n+main_configs = [\"configs/disallow_concurrency.xml\", generate_cluster_def()]\n+user_configs = [\"configs/allow_database_types.xml\"]\n+\n+nodes = []\n+for i in range(num_nodes):\n+    nodes.append(\n+        cluster.add_instance(\n+            f\"node{i}\",\n+            main_configs=main_configs,\n+            user_configs=user_configs,\n+            external_dirs=[\"/backups/\"],\n+            macros={\"replica\": f\"node{i}\", \"shard\": \"shard1\"},\n+            with_zookeeper=True,\n+        )\n+    )\n+\n+node0 = nodes[0]\n+\n+\n+@pytest.fixture(scope=\"module\", autouse=True)\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+@pytest.fixture(autouse=True)\n+def drop_after_test():\n+    try:\n+        yield\n+    finally:\n+        node0.query(\"DROP TABLE IF EXISTS tbl ON CLUSTER 'cluster' NO DELAY\")\n+        node0.query(\"DROP DATABASE IF EXISTS mydb ON CLUSTER 'cluster' NO DELAY\")\n+\n+\n+backup_id_counter = 0\n+\n+\n+def new_backup_name():\n+    global backup_id_counter\n+    backup_id_counter += 1\n+    return f\"Disk('backups', '{backup_id_counter}')\"\n+\n+\n+def create_and_fill_table():\n+    node0.query(\n+        \"CREATE TABLE tbl ON CLUSTER 'cluster' (\"\n+        \"x UInt64\"\n+        \") ENGINE=ReplicatedMergeTree('/clickhouse/tables/tbl/', '{replica}')\"\n+        \"ORDER BY x\"\n+    )\n+    for i in range(num_nodes):\n+        nodes[i].query(f\"INSERT INTO tbl SELECT number FROM numbers(40000000)\")\n+\n+\n+# All the tests have concurrent backup/restores with same backup names\n+# The same works with different backup names too. Since concurrency\n+# check comes before backup name check, separate tests are not added for different names\n+\n+\n+def test_concurrent_backups_on_same_node():\n+    create_and_fill_table()\n+\n+    backup_name = new_backup_name()\n+\n+    id = (\n+        nodes[0]\n+        .query(f\"BACKUP TABLE tbl ON CLUSTER 'cluster' TO {backup_name} ASYNC\")\n+        .split(\"\\t\")[0]\n+    )\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'CREATING_BACKUP' AND id = '{id}'\",\n+        \"CREATING_BACKUP\",\n+    )\n+    assert \"Concurrent backups not supported\" in nodes[0].query_and_get_error(\n+        f\"BACKUP TABLE tbl ON CLUSTER 'cluster' TO {backup_name}\"\n+    )\n+\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'BACKUP_CREATED' AND id = '{id}'\",\n+        \"BACKUP_CREATED\",\n+    )\n+\n+    # This restore part is added to confirm creating an internal backup & restore work\n+    # even when a concurrent backup is stopped\n+    nodes[0].query(f\"DROP TABLE tbl ON CLUSTER 'cluster' NO DELAY\")\n+    nodes[0].query(f\"RESTORE TABLE tbl ON CLUSTER 'cluster' FROM {backup_name}\")\n+    nodes[0].query(\"SYSTEM SYNC REPLICA ON CLUSTER 'cluster' tbl\")\n+\n+\n+def test_concurrent_backups_on_different_nodes():\n+    create_and_fill_table()\n+\n+    backup_name = new_backup_name()\n+\n+    nodes[1].query(f\"BACKUP TABLE tbl ON CLUSTER 'cluster' TO {backup_name} ASYNC\")\n+    assert_eq_with_retry(\n+        nodes[1],\n+        f\"SELECT status FROM system.backups WHERE status == 'CREATING_BACKUP'\",\n+        \"CREATING_BACKUP\",\n+    )\n+    assert \"Concurrent backups not supported\" in nodes[2].query_and_get_error(\n+        f\"BACKUP TABLE tbl ON CLUSTER 'cluster' TO {backup_name}\"\n+    )\n+\n+\n+def test_concurrent_restores_on_same_node():\n+    create_and_fill_table()\n+\n+    backup_name = new_backup_name()\n+\n+    id = (\n+        nodes[0]\n+        .query(f\"BACKUP TABLE tbl ON CLUSTER 'cluster' TO {backup_name} ASYNC\")\n+        .split(\"\\t\")[0]\n+    )\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'CREATING_BACKUP' AND id = '{id}'\",\n+        \"CREATING_BACKUP\",\n+    )\n+\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'BACKUP_CREATED' AND id = '{id}'\",\n+        \"BACKUP_CREATED\",\n+    )\n+\n+    nodes[0].query(f\"DROP TABLE tbl ON CLUSTER 'cluster' NO DELAY\")\n+    nodes[0].query(f\"RESTORE TABLE tbl ON CLUSTER 'cluster' FROM {backup_name} ASYNC\")\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'RESTORING'\",\n+        \"RESTORING\",\n+    )\n+    assert \"Concurrent restores not supported\" in nodes[0].query_and_get_error(\n+        f\"RESTORE TABLE tbl ON CLUSTER 'cluster' FROM {backup_name}\"\n+    )\n+\n+\n+def test_concurrent_restores_on_different_node():\n+    create_and_fill_table()\n+\n+    backup_name = new_backup_name()\n+\n+    id = (\n+        nodes[0]\n+        .query(f\"BACKUP TABLE tbl ON CLUSTER 'cluster' TO {backup_name} ASYNC\")\n+        .split(\"\\t\")[0]\n+    )\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'CREATING_BACKUP' AND id = '{id}'\",\n+        \"CREATING_BACKUP\",\n+    )\n+\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'BACKUP_CREATED' AND id = '{id}'\",\n+        \"BACKUP_CREATED\",\n+    )\n+\n+    nodes[0].query(f\"DROP TABLE tbl ON CLUSTER 'cluster' NO DELAY\")\n+    nodes[0].query(f\"RESTORE TABLE tbl ON CLUSTER 'cluster' FROM {backup_name} ASYNC\")\n+    assert_eq_with_retry(\n+        nodes[0],\n+        f\"SELECT status FROM system.backups WHERE status == 'RESTORING'\",\n+        \"RESTORING\",\n+    )\n+    assert \"Concurrent restores not supported\" in nodes[1].query_and_get_error(\n+        f\"RESTORE TABLE tbl ON CLUSTER 'cluster' FROM {backup_name}\"\n+    )\n",
  "problem_statement": "Introduce a server setting to disallow concurrently running backups.\n> (you don't have to strictly follow this form)\r\n\r\n**Use case**\r\n\r\nSometimes, for really sophisticated scenarios it can be hard to control the amount of in-progress backups run externally. For the purpose of simpler control we can introduce server-level setting which disallows running concurrent backups of any type. \r\n\r\nSomething like this in `config.xml` (enabled by default):\r\n`<allow_concurrent_backups>true</allow_concurrent_backups>`\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt's important that backups can be executed in distributed way on multiple servers. Need to take it into account.\r\n\r\n**Describe alternatives you've considered**\r\nExternal check of `system.backups` table, but it can be complex.\r\n\n",
  "hints_text": "cc @vitlibar ",
  "created_at": "2023-01-09T17:16:58Z"
}