{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 27794,
  "instance_id": "ClickHouse__ClickHouse-27794",
  "issue_numbers": [
    "19255"
  ],
  "base_commit": "0c2582d2b48b16aa9e4abf503c4814ee1b7ae005",
  "patch": "diff --git a/src/Storages/Kafka/KafkaBlockInputStream.cpp b/src/Storages/Kafka/KafkaBlockInputStream.cpp\nindex 5d9b19b1972c..95fa1459e760 100644\n--- a/src/Storages/Kafka/KafkaBlockInputStream.cpp\n+++ b/src/Storages/Kafka/KafkaBlockInputStream.cpp\n@@ -252,7 +252,11 @@ Block KafkaBlockInputStream::readImpl()\n         }\n         else\n         {\n-            LOG_WARNING(log, \"Parsing of message (topic: {}, partition: {}, offset: {}) return no rows.\", buffer->currentTopic(), buffer->currentPartition(), buffer->currentOffset());\n+            // We came here in case of tombstone (or sometimes zero-length) messages, and it is not something abnormal\n+            // TODO: it seems like in case of put_error_to_stream=true we may need to process those differently\n+            // currently we just skip them with note in logs.\n+            buffer->storeLastReadMessageOffset();\n+            LOG_DEBUG(log, \"Parsing of message (topic: {}, partition: {}, offset: {}) return no rows.\", buffer->currentTopic(), buffer->currentPartition(), buffer->currentOffset());\n         }\n \n         if (!buffer->hasMorePolledMessages()\ndiff --git a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\nindex bd25607a5f39..86037276166e 100644\n--- a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n+++ b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n@@ -466,13 +466,19 @@ bool ReadBufferFromKafkaConsumer::nextImpl()\n     if (!allowed || !hasMorePolledMessages())\n         return false;\n \n-    // XXX: very fishy place with const casting.\n-    auto * new_position = reinterpret_cast<char *>(const_cast<unsigned char *>(current->get_payload().get_data()));\n-    BufferBase::set(new_position, current->get_payload().get_size(), 0);\n-    allowed = false;\n+    const auto * message_data = current->get_payload().get_data();\n+    size_t message_size = current->get_payload().get_size();\n \n+    allowed = false;\n     ++current;\n \n+    /// If message is empty, return end of stream.\n+    if (message_data == nullptr)\n+        return false;\n+\n+    /// const_cast is needed, because ReadBuffer works with non-const char *.\n+    auto * new_position = reinterpret_cast<char *>(const_cast<unsigned char *>(message_data));\n+    BufferBase::set(new_position, message_size, 0);\n     return true;\n }\n \n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex cff2b9729836..21d6c7c10ab6 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -283,6 +283,11 @@ def test_kafka_json_as_string(kafka_cluster):\n     kafka_produce(kafka_cluster, 'kafka_json_as_string', ['{\"t\": 123, \"e\": {\"x\": \"woof\"} }', '', '{\"t\": 124, \"e\": {\"x\": \"test\"} }',\n                                            '{\"F1\":\"V1\",\"F2\":{\"F21\":\"V21\",\"F22\":{},\"F23\":\"V23\",\"F24\":\"2019-12-24T16:28:04\"},\"F3\":\"V3\"}'])\n \n+    # 'tombstone' record (null value) = marker of deleted record\n+    producer = KafkaProducer(bootstrap_servers=\"localhost:{}\".format(cluster.kafka_port), value_serializer=producer_serializer, key_serializer=producer_serializer)\n+    producer.send(topic='kafka_json_as_string', key='xxx')\n+    producer.flush()\n+\n     instance.query('''\n         CREATE TABLE test.kafka (field String)\n             ENGINE = Kafka\n",
  "problem_statement": "A bug in Kafka engine: Segmentation fault (JSONEachRowRowInputFormat)\nKafka ingestion crashed the server.\r\nData is serialized via jackson (possible missing field referenced  by queue/topic table)\r\n\r\n```\r\n2021.01.18 16:19:09.348955 [ 30912 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.01.18 16:19:09.349037 [ 30912 ] {} <Fatal> BaseDaemon: (version 20.8.3.18, no build id) (from thread 4994) (no query) Received signal Segmentation fault (11)\r\n2021.01.18 16:19:09.349063 [ 30912 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Address not mapped to object.\r\n2021.01.18 16:19:09.349096 [ 30912 ] {} <Fatal> BaseDaemon: Stack trace: 0x1131d9c0 0x118459ac 0x112719d7 0x11890535 0x11891774 0x10a7675d 0x10a91b8a 0x11472f8d 0x11473988 0x1149cde2 0x1149d20a 0x1149d30f 0xa43cc4d 0xa43b3ff 0x7fa73043bea5 0x7fa72fd5896d\r\n2021.01.18 16:19:09.349160 [ 30912 ] {} <Fatal> BaseDaemon: 3. DB::JSONEachRowRowInputFormat::readPrefix() @ 0x1131d9c0 in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349189 [ 30912 ] {} <Fatal> BaseDaemon: 4. DB::IRowInputFormat::generate() @ 0x118459ac in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349212 [ 30912 ] {} <Fatal> BaseDaemon: 5. DB::ISource::work() @ 0x112719d7 in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349229 [ 30912 ] {} <Fatal> BaseDaemon: 6. ? @ 0x11890535 in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349246 [ 30912 ] {} <Fatal> BaseDaemon: 7. DB::KafkaBlockInputStream::readImpl() @ 0x11891774 in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349264 [ 30912 ] {} <Fatal> BaseDaemon: 8. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349289 [ 30912 ] {} <Fatal> BaseDaemon: 9. DB::copyData(DB::IBlockInputStream&, DB::IBlockOutputStream&, std::__1::atomic<bool>*) @ 0x10a91b8a in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349307 [ 30912 ] {} <Fatal> BaseDaemon: 10. DB::StorageKafka::streamToViews() @ 0x11472f8d in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349324 [ 30912 ] {} <Fatal> BaseDaemon: 11. DB::StorageKafka::threadFunc() @ 0x11473988 in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349341 [ 30912 ] {} <Fatal> BaseDaemon: 12. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0x1149cde2 in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349358 [ 30912 ] {} <Fatal> BaseDaemon: 13. DB::BackgroundSchedulePool::threadFunction() @ 0x1149d20a in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349373 [ 30912 ] {} <Fatal> BaseDaemon: 14. ? @ 0x1149d30f in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349395 [ 30912 ] {} <Fatal> BaseDaemon: 15. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xa43cc4d in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349411 [ 30912 ] {} <Fatal> BaseDaemon: 16. ? @ 0xa43b3ff in /usr/bin/clickhouse\r\n2021.01.18 16:19:09.349435 [ 30912 ] {} <Fatal> BaseDaemon: 17. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n2021.01.18 16:19:09.349459 [ 30912 ] {} <Fatal> BaseDaemon: 18. __clone @ 0xfe96d in /usr/lib64/libc-2.17.so\r\n```\r\n\r\nThank you for your effort and time.\r\n\r\nRegards,\r\nIvan\r\n\n",
  "hints_text": "Does it reproduce on version 20.8.12 or 21.1?\r\nThese versions contain bugfixes.\nAdditional info about environment and package:\r\n\r\n\r\n- rpm: clickhouse-server-20.8.3.18-1.el7.x86_64 (altinity)\r\n- Linux data.server.com 3.10.0-1160.6.1.el7.x86_64 #1 SMP Tue Nov 17 13:59:11 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n- CentOS Linux release 7.9.2009 (Core)\r\n\r\n```\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7f8cf47fc700 (LWP 32084)]\r\n0x000000001131d9c0 in DB::JSONEachRowRowInputFormat::readPrefix() ()\r\n(gdb) bt\r\n#0  0x000000001131d9c0 in DB::JSONEachRowRowInputFormat::readPrefix() ()\r\n#1  0x00000000118459ac in DB::IRowInputFormat::generate() ()\r\n#2  0x00000000112719d7 in DB::ISource::work() ()\r\n#3  0x0000000011890535 in DB::KafkaBlockInputStream::readImpl()::{lambda()#1}::operator()() const ()\r\n#4  0x0000000011891774 in DB::KafkaBlockInputStream::readImpl() ()\r\n#5  0x0000000010a7675d in DB::IBlockInputStream::read() ()\r\n#6  0x0000000010a91b8a in DB::copyData(DB::IBlockInputStream&, DB::IBlockOutputStream&, std::__1::atomic<bool>*) ()\r\n#7  0x0000000011472f8d in DB::StorageKafka::streamToViews() ()\r\n#8  0x0000000011473988 in DB::StorageKafka::threadFunc() ()\r\n#9  0x000000001149cde2 in DB::BackgroundSchedulePoolTaskInfo::execute() ()\r\n#10 0x000000001149d20a in DB::BackgroundSchedulePool::threadFunction() ()\r\n#11 0x000000001149d30f in _ZZN20ThreadFromGlobalPoolC4IZN2DB22BackgroundSchedulePoolC4EmmmPKcEUlvE_JEEEOT_DpOT0_ENKUlvE_clEv ()\r\n#12 0x000000000a43cc4d in ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) ()\r\n#13 0x000000000a43b3ff in void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#3}> >(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#3}>) ()\r\n#14 0x00007f8d2d368ea5 in start_thread () from /lib64/libpthread.so.0\r\n#15 0x00007f8d2cc8596d in clone () from /lib64/libc.so.6\r\n```\r\n\r\n\r\n\r\n\n> Does it reproduce on version 20.8.12 or 21.1?\r\n> These versions contain bugfixes.\r\n\r\nThank you for the quick reply.\r\nI must check it because I use Altinity repo (obviously not so recent?)\r\n\nDo you suggest building from the source (latest stable) and avoid obsoleted yum repos like the one I currently use?\r\n\r\nThank you for your time!\r\nRegards,\r\nIvan\r\n\n@ivanprostran You can use ClickHouse official repository:\r\n\r\nhttps://clickhouse.tech/\r\n\\- go to Quick Start;\r\n\\- select \"CentOS or RedHat\".\r\n\r\nThe packages in our repository and in Altinity repository are bitwise identical - the Altinity repo is just a mirror (that may lag behind but usually should not).\n> The packages in our repository and in Altinity repository are bitwise identical - the Altinity repo is just a mirror\r\n\r\nThat is NOT true currently - Altinity rebuild the binary from sources and uses a packaging which is out of sync with official. \r\n\r\nSo the code is exactly the same, but the package/binary is not byte-identical.\r\n\r\nThat will be changed to get rid of the differences in future releases (@alex-zaitsev will it?) \nI think it happens on empty message.\r\n\r\nUPD. no, it's checked by tests: https://github.com/ClickHouse/ClickHouse/blob/b8fcd75de75c908a422b221a6d10c30b53fe8aa4/tests/integration/test_storage_kafka/test.py#L193\nIf it will happen on newer versions - we will need a CREATE TABLE for Kafka table and message sample. \nThank you for the  suggestions.\r\nUpgraded to newer version (for the moment without segmentation fault)\r\n\r\nNow we have problem with a read only mode (all tables)\r\n\r\n```\r\n2021.01.19 09:55:23.538606 [ 30176 ] {} <Error> void DB::StorageKafka::threadFunc(size_t): Code: 242, e.displayText() = DB::Exception: Table is in readonly mode (zookeeper path: /clickhouse/tables/01-01/cm_downstream_quality_R), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x8692d81 in /usr/bin/clickhouse\r\n1. DB::StorageReplicatedMergeTree::assertNotReadonly() const @ 0xf4386d9 in /usr/bin/clickhouse\r\n2. DB::StorageReplicatedMergeTree::write(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Context const&) @ 0xf4387f2 in /usr/bin/clickhouse\r\n3. DB::PushingToViewsBlockOutputStream::PushingToViewsBlockOutputStream(std::__1::shared_ptr<DB::IStorage> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Context const&, std::__1::shared_ptr<DB::IAST> const&, bool) @ 0xee35528 in /usr/bin/clickhouse\r\n4. DB::InterpreterInsertQuery::execute() @ 0xee293e1 in /usr/bin/clickhouse\r\n5. DB::PushingToViewsBlockOutputStream::PushingToViewsBlockOutputStream(std::__1::shared_ptr<DB::IStorage> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Context const&, std::__1::shared_ptr<DB::IAST> const&, bool) @ 0xee34d54 in /usr/bin/clickhouse\r\n6. DB::InterpreterInsertQuery::execute() @ 0xee293e1 in /usr/bin/clickhouse\r\n7. DB::StorageKafka::streamToViews() @ 0xf4f41be in /usr/bin/clickhouse\r\n8. DB::StorageKafka::threadFunc(unsigned long) @ 0xf4f346e in /usr/bin/clickhouse\r\n9. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0xeb39ae2 in /usr/bin/clickhouse\r\n10. DB::BackgroundSchedulePool::threadFunction() @ 0xeb3bf17 in /usr/bin/clickhouse\r\n11. ? @ 0xeb3d191 in /usr/bin/clickhouse\r\n12. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x86415ed in /usr/bin/clickhouse\r\n13. ? @ 0x86451a3 in /usr/bin/clickhouse\r\n14. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n15. __clone @ 0xfe96d in /usr/lib64/libc-2.17.so\r\n (version 21.1.2.15 (official build))\r\n```\r\n\r\n\r\n\n> Thank you for the suggestions.\r\n> Upgraded to newer version (for the moment without segmentation fault)\r\n> \r\n> Now we have problem with a read only mode (all tables)\r\n> \r\n> ```\r\n> 2021.01.19 09:55:23.538606 [ 30176 ] {} <Error> void DB::StorageKafka::threadFunc(size_t): Code: 242, e.displayText() = DB::Exception: Table is in readonly mode (zookeeper path: /clickhouse/tables/01-01/cm_downstream_quality_R), Stack trace (when copying this message, always include the lines below):\r\n> \r\n> 0. DB::Exception::Exception<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x8692d81 in /usr/bin/clickhouse\r\n> 1. DB::StorageReplicatedMergeTree::assertNotReadonly() const @ 0xf4386d9 in /usr/bin/clickhouse\r\n> 2. DB::StorageReplicatedMergeTree::write(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Context const&) @ 0xf4387f2 in /usr/bin/clickhouse\r\n> 3. DB::PushingToViewsBlockOutputStream::PushingToViewsBlockOutputStream(std::__1::shared_ptr<DB::IStorage> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Context const&, std::__1::shared_ptr<DB::IAST> const&, bool) @ 0xee35528 in /usr/bin/clickhouse\r\n> 4. DB::InterpreterInsertQuery::execute() @ 0xee293e1 in /usr/bin/clickhouse\r\n> 5. DB::PushingToViewsBlockOutputStream::PushingToViewsBlockOutputStream(std::__1::shared_ptr<DB::IStorage> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Context const&, std::__1::shared_ptr<DB::IAST> const&, bool) @ 0xee34d54 in /usr/bin/clickhouse\r\n> 6. DB::InterpreterInsertQuery::execute() @ 0xee293e1 in /usr/bin/clickhouse\r\n> 7. DB::StorageKafka::streamToViews() @ 0xf4f41be in /usr/bin/clickhouse\r\n> 8. DB::StorageKafka::threadFunc(unsigned long) @ 0xf4f346e in /usr/bin/clickhouse\r\n> 9. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0xeb39ae2 in /usr/bin/clickhouse\r\n> 10. DB::BackgroundSchedulePool::threadFunction() @ 0xeb3bf17 in /usr/bin/clickhouse\r\n> 11. ? @ 0xeb3d191 in /usr/bin/clickhouse\r\n> 12. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x86415ed in /usr/bin/clickhouse\r\n> 13. ? @ 0x86451a3 in /usr/bin/clickhouse\r\n> 14. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n> 15. __clone @ 0xfe96d in /usr/lib64/libc-2.17.so\r\n>  (version 21.1.2.15 (official build))\r\n> ```\r\n\r\nIgore this (It was problem with zookeeper section not being referenced within config.d/config.xml)\r\n\r\nmetrika.xml:\r\n\r\n```\r\n\t<zookeeper-servers>\r\n\t\t<node index=\"1\">\r\n\t\t\t<host>localhost</host>\r\n\t\t\t<port>2181</port>\r\n\t\t</node>\r\n\t</zookeeper-servers>\r\n```\r\n\r\nconfig.d/config.xml:\r\n\r\n```\r\n  <remote_servers  incl=\"clickhouse_remote_servers\"/>\r\n   <zookeeper incl=\"zookeeper-servers\" optional=\"true\" />\r\n```\r\n\r\nWorks now.\r\nStill checking segmentation fault occurences.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\nHi,\r\nConfirmed  - latest version works OK.\r\nWe have not found any issues so far.\r\n\r\n` (version 21.1.2.15 (official build))`\r\n\r\nHowever migration from \"20.8.3.18-1\" to \"21.1.2.15\" needed some minor changes:\r\n\r\nDefault value (uuid field type) toUUID(0) was raising an error.\r\ntoUUID('00000000-0000-0000-0000-000000000000') solved the problem.\r\n\r\nThank you for your effort and time.\r\n\r\nRegards,\r\nIvan Prostran\r\n\r\n\n20.8.12.2 also has the problem\r\n![Uploading image.png\u2026]()\r\n\n20.8.12.2 also has the problem\r\n![image](https://user-images.githubusercontent.com/37525039/118956914-9d476e00-b992-11eb-8316-9bac3ed43935.png)\r\n\n@chengbinghan Update to the latest stable version 21.4.\n> @chengbinghan Update to the latest stable version 21.4.\r\n\r\nWhat exactly caused it?\nAs long as it is fixed in recent release - I don't care.\n> > @chengbinghan Update to the latest stable version 21.4.\r\n> \r\n> What exactly caused it?\r\n\r\nIf you can reproduce it in test (like [here](https://github.com/ClickHouse/ClickHouse/blob/d56d7145c178811de2b680dbecdb895b71ea113b/tests/integration/test_storage_kafka/test.py#L283-L346)) I can give you the exact answer to your question :)  \n> > > @chengbinghan Update to the latest stable version 21.4.\r\n> > \r\n> > \r\n> > What exactly caused it?\r\n> \r\n> If you can reproduce it in test (like [here](https://github.com/ClickHouse/ClickHouse/blob/d56d7145c178811de2b680dbecdb895b71ea113b/tests/integration/test_storage_kafka/test.py#L283-L346)) I can give you the exact answer to your question :)\r\n\r\nI found out what caused it.\r\nWhen the data of current is null, the eof() function thinks that there is a piece of data in current, so it uses *buf to fetch the data directly. In fact, buf is null.\r\n\r\n![image](https://user-images.githubusercontent.com/16408168/119271726-50cf8d00-bc35-11eb-96d9-74d3d21b70d7.png)\r\n![image](https://user-images.githubusercontent.com/16408168/119271733-5927c800-bc35-11eb-8432-de5265f61ac0.png)\r\n\nYou mean empty message? I think we have test for that (which is also checked with address sanitizer). \r\n\r\n> eof() function thinks that there is a piece of data in current\r\n\r\nWhy exactly? `avaliable` should be zero, so `hasPandengData` should return false.\n> You mean empty message? I think we have test for that (which is also checked with address sanitizer).\r\n> \r\n> > eof() function thinks that there is a piece of data in current\r\n> \r\n> Why exactly? `avaliable` should be zero, so `hasPandengData` should return false.\r\n\r\nhasPandengData returns false, but it also judges the next() function, and returns a null buf in the next() function.\r\n\r\n![image](https://user-images.githubusercontent.com/16408168/120286817-3cecf080-c2f1-11eb-865e-fc4925238a00.png)\r\n\n> In fact, buf is null.\r\n\r\nI can not reproduce it in tests :| Can you help me? \r\n\r\nI don't like 'blind' fixes, and in all my tests I always get a valid pointer there...\r\n\r\nCan you explain how exactly you produce the data / what is the topic configuration / broker versions? Non standard librdkafka configuration? Do you use official build?\n> > In fact, buf is null.\r\n> \r\n> I can not reproduce it in tests :| Can you help me?\r\n> \r\n> I don't like 'blind' fixes, and in all my tests I always get a valid pointer there...\r\n> \r\n> Can you explain how exactly you produce the data / what is the topic configuration / broker versions? Non standard librdkafka configuration? Do you use official build?\r\n\r\nLook at this commit, produce None to Kafka, it can be reproduced.\r\nhttps://github.com/aiwhj/ClickHouse/commit/88f45540672d182522af1cb3bf62f89c59e34686\r\n\r\nAnd after this commit, there will be one more buf.eof() judgment in skipBOMIfExists(in), Clickhouse will not crash, but the problem still exists.\r\nhttps://github.com/ClickHouse/ClickHouse/commit/4a336e381497b5b1b254e404c5578e0d2e829e14\n> And [after this commit](https://github.com/ClickHouse/ClickHouse/commit/4a336e381497b5b1b254e404c5578e0d2e829e14) \r\n\r\nBut that commit was merged in 20.9, so the problem is solved?\r\n\r\n> Clickhouse will not crash, but the problem still exists.\r\n\r\nWhat do you mean by the problem in that case? nullptr? \n> > And [after this commit](https://github.com/ClickHouse/ClickHouse/commit/4a336e381497b5b1b254e404c5578e0d2e829e14)\r\n> \r\n> But that commit was merged in 20.9, so the problem is solved?\r\n> \r\n> > Clickhouse will not crash, but the problem still exists.\r\n> \r\n> What do you mean by the problem in that case? nullptr?\r\n\r\nYes, skipBOMIfExists was originally used to skip the BOM, and by the way, the nullptr problem was also avoided.\n> Yes, skipBOMIfExists was originally used to skip the BOM, and by the way, the nullptr problem was also avoided.\r\n\r\nSo if the problem does not reproduces anymore - I will not do any fixes.",
  "created_at": "2021-08-17T17:17:26Z",
  "modified_files": [
    "src/Storages/Kafka/KafkaBlockInputStream.cpp",
    "src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_kafka/test.py"
  ]
}