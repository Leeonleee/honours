{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 56809,
  "instance_id": "ClickHouse__ClickHouse-56809",
  "issue_numbers": [
    "47643"
  ],
  "base_commit": "bf1098951508dff6f1ec5e1787aaf4cae6f3de25",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 3b90a3e068b8..ca3d33700c3d 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -140,6 +140,7 @@ class IColumn;\n     \\\n     M(UInt64, alter_sync, 1, \"Wait for actions to manipulate the partitions. 0 - do not wait, 1 - wait for execution only of itself, 2 - wait for everyone.\", 0) ALIAS(replication_alter_partitions_sync) \\\n     M(Int64, replication_wait_for_inactive_replica_timeout, 120, \"Wait for inactive replica to execute ALTER/OPTIMIZE. Time in seconds, 0 - do not wait, negative - wait for unlimited time.\", 0) \\\n+    M(Bool, alter_move_to_space_execute_async, false, \"Execute ALTER TABLE MOVE ... TO [DISK|VOLUME] asynchronously\", 0) \\\n     \\\n     M(LoadBalancing, load_balancing, LoadBalancing::RANDOM, \"Which replicas (among healthy replicas) to preferably send a query to (on the first attempt) for distributed processing.\", 0) \\\n     M(UInt64, load_balancing_first_offset, 0, \"Which replica to preferably send a query when FIRST_OR_RANDOM load balancing strategy is used.\", 0) \\\ndiff --git a/src/Storages/MergeTree/BackgroundJobsAssignee.cpp b/src/Storages/MergeTree/BackgroundJobsAssignee.cpp\nindex f06ff9097992..32714b3b07fc 100644\n--- a/src/Storages/MergeTree/BackgroundJobsAssignee.cpp\n+++ b/src/Storages/MergeTree/BackgroundJobsAssignee.cpp\n@@ -67,10 +67,11 @@ void BackgroundJobsAssignee::scheduleFetchTask(ExecutableTaskPtr fetch_task)\n }\n \n \n-void BackgroundJobsAssignee::scheduleMoveTask(ExecutableTaskPtr move_task)\n+bool BackgroundJobsAssignee::scheduleMoveTask(ExecutableTaskPtr move_task)\n {\n     bool res = getContext()->getMovesExecutor()->trySchedule(move_task);\n     res ? trigger() : postpone();\n+    return res;\n }\n \n \ndiff --git a/src/Storages/MergeTree/BackgroundJobsAssignee.h b/src/Storages/MergeTree/BackgroundJobsAssignee.h\nindex 27e75a79b973..f1c7eadf5f72 100644\n--- a/src/Storages/MergeTree/BackgroundJobsAssignee.h\n+++ b/src/Storages/MergeTree/BackgroundJobsAssignee.h\n@@ -67,7 +67,7 @@ class BackgroundJobsAssignee : public WithContext\n \n     bool scheduleMergeMutateTask(ExecutableTaskPtr merge_task);\n     void scheduleFetchTask(ExecutableTaskPtr fetch_task);\n-    void scheduleMoveTask(ExecutableTaskPtr move_task);\n+    bool scheduleMoveTask(ExecutableTaskPtr move_task);\n     void scheduleCommonTask(ExecutableTaskPtr common_task, bool need_trigger);\n \n     /// Just call finish\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex d5a82fb032c5..d5a2204df8d1 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -192,6 +192,7 @@ namespace ErrorCodes\n     extern const int NOT_INITIALIZED;\n     extern const int SERIALIZATION_ERROR;\n     extern const int TOO_MANY_MUTATIONS;\n+    extern const int CANNOT_SCHEDULE_TASK;\n }\n \n static void checkSuspiciousIndices(const ASTFunction * index_function)\n@@ -4832,17 +4833,36 @@ void MergeTreeData::movePartitionToDisk(const ASTPtr & partition, const String &\n             throw Exception(ErrorCodes::UNKNOWN_DISK, \"All parts of partition '{}' are already on disk '{}'\", partition_id, disk->getName());\n     }\n \n-    MovePartsOutcome moves_outcome = movePartsToSpace(parts, std::static_pointer_cast<Space>(disk), local_context->getReadSettings(), local_context->getWriteSettings());\n-    switch (moves_outcome)\n+    if (parts_mover.moves_blocker.isCancelled())\n+        throw Exception(ErrorCodes::ABORTED, \"Cannot move parts because moves are manually disabled\");\n+\n+    auto moving_tagger = checkPartsForMove(parts, std::static_pointer_cast<Space>(disk));\n+    if (moving_tagger->parts_to_move.empty())\n+        throw Exception(ErrorCodes::NO_SUCH_DATA_PART, \"No parts to move are found in partition {}\", partition_id);\n+\n+    const auto & query_settings = local_context->getSettingsRef();\n+    std::future<MovePartsOutcome> moves_future = movePartsToSpace(moving_tagger, local_context->getReadSettings(), local_context->getWriteSettings(), query_settings.alter_move_to_space_execute_async);\n+\n+    if (query_settings.alter_move_to_space_execute_async && moves_future.wait_for(std::chrono::seconds(0)) != std::future_status::ready)\n     {\n-        case MovePartsOutcome::MovesAreCancelled:\n-            throw Exception(ErrorCodes::ABORTED, \"Cannot move parts because moves are manually disabled\");\n-        case MovePartsOutcome::NothingToMove:\n-            throw Exception(ErrorCodes::NO_SUCH_DATA_PART, \"No parts to move are found in partition {}\", partition_id);\n-        case MovePartsOutcome::MoveWasPostponedBecauseOfZeroCopy:\n-            throw Exception(ErrorCodes::PART_IS_TEMPORARILY_LOCKED, \"Move was not finished, because zero copy mode is enabled and someone other is moving the same parts right now\");\n-        case MovePartsOutcome::PartsMoved:\n-            break;\n+        return;\n+    }\n+    else\n+    {\n+        auto moves_outcome = moves_future.get();\n+        switch (moves_outcome)\n+        {\n+            case MovePartsOutcome::MovesAreCancelled:\n+                throw Exception(ErrorCodes::ABORTED, \"Cannot move parts because moves are manually disabled\");\n+            case MovePartsOutcome::NothingToMove:\n+                throw Exception(ErrorCodes::NO_SUCH_DATA_PART, \"No parts to move are found in partition {}\", partition_id);\n+            case MovePartsOutcome::MoveWasPostponedBecauseOfZeroCopy:\n+                throw Exception(ErrorCodes::PART_IS_TEMPORARILY_LOCKED, \"Move was not finished, because zero copy mode is enabled and someone other is moving the same parts right now\");\n+            case MovePartsOutcome::CannotScheduleMove:\n+                throw Exception(ErrorCodes::CANNOT_SCHEDULE_TASK, \"Cannot schedule move, no free threads, try to wait until all in-progress move finish or increase <background_move_pool_size>\");\n+            case MovePartsOutcome::PartsMoved:\n+                break;\n+        }\n     }\n }\n \n@@ -4895,17 +4915,36 @@ void MergeTreeData::movePartitionToVolume(const ASTPtr & partition, const String\n             throw Exception(ErrorCodes::UNKNOWN_DISK, \"All parts of partition '{}' are already on volume '{}'\", partition_id, volume->getName());\n     }\n \n-    MovePartsOutcome moves_outcome = movePartsToSpace(parts, std::static_pointer_cast<Space>(volume), local_context->getReadSettings(), local_context->getWriteSettings());\n-    switch (moves_outcome)\n+    if (parts_mover.moves_blocker.isCancelled())\n+        throw Exception(ErrorCodes::ABORTED, \"Cannot move parts because moves are manually disabled\");\n+\n+    auto moving_tagger = checkPartsForMove(parts, std::static_pointer_cast<Space>(volume));\n+    if (moving_tagger->parts_to_move.empty())\n+        throw Exception(ErrorCodes::NO_SUCH_DATA_PART, \"No parts to move are found in partition {}\", partition_id);\n+\n+    const auto & query_settings = local_context->getSettingsRef();\n+    std::future<MovePartsOutcome> moves_future = movePartsToSpace(moving_tagger, local_context->getReadSettings(), local_context->getWriteSettings(), query_settings.alter_move_to_space_execute_async);\n+\n+    if (query_settings.alter_move_to_space_execute_async && moves_future.wait_for(std::chrono::seconds(0)) != std::future_status::ready)\n     {\n-        case MovePartsOutcome::MovesAreCancelled:\n-            throw Exception(ErrorCodes::ABORTED, \"Cannot move parts because moves are manually disabled\");\n-        case MovePartsOutcome::NothingToMove:\n-            throw Exception(ErrorCodes::NO_SUCH_DATA_PART, \"No parts to move are found in partition {}\", partition_id);\n-        case MovePartsOutcome::MoveWasPostponedBecauseOfZeroCopy:\n-            throw Exception(ErrorCodes::PART_IS_TEMPORARILY_LOCKED, \"Move was not finished, because zero copy mode is enabled and someone other is moving the same parts right now\");\n-        case MovePartsOutcome::PartsMoved:\n-            break;\n+        return;\n+    }\n+    else\n+    {\n+        auto moves_outcome = moves_future.get();\n+        switch (moves_outcome)\n+        {\n+            case MovePartsOutcome::MovesAreCancelled:\n+                throw Exception(ErrorCodes::ABORTED, \"Cannot move parts because moves are manually disabled\");\n+            case MovePartsOutcome::NothingToMove:\n+                throw Exception(ErrorCodes::NO_SUCH_DATA_PART, \"No parts to move are found in partition {}\", partition_id);\n+            case MovePartsOutcome::MoveWasPostponedBecauseOfZeroCopy:\n+                throw Exception(ErrorCodes::PART_IS_TEMPORARILY_LOCKED, \"Move was not finished, because zero copy mode is enabled and someone other is moving the same parts right now\");\n+            case MovePartsOutcome::CannotScheduleMove:\n+                throw Exception(ErrorCodes::CANNOT_SCHEDULE_TASK, \"Cannot schedule move, no free threads, try to wait until all in-progress move finish or increase <background_move_pool_size>\");\n+            case MovePartsOutcome::PartsMoved:\n+                break;\n+        }\n     }\n }\n \n@@ -7476,16 +7515,33 @@ bool MergeTreeData::areBackgroundMovesNeeded() const\n     return policy->getVolumes().size() == 1 && policy->getVolumes()[0]->getDisks().size() > 1;\n }\n \n-MovePartsOutcome MergeTreeData::movePartsToSpace(const DataPartsVector & parts, SpacePtr space, const ReadSettings & read_settings, const WriteSettings & write_settings)\n+std::future<MovePartsOutcome> MergeTreeData::movePartsToSpace(const CurrentlyMovingPartsTaggerPtr & moving_tagger, const ReadSettings & read_settings, const WriteSettings & write_settings, bool async)\n {\n-    if (parts_mover.moves_blocker.isCancelled())\n-        return MovePartsOutcome::MovesAreCancelled;\n+    auto finish_move_promise = std::make_shared<std::promise<MovePartsOutcome>>();\n+    auto finish_move_future = finish_move_promise->get_future();\n \n-    auto moving_tagger = checkPartsForMove(parts, space);\n-    if (moving_tagger->parts_to_move.empty())\n-        return MovePartsOutcome::NothingToMove;\n+    if (async)\n+    {\n+        bool is_scheduled = background_moves_assignee.scheduleMoveTask(std::make_shared<ExecutableLambdaAdapter>(\n+            [this, finish_move_promise, moving_tagger, read_settings, write_settings] () mutable\n+            {\n+                auto outcome = moveParts(moving_tagger, read_settings, write_settings, /* wait_for_move_if_zero_copy= */ true);\n+\n+                finish_move_promise->set_value(outcome);\n+\n+                return outcome == MovePartsOutcome::PartsMoved;\n+            }, moves_assignee_trigger, getStorageID()));\n+\n+        if (!is_scheduled)\n+            finish_move_promise->set_value(MovePartsOutcome::CannotScheduleMove);\n+    }\n+    else\n+    {\n+        auto outcome = moveParts(moving_tagger, read_settings, write_settings, /* wait_for_move_if_zero_copy= */ true);\n+        finish_move_promise->set_value(outcome);\n+    }\n \n-    return moveParts(moving_tagger, read_settings, write_settings, /* wait_for_move_if_zero_copy= */ true);\n+    return finish_move_future;\n }\n \n MergeTreeData::CurrentlyMovingPartsTaggerPtr MergeTreeData::selectPartsForMove()\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 75e334af69fb..4c46980f3334 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -1359,8 +1359,6 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// method has different implementations for replicated and non replicated\n     /// MergeTree because they store mutations in different way.\n     virtual std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const DataPartPtr & part) const = 0;\n-    /// Moves part to specified space, used in ALTER ... MOVE ... queries\n-    MovePartsOutcome movePartsToSpace(const DataPartsVector & parts, SpacePtr space, const ReadSettings & read_settings, const WriteSettings & write_settings);\n \n     struct PartBackupEntries\n     {\n@@ -1513,6 +1511,9 @@ class MergeTreeData : public IStorage, public WithMutableContext\n \n     using CurrentlyMovingPartsTaggerPtr = std::shared_ptr<CurrentlyMovingPartsTagger>;\n \n+    /// Moves part to specified space, used in ALTER ... MOVE ... queries\n+    std::future<MovePartsOutcome> movePartsToSpace(const CurrentlyMovingPartsTaggerPtr & moving_tagger, const ReadSettings & read_settings, const WriteSettings & write_settings, bool async);\n+\n     /// Move selected parts to corresponding disks\n     MovePartsOutcome moveParts(const CurrentlyMovingPartsTaggerPtr & moving_tagger, const ReadSettings & read_settings, const WriteSettings & write_settings, bool wait_for_move_if_zero_copy);\n \ndiff --git a/src/Storages/MergeTree/MergeTreePartsMover.h b/src/Storages/MergeTree/MergeTreePartsMover.h\nindex f172dade40ef..b9109e51309c 100644\n--- a/src/Storages/MergeTree/MergeTreePartsMover.h\n+++ b/src/Storages/MergeTree/MergeTreePartsMover.h\n@@ -18,6 +18,7 @@ enum class MovePartsOutcome\n     NothingToMove,\n     MovesAreCancelled,\n     MoveWasPostponedBecauseOfZeroCopy,\n+    CannotScheduleMove,\n };\n \n /// Active part from storage and destination reservation where it has to be moved\n",
  "test_patch": "diff --git a/tests/integration/test_move_partition_to_volume_async/__init__.py b/tests/integration/test_move_partition_to_volume_async/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_move_partition_to_volume_async/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_move_partition_to_volume_async/configs/storage_policy.xml b/tests/integration/test_move_partition_to_volume_async/configs/storage_policy.xml\nnew file mode 100644\nindex 000000000000..f5d826562103\n--- /dev/null\n+++ b/tests/integration/test_move_partition_to_volume_async/configs/storage_policy.xml\n@@ -0,0 +1,38 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<clickhouse>\n+  <storage_configuration>\n+    <disks>\n+        <default/>\n+\n+        <s3>\n+            <type>s3</type>\n+            <endpoint>http://minio1:9001/root/data/</endpoint>\n+            <access_key_id>minio</access_key_id>\n+            <secret_access_key>minio123</secret_access_key>\n+        </s3>\n+\n+        <broken_s3>\n+            <type>s3</type>\n+            <endpoint>http://resolver:8083/root/data/</endpoint>\n+            <access_key_id>minio</access_key_id>\n+            <secret_access_key>minio123</secret_access_key>\n+        </broken_s3>\n+    </disks>\n+\n+    <policies>\n+        <slow_s3>\n+            <volumes>\n+                <main>\n+                    <disk>default</disk>\n+                </main>\n+                <broken>\n+                    <disk>broken_s3</disk>\n+                </broken>\n+            </volumes>\n+\n+            <move_factor>0.0</move_factor>\n+        </slow_s3>\n+    </policies>\n+  </storage_configuration>\n+\n+</clickhouse>\ndiff --git a/tests/integration/test_move_partition_to_volume_async/test.py b/tests/integration/test_move_partition_to_volume_async/test.py\nnew file mode 100644\nindex 000000000000..cdd2ee126c01\n--- /dev/null\n+++ b/tests/integration/test_move_partition_to_volume_async/test.py\n@@ -0,0 +1,133 @@\n+#!/usr/bin/env python3\n+import logging\n+import time\n+import os\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+from helpers.mock_servers import start_s3_mock, start_mock_servers\n+from helpers.utility import generate_values, replace_config, SafeThread\n+from helpers.wait_for_helpers import wait_for_delete_inactive_parts\n+from helpers.wait_for_helpers import wait_for_delete_empty_parts\n+from helpers.wait_for_helpers import wait_for_merges\n+\n+\n+SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def init_broken_s3(cluster):\n+    yield start_s3_mock(cluster, \"broken_s3\", \"8083\")\n+\n+\n+@pytest.fixture(scope=\"function\")\n+def broken_s3(init_broken_s3):\n+    init_broken_s3.reset()\n+    yield init_broken_s3\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def cluster():\n+    try:\n+        cluster = ClickHouseCluster(__file__)\n+        cluster.add_instance(\n+            \"node\",\n+            main_configs=[\n+                \"configs/storage_policy.xml\",\n+            ],\n+            with_minio=True,\n+        )\n+\n+        cluster.start()\n+        logging.info(\"Cluster started\")\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_async_alter_move(cluster, broken_s3):\n+    node = cluster.instances[\"node\"]\n+\n+    node.query(\n+        \"\"\"\n+    CREATE TABLE moving_table_async\n+    (\n+        key UInt64,\n+        data String\n+    )\n+    ENGINE MergeTree()\n+    ORDER BY tuple()\n+    SETTINGS storage_policy = 'slow_s3'\n+    \"\"\"\n+    )\n+\n+    node.query(\n+        \"INSERT INTO moving_table_async SELECT number, randomPrintableASCII(1000) FROM numbers(10000)\"\n+    )\n+\n+    broken_s3.setup_slow_answers(\n+        timeout=5,\n+        count=1000000,\n+    )\n+\n+    node.query(\n+        \"ALTER TABLE moving_table_async MOVE PARTITION tuple() TO DISK 'broken_s3'\",\n+        settings={\"alter_move_to_space_execute_async\": True},\n+        timeout=10,\n+    )\n+\n+    # not flaky, just introduce some wait\n+    time.sleep(3)\n+\n+    for i in range(100):\n+        count = node.query(\n+            \"SELECT count() FROM system.moves where table = 'moving_table_async'\"\n+        )\n+        if count == \"1\\n\":\n+            break\n+        time.sleep(0.1)\n+    else:\n+        assert False, \"Cannot find any moving background operation\"\n+\n+\n+def test_sync_alter_move(cluster, broken_s3):\n+    node = cluster.instances[\"node\"]\n+\n+    node.query(\n+        \"\"\"\n+    CREATE TABLE moving_table_sync\n+    (\n+        key UInt64,\n+        data String\n+    )\n+    ENGINE MergeTree()\n+    ORDER BY tuple()\n+    SETTINGS storage_policy = 'slow_s3'\n+    \"\"\"\n+    )\n+\n+    node.query(\n+        \"INSERT INTO moving_table_sync SELECT number, randomPrintableASCII(1000) FROM numbers(10000)\"\n+    )\n+\n+    broken_s3.reset()\n+\n+    node.query(\n+        \"ALTER TABLE moving_table_sync MOVE PARTITION tuple() TO DISK 'broken_s3'\",\n+        timeout=30,\n+    )\n+    # not flaky, just introduce some wait\n+    time.sleep(3)\n+\n+    assert (\n+        node.query(\"SELECT count() FROM system.moves where table = 'moving_table_sync'\")\n+        == \"0\\n\"\n+    )\n+\n+    assert (\n+        node.query(\n+            \"SELECT disk_name FROM system.parts WHERE table = 'moving_table_sync'\"\n+        )\n+        == \"broken_s3\\n\"\n+    )\n",
  "problem_statement": "Make ALTER TABLE MOVE PARTITION TO DISK asynchronous\nClickHouse has particular settings to make ALTER DELETE or MODIFY COLUMN asynchronous, but there is no such setting for MOVE PARTITION TO DISK.\r\n\r\nI believe it's a good idea to add such setting for MOVE PARTITION TO DISK\r\n\n",
  "hints_text": "",
  "created_at": "2023-11-15T17:36:45Z",
  "modified_files": [
    "src/Core/Settings.h",
    "src/Storages/MergeTree/BackgroundJobsAssignee.cpp",
    "src/Storages/MergeTree/BackgroundJobsAssignee.h",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeData.h",
    "src/Storages/MergeTree/MergeTreePartsMover.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_move_partition_to_volume_async/__init__.py",
    "b/tests/integration/test_move_partition_to_volume_async/configs/storage_policy.xml",
    "b/tests/integration/test_move_partition_to_volume_async/test.py"
  ]
}