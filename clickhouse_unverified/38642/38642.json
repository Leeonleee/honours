{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 38642,
  "instance_id": "ClickHouse__ClickHouse-38642",
  "issue_numbers": [
    "37359"
  ],
  "base_commit": "7146685f9d5a91207c103922288ffe5bb8f6bb94",
  "patch": "diff --git a/src/Columns/ColumnNullable.cpp b/src/Columns/ColumnNullable.cpp\nindex d8e98ec9406c..8d61f6e726aa 100644\n--- a/src/Columns/ColumnNullable.cpp\n+++ b/src/Columns/ColumnNullable.cpp\n@@ -793,4 +793,18 @@ ColumnPtr makeNullable(const ColumnPtr & column)\n     return ColumnNullable::create(column, ColumnUInt8::create(column->size(), 0));\n }\n \n+ColumnPtr makeNullableSafe(const ColumnPtr & column)\n+{\n+    if (isColumnNullable(*column))\n+        return column;\n+\n+    if (isColumnConst(*column))\n+        return ColumnConst::create(makeNullableSafe(assert_cast<const ColumnConst &>(*column).getDataColumnPtr()), column->size());\n+\n+    if (column->canBeInsideNullable())\n+        return makeNullable(column);\n+\n+    return column;\n+}\n+\n }\ndiff --git a/src/Columns/ColumnNullable.h b/src/Columns/ColumnNullable.h\nindex 52e57f7f0d04..e832f6d20e57 100644\n--- a/src/Columns/ColumnNullable.h\n+++ b/src/Columns/ColumnNullable.h\n@@ -223,5 +223,6 @@ class ColumnNullable final : public COWHelper<IColumn, ColumnNullable>\n };\n \n ColumnPtr makeNullable(const ColumnPtr & column);\n+ColumnPtr makeNullableSafe(const ColumnPtr & column);\n \n }\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex bda72f089eb1..71af2421cda9 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -132,6 +132,8 @@ static constexpr UInt64 operator\"\"_GiB(unsigned long long value)\n     M(UInt64, aggregation_memory_efficient_merge_threads, 0, \"Number of threads to use for merge intermediate aggregation results in memory efficient mode. When bigger, then more memory is consumed. 0 means - same as 'max_threads'.\", 0) \\\n     M(Bool, enable_positional_arguments, true, \"Enable positional arguments in ORDER BY, GROUP BY and LIMIT BY\", 0) \\\n     \\\n+    M(Bool, group_by_use_nulls, false, \"Treat columns mentioned in ROLLUP, CUBE or GROUPING SETS as Nullable\", 0) \\\n+    \\\n     M(UInt64, max_parallel_replicas, 1, \"The maximum number of replicas of each shard used when the query is executed. For consistency (to get different parts of the same partition), this option only works for the specified sampling key. The lag of the replicas is not controlled.\", 0) \\\n     M(UInt64, parallel_replicas_count, 0, \"\", 0) \\\n     M(UInt64, parallel_replica_offset, 0, \"\", 0) \\\ndiff --git a/src/DataTypes/DataTypeNullable.cpp b/src/DataTypes/DataTypeNullable.cpp\nindex b354b1278be8..a14fb785b967 100644\n--- a/src/DataTypes/DataTypeNullable.cpp\n+++ b/src/DataTypes/DataTypeNullable.cpp\n@@ -85,6 +85,13 @@ DataTypePtr makeNullable(const DataTypePtr & type)\n     return std::make_shared<DataTypeNullable>(type);\n }\n \n+DataTypePtr makeNullableSafe(const DataTypePtr & type)\n+{\n+    if (type->canBeInsideNullable())\n+        return makeNullable(type);\n+    return type;\n+}\n+\n DataTypePtr removeNullable(const DataTypePtr & type)\n {\n     if (type->isNullable())\ndiff --git a/src/DataTypes/DataTypeNullable.h b/src/DataTypes/DataTypeNullable.h\nindex c87e4f77008e..379119b364c0 100644\n--- a/src/DataTypes/DataTypeNullable.h\n+++ b/src/DataTypes/DataTypeNullable.h\n@@ -51,6 +51,7 @@ class DataTypeNullable final : public IDataType\n \n \n DataTypePtr makeNullable(const DataTypePtr & type);\n+DataTypePtr makeNullableSafe(const DataTypePtr & type);\n DataTypePtr removeNullable(const DataTypePtr & type);\n \n }\ndiff --git a/src/DataTypes/IDataType.h b/src/DataTypes/IDataType.h\nindex fce8906abe5f..a26c703cd8a1 100644\n--- a/src/DataTypes/IDataType.h\n+++ b/src/DataTypes/IDataType.h\n@@ -532,6 +532,12 @@ inline bool isBool(const DataTypePtr & data_type)\n     return data_type->getName() == \"Bool\";\n }\n \n+inline bool isAggregateFunction(const DataTypePtr & data_type)\n+{\n+    WhichDataType which(data_type);\n+    return which.isAggregateFunction();\n+}\n+\n template <typename DataType> constexpr bool IsDataTypeDecimal = false;\n template <typename DataType> constexpr bool IsDataTypeNumber = false;\n template <typename DataType> constexpr bool IsDataTypeDateOrDateTime = false;\ndiff --git a/src/Interpreters/ExpressionAnalyzer.cpp b/src/Interpreters/ExpressionAnalyzer.cpp\nindex 23258c600991..a4bdc4ed2523 100644\n--- a/src/Interpreters/ExpressionAnalyzer.cpp\n+++ b/src/Interpreters/ExpressionAnalyzer.cpp\n@@ -45,6 +45,9 @@\n \n #include <Common/typeid_cast.h>\n #include <Common/StringUtils/StringUtils.h>\n+#include <Columns/ColumnNullable.h>\n+#include <Core/ColumnsWithTypeAndName.h>\n+#include <DataTypes/IDataType.h>\n #include <Core/SettingsEnums.h>\n #include <Core/ColumnNumbers.h>\n #include <Core/Names.h>\n@@ -345,6 +348,7 @@ void ExpressionAnalyzer::analyzeAggregation(ActionsDAGPtr & temp_actions)\n                 group_by_kind = GroupByKind::GROUPING_SETS;\n             else\n                 group_by_kind = GroupByKind::ORDINARY;\n+            bool use_nulls = group_by_kind != GroupByKind::ORDINARY && getContext()->getSettingsRef().group_by_use_nulls;\n \n             /// For GROUPING SETS with multiple groups we always add virtual __grouping_set column\n             /// With set number, which is used as an additional key at the stage of merging aggregating data.\n@@ -399,7 +403,7 @@ void ExpressionAnalyzer::analyzeAggregation(ActionsDAGPtr & temp_actions)\n                             }\n                         }\n \n-                        NameAndTypePair key{column_name, node->result_type};\n+                        NameAndTypePair key{column_name, use_nulls ? makeNullableSafe(node->result_type) : node->result_type };\n \n                         grouping_set_list.push_back(key);\n \n@@ -453,7 +457,7 @@ void ExpressionAnalyzer::analyzeAggregation(ActionsDAGPtr & temp_actions)\n                         }\n                     }\n \n-                    NameAndTypePair key{column_name, node->result_type};\n+                    NameAndTypePair key = NameAndTypePair{ column_name, use_nulls ? makeNullableSafe(node->result_type) : node->result_type };\n \n                     /// Aggregation keys are uniqued.\n                     if (!unique_keys.contains(key.name))\n@@ -1489,6 +1493,28 @@ void SelectQueryExpressionAnalyzer::appendExpressionsAfterWindowFunctions(Expres\n     }\n }\n \n+void SelectQueryExpressionAnalyzer::appendGroupByModifiers(ActionsDAGPtr & before_aggregation, ExpressionActionsChain & chain, bool /* only_types */)\n+{\n+    const auto * select_query = getAggregatingQuery();\n+\n+    if (!select_query->groupBy() || !(select_query->group_by_with_rollup || select_query->group_by_with_cube))\n+        return;\n+\n+    auto source_columns = before_aggregation->getResultColumns();\n+    ColumnsWithTypeAndName result_columns;\n+\n+    for (const auto & source_column : source_columns)\n+    {\n+        if (source_column.type->canBeInsideNullable())\n+            result_columns.emplace_back(makeNullableSafe(source_column.type), source_column.name);\n+        else\n+            result_columns.push_back(source_column);\n+    }\n+    ExpressionActionsChain::Step & step = chain.lastStep(before_aggregation->getNamesAndTypesList());\n+\n+    step.actions() = ActionsDAG::makeConvertingActions(source_columns, result_columns, ActionsDAG::MatchColumnsMode::Position);\n+}\n+\n void SelectQueryExpressionAnalyzer::appendSelectSkipWindowExpressions(ExpressionActionsChain::Step & step, ASTPtr const & node)\n {\n     if (auto * function = node->as<ASTFunction>())\n@@ -1956,6 +1982,9 @@ ExpressionAnalysisResult::ExpressionAnalysisResult(\n             query_analyzer.appendAggregateFunctionsArguments(chain, only_types || !first_stage);\n             before_aggregation = chain.getLastActions();\n \n+            if (settings.group_by_use_nulls)\n+                query_analyzer.appendGroupByModifiers(before_aggregation, chain, only_types);\n+\n             finalize_chain(chain);\n \n             if (query_analyzer.appendHaving(chain, only_types || !second_stage))\ndiff --git a/src/Interpreters/ExpressionAnalyzer.h b/src/Interpreters/ExpressionAnalyzer.h\nindex 019cda8b924a..10c11499f14b 100644\n--- a/src/Interpreters/ExpressionAnalyzer.h\n+++ b/src/Interpreters/ExpressionAnalyzer.h\n@@ -412,6 +412,8 @@ class SelectQueryExpressionAnalyzer : public ExpressionAnalyzer\n     void appendExpressionsAfterWindowFunctions(ExpressionActionsChain & chain, bool only_types);\n     void appendSelectSkipWindowExpressions(ExpressionActionsChain::Step & step, ASTPtr const & node);\n \n+    void appendGroupByModifiers(ActionsDAGPtr & before_aggregation, ExpressionActionsChain & chain, bool only_types);\n+\n     /// After aggregation:\n     bool appendHaving(ExpressionActionsChain & chain, bool only_types);\n     ///  appendSelect\ndiff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex ac31588d2102..a05d353ac733 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -786,8 +786,16 @@ Block InterpreterSelectQuery::getSampleBlockImpl()\n         if (analysis_result.use_grouping_set_key)\n             res.insert({ nullptr, std::make_shared<DataTypeUInt64>(), \"__grouping_set\" });\n \n-        for (const auto & key : query_analyzer->aggregationKeys())\n-            res.insert({nullptr, header.getByName(key.name).type, key.name});\n+        if (context->getSettingsRef().group_by_use_nulls && analysis_result.use_grouping_set_key)\n+        {\n+            for (const auto & key : query_analyzer->aggregationKeys())\n+                res.insert({nullptr, makeNullableSafe(header.getByName(key.name).type), key.name});\n+        }\n+        else\n+        {\n+            for (const auto & key : query_analyzer->aggregationKeys())\n+                res.insert({nullptr, header.getByName(key.name).type, key.name});\n+        }\n \n         for (const auto & aggregate : query_analyzer->aggregates())\n         {\n@@ -2326,6 +2334,7 @@ void InterpreterSelectQuery::executeAggregation(QueryPlan & query_plan, const Ac\n         merge_threads,\n         temporary_data_merge_threads,\n         storage_has_evenly_distributed_read,\n+        settings.group_by_use_nulls,\n         std::move(group_by_info),\n         std::move(group_by_sort_description),\n         should_produce_results_in_order_of_bucket_number);\n@@ -2402,9 +2411,9 @@ void InterpreterSelectQuery::executeRollupOrCube(QueryPlan & query_plan, Modific\n \n     QueryPlanStepPtr step;\n     if (modificator == Modificator::ROLLUP)\n-        step = std::make_unique<RollupStep>(query_plan.getCurrentDataStream(), std::move(params), final);\n+        step = std::make_unique<RollupStep>(query_plan.getCurrentDataStream(), std::move(params), final, settings.group_by_use_nulls);\n     else if (modificator == Modificator::CUBE)\n-        step = std::make_unique<CubeStep>(query_plan.getCurrentDataStream(), std::move(params), final);\n+        step = std::make_unique<CubeStep>(query_plan.getCurrentDataStream(), std::move(params), final, settings.group_by_use_nulls);\n \n     query_plan.addStep(std::move(step));\n }\ndiff --git a/src/Processors/QueryPlan/AggregatingStep.cpp b/src/Processors/QueryPlan/AggregatingStep.cpp\nindex 0a4b12084ebe..f4e3749bd70b 100644\n--- a/src/Processors/QueryPlan/AggregatingStep.cpp\n+++ b/src/Processors/QueryPlan/AggregatingStep.cpp\n@@ -11,6 +11,7 @@\n #include <Processors/Merges/AggregatingSortedTransform.h>\n #include <Processors/Merges/FinishAggregatingInOrderTransform.h>\n #include <Interpreters/Aggregator.h>\n+#include <Functions/FunctionFactory.h>\n #include <Processors/QueryPlan/IQueryPlanStep.h>\n #include <Columns/ColumnFixedString.h>\n #include <DataTypes/DataTypesNumber.h>\n@@ -46,22 +47,32 @@ Block appendGroupingSetColumn(Block header)\n     return res;\n }\n \n-static Block appendGroupingColumn(Block block, const GroupingSetsParamsList & params)\n+static inline void convertToNullable(Block & header, const Names & keys)\n {\n-    if (params.empty())\n-        return block;\n+    for (const auto & key : keys)\n+    {\n+        auto & column = header.getByName(key);\n \n-    Block res;\n+        column.type = makeNullableSafe(column.type);\n+        column.column = makeNullableSafe(column.column);\n+    }\n+}\n \n-    size_t rows = block.rows();\n-    auto column = ColumnUInt64::create(rows);\n+Block generateOutputHeader(const Block & input_header, const Names & keys, bool use_nulls)\n+{\n+    auto header = appendGroupingSetColumn(input_header);\n+    if (use_nulls)\n+        convertToNullable(header, keys);\n+    return header;\n+}\n \n-    res.insert({ColumnPtr(std::move(column)), std::make_shared<DataTypeUInt64>(), \"__grouping_set\"});\n \n-    for (auto & col : block)\n-        res.insert(std::move(col));\n+static Block appendGroupingColumn(Block block, const Names & keys, const GroupingSetsParamsList & params, bool use_nulls)\n+{\n+    if (params.empty())\n+        return block;\n \n-    return res;\n+    return generateOutputHeader(block, keys, use_nulls);\n }\n \n AggregatingStep::AggregatingStep(\n@@ -74,11 +85,12 @@ AggregatingStep::AggregatingStep(\n     size_t merge_threads_,\n     size_t temporary_data_merge_threads_,\n     bool storage_has_evenly_distributed_read_,\n+    bool group_by_use_nulls_,\n     InputOrderInfoPtr group_by_info_,\n     SortDescription group_by_sort_description_,\n     bool should_produce_results_in_order_of_bucket_number_)\n     : ITransformingStep(\n-        input_stream_, appendGroupingColumn(params_.getHeader(input_stream_.header, final_), grouping_sets_params_), getTraits(should_produce_results_in_order_of_bucket_number_), false)\n+        input_stream_, appendGroupingColumn(params_.getHeader(input_stream_.header, final_), params_.keys, grouping_sets_params_, group_by_use_nulls_), getTraits(should_produce_results_in_order_of_bucket_number_), false)\n     , params(std::move(params_))\n     , grouping_sets_params(std::move(grouping_sets_params_))\n     , final(final_)\n@@ -87,6 +99,7 @@ AggregatingStep::AggregatingStep(\n     , merge_threads(merge_threads_)\n     , temporary_data_merge_threads(temporary_data_merge_threads_)\n     , storage_has_evenly_distributed_read(storage_has_evenly_distributed_read_)\n+    , group_by_use_nulls(group_by_use_nulls_)\n     , group_by_info(std::move(group_by_info_))\n     , group_by_sort_description(std::move(group_by_sort_description_))\n     , should_produce_results_in_order_of_bucket_number(should_produce_results_in_order_of_bucket_number_)\n@@ -217,6 +230,8 @@ void AggregatingStep::transformPipeline(QueryPipelineBuilder & pipeline, const B\n \n             assert(ports.size() == grouping_sets_size);\n             auto output_header = transform_params->getHeader();\n+            if (group_by_use_nulls)\n+                convertToNullable(output_header, params.keys);\n \n             for (size_t set_counter = 0; set_counter < grouping_sets_size; ++set_counter)\n             {\n@@ -236,6 +251,7 @@ void AggregatingStep::transformPipeline(QueryPipelineBuilder & pipeline, const B\n \n                 const auto & missing_columns = grouping_sets_params[set_counter].missing_keys;\n \n+                auto to_nullable_function = FunctionFactory::instance().get(\"toNullable\", nullptr);\n                 for (size_t i = 0; i < output_header.columns(); ++i)\n                 {\n                     auto & col = output_header.getByPosition(i);\n@@ -251,7 +267,13 @@ void AggregatingStep::transformPipeline(QueryPipelineBuilder & pipeline, const B\n                         index.push_back(node);\n                     }\n                     else\n-                        index.push_back(dag->getIndex()[header.getPositionByName(col.name)]);\n+                    {\n+                        const auto * column_node = dag->getIndex()[header.getPositionByName(col.name)];\n+                        if (group_by_use_nulls && column_node->result_type->canBeInsideNullable())\n+                            index.push_back(&dag->addFunction(to_nullable_function, { column_node }, col.name));\n+                        else\n+                            index.push_back(column_node);\n+                    }\n                 }\n \n                 dag->getIndex().swap(index);\n@@ -396,7 +418,7 @@ void AggregatingStep::updateOutputStream()\n {\n     output_stream = createOutputStream(\n         input_streams.front(),\n-        appendGroupingColumn(params.getHeader(input_streams.front().header, final), grouping_sets_params),\n+        appendGroupingColumn(params.getHeader(input_streams.front().header, final), params.keys, grouping_sets_params, group_by_use_nulls),\n         getDataStreamTraits());\n }\n \ndiff --git a/src/Processors/QueryPlan/AggregatingStep.h b/src/Processors/QueryPlan/AggregatingStep.h\nindex 0e982d769404..71130b65adb8 100644\n--- a/src/Processors/QueryPlan/AggregatingStep.h\n+++ b/src/Processors/QueryPlan/AggregatingStep.h\n@@ -20,6 +20,7 @@ struct GroupingSetsParams\n using GroupingSetsParamsList = std::vector<GroupingSetsParams>;\n \n Block appendGroupingSetColumn(Block header);\n+Block generateOutputHeader(const Block & input_header, const Names & keys, bool use_nulls);\n \n /// Aggregation. See AggregatingTransform.\n class AggregatingStep : public ITransformingStep\n@@ -35,6 +36,7 @@ class AggregatingStep : public ITransformingStep\n         size_t merge_threads_,\n         size_t temporary_data_merge_threads_,\n         bool storage_has_evenly_distributed_read_,\n+        bool group_by_use_nulls_,\n         InputOrderInfoPtr group_by_info_,\n         SortDescription group_by_sort_description_,\n         bool should_produce_results_in_order_of_bucket_number_);\n@@ -62,6 +64,7 @@ class AggregatingStep : public ITransformingStep\n     size_t temporary_data_merge_threads;\n \n     bool storage_has_evenly_distributed_read;\n+    bool group_by_use_nulls;\n \n     InputOrderInfoPtr group_by_info;\n     SortDescription group_by_sort_description;\ndiff --git a/src/Processors/QueryPlan/CubeStep.cpp b/src/Processors/QueryPlan/CubeStep.cpp\nindex b0c57491085c..52539dec75fb 100644\n--- a/src/Processors/QueryPlan/CubeStep.cpp\n+++ b/src/Processors/QueryPlan/CubeStep.cpp\n@@ -4,6 +4,7 @@\n #include <Processors/QueryPlan/AggregatingStep.h>\n #include <QueryPipeline/QueryPipelineBuilder.h>\n #include <DataTypes/DataTypesNumber.h>\n+#include <Functions/FunctionFactory.h>\n \n namespace DB\n {\n@@ -24,27 +25,41 @@ static ITransformingStep::Traits getTraits()\n     };\n }\n \n-CubeStep::CubeStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_)\n-    : ITransformingStep(input_stream_, appendGroupingSetColumn(params_.getHeader(input_stream_.header, final_)), getTraits())\n+CubeStep::CubeStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_, bool use_nulls_)\n+    : ITransformingStep(input_stream_, generateOutputHeader(params_.getHeader(input_stream_.header, final_), params_.keys, use_nulls_), getTraits())\n     , keys_size(params_.keys_size)\n     , params(std::move(params_))\n     , final(final_)\n+    , use_nulls(use_nulls_)\n {\n     /// Aggregation keys are distinct\n     for (const auto & key : params.keys)\n         output_stream->distinct_columns.insert(key);\n }\n \n-ProcessorPtr addGroupingSetForTotals(const Block & header, const BuildQueryPipelineSettings & settings, UInt64 grouping_set_number)\n+ProcessorPtr addGroupingSetForTotals(const Block & header, const Names & keys, bool use_nulls, const BuildQueryPipelineSettings & settings, UInt64 grouping_set_number)\n {\n     auto dag = std::make_shared<ActionsDAG>(header.getColumnsWithTypeAndName());\n+    auto & index = dag->getIndex();\n+\n+    if (use_nulls)\n+    {\n+        auto to_nullable = FunctionFactory::instance().get(\"toNullable\", nullptr);\n+        for (const auto & key : keys)\n+        {\n+            const auto * node = dag->getIndex()[header.getPositionByName(key)];\n+            if (node->result_type->canBeInsideNullable())\n+            {\n+                dag->addOrReplaceInIndex(dag->addFunction(to_nullable, { node }, node->result_name));\n+            }\n+        }\n+    }\n \n     auto grouping_col = ColumnUInt64::create(1, grouping_set_number);\n     const auto * grouping_node = &dag->addColumn(\n         {ColumnPtr(std::move(grouping_col)), std::make_shared<DataTypeUInt64>(), \"__grouping_set\"});\n \n     grouping_node = &dag->materializeNode(*grouping_node);\n-    auto & index = dag->getIndex();\n     index.insert(index.begin(), grouping_node);\n \n     auto expression = std::make_shared<ExpressionActions>(dag, settings.getActionsSettings());\n@@ -58,10 +73,10 @@ void CubeStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQue\n     pipeline.addSimpleTransform([&](const Block & header, QueryPipelineBuilder::StreamType stream_type) -> ProcessorPtr\n     {\n         if (stream_type == QueryPipelineBuilder::StreamType::Totals)\n-            return addGroupingSetForTotals(header, settings, (UInt64(1) << keys_size) - 1);\n+            return addGroupingSetForTotals(header, params.keys, use_nulls, settings, (UInt64(1) << keys_size) - 1);\n \n         auto transform_params = std::make_shared<AggregatingTransformParams>(header, std::move(params), final);\n-        return std::make_shared<CubeTransform>(header, std::move(transform_params));\n+        return std::make_shared<CubeTransform>(header, std::move(transform_params), use_nulls);\n     });\n }\n \n@@ -73,7 +88,7 @@ const Aggregator::Params & CubeStep::getParams() const\n void CubeStep::updateOutputStream()\n {\n     output_stream = createOutputStream(\n-        input_streams.front(), appendGroupingSetColumn(params.getHeader(input_streams.front().header, final)), getDataStreamTraits());\n+        input_streams.front(), generateOutputHeader(params.getHeader(input_streams.front().header, final), params.keys, use_nulls), getDataStreamTraits());\n \n     /// Aggregation keys are distinct\n     for (const auto & key : params.keys)\ndiff --git a/src/Processors/QueryPlan/CubeStep.h b/src/Processors/QueryPlan/CubeStep.h\nindex 87f22de7fc6b..8a03a33a0882 100644\n--- a/src/Processors/QueryPlan/CubeStep.h\n+++ b/src/Processors/QueryPlan/CubeStep.h\n@@ -13,7 +13,7 @@ using AggregatingTransformParamsPtr = std::shared_ptr<AggregatingTransformParams\n class CubeStep : public ITransformingStep\n {\n public:\n-    CubeStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_);\n+    CubeStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_, bool use_nulls_);\n \n     String getName() const override { return \"Cube\"; }\n \n@@ -26,6 +26,7 @@ class CubeStep : public ITransformingStep\n     size_t keys_size;\n     Aggregator::Params params;\n     bool final;\n+    bool use_nulls;\n };\n \n }\ndiff --git a/src/Processors/QueryPlan/RollupStep.cpp b/src/Processors/QueryPlan/RollupStep.cpp\nindex 169976195eac..3305f24602fd 100644\n--- a/src/Processors/QueryPlan/RollupStep.cpp\n+++ b/src/Processors/QueryPlan/RollupStep.cpp\n@@ -22,18 +22,19 @@ static ITransformingStep::Traits getTraits()\n     };\n }\n \n-RollupStep::RollupStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_)\n-    : ITransformingStep(input_stream_, appendGroupingSetColumn(params_.getHeader(input_stream_.header, final_)), getTraits())\n+RollupStep::RollupStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_, bool use_nulls_)\n+    : ITransformingStep(input_stream_, generateOutputHeader(params_.getHeader(input_stream_.header, final_), params_.keys, use_nulls_), getTraits())\n     , params(std::move(params_))\n     , keys_size(params.keys_size)\n     , final(final_)\n+    , use_nulls(use_nulls_)\n {\n     /// Aggregation keys are distinct\n     for (const auto & key : params.keys)\n         output_stream->distinct_columns.insert(key);\n }\n \n-ProcessorPtr addGroupingSetForTotals(const Block & header, const BuildQueryPipelineSettings & settings, UInt64 grouping_set_number);\n+ProcessorPtr addGroupingSetForTotals(const Block & header, const Names & keys, bool use_nulls, const BuildQueryPipelineSettings & settings, UInt64 grouping_set_number);\n \n void RollupStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings & settings)\n {\n@@ -42,10 +43,10 @@ void RollupStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQ\n     pipeline.addSimpleTransform([&](const Block & header, QueryPipelineBuilder::StreamType stream_type) -> ProcessorPtr\n     {\n         if (stream_type == QueryPipelineBuilder::StreamType::Totals)\n-            return addGroupingSetForTotals(header, settings, keys_size);\n+            return addGroupingSetForTotals(header, params.keys, use_nulls, settings, keys_size);\n \n         auto transform_params = std::make_shared<AggregatingTransformParams>(header, std::move(params), true);\n-        return std::make_shared<RollupTransform>(header, std::move(transform_params));\n+        return std::make_shared<RollupTransform>(header, std::move(transform_params), use_nulls);\n     });\n }\n \ndiff --git a/src/Processors/QueryPlan/RollupStep.h b/src/Processors/QueryPlan/RollupStep.h\nindex c59bf9f3ee90..866de7178fa9 100644\n--- a/src/Processors/QueryPlan/RollupStep.h\n+++ b/src/Processors/QueryPlan/RollupStep.h\n@@ -13,7 +13,7 @@ using AggregatingTransformParamsPtr = std::shared_ptr<AggregatingTransformParams\n class RollupStep : public ITransformingStep\n {\n public:\n-    RollupStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_);\n+    RollupStep(const DataStream & input_stream_, Aggregator::Params params_, bool final_, bool use_nulls_);\n \n     String getName() const override { return \"Rollup\"; }\n \n@@ -25,6 +25,7 @@ class RollupStep : public ITransformingStep\n     Aggregator::Params params;\n     size_t keys_size;\n     bool final;\n+    bool use_nulls;\n };\n \n }\ndiff --git a/src/Processors/Transforms/CubeTransform.cpp b/src/Processors/Transforms/CubeTransform.cpp\nindex b80ca29327fc..669aaddd1df6 100644\n--- a/src/Processors/Transforms/CubeTransform.cpp\n+++ b/src/Processors/Transforms/CubeTransform.cpp\n@@ -1,6 +1,7 @@\n #include <Processors/Transforms/CubeTransform.h>\n #include <Processors/Transforms/TotalsHavingTransform.h>\n #include <Processors/QueryPlan/AggregatingStep.h>\n+#include \"Processors/Transforms/RollupTransform.h\"\n \n namespace DB\n {\n@@ -9,61 +10,32 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n }\n \n-CubeTransform::CubeTransform(Block header, AggregatingTransformParamsPtr params_)\n-    : IAccumulatingTransform(std::move(header), appendGroupingSetColumn(params_->getHeader()))\n-    , params(std::move(params_))\n+CubeTransform::CubeTransform(Block header, AggregatingTransformParamsPtr params_, bool use_nulls_)\n+    : GroupByModifierTransform(std::move(header), params_, use_nulls_)\n     , aggregates_mask(getAggregatesMask(params->getHeader(), params->params.aggregates))\n {\n-    keys.reserve(params->params.keys_size);\n-    for (const auto & key : params->params.keys)\n-        keys.emplace_back(input.getHeader().getPositionByName(key));\n-\n     if (keys.size() >= 8 * sizeof(mask))\n         throw Exception(\"Too many keys are used for CubeTransform.\", ErrorCodes::LOGICAL_ERROR);\n }\n \n-Chunk CubeTransform::merge(Chunks && chunks, bool final)\n-{\n-    BlocksList rollup_blocks;\n-    for (auto & chunk : chunks)\n-        rollup_blocks.emplace_back(getInputPort().getHeader().cloneWithColumns(chunk.detachColumns()));\n-\n-    auto rollup_block = params->aggregator.mergeBlocks(rollup_blocks, final);\n-    auto num_rows = rollup_block.rows();\n-    return Chunk(rollup_block.getColumns(), num_rows);\n-}\n-\n-void CubeTransform::consume(Chunk chunk)\n-{\n-    consumed_chunks.emplace_back(std::move(chunk));\n-}\n-\n-MutableColumnPtr getColumnWithDefaults(Block const & header, size_t key, size_t n);\n-\n Chunk CubeTransform::generate()\n {\n     if (!consumed_chunks.empty())\n     {\n-        if (consumed_chunks.size() > 1)\n-            cube_chunk = merge(std::move(consumed_chunks), false);\n-        else\n-            cube_chunk = std::move(consumed_chunks.front());\n-\n-        consumed_chunks.clear();\n+        mergeConsumed();\n \n-        auto num_rows = cube_chunk.getNumRows();\n+        auto num_rows = current_chunk.getNumRows();\n         mask = (static_cast<UInt64>(1) << keys.size()) - 1;\n \n-        current_columns = cube_chunk.getColumns();\n+        current_columns = current_chunk.getColumns();\n         current_zero_columns.clear();\n         current_zero_columns.reserve(keys.size());\n \n-        auto const & input_header = getInputPort().getHeader();\n         for (auto key : keys)\n-            current_zero_columns.emplace_back(getColumnWithDefaults(input_header, key, num_rows));\n+            current_zero_columns.emplace_back(getColumnWithDefaults(key, num_rows));\n     }\n \n-    auto gen_chunk = std::move(cube_chunk);\n+    auto gen_chunk = std::move(current_chunk);\n \n     if (mask)\n     {\n@@ -78,7 +50,7 @@ Chunk CubeTransform::generate()\n \n         Chunks chunks;\n         chunks.emplace_back(std::move(columns), current_columns.front()->size());\n-        cube_chunk = merge(std::move(chunks), false);\n+        current_chunk = merge(std::move(chunks), !use_nulls, false);\n     }\n \n     finalizeChunk(gen_chunk, aggregates_mask);\ndiff --git a/src/Processors/Transforms/CubeTransform.h b/src/Processors/Transforms/CubeTransform.h\nindex bd33eabd7506..54a41e8f44e4 100644\n--- a/src/Processors/Transforms/CubeTransform.h\n+++ b/src/Processors/Transforms/CubeTransform.h\n@@ -1,6 +1,7 @@\n #pragma once\n #include <Processors/IInflatingTransform.h>\n #include <Processors/Transforms/AggregatingTransform.h>\n+#include <Processors/Transforms/RollupTransform.h>\n #include <Processors/Transforms/finalizeChunk.h>\n \n \n@@ -9,30 +10,23 @@ namespace DB\n \n /// Takes blocks after grouping, with non-finalized aggregate functions.\n /// Calculates all subsets of columns and aggregates over them.\n-class CubeTransform : public IAccumulatingTransform\n+class CubeTransform : public GroupByModifierTransform\n {\n public:\n-    CubeTransform(Block header, AggregatingTransformParamsPtr params);\n+    CubeTransform(Block header, AggregatingTransformParamsPtr params, bool use_nulls_);\n     String getName() const override { return \"CubeTransform\"; }\n \n protected:\n-    void consume(Chunk chunk) override;\n     Chunk generate() override;\n \n private:\n-    AggregatingTransformParamsPtr params;\n-    ColumnNumbers keys;\n     const ColumnsMask aggregates_mask;\n \n-    Chunks consumed_chunks;\n-    Chunk cube_chunk;\n     Columns current_columns;\n     Columns current_zero_columns;\n \n     UInt64 mask = 0;\n     UInt64 grouping_set = 0;\n-\n-    Chunk merge(Chunks && chunks, bool final);\n };\n \n }\ndiff --git a/src/Processors/Transforms/RollupTransform.cpp b/src/Processors/Transforms/RollupTransform.cpp\nindex e5351d1d5e21..a5d67fb2f157 100644\n--- a/src/Processors/Transforms/RollupTransform.cpp\n+++ b/src/Processors/Transforms/RollupTransform.cpp\n@@ -1,36 +1,80 @@\n #include <Processors/Transforms/RollupTransform.h>\n #include <Processors/Transforms/TotalsHavingTransform.h>\n #include <Processors/QueryPlan/AggregatingStep.h>\n+#include <Columns/ColumnNullable.h>\n \n namespace DB\n {\n \n-RollupTransform::RollupTransform(Block header, AggregatingTransformParamsPtr params_)\n-    : IAccumulatingTransform(std::move(header), appendGroupingSetColumn(params_->getHeader()))\n+GroupByModifierTransform::GroupByModifierTransform(Block header, AggregatingTransformParamsPtr params_, bool use_nulls_)\n+    : IAccumulatingTransform(std::move(header), generateOutputHeader(params_->getHeader(), params_->params.keys, use_nulls_))\n     , params(std::move(params_))\n-    , aggregates_mask(getAggregatesMask(params->getHeader(), params->params.aggregates))\n+    , use_nulls(use_nulls_)\n {\n     keys.reserve(params->params.keys_size);\n     for (const auto & key : params->params.keys)\n         keys.emplace_back(input.getHeader().getPositionByName(key));\n+\n+    intermediate_header = getOutputPort().getHeader();\n+    intermediate_header.erase(0);\n+\n+    if (use_nulls)\n+    {\n+        auto output_aggregator_params = params->params;\n+        output_aggregator = std::make_unique<Aggregator>(intermediate_header, output_aggregator_params);\n+    }\n }\n \n-void RollupTransform::consume(Chunk chunk)\n+void GroupByModifierTransform::consume(Chunk chunk)\n {\n     consumed_chunks.emplace_back(std::move(chunk));\n }\n \n-Chunk RollupTransform::merge(Chunks && chunks, bool final)\n+void GroupByModifierTransform::mergeConsumed()\n+{\n+    if (consumed_chunks.size() > 1)\n+        current_chunk = merge(std::move(consumed_chunks), true, false);\n+    else\n+        current_chunk = std::move(consumed_chunks.front());\n+\n+    size_t rows = current_chunk.getNumRows();\n+    auto columns = current_chunk.getColumns();\n+    if (use_nulls)\n+    {\n+        for (auto key : keys)\n+            columns[key] = makeNullableSafe(columns[key]);\n+    }\n+    current_chunk = Chunk{ columns, rows };\n+\n+    consumed_chunks.clear();\n+}\n+\n+Chunk GroupByModifierTransform::merge(Chunks && chunks, bool is_input, bool final)\n {\n-    BlocksList rollup_blocks;\n+    auto header = is_input ? getInputPort().getHeader() : intermediate_header;\n+\n+    BlocksList blocks;\n     for (auto & chunk : chunks)\n-        rollup_blocks.emplace_back(getInputPort().getHeader().cloneWithColumns(chunk.detachColumns()));\n+        blocks.emplace_back(header.cloneWithColumns(chunk.detachColumns()));\n \n-    auto rollup_block = params->aggregator.mergeBlocks(rollup_blocks, final);\n-    auto num_rows = rollup_block.rows();\n-    return Chunk(rollup_block.getColumns(), num_rows);\n+    auto current_block = is_input ? params->aggregator.mergeBlocks(blocks, final) : output_aggregator->mergeBlocks(blocks, final);\n+    auto num_rows = current_block.rows();\n+    return Chunk(current_block.getColumns(), num_rows);\n }\n \n+MutableColumnPtr GroupByModifierTransform::getColumnWithDefaults(size_t key, size_t n) const\n+{\n+    auto const & col = intermediate_header.getByPosition(key);\n+    auto result_column = col.column->cloneEmpty();\n+    col.type->insertManyDefaultsInto(*result_column, n);\n+    return result_column;\n+}\n+\n+RollupTransform::RollupTransform(Block header, AggregatingTransformParamsPtr params_, bool use_nulls_)\n+    : GroupByModifierTransform(std::move(header), params_, use_nulls_)\n+    , aggregates_mask(getAggregatesMask(params->getHeader(), params->params.aggregates))\n+{}\n+\n MutableColumnPtr getColumnWithDefaults(Block const & header, size_t key, size_t n)\n {\n     auto const & col = header.getByPosition(key);\n@@ -43,16 +87,11 @@ Chunk RollupTransform::generate()\n {\n     if (!consumed_chunks.empty())\n     {\n-        if (consumed_chunks.size() > 1)\n-            rollup_chunk = merge(std::move(consumed_chunks), false);\n-        else\n-            rollup_chunk = std::move(consumed_chunks.front());\n-\n-        consumed_chunks.clear();\n+        mergeConsumed();\n         last_removed_key = keys.size();\n     }\n \n-    auto gen_chunk = std::move(rollup_chunk);\n+    auto gen_chunk = std::move(current_chunk);\n \n     if (last_removed_key)\n     {\n@@ -61,11 +100,11 @@ Chunk RollupTransform::generate()\n \n         auto num_rows = gen_chunk.getNumRows();\n         auto columns = gen_chunk.getColumns();\n-        columns[key] = getColumnWithDefaults(getInputPort().getHeader(), key, num_rows);\n+        columns[key] = getColumnWithDefaults(key, num_rows);\n \n         Chunks chunks;\n         chunks.emplace_back(std::move(columns), num_rows);\n-        rollup_chunk = merge(std::move(chunks), false);\n+        current_chunk = merge(std::move(chunks), !use_nulls, false);\n     }\n \n     finalizeChunk(gen_chunk, aggregates_mask);\ndiff --git a/src/Processors/Transforms/RollupTransform.h b/src/Processors/Transforms/RollupTransform.h\nindex 1630df235798..e9fa0818779a 100644\n--- a/src/Processors/Transforms/RollupTransform.h\n+++ b/src/Processors/Transforms/RollupTransform.h\n@@ -1,4 +1,6 @@\n #pragma once\n+#include <memory>\n+#include <Core/ColumnNumbers.h>\n #include <Processors/IAccumulatingTransform.h>\n #include <Processors/Transforms/AggregatingTransform.h>\n #include <Processors/Transforms/finalizeChunk.h>\n@@ -6,29 +8,49 @@\n namespace DB\n {\n \n+struct GroupByModifierTransform : public IAccumulatingTransform\n+{\n+    GroupByModifierTransform(Block header, AggregatingTransformParamsPtr params_, bool use_nulls_);\n+\n+protected:\n+    void consume(Chunk chunk) override;\n+\n+    void mergeConsumed();\n+\n+    Chunk merge(Chunks && chunks, bool is_input, bool final);\n+\n+    MutableColumnPtr getColumnWithDefaults(size_t key, size_t n) const;\n+\n+    AggregatingTransformParamsPtr params;\n+\n+    bool use_nulls;\n+\n+    ColumnNumbers keys;\n+\n+    std::unique_ptr<Aggregator> output_aggregator;\n+\n+    Block intermediate_header;\n+\n+    Chunks consumed_chunks;\n+    Chunk current_chunk;\n+};\n+\n /// Takes blocks after grouping, with non-finalized aggregate functions.\n /// Calculates subtotals and grand totals values for a set of columns.\n-class RollupTransform : public IAccumulatingTransform\n+class RollupTransform : public GroupByModifierTransform\n {\n public:\n-    RollupTransform(Block header, AggregatingTransformParamsPtr params);\n+    RollupTransform(Block header, AggregatingTransformParamsPtr params, bool use_nulls_);\n     String getName() const override { return \"RollupTransform\"; }\n \n protected:\n-    void consume(Chunk chunk) override;\n     Chunk generate() override;\n \n private:\n-    AggregatingTransformParamsPtr params;\n-    ColumnNumbers keys;\n     const ColumnsMask aggregates_mask;\n \n-    Chunks consumed_chunks;\n-    Chunk rollup_chunk;\n     size_t last_removed_key = 0;\n     size_t set_counter = 0;\n-\n-    Chunk merge(Chunks && chunks, bool final);\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex d9fc8ccaf42b..3916eae1556e 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -383,6 +383,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(\n                     merge_threads,\n                     temporary_data_merge_threads,\n                     /* storage_has_evenly_distributed_read_= */ false,\n+                    /* group_by_use_nulls */ false,\n                     std::move(group_by_info),\n                     std::move(group_by_sort_description),\n                     should_produce_results_in_order_of_bucket_number);\n",
  "test_patch": "diff --git a/docker/test/stress/stress b/docker/test/stress/stress\nindex ab25d13695bb..6d90b9d5437b 100755\n--- a/docker/test/stress/stress\n+++ b/docker/test/stress/stress\n@@ -46,6 +46,9 @@ def get_options(i, backward_compatibility_check):\n     if i == 13:\n         client_options.append(\"memory_tracker_fault_probability=0.001\")\n \n+    if i % 2 == 1 and not backward_compatibility_check:\n+        client_options.append(\"group_by_use_nulls=1\")\n+\n     if client_options:\n         options.append(\" --client-option \" + \" \".join(client_options))\n \ndiff --git a/tests/queries/0_stateless/02313_group_by_modifiers_with_non-default_types.reference b/tests/queries/0_stateless/02313_group_by_modifiers_with_non_default_types.reference\nsimilarity index 100%\nrename from tests/queries/0_stateless/02313_group_by_modifiers_with_non-default_types.reference\nrename to tests/queries/0_stateless/02313_group_by_modifiers_with_non_default_types.reference\ndiff --git a/tests/queries/0_stateless/02313_group_by_modifiers_with_non-default_types.sql b/tests/queries/0_stateless/02313_group_by_modifiers_with_non_default_types.sql\nsimilarity index 100%\nrename from tests/queries/0_stateless/02313_group_by_modifiers_with_non-default_types.sql\nrename to tests/queries/0_stateless/02313_group_by_modifiers_with_non_default_types.sql\ndiff --git a/tests/queries/0_stateless/02343_group_by_use_nulls.reference b/tests/queries/0_stateless/02343_group_by_use_nulls.reference\nnew file mode 100644\nindex 000000000000..24b7bb5277cb\n--- /dev/null\n+++ b/tests/queries/0_stateless/02343_group_by_use_nulls.reference\n@@ -0,0 +1,215 @@\n+-- { echoOn }\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+0\t0\t0\n+0\t\\N\t0\n+1\t1\t1\n+1\t\\N\t1\n+2\t0\t2\n+2\t\\N\t2\n+3\t1\t3\n+3\t\\N\t3\n+4\t0\t4\n+4\t\\N\t4\n+5\t1\t5\n+5\t\\N\t5\n+6\t0\t6\n+6\t\\N\t6\n+7\t1\t7\n+7\t\\N\t7\n+8\t0\t8\n+8\t\\N\t8\n+9\t1\t9\n+9\t\\N\t9\n+\\N\t\\N\t45\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+0\t0\t0\n+0\t0\t0\n+0\t0\t45\n+1\t0\t1\n+1\t1\t1\n+2\t0\t2\n+2\t0\t2\n+3\t0\t3\n+3\t1\t3\n+4\t0\t4\n+4\t0\t4\n+5\t0\t5\n+5\t1\t5\n+6\t0\t6\n+6\t0\t6\n+7\t0\t7\n+7\t1\t7\n+8\t0\t8\n+8\t0\t8\n+9\t0\t9\n+9\t1\t9\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+0\t0\t0\n+0\t\\N\t0\n+1\t1\t1\n+1\t\\N\t1\n+2\t0\t2\n+2\t\\N\t2\n+3\t1\t3\n+3\t\\N\t3\n+4\t0\t4\n+4\t\\N\t4\n+5\t1\t5\n+5\t\\N\t5\n+6\t0\t6\n+6\t\\N\t6\n+7\t1\t7\n+7\t\\N\t7\n+8\t0\t8\n+8\t\\N\t8\n+9\t1\t9\n+9\t\\N\t9\n+\\N\t0\t20\n+\\N\t1\t25\n+\\N\t\\N\t45\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+0\t0\t0\n+0\t0\t0\n+0\t0\t20\n+0\t0\t45\n+0\t1\t25\n+1\t0\t1\n+1\t1\t1\n+2\t0\t2\n+2\t0\t2\n+3\t0\t3\n+3\t1\t3\n+4\t0\t4\n+4\t0\t4\n+5\t0\t5\n+5\t1\t5\n+6\t0\t6\n+6\t0\t6\n+7\t0\t7\n+7\t1\t7\n+8\t0\t8\n+8\t0\t8\n+9\t0\t9\n+9\t1\t9\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM numbers(10)\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 1;\n+0\t\\N\t0\n+1\t\\N\t1\n+2\t\\N\t2\n+3\t\\N\t3\n+4\t\\N\t4\n+5\t\\N\t5\n+6\t\\N\t6\n+7\t\\N\t7\n+8\t\\N\t8\n+9\t\\N\t9\n+\\N\t0\t20\n+\\N\t1\t25\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM numbers(10)\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 0;\n+0\t0\t0\n+0\t0\t20\n+0\t1\t25\n+1\t0\t1\n+2\t0\t2\n+3\t0\t3\n+4\t0\t4\n+5\t0\t5\n+6\t0\t6\n+7\t0\t7\n+8\t0\t8\n+9\t0\t9\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY ROLLUP(number, number % 2) WITH TOTALS\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+0\t0\t0\n+0\t\\N\t0\n+1\t1\t1\n+1\t\\N\t1\n+2\t0\t2\n+2\t\\N\t2\n+3\t1\t3\n+3\t\\N\t3\n+4\t0\t4\n+4\t\\N\t4\n+5\t1\t5\n+5\t\\N\t5\n+6\t0\t6\n+6\t\\N\t6\n+7\t1\t7\n+7\t\\N\t7\n+8\t0\t8\n+8\t\\N\t8\n+9\t1\t9\n+9\t\\N\t9\n+\\N\t\\N\t45\n+\n+0\t0\t45\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY CUBE(number, number % 2) WITH TOTALS\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+0\t0\t0\n+0\t\\N\t0\n+1\t1\t1\n+1\t\\N\t1\n+2\t0\t2\n+2\t\\N\t2\n+3\t1\t3\n+3\t\\N\t3\n+4\t0\t4\n+4\t\\N\t4\n+5\t1\t5\n+5\t\\N\t5\n+6\t0\t6\n+6\t\\N\t6\n+7\t1\t7\n+7\t\\N\t7\n+8\t0\t8\n+8\t\\N\t8\n+9\t1\t9\n+9\t\\N\t9\n+\\N\t0\t20\n+\\N\t1\t25\n+\\N\t\\N\t45\n+\n+0\t0\t45\ndiff --git a/tests/queries/0_stateless/02343_group_by_use_nulls.sql b/tests/queries/0_stateless/02343_group_by_use_nulls.sql\nnew file mode 100644\nindex 000000000000..a14db8240139\n--- /dev/null\n+++ b/tests/queries/0_stateless/02343_group_by_use_nulls.sql\n@@ -0,0 +1,62 @@\n+-- { echoOn }\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM numbers(10)\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 1;\n+\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM numbers(10)\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 0;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY ROLLUP(number, number % 2) WITH TOTALS\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM numbers(10)\n+GROUP BY CUBE(number, number % 2) WITH TOTALS\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\ndiff --git a/tests/queries/0_stateless/02343_group_by_use_nulls_distributed.reference b/tests/queries/0_stateless/02343_group_by_use_nulls_distributed.reference\nnew file mode 100644\nindex 000000000000..7a9263e883c4\n--- /dev/null\n+++ b/tests/queries/0_stateless/02343_group_by_use_nulls_distributed.reference\n@@ -0,0 +1,157 @@\n+-- { echoOn }\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+0\t0\t0\n+0\t\\N\t0\n+1\t1\t2\n+1\t\\N\t2\n+2\t0\t4\n+2\t\\N\t4\n+3\t1\t6\n+3\t\\N\t6\n+4\t0\t8\n+4\t\\N\t8\n+5\t1\t10\n+5\t\\N\t10\n+6\t0\t12\n+6\t\\N\t12\n+7\t1\t14\n+7\t\\N\t14\n+8\t0\t16\n+8\t\\N\t16\n+9\t1\t18\n+9\t\\N\t18\n+\\N\t\\N\t90\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+0\t0\t0\n+0\t0\t0\n+0\t0\t90\n+1\t0\t2\n+1\t1\t2\n+2\t0\t4\n+2\t0\t4\n+3\t0\t6\n+3\t1\t6\n+4\t0\t8\n+4\t0\t8\n+5\t0\t10\n+5\t1\t10\n+6\t0\t12\n+6\t0\t12\n+7\t0\t14\n+7\t1\t14\n+8\t0\t16\n+8\t0\t16\n+9\t0\t18\n+9\t1\t18\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+0\t0\t0\n+0\t\\N\t0\n+1\t1\t2\n+1\t\\N\t2\n+2\t0\t4\n+2\t\\N\t4\n+3\t1\t6\n+3\t\\N\t6\n+4\t0\t8\n+4\t\\N\t8\n+5\t1\t10\n+5\t\\N\t10\n+6\t0\t12\n+6\t\\N\t12\n+7\t1\t14\n+7\t\\N\t14\n+8\t0\t16\n+8\t\\N\t16\n+9\t1\t18\n+9\t\\N\t18\n+\\N\t0\t40\n+\\N\t1\t50\n+\\N\t\\N\t90\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+0\t0\t0\n+0\t0\t0\n+0\t0\t40\n+0\t0\t90\n+0\t1\t50\n+1\t0\t2\n+1\t1\t2\n+2\t0\t4\n+2\t0\t4\n+3\t0\t6\n+3\t1\t6\n+4\t0\t8\n+4\t0\t8\n+5\t0\t10\n+5\t1\t10\n+6\t0\t12\n+6\t0\t12\n+7\t0\t14\n+7\t1\t14\n+8\t0\t16\n+8\t0\t16\n+9\t0\t18\n+9\t1\t18\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 1;\n+0\t\\N\t0\n+1\t\\N\t2\n+2\t\\N\t4\n+3\t\\N\t6\n+4\t\\N\t8\n+5\t\\N\t10\n+6\t\\N\t12\n+7\t\\N\t14\n+8\t\\N\t16\n+9\t\\N\t18\n+\\N\t0\t40\n+\\N\t1\t50\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 0;\n+0\t0\t0\n+0\t0\t40\n+0\t1\t50\n+1\t0\t2\n+2\t0\t4\n+3\t0\t6\n+4\t0\t8\n+5\t0\t10\n+6\t0\t12\n+7\t0\t14\n+8\t0\t16\n+9\t0\t18\ndiff --git a/tests/queries/0_stateless/02343_group_by_use_nulls_distributed.sql b/tests/queries/0_stateless/02343_group_by_use_nulls_distributed.sql\nnew file mode 100644\nindex 000000000000..15ac1127de79\n--- /dev/null\n+++ b/tests/queries/0_stateless/02343_group_by_use_nulls_distributed.sql\n@@ -0,0 +1,51 @@\n+-- { echoOn }\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY ROLLUP(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=1;\n+\n+SELECT number, number % 2, sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY CUBE(number, number % 2)\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls=0;\n+\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 1;\n+\n+SELECT\n+    number,\n+    number % 2,\n+    sum(number) AS val\n+FROM remote('127.0.0.{2,3}', numbers(10))\n+GROUP BY\n+    GROUPING SETS (\n+        (number),\n+        (number % 2)\n+    )\n+ORDER BY (number, number % 2, val)\n+SETTINGS group_by_use_nulls = 0;\n+\ndiff --git a/tests/queries/1_stateful/00173_group_by_use_nulls.reference b/tests/queries/1_stateful/00173_group_by_use_nulls.reference\nnew file mode 100644\nindex 000000000000..02723bf14dd4\n--- /dev/null\n+++ b/tests/queries/1_stateful/00173_group_by_use_nulls.reference\n@@ -0,0 +1,10 @@\n+1704509\t1384\n+732797\t1336\n+598875\t1384\n+792887\t1336\n+3807842\t1336\n+25703952\t1336\n+716829\t1384\n+59183\t1336\n+33010362\t1336\n+800784\t1336\ndiff --git a/tests/queries/1_stateful/00173_group_by_use_nulls.sql b/tests/queries/1_stateful/00173_group_by_use_nulls.sql\nnew file mode 100644\nindex 000000000000..7acacc4e579d\n--- /dev/null\n+++ b/tests/queries/1_stateful/00173_group_by_use_nulls.sql\n@@ -0,0 +1,10 @@\n+SELECT\n+    CounterID AS k,\n+    quantileBFloat16(0.5)(ResolutionWidth)\n+FROM remote('127.0.0.{1,2}', test, hits)\n+GROUP BY k\n+ORDER BY\n+    count() DESC,\n+    CounterID ASC\n+LIMIT 10\n+SETTINGS group_by_use_nulls = 1;\n",
  "problem_statement": "A setting `rollup_use_nulls` for standard compatibility\n**Use case**\r\n\r\nWhen it is set and GROUP BY has WITH ROLLUP, WITH CUBE, or GROUPING SETS, the columns from GROUP BY are interpreted as Nullable.\r\n\n",
  "hints_text": "",
  "created_at": "2022-06-30T15:18:31Z"
}