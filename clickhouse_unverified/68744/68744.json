{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 68744,
  "instance_id": "ClickHouse__ClickHouse-68744",
  "issue_numbers": [
    "68532"
  ],
  "base_commit": "8921eec6725ce35c9896d5812fe7f7066ce91e7e",
  "patch": "diff --git a/src/Interpreters/Aggregator.h b/src/Interpreters/Aggregator.h\nindex f4f1e9a1df3b..2cb04fc7c51e 100644\n--- a/src/Interpreters/Aggregator.h\n+++ b/src/Interpreters/Aggregator.h\n@@ -59,6 +59,18 @@ class CompiledAggregateFunctionsHolder;\n class NativeWriter;\n struct OutputBlockColumns;\n \n+struct GroupingSetsParams\n+{\n+    GroupingSetsParams() = default;\n+\n+    GroupingSetsParams(Names used_keys_, Names missing_keys_) : used_keys(std::move(used_keys_)), missing_keys(std::move(missing_keys_)) { }\n+\n+    Names used_keys;\n+    Names missing_keys;\n+};\n+\n+using GroupingSetsParamsList = std::vector<GroupingSetsParams>;\n+\n /** How are \"total\" values calculated with WITH TOTALS?\n   * (For more details, see TotalsHavingTransform.)\n   *\ndiff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 0c79f4310ce5..ca0e84a52670 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -347,6 +347,27 @@ bool shouldIgnoreQuotaAndLimits(const StorageID & table_id)\n     return false;\n }\n \n+GroupingSetsParamsList getAggregatorGroupingSetsParams(const NamesAndTypesLists & aggregation_keys_list, const Names & all_keys)\n+{\n+    GroupingSetsParamsList result;\n+\n+    for (const auto & aggregation_keys : aggregation_keys_list)\n+    {\n+        NameSet keys;\n+        for (const auto & key : aggregation_keys)\n+            keys.insert(key.name);\n+\n+        Names missing_keys;\n+        for (const auto & key : all_keys)\n+            if (!keys.contains(key))\n+                missing_keys.push_back(key);\n+\n+        result.emplace_back(aggregation_keys.getNames(), std::move(missing_keys));\n+    }\n+\n+    return result;\n+}\n+\n }\n \n InterpreterSelectQuery::InterpreterSelectQuery(\n@@ -2005,13 +2026,12 @@ static void executeMergeAggregatedImpl(\n     bool has_grouping_sets,\n     const Settings & settings,\n     const NamesAndTypesList & aggregation_keys,\n+    const NamesAndTypesLists & aggregation_keys_list,\n     const AggregateDescriptions & aggregates,\n     bool should_produce_results_in_order_of_bucket_number,\n     SortDescription group_by_sort_description)\n {\n     auto keys = aggregation_keys.getNames();\n-    if (has_grouping_sets)\n-        keys.insert(keys.begin(), \"__grouping_set\");\n \n     /** There are two modes of distributed aggregation.\n       *\n@@ -2029,10 +2049,12 @@ static void executeMergeAggregatedImpl(\n       */\n \n     Aggregator::Params params(keys, aggregates, overflow_row, settings.max_threads, settings.max_block_size, settings.min_hit_rate_to_use_consecutive_keys_optimization);\n+    auto grouping_sets_params = getAggregatorGroupingSetsParams(aggregation_keys_list, keys);\n \n     auto merging_aggregated = std::make_unique<MergingAggregatedStep>(\n         query_plan.getCurrentDataStream(),\n         params,\n+        grouping_sets_params,\n         final,\n         /// Grouping sets don't work with distributed_aggregation_memory_efficient enabled (#43989)\n         settings.distributed_aggregation_memory_efficient && is_remote_storage && !has_grouping_sets,\n@@ -2653,30 +2675,6 @@ static Aggregator::Params getAggregatorParams(\n     };\n }\n \n-static GroupingSetsParamsList getAggregatorGroupingSetsParams(const SelectQueryExpressionAnalyzer & query_analyzer, const Names & all_keys)\n-{\n-    GroupingSetsParamsList result;\n-    if (query_analyzer.useGroupingSetKey())\n-    {\n-        auto const & aggregation_keys_list = query_analyzer.aggregationKeysList();\n-\n-        for (const auto & aggregation_keys : aggregation_keys_list)\n-        {\n-            NameSet keys;\n-            for (const auto & key : aggregation_keys)\n-                keys.insert(key.name);\n-\n-            Names missing_keys;\n-            for (const auto & key : all_keys)\n-                if (!keys.contains(key))\n-                    missing_keys.push_back(key);\n-\n-            result.emplace_back(aggregation_keys.getNames(), std::move(missing_keys));\n-        }\n-    }\n-    return result;\n-}\n-\n void InterpreterSelectQuery::executeAggregation(QueryPlan & query_plan, const ActionsAndProjectInputsFlagPtr & expression, bool overflow_row, bool final, InputOrderInfoPtr group_by_info)\n {\n     executeExpression(query_plan, expression, \"Before GROUP BY\");\n@@ -2696,7 +2694,7 @@ void InterpreterSelectQuery::executeAggregation(QueryPlan & query_plan, const Ac\n         settings.group_by_two_level_threshold,\n         settings.group_by_two_level_threshold_bytes);\n \n-    auto grouping_sets_params = getAggregatorGroupingSetsParams(*query_analyzer, keys);\n+    auto grouping_sets_params = getAggregatorGroupingSetsParams(query_analyzer->aggregationKeysList(), keys);\n \n     SortDescription group_by_sort_description;\n     SortDescription sort_description_for_merging;\n@@ -2764,6 +2762,7 @@ void InterpreterSelectQuery::executeMergeAggregated(QueryPlan & query_plan, bool\n         has_grouping_sets,\n         context->getSettingsRef(),\n         query_analyzer->aggregationKeys(),\n+        query_analyzer->aggregationKeysList(),\n         query_analyzer->aggregates(),\n         should_produce_results_in_order_of_bucket_number,\n         std::move(group_by_sort_description));\ndiff --git a/src/Planner/Planner.cpp b/src/Planner/Planner.cpp\nindex d3d20c6fba0f..7b5101c5c7d4 100644\n--- a/src/Planner/Planner.cpp\n+++ b/src/Planner/Planner.cpp\n@@ -504,8 +504,6 @@ void addMergingAggregatedStep(QueryPlan & query_plan,\n       */\n \n     auto keys = aggregation_analysis_result.aggregation_keys;\n-    if (!aggregation_analysis_result.grouping_sets_parameters_list.empty())\n-        keys.insert(keys.begin(), \"__grouping_set\");\n \n     Aggregator::Params params(keys,\n         aggregation_analysis_result.aggregate_descriptions,\n@@ -530,6 +528,7 @@ void addMergingAggregatedStep(QueryPlan & query_plan,\n     auto merging_aggregated = std::make_unique<MergingAggregatedStep>(\n         query_plan.getCurrentDataStream(),\n         params,\n+        aggregation_analysis_result.grouping_sets_parameters_list,\n         query_analysis_result.aggregate_final,\n         /// Grouping sets don't work with distributed_aggregation_memory_efficient enabled (#43989)\n         settings.distributed_aggregation_memory_efficient && (is_remote_storage || parallel_replicas_from_merge_tree) && !query_analysis_result.aggregation_with_rollup_or_cube_or_grouping_sets,\ndiff --git a/src/Processors/QueryPlan/AggregatingStep.cpp b/src/Processors/QueryPlan/AggregatingStep.cpp\nindex 8a5ed7fde650..a4d707704b11 100644\n--- a/src/Processors/QueryPlan/AggregatingStep.cpp\n+++ b/src/Processors/QueryPlan/AggregatingStep.cpp\n@@ -151,6 +151,61 @@ void AggregatingStep::applyOrder(SortDescription sort_description_for_merging_,\n     explicit_sorting_required_for_aggregation_in_order = false;\n }\n \n+ActionsDAG AggregatingStep::makeCreatingMissingKeysForGroupingSetDAG(\n+    const Block & in_header,\n+    const Block & out_header,\n+    const GroupingSetsParamsList & grouping_sets_params,\n+    UInt64 group,\n+    bool group_by_use_nulls)\n+{\n+    /// Here we create a DAG which fills missing keys and adds `__grouping_set` column\n+    ActionsDAG dag(in_header.getColumnsWithTypeAndName());\n+    ActionsDAG::NodeRawConstPtrs outputs;\n+    outputs.reserve(out_header.columns() + 1);\n+\n+    auto grouping_col = ColumnConst::create(ColumnUInt64::create(1, group), 0);\n+    const auto * grouping_node = &dag.addColumn(\n+        {ColumnPtr(std::move(grouping_col)), std::make_shared<DataTypeUInt64>(), \"__grouping_set\"});\n+\n+    grouping_node = &dag.materializeNode(*grouping_node);\n+    outputs.push_back(grouping_node);\n+\n+    const auto & missing_columns = grouping_sets_params[group].missing_keys;\n+    const auto & used_keys = grouping_sets_params[group].used_keys;\n+\n+    auto to_nullable_function = FunctionFactory::instance().get(\"toNullable\", nullptr);\n+    for (size_t i = 0; i < out_header.columns(); ++i)\n+    {\n+        const auto & col = out_header.getByPosition(i);\n+        const auto missing_it = std::find_if(\n+            missing_columns.begin(), missing_columns.end(), [&](const auto & missing_col) { return missing_col == col.name; });\n+        const auto used_it = std::find_if(\n+            used_keys.begin(), used_keys.end(), [&](const auto & used_col) { return used_col == col.name; });\n+        if (missing_it != missing_columns.end())\n+        {\n+            auto column_with_default = col.column->cloneEmpty();\n+            col.type->insertDefaultInto(*column_with_default);\n+            column_with_default->finalize();\n+\n+            auto column = ColumnConst::create(std::move(column_with_default), 0);\n+            const auto * node = &dag.addColumn({ColumnPtr(std::move(column)), col.type, col.name});\n+            node = &dag.materializeNode(*node);\n+            outputs.push_back(node);\n+        }\n+        else\n+        {\n+            const auto * column_node = dag.getOutputs()[in_header.getPositionByName(col.name)];\n+            if (used_it != used_keys.end() && group_by_use_nulls && column_node->result_type->canBeInsideNullable())\n+                outputs.push_back(&dag.addFunction(to_nullable_function, { column_node }, col.name));\n+            else\n+                outputs.push_back(column_node);\n+        }\n+    }\n+\n+    dag.getOutputs().swap(outputs);\n+    return dag;\n+}\n+\n void AggregatingStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings & settings)\n {\n     QueryPipelineProcessorsCollector collector(pipeline, this);\n@@ -300,51 +355,7 @@ void AggregatingStep::transformPipeline(QueryPipelineBuilder & pipeline, const B\n             {\n                 const auto & header = ports[set_counter]->getHeader();\n \n-                /// Here we create a DAG which fills missing keys and adds `__grouping_set` column\n-                ActionsDAG dag(header.getColumnsWithTypeAndName());\n-                ActionsDAG::NodeRawConstPtrs outputs;\n-                outputs.reserve(output_header.columns() + 1);\n-\n-                auto grouping_col = ColumnConst::create(ColumnUInt64::create(1, set_counter), 0);\n-                const auto * grouping_node = &dag.addColumn(\n-                    {ColumnPtr(std::move(grouping_col)), std::make_shared<DataTypeUInt64>(), \"__grouping_set\"});\n-\n-                grouping_node = &dag.materializeNode(*grouping_node);\n-                outputs.push_back(grouping_node);\n-\n-                const auto & missing_columns = grouping_sets_params[set_counter].missing_keys;\n-                const auto & used_keys = grouping_sets_params[set_counter].used_keys;\n-\n-                auto to_nullable_function = FunctionFactory::instance().get(\"toNullable\", nullptr);\n-                for (size_t i = 0; i < output_header.columns(); ++i)\n-                {\n-                    auto & col = output_header.getByPosition(i);\n-                    const auto missing_it = std::find_if(\n-                        missing_columns.begin(), missing_columns.end(), [&](const auto & missing_col) { return missing_col == col.name; });\n-                    const auto used_it = std::find_if(\n-                        used_keys.begin(), used_keys.end(), [&](const auto & used_col) { return used_col == col.name; });\n-                    if (missing_it != missing_columns.end())\n-                    {\n-                        auto column_with_default = col.column->cloneEmpty();\n-                        col.type->insertDefaultInto(*column_with_default);\n-                        column_with_default->finalize();\n-\n-                        auto column = ColumnConst::create(std::move(column_with_default), 0);\n-                        const auto * node = &dag.addColumn({ColumnPtr(std::move(column)), col.type, col.name});\n-                        node = &dag.materializeNode(*node);\n-                        outputs.push_back(node);\n-                    }\n-                    else\n-                    {\n-                        const auto * column_node = dag.getOutputs()[header.getPositionByName(col.name)];\n-                        if (used_it != used_keys.end() && group_by_use_nulls && column_node->result_type->canBeInsideNullable())\n-                            outputs.push_back(&dag.addFunction(to_nullable_function, { column_node }, col.name));\n-                        else\n-                            outputs.push_back(column_node);\n-                    }\n-                }\n-\n-                dag.getOutputs().swap(outputs);\n+                auto dag = makeCreatingMissingKeysForGroupingSetDAG(header, output_header, grouping_sets_params, set_counter, group_by_use_nulls);\n                 auto expression = std::make_shared<ExpressionActions>(std::move(dag), settings.getActionsSettings());\n                 auto transform = std::make_shared<ExpressionTransform>(header, expression);\n \ndiff --git a/src/Processors/QueryPlan/AggregatingStep.h b/src/Processors/QueryPlan/AggregatingStep.h\nindex ae43295024a5..4e4078047f11 100644\n--- a/src/Processors/QueryPlan/AggregatingStep.h\n+++ b/src/Processors/QueryPlan/AggregatingStep.h\n@@ -7,18 +7,6 @@\n namespace DB\n {\n \n-struct GroupingSetsParams\n-{\n-    GroupingSetsParams() = default;\n-\n-    GroupingSetsParams(Names used_keys_, Names missing_keys_) : used_keys(std::move(used_keys_)), missing_keys(std::move(missing_keys_)) { }\n-\n-    Names used_keys;\n-    Names missing_keys;\n-};\n-\n-using GroupingSetsParamsList = std::vector<GroupingSetsParams>;\n-\n Block appendGroupingSetColumn(Block header);\n Block generateOutputHeader(const Block & input_header, const Names & keys, bool use_nulls);\n \n@@ -77,6 +65,13 @@ class AggregatingStep : public ITransformingStep\n     /// Argument input_stream would be the second input (from projection).\n     std::unique_ptr<AggregatingProjectionStep> convertToAggregatingProjection(const DataStream & input_stream) const;\n \n+    static ActionsDAG makeCreatingMissingKeysForGroupingSetDAG(\n+        const Block & in_header,\n+        const Block & out_header,\n+        const GroupingSetsParamsList & grouping_sets_params,\n+        UInt64 group,\n+        bool group_by_use_nulls);\n+\n private:\n     void updateOutputStream() override;\n \ndiff --git a/src/Processors/QueryPlan/MergingAggregatedStep.cpp b/src/Processors/QueryPlan/MergingAggregatedStep.cpp\nindex a5062ac8216f..f3eb352faacc 100644\n--- a/src/Processors/QueryPlan/MergingAggregatedStep.cpp\n+++ b/src/Processors/QueryPlan/MergingAggregatedStep.cpp\n@@ -10,6 +10,11 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n static bool memoryBoundMergingWillBeUsed(\n     const DataStream & input_stream,\n     bool memory_bound_merging_of_aggregation_results_enabled,\n@@ -37,6 +42,7 @@ static ITransformingStep::Traits getTraits(bool should_produce_results_in_order_\n MergingAggregatedStep::MergingAggregatedStep(\n     const DataStream & input_stream_,\n     Aggregator::Params params_,\n+    GroupingSetsParamsList grouping_sets_params_,\n     bool final_,\n     bool memory_efficient_aggregation_,\n     size_t max_threads_,\n@@ -48,9 +54,10 @@ MergingAggregatedStep::MergingAggregatedStep(\n     bool memory_bound_merging_of_aggregation_results_enabled_)\n     : ITransformingStep(\n         input_stream_,\n-        params_.getHeader(input_stream_.header, final_),\n+        MergingAggregatedTransform::appendGroupingIfNeeded(input_stream_.header, params_.getHeader(input_stream_.header, final_)),\n         getTraits(should_produce_results_in_order_of_bucket_number_))\n     , params(std::move(params_))\n+    , grouping_sets_params(std::move(grouping_sets_params_))\n     , final(final_)\n     , memory_efficient_aggregation(memory_efficient_aggregation_)\n     , max_threads(max_threads_)\n@@ -89,10 +96,13 @@ void MergingAggregatedStep::applyOrder(SortDescription sort_description, DataStr\n \n void MergingAggregatedStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &)\n {\n-    auto transform_params = std::make_shared<AggregatingTransformParams>(pipeline.getHeader(), std::move(params), final);\n-\n     if (memoryBoundMergingWillBeUsed())\n     {\n+        if (input_streams.front().header.has(\"__grouping_set\") || !grouping_sets_params.empty())\n+            throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                 \"Memory bound merging of aggregated results is not supported for grouping sets.\");\n+\n+        auto transform_params = std::make_shared<AggregatingTransformParams>(pipeline.getHeader(), std::move(params), final);\n         auto transform = std::make_shared<FinishAggregatingInOrderTransform>(\n             pipeline.getHeader(),\n             pipeline.getNumStreams(),\n@@ -127,15 +137,19 @@ void MergingAggregatedStep::transformPipeline(QueryPipelineBuilder & pipeline, c\n         pipeline.resize(1);\n \n         /// Now merge the aggregated blocks\n-        pipeline.addSimpleTransform([&](const Block & header)\n-                                    { return std::make_shared<MergingAggregatedTransform>(header, transform_params, max_threads); });\n+        auto transform = std::make_shared<MergingAggregatedTransform>(pipeline.getHeader(), params, final, grouping_sets_params, max_threads);\n+        pipeline.addTransform(std::move(transform));\n     }\n     else\n     {\n+        if (input_streams.front().header.has(\"__grouping_set\") || !grouping_sets_params.empty())\n+            throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                 \"Memory efficient merging of aggregated results is not supported for grouping sets.\");\n         auto num_merge_threads = memory_efficient_merge_threads\n                                  ? memory_efficient_merge_threads\n                                  : max_threads;\n \n+        auto transform_params = std::make_shared<AggregatingTransformParams>(pipeline.getHeader(), std::move(params), final);\n         pipeline.addMergingAggregatedMemoryEfficientTransform(transform_params, num_merge_threads);\n     }\n \n@@ -154,7 +168,9 @@ void MergingAggregatedStep::describeActions(JSONBuilder::JSONMap & map) const\n \n void MergingAggregatedStep::updateOutputStream()\n {\n-    output_stream = createOutputStream(input_streams.front(), params.getHeader(input_streams.front().header, final), getDataStreamTraits());\n+    const auto & in_header = input_streams.front().header;\n+    output_stream = createOutputStream(input_streams.front(),\n+        MergingAggregatedTransform::appendGroupingIfNeeded(in_header, params.getHeader(in_header, final)), getDataStreamTraits());\n     if (is_order_overwritten)  /// overwrite order again\n         applyOrder(group_by_sort_description, overwritten_sort_scope);\n }\ndiff --git a/src/Processors/QueryPlan/MergingAggregatedStep.h b/src/Processors/QueryPlan/MergingAggregatedStep.h\nindex 654f794d5f52..5c3842a6c33b 100644\n--- a/src/Processors/QueryPlan/MergingAggregatedStep.h\n+++ b/src/Processors/QueryPlan/MergingAggregatedStep.h\n@@ -16,6 +16,7 @@ class MergingAggregatedStep : public ITransformingStep\n     MergingAggregatedStep(\n         const DataStream & input_stream_,\n         Aggregator::Params params_,\n+        GroupingSetsParamsList grouping_sets_params_,\n         bool final_,\n         bool memory_efficient_aggregation_,\n         size_t max_threads_,\n@@ -43,6 +44,7 @@ class MergingAggregatedStep : public ITransformingStep\n \n \n     Aggregator::Params params;\n+    GroupingSetsParamsList grouping_sets_params;\n     bool final;\n     bool memory_efficient_aggregation;\n     size_t max_threads;\ndiff --git a/src/Processors/Transforms/MergingAggregatedTransform.cpp b/src/Processors/Transforms/MergingAggregatedTransform.cpp\nindex 446e60a0b818..9b76acb8081a 100644\n--- a/src/Processors/Transforms/MergingAggregatedTransform.cpp\n+++ b/src/Processors/Transforms/MergingAggregatedTransform.cpp\n@@ -1,7 +1,10 @@\n #include <Processors/Transforms/MergingAggregatedTransform.h>\n #include <Processors/Transforms/AggregatingTransform.h>\n #include <Processors/Transforms/AggregatingInOrderTransform.h>\n+#include <Processors/QueryPlan/AggregatingStep.h>\n #include <Common/logger_useful.h>\n+#include <Interpreters/ExpressionActions.h>\n+#include <DataTypes/DataTypesNumber.h>\n \n namespace DB\n {\n@@ -10,11 +13,192 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n }\n \n+Block MergingAggregatedTransform::appendGroupingIfNeeded(const Block & in_header, Block out_header)\n+{\n+    /// __grouping_set is neither GROUP BY key nor an aggregate function.\n+    /// It behaves like a GROUP BY key, but we cannot append it to keys\n+    /// because it changes hashing method and buckets for two level aggregation.\n+    /// Now, this column is processed \"manually\" by merging each group separately.\n+    if (in_header.has(\"__grouping_set\"))\n+        out_header.insert(0, in_header.getByName(\"__grouping_set\"));\n+\n+    return out_header;\n+}\n+\n+/// We should keep the order for GROUPING SET keys.\n+/// Initiator creates a separate Aggregator for every group, so should we do here.\n+/// Otherwise, two-level aggregation will split the data into different buckets,\n+/// and the result may have duplicating rows.\n+static ActionsDAG makeReorderingActions(const Block & in_header, const GroupingSetsParams & params)\n+{\n+    ActionsDAG reordering(in_header.getColumnsWithTypeAndName());\n+    auto & outputs = reordering.getOutputs();\n+    ActionsDAG::NodeRawConstPtrs new_outputs;\n+    new_outputs.reserve(in_header.columns() + params.used_keys.size() - params.used_keys.size());\n+\n+    std::unordered_map<std::string_view, size_t> index;\n+    for (size_t pos = 0; pos < outputs.size(); ++pos)\n+        index.emplace(outputs[pos]->result_name, pos);\n+\n+    for (const auto & used_name : params.used_keys)\n+    {\n+        auto & idx = index[used_name];\n+        new_outputs.push_back(outputs[idx]);\n+    }\n+\n+    for (const auto & used_name : params.used_keys)\n+        index[used_name] = outputs.size();\n+    for (const auto & missing_name : params.missing_keys)\n+        index[missing_name] = outputs.size();\n+\n+    for (const auto * output : outputs)\n+    {\n+        if (index[output->result_name] != outputs.size())\n+            new_outputs.push_back(output);\n+    }\n+\n+    outputs.swap(new_outputs);\n+    return reordering;\n+}\n+\n+MergingAggregatedTransform::~MergingAggregatedTransform() = default;\n+\n MergingAggregatedTransform::MergingAggregatedTransform(\n-    Block header_, AggregatingTransformParamsPtr params_, size_t max_threads_)\n-    : IAccumulatingTransform(std::move(header_), params_->getHeader())\n-    , params(std::move(params_)), max_threads(max_threads_)\n+    Block header_,\n+    Aggregator::Params params,\n+    bool final,\n+    GroupingSetsParamsList grouping_sets_params,\n+    size_t max_threads_)\n+    : IAccumulatingTransform(header_, appendGroupingIfNeeded(header_, params.getHeader(header_, final)))\n+    , max_threads(max_threads_)\n {\n+    if (!grouping_sets_params.empty())\n+    {\n+        if (!header_.has(\"__grouping_set\"))\n+            throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                \"Cannot find __grouping_set column in header of MergingAggregatedTransform with grouping sets.\"\n+                \"Header {}\", header_.dumpStructure());\n+\n+        auto in_header = header_;\n+        in_header.erase(header_.getPositionByName(\"__grouping_set\"));\n+        auto out_header = params.getHeader(header_, final);\n+\n+        grouping_sets.reserve(grouping_sets_params.size());\n+        for (const auto & grouping_set_params : grouping_sets_params)\n+        {\n+            size_t group = grouping_sets.size();\n+\n+            auto reordering = makeReorderingActions(in_header, grouping_set_params);\n+\n+            Aggregator::Params set_params(grouping_set_params.used_keys,\n+                params.aggregates,\n+                params.overflow_row,\n+                params.max_threads,\n+                params.max_block_size,\n+                params.min_hit_rate_to_use_consecutive_keys_optimization);\n+\n+            auto transform_params = std::make_shared<AggregatingTransformParams>(reordering.updateHeader(in_header), std::move(set_params), final);\n+\n+            auto creating = AggregatingStep::makeCreatingMissingKeysForGroupingSetDAG(\n+                transform_params->getHeader(),\n+                out_header,\n+                grouping_sets_params, group, false);\n+\n+            auto & groupiung_set = grouping_sets.emplace_back();\n+            groupiung_set.reordering_key_columns_actions = std::make_shared<ExpressionActions>(std::move(reordering));\n+            groupiung_set.creating_missing_keys_actions = std::make_shared<ExpressionActions>(std::move(creating));\n+            groupiung_set.params = std::move(transform_params);\n+        }\n+    }\n+    else\n+    {\n+        auto & groupiung_set = grouping_sets.emplace_back();\n+        groupiung_set.params = std::make_shared<AggregatingTransformParams>(header_, std::move(params), final);\n+    }\n+}\n+\n+void MergingAggregatedTransform::addBlock(Block block)\n+{\n+    if (grouping_sets.size() == 1)\n+    {\n+        auto bucket = block.info.bucket_num;\n+        if (grouping_sets[0].reordering_key_columns_actions)\n+            grouping_sets[0].reordering_key_columns_actions->execute(block);\n+        grouping_sets[0].bucket_to_blocks[bucket].emplace_back(std::move(block));\n+        return;\n+    }\n+\n+    auto grouping_position = block.getPositionByName(\"__grouping_set\");\n+    auto grouping_column = block.getByPosition(grouping_position).column;\n+    block.erase(grouping_position);\n+\n+    /// Split a block by __grouping_set values.\n+\n+    const auto * grouping_column_typed = typeid_cast<const ColumnUInt64 *>(grouping_column.get());\n+    if (!grouping_column_typed)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Expected UInt64 column for __grouping_set, got {}\", grouping_column->getName());\n+\n+    IColumn::Selector selector;\n+\n+    const auto & grouping_data = grouping_column_typed->getData();\n+    size_t num_rows = grouping_data.size();\n+    UInt64 last_group = grouping_data[0];\n+    UInt64 max_group = last_group;\n+    for (size_t row = 1; row < num_rows; ++row)\n+    {\n+        auto group = grouping_data[row];\n+\n+        /// Optimization for equal ranges.\n+        if (last_group == group)\n+            continue;\n+\n+        /// Optimization for single group.\n+        if (selector.empty())\n+            selector.reserve(num_rows);\n+\n+        /// Fill the last equal range.\n+        selector.resize_fill(row, last_group);\n+        last_group = group;\n+        max_group = std::max(last_group, max_group);\n+    }\n+\n+    if (max_group >= grouping_sets.size())\n+        throw Exception(ErrorCodes::LOGICAL_ERROR,\n+            \"Invalid group number {}. Number of groups {}.\", last_group, grouping_sets.size());\n+\n+    /// Optimization for single group.\n+    if (selector.empty())\n+    {\n+        auto bucket = block.info.bucket_num;\n+        grouping_sets[last_group].reordering_key_columns_actions->execute(block);\n+        grouping_sets[last_group].bucket_to_blocks[bucket].emplace_back(std::move(block));\n+        return;\n+    }\n+\n+    /// Fill the last equal range.\n+    selector.resize_fill(num_rows, last_group);\n+\n+    const size_t num_groups = max_group + 1;\n+    Blocks splitted_blocks(num_groups);\n+\n+    for (size_t group_id = 0; group_id < num_groups; ++group_id)\n+        splitted_blocks[group_id] = block.cloneEmpty();\n+\n+    size_t columns_in_block = block.columns();\n+    for (size_t col_idx_in_block = 0; col_idx_in_block < columns_in_block; ++col_idx_in_block)\n+    {\n+        MutableColumns splitted_columns = block.getByPosition(col_idx_in_block).column->scatter(num_groups, selector);\n+        for (size_t group_id = 0; group_id < num_groups; ++group_id)\n+            splitted_blocks[group_id].getByPosition(col_idx_in_block).column = std::move(splitted_columns[group_id]);\n+    }\n+\n+    for (size_t group = 0; group < num_groups; ++group)\n+    {\n+        auto & splitted_block = splitted_blocks[group];\n+        splitted_block.info = block.info;\n+        grouping_sets[group].reordering_key_columns_actions->execute(splitted_block);\n+        grouping_sets[group].bucket_to_blocks[block.info.bucket_num].emplace_back(std::move(splitted_block));\n+    }\n }\n \n void MergingAggregatedTransform::consume(Chunk chunk)\n@@ -46,7 +230,7 @@ void MergingAggregatedTransform::consume(Chunk chunk)\n         block.info.is_overflows = agg_info->is_overflows;\n         block.info.bucket_num = agg_info->bucket_num;\n \n-        bucket_to_blocks[agg_info->bucket_num].emplace_back(std::move(block));\n+        addBlock(std::move(block));\n     }\n     else if (chunk.getChunkInfos().get<ChunkInfoWithAllocatedBytes>())\n     {\n@@ -54,7 +238,7 @@ void MergingAggregatedTransform::consume(Chunk chunk)\n         block.info.is_overflows = false;\n         block.info.bucket_num = -1;\n \n-        bucket_to_blocks[block.info.bucket_num].emplace_back(std::move(block));\n+        addBlock(std::move(block));\n     }\n     else\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Chunk should have AggregatedChunkInfo in MergingAggregatedTransform.\");\n@@ -70,9 +254,23 @@ Chunk MergingAggregatedTransform::generate()\n         /// Exception safety. Make iterator valid in case any method below throws.\n         next_block = blocks.begin();\n \n-        /// TODO: this operation can be made async. Add async for IAccumulatingTransform.\n-        params->aggregator.mergeBlocks(std::move(bucket_to_blocks), data_variants, max_threads, is_cancelled);\n-        blocks = params->aggregator.convertToBlocks(data_variants, params->final, max_threads);\n+        for (auto & grouping_set : grouping_sets)\n+        {\n+            auto & params = grouping_set.params;\n+            auto & bucket_to_blocks = grouping_set.bucket_to_blocks;\n+            AggregatedDataVariants data_variants;\n+\n+            /// TODO: this operation can be made async. Add async for IAccumulatingTransform.\n+            params->aggregator.mergeBlocks(std::move(bucket_to_blocks), data_variants, max_threads, is_cancelled);\n+            auto merged_blocks = params->aggregator.convertToBlocks(data_variants, params->final, max_threads);\n+\n+            if (grouping_set.creating_missing_keys_actions)\n+                for (auto & block : merged_blocks)\n+                    grouping_set.creating_missing_keys_actions->execute(block);\n+\n+            blocks.splice(blocks.end(), std::move(merged_blocks));\n+        }\n+\n         next_block = blocks.begin();\n     }\n \ndiff --git a/src/Processors/Transforms/MergingAggregatedTransform.h b/src/Processors/Transforms/MergingAggregatedTransform.h\nindex ade76b2f3048..3a043ad74b84 100644\n--- a/src/Processors/Transforms/MergingAggregatedTransform.h\n+++ b/src/Processors/Transforms/MergingAggregatedTransform.h\n@@ -6,26 +6,46 @@\n namespace DB\n {\n \n+class ExpressionActions;\n+using ExpressionActionsPtr = std::shared_ptr<ExpressionActions>;\n+\n /** A pre-aggregate stream of blocks in which each block is already aggregated.\n   * Aggregate functions in blocks should not be finalized so that their states can be merged.\n   */\n class MergingAggregatedTransform : public IAccumulatingTransform\n {\n public:\n-    MergingAggregatedTransform(Block header_, AggregatingTransformParamsPtr params_, size_t max_threads_);\n+    MergingAggregatedTransform(\n+        Block header_,\n+        Aggregator::Params params_,\n+        bool final_,\n+        GroupingSetsParamsList grouping_sets_params,\n+        size_t max_threads_);\n+\n+    ~MergingAggregatedTransform() override;\n+\n     String getName() const override { return \"MergingAggregatedTransform\"; }\n \n+    static Block appendGroupingIfNeeded(const Block & in_header, Block out_header);\n+\n protected:\n     void consume(Chunk chunk) override;\n     Chunk generate() override;\n \n private:\n-    AggregatingTransformParamsPtr params;\n     LoggerPtr log = getLogger(\"MergingAggregatedTransform\");\n     size_t max_threads;\n \n-    AggregatedDataVariants data_variants;\n-    Aggregator::BucketToBlocks bucket_to_blocks;\n+    struct GroupingSet\n+    {\n+        Aggregator::BucketToBlocks bucket_to_blocks;\n+        ExpressionActionsPtr reordering_key_columns_actions;\n+        ExpressionActionsPtr creating_missing_keys_actions;\n+        AggregatingTransformParamsPtr params;\n+    };\n+\n+    using GroupingSets = std::vector<GroupingSet>;\n+    GroupingSets grouping_sets;\n \n     UInt64 total_input_rows = 0;\n     UInt64 total_input_blocks = 0;\n@@ -35,6 +55,8 @@ class MergingAggregatedTransform : public IAccumulatingTransform\n \n     bool consume_started = false;\n     bool generate_started = false;\n+\n+    void addBlock(Block block);\n };\n \n }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02165_replicated_grouping_sets.reference b/tests/queries/0_stateless/02165_replicated_grouping_sets.reference\nindex 659cd98368d3..31cbf2ad670d 100644\n--- a/tests/queries/0_stateless/02165_replicated_grouping_sets.reference\n+++ b/tests/queries/0_stateless/02165_replicated_grouping_sets.reference\n@@ -11,3 +11,215 @@\n 0\t6\t4\n 1\t10\t4\n 2\t14\t4\n+-- { echo On }\n+\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+1\t['.']\n+2\t['.','.']\n+2\t['.','.','.']\n+2\t['.','.','.','.']\n+2\t['.','.','.','.','.']\n+2\t['.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.','.']\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+1\t['.']\n+1\t['.']\n+2\t['.','.']\n+2\t['.','.']\n+2\t['.','.','.']\n+2\t['.','.','.']\n+2\t['.','.','.','.']\n+2\t['.','.','.','.']\n+2\t['.','.','.','.','.']\n+2\t['.','.','.','.','.']\n+2\t['.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.','.']\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+1\t1\n+2\t2\n+2\t3\n+2\t4\n+2\t5\n+2\t6\n+2\t7\n+2\t8\n+2\t9\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+1\t1\n+1\t1\n+2\t2\n+2\t2\n+2\t3\n+2\t3\n+2\t4\n+2\t4\n+2\t5\n+2\t5\n+2\t6\n+2\t6\n+2\t7\n+2\t7\n+2\t8\n+2\t8\n+2\t9\n+2\t9\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+1\t1\n+1\t1\n+2\t2\n+2\t2\n+2\t3\n+2\t3\n+2\t4\n+2\t4\n+2\t5\n+2\t5\n+2\t6\n+2\t6\n+2\t7\n+2\t7\n+2\t8\n+2\t8\n+2\t9\n+2\t9\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k), (k, number + 2)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+1\t1\n+1\t1\n+1\t1\n+2\t2\n+2\t2\n+2\t2\n+2\t3\n+2\t3\n+2\t3\n+2\t4\n+2\t4\n+2\t4\n+2\t5\n+2\t5\n+2\t5\n+2\t6\n+2\t6\n+2\t6\n+2\t7\n+2\t7\n+2\t7\n+2\t8\n+2\t8\n+2\t8\n+2\t9\n+2\t9\n+2\t9\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+2\t['.']\n+2\t['.','.']\n+2\t['.','.','.']\n+2\t['.','.','.','.']\n+2\t['.','.','.','.','.']\n+2\t['.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.','.']\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+2\t['.']\n+2\t['.']\n+2\t['.','.']\n+2\t['.','.']\n+2\t['.','.','.']\n+2\t['.','.','.']\n+2\t['.','.','.','.']\n+2\t['.','.','.','.']\n+2\t['.','.','.','.','.']\n+2\t['.','.','.','.','.']\n+2\t['.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.','.']\n+2\t['.','.','.','.','.','.','.','.','.']\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+2\t1\n+2\t2\n+2\t3\n+2\t4\n+2\t5\n+2\t6\n+2\t7\n+2\t8\n+2\t9\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+2\t1\n+2\t1\n+2\t2\n+2\t2\n+2\t3\n+2\t3\n+2\t4\n+2\t4\n+2\t5\n+2\t5\n+2\t6\n+2\t6\n+2\t7\n+2\t7\n+2\t8\n+2\t8\n+2\t9\n+2\t9\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+2\t1\n+2\t1\n+2\t2\n+2\t2\n+2\t3\n+2\t3\n+2\t4\n+2\t4\n+2\t5\n+2\t5\n+2\t6\n+2\t6\n+2\t7\n+2\t7\n+2\t8\n+2\t8\n+2\t9\n+2\t9\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k), (k, number + 2)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+2\t1\n+2\t1\n+2\t1\n+2\t2\n+2\t2\n+2\t2\n+2\t3\n+2\t3\n+2\t3\n+2\t4\n+2\t4\n+2\t4\n+2\t5\n+2\t5\n+2\t5\n+2\t6\n+2\t6\n+2\t6\n+2\t7\n+2\t7\n+2\t7\n+2\t8\n+2\t8\n+2\t8\n+2\t9\n+2\t9\n+2\t9\ndiff --git a/tests/queries/0_stateless/02165_replicated_grouping_sets.sql b/tests/queries/0_stateless/02165_replicated_grouping_sets.sql\nindex d92d92c3e724..47d4446f3488 100644\n--- a/tests/queries/0_stateless/02165_replicated_grouping_sets.sql\n+++ b/tests/queries/0_stateless/02165_replicated_grouping_sets.sql\n@@ -43,3 +43,23 @@ GROUP BY\n ORDER BY\n     sum_value ASC,\n     count_value ASC;\n+\n+set prefer_localhost_replica = 1;\n+\n+-- { echo On }\n+\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k), (k, number + 2)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n+SELECT count(), toString(number) AS k FROM remote('127.0.0.{3,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (number + 1, k), (k, number + 2)) ORDER BY k settings group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\n",
  "problem_statement": "Stateless test `03023_group_by_use_nulls_analyzer_crashes` has failed\nhttps://s3.amazonaws.com/clickhouse-test-reports/0/da1d09b2d9c8f9a70527f704de54019f64331bc7/stateless_tests__ubsan__[2_2].html\n",
  "hints_text": "```\r\nSELECT arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k)) ORDER BY k settings group_by_use_nulls=1, group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000;\r\n\r\nSELECT arrayMap(x -> '.', range(number % 10)) AS k\r\nFROM remote('127.0.0.{1,2}', numbers(10))\r\nWHERE number > (queryID() = initialQueryID())\r\nGROUP BY\r\n    GROUPING SETS ((k))\r\nORDER BY k ASC\r\nSETTINGS group_by_use_nulls = 1, group_by_two_level_threshold = 9, max_bytes_before_external_group_by = 10000000000\r\n\r\nQuery id: 91e0b7d6-4505-4cdf-98ab-26e9918c1d3e\r\n\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 24.9.1.\r\n\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502 ['.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n2. \u2502 ['.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n3. \u2502 ['.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n4. \u2502 ['.','.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n5. \u2502 ['.','.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n6. \u2502 ['.','.','.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n7. \u2502 ['.','.','.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n8. \u2502 ['.','.','.','.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n9. \u2502 ['.','.','.','.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n10. \u2502 ['.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n11. \u2502 ['.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n12. \u2502 ['.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n13. \u2502 ['.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n14. \u2502 ['.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n15. \u2502 ['.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n16. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n17. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\nOnly for a new analzyer\nNot only with new analyzer. \r\n\r\n```\r\nip-172-31-26-174.ec2.internal :) SELECT arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_use_nulls=1, group_by_two_level_threshold=9, max_bytes_before_external_group_by=10000000000, allow_experimental_analyzer=0;\r\n\r\nSELECT arrayMap(x -> '.', range(number % 10)) AS k\r\nFROM remote('127.0.0.{1,2}', numbers(10))\r\nWHERE number > (queryID() = initialQueryID())\r\nGROUP BY\r\n    GROUPING SETS (\r\n        (k),\r\n        (k, k))\r\nORDER BY k ASC\r\nSETTINGS group_by_use_nulls = 1, group_by_two_level_threshold = 9, max_bytes_before_external_group_by = 10000000000, allow_experimental_analyzer = 0\r\n\r\nQuery id: 34fa02d2-9cd8-4514-bbca-7494f1edd744\r\n\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 24.9.1.\r\n\r\n   \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502 ['.']         \u2502\r\n2. \u2502 ['.']         \u2502\r\n3. \u2502 ['.','.']     \u2502\r\n4. \u2502 ['.','.']     \u2502\r\n5. \u2502 ['.','.']     \u2502\r\n6. \u2502 ['.','.']     \u2502\r\n7. \u2502 ['.','.','.'] \u2502\r\n8. \u2502 ['.','.','.'] \u2502\r\n9. \u2502 ['.','.','.'] \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n10. \u2502 ['.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n11. \u2502 ['.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n12. \u2502 ['.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n13. \u2502 ['.','.','.','.']     \u2502\r\n14. \u2502 ['.','.','.','.']     \u2502\r\n15. \u2502 ['.','.','.','.','.'] \u2502\r\n16. \u2502 ['.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n17. \u2502 ['.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n18. \u2502 ['.','.','.','.','.']     \u2502\r\n19. \u2502 ['.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n20. \u2502 ['.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n21. \u2502 ['.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n22. \u2502 ['.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n23. \u2502 ['.','.','.','.','.','.','.']     \u2502\r\n24. \u2502 ['.','.','.','.','.','.','.']     \u2502\r\n25. \u2502 ['.','.','.','.','.','.','.']     \u2502\r\n26. \u2502 ['.','.','.','.','.','.','.']     \u2502\r\n27. \u2502 ['.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n28. \u2502 ['.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n29. \u2502 ['.','.','.','.','.','.','.','.']     \u2502\r\n30. \u2502 ['.','.','.','.','.','.','.','.']     \u2502\r\n31. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n32. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n33. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n34. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n34 rows in set. Elapsed: 0.016 sec. \r\n\r\nip-172-31-26-174.ec2.internal :) SELECT arrayMap(x -> '.', range(number % 10)) AS k FROM remote('127.0.0.{1,2}', numbers(10)) where number > ( queryID() = initialQueryID()) GROUP BY GROUPING SETS ((k), (k, k)) ORDER BY k settings group_by_use_nulls=1, group_by_two_level_threshold=10, max_bytes_before_external_group_by=10000000000, allow_experimental_analyzer=0;\r\n\r\nSELECT arrayMap(x -> '.', range(number % 10)) AS k\r\nFROM remote('127.0.0.{1,2}', numbers(10))\r\nWHERE number > (queryID() = initialQueryID())\r\nGROUP BY\r\n    GROUPING SETS (\r\n        (k),\r\n        (k, k))\r\nORDER BY k ASC\r\nSETTINGS group_by_use_nulls = 1, group_by_two_level_threshold = 10, max_bytes_before_external_group_by = 10000000000, allow_experimental_analyzer = 0\r\n\r\nQuery id: 560ef8b0-5ca4-475b-ab23-20cf3070ca6f\r\n\r\n    \u250c\u2500k\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n 1. \u2502 ['.']                                 \u2502\r\n 2. \u2502 ['.']                                 \u2502\r\n 3. \u2502 ['.','.']                             \u2502\r\n 4. \u2502 ['.','.']                             \u2502\r\n 5. \u2502 ['.','.','.']                         \u2502\r\n 6. \u2502 ['.','.','.']                         \u2502\r\n 7. \u2502 ['.','.','.','.']                     \u2502\r\n 8. \u2502 ['.','.','.','.']                     \u2502\r\n 9. \u2502 ['.','.','.','.','.']                 \u2502\r\n10. \u2502 ['.','.','.','.','.']                 \u2502\r\n11. \u2502 ['.','.','.','.','.','.']             \u2502\r\n12. \u2502 ['.','.','.','.','.','.']             \u2502\r\n13. \u2502 ['.','.','.','.','.','.','.']         \u2502\r\n14. \u2502 ['.','.','.','.','.','.','.']         \u2502\r\n15. \u2502 ['.','.','.','.','.','.','.','.']     \u2502\r\n16. \u2502 ['.','.','.','.','.','.','.','.']     \u2502\r\n17. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n18. \u2502 ['.','.','.','.','.','.','.','.','.'] \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n18 rows in set. Elapsed: 0.010 sec. \r\n\r\n```",
  "created_at": "2024-08-22T18:03:36Z"
}