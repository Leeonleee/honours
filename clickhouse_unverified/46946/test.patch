diff --git a/tests/integration/test_disk_over_web_server/test.py b/tests/integration/test_disk_over_web_server/test.py
index 363df4595b21..fd71389f71ab 100644
--- a/tests/integration/test_disk_over_web_server/test.py
+++ b/tests/integration/test_disk_over_web_server/test.py
@@ -21,23 +21,31 @@ def cluster():
         cluster.add_instance(
             "node3", main_configs=["configs/storage_conf_web.xml"], with_nginx=True
         )
+
+        cluster.add_instance(
+            "node4",
+            main_configs=["configs/storage_conf.xml"],
+            with_nginx=True,
+            stay_alive=True,
+            with_installed_binary=True,
+            image="clickhouse/clickhouse-server",
+            tag="22.8.14.53",
+        )
+
         cluster.start()
 
-        node1 = cluster.instances["node1"]
-        expected = ""
-        global uuids
-        for i in range(3):
-            node1.query(
+        def create_table_and_upload_data(node, i):
+            node.query(
                 f"CREATE TABLE data{i} (id Int32) ENGINE = MergeTree() ORDER BY id SETTINGS storage_policy = 'def', min_bytes_for_wide_part=1;"
             )
 
             for _ in range(10):
-                node1.query(
+                node.query(
                     f"INSERT INTO data{i} SELECT number FROM numbers(500000 * {i+1})"
                 )
-            expected = node1.query(f"SELECT * FROM data{i} ORDER BY id")
+            node.query(f"SELECT * FROM data{i} ORDER BY id")
 
-            metadata_path = node1.query(
+            metadata_path = node.query(
                 f"SELECT data_paths FROM system.tables WHERE name='data{i}'"
             )
             metadata_path = metadata_path[
@@ -45,7 +53,7 @@ def cluster():
             ]
             print(f"Metadata: {metadata_path}")
 
-            node1.exec_in_container(
+            node.exec_in_container(
                 [
                     "bash",
                     "-c",
@@ -56,8 +64,20 @@ def cluster():
                 user="root",
             )
             parts = metadata_path.split("/")
-            uuids.append(parts[3])
             print(f"UUID: {parts[3]}")
+            return parts[3]
+
+        node1 = cluster.instances["node1"]
+
+        global uuids
+        for i in range(2):
+            uuid = create_table_and_upload_data(node1, i)
+            uuids.append(uuid)
+
+        node4 = cluster.instances["node4"]
+
+        uuid = create_table_and_upload_data(node4, 2)
+        uuids.append(uuid)
 
         yield cluster
 
@@ -68,6 +88,7 @@ def cluster():
 @pytest.mark.parametrize("node_name", ["node2"])
 def test_usage(cluster, node_name):
     node1 = cluster.instances["node1"]
+    node4 = cluster.instances["node4"]
     node2 = cluster.instances[node_name]
     global uuids
     assert len(uuids) == 3
@@ -90,7 +111,11 @@ def test_usage(cluster, node_name):
         result = node2.query(
             "SELECT id FROM test{} WHERE id % 56 = 3 ORDER BY id".format(i)
         )
-        assert result == node1.query(
+        node = node1
+        if i == 2:
+            node = node4
+
+        assert result == node.query(
             "SELECT id FROM data{} WHERE id % 56 = 3 ORDER BY id".format(i)
         )
 
@@ -99,7 +124,7 @@ def test_usage(cluster, node_name):
                 i
             )
         )
-        assert result == node1.query(
+        assert result == node.query(
             "SELECT id FROM data{} WHERE id > 789999 AND id < 999999 ORDER BY id".format(
                 i
             )
@@ -141,6 +166,7 @@ def test_incorrect_usage(cluster):
 @pytest.mark.parametrize("node_name", ["node2"])
 def test_cache(cluster, node_name):
     node1 = cluster.instances["node1"]
+    node4 = cluster.instances["node4"]
     node2 = cluster.instances[node_name]
     global uuids
     assert len(uuids) == 3
@@ -178,7 +204,12 @@ def test_cache(cluster, node_name):
         result = node2.query(
             "SELECT id FROM test{} WHERE id % 56 = 3 ORDER BY id".format(i)
         )
-        assert result == node1.query(
+
+        node = node1
+        if i == 2:
+            node = node4
+
+        assert result == node.query(
             "SELECT id FROM data{} WHERE id % 56 = 3 ORDER BY id".format(i)
         )
 
@@ -187,7 +218,7 @@ def test_cache(cluster, node_name):
                 i
             )
         )
-        assert result == node1.query(
+        assert result == node.query(
             "SELECT id FROM data{} WHERE id > 789999 AND id < 999999 ORDER BY id".format(
                 i
             )
diff --git a/tests/integration/test_merge_tree_hdfs/test.py b/tests/integration/test_merge_tree_hdfs/test.py
index 3950077e619b..782237539fa6 100644
--- a/tests/integration/test_merge_tree_hdfs/test.py
+++ b/tests/integration/test_merge_tree_hdfs/test.py
@@ -43,8 +43,18 @@ def create_table(cluster, table_name, additional_settings=None):
 
 FILES_OVERHEAD = 1
 FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files
-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1
-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1
+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1
+FILES_OVERHEAD_METADATA_VERSION = 1
+FILES_OVERHEAD_PER_PART_WIDE = (
+    FILES_OVERHEAD_PER_COLUMN * 3
+    + 2
+    + 6
+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC
+    + FILES_OVERHEAD_METADATA_VERSION
+)
+FILES_OVERHEAD_PER_PART_COMPACT = (
+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION
+)
 
 
 @pytest.fixture(scope="module")
diff --git a/tests/integration/test_merge_tree_s3/test.py b/tests/integration/test_merge_tree_s3/test.py
index f0f81100320a..696c016f7603 100644
--- a/tests/integration/test_merge_tree_s3/test.py
+++ b/tests/integration/test_merge_tree_s3/test.py
@@ -52,8 +52,18 @@ def cluster():
 
 FILES_OVERHEAD = 1
 FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files
-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1
-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1
+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1
+FILES_OVERHEAD_METADATA_VERSION = 1
+FILES_OVERHEAD_PER_PART_WIDE = (
+    FILES_OVERHEAD_PER_COLUMN * 3
+    + 2
+    + 6
+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC
+    + FILES_OVERHEAD_METADATA_VERSION
+)
+FILES_OVERHEAD_PER_PART_COMPACT = (
+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION
+)
 
 
 def create_table(node, table_name, **additional_settings):
@@ -232,7 +242,6 @@ def test_insert_same_partition_and_merge(cluster, merge_vertical, node_name):
 def test_alter_table_columns(cluster, node_name):
     node = cluster.instances[node_name]
     create_table(node, "s3_test")
-    minio = cluster.minio_client
 
     node.query(
         "INSERT INTO s3_test VALUES {}".format(generate_values("2020-01-03", 4096))
diff --git a/tests/integration/test_merge_tree_s3_failover/test.py b/tests/integration/test_merge_tree_s3_failover/test.py
index cf71b4237131..3cc2b17dce2c 100644
--- a/tests/integration/test_merge_tree_s3_failover/test.py
+++ b/tests/integration/test_merge_tree_s3_failover/test.py
@@ -89,7 +89,7 @@ def drop_table(cluster):
 
 
 # S3 request will be failed for an appropriate part file write.
-FILES_PER_PART_BASE = 5  # partition.dat, default_compression_codec.txt, count.txt, columns.txt, checksums.txt
+FILES_PER_PART_BASE = 6  # partition.dat, metadata_version.txt, default_compression_codec.txt, count.txt, columns.txt, checksums.txt
 FILES_PER_PART_WIDE = (
     FILES_PER_PART_BASE + 1 + 1 + 3 * 2
 )  # Primary index, MinMax, Mark and data file for column(s)
diff --git a/tests/integration/test_partition/test.py b/tests/integration/test_partition/test.py
index a34141c61890..5a972b58f999 100644
--- a/tests/integration/test_partition/test.py
+++ b/tests/integration/test_partition/test.py
@@ -105,6 +105,8 @@ def partition_complex_assert_checksums():
         "c4ca4238a0b923820dcc509a6f75849b\tshadow/1/data/test/partition_complex/19700102_2_2_0/count.txt
"
         "c4ca4238a0b923820dcc509a6f75849b\tshadow/1/data/test/partition_complex/19700201_1_1_0/count.txt
"
         "cfcb770c3ecd0990dcceb1bde129e6c6\tshadow/1/data/test/partition_complex/19700102_2_2_0/p.bin
"
+        "cfcd208495d565ef66e7dff9f98764da\tshadow/1/data/test/partition_complex/19700102_2_2_0/metadata_version.txt
"
+        "cfcd208495d565ef66e7dff9f98764da\tshadow/1/data/test/partition_complex/19700201_1_1_0/metadata_version.txt
"
         "e2af3bef1fd129aea73a890ede1e7a30\tshadow/1/data/test/partition_complex/19700201_1_1_0/k.bin
"
         "f2312862cc01adf34a93151377be2ddf\tshadow/1/data/test/partition_complex/19700201_1_1_0/minmax_p.idx
"
     )
diff --git a/tests/integration/test_replicated_merge_tree_s3/test.py b/tests/integration/test_replicated_merge_tree_s3/test.py
index 0d978bb6967d..b90e28dfdb20 100644
--- a/tests/integration/test_replicated_merge_tree_s3/test.py
+++ b/tests/integration/test_replicated_merge_tree_s3/test.py
@@ -44,8 +44,18 @@ def cluster():
 
 FILES_OVERHEAD = 1
 FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files
-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1
-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1
+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1
+FILES_OVERHEAD_METADATA_VERSION = 1
+FILES_OVERHEAD_PER_PART_WIDE = (
+    FILES_OVERHEAD_PER_COLUMN * 3
+    + 2
+    + 6
+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC
+    + FILES_OVERHEAD_METADATA_VERSION
+)
+FILES_OVERHEAD_PER_PART_COMPACT = (
+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION
+)
 
 
 def random_string(length):
diff --git a/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py b/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py
index 1a5f2e127060..eca188200168 100644
--- a/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py
+++ b/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py
@@ -47,8 +47,18 @@ def cluster():
 
 FILES_OVERHEAD = 1
 FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files
-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1
-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1
+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1
+FILES_OVERHEAD_METADATA_VERSION = 1
+FILES_OVERHEAD_PER_PART_WIDE = (
+    FILES_OVERHEAD_PER_COLUMN * 3
+    + 2
+    + 6
+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC
+    + FILES_OVERHEAD_METADATA_VERSION
+)
+FILES_OVERHEAD_PER_PART_COMPACT = (
+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION
+)
 
 
 def random_string(length):
diff --git a/tests/integration/test_s3_zero_copy_ttl/test.py b/tests/integration/test_s3_zero_copy_ttl/test.py
index 9a782aacef6b..7dcf3734653d 100644
--- a/tests/integration/test_s3_zero_copy_ttl/test.py
+++ b/tests/integration/test_s3_zero_copy_ttl/test.py
@@ -86,9 +86,9 @@ def test_ttl_move_and_s3(started_cluster):
 
         print(f"Total objects: {counter}")
 
-        if counter == 300:
+        if counter == 330:
             break
 
         print(f"Attempts remaining: {attempt}")
 
-    assert counter == 300
+    assert counter == 330
diff --git a/tests/queries/0_stateless/01278_alter_rename_combination.reference b/tests/queries/0_stateless/01278_alter_rename_combination.reference
index cc912e9b265b..e70c2d2e6f8f 100644
--- a/tests/queries/0_stateless/01278_alter_rename_combination.reference
+++ b/tests/queries/0_stateless/01278_alter_rename_combination.reference
@@ -1,7 +1,7 @@
-CREATE TABLE default.rename_table
(
    `key` Int32,
    `old_value1` Int32,
    `value1` Int32
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192
+CREATE TABLE default.rename_table
(
    `key` Int32,
    `old_value1` Int32,
    `value1` Int32
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192
 key	old_value1	value1
 1	2	3
-CREATE TABLE default.rename_table
(
    `k` Int32,
    `v1` Int32,
    `v2` Int32
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192
+CREATE TABLE default.rename_table
(
    `k` Int32,
    `v1` Int32,
    `v2` Int32
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192
 k	v1	v2
 1	2	3
 4	5	6
diff --git a/tests/queries/0_stateless/01278_alter_rename_combination.sql b/tests/queries/0_stateless/01278_alter_rename_combination.sql
index fa73362622c8..51322f5d86f1 100644
--- a/tests/queries/0_stateless/01278_alter_rename_combination.sql
+++ b/tests/queries/0_stateless/01278_alter_rename_combination.sql
@@ -1,6 +1,6 @@
 DROP TABLE IF EXISTS rename_table;
 
-CREATE TABLE rename_table (key Int32, value1 Int32, value2 Int32) ENGINE = MergeTree ORDER BY tuple();
+CREATE TABLE rename_table (key Int32, value1 Int32, value2 Int32) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0;
 
 INSERT INTO rename_table VALUES (1, 2, 3);
 
diff --git a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference
index bf3358aea60a..532b8ce87123 100644
--- a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference
+++ b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference
@@ -1,11 +1,11 @@
-CREATE TABLE default.rename_table_multiple
(
    `key` Int32,
    `value1_string` String,
    `value2` Int32
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192
+CREATE TABLE default.rename_table_multiple
(
    `key` Int32,
    `value1_string` String,
    `value2` Int32
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192
 key	value1_string	value2
 1	2	3
-CREATE TABLE default.rename_table_multiple
(
    `key` Int32,
    `value1_string` String,
    `value2_old` Int32,
    `value2` Int64 DEFAULT 7
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192
+CREATE TABLE default.rename_table_multiple
(
    `key` Int32,
    `value1_string` String,
    `value2_old` Int32,
    `value2` Int64 DEFAULT 7
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192
 key	value1_string	value2_old	value2
 1	2	3	7
 4	5	6	7
-CREATE TABLE default.rename_table_multiple
(
    `key` Int32,
    `value1_string` String,
    `value2_old` Int64 DEFAULT 7
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192
+CREATE TABLE default.rename_table_multiple
(
    `key` Int32,
    `value1_string` String,
    `value2_old` Int64 DEFAULT 7
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192
 key	value1_string	value2_old
 1	2	7
 4	5	7
diff --git a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql
index f9462f0478e3..b0ccd7751ab0 100644
--- a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql
+++ b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql
@@ -1,6 +1,6 @@
 DROP TABLE IF EXISTS rename_table_multiple;
 
-CREATE TABLE rename_table_multiple (key Int32, value1 String, value2 Int32) ENGINE = MergeTree ORDER BY tuple();
+CREATE TABLE rename_table_multiple (key Int32, value1 String, value2 Int32) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0;
 
 INSERT INTO rename_table_multiple VALUES (1, 2, 3);
 
diff --git a/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference b/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference
index bbca9bbbfee7..f3fac9b32d31 100644
--- a/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference
+++ b/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference
@@ -7,25 +7,25 @@ file_segment_range_begin: 0
 file_segment_range_end:   745
 size:                     746
 state:                    DOWNLOADED
-7
-7
+8
+8
 0
 2
 2
-7
+8
 Row 1:
 ──────
 file_segment_range_begin: 0
 file_segment_range_end:   1659
 size:                     1660
 state:                    DOWNLOADED
-7
-7
-7
-7
-21
-31
-38
+8
+8
+8
+8
+24
+35
+43
 5010500
 18816
 Using storage policy: local_cache
@@ -37,24 +37,24 @@ file_segment_range_begin: 0
 file_segment_range_end:   745
 size:                     746
 state:                    DOWNLOADED
-7
-7
+8
+8
 0
 2
 2
-7
+8
 Row 1:
 ──────
 file_segment_range_begin: 0
 file_segment_range_end:   1659
 size:                     1660
 state:                    DOWNLOADED
-7
-7
-7
-7
-21
-31
-38
+8
+8
+8
+8
+24
+35
+43
 5010500
 18816
diff --git a/tests/queries/0_stateless/02361_fsync_profile_events.sh b/tests/queries/0_stateless/02361_fsync_profile_events.sh
index 44a1bd58d36e..5b603133f6c7 100755
--- a/tests/queries/0_stateless/02361_fsync_profile_events.sh
+++ b/tests/queries/0_stateless/02361_fsync_profile_events.sh
@@ -44,8 +44,8 @@ for i in {1..100}; do
     ")"
 
     # Non retriable errors
-    if [[ $FileSync -ne 7 ]]; then
-        echo "FileSync: $FileSync != 11" >&2
+    if [[ $FileSync -ne 8 ]]; then
+        echo "FileSync: $FileSync != 8" >&2
         exit 2
     fi
     # Check that all files was synced
diff --git a/tests/queries/0_stateless/02538_alter_rename_sequence.reference b/tests/queries/0_stateless/02538_alter_rename_sequence.reference
new file mode 100644
index 000000000000..73aa1b7e8d8c
--- /dev/null
+++ b/tests/queries/0_stateless/02538_alter_rename_sequence.reference
@@ -0,0 +1,8 @@
+1	2	3
+4	5	6
+{"column1_renamed":"1","column2_renamed":"2","column3":"3"}
+{"column1_renamed":"4","column2_renamed":"5","column3":"6"}
+1	2	3
+4	5	6
+{"column1_renamed":"1","column2_renamed":"2","column3":"3"}
+{"column1_renamed":"4","column2_renamed":"5","column3":"6"}
diff --git a/tests/queries/0_stateless/02538_alter_rename_sequence.sql b/tests/queries/0_stateless/02538_alter_rename_sequence.sql
new file mode 100644
index 000000000000..d7df27dc7020
--- /dev/null
+++ b/tests/queries/0_stateless/02538_alter_rename_sequence.sql
@@ -0,0 +1,59 @@
+DROP TABLE IF EXISTS wrong_metadata;
+
+CREATE TABLE wrong_metadata(
+    column1 UInt64,
+    column2 UInt64,
+    column3 UInt64
+)
+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata', '1')
+ORDER BY tuple();
+
+INSERT INTO wrong_metadata VALUES (1, 2, 3);
+
+SYSTEM STOP REPLICATION QUEUES wrong_metadata;
+
+ALTER TABLE wrong_metadata RENAME COLUMN column1 TO column1_renamed SETTINGS replication_alter_partitions_sync = 0;
+
+INSERT INTO wrong_metadata VALUES (4, 5, 6);
+
+SELECT * FROM wrong_metadata ORDER BY column1;
+
+SYSTEM START REPLICATION QUEUES wrong_metadata;
+
+SYSTEM SYNC REPLICA wrong_metadata;
+
+ALTER TABLE wrong_metadata RENAME COLUMN column2 to column2_renamed SETTINGS replication_alter_partitions_sync = 2;
+
+SELECT * FROM wrong_metadata ORDER BY column1_renamed FORMAT JSONEachRow;
+
+DROP TABLE IF EXISTS wrong_metadata;
+
+
+CREATE TABLE wrong_metadata_wide(
+    column1 UInt64,
+    column2 UInt64,
+    column3 UInt64
+)
+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata_wide', '1')
+ORDER BY tuple()
+SETTINGS min_bytes_for_wide_part = 0;
+
+INSERT INTO wrong_metadata_wide VALUES (1, 2, 3);
+
+SYSTEM STOP REPLICATION QUEUES wrong_metadata_wide;
+
+ALTER TABLE wrong_metadata_wide RENAME COLUMN column1 TO column1_renamed SETTINGS replication_alter_partitions_sync = 0;
+
+INSERT INTO wrong_metadata_wide VALUES (4, 5, 6);
+
+SELECT * FROM wrong_metadata_wide ORDER by column1;
+
+SYSTEM START REPLICATION QUEUES wrong_metadata_wide;
+
+SYSTEM SYNC REPLICA wrong_metadata_wide;
+
+ALTER TABLE wrong_metadata_wide RENAME COLUMN column2 to column2_renamed SETTINGS replication_alter_partitions_sync = 2;
+
+SELECT * FROM wrong_metadata_wide ORDER BY column1_renamed FORMAT JSONEachRow;
+
+DROP TABLE IF EXISTS wrong_metadata_wide;
diff --git a/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference
new file mode 100644
index 000000000000..156128e3dd28
--- /dev/null
+++ b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference
@@ -0,0 +1,1 @@
+{"v":"1","v2":"77"}
diff --git a/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh
new file mode 100755
index 000000000000..adaf1846552f
--- /dev/null
+++ b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh
@@ -0,0 +1,58 @@
+#!/usr/bin/env bash
+
+CURDIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
+# shellcheck source=../shell_config.sh
+. "$CURDIR"/../shell_config.sh
+
+
+$CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS table_to_rename"
+
+$CLICKHOUSE_CLIENT --query="CREATE TABLE table_to_rename(v UInt64, v1 UInt64)ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0"
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO table_to_rename VALUES (1, 1)"
+
+
+# we want to following mutations to stuck
+# That is why we stop merges and wait in loops until they actually start
+$CLICKHOUSE_CLIENT --query="SYSTEM STOP MERGES table_to_rename"
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE table_to_rename RENAME COLUMN v1 to v2" &
+
+counter=0 retries=60
+
+I=0
+while [[ $counter -lt $retries ]]; do
+    I=$((I + 1))
+    result=$($CLICKHOUSE_CLIENT --query "show create table table_to_rename")
+    if [[ $result == *"v2"* ]]; then
+        break;
+    fi
+    sleep 0.1
+    ((++counter))
+done
+
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE table_to_rename UPDATE v2 = 77 WHERE 1 = 1 SETTINGS mutations_sync = 2" &
+
+counter=0 retries=60
+
+I=0
+while [[ $counter -lt $retries ]]; do
+    I=$((I + 1))
+    result=$($CLICKHOUSE_CLIENT --query "SELECT count() from system.mutations where database='${CLICKHOUSE_DATABASE}' and table='table_to_rename'")
+    if [[ $result == "2" ]]; then
+        break;
+    fi
+    sleep 0.1
+    ((++counter))
+done
+
+
+$CLICKHOUSE_CLIENT --query="SYSTEM START MERGES table_to_rename"
+
+wait
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM table_to_rename FORMAT JSONEachRow"
+
+
+ $CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS table_to_rename"
diff --git a/tests/queries/0_stateless/02543_alter_update_rename_stuck.reference b/tests/queries/0_stateless/02543_alter_update_rename_stuck.reference
new file mode 100644
index 000000000000..156128e3dd28
--- /dev/null
+++ b/tests/queries/0_stateless/02543_alter_update_rename_stuck.reference
@@ -0,0 +1,1 @@
+{"v":"1","v2":"77"}
diff --git a/tests/queries/0_stateless/02543_alter_update_rename_stuck.sh b/tests/queries/0_stateless/02543_alter_update_rename_stuck.sh
new file mode 100755
index 000000000000..e801fbedab79
--- /dev/null
+++ b/tests/queries/0_stateless/02543_alter_update_rename_stuck.sh
@@ -0,0 +1,48 @@
+#!/usr/bin/env bash
+
+CURDIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
+# shellcheck source=../shell_config.sh
+. "$CURDIR"/../shell_config.sh
+
+
+$CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS table_to_rename"
+
+$CLICKHOUSE_CLIENT --query="CREATE TABLE table_to_rename(v UInt64, v1 UInt64)ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0"
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO table_to_rename VALUES (1, 1)"
+
+
+# we want to following mutations to stuck
+# That is why we stop merges and wait in loops until they actually start
+$CLICKHOUSE_CLIENT --query="SYSTEM STOP MERGES table_to_rename"
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE table_to_rename UPDATE v1 = 77 WHERE 1 = 1 SETTINGS mutations_sync = 2" &
+
+counter=0 retries=60
+
+I=0
+while [[ $counter -lt $retries ]]; do
+    I=$((I + 1))
+    result=$($CLICKHOUSE_CLIENT --query "SELECT count() from system.mutations where database='${CLICKHOUSE_DATABASE}' and table='table_to_rename'")
+    if [[ $result == "1" ]]; then
+        break;
+    fi
+    sleep 0.1
+    ((++counter))
+done
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE table_to_rename RENAME COLUMN v1 to v2" &
+
+
+# it will not introduce any flakyness
+# just wait that mutation doesn't start
+sleep 3
+
+$CLICKHOUSE_CLIENT --query="SYSTEM START MERGES table_to_rename"
+
+wait
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM table_to_rename FORMAT JSONEachRow"
+
+
+ $CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS table_to_rename"
diff --git a/tests/queries/0_stateless/02555_davengers_rename_chain.reference b/tests/queries/0_stateless/02555_davengers_rename_chain.reference
new file mode 100644
index 000000000000..a9fc4b395e20
--- /dev/null
+++ b/tests/queries/0_stateless/02555_davengers_rename_chain.reference
@@ -0,0 +1,26 @@
+{"a1":"1","b1":"2","c":"3"}
+~~~~~~~
+{"a1":"1","b1":"2","c":"3"}
+{"a1":"4","b1":"5","c":"6"}
+~~~~~~~
+{"a1":"1","b1":"2","c":"3"}
+{"a1":"4","b1":"5","c":"6"}
+{"a1":"7","b1":"8","c":"9"}
+~~~~~~~
+{"b":"1","a":"2","c":"3"}
+{"b":"4","a":"5","c":"6"}
+{"b":"7","a":"8","c":"9"}
+~~~~~~~
+{"a1":"1","b1":"2","c":"3"}
+~~~~~~~
+{"a1":"1","b1":"2","c":"3"}
+{"a1":"4","b1":"5","c":"6"}
+~~~~~~~
+{"a1":"1","b1":"2","c":"3"}
+{"a1":"4","b1":"5","c":"6"}
+{"a1":"7","b1":"8","c":"9"}
+~~~~~~~
+{"b":"1","a":"2","c":"3"}
+{"b":"4","a":"5","c":"6"}
+{"b":"7","a":"8","c":"9"}
+~~~~~~~
diff --git a/tests/queries/0_stateless/02555_davengers_rename_chain.sh b/tests/queries/0_stateless/02555_davengers_rename_chain.sh
new file mode 100755
index 000000000000..b23f8085fd70
--- /dev/null
+++ b/tests/queries/0_stateless/02555_davengers_rename_chain.sh
@@ -0,0 +1,143 @@
+#!/usr/bin/env bash
+# Tags: replica
+CUR_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
+# shellcheck source=../shell_config.sh
+. "$CUR_DIR"/../shell_config.sh
+
+$CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS wrong_metadata"
+
+$CLICKHOUSE_CLIENT -n --query="CREATE TABLE wrong_metadata(
+    a UInt64,
+    b UInt64,
+    c UInt64
+)
+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata', '1')
+ORDER BY tuple()
+SETTINGS min_bytes_for_wide_part = 0"
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO wrong_metadata VALUES (1, 2, 3)"
+
+
+$CLICKHOUSE_CLIENT --query="SYSTEM STOP MERGES wrong_metadata"
+
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE wrong_metadata RENAME COLUMN a TO a1, RENAME COLUMN b to b1 SETTINGS replication_alter_partitions_sync = 0"
+
+counter=0 retries=60
+I=0
+while [[ $counter -lt $retries ]]; do
+    I=$((I + 1))
+    result=$($CLICKHOUSE_CLIENT --query "SHOW CREATE TABLE wrong_metadata")
+    if [[ $result == *"\`a1\` UInt64"* ]]; then
+        break;
+    fi
+    sleep 0.1
+    ((++counter))
+done
+
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata ORDER BY a1 FORMAT JSONEachRow"
+
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO wrong_metadata VALUES (4, 5, 6)"
+
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata ORDER BY a1 FORMAT JSONEachRow"
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE wrong_metadata RENAME COLUMN a1 TO b, RENAME COLUMN b1 to a SETTINGS replication_alter_partitions_sync = 0"
+
+counter=0 retries=60
+I=0
+while [[ $counter -lt $retries ]]; do
+    I=$((I + 1))
+    result=$($CLICKHOUSE_CLIENT --query "SELECT * FROM system.mutations WHERE table = 'wrong_metadata' AND database='${CLICKHOUSE_DATABASE}'")
+    if [[ $result == *"b1 TO a"* ]]; then
+        break;
+    fi
+    sleep 0.1
+    ((++counter))
+done
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO wrong_metadata VALUES (7, 8, 9)"
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata ORDER by a1 FORMAT JSONEachRow"
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+$CLICKHOUSE_CLIENT --query="SYSTEM START MERGES wrong_metadata"
+
+$CLICKHOUSE_CLIENT --query="SYSTEM SYNC REPLICA wrong_metadata"
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata order by a FORMAT JSONEachRow"
+
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+
+$CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS wrong_metadata"
+
+$CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS wrong_metadata_compact"
+
+$CLICKHOUSE_CLIENT -n --query="CREATE TABLE wrong_metadata_compact(
+    a UInt64,
+    b UInt64,
+    c UInt64
+)
+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata_compact', '1')
+ORDER BY tuple()
+SETTINGS min_bytes_for_wide_part = 10000000"
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO wrong_metadata_compact VALUES (1, 2, 3)"
+
+$CLICKHOUSE_CLIENT --query="SYSTEM STOP MERGES wrong_metadata_compact"
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE wrong_metadata_compact RENAME COLUMN a TO a1, RENAME COLUMN b to b1 SETTINGS replication_alter_partitions_sync = 0"
+
+counter=0 retries=60
+I=0
+while [[ $counter -lt $retries ]]; do
+    I=$((I + 1))
+    result=$($CLICKHOUSE_CLIENT --query "SHOW CREATE TABLE wrong_metadata_compact")
+    if [[ $result == *"\`a1\` UInt64"* ]]; then
+        break;
+    fi
+    sleep 0.1
+    ((++counter))
+done
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata_compact ORDER BY a1 FORMAT JSONEachRow"
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO wrong_metadata_compact VALUES (4, 5, 6)"
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata_compact ORDER BY a1 FORMAT JSONEachRow"
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+$CLICKHOUSE_CLIENT --query="ALTER TABLE wrong_metadata_compact RENAME COLUMN a1 TO b, RENAME COLUMN b1 to a SETTINGS replication_alter_partitions_sync = 0"
+
+counter=0 retries=60
+I=0
+while [[ $counter -lt $retries ]]; do
+    I=$((I + 1))
+    result=$($CLICKHOUSE_CLIENT --query "SELECT * FROM system.mutations WHERE table = 'wrong_metadata_compact' AND database='${CLICKHOUSE_DATABASE}'")
+    if [[ $result == *"b1 TO a"* ]]; then
+        break;
+    fi
+    sleep 0.1
+    ((++counter))
+done
+
+$CLICKHOUSE_CLIENT --query="INSERT INTO wrong_metadata_compact VALUES (7, 8, 9)"
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata_compact ORDER by a1 FORMAT JSONEachRow"
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+$CLICKHOUSE_CLIENT --query="SYSTEM START MERGES wrong_metadata_compact"
+
+$CLICKHOUSE_CLIENT --query="SYSTEM SYNC REPLICA wrong_metadata_compact"
+
+$CLICKHOUSE_CLIENT --query="SELECT * FROM wrong_metadata_compact order by a FORMAT JSONEachRow"
+$CLICKHOUSE_CLIENT --query="SELECT '~~~~~~~'"
+
+$CLICKHOUSE_CLIENT --query="DROP TABLE IF EXISTS wrong_metadata_compact"
diff --git a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference
index 2d41f5dae893..00e93b1db3d1 100644
--- a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference
+++ b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference
@@ -9,15 +9,15 @@ CHECK WITH query_log
 QueryFinish	S3CreateMultipartUpload	1	S3UploadPart	1	S3CompleteMultipartUpload	1	S3PutObject	0
 CREATE
 INSERT
- [ 0 ] FileOpen: 7 
+ [ 0 ] FileOpen: 8 
 READ
 INSERT and READ INSERT
- [ 0 ] FileOpen: 7 
- [ 0 ] FileOpen: 7 
+ [ 0 ] FileOpen: 8 
+ [ 0 ] FileOpen: 8 
 DROP
 CHECK with query_log
-QueryFinish	INSERT INTO times SELECT now() + INTERVAL 1 day SETTINGS optimize_on_insert = 0;	FileOpen	7
+QueryFinish	INSERT INTO times SELECT now() + INTERVAL 1 day SETTINGS optimize_on_insert = 0;	FileOpen	8
 QueryFinish	SELECT \'1\', min(t) FROM times;	FileOpen	0
-QueryFinish	INSERT INTO times SELECT now() + INTERVAL 2 day SETTINGS optimize_on_insert = 0;	FileOpen	7
+QueryFinish	INSERT INTO times SELECT now() + INTERVAL 2 day SETTINGS optimize_on_insert = 0;	FileOpen	8
 QueryFinish	SELECT \'2\', min(t) FROM times;	FileOpen	0
-QueryFinish	INSERT INTO times SELECT now() + INTERVAL 3 day SETTINGS optimize_on_insert = 0;	FileOpen	7
+QueryFinish	INSERT INTO times SELECT now() + INTERVAL 3 day SETTINGS optimize_on_insert = 0;	FileOpen	8
