{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 46946,
  "instance_id": "ClickHouse__ClickHouse-46946",
  "issue_numbers": [
    "34342"
  ],
  "base_commit": "17aecb797cb863ea02251b9f0daa4a4a12551de2",
  "patch": "diff --git a/src/Storages/MergeTree/AlterConversions.cpp b/src/Storages/MergeTree/AlterConversions.cpp\nnew file mode 100644\nindex 000000000000..7a298b0f6ca0\n--- /dev/null\n+++ b/src/Storages/MergeTree/AlterConversions.cpp\n@@ -0,0 +1,55 @@\n+#include <Storages/MergeTree/AlterConversions.h>\n+#include <Common/Exception.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+bool AlterConversions::columnHasNewName(const std::string & old_name) const\n+{\n+    for (const auto & [new_name, prev_name] : rename_map)\n+    {\n+        if (old_name == prev_name)\n+            return true;\n+    }\n+\n+    return false;\n+}\n+\n+std::string AlterConversions::getColumnNewName(const std::string & old_name) const\n+{\n+    for (const auto & [new_name, prev_name] : rename_map)\n+    {\n+        if (old_name == prev_name)\n+            return new_name;\n+    }\n+\n+    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Column {} was not renamed\", old_name);\n+}\n+\n+\n+bool AlterConversions::isColumnRenamed(const std::string & new_name) const\n+{\n+    for (const auto & [name_to, name_from] : rename_map)\n+    {\n+        if (name_to == new_name)\n+            return true;\n+    }\n+    return false;\n+}\n+/// Get column old name before rename (lookup by key in rename_map)\n+std::string AlterConversions::getColumnOldName(const std::string & new_name) const\n+{\n+    for (const auto & [name_to, name_from] : rename_map)\n+    {\n+        if (name_to == new_name)\n+            return name_from;\n+    }\n+    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Column {} was not renamed\", new_name);\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/AlterConversions.h b/src/Storages/MergeTree/AlterConversions.h\nindex 0d58499d424b..ada385d61008 100644\n--- a/src/Storages/MergeTree/AlterConversions.h\n+++ b/src/Storages/MergeTree/AlterConversions.h\n@@ -14,11 +14,22 @@ namespace DB\n /// part->getColumns() and storage->getColumns().\n struct AlterConversions\n {\n+    struct RenamePair\n+    {\n+        std::string rename_to;\n+        std::string rename_from;\n+    };\n     /// Rename map new_name -> old_name\n-    std::unordered_map<std::string, std::string> rename_map;\n+    std::vector<RenamePair> rename_map;\n \n-    bool isColumnRenamed(const std::string & new_name) const { return rename_map.count(new_name) > 0; }\n-    std::string getColumnOldName(const std::string & new_name) const { return rename_map.at(new_name); }\n+    /// Column was renamed (lookup by value in rename_map)\n+    bool columnHasNewName(const std::string & old_name) const;\n+    /// Get new name for column (lookup by value in rename_map)\n+    std::string getColumnNewName(const std::string & old_name) const;\n+    /// Is this name is new name of column (lookup by key in rename_map)\n+    bool isColumnRenamed(const std::string & new_name) const;\n+    /// Get column old name before rename (lookup by key in rename_map)\n+    std::string getColumnOldName(const std::string & new_name) const;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp b/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\nindex 9b601d9f3afb..09456088d74d 100644\n--- a/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\n+++ b/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\n@@ -214,6 +214,11 @@ bool DataPartStorageOnDiskBase::isBroken() const\n     return volume->getDisk()->isBroken();\n }\n \n+bool DataPartStorageOnDiskBase::isReadonly() const\n+{\n+    return volume->getDisk()->isReadOnly();\n+}\n+\n void DataPartStorageOnDiskBase::syncRevision(UInt64 revision) const\n {\n     volume->getDisk()->syncRevision(revision);\n@@ -685,6 +690,7 @@ void DataPartStorageOnDiskBase::clearDirectory(\n         request.emplace_back(fs::path(dir) / \"default_compression_codec.txt\", true);\n         request.emplace_back(fs::path(dir) / \"delete-on-destroy.txt\", true);\n         request.emplace_back(fs::path(dir) / \"txn_version.txt\", true);\n+        request.emplace_back(fs::path(dir) / \"metadata_version.txt\", true);\n \n         disk->removeSharedFiles(request, !can_remove_shared_data, names_not_to_remove);\n         disk->removeDirectory(dir);\ndiff --git a/src/Storages/MergeTree/DataPartStorageOnDiskBase.h b/src/Storages/MergeTree/DataPartStorageOnDiskBase.h\nindex 11806e25a1ea..fcc771f1250b 100644\n--- a/src/Storages/MergeTree/DataPartStorageOnDiskBase.h\n+++ b/src/Storages/MergeTree/DataPartStorageOnDiskBase.h\n@@ -39,6 +39,7 @@ class DataPartStorageOnDiskBase : public IDataPartStorage\n     bool supportZeroCopyReplication() const override;\n     bool supportParallelWrite() const override;\n     bool isBroken() const override;\n+    bool isReadonly() const override;\n     void syncRevision(UInt64 revision) const override;\n     UInt64 getRevision() const override;\n     std::string getDiskPath() const override;\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex a80870448f18..46c6d09eca47 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -64,8 +64,9 @@ constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_DEFAULT_COMPRESSION = 4;\n constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_UUID = 5;\n constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_ZERO_COPY = 6;\n constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PROJECTION = 7;\n+constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION = 8;\n // Reserved for ALTER PRIMARY KEY\n-// constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PRIMARY_KEY = 8;\n+// constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PRIMARY_KEY = 9;\n \n std::string getEndpointId(const std::string & node_id)\n {\n@@ -121,7 +122,7 @@ void Service::processQuery(const HTMLForm & params, ReadBuffer & /*body*/, Write\n     MergeTreePartInfo::fromPartName(part_name, data.format_version);\n \n     /// We pretend to work as older server version, to be sure that client will correctly process our version\n-    response.addCookie({\"server_protocol_version\", toString(std::min(client_protocol_version, REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PROJECTION))});\n+    response.addCookie({\"server_protocol_version\", toString(std::min(client_protocol_version, REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION))});\n \n     LOG_TRACE(log, \"Sending part {}\", part_name);\n \n@@ -282,6 +283,10 @@ MergeTreeData::DataPart::Checksums Service::sendPartFromDisk(\n             && name == IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME)\n             continue;\n \n+        if (client_protocol_version < REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION\n+            && name == IMergeTreeDataPart::METADATA_VERSION_FILE_NAME)\n+            continue;\n+\n         files_to_replicate.insert(name);\n     }\n \n@@ -409,7 +414,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchSelectedPart(\n     {\n         {\"endpoint\",                getEndpointId(replica_path)},\n         {\"part\",                    part_name},\n-        {\"client_protocol_version\", toString(REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PROJECTION)},\n+        {\"client_protocol_version\", toString(REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION)},\n         {\"compress\",                \"false\"}\n     });\n \n@@ -709,7 +714,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(\n     auto block = block_in.read();\n     throttler->add(block.bytes());\n \n-    new_data_part->setColumns(block.getNamesAndTypesList(), {});\n+    new_data_part->setColumns(block.getNamesAndTypesList(), {}, metadata_snapshot->getMetadataVersion());\n \n     if (!is_projection)\n     {\n@@ -785,7 +790,8 @@ void Fetcher::downloadBaseOrProjectionPartToDisk(\n \n         if (file_name != \"checksums.txt\" &&\n             file_name != \"columns.txt\" &&\n-            file_name != IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME)\n+            file_name != IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME &&\n+            file_name != IMergeTreeDataPart::METADATA_VERSION_FILE_NAME)\n             checksums.addFile(file_name, file_size, expected_hash);\n     }\n \ndiff --git a/src/Storages/MergeTree/IDataPartStorage.h b/src/Storages/MergeTree/IDataPartStorage.h\nindex 2d61f9ee77be..b0b42b331cdc 100644\n--- a/src/Storages/MergeTree/IDataPartStorage.h\n+++ b/src/Storages/MergeTree/IDataPartStorage.h\n@@ -150,6 +150,7 @@ class IDataPartStorage : public boost::noncopyable\n     virtual bool supportZeroCopyReplication() const { return false; }\n     virtual bool supportParallelWrite() const = 0;\n     virtual bool isBroken() const = 0;\n+    virtual bool isReadonly() const = 0;\n \n     /// TODO: remove or at least remove const.\n     virtual void syncRevision(UInt64 revision) const = 0;\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 5ed4987a6d57..148cbf939489 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -418,10 +418,11 @@ std::pair<time_t, time_t> IMergeTreeDataPart::getMinMaxTime() const\n }\n \n \n-void IMergeTreeDataPart::setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos)\n+void IMergeTreeDataPart::setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos, int32_t metadata_version_)\n {\n     columns = new_columns;\n     serialization_infos = new_infos;\n+    metadata_version = metadata_version_;\n \n     column_name_to_position.clear();\n     column_name_to_position.reserve(new_columns.size());\n@@ -662,6 +663,7 @@ void IMergeTreeDataPart::appendFilesOfColumnsChecksumsIndexes(Strings & files, b\n         appendFilesOfPartitionAndMinMaxIndex(files);\n         appendFilesOfTTLInfos(files);\n         appendFilesOfDefaultCompressionCodec(files);\n+        appendFilesOfMetadataVersion(files);\n     }\n \n     if (!parent_part && include_projection)\n@@ -800,6 +802,9 @@ NameSet IMergeTreeDataPart::getFileNamesWithoutChecksums() const\n     if (getDataPartStorage().exists(TXN_VERSION_METADATA_FILE_NAME))\n         result.emplace(TXN_VERSION_METADATA_FILE_NAME);\n \n+    if (getDataPartStorage().exists(METADATA_VERSION_FILE_NAME))\n+        result.emplace(METADATA_VERSION_FILE_NAME);\n+\n     return result;\n }\n \n@@ -973,11 +978,22 @@ void IMergeTreeDataPart::removeVersionMetadata()\n     getDataPartStorage().removeFileIfExists(\"txn_version.txt\");\n }\n \n+\n+void IMergeTreeDataPart::removeMetadataVersion()\n+{\n+    getDataPartStorage().removeFileIfExists(METADATA_VERSION_FILE_NAME);\n+}\n+\n void IMergeTreeDataPart::appendFilesOfDefaultCompressionCodec(Strings & files)\n {\n     files.push_back(DEFAULT_COMPRESSION_CODEC_FILE_NAME);\n }\n \n+void IMergeTreeDataPart::appendFilesOfMetadataVersion(Strings & files)\n+{\n+    files.push_back(METADATA_VERSION_FILE_NAME);\n+}\n+\n CompressionCodecPtr IMergeTreeDataPart::detectDefaultCompressionCodec() const\n {\n     /// In memory parts doesn't have any compression\n@@ -1290,8 +1306,9 @@ void IMergeTreeDataPart::loadColumns(bool require)\n         metadata_snapshot = metadata_snapshot->projections.get(name).metadata;\n     NamesAndTypesList loaded_columns;\n \n-    bool exists = metadata_manager->exists(\"columns.txt\");\n-    if (!exists)\n+    bool is_readonly_storage = getDataPartStorage().isReadonly();\n+\n+    if (!metadata_manager->exists(\"columns.txt\"))\n     {\n         /// We can get list of columns only from columns.txt in compact parts.\n         if (require || part_type == Type::Compact)\n@@ -1306,7 +1323,8 @@ void IMergeTreeDataPart::loadColumns(bool require)\n         if (columns.empty())\n             throw Exception(ErrorCodes::NO_FILE_IN_DATA_PART, \"No columns in part {}\", name);\n \n-        writeColumns(loaded_columns, {});\n+        if (!is_readonly_storage)\n+            writeColumns(loaded_columns, {});\n     }\n     else\n     {\n@@ -1324,16 +1342,35 @@ void IMergeTreeDataPart::loadColumns(bool require)\n     };\n \n     SerializationInfoByName infos(loaded_columns, settings);\n-    exists =  metadata_manager->exists(SERIALIZATION_FILE_NAME);\n-    if (exists)\n+    if (metadata_manager->exists(SERIALIZATION_FILE_NAME))\n     {\n         auto in = metadata_manager->read(SERIALIZATION_FILE_NAME);\n         infos.readJSON(*in);\n     }\n \n-    setColumns(loaded_columns, infos);\n+    int32_t loaded_metadata_version;\n+    if (metadata_manager->exists(METADATA_VERSION_FILE_NAME))\n+    {\n+        auto in = metadata_manager->read(METADATA_VERSION_FILE_NAME);\n+        readIntText(loaded_metadata_version, *in);\n+    }\n+    else\n+    {\n+        loaded_metadata_version = metadata_snapshot->getMetadataVersion();\n+\n+        if (!is_readonly_storage)\n+        {\n+            writeMetadata(METADATA_VERSION_FILE_NAME, {}, [loaded_metadata_version](auto & buffer)\n+            {\n+                writeIntText(loaded_metadata_version, buffer);\n+            });\n+        }\n+    }\n+\n+    setColumns(loaded_columns, infos, loaded_metadata_version);\n }\n \n+\n /// Project part / part with project parts / compact part doesn't support LWD.\n bool IMergeTreeDataPart::supportLightweightDeleteMutate() const\n {\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex 4403f79dfaa6..a36634d2cf9c 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -137,7 +137,11 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     String getTypeName() const { return getType().toString(); }\n \n-    void setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos);\n+    /// We could have separate method like setMetadata, but it's much more convenient to set it up with columns\n+    void setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos, int32_t metadata_version_);\n+\n+    /// Version of metadata for part (columns, pk and so on)\n+    int32_t getMetadataVersion() const { return metadata_version; }\n \n     const NamesAndTypesList & getColumns() const { return columns; }\n     const ColumnsDescription & getColumnsDescription() const { return columns_description; }\n@@ -312,6 +316,9 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     mutable VersionMetadata version;\n \n+    /// Version of part metadata (columns, pk and so on). Managed properly only for replicated merge tree.\n+    int32_t metadata_version;\n+\n     /// For data in RAM ('index')\n     UInt64 getIndexSizeInBytes() const;\n     UInt64 getIndexSizeInAllocatedBytes() const;\n@@ -383,8 +390,12 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     /// (number of rows, number of rows with default values, etc).\n     static inline constexpr auto SERIALIZATION_FILE_NAME = \"serialization.json\";\n \n+    /// Version used for transactions.\n     static inline constexpr auto TXN_VERSION_METADATA_FILE_NAME = \"txn_version.txt\";\n \n+\n+    static inline constexpr auto METADATA_VERSION_FILE_NAME = \"metadata_version.txt\";\n+\n     /// One of part files which is used to check how many references (I'd like\n     /// to say hardlinks, but it will confuse even more) we have for the part\n     /// for zero copy replication. Sadly it's very complex.\n@@ -447,7 +458,11 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     void writeDeleteOnDestroyMarker();\n     void removeDeleteOnDestroyMarker();\n+    /// It may look like a stupid joke. but these two methods are absolutely unrelated.\n+    /// This one is about removing file with metadata about part version (for transactions)\n     void removeVersionMetadata();\n+    /// This one is about removing file with version of part's metadata (columns, pk and so on)\n+    void removeMetadataVersion();\n \n     mutable std::atomic<DataPartRemovalState> removal_state = DataPartRemovalState::NOT_ATTEMPTED;\n \n@@ -586,6 +601,8 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     static void appendFilesOfDefaultCompressionCodec(Strings & files);\n \n+    static void appendFilesOfMetadataVersion(Strings & files);\n+\n     /// Found column without specific compression and return codec\n     /// for this column with default parameters.\n     CompressionCodecPtr detectDefaultCompressionCodec() const;\ndiff --git a/src/Storages/MergeTree/MergeTask.cpp b/src/Storages/MergeTree/MergeTask.cpp\nindex 749bafab6b81..df759b3bd45f 100644\n--- a/src/Storages/MergeTree/MergeTask.cpp\n+++ b/src/Storages/MergeTree/MergeTask.cpp\n@@ -246,7 +246,7 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare()\n         }\n     }\n \n-    global_ctx->new_data_part->setColumns(global_ctx->storage_columns, infos);\n+    global_ctx->new_data_part->setColumns(global_ctx->storage_columns, infos, global_ctx->metadata_snapshot->getMetadataVersion());\n \n     const auto & local_part_min_ttl = global_ctx->new_data_part->ttl_infos.part_min_ttl;\n     if (local_part_min_ttl && local_part_min_ttl <= global_ctx->time_of_merge)\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex aace98806ec8..45759c449f6c 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -4497,6 +4497,11 @@ MergeTreeData::DataPartPtr MergeTreeData::getPartIfExistsUnlocked(const MergeTre\n \n static void loadPartAndFixMetadataImpl(MergeTreeData::MutableDataPartPtr part)\n {\n+    /// Remove metadata version file and take it from table.\n+    /// Currently we cannot attach parts with different schema, so\n+    /// we can assume that it's equal to table's current schema.\n+    part->removeMetadataVersion();\n+\n     part->loadColumnsChecksumsIndexes(false, true);\n     part->modification_time = part->getDataPartStorage().getLastModified().epochTime();\n     part->removeDeleteOnDestroyMarker();\n@@ -7744,15 +7749,23 @@ bool MergeTreeData::canUsePolymorphicParts(const MergeTreeSettings & settings, S\n \n AlterConversions MergeTreeData::getAlterConversionsForPart(const MergeTreeDataPartPtr part) const\n {\n-    MutationCommands commands = getFirstAlterMutationCommandsForPart(part);\n+    std::map<int64_t, MutationCommands> commands_map = getAlterMutationCommandsForPart(part);\n \n     AlterConversions result{};\n-    for (const auto & command : commands)\n-        /// Currently we need explicit conversions only for RENAME alter\n-        /// all other conversions can be deduced from diff between part columns\n-        /// and columns in storage.\n-        if (command.type == MutationCommand::Type::RENAME_COLUMN)\n-            result.rename_map[command.rename_to] = command.column_name;\n+    auto & rename_map = result.rename_map;\n+    for (const auto & [version, commands] : commands_map)\n+    {\n+        for (const auto & command : commands)\n+        {\n+            /// Currently we need explicit conversions only for RENAME alter\n+            /// all other conversions can be deduced from diff between part columns\n+            /// and columns in storage.\n+            if (command.type == MutationCommand::Type::RENAME_COLUMN)\n+            {\n+                rename_map.emplace_back(AlterConversions::RenamePair{command.rename_to, command.column_name});\n+            }\n+        }\n+    }\n \n     return result;\n }\n@@ -8158,7 +8171,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeData::createEmptyPart(\n     if (settings->assign_part_uuids)\n         new_data_part->uuid = UUIDHelpers::generateV4();\n \n-    new_data_part->setColumns(columns, {});\n+    new_data_part->setColumns(columns, {}, metadata_snapshot->getMetadataVersion());\n     new_data_part->rows_count = block.rows();\n \n     new_data_part->partition = partition;\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex bb03831bb081..b03b7d4a71ee 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -1310,7 +1310,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Used to receive AlterConversions for part and apply them on fly. This\n     /// method has different implementations for replicated and non replicated\n     /// MergeTree because they store mutations in different way.\n-    virtual MutationCommands getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const = 0;\n+    virtual std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const DataPartPtr & part) const = 0;\n     /// Moves part to specified space, used in ALTER ... MOVE ... queries\n     MovePartsOutcome movePartsToSpace(const DataPartsVector & parts, SpacePtr space);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\nindex 20049976acfc..5b1054d0a0e4 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n@@ -73,7 +73,7 @@ MutableDataPartStoragePtr MergeTreeDataPartInMemory::flushToDisk(const String &\n     new_data_part_storage->beginTransaction();\n \n     new_data_part->uuid = uuid;\n-    new_data_part->setColumns(columns, {});\n+    new_data_part->setColumns(columns, {}, metadata_snapshot->getMetadataVersion());\n     new_data_part->partition.value = partition.value;\n     new_data_part->minmax_idx = minmax_idx;\n \n@@ -104,7 +104,7 @@ MutableDataPartStoragePtr MergeTreeDataPartInMemory::flushToDisk(const String &\n                 .build();\n \n             new_projection_part->is_temp = false; // clean up will be done on parent part\n-            new_projection_part->setColumns(projection->getColumns(), {});\n+            new_projection_part->setColumns(projection->getColumns(), {}, metadata_snapshot->getMetadataVersion());\n \n             auto new_projection_part_storage = new_projection_part->getDataPartStoragePtr();\n             if (new_projection_part_storage->exists())\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 37cfe4d065e0..adb7505a8ba8 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -464,7 +464,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     SerializationInfoByName infos(columns, settings);\n     infos.add(block);\n \n-    new_data_part->setColumns(columns, infos);\n+    new_data_part->setColumns(columns, infos, metadata_snapshot->getMetadataVersion());\n     new_data_part->rows_count = block.rows();\n     new_data_part->partition = std::move(partition);\n     new_data_part->minmax_idx = std::move(minmax_idx);\n@@ -586,7 +586,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n     SerializationInfoByName infos(columns, settings);\n     infos.add(block);\n \n-    new_data_part->setColumns(columns, infos);\n+    new_data_part->setColumns(columns, infos, metadata_snapshot->getMetadataVersion());\n \n     if (new_data_part->isStoredOnDisk())\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\nindex ed8866b0044b..c6bb021e80fc 100644\n--- a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n+++ b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n@@ -106,6 +106,15 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n     // representation.\n     PODArray<MarkInCompressedFile> plain_marks(marks_count * columns_in_mark); // temporary\n \n+    if (file_size == 0 && marks_count != 0)\n+    {\n+        throw Exception(\n+            ErrorCodes::CORRUPTED_DATA,\n+            \"Empty marks file '{}': {}, must be: {}\",\n+            std::string(fs::path(data_part_storage->getFullPath()) / mrk_path),\n+            file_size, expected_uncompressed_size);\n+    }\n+\n     if (!index_granularity_info.mark_type.compressed && expected_uncompressed_size != file_size)\n         throw Exception(\n             ErrorCodes::CORRUPTED_DATA,\n@@ -148,7 +157,12 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n         }\n \n         if (i * mark_size != expected_uncompressed_size)\n-            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Cannot read all marks from file {}\", mrk_path);\n+        {\n+            throw Exception(\n+                ErrorCodes::CANNOT_READ_ALL_DATA,\n+                \"Cannot read all marks from file {}, marks expected {} (bytes size {}), marks read {} (bytes size {})\",\n+                mrk_path, marks_count, expected_uncompressed_size, i, reader->count());\n+        }\n     }\n \n     auto res = std::make_shared<MarksInCompressedFile>(plain_marks);\ndiff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\nindex 93e07a38bb18..23832464758b 100644\n--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n@@ -230,7 +230,7 @@ MergeTreeData::MutableDataPartsVector MergeTreeWriteAheadLog::restore(\n \n             part->minmax_idx->update(block, storage.getMinMaxColumnsNames(metadata_snapshot->getPartitionKey()));\n             part->partition.create(metadata_snapshot, block, 0, context);\n-            part->setColumns(block.getNamesAndTypesList(), {});\n+            part->setColumns(block.getNamesAndTypesList(), {}, metadata_snapshot->getMetadataVersion());\n             if (metadata_snapshot->hasSortingKey())\n                 metadata_snapshot->getSortingKey().expression->execute(block);\n \ndiff --git a/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\nindex 715c218c2db3..d97da5a0b508 100644\n--- a/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n@@ -176,7 +176,7 @@ MergedBlockOutputStream::Finalizer MergedBlockOutputStream::finalizePartAsync(\n         serialization_infos.replaceData(new_serialization_infos);\n         files_to_remove_after_sync = removeEmptyColumnsFromPart(new_part, part_columns, serialization_infos, checksums);\n \n-        new_part->setColumns(part_columns, serialization_infos);\n+        new_part->setColumns(part_columns, serialization_infos, metadata_snapshot->getMetadataVersion());\n     }\n \n     auto finalizer = std::make_unique<Finalizer::Impl>(*writer, new_part, files_to_remove_after_sync, sync);\n@@ -290,6 +290,14 @@ MergedBlockOutputStream::WrittenFiles MergedBlockOutputStream::finalizePartOnDis\n         written_files.emplace_back(std::move(out));\n     }\n \n+    {\n+        /// Write a file with a description of columns.\n+        auto out = new_part->getDataPartStorage().writeFile(IMergeTreeDataPart::METADATA_VERSION_FILE_NAME, 4096, write_settings);\n+        DB::writeIntText(new_part->getMetadataVersion(), *out);\n+        out->preFinalize();\n+        written_files.emplace_back(std::move(out));\n+    }\n+\n     if (default_codec != nullptr)\n     {\n         auto out = new_part->getDataPartStorage().writeFile(IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME, 4096, write_settings);\ndiff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\nindex f6fc40884a1d..3b2eb96f2d4e 100644\n--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n@@ -85,7 +85,7 @@ MergedColumnOnlyOutputStream::fillChecksums(\n             all_checksums.files.erase(removed_file);\n     }\n \n-    new_part->setColumns(columns, serialization_infos);\n+    new_part->setColumns(columns, serialization_infos, metadata_snapshot->getMetadataVersion());\n     return checksums;\n }\n \ndiff --git a/src/Storages/MergeTree/MutateTask.cpp b/src/Storages/MergeTree/MutateTask.cpp\nindex b4d0a3107858..6f96683971b5 100644\n--- a/src/Storages/MergeTree/MutateTask.cpp\n+++ b/src/Storages/MergeTree/MutateTask.cpp\n@@ -54,7 +54,7 @@ static bool checkOperationIsNotCanceled(ActionBlocker & merges_blocker, MergeLis\n *   First part should be executed by mutations interpreter.\n *   Other is just simple drop/renames, so they can be executed without interpreter.\n */\n-static void splitMutationCommands(\n+static void splitAndModifyMutationCommands(\n     MergeTreeData::DataPartPtr part,\n     const MutationCommands & commands,\n     MutationCommands & for_interpreter,\n@@ -64,7 +64,7 @@ static void splitMutationCommands(\n \n     if (!isWidePart(part) || !isFullPartStorage(part->getDataPartStorage()))\n     {\n-        NameSet mutated_columns;\n+        NameSet mutated_columns, dropped_columns;\n         for (const auto & command : commands)\n         {\n             if (command.type == MutationCommand::Type::MATERIALIZE_INDEX\n@@ -98,26 +98,63 @@ static void splitMutationCommands(\n                     }\n                     else\n                         mutated_columns.emplace(command.column_name);\n+\n+                    if (command.type == MutationCommand::Type::DROP_COLUMN)\n+                        dropped_columns.emplace(command.column_name);\n                 }\n+            }\n+\n+        }\n \n-                if (command.type == MutationCommand::Type::RENAME_COLUMN)\n+        auto alter_conversions = part->storage.getAlterConversionsForPart(part);\n+\n+        /// We don't add renames from commands, instead we take them from rename_map.\n+        /// It's important because required renames depend not only on part's data version (i.e. mutation version)\n+        /// but also on part's metadata version. Why we have such logic only for renames? Because all other types of alter\n+        /// can be deduced based on difference between part's schema and table schema.\n+        for (const auto & [rename_to, rename_from] : alter_conversions.rename_map)\n+        {\n+            if (part_columns.has(rename_from))\n+            {\n+                /// Actual rename\n+                for_interpreter.push_back(\n                 {\n-                    for_interpreter.push_back(\n-                    {\n-                        .type = MutationCommand::Type::READ_COLUMN,\n-                        .column_name = command.rename_to,\n-                    });\n-                    part_columns.rename(command.column_name, command.rename_to);\n-                }\n+                    .type = MutationCommand::Type::READ_COLUMN,\n+                    .column_name = rename_to,\n+                });\n+\n+                /// Not needed for compact parts (not executed), added here only to produce correct\n+                /// set of columns for new part and their serializations\n+                for_file_renames.push_back(\n+                {\n+                     .type = MutationCommand::Type::RENAME_COLUMN,\n+                     .column_name = rename_from,\n+                     .rename_to = rename_to\n+                });\n+\n+                part_columns.rename(rename_from, rename_to);\n             }\n         }\n+\n         /// If it's compact part, then we don't need to actually remove files\n         /// from disk we just don't read dropped columns\n-        for (const auto & column : part->getColumns())\n+        for (const auto & column : part_columns)\n         {\n             if (!mutated_columns.contains(column.name))\n+            {\n                 for_interpreter.emplace_back(\n                     MutationCommand{.type = MutationCommand::Type::READ_COLUMN, .column_name = column.name, .data_type = column.type});\n+            }\n+            else if (dropped_columns.contains(column.name))\n+            {\n+                /// Not needed for compact parts (not executed), added here only to produce correct\n+                /// set of columns for new part and their serializations\n+                for_file_renames.push_back(\n+                {\n+                     .type = MutationCommand::Type::DROP_COLUMN,\n+                     .column_name = column.name,\n+                });\n+            }\n         }\n     }\n     else\n@@ -149,9 +186,21 @@ static void splitMutationCommands(\n                 for_file_renames.push_back(command);\n             }\n         }\n+\n+        auto alter_conversions = part->storage.getAlterConversionsForPart(part);\n+        /// We don't add renames from commands, instead we take them from rename_map.\n+        /// It's important because required renames depend not only on part's data version (i.e. mutation version)\n+        /// but also on part's metadata version. Why we have such logic only for renames? Because all other types of alter\n+        /// can be deduced based on difference between part's schema and table schema.\n+\n+        for (const auto & [rename_to, rename_from] : alter_conversions.rename_map)\n+        {\n+            for_file_renames.push_back({.type = MutationCommand::Type::RENAME_COLUMN, .column_name = rename_from, .rename_to = rename_to});\n+        }\n     }\n }\n \n+\n /// Get the columns list of the resulting part in the same order as storage_columns.\n static std::pair<NamesAndTypesList, SerializationInfoByName>\n getColumnsForNewDataPart(\n@@ -159,8 +208,13 @@ getColumnsForNewDataPart(\n     const Block & updated_header,\n     NamesAndTypesList storage_columns,\n     const SerializationInfoByName & serialization_infos,\n+    const MutationCommands & commands_for_interpreter,\n     const MutationCommands & commands_for_removes)\n {\n+    MutationCommands all_commands;\n+    all_commands.insert(all_commands.end(), commands_for_interpreter.begin(), commands_for_interpreter.end());\n+    all_commands.insert(all_commands.end(), commands_for_removes.begin(), commands_for_removes.end());\n+\n     NameSet removed_columns;\n     NameToNameMap renamed_columns_to_from;\n     NameToNameMap renamed_columns_from_to;\n@@ -176,8 +230,7 @@ getColumnsForNewDataPart(\n             storage_columns.emplace_back(column);\n     }\n \n-    /// All commands are validated in AlterCommand so we don't care about order\n-    for (const auto & command : commands_for_removes)\n+    for (const auto & command : all_commands)\n     {\n         if (command.type == MutationCommand::UPDATE)\n         {\n@@ -192,10 +245,14 @@ getColumnsForNewDataPart(\n \n         /// If we don't have this column in source part, than we don't need to materialize it\n         if (!part_columns.has(command.column_name))\n+        {\n             continue;\n+        }\n \n         if (command.type == MutationCommand::DROP_COLUMN)\n+        {\n             removed_columns.insert(command.column_name);\n+        }\n \n         if (command.type == MutationCommand::RENAME_COLUMN)\n         {\n@@ -294,20 +351,38 @@ getColumnsForNewDataPart(\n                 /// should it's previous version should be dropped or removed\n                 if (renamed_columns_to_from.contains(it->name) && !was_renamed && !was_removed)\n                     throw Exception(\n-                                    ErrorCodes::LOGICAL_ERROR,\n-                                    \"Incorrect mutation commands, trying to rename column {} to {}, \"\n-                                    \"but part {} already has column {}\",\n-                                    renamed_columns_to_from[it->name], it->name, source_part->name, it->name);\n+                        ErrorCodes::LOGICAL_ERROR,\n+                        \"Incorrect mutation commands, trying to rename column {} to {}, \"\n+                        \"but part {} already has column {}\",\n+                        renamed_columns_to_from[it->name], it->name, source_part->name, it->name);\n \n                 /// Column was renamed and no other column renamed to it's name\n                 /// or column is dropped.\n                 if (!renamed_columns_to_from.contains(it->name) && (was_renamed || was_removed))\n+                {\n                     it = storage_columns.erase(it);\n+                }\n                 else\n                 {\n-                    /// Take a type from source part column.\n-                    /// It may differ from column type in storage.\n-                    it->type = source_col->second;\n+\n+                    if (was_removed)\n+                    { /// DROP COLUMN xxx, RENAME COLUMN yyy TO xxx\n+                        auto renamed_from = renamed_columns_to_from.at(it->name);\n+                        auto maybe_name_and_type = source_columns.tryGetByName(renamed_from);\n+                        if (!maybe_name_and_type)\n+                            throw Exception(\n+                                ErrorCodes::LOGICAL_ERROR,\n+                                \"Got incorrect mutation commands, column {} was renamed from {}, but it doesn't exist in source columns {}\",\n+                                it->name, renamed_from, source_columns.toString());\n+\n+                        it->type = maybe_name_and_type->type;\n+                    }\n+                    else\n+                    {\n+                        /// Take a type from source part column.\n+                        /// It may differ from column type in storage.\n+                        it->type = source_col->second;\n+                    }\n                     ++it;\n                 }\n             }\n@@ -573,6 +648,13 @@ static NameToNameVector collectFilesForRenames(\n     /// Collect counts for shared streams of different columns. As an example, Nested columns have shared stream with array sizes.\n     auto stream_counts = getStreamCounts(source_part, source_part->getColumns().getNames());\n     NameToNameVector rename_vector;\n+    NameSet collected_names;\n+\n+    auto add_rename = [&rename_vector, &collected_names] (const std::string & file_rename_from, const std::string & file_rename_to)\n+    {\n+        if (collected_names.emplace(file_rename_from).second)\n+            rename_vector.emplace_back(file_rename_from, file_rename_to);\n+    };\n \n     /// Remove old data\n     for (const auto & command : commands_for_removes)\n@@ -581,19 +663,19 @@ static NameToNameVector collectFilesForRenames(\n         {\n             if (source_part->checksums.has(INDEX_FILE_PREFIX + command.column_name + \".idx2\"))\n             {\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + \".idx2\", \"\");\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + \".idx2\", \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n             }\n             else if (source_part->checksums.has(INDEX_FILE_PREFIX + command.column_name + \".idx\"))\n             {\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + \".idx\", \"\");\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + \".idx\", \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n             }\n         }\n         else if (command.type == MutationCommand::Type::DROP_PROJECTION)\n         {\n             if (source_part->checksums.has(command.column_name + \".proj\"))\n-                rename_vector.emplace_back(command.column_name + \".proj\", \"\");\n+                add_rename(command.column_name + \".proj\", \"\");\n         }\n         else if (command.type == MutationCommand::Type::DROP_COLUMN)\n         {\n@@ -603,8 +685,8 @@ static NameToNameVector collectFilesForRenames(\n                 /// Delete files if they are no longer shared with another column.\n                 if (--stream_counts[stream_name] == 0)\n                 {\n-                    rename_vector.emplace_back(stream_name + \".bin\", \"\");\n-                    rename_vector.emplace_back(stream_name + mrk_extension, \"\");\n+                    add_rename(stream_name + \".bin\", \"\");\n+                    add_rename(stream_name + mrk_extension, \"\");\n                 }\n             };\n \n@@ -623,8 +705,8 @@ static NameToNameVector collectFilesForRenames(\n \n                 if (stream_from != stream_to)\n                 {\n-                    rename_vector.emplace_back(stream_from + \".bin\", stream_to + \".bin\");\n-                    rename_vector.emplace_back(stream_from + mrk_extension, stream_to + mrk_extension);\n+                    add_rename(stream_from + \".bin\", stream_to + \".bin\");\n+                    add_rename(stream_from + mrk_extension, stream_to + mrk_extension);\n                 }\n             };\n \n@@ -644,8 +726,8 @@ static NameToNameVector collectFilesForRenames(\n             {\n                 if (!new_streams.contains(old_stream) && --stream_counts[old_stream] == 0)\n                 {\n-                    rename_vector.emplace_back(old_stream + \".bin\", \"\");\n-                    rename_vector.emplace_back(old_stream + mrk_extension, \"\");\n+                    add_rename(old_stream + \".bin\", \"\");\n+                    add_rename(old_stream + mrk_extension, \"\");\n                 }\n             }\n         }\n@@ -668,6 +750,7 @@ void finalizeMutatedPart(\n     ExecuteTTLType execute_ttl_type,\n     const CompressionCodecPtr & codec,\n     ContextPtr context,\n+    StorageMetadataPtr metadata_snapshot,\n     bool sync)\n {\n     std::vector<std::unique_ptr<WriteBufferFromFileBase>> written_files;\n@@ -716,6 +799,12 @@ void finalizeMutatedPart(\n         written_files.push_back(std::move(out_comp));\n     }\n \n+    {\n+        auto out_metadata = new_data_part->getDataPartStorage().writeFile(IMergeTreeDataPart::METADATA_VERSION_FILE_NAME, 4096, context->getWriteSettings());\n+        DB::writeText(metadata_snapshot->getMetadataVersion(), *out_metadata);\n+        written_files.push_back(std::move(out_metadata));\n+    }\n+\n     {\n         /// Write a file with a description of columns.\n         auto out_columns = new_data_part->getDataPartStorage().writeFile(\"columns.txt\", 4096, context->getWriteSettings());\n@@ -1353,13 +1442,27 @@ class MutateSomePartColumnsTask : public IExecutableTask\n         ctx->new_data_part->storeVersionMetadata();\n \n         NameSet hardlinked_files;\n+\n+        /// NOTE: Renames must be done in order\n+        for (const auto & [rename_from, rename_to] : ctx->files_to_rename)\n+        {\n+            if (rename_to.empty()) /// It's DROP COLUMN\n+            {\n+                /// pass\n+            }\n+            else\n+            {\n+                ctx->new_data_part->getDataPartStorage().createHardLinkFrom(\n+                    ctx->source_part->getDataPartStorage(), rename_from, rename_to);\n+                hardlinked_files.insert(rename_from);\n+            }\n+        }\n         /// Create hardlinks for unchanged files\n         for (auto it = ctx->source_part->getDataPartStorage().iterate(); it->isValid(); it->next())\n         {\n             if (ctx->files_to_skip.contains(it->name()))\n                 continue;\n \n-            String destination;\n             String file_name = it->name();\n \n             auto rename_it = std::find_if(ctx->files_to_rename.begin(), ctx->files_to_rename.end(), [&file_name](const auto & rename_pair)\n@@ -1369,20 +1472,17 @@ class MutateSomePartColumnsTask : public IExecutableTask\n \n             if (rename_it != ctx->files_to_rename.end())\n             {\n-                if (rename_it->second.empty())\n-                    continue;\n-                destination = rename_it->second;\n-            }\n-            else\n-            {\n-                destination = it->name();\n+                /// RENAMEs and DROPs already processed\n+                continue;\n             }\n \n+            String destination = it->name();\n+\n             if (it->isFile())\n             {\n                 ctx->new_data_part->getDataPartStorage().createHardLinkFrom(\n-                    ctx->source_part->getDataPartStorage(), it->name(), destination);\n-                hardlinked_files.insert(it->name());\n+                    ctx->source_part->getDataPartStorage(), file_name, destination);\n+                hardlinked_files.insert(file_name);\n             }\n             else if (!endsWith(it->name(), \".tmp_proj\")) // ignore projection tmp merge dir\n             {\n@@ -1478,7 +1578,7 @@ class MutateSomePartColumnsTask : public IExecutableTask\n             }\n         }\n \n-        MutationHelpers::finalizeMutatedPart(ctx->source_part, ctx->new_data_part, ctx->execute_ttl_type, ctx->compression_codec, ctx->context, ctx->need_sync);\n+        MutationHelpers::finalizeMutatedPart(ctx->source_part, ctx->new_data_part, ctx->execute_ttl_type, ctx->compression_codec, ctx->context, ctx->metadata_snapshot, ctx->need_sync);\n     }\n \n \n@@ -1676,7 +1776,7 @@ bool MutateTask::prepare()\n     context_for_reading->setSetting(\"allow_asynchronous_read_from_io_pool_for_merge_tree\", false);\n     context_for_reading->setSetting(\"max_streams_for_merge_tree_reading\", Field(0));\n \n-    MutationHelpers::splitMutationCommands(ctx->source_part, ctx->commands_for_part, ctx->for_interpreter, ctx->for_file_renames);\n+    MutationHelpers::splitAndModifyMutationCommands(ctx->source_part, ctx->commands_for_part, ctx->for_interpreter, ctx->for_file_renames);\n \n     ctx->stage_progress = std::make_unique<MergeStageProgress>(1.0);\n \n@@ -1721,9 +1821,9 @@ bool MutateTask::prepare()\n \n     auto [new_columns, new_infos] = MutationHelpers::getColumnsForNewDataPart(\n         ctx->source_part, ctx->updated_header, ctx->storage_columns,\n-        ctx->source_part->getSerializationInfos(), ctx->commands_for_part);\n+        ctx->source_part->getSerializationInfos(), ctx->for_interpreter, ctx->for_file_renames);\n \n-    ctx->new_data_part->setColumns(new_columns, new_infos);\n+    ctx->new_data_part->setColumns(new_columns, new_infos, ctx->metadata_snapshot->getMetadataVersion());\n     ctx->new_data_part->partition.assign(ctx->source_part->partition);\n \n     /// Don't change granularity type while mutating subset of columns\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\nindex c859c9948186..8c91265e9963 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\n@@ -149,7 +149,7 @@ void ReplicatedMergeTreeAttachThread::runImpl()\n     const bool replica_metadata_version_exists = zookeeper->tryGet(replica_path + \"/metadata_version\", replica_metadata_version);\n     if (replica_metadata_version_exists)\n     {\n-        storage.metadata_version = parse<int>(replica_metadata_version);\n+        storage.setInMemoryMetadata(metadata_snapshot->withMetadataVersion(parse<int>(replica_metadata_version)));\n     }\n     else\n     {\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\nindex 623210ae04c5..dd4edb169fc4 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n@@ -11,6 +11,7 @@\n #include <Parsers/formatAST.h>\n #include <base/sort.h>\n \n+#include <ranges>\n \n namespace DB\n {\n@@ -1758,19 +1759,40 @@ ReplicatedMergeTreeMergePredicate ReplicatedMergeTreeQueue::getMergePredicate(zk\n }\n \n \n-MutationCommands ReplicatedMergeTreeQueue::getFirstAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const\n+std::map<int64_t, MutationCommands> ReplicatedMergeTreeQueue::getAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const\n {\n-    std::lock_guard lock(state_mutex);\n+    std::unique_lock lock(state_mutex);\n     auto in_partition = mutations_by_partition.find(part->info.partition_id);\n     if (in_partition == mutations_by_partition.end())\n-        return MutationCommands{};\n+        return {};\n \n-    Int64 part_version = part->info.getDataVersion();\n-    for (auto [mutation_version, mutation_status] : in_partition->second)\n-        if (mutation_version > part_version && mutation_status->entry->alter_version != -1)\n-            return mutation_status->entry->commands;\n+    Int64 part_metadata_version = part->getMetadataVersion();\n+    std::map<int64_t, MutationCommands> result;\n+    /// Here we return mutation commands for part which has bigger alter version than part metadata version.\n+    /// Please note, we don't use getDataVersion(). It's because these alter commands are used for in-fly conversions\n+    /// of part's metadata.\n+    for (const auto & [mutation_version, mutation_status] : in_partition->second | std::views::reverse)\n+    {\n+        int32_t alter_version = mutation_status->entry->alter_version;\n+        if (alter_version != -1)\n+        {\n+            if (alter_version > storage.getInMemoryMetadataPtr()->getMetadataVersion())\n+                continue;\n+\n+            /// we take commands with bigger metadata version\n+            if (alter_version > part_metadata_version)\n+            {\n+                result[mutation_version] = mutation_status->entry->commands;\n+            }\n+            else\n+            {\n+                /// entries are ordered, we processing them in reverse order so we can break\n+                break;\n+            }\n+        }\n+    }\n \n-    return MutationCommands{};\n+    return result;\n }\n \n MutationCommands ReplicatedMergeTreeQueue::getMutationCommands(\n@@ -1812,7 +1834,10 @@ MutationCommands ReplicatedMergeTreeQueue::getMutationCommands(\n \n     MutationCommands commands;\n     for (auto it = begin; it != end; ++it)\n-        commands.insert(commands.end(), it->second->entry->commands.begin(), it->second->entry->commands.end());\n+    {\n+        const auto & commands_from_entry = it->second->entry->commands;\n+        commands.insert(commands.end(), commands_from_entry.begin(), commands_from_entry.end());\n+    }\n \n     return commands;\n }\n@@ -2383,12 +2408,26 @@ std::optional<std::pair<Int64, int>> ReplicatedMergeTreeMergePredicate::getDesir\n         return {};\n \n     Int64 current_version = queue.getCurrentMutationVersionImpl(part->info.partition_id, part->info.getDataVersion(), lock);\n-    Int64 max_version = in_partition->second.rbegin()->first;\n+    Int64 max_version = in_partition->second.begin()->first;\n \n     int alter_version = -1;\n+    bool barrier_found = false;\n     for (auto [mutation_version, mutation_status] : in_partition->second)\n     {\n+        /// Some commands cannot stick together with other commands\n+        if (mutation_status->entry->commands.containBarrierCommand())\n+        {\n+            /// We already collected some mutation, we don't want to stick it with barrier\n+            if (max_version != mutation_version && max_version > current_version)\n+                break;\n+\n+            /// This mutations is fresh, but it's barrier, let's execute only it\n+            if (mutation_version > current_version)\n+                barrier_found = true;\n+        }\n+\n         max_version = mutation_version;\n+\n         if (mutation_status->entry->isAlterMutation())\n         {\n             /// We want to assign mutations for part which version is bigger\n@@ -2401,6 +2440,9 @@ std::optional<std::pair<Int64, int>> ReplicatedMergeTreeMergePredicate::getDesir\n                 break;\n             }\n         }\n+\n+        if (barrier_found == true)\n+            break;\n     }\n \n     if (current_version >= max_version)\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\nindex 841c26bb3618..2d71a1463e8e 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n@@ -394,10 +394,10 @@ class ReplicatedMergeTreeQueue\n \n     MutationCommands getMutationCommands(const MergeTreeData::DataPartPtr & part, Int64 desired_mutation_version) const;\n \n-    /// Return mutation commands for part with smallest mutation version bigger\n-    /// than data part version. Used when we apply alter commands on fly,\n+    /// Return mutation commands for part which could be not applied to\n+    /// it according to part mutation version. Used when we apply alter commands on fly,\n     /// without actual data modification on disk.\n-    MutationCommands getFirstAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const;\n+    std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const;\n \n     /// Mark finished mutations as done. If the function needs to be called again at some later time\n     /// (because some mutations are probably done but we are not sure yet), returns true.\ndiff --git a/src/Storages/MutationCommands.cpp b/src/Storages/MutationCommands.cpp\nindex 0c9e9223929f..aa77988348dd 100644\n--- a/src/Storages/MutationCommands.cpp\n+++ b/src/Storages/MutationCommands.cpp\n@@ -23,6 +23,12 @@ namespace ErrorCodes\n     extern const int MULTIPLE_ASSIGNMENTS_TO_COLUMN;\n }\n \n+\n+bool MutationCommand::isBarrierCommand() const\n+{\n+    return type == RENAME_COLUMN;\n+}\n+\n std::optional<MutationCommand> MutationCommand::parse(ASTAlterCommand * command, bool parse_alter_commands)\n {\n     if (command->type == ASTAlterCommand::DELETE)\n@@ -212,4 +218,14 @@ bool MutationCommands::hasNonEmptyMutationCommands() const\n     return false;\n }\n \n+bool MutationCommands::containBarrierCommand() const\n+{\n+    for (const auto & command : *this)\n+    {\n+        if (command.isBarrierCommand())\n+            return true;\n+    }\n+    return false;\n+}\n+\n }\ndiff --git a/src/Storages/MutationCommands.h b/src/Storages/MutationCommands.h\nindex aca91c16e85d..5ef0cfda1be6 100644\n--- a/src/Storages/MutationCommands.h\n+++ b/src/Storages/MutationCommands.h\n@@ -67,6 +67,9 @@ struct MutationCommand\n \n     /// If parse_alter_commands, than consider more Alter commands as mutation commands\n     static std::optional<MutationCommand> parse(ASTAlterCommand * command, bool parse_alter_commands = false);\n+\n+    /// This command shouldn't stick with other commands\n+    bool isBarrierCommand() const;\n };\n \n /// Multiple mutation commands, possible from different ALTER queries\n@@ -79,6 +82,11 @@ class MutationCommands : public std::vector<MutationCommand>\n     void readText(ReadBuffer & in);\n     std::string toString() const;\n     bool hasNonEmptyMutationCommands() const;\n+\n+    /// These set of commands contain barrier command and shouldn't\n+    /// stick with other commands. Commands from one set have already been validated\n+    /// to be executed without issues on the creation state.\n+    bool containBarrierCommand() const;\n };\n \n using MutationCommandsConstPtr = std::shared_ptr<MutationCommands>;\ndiff --git a/src/Storages/StorageInMemoryMetadata.cpp b/src/Storages/StorageInMemoryMetadata.cpp\nindex f6550c6cd5da..45abd4bebef7 100644\n--- a/src/Storages/StorageInMemoryMetadata.cpp\n+++ b/src/Storages/StorageInMemoryMetadata.cpp\n@@ -41,6 +41,7 @@ StorageInMemoryMetadata::StorageInMemoryMetadata(const StorageInMemoryMetadata &\n     , settings_changes(other.settings_changes ? other.settings_changes->clone() : nullptr)\n     , select(other.select)\n     , comment(other.comment)\n+    , metadata_version(other.metadata_version)\n {\n }\n \n@@ -69,6 +70,7 @@ StorageInMemoryMetadata & StorageInMemoryMetadata::operator=(const StorageInMemo\n         settings_changes.reset();\n     select = other.select;\n     comment = other.comment;\n+    metadata_version = other.metadata_version;\n     return *this;\n }\n \n@@ -122,6 +124,18 @@ void StorageInMemoryMetadata::setSelectQuery(const SelectQueryDescription & sele\n     select = select_;\n }\n \n+void StorageInMemoryMetadata::setMetadataVersion(int32_t metadata_version_)\n+{\n+    metadata_version = metadata_version_;\n+}\n+\n+StorageInMemoryMetadata StorageInMemoryMetadata::withMetadataVersion(int32_t metadata_version_) const\n+{\n+    StorageInMemoryMetadata copy(*this);\n+    copy.setMetadataVersion(metadata_version_);\n+    return copy;\n+}\n+\n const ColumnsDescription & StorageInMemoryMetadata::getColumns() const\n {\n     return columns;\ndiff --git a/src/Storages/StorageInMemoryMetadata.h b/src/Storages/StorageInMemoryMetadata.h\nindex eadce5813347..25618c5b03fd 100644\n--- a/src/Storages/StorageInMemoryMetadata.h\n+++ b/src/Storages/StorageInMemoryMetadata.h\n@@ -50,6 +50,10 @@ struct StorageInMemoryMetadata\n \n     String comment;\n \n+    /// Version of metadata. Managed properly by ReplicatedMergeTree only\n+    /// (zero-initialization is important)\n+    int32_t metadata_version = 0;\n+\n     StorageInMemoryMetadata() = default;\n \n     StorageInMemoryMetadata(const StorageInMemoryMetadata & other);\n@@ -58,7 +62,7 @@ struct StorageInMemoryMetadata\n     StorageInMemoryMetadata(StorageInMemoryMetadata && other) = default;\n     StorageInMemoryMetadata & operator=(StorageInMemoryMetadata && other) = default;\n \n-    /// NOTE: Thread unsafe part. You should modify same StorageInMemoryMetadata\n+    /// NOTE: Thread unsafe part. You should not modify same StorageInMemoryMetadata\n     /// structure from different threads. It should be used as MultiVersion\n     /// object. See example in IStorage.\n \n@@ -90,6 +94,11 @@ struct StorageInMemoryMetadata\n     /// Set SELECT query for (Materialized)View\n     void setSelectQuery(const SelectQueryDescription & select_);\n \n+    /// Set version of metadata.\n+    void setMetadataVersion(int32_t metadata_version_);\n+    /// Get copy of current metadata with metadata_version_\n+    StorageInMemoryMetadata withMetadataVersion(int32_t metadata_version_) const;\n+\n     /// Returns combined set of columns\n     const ColumnsDescription & getColumns() const;\n \n@@ -218,6 +227,9 @@ struct StorageInMemoryMetadata\n     const SelectQueryDescription & getSelectQuery() const;\n     bool hasSelectQuery() const;\n \n+    /// Get version of metadata\n+    int32_t getMetadataVersion() const { return metadata_version; }\n+\n     /// Check that all the requested names are in the table and have the correct types.\n     void check(const NamesAndTypesList & columns) const;\n \ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 71a826fbc229..34bf5d55270e 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -326,6 +326,24 @@ void StorageMergeTree::alter(\n     }\n     else\n     {\n+        if (!maybe_mutation_commands.empty() && maybe_mutation_commands.containBarrierCommand())\n+        {\n+            int64_t prev_mutation = 0;\n+            {\n+                std::lock_guard lock(currently_processing_in_background_mutex);\n+                auto it = current_mutations_by_version.rbegin();\n+                if (it != current_mutations_by_version.rend())\n+                    prev_mutation = it->first;\n+            }\n+\n+            if (prev_mutation != 0)\n+            {\n+                LOG_DEBUG(log, \"Cannot change metadata with barrier alter query, will wait for mutation {}\", prev_mutation);\n+                waitForMutation(prev_mutation);\n+                LOG_DEBUG(log, \"Mutation {} finished\", prev_mutation);\n+            }\n+        }\n+\n         {\n             changeSettings(new_metadata.settings_changes, table_lock_holder);\n             checkTTLExpressions(new_metadata, old_metadata);\n@@ -1150,9 +1168,24 @@ MergeMutateSelectedEntryPtr StorageMergeTree::selectPartsToMutate(\n             if (current_ast_elements + commands_size >= max_ast_elements)\n                 break;\n \n-            current_ast_elements += commands_size;\n-            commands->insert(commands->end(), it->second.commands.begin(), it->second.commands.end());\n-            last_mutation_to_apply = it;\n+            const auto & single_mutation_commands = it->second.commands;\n+\n+            if (single_mutation_commands.containBarrierCommand())\n+            {\n+                if (commands->empty())\n+                {\n+                    commands->insert(commands->end(), single_mutation_commands.begin(), single_mutation_commands.end());\n+                    last_mutation_to_apply = it;\n+                }\n+                break;\n+            }\n+            else\n+            {\n+                current_ast_elements += commands_size;\n+                commands->insert(commands->end(), single_mutation_commands.begin(), single_mutation_commands.end());\n+                last_mutation_to_apply = it;\n+            }\n+\n         }\n \n         assert(commands->empty() == (last_mutation_to_apply == mutations_end_it));\n@@ -1247,7 +1280,10 @@ bool StorageMergeTree::scheduleDataProcessingJob(BackgroundJobsAssignee & assign\n     }\n     if (mutate_entry)\n     {\n-        auto task = std::make_shared<MutatePlainMergeTreeTask>(*this, metadata_snapshot, mutate_entry, shared_lock, common_assignee_trigger);\n+        /// We take new metadata snapshot here. It's because mutation commands can be executed only with metadata snapshot\n+        /// which is equal or more fresh than commands themselves. In extremely rare case it can happen that we will have alter\n+        /// in between we took snapshot above and selected commands. That is why we take new snapshot here.\n+        auto task = std::make_shared<MutatePlainMergeTreeTask>(*this, getInMemoryMetadataPtr(), mutate_entry, shared_lock, common_assignee_trigger);\n         assignee.scheduleMergeMutateTask(task);\n         return true;\n     }\n@@ -2116,14 +2152,22 @@ void StorageMergeTree::attachRestoredParts(MutableDataPartsVector && parts)\n }\n \n \n-MutationCommands StorageMergeTree::getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const\n+std::map<int64_t, MutationCommands> StorageMergeTree::getAlterMutationCommandsForPart(const DataPartPtr & part) const\n {\n     std::lock_guard lock(currently_processing_in_background_mutex);\n \n-    auto it = current_mutations_by_version.upper_bound(part->info.getDataVersion());\n-    if (it == current_mutations_by_version.end())\n-        return {};\n-    return it->second.commands;\n+    Int64 part_data_version = part->info.getDataVersion();\n+\n+    std::map<int64_t, MutationCommands> result;\n+    if (!current_mutations_by_version.empty())\n+    {\n+        const auto & [latest_mutation_id, latest_commands] = *current_mutations_by_version.rbegin();\n+        if (part_data_version < static_cast<int64_t>(latest_mutation_id))\n+        {\n+            result[latest_mutation_id] = latest_commands.commands;\n+        }\n+    }\n+    return result;\n }\n \n void StorageMergeTree::startBackgroundMovesIfNeeded()\ndiff --git a/src/Storages/StorageMergeTree.h b/src/Storages/StorageMergeTree.h\nindex 052a672a1873..a0629bb8d3ef 100644\n--- a/src/Storages/StorageMergeTree.h\n+++ b/src/Storages/StorageMergeTree.h\n@@ -267,7 +267,7 @@ class StorageMergeTree final : public MergeTreeData\n \n protected:\n \n-    MutationCommands getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n+    std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n };\n \n }\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 5e99426ba7b2..5cd02c33d554 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -462,7 +462,7 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n \n             Coordination::Stat metadata_stat;\n             current_zookeeper->get(zookeeper_path + \"/metadata\", &metadata_stat);\n-            metadata_version = metadata_stat.version;\n+            setInMemoryMetadata(metadata_snapshot->withMetadataVersion(metadata_stat.version));\n         }\n         catch (Coordination::Exception & e)\n         {\n@@ -784,7 +784,7 @@ bool StorageReplicatedMergeTree::createTableIfNotExists(const StorageMetadataPtr\n             zkutil::CreateMode::Persistent));\n         ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/columns\", metadata_snapshot->getColumns().toString(),\n             zkutil::CreateMode::Persistent));\n-        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", std::to_string(metadata_version),\n+        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", toString(metadata_snapshot->getMetadataVersion()),\n             zkutil::CreateMode::Persistent));\n \n         /// The following 3 nodes were added in version 1.1.xxx, so we create them here, not in createNewZooKeeperNodes()\n@@ -857,7 +857,7 @@ void StorageReplicatedMergeTree::createReplica(const StorageMetadataPtr & metada\n             zkutil::CreateMode::Persistent));\n         ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/columns\", metadata_snapshot->getColumns().toString(),\n             zkutil::CreateMode::Persistent));\n-        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", std::to_string(metadata_version),\n+        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", toString(metadata_snapshot->getMetadataVersion()),\n             zkutil::CreateMode::Persistent));\n \n         /// The following 3 nodes were added in version 1.1.xxx, so we create them here, not in createNewZooKeeperNodes()\n@@ -1162,16 +1162,19 @@ void StorageReplicatedMergeTree::checkTableStructure(const String & zookeeper_pr\n }\n \n void StorageReplicatedMergeTree::setTableStructure(const StorageID & table_id, const ContextPtr & local_context,\n-    ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff)\n+    ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff, int32_t new_metadata_version)\n {\n     StorageInMemoryMetadata old_metadata = getInMemoryMetadata();\n+\n     StorageInMemoryMetadata new_metadata = metadata_diff.getNewMetadata(new_columns, local_context, old_metadata);\n+    new_metadata.setMetadataVersion(new_metadata_version);\n \n     /// Even if the primary/sorting/partition keys didn't change we must reinitialize it\n     /// because primary/partition key column types might have changed.\n     checkTTLExpressions(new_metadata, old_metadata);\n     setProperties(new_metadata, old_metadata);\n \n+\n     DatabaseCatalog::instance().getDatabase(table_id.database_name)->alterTable(local_context, table_id, new_metadata);\n }\n \n@@ -2793,8 +2796,9 @@ void StorageReplicatedMergeTree::cloneMetadataIfNeeded(const String & source_rep\n         return;\n     }\n \n+    auto metadata_snapshot = getInMemoryMetadataPtr();\n     Int32 source_metadata_version = parse<Int32>(source_metadata_version_str);\n-    if (metadata_version == source_metadata_version)\n+    if (metadata_snapshot->getMetadataVersion() == source_metadata_version)\n         return;\n \n     /// Our metadata it not up to date with source replica metadata.\n@@ -2812,7 +2816,7 @@ void StorageReplicatedMergeTree::cloneMetadataIfNeeded(const String & source_rep\n     /// if all such entries were cleaned up from the log and source_queue.\n \n     LOG_WARNING(log, \"Metadata version ({}) on replica is not up to date with metadata ({}) on source replica {}\",\n-                metadata_version, source_metadata_version, source_replica);\n+                metadata_snapshot->getMetadataVersion(), source_metadata_version, source_replica);\n \n     String source_metadata;\n     String source_columns;\n@@ -4987,14 +4991,15 @@ bool StorageReplicatedMergeTree::optimize(\n \n bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMergeTree::LogEntry & entry)\n {\n-    if (entry.alter_version < metadata_version)\n+    auto current_metadata = getInMemoryMetadataPtr();\n+    if (entry.alter_version < current_metadata->getMetadataVersion())\n     {\n         /// TODO Can we replace it with LOGICAL_ERROR?\n         /// As for now, it may rarely happen due to reordering of ALTER_METADATA entries in the queue of\n         /// non-initial replica and also may happen after stale replica recovery.\n         LOG_WARNING(log, \"Attempt to update metadata of version {} \"\n                          \"to older version {} when processing log entry {}: {}\",\n-                         metadata_version, entry.alter_version, entry.znode_name, entry.toString());\n+                         current_metadata->getMetadataVersion(), entry.alter_version, entry.znode_name, entry.toString());\n         return true;\n     }\n \n@@ -5042,10 +5047,10 @@ bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMer\n         LOG_INFO(log, \"Metadata changed in ZooKeeper. Applying changes locally.\");\n \n         auto metadata_diff = ReplicatedMergeTreeTableMetadata(*this, getInMemoryMetadataPtr()).checkAndFindDiff(metadata_from_entry, getInMemoryMetadataPtr()->getColumns(), getContext());\n-        setTableStructure(table_id, alter_context, std::move(columns_from_entry), metadata_diff);\n-        metadata_version = entry.alter_version;\n+        setTableStructure(table_id, alter_context, std::move(columns_from_entry), metadata_diff, entry.alter_version);\n \n-        LOG_INFO(log, \"Applied changes to the metadata of the table. Current metadata version: {}\", metadata_version);\n+        current_metadata = getInMemoryMetadataPtr();\n+        LOG_INFO(log, \"Applied changes to the metadata of the table. Current metadata version: {}\", current_metadata->getMetadataVersion());\n     }\n \n     {\n@@ -5057,7 +5062,7 @@ bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMer\n \n     /// This transaction may not happen, but it's OK, because on the next retry we will eventually create/update this node\n     /// TODO Maybe do in in one transaction for Replicated database?\n-    zookeeper->createOrUpdate(fs::path(replica_path) / \"metadata_version\", std::to_string(metadata_version), zkutil::CreateMode::Persistent);\n+    zookeeper->createOrUpdate(fs::path(replica_path) / \"metadata_version\", std::to_string(current_metadata->getMetadataVersion()), zkutil::CreateMode::Persistent);\n \n     return true;\n }\n@@ -5181,7 +5186,7 @@ void StorageReplicatedMergeTree::alter(\n         size_t mutation_path_idx = std::numeric_limits<size_t>::max();\n \n         String new_metadata_str = future_metadata_in_zk.toString();\n-        ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"metadata\", new_metadata_str, metadata_version));\n+        ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"metadata\", new_metadata_str, current_metadata->getMetadataVersion()));\n \n         String new_columns_str = future_metadata.columns.toString();\n         ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"columns\", new_columns_str, -1));\n@@ -5197,7 +5202,7 @@ void StorageReplicatedMergeTree::alter(\n \n         /// We can be sure, that in case of successful commit in zookeeper our\n         /// version will increments by 1. Because we update with version check.\n-        int new_metadata_version = metadata_version + 1;\n+        int new_metadata_version = current_metadata->getMetadataVersion() + 1;\n \n         alter_entry->type = LogEntry::ALTER_METADATA;\n         alter_entry->source_replica = replica_name;\n@@ -7989,9 +7994,9 @@ bool StorageReplicatedMergeTree::canUseAdaptiveGranularity() const\n }\n \n \n-MutationCommands StorageReplicatedMergeTree::getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const\n+std::map<int64_t, MutationCommands> StorageReplicatedMergeTree::getAlterMutationCommandsForPart(const DataPartPtr & part) const\n {\n-    return queue.getFirstAlterMutationCommandsForPart(part);\n+    return queue.getAlterMutationCommandsForPart(part);\n }\n \n \ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex d410350af310..e99af135133e 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -216,8 +216,6 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     /// It's used if not set in engine's arguments while creating a replicated table.\n     static String getDefaultReplicaName(const ContextPtr & context_);\n \n-    int getMetadataVersion() const { return metadata_version; }\n-\n     /// Modify a CREATE TABLE query to make a variant which must be written to a backup.\n     void adjustCreateQueryForBackup(ASTPtr & create_query) const override;\n \n@@ -430,7 +428,6 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     std::atomic<bool> shutdown_called {false};\n     std::atomic<bool> flush_called {false};\n \n-    int metadata_version = 0;\n     /// Threads.\n \n     /// A task that keeps track of the updates in the logs of all replicas and loads them into the queue.\n@@ -517,8 +514,10 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n \n     /// A part of ALTER: apply metadata changes only (data parts are altered separately).\n     /// Must be called under IStorage::lockForAlter() lock.\n-    void setTableStructure(const StorageID & table_id, const ContextPtr & local_context,\n-                           ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff);\n+    void setTableStructure(\n+        const StorageID & table_id, const ContextPtr & local_context,\n+        ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff,\n+        int32_t new_metadata_version);\n \n     /** Check that the set of parts corresponds to that in ZK (/replicas/me/parts/).\n       * If any parts described in ZK are not locally, throw an exception.\n@@ -842,7 +841,7 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     void waitMutationToFinishOnReplicas(\n         const Strings & replicas, const String & mutation_id) const;\n \n-    MutationCommands getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n+    std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n \n     void startBackgroundMovesIfNeeded() override;\n \n",
  "test_patch": "diff --git a/tests/integration/test_disk_over_web_server/test.py b/tests/integration/test_disk_over_web_server/test.py\nindex 363df4595b21..fd71389f71ab 100644\n--- a/tests/integration/test_disk_over_web_server/test.py\n+++ b/tests/integration/test_disk_over_web_server/test.py\n@@ -21,23 +21,31 @@ def cluster():\n         cluster.add_instance(\n             \"node3\", main_configs=[\"configs/storage_conf_web.xml\"], with_nginx=True\n         )\n+\n+        cluster.add_instance(\n+            \"node4\",\n+            main_configs=[\"configs/storage_conf.xml\"],\n+            with_nginx=True,\n+            stay_alive=True,\n+            with_installed_binary=True,\n+            image=\"clickhouse/clickhouse-server\",\n+            tag=\"22.8.14.53\",\n+        )\n+\n         cluster.start()\n \n-        node1 = cluster.instances[\"node1\"]\n-        expected = \"\"\n-        global uuids\n-        for i in range(3):\n-            node1.query(\n+        def create_table_and_upload_data(node, i):\n+            node.query(\n                 f\"CREATE TABLE data{i} (id Int32) ENGINE = MergeTree() ORDER BY id SETTINGS storage_policy = 'def', min_bytes_for_wide_part=1;\"\n             )\n \n             for _ in range(10):\n-                node1.query(\n+                node.query(\n                     f\"INSERT INTO data{i} SELECT number FROM numbers(500000 * {i+1})\"\n                 )\n-            expected = node1.query(f\"SELECT * FROM data{i} ORDER BY id\")\n+            node.query(f\"SELECT * FROM data{i} ORDER BY id\")\n \n-            metadata_path = node1.query(\n+            metadata_path = node.query(\n                 f\"SELECT data_paths FROM system.tables WHERE name='data{i}'\"\n             )\n             metadata_path = metadata_path[\n@@ -45,7 +53,7 @@ def cluster():\n             ]\n             print(f\"Metadata: {metadata_path}\")\n \n-            node1.exec_in_container(\n+            node.exec_in_container(\n                 [\n                     \"bash\",\n                     \"-c\",\n@@ -56,8 +64,20 @@ def cluster():\n                 user=\"root\",\n             )\n             parts = metadata_path.split(\"/\")\n-            uuids.append(parts[3])\n             print(f\"UUID: {parts[3]}\")\n+            return parts[3]\n+\n+        node1 = cluster.instances[\"node1\"]\n+\n+        global uuids\n+        for i in range(2):\n+            uuid = create_table_and_upload_data(node1, i)\n+            uuids.append(uuid)\n+\n+        node4 = cluster.instances[\"node4\"]\n+\n+        uuid = create_table_and_upload_data(node4, 2)\n+        uuids.append(uuid)\n \n         yield cluster\n \n@@ -68,6 +88,7 @@ def cluster():\n @pytest.mark.parametrize(\"node_name\", [\"node2\"])\n def test_usage(cluster, node_name):\n     node1 = cluster.instances[\"node1\"]\n+    node4 = cluster.instances[\"node4\"]\n     node2 = cluster.instances[node_name]\n     global uuids\n     assert len(uuids) == 3\n@@ -90,7 +111,11 @@ def test_usage(cluster, node_name):\n         result = node2.query(\n             \"SELECT id FROM test{} WHERE id % 56 = 3 ORDER BY id\".format(i)\n         )\n-        assert result == node1.query(\n+        node = node1\n+        if i == 2:\n+            node = node4\n+\n+        assert result == node.query(\n             \"SELECT id FROM data{} WHERE id % 56 = 3 ORDER BY id\".format(i)\n         )\n \n@@ -99,7 +124,7 @@ def test_usage(cluster, node_name):\n                 i\n             )\n         )\n-        assert result == node1.query(\n+        assert result == node.query(\n             \"SELECT id FROM data{} WHERE id > 789999 AND id < 999999 ORDER BY id\".format(\n                 i\n             )\n@@ -141,6 +166,7 @@ def test_incorrect_usage(cluster):\n @pytest.mark.parametrize(\"node_name\", [\"node2\"])\n def test_cache(cluster, node_name):\n     node1 = cluster.instances[\"node1\"]\n+    node4 = cluster.instances[\"node4\"]\n     node2 = cluster.instances[node_name]\n     global uuids\n     assert len(uuids) == 3\n@@ -178,7 +204,12 @@ def test_cache(cluster, node_name):\n         result = node2.query(\n             \"SELECT id FROM test{} WHERE id % 56 = 3 ORDER BY id\".format(i)\n         )\n-        assert result == node1.query(\n+\n+        node = node1\n+        if i == 2:\n+            node = node4\n+\n+        assert result == node.query(\n             \"SELECT id FROM data{} WHERE id % 56 = 3 ORDER BY id\".format(i)\n         )\n \n@@ -187,7 +218,7 @@ def test_cache(cluster, node_name):\n                 i\n             )\n         )\n-        assert result == node1.query(\n+        assert result == node.query(\n             \"SELECT id FROM data{} WHERE id > 789999 AND id < 999999 ORDER BY id\".format(\n                 i\n             )\ndiff --git a/tests/integration/test_merge_tree_hdfs/test.py b/tests/integration/test_merge_tree_hdfs/test.py\nindex 3950077e619b..782237539fa6 100644\n--- a/tests/integration/test_merge_tree_hdfs/test.py\n+++ b/tests/integration/test_merge_tree_hdfs/test.py\n@@ -43,8 +43,18 @@ def create_table(cluster, table_name, additional_settings=None):\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n @pytest.fixture(scope=\"module\")\ndiff --git a/tests/integration/test_merge_tree_s3/test.py b/tests/integration/test_merge_tree_s3/test.py\nindex f0f81100320a..696c016f7603 100644\n--- a/tests/integration/test_merge_tree_s3/test.py\n+++ b/tests/integration/test_merge_tree_s3/test.py\n@@ -52,8 +52,18 @@ def cluster():\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n def create_table(node, table_name, **additional_settings):\n@@ -232,7 +242,6 @@ def test_insert_same_partition_and_merge(cluster, merge_vertical, node_name):\n def test_alter_table_columns(cluster, node_name):\n     node = cluster.instances[node_name]\n     create_table(node, \"s3_test\")\n-    minio = cluster.minio_client\n \n     node.query(\n         \"INSERT INTO s3_test VALUES {}\".format(generate_values(\"2020-01-03\", 4096))\ndiff --git a/tests/integration/test_merge_tree_s3_failover/test.py b/tests/integration/test_merge_tree_s3_failover/test.py\nindex cf71b4237131..3cc2b17dce2c 100644\n--- a/tests/integration/test_merge_tree_s3_failover/test.py\n+++ b/tests/integration/test_merge_tree_s3_failover/test.py\n@@ -89,7 +89,7 @@ def drop_table(cluster):\n \n \n # S3 request will be failed for an appropriate part file write.\n-FILES_PER_PART_BASE = 5  # partition.dat, default_compression_codec.txt, count.txt, columns.txt, checksums.txt\n+FILES_PER_PART_BASE = 6  # partition.dat, metadata_version.txt, default_compression_codec.txt, count.txt, columns.txt, checksums.txt\n FILES_PER_PART_WIDE = (\n     FILES_PER_PART_BASE + 1 + 1 + 3 * 2\n )  # Primary index, MinMax, Mark and data file for column(s)\ndiff --git a/tests/integration/test_partition/test.py b/tests/integration/test_partition/test.py\nindex a34141c61890..5a972b58f999 100644\n--- a/tests/integration/test_partition/test.py\n+++ b/tests/integration/test_partition/test.py\n@@ -105,6 +105,8 @@ def partition_complex_assert_checksums():\n         \"c4ca4238a0b923820dcc509a6f75849b\\tshadow/1/data/test/partition_complex/19700102_2_2_0/count.txt\\n\"\n         \"c4ca4238a0b923820dcc509a6f75849b\\tshadow/1/data/test/partition_complex/19700201_1_1_0/count.txt\\n\"\n         \"cfcb770c3ecd0990dcceb1bde129e6c6\\tshadow/1/data/test/partition_complex/19700102_2_2_0/p.bin\\n\"\n+        \"cfcd208495d565ef66e7dff9f98764da\\tshadow/1/data/test/partition_complex/19700102_2_2_0/metadata_version.txt\\n\"\n+        \"cfcd208495d565ef66e7dff9f98764da\\tshadow/1/data/test/partition_complex/19700201_1_1_0/metadata_version.txt\\n\"\n         \"e2af3bef1fd129aea73a890ede1e7a30\\tshadow/1/data/test/partition_complex/19700201_1_1_0/k.bin\\n\"\n         \"f2312862cc01adf34a93151377be2ddf\\tshadow/1/data/test/partition_complex/19700201_1_1_0/minmax_p.idx\\n\"\n     )\ndiff --git a/tests/integration/test_replicated_merge_tree_s3/test.py b/tests/integration/test_replicated_merge_tree_s3/test.py\nindex 0d978bb6967d..b90e28dfdb20 100644\n--- a/tests/integration/test_replicated_merge_tree_s3/test.py\n+++ b/tests/integration/test_replicated_merge_tree_s3/test.py\n@@ -44,8 +44,18 @@ def cluster():\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n def random_string(length):\ndiff --git a/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py b/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py\nindex 1a5f2e127060..eca188200168 100644\n--- a/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py\n+++ b/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py\n@@ -47,8 +47,18 @@ def cluster():\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n def random_string(length):\ndiff --git a/tests/integration/test_s3_zero_copy_ttl/test.py b/tests/integration/test_s3_zero_copy_ttl/test.py\nindex 9a782aacef6b..7dcf3734653d 100644\n--- a/tests/integration/test_s3_zero_copy_ttl/test.py\n+++ b/tests/integration/test_s3_zero_copy_ttl/test.py\n@@ -86,9 +86,9 @@ def test_ttl_move_and_s3(started_cluster):\n \n         print(f\"Total objects: {counter}\")\n \n-        if counter == 300:\n+        if counter == 330:\n             break\n \n         print(f\"Attempts remaining: {attempt}\")\n \n-    assert counter == 300\n+    assert counter == 330\ndiff --git a/tests/queries/0_stateless/01278_alter_rename_combination.reference b/tests/queries/0_stateless/01278_alter_rename_combination.reference\nindex cc912e9b265b..e70c2d2e6f8f 100644\n--- a/tests/queries/0_stateless/01278_alter_rename_combination.reference\n+++ b/tests/queries/0_stateless/01278_alter_rename_combination.reference\n@@ -1,7 +1,7 @@\n-CREATE TABLE default.rename_table\\n(\\n    `key` Int32,\\n    `old_value1` Int32,\\n    `value1` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table\\n(\\n    `key` Int32,\\n    `old_value1` Int32,\\n    `value1` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\told_value1\tvalue1\n 1\t2\t3\n-CREATE TABLE default.rename_table\\n(\\n    `k` Int32,\\n    `v1` Int32,\\n    `v2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table\\n(\\n    `k` Int32,\\n    `v1` Int32,\\n    `v2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n k\tv1\tv2\n 1\t2\t3\n 4\t5\t6\ndiff --git a/tests/queries/0_stateless/01278_alter_rename_combination.sql b/tests/queries/0_stateless/01278_alter_rename_combination.sql\nindex fa73362622c8..51322f5d86f1 100644\n--- a/tests/queries/0_stateless/01278_alter_rename_combination.sql\n+++ b/tests/queries/0_stateless/01278_alter_rename_combination.sql\n@@ -1,6 +1,6 @@\n DROP TABLE IF EXISTS rename_table;\n \n-CREATE TABLE rename_table (key Int32, value1 Int32, value2 Int32) ENGINE = MergeTree ORDER BY tuple();\n+CREATE TABLE rename_table (key Int32, value1 Int32, value2 Int32) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0;\n \n INSERT INTO rename_table VALUES (1, 2, 3);\n \ndiff --git a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference\nindex bf3358aea60a..532b8ce87123 100644\n--- a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference\n+++ b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference\n@@ -1,11 +1,11 @@\n-CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\tvalue1_string\tvalue2\n 1\t2\t3\n-CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int32,\\n    `value2` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int32,\\n    `value2` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\tvalue1_string\tvalue2_old\tvalue2\n 1\t2\t3\t7\n 4\t5\t6\t7\n-CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\tvalue1_string\tvalue2_old\n 1\t2\t7\n 4\t5\t7\ndiff --git a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql\nindex f9462f0478e3..b0ccd7751ab0 100644\n--- a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql\n+++ b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql\n@@ -1,6 +1,6 @@\n DROP TABLE IF EXISTS rename_table_multiple;\n \n-CREATE TABLE rename_table_multiple (key Int32, value1 String, value2 Int32) ENGINE = MergeTree ORDER BY tuple();\n+CREATE TABLE rename_table_multiple (key Int32, value1 String, value2 Int32) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0;\n \n INSERT INTO rename_table_multiple VALUES (1, 2, 3);\n \ndiff --git a/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference b/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference\nindex bbca9bbbfee7..f3fac9b32d31 100644\n--- a/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference\n+++ b/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference\n@@ -7,25 +7,25 @@ file_segment_range_begin: 0\n file_segment_range_end:   745\n size:                     746\n state:                    DOWNLOADED\n-7\n-7\n+8\n+8\n 0\n 2\n 2\n-7\n+8\n Row 1:\n \u2500\u2500\u2500\u2500\u2500\u2500\n file_segment_range_begin: 0\n file_segment_range_end:   1659\n size:                     1660\n state:                    DOWNLOADED\n-7\n-7\n-7\n-7\n-21\n-31\n-38\n+8\n+8\n+8\n+8\n+24\n+35\n+43\n 5010500\n 18816\n Using storage policy: local_cache\n@@ -37,24 +37,24 @@ file_segment_range_begin: 0\n file_segment_range_end:   745\n size:                     746\n state:                    DOWNLOADED\n-7\n-7\n+8\n+8\n 0\n 2\n 2\n-7\n+8\n Row 1:\n \u2500\u2500\u2500\u2500\u2500\u2500\n file_segment_range_begin: 0\n file_segment_range_end:   1659\n size:                     1660\n state:                    DOWNLOADED\n-7\n-7\n-7\n-7\n-21\n-31\n-38\n+8\n+8\n+8\n+8\n+24\n+35\n+43\n 5010500\n 18816\ndiff --git a/tests/queries/0_stateless/02361_fsync_profile_events.sh b/tests/queries/0_stateless/02361_fsync_profile_events.sh\nindex 44a1bd58d36e..5b603133f6c7 100755\n--- a/tests/queries/0_stateless/02361_fsync_profile_events.sh\n+++ b/tests/queries/0_stateless/02361_fsync_profile_events.sh\n@@ -44,8 +44,8 @@ for i in {1..100}; do\n     \")\"\n \n     # Non retriable errors\n-    if [[ $FileSync -ne 7 ]]; then\n-        echo \"FileSync: $FileSync != 11\" >&2\n+    if [[ $FileSync -ne 8 ]]; then\n+        echo \"FileSync: $FileSync != 8\" >&2\n         exit 2\n     fi\n     # Check that all files was synced\ndiff --git a/tests/queries/0_stateless/02538_alter_rename_sequence.reference b/tests/queries/0_stateless/02538_alter_rename_sequence.reference\nnew file mode 100644\nindex 000000000000..73aa1b7e8d8c\n--- /dev/null\n+++ b/tests/queries/0_stateless/02538_alter_rename_sequence.reference\n@@ -0,0 +1,8 @@\n+1\t2\t3\n+4\t5\t6\n+{\"column1_renamed\":\"1\",\"column2_renamed\":\"2\",\"column3\":\"3\"}\n+{\"column1_renamed\":\"4\",\"column2_renamed\":\"5\",\"column3\":\"6\"}\n+1\t2\t3\n+4\t5\t6\n+{\"column1_renamed\":\"1\",\"column2_renamed\":\"2\",\"column3\":\"3\"}\n+{\"column1_renamed\":\"4\",\"column2_renamed\":\"5\",\"column3\":\"6\"}\ndiff --git a/tests/queries/0_stateless/02538_alter_rename_sequence.sql b/tests/queries/0_stateless/02538_alter_rename_sequence.sql\nnew file mode 100644\nindex 000000000000..d7df27dc7020\n--- /dev/null\n+++ b/tests/queries/0_stateless/02538_alter_rename_sequence.sql\n@@ -0,0 +1,59 @@\n+DROP TABLE IF EXISTS wrong_metadata;\n+\n+CREATE TABLE wrong_metadata(\n+    column1 UInt64,\n+    column2 UInt64,\n+    column3 UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata', '1')\n+ORDER BY tuple();\n+\n+INSERT INTO wrong_metadata VALUES (1, 2, 3);\n+\n+SYSTEM STOP REPLICATION QUEUES wrong_metadata;\n+\n+ALTER TABLE wrong_metadata RENAME COLUMN column1 TO column1_renamed SETTINGS replication_alter_partitions_sync = 0;\n+\n+INSERT INTO wrong_metadata VALUES (4, 5, 6);\n+\n+SELECT * FROM wrong_metadata ORDER BY column1;\n+\n+SYSTEM START REPLICATION QUEUES wrong_metadata;\n+\n+SYSTEM SYNC REPLICA wrong_metadata;\n+\n+ALTER TABLE wrong_metadata RENAME COLUMN column2 to column2_renamed SETTINGS replication_alter_partitions_sync = 2;\n+\n+SELECT * FROM wrong_metadata ORDER BY column1_renamed FORMAT JSONEachRow;\n+\n+DROP TABLE IF EXISTS wrong_metadata;\n+\n+\n+CREATE TABLE wrong_metadata_wide(\n+    column1 UInt64,\n+    column2 UInt64,\n+    column3 UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata_wide', '1')\n+ORDER BY tuple()\n+SETTINGS min_bytes_for_wide_part = 0;\n+\n+INSERT INTO wrong_metadata_wide VALUES (1, 2, 3);\n+\n+SYSTEM STOP REPLICATION QUEUES wrong_metadata_wide;\n+\n+ALTER TABLE wrong_metadata_wide RENAME COLUMN column1 TO column1_renamed SETTINGS replication_alter_partitions_sync = 0;\n+\n+INSERT INTO wrong_metadata_wide VALUES (4, 5, 6);\n+\n+SELECT * FROM wrong_metadata_wide ORDER by column1;\n+\n+SYSTEM START REPLICATION QUEUES wrong_metadata_wide;\n+\n+SYSTEM SYNC REPLICA wrong_metadata_wide;\n+\n+ALTER TABLE wrong_metadata_wide RENAME COLUMN column2 to column2_renamed SETTINGS replication_alter_partitions_sync = 2;\n+\n+SELECT * FROM wrong_metadata_wide ORDER BY column1_renamed FORMAT JSONEachRow;\n+\n+DROP TABLE IF EXISTS wrong_metadata_wide;\ndiff --git a/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference\nnew file mode 100644\nindex 000000000000..156128e3dd28\n--- /dev/null\n+++ b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference\n@@ -0,0 +1,1 @@\n+{\"v\":\"1\",\"v2\":\"77\"}\ndiff --git a/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh\nnew file mode 100755\nindex 000000000000..adaf1846552f\n--- /dev/null\n+++ b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh\n@@ -0,0 +1,58 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS table_to_rename\"\n+\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE table_to_rename(v UInt64, v1 UInt64)ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO table_to_rename VALUES (1, 1)\"\n+\n+\n+# we want to following mutations to stuck\n+# That is why we stop merges and wait in loops until they actually start\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM STOP MERGES table_to_rename\"\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE table_to_rename RENAME COLUMN v1 to v2\" &\n+\n+counter=0 retries=60\n+\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"show create table table_to_rename\")\n+    if [[ $result == *\"v2\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE table_to_rename UPDATE v2 = 77 WHERE 1 = 1 SETTINGS mutations_sync = 2\" &\n+\n+counter=0 retries=60\n+\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SELECT count() from system.mutations where database='${CLICKHOUSE_DATABASE}' and table='table_to_rename'\")\n+    if [[ $result == \"2\" ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM START MERGES table_to_rename\"\n+\n+wait\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM table_to_rename FORMAT JSONEachRow\"\n+\n+\n+ $CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS table_to_rename\"\ndiff --git a/tests/queries/0_stateless/02543_alter_update_rename_stuck.reference b/tests/queries/0_stateless/02543_alter_update_rename_stuck.reference\nnew file mode 100644\nindex 000000000000..156128e3dd28\n--- /dev/null\n+++ b/tests/queries/0_stateless/02543_alter_update_rename_stuck.reference\n@@ -0,0 +1,1 @@\n+{\"v\":\"1\",\"v2\":\"77\"}\ndiff --git a/tests/queries/0_stateless/02543_alter_update_rename_stuck.sh b/tests/queries/0_stateless/02543_alter_update_rename_stuck.sh\nnew file mode 100755\nindex 000000000000..e801fbedab79\n--- /dev/null\n+++ b/tests/queries/0_stateless/02543_alter_update_rename_stuck.sh\n@@ -0,0 +1,48 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS table_to_rename\"\n+\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE table_to_rename(v UInt64, v1 UInt64)ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO table_to_rename VALUES (1, 1)\"\n+\n+\n+# we want to following mutations to stuck\n+# That is why we stop merges and wait in loops until they actually start\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM STOP MERGES table_to_rename\"\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE table_to_rename UPDATE v1 = 77 WHERE 1 = 1 SETTINGS mutations_sync = 2\" &\n+\n+counter=0 retries=60\n+\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SELECT count() from system.mutations where database='${CLICKHOUSE_DATABASE}' and table='table_to_rename'\")\n+    if [[ $result == \"1\" ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE table_to_rename RENAME COLUMN v1 to v2\" &\n+\n+\n+# it will not introduce any flakyness\n+# just wait that mutation doesn't start\n+sleep 3\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM START MERGES table_to_rename\"\n+\n+wait\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM table_to_rename FORMAT JSONEachRow\"\n+\n+\n+ $CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS table_to_rename\"\ndiff --git a/tests/queries/0_stateless/02555_davengers_rename_chain.reference b/tests/queries/0_stateless/02555_davengers_rename_chain.reference\nnew file mode 100644\nindex 000000000000..a9fc4b395e20\n--- /dev/null\n+++ b/tests/queries/0_stateless/02555_davengers_rename_chain.reference\n@@ -0,0 +1,26 @@\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+{\"a1\":\"7\",\"b1\":\"8\",\"c\":\"9\"}\n+~~~~~~~\n+{\"b\":\"1\",\"a\":\"2\",\"c\":\"3\"}\n+{\"b\":\"4\",\"a\":\"5\",\"c\":\"6\"}\n+{\"b\":\"7\",\"a\":\"8\",\"c\":\"9\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+{\"a1\":\"7\",\"b1\":\"8\",\"c\":\"9\"}\n+~~~~~~~\n+{\"b\":\"1\",\"a\":\"2\",\"c\":\"3\"}\n+{\"b\":\"4\",\"a\":\"5\",\"c\":\"6\"}\n+{\"b\":\"7\",\"a\":\"8\",\"c\":\"9\"}\n+~~~~~~~\ndiff --git a/tests/queries/0_stateless/02555_davengers_rename_chain.sh b/tests/queries/0_stateless/02555_davengers_rename_chain.sh\nnew file mode 100755\nindex 000000000000..b23f8085fd70\n--- /dev/null\n+++ b/tests/queries/0_stateless/02555_davengers_rename_chain.sh\n@@ -0,0 +1,143 @@\n+#!/usr/bin/env bash\n+# Tags: replica\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT -n --query=\"CREATE TABLE wrong_metadata(\n+    a UInt64,\n+    b UInt64,\n+    c UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata', '1')\n+ORDER BY tuple()\n+SETTINGS min_bytes_for_wide_part = 0\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata VALUES (1, 2, 3)\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM STOP MERGES wrong_metadata\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata RENAME COLUMN a TO a1, RENAME COLUMN b to b1 SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SHOW CREATE TABLE wrong_metadata\")\n+    if [[ $result == *\"\\`a1\\` UInt64\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata ORDER BY a1 FORMAT JSONEachRow\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata VALUES (4, 5, 6)\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata ORDER BY a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata RENAME COLUMN a1 TO b, RENAME COLUMN b1 to a SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SELECT * FROM system.mutations WHERE table = 'wrong_metadata' AND database='${CLICKHOUSE_DATABASE}'\")\n+    if [[ $result == *\"b1 TO a\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata VALUES (7, 8, 9)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata ORDER by a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM START MERGES wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM SYNC REPLICA wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata order by a FORMAT JSONEachRow\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT -n --query=\"CREATE TABLE wrong_metadata_compact(\n+    a UInt64,\n+    b UInt64,\n+    c UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata_compact', '1')\n+ORDER BY tuple()\n+SETTINGS min_bytes_for_wide_part = 10000000\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata_compact VALUES (1, 2, 3)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM STOP MERGES wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata_compact RENAME COLUMN a TO a1, RENAME COLUMN b to b1 SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SHOW CREATE TABLE wrong_metadata_compact\")\n+    if [[ $result == *\"\\`a1\\` UInt64\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact ORDER BY a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata_compact VALUES (4, 5, 6)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact ORDER BY a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata_compact RENAME COLUMN a1 TO b, RENAME COLUMN b1 to a SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SELECT * FROM system.mutations WHERE table = 'wrong_metadata_compact' AND database='${CLICKHOUSE_DATABASE}'\")\n+    if [[ $result == *\"b1 TO a\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata_compact VALUES (7, 8, 9)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact ORDER by a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM START MERGES wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM SYNC REPLICA wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact order by a FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata_compact\"\ndiff --git a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference\nindex 2d41f5dae893..00e93b1db3d1 100644\n--- a/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference\n+++ b/tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference\n@@ -9,15 +9,15 @@ CHECK WITH query_log\n QueryFinish\tS3CreateMultipartUpload\t1\tS3UploadPart\t1\tS3CompleteMultipartUpload\t1\tS3PutObject\t0\n CREATE\n INSERT\n- [ 0 ] FileOpen: 7 \n+ [ 0 ] FileOpen: 8 \n READ\n INSERT and READ INSERT\n- [ 0 ] FileOpen: 7 \n- [ 0 ] FileOpen: 7 \n+ [ 0 ] FileOpen: 8 \n+ [ 0 ] FileOpen: 8 \n DROP\n CHECK with query_log\n-QueryFinish\tINSERT INTO times SELECT now() + INTERVAL 1 day SETTINGS optimize_on_insert = 0;\tFileOpen\t7\n+QueryFinish\tINSERT INTO times SELECT now() + INTERVAL 1 day SETTINGS optimize_on_insert = 0;\tFileOpen\t8\n QueryFinish\tSELECT \\'1\\', min(t) FROM times;\tFileOpen\t0\n-QueryFinish\tINSERT INTO times SELECT now() + INTERVAL 2 day SETTINGS optimize_on_insert = 0;\tFileOpen\t7\n+QueryFinish\tINSERT INTO times SELECT now() + INTERVAL 2 day SETTINGS optimize_on_insert = 0;\tFileOpen\t8\n QueryFinish\tSELECT \\'2\\', min(t) FROM times;\tFileOpen\t0\n-QueryFinish\tINSERT INTO times SELECT now() + INTERVAL 3 day SETTINGS optimize_on_insert = 0;\tFileOpen\t7\n+QueryFinish\tINSERT INTO times SELECT now() + INTERVAL 3 day SETTINGS optimize_on_insert = 0;\tFileOpen\t8\n",
  "problem_statement": "Mutations may fail when coalesced together\n**Describe what's wrong**\r\n\r\nThe following example should succeed. If merges are not stopped and each mutation is given opportunity to complete, then no errors arise. However, they fail when ClickHouse decides to coalesce them and run in a single task.\r\n\r\n**How to reproduce**\r\n\r\n```sql\r\nCREATE TABLE t\r\n(\r\n    `v` DateTime,\r\n    `v1` DateTime\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(v)\r\nORDER BY tuple()\r\nSETTINGS min_bytes_for_wide_part = 0;\r\n\r\nINSERT INTO t VALUES (1,1);\r\nSYSTEM STOP MERGES;\r\nALTER TABLE t RENAME COLUMN v1 to v2; -- this blocks, start a new client/connection for the next query\r\nALTER TABLE t UPDATE v2 = 1 WHERE 1 = 1;\r\nSYSTEM START MERGES;\r\n```\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\n\r\nQuery id: 3c225cd0-a5ba-4395-80e5-4a272446b5d2\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500mutation_id\u2500\u2500\u2500\u2500\u252c\u2500command\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500create_time\u2500\u252c\u2500block_numbers.partition_id\u2500\u252c\u2500block_numbers.number\u2500\u252c\u2500parts_to_do_names\u2500\u252c\u2500parts_to_do\u2500\u252c\u2500is_done\u2500\u252c\u2500latest_failed_part\u2500\u252c\u2500\u2500\u2500\u2500latest_fail_time\u2500\u252c\u2500latest_fail_reason\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 default  \u2502 t     \u2502 mutation_2.txt \u2502 RENAME COLUMN v1 TO v2    \u2502 2022-02-05 21:19:11 \u2502 ['']                       \u2502 [2]                  \u2502 ['197001_1_1_0']  \u2502           1 \u2502       0 \u2502 197001_1_1_0       \u2502 2022-02-05 21:19:23 \u2502 Code: 246. DB::Exception: Bad size of marks file '/var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/197001_1_1_0/v1.mrk2': 0, must be: 48: While executing MergeTreeInOrder. (CORRUPTED_DATA) (version 22.1.3.7 (official build)) \u2502\r\n\u2502 default  \u2502 t     \u2502 mutation_3.txt \u2502 UPDATE v2 = 1 WHERE 1 = 1 \u2502 2022-02-05 21:19:16 \u2502 ['']                       \u2502 [3]                  \u2502 ['197001_1_1_0']  \u2502           1 \u2502       0 \u2502 197001_1_1_0       \u2502 2022-02-05 21:19:23 \u2502 Code: 246. DB::Exception: Bad size of marks file '/var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/197001_1_1_0/v1.mrk2': 0, must be: 48: While executing MergeTreeInOrder. (CORRUPTED_DATA) (version 22.1.3.7 (official build)) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nand logs\r\n\r\n```\r\n2022.02.05 21:19:28.592575 [ 73 ] {3668606c-16fe-4e35-8a4e-3b2bd9def292::197001_1_1_0_3} <Error> default.t (3668606c-16fe-4e35-8a4e-3b2bd9def292): Cannot quickly remove directory /var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/delete_tmp_tmp_mut_197001_1_1_0_3 by removing files; fallback to recursive removal. Reason: Code: 458. DB::ErrnoException: Cannot unlink file /var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/delete_tmp_tmp_mut_197001_1_1_0_3/v1.bin, errno: 2, strerror: No such file or directory. (CANNOT_UNLINK) (version 22.1.3.7 (official build))\r\n```\n",
  "hints_text": "@alesapin the solution can be very simple: don't coalesce mutations involving RENAME column.\r\nThe mutation with RENAME will work as a barrier, it will split everything into three parts: everything before it, itself, and everything after it.",
  "created_at": "2023-02-27T11:28:30Z",
  "modified_files": [
    "b/src/Storages/MergeTree/AlterConversions.cpp",
    "src/Storages/MergeTree/AlterConversions.h",
    "src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp",
    "src/Storages/MergeTree/DataPartStorageOnDiskBase.h",
    "src/Storages/MergeTree/DataPartsExchange.cpp",
    "src/Storages/MergeTree/IDataPartStorage.h",
    "src/Storages/MergeTree/IMergeTreeDataPart.cpp",
    "src/Storages/MergeTree/IMergeTreeDataPart.h",
    "src/Storages/MergeTree/MergeTask.cpp",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeData.h",
    "src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp",
    "src/Storages/MergeTree/MergeTreeDataWriter.cpp",
    "src/Storages/MergeTree/MergeTreeMarksLoader.cpp",
    "src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp",
    "src/Storages/MergeTree/MergedBlockOutputStream.cpp",
    "src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp",
    "src/Storages/MergeTree/MutateTask.cpp",
    "src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp",
    "src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp",
    "src/Storages/MergeTree/ReplicatedMergeTreeQueue.h",
    "src/Storages/MutationCommands.cpp",
    "src/Storages/MutationCommands.h",
    "src/Storages/StorageInMemoryMetadata.cpp",
    "src/Storages/StorageInMemoryMetadata.h",
    "src/Storages/StorageMergeTree.cpp",
    "src/Storages/StorageMergeTree.h",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "tests/integration/test_disk_over_web_server/test.py",
    "tests/integration/test_merge_tree_hdfs/test.py",
    "tests/integration/test_merge_tree_s3/test.py",
    "tests/integration/test_merge_tree_s3_failover/test.py",
    "tests/integration/test_partition/test.py",
    "tests/integration/test_replicated_merge_tree_s3/test.py",
    "tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py",
    "tests/integration/test_s3_zero_copy_ttl/test.py",
    "tests/queries/0_stateless/01278_alter_rename_combination.reference",
    "tests/queries/0_stateless/01278_alter_rename_combination.sql",
    "tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference",
    "tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql",
    "tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference",
    "tests/queries/0_stateless/02361_fsync_profile_events.sh",
    "b/tests/queries/0_stateless/02538_alter_rename_sequence.reference",
    "b/tests/queries/0_stateless/02538_alter_rename_sequence.sql",
    "b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference",
    "b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh",
    "b/tests/queries/0_stateless/02543_alter_update_rename_stuck.reference",
    "b/tests/queries/0_stateless/02543_alter_update_rename_stuck.sh",
    "b/tests/queries/0_stateless/02555_davengers_rename_chain.reference",
    "b/tests/queries/0_stateless/02555_davengers_rename_chain.sh",
    "tests/queries/0_stateless/02675_profile_events_from_query_log_and_client.reference"
  ]
}