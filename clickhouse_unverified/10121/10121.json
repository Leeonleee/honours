{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 10121,
  "instance_id": "ClickHouse__ClickHouse-10121",
  "issue_numbers": [
    "10098"
  ],
  "base_commit": "1e3ec9113a77d44772336050b6fe8fd7d41b43c0",
  "patch": "diff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 31456c8d1f17..1af86f7d5f1c 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -246,6 +246,11 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n \n         createTableIfNotExists();\n \n+        /// We have to check granularity on other replicas. If it's fixed we\n+        /// must create our new replica with fixed granularity and store this\n+        /// information in /replica/metadata.\n+        other_replicas_fixed_granularity = checkFixedGranualrityInZookeeper();\n+\n         checkTableStructure(zookeeper_path);\n \n         Coordination::Stat metadata_stat;\n@@ -256,11 +261,14 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     }\n     else\n     {\n+\n         /// In old tables this node may missing or be empty\n         String replica_metadata;\n         bool replica_metadata_exists = current_zookeeper->tryGet(replica_path + \"/metadata\", replica_metadata);\n         if (!replica_metadata_exists || replica_metadata.empty())\n         {\n+            /// We have to check shared node granularity before we create ours.\n+            other_replicas_fixed_granularity = checkFixedGranualrityInZookeeper();\n             ReplicatedMergeTreeTableMetadata current_metadata(*this);\n             current_zookeeper->createOrUpdate(replica_path + \"/metadata\", current_metadata.toString(), zkutil::CreateMode::Persistent);\n         }\n@@ -291,7 +299,6 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     createNewZooKeeperNodes();\n \n \n-    other_replicas_fixed_granularity = checkFixedGranualrityInZookeeper();\n }\n \n \n",
  "test_patch": "diff --git a/tests/integration/test_adaptive_granularity_different_settings/__init__.py b/tests/integration/test_adaptive_granularity_different_settings/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_adaptive_granularity_different_settings/test.py b/tests/integration/test_adaptive_granularity_different_settings/test.py\nnew file mode 100644\nindex 000000000000..b066c437e06a\n--- /dev/null\n+++ b/tests/integration/test_adaptive_granularity_different_settings/test.py\n@@ -0,0 +1,49 @@\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', with_zookeeper=True)\n+node2 = cluster.add_instance('node2', with_zookeeper=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_attach_detach(start_cluster):\n+\n+    node1.query(\"\"\"\n+        CREATE TABLE test (key UInt64)\n+        ENGINE = ReplicatedMergeTree('/clickhouse/test', '1')\n+        ORDER BY tuple()\n+        SETTINGS index_granularity_bytes = 0\"\"\")\n+\n+    node1.query(\"INSERT INTO test VALUES (1), (2)\")\n+\n+    node2.query(\"\"\"\n+        CREATE TABLE test (key UInt64)\n+        ENGINE = ReplicatedMergeTree('/clickhouse/test', '2')\n+        ORDER BY tuple()\"\"\")\n+\n+    node2.query(\"INSERT INTO test VALUES (3), (4)\")\n+\n+    node1.query(\"SYSTEM SYNC REPLICA test\")\n+    node2.query(\"SYSTEM SYNC REPLICA test\")\n+\n+    assert node1.query(\"SELECT COUNT() FROM  test\") == \"4\\n\"\n+    assert node2.query(\"SELECT COUNT() FROM  test\") == \"4\\n\"\n+\n+    node1.query(\"DETACH TABLE test\")\n+    node2.query(\"DETACH TABLE test\")\n+\n+    node1.query(\"ATTACH TABLE test\")\n+    node2.query(\"ATTACH TABLE test\")\n+\n+    assert node1.query(\"SELECT COUNT() FROM  test\") == \"4\\n\"\n+    assert node2.query(\"SELECT COUNT() FROM  test\") == \"4\\n\"\n",
  "problem_statement": "20.3 index_granularity_bytes for replicated tables can be written into ZK incorrectly\nWhen a new replica is created, the default value for `index granularity bytes` gets written in its metadata in ZK.\r\n\r\n**Describe the bug**\r\nI have replicated tables that were created a few years ago, before adaptive granularity was invented.\r\nTheir metadata in ZK looks like this:\r\n```\r\n/clickhouse/tables/apps/metadata\r\nmetadata format version: 1\r\ndate column:\r\nsampling expression: \r\nindex granularity: 8192\r\nmode: 5\r\nsign column: \r\nprimary key: apps_key\r\n/clickhouse/tables/apps/metadata/ctime: 2019-06-04 08:51:20\r\n\r\n/clickhouse/tables/apps/replicas/r1/metadata:\r\nmetadata format version: 1\r\ndate column:\r\nsampling expression: \r\nindex granularity: 8192\r\nmode: 5\r\nsign column: \r\nprimary key: apps_key\r\n/clickhouse/tables/apps/replicas/r1/metadata/ctime: 2019-06-17 08:51:19\r\n```\r\nI upgraded the cluster to 20.3, and then added another replica for the table.\r\nThe metadata for the new replica looks like this:\r\n```\r\n/clickhouse/tables/apps/replicas/r2/metadata:\r\nmetadata format version: 1\r\ndate column:\r\nsampling expression: \r\nindex granularity: 8192\r\nmode: 5\r\nsign column: \r\nprimary key: apps_key\r\ngranularity bytes: 10485760\r\n/clickhouse/tables/apps/replicas/r2/metadata/ctime: 2020-04-07 12:10:54\r\n```\r\nEverything works well until a restart.\r\nClickHouse cannot start up with an exception `Existing table metadata in ZooKeeper differs in index granularity bytes. Stored in ZooKeeper: 10485760, local: 0`:\r\n```\r\n2020.04.06 07:24:25.380164 [ 719 ] {} <Error> ThreadPool: Exception in ThreadPool(max_threads: 4, max_free_threads: 4, queue_size: 4, shutdown_on_exception: 1).: Code: 342, e.displayText() = DB::Exception: Existing table metadata in ZooKeeper differs in index granularity bytes. Stored in ZooKeeper: 10485760, local: 0: Cannot attach table `db`.`apps` from metadata file /var/lib/clickhouse/metadata/db/apps.sql from query ATTACH TABLE apps (...) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/apps', '{replica}') PARTITION BY tuple() ORDER BY apps_key SETTINGS index_granularity = 8192, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x102e0d8c in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8f2d989 in /usr/bin/clickhouse\r\n2. ? @ 0xd94d5ee in /usr/bin/clickhouse\r\n3. DB::StorageReplicatedMergeTree::checkTableStructure(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xd5b4c8b in /usr/bin/clickhouse\r\n4. DB::StorageReplicatedMergeTree::StorageReplicatedMergeTree(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata const&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::MergeTreeData::MergingParams const&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >, bool) @ 0xd5da56b in /usr/bin/clickhouse\r\n5. ? @ 0xd9585da in /usr/bin/clickhouse\r\n6. std::__1::__function::__func<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&), std::__1::allocator<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&)>, std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) @ 0xd95baf3 in /usr/bin/clickhouse\r\n7. DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, DB::Context&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) const @ 0xd4fc66c in /usr/bin/clickhouse\r\n8. DB::createTableFromAST(DB::ASTCreateQuery, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool) @ 0xcedc88e in /usr/bin/clickhouse\r\n9. ? @ 0xced33bf in /usr/bin/clickhouse\r\n10. ? @ 0xced3b71 in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x8f51c4b in /usr/bin/clickhouse\r\n12. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const @ 0x8f528c4 in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8f50b4b in /usr/bin/clickhouse\r\n14. ? @ 0x8f4f00f in /usr/bin/clickhouse\r\n15. start_thread @ 0x74a4 in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n16. clone @ 0xe8d0f in /lib/x86_64-linux-gnu/libc-2.24.so\r\n (version 20.3.4.10 (official build))\r\n```\r\nCH was able to start up after a downgrade to 19.16.\r\nWhen a table like this gets attached by 19.16, the replica's `metadata` in ZK gets changed to the same value as the table's `metadata`.\r\nAlternatively CH 20.3 could be brought up by deleting the table's data folders.\r\n\r\n**How to reproduce**\r\n1. Set `index_granularity_bytes` to 0.\r\n2. Restart 20.3\r\n3. Create a replicated table without specifying `index_granularity_bytes`.\r\n4. Insert some data into the table.\r\n5. Set `index_granularity_bytes` to 10485760 (or remove the setting form config files).\r\n6. Restart 20.3\r\n7. Create another replica for the same table without specifying `index_granularity_bytes`.\r\n8. Restart 20.3\r\n\r\n**Expected behavior**\r\nThe correct `index granularity bytes` should be written to `metadata` for a new table, not the default.\r\n\n",
  "hints_text": "Same error occur using versioin `20.3.5.21` of clickhouse.\r\n\r\n**Reproduce**:\r\n1. Just create a replicated table with new replica and do nothing with `index_granularity_bytes`.\r\n2. Restart, and clickhouse-server can't be started. Error says \r\n\r\n```\r\n2020.04.08 14:06:51.814429 [ 13328 ] {} <Error> Application: DB::Exception: Existing table metadata in ZooKeeper differs in index granularity bytes. Stored in ZooKeeper: 10485760, local: 0: Cannot attach table\r\n```\r\n\r\nSo should we change some config when upgrad to this verion to make replicated tables work well?\nehh, stupid bug :( Will be fixed in next 20.3.",
  "created_at": "2020-04-08T09:52:51Z"
}