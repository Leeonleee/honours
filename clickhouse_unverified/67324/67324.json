{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 67324,
  "instance_id": "ClickHouse__ClickHouse-67324",
  "issue_numbers": [
    "67292"
  ],
  "base_commit": "b8050cc471bb59c39d3c9f06665bbc81f37ada52",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 6988a66cf1ed..0d498ce76999 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -1127,6 +1127,7 @@ class IColumn;\n     M(Bool, input_format_json_throw_on_bad_escape_sequence, true, \"Throw an exception if JSON string contains bad escape sequence in JSON input formats. If disabled, bad escape sequences will remain as is in the data\", 0) \\\n     M(Bool, input_format_json_ignore_unnecessary_fields, true, \"Ignore unnecessary fields and not parse them. Enabling this may not throw exceptions on json strings of invalid format or with duplicated fields\", 0) \\\n     M(Bool, input_format_json_case_insensitive_column_matching, false, \"Ignore case when matching JSON keys with CH columns\", 0) \\\n+    M(UInt64, input_format_json_max_depth, 1000, \"Maximum depth of a field in JSON. This is not a strict limit, it does not have to be applied precisely.\", 0) \\\n     M(Bool, input_format_try_infer_integers, true, \"Try to infer integers instead of floats while schema inference in text formats\", 0) \\\n     M(Bool, input_format_try_infer_dates, true, \"Try to infer dates from string fields while schema inference in text formats\", 0) \\\n     M(Bool, input_format_try_infer_datetimes, true, \"Try to infer datetimes from string fields while schema inference in text formats\", 0) \\\ndiff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp\nindex b6ef654438e5..5b94391bade0 100644\n--- a/src/Core/SettingsChangesHistory.cpp\n+++ b/src/Core/SettingsChangesHistory.cpp\n@@ -75,6 +75,7 @@ static std::initializer_list<std::pair<ClickHouseVersion, SettingsChangesHistory\n     },\n     {\"24.8\",\n         {\n+            {\"input_format_json_max_depth\", 1000000, 1000, \"It was unlimited in previous versions, but that was unsafe.\"},\n             {\"merge_tree_min_bytes_per_task_for_remote_reading\", 4194304, 2097152, \"Value is unified with `filesystem_prefetch_min_bytes_for_single_read_task`\"},\n             {\"allow_archive_path_syntax\", true, true, \"Added new setting to allow disabling archive path syntax.\"},\n         }\ndiff --git a/src/Formats/FormatFactory.cpp b/src/Formats/FormatFactory.cpp\nindex 15fcbf2b1b89..68946c895bb5 100644\n--- a/src/Formats/FormatFactory.cpp\n+++ b/src/Formats/FormatFactory.cpp\n@@ -123,6 +123,7 @@ FormatSettings getFormatSettings(const ContextPtr & context, const Settings & se\n     format_settings.import_nested_json = settings.input_format_import_nested_json;\n     format_settings.input_allow_errors_num = settings.input_format_allow_errors_num;\n     format_settings.input_allow_errors_ratio = settings.input_format_allow_errors_ratio;\n+    format_settings.json.max_depth = settings.input_format_json_max_depth;\n     format_settings.json.array_of_rows = settings.output_format_json_array_of_rows;\n     format_settings.json.escape_forward_slashes = settings.output_format_json_escape_forward_slashes;\n     format_settings.json.write_named_tuples_as_objects = settings.output_format_json_named_tuples_as_objects;\ndiff --git a/src/Formats/FormatSettings.h b/src/Formats/FormatSettings.h\nindex 225d03e54fdf..72104518eccc 100644\n--- a/src/Formats/FormatSettings.h\n+++ b/src/Formats/FormatSettings.h\n@@ -205,6 +205,7 @@ struct FormatSettings\n \n     struct JSON\n     {\n+        size_t max_depth = 1000;\n         bool array_of_rows = false;\n         bool quote_64bit_integers = true;\n         bool quote_64bit_floats = false;\ndiff --git a/src/IO/ReadHelpers.cpp b/src/IO/ReadHelpers.cpp\nindex c771fced73a9..9559462e62b5 100644\n--- a/src/IO/ReadHelpers.cpp\n+++ b/src/IO/ReadHelpers.cpp\n@@ -3,13 +3,13 @@\n #include <Common/PODArray.h>\n #include <Common/StringUtils.h>\n #include <Common/memcpySmall.h>\n+#include <Common/checkStackSize.h>\n #include <Formats/FormatSettings.h>\n #include <IO/WriteBufferFromString.h>\n #include <IO/BufferWithOwnMemory.h>\n #include <IO/PeekableReadBuffer.h>\n #include <IO/readFloatText.h>\n #include <IO/Operators.h>\n-#include <base/find_symbols.h>\n #include <cstdlib>\n #include <bit>\n \n@@ -39,6 +39,7 @@ namespace ErrorCodes\n     extern const int ATTEMPT_TO_READ_AFTER_EOF;\n     extern const int LOGICAL_ERROR;\n     extern const int BAD_ARGUMENTS;\n+    extern const int TOO_DEEP_RECURSION;\n }\n \n template <size_t num_bytes, typename IteratorSrc, typename IteratorDst>\n@@ -1494,10 +1495,20 @@ template bool readDateTimeTextFallback<bool, true>(time_t &, ReadBuffer &, const\n \n \n template <typename ReturnType>\n-ReturnType skipJSONFieldImpl(ReadBuffer & buf, StringRef name_of_field, const FormatSettings::JSON & settings)\n+ReturnType skipJSONFieldImpl(ReadBuffer & buf, StringRef name_of_field, const FormatSettings::JSON & settings, size_t current_depth)\n {\n     static constexpr bool throw_exception = std::is_same_v<ReturnType, void>;\n \n+    if (unlikely(current_depth > settings.max_depth))\n+    {\n+        if constexpr (throw_exception)\n+            throw Exception(ErrorCodes::TOO_DEEP_RECURSION, \"JSON is too deep for key '{}'\", name_of_field.toString());\n+        return ReturnType(false);\n+    }\n+\n+    if (unlikely(current_depth > 0 && current_depth % 1024 == 0))\n+        checkStackSize();\n+\n     if (buf.eof())\n     {\n         if constexpr (throw_exception)\n@@ -1560,8 +1571,8 @@ ReturnType skipJSONFieldImpl(ReadBuffer & buf, StringRef name_of_field, const Fo\n         while (true)\n         {\n             if constexpr (throw_exception)\n-                skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings);\n-            else if (!skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings))\n+                skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings, current_depth + 1);\n+            else if (!skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings, current_depth + 1))\n                 return ReturnType(false);\n \n             skipWhitespaceIfAny(buf);\n@@ -1619,8 +1630,8 @@ ReturnType skipJSONFieldImpl(ReadBuffer & buf, StringRef name_of_field, const Fo\n             skipWhitespaceIfAny(buf);\n \n             if constexpr (throw_exception)\n-                skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings);\n-            else if (!skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings))\n+                skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings, current_depth + 1);\n+            else if (!skipJSONFieldImpl<ReturnType>(buf, name_of_field, settings, current_depth + 1))\n                 return ReturnType(false);\n \n             skipWhitespaceIfAny(buf);\n@@ -1659,12 +1670,12 @@ ReturnType skipJSONFieldImpl(ReadBuffer & buf, StringRef name_of_field, const Fo\n \n void skipJSONField(ReadBuffer & buf, StringRef name_of_field, const FormatSettings::JSON & settings)\n {\n-    skipJSONFieldImpl<void>(buf, name_of_field, settings);\n+    skipJSONFieldImpl<void>(buf, name_of_field, settings, 0);\n }\n \n bool trySkipJSONField(ReadBuffer & buf, StringRef name_of_field, const FormatSettings::JSON & settings)\n {\n-    return skipJSONFieldImpl<bool>(buf, name_of_field, settings);\n+    return skipJSONFieldImpl<bool>(buf, name_of_field, settings, 0);\n }\n \n \ndiff --git a/src/Planner/findParallelReplicasQuery.cpp b/src/Planner/findParallelReplicasQuery.cpp\nindex c89a70be541a..39edb1e6516b 100644\n--- a/src/Planner/findParallelReplicasQuery.cpp\n+++ b/src/Planner/findParallelReplicasQuery.cpp\n@@ -113,13 +113,13 @@ std::stack<const QueryNode *> getSupportingParallelReplicasQuery(const IQueryTre\n     return res;\n }\n \n-class ReplaceTableNodeToDummyVisitor : public InDepthQueryTreeVisitor<ReplaceTableNodeToDummyVisitor, true>\n+class ReplaceTableNodeToDummyVisitor : public InDepthQueryTreeVisitorWithContext<ReplaceTableNodeToDummyVisitor>\n {\n public:\n-    using Base = InDepthQueryTreeVisitor<ReplaceTableNodeToDummyVisitor, true>;\n+    using Base = InDepthQueryTreeVisitorWithContext<ReplaceTableNodeToDummyVisitor>;\n     using Base::Base;\n \n-    void visitImpl(const QueryTreeNodePtr & node)\n+    void enterImpl(QueryTreeNodePtr & node)\n     {\n         auto * table_node = node->as<TableNode>();\n         auto * table_function_node = node->as<TableFunctionNode>();\n@@ -134,21 +134,19 @@ class ReplaceTableNodeToDummyVisitor : public InDepthQueryTreeVisitor<ReplaceTab\n                 ColumnsDescription(storage_snapshot->getColumns(get_column_options)),\n                 storage_snapshot);\n \n-            auto dummy_table_node = std::make_shared<TableNode>(std::move(storage_dummy), context);\n+            auto dummy_table_node = std::make_shared<TableNode>(std::move(storage_dummy), getContext());\n \n             dummy_table_node->setAlias(node->getAlias());\n             replacement_map.emplace(node.get(), std::move(dummy_table_node));\n         }\n     }\n \n-    ContextPtr context;\n     std::unordered_map<const IQueryTreeNode *, QueryTreeNodePtr> replacement_map;\n };\n \n-QueryTreeNodePtr replaceTablesWithDummyTables(const QueryTreeNodePtr & query, const ContextPtr & context)\n+QueryTreeNodePtr replaceTablesWithDummyTables(QueryTreeNodePtr query, const ContextPtr & context)\n {\n-    ReplaceTableNodeToDummyVisitor visitor;\n-    visitor.context = context;\n+    ReplaceTableNodeToDummyVisitor visitor(context);\n     visitor.visit(query);\n \n     return query->cloneAndReplace(visitor.replacement_map);\ndiff --git a/src/Planner/findQueryForParallelReplicas.h b/src/Planner/findQueryForParallelReplicas.h\nindex f5dc69dfa0e0..cdce4ad0b47c 100644\n--- a/src/Planner/findQueryForParallelReplicas.h\n+++ b/src/Planner/findQueryForParallelReplicas.h\n@@ -13,7 +13,7 @@ using QueryTreeNodePtr = std::shared_ptr<IQueryTreeNode>;\n \n struct SelectQueryOptions;\n \n-/// Find a qury which can be executed with parallel replicas up to WithMergableStage.\n+/// Find a query which can be executed with parallel replicas up to WithMergableStage.\n /// Returned query will always contain some (>1) subqueries, possibly with joins.\n const QueryNode * findQueryForParallelReplicas(const QueryTreeNodePtr & query_tree_node, SelectQueryOptions & select_query_options);\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03213_deep_json.reference b/tests/queries/0_stateless/03213_deep_json.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/03213_deep_json.sql b/tests/queries/0_stateless/03213_deep_json.sql\nnew file mode 100644\nindex 000000000000..2a9476381ff1\n--- /dev/null\n+++ b/tests/queries/0_stateless/03213_deep_json.sql\n@@ -0,0 +1,5 @@\n+-- The default limit works.\n+SELECT * FROM format(\"JSONCompactEachRow\", 'x UInt32, y UInt32', REPEAT('[1,1,', 100000)) SETTINGS input_format_json_compact_allow_variable_number_of_columns = 1; -- { serverError TOO_DEEP_RECURSION, INCORRECT_DATA }\n+-- Even if we relax the limit, it is also safe.\n+SET input_format_json_max_depth = 100000;\n+SELECT * FROM format(\"JSONCompactEachRow\", 'x UInt32, y UInt32', REPEAT('[1,1,', 100000)) SETTINGS input_format_json_compact_allow_variable_number_of_columns = 1; -- { serverError TOO_DEEP_RECURSION, INCORRECT_DATA }\ndiff --git a/tests/queries/0_stateless/03215_analyzer_replace_with_dummy_tables.reference b/tests/queries/0_stateless/03215_analyzer_replace_with_dummy_tables.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/03215_analyzer_replace_with_dummy_tables.sql b/tests/queries/0_stateless/03215_analyzer_replace_with_dummy_tables.sql\nnew file mode 100644\nindex 000000000000..6d084c2ac501\n--- /dev/null\n+++ b/tests/queries/0_stateless/03215_analyzer_replace_with_dummy_tables.sql\n@@ -0,0 +1,15 @@\n+create table t (number UInt64) engine MergeTree order by number;\n+\n+SELECT 1\n+FROM\n+(\n+    SELECT number IN (\n+            SELECT number\n+            FROM view(\n+                SELECT number\n+                FROM numbers(1)\n+            )\n+        )\n+    FROM t\n+)\n+SETTINGS allow_experimental_parallel_reading_from_replicas = 1, max_parallel_replicas = 2, allow_experimental_analyzer = 1; -- { serverError CLUSTER_DOESNT_EXIST }\n",
  "problem_statement": "Server crashes by format(JSONCompactEachRow, ...) with crafted arguments and settings\n**Describe the bug**\r\nServer crashes by format(JSONCompactEachRow, ...) with crafted arguments and `input_format_json_compact_allow_variable_number_of_columns` enabled.\r\nIt was found by an in-development fuzzer of WINGFUZZ.\r\n\r\n**How to reproduce**\r\nThe SQL statement to reproduce:\r\n```sql\r\nSELECT * FROM format(\"JSONCompactEachRow\", 'x UInt32, y UInt32', REPEAT('[1,1,', 100000)) SETTINGS input_format_json_compact_allow_variable_number_of_columns = 1\r\n```\r\n\r\nIt can be reproduced on the official docker image. (clickhouse/clickhouse-server:head (version 24.8.1.473) and clickhouse/clickhouse-server:latest (version 24.6.2.17)).\r\n\r\nThe log traced by ClickHouse Server:\r\n```\r\nSELECT *\r\nFROM format(JSONCompactEachRow, 'x UInt32, y UInt32', REPEAT('[1,1,', 100000))\r\nSETTINGS input_format_json_compact_allow_variable_number_of_columns = 1\r\n\r\nQuery id: f55f7931-1fed-41f8-b5a3-789d14ee5a71\r\n\r\n[1c6fc9302e45] 2024.07.28 07:49:01.878836 [ 755 ] <Fatal> BaseDaemon: ########################################\r\n[1c6fc9302e45] 2024.07.28 07:49:01.878905 [ 755 ] <Fatal> BaseDaemon: (version 24.8.1.473 (official build), build id: AB57C74DD7456AA12E643F366E718F749FADFC24, git hash: 206a5ff4a1f09f1f4b6826de1f0b35a605697fed) (from thread 745) (query_id: f55f7931-1fed-41f8-b5a3-789d14ee5a71) (query: SELECT * FROM format(\"JSONCompactEachRow\", 'x UInt32, y UInt32', REPEAT('[1,1,', 100000)) SETTINGS input_format_json_compact_allow_variable_number_of_columns = 1;) Received signal Segmentation fault (11)\r\n[1c6fc9302e45] 2024.07.28 07:49:01.878931 [ 755 ] <Fatal> BaseDaemon: Address: 0x7fab4c7c8fe8. Access: write. Attempted access has violated the permissions assigned to the memory area.\r\n[1c6fc9302e45] 2024.07.28 07:49:01.878951 [ 755 ] <Fatal> BaseDaemon: Stack trace: 0x000000000d8bd724 0x00007facf510d420 0x000000000d679af1 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879019 [ 755 ] <Fatal> BaseDaemon: 0. signalHandler(int, siginfo_t*, void*) @ 0x000000000d8bd724\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879053 [ 755 ] <Fatal> BaseDaemon: 1. ? @ 0x00007facf510d420\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879088 [ 755 ] <Fatal> BaseDaemon: 2. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d679af1\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879116 [ 755 ] <Fatal> BaseDaemon: 3. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879148 [ 755 ] <Fatal> BaseDaemon: 4. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879175 [ 755 ] <Fatal> BaseDaemon: 5. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879200 [ 755 ] <Fatal> BaseDaemon: 6. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879224 [ 755 ] <Fatal> BaseDaemon: 7. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879267 [ 755 ] <Fatal> BaseDaemon: 8. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879292 [ 755 ] <Fatal> BaseDaemon: 9. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879318 [ 755 ] <Fatal> BaseDaemon: 10. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879345 [ 755 ] <Fatal> BaseDaemon: 11. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879368 [ 755 ] <Fatal> BaseDaemon: 12. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879396 [ 755 ] <Fatal> BaseDaemon: 13. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879423 [ 755 ] <Fatal> BaseDaemon: 14. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879453 [ 755 ] <Fatal> BaseDaemon: 15. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879485 [ 755 ] <Fatal> BaseDaemon: 16. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879509 [ 755 ] <Fatal> BaseDaemon: 17. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879536 [ 755 ] <Fatal> BaseDaemon: 18. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879565 [ 755 ] <Fatal> BaseDaemon: 19. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879590 [ 755 ] <Fatal> BaseDaemon: 20. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879617 [ 755 ] <Fatal> BaseDaemon: 21. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879640 [ 755 ] <Fatal> BaseDaemon: 22. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879677 [ 755 ] <Fatal> BaseDaemon: 23. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879703 [ 755 ] <Fatal> BaseDaemon: 24. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879749 [ 755 ] <Fatal> BaseDaemon: 25. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879774 [ 755 ] <Fatal> BaseDaemon: 26. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879795 [ 755 ] <Fatal> BaseDaemon: 27. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879825 [ 755 ] <Fatal> BaseDaemon: 28. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879853 [ 755 ] <Fatal> BaseDaemon: 29. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879883 [ 755 ] <Fatal> BaseDaemon: 30. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879915 [ 755 ] <Fatal> BaseDaemon: 31. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879937 [ 755 ] <Fatal> BaseDaemon: 32. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879962 [ 755 ] <Fatal> BaseDaemon: 33. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.879991 [ 755 ] <Fatal> BaseDaemon: 34. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880019 [ 755 ] <Fatal> BaseDaemon: 35. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880045 [ 755 ] <Fatal> BaseDaemon: 36. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880073 [ 755 ] <Fatal> BaseDaemon: 37. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880100 [ 755 ] <Fatal> BaseDaemon: 38. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880130 [ 755 ] <Fatal> BaseDaemon: 39. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880154 [ 755 ] <Fatal> BaseDaemon: 40. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880178 [ 755 ] <Fatal> BaseDaemon: 41. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880198 [ 755 ] <Fatal> BaseDaemon: 42. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880227 [ 755 ] <Fatal> BaseDaemon: 43. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:01.880253 [ 755 ] <Fatal> BaseDaemon: 44. void DB::skipJSONFieldImpl<void>(DB::ReadBuffer&, StringRef, DB::FormatSettings::JSON const&) @ 0x000000000d67a0c5\r\n[1c6fc9302e45] 2024.07.28 07:49:02.031011 [ 755 ] <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: EFFEC9494498ABD92EF154C349356F59)\r\n[1c6fc9302e45] 2024.07.28 07:49:02.031368 [ 755 ] <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\r\n[1c6fc9302e45] 2024.07.28 07:49:02.031506 [ 755 ] <Fatal> BaseDaemon: Changed settings: input_format_json_compact_allow_variable_number_of_columns = true\r\n```\n",
  "hints_text": "",
  "created_at": "2024-07-28T23:41:59Z"
}