diff --git a/base/loggers/OwnSplitChannel.cpp b/base/loggers/OwnSplitChannel.cpp
index 62fdddad28c8..2349c60856fc 100644
--- a/base/loggers/OwnSplitChannel.cpp
+++ b/base/loggers/OwnSplitChannel.cpp
@@ -100,7 +100,7 @@ void OwnSplitChannel::logSplit(const Poco::Message & msg)
         columns[i++]->insert(msg.getSource());
         columns[i++]->insert(msg.getText());
 
-        logs_queue->emplace(std::move(columns));
+        [[maybe_unused]] bool push_result = logs_queue->emplace(std::move(columns));
     }
 
     /// Also log to system.text_log table, if message is not too noisy
diff --git a/src/Access/ReplicatedAccessStorage.cpp b/src/Access/ReplicatedAccessStorage.cpp
index d50ecda0c9ee..7b29aab3a896 100644
--- a/src/Access/ReplicatedAccessStorage.cpp
+++ b/src/Access/ReplicatedAccessStorage.cpp
@@ -34,6 +34,7 @@ ReplicatedAccessStorage::ReplicatedAccessStorage(
     : IAccessStorage(storage_name_)
     , zookeeper_path(zookeeper_path_)
     , get_zookeeper(get_zookeeper_)
+    , refresh_queue(std::numeric_limits<size_t>::max())
 {
     if (zookeeper_path.empty())
         throw Exception("ZooKeeper path must be non-empty", ErrorCodes::BAD_ARGUMENTS);
@@ -63,12 +64,10 @@ void ReplicatedAccessStorage::shutdown()
     bool prev_stop_flag = stop_flag.exchange(true);
     if (!prev_stop_flag)
     {
+        refresh_queue.finish();
+
         if (worker_thread.joinable())
-        {
-            /// Notify the worker thread to stop waiting for new queue items
-            refresh_queue.push(UUIDHelpers::Nil);
             worker_thread.join();
-        }
     }
 }
 
@@ -368,7 +367,7 @@ void ReplicatedAccessStorage::refreshEntities(const zkutil::ZooKeeperPtr & zooke
     const String zookeeper_uuids_path = zookeeper_path + "/uuid";
     auto watch_entities_list = [this](const Coordination::WatchResponse &)
     {
-        refresh_queue.push(UUIDHelpers::Nil);
+        [[maybe_unused]] bool push_result = refresh_queue.push(UUIDHelpers::Nil);
     };
     Coordination::Stat stat;
     const auto entity_uuid_strs = zookeeper->getChildrenWatch(zookeeper_uuids_path, &stat, watch_entities_list);
@@ -420,7 +419,7 @@ void ReplicatedAccessStorage::refreshEntityNoLock(const zkutil::ZooKeeperPtr & z
     const auto watch_entity = [this, id](const Coordination::WatchResponse & response)
     {
         if (response.type == Coordination::Event::CHANGED)
-            refresh_queue.push(id);
+            [[maybe_unused]] bool push_result = refresh_queue.push(id);
     };
     Coordination::Stat entity_stat;
     const String entity_path = zookeeper_path + "/uuid/" + toString(id);
diff --git a/src/Access/ReplicatedAccessStorage.h b/src/Access/ReplicatedAccessStorage.h
index 0df1d5ef5ff1..458bc0d614b8 100644
--- a/src/Access/ReplicatedAccessStorage.h
+++ b/src/Access/ReplicatedAccessStorage.h
@@ -1,17 +1,20 @@
 #pragma once
 
-#include <Access/IAccessStorage.h>
-#include <Common/ThreadPool.h>
-#include <Common/ZooKeeper/Common.h>
-#include <Common/ZooKeeper/ZooKeeper.h>
-#include <base/scope_guard.h>
-#include <Coordination/ThreadSafeQueue.h>
 #include <atomic>
 #include <list>
 #include <memory>
 #include <mutex>
 #include <unordered_map>
 
+#include <base/scope_guard.h>
+
+#include <Common/ThreadPool.h>
+#include <Common/ZooKeeper/Common.h>
+#include <Common/ZooKeeper/ZooKeeper.h>
+#include <Common/ConcurrentBoundedQueue.h>
+
+#include <Access/IAccessStorage.h>
+
 
 namespace DB
 {
@@ -36,7 +39,7 @@ class ReplicatedAccessStorage : public IAccessStorage
     std::atomic<bool> initialized = false;
     std::atomic<bool> stop_flag = false;
     ThreadFromGlobalPool worker_thread;
-    ThreadSafeQueue<UUID> refresh_queue;
+    ConcurrentBoundedQueue<UUID> refresh_queue;
 
     UUID insertImpl(const AccessEntityPtr & entity, bool replace_if_exists) override;
     void removeImpl(const UUID & id) override;
diff --git a/src/Common/ConcurrentBoundedQueue.h b/src/Common/ConcurrentBoundedQueue.h
index 240c433f923d..4e422f7482eb 100644
--- a/src/Common/ConcurrentBoundedQueue.h
+++ b/src/Common/ConcurrentBoundedQueue.h
@@ -3,153 +3,217 @@
 #include <queue>
 #include <type_traits>
 #include <atomic>
-
-#include <Poco/Mutex.h>
-#include <Poco/Semaphore.h>
+#include <condition_variable>
+#include <mutex>
+#include <optional>
 
 #include <base/MoveOrCopyIfThrow.h>
-#include <Common/Exception.h>
 
-namespace DB
-{
-namespace ErrorCodes
-{
-    extern const int LOGICAL_ERROR;
-}
-}
 
 /** A very simple thread-safe queue of limited size.
-  * If you try to pop an item from an empty queue, the thread is blocked until the queue becomes nonempty.
-  * If you try to push an element into an overflowed queue, the thread is blocked until space appears in the queue.
+  * If you try to pop an item from an empty queue, the thread is blocked until the queue becomes nonempty or queue is finished.
+  * If you try to push an element into an overflowed queue, the thread is blocked until space appears in the queue or queue is finished.
   */
 template <typename T>
 class ConcurrentBoundedQueue
 {
 private:
     std::queue<T> queue;
-    mutable Poco::FastMutex mutex;
-    Poco::Semaphore fill_count;
-    Poco::Semaphore empty_count;
-    std::atomic_bool closed = false;
 
-    template <typename... Args>
-    bool tryEmplaceImpl(Args &&... args)
-    {
-        bool emplaced = true;
+    mutable std::mutex queue_mutex;
+    std::condition_variable push_condition;
+    std::condition_variable pop_condition;
+
+    bool is_finished = false;
 
+    size_t max_fill = 0;
+
+    template <typename ... Args>
+    bool emplaceImpl(std::optional<UInt64> timeout_milliseconds, Args &&...args)
+    {
         {
-            Poco::ScopedLock<Poco::FastMutex> lock(mutex);
-            if (closed)
-                emplaced = false;
+            std::unique_lock<std::mutex> queue_lock(queue_mutex);
+
+            auto predicate = [&]() { return is_finished || queue.size() < max_fill; };
+
+            if (timeout_milliseconds.has_value())
+            {
+                bool wait_result = push_condition.wait_for(queue_lock, std::chrono::milliseconds(timeout_milliseconds.value()), predicate);
+
+                if (!wait_result)
+                    return false;
+            }
             else
-                queue.emplace(std::forward<Args>(args)...);
-        }
+            {
+                push_condition.wait(queue_lock, predicate);
+            }
+
+            if (is_finished)
+                return false;
 
-        if (emplaced)
-            fill_count.set();
-        else
-            empty_count.set();
+            queue.emplace(std::forward<Args>(args)...);
+        }
 
-        return emplaced;
+        pop_condition.notify_one();
+        return true;
     }
 
-    void popImpl(T & x)
+    bool popImpl(T & x, std::optional<UInt64> timeout_milliseconds)
     {
         {
-            Poco::ScopedLock<Poco::FastMutex> lock(mutex);
+            std::unique_lock<std::mutex> queue_lock(queue_mutex);
+
+            auto predicate = [&]() { return is_finished || !queue.empty(); };
+
+            if (timeout_milliseconds.has_value())
+            {
+                bool wait_result = pop_condition.wait_for(queue_lock, std::chrono::milliseconds(timeout_milliseconds.value()), predicate);
+
+                if (!wait_result)
+                    return false;
+            }
+            else
+            {
+                pop_condition.wait(queue_lock, predicate);
+            }
+
+            if (is_finished && queue.empty())
+                return false;
+
             detail::moveOrCopyIfThrow(std::move(queue.front()), x);
             queue.pop();
         }
-        empty_count.set();
+
+        push_condition.notify_one();
+        return true;
     }
 
 public:
-    explicit ConcurrentBoundedQueue(size_t max_fill)
-        : fill_count(0, max_fill)
-        , empty_count(max_fill, max_fill)
+
+    explicit ConcurrentBoundedQueue(size_t max_fill_)
+        : max_fill(max_fill_)
     {}
 
-    void push(const T & x)
+    /// Returns false if queue is finished
+    [[nodiscard]] bool push(const T & x)
     {
-        empty_count.wait();
-        if (!tryEmplaceImpl(x))
-            throw DB::Exception(DB::ErrorCodes::LOGICAL_ERROR, "tryPush/tryEmplace must be used with close()");
+        return emplace(x);
     }
 
+    /// Returns false if queue is finished
     template <typename... Args>
-    void emplace(Args &&... args)
+    [[nodiscard]] bool emplace(Args &&... args)
     {
-        empty_count.wait();
-        if (!tryEmplaceImpl(std::forward<Args>(args)...))
-            throw DB::Exception(DB::ErrorCodes::LOGICAL_ERROR, "tryPush/tryEmplace must be used with close()");
+        emplaceImpl(std::nullopt /* timeout in milliseconds */, std::forward<Args...>(args...));
+        return true;
     }
 
-    void pop(T & x)
+    /// Returns false if queue is finished and empty
+    [[nodiscard]] bool pop(T & x)
     {
-        fill_count.wait();
-        popImpl(x);
+        return popImpl(x, std::nullopt /*timeout in milliseconds*/);
     }
 
-    bool tryPush(const T & x, UInt64 milliseconds = 0)
+    /// Returns false if queue is finished or object was not pushed during timeout
+    [[nodiscard]] bool tryPush(const T & x, UInt64 milliseconds = 0)
     {
-        if (!empty_count.tryWait(milliseconds))
-            return false;
-
-        return tryEmplaceImpl(x);
+        return emplaceImpl(milliseconds, x);
     }
 
+    /// Returns false if queue is finished or object was not emplaced during timeout
     template <typename... Args>
-    bool tryEmplace(UInt64 milliseconds, Args &&... args)
+    [[nodiscard]] bool tryEmplace(UInt64 milliseconds, Args &&... args)
     {
-        if (!empty_count.tryWait(milliseconds))
-            return false;
-
-        return tryEmplaceImpl(std::forward<Args>(args)...);
+        return emplaceImpl(milliseconds, std::forward<Args...>(args...));
     }
 
-    bool tryPop(T & x, UInt64 milliseconds = 0)
+    /// Returns false if queue is (finished and empty) or (object was not popped during timeout)
+    [[nodiscard]] bool tryPop(T & x, UInt64 milliseconds = 0)
     {
-        if (!fill_count.tryWait(milliseconds))
-            return false;
-
-        popImpl(x);
-        return true;
+        return popImpl(x, milliseconds);
     }
 
+    /// Returns size of queue
     size_t size() const
     {
-        Poco::ScopedLock<Poco::FastMutex> lock(mutex);
+        std::lock_guard<std::mutex> lock(queue_mutex);
         return queue.size();
     }
 
-    size_t empty() const
+    /// Returns if queue is empty
+    bool empty() const
     {
-        Poco::ScopedLock<Poco::FastMutex> lock(mutex);
+        std::lock_guard<std::mutex> lock(queue_mutex);
         return queue.empty();
     }
 
-    /// Forbids to push new elements to queue.
-    /// Returns false if queue was not closed before call, returns true if queue was already closed.
-    bool close()
+    /** Clear and finish queue
+      * After that push operation will return false
+      * pop operations will return values until queue become empty
+      * Returns true if queue was already finished
+      */
+    bool finish()
     {
-        Poco::ScopedLock<Poco::FastMutex> lock(mutex);
-        return closed.exchange(true);
+        bool was_finished_before = false;
+
+        {
+            std::lock_guard<std::mutex> lock(queue_mutex);
+
+            if (is_finished)
+                return true;
+
+            was_finished_before = is_finished;
+            is_finished = true;
+        }
+
+        pop_condition.notify_all();
+        push_condition.notify_all();
+
+        return was_finished_before;
     }
 
-    bool isClosed() const
+    /// Returns if queue is finished
+    bool isFinished() const
     {
-        return closed.load();
+        std::lock_guard<std::mutex> lock(queue_mutex);
+        return is_finished;
     }
 
+    /// Returns if queue is finished and empty
+    bool isFinishedAndEmpty() const
+    {
+        std::lock_guard<std::mutex> lock(queue_mutex);
+        return is_finished && queue.empty();
+    }
+
+    /// Clear queue
     void clear()
     {
-        while (fill_count.tryWait(0))
         {
-            {
-                Poco::ScopedLock<Poco::FastMutex> lock(mutex);
-                queue.pop();
-            }
-            empty_count.set();
+            std::lock_guard<std::mutex> lock(queue_mutex);
+
+            if (is_finished)
+                return;
+
+            std::queue<T> empty_queue;
+            queue.swap(empty_queue);
+        }
+
+        push_condition.notify_all();
+    }
+
+    /// Clear and finish queue
+    void clearAndFinish()
+    {
+        {
+            std::lock_guard<std::mutex> lock(queue_mutex);
+
+            std::queue<T> empty_queue;
+            queue.swap(empty_queue);
+            is_finished = true;
         }
+
+        pop_condition.notify_all();
+        push_condition.notify_all();
     }
 };
diff --git a/src/Common/ZooKeeper/ZooKeeperImpl.cpp b/src/Common/ZooKeeper/ZooKeeperImpl.cpp
index 59ed906db156..63b4a61b8663 100644
--- a/src/Common/ZooKeeper/ZooKeeperImpl.cpp
+++ b/src/Common/ZooKeeper/ZooKeeperImpl.cpp
@@ -545,7 +545,7 @@ void ZooKeeper::sendThread()
 
     try
     {
-        while (!requests_queue.isClosed())
+        while (!requests_queue.isFinished())
         {
             auto prev_bytes_sent = out->count();
 
@@ -577,7 +577,7 @@ void ZooKeeper::sendThread()
                         info.request->has_watch = true;
                     }
 
-                    if (requests_queue.isClosed())
+                    if (requests_queue.isFinished())
                     {
                         break;
                     }
@@ -622,7 +622,7 @@ void ZooKeeper::receiveThread()
     try
     {
         Int64 waited = 0;
-        while (!requests_queue.isClosed())
+        while (!requests_queue.isFinished())
         {
             auto prev_bytes_received = in->count();
 
@@ -645,7 +645,7 @@ void ZooKeeper::receiveThread()
 
             if (in->poll(max_wait))
             {
-                if (requests_queue.isClosed())
+                if (requests_queue.isFinished())
                     break;
 
                 receiveEvent();
@@ -842,17 +842,17 @@ void ZooKeeper::finalize(bool error_send, bool error_receive, const String & rea
     /// If some thread (send/receive) already finalizing session don't try to do it
     bool already_started = finalization_started.exchange(true);
 
-    LOG_TEST(log, "Finalizing session {}: finalization_started={}, queue_closed={}, reason={}",
-             session_id, already_started, requests_queue.isClosed(), reason);
+    LOG_TEST(log, "Finalizing session {}: finalization_started={}, queue_finished={}, reason={}",
+             session_id, already_started, requests_queue.isFinished(), reason);
 
     if (already_started)
         return;
 
     auto expire_session_if_not_expired = [&]
     {
-        /// No new requests will appear in queue after close()
-        bool was_already_closed = requests_queue.close();
-        if (!was_already_closed)
+        /// No new requests will appear in queue after finish()
+        bool was_already_finished = requests_queue.finish();
+        if (!was_already_finished)
             active_session_metric_increment.destroy();
     };
 
@@ -1026,13 +1026,11 @@ void ZooKeeper::pushRequest(RequestInfo && info)
             }
         }
 
-        if (requests_queue.isClosed())
-            throw Exception("Session expired", Error::ZSESSIONEXPIRED);
-
         if (!requests_queue.tryPush(std::move(info), operation_timeout.totalMilliseconds()))
         {
-            if (requests_queue.isClosed())
+            if (requests_queue.isFinished())
                 throw Exception("Session expired", Error::ZSESSIONEXPIRED);
+
             throw Exception("Cannot push request to queue within operation timeout", Error::ZOPERATIONTIMEOUT);
         }
     }
diff --git a/src/Common/ZooKeeper/ZooKeeperImpl.h b/src/Common/ZooKeeper/ZooKeeperImpl.h
index 74c0148e7b6d..b87469bd3399 100644
--- a/src/Common/ZooKeeper/ZooKeeperImpl.h
+++ b/src/Common/ZooKeeper/ZooKeeperImpl.h
@@ -121,7 +121,7 @@ class ZooKeeper final : public IKeeper
 
 
     /// If expired, you can only destroy the object. All other methods will throw exception.
-    bool isExpired() const override { return requests_queue.isClosed(); }
+    bool isExpired() const override { return requests_queue.isFinished(); }
 
     /// Useful to check owner of ephemeral node.
     int64_t getSessionID() const override { return session_id; }
diff --git a/src/Coordination/KeeperDispatcher.cpp b/src/Coordination/KeeperDispatcher.cpp
index b4dc367ff626..a28e8d969152 100644
--- a/src/Coordination/KeeperDispatcher.cpp
+++ b/src/Coordination/KeeperDispatcher.cpp
@@ -11,10 +11,12 @@ namespace ErrorCodes
 {
     extern const int LOGICAL_ERROR;
     extern const int TIMEOUT_EXCEEDED;
+    extern const int SYSTEM_ERROR;
 }
 
 KeeperDispatcher::KeeperDispatcher()
     : coordination_settings(std::make_shared<CoordinationSettings>())
+    , responses_queue(std::numeric_limits<size_t>::max())
     , log(&Poco::Logger::get("KeeperDispatcher"))
 {
 }
@@ -164,7 +166,8 @@ void KeeperDispatcher::snapshotThread()
     while (!shutdown_called)
     {
         CreateSnapshotTask task;
-        snapshots_queue.pop(task);
+        if (!snapshots_queue.pop(task))
+            break;
 
         if (shutdown_called)
             break;
@@ -235,9 +238,15 @@ bool KeeperDispatcher::putRequest(const Coordination::ZooKeeperRequestPtr & requ
 
     /// Put close requests without timeouts
     if (request->getOpNum() == Coordination::OpNum::Close)
-        requests_queue->push(std::move(request_info));
+    {
+        if (!requests_queue->push(std::move(request_info)))
+            throw Exception("Cannot push request to queue", ErrorCodes::SYSTEM_ERROR);
+    }
     else if (!requests_queue->tryPush(std::move(request_info), coordination_settings->operation_timeout_ms.totalMilliseconds()))
+    {
         throw Exception("Cannot push request to queue within operation timeout", ErrorCodes::TIMEOUT_EXCEEDED);
+    }
+
     return true;
 }
 
@@ -295,16 +304,17 @@ void KeeperDispatcher::shutdown()
 
             if (requests_queue)
             {
-                requests_queue->push({});
+                requests_queue->finish();
+
                 if (request_thread.joinable())
                     request_thread.join();
             }
 
-            responses_queue.push({});
+            responses_queue.finish();
             if (responses_thread.joinable())
                 responses_thread.join();
 
-            snapshots_queue.push({});
+            snapshots_queue.finish();
             if (snapshot_thread.joinable())
                 snapshot_thread.join();
         }
@@ -317,16 +327,9 @@ void KeeperDispatcher::shutdown()
         /// Set session expired for all pending requests
         while (requests_queue && requests_queue->tryPop(request_for_session))
         {
-            if (request_for_session.request)
-            {
-                auto response = request_for_session.request->makeResponse();
-                response->error = Coordination::Error::ZSESSIONEXPIRED;
-                setResponse(request_for_session.session_id, response);
-            }
-            else
-            {
-                break;
-            }
+            auto response = request_for_session.request->makeResponse();
+            response->error = Coordination::Error::ZSESSIONEXPIRED;
+            setResponse(request_for_session.session_id, response);
         }
 
         /// Clear all registered sessions
@@ -379,7 +382,8 @@ void KeeperDispatcher::sessionCleanerTask()
                     request_info.session_id = dead_session;
                     {
                         std::lock_guard lock(push_request_mutex);
-                        requests_queue->push(std::move(request_info));
+                        if (!requests_queue->push(std::move(request_info)))
+                            LOG_INFO(log, "Cannot push close request to queue while cleaning outdated sessions");
                     }
 
                     /// Remove session from registered sessions
@@ -414,7 +418,12 @@ void KeeperDispatcher::addErrorResponses(const KeeperStorage::RequestsForSession
         response->xid = request->xid;
         response->zxid = 0;
         response->error = error;
-        responses_queue.push(DB::KeeperStorage::ResponseForSession{session_id, response});
+        if (!responses_queue.push(DB::KeeperStorage::ResponseForSession{session_id, response}))
+            throw Exception(ErrorCodes::SYSTEM_ERROR,
+                "Could not push error response xid {} zxid {} error message {} to responses queue",
+                response->xid,
+                response->zxid,
+                errorMessage(error));
     }
 }
 
diff --git a/src/Coordination/KeeperStateMachine.cpp b/src/Coordination/KeeperStateMachine.cpp
index 682a523fcafd..fe64f8afde70 100644
--- a/src/Coordination/KeeperStateMachine.cpp
+++ b/src/Coordination/KeeperStateMachine.cpp
@@ -12,6 +12,7 @@ namespace DB
 namespace ErrorCodes
 {
     extern const int LOGICAL_ERROR;
+    extern const int SYSTEM_ERROR;
 }
 
 namespace
@@ -120,7 +121,8 @@ nuraft::ptr<nuraft::buffer> KeeperStateMachine::commit(const uint64_t log_idx, n
             session_id = storage->getSessionID(session_id_request.session_timeout_ms);
             LOG_DEBUG(log, "Session ID response {} with timeout {}", session_id, session_id_request.session_timeout_ms);
             response->session_id = session_id;
-            responses_queue.push(response_for_session);
+            if (!responses_queue.push(response_for_session))
+                throw Exception(ErrorCodes::SYSTEM_ERROR, "Could not push response with session id {} into responses queue", session_id);
         }
     }
     else
@@ -128,7 +130,8 @@ nuraft::ptr<nuraft::buffer> KeeperStateMachine::commit(const uint64_t log_idx, n
         std::lock_guard lock(storage_and_responses_lock);
         KeeperStorage::ResponsesForSessions responses_for_sessions = storage->processRequest(request_for_session.request, request_for_session.session_id, log_idx);
         for (auto & response_for_session : responses_for_sessions)
-            responses_queue.push(response_for_session);
+            if (!responses_queue.push(response_for_session))
+                throw Exception(ErrorCodes::SYSTEM_ERROR, "Could not push response with session id {} into responses queue", response_for_session.session_id);
     }
 
     last_committed_idx = log_idx;
@@ -218,7 +221,8 @@ void KeeperStateMachine::create_snapshot(
 
     LOG_DEBUG(log, "In memory snapshot {} created, queueing task to flash to disk", s.get_last_log_idx());
     /// Flush snapshot to disk in a separate thread.
-    snapshots_queue.push(std::move(snapshot_task));
+    if (!snapshots_queue.push(std::move(snapshot_task)))
+        LOG_WARNING(log, "Cannot push snapshot task into queue");
 }
 
 void KeeperStateMachine::save_logical_snp_obj(
@@ -304,7 +308,8 @@ void KeeperStateMachine::processReadRequest(const KeeperStorage::RequestForSessi
     std::lock_guard lock(storage_and_responses_lock);
     auto responses = storage->processRequest(request_for_session.request, request_for_session.session_id, std::nullopt);
     for (const auto & response : responses)
-        responses_queue.push(response);
+        if (!responses_queue.push(response))
+            throw Exception(ErrorCodes::SYSTEM_ERROR, "Could not push response with session id {} into responses queue", response.session_id);
 }
 
 std::vector<int64_t> KeeperStateMachine::getDeadSessions()
diff --git a/src/Coordination/KeeperStateMachine.h b/src/Coordination/KeeperStateMachine.h
index fcf9c7d14c4b..983692f7b7f1 100644
--- a/src/Coordination/KeeperStateMachine.h
+++ b/src/Coordination/KeeperStateMachine.h
@@ -1,16 +1,17 @@
 #pragma once
 
+#include <Common/ConcurrentBoundedQueue.h>
 #include <Coordination/KeeperStorage.h>
 #include <libnuraft/nuraft.hxx> // Y_IGNORE
 #include <base/logger_useful.h>
-#include <Coordination/ThreadSafeQueue.h>
 #include <Coordination/CoordinationSettings.h>
 #include <Coordination/KeeperSnapshotManager.h>
 
+
 namespace DB
 {
 
-using ResponsesQueue = ThreadSafeQueue<KeeperStorage::ResponseForSession>;
+using ResponsesQueue = ConcurrentBoundedQueue<KeeperStorage::ResponseForSession>;
 using SnapshotsQueue = ConcurrentBoundedQueue<CreateSnapshotTask>;
 
 /// ClickHouse Keeper state machine. Wrapper for KeeperStorage.
diff --git a/src/Coordination/ThreadSafeQueue.h b/src/Coordination/ThreadSafeQueue.h
deleted file mode 100644
index d36e25244bb4..000000000000
--- a/src/Coordination/ThreadSafeQueue.h
+++ /dev/null
@@ -1,45 +0,0 @@
-#pragma once
-
-#include <queue>
-#include <mutex>
-
-namespace DB
-{
-
-/// Queue with mutex and condvar. As simple as possible.
-template <typename T>
-class ThreadSafeQueue
-{
-private:
-    mutable std::mutex queue_mutex;
-    std::condition_variable cv;
-    std::queue<T> queue;
-public:
-
-    void push(const T & response)
-    {
-        std::lock_guard lock(queue_mutex);
-        queue.push(response);
-        cv.notify_one();
-    }
-
-    bool tryPop(T & response, int64_t timeout_ms = 0)
-    {
-        std::unique_lock lock(queue_mutex);
-        if (!cv.wait_for(lock,
-                std::chrono::milliseconds(timeout_ms), [this] { return !queue.empty(); }))
-            return false;
-
-        response = queue.front();
-        queue.pop();
-        return true;
-    }
-
-    size_t size() const
-    {
-        std::lock_guard lock(queue_mutex);
-        return queue.size();
-    }
-};
-
-}
diff --git a/src/DataStreams/RemoteQueryExecutor.cpp b/src/DataStreams/RemoteQueryExecutor.cpp
index 08d3db748b7e..b6a5e6f63d00 100644
--- a/src/DataStreams/RemoteQueryExecutor.cpp
+++ b/src/DataStreams/RemoteQueryExecutor.cpp
@@ -35,6 +35,7 @@ namespace ErrorCodes
     extern const int LOGICAL_ERROR;
     extern const int UNKNOWN_PACKET_FROM_SERVER;
     extern const int DUPLICATED_PART_UUIDS;
+    extern const int SYSTEM_ERROR;
 }
 
 RemoteQueryExecutor::RemoteQueryExecutor(
@@ -396,7 +397,8 @@ std::optional<Block> RemoteQueryExecutor::processPacket(Packet packet)
         case Protocol::Server::ProfileEvents:
             /// Pass profile events from remote server to client
             if (auto profile_queue = CurrentThread::getInternalProfileEventsQueue())
-                profile_queue->emplace(std::move(packet.block));
+                if (!profile_queue->emplace(std::move(packet.block)))
+                    throw Exception(ErrorCodes::SYSTEM_ERROR, "Could not push into profile queue");
             break;
 
         default:
diff --git a/src/Dictionaries/CacheDictionaryUpdateQueue.cpp b/src/Dictionaries/CacheDictionaryUpdateQueue.cpp
index 1d96fcc108b6..2077f846f093 100644
--- a/src/Dictionaries/CacheDictionaryUpdateQueue.cpp
+++ b/src/Dictionaries/CacheDictionaryUpdateQueue.cpp
@@ -35,9 +35,11 @@ CacheDictionaryUpdateQueue<dictionary_key_type>::CacheDictionaryUpdateQueue(
 template <DictionaryKeyType dictionary_key_type>
 CacheDictionaryUpdateQueue<dictionary_key_type>::~CacheDictionaryUpdateQueue()
 {
+    if (update_queue.isFinished())
+        return;
+
     try {
-        if (!finished)
-            stopAndWait();
+        stopAndWait();
     }
     catch (...)
     {
@@ -48,7 +50,7 @@ CacheDictionaryUpdateQueue<dictionary_key_type>::~CacheDictionaryUpdateQueue()
 template <DictionaryKeyType dictionary_key_type>
 void CacheDictionaryUpdateQueue<dictionary_key_type>::tryPushToUpdateQueueOrThrow(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr)
 {
-    if (finished)
+    if (update_queue.isFinished())
         throw Exception(ErrorCodes::UNSUPPORTED_METHOD, "CacheDictionaryUpdateQueue finished");
 
     if (!update_queue.tryPush(update_unit_ptr, configuration.update_queue_push_timeout_milliseconds))
@@ -63,7 +65,7 @@ void CacheDictionaryUpdateQueue<dictionary_key_type>::tryPushToUpdateQueueOrThro
 template <DictionaryKeyType dictionary_key_type>
 void CacheDictionaryUpdateQueue<dictionary_key_type>::waitForCurrentUpdateFinish(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr) const
 {
-    if (finished)
+    if (update_queue.isFinished())
         throw Exception(ErrorCodes::UNSUPPORTED_METHOD, "CacheDictionaryUpdateQueue finished");
 
     std::unique_lock<std::mutex> update_lock(update_mutex);
@@ -108,15 +110,10 @@ void CacheDictionaryUpdateQueue<dictionary_key_type>::waitForCurrentUpdateFinish
 template <DictionaryKeyType dictionary_key_type>
 void CacheDictionaryUpdateQueue<dictionary_key_type>::stopAndWait()
 {
-    finished = true;
-    update_queue.clear();
-
-    for (size_t i = 0; i < configuration.max_threads_for_updates; ++i)
-    {
-        auto empty_finishing_ptr = std::make_shared<CacheDictionaryUpdateUnit<dictionary_key_type>>();
-        update_queue.push(empty_finishing_ptr);
-    }
+    if (update_queue.isFinished())
+        throw Exception(ErrorCodes::UNSUPPORTED_METHOD, "CacheDictionaryUpdateQueue finished");
 
+    update_queue.clearAndFinish();
     update_pool.wait();
 }
 
@@ -125,12 +122,10 @@ void CacheDictionaryUpdateQueue<dictionary_key_type>::updateThreadFunction()
 {
     setThreadName("UpdQueue");
 
-    while (!finished)
+    while (!update_queue.isFinished())
     {
         CacheDictionaryUpdateUnitPtr<dictionary_key_type> unit_to_update;
-        update_queue.pop(unit_to_update);
-
-        if (finished)
+        if (!update_queue.pop(unit_to_update))
             break;
 
         try
diff --git a/src/Dictionaries/CacheDictionaryUpdateQueue.h b/src/Dictionaries/CacheDictionaryUpdateQueue.h
index bcad376bc53c..7725ce7588f2 100644
--- a/src/Dictionaries/CacheDictionaryUpdateQueue.h
+++ b/src/Dictionaries/CacheDictionaryUpdateQueue.h
@@ -122,7 +122,7 @@ class CacheDictionaryUpdateQueue
     const CacheDictionaryUpdateQueueConfiguration & getConfiguration() const { return configuration; }
 
     /// Is queue finished
-    bool isFinished() const { return finished; }
+    bool isFinished() const { return update_queue.isFinished(); }
 
     /// Synchronous wait for update queue to stop
     void stopAndWait();
@@ -162,8 +162,6 @@ class CacheDictionaryUpdateQueue
 
     mutable std::mutex update_mutex;
     mutable std::condition_variable is_update_finished;
-
-    std::atomic<bool> finished{false};
 };
 
 extern template class CacheDictionaryUpdateQueue<DictionaryKeyType::Simple>;
diff --git a/src/Interpreters/InternalTextLogsQueue.cpp b/src/Interpreters/InternalTextLogsQueue.cpp
index 8329fc38ba7d..e541e1c76ccb 100644
--- a/src/Interpreters/InternalTextLogsQueue.cpp
+++ b/src/Interpreters/InternalTextLogsQueue.cpp
@@ -41,7 +41,7 @@ void InternalTextLogsQueue::pushBlock(Block && log_block)
     static Block sample_block = getSampleBlock();
 
     if (blocksHaveEqualStructure(sample_block, log_block))
-        emplace(log_block.mutateColumns());
+        (void)(emplace(log_block.mutateColumns()));
     else
         LOG_WARNING(&Poco::Logger::get("InternalTextLogsQueue"), "Log block have different structure");
 }
diff --git a/src/Processors/Formats/LazyOutputFormat.cpp b/src/Processors/Formats/LazyOutputFormat.cpp
index 0663ff28f84e..792d805eac3b 100644
--- a/src/Processors/Formats/LazyOutputFormat.cpp
+++ b/src/Processors/Formats/LazyOutputFormat.cpp
@@ -9,11 +9,8 @@ WriteBuffer LazyOutputFormat::out(nullptr, 0);
 
 Chunk LazyOutputFormat::getChunk(UInt64 milliseconds)
 {
-    if (finished_processing)
-    {
-        if (queue.empty())
-            return {};
-    }
+    if (isFinished())
+        return {};
 
     Chunk chunk;
     if (milliseconds)
@@ -22,7 +19,10 @@ Chunk LazyOutputFormat::getChunk(UInt64 milliseconds)
             return {};
     }
     else
-        queue.pop(chunk);
+    {
+        if (!queue.pop(chunk))
+            return {};
+    }
 
     if (chunk)
         info.update(chunk.getNumRows(), chunk.allocatedBytes());
diff --git a/src/Processors/Formats/LazyOutputFormat.h b/src/Processors/Formats/LazyOutputFormat.h
index 2c29f55c4f37..50dc87f2e70a 100644
--- a/src/Processors/Formats/LazyOutputFormat.h
+++ b/src/Processors/Formats/LazyOutputFormat.h
@@ -15,7 +15,7 @@ class LazyOutputFormat : public IOutputFormat
 
 public:
     explicit LazyOutputFormat(const Block & header)
-        : IOutputFormat(header, out), queue(2), finished_processing(false) {}
+        : IOutputFormat(header, out), queue(2) {}
 
     String getName() const override { return "LazyOutputFormat"; }
 
@@ -23,7 +23,7 @@ class LazyOutputFormat : public IOutputFormat
     Chunk getTotals();
     Chunk getExtremes();
 
-    bool isFinished() { return finished_processing && queue.size() == 0; }
+    bool isFinished() { return queue.isFinishedAndEmpty(); }
 
     BlockStreamProfileInfo & getProfileInfo() { return info; }
 
@@ -31,17 +31,12 @@ class LazyOutputFormat : public IOutputFormat
 
     void onCancel() override
     {
-        finished_processing = true;
-        /// Clear queue in case if somebody is waiting lazy_format to push.
-        queue.clear();
+        queue.clearAndFinish();
     }
 
     void finalize() override
     {
-        finished_processing = true;
-
-        /// In case we are waiting for result.
-        queue.emplace(Chunk());
+        queue.finish();
     }
 
     bool expectMaterializedColumns() const override { return false; }
@@ -49,8 +44,7 @@ class LazyOutputFormat : public IOutputFormat
 protected:
     void consume(Chunk chunk) override
     {
-        if (!finished_processing)
-            queue.emplace(std::move(chunk));
+        (void)(queue.emplace(std::move(chunk)));
     }
 
     void consumeTotals(Chunk chunk) override { totals = std::move(chunk); }
@@ -66,8 +60,6 @@ class LazyOutputFormat : public IOutputFormat
     static WriteBuffer out;
 
     BlockStreamProfileInfo info;
-
-    std::atomic<bool> finished_processing;
 };
 
 }
diff --git a/src/Server/KeeperTCPHandler.cpp b/src/Server/KeeperTCPHandler.cpp
index 7ebbda9dfe6e..b19b02f960d7 100644
--- a/src/Server/KeeperTCPHandler.cpp
+++ b/src/Server/KeeperTCPHandler.cpp
@@ -198,7 +198,7 @@ KeeperTCPHandler::KeeperTCPHandler(IServer & server_, const Poco::Net::StreamSoc
     , operation_timeout(0, global_context->getConfigRef().getUInt("keeper_server.operation_timeout_ms", Coordination::DEFAULT_OPERATION_TIMEOUT_MS) * 1000)
     , session_timeout(0, global_context->getConfigRef().getUInt("keeper_server.session_timeout_ms", Coordination::DEFAULT_SESSION_TIMEOUT_MS) * 1000)
     , poll_wrapper(std::make_unique<SocketInterruptablePollWrapper>(socket_))
-    , responses(std::make_unique<ThreadSafeResponseQueue>())
+    , responses(std::make_unique<ThreadSafeResponseQueue>(std::numeric_limits<size_t>::max()))
 {
 }
 
@@ -314,7 +314,12 @@ void KeeperTCPHandler::runImpl()
     auto response_fd = poll_wrapper->getResponseFD();
     auto response_callback = [this, response_fd] (const Coordination::ZooKeeperResponsePtr & response)
     {
-        responses->push(response);
+        if (!responses->push(response))
+            throw Exception(ErrorCodes::SYSTEM_ERROR,
+                "Could not push response with xid {} and zxid {}",
+                response->xid,
+                response->zxid);
+
         UInt8 single_byte = 1;
         [[maybe_unused]] int result = write(response_fd, &single_byte, sizeof(single_byte));
     };
diff --git a/src/Server/KeeperTCPHandler.h b/src/Server/KeeperTCPHandler.h
index 7abfb72c8464..274fb21af636 100644
--- a/src/Server/KeeperTCPHandler.h
+++ b/src/Server/KeeperTCPHandler.h
@@ -13,10 +13,10 @@
 #include <Interpreters/Context.h>
 #include <Common/ZooKeeper/ZooKeeperCommon.h>
 #include <Common/ZooKeeper/ZooKeeperConstants.h>
+#include <Common/ConcurrentBoundedQueue.h>
 #include <Coordination/KeeperDispatcher.h>
 #include <IO/WriteBufferFromPocoSocket.h>
 #include <IO/ReadBufferFromPocoSocket.h>
-#include <Coordination/ThreadSafeQueue.h>
 #include <unordered_map>
 
 namespace DB
@@ -25,7 +25,7 @@ namespace DB
 struct SocketInterruptablePollWrapper;
 using SocketInterruptablePollWrapperPtr = std::unique_ptr<SocketInterruptablePollWrapper>;
 
-using ThreadSafeResponseQueue = ThreadSafeQueue<Coordination::ZooKeeperResponsePtr>;
+using ThreadSafeResponseQueue = ConcurrentBoundedQueue<Coordination::ZooKeeperResponsePtr>;
 
 using ThreadSafeResponseQueuePtr = std::unique_ptr<ThreadSafeResponseQueue>;
 
diff --git a/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp b/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp
index 2e2807d8297c..ac60d748e366 100644
--- a/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp
+++ b/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp
@@ -14,6 +14,11 @@
 namespace DB
 {
 
+namespace ErrorCodes
+{
+    extern const int LOGICAL_ERROR;
+}
+
 ReadBufferFromRabbitMQConsumer::ReadBufferFromRabbitMQConsumer(
         ChannelPtr consumer_channel_,
         RabbitMQHandler & event_handler_,
@@ -64,9 +69,10 @@ void ReadBufferFromRabbitMQConsumer::subscribe()
                 if (row_delimiter != '\0')
                     message_received += row_delimiter;
 
-                received.push({message_received, message.hasMessageID() ? message.messageID() : "",
+                if (!received.push({message_received, message.hasMessageID() ? message.messageID() : "",
                         message.hasTimestamp() ? message.timestamp() : 0,
-                        redelivered, AckTracker(delivery_tag, channel_id)});
+                        redelivered, AckTracker(delivery_tag, channel_id)}))
+                    throw Exception(ErrorCodes::LOGICAL_ERROR, "Could not push to received queue");
             }
         })
         .onError([&](const char * message)
diff --git a/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp b/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp
index 1929a103414d..8d891e34a64b 100644
--- a/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp
+++ b/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp
@@ -21,6 +21,7 @@ static const auto RETURNED_LIMIT = 50000;
 namespace ErrorCodes
 {
     extern const int CANNOT_CONNECT_RABBITMQ;
+    extern const int LOGICAL_ERROR;
 }
 
 WriteBufferToRabbitMQProducer::WriteBufferToRabbitMQProducer(
@@ -102,7 +103,8 @@ void WriteBufferToRabbitMQProducer::countRow()
         reinitializeChunks();
 
         ++payload_counter;
-        payloads.push(std::make_pair(payload_counter, payload));
+        if (!payloads.push(std::make_pair(payload_counter, payload)))
+            throw Exception(ErrorCodes::LOGICAL_ERROR, "Could not push to payloads queue");
     }
 }
 
@@ -122,7 +124,8 @@ void WriteBufferToRabbitMQProducer::setupChannel()
          * they are republished because after channel recovery they will acquire new delivery tags, so all previous records become invalid
          */
         for (const auto & record : delivery_record)
-            returned.tryPush(record.second);
+            if (!returned.push(record.second))
+                throw Exception(ErrorCodes::LOGICAL_ERROR, "Could not push to returned queue");
 
         LOG_DEBUG(log, "Producer on channel {} hasn't confirmed {} messages, {} waiting to be published",
                 channel_id, delivery_record.size(), payloads.size());
@@ -170,7 +173,8 @@ void WriteBufferToRabbitMQProducer::removeRecord(UInt64 received_delivery_tag, b
 
         if (republish)
             for (auto record = delivery_record.begin(); record != record_iter; ++record)
-                returned.tryPush(record->second);
+                if (!returned.push(record->second))
+                    throw Exception(ErrorCodes::LOGICAL_ERROR, "Could not push to returned queue");
 
         /// Delete the records even in case when republished because new delivery tags will be assigned by the server.
         delivery_record.erase(delivery_record.begin(), record_iter);
@@ -178,7 +182,8 @@ void WriteBufferToRabbitMQProducer::removeRecord(UInt64 received_delivery_tag, b
     else
     {
         if (republish)
-            returned.tryPush(record_iter->second);
+            if (!returned.push(record_iter->second))
+                throw Exception(ErrorCodes::LOGICAL_ERROR, "Could not push to returned queue");
 
         delivery_record.erase(record_iter);
     }
@@ -194,7 +199,11 @@ void WriteBufferToRabbitMQProducer::publish(ConcurrentBoundedQueue<std::pair<UIn
      */
     while (!messages.empty() && producer_channel->usable() && delivery_record.size() < RETURNED_LIMIT)
     {
-        messages.pop(payload);
+        bool pop_result = messages.pop(payload);
+
+        if (!pop_result)
+            throw Exception(ErrorCodes::LOGICAL_ERROR, "Could not pop payload");
+
         AMQP::Envelope envelope(payload.second.data(), payload.second.size());
 
         /// if headers exchange is used, routing keys are added here via headers, if not - it is just empty
diff --git a/utils/keeper-data-dumper/main.cpp b/utils/keeper-data-dumper/main.cpp
index b238c2ef569b..ed6a7aea9723 100644
--- a/utils/keeper-data-dumper/main.cpp
+++ b/utils/keeper-data-dumper/main.cpp
@@ -59,7 +59,7 @@ int main(int argc, char *argv[])
         Poco::Logger::root().setLevel("trace");
     }
     auto * logger = &Poco::Logger::get("keeper-dumper");
-    ResponsesQueue queue;
+    ResponsesQueue queue(std::numeric_limits<size_t>::max());
     SnapshotsQueue snapshots_queue{1};
     CoordinationSettingsPtr settings = std::make_shared<CoordinationSettings>();
     auto state_machine = std::make_shared<KeeperStateMachine>(queue, snapshots_queue, argv[1], settings);
