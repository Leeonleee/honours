{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 29801,
  "instance_id": "ClickHouse__ClickHouse-29801",
  "issue_numbers": [
    "29720"
  ],
  "base_commit": "a163230f37d82ee95b3878ed0194135796318970",
  "patch": "diff --git a/base/loggers/OwnSplitChannel.cpp b/base/loggers/OwnSplitChannel.cpp\nindex 62fdddad28c8..2349c60856fc 100644\n--- a/base/loggers/OwnSplitChannel.cpp\n+++ b/base/loggers/OwnSplitChannel.cpp\n@@ -100,7 +100,7 @@ void OwnSplitChannel::logSplit(const Poco::Message & msg)\n         columns[i++]->insert(msg.getSource());\n         columns[i++]->insert(msg.getText());\n \n-        logs_queue->emplace(std::move(columns));\n+        [[maybe_unused]] bool push_result = logs_queue->emplace(std::move(columns));\n     }\n \n     /// Also log to system.text_log table, if message is not too noisy\ndiff --git a/src/Access/ReplicatedAccessStorage.cpp b/src/Access/ReplicatedAccessStorage.cpp\nindex d50ecda0c9ee..7b29aab3a896 100644\n--- a/src/Access/ReplicatedAccessStorage.cpp\n+++ b/src/Access/ReplicatedAccessStorage.cpp\n@@ -34,6 +34,7 @@ ReplicatedAccessStorage::ReplicatedAccessStorage(\n     : IAccessStorage(storage_name_)\n     , zookeeper_path(zookeeper_path_)\n     , get_zookeeper(get_zookeeper_)\n+    , refresh_queue(std::numeric_limits<size_t>::max())\n {\n     if (zookeeper_path.empty())\n         throw Exception(\"ZooKeeper path must be non-empty\", ErrorCodes::BAD_ARGUMENTS);\n@@ -63,12 +64,10 @@ void ReplicatedAccessStorage::shutdown()\n     bool prev_stop_flag = stop_flag.exchange(true);\n     if (!prev_stop_flag)\n     {\n+        refresh_queue.finish();\n+\n         if (worker_thread.joinable())\n-        {\n-            /// Notify the worker thread to stop waiting for new queue items\n-            refresh_queue.push(UUIDHelpers::Nil);\n             worker_thread.join();\n-        }\n     }\n }\n \n@@ -368,7 +367,7 @@ void ReplicatedAccessStorage::refreshEntities(const zkutil::ZooKeeperPtr & zooke\n     const String zookeeper_uuids_path = zookeeper_path + \"/uuid\";\n     auto watch_entities_list = [this](const Coordination::WatchResponse &)\n     {\n-        refresh_queue.push(UUIDHelpers::Nil);\n+        [[maybe_unused]] bool push_result = refresh_queue.push(UUIDHelpers::Nil);\n     };\n     Coordination::Stat stat;\n     const auto entity_uuid_strs = zookeeper->getChildrenWatch(zookeeper_uuids_path, &stat, watch_entities_list);\n@@ -420,7 +419,7 @@ void ReplicatedAccessStorage::refreshEntityNoLock(const zkutil::ZooKeeperPtr & z\n     const auto watch_entity = [this, id](const Coordination::WatchResponse & response)\n     {\n         if (response.type == Coordination::Event::CHANGED)\n-            refresh_queue.push(id);\n+            [[maybe_unused]] bool push_result = refresh_queue.push(id);\n     };\n     Coordination::Stat entity_stat;\n     const String entity_path = zookeeper_path + \"/uuid/\" + toString(id);\ndiff --git a/src/Access/ReplicatedAccessStorage.h b/src/Access/ReplicatedAccessStorage.h\nindex 0df1d5ef5ff1..458bc0d614b8 100644\n--- a/src/Access/ReplicatedAccessStorage.h\n+++ b/src/Access/ReplicatedAccessStorage.h\n@@ -1,17 +1,20 @@\n #pragma once\n \n-#include <Access/IAccessStorage.h>\n-#include <Common/ThreadPool.h>\n-#include <Common/ZooKeeper/Common.h>\n-#include <Common/ZooKeeper/ZooKeeper.h>\n-#include <base/scope_guard.h>\n-#include <Coordination/ThreadSafeQueue.h>\n #include <atomic>\n #include <list>\n #include <memory>\n #include <mutex>\n #include <unordered_map>\n \n+#include <base/scope_guard.h>\n+\n+#include <Common/ThreadPool.h>\n+#include <Common/ZooKeeper/Common.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n+#include <Common/ConcurrentBoundedQueue.h>\n+\n+#include <Access/IAccessStorage.h>\n+\n \n namespace DB\n {\n@@ -36,7 +39,7 @@ class ReplicatedAccessStorage : public IAccessStorage\n     std::atomic<bool> initialized = false;\n     std::atomic<bool> stop_flag = false;\n     ThreadFromGlobalPool worker_thread;\n-    ThreadSafeQueue<UUID> refresh_queue;\n+    ConcurrentBoundedQueue<UUID> refresh_queue;\n \n     UUID insertImpl(const AccessEntityPtr & entity, bool replace_if_exists) override;\n     void removeImpl(const UUID & id) override;\ndiff --git a/src/Common/ConcurrentBoundedQueue.h b/src/Common/ConcurrentBoundedQueue.h\nindex 240c433f923d..4e422f7482eb 100644\n--- a/src/Common/ConcurrentBoundedQueue.h\n+++ b/src/Common/ConcurrentBoundedQueue.h\n@@ -3,153 +3,217 @@\n #include <queue>\n #include <type_traits>\n #include <atomic>\n-\n-#include <Poco/Mutex.h>\n-#include <Poco/Semaphore.h>\n+#include <condition_variable>\n+#include <mutex>\n+#include <optional>\n \n #include <base/MoveOrCopyIfThrow.h>\n-#include <Common/Exception.h>\n \n-namespace DB\n-{\n-namespace ErrorCodes\n-{\n-    extern const int LOGICAL_ERROR;\n-}\n-}\n \n /** A very simple thread-safe queue of limited size.\n-  * If you try to pop an item from an empty queue, the thread is blocked until the queue becomes nonempty.\n-  * If you try to push an element into an overflowed queue, the thread is blocked until space appears in the queue.\n+  * If you try to pop an item from an empty queue, the thread is blocked until the queue becomes nonempty or queue is finished.\n+  * If you try to push an element into an overflowed queue, the thread is blocked until space appears in the queue or queue is finished.\n   */\n template <typename T>\n class ConcurrentBoundedQueue\n {\n private:\n     std::queue<T> queue;\n-    mutable Poco::FastMutex mutex;\n-    Poco::Semaphore fill_count;\n-    Poco::Semaphore empty_count;\n-    std::atomic_bool closed = false;\n \n-    template <typename... Args>\n-    bool tryEmplaceImpl(Args &&... args)\n-    {\n-        bool emplaced = true;\n+    mutable std::mutex queue_mutex;\n+    std::condition_variable push_condition;\n+    std::condition_variable pop_condition;\n+\n+    bool is_finished = false;\n \n+    size_t max_fill = 0;\n+\n+    template <typename ... Args>\n+    bool emplaceImpl(std::optional<UInt64> timeout_milliseconds, Args &&...args)\n+    {\n         {\n-            Poco::ScopedLock<Poco::FastMutex> lock(mutex);\n-            if (closed)\n-                emplaced = false;\n+            std::unique_lock<std::mutex> queue_lock(queue_mutex);\n+\n+            auto predicate = [&]() { return is_finished || queue.size() < max_fill; };\n+\n+            if (timeout_milliseconds.has_value())\n+            {\n+                bool wait_result = push_condition.wait_for(queue_lock, std::chrono::milliseconds(timeout_milliseconds.value()), predicate);\n+\n+                if (!wait_result)\n+                    return false;\n+            }\n             else\n-                queue.emplace(std::forward<Args>(args)...);\n-        }\n+            {\n+                push_condition.wait(queue_lock, predicate);\n+            }\n+\n+            if (is_finished)\n+                return false;\n \n-        if (emplaced)\n-            fill_count.set();\n-        else\n-            empty_count.set();\n+            queue.emplace(std::forward<Args>(args)...);\n+        }\n \n-        return emplaced;\n+        pop_condition.notify_one();\n+        return true;\n     }\n \n-    void popImpl(T & x)\n+    bool popImpl(T & x, std::optional<UInt64> timeout_milliseconds)\n     {\n         {\n-            Poco::ScopedLock<Poco::FastMutex> lock(mutex);\n+            std::unique_lock<std::mutex> queue_lock(queue_mutex);\n+\n+            auto predicate = [&]() { return is_finished || !queue.empty(); };\n+\n+            if (timeout_milliseconds.has_value())\n+            {\n+                bool wait_result = pop_condition.wait_for(queue_lock, std::chrono::milliseconds(timeout_milliseconds.value()), predicate);\n+\n+                if (!wait_result)\n+                    return false;\n+            }\n+            else\n+            {\n+                pop_condition.wait(queue_lock, predicate);\n+            }\n+\n+            if (is_finished && queue.empty())\n+                return false;\n+\n             detail::moveOrCopyIfThrow(std::move(queue.front()), x);\n             queue.pop();\n         }\n-        empty_count.set();\n+\n+        push_condition.notify_one();\n+        return true;\n     }\n \n public:\n-    explicit ConcurrentBoundedQueue(size_t max_fill)\n-        : fill_count(0, max_fill)\n-        , empty_count(max_fill, max_fill)\n+\n+    explicit ConcurrentBoundedQueue(size_t max_fill_)\n+        : max_fill(max_fill_)\n     {}\n \n-    void push(const T & x)\n+    /// Returns false if queue is finished\n+    [[nodiscard]] bool push(const T & x)\n     {\n-        empty_count.wait();\n-        if (!tryEmplaceImpl(x))\n-            throw DB::Exception(DB::ErrorCodes::LOGICAL_ERROR, \"tryPush/tryEmplace must be used with close()\");\n+        return emplace(x);\n     }\n \n+    /// Returns false if queue is finished\n     template <typename... Args>\n-    void emplace(Args &&... args)\n+    [[nodiscard]] bool emplace(Args &&... args)\n     {\n-        empty_count.wait();\n-        if (!tryEmplaceImpl(std::forward<Args>(args)...))\n-            throw DB::Exception(DB::ErrorCodes::LOGICAL_ERROR, \"tryPush/tryEmplace must be used with close()\");\n+        emplaceImpl(std::nullopt /* timeout in milliseconds */, std::forward<Args...>(args...));\n+        return true;\n     }\n \n-    void pop(T & x)\n+    /// Returns false if queue is finished and empty\n+    [[nodiscard]] bool pop(T & x)\n     {\n-        fill_count.wait();\n-        popImpl(x);\n+        return popImpl(x, std::nullopt /*timeout in milliseconds*/);\n     }\n \n-    bool tryPush(const T & x, UInt64 milliseconds = 0)\n+    /// Returns false if queue is finished or object was not pushed during timeout\n+    [[nodiscard]] bool tryPush(const T & x, UInt64 milliseconds = 0)\n     {\n-        if (!empty_count.tryWait(milliseconds))\n-            return false;\n-\n-        return tryEmplaceImpl(x);\n+        return emplaceImpl(milliseconds, x);\n     }\n \n+    /// Returns false if queue is finished or object was not emplaced during timeout\n     template <typename... Args>\n-    bool tryEmplace(UInt64 milliseconds, Args &&... args)\n+    [[nodiscard]] bool tryEmplace(UInt64 milliseconds, Args &&... args)\n     {\n-        if (!empty_count.tryWait(milliseconds))\n-            return false;\n-\n-        return tryEmplaceImpl(std::forward<Args>(args)...);\n+        return emplaceImpl(milliseconds, std::forward<Args...>(args...));\n     }\n \n-    bool tryPop(T & x, UInt64 milliseconds = 0)\n+    /// Returns false if queue is (finished and empty) or (object was not popped during timeout)\n+    [[nodiscard]] bool tryPop(T & x, UInt64 milliseconds = 0)\n     {\n-        if (!fill_count.tryWait(milliseconds))\n-            return false;\n-\n-        popImpl(x);\n-        return true;\n+        return popImpl(x, milliseconds);\n     }\n \n+    /// Returns size of queue\n     size_t size() const\n     {\n-        Poco::ScopedLock<Poco::FastMutex> lock(mutex);\n+        std::lock_guard<std::mutex> lock(queue_mutex);\n         return queue.size();\n     }\n \n-    size_t empty() const\n+    /// Returns if queue is empty\n+    bool empty() const\n     {\n-        Poco::ScopedLock<Poco::FastMutex> lock(mutex);\n+        std::lock_guard<std::mutex> lock(queue_mutex);\n         return queue.empty();\n     }\n \n-    /// Forbids to push new elements to queue.\n-    /// Returns false if queue was not closed before call, returns true if queue was already closed.\n-    bool close()\n+    /** Clear and finish queue\n+      * After that push operation will return false\n+      * pop operations will return values until queue become empty\n+      * Returns true if queue was already finished\n+      */\n+    bool finish()\n     {\n-        Poco::ScopedLock<Poco::FastMutex> lock(mutex);\n-        return closed.exchange(true);\n+        bool was_finished_before = false;\n+\n+        {\n+            std::lock_guard<std::mutex> lock(queue_mutex);\n+\n+            if (is_finished)\n+                return true;\n+\n+            was_finished_before = is_finished;\n+            is_finished = true;\n+        }\n+\n+        pop_condition.notify_all();\n+        push_condition.notify_all();\n+\n+        return was_finished_before;\n     }\n \n-    bool isClosed() const\n+    /// Returns if queue is finished\n+    bool isFinished() const\n     {\n-        return closed.load();\n+        std::lock_guard<std::mutex> lock(queue_mutex);\n+        return is_finished;\n     }\n \n+    /// Returns if queue is finished and empty\n+    bool isFinishedAndEmpty() const\n+    {\n+        std::lock_guard<std::mutex> lock(queue_mutex);\n+        return is_finished && queue.empty();\n+    }\n+\n+    /// Clear queue\n     void clear()\n     {\n-        while (fill_count.tryWait(0))\n         {\n-            {\n-                Poco::ScopedLock<Poco::FastMutex> lock(mutex);\n-                queue.pop();\n-            }\n-            empty_count.set();\n+            std::lock_guard<std::mutex> lock(queue_mutex);\n+\n+            if (is_finished)\n+                return;\n+\n+            std::queue<T> empty_queue;\n+            queue.swap(empty_queue);\n+        }\n+\n+        push_condition.notify_all();\n+    }\n+\n+    /// Clear and finish queue\n+    void clearAndFinish()\n+    {\n+        {\n+            std::lock_guard<std::mutex> lock(queue_mutex);\n+\n+            std::queue<T> empty_queue;\n+            queue.swap(empty_queue);\n+            is_finished = true;\n         }\n+\n+        pop_condition.notify_all();\n+        push_condition.notify_all();\n     }\n };\ndiff --git a/src/Common/ZooKeeper/ZooKeeperImpl.cpp b/src/Common/ZooKeeper/ZooKeeperImpl.cpp\nindex 59ed906db156..63b4a61b8663 100644\n--- a/src/Common/ZooKeeper/ZooKeeperImpl.cpp\n+++ b/src/Common/ZooKeeper/ZooKeeperImpl.cpp\n@@ -545,7 +545,7 @@ void ZooKeeper::sendThread()\n \n     try\n     {\n-        while (!requests_queue.isClosed())\n+        while (!requests_queue.isFinished())\n         {\n             auto prev_bytes_sent = out->count();\n \n@@ -577,7 +577,7 @@ void ZooKeeper::sendThread()\n                         info.request->has_watch = true;\n                     }\n \n-                    if (requests_queue.isClosed())\n+                    if (requests_queue.isFinished())\n                     {\n                         break;\n                     }\n@@ -622,7 +622,7 @@ void ZooKeeper::receiveThread()\n     try\n     {\n         Int64 waited = 0;\n-        while (!requests_queue.isClosed())\n+        while (!requests_queue.isFinished())\n         {\n             auto prev_bytes_received = in->count();\n \n@@ -645,7 +645,7 @@ void ZooKeeper::receiveThread()\n \n             if (in->poll(max_wait))\n             {\n-                if (requests_queue.isClosed())\n+                if (requests_queue.isFinished())\n                     break;\n \n                 receiveEvent();\n@@ -842,17 +842,17 @@ void ZooKeeper::finalize(bool error_send, bool error_receive, const String & rea\n     /// If some thread (send/receive) already finalizing session don't try to do it\n     bool already_started = finalization_started.exchange(true);\n \n-    LOG_TEST(log, \"Finalizing session {}: finalization_started={}, queue_closed={}, reason={}\",\n-             session_id, already_started, requests_queue.isClosed(), reason);\n+    LOG_TEST(log, \"Finalizing session {}: finalization_started={}, queue_finished={}, reason={}\",\n+             session_id, already_started, requests_queue.isFinished(), reason);\n \n     if (already_started)\n         return;\n \n     auto expire_session_if_not_expired = [&]\n     {\n-        /// No new requests will appear in queue after close()\n-        bool was_already_closed = requests_queue.close();\n-        if (!was_already_closed)\n+        /// No new requests will appear in queue after finish()\n+        bool was_already_finished = requests_queue.finish();\n+        if (!was_already_finished)\n             active_session_metric_increment.destroy();\n     };\n \n@@ -1026,13 +1026,11 @@ void ZooKeeper::pushRequest(RequestInfo && info)\n             }\n         }\n \n-        if (requests_queue.isClosed())\n-            throw Exception(\"Session expired\", Error::ZSESSIONEXPIRED);\n-\n         if (!requests_queue.tryPush(std::move(info), operation_timeout.totalMilliseconds()))\n         {\n-            if (requests_queue.isClosed())\n+            if (requests_queue.isFinished())\n                 throw Exception(\"Session expired\", Error::ZSESSIONEXPIRED);\n+\n             throw Exception(\"Cannot push request to queue within operation timeout\", Error::ZOPERATIONTIMEOUT);\n         }\n     }\ndiff --git a/src/Common/ZooKeeper/ZooKeeperImpl.h b/src/Common/ZooKeeper/ZooKeeperImpl.h\nindex 74c0148e7b6d..b87469bd3399 100644\n--- a/src/Common/ZooKeeper/ZooKeeperImpl.h\n+++ b/src/Common/ZooKeeper/ZooKeeperImpl.h\n@@ -121,7 +121,7 @@ class ZooKeeper final : public IKeeper\n \n \n     /// If expired, you can only destroy the object. All other methods will throw exception.\n-    bool isExpired() const override { return requests_queue.isClosed(); }\n+    bool isExpired() const override { return requests_queue.isFinished(); }\n \n     /// Useful to check owner of ephemeral node.\n     int64_t getSessionID() const override { return session_id; }\ndiff --git a/src/Coordination/KeeperDispatcher.cpp b/src/Coordination/KeeperDispatcher.cpp\nindex b4dc367ff626..a28e8d969152 100644\n--- a/src/Coordination/KeeperDispatcher.cpp\n+++ b/src/Coordination/KeeperDispatcher.cpp\n@@ -11,10 +11,12 @@ namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n     extern const int TIMEOUT_EXCEEDED;\n+    extern const int SYSTEM_ERROR;\n }\n \n KeeperDispatcher::KeeperDispatcher()\n     : coordination_settings(std::make_shared<CoordinationSettings>())\n+    , responses_queue(std::numeric_limits<size_t>::max())\n     , log(&Poco::Logger::get(\"KeeperDispatcher\"))\n {\n }\n@@ -164,7 +166,8 @@ void KeeperDispatcher::snapshotThread()\n     while (!shutdown_called)\n     {\n         CreateSnapshotTask task;\n-        snapshots_queue.pop(task);\n+        if (!snapshots_queue.pop(task))\n+            break;\n \n         if (shutdown_called)\n             break;\n@@ -235,9 +238,15 @@ bool KeeperDispatcher::putRequest(const Coordination::ZooKeeperRequestPtr & requ\n \n     /// Put close requests without timeouts\n     if (request->getOpNum() == Coordination::OpNum::Close)\n-        requests_queue->push(std::move(request_info));\n+    {\n+        if (!requests_queue->push(std::move(request_info)))\n+            throw Exception(\"Cannot push request to queue\", ErrorCodes::SYSTEM_ERROR);\n+    }\n     else if (!requests_queue->tryPush(std::move(request_info), coordination_settings->operation_timeout_ms.totalMilliseconds()))\n+    {\n         throw Exception(\"Cannot push request to queue within operation timeout\", ErrorCodes::TIMEOUT_EXCEEDED);\n+    }\n+\n     return true;\n }\n \n@@ -295,16 +304,17 @@ void KeeperDispatcher::shutdown()\n \n             if (requests_queue)\n             {\n-                requests_queue->push({});\n+                requests_queue->finish();\n+\n                 if (request_thread.joinable())\n                     request_thread.join();\n             }\n \n-            responses_queue.push({});\n+            responses_queue.finish();\n             if (responses_thread.joinable())\n                 responses_thread.join();\n \n-            snapshots_queue.push({});\n+            snapshots_queue.finish();\n             if (snapshot_thread.joinable())\n                 snapshot_thread.join();\n         }\n@@ -317,16 +327,9 @@ void KeeperDispatcher::shutdown()\n         /// Set session expired for all pending requests\n         while (requests_queue && requests_queue->tryPop(request_for_session))\n         {\n-            if (request_for_session.request)\n-            {\n-                auto response = request_for_session.request->makeResponse();\n-                response->error = Coordination::Error::ZSESSIONEXPIRED;\n-                setResponse(request_for_session.session_id, response);\n-            }\n-            else\n-            {\n-                break;\n-            }\n+            auto response = request_for_session.request->makeResponse();\n+            response->error = Coordination::Error::ZSESSIONEXPIRED;\n+            setResponse(request_for_session.session_id, response);\n         }\n \n         /// Clear all registered sessions\n@@ -379,7 +382,8 @@ void KeeperDispatcher::sessionCleanerTask()\n                     request_info.session_id = dead_session;\n                     {\n                         std::lock_guard lock(push_request_mutex);\n-                        requests_queue->push(std::move(request_info));\n+                        if (!requests_queue->push(std::move(request_info)))\n+                            LOG_INFO(log, \"Cannot push close request to queue while cleaning outdated sessions\");\n                     }\n \n                     /// Remove session from registered sessions\n@@ -414,7 +418,12 @@ void KeeperDispatcher::addErrorResponses(const KeeperStorage::RequestsForSession\n         response->xid = request->xid;\n         response->zxid = 0;\n         response->error = error;\n-        responses_queue.push(DB::KeeperStorage::ResponseForSession{session_id, response});\n+        if (!responses_queue.push(DB::KeeperStorage::ResponseForSession{session_id, response}))\n+            throw Exception(ErrorCodes::SYSTEM_ERROR,\n+                \"Could not push error response xid {} zxid {} error message {} to responses queue\",\n+                response->xid,\n+                response->zxid,\n+                errorMessage(error));\n     }\n }\n \ndiff --git a/src/Coordination/KeeperStateMachine.cpp b/src/Coordination/KeeperStateMachine.cpp\nindex 682a523fcafd..fe64f8afde70 100644\n--- a/src/Coordination/KeeperStateMachine.cpp\n+++ b/src/Coordination/KeeperStateMachine.cpp\n@@ -12,6 +12,7 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n+    extern const int SYSTEM_ERROR;\n }\n \n namespace\n@@ -120,7 +121,8 @@ nuraft::ptr<nuraft::buffer> KeeperStateMachine::commit(const uint64_t log_idx, n\n             session_id = storage->getSessionID(session_id_request.session_timeout_ms);\n             LOG_DEBUG(log, \"Session ID response {} with timeout {}\", session_id, session_id_request.session_timeout_ms);\n             response->session_id = session_id;\n-            responses_queue.push(response_for_session);\n+            if (!responses_queue.push(response_for_session))\n+                throw Exception(ErrorCodes::SYSTEM_ERROR, \"Could not push response with session id {} into responses queue\", session_id);\n         }\n     }\n     else\n@@ -128,7 +130,8 @@ nuraft::ptr<nuraft::buffer> KeeperStateMachine::commit(const uint64_t log_idx, n\n         std::lock_guard lock(storage_and_responses_lock);\n         KeeperStorage::ResponsesForSessions responses_for_sessions = storage->processRequest(request_for_session.request, request_for_session.session_id, log_idx);\n         for (auto & response_for_session : responses_for_sessions)\n-            responses_queue.push(response_for_session);\n+            if (!responses_queue.push(response_for_session))\n+                throw Exception(ErrorCodes::SYSTEM_ERROR, \"Could not push response with session id {} into responses queue\", response_for_session.session_id);\n     }\n \n     last_committed_idx = log_idx;\n@@ -218,7 +221,8 @@ void KeeperStateMachine::create_snapshot(\n \n     LOG_DEBUG(log, \"In memory snapshot {} created, queueing task to flash to disk\", s.get_last_log_idx());\n     /// Flush snapshot to disk in a separate thread.\n-    snapshots_queue.push(std::move(snapshot_task));\n+    if (!snapshots_queue.push(std::move(snapshot_task)))\n+        LOG_WARNING(log, \"Cannot push snapshot task into queue\");\n }\n \n void KeeperStateMachine::save_logical_snp_obj(\n@@ -304,7 +308,8 @@ void KeeperStateMachine::processReadRequest(const KeeperStorage::RequestForSessi\n     std::lock_guard lock(storage_and_responses_lock);\n     auto responses = storage->processRequest(request_for_session.request, request_for_session.session_id, std::nullopt);\n     for (const auto & response : responses)\n-        responses_queue.push(response);\n+        if (!responses_queue.push(response))\n+            throw Exception(ErrorCodes::SYSTEM_ERROR, \"Could not push response with session id {} into responses queue\", response.session_id);\n }\n \n std::vector<int64_t> KeeperStateMachine::getDeadSessions()\ndiff --git a/src/Coordination/KeeperStateMachine.h b/src/Coordination/KeeperStateMachine.h\nindex fcf9c7d14c4b..983692f7b7f1 100644\n--- a/src/Coordination/KeeperStateMachine.h\n+++ b/src/Coordination/KeeperStateMachine.h\n@@ -1,16 +1,17 @@\n #pragma once\n \n+#include <Common/ConcurrentBoundedQueue.h>\n #include <Coordination/KeeperStorage.h>\n #include <libnuraft/nuraft.hxx> // Y_IGNORE\n #include <base/logger_useful.h>\n-#include <Coordination/ThreadSafeQueue.h>\n #include <Coordination/CoordinationSettings.h>\n #include <Coordination/KeeperSnapshotManager.h>\n \n+\n namespace DB\n {\n \n-using ResponsesQueue = ThreadSafeQueue<KeeperStorage::ResponseForSession>;\n+using ResponsesQueue = ConcurrentBoundedQueue<KeeperStorage::ResponseForSession>;\n using SnapshotsQueue = ConcurrentBoundedQueue<CreateSnapshotTask>;\n \n /// ClickHouse Keeper state machine. Wrapper for KeeperStorage.\ndiff --git a/src/Coordination/ThreadSafeQueue.h b/src/Coordination/ThreadSafeQueue.h\ndeleted file mode 100644\nindex d36e25244bb4..000000000000\n--- a/src/Coordination/ThreadSafeQueue.h\n+++ /dev/null\n@@ -1,45 +0,0 @@\n-#pragma once\n-\n-#include <queue>\n-#include <mutex>\n-\n-namespace DB\n-{\n-\n-/// Queue with mutex and condvar. As simple as possible.\n-template <typename T>\n-class ThreadSafeQueue\n-{\n-private:\n-    mutable std::mutex queue_mutex;\n-    std::condition_variable cv;\n-    std::queue<T> queue;\n-public:\n-\n-    void push(const T & response)\n-    {\n-        std::lock_guard lock(queue_mutex);\n-        queue.push(response);\n-        cv.notify_one();\n-    }\n-\n-    bool tryPop(T & response, int64_t timeout_ms = 0)\n-    {\n-        std::unique_lock lock(queue_mutex);\n-        if (!cv.wait_for(lock,\n-                std::chrono::milliseconds(timeout_ms), [this] { return !queue.empty(); }))\n-            return false;\n-\n-        response = queue.front();\n-        queue.pop();\n-        return true;\n-    }\n-\n-    size_t size() const\n-    {\n-        std::lock_guard lock(queue_mutex);\n-        return queue.size();\n-    }\n-};\n-\n-}\ndiff --git a/src/DataStreams/RemoteQueryExecutor.cpp b/src/DataStreams/RemoteQueryExecutor.cpp\nindex 08d3db748b7e..b6a5e6f63d00 100644\n--- a/src/DataStreams/RemoteQueryExecutor.cpp\n+++ b/src/DataStreams/RemoteQueryExecutor.cpp\n@@ -35,6 +35,7 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n     extern const int UNKNOWN_PACKET_FROM_SERVER;\n     extern const int DUPLICATED_PART_UUIDS;\n+    extern const int SYSTEM_ERROR;\n }\n \n RemoteQueryExecutor::RemoteQueryExecutor(\n@@ -396,7 +397,8 @@ std::optional<Block> RemoteQueryExecutor::processPacket(Packet packet)\n         case Protocol::Server::ProfileEvents:\n             /// Pass profile events from remote server to client\n             if (auto profile_queue = CurrentThread::getInternalProfileEventsQueue())\n-                profile_queue->emplace(std::move(packet.block));\n+                if (!profile_queue->emplace(std::move(packet.block)))\n+                    throw Exception(ErrorCodes::SYSTEM_ERROR, \"Could not push into profile queue\");\n             break;\n \n         default:\ndiff --git a/src/Dictionaries/CacheDictionaryUpdateQueue.cpp b/src/Dictionaries/CacheDictionaryUpdateQueue.cpp\nindex 1d96fcc108b6..2077f846f093 100644\n--- a/src/Dictionaries/CacheDictionaryUpdateQueue.cpp\n+++ b/src/Dictionaries/CacheDictionaryUpdateQueue.cpp\n@@ -35,9 +35,11 @@ CacheDictionaryUpdateQueue<dictionary_key_type>::CacheDictionaryUpdateQueue(\n template <DictionaryKeyType dictionary_key_type>\n CacheDictionaryUpdateQueue<dictionary_key_type>::~CacheDictionaryUpdateQueue()\n {\n+    if (update_queue.isFinished())\n+        return;\n+\n     try {\n-        if (!finished)\n-            stopAndWait();\n+        stopAndWait();\n     }\n     catch (...)\n     {\n@@ -48,7 +50,7 @@ CacheDictionaryUpdateQueue<dictionary_key_type>::~CacheDictionaryUpdateQueue()\n template <DictionaryKeyType dictionary_key_type>\n void CacheDictionaryUpdateQueue<dictionary_key_type>::tryPushToUpdateQueueOrThrow(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr)\n {\n-    if (finished)\n+    if (update_queue.isFinished())\n         throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"CacheDictionaryUpdateQueue finished\");\n \n     if (!update_queue.tryPush(update_unit_ptr, configuration.update_queue_push_timeout_milliseconds))\n@@ -63,7 +65,7 @@ void CacheDictionaryUpdateQueue<dictionary_key_type>::tryPushToUpdateQueueOrThro\n template <DictionaryKeyType dictionary_key_type>\n void CacheDictionaryUpdateQueue<dictionary_key_type>::waitForCurrentUpdateFinish(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr) const\n {\n-    if (finished)\n+    if (update_queue.isFinished())\n         throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"CacheDictionaryUpdateQueue finished\");\n \n     std::unique_lock<std::mutex> update_lock(update_mutex);\n@@ -108,15 +110,10 @@ void CacheDictionaryUpdateQueue<dictionary_key_type>::waitForCurrentUpdateFinish\n template <DictionaryKeyType dictionary_key_type>\n void CacheDictionaryUpdateQueue<dictionary_key_type>::stopAndWait()\n {\n-    finished = true;\n-    update_queue.clear();\n-\n-    for (size_t i = 0; i < configuration.max_threads_for_updates; ++i)\n-    {\n-        auto empty_finishing_ptr = std::make_shared<CacheDictionaryUpdateUnit<dictionary_key_type>>();\n-        update_queue.push(empty_finishing_ptr);\n-    }\n+    if (update_queue.isFinished())\n+        throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"CacheDictionaryUpdateQueue finished\");\n \n+    update_queue.clearAndFinish();\n     update_pool.wait();\n }\n \n@@ -125,12 +122,10 @@ void CacheDictionaryUpdateQueue<dictionary_key_type>::updateThreadFunction()\n {\n     setThreadName(\"UpdQueue\");\n \n-    while (!finished)\n+    while (!update_queue.isFinished())\n     {\n         CacheDictionaryUpdateUnitPtr<dictionary_key_type> unit_to_update;\n-        update_queue.pop(unit_to_update);\n-\n-        if (finished)\n+        if (!update_queue.pop(unit_to_update))\n             break;\n \n         try\ndiff --git a/src/Dictionaries/CacheDictionaryUpdateQueue.h b/src/Dictionaries/CacheDictionaryUpdateQueue.h\nindex bcad376bc53c..7725ce7588f2 100644\n--- a/src/Dictionaries/CacheDictionaryUpdateQueue.h\n+++ b/src/Dictionaries/CacheDictionaryUpdateQueue.h\n@@ -122,7 +122,7 @@ class CacheDictionaryUpdateQueue\n     const CacheDictionaryUpdateQueueConfiguration & getConfiguration() const { return configuration; }\n \n     /// Is queue finished\n-    bool isFinished() const { return finished; }\n+    bool isFinished() const { return update_queue.isFinished(); }\n \n     /// Synchronous wait for update queue to stop\n     void stopAndWait();\n@@ -162,8 +162,6 @@ class CacheDictionaryUpdateQueue\n \n     mutable std::mutex update_mutex;\n     mutable std::condition_variable is_update_finished;\n-\n-    std::atomic<bool> finished{false};\n };\n \n extern template class CacheDictionaryUpdateQueue<DictionaryKeyType::Simple>;\ndiff --git a/src/Interpreters/InternalTextLogsQueue.cpp b/src/Interpreters/InternalTextLogsQueue.cpp\nindex 8329fc38ba7d..e541e1c76ccb 100644\n--- a/src/Interpreters/InternalTextLogsQueue.cpp\n+++ b/src/Interpreters/InternalTextLogsQueue.cpp\n@@ -41,7 +41,7 @@ void InternalTextLogsQueue::pushBlock(Block && log_block)\n     static Block sample_block = getSampleBlock();\n \n     if (blocksHaveEqualStructure(sample_block, log_block))\n-        emplace(log_block.mutateColumns());\n+        (void)(emplace(log_block.mutateColumns()));\n     else\n         LOG_WARNING(&Poco::Logger::get(\"InternalTextLogsQueue\"), \"Log block have different structure\");\n }\ndiff --git a/src/Processors/Formats/LazyOutputFormat.cpp b/src/Processors/Formats/LazyOutputFormat.cpp\nindex 0663ff28f84e..792d805eac3b 100644\n--- a/src/Processors/Formats/LazyOutputFormat.cpp\n+++ b/src/Processors/Formats/LazyOutputFormat.cpp\n@@ -9,11 +9,8 @@ WriteBuffer LazyOutputFormat::out(nullptr, 0);\n \n Chunk LazyOutputFormat::getChunk(UInt64 milliseconds)\n {\n-    if (finished_processing)\n-    {\n-        if (queue.empty())\n-            return {};\n-    }\n+    if (isFinished())\n+        return {};\n \n     Chunk chunk;\n     if (milliseconds)\n@@ -22,7 +19,10 @@ Chunk LazyOutputFormat::getChunk(UInt64 milliseconds)\n             return {};\n     }\n     else\n-        queue.pop(chunk);\n+    {\n+        if (!queue.pop(chunk))\n+            return {};\n+    }\n \n     if (chunk)\n         info.update(chunk.getNumRows(), chunk.allocatedBytes());\ndiff --git a/src/Processors/Formats/LazyOutputFormat.h b/src/Processors/Formats/LazyOutputFormat.h\nindex 2c29f55c4f37..50dc87f2e70a 100644\n--- a/src/Processors/Formats/LazyOutputFormat.h\n+++ b/src/Processors/Formats/LazyOutputFormat.h\n@@ -15,7 +15,7 @@ class LazyOutputFormat : public IOutputFormat\n \n public:\n     explicit LazyOutputFormat(const Block & header)\n-        : IOutputFormat(header, out), queue(2), finished_processing(false) {}\n+        : IOutputFormat(header, out), queue(2) {}\n \n     String getName() const override { return \"LazyOutputFormat\"; }\n \n@@ -23,7 +23,7 @@ class LazyOutputFormat : public IOutputFormat\n     Chunk getTotals();\n     Chunk getExtremes();\n \n-    bool isFinished() { return finished_processing && queue.size() == 0; }\n+    bool isFinished() { return queue.isFinishedAndEmpty(); }\n \n     BlockStreamProfileInfo & getProfileInfo() { return info; }\n \n@@ -31,17 +31,12 @@ class LazyOutputFormat : public IOutputFormat\n \n     void onCancel() override\n     {\n-        finished_processing = true;\n-        /// Clear queue in case if somebody is waiting lazy_format to push.\n-        queue.clear();\n+        queue.clearAndFinish();\n     }\n \n     void finalize() override\n     {\n-        finished_processing = true;\n-\n-        /// In case we are waiting for result.\n-        queue.emplace(Chunk());\n+        queue.finish();\n     }\n \n     bool expectMaterializedColumns() const override { return false; }\n@@ -49,8 +44,7 @@ class LazyOutputFormat : public IOutputFormat\n protected:\n     void consume(Chunk chunk) override\n     {\n-        if (!finished_processing)\n-            queue.emplace(std::move(chunk));\n+        (void)(queue.emplace(std::move(chunk)));\n     }\n \n     void consumeTotals(Chunk chunk) override { totals = std::move(chunk); }\n@@ -66,8 +60,6 @@ class LazyOutputFormat : public IOutputFormat\n     static WriteBuffer out;\n \n     BlockStreamProfileInfo info;\n-\n-    std::atomic<bool> finished_processing;\n };\n \n }\ndiff --git a/src/Server/KeeperTCPHandler.cpp b/src/Server/KeeperTCPHandler.cpp\nindex 7ebbda9dfe6e..b19b02f960d7 100644\n--- a/src/Server/KeeperTCPHandler.cpp\n+++ b/src/Server/KeeperTCPHandler.cpp\n@@ -198,7 +198,7 @@ KeeperTCPHandler::KeeperTCPHandler(IServer & server_, const Poco::Net::StreamSoc\n     , operation_timeout(0, global_context->getConfigRef().getUInt(\"keeper_server.operation_timeout_ms\", Coordination::DEFAULT_OPERATION_TIMEOUT_MS) * 1000)\n     , session_timeout(0, global_context->getConfigRef().getUInt(\"keeper_server.session_timeout_ms\", Coordination::DEFAULT_SESSION_TIMEOUT_MS) * 1000)\n     , poll_wrapper(std::make_unique<SocketInterruptablePollWrapper>(socket_))\n-    , responses(std::make_unique<ThreadSafeResponseQueue>())\n+    , responses(std::make_unique<ThreadSafeResponseQueue>(std::numeric_limits<size_t>::max()))\n {\n }\n \n@@ -314,7 +314,12 @@ void KeeperTCPHandler::runImpl()\n     auto response_fd = poll_wrapper->getResponseFD();\n     auto response_callback = [this, response_fd] (const Coordination::ZooKeeperResponsePtr & response)\n     {\n-        responses->push(response);\n+        if (!responses->push(response))\n+            throw Exception(ErrorCodes::SYSTEM_ERROR,\n+                \"Could not push response with xid {} and zxid {}\",\n+                response->xid,\n+                response->zxid);\n+\n         UInt8 single_byte = 1;\n         [[maybe_unused]] int result = write(response_fd, &single_byte, sizeof(single_byte));\n     };\ndiff --git a/src/Server/KeeperTCPHandler.h b/src/Server/KeeperTCPHandler.h\nindex 7abfb72c8464..274fb21af636 100644\n--- a/src/Server/KeeperTCPHandler.h\n+++ b/src/Server/KeeperTCPHandler.h\n@@ -13,10 +13,10 @@\n #include <Interpreters/Context.h>\n #include <Common/ZooKeeper/ZooKeeperCommon.h>\n #include <Common/ZooKeeper/ZooKeeperConstants.h>\n+#include <Common/ConcurrentBoundedQueue.h>\n #include <Coordination/KeeperDispatcher.h>\n #include <IO/WriteBufferFromPocoSocket.h>\n #include <IO/ReadBufferFromPocoSocket.h>\n-#include <Coordination/ThreadSafeQueue.h>\n #include <unordered_map>\n \n namespace DB\n@@ -25,7 +25,7 @@ namespace DB\n struct SocketInterruptablePollWrapper;\n using SocketInterruptablePollWrapperPtr = std::unique_ptr<SocketInterruptablePollWrapper>;\n \n-using ThreadSafeResponseQueue = ThreadSafeQueue<Coordination::ZooKeeperResponsePtr>;\n+using ThreadSafeResponseQueue = ConcurrentBoundedQueue<Coordination::ZooKeeperResponsePtr>;\n \n using ThreadSafeResponseQueuePtr = std::unique_ptr<ThreadSafeResponseQueue>;\n \ndiff --git a/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp b/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp\nindex 2e2807d8297c..ac60d748e366 100644\n--- a/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp\n+++ b/src/Storages/RabbitMQ/ReadBufferFromRabbitMQConsumer.cpp\n@@ -14,6 +14,11 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n ReadBufferFromRabbitMQConsumer::ReadBufferFromRabbitMQConsumer(\n         ChannelPtr consumer_channel_,\n         RabbitMQHandler & event_handler_,\n@@ -64,9 +69,10 @@ void ReadBufferFromRabbitMQConsumer::subscribe()\n                 if (row_delimiter != '\\0')\n                     message_received += row_delimiter;\n \n-                received.push({message_received, message.hasMessageID() ? message.messageID() : \"\",\n+                if (!received.push({message_received, message.hasMessageID() ? message.messageID() : \"\",\n                         message.hasTimestamp() ? message.timestamp() : 0,\n-                        redelivered, AckTracker(delivery_tag, channel_id)});\n+                        redelivered, AckTracker(delivery_tag, channel_id)}))\n+                    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Could not push to received queue\");\n             }\n         })\n         .onError([&](const char * message)\ndiff --git a/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp b/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp\nindex 1929a103414d..8d891e34a64b 100644\n--- a/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp\n+++ b/src/Storages/RabbitMQ/WriteBufferToRabbitMQProducer.cpp\n@@ -21,6 +21,7 @@ static const auto RETURNED_LIMIT = 50000;\n namespace ErrorCodes\n {\n     extern const int CANNOT_CONNECT_RABBITMQ;\n+    extern const int LOGICAL_ERROR;\n }\n \n WriteBufferToRabbitMQProducer::WriteBufferToRabbitMQProducer(\n@@ -102,7 +103,8 @@ void WriteBufferToRabbitMQProducer::countRow()\n         reinitializeChunks();\n \n         ++payload_counter;\n-        payloads.push(std::make_pair(payload_counter, payload));\n+        if (!payloads.push(std::make_pair(payload_counter, payload)))\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Could not push to payloads queue\");\n     }\n }\n \n@@ -122,7 +124,8 @@ void WriteBufferToRabbitMQProducer::setupChannel()\n          * they are republished because after channel recovery they will acquire new delivery tags, so all previous records become invalid\n          */\n         for (const auto & record : delivery_record)\n-            returned.tryPush(record.second);\n+            if (!returned.push(record.second))\n+                throw Exception(ErrorCodes::LOGICAL_ERROR, \"Could not push to returned queue\");\n \n         LOG_DEBUG(log, \"Producer on channel {} hasn't confirmed {} messages, {} waiting to be published\",\n                 channel_id, delivery_record.size(), payloads.size());\n@@ -170,7 +173,8 @@ void WriteBufferToRabbitMQProducer::removeRecord(UInt64 received_delivery_tag, b\n \n         if (republish)\n             for (auto record = delivery_record.begin(); record != record_iter; ++record)\n-                returned.tryPush(record->second);\n+                if (!returned.push(record->second))\n+                    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Could not push to returned queue\");\n \n         /// Delete the records even in case when republished because new delivery tags will be assigned by the server.\n         delivery_record.erase(delivery_record.begin(), record_iter);\n@@ -178,7 +182,8 @@ void WriteBufferToRabbitMQProducer::removeRecord(UInt64 received_delivery_tag, b\n     else\n     {\n         if (republish)\n-            returned.tryPush(record_iter->second);\n+            if (!returned.push(record_iter->second))\n+                throw Exception(ErrorCodes::LOGICAL_ERROR, \"Could not push to returned queue\");\n \n         delivery_record.erase(record_iter);\n     }\n@@ -194,7 +199,11 @@ void WriteBufferToRabbitMQProducer::publish(ConcurrentBoundedQueue<std::pair<UIn\n      */\n     while (!messages.empty() && producer_channel->usable() && delivery_record.size() < RETURNED_LIMIT)\n     {\n-        messages.pop(payload);\n+        bool pop_result = messages.pop(payload);\n+\n+        if (!pop_result)\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Could not pop payload\");\n+\n         AMQP::Envelope envelope(payload.second.data(), payload.second.size());\n \n         /// if headers exchange is used, routing keys are added here via headers, if not - it is just empty\ndiff --git a/utils/keeper-data-dumper/main.cpp b/utils/keeper-data-dumper/main.cpp\nindex b238c2ef569b..ed6a7aea9723 100644\n--- a/utils/keeper-data-dumper/main.cpp\n+++ b/utils/keeper-data-dumper/main.cpp\n@@ -59,7 +59,7 @@ int main(int argc, char *argv[])\n         Poco::Logger::root().setLevel(\"trace\");\n     }\n     auto * logger = &Poco::Logger::get(\"keeper-dumper\");\n-    ResponsesQueue queue;\n+    ResponsesQueue queue(std::numeric_limits<size_t>::max());\n     SnapshotsQueue snapshots_queue{1};\n     CoordinationSettingsPtr settings = std::make_shared<CoordinationSettings>();\n     auto state_machine = std::make_shared<KeeperStateMachine>(queue, snapshots_queue, argv[1], settings);\n",
  "test_patch": "diff --git a/src/Coordination/tests/gtest_coordination.cpp b/src/Coordination/tests/gtest_coordination.cpp\nindex 99dd64a21cd3..0cb5972f7184 100644\n--- a/src/Coordination/tests/gtest_coordination.cpp\n+++ b/src/Coordination/tests/gtest_coordination.cpp\n@@ -1159,7 +1159,7 @@ void testLogAndStateMachine(Coordination::CoordinationSettingsPtr settings, uint\n     ChangelogDirTest snapshots(\"./snapshots\");\n     ChangelogDirTest logs(\"./logs\");\n \n-    ResponsesQueue queue;\n+    ResponsesQueue queue(std::numeric_limits<size_t>::max());\n     SnapshotsQueue snapshots_queue{1};\n     auto state_machine = std::make_shared<KeeperStateMachine>(queue, snapshots_queue, \"./snapshots\", settings);\n     state_machine->init();\n@@ -1186,7 +1186,9 @@ void testLogAndStateMachine(Coordination::CoordinationSettingsPtr settings, uint\n \n             state_machine->create_snapshot(s, when_done);\n             CreateSnapshotTask snapshot_task;\n-            snapshots_queue.pop(snapshot_task);\n+            bool pop_result = snapshots_queue.pop(snapshot_task);\n+            EXPECT_TRUE(pop_result);\n+\n             snapshot_task.create_snapshot(std::move(snapshot_task.snapshot));\n         }\n         if (snapshot_created)\n@@ -1308,7 +1310,7 @@ TEST_P(CoordinationTest, TestEphemeralNodeRemove)\n     ChangelogDirTest snapshots(\"./snapshots\");\n     CoordinationSettingsPtr settings = std::make_shared<CoordinationSettings>();\n \n-    ResponsesQueue queue;\n+    ResponsesQueue queue(std::numeric_limits<size_t>::max());\n     SnapshotsQueue snapshots_queue{1};\n     auto state_machine = std::make_shared<KeeperStateMachine>(queue, snapshots_queue, \"./snapshots\", settings);\n     state_machine->init();\n",
  "problem_statement": "CacheDictionaryUpdateQueue refactor\nChangelog category (leave one):\r\n- Not for changelog (changelog entry is not required)\r\n\r\nCloses #29543\n",
  "hints_text": "",
  "created_at": "2021-10-06T11:03:04Z"
}