{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 40991,
  "instance_id": "ClickHouse__ClickHouse-40991",
  "issue_numbers": [
    "41421"
  ],
  "base_commit": "779a8f1305c5041cc0523607b21ff688c4a36db2",
  "patch": "diff --git a/src/Coordination/KeeperServer.cpp b/src/Coordination/KeeperServer.cpp\nindex 42d7d967b1fa..08092cf68f1b 100644\n--- a/src/Coordination/KeeperServer.cpp\n+++ b/src/Coordination/KeeperServer.cpp\n@@ -705,7 +705,7 @@ void KeeperServer::waitInit()\n \n     int64_t timeout = coordination_settings->startup_timeout.totalMilliseconds();\n     if (!initialized_cv.wait_for(lock, std::chrono::milliseconds(timeout), [&] { return initialized_flag.load(); }))\n-        throw Exception(ErrorCodes::RAFT_ERROR, \"Failed to wait RAFT initialization\");\n+        LOG_WARNING(log, \"Failed to wait for RAFT initialization in {}ms, will continue in background\", timeout);\n }\n \n std::vector<int64_t> KeeperServer::getDeadSessions()\n",
  "test_patch": "diff --git a/tests/integration/helpers/keeper_utils.py b/tests/integration/helpers/keeper_utils.py\nnew file mode 100644\nindex 000000000000..681407e5e8c2\n--- /dev/null\n+++ b/tests/integration/helpers/keeper_utils.py\n@@ -0,0 +1,41 @@\n+import socket\n+import time\n+\n+\n+def get_keeper_socket(cluster, node, port=9181):\n+    hosts = cluster.get_instance_ip(node.name)\n+    client = socket.socket()\n+    client.settimeout(10)\n+    client.connect((hosts, port))\n+    return client\n+\n+\n+def send_4lw_cmd(cluster, node, cmd=\"ruok\", port=9181):\n+    client = None\n+    try:\n+        client = get_keeper_socket(cluster, node, port)\n+        client.send(cmd.encode())\n+        data = client.recv(100_000)\n+        data = data.decode()\n+        return data\n+    finally:\n+        if client is not None:\n+            client.close()\n+\n+\n+NOT_SERVING_REQUESTS_ERROR_MSG = \"This instance is not currently serving requests\"\n+\n+\n+def wait_until_connected(cluster, node, port=9181):\n+    while send_4lw_cmd(cluster, node, \"mntr\", port) == NOT_SERVING_REQUESTS_ERROR_MSG:\n+        time.sleep(0.1)\n+\n+\n+def wait_until_quorum_lost(cluster, node, port=9181):\n+    while send_4lw_cmd(cluster, node, \"mntr\", port) != NOT_SERVING_REQUESTS_ERROR_MSG:\n+        time.sleep(0.1)\n+\n+\n+def wait_nodes(cluster, nodes):\n+    for node in nodes:\n+        wait_until_connected(cluster, node)\ndiff --git a/tests/integration/test_keeper_and_access_storage/test.py b/tests/integration/test_keeper_and_access_storage/test.py\nindex ae6b0085094d..6ec307f7082f 100644\n--- a/tests/integration/test_keeper_and_access_storage/test.py\n+++ b/tests/integration/test_keeper_and_access_storage/test.py\n@@ -15,6 +15,7 @@\n def started_cluster():\n     try:\n         cluster.start()\n+\n         yield cluster\n     finally:\n         cluster.shutdown()\ndiff --git a/tests/integration/test_keeper_clickhouse_hard_restart/configs/enable_keeper.xml b/tests/integration/test_keeper_clickhouse_hard_restart/configs/enable_keeper.xml\ndeleted file mode 100644\nindex c1d38a1de52f..000000000000\n--- a/tests/integration/test_keeper_clickhouse_hard_restart/configs/enable_keeper.xml\n+++ /dev/null\n@@ -1,22 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>1</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>1</id>\n-                <hostname>node1</hostname>\n-                <port>9234</port>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_clickhouse_hard_restart/configs/keeper_conf.xml b/tests/integration/test_keeper_clickhouse_hard_restart/configs/keeper_conf.xml\ndeleted file mode 100644\nindex ebb0d98ddf46..000000000000\n--- a/tests/integration/test_keeper_clickhouse_hard_restart/configs/keeper_conf.xml\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-<clickhouse>\n-    <zookeeper>\n-        <node index=\"1\">\n-            <host>node1</host>\n-            <port>9181</port>\n-        </node>\n-    </zookeeper>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_force_recovery/test.py b/tests/integration/test_keeper_force_recovery/test.py\nindex f3bb0ca56e3b..f7c3787b4d8b 100644\n--- a/tests/integration/test_keeper_force_recovery/test.py\n+++ b/tests/integration/test_keeper_force_recovery/test.py\n@@ -2,6 +2,7 @@\n import pytest\n import socket\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import time\n \n \n@@ -62,37 +63,6 @@ def get_fake_zk(nodename, timeout=30.0):\n     return _fake_zk_instance\n \n \n-def get_keeper_socket(node_name):\n-    hosts = cluster.get_instance_ip(node_name)\n-    client = socket.socket()\n-    client.settimeout(10)\n-    client.connect((hosts, 9181))\n-    return client\n-\n-\n-def send_4lw_cmd(node_name, cmd=\"ruok\"):\n-    client = None\n-    try:\n-        client = get_keeper_socket(node_name)\n-        client.send(cmd.encode())\n-        data = client.recv(100_000)\n-        data = data.decode()\n-        return data\n-    finally:\n-        if client is not None:\n-            client.close()\n-\n-\n-def wait_until_connected(node_name):\n-    while send_4lw_cmd(node_name, \"mntr\") == NOT_SERVING_REQUESTS_ERROR_MSG:\n-        time.sleep(0.1)\n-\n-\n-def wait_nodes(nodes):\n-    for node in nodes:\n-        wait_until_connected(node.name)\n-\n-\n def wait_and_assert_data(zk, path, data):\n     while zk.retry(zk.exists, path) is None:\n         time.sleep(0.1)\n@@ -104,9 +74,6 @@ def close_zk(zk):\n     zk.close()\n \n \n-NOT_SERVING_REQUESTS_ERROR_MSG = \"This instance is not currently serving requests\"\n-\n-\n def test_cluster_recovery(started_cluster):\n     node_zks = []\n     try:\n@@ -114,7 +81,7 @@ def test_cluster_recovery(started_cluster):\n         for node in nodes[CLUSTER_SIZE:]:\n             node.stop_clickhouse()\n \n-        wait_nodes(nodes[:CLUSTER_SIZE])\n+        keeper_utils.wait_nodes(cluster, nodes[:CLUSTER_SIZE])\n \n         node_zks = [get_fake_zk(node.name) for node in nodes[:CLUSTER_SIZE]]\n \n@@ -152,7 +119,7 @@ def assert_all_data(zk):\n             wait_and_assert_data(node_zk, \"/test_force_recovery_extra\", \"somedataextra\")\n \n         nodes[0].start_clickhouse()\n-        wait_until_connected(nodes[0].name)\n+        keeper_utils.wait_until_connected(cluster, nodes[0])\n         node_zks[0] = get_fake_zk(nodes[0].name)\n         wait_and_assert_data(node_zks[0], \"/test_force_recovery_extra\", \"somedataextra\")\n \n@@ -167,8 +134,7 @@ def assert_all_data(zk):\n             node.stop_clickhouse()\n \n         # wait for node1 to lose quorum\n-        while send_4lw_cmd(nodes[0].name, \"mntr\") != NOT_SERVING_REQUESTS_ERROR_MSG:\n-            time.sleep(0.2)\n+        keeper_utils.wait_until_quorum_lost(cluster, nodes[0])\n \n         nodes[0].copy_file_to_container(\n             os.path.join(CONFIG_DIR, \"recovered_keeper1.xml\"),\n@@ -177,9 +143,15 @@ def assert_all_data(zk):\n \n         nodes[0].query(\"SYSTEM RELOAD CONFIG\")\n \n-        assert send_4lw_cmd(nodes[0].name, \"mntr\") == NOT_SERVING_REQUESTS_ERROR_MSG\n-        send_4lw_cmd(nodes[0].name, \"rcvr\")\n-        assert send_4lw_cmd(nodes[0].name, \"mntr\") == NOT_SERVING_REQUESTS_ERROR_MSG\n+        assert (\n+            keeper_utils.send_4lw_cmd(cluster, nodes[0], \"mntr\")\n+            == keeper_utils.NOT_SERVING_REQUESTS_ERROR_MSG\n+        )\n+        keeper_utils.send_4lw_cmd(cluster, nodes[0], \"rcvr\")\n+        assert (\n+            keeper_utils.send_4lw_cmd(cluster, nodes[0], \"mntr\")\n+            == keeper_utils.NOT_SERVING_REQUESTS_ERROR_MSG\n+        )\n \n         # add one node to restore the quorum\n         nodes[CLUSTER_SIZE].copy_file_to_container(\n@@ -191,10 +163,10 @@ def assert_all_data(zk):\n         )\n \n         nodes[CLUSTER_SIZE].start_clickhouse()\n-        wait_until_connected(nodes[CLUSTER_SIZE].name)\n+        keeper_utils.wait_until_connected(cluster, nodes[CLUSTER_SIZE])\n \n         # node1 should have quorum now and accept requests\n-        wait_until_connected(nodes[0].name)\n+        keeper_utils.wait_until_connected(cluster, nodes[0])\n \n         node_zks.append(get_fake_zk(nodes[CLUSTER_SIZE].name))\n \n@@ -206,7 +178,7 @@ def assert_all_data(zk):\n                 f\"/etc/clickhouse-server/config.d/enable_keeper{i+1}.xml\",\n             )\n             node.start_clickhouse()\n-            wait_until_connected(node.name)\n+            keeper_utils.wait_until_connected(cluster, node)\n             node_zks.append(get_fake_zk(node.name))\n \n         # refresh old zk sessions\n@@ -223,7 +195,7 @@ def assert_all_data(zk):\n         wait_and_assert_data(node_zks[-1], \"/test_force_recovery_last\", \"somedatalast\")\n \n         nodes[0].start_clickhouse()\n-        wait_until_connected(nodes[0].name)\n+        keeper_utils.wait_until_connected(cluster, nodes[0])\n         node_zks[0] = get_fake_zk(nodes[0].name)\n         for zk in node_zks[:nodes_left]:\n             assert_all_data(zk)\ndiff --git a/tests/integration/test_keeper_force_recovery_single_node/test.py b/tests/integration/test_keeper_force_recovery_single_node/test.py\nindex 0a554e331195..1c0d5e9a3062 100644\n--- a/tests/integration/test_keeper_force_recovery_single_node/test.py\n+++ b/tests/integration/test_keeper_force_recovery_single_node/test.py\n@@ -2,10 +2,11 @@\n import pytest\n import socket\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import time\n \n \n-from kazoo.client import KazooClient\n+from kazoo.client import KazooClient, KazooRetry\n \n CLUSTER_SIZE = 3\n \n@@ -45,47 +46,19 @@ def started_cluster():\n \n def get_fake_zk(nodename, timeout=30.0):\n     _fake_zk_instance = KazooClient(\n-        hosts=cluster.get_instance_ip(nodename) + \":9181\", timeout=timeout\n+        hosts=cluster.get_instance_ip(nodename) + \":9181\",\n+        timeout=timeout,\n+        connection_retry=KazooRetry(max_tries=10),\n+        command_retry=KazooRetry(max_tries=10),\n     )\n     _fake_zk_instance.start()\n     return _fake_zk_instance\n \n \n-def get_keeper_socket(node_name):\n-    hosts = cluster.get_instance_ip(node_name)\n-    client = socket.socket()\n-    client.settimeout(10)\n-    client.connect((hosts, 9181))\n-    return client\n-\n-\n-def send_4lw_cmd(node_name, cmd=\"ruok\"):\n-    client = None\n-    try:\n-        client = get_keeper_socket(node_name)\n-        client.send(cmd.encode())\n-        data = client.recv(100_000)\n-        data = data.decode()\n-        return data\n-    finally:\n-        if client is not None:\n-            client.close()\n-\n-\n-def wait_until_connected(node_name):\n-    while send_4lw_cmd(node_name, \"mntr\") == NOT_SERVING_REQUESTS_ERROR_MSG:\n-        time.sleep(0.1)\n-\n-\n-def wait_nodes(nodes):\n-    for node in nodes:\n-        wait_until_connected(node.name)\n-\n-\n def wait_and_assert_data(zk, path, data):\n-    while zk.exists(path) is None:\n+    while zk.retry(zk.exists, path) is None:\n         time.sleep(0.1)\n-    assert zk.get(path)[0] == data.encode()\n+    assert zk.retry(zk.get, path)[0] == data.encode()\n \n \n def close_zk(zk):\n@@ -93,20 +66,17 @@ def close_zk(zk):\n     zk.close()\n \n \n-NOT_SERVING_REQUESTS_ERROR_MSG = \"This instance is not currently serving requests\"\n-\n-\n def test_cluster_recovery(started_cluster):\n     node_zks = []\n     try:\n-        wait_nodes(nodes)\n+        keeper_utils.wait_nodes(cluster, nodes)\n \n         node_zks = [get_fake_zk(node.name) for node in nodes]\n \n         data_in_cluster = []\n \n         def add_data(zk, path, data):\n-            zk.create(path, data.encode())\n+            zk.retry(zk.create, path, data.encode())\n             data_in_cluster.append((path, data))\n \n         def assert_all_data(zk):\n@@ -137,7 +107,7 @@ def assert_all_data(zk):\n             wait_and_assert_data(node_zk, \"/test_force_recovery_extra\", \"somedataextra\")\n \n         nodes[0].start_clickhouse()\n-        wait_until_connected(nodes[0].name)\n+        keeper_utils.wait_until_connected(cluster, nodes[0])\n \n         node_zks[0] = get_fake_zk(nodes[0].name)\n         wait_and_assert_data(node_zks[0], \"/test_force_recovery_extra\", \"somedataextra\")\n@@ -156,7 +126,7 @@ def assert_all_data(zk):\n         )\n \n         nodes[0].start_clickhouse()\n-        wait_until_connected(nodes[0].name)\n+        keeper_utils.wait_until_connected(cluster, nodes[0])\n \n         assert_all_data(get_fake_zk(nodes[0].name))\n     finally:\ndiff --git a/tests/integration/test_keeper_four_word_command/test.py b/tests/integration/test_keeper_four_word_command/test.py\nindex e8136d322d34..30abc7422c49 100644\n--- a/tests/integration/test_keeper_four_word_command/test.py\n+++ b/tests/integration/test_keeper_four_word_command/test.py\n@@ -1,6 +1,7 @@\n import socket\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -25,6 +26,10 @@\n from kazoo.client import KazooClient, KazooState\n \n \n+def wait_nodes():\n+    keeper_utils.wait_nodes(cluster, [node1, node2, node3])\n+\n+\n @pytest.fixture(scope=\"module\")\n def started_cluster():\n     try:\n@@ -56,28 +61,6 @@ def clear_znodes():\n         destroy_zk_client(zk)\n \n \n-def wait_node(node):\n-    for _ in range(100):\n-        zk = None\n-        try:\n-            zk = get_fake_zk(node.name, timeout=30.0)\n-            # zk.create(\"/test\", sequence=True)\n-            print(\"node\", node.name, \"ready\")\n-            break\n-        except Exception as ex:\n-            time.sleep(0.2)\n-            print(\"Waiting until\", node.name, \"will be ready, exception\", ex)\n-        finally:\n-            destroy_zk_client(zk)\n-    else:\n-        raise Exception(\"Can't wait node\", node.name, \"to become ready\")\n-\n-\n-def wait_nodes():\n-    for n in [node1, node2, node3]:\n-        wait_node(n)\n-\n-\n def get_fake_zk(nodename, timeout=30.0):\n     _fake_zk_instance = KazooClient(\n         hosts=cluster.get_instance_ip(nodename) + \":9181\", timeout=timeout\n@@ -86,23 +69,15 @@ def get_fake_zk(nodename, timeout=30.0):\n     return _fake_zk_instance\n \n \n-def get_keeper_socket(node_name):\n-    hosts = cluster.get_instance_ip(node_name)\n-    client = socket.socket()\n-    client.settimeout(10)\n-    client.connect((hosts, 9181))\n-    return client\n-\n-\n def close_keeper_socket(cli):\n     if cli is not None:\n         cli.close()\n \n \n-def reset_node_stats(node_name=node1.name):\n+def reset_node_stats(node=node1):\n     client = None\n     try:\n-        client = get_keeper_socket(node_name)\n+        client = keeper_utils.get_keeper_socket(cluster, node)\n         client.send(b\"srst\")\n         client.recv(10)\n     finally:\n@@ -110,23 +85,10 @@ def reset_node_stats(node_name=node1.name):\n             client.close()\n \n \n-def send_4lw_cmd(node_name=node1.name, cmd=\"ruok\"):\n-    client = None\n-    try:\n-        client = get_keeper_socket(node_name)\n-        client.send(cmd.encode())\n-        data = client.recv(100_000)\n-        data = data.decode()\n-        return data\n-    finally:\n-        if client is not None:\n-            client.close()\n-\n-\n-def reset_conn_stats(node_name=node1.name):\n+def reset_conn_stats(node=node1):\n     client = None\n     try:\n-        client = get_keeper_socket(node_name)\n+        client = keeper_utils.get_keeper_socket(cluster, node)\n         client.send(b\"crst\")\n         client.recv(10_000)\n     finally:\n@@ -138,7 +100,7 @@ def test_cmd_ruok(started_cluster):\n     client = None\n     try:\n         wait_nodes()\n-        data = send_4lw_cmd(cmd=\"ruok\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"ruok\")\n         assert data == \"imok\"\n     finally:\n         close_keeper_socket(client)\n@@ -187,7 +149,7 @@ def test_cmd_mntr(started_cluster):\n         clear_znodes()\n \n         # reset stat first\n-        reset_node_stats(node1.name)\n+        reset_node_stats(node1)\n \n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(\n@@ -200,7 +162,7 @@ def test_cmd_mntr(started_cluster):\n             delete_cnt=2,\n         )\n \n-        data = send_4lw_cmd(cmd=\"mntr\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"mntr\")\n \n         # print(data.decode())\n         reader = csv.reader(data.split(\"\\n\"), delimiter=\"\\t\")\n@@ -252,10 +214,10 @@ def test_cmd_srst(started_cluster):\n         wait_nodes()\n         clear_znodes()\n \n-        data = send_4lw_cmd(cmd=\"srst\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"srst\")\n         assert data.strip() == \"Server stats reset.\"\n \n-        data = send_4lw_cmd(cmd=\"mntr\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"mntr\")\n         assert len(data) != 0\n \n         # print(data)\n@@ -279,7 +241,7 @@ def test_cmd_conf(started_cluster):\n         wait_nodes()\n         clear_znodes()\n \n-        data = send_4lw_cmd(cmd=\"conf\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"conf\")\n \n         reader = csv.reader(data.split(\"\\n\"), delimiter=\"=\")\n         result = {}\n@@ -335,8 +297,8 @@ def test_cmd_conf(started_cluster):\n \n def test_cmd_isro(started_cluster):\n     wait_nodes()\n-    assert send_4lw_cmd(node1.name, \"isro\") == \"rw\"\n-    assert send_4lw_cmd(node2.name, \"isro\") == \"ro\"\n+    assert keeper_utils.send_4lw_cmd(cluster, node1, \"isro\") == \"rw\"\n+    assert keeper_utils.send_4lw_cmd(cluster, node2, \"isro\") == \"ro\"\n \n \n def test_cmd_srvr(started_cluster):\n@@ -345,12 +307,12 @@ def test_cmd_srvr(started_cluster):\n         wait_nodes()\n         clear_znodes()\n \n-        reset_node_stats(node1.name)\n+        reset_node_stats(node1)\n \n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, create_cnt=10)\n \n-        data = send_4lw_cmd(cmd=\"srvr\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"srvr\")\n \n         print(\"srvr output -------------------------------------\")\n         print(data)\n@@ -380,13 +342,13 @@ def test_cmd_stat(started_cluster):\n     try:\n         wait_nodes()\n         clear_znodes()\n-        reset_node_stats(node1.name)\n-        reset_conn_stats(node1.name)\n+        reset_node_stats(node1)\n+        reset_conn_stats(node1)\n \n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, create_cnt=10)\n \n-        data = send_4lw_cmd(cmd=\"stat\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"stat\")\n \n         print(\"stat output -------------------------------------\")\n         print(data)\n@@ -440,7 +402,7 @@ def test_cmd_cons(started_cluster):\n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, create_cnt=10)\n \n-        data = send_4lw_cmd(cmd=\"cons\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"cons\")\n \n         print(\"cons output -------------------------------------\")\n         print(data)\n@@ -485,12 +447,12 @@ def test_cmd_crst(started_cluster):\n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, create_cnt=10)\n \n-        data = send_4lw_cmd(cmd=\"crst\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"crst\")\n \n         print(\"crst output -------------------------------------\")\n         print(data)\n \n-        data = send_4lw_cmd(cmd=\"cons\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"cons\")\n         print(\"cons output(after crst) -------------------------------------\")\n         print(data)\n \n@@ -537,7 +499,7 @@ def test_cmd_dump(started_cluster):\n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, ephemeral_cnt=2)\n \n-        data = send_4lw_cmd(cmd=\"dump\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"dump\")\n \n         print(\"dump output -------------------------------------\")\n         print(data)\n@@ -563,7 +525,7 @@ def test_cmd_wchs(started_cluster):\n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, create_cnt=2, watch_cnt=2)\n \n-        data = send_4lw_cmd(cmd=\"wchs\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"wchs\")\n \n         print(\"wchs output -------------------------------------\")\n         print(data)\n@@ -598,7 +560,7 @@ def test_cmd_wchc(started_cluster):\n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, create_cnt=2, watch_cnt=2)\n \n-        data = send_4lw_cmd(cmd=\"wchc\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"wchc\")\n \n         print(\"wchc output -------------------------------------\")\n         print(data)\n@@ -622,7 +584,7 @@ def test_cmd_wchp(started_cluster):\n         zk = get_fake_zk(node1.name, timeout=30.0)\n         do_some_action(zk, create_cnt=2, watch_cnt=2)\n \n-        data = send_4lw_cmd(cmd=\"wchp\")\n+        data = keeper_utils.send_4lw_cmd(cluster, node1, cmd=\"wchp\")\n \n         print(\"wchp output -------------------------------------\")\n         print(data)\ndiff --git a/tests/integration/test_keeper_incorrect_config/test.py b/tests/integration/test_keeper_incorrect_config/test.py\nindex cedb195a6e0f..95482745b314 100644\n--- a/tests/integration/test_keeper_incorrect_config/test.py\n+++ b/tests/integration/test_keeper_incorrect_config/test.py\n@@ -204,7 +204,7 @@ def started_cluster():\n \"\"\"\n \n \n-def test_duplicate_endpoint(started_cluster):\n+def test_invalid_configs(started_cluster):\n     node1.stop_clickhouse()\n \n     def assert_config_fails(config):\ndiff --git a/tests/integration/test_keeper_mntr_pressure/test.py b/tests/integration/test_keeper_mntr_pressure/test.py\nindex 471767210d6a..d351b238eade 100644\n--- a/tests/integration/test_keeper_mntr_pressure/test.py\n+++ b/tests/integration/test_keeper_mntr_pressure/test.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n \n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import pytest\n import random\n import string\n@@ -37,40 +38,22 @@ def started_cluster():\n         cluster.shutdown()\n \n \n-def get_keeper_socket(node_name):\n-    hosts = cluster.get_instance_ip(node_name)\n-    client = socket.socket()\n-    client.settimeout(10)\n-    client.connect((hosts, 9181))\n-    return client\n-\n-\n def close_keeper_socket(cli):\n     if cli is not None:\n         cli.close()\n \n \n-def send_4lw_cmd(node_name, cmd=\"ruok\"):\n-    client = None\n-    try:\n-        client = get_keeper_socket(node_name)\n-        client.send(cmd.encode())\n-        data = client.recv(100_000)\n-        data = data.decode()\n-        return data\n-    finally:\n-        if client is not None:\n-            client.close()\n-\n-\n def test_aggressive_mntr(started_cluster):\n-    def go_mntr(node_name):\n-        for _ in range(100000):\n-            print(node_name, send_4lw_cmd(node_name, \"mntr\"))\n-\n-    node1_thread = threading.Thread(target=lambda: go_mntr(node1.name))\n-    node2_thread = threading.Thread(target=lambda: go_mntr(node2.name))\n-    node3_thread = threading.Thread(target=lambda: go_mntr(node3.name))\n+    def go_mntr(node):\n+        for _ in range(10000):\n+            try:\n+                print(node.name, keeper_utils.send_4lw_cmd(cluster, node, \"mntr\"))\n+            except ConnectionRefusedError:\n+                pass\n+\n+    node1_thread = threading.Thread(target=lambda: go_mntr(node1))\n+    node2_thread = threading.Thread(target=lambda: go_mntr(node2))\n+    node3_thread = threading.Thread(target=lambda: go_mntr(node3))\n     node1_thread.start()\n     node2_thread.start()\n     node3_thread.start()\n@@ -78,8 +61,7 @@ def go_mntr(node_name):\n     node2.stop_clickhouse()\n     node3.stop_clickhouse()\n \n-    while send_4lw_cmd(node1.name, \"mntr\") != NOT_SERVING_REQUESTS_ERROR_MSG:\n-        time.sleep(0.2)\n+    keeper_utils.wait_until_quorum_lost(cluster, node1)\n \n     node1.stop_clickhouse()\n     starters = []\ndiff --git a/tests/integration/test_keeper_multinode_blocade_leader/test.py b/tests/integration/test_keeper_multinode_blocade_leader/test.py\nindex d6d01a5d0a64..a7a80d90a58e 100644\n--- a/tests/integration/test_keeper_multinode_blocade_leader/test.py\n+++ b/tests/integration/test_keeper_multinode_blocade_leader/test.py\n@@ -1,5 +1,6 @@\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -55,31 +56,6 @@ def smaller_exception(ex):\n     return \"\\n\".join(str(ex).split(\"\\n\")[0:2])\n \n \n-def wait_node(node):\n-    for _ in range(100):\n-        zk = None\n-        try:\n-            node.query(\"SELECT * FROM system.zookeeper WHERE path = '/'\")\n-            zk = get_fake_zk(node.name, timeout=30.0)\n-            zk.create(\"/test\", sequence=True)\n-            print(\"node\", node.name, \"ready\")\n-            break\n-        except Exception as ex:\n-            time.sleep(0.2)\n-            print(\"Waiting until\", node.name, \"will be ready, exception\", ex)\n-        finally:\n-            if zk:\n-                zk.stop()\n-                zk.close()\n-    else:\n-        raise Exception(\"Can't wait node\", node.name, \"to become ready\")\n-\n-\n-def wait_nodes():\n-    for node in [node1, node2, node3]:\n-        wait_node(node)\n-\n-\n def get_fake_zk(nodename, timeout=30.0):\n     _fake_zk_instance = KazooClient(\n         hosts=cluster.get_instance_ip(nodename) + \":9181\", timeout=timeout\n@@ -88,6 +64,10 @@ def get_fake_zk(nodename, timeout=30.0):\n     return _fake_zk_instance\n \n \n+def wait_nodes():\n+    keeper_utils.wait_nodes(cluster, [node1, node2, node3])\n+\n+\n # in extremely rare case it can take more than 5 minutes in debug build with sanitizer\n @pytest.mark.timeout(600)\n def test_blocade_leader(started_cluster):\ndiff --git a/tests/integration/test_keeper_multinode_simple/test.py b/tests/integration/test_keeper_multinode_simple/test.py\nindex 694600acc677..1dcbb290fa8c 100644\n--- a/tests/integration/test_keeper_multinode_simple/test.py\n+++ b/tests/integration/test_keeper_multinode_simple/test.py\n@@ -1,5 +1,6 @@\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -43,29 +44,8 @@ def smaller_exception(ex):\n     return \"\\n\".join(str(ex).split(\"\\n\")[0:2])\n \n \n-def wait_node(node):\n-    for _ in range(100):\n-        zk = None\n-        try:\n-            node.query(\"SELECT * FROM system.zookeeper WHERE path = '/'\")\n-            zk = get_fake_zk(node.name, timeout=30.0)\n-            zk.create(\"/test\", sequence=True)\n-            print(\"node\", node.name, \"ready\")\n-            break\n-        except Exception as ex:\n-            time.sleep(0.2)\n-            print(\"Waiting until\", node.name, \"will be ready, exception\", ex)\n-        finally:\n-            if zk:\n-                zk.stop()\n-                zk.close()\n-    else:\n-        raise Exception(\"Can't wait node\", node.name, \"to become ready\")\n-\n-\n def wait_nodes():\n-    for node in [node1, node2, node3]:\n-        wait_node(node)\n+    keeper_utils.wait_nodes(cluster, [node1, node2, node3])\n \n \n def get_fake_zk(nodename, timeout=30.0):\ndiff --git a/tests/integration/test_keeper_nodes_add/test.py b/tests/integration/test_keeper_nodes_add/test.py\nindex c3449534e872..aad674332ac3 100644\n--- a/tests/integration/test_keeper_nodes_add/test.py\n+++ b/tests/integration/test_keeper_nodes_add/test.py\n@@ -2,6 +2,7 @@\n \n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -41,9 +42,11 @@ def started_cluster():\n \n def start(node):\n     node.start_clickhouse()\n+    keeper_utils.wait_until_connected(cluster, node)\n \n \n def test_nodes_add(started_cluster):\n+    keeper_utils.wait_until_connected(cluster, node1)\n     zk_conn = get_fake_zk(node1)\n \n     for i in range(100):\n@@ -62,6 +65,7 @@ def test_nodes_add(started_cluster):\n     )\n     node1.query(\"SYSTEM RELOAD CONFIG\")\n     waiter.wait()\n+    keeper_utils.wait_until_connected(cluster, node2)\n \n     zk_conn2 = get_fake_zk(node2)\n \n@@ -93,6 +97,7 @@ def test_nodes_add(started_cluster):\n     node2.query(\"SYSTEM RELOAD CONFIG\")\n \n     waiter.wait()\n+    keeper_utils.wait_until_connected(cluster, node3)\n     zk_conn3 = get_fake_zk(node3)\n \n     for i in range(100):\ndiff --git a/tests/integration/test_keeper_nodes_move/test.py b/tests/integration/test_keeper_nodes_move/test.py\nindex 31082846fb8b..1e3bd95c5e78 100644\n--- a/tests/integration/test_keeper_nodes_move/test.py\n+++ b/tests/integration/test_keeper_nodes_move/test.py\n@@ -11,6 +11,7 @@\n import time\n from multiprocessing.dummy import Pool\n from helpers.test_tools import assert_eq_with_retry\n+import helpers.keeper_utils as keeper_utils\n from kazoo.client import KazooClient, KazooState\n \n cluster = ClickHouseCluster(__file__)\n@@ -41,6 +42,7 @@ def started_cluster():\n \n def start(node):\n     node.start_clickhouse()\n+    keeper_utils.wait_until_connected(cluster, node)\n \n \n def get_fake_zk(node, timeout=30.0):\ndiff --git a/tests/integration/test_keeper_nodes_remove/test.py b/tests/integration/test_keeper_nodes_remove/test.py\nindex 13303d320eb4..59bdaadf2e2b 100644\n--- a/tests/integration/test_keeper_nodes_remove/test.py\n+++ b/tests/integration/test_keeper_nodes_remove/test.py\n@@ -2,6 +2,7 @@\n \n import pytest\n from helpers.cluster import ClickHouseCluster\n+import time\n import os\n from kazoo.client import KazooClient, KazooState\n \n@@ -79,9 +80,12 @@ def test_nodes_remove(started_cluster):\n         assert zk_conn.exists(\"test_two_\" + str(i)) is not None\n         assert zk_conn.exists(\"test_two_\" + str(100 + i)) is not None\n \n-    with pytest.raises(Exception):\n+    try:\n         zk_conn3 = get_fake_zk(node3)\n         zk_conn3.sync(\"/test_two_0\")\n+        time.sleep(0.1)\n+    except Exception:\n+        pass\n \n     node3.stop_clickhouse()\n \n@@ -91,6 +95,7 @@ def test_nodes_remove(started_cluster):\n     )\n \n     node1.query(\"SYSTEM RELOAD CONFIG\")\n+\n     zk_conn = get_fake_zk(node1)\n     zk_conn.sync(\"/test_two_0\")\n \n@@ -98,8 +103,11 @@ def test_nodes_remove(started_cluster):\n         assert zk_conn.exists(\"test_two_\" + str(i)) is not None\n         assert zk_conn.exists(\"test_two_\" + str(100 + i)) is not None\n \n-    with pytest.raises(Exception):\n+    try:\n         zk_conn2 = get_fake_zk(node2)\n         zk_conn2.sync(\"/test_two_0\")\n+        time.sleep(0.1)\n+    except Exception:\n+        pass\n \n     node2.stop_clickhouse()\ndiff --git a/tests/integration/test_keeper_persistent_log/test.py b/tests/integration/test_keeper_persistent_log/test.py\nindex 377fa436a87a..70cc14fe26df 100644\n--- a/tests/integration/test_keeper_persistent_log/test.py\n+++ b/tests/integration/test_keeper_persistent_log/test.py\n@@ -46,6 +46,10 @@ def get_connection_zk(nodename, timeout=30.0):\n     return _fake_zk_instance\n \n \n+def restart_clickhouse():\n+    node.restart_clickhouse(kill=True)\n+\n+\n def test_state_after_restart(started_cluster):\n     try:\n         node_zk = None\n@@ -62,7 +66,7 @@ def test_state_after_restart(started_cluster):\n             if i % 7 == 0:\n                 node_zk.delete(\"/test_state_after_restart/node\" + str(i))\n \n-        node.restart_clickhouse(kill=True)\n+        restart_clickhouse()\n \n         node_zk2 = get_connection_zk(\"node\")\n \n@@ -111,7 +115,7 @@ def test_state_duplicate_restart(started_cluster):\n             if i % 7 == 0:\n                 node_zk.delete(\"/test_state_duplicated_restart/node\" + str(i))\n \n-        node.restart_clickhouse(kill=True)\n+        restart_clickhouse()\n \n         node_zk2 = get_connection_zk(\"node\")\n \n@@ -119,7 +123,7 @@ def test_state_duplicate_restart(started_cluster):\n         node_zk2.create(\"/test_state_duplicated_restart/just_test2\")\n         node_zk2.create(\"/test_state_duplicated_restart/just_test3\")\n \n-        node.restart_clickhouse(kill=True)\n+        restart_clickhouse()\n \n         node_zk3 = get_connection_zk(\"node\")\n \n@@ -159,6 +163,7 @@ def test_state_duplicate_restart(started_cluster):\n \n # http://zookeeper-user.578899.n2.nabble.com/Why-are-ephemeral-nodes-written-to-disk-tp7583403p7583418.html\n def test_ephemeral_after_restart(started_cluster):\n+\n     try:\n         node_zk = None\n         node_zk2 = None\n@@ -176,7 +181,7 @@ def test_ephemeral_after_restart(started_cluster):\n             if i % 7 == 0:\n                 node_zk.delete(\"/test_ephemeral_after_restart/node\" + str(i))\n \n-        node.restart_clickhouse(kill=True)\n+        restart_clickhouse()\n \n         node_zk2 = get_connection_zk(\"node\")\n \ndiff --git a/tests/integration/test_keeper_persistent_log_multinode/test.py b/tests/integration/test_keeper_persistent_log_multinode/test.py\nindex f15e772fd5f3..1552abd32e94 100644\n--- a/tests/integration/test_keeper_persistent_log_multinode/test.py\n+++ b/tests/integration/test_keeper_persistent_log_multinode/test.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -26,10 +27,15 @@\n from kazoo.client import KazooClient, KazooState\n \n \n+def wait_nodes():\n+    keeper_utils.wait_nodes(cluster, [node1, node2, node3])\n+\n+\n @pytest.fixture(scope=\"module\")\n def started_cluster():\n     try:\n         cluster.start()\n+        wait_nodes()\n \n         yield cluster\n \n@@ -100,6 +106,8 @@ def test_restart_multinode(started_cluster):\n     node1.restart_clickhouse(kill=True)\n     node2.restart_clickhouse(kill=True)\n     node3.restart_clickhouse(kill=True)\n+    wait_nodes()\n+\n     for i in range(100):\n         try:\n             node1_zk = get_fake_zk(\"node1\")\ndiff --git a/tests/integration/test_keeper_remove_leader/configs/enable_keeper1.xml b/tests/integration/test_keeper_remove_leader/configs/enable_keeper1.xml\ndeleted file mode 100644\nindex 1e57d42016d1..000000000000\n--- a/tests/integration/test_keeper_remove_leader/configs/enable_keeper1.xml\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>1</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>1</id>\n-                <hostname>node1</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_remove_leader/configs/enable_keeper2.xml b/tests/integration/test_keeper_remove_leader/configs/enable_keeper2.xml\ndeleted file mode 100644\nindex 98422b41c9b1..000000000000\n--- a/tests/integration/test_keeper_remove_leader/configs/enable_keeper2.xml\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>2</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>1</id>\n-                <hostname>node1</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_remove_leader/configs/enable_keeper3.xml b/tests/integration/test_keeper_remove_leader/configs/enable_keeper3.xml\ndeleted file mode 100644\nindex 43800bd2dfb6..000000000000\n--- a/tests/integration/test_keeper_remove_leader/configs/enable_keeper3.xml\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>3</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>1</id>\n-                <hostname>node1</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_1.xml b/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_1.xml\ndeleted file mode 100644\nindex d51e420f7331..000000000000\n--- a/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_1.xml\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>1</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_2.xml b/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_2.xml\ndeleted file mode 100644\nindex 3f1ee1e01a8a..000000000000\n--- a/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_2.xml\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>2</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_3.xml b/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_3.xml\ndeleted file mode 100644\nindex a99bd5d5296b..000000000000\n--- a/tests/integration/test_keeper_remove_leader/configs/enable_keeper_two_nodes_3.xml\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>3</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_restore_from_snapshot/test.py b/tests/integration/test_keeper_restore_from_snapshot/test.py\nindex 7270c84bdda3..bc33689dd200 100644\n--- a/tests/integration/test_keeper_restore_from_snapshot/test.py\n+++ b/tests/integration/test_keeper_restore_from_snapshot/test.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -84,6 +85,7 @@ def test_recover_from_snapshot(started_cluster):\n     # stale node should recover from leader's snapshot\n     # with some sanitizers can start longer than 5 seconds\n     node3.start_clickhouse(20)\n+    keeper_utils.wait_until_connected(cluster, node3)\n     print(\"Restarted\")\n \n     try:\ndiff --git a/tests/integration/test_keeper_secure_client/test.py b/tests/integration/test_keeper_secure_client/test.py\nindex 55e00880da04..2a17afac75ba 100644\n--- a/tests/integration/test_keeper_secure_client/test.py\n+++ b/tests/integration/test_keeper_secure_client/test.py\n@@ -40,4 +40,4 @@ def started_cluster():\n \n def test_connection(started_cluster):\n     # just nothrow\n-    node2.query(\"SELECT * FROM system.zookeeper WHERE path = '/'\")\n+    node2.query_with_retry(\"SELECT * FROM system.zookeeper WHERE path = '/'\")\ndiff --git a/tests/integration/test_keeper_session/test.py b/tests/integration/test_keeper_session/test.py\nindex 4b3aa7e3fdfd..72a162c17656 100644\n--- a/tests/integration/test_keeper_session/test.py\n+++ b/tests/integration/test_keeper_session/test.py\n@@ -1,5 +1,6 @@\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import time\n import socket\n import struct\n@@ -52,25 +53,8 @@ def destroy_zk_client(zk):\n         pass\n \n \n-def wait_node(node):\n-    for _ in range(100):\n-        zk = None\n-        try:\n-            zk = get_fake_zk(node.name, timeout=30.0)\n-            print(\"node\", node.name, \"ready\")\n-            break\n-        except Exception as ex:\n-            time.sleep(0.2)\n-            print(\"Waiting until\", node.name, \"will be ready, exception\", ex)\n-        finally:\n-            destroy_zk_client(zk)\n-    else:\n-        raise Exception(\"Can't wait node\", node.name, \"to become ready\")\n-\n-\n def wait_nodes():\n-    for n in [node1, node2, node3]:\n-        wait_node(n)\n+    keeper_utils.wait_nodes(cluster, [node1, node2, node3])\n \n \n def get_fake_zk(nodename, timeout=30.0):\ndiff --git a/tests/integration/test_keeper_snapshot_small_distance/test.py b/tests/integration/test_keeper_snapshot_small_distance/test.py\nindex 4351c5ac96f8..6a64cf0ac92f 100644\n--- a/tests/integration/test_keeper_snapshot_small_distance/test.py\n+++ b/tests/integration/test_keeper_snapshot_small_distance/test.py\n@@ -2,6 +2,7 @@\n ##!/usr/bin/env python3\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n from multiprocessing.dummy import Pool\n from kazoo.client import KazooClient, KazooState\n import random\n@@ -22,7 +23,7 @@\n \n \n def start_zookeeper(node):\n-    node1.exec_in_container([\"bash\", \"-c\", \"/opt/zookeeper/bin/zkServer.sh start\"])\n+    node.exec_in_container([\"bash\", \"-c\", \"/opt/zookeeper/bin/zkServer.sh start\"])\n \n \n def stop_zookeeper(node):\n@@ -66,6 +67,7 @@ def stop_clickhouse(node):\n \n def start_clickhouse(node):\n     node.start_clickhouse()\n+    keeper_utils.wait_until_connected(cluster, node)\n \n \n def copy_zookeeper_data(make_zk_snapshots, node):\ndiff --git a/tests/integration/test_keeper_snapshots/test.py b/tests/integration/test_keeper_snapshots/test.py\nindex 08f60e538a4a..ce57a852dcab 100644\n--- a/tests/integration/test_keeper_snapshots/test.py\n+++ b/tests/integration/test_keeper_snapshots/test.py\n@@ -3,6 +3,7 @@\n #!/usr/bin/env python3\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -50,6 +51,11 @@ def get_connection_zk(nodename, timeout=30.0):\n     return _fake_zk_instance\n \n \n+def restart_clickhouse():\n+    node.restart_clickhouse(kill=True)\n+    keeper_utils.wait_until_connected(cluster, node)\n+\n+\n def test_state_after_restart(started_cluster):\n     try:\n         node_zk = None\n@@ -69,7 +75,7 @@ def test_state_after_restart(started_cluster):\n             else:\n                 existing_children.append(\"node\" + str(i))\n \n-        node.restart_clickhouse(kill=True)\n+        restart_clickhouse()\n \n         node_zk2 = get_connection_zk(\"node\")\n \n@@ -123,7 +129,7 @@ def test_ephemeral_after_restart(started_cluster):\n             else:\n                 existing_children.append(\"node\" + str(i))\n \n-        node.restart_clickhouse(kill=True)\n+        restart_clickhouse()\n \n         node_zk2 = get_connection_zk(\"node\")\n \ndiff --git a/tests/integration/test_keeper_snapshots_multinode/test.py b/tests/integration/test_keeper_snapshots_multinode/test.py\nindex 1461f35e6a47..a68a34dae2e5 100644\n--- a/tests/integration/test_keeper_snapshots_multinode/test.py\n+++ b/tests/integration/test_keeper_snapshots_multinode/test.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -20,6 +21,10 @@\n from kazoo.client import KazooClient, KazooState\n \n \n+def wait_nodes():\n+    keeper_utils.wait_nodes(cluster, [node1, node2, node3])\n+\n+\n @pytest.fixture(scope=\"module\")\n def started_cluster():\n     try:\n@@ -94,6 +99,8 @@ def test_restart_multinode(started_cluster):\n     node1.restart_clickhouse(kill=True)\n     node2.restart_clickhouse(kill=True)\n     node3.restart_clickhouse(kill=True)\n+    wait_nodes()\n+\n     for i in range(100):\n         try:\n             node1_zk = get_fake_zk(\"node1\")\ndiff --git a/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper1.xml b/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper1.xml\ndeleted file mode 100644\nindex 1e57d42016d1..000000000000\n--- a/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper1.xml\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>1</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>1</id>\n-                <hostname>node1</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper2.xml b/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper2.xml\ndeleted file mode 100644\nindex 98422b41c9b1..000000000000\n--- a/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper2.xml\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>2</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>1</id>\n-                <hostname>node1</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper3.xml b/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper3.xml\ndeleted file mode 100644\nindex 43800bd2dfb6..000000000000\n--- a/tests/integration/test_keeper_start_as_follower_multinode/configs/enable_keeper3.xml\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-<clickhouse>\n-    <keeper_server>\n-        <tcp_port>9181</tcp_port>\n-        <server_id>3</server_id>\n-        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n-        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n-\n-        <coordination_settings>\n-            <operation_timeout_ms>5000</operation_timeout_ms>\n-            <session_timeout_ms>10000</session_timeout_ms>\n-            <raft_logs_level>trace</raft_logs_level>\n-        </coordination_settings>\n-\n-        <raft_configuration>\n-            <server>\n-                <id>1</id>\n-                <hostname>node1</hostname>\n-                <port>9234</port>\n-            </server>\n-            <server>\n-                <id>2</id>\n-                <hostname>node2</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-            <server>\n-                <id>3</id>\n-                <hostname>node3</hostname>\n-                <port>9234</port>\n-                <start_as_follower>true</start_as_follower>\n-            </server>\n-        </raft_configuration>\n-    </keeper_server>\n-</clickhouse>\ndiff --git a/tests/integration/test_keeper_three_nodes_two_alive/test.py b/tests/integration/test_keeper_three_nodes_two_alive/test.py\nindex f1de469c5a12..bd29ded357fa 100644\n--- a/tests/integration/test_keeper_three_nodes_two_alive/test.py\n+++ b/tests/integration/test_keeper_three_nodes_two_alive/test.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -48,6 +49,7 @@ def started_cluster():\n \n def start(node):\n     node.start_clickhouse()\n+    keeper_utils.wait_until_connected(cluster, node)\n \n \n def delete_with_retry(node_name, path):\n@@ -138,6 +140,7 @@ def test_restart_third_node(started_cluster):\n     node1_zk.create(\"/test_restart\", b\"aaaa\")\n \n     node3.restart_clickhouse()\n+    keeper_utils.wait_until_connected(cluster, node3)\n \n     assert node3.contains_in_log(\n         \"Connected to ZooKeeper (or Keeper) before internal Keeper start\"\ndiff --git a/tests/integration/test_keeper_two_nodes_cluster/test.py b/tests/integration/test_keeper_two_nodes_cluster/test.py\nindex 8c0276f7d775..c6bc0ebd33a9 100644\n--- a/tests/integration/test_keeper_two_nodes_cluster/test.py\n+++ b/tests/integration/test_keeper_two_nodes_cluster/test.py\n@@ -2,6 +2,7 @@\n \n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -40,29 +41,8 @@ def smaller_exception(ex):\n     return \"\\n\".join(str(ex).split(\"\\n\")[0:2])\n \n \n-def wait_node(node):\n-    for _ in range(100):\n-        zk = None\n-        try:\n-            node.query(\"SELECT * FROM system.zookeeper WHERE path = '/'\")\n-            zk = get_fake_zk(node.name, timeout=30.0)\n-            zk.create(\"/test\", sequence=True)\n-            print(\"node\", node.name, \"ready\")\n-            break\n-        except Exception as ex:\n-            time.sleep(0.2)\n-            print(\"Waiting until\", node.name, \"will be ready, exception\", ex)\n-        finally:\n-            if zk:\n-                zk.stop()\n-                zk.close()\n-    else:\n-        raise Exception(\"Can't wait node\", node.name, \"to become ready\")\n-\n-\n def wait_nodes():\n-    for node in [node1, node2]:\n-        wait_node(node)\n+    keeper_utils.wait_nodes(cluster, [node1, node2])\n \n \n def get_fake_zk(nodename, timeout=30.0):\ndiff --git a/tests/integration/test_keeper_znode_time/test.py b/tests/integration/test_keeper_znode_time/test.py\nindex bff3d52014e1..f2076acc4d26 100644\n--- a/tests/integration/test_keeper_znode_time/test.py\n+++ b/tests/integration/test_keeper_znode_time/test.py\n@@ -1,5 +1,6 @@\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n import random\n import string\n import os\n@@ -42,29 +43,8 @@ def smaller_exception(ex):\n     return \"\\n\".join(str(ex).split(\"\\n\")[0:2])\n \n \n-def wait_node(node):\n-    for _ in range(100):\n-        zk = None\n-        try:\n-            node.query(\"SELECT * FROM system.zookeeper WHERE path = '/'\")\n-            zk = get_fake_zk(node.name, timeout=30.0)\n-            zk.create(\"/test\", sequence=True)\n-            print(\"node\", node.name, \"ready\")\n-            break\n-        except Exception as ex:\n-            time.sleep(0.2)\n-            print(\"Waiting until\", node.name, \"will be ready, exception\", ex)\n-        finally:\n-            if zk:\n-                zk.stop()\n-                zk.close()\n-    else:\n-        raise Exception(\"Can't wait node\", node.name, \"to become ready\")\n-\n-\n def wait_nodes():\n-    for node in [node1, node2, node3]:\n-        wait_node(node)\n+    keeper_utils.wait_nodes(cluster, [node1, node2, node3])\n \n \n def get_fake_zk(nodename, timeout=30.0):\n@@ -129,6 +109,7 @@ def test_server_restart(started_cluster):\n             node1_zk.set(\"/test_server_restart/\" + str(child_node), b\"somevalue\")\n \n         node3.restart_clickhouse(kill=True)\n+        keeper_utils.wait_until_connected(cluster, node3)\n \n         node2_zk = get_fake_zk(\"node2\")\n         node3_zk = get_fake_zk(\"node3\")\ndiff --git a/tests/integration/test_keeper_zookeeper_converter/test.py b/tests/integration/test_keeper_zookeeper_converter/test.py\nindex 50a9ee6a4a73..af8d1ca4bf9d 100644\n--- a/tests/integration/test_keeper_zookeeper_converter/test.py\n+++ b/tests/integration/test_keeper_zookeeper_converter/test.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n import pytest\n from helpers.cluster import ClickHouseCluster\n+import helpers.keeper_utils as keeper_utils\n from kazoo.client import KazooClient, KazooState\n from kazoo.security import ACL, make_digest_acl, make_acl\n from kazoo.exceptions import (\n@@ -60,6 +61,7 @@ def stop_clickhouse():\n \n def start_clickhouse():\n     node.start_clickhouse()\n+    keeper_utils.wait_until_connected(cluster, node)\n \n \n def copy_zookeeper_data(make_zk_snapshots):\n",
  "problem_statement": "Keeper startup behaviour.\n(tested on 22.9)\r\n\r\nApache Zookeeper service starts and waits for peers (quorum). It waits forever and does not serve clients requests until get the quorum.\r\n\r\nClickhouse-keeper has a different behavior. It creates a lot of hustle, you need to start all keeper nodes at about the same time, otherwise the first node will go down while you start the second node.\r\n\r\nAnother problem with embedded keeper. \r\n\r\nImagine you have TWO nodes of Clickhouse with embedded keeper (both). It works great despite only having two nodes.\r\nAnd if one node is missing, the other node becomes read-only, and that's expected, and that node continues to serve SELECT queries without issue indefinitely.\r\nBUT if you try to restart this node, it is impossible, because Clickhouse does not start due to the embedded keeper and after the timeout, embedded keeper shuts down the instance.\r\n\r\nIf the keeper is not embedded,   then this single Clickhouse will start up and continue serving the select requests.\r\n\r\nThis is inconsistent behavior - Clickhouse serves select requests normally, but restarting breaks it.\r\n\r\nIt would be nice to implement the zookeeper behavior, the Clickhouse-keeper process starts and waits indefinitely for a RAFT quorum and does not process client requests if there is no quorum. This will allow the Clickhouse server with embedded keeper to start normally in read-only mode.\r\n\n",
  "hints_text": "",
  "created_at": "2022-09-05T08:29:50Z"
}