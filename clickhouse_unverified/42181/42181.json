{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 42181,
  "instance_id": "ClickHouse__ClickHouse-42181",
  "issue_numbers": [
    "4699"
  ],
  "base_commit": "4b28b39f7c6e25291db99868b68b4dbc054c2861",
  "patch": "diff --git a/src/Access/Common/AccessType.h b/src/Access/Common/AccessType.h\nindex 366667410d57..f1f99fc91665 100644\n--- a/src/Access/Common/AccessType.h\n+++ b/src/Access/Common/AccessType.h\n@@ -167,6 +167,7 @@ enum class AccessType\n     M(SYSTEM_SYNC_REPLICA, \"SYNC REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_RESTART_REPLICA, \"RESTART REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_RESTORE_REPLICA, \"RESTORE REPLICA\", TABLE, SYSTEM) \\\n+    M(SYSTEM_WAIT_LOADING_PARTS, \"WAIT LOADING PARTS\", TABLE, SYSTEM) \\\n     M(SYSTEM_SYNC_DATABASE_REPLICA, \"SYNC DATABASE REPLICA\", DATABASE, SYSTEM) \\\n     M(SYSTEM_SYNC_TRANSACTION_LOG, \"SYNC TRANSACTION LOG\", GLOBAL, SYSTEM) \\\n     M(SYSTEM_FLUSH_DISTRIBUTED, \"FLUSH DISTRIBUTED\", TABLE, SYSTEM_FLUSH) \\\ndiff --git a/src/Interpreters/InterpreterSystemQuery.cpp b/src/Interpreters/InterpreterSystemQuery.cpp\nindex 1f1ef68492c1..a82a11e7c970 100644\n--- a/src/Interpreters/InterpreterSystemQuery.cpp\n+++ b/src/Interpreters/InterpreterSystemQuery.cpp\n@@ -487,7 +487,7 @@ BlockIO InterpreterSystemQuery::execute()\n             dropDatabaseReplica(query);\n             break;\n         case Type::SYNC_REPLICA:\n-            syncReplica(query);\n+            syncReplica();\n             break;\n         case Type::SYNC_DATABASE_REPLICA:\n             syncReplicatedDatabase(query);\n@@ -507,6 +507,9 @@ BlockIO InterpreterSystemQuery::execute()\n         case Type::RESTORE_REPLICA:\n             restoreReplica();\n             break;\n+        case Type::WAIT_LOADING_PARTS:\n+            waitLoadingParts();\n+            break;\n         case Type::RESTART_DISK:\n             restartDisk(query.disk);\n         case Type::FLUSH_LOGS:\n@@ -852,7 +855,7 @@ void InterpreterSystemQuery::dropDatabaseReplica(ASTSystemQuery & query)\n         throw Exception(\"Invalid query\", ErrorCodes::LOGICAL_ERROR);\n }\n \n-void InterpreterSystemQuery::syncReplica(ASTSystemQuery &)\n+void InterpreterSystemQuery::syncReplica()\n {\n     getContext()->checkAccess(AccessType::SYSTEM_SYNC_REPLICA, table_id);\n     StoragePtr table = DatabaseCatalog::instance().getTable(table_id, getContext());\n@@ -872,6 +875,23 @@ void InterpreterSystemQuery::syncReplica(ASTSystemQuery &)\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, table_is_not_replicated.data(), table_id.getNameForLogs());\n }\n \n+void InterpreterSystemQuery::waitLoadingParts()\n+{\n+    getContext()->checkAccess(AccessType::SYSTEM_WAIT_LOADING_PARTS, table_id);\n+    StoragePtr table = DatabaseCatalog::instance().getTable(table_id, getContext());\n+\n+    if (auto * merge_tree = dynamic_cast<MergeTreeData *>(table.get()))\n+    {\n+        LOG_TRACE(log, \"Waiting for loading of parts of table {}\", table_id.getFullTableName());\n+        merge_tree->waitForOutdatedPartsToBeLoaded();\n+        LOG_TRACE(log, \"Finished waiting for loading of parts of table {}\", table_id.getFullTableName());\n+    }\n+    else\n+    {\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"Command WAIT LOADING PARTS is supported only for MergeTree table, but got: {}\", table->getName());\n+    }\n+}\n \n void InterpreterSystemQuery::syncReplicatedDatabase(ASTSystemQuery & query)\n {\n@@ -1071,6 +1091,11 @@ AccessRightsElements InterpreterSystemQuery::getRequiredAccessForDDLOnCluster()\n             required_access.emplace_back(AccessType::SYSTEM_RESTART_REPLICA);\n             break;\n         }\n+        case Type::WAIT_LOADING_PARTS:\n+        {\n+            required_access.emplace_back(AccessType::SYSTEM_WAIT_LOADING_PARTS, query.getDatabase(), query.getTable());\n+            break;\n+        }\n         case Type::SYNC_DATABASE_REPLICA:\n         {\n             required_access.emplace_back(AccessType::SYSTEM_SYNC_DATABASE_REPLICA, query.getDatabase());\ndiff --git a/src/Interpreters/InterpreterSystemQuery.h b/src/Interpreters/InterpreterSystemQuery.h\nindex 0058d0c9defd..5673890daf38 100644\n--- a/src/Interpreters/InterpreterSystemQuery.h\n+++ b/src/Interpreters/InterpreterSystemQuery.h\n@@ -56,7 +56,8 @@ class InterpreterSystemQuery : public IInterpreter, WithMutableContext\n \n     void restartReplica(const StorageID & replica, ContextMutablePtr system_context);\n     void restartReplicas(ContextMutablePtr system_context);\n-    void syncReplica(ASTSystemQuery & query);\n+    void syncReplica();\n+    void waitLoadingParts();\n \n     void syncReplicatedDatabase(ASTSystemQuery & query);\n \ndiff --git a/src/Interpreters/MergeTreeTransaction.cpp b/src/Interpreters/MergeTreeTransaction.cpp\nindex f438194b87b2..f16ece46530e 100644\n--- a/src/Interpreters/MergeTreeTransaction.cpp\n+++ b/src/Interpreters/MergeTreeTransaction.cpp\n@@ -303,7 +303,6 @@ bool MergeTreeTransaction::rollback() noexcept\n         part->version.unlockRemovalTID(tid, TransactionInfoContext{part->storage.getStorageID(), part->name});\n     }\n \n-\n     assert([&]()\n     {\n         std::lock_guard lock{mutex};\ndiff --git a/src/Parsers/ASTSystemQuery.cpp b/src/Parsers/ASTSystemQuery.cpp\nindex 5ed77f48ceb4..bfc7c5e6a45c 100644\n--- a/src/Parsers/ASTSystemQuery.cpp\n+++ b/src/Parsers/ASTSystemQuery.cpp\n@@ -166,6 +166,7 @@ void ASTSystemQuery::formatImpl(const FormatSettings & settings, FormatState &,\n     else if (  type == Type::RESTART_REPLICA\n             || type == Type::RESTORE_REPLICA\n             || type == Type::SYNC_REPLICA\n+            || type == Type::WAIT_LOADING_PARTS\n             || type == Type::FLUSH_DISTRIBUTED\n             || type == Type::RELOAD_DICTIONARY\n             || type == Type::RELOAD_MODEL\ndiff --git a/src/Parsers/ASTSystemQuery.h b/src/Parsers/ASTSystemQuery.h\nindex 76788fd31fe4..ae08fe464adb 100644\n--- a/src/Parsers/ASTSystemQuery.h\n+++ b/src/Parsers/ASTSystemQuery.h\n@@ -35,6 +35,7 @@ class ASTSystemQuery : public IAST, public ASTQueryWithOnCluster\n         RESTART_REPLICAS,\n         RESTART_REPLICA,\n         RESTORE_REPLICA,\n+        WAIT_LOADING_PARTS,\n         DROP_REPLICA,\n         DROP_DATABASE_REPLICA,\n         SYNC_REPLICA,\ndiff --git a/src/Parsers/ParserSystemQuery.cpp b/src/Parsers/ParserSystemQuery.cpp\nindex a44516fc4a37..0eb263869b11 100644\n--- a/src/Parsers/ParserSystemQuery.cpp\n+++ b/src/Parsers/ParserSystemQuery.cpp\n@@ -253,6 +253,7 @@ bool ParserSystemQuery::parseImpl(IParser::Pos & pos, ASTPtr & node, Expected &\n \n         case Type::RESTART_REPLICA:\n         case Type::SYNC_REPLICA:\n+        case Type::WAIT_LOADING_PARTS:\n         {\n             if (!parseQueryWithOnCluster(res, pos, expected))\n                 return false;\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 2dcc0a560fbc..248e0f0426a8 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -18,6 +18,7 @@\n #include <Functions/IFunction.h>\n #include <IO/Operators.h>\n #include <IO/WriteBufferFromString.h>\n+#include <IO/S3Common.h>\n #include <Interpreters/Aggregator.h>\n #include <Interpreters/ExpressionAnalyzer.h>\n #include <Interpreters/PartLog.h>\n@@ -155,7 +156,10 @@ namespace ErrorCodes\n     extern const int INCORRECT_QUERY;\n     extern const int CANNOT_RESTORE_TABLE;\n     extern const int ZERO_COPY_REPLICATION_ERROR;\n+    extern const int NOT_INITIALIZED;\n     extern const int SERIALIZATION_ERROR;\n+    extern const int NETWORK_ERROR;\n+    extern const int SOCKET_TIMEOUT;\n }\n \n \n@@ -947,13 +951,313 @@ Int64 MergeTreeData::getMaxBlockNumber() const\n     return max_block_num;\n }\n \n-void MergeTreeData::loadDataPartsFromDisk(\n-    MutableDataPartsVector & broken_parts_to_detach,\n-    MutableDataPartsVector & duplicate_parts_to_remove,\n+void MergeTreeData::PartLoadingTree::add(const MergeTreePartInfo & info, const String & name, const DiskPtr & disk)\n+{\n+    auto & current_ptr = root_by_partition[info.partition_id];\n+    if (!current_ptr)\n+        current_ptr = std::make_shared<Node>(MergeTreePartInfo{}, \"\", disk);\n+\n+    auto * current = current_ptr.get();\n+    while (true)\n+    {\n+        auto it = current->children.lower_bound(info);\n+        if (it != current->children.begin())\n+        {\n+            auto prev = std::prev(it);\n+            const auto & prev_info = prev->first;\n+\n+            if (prev_info.contains(info))\n+            {\n+                current = prev->second.get();\n+                continue;\n+            }\n+            else if (!prev_info.isDisjoint(info))\n+            {\n+                throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                    \"Part {} intersects previous part {}. It is a bug!\",\n+                    name, prev->second->name);\n+            }\n+        }\n+\n+        if (it != current->children.end())\n+        {\n+            const auto & next_info = it->first;\n+\n+            if (next_info.contains(info))\n+            {\n+                current = it->second.get();\n+                continue;\n+            }\n+            else if (!next_info.isDisjoint(info))\n+            {\n+                throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                    \"Part {} intersects next part {}. It is a bug!\",\n+                    name, it->second->name);\n+            }\n+        }\n+\n+        current->children.emplace(info, std::make_shared<Node>(info, name, disk));\n+        break;\n+    }\n+}\n+\n+template <typename Func>\n+void MergeTreeData::PartLoadingTree::traverse(bool recursive, Func && func)\n+{\n+    std::function<void(const NodePtr &)> traverse_impl = [&](const auto & node)\n+    {\n+        func(node);\n+        if (recursive)\n+            for (const auto & [_, child] : node->children)\n+                traverse_impl(child);\n+    };\n+\n+    for (const auto & elem : root_by_partition)\n+        for (const auto & [_, node] : elem.second->children)\n+            traverse_impl(node);\n+}\n+\n+MergeTreeData::PartLoadingTree\n+MergeTreeData::PartLoadingTree::build(PartLoadingInfos nodes)\n+{\n+    std::sort(nodes.begin(), nodes.end(), [](const auto & lhs, const auto & rhs)\n+    {\n+        return std::tie(lhs.info.level, lhs.info.mutation) > std::tie(rhs.info.level, rhs.info.mutation);\n+    });\n+\n+    PartLoadingTree tree;\n+    for (const auto & [info, name, disk] : nodes)\n+        tree.add(info, name, disk);\n+    return tree;\n+}\n+\n+static std::optional<size_t> calculatePartSizeSafe(\n+    const MergeTreeData::DataPartPtr & part, Poco::Logger * log)\n+{\n+    try\n+    {\n+        return part->getDataPartStorage().calculateTotalSizeOnDisk();\n+    }\n+    catch (...)\n+    {\n+        tryLogCurrentException(log, fmt::format(\"while calculating part size {} on path {}\",\n+            part->name, part->getDataPartStorage().getRelativePath()));\n+        return {};\n+    }\n+}\n+\n+static void preparePartForRemoval(const MergeTreeMutableDataPartPtr & part)\n+{\n+    part->remove_time.store(part->modification_time, std::memory_order_relaxed);\n+    auto creation_csn = part->version.creation_csn.load(std::memory_order_relaxed);\n+    if (creation_csn != Tx::RolledBackCSN && creation_csn != Tx::PrehistoricCSN && !part->version.isRemovalTIDLocked())\n+    {\n+        /// It's possible that covering part was created without transaction,\n+        /// but if covered part was created with transaction (i.e. creation_tid is not prehistoric),\n+        /// then it must have removal tid in metadata file.\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Data part {} is Outdated and has creation TID {} and CSN {}, \"\n+                        \"but does not have removal tid. It's a bug or a result of manual intervention.\",\n+                        part->name, part->version.creation_tid, creation_csn);\n+    }\n+\n+    /// Explicitly set removal_tid_lock for parts w/o transaction (i.e. w/o txn_version.txt)\n+    /// to avoid keeping part forever (see VersionMetadata::canBeRemoved())\n+    if (!part->version.isRemovalTIDLocked())\n+    {\n+        TransactionInfoContext transaction_context{part->storage.getStorageID(), part->name};\n+        part->version.lockRemovalTID(Tx::PrehistoricTID, transaction_context);\n+    }\n+}\n+\n+static bool isRetryableException(const Exception & e)\n+{\n+    if (isNotEnoughMemoryErrorCode(e.code()))\n+        return true;\n+\n+    if (e.code() == ErrorCodes::NETWORK_ERROR || e.code() == ErrorCodes::SOCKET_TIMEOUT)\n+        return true;\n+\n+#if USE_AWS_S3\n+    const auto * s3_exception = dynamic_cast<const S3Exception *>(&e);\n+    if (s3_exception && s3_exception->isRetryableError())\n+        return true;\n+#endif\n+\n+    /// In fact, there can be other similar situations.\n+    /// But it is OK, because there is a safety guard against deleting too many parts.\n+    return false;\n+}\n+\n+MergeTreeData::LoadPartResult MergeTreeData::loadDataPart(\n+    const MergeTreePartInfo & part_info,\n+    const String & part_name,\n+    const DiskPtr & part_disk_ptr,\n+    MergeTreeDataPartState to_state,\n+    std::mutex & part_loading_mutex)\n+{\n+    LOG_TRACE(log, \"Loading {} part {} from disk {}\", magic_enum::enum_name(to_state), part_name, part_disk_ptr->getName());\n+\n+    LoadPartResult res;\n+    auto single_disk_volume = std::make_shared<SingleDiskVolume>(\"volume_\" + part_name, part_disk_ptr, 0);\n+    auto data_part_storage = std::make_shared<DataPartStorageOnDisk>(single_disk_volume, relative_data_path, part_name);\n+\n+    res.part = createPart(part_name, part_info, data_part_storage);\n+\n+    String part_path = fs::path(relative_data_path) / part_name;\n+    String marker_path = fs::path(part_path) / IMergeTreeDataPart::DELETE_ON_DESTROY_MARKER_FILE_NAME;\n+\n+    if (part_disk_ptr->exists(marker_path))\n+    {\n+        /// NOTE: getBytesOnDisk() cannot be used here, since it may be zero if checksums.txt does not exist.\n+        res.size_of_part = calculatePartSizeSafe(res.part, log);\n+        res.is_broken = true;\n+\n+        auto part_size_str = res.size_of_part ? formatReadableSizeWithBinarySuffix(*res.size_of_part) : \"failed to calculate size\";\n+\n+        LOG_WARNING(log,\n+            \"Detaching stale part {} (size: {}), which should have been deleted after a move. \"\n+            \"That can only happen after unclean restart of ClickHouse after move of a part having an operation blocking that stale copy of part.\",\n+            res.part->getDataPartStorage().getFullPath(), part_size_str);\n+\n+        return res;\n+    }\n+\n+    try\n+    {\n+        res.part->loadColumnsChecksumsIndexes(require_part_metadata, true);\n+    }\n+    catch (const Exception & e)\n+    {\n+        /// Don't count the part as broken if there was a retryalbe error\n+        /// during loading, such as \"not enough memory\" or network error.\n+        if (isRetryableException(e))\n+            throw;\n+\n+        res.is_broken = true;\n+        tryLogCurrentException(log, fmt::format(\"while loading part {} on path {}\", res.part->name, part_path));\n+    }\n+    catch (...)\n+    {\n+        res.is_broken = true;\n+        tryLogCurrentException(log, fmt::format(\"while loading part {} on path {}\", res.part->name, part_path));\n+    }\n+\n+    /// Ignore broken parts that can appear as a result of hard server restart.\n+    if (res.is_broken)\n+    {\n+        res.size_of_part = calculatePartSizeSafe(res.part, log);\n+        auto part_size_str = res.size_of_part ? formatReadableSizeWithBinarySuffix(*res.size_of_part) : \"failed to calculate size\";\n+\n+        LOG_ERROR(log,\n+            \"Detaching broken part {}{} (size: {}). \"\n+            \"If it happened after update, it is likely because of backward incompatibility. \"\n+            \"You need to resolve this manually\",\n+            getFullPathOnDisk(part_disk_ptr), part_name, part_size_str);\n+\n+        return res;\n+    }\n+\n+    res.part->modification_time = part_disk_ptr->getLastModified(fs::path(relative_data_path) / part_name).epochTime();\n+    res.part->loadVersionMetadata();\n+\n+    if (res.part->wasInvolvedInTransaction())\n+    {\n+        /// Check if CSNs were written after committing transaction, update and write if needed.\n+        bool version_updated = false;\n+        auto & version = res.part->version;\n+        chassert(!version.creation_tid.isEmpty());\n+\n+        if (!res.part->version.creation_csn)\n+        {\n+            auto min = TransactionLog::getCSN(res.part->version.creation_tid);\n+            if (!min)\n+            {\n+                /// Transaction that created this part was not committed. Remove part.\n+                TransactionLog::assertTIDIsNotOutdated(res.part->version.creation_tid);\n+                min = Tx::RolledBackCSN;\n+            }\n+\n+            LOG_TRACE(log, \"Will fix version metadata of {} after unclean restart: part has creation_tid={}, setting creation_csn={}\",\n+                        res.part->name, res.part->version.creation_tid, min);\n+\n+            version.creation_csn = min;\n+            version_updated = true;\n+        }\n+\n+        if (!version.removal_tid.isEmpty() && !version.removal_csn)\n+        {\n+            auto max = TransactionLog::getCSN(version.removal_tid);\n+            if (max)\n+            {\n+                LOG_TRACE(log, \"Will fix version metadata of {} after unclean restart: part has removal_tid={}, setting removal_csn={}\",\n+                            res.part->name, version.removal_tid, max);\n+                version.removal_csn = max;\n+            }\n+            else\n+            {\n+                TransactionLog::assertTIDIsNotOutdated(version.removal_tid);\n+                /// Transaction that tried to remove this part was not committed. Clear removal_tid.\n+                LOG_TRACE(log, \"Will fix version metadata of {} after unclean restart: clearing removal_tid={}\",\n+                            res.part->name, version.removal_tid);\n+                version.unlockRemovalTID(version.removal_tid, TransactionInfoContext{getStorageID(), res.part->name});\n+            }\n+\n+            version_updated = true;\n+        }\n+\n+        /// Sanity checks\n+        bool csn_order = !version.removal_csn || version.creation_csn <= version.removal_csn || version.removal_csn == Tx::PrehistoricCSN;\n+        bool min_start_csn_order = version.creation_tid.start_csn <= version.creation_csn;\n+        bool max_start_csn_order = version.removal_tid.start_csn <= version.removal_csn;\n+        bool creation_csn_known = version.creation_csn;\n+        if (!csn_order || !min_start_csn_order || !max_start_csn_order || !creation_csn_known)\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Part {} has invalid version metadata: {}\", res.part->name, version.toString());\n+\n+        if (version_updated)\n+            res.part->storeVersionMetadata(/* force */ true);\n+\n+        /// Deactivate part if creation was not committed or if removal was.\n+        if (version.creation_csn == Tx::RolledBackCSN || version.removal_csn)\n+        {\n+            preparePartForRemoval(res.part);\n+            to_state = DataPartState::Outdated;\n+        }\n+    }\n+\n+    res.part->setState(to_state);\n+\n+    DataPartIteratorByInfo it;\n+    bool inserted;\n+\n+    {\n+        std::lock_guard lock(part_loading_mutex);\n+        std::tie(it, inserted) = data_parts_indexes.insert(res.part);\n+    }\n+\n+    /// Remove duplicate parts with the same checksum.\n+    if (!inserted)\n+    {\n+        if ((*it)->checksums.getTotalChecksumHex() == res.part->checksums.getTotalChecksumHex())\n+        {\n+            LOG_ERROR(log, \"Remove duplicate part {}\", data_part_storage->getFullPath());\n+            res.part->is_duplicate = true;\n+            return res;\n+        }\n+        else\n+            throw Exception(\"Part \" + res.part->name + \" already exists but with different checksums\", ErrorCodes::DUPLICATE_DATA_PART);\n+    }\n+\n+    if (to_state == DataPartState::Active)\n+        addPartContributionToDataVolume(res.part);\n+\n+    LOG_TRACE(log, \"Finished loading {} part {} on disk {}\", magic_enum::enum_name(to_state), part_name, part_disk_ptr->getName());\n+    return res;\n+};\n+\n+std::vector<MergeTreeData::LoadPartResult> MergeTreeData::loadDataPartsFromDisk(\n     ThreadPool & pool,\n     size_t num_parts,\n-    std::queue<std::vector<std::pair<String, DiskPtr>>> & parts_queue,\n-    bool skip_sanity_checks,\n+    std::queue<PartLoadingTreeNodes> & parts_queue,\n     const MergeTreeSettingsPtr & settings)\n {\n     /// Parallel loading of data parts.\n@@ -967,9 +1271,10 @@ void MergeTreeData::loadDataPartsFromDisk(\n \n     /// Prepare data parts for parallel loading. Threads will focus on given disk first, then steal\n     /// others' tasks when finish current disk part loading process.\n-    std::vector<std::vector<std::pair<String, DiskPtr>>> threads_parts(num_threads);\n+    std::vector<PartLoadingTreeNodes> threads_parts(num_threads);\n     std::set<size_t> remaining_thread_parts;\n     std::queue<size_t> threads_queue;\n+\n     for (size_t i = 0; i < num_threads; ++i)\n     {\n         remaining_thread_parts.insert(i);\n@@ -982,11 +1287,12 @@ void MergeTreeData::loadDataPartsFromDisk(\n         size_t i = threads_queue.front();\n         auto & need_parts = parts_per_thread[i];\n         assert(need_parts > 0);\n+\n         auto & thread_parts = threads_parts[i];\n         auto & current_parts = parts_queue.front();\n         assert(!current_parts.empty());\n-        auto parts_to_grab = std::min(need_parts, current_parts.size());\n \n+        auto parts_to_grab = std::min(need_parts, current_parts.size());\n         thread_parts.insert(thread_parts.end(), current_parts.end() - parts_to_grab, current_parts.end());\n         current_parts.resize(current_parts.size() - parts_to_grab);\n         need_parts -= parts_to_grab;\n@@ -998,140 +1304,27 @@ void MergeTreeData::loadDataPartsFromDisk(\n         /// If current disk still has some parts, push it to the tail.\n         if (!current_parts.empty())\n             parts_queue.push(std::move(current_parts));\n+\n         parts_queue.pop();\n \n         /// If current thread still want some parts, push it to the tail.\n         if (need_parts > 0)\n             threads_queue.push(i);\n+\n         threads_queue.pop();\n     }\n+\n     assert(threads_queue.empty());\n-    assert(std::all_of(threads_parts.begin(), threads_parts.end(), [](const std::vector<std::pair<String, DiskPtr>> & parts)\n+    assert(std::all_of(threads_parts.begin(), threads_parts.end(), [](const auto & parts)\n     {\n         return !parts.empty();\n     }));\n \n-    size_t suspicious_broken_parts = 0;\n-    size_t suspicious_broken_parts_bytes = 0;\n-    std::atomic<bool> has_adaptive_parts = false;\n-    std::atomic<bool> has_non_adaptive_parts = false;\n-    std::atomic<bool> has_lightweight_deletes_in_parts = false;\n-\n-    std::mutex mutex;\n-    auto load_part = [&](const String & part_name, const DiskPtr & part_disk_ptr)\n-    {\n-        auto part_opt = MergeTreePartInfo::tryParsePartName(part_name, format_version);\n-        if (!part_opt)\n-            return;\n-\n-        const auto & part_info = *part_opt;\n-        auto single_disk_volume = std::make_shared<SingleDiskVolume>(\"volume_\" + part_name, part_disk_ptr, 0);\n-        auto data_part_storage = std::make_shared<DataPartStorageOnDisk>(single_disk_volume, relative_data_path, part_name);\n-        auto part = createPart(part_name, part_info, data_part_storage);\n-        bool broken = false;\n-\n-        LOG_TRACE(log, \"Loading part {} ({}) from disk {}\", part_name, part->getType().toString(), part_disk_ptr->getName());\n-\n-        String part_path = fs::path(relative_data_path) / part_name;\n-        String marker_path = fs::path(part_path) / IMergeTreeDataPart::DELETE_ON_DESTROY_MARKER_FILE_NAME;\n-        if (part_disk_ptr->exists(marker_path))\n-        {\n-            /// NOTE: getBytesOnDisk() cannot be used here, since it maybe zero of checksums.txt will not exist\n-            size_t size_of_part = data_part_storage->calculateTotalSizeOnDisk();\n-            LOG_WARNING(log,\n-                \"Detaching stale part {}{} (size: {}), which should have been deleted after a move. \"\n-                \"That can only happen after unclean restart of ClickHouse after move of a part having an operation blocking that stale copy of part.\",\n-                getFullPathOnDisk(part_disk_ptr), part_name, formatReadableSizeWithBinarySuffix(size_of_part));\n-            std::lock_guard loading_lock(mutex);\n-            broken_parts_to_detach.push_back(part);\n-            ++suspicious_broken_parts;\n-            suspicious_broken_parts_bytes += size_of_part;\n-            return;\n-        }\n-\n-        try\n-        {\n-            part->loadColumnsChecksumsIndexes(require_part_metadata, true);\n-        }\n-        catch (const Exception & e)\n-        {\n-            /// Don't count the part as broken if there is not enough memory to load it.\n-            /// In fact, there can be many similar situations.\n-            /// But it is OK, because there is a safety guard against deleting too many parts.\n-            if (isNotEnoughMemoryErrorCode(e.code()))\n-                throw;\n-\n-            broken = true;\n-            tryLogCurrentException(log, fmt::format(\"while loading part {} on path {}\", part->name, part_path));\n-        }\n-        catch (...)\n-        {\n-            broken = true;\n-            tryLogCurrentException(log, fmt::format(\"while loading part {} on path {}\", part->name, part_path));\n-        }\n-\n-        /// Ignore broken parts that can appear as a result of hard server restart.\n-        if (broken)\n-        {\n-            std::optional<size_t> size_of_part;\n-            try\n-            {\n-                /// NOTE: getBytesOnDisk() cannot be used here, since it maybe zero of checksums.txt will not exist\n-                size_of_part = data_part_storage->calculateTotalSizeOnDisk();\n-            }\n-            catch (...)\n-            {\n-                tryLogCurrentException(log, fmt::format(\"while calculating part size {} on path {}\", part->name, part_path));\n-            }\n-\n-            std::string part_size_str = \"failed to calculate size\";\n-            if (size_of_part.has_value())\n-                part_size_str = formatReadableSizeWithBinarySuffix(*size_of_part);\n-\n-            LOG_ERROR(log,\n-                \"Detaching broken part {}{} (size: {}). \"\n-                \"If it happened after update, it is likely because of backward incompatibility. \"\n-                \"You need to resolve this manually\",\n-                getFullPathOnDisk(part_disk_ptr), part_name, part_size_str);\n-            std::lock_guard loading_lock(mutex);\n-            broken_parts_to_detach.push_back(part);\n-            ++suspicious_broken_parts;\n-            if (size_of_part.has_value())\n-                suspicious_broken_parts_bytes += *size_of_part;\n-            return;\n-        }\n-        if (!part->index_granularity_info.mark_type.adaptive)\n-            has_non_adaptive_parts.store(true, std::memory_order_relaxed);\n-        else\n-            has_adaptive_parts.store(true, std::memory_order_relaxed);\n-\n-        /// Check if there is lightweight delete in part\n-        if (part->hasLightweightDelete())\n-            has_lightweight_deletes_in_parts.store(true, std::memory_order_relaxed);\n-\n-        part->modification_time = part_disk_ptr->getLastModified(fs::path(relative_data_path) / part_name).epochTime();\n-        /// Assume that all parts are Active, covered parts will be detected and marked as Outdated later\n-        part->setState(DataPartState::Active);\n-\n-        std::lock_guard loading_lock(mutex);\n-        auto [it, inserted] = data_parts_indexes.insert(part);\n-        /// Remove duplicate parts with the same checksum.\n-        if (!inserted)\n-        {\n-            if ((*it)->checksums.getTotalChecksumHex() == part->checksums.getTotalChecksumHex())\n-            {\n-                LOG_ERROR(log, \"Remove duplicate part {}\", data_part_storage->getFullPath());\n-                duplicate_parts_to_remove.push_back(part);\n-            }\n-            else\n-                throw Exception(\"Part \" + part->name + \" already exists but with different checksums\", ErrorCodes::DUPLICATE_DATA_PART);\n-        }\n+    std::mutex part_select_mutex;\n+    std::mutex part_loading_mutex;\n \n-        addPartContributionToDataVolume(part);\n-        LOG_TRACE(log, \"Finished part {} load on disk {}\", part_name, part_disk_ptr->getName());\n-    };\n+    std::vector<LoadPartResult> loaded_parts;\n \n-    std::mutex part_select_mutex;\n     try\n     {\n         for (size_t thread = 0; thread < num_threads; ++thread)\n@@ -1147,15 +1340,16 @@ void MergeTreeData::loadDataPartsFromDisk(\n \n                 while (true)\n                 {\n-                    std::pair<String, DiskPtr> thread_part;\n+                    PartLoadingTree::NodePtr thread_part;\n+                    size_t thread_idx = thread;\n+\n                     {\n-                        const std::lock_guard lock{part_select_mutex};\n+                        std::lock_guard lock{part_select_mutex};\n \n                         if (remaining_thread_parts.empty())\n                             return;\n \n                         /// Steal task if nothing to do\n-                        auto thread_idx = thread;\n                         if (threads_parts[thread].empty())\n                         {\n                             // Try random steal tasks from the next thread\n@@ -1164,13 +1358,34 @@ void MergeTreeData::loadDataPartsFromDisk(\n                             std::advance(it, distribution(thread_local_rng));\n                             thread_idx = *it;\n                         }\n+\n                         auto & thread_parts = threads_parts[thread_idx];\n                         thread_part = thread_parts.back();\n                         thread_parts.pop_back();\n                         if (thread_parts.empty())\n                             remaining_thread_parts.erase(thread_idx);\n                     }\n-                    load_part(thread_part.first, thread_part.second);\n+\n+                    /// Pass a separate mutex to guard the set of parts, because this lambda\n+                    /// is called concurrently but with already locked @data_parts_mutex.\n+                    auto res = loadDataPart(thread_part->info, thread_part->name, thread_part->disk, DataPartState::Active, part_loading_mutex);\n+                    thread_part->is_loaded = true;\n+\n+                    bool is_active_part = res.part->getState() == DataPartState::Active;\n+                    /// If part is broken or duplicate or should be removed according to transaction\n+                    /// and it has any covered parts then try to load them to replace this part.\n+                    if (!is_active_part && !thread_part->children.empty())\n+                    {\n+                        std::lock_guard lock{part_select_mutex};\n+                        for (const auto & [_, node] : thread_part->children)\n+                            threads_parts[thread].push_back(node);\n+                        remaining_thread_parts.insert(thread);\n+                    }\n+\n+                    {\n+                        std::lock_guard lock(part_loading_mutex);\n+                        loaded_parts.push_back(std::move(res));\n+                    }\n                 }\n             });\n         }\n@@ -1183,57 +1398,42 @@ void MergeTreeData::loadDataPartsFromDisk(\n     }\n \n     pool.wait();\n-\n-    if (has_non_adaptive_parts && has_adaptive_parts && !settings->enable_mixed_granularity_parts)\n-        throw Exception(\n-            \"Table contains parts with adaptive and non adaptive marks, but `setting enable_mixed_granularity_parts` is disabled\",\n-            ErrorCodes::LOGICAL_ERROR);\n-\n-    has_non_adaptive_index_granularity_parts = has_non_adaptive_parts;\n-\n-    if (has_lightweight_deletes_in_parts)\n-        has_lightweight_delete_parts.store(true);\n-\n-    if (suspicious_broken_parts > settings->max_suspicious_broken_parts && !skip_sanity_checks)\n-        throw Exception(ErrorCodes::TOO_MANY_UNEXPECTED_DATA_PARTS,\n-            \"Suspiciously many ({} parts, {} in total) broken parts to remove while maximum allowed broken parts count is {}. You can change the maximum value \"\n-                        \"with merge tree setting 'max_suspicious_broken_parts' in <merge_tree> configuration section or in table settings in .sql file \"\n-                        \"(don't forget to return setting back to default value)\",\n-            suspicious_broken_parts, formatReadableSizeWithBinarySuffix(suspicious_broken_parts_bytes), settings->max_suspicious_broken_parts);\n-\n-    if (suspicious_broken_parts_bytes > settings->max_suspicious_broken_parts_bytes && !skip_sanity_checks)\n-        throw Exception(ErrorCodes::TOO_MANY_UNEXPECTED_DATA_PARTS,\n-            \"Suspiciously big size ({} parts, {} in total) of all broken parts to remove while maximum allowed broken parts size is {}. \"\n-            \"You can change the maximum value with merge tree setting 'max_suspicious_broken_parts_bytes' in <merge_tree> configuration \"\n-            \"section or in table settings in .sql file (don't forget to return setting back to default value)\",\n-            suspicious_broken_parts, formatReadableSizeWithBinarySuffix(suspicious_broken_parts_bytes),\n-            formatReadableSizeWithBinarySuffix(settings->max_suspicious_broken_parts_bytes));\n+    return loaded_parts;\n }\n \n \n-void MergeTreeData::loadDataPartsFromWAL(\n-    MutableDataPartsVector & duplicate_parts_to_remove,\n-    MutableDataPartsVector & parts_from_wal)\n+void MergeTreeData::loadDataPartsFromWAL(MutableDataPartsVector & parts_from_wal)\n {\n+    std::sort(parts_from_wal.begin(), parts_from_wal.end(), [](const auto & lhs, const auto & rhs)\n+    {\n+        return std::tie(lhs->info.level, lhs->info.mutation) > std::tie(rhs->info.level, rhs->info.mutation);\n+    });\n+\n     for (auto & part : parts_from_wal)\n     {\n         part->modification_time = time(nullptr);\n-        /// Assume that all parts are Active, covered parts will be detected and marked as Outdated later\n-        part->setState(DataPartState::Active);\n+        auto lo = data_parts_by_state_and_info.lower_bound(DataPartStateAndInfo{DataPartState::Active, part->info});\n+\n+        if (lo != data_parts_by_state_and_info.begin() && (*std::prev(lo))->info.contains(part->info))\n+            continue;\n+\n+        if (lo != data_parts_by_state_and_info.end() && (*lo)->info.contains(part->info))\n+            continue;\n \n+        part->setState(DataPartState::Active);\n         auto [it, inserted] = data_parts_indexes.insert(part);\n+\n         if (!inserted)\n         {\n             if ((*it)->checksums.getTotalChecksumHex() == part->checksums.getTotalChecksumHex())\n-            {\n                 LOG_ERROR(log, \"Remove duplicate part {}\", part->getDataPartStorage().getFullPath());\n-                duplicate_parts_to_remove.push_back(part);\n-            }\n             else\n                 throw Exception(\"Part \" + part->name + \" already exists but with different checksums\", ErrorCodes::DUPLICATE_DATA_PART);\n         }\n-\n-        addPartContributionToDataVolume(part);\n+        else\n+        {\n+            addPartContributionToDataVolume(part);\n+        }\n     }\n }\n \n@@ -1298,16 +1498,16 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)\n         }\n     }\n \n-    /// Collect part names by disk.\n-    std::map<String, std::vector<std::pair<String, DiskPtr>>> disk_part_map;\n     ThreadPool pool(disks.size());\n+    std::vector<PartLoadingTree::PartLoadingInfos> parts_to_load_by_disk(disks.size());\n \n-    for (const auto & disk_ptr : disks)\n+    for (size_t i = 0; i < disks.size(); ++i)\n     {\n+        const auto & disk_ptr = disks[i];\n         if (disk_ptr->isBroken())\n             continue;\n \n-        auto & disk_parts = disk_part_map[disk_ptr->getName()];\n+        auto & disk_parts = parts_to_load_by_disk[i];\n \n         pool.scheduleOrThrowOnError([&, disk_ptr]()\n         {\n@@ -1315,24 +1515,42 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)\n             {\n                 /// Skip temporary directories, file 'format_version.txt' and directory 'detached'.\n                 if (startsWith(it->name(), \"tmp\") || it->name() == MergeTreeData::FORMAT_VERSION_FILE_NAME\n-                    || it->name() == MergeTreeData::DETACHED_DIR_NAME)\n+                    || it->name() == MergeTreeData::DETACHED_DIR_NAME\n+                    || startsWith(it->name(), MergeTreeWriteAheadLog::WAL_FILE_NAME))\n                     continue;\n \n-                if (!startsWith(it->name(), MergeTreeWriteAheadLog::WAL_FILE_NAME))\n-                    disk_parts.emplace_back(std::make_pair(it->name(), disk_ptr));\n+                if (auto part_info = MergeTreePartInfo::tryParsePartName(it->name(), format_version))\n+                    disk_parts.emplace_back(*part_info, it->name(), disk_ptr);\n             }\n         });\n     }\n+\n     pool.wait();\n \n+    PartLoadingTree::PartLoadingInfos parts_to_load;\n+    for (auto & disk_parts : parts_to_load_by_disk)\n+        std::move(disk_parts.begin(), disk_parts.end(), std::back_inserter(parts_to_load));\n+\n+    auto loading_tree = PartLoadingTree::build(std::move(parts_to_load));\n+    /// Collect parts by disks' names.\n+    std::map<String, PartLoadingTreeNodes> disk_part_map;\n+\n+    /// Collect only \"the most covering\" parts from the top level of the tree.\n+    loading_tree.traverse(/*recursive=*/ false, [&](const auto & node)\n+    {\n+        disk_part_map[node->disk->getName()].emplace_back(node);\n+    });\n+\n     size_t num_parts = 0;\n-    std::queue<std::vector<std::pair<String, DiskPtr>>> parts_queue;\n+    std::queue<PartLoadingTreeNodes> parts_queue;\n+\n     for (auto & [disk_name, disk_parts] : disk_part_map)\n     {\n         LOG_INFO(log, \"Found {} parts for disk '{}' to load\", disk_parts.size(), disk_name);\n \n         if (disk_parts.empty())\n             continue;\n+\n         num_parts += disk_parts.size();\n         parts_queue.push(std::move(disk_parts));\n     }\n@@ -1343,23 +1561,57 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)\n     MutableDataPartsVector broken_parts_to_detach;\n     MutableDataPartsVector duplicate_parts_to_remove;\n \n-    if (num_parts > 0)\n-        loadDataPartsFromDisk(\n-            broken_parts_to_detach, duplicate_parts_to_remove, pool, num_parts, parts_queue, skip_sanity_checks, settings);\n+    size_t suspicious_broken_parts = 0;\n+    size_t suspicious_broken_parts_bytes = 0;\n+    bool have_adaptive_parts = false;\n+    bool have_non_adaptive_parts = false;\n+    bool have_lightweight_in_parts = false;\n+    bool have_parts_with_version_metadata = false;\n \n     bool is_static_storage = isStaticStorage();\n \n-    if (settings->in_memory_parts_enable_wal)\n+    if (num_parts > 0)\n     {\n-        std::map<String, MutableDataPartsVector> disk_wal_part_map;\n+        auto loaded_parts = loadDataPartsFromDisk(pool, num_parts, parts_queue, settings);\n \n+        for (const auto & res : loaded_parts)\n+        {\n+            if (res.is_broken)\n+            {\n+                broken_parts_to_detach.push_back(res.part);\n+                ++suspicious_broken_parts;\n+                if (res.size_of_part)\n+                    suspicious_broken_parts_bytes += *res.size_of_part;\n+            }\n+            else if (res.part->is_duplicate)\n+            {\n+                if (!is_static_storage)\n+                    res.part->remove();\n+            }\n+            else\n+            {\n+                bool is_adaptive = res.part->index_granularity_info.mark_type.adaptive;\n+                have_adaptive_parts |= is_adaptive;\n+                have_non_adaptive_parts |= !is_adaptive;\n+                have_lightweight_in_parts |= res.part->hasLightweightDelete();\n+                have_parts_with_version_metadata |= res.part->wasInvolvedInTransaction();\n+            }\n+        }\n+    }\n+\n+    if (settings->in_memory_parts_enable_wal)\n+    {\n+        pool.setMaxThreads(disks.size());\n+        std::vector<MutableDataPartsVector> disks_wal_parts(disks.size());\n         std::mutex wal_init_lock;\n-        for (const auto & disk_ptr : disks)\n+\n+        for (size_t i = 0; i < disks.size(); ++i)\n         {\n+            const auto & disk_ptr = disks[i];\n             if (disk_ptr->isBroken())\n                 continue;\n \n-            auto & disk_wal_parts = disk_wal_part_map[disk_ptr->getName()];\n+            auto & disk_wal_parts = disks_wal_parts[i];\n \n             pool.scheduleOrThrowOnError([&, disk_ptr]()\n             {\n@@ -1393,12 +1645,10 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)\n         pool.wait();\n \n         MutableDataPartsVector parts_from_wal;\n-        for (auto & [_, disk_wal_parts] : disk_wal_part_map)\n-            parts_from_wal.insert(\n-                parts_from_wal.end(), std::make_move_iterator(disk_wal_parts.begin()), std::make_move_iterator(disk_wal_parts.end()));\n-\n-        loadDataPartsFromWAL(duplicate_parts_to_remove, parts_from_wal);\n+        for (auto & disk_wal_parts : disks_wal_parts)\n+            std::move(disk_wal_parts.begin(), disk_wal_parts.end(), std::back_inserter(parts_from_wal));\n \n+        loadDataPartsFromWAL(parts_from_wal);\n         num_parts += parts_from_wal.size();\n     }\n \n@@ -1409,185 +1659,168 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)\n         return;\n     }\n \n+    if (have_non_adaptive_parts && have_adaptive_parts && !settings->enable_mixed_granularity_parts)\n+        throw Exception(\n+            \"Table contains parts with adaptive and non adaptive marks, but `setting enable_mixed_granularity_parts` is disabled\",\n+            ErrorCodes::LOGICAL_ERROR);\n+\n+    has_non_adaptive_index_granularity_parts = have_non_adaptive_parts;\n+    has_lightweight_delete_parts = have_lightweight_in_parts;\n+    transactions_enabled = have_parts_with_version_metadata;\n+\n+    if (suspicious_broken_parts > settings->max_suspicious_broken_parts && !skip_sanity_checks)\n+        throw Exception(ErrorCodes::TOO_MANY_UNEXPECTED_DATA_PARTS,\n+            \"Suspiciously many ({} parts, {} in total) broken parts to remove while maximum allowed broken parts count is {}. You can change the maximum value \"\n+                        \"with merge tree setting 'max_suspicious_broken_parts' in <merge_tree> configuration section or in table settings in .sql file \"\n+                        \"(don't forget to return setting back to default value)\",\n+            suspicious_broken_parts, formatReadableSizeWithBinarySuffix(suspicious_broken_parts_bytes), settings->max_suspicious_broken_parts);\n+\n+    if (suspicious_broken_parts_bytes > settings->max_suspicious_broken_parts_bytes && !skip_sanity_checks)\n+        throw Exception(ErrorCodes::TOO_MANY_UNEXPECTED_DATA_PARTS,\n+            \"Suspiciously big size ({} parts, {} in total) of all broken parts to remove while maximum allowed broken parts size is {}. \"\n+            \"You can change the maximum value with merge tree setting 'max_suspicious_broken_parts_bytes' in <merge_tree> configuration \"\n+            \"section or in table settings in .sql file (don't forget to return setting back to default value)\",\n+            suspicious_broken_parts, formatReadableSizeWithBinarySuffix(suspicious_broken_parts_bytes),\n+            formatReadableSizeWithBinarySuffix(settings->max_suspicious_broken_parts_bytes));\n+\n     if (!is_static_storage)\n-    {\n         for (auto & part : broken_parts_to_detach)\n-        {\n-            /// detached parts must not have '_' in prefixes\n-            part->renameToDetached(\"broken-on-start\");\n-        }\n+            part->renameToDetached(\"broken-on-start\"); /// detached parts must not have '_' in prefixes\n \n-        for (auto & part : duplicate_parts_to_remove)\n-            part->remove();\n-    }\n+    resetObjectColumnsFromActiveParts(part_lock);\n+    calculateColumnAndSecondaryIndexSizesImpl();\n \n-    auto deactivate_part = [&] (DataPartIteratorByStateAndInfo it)\n+    PartLoadingTreeNodes unloaded_parts;\n+    loading_tree.traverse(/*recursive=*/ true, [&](const auto & node)\n     {\n-        const DataPartPtr & part = *it;\n+        if (!node->is_loaded)\n+            unloaded_parts.push_back(node);\n+    });\n \n-        part->remove_time.store(part->modification_time, std::memory_order_relaxed);\n-        auto creation_csn = part->version.creation_csn.load(std::memory_order_relaxed);\n-        if (creation_csn != Tx::RolledBackCSN && creation_csn != Tx::PrehistoricCSN && !part->version.isRemovalTIDLocked())\n-        {\n-            /// It's possible that covering part was created without transaction,\n-            /// but if covered part was created with transaction (i.e. creation_tid is not prehistoric),\n-            /// then it must have removal tid in metadata file.\n-            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Data part {} is Outdated and has creation TID {} and CSN {}, \"\n-                            \"but does not have removal tid. It's a bug or a result of manual intervention.\",\n-                            part->name, part->version.creation_tid, creation_csn);\n-        }\n-        modifyPartState(it, DataPartState::Outdated);\n-        removePartContributionToDataVolume(part);\n+    if (!unloaded_parts.empty())\n+    {\n+        LOG_DEBUG(log, \"Found {} outdated data parts. They will be loaded asynchronously\", unloaded_parts.size());\n \n-        /// Explicitly set removal_tid_lock for parts w/o transaction (i.e. w/o txn_version.txt)\n-        /// to avoid keeping part forever (see VersionMetadata::canBeRemoved())\n-        if (!part->version.isRemovalTIDLocked())\n         {\n-            TransactionInfoContext transaction_context{getStorageID(), part->name};\n-            part->version.lockRemovalTID(Tx::PrehistoricTID, transaction_context);\n+            std::lock_guard lock(outdated_data_parts_mutex);\n+            outdated_unloaded_data_parts = std::move(unloaded_parts);\n         }\n-    };\n \n-    /// All parts are in \"Active\" state after loading\n-    assert(std::find_if(data_parts_by_state_and_info.begin(), data_parts_by_state_and_info.end(),\n-    [](const auto & part)\n+        outdated_data_parts_loading_task = getContext()->getSchedulePool().createTask(\n+            \"MergeTreeData::loadOutdatedDataParts\",\n+            [this] { loadOutdatedDataParts(/*is_async=*/ true); });\n+    }\n+\n+    LOG_DEBUG(log, \"Loaded data parts ({} items)\", data_parts_indexes.size());\n+    data_parts_loading_finished = true;\n+}\n+\n+void MergeTreeData::loadOutdatedDataParts(bool is_async)\n+try\n+{\n     {\n-        return part->getState() != DataPartState::Active;\n-    }) == data_parts_by_state_and_info.end());\n+        std::lock_guard lock(outdated_data_parts_mutex);\n+        if (outdated_unloaded_data_parts.empty())\n+            return;\n \n-    bool have_parts_with_version_metadata = false;\n-    auto iter = data_parts_by_state_and_info.begin();\n-    while (iter != data_parts_by_state_and_info.end() && (*iter)->getState() == DataPartState::Active)\n+        LOG_DEBUG(log, \"Loading {} outdated data parts {}\",\n+            outdated_unloaded_data_parts.size(),\n+            is_async ? \"asynchronously\" : \"synchronously\");\n+    }\n+\n+    /// Acquire shared lock because 'relative_data_path' is used while loading parts.\n+    TableLockHolder shared_lock;\n+    if (is_async)\n+        shared_lock = lockForShare(RWLockImpl::NO_QUERY, getSettings()->lock_acquire_timeout_for_background_operations);\n+\n+    size_t num_loaded_parts = 0;\n+    while (true)\n     {\n-        const DataPartPtr & part = *iter;\n-        part->loadVersionMetadata();\n-        VersionMetadata & version = part->version;\n-        if (part->wasInvolvedInTransaction())\n-        {\n-            have_parts_with_version_metadata = true;\n-        }\n-        else\n-        {\n-            ++iter;\n-            continue;\n-        }\n+        PartLoadingTree::NodePtr part;\n \n-        /// Check if CSNs were written after committing transaction, update and write if needed.\n-        bool version_updated = false;\n-        chassert(!version.creation_tid.isEmpty());\n-        if (!part->version.creation_csn)\n         {\n-            auto min = TransactionLog::getCSN(version.creation_tid);\n-            if (!min)\n-            {\n-                /// Transaction that created this part was not committed. Remove part.\n-                TransactionLog::assertTIDIsNotOutdated(version.creation_tid);\n-                min = Tx::RolledBackCSN;\n-            }\n-            LOG_TRACE(log, \"Will fix version metadata of {} after unclean restart: part has creation_tid={}, setting creation_csn={}\",\n-                      part->name, version.creation_tid, min);\n-            version.creation_csn = min;\n-            version_updated = true;\n-        }\n-        if (!version.removal_tid.isEmpty() && !part->version.removal_csn)\n-        {\n-            auto max = TransactionLog::getCSN(version.removal_tid);\n-            if (max)\n-            {\n-                LOG_TRACE(log, \"Will fix version metadata of {} after unclean restart: part has removal_tid={}, setting removal_csn={}\",\n-                          part->name, version.removal_tid, max);\n-                version.removal_csn = max;\n-            }\n-            else\n+            std::lock_guard lock(outdated_data_parts_mutex);\n+\n+            if (is_async && outdated_data_parts_loading_canceled)\n             {\n-                TransactionLog::assertTIDIsNotOutdated(version.removal_tid);\n-                /// Transaction that tried to remove this part was not committed. Clear removal_tid.\n-                LOG_TRACE(log, \"Will fix version metadata of {} after unclean restart: clearing removal_tid={}\",\n-                          part->name, version.removal_tid);\n-                version.unlockRemovalTID(version.removal_tid, TransactionInfoContext{getStorageID(), part->name});\n+                LOG_DEBUG(log,\n+                    \"Stopped loading outdated data parts because task was canceled. \"\n+                    \"Loaded {} parts, {} left unloaded\", num_loaded_parts, outdated_unloaded_data_parts.size());\n+                return;\n             }\n-            version_updated = true;\n-        }\n-\n-        /// Sanity checks\n-        bool csn_order = !version.removal_csn || version.creation_csn <= version.removal_csn || version.removal_csn == Tx::PrehistoricCSN;\n-        bool min_start_csn_order = version.creation_tid.start_csn <= version.creation_csn;\n-        bool max_start_csn_order = version.removal_tid.start_csn <= version.removal_csn;\n-        bool creation_csn_known = version.creation_csn;\n-        if (!csn_order || !min_start_csn_order || !max_start_csn_order || !creation_csn_known)\n-            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Part {} has invalid version metadata: {}\", part->name, version.toString());\n \n-        if (version_updated)\n-            part->storeVersionMetadata(/* force */ true);\n+            if (outdated_unloaded_data_parts.empty())\n+                break;\n \n-        /// Deactivate part if creation was not committed or if removal was.\n-        if (version.creation_csn == Tx::RolledBackCSN || version.removal_csn)\n-        {\n-            auto next_it = std::next(iter);\n-            deactivate_part(iter);\n-            iter = next_it;\n+            part = std::move(outdated_unloaded_data_parts.back());\n+            outdated_unloaded_data_parts.pop_back();\n         }\n+\n+        auto res = loadDataPart(part->info, part->name, part->disk, MergeTreeDataPartState::Outdated, data_parts_mutex);\n+        ++num_loaded_parts;\n+\n+        if (res.is_broken)\n+            res.part->renameToDetached(\"broken-on-start\"); /// detached parts must not have '_' in prefixes\n+        else if (res.part->is_duplicate)\n+            res.part->remove();\n         else\n-        {\n-            ++iter;\n-        }\n+            preparePartForRemoval(res.part);\n     }\n \n-    if (have_parts_with_version_metadata)\n-        transactions_enabled.store(true);\n+    LOG_DEBUG(log, \"Loaded {} outdated data parts {}\",\n+        num_loaded_parts, is_async ? \"asynchronously\" : \"synchronously\");\n+    outdated_data_parts_cv.notify_all();\n+}\n+catch (...)\n+{\n+    LOG_ERROR(log, \"Loading of outdated parts failed. \"\n+        \"Will terminate to avoid undefined behaviour due to inconsistent set of parts. \"\n+        \"Exception: \", getCurrentExceptionMessage(true));\n+    std::terminate();\n+}\n \n-    /// Delete from the set of current parts those parts that are covered by another part (those parts that\n-    /// were merged), but that for some reason are still not deleted from the filesystem.\n-    /// Deletion of files will be performed later in the clearOldParts() method.\n+/// No TSA because of std::unique_lock and std::condition_variable.\n+void MergeTreeData::waitForOutdatedPartsToBeLoaded() const TSA_NO_THREAD_SAFETY_ANALYSIS\n+{\n+    /// Background tasks are not run if storage is static.\n+    if (isStaticStorage())\n+        return;\n \n-    auto active_parts_range = getDataPartsStateRange(DataPartState::Active);\n-    auto prev_it = active_parts_range.begin();\n-    auto end_it = active_parts_range.end();\n+    std::unique_lock lock(outdated_data_parts_mutex);\n+    if (outdated_unloaded_data_parts.empty())\n+        return;\n \n-    bool less_than_two_active_parts = prev_it == end_it || std::next(prev_it) == end_it;\n+    LOG_TRACE(log, \"Will wait for outdated data parts to be loaded\");\n \n-    if (!less_than_two_active_parts)\n+    outdated_data_parts_cv.wait(lock, [this]() TSA_NO_THREAD_SAFETY_ANALYSIS\n     {\n-        (*prev_it)->assertState({DataPartState::Active});\n-        auto curr_it = std::next(prev_it);\n+        return outdated_unloaded_data_parts.empty() || outdated_data_parts_loading_canceled;\n+    });\n \n-        while (curr_it != data_parts_by_state_and_info.end() && (*curr_it)->getState() == DataPartState::Active)\n-        {\n-            (*curr_it)->assertState({DataPartState::Active});\n+    if (outdated_data_parts_loading_canceled)\n+        throw Exception(ErrorCodes::NOT_INITIALIZED, \"Loading of outdated data parts was canceled\");\n \n-            /// Don't consider data parts belonging to different partitions.\n-            if ((*curr_it)->info.partition_id != (*prev_it)->info.partition_id)\n-            {\n-                ++prev_it;\n-                ++curr_it;\n-                continue;\n-            }\n+    LOG_TRACE(log, \"Finished waiting for outdated data parts to be loaded\");\n+}\n \n-            if ((*curr_it)->contains(**prev_it))\n-            {\n-                deactivate_part(prev_it);\n-                prev_it = curr_it;\n-                ++curr_it;\n-            }\n-            else if ((*prev_it)->contains(**curr_it))\n-            {\n-                auto next = std::next(curr_it);\n-                deactivate_part(curr_it);\n-                curr_it = next;\n-            }\n-            else\n-            {\n-                ++prev_it;\n-                ++curr_it;\n-            }\n-        }\n-    }\n+void MergeTreeData::startOutdatedDataPartsLoadingTask()\n+{\n+    if (outdated_data_parts_loading_task)\n+        outdated_data_parts_loading_task->activateAndSchedule();\n+}\n \n-    resetObjectColumnsFromActiveParts(part_lock);\n-    calculateColumnAndSecondaryIndexSizesImpl();\n+void MergeTreeData::stopOutdatedDataPartsLoadingTask()\n+{\n+    if (!outdated_data_parts_loading_task)\n+        return;\n \n-    LOG_DEBUG(log, \"Loaded data parts ({} items)\", data_parts_indexes.size());\n-    data_parts_loading_finished = true;\n+    {\n+        std::lock_guard lock(outdated_data_parts_mutex);\n+        outdated_data_parts_loading_canceled = true;\n+    }\n+\n+    outdated_data_parts_loading_task->deactivate();\n+    outdated_data_parts_cv.notify_all();\n }\n \n /// Is the part directory old.\n@@ -1694,12 +1927,12 @@ scope_guard MergeTreeData::getTemporaryPartDirectoryHolder(const String & part_d\n     return [this, part_dir_name]() { temporary_parts.remove(part_dir_name); };\n }\n \n-MergeTreeData::MutableDataPartPtr MergeTreeData::preparePartForRemoval(const DataPartPtr & part)\n+MergeTreeData::MutableDataPartPtr MergeTreeData::asMutableDeletingPart(const DataPartPtr & part)\n {\n     auto state = part->getState();\n     if (state != DataPartState::Deleting && state != DataPartState::DeleteOnDestroy)\n         throw Exception(ErrorCodes::LOGICAL_ERROR,\n-            \"Cannot remove part {}, because it has state: {}\", part->name, magic_enum::enum_name(part->getState()));\n+            \"Cannot remove part {}, because it has state: {}\", part->name, magic_enum::enum_name(state));\n \n     return std::const_pointer_cast<IMergeTreeDataPart>(part);\n }\n@@ -1975,7 +2208,7 @@ void MergeTreeData::clearPartsFromFilesystemImpl(const DataPartsVector & parts_t\n             log, \"Removing {} parts from filesystem (serially): Parts: [{}]\", parts_to_remove.size(), fmt::join(parts_to_remove, \", \"));\n         for (const DataPartPtr & part : parts_to_remove)\n         {\n-            preparePartForRemoval(part)->remove();\n+            asMutableDeletingPart(part)->remove();\n             if (part_names_succeed)\n                 part_names_succeed->insert(part->name);\n         }\n@@ -2019,7 +2252,7 @@ void MergeTreeData::clearPartsFromFilesystemImpl(const DataPartsVector & parts_t\n                 if (thread_group)\n                     CurrentThread::attachToIfDetached(thread_group);\n \n-                preparePartForRemoval(part)->remove();\n+                asMutableDeletingPart(part)->remove();\n                 if (part_names_succeed)\n                 {\n                     std::lock_guard lock(part_names_mutex);\n@@ -2084,7 +2317,7 @@ void MergeTreeData::clearPartsFromFilesystemImpl(const DataPartsVector & parts_t\n \n             for (const auto & part : batch)\n             {\n-                preparePartForRemoval(part)->remove();\n+                asMutableDeletingPart(part)->remove();\n                 if (part_names_succeed)\n                 {\n                     std::lock_guard lock(part_names_mutex);\n@@ -3473,6 +3706,13 @@ DataPartsVector MergeTreeData::grabActivePartsToRemoveForDropRange(\n MergeTreeData::PartsToRemoveFromZooKeeper MergeTreeData::removePartsInRangeFromWorkingSetAndGetPartsToRemoveFromZooKeeper(\n         MergeTreeTransaction * txn, const MergeTreePartInfo & drop_range, DataPartsLock & lock)\n {\n+#ifndef NDEBUG\n+    {\n+        /// All parts (including outdated) must be loaded at this moment.\n+        std::lock_guard outdated_parts_lock(outdated_data_parts_mutex);\n+        assert(outdated_unloaded_data_parts.empty());\n+    }\n+#endif\n \n     auto parts_to_remove = grabActivePartsToRemoveForDropRange(txn, drop_range, lock);\n \n@@ -3555,7 +3795,7 @@ void MergeTreeData::forcefullyMovePartToDetachedAndRemoveFromMemory(const MergeT\n     }\n \n     modifyPartState(it_part, DataPartState::Deleting);\n-    preparePartForRemoval(part)->renameToDetached(prefix);\n+    asMutableDeletingPart(part)->renameToDetached(prefix);\n     data_parts_indexes.erase(it_part);\n \n     if (restore_covered && part->info.level == 0)\n@@ -3709,7 +3949,7 @@ void MergeTreeData::tryRemovePartImmediately(DataPartPtr && part)\n \n     try\n     {\n-        preparePartForRemoval(part_to_delete)->remove();\n+        asMutableDeletingPart(part_to_delete)->remove();\n     }\n     catch (...)\n     {\n@@ -3945,7 +4185,7 @@ void MergeTreeData::swapActivePart(MergeTreeData::DataPartPtr part_copy)\n             /// All other locks are taken in StorageReplicatedMergeTree\n             lockSharedData(*part_copy);\n \n-            preparePartForRemoval(original_active_part)->writeDeleteOnDestroyMarker();\n+            asMutableDeletingPart(original_active_part)->writeDeleteOnDestroyMarker();\n             return;\n         }\n     }\n@@ -4343,6 +4583,11 @@ Pipe MergeTreeData::alterPartition(\n     const PartitionCommands & commands,\n     ContextPtr query_context)\n {\n+    /// Wait for loading of outdated parts\n+    /// because partition commands (DROP, MOVE, etc.)\n+    /// must be applied to all parts on disk.\n+    waitForOutdatedPartsToBeLoaded();\n+\n     PartitionCommandsResultInfo result;\n     for (const PartitionCommand & command : commands)\n     {\n@@ -4388,6 +4633,14 @@ Pipe MergeTreeData::alterPartition(\n                     {\n                         String dest_database = query_context->resolveDatabase(command.to_database);\n                         auto dest_storage = DatabaseCatalog::instance().getTable({dest_database, command.to_table}, query_context);\n+\n+                        auto * dest_storage_merge_tree = dynamic_cast<MergeTreeData *>(dest_storage.get());\n+                        if (!dest_storage_merge_tree)\n+                            throw Exception(ErrorCodes::NOT_IMPLEMENTED,\n+                                \"Cannot move partition from table {} to table {} with storage {}\",\n+                                getStorageID().getNameForLogs(), dest_storage->getStorageID().getNameForLogs(), dest_storage->getName());\n+\n+                        dest_storage_merge_tree->waitForOutdatedPartsToBeLoaded();\n                         movePartitionToTable(dest_storage, command.partition, query_context);\n                     }\n                     break;\n@@ -4409,8 +4662,17 @@ Pipe MergeTreeData::alterPartition(\n             {\n                 if (command.replace)\n                     checkPartitionCanBeDropped(command.partition, query_context);\n+\n                 String from_database = query_context->resolveDatabase(command.from_database);\n                 auto from_storage = DatabaseCatalog::instance().getTable({from_database, command.from_table}, query_context);\n+\n+                auto * from_storage_merge_tree = dynamic_cast<MergeTreeData *>(from_storage.get());\n+                if (!from_storage_merge_tree)\n+                    throw Exception(ErrorCodes::NOT_IMPLEMENTED,\n+                        \"Cannot replace partition from table {} with storage {} to table {}\",\n+                        from_storage->getStorageID().getNameForLogs(), from_storage->getName(), getStorageID().getNameForLogs());\n+\n+                from_storage_merge_tree->waitForOutdatedPartsToBeLoaded();\n                 replacePartitionFrom(from_storage, command.partition, command.replace, query_context);\n             }\n             break;\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 670c755cf721..09a757c4419a 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -1050,6 +1050,8 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Returns an object that protects temporary directory from cleanup\n     scope_guard getTemporaryPartDirectoryHolder(const String & part_dir_name) const;\n \n+    void waitForOutdatedPartsToBeLoaded() const;\n+\n protected:\n     friend class IMergeTreeDataPart;\n     friend class MergeTreeDataMergerMutator;\n@@ -1066,7 +1068,6 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// under lockForShare if rename is possible.\n     String relative_data_path;\n \n-\n     /// Current column sizes in compressed and uncompressed form.\n     ColumnSizeByName column_sizes;\n \n@@ -1328,6 +1329,88 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     void resetObjectColumnsFromActiveParts(const DataPartsLock & lock);\n     void updateObjectColumns(const DataPartPtr & part, const DataPartsLock & lock);\n \n+    /** A structure that explicitly represents a \"merge tree\" of parts\n+     *  which is implicitly presented by min-max block numbers and levels of parts.\n+     *  The children of node are parts which are covered by parent part.\n+     *  This tree provides the order of loading of parts.\n+     *\n+     *  We start to traverse tree from the top level and load parts\n+     *  corresposponded to nodes. If part is loaded successfully then\n+     *  we stop traversal at this node. Otherwise part is broken and we\n+     *  traverse its children and try to load covered parts which will\n+     *  replace broken covering part. Unloaded nodes represent outdated parts\n+     *  nd they are pushed to background task and loaded asynchronoulsy.\n+     */\n+    class PartLoadingTree\n+    {\n+    public:\n+        struct Node\n+        {\n+            Node(const MergeTreePartInfo & info_, const String & name_, const DiskPtr & disk_)\n+                : info(info_), name(name_), disk(disk_)\n+            {\n+            }\n+\n+            const MergeTreePartInfo info;\n+            const String name;\n+            const DiskPtr disk;\n+\n+            bool is_loaded = false;\n+            std::map<MergeTreePartInfo, std::shared_ptr<Node>> children;\n+        };\n+\n+        struct PartLoadingInfo\n+        {\n+            PartLoadingInfo(const MergeTreePartInfo & info_, const String & name_, const DiskPtr & disk_)\n+                : info(info_), name(name_), disk(disk_)\n+            {\n+            }\n+\n+            /// Store name explicitly because it cannot be easily\n+            /// retrieved from info in tables with old syntax.\n+            MergeTreePartInfo info;\n+            String name;\n+            DiskPtr disk;\n+        };\n+\n+        using NodePtr = std::shared_ptr<Node>;\n+        using PartLoadingInfos = std::vector<PartLoadingInfo>;\n+\n+        /// Builds a tree from the list of part infos.\n+        static PartLoadingTree build(PartLoadingInfos nodes);\n+\n+        /// Traverses a tree and call @func on each node.\n+        /// If recursive is false traverses only the top level.\n+        template <typename Func>\n+        void traverse(bool recursive, Func && func);\n+\n+    private:\n+        /// NOTE: Parts should be added in descending order of their levels\n+        /// because rearranging tree to the new root is not supported.\n+        void add(const MergeTreePartInfo & info, const String & name, const DiskPtr & disk);\n+        std::unordered_map<String, NodePtr> root_by_partition;\n+    };\n+\n+    using PartLoadingTreeNodes = std::vector<PartLoadingTree::NodePtr>;\n+\n+    struct LoadPartResult\n+    {\n+        bool is_broken = false;\n+        std::optional<size_t> size_of_part;\n+        MutableDataPartPtr part;\n+    };\n+\n+    mutable std::mutex outdated_data_parts_mutex;\n+    mutable std::condition_variable outdated_data_parts_cv;\n+\n+    BackgroundSchedulePool::TaskHolder outdated_data_parts_loading_task;\n+    PartLoadingTreeNodes outdated_unloaded_data_parts TSA_GUARDED_BY(outdated_data_parts_mutex);\n+    bool outdated_data_parts_loading_canceled TSA_GUARDED_BY(outdated_data_parts_mutex) = false;\n+\n+    void loadOutdatedDataParts(bool is_async);\n+    void startOutdatedDataPartsLoadingTask();\n+    void stopOutdatedDataPartsLoadingTask();\n+\n     static void incrementInsertedPartsProfileEvent(MergeTreeDataPartType type);\n     static void incrementMergedPartsProfileEvent(MergeTreeDataPartType type);\n \n@@ -1406,18 +1489,20 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Returns default settings for storage with possible changes from global config.\n     virtual std::unique_ptr<MergeTreeSettings> getDefaultSettings() const = 0;\n \n-    void loadDataPartsFromDisk(\n-        MutableDataPartsVector & broken_parts_to_detach,\n-        MutableDataPartsVector & duplicate_parts_to_remove,\n+    LoadPartResult loadDataPart(\n+        const MergeTreePartInfo & part_info,\n+        const String & part_name,\n+        const DiskPtr & part_disk_ptr,\n+        MergeTreeDataPartState to_state,\n+        std::mutex & part_loading_mutex);\n+\n+    std::vector<LoadPartResult> loadDataPartsFromDisk(\n         ThreadPool & pool,\n         size_t num_parts,\n-        std::queue<std::vector<std::pair<String, DiskPtr>>> & parts_queue,\n-        bool skip_sanity_checks,\n+        std::queue<PartLoadingTreeNodes> & parts_queue,\n         const MergeTreeSettingsPtr & settings);\n \n-    void loadDataPartsFromWAL(\n-        MutableDataPartsVector & duplicate_parts_to_remove,\n-        MutableDataPartsVector & parts_from_wal);\n+    void loadDataPartsFromWAL(MutableDataPartsVector & parts_from_wal);\n \n     /// Create zero-copy exclusive lock for part and disk. Useful for coordination of\n     /// distributed operations which can lead to data duplication. Implemented only in ReplicatedMergeTree.\n@@ -1428,7 +1513,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Otherwise, in non-parallel case will break and return.\n     void clearPartsFromFilesystemImpl(const DataPartsVector & parts, NameSet * part_names_succeed);\n \n-    static MutableDataPartPtr preparePartForRemoval(const DataPartPtr & part);\n+    static MutableDataPartPtr asMutableDeletingPart(const DataPartPtr & part);\n \n     mutable TemporaryParts temporary_parts;\n };\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\nindex 29528e9ff807..b63abfcb6abd 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp\n@@ -147,7 +147,6 @@ bool ReplicatedMergeTreeRestartingThread::runImpl()\n     storage.part_check_thread.start();\n \n     LOG_DEBUG(log, \"Table started successfully\");\n-\n     return true;\n }\n \ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex d4b9020f27f3..c1e5a3d4f8ea 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -112,7 +112,6 @@ StorageMergeTree::StorageMergeTree(\n     increment.set(getMaxBlockNumber());\n \n     loadMutations();\n-\n     loadDeduplicationLog();\n }\n \n@@ -138,6 +137,7 @@ void StorageMergeTree::startup()\n     {\n         background_operations_assignee.start();\n         startBackgroundMovesIfNeeded();\n+        startOutdatedDataPartsLoadingTask();\n     }\n     catch (...)\n     {\n@@ -171,6 +171,8 @@ void StorageMergeTree::shutdown()\n     if (shutdown_called.exchange(true))\n         return;\n \n+    stopOutdatedDataPartsLoadingTask();\n+\n     /// Unlock all waiting mutations\n     {\n         std::lock_guard lock(mutation_wait_mutex);\n@@ -1189,6 +1191,7 @@ bool StorageMergeTree::scheduleDataProcessingJob(BackgroundJobsAssignee & assign\n         scheduled = true;\n     }\n \n+\n     return scheduled;\n }\n \n@@ -1509,6 +1512,7 @@ void StorageMergeTree::truncate(const ASTPtr &, const StorageMetadataPtr &, Cont\n     /// Asks to complete merges and does not allow them to start.\n     /// This protects against \"revival\" of data for a removed partition after completion of merge.\n     auto merge_blocker = stopMergesAndWait();\n+    waitForOutdatedPartsToBeLoaded();\n \n     Stopwatch watch;\n \ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex d7d7afd222d6..34f384b66210 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -908,6 +908,16 @@ void StorageReplicatedMergeTree::drop()\n         dropReplica(zookeeper, zookeeper_path, replica_name, log, getSettings());\n     }\n \n+    /// Wait for loading of all outdated parts because\n+    /// in case of zero copy recursive removal of directory\n+    /// is not supported and table cannot be dropped.\n+    if (canUseZeroCopyReplication())\n+    {\n+        /// Load remaining parts synchronously because task\n+        /// for loading is already cancelled in shutdown().\n+        loadOutdatedDataParts(/*is_async=*/ false);\n+    }\n+\n     dropAllData();\n }\n \n@@ -1863,6 +1873,11 @@ MutableDataPartStoragePtr StorageReplicatedMergeTree::executeFetchShared(\n void StorageReplicatedMergeTree::executeDropRange(const LogEntry & entry)\n {\n     LOG_TRACE(log, \"Executing DROP_RANGE {}\", entry.new_part_name);\n+\n+    /// Wait for loading of outdated parts because DROP_RANGE\n+    /// command must be applied to all parts on disk.\n+    waitForOutdatedPartsToBeLoaded();\n+\n     auto drop_range_info = MergeTreePartInfo::fromPartName(entry.new_part_name, format_version);\n     getContext()->getMergeList().cancelInPartition(getStorageID(), drop_range_info.partition_id, drop_range_info.max_block);\n     queue.removePartProducingOpsInRange(getZooKeeper(), drop_range_info, entry, /* fetch_entry_znode= */ {});\n@@ -1927,6 +1942,11 @@ bool StorageReplicatedMergeTree::executeReplaceRange(const LogEntry & entry)\n     LOG_DEBUG(log, \"Executing log entry {} to replace parts range {} with {} parts from {}.{}\",\n               entry.znode_name, entry_replace.drop_range_part_name, entry_replace.new_part_names.size(),\n               entry_replace.from_database, entry_replace.from_table);\n+\n+    /// Wait for loading of outdated parts because REPLACE_RANGE\n+    /// command must be applied to all parts on disk.\n+    waitForOutdatedPartsToBeLoaded();\n+\n     auto metadata_snapshot = getInMemoryMetadataPtr();\n     auto storage_settings_ptr = getSettings();\n \n@@ -4282,6 +4302,7 @@ MutableDataPartStoragePtr StorageReplicatedMergeTree::fetchExistsPart(\n \n void StorageReplicatedMergeTree::startup()\n {\n+    startOutdatedDataPartsLoadingTask();\n     if (attach_thread)\n     {\n         attach_thread->start();\n@@ -4398,6 +4419,7 @@ void StorageReplicatedMergeTree::shutdown()\n         return;\n \n     session_expired_callback_handler.reset();\n+    stopOutdatedDataPartsLoadingTask();\n \n     /// Cancel fetches, merges and mutations to force the queue_task to finish ASAP.\n     fetcher.blocker.cancelForever();\n@@ -5291,7 +5313,6 @@ void StorageReplicatedMergeTree::restoreMetadataInZooKeeper()\n     if (!is_readonly)\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Replica must be readonly\");\n \n-\n     if (getZooKeeper()->exists(replica_path))\n         throw Exception(ErrorCodes::BAD_ARGUMENTS,\n                         \"Replica path is present at {} - nothing to restore. \"\n@@ -5308,6 +5329,7 @@ void StorageReplicatedMergeTree::restoreMetadataInZooKeeper()\n \n     auto metadata_snapshot = getInMemoryMetadataPtr();\n \n+    waitForOutdatedPartsToBeLoaded();\n     const DataPartsVector all_parts = getAllDataPartsVector();\n     Strings active_parts_names;\n \n@@ -5422,6 +5444,7 @@ void StorageReplicatedMergeTree::truncate(\n     if (!is_leader)\n         throw Exception(\"TRUNCATE cannot be done on this replica because it is not a leader\", ErrorCodes::NOT_A_LEADER);\n \n+    waitForOutdatedPartsToBeLoaded();\n     zkutil::ZooKeeperPtr zookeeper = getZooKeeperAndAssertNotReadonly();\n     dropAllPartitionsImpl(zookeeper, /* detach */ false, query_context);\n }\n",
  "test_patch": "diff --git a/src/Access/tests/gtest_access_rights_ops.cpp b/src/Access/tests/gtest_access_rights_ops.cpp\nindex 02aafb7415b4..e21ebda2a31a 100644\n--- a/src/Access/tests/gtest_access_rights_ops.cpp\n+++ b/src/Access/tests/gtest_access_rights_ops.cpp\n@@ -53,7 +53,7 @@ TEST(AccessRights, Union)\n               \"SHOW ROW POLICIES, SYSTEM MERGES, SYSTEM TTL MERGES, SYSTEM FETCHES, \"\n               \"SYSTEM MOVES, SYSTEM SENDS, SYSTEM REPLICATION QUEUES, \"\n               \"SYSTEM DROP REPLICA, SYSTEM SYNC REPLICA, SYSTEM RESTART REPLICA, \"\n-              \"SYSTEM RESTORE REPLICA, SYSTEM SYNC DATABASE REPLICA, SYSTEM FLUSH DISTRIBUTED, dictGet ON db1.*\");\n+              \"SYSTEM RESTORE REPLICA, SYSTEM WAIT LOADING PARTS, SYSTEM SYNC DATABASE REPLICA, SYSTEM FLUSH DISTRIBUTED, dictGet ON db1.*\");\n }\n \n \ndiff --git a/tests/integration/test_grant_and_revoke/test.py b/tests/integration/test_grant_and_revoke/test.py\nindex d0a2f2ab9334..b7cb9281ba67 100644\n--- a/tests/integration/test_grant_and_revoke/test.py\n+++ b/tests/integration/test_grant_and_revoke/test.py\n@@ -181,7 +181,7 @@ def test_grant_all_on_table():\n         == \"GRANT SHOW TABLES, SHOW COLUMNS, SHOW DICTIONARIES, SELECT, INSERT, ALTER TABLE, ALTER VIEW, CREATE TABLE, CREATE VIEW, CREATE DICTIONARY, \"\n         \"DROP TABLE, DROP VIEW, DROP DICTIONARY, TRUNCATE, OPTIMIZE, BACKUP, CREATE ROW POLICY, ALTER ROW POLICY, DROP ROW POLICY, SHOW ROW POLICIES, \"\n         \"SYSTEM MERGES, SYSTEM TTL MERGES, SYSTEM FETCHES, SYSTEM MOVES, SYSTEM SENDS, SYSTEM REPLICATION QUEUES, SYSTEM DROP REPLICA, SYSTEM SYNC REPLICA, \"\n-        \"SYSTEM RESTART REPLICA, SYSTEM RESTORE REPLICA, SYSTEM FLUSH DISTRIBUTED, dictGet ON test.table TO B\\n\"\n+        \"SYSTEM RESTART REPLICA, SYSTEM RESTORE REPLICA, SYSTEM WAIT LOADING PARTS, SYSTEM FLUSH DISTRIBUTED, dictGet ON test.table TO B\\n\"\n     )\n     instance.query(\"REVOKE ALL ON test.table FROM B\", user=\"A\")\n     assert instance.query(\"SHOW GRANTS FOR B\") == \"\"\ndiff --git a/tests/integration/test_merge_tree_load_parts/__init__.py b/tests/integration/test_merge_tree_load_parts/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_merge_tree_load_parts/configs/fast_background_pool.xml b/tests/integration/test_merge_tree_load_parts/configs/fast_background_pool.xml\nnew file mode 100644\nindex 000000000000..74038c5472f9\n--- /dev/null\n+++ b/tests/integration/test_merge_tree_load_parts/configs/fast_background_pool.xml\n@@ -0,0 +1,9 @@\n+<clickhouse>\n+    <background_processing_pool_thread_sleep_seconds>1</background_processing_pool_thread_sleep_seconds>\n+    <background_processing_pool_thread_sleep_seconds_random_part>0</background_processing_pool_thread_sleep_seconds_random_part>\n+    <background_processing_pool_thread_sleep_seconds_if_nothing_to_do>0.0</background_processing_pool_thread_sleep_seconds_if_nothing_to_do>\n+    <background_processing_pool_task_sleep_seconds_when_no_work_min>0</background_processing_pool_task_sleep_seconds_when_no_work_min>\n+    <background_processing_pool_task_sleep_seconds_when_no_work_max>1</background_processing_pool_task_sleep_seconds_when_no_work_max>\n+    <background_processing_pool_task_sleep_seconds_when_no_work_multiplier>1</background_processing_pool_task_sleep_seconds_when_no_work_multiplier>\n+    <background_processing_pool_task_sleep_seconds_when_no_work_random_part>0</background_processing_pool_task_sleep_seconds_when_no_work_random_part>\n+</clickhouse>\ndiff --git a/tests/integration/test_merge_tree_load_parts/test.py b/tests/integration/test_merge_tree_load_parts/test.py\nnew file mode 100644\nindex 000000000000..777b6f14fc6e\n--- /dev/null\n+++ b/tests/integration/test_merge_tree_load_parts/test.py\n@@ -0,0 +1,196 @@\n+import pytest\n+import helpers.client\n+import helpers.cluster\n+import time\n+from helpers.corrupt_part_data_on_disk import corrupt_part_data_on_disk\n+\n+\n+cluster = helpers.cluster.ClickHouseCluster(__file__)\n+node1 = cluster.add_instance(\n+    \"node1\",\n+    main_configs=[\"configs/fast_background_pool.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+)\n+node2 = cluster.add_instance(\n+    \"node2\",\n+    main_configs=[\"configs/fast_background_pool.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_merge_tree_load_parts(started_cluster):\n+    node1.query(\n+        \"\"\"\n+        CREATE TABLE mt_load_parts (pk UInt32, id UInt32, s String)\n+        ENGINE = MergeTree ORDER BY id PARTITION BY pk\"\"\"\n+    )\n+\n+    node1.query(\"SYSTEM STOP MERGES mt_load_parts\")\n+\n+    for i in range(20):\n+        node1.query(\n+            f\"INSERT INTO mt_load_parts VALUES (44, {i}, randomPrintableASCII(10))\"\n+        )\n+\n+    node1.restart_clickhouse(kill=True)\n+    for i in range(1, 21):\n+        assert node1.contains_in_log(f\"Loading Active part 44_{i}_{i}_0\")\n+\n+    node1.query(\"OPTIMIZE TABLE mt_load_parts FINAL\")\n+    node1.restart_clickhouse(kill=True)\n+\n+    node1.query(\"SYSTEM WAIT LOADING PARTS mt_load_parts\")\n+\n+    assert node1.contains_in_log(\"Loading Active part 44_1_20\")\n+    for i in range(1, 21):\n+        assert not node1.contains_in_log(f\"Loading Active part 44_{i}_{i}_0\")\n+        assert node1.contains_in_log(f\"Loading Outdated part 44_{i}_{i}_0\")\n+\n+    assert node1.query(\"SELECT count() FROM mt_load_parts\") == \"20\\n\"\n+\n+    assert (\n+        node1.query(\n+            \"SELECT count() FROM system.parts WHERE table = 'mt_load_parts' AND active\"\n+        )\n+        == \"1\\n\"\n+    )\n+\n+    node1.query(\"ALTER TABLE mt_load_parts MODIFY SETTING old_parts_lifetime = 1\")\n+    node1.query(\"DETACH TABLE mt_load_parts\")\n+    node1.query(\"ATTACH TABLE mt_load_parts\")\n+\n+    node1.query(\"SYSTEM WAIT LOADING PARTS mt_load_parts\")\n+\n+    table_path = node1.query(\n+        \"SELECT data_paths[1] FROM system.tables WHERE table = 'mt_load_parts'\"\n+    ).strip()\n+\n+    part_dirs = node1.exec_in_container([\"bash\", \"-c\", f\"ls {table_path}\"], user=\"root\")\n+\n+    part_dirs = list(\n+        set(part_dirs.strip().split(\"\\n\")) - {\"detached\", \"format_version.txt\"}\n+    )\n+\n+    MAX_RETRY = 10\n+    part_dirs_ok = False\n+    for _ in range(MAX_RETRY):\n+        part_dirs = node1.exec_in_container(\n+            [\"bash\", \"-c\", f\"ls {table_path}\"], user=\"root\"\n+        )\n+        part_dirs = list(\n+            set(part_dirs.strip().split(\"\\n\")) - {\"detached\", \"format_version.txt\"}\n+        )\n+        part_dirs_ok = len(part_dirs) == 1 and part_dirs[0].startswith(\"44_1_20\")\n+        if part_dirs_ok:\n+            break\n+        time.sleep(2)\n+\n+    assert part_dirs_ok\n+\n+\n+def test_merge_tree_load_parts_corrupted(started_cluster):\n+    for i, node in enumerate([node1, node2]):\n+        node.query(\n+            f\"\"\"\n+            CREATE TABLE mt_load_parts_2 (pk UInt32, id UInt32, s String)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/0/mt_load_parts_2', '{i}') ORDER BY id PARTITION BY pk\"\"\"\n+        )\n+\n+    \"\"\"min-max blocks in created parts: 1_1_0, 2_2_0, 1_2_1, 3_3_0, 1_3_2\"\"\"\n+    for partition in [111, 222, 333]:\n+        node1.query(\n+            f\"INSERT INTO mt_load_parts_2 VALUES ({partition}, 0, randomPrintableASCII(10))\"\n+        )\n+\n+        node1.query(\n+            f\"INSERT INTO mt_load_parts_2 VALUES ({partition}, 1, randomPrintableASCII(10))\"\n+        )\n+\n+        node1.query(f\"OPTIMIZE TABLE mt_load_parts_2 PARTITION {partition} FINAL\")\n+\n+        node1.query(\n+            f\"INSERT INTO mt_load_parts_2 VALUES ({partition}, 2, randomPrintableASCII(10))\"\n+        )\n+\n+        node1.query(f\"OPTIMIZE TABLE mt_load_parts_2 PARTITION {partition} FINAL\")\n+\n+    node2.query(\"SYSTEM SYNC REPLICA mt_load_parts_2\", timeout=30)\n+\n+    def get_part_name(node, partition, min_block, max_block):\n+        return node.query(\n+            f\"\"\"\n+            SELECT name FROM system.parts\n+            WHERE table = 'mt_load_parts_2'\n+            AND partition = '{partition}'\n+            AND min_block_number = {min_block}\n+            AND max_block_number = {max_block}\"\"\"\n+        ).strip()\n+\n+    corrupt_part_data_on_disk(node1, \"mt_load_parts_2\", get_part_name(node1, 111, 0, 2))\n+    corrupt_part_data_on_disk(node1, \"mt_load_parts_2\", get_part_name(node1, 222, 0, 2))\n+    corrupt_part_data_on_disk(node1, \"mt_load_parts_2\", get_part_name(node1, 222, 0, 1))\n+    corrupt_part_data_on_disk(node1, \"mt_load_parts_2\", get_part_name(node1, 333, 0, 1))\n+    corrupt_part_data_on_disk(node1, \"mt_load_parts_2\", get_part_name(node1, 333, 2, 2))\n+\n+    node1.restart_clickhouse(kill=True)\n+    node1.query(\"SYSTEM WAIT LOADING PARTS mt_load_parts_2\")\n+\n+    def check_parts_loading(node, partition, loaded, failed, skipped):\n+        for (min_block, max_block) in loaded:\n+            part_name = f\"{partition}_{min_block}_{max_block}\"\n+            assert node.contains_in_log(f\"Loading Active part {part_name}\")\n+            assert node.contains_in_log(f\"Finished loading Active part {part_name}\")\n+\n+        for (min_block, max_block) in failed:\n+            part_name = f\"{partition}_{min_block}_{max_block}\"\n+            assert node.contains_in_log(f\"Loading Active part {part_name}\")\n+            assert not node.contains_in_log(f\"Finished loading Active part {part_name}\")\n+\n+        for (min_block, max_block) in skipped:\n+            part_name = f\"{partition}_{min_block}_{max_block}\"\n+            assert not node.contains_in_log(f\"Loading Active part {part_name}\")\n+            assert not node.contains_in_log(f\"Finished loading Active part {part_name}\")\n+\n+    check_parts_loading(\n+        node1, 111, loaded=[(0, 1), (2, 2)], failed=[(0, 2)], skipped=[(0, 0), (1, 1)]\n+    )\n+    check_parts_loading(\n+        node1, 222, loaded=[(0, 0), (1, 1), (2, 2)], failed=[(0, 2), (0, 1)], skipped=[]\n+    )\n+    check_parts_loading(\n+        node1, 333, loaded=[(0, 2)], failed=[], skipped=[(0, 0), (1, 1), (2, 2), (0, 1)]\n+    )\n+\n+    node1.query(\"SYSTEM SYNC REPLICA mt_load_parts_2\", timeout=30)\n+    node1.query(\"OPTIMIZE TABLE mt_load_parts_2 FINAL\")\n+    node1.query(\"SYSTEM SYNC REPLICA mt_load_parts_2\", timeout=30)\n+\n+    assert (\n+        node1.query(\n+            \"\"\"\n+            SELECT pk, count() FROM mt_load_parts_2\n+            GROUP BY pk ORDER BY pk\"\"\"\n+        )\n+        == \"111\\t3\\n222\\t3\\n333\\t3\\n\"\n+    )\n+    assert (\n+        node1.query(\n+            \"\"\"\n+            SELECT partition, count()\n+            FROM system.parts WHERE table = 'mt_load_parts_2' AND active\n+            GROUP BY partition ORDER BY partition\"\"\"\n+        )\n+        == \"111\\t1\\n222\\t1\\n333\\t1\\n\"\n+    )\ndiff --git a/tests/integration/test_transactions/test.py b/tests/integration/test_transactions/test.py\nindex 7902d1687078..a12d30915dd0 100644\n--- a/tests/integration/test_transactions/test.py\n+++ b/tests/integration/test_transactions/test.py\n@@ -26,7 +26,7 @@ def tx(session, query):\n \n def test_rollback_unfinished_on_restart1(start_cluster):\n     node.query(\n-        \"create table mt (n int, m int) engine=MergeTree order by n partition by n % 2\"\n+        \"create table mt (n int, m int) engine=MergeTree order by n partition by n % 2 settings remove_empty_parts = 0\"\n     )\n     node.query(\"insert into mt values (1, 10), (2, 20)\")\n     tid0 = \"(1,1,'00000000-0000-0000-0000-000000000000')\"\n@@ -82,6 +82,7 @@ def test_rollback_unfinished_on_restart1(start_cluster):\n     ).strip()\n \n     node.restart_clickhouse(kill=True)\n+    node.query(\"SYSTEM WAIT LOADING PARTS mt\")\n \n     assert (\n         node.query(\"select *, _part from mt order by n\")\n@@ -116,7 +117,7 @@ def test_rollback_unfinished_on_restart1(start_cluster):\n \n def test_rollback_unfinished_on_restart2(start_cluster):\n     node.query(\n-        \"create table mt2 (n int, m int) engine=MergeTree order by n partition by n % 2\"\n+        \"create table mt2 (n int, m int) engine=MergeTree order by n partition by n % 2 settings remove_empty_parts = 0\"\n     )\n     node.query(\"insert into mt2 values (1, 10), (2, 20)\")\n     tid0 = \"(1,1,'00000000-0000-0000-0000-000000000000')\"\n@@ -171,6 +172,7 @@ def test_rollback_unfinished_on_restart2(start_cluster):\n     ).strip()\n \n     node.restart_clickhouse(kill=True)\n+    node.query(\"SYSTEM WAIT LOADING PARTS mt2\")\n \n     assert (\n         node.query(\"select *, _part from mt2 order by n\")\ndiff --git a/tests/queries/0_stateless/01271_show_privileges.reference b/tests/queries/0_stateless/01271_show_privileges.reference\nindex f2c3e8eda9da..f088cfaf00c2 100644\n--- a/tests/queries/0_stateless/01271_show_privileges.reference\n+++ b/tests/queries/0_stateless/01271_show_privileges.reference\n@@ -121,6 +121,7 @@ SYSTEM DROP REPLICA\t['DROP REPLICA']\tTABLE\tSYSTEM\n SYSTEM SYNC REPLICA\t['SYNC REPLICA']\tTABLE\tSYSTEM\n SYSTEM RESTART REPLICA\t['RESTART REPLICA']\tTABLE\tSYSTEM\n SYSTEM RESTORE REPLICA\t['RESTORE REPLICA']\tTABLE\tSYSTEM\n+SYSTEM WAIT LOADING PARTS\t['WAIT LOADING PARTS']\tTABLE\tSYSTEM\n SYSTEM SYNC DATABASE REPLICA\t['SYNC DATABASE REPLICA']\tDATABASE\tSYSTEM\n SYSTEM SYNC TRANSACTION LOG\t['SYNC TRANSACTION LOG']\tGLOBAL\tSYSTEM\n SYSTEM FLUSH DISTRIBUTED\t['FLUSH DISTRIBUTED']\tTABLE\tSYSTEM FLUSH\ndiff --git a/tests/queries/0_stateless/02117_show_create_table_system.reference b/tests/queries/0_stateless/02117_show_create_table_system.reference\nindex c866f3e7b52c..9e065c455e9f 100644\n--- a/tests/queries/0_stateless/02117_show_create_table_system.reference\n+++ b/tests/queries/0_stateless/02117_show_create_table_system.reference\n@@ -286,7 +286,7 @@ CREATE TABLE system.grants\n (\n     `user_name` Nullable(String),\n     `role_name` Nullable(String),\n-    `access_type` Enum16('SHOW DATABASES' = 0, 'SHOW TABLES' = 1, 'SHOW COLUMNS' = 2, 'SHOW DICTIONARIES' = 3, 'SHOW' = 4, 'SHOW FILESYSTEM CACHES' = 5, 'SELECT' = 6, 'INSERT' = 7, 'ALTER UPDATE' = 8, 'ALTER DELETE' = 9, 'ALTER ADD COLUMN' = 10, 'ALTER MODIFY COLUMN' = 11, 'ALTER DROP COLUMN' = 12, 'ALTER COMMENT COLUMN' = 13, 'ALTER CLEAR COLUMN' = 14, 'ALTER RENAME COLUMN' = 15, 'ALTER MATERIALIZE COLUMN' = 16, 'ALTER COLUMN' = 17, 'ALTER MODIFY COMMENT' = 18, 'ALTER ORDER BY' = 19, 'ALTER SAMPLE BY' = 20, 'ALTER ADD INDEX' = 21, 'ALTER DROP INDEX' = 22, 'ALTER MATERIALIZE INDEX' = 23, 'ALTER CLEAR INDEX' = 24, 'ALTER INDEX' = 25, 'ALTER ADD PROJECTION' = 26, 'ALTER DROP PROJECTION' = 27, 'ALTER MATERIALIZE PROJECTION' = 28, 'ALTER CLEAR PROJECTION' = 29, 'ALTER PROJECTION' = 30, 'ALTER ADD CONSTRAINT' = 31, 'ALTER DROP CONSTRAINT' = 32, 'ALTER CONSTRAINT' = 33, 'ALTER TTL' = 34, 'ALTER MATERIALIZE TTL' = 35, 'ALTER SETTINGS' = 36, 'ALTER MOVE PARTITION' = 37, 'ALTER FETCH PARTITION' = 38, 'ALTER FREEZE PARTITION' = 39, 'ALTER DATABASE SETTINGS' = 40, 'ALTER NAMED COLLECTION' = 41, 'ALTER TABLE' = 42, 'ALTER DATABASE' = 43, 'ALTER VIEW REFRESH' = 44, 'ALTER VIEW MODIFY QUERY' = 45, 'ALTER VIEW' = 46, 'ALTER' = 47, 'CREATE DATABASE' = 48, 'CREATE TABLE' = 49, 'CREATE VIEW' = 50, 'CREATE DICTIONARY' = 51, 'CREATE TEMPORARY TABLE' = 52, 'CREATE FUNCTION' = 53, 'CREATE NAMED COLLECTION' = 54, 'CREATE' = 55, 'DROP DATABASE' = 56, 'DROP TABLE' = 57, 'DROP VIEW' = 58, 'DROP DICTIONARY' = 59, 'DROP FUNCTION' = 60, 'DROP NAMED COLLECTION' = 61, 'DROP' = 62, 'TRUNCATE' = 63, 'OPTIMIZE' = 64, 'BACKUP' = 65, 'KILL QUERY' = 66, 'KILL TRANSACTION' = 67, 'MOVE PARTITION BETWEEN SHARDS' = 68, 'CREATE USER' = 69, 'ALTER USER' = 70, 'DROP USER' = 71, 'CREATE ROLE' = 72, 'ALTER ROLE' = 73, 'DROP ROLE' = 74, 'ROLE ADMIN' = 75, 'CREATE ROW POLICY' = 76, 'ALTER ROW POLICY' = 77, 'DROP ROW POLICY' = 78, 'CREATE QUOTA' = 79, 'ALTER QUOTA' = 80, 'DROP QUOTA' = 81, 'CREATE SETTINGS PROFILE' = 82, 'ALTER SETTINGS PROFILE' = 83, 'DROP SETTINGS PROFILE' = 84, 'SHOW USERS' = 85, 'SHOW ROLES' = 86, 'SHOW ROW POLICIES' = 87, 'SHOW QUOTAS' = 88, 'SHOW SETTINGS PROFILES' = 89, 'SHOW ACCESS' = 90, 'SHOW NAMED COLLECTIONS' = 91, 'ACCESS MANAGEMENT' = 92, 'SYSTEM SHUTDOWN' = 93, 'SYSTEM DROP DNS CACHE' = 94, 'SYSTEM DROP MARK CACHE' = 95, 'SYSTEM DROP UNCOMPRESSED CACHE' = 96, 'SYSTEM DROP MMAP CACHE' = 97, 'SYSTEM DROP COMPILED EXPRESSION CACHE' = 98, 'SYSTEM DROP FILESYSTEM CACHE' = 99, 'SYSTEM DROP SCHEMA CACHE' = 100, 'SYSTEM DROP CACHE' = 101, 'SYSTEM RELOAD CONFIG' = 102, 'SYSTEM RELOAD USERS' = 103, 'SYSTEM RELOAD SYMBOLS' = 104, 'SYSTEM RELOAD DICTIONARY' = 105, 'SYSTEM RELOAD MODEL' = 106, 'SYSTEM RELOAD FUNCTION' = 107, 'SYSTEM RELOAD EMBEDDED DICTIONARIES' = 108, 'SYSTEM RELOAD' = 109, 'SYSTEM RESTART DISK' = 110, 'SYSTEM MERGES' = 111, 'SYSTEM TTL MERGES' = 112, 'SYSTEM FETCHES' = 113, 'SYSTEM MOVES' = 114, 'SYSTEM DISTRIBUTED SENDS' = 115, 'SYSTEM REPLICATED SENDS' = 116, 'SYSTEM SENDS' = 117, 'SYSTEM REPLICATION QUEUES' = 118, 'SYSTEM DROP REPLICA' = 119, 'SYSTEM SYNC REPLICA' = 120, 'SYSTEM RESTART REPLICA' = 121, 'SYSTEM RESTORE REPLICA' = 122, 'SYSTEM SYNC DATABASE REPLICA' = 123, 'SYSTEM SYNC TRANSACTION LOG' = 124, 'SYSTEM FLUSH DISTRIBUTED' = 125, 'SYSTEM FLUSH LOGS' = 126, 'SYSTEM FLUSH' = 127, 'SYSTEM THREAD FUZZER' = 128, 'SYSTEM UNFREEZE' = 129, 'SYSTEM' = 130, 'dictGet' = 131, 'addressToLine' = 132, 'addressToLineWithInlines' = 133, 'addressToSymbol' = 134, 'demangle' = 135, 'INTROSPECTION' = 136, 'FILE' = 137, 'URL' = 138, 'REMOTE' = 139, 'MONGO' = 140, 'MEILISEARCH' = 141, 'MYSQL' = 142, 'POSTGRES' = 143, 'SQLITE' = 144, 'ODBC' = 145, 'JDBC' = 146, 'HDFS' = 147, 'S3' = 148, 'HIVE' = 149, 'SOURCES' = 150, 'CLUSTER' = 151, 'ALL' = 152, 'NONE' = 153),\n+    `access_type` Enum16('SHOW DATABASES' = 0, 'SHOW TABLES' = 1, 'SHOW COLUMNS' = 2, 'SHOW DICTIONARIES' = 3, 'SHOW' = 4, 'SHOW FILESYSTEM CACHES' = 5, 'SELECT' = 6, 'INSERT' = 7, 'ALTER UPDATE' = 8, 'ALTER DELETE' = 9, 'ALTER ADD COLUMN' = 10, 'ALTER MODIFY COLUMN' = 11, 'ALTER DROP COLUMN' = 12, 'ALTER COMMENT COLUMN' = 13, 'ALTER CLEAR COLUMN' = 14, 'ALTER RENAME COLUMN' = 15, 'ALTER MATERIALIZE COLUMN' = 16, 'ALTER COLUMN' = 17, 'ALTER MODIFY COMMENT' = 18, 'ALTER ORDER BY' = 19, 'ALTER SAMPLE BY' = 20, 'ALTER ADD INDEX' = 21, 'ALTER DROP INDEX' = 22, 'ALTER MATERIALIZE INDEX' = 23, 'ALTER CLEAR INDEX' = 24, 'ALTER INDEX' = 25, 'ALTER ADD PROJECTION' = 26, 'ALTER DROP PROJECTION' = 27, 'ALTER MATERIALIZE PROJECTION' = 28, 'ALTER CLEAR PROJECTION' = 29, 'ALTER PROJECTION' = 30, 'ALTER ADD CONSTRAINT' = 31, 'ALTER DROP CONSTRAINT' = 32, 'ALTER CONSTRAINT' = 33, 'ALTER TTL' = 34, 'ALTER MATERIALIZE TTL' = 35, 'ALTER SETTINGS' = 36, 'ALTER MOVE PARTITION' = 37, 'ALTER FETCH PARTITION' = 38, 'ALTER FREEZE PARTITION' = 39, 'ALTER DATABASE SETTINGS' = 40, 'ALTER NAMED COLLECTION' = 41, 'ALTER TABLE' = 42, 'ALTER DATABASE' = 43, 'ALTER VIEW REFRESH' = 44, 'ALTER VIEW MODIFY QUERY' = 45, 'ALTER VIEW' = 46, 'ALTER' = 47, 'CREATE DATABASE' = 48, 'CREATE TABLE' = 49, 'CREATE VIEW' = 50, 'CREATE DICTIONARY' = 51, 'CREATE TEMPORARY TABLE' = 52, 'CREATE FUNCTION' = 53, 'CREATE NAMED COLLECTION' = 54, 'CREATE' = 55, 'DROP DATABASE' = 56, 'DROP TABLE' = 57, 'DROP VIEW' = 58, 'DROP DICTIONARY' = 59, 'DROP FUNCTION' = 60, 'DROP NAMED COLLECTION' = 61, 'DROP' = 62, 'TRUNCATE' = 63, 'OPTIMIZE' = 64, 'BACKUP' = 65, 'KILL QUERY' = 66, 'KILL TRANSACTION' = 67, 'MOVE PARTITION BETWEEN SHARDS' = 68, 'CREATE USER' = 69, 'ALTER USER' = 70, 'DROP USER' = 71, 'CREATE ROLE' = 72, 'ALTER ROLE' = 73, 'DROP ROLE' = 74, 'ROLE ADMIN' = 75, 'CREATE ROW POLICY' = 76, 'ALTER ROW POLICY' = 77, 'DROP ROW POLICY' = 78, 'CREATE QUOTA' = 79, 'ALTER QUOTA' = 80, 'DROP QUOTA' = 81, 'CREATE SETTINGS PROFILE' = 82, 'ALTER SETTINGS PROFILE' = 83, 'DROP SETTINGS PROFILE' = 84, 'SHOW USERS' = 85, 'SHOW ROLES' = 86, 'SHOW ROW POLICIES' = 87, 'SHOW QUOTAS' = 88, 'SHOW SETTINGS PROFILES' = 89, 'SHOW ACCESS' = 90, 'SHOW NAMED COLLECTIONS' = 91, 'ACCESS MANAGEMENT' = 92, 'SYSTEM SHUTDOWN' = 93, 'SYSTEM DROP DNS CACHE' = 94, 'SYSTEM DROP MARK CACHE' = 95, 'SYSTEM DROP UNCOMPRESSED CACHE' = 96, 'SYSTEM DROP MMAP CACHE' = 97, 'SYSTEM DROP COMPILED EXPRESSION CACHE' = 98, 'SYSTEM DROP FILESYSTEM CACHE' = 99, 'SYSTEM DROP SCHEMA CACHE' = 100, 'SYSTEM DROP CACHE' = 101, 'SYSTEM RELOAD CONFIG' = 102, 'SYSTEM RELOAD USERS' = 103, 'SYSTEM RELOAD SYMBOLS' = 104, 'SYSTEM RELOAD DICTIONARY' = 105, 'SYSTEM RELOAD MODEL' = 106, 'SYSTEM RELOAD FUNCTION' = 107, 'SYSTEM RELOAD EMBEDDED DICTIONARIES' = 108, 'SYSTEM RELOAD' = 109, 'SYSTEM RESTART DISK' = 110, 'SYSTEM MERGES' = 111, 'SYSTEM TTL MERGES' = 112, 'SYSTEM FETCHES' = 113, 'SYSTEM MOVES' = 114, 'SYSTEM DISTRIBUTED SENDS' = 115, 'SYSTEM REPLICATED SENDS' = 116, 'SYSTEM SENDS' = 117, 'SYSTEM REPLICATION QUEUES' = 118, 'SYSTEM DROP REPLICA' = 119, 'SYSTEM SYNC REPLICA' = 120, 'SYSTEM RESTART REPLICA' = 121, 'SYSTEM RESTORE REPLICA' = 122, 'SYSTEM WAIT LOADING PARTS' = 123, 'SYSTEM SYNC DATABASE REPLICA' = 124, 'SYSTEM SYNC TRANSACTION LOG' = 125, 'SYSTEM FLUSH DISTRIBUTED' = 126, 'SYSTEM FLUSH LOGS' = 127, 'SYSTEM FLUSH' = 128, 'SYSTEM THREAD FUZZER' = 129, 'SYSTEM UNFREEZE' = 130, 'SYSTEM' = 131, 'dictGet' = 132, 'addressToLine' = 133, 'addressToLineWithInlines' = 134, 'addressToSymbol' = 135, 'demangle' = 136, 'INTROSPECTION' = 137, 'FILE' = 138, 'URL' = 139, 'REMOTE' = 140, 'MONGO' = 141, 'MEILISEARCH' = 142, 'MYSQL' = 143, 'POSTGRES' = 144, 'SQLITE' = 145, 'ODBC' = 146, 'JDBC' = 147, 'HDFS' = 148, 'S3' = 149, 'HIVE' = 150, 'SOURCES' = 151, 'CLUSTER' = 152, 'ALL' = 153, 'NONE' = 154),\n     `database` Nullable(String),\n     `table` Nullable(String),\n     `column` Nullable(String),\n@@ -567,10 +567,10 @@ ENGINE = SystemPartsColumns\n COMMENT 'SYSTEM TABLE is built on the fly.'\n CREATE TABLE system.privileges\n (\n-    `privilege` Enum16('SHOW DATABASES' = 0, 'SHOW TABLES' = 1, 'SHOW COLUMNS' = 2, 'SHOW DICTIONARIES' = 3, 'SHOW' = 4, 'SHOW FILESYSTEM CACHES' = 5, 'SELECT' = 6, 'INSERT' = 7, 'ALTER UPDATE' = 8, 'ALTER DELETE' = 9, 'ALTER ADD COLUMN' = 10, 'ALTER MODIFY COLUMN' = 11, 'ALTER DROP COLUMN' = 12, 'ALTER COMMENT COLUMN' = 13, 'ALTER CLEAR COLUMN' = 14, 'ALTER RENAME COLUMN' = 15, 'ALTER MATERIALIZE COLUMN' = 16, 'ALTER COLUMN' = 17, 'ALTER MODIFY COMMENT' = 18, 'ALTER ORDER BY' = 19, 'ALTER SAMPLE BY' = 20, 'ALTER ADD INDEX' = 21, 'ALTER DROP INDEX' = 22, 'ALTER MATERIALIZE INDEX' = 23, 'ALTER CLEAR INDEX' = 24, 'ALTER INDEX' = 25, 'ALTER ADD PROJECTION' = 26, 'ALTER DROP PROJECTION' = 27, 'ALTER MATERIALIZE PROJECTION' = 28, 'ALTER CLEAR PROJECTION' = 29, 'ALTER PROJECTION' = 30, 'ALTER ADD CONSTRAINT' = 31, 'ALTER DROP CONSTRAINT' = 32, 'ALTER CONSTRAINT' = 33, 'ALTER TTL' = 34, 'ALTER MATERIALIZE TTL' = 35, 'ALTER SETTINGS' = 36, 'ALTER MOVE PARTITION' = 37, 'ALTER FETCH PARTITION' = 38, 'ALTER FREEZE PARTITION' = 39, 'ALTER DATABASE SETTINGS' = 40, 'ALTER NAMED COLLECTION' = 41, 'ALTER TABLE' = 42, 'ALTER DATABASE' = 43, 'ALTER VIEW REFRESH' = 44, 'ALTER VIEW MODIFY QUERY' = 45, 'ALTER VIEW' = 46, 'ALTER' = 47, 'CREATE DATABASE' = 48, 'CREATE TABLE' = 49, 'CREATE VIEW' = 50, 'CREATE DICTIONARY' = 51, 'CREATE TEMPORARY TABLE' = 52, 'CREATE FUNCTION' = 53, 'CREATE NAMED COLLECTION' = 54, 'CREATE' = 55, 'DROP DATABASE' = 56, 'DROP TABLE' = 57, 'DROP VIEW' = 58, 'DROP DICTIONARY' = 59, 'DROP FUNCTION' = 60, 'DROP NAMED COLLECTION' = 61, 'DROP' = 62, 'TRUNCATE' = 63, 'OPTIMIZE' = 64, 'BACKUP' = 65, 'KILL QUERY' = 66, 'KILL TRANSACTION' = 67, 'MOVE PARTITION BETWEEN SHARDS' = 68, 'CREATE USER' = 69, 'ALTER USER' = 70, 'DROP USER' = 71, 'CREATE ROLE' = 72, 'ALTER ROLE' = 73, 'DROP ROLE' = 74, 'ROLE ADMIN' = 75, 'CREATE ROW POLICY' = 76, 'ALTER ROW POLICY' = 77, 'DROP ROW POLICY' = 78, 'CREATE QUOTA' = 79, 'ALTER QUOTA' = 80, 'DROP QUOTA' = 81, 'CREATE SETTINGS PROFILE' = 82, 'ALTER SETTINGS PROFILE' = 83, 'DROP SETTINGS PROFILE' = 84, 'SHOW USERS' = 85, 'SHOW ROLES' = 86, 'SHOW ROW POLICIES' = 87, 'SHOW QUOTAS' = 88, 'SHOW SETTINGS PROFILES' = 89, 'SHOW ACCESS' = 90, 'SHOW NAMED COLLECTIONS' = 91, 'ACCESS MANAGEMENT' = 92, 'SYSTEM SHUTDOWN' = 93, 'SYSTEM DROP DNS CACHE' = 94, 'SYSTEM DROP MARK CACHE' = 95, 'SYSTEM DROP UNCOMPRESSED CACHE' = 96, 'SYSTEM DROP MMAP CACHE' = 97, 'SYSTEM DROP COMPILED EXPRESSION CACHE' = 98, 'SYSTEM DROP FILESYSTEM CACHE' = 99, 'SYSTEM DROP SCHEMA CACHE' = 100, 'SYSTEM DROP CACHE' = 101, 'SYSTEM RELOAD CONFIG' = 102, 'SYSTEM RELOAD USERS' = 103, 'SYSTEM RELOAD SYMBOLS' = 104, 'SYSTEM RELOAD DICTIONARY' = 105, 'SYSTEM RELOAD MODEL' = 106, 'SYSTEM RELOAD FUNCTION' = 107, 'SYSTEM RELOAD EMBEDDED DICTIONARIES' = 108, 'SYSTEM RELOAD' = 109, 'SYSTEM RESTART DISK' = 110, 'SYSTEM MERGES' = 111, 'SYSTEM TTL MERGES' = 112, 'SYSTEM FETCHES' = 113, 'SYSTEM MOVES' = 114, 'SYSTEM DISTRIBUTED SENDS' = 115, 'SYSTEM REPLICATED SENDS' = 116, 'SYSTEM SENDS' = 117, 'SYSTEM REPLICATION QUEUES' = 118, 'SYSTEM DROP REPLICA' = 119, 'SYSTEM SYNC REPLICA' = 120, 'SYSTEM RESTART REPLICA' = 121, 'SYSTEM RESTORE REPLICA' = 122, 'SYSTEM SYNC DATABASE REPLICA' = 123, 'SYSTEM SYNC TRANSACTION LOG' = 124, 'SYSTEM FLUSH DISTRIBUTED' = 125, 'SYSTEM FLUSH LOGS' = 126, 'SYSTEM FLUSH' = 127, 'SYSTEM THREAD FUZZER' = 128, 'SYSTEM UNFREEZE' = 129, 'SYSTEM' = 130, 'dictGet' = 131, 'addressToLine' = 132, 'addressToLineWithInlines' = 133, 'addressToSymbol' = 134, 'demangle' = 135, 'INTROSPECTION' = 136, 'FILE' = 137, 'URL' = 138, 'REMOTE' = 139, 'MONGO' = 140, 'MEILISEARCH' = 141, 'MYSQL' = 142, 'POSTGRES' = 143, 'SQLITE' = 144, 'ODBC' = 145, 'JDBC' = 146, 'HDFS' = 147, 'S3' = 148, 'HIVE' = 149, 'SOURCES' = 150, 'CLUSTER' = 151, 'ALL' = 152, 'NONE' = 153),\n+    `privilege` Enum16('SHOW DATABASES' = 0, 'SHOW TABLES' = 1, 'SHOW COLUMNS' = 2, 'SHOW DICTIONARIES' = 3, 'SHOW' = 4, 'SHOW FILESYSTEM CACHES' = 5, 'SELECT' = 6, 'INSERT' = 7, 'ALTER UPDATE' = 8, 'ALTER DELETE' = 9, 'ALTER ADD COLUMN' = 10, 'ALTER MODIFY COLUMN' = 11, 'ALTER DROP COLUMN' = 12, 'ALTER COMMENT COLUMN' = 13, 'ALTER CLEAR COLUMN' = 14, 'ALTER RENAME COLUMN' = 15, 'ALTER MATERIALIZE COLUMN' = 16, 'ALTER COLUMN' = 17, 'ALTER MODIFY COMMENT' = 18, 'ALTER ORDER BY' = 19, 'ALTER SAMPLE BY' = 20, 'ALTER ADD INDEX' = 21, 'ALTER DROP INDEX' = 22, 'ALTER MATERIALIZE INDEX' = 23, 'ALTER CLEAR INDEX' = 24, 'ALTER INDEX' = 25, 'ALTER ADD PROJECTION' = 26, 'ALTER DROP PROJECTION' = 27, 'ALTER MATERIALIZE PROJECTION' = 28, 'ALTER CLEAR PROJECTION' = 29, 'ALTER PROJECTION' = 30, 'ALTER ADD CONSTRAINT' = 31, 'ALTER DROP CONSTRAINT' = 32, 'ALTER CONSTRAINT' = 33, 'ALTER TTL' = 34, 'ALTER MATERIALIZE TTL' = 35, 'ALTER SETTINGS' = 36, 'ALTER MOVE PARTITION' = 37, 'ALTER FETCH PARTITION' = 38, 'ALTER FREEZE PARTITION' = 39, 'ALTER DATABASE SETTINGS' = 40, 'ALTER NAMED COLLECTION' = 41, 'ALTER TABLE' = 42, 'ALTER DATABASE' = 43, 'ALTER VIEW REFRESH' = 44, 'ALTER VIEW MODIFY QUERY' = 45, 'ALTER VIEW' = 46, 'ALTER' = 47, 'CREATE DATABASE' = 48, 'CREATE TABLE' = 49, 'CREATE VIEW' = 50, 'CREATE DICTIONARY' = 51, 'CREATE TEMPORARY TABLE' = 52, 'CREATE FUNCTION' = 53, 'CREATE NAMED COLLECTION' = 54, 'CREATE' = 55, 'DROP DATABASE' = 56, 'DROP TABLE' = 57, 'DROP VIEW' = 58, 'DROP DICTIONARY' = 59, 'DROP FUNCTION' = 60, 'DROP NAMED COLLECTION' = 61, 'DROP' = 62, 'TRUNCATE' = 63, 'OPTIMIZE' = 64, 'BACKUP' = 65, 'KILL QUERY' = 66, 'KILL TRANSACTION' = 67, 'MOVE PARTITION BETWEEN SHARDS' = 68, 'CREATE USER' = 69, 'ALTER USER' = 70, 'DROP USER' = 71, 'CREATE ROLE' = 72, 'ALTER ROLE' = 73, 'DROP ROLE' = 74, 'ROLE ADMIN' = 75, 'CREATE ROW POLICY' = 76, 'ALTER ROW POLICY' = 77, 'DROP ROW POLICY' = 78, 'CREATE QUOTA' = 79, 'ALTER QUOTA' = 80, 'DROP QUOTA' = 81, 'CREATE SETTINGS PROFILE' = 82, 'ALTER SETTINGS PROFILE' = 83, 'DROP SETTINGS PROFILE' = 84, 'SHOW USERS' = 85, 'SHOW ROLES' = 86, 'SHOW ROW POLICIES' = 87, 'SHOW QUOTAS' = 88, 'SHOW SETTINGS PROFILES' = 89, 'SHOW ACCESS' = 90, 'SHOW NAMED COLLECTIONS' = 91, 'ACCESS MANAGEMENT' = 92, 'SYSTEM SHUTDOWN' = 93, 'SYSTEM DROP DNS CACHE' = 94, 'SYSTEM DROP MARK CACHE' = 95, 'SYSTEM DROP UNCOMPRESSED CACHE' = 96, 'SYSTEM DROP MMAP CACHE' = 97, 'SYSTEM DROP COMPILED EXPRESSION CACHE' = 98, 'SYSTEM DROP FILESYSTEM CACHE' = 99, 'SYSTEM DROP SCHEMA CACHE' = 100, 'SYSTEM DROP CACHE' = 101, 'SYSTEM RELOAD CONFIG' = 102, 'SYSTEM RELOAD USERS' = 103, 'SYSTEM RELOAD SYMBOLS' = 104, 'SYSTEM RELOAD DICTIONARY' = 105, 'SYSTEM RELOAD MODEL' = 106, 'SYSTEM RELOAD FUNCTION' = 107, 'SYSTEM RELOAD EMBEDDED DICTIONARIES' = 108, 'SYSTEM RELOAD' = 109, 'SYSTEM RESTART DISK' = 110, 'SYSTEM MERGES' = 111, 'SYSTEM TTL MERGES' = 112, 'SYSTEM FETCHES' = 113, 'SYSTEM MOVES' = 114, 'SYSTEM DISTRIBUTED SENDS' = 115, 'SYSTEM REPLICATED SENDS' = 116, 'SYSTEM SENDS' = 117, 'SYSTEM REPLICATION QUEUES' = 118, 'SYSTEM DROP REPLICA' = 119, 'SYSTEM SYNC REPLICA' = 120, 'SYSTEM RESTART REPLICA' = 121, 'SYSTEM RESTORE REPLICA' = 122, 'SYSTEM WAIT LOADING PARTS' = 123, 'SYSTEM SYNC DATABASE REPLICA' = 124, 'SYSTEM SYNC TRANSACTION LOG' = 125, 'SYSTEM FLUSH DISTRIBUTED' = 126, 'SYSTEM FLUSH LOGS' = 127, 'SYSTEM FLUSH' = 128, 'SYSTEM THREAD FUZZER' = 129, 'SYSTEM UNFREEZE' = 130, 'SYSTEM' = 131, 'dictGet' = 132, 'addressToLine' = 133, 'addressToLineWithInlines' = 134, 'addressToSymbol' = 135, 'demangle' = 136, 'INTROSPECTION' = 137, 'FILE' = 138, 'URL' = 139, 'REMOTE' = 140, 'MONGO' = 141, 'MEILISEARCH' = 142, 'MYSQL' = 143, 'POSTGRES' = 144, 'SQLITE' = 145, 'ODBC' = 146, 'JDBC' = 147, 'HDFS' = 148, 'S3' = 149, 'HIVE' = 150, 'SOURCES' = 151, 'CLUSTER' = 152, 'ALL' = 153, 'NONE' = 154),\n     `aliases` Array(String),\n     `level` Nullable(Enum8('GLOBAL' = 0, 'DATABASE' = 1, 'TABLE' = 2, 'DICTIONARY' = 3, 'VIEW' = 4, 'COLUMN' = 5)),\n-    `parent_group` Nullable(Enum16('SHOW DATABASES' = 0, 'SHOW TABLES' = 1, 'SHOW COLUMNS' = 2, 'SHOW DICTIONARIES' = 3, 'SHOW' = 4, 'SHOW FILESYSTEM CACHES' = 5, 'SELECT' = 6, 'INSERT' = 7, 'ALTER UPDATE' = 8, 'ALTER DELETE' = 9, 'ALTER ADD COLUMN' = 10, 'ALTER MODIFY COLUMN' = 11, 'ALTER DROP COLUMN' = 12, 'ALTER COMMENT COLUMN' = 13, 'ALTER CLEAR COLUMN' = 14, 'ALTER RENAME COLUMN' = 15, 'ALTER MATERIALIZE COLUMN' = 16, 'ALTER COLUMN' = 17, 'ALTER MODIFY COMMENT' = 18, 'ALTER ORDER BY' = 19, 'ALTER SAMPLE BY' = 20, 'ALTER ADD INDEX' = 21, 'ALTER DROP INDEX' = 22, 'ALTER MATERIALIZE INDEX' = 23, 'ALTER CLEAR INDEX' = 24, 'ALTER INDEX' = 25, 'ALTER ADD PROJECTION' = 26, 'ALTER DROP PROJECTION' = 27, 'ALTER MATERIALIZE PROJECTION' = 28, 'ALTER CLEAR PROJECTION' = 29, 'ALTER PROJECTION' = 30, 'ALTER ADD CONSTRAINT' = 31, 'ALTER DROP CONSTRAINT' = 32, 'ALTER CONSTRAINT' = 33, 'ALTER TTL' = 34, 'ALTER MATERIALIZE TTL' = 35, 'ALTER SETTINGS' = 36, 'ALTER MOVE PARTITION' = 37, 'ALTER FETCH PARTITION' = 38, 'ALTER FREEZE PARTITION' = 39, 'ALTER DATABASE SETTINGS' = 40, 'ALTER NAMED COLLECTION' = 41, 'ALTER TABLE' = 42, 'ALTER DATABASE' = 43, 'ALTER VIEW REFRESH' = 44, 'ALTER VIEW MODIFY QUERY' = 45, 'ALTER VIEW' = 46, 'ALTER' = 47, 'CREATE DATABASE' = 48, 'CREATE TABLE' = 49, 'CREATE VIEW' = 50, 'CREATE DICTIONARY' = 51, 'CREATE TEMPORARY TABLE' = 52, 'CREATE FUNCTION' = 53, 'CREATE NAMED COLLECTION' = 54, 'CREATE' = 55, 'DROP DATABASE' = 56, 'DROP TABLE' = 57, 'DROP VIEW' = 58, 'DROP DICTIONARY' = 59, 'DROP FUNCTION' = 60, 'DROP NAMED COLLECTION' = 61, 'DROP' = 62, 'TRUNCATE' = 63, 'OPTIMIZE' = 64, 'BACKUP' = 65, 'KILL QUERY' = 66, 'KILL TRANSACTION' = 67, 'MOVE PARTITION BETWEEN SHARDS' = 68, 'CREATE USER' = 69, 'ALTER USER' = 70, 'DROP USER' = 71, 'CREATE ROLE' = 72, 'ALTER ROLE' = 73, 'DROP ROLE' = 74, 'ROLE ADMIN' = 75, 'CREATE ROW POLICY' = 76, 'ALTER ROW POLICY' = 77, 'DROP ROW POLICY' = 78, 'CREATE QUOTA' = 79, 'ALTER QUOTA' = 80, 'DROP QUOTA' = 81, 'CREATE SETTINGS PROFILE' = 82, 'ALTER SETTINGS PROFILE' = 83, 'DROP SETTINGS PROFILE' = 84, 'SHOW USERS' = 85, 'SHOW ROLES' = 86, 'SHOW ROW POLICIES' = 87, 'SHOW QUOTAS' = 88, 'SHOW SETTINGS PROFILES' = 89, 'SHOW ACCESS' = 90, 'SHOW NAMED COLLECTIONS' = 91, 'ACCESS MANAGEMENT' = 92, 'SYSTEM SHUTDOWN' = 93, 'SYSTEM DROP DNS CACHE' = 94, 'SYSTEM DROP MARK CACHE' = 95, 'SYSTEM DROP UNCOMPRESSED CACHE' = 96, 'SYSTEM DROP MMAP CACHE' = 97, 'SYSTEM DROP COMPILED EXPRESSION CACHE' = 98, 'SYSTEM DROP FILESYSTEM CACHE' = 99, 'SYSTEM DROP SCHEMA CACHE' = 100, 'SYSTEM DROP CACHE' = 101, 'SYSTEM RELOAD CONFIG' = 102, 'SYSTEM RELOAD USERS' = 103, 'SYSTEM RELOAD SYMBOLS' = 104, 'SYSTEM RELOAD DICTIONARY' = 105, 'SYSTEM RELOAD MODEL' = 106, 'SYSTEM RELOAD FUNCTION' = 107, 'SYSTEM RELOAD EMBEDDED DICTIONARIES' = 108, 'SYSTEM RELOAD' = 109, 'SYSTEM RESTART DISK' = 110, 'SYSTEM MERGES' = 111, 'SYSTEM TTL MERGES' = 112, 'SYSTEM FETCHES' = 113, 'SYSTEM MOVES' = 114, 'SYSTEM DISTRIBUTED SENDS' = 115, 'SYSTEM REPLICATED SENDS' = 116, 'SYSTEM SENDS' = 117, 'SYSTEM REPLICATION QUEUES' = 118, 'SYSTEM DROP REPLICA' = 119, 'SYSTEM SYNC REPLICA' = 120, 'SYSTEM RESTART REPLICA' = 121, 'SYSTEM RESTORE REPLICA' = 122, 'SYSTEM SYNC DATABASE REPLICA' = 123, 'SYSTEM SYNC TRANSACTION LOG' = 124, 'SYSTEM FLUSH DISTRIBUTED' = 125, 'SYSTEM FLUSH LOGS' = 126, 'SYSTEM FLUSH' = 127, 'SYSTEM THREAD FUZZER' = 128, 'SYSTEM UNFREEZE' = 129, 'SYSTEM' = 130, 'dictGet' = 131, 'addressToLine' = 132, 'addressToLineWithInlines' = 133, 'addressToSymbol' = 134, 'demangle' = 135, 'INTROSPECTION' = 136, 'FILE' = 137, 'URL' = 138, 'REMOTE' = 139, 'MONGO' = 140, 'MEILISEARCH' = 141, 'MYSQL' = 142, 'POSTGRES' = 143, 'SQLITE' = 144, 'ODBC' = 145, 'JDBC' = 146, 'HDFS' = 147, 'S3' = 148, 'HIVE' = 149, 'SOURCES' = 150, 'CLUSTER' = 151, 'ALL' = 152, 'NONE' = 153))\n+    `parent_group` Nullable(Enum16('SHOW DATABASES' = 0, 'SHOW TABLES' = 1, 'SHOW COLUMNS' = 2, 'SHOW DICTIONARIES' = 3, 'SHOW' = 4, 'SHOW FILESYSTEM CACHES' = 5, 'SELECT' = 6, 'INSERT' = 7, 'ALTER UPDATE' = 8, 'ALTER DELETE' = 9, 'ALTER ADD COLUMN' = 10, 'ALTER MODIFY COLUMN' = 11, 'ALTER DROP COLUMN' = 12, 'ALTER COMMENT COLUMN' = 13, 'ALTER CLEAR COLUMN' = 14, 'ALTER RENAME COLUMN' = 15, 'ALTER MATERIALIZE COLUMN' = 16, 'ALTER COLUMN' = 17, 'ALTER MODIFY COMMENT' = 18, 'ALTER ORDER BY' = 19, 'ALTER SAMPLE BY' = 20, 'ALTER ADD INDEX' = 21, 'ALTER DROP INDEX' = 22, 'ALTER MATERIALIZE INDEX' = 23, 'ALTER CLEAR INDEX' = 24, 'ALTER INDEX' = 25, 'ALTER ADD PROJECTION' = 26, 'ALTER DROP PROJECTION' = 27, 'ALTER MATERIALIZE PROJECTION' = 28, 'ALTER CLEAR PROJECTION' = 29, 'ALTER PROJECTION' = 30, 'ALTER ADD CONSTRAINT' = 31, 'ALTER DROP CONSTRAINT' = 32, 'ALTER CONSTRAINT' = 33, 'ALTER TTL' = 34, 'ALTER MATERIALIZE TTL' = 35, 'ALTER SETTINGS' = 36, 'ALTER MOVE PARTITION' = 37, 'ALTER FETCH PARTITION' = 38, 'ALTER FREEZE PARTITION' = 39, 'ALTER DATABASE SETTINGS' = 40, 'ALTER NAMED COLLECTION' = 41, 'ALTER TABLE' = 42, 'ALTER DATABASE' = 43, 'ALTER VIEW REFRESH' = 44, 'ALTER VIEW MODIFY QUERY' = 45, 'ALTER VIEW' = 46, 'ALTER' = 47, 'CREATE DATABASE' = 48, 'CREATE TABLE' = 49, 'CREATE VIEW' = 50, 'CREATE DICTIONARY' = 51, 'CREATE TEMPORARY TABLE' = 52, 'CREATE FUNCTION' = 53, 'CREATE NAMED COLLECTION' = 54, 'CREATE' = 55, 'DROP DATABASE' = 56, 'DROP TABLE' = 57, 'DROP VIEW' = 58, 'DROP DICTIONARY' = 59, 'DROP FUNCTION' = 60, 'DROP NAMED COLLECTION' = 61, 'DROP' = 62, 'TRUNCATE' = 63, 'OPTIMIZE' = 64, 'BACKUP' = 65, 'KILL QUERY' = 66, 'KILL TRANSACTION' = 67, 'MOVE PARTITION BETWEEN SHARDS' = 68, 'CREATE USER' = 69, 'ALTER USER' = 70, 'DROP USER' = 71, 'CREATE ROLE' = 72, 'ALTER ROLE' = 73, 'DROP ROLE' = 74, 'ROLE ADMIN' = 75, 'CREATE ROW POLICY' = 76, 'ALTER ROW POLICY' = 77, 'DROP ROW POLICY' = 78, 'CREATE QUOTA' = 79, 'ALTER QUOTA' = 80, 'DROP QUOTA' = 81, 'CREATE SETTINGS PROFILE' = 82, 'ALTER SETTINGS PROFILE' = 83, 'DROP SETTINGS PROFILE' = 84, 'SHOW USERS' = 85, 'SHOW ROLES' = 86, 'SHOW ROW POLICIES' = 87, 'SHOW QUOTAS' = 88, 'SHOW SETTINGS PROFILES' = 89, 'SHOW ACCESS' = 90, 'SHOW NAMED COLLECTIONS' = 91, 'ACCESS MANAGEMENT' = 92, 'SYSTEM SHUTDOWN' = 93, 'SYSTEM DROP DNS CACHE' = 94, 'SYSTEM DROP MARK CACHE' = 95, 'SYSTEM DROP UNCOMPRESSED CACHE' = 96, 'SYSTEM DROP MMAP CACHE' = 97, 'SYSTEM DROP COMPILED EXPRESSION CACHE' = 98, 'SYSTEM DROP FILESYSTEM CACHE' = 99, 'SYSTEM DROP SCHEMA CACHE' = 100, 'SYSTEM DROP CACHE' = 101, 'SYSTEM RELOAD CONFIG' = 102, 'SYSTEM RELOAD USERS' = 103, 'SYSTEM RELOAD SYMBOLS' = 104, 'SYSTEM RELOAD DICTIONARY' = 105, 'SYSTEM RELOAD MODEL' = 106, 'SYSTEM RELOAD FUNCTION' = 107, 'SYSTEM RELOAD EMBEDDED DICTIONARIES' = 108, 'SYSTEM RELOAD' = 109, 'SYSTEM RESTART DISK' = 110, 'SYSTEM MERGES' = 111, 'SYSTEM TTL MERGES' = 112, 'SYSTEM FETCHES' = 113, 'SYSTEM MOVES' = 114, 'SYSTEM DISTRIBUTED SENDS' = 115, 'SYSTEM REPLICATED SENDS' = 116, 'SYSTEM SENDS' = 117, 'SYSTEM REPLICATION QUEUES' = 118, 'SYSTEM DROP REPLICA' = 119, 'SYSTEM SYNC REPLICA' = 120, 'SYSTEM RESTART REPLICA' = 121, 'SYSTEM RESTORE REPLICA' = 122, 'SYSTEM WAIT LOADING PARTS' = 123, 'SYSTEM SYNC DATABASE REPLICA' = 124, 'SYSTEM SYNC TRANSACTION LOG' = 125, 'SYSTEM FLUSH DISTRIBUTED' = 126, 'SYSTEM FLUSH LOGS' = 127, 'SYSTEM FLUSH' = 128, 'SYSTEM THREAD FUZZER' = 129, 'SYSTEM UNFREEZE' = 130, 'SYSTEM' = 131, 'dictGet' = 132, 'addressToLine' = 133, 'addressToLineWithInlines' = 134, 'addressToSymbol' = 135, 'demangle' = 136, 'INTROSPECTION' = 137, 'FILE' = 138, 'URL' = 139, 'REMOTE' = 140, 'MONGO' = 141, 'MEILISEARCH' = 142, 'MYSQL' = 143, 'POSTGRES' = 144, 'SQLITE' = 145, 'ODBC' = 146, 'JDBC' = 147, 'HDFS' = 148, 'S3' = 149, 'HIVE' = 150, 'SOURCES' = 151, 'CLUSTER' = 152, 'ALL' = 153, 'NONE' = 154))\n )\n ENGINE = SystemPrivileges\n COMMENT 'SYSTEM TABLE is built on the fly.'\ndiff --git a/tests/queries/0_stateless/02482_load_parts_refcounts.reference b/tests/queries/0_stateless/02482_load_parts_refcounts.reference\nnew file mode 100644\nindex 000000000000..d00491fd7e5b\n--- /dev/null\n+++ b/tests/queries/0_stateless/02482_load_parts_refcounts.reference\n@@ -0,0 +1,1 @@\n+1\ndiff --git a/tests/queries/0_stateless/02482_load_parts_refcounts.sh b/tests/queries/0_stateless/02482_load_parts_refcounts.sh\nnew file mode 100755\nindex 000000000000..27549499a45a\n--- /dev/null\n+++ b/tests/queries/0_stateless/02482_load_parts_refcounts.sh\n@@ -0,0 +1,54 @@\n+#!/usr/bin/env bash\n+# Tags: zookeeper\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+function query_with_retry\n+{\n+    retry=0\n+    until [ $retry -ge 5 ]\n+    do\n+        result=$($CLICKHOUSE_CLIENT $2 --query=\"$1\" 2>&1)\n+        if [ \"$?\" == 0 ]; then\n+            echo -n \"$result\"\n+            return\n+        else\n+            retry=$(($retry + 1))\n+            sleep 3\n+        fi\n+    done\n+    echo \"Query '$1' failed with '$result'\"\n+}\n+\n+$CLICKHOUSE_CLIENT -n --query \"\n+    DROP TABLE IF EXISTS load_parts_refcounts SYNC;\n+\n+    CREATE TABLE load_parts_refcounts (id UInt32)\n+    ENGINE = ReplicatedMergeTree('/test/02482_load_parts_refcounts/{database}/{table}', '1')\n+    ORDER BY id;\n+\n+    SYSTEM STOP MERGES load_parts_refcounts;\n+\n+    INSERT INTO load_parts_refcounts VALUES (1);\n+    INSERT INTO load_parts_refcounts VALUES (2);\n+    INSERT INTO load_parts_refcounts VALUES (3);\n+\n+    SYSTEM START MERGES load_parts_refcounts;\n+\"\n+\n+query_with_retry \"OPTIMIZE TABLE load_parts_refcounts FINAL SETTINGS optimize_throw_if_noop = 1\"\n+\n+$CLICKHOUSE_CLIENT --query \"DETACH TABLE load_parts_refcounts\"\n+$CLICKHOUSE_CLIENT --query \"ATTACH TABLE load_parts_refcounts\"\n+\n+query_with_retry \"\n+    SELECT throwIf(count() == 0) FROM system.parts\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 'load_parts_refcounts' AND NOT active FORMAT Null\"\n+\n+$CLICKHOUSE_CLIENT --query \"\n+    SELECT DISTINCT refcount FROM system.parts\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 'load_parts_refcounts' AND NOT active\"\n+\n+$CLICKHOUSE_CLIENT --query \"DROP TABLE load_parts_refcounts SYNC\"\n",
  "problem_statement": "Speed up loading of data parts at startup.\n1. Do not load inactive data parts that are old enough (they will be deleted nevertheless). But still use them if we need to repair broken part at startup. Delete old inactive data parts at startup.\r\n\r\n2. Use multiple threads for loading data parts to mitigate latency.\n",
  "hints_text": "\u21162 was implemented.\r\n\u21161 is still worth doing.\nPerhaps we should also separate table loading and table starting up, or else merge/fetch will interfere with the part loading process.\nParts are not loaded in parallel :(\r\n\r\n```\r\nThread 53 (Thread 0x7f33a65fd700 (LWP 9216)):\r\n#0  __libc_pread64 (offset=<optimized out>, count=3288, buf=0x7f33b9e44000, fd=25) at ../sysdeps/unix/sysv/linux/pread64.c:29\r\n#1  __libc_pread64 (fd=25, buf=0x7f33b9e44000, count=3288, offset=0) at ../sysdeps/unix/sysv/linux/pread64.c:27\r\n#2  0x000000000a729dc1 in DB::ReadBufferFromFileDescriptor::nextImpl (this=0x7f33b9e53000) at ./src/IO/ReadBufferFromFileDescriptor.cpp:73\r\n#3  0x000000001319b7ac in DB::ReadBuffer::next (this=0x7f33b9e53000) at ./src/IO/ReadBuffer.h:62\r\n#4  DB::ReadBuffer::eof (this=0x7f33b9e53000) at ./src/IO/ReadBuffer.h:96\r\n#5  DB::MergeTreeDataPartWide::loadIndexGranularity (this=0x7f331384cc18) at ./src/Storages/MergeTree/MergeTreeDataPartWide.cpp:129\r\n#6  0x00000000130d9f88 in DB::IMergeTreeDataPart::loadColumnsChecksumsIndexes (this=0x7f331384cc18, require_columns_checksums=<optimized out>, check_consistency=true) at ./src/Storages/MergeTree/IMergeTreeDataPart.cpp:618\r\n#7  0x0000000013173458 in DB::MergeTreeData::loadDataPartsFromDisk(std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const> > >&, std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const> > >&, ThreadPoolImpl<ThreadFromGlobalPool>&, unsigned long, std::__1::queue<std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> > > >, std::__1::deque<std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> > > >, std::__1::allocator<std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> > > > > > >&, bool, std::__1::shared_ptr<DB::MergeTreeSettings const> const&)::$_11::operator()(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::IDisk> const&) const (this=0x7f33a6df2f50, part_name=..., part_disk_ptr=...) at ./src/Storages/MergeTree/MergeTreeData.cpp:1016\r\n#8  DB::MergeTreeData::loadDataPartsFromDisk(std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const> > >&, std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const> > >&, ThreadPoolImpl<ThreadFromGlobalPool>&, unsigned long, std::__1::queue<std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> > > >, std::__1::deque<std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> > > >, std::__1::allocator<std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::shared_ptr<DB::IDisk> > > > > > >&, bool, std::__1::shared_ptr<DB::MergeTreeSettings const> const&)::$_12::operator()() const (this=<optimized out>) at ./src/Storages/MergeTree/MergeTreeData.cpp:1110\r\n#9  0x000000000a76db09 in std::__1::__function::__policy_func<void ()>::operator()() const (this=0x7f33a65f3cd0) at ./contrib/libcxx/include/functional:2221\r\n#10 std::__1::function<void ()>::operator()() const (this=0x7f33a65f3cd0) at ./contrib/libcxx/include/functional:2560\r\n#11 ThreadPoolImpl<ThreadFromGlobalPool>::worker (this=this@entry=0x7f33a6df3260, thread_it=thread_it@entry=...) at ./src/Common/ThreadPool.cpp:274\r\n#12 0x000000000a76f38c in ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}::operator()() const (this=<optimized out>) at ./src/Common/ThreadPool.cpp:139\r\n#13 std::__1::__invoke_constexpr<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}&> (__f=...) at ./contrib/libcxx/include/type_traits:3682\r\n#14 std::__1::__apply_tuple_impl<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}&, std::__1::tuple<>&>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}&, std::__1::tuple<>&, std::__1::__tuple_indices<>) (__f=..., __t=...) at ./contrib/libcxx/include/tuple:1415\r\n#15 std::__1::apply<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}&, std::__1::tuple<>&>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}&, std::__1::tuple<>&) (__f=..., __t=...) at ./contrib/libcxx/include/tuple:1424\r\n#16 ThreadFromGlobalPool::ThreadFromGlobalPool<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}&&)::{lambda()#1}::operator()() (this=0x7f33bd8ec940) at ./src/Common/ThreadPool.h:188\r\n#17 0x000000000a76c0aa in std::__1::__function::__policy_func<void ()>::operator()() const (this=0x7f33a65f4010) at ./contrib/libcxx/include/functional:2221\r\n#18 std::__1::function<void ()>::operator()() const (this=0x7f33a65f4010) at ./contrib/libcxx/include/functional:2560\r\n#19 ThreadPoolImpl<std::__1::thread>::worker (this=0x7f344903ea00, thread_it=...) at ./src/Common/ThreadPool.cpp:274\r\n#20 0x000000000a76e50e in ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}::operator()() const (this=0x7f34490847e8) at ./src/Common/ThreadPool.cpp:139\r\n#21 std::__1::__invoke<ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}> (__f=...) at ./contrib/libcxx/include/type_traits:3676\r\n#22 std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}>&, std::__1::__tuple_indices<>) (__t=...) at ./contrib/libcxx/include/thread:280\r\n#23 std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}> >(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::{lambda()#2}>) (__vp=0x7f34490847e0) at ./contrib/libcxx/include/thread:291\r\n#24 0x00007f344a125609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#25 0x00007f344a04c293 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\nBroken here: #6489\n@CurtizJ said he can implement \u21161.\r\nIt is nontrivial - requires building a tree of data parts according to their names, then start loading by breadth-first traversal.",
  "created_at": "2022-10-08T01:44:22Z",
  "modified_files": [
    "src/Access/Common/AccessType.h",
    "src/Interpreters/InterpreterSystemQuery.cpp",
    "src/Interpreters/InterpreterSystemQuery.h",
    "src/Interpreters/MergeTreeTransaction.cpp",
    "src/Parsers/ASTSystemQuery.cpp",
    "src/Parsers/ASTSystemQuery.h",
    "src/Parsers/ParserSystemQuery.cpp",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeData.h",
    "src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp",
    "src/Storages/StorageMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp"
  ],
  "modified_test_files": [
    "src/Access/tests/gtest_access_rights_ops.cpp",
    "tests/integration/test_grant_and_revoke/test.py",
    "b/tests/integration/test_merge_tree_load_parts/configs/fast_background_pool.xml",
    "b/tests/integration/test_merge_tree_load_parts/test.py",
    "tests/integration/test_transactions/test.py",
    "tests/queries/0_stateless/01271_show_privileges.reference",
    "tests/queries/0_stateless/02117_show_create_table_system.reference",
    "b/tests/queries/0_stateless/02482_load_parts_refcounts.reference",
    "b/tests/queries/0_stateless/02482_load_parts_refcounts.sh"
  ]
}