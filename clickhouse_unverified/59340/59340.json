{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 59340,
  "instance_id": "ClickHouse__ClickHouse-59340",
  "issue_numbers": [
    "57773"
  ],
  "base_commit": "a51aa7b6683ec58d93aab797c1a71d5ebb2460fd",
  "patch": "diff --git a/src/Processors/Transforms/ColumnGathererTransform.cpp b/src/Processors/Transforms/ColumnGathererTransform.cpp\nindex d7f52a538e1b..b2e8e9bc89e8 100644\n--- a/src/Processors/Transforms/ColumnGathererTransform.cpp\n+++ b/src/Processors/Transforms/ColumnGathererTransform.cpp\n@@ -17,9 +17,14 @@ namespace ErrorCodes\n }\n \n ColumnGathererStream::ColumnGathererStream(\n-    size_t num_inputs, ReadBuffer & row_sources_buf_, size_t block_preferred_size_)\n-    : sources(num_inputs), row_sources_buf(row_sources_buf_)\n-    , block_preferred_size(block_preferred_size_)\n+    size_t num_inputs,\n+    ReadBuffer & row_sources_buf_,\n+    size_t block_preferred_size_rows_,\n+    size_t block_preferred_size_bytes_)\n+    : sources(num_inputs)\n+    , row_sources_buf(row_sources_buf_)\n+    , block_preferred_size_rows(block_preferred_size_rows_)\n+    , block_preferred_size_bytes(block_preferred_size_bytes_)\n {\n     if (num_inputs == 0)\n         throw Exception(ErrorCodes::EMPTY_DATA_PASSED, \"There are no streams to gather\");\n@@ -124,10 +129,11 @@ ColumnGathererTransform::ColumnGathererTransform(\n     const Block & header,\n     size_t num_inputs,\n     ReadBuffer & row_sources_buf_,\n-    size_t block_preferred_size_)\n+    size_t block_preferred_size_rows_,\n+    size_t block_preferred_size_bytes_)\n     : IMergingTransform<ColumnGathererStream>(\n         num_inputs, header, header, /*have_all_inputs_=*/ true, /*limit_hint_=*/ 0, /*always_read_till_end_=*/ false,\n-        num_inputs, row_sources_buf_, block_preferred_size_)\n+        num_inputs, row_sources_buf_, block_preferred_size_rows_, block_preferred_size_bytes_)\n     , log(getLogger(\"ColumnGathererStream\"))\n {\n     if (header.columns() != 1)\ndiff --git a/src/Processors/Transforms/ColumnGathererTransform.h b/src/Processors/Transforms/ColumnGathererTransform.h\nindex 885cb3f81ba3..821d04db0df1 100644\n--- a/src/Processors/Transforms/ColumnGathererTransform.h\n+++ b/src/Processors/Transforms/ColumnGathererTransform.h\n@@ -5,7 +5,6 @@\n #include <Processors/Merges/Algorithms/IMergingAlgorithm.h>\n #include <Processors/Merges/IMergingTransform.h>\n \n-\n namespace Poco { class Logger; }\n \n \n@@ -57,7 +56,11 @@ using MergedRowSources = PODArray<RowSourcePart>;\n class ColumnGathererStream final : public IMergingAlgorithm\n {\n public:\n-    ColumnGathererStream(size_t num_inputs, ReadBuffer & row_sources_buf_, size_t block_preferred_size_ = DEFAULT_BLOCK_SIZE);\n+    ColumnGathererStream(\n+        size_t num_inputs,\n+        ReadBuffer & row_sources_buf_,\n+        size_t block_preferred_size_rows_,\n+        size_t block_preferred_size_bytes_);\n \n     const char * getName() const override { return \"ColumnGathererStream\"; }\n     void initialize(Inputs inputs) override;\n@@ -92,13 +95,12 @@ class ColumnGathererStream final : public IMergingAlgorithm\n     std::vector<Source> sources;\n     ReadBuffer & row_sources_buf;\n \n-    const size_t block_preferred_size;\n+    const size_t block_preferred_size_rows;\n+    const size_t block_preferred_size_bytes;\n \n     Source * source_to_fully_copy = nullptr;\n \n     ssize_t next_required_source = -1;\n-    size_t cur_block_preferred_size = 0;\n-\n     UInt64 merged_rows = 0;\n     UInt64 merged_bytes = 0;\n };\n@@ -110,7 +112,8 @@ class ColumnGathererTransform final : public IMergingTransform<ColumnGathererStr\n         const Block & header,\n         size_t num_inputs,\n         ReadBuffer & row_sources_buf_,\n-        size_t block_preferred_size_ = DEFAULT_BLOCK_SIZE);\n+        size_t block_preferred_size_rows_,\n+        size_t block_preferred_size_bytes_);\n \n     String getName() const override { return \"ColumnGathererTransform\"; }\n \n@@ -134,14 +137,17 @@ void ColumnGathererStream::gather(Column & column_res)\n     if (next_required_source == -1)\n     {\n         /// Start new column.\n-        cur_block_preferred_size = std::min(static_cast<size_t>(row_sources_end - row_source_pos), block_preferred_size);\n-        column_res.reserve(cur_block_preferred_size);\n+        /// Actually reserve works only for fixed size columns.\n+        /// So it's safe to ignore preferred size in bytes and call reserve for number of rows.\n+        size_t size_to_reserve = std::min(static_cast<size_t>(row_sources_end - row_source_pos), block_preferred_size_rows);\n+        column_res.reserve(size_to_reserve);\n     }\n \n-    size_t cur_size = column_res.size();\n     next_required_source = -1;\n \n-    while (row_source_pos < row_sources_end && cur_size < cur_block_preferred_size)\n+    while (row_source_pos < row_sources_end\n+        && column_res.size() < block_preferred_size_rows\n+        && column_res.allocatedBytes() < block_preferred_size_bytes)\n     {\n         RowSourcePart row_source = *row_source_pos;\n         size_t source_num = row_source.getSourceNum();\n@@ -159,6 +165,7 @@ void ColumnGathererStream::gather(Column & column_res)\n         /// Consecutive optimization. TODO: precompute lengths\n         size_t len = 1;\n         size_t max_len = std::min(static_cast<size_t>(row_sources_end - row_source_pos), source.size - source.pos); // interval should be in the same block\n+\n         while (len < max_len && row_source_pos->data == row_source.data)\n         {\n             ++len;\n@@ -181,8 +188,6 @@ void ColumnGathererStream::gather(Column & column_res)\n                 column_res.insertFrom(*source.column, source.pos);\n             else\n                 column_res.insertRangeFrom(*source.column, source.pos, len);\n-\n-            cur_size += len;\n         }\n \n         source.pos += len;\ndiff --git a/src/Storages/MergeTree/MergeTask.cpp b/src/Storages/MergeTree/MergeTask.cpp\nindex 4b5b7ca8018a..59bdb7006b3a 100644\n--- a/src/Storages/MergeTree/MergeTask.cpp\n+++ b/src/Storages/MergeTree/MergeTask.cpp\n@@ -588,7 +588,15 @@ void MergeTask::VerticalMergeStage::prepareVerticalMergeForOneColumn() const\n     auto pipe = Pipe::unitePipes(std::move(pipes));\n \n     ctx->rows_sources_read_buf->seek(0, 0);\n-    auto transform = std::make_unique<ColumnGathererTransform>(pipe.getHeader(), pipe.numOutputPorts(), *ctx->rows_sources_read_buf);\n+\n+    const auto data_settings = global_ctx->data->getSettings();\n+    auto transform = std::make_unique<ColumnGathererTransform>(\n+        pipe.getHeader(),\n+        pipe.numOutputPorts(),\n+        *ctx->rows_sources_read_buf,\n+        data_settings->merge_max_block_size,\n+        data_settings->merge_max_block_size_bytes);\n+\n     pipe.addTransform(std::move(transform));\n \n     ctx->column_parts_pipeline = QueryPipeline(std::move(pipe));\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02981_vertical_merges_memory_usage.reference b/tests/queries/0_stateless/02981_vertical_merges_memory_usage.reference\nnew file mode 100644\nindex 000000000000..60c254e152bc\n--- /dev/null\n+++ b/tests/queries/0_stateless/02981_vertical_merges_memory_usage.reference\n@@ -0,0 +1,1 @@\n+Vertical\tOK\ndiff --git a/tests/queries/0_stateless/02981_vertical_merges_memory_usage.sql b/tests/queries/0_stateless/02981_vertical_merges_memory_usage.sql\nnew file mode 100644\nindex 000000000000..1305f02c0444\n--- /dev/null\n+++ b/tests/queries/0_stateless/02981_vertical_merges_memory_usage.sql\n@@ -0,0 +1,35 @@\n+-- Tags: long\n+\n+DROP TABLE IF EXISTS t_vertical_merge_memory;\n+\n+CREATE TABLE t_vertical_merge_memory (id UInt64, arr Array(String))\n+ENGINE = MergeTree ORDER BY id\n+SETTINGS\n+    min_bytes_for_wide_part = 0,\n+    vertical_merge_algorithm_min_rows_to_activate = 1,\n+    vertical_merge_algorithm_min_columns_to_activate = 1,\n+    index_granularity = 8192,\n+    index_granularity_bytes = '10M',\n+    merge_max_block_size = 8192,\n+    merge_max_block_size_bytes = '10M';\n+\n+INSERT INTO t_vertical_merge_memory SELECT number, arrayMap(x -> repeat('a', 50), range(1000)) FROM numbers(30000);\n+INSERT INTO t_vertical_merge_memory SELECT number, arrayMap(x -> repeat('a', 50), range(1000)) FROM numbers(30000);\n+\n+OPTIMIZE TABLE t_vertical_merge_memory FINAL;\n+\n+SYSTEM FLUSH LOGS;\n+\n+SELECT\n+    merge_algorithm,\n+    peak_memory_usage < 500 * 1024 * 1024\n+        ? 'OK'\n+        : format('FAIL: memory usage: {}', formatReadableSize(peak_memory_usage))\n+FROM system.part_log\n+WHERE\n+    database = currentDatabase()\n+    AND table = 't_vertical_merge_memory'\n+    AND event_type = 'MergeParts'\n+    AND length(merged_from) = 2;\n+\n+DROP TABLE IF EXISTS t_vertical_merge_memory;\n",
  "problem_statement": "Vertical merge could consume a lot of memory\nSee the stack trace with memory allocations.\r\nhttps://pastila.nl/?0001d4ae/376da778773f6223011c64914c06ec3e#xGswcayE+r52tA2oBpGs6A==\r\n\r\nStack trace in short:\r\nThere are big allocations (up to 2,5Gb) happen in \r\n```\r\nAllocator<false, false>, 63ul, 64ul>::resize<>(unsigned long)\r\nDB::ColumnString::insertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) | DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nDB::ColumnString::gather(DB::ColumnGathererStream&)','DB::ColumnGathererStream::merge()\r\n```\r\n\r\nThat happen because the value size is highly different. Table has a string column with \r\n```\r\nSELECT quantilesExactExclusive(0.5, 0.9, 0.99, 0.999)(length(column))\r\nFROM table\r\n\r\n\u250c\u2500quantilesExactExclusive(0.5, 0.9, 0.99, 0.999)(length(column))\u2500\u2510\r\n\u2502 [79064,121150,135871,1413296]                                      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```\r\nSELECT\r\n    max(length(column))\r\nFROM table\r\n\u250c\u2500max(length(column))\u2500\u2510\r\n\u2502      9834237        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThere is a big string could happen.\r\n\r\nColumnGathererStream doesn't care about column size in bytes. It always takes `#define DEFAULT_BLOCK_SIZE 65409` rows in gathered column.\r\n\r\nThat is very annoying reason why 8Gb memory is not enough for small databases. it leads to the situation when all data is read into the memory.\n",
  "hints_text": "Just add a `preferred_block_size_bytes` parameter to `ColumnGathererStream`.",
  "created_at": "2024-01-29T14:07:05Z"
}