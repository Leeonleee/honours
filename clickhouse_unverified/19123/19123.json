{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 19123,
  "instance_id": "ClickHouse__ClickHouse-19123",
  "issue_numbers": [
    "18913"
  ],
  "base_commit": "8d58ce532aaa3d1840fcba41cb84f118c9b944f9",
  "patch": "diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex ef64ec28e794..982345f8240f 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -26,7 +26,7 @@ Granules getGranulesToWrite(const MergeTreeIndexGranularity & index_granularity,\n \n     Granules result;\n     size_t current_row = 0;\n-    /// When our last mark is not finished yet and we have to write in rows into it\n+    /// When our last mark is not finished yet and we have to write rows into it\n     if (rows_written_in_last_mark > 0)\n     {\n         size_t rows_left_in_last_mark = index_granularity.getMarkRows(current_mark) - rows_written_in_last_mark;\n@@ -183,8 +183,8 @@ void MergeTreeDataPartWriterWide::write(const Block & block, const IColumn::Perm\n                 /// adjust last mark rows and flush to disk.\n                 if (rows_written_in_last_mark >= index_granularity_for_block)\n                     adjustLastMarkIfNeedAndFlushToDisk(rows_written_in_last_mark);\n-                else /// We still can write some rows from new block into previous granule.\n-                    adjustLastMarkIfNeedAndFlushToDisk(index_granularity_for_block - rows_written_in_last_mark);\n+                else /// We still can write some rows from new block into previous granule. So the granule size will be block granularity size.\n+                    adjustLastMarkIfNeedAndFlushToDisk(index_granularity_for_block);\n             }\n         }\n \n@@ -440,6 +440,13 @@ void MergeTreeDataPartWriterWide::validateColumnOfFixedSize(const String & name,\n                         \"Still have {} rows in bin stream, last mark #{} index granularity size {}, last rows {}\", column->size(), mark_num, index_granularity.getMarksCount(), index_granularity_rows);\n         }\n \n+        if (index_granularity_rows > data_part->index_granularity_info.fixed_index_granularity)\n+        {\n+            throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                            \"Mark #{} has {} rows, but max fixed granularity is {}, index granularity size {}\",\n+                            mark_num, index_granularity_rows, data_part->index_granularity_info.fixed_index_granularity, index_granularity.getMarksCount());\n+        }\n+\n         if (index_granularity_rows != index_granularity.getMarkRows(mark_num))\n             throw Exception(\n                 ErrorCodes::LOGICAL_ERROR, \"Incorrect mark rows for part {} for mark #{} (compressed offset {}, decompressed offset {}), in-memory {}, on disk {}, total marks {}\",\n@@ -607,6 +614,11 @@ void MergeTreeDataPartWriterWide::fillIndexGranularity(size_t index_granularity_\n \n void MergeTreeDataPartWriterWide::adjustLastMarkIfNeedAndFlushToDisk(size_t new_rows_in_last_mark)\n {\n+    /// We don't want to split already written granules to smaller\n+    if (rows_written_in_last_mark > new_rows_in_last_mark)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Tryin to make mark #{} smaller ({} rows) then it already has {}\",\n+                        getCurrentMark(), new_rows_in_last_mark, rows_written_in_last_mark);\n+\n     /// We can adjust marks only if we computed granularity for blocks.\n     /// Otherwise we cannot change granularity because it will differ from\n     /// other columns\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01654_test_writer_block_sequence.python b/tests/queries/0_stateless/01654_test_writer_block_sequence.python\nnew file mode 100644\nindex 000000000000..e80cc273076f\n--- /dev/null\n+++ b/tests/queries/0_stateless/01654_test_writer_block_sequence.python\n@@ -0,0 +1,58 @@\n+#!/usr/bin/env python3\n+import os\n+import sys\n+import random\n+import string\n+\n+CURDIR = os.path.dirname(os.path.realpath(__file__))\n+sys.path.insert(0, os.path.join(CURDIR, 'helpers'))\n+\n+from pure_http_client import ClickHouseClient\n+\n+def get_random_string(length):\n+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(length))\n+\n+client = ClickHouseClient()\n+\n+def insert_block(table_name, block_granularity_rows, block_rows):\n+    global client\n+    block_data = []\n+    index_granularity_bytes = 10 * 1024 * 1024\n+    row_bytes = index_granularity_bytes // block_granularity_rows\n+    for _ in range(block_rows):\n+        block_data.append(get_random_string(row_bytes - 1))\n+\n+    values_row = \", \".join(\"(1, '\" + row + \"')\" for row in block_data)\n+    client.query(\"INSERT INTO {} VALUES {}\".format(table_name, values_row))\n+\n+try:\n+    client.query(\"DROP TABLE IF EXISTS t\")\n+    client.query(\"CREATE TABLE t (v UInt8, data String) ENGINE = MergeTree() ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0\")\n+\n+    client.query(\"SYSTEM STOP MERGES t\")\n+\n+    # These blocks size taken from the real table which reproduces the error\n+    # when we get granule with more rows then fixed granularity after horizontal merge.\n+    # About 10k rows when max is 8912.\n+    #\n+    # Why these blocks are special?\n+    # 1) The first one contains 1811 rows, but its granule should have 6853.\n+    # So we write 1811 and get unfinished granule with 6853 - 1811 = 5042 rows to write from the next blocks.\n+    #\n+    # 2) The second block has fewer rows than rows left in the unfinished granule (3094 < 5042).\n+    # It can be written entirely in this unfinished granule and we will still have some rows left. But it's granularity\n+    # should be smaller than rows left in granule (3094 < 5042), so clickhouse will adjust (make smaller) this last unfinished granule.\n+    # This adjust logic contained a bug: we adjust not to the new block's granularity (3094), but to the difference of the new block granularity and\n+    # already written rows (3094 - 1811 = 1283). This lead to several unsigned integer overflows in code and huge granules as result.\n+    #\n+    # 3) Last block just triggers the check that each granule has fewer rows than fixed granularity rows. If the bug from 2) exists then it will fail.\n+    insert_block(\"t\", block_granularity_rows=6853, block_rows=1811)\n+    insert_block(\"t\", block_granularity_rows=3094, block_rows=3094)\n+    insert_block(\"t\", block_granularity_rows=6092, block_rows=6092)\n+\n+    client.query(\"SYSTEM START MERGES t\")\n+    client.query(\"OPTIMIZE TABLE t FINAL\")\n+\n+    print(client.query_return_df(\"SELECT COUNT() as C FROM t FORMAT TabSeparatedWithNames\")['C'][0])\n+finally:\n+    client.query(\"DROP TABLE IF EXISTS t\")\ndiff --git a/tests/queries/0_stateless/01654_test_writer_block_sequence.reference b/tests/queries/0_stateless/01654_test_writer_block_sequence.reference\nnew file mode 100644\nindex 000000000000..5aebdef792b5\n--- /dev/null\n+++ b/tests/queries/0_stateless/01654_test_writer_block_sequence.reference\n@@ -0,0 +1,1 @@\n+10997\ndiff --git a/tests/queries/0_stateless/01654_test_writer_block_sequence.sh b/tests/queries/0_stateless/01654_test_writer_block_sequence.sh\nnew file mode 100755\nindex 000000000000..3330148d91a2\n--- /dev/null\n+++ b/tests/queries/0_stateless/01654_test_writer_block_sequence.sh\n@@ -0,0 +1,7 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+python3 \"$CURDIR\"/01654_test_writer_block_sequence.python\n",
  "problem_statement": "Incorrect mark is found while running debug tests\n**Describe the bug**\r\nhttps://github.com/ClickHouse/ClickHouse/pull/18899#issuecomment-757571194\r\n\n",
  "hints_text": "```\r\n2021.01.10 17:54:10.588483 [ 229040 ] {} <Debug> system.text_log (29ec3865-0db2-4a55-834b-8fcd9c7fc97e) (MergerMutator): Merging 6 parts: from 202101_1_236_38 to 202101_1214_1276_12 into Wide\r\n2021.01.10 17:54:10.588922 [ 229040 ] {} <Debug> system.text_log (29ec3865-0db2-4a55-834b-8fcd9c7fc97e) (MergerMutator): Selected MergeAlgorithm: Vertical\r\n2021.01.10 17:54:10.589664 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 30 marks from part 202101_1_236_38, total 231097 rows starting from the beginning of the part\r\n2021.01.10 17:54:10.590701 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 60 marks from part 202101_237_630_29, total 486482 rows starting from the beginning of the part\r\n2021.01.10 17:54:10.591648 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 42 marks from part 202101_631_803_15, total 334259 rows starting from the beginning of the part\r\n2021.01.10 17:54:10.592655 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 28 marks from part 202101_804_1009_19, total 216171 rows starting from the beginning of the part\r\n2021.01.10 17:54:10.593577 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 16 marks from part 202101_1010_1213_25, total 111962 rows starting from the beginning of the part\r\n2021.01.10 17:54:10.594493 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 4 marks from part 202101_1214_1276_12, total 22793 rows starting from the beginning of the part\r\n2021.01.10 17:54:10.750617 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 30 marks from part 202101_1_236_38, total 231097 rows starting from the beginning of the part, column event_time_microseconds\r\n2021.01.10 17:54:10.751308 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 60 marks from part 202101_237_630_29, total 486482 rows starting from the beginning of the part, column event_time_microseconds\r\n2021.01.10 17:54:10.751918 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 42 marks from part 202101_631_803_15, total 334259 rows starting from the beginning of the part, column event_time_microseconds\r\n2021.01.10 17:54:10.752542 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 28 marks from part 202101_804_1009_19, total 216171 rows starting from the beginning of the part, column event_time_microseconds\r\n2021.01.10 17:54:10.753175 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 16 marks from part 202101_1010_1213_25, total 111962 rows starting from the beginning of the part, column event_time_microseconds\r\n2021.01.10 17:54:10.753799 [ 229040 ] {} <Trace> MergeTreeSequentialSource: Reading 4 marks from part 202101_1214_1276_12, total 22793 rows starting from the beginning of the part, column event_time_microseconds\r\n2021.01.10 17:54:11.277107 [ 229040 ] {} <Fatal> : Logical error: 'Incorrect mark rows for mark #174 (compressed offset 6655963, decompressed offset 0), actually in bin file 7610, in mrk file 8139'.\r\n```\r\nUnfortunately it's a merge of text log :( so it's not easy to reproduce. Will try to read the code...\nIn the failed example all source parts participating in the merge are wide and were merged with the horizontal merge.  It means that their marks were correct and passed the same check which failed for vertical merge.\r\n\r\nSo the error related to a single vertical merge and it's not accumulated from some previous merges. During the vertical merge, we don't compute index granularity. We use source blocks sizes as granules size. So we should never adjust granules, have partially written granules, and avoid other complex cases.  The only way how we can break this invariant -- when we get a block that is bigger than the fixed `index_granularity` for table https://github.com/ClickHouse/ClickHouse/blob/master/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp#L130. But I don't know how to achieve this, because merged blocks from merging algorithm must have average granularity from their source blocks. And their granularity must be always lower than the fixed `index_granularity`. \r\n\r\nI don't want to make a blind fix, so I've just added more checks to the writer which should better detect this case. Hopefully, we will catch it in CI again.\nGot it: https://github.com/ClickHouse/ClickHouse/pull/19011\r\n```\r\n2021.01.13 17:39:45.410267 [ 5781 ] {} <Fatal> : Logical error: 'Incomplete granules are not allowed while blocks are granules size. Mark number 9 (rows 8192), rows written in last mark 0, rows to write in last mark from block 4434 (from row 8192), total marks currently 10'.\r\n2021.01.13 17:39:45.411101 [ 38507 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.01.13 17:39:45.411249 [ 38507 ] {} <Fatal> BaseDaemon: (version 21.1.1.5665, build id: 5B2C6781B06DCEB60EFD88F724D1A954AF2632DE) (from thread 5781) (no query) Received signal Aborted (6)\r\n2021.01.13 17:39:45.411361 [ 38507 ] {} <Fatal> BaseDaemon:\r\n2021.01.13 17:39:45.411526 [ 38507 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f929722918b 0x7f9297208859 0x10734316 0x107343c1 0x1a7542b0 0x1a74d4e7 0x1a74ddbf 0x1a8893d9 0x1a889361 0x1a6e0195 0x1a3f6f41 0x1a40352f 0x1a40347d 0x1a40344d 0x1a403425 0x1a403400 0x10773c09 0x10772e45 0x1a62ce5a 0x1a62ce0d 0x1a62cddd 0x1a62cdb5 0x1a62cd90 0x10773c09 0x10772e45 0x107978cf 0x107a1214 0x107a11dd\r\n2021.01.13 17:39:45.411751 [ 38507 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.01.13 17:39:45.411895 [ 38507 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.01.13 17:39:45.412153 [ 38507 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:50: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)\r\n@ 0x10734316 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.412403 [ 38507 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:56: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x107343c1 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.464910 [ 38507 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Common/Exception.h:38: DB::Exception::Exception<unsigned long&, unsigned long, unsigned long&, unsigned long&, unsigned long&, unsigned long>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long&, unsigned long&&, unsigned long&, unsigned long&, unsigned long&, unsigned long&&) @ 0x1a7542b0 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.515754 [ 38507 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp:144: DB::MergeTreeDataPartWriterWide::shiftCurrentMark(std::__1::vector<DB::Granule, std::__1::allocator<DB::Granule> > const&) @ 0x1a74d4e7 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.567325 [ 38507 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp:238: DB::MergeTreeDataPartWriterWide::write(DB::Block const&, DB::PODArray<unsigned long, 4096ul, Allocator<false, false>, 15ul, 16ul> const*) @ 0x1a74ddbf in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.620697 [ 38507 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergedBlockOutputStream.cpp:189: DB::MergedBlockOutputStream::writeImpl(DB::Block const&, DB::PODArray<unsigned long, 4096ul,\r\nAllocator<false, false>, 15ul, 16ul> const*) @ 0x1a8893d9 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.673558 [ 38507 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergedBlockOutputStream.cpp:45: DB::MergedBlockOutputStream::write(DB::Block const&) @ 0x1a889361 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.725997 [ 38507 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp:941: DB::MergeTreeDataMergerMutator::mergePartsToTemporaryPart(DB::FutureMergedMutatedPart const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::BackgroundProcessListEntry<DB::MergeListElement, DB::MergeInfo>&, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, long, DB::Context const&, std::__1::unique_ptr<DB::IReservation, std::__1::default_delete<DB::IReservation> > const&, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0x1a6e0195 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.775888 [ 38507 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:793: DB::StorageMergeTree::mergeSelectedParts(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, DB::StorageMergeTree::MergeMutateSelectedEntry&, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&) @ 0x1a3f6f41 in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.825927 [ 38507 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:962: DB::StorageMergeTree::getDataProcessingJob()::$_4::operator()() @ 0x1a40352f in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.876727 [ 38507 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::StorageMergeTree::getDataProcessingJob()::$_4&>(fp)()) std::__1::__invoke<DB::StorageMergeTree::getDataProcessingJob()::$_4&>(DB::StorageMergeTree::getDataProcessingJob()::$_4&) @ 0x1a40347d in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.926640 [ 38507 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::StorageMergeTree::getDataProcessingJob()::$_4&>(DB::StorageMergeTree::getDataProcessingJob()::$_4&) @ 0x1a40344d in /usr/bin/clickhouse\r\n2021.01.13 17:39:45.976836 [ 38507 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::StorageMergeTree::getDataProcessingJob()::$_4, void ()>::operator()() @ 0x1a403425 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.026321 [ 38507 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::StorageMergeTree::getDataProcessingJob()::$_4, void ()> >(std::__1::__function::__policy_storage const*) @ 0x1a403400 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.026813 [ 38507 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x10773c09 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.027117 [ 38507 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x10772e45 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.079672 [ 38507 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/BackgroundJobsExecutor.cpp:101: DB::IBackgroundJobExecutor::jobExecutingTask()::$_0::operator()() const @ 0x1a62ce5a in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.130978 [ 38507 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::IBackgroundJobExecutor::jobExecutingTask()::$_0&>(fp)()) std::__1::__invoke<DB::IBackgroundJobExecutor::jobExecutingTask()::$_0&>(DB::IBackgroundJobExecutor::jobExecutingTask()::$_0&) @ 0x1a62ce0d in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.182159 [ 38507 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::IBackgroundJobExecutor::jobExecutingTask()::$_0&>(DB::IBackgroundJobExecutor::jobExecutingTask()::$_0&) @ 0x1a62cddd in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.234318 [ 38507 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::IBackgroundJobExecutor::jobExecutingTask()::$_0, void ()>::operator()() @ 0x1a62cdb5 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.284851 [ 38507 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::IBackgroundJobExecutor::jobExecutingTask()::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0x1a62cd90 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.285320 [ 38507 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x10773c09 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.285630 [ 38507 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x10772e45 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.286047 [ 38507 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:243: ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x107978cf in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.287005 [ 38507 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:124: void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const @ 0x107a1214 in /usr/bin/clickhouse\r\n2021.01.13 17:39:46.287983 [ 38507 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&>(fp0)...)) std::__1::__invoke_constexpr<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&...) @ 0x107a11dd in /usr/bin/clickhouse\r\n2021.01.13 17:39:47.411615 [ 38507 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: E6571A7BE27437B0088CCB4685C0CF26. There is no information about the reference checksum.\r\n2021.01.13 17:39:57.628690 [ 154 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n```",
  "created_at": "2021-01-15T12:42:40Z"
}