{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 36762,
  "instance_id": "ClickHouse__ClickHouse-36762",
  "issue_numbers": [
    "35392"
  ],
  "base_commit": "d8fa806fca1e2130ad43977db9b5bf16e8f94a2d",
  "patch": "diff --git a/src/Columns/ColumnObject.cpp b/src/Columns/ColumnObject.cpp\nindex 64c7a84c263f..c862aa0c3449 100644\n--- a/src/Columns/ColumnObject.cpp\n+++ b/src/Columns/ColumnObject.cpp\n@@ -334,7 +334,6 @@ void ColumnObject::Subcolumn::insert(Field field, FieldInfo info)\n void ColumnObject::Subcolumn::insertRangeFrom(const Subcolumn & src, size_t start, size_t length)\n {\n     assert(src.isFinalized());\n-\n     const auto & src_column = src.data.back();\n     const auto & src_type = src.least_common_type.get();\n \n@@ -646,9 +645,17 @@ void ColumnObject::get(size_t n, Field & res) const\n     }\n }\n \n+void ColumnObject::insertFrom(const IColumn & src, size_t n)\n+{\n+    insert(src[n]);\n+    finalize();\n+}\n+\n void ColumnObject::insertRangeFrom(const IColumn & src, size_t start, size_t length)\n {\n     const auto & src_object = assert_cast<const ColumnObject &>(src);\n+    if (!src_object.isFinalized())\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot insertRangeFrom non-finalized ColumnObject\");\n \n     for (auto & entry : subcolumns)\n     {\n@@ -658,6 +665,33 @@ void ColumnObject::insertRangeFrom(const IColumn & src, size_t start, size_t len\n             entry->data.insertManyDefaults(length);\n     }\n \n+    for (const auto & entry : src_object.subcolumns)\n+    {\n+        if (!hasSubcolumn(entry->path))\n+        {\n+            if (entry->path.hasNested())\n+            {\n+                const auto & base_type = entry->data.getLeastCommonTypeBase();\n+                FieldInfo field_info\n+                {\n+                    .scalar_type = base_type,\n+                    .have_nulls = base_type->isNullable(),\n+                    .need_convert = false,\n+                    .num_dimensions = entry->data.getNumberOfDimensions(),\n+                };\n+\n+                addNestedSubcolumn(entry->path, field_info, num_rows);\n+            }\n+            else\n+            {\n+                addSubcolumn(entry->path, num_rows);\n+            }\n+\n+            auto & subcolumn = getSubcolumn(entry->path);\n+            subcolumn.insertRangeFrom(entry->data, start, length);\n+        }\n+    }\n+\n     num_rows += length;\n     finalize();\n }\n@@ -685,6 +719,36 @@ void ColumnObject::popBack(size_t length)\n     num_rows -= length;\n }\n \n+template <typename Func>\n+ColumnPtr ColumnObject::applyForSubcolumns(Func && func, std::string_view func_name) const\n+{\n+    if (!isFinalized())\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot {} non-finalized ColumnObject\", func_name);\n+\n+    auto res = ColumnObject::create(is_nullable);\n+    for (const auto & subcolumn : subcolumns)\n+    {\n+        auto new_subcolumn = func(subcolumn->data.getFinalizedColumn());\n+        res->addSubcolumn(subcolumn->path, new_subcolumn->assumeMutable());\n+    }\n+    return res;\n+}\n+\n+ColumnPtr ColumnObject::permute(const Permutation & perm, size_t limit) const\n+{\n+    return applyForSubcolumns([&](const auto & subcolumn) { return subcolumn.permute(perm, limit); }, \"permute\");\n+}\n+\n+ColumnPtr ColumnObject::filter(const Filter & filter, ssize_t result_size_hint) const\n+{\n+    return applyForSubcolumns([&](const auto & subcolumn) { return subcolumn.filter(filter, result_size_hint); }, \"filter\");\n+}\n+\n+ColumnPtr ColumnObject::index(const IColumn & indexes, size_t limit) const\n+{\n+    return applyForSubcolumns([&](const auto & subcolumn) { return subcolumn.index(indexes, limit); }, \"index\");\n+}\n+\n const ColumnObject::Subcolumn & ColumnObject::getSubcolumn(const PathInData & key) const\n {\n     if (const auto * node = subcolumns.findLeaf(key))\ndiff --git a/src/Columns/ColumnObject.h b/src/Columns/ColumnObject.h\nindex 9d61b1650420..73fc738cb8d4 100644\n--- a/src/Columns/ColumnObject.h\n+++ b/src/Columns/ColumnObject.h\n@@ -68,6 +68,8 @@ class ColumnObject final : public COWHelper<IColumn, ColumnObject>\n \n         bool isFinalized() const;\n         const DataTypePtr & getLeastCommonType() const { return least_common_type.get(); }\n+        const DataTypePtr & getLeastCommonTypeBase() const { return least_common_type.getBase(); }\n+        size_t getNumberOfDimensions() const { return least_common_type.getNumberOfDimensions(); }\n \n         /// Checks the consistency of column's parts stored in @data.\n         void checkTypes() const;\n@@ -193,15 +195,18 @@ class ColumnObject final : public COWHelper<IColumn, ColumnObject>\n     void forEachSubcolumn(ColumnCallback callback) override;\n     void insert(const Field & field) override;\n     void insertDefault() override;\n+    void insertFrom(const IColumn & src, size_t n) override;\n     void insertRangeFrom(const IColumn & src, size_t start, size_t length) override;\n     ColumnPtr replicate(const Offsets & offsets) const override;\n     void popBack(size_t length) override;\n     Field operator[](size_t n) const override;\n     void get(size_t n, Field & res) const override;\n+    ColumnPtr permute(const Permutation & perm, size_t limit) const override;\n+    ColumnPtr filter(const Filter & filter, ssize_t result_size_hint) const override;\n+    ColumnPtr index(const IColumn & indexes, size_t limit) const override;\n \n     /// All other methods throw exception.\n \n-    ColumnPtr decompress() const override { throwMustBeConcrete(); }\n     StringRef getDataAt(size_t) const override { throwMustBeConcrete(); }\n     bool isDefaultAt(size_t) const override { throwMustBeConcrete(); }\n     void insertData(const char *, size_t) override { throwMustBeConcrete(); }\n@@ -211,10 +216,7 @@ class ColumnObject final : public COWHelper<IColumn, ColumnObject>\n     void updateHashWithValue(size_t, SipHash &) const override { throwMustBeConcrete(); }\n     void updateWeakHash32(WeakHash32 &) const override { throwMustBeConcrete(); }\n     void updateHashFast(SipHash &) const override { throwMustBeConcrete(); }\n-    ColumnPtr filter(const Filter &, ssize_t) const override { throwMustBeConcrete(); }\n     void expand(const Filter &, bool) override { throwMustBeConcrete(); }\n-    ColumnPtr permute(const Permutation &, size_t) const override { throwMustBeConcrete(); }\n-    ColumnPtr index(const IColumn &, size_t) const override { throwMustBeConcrete(); }\n     int compareAt(size_t, size_t, const IColumn &, int) const override { throwMustBeConcrete(); }\n     void compareColumn(const IColumn &, size_t, PaddedPODArray<UInt64> *, PaddedPODArray<Int8> &, int, int) const override { throwMustBeConcrete(); }\n     bool hasEqualValues() const override { throwMustBeConcrete(); }\n@@ -232,6 +234,9 @@ class ColumnObject final : public COWHelper<IColumn, ColumnObject>\n     {\n         throw Exception(\"ColumnObject must be converted to ColumnTuple before use\", ErrorCodes::LOGICAL_ERROR);\n     }\n+\n+    template <typename Func>\n+    ColumnPtr applyForSubcolumns(Func && func, std::string_view func_name) const;\n };\n \n }\ndiff --git a/src/DataTypes/ObjectUtils.cpp b/src/DataTypes/ObjectUtils.cpp\nindex 044e03afd104..0d8dea4315d0 100644\n--- a/src/DataTypes/ObjectUtils.cpp\n+++ b/src/DataTypes/ObjectUtils.cpp\n@@ -107,6 +107,9 @@ DataTypePtr getDataTypeByColumn(const IColumn & column)\n     if (WhichDataType(idx).isSimple())\n         return DataTypeFactory::instance().get(String(magic_enum::enum_name(idx)));\n \n+    if (WhichDataType(idx).isNothing())\n+        return std::make_shared<DataTypeNothing>();\n+\n     if (const auto * column_array = checkAndGetColumn<ColumnArray>(&column))\n         return std::make_shared<DataTypeArray>(getDataTypeByColumn(column_array->getData()));\n \ndiff --git a/src/Interpreters/InterpreterInsertQuery.cpp b/src/Interpreters/InterpreterInsertQuery.cpp\nindex b0aaac6e7455..8408b0ac5fcb 100644\n--- a/src/Interpreters/InterpreterInsertQuery.cpp\n+++ b/src/Interpreters/InterpreterInsertQuery.cpp\n@@ -438,7 +438,7 @@ BlockIO InterpreterInsertQuery::execute()\n         });\n \n         /// We need to convert Sparse columns to full, because it's destination storage\n-        /// may not support it may have different settings for applying Sparse serialization.\n+        /// may not support it or may have different settings for applying Sparse serialization.\n         pipeline.addSimpleTransform([&](const Block & in_header) -> ProcessorPtr\n         {\n             return std::make_shared<MaterializingTransform>(in_header);\ndiff --git a/src/Processors/Formats/IRowInputFormat.cpp b/src/Processors/Formats/IRowInputFormat.cpp\nindex f29f96bbb3bc..9896f95bb541 100644\n--- a/src/Processors/Formats/IRowInputFormat.cpp\n+++ b/src/Processors/Formats/IRowInputFormat.cpp\n@@ -213,7 +213,6 @@ Chunk IRowInputFormat::generate()\n \n     finalizeObjectColumns(columns);\n     Chunk chunk(std::move(columns), num_rows);\n-    //chunk.setChunkInfo(std::move(chunk_missing_values));\n     return chunk;\n }\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01825_type_json_ghdata.sh b/tests/queries/0_stateless/01825_type_json_ghdata.sh\nindex 7486571cc22d..bdb439f756f1 100755\n--- a/tests/queries/0_stateless/01825_type_json_ghdata.sh\n+++ b/tests/queries/0_stateless/01825_type_json_ghdata.sh\n@@ -6,7 +6,6 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n . \"$CUR_DIR\"/../shell_config.sh\n \n ${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS ghdata\"\n-\n ${CLICKHOUSE_CLIENT} -q \"CREATE TABLE ghdata (data JSON) ENGINE = MergeTree ORDER BY tuple()\" --allow_experimental_object_type 1\n \n cat $CUR_DIR/data_json/ghdata_sample.json | ${CLICKHOUSE_CLIENT} -q \"INSERT INTO ghdata FORMAT JSONAsObject\"\ndiff --git a/tests/queries/0_stateless/01825_type_json_ghdata_insert_select.reference b/tests/queries/0_stateless/01825_type_json_ghdata_insert_select.reference\nnew file mode 100644\nindex 000000000000..d00491fd7e5b\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_ghdata_insert_select.reference\n@@ -0,0 +1,1 @@\n+1\ndiff --git a/tests/queries/0_stateless/01825_type_json_ghdata_insert_select.sh b/tests/queries/0_stateless/01825_type_json_ghdata_insert_select.sh\nnew file mode 100755\nindex 000000000000..487c95137ae1\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_ghdata_insert_select.sh\n@@ -0,0 +1,27 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS ghdata_2\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS ghdata_2_string\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS ghdata_2_from_string\"\n+\n+${CLICKHOUSE_CLIENT} -q \"CREATE TABLE ghdata_2 (data JSON) ENGINE = MergeTree ORDER BY tuple()\" --allow_experimental_object_type 1\n+${CLICKHOUSE_CLIENT} -q \"CREATE TABLE ghdata_2_string (data String) ENGINE = MergeTree ORDER BY tuple()\"\n+${CLICKHOUSE_CLIENT} -q \"CREATE TABLE ghdata_2_from_string (data JSON) ENGINE = MergeTree ORDER BY tuple()\" --allow_experimental_object_type 1\n+\n+cat $CUR_DIR/data_json/ghdata_sample.json | ${CLICKHOUSE_CLIENT} -q \"INSERT INTO ghdata_2 FORMAT JSONAsObject\"\n+cat $CUR_DIR/data_json/ghdata_sample.json | ${CLICKHOUSE_CLIENT} -q \"INSERT INTO ghdata_2_string FORMAT JSONAsString\"\n+\n+${CLICKHOUSE_CLIENT} -q \"INSERT INTO ghdata_2_from_string SELECT data FROM ghdata_2_string\"\n+\n+${CLICKHOUSE_CLIENT} -q \"SELECT \\\n+    (SELECT toTypeName(any(data)), sum(cityHash64(flattenTuple(data))) FROM ghdata_2_from_string) = \\\n+    (SELECT toTypeName(any(data)), sum(cityHash64(flattenTuple(data))) FROM ghdata_2)\"\n+\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS ghdata_2\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS ghdata_2_string\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS ghdata_2_from_string\"\ndiff --git a/tests/queries/0_stateless/01825_type_json_insert_select.reference b/tests/queries/0_stateless/01825_type_json_insert_select.reference\nindex 8283cc5af484..6e50983f5eef 100644\n--- a/tests/queries/0_stateless/01825_type_json_insert_select.reference\n+++ b/tests/queries/0_stateless/01825_type_json_insert_select.reference\n@@ -10,3 +10,6 @@ Tuple(arr Nested(k11 Int8, k22 String, k33 Int8), k1 Int8, k2 String, k3 String)\n 3\t([],3,'','aaa')\n 4\t([(5,'6',0),(7,'0',8)],0,'','')\n 5\t([(0,'str1',0)],0,'','')\n+{\"data\":{\"k1\":1,\"k10\":[{\"a\":\"1\",\"b\":\"2\",\"c\":{\"k11\":\"\"}},{\"a\":\"2\",\"b\":\"3\",\"c\":{\"k11\":\"\"}}]}}\n+{\"data\":{\"k1\":2,\"k10\":[{\"a\":\"1\",\"b\":\"2\",\"c\":{\"k11\":\"haha\"}}]}}\n+Tuple(k1 Int8, k10 Nested(a String, b String, c Tuple(k11 String)))\ndiff --git a/tests/queries/0_stateless/01825_type_json_insert_select.sql b/tests/queries/0_stateless/01825_type_json_insert_select.sql\nindex 8bb03f84f5a1..b80fa08382dd 100644\n--- a/tests/queries/0_stateless/01825_type_json_insert_select.sql\n+++ b/tests/queries/0_stateless/01825_type_json_insert_select.sql\n@@ -34,3 +34,17 @@ SELECT id, data FROM type_json_dst ORDER BY id;\n \n DROP TABLE type_json_src;\n DROP TABLE type_json_dst;\n+\n+CREATE TABLE type_json_dst (data JSON) ENGINE = MergeTree ORDER BY tuple();\n+CREATE TABLE type_json_src (data String) ENGINE = MergeTree ORDER BY tuple();\n+\n+INSERT INTO type_json_src FORMAT JSONAsString {\"k1\": 1, \"k10\": [{\"a\": \"1\", \"b\": \"2\"}, {\"a\": \"2\", \"b\": \"3\"}]};\n+INSERT INTO type_json_src FORMAT JSONAsString  {\"k1\": 2, \"k10\": [{\"a\": \"1\", \"b\": \"2\", \"c\": {\"k11\": \"haha\"}}]};\n+INSERT INTO type_json_dst SELECT data FROM type_json_src;\n+\n+SET output_format_json_named_tuples_as_objects = 1;\n+SELECT * FROM type_json_dst ORDER BY data.k1 FORMAT JSONEachRow;\n+SELECT toTypeName(data) FROM type_json_dst LIMIT 1;\n+\n+DROP TABLE type_json_src;\n+DROP TABLE type_json_dst;\ndiff --git a/tests/queries/0_stateless/01825_type_json_multiple_files.reference b/tests/queries/0_stateless/01825_type_json_multiple_files.reference\nnew file mode 100644\nindex 000000000000..b887abc85900\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_multiple_files.reference\n@@ -0,0 +1,14 @@\n+{\"data\":{\"k0\":100,\"k1\":0,\"k2\":0,\"k3\":0,\"k4\":0,\"k5\":0}}\n+{\"data\":{\"k0\":0,\"k1\":100,\"k2\":0,\"k3\":0,\"k4\":0,\"k5\":0}}\n+{\"data\":{\"k0\":0,\"k1\":0,\"k2\":100,\"k3\":0,\"k4\":0,\"k5\":0}}\n+{\"data\":{\"k0\":0,\"k1\":0,\"k2\":0,\"k3\":100,\"k4\":0,\"k5\":0}}\n+{\"data\":{\"k0\":0,\"k1\":0,\"k2\":0,\"k3\":0,\"k4\":100,\"k5\":0}}\n+{\"data\":{\"k0\":0,\"k1\":0,\"k2\":0,\"k3\":0,\"k4\":0,\"k5\":100}}\n+Tuple(k0 Int8, k1 Int8, k2 Int8, k3 Int8, k4 Int8, k5 Int8)\n+{\"data\":{\"k0\":100,\"k1\":0,\"k2\":0}}\n+{\"data\":{\"k0\":0,\"k1\":100,\"k2\":0}}\n+{\"data\":{\"k0\":0,\"k1\":0,\"k2\":100}}\n+Tuple(k0 Int8, k1 Int8, k2 Int8)\n+{\"data\":{\"k1\":100,\"k3\":0}}\n+{\"data\":{\"k1\":0,\"k3\":100}}\n+Tuple(k1 Int8, k3 Int8)\ndiff --git a/tests/queries/0_stateless/01825_type_json_multiple_files.sh b/tests/queries/0_stateless/01825_type_json_multiple_files.sh\nnew file mode 100755\nindex 000000000000..9120568bb171\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_multiple_files.sh\n@@ -0,0 +1,44 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest, no-parallel\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+user_files_path=$($CLICKHOUSE_CLIENT --query \"select _path,_file from file('nonexist.txt', 'CSV', 'val1 char')\" 2>&1 | grep -E '^Code: 107.*FILE_DOESNT_EXIST' | head -1 | awk '{gsub(\"/nonexist.txt\",\"\",$9); print $9}')\n+for f in \"$user_files_path\"/01825_file_*.json; do\n+    [ -e $f ] && rm $f\n+done\n+\n+for i in {0..5}; do\n+    echo \"{\\\"k$i\\\": 100}\" > \"$user_files_path\"/01825_file_$i.json\n+done\n+\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS t_json_files\"\n+${CLICKHOUSE_CLIENT} -q \"CREATE TABLE t_json_files (file String, data JSON) ENGINE = MergeTree ORDER BY tuple()\" --allow_experimental_object_type 1\n+\n+${CLICKHOUSE_CLIENT} -q \"INSERT INTO t_json_files SELECT _file, data FROM file('01825_file_*.json', 'JSONAsObject', 'data JSON')\"\n+\n+${CLICKHOUSE_CLIENT} -q \"SELECT data FROM t_json_files ORDER BY file FORMAT JSONEachRow\" --output_format_json_named_tuples_as_objects 1\n+${CLICKHOUSE_CLIENT} -q \"SELECT toTypeName(data) FROM t_json_files LIMIT 1\"\n+\n+${CLICKHOUSE_CLIENT} -q \"TRUNCATE TABLE IF EXISTS t_json_files\"\n+\n+${CLICKHOUSE_CLIENT} -q \"INSERT INTO t_json_files \\\n+    SELECT _file, data FROM file('01825_file_*.json', 'JSONAsObject', 'data JSON') \\\n+    ORDER BY _file LIMIT 3\" --max_threads 1 --min_insert_block_size_rows 1 --max_insert_block_size 1 --max_block_size 1\n+\n+${CLICKHOUSE_CLIENT} -q \"SELECT data FROM t_json_files ORDER BY file FORMAT JSONEachRow\" --output_format_json_named_tuples_as_objects 1\n+${CLICKHOUSE_CLIENT} -q \"SELECT toTypeName(data) FROM t_json_files LIMIT 1\"\n+\n+${CLICKHOUSE_CLIENT} -q \"TRUNCATE TABLE IF EXISTS t_json_files\"\n+\n+${CLICKHOUSE_CLIENT} -q \"INSERT INTO t_json_files \\\n+    SELECT _file, data FROM file('01825_file_*.json', 'JSONAsObject', 'data JSON') \\\n+    WHERE _file IN ('01825_file_1.json', '01825_file_3.json')\"\n+\n+${CLICKHOUSE_CLIENT} -q \"SELECT data FROM t_json_files ORDER BY file FORMAT JSONEachRow\" --output_format_json_named_tuples_as_objects 1\n+${CLICKHOUSE_CLIENT} -q \"SELECT toTypeName(data) FROM t_json_files LIMIT 1\"\n+\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS t_json_files\"\n+rm \"$user_files_path\"/01825_file_*.json\ndiff --git a/tests/queries/0_stateless/01825_type_json_nbagames.reference b/tests/queries/0_stateless/01825_type_json_nbagames.reference\nindex 8f86bfe613e4..863f5a0db841 100644\n--- a/tests/queries/0_stateless/01825_type_json_nbagames.reference\n+++ b/tests/queries/0_stateless/01825_type_json_nbagames.reference\n@@ -10,3 +10,4 @@ Clyde Drexler\t4\n Alvin Robertson\t3\n Magic Johnson\t3\n Charles Barkley\t2\n+1\ndiff --git a/tests/queries/0_stateless/01825_type_json_nbagames.sh b/tests/queries/0_stateless/01825_type_json_nbagames.sh\nindex 18e7c0506801..e13d004ac58a 100755\n--- a/tests/queries/0_stateless/01825_type_json_nbagames.sh\n+++ b/tests/queries/0_stateless/01825_type_json_nbagames.sh\n@@ -6,6 +6,8 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n . \"$CUR_DIR\"/../shell_config.sh\n \n ${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS nbagames\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS nbagames_string\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS nbagames_from_string\"\n \n ${CLICKHOUSE_CLIENT} -q \"CREATE TABLE nbagames (data JSON) ENGINE = MergeTree ORDER BY tuple()\" --allow_experimental_object_type 1\n \n@@ -36,5 +38,16 @@ ${CLICKHOUSE_CLIENT} -q \\\n ) \\\n GROUP BY player ORDER BY triple_doubles DESC, player LIMIT 5\"\n \n+${CLICKHOUSE_CLIENT} -q \"CREATE TABLE nbagames_string (data String) ENGINE = MergeTree ORDER BY tuple()\"\n+${CLICKHOUSE_CLIENT} -q \"CREATE TABLE nbagames_from_string (data JSON) ENGINE = MergeTree ORDER BY tuple()\" --allow_experimental_object_type 1\n+\n+cat $CUR_DIR/data_json/nbagames_sample.json | ${CLICKHOUSE_CLIENT} -q \"INSERT INTO nbagames_string FORMAT JSONAsString\"\n+${CLICKHOUSE_CLIENT} -q \"INSERT INTO nbagames_from_string SELECT data FROM nbagames_string\"\n+\n+${CLICKHOUSE_CLIENT} -q \"SELECT \\\n+    (SELECT toTypeName(any(data)), sum(cityHash64(flattenTuple(data))) FROM nbagames_from_string) = \\\n+    (SELECT toTypeName(any(data)), sum(cityHash64(flattenTuple(data))) FROM nbagames)\"\n \n ${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS nbagames\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS nbagames_string\"\n+${CLICKHOUSE_CLIENT} -q \"DROP TABLE IF EXISTS nbagames_from_string\"\n",
  "problem_statement": "Missing subcolumns when insert JSON type with objects contains Array type\n**How to reproduce**\r\n\r\nCreate tables with Object type\r\n```  sql\r\nCREATE TABLE default.t_json_5\r\n(\r\n    `data` Object('json')\r\n)\r\nENGINE = MergeTree\r\nORDER BY tuple()\r\nSETTINGS min_bytes_for_wide_part = 0, min_rows_for_wide_part = 0, index_granularity = 8192 \r\n\r\nCREATE TABLE default.t_json_str_5\r\n(\r\n    `data` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY tuple()\r\nSETTINGS min_bytes_for_wide_part = 0, min_rows_for_wide_part = 0, index_granularity = 8192 \r\n```\r\n\r\nInsert data\r\n``` sql \r\nINSERT INTO t_json_str_5 FORMAT JSONAsString {\"k1\": 1, \"k10\": [{\"a\": \"1\", \"b\": \"2\"}, {\"a\": \"2\", \"b\": \"3\"}]} \r\nINSERT INTO t_json_str_5 FORMAT JSONAsString  {\"k1\": 1, \"k10\": [{\"a\": \"1\", \"b\": \"2\", \"c\": {\"k11\": \"haha\"}}]}\r\ninsert into t_json_5 select data from t_json_str_5; \r\n``` \r\n\r\nQuery data \r\n\r\n``` sql\r\nsg-ch-test006.bigdata.bigo.inner :) select * from t_json_str_5; \r\n\r\nSELECT *\r\nFROM t_json_str_5\r\n\r\nQuery id: 6a8c9b71-45df-43b3-85f6-3cfb06ede9ee\r\n\r\n\u250c\u2500data\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 {\"k1\": 1, \"k10\": [{\"a\": \"1\", \"b\": \"2\"}, {\"a\": \"2\", \"b\": \"3\"}]} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500data\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 {\"k1\": 1, \"k10\": [{\"a\": \"1\", \"b\": \"2\", \"c\": {\"k11\": \"haha\"}}]} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n2 rows in set. Elapsed: 0.002 sec. \r\n```\r\n\r\nWe can see query result of  table `t_json_5 ` is not as expected because subcolumn `data.k10.c` is missing. \r\n``` sql \r\nsg-ch-test006.bigdata.bigo.inner :) select * from t_json_5 FORMAT TSVWithNamesAndTypes;\r\n\r\nSELECT *\r\nFROM t_json_5\r\nFORMAT TSVWithNamesAndTypes\r\n\r\nQuery id: 11be876a-5a7a-4f00-aef7-34a3b49be3ad\r\n\r\ndata\r\nTuple(k1 Int8, k10 Nested(a String, b String))\r\n(1,[('1','2'),('2','3')])\r\n(1,[('1','2')])\r\n```\r\n\r\n* Which ClickHouse server version to use\r\nMaster version \r\n\r\n\r\n\r\n\n",
  "hints_text": "I guess the cause is that `k10` is `Nested` type, and it doesn't support adding column dynamicly. \r\nOnce we insert first row without key `data.k10.c`,  the type of k10 is initialized as `Nested(a String, b String)`. \r\nWhen we insert second row,  the key `data.10.c` is skipped while parsing ?  \nLooks like it'a bug in converting from `String` to `Object`. If load data directly your example works:\r\n\r\n```sql\r\nINSERT INTO t_json_5 FORMAT JSONAsObject {\"k1\": 1, \"k10\": [{\"a\": \"1\", \"b\": \"2\"}, {\"a\": \"2\", \"b\": \"3\"}]}\r\n\r\nSELECT * FROM t_json_5 FORMAT JSONEachRow\r\n\r\n{\"data\":{\"k1\":1,\"k10\":[{\"a\":\"1\",\"b\":\"2\"},{\"a\":\"2\",\"b\":\"3\"}]}}\r\n\r\nINSERT INTO t_json_5 FORMAT JSONAsObject  {\"k1\": 1, \"k10\": [{\"a\": \"1\", \"b\": \"2\", \"c\": {\"k11\": \"haha\"}}]}\r\n                                            \r\nSELECT * FROM t_json_5 FORMAT JSONEachRow\r\n\r\n{\"data\":{\"k1\":1,\"k10\":[{\"a\":\"1\",\"b\":\"2\",\"c\":{\"k11\":\"haha\"}}]}}\r\n{\"data\":{\"k1\":1,\"k10\":[{\"a\":\"1\",\"b\":\"2\",\"c\":{\"k11\":\"\"}},{\"a\":\"2\",\"b\":\"3\",\"c\":{\"k11\":\"\"}}]}}\r\n\r\n```\n@CurtizJ I see.  May I fix it ? \nYes, you can",
  "created_at": "2022-04-28T18:53:34Z"
}