{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 44954,
  "instance_id": "ClickHouse__ClickHouse-44954",
  "issue_numbers": [
    "44902"
  ],
  "base_commit": "4b28b39f7c6e25291db99868b68b4dbc054c2861",
  "patch": "diff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 2dcc0a560fbc..5830e0145bc3 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -3780,7 +3780,7 @@ std::pair<size_t, size_t> MergeTreeData::getMaxPartsCountAndSizeForPartition() c\n }\n \n \n-size_t MergeTreeData::getMaxInactivePartsCountForPartition() const\n+size_t MergeTreeData::getMaxOutdatedPartsCountForPartition() const\n {\n     return getMaxPartsCountAndSizeForPartitionWithState(DataPartState::Outdated).first;\n }\n@@ -3801,70 +3801,102 @@ std::optional<Int64> MergeTreeData::getMinPartDataVersion() const\n }\n \n \n-void MergeTreeData::delayInsertOrThrowIfNeeded(Poco::Event * until, ContextPtr query_context) const\n+void MergeTreeData::delayInsertOrThrowIfNeeded(Poco::Event * until, const ContextPtr & query_context) const\n {\n     const auto settings = getSettings();\n     const auto & query_settings = query_context->getSettingsRef();\n     const size_t parts_count_in_total = getPartsCount();\n+\n+    /// check if have too many parts in total\n     if (parts_count_in_total >= settings->max_parts_in_total)\n     {\n         ProfileEvents::increment(ProfileEvents::RejectedInserts);\n-        throw Exception(\"Too many parts (\" + toString(parts_count_in_total) + \") in all partitions in total. This indicates wrong choice of partition key. The threshold can be modified with 'max_parts_in_total' setting in <merge_tree> element in config.xml or with per-table setting.\", ErrorCodes::TOO_MANY_PARTS);\n+        throw Exception(\n+            ErrorCodes::TOO_MANY_PARTS,\n+            \"Too many parts ({}) in all partitions in total. This indicates wrong choice of partition key. The threshold can be modified \"\n+            \"with 'max_parts_in_total' setting in <merge_tree> element in config.xml or with per-table setting.\",\n+            toString(parts_count_in_total));\n     }\n \n-    auto [parts_count_in_partition, size_of_partition] = getMaxPartsCountAndSizeForPartition();\n-    ssize_t k_inactive = -1;\n-    if (settings->inactive_parts_to_throw_insert > 0 || settings->inactive_parts_to_delay_insert > 0)\n+    size_t outdated_parts_over_threshold = 0;\n     {\n-        size_t inactive_parts_count_in_partition = getMaxInactivePartsCountForPartition();\n-        if (settings->inactive_parts_to_throw_insert > 0 && inactive_parts_count_in_partition >= settings->inactive_parts_to_throw_insert)\n+        size_t outdated_parts_count_in_partition = 0;\n+        if (settings->inactive_parts_to_throw_insert > 0 || settings->inactive_parts_to_delay_insert > 0)\n+            outdated_parts_count_in_partition = getMaxOutdatedPartsCountForPartition();\n+\n+        if (settings->inactive_parts_to_throw_insert > 0 && outdated_parts_count_in_partition >= settings->inactive_parts_to_throw_insert)\n         {\n             ProfileEvents::increment(ProfileEvents::RejectedInserts);\n             throw Exception(\n                 ErrorCodes::TOO_MANY_PARTS,\n                 \"Too many inactive parts ({}). Parts cleaning are processing significantly slower than inserts\",\n-                inactive_parts_count_in_partition);\n+                outdated_parts_count_in_partition);\n         }\n-        k_inactive = static_cast<ssize_t>(inactive_parts_count_in_partition) - static_cast<ssize_t>(settings->inactive_parts_to_delay_insert);\n+        if (settings->inactive_parts_to_delay_insert > 0 && outdated_parts_count_in_partition >= settings->inactive_parts_to_delay_insert)\n+            outdated_parts_over_threshold = outdated_parts_count_in_partition - settings->inactive_parts_to_delay_insert + 1;\n     }\n \n-    auto parts_to_delay_insert = query_settings.parts_to_delay_insert ? query_settings.parts_to_delay_insert : settings->parts_to_delay_insert;\n-    auto parts_to_throw_insert = query_settings.parts_to_throw_insert ? query_settings.parts_to_throw_insert : settings->parts_to_throw_insert;\n-\n+    auto [parts_count_in_partition, size_of_partition] = getMaxPartsCountAndSizeForPartition();\n     size_t average_part_size = parts_count_in_partition ? size_of_partition / parts_count_in_partition : 0;\n-    bool parts_are_large_enough_in_average = settings->max_avg_part_size_for_too_many_parts\n-        && average_part_size > settings->max_avg_part_size_for_too_many_parts;\n-\n-    if (parts_count_in_partition >= parts_to_throw_insert && !parts_are_large_enough_in_average)\n+    const auto active_parts_to_delay_insert\n+        = query_settings.parts_to_delay_insert ? query_settings.parts_to_delay_insert : settings->parts_to_delay_insert;\n+    const auto active_parts_to_throw_insert\n+        = query_settings.parts_to_throw_insert ? query_settings.parts_to_throw_insert : settings->parts_to_throw_insert;\n+    size_t active_parts_over_threshold = 0;\n     {\n-        ProfileEvents::increment(ProfileEvents::RejectedInserts);\n-        throw Exception(\n-            ErrorCodes::TOO_MANY_PARTS,\n-            \"Too many parts ({} with average size of {}). Merges are processing significantly slower than inserts\",\n-            parts_count_in_partition, ReadableSize(average_part_size));\n+        bool parts_are_large_enough_in_average\n+            = settings->max_avg_part_size_for_too_many_parts && average_part_size > settings->max_avg_part_size_for_too_many_parts;\n+\n+        if (parts_count_in_partition >= active_parts_to_throw_insert && !parts_are_large_enough_in_average)\n+        {\n+            ProfileEvents::increment(ProfileEvents::RejectedInserts);\n+            throw Exception(\n+                ErrorCodes::TOO_MANY_PARTS,\n+                \"Too many parts ({} with average size of {}). Merges are processing significantly slower than inserts\",\n+                parts_count_in_partition,\n+                ReadableSize(average_part_size));\n+        }\n+        if (active_parts_to_delay_insert > 0 && parts_count_in_partition >= active_parts_to_delay_insert\n+            && !parts_are_large_enough_in_average)\n+            /// if parts_count == parts_to_delay_insert -> we're 1 part over threshold\n+            active_parts_over_threshold = parts_count_in_partition - active_parts_to_delay_insert + 1;\n     }\n \n-    if (k_inactive < 0 && (parts_count_in_partition < parts_to_delay_insert || parts_are_large_enough_in_average))\n+    /// no need for delay\n+    if (!active_parts_over_threshold && !outdated_parts_over_threshold)\n         return;\n \n-    const ssize_t k_active = ssize_t(parts_count_in_partition) - ssize_t(parts_to_delay_insert);\n-    size_t max_k;\n-    size_t k;\n-    if (k_active > k_inactive)\n-    {\n-        max_k = parts_to_throw_insert - parts_to_delay_insert;\n-        k = k_active + 1;\n-    }\n-    else\n+    UInt64 delay_milliseconds = 0;\n     {\n-        max_k = settings->inactive_parts_to_throw_insert - settings->inactive_parts_to_delay_insert;\n-        k = k_inactive + 1;\n-    }\n+        size_t parts_over_threshold = 0;\n+        size_t allowed_parts_over_threshold = 1;\n+        const bool use_active_parts_threshold = (active_parts_over_threshold >= outdated_parts_over_threshold);\n+        if (use_active_parts_threshold)\n+        {\n+            parts_over_threshold = active_parts_over_threshold;\n+            allowed_parts_over_threshold = active_parts_to_throw_insert - active_parts_to_delay_insert;\n+        }\n+        else\n+        {\n+            parts_over_threshold = outdated_parts_over_threshold;\n+            allowed_parts_over_threshold = outdated_parts_over_threshold; /// if throw threshold is not set, will use max delay\n+            if (settings->inactive_parts_to_throw_insert > 0)\n+                allowed_parts_over_threshold = settings->inactive_parts_to_throw_insert - settings->inactive_parts_to_delay_insert;\n+        }\n \n-    const UInt64 max_delay_milliseconds = (settings->max_delay_to_insert > 0 ? settings->max_delay_to_insert * 1000 : 1000);\n-    /// min() as a save guard here\n-    const UInt64 delay_milliseconds\n-        = std::min(max_delay_milliseconds, static_cast<UInt64>(::pow(max_delay_milliseconds, static_cast<double>(k) / max_k)));\n+        if (allowed_parts_over_threshold == 0 || parts_over_threshold > allowed_parts_over_threshold) [[unlikely]]\n+            throw Exception(\n+                ErrorCodes::LOGICAL_ERROR,\n+                \"Incorrect calculation of {} parts over threshold: allowed_parts_over_threshold={}, parts_over_threshold={}\",\n+                (use_active_parts_threshold ? \"active\" : \"inactive\"),\n+                allowed_parts_over_threshold,\n+                parts_over_threshold);\n+\n+        const UInt64 max_delay_milliseconds = (settings->max_delay_to_insert > 0 ? settings->max_delay_to_insert * 1000 : 1000);\n+        double delay_factor = static_cast<double>(parts_over_threshold) / allowed_parts_over_threshold;\n+        const UInt64 min_delay_milliseconds = settings->min_delay_to_insert_ms;\n+        delay_milliseconds = std::max(min_delay_milliseconds, static_cast<UInt64>(max_delay_milliseconds * delay_factor));\n+    }\n \n     ProfileEvents::increment(ProfileEvents::DelayedInserts);\n     ProfileEvents::increment(ProfileEvents::DelayedInsertsMilliseconds, delay_milliseconds);\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 670c755cf721..f846ba5e1846 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -533,7 +533,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     std::pair<size_t, size_t> getMaxPartsCountAndSizeForPartitionWithState(DataPartState state) const;\n     std::pair<size_t, size_t> getMaxPartsCountAndSizeForPartition() const;\n \n-    size_t getMaxInactivePartsCountForPartition() const;\n+    size_t getMaxOutdatedPartsCountForPartition() const;\n \n     /// Get min value of part->info.getDataVersion() for all active parts.\n     /// Makes sense only for ordinary MergeTree engines because for them block numbering doesn't depend on partition.\n@@ -553,7 +553,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n \n     /// If the table contains too many active parts, sleep for a while to give them time to merge.\n     /// If until is non-null, wake up from the sleep earlier if the event happened.\n-    void delayInsertOrThrowIfNeeded(Poco::Event * until, ContextPtr query_context) const;\n+    void delayInsertOrThrowIfNeeded(Poco::Event * until, const ContextPtr & query_context) const;\n \n     /// Renames temporary part to a permanent part and adds it to the parts set.\n     /// It is assumed that the part does not intersect with existing parts.\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 37e9bf5779c7..d1f957740e27 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -68,12 +68,13 @@ struct Settings;\n     M(Bool, remove_rolled_back_parts_immediately, 1, \"Setting for an incomplete experimental feature.\", 0) \\\n     \\\n     /** Inserts settings. */ \\\n-    M(UInt64, parts_to_delay_insert, 150, \"If table contains at least that many active parts in single partition, artificially slow down insert into table.\", 0) \\\n+    M(UInt64, parts_to_delay_insert, 150, \"If table contains at least that many active parts in single partition, artificially slow down insert into table. Disabled if set to 0\", 0) \\\n     M(UInt64, inactive_parts_to_delay_insert, 0, \"If table contains at least that many inactive parts in single partition, artificially slow down insert into table.\", 0) \\\n     M(UInt64, parts_to_throw_insert, 300, \"If more than this number active parts in single partition, throw 'Too many parts ...' exception.\", 0) \\\n     M(UInt64, inactive_parts_to_throw_insert, 0, \"If more than this number inactive parts in single partition, throw 'Too many inactive parts ...' exception.\", 0) \\\n     M(UInt64, max_avg_part_size_for_too_many_parts, 10ULL * 1024 * 1024 * 1024, \"The 'too many parts' check according to 'parts_to_delay_insert' and 'parts_to_throw_insert' will be active only if the average part size (in the relevant partition) is not larger than the specified threshold. If it is larger than the specified threshold, the INSERTs will be neither delayed or rejected. This allows to have hundreds of terabytes in a single table on a single server if the parts are successfully merged to larger parts. This does not affect the thresholds on inactive parts or total parts.\", 0) \\\n     M(UInt64, max_delay_to_insert, 1, \"Max delay of inserting data into MergeTree table in seconds, if there are a lot of unmerged parts in single partition.\", 0) \\\n+    M(UInt64, min_delay_to_insert_ms, 10, \"Min delay of inserting data into MergeTree table in milliseconds, if there are a lot of unmerged parts in single partition.\", 0) \\\n     M(UInt64, max_parts_in_total, 100000, \"If more than this number active parts in all partitions in total, throw 'Too many parts ...' exception.\", 0) \\\n     \\\n     /* Part removal settings. */ \\\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02521_incorrect_dealy_for_insert_bug_44902.reference b/tests/queries/0_stateless/02521_incorrect_dealy_for_insert_bug_44902.reference\nnew file mode 100644\nindex 000000000000..c104ff58affd\n--- /dev/null\n+++ b/tests/queries/0_stateless/02521_incorrect_dealy_for_insert_bug_44902.reference\n@@ -0,0 +1,6 @@\n+0\n+300\n+500\n+750\n+1000\n+TOO_MANY_PARTS\ndiff --git a/tests/queries/0_stateless/02521_incorrect_dealy_for_insert_bug_44902.sh b/tests/queries/0_stateless/02521_incorrect_dealy_for_insert_bug_44902.sh\nnew file mode 100755\nindex 000000000000..5f91ef19a5ab\n--- /dev/null\n+++ b/tests/queries/0_stateless/02521_incorrect_dealy_for_insert_bug_44902.sh\n@@ -0,0 +1,24 @@\n+#!/usr/bin/env bash\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS test_02521_insert_delay\"\n+# Create MergeTree with settings which allow to insert maximum 5 parts, on 6th it'll throw TOO_MANY_PARTS\n+$CLICKHOUSE_CLIENT -q \"CREATE TABLE test_02521_insert_delay (key UInt32, value String) Engine=MergeTree() ORDER BY tuple() SETTINGS parts_to_delay_insert=1, parts_to_throw_insert=5, max_delay_to_insert=1, min_delay_to_insert_ms=300\"\n+$CLICKHOUSE_CLIENT -q \"SYSTEM STOP MERGES test_02521_insert_delay\"\n+\n+# Every delay is increased by max_delay_to_insert*1000/(parts_to_throw_insert - parts_to_delay_insert + 1), here it's 250ms\n+# 0-indexed INSERT - no delay, 1-indexed INSERT - 300ms instead of 250ms due to min_delay_to_insert_ms\n+for i in {0..4}\n+do\n+    query_id=\"${CLICKHOUSE_DATABASE}_02521_${i}_$RANDOM$RANDOM\"\n+    $CLICKHOUSE_CLIENT --query_id=\"$query_id\" -q \"INSERT INTO test_02521_insert_delay SELECT number, toString(number) FROM numbers(${i}, 1)\"\n+    $CLICKHOUSE_CLIENT -q \"SYSTEM FLUSH LOGS\"\n+    $CLICKHOUSE_CLIENT --param_query_id=\"$query_id\" -q \"select ProfileEvents['DelayedInsertsMilliseconds'] as delay from system.query_log where event_date >= yesterday() and query_id = {query_id:String} order by delay desc limit 1\"\n+done\n+\n+$CLICKHOUSE_CLIENT -q \"INSERT INTO test_02521_insert_delay VALUES(0, 'This query throws error')\" 2>&1 | grep -o 'TOO_MANY_PARTS' | head -n 1\n+\n+$CLICKHOUSE_CLIENT -q \"DROP TABLE test_02521_insert_delay\"\n",
  "problem_statement": "Fix formula for insert delay time calculation\nWe have following function which allows to delay inserts in case of TOO_MANY_PARTS:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/c09f83654dae58fd964e024f7842de64f59ab579/src/Storages/MergeTree/MergeTreeData.cpp#L3724-L3798.\r\n\r\nDelay time formula looks really strange and can lead to enormous value of sleep time, like:\r\n```\r\nDelaying inserting block by 9223372036854775808 ms. because there are 199 parts and their average size is 1.85 GiB\r\n```\r\nThis can lead to unexpected errors from `tryWait` function like:\r\n```\r\n0. Poco::EventImpl::waitImpl(long) @ 0x1730d6e6 in /usr/bin/clickhouse\r\n1. DB::MergeTreeData::delayInsertOrThrowIfNeeded(Poco::Event*, std::__1::shared_ptr<DB::Context const>) const @ 0x13e9ca32 in /usr/bin/clickhouse\r\n2. ? @ 0x145d9379 in /usr/bin/clickhouse\r\n3. DB::ExceptionKeepingTransform::work() @ 0x145d8b54 in /usr/bin/clickhouse\r\n4. DB::ExecutionThreadContext::executeTask() @ 0x14409026 in /usr/bin/clickhouse\r\n5. DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x143fdadc in /usr/bin/clickhouse\r\n6. DB::PipelineExecutor::executeImpl(unsigned long) @ 0x143fc519 in /usr/bin/clickhouse\r\n7. DB::PipelineExecutor::execute(unsigned long) @ 0x143fc31d in /usr/bin/clickhouse\r\n8. DB::CompletedPipelineExecutor::execute() @ 0x143fad3d in /usr/bin/clickhouse\r\n9. DB::AsynchronousInsertQueue::processData(DB::AsynchronousInsertQueue::InsertQuery, std::__1::unique_ptr<DB::AsynchronousInsertQueue::InsertData, std::__1::default_delete<DB::AsynchronousInsertQueue::InsertData>>, std::__1::shared_ptr<DB::Context const>) @ 0x1293f3f4 in /usr/bin/clickhouse\r\n10. ? @ 0x12941ed0 in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__1::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0xda87576 in /usr/bin/clickhouse\r\n12. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__1::__function::__policy_storage const*) @ 0xda89d37 in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xda83b96 in /usr/bin/clickhouse\r\n14. ? @ 0xda88d01 in /usr/bin/clickhouse\r\n15. ? @ 0x7f636c46c609 in ?\r\n16. clone @ 0x7f636c391133 in ?\r\n```\r\n\r\nNeed to rethink the logic of delay time calculation and introduce better formula or some other thresholds.\n",
  "hints_text": "",
  "created_at": "2023-01-05T23:02:46Z"
}