{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 8969,
  "instance_id": "ClickHouse__ClickHouse-8969",
  "issue_numbers": [
    "7198"
  ],
  "base_commit": "9feb848e843a339b394794f5db4771bf7649a73b",
  "patch": "diff --git a/dbms/src/Formats/FormatFactory.h b/dbms/src/Formats/FormatFactory.h\nindex 345ceaee6901..7c18971e0eb0 100644\n--- a/dbms/src/Formats/FormatFactory.h\n+++ b/dbms/src/Formats/FormatFactory.h\n@@ -1,6 +1,7 @@\n #pragma once\n \n #include <Core/Types.h>\n+#include <Columns/IColumn.h>\n #include <DataStreams/IBlockStream_fwd.h>\n #include <IO/BufferWithOwnMemory.h>\n \n@@ -9,7 +10,6 @@\n #include <unordered_map>\n #include <boost/noncopyable.hpp>\n \n-\n namespace DB\n {\n \n@@ -53,7 +53,9 @@ class FormatFactory final : private boost::noncopyable\n \n     /// This callback allows to perform some additional actions after writing a single row.\n     /// It's initial purpose was to flush Kafka message for each row.\n-    using WriteCallback = std::function<void()>;\n+    using WriteCallback = std::function<void(\n+        const Columns & columns,\n+        size_t row)>;\n \n private:\n     using InputCreator = std::function<BlockInputStreamPtr(\ndiff --git a/dbms/src/Processors/Formats/IRowOutputFormat.cpp b/dbms/src/Processors/Formats/IRowOutputFormat.cpp\nindex 9d40f056cd32..30455db32842 100644\n--- a/dbms/src/Processors/Formats/IRowOutputFormat.cpp\n+++ b/dbms/src/Processors/Formats/IRowOutputFormat.cpp\n@@ -13,7 +13,7 @@ void IRowOutputFormat::consume(DB::Chunk chunk)\n     auto num_rows = chunk.getNumRows();\n     auto & columns = chunk.getColumns();\n \n-    for (UInt64 row = 0; row < num_rows; ++row)\n+    for (size_t row = 0; row < num_rows; ++row)\n     {\n         if (!first_row)\n             writeRowBetweenDelimiter();\n@@ -22,7 +22,7 @@ void IRowOutputFormat::consume(DB::Chunk chunk)\n         write(columns, row);\n \n         if (write_single_row_callback)\n-            write_single_row_callback();\n+            write_single_row_callback(columns, row);\n     }\n }\n \ndiff --git a/dbms/src/Storages/Kafka/KafkaBlockOutputStream.cpp b/dbms/src/Storages/Kafka/KafkaBlockOutputStream.cpp\nindex b887d5734520..b533f28382bc 100644\n--- a/dbms/src/Storages/Kafka/KafkaBlockOutputStream.cpp\n+++ b/dbms/src/Storages/Kafka/KafkaBlockOutputStream.cpp\n@@ -28,11 +28,11 @@ Block KafkaBlockOutputStream::getHeader() const\n \n void KafkaBlockOutputStream::writePrefix()\n {\n-    buffer = storage.createWriteBuffer();\n+    buffer = storage.createWriteBuffer(getHeader());\n     if (!buffer)\n         throw Exception(\"Failed to create Kafka producer!\", ErrorCodes::CANNOT_CREATE_IO_BUFFER);\n \n-    child = FormatFactory::instance().getOutput(storage.getFormatName(), *buffer, getHeader(), context, [this]{ buffer->count_row(); });\n+    child = FormatFactory::instance().getOutput(storage.getFormatName(), *buffer, getHeader(), context, [this](const Columns & columns, size_t row){ buffer->count_row(columns, row); });\n }\n \n void KafkaBlockOutputStream::write(const Block & block)\ndiff --git a/dbms/src/Storages/Kafka/StorageKafka.cpp b/dbms/src/Storages/Kafka/StorageKafka.cpp\nindex 6b0bab72bb01..6e3e63a0e097 100644\n--- a/dbms/src/Storages/Kafka/StorageKafka.cpp\n+++ b/dbms/src/Storages/Kafka/StorageKafka.cpp\n@@ -231,7 +231,7 @@ ConsumerBufferPtr StorageKafka::popReadBuffer(std::chrono::milliseconds timeout)\n }\n \n \n-ProducerBufferPtr StorageKafka::createWriteBuffer()\n+ProducerBufferPtr StorageKafka::createWriteBuffer(const Block & header)\n {\n     cppkafka::Configuration conf;\n     conf.set(\"metadata.broker.list\", brokers);\n@@ -245,7 +245,7 @@ ProducerBufferPtr StorageKafka::createWriteBuffer()\n     size_t poll_timeout = settings.stream_poll_timeout_ms.totalMilliseconds();\n \n     return std::make_shared<WriteBufferToKafkaProducer>(\n-        producer, topics[0], row_delimiter ? std::optional<char>{row_delimiter} : std::optional<char>(), 1, 1024, std::chrono::milliseconds(poll_timeout));\n+        producer, topics[0], row_delimiter ? std::optional<char>{row_delimiter} : std::nullopt, 1, 1024, std::chrono::milliseconds(poll_timeout), header);\n }\n \n \ndiff --git a/dbms/src/Storages/Kafka/StorageKafka.h b/dbms/src/Storages/Kafka/StorageKafka.h\nindex bf710f58202c..aa35ecc84530 100644\n--- a/dbms/src/Storages/Kafka/StorageKafka.h\n+++ b/dbms/src/Storages/Kafka/StorageKafka.h\n@@ -53,7 +53,7 @@ class StorageKafka : public ext::shared_ptr_helper<StorageKafka>, public IStorag\n     ConsumerBufferPtr popReadBuffer();\n     ConsumerBufferPtr popReadBuffer(std::chrono::milliseconds timeout);\n \n-    ProducerBufferPtr createWriteBuffer();\n+    ProducerBufferPtr createWriteBuffer(const Block & header);\n \n     const auto & getTopics() const { return topics; }\n     const auto & getFormatName() const { return format_name; }\ndiff --git a/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.cpp b/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.cpp\nindex f88b7eaad5c0..8d6c1ae537fc 100644\n--- a/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.cpp\n+++ b/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.cpp\n@@ -1,4 +1,7 @@\n #include \"WriteBufferToKafkaProducer.h\"\n+#include \"Core/Block.h\"\n+#include \"Columns/ColumnString.h\"\n+#include \"Columns/ColumnsNumber.h\"\n \n namespace DB\n {\n@@ -8,7 +11,9 @@ WriteBufferToKafkaProducer::WriteBufferToKafkaProducer(\n     std::optional<char> delimiter,\n     size_t rows_per_message,\n     size_t chunk_size_,\n-    std::chrono::milliseconds poll_timeout)\n+    std::chrono::milliseconds poll_timeout,\n+    const Block & header\n+    )\n     : WriteBuffer(nullptr, 0)\n     , producer(producer_)\n     , topic(topic_)\n@@ -17,6 +22,26 @@ WriteBufferToKafkaProducer::WriteBufferToKafkaProducer(\n     , chunk_size(chunk_size_)\n     , timeout(poll_timeout)\n {\n+    if (header.has(\"_key\"))\n+    {\n+        auto column_index = header.getPositionByName(\"_key\");\n+        auto column_info = header.getByPosition(column_index);\n+        if (isString(column_info.type))\n+        {\n+            key_column_index = column_index;\n+        }\n+        // else ? (not sure it's a good place to report smth to user)\n+    }\n+\n+    if (header.has(\"_timestamp\"))\n+    {\n+        auto column_index = header.getPositionByName(\"_timestamp\");\n+        auto column_info = header.getByPosition(column_index);\n+        if (isDateTime(column_info.type))\n+        {\n+            timestamp_column_index = column_index;\n+        }\n+    }\n }\n \n WriteBufferToKafkaProducer::~WriteBufferToKafkaProducer()\n@@ -24,22 +49,51 @@ WriteBufferToKafkaProducer::~WriteBufferToKafkaProducer()\n     assert(rows == 0 && chunks.empty());\n }\n \n-void WriteBufferToKafkaProducer::count_row()\n+void WriteBufferToKafkaProducer::count_row(const Columns & columns, size_t current_row)\n {\n+\n     if (++rows % max_rows == 0)\n     {\n+        const std::string & last_chunk = chunks.back();\n+        size_t last_chunk_size = offset();\n+\n+        // if last character of last chunk is delimeter - we don't need it\n+        if (delim && last_chunk[last_chunk_size - 1] == delim)\n+            --last_chunk_size;\n+\n         std::string payload;\n-        payload.reserve((chunks.size() - 1) * chunk_size + offset());\n+        payload.reserve((chunks.size() - 1) * chunk_size + last_chunk_size);\n+\n+        // concat all chunks except the last one\n         for (auto i = chunks.begin(), e = --chunks.end(); i != e; ++i)\n             payload.append(*i);\n-        int trunk_delim = delim && chunks.back()[offset() - 1] == delim ? 1 : 0;\n-        payload.append(chunks.back(), 0, offset() - trunk_delim);\n+\n+        // add last one\n+        payload.append(last_chunk, 0, last_chunk_size);\n+\n+        cppkafka::MessageBuilder builder(topic);\n+        builder.payload(payload);\n+\n+        // Note: if it will be few rows per message - it will take the value from last row of block\n+        if (key_column_index)\n+        {\n+            const auto & key_column = assert_cast<const ColumnString &>(*columns[key_column_index.value()]);\n+            const auto key_data = key_column.getDataAt(current_row);\n+            builder.key(cppkafka::Buffer(key_data.data, key_data.size));\n+        }\n+\n+        if (timestamp_column_index)\n+        {\n+            const auto & timestamp_column = assert_cast<const ColumnUInt32 &>(*columns[timestamp_column_index.value()]);\n+            const auto timestamp = std::chrono::seconds{timestamp_column.getElement(current_row)};\n+            builder.timestamp(timestamp);\n+        }\n \n         while (true)\n         {\n             try\n             {\n-                producer->produce(cppkafka::MessageBuilder(topic).payload(payload));\n+                producer->produce(builder);\n             }\n             catch (cppkafka::HandleException & e)\n             {\ndiff --git a/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.h b/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.h\nindex b6751551ec7d..731dc2dc69f6 100644\n--- a/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.h\n+++ b/dbms/src/Storages/Kafka/WriteBufferToKafkaProducer.h\n@@ -1,6 +1,7 @@\n #pragma once\n \n #include <IO/WriteBuffer.h>\n+#include <Columns/IColumn.h>\n \n #include <cppkafka/cppkafka.h>\n \n@@ -8,7 +9,7 @@\n \n namespace DB\n {\n-\n+class Block;\n using ProducerPtr = std::shared_ptr<cppkafka::Producer>;\n \n class WriteBufferToKafkaProducer : public WriteBuffer\n@@ -20,10 +21,11 @@ class WriteBufferToKafkaProducer : public WriteBuffer\n         std::optional<char> delimiter,\n         size_t rows_per_message,\n         size_t chunk_size_,\n-        std::chrono::milliseconds poll_timeout);\n+        std::chrono::milliseconds poll_timeout,\n+        const Block & header);\n     ~WriteBufferToKafkaProducer() override;\n \n-    void count_row();\n+    void count_row(const Columns & columns, size_t row);\n     void flush();\n \n private:\n@@ -38,6 +40,8 @@ class WriteBufferToKafkaProducer : public WriteBuffer\n \n     size_t rows = 0;\n     std::list<std::string> chunks;\n+    std::optional<size_t> key_column_index;\n+    std::optional<size_t> timestamp_column_index;\n };\n \n }\n",
  "test_patch": "diff --git a/dbms/src/Formats/tests/tab_separated_streams.cpp b/dbms/src/Formats/tests/tab_separated_streams.cpp\nindex 671043b9aac6..4071e4535029 100644\n--- a/dbms/src/Formats/tests/tab_separated_streams.cpp\n+++ b/dbms/src/Formats/tests/tab_separated_streams.cpp\n@@ -45,7 +45,7 @@ try\n     BlockInputStreamPtr block_input = std::make_shared<InputStreamFromInputFormat>(std::move(input_format));\n \n     BlockOutputStreamPtr block_output = std::make_shared<OutputStreamToOutputFormat>(\n-        std::make_shared<TabSeparatedRowOutputFormat>(out_buf, sample, false, false, [] {}, format_settings));\n+        std::make_shared<TabSeparatedRowOutputFormat>(out_buf, sample, false, false, [](const Columns & /* columns */, size_t /* row */){}, format_settings));\n \n     copyData(*block_input, *block_output);\n     return 0;\ndiff --git a/dbms/tests/integration/test_storage_kafka/test.py b/dbms/tests/integration/test_storage_kafka/test.py\nindex c37482ae18b7..8c4f2fbc9ef5 100644\n--- a/dbms/tests/integration/test_storage_kafka/test.py\n+++ b/dbms/tests/integration/test_storage_kafka/test.py\n@@ -789,6 +789,55 @@ def test_kafka_virtual_columns2(kafka_cluster):\n     assert TSV(result) == TSV(expected)\n \n \n+\n+@pytest.mark.timeout(240)\n+def test_kafka_produce_key_timestamp(kafka_cluster):\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.view;\n+        DROP TABLE IF EXISTS test.consumer;\n+        CREATE TABLE test.kafka_writer (key UInt64, value UInt64, _key String, _timestamp DateTime)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'insert3',\n+                     kafka_group_name = 'insert3',\n+                     kafka_format = 'TSV',\n+                     kafka_row_delimiter = '\\\\n';\n+\n+        CREATE TABLE test.kafka (key UInt64, value UInt64, inserted_key String, inserted_timestamp DateTime)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'insert3',\n+                     kafka_group_name = 'insert3',\n+                     kafka_format = 'TSV',\n+                     kafka_row_delimiter = '\\\\n';\n+\n+        CREATE MATERIALIZED VIEW test.view Engine=Log AS\n+            SELECT key, value, inserted_key, toUnixTimestamp(inserted_timestamp), _key, _topic, _partition, _offset, toUnixTimestamp(_timestamp) FROM test.kafka;\n+    ''')\n+\n+    instance.query(\"INSERT INTO test.kafka_writer VALUES ({},{},'{}',toDateTime({}))\".format(1,1,'k1',1577836801))\n+    instance.query(\"INSERT INTO test.kafka_writer VALUES ({},{},'{}',toDateTime({}))\".format(2,2,'k2',1577836802))\n+    instance.query(\"INSERT INTO test.kafka_writer VALUES ({},{},'{}',toDateTime({})),({},{},'{}',toDateTime({}))\".format(3,3,'k3',1577836803,4,4,'k4',1577836804))\n+    instance.query(\"INSERT INTO test.kafka_writer VALUES ({},{},'{}',toDateTime({}))\".format(5,5,'k5',1577836805))\n+\n+    time.sleep(10)\n+\n+    result = instance.query(\"SELECT * FROM test.view ORDER BY value\", ignore_error=True)\n+\n+    # print(result)\n+\n+    expected = '''\\\n+1\t1\tk1\t1577836801\tk1\tinsert3\t0\t0\t1577836801\n+2\t2\tk2\t1577836802\tk2\tinsert3\t0\t1\t1577836802\n+3\t3\tk3\t1577836803\tk3\tinsert3\t0\t2\t1577836803\n+4\t4\tk4\t1577836804\tk4\tinsert3\t0\t3\t1577836804\n+5\t5\tk5\t1577836805\tk5\tinsert3\t0\t4\t1577836805\n+'''\n+\n+    assert TSV(result) == TSV(expected)\n+\n+\n+\n @pytest.mark.timeout(600)\n def test_kafka_flush_by_time(kafka_cluster):\n     instance.query('''\n@@ -876,9 +925,9 @@ def test_kafka_flush_by_block_size(kafka_cluster):\n     time.sleep(1)\n \n     result = instance.query('SELECT count() FROM test.view')\n-    print(result)\n+    # print(result)\n \n-   # kafka_cluster.open_bash_shell('instance')\n+    # kafka_cluster.open_bash_shell('instance')\n \n     instance.query('''\n         DROP TABLE test.consumer;\n",
  "problem_statement": "Kafka producer / inserts - ability to set key & timestamp \nvia virtual columns _key and _timestamp?\n",
  "hints_text": "",
  "created_at": "2020-02-03T11:42:34Z"
}