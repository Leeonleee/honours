{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 16410,
  "instance_id": "ClickHouse__ClickHouse-16410",
  "issue_numbers": [
    "15683"
  ],
  "base_commit": "cc5f15da291d96cafb4f5d47beff424e73d51839",
  "patch": "diff --git a/src/AggregateFunctions/ReservoirSamplerDeterministic.h b/src/AggregateFunctions/ReservoirSamplerDeterministic.h\nindex 3097070c651e..eae24c1f3e92 100644\n--- a/src/AggregateFunctions/ReservoirSamplerDeterministic.h\n+++ b/src/AggregateFunctions/ReservoirSamplerDeterministic.h\n@@ -39,8 +39,8 @@ namespace ErrorCodes\n \n namespace detail\n {\n-const size_t DEFAULT_SAMPLE_COUNT = 8192;\n-const auto MAX_SKIP_DEGREE = sizeof(UInt32) * 8;\n+    const size_t DEFAULT_MAX_SAMPLE_SIZE = 8192;\n+    const auto MAX_SKIP_DEGREE = sizeof(UInt32) * 8;\n }\n \n /// What if there is not a single value - throw an exception, or return 0 or NaN in the case of double?\n@@ -50,6 +50,7 @@ enum class ReservoirSamplerDeterministicOnEmpty\n     RETURN_NAN_OR_ZERO,\n };\n \n+\n template <typename T,\n     ReservoirSamplerDeterministicOnEmpty OnEmpty = ReservoirSamplerDeterministicOnEmpty::THROW>\n class ReservoirSamplerDeterministic\n@@ -60,8 +61,8 @@ class ReservoirSamplerDeterministic\n     }\n \n public:\n-    ReservoirSamplerDeterministic(const size_t sample_count_ = DEFAULT_SAMPLE_COUNT)\n-        : sample_count{sample_count_}\n+    ReservoirSamplerDeterministic(const size_t max_sample_size_ = detail::DEFAULT_MAX_SAMPLE_SIZE)\n+        : max_sample_size{max_sample_size_}\n     {\n     }\n \n@@ -131,8 +132,8 @@ class ReservoirSamplerDeterministic\n \n     void merge(const ReservoirSamplerDeterministic & b)\n     {\n-        if (sample_count != b.sample_count)\n-            throw Poco::Exception(\"Cannot merge ReservoirSamplerDeterministic's with different sample_count\");\n+        if (max_sample_size != b.max_sample_size)\n+            throw Poco::Exception(\"Cannot merge ReservoirSamplerDeterministic's with different max sample size\");\n         sorted = false;\n \n         if (b.skip_degree > skip_degree)\n@@ -150,11 +151,16 @@ class ReservoirSamplerDeterministic\n \n     void read(DB::ReadBuffer & buf)\n     {\n-        DB::readIntBinary<size_t>(sample_count, buf);\n+        size_t size = 0;\n+        DB::readIntBinary<size_t>(size, buf);\n         DB::readIntBinary<size_t>(total_values, buf);\n-        samples.resize(std::min(total_values, sample_count));\n \n-        for (size_t i = 0; i < samples.size(); ++i)\n+        /// Compatibility with old versions.\n+        if (size > total_values)\n+            size = total_values;\n+\n+        samples.resize(size);\n+        for (size_t i = 0; i < size; ++i)\n             DB::readPODBinary(samples[i], buf);\n \n         sorted = false;\n@@ -162,10 +168,11 @@ class ReservoirSamplerDeterministic\n \n     void write(DB::WriteBuffer & buf) const\n     {\n-        DB::writeIntBinary<size_t>(sample_count, buf);\n+        size_t size = samples.size();\n+        DB::writeIntBinary<size_t>(size, buf);\n         DB::writeIntBinary<size_t>(total_values, buf);\n \n-        for (size_t i = 0; i < std::min(sample_count, total_values); ++i)\n+        for (size_t i = 0; i < size; ++i)\n             DB::writePODBinary(samples[i], buf);\n     }\n \n@@ -174,18 +181,19 @@ class ReservoirSamplerDeterministic\n     using Element = std::pair<T, UInt32>;\n     using Array = DB::PODArray<Element, 64>;\n \n-    size_t sample_count;\n-    size_t total_values{};\n-    bool sorted{};\n+    const size_t max_sample_size; /// Maximum amount of stored values.\n+    size_t total_values = 0;   /// How many values were inserted (regardless if they remain in sample or not).\n+    bool sorted = false;\n     Array samples;\n-    UInt8 skip_degree{};\n+    UInt8 skip_degree = 0;     /// The number N determining that we save only one per 2^N elements in average.\n \n     void insertImpl(const T & v, const UInt32 hash)\n     {\n-        /// @todo why + 1? I don't quite recall\n-        while (samples.size() + 1 >= sample_count)\n+        /// Make a room for plus one element.\n+        while (samples.size() >= max_sample_size)\n         {\n-            if (++skip_degree > detail::MAX_SKIP_DEGREE)\n+            ++skip_degree;\n+            if (skip_degree > detail::MAX_SKIP_DEGREE)\n                 throw DB::Exception{\"skip_degree exceeds maximum value\", DB::ErrorCodes::MEMORY_LIMIT_EXCEEDED};\n             thinOut();\n         }\n@@ -195,35 +203,17 @@ class ReservoirSamplerDeterministic\n \n     void thinOut()\n     {\n-        auto size = samples.size();\n-        for (size_t i = 0; i < size;)\n-        {\n-            if (!good(samples[i].second))\n-            {\n-                /// swap current element with the last one\n-                std::swap(samples[size - 1], samples[i]);\n-                --size;\n-            }\n-            else\n-                ++i;\n-        }\n-\n-        if (size != samples.size())\n-        {\n-            samples.resize(size);\n-            sorted = false;\n-        }\n+        samples.resize(std::distance(samples.begin(),\n+            std::remove_if(samples.begin(), samples.end(), [this](const auto & elem){ return !good(elem.second); })));\n+        sorted = false;\n     }\n \n     void sortIfNeeded()\n     {\n         if (sorted)\n             return;\n+        std::sort(samples.begin(), samples.end(), [](const auto & lhs, const auto & rhs) { return lhs.first < rhs.first; });\n         sorted = true;\n-        std::sort(samples.begin(), samples.end(), [] (const std::pair<T, UInt32> & lhs, const std::pair<T, UInt32> & rhs)\n-        {\n-            return lhs.first < rhs.first;\n-        });\n     }\n \n     template <typename ResultType>\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/00273_quantiles.reference b/tests/queries/0_stateless/00273_quantiles.reference\nindex d75d11191f4e..616e06841e4f 100644\n--- a/tests/queries/0_stateless/00273_quantiles.reference\n+++ b/tests/queries/0_stateless/00273_quantiles.reference\n@@ -6,17 +6,17 @@\n [0,1,10,50,100,200,300,400,500,600,700,800,900,950,990,999,1000]\n [0,0.50100005,9.51,49.55,99.6,199.7,299.8,399.9,500,600.1,700.2,800.3,900.4,950.45,990.49,999.499,1000]\n [0,1,10,50,100,200,300,400,500,600,700,800,900,950,990,999,1000]\n-1\t333334\t[699140.3,835642,967430.8]\t[699999,833333,966666]\n+1\t333334\t[699144.2,835663,967429.2]\t[699999,833333,966666]\n 2\t266667\t[426549.5,536255.5,638957.6]\t[426665,533332,639999]\n-3\t114285\t[296938.5,342335,388777.5]\t[297142,342856,388570]\n+3\t114285\t[296938,342324,388778]\t[297142,342856,388570]\n 4\t63492\t[228370.2,254019.5,279351.4]\t[228571,253968,279364]\n 5\t40404\t[185603.4,202009,218107]\t[185858,202020,218181]\n-6\t27972\t[156598.6,167864,179118.40000000002]\t[156643,167832,179020]\n-7\t20513\t[135401,143553.5,151792.5]\t[135384,143589,151794]\n+6\t27972\t[156598.7,167866,179118.3]\t[156643,167832,179020]\n+7\t20513\t[135400.8,143550,151792.6]\t[135384,143589,151794]\n 8\t15686\t[119239.20000000001,125463,131772.40000000002]\t[119215,125490,131764]\n-9\t12384\t[106509.79999999999,111538,116415.8]\t[106501,111455,116408]\n-10\t10025\t[96223.6,100347,104288.6]\t[96240,100250,104260]\n-11\t8282\t[87732.70000000001,91035,94408.6]\t[87784,91097,94409]\n+9\t12384\t[106510.20000000001,111539,116415.7]\t[106501,111455,116408]\n+10\t10025\t[96223.2,100346,104288.7]\t[96240,100250,104260]\n+11\t8282\t[87732.8,91036,94410.20000000001]\t[87784,91097,94409]\n 12\t6957\t[80694.6,83477,86259.4]\t[80694,83477,86260]\n 13\t5925\t[74666.40000000001,77036,79405.6]\t[74666,77036,79406]\n 14\t5109\t[69475.8,71519,73562.2]\t[69475,71519,73563]\ndiff --git a/tests/queries/0_stateless/01533_quantile_deterministic_assert.reference b/tests/queries/0_stateless/01533_quantile_deterministic_assert.reference\nnew file mode 100644\nindex 000000000000..231c72269ca1\n--- /dev/null\n+++ b/tests/queries/0_stateless/01533_quantile_deterministic_assert.reference\n@@ -0,0 +1,1 @@\n+3998\ndiff --git a/tests/queries/0_stateless/01533_quantile_deterministic_assert.sql b/tests/queries/0_stateless/01533_quantile_deterministic_assert.sql\nnew file mode 100644\nindex 000000000000..c75e5dd501fb\n--- /dev/null\n+++ b/tests/queries/0_stateless/01533_quantile_deterministic_assert.sql\n@@ -0,0 +1,1 @@\n+SELECT quantileDeterministic(number, sipHash64(number)) FROM remote('127.0.0.{1,2}', numbers(8193));\n",
  "problem_statement": "Assert in quantilesDeterministic\nFrom https://clickhouse-test-reports.s3.yandex.net/15645/3564ba1c632f182ddfacd996dbbfef2d0db2ed45/fuzzer/report.html#fail1\r\n\r\n```\r\n2020.10.06 15:22:04.796774 [ 95 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Debug> executeQuery: (from [::1]:39472) SELECT quantilesDeterministic(0.5, 0.9)(number, number) FROM remote('127.0.0.{1,2}', numbers(1048575))\r\n2020.10.06 15:22:04.797300 [ 95 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> ContextAccess (default): Access granted: CREATE TEMPORARY TABLE, REMOTE ON *.*\r\n2020.10.06 15:22:04.797882 [ 95 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.*\r\n2020.10.06 15:22:04.802513 [ 95 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.*\r\n2020.10.06 15:22:04.802993 [ 95 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.*\r\n2020.10.06 15:22:04.804962 [ 95 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n2020.10.06 15:22:04.807012 [ 95 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n2020.10.06 15:22:04.810364 [ 198 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> Connection (127.0.0.2:9000): Connecting. Database: (not specified). User: default\r\n2020.10.06 15:22:04.810758 [ 97 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::ffff:127.0.0.1]:35640\r\n2020.10.06 15:22:04.810910 [ 196 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> AggregatingTransform: Aggregating\r\n2020.10.06 15:22:04.810996 [ 196 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> Aggregator: Aggregation method: without_key\r\n2020.10.06 15:22:04.811072 [ 97 ] {} <Debug> TCPHandler: Connected ClickHouse server version 20.10.0, revision: 54441, user: default.\r\n2020.10.06 15:22:04.811694 [ 198 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> Connection (127.0.0.2:9000): Connected to ClickHouse server version 20.10.1.\r\n2020.10.06 15:22:04.815063 [ 97 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Debug> executeQuery: (from [::ffff:127.0.0.1]:35640, initial_query_id: 08e428a7-6547-42bf-a65d-d1a6b5fef24e) SELECT quantilesDeterministic(0.5, 0.9)(number, number) FROM numbers(1048575)\r\n2020.10.06 15:22:04.815611 [ 97 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Trace> ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.*\r\n2020.10.06 15:22:04.817789 [ 97 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n2020.10.06 15:22:04.820745 [ 193 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Trace> AggregatingTransform: Aggregating\r\n2020.10.06 15:22:04.820838 [ 193 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Trace> Aggregator: Aggregation method: without_key\r\n2020.10.06 15:22:04.870011 [ 196 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> AggregatingTransform: Aggregated. 1048575 to 1 rows (from 8.00 MiB) in 0.0641865 sec. (16336379.145147346 rows/sec., 124.64 MiB/sec.)\r\n2020.10.06 15:22:04.870099 [ 196 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> Aggregator: Merging aggregated data\r\n2020.10.06 15:22:04.870891 [ 196 ] {08e428a7-6547-42bf-a65d-d1a6b5fef24e} <Trace> MergingAggregatedTransform: Reading blocks of partially aggregated data.\r\n2020.10.06 15:22:04.879684 [ 193 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Trace> AggregatingTransform: Aggregated. 1048575 to 1 rows (from 8.00 MiB) in 0.061079131 sec. (17167483.93162306 rows/sec., 130.98 MiB/sec.)\r\n2020.10.06 15:22:04.879762 [ 193 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Trace> Aggregator: Merging aggregated data\r\n2020.10.06 15:22:04.880461 [ 193 ] {8141e38d-9866-4703-87c1-37d707b9c557} <Trace> PipelineExecutor: Thread finished. Total time: 0.060201491 sec. Execution time: 0.05824177 sec. Processing time: 0.001842921 sec. Wait time: 0.0001168 sec.\r\nclickhouse-server: ../src/Common/PODArray.h:376: const T &DB::PODArray<std::__1::pair<unsigned long, unsigned int>, 64, Allocator<false, false>, 0, 0>::operator[](ssize_t) const [T = std::__1::pair<unsigned long, unsigned int>, initial_bytes = 64, TAllocator = Allocator<false, false>, pad_right_ = 0, pad_left_ = 0]: Assertion `(n >= (static_cast<ssize_t>(pad_left_) ? -1 : 0)) && (n <= static_cast<ssize_t>(this->size()))' failed.\r\n2020.10.06 15:22:04.881139 [ 64 ] {} <Trace> BaseDaemon: Received signal 6\r\n2020.10.06 15:22:04.881546 [ 199 ] {} <Fatal> BaseDaemon: ########################################\r\n2020.10.06 15:22:04.882112 [ 199 ] {} <Fatal> BaseDaemon: (version 20.10.1.4848, build id: 628FF441861A815F) (from thread 97) (query_id: 8141e38d-9866-4703-87c1-37d707b9c557) Received signal Aborted (6)\r\n2020.10.06 15:22:04.882301 [ 199 ] {} <Fatal> BaseDaemon: \r\n2020.10.06 15:22:04.882496 [ 199 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fc6da5e8f47 0x7fc6da5ea8b1 0x7fc6da5da42a 0x7fc6da5da4a2 0x10435c26 0x10435b44 0x10435aad 0x1045b6d9 0x17f58cf2 0xfd38144 0x1897f674 0x1897fab4 0x195772ec 0x1957608c 0x195723e9 0x1957ac88 0x1d22e54c 0x1d22ed5c 0x1d371093 0x1d36dfcd 0x1d36ce58 0x7fc6dadae6db 0x7fc6da6cba3f\r\n2020.10.06 15:22:04.882980 [ 199 ] {} <Fatal> BaseDaemon: 4. /build/glibc-2ORdQG/glibc-2.27/signal/../sysdeps/unix/sysv/linux/raise.c:51: raise @ 0x3ef47 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n2020.10.06 15:22:04.883276 [ 199 ] {} <Fatal> BaseDaemon: 5. /build/glibc-2ORdQG/glibc-2.27/stdlib/abort.c:81: __GI_abort @ 0x408b1 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n2020.10.06 15:22:04.883553 [ 199 ] {} <Fatal> BaseDaemon: 6. /build/glibc-2ORdQG/glibc-2.27/assert/assert.c:89: __assert_fail_base @ 0x3042a in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n2020.10.06 15:22:04.883997 [ 199 ] {} <Fatal> BaseDaemon: 7. ? @ 0x304a2 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n2020.10.06 15:22:04.893888 [ 199 ] {} <Fatal> BaseDaemon: 8. /build/obj-x86_64-linux-gnu/../src/Common/PODArray.h:0: DB::PODArray<std::__1::pair<unsigned long, unsigned int>, 64ul, Allocator<false, false>, 0ul, 0ul>::operator[](long) const @ 0x10435c26 in /workspace/clickhouse\r\n2020.10.06 15:22:04.903678 [ 199 ] {} <Fatal> BaseDaemon: 9. /build/obj-x86_64-linux-gnu/../src/AggregateFunctions/ReservoirSamplerDeterministic.h:169: ReservoirSamplerDeterministic<unsigned long, (ReservoirSamplerDeterministicOnEmpty)1>::write(DB::WriteBuffer&) const @ 0x10435b44 in /workspace/clickhouse\r\n2020.10.06 15:22:04.913810 [ 199 ] {} <Fatal> BaseDaemon: 10. /build/obj-x86_64-linux-gnu/../src/AggregateFunctions/QuantileReservoirSamplerDeterministic.h:47: DB::QuantileReservoirSamplerDeterministic<unsigned long>::serialize(DB::WriteBuffer&) const @ 0x10435aad in /workspace/clickhouse\r\n2020.10.06 15:22:04.925088 [ 199 ] {} <Fatal> BaseDaemon: 11. /build/obj-x86_64-linux-gnu/../src/AggregateFunctions/AggregateFunctionQuantile.h:134: DB::AggregateFunctionQuantile<unsigned long, DB::QuantileReservoirSamplerDeterministic<unsigned long>, DB::NameQuantilesDeterministic, true, double, true>::serialize(char const*, DB::WriteBuffer&) const @ 0x1045b6d9 in /workspace/clickhouse\r\n2020.10.06 15:22:04.958822 [ 199 ] {} <Fatal> BaseDaemon: 12. /build/obj-x86_64-linux-gnu/../src/DataTypes/DataTypeAggregateFunction.cpp:118: DB::DataTypeAggregateFunction::serializeBinaryBulk(DB::IColumn const&, DB::WriteBuffer&, unsigned long, unsigned long) const @ 0x17f58cf2 in /workspace/clickhouse\r\n2020.10.06 15:22:04.960514 [ 199 ] {} <Fatal> BaseDaemon: 13. /build/obj-x86_64-linux-gnu/../src/DataTypes/IDataType.h:185: DB::IDataType::serializeBinaryBulkWithMultipleStreams(DB::IColumn const&, unsigned long, unsigned long, DB::IDataType::SerializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::SerializeBinaryBulkState>&) const @ 0xfd38144 in /workspace/clickhouse\r\n2020.10.06 15:22:04.997478 [ 199 ] {} <Fatal> BaseDaemon: 14. /build/obj-x86_64-linux-gnu/../src/DataStreams/NativeBlockOutputStream.cpp:58: DB::NativeBlockOutputStream::writeData(DB::IDataType const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&, DB::WriteBuffer&, unsigned long, unsigned long) @ 0x1897f674 in /workspace/clickhouse\r\n2020.10.06 15:22:05.005873 [ 188 ] {} <Debug> system.metric_log (20fa8402-c2d9-4e7b-ad54-6839342b3d0c) (MergerMutator): Merge sorted 2262 rows, containing 264 columns (264 merged, 0 gathered) in 0.513521944 sec., 4404.8750524281395 rows/sec., 8.83 MiB/sec.\r\n2020.10.06 15:22:05.034728 [ 199 ] {} <Fatal> BaseDaemon: 15. /build/obj-x86_64-linux-gnu/../src/DataStreams/NativeBlockOutputStream.cpp:124: DB::NativeBlockOutputStream::write(DB::Block const&) @ 0x1897fab4 in /workspace/clickhouse\r\n2020.10.06 15:22:05.055018 [ 188 ] {} <Trace> system.metric_log (20fa8402-c2d9-4e7b-ad54-6839342b3d0c): Renaming temporary part tmp_merge_202010_1_296_59 to 202010_1_296_59.\r\n2020.10.06 15:22:05.056352 [ 188 ] {} <Trace> system.metric_log (20fa8402-c2d9-4e7b-ad54-6839342b3d0c) (MergerMutator): Merged 6 parts: from 202010_1_291_58 to 202010_296_296_0\r\n2020.10.06 15:22:05.056483 [ 188 ] {} <Debug> MemoryTracker: Peak memory usage: 16.36 MiB.\r\n2020.10.06 15:22:05.080312 [ 199 ] {} <Fatal> BaseDaemon: 16. /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1216: DB::TCPHandler::sendData(DB::Block const&) @ 0x195772ec in /workspace/clickhouse\r\n2020.10.06 15:22:05.125986 [ 199 ] {} <Fatal> BaseDaemon: 17. /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:618: DB::TCPHandler::processOrdinaryQueryWithProcessors() @ 0x1957608c in /workspace/clickhouse\r\n2020.10.06 15:22:05.170191 [ 199 ] {} <Fatal> BaseDaemon: 18. /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:273: DB::TCPHandler::runImpl() @ 0x195723e9 in /workspace/clickhouse\r\n2020.10.06 15:22:05.216264 [ 199 ] {} <Fatal> BaseDaemon: 19. /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1311: DB::TCPHandler::run() @ 0x1957ac88 in /workspace/clickhouse\r\n2020.10.06 15:22:05.267976 [ 199 ] {} <Fatal> BaseDaemon: 20. /build/obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43: Poco::Net::TCPServerConnection::start() @ 0x1d22e54c in /workspace/clickhouse\r\n2020.10.06 15:22:05.320522 [ 199 ] {} <Fatal> BaseDaemon: 21. /build/obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:114: Poco::Net::TCPServerDispatcher::run() @ 0x1d22ed5c in /workspace/clickhouse\r\n2020.10.06 15:22:05.371826 [ 199 ] {} <Fatal> BaseDaemon: 22. /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199: Poco::PooledThread::run() @ 0x1d371093 in /workspace/clickhouse\r\n2020.10.06 15:22:05.416179 [ 68 ] {} <Trace> SystemLog (system.query_thread_log): Flushing system log, 313 entries to flush\r\n2020.10.06 15:22:05.424861 [ 199 ] {} <Fatal> BaseDaemon: 23. /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:56: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x1d36dfcd in /workspace/clickhouse\r\n2020.10.06 15:22:05.433571 [ 68 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 1.08 TiB.\r\n2020.10.06 15:22:05.445462 [ 68 ] {} <Trace> system.query_thread_log (641f7e7c-9710-444c-a6fd-f97afd7e6d51): Renaming temporary part tmp_insert_202010_283_283_0 to 202010_283_283_0.\r\n2020.10.06 15:22:05.447326 [ 68 ] {} <Trace> SystemLog (system.query_thread_log): Flushed system log\r\n2020.10.06 15:22:05.477696 [ 199 ] {} <Fatal> BaseDaemon: 24. /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345: Poco::ThreadImpl::runnableEntry(void*) @ 0x1d36ce58 in /workspace/clickhouse\r\n2020.10.06 15:22:05.478044 [ 199 ] {} <Fatal> BaseDaemon: 25. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n2020.10.06 15:22:05.478459 [ 199 ] {} <Fatal> BaseDaemon: 26. /build/glibc-2ORdQG/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: __clone @ 0x121a3f in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n2020.10.06 15:22:06.064988 [ 199 ] {} <Information> SentryWriter: Not sending crash report\r\n```\n",
  "hints_text": "Cannot reproduce it locally.\nThe algorithm is simply wrong.\nOnly happens in debug build.\nNot led to bugs.",
  "created_at": "2020-10-26T21:16:39Z"
}