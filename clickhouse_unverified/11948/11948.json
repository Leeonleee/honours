{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 11948,
  "instance_id": "ClickHouse__ClickHouse-11948",
  "issue_numbers": [
    "10286"
  ],
  "base_commit": "87b3984d1776545fa3bd05ec23317d556ef4bcfd",
  "patch": "diff --git a/src/Common/FileSyncGuard.h b/src/Common/FileSyncGuard.h\nnew file mode 100644\nindex 000000000000..6451f6ebf36b\n--- /dev/null\n+++ b/src/Common/FileSyncGuard.h\n@@ -0,0 +1,41 @@\n+#pragma once\n+\n+#include <Disks/IDisk.h>\n+\n+namespace DB\n+{\n+\n+/// Helper class, that recieves file descriptor and does fsync for it in destructor.\n+/// It's used to keep descriptor open, while doing some operations with it, and do fsync at the end.\n+/// Guaranties of sequence 'close-reopen-fsync' may depend on kernel version.\n+/// Source: linux-fsdevel mailing-list https://marc.info/?l=linux-fsdevel&m=152535409207496\n+class FileSyncGuard\n+{\n+public:\n+    /// NOTE: If you have already opened descriptor, it's preffered to use\n+    /// this constructor instead of construnctor with path.\n+    FileSyncGuard(const DiskPtr & disk_, int fd_) : disk(disk_), fd(fd_) {}\n+\n+    FileSyncGuard(const DiskPtr & disk_, const String & path)\n+        : disk(disk_), fd(disk_->open(path, O_RDWR)) {}\n+\n+    ~FileSyncGuard()\n+    {\n+        try\n+        {\n+            disk->sync(fd);\n+            disk->close(fd);\n+        }\n+        catch (...)\n+        {\n+            tryLogCurrentException(__PRETTY_FUNCTION__);\n+        }\n+    }\n+\n+private:\n+    DiskPtr disk;\n+    int fd = -1;\n+};\n+\n+}\n+\ndiff --git a/src/Disks/DiskDecorator.cpp b/src/Disks/DiskDecorator.cpp\nindex e55534e347fa..7f2ea58d7cfe 100644\n--- a/src/Disks/DiskDecorator.cpp\n+++ b/src/Disks/DiskDecorator.cpp\n@@ -165,4 +165,19 @@ void DiskDecorator::truncateFile(const String & path, size_t size)\n     delegate->truncateFile(path, size);\n }\n \n+int DiskDecorator::open(const String & path, mode_t mode) const\n+{\n+    return delegate->open(path, mode);\n+}\n+\n+void DiskDecorator::close(int fd) const\n+{\n+    delegate->close(fd);\n+}\n+\n+void DiskDecorator::sync(int fd) const\n+{\n+    delegate->sync(fd);\n+}\n+\n }\ndiff --git a/src/Disks/DiskDecorator.h b/src/Disks/DiskDecorator.h\nindex 71bb100c576f..f1ddfff49520 100644\n--- a/src/Disks/DiskDecorator.h\n+++ b/src/Disks/DiskDecorator.h\n@@ -42,6 +42,9 @@ class DiskDecorator : public IDisk\n     void setReadOnly(const String & path) override;\n     void createHardLink(const String & src_path, const String & dst_path) override;\n     void truncateFile(const String & path, size_t size) override;\n+    int open(const String & path, mode_t mode) const override;\n+    void close(int fd) const override;\n+    void sync(int fd) const override;\n     const String getType() const override { return delegate->getType(); }\n \n protected:\ndiff --git a/src/Disks/DiskLocal.cpp b/src/Disks/DiskLocal.cpp\nindex f9e988211da2..a09ab7c5ac5f 100644\n--- a/src/Disks/DiskLocal.cpp\n+++ b/src/Disks/DiskLocal.cpp\n@@ -8,7 +8,7 @@\n \n #include <IO/createReadBufferFromFileBase.h>\n #include <IO/createWriteBufferFromFileBase.h>\n-\n+#include <unistd.h>\n \n namespace DB\n {\n@@ -19,6 +19,10 @@ namespace ErrorCodes\n     extern const int EXCESSIVE_ELEMENT_IN_CONFIG;\n     extern const int PATH_ACCESS_DENIED;\n     extern const int INCORRECT_DISK_INDEX;\n+    extern const int FILE_DOESNT_EXIST;\n+    extern const int CANNOT_OPEN_FILE;\n+    extern const int CANNOT_FSYNC;\n+    extern const int CANNOT_CLOSE_FILE;\n     extern const int CANNOT_TRUNCATE_FILE;\n }\n \n@@ -292,6 +296,28 @@ void DiskLocal::copy(const String & from_path, const std::shared_ptr<IDisk> & to\n         IDisk::copy(from_path, to_disk, to_path); /// Copy files through buffers.\n }\n \n+int DiskLocal::open(const String & path, mode_t mode) const\n+{\n+    String full_path = disk_path + path;\n+    int fd = ::open(full_path.c_str(), mode);\n+    if (-1 == fd)\n+        throwFromErrnoWithPath(\"Cannot open file \" + full_path, full_path,\n+                        errno == ENOENT ? ErrorCodes::FILE_DOESNT_EXIST : ErrorCodes::CANNOT_OPEN_FILE);\n+    return fd;\n+}\n+\n+void DiskLocal::close(int fd) const\n+{\n+    if (-1 == ::close(fd))\n+        throw Exception(\"Cannot close file\", ErrorCodes::CANNOT_CLOSE_FILE);\n+}\n+\n+void DiskLocal::sync(int fd) const\n+{\n+    if (-1 == ::fsync(fd))\n+        throw Exception(\"Cannot fsync\", ErrorCodes::CANNOT_FSYNC);\n+}\n+\n DiskPtr DiskLocalReservation::getDisk(size_t i) const\n {\n     if (i != 0)\ndiff --git a/src/Disks/DiskLocal.h b/src/Disks/DiskLocal.h\nindex 71c4dc0aec9b..762a8502faae 100644\n--- a/src/Disks/DiskLocal.h\n+++ b/src/Disks/DiskLocal.h\n@@ -99,6 +99,10 @@ class DiskLocal : public IDisk\n \n     void createHardLink(const String & src_path, const String & dst_path) override;\n \n+    int open(const String & path, mode_t mode) const override;\n+    void close(int fd) const override;\n+    void sync(int fd) const override;\n+\n     void truncateFile(const String & path, size_t size) override;\n \n     const String getType() const override { return \"local\"; }\ndiff --git a/src/Disks/DiskMemory.cpp b/src/Disks/DiskMemory.cpp\nindex 96d9e22c414f..d185263d48c9 100644\n--- a/src/Disks/DiskMemory.cpp\n+++ b/src/Disks/DiskMemory.cpp\n@@ -408,6 +408,21 @@ void DiskMemory::setReadOnly(const String &)\n     throw Exception(\"Method setReadOnly is not implemented for memory disks\", ErrorCodes::NOT_IMPLEMENTED);\n }\n \n+int DiskMemory::open(const String & /*path*/, mode_t /*mode*/) const\n+{\n+    throw Exception(\"Method open is not implemented for memory disks\", ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n+void DiskMemory::close(int /*fd*/) const\n+{\n+    throw Exception(\"Method close is not implemented for memory disks\", ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n+void DiskMemory::sync(int /*fd*/) const\n+{\n+    throw Exception(\"Method sync is not implemented for memory disks\", ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n void DiskMemory::truncateFile(const String & path, size_t size)\n {\n     std::lock_guard lock(mutex);\ndiff --git a/src/Disks/DiskMemory.h b/src/Disks/DiskMemory.h\nindex fc265ddef031..4d4b947098b2 100644\n--- a/src/Disks/DiskMemory.h\n+++ b/src/Disks/DiskMemory.h\n@@ -90,6 +90,10 @@ class DiskMemory : public IDisk\n \n     void createHardLink(const String & src_path, const String & dst_path) override;\n \n+    int open(const String & path, mode_t mode) const override;\n+    void close(int fd) const override;\n+    void sync(int fd) const override;\n+\n     void truncateFile(const String & path, size_t size) override;\n \n     const String getType() const override { return \"memory\"; }\ndiff --git a/src/Disks/IDisk.h b/src/Disks/IDisk.h\nindex 17de6db3487b..47387fb370a8 100644\n--- a/src/Disks/IDisk.h\n+++ b/src/Disks/IDisk.h\n@@ -177,6 +177,15 @@ class IDisk : public Space\n     /// Create hardlink from `src_path` to `dst_path`.\n     virtual void createHardLink(const String & src_path, const String & dst_path) = 0;\n \n+    /// Wrapper for POSIX open\n+    virtual int open(const String & path, mode_t mode) const = 0;\n+\n+    /// Wrapper for POSIX close\n+    virtual void close(int fd) const = 0;\n+\n+    /// Wrapper for POSIX fsync\n+    virtual void sync(int fd) const = 0;\n+\n     /// Truncate file to specified size.\n     virtual void truncateFile(const String & path, size_t size);\n \ndiff --git a/src/Disks/S3/DiskS3.cpp b/src/Disks/S3/DiskS3.cpp\nindex 3dcb55c2c441..6abb72efeb05 100644\n--- a/src/Disks/S3/DiskS3.cpp\n+++ b/src/Disks/S3/DiskS3.cpp\n@@ -33,6 +33,7 @@ namespace ErrorCodes\n     extern const int CANNOT_SEEK_THROUGH_FILE;\n     extern const int UNKNOWN_FORMAT;\n     extern const int INCORRECT_DISK_INDEX;\n+    extern const int NOT_IMPLEMENTED;\n }\n \n namespace\n@@ -746,6 +747,21 @@ void DiskS3::setReadOnly(const String & path)\n     Poco::File(metadata_path + path).setReadOnly(true);\n }\n \n+int DiskS3::open(const String & /*path*/, mode_t /*mode*/) const\n+{\n+    throw Exception(\"Method open is not implemented for S3 disks\", ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n+void DiskS3::close(int /*fd*/) const\n+{\n+    throw Exception(\"Method close is not implemented for S3 disks\", ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n+void DiskS3::sync(int /*fd*/) const\n+{\n+    throw Exception(\"Method sync is not implemented for S3 disks\", ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n void DiskS3::shutdown()\n {\n     /// This call stops any next retry attempts for ongoing S3 requests.\ndiff --git a/src/Disks/S3/DiskS3.h b/src/Disks/S3/DiskS3.h\nindex db352feb0635..2d9c7f798652 100644\n--- a/src/Disks/S3/DiskS3.h\n+++ b/src/Disks/S3/DiskS3.h\n@@ -100,6 +100,10 @@ class DiskS3 : public IDisk\n \n     void setReadOnly(const String & path) override;\n \n+    int open(const String & path, mode_t mode) const override;\n+    void close(int fd) const override;\n+    void sync(int fd) const override;\n+\n     const String getType() const override { return \"s3\"; }\n \n     void shutdown() override;\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex e4bdedc17f80..0e42d2677290 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -5,6 +5,7 @@\n #include <Disks/SingleDiskVolume.h>\n #include <Common/CurrentMetrics.h>\n #include <Common/NetException.h>\n+#include <Common/FileSyncGuard.h>\n #include <DataStreams/NativeBlockOutputStream.h>\n #include <IO/HTTPCommon.h>\n #include <ext/scope_guard.h>\n@@ -263,9 +264,9 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(\n     int server_protocol_version = parse<int>(in.getResponseCookie(\"server_protocol_version\", \"0\"));\n \n     ReservationPtr reservation;\n+    size_t sum_files_size = 0;\n     if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_SIZE)\n     {\n-        size_t sum_files_size;\n         readBinary(sum_files_size, in);\n         if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_SIZE_AND_TTL_INFOS)\n         {\n@@ -286,12 +287,15 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(\n         reservation = data.makeEmptyReservationOnLargestDisk();\n     }\n \n+    bool sync = (data_settings->min_compressed_bytes_to_fsync_after_fetch\n+                    && sum_files_size >= data_settings->min_compressed_bytes_to_fsync_after_fetch);\n+\n     String part_type = \"Wide\";\n     if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_TYPE)\n         readStringBinary(part_type, in);\n \n     return part_type == \"InMemory\" ? downloadPartToMemory(part_name, metadata_snapshot, std::move(reservation), in)\n-        : downloadPartToDisk(part_name, replica_path, to_detached, tmp_prefix_, std::move(reservation), in);\n+        : downloadPartToDisk(part_name, replica_path, to_detached, tmp_prefix_, sync, std::move(reservation), in);\n }\n \n MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(\n@@ -330,6 +334,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(\n     const String & replica_path,\n     bool to_detached,\n     const String & tmp_prefix_,\n+    bool sync,\n     const ReservationPtr reservation,\n     PooledReadWriteBufferFromHTTP & in)\n {\n@@ -351,6 +356,10 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(\n \n     disk->createDirectories(part_download_path);\n \n+    std::optional<FileSyncGuard> sync_guard;\n+    if (data.getSettings()->fsync_part_directory)\n+        sync_guard.emplace(disk, part_download_path);\n+\n     MergeTreeData::DataPart::Checksums checksums;\n     for (size_t i = 0; i < files; ++i)\n     {\n@@ -392,6 +401,9 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(\n             file_name != \"columns.txt\" &&\n             file_name != IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME)\n             checksums.addFile(file_name, file_size, expected_hash);\n+\n+        if (sync)\n+            hashing_out.sync();\n     }\n \n     assertEOF(in);\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.h b/src/Storages/MergeTree/DataPartsExchange.h\nindex b1db64596399..52a34a2239a2 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.h\n+++ b/src/Storages/MergeTree/DataPartsExchange.h\n@@ -74,6 +74,7 @@ class Fetcher final\n             const String & replica_path,\n             bool to_detached,\n             const String & tmp_prefix_,\n+            bool sync,\n             const ReservationPtr reservation,\n             PooledReadWriteBufferFromHTTP & in);\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 872e34adb830..486e444763d2 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -11,6 +11,7 @@\n #include <Storages/MergeTree/checkDataPart.h>\n #include <Common/StringUtils/StringUtils.h>\n #include <Common/escapeForFileName.h>\n+#include <Common/FileSyncGuard.h>\n #include <common/JSON.h>\n #include <common/logger_useful.h>\n #include <Compression/getCompressionCodecForFile.h>\n@@ -778,6 +779,10 @@ void IMergeTreeDataPart::renameTo(const String & new_relative_path, bool remove_\n     String from = getFullRelativePath();\n     String to = storage.relative_data_path + new_relative_path + \"/\";\n \n+    std::optional<FileSyncGuard> sync_guard;\n+    if (storage.getSettings()->fsync_part_directory)\n+        sync_guard.emplace(volume->getDisk(), to);\n+\n     if (!volume->getDisk()->exists(from))\n         throw Exception(\"Part directory \" + fullPath(volume->getDisk(), from) + \" doesn't exist. Most likely it is logical error.\", ErrorCodes::FILE_DOESNT_EXIST);\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\nindex 959d6af9ed2f..4a42a58a65b3 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n@@ -51,9 +51,9 @@ class IMergeTreeDataPartWriter : private boost::noncopyable\n     virtual void initSkipIndices() {}\n     virtual void initPrimaryIndex() {}\n \n-    virtual void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) = 0;\n-    virtual void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & /* checksums */) {}\n-    virtual void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & /* checksums */) {}\n+    virtual void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) = 0;\n+    virtual void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & /* checksums */, bool /* sync */) {}\n+    virtual void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & /* checksums */, bool /* sync */) {}\n \n     Columns releaseIndexColumns();\n     const MergeTreeIndexGranularity & getIndexGranularity() const { return index_granularity; }\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 03da0033f9dc..5969ecc5baf7 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -221,6 +221,8 @@ MergeTreeData::MergeTreeData(\n         format_version = min_format_version;\n         auto buf = version_file.second->writeFile(version_file.first);\n         writeIntText(format_version.toUnderType(), *buf);\n+        if (global_context.getSettingsRef().fsync_metadata)\n+            buf->sync();\n     }\n     else\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex a8f7e265f68e..35ab90a80838 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -29,6 +29,7 @@\n #include <Common/interpolate.h>\n #include <Common/typeid_cast.h>\n #include <Common/escapeForFileName.h>\n+#include <Common/FileSyncGuard.h>\n #include <Parsers/queryToString.h>\n \n #include <cmath>\n@@ -613,6 +614,13 @@ class MergeProgressCallback\n     }\n };\n \n+static bool needSyncPart(const size_t input_rows, size_t input_bytes, const MergeTreeSettings & settings)\n+{\n+    return ((settings.min_rows_to_fsync_after_merge && input_rows >= settings.min_rows_to_fsync_after_merge)\n+        || (settings.min_compressed_bytes_to_fsync_after_merge && input_bytes >= settings.min_compressed_bytes_to_fsync_after_merge));\n+}\n+\n+\n /// parts should be sorted.\n MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTemporaryPart(\n     const FutureMergedMutatedPart & future_part,\n@@ -698,6 +706,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n     }\n \n     size_t sum_input_rows_upper_bound = merge_entry->total_rows_count;\n+    size_t sum_compressed_bytes_upper_bound = merge_entry->total_size_bytes_compressed;\n     MergeAlgorithm merge_alg = chooseMergeAlgorithm(parts, sum_input_rows_upper_bound, gathering_columns, deduplicate, need_remove_expired_values);\n \n     LOG_DEBUG(log, \"Selected MergeAlgorithm: {}\", ((merge_alg == MergeAlgorithm::Vertical) ? \"Vertical\" : \"Horizontal\"));\n@@ -735,6 +744,10 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n         gathering_column_names.clear();\n     }\n \n+    std::optional<FileSyncGuard> sync_guard;\n+    if (data.getSettings()->fsync_part_directory)\n+        sync_guard.emplace(disk, new_part_tmp_path);\n+\n     /** Read from all parts, merge and write into a new one.\n       * In passing, we calculate expression for sorting.\n       */\n@@ -858,7 +871,6 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n     if (need_remove_expired_values)\n         merged_stream = std::make_shared<TTLBlockInputStream>(merged_stream, data, metadata_snapshot, new_data_part, time_of_merge, force_ttl);\n \n-\n     if (metadata_snapshot->hasSecondaryIndices())\n     {\n         const auto & indices = metadata_snapshot->getSecondaryIndices();\n@@ -918,6 +930,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n     if (need_remove_expired_values && ttl_merges_blocker.isCancelled())\n         throw Exception(\"Cancelled merging parts with expired TTL\", ErrorCodes::ABORTED);\n \n+    bool need_sync = needSyncPart(sum_input_rows_upper_bound, sum_compressed_bytes_upper_bound, *data_settings);\n     MergeTreeData::DataPart::Checksums checksums_gathered_columns;\n \n     /// Gather ordinary columns\n@@ -1001,7 +1014,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n                 throw Exception(\"Cancelled merging parts\", ErrorCodes::ABORTED);\n \n             column_gathered_stream.readSuffix();\n-            auto changed_checksums = column_to.writeSuffixAndGetChecksums(new_data_part, checksums_gathered_columns);\n+            auto changed_checksums = column_to.writeSuffixAndGetChecksums(new_data_part, checksums_gathered_columns, need_sync);\n             checksums_gathered_columns.add(std::move(changed_checksums));\n \n             if (rows_written != column_elems_written)\n@@ -1038,9 +1051,9 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n     }\n \n     if (merge_alg != MergeAlgorithm::Vertical)\n-        to.writeSuffixAndFinalizePart(new_data_part);\n+        to.writeSuffixAndFinalizePart(new_data_part, need_sync);\n     else\n-        to.writeSuffixAndFinalizePart(new_data_part, &storage_columns, &checksums_gathered_columns);\n+        to.writeSuffixAndFinalizePart(new_data_part, need_sync, &storage_columns, &checksums_gathered_columns);\n \n     return new_data_part;\n }\n@@ -1130,10 +1143,14 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mutatePartToTempor\n \n     disk->createDirectories(new_part_tmp_path);\n \n+    std::optional<FileSyncGuard> sync_guard;\n+    if (data.getSettings()->fsync_part_directory)\n+        sync_guard.emplace(disk, new_part_tmp_path);\n+\n     /// Don't change granularity type while mutating subset of columns\n     auto mrk_extension = source_part->index_granularity_info.is_adaptive ? getAdaptiveMrkExtension(new_data_part->getType())\n                                                                          : getNonAdaptiveMrkExtension();\n-\n+    bool need_sync = needSyncPart(source_part->rows_count, source_part->getBytesOnDisk(), *data_settings);\n     bool need_remove_expired_values = false;\n \n     if (in && shouldExecuteTTL(metadata_snapshot, in->getHeader().getNamesAndTypesList().getNames(), commands_for_part))\n@@ -1158,7 +1175,8 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mutatePartToTempor\n             time_of_mutation,\n             compression_codec,\n             merge_entry,\n-            need_remove_expired_values);\n+            need_remove_expired_values,\n+            need_sync);\n \n         /// no finalization required, because mutateAllPartColumns use\n         /// MergedBlockOutputStream which finilaze all part fields itself\n@@ -1214,7 +1232,8 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mutatePartToTempor\n                 time_of_mutation,\n                 compression_codec,\n                 merge_entry,\n-                need_remove_expired_values);\n+                need_remove_expired_values,\n+                need_sync);\n         }\n \n         for (const auto & [rename_from, rename_to] : files_to_rename)\n@@ -1695,7 +1714,8 @@ void MergeTreeDataMergerMutator::mutateAllPartColumns(\n     time_t time_of_mutation,\n     const CompressionCodecPtr & compression_codec,\n     MergeListEntry & merge_entry,\n-    bool need_remove_expired_values) const\n+    bool need_remove_expired_values,\n+    bool need_sync) const\n {\n     if (mutating_stream == nullptr)\n         throw Exception(\"Cannot mutate part columns with uninitialized mutations stream. It's a bug\", ErrorCodes::LOGICAL_ERROR);\n@@ -1731,7 +1751,7 @@ void MergeTreeDataMergerMutator::mutateAllPartColumns(\n \n     new_data_part->minmax_idx = std::move(minmax_idx);\n     mutating_stream->readSuffix();\n-    out.writeSuffixAndFinalizePart(new_data_part);\n+    out.writeSuffixAndFinalizePart(new_data_part, need_sync);\n }\n \n void MergeTreeDataMergerMutator::mutateSomePartColumns(\n@@ -1744,7 +1764,8 @@ void MergeTreeDataMergerMutator::mutateSomePartColumns(\n     time_t time_of_mutation,\n     const CompressionCodecPtr & compression_codec,\n     MergeListEntry & merge_entry,\n-    bool need_remove_expired_values) const\n+    bool need_remove_expired_values,\n+    bool need_sync) const\n {\n     if (mutating_stream == nullptr)\n         throw Exception(\"Cannot mutate part columns with uninitialized mutations stream. It's a bug\", ErrorCodes::LOGICAL_ERROR);\n@@ -1778,10 +1799,9 @@ void MergeTreeDataMergerMutator::mutateSomePartColumns(\n \n     mutating_stream->readSuffix();\n \n-    auto changed_checksums = out.writeSuffixAndGetChecksums(new_data_part, new_data_part->checksums);\n+    auto changed_checksums = out.writeSuffixAndGetChecksums(new_data_part, new_data_part->checksums, need_sync);\n \n     new_data_part->checksums.add(std::move(changed_checksums));\n-\n }\n \n void MergeTreeDataMergerMutator::finalizeMutatedPart(\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\nindex 96ab14ba57bf..bfbaccd1b1ea 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\n@@ -196,7 +196,8 @@ class MergeTreeDataMergerMutator\n         time_t time_of_mutation,\n         const CompressionCodecPtr & codec,\n         MergeListEntry & merge_entry,\n-        bool need_remove_expired_values) const;\n+        bool need_remove_expired_values,\n+        bool need_sync) const;\n \n     /// Mutate some columns of source part with mutation_stream\n     void mutateSomePartColumns(\n@@ -209,7 +210,8 @@ class MergeTreeDataMergerMutator\n         time_t time_of_mutation,\n         const CompressionCodecPtr & codec,\n         MergeListEntry & merge_entry,\n-        bool need_remove_expired_values) const;\n+        bool need_remove_expired_values,\n+        bool need_sync) const;\n \n     /// Initialize and write to disk new part fields like checksums, columns,\n     /// etc.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex 9c3325d3d5a3..235c76e744b9 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -160,7 +160,7 @@ void MergeTreeDataPartWriterCompact::writeColumnSingleGranule(\n     column.type->serializeBinaryBulkStateSuffix(serialize_settings, state);\n }\n \n-void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync)\n {\n     if (columns_buffer.size() != 0)\n         writeBlock(header.cloneWithColumns(columns_buffer.releaseColumns()));\n@@ -184,6 +184,12 @@ void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart:\n     plain_file->next();\n     marks.next();\n     addToChecksums(checksums);\n+\n+    if (sync)\n+    {\n+        plain_file->sync();\n+        marks_file->sync();\n+    }\n }\n \n static void fillIndexGranularityImpl(\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\nindex fecf5ce40e86..4beb0dba340a 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n@@ -20,7 +20,7 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n     void write(const Block & block, const IColumn::Permutation * permutation,\n         const Block & primary_key_block, const Block & skip_indexes_block) override;\n \n-    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) override;\n+    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) override;\n \n protected:\n     void fillIndexGranularity(size_t index_granularity_for_block, size_t rows_in_block) override;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp\nindex a74861587373..f0738a1130a4 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp\n@@ -70,7 +70,7 @@ void MergeTreeDataPartWriterInMemory::calculateAndSerializePrimaryIndex(const Bl\n     }\n }\n \n-void MergeTreeDataPartWriterInMemory::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterInMemory::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool)\n {\n     /// If part is empty we still need to initialize block by empty columns.\n     if (!part_in_memory->block)\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h\nindex 92e4228a90db..6e59cdd08a93 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h\n@@ -18,7 +18,7 @@ class MergeTreeDataPartWriterInMemory : public IMergeTreeDataPartWriter\n     void write(const Block & block, const IColumn::Permutation * permutation,\n         const Block & primary_key_block, const Block & skip_indexes_block) override;\n \n-    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) override;\n+    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) override;\n \n     void calculateAndSerializePrimaryIndex(const Block & primary_index_block) override;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\nindex d773e76f7b3f..8295b881d874 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n@@ -308,7 +308,8 @@ void MergeTreeDataPartWriterOnDisk::calculateAndSerializeSkipIndices(const Block\n     skip_index_data_mark = skip_index_current_data_mark;\n }\n \n-void MergeTreeDataPartWriterOnDisk::finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterOnDisk::finishPrimaryIndexSerialization(\n+        MergeTreeData::DataPart::Checksums & checksums, bool sync)\n {\n     bool write_final_mark = (with_final_mark && data_written);\n     if (write_final_mark && compute_granularity)\n@@ -330,12 +331,14 @@ void MergeTreeDataPartWriterOnDisk::finishPrimaryIndexSerialization(MergeTreeDat\n         index_stream->next();\n         checksums.files[\"primary.idx\"].file_size = index_stream->count();\n         checksums.files[\"primary.idx\"].file_hash = index_stream->getHash();\n+        if (sync)\n+            index_file_stream->sync();\n         index_stream = nullptr;\n     }\n }\n \n void MergeTreeDataPartWriterOnDisk::finishSkipIndicesSerialization(\n-        MergeTreeData::DataPart::Checksums & checksums)\n+        MergeTreeData::DataPart::Checksums & checksums, bool sync)\n {\n     for (size_t i = 0; i < skip_indices.size(); ++i)\n     {\n@@ -348,6 +351,8 @@ void MergeTreeDataPartWriterOnDisk::finishSkipIndicesSerialization(\n     {\n         stream->finalize();\n         stream->addToChecksums(checksums);\n+        if (sync)\n+            stream->sync();\n     }\n \n     skip_indices_streams.clear();\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\nindex 8598a5143cb9..f4bd900977f3 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\n@@ -81,8 +81,8 @@ class MergeTreeDataPartWriterOnDisk : public IMergeTreeDataPartWriter\n     void initSkipIndices() final;\n     void initPrimaryIndex() final;\n \n-    void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & checksums) final;\n-    void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & checksums) final;\n+    void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & checksums, bool sync) final;\n+    void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & checksums, bool sync) final;\n \n     void setWrittenOffsetColumns(WrittenOffsetColumns * written_offset_columns_)\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex 835139345d4d..a558c57b5f0b 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -265,7 +265,7 @@ void MergeTreeDataPartWriterWide::writeColumn(\n     next_index_offset = current_row - total_rows;\n }\n \n-void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums)\n+void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync)\n {\n     const auto & global_settings = storage.global_context.getSettingsRef();\n     IDataType::SerializeBinaryBulkSettings serialize_settings;\n@@ -296,6 +296,8 @@ void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Ch\n     {\n         stream.second->finalize();\n         stream.second->addToChecksums(checksums);\n+        if (sync)\n+            stream.second->sync();\n     }\n \n     column_streams.clear();\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\nindex ab40f1c7d2a3..02ab2a7ca567 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n@@ -23,7 +23,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n     void write(const Block & block, const IColumn::Permutation * permutation,\n         const Block & primary_key_block, const Block & skip_indexes_block) override;\n \n-    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) override;\n+    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) override;\n \n     IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenOffsetColumns & offset_columns);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 1d7bf545009b..e5b684a13619 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -12,6 +12,7 @@\n #include <IO/WriteHelpers.h>\n #include <Poco/File.h>\n #include <Common/typeid_cast.h>\n+#include <Common/FileSyncGuard.h>\n \n #include <Parsers/queryToString.h>\n \n@@ -251,6 +252,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataWriter::writeTempPart(BlockWithPa\n     new_data_part->minmax_idx = std::move(minmax_idx);\n     new_data_part->is_temp = true;\n \n+    std::optional<FileSyncGuard> sync_guard;\n     if (new_data_part->isStoredOnDisk())\n     {\n         /// The name could be non-unique in case of stale files from previous runs.\n@@ -262,7 +264,11 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataWriter::writeTempPart(BlockWithPa\n             new_data_part->volume->getDisk()->removeRecursive(full_path);\n         }\n \n-        new_data_part->volume->getDisk()->createDirectories(full_path);\n+        const auto disk = new_data_part->volume->getDisk();\n+        disk->createDirectories(full_path);\n+\n+        if (data.getSettings()->fsync_part_directory)\n+            sync_guard.emplace(disk, full_path);\n     }\n \n     /// If we need to calculate some columns to sort.\n@@ -311,10 +317,11 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataWriter::writeTempPart(BlockWithPa\n \n     const auto & index_factory = MergeTreeIndexFactory::instance();\n     MergedBlockOutputStream out(new_data_part, metadata_snapshot, columns, index_factory.getMany(metadata_snapshot->getSecondaryIndices()), compression_codec);\n+    bool sync_on_insert = data.getSettings()->fsync_after_insert;\n \n     out.writePrefix();\n     out.writeWithPermutation(block, perm_ptr);\n-    out.writeSuffixAndFinalizePart(new_data_part);\n+    out.writeSuffixAndFinalizePart(new_data_part, sync_on_insert);\n \n     ProfileEvents::increment(ProfileEvents::MergeTreeDataWriterRows, block.rows());\n     ProfileEvents::increment(ProfileEvents::MergeTreeDataWriterUncompressedBytes, block.bytes());\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 49847617d30b..8652a6ef6918 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -40,6 +40,14 @@ struct Settings;\n     M(Seconds, old_parts_lifetime, 8 * 60, \"How many seconds to keep obsolete parts.\", 0) \\\n     M(Seconds, temporary_directories_lifetime, 86400, \"How many seconds to keep tmp_-directories.\", 0) \\\n     M(Seconds, lock_acquire_timeout_for_background_operations, DBMS_DEFAULT_LOCK_ACQUIRE_TIMEOUT_SEC, \"For background operations like merges, mutations etc. How many seconds before failing to acquire table locks.\", 0) \\\n+    M(UInt64, min_rows_to_fsync_after_merge, 0, \"Minimal number of rows to do fsync for part after merge (0 - disabled)\", 0) \\\n+    M(UInt64, min_compressed_bytes_to_fsync_after_merge, 0, \"Minimal number of compressed bytes to do fsync for part after merge (0 - disabled)\", 0) \\\n+    M(UInt64, min_compressed_bytes_to_fsync_after_fetch, 0, \"Minimal number of compressed bytes to do fsync for part after fetch (0 - disabled)\", 0) \\\n+    M(Bool, fsync_after_insert, false, \"Do fsync for every inserted part. Significantly decreases performance of inserts, not recommended to use with wide parts.\", 0) \\\n+    M(Bool, fsync_part_directory, false, \"Do fsync for part directory after all part operations (writes, renames, etc.).\", 0) \\\n+    M(UInt64, write_ahead_log_bytes_to_fsync, 100ULL * 1024 * 1024, \"Amount of bytes, accumulated in WAL to do fsync.\", 0) \\\n+    M(UInt64, write_ahead_log_interval_ms_to_fsync, 100, \"Interval in milliseconds after which fsync for WAL is being done.\", 0) \\\n+    M(Bool, in_memory_parts_insert_sync, false, \"If true insert of part with in-memory format will wait for fsync of WAL\", 0) \\\n     \\\n     /** Inserts settings. */ \\\n     M(UInt64, parts_to_delay_insert, 150, \"If table contains at least that many active parts in single partition, artificially slow down insert into table.\", 0) \\\ndiff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\nindex 53ef72f32081..35fadb999b45 100644\n--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n@@ -4,6 +4,7 @@\n #include <Storages/MergeTree/MergedBlockOutputStream.h>\n #include <IO/ReadHelpers.h>\n #include <Poco/File.h>\n+#include <sys/time.h>\n \n namespace DB\n {\n@@ -16,17 +17,31 @@ namespace ErrorCodes\n     extern const int CORRUPTED_DATA;\n }\n \n-\n MergeTreeWriteAheadLog::MergeTreeWriteAheadLog(\n-    const MergeTreeData & storage_,\n+    MergeTreeData & storage_,\n     const DiskPtr & disk_,\n     const String & name_)\n     : storage(storage_)\n     , disk(disk_)\n     , name(name_)\n     , path(storage.getRelativeDataPath() + name_)\n+    , pool(storage.global_context.getSchedulePool())\n {\n     init();\n+    sync_task = pool.createTask(\"MergeTreeWriteAheadLog::sync\", [this]\n+    {\n+        std::lock_guard lock(write_mutex);\n+        out->sync();\n+        sync_scheduled = false;\n+        sync_cv.notify_all();\n+    });\n+}\n+\n+MergeTreeWriteAheadLog::~MergeTreeWriteAheadLog()\n+{\n+    std::unique_lock lock(write_mutex);\n+    if (sync_scheduled)\n+        sync_cv.wait(lock, [this] { return !sync_scheduled; });\n }\n \n void MergeTreeWriteAheadLog::init()\n@@ -38,11 +53,12 @@ void MergeTreeWriteAheadLog::init()\n     block_out = std::make_unique<NativeBlockOutputStream>(*out, 0, Block{});\n     min_block_number = std::numeric_limits<Int64>::max();\n     max_block_number = -1;\n+    bytes_at_last_sync = 0;\n }\n \n void MergeTreeWriteAheadLog::addPart(const Block & block, const String & part_name)\n {\n-    std::lock_guard lock(write_mutex);\n+    std::unique_lock lock(write_mutex);\n \n     auto part_info = MergeTreePartInfo::fromPartName(part_name, storage.format_version);\n     min_block_number = std::min(min_block_number, part_info.min_block);\n@@ -53,6 +69,7 @@ void MergeTreeWriteAheadLog::addPart(const Block & block, const String & part_na\n     writeStringBinary(part_name, *out);\n     block_out->write(block);\n     block_out->flush();\n+    sync(lock);\n \n     auto max_wal_bytes = storage.getSettings()->write_ahead_log_max_bytes;\n     if (out->count() > max_wal_bytes)\n@@ -61,14 +78,15 @@ void MergeTreeWriteAheadLog::addPart(const Block & block, const String & part_na\n \n void MergeTreeWriteAheadLog::dropPart(const String & part_name)\n {\n-    std::lock_guard lock(write_mutex);\n+    std::unique_lock lock(write_mutex);\n \n     writeIntBinary(static_cast<UInt8>(0), *out);\n     writeIntBinary(static_cast<UInt8>(ActionType::DROP_PART), *out);\n     writeStringBinary(part_name, *out);\n+    sync(lock);\n }\n \n-void MergeTreeWriteAheadLog::rotate(const std::lock_guard<std::mutex> &)\n+void MergeTreeWriteAheadLog::rotate(const std::unique_lock<std::mutex> &)\n {\n     String new_name = String(WAL_FILE_NAME) + \"_\"\n         + toString(min_block_number) + \"_\"\n@@ -80,7 +98,7 @@ void MergeTreeWriteAheadLog::rotate(const std::lock_guard<std::mutex> &)\n \n MergeTreeData::MutableDataPartsVector MergeTreeWriteAheadLog::restore(const StorageMetadataPtr & metadata_snapshot)\n {\n-    std::lock_guard lock(write_mutex);\n+    std::unique_lock lock(write_mutex);\n \n     MergeTreeData::MutableDataPartsVector parts;\n     auto in = disk->readFile(path, DBMS_DEFAULT_BUFFER_SIZE);\n@@ -175,6 +193,27 @@ MergeTreeData::MutableDataPartsVector MergeTreeWriteAheadLog::restore(const Stor\n     return result;\n }\n \n+void MergeTreeWriteAheadLog::sync(std::unique_lock<std::mutex> & lock)\n+{\n+    size_t bytes_to_sync = storage.getSettings()->write_ahead_log_bytes_to_fsync;\n+    time_t time_to_sync = storage.getSettings()->write_ahead_log_interval_ms_to_fsync;\n+    size_t current_bytes = out->count();\n+\n+    if (bytes_to_sync && current_bytes - bytes_at_last_sync > bytes_to_sync)\n+    {\n+        sync_task->schedule();\n+        bytes_at_last_sync = current_bytes;\n+    }\n+    else if (time_to_sync && !sync_scheduled)\n+    {\n+        sync_task->scheduleAfter(time_to_sync);\n+        sync_scheduled = true;\n+    }\n+\n+    if (storage.getSettings()->in_memory_parts_insert_sync)\n+        sync_cv.wait(lock, [this] { return !sync_scheduled; });\n+}\n+\n std::optional<MergeTreeWriteAheadLog::MinMaxBlockNumber>\n MergeTreeWriteAheadLog::tryParseMinMaxBlockNumber(const String & filename)\n {\ndiff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.h b/src/Storages/MergeTree/MergeTreeWriteAheadLog.h\nindex 2cc3c2b4181a..77c7c7e11e74 100644\n--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.h\n+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.h\n@@ -3,6 +3,7 @@\n #include <DataStreams/NativeBlockInputStream.h>\n #include <DataStreams/NativeBlockOutputStream.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n+#include <Core/BackgroundSchedulePool.h>\n #include <Disks/IDisk.h>\n \n namespace DB\n@@ -31,9 +32,11 @@ class MergeTreeWriteAheadLog\n     constexpr static auto WAL_FILE_EXTENSION = \".bin\";\n     constexpr static auto DEFAULT_WAL_FILE_NAME = \"wal.bin\";\n \n-    MergeTreeWriteAheadLog(const MergeTreeData & storage_, const DiskPtr & disk_,\n+    MergeTreeWriteAheadLog(MergeTreeData & storage_, const DiskPtr & disk_,\n         const String & name = DEFAULT_WAL_FILE_NAME);\n \n+    ~MergeTreeWriteAheadLog();\n+\n     void addPart(const Block & block, const String & part_name);\n     void dropPart(const String & part_name);\n     std::vector<MergeTreeMutableDataPartPtr> restore(const StorageMetadataPtr & metadata_snapshot);\n@@ -43,7 +46,8 @@ class MergeTreeWriteAheadLog\n \n private:\n     void init();\n-    void rotate(const std::lock_guard<std::mutex> & lock);\n+    void rotate(const std::unique_lock<std::mutex> & lock);\n+    void sync(std::unique_lock<std::mutex> & lock);\n \n     const MergeTreeData & storage;\n     DiskPtr disk;\n@@ -56,6 +60,13 @@ class MergeTreeWriteAheadLog\n     Int64 min_block_number = std::numeric_limits<Int64>::max();\n     Int64 max_block_number = -1;\n \n+    BackgroundSchedulePool & pool;\n+    BackgroundSchedulePoolTaskHolder sync_task;\n+    std::condition_variable sync_cv;\n+\n+    size_t bytes_at_last_sync = 0;\n+    bool sync_scheduled = false;\n+\n     mutable std::mutex write_mutex;\n };\n \ndiff --git a/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\nindex b78b23b20809..c91ed545ac53 100644\n--- a/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n@@ -91,6 +91,7 @@ void MergedBlockOutputStream::writeSuffix()\n \n void MergedBlockOutputStream::writeSuffixAndFinalizePart(\n         MergeTreeData::MutableDataPartPtr & new_part,\n+        bool sync,\n         const NamesAndTypesList * total_columns_list,\n         MergeTreeData::DataPart::Checksums * additional_column_checksums)\n {\n@@ -101,9 +102,9 @@ void MergedBlockOutputStream::writeSuffixAndFinalizePart(\n         checksums = std::move(*additional_column_checksums);\n \n     /// Finish columns serialization.\n-    writer->finishDataSerialization(checksums);\n-    writer->finishPrimaryIndexSerialization(checksums);\n-    writer->finishSkipIndicesSerialization(checksums);\n+    writer->finishDataSerialization(checksums, sync);\n+    writer->finishPrimaryIndexSerialization(checksums, sync);\n+    writer->finishSkipIndicesSerialization(checksums, sync);\n \n     NamesAndTypesList part_columns;\n     if (!total_columns_list)\n@@ -112,7 +113,7 @@ void MergedBlockOutputStream::writeSuffixAndFinalizePart(\n         part_columns = *total_columns_list;\n \n     if (new_part->isStoredOnDisk())\n-        finalizePartOnDisk(new_part, part_columns, checksums);\n+        finalizePartOnDisk(new_part, part_columns, checksums, sync);\n \n     new_part->setColumns(part_columns);\n     new_part->rows_count = rows_count;\n@@ -129,7 +130,8 @@ void MergedBlockOutputStream::writeSuffixAndFinalizePart(\n void MergedBlockOutputStream::finalizePartOnDisk(\n     const MergeTreeData::MutableDataPartPtr & new_part,\n     NamesAndTypesList & part_columns,\n-    MergeTreeData::DataPart::Checksums & checksums)\n+    MergeTreeData::DataPart::Checksums & checksums,\n+    bool sync)\n {\n     if (storage.format_version >= MERGE_TREE_DATA_MIN_FORMAT_VERSION_WITH_CUSTOM_PARTITIONING || isCompactPart(new_part))\n     {\n@@ -146,6 +148,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(\n         count_out_hashing.next();\n         checksums.files[\"count.txt\"].file_size = count_out_hashing.count();\n         checksums.files[\"count.txt\"].file_hash = count_out_hashing.getHash();\n+        if (sync)\n+            count_out->sync();\n     }\n \n     if (!new_part->ttl_infos.empty())\n@@ -156,6 +160,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(\n         new_part->ttl_infos.write(out_hashing);\n         checksums.files[\"ttl.txt\"].file_size = out_hashing.count();\n         checksums.files[\"ttl.txt\"].file_hash = out_hashing.getHash();\n+        if (sync)\n+            out->sync();\n     }\n \n     removeEmptyColumnsFromPart(new_part, part_columns, checksums);\n@@ -164,6 +170,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(\n         /// Write a file with a description of columns.\n         auto out = volume->getDisk()->writeFile(part_path + \"columns.txt\", 4096);\n         part_columns.writeText(*out);\n+        if (sync)\n+            out->sync();\n     }\n \n     if (default_codec != nullptr)\n@@ -181,6 +189,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(\n         /// Write file with checksums.\n         auto out = volume->getDisk()->writeFile(part_path + \"checksums.txt\", 4096);\n         checksums.write(*out);\n+        if (sync)\n+            out->sync();\n     }\n }\n \ndiff --git a/src/Storages/MergeTree/MergedBlockOutputStream.h b/src/Storages/MergeTree/MergedBlockOutputStream.h\nindex 6addd4688577..e12d937239d2 100644\n--- a/src/Storages/MergeTree/MergedBlockOutputStream.h\n+++ b/src/Storages/MergeTree/MergedBlockOutputStream.h\n@@ -46,6 +46,7 @@ class MergedBlockOutputStream final : public IMergedBlockOutputStream\n     /// Finilize writing part and fill inner structures\n     void writeSuffixAndFinalizePart(\n             MergeTreeData::MutableDataPartPtr & new_part,\n+            bool sync = false,\n             const NamesAndTypesList * total_columns_list = nullptr,\n             MergeTreeData::DataPart::Checksums * additional_column_checksums = nullptr);\n \n@@ -58,7 +59,8 @@ class MergedBlockOutputStream final : public IMergedBlockOutputStream\n     void finalizePartOnDisk(\n             const MergeTreeData::MutableDataPartPtr & new_part,\n             NamesAndTypesList & part_columns,\n-            MergeTreeData::DataPart::Checksums & checksums);\n+            MergeTreeData::DataPart::Checksums & checksums,\n+            bool sync);\n \n private:\n     NamesAndTypesList columns_list;\ndiff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\nindex cacecf07b1f4..8ce4ea126ed7 100644\n--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n@@ -67,12 +67,15 @@ void MergedColumnOnlyOutputStream::writeSuffix()\n }\n \n MergeTreeData::DataPart::Checksums\n-MergedColumnOnlyOutputStream::writeSuffixAndGetChecksums(MergeTreeData::MutableDataPartPtr & new_part, MergeTreeData::DataPart::Checksums & all_checksums)\n+MergedColumnOnlyOutputStream::writeSuffixAndGetChecksums(\n+    MergeTreeData::MutableDataPartPtr & new_part,\n+    MergeTreeData::DataPart::Checksums & all_checksums,\n+    bool sync)\n {\n     /// Finish columns serialization.\n     MergeTreeData::DataPart::Checksums checksums;\n-    writer->finishDataSerialization(checksums);\n-    writer->finishSkipIndicesSerialization(checksums);\n+    writer->finishDataSerialization(checksums, sync);\n+    writer->finishSkipIndicesSerialization(checksums, sync);\n \n     auto columns = new_part->getColumns();\n \ndiff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h\nindex 902138ced9d6..507a964ede0c 100644\n--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h\n+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h\n@@ -27,7 +27,7 @@ class MergedColumnOnlyOutputStream final : public IMergedBlockOutputStream\n     void write(const Block & block) override;\n     void writeSuffix() override;\n     MergeTreeData::DataPart::Checksums\n-    writeSuffixAndGetChecksums(MergeTreeData::MutableDataPartPtr & new_part, MergeTreeData::DataPart::Checksums & all_checksums);\n+    writeSuffixAndGetChecksums(MergeTreeData::MutableDataPartPtr & new_part, MergeTreeData::DataPart::Checksums & all_checksums, bool sync = false);\n \n private:\n     Block header;\n",
  "test_patch": "diff --git a/utils/durability-test/create.sql b/utils/durability-test/create.sql\nnew file mode 100644\nindex 000000000000..1ec394100e20\n--- /dev/null\n+++ b/utils/durability-test/create.sql\n@@ -0,0 +1,1 @@\n+CREATE TABLE test (a Int, s String) ENGINE = MergeTree ORDER BY a;\ndiff --git a/utils/durability-test/create_sync.sql b/utils/durability-test/create_sync.sql\nnew file mode 100644\nindex 000000000000..2cc88d2c9432\n--- /dev/null\n+++ b/utils/durability-test/create_sync.sql\n@@ -0,0 +1,1 @@\n+CREATE TABLE test_sync (a Int, s String) ENGINE = MergeTree ORDER BY a SETTINGS fsync_after_insert = 1, min_compressed_bytes_to_fsync_after_merge = 1;\ndiff --git a/utils/durability-test/durability-test.sh b/utils/durability-test/durability-test.sh\nnew file mode 100755\nindex 000000000000..97c39473b69e\n--- /dev/null\n+++ b/utils/durability-test/durability-test.sh\n@@ -0,0 +1,168 @@\n+#!/bin/bash\n+\n+: '\n+A simple test for durability. It starts up clickhouse server in qemu VM and runs\n+inserts via clickhouse benchmark tool. Then it kills VM in random moment and\n+checks whether table contains broken parts. With enabled fsync no broken parts\n+should be appeared.\n+\n+Usage:\n+\n+./install.sh\n+./durability-test.sh <table name> <file with create query> <file with insert query>\n+'\n+\n+URL=http://cloud-images.ubuntu.com/bionic/current\n+IMAGE=bionic-server-cloudimg-amd64.img\n+SSH_PORT=11022\n+CLICKHOUSE_PORT=9090\n+PASSWORD=root\n+\n+TABLE_NAME=$1\n+CREATE_QUERY=$2\n+INSERT_QUERY=$3\n+\n+if [[ -z $TABLE_NAME || -z $CREATE_QUERY || -z $INSERT_QUERY ]]; then\n+    echo \"Required 3 arguments: table name, file with create query, file with insert query\"\n+    exit 1\n+fi\n+\n+function run()\n+{\n+    sshpass -p $PASSWORD ssh -p $SSH_PORT root@localhost \"$1\" 2>/dev/null\n+}\n+\n+function copy()\n+{\n+    sshpass -p $PASSWORD scp -r -P $SSH_PORT $1 root@localhost:$2 2>/dev/null\n+}\n+\n+function wait_vm_for_start()\n+{\n+    echo \"Waiting until VM started...\"\n+    started=0\n+    for i in {0..100}; do\n+        run \"exit\"\n+        if [ $? -eq 0 ]; then\n+            started=1\n+            break\n+        fi \n+        sleep 1s\n+    done\n+\n+    if ((started == 0)); then\n+        echo \"Can't start or connect to VM.\"\n+        exit 1\n+    fi\n+\n+    echo \"Started VM\"\n+}\n+\n+function wait_clickhouse_for_start()\n+{\n+    echo \"Waiting until ClickHouse started...\"\n+    started=0\n+    for i in {0..30}; do\n+        run \"clickhouse client --query 'select 1'\" > /dev/null\n+        if [ $? -eq 0 ]; then\n+            started=1\n+            break\n+        fi\n+        sleep 1s\n+    done\n+\n+    if ((started == 0)); then\n+        echo \"Can't start ClickHouse.\"\n+    fi\n+\n+    echo \"Started ClickHouse\"\n+}\n+\n+echo \"Downloading image\"\n+curl -O $URL/$IMAGE\n+\n+qemu-img resize $IMAGE +10G\n+virt-customize -a $IMAGE --root-password password:$PASSWORD > /dev/null 2>&1\n+virt-copy-in -a $IMAGE sshd_config /etc/ssh\n+\n+echo \"Starting VM\"\n+\n+chmod +x ./startup.exp\n+./startup.exp > qemu.log 2>&1 &\n+\n+wait_vm_for_start\n+\n+echo \"Preparing VM\"\n+\n+# Resize partition\n+run \"growpart /dev/sda 1 && resize2fs /dev/sda1\"\n+\n+if [[ -z $CLICKHOUSE_BINARY ]]; then\n+    CLICKHOUSE_BINARY=/usr/bin/clickhouse\n+fi\n+\n+if [[ -z $CLICKHOUSE_CONFIG_DIR ]]; then\n+    CLICKHOUSE_CONFIG_DIR=/etc/clickhouse-server\n+fi\n+\n+echo \"Using ClickHouse binary:\" $CLICKHOUSE_BINARY\n+echo \"Using ClickHouse config from:\" $CLICKHOUSE_CONFIG_DIR\n+\n+copy $CLICKHOUSE_BINARY /usr/bin\n+copy $CLICKHOUSE_CONFIG_DIR /etc\n+run \"mv /etc/$CLICKHOUSE_CONFIG_DIR /etc/clickhouse-server\"\n+\n+echo \"Prepared VM\"\n+echo \"Starting ClickHouse\"\n+\n+run \"clickhouse server --config-file=/etc/clickhouse-server/config.xml > clickhouse-server.log 2>&1\" &\n+wait_clickhouse_for_start\n+\n+query=`cat $CREATE_QUERY`\n+echo \"Executing query:\" $query\n+run \"clickhouse client --query '$query'\"\n+\n+query=`cat $INSERT_QUERY`\n+echo \"Will run in a loop query: \" $query\n+run \"clickhouse benchmark <<< '$query' -c 8\" &\n+echo \"Running queries\"\n+\n+pid=`pidof qemu-system-x86_64`\n+sec=$(( (RANDOM % 5) + 25 ))\n+ms=$(( RANDOM % 1000 ))\n+\n+echo \"Will kill VM in $sec.$ms sec\"\n+\n+sleep $sec.$ms\n+kill -9 $pid\n+\n+echo \"Restarting\"\n+\n+sleep 5s\n+\n+./startup.exp > qemu.log 2>&1 &\n+wait_vm_for_start\n+\n+run \"rm -r *data/system\"\n+run \"clickhouse server --config-file=/etc/clickhouse-server/config.xml > clickhouse-server.log 2>&1\" &\n+wait_clickhouse_for_start\n+\n+pid=`pidof qemu-system-x86_64`\n+result=`run \"grep $TABLE_NAME clickhouse-server.log | grep 'Caught exception while loading metadata'\"`\n+if [[ -n $result ]]; then\n+    echo \"FAIL. Can't attach table:\"\n+    echo $result\n+    kill -9 $pid\n+    exit 1\n+fi\n+\n+result=`run \"grep $TABLE_NAME clickhouse-server.log | grep 'Considering to remove broken part'\"`\n+if [[ -n $result ]]; then\n+    echo \"FAIL. Have broken parts:\"\n+    echo $result\n+    kill -9 $pid\n+    exit 1\n+fi\n+\n+kill -9 $pid\n+echo OK\ndiff --git a/utils/durability-test/insert.sql b/utils/durability-test/insert.sql\nnew file mode 100644\nindex 000000000000..8982ad47228f\n--- /dev/null\n+++ b/utils/durability-test/insert.sql\n@@ -0,0 +1,1 @@\n+INSERT INTO test SELECT number, toString(number) FROM numbers(10)\ndiff --git a/utils/durability-test/insert_sync.sql b/utils/durability-test/insert_sync.sql\nnew file mode 100644\nindex 000000000000..a1ad2ff4ea58\n--- /dev/null\n+++ b/utils/durability-test/insert_sync.sql\n@@ -0,0 +1,1 @@\n+INSERT INTO test_sync SELECT number, toString(number) FROM numbers(10)\ndiff --git a/utils/durability-test/install.sh b/utils/durability-test/install.sh\nnew file mode 100644\nindex 000000000000..526cde6743fc\n--- /dev/null\n+++ b/utils/durability-test/install.sh\n@@ -0,0 +1,3 @@\n+#!/bin/bash\n+\n+apt update && apt install qemu-kvm qemu virt-manager virt-viewer libguestfs-tools sshpass expect\ndiff --git a/utils/durability-test/sshd_config b/utils/durability-test/sshd_config\nnew file mode 100644\nindex 000000000000..6ed06d3d8ad3\n--- /dev/null\n+++ b/utils/durability-test/sshd_config\n@@ -0,0 +1,8 @@\n+PermitRootLogin yes\n+PasswordAuthentication yes\n+ChallengeResponseAuthentication no\n+UsePAM yes\n+X11Forwarding yes\n+PrintMotd no\n+AcceptEnv LANG LC_*\n+Subsystem\tsftp\t/usr/lib/openssh/sftp-server\ndiff --git a/utils/durability-test/startup.exp b/utils/durability-test/startup.exp\nnew file mode 100755\nindex 000000000000..540cfc0e4b8f\n--- /dev/null\n+++ b/utils/durability-test/startup.exp\n@@ -0,0 +1,23 @@\n+#!/usr/bin/expect -f\n+\n+# Wait enough (forever) until a long-time boot\n+set timeout -1\n+\n+spawn qemu-system-x86_64 \\\n+    -hda bionic-server-cloudimg-amd64.img \\\n+    -cpu qemu64,+ssse3,+sse4.1,+sse4.2,+popcnt -smp 8 \\\n+    -net nic -net user,hostfwd=tcp::11022-:22 \\\n+    -m 4096 -nographic\n+\n+expect \"login: \"\n+send \"root\\n\"\n+\n+expect \"Password: \"\n+send \"root\\n\"\n+\n+# Without it ssh is not working on guest machine for some reason\n+expect \"# \"\n+send \"dhclient && ssh-keygen -A && systemctl restart sshd.service\\n\"\n+\n+# Wait forever\n+expect \"########\"\n",
  "problem_statement": "Options to turn on fsync on write for some cases.\nThis feature was ordered by Yandex Cloud.\r\n\r\nOn decent SSD we can simply fsync every written data part. On slower devices we can support sync of large enough parts in background.\r\n\r\nNote that data parts are 100% immutable (in versions 20.3+). If data part was written it's never modified.\r\n\r\n1. MergeTree settings to do fsync of data part after background merge of sufficient size:\r\n`min_rows_to_sync_after_merge`\r\n`min_compressed_bytes_to_sync_after_merge`\r\n\r\nNote that we also have a setting `min_merge_bytes_to_use_direct_io` but it's only loosely related because it doesn't sync metadata.\r\n\r\n2. MergeTree settings to do fsync of data part after fetching from replica:\r\n`min_rows_to_sync_after_fetch`\r\n`min_compressed_bytes_to_sync_after_fetch`\r\n\r\nMaybe we should introduce a setting `min_fetch_bytes_to_use_direct_io` and set it to the same default value as `min_merge_bytes_to_use_direct_io`.\r\n\r\n3. Sync of merged data part on removal of any of source data parts.\r\n`min_rows_to_sync_before_removing_source`\r\n`min_compressed_bytes_to_sync_before_removing_source`\r\n\r\n4. Sync on INSERT. \r\nThe most dangerous option - simply issue fsync for every part files before returning from INSERT.\r\nFor ReplicatedMergeTree this step should be done after writing temporary data part on disk. And then, after renaming the directory, additional fsync command should be issued for directory.\r\n\r\n5. Options to control sync of directories. It may be or may not be needed depending on filesystem.\r\n\r\nReferences: http://danluu.com/file-consistency/\n",
  "hints_text": "> Sync of merged data part on removal of any of source data parts.\r\n\r\nThis is in question because fsync of a file that was written and closed is not guaranteed to work properly on Linux: https://stackoverflow.com/questions/37288453/calling-fsync2-after-close2\nJust to make it clear. If we suppose the most paranoid setup(and have no bugs):\r\n```\r\nmin_rows_to_sync_after_merge=1\r\nmin_rows_to_sync_before_removing_source=1\r\nsync_on_insert=1\r\n```\r\n+ using RAID under fs.\r\nWe can't lose data after answering OK to user in case of sudden power failure or single disk failure? \r\n\r\n",
  "created_at": "2020-06-25T16:57:29Z"
}