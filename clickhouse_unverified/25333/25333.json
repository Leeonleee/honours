{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 25333,
  "instance_id": "ClickHouse__ClickHouse-25333",
  "issue_numbers": [
    "6613"
  ],
  "base_commit": "bed2206ae91134a9422a3be3dfb8fe76fcf5f93a",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 17e4d27bbcdd..1e6450d76137 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -769,6 +769,7 @@ static constexpr UInt64 operator\"\"_GiB(unsigned long long value)\n     M(Bool, output_format_pretty_row_numbers, false, \"Add row numbers before each row for pretty output format\", 0) \\\n     M(Bool, insert_distributed_one_random_shard, false, \"If setting is enabled, inserting into distributed table will choose a random shard to write when there is no sharding key\", 0) \\\n     \\\n+    M(Bool, exact_rows_before_limit, false, \"When enabled, ClickHouse will provide exact value for rows_before_limit_at_least statistic, but with the cost that the data before limit will have to be read completely\", 0) \\\n     M(UInt64, cross_to_inner_join_rewrite, 1, \"Use inner join instead of comma/cross join if there're joining expressions in the WHERE section. Values: 0 - no rewrite, 1 - apply if possible for comma/cross, 2 - force rewrite all comma joins, cross - if possible\", 0) \\\n     \\\n     M(Bool, output_format_arrow_low_cardinality_as_dictionary, false, \"Enable output LowCardinality type as Dictionary Arrow type\", 0) \\\ndiff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 24bbaea7dcf6..de01115abec8 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -2712,11 +2712,14 @@ void InterpreterSelectQuery::executePreLimit(QueryPlan & query_plan, bool do_not\n             limit_offset = 0;\n         }\n \n-        auto limit = std::make_unique<LimitStep>(query_plan.getCurrentDataStream(), limit_length, limit_offset);\n+        const Settings & settings = context->getSettingsRef();\n+\n+        auto limit = std::make_unique<LimitStep>(query_plan.getCurrentDataStream(), limit_length, limit_offset, settings.exact_rows_before_limit);\n         if (do_not_skip_offset)\n             limit->setStepDescription(\"preliminary LIMIT (with OFFSET)\");\n         else\n             limit->setStepDescription(\"preliminary LIMIT (without OFFSET)\");\n+\n         query_plan.addStep(std::move(limit));\n     }\n }\n@@ -2778,7 +2781,8 @@ void InterpreterSelectQuery::executeLimit(QueryPlan & query_plan)\n           *  if there is WITH TOTALS and there is no ORDER BY, then read the data to the end,\n           *  otherwise TOTALS is counted according to incomplete data.\n           */\n-        bool always_read_till_end = false;\n+        const Settings & settings = context->getSettingsRef();\n+        bool always_read_till_end = settings.exact_rows_before_limit;\n \n         if (query.group_by_with_totals && !query.orderBy())\n             always_read_till_end = true;\ndiff --git a/src/Interpreters/InterpreterSelectWithUnionQuery.cpp b/src/Interpreters/InterpreterSelectWithUnionQuery.cpp\nindex b26226077609..87a182e70aeb 100644\n--- a/src/Interpreters/InterpreterSelectWithUnionQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectWithUnionQuery.cpp\n@@ -344,7 +344,7 @@ void InterpreterSelectWithUnionQuery::buildQueryPlan(QueryPlan & query_plan)\n     {\n         if (settings.limit > 0)\n         {\n-            auto limit = std::make_unique<LimitStep>(query_plan.getCurrentDataStream(), settings.limit, settings.offset);\n+            auto limit = std::make_unique<LimitStep>(query_plan.getCurrentDataStream(), settings.limit, settings.offset, settings.exact_rows_before_limit);\n             limit->setStepDescription(\"LIMIT OFFSET for SETTINGS\");\n             query_plan.addStep(std::move(limit));\n         }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01913_exact_rows_before_limit.reference b/tests/queries/0_stateless/01913_exact_rows_before_limit.reference\nnew file mode 100644\nindex 000000000000..af808adf83d5\n--- /dev/null\n+++ b/tests/queries/0_stateless/01913_exact_rows_before_limit.reference\n@@ -0,0 +1,72 @@\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"0\",\n+\t\t\t\"type\": \"UInt8\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0]\n+\t],\n+\n+\t\"rows\": 1,\n+\n+\t\"rows_before_limit_at_least\": 10000\n+}\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"0\",\n+\t\t\t\"type\": \"UInt8\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0]\n+\t],\n+\n+\t\"rows\": 1,\n+\n+\t\"rows_before_limit_at_least\": 20000\n+}\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"0\",\n+\t\t\t\"type\": \"UInt8\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0]\n+\t],\n+\n+\t\"rows\": 1,\n+\n+\t\"rows_before_limit_at_least\": 1\n+}\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"0\",\n+\t\t\t\"type\": \"UInt8\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0]\n+\t],\n+\n+\t\"rows\": 1,\n+\n+\t\"rows_before_limit_at_least\": 20000\n+}\ndiff --git a/tests/queries/0_stateless/01913_exact_rows_before_limit.sql b/tests/queries/0_stateless/01913_exact_rows_before_limit.sql\nnew file mode 100644\nindex 000000000000..f6b02d0a5103\n--- /dev/null\n+++ b/tests/queries/0_stateless/01913_exact_rows_before_limit.sql\n@@ -0,0 +1,17 @@\n+-- Tags: no-parallel\n+drop table if exists test_rows_compact_part;\n+create table test_rows_compact_part(f1 int,f2 int)  engine=MergeTree partition by f1 order by f2 settings min_bytes_for_wide_part=10485760;\n+insert into test_rows_compact_part select  0,arrayJoin(range(10000)) ;\n+insert into test_rows_compact_part select  1,arrayJoin(range(10000));\n+select 0 from test_rows_compact_part limit 1 FORMAT JSONCompact settings exact_rows_before_limit = 0,output_format_write_statistics = 0;\n+select 0 from test_rows_compact_part limit 1 FORMAT JSONCompact settings exact_rows_before_limit = 1, output_format_write_statistics = 0;\n+drop table if exists test_rows_compact_part;\n+\n+\n+drop table if exists test_rows_wide_part;\n+create table test_rows_wide_part(f1 int,f2 int)  engine=MergeTree partition by f1 order by f2 settings min_bytes_for_wide_part=0;\n+insert into test_rows_wide_part select  0,arrayJoin(range(10000)) ;\n+insert into test_rows_wide_part select  1,arrayJoin(range(10000));\n+select 0 from test_rows_wide_part limit 1 FORMAT JSONCompact settings exact_rows_before_limit = 0,output_format_write_statistics = 0;\n+select 0 from test_rows_wide_part limit 1 FORMAT JSONCompact settings exact_rows_before_limit = 1, output_format_write_statistics = 0;\n+drop table if exists test_rows_compact_part;\n\\ No newline at end of file\n",
  "problem_statement": "Allow to provide exact value instead of \"rows_before_limit_at_least\"\n**Describe the solution you'd like**\r\nA setting `exact_rows_before_limit` = 0|1. When enabled, `LimitBlockInputStream` will `always_read_till_end_` (we already have this feature in code) and the field `rows_before_limit_at_least` will have precise value.\r\n\n",
  "hints_text": "Hi Alexey,\r\n\r\nWhat's the status of this feature? We noted that when running limited queries with joins, the 'rows_before_limit_at_least' outputs a correct value. However, whenever running 'simple' queries on a single table the 'rows_before_limit_at_least' outputs a wrong number (smaller than the 'exact' value).\r\n\r\nIt looks like this feature will fix this. Could you tell me whenever you plan to implement this?\nThis feature can be easily implemented, but I don't have it in plans for my team.\ni want try this \n@MaxWk Thank you, this will be very appreciated!\n> Hi Alexey,\r\n> \r\n> What's the status of this feature? We noted that when running limited queries with joins, the 'rows_before_limit_at_least' outputs a correct value. However, whenever running 'simple' queries on a single table the 'rows_before_limit_at_least' outputs a wrong number (smaller than the 'exact' value).\r\n> \r\n> It looks like this feature will fix this. Could you tell me whenever you plan to implement this?\r\n\r\nHi lwolters ,can you give me some example to reproduce this",
  "created_at": "2021-06-16T13:01:21Z",
  "modified_files": [
    "src/Core/Settings.h",
    "src/Interpreters/InterpreterSelectQuery.cpp",
    "src/Interpreters/InterpreterSelectWithUnionQuery.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/01913_exact_rows_before_limit.reference",
    "b/tests/queries/0_stateless/01913_exact_rows_before_limit.sql"
  ]
}