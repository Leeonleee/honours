{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 21520,
  "instance_id": "ClickHouse__ClickHouse-21520",
  "issue_numbers": [
    "18583"
  ],
  "base_commit": "b2a6a8feb8b3610a32812568112800b86225981f",
  "patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 76b79a0b6c84..7c3571f11185 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -155,7 +155,6 @@ option(ENABLE_TESTS \"Provide unit_test_dbms target with Google.Test unit tests\"\n \n if (OS_LINUX AND NOT UNBUNDLED AND MAKE_STATIC_LIBRARIES AND NOT SPLIT_SHARED_LIBRARIES AND CMAKE_VERSION VERSION_GREATER \"3.9.0\")\n     # Only for Linux, x86_64.\n-    # Implies ${ENABLE_FASTMEMCPY}\n     option(GLIBC_COMPATIBILITY \"Enable compatibility with older glibc libraries.\" ON)\n elseif(GLIBC_COMPATIBILITY)\n     message (${RECONFIGURE_MESSAGE_LEVEL} \"Glibc compatibility cannot be enabled in current configuration\")\n@@ -536,7 +535,7 @@ macro (add_executable target)\n     # explicitly acquire and interpose malloc symbols by clickhouse_malloc\n     # if GLIBC_COMPATIBILITY is ON and ENABLE_THINLTO is on than provide memcpy symbol explicitly to neutrialize thinlto's libcall generation.\n     if (GLIBC_COMPATIBILITY AND ENABLE_THINLTO)\n-        _add_executable (${ARGV} $<TARGET_OBJECTS:clickhouse_malloc> $<TARGET_OBJECTS:clickhouse_memcpy>)\n+        _add_executable (${ARGV} $<TARGET_OBJECTS:clickhouse_malloc> $<TARGET_OBJECTS:memcpy>)\n     else ()\n         _add_executable (${ARGV} $<TARGET_OBJECTS:clickhouse_malloc>)\n     endif ()\ndiff --git a/base/common/CMakeLists.txt b/base/common/CMakeLists.txt\nindex cea52b443ddc..b4bf4f55466a 100644\n--- a/base/common/CMakeLists.txt\n+++ b/base/common/CMakeLists.txt\n@@ -74,7 +74,6 @@ target_link_libraries (common\n         ${CITYHASH_LIBRARIES}\n         boost::headers_only\n         boost::system\n-        FastMemcpy\n         Poco::Net\n         Poco::Net::SSL\n         Poco::Util\ndiff --git a/base/glibc-compatibility/CMakeLists.txt b/base/glibc-compatibility/CMakeLists.txt\nindex 684c6162941d..e785e2ab2ce4 100644\n--- a/base/glibc-compatibility/CMakeLists.txt\n+++ b/base/glibc-compatibility/CMakeLists.txt\n@@ -1,5 +1,8 @@\n if (GLIBC_COMPATIBILITY)\n-    set (ENABLE_FASTMEMCPY ON)\n+    add_subdirectory(memcpy)\n+    if(TARGET memcpy)\n+        set(MEMCPY_LIBRARY memcpy)\n+    endif()\n \n     enable_language(ASM)\n     include(CheckIncludeFile)\n@@ -27,13 +30,6 @@ if (GLIBC_COMPATIBILITY)\n         list(APPEND glibc_compatibility_sources musl/getentropy.c)\n     endif()\n \n-    if (NOT ARCH_ARM)\n-        # clickhouse_memcpy don't support ARCH_ARM, see https://github.com/ClickHouse/ClickHouse/issues/18951\n-        add_library (clickhouse_memcpy OBJECT\n-            ${ClickHouse_SOURCE_DIR}/contrib/FastMemcpy/memcpy_wrapper.c\n-        )\n-    endif()\n-\n     # Need to omit frame pointers to match the performance of glibc\n     set (CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fomit-frame-pointer\")\n \n@@ -51,15 +47,16 @@ if (GLIBC_COMPATIBILITY)\n         target_compile_options(glibc-compatibility PRIVATE -fPIC)\n     endif ()\n \n-    target_link_libraries(global-libs INTERFACE glibc-compatibility)\n+    target_link_libraries(global-libs INTERFACE glibc-compatibility ${MEMCPY_LIBRARY})\n \n     install(\n-        TARGETS glibc-compatibility\n+        TARGETS glibc-compatibility ${MEMCPY_LIBRARY}\n         EXPORT global\n         ARCHIVE DESTINATION lib\n     )\n \n     message (STATUS \"Some symbols from glibc will be replaced for compatibility\")\n+\n elseif (YANDEX_OFFICIAL_BUILD)\n     message (WARNING \"Option GLIBC_COMPATIBILITY must be turned on for production builds.\")\n endif ()\ndiff --git a/base/glibc-compatibility/memcpy/CMakeLists.txt b/base/glibc-compatibility/memcpy/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..133995d9b964\n--- /dev/null\n+++ b/base/glibc-compatibility/memcpy/CMakeLists.txt\n@@ -0,0 +1,8 @@\n+if (ARCH_AMD64)\n+    add_library(memcpy STATIC memcpy.cpp)\n+\n+    # We allow to include memcpy.h from user code for better inlining.\n+    target_include_directories(memcpy PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>)\n+\n+    target_compile_options(memcpy PRIVATE -fno-builtin-memcpy)\n+endif ()\ndiff --git a/base/glibc-compatibility/memcpy/memcpy.cpp b/base/glibc-compatibility/memcpy/memcpy.cpp\nnew file mode 100644\nindex 000000000000..ec43a2c36496\n--- /dev/null\n+++ b/base/glibc-compatibility/memcpy/memcpy.cpp\n@@ -0,0 +1,6 @@\n+#include \"memcpy.h\"\n+\n+extern \"C\" void * memcpy(void * __restrict dst, const void * __restrict src, size_t size)\n+{\n+    return inline_memcpy(dst, src, size);\n+}\ndiff --git a/base/glibc-compatibility/memcpy/memcpy.h b/base/glibc-compatibility/memcpy/memcpy.h\nnew file mode 100644\nindex 000000000000..f9f81bcb0fe3\n--- /dev/null\n+++ b/base/glibc-compatibility/memcpy/memcpy.h\n@@ -0,0 +1,217 @@\n+#include <cstddef>\n+\n+#include <emmintrin.h>\n+\n+\n+/** Custom memcpy implementation for ClickHouse.\n+  * It has the following benefits over using glibc's implementation:\n+  * 1. Avoiding dependency on specific version of glibc's symbol, like memcpy@@GLIBC_2.14 for portability.\n+  * 2. Avoiding indirect call via PLT due to shared linking, that can be less efficient.\n+  * 3. It's possible to include this header and call inline_memcpy directly for better inlining or interprocedural analysis.\n+  * 4. Better results on our performance tests on current CPUs: up to 25% on some queries and up to 0.7%..1% in average across all queries.\n+  *\n+  * Writing our own memcpy is extremely difficult for the following reasons:\n+  * 1. The optimal variant depends on the specific CPU model.\n+  * 2. The optimal variant depends on the distribution of size arguments.\n+  * 3. It depends on the number of threads copying data concurrently.\n+  * 4. It also depends on how the calling code is using the copied data and how the different memcpy calls are related to each other.\n+  * Due to vast range of scenarios it makes proper testing especially difficult.\n+  * When writing our own memcpy there is a risk to overoptimize it\n+  * on non-representative microbenchmarks while making real-world use cases actually worse.\n+  *\n+  * Most of the benchmarks for memcpy on the internet are wrong.\n+  *\n+  * Let's look at the details:\n+  *\n+  * For small size, the order of branches in code is important.\n+  * There are variants with specific order of branches (like here or in glibc)\n+  * or with jump table (in asm code see example from Cosmopolitan libc:\n+  * https://github.com/jart/cosmopolitan/blob/de09bec215675e9b0beb722df89c6f794da74f3f/libc/nexgen32e/memcpy.S#L61)\n+  * or with Duff device in C (see https://github.com/skywind3000/FastMemcpy/)\n+  *\n+  * It's also important how to copy uneven sizes.\n+  * Almost every implementation, including this, is using two overlapping movs.\n+  *\n+  * It is important to disable -ftree-loop-distribute-patterns when compiling memcpy implementation,\n+  * otherwise the compiler can replace internal loops to a call to memcpy that will lead to infinite recursion.\n+  *\n+  * For larger sizes it's important to choose the instructions used:\n+  * - SSE or AVX or AVX-512;\n+  * - rep movsb;\n+  * Performance will depend on the size threshold, on the CPU model, on the \"erms\" flag\n+  * (\"Enhansed Rep MovS\" - it indicates that performance of \"rep movsb\" is decent for large sizes)\n+  * https://stackoverflow.com/questions/43343231/enhanced-rep-movsb-for-memcpy\n+  *\n+  * Using AVX-512 can be bad due to throttling.\n+  * Using AVX can be bad if most code is using SSE due to switching penalty\n+  * (it also depends on the usage of \"vzeroupper\" instruction).\n+  * But in some cases AVX gives a win.\n+  *\n+  * It also depends on how many times the loop will be unrolled.\n+  * We are unrolling the loop 8 times (by the number of available registers), but it not always the best.\n+  *\n+  * It also depends on the usage of aligned or unaligned loads/stores.\n+  * We are using unaligned loads and aligned stores.\n+  *\n+  * It also depends on the usage of prefetch instructions. It makes sense on some Intel CPUs but can slow down performance on AMD.\n+  * Setting up correct offset for prefetching is non-obvious.\n+  *\n+  * Non-temporary (cache bypassing) stores can be used for very large sizes (more than a half of L3 cache).\n+  * But the exact threshold is unclear - when doing memcpy from multiple threads the optimal threshold can be lower,\n+  * because L3 cache is shared (and L2 cache is partially shared).\n+  *\n+  * Very large size of memcpy typically indicates suboptimal (not cache friendly) algorithms in code or unrealistic scenarios,\n+  * so we don't pay attention to using non-temporary stores.\n+  *\n+  * On recent Intel CPUs, the presence of \"erms\" makes \"rep movsb\" the most benefitial,\n+  * even comparing to non-temporary aligned unrolled stores even with the most wide registers.\n+  *\n+  * memcpy can be written in asm, C or C++. The latter can also use inline asm.\n+  * The asm implementation can be better to make sure that compiler won't make the code worse,\n+  * to ensure the order of branches, the code layout, the usage of all required registers.\n+  * But if it is located in separate translation unit, inlining will not be possible\n+  * (inline asm can be used to overcome this limitation).\n+  * Sometimes C or C++ code can be further optimized by compiler.\n+  * For example, clang is capable replacing SSE intrinsics to AVX code if -mavx is used.\n+  *\n+  * Please note that compiler can replace plain code to memcpy and vice versa.\n+  * - memcpy with compile-time known small size is replaced to simple instructions without a call to memcpy;\n+  *   it is controlled by -fbuiltin-memcpy and can be manually ensured by calling __builtin_memcpy.\n+  *   This is often used to implement unaligned load/store without undefined behaviour in C++.\n+  * - a loop with copying bytes can be recognized and replaced by a call to memcpy;\n+  *   it is controlled by -ftree-loop-distribute-patterns.\n+  * - also note that a loop with copying bytes can be unrolled, peeled and vectorized that will give you\n+  *   inline code somewhat similar to a decent implementation of memcpy.\n+  *\n+  * This description is up to date as of Mar 2021.\n+  *\n+  * How to test the memcpy implementation for performance:\n+  * 1. Test on real production workload.\n+  * 2. For synthetic test, see utils/memcpy-bench, but make sure you will do the best to exhaust the wide range of scenarios.\n+  *\n+  * TODO: Add self-tuning memcpy with bayesian bandits algorithm for large sizes.\n+  * See https://habr.com/en/company/yandex/blog/457612/\n+  */\n+\n+\n+static inline void * inline_memcpy(void * __restrict dst_, const void * __restrict src_, size_t size)\n+{\n+    /// We will use pointer arithmetic, so char pointer will be used.\n+    /// Note that __restrict makes sense (otherwise compiler will reload data from memory\n+    /// instead of using the value of registers due to possible aliasing).\n+    char * __restrict dst = reinterpret_cast<char * __restrict>(dst_);\n+    const char * __restrict src = reinterpret_cast<const char * __restrict>(src_);\n+\n+    /// Standard memcpy returns the original value of dst. It is rarely used but we have to do it.\n+    /// If you use memcpy with small but non-constant sizes, you can call inline_memcpy directly\n+    /// for inlining and removing this single instruction.\n+    void * ret = dst;\n+\n+tail:\n+    /// Small sizes and tails after the loop for large sizes.\n+    /// The order of branches is important but in fact the optimal order depends on the distribution of sizes in your application.\n+    /// This order of branches is from the disassembly of glibc's code.\n+    /// We copy chunks of possibly uneven size with two overlapping movs.\n+    /// Example: to copy 5 bytes [0, 1, 2, 3, 4] we will copy tail [1, 2, 3, 4] first and then head [0, 1, 2, 3].\n+    if (size <= 16)\n+    {\n+        if (size >= 8)\n+        {\n+            /// Chunks of 8..16 bytes.\n+            __builtin_memcpy(dst + size - 8, src + size - 8, 8);\n+            __builtin_memcpy(dst, src, 8);\n+        }\n+        else if (size >= 4)\n+        {\n+            /// Chunks of 4..7 bytes.\n+            __builtin_memcpy(dst + size - 4, src + size - 4, 4);\n+            __builtin_memcpy(dst, src, 4);\n+        }\n+        else if (size >= 2)\n+        {\n+            /// Chunks of 2..3 bytes.\n+            __builtin_memcpy(dst + size - 2, src + size - 2, 2);\n+            __builtin_memcpy(dst, src, 2);\n+        }\n+        else if (size >= 1)\n+        {\n+            /// A single byte.\n+            *dst = *src;\n+        }\n+        /// No bytes remaining.\n+    }\n+    else\n+    {\n+        /// Medium and large sizes.\n+        if (size <= 128)\n+        {\n+            /// Medium size, not enough for full loop unrolling.\n+\n+            /// We will copy the last 16 bytes.\n+            _mm_storeu_si128(reinterpret_cast<__m128i *>(dst + size - 16), _mm_loadu_si128(reinterpret_cast<const __m128i *>(src + size - 16)));\n+\n+            /// Then we will copy every 16 bytes from the beginning in a loop.\n+            /// The last loop iteration will possibly overwrite some part of already copied last 16 bytes.\n+            /// This is Ok, similar to the code for small sizes above.\n+            while (size > 16)\n+            {\n+                _mm_storeu_si128(reinterpret_cast<__m128i *>(dst), _mm_loadu_si128(reinterpret_cast<const __m128i *>(src)));\n+                dst += 16;\n+                src += 16;\n+                size -= 16;\n+            }\n+        }\n+        else\n+        {\n+            /// Large size with fully unrolled loop.\n+\n+            /// Align destination to 16 bytes boundary.\n+            size_t padding = (16 - (reinterpret_cast<size_t>(dst) & 15)) & 15;\n+\n+            /// If not aligned - we will copy first 16 bytes with unaligned stores.\n+            if (padding > 0)\n+            {\n+                __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+                _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+                dst += padding;\n+                src += padding;\n+                size -= padding;\n+            }\n+\n+            /// Aligned unrolled copy. We will use all available SSE registers.\n+            /// It's not possible to have both src and dst aligned.\n+            /// So, we will use aligned stores and unaligned loads.\n+            __m128i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+            while (size >= 128)\n+            {\n+                c0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 0);\n+                c1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 1);\n+                c2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 2);\n+                c3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 3);\n+                c4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 4);\n+                c5 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 5);\n+                c6 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 6);\n+                c7 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 7);\n+                src += 128;\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 0), c0);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 1), c1);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 2), c2);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 3), c3);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 4), c4);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 5), c5);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 6), c6);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 7), c7);\n+                dst += 128;\n+\n+                size -= 128;\n+            }\n+\n+            /// The latest remaining 0..127 bytes will be processed as usual.\n+            goto tail;\n+        }\n+    }\n+\n+    return ret;\n+}\n+\ndiff --git a/contrib/CMakeLists.txt b/contrib/CMakeLists.txt\nindex bf4bf5eb472b..6a56b4cc7334 100644\n--- a/contrib/CMakeLists.txt\n+++ b/contrib/CMakeLists.txt\n@@ -38,7 +38,6 @@ add_subdirectory (boost-cmake)\n add_subdirectory (cctz-cmake)\n add_subdirectory (consistent-hashing)\n add_subdirectory (dragonbox-cmake)\n-add_subdirectory (FastMemcpy)\n add_subdirectory (hyperscan-cmake)\n add_subdirectory (jemalloc-cmake)\n add_subdirectory (libcpuid-cmake)\ndiff --git a/contrib/FastMemcpy/CMakeLists.txt b/contrib/FastMemcpy/CMakeLists.txt\ndeleted file mode 100644\nindex 8efe6d45dff9..000000000000\n--- a/contrib/FastMemcpy/CMakeLists.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-option (ENABLE_FASTMEMCPY \"Enable FastMemcpy library (only internal)\" ${ENABLE_LIBRARIES})\n-\n-if (NOT OS_LINUX OR ARCH_AARCH64)\n-    set (ENABLE_FASTMEMCPY OFF)\n-endif ()\n-\n-if (ENABLE_FASTMEMCPY)\n-    set (LIBRARY_DIR ${ClickHouse_SOURCE_DIR}/contrib/FastMemcpy)\n-\n-    set (SRCS\n-        ${LIBRARY_DIR}/FastMemcpy.c\n-\n-        memcpy_wrapper.c\n-    )\n-\n-    add_library (FastMemcpy ${SRCS})\n-    target_include_directories (FastMemcpy PUBLIC ${LIBRARY_DIR})\n-\n-    target_compile_definitions(FastMemcpy PUBLIC USE_FASTMEMCPY=1)\n-\n-    message (STATUS \"Using FastMemcpy\")\n-else ()\n-    add_library (FastMemcpy INTERFACE)\n-\n-    target_compile_definitions(FastMemcpy INTERFACE USE_FASTMEMCPY=0)\n-\n-    message (STATUS \"Not using FastMemcpy\")\n-endif ()\ndiff --git a/contrib/FastMemcpy/FastMemcpy.c b/contrib/FastMemcpy/FastMemcpy.c\ndeleted file mode 100644\nindex 5021bcc7d165..000000000000\n--- a/contrib/FastMemcpy/FastMemcpy.c\n+++ /dev/null\n@@ -1,220 +0,0 @@\n-//=====================================================================\r\n-//\r\n-// FastMemcpy.c - skywind3000@163.com, 2015\r\n-//\r\n-// feature:\r\n-// 50% speed up in avg. vs standard memcpy (tested in vc2012/gcc4.9)\r\n-//\r\n-//=====================================================================\r\n-#include <stdio.h>\r\n-#include <stdlib.h>\r\n-#include <string.h>\r\n-#include <time.h>\r\n-\r\n-#if (defined(_WIN32) || defined(WIN32))\r\n-#include <windows.h>\r\n-#include <mmsystem.h>\r\n-#ifdef _MSC_VER\r\n-#pragma comment(lib, \"winmm.lib\")\r\n-#endif\r\n-#elif defined(__unix)\r\n-#include <sys/time.h>\r\n-#include <unistd.h>\r\n-#else\r\n-#error it can only be compiled under windows or unix\r\n-#endif\r\n-\r\n-#include \"FastMemcpy.h\"\r\n-\r\n-unsigned int gettime()\r\n-{\r\n-\t#if (defined(_WIN32) || defined(WIN32))\r\n-\treturn timeGetTime();\r\n-\t#else\r\n-\tstatic struct timezone tz={ 0,0 };\r\n-\tstruct timeval time;\r\n-\tgettimeofday(&time,&tz);\r\n-\treturn (time.tv_sec * 1000 + time.tv_usec / 1000);\r\n-\t#endif\r\n-}\r\n-\r\n-void sleepms(unsigned int millisec)\r\n-{\r\n-#if defined(_WIN32) || defined(WIN32)\r\n-\tSleep(millisec);\r\n-#else\r\n-\tusleep(millisec * 1000);\r\n-#endif\r\n-}\r\n-\r\n-\r\n-void benchmark(int dstalign, int srcalign, size_t size, int times)\r\n-{\r\n-\tchar *DATA1 = (char*)malloc(size + 64);\r\n-\tchar *DATA2 = (char*)malloc(size + 64);\r\n-\tsize_t LINEAR1 = ((size_t)DATA1);\r\n-\tsize_t LINEAR2 = ((size_t)DATA2);\r\n-\tchar *ALIGN1 = (char*)(((64 - (LINEAR1 & 63)) & 63) + LINEAR1);\r\n-\tchar *ALIGN2 = (char*)(((64 - (LINEAR2 & 63)) & 63) + LINEAR2);\r\n-\tchar *dst = (dstalign)? ALIGN1 : (ALIGN1 + 1);\r\n-\tchar *src = (srcalign)? ALIGN2 : (ALIGN2 + 3);\r\n-\tunsigned int t1, t2;\r\n-\tint k;\r\n-\t\r\n-\tsleepms(100);\r\n-\tt1 = gettime();\r\n-\tfor (k = times; k > 0; k--) {\r\n-\t\tmemcpy(dst, src, size);\r\n-\t}\r\n-\tt1 = gettime() - t1;\r\n-\tsleepms(100);\r\n-\tt2 = gettime();\r\n-\tfor (k = times; k > 0; k--) {\r\n-\t\tmemcpy_fast(dst, src, size);\r\n-\t}\r\n-\tt2 = gettime() - t2;\r\n-\r\n-\tfree(DATA1);\r\n-\tfree(DATA2);\r\n-\r\n-\tprintf(\"result(dst %s, src %s): memcpy_fast=%dms memcpy=%d ms\\n\",  \r\n-\t\tdstalign? \"aligned\" : \"unalign\", \r\n-\t\tsrcalign? \"aligned\" : \"unalign\", (int)t2, (int)t1);\r\n-}\r\n-\r\n-\r\n-void bench(int copysize, int times)\r\n-{\r\n-\tprintf(\"benchmark(size=%d bytes, times=%d):\\n\", copysize, times);\r\n-\tbenchmark(1, 1, copysize, times);\r\n-\tbenchmark(1, 0, copysize, times);\r\n-\tbenchmark(0, 1, copysize, times);\r\n-\tbenchmark(0, 0, copysize, times);\r\n-\tprintf(\"\\n\");\r\n-}\r\n-\r\n-\r\n-void random_bench(int maxsize, int times)\r\n-{\r\n-\tstatic char A[11 * 1024 * 1024 + 2];\r\n-\tstatic char B[11 * 1024 * 1024 + 2];\r\n-\tstatic int random_offsets[0x10000];\r\n-\tstatic int random_sizes[0x8000];\r\n-\tunsigned int i, p1, p2;\r\n-\tunsigned int t1, t2;\r\n-\tfor (i = 0; i < 0x10000; i++) {\t// generate random offsets\r\n-\t\trandom_offsets[i] = rand() % (10 * 1024 * 1024 + 1);\r\n-\t}\r\n-\tfor (i = 0; i < 0x8000; i++) {\t// generate random sizes\r\n-\t\trandom_sizes[i] = 1 + rand() % maxsize;\r\n-\t}\r\n-\tsleepms(100);\r\n-\tt1 = gettime();\r\n-\tfor (p1 = 0, p2 = 0, i = 0; i < times; i++) {\r\n-\t\tint offset1 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint offset2 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint size = random_sizes[(p2++) & 0x7fff];\r\n-\t\tmemcpy(A + offset1, B + offset2, size);\r\n-\t}\r\n-\tt1 = gettime() - t1;\r\n-\tsleepms(100);\r\n-\tt2 = gettime();\r\n-\tfor (p1 = 0, p2 = 0, i = 0; i < times; i++) {\r\n-\t\tint offset1 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint offset2 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint size = random_sizes[(p2++) & 0x7fff];\r\n-\t\tmemcpy_fast(A + offset1, B + offset2, size);\r\n-\t}\r\n-\tt2 = gettime() - t2;\r\n-\tprintf(\"benchmark random access:\\n\");\r\n-\tprintf(\"memcpy_fast=%dms memcpy=%dms\\n\\n\", (int)t2, (int)t1);\r\n-}\r\n-\r\n-\r\n-#ifdef _MSC_VER\r\n-#pragma comment(lib, \"winmm.lib\")\r\n-#endif\r\n-\r\n-int main(void)\r\n-{\r\n-\tbench(32, 0x1000000);\r\n-\tbench(64, 0x1000000);\r\n-\tbench(512, 0x800000);\r\n-\tbench(1024, 0x400000);\r\n-\tbench(4096, 0x80000);\r\n-\tbench(8192, 0x40000);\r\n-\tbench(1024 * 1024 * 1, 0x800);\r\n-\tbench(1024 * 1024 * 4, 0x200);\r\n-\tbench(1024 * 1024 * 8, 0x100);\r\n-\t\r\n-\trandom_bench(2048, 8000000);\r\n-\r\n-\treturn 0;\r\n-}\r\n-\r\n-\r\n-\r\n-\r\n-/*\r\n-benchmark(size=32 bytes, times=16777216):\r\n-result(dst aligned, src aligned): memcpy_fast=78ms memcpy=260 ms\r\n-result(dst aligned, src unalign): memcpy_fast=78ms memcpy=250 ms\r\n-result(dst unalign, src aligned): memcpy_fast=78ms memcpy=266 ms\r\n-result(dst unalign, src unalign): memcpy_fast=78ms memcpy=234 ms\r\n-\r\n-benchmark(size=64 bytes, times=16777216):\r\n-result(dst aligned, src aligned): memcpy_fast=109ms memcpy=281 ms\r\n-result(dst aligned, src unalign): memcpy_fast=109ms memcpy=328 ms\r\n-result(dst unalign, src aligned): memcpy_fast=109ms memcpy=343 ms\r\n-result(dst unalign, src unalign): memcpy_fast=93ms memcpy=344 ms\r\n-\r\n-benchmark(size=512 bytes, times=8388608):\r\n-result(dst aligned, src aligned): memcpy_fast=125ms memcpy=218 ms\r\n-result(dst aligned, src unalign): memcpy_fast=156ms memcpy=484 ms\r\n-result(dst unalign, src aligned): memcpy_fast=172ms memcpy=546 ms\r\n-result(dst unalign, src unalign): memcpy_fast=172ms memcpy=515 ms\r\n-\r\n-benchmark(size=1024 bytes, times=4194304):\r\n-result(dst aligned, src aligned): memcpy_fast=109ms memcpy=172 ms\r\n-result(dst aligned, src unalign): memcpy_fast=187ms memcpy=453 ms\r\n-result(dst unalign, src aligned): memcpy_fast=172ms memcpy=437 ms\r\n-result(dst unalign, src unalign): memcpy_fast=156ms memcpy=452 ms\r\n-\r\n-benchmark(size=4096 bytes, times=524288):\r\n-result(dst aligned, src aligned): memcpy_fast=62ms memcpy=78 ms\r\n-result(dst aligned, src unalign): memcpy_fast=109ms memcpy=202 ms\r\n-result(dst unalign, src aligned): memcpy_fast=94ms memcpy=203 ms\r\n-result(dst unalign, src unalign): memcpy_fast=110ms memcpy=218 ms\r\n-\r\n-benchmark(size=8192 bytes, times=262144):\r\n-result(dst aligned, src aligned): memcpy_fast=62ms memcpy=78 ms\r\n-result(dst aligned, src unalign): memcpy_fast=78ms memcpy=202 ms\r\n-result(dst unalign, src aligned): memcpy_fast=78ms memcpy=203 ms\r\n-result(dst unalign, src unalign): memcpy_fast=94ms memcpy=203 ms\r\n-\r\n-benchmark(size=1048576 bytes, times=2048):\r\n-result(dst aligned, src aligned): memcpy_fast=203ms memcpy=191 ms\r\n-result(dst aligned, src unalign): memcpy_fast=219ms memcpy=281 ms\r\n-result(dst unalign, src aligned): memcpy_fast=218ms memcpy=328 ms\r\n-result(dst unalign, src unalign): memcpy_fast=218ms memcpy=312 ms\r\n-\r\n-benchmark(size=4194304 bytes, times=512):\r\n-result(dst aligned, src aligned): memcpy_fast=312ms memcpy=406 ms\r\n-result(dst aligned, src unalign): memcpy_fast=296ms memcpy=421 ms\r\n-result(dst unalign, src aligned): memcpy_fast=312ms memcpy=468 ms\r\n-result(dst unalign, src unalign): memcpy_fast=297ms memcpy=452 ms\r\n-\r\n-benchmark(size=8388608 bytes, times=256):\r\n-result(dst aligned, src aligned): memcpy_fast=281ms memcpy=452 ms\r\n-result(dst aligned, src unalign): memcpy_fast=280ms memcpy=468 ms\r\n-result(dst unalign, src aligned): memcpy_fast=298ms memcpy=514 ms\r\n-result(dst unalign, src unalign): memcpy_fast=344ms memcpy=472 ms\r\n-\r\n-benchmark random access:\r\n-memcpy_fast=515ms memcpy=1014ms\r\n-\r\n-*/\r\n-\r\n-\r\n-\r\n-\r\ndiff --git a/contrib/FastMemcpy/FastMemcpy.h b/contrib/FastMemcpy/FastMemcpy.h\ndeleted file mode 100644\nindex 5dcbfcf16566..000000000000\n--- a/contrib/FastMemcpy/FastMemcpy.h\n+++ /dev/null\n@@ -1,694 +0,0 @@\n-//=====================================================================\r\n-//\r\n-// FastMemcpy.c - skywind3000@163.com, 2015\r\n-//\r\n-// feature:\r\n-// 50% speed up in avg. vs standard memcpy (tested in vc2012/gcc5.1)\r\n-//\r\n-//=====================================================================\r\n-#ifndef __FAST_MEMCPY_H__\r\n-#define __FAST_MEMCPY_H__\r\n-\r\n-#include <stddef.h>\r\n-#include <stdint.h>\r\n-#include <emmintrin.h>\r\n-\r\n-\r\n-//---------------------------------------------------------------------\r\n-// force inline for compilers\r\n-//---------------------------------------------------------------------\r\n-#ifndef INLINE\r\n-#ifdef __GNUC__\r\n-#if (__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 1))\r\n-    #define INLINE         __inline__ __attribute__((always_inline))\r\n-#else\r\n-    #define INLINE         __inline__\r\n-#endif\r\n-#elif defined(_MSC_VER)\r\n-\t#define INLINE __forceinline\r\n-#elif (defined(__BORLANDC__) || defined(__WATCOMC__))\r\n-    #define INLINE __inline\r\n-#else\r\n-    #define INLINE\r\n-#endif\r\n-#endif\r\n-\r\n-typedef __attribute__((__aligned__(1))) uint16_t uint16_unaligned_t;\r\n-typedef __attribute__((__aligned__(1))) uint32_t uint32_unaligned_t;\r\n-typedef __attribute__((__aligned__(1))) uint64_t uint64_unaligned_t;\r\n-\r\n-//---------------------------------------------------------------------\r\n-// fast copy for different sizes\r\n-//---------------------------------------------------------------------\r\n-static INLINE void memcpy_sse2_16(void *dst, const void *src) {\r\n-\t__m128i m0 = _mm_loadu_si128(((const __m128i*)src) + 0);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 0, m0);\r\n-}\r\n-\r\n-static INLINE void memcpy_sse2_32(void *dst, const void *src) {\r\n-\t__m128i m0 = _mm_loadu_si128(((const __m128i*)src) + 0);\r\n-\t__m128i m1 = _mm_loadu_si128(((const __m128i*)src) + 1);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 0, m0);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 1, m1);\r\n-}\r\n-\r\n-static INLINE void memcpy_sse2_64(void *dst, const void *src) {\r\n-\t__m128i m0 = _mm_loadu_si128(((const __m128i*)src) + 0);\r\n-\t__m128i m1 = _mm_loadu_si128(((const __m128i*)src) + 1);\r\n-\t__m128i m2 = _mm_loadu_si128(((const __m128i*)src) + 2);\r\n-\t__m128i m3 = _mm_loadu_si128(((const __m128i*)src) + 3);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 0, m0);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 1, m1);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 2, m2);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 3, m3);\r\n-}\r\n-\r\n-static INLINE void memcpy_sse2_128(void *dst, const void *src) {\r\n-\t__m128i m0 = _mm_loadu_si128(((const __m128i*)src) + 0);\r\n-\t__m128i m1 = _mm_loadu_si128(((const __m128i*)src) + 1);\r\n-\t__m128i m2 = _mm_loadu_si128(((const __m128i*)src) + 2);\r\n-\t__m128i m3 = _mm_loadu_si128(((const __m128i*)src) + 3);\r\n-\t__m128i m4 = _mm_loadu_si128(((const __m128i*)src) + 4);\r\n-\t__m128i m5 = _mm_loadu_si128(((const __m128i*)src) + 5);\r\n-\t__m128i m6 = _mm_loadu_si128(((const __m128i*)src) + 6);\r\n-\t__m128i m7 = _mm_loadu_si128(((const __m128i*)src) + 7);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 0, m0);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 1, m1);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 2, m2);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 3, m3);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 4, m4);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 5, m5);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 6, m6);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 7, m7);\r\n-}\r\n-\r\n-\r\n-//---------------------------------------------------------------------\r\n-// tiny memory copy with jump table optimized\r\n-//---------------------------------------------------------------------\r\n-/// Attribute is used to avoid an error with undefined behaviour sanitizer\r\n-/// ../contrib/FastMemcpy/FastMemcpy.h:91:56: runtime error: applying zero offset to null pointer\r\n-/// Found by 01307_orc_output_format.sh, cause - ORCBlockInputFormat and external ORC library.\r\n-__attribute__((__no_sanitize__(\"undefined\"))) static INLINE void *memcpy_tiny(void *dst, const void *src, size_t size) {\r\n-\tunsigned char *dd = ((unsigned char*)dst) + size;\r\n-\tconst unsigned char *ss = ((const unsigned char*)src) + size;\r\n-\r\n-\tswitch (size) {\r\n-\tcase 64:\r\n-\t\tmemcpy_sse2_64(dd - 64, ss - 64);\r\n-\tcase 0:\r\n-\t\tbreak;\r\n-\r\n-\tcase 65:\r\n-\t\tmemcpy_sse2_64(dd - 65, ss - 65);\r\n-\tcase 1:\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 66:\r\n-\t\tmemcpy_sse2_64(dd - 66, ss - 66);\r\n-\tcase 2:\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 67:\r\n-\t\tmemcpy_sse2_64(dd - 67, ss - 67);\r\n-\tcase 3:\r\n-\t\t*((uint16_unaligned_t*)(dd - 3)) = *((uint16_unaligned_t*)(ss - 3));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 68:\r\n-\t\tmemcpy_sse2_64(dd - 68, ss - 68);\r\n-\tcase 4:\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 69:\r\n-\t\tmemcpy_sse2_64(dd - 69, ss - 69);\r\n-\tcase 5:\r\n-\t\t*((uint32_unaligned_t*)(dd - 5)) = *((uint32_unaligned_t*)(ss - 5));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 70:\r\n-\t\tmemcpy_sse2_64(dd - 70, ss - 70);\r\n-\tcase 6:\r\n-\t\t*((uint32_unaligned_t*)(dd - 6)) = *((uint32_unaligned_t*)(ss - 6));\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 71:\r\n-\t\tmemcpy_sse2_64(dd - 71, ss - 71);\r\n-\tcase 7:\r\n-\t\t*((uint32_unaligned_t*)(dd - 7)) = *((uint32_unaligned_t*)(ss - 7));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 72:\r\n-\t\tmemcpy_sse2_64(dd - 72, ss - 72);\r\n-\tcase 8:\r\n-\t\t*((uint64_unaligned_t*)(dd - 8)) = *((uint64_unaligned_t*)(ss - 8));\r\n-\t\tbreak;\r\n-\r\n-\tcase 73:\r\n-\t\tmemcpy_sse2_64(dd - 73, ss - 73);\r\n-\tcase 9:\r\n-\t\t*((uint64_unaligned_t*)(dd - 9)) = *((uint64_unaligned_t*)(ss - 9));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 74:\r\n-\t\tmemcpy_sse2_64(dd - 74, ss - 74);\r\n-\tcase 10:\r\n-\t\t*((uint64_unaligned_t*)(dd - 10)) = *((uint64_unaligned_t*)(ss - 10));\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 75:\r\n-\t\tmemcpy_sse2_64(dd - 75, ss - 75);\r\n-\tcase 11:\r\n-\t\t*((uint64_unaligned_t*)(dd - 11)) = *((uint64_unaligned_t*)(ss - 11));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 76:\r\n-\t\tmemcpy_sse2_64(dd - 76, ss - 76);\r\n-\tcase 12:\r\n-\t\t*((uint64_unaligned_t*)(dd - 12)) = *((uint64_unaligned_t*)(ss - 12));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 77:\r\n-\t\tmemcpy_sse2_64(dd - 77, ss - 77);\r\n-\tcase 13:\r\n-\t\t*((uint64_unaligned_t*)(dd - 13)) = *((uint64_unaligned_t*)(ss - 13));\r\n-\t\t*((uint32_unaligned_t*)(dd - 5)) = *((uint32_unaligned_t*)(ss - 5));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 78:\r\n-\t\tmemcpy_sse2_64(dd - 78, ss - 78);\r\n-\tcase 14:\r\n-\t\t*((uint64_unaligned_t*)(dd - 14)) = *((uint64_unaligned_t*)(ss - 14));\r\n-\t\t*((uint64_unaligned_t*)(dd - 8)) = *((uint64_unaligned_t*)(ss - 8));\r\n-\t\tbreak;\r\n-\r\n-\tcase 79:\r\n-\t\tmemcpy_sse2_64(dd - 79, ss - 79);\r\n-\tcase 15:\r\n-\t\t*((uint64_unaligned_t*)(dd - 15)) = *((uint64_unaligned_t*)(ss - 15));\r\n-\t\t*((uint64_unaligned_t*)(dd - 8)) = *((uint64_unaligned_t*)(ss - 8));\r\n-\t\tbreak;\r\n-\r\n-\tcase 80:\r\n-\t\tmemcpy_sse2_64(dd - 80, ss - 80);\r\n-\tcase 16:\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 81:\r\n-\t\tmemcpy_sse2_64(dd - 81, ss - 81);\r\n-\tcase 17:\r\n-\t\tmemcpy_sse2_16(dd - 17, ss - 17);\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 82:\r\n-\t\tmemcpy_sse2_64(dd - 82, ss - 82);\r\n-\tcase 18:\r\n-\t\tmemcpy_sse2_16(dd - 18, ss - 18);\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 83:\r\n-\t\tmemcpy_sse2_64(dd - 83, ss - 83);\r\n-\tcase 19:\r\n-\t\tmemcpy_sse2_16(dd - 19, ss - 19);\r\n-\t\t*((uint16_unaligned_t*)(dd - 3)) = *((uint16_unaligned_t*)(ss - 3));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 84:\r\n-\t\tmemcpy_sse2_64(dd - 84, ss - 84);\r\n-\tcase 20:\r\n-\t\tmemcpy_sse2_16(dd - 20, ss - 20);\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 85:\r\n-\t\tmemcpy_sse2_64(dd - 85, ss - 85);\r\n-\tcase 21:\r\n-\t\tmemcpy_sse2_16(dd - 21, ss - 21);\r\n-\t\t*((uint32_unaligned_t*)(dd - 5)) = *((uint32_unaligned_t*)(ss - 5));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 86:\r\n-\t\tmemcpy_sse2_64(dd - 86, ss - 86);\r\n-\tcase 22:\r\n-\t\tmemcpy_sse2_16(dd - 22, ss - 22);\r\n-\t\t*((uint32_unaligned_t*)(dd - 6)) = *((uint32_unaligned_t*)(ss - 6));\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 87:\r\n-\t\tmemcpy_sse2_64(dd - 87, ss - 87);\r\n-\tcase 23:\r\n-\t\tmemcpy_sse2_16(dd - 23, ss - 23);\r\n-\t\t*((uint32_unaligned_t*)(dd - 7)) = *((uint32_unaligned_t*)(ss - 7));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 88:\r\n-\t\tmemcpy_sse2_64(dd - 88, ss - 88);\r\n-\tcase 24:\r\n-\t\tmemcpy_sse2_16(dd - 24, ss - 24);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 89:\r\n-\t\tmemcpy_sse2_64(dd - 89, ss - 89);\r\n-\tcase 25:\r\n-\t\tmemcpy_sse2_16(dd - 25, ss - 25);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 90:\r\n-\t\tmemcpy_sse2_64(dd - 90, ss - 90);\r\n-\tcase 26:\r\n-\t\tmemcpy_sse2_16(dd - 26, ss - 26);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 91:\r\n-\t\tmemcpy_sse2_64(dd - 91, ss - 91);\r\n-\tcase 27:\r\n-\t\tmemcpy_sse2_16(dd - 27, ss - 27);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 92:\r\n-\t\tmemcpy_sse2_64(dd - 92, ss - 92);\r\n-\tcase 28:\r\n-\t\tmemcpy_sse2_16(dd - 28, ss - 28);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 93:\r\n-\t\tmemcpy_sse2_64(dd - 93, ss - 93);\r\n-\tcase 29:\r\n-\t\tmemcpy_sse2_16(dd - 29, ss - 29);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 94:\r\n-\t\tmemcpy_sse2_64(dd - 94, ss - 94);\r\n-\tcase 30:\r\n-\t\tmemcpy_sse2_16(dd - 30, ss - 30);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 95:\r\n-\t\tmemcpy_sse2_64(dd - 95, ss - 95);\r\n-\tcase 31:\r\n-\t\tmemcpy_sse2_16(dd - 31, ss - 31);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 96:\r\n-\t\tmemcpy_sse2_64(dd - 96, ss - 96);\r\n-\tcase 32:\r\n-\t\tmemcpy_sse2_32(dd - 32, ss - 32);\r\n-\t\tbreak;\r\n-\r\n-\tcase 97:\r\n-\t\tmemcpy_sse2_64(dd - 97, ss - 97);\r\n-\tcase 33:\r\n-\t\tmemcpy_sse2_32(dd - 33, ss - 33);\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 98:\r\n-\t\tmemcpy_sse2_64(dd - 98, ss - 98);\r\n-\tcase 34:\r\n-\t\tmemcpy_sse2_32(dd - 34, ss - 34);\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 99:\r\n-\t\tmemcpy_sse2_64(dd - 99, ss - 99);\r\n-\tcase 35:\r\n-\t\tmemcpy_sse2_32(dd - 35, ss - 35);\r\n-\t\t*((uint16_unaligned_t*)(dd - 3)) = *((uint16_unaligned_t*)(ss - 3));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 100:\r\n-\t\tmemcpy_sse2_64(dd - 100, ss - 100);\r\n-\tcase 36:\r\n-\t\tmemcpy_sse2_32(dd - 36, ss - 36);\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 101:\r\n-\t\tmemcpy_sse2_64(dd - 101, ss - 101);\r\n-\tcase 37:\r\n-\t\tmemcpy_sse2_32(dd - 37, ss - 37);\r\n-\t\t*((uint32_unaligned_t*)(dd - 5)) = *((uint32_unaligned_t*)(ss - 5));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 102:\r\n-\t\tmemcpy_sse2_64(dd - 102, ss - 102);\r\n-\tcase 38:\r\n-\t\tmemcpy_sse2_32(dd - 38, ss - 38);\r\n-\t\t*((uint32_unaligned_t*)(dd - 6)) = *((uint32_unaligned_t*)(ss - 6));\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 103:\r\n-\t\tmemcpy_sse2_64(dd - 103, ss - 103);\r\n-\tcase 39:\r\n-\t\tmemcpy_sse2_32(dd - 39, ss - 39);\r\n-\t\t*((uint32_unaligned_t*)(dd - 7)) = *((uint32_unaligned_t*)(ss - 7));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 104:\r\n-\t\tmemcpy_sse2_64(dd - 104, ss - 104);\r\n-\tcase 40:\r\n-\t\tmemcpy_sse2_32(dd - 40, ss - 40);\r\n-\t\t*((uint64_unaligned_t*)(dd - 8)) = *((uint64_unaligned_t*)(ss - 8));\r\n-\t\tbreak;\r\n-\r\n-\tcase 105:\r\n-\t\tmemcpy_sse2_64(dd - 105, ss - 105);\r\n-\tcase 41:\r\n-\t\tmemcpy_sse2_32(dd - 41, ss - 41);\r\n-\t\t*((uint64_unaligned_t*)(dd - 9)) = *((uint64_unaligned_t*)(ss - 9));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 106:\r\n-\t\tmemcpy_sse2_64(dd - 106, ss - 106);\r\n-\tcase 42:\r\n-\t\tmemcpy_sse2_32(dd - 42, ss - 42);\r\n-\t\t*((uint64_unaligned_t*)(dd - 10)) = *((uint64_unaligned_t*)(ss - 10));\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 107:\r\n-\t\tmemcpy_sse2_64(dd - 107, ss - 107);\r\n-\tcase 43:\r\n-\t\tmemcpy_sse2_32(dd - 43, ss - 43);\r\n-\t\t*((uint64_unaligned_t*)(dd - 11)) = *((uint64_unaligned_t*)(ss - 11));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 108:\r\n-\t\tmemcpy_sse2_64(dd - 108, ss - 108);\r\n-\tcase 44:\r\n-\t\tmemcpy_sse2_32(dd - 44, ss - 44);\r\n-\t\t*((uint64_unaligned_t*)(dd - 12)) = *((uint64_unaligned_t*)(ss - 12));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 109:\r\n-\t\tmemcpy_sse2_64(dd - 109, ss - 109);\r\n-\tcase 45:\r\n-\t\tmemcpy_sse2_32(dd - 45, ss - 45);\r\n-\t\t*((uint64_unaligned_t*)(dd - 13)) = *((uint64_unaligned_t*)(ss - 13));\r\n-\t\t*((uint32_unaligned_t*)(dd - 5)) = *((uint32_unaligned_t*)(ss - 5));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 110:\r\n-\t\tmemcpy_sse2_64(dd - 110, ss - 110);\r\n-\tcase 46:\r\n-\t\tmemcpy_sse2_32(dd - 46, ss - 46);\r\n-\t\t*((uint64_unaligned_t*)(dd - 14)) = *((uint64_unaligned_t*)(ss - 14));\r\n-\t\t*((uint64_unaligned_t*)(dd - 8)) = *((uint64_unaligned_t*)(ss - 8));\r\n-\t\tbreak;\r\n-\r\n-\tcase 111:\r\n-\t\tmemcpy_sse2_64(dd - 111, ss - 111);\r\n-\tcase 47:\r\n-\t\tmemcpy_sse2_32(dd - 47, ss - 47);\r\n-\t\t*((uint64_unaligned_t*)(dd - 15)) = *((uint64_unaligned_t*)(ss - 15));\r\n-\t\t*((uint64_unaligned_t*)(dd - 8)) = *((uint64_unaligned_t*)(ss - 8));\r\n-\t\tbreak;\r\n-\r\n-\tcase 112:\r\n-\t\tmemcpy_sse2_64(dd - 112, ss - 112);\r\n-\tcase 48:\r\n-\t\tmemcpy_sse2_32(dd - 48, ss - 48);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 113:\r\n-\t\tmemcpy_sse2_64(dd - 113, ss - 113);\r\n-\tcase 49:\r\n-\t\tmemcpy_sse2_32(dd - 49, ss - 49);\r\n-\t\tmemcpy_sse2_16(dd - 17, ss - 17);\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 114:\r\n-\t\tmemcpy_sse2_64(dd - 114, ss - 114);\r\n-\tcase 50:\r\n-\t\tmemcpy_sse2_32(dd - 50, ss - 50);\r\n-\t\tmemcpy_sse2_16(dd - 18, ss - 18);\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 115:\r\n-\t\tmemcpy_sse2_64(dd - 115, ss - 115);\r\n-\tcase 51:\r\n-\t\tmemcpy_sse2_32(dd - 51, ss - 51);\r\n-\t\tmemcpy_sse2_16(dd - 19, ss - 19);\r\n-\t\t*((uint16_unaligned_t*)(dd - 3)) = *((uint16_unaligned_t*)(ss - 3));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 116:\r\n-\t\tmemcpy_sse2_64(dd - 116, ss - 116);\r\n-\tcase 52:\r\n-\t\tmemcpy_sse2_32(dd - 52, ss - 52);\r\n-\t\tmemcpy_sse2_16(dd - 20, ss - 20);\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 117:\r\n-\t\tmemcpy_sse2_64(dd - 117, ss - 117);\r\n-\tcase 53:\r\n-\t\tmemcpy_sse2_32(dd - 53, ss - 53);\r\n-\t\tmemcpy_sse2_16(dd - 21, ss - 21);\r\n-\t\t*((uint32_unaligned_t*)(dd - 5)) = *((uint32_unaligned_t*)(ss - 5));\r\n-\t\tdd[-1] = ss[-1];\r\n-\t\tbreak;\r\n-\r\n-\tcase 118:\r\n-\t\tmemcpy_sse2_64(dd - 118, ss - 118);\r\n-\tcase 54:\r\n-\t\tmemcpy_sse2_32(dd - 54, ss - 54);\r\n-\t\tmemcpy_sse2_16(dd - 22, ss - 22);\r\n-\t\t*((uint32_unaligned_t*)(dd - 6)) = *((uint32_unaligned_t*)(ss - 6));\r\n-\t\t*((uint16_unaligned_t*)(dd - 2)) = *((uint16_unaligned_t*)(ss - 2));\r\n-\t\tbreak;\r\n-\r\n-\tcase 119:\r\n-\t\tmemcpy_sse2_64(dd - 119, ss - 119);\r\n-\tcase 55:\r\n-\t\tmemcpy_sse2_32(dd - 55, ss - 55);\r\n-\t\tmemcpy_sse2_16(dd - 23, ss - 23);\r\n-\t\t*((uint32_unaligned_t*)(dd - 7)) = *((uint32_unaligned_t*)(ss - 7));\r\n-\t\t*((uint32_unaligned_t*)(dd - 4)) = *((uint32_unaligned_t*)(ss - 4));\r\n-\t\tbreak;\r\n-\r\n-\tcase 120:\r\n-\t\tmemcpy_sse2_64(dd - 120, ss - 120);\r\n-\tcase 56:\r\n-\t\tmemcpy_sse2_32(dd - 56, ss - 56);\r\n-\t\tmemcpy_sse2_16(dd - 24, ss - 24);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 121:\r\n-\t\tmemcpy_sse2_64(dd - 121, ss - 121);\r\n-\tcase 57:\r\n-\t\tmemcpy_sse2_32(dd - 57, ss - 57);\r\n-\t\tmemcpy_sse2_16(dd - 25, ss - 25);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 122:\r\n-\t\tmemcpy_sse2_64(dd - 122, ss - 122);\r\n-\tcase 58:\r\n-\t\tmemcpy_sse2_32(dd - 58, ss - 58);\r\n-\t\tmemcpy_sse2_16(dd - 26, ss - 26);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 123:\r\n-\t\tmemcpy_sse2_64(dd - 123, ss - 123);\r\n-\tcase 59:\r\n-\t\tmemcpy_sse2_32(dd - 59, ss - 59);\r\n-\t\tmemcpy_sse2_16(dd - 27, ss - 27);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 124:\r\n-\t\tmemcpy_sse2_64(dd - 124, ss - 124);\r\n-\tcase 60:\r\n-\t\tmemcpy_sse2_32(dd - 60, ss - 60);\r\n-\t\tmemcpy_sse2_16(dd - 28, ss - 28);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 125:\r\n-\t\tmemcpy_sse2_64(dd - 125, ss - 125);\r\n-\tcase 61:\r\n-\t\tmemcpy_sse2_32(dd - 61, ss - 61);\r\n-\t\tmemcpy_sse2_16(dd - 29, ss - 29);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 126:\r\n-\t\tmemcpy_sse2_64(dd - 126, ss - 126);\r\n-\tcase 62:\r\n-\t\tmemcpy_sse2_32(dd - 62, ss - 62);\r\n-\t\tmemcpy_sse2_16(dd - 30, ss - 30);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 127:\r\n-\t\tmemcpy_sse2_64(dd - 127, ss - 127);\r\n-\tcase 63:\r\n-\t\tmemcpy_sse2_32(dd - 63, ss - 63);\r\n-\t\tmemcpy_sse2_16(dd - 31, ss - 31);\r\n-\t\tmemcpy_sse2_16(dd - 16, ss - 16);\r\n-\t\tbreak;\r\n-\r\n-\tcase 128:\r\n-\t\tmemcpy_sse2_128(dd - 128, ss - 128);\r\n-\t\tbreak;\r\n-\t}\r\n-\r\n-\treturn dst;\r\n-}\r\n-\r\n-\r\n-//---------------------------------------------------------------------\r\n-// main routine\r\n-//---------------------------------------------------------------------\r\n-static void* memcpy_fast(void *destination, const void *source, size_t size)\r\n-{\r\n-\tunsigned char *dst = (unsigned char*)destination;\r\n-\tconst unsigned char *src = (const unsigned char*)source;\r\n-\tstatic size_t cachesize = 0x200000; // L2-cache size\r\n-\tsize_t padding;\r\n-\r\n-\t// small memory copy\r\n-\tif (size <= 128) {\r\n-\t\treturn memcpy_tiny(dst, src, size);\r\n-\t}\r\n-\r\n-\t// align destination to 16 bytes boundary\r\n-\tpadding = (16 - (((size_t)dst) & 15)) & 15;\r\n-\r\n-\tif (padding > 0) {\r\n-\t\t__m128i head = _mm_loadu_si128((const __m128i*)src);\r\n-\t\t_mm_storeu_si128((__m128i*)dst, head);\r\n-\t\tdst += padding;\r\n-\t\tsrc += padding;\r\n-\t\tsize -= padding;\r\n-\t}\r\n-\r\n-\t// medium size copy\r\n-\tif (size <= cachesize) {\r\n-\t\t__m128i c0, c1, c2, c3, c4, c5, c6, c7;\r\n-\r\n-\t\tfor (; size >= 128; size -= 128) {\r\n-\t\t\tc0 = _mm_loadu_si128(((const __m128i*)src) + 0);\r\n-\t\t\tc1 = _mm_loadu_si128(((const __m128i*)src) + 1);\r\n-\t\t\tc2 = _mm_loadu_si128(((const __m128i*)src) + 2);\r\n-\t\t\tc3 = _mm_loadu_si128(((const __m128i*)src) + 3);\r\n-\t\t\tc4 = _mm_loadu_si128(((const __m128i*)src) + 4);\r\n-\t\t\tc5 = _mm_loadu_si128(((const __m128i*)src) + 5);\r\n-\t\t\tc6 = _mm_loadu_si128(((const __m128i*)src) + 6);\r\n-\t\t\tc7 = _mm_loadu_si128(((const __m128i*)src) + 7);\r\n-\t\t\t_mm_prefetch((const char*)(src + 256), _MM_HINT_NTA);\r\n-\t\t\tsrc += 128;\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 0), c0);\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 1), c1);\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 2), c2);\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 3), c3);\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 4), c4);\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 5), c5);\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 6), c6);\r\n-\t\t\t_mm_store_si128((((__m128i*)dst) + 7), c7);\r\n-\t\t\tdst += 128;\r\n-\t\t}\r\n-\t}\r\n-\telse {\t\t// big memory copy\r\n-\t\t__m128i c0, c1, c2, c3, c4, c5, c6, c7;\r\n-\r\n-\t\t_mm_prefetch((const char*)(src), _MM_HINT_NTA);\r\n-\r\n-\t\tif ((((size_t)src) & 15) == 0) {\t// source aligned\r\n-\t\t\tfor (; size >= 128; size -= 128) {\r\n-\t\t\t\tc0 = _mm_load_si128(((const __m128i*)src) + 0);\r\n-\t\t\t\tc1 = _mm_load_si128(((const __m128i*)src) + 1);\r\n-\t\t\t\tc2 = _mm_load_si128(((const __m128i*)src) + 2);\r\n-\t\t\t\tc3 = _mm_load_si128(((const __m128i*)src) + 3);\r\n-\t\t\t\tc4 = _mm_load_si128(((const __m128i*)src) + 4);\r\n-\t\t\t\tc5 = _mm_load_si128(((const __m128i*)src) + 5);\r\n-\t\t\t\tc6 = _mm_load_si128(((const __m128i*)src) + 6);\r\n-\t\t\t\tc7 = _mm_load_si128(((const __m128i*)src) + 7);\r\n-\t\t\t\t_mm_prefetch((const char*)(src + 256), _MM_HINT_NTA);\r\n-\t\t\t\tsrc += 128;\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 0), c0);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 1), c1);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 2), c2);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 3), c3);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 4), c4);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 5), c5);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 6), c6);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 7), c7);\r\n-\t\t\t\tdst += 128;\r\n-\t\t\t}\r\n-\t\t}\r\n-\t\telse {\t\t\t\t\t\t\t// source unaligned\r\n-\t\t\tfor (; size >= 128; size -= 128) {\r\n-\t\t\t\tc0 = _mm_loadu_si128(((const __m128i*)src) + 0);\r\n-\t\t\t\tc1 = _mm_loadu_si128(((const __m128i*)src) + 1);\r\n-\t\t\t\tc2 = _mm_loadu_si128(((const __m128i*)src) + 2);\r\n-\t\t\t\tc3 = _mm_loadu_si128(((const __m128i*)src) + 3);\r\n-\t\t\t\tc4 = _mm_loadu_si128(((const __m128i*)src) + 4);\r\n-\t\t\t\tc5 = _mm_loadu_si128(((const __m128i*)src) + 5);\r\n-\t\t\t\tc6 = _mm_loadu_si128(((const __m128i*)src) + 6);\r\n-\t\t\t\tc7 = _mm_loadu_si128(((const __m128i*)src) + 7);\r\n-\t\t\t\t_mm_prefetch((const char*)(src + 256), _MM_HINT_NTA);\r\n-\t\t\t\tsrc += 128;\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 0), c0);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 1), c1);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 2), c2);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 3), c3);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 4), c4);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 5), c5);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 6), c6);\r\n-\t\t\t\t_mm_stream_si128((((__m128i*)dst) + 7), c7);\r\n-\t\t\t\tdst += 128;\r\n-\t\t\t}\r\n-\t\t}\r\n-\t\t_mm_sfence();\r\n-\t}\r\n-\r\n-\tmemcpy_tiny(dst, src, size);\r\n-\r\n-\treturn destination;\r\n-}\r\n-\r\n-\r\n-#endif\r\ndiff --git a/contrib/FastMemcpy/FastMemcpy_Avx.c b/contrib/FastMemcpy/FastMemcpy_Avx.c\ndeleted file mode 100644\nindex 6538c6b21260..000000000000\n--- a/contrib/FastMemcpy/FastMemcpy_Avx.c\n+++ /dev/null\n@@ -1,171 +0,0 @@\n-//=====================================================================\r\n-//\r\n-// FastMemcpy.c - skywind3000@163.com, 2015\r\n-//\r\n-// feature:\r\n-// 50% speed up in avg. vs standard memcpy (tested in vc2012/gcc4.9)\r\n-//\r\n-//=====================================================================\r\n-#include <stdio.h>\r\n-#include <stdlib.h>\r\n-#include <string.h>\r\n-#include <time.h>\r\n-#include <assert.h>\r\n-\r\n-#if (defined(_WIN32) || defined(WIN32))\r\n-#include <windows.h>\r\n-#include <mmsystem.h>\r\n-#ifdef _MSC_VER\r\n-#pragma comment(lib, \"winmm.lib\")\r\n-#endif\r\n-#elif defined(__unix)\r\n-#include <sys/time.h>\r\n-#include <unistd.h>\r\n-#else\r\n-#error it can only be compiled under windows or unix\r\n-#endif\r\n-\r\n-#include \"FastMemcpy_Avx.h\"\r\n-\r\n-\r\n-unsigned int gettime()\r\n-{\r\n-\t#if (defined(_WIN32) || defined(WIN32))\r\n-\treturn timeGetTime();\r\n-\t#else\r\n-\tstatic struct timezone tz={ 0,0 };\r\n-\tstruct timeval time;\r\n-\tgettimeofday(&time,&tz);\r\n-\treturn (time.tv_sec * 1000 + time.tv_usec / 1000);\r\n-\t#endif\r\n-}\r\n-\r\n-void sleepms(unsigned int millisec)\r\n-{\r\n-#if defined(_WIN32) || defined(WIN32)\r\n-\tSleep(millisec);\r\n-#else\r\n-\tusleep(millisec * 1000);\r\n-#endif\r\n-}\r\n-\r\n-\r\n-\r\n-void benchmark(int dstalign, int srcalign, size_t size, int times)\r\n-{\r\n-\tchar *DATA1 = (char*)malloc(size + 64);\r\n-\tchar *DATA2 = (char*)malloc(size + 64);\r\n-\tsize_t LINEAR1 = ((size_t)DATA1);\r\n-\tsize_t LINEAR2 = ((size_t)DATA2);\r\n-\tchar *ALIGN1 = (char*)(((64 - (LINEAR1 & 63)) & 63) + LINEAR1);\r\n-\tchar *ALIGN2 = (char*)(((64 - (LINEAR2 & 63)) & 63) + LINEAR2);\r\n-\tchar *dst = (dstalign)? ALIGN1 : (ALIGN1 + 1);\r\n-\tchar *src = (srcalign)? ALIGN2 : (ALIGN2 + 3);\r\n-\tunsigned int t1, t2;\r\n-\tint k;\r\n-\t\r\n-\tsleepms(100);\r\n-\tt1 = gettime();\r\n-\tfor (k = times; k > 0; k--) {\r\n-\t\tmemcpy(dst, src, size);\r\n-\t}\r\n-\tt1 = gettime() - t1;\r\n-\tsleepms(100);\r\n-\tt2 = gettime();\r\n-\tfor (k = times; k > 0; k--) {\r\n-\t\tmemcpy_fast(dst, src, size);\r\n-\t}\r\n-\tt2 = gettime() - t2;\r\n-\r\n-\tfree(DATA1);\r\n-\tfree(DATA2);\r\n-\r\n-\tprintf(\"result(dst %s, src %s): memcpy_fast=%dms memcpy=%d ms\\n\",  \r\n-\t\tdstalign? \"aligned\" : \"unalign\", \r\n-\t\tsrcalign? \"aligned\" : \"unalign\", (int)t2, (int)t1);\r\n-}\r\n-\r\n-\r\n-void bench(int copysize, int times)\r\n-{\r\n-\tprintf(\"benchmark(size=%d bytes, times=%d):\\n\", copysize, times);\r\n-\tbenchmark(1, 1, copysize, times);\r\n-\tbenchmark(1, 0, copysize, times);\r\n-\tbenchmark(0, 1, copysize, times);\r\n-\tbenchmark(0, 0, copysize, times);\r\n-\tprintf(\"\\n\");\r\n-}\r\n-\r\n-\r\n-void random_bench(int maxsize, int times)\r\n-{\r\n-\tstatic char A[11 * 1024 * 1024 + 2];\r\n-\tstatic char B[11 * 1024 * 1024 + 2];\r\n-\tstatic int random_offsets[0x10000];\r\n-\tstatic int random_sizes[0x8000];\r\n-\tunsigned int i, p1, p2;\r\n-\tunsigned int t1, t2;\r\n-\tfor (i = 0; i < 0x10000; i++) {\t// generate random offsets\r\n-\t\trandom_offsets[i] = rand() % (10 * 1024 * 1024 + 1);\r\n-\t}\r\n-\tfor (i = 0; i < 0x8000; i++) {\t// generate random sizes\r\n-\t\trandom_sizes[i] = 1 + rand() % maxsize;\r\n-\t}\r\n-\tsleepms(100);\r\n-\tt1 = gettime();\r\n-\tfor (p1 = 0, p2 = 0, i = 0; i < times; i++) {\r\n-\t\tint offset1 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint offset2 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint size = random_sizes[(p2++) & 0x7fff];\r\n-\t\tmemcpy(A + offset1, B + offset2, size);\r\n-\t}\r\n-\tt1 = gettime() - t1;\r\n-\tsleepms(100);\r\n-\tt2 = gettime();\r\n-\tfor (p1 = 0, p2 = 0, i = 0; i < times; i++) {\r\n-\t\tint offset1 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint offset2 = random_offsets[(p1++) & 0xffff];\r\n-\t\tint size = random_sizes[(p2++) & 0x7fff];\r\n-\t\tmemcpy_fast(A + offset1, B + offset2, size);\r\n-\t}\r\n-\tt2 = gettime() - t2;\r\n-\tprintf(\"benchmark random access:\\n\");\r\n-\tprintf(\"memcpy_fast=%dms memcpy=%dms\\n\\n\", (int)t2, (int)t1);\r\n-}\r\n-\r\n-\r\n-#ifdef _MSC_VER\r\n-#pragma comment(lib, \"winmm.lib\")\r\n-#endif\r\n-\r\n-int main(void)\r\n-{\r\n-#if 1\r\n-\tbench(32, 0x1000000);\r\n-\tbench(64, 0x1000000);\r\n-\tbench(512, 0x800000);\r\n-\tbench(1024, 0x400000);\r\n-#endif\r\n-\tbench(4096, 0x80000);\r\n-\tbench(8192, 0x40000);\r\n-#if 1\r\n-\tbench(1024 * 1024 * 1, 0x800);\r\n-\tbench(1024 * 1024 * 4, 0x200);\r\n-#endif\r\n-\tbench(1024 * 1024 * 8, 0x100);\r\n-\t\r\n-\trandom_bench(2048, 8000000);\r\n-\r\n-\treturn 0;\r\n-}\r\n-\r\n-\r\n-\r\n-\r\n-/*\r\n-\r\n-*/\r\n-\r\n-\r\n-\r\n-\r\ndiff --git a/contrib/FastMemcpy/FastMemcpy_Avx.h b/contrib/FastMemcpy/FastMemcpy_Avx.h\ndeleted file mode 100644\nindex 8ba064b03508..000000000000\n--- a/contrib/FastMemcpy/FastMemcpy_Avx.h\n+++ /dev/null\n@@ -1,492 +0,0 @@\n-//=====================================================================\r\n-//\r\n-// FastMemcpy.c - skywind3000@163.com, 2015\r\n-//\r\n-// feature:\r\n-// 50% speed up in avg. vs standard memcpy (tested in vc2012/gcc5.1)\r\n-//\r\n-//=====================================================================\r\n-#ifndef __FAST_MEMCPY_H__\r\n-#define __FAST_MEMCPY_H__\r\n-\r\n-#include <stddef.h>\r\n-#include <stdint.h>\r\n-#include <immintrin.h>\r\n-\r\n-\r\n-//---------------------------------------------------------------------\r\n-// force inline for compilers\r\n-//---------------------------------------------------------------------\r\n-#ifndef INLINE\r\n-#ifdef __GNUC__\r\n-#if (__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 1))\r\n-    #define INLINE         __inline__ __attribute__((always_inline))\r\n-#else\r\n-    #define INLINE         __inline__\r\n-#endif\r\n-#elif defined(_MSC_VER)\r\n-\t#define INLINE __forceinline\r\n-#elif (defined(__BORLANDC__) || defined(__WATCOMC__))\r\n-    #define INLINE __inline\r\n-#else\r\n-    #define INLINE \r\n-#endif\r\n-#endif\r\n-\r\n-\r\n-\r\n-//---------------------------------------------------------------------\r\n-// fast copy for different sizes\r\n-//---------------------------------------------------------------------\r\n-static INLINE void memcpy_avx_16(void *dst, const void *src) {\r\n-#if 1\r\n-\t__m128i m0 = _mm_loadu_si128(((const __m128i*)src) + 0);\r\n-\t_mm_storeu_si128(((__m128i*)dst) + 0, m0);\r\n-#else\r\n-\t*((uint64_t*)((char*)dst + 0)) = *((uint64_t*)((const char*)src + 0));\r\n-\t*((uint64_t*)((char*)dst + 8)) = *((uint64_t*)((const char*)src + 8));\r\n-#endif\r\n-}\r\n-\r\n-static INLINE void memcpy_avx_32(void *dst, const void *src) {\r\n-\t__m256i m0 = _mm256_loadu_si256(((const __m256i*)src) + 0);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 0, m0);\r\n-}\r\n-\r\n-static INLINE void memcpy_avx_64(void *dst, const void *src) {\r\n-\t__m256i m0 = _mm256_loadu_si256(((const __m256i*)src) + 0);\r\n-\t__m256i m1 = _mm256_loadu_si256(((const __m256i*)src) + 1);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 0, m0);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 1, m1);\r\n-}\r\n-\r\n-static INLINE void memcpy_avx_128(void *dst, const void *src) {\r\n-\t__m256i m0 = _mm256_loadu_si256(((const __m256i*)src) + 0);\r\n-\t__m256i m1 = _mm256_loadu_si256(((const __m256i*)src) + 1);\r\n-\t__m256i m2 = _mm256_loadu_si256(((const __m256i*)src) + 2);\r\n-\t__m256i m3 = _mm256_loadu_si256(((const __m256i*)src) + 3);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 0, m0);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 1, m1);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 2, m2);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 3, m3);\r\n-}\r\n-\r\n-static INLINE void memcpy_avx_256(void *dst, const void *src) {\r\n-\t__m256i m0 = _mm256_loadu_si256(((const __m256i*)src) + 0);\r\n-\t__m256i m1 = _mm256_loadu_si256(((const __m256i*)src) + 1);\r\n-\t__m256i m2 = _mm256_loadu_si256(((const __m256i*)src) + 2);\r\n-\t__m256i m3 = _mm256_loadu_si256(((const __m256i*)src) + 3);\r\n-\t__m256i m4 = _mm256_loadu_si256(((const __m256i*)src) + 4);\r\n-\t__m256i m5 = _mm256_loadu_si256(((const __m256i*)src) + 5);\r\n-\t__m256i m6 = _mm256_loadu_si256(((const __m256i*)src) + 6);\r\n-\t__m256i m7 = _mm256_loadu_si256(((const __m256i*)src) + 7);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 0, m0);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 1, m1);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 2, m2);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 3, m3);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 4, m4);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 5, m5);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 6, m6);\r\n-\t_mm256_storeu_si256(((__m256i*)dst) + 7, m7);\r\n-}\r\n-\r\n-\r\n-//---------------------------------------------------------------------\r\n-// tiny memory copy with jump table optimized\r\n-//---------------------------------------------------------------------\r\n-static INLINE void *memcpy_tiny(void *dst, const void *src, size_t size) {\r\n-\tunsigned char *dd = ((unsigned char*)dst) + size;\r\n-\tconst unsigned char *ss = ((const unsigned char*)src) + size;\r\n-\r\n-\tswitch (size) { \r\n-\tcase 128: memcpy_avx_128(dd - 128, ss - 128);\r\n-\tcase 0:  break;\r\n-\tcase 129: memcpy_avx_128(dd - 129, ss - 129);\r\n-\tcase 1: dd[-1] = ss[-1]; break;\r\n-\tcase 130: memcpy_avx_128(dd - 130, ss - 130);\r\n-\tcase 2: *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\r\n-\tcase 131: memcpy_avx_128(dd - 131, ss - 131);\r\n-\tcase 3: *((uint16_t*)(dd - 3)) = *((uint16_t*)(ss - 3)); dd[-1] = ss[-1]; break;\r\n-\tcase 132: memcpy_avx_128(dd - 132, ss - 132);\r\n-\tcase 4: *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 133: memcpy_avx_128(dd - 133, ss - 133);\r\n-\tcase 5: *((uint32_t*)(dd - 5)) = *((uint32_t*)(ss - 5)); dd[-1] = ss[-1]; break;\r\n-\tcase 134: memcpy_avx_128(dd - 134, ss - 134);\r\n-\tcase 6: *((uint32_t*)(dd - 6)) = *((uint32_t*)(ss - 6)); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\r\n-\tcase 135: memcpy_avx_128(dd - 135, ss - 135);\r\n-\tcase 7: *((uint32_t*)(dd - 7)) = *((uint32_t*)(ss - 7)); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 136: memcpy_avx_128(dd - 136, ss - 136);\r\n-\tcase 8: *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 137: memcpy_avx_128(dd - 137, ss - 137);\r\n-\tcase 9: *((uint64_t*)(dd - 9)) = *((uint64_t*)(ss - 9)); dd[-1] = ss[-1]; break;\r\n-\tcase 138: memcpy_avx_128(dd - 138, ss - 138);\r\n-\tcase 10: *((uint64_t*)(dd - 10)) = *((uint64_t*)(ss - 10)); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\r\n-\tcase 139: memcpy_avx_128(dd - 139, ss - 139);\r\n-\tcase 11: *((uint64_t*)(dd - 11)) = *((uint64_t*)(ss - 11)); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 140: memcpy_avx_128(dd - 140, ss - 140);\r\n-\tcase 12: *((uint64_t*)(dd - 12)) = *((uint64_t*)(ss - 12)); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 141: memcpy_avx_128(dd - 141, ss - 141);\r\n-\tcase 13: *((uint64_t*)(dd - 13)) = *((uint64_t*)(ss - 13)); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 142: memcpy_avx_128(dd - 142, ss - 142);\r\n-\tcase 14: *((uint64_t*)(dd - 14)) = *((uint64_t*)(ss - 14)); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 143: memcpy_avx_128(dd - 143, ss - 143);\r\n-\tcase 15: *((uint64_t*)(dd - 15)) = *((uint64_t*)(ss - 15)); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 144: memcpy_avx_128(dd - 144, ss - 144);\r\n-\tcase 16: memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 145: memcpy_avx_128(dd - 145, ss - 145);\r\n-\tcase 17: memcpy_avx_16(dd - 17, ss - 17); dd[-1] = ss[-1]; break;\r\n-\tcase 146: memcpy_avx_128(dd - 146, ss - 146);\r\n-\tcase 18: memcpy_avx_16(dd - 18, ss - 18); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\r\n-\tcase 147: memcpy_avx_128(dd - 147, ss - 147);\r\n-\tcase 19: memcpy_avx_16(dd - 19, ss - 19); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 148: memcpy_avx_128(dd - 148, ss - 148);\r\n-\tcase 20: memcpy_avx_16(dd - 20, ss - 20); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 149: memcpy_avx_128(dd - 149, ss - 149);\r\n-\tcase 21: memcpy_avx_16(dd - 21, ss - 21); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 150: memcpy_avx_128(dd - 150, ss - 150);\r\n-\tcase 22: memcpy_avx_16(dd - 22, ss - 22); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 151: memcpy_avx_128(dd - 151, ss - 151);\r\n-\tcase 23: memcpy_avx_16(dd - 23, ss - 23); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 152: memcpy_avx_128(dd - 152, ss - 152);\r\n-\tcase 24: memcpy_avx_16(dd - 24, ss - 24); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 153: memcpy_avx_128(dd - 153, ss - 153);\r\n-\tcase 25: memcpy_avx_16(dd - 25, ss - 25); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 154: memcpy_avx_128(dd - 154, ss - 154);\r\n-\tcase 26: memcpy_avx_16(dd - 26, ss - 26); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 155: memcpy_avx_128(dd - 155, ss - 155);\r\n-\tcase 27: memcpy_avx_16(dd - 27, ss - 27); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 156: memcpy_avx_128(dd - 156, ss - 156);\r\n-\tcase 28: memcpy_avx_16(dd - 28, ss - 28); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 157: memcpy_avx_128(dd - 157, ss - 157);\r\n-\tcase 29: memcpy_avx_16(dd - 29, ss - 29); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 158: memcpy_avx_128(dd - 158, ss - 158);\r\n-\tcase 30: memcpy_avx_16(dd - 30, ss - 30); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 159: memcpy_avx_128(dd - 159, ss - 159);\r\n-\tcase 31: memcpy_avx_16(dd - 31, ss - 31); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 160: memcpy_avx_128(dd - 160, ss - 160);\r\n-\tcase 32: memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 161: memcpy_avx_128(dd - 161, ss - 161);\r\n-\tcase 33: memcpy_avx_32(dd - 33, ss - 33); dd[-1] = ss[-1]; break;\r\n-\tcase 162: memcpy_avx_128(dd - 162, ss - 162);\r\n-\tcase 34: memcpy_avx_32(dd - 34, ss - 34); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\r\n-\tcase 163: memcpy_avx_128(dd - 163, ss - 163);\r\n-\tcase 35: memcpy_avx_32(dd - 35, ss - 35); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 164: memcpy_avx_128(dd - 164, ss - 164);\r\n-\tcase 36: memcpy_avx_32(dd - 36, ss - 36); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 165: memcpy_avx_128(dd - 165, ss - 165);\r\n-\tcase 37: memcpy_avx_32(dd - 37, ss - 37); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 166: memcpy_avx_128(dd - 166, ss - 166);\r\n-\tcase 38: memcpy_avx_32(dd - 38, ss - 38); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 167: memcpy_avx_128(dd - 167, ss - 167);\r\n-\tcase 39: memcpy_avx_32(dd - 39, ss - 39); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 168: memcpy_avx_128(dd - 168, ss - 168);\r\n-\tcase 40: memcpy_avx_32(dd - 40, ss - 40); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 169: memcpy_avx_128(dd - 169, ss - 169);\r\n-\tcase 41: memcpy_avx_32(dd - 41, ss - 41); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 170: memcpy_avx_128(dd - 170, ss - 170);\r\n-\tcase 42: memcpy_avx_32(dd - 42, ss - 42); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 171: memcpy_avx_128(dd - 171, ss - 171);\r\n-\tcase 43: memcpy_avx_32(dd - 43, ss - 43); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 172: memcpy_avx_128(dd - 172, ss - 172);\r\n-\tcase 44: memcpy_avx_32(dd - 44, ss - 44); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 173: memcpy_avx_128(dd - 173, ss - 173);\r\n-\tcase 45: memcpy_avx_32(dd - 45, ss - 45); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 174: memcpy_avx_128(dd - 174, ss - 174);\r\n-\tcase 46: memcpy_avx_32(dd - 46, ss - 46); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 175: memcpy_avx_128(dd - 175, ss - 175);\r\n-\tcase 47: memcpy_avx_32(dd - 47, ss - 47); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 176: memcpy_avx_128(dd - 176, ss - 176);\r\n-\tcase 48: memcpy_avx_32(dd - 48, ss - 48); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 177: memcpy_avx_128(dd - 177, ss - 177);\r\n-\tcase 49: memcpy_avx_32(dd - 49, ss - 49); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 178: memcpy_avx_128(dd - 178, ss - 178);\r\n-\tcase 50: memcpy_avx_32(dd - 50, ss - 50); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 179: memcpy_avx_128(dd - 179, ss - 179);\r\n-\tcase 51: memcpy_avx_32(dd - 51, ss - 51); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 180: memcpy_avx_128(dd - 180, ss - 180);\r\n-\tcase 52: memcpy_avx_32(dd - 52, ss - 52); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 181: memcpy_avx_128(dd - 181, ss - 181);\r\n-\tcase 53: memcpy_avx_32(dd - 53, ss - 53); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 182: memcpy_avx_128(dd - 182, ss - 182);\r\n-\tcase 54: memcpy_avx_32(dd - 54, ss - 54); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 183: memcpy_avx_128(dd - 183, ss - 183);\r\n-\tcase 55: memcpy_avx_32(dd - 55, ss - 55); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 184: memcpy_avx_128(dd - 184, ss - 184);\r\n-\tcase 56: memcpy_avx_32(dd - 56, ss - 56); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 185: memcpy_avx_128(dd - 185, ss - 185);\r\n-\tcase 57: memcpy_avx_32(dd - 57, ss - 57); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 186: memcpy_avx_128(dd - 186, ss - 186);\r\n-\tcase 58: memcpy_avx_32(dd - 58, ss - 58); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 187: memcpy_avx_128(dd - 187, ss - 187);\r\n-\tcase 59: memcpy_avx_32(dd - 59, ss - 59); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 188: memcpy_avx_128(dd - 188, ss - 188);\r\n-\tcase 60: memcpy_avx_32(dd - 60, ss - 60); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 189: memcpy_avx_128(dd - 189, ss - 189);\r\n-\tcase 61: memcpy_avx_32(dd - 61, ss - 61); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 190: memcpy_avx_128(dd - 190, ss - 190);\r\n-\tcase 62: memcpy_avx_32(dd - 62, ss - 62); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 191: memcpy_avx_128(dd - 191, ss - 191);\r\n-\tcase 63: memcpy_avx_32(dd - 63, ss - 63); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 192: memcpy_avx_128(dd - 192, ss - 192);\r\n-\tcase 64: memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 193: memcpy_avx_128(dd - 193, ss - 193);\r\n-\tcase 65: memcpy_avx_64(dd - 65, ss - 65); dd[-1] = ss[-1]; break;\r\n-\tcase 194: memcpy_avx_128(dd - 194, ss - 194);\r\n-\tcase 66: memcpy_avx_64(dd - 66, ss - 66); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\r\n-\tcase 195: memcpy_avx_128(dd - 195, ss - 195);\r\n-\tcase 67: memcpy_avx_64(dd - 67, ss - 67); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 196: memcpy_avx_128(dd - 196, ss - 196);\r\n-\tcase 68: memcpy_avx_64(dd - 68, ss - 68); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\r\n-\tcase 197: memcpy_avx_128(dd - 197, ss - 197);\r\n-\tcase 69: memcpy_avx_64(dd - 69, ss - 69); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 198: memcpy_avx_128(dd - 198, ss - 198);\r\n-\tcase 70: memcpy_avx_64(dd - 70, ss - 70); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 199: memcpy_avx_128(dd - 199, ss - 199);\r\n-\tcase 71: memcpy_avx_64(dd - 71, ss - 71); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 200: memcpy_avx_128(dd - 200, ss - 200);\r\n-\tcase 72: memcpy_avx_64(dd - 72, ss - 72); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\r\n-\tcase 201: memcpy_avx_128(dd - 201, ss - 201);\r\n-\tcase 73: memcpy_avx_64(dd - 73, ss - 73); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 202: memcpy_avx_128(dd - 202, ss - 202);\r\n-\tcase 74: memcpy_avx_64(dd - 74, ss - 74); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 203: memcpy_avx_128(dd - 203, ss - 203);\r\n-\tcase 75: memcpy_avx_64(dd - 75, ss - 75); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 204: memcpy_avx_128(dd - 204, ss - 204);\r\n-\tcase 76: memcpy_avx_64(dd - 76, ss - 76); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 205: memcpy_avx_128(dd - 205, ss - 205);\r\n-\tcase 77: memcpy_avx_64(dd - 77, ss - 77); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 206: memcpy_avx_128(dd - 206, ss - 206);\r\n-\tcase 78: memcpy_avx_64(dd - 78, ss - 78); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 207: memcpy_avx_128(dd - 207, ss - 207);\r\n-\tcase 79: memcpy_avx_64(dd - 79, ss - 79); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 208: memcpy_avx_128(dd - 208, ss - 208);\r\n-\tcase 80: memcpy_avx_64(dd - 80, ss - 80); memcpy_avx_16(dd - 16, ss - 16); break;\r\n-\tcase 209: memcpy_avx_128(dd - 209, ss - 209);\r\n-\tcase 81: memcpy_avx_64(dd - 81, ss - 81); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 210: memcpy_avx_128(dd - 210, ss - 210);\r\n-\tcase 82: memcpy_avx_64(dd - 82, ss - 82); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 211: memcpy_avx_128(dd - 211, ss - 211);\r\n-\tcase 83: memcpy_avx_64(dd - 83, ss - 83); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 212: memcpy_avx_128(dd - 212, ss - 212);\r\n-\tcase 84: memcpy_avx_64(dd - 84, ss - 84); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 213: memcpy_avx_128(dd - 213, ss - 213);\r\n-\tcase 85: memcpy_avx_64(dd - 85, ss - 85); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 214: memcpy_avx_128(dd - 214, ss - 214);\r\n-\tcase 86: memcpy_avx_64(dd - 86, ss - 86); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 215: memcpy_avx_128(dd - 215, ss - 215);\r\n-\tcase 87: memcpy_avx_64(dd - 87, ss - 87); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 216: memcpy_avx_128(dd - 216, ss - 216);\r\n-\tcase 88: memcpy_avx_64(dd - 88, ss - 88); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 217: memcpy_avx_128(dd - 217, ss - 217);\r\n-\tcase 89: memcpy_avx_64(dd - 89, ss - 89); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 218: memcpy_avx_128(dd - 218, ss - 218);\r\n-\tcase 90: memcpy_avx_64(dd - 90, ss - 90); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 219: memcpy_avx_128(dd - 219, ss - 219);\r\n-\tcase 91: memcpy_avx_64(dd - 91, ss - 91); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 220: memcpy_avx_128(dd - 220, ss - 220);\r\n-\tcase 92: memcpy_avx_64(dd - 92, ss - 92); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 221: memcpy_avx_128(dd - 221, ss - 221);\r\n-\tcase 93: memcpy_avx_64(dd - 93, ss - 93); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 222: memcpy_avx_128(dd - 222, ss - 222);\r\n-\tcase 94: memcpy_avx_64(dd - 94, ss - 94); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 223: memcpy_avx_128(dd - 223, ss - 223);\r\n-\tcase 95: memcpy_avx_64(dd - 95, ss - 95); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 224: memcpy_avx_128(dd - 224, ss - 224);\r\n-\tcase 96: memcpy_avx_64(dd - 96, ss - 96); memcpy_avx_32(dd - 32, ss - 32); break;\r\n-\tcase 225: memcpy_avx_128(dd - 225, ss - 225);\r\n-\tcase 97: memcpy_avx_64(dd - 97, ss - 97); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 226: memcpy_avx_128(dd - 226, ss - 226);\r\n-\tcase 98: memcpy_avx_64(dd - 98, ss - 98); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 227: memcpy_avx_128(dd - 227, ss - 227);\r\n-\tcase 99: memcpy_avx_64(dd - 99, ss - 99); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 228: memcpy_avx_128(dd - 228, ss - 228);\r\n-\tcase 100: memcpy_avx_64(dd - 100, ss - 100); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 229: memcpy_avx_128(dd - 229, ss - 229);\r\n-\tcase 101: memcpy_avx_64(dd - 101, ss - 101); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 230: memcpy_avx_128(dd - 230, ss - 230);\r\n-\tcase 102: memcpy_avx_64(dd - 102, ss - 102); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 231: memcpy_avx_128(dd - 231, ss - 231);\r\n-\tcase 103: memcpy_avx_64(dd - 103, ss - 103); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 232: memcpy_avx_128(dd - 232, ss - 232);\r\n-\tcase 104: memcpy_avx_64(dd - 104, ss - 104); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 233: memcpy_avx_128(dd - 233, ss - 233);\r\n-\tcase 105: memcpy_avx_64(dd - 105, ss - 105); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 234: memcpy_avx_128(dd - 234, ss - 234);\r\n-\tcase 106: memcpy_avx_64(dd - 106, ss - 106); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 235: memcpy_avx_128(dd - 235, ss - 235);\r\n-\tcase 107: memcpy_avx_64(dd - 107, ss - 107); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 236: memcpy_avx_128(dd - 236, ss - 236);\r\n-\tcase 108: memcpy_avx_64(dd - 108, ss - 108); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 237: memcpy_avx_128(dd - 237, ss - 237);\r\n-\tcase 109: memcpy_avx_64(dd - 109, ss - 109); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 238: memcpy_avx_128(dd - 238, ss - 238);\r\n-\tcase 110: memcpy_avx_64(dd - 110, ss - 110); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 239: memcpy_avx_128(dd - 239, ss - 239);\r\n-\tcase 111: memcpy_avx_64(dd - 111, ss - 111); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 240: memcpy_avx_128(dd - 240, ss - 240);\r\n-\tcase 112: memcpy_avx_64(dd - 112, ss - 112); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 241: memcpy_avx_128(dd - 241, ss - 241);\r\n-\tcase 113: memcpy_avx_64(dd - 113, ss - 113); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 242: memcpy_avx_128(dd - 242, ss - 242);\r\n-\tcase 114: memcpy_avx_64(dd - 114, ss - 114); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 243: memcpy_avx_128(dd - 243, ss - 243);\r\n-\tcase 115: memcpy_avx_64(dd - 115, ss - 115); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 244: memcpy_avx_128(dd - 244, ss - 244);\r\n-\tcase 116: memcpy_avx_64(dd - 116, ss - 116); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 245: memcpy_avx_128(dd - 245, ss - 245);\r\n-\tcase 117: memcpy_avx_64(dd - 117, ss - 117); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 246: memcpy_avx_128(dd - 246, ss - 246);\r\n-\tcase 118: memcpy_avx_64(dd - 118, ss - 118); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 247: memcpy_avx_128(dd - 247, ss - 247);\r\n-\tcase 119: memcpy_avx_64(dd - 119, ss - 119); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 248: memcpy_avx_128(dd - 248, ss - 248);\r\n-\tcase 120: memcpy_avx_64(dd - 120, ss - 120); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 249: memcpy_avx_128(dd - 249, ss - 249);\r\n-\tcase 121: memcpy_avx_64(dd - 121, ss - 121); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 250: memcpy_avx_128(dd - 250, ss - 250);\r\n-\tcase 122: memcpy_avx_64(dd - 122, ss - 122); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 251: memcpy_avx_128(dd - 251, ss - 251);\r\n-\tcase 123: memcpy_avx_64(dd - 123, ss - 123); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 252: memcpy_avx_128(dd - 252, ss - 252);\r\n-\tcase 124: memcpy_avx_64(dd - 124, ss - 124); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 253: memcpy_avx_128(dd - 253, ss - 253);\r\n-\tcase 125: memcpy_avx_64(dd - 125, ss - 125); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 254: memcpy_avx_128(dd - 254, ss - 254);\r\n-\tcase 126: memcpy_avx_64(dd - 126, ss - 126); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 255: memcpy_avx_128(dd - 255, ss - 255);\r\n-\tcase 127: memcpy_avx_64(dd - 127, ss - 127); memcpy_avx_64(dd - 64, ss - 64); break;\r\n-\tcase 256: memcpy_avx_256(dd - 256, ss - 256); break;\r\n-\t}\r\n-\r\n-\treturn dst;\r\n-}\r\n-\r\n-\r\n-//---------------------------------------------------------------------\r\n-// main routine\r\n-//---------------------------------------------------------------------\r\n-static void* memcpy_fast(void *destination, const void *source, size_t size)\r\n-{\r\n-\tunsigned char *dst = (unsigned char*)destination;\r\n-\tconst unsigned char *src = (const unsigned char*)source;\r\n-\tstatic size_t cachesize = 0x200000; // L3-cache size\r\n-\tsize_t padding;\r\n-\r\n-\t// small memory copy\r\n-\tif (size <= 256) {\r\n-\t\tmemcpy_tiny(dst, src, size);\r\n-\t\t_mm256_zeroupper();\r\n-\t\treturn destination;\r\n-\t}\r\n-\r\n-\t// align destination to 16 bytes boundary\r\n-\tpadding = (32 - (((size_t)dst) & 31)) & 31;\r\n-\r\n-#if 0\r\n-\tif (padding > 0) {\r\n-\t\t__m256i head = _mm256_loadu_si256((const __m256i*)src);\r\n-\t\t_mm256_storeu_si256((__m256i*)dst, head);\r\n-\t\tdst += padding;\r\n-\t\tsrc += padding;\r\n-\t\tsize -= padding;\r\n-\t}\r\n-#else\r\n-\t__m256i head = _mm256_loadu_si256((const __m256i*)src);\r\n-\t_mm256_storeu_si256((__m256i*)dst, head);\r\n-\tdst += padding;\r\n-\tsrc += padding;\r\n-\tsize -= padding;\r\n-#endif\r\n-\r\n-\t// medium size copy\r\n-\tif (size <= cachesize) {\r\n-\t\t__m256i c0, c1, c2, c3, c4, c5, c6, c7;\r\n-\r\n-\t\tfor (; size >= 256; size -= 256) {\r\n-\t\t\tc0 = _mm256_loadu_si256(((const __m256i*)src) + 0);\r\n-\t\t\tc1 = _mm256_loadu_si256(((const __m256i*)src) + 1);\r\n-\t\t\tc2 = _mm256_loadu_si256(((const __m256i*)src) + 2);\r\n-\t\t\tc3 = _mm256_loadu_si256(((const __m256i*)src) + 3);\r\n-\t\t\tc4 = _mm256_loadu_si256(((const __m256i*)src) + 4);\r\n-\t\t\tc5 = _mm256_loadu_si256(((const __m256i*)src) + 5);\r\n-\t\t\tc6 = _mm256_loadu_si256(((const __m256i*)src) + 6);\r\n-\t\t\tc7 = _mm256_loadu_si256(((const __m256i*)src) + 7);\r\n-\t\t\t_mm_prefetch((const char*)(src + 512), _MM_HINT_NTA);\r\n-\t\t\tsrc += 256;\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 0), c0);\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 1), c1);\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 2), c2);\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 3), c3);\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 4), c4);\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 5), c5);\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 6), c6);\r\n-\t\t\t_mm256_storeu_si256((((__m256i*)dst) + 7), c7);\r\n-\t\t\tdst += 256;\r\n-\t\t}\r\n-\t}\r\n-\telse {\t\t// big memory copy\r\n-\t\t__m256i c0, c1, c2, c3, c4, c5, c6, c7;\r\n-\t\t/* __m256i c0, c1, c2, c3, c4, c5, c6, c7; */\r\n-\r\n-\t\t_mm_prefetch((const char*)(src), _MM_HINT_NTA);\r\n-\r\n-\t\tif ((((size_t)src) & 31) == 0) {\t// source aligned\r\n-\t\t\tfor (; size >= 256; size -= 256) {\r\n-\t\t\t\tc0 = _mm256_load_si256(((const __m256i*)src) + 0);\r\n-\t\t\t\tc1 = _mm256_load_si256(((const __m256i*)src) + 1);\r\n-\t\t\t\tc2 = _mm256_load_si256(((const __m256i*)src) + 2);\r\n-\t\t\t\tc3 = _mm256_load_si256(((const __m256i*)src) + 3);\r\n-\t\t\t\tc4 = _mm256_load_si256(((const __m256i*)src) + 4);\r\n-\t\t\t\tc5 = _mm256_load_si256(((const __m256i*)src) + 5);\r\n-\t\t\t\tc6 = _mm256_load_si256(((const __m256i*)src) + 6);\r\n-\t\t\t\tc7 = _mm256_load_si256(((const __m256i*)src) + 7);\r\n-\t\t\t\t_mm_prefetch((const char*)(src + 512), _MM_HINT_NTA);\r\n-\t\t\t\tsrc += 256;\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 0), c0);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 1), c1);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 2), c2);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 3), c3);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 4), c4);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 5), c5);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 6), c6);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 7), c7);\r\n-\t\t\t\tdst += 256;\r\n-\t\t\t}\r\n-\t\t}\r\n-\t\telse {\t\t\t\t\t\t\t// source unaligned\r\n-\t\t\tfor (; size >= 256; size -= 256) {\r\n-\t\t\t\tc0 = _mm256_loadu_si256(((const __m256i*)src) + 0);\r\n-\t\t\t\tc1 = _mm256_loadu_si256(((const __m256i*)src) + 1);\r\n-\t\t\t\tc2 = _mm256_loadu_si256(((const __m256i*)src) + 2);\r\n-\t\t\t\tc3 = _mm256_loadu_si256(((const __m256i*)src) + 3);\r\n-\t\t\t\tc4 = _mm256_loadu_si256(((const __m256i*)src) + 4);\r\n-\t\t\t\tc5 = _mm256_loadu_si256(((const __m256i*)src) + 5);\r\n-\t\t\t\tc6 = _mm256_loadu_si256(((const __m256i*)src) + 6);\r\n-\t\t\t\tc7 = _mm256_loadu_si256(((const __m256i*)src) + 7);\r\n-\t\t\t\t_mm_prefetch((const char*)(src + 512), _MM_HINT_NTA);\r\n-\t\t\t\tsrc += 256;\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 0), c0);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 1), c1);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 2), c2);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 3), c3);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 4), c4);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 5), c5);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 6), c6);\r\n-\t\t\t\t_mm256_stream_si256((((__m256i*)dst) + 7), c7);\r\n-\t\t\t\tdst += 256;\r\n-\t\t\t}\r\n-\t\t}\r\n-\t\t_mm_sfence();\r\n-\t}\r\n-\r\n-\tmemcpy_tiny(dst, src, size);\r\n-\t_mm256_zeroupper();\r\n-\r\n-\treturn destination;\r\n-}\r\n-\r\n-\r\n-#endif\r\n-\r\n-\r\n-\r\ndiff --git a/contrib/FastMemcpy/LICENSE b/contrib/FastMemcpy/LICENSE\ndeleted file mode 100644\nindex c449da6aa8ac..000000000000\n--- a/contrib/FastMemcpy/LICENSE\n+++ /dev/null\n@@ -1,22 +0,0 @@\n-The MIT License (MIT)\n-\n-Copyright (c) 2015 Linwei\n-\n-Permission is hereby granted, free of charge, to any person obtaining a copy\n-of this software and associated documentation files (the \"Software\"), to deal\n-in the Software without restriction, including without limitation the rights\n-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-copies of the Software, and to permit persons to whom the Software is\n-furnished to do so, subject to the following conditions:\n-\n-The above copyright notice and this permission notice shall be included in all\n-copies or substantial portions of the Software.\n-\n-THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-SOFTWARE.\n-\ndiff --git a/contrib/FastMemcpy/README.md b/contrib/FastMemcpy/README.md\ndeleted file mode 100644\nindex e253f6bf5ddf..000000000000\n--- a/contrib/FastMemcpy/README.md\n+++ /dev/null\n@@ -1,20 +0,0 @@\n-Internal implementation of `memcpy` function.\n-\n-It has the following advantages over `libc`-supplied implementation:\n-- it is linked statically, so the function is called directly, not through a `PLT` (procedure lookup table of shared library);\n-- it is linked statically, so the function can have position-dependent code;\n-- your binaries will not depend on `glibc`'s memcpy, that forces dependency on specific symbol version like `memcpy@@GLIBC_2.14` and consequently on specific version of `glibc` library;\n-- you can include `memcpy.h` directly and the function has the chance to be inlined, which is beneficial for small but unknown at compile time sizes of memory regions;\n-- this version of `memcpy` pretend to be faster (in our benchmarks, the difference is within few percents).\n-\n-Currently it uses the implementation from **Linwei** (skywind3000@163.com).\n-Look at https://www.zhihu.com/question/35172305 for discussion.\n-\n-Drawbacks:\n-- only use SSE 2, doesn't use wider (AVX, AVX 512) vector registers when available;\n-- no CPU dispatching; doesn't take into account actual cache size.\n-\n-Also worth to look at:\n-- simple implementation from Facebook: https://github.com/facebook/folly/blob/master/folly/memcpy.S\n-- implementation from Agner Fog: http://www.agner.org/optimize/\n-- glibc source code.\ndiff --git a/contrib/FastMemcpy/memcpy_wrapper.c b/contrib/FastMemcpy/memcpy_wrapper.c\ndeleted file mode 100644\nindex 1f57345980ad..000000000000\n--- a/contrib/FastMemcpy/memcpy_wrapper.c\n+++ /dev/null\n@@ -1,6 +0,0 @@\n-#include \"FastMemcpy.h\"\n-\n-void * memcpy(void * __restrict destination, const void * __restrict source, size_t size)\n-{\n-    return memcpy_fast(destination, source, size);\n-}\ndiff --git a/utils/CMakeLists.txt b/utils/CMakeLists.txt\nindex 8a39d5916126..d38b34f34193 100644\n--- a/utils/CMakeLists.txt\n+++ b/utils/CMakeLists.txt\n@@ -32,6 +32,7 @@ if (NOT DEFINED ENABLE_UTILS OR ENABLE_UTILS)\n     add_subdirectory (db-generator)\n     add_subdirectory (wal-dump)\n     add_subdirectory (check-mysql-binlog)\n+    add_subdirectory (memcpy-bench)\n endif ()\n \n if (ENABLE_CODE_QUALITY)\ndiff --git a/utils/memcpy-bench/CMakeLists.txt b/utils/memcpy-bench/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..54dd0398912a\n--- /dev/null\n+++ b/utils/memcpy-bench/CMakeLists.txt\n@@ -0,0 +1,5 @@\n+enable_language(ASM)\n+add_executable (memcpy-bench memcpy-bench.cpp memcpy_jart.S)\n+#target_compile_options(memcpy-bench PRIVATE -mavx)\n+target_link_libraries(memcpy-bench PRIVATE dbms)\n+\ndiff --git a/utils/memcpy-bench/FastMemcpy.h b/utils/memcpy-bench/FastMemcpy.h\nnew file mode 100644\nindex 000000000000..9c37524443aa\n--- /dev/null\n+++ b/utils/memcpy-bench/FastMemcpy.h\n@@ -0,0 +1,770 @@\n+#pragma once\n+\n+//=====================================================================\n+//\n+// FastMemcpy.c - skywind3000@163.com, 2015\n+//\n+// feature:\n+// 50% speed up in avg. vs standard memcpy (tested in vc2012/gcc5.1)\n+//\n+//=====================================================================\n+\n+#include <stddef.h>\n+#include <stdint.h>\n+#include <emmintrin.h>\n+\n+\n+//---------------------------------------------------------------------\n+// force inline for compilers\n+//---------------------------------------------------------------------\n+#ifndef INLINE\n+#ifdef __GNUC__\n+#if (__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 1))\n+    #define INLINE         __inline__ __attribute__((always_inline))\n+#else\n+    #define INLINE         __inline__\n+#endif\n+#elif defined(_MSC_VER)\n+    #define INLINE __forceinline\n+#elif (defined(__BORLANDC__) || defined(__WATCOMC__))\n+    #define INLINE __inline\n+#else\n+    #define INLINE\n+#endif\n+#endif\n+\n+typedef __attribute__((__aligned__(1))) uint16_t uint16_unaligned_t;\n+typedef __attribute__((__aligned__(1))) uint32_t uint32_unaligned_t;\n+typedef __attribute__((__aligned__(1))) uint64_t uint64_unaligned_t;\n+\n+//---------------------------------------------------------------------\n+// fast copy for different sizes\n+//---------------------------------------------------------------------\n+static INLINE void memcpy_sse2_16(void * __restrict dst, const void * __restrict src)\n+{\n+    __m128i m0 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 0);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 0, m0);\n+}\n+\n+static INLINE void memcpy_sse2_32(void * __restrict dst, const void * __restrict src)\n+{\n+    __m128i m0 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 0);\n+    __m128i m1 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 1);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 0, m0);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 1, m1);\n+}\n+\n+static INLINE void memcpy_sse2_64(void * __restrict dst, const void * __restrict src)\n+{\n+    __m128i m0 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 0);\n+    __m128i m1 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 1);\n+    __m128i m2 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 2);\n+    __m128i m3 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 3);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 0, m0);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 1, m1);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 2, m2);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 3, m3);\n+}\n+\n+static INLINE void memcpy_sse2_128(void * __restrict dst, const void * __restrict src)\n+{\n+    __m128i m0 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 0);\n+    __m128i m1 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 1);\n+    __m128i m2 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 2);\n+    __m128i m3 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 3);\n+    __m128i m4 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 4);\n+    __m128i m5 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 5);\n+    __m128i m6 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 6);\n+    __m128i m7 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 7);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 0, m0);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 1, m1);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 2, m2);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 3, m3);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 4, m4);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 5, m5);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 6, m6);\n+    _mm_storeu_si128((reinterpret_cast<__m128i*>(dst)) + 7, m7);\n+}\n+\n+\n+//---------------------------------------------------------------------\n+// tiny memory copy with jump table optimized\n+//---------------------------------------------------------------------\n+/// Attribute is used to avoid an error with undefined behaviour sanitizer\n+/// ../contrib/FastMemcpy/FastMemcpy.h:91:56: runtime error: applying zero offset to null pointer\n+/// Found by 01307_orc_output_format.sh, cause - ORCBlockInputFormat and external ORC library.\n+__attribute__((__no_sanitize__(\"undefined\"))) static INLINE void *memcpy_tiny(void * __restrict dst, const void * __restrict src, size_t size)\n+{\n+    unsigned char *dd = ((unsigned char*)dst) + size;\n+    const unsigned char *ss = ((const unsigned char*)src) + size;\n+\n+    switch (size)\n+    {\n+    case 64:\n+        memcpy_sse2_64(dd - 64, ss - 64);\n+        [[fallthrough]];\n+    case 0:\n+        break;\n+\n+    case 65:\n+        memcpy_sse2_64(dd - 65, ss - 65);\n+        [[fallthrough]];\n+    case 1:\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 66:\n+        memcpy_sse2_64(dd - 66, ss - 66);\n+        [[fallthrough]];\n+    case 2:\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 67:\n+        memcpy_sse2_64(dd - 67, ss - 67);\n+        [[fallthrough]];\n+    case 3:\n+        *((uint16_unaligned_t*)(dd - 3)) = *((const uint16_unaligned_t*)(ss - 3));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 68:\n+        memcpy_sse2_64(dd - 68, ss - 68);\n+        [[fallthrough]];\n+    case 4:\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 69:\n+        memcpy_sse2_64(dd - 69, ss - 69);\n+        [[fallthrough]];\n+    case 5:\n+        *((uint32_unaligned_t*)(dd - 5)) = *((const uint32_unaligned_t*)(ss - 5));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 70:\n+        memcpy_sse2_64(dd - 70, ss - 70);\n+        [[fallthrough]];\n+    case 6:\n+        *((uint32_unaligned_t*)(dd - 6)) = *((const uint32_unaligned_t*)(ss - 6));\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 71:\n+        memcpy_sse2_64(dd - 71, ss - 71);\n+        [[fallthrough]];\n+    case 7:\n+        *((uint32_unaligned_t*)(dd - 7)) = *((const uint32_unaligned_t*)(ss - 7));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 72:\n+        memcpy_sse2_64(dd - 72, ss - 72);\n+        [[fallthrough]];\n+    case 8:\n+        *((uint64_unaligned_t*)(dd - 8)) = *((const uint64_unaligned_t*)(ss - 8));\n+        break;\n+\n+    case 73:\n+        memcpy_sse2_64(dd - 73, ss - 73);\n+        [[fallthrough]];\n+    case 9:\n+        *((uint64_unaligned_t*)(dd - 9)) = *((const uint64_unaligned_t*)(ss - 9));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 74:\n+        memcpy_sse2_64(dd - 74, ss - 74);\n+        [[fallthrough]];\n+    case 10:\n+        *((uint64_unaligned_t*)(dd - 10)) = *((const uint64_unaligned_t*)(ss - 10));\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 75:\n+        memcpy_sse2_64(dd - 75, ss - 75);\n+        [[fallthrough]];\n+    case 11:\n+        *((uint64_unaligned_t*)(dd - 11)) = *((const uint64_unaligned_t*)(ss - 11));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 76:\n+        memcpy_sse2_64(dd - 76, ss - 76);\n+        [[fallthrough]];\n+    case 12:\n+        *((uint64_unaligned_t*)(dd - 12)) = *((const uint64_unaligned_t*)(ss - 12));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 77:\n+        memcpy_sse2_64(dd - 77, ss - 77);\n+        [[fallthrough]];\n+    case 13:\n+        *((uint64_unaligned_t*)(dd - 13)) = *((const uint64_unaligned_t*)(ss - 13));\n+        *((uint32_unaligned_t*)(dd - 5)) = *((const uint32_unaligned_t*)(ss - 5));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 78:\n+        memcpy_sse2_64(dd - 78, ss - 78);\n+        [[fallthrough]];\n+    case 14:\n+        *((uint64_unaligned_t*)(dd - 14)) = *((const uint64_unaligned_t*)(ss - 14));\n+        *((uint64_unaligned_t*)(dd - 8)) = *((const uint64_unaligned_t*)(ss - 8));\n+        break;\n+\n+    case 79:\n+        memcpy_sse2_64(dd - 79, ss - 79);\n+        [[fallthrough]];\n+    case 15:\n+        *((uint64_unaligned_t*)(dd - 15)) = *((const uint64_unaligned_t*)(ss - 15));\n+        *((uint64_unaligned_t*)(dd - 8)) = *((const uint64_unaligned_t*)(ss - 8));\n+        break;\n+\n+    case 80:\n+        memcpy_sse2_64(dd - 80, ss - 80);\n+        [[fallthrough]];\n+    case 16:\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 81:\n+        memcpy_sse2_64(dd - 81, ss - 81);\n+        [[fallthrough]];\n+    case 17:\n+        memcpy_sse2_16(dd - 17, ss - 17);\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 82:\n+        memcpy_sse2_64(dd - 82, ss - 82);\n+        [[fallthrough]];\n+    case 18:\n+        memcpy_sse2_16(dd - 18, ss - 18);\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 83:\n+        memcpy_sse2_64(dd - 83, ss - 83);\n+        [[fallthrough]];\n+    case 19:\n+        memcpy_sse2_16(dd - 19, ss - 19);\n+        *((uint16_unaligned_t*)(dd - 3)) = *((const uint16_unaligned_t*)(ss - 3));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 84:\n+        memcpy_sse2_64(dd - 84, ss - 84);\n+        [[fallthrough]];\n+    case 20:\n+        memcpy_sse2_16(dd - 20, ss - 20);\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 85:\n+        memcpy_sse2_64(dd - 85, ss - 85);\n+        [[fallthrough]];\n+    case 21:\n+        memcpy_sse2_16(dd - 21, ss - 21);\n+        *((uint32_unaligned_t*)(dd - 5)) = *((const uint32_unaligned_t*)(ss - 5));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 86:\n+        memcpy_sse2_64(dd - 86, ss - 86);\n+        [[fallthrough]];\n+    case 22:\n+        memcpy_sse2_16(dd - 22, ss - 22);\n+        *((uint32_unaligned_t*)(dd - 6)) = *((const uint32_unaligned_t*)(ss - 6));\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 87:\n+        memcpy_sse2_64(dd - 87, ss - 87);\n+        [[fallthrough]];\n+    case 23:\n+        memcpy_sse2_16(dd - 23, ss - 23);\n+        *((uint32_unaligned_t*)(dd - 7)) = *((const uint32_unaligned_t*)(ss - 7));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 88:\n+        memcpy_sse2_64(dd - 88, ss - 88);\n+        [[fallthrough]];\n+    case 24:\n+        memcpy_sse2_16(dd - 24, ss - 24);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 89:\n+        memcpy_sse2_64(dd - 89, ss - 89);\n+        [[fallthrough]];\n+    case 25:\n+        memcpy_sse2_16(dd - 25, ss - 25);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 90:\n+        memcpy_sse2_64(dd - 90, ss - 90);\n+        [[fallthrough]];\n+    case 26:\n+        memcpy_sse2_16(dd - 26, ss - 26);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 91:\n+        memcpy_sse2_64(dd - 91, ss - 91);\n+        [[fallthrough]];\n+    case 27:\n+        memcpy_sse2_16(dd - 27, ss - 27);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 92:\n+        memcpy_sse2_64(dd - 92, ss - 92);\n+        [[fallthrough]];\n+    case 28:\n+        memcpy_sse2_16(dd - 28, ss - 28);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 93:\n+        memcpy_sse2_64(dd - 93, ss - 93);\n+        [[fallthrough]];\n+    case 29:\n+        memcpy_sse2_16(dd - 29, ss - 29);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 94:\n+        memcpy_sse2_64(dd - 94, ss - 94);\n+        [[fallthrough]];\n+    case 30:\n+        memcpy_sse2_16(dd - 30, ss - 30);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 95:\n+        memcpy_sse2_64(dd - 95, ss - 95);\n+        [[fallthrough]];\n+    case 31:\n+        memcpy_sse2_16(dd - 31, ss - 31);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 96:\n+        memcpy_sse2_64(dd - 96, ss - 96);\n+        [[fallthrough]];\n+    case 32:\n+        memcpy_sse2_32(dd - 32, ss - 32);\n+        break;\n+\n+    case 97:\n+        memcpy_sse2_64(dd - 97, ss - 97);\n+        [[fallthrough]];\n+    case 33:\n+        memcpy_sse2_32(dd - 33, ss - 33);\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 98:\n+        memcpy_sse2_64(dd - 98, ss - 98);\n+        [[fallthrough]];\n+    case 34:\n+        memcpy_sse2_32(dd - 34, ss - 34);\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 99:\n+        memcpy_sse2_64(dd - 99, ss - 99);\n+        [[fallthrough]];\n+    case 35:\n+        memcpy_sse2_32(dd - 35, ss - 35);\n+        *((uint16_unaligned_t*)(dd - 3)) = *((const uint16_unaligned_t*)(ss - 3));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 100:\n+        memcpy_sse2_64(dd - 100, ss - 100);\n+        [[fallthrough]];\n+    case 36:\n+        memcpy_sse2_32(dd - 36, ss - 36);\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 101:\n+        memcpy_sse2_64(dd - 101, ss - 101);\n+        [[fallthrough]];\n+    case 37:\n+        memcpy_sse2_32(dd - 37, ss - 37);\n+        *((uint32_unaligned_t*)(dd - 5)) = *((const uint32_unaligned_t*)(ss - 5));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 102:\n+        memcpy_sse2_64(dd - 102, ss - 102);\n+        [[fallthrough]];\n+    case 38:\n+        memcpy_sse2_32(dd - 38, ss - 38);\n+        *((uint32_unaligned_t*)(dd - 6)) = *((const uint32_unaligned_t*)(ss - 6));\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 103:\n+        memcpy_sse2_64(dd - 103, ss - 103);\n+        [[fallthrough]];\n+    case 39:\n+        memcpy_sse2_32(dd - 39, ss - 39);\n+        *((uint32_unaligned_t*)(dd - 7)) = *((const uint32_unaligned_t*)(ss - 7));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 104:\n+        memcpy_sse2_64(dd - 104, ss - 104);\n+        [[fallthrough]];\n+    case 40:\n+        memcpy_sse2_32(dd - 40, ss - 40);\n+        *((uint64_unaligned_t*)(dd - 8)) = *((const uint64_unaligned_t*)(ss - 8));\n+        break;\n+\n+    case 105:\n+        memcpy_sse2_64(dd - 105, ss - 105);\n+        [[fallthrough]];\n+    case 41:\n+        memcpy_sse2_32(dd - 41, ss - 41);\n+        *((uint64_unaligned_t*)(dd - 9)) = *((const uint64_unaligned_t*)(ss - 9));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 106:\n+        memcpy_sse2_64(dd - 106, ss - 106);\n+        [[fallthrough]];\n+    case 42:\n+        memcpy_sse2_32(dd - 42, ss - 42);\n+        *((uint64_unaligned_t*)(dd - 10)) = *((const uint64_unaligned_t*)(ss - 10));\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 107:\n+        memcpy_sse2_64(dd - 107, ss - 107);\n+        [[fallthrough]];\n+    case 43:\n+        memcpy_sse2_32(dd - 43, ss - 43);\n+        *((uint64_unaligned_t*)(dd - 11)) = *((const uint64_unaligned_t*)(ss - 11));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 108:\n+        memcpy_sse2_64(dd - 108, ss - 108);\n+        [[fallthrough]];\n+    case 44:\n+        memcpy_sse2_32(dd - 44, ss - 44);\n+        *((uint64_unaligned_t*)(dd - 12)) = *((const uint64_unaligned_t*)(ss - 12));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 109:\n+        memcpy_sse2_64(dd - 109, ss - 109);\n+        [[fallthrough]];\n+    case 45:\n+        memcpy_sse2_32(dd - 45, ss - 45);\n+        *((uint64_unaligned_t*)(dd - 13)) = *((const uint64_unaligned_t*)(ss - 13));\n+        *((uint32_unaligned_t*)(dd - 5)) = *((const uint32_unaligned_t*)(ss - 5));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 110:\n+        memcpy_sse2_64(dd - 110, ss - 110);\n+        [[fallthrough]];\n+    case 46:\n+        memcpy_sse2_32(dd - 46, ss - 46);\n+        *((uint64_unaligned_t*)(dd - 14)) = *((const uint64_unaligned_t*)(ss - 14));\n+        *((uint64_unaligned_t*)(dd - 8)) = *((const uint64_unaligned_t*)(ss - 8));\n+        break;\n+\n+    case 111:\n+        memcpy_sse2_64(dd - 111, ss - 111);\n+        [[fallthrough]];\n+    case 47:\n+        memcpy_sse2_32(dd - 47, ss - 47);\n+        *((uint64_unaligned_t*)(dd - 15)) = *((const uint64_unaligned_t*)(ss - 15));\n+        *((uint64_unaligned_t*)(dd - 8)) = *((const uint64_unaligned_t*)(ss - 8));\n+        break;\n+\n+    case 112:\n+        memcpy_sse2_64(dd - 112, ss - 112);\n+        [[fallthrough]];\n+    case 48:\n+        memcpy_sse2_32(dd - 48, ss - 48);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 113:\n+        memcpy_sse2_64(dd - 113, ss - 113);\n+        [[fallthrough]];\n+    case 49:\n+        memcpy_sse2_32(dd - 49, ss - 49);\n+        memcpy_sse2_16(dd - 17, ss - 17);\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 114:\n+        memcpy_sse2_64(dd - 114, ss - 114);\n+        [[fallthrough]];\n+    case 50:\n+        memcpy_sse2_32(dd - 50, ss - 50);\n+        memcpy_sse2_16(dd - 18, ss - 18);\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 115:\n+        memcpy_sse2_64(dd - 115, ss - 115);\n+        [[fallthrough]];\n+    case 51:\n+        memcpy_sse2_32(dd - 51, ss - 51);\n+        memcpy_sse2_16(dd - 19, ss - 19);\n+        *((uint16_unaligned_t*)(dd - 3)) = *((const uint16_unaligned_t*)(ss - 3));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 116:\n+        memcpy_sse2_64(dd - 116, ss - 116);\n+        [[fallthrough]];\n+    case 52:\n+        memcpy_sse2_32(dd - 52, ss - 52);\n+        memcpy_sse2_16(dd - 20, ss - 20);\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 117:\n+        memcpy_sse2_64(dd - 117, ss - 117);\n+        [[fallthrough]];\n+    case 53:\n+        memcpy_sse2_32(dd - 53, ss - 53);\n+        memcpy_sse2_16(dd - 21, ss - 21);\n+        *((uint32_unaligned_t*)(dd - 5)) = *((const uint32_unaligned_t*)(ss - 5));\n+        dd[-1] = ss[-1];\n+        break;\n+\n+    case 118:\n+        memcpy_sse2_64(dd - 118, ss - 118);\n+        [[fallthrough]];\n+    case 54:\n+        memcpy_sse2_32(dd - 54, ss - 54);\n+        memcpy_sse2_16(dd - 22, ss - 22);\n+        *((uint32_unaligned_t*)(dd - 6)) = *((const uint32_unaligned_t*)(ss - 6));\n+        *((uint16_unaligned_t*)(dd - 2)) = *((const uint16_unaligned_t*)(ss - 2));\n+        break;\n+\n+    case 119:\n+        memcpy_sse2_64(dd - 119, ss - 119);\n+        [[fallthrough]];\n+    case 55:\n+        memcpy_sse2_32(dd - 55, ss - 55);\n+        memcpy_sse2_16(dd - 23, ss - 23);\n+        *((uint32_unaligned_t*)(dd - 7)) = *((const uint32_unaligned_t*)(ss - 7));\n+        *((uint32_unaligned_t*)(dd - 4)) = *((const uint32_unaligned_t*)(ss - 4));\n+        break;\n+\n+    case 120:\n+        memcpy_sse2_64(dd - 120, ss - 120);\n+        [[fallthrough]];\n+    case 56:\n+        memcpy_sse2_32(dd - 56, ss - 56);\n+        memcpy_sse2_16(dd - 24, ss - 24);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 121:\n+        memcpy_sse2_64(dd - 121, ss - 121);\n+        [[fallthrough]];\n+    case 57:\n+        memcpy_sse2_32(dd - 57, ss - 57);\n+        memcpy_sse2_16(dd - 25, ss - 25);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 122:\n+        memcpy_sse2_64(dd - 122, ss - 122);\n+        [[fallthrough]];\n+    case 58:\n+        memcpy_sse2_32(dd - 58, ss - 58);\n+        memcpy_sse2_16(dd - 26, ss - 26);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 123:\n+        memcpy_sse2_64(dd - 123, ss - 123);\n+        [[fallthrough]];\n+    case 59:\n+        memcpy_sse2_32(dd - 59, ss - 59);\n+        memcpy_sse2_16(dd - 27, ss - 27);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 124:\n+        memcpy_sse2_64(dd - 124, ss - 124);\n+        [[fallthrough]];\n+    case 60:\n+        memcpy_sse2_32(dd - 60, ss - 60);\n+        memcpy_sse2_16(dd - 28, ss - 28);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 125:\n+        memcpy_sse2_64(dd - 125, ss - 125);\n+        [[fallthrough]];\n+    case 61:\n+        memcpy_sse2_32(dd - 61, ss - 61);\n+        memcpy_sse2_16(dd - 29, ss - 29);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 126:\n+        memcpy_sse2_64(dd - 126, ss - 126);\n+        [[fallthrough]];\n+    case 62:\n+        memcpy_sse2_32(dd - 62, ss - 62);\n+        memcpy_sse2_16(dd - 30, ss - 30);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 127:\n+        memcpy_sse2_64(dd - 127, ss - 127);\n+        [[fallthrough]];\n+    case 63:\n+        memcpy_sse2_32(dd - 63, ss - 63);\n+        memcpy_sse2_16(dd - 31, ss - 31);\n+        memcpy_sse2_16(dd - 16, ss - 16);\n+        break;\n+\n+    case 128:\n+        memcpy_sse2_128(dd - 128, ss - 128);\n+        break;\n+    }\n+\n+    return dst;\n+}\n+\n+\n+//---------------------------------------------------------------------\n+// main routine\n+//---------------------------------------------------------------------\n+void* memcpy_fast_sse(void * __restrict destination, const void * __restrict source, size_t size)\n+{\n+    unsigned char *dst = (unsigned char*)destination;\n+    const unsigned char *src = (const unsigned char*)source;\n+    static size_t cachesize = 0x200000; // L2-cache size\n+    size_t padding;\n+\n+    // small memory copy\n+    if (size <= 128)\n+{\n+        return memcpy_tiny(dst, src, size);\n+    }\n+\n+    // align destination to 16 bytes boundary\n+    padding = (16 - (((size_t)dst) & 15)) & 15;\n+\n+    if (padding > 0)\n+    {\n+        __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+\n+    // medium size copy\n+    if (size <= cachesize)\n+    {\n+        __m128i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+        for (; size >= 128; size -= 128)\n+        {\n+            c0 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 0);\n+            c1 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 1);\n+            c2 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 2);\n+            c3 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 3);\n+            c4 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 4);\n+            c5 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 5);\n+            c6 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 6);\n+            c7 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 7);\n+            _mm_prefetch((const char*)(src + 256), _MM_HINT_NTA);\n+            src += 128;\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 0), c0);\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 1), c1);\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 2), c2);\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 3), c3);\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 4), c4);\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 5), c5);\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 6), c6);\n+            _mm_store_si128(((reinterpret_cast<__m128i*>(dst)) + 7), c7);\n+            dst += 128;\n+        }\n+    }\n+    else\n+    {        // big memory copy\n+        __m128i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+        _mm_prefetch((const char*)(src), _MM_HINT_NTA);\n+\n+        if ((((size_t)src) & 15) == 0)\n+        {    // source aligned\n+            for (; size >= 128; size -= 128)\n+            {\n+                c0 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 0);\n+                c1 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 1);\n+                c2 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 2);\n+                c3 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 3);\n+                c4 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 4);\n+                c5 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 5);\n+                c6 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 6);\n+                c7 = _mm_load_si128((reinterpret_cast<const __m128i*>(src)) + 7);\n+                _mm_prefetch((const char*)(src + 256), _MM_HINT_NTA);\n+                src += 128;\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 0), c0);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 1), c1);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 2), c2);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 3), c3);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 4), c4);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 5), c5);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 6), c6);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 7), c7);\n+                dst += 128;\n+            }\n+        }\n+        else\n+        {                            // source unaligned\n+            for (; size >= 128; size -= 128)\n+            {\n+                c0 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 0);\n+                c1 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 1);\n+                c2 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 2);\n+                c3 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 3);\n+                c4 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 4);\n+                c5 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 5);\n+                c6 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 6);\n+                c7 = _mm_loadu_si128((reinterpret_cast<const __m128i*>(src)) + 7);\n+                _mm_prefetch((const char*)(src + 256), _MM_HINT_NTA);\n+                src += 128;\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 0), c0);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 1), c1);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 2), c2);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 3), c3);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 4), c4);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 5), c5);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 6), c6);\n+                _mm_stream_si128(((reinterpret_cast<__m128i*>(dst)) + 7), c7);\n+                dst += 128;\n+            }\n+        }\n+        _mm_sfence();\n+    }\n+\n+    memcpy_tiny(dst, src, size);\n+\n+    return destination;\n+}\ndiff --git a/utils/memcpy-bench/FastMemcpy_Avx.h b/utils/memcpy-bench/FastMemcpy_Avx.h\nnew file mode 100644\nindex 000000000000..ee7d4e19536f\n--- /dev/null\n+++ b/utils/memcpy-bench/FastMemcpy_Avx.h\n@@ -0,0 +1,496 @@\n+#pragma once\n+\n+//=====================================================================\n+//\n+// FastMemcpy.c - skywind3000@163.com, 2015\n+//\n+// feature:\n+// 50% speed up in avg. vs standard memcpy (tested in vc2012/gcc5.1)\n+//\n+//=====================================================================\n+\n+#include <stddef.h>\n+#include <stdint.h>\n+#include <immintrin.h>\n+\n+\n+//---------------------------------------------------------------------\n+// force inline for compilers\n+//---------------------------------------------------------------------\n+#ifndef INLINE\n+#ifdef __GNUC__\n+#if (__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 1))\n+    #define INLINE         __inline__ __attribute__((always_inline))\n+#else\n+    #define INLINE         __inline__\n+#endif\n+#elif defined(_MSC_VER)\n+    #define INLINE __forceinline\n+#elif (defined(__BORLANDC__) || defined(__WATCOMC__))\n+    #define INLINE __inline\n+#else\n+    #define INLINE\n+#endif\n+#endif\n+\n+\n+//---------------------------------------------------------------------\n+// fast copy for different sizes\n+//---------------------------------------------------------------------\n+static INLINE void memcpy_avx_16(void * __restrict dst, const void * __restrict src)\n+{\n+#if 1\n+    __m128i m0 = _mm_loadu_si128(((const __m128i*)src) + 0);\n+    _mm_storeu_si128(((__m128i*)dst) + 0, m0);\n+#else\n+    *((uint64_t*)((char*)dst + 0)) = *((uint64_t*)((const char*)src + 0));\n+    *((uint64_t*)((char*)dst + 8)) = *((uint64_t*)((const char*)src + 8));\n+#endif\n+}\n+\n+static INLINE void memcpy_avx_32(void *dst, const void *src)\n+{\n+    __m256i m0 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 0, m0);\n+}\n+\n+static INLINE void memcpy_avx_64(void *dst, const void *src)\n+{\n+    __m256i m0 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+    __m256i m1 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 1);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 0, m0);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 1, m1);\n+}\n+\n+static INLINE void memcpy_avx_128(void *dst, const void *src)\n+{\n+    __m256i m0 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+    __m256i m1 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 1);\n+    __m256i m2 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 2);\n+    __m256i m3 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 3);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 0, m0);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 1, m1);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 2, m2);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 3, m3);\n+}\n+\n+static INLINE void memcpy_avx_256(void *dst, const void *src)\n+{\n+    __m256i m0 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+    __m256i m1 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 1);\n+    __m256i m2 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 2);\n+    __m256i m3 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 3);\n+    __m256i m4 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 4);\n+    __m256i m5 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 5);\n+    __m256i m6 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 6);\n+    __m256i m7 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 7);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 0, m0);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 1, m1);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 2, m2);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 3, m3);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 4, m4);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 5, m5);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 6, m6);\n+    _mm256_storeu_si256((reinterpret_cast<__m256i*>(dst)) + 7, m7);\n+}\n+\n+\n+//---------------------------------------------------------------------\n+// tiny memory copy with jump table optimized\n+//---------------------------------------------------------------------\n+static INLINE void *memcpy_tiny_avx(void * __restrict dst, const void * __restrict src, size_t size)\n+{\n+    unsigned char *dd = reinterpret_cast<unsigned char *>(dst) + size;\n+    const unsigned char *ss = reinterpret_cast<const unsigned char*>(src) + size;\n+\n+    switch (size)\n+    {\n+    case 128: memcpy_avx_128(dd - 128, ss - 128); [[fallthrough]];\n+    case 0:  break;\n+    case 129: memcpy_avx_128(dd - 129, ss - 129); [[fallthrough]];\n+    case 1: dd[-1] = ss[-1]; break;\n+    case 130: memcpy_avx_128(dd - 130, ss - 130); [[fallthrough]];\n+    case 2: *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\n+    case 131: memcpy_avx_128(dd - 131, ss - 131); [[fallthrough]];\n+    case 3: *((uint16_t*)(dd - 3)) = *((uint16_t*)(ss - 3)); dd[-1] = ss[-1]; break;\n+    case 132: memcpy_avx_128(dd - 132, ss - 132); [[fallthrough]];\n+    case 4: *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 133: memcpy_avx_128(dd - 133, ss - 133); [[fallthrough]];\n+    case 5: *((uint32_t*)(dd - 5)) = *((uint32_t*)(ss - 5)); dd[-1] = ss[-1]; break;\n+    case 134: memcpy_avx_128(dd - 134, ss - 134); [[fallthrough]];\n+    case 6: *((uint32_t*)(dd - 6)) = *((uint32_t*)(ss - 6)); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\n+    case 135: memcpy_avx_128(dd - 135, ss - 135); [[fallthrough]];\n+    case 7: *((uint32_t*)(dd - 7)) = *((uint32_t*)(ss - 7)); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 136: memcpy_avx_128(dd - 136, ss - 136); [[fallthrough]];\n+    case 8: *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 137: memcpy_avx_128(dd - 137, ss - 137); [[fallthrough]];\n+    case 9: *((uint64_t*)(dd - 9)) = *((uint64_t*)(ss - 9)); dd[-1] = ss[-1]; break;\n+    case 138: memcpy_avx_128(dd - 138, ss - 138); [[fallthrough]];\n+    case 10: *((uint64_t*)(dd - 10)) = *((uint64_t*)(ss - 10)); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\n+    case 139: memcpy_avx_128(dd - 139, ss - 139); [[fallthrough]];\n+    case 11: *((uint64_t*)(dd - 11)) = *((uint64_t*)(ss - 11)); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 140: memcpy_avx_128(dd - 140, ss - 140); [[fallthrough]];\n+    case 12: *((uint64_t*)(dd - 12)) = *((uint64_t*)(ss - 12)); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 141: memcpy_avx_128(dd - 141, ss - 141); [[fallthrough]];\n+    case 13: *((uint64_t*)(dd - 13)) = *((uint64_t*)(ss - 13)); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 142: memcpy_avx_128(dd - 142, ss - 142); [[fallthrough]];\n+    case 14: *((uint64_t*)(dd - 14)) = *((uint64_t*)(ss - 14)); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 143: memcpy_avx_128(dd - 143, ss - 143); [[fallthrough]];\n+    case 15: *((uint64_t*)(dd - 15)) = *((uint64_t*)(ss - 15)); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 144: memcpy_avx_128(dd - 144, ss - 144); [[fallthrough]];\n+    case 16: memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 145: memcpy_avx_128(dd - 145, ss - 145); [[fallthrough]];\n+    case 17: memcpy_avx_16(dd - 17, ss - 17); dd[-1] = ss[-1]; break;\n+    case 146: memcpy_avx_128(dd - 146, ss - 146); [[fallthrough]];\n+    case 18: memcpy_avx_16(dd - 18, ss - 18); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\n+    case 147: memcpy_avx_128(dd - 147, ss - 147); [[fallthrough]];\n+    case 19: memcpy_avx_16(dd - 19, ss - 19); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 148: memcpy_avx_128(dd - 148, ss - 148); [[fallthrough]];\n+    case 20: memcpy_avx_16(dd - 20, ss - 20); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 149: memcpy_avx_128(dd - 149, ss - 149); [[fallthrough]];\n+    case 21: memcpy_avx_16(dd - 21, ss - 21); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 150: memcpy_avx_128(dd - 150, ss - 150); [[fallthrough]];\n+    case 22: memcpy_avx_16(dd - 22, ss - 22); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 151: memcpy_avx_128(dd - 151, ss - 151); [[fallthrough]];\n+    case 23: memcpy_avx_16(dd - 23, ss - 23); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 152: memcpy_avx_128(dd - 152, ss - 152); [[fallthrough]];\n+    case 24: memcpy_avx_16(dd - 24, ss - 24); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 153: memcpy_avx_128(dd - 153, ss - 153); [[fallthrough]];\n+    case 25: memcpy_avx_16(dd - 25, ss - 25); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 154: memcpy_avx_128(dd - 154, ss - 154); [[fallthrough]];\n+    case 26: memcpy_avx_16(dd - 26, ss - 26); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 155: memcpy_avx_128(dd - 155, ss - 155); [[fallthrough]];\n+    case 27: memcpy_avx_16(dd - 27, ss - 27); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 156: memcpy_avx_128(dd - 156, ss - 156); [[fallthrough]];\n+    case 28: memcpy_avx_16(dd - 28, ss - 28); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 157: memcpy_avx_128(dd - 157, ss - 157); [[fallthrough]];\n+    case 29: memcpy_avx_16(dd - 29, ss - 29); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 158: memcpy_avx_128(dd - 158, ss - 158); [[fallthrough]];\n+    case 30: memcpy_avx_16(dd - 30, ss - 30); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 159: memcpy_avx_128(dd - 159, ss - 159); [[fallthrough]];\n+    case 31: memcpy_avx_16(dd - 31, ss - 31); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 160: memcpy_avx_128(dd - 160, ss - 160); [[fallthrough]];\n+    case 32: memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 161: memcpy_avx_128(dd - 161, ss - 161); [[fallthrough]];\n+    case 33: memcpy_avx_32(dd - 33, ss - 33); dd[-1] = ss[-1]; break;\n+    case 162: memcpy_avx_128(dd - 162, ss - 162); [[fallthrough]];\n+    case 34: memcpy_avx_32(dd - 34, ss - 34); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\n+    case 163: memcpy_avx_128(dd - 163, ss - 163); [[fallthrough]];\n+    case 35: memcpy_avx_32(dd - 35, ss - 35); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 164: memcpy_avx_128(dd - 164, ss - 164); [[fallthrough]];\n+    case 36: memcpy_avx_32(dd - 36, ss - 36); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 165: memcpy_avx_128(dd - 165, ss - 165); [[fallthrough]];\n+    case 37: memcpy_avx_32(dd - 37, ss - 37); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 166: memcpy_avx_128(dd - 166, ss - 166); [[fallthrough]];\n+    case 38: memcpy_avx_32(dd - 38, ss - 38); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 167: memcpy_avx_128(dd - 167, ss - 167); [[fallthrough]];\n+    case 39: memcpy_avx_32(dd - 39, ss - 39); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 168: memcpy_avx_128(dd - 168, ss - 168); [[fallthrough]];\n+    case 40: memcpy_avx_32(dd - 40, ss - 40); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 169: memcpy_avx_128(dd - 169, ss - 169); [[fallthrough]];\n+    case 41: memcpy_avx_32(dd - 41, ss - 41); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 170: memcpy_avx_128(dd - 170, ss - 170); [[fallthrough]];\n+    case 42: memcpy_avx_32(dd - 42, ss - 42); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 171: memcpy_avx_128(dd - 171, ss - 171); [[fallthrough]];\n+    case 43: memcpy_avx_32(dd - 43, ss - 43); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 172: memcpy_avx_128(dd - 172, ss - 172); [[fallthrough]];\n+    case 44: memcpy_avx_32(dd - 44, ss - 44); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 173: memcpy_avx_128(dd - 173, ss - 173); [[fallthrough]];\n+    case 45: memcpy_avx_32(dd - 45, ss - 45); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 174: memcpy_avx_128(dd - 174, ss - 174); [[fallthrough]];\n+    case 46: memcpy_avx_32(dd - 46, ss - 46); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 175: memcpy_avx_128(dd - 175, ss - 175); [[fallthrough]];\n+    case 47: memcpy_avx_32(dd - 47, ss - 47); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 176: memcpy_avx_128(dd - 176, ss - 176); [[fallthrough]];\n+    case 48: memcpy_avx_32(dd - 48, ss - 48); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 177: memcpy_avx_128(dd - 177, ss - 177); [[fallthrough]];\n+    case 49: memcpy_avx_32(dd - 49, ss - 49); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 178: memcpy_avx_128(dd - 178, ss - 178); [[fallthrough]];\n+    case 50: memcpy_avx_32(dd - 50, ss - 50); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 179: memcpy_avx_128(dd - 179, ss - 179); [[fallthrough]];\n+    case 51: memcpy_avx_32(dd - 51, ss - 51); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 180: memcpy_avx_128(dd - 180, ss - 180); [[fallthrough]];\n+    case 52: memcpy_avx_32(dd - 52, ss - 52); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 181: memcpy_avx_128(dd - 181, ss - 181); [[fallthrough]];\n+    case 53: memcpy_avx_32(dd - 53, ss - 53); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 182: memcpy_avx_128(dd - 182, ss - 182); [[fallthrough]];\n+    case 54: memcpy_avx_32(dd - 54, ss - 54); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 183: memcpy_avx_128(dd - 183, ss - 183); [[fallthrough]];\n+    case 55: memcpy_avx_32(dd - 55, ss - 55); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 184: memcpy_avx_128(dd - 184, ss - 184); [[fallthrough]];\n+    case 56: memcpy_avx_32(dd - 56, ss - 56); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 185: memcpy_avx_128(dd - 185, ss - 185); [[fallthrough]];\n+    case 57: memcpy_avx_32(dd - 57, ss - 57); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 186: memcpy_avx_128(dd - 186, ss - 186); [[fallthrough]];\n+    case 58: memcpy_avx_32(dd - 58, ss - 58); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 187: memcpy_avx_128(dd - 187, ss - 187); [[fallthrough]];\n+    case 59: memcpy_avx_32(dd - 59, ss - 59); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 188: memcpy_avx_128(dd - 188, ss - 188); [[fallthrough]];\n+    case 60: memcpy_avx_32(dd - 60, ss - 60); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 189: memcpy_avx_128(dd - 189, ss - 189); [[fallthrough]];\n+    case 61: memcpy_avx_32(dd - 61, ss - 61); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 190: memcpy_avx_128(dd - 190, ss - 190); [[fallthrough]];\n+    case 62: memcpy_avx_32(dd - 62, ss - 62); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 191: memcpy_avx_128(dd - 191, ss - 191); [[fallthrough]];\n+    case 63: memcpy_avx_32(dd - 63, ss - 63); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 192: memcpy_avx_128(dd - 192, ss - 192); [[fallthrough]];\n+    case 64: memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 193: memcpy_avx_128(dd - 193, ss - 193); [[fallthrough]];\n+    case 65: memcpy_avx_64(dd - 65, ss - 65); dd[-1] = ss[-1]; break;\n+    case 194: memcpy_avx_128(dd - 194, ss - 194); [[fallthrough]];\n+    case 66: memcpy_avx_64(dd - 66, ss - 66); *((uint16_t*)(dd - 2)) = *((uint16_t*)(ss - 2)); break;\n+    case 195: memcpy_avx_128(dd - 195, ss - 195); [[fallthrough]];\n+    case 67: memcpy_avx_64(dd - 67, ss - 67); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 196: memcpy_avx_128(dd - 196, ss - 196); [[fallthrough]];\n+    case 68: memcpy_avx_64(dd - 68, ss - 68); *((uint32_t*)(dd - 4)) = *((uint32_t*)(ss - 4)); break;\n+    case 197: memcpy_avx_128(dd - 197, ss - 197); [[fallthrough]];\n+    case 69: memcpy_avx_64(dd - 69, ss - 69); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 198: memcpy_avx_128(dd - 198, ss - 198); [[fallthrough]];\n+    case 70: memcpy_avx_64(dd - 70, ss - 70); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 199: memcpy_avx_128(dd - 199, ss - 199); [[fallthrough]];\n+    case 71: memcpy_avx_64(dd - 71, ss - 71); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 200: memcpy_avx_128(dd - 200, ss - 200); [[fallthrough]];\n+    case 72: memcpy_avx_64(dd - 72, ss - 72); *((uint64_t*)(dd - 8)) = *((uint64_t*)(ss - 8)); break;\n+    case 201: memcpy_avx_128(dd - 201, ss - 201); [[fallthrough]];\n+    case 73: memcpy_avx_64(dd - 73, ss - 73); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 202: memcpy_avx_128(dd - 202, ss - 202); [[fallthrough]];\n+    case 74: memcpy_avx_64(dd - 74, ss - 74); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 203: memcpy_avx_128(dd - 203, ss - 203); [[fallthrough]];\n+    case 75: memcpy_avx_64(dd - 75, ss - 75); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 204: memcpy_avx_128(dd - 204, ss - 204); [[fallthrough]];\n+    case 76: memcpy_avx_64(dd - 76, ss - 76); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 205: memcpy_avx_128(dd - 205, ss - 205); [[fallthrough]];\n+    case 77: memcpy_avx_64(dd - 77, ss - 77); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 206: memcpy_avx_128(dd - 206, ss - 206); [[fallthrough]];\n+    case 78: memcpy_avx_64(dd - 78, ss - 78); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 207: memcpy_avx_128(dd - 207, ss - 207); [[fallthrough]];\n+    case 79: memcpy_avx_64(dd - 79, ss - 79); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 208: memcpy_avx_128(dd - 208, ss - 208); [[fallthrough]];\n+    case 80: memcpy_avx_64(dd - 80, ss - 80); memcpy_avx_16(dd - 16, ss - 16); break;\n+    case 209: memcpy_avx_128(dd - 209, ss - 209); [[fallthrough]];\n+    case 81: memcpy_avx_64(dd - 81, ss - 81); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 210: memcpy_avx_128(dd - 210, ss - 210); [[fallthrough]];\n+    case 82: memcpy_avx_64(dd - 82, ss - 82); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 211: memcpy_avx_128(dd - 211, ss - 211); [[fallthrough]];\n+    case 83: memcpy_avx_64(dd - 83, ss - 83); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 212: memcpy_avx_128(dd - 212, ss - 212); [[fallthrough]];\n+    case 84: memcpy_avx_64(dd - 84, ss - 84); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 213: memcpy_avx_128(dd - 213, ss - 213); [[fallthrough]];\n+    case 85: memcpy_avx_64(dd - 85, ss - 85); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 214: memcpy_avx_128(dd - 214, ss - 214); [[fallthrough]];\n+    case 86: memcpy_avx_64(dd - 86, ss - 86); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 215: memcpy_avx_128(dd - 215, ss - 215); [[fallthrough]];\n+    case 87: memcpy_avx_64(dd - 87, ss - 87); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 216: memcpy_avx_128(dd - 216, ss - 216); [[fallthrough]];\n+    case 88: memcpy_avx_64(dd - 88, ss - 88); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 217: memcpy_avx_128(dd - 217, ss - 217); [[fallthrough]];\n+    case 89: memcpy_avx_64(dd - 89, ss - 89); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 218: memcpy_avx_128(dd - 218, ss - 218); [[fallthrough]];\n+    case 90: memcpy_avx_64(dd - 90, ss - 90); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 219: memcpy_avx_128(dd - 219, ss - 219); [[fallthrough]];\n+    case 91: memcpy_avx_64(dd - 91, ss - 91); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 220: memcpy_avx_128(dd - 220, ss - 220); [[fallthrough]];\n+    case 92: memcpy_avx_64(dd - 92, ss - 92); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 221: memcpy_avx_128(dd - 221, ss - 221); [[fallthrough]];\n+    case 93: memcpy_avx_64(dd - 93, ss - 93); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 222: memcpy_avx_128(dd - 222, ss - 222); [[fallthrough]];\n+    case 94: memcpy_avx_64(dd - 94, ss - 94); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 223: memcpy_avx_128(dd - 223, ss - 223); [[fallthrough]];\n+    case 95: memcpy_avx_64(dd - 95, ss - 95); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 224: memcpy_avx_128(dd - 224, ss - 224); [[fallthrough]];\n+    case 96: memcpy_avx_64(dd - 96, ss - 96); memcpy_avx_32(dd - 32, ss - 32); break;\n+    case 225: memcpy_avx_128(dd - 225, ss - 225); [[fallthrough]];\n+    case 97: memcpy_avx_64(dd - 97, ss - 97); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 226: memcpy_avx_128(dd - 226, ss - 226); [[fallthrough]];\n+    case 98: memcpy_avx_64(dd - 98, ss - 98); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 227: memcpy_avx_128(dd - 227, ss - 227); [[fallthrough]];\n+    case 99: memcpy_avx_64(dd - 99, ss - 99); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 228: memcpy_avx_128(dd - 228, ss - 228); [[fallthrough]];\n+    case 100: memcpy_avx_64(dd - 100, ss - 100); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 229: memcpy_avx_128(dd - 229, ss - 229); [[fallthrough]];\n+    case 101: memcpy_avx_64(dd - 101, ss - 101); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 230: memcpy_avx_128(dd - 230, ss - 230); [[fallthrough]];\n+    case 102: memcpy_avx_64(dd - 102, ss - 102); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 231: memcpy_avx_128(dd - 231, ss - 231); [[fallthrough]];\n+    case 103: memcpy_avx_64(dd - 103, ss - 103); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 232: memcpy_avx_128(dd - 232, ss - 232); [[fallthrough]];\n+    case 104: memcpy_avx_64(dd - 104, ss - 104); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 233: memcpy_avx_128(dd - 233, ss - 233); [[fallthrough]];\n+    case 105: memcpy_avx_64(dd - 105, ss - 105); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 234: memcpy_avx_128(dd - 234, ss - 234); [[fallthrough]];\n+    case 106: memcpy_avx_64(dd - 106, ss - 106); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 235: memcpy_avx_128(dd - 235, ss - 235); [[fallthrough]];\n+    case 107: memcpy_avx_64(dd - 107, ss - 107); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 236: memcpy_avx_128(dd - 236, ss - 236); [[fallthrough]];\n+    case 108: memcpy_avx_64(dd - 108, ss - 108); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 237: memcpy_avx_128(dd - 237, ss - 237); [[fallthrough]];\n+    case 109: memcpy_avx_64(dd - 109, ss - 109); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 238: memcpy_avx_128(dd - 238, ss - 238); [[fallthrough]];\n+    case 110: memcpy_avx_64(dd - 110, ss - 110); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 239: memcpy_avx_128(dd - 239, ss - 239); [[fallthrough]];\n+    case 111: memcpy_avx_64(dd - 111, ss - 111); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 240: memcpy_avx_128(dd - 240, ss - 240); [[fallthrough]];\n+    case 112: memcpy_avx_64(dd - 112, ss - 112); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 241: memcpy_avx_128(dd - 241, ss - 241); [[fallthrough]];\n+    case 113: memcpy_avx_64(dd - 113, ss - 113); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 242: memcpy_avx_128(dd - 242, ss - 242); [[fallthrough]];\n+    case 114: memcpy_avx_64(dd - 114, ss - 114); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 243: memcpy_avx_128(dd - 243, ss - 243); [[fallthrough]];\n+    case 115: memcpy_avx_64(dd - 115, ss - 115); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 244: memcpy_avx_128(dd - 244, ss - 244); [[fallthrough]];\n+    case 116: memcpy_avx_64(dd - 116, ss - 116); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 245: memcpy_avx_128(dd - 245, ss - 245); [[fallthrough]];\n+    case 117: memcpy_avx_64(dd - 117, ss - 117); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 246: memcpy_avx_128(dd - 246, ss - 246); [[fallthrough]];\n+    case 118: memcpy_avx_64(dd - 118, ss - 118); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 247: memcpy_avx_128(dd - 247, ss - 247); [[fallthrough]];\n+    case 119: memcpy_avx_64(dd - 119, ss - 119); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 248: memcpy_avx_128(dd - 248, ss - 248); [[fallthrough]];\n+    case 120: memcpy_avx_64(dd - 120, ss - 120); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 249: memcpy_avx_128(dd - 249, ss - 249); [[fallthrough]];\n+    case 121: memcpy_avx_64(dd - 121, ss - 121); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 250: memcpy_avx_128(dd - 250, ss - 250); [[fallthrough]];\n+    case 122: memcpy_avx_64(dd - 122, ss - 122); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 251: memcpy_avx_128(dd - 251, ss - 251); [[fallthrough]];\n+    case 123: memcpy_avx_64(dd - 123, ss - 123); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 252: memcpy_avx_128(dd - 252, ss - 252); [[fallthrough]];\n+    case 124: memcpy_avx_64(dd - 124, ss - 124); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 253: memcpy_avx_128(dd - 253, ss - 253); [[fallthrough]];\n+    case 125: memcpy_avx_64(dd - 125, ss - 125); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 254: memcpy_avx_128(dd - 254, ss - 254); [[fallthrough]];\n+    case 126: memcpy_avx_64(dd - 126, ss - 126); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 255: memcpy_avx_128(dd - 255, ss - 255); [[fallthrough]];\n+    case 127: memcpy_avx_64(dd - 127, ss - 127); memcpy_avx_64(dd - 64, ss - 64); break;\n+    case 256: memcpy_avx_256(dd - 256, ss - 256); break;\n+    }\n+\n+    return dst;\n+}\n+\n+\n+//---------------------------------------------------------------------\n+// main routine\n+//---------------------------------------------------------------------\n+void* memcpy_fast_avx(void * __restrict destination, const void * __restrict source, size_t size)\n+{\n+    unsigned char *dst = reinterpret_cast<unsigned char*>(destination);\n+    const unsigned char *src = reinterpret_cast<const unsigned char*>(source);\n+    static size_t cachesize = 0x200000; // L3-cache size\n+    size_t padding;\n+\n+    // small memory copy\n+    if (size <= 256)\n+    {\n+        memcpy_tiny_avx(dst, src, size);\n+        _mm256_zeroupper();\n+        return destination;\n+    }\n+\n+    // align destination to 16 bytes boundary\n+    padding = (32 - (((size_t)dst) & 31)) & 31;\n+\n+#if 0\n+    if (padding > 0)\n+    {\n+        __m256i head = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src));\n+        _mm256_storeu_si256((__m256i*)dst, head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+#else\n+    __m256i head = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src));\n+    _mm256_storeu_si256((__m256i*)dst, head);\n+    dst += padding;\n+    src += padding;\n+    size -= padding;\n+#endif\n+\n+    // medium size copy\n+    if (size <= cachesize)\n+    {\n+        __m256i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+        for (; size >= 256; size -= 256)\n+        {\n+            c0 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+            c1 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 1);\n+            c2 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 2);\n+            c3 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 3);\n+            c4 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 4);\n+            c5 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 5);\n+            c6 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 6);\n+            c7 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 7);\n+            src += 256;\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 0), c0);\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 1), c1);\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 2), c2);\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 3), c3);\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 4), c4);\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 5), c5);\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 6), c6);\n+            _mm256_storeu_si256(((reinterpret_cast<__m256i*>(dst)) + 7), c7);\n+            dst += 256;\n+        }\n+    }\n+    else\n+    {        // big memory copy\n+        __m256i c0, c1, c2, c3, c4, c5, c6, c7;\n+        /* __m256i c0, c1, c2, c3, c4, c5, c6, c7; */\n+\n+        if ((((size_t)src) & 31) == 0)\n+        {    // source aligned\n+            for (; size >= 256; size -= 256)\n+            {\n+                c0 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+                c1 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 1);\n+                c2 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 2);\n+                c3 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 3);\n+                c4 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 4);\n+                c5 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 5);\n+                c6 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 6);\n+                c7 = _mm256_load_si256((reinterpret_cast<const __m256i*>(src)) + 7);\n+                src += 256;\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 0), c0);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 1), c1);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 2), c2);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 3), c3);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 4), c4);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 5), c5);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 6), c6);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 7), c7);\n+                dst += 256;\n+            }\n+        }\n+        else\n+        {                            // source unaligned\n+            for (; size >= 256; size -= 256)\n+            {\n+                c0 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+                c1 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 1);\n+                c2 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 2);\n+                c3 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 3);\n+                c4 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 4);\n+                c5 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 5);\n+                c6 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 6);\n+                c7 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 7);\n+                src += 256;\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 0), c0);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 1), c1);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 2), c2);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 3), c3);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 4), c4);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 5), c5);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 6), c6);\n+                _mm256_stream_si256(((reinterpret_cast<__m256i*>(dst)) + 7), c7);\n+                dst += 256;\n+            }\n+        }\n+        _mm_sfence();\n+    }\n+\n+    memcpy_tiny_avx(dst, src, size);\n+    _mm256_zeroupper();\n+\n+    return destination;\n+}\ndiff --git a/utils/memcpy-bench/memcpy-bench.cpp b/utils/memcpy-bench/memcpy-bench.cpp\nnew file mode 100644\nindex 000000000000..2df72cb5ccbf\n--- /dev/null\n+++ b/utils/memcpy-bench/memcpy-bench.cpp\n@@ -0,0 +1,620 @@\n+#include <memory>\n+#include <cstddef>\n+#include <string>\n+#include <random>\n+#include <iostream>\n+#include <iomanip>\n+#include <thread>\n+\n+#include <dlfcn.h>\n+\n+#include <pcg_random.hpp>\n+\n+#include <common/defines.h>\n+\n+#include <Common/Stopwatch.h>\n+\n+#pragma GCC diagnostic ignored \"-Wold-style-cast\"\n+#pragma GCC diagnostic ignored \"-Wcast-align\"\n+#pragma GCC diagnostic ignored \"-Wcast-qual\"\n+#include \"FastMemcpy.h\"\n+//#include \"FastMemcpy_Avx.h\"\n+\n+#include <emmintrin.h>\n+#include <immintrin.h>\n+\n+\n+template <typename F, typename MemcpyImpl>\n+void NO_INLINE loop(uint8_t * dst, uint8_t * src, size_t size, F && chunk_size_distribution, MemcpyImpl && impl)\n+{\n+    while (size)\n+    {\n+        size_t bytes_to_copy = std::min<size_t>(size, chunk_size_distribution());\n+\n+        impl(dst, src, bytes_to_copy);\n+\n+        dst += bytes_to_copy;\n+        src += bytes_to_copy;\n+        size -= bytes_to_copy;\n+    }\n+}\n+\n+\n+using RNG = pcg32_fast;\n+\n+template <size_t N>\n+size_t generatorUniform(RNG & rng) { return rng() % N; };\n+\n+\n+template <typename F, typename MemcpyImpl>\n+void test(uint8_t * dst, uint8_t * src, size_t size, size_t iterations, size_t num_threads, F && generator, MemcpyImpl && impl)\n+{\n+    Stopwatch watch;\n+\n+    std::vector<std::thread> threads;\n+    threads.reserve(num_threads);\n+\n+    for (size_t thread_num = 0; thread_num < num_threads; ++thread_num)\n+    {\n+        size_t begin = size * thread_num / num_threads;\n+        size_t end = size * (thread_num + 1) / num_threads;\n+\n+        threads.emplace_back([begin, end, iterations, &src, &dst, &generator, &impl]\n+        {\n+            for (size_t iteration = 0; iteration < iterations; ++iteration)\n+            {\n+                loop(\n+                    iteration % 2 ? &src[begin] : &dst[begin],\n+                    iteration % 2 ? &dst[begin] : &src[begin],\n+                    end - begin,\n+                    [rng = RNG(), &generator]() mutable { return generator(rng); },\n+                    std::forward<MemcpyImpl>(impl));\n+            }\n+        });\n+    }\n+\n+    for (auto & thread : threads)\n+        thread.join();\n+\n+    double elapsed_ns = watch.elapsed();\n+\n+    /// Validation\n+    size_t sum = 0;\n+    for (size_t i = 0; i < size; ++i)\n+        sum += dst[i];\n+\n+    std::cerr << std::fixed << std::setprecision(3)\n+        << \"Processed in \" << (elapsed_ns / 1e9) << \"sec, \" << (size * iterations * 1.0 / elapsed_ns) << \" GB/sec (sum = \" << sum << \")\\n\";\n+}\n+\n+\n+using memcpy_type = void * (*)(const void * __restrict, void * __restrict, size_t);\n+\n+\n+static void * memcpy_erms(void * dst, const void * src, size_t size)\n+{\n+    asm volatile (\n+        \"rep movsb\"\n+        : \"=D\"(dst), \"=S\"(src), \"=c\"(size)\n+        : \"0\"(dst), \"1\"(src), \"2\"(size)\n+        : \"memory\");\n+    return dst;\n+}\n+\n+extern \"C\" void * memcpy_jart(void * dst, const void * src, size_t size);\n+extern \"C\" void MemCpy(void * dst, const void * src, size_t size);\n+\n+\n+static void * memcpySSE2(void * __restrict destination, const void * __restrict source, size_t size)\n+{\n+    unsigned char *dst = reinterpret_cast<unsigned char *>(destination);\n+    const unsigned char *src = reinterpret_cast<const unsigned char *>(source);\n+    size_t padding;\n+\n+    // small memory copy\n+    if (size <= 16)\n+        return memcpy_tiny(dst, src, size);\n+\n+    // align destination to 16 bytes boundary\n+    padding = (16 - (reinterpret_cast<size_t>(dst) & 15)) & 15;\n+\n+    if (padding > 0)\n+    {\n+        __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+\n+    // medium size copy\n+    __m128i c0;\n+\n+    for (; size >= 16; size -= 16)\n+    {\n+        c0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+        src += 16;\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst)), c0);\n+        dst += 16;\n+    }\n+\n+    memcpy_tiny(dst, src, size);\n+    return destination;\n+}\n+\n+static void * memcpySSE2Unrolled2(void * __restrict destination, const void * __restrict source, size_t size)\n+{\n+    unsigned char *dst = reinterpret_cast<unsigned char *>(destination);\n+    const unsigned char *src = reinterpret_cast<const unsigned char *>(source);\n+    size_t padding;\n+\n+    // small memory copy\n+    if (size <= 32)\n+        return memcpy_tiny(dst, src, size);\n+\n+    // align destination to 16 bytes boundary\n+    padding = (16 - (reinterpret_cast<size_t>(dst) & 15)) & 15;\n+\n+    if (padding > 0)\n+    {\n+        __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+\n+    // medium size copy\n+    __m128i c0, c1;\n+\n+    for (; size >= 32; size -= 32)\n+    {\n+        c0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 0);\n+        c1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 1);\n+        src += 32;\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 0), c0);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 1), c1);\n+        dst += 32;\n+    }\n+\n+    memcpy_tiny(dst, src, size);\n+    return destination;\n+}\n+\n+static void * memcpySSE2Unrolled4(void * __restrict destination, const void * __restrict source, size_t size)\n+{\n+    unsigned char *dst = reinterpret_cast<unsigned char *>(destination);\n+    const unsigned char *src = reinterpret_cast<const unsigned char *>(source);\n+    size_t padding;\n+\n+    // small memory copy\n+    if (size <= 64)\n+        return memcpy_tiny(dst, src, size);\n+\n+    // align destination to 16 bytes boundary\n+    padding = (16 - (reinterpret_cast<size_t>(dst) & 15)) & 15;\n+\n+    if (padding > 0)\n+    {\n+        __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+\n+    // medium size copy\n+    __m128i c0, c1, c2, c3;\n+\n+    for (; size >= 64; size -= 64)\n+    {\n+        c0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 0);\n+        c1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 1);\n+        c2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 2);\n+        c3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 3);\n+        src += 64;\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 0), c0);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 1), c1);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 2), c2);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 3), c3);\n+        dst += 64;\n+    }\n+\n+    memcpy_tiny(dst, src, size);\n+    return destination;\n+}\n+\n+\n+static void * memcpySSE2Unrolled8(void * __restrict destination, const void * __restrict source, size_t size)\n+{\n+    unsigned char *dst = reinterpret_cast<unsigned char *>(destination);\n+    const unsigned char *src = reinterpret_cast<const unsigned char *>(source);\n+    size_t padding;\n+\n+    // small memory copy\n+    if (size <= 128)\n+        return memcpy_tiny(dst, src, size);\n+\n+    // align destination to 16 bytes boundary\n+    padding = (16 - (reinterpret_cast<size_t>(dst) & 15)) & 15;\n+\n+    if (padding > 0)\n+    {\n+        __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+\n+    // medium size copy\n+    __m128i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+    for (; size >= 128; size -= 128)\n+    {\n+        c0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 0);\n+        c1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 1);\n+        c2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 2);\n+        c3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 3);\n+        c4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 4);\n+        c5 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 5);\n+        c6 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 6);\n+        c7 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 7);\n+        src += 128;\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 0), c0);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 1), c1);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 2), c2);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 3), c3);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 4), c4);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 5), c5);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 6), c6);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 7), c7);\n+        dst += 128;\n+    }\n+\n+    memcpy_tiny(dst, src, size);\n+    return destination;\n+}\n+\n+\n+//static __attribute__((__always_inline__, __target__(\"sse2\")))\n+__attribute__((__always_inline__))\n+void memcpy_my_medium_sse(uint8_t * __restrict & dst, const uint8_t * __restrict & src, size_t & size)\n+{\n+    /// Align destination to 16 bytes boundary.\n+    size_t padding = (16 - (reinterpret_cast<size_t>(dst) & 15)) & 15;\n+\n+    if (padding > 0)\n+    {\n+        __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+\n+    /// Aligned unrolled copy.\n+    __m128i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+    while (size >= 128)\n+    {\n+        c0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 0);\n+        c1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 1);\n+        c2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 2);\n+        c3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 3);\n+        c4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 4);\n+        c5 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 5);\n+        c6 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 6);\n+        c7 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 7);\n+        src += 128;\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 0), c0);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 1), c1);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 2), c2);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 3), c3);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 4), c4);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 5), c5);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 6), c6);\n+        _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 7), c7);\n+        dst += 128;\n+\n+        size -= 128;\n+    }\n+}\n+\n+__attribute__((__target__(\"avx\")))\n+void memcpy_my_medium_avx(uint8_t * __restrict & __restrict dst, const uint8_t * __restrict & __restrict src, size_t & __restrict size)\n+{\n+    size_t padding = (32 - (reinterpret_cast<size_t>(dst) & 31)) & 31;\n+\n+    if (padding > 0)\n+    {\n+        __m256i head = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src));\n+        _mm256_storeu_si256((__m256i*)dst, head);\n+        dst += padding;\n+        src += padding;\n+        size -= padding;\n+    }\n+\n+    __m256i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+    while (size >= 256)\n+    {\n+        c0 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 0);\n+        c1 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 1);\n+        c2 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 2);\n+        c3 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 3);\n+        c4 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 4);\n+        c5 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 5);\n+        c6 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 6);\n+        c7 = _mm256_loadu_si256((reinterpret_cast<const __m256i*>(src)) + 7);\n+        src += 256;\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 0), c0);\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 1), c1);\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 2), c2);\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 3), c3);\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 4), c4);\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 5), c5);\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 6), c6);\n+        _mm256_store_si256(((reinterpret_cast<__m256i*>(dst)) + 7), c7);\n+        dst += 256;\n+\n+        size -= 256;\n+    }\n+}\n+\n+bool have_avx = true;\n+\n+static uint8_t * memcpy_my(uint8_t * __restrict dst, const uint8_t * __restrict src, size_t size)\n+{\n+    uint8_t * ret = dst;\n+\n+tail:\n+    if (size <= 16)\n+    {\n+        if (size >= 8)\n+        {\n+            __builtin_memcpy(dst + size - 8, src + size - 8, 8);\n+            __builtin_memcpy(dst, src, 8);\n+        }\n+        else if (size >= 4)\n+        {\n+            __builtin_memcpy(dst + size - 4, src + size - 4, 4);\n+            __builtin_memcpy(dst, src, 4);\n+        }\n+        else if (size >= 2)\n+        {\n+            __builtin_memcpy(dst + size - 2, src + size - 2, 2);\n+            __builtin_memcpy(dst, src, 2);\n+        }\n+        else if (size >= 1)\n+        {\n+            *dst = *src;\n+        }\n+    }\n+    else if (have_avx)\n+    {\n+        if (size <= 32)\n+        {\n+            __builtin_memcpy(dst, src, 8);\n+            __builtin_memcpy(dst + 8, src + 8, 8);\n+\n+            dst += 16;\n+            src += 16;\n+            size -= 16;\n+\n+            goto tail;\n+        }\n+\n+        if (size <= 256)\n+        {\n+            __asm__(\n+                \"vmovups    -0x20(%[s],%[size],1), %%ymm0\\n\"\n+                \"vmovups    %%ymm0, -0x20(%[d],%[size],1)\\n\"\n+                : [d]\"+r\"(dst), [s]\"+r\"(src)\n+                : [size]\"r\"(size)\n+                : \"ymm0\", \"memory\");\n+\n+            while (size > 32)\n+            {\n+                __asm__(\n+                    \"vmovups    (%[s]), %%ymm0\\n\"\n+                    \"vmovups    %%ymm0, (%[d])\\n\"\n+                    : [d]\"+r\"(dst), [s]\"+r\"(src)\n+                    :\n+                    : \"ymm0\", \"memory\");\n+\n+                dst += 32;\n+                src += 32;\n+                size -= 32;\n+            }\n+        }\n+        else\n+        {\n+            size_t padding = (32 - (reinterpret_cast<size_t>(dst) & 31)) & 31;\n+\n+            if (padding > 0)\n+            {\n+                __asm__(\n+                    \"vmovups    (%[s]), %%ymm0\\n\"\n+                    \"vmovups    %%ymm0, (%[d])\\n\"\n+                    : [d]\"+r\"(dst), [s]\"+r\"(src)\n+                    :\n+                    : \"ymm0\", \"memory\");\n+\n+                dst += padding;\n+                src += padding;\n+                size -= padding;\n+            }\n+\n+            while (size >= 256)\n+            {\n+                __asm__(\n+                    \"vmovups    (%[s]), %%ymm0\\n\"\n+                    \"vmovups    0x20(%[s]), %%ymm1\\n\"\n+                    \"vmovups    0x40(%[s]), %%ymm2\\n\"\n+                    \"vmovups    0x60(%[s]), %%ymm3\\n\"\n+                    \"vmovups    0x80(%[s]), %%ymm4\\n\"\n+                    \"vmovups    0xa0(%[s]), %%ymm5\\n\"\n+                    \"vmovups    0xc0(%[s]), %%ymm6\\n\"\n+                    \"vmovups    0xe0(%[s]), %%ymm7\\n\"\n+                    \"add        $0x100,%[s]\\n\"\n+                    \"vmovaps    %%ymm0, (%[d])\\n\"\n+                    \"vmovaps    %%ymm1, 0x20(%[d])\\n\"\n+                    \"vmovaps    %%ymm2, 0x40(%[d])\\n\"\n+                    \"vmovaps    %%ymm3, 0x60(%[d])\\n\"\n+                    \"vmovaps    %%ymm4, 0x80(%[d])\\n\"\n+                    \"vmovaps    %%ymm5, 0xa0(%[d])\\n\"\n+                    \"vmovaps    %%ymm6, 0xc0(%[d])\\n\"\n+                    \"vmovaps    %%ymm7, 0xe0(%[d])\\n\"\n+                    \"add        $0x100, %[d]\\n\"\n+                    : [d]\"+r\"(dst), [s]\"+r\"(src)\n+                    :\n+                    : \"ymm0\", \"ymm1\", \"ymm2\", \"ymm3\", \"ymm4\", \"ymm5\", \"ymm6\", \"ymm7\", \"memory\");\n+\n+                size -= 256;\n+            }\n+\n+            goto tail;\n+        }\n+    }\n+    else\n+    {\n+        if (size <= 128)\n+        {\n+            _mm_storeu_si128(reinterpret_cast<__m128i *>(dst + size - 16), _mm_loadu_si128(reinterpret_cast<const __m128i *>(src + size - 16)));\n+\n+            while (size > 16)\n+            {\n+                _mm_storeu_si128(reinterpret_cast<__m128i *>(dst), _mm_loadu_si128(reinterpret_cast<const __m128i *>(src)));\n+                dst += 16;\n+                src += 16;\n+                size -= 16;\n+            }\n+        }\n+        else\n+        {\n+            /// Align destination to 16 bytes boundary.\n+            size_t padding = (16 - (reinterpret_cast<size_t>(dst) & 15)) & 15;\n+\n+            if (padding > 0)\n+            {\n+                __m128i head = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n+                _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), head);\n+                dst += padding;\n+                src += padding;\n+                size -= padding;\n+            }\n+\n+            /// Aligned unrolled copy.\n+            __m128i c0, c1, c2, c3, c4, c5, c6, c7;\n+\n+            while (size >= 128)\n+            {\n+                c0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 0);\n+                c1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 1);\n+                c2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 2);\n+                c3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 3);\n+                c4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 4);\n+                c5 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 5);\n+                c6 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 6);\n+                c7 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src) + 7);\n+                src += 128;\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 0), c0);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 1), c1);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 2), c2);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 3), c3);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 4), c4);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 5), c5);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 6), c6);\n+                _mm_store_si128((reinterpret_cast<__m128i*>(dst) + 7), c7);\n+                dst += 128;\n+\n+                size -= 128;\n+            }\n+\n+            goto tail;\n+        }\n+    }\n+\n+    return ret;\n+}\n+\n+\n+template <typename F>\n+void dispatchMemcpyVariants(size_t memcpy_variant, uint8_t * dst, uint8_t * src, size_t size, size_t iterations, size_t num_threads, F && generator)\n+{\n+    memcpy_type memcpy_libc = reinterpret_cast<memcpy_type>(dlsym(RTLD_NEXT, \"memcpy\"));\n+\n+    if (memcpy_variant == 1)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpy);\n+    if (memcpy_variant == 2)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpy_libc);\n+    if (memcpy_variant == 3)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpy_erms);\n+    if (memcpy_variant == 4)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), MemCpy);\n+    if (memcpy_variant == 5)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpySSE2);\n+    if (memcpy_variant == 6)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpySSE2Unrolled2);\n+    if (memcpy_variant == 7)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpySSE2Unrolled4);\n+    if (memcpy_variant == 8)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpySSE2Unrolled8);\n+//    if (memcpy_variant == 9)\n+//        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpy_fast_avx);\n+    if (memcpy_variant == 10)\n+        test(dst, src, size, iterations, num_threads, std::forward<F>(generator), memcpy_my);\n+}\n+\n+void dispatchVariants(size_t memcpy_variant, size_t generator_variant, uint8_t * dst, uint8_t * src, size_t size, size_t iterations, size_t num_threads)\n+{\n+    if (generator_variant == 1)\n+        dispatchMemcpyVariants(memcpy_variant, dst, src, size, iterations, num_threads, generatorUniform<16>);\n+    if (generator_variant == 2)\n+        dispatchMemcpyVariants(memcpy_variant, dst, src, size, iterations, num_threads, generatorUniform<256>);\n+    if (generator_variant == 3)\n+        dispatchMemcpyVariants(memcpy_variant, dst, src, size, iterations, num_threads, generatorUniform<4096>);\n+    if (generator_variant == 4)\n+        dispatchMemcpyVariants(memcpy_variant, dst, src, size, iterations, num_threads, generatorUniform<65536>);\n+    if (generator_variant == 5)\n+        dispatchMemcpyVariants(memcpy_variant, dst, src, size, iterations, num_threads, generatorUniform<1048576>);\n+}\n+\n+\n+int main(int argc, char ** argv)\n+{\n+    size_t size = 1000000000;\n+    if (argc >= 2)\n+        size = std::stoull(argv[1]);\n+\n+    size_t iterations = 10;\n+    if (argc >= 3)\n+        iterations = std::stoull(argv[2]);\n+\n+    size_t num_threads = 1;\n+    if (argc >= 4)\n+        num_threads = std::stoull(argv[3]);\n+\n+    size_t memcpy_variant = 1;\n+    if (argc >= 5)\n+        memcpy_variant = std::stoull(argv[4]);\n+\n+    size_t generator_variant = 1;\n+    if (argc >= 6)\n+        generator_variant = std::stoull(argv[5]);\n+\n+    std::unique_ptr<uint8_t[]> src(new uint8_t[size]);\n+    std::unique_ptr<uint8_t[]> dst(new uint8_t[size]);\n+\n+    /// Fill src with some pattern for validation.\n+    for (size_t i = 0; i < size; ++i)\n+        src[i] = i;\n+\n+    /// Fill dst to avoid page faults.\n+    memset(dst.get(), 0, size);\n+\n+    dispatchVariants(memcpy_variant, generator_variant, dst.get(), src.get(), size, iterations, num_threads);\n+\n+    return 0;\n+}\ndiff --git a/utils/memcpy-bench/memcpy_jart.S b/utils/memcpy-bench/memcpy_jart.S\nnew file mode 100644\nindex 000000000000..50430d0abe07\n--- /dev/null\n+++ b/utils/memcpy-bench/memcpy_jart.S\n@@ -0,0 +1,138 @@\n+/*-*- mode:unix-assembly; indent-tabs-mode:t; tab-width:8; coding:utf-8     -*-\u2502\n+\u2502vi: set et ft=asm ts=8 tw=8 fenc=utf-8                                     :vi\u2502\n+\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n+\u2502 Copyright 2020 Justine Alexandra Roberts Tunney                              \u2502\n+\u2502                                                                              \u2502\n+\u2502 Permission to use, copy, modify, and/or distribute this software for         \u2502\n+\u2502 any purpose with or without fee is hereby granted, provided that the         \u2502\n+\u2502 above copyright notice and this permission notice appear in all copies.      \u2502\n+\u2502                                                                              \u2502\n+\u2502 THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL                \u2502\n+\u2502 WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED                \u2502\n+\u2502 WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE             \u2502\n+\u2502 AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL         \u2502\n+\u2502 DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR        \u2502\n+\u2502 PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER               \u2502\n+\u2502 TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR             \u2502\n+\u2502 PERFORMANCE OF THIS SOFTWARE.                                                \u2502\n+\u255a\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500*/\n+\n+//\tCopies memory.\n+//\n+//\tDEST and SRC must not overlap, unless DEST\u2264SRC.\n+//\n+//\t@param\trdi is dest\n+//\t@param\trsi is src\n+//\t@param\trdx is number of bytes\n+//\t@return\toriginal rdi copied to rax\n+//\t@mode\tlong\n+//\t@asyncsignalsafe\n+memcpy_jart:\tmov\t%rdi,%rax\n+//\t\ud835\udc60\ud835\udc59\ud835\udc56\ud835\udc51\ud835\udc52\n+\t.align\t16\n+\t.type\tmemcpy_jart,@function\n+\t.size\tmemcpy_jart,.-memcpy_jart\n+\t.globl\tmemcpy_jart\n+\n+//\tCopies memory w/ minimal impact ABI.\n+//\n+//\t@param\trdi is dest\n+//\t@param\trsi is src\n+//\t@param\trdx is number of bytes\n+//\t@clob\tflags,rcx,xmm3,xmm4\n+//\t@mode\tlong\n+MemCpy:\tmov\t$.Lmemcpytab.size,%ecx\n+\tcmp\t%rcx,%rdx\n+\tcmovb\t%rdx,%rcx\n+\tjmp\t*memcpytab(,%rcx,8)\n+.Lanchorpoint:\n+.L16r:\tcmp\t$1024,%rdx\n+\tjae\t.Lerms\n+.L16:\tmovdqu\t-16(%rsi,%rdx),%xmm4\n+\tmov\t$16,%rcx\n+0:\tadd\t$16,%rcx\n+\tmovdqu\t-32(%rsi,%rcx),%xmm3\n+\tmovdqu\t%xmm3,-32(%rdi,%rcx)\n+\tcmp\t%rcx,%rdx\n+\tja\t0b\n+\tmovdqu\t%xmm4,-16(%rdi,%rdx)\n+\tpxor\t%xmm4,%xmm4\n+\tpxor\t%xmm3,%xmm3\n+\tjmp\t.L0\n+.L8:\tpush\t%rbx\n+\tmov\t(%rsi),%rcx\n+\tmov\t-8(%rsi,%rdx),%rbx\n+\tmov\t%rcx,(%rdi)\n+\tmov\t%rbx,-8(%rdi,%rdx)\n+1:\tpop\t%rbx\n+.L0:\tret\n+.L4:\tpush\t%rbx\n+\tmov\t(%rsi),%ecx\n+\tmov\t-4(%rsi,%rdx),%ebx\n+\tmov\t%ecx,(%rdi)\n+\tmov\t%ebx,-4(%rdi,%rdx)\n+\tjmp\t1b\n+.L3:\tpush\t%rbx\n+\tmov\t(%rsi),%cx\n+\tmov\t-2(%rsi,%rdx),%bx\n+\tmov\t%cx,(%rdi)\n+\tmov\t%bx,-2(%rdi,%rdx)\n+\tjmp\t1b\n+.L2:\tmov\t(%rsi),%cx\n+\tmov\t%cx,(%rdi)\n+\tjmp\t.L0\n+.L1:\tmov\t(%rsi),%cl\n+\tmov\t%cl,(%rdi)\n+\tjmp\t.L0\n+.Lerms:\tcmp\t$1024*1024,%rdx\n+\tja\t.Lnts\n+\tpush\t%rdi\n+\tpush\t%rsi\n+\tmov\t%rdx,%rcx\n+\trep movsb\n+\tpop\t%rsi\n+\tpop\t%rdi\n+\tjmp\t.L0\n+.Lnts:\tmovdqu\t(%rsi),%xmm3\n+\tmovdqu\t%xmm3,(%rdi)\n+\tlea\t16(%rdi),%rcx\n+\tand\t$-16,%rcx\n+\tsub\t%rdi,%rcx\n+\tadd\t%rcx,%rdi\n+\tadd\t%rcx,%rsi\n+\tsub\t%rcx,%rdx\n+\tmov\t$16,%rcx\n+0:\tadd\t$16,%rcx\n+\tmovdqu\t-32(%rsi,%rcx),%xmm3\n+\tmovntdq\t%xmm3,-32(%rdi,%rcx)\n+\tcmp\t%rcx,%rdx\n+\tja\t0b\n+\tsfence\n+\tmovdqu\t-16(%rsi,%rdx),%xmm3\n+\tmovdqu\t%xmm3,-16(%rdi,%rdx)\n+\tpxor\t%xmm3,%xmm3\n+\tjmp\t.L0\n+\t.type\tMemCpy,@function\n+\t.size\tMemCpy,.-MemCpy\n+\t.globl\tMemCpy\n+\n+\t.section .rodata\n+\t.align\t8\n+memcpytab:\n+\t.quad\t.L0\n+\t.quad\t.L1\n+\t.quad\t.L2\n+\t.quad\t.L3\n+\t.rept\t4\n+\t.quad\t.L4\n+\t.endr\n+\t.rept\t8\n+\t.quad\t.L8\n+\t.endr\n+\t.rept\t16\n+\t.quad\t.L16\n+\t.endr\n+\t.equ\t.Lmemcpytab.size,(.-memcpytab)/8\n+\t.quad\t.L16r # SSE + ERMS + NTS\n+\t.type\tmemcpytab,@object\n+\t.previous\n",
  "test_patch": "diff --git a/base/common/tests/CMakeLists.txt b/base/common/tests/CMakeLists.txt\nindex b7082ee9900c..6775d443fb6a 100644\n--- a/base/common/tests/CMakeLists.txt\n+++ b/base/common/tests/CMakeLists.txt\n@@ -11,7 +11,7 @@ set(PLATFORM_LIBS ${CMAKE_DL_LIBS})\n target_link_libraries (date_lut2 PRIVATE common ${PLATFORM_LIBS})\n target_link_libraries (date_lut3 PRIVATE common ${PLATFORM_LIBS})\n target_link_libraries (date_lut_default_timezone PRIVATE common ${PLATFORM_LIBS})\n-target_link_libraries (local_date_time_comparison PRIVATE common)\n+target_link_libraries (local_date_time_comparison PRIVATE common ${PLATFORM_LIBS})\n target_link_libraries (realloc-perf PRIVATE common)\n add_check(local_date_time_comparison)\n \n",
  "problem_statement": "Evaluate another implementation of `memcpy`\nhttps://github.com/jart/cosmopolitan/blob/de09bec215675e9b0beb722df89c6f794da74f3f/libc/nexgen32e/memcpy.S#L49\r\n\r\nNote: it has some dependencies and requires initialization (to check for AVX2).\n",
  "hints_text": "We may want to add\r\n`.cfi_startproc`\r\n`.cfi_endproc`\r\ndirectives, so the function will be visible in profiler.\r\n\n@jart \r\n\r\nIt's unclear where to plug in initialization code like calls to `_init_kHalfCache3`, `_init_kCpuids` and `_init_memcpy` if we are using glibc (and not using startup code of cosmopolitan).\r\n\r\nLooks like glibc does not automatically call functions from\r\n```\r\n  [19] .init.300._init_m PROGBITS         0000000000331ff1  00130ff1\r\n       0000000000000014  0000000000000000  AX       0     0     1\r\n  [20] .init.201._init_k PROGBITS         0000000000332005  00131005\r\n       000000000000005e  0000000000000000  AX       0     0     1\r\n  [21] .init.202._init_k PROGBITS         0000000000332063  00131063\r\n       0000000000000064  0000000000000000  AX       0     0     1\r\n```\nDo I understand correctly that the asm code assumes specific kind of relocations: `kCpuids(%rip)` and have to be built or linked with specific options?\nMaybe if only thing that I want is memcpy, it will be simpler to rewrite the code instead of trying to plug it as is?\n@jart I see that these sections are in the binary, but the functions are not called.\r\nI tried to call them directly but the first attempt failed:\r\n\r\n```\r\nextern \"C\" void _init_kHalfCache3();\r\nextern \"C\" void _init_kCpuids();\r\n\r\n__attribute__((__constructor__)) void memcpy_init()\r\n{\r\n    _init_kHalfCache3();\r\n    _init_kCpuids();\r\n}\r\n```\n```\r\n    pxor %xmm4,%xmm4\r\n    pxor %xmm3,%xmm3\r\n```\r\n\r\n```\r\n    vxorps %ymm4,%ymm4,%ymm4\r\n    vxorps %ymm3,%ymm3,%ymm3\r\n```\r\n\r\nWhy do we zeroing these registers before return? Is it required by ABI?\n@jart \r\n\r\nI see that there are AVX non-temporal stores: https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=stream_si256&expand=5673,5672,5673\r\nbut we don't use them.\r\n\r\nAre they beneficial?\n@jart\r\n\r\nI have managed to create standalone version of memcpy:\r\nhttps://gist.github.com/alexey-milovidov/2c3b4e532b11bfb1708c89bccc7d6576\r\n\r\nI tried to understand your code and write comments but I'm not sure if I get everthing right.\r\nCould you please take a look at the comments in code?\r\n\r\nAlso I don't understand how it get managed for initialization functions like `_init_memcpy` to get called with correct arguments during startup. In my example I call it manually.\nThank you!\r\n\r\nDo I understand correctly that the header file with macro is needed to use the fact that less number of registers are clobbered in memcpy than it's usually expected?\r\n\r\nSadly this macro is incompatible with libc++ code:\r\n```\r\n/home/milovidov/work/ClickHouse/contrib/libcxx/include/utility:973:12: error: expected unqualified-id\r\n    _VSTD::memcpy(&__r, __p, sizeof(__r));\r\n```\r\n\r\nWe still can use it if we include it as the last header file.\r\n\r\nI will first try to use it without this header, as usual `memcpy` function call...\nIf someone will try to write `std::memcpy` it will also fail.",
  "created_at": "2021-03-08T10:22:48Z"
}