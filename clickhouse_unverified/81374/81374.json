{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 81374,
  "instance_id": "ClickHouse__ClickHouse-81374",
  "issue_numbers": [
    "81263"
  ],
  "base_commit": "ef129ccb1f9c2e54d441212fc2a9de05ccc020b7",
  "patch": "diff --git a/src/Storages/Kafka/StorageKafka2.cpp b/src/Storages/Kafka/StorageKafka2.cpp\nindex 6a498e93d191..a154dd85dd1b 100644\n--- a/src/Storages/Kafka/StorageKafka2.cpp\n+++ b/src/Storages/Kafka/StorageKafka2.cpp\n@@ -124,8 +124,6 @@ namespace\n {\n constexpr auto MAX_FAILED_POLL_ATTEMPTS = 10;\n constexpr auto TMP_LOCKS_REFRESH_POLLS = 15;\n-constexpr auto TMP_LOCKS_QUOTA_STEP = 2;\n-constexpr auto LOCKS_QUOTA_MIN = 1U;\n }\n \n StorageKafka2::StorageKafka2(\n@@ -755,18 +753,18 @@ void StorageKafka2::dropReplica()\n }\n \n \n-// We go through all the replicas, count the number of live replicas,\n+// We go through all the topic partitions, count the number of live replicas,\n // and see which partitions are already locked by other replicas\n-std::pair<StorageKafka2::TopicPartitionSet, UInt32> StorageKafka2::getLockedTopicPartitions(zkutil::ZooKeeper & keeper_to_use)\n+std::pair<StorageKafka2::TopicPartitionSet, StorageKafka2::ActiveReplicasInfo> StorageKafka2::getLockedTopicPartitions(zkutil::ZooKeeper & keeper_to_use)\n {\n     LOG_TRACE(log, \"Starting to lookup replica's state\");\n-    auto replicas = keeper_to_use.getChildren(keeper_path + \"/replicas\");\n-\n     StorageKafka2::TopicPartitionSet locked_partitions;\n     auto lock_nodes = keeper_to_use.getChildren(keeper_path + \"/topic_partition_locks\");\n+    std::unordered_set<String> replicas_with_lock;\n \n     for (const auto & lock_name : lock_nodes)\n     {\n+        replicas_with_lock.insert(keeper_to_use.get(keeper_path + \"/topic_partition_locks/\" + lock_name));\n         auto base = lock_name.substr(0, lock_name.size() - 5); // drop \".lock\"\n         auto sep  = base.rfind('_');\n         if (sep == String::npos)\n@@ -794,10 +792,13 @@ std::pair<StorageKafka2::TopicPartitionSet, UInt32> StorageKafka2::getLockedTopi\n         boost::algorithm::join(already_locked_partitions_str, \", \")\n     );\n \n-    return {locked_partitions, replicas.size()};\n+    const auto replicas_count = keeper_to_use.getChildren(keeper_path + \"/replicas\").size();\n+    LOG_TEST(log, \"There are {} replicas with lock and there are {} replicas in total\", replicas_with_lock.size(), replicas_count);\n+    const auto has_replica_without_locks = replicas_with_lock.size() < replicas_count;\n+    return {locked_partitions, ActiveReplicasInfo{replicas_count, has_replica_without_locks}};\n }\n \n-std::pair<StorageKafka2::TopicPartitions, UInt32> StorageKafka2::getAvailableTopicPartitions(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & all_topic_partitions)\n+std::pair<StorageKafka2::TopicPartitions, StorageKafka2::ActiveReplicasInfo> StorageKafka2::getAvailableTopicPartitions(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & all_topic_partitions)\n {\n     const auto get_locked_partitions_res = getLockedTopicPartitions(keeper_to_use);\n     const auto & already_locked_partitions = get_locked_partitions_res.first;\n@@ -858,30 +859,46 @@ std::optional<StorageKafka2::LockedTopicPartitionInfo> StorageKafka2::createLock\n     }\n }\n \n-// First we delete all current temporary locks,\n-// then we create new locks from free partitions\n-void StorageKafka2::updateTemporaryLocks(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & topic_partitions, TopicPartitionLocks & tmp_locks, std::optional<size_t> & tmp_locks_quota)\n+void StorageKafka2::lockTemporaryLocks(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & available_topic_partitions, TopicPartitionLocks & tmp_locks, size_t & tmp_locks_quota, const bool has_replica_without_locks)\n {\n-    LOG_TRACE(log, \"Starting to update temporary locks\");\n-    auto available_topic_partitions_res = getAvailableTopicPartitions(keeper_to_use, topic_partitions);\n-    pcg64 generator(randomSeed());\n-    std::shuffle(available_topic_partitions_res.first.begin(), available_topic_partitions_res.first.end(), generator);\n-    const auto & available_topic_partitions = available_topic_partitions_res.first;\n+    /// There are no available topic partitions, so drop the quota to 0\n+    if (available_topic_partitions.empty())\n+    {\n+        LOG_TRACE(log, \"There are no available topic partitions to lock\");\n+        tmp_locks_quota = 0;\n+        return;\n+    }\n+    LOG_TRACE(log, \"Starting to lock temporary locks\");\n \n-    /// tmp_locks_quota is increased by TMP_LOCKS_QUOTA_STEP\n-    /// but always stays within [LOCKS_QUOTA_MIN, free_partitions - TMP_LOCKS_QUOTA_STEP].\n-    if (!tmp_locks_quota.has_value())\n-        tmp_locks_quota = LOCKS_QUOTA_MIN;\n+    /// We have some temporary lock quota, but there is at least one replica without locks, let's drop the quote to give the other replica a chance to lock some partitions\n+    if (tmp_locks_quota > 0 && has_replica_without_locks)\n+    {\n+        LOG_TRACE(log, \"There is at least one consumer without locks, won't lock any temporary locks this round\");\n+        tmp_locks_quota = 0;\n+        return;\n+    }\n+\n+    /// We have some temporary lock quota, but it is greater than the number of available topic partitions,\n+    /// so we will reduce the quota to give other replicas a chance to lock some partitions\n+    if (tmp_locks_quota > 0 && tmp_locks_quota <= available_topic_partitions.size())\n+    {\n+\n+        LOG_TRACE(log, \"Reducing temporary locks to give other replicas a chance to lock some partitions\");\n+        tmp_locks_quota = std::min(tmp_locks_quota - 1, available_topic_partitions.size() - 1);\n+    }\n     else\n     {\n-        using quota_t = std::make_signed_t<size_t>;\n-        quota_t q = static_cast<quota_t>(tmp_locks_quota.value());\n-        quota_t fp = static_cast<quota_t>(available_topic_partitions.size());\n-        tmp_locks_quota = static_cast<size_t>(std::clamp(q + TMP_LOCKS_QUOTA_STEP, static_cast<quota_t>(LOCKS_QUOTA_MIN), fp - TMP_LOCKS_QUOTA_STEP));\n+        tmp_locks_quota = std::min(available_topic_partitions.size(), tmp_locks_quota + 1);\n     }\n-    LOG_INFO(log, \"The replica can take {} locks in the current round\", tmp_locks_quota.value());\n+    LOG_INFO(log, \"The replica can take {} temporary locks in the current round\", tmp_locks_quota);\n+\n+    if (tmp_locks_quota == 0)\n+        return;\n+\n+    auto available_topic_partitions_copy = available_topic_partitions;\n+    pcg64 generator(randomSeed());\n+    std::shuffle(available_topic_partitions_copy.begin(), available_topic_partitions_copy.end(), generator);\n \n-    tmp_locks.clear(); // to maintain order: looked at the old state, updated the state, committed the new state\n     for (const auto & tp : available_topic_partitions)\n     {\n         if (tmp_locks.size() >= tmp_locks_quota)\n@@ -895,33 +912,36 @@ void StorageKafka2::updateTemporaryLocks(zkutil::ZooKeeper & keeper_to_use, cons\n \n // If the number of locks on a replica is greater than it can hold, then we first release the partitions that we can no longer hold.\n // Otherwise, we try to lock free partitions one by one.\n-bool StorageKafka2::updatePermanentLocks(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & topic_partitions, TopicPartitionLocks & permanent_locks)\n+void StorageKafka2::updatePermanentLocks(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & available_topic_partitions, TopicPartitionLocks & permanent_locks, const size_t topic_partitions_count, const size_t active_replica_count)\n {\n     LOG_TRACE(log, \"Starting to update permanent locks\");\n-    const auto available_topic_partitions_res = getAvailableTopicPartitions(keeper_to_use, topic_partitions);\n-    const auto & available_topic_partitions = available_topic_partitions_res.first;\n-    const auto active_replica_count = available_topic_partitions_res.second;\n-    size_t can_lock_partitions = std::max<size_t>(active_replica_count != 0 ?\n-        topic_partitions.size() / static_cast<size_t>(active_replica_count) : LOCKS_QUOTA_MIN,\n-        LOCKS_QUOTA_MIN);\n-\n-    // tells us \u201cthe set of permanent assignments shifted\u201d.\n-    // We use this flag to trigger an immediate refresh of temporary locks\n-    // so that no stale partitions linger when the \u201cstable\u201d assignment changes under us.\n-    bool permanent_locks_changed = false;\n+    chassert(active_replica_count > 0 && \"There should be at least one active replica, because we are active\");\n+    size_t can_lock_partitions = std::max<size_t>(topic_partitions_count / static_cast<size_t>(active_replica_count), 1);\n+\n+    LOG_TRACE(log, \"The replica can have {} permanent locks after the current round\", can_lock_partitions);\n+\n+    if (can_lock_partitions == permanent_locks.size())\n+    {\n+        LOG_TRACE(log, \"The number of permanent locks is equal to the number of locks that can be taken, will not update them\");\n+        return;\n+    }\n+\n     if (can_lock_partitions < permanent_locks.size())\n     {\n+        LOG_TRACE(log, \"Will release the extra {} topic partition locks\", permanent_locks.size() - can_lock_partitions);\n         size_t need_to_unlock = permanent_locks.size() - can_lock_partitions;\n         auto permanent_locks_it = permanent_locks.begin();\n         for (size_t i = 0; i < need_to_unlock && permanent_locks_it != permanent_locks.end(); ++i)\n         {\n-            permanent_locks_changed = true;\n+            LOG_TEST(log, \"Releasing topic partition lock for [{}:{}] at offset\",\n+                permanent_locks_it->first.topic, permanent_locks_it->first.partition_id);\n             permanent_locks_it = permanent_locks.erase(permanent_locks_it);\n         }\n     }\n     else\n     {\n         size_t need_to_lock = can_lock_partitions - permanent_locks.size();\n+        LOG_TRACE(log, \"Will try to lock {} topic partitions\", need_to_lock);\n         auto tp_it = available_topic_partitions.begin();\n         for (size_t i = 0; i < need_to_lock && tp_it != available_topic_partitions.end();)\n         {\n@@ -930,13 +950,10 @@ bool StorageKafka2::updatePermanentLocks(zkutil::ZooKeeper & keeper_to_use, cons\n             auto maybe_lock = createLocksInfoIfFree(keeper_to_use, tp);\n             if (!maybe_lock.has_value())\n                 continue;\n-            permanent_locks_changed = true;\n             permanent_locks.emplace(TopicPartition(tp), std::move(*maybe_lock));\n             ++i;\n         }\n     }\n-\n-    return permanent_locks_changed;\n }\n \n void StorageKafka2::saveTopicPartitionInfo(zkutil::ZooKeeper & keeper_to_use, const std::filesystem::path & keeper_path_to_data, const String & data)\n@@ -1269,12 +1286,12 @@ std::optional<StorageKafka2::StallReason> StorageKafka2::streamToViews(size_t id\n                 return StallReason::NoMetadata;\n             }\n \n-            const auto permanent_locks_changed = updatePermanentLocks(*consumer_info.keeper, all_topic_partitions, consumer_info.permanent_locks);\n-            if (permanent_locks_changed || consumer_info.poll_count >= TMP_LOCKS_REFRESH_POLLS)\n-            {\n-                updateTemporaryLocks(*consumer_info.keeper, all_topic_partitions, consumer_info.tmp_locks, consumer_info.tmp_locks_quota);\n-                consumer_info.poll_count = 0;\n-            }\n+            // Clear temporary locks to give a chance to lock them as permanent locks and to make it possible to gather available topic partitions only once.\n+            consumer_info.tmp_locks.clear();\n+            const auto [available_topic_partitions, active_replicas_info] = getAvailableTopicPartitions(*consumer_info.keeper, all_topic_partitions);\n+            updatePermanentLocks(*consumer_info.keeper, available_topic_partitions, consumer_info.permanent_locks, all_topic_partitions.size(), active_replicas_info.active_replica_count);\n+            lockTemporaryLocks(*consumer_info.keeper, available_topic_partitions, consumer_info.tmp_locks, consumer_info.tmp_locks_quota, active_replicas_info.has_replica_without_locks);\n+            consumer_info.poll_count = 0;\n \n             // Now we always have some assignment\n             consumer_info.topic_partitions.clear();\ndiff --git a/src/Storages/Kafka/StorageKafka2.h b/src/Storages/Kafka/StorageKafka2.h\nindex 5884b82913b9..65142810d268 100644\n--- a/src/Storages/Kafka/StorageKafka2.h\n+++ b/src/Storages/Kafka/StorageKafka2.h\n@@ -122,10 +122,8 @@ class StorageKafka2 final : public IStorage, WithContext\n         KafkaConsumer2::OnlyTopicNameAndPartitionIdHash,\n         KafkaConsumer2::OnlyTopicNameAndPartitionIdEquality>;\n \n-    using TopicPartitionSet = std::unordered_set<\n-        TopicPartition,\n-        KafkaConsumer2::OnlyTopicNameAndPartitionIdHash,\n-        KafkaConsumer2::OnlyTopicNameAndPartitionIdEquality>;\n+    using TopicPartitionSet = std::\n+        unordered_set<TopicPartition, KafkaConsumer2::OnlyTopicNameAndPartitionIdHash, KafkaConsumer2::OnlyTopicNameAndPartitionIdEquality>;\n \n     struct ConsumerAndAssignmentInfo\n     {\n@@ -139,7 +137,7 @@ class StorageKafka2 final : public IStorage, WithContext\n         TopicPartitionLocks tmp_locks{};\n \n         // Quota, how many temporary locks can be taken in current round\n-        std::optional<size_t> tmp_locks_quota = std::nullopt;\n+        size_t tmp_locks_quota{};\n \n         // Searches first in permanent_locks, then in tmp_locks.\n         // Returns a pointer to the lock if found; otherwise, returns nullptr.\n@@ -151,7 +149,11 @@ class StorageKafka2 final : public IStorage, WithContext\n             locks_it = tmp_locks.find(topic_partition);\n             if (locks_it != tmp_locks.end())\n                 return &locks_it->second;\n-            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot find locks for topic partition {}:{}\", topic_partition.topic, topic_partition.partition_id);\n+            throw Exception(\n+                ErrorCodes::LOGICAL_ERROR,\n+                \"Cannot find locks for topic partition {}:{}\",\n+                topic_partition.topic,\n+                topic_partition.partition_id);\n         }\n     };\n \n@@ -166,7 +168,10 @@ class StorageKafka2 final : public IStorage, WithContext\n     {\n         BackgroundSchedulePoolTaskHolder holder;\n         std::atomic<bool> stream_cancelled{false};\n-        explicit TaskContext(BackgroundSchedulePoolTaskHolder && task_) : holder(std::move(task_)) { }\n+        explicit TaskContext(BackgroundSchedulePoolTaskHolder && task_)\n+            : holder(std::move(task_))\n+        {\n+        }\n     };\n \n     enum class AssignmentChange\n@@ -255,14 +260,32 @@ class StorageKafka2 final : public IStorage, WithContext\n     void createReplica();\n     void dropReplica();\n \n-    // The second number in the pair is the number of active replicas\n-    std::pair<TopicPartitionSet, UInt32> getLockedTopicPartitions(zkutil::ZooKeeper & keeper_to_use);\n-    std::pair<TopicPartitions, UInt32> getAvailableTopicPartitions(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & all_topic_partitions);\n-    std::optional<LockedTopicPartitionInfo> createLocksInfoIfFree(zkutil::ZooKeeper & keeper_to_use, const TopicPartition & partition_to_lock);\n-\n-    // Takes lock over topic partitions and sets the committed offset in topic_partitions\n-    void updateTemporaryLocks(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & topic_partitions, TopicPartitionLocks & tmp_locks, std::optional<size_t> & tmp_locks_quota);\n-    bool updatePermanentLocks(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & topic_partitions, TopicPartitionLocks & permanent_locks);\n+    struct ActiveReplicasInfo\n+    {\n+        UInt64 active_replica_count{0};\n+        bool has_replica_without_locks{false};\n+    };\n+    std::pair<TopicPartitionSet, ActiveReplicasInfo> getLockedTopicPartitions(zkutil::ZooKeeper & keeper_to_use);\n+\n+    std::pair<TopicPartitions, ActiveReplicasInfo>\n+    getAvailableTopicPartitions(zkutil::ZooKeeper & keeper_to_use, const TopicPartitions & all_topic_partitions);\n+\n+    std::optional<LockedTopicPartitionInfo>\n+    createLocksInfoIfFree(zkutil::ZooKeeper & keeper_to_use, const TopicPartition & partition_to_lock);\n+\n+    void lockTemporaryLocks(\n+        zkutil::ZooKeeper & keeper_to_use,\n+        const TopicPartitions & available_topic_partitions,\n+        TopicPartitionLocks & tmp_locks,\n+        size_t & tmp_locks_quota,\n+        bool has_replica_without_locks);\n+\n+    void updatePermanentLocks(\n+        zkutil::ZooKeeper & keeper_to_use,\n+        const TopicPartitions & topic_partitions,\n+        TopicPartitionLocks & permanent_locks,\n+        size_t topic_partitions_count,\n+        size_t active_replica_count);\n \n     // To save commit and intent nodes\n     void saveTopicPartitionInfo(zkutil::ZooKeeper & keeper_to_use, const std::filesystem::path & keeper_path_to_data, const String & data);\n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test_batch_slow_6.py b/tests/integration/test_storage_kafka/test_batch_slow_6.py\nindex 82e221acbe10..71295a80eba7 100644\n--- a/tests/integration/test_storage_kafka/test_batch_slow_6.py\n+++ b/tests/integration/test_storage_kafka/test_batch_slow_6.py\n@@ -201,7 +201,7 @@ def test_block_based_formats_2(kafka_cluster, create_query_generator):\n     ],\n )\n def test_kafka_rebalance(kafka_cluster, create_query_generator, log_line):\n-    NUMBER_OF_CONSURRENT_CONSUMERS = 11\n+    NUMBER_OF_CONSURRENT_CONSUMERS = 5\n \n     instance.query(\n         \"\"\"\ndiff --git a/tests/integration/test_storage_kafka/test_zookeeper_locks.py b/tests/integration/test_storage_kafka/test_zookeeper_locks.py\nindex 01bd47976051..57bb3a4f1d55 100644\n--- a/tests/integration/test_storage_kafka/test_zookeeper_locks.py\n+++ b/tests/integration/test_storage_kafka/test_zookeeper_locks.py\n@@ -102,7 +102,7 @@ def test_zookeeper_partition_locks(kafka_cluster):\n         for i in range(num_partitions):\n             messages.append(json.dumps({\"key\": i, \"value\": i}))\n         k.kafka_produce(kafka_cluster, topic_name, messages, retries=5)\n-        \n+\n         base = f\"{keeper_path}/topic_partition_locks\"\n         expected_locks = {f\"zk_locks_topic_{pid}.lock\" for pid in range(num_partitions)}\n         with KeeperClient.from_cluster(kafka_cluster, keeper_node=\"zoo1\") as zk:\n@@ -180,7 +180,7 @@ def test_three_replicas_ten_partitions_rebalance(kafka_cluster):\n                 time.sleep(interval)\n             else:\n                 pytest.fail(f\"Timed out waiting for locks in ZK: got {children!r}, expected {expected_locks!r}\")\n-            \n+\n             counts = {replica: 0 for replica in replica_names}\n             for lock in expected_locks:\n                 owner = zk.get(f\"{base}/{lock}\")\n@@ -191,5 +191,5 @@ def test_three_replicas_ten_partitions_rebalance(kafka_cluster):\n             base_count = num_partitions // len(replica_names)\n             values = sorted(counts.values())\n             assert sum(values) == num_partitions\n-            assert all(v in (base_count-1, base_count, base_count+1) for v in values)\n+            assert all(v in (base_count-1, base_count, base_count+1) for v in values), f\"Values: {values}\"\n             assert values[-1] - values[0] <= 2\n",
  "problem_statement": "test_storage_kafka/test_batch_slow_6.py::test_kafka_rebalance is flaky\n```\nE               Exception: Command [docker exec rootteststoragekafkabatchslow6-gw1-instance-1 bash -c timeout 30 tail -Fn100 \"/var/log/clickhouse-server/clickhouse-server.log\" | grep -Em 1 'kafka_consumer10.*Saved offset'] return non-zero code 1:\n```\n\n```\n$ grep -c -E 'kafka_consumer10.*Saved offset' test_storage_kafka/_instances-batch_slow_6-0-gw1/instance/logs/clickhouse-server.log\n0\n```\n\nhttps://s3.amazonaws.com/clickhouse-test-reports/json.html?PR=81234&sha=be8f6729dc8e6bae54cb53dbba04d2df97fbdd17&name_0=PR&name_1=Integration%20tests%20%28asan%2C%20old%20analyzer%2C%203%2F6%29\n",
  "hints_text": "Seems it started to be flaky right after merging https://github.com/ClickHouse/ClickHouse/pull/78726\n\n@antaljanosbenjamin Let's fix or revert the PR until it's fixed",
  "created_at": "2025-06-05T15:59:46Z"
}