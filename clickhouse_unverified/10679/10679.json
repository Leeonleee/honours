{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 10679,
  "instance_id": "ClickHouse__ClickHouse-10679",
  "issue_numbers": [
    "7080"
  ],
  "base_commit": "d147cd646a1e6cc41d42f92d57f016a5c49d04de",
  "patch": "diff --git a/docs/en/sql-reference/statements/system.md b/docs/en/sql-reference/statements/system.md\nindex 0987c15bcddd..4235af1c255f 100644\n--- a/docs/en/sql-reference/statements/system.md\n+++ b/docs/en/sql-reference/statements/system.md\n@@ -12,6 +12,7 @@ toc_title: SYSTEM\n -   [DROP MARK CACHE](#query_language-system-drop-mark-cache)\n -   [DROP UNCOMPRESSED CACHE](#query_language-system-drop-uncompressed-cache)\n -   [DROP COMPILED EXPRESSION CACHE](#query_language-system-drop-compiled-expression-cache)\n+-   [DROP REPLICA](#query_language-system-drop-replica)\n -   [FLUSH LOGS](#query_language-system-flush_logs)\n -   [RELOAD CONFIG](#query_language-system-reload-config)\n -   [SHUTDOWN](#query_language-system-shutdown)\n@@ -67,6 +68,26 @@ For more convenient (automatic) cache management, see disable\\_internal\\_dns\\_ca\n \n Resets the mark cache. Used in development of ClickHouse and performance tests.\n \n+## DROP REPLICA {#query_language-system-drop-replica}\n+\n+Replicas can be dropped using following syntax:\n+\n+```sql\n+SYSTEM DROP REPLICA 'replica_name';\n+SYSTEM DROP REPLICA 'replica_name' FROM DATABASE database;\n+SYSTEM DROP REPLICA 'replica_name' FROM TABLE database.table;\n+```\n+\n+Queries will remove the replica path in zookeeper, it's useful when you want to decrease your replica factor. It will only drop the inactive/stale replica, and it can't drop local replica, please use `SYSTEM DROP REPLICA` for that.\n+\n+If you want to drop a inactive/stale replicate table that does not have a local replica, you can following syntax:\n+\n+```sql\n+SYSTEM DROP REPLICA 'replica_name' FROM ZKPATH '/path/to/table/in/zk';\n+```\n+\n+It's useful to remove metadata of dead replica from ZooKeeper. The right way to decrease replication factor is `DROP TABLE`.\n+\n ## DROP UNCOMPRESSED CACHE {#query_language-system-drop-uncompressed-cache}\n \n Reset the uncompressed data cache. Used in development of ClickHouse and performance tests.\ndiff --git a/src/Access/AccessType.h b/src/Access/AccessType.h\nindex c4fdbc46b71f..93f5fda9dbb0 100644\n--- a/src/Access/AccessType.h\n+++ b/src/Access/AccessType.h\n@@ -133,6 +133,7 @@ enum class AccessType\n     M(SYSTEM_REPLICATED_SENDS, \"SYSTEM STOP REPLICATED SENDS, SYSTEM START REPLICATED SENDS, STOP_REPLICATED_SENDS, START REPLICATED SENDS\", TABLE, SYSTEM_SENDS) \\\n     M(SYSTEM_SENDS, \"SYSTEM STOP SENDS, SYSTEM START SENDS, STOP SENDS, START SENDS\", GROUP, SYSTEM) \\\n     M(SYSTEM_REPLICATION_QUEUES, \"SYSTEM STOP REPLICATION QUEUES, SYSTEM START REPLICATION QUEUES, STOP_REPLICATION_QUEUES, START REPLICATION QUEUES\", TABLE, SYSTEM) \\\n+    M(SYSTEM_DROP_REPLICA, \"DROP REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_SYNC_REPLICA, \"SYNC REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_RESTART_REPLICA, \"RESTART REPLICA\", TABLE, SYSTEM) \\\n     M(SYSTEM_FLUSH_DISTRIBUTED, \"FLUSH DISTRIBUTED\", TABLE, SYSTEM_FLUSH) \\\ndiff --git a/src/Interpreters/InterpreterSystemQuery.cpp b/src/Interpreters/InterpreterSystemQuery.cpp\nindex a11e0b8feb2a..d4bddd026574 100644\n--- a/src/Interpreters/InterpreterSystemQuery.cpp\n+++ b/src/Interpreters/InterpreterSystemQuery.cpp\n@@ -185,7 +185,7 @@ BlockIO InterpreterSystemQuery::execute()\n \n     /// Make canonical query for simpler processing\n     if (!query.table.empty())\n-         table_id = context.resolveStorageID(StorageID(query.database, query.table), Context::ResolveOrdinary);\n+        table_id = context.resolveStorageID(StorageID(query.database, query.table), Context::ResolveOrdinary);\n \n     if (!query.target_dictionary.empty() && !query.database.empty())\n         query.target_dictionary = query.database + \".\" + query.target_dictionary;\n@@ -285,6 +285,9 @@ BlockIO InterpreterSystemQuery::execute()\n         case Type::START_DISTRIBUTED_SENDS:\n             startStopAction(ActionLocks::DistributedSend, true);\n             break;\n+        case Type::DROP_REPLICA:\n+            dropReplica(query);\n+            break;\n         case Type::SYNC_REPLICA:\n             syncReplica(query);\n             break;\n@@ -400,6 +403,117 @@ void InterpreterSystemQuery::restartReplicas(Context & system_context)\n     pool.wait();\n }\n \n+void InterpreterSystemQuery::dropReplica(ASTSystemQuery & query)\n+{\n+    StorageReplicatedMergeTree::Status status;\n+    auto zookeeper = context.getZooKeeper();\n+\n+    if (!table_id.empty())\n+    {\n+        context.checkAccess(AccessType::SYSTEM_DROP_REPLICA, table_id);\n+        StoragePtr table = DatabaseCatalog::instance().getTable(table_id, context);\n+\n+        if (auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(table.get()))\n+        {\n+            storage_replicated->getStatus(status);\n+            if (query.replica == status.replica_name)\n+                throw Exception(\"We can't drop local replica, please use `DROP TABLE` if you want to clean the data and drop this replica\", ErrorCodes::LOGICAL_ERROR);\n+            if (zookeeper->exists(status.zookeeper_path + \"/replicas/\" + query.replica + \"/is_active\"))\n+                throw Exception(\"Can't drop replica: \" + query.replica + \", because it's active\",\n+                    ErrorCodes::LOGICAL_ERROR);\n+            storage_replicated->dropReplica(zookeeper, status.zookeeper_path, query.replica, status.is_readonly);\n+            LOG_TRACE(log, \"DROP REPLICA \" + table_id.getNameForLogs() +  \" [\" + query.replica + \"]: OK\");\n+        }\n+        else\n+            throw Exception(\"Table \" + table_id.getNameForLogs() + \" is not replicated\", ErrorCodes::BAD_ARGUMENTS);\n+    }\n+    else if (!query.database.empty())\n+    {\n+        DatabasePtr database = DatabaseCatalog::instance().tryGetDatabase(query.database);\n+        if (database.get() != NULL)\n+        {\n+            for (auto iterator = database->getTablesIterator(context); iterator->isValid(); iterator->next())\n+            {\n+                if (auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(iterator->table().get()))\n+                {\n+                    context.checkAccess(AccessType::SYSTEM_DROP_REPLICA, iterator->table()->getStorageID());\n+                    storage_replicated->getStatus(status);\n+                    if (query.replica == status.replica_name)\n+                        throw Exception(\"We can't drop local replica, please use `DROP TABLE` if you want to clean the data and drop this replica\", ErrorCodes::LOGICAL_ERROR);\n+                    if (zookeeper->exists(status.zookeeper_path + \"/replicas/\" + query.replica + \"/is_active\"))\n+                        throw Exception(\"Can't drop replica: \" + query.replica + \", because it's active\",\n+                            ErrorCodes::LOGICAL_ERROR);\n+                    storage_replicated->dropReplica(zookeeper, status.zookeeper_path, query.replica, status.is_readonly);\n+                }\n+            }\n+            LOG_TRACE(log, \"DROP REPLICA \" + query.replica + \" DATABSE \" +  database->getDatabaseName() + \": OK\");\n+        }\n+        else\n+            throw Exception(\"DATABSE \" + query.database + \" doesn't exist\", ErrorCodes::BAD_ARGUMENTS);\n+    }\n+    else if (!query.replica_zk_path.empty())\n+    {\n+        auto remote_replica_path = query.replica_zk_path  + \"/replicas/\" + query.replica;\n+        auto & catalog = DatabaseCatalog::instance();\n+\n+        for (auto & elem : catalog.getDatabases())\n+        {\n+            DatabasePtr & database = elem.second;\n+            for (auto iterator = database->getTablesIterator(context); iterator->isValid(); iterator->next())\n+            {\n+                if (auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(iterator->table().get()))\n+                {\n+                    storage_replicated->getStatus(status);\n+                    if (status.replica_path.compare(remote_replica_path) == 0)\n+                        throw Exception(\"We can't drop local replica, please use `DROP TABLE` if you want to clean the data and drop this replica\",\n+                            ErrorCodes::LOGICAL_ERROR);\n+                    if (status.replica_path.compare(query.replica_zk_path + \"/replicas/\" + status.replica_name) == 0)\n+                    {\n+                        if (zookeeper->exists(query.replica_zk_path + \"/replicas/\" + query.replica + \"/is_active\"))\n+                            throw Exception(\"Can't drop replica: \" + query.replica + \", because it's active\",\n+                                ErrorCodes::LOGICAL_ERROR);\n+                        storage_replicated->dropReplica(zookeeper, query.replica_zk_path, query.replica, status.is_readonly);\n+                        return;\n+                    }\n+                }\n+            }\n+        }\n+\n+        /// It may left some garbage if replica_path subtree are concurently modified\n+        /// check if is active replica if we drop other replicas\n+        if (zookeeper->exists(remote_replica_path + \"/is_active\"))\n+            throw Exception(\"Can't remove replica: \" + query.replica + \", because it's active\",\n+                ErrorCodes::LOGICAL_ERROR);\n+\n+        zookeeper->tryRemoveRecursive(remote_replica_path);\n+        LOG_INFO(log, \"Removing replica {}\", query.replica_zk_path  + \"/replicas/\" + query.replica);\n+    }\n+    else if (query.is_drop_whole_replica)\n+    {\n+        auto databases = DatabaseCatalog::instance().getDatabases();\n+\n+        for (auto & elem : databases)\n+        {\n+            DatabasePtr & database = elem.second;\n+            for (auto iterator = database->getTablesIterator(context); iterator->isValid(); iterator->next())\n+            {\n+                if (auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(iterator->table().get()))\n+                {\n+                    context.checkAccess(AccessType::SYSTEM_DROP_REPLICA, iterator->table()->getStorageID());\n+                    storage_replicated->getStatus(status);\n+                    if (query.replica == status.replica_name)\n+                        throw Exception(\"We can't drop local replica, please use `DROP TABLE` if you want to clean the data and drop this replica\", ErrorCodes::LOGICAL_ERROR);\n+                    if (zookeeper->exists(status.zookeeper_path + \"/replicas/\" + query.replica + \"/is_active\"))\n+                        throw Exception(\"Can't drop replica: \" + query.replica + \", because it's active\",\n+                            ErrorCodes::LOGICAL_ERROR);\n+                    storage_replicated->dropReplica(zookeeper, status.zookeeper_path, query.replica, status.is_readonly);\n+                }\n+            }\n+            LOG_TRACE(log, \"DROP REPLICA \" + query.replica + \" DATABSE \" +  database->getDatabaseName() + \": OK\");\n+        }\n+    }\n+}\n+\n void InterpreterSystemQuery::syncReplica(ASTSystemQuery &)\n {\n     context.checkAccess(AccessType::SYSTEM_SYNC_REPLICA, table_id);\n@@ -530,6 +644,11 @@ AccessRightsElements InterpreterSystemQuery::getRequiredAccessForDDLOnCluster()\n                 required_access.emplace_back(AccessType::SYSTEM_REPLICATION_QUEUES, query.database, query.table);\n             break;\n         }\n+        case Type::DROP_REPLICA:\n+        {\n+            required_access.emplace_back(AccessType::SYSTEM_DROP_REPLICA, query.database, query.table);\n+            break;\n+        }\n         case Type::SYNC_REPLICA:\n         {\n             required_access.emplace_back(AccessType::SYSTEM_SYNC_REPLICA, query.database, query.table);\ndiff --git a/src/Interpreters/InterpreterSystemQuery.h b/src/Interpreters/InterpreterSystemQuery.h\nindex b55d1bda09bc..c57a6d95760d 100644\n--- a/src/Interpreters/InterpreterSystemQuery.h\n+++ b/src/Interpreters/InterpreterSystemQuery.h\n@@ -51,6 +51,7 @@ class InterpreterSystemQuery : public IInterpreter\n \n     void restartReplicas(Context & system_context);\n     void syncReplica(ASTSystemQuery & query);\n+    void dropReplica(ASTSystemQuery & query);\n     void flushDistributed(ASTSystemQuery & query);\n \n     AccessRightsElements getRequiredAccessForDDLOnCluster() const;\ndiff --git a/src/Parsers/ASTSystemQuery.cpp b/src/Parsers/ASTSystemQuery.cpp\nindex deaed0704846..52de931d50ea 100644\n--- a/src/Parsers/ASTSystemQuery.cpp\n+++ b/src/Parsers/ASTSystemQuery.cpp\n@@ -39,6 +39,8 @@ const char * ASTSystemQuery::typeToString(Type type)\n             return \"RESTART REPLICAS\";\n         case Type::RESTART_REPLICA:\n             return \"RESTART REPLICA\";\n+        case Type::DROP_REPLICA:\n+            return \"DROP REPLICA\";\n         case Type::SYNC_REPLICA:\n             return \"SYNC REPLICA\";\n         case Type::FLUSH_DISTRIBUTED:\n@@ -116,6 +118,26 @@ void ASTSystemQuery::formatImpl(const FormatSettings & settings, FormatState &,\n                       << (settings.hilite ? hilite_none : \"\");\n     };\n \n+    auto print_drop_replica = [&] {\n+        settings.ostr << \" \" << quoteString(replica) << (settings.hilite ? hilite_none : \"\");\n+        if (!table.empty())\n+        {\n+            settings.ostr << \" FROM TABLE\";\n+            print_database_table();\n+        }\n+        else if (!replica_zk_path.empty())\n+        {\n+            settings.ostr << \" FROM ZKPATH \" << (settings.hilite ? hilite_identifier : \"\") << quoteString(replica_zk_path)\n+                      << (settings.hilite ? hilite_none : \"\");\n+        }\n+        else if (!database.empty())\n+        {\n+            settings.ostr << \" FROM DATABASE \";\n+            settings.ostr << (settings.hilite ? hilite_identifier : \"\") << backQuoteIfNeed(database)\n+                        << (settings.hilite ? hilite_none : \"\");\n+        }\n+    };\n+\n     if (!cluster.empty())\n         formatOnCluster(settings);\n \n@@ -143,6 +165,8 @@ void ASTSystemQuery::formatImpl(const FormatSettings & settings, FormatState &,\n     }\n     else if (type == Type::RELOAD_DICTIONARY)\n         print_database_dictionary();\n+    else if (type == Type::DROP_REPLICA)\n+        print_drop_replica();\n }\n \n \ndiff --git a/src/Parsers/ASTSystemQuery.h b/src/Parsers/ASTSystemQuery.h\nindex eac96e50cb10..b2ffa706e196 100644\n--- a/src/Parsers/ASTSystemQuery.h\n+++ b/src/Parsers/ASTSystemQuery.h\n@@ -30,6 +30,7 @@ class ASTSystemQuery : public IAST, public ASTQueryWithOnCluster\n         START_LISTEN_QUERIES,\n         RESTART_REPLICAS,\n         RESTART_REPLICA,\n+        DROP_REPLICA,\n         SYNC_REPLICA,\n         RELOAD_DICTIONARY,\n         RELOAD_DICTIONARIES,\n@@ -61,6 +62,9 @@ class ASTSystemQuery : public IAST, public ASTQueryWithOnCluster\n     String target_dictionary;\n     String database;\n     String table;\n+    String replica;\n+    String replica_zk_path;\n+    bool is_drop_whole_replica;\n \n     String getID(char) const override { return \"SYSTEM query\"; }\n \ndiff --git a/src/Parsers/ParserSystemQuery.cpp b/src/Parsers/ParserSystemQuery.cpp\nindex 70a2b339f28e..a98ca2d4922c 100644\n--- a/src/Parsers/ParserSystemQuery.cpp\n+++ b/src/Parsers/ParserSystemQuery.cpp\n@@ -2,6 +2,7 @@\n #include <Parsers/ASTSystemQuery.h>\n #include <Parsers/CommonParsers.h>\n #include <Parsers/ExpressionElementParsers.h>\n+#include <Parsers/ASTIdentifier.h>\n #include <Parsers/ASTLiteral.h>\n #include <Parsers/parseDatabaseAndTableName.h>\n \n@@ -57,6 +58,48 @@ bool ParserSystemQuery::parseImpl(IParser::Pos & pos, ASTPtr & node, Expected &\n             break;\n         }\n \n+        case Type::DROP_REPLICA:\n+        {\n+            ASTPtr ast;\n+            if (!ParserStringLiteral{}.parse(pos, ast, expected))\n+                return false;\n+            res->replica = ast->as<ASTLiteral &>().value.safeGet<String>();\n+            if (ParserKeyword{\"FROM\"}.ignore(pos, expected))\n+            {\n+                // way 1. parse replica database\n+                // way 2. parse replica tables\n+                // way 3. parse replica zkpath\n+                if (ParserKeyword{\"DATABASE\"}.ignore(pos, expected))\n+                {\n+                    ParserIdentifier database_parser;\n+                    ASTPtr database;\n+                    if (!database_parser.parse(pos, database, expected))\n+                        return false;\n+                    tryGetIdentifierNameInto(database, res->database);\n+                }\n+                else if (ParserKeyword{\"TABLE\"}.ignore(pos, expected))\n+                {\n+                    parseDatabaseAndTableName(pos, expected, res->database, res->table);\n+                }\n+                else if (ParserKeyword{\"ZKPATH\"}.ignore(pos, expected))\n+                {\n+                    ASTPtr path_ast;\n+                    if (!ParserStringLiteral{}.parse(pos, path_ast, expected))\n+                        return false;\n+                    String zk_path = path_ast->as<ASTLiteral &>().value.safeGet<String>();\n+                    if (!zk_path.empty() && zk_path[zk_path.size() - 1] == '/')\n+                        zk_path.pop_back();\n+                    res->replica_zk_path = zk_path;\n+                }\n+                else\n+                    return false;\n+            }\n+            else\n+                res->is_drop_whole_replica = true;\n+\n+            break;\n+        }\n+\n         case Type::RESTART_REPLICA:\n         case Type::SYNC_REPLICA:\n             if (!parseDatabaseAndTableName(pos, expected, res->database, res->table))\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 9b69db31ad6a..a9f68a54ce4e 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -632,98 +632,12 @@ void StorageReplicatedMergeTree::drop()\n             throw Exception(\"Can't drop readonly replicated table (need to drop data in ZooKeeper as well)\", ErrorCodes::TABLE_IS_READ_ONLY);\n \n         shutdown();\n-\n-        if (zookeeper->expired())\n-            throw Exception(\"Table was not dropped because ZooKeeper session has expired.\", ErrorCodes::TABLE_WAS_NOT_DROPPED);\n-\n-        LOG_INFO(log, \"Removing replica {}\", replica_path);\n-        replica_is_active_node = nullptr;\n-        /// It may left some garbage if replica_path subtree are concurently modified\n-        zookeeper->tryRemoveRecursive(replica_path);\n-        if (zookeeper->exists(replica_path))\n-            LOG_ERROR(log, \"Replica was not completely removed from ZooKeeper, {} still exists and may contain some garbage.\", replica_path);\n-\n-        /// Check that `zookeeper_path` exists: it could have been deleted by another replica after execution of previous line.\n-        Strings replicas;\n-        if (Coordination::Error::ZOK == zookeeper->tryGetChildren(zookeeper_path + \"/replicas\", replicas) && replicas.empty())\n-        {\n-            LOG_INFO(log, \"{} is the last replica, will remove table\", replica_path);\n-\n-            /** At this moment, another replica can be created and we cannot remove the table.\n-              * Try to remove /replicas node first. If we successfully removed it,\n-              * it guarantees that we are the only replica that proceed to remove the table\n-              * and no new replicas can be created after that moment (it requires the existence of /replicas node).\n-              * and table cannot be recreated with new /replicas node on another servers while we are removing data,\n-              * because table creation is executed in single transaction that will conflict with remaining nodes.\n-              */\n-\n-            Coordination::Requests ops;\n-            Coordination::Responses responses;\n-            ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/replicas\", -1));\n-            ops.emplace_back(zkutil::makeCreateRequest(zookeeper_path + \"/dropped\", \"\", zkutil::CreateMode::Persistent));\n-            Coordination::Error code = zookeeper->tryMulti(ops, responses);\n-\n-            if (code == Coordination::Error::ZNONODE || code == Coordination::Error::ZNODEEXISTS)\n-            {\n-                LOG_WARNING(log, \"Table {} is already started to be removing by another replica right now\", replica_path);\n-            }\n-            else if (code == Coordination::Error::ZNOTEMPTY)\n-            {\n-                LOG_WARNING(log, \"Another replica was suddenly created, will keep the table {}\", replica_path);\n-            }\n-            else if (code != Coordination::Error::ZOK)\n-            {\n-                zkutil::KeeperMultiException::check(code, ops, responses);\n-            }\n-            else\n-            {\n-                LOG_INFO(log, \"Removing table {} (this might take several minutes)\", zookeeper_path);\n-\n-                Strings children;\n-                code = zookeeper->tryGetChildren(zookeeper_path, children);\n-                if (code == Coordination::Error::ZNONODE)\n-                {\n-                    LOG_WARNING(log, \"Table {} is already finished removing by another replica right now\", replica_path);\n-                }\n-                else\n-                {\n-                    for (const auto & child : children)\n-                        if (child != \"dropped\")\n-                            zookeeper->tryRemoveRecursive(zookeeper_path + \"/\" + child);\n-\n-                    ops.clear();\n-                    responses.clear();\n-                    ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/dropped\", -1));\n-                    ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path, -1));\n-                    code = zookeeper->tryMulti(ops, responses);\n-\n-                    if (code == Coordination::Error::ZNONODE)\n-                    {\n-                        LOG_WARNING(log, \"Table {} is already finished removing by another replica right now\", replica_path);\n-                    }\n-                    else if (code == Coordination::Error::ZNOTEMPTY)\n-                    {\n-                        LOG_ERROR(log, \"Table was not completely removed from ZooKeeper, {} still exists and may contain some garbage.\",\n-                            zookeeper_path);\n-                    }\n-                    else if (code != Coordination::Error::ZOK)\n-                    {\n-                        /// It is still possible that ZooKeeper session is expired or server is killed in the middle of the delete operation.\n-                        zkutil::KeeperMultiException::check(code, ops, responses);\n-                    }\n-                    else\n-                    {\n-                        LOG_INFO(log, \"Table {} was successfully removed from ZooKeeper\", zookeeper_path);\n-                    }\n-                }\n-            }\n-        }\n+        dropReplica(zookeeper, zookeeper_path, replica_name, is_readonly);\n     }\n \n     dropAllData();\n }\n \n-\n /** Verify that list of columns and table storage_settings_ptr match those specified in ZK (/ metadata).\n     * If not, throw an exception.\n     */\n@@ -836,6 +750,100 @@ static time_t tryGetPartCreateTime(zkutil::ZooKeeperPtr & zookeeper, const Strin\n     return res;\n }\n \n+void StorageReplicatedMergeTree::dropReplica(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path, const String & replica, bool is_readonly)\n+{\n+    static Poco::Logger * log = &Poco::Logger::get(\"StorageReplicatedMergeTree::dropReplica\");\n+\n+    /// If probably there is metadata in ZooKeeper, we don't allow to drop the table.\n+    if (is_readonly || !zookeeper)\n+        throw Exception(\"Can't drop readonly replicated table (need to drop data in ZooKeeper as well)\", ErrorCodes::TABLE_IS_READ_ONLY);\n+\n+    if (zookeeper->expired())\n+        throw Exception(\"Table was not dropped because ZooKeeper session has expired.\", ErrorCodes::TABLE_WAS_NOT_DROPPED);\n+\n+    auto remote_replica_path = zookeeper_path + \"/replicas\" + \"/\" + replica;\n+    LOG_INFO(log, \"Removing replica {}\", remote_replica_path);\n+    /// It may left some garbage if replica_path subtree are concurently modified\n+    zookeeper->tryRemoveRecursive(remote_replica_path);\n+    if (zookeeper->exists(remote_replica_path))\n+        LOG_ERROR(log, \"Replica was not completely removed from ZooKeeper, {} still exists and may contain some garbage.\", remote_replica_path);\n+\n+    /// Check that `zookeeper_path` exists: it could have been deleted by another replica after execution of previous line.\n+    Strings replicas;\n+    if (Coordination::Error::ZOK == zookeeper->tryGetChildren(zookeeper_path + \"/replicas\", replicas) && replicas.empty())\n+    {\n+        LOG_INFO(log, \"{} is the last replica, will remove table\", remote_replica_path);\n+\n+        /** At this moment, another replica can be created and we cannot remove the table.\n+          * Try to remove /replicas node first. If we successfully removed it,\n+          * it guarantees that we are the only replica that proceed to remove the table\n+          * and no new replicas can be created after that moment (it requires the existence of /replicas node).\n+          * and table cannot be recreated with new /replicas node on another servers while we are removing data,\n+          * because table creation is executed in single transaction that will conflict with remaining nodes.\n+          */\n+\n+        Coordination::Requests ops;\n+        Coordination::Responses responses;\n+        ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/replicas\", -1));\n+        ops.emplace_back(zkutil::makeCreateRequest(zookeeper_path + \"/dropped\", \"\", zkutil::CreateMode::Persistent));\n+        Coordination::Error code = zookeeper->tryMulti(ops, responses);\n+\n+        if (code == Coordination::Error::ZNONODE || code == Coordination::Error::ZNODEEXISTS)\n+        {\n+            LOG_WARNING(log, \"Table {} is already started to be removing by another replica right now\", remote_replica_path);\n+        }\n+        else if (code == Coordination::Error::ZNOTEMPTY)\n+        {\n+            LOG_WARNING(log, \"Another replica was suddenly created, will keep the table {}\", remote_replica_path);\n+        }\n+        else if (code != Coordination::Error::ZOK)\n+        {\n+            zkutil::KeeperMultiException::check(code, ops, responses);\n+        }\n+        else\n+        {\n+            LOG_INFO(log, \"Removing table {} (this might take several minutes)\", zookeeper_path);\n+\n+            Strings children;\n+            code = zookeeper->tryGetChildren(zookeeper_path, children);\n+            if (code == Coordination::Error::ZNONODE)\n+            {\n+                LOG_WARNING(log, \"Table {} is already finished removing by another replica right now\", remote_replica_path);\n+            }\n+            else\n+            {\n+                for (const auto & child : children)\n+                    if (child != \"dropped\")\n+                        zookeeper->tryRemoveRecursive(zookeeper_path + \"/\" + child);\n+\n+                ops.clear();\n+                responses.clear();\n+                ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/dropped\", -1));\n+                ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path, -1));\n+                code = zookeeper->tryMulti(ops, responses);\n+\n+                if (code == Coordination::Error::ZNONODE)\n+                {\n+                    LOG_WARNING(log, \"Table {} is already finished removing by another replica right now\", remote_replica_path);\n+                }\n+                else if (code == Coordination::Error::ZNOTEMPTY)\n+                {\n+                    LOG_ERROR(log, \"Table was not completely removed from ZooKeeper, {} still exists and may contain some garbage.\",\n+                        zookeeper_path);\n+                }\n+                else if (code != Coordination::Error::ZOK)\n+                {\n+                    /// It is still possible that ZooKeeper session is expired or server is killed in the middle of the delete operation.\n+                    zkutil::KeeperMultiException::check(code, ops, responses);\n+                }\n+                else\n+                {\n+                    LOG_INFO(log, \"Table {} was successfully removed from ZooKeeper\", zookeeper_path);\n+                }\n+            }\n+        }\n+    }\n+}\n \n void StorageReplicatedMergeTree::checkParts(bool skip_sanity_checks)\n {\n@@ -4038,7 +4046,6 @@ void StorageReplicatedMergeTree::checkPartitionCanBeDropped(const ASTPtr & parti\n     global_context.checkPartitionCanBeDropped(table_id.database_name, table_id.table_name, partition_size);\n }\n \n-\n void StorageReplicatedMergeTree::rename(const String & new_path_to_table_data, const StorageID & new_table_id)\n {\n     MergeTreeData::rename(new_path_to_table_data, new_table_id);\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 7d81d7a1d19c..3dcfbb973b4d 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -180,6 +180,10 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n \n     int getMetadataVersion() const { return metadata_version; }\n \n+    /** Remove a specific replica from zookeeper.\n+     */\n+    static void dropReplica(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path, const String & replica, bool is_readonly);\n+\n private:\n \n     /// Get a sequential consistent view of current parts.\n",
  "test_patch": "diff --git a/tests/integration/test_drop_replica/__init__.py b/tests/integration/test_drop_replica/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_drop_replica/configs/remote_servers.xml b/tests/integration/test_drop_replica/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..7978f921b2e6\n--- /dev/null\n+++ b/tests/integration/test_drop_replica/configs/remote_servers.xml\n@@ -0,0 +1,21 @@\n+<yandex>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node_1_1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node_1_2</host>\n+                    <port>9000</port>\n+                </replica>\n+                 <replica>\n+                    <host>node_1_3</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+</yandex>\ndiff --git a/tests/integration/test_drop_replica/test.py b/tests/integration/test_drop_replica/test.py\nnew file mode 100644\nindex 000000000000..12e5c9a8544a\n--- /dev/null\n+++ b/tests/integration/test_drop_replica/test.py\n@@ -0,0 +1,126 @@\n+import time\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.cluster import ClickHouseKiller\n+from helpers.test_tools import assert_eq_with_retry\n+from helpers.network import PartitionManager\n+\n+def fill_nodes(nodes, shard):\n+    for node in nodes:\n+        node.query(\n+        '''\n+            CREATE DATABASE test;\n+\n+            CREATE TABLE test.test_table(date Date, id UInt32)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/{shard}/replicated/test_table', '{replica}') ORDER BY id PARTITION BY toYYYYMM(date) SETTINGS min_replicated_logs_to_keep=3, max_replicated_logs_to_keep=5, cleanup_delay_period=0, cleanup_delay_period_random_add=0;\n+        '''.format(shard=shard, replica=node.name))\n+\n+        node.query(\n+        '''\n+            CREATE DATABASE test1;\n+\n+            CREATE TABLE test1.test_table(date Date, id UInt32)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test1/{shard}/replicated/test_table', '{replica}') ORDER BY id PARTITION BY toYYYYMM(date) SETTINGS min_replicated_logs_to_keep=3, max_replicated_logs_to_keep=5, cleanup_delay_period=0, cleanup_delay_period_random_add=0;\n+        '''.format(shard=shard, replica=node.name))\n+\n+        node.query(\n+        '''\n+            CREATE DATABASE test2;\n+\n+            CREATE TABLE test2.test_table(date Date, id UInt32)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test2/{shard}/replicated/test_table', '{replica}') ORDER BY id PARTITION BY toYYYYMM(date) SETTINGS min_replicated_logs_to_keep=3, max_replicated_logs_to_keep=5, cleanup_delay_period=0, cleanup_delay_period_random_add=0;\n+        '''.format(shard=shard, replica=node.name))\n+\n+\n+        node.query(\n+        '''\n+            CREATE DATABASE test3;\n+\n+            CREATE TABLE test3.test_table(date Date, id UInt32)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test3/{shard}/replicated/test_table', '{replica}') ORDER BY id PARTITION BY toYYYYMM(date) SETTINGS min_replicated_logs_to_keep=3, max_replicated_logs_to_keep=5, cleanup_delay_period=0, cleanup_delay_period_random_add=0;\n+        '''.format(shard=shard, replica=node.name))\n+\n+        node.query(\n+        '''\n+            CREATE DATABASE test4;\n+\n+            CREATE TABLE test4.test_table(date Date, id UInt32)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/test4/{shard}/replicated/test_table', '{replica}') ORDER BY id PARTITION BY toYYYYMM(date) SETTINGS min_replicated_logs_to_keep=3, max_replicated_logs_to_keep=5, cleanup_delay_period=0, cleanup_delay_period_random_add=0;\n+        '''.format(shard=shard, replica=node.name))\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node_1_1 = cluster.add_instance('node_1_1', with_zookeeper=True, main_configs=['configs/remote_servers.xml'])\n+node_1_2 = cluster.add_instance('node_1_2', with_zookeeper=True, main_configs=['configs/remote_servers.xml'])\n+node_1_3 = cluster.add_instance('node_1_3', with_zookeeper=True, main_configs=['configs/remote_servers.xml'])\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+\n+        fill_nodes([node_1_1, node_1_2], 1)\n+\n+        yield cluster\n+\n+    except Exception as ex:\n+        print ex\n+\n+    finally:\n+        cluster.shutdown()\n+\n+def test_drop_replica(start_cluster):\n+    for i in range(100):\n+        node_1_1.query(\"INSERT INTO test.test_table VALUES (1, {})\".format(i))\n+        node_1_1.query(\"INSERT INTO test1.test_table VALUES (1, {})\".format(i))\n+        node_1_1.query(\"INSERT INTO test2.test_table VALUES (1, {})\".format(i))\n+        node_1_1.query(\"INSERT INTO test3.test_table VALUES (1, {})\".format(i))\n+        node_1_1.query(\"INSERT INTO test4.test_table VALUES (1, {})\".format(i))\n+\n+    zk = cluster.get_kazoo_client('zoo1')\n+    assert \"can't drop local replica\" in node_1_1.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1'\")\n+    assert \"can't drop local replica\" in node_1_1.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM DATABASE test\")\n+    assert \"can't drop local replica\" in node_1_1.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM TABLE test.test_table\")\n+    assert \"can't drop local replica\" in \\\n+        node_1_1.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM ZKPATH '/clickhouse/tables/test/{shard}/replicated/test_table'\".format(shard=1))\n+    assert \"it's active\" in node_1_2.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1'\")\n+    assert \"it's active\" in node_1_2.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM DATABASE test\")\n+    assert \"it's active\" in node_1_2.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM TABLE test.test_table\")\n+    assert \"it's active\" in \\\n+        node_1_2.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM ZKPATH '/clickhouse/tables/test/{shard}/replicated/test_table'\".format(shard=1))\n+\n+    with PartitionManager() as pm:\n+        ## make node_1_1 dead\n+        pm.drop_instance_zk_connections(node_1_1)\n+        time.sleep(10)\n+\n+        assert \"doesn't exist\" in node_1_3.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM TABLE test.test_table\")\n+\n+        assert \"doesn't exist\" in node_1_3.query_and_get_error(\"SYSTEM DROP REPLICA 'node_1_1' FROM DATABASE test1\")\n+\n+        node_1_3.query(\"SYSTEM DROP REPLICA 'node_1_1'\")\n+        exists_replica_1_1 = zk.exists(\"/clickhouse/tables/test3/{shard}/replicated/test_table/replicas/{replica}\".format(shard=1, replica='node_1_1'))\n+        assert (exists_replica_1_1 != None)\n+\n+        ## If you want to drop a inactive/stale replicate table that does not have a local replica, you can following syntax(ZKPATH):\n+        node_1_3.query(\"SYSTEM DROP REPLICA 'node_1_1' FROM ZKPATH '/clickhouse/tables/test2/{shard}/replicated/test_table'\".format(shard=1))\n+        exists_replica_1_1 = zk.exists(\"/clickhouse/tables/test2/{shard}/replicated/test_table/replicas/{replica}\".format(shard=1, replica='node_1_1'))\n+        assert (exists_replica_1_1 == None)\n+\n+        node_1_2.query(\"SYSTEM DROP REPLICA 'node_1_1' FROM TABLE test.test_table\")\n+        exists_replica_1_1 = zk.exists(\"/clickhouse/tables/test/{shard}/replicated/test_table/replicas/{replica}\".format(shard=1, replica='node_1_1'))\n+        assert (exists_replica_1_1 == None)\n+\n+        node_1_2.query(\"SYSTEM DROP REPLICA 'node_1_1' FROM DATABASE test1\")\n+        exists_replica_1_1 = zk.exists(\"/clickhouse/tables/test1/{shard}/replicated/test_table/replicas/{replica}\".format(shard=1, replica='node_1_1'))\n+        assert (exists_replica_1_1 == None)\n+\n+        node_1_2.query(\"SYSTEM DROP REPLICA 'node_1_1' FROM ZKPATH '/clickhouse/tables/test3/{shard}/replicated/test_table'\".format(shard=1))\n+        exists_replica_1_1 = zk.exists(\"/clickhouse/tables/test3/{shard}/replicated/test_table/replicas/{replica}\".format(shard=1, replica='node_1_1'))\n+        assert (exists_replica_1_1 == None)\n+\n+        node_1_2.query(\"SYSTEM DROP REPLICA 'node_1_1'\")\n+        exists_replica_1_1 = zk.exists(\"/clickhouse/tables/test4/{shard}/replicated/test_table/replicas/{replica}\".format(shard=1, replica='node_1_1'))\n+        assert (exists_replica_1_1 == None)\ndiff --git a/tests/queries/0_stateless/01271_show_privileges.reference b/tests/queries/0_stateless/01271_show_privileges.reference\nindex e85dbd898012..250178dfb0c4 100644\n--- a/tests/queries/0_stateless/01271_show_privileges.reference\n+++ b/tests/queries/0_stateless/01271_show_privileges.reference\n@@ -89,6 +89,7 @@ SYSTEM DISTRIBUTED SENDS\t['SYSTEM STOP DISTRIBUTED SENDS','SYSTEM START DISTRIBU\n SYSTEM REPLICATED SENDS\t['SYSTEM STOP REPLICATED SENDS','SYSTEM START REPLICATED SENDS','STOP_REPLICATED_SENDS','START REPLICATED SENDS']\tTABLE\tSYSTEM SENDS\n SYSTEM SENDS\t['SYSTEM STOP SENDS','SYSTEM START SENDS','STOP SENDS','START SENDS']\t\\N\tSYSTEM\n SYSTEM REPLICATION QUEUES\t['SYSTEM STOP REPLICATION QUEUES','SYSTEM START REPLICATION QUEUES','STOP_REPLICATION_QUEUES','START REPLICATION QUEUES']\tTABLE\tSYSTEM\n+SYSTEM DROP REPLICA\t['DROP REPLICA']\tTABLE\tSYSTEM\n SYSTEM SYNC REPLICA\t['SYNC REPLICA']\tTABLE\tSYSTEM\n SYSTEM RESTART REPLICA\t['RESTART REPLICA']\tTABLE\tSYSTEM\n SYSTEM FLUSH DISTRIBUTED\t['FLUSH DISTRIBUTED']\tTABLE\tSYSTEM FLUSH\n",
  "problem_statement": "SYSTEM DROP REPLICA\nSometime you need to remove the 'dead' replica. \r\nattach + DROP is a bit too 'costy' and complicated.\r\nremoving nodes in zookeeper is also a bad option as requires knowledge of ClickHouse internals.  \r\n\r\nIt would be better to have a possibility to remove 'dead' replica from healthy one (only in case if it's offline and stale, exception otherwise). \r\n\n",
  "hints_text": "Typical usecase: server was off for a long time / replace replica with new one.\r\n\r\nCurrently it will not allow you to CREATE (replica already exists), ATTACH has not to much sense and will requite enabling replica recovery, and will try to 'execute' obsolete queue.  \nHi, is there any timeline for supporting this? This would be a great feature.\nThere is no timeline for this task. But it is easy to implement.\r\n\r\nThe only important thing is to check that replica is not active, and while removing - remove some nodes first in a multi transaction with check of `active` node - to avoid possible race condition.\nBump, is there any update on supporting this? This would be a super useful feature!  \nHi, is it safe to `rmr /path/to/replica` on ZK for this case? Is there anything else to be deleted?\nAlmost safe. You only have to check that the replica is not active and do it within the same transaction.\r\n\r\nYes, only `/path/to/replica` has to be deleted.\nThank you! \r\n",
  "created_at": "2020-05-05T15:06:24Z",
  "modified_files": [
    "docs/en/sql-reference/statements/system.md",
    "src/Access/AccessType.h",
    "src/Interpreters/InterpreterSystemQuery.cpp",
    "src/Interpreters/InterpreterSystemQuery.h",
    "src/Parsers/ASTSystemQuery.cpp",
    "src/Parsers/ASTSystemQuery.h",
    "src/Parsers/ParserSystemQuery.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_drop_replica/configs/remote_servers.xml",
    "b/tests/integration/test_drop_replica/test.py",
    "tests/queries/0_stateless/01271_show_privileges.reference"
  ]
}