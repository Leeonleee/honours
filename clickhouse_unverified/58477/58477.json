{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 58477,
  "instance_id": "ClickHouse__ClickHouse-58477",
  "issue_numbers": [
    "58405"
  ],
  "base_commit": "6b6f395cc7fd944f345a0ac088a6822ed05bca37",
  "patch": "diff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex 1cb810cf8ad3..d8386839a6f8 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -8,8 +8,10 @@\n #include <DataTypes/DataTypeNullable.h>\n #include <DataTypes/DataTypeString.h>\n #include <DataTypes/DataTypesNumber.h>\n+#include <Formats/FormatFactory.h>\n #include <Interpreters/Context.h>\n #include <Interpreters/InterpreterInsertQuery.h>\n+#include <Interpreters/InterpreterSelectQuery.h>\n #include <Interpreters/evaluateConstantExpression.h>\n #include <Parsers/ASTCreateQuery.h>\n #include <Parsers/ASTExpressionList.h>\n@@ -17,18 +19,19 @@\n #include <Parsers/ASTInsertQuery.h>\n #include <Parsers/ASTLiteral.h>\n #include <Processors/Executors/CompletedPipelineExecutor.h>\n-#include <QueryPipeline/QueryPipeline.h>\n+#include <Processors/QueryPlan/ISourceStep.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n #include <QueryPipeline/Pipe.h>\n-#include <Storages/MessageQueueSink.h>\n+#include <QueryPipeline/QueryPipeline.h>\n+#include <QueryPipeline/QueryPipelineBuilder.h>\n #include <Storages/Kafka/KafkaProducer.h>\n #include <Storages/Kafka/KafkaSettings.h>\n #include <Storages/Kafka/KafkaSource.h>\n+#include <Storages/MessageQueueSink.h>\n+#include <Storages/NamedCollectionsHelpers.h>\n #include <Storages/StorageFactory.h>\n #include <Storages/StorageMaterializedView.h>\n-#include <Storages/NamedCollectionsHelpers.h>\n #include <base/getFQDNOrHostName.h>\n-#include <Common/Stopwatch.h>\n-#include <Common/logger_useful.h>\n #include <boost/algorithm/string/replace.hpp>\n #include <boost/algorithm/string/split.hpp>\n #include <boost/algorithm/string/trim.hpp>\n@@ -37,11 +40,12 @@\n #include <Poco/Util/AbstractConfiguration.h>\n #include <Common/Exception.h>\n #include <Common/Macros.h>\n+#include <Common/Stopwatch.h>\n #include <Common/formatReadable.h>\n #include <Common/getNumberOfPhysicalCPUCores.h>\n+#include <Common/logger_useful.h>\n #include <Common/quoteString.h>\n #include <Common/setThreadName.h>\n-#include <Formats/FormatFactory.h>\n \n #include <Storages/ColumnDefault.h>\n #include <Common/config_version.h>\n@@ -174,6 +178,92 @@ struct StorageKafkaInterceptors\n     }\n };\n \n+class ReadFromStorageKafkaStep final : public ISourceStep\n+{\n+public:\n+    ReadFromStorageKafkaStep(\n+        const Names & column_names_,\n+        StoragePtr storage_,\n+        const StorageSnapshotPtr & storage_snapshot_,\n+        SelectQueryInfo & query_info,\n+        ContextPtr context_)\n+        : ISourceStep{DataStream{.header = storage_snapshot_->getSampleBlockForColumns(column_names_)}}\n+        , column_names{column_names_}\n+        , storage{storage_}\n+        , storage_snapshot{storage_snapshot_}\n+        , storage_limits{query_info.storage_limits}\n+        , context{context_}\n+    {\n+    }\n+\n+    String getName() const override { return \"ReadFromStorageKafka\"; }\n+\n+    void initializePipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &) override\n+    {\n+        auto pipe = makePipe();\n+\n+        /// Add storage limits.\n+        for (const auto & processor : pipe.getProcessors())\n+            processor->setStorageLimits(storage_limits);\n+\n+        /// Add to processors to get processor info through explain pipeline statement.\n+        for (const auto & processor : pipe.getProcessors())\n+            processors.emplace_back(processor);\n+\n+        pipeline.init(std::move(pipe));\n+    }\n+\n+private:\n+    Pipe makePipe()\n+    {\n+        auto & kafka_storage = storage->as<StorageKafka &>();\n+        if (kafka_storage.shutdown_called)\n+            throw Exception(ErrorCodes::ABORTED, \"Table is detached\");\n+\n+        if (!context->getSettingsRef().stream_like_engine_allow_direct_select)\n+            throw Exception(\n+                ErrorCodes::QUERY_NOT_ALLOWED,\n+                \"Direct select is not allowed. To enable use setting `stream_like_engine_allow_direct_select`\");\n+\n+        if (kafka_storage.mv_attached)\n+            throw Exception(ErrorCodes::QUERY_NOT_ALLOWED, \"Cannot read from StorageKafka with attached materialized views\");\n+\n+        ProfileEvents::increment(ProfileEvents::KafkaDirectReads);\n+\n+        /// Always use all consumers at once, otherwise SELECT may not read messages from all partitions.\n+        Pipes pipes;\n+        pipes.reserve(kafka_storage.num_consumers);\n+        auto modified_context = Context::createCopy(context);\n+        modified_context->applySettingsChanges(kafka_storage.settings_adjustments);\n+\n+        // Claim as many consumers as requested, but don't block\n+        for (size_t i = 0; i < kafka_storage.num_consumers; ++i)\n+        {\n+            /// Use block size of 1, otherwise LIMIT won't work properly as it will buffer excess messages in the last block\n+            /// TODO: probably that leads to awful performance.\n+            /// FIXME: seems that doesn't help with extra reading and committing unprocessed messages.\n+            pipes.emplace_back(std::make_shared<KafkaSource>(\n+                kafka_storage,\n+                storage_snapshot,\n+                modified_context,\n+                column_names,\n+                kafka_storage.log,\n+                1,\n+                kafka_storage.kafka_settings->kafka_commit_on_select));\n+        }\n+\n+        LOG_DEBUG(kafka_storage.log, \"Starting reading {} streams\", pipes.size());\n+        return Pipe::unitePipes(std::move(pipes));\n+    }\n+    ActionsDAGPtr buildFilterDAG();\n+\n+    const Names column_names;\n+    StoragePtr storage;\n+    StorageSnapshotPtr storage_snapshot;\n+    std::shared_ptr<const StorageLimitsList> storage_limits;\n+    ContextPtr context;\n+};\n+\n namespace\n {\n     const String CONFIG_KAFKA_TAG = \"kafka\";\n@@ -347,45 +437,18 @@ String StorageKafka::getDefaultClientId(const StorageID & table_id_)\n     return fmt::format(\"{}-{}-{}-{}\", VERSION_NAME, getFQDNOrHostName(), table_id_.database_name, table_id_.table_name);\n }\n \n-\n-Pipe StorageKafka::read(\n+void StorageKafka::read(\n+    QueryPlan & query_plan,\n     const Names & column_names,\n     const StorageSnapshotPtr & storage_snapshot,\n-    SelectQueryInfo & /* query_info */,\n-    ContextPtr local_context,\n+    SelectQueryInfo & query_info,\n+    ContextPtr query_context,\n     QueryProcessingStage::Enum /* processed_stage */,\n     size_t /* max_block_size */,\n     size_t /* num_streams */)\n {\n-    if (shutdown_called)\n-        throw Exception(ErrorCodes::ABORTED, \"Table is detached\");\n-\n-    if (!local_context->getSettingsRef().stream_like_engine_allow_direct_select)\n-        throw Exception(ErrorCodes::QUERY_NOT_ALLOWED,\n-                        \"Direct select is not allowed. To enable use setting `stream_like_engine_allow_direct_select`\");\n-\n-    if (mv_attached)\n-        throw Exception(ErrorCodes::QUERY_NOT_ALLOWED, \"Cannot read from StorageKafka with attached materialized views\");\n-\n-    ProfileEvents::increment(ProfileEvents::KafkaDirectReads);\n-\n-    /// Always use all consumers at once, otherwise SELECT may not read messages from all partitions.\n-    Pipes pipes;\n-    pipes.reserve(num_consumers);\n-    auto modified_context = Context::createCopy(local_context);\n-    modified_context->applySettingsChanges(settings_adjustments);\n-\n-    // Claim as many consumers as requested, but don't block\n-    for (size_t i = 0; i < num_consumers; ++i)\n-    {\n-        /// Use block size of 1, otherwise LIMIT won't work properly as it will buffer excess messages in the last block\n-        /// TODO: probably that leads to awful performance.\n-        /// FIXME: seems that doesn't help with extra reading and committing unprocessed messages.\n-        pipes.emplace_back(std::make_shared<KafkaSource>(*this, storage_snapshot, modified_context, column_names, log, 1, kafka_settings->kafka_commit_on_select));\n-    }\n-\n-    LOG_DEBUG(log, \"Starting reading {} streams\", pipes.size());\n-    return Pipe::unitePipes(std::move(pipes));\n+    query_plan.addStep(std::make_unique<ReadFromStorageKafkaStep>(\n+        column_names, shared_from_this(), storage_snapshot, query_info, std::move(query_context)));\n }\n \n \ndiff --git a/src/Storages/Kafka/StorageKafka.h b/src/Storages/Kafka/StorageKafka.h\nindex f60719538cfe..541fd32429df 100644\n--- a/src/Storages/Kafka/StorageKafka.h\n+++ b/src/Storages/Kafka/StorageKafka.h\n@@ -20,6 +20,7 @@ namespace DB\n {\n \n class StorageSystemKafkaConsumers;\n+class ReadFromStorageKafkaStep;\n \n struct StorageKafkaInterceptors;\n \n@@ -48,7 +49,8 @@ class StorageKafka final : public IStorage, WithContext\n     void startup() override;\n     void shutdown(bool is_drop) override;\n \n-    Pipe read(\n+    void read(\n+        QueryPlan & query_plan,\n         const Names & column_names,\n         const StorageSnapshotPtr & storage_snapshot,\n         SelectQueryInfo & query_info,\n@@ -86,6 +88,8 @@ class StorageKafka final : public IStorage, WithContext\n     SafeConsumers getSafeConsumers() { return {shared_from_this(), std::unique_lock(mutex), consumers};  }\n \n private:\n+    friend class ReadFromStorageKafkaStep;\n+\n     // Configuration and state\n     std::unique_ptr<KafkaSettings> kafka_settings;\n     Macros::MacroExpansionInfo macros_info;\n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex 2176b0151ffb..1c7814435dbb 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -892,12 +892,14 @@ def test_kafka_formats(kafka_cluster):\n \"\"\"\n \n     expected_rows_count = raw_expected.count(\"\\n\")\n-    instance.query_with_retry(\n+    result_checker = lambda res: res.count(\"\\n\") == expected_rows_count\n+    res = instance.query_with_retry(\n         f\"SELECT * FROM test.kafka_{list(all_formats.keys())[-1]}_mv;\",\n         retry_count=30,\n         sleep_time=1,\n-        check_callback=lambda res: res.count(\"\\n\") == expected_rows_count,\n+        check_callback=result_checker,\n     )\n+    assert result_checker(res)\n \n     for format_name, format_opts in list(all_formats.items()):\n         logging.debug((\"Checking {}\".format(format_name)))\n@@ -3808,12 +3810,14 @@ def test_kafka_formats_with_broken_message(kafka_cluster):\n \"\"\"\n \n     expected_rows_count = raw_expected.count(\"\\n\")\n-    instance.query_with_retry(\n+    result_checker = lambda res: res.count(\"\\n\") == expected_rows_count\n+    res = instance.query_with_retry(\n         f\"SELECT * FROM test.kafka_data_{list(all_formats.keys())[-1]}_mv;\",\n         retry_count=30,\n         sleep_time=1,\n-        check_callback=lambda res: res.count(\"\\n\") == expected_rows_count,\n+        check_callback=result_checker,\n     )\n+    assert result_checker(res)\n \n     for format_name, format_opts in list(all_formats.items()):\n         logging.debug(f\"Checking {format_name}\")\n@@ -4931,6 +4935,80 @@ def test_formats_errors(kafka_cluster):\n         instance.query(\"DROP TABLE test.view\")\n \n \n+def test_multiple_read_in_materialized_views(kafka_cluster, max_retries=15):\n+    admin_client = KafkaAdminClient(\n+        bootstrap_servers=\"localhost:{}\".format(kafka_cluster.kafka_port)\n+    )\n+\n+    topic = \"multiple_read_from_mv\"\n+    kafka_create_topic(admin_client, topic)\n+\n+    instance.query(\n+        f\"\"\"\n+        DROP TABLE IF EXISTS test.kafka_multiple_read_input;\n+        DROP TABLE IF EXISTS test.kafka_multiple_read_table;\n+        DROP TABLE IF EXISTS test.kafka_multiple_read_mv;\n+\n+        CREATE TABLE test.kafka_multiple_read_input (id Int64)\n+        ENGINE = Kafka\n+        SETTINGS\n+            kafka_broker_list = 'kafka1:19092',\n+            kafka_topic_list = '{topic}',\n+            kafka_group_name = '{topic}',\n+            kafka_format = 'JSONEachRow';\n+\n+        CREATE TABLE test.kafka_multiple_read_table (id Int64)\n+        ENGINE = MergeTree\n+        ORDER BY id;\n+\n+\n+        CREATE MATERIALIZED VIEW IF NOT EXISTS test.kafka_multiple_read_mv TO test.kafka_multiple_read_table AS\n+        SELECT id\n+        FROM test.kafka_multiple_read_input\n+        WHERE id NOT IN (\n+            SELECT id\n+            FROM test.kafka_multiple_read_table\n+            WHERE id IN (\n+                SELECT id\n+                FROM test.kafka_multiple_read_input\n+            )\n+        );\n+        \"\"\"\n+    )\n+\n+    kafka_produce(\n+        kafka_cluster, topic, [json.dumps({\"id\": 42}), json.dumps({\"id\": 43})]\n+    )\n+\n+    expected_result = \"42\\n43\\n\"\n+    res = instance.query_with_retry(\n+        f\"SELECT id FROM test.kafka_multiple_read_table ORDER BY id\",\n+        retry_count=30,\n+        sleep_time=0.5,\n+        check_callback=lambda res: res == expected_result,\n+    )\n+    assert res == expected_result\n+\n+    # Verify that the query deduplicates the records as it meant to be\n+    messages = []\n+    for i in range(0, 10):\n+        messages.append(json.dumps({\"id\": 42}))\n+        messages.append(json.dumps({\"id\": 43}))\n+\n+    messages.append(json.dumps({\"id\": 44}))\n+\n+    kafka_produce(kafka_cluster, topic, messages)\n+\n+    expected_result = \"42\\n43\\n44\\n\"\n+    res = instance.query_with_retry(\n+        f\"SELECT id FROM test.kafka_multiple_read_table ORDER BY id\",\n+        retry_count=30,\n+        sleep_time=0.5,\n+        check_callback=lambda res: res == expected_result,\n+    )\n+    assert res == expected_result\n+\n+\n if __name__ == \"__main__\":\n     cluster.start()\n     input(\"Cluster created, press any key to destroy...\")\n",
  "problem_statement": "Materialized View Regression for Kafka input tables (23.12.1 version)\n**Describe what's wrong**\r\n\r\nWe are using Kafka Engine table to ingest the data from Kafka to Clickhouse. Then we connect materialized view to it and store the results to target table. As we want to achieve first write wins strategy we need to check the target contents first to deduplicate incoming records. The fastest way based on benchmark was to use IN operator (JOINS were slow). To be able to do that we need to read records from source table (insert batch) twice inside materialized view. AFAIK it should be supported.\r\n\r\nThis stops to work in 23.12 with this error:\r\n\r\n```\r\ncom.clickhouse.client.ClickHouseException: Code: 620. DB::Exception: Direct select is not allowed. To enable use setting \r\n`stream_like_engine_allow_direct_select`: While processing id IN ((SELECT id FROM kafka_input GROUP BY id) AS _subquery72): \r\nWhile processing id NOT IN ((SELECT id FROM deduplicate WHERE id IN (SELECT id FROM kafka_input GROUP BY id)) AS \r\n_subquery71). (QUERY_NOT_ALLOWED) (version 23.12.1.1368 (official build))\r\n```\r\n\r\nThe view looks like this. Notice that we query FROM kafka_input twice (source table for materialized view)\r\n```\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS deduplicate_mv TO deduplicate \r\nAS SELECT \r\n\tid,time,any(value) AS value\r\nFROM kafka_input \r\nWHERE id NOT IN (\r\n\tSELECT id FROM deduplicate WHERE id IN (\r\n\t\tSELECT id FROM kafka_input GROUP BY id)\r\n\t)\r\nGROUP BY id,time;\r\n```\r\n\r\nSimilar example using NULL engine works as expected. Here is a fiddle:\r\nhttps://fiddle.clickhouse.com/a0207085-22c8-44f4-9c7f-acf751058644\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes. It is a regression of 23.12 release.\r\n\r\n\r\n**How to reproduce**\r\n\r\n* Use ClickHouse 23.12\r\n* Use Kafka engine input table\r\n* Use Materialized view that uses Kafka input table twice as in our example\r\n\r\n**Expected behavior**\r\n\r\nIt should be possible to create such a materialized view as query is not querying kafka table itself, but the batch of inserts produced by Kafka engine table.\r\n\n",
  "hints_text": "I guess you can simply enable that setting in the default profile.\r\n\r\n```\r\nstream_like_engine_allow_direct_select=1\r\n```\r\n\nAnd as a workaround you can utilize Null engine. \r\nKafka -> MV1 -> Null -> MV2 (with the subquery to Null) -> MergeTree\nHere I can see it should not help for MV: https://clickhouse.com/docs/en/whats-new/changelog/2021\r\n\r\n\"Can be enabled by setting stream_like_engine_allow_direct_select. Direct select will be not allowed even if enabled by setting, in case there is an attached materialized view.\"|\r\n\r\nYes I was thinking about using Null workaround as well, but I would rather want to see why it has happened as more people can meet this.\n>Here I can see it should not help for MV: https://clickhouse.com/docs/en/whats-new/changelog/2021\r\n\r\nExcept that you never really read the kafka table. Just try.\n> This stops to work in 23.12 with this error:\r\n\r\nDo you have a full stack trace of the error?\n> I guess you can simply enable that setting in the default profile. stream_like_engine_allow_direct_select=1\r\n\r\nThis one is a bad / dangerous option, please avoid doing that. Workaround with Null table is safer.\nHi @tomaskolda, just a few questions to verify:\r\n- you have upgraded from 23.11, right?\r\n- the same (and not a very similar) query with same settings worked before, but not in 23.12, right?\n@antaljanosbenjamin yes it was working as expected (materialized view can use source table multiple times AFAIK). It was broken over Xmass as one of our test is using docker image with \"latest\" so it started to use 23.12 version. It started to break the build and that is how it was identified. \r\n\r\nSo the only change that happened was an \"upgrade\" of the docker. Then it started to fail with exactly same code.\r\n\n@filimonov here is a stack, but it just shows an exception (error) we receive over the wire.\r\n\r\n```\r\n10:16:00      Caused by: com.clickhouse.client.ClickHouseException: Code: 620. DB::Exception: Direct select is not allowed. To enable use setting \r\n`stream_like_engine_allow_direct_select`: While processing id IN ((SELECT id FROM kafka_input GROUP BY id) AS _subquery72): \r\nWhile processing id NOT IN ((SELECT id FROM deduplicate WHERE id IN (SELECT id FROM kafka_input GROUP BY id)) AS \r\n_subquery71). (QUERY_NOT_ALLOWED) (version 23.12.1.1368 (official build))\r\n10:16:00      , server ClickHouseNode [uri=http://localhost:49155/metering]@-1768083100\r\n10:16:00      \tat com.clickhouse.client.ClickHouseException.of(ClickHouseException.java:168)\r\n10:16:00      \tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:282)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseClient.lambda$send$11(ClickHouseClient.java:723)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseClient.run(ClickHouseClient.java:232)\r\n10:16:00      \tat com.clickhouse.client.ClickHouseClient.lambda$submit$4(ClickHouseClient.java:284)\r\n10:16:00      \tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n10:16:00      \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n10:16:00      \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n10:16:00      \tat java.base/java.lang.Thread.run(Thread.java:833)\r\n10:16:00      Caused by: java.io.IOException: Code: 620. DB::Exception: Direct select is not allowed. To enable use setting \r\n`stream_like_engine_allow_direct_select`: While processing id IN ((SELECT id FROM kafka_input GROUP BY id) AS _subquery72): \r\nWhile processing id NOT IN ((SELECT id FROM deduplicate WHERE id IN (SELECT id FROM kafka_input GROUP BY id)) AS \r\n_subquery71). (QUERY_NOT_ALLOWED) (version 23.12.1.1368 (official build))\r\n10:16:00  \r\n10:16:00      \tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\r\n10:16:00      \tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\r\n10:16:00      \tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\r\n10:16:00      \tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\r\n10:16:00      \t... 12 common frames omitted\r\n```\nI was able to reproduce it with the binary build from commit 58c075b8cc3858c9aa6e4711d20a85c28db4722f on the [`23.12` PR](https://github.com/ClickHouse/ClickHouse/pull/58287). Here is the logs with the full stacktrace:\r\n```\r\n2024.01.03 10:21:50.986992 [ 27014 ] {} <Trace> LOCAL-Session: e8b1e7f8-f776-478b-96c8-adc989b0d5b4 Creating query context from session context, user_id: 94309d50-4f52-5250-31bd-74fecac179db, parent context user: default\r\n2024.01.03 10:21:50.987185 [ 27014 ] {5c41b1a6-25ca-43ba-8b9d-4dce4961c4a2} <Debug> executeQuery: (from 0.0.0.0:0, user: ) CREATE MATERIALIZED VIEW IF NOT EXISTS deduplicate_mv TO deduplicate AS SELECT id,time,any(value) AS value FROM kafka_input WHERE id NOT IN ( SELECT id FROM deduplicate WHERE id IN ( SELECT id FROM kafka_input GROUP BY id) ) GROUP BY id,time; (stage: Complete)\r\n2024.01.03 10:21:50.987226 [ 27014 ] {5c41b1a6-25ca-43ba-8b9d-4dce4961c4a2} <Trace> ContextAccess (default): Access granted: CREATE VIEW ON default.deduplicate_mv\r\n2024.01.03 10:21:50.987238 [ 27014 ] {5c41b1a6-25ca-43ba-8b9d-4dce4961c4a2} <Trace> ContextAccess (default): Access granted: SELECT, INSERT ON default.deduplicate\r\n2024.01.03 10:21:50.987715 [ 27014 ] {5c41b1a6-25ca-43ba-8b9d-4dce4961c4a2} <Trace> ContextAccess (default): Access granted: SELECT(id) ON default.kafka_input\r\n2024.01.03 10:21:51.002551 [ 27014 ] {5c41b1a6-25ca-43ba-8b9d-4dce4961c4a2} <Error> executeQuery: Code: 620. DB::Exception: Direct select is not allowed. To enable use setting `stream_like_engine_allow_direct_select`: While processing id IN ((SELECT id FROM default.kafka_input GROUP BY id) AS _subquery2): While processing id NOT IN ((SELECT id FROM default.deduplicate WHERE id IN (SELECT id FROM default.kafka_input GROUP BY id)) AS _subquery1). (QUERY_NOT_ALLOWED) (version 23.12.2.50) (from 0.0.0.0:0) (in query: CREATE MATERIALIZED VIEW IF NOT EXISTS deduplicate_mv TO deduplicate AS SELECT id,time,any(value) AS value FROM kafka_input WHERE id NOT IN ( SELECT id FROM deduplicate WHERE id IN ( SELECT id FROM kafka_input GROUP BY id) ) GROUP BY id,time;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ./build_docker/./src/Common/Exception.cpp:96: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c6cf8fb in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n1. DB::Exception::Exception<>(int, FormatStringHelperImpl<>) @ 0x000000000716642d in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n2. ./build_docker/./src/Storages/Kafka/StorageKafka.cpp:350: DB::StorageKafka::read(std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageSnapshot> const&, DB::SelectQueryInfo&, std::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned long) @ 0x000000000fecabc2 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n3. ./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:815: DB::IStorage::read(DB::QueryPlan&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageSnapshot> const&, DB::SelectQueryInfo&, std::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned long) @ 0x0000000011a6de99 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n4. ./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:815: DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::optional<DB::Pipe>) @ 0x00000000112ef89e in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n5. ./contrib/llvm-project/libcxx/include/optional:260: DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) @ 0x00000000112e2374 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n6. ./build_docker/./src/Interpreters/InterpreterSelectWithUnionQuery.cpp:0: DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) @ 0x0000000011394c96 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n7. ./build_docker/./src/Interpreters/ActionsVisitor.cpp:1463: DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x0000000010abe6dc in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n8. ./src/Interpreters/InDepthNodeVisitor.h:78: DB::InDepthNodeVisitor<DB::ActionsMatcher, true, false, std::shared_ptr<DB::IAST> const>::doVisit(std::shared_ptr<DB::IAST> const&) @ 0x0000000010aae715 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n9. ./src/Interpreters/InDepthNodeVisitor.h:0: DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x0000000010a89bbb in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n10. ./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:701: DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x0000000010a96d1c in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n11. ./src/Interpreters/ExpressionAnalyzer.h:213: DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x00000000112f3616 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n12. ./build_docker/./src/Interpreters/InterpreterSelectQuery.cpp:780: DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>)::$_0::operator()(bool) const @ 0x00000000112e021e in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n13. ./build_docker/./src/Interpreters/InterpreterSelectQuery.cpp:0: DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x00000000112d5955 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n14. ./src/Interpreters/InterpreterSelectQuery.cpp:0: DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000011390db8 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n15. ./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:701: DB::interpretSubquery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, std::vector<String, std::allocator<String>> const&, DB::SelectQueryOptions const&) @ 0x000000001173aff1 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n16. ./src/Interpreters/interpretSubquery.cpp:0: DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x0000000010abdbef in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n17. ./src/Interpreters/InDepthNodeVisitor.h:78: DB::InDepthNodeVisitor<DB::ActionsMatcher, true, false, std::shared_ptr<DB::IAST> const>::doVisit(std::shared_ptr<DB::IAST> const&) @ 0x0000000010aae715 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n18. ./src/Interpreters/InDepthNodeVisitor.h:0: DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x0000000010a89bbb in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n19. ./contrib/llvm-project/libcxx/include/__memory/shared_ptr.h:701: DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x0000000010a96d1c in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n20. ./src/Interpreters/ExpressionAnalyzer.h:213: DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x00000000112f3616 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n21. ./build_docker/./src/Interpreters/InterpreterSelectQuery.cpp:780: DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>)::$_0::operator()(bool) const @ 0x00000000112e021e in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n22. ./build_docker/./src/Interpreters/InterpreterSelectQuery.cpp:0: DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x00000000112d5955 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n23. ./src/Interpreters/InterpreterSelectQuery.cpp:0: DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000011390db8 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n24. ./contrib/llvm-project/libcxx/include/vector:434: DB::InterpreterCreateQuery::getTablePropertiesAndNormalizeCreateQuery(DB::ASTCreateQuery&) const @ 0x00000000110d6317 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n25. ./build_docker/./src/Interpreters/InterpreterCreateQuery.cpp:1244: DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000110de84f in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n26. ./build_docker/./src/Interpreters/InterpreterCreateQuery.cpp:1803: DB::InterpreterCreateQuery::execute() @ 0x00000000110f032f in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n27. ./build_docker/./src/Interpreters/executeQuery.cpp:0: DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000011721422 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n28. ./build_docker/./src/Interpreters/executeQuery.cpp:1286: DB::executeQuery(String const&, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum) @ 0x000000001171b01a in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n29. ./build_docker/./src/Client/LocalConnection.cpp:134: DB::LocalConnection::sendQuery(DB::ConnectionTimeouts const&, String const&, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>> const&, String const&, unsigned long, DB::Settings const*, DB::ClientInfo const*, bool, std::function<void (DB::Progress const&)>) @ 0x00000000125acb54 in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n30. ./contrib/llvm-project/libcxx/include/__functional/function.h:818: ? @ 0x000000001254fc5f in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n31. ./build_docker/./src/Client/ClientBase.cpp:867: DB::ClientBase::processTextAsSingleQuery(String const&) @ 0x000000001254dc7a in /home/ubuntu/Repos/ClickHouse/programs/clickhouse-23-12\r\n```\r\n\r\nAs I received the error when creating the materialized view and it came from `getSampleBlockImpl`, I assume the issue arises when ClickHouse tries to analyze the query in the MV. \nIt was maybe introduced in #57855. I will try to verify it.\nConfirmed. That PR introduced the regression. Let me figure out what went wrong and make a PR about it.\nNice @antaljanosbenjamin ! Thank you so much.",
  "created_at": "2024-01-03T17:17:17Z",
  "modified_files": [
    "src/Storages/Kafka/StorageKafka.cpp",
    "src/Storages/Kafka/StorageKafka.h"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_kafka/test.py"
  ]
}