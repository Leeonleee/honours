diff --git a/tests/clickhouse-test b/tests/clickhouse-test
index 03ade3f66c8e..71f603a69ca2 100755
--- a/tests/clickhouse-test
+++ b/tests/clickhouse-test
@@ -55,8 +55,6 @@ MESSAGES_TO_RETRY = [
     "ConnectionPoolWithFailover: Connection failed at try",
     "DB::Exception: New table appeared in database being dropped or detached. Try again",
     "is already started to be removing by another replica right now",
-    "DB::Exception: Cannot enqueue query",
-    "is executing longer than distributed_ddl_task_timeout",  # FIXME
 ]
 
 MAX_RETRIES = 3
diff --git a/tests/config/users.d/database_replicated.xml b/tests/config/users.d/database_replicated.xml
index 279591494c19..2b96e7418b61 100644
--- a/tests/config/users.d/database_replicated.xml
+++ b/tests/config/users.d/database_replicated.xml
@@ -6,7 +6,7 @@
             <database_replicated_initial_query_timeout_sec>120</database_replicated_initial_query_timeout_sec>
             <distributed_ddl_task_timeout>120</distributed_ddl_task_timeout>
             <database_replicated_always_detach_permanently>1</database_replicated_always_detach_permanently>
-            <distributed_ddl_entry_format_version>2</distributed_ddl_entry_format_version>
+            <database_replicated_enforce_synchronous_settings>1</database_replicated_enforce_synchronous_settings>
         </default>
     </profiles>
 </clickhouse>
diff --git a/tests/integration/test_distributed_ddl/test.py b/tests/integration/test_distributed_ddl/test.py
index 85d0a5f0999d..065795b55ebd 100755
--- a/tests/integration/test_distributed_ddl/test.py
+++ b/tests/integration/test_distributed_ddl/test.py
@@ -49,6 +49,7 @@ def test_default_database(test_cluster):
     test_cluster.ddl_check_query(
         instance,
         "CREATE TABLE null ON CLUSTER 'cluster2' (s String DEFAULT 'escape\t
me') ENGINE = Null",
+        settings={"distributed_ddl_entry_format_version": 2},
     )
 
     contents = instance.query(
@@ -57,7 +58,9 @@ def test_default_database(test_cluster):
     assert TSV(contents) == TSV("ch1\tdefault
ch2\ttest2
ch3\tdefault
ch4\ttest2
")
 
     test_cluster.ddl_check_query(
-        instance, "DROP TABLE IF EXISTS null ON CLUSTER cluster2"
+        instance,
+        "DROP TABLE IF EXISTS null ON CLUSTER cluster2",
+        settings={"distributed_ddl_entry_format_version": 2},
     )
     test_cluster.ddl_check_query(
         instance, "DROP DATABASE IF EXISTS test2 ON CLUSTER 'cluster'"
diff --git a/tests/integration/test_distributed_ddl_on_cross_replication/configs/settings.xml b/tests/integration/test_distributed_ddl_on_cross_replication/configs/settings.xml
new file mode 100644
index 000000000000..2387e2661e4c
--- /dev/null
+++ b/tests/integration/test_distributed_ddl_on_cross_replication/configs/settings.xml
@@ -0,0 +1,7 @@
+<clickhouse>
+    <profiles>
+        <default>
+            <distributed_ddl_entry_format_version>2</distributed_ddl_entry_format_version>
+        </default>
+    </profiles>
+</clickhouse>
\ No newline at end of file
diff --git a/tests/integration/test_distributed_ddl_on_cross_replication/test.py b/tests/integration/test_distributed_ddl_on_cross_replication/test.py
index b89091d4034b..2b710b4a3e3c 100644
--- a/tests/integration/test_distributed_ddl_on_cross_replication/test.py
+++ b/tests/integration/test_distributed_ddl_on_cross_replication/test.py
@@ -7,18 +7,21 @@
 node1 = cluster.add_instance(
     "node1",
     main_configs=["configs/remote_servers.xml"],
+    user_configs=["configs/settings.xml"],
     with_zookeeper=True,
     macros={"shard": 1, "replica": 1, "shard_bk": 3, "replica_bk": 2},
 )
 node2 = cluster.add_instance(
     "node2",
     main_configs=["configs/remote_servers.xml"],
+    user_configs=["configs/settings.xml"],
     with_zookeeper=True,
     macros={"shard": 2, "replica": 1, "shard_bk": 1, "replica_bk": 2},
 )
 node3 = cluster.add_instance(
     "node3",
     main_configs=["configs/remote_servers.xml"],
+    user_configs=["configs/settings.xml"],
     with_zookeeper=True,
     macros={"shard": 3, "replica": 1, "shard_bk": 2, "replica_bk": 2},
 )
diff --git a/tests/integration/test_replicated_database/test.py b/tests/integration/test_replicated_database/test.py
index 11ca0d2f9620..f716fac85082 100644
--- a/tests/integration/test_replicated_database/test.py
+++ b/tests/integration/test_replicated_database/test.py
@@ -3,6 +3,7 @@
 import time
 import re
 import pytest
+import threading
 
 from helpers.cluster import ClickHouseCluster
 from helpers.test_tools import assert_eq_with_retry, assert_logs_contain
@@ -417,7 +418,7 @@ def test_alters_from_different_replicas(started_cluster):
         "distributed_ddl_task_timeout": 5,
         "distributed_ddl_output_mode": "null_status_on_timeout",
     }
-    assert "shard1|replica2\t\\N\t\\N" in main_node.query(
+    assert "shard1\treplica2\tQUEUED\t" in main_node.query(
         "ALTER TABLE testdb.concurrent_test ADD COLUMN Added2 UInt32;",
         settings=settings,
     )
@@ -425,7 +426,7 @@ def test_alters_from_different_replicas(started_cluster):
         "distributed_ddl_task_timeout": 5,
         "distributed_ddl_output_mode": "never_throw",
     }
-    assert "shard1|replica2\t\\N\t\\N" in competing_node.query(
+    assert "shard1\treplica2\tQUEUED\t" in competing_node.query(
         "ALTER TABLE testdb.concurrent_test ADD COLUMN Added1 UInt32 AFTER Added0;",
         settings=settings,
     )
@@ -495,11 +496,11 @@ def test_alters_from_different_replicas(started_cluster):
     )
     res = main_node.query("ALTER TABLE testdb.concurrent_test DELETE WHERE UserID % 2")
     assert (
-        "shard1|replica1" in res
-        and "shard1|replica2" in res
-        and "shard1|replica3" in res
+        "shard1\treplica1\tOK" in res
+        and "shard1\treplica2\tOK" in res
+        and "shard1\treplica3\tOK" in res
     )
-    assert "shard2|replica1" in res and "shard2|replica2" in res
+    assert "shard2\treplica1\tOK" in res and "shard2\treplica2\tOK" in res
 
     expected = (
         "1\t1\tmain_node
"
@@ -553,65 +554,76 @@ def test_alters_from_different_replicas(started_cluster):
     snapshot_recovering_node.query("DROP DATABASE testdb SYNC")
 
 
-def test_recover_staled_replica(started_cluster):
+def create_some_tables(db):
+    settings = {"distributed_ddl_task_timeout": 0}
     main_node.query(
-        "CREATE DATABASE recover ENGINE = Replicated('/clickhouse/databases/recover', 'shard1', 'replica1');"
-    )
-    started_cluster.get_kazoo_client("zoo1").set(
-        "/clickhouse/databases/recover/logs_to_keep", b"10"
+        "CREATE TABLE {}.t1 (n int) ENGINE=Memory".format(db), settings=settings
     )
     dummy_node.query(
-        "CREATE DATABASE recover ENGINE = Replicated('/clickhouse/databases/recover', 'shard1', 'replica2');"
-    )
-
-    settings = {"distributed_ddl_task_timeout": 0}
-    main_node.query("CREATE TABLE recover.t1 (n int) ENGINE=Memory", settings=settings)
-    dummy_node.query(
-        "CREATE TABLE recover.t2 (s String) ENGINE=Memory", settings=settings
+        "CREATE TABLE {}.t2 (s String) ENGINE=Memory".format(db), settings=settings
     )
     main_node.query(
-        "CREATE TABLE recover.mt1 (n int) ENGINE=MergeTree order by n",
+        "CREATE TABLE {}.mt1 (n int) ENGINE=MergeTree order by n".format(db),
         settings=settings,
     )
     dummy_node.query(
-        "CREATE TABLE recover.mt2 (n int) ENGINE=MergeTree order by n",
+        "CREATE TABLE {}.mt2 (n int) ENGINE=MergeTree order by n".format(db),
         settings=settings,
     )
     main_node.query(
-        "CREATE TABLE recover.rmt1 (n int) ENGINE=ReplicatedMergeTree order by n",
+        "CREATE TABLE {}.rmt1 (n int) ENGINE=ReplicatedMergeTree order by n".format(db),
         settings=settings,
     )
     dummy_node.query(
-        "CREATE TABLE recover.rmt2 (n int) ENGINE=ReplicatedMergeTree order by n",
+        "CREATE TABLE {}.rmt2 (n int) ENGINE=ReplicatedMergeTree order by n".format(db),
         settings=settings,
     )
     main_node.query(
-        "CREATE TABLE recover.rmt3 (n int) ENGINE=ReplicatedMergeTree order by n",
+        "CREATE TABLE {}.rmt3 (n int) ENGINE=ReplicatedMergeTree order by n".format(db),
         settings=settings,
     )
     dummy_node.query(
-        "CREATE TABLE recover.rmt5 (n int) ENGINE=ReplicatedMergeTree order by n",
+        "CREATE TABLE {}.rmt5 (n int) ENGINE=ReplicatedMergeTree order by n".format(db),
         settings=settings,
     )
     main_node.query(
-        "CREATE MATERIALIZED VIEW recover.mv1 (n int) ENGINE=ReplicatedMergeTree order by n AS SELECT n FROM recover.rmt1",
+        "CREATE MATERIALIZED VIEW {}.mv1 (n int) ENGINE=ReplicatedMergeTree order by n AS SELECT n FROM recover.rmt1".format(
+            db
+        ),
         settings=settings,
     )
     dummy_node.query(
-        "CREATE MATERIALIZED VIEW recover.mv2 (n int) ENGINE=ReplicatedMergeTree order by n  AS SELECT n FROM recover.rmt2",
+        "CREATE MATERIALIZED VIEW {}.mv2 (n int) ENGINE=ReplicatedMergeTree order by n  AS SELECT n FROM recover.rmt2".format(
+            db
+        ),
         settings=settings,
     )
     main_node.query(
-        "CREATE DICTIONARY recover.d1 (n int DEFAULT 0, m int DEFAULT 1) PRIMARY KEY n "
+        "CREATE DICTIONARY {}.d1 (n int DEFAULT 0, m int DEFAULT 1) PRIMARY KEY n "
         "SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'rmt1' PASSWORD '' DB 'recover')) "
-        "LIFETIME(MIN 1 MAX 10) LAYOUT(FLAT())"
+        "LIFETIME(MIN 1 MAX 10) LAYOUT(FLAT())".format(db)
     )
     dummy_node.query(
-        "CREATE DICTIONARY recover.d2 (n int DEFAULT 0, m int DEFAULT 1) PRIMARY KEY n "
+        "CREATE DICTIONARY {}.d2 (n int DEFAULT 0, m int DEFAULT 1) PRIMARY KEY n "
         "SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'rmt2' PASSWORD '' DB 'recover')) "
-        "LIFETIME(MIN 1 MAX 10) LAYOUT(FLAT())"
+        "LIFETIME(MIN 1 MAX 10) LAYOUT(FLAT())".format(db)
     )
 
+
+def test_recover_staled_replica(started_cluster):
+    main_node.query(
+        "CREATE DATABASE recover ENGINE = Replicated('/clickhouse/databases/recover', 'shard1', 'replica1');"
+    )
+    started_cluster.get_kazoo_client("zoo1").set(
+        "/clickhouse/databases/recover/logs_to_keep", b"10"
+    )
+    dummy_node.query(
+        "CREATE DATABASE recover ENGINE = Replicated('/clickhouse/databases/recover', 'shard1', 'replica2');"
+    )
+
+    settings = {"distributed_ddl_task_timeout": 0}
+    create_some_tables("recover")
+
     for table in ["t1", "t2", "mt1", "mt2", "rmt1", "rmt2", "rmt3", "rmt5"]:
         main_node.query("INSERT INTO recover.{} VALUES (42)".format(table))
     for table in ["t1", "t2", "mt1", "mt2"]:
@@ -867,3 +879,106 @@ def test_sync_replica(started_cluster):
     )
     assert lp1 == max_lp
     assert lp2 == max_lp
+
+
+def test_force_synchronous_settings(started_cluster):
+    main_node.query(
+        "CREATE DATABASE test_force_synchronous_settings ENGINE = Replicated('/clickhouse/databases/test2', 'shard1', 'replica1');"
+    )
+    dummy_node.query(
+        "CREATE DATABASE test_force_synchronous_settings ENGINE = Replicated('/clickhouse/databases/test2', 'shard1', 'replica2');"
+    )
+    snapshotting_node.query(
+        "CREATE DATABASE test_force_synchronous_settings ENGINE = Replicated('/clickhouse/databases/test2', 'shard2', 'replica1');"
+    )
+    main_node.query(
+        "CREATE TABLE test_force_synchronous_settings.t (n int) ENGINE=ReplicatedMergeTree('/test/same/path/{shard}', '{replica}') ORDER BY tuple()"
+    )
+    main_node.query(
+        "INSERT INTO test_force_synchronous_settings.t SELECT * FROM numbers(10)"
+    )
+    snapshotting_node.query(
+        "INSERT INTO test_force_synchronous_settings.t SELECT * FROM numbers(10)"
+    )
+    snapshotting_node.query(
+        "SYSTEM SYNC DATABASE REPLICA test_force_synchronous_settings"
+    )
+    dummy_node.query("SYSTEM SYNC DATABASE REPLICA test_force_synchronous_settings")
+
+    snapshotting_node.query("SYSTEM STOP MERGES test_force_synchronous_settings.t")
+
+    def start_merges_func():
+        time.sleep(5)
+        snapshotting_node.query("SYSTEM START MERGES test_force_synchronous_settings.t")
+
+    start_merges_thread = threading.Thread(target=start_merges_func)
+    start_merges_thread.start()
+
+    settings = {
+        "mutations_sync": 2,
+        "database_replicated_enforce_synchronous_settings": 1,
+    }
+    main_node.query(
+        "ALTER TABLE test_force_synchronous_settings.t UPDATE n = n * 10 WHERE 1",
+        settings=settings,
+    )
+    assert "10\t450
" == snapshotting_node.query(
+        "SELECT count(), sum(n) FROM test_force_synchronous_settings.t"
+    )
+    start_merges_thread.join()
+
+    def select_func():
+        dummy_node.query(
+            "SELECT sleepEachRow(1) FROM test_force_synchronous_settings.t"
+        )
+
+    select_thread = threading.Thread(target=select_func)
+    select_thread.start()
+
+    settings = {"database_replicated_enforce_synchronous_settings": 1}
+    snapshotting_node.query(
+        "DROP TABLE test_force_synchronous_settings.t SYNC", settings=settings
+    )
+    main_node.query(
+        "CREATE TABLE test_force_synchronous_settings.t (n String) ENGINE=ReplicatedMergeTree('/test/same/path/{shard}', '{replica}') ORDER BY tuple()"
+    )
+    select_thread.join()
+
+
+def test_recover_digest_mismatch(started_cluster):
+    main_node.query(
+        "CREATE DATABASE recover_digest_mismatch ENGINE = Replicated('/clickhouse/databases/recover_digest_mismatch', 'shard1', 'replica1');"
+    )
+    dummy_node.query(
+        "CREATE DATABASE recover_digest_mismatch ENGINE = Replicated('/clickhouse/databases/recover_digest_mismatch', 'shard1', 'replica2');"
+    )
+
+    create_some_tables("recover_digest_mismatch")
+
+    ways_to_corrupt_metadata = [
+        f"mv /var/lib/clickhouse/metadata/recover_digest_mismatch/t1.sql /var/lib/clickhouse/metadata/recover_digest_mismatch/m1.sql",
+        f"sed --follow-symlinks -i 's/Int32/String/' /var/lib/clickhouse/metadata/recover_digest_mismatch/mv1.sql",
+        f"rm -f /var/lib/clickhouse/metadata/recover_digest_mismatch/d1.sql",
+        # f"rm -rf /var/lib/clickhouse/metadata/recover_digest_mismatch/", # Directory already exists
+        f"rm -rf /var/lib/clickhouse/store",
+    ]
+
+    for command in ways_to_corrupt_metadata:
+        need_remove_is_active_node = "rm -rf" in command
+        dummy_node.stop_clickhouse(kill=not need_remove_is_active_node)
+        dummy_node.exec_in_container(["bash", "-c", command])
+        dummy_node.start_clickhouse()
+
+        query = (
+            "SELECT name, uuid, create_table_query FROM system.tables WHERE database='recover_digest_mismatch' AND name NOT LIKE '.inner_id.%' "
+            "ORDER BY name SETTINGS show_table_uuid_in_table_create_query_if_not_nil=1"
+        )
+        expected = main_node.query(query)
+
+        if "rm -rf" in command:
+            # NOTE Otherwise it fails to recreate ReplicatedMergeTree table due to "Replica already exists"
+            main_node.query(
+                "SYSTEM DROP REPLICA '2' FROM DATABASE recover_digest_mismatch"
+            )
+
+        assert_eq_with_retry(dummy_node, query, expected)
diff --git a/tests/queries/0_stateless/00993_system_parts_race_condition_drop_zookeeper.sh b/tests/queries/0_stateless/00993_system_parts_race_condition_drop_zookeeper.sh
index 55ef2edd42b0..dc01ce40398f 100755
--- a/tests/queries/0_stateless/00993_system_parts_race_condition_drop_zookeeper.sh
+++ b/tests/queries/0_stateless/00993_system_parts_race_condition_drop_zookeeper.sh
@@ -58,7 +58,7 @@ function thread6()
         $CLICKHOUSE_CLIENT -n -q "DROP TABLE IF EXISTS alter_table_$REPLICA;
             CREATE TABLE alter_table_$REPLICA (a UInt8, b Int16, c Float32, d String, e Array(UInt8), f Nullable(UUID), g Tuple(UInt8, UInt16)) ENGINE = ReplicatedMergeTree('/clickhouse/tables/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX/alter_table', 'r_$REPLICA') ORDER BY a PARTITION BY b % 10 SETTINGS old_parts_lifetime = 1, cleanup_delay_period = 0, cleanup_delay_period_random_add = 0;";
         sleep 0.$RANDOM;
-        done
+    done
 }
 
 # https://stackoverflow.com/questions/9954794/execute-a-shell-function-with-timeout
diff --git a/tests/queries/0_stateless/01111_create_drop_replicated_db_stress.reference b/tests/queries/0_stateless/01111_create_drop_replicated_db_stress.reference
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/tests/queries/0_stateless/01111_create_drop_replicated_db_stress.sh b/tests/queries/0_stateless/01111_create_drop_replicated_db_stress.sh
new file mode 100755
index 000000000000..addf503e44aa
--- /dev/null
+++ b/tests/queries/0_stateless/01111_create_drop_replicated_db_stress.sh
@@ -0,0 +1,108 @@
+#!/usr/bin/env bash
+# Tags: race, zookeeper, no-backward-compatibility-check
+
+CURDIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
+# shellcheck source=../shell_config.sh
+. "$CURDIR"/../shell_config.sh
+
+
+function create_db()
+{
+    while true; do
+        SHARD=$(($RANDOM % 2))
+        REPLICA=$(($RANDOM % 2))
+        SUFFIX=$(($RANDOM % 16))
+        # Multiple database replicas on one server are actually not supported (until we have namespaces).
+        # So CREATE TABLE queries will fail on all replicas except one. But it's still makes sense for a stress test.
+        $CLICKHOUSE_CLIENT --allow_experimental_database_replicated=1 --query \
+        "create database if not exists ${CLICKHOUSE_DATABASE}_repl_$SUFFIX engine=Replicated('/test/01111/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX', '$SHARD', '$REPLICA')" \
+         2>&1| grep -Fa "Exception: " | grep -Fv "REPLICA_IS_ALREADY_EXIST" | grep -Fiv "Will not try to start it up" | \
+         grep -Fv "Coordination::Exception" | grep -Fv "already contains some data and it does not look like Replicated database path"
+        sleep 0.$RANDOM
+    done
+}
+
+function drop_db()
+{
+    while true; do
+        database=$($CLICKHOUSE_CLIENT -q "select name from system.databases where name like '${CLICKHOUSE_DATABASE}%' order by rand() limit 1")
+        if [[ "$database" == "$CLICKHOUSE_DATABASE" ]]; then return; fi
+        if [ -z "$database" ]; then return; fi
+        $CLICKHOUSE_CLIENT -n --query \
+        "drop database if exists $database" 2>&1| grep -Fa "Exception: "
+        sleep 0.$RANDOM
+    done
+}
+
+function sync_db()
+{
+    while true; do
+        database=$($CLICKHOUSE_CLIENT -q "select name from system.databases where name like '${CLICKHOUSE_DATABASE}%' order by rand() limit 1")
+        if [ -z "$database" ]; then return; fi
+        $CLICKHOUSE_CLIENT --receive_timeout=1 -q \
+        "system sync database replica $database" 2>&1| grep -Fa "Exception: " | grep -Fv TIMEOUT_EXCEEDED | grep -Fv "only with Replicated engine" | grep -Fv UNKNOWN_DATABASE
+        sleep 0.$RANDOM
+    done
+}
+
+function create_table()
+{
+    while true; do
+        database=$($CLICKHOUSE_CLIENT -q "select name from system.databases where name like '${CLICKHOUSE_DATABASE}%' order by rand() limit 1")
+        if [ -z "$database" ]; then return; fi
+        $CLICKHOUSE_CLIENT --distributed_ddl_task_timeout=0 -q \
+        "create table $database.rmt_$RANDOM (n int) engine=ReplicatedMergeTree order by tuple() -- suppress $CLICKHOUSE_TEST_ZOOKEEPER_PREFIX" \
+        2>&1| grep -Fa "Exception: " | grep -Fv "Macro 'uuid' and empty arguments" | grep -Fv "Cannot enqueue query" | grep -Fv "ZooKeeper session expired" | grep -Fv UNKNOWN_DATABASE
+        sleep 0.$RANDOM
+    done
+}
+
+function alter_table()
+{
+    while true; do
+        table=$($CLICKHOUSE_CLIENT -q "select database || '.' || name from system.tables where database like '${CLICKHOUSE_DATABASE}%' order by rand() limit 1")
+        if [ -z "$table" ]; then return; fi
+        $CLICKHOUSE_CLIENT --distributed_ddl_task_timeout=0 -q \
+        "alter table $table on cluster $database update n = n + (select max(n) from merge(REGEXP('${CLICKHOUSE_DATABASE}.*'), '.*')) where 1 settings allow_nondeterministic_mutations=1" \
+        2>&1| grep -Fa "Exception: " | grep -Fv "Cannot enqueue query" | grep -Fv "ZooKeeper session expired" | grep -Fv UNKNOWN_DATABASE | grep -Fv UNKNOWN_TABLE | grep -Fv TABLE_IS_READ_ONLY
+        sleep 0.$RANDOM
+    done
+}
+
+function insert()
+{
+    while true; do
+        table=$($CLICKHOUSE_CLIENT -q "select database || '.' || name from system.tables where database like '${CLICKHOUSE_DATABASE}%' order by rand() limit 1")
+        if [ -z "$table" ]; then return; fi
+        $CLICKHOUSE_CLIENT -q \
+        "insert into $table values ($RANDOM)" 2>&1| grep -Fa "Exception: " | grep -Fv UNKNOWN_DATABASE | grep -Fv UNKNOWN_TABLE | grep -Fv TABLE_IS_READ_ONLY
+    done
+}
+
+
+
+export -f create_db
+export -f drop_db
+export -f sync_db
+export -f create_table
+export -f alter_table
+export -f insert
+
+TIMEOUT=30
+
+timeout $TIMEOUT bash -c create_db &
+timeout $TIMEOUT bash -c sync_db &
+timeout $TIMEOUT bash -c create_table &
+timeout $TIMEOUT bash -c alter_table &
+timeout $TIMEOUT bash -c insert &
+
+sleep 1 # give other queries a head start
+timeout $TIMEOUT bash -c drop_db &
+
+wait
+
+readarray -t databases_arr < <(${CLICKHOUSE_CLIENT} -q "select name from system.databases where name like '${CLICKHOUSE_DATABASE}_%'")
+for db in "${databases_arr[@]}"
+do
+    $CLICKHOUSE_CLIENT -q "drop database if exists $db"
+done
diff --git a/tests/queries/0_stateless/01152_cross_replication.sql b/tests/queries/0_stateless/01152_cross_replication.sql
index 60b2c34be07c..5d0134005394 100644
--- a/tests/queries/0_stateless/01152_cross_replication.sql
+++ b/tests/queries/0_stateless/01152_cross_replication.sql
@@ -8,6 +8,8 @@ DROP TABLE IF EXISTS demo_loan_01568_dist;
 CREATE DATABASE shard_0;
 CREATE DATABASE shard_1;
 
+CREATE TABLE demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat); -- { serverError 48 }
+SET distributed_ddl_entry_format_version = 2;
 CREATE TABLE demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat); -- { serverError 371 }
 SET distributed_ddl_output_mode='throw';
 CREATE TABLE shard_0.demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat);
diff --git a/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.reference b/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.reference
index bedf9e9a091c..4397810b68d7 100644
--- a/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.reference
+++ b/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.reference
@@ -27,19 +27,19 @@ localhost	9000	57	Code: 57. Error: Table default.never_throw already exists. (TA
 localhost	9000	0		1	0
 localhost	1	\N	\N	1	0
 distributed_ddl_queue
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.none ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.none ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.none already exists. (TABLE_ALREADY_EXISTS)	1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.none ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.none ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.none already exists. (TABLE_ALREADY_EXISTS)	1	1
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.none ON CLUSTER test_unavailable_shard	1	localhost	1	Inactive	\N	\N	\N	\N
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.none ON CLUSTER test_unavailable_shard	1	localhost	9000	Finished	0		1	1
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.throw ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.throw ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.throw already exists. (TABLE_ALREADY_EXISTS)	1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.throw ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.throw ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.throw already exists. (TABLE_ALREADY_EXISTS)	1	1
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.throw ON CLUSTER test_unavailable_shard	1	localhost	1	Inactive	\N	\N	\N	\N
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.throw ON CLUSTER test_unavailable_shard	1	localhost	9000	Finished	0		1	1
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.null_status ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.null_status ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.null_status already exists. (TABLE_ALREADY_EXISTS)	1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.null_status ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.null_status ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.null_status already exists. (TABLE_ALREADY_EXISTS)	1	1
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.null_status ON CLUSTER test_unavailable_shard	1	localhost	1	Inactive	\N	\N	\N	\N
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.null_status ON CLUSTER test_unavailable_shard	1	localhost	9000	Finished	0		1	1
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.never_throw ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
-2	localhost	9000	test_shard_localhost	CREATE TABLE default.never_throw ON CLUSTER test_shard_localhost (`n` int) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.never_throw already exists. (TABLE_ALREADY_EXISTS)	1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.never_throw ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	0		1	1
+2	localhost	9000	test_shard_localhost	CREATE TABLE default.never_throw ON CLUSTER test_shard_localhost (`n` Int32) ENGINE = Memory	1	localhost	9000	Finished	57	Code: 57. DB::Error: Table default.never_throw already exists. (TABLE_ALREADY_EXISTS)	1	1
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.never_throw ON CLUSTER test_unavailable_shard	1	localhost	1	Inactive	\N	\N	\N	\N
 2	localhost	9000	test_unavailable_shard	DROP TABLE IF EXISTS default.never_throw ON CLUSTER test_unavailable_shard	1	localhost	9000	Finished	0		1	1
diff --git a/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.sh b/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.sh
index e632841bd011..c18514d0ecc4 100755
--- a/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.sh
+++ b/tests/queries/0_stateless/01175_distributed_ddl_output_mode_long.sh
@@ -38,7 +38,6 @@ LOG_COMMENT="${CLICKHOUSE_LOG_COMMENT}_$RAND_COMMENT"
 
 CLICKHOUSE_CLIENT_WITH_SETTINGS=${CLICKHOUSE_CLIENT/--log_comment ${CLICKHOUSE_LOG_COMMENT}/--log_comment ${LOG_COMMENT}}
 CLICKHOUSE_CLIENT_WITH_SETTINGS+=" --output_format_parallel_formatting=0 "
-CLICKHOUSE_CLIENT_WITH_SETTINGS+=" --distributed_ddl_entry_format_version=2 "
 
 CLIENT=${CLICKHOUSE_CLIENT_WITH_SETTINGS}
 CLIENT+=" --distributed_ddl_task_timeout=$TIMEOUT "
diff --git a/tests/queries/0_stateless/02232_allow_only_replicated_engine.sh b/tests/queries/0_stateless/02232_allow_only_replicated_engine.sh
index c84b1ab0e555..3ff2dabfa43c 100755
--- a/tests/queries/0_stateless/02232_allow_only_replicated_engine.sh
+++ b/tests/queries/0_stateless/02232_allow_only_replicated_engine.sh
@@ -11,8 +11,8 @@ ${CLICKHOUSE_CLIENT} -q "CREATE USER user_${CLICKHOUSE_DATABASE} settings databa
 ${CLICKHOUSE_CLIENT} -q "GRANT CREATE TABLE ON ${CLICKHOUSE_DATABASE}_db.* TO user_${CLICKHOUSE_DATABASE}"
 ${CLICKHOUSE_CLIENT} --allow_experimental_database_replicated=1 --query "CREATE DATABASE ${CLICKHOUSE_DATABASE}_db engine = Replicated('/clickhouse/databases/${CLICKHOUSE_TEST_ZOOKEEPER_PREFIX}/${CLICKHOUSE_DATABASE}_db', '{shard}', '{replica}')"
 ${CLICKHOUSE_CLIENT} --distributed_ddl_output_mode=none --user "user_${CLICKHOUSE_DATABASE}" --query "CREATE TABLE ${CLICKHOUSE_DATABASE}_db.tab_memory (x UInt32) engine = Memory;"
-${CLICKHOUSE_CLIENT} --distributed_ddl_output_mode=none --user "user_${CLICKHOUSE_DATABASE}" -n --query "set distributed_ddl_entry_format_version=2; CREATE TABLE ${CLICKHOUSE_DATABASE}_db.tab_mt (x UInt32) engine = MergeTree order by x;" 2>&1 | grep -o "Only tables with a Replicated engine"
-${CLICKHOUSE_CLIENT} --distributed_ddl_output_mode=none -n --query "set distributed_ddl_entry_format_version=2; CREATE TABLE ${CLICKHOUSE_DATABASE}_db.tab_mt (x UInt32) engine = MergeTree order by x;"
-${CLICKHOUSE_CLIENT} --distributed_ddl_output_mode=none --user "user_${CLICKHOUSE_DATABASE}" -n --query "set distributed_ddl_entry_format_version=2; CREATE TABLE ${CLICKHOUSE_DATABASE}_db.tab_rmt (x UInt32) engine = ReplicatedMergeTree order by x;"
+${CLICKHOUSE_CLIENT} --distributed_ddl_output_mode=none --user "user_${CLICKHOUSE_DATABASE}" -n --query "CREATE TABLE ${CLICKHOUSE_DATABASE}_db.tab_mt (x UInt32) engine = MergeTree order by x;" 2>&1 | grep -o "Only tables with a Replicated engine"
+${CLICKHOUSE_CLIENT} --distributed_ddl_output_mode=none -n --query "CREATE TABLE ${CLICKHOUSE_DATABASE}_db.tab_mt (x UInt32) engine = MergeTree order by x;"
+${CLICKHOUSE_CLIENT} --distributed_ddl_output_mode=none --user "user_${CLICKHOUSE_DATABASE}" -n --query "CREATE TABLE ${CLICKHOUSE_DATABASE}_db.tab_rmt (x UInt32) engine = ReplicatedMergeTree order by x;"
 ${CLICKHOUSE_CLIENT} --query "DROP DATABASE ${CLICKHOUSE_DATABASE}_db"
 ${CLICKHOUSE_CLIENT} -q "DROP USER user_${CLICKHOUSE_DATABASE}"
diff --git a/tests/queries/0_stateless/02400_create_table_on_cluster_normalization.reference b/tests/queries/0_stateless/02400_create_table_on_cluster_normalization.reference
new file mode 100644
index 000000000000..c00653f2bb3a
--- /dev/null
+++ b/tests/queries/0_stateless/02400_create_table_on_cluster_normalization.reference
@@ -0,0 +1,5 @@
+localhost	9000	0		0	0
+localhost	9000	0		0	0
+1	2	3	4
+5	6	7	8
+CREATE TABLE default.t_l5ydey
(
    `c_qv5rv` Int32,
    `c_rutjs4` Int32,
    `c_wmj` Int32,
    `c_m3` String
)
ENGINE = Distributed(\'test_shard_localhost\', \'default\', \'local_t_l5ydey\', rand())
diff --git a/tests/queries/0_stateless/02400_create_table_on_cluster_normalization.sql b/tests/queries/0_stateless/02400_create_table_on_cluster_normalization.sql
new file mode 100644
index 000000000000..54e4ccf6762d
--- /dev/null
+++ b/tests/queries/0_stateless/02400_create_table_on_cluster_normalization.sql
@@ -0,0 +1,27 @@
+-- Tags: no-replicated-database
+-- Tag no-replicated-database: ON CLUSTER is not allowed
+drop table if exists local_t_l5ydey;
+
+create table local_t_l5ydey on cluster test_shard_localhost (
+    c_qv5rv INTEGER ,
+    c_rutjs4 INTEGER ,
+    c_wmj INTEGER ,
+    c_m3 TEXT NOT NULL,
+    primary key(c_qv5rv)
+) engine=ReplicatedMergeTree('/clickhouse/tables/test_' || currentDatabase() || '/{shard}/local_t_l5ydey', '{replica}');
+
+create table t_l5ydey on cluster test_shard_localhost as local_t_l5ydey
+    engine=Distributed('test_shard_localhost', currentDatabase(),'local_t_l5ydey', rand());
+
+insert into local_t_l5ydey values (1, 2, 3, '4');
+insert into t_l5ydey values (5, 6, 7, '8');
+system flush distributed t_l5ydey;
+
+select * from t_l5ydey order by c_qv5rv;
+show create t_l5ydey;
+
+-- Correct error code if creating database with the same path as table has
+set allow_experimental_database_replicated=1;
+create database local_t_l5ydey engine=Replicated('/clickhouse/tables/test_' || currentDatabase() || '/{shard}/local_t_l5ydey', '1', '1'); -- { serverError BAD_ARGUMENTS }
+
+drop table local_t_l5ydey;
