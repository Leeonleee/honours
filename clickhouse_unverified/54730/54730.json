{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 54730,
  "instance_id": "ClickHouse__ClickHouse-54730",
  "issue_numbers": [
    "54381",
    "48430"
  ],
  "base_commit": "df037e079dc9d1044c4c8c28340a3020180959bb",
  "patch": "diff --git a/src/Client/ClientBase.cpp b/src/Client/ClientBase.cpp\nindex 30ffffdeeb1e..9e368c9c3d15 100644\n--- a/src/Client/ClientBase.cpp\n+++ b/src/Client/ClientBase.cpp\n@@ -106,6 +106,7 @@ namespace ErrorCodes\n     extern const int CANNOT_OPEN_FILE;\n     extern const int FILE_ALREADY_EXISTS;\n     extern const int USER_SESSION_LIMIT_EXCEEDED;\n+    extern const int NOT_IMPLEMENTED;\n }\n \n }\n@@ -1384,7 +1385,7 @@ void ClientBase::processInsertQuery(const String & query_to_execute, ASTPtr pars\n     }\n \n     /// Process the query that requires transferring data blocks to the server.\n-    const auto parsed_insert_query = parsed_query->as<ASTInsertQuery &>();\n+    const auto & parsed_insert_query = parsed_query->as<ASTInsertQuery &>();\n     if ((!parsed_insert_query.data && !parsed_insert_query.infile) && (is_interactive || (!stdin_is_a_tty && std_in.eof())))\n     {\n         const auto & settings = global_context->getSettingsRef();\n@@ -1830,10 +1831,20 @@ void ClientBase::processParsedSingleQuery(const String & full_query, const Strin\n         if (insert && insert->select)\n             insert->tryFindInputFunction(input_function);\n \n-        bool is_async_insert = global_context->getSettingsRef().async_insert && insert && insert->hasInlinedData();\n+        bool is_async_insert_with_inlined_data = global_context->getSettingsRef().async_insert && insert && insert->hasInlinedData();\n+\n+        if (is_async_insert_with_inlined_data)\n+        {\n+            bool have_data_in_stdin = !is_interactive && !stdin_is_a_tty && !std_in.eof();\n+            bool have_external_data = have_data_in_stdin || insert->infile;\n+\n+            if (have_external_data)\n+                throw Exception(ErrorCodes::NOT_IMPLEMENTED,\n+                    \"Processing async inserts with both inlined and external data (from stdin or infile) is not supported\");\n+        }\n \n         /// INSERT query for which data transfer is needed (not an INSERT SELECT or input()) is processed separately.\n-        if (insert && (!insert->select || input_function) && !insert->watch && !is_async_insert)\n+        if (insert && (!insert->select || input_function) && !insert->watch && !is_async_insert_with_inlined_data)\n         {\n             if (input_function && insert->format.empty())\n                 throw Exception(ErrorCodes::INVALID_USAGE_OF_INPUT, \"FORMAT must be specified for function input()\");\ndiff --git a/src/Client/LocalConnection.cpp b/src/Client/LocalConnection.cpp\nindex c05f039ffb61..15ba1bcd1991 100644\n--- a/src/Client/LocalConnection.cpp\n+++ b/src/Client/LocalConnection.cpp\n@@ -125,7 +125,7 @@ void LocalConnection::sendQuery(\n \n     try\n     {\n-        state->io = executeQuery(state->query, query_context, false, state->stage);\n+        state->io = executeQuery(state->query, query_context, false, state->stage).second;\n \n         if (state->io.pipeline.pushing())\n         {\ndiff --git a/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp b/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp\nindex 7409e80ac8cd..3d10e66e9645 100644\n--- a/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp\n+++ b/src/Databases/MySQL/MaterializedMySQLSyncThread.cpp\n@@ -75,7 +75,7 @@ static BlockIO tryToExecuteQuery(const String & query_to_execute, ContextMutable\n         if (!database.empty())\n             query_context->setCurrentDatabase(database);\n \n-        return executeQuery(\"/*\" + comment + \"*/ \" + query_to_execute, query_context, true);\n+        return executeQuery(\"/*\" + comment + \"*/ \" + query_to_execute, query_context, true).second;\n     }\n     catch (...)\n     {\ndiff --git a/src/Dictionaries/ClickHouseDictionarySource.cpp b/src/Dictionaries/ClickHouseDictionarySource.cpp\nindex 2dc7f6145b31..92fae2bc4959 100644\n--- a/src/Dictionaries/ClickHouseDictionarySource.cpp\n+++ b/src/Dictionaries/ClickHouseDictionarySource.cpp\n@@ -168,8 +168,7 @@ QueryPipeline ClickHouseDictionarySource::createStreamForQuery(const String & qu\n \n     if (configuration.is_local)\n     {\n-        pipeline = executeQuery(query, context_copy, true).pipeline;\n-\n+        pipeline = executeQuery(query, context_copy, true).second.pipeline;\n         pipeline.convertStructureTo(empty_sample_block.getColumnsWithTypeAndName());\n     }\n     else\n@@ -191,7 +190,7 @@ std::string ClickHouseDictionarySource::doInvalidateQuery(const std::string & re\n \n     if (configuration.is_local)\n     {\n-        return readInvalidateQuery(executeQuery(request, context_copy, true).pipeline);\n+        return readInvalidateQuery(executeQuery(request, context_copy, true).second.pipeline);\n     }\n     else\n     {\ndiff --git a/src/Interpreters/Access/InterpreterShowAccessEntitiesQuery.cpp b/src/Interpreters/Access/InterpreterShowAccessEntitiesQuery.cpp\nindex e7b9237b680f..b0937dc2f668 100644\n--- a/src/Interpreters/Access/InterpreterShowAccessEntitiesQuery.cpp\n+++ b/src/Interpreters/Access/InterpreterShowAccessEntitiesQuery.cpp\n@@ -23,7 +23,7 @@ InterpreterShowAccessEntitiesQuery::InterpreterShowAccessEntitiesQuery(const AST\n \n BlockIO InterpreterShowAccessEntitiesQuery::execute()\n {\n-    return executeQuery(getRewrittenQuery(), getContext(), true);\n+    return executeQuery(getRewrittenQuery(), getContext(), true).second;\n }\n \n \ndiff --git a/src/Interpreters/Access/InterpreterShowPrivilegesQuery.cpp b/src/Interpreters/Access/InterpreterShowPrivilegesQuery.cpp\nindex 05aa74d7dc43..213e3c813faa 100644\n--- a/src/Interpreters/Access/InterpreterShowPrivilegesQuery.cpp\n+++ b/src/Interpreters/Access/InterpreterShowPrivilegesQuery.cpp\n@@ -12,7 +12,7 @@ InterpreterShowPrivilegesQuery::InterpreterShowPrivilegesQuery(const ASTPtr & qu\n \n BlockIO InterpreterShowPrivilegesQuery::execute()\n {\n-    return executeQuery(\"SELECT * FROM system.privileges\", context, true);\n+    return executeQuery(\"SELECT * FROM system.privileges\", context, true).second;\n }\n \n }\ndiff --git a/src/Interpreters/AsynchronousInsertLog.cpp b/src/Interpreters/AsynchronousInsertLog.cpp\nindex d7c9059d9de8..092862bb2b11 100644\n--- a/src/Interpreters/AsynchronousInsertLog.cpp\n+++ b/src/Interpreters/AsynchronousInsertLog.cpp\n@@ -24,6 +24,13 @@ NamesAndTypesList AsynchronousInsertLogElement::getNamesAndTypes()\n             {\"FlushError\",   static_cast<Int8>(Status::FlushError)},\n         });\n \n+    auto type_data_kind = std::make_shared<DataTypeEnum8>(\n+        DataTypeEnum8::Values\n+        {\n+            {\"Parsed\",       static_cast<Int8>(DataKind::Parsed)},\n+            {\"Preprocessed\", static_cast<Int8>(DataKind::Preprocessed)},\n+        });\n+\n     return\n     {\n         {\"event_date\", std::make_shared<DataTypeDate>()},\n@@ -39,6 +46,7 @@ NamesAndTypesList AsynchronousInsertLogElement::getNamesAndTypes()\n         {\"rows\", std::make_shared<DataTypeUInt64>()},\n         {\"exception\", std::make_shared<DataTypeString>()},\n         {\"status\", type_status},\n+        {\"data_kind\", type_data_kind},\n \n         {\"flush_time\", std::make_shared<DataTypeDateTime>()},\n         {\"flush_time_microseconds\", std::make_shared<DataTypeDateTime64>(6)},\n@@ -64,6 +72,7 @@ void AsynchronousInsertLogElement::appendToBlock(MutableColumns & columns) const\n     columns[i++]->insert(rows);\n     columns[i++]->insert(exception);\n     columns[i++]->insert(status);\n+    columns[i++]->insert(data_kind);\n \n     columns[i++]->insert(flush_time);\n     columns[i++]->insert(flush_time_microseconds);\ndiff --git a/src/Interpreters/AsynchronousInsertLog.h b/src/Interpreters/AsynchronousInsertLog.h\nindex a76db78d3eaa..3a93b29dabe5 100644\n--- a/src/Interpreters/AsynchronousInsertLog.h\n+++ b/src/Interpreters/AsynchronousInsertLog.h\n@@ -1,6 +1,6 @@\n #pragma once\n \n-#include \"Common/Exception.h\"\n+#include <Interpreters/AsynchronousInsertQueue.h>\n #include <Interpreters/SystemLog.h>\n #include <Core/NamesAndTypes.h>\n #include <Core/NamesAndAliases.h>\n@@ -31,6 +31,9 @@ struct AsynchronousInsertLogElement\n     String exception;\n     Status status{};\n \n+    using DataKind = AsynchronousInsertQueue::DataKind;\n+    DataKind data_kind{};\n+\n     time_t flush_time{};\n     Decimal64 flush_time_microseconds{};\n     String flush_query_id;\ndiff --git a/src/Interpreters/AsynchronousInsertQueue.cpp b/src/Interpreters/AsynchronousInsertQueue.cpp\nindex a343fae6251e..70ce2df813c9 100644\n--- a/src/Interpreters/AsynchronousInsertQueue.cpp\n+++ b/src/Interpreters/AsynchronousInsertQueue.cpp\n@@ -58,47 +58,33 @@ namespace ErrorCodes\n     extern const int UNKNOWN_EXCEPTION;\n     extern const int UNKNOWN_FORMAT;\n     extern const int BAD_ARGUMENTS;\n+    extern const int LOGICAL_ERROR;\n }\n \n-AsynchronousInsertQueue::InsertQuery::InsertQuery(const ASTPtr & query_, const Settings & settings_, const std::optional<UUID> & user_id_, const std::vector<UUID> & current_roles_)\n+static const NameSet settings_to_skip\n+{\n+    /// We don't consider this setting because it is only for deduplication,\n+    /// which means we can put two inserts with different tokens in the same block safely.\n+    \"insert_deduplication_token\",\n+    \"log_comment\",\n+};\n+\n+AsynchronousInsertQueue::InsertQuery::InsertQuery(\n+    const ASTPtr & query_,\n+    const std::optional<UUID> & user_id_,\n+    const std::vector<UUID> & current_roles_,\n+    const Settings & settings_,\n+    DataKind data_kind_)\n     : query(query_->clone())\n     , query_str(queryToString(query))\n-    , settings(settings_)\n     , user_id(user_id_)\n     , current_roles(current_roles_)\n-    , hash(calculateHash())\n-{\n-}\n-\n-AsynchronousInsertQueue::InsertQuery::InsertQuery(const InsertQuery & other)\n-    : query(other.query->clone())\n-    , query_str(other.query_str)\n-    , settings(other.settings)\n-    , user_id(other.user_id)\n-    , current_roles(other.current_roles)\n-    , hash(other.hash)\n-{\n-}\n-\n-AsynchronousInsertQueue::InsertQuery &\n-AsynchronousInsertQueue::InsertQuery::operator=(const InsertQuery & other)\n-{\n-    if (this != &other)\n-    {\n-        query = other.query->clone();\n-        query_str = other.query_str;\n-        user_id = other.user_id;\n-        current_roles = other.current_roles;\n-        settings = other.settings;\n-        hash = other.hash;\n-    }\n-\n-    return *this;\n-}\n-\n-UInt128 AsynchronousInsertQueue::InsertQuery::calculateHash() const\n+    , settings(settings_)\n+    , data_kind(data_kind_)\n {\n     SipHash siphash;\n+\n+    siphash.update(data_kind);\n     query->updateTreeHash(siphash);\n \n     if (user_id)\n@@ -110,26 +96,50 @@ UInt128 AsynchronousInsertQueue::InsertQuery::calculateHash() const\n \n     for (const auto & setting : settings.allChanged())\n     {\n-        /// We don't consider this setting because it is only for deduplication,\n-        /// which means we can put two inserts with different tokens in the same block safely.\n-        if (setting.getName() == \"insert_deduplication_token\")\n+        if (settings_to_skip.contains(setting.getName()))\n             continue;\n+\n+        setting_changes.emplace_back(setting.getName(), setting.getValue());\n         siphash.update(setting.getName());\n         applyVisitor(FieldVisitorHash(siphash), setting.getValue());\n     }\n \n-    return siphash.get128();\n+    hash = siphash.get128();\n+}\n+\n+AsynchronousInsertQueue::InsertQuery &\n+AsynchronousInsertQueue::InsertQuery::operator=(const InsertQuery & other)\n+{\n+    if (this != &other)\n+    {\n+        query = other.query->clone();\n+        query_str = other.query_str;\n+        user_id = other.user_id;\n+        current_roles = other.current_roles;\n+        settings = other.settings;\n+        data_kind = other.data_kind;\n+        hash = other.hash;\n+        setting_changes = other.setting_changes;\n+    }\n+\n+    return *this;\n }\n \n bool AsynchronousInsertQueue::InsertQuery::operator==(const InsertQuery & other) const\n {\n-    return query_str == other.query_str && user_id == other.user_id && current_roles == other.current_roles && settings == other.settings;\n+    return toTupleCmp() == other.toTupleCmp();\n }\n \n-AsynchronousInsertQueue::InsertData::Entry::Entry(String && bytes_, String && query_id_, const String & async_dedup_token_, MemoryTracker * user_memory_tracker_)\n-    : bytes(std::move(bytes_))\n+AsynchronousInsertQueue::InsertData::Entry::Entry(\n+    DataChunk && chunk_,\n+    String && query_id_,\n+    const String & async_dedup_token_,\n+    const String & format_,\n+    MemoryTracker * user_memory_tracker_)\n+    : chunk(std::move(chunk_))\n     , query_id(std::move(query_id_))\n     , async_dedup_token(async_dedup_token_)\n+    , format(format_)\n     , user_memory_tracker(user_memory_tracker_)\n     , create_time(std::chrono::system_clock::now())\n {\n@@ -146,7 +156,7 @@ void AsynchronousInsertQueue::InsertData::Entry::finish(std::exception_ptr excep\n         // Each entry in the list may correspond to a different user,\n         // so we need to switch current thread's MemoryTracker.\n         MemoryTrackerSwitcher switcher(user_memory_tracker);\n-        bytes = \"\";\n+        chunk = {};\n     }\n \n     if (exception_)\n@@ -216,15 +226,12 @@ void AsynchronousInsertQueue::scheduleDataProcessingJob(const InsertQuery & key,\n     });\n }\n \n-AsynchronousInsertQueue::PushResult\n-AsynchronousInsertQueue::push(ASTPtr query, ContextPtr query_context)\n+void AsynchronousInsertQueue::preprocessInsertQuery(const ASTPtr & query, const ContextPtr & query_context)\n {\n-    query = query->clone();\n-    const auto & settings = query_context->getSettingsRef();\n     auto & insert_query = query->as<ASTInsertQuery &>();\n     insert_query.async_insert_flush = true;\n \n-    InterpreterInsertQuery interpreter(query, query_context, settings.insert_allow_materialized_columns);\n+    InterpreterInsertQuery interpreter(query, query_context, query_context->getSettingsRef().insert_allow_materialized_columns);\n     auto table = interpreter.getTable(insert_query);\n     auto sample_block = interpreter.getSampleBlock(insert_query, table, table->getInMemoryMetadataPtr());\n \n@@ -235,6 +242,13 @@ AsynchronousInsertQueue::push(ASTPtr query, ContextPtr query_context)\n     /// InterpreterInsertQuery::getTable() -> ITableFunction::execute().\n     if (insert_query.table_id)\n         query_context->checkAccess(AccessType::INSERT, insert_query.table_id, sample_block.getNames());\n+}\n+\n+AsynchronousInsertQueue::PushResult\n+AsynchronousInsertQueue::pushQueryWithInlinedData(ASTPtr query, ContextPtr query_context)\n+{\n+    query = query->clone();\n+    preprocessInsertQuery(query, query_context);\n \n     String bytes;\n     {\n@@ -245,7 +259,7 @@ AsynchronousInsertQueue::push(ASTPtr query, ContextPtr query_context)\n         auto read_buf = getReadBufferFromASTInsertQuery(query);\n \n         LimitReadBuffer limit_buf(\n-            *read_buf, settings.async_insert_max_data_size,\n+            *read_buf, query_context->getSettingsRef().async_insert_max_data_size,\n             /*throw_exception=*/ false, /*exact_limit=*/ {});\n \n         WriteBufferFromString write_buf(bytes);\n@@ -270,9 +284,35 @@ AsynchronousInsertQueue::push(ASTPtr query, ContextPtr query_context)\n         }\n     }\n \n-    auto entry = std::make_shared<InsertData::Entry>(std::move(bytes), query_context->getCurrentQueryId(), settings.insert_deduplication_token, CurrentThread::getUserMemoryTracker());\n+    return pushDataChunk(std::move(query), std::move(bytes), std::move(query_context));\n+}\n+\n+AsynchronousInsertQueue::PushResult\n+AsynchronousInsertQueue::pushQueryWithBlock(ASTPtr query, Block block, ContextPtr query_context)\n+{\n+    query = query->clone();\n+    preprocessInsertQuery(query, query_context);\n+    return pushDataChunk(std::move(query), std::move(block), std::move(query_context));\n+}\n+\n+AsynchronousInsertQueue::PushResult\n+AsynchronousInsertQueue::pushDataChunk(ASTPtr query, DataChunk chunk, ContextPtr query_context)\n+{\n+    const auto & settings = query_context->getSettingsRef();\n+    auto & insert_query = query->as<ASTInsertQuery &>();\n+\n+    auto data_kind = chunk.getDataKind();\n+    auto entry = std::make_shared<InsertData::Entry>(\n+        std::move(chunk), query_context->getCurrentQueryId(),\n+        settings.insert_deduplication_token, insert_query.format,\n+        CurrentThread::getUserMemoryTracker());\n \n-    InsertQuery key{query, settings, query_context->getUserID(), query_context->getCurrentRoles()};\n+    /// If data is parsed on client we don't care of format which is written\n+    /// in INSERT query. Replace it to put all such queries into one bucket in queue.\n+    if (data_kind == DataKind::Preprocessed)\n+        insert_query.format = \"Native\";\n+\n+    InsertQuery key{query, query_context->getUserID(), query_context->getCurrentRoles(), settings, data_kind};\n     InsertDataPtr data_to_process;\n     std::future<void> insert_future;\n \n@@ -292,7 +332,7 @@ AsynchronousInsertQueue::push(ASTPtr query, ContextPtr query_context)\n \n         auto queue_it = it->second;\n         auto & data = queue_it->second.data;\n-        size_t entry_data_size = entry->bytes.size();\n+        size_t entry_data_size = entry->chunk.byteSize();\n \n         assert(data);\n         data->size_in_bytes += entry_data_size;\n@@ -456,6 +496,13 @@ catch (...)\n     tryLogCurrentException(\"AsynchronousInsertQueue\", \"Failed to add elements to AsynchronousInsertLog\");\n }\n \n+String serializeQuery(const IAST & query, size_t max_length)\n+{\n+    return query.hasSecretParts()\n+        ? query.formatForLogging(max_length)\n+        : wipeSensitiveDataAndCutToLength(serializeAST(query), max_length);\n+}\n+\n }\n \n // static\n@@ -469,6 +516,7 @@ try\n \n     const auto * log = &Poco::Logger::get(\"AsynchronousInsertQueue\");\n     const auto & insert_query = assert_cast<const ASTInsertQuery &>(*key.query);\n+\n     auto insert_context = Context::createCopy(global_context);\n     bool internal = false; // To enable logging this query\n     bool async_insert = true;\n@@ -478,6 +526,10 @@ try\n \n     /// 'resetParser' doesn't work for parallel parsing.\n     key.settings.set(\"input_format_parallel_parsing\", false);\n+    /// It maybe insert into distributed table.\n+    /// It doesn't make sense to make insert into destination tables asynchronous.\n+    key.settings.set(\"async_insert\", false);\n+\n     insert_context->makeQueryContext();\n \n     /// Access rights must be checked for the user who executed the initial INSERT query.\n@@ -491,6 +543,7 @@ try\n \n     auto insert_query_id = insert_context->getCurrentQueryId();\n     auto query_start_time = std::chrono::system_clock::now();\n+\n     Stopwatch start_watch{CLOCK_MONOTONIC};\n     insert_context->setQueryKind(ClientInfo::QueryKind::INITIAL_QUERY);\n     insert_context->setInitialQueryStartTime(query_start_time);\n@@ -499,36 +552,45 @@ try\n \n     DB::CurrentThread::QueryScope query_scope_holder(insert_context);\n \n-    size_t log_queries_cut_to_length = insert_context->getSettingsRef().log_queries_cut_to_length;\n-    String query_for_logging = insert_query.hasSecretParts()\n-        ? insert_query.formatForLogging(log_queries_cut_to_length)\n-        : wipeSensitiveDataAndCutToLength(serializeAST(insert_query), log_queries_cut_to_length);\n+    auto query_for_logging = serializeQuery(*key.query, insert_context->getSettingsRef().log_queries_cut_to_length);\n \n     /// We add it to the process list so\n     /// a) it appears in system.processes\n     /// b) can be cancelled if we want to\n     /// c) has an associated process list element where runtime metrics are stored\n-    auto process_list_entry\n-        = insert_context->getProcessList().insert(query_for_logging, key.query.get(), insert_context, start_watch.getStart());\n+    auto process_list_entry = insert_context->getProcessList().insert(\n+        query_for_logging,\n+        key.query.get(),\n+        insert_context,\n+        start_watch.getStart());\n+\n     auto query_status = process_list_entry->getQueryStatus();\n     insert_context->setProcessListElement(std::move(query_status));\n \n-    String query_database{};\n-    String query_table{};\n+    String query_database;\n+    String query_table;\n+\n     if (insert_query.table_id)\n     {\n         query_database = insert_query.table_id.getDatabaseName();\n         query_table = insert_query.table_id.getTableName();\n         insert_context->setInsertionTable(insert_query.table_id);\n     }\n+\n     std::unique_ptr<DB::IInterpreter> interpreter;\n     QueryPipeline pipeline;\n     QueryLogElement query_log_elem;\n \n+    auto async_insert_log = global_context->getAsynchronousInsertLog();\n+    std::vector<AsynchronousInsertLogElement> log_elements;\n+    if (async_insert_log)\n+        log_elements.reserve(data->entries.size());\n+\n     try\n     {\n         interpreter = std::make_unique<InterpreterInsertQuery>(\n             key.query, insert_context, key.settings.insert_allow_materialized_columns, false, false, true);\n+\n         pipeline = interpreter->execute().pipeline;\n         chassert(pipeline.pushing());\n \n@@ -550,92 +612,39 @@ try\n         throw;\n     }\n \n-    auto header = pipeline.getHeader();\n-    auto format = getInputFormatFromASTInsertQuery(key.query, false, header, insert_context, nullptr);\n-\n-    size_t total_rows = 0;\n-    InsertData::EntryPtr current_entry;\n-    String current_exception;\n-\n-    auto on_error = [&](const MutableColumns & result_columns, Exception & e)\n+    auto add_entry_to_log = [&](\n+        const auto & entry, const auto & entry_query_for_logging,\n+        const auto & exception, size_t num_rows, size_t num_bytes)\n     {\n-        current_exception = e.displayText();\n-        LOG_ERROR(log, \"Failed parsing for query '{}' with query id {}. {}\",\n-            key.query_str, current_entry->query_id, current_exception);\n-\n-        for (const auto & column : result_columns)\n-            if (column->size() > total_rows)\n-                column->popBack(column->size() - total_rows);\n-\n-        current_entry->finish(std::current_exception());\n-        return 0;\n-    };\n-\n-    std::shared_ptr<ISimpleTransform> adding_defaults_transform;\n-    if (insert_context->getSettingsRef().input_format_defaults_for_omitted_fields && insert_query.table_id)\n-    {\n-        StoragePtr storage = DatabaseCatalog::instance().getTable(insert_query.table_id, insert_context);\n-        auto metadata_snapshot = storage->getInMemoryMetadataPtr();\n-        const auto & columns = metadata_snapshot->getColumns();\n-        if (columns.hasDefaults())\n-            adding_defaults_transform = std::make_shared<AddingDefaultsTransform>(header, columns, *format, insert_context);\n-    }\n-\n-    auto insert_log = global_context->getAsynchronousInsertLog();\n-    std::vector<AsynchronousInsertLogElement> log_elements;\n-\n-    if (insert_log)\n-        log_elements.reserve(data->entries.size());\n-\n-    StreamingFormatExecutor executor(header, format, std::move(on_error), std::move(adding_defaults_transform));\n-    std::unique_ptr<ReadBuffer> last_buffer;\n-    auto chunk_info = std::make_shared<AsyncInsertInfo>();\n-    for (const auto & entry : data->entries)\n-    {\n-        auto buffer = std::make_unique<ReadBufferFromString>(entry->bytes);\n-        current_entry = entry;\n-        auto bytes_size = entry->bytes.size();\n-        size_t num_rows = executor.execute(*buffer);\n-        total_rows += num_rows;\n-        chunk_info->offsets.push_back(total_rows);\n-        chunk_info->tokens.push_back(entry->async_dedup_token);\n-\n-        /// Keep buffer, because it still can be used\n-        /// in destructor, while resetting buffer at next iteration.\n-        last_buffer = std::move(buffer);\n-\n-        if (insert_log)\n+        if (!async_insert_log)\n+            return;\n+\n+        AsynchronousInsertLogElement elem;\n+        elem.event_time = timeInSeconds(entry->create_time);\n+        elem.event_time_microseconds = timeInMicroseconds(entry->create_time);\n+        elem.query_for_logging = entry_query_for_logging;\n+        elem.database = query_database;\n+        elem.table = query_table;\n+        elem.format = entry->format;\n+        elem.query_id = entry->query_id;\n+        elem.bytes = num_bytes;\n+        elem.rows = num_rows;\n+        elem.exception = exception;\n+        elem.data_kind = entry->chunk.getDataKind();\n+\n+        /// If there was a parsing error,\n+        /// the entry won't be flushed anyway,\n+        /// so add the log element immediately.\n+        if (!elem.exception.empty())\n         {\n-            AsynchronousInsertLogElement elem;\n-            elem.event_time = timeInSeconds(entry->create_time);\n-            elem.event_time_microseconds = timeInMicroseconds(entry->create_time);\n-            elem.query_for_logging = query_for_logging;\n-            elem.database = query_database;\n-            elem.table = query_table;\n-            elem.format = insert_query.format;\n-            elem.query_id = entry->query_id;\n-            elem.bytes = bytes_size;\n-            elem.rows = num_rows;\n-            elem.exception = current_exception;\n-            current_exception.clear();\n-\n-            /// If there was a parsing error,\n-            /// the entry won't be flushed anyway,\n-            /// so add the log element immediately.\n-            if (!elem.exception.empty())\n-            {\n-                elem.status = AsynchronousInsertLogElement::ParsingError;\n-                insert_log->add(std::move(elem));\n-            }\n-            else\n-            {\n-                log_elements.push_back(elem);\n-            }\n+            elem.status = AsynchronousInsertLogElement::ParsingError;\n+            async_insert_log->add(std::move(elem));\n         }\n-    }\n-\n-    format->addBuffer(std::move(last_buffer));\n-    ProfileEvents::increment(ProfileEvents::AsyncInsertRows, total_rows);\n+        else\n+        {\n+            log_elements.push_back(elem);\n+        }\n+    };\n \n     auto finish_entries = [&]\n     {\n@@ -648,11 +657,21 @@ try\n         if (!log_elements.empty())\n         {\n             auto flush_time = std::chrono::system_clock::now();\n-            appendElementsToLogSafe(*insert_log, std::move(log_elements), flush_time, insert_query_id, \"\");\n+            appendElementsToLogSafe(*async_insert_log, std::move(log_elements), flush_time, insert_query_id, \"\");\n         }\n     };\n \n-    if (total_rows == 0)\n+    Chunk chunk;\n+    auto header = pipeline.getHeader();\n+\n+    if (key.data_kind == DataKind::Parsed)\n+        chunk = processEntriesWithParsing(key, data->entries, header, insert_context, log, add_entry_to_log);\n+    else\n+        chunk = processPreprocessedEntries(key, data->entries, header, insert_context, add_entry_to_log);\n+\n+    ProfileEvents::increment(ProfileEvents::AsyncInsertRows, chunk.getNumRows());\n+\n+    if (chunk.getNumRows() == 0)\n     {\n         finish_entries();\n         return;\n@@ -660,9 +679,8 @@ try\n \n     try\n     {\n-        auto chunk = Chunk(executor.getResultColumns(), total_rows);\n-        chunk.setChunkInfo(std::move(chunk_info));\n-        size_t total_bytes = chunk.bytes();\n+        size_t num_rows = chunk.getNumRows();\n+        size_t num_bytes = chunk.bytes();\n \n         auto source = std::make_shared<SourceFromSingleChunk>(header, std::move(chunk));\n         pipeline.complete(Pipe(std::move(source)));\n@@ -670,8 +688,7 @@ try\n         CompletedPipelineExecutor completed_executor(pipeline);\n         completed_executor.execute();\n \n-        LOG_INFO(log, \"Flushed {} rows, {} bytes for query '{}'\",\n-            total_rows, total_bytes, key.query_str);\n+        LOG_INFO(log, \"Flushed {} rows, {} bytes for query '{}'\", num_rows, num_bytes, key.query_str);\n \n         bool pulling_pipeline = false;\n         logQueryFinish(query_log_elem, insert_context, key.query, pipeline, pulling_pipeline, query_span, QueryCache::Usage::None, internal);\n@@ -684,7 +701,7 @@ try\n         {\n             auto exception = getCurrentExceptionMessage(false);\n             auto flush_time = std::chrono::system_clock::now();\n-            appendElementsToLogSafe(*insert_log, std::move(log_elements), flush_time, insert_query_id, exception);\n+            appendElementsToLogSafe(*async_insert_log, std::move(log_elements), flush_time, insert_query_id, exception);\n         }\n         throw;\n     }\n@@ -708,6 +725,133 @@ catch (...)\n     finishWithException(key.query, data->entries, Exception(ErrorCodes::UNKNOWN_EXCEPTION, \"Unknown exception\"));\n }\n \n+template <typename LogFunc>\n+Chunk AsynchronousInsertQueue::processEntriesWithParsing(\n+    const InsertQuery & key,\n+    const std::list<InsertData::EntryPtr> & entries,\n+    const Block & header,\n+    const ContextPtr & insert_context,\n+    const Poco::Logger * logger,\n+    LogFunc && add_to_async_insert_log)\n+{\n+    size_t total_rows = 0;\n+    InsertData::EntryPtr current_entry;\n+    String current_exception;\n+\n+    const auto & insert_query = assert_cast<const ASTInsertQuery &>(*key.query);\n+    auto format = getInputFormatFromASTInsertQuery(key.query, false, header, insert_context, nullptr);\n+    std::shared_ptr<ISimpleTransform> adding_defaults_transform;\n+\n+    if (insert_context->getSettingsRef().input_format_defaults_for_omitted_fields && insert_query.table_id)\n+    {\n+        StoragePtr storage = DatabaseCatalog::instance().getTable(insert_query.table_id, insert_context);\n+        auto metadata_snapshot = storage->getInMemoryMetadataPtr();\n+        const auto & columns = metadata_snapshot->getColumns();\n+        if (columns.hasDefaults())\n+            adding_defaults_transform = std::make_shared<AddingDefaultsTransform>(header, columns, *format, insert_context);\n+    }\n+\n+    auto on_error = [&](const MutableColumns & result_columns, Exception & e)\n+    {\n+        current_exception = e.displayText();\n+        LOG_ERROR(logger, \"Failed parsing for query '{}' with query id {}. {}\",\n+            key.query_str, current_entry->query_id, current_exception);\n+\n+        for (const auto & column : result_columns)\n+            if (column->size() > total_rows)\n+                column->popBack(column->size() - total_rows);\n+\n+        current_entry->finish(std::current_exception());\n+        return 0;\n+    };\n+\n+    StreamingFormatExecutor executor(header, format, std::move(on_error), std::move(adding_defaults_transform));\n+    std::unique_ptr<ReadBuffer> last_buffer;\n+    auto chunk_info = std::make_shared<AsyncInsertInfo>();\n+    auto query_for_logging = serializeQuery(*key.query, insert_context->getSettingsRef().log_queries_cut_to_length);\n+\n+    for (const auto & entry : entries)\n+    {\n+        current_entry = entry;\n+\n+        const auto * bytes = entry->chunk.asString();\n+        if (!bytes)\n+            throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                \"Expected entry with data kind Parsed. Got: {}\", entry->chunk.getDataKind());\n+\n+        auto buffer = std::make_unique<ReadBufferFromString>(*bytes);\n+        size_t num_bytes = bytes->size();\n+        size_t num_rows = executor.execute(*buffer);\n+\n+        /// Keep buffer, because it still can be used\n+        /// in destructor, while resetting buffer at next iteration.\n+        last_buffer = std::move(buffer);\n+\n+        total_rows += num_rows;\n+        chunk_info->offsets.push_back(total_rows);\n+        chunk_info->tokens.push_back(entry->async_dedup_token);\n+\n+        add_to_async_insert_log(entry, query_for_logging, current_exception, num_rows, num_bytes);\n+        current_exception.clear();\n+    }\n+\n+    format->addBuffer(std::move(last_buffer));\n+\n+    Chunk chunk(executor.getResultColumns(), total_rows);\n+    chunk.setChunkInfo(std::move(chunk_info));\n+    return chunk;\n+}\n+\n+template <typename LogFunc>\n+Chunk AsynchronousInsertQueue::processPreprocessedEntries(\n+    const InsertQuery & key,\n+    const std::list<InsertData::EntryPtr> & entries,\n+    const Block & header,\n+    const ContextPtr & insert_context,\n+    LogFunc && add_to_async_insert_log)\n+{\n+    size_t total_rows = 0;\n+    auto chunk_info = std::make_shared<AsyncInsertInfo>();\n+    auto result_columns = header.cloneEmptyColumns();\n+\n+    std::unordered_map<String, String> format_to_query;\n+\n+    auto get_query_by_format = [&](const String & format) -> const String &\n+    {\n+        auto [it, inserted] = format_to_query.try_emplace(format);\n+        if (!inserted)\n+            return it->second;\n+\n+        auto query = key.query->clone();\n+        assert_cast<ASTInsertQuery &>(*query).format = format;\n+        it->second = serializeQuery(*query, insert_context->getSettingsRef().log_queries_cut_to_length);\n+        return it->second;\n+    };\n+\n+    for (const auto & entry : entries)\n+    {\n+        const auto * block = entry->chunk.asBlock();\n+        if (!block)\n+            throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                \"Expected entry with data kind Preprocessed. Got: {}\", entry->chunk.getDataKind());\n+\n+        auto columns = block->getColumns();\n+        for (size_t i = 0, s = columns.size(); i < s; ++i)\n+            result_columns[i]->insertRangeFrom(*columns[i], 0, columns[i]->size());\n+\n+        total_rows += block->rows();\n+        chunk_info->offsets.push_back(total_rows);\n+        chunk_info->tokens.push_back(entry->async_dedup_token);\n+\n+        const auto & query_for_logging = get_query_by_format(entry->format);\n+        add_to_async_insert_log(entry, query_for_logging, \"\", block->rows(), block->bytes());\n+    }\n+\n+    Chunk chunk(std::move(result_columns), total_rows);\n+    chunk.setChunkInfo(std::move(chunk_info));\n+    return chunk;\n+}\n+\n template <typename E>\n void AsynchronousInsertQueue::finishWithException(\n     const ASTPtr & query, const std::list<InsertData::EntryPtr> & entries, const E & exception)\ndiff --git a/src/Interpreters/AsynchronousInsertQueue.h b/src/Interpreters/AsynchronousInsertQueue.h\nindex 2b92e336d098..99394d0fb14d 100644\n--- a/src/Interpreters/AsynchronousInsertQueue.h\n+++ b/src/Interpreters/AsynchronousInsertQueue.h\n@@ -1,13 +1,16 @@\n #pragma once\n \n+#include <Core/Block.h>\n #include <Core/Settings.h>\n #include <Parsers/IAST_fwd.h>\n #include <Poco/Logger.h>\n #include <Common/CurrentThread.h>\n #include <Common/MemoryTrackerSwitcher.h>\n #include <Common/ThreadPool.h>\n+#include <Processors/Chunk.h>\n \n #include <future>\n+#include <variant>\n \n namespace DB\n {\n@@ -38,11 +41,23 @@ class AsynchronousInsertQueue : public WithContext\n         /// Read buffer that contains extracted\n         /// from query data in case of too much data.\n         std::unique_ptr<ReadBuffer> insert_data_buffer;\n+\n+        /// Block that contains received by Native\n+        /// protocol data in case of too much data.\n+        Block insert_block;\n+    };\n+\n+    enum class DataKind\n+    {\n+        Parsed = 0,\n+        Preprocessed = 1,\n     };\n \n     /// Force flush the whole queue.\n     void flushAll();\n-    PushResult push(ASTPtr query, ContextPtr query_context);\n+\n+    PushResult pushQueryWithInlinedData(ASTPtr query, ContextPtr query_context);\n+    PushResult pushQueryWithBlock(ASTPtr query, Block block, ContextPtr query_context);\n     size_t getPoolSize() const { return pool_size; }\n \n private:\n@@ -52,18 +67,55 @@ class AsynchronousInsertQueue : public WithContext\n     public:\n         ASTPtr query;\n         String query_str;\n-        Settings settings;\n         std::optional<UUID> user_id;\n         std::vector<UUID> current_roles;\n+        Settings settings;\n+\n+        DataKind data_kind;\n         UInt128 hash;\n \n-        InsertQuery(const ASTPtr & query_, const Settings & settings_, const std::optional<UUID> & user_id_, const std::vector<UUID> & current_roles_);\n-        InsertQuery(const InsertQuery & other);\n+        InsertQuery(\n+            const ASTPtr & query_,\n+            const std::optional<UUID> & user_id_,\n+            const std::vector<UUID> & current_roles_,\n+            const Settings & settings_,\n+            DataKind data_kind_);\n+\n+        InsertQuery(const InsertQuery & other) { *this = other; }\n         InsertQuery & operator=(const InsertQuery & other);\n         bool operator==(const InsertQuery & other) const;\n \n     private:\n-        UInt128 calculateHash() const;\n+        auto toTupleCmp() const { return std::tie(data_kind, query_str, user_id, current_roles, setting_changes); }\n+\n+        std::vector<SettingChange> setting_changes;\n+    };\n+\n+    struct DataChunk : public std::variant<String, Block>\n+    {\n+        using std::variant<String, Block>::variant;\n+\n+        size_t byteSize() const\n+        {\n+            return std::visit([]<typename T>(const T & arg)\n+            {\n+                if constexpr (std::is_same_v<T, Block>)\n+                    return arg.bytes();\n+                else\n+                    return arg.size();\n+            }, *this);\n+        }\n+\n+        DataKind getDataKind() const\n+        {\n+            if (std::holds_alternative<Block>(*this))\n+                return DataKind::Preprocessed;\n+            else\n+                return DataKind::Parsed;\n+        }\n+\n+        const String * asString() const { return std::get_if<String>(this); }\n+        const Block * asBlock() const { return std::get_if<Block>(this); }\n     };\n \n     struct InsertData\n@@ -71,13 +123,19 @@ class AsynchronousInsertQueue : public WithContext\n         struct Entry\n         {\n         public:\n-            String bytes;\n+            DataChunk chunk;\n             const String query_id;\n             const String async_dedup_token;\n+            const String format;\n             MemoryTracker * const user_memory_tracker;\n             const std::chrono::time_point<std::chrono::system_clock> create_time;\n \n-            Entry(String && bytes_, String && query_id_, const String & async_dedup_token, MemoryTracker * user_memory_tracker_);\n+            Entry(\n+                DataChunk && chunk_,\n+                String && query_id_,\n+                const String & async_dedup_token_,\n+                const String & format_,\n+                MemoryTracker * user_memory_tracker_);\n \n             void finish(std::exception_ptr exception_ = nullptr);\n             std::future<void> getFuture() { return promise.get_future(); }\n@@ -158,11 +216,31 @@ class AsynchronousInsertQueue : public WithContext\n \n     Poco::Logger * log = &Poco::Logger::get(\"AsynchronousInsertQueue\");\n \n+    PushResult pushDataChunk(ASTPtr query, DataChunk chunk, ContextPtr query_context);\n+    void preprocessInsertQuery(const ASTPtr & query, const ContextPtr & query_context);\n+\n     void processBatchDeadlines(size_t shard_num);\n     void scheduleDataProcessingJob(const InsertQuery & key, InsertDataPtr data, ContextPtr global_context);\n \n     static void processData(InsertQuery key, InsertDataPtr data, ContextPtr global_context);\n \n+    template <typename LogFunc>\n+    static Chunk processEntriesWithParsing(\n+        const InsertQuery & key,\n+        const std::list<InsertData::EntryPtr> & entries,\n+        const Block & header,\n+        const ContextPtr & insert_context,\n+        const Poco::Logger * logger,\n+        LogFunc && add_to_async_insert_log);\n+\n+    template <typename LogFunc>\n+    static Chunk processPreprocessedEntries(\n+        const InsertQuery & key,\n+        const std::list<InsertData::EntryPtr> & entries,\n+        const Block & header,\n+        const ContextPtr & insert_context,\n+        LogFunc && add_to_async_insert_log);\n+\n     template <typename E>\n     static void finishWithException(const ASTPtr & query, const std::list<InsertData::EntryPtr> & entries, const E & exception);\n \ndiff --git a/src/Interpreters/InterpreterKillQueryQuery.cpp b/src/Interpreters/InterpreterKillQueryQuery.cpp\nindex 590b7fe37b8f..1c2e3ff67772 100644\n--- a/src/Interpreters/InterpreterKillQueryQuery.cpp\n+++ b/src/Interpreters/InterpreterKillQueryQuery.cpp\n@@ -420,7 +420,7 @@ Block InterpreterKillQueryQuery::getSelectResult(const String & columns, const S\n     if (where_expression)\n         select_query += \" WHERE \" + queryToString(where_expression);\n \n-    auto io = executeQuery(select_query, getContext(), true);\n+    auto io = executeQuery(select_query, getContext(), true).second;\n     PullingPipelineExecutor executor(io.pipeline);\n     Block res;\n     while (!res && executor.pull(res));\ndiff --git a/src/Interpreters/InterpreterShowColumnsQuery.cpp b/src/Interpreters/InterpreterShowColumnsQuery.cpp\nindex e1f736ba4fb3..025499affda0 100644\n--- a/src/Interpreters/InterpreterShowColumnsQuery.cpp\n+++ b/src/Interpreters/InterpreterShowColumnsQuery.cpp\n@@ -94,7 +94,7 @@ WHERE\n \n BlockIO InterpreterShowColumnsQuery::execute()\n {\n-    return executeQuery(getRewrittenQuery(), getContext(), true);\n+    return executeQuery(getRewrittenQuery(), getContext(), true).second;\n }\n \n \ndiff --git a/src/Interpreters/InterpreterShowEngineQuery.cpp b/src/Interpreters/InterpreterShowEngineQuery.cpp\nindex 8fd829f39ec9..a2367e9bfdf4 100644\n--- a/src/Interpreters/InterpreterShowEngineQuery.cpp\n+++ b/src/Interpreters/InterpreterShowEngineQuery.cpp\n@@ -12,7 +12,7 @@ namespace DB\n \n BlockIO InterpreterShowEnginesQuery::execute()\n {\n-    return executeQuery(\"SELECT * FROM system.table_engines ORDER BY name\", getContext(), true);\n+    return executeQuery(\"SELECT * FROM system.table_engines ORDER BY name\", getContext(), true).second;\n }\n \n }\ndiff --git a/src/Interpreters/InterpreterShowFunctionsQuery.cpp b/src/Interpreters/InterpreterShowFunctionsQuery.cpp\nindex efadb929451f..ace22ca4bb60 100644\n--- a/src/Interpreters/InterpreterShowFunctionsQuery.cpp\n+++ b/src/Interpreters/InterpreterShowFunctionsQuery.cpp\n@@ -15,7 +15,7 @@ InterpreterShowFunctionsQuery::InterpreterShowFunctionsQuery(const ASTPtr & quer\n \n BlockIO InterpreterShowFunctionsQuery::execute()\n {\n-    return executeQuery(getRewrittenQuery(), getContext(), true);\n+    return executeQuery(getRewrittenQuery(), getContext(), true).second;\n }\n \n String InterpreterShowFunctionsQuery::getRewrittenQuery()\ndiff --git a/src/Interpreters/InterpreterShowIndexesQuery.cpp b/src/Interpreters/InterpreterShowIndexesQuery.cpp\nindex 149420006fb7..9b36f1496e70 100644\n--- a/src/Interpreters/InterpreterShowIndexesQuery.cpp\n+++ b/src/Interpreters/InterpreterShowIndexesQuery.cpp\n@@ -107,7 +107,7 @@ ORDER BY index_type, expression, column_name, seq_in_index;)\", database, table,\n \n BlockIO InterpreterShowIndexesQuery::execute()\n {\n-    return executeQuery(getRewrittenQuery(), getContext(), true);\n+    return executeQuery(getRewrittenQuery(), getContext(), true).second;\n }\n \n \ndiff --git a/src/Interpreters/InterpreterShowProcesslistQuery.cpp b/src/Interpreters/InterpreterShowProcesslistQuery.cpp\nindex f9241368a8f7..4ed5f4171c69 100644\n--- a/src/Interpreters/InterpreterShowProcesslistQuery.cpp\n+++ b/src/Interpreters/InterpreterShowProcesslistQuery.cpp\n@@ -12,7 +12,7 @@ namespace DB\n \n BlockIO InterpreterShowProcesslistQuery::execute()\n {\n-    return executeQuery(\"SELECT * FROM system.processes ORDER BY elapsed DESC\", getContext(), true);\n+    return executeQuery(\"SELECT * FROM system.processes ORDER BY elapsed DESC\", getContext(), true).second;\n }\n \n }\ndiff --git a/src/Interpreters/InterpreterShowTablesQuery.cpp b/src/Interpreters/InterpreterShowTablesQuery.cpp\nindex 5fe0a862e057..97bd8e7a8b70 100644\n--- a/src/Interpreters/InterpreterShowTablesQuery.cpp\n+++ b/src/Interpreters/InterpreterShowTablesQuery.cpp\n@@ -188,7 +188,7 @@ BlockIO InterpreterShowTablesQuery::execute()\n         return res;\n     }\n \n-    return executeQuery(getRewrittenQuery(), getContext(), true);\n+    return executeQuery(getRewrittenQuery(), getContext(), true).second;\n }\n \n /// (*) Sorting is strictly speaking not necessary but 1. it is convenient for users, 2. SQL currently does not allow to\ndiff --git a/src/Interpreters/executeQuery.cpp b/src/Interpreters/executeQuery.cpp\nindex eb9c7e344a66..c41cffa3ceb9 100644\n--- a/src/Interpreters/executeQuery.cpp\n+++ b/src/Interpreters/executeQuery.cpp\n@@ -925,12 +925,10 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n                 reason = \"asynchronous insert queue is not configured\";\n             else if (insert_query->select)\n                 reason = \"insert query has select\";\n-            else if (!insert_query->hasInlinedData())\n-                reason = \"insert query doesn't have inlined data\";\n-            else\n+            else if (insert_query->hasInlinedData())\n                 async_insert = true;\n \n-            if (!async_insert)\n+            if (!reason.empty())\n                 LOG_DEBUG(logger, \"Setting async_insert=1, but INSERT query will be executed synchronously (reason: {})\", reason);\n         }\n \n@@ -953,7 +951,7 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n                 quota->checkExceeded(QuotaType::ERRORS);\n             }\n \n-            auto result = queue->push(ast, context);\n+            auto result = queue->pushQueryWithInlinedData(ast, context);\n \n             if (result.status == AsynchronousInsertQueue::PushResult::OK)\n             {\n@@ -1223,19 +1221,20 @@ static std::tuple<ASTPtr, BlockIO> executeQueryImpl(\n         throw;\n     }\n \n-    return std::make_tuple(ast, std::move(res));\n+    return std::make_tuple(std::move(ast), std::move(res));\n }\n \n \n-BlockIO executeQuery(\n+std::pair<ASTPtr, BlockIO> executeQuery(\n     const String & query,\n     ContextMutablePtr context,\n     bool internal,\n     QueryProcessingStage::Enum stage)\n {\n     ASTPtr ast;\n-    BlockIO streams;\n-    std::tie(ast, streams) = executeQueryImpl(query.data(), query.data() + query.size(), context, internal, stage, nullptr);\n+    BlockIO res;\n+\n+    std::tie(ast, res) = executeQueryImpl(query.data(), query.data() + query.size(), context, internal, stage, nullptr);\n \n     if (const auto * ast_query_with_output = dynamic_cast<const ASTQueryWithOutput *>(ast.get()))\n     {\n@@ -1244,26 +1243,12 @@ BlockIO executeQuery(\n                 : context->getDefaultFormat();\n \n         if (format_name == \"Null\")\n-            streams.null_format = true;\n+            res.null_format = true;\n     }\n \n-    return streams;\n+    return std::make_pair(std::move(ast), std::move(res));\n }\n \n-BlockIO executeQuery(\n-    bool allow_processors,\n-    const String & query,\n-    ContextMutablePtr context,\n-    bool internal,\n-    QueryProcessingStage::Enum stage)\n-{\n-    if (!allow_processors)\n-        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Flag allow_processors is deprecated for executeQuery\");\n-\n-    return executeQuery(query, context, internal, stage);\n-}\n-\n-\n void executeQuery(\n     ReadBuffer & istr,\n     WriteBuffer & ostr,\ndiff --git a/src/Interpreters/executeQuery.h b/src/Interpreters/executeQuery.h\nindex a31d4f2f08cf..6f14f54d7d6d 100644\n--- a/src/Interpreters/executeQuery.h\n+++ b/src/Interpreters/executeQuery.h\n@@ -43,7 +43,7 @@ void executeQuery(\n \n /// More low-level function for server-to-server interaction.\n /// Prepares a query for execution but doesn't execute it.\n-/// Returns a pair of block streams which, when used, will result in query execution.\n+/// Returns a pair of parsed query and BlockIO which, when used, will result in query execution.\n /// This means that the caller can to the extent control the query execution pipeline.\n ///\n /// To execute:\n@@ -55,22 +55,13 @@ void executeQuery(\n ///\n /// Correctly formatting the results (according to INTO OUTFILE and FORMAT sections)\n /// must be done separately.\n-BlockIO executeQuery(\n+std::pair<ASTPtr, BlockIO> executeQuery(\n     const String & query,     /// Query text without INSERT data. The latter must be written to BlockIO::out.\n     ContextMutablePtr context,       /// DB, tables, data types, storage engines, functions, aggregate functions...\n     bool internal = false,    /// If true, this query is caused by another query and thus needn't be registered in the ProcessList.\n     QueryProcessingStage::Enum stage = QueryProcessingStage::Complete    /// To which stage the query must be executed.\n );\n \n-/// Old interface with allow_processors flag. For compatibility.\n-BlockIO executeQuery(\n-    bool allow_processors,  /// If can use processors pipeline\n-    const String & query,\n-    ContextMutablePtr context,\n-    bool internal = false,\n-    QueryProcessingStage::Enum stage = QueryProcessingStage::Complete\n-);\n-\n /// Executes BlockIO returned from executeQuery(...)\n /// if built pipeline does not require any input and does not produce any output.\n void executeTrivialBlockIO(BlockIO & streams, ContextPtr context);\ndiff --git a/src/Interpreters/fuzzers/execute_query_fuzzer.cpp b/src/Interpreters/fuzzers/execute_query_fuzzer.cpp\nindex f12c01120cfe..0f6bfc1ae584 100644\n--- a/src/Interpreters/fuzzers/execute_query_fuzzer.cpp\n+++ b/src/Interpreters/fuzzers/execute_query_fuzzer.cpp\n@@ -42,7 +42,7 @@ extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t * data, size_t size)\n         static bool initialized = initialize();\n         (void) initialized;\n \n-        auto io = DB::executeQuery(input, context, true, QueryProcessingStage::Complete);\n+        auto io = DB::executeQuery(input, context, true, QueryProcessingStage::Complete).second;\n \n         PullingPipelineExecutor executor(io.pipeline);\n         Block res;\ndiff --git a/src/Interpreters/loadMetadata.cpp b/src/Interpreters/loadMetadata.cpp\nindex aeb45c08bea9..faa1dcda2c0f 100644\n--- a/src/Interpreters/loadMetadata.cpp\n+++ b/src/Interpreters/loadMetadata.cpp\n@@ -282,7 +282,7 @@ static void convertOrdinaryDatabaseToAtomic(Poco::Logger * log, ContextMutablePt\n     LOG_INFO(log, \"Will convert database {} from Ordinary to Atomic\", name_quoted);\n \n     String create_database_query = fmt::format(\"CREATE DATABASE IF NOT EXISTS {}\", tmp_name_quoted);\n-    auto res = executeQuery(create_database_query, context, true);\n+    auto res = executeQuery(create_database_query, context, true).second;\n     executeTrivialBlockIO(res, context);\n     res = {};\n     auto tmp_database = DatabaseCatalog::instance().getDatabase(tmp_name);\n@@ -322,7 +322,7 @@ static void convertOrdinaryDatabaseToAtomic(Poco::Logger * log, ContextMutablePt\n         String tmp_qualified_quoted_name = id.getFullTableName();\n \n         String move_table_query = fmt::format(\"RENAME TABLE {} TO {}\", qualified_quoted_name, tmp_qualified_quoted_name);\n-        res = executeQuery(move_table_query, context, true);\n+        res = executeQuery(move_table_query, context, true).second;\n         executeTrivialBlockIO(res, context);\n         res = {};\n     }\n@@ -334,12 +334,12 @@ static void convertOrdinaryDatabaseToAtomic(Poco::Logger * log, ContextMutablePt\n \n     String drop_query = fmt::format(\"DROP DATABASE {}\", name_quoted);\n     context->setSetting(\"force_remove_data_recursively_on_drop\", false);\n-    res = executeQuery(drop_query, context, true);\n+    res = executeQuery(drop_query, context, true).second;\n     executeTrivialBlockIO(res, context);\n     res = {};\n \n     String rename_query = fmt::format(\"RENAME DATABASE {} TO {}\", tmp_name_quoted, name_quoted);\n-    res = executeQuery(rename_query, context, true);\n+    res = executeQuery(rename_query, context, true).second;\n     executeTrivialBlockIO(res, context);\n \n     LOG_INFO(log, \"Finished database engine conversion of {}\", name_quoted);\n@@ -409,7 +409,7 @@ static void maybeConvertOrdinaryDatabaseToAtomic(ContextMutablePtr context, cons\n \n         /// Reload database just in case (and update logger name)\n         String detach_query = fmt::format(\"DETACH DATABASE {}\", backQuoteIfNeed(database_name));\n-        auto res = executeQuery(detach_query, context, true);\n+        auto res = executeQuery(detach_query, context, true).second;\n         executeTrivialBlockIO(res, context);\n         res = {};\n \ndiff --git a/src/Processors/Executors/StreamingFormatExecutor.cpp b/src/Processors/Executors/StreamingFormatExecutor.cpp\nindex 468189890320..281961f7c7c4 100644\n--- a/src/Processors/Executors/StreamingFormatExecutor.cpp\n+++ b/src/Processors/Executors/StreamingFormatExecutor.cpp\n@@ -66,21 +66,9 @@ size_t StreamingFormatExecutor::execute()\n                     return new_rows;\n \n                 case IProcessor::Status::PortFull:\n-                {\n-                    auto chunk = port.pull();\n-                    if (adding_defaults_transform)\n-                        adding_defaults_transform->transform(chunk);\n-\n-                    auto chunk_rows = chunk.getNumRows();\n-                    new_rows += chunk_rows;\n-\n-                    auto columns = chunk.detachColumns();\n-\n-                    for (size_t i = 0, s = columns.size(); i < s; ++i)\n-                        result_columns[i]->insertRangeFrom(*columns[i], 0, columns[i]->size());\n-\n+                    new_rows += insertChunk(port.pull());\n                     break;\n-                }\n+\n                 case IProcessor::Status::NeedData:\n                 case IProcessor::Status::Async:\n                 case IProcessor::Status::ExpandPipeline:\n@@ -107,4 +95,17 @@ size_t StreamingFormatExecutor::execute()\n     }\n }\n \n+size_t StreamingFormatExecutor::insertChunk(Chunk chunk)\n+{\n+    size_t chunk_rows = chunk.getNumRows();\n+    if (adding_defaults_transform)\n+        adding_defaults_transform->transform(chunk);\n+\n+    auto columns = chunk.detachColumns();\n+    for (size_t i = 0, s = columns.size(); i < s; ++i)\n+        result_columns[i]->insertRangeFrom(*columns[i], 0, columns[i]->size());\n+\n+    return chunk_rows;\n+}\n+\n }\ndiff --git a/src/Processors/Executors/StreamingFormatExecutor.h b/src/Processors/Executors/StreamingFormatExecutor.h\nindex f5a1562a3408..3aa90ab03602 100644\n--- a/src/Processors/Executors/StreamingFormatExecutor.h\n+++ b/src/Processors/Executors/StreamingFormatExecutor.h\n@@ -33,6 +33,9 @@ class StreamingFormatExecutor\n     /// Execute with provided read buffer.\n     size_t execute(ReadBuffer & buffer);\n \n+    /// Inserts into result columns already preprocessed chunk.\n+    size_t insertChunk(Chunk chunk);\n+\n     /// Releases currently accumulated columns.\n     MutableColumns getResultColumns();\n \ndiff --git a/src/Processors/Transforms/SquashingChunksTransform.h b/src/Processors/Transforms/SquashingChunksTransform.h\nindex df13f539b906..f82e9e46a617 100644\n--- a/src/Processors/Transforms/SquashingChunksTransform.h\n+++ b/src/Processors/Transforms/SquashingChunksTransform.h\n@@ -22,7 +22,6 @@ class SquashingChunksTransform : public ExceptionKeepingTransform\n     GenerateResult onGenerate() override;\n     void onFinish() override;\n \n-\n private:\n     SquashingTransform squashing;\n     Chunk cur_chunk;\ndiff --git a/src/Server/GRPCServer.cpp b/src/Server/GRPCServer.cpp\nindex 4d9f5c983c57..812c2b5489db 100644\n--- a/src/Server/GRPCServer.cpp\n+++ b/src/Server/GRPCServer.cpp\n@@ -949,7 +949,7 @@ namespace\n             query_end = insert_query->data;\n         }\n         String query(begin, query_end);\n-        io = ::DB::executeQuery(true, query, query_context);\n+        io = ::DB::executeQuery(query, query_context).second;\n     }\n \n     void Call::processInput()\ndiff --git a/src/Server/TCPHandler.cpp b/src/Server/TCPHandler.cpp\nindex 8303ee2c9a1b..4908bf82b460 100644\n--- a/src/Server/TCPHandler.cpp\n+++ b/src/Server/TCPHandler.cpp\n@@ -1,3 +1,7 @@\n+#include \"Interpreters/AsynchronousInsertQueue.h\"\n+#include \"Interpreters/Context_fwd.h\"\n+#include \"Interpreters/SquashingTransform.h\"\n+#include \"Parsers/ASTInsertQuery.h\"\n #include <algorithm>\n #include <exception>\n #include <iterator>\n@@ -99,6 +103,7 @@ namespace DB::ErrorCodes\n     extern const int AUTHENTICATION_FAILED;\n     extern const int QUERY_WAS_CANCELLED;\n     extern const int CLIENT_INFO_DOES_NOT_MATCH;\n+    extern const int TIMEOUT_EXCEEDED;\n     extern const int SUPPORT_IS_DISABLED;\n     extern const int UNSUPPORTED_METHOD;\n     extern const int FUNCTION_NOT_ALLOWED;\n@@ -497,7 +502,7 @@ void TCPHandler::runImpl()\n             });\n \n             /// Processing Query\n-            state.io = executeQuery(state.query, query_context, false, state.stage);\n+            std::tie(state.parsed_query, state.io) = executeQuery(state.query, query_context, false, state.stage);\n \n             after_check_cancelled.restart();\n             after_send_progress.restart();\n@@ -810,35 +815,66 @@ void TCPHandler::skipData()\n         throw Exception(ErrorCodes::QUERY_WAS_CANCELLED, \"Query was cancelled\");\n }\n \n+void TCPHandler::startInsertQuery()\n+{\n+    /// Send ColumnsDescription for insertion table\n+    if (client_tcp_protocol_version >= DBMS_MIN_REVISION_WITH_COLUMN_DEFAULTS_METADATA)\n+    {\n+        const auto & table_id = query_context->getInsertionTable();\n+        if (query_context->getSettingsRef().input_format_defaults_for_omitted_fields)\n+        {\n+            if (!table_id.empty())\n+            {\n+                auto storage_ptr = DatabaseCatalog::instance().getTable(table_id, query_context);\n+                sendTableColumns(storage_ptr->getInMemoryMetadataPtr()->getColumns());\n+            }\n+        }\n+    }\n+\n+    /// Send block to the client - table structure.\n+    sendData(state.io.pipeline.getHeader());\n+    sendLogs();\n+}\n+\n+AsynchronousInsertQueue::PushResult TCPHandler::processAsyncInsertQuery(AsynchronousInsertQueue & insert_queue)\n+{\n+    using PushResult = AsynchronousInsertQueue::PushResult;\n+\n+    startInsertQuery();\n+    SquashingTransform squashing(0, query_context->getSettingsRef().async_insert_max_data_size);\n+\n+    while (readDataNext())\n+    {\n+        auto result = squashing.add(std::move(state.block_for_insert));\n+        if (result)\n+        {\n+            return PushResult\n+            {\n+                .status = PushResult::TOO_MUCH_DATA,\n+                .insert_block = std::move(result),\n+            };\n+        }\n+    }\n+\n+    auto result = squashing.add({});\n+    return insert_queue.pushQueryWithBlock(state.parsed_query, std::move(result), query_context);\n+}\n \n void TCPHandler::processInsertQuery()\n {\n     size_t num_threads = state.io.pipeline.getNumThreads();\n \n-    auto run_executor = [&](auto & executor)\n+    auto run_executor = [&](auto & executor, Block processed_data)\n     {\n         /// Made above the rest of the lines,\n-        /// so that in case of `writePrefix` function throws an exception,\n+        /// so that in case of `start` function throws an exception,\n         /// client receive exception before sending data.\n         executor.start();\n \n-        /// Send ColumnsDescription for insertion table\n-        if (client_tcp_protocol_version >= DBMS_MIN_REVISION_WITH_COLUMN_DEFAULTS_METADATA)\n-        {\n-            const auto & table_id = query_context->getInsertionTable();\n-            if (query_context->getSettingsRef().input_format_defaults_for_omitted_fields)\n-            {\n-                if (!table_id.empty())\n-                {\n-                    auto storage_ptr = DatabaseCatalog::instance().getTable(table_id, query_context);\n-                    sendTableColumns(storage_ptr->getInMemoryMetadataPtr()->getColumns());\n-                }\n-            }\n-        }\n-\n-        /// Send block to the client - table structure.\n-        sendData(executor.getHeader());\n-        sendLogs();\n+        if (processed_data)\n+            executor.push(std::move(processed_data));\n+        else\n+            startInsertQuery();\n \n         while (readDataNext())\n             executor.push(std::move(state.block_for_insert));\n@@ -849,15 +885,55 @@ void TCPHandler::processInsertQuery()\n             executor.finish();\n     };\n \n+    Block processed_block;\n+    const auto & settings = query_context->getSettingsRef();\n+\n+    auto * insert_queue = query_context->getAsynchronousInsertQueue();\n+    const auto & insert_query = assert_cast<const ASTInsertQuery &>(*state.parsed_query);\n+\n+    bool async_insert_enabled = settings.async_insert;\n+    if (insert_query.table_id)\n+        if (auto table = DatabaseCatalog::instance().tryGetTable(insert_query.table_id, query_context))\n+            async_insert_enabled |= table->areAsynchronousInsertsEnabled();\n+\n+    if (insert_queue && async_insert_enabled && !insert_query.select)\n+    {\n+        auto result = processAsyncInsertQuery(*insert_queue);\n+        if (result.status == AsynchronousInsertQueue::PushResult::OK)\n+        {\n+            if (settings.wait_for_async_insert)\n+            {\n+                size_t timeout_ms = settings.wait_for_async_insert_timeout.totalMilliseconds();\n+                auto wait_status = result.future.wait_for(std::chrono::milliseconds(timeout_ms));\n+\n+                if (wait_status == std::future_status::deferred)\n+                    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Logical error: got future in deferred state\");\n+\n+                if (wait_status == std::future_status::timeout)\n+                    throw Exception(ErrorCodes::TIMEOUT_EXCEEDED, \"Wait for async insert timeout ({} ms) exceeded)\", timeout_ms);\n+\n+                result.future.get();\n+            }\n+\n+            sendInsertProfileEvents();\n+            return;\n+        }\n+        else if (result.status == AsynchronousInsertQueue::PushResult::TOO_MUCH_DATA)\n+        {\n+            LOG_DEBUG(log, \"Setting async_insert=1, but INSERT query will be executed synchronously because it has too much data\");\n+            processed_block = std::move(result.insert_block);\n+        }\n+    }\n+\n     if (num_threads > 1)\n     {\n         PushingAsyncPipelineExecutor executor(state.io.pipeline);\n-        run_executor(executor);\n+        run_executor(executor, std::move(processed_block));\n     }\n     else\n     {\n         PushingPipelineExecutor executor(state.io.pipeline);\n-        run_executor(executor);\n+        run_executor(executor, processed_block);\n     }\n \n     sendInsertProfileEvents();\ndiff --git a/src/Server/TCPHandler.h b/src/Server/TCPHandler.h\nindex 7ebb605e1c93..9fd243baa6c5 100644\n--- a/src/Server/TCPHandler.h\n+++ b/src/Server/TCPHandler.h\n@@ -21,6 +21,7 @@\n #include <Formats/NativeWriter.h>\n \n #include \"IServer.h\"\n+#include \"Interpreters/AsynchronousInsertQueue.h\"\n #include \"Server/TCPProtocolStackData.h\"\n #include \"Storages/MergeTree/RequestResponse.h\"\n #include \"base/types.h\"\n@@ -73,6 +74,8 @@ struct QueryState\n \n     /// Query text.\n     String query;\n+    /// Parsed query\n+    ASTPtr parsed_query;\n     /// Streams of blocks, that are processing the query.\n     BlockIO io;\n \n@@ -247,7 +250,9 @@ class TCPHandler : public Poco::Net::TCPServerConnection\n     [[noreturn]] void receiveUnexpectedTablesStatusRequest();\n \n     /// Process INSERT query\n+    void startInsertQuery();\n     void processInsertQuery();\n+    AsynchronousInsertQueue::PushResult processAsyncInsertQuery(AsynchronousInsertQueue & insert_queue);\n \n     /// Process a request that does not require the receiving of data blocks from the client\n     void processOrdinaryQuery();\ndiff --git a/src/Storages/System/StorageSystemAsynchronousInserts.cpp b/src/Storages/System/StorageSystemAsynchronousInserts.cpp\nindex 15258ccfd7f1..ec3a9d92f308 100644\n--- a/src/Storages/System/StorageSystemAsynchronousInserts.cpp\n+++ b/src/Storages/System/StorageSystemAsynchronousInserts.cpp\n@@ -82,7 +82,7 @@ void StorageSystemAsynchronousInserts::fillData(MutableColumns & res_columns, Co\n             for (const auto & entry : data->entries)\n             {\n                 arr_query_id.push_back(entry->query_id);\n-                arr_bytes.push_back(entry->bytes.size());\n+                arr_bytes.push_back(entry->chunk.byteSize());\n             }\n \n             res_columns[i++]->insert(arr_query_id);\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02456_async_inserts_logs.sh b/tests/queries/0_stateless/02456_async_inserts_logs.sh\nindex 43cd73d72313..3251c777dfcc 100755\n--- a/tests/queries/0_stateless/02456_async_inserts_logs.sh\n+++ b/tests/queries/0_stateless/02456_async_inserts_logs.sh\n@@ -33,7 +33,12 @@ ${CLICKHOUSE_CLIENT} -q \"\n     SELECT table, format, bytes, rows, empty(exception), status,\n     status = 'ParsingError' ? flush_time_microseconds = 0 : flush_time_microseconds > event_time_microseconds AS time_ok\n     FROM system.asynchronous_insert_log\n-    WHERE database = '$CLICKHOUSE_DATABASE' OR query ILIKE 'INSERT INTO FUNCTION%$CLICKHOUSE_DATABASE%'\n+    WHERE\n+    (\n+        database = '$CLICKHOUSE_DATABASE' AND table = 't_async_inserts_logs'\n+        OR query ILIKE 'INSERT INTO FUNCTION%$CLICKHOUSE_DATABASE%t_async_inserts_logs%'\n+    )\n+    AND data_kind = 'Parsed'\n     ORDER BY table, status, format\"\n \n ${CLICKHOUSE_CLIENT} -q \"DROP TABLE t_async_inserts_logs\"\ndiff --git a/tests/queries/0_stateless/02784_disable_async_with_dedup_correctly.reference b/tests/queries/0_stateless/02784_disable_async_with_dedup_correctly.reference\nindex 014be4ce1a9c..3999dc78a157 100644\n--- a/tests/queries/0_stateless/02784_disable_async_with_dedup_correctly.reference\n+++ b/tests/queries/0_stateless/02784_disable_async_with_dedup_correctly.reference\n@@ -1,6 +1,6 @@\n 0\n 1\n-1\n+0\n 1\n 2\n 3\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_1.reference b/tests/queries/0_stateless/02884_async_insert_native_protocol_1.reference\nnew file mode 100644\nindex 000000000000..44f4e24d7dfc\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_1.reference\n@@ -0,0 +1,7 @@\n+NOT_IMPLEMENTED\n+2\n+1\taaa\n+2\tbbb\n+3\tccc\n+Ok\t2\tPreprocessed\tJSONEachRow\n+Ok\t1\tParsed\tJSONEachRow\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_1.sh b/tests/queries/0_stateless/02884_async_insert_native_protocol_1.sh\nnew file mode 100755\nindex 000000000000..82e2bb709f90\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_1.sh\n@@ -0,0 +1,41 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+set -e\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    DROP TABLE IF EXISTS t_async_insert_native_1;\n+    CREATE TABLE t_async_insert_native_1 (id UInt64, s String) ENGINE = MergeTree ORDER BY id;\n+\"\n+\n+async_insert_options=\"--async_insert 1 --wait_for_async_insert 0 --async_insert_busy_timeout_ms 1000000\"\n+\n+echo '{\"id\": 1, \"s\": \"aaa\"} {\"id\": 2, \"s\": \"bbb\"}' | $CLICKHOUSE_CLIENT $async_insert_options -q 'INSERT INTO t_async_insert_native_1 FORMAT JSONEachRow'\n+$CLICKHOUSE_CLIENT $async_insert_options  -q 'INSERT INTO t_async_insert_native_1 FORMAT JSONEachRow {\"id\": 3, \"s\": \"ccc\"}'\n+\n+# Mixed inlined and external data is not supported.\n+echo '{\"id\": 1, \"s\": \"aaa\"}' \\\n+    | $CLICKHOUSE_CLIENT $async_insert_options -q 'INSERT INTO t_async_insert_native_1 FORMAT JSONEachRow {\"id\": 2, \"s\": \"bbb\"}' 2>&1 \\\n+    | grep -o \"NOT_IMPLEMENTED\"\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    SELECT sum(length(entries.bytes)) FROM system.asynchronous_inserts\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 't_async_insert_native_1';\n+\n+    SYSTEM FLUSH ASYNC INSERT QUEUE;\n+\n+    SELECT * FROM t_async_insert_native_1 ORDER BY id;\n+\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT status, rows, data_kind, format\n+    FROM system.asynchronous_insert_log\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 't_async_insert_native_1'\n+    ORDER BY event_time_microseconds;\n+\n+    DROP TABLE t_async_insert_native_1;\n+\"\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_2.reference b/tests/queries/0_stateless/02884_async_insert_native_protocol_2.reference\nnew file mode 100644\nindex 000000000000..75c015cfab87\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_2.reference\n@@ -0,0 +1,7 @@\n+1\taaa\n+2\tbbb\n+3\tccc\n+4\tddd\n+5\teee\n+JSONEachRow\tOk\t2\tPreprocessed\tJSONEachRow\n+Values\tOk\t3\tPreprocessed\tValues\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_2.sh b/tests/queries/0_stateless/02884_async_insert_native_protocol_2.sh\nnew file mode 100755\nindex 000000000000..b9b1854eaefd\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_2.sh\n@@ -0,0 +1,32 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+set -e\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    DROP TABLE IF EXISTS t_async_insert_native_2;\n+    CREATE TABLE t_async_insert_native_2 (id UInt64, s String) ENGINE = MergeTree ORDER BY id;\n+\"\n+\n+async_insert_options=\"--async_insert 1 --wait_for_async_insert 1\"\n+\n+echo '{\"id\": 1, \"s\": \"aaa\"} {\"id\": 2, \"s\": \"bbb\"}' | $CLICKHOUSE_CLIENT $async_insert_options -q 'INSERT INTO t_async_insert_native_2 FORMAT JSONEachRow' &\n+echo \"(3, 'ccc') (4, 'ddd') (5, 'eee')\" | $CLICKHOUSE_CLIENT $async_insert_options -q 'INSERT INTO t_async_insert_native_2 FORMAT Values' &\n+\n+wait\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    SELECT * FROM t_async_insert_native_2 ORDER BY id;\n+\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT format, status, rows, data_kind, format\n+    FROM system.asynchronous_insert_log\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 't_async_insert_native_2'\n+    ORDER BY format;\n+\n+    DROP TABLE t_async_insert_native_2;\n+\"\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_3.reference b/tests/queries/0_stateless/02884_async_insert_native_protocol_3.reference\nnew file mode 100644\nindex 000000000000..8d5d28463083\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_3.reference\n@@ -0,0 +1,13 @@\n+Native\t3\n+Values\t1\n+1\taaa\n+2\tbbb\n+3\tccc\n+4\tddd\n+5\teee\n+6\tfff\n+7\tggg\n+JSONEachRow\tOk\t2\tPreprocessed\tJSONEachRow\n+Values\tOk\t3\tPreprocessed\tValues\n+CSV\tOk\t1\tPreprocessed\tCSV\n+Values\tOk\t1\tParsed\tValues\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_3.sh b/tests/queries/0_stateless/02884_async_insert_native_protocol_3.sh\nnew file mode 100755\nindex 000000000000..abe6be9e2bc6\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_3.sh\n@@ -0,0 +1,41 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+set -e\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    DROP TABLE IF EXISTS t_async_insert_native_3;\n+    CREATE TABLE t_async_insert_native_3 (id UInt64, s String) ENGINE = MergeTree ORDER BY id;\n+\"\n+\n+async_insert_options=\"--async_insert 1 --wait_for_async_insert 0 --async_insert_busy_timeout_ms 1000000\"\n+\n+echo '{\"id\": 1, \"s\": \"aaa\"} {\"id\": 2, \"s\": \"bbb\"}' | $CLICKHOUSE_CLIENT $async_insert_options -q 'INSERT INTO t_async_insert_native_3 FORMAT JSONEachRow'\n+echo \"(3, 'ccc') (4, 'ddd') (5, 'eee')\" | $CLICKHOUSE_CLIENT $async_insert_options -q 'INSERT INTO t_async_insert_native_3 FORMAT Values'\n+echo '6,\"fff\"' | $CLICKHOUSE_CLIENT $async_insert_options -q 'INSERT INTO t_async_insert_native_3 FORMAT CSV'\n+$CLICKHOUSE_CLIENT $async_insert_options -q \"INSERT INTO t_async_insert_native_3 VALUES (7, 'ggg')\"\n+\n+wait\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    SELECT format, length(entries.bytes) FROM system.asynchronous_inserts\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 't_async_insert_native_3'\n+    ORDER BY format;\n+\n+    SYSTEM FLUSH ASYNC INSERT QUEUE;\n+\n+    SELECT * FROM t_async_insert_native_3 ORDER BY id;\n+\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT format, status, rows, data_kind, format\n+    FROM system.asynchronous_insert_log\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 't_async_insert_native_3'\n+    ORDER BY event_time_microseconds;\n+\n+    DROP TABLE t_async_insert_native_3;\n+\"\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_4.reference b/tests/queries/0_stateless/02884_async_insert_native_protocol_4.reference\nnew file mode 100644\nindex 000000000000..aaabca435478\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_4.reference\n@@ -0,0 +1,8 @@\n+0\n+1\n+1\n+2\n+3\n+4\n+5\n+Values\tOk\t1\tPreprocessed\tValues\ndiff --git a/tests/queries/0_stateless/02884_async_insert_native_protocol_4.sh b/tests/queries/0_stateless/02884_async_insert_native_protocol_4.sh\nnew file mode 100755\nindex 000000000000..9118c11315c2\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_native_protocol_4.sh\n@@ -0,0 +1,34 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    DROP TABLE IF EXISTS t_async_insert_native_4;\n+    CREATE TABLE t_async_insert_native_4 (id UInt64) ENGINE = MergeTree ORDER BY id;\n+\"\n+\n+async_insert_options=\"--async_insert 1 --wait_for_async_insert 1\"\n+CLICKHOUSE_CLIENT_WITH_LOG=$(echo ${CLICKHOUSE_CLIENT} | sed 's/'\"--send_logs_level=${CLICKHOUSE_CLIENT_SERVER_LOGS_LEVEL}\"'/--send_logs_level=trace/g')\n+\n+echo \"(1)\" | $CLICKHOUSE_CLIENT_WITH_LOG $async_insert_options --async_insert_max_data_size 10 \\\n+     -q 'INSERT INTO t_async_insert_native_4 FORMAT Values' 2>&1 \\\n+     | grep -c \"too much data\"\n+\n+echo \"(2) (3) (4) (5)\" | $CLICKHOUSE_CLIENT_WITH_LOG $async_insert_options --async_insert_max_data_size 10 \\\n+    -q 'INSERT INTO t_async_insert_native_4 FORMAT Values' 2>&1 \\\n+    | grep -c \"too much data\"\n+\n+$CLICKHOUSE_CLIENT -n -q \"\n+    SELECT * FROM t_async_insert_native_4 ORDER BY id;\n+\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT format, status, rows, data_kind, format\n+    FROM system.asynchronous_insert_log\n+    WHERE database = '$CLICKHOUSE_DATABASE' AND table = 't_async_insert_native_4'\n+    ORDER BY format;\n+\n+    DROP TABLE t_async_insert_native_4;\n+\"\ndiff --git a/tests/queries/0_stateless/02884_async_insert_skip_settings.reference b/tests/queries/0_stateless/02884_async_insert_skip_settings.reference\nnew file mode 100644\nindex 000000000000..318966ce93b4\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_skip_settings.reference\n@@ -0,0 +1,5 @@\n+4\n+1\n+1\n+2\n+1\ndiff --git a/tests/queries/0_stateless/02884_async_insert_skip_settings.sql b/tests/queries/0_stateless/02884_async_insert_skip_settings.sql\nnew file mode 100644\nindex 000000000000..facd39d10790\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_async_insert_skip_settings.sql\n@@ -0,0 +1,45 @@\n+-- Tags: no-parallel\n+\n+DROP TABLE IF EXISTS t_async_insert_skip_settings SYNC;\n+\n+CREATE TABLE t_async_insert_skip_settings (id UInt64)\n+ENGINE = ReplicatedMergeTree('/clickhouse/{database}/tables/t_async_insert_skip_settings', '1')\n+ORDER BY id;\n+\n+SET async_insert = 1;\n+SET async_insert_deduplicate = 1;\n+SET wait_for_async_insert = 0;\n+SET async_insert_busy_timeout_ms = 100000;\n+\n+SET insert_deduplication_token = '1';\n+SET log_comment = 'async_insert_skip_settings_1';\n+INSERT INTO t_async_insert_skip_settings VALUES (1);\n+\n+SET insert_deduplication_token = '2';\n+SET log_comment = 'async_insert_skip_settings_2';\n+INSERT INTO t_async_insert_skip_settings VALUES (1);\n+\n+SET insert_deduplication_token = '1';\n+SET log_comment = 'async_insert_skip_settings_3';\n+INSERT INTO t_async_insert_skip_settings VALUES (2);\n+\n+SET insert_deduplication_token = '3';\n+SET log_comment = 'async_insert_skip_settings_4';\n+INSERT INTO t_async_insert_skip_settings VALUES (2);\n+\n+SYSTEM FLUSH LOGS;\n+\n+SELECT length(entries.bytes) FROM system.asynchronous_inserts\n+WHERE database = currentDatabase() AND table = 't_async_insert_skip_settings'\n+ORDER BY first_update;\n+\n+SYSTEM FLUSH ASYNC INSERT QUEUE;\n+\n+SELECT * FROM t_async_insert_skip_settings ORDER BY id;\n+\n+SYSTEM FLUSH LOGS;\n+\n+SELECT uniqExact(flush_query_id) FROM system.asynchronous_insert_log\n+WHERE database = currentDatabase() AND table = 't_async_insert_skip_settings';\n+\n+DROP TABLE t_async_insert_skip_settings SYNC;\n",
  "problem_statement": "Support for `async_insert` for prepared blocks sent over the native protocol.\n**Use case**\r\n\r\nMany clients are concurrently sending data using Distributed tables.\r\n\r\n**Describe the solution you'd like**\r\n\r\nDeserialize blocks as usual, put them in the queue for async inserts, and squash them.\r\nThis is slightly different from how async inserts operate, but in fact, even simpler (no parsing).\r\n\nAsync inserts don't work if people are using log_comment setting with different values\n**Describe the unexpected behaviour**\r\nAsync inserts use all settings in order to calculate hash for that insert.\r\nIf people use log_comment setting, it does mean that such inserts will be put in different queues and make async inserts useless.\r\n\r\nhttps://github.com/ClickHouse/ClickHouse/blob/d7879c16e487798e2fd38be4a15c78c046d9a4ff/src/Interpreters/AsynchronousInsertQueue.cpp#L94\r\n\r\n\r\n**Expected behavior**\r\nlog_comment and other similar settings will be excluded from hash calculation\r\n\n",
  "hints_text": "Do we want to support `INSERT SELECT`?\r\n\nI think at a minimum this should be called out as a big potential trap in [the documentation for asynchronous inserts](https://clickhouse.com/docs/en/optimize/asynchronous-inserts). We fell into this trap with our production cluster because we included a unique identifier in the `log_comment` of each insert query that we issued, which functionally disabled `async_insert` for us. We muddled through this for a long time, thinking that we were batching inserts, and not knowing the root cause of our relatively poor ingestion health.",
  "created_at": "2023-09-16T21:22:50Z"
}