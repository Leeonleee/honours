{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 23041,
  "instance_id": "ClickHouse__ClickHouse-23041",
  "issue_numbers": [
    "22671"
  ],
  "base_commit": "643e697918afe7cb01a47bcfc96f6ec89bac1922",
  "patch": "diff --git a/src/Common/ZooKeeper/TestKeeper.cpp b/src/Common/ZooKeeper/TestKeeper.cpp\nindex 5951164f58f2..36c875fe3259 100644\n--- a/src/Common/ZooKeeper/TestKeeper.cpp\n+++ b/src/Common/ZooKeeper/TestKeeper.cpp\n@@ -421,26 +421,38 @@ std::pair<ResponsePtr, Undo> TestKeeperMultiRequest::process(TestKeeper::Contain\n \n     try\n     {\n-        for (const auto & request : requests)\n+        auto request_it = requests.begin();\n+        response.error = Error::ZOK;\n+        while (request_it != requests.end())\n         {\n-            const TestKeeperRequest & concrete_request = dynamic_cast<const TestKeeperRequest &>(*request);\n+            const TestKeeperRequest & concrete_request = dynamic_cast<const TestKeeperRequest &>(**request_it);\n+            ++request_it;\n             auto [ cur_response, undo_action ] = concrete_request.process(container, zxid);\n             response.responses.emplace_back(cur_response);\n             if (cur_response->error != Error::ZOK)\n             {\n                 response.error = cur_response->error;\n+                break;\n+            }\n+\n+            undo_actions.emplace_back(std::move(undo_action));\n+        }\n \n-                for (auto it = undo_actions.rbegin(); it != undo_actions.rend(); ++it)\n-                    if (*it)\n-                        (*it)();\n+        if (response.error != Error::ZOK)\n+        {\n+            for (auto it = undo_actions.rbegin(); it != undo_actions.rend(); ++it)\n+                if (*it)\n+                    (*it)();\n \n-                return { std::make_shared<MultiResponse>(response), {} };\n+            while (request_it != requests.end())\n+            {\n+                const TestKeeperRequest & concrete_request = dynamic_cast<const TestKeeperRequest &>(**request_it);\n+                ++request_it;\n+                response.responses.emplace_back(concrete_request.createResponse());\n+                response.responses.back()->error = Error::ZRUNTIMEINCONSISTENCY;\n             }\n-            else\n-                undo_actions.emplace_back(std::move(undo_action));\n         }\n \n-        response.error = Error::ZOK;\n         return { std::make_shared<MultiResponse>(response), {} };\n     }\n     catch (...)\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01152_cross_replication.reference b/tests/queries/0_stateless/01152_cross_replication.reference\nnew file mode 100644\nindex 000000000000..389d14ff28b6\n--- /dev/null\n+++ b/tests/queries/0_stateless/01152_cross_replication.reference\n@@ -0,0 +1,10 @@\n+localhost\t9000\t0\t\t0\t0\n+localhost\t9000\t0\t\t0\t0\n+demo_loan_01568\n+demo_loan_01568\n+CREATE TABLE shard_0.demo_loan_01568\\n(\\n    `id` Int64 COMMENT \\'id\\',\\n    `date_stat` Date COMMENT \\'date of stat\\',\\n    `customer_no` String COMMENT \\'customer no\\',\\n    `loan_principal` Float64 COMMENT \\'loan principal\\'\\n)\\nENGINE = ReplacingMergeTree\\nPARTITION BY toYYYYMM(date_stat)\\nORDER BY id\\nSETTINGS index_granularity = 8192\n+CREATE TABLE shard_1.demo_loan_01568\\n(\\n    `id` Int64 COMMENT \\'id\\',\\n    `date_stat` Date COMMENT \\'date of stat\\',\\n    `customer_no` String COMMENT \\'customer no\\',\\n    `loan_principal` Float64 COMMENT \\'loan principal\\'\\n)\\nENGINE = ReplacingMergeTree\\nPARTITION BY toYYYYMM(date_stat)\\nORDER BY id\\nSETTINGS index_granularity = 8192\n+1\t2021-04-13\tqwerty\t3.14159\n+2\t2021-04-14\tasdfgh\t2.71828\n+2\t2021-04-14\tasdfgh\t2.71828\n+1\t2021-04-13\tqwerty\t3.14159\ndiff --git a/tests/queries/0_stateless/01152_cross_replication.sql b/tests/queries/0_stateless/01152_cross_replication.sql\nnew file mode 100644\nindex 000000000000..23507c41fd02\n--- /dev/null\n+++ b/tests/queries/0_stateless/01152_cross_replication.sql\n@@ -0,0 +1,30 @@\n+DROP DATABASE IF EXISTS shard_0;\n+DROP DATABASE IF EXISTS shard_1;\n+SET distributed_ddl_output_mode='none';\n+DROP TABLE IF EXISTS demo_loan_01568_dist;\n+\n+CREATE DATABASE shard_0;\n+CREATE DATABASE shard_1;\n+\n+CREATE TABLE demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat); -- { serverError 371 }\n+SET distributed_ddl_output_mode='throw';\n+CREATE TABLE shard_0.demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat);\n+CREATE TABLE shard_1.demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat);\n+SET distributed_ddl_output_mode='none';\n+\n+SHOW TABLES FROM shard_0;\n+SHOW TABLES FROM shard_1;\n+SHOW CREATE TABLE shard_0.demo_loan_01568;\n+SHOW CREATE TABLE shard_1.demo_loan_01568;\n+\n+CREATE TABLE demo_loan_01568_dist AS shard_0.demo_loan_01568 ENGINE=Distributed('test_cluster_two_shards_different_databases', '', 'demo_loan_01568', id % 2);\n+INSERT INTO demo_loan_01568_dist VALUES (1, '2021-04-13', 'qwerty', 3.14159), (2, '2021-04-14', 'asdfgh', 2.71828);\n+SYSTEM FLUSH DISTRIBUTED demo_loan_01568_dist;\n+SELECT * FROM demo_loan_01568_dist ORDER BY id;\n+\n+SELECT * FROM shard_0.demo_loan_01568;\n+SELECT * FROM shard_1.demo_loan_01568;\n+\n+DROP DATABASE shard_0;\n+DROP DATABASE shard_1;\n+DROP TABLE demo_loan_01568_dist;\ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex d41a41bd5244..59ee6ae89f45 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -557,6 +557,7 @@\n         \"01135_default_and_alter_zookeeper\",\n         \"01148_zookeeper_path_macros_unfolding\",\n         \"01150_ddl_guard_rwr\",\n+        \"01152_cross_replication\",\n         \"01185_create_or_replace_table\",\n         \"01190_full_attach_syntax\",\n         \"01191_rename_dictionary\",\n",
  "problem_statement": "assertion failure in `ON CLUSTER` query with TestKeeper\n```\r\n2021.04.05 20:41:11.963398 [ 589679 ] {f459c27c-24b9-4ca7-ba41-6da0b34e1162} <Debug> executeQuery: (from [::1]:36122, using production parser) (comment: /home/akuzm/ch2/ch/tests/queries/0_stateless/01568_window_functions_distributed.sql) \r\n CREATE TABLE demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat);\r\n2021.04.05 20:41:11.964234 [ 589679 ] {f459c27c-24b9-4ca7-ba41-6da0b34e1162} <Trace> ContextAccess (default): Access granted: CREATE TABLE ON shard_0.demo_loan_01568\r\n2021.04.05 20:41:11.964328 [ 589679 ] {f459c27c-24b9-4ca7-ba41-6da0b34e1162} <Trace> ContextAccess (default): Access granted: CREATE TABLE ON shard_1.demo_loan_01568\r\n2021.04.05 20:41:11.965306 [ 589730 ] {} <Debug> DDLWorker: Scheduling tasks\r\n2021.04.05 20:41:11.965349 [ 589708 ] {} <Trace> DDLWorker: Too early to clean queue, will do it later.\r\n2021.04.05 20:41:11.965527 [ 589730 ] {} <Debug> DDLWorker: Will schedule 1 tasks starting from query-0000000000\r\n2021.04.05 20:41:11.965596 [ 589730 ] {} <Trace> DDLWorker: Checking task query-0000000000\r\n2021.04.05 20:41:11.966655 [ 589730 ] {} <Warning> DDLWorker: There are two the same ClickHouse instances in task query-0000000000: localhost:9000 and localhost:9000. Will use the first one only.\r\n2021.04.05 20:41:11.966853 [ 589679 ] {f459c27c-24b9-4ca7-ba41-6da0b34e1162} <Debug> executeQuery: Query pipeline:\r\nDDLQueryStatusInputStream\r\n\r\n2021.04.05 20:41:12.020599 [ 589730 ] {} <Error> DDLWorker: Cannot parse DDL task query-0000000000: Cannot parse query or obtain cluster info. Will try to send error status: 371\r\nCode: 371, e.displayText() = DB::Exception: For a distributed DDL on circular replicated cluster its table name must be qualified by database name. (version 21.4.1.1)\r\nclickhouse-server: /home/akuzm/ch2/ch/src/Interpreters/DDLWorker.cpp:1011: void DB::DDLWorker::createStatusDirs(const std::string &, const DB::ZooKeeperPtr &): Assertion `Coordination::isHardwareError(code) || code == Coordination::Error::ZNONODE' failed.\r\n2021.04.05 20:41:12.021227 [ 589676 ] {} <Trace> BaseDaemon: Received signal 6\r\n2021.04.05 20:41:12.021622 [ 589955 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.04.05 20:41:12.021737 [ 589955 ] {} <Fatal> BaseDaemon: (version 21.4.1.1, build id: 1D9C4F29D5439E3A) (from thread 589730) (no query) Received signal Aborted (6)\r\n2021.04.05 20:41:12.021796 [ 589955 ] {} <Fatal> BaseDaemon: \r\n2021.04.05 20:41:12.021913 [ 589955 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f9fd341a18b 0x7f9fd33f9859 0x7f9fd33f9729 0x7f9fd340af36 0x7f9fbf8f6ebb 0x7f9fbf8ef6a7 0x7f9fbf8eeff4 0x7f9fbf8f23b0 0x7f9fbf8ec7e7 0x7f9fbf907407 0x7f9fbf90736a 0x7f9fbf907292 0x7f9fbf90717f 0x7f9fbf9070ad 0x7f9fbf90707d 0x7f9fbf907055 0x7f9fbf907030 0x7f9fd5c107b9 0x7f9fd5c10765 0x7f9fd550efa4 0x7f9fd5515bb4 0x7f9fd5515b3d 0x7f9fd5515a95 0x7f9fd5515462 0x7f9fd37ad609 0x7f9fd34f6293\r\n2021.04.05 20:41:12.024448 [ 589955 ] {} <Fatal> BaseDaemon: 4. /build/glibc-eX1tMB/glibc-2.31/signal/../sysdeps/unix/sysv/linux/raise.c:51: raise @ 0x4618b in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.04.05 20:41:12.028554 [ 589955 ] {} <Fatal> BaseDaemon: 5. /build/glibc-eX1tMB/glibc-2.31/stdlib/abort.c:81: abort @ 0x25859 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.04.05 20:41:12.035536 [ 589955 ] {} <Fatal> BaseDaemon: 6.1. inlined from /build/glibc-eX1tMB/glibc-2.31/intl/loadmsgcat.c:509: get_sysdep_segment_value\r\n2021.04.05 20:41:12.035619 [ 589955 ] {} <Fatal> BaseDaemon: 6. loadmsgcat.c:970: _nl_load_domain.cold @ 0x25729 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.04.05 20:41:12.080832 [ 589955 ] {} <Fatal> BaseDaemon: 7. ? @ 0x36f36 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.04.05 20:41:12.481819 [ 589955 ] {} <Fatal> BaseDaemon: 8. /home/akuzm/ch2/ch/src/Interpreters/DDLWorker.cpp:1012: DB::DDLWorker::createStatusDirs(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<zkutil::ZooKeeper> const&) @ 0x1611ebb in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:12.501654 [ 589697 ] {} <Trace> SystemLog (system.query_thread_log): Flushing system log, 23 entries to flush\r\n2021.04.05 20:41:12.510299 [ 589697 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 74.42 GiB.\r\n2021.04.05 20:41:12.520028 [ 589697 ] {} <Trace> system.query_thread_log: Renaming temporary part tmp_insert_202104_3_3_0 to 202104_1606_1606_0.\r\n2021.04.05 20:41:12.521328 [ 589697 ] {} <Trace> SystemLog (system.query_thread_log): Flushed system log\r\n2021.04.05 20:41:12.531652 [ 589693 ] {} <Trace> SystemLog (system.query_log): Flushing system log, 25 entries to flush\r\n2021.04.05 20:41:12.547796 [ 589693 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 74.42 GiB.\r\n2021.04.05 20:41:12.561346 [ 589693 ] {} <Trace> system.query_log: Renaming temporary part tmp_insert_202104_3_3_0 to 202104_1653_1653_0.\r\n2021.04.05 20:41:12.564002 [ 589693 ] {} <Trace> SystemLog (system.query_log): Flushed system log\r\n2021.04.05 20:41:12.901806 [ 589955 ] {} <Fatal> BaseDaemon: 9. /home/akuzm/ch2/ch/src/Interpreters/DDLWorker.cpp:250: DB::DDLWorker::initAndCheckTask(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, std::__1::shared_ptr<zkutil::ZooKeeper> const&)::$_2::operator()(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x160a6a7 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:13.162367 [ 589691 ] {} <Debug> DNSResolver: Updating DNS cache\r\n2021.04.05 20:41:13.162884 [ 589691 ] {} <Debug> DNSResolver: Updated DNS cache\r\n2021.04.05 20:41:13.288263 [ 589955 ] {} <Fatal> BaseDaemon: 10. /home/akuzm/ch2/ch/src/Interpreters/DDLWorker.cpp:288: DB::DDLWorker::initAndCheckTask(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, std::__1::shared_ptr<zkutil::ZooKeeper> const&) @ 0x1609ff4 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:13.674881 [ 589955 ] {} <Fatal> BaseDaemon: 11. /home/akuzm/ch2/ch/src/Interpreters/DDLWorker.cpp:435: DB::DDLWorker::scheduleTasks(bool) @ 0x160d3b0 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:14.080896 [ 589955 ] {} <Fatal> BaseDaemon: 12. /home/akuzm/ch2/ch/src/Interpreters/DDLWorker.cpp:1117: DB::DDLWorker::runMainThread() @ 0x16077e7 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:14.520270 [ 589955 ] {} <Fatal> BaseDaemon: 13. /home/akuzm/ch2/ch/contrib/libcxx/include/type_traits:3624: decltype(*(std::__1::forward<DB::DDLWorker*&>(fp0)).*fp()) std::__1::__invoke_constexpr<void (DB::DDLWorker::*&)(), DB::DDLWorker*&, void>(void (DB::DDLWorker::*&)(), DB::DDLWorker*&) @ 0x1622407 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:14.951255 [ 589955 ] {} <Fatal> BaseDaemon: 14. /home/akuzm/ch2/ch/contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<void (DB::DDLWorker::*&)(), std::__1::tuple<DB::DDLWorker*>&, 0ul>(void (DB::DDLWorker::*&)(), std::__1::tuple<DB::DDLWorker*>&, std::__1::__tuple_indices<0ul>) @ 0x162236a in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:15.384774 [ 589955 ] {} <Fatal> BaseDaemon: 15. /home/akuzm/ch2/ch/contrib/libcxx/include/tuple:1424: decltype(auto) std::__1::apply<void (DB::DDLWorker::*&)(), std::__1::tuple<DB::DDLWorker*>&>(void (DB::DDLWorker::*&)(), std::__1::tuple<DB::DDLWorker*>&) @ 0x1622292 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:15.843730 [ 589955 ] {} <Fatal> BaseDaemon: 16. /home/akuzm/ch2/ch/src/Common/ThreadPool.h:178: ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'()::operator()() @ 0x162217f in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:16.296689 [ 589955 ] {} <Fatal> BaseDaemon: 17. /home/akuzm/ch2/ch/contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<void (DB::DDLWorker::*)()>(fp)(std::__1::forward<DB::DDLWorker*>(fp0))) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'()&>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&) @ 0x16220ad in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:16.754507 [ 589955 ] {} <Fatal> BaseDaemon: 18. /home/akuzm/ch2/ch/contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'()&>(void (DB::DDLWorker::*&&)()...) @ 0x162207d in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:17.205419 [ 589955 ] {} <Fatal> BaseDaemon: 19. /home/akuzm/ch2/ch/contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()>::operator()() @ 0x1622055 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:17.660224 [ 589955 ] {} <Fatal> BaseDaemon: 20. /home/akuzm/ch2/ch/contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0x1622030 in /home/akuzm/ch2/build-clang11/src/libclickhouse_interpretersd.so\r\n2021.04.05 20:41:18.113033 [ 589955 ] {} <Fatal> BaseDaemon: 21. /home/akuzm/ch2/ch/contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x10e7b9 in /home/akuzm/ch2/build-clang11/programs/server/libclickhouse-server-libd.so\r\n2021.04.05 20:41:18.519333 [ 589955 ] {} <Fatal> BaseDaemon: 22. /home/akuzm/ch2/ch/contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x10e765 in /home/akuzm/ch2/build-clang11/programs/server/libclickhouse-server-libd.so\r\n2021.04.05 20:41:18.613202 [ 589955 ] {} <Fatal> BaseDaemon: 23. /home/akuzm/ch2/ch/src/Common/ThreadPool.cpp:247: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x33afa4 in /home/akuzm/ch2/build-clang11/src/libclickhouse_common_iod.so\r\n2021.04.05 20:41:18.674626 [ 589955 ] {} <Fatal> BaseDaemon: 24. /home/akuzm/ch2/ch/src/Common/ThreadPool.cpp:124: void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const @ 0x341bb4 in /home/akuzm/ch2/build-clang11/src/libclickhouse_common_iod.so\r\n2021.04.05 20:41:18.733711 [ 589955 ] {} <Fatal> BaseDaemon: 25. /home/akuzm/ch2/ch/contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...) @ 0x341b3d in /home/akuzm/ch2/build-clang11/src/libclickhouse_common_iod.so\r\n2021.04.05 20:41:18.792485 [ 589955 ] {} <Fatal> BaseDaemon: 26. /home/akuzm/ch2/ch/contrib/libcxx/include/thread:281: void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>&, std::__1::__tuple_indices<>) @ 0x341a95 in /home/akuzm/ch2/build-clang11/src/libclickhouse_common_iod.so\r\n2021.04.05 20:41:18.851158 [ 589955 ] {} <Fatal> BaseDaemon: 27. /home/akuzm/ch2/ch/contrib/libcxx/include/thread:291: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x341462 in /home/akuzm/ch2/build-clang11/src/libclickhouse_common_iod.so\r\n2021.04.05 20:41:18.851254 [ 589955 ] {} <Fatal> BaseDaemon: 28. start_thread @ 0x9609 in /lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.04.05 20:41:18.851572 [ 589955 ] {} <Fatal> BaseDaemon: 29. /build/glibc-eX1tMB/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: clone @ 0x122293 in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.04.05 20:41:18.851653 [ 589955 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: 8F064C5E1A2EABEDFA9A4E91C53B5568. There is no information about the reference checksum.\r\n```\n",
  "hints_text": "What does \"without ZooKeeper\" mean?\n> What does \"without ZooKeeper\" mean?\r\n\r\nNot sure :) I just don't have any Zookeeper connection configured.\r\n\r\nIt reproduces each time for me, the cluster `test_cluster_two_shards_different_databases` is from the stateless tests.\n> > What does \"without ZooKeeper\" mean?\r\n> \r\n> Not sure :) I just don't have any Zookeeper connection configured.\r\n> \r\n> It reproduces each time for me, the cluster `test_cluster_two_shards_different_databases` is from the stateless tests.\r\n\r\nYou have, but probably it's TestKeeper or ClickHouse Keeper, not Apache ZooKeeper.\r\n\r\nWith Apache ZooKeeper it works as expected for me:\r\n```\r\navtokmakov-dev.sas.yp-c.yandex.net :) CREATE TABLE demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat);\r\n\r\nQuery id: 42a1374a-d512-4d56-a25b-4bd6f6698d35\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 localhost \u2502 9000 \u2502    371 \u2502 Code: 371, e.displayText() = DB::Exception: For a distributed DDL on circular replicated cluster its table name must be qualified by database name. (version 21.5.1.1) \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u2190 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.)  0%\r\n1 rows in set. Elapsed: 0.337 sec. \r\n\r\nReceived exception from server (version 21.5.1):\r\nCode: 371. DB::Exception: Received from localhost:9000. DB::Exception: There was an error on [localhost:9000]: Code: 371, e.displayText() = DB::Exception: For a distributed DDL on circular replicated cluster its table name must be qualified by database name. (version 21.5.1.1). \r\n\r\navtokmakov-dev.sas.yp-c.yandex.net :) CREATE TABLE shard_0.demo_loan_01568 ON CLUSTER test_cluster_two_shards_different_databases ( `id` Int64 COMMENT 'id', `date_stat` Date COMMENT 'date of stat', `customer_no` String COMMENT 'customer no', `loan_principal` Float64 COMMENT 'loan principal' ) ENGINE=ReplacingMergeTree() ORDER BY id PARTITION BY toYYYYMM(date_stat);\r\n\r\nQuery id: 00e5921f-6a16-4a3f-abd3-63a8bb2710d9\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 localhost \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\nSeems like other *Keepers response with different error codes on attempt to create existing nodes in `multi` request and `DDLWorker` does not expect to get such error codes.\n> You have, but probably it's TestKeeper or ClickHouse Keeper, not Apache ZooKeeper.\r\n\r\nYou're right:\r\n```\r\n    <zookeeper>\r\n        <implementation>testkeeper</implementation>\r\n    </zookeeper>\r\n```\r\nDoes it reproduce for you with TestKeeper?\nYes, it reproduces with TestKeeper if table name is not qualified.",
  "created_at": "2021-04-13T12:52:54Z"
}