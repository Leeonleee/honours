{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 69660,
  "instance_id": "ClickHouse__ClickHouse-69660",
  "issue_numbers": [
    "66013"
  ],
  "base_commit": "477a58bdf092d8c8eea5e6fe497d0c1f376523f7",
  "patch": "diff --git a/programs/server/Server.cpp b/programs/server/Server.cpp\nindex dcf5b32d6b79..d7fe3734f3f4 100644\n--- a/programs/server/Server.cpp\n+++ b/programs/server/Server.cpp\n@@ -2393,14 +2393,23 @@ try\n         if (has_zookeeper && config().has(\"distributed_ddl\"))\n         {\n             /// DDL worker should be started after all tables were loaded\n-            String ddl_zookeeper_path = config().getString(\"distributed_ddl.path\", \"/clickhouse/task_queue/ddl/\");\n+            String ddl_queue_path = config().getString(\"distributed_ddl.path\", \"/clickhouse/task_queue/ddl/\");\n+            String ddl_replicas_path = config().getString(\"distributed_ddl.replicas_path\", \"/clickhouse/task_queue/replicas/\");\n             int pool_size = config().getInt(\"distributed_ddl.pool_size\", 1);\n             if (pool_size < 1)\n                 throw Exception(ErrorCodes::ARGUMENT_OUT_OF_BOUND, \"distributed_ddl.pool_size should be greater then 0\");\n-            global_context->setDDLWorker(std::make_unique<DDLWorker>(pool_size, ddl_zookeeper_path, global_context, &config(),\n-                                                                     \"distributed_ddl\", \"DDLWorker\",\n-                                                                     &CurrentMetrics::MaxDDLEntryID, &CurrentMetrics::MaxPushedDDLEntryID),\n-                                         load_metadata_tasks);\n+            global_context->setDDLWorker(\n+                std::make_unique<DDLWorker>(\n+                    pool_size,\n+                    ddl_queue_path,\n+                    ddl_replicas_path,\n+                    global_context,\n+                    &config(),\n+                    \"distributed_ddl\",\n+                    \"DDLWorker\",\n+                    &CurrentMetrics::MaxDDLEntryID,\n+                    &CurrentMetrics::MaxPushedDDLEntryID),\n+                load_metadata_tasks);\n         }\n \n         /// Do not keep tasks in server, they should be kept inside databases. Used here to make dependent tasks only.\ndiff --git a/programs/server/config.xml b/programs/server/config.xml\nindex 28f1f465c719..15649b5c95db 100644\n--- a/programs/server/config.xml\n+++ b/programs/server/config.xml\n@@ -1450,6 +1450,8 @@\n     <distributed_ddl>\n         <!-- Path in ZooKeeper to queue with DDL queries -->\n         <path>/clickhouse/task_queue/ddl</path>\n+        <!-- Path in ZooKeeper to store running DDL hosts -->\n+        <replicas_path>/clickhouse/task_queue/replicas</replicas_path>\n \n         <!-- Settings from this profile will be used to execute DDL queries -->\n         <!-- <profile>default</profile> -->\ndiff --git a/src/Databases/DatabaseReplicated.cpp b/src/Databases/DatabaseReplicated.cpp\nindex 387667b1b42e..8992a9d85485 100644\n--- a/src/Databases/DatabaseReplicated.cpp\n+++ b/src/Databases/DatabaseReplicated.cpp\n@@ -4,47 +4,49 @@\n \n #include <Backups/IRestoreCoordination.h>\n #include <Backups/RestorerFromBackup.h>\n-#include <base/chrono_io.h>\n-#include <base/getFQDNOrHostName.h>\n-#include <Common/Exception.h>\n-#include <Common/Macros.h>\n-#include <Common/OpenTelemetryTraceContext.h>\n-#include <Common/ZooKeeper/KeeperException.h>\n-#include <Common/ZooKeeper/Types.h>\n-#include <Common/ZooKeeper/ZooKeeper.h>\n-#include <Common/ZooKeeper/IKeeper.h>\n-#include <Common/PoolId.h>\n #include <Core/ServerSettings.h>\n #include <Core/Settings.h>\n+#include <Databases/DDLDependencyVisitor.h>\n #include <Databases/DatabaseFactory.h>\n #include <Databases/DatabaseReplicated.h>\n #include <Databases/DatabaseReplicatedWorker.h>\n-#include <Databases/DDLDependencyVisitor.h>\n #include <Databases/TablesDependencyGraph.h>\n #include <Databases/enableAllExperimentalSettings.h>\n+#include <IO/ReadBufferFromFile.h>\n+#include <IO/ReadBufferFromString.h>\n+#include <IO/ReadHelpers.h>\n+#include <IO/SharedThreadPools.h>\n+#include <IO/WriteHelpers.h>\n #include <Interpreters/Cluster.h>\n #include <Interpreters/Context.h>\n-#include <Interpreters/DatabaseCatalog.h>\n #include <Interpreters/DDLTask.h>\n+#include <Interpreters/DatabaseCatalog.h>\n+#include <Interpreters/InterpreterCreateQuery.h>\n+#include <Interpreters/ReplicatedDatabaseQueryStatusSource.h>\n #include <Interpreters/evaluateConstantExpression.h>\n #include <Interpreters/executeDDLQueryOnCluster.h>\n #include <Interpreters/executeQuery.h>\n-#include <Interpreters/InterpreterCreateQuery.h>\n-#include <IO/ReadBufferFromFile.h>\n-#include <IO/ReadBufferFromString.h>\n-#include <IO/ReadHelpers.h>\n-#include <IO/WriteHelpers.h>\n-#include <IO/SharedThreadPools.h>\n #include <Parsers/ASTAlterQuery.h>\n+#include <Parsers/ASTDeleteQuery.h>\n #include <Parsers/ASTDropQuery.h>\n #include <Parsers/ASTFunction.h>\n-#include <Parsers/ASTDeleteQuery.h>\n+#include <Parsers/ParserCreateQuery.h>\n #include <Parsers/formatAST.h>\n #include <Parsers/parseQuery.h>\n-#include <Parsers/ParserCreateQuery.h>\n #include <Parsers/queryToString.h>\n-#include <Storages/StorageKeeperMap.h>\n+#include <Processors/Sinks/EmptySink.h>\n #include <Storages/AlterCommands.h>\n+#include <Storages/StorageKeeperMap.h>\n+#include <base/chrono_io.h>\n+#include <base/getFQDNOrHostName.h>\n+#include <Common/Exception.h>\n+#include <Common/Macros.h>\n+#include <Common/OpenTelemetryTraceContext.h>\n+#include <Common/PoolId.h>\n+#include <Common/ZooKeeper/IKeeper.h>\n+#include <Common/ZooKeeper/KeeperException.h>\n+#include <Common/ZooKeeper/Types.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n \n namespace DB\n {\n@@ -55,6 +57,8 @@ namespace Setting\n     extern const SettingsUInt64 max_parser_backtracks;\n     extern const SettingsUInt64 max_parser_depth;\n     extern const SettingsUInt64 max_query_size;\n+    extern const SettingsDistributedDDLOutputMode distributed_ddl_output_mode;\n+    extern const SettingsInt64 distributed_ddl_task_timeout;\n     extern const SettingsBool throw_on_unsupported_query_inside_transaction;\n }\n \n@@ -443,7 +447,6 @@ void DatabaseReplicated::fillClusterAuthInfo(String collection_name, const Poco:\n     cluster_auth_info.cluster_secure_connection = config_ref.getBool(config_prefix + \".cluster_secure_connection\", false);\n }\n \n-\n void DatabaseReplicated::tryConnectToZooKeeperAndInitDatabase(LoadingStrictnessLevel mode)\n {\n     try\n@@ -1096,7 +1099,8 @@ BlockIO DatabaseReplicated::tryEnqueueReplicatedDDL(const ASTPtr & query, Contex\n             hosts_to_wait.push_back(unfiltered_hosts[i]);\n     }\n \n-    return getDistributedDDLStatus(node_path, entry, query_context, &hosts_to_wait);\n+\n+    return getQueryStatus(node_path, fs::path(zookeeper_path) / \"replicas\", query_context, hosts_to_wait);\n }\n \n static UUID getTableUUIDIfReplicated(const String & metadata, ContextPtr context)\n@@ -2040,4 +2044,21 @@ void registerDatabaseReplicated(DatabaseFactory & factory)\n     };\n     factory.registerDatabase(\"Replicated\", create_fn, {.supports_arguments = true, .supports_settings = true});\n }\n+\n+BlockIO DatabaseReplicated::getQueryStatus(\n+    const String & node_path, const String & replicas_path, ContextPtr context_, const Strings & hosts_to_wait)\n+{\n+    BlockIO io;\n+    if (context_->getSettingsRef()[Setting::distributed_ddl_task_timeout] == 0)\n+        return io;\n+\n+    auto source = std::make_shared<ReplicatedDatabaseQueryStatusSource>(node_path, replicas_path, context_, hosts_to_wait);\n+    io.pipeline = QueryPipeline(std::move(source));\n+\n+    if (context_->getSettingsRef()[Setting::distributed_ddl_output_mode] == DistributedDDLOutputMode::NONE\n+        || context_->getSettingsRef()[Setting::distributed_ddl_output_mode] == DistributedDDLOutputMode::NONE_ONLY_ACTIVE)\n+        io.pipeline.complete(std::make_shared<EmptySink>(io.pipeline.getHeader()));\n+\n+    return io;\n+}\n }\ndiff --git a/src/Databases/DatabaseReplicated.h b/src/Databases/DatabaseReplicated.h\nindex 3195de48c1fa..fb239435dc12 100644\n--- a/src/Databases/DatabaseReplicated.h\n+++ b/src/Databases/DatabaseReplicated.h\n@@ -151,6 +151,9 @@ class DatabaseReplicated : public DatabaseAtomic\n     void waitDatabaseStarted() const override;\n     void stopLoading() override;\n \n+    static BlockIO\n+    getQueryStatus(const String & node_path, const String & replicas_path, ContextPtr context, const Strings & hosts_to_wait);\n+\n     String zookeeper_path;\n     String shard_name;\n     String replica_name;\ndiff --git a/src/Databases/DatabaseReplicatedWorker.cpp b/src/Databases/DatabaseReplicatedWorker.cpp\nindex 3b70383c28be..5d75dff391aa 100644\n--- a/src/Databases/DatabaseReplicatedWorker.cpp\n+++ b/src/Databases/DatabaseReplicatedWorker.cpp\n@@ -39,7 +39,14 @@ namespace ErrorCodes\n static constexpr const char * FORCE_AUTO_RECOVERY_DIGEST = \"42\";\n \n DatabaseReplicatedDDLWorker::DatabaseReplicatedDDLWorker(DatabaseReplicated * db, ContextPtr context_)\n-    : DDLWorker(/* pool_size */ 1, db->zookeeper_path + \"/log\", context_, nullptr, {}, fmt::format(\"DDLWorker({})\", db->getDatabaseName()))\n+    : DDLWorker(\n+          /* pool_size */ 1,\n+          db->zookeeper_path + \"/log\",\n+          db->zookeeper_path + \"/replicas\",\n+          context_,\n+          nullptr,\n+          {},\n+          fmt::format(\"DDLWorker({})\", db->getDatabaseName()))\n     , database(db)\n {\n     /// Pool size must be 1 to avoid reordering of log entries.\ndiff --git a/src/Databases/DatabaseReplicatedWorker.h b/src/Databases/DatabaseReplicatedWorker.h\nindex e741037e7021..b690854e2496 100644\n--- a/src/Databases/DatabaseReplicatedWorker.h\n+++ b/src/Databases/DatabaseReplicatedWorker.h\n@@ -38,9 +38,14 @@ class DatabaseReplicatedDDLWorker : public DDLWorker\n     UInt32 getLogPointer() const;\n \n     UInt64 getCurrentInitializationDurationMs() const;\n+\n private:\n     bool initializeMainThread() override;\n-    void initializeReplication();\n+    void initializeReplication() override;\n+\n+    void createReplicaDirs(const ZooKeeperPtr &, const NameSet &) override { }\n+    void markReplicasActive(bool) override { }\n+\n     void initializeLogPointer(const String & processed_entry_name);\n \n     DDLTaskPtr initAndCheckTask(const String & entry_name, String & out_reason, const ZooKeeperPtr & zookeeper, bool dry_run) override;\ndiff --git a/src/Interpreters/DDLOnClusterQueryStatusSource.cpp b/src/Interpreters/DDLOnClusterQueryStatusSource.cpp\nnew file mode 100644\nindex 000000000000..9b5215eb41a3\n--- /dev/null\n+++ b/src/Interpreters/DDLOnClusterQueryStatusSource.cpp\n@@ -0,0 +1,157 @@\n+#include <unordered_set>\n+#include <Core/Settings.h>\n+#include <DataTypes/DataTypeEnum.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Databases/DatabaseReplicated.h>\n+#include <Interpreters/DDLOnClusterQueryStatusSource.h>\n+#include <Common/DNSResolver.h>\n+#include <Common/isLocalAddress.h>\n+\n+namespace DB\n+{\n+namespace Setting\n+{\n+extern const SettingsDistributedDDLOutputMode distributed_ddl_output_mode;\n+}\n+\n+namespace ErrorCodes\n+{\n+extern const int TIMEOUT_EXCEEDED;\n+}\n+\n+DDLOnClusterQueryStatusSource::DDLOnClusterQueryStatusSource(\n+    const String & zk_node_path, const String & zk_replicas_path, ContextPtr context_, const Strings & hosts_to_wait)\n+    : DistributedQueryStatusSource(\n+          zk_node_path, zk_replicas_path, getSampleBlock(context_), context_, hosts_to_wait, \"DDLOnClusterQueryStatusSource\")\n+{\n+}\n+\n+ExecutionStatus DDLOnClusterQueryStatusSource::checkStatus(const String & host_id)\n+{\n+    fs::path status_path = fs::path(node_path) / \"finished\" / host_id;\n+    return getExecutionStatus(status_path);\n+}\n+\n+Chunk DDLOnClusterQueryStatusSource::generateChunkWithUnfinishedHosts() const\n+{\n+    NameSet unfinished_hosts = waiting_hosts;\n+    for (const auto & host_id : finished_hosts)\n+        unfinished_hosts.erase(host_id);\n+\n+    NameSet active_hosts_set = NameSet{current_active_hosts.begin(), current_active_hosts.end()};\n+\n+    /// Query is not finished on the rest hosts, so fill the corresponding rows with NULLs.\n+    MutableColumns columns = output.getHeader().cloneEmptyColumns();\n+    for (const String & host_id : unfinished_hosts)\n+    {\n+        size_t num = 0;\n+        auto [host, port] = parseHostAndPort(host_id);\n+        columns[num++]->insert(host);\n+        columns[num++]->insert(port);\n+        columns[num++]->insert(Field{});\n+        columns[num++]->insert(Field{});\n+        columns[num++]->insert(unfinished_hosts.size());\n+        columns[num++]->insert(current_active_hosts.size());\n+    }\n+    return Chunk(std::move(columns), unfinished_hosts.size());\n+}\n+\n+Strings DDLOnClusterQueryStatusSource::getNodesToWait()\n+{\n+    return {String(fs::path(node_path) / \"finished\"), String(fs::path(node_path) / \"active\")};\n+}\n+Chunk DDLOnClusterQueryStatusSource::handleTimeoutExceeded()\n+{\n+    timeout_exceeded = true;\n+\n+    size_t num_unfinished_hosts = waiting_hosts.size() - num_hosts_finished;\n+    size_t num_active_hosts = current_active_hosts.size();\n+\n+    constexpr auto msg_format = \"Distributed DDL task {} is not finished on {} of {} hosts \"\n+                                \"({} of them are currently executing the task, {} are inactive). \"\n+                                \"They are going to execute the query in background. Was waiting for {} seconds{}\";\n+\n+    if (throw_on_timeout || (throw_on_timeout_only_active && !stop_waiting_offline_hosts))\n+    {\n+        if (!first_exception)\n+            first_exception = std::make_unique<Exception>(Exception(\n+                ErrorCodes::TIMEOUT_EXCEEDED,\n+                msg_format,\n+                node_path,\n+                num_unfinished_hosts,\n+                waiting_hosts.size(),\n+                num_active_hosts,\n+                offline_hosts.size(),\n+                watch.elapsedSeconds(),\n+                stop_waiting_offline_hosts ? \"\" : \", which is longer than distributed_ddl_task_timeout\"));\n+\n+        return {};\n+    }\n+\n+    LOG_INFO(\n+        log,\n+        msg_format,\n+        node_path,\n+        num_unfinished_hosts,\n+        waiting_hosts.size(),\n+        num_active_hosts,\n+        offline_hosts.size(),\n+        watch.elapsedSeconds(),\n+        stop_waiting_offline_hosts ? \"\" : \"which is longer than distributed_ddl_task_timeout\");\n+\n+    return generateChunkWithUnfinishedHosts();\n+}\n+Chunk DDLOnClusterQueryStatusSource::stopWaitingOfflineHosts()\n+{\n+    // Same logic as timeout exceeded\n+    return handleTimeoutExceeded();\n+}\n+void DDLOnClusterQueryStatusSource::handleNonZeroStatusCode(const ExecutionStatus & status, const String & host_id)\n+{\n+    assert(status.code != 0);\n+\n+    if (!first_exception && context->getSettingsRef()[Setting::distributed_ddl_output_mode] != DistributedDDLOutputMode::NEVER_THROW)\n+    {\n+        auto [host, port] = parseHostAndPort(host_id);\n+        first_exception\n+            = std::make_unique<Exception>(Exception(status.code, \"There was an error on [{}:{}]: {}\", host, port, status.message));\n+    }\n+}\n+void DDLOnClusterQueryStatusSource::fillHostStatus(const String & host_id, const ExecutionStatus & status, MutableColumns & columns)\n+{\n+    size_t num = 0;\n+    auto [host, port] = parseHostAndPort(host_id);\n+    columns[num++]->insert(host);\n+    columns[num++]->insert(port);\n+    columns[num++]->insert(status.code);\n+    columns[num++]->insert(status.message);\n+    columns[num++]->insert(waiting_hosts.size() - num_hosts_finished);\n+    columns[num++]->insert(current_active_hosts.size());\n+}\n+\n+Block DDLOnClusterQueryStatusSource::getSampleBlock(ContextPtr context_)\n+{\n+    auto output_mode = context_->getSettingsRef()[Setting::distributed_ddl_output_mode];\n+\n+    auto maybe_make_nullable = [&](const DataTypePtr & type) -> DataTypePtr\n+    {\n+        if (output_mode == DistributedDDLOutputMode::THROW || output_mode == DistributedDDLOutputMode::NONE\n+            || output_mode == DistributedDDLOutputMode::NONE_ONLY_ACTIVE)\n+            return type;\n+        return std::make_shared<DataTypeNullable>(type);\n+    };\n+\n+\n+    return Block{\n+        {std::make_shared<DataTypeString>(), \"host\"},\n+        {std::make_shared<DataTypeUInt16>(), \"port\"},\n+        {maybe_make_nullable(std::make_shared<DataTypeInt64>()), \"status\"},\n+        {maybe_make_nullable(std::make_shared<DataTypeString>()), \"error\"},\n+        {std::make_shared<DataTypeUInt64>(), \"num_hosts_remaining\"},\n+        {std::make_shared<DataTypeUInt64>(), \"num_hosts_active\"},\n+    };\n+}\n+\n+}\ndiff --git a/src/Interpreters/DDLOnClusterQueryStatusSource.h b/src/Interpreters/DDLOnClusterQueryStatusSource.h\nnew file mode 100644\nindex 000000000000..cb50bde40f31\n--- /dev/null\n+++ b/src/Interpreters/DDLOnClusterQueryStatusSource.h\n@@ -0,0 +1,30 @@\n+#pragma once\n+\n+#include <Interpreters/Context_fwd.h>\n+#include <Interpreters/DDLTask.h>\n+#include <Interpreters/DistributedQueryStatusSource.h>\n+#include <Common/ZooKeeper/ZooKeeperRetries.h>\n+\n+namespace DB\n+{\n+class DDLOnClusterQueryStatusSource final : public DistributedQueryStatusSource\n+{\n+public:\n+    DDLOnClusterQueryStatusSource(\n+        const String & zk_node_path, const String & zk_replicas_path, ContextPtr context_, const Strings & hosts_to_wait);\n+\n+    String getName() const override { return \"DDLOnClusterQueryStatus\"; }\n+\n+protected:\n+    ExecutionStatus checkStatus(const String & host_id) override;\n+    Chunk generateChunkWithUnfinishedHosts() const override;\n+    Strings getNodesToWait() override;\n+    Chunk handleTimeoutExceeded() override;\n+    Chunk stopWaitingOfflineHosts() override;\n+    void handleNonZeroStatusCode(const ExecutionStatus & status, const String & host_id) override;\n+    void fillHostStatus(const String & host_id, const ExecutionStatus & status, MutableColumns & columns) override;\n+\n+private:\n+    static Block getSampleBlock(ContextPtr context_);\n+};\n+}\ndiff --git a/src/Interpreters/DDLWorker.cpp b/src/Interpreters/DDLWorker.cpp\nindex 497ff7d5d07e..1be1a0c9bb92 100644\n--- a/src/Interpreters/DDLWorker.cpp\n+++ b/src/Interpreters/DDLWorker.cpp\n@@ -1,48 +1,47 @@\n-#include <filesystem>\n \n-#include <Interpreters/DDLWorker.h>\n+#include <Core/ServerUUID.h>\n+#include <Core/Settings.h>\n+#include <IO/ReadBufferFromString.h>\n+#include <IO/ReadHelpers.h>\n+#include <IO/WriteHelpers.h>\n+#include <Interpreters/Cluster.h>\n+#include <Interpreters/Context.h>\n #include <Interpreters/DDLTask.h>\n+#include <Interpreters/DDLWorker.h>\n+#include <Interpreters/ZooKeeperLog.h>\n+#include <Interpreters/executeQuery.h>\n #include <Parsers/ASTAlterQuery.h>\n+#include <Parsers/ASTCreateIndexQuery.h>\n+#include <Parsers/ASTDropIndexQuery.h>\n #include <Parsers/ASTDropQuery.h>\n #include <Parsers/ASTOptimizeQuery.h>\n #include <Parsers/ASTQueryWithOnCluster.h>\n #include <Parsers/ASTQueryWithTableAndOutput.h>\n-#include <Parsers/ASTCreateIndexQuery.h>\n-#include <Parsers/ASTDropIndexQuery.h>\n #include <Parsers/ParserQuery.h>\n-#include <IO/WriteHelpers.h>\n-#include <IO/ReadHelpers.h>\n-#include <IO/ReadBufferFromString.h>\n #include <Storages/IStorage.h>\n-#include <Interpreters/executeQuery.h>\n-#include <Interpreters/Cluster.h>\n-#include <Interpreters/Context.h>\n+#include <Storages/StorageReplicatedMergeTree.h>\n+#include <Poco/Timestamp.h>\n #include <Common/OpenTelemetryTraceContext.h>\n-#include <Common/setThreadName.h>\n-#include <Common/randomSeed.h>\n-#include <Common/ZooKeeper/ZooKeeper.h>\n+#include <Common/ThreadPool.h>\n #include <Common/ZooKeeper/KeeperException.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n #include <Common/ZooKeeper/ZooKeeperLock.h>\n #include <Common/isLocalAddress.h>\n-#include <Core/ServerUUID.h>\n-#include <Core/Settings.h>\n-#include <Storages/StorageReplicatedMergeTree.h>\n-#include <Poco/Timestamp.h>\n-#include <base/sleep.h>\n-#include <base/getFQDNOrHostName.h>\n #include <Common/logger_useful.h>\n+#include <Common/randomSeed.h>\n+#include <Common/scope_guard_safe.h>\n+#include <Common/setThreadName.h>\n+\n+#include <base/getFQDNOrHostName.h>\n+#include <base/sleep.h>\n #include <base/sort.h>\n+\n #include <memory>\n #include <random>\n #include <pcg_random.hpp>\n-#include <Common/scope_guard_safe.h>\n-#include <Common/ThreadPool.h>\n-\n-#include <Interpreters/ZooKeeperLog.h>\n \n namespace fs = std::filesystem;\n \n-\n namespace CurrentMetrics\n {\n     extern const Metric DDLWorkerThreads;\n@@ -78,7 +77,8 @@ constexpr const char * TASK_PROCESSED_OUT_REASON = \"Task has been already proces\n \n DDLWorker::DDLWorker(\n     int pool_size_,\n-    const std::string & zk_root_dir,\n+    const std::string & zk_queue_dir,\n+    const std::string & zk_replicas_dir,\n     ContextPtr context_,\n     const Poco::Util::AbstractConfiguration * config,\n     const String & prefix,\n@@ -104,10 +104,15 @@ DDLWorker::DDLWorker(\n         worker_pool = std::make_unique<ThreadPool>(CurrentMetrics::DDLWorkerThreads, CurrentMetrics::DDLWorkerThreadsActive, CurrentMetrics::DDLWorkerThreadsScheduled, pool_size);\n     }\n \n-    queue_dir = zk_root_dir;\n+    queue_dir = zk_queue_dir;\n     if (queue_dir.back() == '/')\n         queue_dir.resize(queue_dir.size() - 1);\n \n+    replicas_dir = zk_replicas_dir;\n+    if (replicas_dir.back() == '/')\n+        replicas_dir.resize(replicas_dir.size() - 1);\n+\n+\n     if (config)\n     {\n         task_max_lifetime = config->getUInt64(prefix + \".task_max_lifetime\", static_cast<UInt64>(task_max_lifetime));\n@@ -1058,6 +1063,11 @@ String DDLWorker::enqueueQuery(DDLLogEntry & entry)\n     String query_path_prefix = fs::path(queue_dir) / \"query-\";\n     zookeeper->createAncestors(query_path_prefix);\n \n+    NameSet host_ids;\n+    for (const HostID & host : entry.hosts)\n+        host_ids.emplace(host.toString());\n+    createReplicaDirs(zookeeper, host_ids);\n+\n     String node_path = zookeeper->create(query_path_prefix, entry.toString(), zkutil::CreateMode::PersistentSequential);\n     if (max_pushed_entry_metric)\n     {\n@@ -1097,6 +1107,7 @@ bool DDLWorker::initializeMainThread()\n         {\n             auto zookeeper = getAndSetZooKeeper();\n             zookeeper->createAncestors(fs::path(queue_dir) / \"\");\n+            initializeReplication();\n             initialized = true;\n             return true;\n         }\n@@ -1158,6 +1169,14 @@ void DDLWorker::runMainThread()\n             }\n \n             cleanup_event->set();\n+            try\n+            {\n+                markReplicasActive(reinitialized);\n+            }\n+            catch (...)\n+            {\n+                tryLogCurrentException(log, \"An error occurred when markReplicasActive: \");\n+            }\n             scheduleTasks(reinitialized);\n             subsequent_errors_count = 0;\n \n@@ -1215,6 +1234,97 @@ void DDLWorker::runMainThread()\n }\n \n \n+void DDLWorker::initializeReplication()\n+{\n+    auto zookeeper = getAndSetZooKeeper();\n+\n+    zookeeper->createAncestors(fs::path(replicas_dir) / \"\");\n+\n+    NameSet host_id_set;\n+    for (const auto & it : context->getClusters())\n+    {\n+        auto cluster = it.second;\n+        for (const auto & host_ids : cluster->getHostIDs())\n+            for (const auto & host_id : host_ids)\n+                host_id_set.emplace(host_id);\n+    }\n+\n+    createReplicaDirs(zookeeper, host_id_set);\n+}\n+\n+void DDLWorker::createReplicaDirs(const ZooKeeperPtr & zookeeper, const NameSet & host_ids)\n+{\n+    for (const auto & host_id : host_ids)\n+        zookeeper->createAncestors(fs::path(replicas_dir) / host_id / \"\");\n+}\n+\n+void DDLWorker::markReplicasActive(bool reinitialized)\n+{\n+    auto zookeeper = getAndSetZooKeeper();\n+\n+    if (reinitialized)\n+    {\n+        // Reset all active_node_holders\n+        for (auto & it : active_node_holders)\n+        {\n+            auto & active_node_holder = it.second.second;\n+            if (active_node_holder)\n+                active_node_holder->setAlreadyRemoved();\n+            active_node_holder.reset();\n+        }\n+\n+        active_node_holders.clear();\n+    }\n+\n+    const auto maybe_secure_port = context->getTCPPortSecure();\n+    const auto port = context->getTCPPort();\n+\n+    Coordination::Stat replicas_stat;\n+    Strings host_ids = zookeeper->getChildren(replicas_dir, &replicas_stat);\n+    NameSet local_host_ids;\n+    for (const auto & host_id : host_ids)\n+    {\n+        if (active_node_holders.contains(host_id))\n+            continue;\n+\n+        try\n+        {\n+            HostID host = HostID::fromString(host_id);\n+            /// The port is considered local if it matches TCP or TCP secure port that the server is listening.\n+            bool is_local_host = (maybe_secure_port && host.isLocalAddress(*maybe_secure_port)) || host.isLocalAddress(port);\n+\n+            if (is_local_host)\n+                local_host_ids.emplace(host_id);\n+        }\n+        catch (const Exception & e)\n+        {\n+            LOG_WARNING(log, \"Unable to check if host {} is a local address, exception: {}\", host_id, e.displayText());\n+            continue;\n+        }\n+    }\n+\n+    for (const auto & host_id : local_host_ids)\n+    {\n+        auto it = active_node_holders.find(host_id);\n+        if (it != active_node_holders.end())\n+        {\n+            continue;\n+        }\n+\n+        String active_path = fs::path(replicas_dir) / host_id / \"active\";\n+        if (zookeeper->exists(active_path))\n+            continue;\n+\n+        String active_id = toString(ServerUUID::get());\n+        LOG_TRACE(log, \"Trying to mark a replica active: active_path={}, active_id={}\", active_path, active_id);\n+\n+        zookeeper->create(active_path, active_id, zkutil::CreateMode::Ephemeral);\n+        auto active_node_holder_zookeeper = zookeeper;\n+        auto active_node_holder = zkutil::EphemeralNodeHolder::existing(active_path, *active_node_holder_zookeeper);\n+        active_node_holders[host_id] = {active_node_holder_zookeeper, active_node_holder};\n+    }\n+}\n+\n void DDLWorker::runCleanupThread()\n {\n     setThreadName(\"DDLWorkerClnr\");\ndiff --git a/src/Interpreters/DDLWorker.h b/src/Interpreters/DDLWorker.h\nindex ac07b086242a..ee17714add9a 100644\n--- a/src/Interpreters/DDLWorker.h\n+++ b/src/Interpreters/DDLWorker.h\n@@ -1,24 +1,24 @@\n #pragma once\n \n-#include <Common/CurrentThread.h>\n+#include <Interpreters/Context.h>\n+#include <Parsers/IAST_fwd.h>\n+#include <Storages/IStorage_fwd.h>\n #include <Common/CurrentMetrics.h>\n+#include <Common/CurrentThread.h>\n #include <Common/DNSResolver.h>\n #include <Common/ThreadPool_fwd.h>\n #include <Common/ZooKeeper/IKeeper.h>\n-#include <Storages/IStorage_fwd.h>\n-#include <Parsers/IAST_fwd.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n #include <Interpreters/Context_fwd.h>\n-\n #include <Poco/Event.h>\n+\n #include <atomic>\n-#include <chrono>\n-#include <condition_variable>\n #include <list>\n #include <mutex>\n #include <shared_mutex>\n-#include <thread>\n #include <unordered_set>\n \n+\n namespace zkutil\n {\n     class ZooKeeper;\n@@ -52,8 +52,16 @@ class AccessRightsElements;\n class DDLWorker\n {\n public:\n-    DDLWorker(int pool_size_, const std::string & zk_root_dir, ContextPtr context_, const Poco::Util::AbstractConfiguration * config, const String & prefix,\n-              const String & logger_name = \"DDLWorker\", const CurrentMetrics::Metric * max_entry_metric_ = nullptr, const CurrentMetrics::Metric * max_pushed_entry_metric_ = nullptr);\n+    DDLWorker(\n+        int pool_size_,\n+        const std::string & zk_queue_dir,\n+        const std::string & zk_replicas_dir,\n+        ContextPtr context_,\n+        const Poco::Util::AbstractConfiguration * config,\n+        const String & prefix,\n+        const String & logger_name = \"DDLWorker\",\n+        const CurrentMetrics::Metric * max_entry_metric_ = nullptr,\n+        const CurrentMetrics::Metric * max_pushed_entry_metric_ = nullptr);\n     virtual ~DDLWorker();\n \n     /// Pushes query into DDL queue, returns path to created node\n@@ -71,6 +79,8 @@ class DDLWorker\n         return queue_dir;\n     }\n \n+    std::string getReplicasDir() const { return replicas_dir; }\n+\n     void startup();\n     virtual void shutdown();\n \n@@ -149,6 +159,10 @@ class DDLWorker\n \n     /// Return false if the worker was stopped (stop_flag = true)\n     virtual bool initializeMainThread();\n+    virtual void initializeReplication();\n+\n+    virtual void createReplicaDirs(const ZooKeeperPtr & zookeeper, const NameSet & host_ids);\n+    virtual void markReplicasActive(bool reinitialized);\n \n     void runMainThread();\n     void runCleanupThread();\n@@ -160,7 +174,8 @@ class DDLWorker\n \n     std::string host_fqdn;      /// current host domain name\n     std::string host_fqdn_id;   /// host_name:port\n-    std::string queue_dir;      /// dir with queue of queries\n+    std::string queue_dir; /// dir with queue of queries\n+    std::string replicas_dir;\n \n     mutable std::mutex zookeeper_mutex;\n     ZooKeeperPtr current_zookeeper TSA_GUARDED_BY(zookeeper_mutex);\n@@ -202,6 +217,8 @@ class DDLWorker\n \n     const CurrentMetrics::Metric * max_entry_metric;\n     const CurrentMetrics::Metric * max_pushed_entry_metric;\n+\n+    std::unordered_map<String, std::pair<ZooKeeperPtr, zkutil::EphemeralNodeHolderPtr>> active_node_holders;\n };\n \n \ndiff --git a/src/Interpreters/DistributedQueryStatusSource.cpp b/src/Interpreters/DistributedQueryStatusSource.cpp\nnew file mode 100644\nindex 000000000000..83701d41c57e\n--- /dev/null\n+++ b/src/Interpreters/DistributedQueryStatusSource.cpp\n@@ -0,0 +1,270 @@\n+#include <Core/Block.h>\n+#include <Core/Settings.h>\n+#include <Core/SettingsEnums.h>\n+#include <DataTypes/DataTypeEnum.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/DistributedQueryStatusSource.h>\n+#include <Common/Exception.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n+\n+namespace DB\n+{\n+namespace Setting\n+{\n+extern const SettingsDistributedDDLOutputMode distributed_ddl_output_mode;\n+extern const SettingsInt64 distributed_ddl_task_timeout;\n+}\n+namespace ErrorCodes\n+{\n+extern const int UNFINISHED;\n+}\n+\n+DistributedQueryStatusSource::DistributedQueryStatusSource(\n+    const String & zk_node_path,\n+    const String & zk_replicas_path,\n+    Block block,\n+    ContextPtr context_,\n+    const Strings & hosts_to_wait,\n+    const char * logger_name)\n+    : ISource(block)\n+    , node_path(zk_node_path)\n+    , replicas_path(zk_replicas_path)\n+    , context(context_)\n+    , watch(CLOCK_MONOTONIC_COARSE)\n+    , log(getLogger(logger_name))\n+{\n+    auto output_mode = context->getSettingsRef()[Setting::distributed_ddl_output_mode];\n+    throw_on_timeout = output_mode == DistributedDDLOutputMode::THROW || output_mode == DistributedDDLOutputMode::NONE;\n+    throw_on_timeout_only_active\n+        = output_mode == DistributedDDLOutputMode::THROW_ONLY_ACTIVE || output_mode == DistributedDDLOutputMode::NONE_ONLY_ACTIVE;\n+\n+    waiting_hosts = NameSet(hosts_to_wait.begin(), hosts_to_wait.end());\n+\n+    only_running_hosts = output_mode == DistributedDDLOutputMode::THROW_ONLY_ACTIVE\n+        || output_mode == DistributedDDLOutputMode::NULL_STATUS_ON_TIMEOUT_ONLY_ACTIVE\n+        || output_mode == DistributedDDLOutputMode::NONE_ONLY_ACTIVE;\n+\n+    addTotalRowsApprox(waiting_hosts.size());\n+    timeout_seconds = context->getSettingsRef()[Setting::distributed_ddl_task_timeout];\n+}\n+\n+\n+IProcessor::Status DistributedQueryStatusSource::prepare()\n+{\n+    /// This method is overloaded to throw exception after all data is read.\n+    /// Exception is pushed into pipe (instead of simply being thrown) to ensure the order of data processing and exception.\n+\n+    if (finished)\n+    {\n+        if (first_exception)\n+        {\n+            if (!output.canPush())\n+                return Status::PortFull;\n+\n+            output.pushException(std::make_exception_ptr(*first_exception));\n+        }\n+\n+        output.finish();\n+        return Status::Finished;\n+    }\n+    else\n+        return ISource::prepare();\n+}\n+\n+NameSet DistributedQueryStatusSource::getOfflineHosts(const NameSet & hosts_to_wait, const ZooKeeperPtr & zookeeper)\n+{\n+    Strings paths;\n+    Strings hosts_array;\n+    for (const auto & host : hosts_to_wait)\n+    {\n+        hosts_array.push_back(host);\n+        paths.push_back(fs::path(replicas_path) / host / \"active\");\n+    }\n+\n+    NameSet offline;\n+    auto res = zookeeper->tryGet(paths);\n+    for (size_t i = 0; i < res.size(); ++i)\n+        if (res[i].error == Coordination::Error::ZNONODE)\n+            offline.insert(hosts_array[i]);\n+\n+    if (offline.size() == hosts_to_wait.size())\n+    {\n+        /// Avoid reporting that all hosts are offline\n+        LOG_WARNING(log, \"Did not find active hosts, will wait for all {} hosts. This should not happen often\", offline.size());\n+        return {};\n+    }\n+\n+    return offline;\n+}\n+\n+Strings DistributedQueryStatusSource::getNewAndUpdate(const Strings & current_finished_hosts)\n+{\n+    Strings diff;\n+    for (const String & host : current_finished_hosts)\n+    {\n+        if (!waiting_hosts.contains(host))\n+        {\n+            if (!ignoring_hosts.contains(host))\n+            {\n+                ignoring_hosts.emplace(host);\n+                LOG_INFO(log, \"Unexpected host {} appeared in task {}\", host, node_path);\n+            }\n+            continue;\n+        }\n+\n+        if (!finished_hosts.contains(host))\n+        {\n+            diff.emplace_back(host);\n+            finished_hosts.emplace(host);\n+        }\n+    }\n+\n+    return diff;\n+}\n+\n+\n+ExecutionStatus DistributedQueryStatusSource::getExecutionStatus(const fs::path & status_path)\n+{\n+    ExecutionStatus status(-1, \"Cannot obtain error message\");\n+\n+    String status_data;\n+    bool finished_exists = false;\n+\n+    auto retries_ctl = ZooKeeperRetriesControl(\n+        \"executeDDLQueryOnCluster\", getLogger(\"DDLQueryStatusSource\"), getRetriesInfo(), context->getProcessListElement());\n+    retries_ctl.retryLoop([&]() { finished_exists = context->getZooKeeper()->tryGet(status_path, status_data); });\n+    if (finished_exists)\n+        status.tryDeserializeText(status_data);\n+\n+    return status;\n+}\n+\n+ZooKeeperRetriesInfo DistributedQueryStatusSource::getRetriesInfo()\n+{\n+    const auto & config_ref = Context::getGlobalContextInstance()->getConfigRef();\n+    return ZooKeeperRetriesInfo(\n+        config_ref.getInt(\"distributed_ddl_keeper_max_retries\", 5),\n+        config_ref.getInt(\"distributed_ddl_keeper_initial_backoff_ms\", 100),\n+        config_ref.getInt(\"distributed_ddl_keeper_max_backoff_ms\", 5000));\n+}\n+\n+std::pair<String, UInt16> DistributedQueryStatusSource::parseHostAndPort(const String & host_id)\n+{\n+    String host = host_id;\n+    UInt16 port = 0;\n+    auto host_and_port = Cluster::Address::fromString(host_id);\n+    host = host_and_port.first;\n+    port = host_and_port.second;\n+    return {host, port};\n+}\n+\n+Chunk DistributedQueryStatusSource::generate()\n+{\n+    bool all_hosts_finished = num_hosts_finished >= waiting_hosts.size();\n+\n+    /// Seems like num_hosts_finished cannot be strictly greater than waiting_hosts.size()\n+    assert(num_hosts_finished <= waiting_hosts.size());\n+\n+    if (all_hosts_finished || timeout_exceeded)\n+        return {};\n+\n+    size_t try_number = 0;\n+    while (true)\n+    {\n+        if (isCancelled())\n+            return {};\n+\n+        if (stop_waiting_offline_hosts)\n+        {\n+            return stopWaitingOfflineHosts();\n+        }\n+\n+        if ((timeout_seconds >= 0 && watch.elapsedSeconds() > timeout_seconds))\n+        {\n+            return handleTimeoutExceeded();\n+        }\n+\n+        sleepForMilliseconds(std::min<size_t>(1000, 50 * try_number));\n+\n+        bool node_exists = false;\n+        Strings tmp_hosts;\n+        Strings tmp_active_hosts;\n+\n+        {\n+            auto retries_ctl = ZooKeeperRetriesControl(\n+                \"executeDistributedQueryOnCluster\", getLogger(getName()), getRetriesInfo(), context->getProcessListElement());\n+            retries_ctl.retryLoop(\n+                [&]()\n+                {\n+                    auto zookeeper = context->getZooKeeper();\n+                    Strings paths = getNodesToWait();\n+                    auto res = zookeeper->tryGetChildren(paths);\n+                    for (size_t i = 0; i < res.size(); ++i)\n+                        if (res[i].error != Coordination::Error::ZOK && res[i].error != Coordination::Error::ZNONODE)\n+                            throw Coordination::Exception::fromPath(res[i].error, paths[i]);\n+\n+                    if (res[0].error == Coordination::Error::ZNONODE)\n+                        node_exists = zookeeper->exists(node_path);\n+                    else\n+                        node_exists = true;\n+                    tmp_hosts = res[0].names;\n+                    tmp_active_hosts = res[1].names;\n+\n+                    if (only_running_hosts)\n+                        offline_hosts = getOfflineHosts(waiting_hosts, zookeeper);\n+                });\n+        }\n+\n+        if (!node_exists)\n+        {\n+            /// Paradoxically, this exception will be throw even in case of \"never_throw\" mode.\n+\n+            if (!first_exception)\n+                first_exception = std::make_unique<Exception>(Exception(\n+                    ErrorCodes::UNFINISHED,\n+                    \"Cannot provide query execution status. The query's node {} has been deleted by the cleaner\"\n+                    \" since it was finished (or its lifetime is expired)\",\n+                    node_path));\n+            return {};\n+        }\n+\n+        Strings new_hosts = getNewAndUpdate(tmp_hosts);\n+        ++try_number;\n+\n+        if (only_running_hosts)\n+        {\n+            size_t num_finished_or_offline = 0;\n+            for (const auto & host : waiting_hosts)\n+                num_finished_or_offline += finished_hosts.contains(host) || offline_hosts.contains(host);\n+\n+            if (num_finished_or_offline == waiting_hosts.size())\n+                stop_waiting_offline_hosts = true;\n+        }\n+\n+        if (new_hosts.empty())\n+            continue;\n+\n+        current_active_hosts = std::move(tmp_active_hosts);\n+\n+        MutableColumns columns = output.getHeader().cloneEmptyColumns();\n+        for (const String & host_id : new_hosts)\n+        {\n+            ExecutionStatus status = checkStatus(host_id);\n+\n+            if (status.code != 0)\n+            {\n+                handleNonZeroStatusCode(status, host_id);\n+            }\n+\n+            ++num_hosts_finished;\n+            fillHostStatus(host_id, status, columns);\n+        }\n+\n+        return Chunk(std::move(columns), new_hosts.size());\n+    }\n+}\n+\n+}\ndiff --git a/src/Interpreters/DistributedQueryStatusSource.h b/src/Interpreters/DistributedQueryStatusSource.h\nnew file mode 100644\nindex 000000000000..4f58085a1f08\n--- /dev/null\n+++ b/src/Interpreters/DistributedQueryStatusSource.h\n@@ -0,0 +1,68 @@\n+#pragma once\n+\n+#include <filesystem>\n+#include <Interpreters/Context_fwd.h>\n+#include <Interpreters/DDLTask.h>\n+#include <Processors/ISource.h>\n+#include <Common/ZooKeeper/ZooKeeperRetries.h>\n+\n+namespace fs = std::filesystem;\n+\n+namespace DB\n+{\n+class DistributedQueryStatusSource : public ISource\n+{\n+public:\n+    DistributedQueryStatusSource(\n+        const String & zk_node_path,\n+        const String & zk_replicas_path,\n+        Block block,\n+        ContextPtr context_,\n+        const Strings & hosts_to_wait,\n+        const char * logger_name);\n+\n+    Chunk generate() override;\n+    Status prepare() override;\n+\n+protected:\n+    virtual ExecutionStatus checkStatus(const String & host_id) = 0;\n+    virtual Chunk generateChunkWithUnfinishedHosts() const = 0;\n+    virtual Strings getNodesToWait() = 0;\n+    virtual Chunk handleTimeoutExceeded() = 0;\n+    virtual Chunk stopWaitingOfflineHosts() = 0;\n+    virtual void handleNonZeroStatusCode(const ExecutionStatus & status, const String & host_id) = 0;\n+    virtual void fillHostStatus(const String & host_id, const ExecutionStatus & status, MutableColumns & columns) = 0;\n+\n+    virtual NameSet getOfflineHosts(const NameSet & hosts_to_wait, const ZooKeeperPtr & zookeeper);\n+\n+    Strings getNewAndUpdate(const Strings & current_finished_hosts);\n+    ExecutionStatus getExecutionStatus(const fs::path & status_path);\n+\n+    static ZooKeeperRetriesInfo getRetriesInfo();\n+    static std::pair<String, UInt16> parseHostAndPort(const String & host_id);\n+\n+    String node_path;\n+    String replicas_path;\n+    ContextPtr context;\n+    Stopwatch watch;\n+    LoggerPtr log;\n+\n+    NameSet waiting_hosts; /// hosts from task host list\n+    NameSet finished_hosts; /// finished hosts from host list\n+    NameSet ignoring_hosts; /// appeared hosts that are not in hosts list\n+    Strings current_active_hosts; /// Hosts that are currently executing the task\n+    NameSet offline_hosts; /// Hosts that are not currently running\n+    size_t num_hosts_finished = 0;\n+\n+    /// Save the first detected error and throw it at the end of execution\n+    std::unique_ptr<Exception> first_exception;\n+\n+    Int64 timeout_seconds = 120;\n+    bool throw_on_timeout = true;\n+    bool throw_on_timeout_only_active = false;\n+    bool only_running_hosts = false;\n+\n+    bool timeout_exceeded = false;\n+    bool stop_waiting_offline_hosts = false;\n+};\n+}\ndiff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp\nindex 22bba01a60fc..a38a7ab45d1f 100644\n--- a/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -1987,6 +1987,12 @@ BlockIO InterpreterCreateQuery::doCreateOrReplaceTable(ASTCreateQuery & create,\n             UInt16 hashed_zk_path = sipHash64(txn->getTaskZooKeeperPath());\n             random_suffix = getHexUIntLowercase(hashed_zk_path);\n         }\n+        else if (!current_context->getCurrentQueryId().empty())\n+        {\n+            random_suffix = getRandomASCIIString(/*length=*/2);\n+            UInt8 hashed_query_id = sipHash64(current_context->getCurrentQueryId());\n+            random_suffix += getHexUIntLowercase(hashed_query_id);\n+        }\n         else\n         {\n             random_suffix = getRandomASCIIString(/*length=*/4);\ndiff --git a/src/Interpreters/ReplicatedDatabaseQueryStatusSource.cpp b/src/Interpreters/ReplicatedDatabaseQueryStatusSource.cpp\nnew file mode 100644\nindex 000000000000..09941b092380\n--- /dev/null\n+++ b/src/Interpreters/ReplicatedDatabaseQueryStatusSource.cpp\n@@ -0,0 +1,170 @@\n+#include <Core/Settings.h>\n+#include <DataTypes/DataTypeEnum.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Databases/DatabaseReplicated.h>\n+#include <Interpreters/ReplicatedDatabaseQueryStatusSource.h>\n+\n+namespace DB\n+{\n+namespace Setting\n+{\n+extern const SettingsBool database_replicated_enforce_synchronous_settings;\n+extern const SettingsDistributedDDLOutputMode distributed_ddl_output_mode;\n+}\n+namespace ErrorCodes\n+{\n+extern const int TIMEOUT_EXCEEDED;\n+extern const int LOGICAL_ERROR;\n+}\n+\n+ReplicatedDatabaseQueryStatusSource::ReplicatedDatabaseQueryStatusSource(\n+    const String & zk_node_path, const String & zk_replicas_path, ContextPtr context_, const Strings & hosts_to_wait)\n+    : DistributedQueryStatusSource(\n+          zk_node_path, zk_replicas_path, getSampleBlock(), context_, hosts_to_wait, \"ReplicatedDatabaseQueryStatusSource\")\n+{\n+}\n+\n+ExecutionStatus ReplicatedDatabaseQueryStatusSource::checkStatus([[maybe_unused]] const String & host_id)\n+{\n+    /// Replicated database retries in case of error, it should not write error status.\n+#ifdef DEBUG_OR_SANITIZER_BUILD\n+    fs::path status_path = fs::path(node_path) / \"finished\" / host_id;\n+    return getExecutionStatus(status_path);\n+#else\n+    return ExecutionStatus{0};\n+#endif\n+}\n+\n+Chunk ReplicatedDatabaseQueryStatusSource::generateChunkWithUnfinishedHosts() const\n+{\n+    NameSet unfinished_hosts = waiting_hosts;\n+    for (const auto & host_id : finished_hosts)\n+        unfinished_hosts.erase(host_id);\n+\n+    NameSet active_hosts_set = NameSet{current_active_hosts.begin(), current_active_hosts.end()};\n+\n+    /// Query is not finished on the rest hosts, so fill the corresponding rows with NULLs.\n+    MutableColumns columns = output.getHeader().cloneEmptyColumns();\n+    for (const String & host_id : unfinished_hosts)\n+    {\n+        size_t num = 0;\n+        auto [shard, replica] = DatabaseReplicated::parseFullReplicaName(host_id);\n+        columns[num++]->insert(shard);\n+        columns[num++]->insert(replica);\n+        if (active_hosts_set.contains(host_id))\n+            columns[num++]->insert(IN_PROGRESS);\n+        else\n+            columns[num++]->insert(QUEUED);\n+\n+        columns[num++]->insert(unfinished_hosts.size());\n+        columns[num++]->insert(current_active_hosts.size());\n+    }\n+    return Chunk(std::move(columns), unfinished_hosts.size());\n+}\n+\n+Strings ReplicatedDatabaseQueryStatusSource::getNodesToWait()\n+{\n+    String node_to_wait = \"finished\";\n+    if (context->getSettingsRef()[Setting::database_replicated_enforce_synchronous_settings])\n+    {\n+        node_to_wait = \"synced\";\n+    }\n+\n+    return {String(fs::path(node_path) / node_to_wait), String(fs::path(node_path) / \"active\")};\n+}\n+\n+Chunk ReplicatedDatabaseQueryStatusSource::handleTimeoutExceeded()\n+{\n+    timeout_exceeded = true;\n+\n+    size_t num_unfinished_hosts = waiting_hosts.size() - num_hosts_finished;\n+    size_t num_active_hosts = current_active_hosts.size();\n+\n+    constexpr auto msg_format = \"ReplicatedDatabase DDL task {} is not finished on {} of {} hosts \"\n+                                \"({} of them are currently executing the task, {} are inactive). \"\n+                                \"They are going to execute the query in background. Was waiting for {} seconds{}\";\n+\n+    if (throw_on_timeout || (throw_on_timeout_only_active && !stop_waiting_offline_hosts))\n+    {\n+        if (!first_exception)\n+            first_exception = std::make_unique<Exception>(Exception(\n+                ErrorCodes::TIMEOUT_EXCEEDED,\n+                msg_format,\n+                node_path,\n+                num_unfinished_hosts,\n+                waiting_hosts.size(),\n+                num_active_hosts,\n+                offline_hosts.size(),\n+                watch.elapsedSeconds(),\n+                stop_waiting_offline_hosts ? \"\" : \", which is longer than distributed_ddl_task_timeout\"));\n+\n+        /// For Replicated database print a list of unfinished hosts as well. Will return empty block on next iteration.\n+        return generateChunkWithUnfinishedHosts();\n+    }\n+\n+    LOG_INFO(\n+        log,\n+        msg_format,\n+        node_path,\n+        num_unfinished_hosts,\n+        waiting_hosts.size(),\n+        num_active_hosts,\n+        offline_hosts.size(),\n+        watch.elapsedSeconds(),\n+        stop_waiting_offline_hosts ? \"\" : \"which is longer than distributed_ddl_task_timeout\");\n+\n+    return generateChunkWithUnfinishedHosts();\n+}\n+\n+Chunk ReplicatedDatabaseQueryStatusSource::stopWaitingOfflineHosts()\n+{\n+    // Same logic as timeout exceeded\n+    return handleTimeoutExceeded();\n+}\n+\n+void ReplicatedDatabaseQueryStatusSource::handleNonZeroStatusCode(const ExecutionStatus & status, const String & host_id)\n+{\n+    assert(status.code != 0);\n+\n+    if (!first_exception && context->getSettingsRef()[Setting::distributed_ddl_output_mode] != DistributedDDLOutputMode::NEVER_THROW)\n+    {\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"There was an error on {}: {} (probably it's a bug)\", host_id, status.message);\n+    }\n+}\n+\n+void ReplicatedDatabaseQueryStatusSource::fillHostStatus(const String & host_id, const ExecutionStatus & status, MutableColumns & columns)\n+{\n+    size_t num = 0;\n+    if (status.code != 0)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"There was an error on {}: {} (probably it's a bug)\", host_id, status.message);\n+    auto [shard, replica] = DatabaseReplicated::parseFullReplicaName(host_id);\n+    columns[num++]->insert(shard);\n+    columns[num++]->insert(replica);\n+    columns[num++]->insert(OK);\n+    columns[num++]->insert(waiting_hosts.size() - num_hosts_finished);\n+    columns[num++]->insert(current_active_hosts.size());\n+}\n+\n+Block ReplicatedDatabaseQueryStatusSource::getSampleBlock()\n+{\n+    auto get_status_enum = []()\n+    {\n+        return std::make_shared<DataTypeEnum8>(DataTypeEnum8::Values{\n+            {\"OK\", static_cast<Int8>(OK)},\n+            {\"IN_PROGRESS\", static_cast<Int8>(IN_PROGRESS)},\n+            {\"QUEUED\", static_cast<Int8>(QUEUED)},\n+        });\n+    };\n+\n+    return Block{\n+        {std::make_shared<DataTypeString>(), \"shard\"},\n+        {std::make_shared<DataTypeString>(), \"replica\"},\n+        {get_status_enum(), \"status\"},\n+        {std::make_shared<DataTypeUInt64>(), \"num_hosts_remaining\"},\n+        {std::make_shared<DataTypeUInt64>(), \"num_hosts_active\"},\n+    };\n+}\n+\n+}\ndiff --git a/src/Interpreters/ReplicatedDatabaseQueryStatusSource.h b/src/Interpreters/ReplicatedDatabaseQueryStatusSource.h\nnew file mode 100644\nindex 000000000000..76a2d5f3f14f\n--- /dev/null\n+++ b/src/Interpreters/ReplicatedDatabaseQueryStatusSource.h\n@@ -0,0 +1,40 @@\n+#pragma once\n+\n+#include <Interpreters/Context_fwd.h>\n+#include <Interpreters/DDLTask.h>\n+#include <Interpreters/DistributedQueryStatusSource.h>\n+#include <Common/ZooKeeper/ZooKeeperRetries.h>\n+\n+namespace DB\n+{\n+class ReplicatedDatabaseQueryStatusSource final : public DistributedQueryStatusSource\n+{\n+public:\n+    ReplicatedDatabaseQueryStatusSource(\n+        const String & zk_node_path, const String & zk_replicas_path, ContextPtr context_, const Strings & hosts_to_wait);\n+\n+    String getName() const override { return \"ReplicatedDatabaseQueryStatus\"; }\n+\n+protected:\n+    ExecutionStatus checkStatus(const String & host_id) override;\n+    Chunk generateChunkWithUnfinishedHosts() const override;\n+    Strings getNodesToWait() override;\n+    Chunk handleTimeoutExceeded() override;\n+    Chunk stopWaitingOfflineHosts() override;\n+    void handleNonZeroStatusCode(const ExecutionStatus & status, const String & host_id) override;\n+    void fillHostStatus(const String & host_id, const ExecutionStatus & status, MutableColumns & columns) override;\n+\n+private:\n+    static Block getSampleBlock();\n+\n+    enum ReplicatedDatabaseQueryStatus\n+    {\n+        /// Query is (successfully) finished\n+        OK = 0,\n+        /// Query is not finished yet, but replica is currently executing it\n+        IN_PROGRESS = 1,\n+        /// Replica is not available or busy with previous queries. It will process query asynchronously\n+        QUEUED = 2,\n+    };\n+};\n+}\ndiff --git a/src/Interpreters/executeDDLQueryOnCluster.cpp b/src/Interpreters/executeDDLQueryOnCluster.cpp\nindex c5d58a873fb0..c0440c755adc 100644\n--- a/src/Interpreters/executeDDLQueryOnCluster.cpp\n+++ b/src/Interpreters/executeDDLQueryOnCluster.cpp\n@@ -1,33 +1,32 @@\n-#include <Interpreters/executeDDLQueryOnCluster.h>\n-#include <Interpreters/DatabaseCatalog.h>\n-#include <Interpreters/DDLWorker.h>\n-#include <Interpreters/DDLTask.h>\n+#include <filesystem>\n+#include <Access/Common/AccessRightsElement.h>\n+#include <Access/ContextAccess.h>\n+#include <Core/Settings.h>\n+#include <DataTypes/DataTypeEnum.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Databases/DatabaseReplicated.h>\n #include <Interpreters/AddDefaultDatabaseVisitor.h>\n #include <Interpreters/Context.h>\n-#include <Parsers/ASTQueryWithOutput.h>\n-#include <Parsers/ASTQueryWithOnCluster.h>\n+#include <Interpreters/DDLOnClusterQueryStatusSource.h>\n+#include <Interpreters/DDLTask.h>\n+#include <Interpreters/DDLWorker.h>\n+#include <Interpreters/DatabaseCatalog.h>\n+#include <Interpreters/executeDDLQueryOnCluster.h>\n #include <Parsers/ASTAlterQuery.h>\n #include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTQueryWithOnCluster.h>\n+#include <Parsers/ASTQueryWithOutput.h>\n+#include <Parsers/ASTSystemQuery.h>\n #include <Parsers/queryToString.h>\n-#include <Access/Common/AccessRightsElement.h>\n-#include <Access/ContextAccess.h>\n-#include <Core/Settings.h>\n-#include <Common/Macros.h>\n-#include <Common/ZooKeeper/ZooKeeper.h>\n-#include \"Parsers/ASTSystemQuery.h\"\n-#include <Databases/DatabaseReplicated.h>\n-#include <DataTypes/DataTypesNumber.h>\n-#include <DataTypes/DataTypeString.h>\n-#include <DataTypes/DataTypeNullable.h>\n-#include <DataTypes/DataTypeEnum.h>\n #include <Processors/Sinks/EmptySink.h>\n #include <QueryPipeline/Pipe.h>\n-#include <filesystem>\n #include <base/sort.h>\n+#include <Common/Macros.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n \n \n-namespace fs = std::filesystem;\n-\n namespace DB\n {\n namespace Setting\n@@ -41,21 +40,11 @@ namespace Setting\n \n namespace ErrorCodes\n {\n-    extern const int NOT_IMPLEMENTED;\n-    extern const int TIMEOUT_EXCEEDED;\n-    extern const int UNFINISHED;\n-    extern const int QUERY_IS_PROHIBITED;\n-    extern const int LOGICAL_ERROR;\n+extern const int NOT_IMPLEMENTED;\n+extern const int QUERY_IS_PROHIBITED;\n+extern const int LOGICAL_ERROR;\n }\n \n-static ZooKeeperRetriesInfo getRetriesInfo()\n-{\n-    const auto & config_ref = Context::getGlobalContextInstance()->getConfigRef();\n-    return ZooKeeperRetriesInfo(\n-        config_ref.getInt(\"distributed_ddl_keeper_max_retries\", 5),\n-        config_ref.getInt(\"distributed_ddl_keeper_initial_backoff_ms\", 100),\n-        config_ref.getInt(\"distributed_ddl_keeper_max_backoff_ms\", 5000));\n-}\n \n bool isSupportedAlterTypeForOnClusterDDLQuery(int type)\n {\n@@ -202,72 +191,19 @@ BlockIO executeDDLQueryOnCluster(const ASTPtr & query_ptr_, ContextPtr context,\n     entry.initial_query_id = context->getClientInfo().initial_query_id;\n     String node_path = ddl_worker.enqueueQuery(entry);\n \n-    return getDistributedDDLStatus(node_path, entry, context, /* hosts_to_wait */ nullptr);\n+    return getDDLOnClusterStatus(node_path, ddl_worker.getReplicasDir(), entry, context);\n }\n \n-\n-class DDLQueryStatusSource final : public ISource\n-{\n-public:\n-    DDLQueryStatusSource(\n-        const String & zk_node_path, const DDLLogEntry & entry, ContextPtr context_, const Strings * hosts_to_wait);\n-\n-    String getName() const override { return \"DDLQueryStatus\"; }\n-    Chunk generate() override;\n-    Status prepare() override;\n-\n-private:\n-    static Block getSampleBlock(ContextPtr context_, bool hosts_to_wait);\n-\n-    Strings getNewAndUpdate(const Strings & current_list_of_finished_hosts);\n-\n-    std::pair<String, UInt16> parseHostAndPort(const String & host_id) const;\n-\n-    Chunk generateChunkWithUnfinishedHosts() const;\n-\n-    enum ReplicatedDatabaseQueryStatus\n-    {\n-        /// Query is (successfully) finished\n-        OK = 0,\n-        /// Query is not finished yet, but replica is currently executing it\n-        IN_PROGRESS = 1,\n-        /// Replica is not available or busy with previous queries. It will process query asynchronously\n-        QUEUED = 2,\n-    };\n-\n-    String node_path;\n-    ContextPtr context;\n-    Stopwatch watch;\n-    LoggerPtr log;\n-\n-    NameSet waiting_hosts;  /// hosts from task host list\n-    NameSet finished_hosts; /// finished hosts from host list\n-    NameSet ignoring_hosts; /// appeared hosts that are not in hosts list\n-    Strings current_active_hosts; /// Hosts that are currently executing the task\n-    NameSet offline_hosts;  /// Hosts that are not currently running\n-    size_t num_hosts_finished = 0;\n-\n-    /// Save the first detected error and throw it at the end of execution\n-    std::unique_ptr<Exception> first_exception;\n-\n-    Int64 timeout_seconds = 120;\n-    bool is_replicated_database = false;\n-    bool throw_on_timeout = true;\n-    bool throw_on_timeout_only_active = false;\n-    bool only_running_hosts = false;\n-\n-    bool timeout_exceeded = false;\n-    bool stop_waiting_offline_hosts = false;\n-};\n-\n-\n-BlockIO getDistributedDDLStatus(const String & node_path, const DDLLogEntry & entry, ContextPtr context, const Strings * hosts_to_wait)\n+BlockIO getDDLOnClusterStatus(const String & node_path, const String & replicas_path, const DDLLogEntry & entry, ContextPtr context)\n {\n     BlockIO io;\n     if (context->getSettingsRef()[Setting::distributed_ddl_task_timeout] == 0)\n         return io;\n+    Strings hosts_to_wait;\n+    for (const HostID & host : entry.hosts)\n+        hosts_to_wait.push_back(host.toString());\n \n-    auto source = std::make_shared<DDLQueryStatusSource>(node_path, entry, context, hosts_to_wait);\n+    auto source = std::make_shared<DDLOnClusterQueryStatusSource>(node_path, replicas_path, context, hosts_to_wait);\n     io.pipeline = QueryPipeline(std::move(source));\n \n     if (context->getSettingsRef()[Setting::distributed_ddl_output_mode] == DistributedDDLOutputMode::NONE\n@@ -277,394 +213,6 @@ BlockIO getDistributedDDLStatus(const String & node_path, const DDLLogEntry & en\n     return io;\n }\n \n-Block DDLQueryStatusSource::getSampleBlock(ContextPtr context_, bool hosts_to_wait)\n-{\n-    auto output_mode = context_->getSettingsRef()[Setting::distributed_ddl_output_mode];\n-\n-    auto maybe_make_nullable = [&](const DataTypePtr & type) -> DataTypePtr\n-    {\n-        if (output_mode == DistributedDDLOutputMode::THROW ||\n-            output_mode == DistributedDDLOutputMode::NONE ||\n-            output_mode == DistributedDDLOutputMode::NONE_ONLY_ACTIVE)\n-            return type;\n-        return std::make_shared<DataTypeNullable>(type);\n-    };\n-\n-    auto get_status_enum = []()\n-    {\n-        return std::make_shared<DataTypeEnum8>(\n-            DataTypeEnum8::Values\n-            {\n-                {\"OK\",              static_cast<Int8>(OK)},\n-                {\"IN_PROGRESS\",     static_cast<Int8>(IN_PROGRESS)},\n-                {\"QUEUED\",          static_cast<Int8>(QUEUED)},\n-            });\n-    };\n-\n-    if (hosts_to_wait)\n-    {\n-        return Block{\n-            {std::make_shared<DataTypeString>(), \"shard\"},\n-            {std::make_shared<DataTypeString>(), \"replica\"},\n-            {get_status_enum(), \"status\"},\n-            {std::make_shared<DataTypeUInt64>(), \"num_hosts_remaining\"},\n-            {std::make_shared<DataTypeUInt64>(), \"num_hosts_active\"},\n-        };\n-    }\n-\n-    return Block{\n-        {std::make_shared<DataTypeString>(), \"host\"},\n-        {std::make_shared<DataTypeUInt16>(), \"port\"},\n-        {maybe_make_nullable(std::make_shared<DataTypeInt64>()), \"status\"},\n-        {maybe_make_nullable(std::make_shared<DataTypeString>()), \"error\"},\n-        {std::make_shared<DataTypeUInt64>(), \"num_hosts_remaining\"},\n-        {std::make_shared<DataTypeUInt64>(), \"num_hosts_active\"},\n-    };\n-}\n-\n-DDLQueryStatusSource::DDLQueryStatusSource(\n-    const String & zk_node_path, const DDLLogEntry & entry, ContextPtr context_, const Strings * hosts_to_wait)\n-    : ISource(getSampleBlock(context_, static_cast<bool>(hosts_to_wait)))\n-    , node_path(zk_node_path)\n-    , context(context_)\n-    , watch(CLOCK_MONOTONIC_COARSE)\n-    , log(getLogger(\"DDLQueryStatusSource\"))\n-{\n-    auto output_mode = context->getSettingsRef()[Setting::distributed_ddl_output_mode];\n-    throw_on_timeout = output_mode == DistributedDDLOutputMode::THROW || output_mode == DistributedDDLOutputMode::NONE;\n-    throw_on_timeout_only_active = output_mode == DistributedDDLOutputMode::THROW_ONLY_ACTIVE || output_mode == DistributedDDLOutputMode::NONE_ONLY_ACTIVE;\n-\n-    if (hosts_to_wait)\n-    {\n-        waiting_hosts = NameSet(hosts_to_wait->begin(), hosts_to_wait->end());\n-        is_replicated_database = true;\n-        only_running_hosts = output_mode == DistributedDDLOutputMode::THROW_ONLY_ACTIVE ||\n-                             output_mode == DistributedDDLOutputMode::NULL_STATUS_ON_TIMEOUT_ONLY_ACTIVE ||\n-                             output_mode == DistributedDDLOutputMode::NONE_ONLY_ACTIVE;\n-    }\n-    else\n-    {\n-        for (const HostID & host : entry.hosts)\n-            waiting_hosts.emplace(host.toString());\n-    }\n-\n-    addTotalRowsApprox(waiting_hosts.size());\n-    timeout_seconds = context->getSettingsRef()[Setting::distributed_ddl_task_timeout];\n-}\n-\n-std::pair<String, UInt16> DDLQueryStatusSource::parseHostAndPort(const String & host_id) const\n-{\n-    String host = host_id;\n-    UInt16 port = 0;\n-    if (!is_replicated_database)\n-    {\n-        auto host_and_port = Cluster::Address::fromString(host_id);\n-        host = host_and_port.first;\n-        port = host_and_port.second;\n-    }\n-    return {host, port};\n-}\n-\n-Chunk DDLQueryStatusSource::generateChunkWithUnfinishedHosts() const\n-{\n-    NameSet unfinished_hosts = waiting_hosts;\n-    for (const auto & host_id : finished_hosts)\n-        unfinished_hosts.erase(host_id);\n-\n-    NameSet active_hosts_set = NameSet{current_active_hosts.begin(), current_active_hosts.end()};\n-\n-    /// Query is not finished on the rest hosts, so fill the corresponding rows with NULLs.\n-    MutableColumns columns = output.getHeader().cloneEmptyColumns();\n-    for (const String & host_id : unfinished_hosts)\n-    {\n-        size_t num = 0;\n-        if (is_replicated_database)\n-        {\n-            auto [shard, replica] = DatabaseReplicated::parseFullReplicaName(host_id);\n-            columns[num++]->insert(shard);\n-            columns[num++]->insert(replica);\n-            if (active_hosts_set.contains(host_id))\n-                columns[num++]->insert(IN_PROGRESS);\n-            else\n-                columns[num++]->insert(QUEUED);\n-        }\n-        else\n-        {\n-            auto [host, port] = parseHostAndPort(host_id);\n-            columns[num++]->insert(host);\n-            columns[num++]->insert(port);\n-            columns[num++]->insert(Field{});\n-            columns[num++]->insert(Field{});\n-        }\n-        columns[num++]->insert(unfinished_hosts.size());\n-        columns[num++]->insert(current_active_hosts.size());\n-    }\n-    return Chunk(std::move(columns), unfinished_hosts.size());\n-}\n-\n-static NameSet getOfflineHosts(const String & node_path, const NameSet & hosts_to_wait, const ZooKeeperPtr & zookeeper, LoggerPtr log)\n-{\n-    fs::path replicas_path;\n-    if (node_path.ends_with('/'))\n-        replicas_path = fs::path(node_path).parent_path().parent_path().parent_path() / \"replicas\";\n-    else\n-        replicas_path = fs::path(node_path).parent_path().parent_path() / \"replicas\";\n-\n-    Strings paths;\n-    Strings hosts_array;\n-    for (const auto & host : hosts_to_wait)\n-    {\n-        hosts_array.push_back(host);\n-        paths.push_back(replicas_path / host / \"active\");\n-    }\n-\n-    NameSet offline;\n-    auto res = zookeeper->tryGet(paths);\n-    for (size_t i = 0; i < res.size(); ++i)\n-        if (res[i].error == Coordination::Error::ZNONODE)\n-            offline.insert(hosts_array[i]);\n-\n-    if (offline.size() == hosts_to_wait.size())\n-    {\n-        /// Avoid reporting that all hosts are offline\n-        LOG_WARNING(log, \"Did not find active hosts, will wait for all {} hosts. This should not happen often\", offline.size());\n-        return {};\n-    }\n-\n-    return offline;\n-}\n-\n-Chunk DDLQueryStatusSource::generate()\n-{\n-    bool all_hosts_finished = num_hosts_finished >= waiting_hosts.size();\n-\n-    /// Seems like num_hosts_finished cannot be strictly greater than waiting_hosts.size()\n-    assert(num_hosts_finished <= waiting_hosts.size());\n-\n-    if (all_hosts_finished || timeout_exceeded)\n-        return {};\n-\n-    String node_to_wait = \"finished\";\n-    if (is_replicated_database && context->getSettingsRef()[Setting::database_replicated_enforce_synchronous_settings])\n-        node_to_wait = \"synced\";\n-\n-    size_t try_number = 0;\n-\n-    while (true)\n-    {\n-        if (isCancelled())\n-            return {};\n-\n-        if (stop_waiting_offline_hosts || (timeout_seconds >= 0 && watch.elapsedSeconds() > timeout_seconds))\n-        {\n-            timeout_exceeded = true;\n-\n-            size_t num_unfinished_hosts = waiting_hosts.size() - num_hosts_finished;\n-            size_t num_active_hosts = current_active_hosts.size();\n-\n-            constexpr auto msg_format = \"Distributed DDL task {} is not finished on {} of {} hosts \"\n-                                        \"({} of them are currently executing the task, {} are inactive). \"\n-                                        \"They are going to execute the query in background. Was waiting for {} seconds{}\";\n-\n-            if (throw_on_timeout || (throw_on_timeout_only_active && !stop_waiting_offline_hosts))\n-            {\n-                if (!first_exception)\n-                    first_exception = std::make_unique<Exception>(Exception(ErrorCodes::TIMEOUT_EXCEEDED,\n-                        msg_format, node_path, num_unfinished_hosts, waiting_hosts.size(), num_active_hosts, offline_hosts.size(),\n-                        watch.elapsedSeconds(), stop_waiting_offline_hosts ? \"\" : \", which is longer than distributed_ddl_task_timeout\"));\n-\n-                /// For Replicated database print a list of unfinished hosts as well. Will return empty block on next iteration.\n-                if (is_replicated_database)\n-                    return generateChunkWithUnfinishedHosts();\n-                return {};\n-            }\n-\n-            LOG_INFO(log, msg_format, node_path, num_unfinished_hosts, waiting_hosts.size(), num_active_hosts, offline_hosts.size(),\n-                     watch.elapsedSeconds(), stop_waiting_offline_hosts ? \"\" : \"which is longer than distributed_ddl_task_timeout\");\n-\n-            return generateChunkWithUnfinishedHosts();\n-        }\n-\n-        sleepForMilliseconds(std::min<size_t>(1000, 50 * try_number));\n-\n-        bool node_exists = false;\n-        Strings tmp_hosts;\n-        Strings tmp_active_hosts;\n-\n-        {\n-            auto retries_ctl = ZooKeeperRetriesControl(\n-                \"executeDDLQueryOnCluster\", getLogger(\"DDLQueryStatusSource\"), getRetriesInfo(), context->getProcessListElement());\n-            retries_ctl.retryLoop([&]()\n-            {\n-                auto zookeeper = context->getZooKeeper();\n-                Strings paths = {String(fs::path(node_path) / node_to_wait), String(fs::path(node_path) / \"active\")};\n-                auto res = zookeeper->tryGetChildren(paths);\n-                for (size_t i = 0; i < res.size(); ++i)\n-                    if (res[i].error != Coordination::Error::ZOK && res[i].error != Coordination::Error::ZNONODE)\n-                        throw Coordination::Exception::fromPath(res[i].error, paths[i]);\n-\n-                if (res[0].error == Coordination::Error::ZNONODE)\n-                    node_exists = zookeeper->exists(node_path);\n-                else\n-                    node_exists = true;\n-                tmp_hosts = res[0].names;\n-                tmp_active_hosts = res[1].names;\n-\n-                if (only_running_hosts)\n-                    offline_hosts = getOfflineHosts(node_path, waiting_hosts, zookeeper, log);\n-            });\n-        }\n-\n-        if (!node_exists)\n-        {\n-            /// Paradoxically, this exception will be throw even in case of \"never_throw\" mode.\n-\n-            if (!first_exception)\n-                first_exception = std::make_unique<Exception>(Exception(ErrorCodes::UNFINISHED,\n-                        \"Cannot provide query execution status. The query's node {} has been deleted by the cleaner\"\n-                        \" since it was finished (or its lifetime is expired)\",\n-                        node_path));\n-            return {};\n-        }\n-\n-        Strings new_hosts = getNewAndUpdate(tmp_hosts);\n-        ++try_number;\n-\n-        if (only_running_hosts)\n-        {\n-            size_t num_finished_or_offline = 0;\n-            for (const auto & host : waiting_hosts)\n-                num_finished_or_offline += finished_hosts.contains(host) || offline_hosts.contains(host);\n-\n-            if (num_finished_or_offline == waiting_hosts.size())\n-                stop_waiting_offline_hosts = true;\n-        }\n-\n-        if (new_hosts.empty())\n-            continue;\n-\n-        current_active_hosts = std::move(tmp_active_hosts);\n-\n-        MutableColumns columns = output.getHeader().cloneEmptyColumns();\n-        for (const String & host_id : new_hosts)\n-        {\n-            ExecutionStatus status(-1, \"Cannot obtain error message\");\n-\n-            /// Replicated database retries in case of error, it should not write error status.\n-#ifdef DEBUG_OR_SANITIZER_BUILD\n-            bool need_check_status = true;\n-#else\n-            bool need_check_status = !is_replicated_database;\n-#endif\n-            if (need_check_status)\n-            {\n-                String status_data;\n-                bool finished_exists = false;\n-\n-                auto retries_ctl = ZooKeeperRetriesControl(\n-                    \"executeDDLQueryOnCluster\",\n-                    getLogger(\"DDLQueryStatusSource\"),\n-                    getRetriesInfo(),\n-                    context->getProcessListElement());\n-                retries_ctl.retryLoop([&]()\n-                {\n-                    finished_exists = context->getZooKeeper()->tryGet(fs::path(node_path) / \"finished\" / host_id, status_data);\n-                });\n-                if (finished_exists)\n-                    status.tryDeserializeText(status_data);\n-            }\n-            else\n-            {\n-                status = ExecutionStatus{0};\n-            }\n-\n-\n-            if (status.code != 0 && !first_exception\n-                && context->getSettingsRef()[Setting::distributed_ddl_output_mode] != DistributedDDLOutputMode::NEVER_THROW)\n-            {\n-                if (is_replicated_database)\n-                    throw Exception(ErrorCodes::LOGICAL_ERROR, \"There was an error on {}: {} (probably it's a bug)\", host_id, status.message);\n-\n-                auto [host, port] = parseHostAndPort(host_id);\n-                first_exception = std::make_unique<Exception>(Exception(status.code,\n-                    \"There was an error on [{}:{}]: {}\", host, port, status.message));\n-            }\n-\n-            ++num_hosts_finished;\n-\n-            size_t num = 0;\n-            if (is_replicated_database)\n-            {\n-                if (status.code != 0)\n-                    throw Exception(ErrorCodes::LOGICAL_ERROR, \"There was an error on {}: {} (probably it's a bug)\", host_id, status.message);\n-                auto [shard, replica] = DatabaseReplicated::parseFullReplicaName(host_id);\n-                columns[num++]->insert(shard);\n-                columns[num++]->insert(replica);\n-                columns[num++]->insert(OK);\n-            }\n-            else\n-            {\n-                auto [host, port] = parseHostAndPort(host_id);\n-                columns[num++]->insert(host);\n-                columns[num++]->insert(port);\n-                columns[num++]->insert(status.code);\n-                columns[num++]->insert(status.message);\n-            }\n-            columns[num++]->insert(waiting_hosts.size() - num_hosts_finished);\n-            columns[num++]->insert(current_active_hosts.size());\n-        }\n-\n-        return Chunk(std::move(columns), new_hosts.size());\n-    }\n-}\n-\n-IProcessor::Status DDLQueryStatusSource::prepare()\n-{\n-    /// This method is overloaded to throw exception after all data is read.\n-    /// Exception is pushed into pipe (instead of simply being thrown) to ensure the order of data processing and exception.\n-\n-    if (finished)\n-    {\n-        if (first_exception)\n-        {\n-            if (!output.canPush())\n-                return Status::PortFull;\n-\n-            output.pushException(std::make_exception_ptr(*first_exception));\n-        }\n-\n-        output.finish();\n-        return Status::Finished;\n-    }\n-    return ISource::prepare();\n-}\n-\n-Strings DDLQueryStatusSource::getNewAndUpdate(const Strings & current_list_of_finished_hosts)\n-{\n-    Strings diff;\n-    for (const String & host : current_list_of_finished_hosts)\n-    {\n-        if (!waiting_hosts.contains(host))\n-        {\n-            if (!ignoring_hosts.contains(host))\n-            {\n-                ignoring_hosts.emplace(host);\n-                LOG_INFO(log, \"Unexpected host {} appeared in task {}\", host, node_path);\n-            }\n-            continue;\n-        }\n-\n-        if (!finished_hosts.contains(host))\n-        {\n-            diff.emplace_back(host);\n-            finished_hosts.emplace(host);\n-        }\n-    }\n-\n-    return diff;\n-}\n-\n-\n bool maybeRemoveOnCluster(const ASTPtr & query_ptr, ContextPtr context)\n {\n     const auto * query = dynamic_cast<const ASTQueryWithTableAndOutput *>(query_ptr.get());\ndiff --git a/src/Interpreters/executeDDLQueryOnCluster.h b/src/Interpreters/executeDDLQueryOnCluster.h\nindex d33655538757..d015e8d8694e 100644\n--- a/src/Interpreters/executeDDLQueryOnCluster.h\n+++ b/src/Interpreters/executeDDLQueryOnCluster.h\n@@ -43,7 +43,7 @@ struct DDLQueryOnClusterParams\n /// Returns DDLQueryStatusSource, which reads results of query execution on each host in the cluster.\n BlockIO executeDDLQueryOnCluster(const ASTPtr & query_ptr, ContextPtr context, const DDLQueryOnClusterParams & params = {});\n \n-BlockIO getDistributedDDLStatus(const String & node_path, const DDLLogEntry & entry, ContextPtr context, const Strings * hosts_to_wait);\n+BlockIO getDDLOnClusterStatus(const String & node_path, const String & replicas_path, const DDLLogEntry & entry, ContextPtr context);\n \n bool maybeRemoveOnCluster(const ASTPtr & query_ptr, ContextPtr context);\n \n",
  "test_patch": "diff --git a/tests/integration/helpers/cluster.py b/tests/integration/helpers/cluster.py\nindex 3c92df51ac44..cb28cae4c994 100644\n--- a/tests/integration/helpers/cluster.py\n+++ b/tests/integration/helpers/cluster.py\n@@ -3944,11 +3944,11 @@ def wait_start(self, start_wait_sec):\n         )\n         logging.info(f\"PS RESULT:\\n{ps_clickhouse}\")\n         pid = self.get_process_pid(\"clickhouse\")\n-        if pid is not None:\n-            self.exec_in_container(\n-                [\"bash\", \"-c\", f\"gdb -batch -ex 'thread apply all bt full' -p {pid}\"],\n-                user=\"root\",\n-            )\n+        # if pid is not None:\n+        #     self.exec_in_container(\n+        #         [\"bash\", \"-c\", f\"gdb -batch -ex 'thread apply all bt full' -p {pid}\"],\n+        #         user=\"root\",\n+        #     )\n         if last_err is not None:\n             raise last_err\n \ndiff --git a/tests/integration/test_config_corresponding_root/configs/config.xml b/tests/integration/test_config_corresponding_root/configs/config.xml\nindex 9a38d02a0369..001a98837c4e 100644\n--- a/tests/integration/test_config_corresponding_root/configs/config.xml\n+++ b/tests/integration/test_config_corresponding_root/configs/config.xml\n@@ -291,6 +291,8 @@\n     <distributed_ddl>\n         <!-- Path in ZooKeeper to queue with DDL queries -->\n         <path>/clickhouse/task_queue/ddl</path>\n+        <!-- Path in ZooKeeper to store running DDL hosts -->\n+        <replicas_path>/clickhouse/task_queue/replicas</replicas_path>\n \n         <!-- Settings from this profile will be used to execute DDL queries -->\n         <!-- <profile>default</profile> -->\ndiff --git a/tests/integration/test_config_xml_full/configs/config.xml b/tests/integration/test_config_xml_full/configs/config.xml\nindex 80b6a702032e..81be2e48122b 100644\n--- a/tests/integration/test_config_xml_full/configs/config.xml\n+++ b/tests/integration/test_config_xml_full/configs/config.xml\n@@ -849,6 +849,8 @@\n     <distributed_ddl>\n         <!-- Path in ZooKeeper to queue with DDL queries -->\n         <path>/clickhouse/task_queue/ddl</path>\n+        <!-- Path in ZooKeeper to store running DDL hosts -->\n+        <replicas_path>/clickhouse/task_queue/replicas</replicas_path>\n \n         <!-- Settings from this profile will be used to execute DDL queries -->\n         <!-- <profile>default</profile> -->\ndiff --git a/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/__init__.py b/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/configs/remote_servers.xml b/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..c505345cf7fc\n--- /dev/null\n+++ b/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/configs/remote_servers.xml\n@@ -0,0 +1,30 @@\n+<clickhouse>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node2</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node3</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node4</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+\n+    <allow_zookeeper_write>1</allow_zookeeper_write>\n+</clickhouse>\ndiff --git a/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/test.py b/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/test.py\nnew file mode 100644\nindex 000000000000..06bdd6f21980\n--- /dev/null\n+++ b/tests/integration/test_ddl_on_cluster_stop_waiting_for_offline_hosts/test.py\n@@ -0,0 +1,88 @@\n+import time\n+\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance(\n+    \"node1\",\n+    main_configs=[\"configs/remote_servers.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+)\n+node2 = cluster.add_instance(\n+    \"node2\", main_configs=[\"configs/remote_servers.xml\"], with_zookeeper=True\n+)\n+node3 = cluster.add_instance(\n+    \"node3\", main_configs=[\"configs/remote_servers.xml\"], with_zookeeper=True\n+)\n+node4 = cluster.add_instance(\n+    \"node4\",\n+    main_configs=[\"configs/remote_servers.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_stop_waiting_for_offline_hosts(started_cluster):\n+    timeout = 10\n+    settings = {\"distributed_ddl_task_timeout\": timeout}\n+\n+    node1.query(\n+        \"DROP TABLE IF EXISTS test_table ON CLUSTER test_cluster SYNC\",\n+        settings=settings,\n+    )\n+\n+    node1.query(\n+        \"CREATE TABLE test_table ON CLUSTER test_cluster (x Int) Engine=Memory\",\n+        settings=settings,\n+    )\n+\n+    try:\n+        node4.stop_clickhouse()\n+\n+        start = time.time()\n+        assert \"Code: 159. DB::Exception\" in node1.query_and_get_error(\n+            \"DROP TABLE IF EXISTS test_table ON CLUSTER test_cluster SYNC\",\n+            settings=settings,\n+        )\n+        assert time.time() - start >= timeout\n+\n+        start = time.time()\n+        assert \"Code: 159. DB::Exception\" in node1.query_and_get_error(\n+            \"CREATE TABLE test_table ON CLUSTER test_cluster (x Int) Engine=Memory\",\n+            settings=settings,\n+        )\n+        assert time.time() - start >= timeout\n+\n+        # set `distributed_ddl_output_mode` = `throw_only_active``\n+        settings = {\n+            \"distributed_ddl_task_timeout\": timeout,\n+            \"distributed_ddl_output_mode\": \"throw_only_active\",\n+        }\n+\n+        start = time.time()\n+        node1.query(\n+            \"DROP TABLE IF EXISTS test_table ON CLUSTER test_cluster SYNC\",\n+            settings=settings,\n+        )\n+\n+        start = time.time()\n+        node1.query(\n+            \"CREATE TABLE test_table ON CLUSTER test_cluster (x Int) Engine=Memory\",\n+            settings=settings,\n+        )\n+    finally:\n+        node4.start_clickhouse()\ndiff --git a/tests/integration/test_ddl_worker_replicas/__init__.py b/tests/integration/test_ddl_worker_replicas/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_ddl_worker_replicas/configs/remote_servers.xml b/tests/integration/test_ddl_worker_replicas/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..c505345cf7fc\n--- /dev/null\n+++ b/tests/integration/test_ddl_worker_replicas/configs/remote_servers.xml\n@@ -0,0 +1,30 @@\n+<clickhouse>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node2</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node3</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node4</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+\n+    <allow_zookeeper_write>1</allow_zookeeper_write>\n+</clickhouse>\ndiff --git a/tests/integration/test_ddl_worker_replicas/test.py b/tests/integration/test_ddl_worker_replicas/test.py\nnew file mode 100644\nindex 000000000000..5ba5f406e4fa\n--- /dev/null\n+++ b/tests/integration/test_ddl_worker_replicas/test.py\n@@ -0,0 +1,77 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance(\n+    \"node1\",\n+    main_configs=[\"configs/remote_servers.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+)\n+node2 = cluster.add_instance(\n+    \"node2\", main_configs=[\"configs/remote_servers.xml\"], with_zookeeper=True\n+)\n+node3 = cluster.add_instance(\n+    \"node3\", main_configs=[\"configs/remote_servers.xml\"], with_zookeeper=True\n+)\n+node4 = cluster.add_instance(\n+    \"node4\",\n+    main_configs=[\"configs/remote_servers.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_ddl_worker_replicas(started_cluster):\n+    for replica in [\"node1:9000\", \"node2:9000\", \"node3:9000\", \"node4:9000\"]:\n+        # wait until the replicas path is created\n+        node1.query_with_retry(\n+            sql=f\"SELECT count() FROM system.zookeeper WHERE path='/clickhouse/task_queue/replicas/{replica}'\",\n+            check_callback=lambda result: result == 1,\n+        )\n+\n+        result = node1.query(\n+            f\"SELECT name, value, ephemeralOwner FROM system.zookeeper WHERE path='/clickhouse/task_queue/replicas/{replica}'\"\n+        ).strip()\n+        print(f\"result: {replica} {result}\")\n+\n+        lines = list(result.split(\"\\n\"))\n+        assert len(lines) == 1\n+        parts = list(lines[0].split(\"\\t\"))\n+        assert len(parts) == 3\n+        assert parts[0] == \"active\"\n+        assert len(parts[1]) != 0\n+        assert len(parts[2]) != 0\n+\n+    try:\n+        node4.stop_clickhouse()\n+\n+        # wait for node4 active path is removed\n+        node1.query_with_retry(\n+            sql=f\"SELECT count() FROM system.zookeeper WHERE path='/clickhouse/task_queue/replicas/node4:9000'\",\n+            check_callback=lambda result: result == 0,\n+        )\n+\n+        result = node1.query_with_retry(\n+            f\"SELECT name, value, ephemeralOwner FROM system.zookeeper WHERE path='/clickhouse/task_queue/replicas/node4:9000'\"\n+        ).strip()\n+\n+        print(f\"result: {replica} {result}\")\n+\n+        lines = list(result.split(\"\\n\"))\n+        assert len(lines) == 1\n+        assert len(lines[0]) == 0\n+    finally:\n+        node4.start_clickhouse()\ndiff --git a/tests/integration/test_https_replication/configs/config.xml b/tests/integration/test_https_replication/configs/config.xml\nindex 9a7a542b16ed..8c1cd9beeb2e 100644\n--- a/tests/integration/test_https_replication/configs/config.xml\n+++ b/tests/integration/test_https_replication/configs/config.xml\n@@ -256,6 +256,8 @@\n     <distributed_ddl>\n         <!-- Path in ZooKeeper to queue with DDL queries -->\n         <path>/clickhouse/task_queue/ddl</path>\n+        <!-- Path in ZooKeeper to store running DDL hosts -->\n+        <replicas_path>/clickhouse/task_queue/replicas</replicas_path>\n \n         <!-- Settings from this profile will be used to execute DDL queries -->\n         <!-- <profile>default</profile> -->\n",
  "problem_statement": "Waiting only on active replicas for database ON CLUSTER queries if distributed_ddl_output_mode is set to be *_only_active\n**Use case**\r\n\r\nWhen certain replicas become inactive, we don't wait for those replicas for Replicated database DDLs if we set distributed_ddl_output_mode being *_only_active. Similar behavior should apply for database create/drop/rename ON CLUSTER queries if the setting is *_only_active.\r\n\r\nThis will allow us to manage databases without blocking on some inactive replicas when we want that behavior, and keep distributed_ddl_timeout being non-zero.\r\n\r\n**Describe the solution you'd like**\r\n\r\nAs stated in description.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nAlternatively we can consider to set the timeout being zero, but it's going to make create temp table then inserts pretty tricky to do, as create table will immediately return without waiting for other active replicas to acknowledge.\r\n\r\n**Additional context**\r\nN/A\n",
  "hints_text": "",
  "created_at": "2024-09-17T08:08:30Z"
}