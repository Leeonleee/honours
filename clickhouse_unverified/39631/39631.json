{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 39631,
  "instance_id": "ClickHouse__ClickHouse-39631",
  "issue_numbers": [
    "34688"
  ],
  "base_commit": "73fc8c8f4b424ef202d3892b184e21421febd5e0",
  "patch": "diff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex c32b593a084e..f61f67450249 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -2929,6 +2929,7 @@ void InterpreterSelectQuery::executeWindow(QueryPlan & query_plan)\n             auto sorting_step = std::make_unique<SortingStep>(\n                 query_plan.getCurrentDataStream(),\n                 window.full_sort_description,\n+                window.partition_by,\n                 0 /* LIMIT */,\n                 sort_settings,\n                 settings.optimize_sorting_by_input_stream_properties);\ndiff --git a/src/Planner/Planner.cpp b/src/Planner/Planner.cpp\nindex 08fe1d56a189..a32567367871 100644\n--- a/src/Planner/Planner.cpp\n+++ b/src/Planner/Planner.cpp\n@@ -898,6 +898,7 @@ void addWindowSteps(QueryPlan & query_plan,\n             auto sorting_step = std::make_unique<SortingStep>(\n                 query_plan.getCurrentDataStream(),\n                 window_description.full_sort_description,\n+                window_description.partition_by,\n                 0 /*limit*/,\n                 sort_settings,\n                 settings.optimize_sorting_by_input_stream_properties);\ndiff --git a/src/Processors/QueryPlan/SortingStep.cpp b/src/Processors/QueryPlan/SortingStep.cpp\nindex 55ce763575ec..641b9036d4c5 100644\n--- a/src/Processors/QueryPlan/SortingStep.cpp\n+++ b/src/Processors/QueryPlan/SortingStep.cpp\n@@ -1,3 +1,4 @@\n+#include <memory>\n #include <stdexcept>\n #include <IO/Operators.h>\n #include <Processors/Merges/MergingSortedTransform.h>\n@@ -9,6 +10,8 @@\n #include <QueryPipeline/QueryPipelineBuilder.h>\n #include <Common/JSONBuilder.h>\n \n+#include <Processors/ResizeProcessor.h>\n+#include <Processors/Transforms/ScatterByPartitionTransform.h>\n \n namespace CurrentMetrics\n {\n@@ -76,6 +79,21 @@ SortingStep::SortingStep(\n     output_stream->sort_scope = DataStream::SortScope::Global;\n }\n \n+SortingStep::SortingStep(\n+        const DataStream & input_stream,\n+        const SortDescription & description_,\n+        const SortDescription & partition_by_description_,\n+        UInt64 limit_,\n+        const Settings & settings_,\n+        bool optimize_sorting_by_input_stream_properties_)\n+    : SortingStep(input_stream, description_, limit_, settings_, optimize_sorting_by_input_stream_properties_)\n+{\n+    partition_by_description = partition_by_description_;\n+\n+    output_stream->sort_description = result_description;\n+    output_stream->sort_scope = DataStream::SortScope::Stream;\n+}\n+\n SortingStep::SortingStep(\n     const DataStream & input_stream_,\n     SortDescription prefix_description_,\n@@ -117,7 +135,11 @@ void SortingStep::updateOutputStream()\n {\n     output_stream = createOutputStream(input_streams.front(), input_streams.front().header, getDataStreamTraits());\n     output_stream->sort_description = result_description;\n-    output_stream->sort_scope = DataStream::SortScope::Global;\n+\n+    if (partition_by_description.empty())\n+        output_stream->sort_scope = DataStream::SortScope::Global;\n+    else\n+        output_stream->sort_scope = DataStream::SortScope::Stream;\n }\n \n void SortingStep::updateLimit(size_t limit_)\n@@ -135,6 +157,55 @@ void SortingStep::convertToFinishSorting(SortDescription prefix_description_)\n     prefix_description = std::move(prefix_description_);\n }\n \n+void SortingStep::scatterByPartitionIfNeeded(QueryPipelineBuilder& pipeline)\n+{\n+    size_t threads = pipeline.getNumThreads();\n+    size_t streams = pipeline.getNumStreams();\n+\n+    if (!partition_by_description.empty() && threads > 1)\n+    {\n+        Block stream_header = pipeline.getHeader();\n+\n+        ColumnNumbers key_columns;\n+        key_columns.reserve(partition_by_description.size());\n+        for (auto & col : partition_by_description)\n+        {\n+            key_columns.push_back(stream_header.getPositionByName(col.column_name));\n+        }\n+\n+        pipeline.transform([&](OutputPortRawPtrs ports)\n+        {\n+            Processors processors;\n+            for (auto * port : ports)\n+            {\n+                auto scatter = std::make_shared<ScatterByPartitionTransform>(stream_header, threads, key_columns);\n+                connect(*port, scatter->getInputs().front());\n+                processors.push_back(scatter);\n+            }\n+            return processors;\n+        });\n+\n+        if (streams > 1)\n+        {\n+            pipeline.transform([&](OutputPortRawPtrs ports)\n+            {\n+                Processors processors;\n+                for (size_t i = 0; i < threads; ++i)\n+                {\n+                    size_t output_it = i;\n+                    auto resize = std::make_shared<ResizeProcessor>(stream_header, streams, 1);\n+                    auto & inputs = resize->getInputs();\n+\n+                    for (auto input_it = inputs.begin(); input_it != inputs.end(); output_it += threads, ++input_it)\n+                        connect(*ports[output_it], *input_it);\n+                    processors.push_back(resize);\n+                }\n+                return processors;\n+            });\n+        }\n+    }\n+}\n+\n void SortingStep::finishSorting(\n     QueryPipelineBuilder & pipeline, const SortDescription & input_sort_desc, const SortDescription & result_sort_desc, const UInt64 limit_)\n {\n@@ -260,10 +331,12 @@ void SortingStep::fullSortStreams(\n void SortingStep::fullSort(\n     QueryPipelineBuilder & pipeline, const SortDescription & result_sort_desc, const UInt64 limit_, const bool skip_partial_sort)\n {\n+    scatterByPartitionIfNeeded(pipeline);\n+\n     fullSortStreams(pipeline, sort_settings, result_sort_desc, limit_, skip_partial_sort);\n \n     /// If there are several streams, then we merge them into one\n-    if (pipeline.getNumStreams() > 1)\n+    if (pipeline.getNumStreams() > 1 && (partition_by_description.empty() || pipeline.getNumThreads() == 1))\n     {\n         auto transform = std::make_shared<MergingSortedTransform>(\n             pipeline.getHeader(),\n@@ -295,6 +368,7 @@ void SortingStep::transformPipeline(QueryPipelineBuilder & pipeline, const Build\n     {\n         bool need_finish_sorting = (prefix_description.size() < result_description.size());\n         mergingSorted(pipeline, prefix_description, (need_finish_sorting ? 0 : limit));\n+\n         if (need_finish_sorting)\n         {\n             finishSorting(pipeline, prefix_description, result_description, limit);\ndiff --git a/src/Processors/QueryPlan/SortingStep.h b/src/Processors/QueryPlan/SortingStep.h\nindex 371a24ac6f2d..52f48f66a32b 100644\n--- a/src/Processors/QueryPlan/SortingStep.h\n+++ b/src/Processors/QueryPlan/SortingStep.h\n@@ -40,6 +40,15 @@ class SortingStep : public ITransformingStep\n         const Settings & settings_,\n         bool optimize_sorting_by_input_stream_properties_);\n \n+    /// Full with partitioning\n+    SortingStep(\n+        const DataStream & input_stream,\n+        const SortDescription & description_,\n+        const SortDescription & partition_by_description_,\n+        UInt64 limit_,\n+        const Settings & settings_,\n+        bool optimize_sorting_by_input_stream_properties_);\n+\n     /// FinishSorting\n     SortingStep(\n         const DataStream & input_stream_,\n@@ -83,14 +92,24 @@ class SortingStep : public ITransformingStep\n         bool skip_partial_sort = false);\n \n private:\n+    void scatterByPartitionIfNeeded(QueryPipelineBuilder& pipeline);\n     void updateOutputStream() override;\n \n-    static void\n-    mergeSorting(QueryPipelineBuilder & pipeline, const Settings & sort_settings, const SortDescription & result_sort_desc, UInt64 limit_);\n+    static void mergeSorting(\n+        QueryPipelineBuilder & pipeline,\n+        const Settings & sort_settings,\n+        const SortDescription & result_sort_desc,\n+        UInt64 limit_);\n \n-    void mergingSorted(QueryPipelineBuilder & pipeline, const SortDescription & result_sort_desc, UInt64 limit_);\n+    void mergingSorted(\n+        QueryPipelineBuilder & pipeline,\n+        const SortDescription & result_sort_desc,\n+        UInt64 limit_);\n     void finishSorting(\n-        QueryPipelineBuilder & pipeline, const SortDescription & input_sort_desc, const SortDescription & result_sort_desc, UInt64 limit_);\n+        QueryPipelineBuilder & pipeline,\n+        const SortDescription & input_sort_desc,\n+        const SortDescription & result_sort_desc,\n+        UInt64 limit_);\n     void fullSort(\n         QueryPipelineBuilder & pipeline,\n         const SortDescription & result_sort_desc,\n@@ -101,6 +120,9 @@ class SortingStep : public ITransformingStep\n \n     SortDescription prefix_description;\n     const SortDescription result_description;\n+\n+    SortDescription partition_by_description;\n+\n     UInt64 limit;\n     bool always_read_till_end = false;\n \ndiff --git a/src/Processors/QueryPlan/WindowStep.cpp b/src/Processors/QueryPlan/WindowStep.cpp\nindex 9c68a4b73d1f..bb4f429d6268 100644\n--- a/src/Processors/QueryPlan/WindowStep.cpp\n+++ b/src/Processors/QueryPlan/WindowStep.cpp\n@@ -67,7 +67,8 @@ void WindowStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQ\n     // This resize is needed for cases such as `over ()` when we don't have a\n     // sort node, and the input might have multiple streams. The sort node would\n     // have resized it.\n-    pipeline.resize(1);\n+    if (window_description.full_sort_description.empty())\n+        pipeline.resize(1);\n \n     pipeline.addSimpleTransform(\n         [&](const Block & /*header*/)\ndiff --git a/src/Processors/Transforms/ScatterByPartitionTransform.cpp b/src/Processors/Transforms/ScatterByPartitionTransform.cpp\nnew file mode 100644\nindex 000000000000..6e3cdc0fda11\n--- /dev/null\n+++ b/src/Processors/Transforms/ScatterByPartitionTransform.cpp\n@@ -0,0 +1,129 @@\n+#include <Processors/Transforms/ScatterByPartitionTransform.h>\n+\n+#include <Common/PODArray.h>\n+#include <Core/ColumnNumbers.h>\n+\n+namespace DB\n+{\n+ScatterByPartitionTransform::ScatterByPartitionTransform(Block header, size_t output_size_, ColumnNumbers key_columns_)\n+    : IProcessor(InputPorts{header}, OutputPorts{output_size_, header})\n+    , output_size(output_size_)\n+    , key_columns(std::move(key_columns_))\n+    , hash(0)\n+{}\n+\n+IProcessor::Status ScatterByPartitionTransform::prepare()\n+{\n+    auto & input = getInputs().front();\n+\n+    /// Check all outputs are finished or ready to get data.\n+\n+    bool all_finished = true;\n+    for (auto & output : outputs)\n+    {\n+        if (output.isFinished())\n+            continue;\n+\n+        all_finished = false;\n+    }\n+\n+    if (all_finished)\n+    {\n+        input.close();\n+        return Status::Finished;\n+    }\n+\n+    if (!all_outputs_processed)\n+    {\n+        auto output_it = outputs.begin();\n+        bool can_push = false;\n+        for (size_t i = 0; i < output_size; ++i, ++output_it)\n+            if (!was_output_processed[i] && output_it->canPush())\n+                can_push = true;\n+        if (!can_push)\n+            return Status::PortFull;\n+        return Status::Ready;\n+    }\n+    /// Try get chunk from input.\n+\n+    if (input.isFinished())\n+    {\n+        for (auto & output : outputs)\n+            output.finish();\n+\n+        return Status::Finished;\n+    }\n+\n+    input.setNeeded();\n+    if (!input.hasData())\n+        return Status::NeedData;\n+\n+    chunk = input.pull();\n+    has_data = true;\n+    was_output_processed.assign(outputs.size(), false);\n+\n+    return Status::Ready;\n+}\n+\n+void ScatterByPartitionTransform::work()\n+{\n+    if (all_outputs_processed)\n+        generateOutputChunks();\n+    all_outputs_processed = true;\n+\n+    size_t chunk_number = 0;\n+    for (auto & output : outputs)\n+    {\n+        auto & was_processed = was_output_processed[chunk_number];\n+        auto & output_chunk = output_chunks[chunk_number];\n+        ++chunk_number;\n+\n+        if (was_processed)\n+            continue;\n+\n+        if (output.isFinished())\n+            continue;\n+\n+        if (!output.canPush())\n+        {\n+            all_outputs_processed = false;\n+            continue;\n+        }\n+\n+        output.push(std::move(output_chunk));\n+        was_processed = true;\n+    }\n+\n+    if (all_outputs_processed)\n+    {\n+        has_data = false;\n+        output_chunks.clear();\n+    }\n+}\n+\n+void ScatterByPartitionTransform::generateOutputChunks()\n+{\n+    auto num_rows = chunk.getNumRows();\n+    const auto & columns = chunk.getColumns();\n+\n+    hash.reset(num_rows);\n+\n+    for (const auto & column_number : key_columns)\n+        columns[column_number]->updateWeakHash32(hash);\n+\n+    const auto & hash_data = hash.getData();\n+    IColumn::Selector selector(num_rows);\n+\n+    for (size_t row = 0; row < num_rows; ++row)\n+        selector[row] = hash_data[row] % output_size;\n+\n+    output_chunks.resize(output_size);\n+    for (const auto & column : columns)\n+    {\n+        auto filtered_columns = column->scatter(output_size, selector);\n+        for (size_t i = 0; i < output_size; ++i)\n+            output_chunks[i].addColumn(std::move(filtered_columns[i]));\n+    }\n+}\n+\n+}\ndiff --git a/src/Processors/Transforms/ScatterByPartitionTransform.h b/src/Processors/Transforms/ScatterByPartitionTransform.h\nnew file mode 100644\nindex 000000000000..327f6dd62b45\n--- /dev/null\n+++ b/src/Processors/Transforms/ScatterByPartitionTransform.h\n@@ -0,0 +1,34 @@\n+#pragma once\n+#include <Common/WeakHash.h>\n+#include <Core/ColumnNumbers.h>\n+#include <Processors/IProcessor.h>\n+\n+namespace DB\n+{\n+\n+struct ScatterByPartitionTransform : IProcessor\n+{\n+    ScatterByPartitionTransform(Block header, size_t output_size_, ColumnNumbers key_columns_);\n+\n+    String getName() const override { return \"ScatterByPartitionTransform\"; }\n+\n+    Status prepare() override;\n+    void work() override;\n+\n+private:\n+\n+    void generateOutputChunks();\n+\n+    size_t output_size;\n+    ColumnNumbers key_columns;\n+\n+    bool has_data = false;\n+    bool all_outputs_processed = true;\n+    std::vector<char> was_output_processed;\n+    Chunk chunk;\n+\n+    WeakHash32 hash;\n+    Chunks output_chunks;\n+};\n+\n+}\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01568_window_functions_distributed.reference b/tests/queries/0_stateless/01568_window_functions_distributed.reference\nindex 13ac0769a245..29ff2e7133c6 100644\n--- a/tests/queries/0_stateless/01568_window_functions_distributed.reference\n+++ b/tests/queries/0_stateless/01568_window_functions_distributed.reference\n@@ -22,6 +22,16 @@ select sum(number) over w as x, max(number) over w as y from t_01568 window w as\n 21\t8\n 21\t8\n 21\t8\n+select sum(number) over w, max(number) over w from t_01568 window w as (partition by p) order by p;\n+3\t2\n+3\t2\n+3\t2\n+12\t5\n+12\t5\n+12\t5\n+21\t8\n+21\t8\n+21\t8\n select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n 6\t2\n 6\t2\n@@ -41,6 +51,25 @@ select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,\n 42\t8\n 42\t8\n 42\t8\n+select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y SETTINGS max_threads = 1;\n+6\t2\n+6\t2\n+6\t2\n+6\t2\n+6\t2\n+6\t2\n+24\t5\n+24\t5\n+24\t5\n+24\t5\n+24\t5\n+24\t5\n+42\t8\n+42\t8\n+42\t8\n+42\t8\n+42\t8\n+42\t8\n select distinct sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n 6\t2\n 24\t5\ndiff --git a/tests/queries/0_stateless/01568_window_functions_distributed.sql b/tests/queries/0_stateless/01568_window_functions_distributed.sql\nindex 95072d6460f1..ecce7b412ba7 100644\n--- a/tests/queries/0_stateless/01568_window_functions_distributed.sql\n+++ b/tests/queries/0_stateless/01568_window_functions_distributed.sql\n@@ -15,8 +15,12 @@ from numbers(9);\n \n select sum(number) over w as x, max(number) over w as y from t_01568 window w as (partition by p) order by x, y;\n \n+select sum(number) over w, max(number) over w from t_01568 window w as (partition by p) order by p;\n+\n select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n \n+select sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y SETTINGS max_threads = 1;\n+\n select distinct sum(number) over w as x, max(number) over w as y from remote('127.0.0.{1,2}', '', t_01568) window w as (partition by p) order by x, y;\n \n -- window functions + aggregation w/shards\ndiff --git a/tests/queries/0_stateless/02884_parallel_window_functions.reference b/tests/queries/0_stateless/02884_parallel_window_functions.reference\nnew file mode 100644\nindex 000000000000..bac15838dc2a\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_parallel_window_functions.reference\n@@ -0,0 +1,100 @@\n+1\n+-- { echoOn }\n+\n+SELECT\n+    nw,\n+    sum(WR) AS R,\n+    sumIf(WR, uniq_rows = 1) AS UNR\n+FROM\n+(\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    GROUP BY ac, nw\n+)\n+GROUP BY nw\n+ORDER BY nw ASC, R DESC\n+LIMIT 10;\n+0\t2\t0\n+1\t2\t0\n+2\t2\t0\n+SELECT\n+    nw,\n+    sum(WR) AS R,\n+    sumIf(WR, uniq_rows = 1) AS UNR\n+FROM\n+(\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    GROUP BY ac, nw\n+)\n+GROUP BY nw\n+ORDER BY nw ASC, R DESC\n+LIMIT 10\n+SETTINGS max_threads = 1;\n+0\t2\t0\n+1\t2\t0\n+2\t2\t0\n+SELECT\n+    nw,\n+    sum(WR) AS R,\n+    sumIf(WR, uniq_rows = 1) AS UNR\n+FROM\n+(\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 0\n+    GROUP BY\n+        ac,\n+        nw\n+    UNION ALL\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 1\n+    GROUP BY\n+        ac,\n+        nw\n+    UNION ALL\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 2\n+    GROUP BY\n+        ac,\n+        nw\n+    UNION ALL\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 3\n+    GROUP BY\n+        ac,\n+        nw\n+)\n+GROUP BY nw\n+ORDER BY nw ASC, R DESC\n+LIMIT 10;\n+0\t2\t0\n+1\t2\t0\n+2\t2\t0\ndiff --git a/tests/queries/0_stateless/02884_parallel_window_functions.sql b/tests/queries/0_stateless/02884_parallel_window_functions.sql\nnew file mode 100644\nindex 000000000000..3151b42f8960\n--- /dev/null\n+++ b/tests/queries/0_stateless/02884_parallel_window_functions.sql\n@@ -0,0 +1,119 @@\n+CREATE TABLE window_funtion_threading\n+Engine = MergeTree\n+ORDER BY (ac, nw)\n+AS SELECT\n+        toUInt64(toFloat32(number % 2) % 20000000) as ac,\n+        toFloat32(1) as wg,        \n+        toUInt16(toFloat32(number % 3) % 400) as nw\n+FROM numbers_mt(10000000);\n+\n+SELECT count() FROM (EXPLAIN PIPELINE SELECT\n+    nw,\n+    sum(WR) AS R,\n+    sumIf(WR, uniq_rows = 1) AS UNR\n+FROM\n+(\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    GROUP BY ac, nw\n+)\n+GROUP BY nw\n+ORDER BY nw ASC, R DESC\n+LIMIT 10) where explain ilike '%ScatterByPartitionTransform%' SETTINGS max_threads = 4;\n+\n+-- { echoOn }\n+\n+SELECT\n+    nw,\n+    sum(WR) AS R,\n+    sumIf(WR, uniq_rows = 1) AS UNR\n+FROM\n+(\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    GROUP BY ac, nw\n+)\n+GROUP BY nw\n+ORDER BY nw ASC, R DESC\n+LIMIT 10;\n+\n+SELECT\n+    nw,\n+    sum(WR) AS R,\n+    sumIf(WR, uniq_rows = 1) AS UNR\n+FROM\n+(\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    GROUP BY ac, nw\n+)\n+GROUP BY nw\n+ORDER BY nw ASC, R DESC\n+LIMIT 10\n+SETTINGS max_threads = 1;\n+\n+SELECT\n+    nw,\n+    sum(WR) AS R,\n+    sumIf(WR, uniq_rows = 1) AS UNR\n+FROM\n+(\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 0\n+    GROUP BY\n+        ac,\n+        nw\n+    UNION ALL\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 1\n+    GROUP BY\n+        ac,\n+        nw\n+    UNION ALL\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 2\n+    GROUP BY\n+        ac,\n+        nw\n+    UNION ALL\n+    SELECT\n+        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\n+        AVG(wg) AS WR,\n+        ac,\n+        nw\n+    FROM window_funtion_threading\n+    WHERE (ac % 4) = 3\n+    GROUP BY\n+        ac,\n+        nw\n+)\n+GROUP BY nw\n+ORDER BY nw ASC, R DESC\n+LIMIT 10;\n",
  "problem_statement": "Multi-threaded calculation of Window Functions \nAs stated in https://github.com/ClickHouse/ClickHouse/issues/18097\r\n\"multithreaded calculation. This can be naturally implemented over hash-based partitioning.\"\r\n\r\nSome synthetic example showing the potential improvements (multithreading archived by UNION ALL)\r\n\r\n```sql\r\nCREATE TABLE test.window_funtion_threading\r\nEngine = MergeTree\r\nORDER BY (ac, nw)\r\nAS SELECT\r\n        toUInt64(rand(1) % 20000000) as ac,\r\n        toFloat32(1) as wg,        \r\n        toUInt16(rand(3) % 400) as nw\r\nFROM numbers_mt(10000000);\r\n\r\n-- existsing (window function executed in single thread)\r\n\r\nSELECT\r\n    nw,\r\n    sum(WR) AS R,\r\n    sumIf(WR, uniq_rows = 1) AS UNR\r\nFROM\r\n(\r\n    SELECT\r\n        uniq(nw) OVER (PARTITION BY ac) AS uniq_rows,\r\n        AVG(wg) AS WR,\r\n        ac,\r\n        nw\r\n    FROM test.window_funtion_threading\r\n    GROUP BY ac, nw\r\n)\r\nGROUP BY nw\r\nORDER BY R DESC\r\nLIMIT 10;\r\n\r\n-- 10 rows in set. Elapsed: 3.353 sec. Processed 10.00 million rows, 140.00 MB (2.98 million rows/s., 41.75 MB/s.)\r\n\r\nSELECT\r\n    nw,\r\n    sum(WR) AS R,\r\n    sumIf(WR, uniq_rows = 1) AS UNR\r\nFROM\r\n(\r\n    SELECT uniq(nw) OVER (PARTITION BY ac) AS uniq_rows, AVG(wg) AS WR,  ac, nw\r\n        FROM test.window_funtion_threading\r\n        WHERE (ac % 4) = 0\r\n        GROUP BY ac, nw\r\n    UNION ALL\r\n    SELECT uniq(nw) OVER (PARTITION BY ac) AS uniq_rows, AVG(wg) AS WR,  ac, nw\r\n        FROM test.window_funtion_threading\r\n        WHERE (ac % 4) = 1\r\n        GROUP BY ac, nw\r\n    UNION ALL\r\n    SELECT uniq(nw) OVER (PARTITION BY ac) AS uniq_rows, AVG(wg) AS WR,  ac, nw\r\n        FROM test.window_funtion_threading\r\n        WHERE (ac % 4) = 2\r\n        GROUP BY ac, nw\r\n    UNION ALL\r\n    SELECT uniq(nw) OVER (PARTITION BY ac) AS uniq_rows, AVG(wg) AS WR,  ac, nw\r\n        FROM test.window_funtion_threading\r\n        WHERE (ac % 4) = 3\r\n        GROUP BY ac, nw\r\n)\r\nGROUP BY nw\r\nORDER BY R DESC\r\nLIMIT 10;\r\n\r\n-- 10 rows in set. Elapsed: 1.456 sec. Processed 40.00 million rows, 560.00 MB (27.47 million rows/s., 384.53 MB/s.)\r\n```\n",
  "hints_text": "",
  "created_at": "2022-07-26T23:28:40Z"
}