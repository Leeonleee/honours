{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 69755,
  "instance_id": "ClickHouse__ClickHouse-69755",
  "issue_numbers": [
    "65951"
  ],
  "base_commit": "0ed1520c3e961f64fe746d3b1faaf9ce5574ddb6",
  "patch": "diff --git a/docs/en/operations/settings/merge-tree-settings.md b/docs/en/operations/settings/merge-tree-settings.md\nindex a13aacc76e6d..8a106720ee00 100644\n--- a/docs/en/operations/settings/merge-tree-settings.md\n+++ b/docs/en/operations/settings/merge-tree-settings.md\n@@ -1064,4 +1064,32 @@ Possible values:\n \n - throw, drop, rebuild\n \n-Default value: throw\n\\ No newline at end of file\n+Default value: throw\n+\n+## min_free_disk_bytes_to_perform_insert\n+\n+The minimum number of bytes that should be free in disk space in order to insert data. If the number of available free bytes is less than `min_free_disk_bytes_to_throw_insert` then an exception is thrown and the insert is not executed. Note that this setting:\n+- takes into account the `keep_free_space_bytes` setting.\n+- does not take into account the amount of data that will be written by the `INSERT` operation.\n+- is only checked if a positive (non-zero) number of bytes is specified\n+\n+Possible values:\n+\n+- Any positive integer.\n+\n+Default value: 0 bytes.\n+\n+## min_free_disk_ratio_to_perform_insert \n+\n+The minimum free to total disk space ratio to perform an `INSERT`. Must be a floating point value between 0 and 1. Note that this setting:\n+- takes into account the `keep_free_space_bytes` setting.\n+- does not take into account the amount of data that will be written by the `INSERT` operation.\n+- is only checked if a positive (non-zero) ratio is specified\n+\n+Possible values:\n+\n+- Float, 0.0 - 1.0\n+\n+Default value: 0.0\n+\n+Note that if both `min_free_disk_ratio_to_perform_insert` and `min_free_disk_bytes_to_perform_insert` are specified, ClickHouse will count on the value that will allow to perform inserts on a bigger amount of free memory.\ndiff --git a/src/Core/Settings.cpp b/src/Core/Settings.cpp\nindex f09d8744ac6c..0e0e3ddec6e0 100644\n--- a/src/Core/Settings.cpp\n+++ b/src/Core/Settings.cpp\n@@ -359,6 +359,8 @@ namespace ErrorCodes\n     M(Int64, distributed_ddl_task_timeout, 180, \"Timeout for DDL query responses from all hosts in the cluster. If a ddl request has not been performed on all hosts, a response will contain a timeout error and a request will be executed in an async mode. A negative value means infinite. Zero means async mode.\", 0) \\\n     M(Milliseconds, stream_flush_interval_ms, 7500, \"Timeout for flushing data from streaming storages.\", 0) \\\n     M(Milliseconds, stream_poll_timeout_ms, 500, \"Timeout for polling data from/to streaming storages.\", 0) \\\n+    M(UInt64, min_free_disk_bytes_to_perform_insert, 0, \"Minimum free disk space bytes to perform an insert.\", 0) \\\n+    M(Double, min_free_disk_ratio_to_perform_insert, 0.0, \"Minimum free disk space ratio to perform an insert.\", 0) \\\n     \\\n     M(Bool, final, false, \"Query with the FINAL modifier by default. If the engine does not support the FINAL, it does not have any effect. On queries with multiple tables, FINAL is applied only to those that support it. It also works on distributed tables\", 0) \\\n     \\\ndiff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp\nindex c6f879dd636a..beeaad5b5d35 100644\n--- a/src/Core/SettingsChangesHistory.cpp\n+++ b/src/Core/SettingsChangesHistory.cpp\n@@ -75,6 +75,8 @@ static std::initializer_list<std::pair<ClickHouseVersion, SettingsChangesHistory\n             {\"show_create_query_identifier_quoting_rule\", \"when_necessary\", \"when_necessary\", \"New setting.\"},\n             {\"show_create_query_identifier_quoting_style\", \"Backticks\", \"Backticks\", \"New setting.\"},\n             {\"enable_secure_identifiers\", false, false, \"New setting.\"},\n+            {\"min_free_disk_bytes_to_perform_insert\", 0, 0, \"New setting.\"},\n+            {\"min_free_disk_ratio_to_perform_insert\", 0.0, 0.0, \"New setting.\"},\n         }\n     },\n     {\"24.9\",\n@@ -92,6 +94,9 @@ static std::initializer_list<std::pair<ClickHouseVersion, SettingsChangesHistory\n             {\"join_to_sort_minimum_perkey_rows\", 0, 40, \"The lower limit of per-key average rows in the right table to determine whether to rerange the right table by key in left or inner join. This setting ensures that the optimization is not applied for sparse table keys\"},\n             {\"join_to_sort_maximum_table_rows\", 0, 10000, \"The maximum number of rows in the right table to determine whether to rerange the right table by key in left or inner join\"},\n             {\"allow_experimental_join_right_table_sorting\", false, false, \"If it is set to true, and the conditions of `join_to_sort_minimum_perkey_rows` and `join_to_sort_maximum_table_rows` are met, rerange the right table by key to improve the performance in left or inner hash join\"},\n+            {\"mongodb_throw_on_unsupported_query\", false, true, \"New setting.\"},\n+            {\"min_free_disk_bytes_to_perform_insert\", 0, 0, \"Maintain some free disk space bytes from inserts while still allowing for temporary writing.\"},\n+            {\"min_free_disk_ratio_to_perform_insert\", 0.0, 0.0, \"Maintain some free disk space bytes expressed as ratio to total disk space from inserts while still allowing for temporary writing.\"},\n         }\n     },\n     {\"24.8\",\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 130d9ca8f6a2..e3a8d211e9cd 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -60,6 +60,8 @@ namespace Setting\n     extern const SettingsBool materialize_statistics_on_insert;\n     extern const SettingsBool optimize_on_insert;\n     extern const SettingsBool throw_on_max_partitions_per_insert_block;\n+    extern const SettingsUInt64 min_free_disk_bytes_to_perform_insert;\n+    extern const SettingsDouble min_free_disk_ratio_to_perform_insert;\n }\n \n namespace ErrorCodes\n@@ -67,6 +69,7 @@ namespace ErrorCodes\n     extern const int ABORTED;\n     extern const int LOGICAL_ERROR;\n     extern const int TOO_MANY_PARTS;\n+    extern const int NOT_ENOUGH_SPACE;\n }\n \n namespace\n@@ -560,6 +563,41 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     VolumePtr volume = data.getStoragePolicy()->getVolume(0);\n     VolumePtr data_part_volume = createVolumeFromReservation(reservation, volume);\n \n+    const auto & global_settings = context->getSettingsRef();\n+    const auto & data_settings = data.getSettings();\n+\n+    const UInt64 & min_bytes_to_perform_insert =\n+        data_settings->min_free_disk_bytes_to_perform_insert.changed\n+        ? data_settings->min_free_disk_bytes_to_perform_insert\n+        : global_settings[Setting::min_free_disk_bytes_to_perform_insert];\n+\n+    const Float64 & min_ratio_to_perform_insert =\n+        data_settings->min_free_disk_ratio_to_perform_insert.changed\n+        ? data_settings->min_free_disk_ratio_to_perform_insert\n+        : global_settings[Setting::min_free_disk_ratio_to_perform_insert];\n+\n+    if (min_bytes_to_perform_insert > 0 || min_ratio_to_perform_insert > 0.0)\n+    {\n+        const auto & disk = data_part_volume->getDisk();\n+        const UInt64 & total_disk_bytes = disk->getTotalSpace().value_or(0);\n+        const UInt64 & free_disk_bytes = disk->getAvailableSpace().value_or(0);\n+\n+        const UInt64 & min_bytes_from_ratio = static_cast<UInt64>(min_ratio_to_perform_insert * total_disk_bytes);\n+        const UInt64 & needed_free_bytes = std::max(min_bytes_to_perform_insert, min_bytes_from_ratio);\n+\n+        if (needed_free_bytes > free_disk_bytes)\n+        {\n+            throw Exception(\n+                ErrorCodes::NOT_ENOUGH_SPACE,\n+                \"Could not perform insert: less than {} free bytes left in the disk space ({}). \"\n+                \"Configure this limit with user settings {} or {}\",\n+                needed_free_bytes,\n+                free_disk_bytes,\n+                \"min_free_disk_bytes_to_perform_insert\",\n+                \"min_free_disk_ratio_to_perform_insert\");\n+        }\n+    }\n+\n     auto new_data_part = data.getDataPartBuilder(part_name, data_part_volume, part_dir)\n         .withPartFormat(data.choosePartFormat(expected_size, block.rows()))\n         .withPartInfo(new_part_info)\n@@ -571,8 +609,6 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     if (data.storage_settings.get()->assign_part_uuids)\n         new_data_part->uuid = UUIDHelpers::generateV4();\n \n-    const auto & data_settings = data.getSettings();\n-\n     SerializationInfo::Settings settings{data_settings->ratio_of_defaults_for_sparse_serialization, true};\n     SerializationInfoByName infos(columns, settings);\n     infos.add(block);\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 699548272994..add20b7cf751 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -101,6 +101,8 @@ struct Settings;\n     M(Bool, optimize_row_order, false, \"Allow reshuffling of rows during part inserts and merges to improve the compressibility of the new part\", 0) \\\n     M(Bool, use_adaptive_write_buffer_for_dynamic_subcolumns, true, \"Allow to use adaptive writer buffers during writing dynamic subcolumns to reduce memory usage\", 0) \\\n     M(UInt64, adaptive_write_buffer_initial_size, 16 * 1024, \"Initial size of an adaptive write buffer\", 0) \\\n+    M(UInt64, min_free_disk_bytes_to_perform_insert, 0, \"Minimum free disk space bytes to perform an insert.\", 0) \\\n+    M(Double, min_free_disk_ratio_to_perform_insert, 0.0, \"Minimum free disk space ratio to perform an insert.\", 0) \\\n     \\\n     /* Part removal settings. */ \\\n     M(UInt64, simultaneous_parts_removal_limit, 0, \"Maximum number of parts to remove during one CleanupThread iteration (0 means unlimited).\", 0) \\\n",
  "test_patch": "diff --git a/tests/integration/test_stop_insert_when_disk_close_to_full/__init__.py b/tests/integration/test_stop_insert_when_disk_close_to_full/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_stop_insert_when_disk_close_to_full/configs/config.d/storage_configuration.xml b/tests/integration/test_stop_insert_when_disk_close_to_full/configs/config.d/storage_configuration.xml\nnew file mode 100644\nindex 000000000000..d4031ff656cd\n--- /dev/null\n+++ b/tests/integration/test_stop_insert_when_disk_close_to_full/configs/config.d/storage_configuration.xml\n@@ -0,0 +1,19 @@\n+<clickhouse>\n+    <storage_configuration>\n+        <disks>\n+            <disk1>\n+                <type>local</type>\n+                <path>/disk1/</path>\n+            </disk1>\n+        </disks>\n+        <policies>\n+            <only_disk1>\n+                <volumes>\n+                    <main>\n+                        <disk>disk1</disk>\n+                    </main>\n+                </volumes>\n+            </only_disk1>\n+        </policies>\n+    </storage_configuration>\n+</clickhouse>\ndiff --git a/tests/integration/test_stop_insert_when_disk_close_to_full/test.py b/tests/integration/test_stop_insert_when_disk_close_to_full/test.py\nnew file mode 100644\nindex 000000000000..b7b904ec20a1\n--- /dev/null\n+++ b/tests/integration/test_stop_insert_when_disk_close_to_full/test.py\n@@ -0,0 +1,122 @@\n+import pytest\n+\n+from helpers.client import QueryRuntimeException\n+from helpers.cluster import ClickHouseCluster, ClickHouseInstance\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node = cluster.add_instance(\n+    \"node\",\n+    main_configs=[\"configs/config.d/storage_configuration.xml\"],\n+    tmpfs=[\"/disk1:size=7M\"],\n+    macros={\"shard\": 0, \"replica\": 1},\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_min_free_disk_settings(start_cluster):\n+    # min_free_disk_bytes_to_perform_insert (default 0)\n+    # min_free_disk_ratio_to_perform_insert (default 0.0)\n+\n+    node.query(\"DROP TABLE IF EXISTS test_table\")\n+\n+    node.query(\n+        f\"\"\"\n+        CREATE TABLE test_table (\n+            id UInt32,\n+            data String\n+        ) ENGINE = MergeTree()\n+        ORDER BY id\n+        SETTINGS storage_policy = 'only_disk1'\n+    \"\"\"\n+    )\n+\n+    node.query(\"INSERT INTO test_table (id, data) values (1, 'a')\")\n+\n+    free_bytes = 7 * 1024 * 1024  # 7MB -- size of disk\n+    node.query(f\"SET min_free_disk_bytes_to_perform_insert = {free_bytes}\")\n+\n+    try:\n+        node.query(\"INSERT INTO test_table (id, data) values (1, 'a')\")\n+    except QueryRuntimeException as e:\n+        assert \"NOT_ENOUGH_SPACE\" in str(e)\n+\n+    node.query(\"SET min_free_disk_bytes_to_perform_insert = 0\")\n+    node.query(\"INSERT INTO test_table (id, data) values (1, 'a')\")\n+\n+    free_ratio = 1.0\n+    node.query(f\"SET min_free_disk_ratio_to_perform_insert = {free_ratio}\")\n+\n+    try:\n+        node.query(\"INSERT INTO test_table (id, data) values (1, 'a')\")\n+    except QueryRuntimeException as e:\n+        assert \"NOT_ENOUGH_SPACE\" in str(e)\n+\n+    node.query(\"DROP TABLE test_table\")\n+\n+    # server setting for min_free_disk_ratio_to_perform_insert is 1 but we can overwrite at table level\n+    node.query(\n+        f\"\"\"\n+        CREATE TABLE test_table (\n+            id UInt32,\n+            data String\n+        ) ENGINE = MergeTree()\n+        ORDER BY id\n+        SETTINGS storage_policy = 'only_disk1', min_free_disk_ratio_to_perform_insert = 0.0\n+    \"\"\"\n+    )\n+\n+    node.query(\"INSERT INTO test_table (id, data) values (1, 'a')\")\n+\n+    node.query(\"DROP TABLE test_table\")\n+    node.query(\"SET min_free_disk_ratio_to_perform_insert = 0.0\")\n+\n+\n+def test_insert_stops_when_disk_full(start_cluster):\n+    node.query(\"DROP TABLE IF EXISTS test_table\")\n+\n+    min_free_bytes = 3 * 1024 * 1024  # 3 MiB\n+\n+    node.query(\n+        f\"\"\"\n+        CREATE TABLE test_table (\n+            id UInt32,\n+            data String\n+        ) ENGINE = MergeTree()\n+        ORDER BY id\n+        SETTINGS storage_policy = 'only_disk1', min_free_disk_bytes_to_perform_insert = {min_free_bytes}\n+    \"\"\"\n+    )\n+\n+    count = 0\n+\n+    # Insert data to fill up disk\n+    try:\n+        for _ in range(100000):\n+            node.query(\n+                \"INSERT INTO test_table SELECT number, repeat('a', 1000 * 1000) FROM numbers(1)\"\n+            )\n+            count += 1\n+    except QueryRuntimeException as e:\n+        assert \"Could not perform insert\" in str(e)\n+        assert \"free bytes left in the disk space\" in str(e)\n+\n+    free_space = int(\n+        node.query(\"SELECT free_space FROM system.disks WHERE name = 'disk1'\").strip()\n+    )\n+    assert (\n+        free_space <= min_free_bytes\n+    ), f\"Free space ({free_space}) is less than min_free_bytes ({min_free_bytes})\"\n+\n+    rows = int(node.query(\"SELECT count() from test_table\").strip())\n+    assert rows == count\n+\n+    node.query(\"DROP TABLE test_table\")\n",
  "problem_statement": "Add setting to stop INSERTs when disk space is close to 100%\n**Use Case**\r\n\r\nWhen ClickHouse fills the disk to 100%, it creates several negative side effects: inability to perform certain SELECT queries (which require temporary data), potential file corruption, inability to write logs, and other cascading issues that complicate recovery. It's preferable to stop writes slightly earlier to avoid these problems.\r\n\r\nThe same logic as `parts_to_throw_insert` can be applied here. Eventually, writes will fail as the disk becomes full; it's better if they fail slightly earlier to prevent cascading problems.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIntroduce user-level settings: `min_free_diskspace_bytes_to_throw_insert ` (default 10GB) and `min_free_diskspace_ratio_to_throw_insert ` (default 0.02, i.e., 2%). When these settings are non-zero, during an INSERT operation, ClickHouse will ensure that the storage policy of the target table has at least `min_free_diskspace_bytes_to_throw_insert` bytes or `min_free_diskspace_ratio_to_throw_insert` free space available. If not, it will throw an exception.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nRelying on user discipline, monitoring, and alerting has proven to be ineffective.\r\n\r\n**Additional context**\r\n\r\nThrottling / delaying / slowing down the inserts in that conditions is more controversial, so I don't think it's really needed.\r\n\r\nMay be we can also make the logic a bit more complex - by checking if the default drive have that amount of free disks space AND there is at least one disk in storage policy of the table with that amount of the free disk space.\r\n\n",
  "hints_text": "> potential file corruption\r\n\r\nThis is probably just a speculation and should not be possible.\nI saw that few times on practice (several months ago last time, so have no extra info currently, if will see again - will try to collect more details if it will be possible).\r\n\r\nI'm not 100% sure about the exact mechanics there, but i suspect it caused by lack of fsync \r\n\r\nAFAIK normally the `write` should take into the account both real free space & size of dirty pages, but i'm not sure if's true for all the cases / filesystems / kernels / modes of write.\r\n\r\nReason for reflection: `ENOSPC` is a valid error code for `fsync`... See [man](https://man7.org/linux/man-pages/man2/fsync.2.html), some relevant discussions: [stackoverflow](https://stackoverflow.com/questions/42434872/writing-programs-to-cope-with-i-o-errors-causing-lost-writes-on-linux), [postgresql wiki](https://wiki.postgresql.org/wiki/ENOSPC), [some study](https://ramalagappan.github.io/pdfs/papers/cuttlefs.pdf), https://news.ycombinator.com/item?id=19238121, fsyncgate etc.\n@filimonov I've actually been looking at this too - thanks for raising it.\r\n\r\nAre you already aware of `keep_free_space_bytes`?\r\nhttps://clickhouse.com/docs/en/operations/system-tables/disks\r\n\r\nThis is a value in bytes on the disk storage policy that somewhat achieves this and results in INSERTs being rejected if you configure it to keep say 2% of disk space free.\r\n\r\nThough one behaviour that I'm less sure about and don't really like is that it affects internal operations like merges, as this setting is evaluated as part of getting free space when scheduling merges.\r\n\r\nI'm curious what your thoughts are on this setting.  \nIndeed, may be we can achieve similar effect with that keep_free_space_bytes...\r\n\r\nThat requires the user / dba to remember to configure that on every disk (not so handy), and may give a bit confusing error messages, and will impact all attemps to reserve space on that disk (not only inserts).\nAlso if it will be a user level setting users will be able to temporary relax it. Or even different limits can be used for different users,\n@filimonov @alexey-milovidov any objections to me taking this and making a PR? \r\n\r\nWe have an intern interested in working with ClickHouse, and would like to work on this with them.",
  "created_at": "2024-09-19T08:55:36Z"
}