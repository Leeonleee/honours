{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 15089,
  "instance_id": "ClickHouse__ClickHouse-15089",
  "issue_numbers": [
    "11397",
    "4609"
  ],
  "base_commit": "7e6536097f7881f44ebd30de62dfd07592e00b78",
  "patch": "diff --git a/src/Compression/CompressionCodecDelta.cpp b/src/Compression/CompressionCodecDelta.cpp\nindex a10d2589576a..f3d6953bd2ae 100644\n--- a/src/Compression/CompressionCodecDelta.cpp\n+++ b/src/Compression/CompressionCodecDelta.cpp\n@@ -136,7 +136,7 @@ void CompressionCodecDelta::doDecompressData(const char * source, UInt32 source_\n namespace\n {\n \n-UInt8 getDeltaBytesSize(DataTypePtr column_type)\n+UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n     if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n@@ -155,7 +155,7 @@ UInt8 getDeltaBytesSize(DataTypePtr column_type)\n void registerCodecDelta(CompressionCodecFactory & factory)\n {\n     UInt8 method_code = UInt8(CompressionMethodByte::Delta);\n-    factory.registerCompressionCodecWithType(\"Delta\", method_code, [&](const ASTPtr & arguments, DataTypePtr column_type) -> CompressionCodecPtr\n+    factory.registerCompressionCodecWithType(\"Delta\", method_code, [&](const ASTPtr & arguments, const IDataType * column_type) -> CompressionCodecPtr\n     {\n         UInt8 delta_bytes_size = 0;\n \ndiff --git a/src/Compression/CompressionCodecDoubleDelta.cpp b/src/Compression/CompressionCodecDoubleDelta.cpp\nindex 96fd29fe3562..6895a80264d3 100644\n--- a/src/Compression/CompressionCodecDoubleDelta.cpp\n+++ b/src/Compression/CompressionCodecDoubleDelta.cpp\n@@ -307,7 +307,7 @@ void decompressDataForType(const char * source, UInt32 source_size, char * dest)\n     }\n }\n \n-UInt8 getDataBytesSize(DataTypePtr column_type)\n+UInt8 getDataBytesSize(const IDataType * column_type)\n {\n     if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec DoubleDelta is not applicable for {} because the data type is not of fixed size\",\n@@ -413,7 +413,7 @@ void registerCodecDoubleDelta(CompressionCodecFactory & factory)\n {\n     UInt8 method_code = UInt8(CompressionMethodByte::DoubleDelta);\n     factory.registerCompressionCodecWithType(\"DoubleDelta\", method_code,\n-        [&](const ASTPtr & arguments, DataTypePtr column_type) -> CompressionCodecPtr\n+        [&](const ASTPtr & arguments, const IDataType * column_type) -> CompressionCodecPtr\n     {\n         if (arguments)\n             throw Exception(\"Codec DoubleDelta does not accept any arguments\", ErrorCodes::BAD_ARGUMENTS);\ndiff --git a/src/Compression/CompressionCodecGorilla.cpp b/src/Compression/CompressionCodecGorilla.cpp\nindex d739623a94b1..582a9f41874d 100644\n--- a/src/Compression/CompressionCodecGorilla.cpp\n+++ b/src/Compression/CompressionCodecGorilla.cpp\n@@ -222,7 +222,7 @@ void decompressDataForType(const char * source, UInt32 source_size, char * dest)\n     }\n }\n \n-UInt8 getDataBytesSize(DataTypePtr column_type)\n+UInt8 getDataBytesSize(const IDataType * column_type)\n {\n     if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Gorilla is not applicable for {} because the data type is not of fixed size\",\n@@ -329,7 +329,7 @@ void registerCodecGorilla(CompressionCodecFactory & factory)\n {\n     UInt8 method_code = UInt8(CompressionMethodByte::Gorilla);\n     factory.registerCompressionCodecWithType(\"Gorilla\", method_code,\n-        [&](const ASTPtr & arguments, DataTypePtr column_type) -> CompressionCodecPtr\n+        [&](const ASTPtr & arguments, const IDataType * column_type) -> CompressionCodecPtr\n     {\n         if (arguments)\n             throw Exception(\"Codec Gorilla does not accept any arguments\", ErrorCodes::BAD_ARGUMENTS);\ndiff --git a/src/Compression/CompressionCodecT64.cpp b/src/Compression/CompressionCodecT64.cpp\nindex f081652f613b..f0fb8351410f 100644\n--- a/src/Compression/CompressionCodecT64.cpp\n+++ b/src/Compression/CompressionCodecT64.cpp\n@@ -136,7 +136,7 @@ TypeIndex baseType(TypeIndex type_idx)\n     return TypeIndex::Nothing;\n }\n \n-TypeIndex typeIdx(const DataTypePtr & data_type)\n+TypeIndex typeIdx(const IDataType * data_type)\n {\n     if (!data_type)\n         return TypeIndex::Nothing;\n@@ -656,7 +656,7 @@ void CompressionCodecT64::updateHash(SipHash & hash) const\n \n void registerCodecT64(CompressionCodecFactory & factory)\n {\n-    auto reg_func = [&](const ASTPtr & arguments, DataTypePtr type) -> CompressionCodecPtr\n+    auto reg_func = [&](const ASTPtr & arguments, const IDataType * type) -> CompressionCodecPtr\n     {\n         Variant variant = Variant::Byte;\n \n@@ -683,7 +683,7 @@ void registerCodecT64(CompressionCodecFactory & factory)\n \n         auto type_idx = typeIdx(type);\n         if (type && type_idx == TypeIndex::Nothing)\n-            throw Exception(\"T64 codec is not supported for specified type\", ErrorCodes::ILLEGAL_SYNTAX_FOR_CODEC_TYPE);\n+            throw Exception(\"T64 codec is not supported for specified type \" + type->getName(), ErrorCodes::ILLEGAL_SYNTAX_FOR_CODEC_TYPE);\n         return std::make_shared<CompressionCodecT64>(type_idx, variant);\n     };\n \ndiff --git a/src/Compression/CompressionFactory.cpp b/src/Compression/CompressionFactory.cpp\nindex cebeee7c5a82..91b4aa4b8dec 100644\n--- a/src/Compression/CompressionFactory.cpp\n+++ b/src/Compression/CompressionFactory.cpp\n@@ -6,6 +6,7 @@\n #include <IO/ReadBuffer.h>\n #include <Parsers/queryToString.h>\n #include <Compression/CompressionCodecMultiple.h>\n+#include <Compression/CompressionCodecNone.h>\n #include <IO/WriteHelpers.h>\n \n #include <boost/algorithm/string/join.hpp>\n@@ -57,7 +58,7 @@ void CompressionCodecFactory::validateCodec(const String & family_name, std::opt\n     }\n }\n \n-ASTPtr CompressionCodecFactory::validateCodecAndGetPreprocessedAST(const ASTPtr & ast, DataTypePtr column_type, bool sanity_check) const\n+ASTPtr CompressionCodecFactory::validateCodecAndGetPreprocessedAST(const ASTPtr & ast, const IDataType * column_type, bool sanity_check) const\n {\n     if (const auto * func = ast->as<ASTFunction>())\n     {\n@@ -67,6 +68,7 @@ ASTPtr CompressionCodecFactory::validateCodecAndGetPreprocessedAST(const ASTPtr\n         bool has_none = false;\n         std::optional<size_t> generic_compression_codec_pos;\n \n+        bool can_substitute_codec_arguments = true;\n         for (size_t i = 0; i < func->arguments->children.size(); ++i)\n         {\n             const auto & inner_codec_ast = func->arguments->children[i];\n@@ -99,7 +101,34 @@ ASTPtr CompressionCodecFactory::validateCodecAndGetPreprocessedAST(const ASTPtr\n             }\n             else\n             {\n-                result_codec = getImpl(codec_family_name, codec_arguments, column_type);\n+                if (column_type)\n+                {\n+                    CompressionCodecPtr prev_codec;\n+                    IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & substream_type)\n+                    {\n+                        if (IDataType::isSpecialCompressionAllowed(substream_path))\n+                        {\n+                            result_codec = getImpl(codec_family_name, codec_arguments, &substream_type);\n+\n+                            /// Case for column Tuple, which compressed with codec which depends on data type, like Delta.\n+                            /// We cannot substitute parameters for such codecs.\n+                            if (prev_codec && prev_codec->getHash() != result_codec->getHash())\n+                                can_substitute_codec_arguments = false;\n+                            prev_codec = result_codec;\n+                        }\n+                    };\n+\n+                    IDataType::SubstreamPath stream_path;\n+                    column_type->enumerateStreams(callback, stream_path);\n+\n+                    if (!result_codec)\n+                        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot find any substream with data type for type {}. It's a bug\", column_type->getName());\n+                }\n+                else\n+                {\n+                    result_codec = getImpl(codec_family_name, codec_arguments, nullptr);\n+                }\n+\n                 codecs_descriptions->children.emplace_back(result_codec->getCodecDesc());\n             }\n \n@@ -140,16 +169,30 @@ ASTPtr CompressionCodecFactory::validateCodecAndGetPreprocessedAST(const ASTPtr\n                     \" (Note: you can enable setting 'allow_suspicious_codecs' to skip this check).\", ErrorCodes::BAD_ARGUMENTS);\n \n         }\n-        std::shared_ptr<ASTFunction> result = std::make_shared<ASTFunction>();\n-        result->name = \"CODEC\";\n-        result->arguments = codecs_descriptions;\n-        return result;\n+        /// For columns with nested types like Tuple(UInt32, UInt64) we\n+        /// obviously cannot substitute parameters for codecs which depend on\n+        /// data type, because for the first column Delta(4) is suitable and\n+        /// Delta(8) for the second. So we should leave codec description as is\n+        /// and deduce them in get method for each subtype separately. For all\n+        /// other types it's better to substitute parameters, for better\n+        /// readability and backward compatibility.\n+        if (can_substitute_codec_arguments)\n+        {\n+            std::shared_ptr<ASTFunction> result = std::make_shared<ASTFunction>();\n+            result->name = \"CODEC\";\n+            result->arguments = codecs_descriptions;\n+            return result;\n+        }\n+        else\n+        {\n+            return ast;\n+        }\n     }\n \n     throw Exception(\"Unknown codec family: \" + queryToString(ast), ErrorCodes::UNKNOWN_CODEC);\n }\n \n-CompressionCodecPtr CompressionCodecFactory::get(const ASTPtr & ast, DataTypePtr column_type, CompressionCodecPtr current_default) const\n+CompressionCodecPtr CompressionCodecFactory::get(const ASTPtr & ast, const IDataType * column_type, CompressionCodecPtr current_default, bool only_generic) const\n {\n     if (current_default == nullptr)\n         current_default = default_codec;\n@@ -175,10 +218,16 @@ CompressionCodecPtr CompressionCodecFactory::get(const ASTPtr & ast, DataTypePtr\n             else\n                 throw Exception(\"Unexpected AST element for compression codec\", ErrorCodes::UNEXPECTED_AST_STRUCTURE);\n \n+            CompressionCodecPtr codec;\n             if (codec_family_name == DEFAULT_CODEC_NAME)\n-                codecs.emplace_back(current_default);\n+                codec = current_default;\n             else\n-                codecs.emplace_back(getImpl(codec_family_name, codec_arguments, column_type));\n+                codec = getImpl(codec_family_name, codec_arguments, column_type);\n+\n+            if (only_generic && !codec->isGenericCompression())\n+                continue;\n+\n+            codecs.emplace_back(codec);\n         }\n \n         CompressionCodecPtr res;\n@@ -187,6 +236,8 @@ CompressionCodecPtr CompressionCodecFactory::get(const ASTPtr & ast, DataTypePtr\n             return codecs.back();\n         else if (codecs.size() > 1)\n             return std::make_shared<CompressionCodecMultiple>(codecs);\n+        else\n+            return std::make_shared<CompressionCodecNone>();\n     }\n \n     throw Exception(\"Unexpected AST structure for compression codec: \" + queryToString(ast), ErrorCodes::UNEXPECTED_AST_STRUCTURE);\n@@ -203,7 +254,7 @@ CompressionCodecPtr CompressionCodecFactory::get(const uint8_t byte_code) const\n }\n \n \n-CompressionCodecPtr CompressionCodecFactory::getImpl(const String & family_name, const ASTPtr & arguments, DataTypePtr column_type) const\n+CompressionCodecPtr CompressionCodecFactory::getImpl(const String & family_name, const ASTPtr & arguments, const IDataType * column_type) const\n {\n     if (family_name == \"Multiple\")\n         throw Exception(\"Codec Multiple cannot be specified directly\", ErrorCodes::UNKNOWN_CODEC);\n@@ -235,7 +286,7 @@ void CompressionCodecFactory::registerCompressionCodecWithType(\n \n void CompressionCodecFactory::registerCompressionCodec(const String & family_name, std::optional<uint8_t> byte_code, Creator creator)\n {\n-    registerCompressionCodecWithType(family_name, byte_code, [family_name, creator](const ASTPtr & ast, DataTypePtr /* data_type */)\n+    registerCompressionCodecWithType(family_name, byte_code, [family_name, creator](const ASTPtr & ast, const IDataType * /* data_type */)\n     {\n         return creator(ast);\n     });\ndiff --git a/src/Compression/CompressionFactory.h b/src/Compression/CompressionFactory.h\nindex f30050f81bab..dc06e8538983 100644\n--- a/src/Compression/CompressionFactory.h\n+++ b/src/Compression/CompressionFactory.h\n@@ -26,7 +26,7 @@ class CompressionCodecFactory final : private boost::noncopyable\n {\n protected:\n     using Creator = std::function<CompressionCodecPtr(const ASTPtr & parameters)>;\n-    using CreatorWithType = std::function<CompressionCodecPtr(const ASTPtr & parameters, DataTypePtr column_type)>;\n+    using CreatorWithType = std::function<CompressionCodecPtr(const ASTPtr & parameters, const IDataType * column_type)>;\n     using SimpleCreator = std::function<CompressionCodecPtr()>;\n     using CompressionCodecsDictionary = std::unordered_map<String, CreatorWithType>;\n     using CompressionCodecsCodeDictionary = std::unordered_map<uint8_t, CreatorWithType>;\n@@ -38,7 +38,13 @@ class CompressionCodecFactory final : private boost::noncopyable\n     CompressionCodecPtr getDefaultCodec() const;\n \n     /// Validate codecs AST specified by user and parses codecs description (substitute default parameters)\n-    ASTPtr validateCodecAndGetPreprocessedAST(const ASTPtr & ast, DataTypePtr column_type, bool sanity_check) const;\n+    ASTPtr validateCodecAndGetPreprocessedAST(const ASTPtr & ast, const IDataType * column_type, bool sanity_check) const;\n+\n+    /// Just wrapper for previous method.\n+    ASTPtr validateCodecAndGetPreprocessedAST(const ASTPtr & ast, const DataTypePtr & column_type, bool sanity_check) const\n+    {\n+        return validateCodecAndGetPreprocessedAST(ast, column_type.get(), sanity_check);\n+    }\n \n     /// Validate codecs AST specified by user\n     void validateCodec(const String & family_name, std::optional<int> level, bool sanity_check) const;\n@@ -47,8 +53,18 @@ class CompressionCodecFactory final : private boost::noncopyable\n     /// information about type to improve inner settings, but every codec should\n     /// be able to work without information about type. Also AST can contain\n     /// codec, which can be alias to current default codec, which can be changed\n-    /// in runtime.\n-    CompressionCodecPtr get(const ASTPtr & ast, DataTypePtr column_type, CompressionCodecPtr current_default = nullptr) const;\n+    /// in runtime. If only_generic is true than method will filter all\n+    /// isGenericCompression() == false codecs from result. If nothing found\n+    /// will return codec NONE. It's useful for auxiliary parts of complex columns\n+    /// like Nullable, Array and so on. If all codecs are non generic and\n+    /// only_generic = true, than codec NONE will be returned.\n+    CompressionCodecPtr get(const ASTPtr & ast, const IDataType * column_type, CompressionCodecPtr current_default = nullptr, bool only_generic = false) const;\n+\n+    /// Just wrapper for previous method.\n+    CompressionCodecPtr get(const ASTPtr & ast, const DataTypePtr & column_type, CompressionCodecPtr current_default = nullptr, bool only_generic = false) const\n+    {\n+        return get(ast, column_type.get(), current_default, only_generic);\n+    }\n \n     /// Get codec by method byte (no params available)\n     CompressionCodecPtr get(const uint8_t byte_code) const;\n@@ -65,7 +81,7 @@ class CompressionCodecFactory final : private boost::noncopyable\n     void registerSimpleCompressionCodec(const String & family_name, std::optional<uint8_t> byte_code, SimpleCreator creator);\n \n protected:\n-    CompressionCodecPtr getImpl(const String & family_name, const ASTPtr & arguments, DataTypePtr column_type) const;\n+    CompressionCodecPtr getImpl(const String & family_name, const ASTPtr & arguments, const IDataType * column_type) const;\n \n private:\n     CompressionCodecsDictionary family_name_with_codec;\ndiff --git a/src/Compression/ICompressionCodec.cpp b/src/Compression/ICompressionCodec.cpp\nindex 1b2c90e5163d..baf6e9b2b86c 100644\n--- a/src/Compression/ICompressionCodec.cpp\n+++ b/src/Compression/ICompressionCodec.cpp\n@@ -7,6 +7,7 @@\n #include <Common/Exception.h>\n #include <Parsers/queryToString.h>\n #include <Parsers/ASTIdentifier.h>\n+#include <Compression/CompressionCodecMultiple.h>\n \n \n namespace DB\ndiff --git a/src/Compression/ICompressionCodec.h b/src/Compression/ICompressionCodec.h\nindex fa143af8b9c5..2018218f2e75 100644\n--- a/src/Compression/ICompressionCodec.h\n+++ b/src/Compression/ICompressionCodec.h\n@@ -17,7 +17,6 @@ using CompressionCodecPtr = std::shared_ptr<ICompressionCodec>;\n using Codecs = std::vector<CompressionCodecPtr>;\n \n class IDataType;\n-using DataTypePtr = std::shared_ptr<const IDataType>;\n \n /**\n * Represents interface for compression codecs like LZ4, ZSTD, etc.\ndiff --git a/src/DataTypes/DataTypeArray.cpp b/src/DataTypes/DataTypeArray.cpp\nindex 49666cca4287..ed570beeda69 100644\n--- a/src/DataTypes/DataTypeArray.cpp\n+++ b/src/DataTypes/DataTypeArray.cpp\n@@ -151,7 +151,7 @@ namespace\n void DataTypeArray::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n {\n     path.push_back(Substream::ArraySizes);\n-    callback(path);\n+    callback(path, *this);\n     path.back() = Substream::ArrayElements;\n     nested->enumerateStreams(callback, path);\n     path.pop_back();\ndiff --git a/src/DataTypes/DataTypeLowCardinality.cpp b/src/DataTypes/DataTypeLowCardinality.cpp\nindex b4f28a8853f5..06bd7c610284 100644\n--- a/src/DataTypes/DataTypeLowCardinality.cpp\n+++ b/src/DataTypes/DataTypeLowCardinality.cpp\n@@ -54,7 +54,7 @@ void DataTypeLowCardinality::enumerateStreams(const StreamCallback & callback, S\n     path.push_back(Substream::DictionaryKeys);\n     dictionary_type->enumerateStreams(callback, path);\n     path.back() = Substream::DictionaryIndexes;\n-    callback(path);\n+    callback(path, *this);\n     path.pop_back();\n }\n \ndiff --git a/src/DataTypes/DataTypeNullable.cpp b/src/DataTypes/DataTypeNullable.cpp\nindex 9c738da9f6a5..ed501939901c 100644\n--- a/src/DataTypes/DataTypeNullable.cpp\n+++ b/src/DataTypes/DataTypeNullable.cpp\n@@ -44,7 +44,7 @@ bool DataTypeNullable::onlyNull() const\n void DataTypeNullable::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n {\n     path.push_back(Substream::NullMap);\n-    callback(path);\n+    callback(path, *this);\n     path.back() = Substream::NullableElements;\n     nested_data_type->enumerateStreams(callback, path);\n     path.pop_back();\ndiff --git a/src/DataTypes/IDataType.cpp b/src/DataTypes/IDataType.cpp\nindex 561166cbc782..d1c9f1bde778 100644\n--- a/src/DataTypes/IDataType.cpp\n+++ b/src/DataTypes/IDataType.cpp\n@@ -130,6 +130,18 @@ String IDataType::getFileNameForStream(const String & column_name, const IDataTy\n }\n \n \n+bool IDataType::isSpecialCompressionAllowed(const SubstreamPath & path)\n+{\n+    for (const Substream & elem : path)\n+    {\n+        if (elem.type == Substream::NullMap\n+            || elem.type == Substream::ArraySizes\n+            || elem.type == Substream::DictionaryIndexes)\n+            return false;\n+    }\n+    return true;\n+}\n+\n void IDataType::insertDefaultInto(IColumn & column) const\n {\n     column.insertDefault();\ndiff --git a/src/DataTypes/IDataType.h b/src/DataTypes/IDataType.h\nindex 6adcc0fda904..5e25d47534ee 100644\n--- a/src/DataTypes/IDataType.h\n+++ b/src/DataTypes/IDataType.h\n@@ -104,10 +104,11 @@ class IDataType : private boost::noncopyable\n \n     using SubstreamPath = std::vector<Substream>;\n \n-    using StreamCallback = std::function<void(const SubstreamPath &)>;\n+    using StreamCallback = std::function<void(const SubstreamPath &, const IDataType &)>;\n+\n     virtual void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n     {\n-        callback(path);\n+        callback(path, *this);\n     }\n     void enumerateStreams(const StreamCallback & callback, SubstreamPath && path) const { enumerateStreams(callback, path); }\n     void enumerateStreams(const StreamCallback & callback) const { enumerateStreams(callback, {}); }\n@@ -442,6 +443,10 @@ class IDataType : private boost::noncopyable\n \n     static String getFileNameForStream(const String & column_name, const SubstreamPath & path);\n \n+    /// Substream path supports special compression methods like codec Delta.\n+    /// For all other substreams (like ArraySizes, NullMasks, etc.) we use only\n+    /// generic compression codecs like LZ4.\n+    static bool isSpecialCompressionAllowed(const SubstreamPath & path);\n private:\n     friend class DataTypeFactory;\n     /// Customize this DataType\n@@ -685,4 +690,3 @@ template <> inline constexpr bool IsDataTypeDateOrDateTime<DataTypeDateTime> = t\n template <> inline constexpr bool IsDataTypeDateOrDateTime<DataTypeDateTime64> = true;\n \n }\n-\ndiff --git a/src/Storages/ColumnsDescription.cpp b/src/Storages/ColumnsDescription.cpp\nindex 6e4bc4dc80c4..48cde6b6aa96 100644\n--- a/src/Storages/ColumnsDescription.cpp\n+++ b/src/Storages/ColumnsDescription.cpp\n@@ -426,6 +426,16 @@ CompressionCodecPtr ColumnsDescription::getCodecOrDefault(const String & column_\n     return getCodecOrDefault(column_name, CompressionCodecFactory::instance().getDefaultCodec());\n }\n \n+ASTPtr ColumnsDescription::getCodecDescOrDefault(const String & column_name, CompressionCodecPtr default_codec) const\n+{\n+    const auto it = columns.get<1>().find(column_name);\n+\n+    if (it == columns.get<1>().end() || !it->codec)\n+        return default_codec->getFullCodecDesc();\n+\n+    return it->codec;\n+}\n+\n ColumnsDescription::ColumnTTLs ColumnsDescription::getColumnTTLs() const\n {\n     ColumnTTLs ret;\ndiff --git a/src/Storages/ColumnsDescription.h b/src/Storages/ColumnsDescription.h\nindex 6e2d32990916..8d1009b72630 100644\n--- a/src/Storages/ColumnsDescription.h\n+++ b/src/Storages/ColumnsDescription.h\n@@ -115,6 +115,7 @@ class ColumnsDescription\n     bool hasCompressionCodec(const String & column_name) const;\n     CompressionCodecPtr getCodecOrDefault(const String & column_name, CompressionCodecPtr default_codec) const;\n     CompressionCodecPtr getCodecOrDefault(const String & column_name) const;\n+    ASTPtr getCodecDescOrDefault(const String & column_name, CompressionCodecPtr default_codec) const;\n \n     String toString() const;\n     static ColumnsDescription parse(const String & str);\ndiff --git a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\nindex 4e98caf066a8..e5cc6d6ecbf1 100644\n--- a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n@@ -51,7 +51,7 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n     for (const NameAndTypePair & column : columns)\n     {\n         column.type->enumerateStreams(\n-            [&](const IDataType::SubstreamPath & substream_path)\n+            [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_path */)\n             {\n                 ++stream_counts[IDataType::getFileNameForStream(column.name, substream_path)];\n             },\n@@ -62,7 +62,7 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n     const String mrk_extension = data_part->getMarksFileExtension();\n     for (const auto & column_name : empty_columns)\n     {\n-        IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path)\n+        IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_path */)\n         {\n             String stream_name = IDataType::getFileNameForStream(column_name, substream_path);\n             /// Delete files if they are no longer shared with another column.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex 91a34efc2b55..73b3fe698cc0 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -1452,7 +1452,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(\n     for (const NameAndTypePair & column : source_part->getColumns())\n     {\n         column.type->enumerateStreams(\n-            [&](const IDataType::SubstreamPath & substream_path)\n+            [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n                 ++stream_counts[IDataType::getFileNameForStream(column.name, substream_path)];\n             },\n@@ -1470,7 +1470,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(\n         }\n         else if (command.type == MutationCommand::Type::DROP_COLUMN)\n         {\n-            IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path)\n+            IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n                 String stream_name = IDataType::getFileNameForStream(command.column_name, substream_path);\n                 /// Delete files if they are no longer shared with another column.\n@@ -1491,7 +1491,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(\n             String escaped_name_from = escapeForFileName(command.column_name);\n             String escaped_name_to = escapeForFileName(command.rename_to);\n \n-            IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path)\n+            IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n                 String stream_from = IDataType::getFileNameForStream(command.column_name, substream_path);\n \n@@ -1524,7 +1524,7 @@ NameSet MergeTreeDataMergerMutator::collectFilesToSkip(\n     /// Skip updated files\n     for (const auto & entry : updated_header)\n     {\n-        IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path)\n+        IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n         {\n             String stream_name = IDataType::getFileNameForStream(entry.name, substream_path);\n             files_to_skip.insert(stream_name + \".bin\");\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\nindex c53362c847dc..39898a7cc57a 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n@@ -77,7 +77,7 @@ ColumnSize MergeTreeDataPartWide::getColumnSizeImpl(\n     if (checksums.empty())\n         return size;\n \n-    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path)\n+    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         String file_name = IDataType::getFileNameForStream(column_name, substream_path);\n \n@@ -155,7 +155,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n             for (const NameAndTypePair & name_type : columns)\n             {\n                 IDataType::SubstreamPath stream_path;\n-                name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path)\n+                name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n                 {\n                     String file_name = IDataType::getFileNameForStream(name_type.name, substream_path);\n                     String mrk_file_name = file_name + index_granularity_info.marks_file_extension;\n@@ -177,7 +177,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n         std::optional<UInt64> marks_size;\n         for (const NameAndTypePair & name_type : columns)\n         {\n-            name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path)\n+            name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n                 auto file_path = path + IDataType::getFileNameForStream(name_type.name, substream_path) + index_granularity_info.marks_file_extension;\n \n@@ -205,7 +205,7 @@ bool MergeTreeDataPartWide::hasColumnFiles(const String & column_name, const IDa\n {\n     bool res = true;\n \n-    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path)\n+    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         String file_name = IDataType::getFileNameForStream(column_name, substream_path);\n \n@@ -222,7 +222,7 @@ bool MergeTreeDataPartWide::hasColumnFiles(const String & column_name, const IDa\n String MergeTreeDataPartWide::getFileNameForColumn(const NameAndTypePair & column) const\n {\n     String filename;\n-    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path)\n+    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         if (filename.empty())\n             filename = IDataType::getFileNameForStream(column.name, substream_path);\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex 235c76e744b9..c81894ee36d1 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -3,6 +3,7 @@\n \n namespace DB\n {\n+\n MergeTreeDataPartWriterCompact::MergeTreeDataPartWriterCompact(\n     const MergeTreeData::DataPartPtr & data_part_,\n     const NamesAndTypesList & columns_list_,\n@@ -30,14 +31,37 @@ MergeTreeDataPartWriterCompact::MergeTreeDataPartWriterCompact(\n {\n     const auto & storage_columns = metadata_snapshot->getColumns();\n     for (const auto & column : columns_list)\n+        addStreams(column.name, *column.type, storage_columns.getCodecDescOrDefault(column.name, default_codec));\n+}\n+\n+void MergeTreeDataPartWriterCompact::addStreams(const String & name, const IDataType & type, const ASTPtr & effective_codec_desc)\n+{\n+    IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & substream_type)\n     {\n-        auto codec = storage_columns.getCodecOrDefault(column.name, default_codec);\n-        auto & stream = streams_by_codec[codec->getHash()];\n+        String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+\n+        /// Shared offsets for Nested type.\n+        if (compressed_streams.count(stream_name))\n+            return;\n+\n+        CompressionCodecPtr compression_codec;\n+\n+        /// If we can use special codec than just get it\n+        if (IDataType::isSpecialCompressionAllowed(substream_path))\n+            compression_codec = CompressionCodecFactory::instance().get(effective_codec_desc, &substream_type, default_codec);\n+        else /// otherwise return only generic codecs and don't use info about data_type\n+            compression_codec = CompressionCodecFactory::instance().get(effective_codec_desc, nullptr, default_codec, true);\n+\n+        UInt64 codec_id = compression_codec->getHash();\n+        auto & stream = streams_by_codec[codec_id];\n         if (!stream)\n-            stream = std::make_shared<CompressedStream>(plain_hashing, codec);\n+            stream = std::make_shared<CompressedStream>(plain_hashing, compression_codec);\n \n-        compressed_streams.push_back(stream);\n-    }\n+        compressed_streams.emplace(stream_name, stream);\n+    };\n+\n+    IDataType::SubstreamPath stream_path;\n+    type.enumerateStreams(callback, stream_path);\n }\n \n void MergeTreeDataPartWriterCompact::write(\n@@ -110,18 +134,37 @@ void MergeTreeDataPartWriterCompact::writeBlock(const Block & block)\n         auto name_and_type = columns_list.begin();\n         for (size_t i = 0; i < columns_list.size(); ++i, ++name_and_type)\n         {\n-            auto & stream = compressed_streams[i];\n+            /// Tricky part, because we share compressed streams between different columns substreams.\n+            /// Compressed streams write data to the single file, but with different compression codecs.\n+            /// So we flush each stream (using next()) before using new one, because otherwise we will override\n+            /// data in result file.\n+            CompressedStreamPtr prev_stream;\n+            auto stream_getter = [&, this](const IDataType::SubstreamPath & substream_path) -> WriteBuffer *\n+            {\n+                String stream_name = IDataType::getFileNameForStream(name_and_type->name, substream_path);\n+\n+                auto & result_stream = compressed_streams[stream_name];\n+                /// Write one compressed block per column in granule for more optimal reading.\n+                if (prev_stream && prev_stream != result_stream)\n+                {\n+                    /// Offset should be 0, because compressed block is written for every granule.\n+                    assert(result_stream->hashing_buf.offset() == 0);\n+                    prev_stream->hashing_buf.next();\n+                }\n+\n+                prev_stream = result_stream;\n+\n+                return &result_stream->hashing_buf;\n+            };\n \n-            /// Offset should be 0, because compressed block is written for every granule.\n-            assert(stream->hashing_buf.offset() == 0);\n \n             writeIntBinary(plain_hashing.count(), marks);\n             writeIntBinary(UInt64(0), marks);\n \n-            writeColumnSingleGranule(block.getByName(name_and_type->name), stream, current_row, rows_to_write);\n+            writeColumnSingleGranule(block.getByName(name_and_type->name), stream_getter, current_row, rows_to_write);\n \n-            /// Write one compressed block per column in granule for more optimal reading.\n-            stream->hashing_buf.next();\n+            /// Each type always have at least one substream\n+            prev_stream->hashing_buf.next(); //-V522\n         }\n \n         ++from_mark;\n@@ -145,13 +188,14 @@ void MergeTreeDataPartWriterCompact::writeBlock(const Block & block)\n \n void MergeTreeDataPartWriterCompact::writeColumnSingleGranule(\n     const ColumnWithTypeAndName & column,\n-    const CompressedStreamPtr & stream,\n-    size_t from_row, size_t number_of_rows)\n+    IDataType::OutputStreamGetter stream_getter,\n+    size_t from_row,\n+    size_t number_of_rows)\n {\n     IDataType::SerializeBinaryBulkStatePtr state;\n     IDataType::SerializeBinaryBulkSettings serialize_settings;\n \n-    serialize_settings.getter = [&stream](IDataType::SubstreamPath) -> WriteBuffer * { return &stream->hashing_buf; };\n+    serialize_settings.getter = stream_getter;\n     serialize_settings.position_independent_encoding = true;\n     serialize_settings.low_cardinality_max_dictionary_size = 0;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\nindex 4beb0dba340a..a5a1a859e7ad 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n@@ -30,6 +30,8 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n \n     void addToChecksums(MergeTreeDataPartChecksums & checksums);\n \n+    void addStreams(const String & name, const IDataType & type, const ASTPtr & effective_codec_desc);\n+\n     Block header;\n \n     /** Simplified SquashingTransform. The original one isn't suitable in this case\n@@ -52,22 +54,25 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n     std::unique_ptr<WriteBufferFromFileBase> plain_file;\n     HashingWriteBuffer plain_hashing;\n \n+    /// Compressed stream which allows to write with codec.\n     struct CompressedStream\n     {\n         CompressedWriteBuffer compressed_buf;\n         HashingWriteBuffer hashing_buf;\n \n         CompressedStream(WriteBuffer & buf, const CompressionCodecPtr & codec)\n-            : compressed_buf(buf, codec), hashing_buf(compressed_buf) {}\n+            : compressed_buf(buf, codec)\n+            , hashing_buf(compressed_buf) {}\n     };\n \n     using CompressedStreamPtr = std::shared_ptr<CompressedStream>;\n \n-    /// Create compressed stream for every different codec.\n+    /// Create compressed stream for every different codec. All streams write to\n+    /// a single file on disk.\n     std::unordered_map<UInt64, CompressedStreamPtr> streams_by_codec;\n \n-    /// For better performance save pointer to stream by every column.\n-    std::vector<CompressedStreamPtr> compressed_streams;\n+    /// Stream for each column's substreams path (look at addStreams).\n+    std::unordered_map<String, CompressedStreamPtr> compressed_streams;\n \n     /// marks -> marks_file\n     std::unique_ptr<WriteBufferFromFileBase> marks_file;\n@@ -76,7 +81,7 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n     /// Write single granule of one column (rows between 2 marks)\n     static void writeColumnSingleGranule(\n         const ColumnWithTypeAndName & column,\n-        const CompressedStreamPtr & stream,\n+        IDataType::OutputStreamGetter stream_getter,\n         size_t from_row,\n         size_t number_of_rows);\n };\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex a558c57b5f0b..c15c39e7b7f4 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -1,5 +1,6 @@\n #include <Storages/MergeTree/MergeTreeDataPartWriterWide.h>\n #include <Interpreters/Context.h>\n+#include <Compression/CompressionFactory.h>\n \n namespace DB\n {\n@@ -28,28 +29,35 @@ MergeTreeDataPartWriterWide::MergeTreeDataPartWriterWide(\n {\n     const auto & columns = metadata_snapshot->getColumns();\n     for (const auto & it : columns_list)\n-        addStreams(it.name, *it.type, columns.getCodecOrDefault(it.name, default_codec), settings.estimated_size);\n+        addStreams(it.name, *it.type, columns.getCodecDescOrDefault(it.name, default_codec), settings.estimated_size);\n }\n \n void MergeTreeDataPartWriterWide::addStreams(\n     const String & name,\n     const IDataType & type,\n-    const CompressionCodecPtr & effective_codec,\n+    const ASTPtr & effective_codec_desc,\n     size_t estimated_size)\n {\n-    IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path)\n+    IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & substream_type)\n     {\n         String stream_name = IDataType::getFileNameForStream(name, substream_path);\n         /// Shared offsets for Nested type.\n         if (column_streams.count(stream_name))\n             return;\n \n+        CompressionCodecPtr compression_codec;\n+        /// If we can use special codec then just get it\n+        if (IDataType::isSpecialCompressionAllowed(substream_path))\n+            compression_codec = CompressionCodecFactory::instance().get(effective_codec_desc, &substream_type, default_codec);\n+        else /// otherwise return only generic codecs and don't use info about the data_type\n+            compression_codec = CompressionCodecFactory::instance().get(effective_codec_desc, nullptr, default_codec, true);\n+\n         column_streams[stream_name] = std::make_unique<Stream>(\n             stream_name,\n             data_part->volume->getDisk(),\n             part_path + stream_name, DATA_FILE_EXTENSION,\n             part_path + stream_name, marks_file_extension,\n-            effective_codec,\n+            compression_codec,\n             settings.max_compress_block_size,\n             estimated_size,\n             settings.aio_threshold);\n@@ -130,7 +138,7 @@ void MergeTreeDataPartWriterWide::writeSingleMark(\n     size_t number_of_rows,\n     DB::IDataType::SubstreamPath & path)\n {\n-     type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path)\n+    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n      {\n          bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n \n@@ -170,7 +178,7 @@ size_t MergeTreeDataPartWriterWide::writeSingleGranule(\n     type.serializeBinaryBulkWithMultipleStreams(column, from_row, number_of_rows, serialize_settings, serialization_state);\n \n     /// So that instead of the marks pointing to the end of the compressed block, there were marks pointing to the beginning of the next one.\n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path)\n+    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n \n@@ -251,7 +259,7 @@ void MergeTreeDataPartWriterWide::writeColumn(\n             current_column_mark++;\n     }\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path)\n+    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n         if (is_offsets)\n@@ -312,7 +320,7 @@ void MergeTreeDataPartWriterWide::writeFinalMark(\n {\n     writeSingleMark(column_name, *column_type, offset_columns, 0, path);\n     /// Memoize information about offsets\n-    column_type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path)\n+    column_type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n         if (is_offsets)\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\nindex 02ab2a7ca567..6dd3104c5b4f 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n@@ -66,7 +66,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n     void addStreams(\n         const String & name,\n         const IDataType & type,\n-        const CompressionCodecPtr & effective_codec,\n+        const ASTPtr & effective_codec_desc,\n         size_t estimated_size);\n \n     SerializationStates serialization_states;\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderWide.cpp b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\nindex 352d1f935894..1dacdacbae09 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n@@ -162,7 +162,7 @@ size_t MergeTreeReaderWide::readRows(size_t from_mark, bool continue_reading, si\n void MergeTreeReaderWide::addStreams(const String & name, const IDataType & type,\n     const ReadBufferFromFileBase::ProfileCallback & profile_callback, clockid_t clock_type)\n {\n-    IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path)\n+    IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         String stream_name = IDataType::getFileNameForStream(name, substream_path);\n \ndiff --git a/src/Storages/MergeTree/checkDataPart.cpp b/src/Storages/MergeTree/checkDataPart.cpp\nindex 63b061b9702d..2838c8eb8812 100644\n--- a/src/Storages/MergeTree/checkDataPart.cpp\n+++ b/src/Storages/MergeTree/checkDataPart.cpp\n@@ -120,7 +120,7 @@ IMergeTreeDataPart::Checksums checkDataPart(\n     {\n         for (const auto & column : columns_list)\n         {\n-            column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path)\n+            column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n                 String file_name = IDataType::getFileNameForStream(column.name, substream_path) + \".bin\";\n                 checksums_data.files[file_name] = checksum_compressed_file(disk, path + file_name);\ndiff --git a/src/Storages/StorageLog.cpp b/src/Storages/StorageLog.cpp\nindex e437bfb05f14..944dc0e58048 100644\n--- a/src/Storages/StorageLog.cpp\n+++ b/src/Storages/StorageLog.cpp\n@@ -362,7 +362,7 @@ void LogBlockOutputStream::writeData(const String & name, const IDataType & type\n {\n     IDataType::SerializeBinaryBulkSettings settings;\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path)\n+    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n     {\n         String stream_name = IDataType::getFileNameForStream(name, path);\n         if (written_streams.count(stream_name))\n@@ -382,7 +382,7 @@ void LogBlockOutputStream::writeData(const String & name, const IDataType & type\n     if (serialize_states.count(name) == 0)\n          type.serializeBinaryBulkStatePrefix(settings, serialize_states[name]);\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path)\n+    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n     {\n         String stream_name = IDataType::getFileNameForStream(name, path);\n         if (written_streams.count(stream_name))\n@@ -400,7 +400,7 @@ void LogBlockOutputStream::writeData(const String & name, const IDataType & type\n \n     type.serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path)\n+    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n     {\n         String stream_name = IDataType::getFileNameForStream(name, path);\n         if (!written_streams.emplace(stream_name).second)\n@@ -487,7 +487,7 @@ void StorageLog::addFiles(const String & column_name, const IDataType & type)\n         throw Exception(\"Duplicate column with name \" + column_name + \" in constructor of StorageLog.\",\n             ErrorCodes::DUPLICATE_COLUMN);\n \n-    IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path)\n+    IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         String stream_name = IDataType::getFileNameForStream(column_name, substream_path);\n \n@@ -597,7 +597,7 @@ const StorageLog::Marks & StorageLog::getMarksWithRealRowCount(const StorageMeta\n       * (Example: for Array data type, first stream is array sizes; and number of array sizes is the number of arrays).\n       */\n     IDataType::SubstreamPath substream_root_path;\n-    column_type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path)\n+    column_type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         if (filename.empty())\n             filename = IDataType::getFileNameForStream(column_name, substream_path);\ndiff --git a/src/Storages/StorageTinyLog.cpp b/src/Storages/StorageTinyLog.cpp\nindex 0bdcab8abf40..a59480f0a0d0 100644\n--- a/src/Storages/StorageTinyLog.cpp\n+++ b/src/Storages/StorageTinyLog.cpp\n@@ -390,7 +390,7 @@ void StorageTinyLog::addFiles(const String & column_name, const IDataType & type\n         throw Exception(\"Duplicate column with name \" + column_name + \" in constructor of StorageTinyLog.\",\n             ErrorCodes::DUPLICATE_COLUMN);\n \n-    IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path)\n+    IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         String stream_name = IDataType::getFileNameForStream(column_name, substream_path);\n         if (!files.count(stream_name))\n",
  "test_patch": "diff --git a/tests/integration/test_compression_nested_columns/__init__.py b/tests/integration/test_compression_nested_columns/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_compression_nested_columns/test.py b/tests/integration/test_compression_nested_columns/test.py\nnew file mode 100644\nindex 000000000000..f73adadd7703\n--- /dev/null\n+++ b/tests/integration/test_compression_nested_columns/test.py\n@@ -0,0 +1,68 @@\n+import random\n+import string\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', with_zookeeper=True)\n+node2 = cluster.add_instance('node2', with_zookeeper=True)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+def get_compression_codec_byte(node, table_name, part_name, filename):\n+    cmd = \"tail -c +17 /var/lib/clickhouse/data/default/{}/{}/{}.bin | od -x -N 1 | head -n 1 | awk '{{print $2}}'\".format(\n+        table_name, part_name, filename)\n+    return node.exec_in_container([\"bash\", \"-c\", cmd]).strip()\n+\n+CODECS_MAPPING = {\n+    'NONE' : '0002',\n+    'LZ4': '0082',\n+    'LZ4HC': '0082',  # not an error, same byte\n+    'ZSTD': '0090',\n+    'Multiple': '0091',\n+    'Delta': '0092',\n+    'T64': '0093',\n+}\n+\n+def test_nested_compression_codec(start_cluster):\n+    for i, node in enumerate([node1, node2]):\n+        node.query(\"\"\"\n+        CREATE TABLE compression_table (\n+            key UInt64,\n+            column_ok Nullable(UInt64) CODEC(Delta, LZ4),\n+            column_array Array(Array(UInt64)) CODEC(T64, LZ4),\n+            column_bad LowCardinality(Int64) CODEC(Delta)\n+        ) ENGINE = ReplicatedMergeTree('/t', '{}') ORDER BY tuple() PARTITION BY key\n+        SETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0;\n+        \"\"\".format(i), settings={\"allow_suspicious_codecs\" : \"1\", \"allow_suspicious_low_cardinality_types\" : \"1\"})\n+\n+    node1.query(\"INSERT INTO compression_table VALUES (1, 1, [[77]], 32)\")\n+\n+    node2.query(\"SYSTEM SYNC REPLICA compression_table\", timeout=5)\n+\n+    node1.query(\"DETACH TABLE compression_table\")\n+    node2.query(\"DETACH TABLE compression_table\")\n+\n+    node1.query(\"ATTACH TABLE compression_table\")\n+    node2.query(\"ATTACH TABLE compression_table\")\n+\n+    for node in [node1, node2]:\n+        assert get_compression_codec_byte(node, \"compression_table\", \"1_0_0_0\", \"column_ok\") == CODECS_MAPPING['Multiple']\n+        assert get_compression_codec_byte(node, \"compression_table\", \"1_0_0_0\", \"column_ok.null\") == CODECS_MAPPING['LZ4']\n+\n+        assert get_compression_codec_byte(node1, \"compression_table\", \"1_0_0_0\", \"column_array\") == CODECS_MAPPING['Multiple']\n+        assert get_compression_codec_byte(node2, \"compression_table\", \"1_0_0_0\", \"column_array.size0\") == CODECS_MAPPING['LZ4']\n+        assert get_compression_codec_byte(node2, \"compression_table\", \"1_0_0_0\", \"column_array.size1\") == CODECS_MAPPING['LZ4']\n+\n+        assert get_compression_codec_byte(node2, \"compression_table\", \"1_0_0_0\", \"column_bad.dict\") == CODECS_MAPPING['Delta']\n+        assert get_compression_codec_byte(node1, \"compression_table\", \"1_0_0_0\", \"column_bad\") == CODECS_MAPPING['NONE']\ndiff --git a/tests/queries/0_stateless/01380_coded_delta_exception_code.sql b/tests/queries/0_stateless/01380_coded_delta_exception_code.sql\nindex fc679e30e3f9..587fac958cd7 100644\n--- a/tests/queries/0_stateless/01380_coded_delta_exception_code.sql\n+++ b/tests/queries/0_stateless/01380_coded_delta_exception_code.sql\n@@ -1,7 +1,3 @@\n-CREATE TABLE delta_codec_synthetic (`id` UInt64 NULL CODEC(Delta, ZSTD(22))) ENGINE = MergeTree() ORDER BY tuple(); -- { serverError 36 }\n-CREATE TABLE delta_codec_synthetic (`id` UInt64 NULL CODEC(DoubleDelta, ZSTD(22))) ENGINE = MergeTree() ORDER BY tuple(); -- { serverError 36 }\n-CREATE TABLE delta_codec_synthetic (`id` UInt64 NULL CODEC(Gorilla, ZSTD(22))) ENGINE = MergeTree() ORDER BY tuple(); -- { serverError 36 }\n-\n CREATE TABLE delta_codec_synthetic (`id` Decimal(38, 10) CODEC(Delta, ZSTD(22))) ENGINE = MergeTree() ORDER BY tuple(); -- { serverError 36 }\n CREATE TABLE delta_codec_synthetic (`id` Decimal(38, 10) CODEC(DoubleDelta, ZSTD(22))) ENGINE = MergeTree() ORDER BY tuple(); -- { serverError 36 }\n CREATE TABLE delta_codec_synthetic (`id` Decimal(38, 10) CODEC(Gorilla, ZSTD(22))) ENGINE = MergeTree() ORDER BY tuple(); -- { serverError 36 }\ndiff --git a/tests/queries/0_stateless/01504_compression_multiple_streams.reference b/tests/queries/0_stateless/01504_compression_multiple_streams.reference\nnew file mode 100644\nindex 000000000000..4d3aba665267\n--- /dev/null\n+++ b/tests/queries/0_stateless/01504_compression_multiple_streams.reference\n@@ -0,0 +1,23 @@\n+1\t1\t[[1]]\t(1,[1])\n+1\t1\t[[1]]\t(1,[1])\n+CREATE TABLE default.columns_with_multiple_streams\\n(\\n    `field0` Nullable(Int64) CODEC(Delta(2), LZ4),\\n    `field1` Nullable(UInt8) CODEC(Delta(8), LZ4),\\n    `field2` Array(Array(Int64)) CODEC(Delta(8), LZ4),\\n    `field3` Tuple(UInt32, Array(UInt64)) CODEC(T64, Default)\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0, index_granularity = 8192\n+1\t1\t[[1]]\t(1,[1])\n+2\t2\t[[2]]\t(2,[2])\n+CREATE TABLE default.columns_with_multiple_streams\\n(\\n    `field0` Nullable(Int64) CODEC(Delta(2), LZ4),\\n    `field1` Nullable(UInt8) CODEC(Delta(8), LZ4),\\n    `field2` Array(Array(Int64)) CODEC(Delta(8), LZ4),\\n    `field3` Tuple(UInt32, Array(UInt64)) CODEC(Delta, Default)\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0, index_granularity = 8192\n+1\t1\t[[1]]\t(1,[1])\n+2\t2\t[[2]]\t(2,[2])\n+3\t3\t[[3]]\t(3,[3])\n+1\t1\t[[1]]\t(1,[1])\n+1\t1\t[[1]]\t(1,[1])\n+CREATE TABLE default.columns_with_multiple_streams_compact\\n(\\n    `field0` Nullable(Int64) CODEC(Delta(2), LZ4),\\n    `field1` Nullable(UInt8) CODEC(Delta(8), LZ4),\\n    `field2` Array(Array(Int64)) CODEC(Delta(8), LZ4),\\n    `field3` Tuple(UInt32, Array(UInt64)) CODEC(Delta, Default)\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_rows_for_wide_part = 100000, min_bytes_for_wide_part = 100000, index_granularity = 8192\n+1\t1\t[[1]]\t(1,[1])\n+2\t2\t[[2]]\t(2,[2])\n+1\t1\t[[1]]\t(1,[1])\n+2\t2\t[[2]]\t(2,[2])\n+CREATE TABLE default.columns_with_multiple_streams_compact\\n(\\n    `field0` Nullable(Int64) CODEC(Delta(2), LZ4),\\n    `field1` Nullable(UInt8) CODEC(Delta(8), LZ4),\\n    `field2` Array(Array(Int64)) CODEC(Delta(8), LZ4),\\n    `field3` Tuple(UInt32, Array(UInt64)) CODEC(Delta, Default)\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_rows_for_wide_part = 100000, min_bytes_for_wide_part = 100000, index_granularity = 8192\n+1\t1\t[[1]]\t(1,[1])\n+2\t2\t[[2]]\t(2,[2])\n+3\t3\t[[3]]\t(3,[3])\n+1\n+2\n+3\ndiff --git a/tests/queries/0_stateless/01504_compression_multiple_streams.sql b/tests/queries/0_stateless/01504_compression_multiple_streams.sql\nnew file mode 100644\nindex 000000000000..7cdf1b526511\n--- /dev/null\n+++ b/tests/queries/0_stateless/01504_compression_multiple_streams.sql\n@@ -0,0 +1,116 @@\n+DROP TABLE IF EXISTS columns_with_multiple_streams;\n+\n+SET mutations_sync = 2;\n+\n+CREATE TABLE columns_with_multiple_streams (\n+  field0 Nullable(Int64) CODEC(Delta(2), LZ4),\n+  field1 Nullable(Int64) CODEC(Delta, LZ4),\n+  field2 Array(Array(Int64)) CODEC(Delta, LZ4),\n+  field3 Tuple(UInt32, Array(UInt64)) CODEC(T64, Default)\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple()\n+SETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0;\n+\n+INSERT INTO columns_with_multiple_streams VALUES(1, 1, [[1]], tuple(1, [1]));\n+\n+SELECT * FROM columns_with_multiple_streams;\n+\n+DETACH TABLE columns_with_multiple_streams;\n+ATTACH TABLE columns_with_multiple_streams;\n+\n+SELECT * FROM columns_with_multiple_streams;\n+\n+ALTER TABLE columns_with_multiple_streams MODIFY COLUMN field1 Nullable(UInt8);\n+\n+INSERT INTO columns_with_multiple_streams VALUES(2, 2, [[2]], tuple(2, [2]));\n+\n+SHOW CREATE TABLE columns_with_multiple_streams;\n+\n+SELECT * FROM columns_with_multiple_streams ORDER BY field0;\n+\n+ALTER TABLE columns_with_multiple_streams MODIFY COLUMN field3 CODEC(Delta, Default);\n+\n+SHOW CREATE TABLE columns_with_multiple_streams;\n+\n+INSERT INTO columns_with_multiple_streams VALUES(3, 3, [[3]], tuple(3, [3]));\n+\n+OPTIMIZE TABLE columns_with_multiple_streams FINAL;\n+\n+SELECT * FROM columns_with_multiple_streams ORDER BY field0;\n+\n+DROP TABLE IF EXISTS columns_with_multiple_streams;\n+\n+DROP TABLE IF EXISTS columns_with_multiple_streams_compact;\n+\n+CREATE TABLE columns_with_multiple_streams_compact (\n+  field0 Nullable(Int64) CODEC(Delta(2), LZ4),\n+  field1 Nullable(Int64) CODEC(Delta, LZ4),\n+  field2 Array(Array(Int64)) CODEC(Delta, LZ4),\n+  field3 Tuple(UInt32, Array(UInt64)) CODEC(Delta, Default)\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple()\n+SETTINGS min_rows_for_wide_part = 100000, min_bytes_for_wide_part = 100000;\n+\n+INSERT INTO columns_with_multiple_streams_compact VALUES(1, 1, [[1]], tuple(1, [1]));\n+\n+SELECT * FROM columns_with_multiple_streams_compact;\n+\n+DETACH TABLE columns_with_multiple_streams_compact;\n+ATTACH TABLE columns_with_multiple_streams_compact;\n+\n+SELECT * FROM columns_with_multiple_streams_compact;\n+\n+ALTER TABLE columns_with_multiple_streams_compact MODIFY COLUMN field1 Nullable(UInt8);\n+\n+INSERT INTO columns_with_multiple_streams_compact VALUES(2, 2, [[2]], tuple(2, [2]));\n+\n+SHOW CREATE TABLE columns_with_multiple_streams_compact;\n+\n+SELECT * FROM columns_with_multiple_streams_compact ORDER BY field0;\n+\n+ALTER TABLE columns_with_multiple_streams_compact MODIFY COLUMN field3 CODEC(Delta, Default);\n+\n+SELECT * FROM columns_with_multiple_streams_compact ORDER BY field0;\n+\n+SHOW CREATE TABLE columns_with_multiple_streams_compact;\n+\n+INSERT INTO columns_with_multiple_streams_compact VALUES(3, 3, [[3]], tuple(3, [3]));\n+\n+SELECT * FROM columns_with_multiple_streams_compact ORDER BY field0;\n+\n+DROP TABLE IF EXISTS columns_with_multiple_streams_compact;\n+\n+DROP TABLE IF EXISTS columns_with_multiple_streams_bad_case;\n+\n+-- validation still works, non-sense codecs checked\n+CREATE TABLE columns_with_multiple_streams_bad_case (\n+  field0 Nullable(String) CODEC(Delta, LZ4)\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple(); --{serverError 36}\n+\n+CREATE TABLE columns_with_multiple_streams_bad_case (\n+  field0 Tuple(Array(UInt64), String) CODEC(T64, LZ4)\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple(); --{serverError 431}\n+\n+SET allow_suspicious_codecs = 1;\n+\n+CREATE TABLE columns_with_multiple_streams_bad_case (\n+  field0 Nullable(UInt64) CODEC(Delta)\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple();\n+\n+INSERT INTO columns_with_multiple_streams_bad_case VALUES(1), (2);\n+\n+INSERT INTO columns_with_multiple_streams_bad_case VALUES(3);\n+\n+OPTIMIZE TABLE columns_with_multiple_streams_bad_case FINAL;\n+\n+SELECT * FROM columns_with_multiple_streams_bad_case ORDER BY field0;\n+\n+DROP TABLE IF EXISTS columns_with_multiple_streams_bad_case;\n",
  "problem_statement": "Support Nullable values for the codec T64 \n**Use case**\r\nWe could possibly use the codec T64 for compress nullable columns.\r\n\r\n**Describe the solution you'd like**\r\nIf possible, than the codec pass in two phases, the first one to store the actual values using his natural type (UInt8, UInt16, etc...) and then a second phase where we store the null table as an UInt8\nDELTA CODEC don't support Nullable ?\n**Describe the bug**\r\n   DELTA CODEC is not supported when column is nullable. As there is no mention of this restriction in the doc , it's either a Documentation bug or a bug in the feature. \r\n \r\n**How to reproduce**\r\n  execute the following SQL :\r\n```sql\r\nCREATE TABLE IF NOT EXISTS some_table (\r\n       `timestamp` DATETIME DEFAULT now(), \r\n       field0 Nullable(Int64)  CODEC(Delta(2),LZ4), \r\n       field1 Nullable(Int64)  CODEC(Delta(2),LZ4)  \r\n) \r\nENGINE = MergeTree PARTITION BY (toStartOfDay(`timestamp`)) \r\nORDER BY (`timestamp`) SETTINGS index_granularity = 8192 ;\r\n```\r\nClickhouse refuse to create the table : DB::Exception: Value of type Nullable(Int64) in memory is not of fixed size. \r\n\r\n* Which ClickHouse server version to use\r\nClickHouse 19.3.6\r\n\r\n\r\n* Which interface to use, if matters\r\nhttp via Tabix \r\n\r\n* Non-default settings, if any\r\nNone\r\n\r\n\r\n\n",
  "hints_text": "Similar for Delta: #4609 \nany progress?\nImplementation for that case in more cumbersome, and questionable\r\n\r\n1) codec is smth we apply on the top of numerical column. Internally Nullable(UIntXX) is 2 columns, it is not very clear how it should be applied there.\r\n2) what is the `Delta(1,Null)` ? ",
  "created_at": "2020-09-21T15:54:07Z"
}