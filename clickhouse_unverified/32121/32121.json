{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 32121,
  "instance_id": "ClickHouse__ClickHouse-32121",
  "issue_numbers": [
    "31339"
  ],
  "base_commit": "5400c5899d5225f6dc25b1c8f8ca63d1bcc5c370",
  "patch": "diff --git a/src/Coordination/KeeperStateManager.cpp b/src/Coordination/KeeperStateManager.cpp\nindex a40cd1608635..c2d4274f972d 100644\n--- a/src/Coordination/KeeperStateManager.cpp\n+++ b/src/Coordination/KeeperStateManager.cpp\n@@ -19,6 +19,11 @@ KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersC\n     Poco::Util::AbstractConfiguration::Keys keys;\n     config.keys(config_prefix + \".raft_configuration\", keys);\n \n+    /// Sometimes (especially in cloud envs) users can provide incorrect\n+    /// configuration with duplicated raft ids or endpoints. We check them\n+    /// on config parsing stage and never commit to quorum.\n+    std::unordered_map<std::string, int> check_duplicated_hostnames;\n+\n     size_t total_servers = 0;\n     for (const auto & server_key : keys)\n     {\n@@ -37,6 +42,24 @@ KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersC\n             result.servers_start_as_followers.insert(new_server_id);\n \n         auto endpoint = hostname + \":\" + std::to_string(port);\n+        if (check_duplicated_hostnames.count(endpoint))\n+        {\n+            throw Exception(ErrorCodes::RAFT_ERROR, \"Raft config contain duplicate endpoints: \"\n+                            \"endpoint {} has been already added with id {}, but going to add it one more time with id {}\",\n+                            endpoint, check_duplicated_hostnames[endpoint], new_server_id);\n+        }\n+        else\n+        {\n+            /// Fullscan to check duplicated ids\n+            for (const auto & [id_endpoint, id] : check_duplicated_hostnames)\n+            {\n+                if (new_server_id == id)\n+                    throw Exception(ErrorCodes::RAFT_ERROR, \"Raft config contain duplicate ids: id {} has been already added with endpoint {}, \"\n+                                    \"but going to add it one more time with endpoint {}\", id, id_endpoint, endpoint);\n+            }\n+            check_duplicated_hostnames.emplace(endpoint, new_server_id);\n+        }\n+\n         auto peer_config = nuraft::cs_new<nuraft::srv_config>(new_server_id, 0, endpoint, \"\", !can_become_leader, priority);\n         if (my_server_id == new_server_id)\n         {\n",
  "test_patch": "diff --git a/tests/integration/test_keeper_incorrect_config/__init__.py b/tests/integration/test_keeper_incorrect_config/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_keeper_incorrect_config/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_keeper_incorrect_config/configs/enable_keeper1.xml b/tests/integration/test_keeper_incorrect_config/configs/enable_keeper1.xml\nnew file mode 100644\nindex 000000000000..c1d38a1de52f\n--- /dev/null\n+++ b/tests/integration/test_keeper_incorrect_config/configs/enable_keeper1.xml\n@@ -0,0 +1,22 @@\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>1</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>node1</hostname>\n+                <port>9234</port>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\ndiff --git a/tests/integration/test_keeper_incorrect_config/test.py b/tests/integration/test_keeper_incorrect_config/test.py\nnew file mode 100644\nindex 000000000000..4ab6b87d8533\n--- /dev/null\n+++ b/tests/integration/test_keeper_incorrect_config/test.py\n@@ -0,0 +1,119 @@\n+#!/usr/bin/env python3\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('node1', main_configs=['configs/enable_keeper1.xml'], stay_alive=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+DUPLICATE_ID_CONFIG = \"\"\"\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>1</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>node1</hostname>\n+                <port>9234</port>\n+            </server>\n+            <server>\n+                <id>1</id>\n+                <hostname>node2</hostname>\n+                <port>9234</port>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\n+\"\"\"\n+\n+DUPLICATE_ENDPOINT_CONFIG = \"\"\"\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>1</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>node1</hostname>\n+                <port>9234</port>\n+            </server>\n+            <server>\n+                <id>2</id>\n+                <hostname>node1</hostname>\n+                <port>9234</port>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\n+\"\"\"\n+\n+NORMAL_CONFIG = \"\"\"\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>1</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>node1</hostname>\n+                <port>9234</port>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\n+\"\"\"\n+\n+def test_duplicate_endpoint(started_cluster):\n+    node1.stop_clickhouse()\n+    node1.replace_config(\"/etc/clickhouse-server/config.d/enable_keeper1.xml\", DUPLICATE_ENDPOINT_CONFIG)\n+\n+    with pytest.raises(Exception):\n+        node1.start_clickhouse(start_wait_sec=10)\n+\n+    node1.replace_config(\"/etc/clickhouse-server/config.d/enable_keeper1.xml\", DUPLICATE_ID_CONFIG)\n+    with pytest.raises(Exception):\n+        node1.start_clickhouse(start_wait_sec=10)\n+\n+    node1.replace_config(\"/etc/clickhouse-server/config.d/enable_keeper1.xml\", NORMAL_CONFIG)\n+    node1.start_clickhouse()\n+\n+    assert node1.query(\"SELECT 1\") == \"1\\n\"\n",
  "problem_statement": "Add assertion on incorrect raft config\n**Describe the unexpected behaviour**\r\nClickHouse Keeper reached consensus and chose the leader with incorrect configuration:\r\n``` yaml\r\nraft_configuration:\r\n          server:\r\n            - id: 0\r\n              hostname: keeper-0\r\n              port: 44444\r\n            - id: 1\r\n              hostname: keeper-0\r\n              port: 44444\r\n            - id: 2\r\n              hostname: keeper-0\r\n              port: 44444\r\n```\r\nNodes had different ids to achieve this.\r\n \r\n**Expected behavior**\r\nAdd sanity checks for raft config:\r\n- No duplicate keeper nodes \r\n- No duplicate ids\r\n\r\nWe can \r\n1) Fail if keeper/server starts with such configuration\r\n2) We should not allow to commit this configuration to log even if bad node proposed such change.\n",
  "hints_text": "",
  "created_at": "2021-12-02T11:47:44Z",
  "modified_files": [
    "src/Coordination/KeeperStateManager.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_keeper_incorrect_config/__init__.py",
    "b/tests/integration/test_keeper_incorrect_config/configs/enable_keeper1.xml",
    "b/tests/integration/test_keeper_incorrect_config/test.py"
  ]
}