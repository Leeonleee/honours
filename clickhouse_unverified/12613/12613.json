{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 12613,
  "instance_id": "ClickHouse__ClickHouse-12613",
  "issue_numbers": [
    "11572"
  ],
  "base_commit": "e3ee555c577c0b3791ebb3b891297a062b7d87fc",
  "patch": "diff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp\nindex b91004fa92fb..ce7f5cf39422 100644\n--- a/src/Common/ErrorCodes.cpp\n+++ b/src/Common/ErrorCodes.cpp\n@@ -336,7 +336,6 @@ namespace ErrorCodes\n     extern const int CURRENT_WRITE_BUFFER_IS_EXHAUSTED = 362;\n     extern const int CANNOT_CREATE_IO_BUFFER = 363;\n     extern const int RECEIVED_ERROR_TOO_MANY_REQUESTS = 364;\n-    extern const int OUTPUT_IS_NOT_SORTED = 365;\n     extern const int SIZES_OF_NESTED_COLUMNS_ARE_INCONSISTENT = 366;\n     extern const int TOO_MANY_FETCHES = 367;\n     extern const int ALL_REPLICAS_ARE_STALE = 369;\ndiff --git a/src/DataStreams/DistinctSortedBlockInputStream.cpp b/src/DataStreams/DistinctSortedBlockInputStream.cpp\nindex aac3c3742520..eab706924c1b 100644\n--- a/src/DataStreams/DistinctSortedBlockInputStream.cpp\n+++ b/src/DataStreams/DistinctSortedBlockInputStream.cpp\n@@ -9,8 +9,8 @@ namespace ErrorCodes\n }\n \n DistinctSortedBlockInputStream::DistinctSortedBlockInputStream(\n-    const BlockInputStreamPtr & input, const SizeLimits & set_size_limits_, UInt64 limit_hint_, const Names & columns)\n-    : description(input->getSortDescription())\n+    const BlockInputStreamPtr & input, SortDescription sort_description, const SizeLimits & set_size_limits_, UInt64 limit_hint_, const Names & columns)\n+    : description(std::move(sort_description))\n     , columns_names(columns)\n     , limit_hint(limit_hint_)\n     , set_size_limits(set_size_limits_)\ndiff --git a/src/DataStreams/DistinctSortedBlockInputStream.h b/src/DataStreams/DistinctSortedBlockInputStream.h\nindex 1fb8c011f6e7..146c9326e5d2 100644\n--- a/src/DataStreams/DistinctSortedBlockInputStream.h\n+++ b/src/DataStreams/DistinctSortedBlockInputStream.h\n@@ -22,7 +22,7 @@ class DistinctSortedBlockInputStream : public IBlockInputStream\n {\n public:\n     /// Empty columns_ means all columns.\n-    DistinctSortedBlockInputStream(const BlockInputStreamPtr & input, const SizeLimits & set_size_limits_, UInt64 limit_hint_, const Names & columns);\n+    DistinctSortedBlockInputStream(const BlockInputStreamPtr & input, SortDescription sort_description, const SizeLimits & set_size_limits_, UInt64 limit_hint_, const Names & columns);\n \n     String getName() const override { return \"DistinctSorted\"; }\n \n@@ -48,7 +48,7 @@ class DistinctSortedBlockInputStream : public IBlockInputStream\n         size_t rows,\n         ClearableSetVariants & variants) const;\n \n-    const SortDescription & description;\n+    SortDescription description;\n \n     struct PreviousBlock\n     {\ndiff --git a/src/DataStreams/IBlockInputStream.cpp b/src/DataStreams/IBlockInputStream.cpp\nindex 6929db45a94d..66c747207e85 100644\n--- a/src/DataStreams/IBlockInputStream.cpp\n+++ b/src/DataStreams/IBlockInputStream.cpp\n@@ -18,7 +18,6 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int QUERY_WAS_CANCELLED;\n-    extern const int OUTPUT_IS_NOT_SORTED;\n     extern const int TOO_MANY_ROWS;\n     extern const int TOO_MANY_BYTES;\n     extern const int TOO_MANY_ROWS_OR_BYTES;\n@@ -26,10 +25,6 @@ namespace ErrorCodes\n     extern const int TOO_DEEP_PIPELINE;\n }\n \n-const SortDescription & IBlockInputStream::getSortDescription() const\n-{\n-    throw Exception(\"Output of \" + getName() + \" is not sorted\", ErrorCodes::OUTPUT_IS_NOT_SORTED);\n-}\n \n /// It's safe to access children without mutex as long as these methods are called before first call to `read()` or `readPrefix()`.\n \ndiff --git a/src/DataStreams/IBlockInputStream.h b/src/DataStreams/IBlockInputStream.h\nindex dff567d15dc6..34e7bbac034f 100644\n--- a/src/DataStreams/IBlockInputStream.h\n+++ b/src/DataStreams/IBlockInputStream.h\n@@ -66,12 +66,6 @@ class IBlockInputStream : public TypePromotion<IBlockInputStream>\n         return none;\n     }\n \n-    /// If this stream generates data in order by some keys, return true.\n-    virtual bool isSortedOutput() const { return false; }\n-\n-    /// In case of isSortedOutput, return corresponding SortDescription\n-    virtual const SortDescription & getSortDescription() const;\n-\n     /** Read next block.\n       * If there are no more blocks, return an empty block (for which operator `bool` returns false).\n       * NOTE: Only one thread can read from one instance of IBlockInputStream simultaneously.\ndiff --git a/src/DataStreams/MergingSortedBlockInputStream.h b/src/DataStreams/MergingSortedBlockInputStream.h\nindex 9a732fff9471..d162fcfb0669 100644\n--- a/src/DataStreams/MergingSortedBlockInputStream.h\n+++ b/src/DataStreams/MergingSortedBlockInputStream.h\n@@ -30,9 +30,6 @@ class MergingSortedBlockInputStream : public IBlockInputStream\n \n     String getName() const override { return \"MergingSorted\"; }\n \n-    bool isSortedOutput() const override { return true; }\n-    const SortDescription & getSortDescription() const override { return description; }\n-\n     Block getHeader() const override { return header; }\n \n protected:\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex b7f400870a7e..46e1b2b9cb6e 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -802,7 +802,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n     BlockInputStreamPtr merged_stream = std::make_shared<TreeExecutorBlockInputStream>(std::move(merged_pipe));\n \n     if (deduplicate)\n-        merged_stream = std::make_shared<DistinctSortedBlockInputStream>(merged_stream, SizeLimits(), 0 /*limit_hint*/, Names());\n+        merged_stream = std::make_shared<DistinctSortedBlockInputStream>(merged_stream, sort_description, SizeLimits(), 0 /*limit_hint*/, Names());\n \n     if (need_remove_expired_values)\n         merged_stream = std::make_shared<TTLBlockInputStream>(merged_stream, data, metadata_snapshot, new_data_part, time_of_merge, force_ttl);\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01412_optimize_deduplicate_bug.reference b/tests/queries/0_stateless/01412_optimize_deduplicate_bug.reference\nnew file mode 100644\nindex 000000000000..9972842f9827\n--- /dev/null\n+++ b/tests/queries/0_stateless/01412_optimize_deduplicate_bug.reference\n@@ -0,0 +1,1 @@\n+1\t1\ndiff --git a/tests/queries/0_stateless/01412_optimize_deduplicate_bug.sql b/tests/queries/0_stateless/01412_optimize_deduplicate_bug.sql\nnew file mode 100644\nindex 000000000000..b75b31243c4c\n--- /dev/null\n+++ b/tests/queries/0_stateless/01412_optimize_deduplicate_bug.sql\n@@ -0,0 +1,10 @@\n+drop table if exists tesd_dedupl;\n+\n+create table tesd_dedupl (x UInt32, y UInt32) engine = MergeTree order by x;\n+insert into tesd_dedupl values (1, 1);\n+insert into tesd_dedupl values (1, 1);\n+\n+OPTIMIZE TABLE tesd_dedupl DEDUPLICATE;\n+select * from tesd_dedupl;\n+\n+drop table if exists tesd_dedupl;\n",
  "problem_statement": "Output of TreeExecutor is not sorted after OPTIMIZE FINAL DEDUPLICATE\n**Describe the bug**\r\nI got the exception after trying to DEDUPLICATE mv. It worked on 20.3\r\n\r\n>OPTIMIZE TABLE targeting_service_mv.ts_last_active_wallet FINAL DEDUPLICATE;\r\n\r\nru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 365, host: 168.63.64.61, port: 8123; Code: 365, e.displayText() = DB::Exception: Output of TreeExecutor is not sorted (version 20.4.4.18 (official build))\r\n\r\nIt works w/o DEDUPLICATE. And does what it intended to do, delete old records with earlier last_active\r\n\r\n**How to reproduce**\r\nClickhouse-server 20.4.4.18\r\n\r\n```\r\nCREATE TABLE queues.client_event (\r\n    `event` LowCardinality(String),\r\n    `created_at` DateTime, \r\n    `wallet_uid` String,\r\n    `device_id` String,    \r\n    `message_id` String,\r\n)\r\nENGINE = ReplacingMergeTree() PARTITION BY toYYYYMM(created_at)\r\nORDER BY (event, created_at, cityHash64(device_id, message_id))\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\n```\r\nCREATE MATERIALIZED VIEW targeting_service_mv.ts_last_active_wallet\r\nENGINE = ReplacingMergeTree(last_active)\r\nORDER BY (wallet_uid)\r\nPOPULATE\r\nAS \r\nSELECT\r\n    wallet_uid, toDate(created_at) as last_active\r\nFROM queues.client_event\r\nWHERE \r\nevent IN ('MW: Home', 'LoyaltyCard: Home', 'MW: Launch', 'MW: First-Launch', 'MW: Loading: Shown')\r\nand wallet_uid != '';\r\n```\r\n\r\nAfter creation I have inserted 12mln records in butches of 50k. Like that\r\n```\r\ninsert into targeting_service_mv.ts_last_active_wallet (wallet_uid, last_active) values (....)\r\n```\r\n\r\n**Expected behavior**\r\nOPTIMIZE executes w/o exceptions, table re merges (records with same wallet_uid but earlier last_active was deleted)\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\nru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 365, host: 168.63.64.61, port: 8123; Code: 365, e.displayText() = DB::Exception: Output of TreeExecutor is not sorted (version 20.4.4.18 (official build))\r\n\r\n\tat ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:58)\r\n\tat ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:28)\r\n\tat ru.yandex.clickhouse.ClickHouseStatementImpl.checkForErrorAndThrow(ClickHouseStatementImpl.java:875)\r\n\tat ru.yandex.clickhouse.ClickHouseStatementImpl.getInputStream(ClickHouseStatementImpl.java:616)\r\n\tat ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:117)\r\n\tat ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:100)\r\n\tat ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:95)\r\n\tat ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:90)\r\n\tat ru.yandex.clickhouse.ClickHouseStatementImpl.execute(ClickHouseStatementImpl.java:226)\r\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\r\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\r\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.executeSql(JDBCInterpreter.java:736)\r\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.interpret(JDBCInterpreter.java:819)\r\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\r\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:632)\r\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\r\n\tat org.apache.zeppelin.scheduler.ParallelScheduler$JobRunner.run(ParallelScheduler.java:162)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.Throwable: Code: 365, e.displayText() = DB::Exception: Output of TreeExecutor is not sorted (version 20.4.4.18 (official build))\r\n\r\n\tat ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:53)\r\n\t... 23 more\r\n```\n",
  "hints_text": "Looks like to have the same reason as #10389\nI'm having the same issue\nLooks like I get this on 20.5.2 as well:\r\n\r\n```sql\r\n:) CREATE TABLE foo\r\n (\r\n    timestamp DateTime CODEC(DoubleDelta, LZ4),\r\n    val UInt64\r\n )\r\n ENGINE = SummingMergeTree()\r\n PARTITION BY toYYYYMM(timestamp)\r\n ORDER BY (timestamp)\r\n\r\n:) INSERT INTO foo VALUES ('2020-01-01 00:00:00', 1)\r\n:) INSERT INTO foo VALUES ('2020-01-01 01:00:00', 2) \r\n:) INSERT INTO foo VALUES ('2020-01-01 00:00:00', 3)\r\n:) SELECT * FROM foo\r\n\r\nSELECT *\r\nFROM default.foo\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp\u2500\u252c\u2500val\u2500\u2510\r\n\u2502 2020-01-01 00:00:00 \u2502   3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp\u2500\u252c\u2500val\u2500\u2510\r\n\u2502 2020-01-01 00:00:00 \u2502   1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp\u2500\u252c\u2500val\u2500\u2510\r\n\u2502 2020-01-01 01:00:00 \u2502   2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 0.003 sec. \r\n\r\n:) OPTIMIZE TABLE default.foo DEDUPLICATE\r\n\r\nOPTIMIZE TABLE default.foo DEDUPLICATE\r\n\r\n\r\nReceived exception from server (version 20.5.2):\r\nCode: 365. DB::Exception: Received from localhost:9000. DB::Exception: Output of TreeExecutor is not sorted. \r\n\r\n0 rows in set. Elapsed: 0.001 sec. \r\n```\r\n\r\nThe trace of which is (looks like the original issue's trace is for JDBC):\r\n\r\n```sql\r\nhostname :) optimize table default.foo deduplicate\r\n\r\nOPTIMIZE TABLE default.foo DEDUPLICATE\r\n\r\n[hostname :)] 2020.07.07 10:18:35.665415 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Debug> executeQuery: (from 127.0.0.1:33642) optimize table default.foo deduplicate\r\n[hostname :)] 2020.07.07 10:18:35.665503 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Trace> ContextAccess (default): Access granted: OPTIMIZE ON default.foo\r\n[hostname :)] 2020.07.07 10:18:35.665534 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Debug> default.foo (MergerMutator): Selected 3 parts from 202001_1_1_0 to 202001_3_3_0\r\n[hostname :)] 2020.07.07 10:18:35.665578 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 405.47 GiB.\r\n[hostname :)] 2020.07.07 10:18:35.665591 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Debug> default.foo (MergerMutator): Merging 3 parts: from 202001_1_1_0 to 202001_3_3_0 into Wide\r\n[hostname :)] 2020.07.07 10:18:35.665607 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Debug> default.foo (MergerMutator): Selected MergeAlgorithm: Horizontal\r\n[hostname :)] 2020.07.07 10:18:35.665621 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Trace> MergeTreeSequentialSource: Reading 2 marks from part 202001_1_1_0, total 1 rows starting from the beginning of the part\r\n[hostname :)] 2020.07.07 10:18:35.665658 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Trace> MergeTreeSequentialSource: Reading 2 marks from part 202001_2_2_0, total 1 rows starting from the beginning of the part\r\n[hostname :)] 2020.07.07 10:18:35.665682 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Trace> MergeTreeSequentialSource: Reading 2 marks from part 202001_3_3_0, total 1 rows starting from the beginning of the part\r\n[hostname :)] 2020.07.07 10:18:35.666004 [ 6748 ] {712f0df5-d575-453c-beb5-efb4a543d40f} <Error> executeQuery: Code: 365, e.displayText() = DB::Exception: Output of TreeExecutor is not sorted (version 20.5.2.7 (official build)) (from 127.0.0.1:33642) (in query: optimize table default.foo deduplicate), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x10ed0da0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x95c923d in /usr/bin/clickhouse\r\n2. DB::IBlockInputStream::getSortDescription() const @ 0xd99c5a9 in /usr/bin/clickhouse\r\n3. DB::DistinctSortedBlockInputStream::DistinctSortedBlockInputStream(std::__1::shared_ptr<DB::IBlockInputStream> const&, DB::SizeLimits const&, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xe447d0b in /usr/bin/clickhouse\r\n4. DB::MergeTreeDataMergerMutator::mergePartsToTemporaryPart(DB::FutureMergedMutatedPart const&, DB::MergeListEntry&, DB::TableStructureReadLockHolder&, long, std::__1::unique_ptr<DB::IReservation, std::__1::default_delete<DB::IReservation> > const&, bool, bool) @ 0xe42d70c in /usr/bin/clickhouse\r\n5. DB::StorageMergeTree::merge(bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*) @ 0xe25356b in /usr/bin/clickhouse\r\n6. DB::StorageMergeTree::optimize(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&, bool, bool, DB::Context const&) @ 0xe2538e9 in /usr/bin/clickhouse\r\n7. DB::InterpreterOptimizeQuery::execute() @ 0xdd8be5a in /usr/bin/clickhouse\r\n8. ? @ 0xe074a59 in /usr/bin/clickhouse\r\n9. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xe07811a in /usr/bin/clickhouse\r\n10. DB::TCPHandler::runImpl() @ 0xe698946 in /usr/bin/clickhouse\r\n11. DB::TCPHandler::run() @ 0xe699660 in /usr/bin/clickhouse\r\n12. Poco::Net::TCPServerConnection::start() @ 0x10deebcb in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerDispatcher::run() @ 0x10def05b in /usr/bin/clickhouse\r\n14. Poco::PooledThread::run() @ 0x10f6db86 in /usr/bin/clickhouse\r\n15. Poco::ThreadImpl::runnableEntry(void*) @ 0x10f68f80 in /usr/bin/clickhouse\r\n16. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n17. /build/glibc-2ORdQG/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: clone @ 0x121a3f in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n\r\n\r\nReceived exception from server (version 20.5.2):\r\nCode: 365. DB::Exception: Received from localhost:9000. DB::Exception: Output of TreeExecutor is not sorted. \r\n\r\n0 rows in set. Elapsed: 0.001 sec. \r\n```\n@laingawbl OPTIMIZE DEDUPLICATE has sense only for simple MergeTree engine. \r\nYou should never use DEDUPLICATE against SummingMergeTree. It's the nonsense. \n@den-crane whoops, you're right. I misunderstood totally. Sorry!\nIt should not give \"Output of TreeExecutor is not sorted.\" exception nevertheless.\nSame exception on OPTIMIZE TABLE xxx DEDUPLICATE\r\n\r\n```\r\n2020.07.13 15:31:14.363796 [ 133 ] {76b755df-95c1-4771-820a-79bc0641c21d} <Error> executeQuery: Code: 365, e.displayText() = DB::Exception: Output of TreeExecutor is not sorted (version 20.5.2.7 (official build)) (from 127.0.0.1:41894) (in query: OPTIMIZE TABLE nessus.reportitems DEDUPLICATE;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x10ed0da0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x95c923d in /usr/bin/clickhouse\r\n2. DB::IBlockInputStream::getSortDescription() const @ 0xd99c5a9 in /usr/bin/clickhouse\r\n3. DB::DistinctSortedBlockInputStream::DistinctSortedBlockInputStream(std::__1::shared_ptr<DB::IBlockInputStream> const&, DB::SizeLimits const&, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xe447d0b in /usr/bin/clickhouse\r\n4. DB::MergeTreeDataMergerMutator::mergePartsToTemporaryPart(DB::FutureMergedMutatedPart const&, DB::MergeListEntry&, DB::TableStructureReadLockHolder&, long, std::__1::unique_ptr<DB::IReservation, std::__1::default_delete<DB::IReservation> > const&, bool, bool) @ 0xe42d70c in /usr/bin/clickhouse\r\n5. DB::StorageMergeTree::merge(bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*) @ 0xe25356b in /usr/bin/clickhouse\r\n6. DB::StorageMergeTree::optimize(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&, bool, bool, DB::Context const&) @ 0xe2538e9 in /usr/bin/clickhouse\r\n7. DB::InterpreterOptimizeQuery::execute() @ 0xdd8be5a in /usr/bin/clickhouse\r\n8. ? @ 0xe074a59 in /usr/bin/clickhouse\r\n9. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xe07811a in /usr/bin/clickhouse\r\n10. DB::TCPHandler::runImpl() @ 0xe698946 in /usr/bin/clickhouse\r\n11. DB::TCPHandler::run() @ 0xe699660 in /usr/bin/clickhouse\r\n12. Poco::Net::TCPServerConnection::start() @ 0x10deebcb in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerDispatcher::run() @ 0x10def05b in /usr/bin/clickhouse\r\n14. Poco::PooledThread::run() @ 0x10f6db86 in /usr/bin/clickhouse\r\n15. Poco::ThreadImpl::runnableEntry(void*) @ 0x10f68f80 in /usr/bin/clickhouse\r\n16. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n17. __clone @ 0x12188f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n```",
  "created_at": "2020-07-21T08:23:26Z"
}