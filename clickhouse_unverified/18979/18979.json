{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 18979,
  "instance_id": "ClickHouse__ClickHouse-18979",
  "issue_numbers": [
    "18690"
  ],
  "base_commit": "8004b0446f257984b58d2c3eb2ed019671388fdd",
  "patch": "diff --git a/src/IO/PeekableReadBuffer.cpp b/src/IO/PeekableReadBuffer.cpp\nindex 8ad0e7b572eb..11ca9cd6a0b4 100644\n--- a/src/IO/PeekableReadBuffer.cpp\n+++ b/src/IO/PeekableReadBuffer.cpp\n@@ -4,13 +4,11 @@ namespace DB\n {\n namespace ErrorCodes\n {\n-    extern const int MEMORY_LIMIT_EXCEEDED;\n     extern const int LOGICAL_ERROR;\n }\n \n-PeekableReadBuffer::PeekableReadBuffer(ReadBuffer & sub_buf_, size_t start_size_ /*= DBMS_DEFAULT_BUFFER_SIZE*/,\n-                                                              size_t unread_limit_ /* = default_limit*/)\n-        : BufferWithOwnMemory(start_size_), sub_buf(sub_buf_), unread_limit(unread_limit_)\n+PeekableReadBuffer::PeekableReadBuffer(ReadBuffer & sub_buf_, size_t start_size_ /*= DBMS_DEFAULT_BUFFER_SIZE*/)\n+        : BufferWithOwnMemory(start_size_), sub_buf(sub_buf_)\n {\n     padded &= sub_buf.isPadded();\n     /// Read from sub-buffer\n@@ -191,8 +189,6 @@ void PeekableReadBuffer::checkStateCorrect() const\n     }\n     if (currentlyReadFromOwnMemory() && !peeked_size)\n         throw DB::Exception(\"Pos in empty own buffer\", ErrorCodes::LOGICAL_ERROR);\n-    if (unread_limit < memory.size())\n-        throw DB::Exception(\"Size limit exceed\", ErrorCodes::LOGICAL_ERROR);\n }\n \n void PeekableReadBuffer::resizeOwnMemoryIfNecessary(size_t bytes_to_append)\n@@ -222,16 +218,11 @@ void PeekableReadBuffer::resizeOwnMemoryIfNecessary(size_t bytes_to_append)\n         }\n         else\n         {\n-            if (unread_limit < new_size)\n-                throw DB::Exception(\"PeekableReadBuffer: Memory limit exceed\", ErrorCodes::MEMORY_LIMIT_EXCEEDED);\n-\n             size_t pos_offset = pos - memory.data();\n \n             size_t new_size_amortized = memory.size() * 2;\n             if (new_size_amortized < new_size)\n                 new_size_amortized = new_size;\n-            else if (unread_limit < new_size_amortized)\n-                new_size_amortized = unread_limit;\n             memory.resize(new_size_amortized);\n \n             if (need_update_checkpoint)\ndiff --git a/src/IO/PeekableReadBuffer.h b/src/IO/PeekableReadBuffer.h\nindex 62b6f08f6218..3088e59d94c7 100644\n--- a/src/IO/PeekableReadBuffer.h\n+++ b/src/IO/PeekableReadBuffer.h\n@@ -20,8 +20,7 @@ class PeekableReadBuffer : public BufferWithOwnMemory<ReadBuffer>\n {\n     friend class PeekableReadBufferCheckpoint;\n public:\n-    explicit PeekableReadBuffer(ReadBuffer & sub_buf_, size_t start_size_ = DBMS_DEFAULT_BUFFER_SIZE,\n-                                                       size_t unread_limit_ = 16 * DBMS_DEFAULT_BUFFER_SIZE);\n+    explicit PeekableReadBuffer(ReadBuffer & sub_buf_, size_t start_size_ = DBMS_DEFAULT_BUFFER_SIZE);\n \n     ~PeekableReadBuffer() override;\n \n@@ -95,7 +94,6 @@ class PeekableReadBuffer : public BufferWithOwnMemory<ReadBuffer>\n \n \n     ReadBuffer & sub_buf;\n-    const size_t unread_limit;\n     size_t peeked_size = 0;\n     Position checkpoint = nullptr;\n     bool checkpoint_in_own_memory = false;\n",
  "test_patch": "diff --git a/src/IO/tests/gtest_peekable_read_buffer.cpp b/src/IO/tests/gtest_peekable_read_buffer.cpp\nindex fb4b0b799b48..8c491338bd3d 100644\n--- a/src/IO/tests/gtest_peekable_read_buffer.cpp\n+++ b/src/IO/tests/gtest_peekable_read_buffer.cpp\n@@ -9,7 +9,6 @@\n namespace DB::ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n-    extern const int MEMORY_LIMIT_EXCEEDED;\n }\n \n static void readAndAssert(DB::ReadBuffer & buf, const char * str)\n@@ -40,7 +39,7 @@ try\n     DB::ReadBufferFromString b4(s4);\n \n     DB::ConcatReadBuffer concat({&b1, &b2, &b3, &b4});\n-    DB::PeekableReadBuffer peekable(concat, 0, 16);\n+    DB::PeekableReadBuffer peekable(concat, 0);\n \n     ASSERT_TRUE(!peekable.eof());\n     assertAvailable(peekable, \"0123456789\");\n@@ -48,6 +47,8 @@ try\n         DB::PeekableReadBufferCheckpoint checkpoint{peekable};\n         readAndAssert(peekable, \"01234\");\n     }\n+\n+#ifndef ABORT_ON_LOGICAL_ERROR\n     bool exception = false;\n     try\n     {\n@@ -60,6 +61,7 @@ try\n         exception = true;\n     }\n     ASSERT_TRUE(exception);\n+#endif\n     assertAvailable(peekable, \"56789\");\n \n     readAndAssert(peekable, \"56\");\n@@ -70,19 +72,10 @@ try\n     peekable.dropCheckpoint();\n     assertAvailable(peekable, \"789\");\n \n-    exception = false;\n-    try\n     {\n         DB::PeekableReadBufferCheckpoint checkpoint{peekable, true};\n-        peekable.ignore(30);\n-    }\n-    catch (DB::Exception & e)\n-    {\n-        if (e.code() != DB::ErrorCodes::MEMORY_LIMIT_EXCEEDED)\n-            throw;\n-        exception = true;\n+        peekable.ignore(20);\n     }\n-    ASSERT_TRUE(exception);\n     assertAvailable(peekable, \"789qwertyuiop\");\n \n     readAndAssert(peekable, \"789qwertyu\");\ndiff --git a/tests/queries/0_stateless/01184_insert_values_huge_strings.reference b/tests/queries/0_stateless/01184_insert_values_huge_strings.reference\nnew file mode 100644\nindex 000000000000..1c42cb6d5edf\n--- /dev/null\n+++ b/tests/queries/0_stateless/01184_insert_values_huge_strings.reference\n@@ -0,0 +1,3 @@\n+1000100\n+1000100\n+1000100\ndiff --git a/tests/queries/0_stateless/01184_insert_values_huge_strings.sh b/tests/queries/0_stateless/01184_insert_values_huge_strings.sh\nnew file mode 100755\nindex 000000000000..9b63f401a59f\n--- /dev/null\n+++ b/tests/queries/0_stateless/01184_insert_values_huge_strings.sh\n@@ -0,0 +1,20 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT -q \"drop table if exists huge_strings\"\n+$CLICKHOUSE_CLIENT -q \"create table huge_strings (n UInt64, l UInt64, s String, h UInt64) engine=MergeTree order by n\"\n+\n+for _ in {1..10}; do\n+  $CLICKHOUSE_CLIENT -q \"select number, (rand() % 100*1000*1000) as l, repeat(randomString(l/1000/1000), 1000*1000) as s, cityHash64(s) from numbers(10) format Values\" | $CLICKHOUSE_CLIENT -q \"insert into huge_strings values\" &\n+  $CLICKHOUSE_CLIENT -q \"select number % 10, (rand() % 100) as l, randomString(l) as s, cityHash64(s) from numbers(100000)\" | $CLICKHOUSE_CLIENT -q \"insert into huge_strings format TSV\" &\n+done;\n+wait\n+\n+$CLICKHOUSE_CLIENT -q \"select count() from huge_strings\"\n+$CLICKHOUSE_CLIENT -q \"select sum(l = length(s)) from huge_strings\"\n+$CLICKHOUSE_CLIENT -q \"select sum(h = cityHash64(s)) from huge_strings\"\n+\n+$CLICKHOUSE_CLIENT -q \"drop table huge_strings\"\n",
  "problem_statement": "PeekableReadBuffer: Memory limit exceed when inserting data by HTTP (20.8)\nError started to appear after migrated 19.9 -> 20.8\r\n\r\nSettings\r\n<input_format_parallel_parsing>0</input_format_parallel_parsing>\r\nand\r\n<input_format_values_interpret_expressions>0</input_format_values_interpret_expressions>\r\n\r\ndo not help\r\n\r\n```\r\n2021.01.03 08:01:15.601198 [ 1968 ] {b4472a5e-e463-4852-a0ed-efd90fae6393} <Error> DynamicQueryHandler: Code: 241, e.displayText() = DB::Exception: PeekableReadBuffer: Memory limit exceed, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x18cd2050 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xe63232d in /usr/bin/clickhouse\r\n2. ? @ 0x164be2e2 in /usr/bin/clickhouse\r\n3. DB::PeekableReadBuffer::peekNext() @ 0x164bd633 in /usr/bin/clickhouse\r\n4. DB::PeekableReadBuffer::nextImpl() @ 0x164bd978 in /usr/bin/clickhouse\r\n5. void DB::readQuotedStringInto<true, DB::PODArray<char8_t, 4096ul, Allocator<false, false>, 15ul, 16ul> >(DB::PODArray<char8_t, 4096ul, Allocator<false, false>, 15ul, 16ul>&, DB::ReadBuffer&) @ 0xe67e4f5 in /usr/bin/clickhouse\r\n6. DB::DataTypeString::deserializeTextQuoted(DB::IColumn&, DB::ReadBuffer&, DB::FormatSettings const&) const @ 0x156445a4 in /usr/bin/clickhouse\r\n7. DB::ValuesBlockInputFormat::readRow(std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, unsigned long) @ 0x16597b4b in /usr/bin/clickhouse\r\n8. DB::ValuesBlockInputFormat::generate() @ 0x165982cd in /usr/bin/clickhouse\r\n9. DB::ISource::work() @ 0x163e821b in /usr/bin/clickhouse\r\n10. DB::InputStreamFromInputFormat::readImpl() @ 0x163ab46d in /usr/bin/clickhouse\r\n11. DB::IBlockInputStream::read() @ 0x155611cd in /usr/bin/clickhouse\r\n12. DB::InputStreamFromASTInsertQuery::readImpl() @ 0x15973f79 in /usr/bin/clickhouse\r\n13. DB::IBlockInputStream::read() @ 0x155611cd in /usr/bin/clickhouse\r\n14. DB::copyData(DB::IBlockInputStream&, DB::IBlockOutputStream&, std::__1::atomic<bool>*) @ 0x1558415e in /usr/bin/clickhouse\r\n15. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>) @ 0x15d06285 in /usr/bin/clickhouse\r\n16. DB::HTTPHandler::processQuery(DB::Context&, Poco::Net::HTTPServerRequest&, HTMLForm&, Poco::Net::HTTPServerResponse&, DB::HTTPHandler::Output&) @ 0x16344c69 in /usr/bin/clickhouse\r\n17. DB::HTTPHandler::handleRequest(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&) @ 0x1634871b in /usr/bin/clickhouse\r\n18. Poco::Net::HTTPServerConnection::run() @ 0x18bb1df3 in /usr/bin/clickhouse\r\n19. Poco::Net::TCPServerConnection::start() @ 0x18bf000b in /usr/bin/clickhouse\r\n20. Poco::Net::TCPServerDispatcher::run() @ 0x18bf0728 in /usr/bin/clickhouse\r\n21. Poco::PooledThread::run() @ 0x18d6ee36 in /usr/bin/clickhouse\r\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x18d6a230 in /usr/bin/clickhouse\r\n23. start_thread @ 0x7494 in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n24. clone @ 0xe8aff in /lib/x86_64-linux-gnu/libc-2.24.so\r\n (version 20.8.11.17 (official build))\r\n\r\n```\n",
  "hints_text": "Do you have reproducible example? DDL ? Data sample? Steps?\n```\r\nCREATE TABLE IF NOT EXISTS bitcoinsv_etl.outputs ON CLUSTER replicated3\r\n(\r\n  tx_date Date,\r\n  tx_time DateTime,\r\n\r\n  blockchain_id UInt32,\r\n  block UInt32,\r\n\r\n  tx_id String,\r\n  tx_index UInt16, -- 0-based index of transaction in block\r\n\r\n  output_index UInt16, -- 0-based index of output in transaction\r\n\r\n  address String,  -- Base58 output address\r\n\r\n  script_type LowCardinality(String),\r\n  script_template UInt32, -- from dictionary script_templates\r\n  script String,\r\n  req_sigs UInt32, -- ReqSigs field\r\n\r\n  value Decimal64(8), -- output amount in BTC\r\n\r\n  direction Enum8('unknown' = 0, 'not_change' = 1, 'change' = 2, 'likely_not_change' = 3, 'likely_change' = 4, 'mining' = 5, 'fee' = 6)\r\n\r\n) ENGINE = ReplicatedMergeTree('/clickhouse/archive_servers/tables/bitcoinsv_etl/outputs', '{replica}')\r\n  ORDER BY block\r\n  TTL tx_date + INTERVAL 1 MONTH\r\n  SETTINGS index_granularity=8192;\r\n```\r\n\r\n\r\n```\r\nINSERT INTO bitcoinsv_etl.outputs(tx_date,tx_time,blockchain_id,block,tx_id,tx_index,output_index,address,script_type,script_template,script,req_sigs,value,direction) VALUES\r\n```\r\n\r\n[data_1609667508.txt](https://github.com/ClickHouse/ClickHouse/files/5762399/data_1609667508.txt)\r\n\n```\r\ncurl 'http://localhost:8123/?query=INSERT+INTO+outputs+VALUES ' --data-binary @data_1609667508.txt\r\nCode: 241, e.displayText() = DB::Exception: PeekableReadBuffer: Memory limit exceed (version 20.13.1.5579)\r\n\r\n\r\n\r\n\r\necho 'INSERT INTO outputs  VALUES'`cat data_1609667508.txt ` |clickhouse-client\r\n\r\nSELECT count() FROM outputs\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    9330 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\nThe error happens because data contain huge string field (at row 8244), it's about 18 MiB. `PeekableReadBuffer` has limitation of maximum 16 MiB for one field, because in most cases fields of such size mean that something is wrong with parsing (for example, quotes are not closed, so parser reads everything until EOF as one field). We can increase or remove this limitation. However, it's usually a bad practice to store blobs in database.\nMay be better to configure it?\n@tavplubix Let's increase it to 1 GiB or remove completely (rely on memory limit).\r\n\r\n> However, it's usually a bad practice to store blobs in database.\r\n\r\nI challenge this limitation in this PR #18842 - there are BLOBs of 500 MiB and everything is ok so far...\nOk.\r\n>> However, it's usually a bad practice to store blobs in database.\r\n>\r\n> there are BLOBs of 500 MiB and everything is ok so far...\r\n\r\nWe had an issue with blobs, it might cause merges to fail with memory limit on huge granules. It's almost fixed in #17120, but AFAIK it's still reproducible in very rare cases.",
  "created_at": "2021-01-12T18:57:33Z",
  "modified_files": [
    "src/IO/PeekableReadBuffer.cpp",
    "src/IO/PeekableReadBuffer.h"
  ],
  "modified_test_files": [
    "src/IO/tests/gtest_peekable_read_buffer.cpp",
    "b/tests/queries/0_stateless/01184_insert_values_huge_strings.reference",
    "b/tests/queries/0_stateless/01184_insert_values_huge_strings.sh"
  ]
}