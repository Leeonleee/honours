{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 68837,
  "instance_id": "ClickHouse__ClickHouse-68837",
  "issue_numbers": [
    "67384"
  ],
  "base_commit": "af9291e613b6dd6c747c7ef887e64ee9b9b54387",
  "patch": "diff --git a/src/Interpreters/AsynchronousInsertQueue.cpp b/src/Interpreters/AsynchronousInsertQueue.cpp\nindex 8522b1387424..461700dcb75d 100644\n--- a/src/Interpreters/AsynchronousInsertQueue.cpp\n+++ b/src/Interpreters/AsynchronousInsertQueue.cpp\n@@ -33,6 +33,8 @@\n #include <Common/SensitiveDataMasker.h>\n #include <Common/SipHash.h>\n #include <Common/logger_useful.h>\n+#include <Parsers/ASTExpressionList.h>\n+#include <Parsers/ASTIdentifier.h>\n \n namespace CurrentMetrics\n {\n@@ -308,6 +310,7 @@ void AsynchronousInsertQueue::preprocessInsertQuery(const ASTPtr & query, const\n         /* no_squash */ false,\n         /* no_destination */ false,\n         /* async_insert */ false);\n+\n     auto table = interpreter.getTable(insert_query);\n     auto sample_block = InterpreterInsertQuery::getSampleBlock(insert_query, table, table->getInMemoryMetadataPtr(), query_context);\n \n@@ -318,6 +321,10 @@ void AsynchronousInsertQueue::preprocessInsertQuery(const ASTPtr & query, const\n     /// InterpreterInsertQuery::getTable() -> ITableFunction::execute().\n     if (insert_query.table_id)\n         query_context->checkAccess(AccessType::INSERT, insert_query.table_id, sample_block.getNames());\n+\n+    insert_query.columns = std::make_shared<ASTExpressionList>();\n+    for (const auto & column : sample_block)\n+        insert_query.columns->children.push_back(std::make_shared<ASTIdentifier>(column.name));\n }\n \n AsynchronousInsertQueue::PushResult\n@@ -696,6 +703,17 @@ catch (...)\n     tryLogCurrentException(\"AsynchronousInsertQueue\", \"Failed to add elements to AsynchronousInsertLog\");\n }\n \n+void convertBlockToHeader(Block & block, const Block & header)\n+{\n+    auto converting_dag = ActionsDAG::makeConvertingActions(\n+        block.getColumnsWithTypeAndName(),\n+        header.getColumnsWithTypeAndName(),\n+        ActionsDAG::MatchColumnsMode::Name);\n+\n+    auto converting_actions = std::make_shared<ExpressionActions>(std::move(converting_dag));\n+    converting_actions->execute(block);\n+}\n+\n String serializeQuery(const IAST & query, size_t max_length)\n {\n     return query.hasSecretParts()\n@@ -791,6 +809,61 @@ try\n     if (async_insert_log)\n         log_elements.reserve(data->entries.size());\n \n+    auto add_entry_to_asynchronous_insert_log = [&, query_by_format = NameToNameMap{}](\n+        const InsertData::EntryPtr & entry,\n+        const String & parsing_exception,\n+        size_t num_rows,\n+        size_t num_bytes) mutable\n+    {\n+        if (!async_insert_log)\n+            return;\n+\n+        AsynchronousInsertLogElement elem;\n+        elem.event_time = timeInSeconds(entry->create_time);\n+        elem.event_time_microseconds = timeInMicroseconds(entry->create_time);\n+        elem.database = query_database;\n+        elem.table = query_table;\n+        elem.format = entry->format;\n+        elem.query_id = entry->query_id;\n+        elem.bytes = num_bytes;\n+        elem.rows = num_rows;\n+        elem.exception = parsing_exception;\n+        elem.data_kind = entry->chunk.getDataKind();\n+        elem.timeout_milliseconds = data->timeout_ms.count();\n+        elem.flush_query_id = insert_query_id;\n+\n+        auto get_query_by_format = [&](const String & format) -> const String &\n+        {\n+            auto [it, inserted] = query_by_format.try_emplace(format);\n+            if (!inserted)\n+                return it->second;\n+\n+            auto query = key.query->clone();\n+            assert_cast<ASTInsertQuery &>(*query).format = format;\n+            it->second = serializeQuery(*query, insert_context->getSettingsRef().log_queries_cut_to_length);\n+            return it->second;\n+        };\n+\n+        if (entry->chunk.getDataKind() == DataKind::Parsed)\n+            elem.query_for_logging = key.query_str;\n+        else\n+            elem.query_for_logging = get_query_by_format(entry->format);\n+\n+        /// If there was a parsing error,\n+        /// the entry won't be flushed anyway,\n+        /// so add the log element immediately.\n+        if (!elem.exception.empty())\n+        {\n+            elem.status = AsynchronousInsertLogElement::ParsingError;\n+            async_insert_log->add(std::move(elem));\n+        }\n+        else\n+        {\n+            elem.status = AsynchronousInsertLogElement::Ok;\n+            log_elements.push_back(std::move(elem));\n+        }\n+    };\n+\n     try\n     {\n         interpreter = std::make_unique<InterpreterInsertQuery>(\n@@ -819,49 +892,20 @@ try\n     catch (...)\n     {\n         logExceptionBeforeStart(query_for_logging, insert_context, key.query, query_span, start_watch.elapsedMilliseconds());\n-        throw;\n-    }\n-\n-    auto add_entry_to_asynchronous_insert_log = [&](const auto & entry,\n-                                                    const auto & entry_query_for_logging,\n-                                                    const auto & exception,\n-                                                    size_t num_rows,\n-                                                    size_t num_bytes,\n-                                                    Milliseconds timeout_ms)\n-    {\n-        if (!async_insert_log)\n-            return;\n \n-        AsynchronousInsertLogElement elem;\n-        elem.event_time = timeInSeconds(entry->create_time);\n-        elem.event_time_microseconds = timeInMicroseconds(entry->create_time);\n-        elem.query_for_logging = entry_query_for_logging;\n-        elem.database = query_database;\n-        elem.table = query_table;\n-        elem.format = entry->format;\n-        elem.query_id = entry->query_id;\n-        elem.bytes = num_bytes;\n-        elem.rows = num_rows;\n-        elem.exception = exception;\n-        elem.data_kind = entry->chunk.getDataKind();\n-        elem.timeout_milliseconds = timeout_ms.count();\n-        elem.flush_query_id = insert_query_id;\n-\n-        /// If there was a parsing error,\n-        /// the entry won't be flushed anyway,\n-        /// so add the log element immediately.\n-        if (!elem.exception.empty())\n-        {\n-            elem.status = AsynchronousInsertLogElement::ParsingError;\n-            async_insert_log->add(std::move(elem));\n-        }\n-        else\n+        if (async_insert_log)\n         {\n-            log_elements.push_back(elem);\n+            for (const auto & entry : data->entries)\n+                add_entry_to_asynchronous_insert_log(entry, /*parsing_exception=*/ \"\", /*num_rows=*/ 0, entry->chunk.byteSize());\n+\n+            auto exception = getCurrentExceptionMessage(false);\n+            auto flush_time = std::chrono::system_clock::now();\n+            appendElementsToLogSafe(*async_insert_log, std::move(log_elements), flush_time, exception);\n         }\n-    };\n+        throw;\n+    }\n \n-    auto finish_entries = [&]\n+    auto finish_entries = [&](size_t num_rows, size_t num_bytes)\n     {\n         for (const auto & entry : data->entries)\n         {\n@@ -874,20 +918,7 @@ try\n             auto flush_time = std::chrono::system_clock::now();\n             appendElementsToLogSafe(*async_insert_log, std::move(log_elements), flush_time, \"\");\n         }\n-    };\n-\n-    Chunk chunk;\n-    auto header = pipeline.getHeader();\n-\n-    if (key.data_kind == DataKind::Parsed)\n-        chunk = processEntriesWithParsing(key, data, header, insert_context, log, add_entry_to_asynchronous_insert_log);\n-    else\n-        chunk = processPreprocessedEntries(key, data, header, insert_context, add_entry_to_asynchronous_insert_log);\n \n-    ProfileEvents::increment(ProfileEvents::AsyncInsertRows, chunk.getNumRows());\n-\n-    auto log_and_add_finish_to_query_log = [&](size_t num_rows, size_t num_bytes)\n-    {\n         LOG_DEBUG(log, \"Flushed {} rows, {} bytes for query '{}'\", num_rows, num_bytes, key.query_str);\n         queue_shard_flush_time_history.updateWithCurrentTime();\n \n@@ -896,16 +927,24 @@ try\n             query_log_elem, insert_context, key.query, pipeline, pulling_pipeline, query_span, QueryCache::Usage::None, internal);\n     };\n \n-\n-    if (chunk.getNumRows() == 0)\n-    {\n-        finish_entries();\n-        log_and_add_finish_to_query_log(0, 0);\n-        return;\n-    }\n-\n     try\n     {\n+        Chunk chunk;\n+        auto header = pipeline.getHeader();\n+\n+        if (key.data_kind == DataKind::Parsed)\n+            chunk = processEntriesWithParsing(key, data, header, insert_context, log, add_entry_to_asynchronous_insert_log);\n+        else\n+            chunk = processPreprocessedEntries(data, header, add_entry_to_asynchronous_insert_log);\n+\n+        ProfileEvents::increment(ProfileEvents::AsyncInsertRows, chunk.getNumRows());\n+\n+        if (chunk.getNumRows() == 0)\n+        {\n+            finish_entries(/*num_rows=*/ 0, /*num_bytes=*/ 0);\n+            return;\n+        }\n+\n         size_t num_rows = chunk.getNumRows();\n         size_t num_bytes = chunk.bytes();\n \n@@ -915,7 +954,7 @@ try\n         CompletedPipelineExecutor completed_executor(pipeline);\n         completed_executor.execute();\n \n-        log_and_add_finish_to_query_log(num_rows, num_bytes);\n+        finish_entries(num_rows, num_bytes);\n     }\n     catch (...)\n     {\n@@ -929,8 +968,6 @@ try\n         }\n         throw;\n     }\n-\n-    finish_entries();\n }\n catch (const Exception & e)\n {\n@@ -991,7 +1028,6 @@ Chunk AsynchronousInsertQueue::processEntriesWithParsing(\n \n     StreamingFormatExecutor executor(header, format, std::move(on_error), std::move(adding_defaults_transform));\n     auto chunk_info = std::make_shared<AsyncInsertInfo>();\n-    auto query_for_logging = serializeQuery(*key.query, insert_context->getSettingsRef().log_queries_cut_to_length);\n \n     for (const auto & entry : data->entries)\n     {\n@@ -1009,7 +1045,8 @@ Chunk AsynchronousInsertQueue::processEntriesWithParsing(\n         size_t num_rows = executor.execute(*buffer);\n \n         total_rows += num_rows;\n-        /// for some reason, client can pass zero rows and bytes to server.\n+\n+        /// For some reason, client can pass zero rows and bytes to server.\n         /// We don't update offsets in this case, because we assume every insert has some rows during dedup\n         /// but we have nothing to deduplicate for this insert.\n         if (num_rows > 0)\n@@ -1018,8 +1055,7 @@ Chunk AsynchronousInsertQueue::processEntriesWithParsing(\n             chunk_info->tokens.push_back(entry->async_dedup_token);\n         }\n \n-        add_to_async_insert_log(entry, query_for_logging, current_exception, num_rows, num_bytes, data->timeout_ms);\n-\n+        add_to_async_insert_log(entry, current_exception, num_rows, num_bytes);\n         current_exception.clear();\n         entry->resetChunk();\n     }\n@@ -1031,30 +1067,14 @@ Chunk AsynchronousInsertQueue::processEntriesWithParsing(\n \n template <typename LogFunc>\n Chunk AsynchronousInsertQueue::processPreprocessedEntries(\n-    const InsertQuery & key,\n     const InsertDataPtr & data,\n     const Block & header,\n-    const ContextPtr & insert_context,\n     LogFunc && add_to_async_insert_log)\n {\n     size_t total_rows = 0;\n     auto chunk_info = std::make_shared<AsyncInsertInfo>();\n     auto result_columns = header.cloneEmptyColumns();\n \n-    std::unordered_map<String, String> format_to_query;\n-\n-    auto get_query_by_format = [&](const String & format) -> const String &\n-    {\n-        auto [it, inserted] = format_to_query.try_emplace(format);\n-        if (!inserted)\n-            return it->second;\n-\n-        auto query = key.query->clone();\n-        assert_cast<ASTInsertQuery &>(*query).format = format;\n-        it->second = serializeQuery(*query, insert_context->getSettingsRef().log_queries_cut_to_length);\n-        return it->second;\n-    };\n-\n     for (const auto & entry : data->entries)\n     {\n         const auto * block = entry->chunk.asBlock();\n@@ -1062,23 +1082,26 @@ Chunk AsynchronousInsertQueue::processPreprocessedEntries(\n             throw Exception(ErrorCodes::LOGICAL_ERROR,\n                 \"Expected entry with data kind Preprocessed. Got: {}\", entry->chunk.getDataKind());\n \n-        auto columns = block->getColumns();\n+        Block block_to_insert = *block;\n+        if (!isCompatibleHeader(block_to_insert, header))\n+            convertBlockToHeader(block_to_insert, header);\n+\n+        auto columns = block_to_insert.getColumns();\n         for (size_t i = 0, s = columns.size(); i < s; ++i)\n             result_columns[i]->insertRangeFrom(*columns[i], 0, columns[i]->size());\n \n-        total_rows += block->rows();\n-        /// for some reason, client can pass zero rows and bytes to server.\n+        total_rows += block_to_insert.rows();\n+\n+        /// For some reason, client can pass zero rows and bytes to server.\n         /// We don't update offsets in this case, because we assume every insert has some rows during dedup,\n         /// but we have nothing to deduplicate for this insert.\n-        if (block->rows())\n+        if (block_to_insert.rows() > 0)\n         {\n             chunk_info->offsets.push_back(total_rows);\n             chunk_info->tokens.push_back(entry->async_dedup_token);\n         }\n \n-        const auto & query_for_logging = get_query_by_format(entry->format);\n-        add_to_async_insert_log(entry, query_for_logging, \"\", block->rows(), block->bytes(), data->timeout_ms);\n-\n+        add_to_async_insert_log(entry, /*parsing_exception=*/ \"\", block_to_insert.rows(), block_to_insert.bytes());\n         entry->resetChunk();\n     }\n \ndiff --git a/src/Interpreters/AsynchronousInsertQueue.h b/src/Interpreters/AsynchronousInsertQueue.h\nindex 17607ac18794..ecb7757d4061 100644\n--- a/src/Interpreters/AsynchronousInsertQueue.h\n+++ b/src/Interpreters/AsynchronousInsertQueue.h\n@@ -288,10 +288,8 @@ class AsynchronousInsertQueue : public WithContext\n \n     template <typename LogFunc>\n     static Chunk processPreprocessedEntries(\n-        const InsertQuery & key,\n         const InsertDataPtr & data,\n         const Block & header,\n-        const ContextPtr & insert_context,\n         LogFunc && add_to_async_insert_log);\n \n     template <typename E>\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02790_async_queries_in_query_log.reference b/tests/queries/0_stateless/02790_async_queries_in_query_log.reference\nindex 567e8d4f4b5d..84254cc87355 100644\n--- a/tests/queries/0_stateless/02790_async_queries_in_query_log.reference\n+++ b/tests/queries/0_stateless/02790_async_queries_in_query_log.reference\n@@ -9,7 +9,7 @@ written_rows:   0\n written_bytes:  0\n result_rows:    0\n result_bytes:   0\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing']\n@@ -26,7 +26,7 @@ written_rows:   4\n written_bytes:  16\n result_rows:    4\n result_bytes:   16\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing']\n@@ -54,7 +54,7 @@ written_rows:   0\n written_bytes:  0\n result_rows:    0\n result_bytes:   0\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing','default.async_insert_target']\n@@ -71,7 +71,7 @@ written_rows:   6\n written_bytes:  24\n result_rows:    6\n result_bytes:   24\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing','default.async_insert_target']\n@@ -118,7 +118,7 @@ written_rows:   0\n written_bytes:  0\n result_rows:    0\n result_bytes:   0\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing','default.async_insert_target']\n@@ -135,7 +135,7 @@ written_rows:   3\n written_bytes:  12\n result_rows:    0\n result_bytes:   0\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 1, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing','default.async_insert_target']\ndiff --git a/tests/queries/0_stateless/03148_async_queries_in_query_log_errors.reference b/tests/queries/0_stateless/03148_async_queries_in_query_log_errors.reference\nindex 685d28268f6d..7cc2efd50ec1 100644\n--- a/tests/queries/0_stateless/03148_async_queries_in_query_log_errors.reference\n+++ b/tests/queries/0_stateless/03148_async_queries_in_query_log_errors.reference\n@@ -4,7 +4,7 @@ Row 1:\n \u2500\u2500\u2500\u2500\u2500\u2500\n database:                 default\n table:                    async_insert_landing\n-query:                    INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 0, async_insert = 1 FORMAT Values\n+query:                    INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 0, async_insert = 1 FORMAT Values\n format:                   Values\n error:                    DB::Exc*****on: Cannot parse string 'Invalid' as UInt32:\n populated_flush_query_id: 1\n@@ -18,7 +18,7 @@ written_rows:   0\n written_bytes:  0\n result_rows:    0\n result_bytes:   0\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 0, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 0, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing']\n@@ -35,7 +35,7 @@ written_rows:   0\n written_bytes:  0\n result_rows:    0\n result_bytes:   0\n-query:          INSERT INTO default.async_insert_landing SETTINGS wait_for_async_insert = 0, async_insert = 1 FORMAT Values\n+query:          INSERT INTO default.async_insert_landing (id) SETTINGS wait_for_async_insert = 0, async_insert = 1 FORMAT Values\n query_kind:     AsyncInsertFlush\n databases:      ['default']\n tables:         ['default.async_insert_landing']\ndiff --git a/tests/queries/0_stateless/03229_async_insert_alter.reference b/tests/queries/0_stateless/03229_async_insert_alter.reference\nnew file mode 100644\nindex 000000000000..f66021d0bfec\n--- /dev/null\n+++ b/tests/queries/0_stateless/03229_async_insert_alter.reference\n@@ -0,0 +1,8 @@\n+42\t24\t0\n+42\t24\t0\n+43\t34\t55\n+42\t24\n+43\t34\n+INSERT INTO default.t_async_insert_alter (id, v1) FORMAT Values\tPreprocessed\tOk\n+INSERT INTO default.t_async_insert_alter (id, v1, value2) FORMAT Values\tPreprocessed\tOk\n+INSERT INTO default.t_async_insert_alter (id, v1, value2) FORMAT Values\tPreprocessed\tFlushError\ndiff --git a/tests/queries/0_stateless/03229_async_insert_alter.sql b/tests/queries/0_stateless/03229_async_insert_alter.sql\nnew file mode 100644\nindex 000000000000..022e386bef0b\n--- /dev/null\n+++ b/tests/queries/0_stateless/03229_async_insert_alter.sql\n@@ -0,0 +1,47 @@\n+-- Tags: no-parallel\n+-- no-parallel because the test uses FLUSH ASYNC INSERT QUEUE\n+\n+SET wait_for_async_insert = 0;\n+SET async_insert_busy_timeout_max_ms = 300000;\n+SET async_insert_busy_timeout_min_ms = 300000;\n+SET async_insert_use_adaptive_busy_timeout = 0;\n+\n+DROP TABLE IF EXISTS t_async_insert_alter;\n+\n+CREATE TABLE t_async_insert_alter (id Int64, v1 Int64) ENGINE = MergeTree ORDER BY id SETTINGS async_insert = 1;\n+\n+-- ADD COLUMN\n+\n+INSERT INTO t_async_insert_alter VALUES (42, 24);\n+\n+ALTER TABLE t_async_insert_alter ADD COLUMN value2 Int64;\n+\n+SYSTEM FLUSH ASYNC INSERT QUEUE;\n+SYSTEM FLUSH LOGS;\n+\n+SELECT * FROM t_async_insert_alter ORDER BY id;\n+\n+-- MODIFY COLUMN\n+\n+INSERT INTO t_async_insert_alter VALUES (43, 34, 55);\n+\n+ALTER TABLE t_async_insert_alter MODIFY COLUMN value2 String;\n+\n+SYSTEM FLUSH ASYNC INSERT QUEUE;\n+SYSTEM FLUSH LOGS;\n+\n+SELECT * FROM t_async_insert_alter ORDER BY id;\n+\n+-- DROP COLUMN\n+\n+INSERT INTO t_async_insert_alter VALUES ('100', '200', '300');\n+\n+ALTER TABLE t_async_insert_alter DROP COLUMN value2;\n+\n+SYSTEM FLUSH ASYNC INSERT QUEUE;\n+SYSTEM FLUSH LOGS;\n+\n+SELECT * FROM t_async_insert_alter ORDER BY id;\n+SELECT query, data_kind, status FROM system.asynchronous_insert_log WHERE database = currentDatabase() AND table = 't_async_insert_alter' ORDER BY event_time_microseconds;\n+\n+DROP TABLE t_async_insert_alter;\ndiff --git a/tests/queries/0_stateless/03229_async_insert_alter_http.reference b/tests/queries/0_stateless/03229_async_insert_alter_http.reference\nnew file mode 100644\nindex 000000000000..195701d2b826\n--- /dev/null\n+++ b/tests/queries/0_stateless/03229_async_insert_alter_http.reference\n@@ -0,0 +1,8 @@\n+42\t24\t0\n+42\t24\t0\n+43\t34\t55\n+42\t24\n+43\t34\n+INSERT INTO default.t_async_insert_alter (id, v1) FORMAT Values\tParsed\tOk\n+INSERT INTO default.t_async_insert_alter (id, v1, value2) FORMAT Values\tParsed\tOk\n+INSERT INTO default.t_async_insert_alter (id, v1, value2) FORMAT Values\tParsed\tFlushError\ndiff --git a/tests/queries/0_stateless/03229_async_insert_alter_http.sh b/tests/queries/0_stateless/03229_async_insert_alter_http.sh\nnew file mode 100755\nindex 000000000000..fe72ed3299a0\n--- /dev/null\n+++ b/tests/queries/0_stateless/03229_async_insert_alter_http.sh\n@@ -0,0 +1,56 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel\n+# no-parallel because the test uses FLUSH ASYNC INSERT QUEUE\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT -q \"\n+    DROP TABLE IF EXISTS t_async_insert_alter;\n+    CREATE TABLE t_async_insert_alter (id Int64, v1 Int64) ENGINE = MergeTree ORDER BY id SETTINGS async_insert = 1;\n+\"\n+\n+url=\"${CLICKHOUSE_URL}&async_insert=1&async_insert_busy_timeout_max_ms=300000&async_insert_busy_timeout_min_ms=300000&wait_for_async_insert=0&async_insert_use_adaptive_busy_timeout=0\"\n+\n+# ADD COLUMN\n+\n+${CLICKHOUSE_CURL} -sS \"$url\" -d \"INSERT INTO t_async_insert_alter VALUES (42, 24)\"\n+\n+$CLICKHOUSE_CLIENT -q \"\n+    ALTER TABLE t_async_insert_alter ADD COLUMN value2 Int64;\n+\n+    SYSTEM FLUSH ASYNC INSERT QUEUE;\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT * FROM t_async_insert_alter ORDER BY id;\n+\"\n+\n+# MODIFY COLUMN\n+\n+${CLICKHOUSE_CURL} -sS \"$url\" -d \"INSERT INTO t_async_insert_alter VALUES (43, 34, 55)\"\n+\n+$CLICKHOUSE_CLIENT -q \"\n+    ALTER TABLE t_async_insert_alter MODIFY COLUMN value2 String;\n+\n+    SYSTEM FLUSH ASYNC INSERT QUEUE;\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT * FROM t_async_insert_alter ORDER BY id;\n+\"\n+\n+## DROP COLUMN\n+\n+${CLICKHOUSE_CURL} -sS \"$url\" -d \"INSERT INTO t_async_insert_alter VALUES ('100', '200', '300')\"\n+\n+$CLICKHOUSE_CLIENT -q \"\n+    ALTER TABLE t_async_insert_alter DROP COLUMN value2;\n+\n+    SYSTEM FLUSH ASYNC INSERT QUEUE;\n+    SYSTEM FLUSH LOGS;\n+\n+    SELECT * FROM t_async_insert_alter ORDER BY id;\n+    SELECT query, data_kind, status FROM system.asynchronous_insert_log WHERE database = currentDatabase() AND table = 't_async_insert_alter' ORDER BY event_time_microseconds;\n+\n+    DROP TABLE t_async_insert_alter;\n+\"\n",
  "problem_statement": "`ALTER` queries are not handled properly with `AsynchronousInsertQueue`\n**Describe what's wrong**\r\n\r\nIf the schema of the table changes while data is buffered for async inserts, the insertion can fail.\r\n\r\n**Does it reproduce on the most recent release?**\r\n\r\nYes.\r\n\r\n**How to reproduce**\r\n```\r\nSET wait_for_async_insert = 0;\r\nSET async_insert_busy_timeout_max_ms = 2000;\r\nSET async_insert_use_adaptive_busy_timeout = 0;\r\n\r\nDROP TABLE IF EXISTS test;\r\nCREATE TABLE test\r\n(\r\n    `id` Int64,\r\n    `value` SimpleAggregateFunction(anyLast, Nullable(Int64)),\r\n)\r\nENGINE = ReplicatedAggregatingMergeTree('/clickhouse/tables/test', 'replica')\r\nORDER BY id\r\nSETTINGS async_insert = 1;\r\n\r\n\r\n\r\nINSERT INTO test VALUES (42, 24);\r\nALTER TABLE test ADD COLUMN IF NOT EXISTS value2 SimpleAggregateFunction(anyLast, Nullable(Int64));\r\n```\r\n\r\nThen check the logs and there will be an error:\r\n```\r\n2024.07.29 13:28:44.457115 [ 1401918 ] {} <Error> AsynchronousInsertQueue: Failed insertion for query 'INSERT INTO default.test FORMAT Native': Code: 49. DB::Exception: Invalid number of rows in Chunk column Nullable(Int64) position 2: expected 1, got 0. (LOGICAL_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ./contrib/llvm-project/libcxx/include/exception:141: Poco::Exception::Exception(String const&, int) @ 0x000000001467ee92\r\n1. ./build/./src/Common/Exception.cpp:111: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bad66d9\r\n2. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000006792a8c\r\n3. DB::Exception::Exception<String, String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&, String&&) @ 0x000000000683f0ab\r\n4. ./build/./src/Processors/Chunk.cpp:0: DB::Chunk::checkNumRowsIsConsistent() @ 0x0000000011af9bf3\r\n5. ./build/./src/Processors/Chunk.cpp:36: DB::Chunk::Chunk(std::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn>>>, unsigned long) @ 0x0000000011af9e6b\r\n6. ./contrib/llvm-project/libcxx/include/vector:438: DB::AsynchronousInsertQueue::processData(DB::AsynchronousInsertQueue::InsertQuery, std::unique_ptr<DB::AsynchronousInsertQueue::InsertData, std::default_delete<DB::AsynchronousInsertQueue::InsertData>>, std::shared_ptr<DB::Context const>, DB::AsynchronousInsertQueue::QueueShardFlushTimeHistory&) @ 0x000000000fedbc3c\r\n7. ./build/./src/Interpreters/AsynchronousInsertQueue.cpp:0: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::AsynchronousInsertQueue::scheduleDataProcessingJob(DB::AsynchronousInsertQueue::InsertQuery const&, std::unique_ptr<DB::AsynchronousInsertQueue::InsertData, std::default_delete<DB::AsynchronousInsertQueue::InsertData>>, std::shared_ptr<DB::Context const>, unsigned long)::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x000000000fede422\r\n8. ./base/base/../base/wide_integer_impl.h:817: ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false, true>, void*>) @ 0x000000000bb8beb6\r\n9. ./build/./src/Common/ThreadPool.cpp:0: ThreadFromGlobalPoolImpl<false, true>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'()::operator()() @ 0x000000000bb8ee11\r\n10. ./base/base/../base/wide_integer_impl.h:817: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000000bb89601\r\n11. ./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: void* std::__thread_proxy[abi:v15007]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000bb8d9ce\r\n12. ? @ 0x000075a6ccc94ac3\r\n13. ? @ 0x000075a6ccd26850\r\n (version 24.8.1.1)\r\n```\r\n**Expected behavior**\r\n\r\nMost importantly don't get a `LOGICAL_ERROR`. Maybe throw a different error code, or somehow make the insert succeed (I am not sure if it is possible).\r\n\r\n**Error message and/or stacktrace**\r\n\r\nSee above.\r\n\n",
  "hints_text": "",
  "created_at": "2024-08-23T16:17:38Z"
}