{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 74541,
  "instance_id": "ClickHouse__ClickHouse-74541",
  "issue_numbers": [
    "68043"
  ],
  "base_commit": "5c53f02e7358b66eb3d53261d6f670fa1dda9d1b",
  "patch": "diff --git a/docs/en/sql-reference/table-functions/deltalake.md b/docs/en/sql-reference/table-functions/deltalake.md\nindex b2caf7a66e62..4833ad59a855 100644\n--- a/docs/en/sql-reference/table-functions/deltalake.md\n+++ b/docs/en/sql-reference/table-functions/deltalake.md\n@@ -9,25 +9,29 @@ title: 'deltaLake'\n \n # deltaLake Table Function\n \n-Provides a read-only table-like interface to the [Delta Lake](https://github.com/delta-io/delta) tables in Amazon S3.\n+Provides a read-only table-like interface to [Delta Lake](https://github.com/delta-io/delta) tables in Amazon S3 or Azure Blob Storage.\n \n ## Syntax {#syntax}\n \n+`deltaLake` is an alias of `deltaLakeS3`, its supported for compatibility.\n+\n+\n ```sql\n deltaLake(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])\n+\n+deltaLakeS3(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])\n+\n+deltaLakeAzure(connection_string|storage_account_url, container_name, blobpath, [,account_name], [,account_key] [,format] [,compression_method])\n ```\n \n ## Arguments {#arguments}\n \n-- `url` \u2014 Bucket url with path to existing Delta Lake table in S3.\n-- `aws_access_key_id`, `aws_secret_access_key` - Long-term credentials for the [AWS](https://aws.amazon.com/) account user.  You can use these to authenticate your requests. These parameters are optional. If credentials are not specified, they are used from the ClickHouse configuration. For more information see [Using S3 for Data Storage](engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-s3).\n-- `format` \u2014 The [format](/interfaces/formats) of the file.\n-- `structure` \u2014 Structure of the table. Format `'column1_name column1_type, column2_name column2_type, ...'`.\n-- `compression` \u2014 Parameter is optional. Supported values: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. By default, compression will be autodetected by the file extension.\n+Description of the arguments coincides with description of arguments in table functions `s3`, `azureBlobStorage`, `HDFS` and `file` correspondingly.\n+`format` stands for the format of data files in the Delta lake table.\n \n **Returned value**\n \n-A table with the specified structure for reading data in the specified Delta Lake table in S3.\n+A table with the specified structure for reading data in the specified Delta Lake table.\n \n **Examples**\n \ndiff --git a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\nindex 7c1d82a2a10b..db46225e1a26 100644\n--- a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\n+++ b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\n@@ -278,12 +278,17 @@ using StorageHDFSIcebergConfiguration = DataLakeConfiguration<StorageHDFSConfigu\n using StorageLocalIcebergConfiguration = DataLakeConfiguration<StorageLocalConfiguration, IcebergMetadata>;\n #endif\n \n-#if USE_PARQUET && USE_AWS_S3\n+#if USE_PARQUET\n+#if USE_AWS_S3\n using StorageS3DeltaLakeConfiguration = DataLakeConfiguration<StorageS3Configuration, DeltaLakeMetadata>;\n #endif\n \n-#if USE_PARQUET\n+#if USE_AZURE_BLOB_STORAGE\n+using StorageAzureDeltaLakeConfiguration = DataLakeConfiguration<StorageAzureConfiguration, DeltaLakeMetadata>;\n+#endif\n+\n using StorageLocalDeltaLakeConfiguration = DataLakeConfiguration<StorageLocalConfiguration, DeltaLakeMetadata>;\n+\n #endif\n \n #if USE_AWS_S3\ndiff --git a/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp b/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp\nindex f62b9cae37f0..6fdd5fbc91c0 100644\n--- a/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp\n+++ b/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp\n@@ -266,6 +266,34 @@ void registerStorageDeltaLake(StorageFactory & factory)\n             .source_access_type = AccessType::S3,\n             .has_builtin_setting_fn = StorageObjectStorageSettings::hasBuiltin,\n         });\n+    factory.registerStorage(\n+        \"DeltaLakeS3\",\n+        [&](const StorageFactory::Arguments & args)\n+        {\n+            auto configuration = std::make_shared<StorageS3DeltaLakeConfiguration>();\n+            return createStorageObjectStorage(args, configuration);\n+        },\n+        {\n+            .supports_settings = true,\n+            .supports_schema_inference = true,\n+            .source_access_type = AccessType::S3,\n+            .has_builtin_setting_fn = StorageObjectStorageSettings::hasBuiltin,\n+        });\n+#    endif\n+#    if USE_AZURE_BLOB_STORAGE\n+    factory.registerStorage(\n+        \"DeltaLakeAzure\",\n+        [&](const StorageFactory::Arguments & args)\n+        {\n+            auto configuration = std::make_shared<StorageAzureDeltaLakeConfiguration>();\n+            return createStorageObjectStorage(args, configuration);\n+        },\n+        {\n+            .supports_settings = true,\n+            .supports_schema_inference = true,\n+            .source_access_type = AccessType::AZURE,\n+            .has_builtin_setting_fn = StorageObjectStorageSettings::hasBuiltin,\n+        });\n #    endif\n     UNUSED(factory);\n }\ndiff --git a/src/Storages/registerStorages.cpp b/src/Storages/registerStorages.cpp\nindex 524aac42e9c6..70f6d44b0422 100644\n--- a/src/Storages/registerStorages.cpp\n+++ b/src/Storages/registerStorages.cpp\n@@ -37,9 +37,10 @@ void registerStorageFuzzJSON(StorageFactory & factory);\n void registerStorageS3(StorageFactory & factory);\n void registerStorageHudi(StorageFactory & factory);\n void registerStorageS3Queue(StorageFactory & factory);\n-#  if USE_PARQUET\n+#endif\n+\n+#if USE_PARQUET && USE_DELTA_KERNEL_RS\n void registerStorageDeltaLake(StorageFactory & factory);\n-#  endif\n #endif\n \n #if USE_AVRO\n@@ -145,11 +146,10 @@ void registerStorages()\n #if USE_AWS_S3\n     registerStorageHudi(factory);\n     registerStorageS3Queue(factory);\n+#endif\n \n-#  if USE_PARQUET && USE_DELTA_KERNEL_RS\n+#if USE_PARQUET && USE_DELTA_KERNEL_RS\n     registerStorageDeltaLake(factory);\n-#  endif\n-\n #endif\n \n #if USE_HDFS\ndiff --git a/src/TableFunctions/TableFunctionObjectStorage.cpp b/src/TableFunctions/TableFunctionObjectStorage.cpp\nindex 27f84dd30142..d70cf1a0835f 100644\n--- a/src/TableFunctions/TableFunctionObjectStorage.cpp\n+++ b/src/TableFunctions/TableFunctionObjectStorage.cpp\n@@ -346,19 +346,38 @@ void registerTableFunctionIceberg(TableFunctionFactory & factory)\n #endif\n \n \n-#if USE_AWS_S3\n #if USE_PARQUET && USE_DELTA_KERNEL_RS\n void registerTableFunctionDeltaLake(TableFunctionFactory & factory)\n {\n+#if USE_AWS_S3\n     factory.registerFunction<TableFunctionDeltaLake>(\n         {.documentation\n-         = {.description = R\"(The table function can be used to read the DeltaLake table stored on object store.)\",\n+         = {.description = R\"(The table function can be used to read the DeltaLake table stored on S3, alias of deltaLakeS3.)\",\n             .examples{{\"deltaLake\", \"SELECT * FROM deltaLake(url, access_key_id, secret_access_key)\", \"\"}},\n             .category{\"\"}},\n          .allow_readonly = false});\n+\n+    factory.registerFunction<TableFunctionDeltaLakeS3>(\n+        {.documentation\n+         = {.description = R\"(The table function can be used to read the DeltaLake table stored on S3.)\",\n+            .examples{{\"deltaLakeS3\", \"SELECT * FROM deltaLakeS3(url, access_key_id, secret_access_key)\", \"\"}},\n+            .category{\"\"}},\n+         .allow_readonly = false});\n+#endif\n+\n+#if USE_AZURE_BLOB_STORAGE\n+    factory.registerFunction<TableFunctionDeltaLakeAzure>(\n+        {.documentation\n+         = {.description = R\"(The table function can be used to read the DeltaLake table stored on Azure object store.)\",\n+            .examples{{\"deltaLakeAzure\", \"SELECT * FROM deltaLakeAzure(connection_string|storage_account_url, container_name, blobpath, \\\"\\n\"\n+ \"                \\\"[account_name, account_key, format, compression, structure])\", \"\"}},\n+            .category{\"\"}},\n+         .allow_readonly = false});\n+#endif\n }\n #endif\n \n+#if USE_AWS_S3\n void registerTableFunctionHudi(TableFunctionFactory & factory)\n {\n     factory.registerFunction<TableFunctionHudi>(\n@@ -368,7 +387,6 @@ void registerTableFunctionHudi(TableFunctionFactory & factory)\n             .category{\"\"}},\n          .allow_readonly = false});\n }\n-\n #endif\n \n void registerDataLakeTableFunctions(TableFunctionFactory & factory)\n@@ -377,10 +395,11 @@ void registerDataLakeTableFunctions(TableFunctionFactory & factory)\n #if USE_AVRO\n     registerTableFunctionIceberg(factory);\n #endif\n-#if USE_AWS_S3\n+\n #if USE_PARQUET && USE_DELTA_KERNEL_RS\n     registerTableFunctionDeltaLake(factory);\n #endif\n+#if USE_AWS_S3\n     registerTableFunctionHudi(factory);\n #endif\n }\ndiff --git a/src/TableFunctions/TableFunctionObjectStorage.h b/src/TableFunctions/TableFunctionObjectStorage.h\nindex 94d3542ad48f..1f1bb7d8d6e0 100644\n--- a/src/TableFunctions/TableFunctionObjectStorage.h\n+++ b/src/TableFunctions/TableFunctionObjectStorage.h\n@@ -101,6 +101,18 @@ struct DeltaLakeDefinition\n     static constexpr auto storage_type_name = \"S3\";\n };\n \n+struct DeltaLakeS3Definition\n+{\n+    static constexpr auto name = \"deltaLakeS3\";\n+    static constexpr auto storage_type_name = \"S3\";\n+};\n+\n+struct DeltaLakeAzureDefinition\n+{\n+    static constexpr auto name = \"deltaLakeAzure\";\n+    static constexpr auto storage_type_name = \"Azure\";\n+};\n+\n struct HudiDefinition\n {\n     static constexpr auto name = \"hudi\";\n@@ -199,10 +211,16 @@ using TableFunctionIcebergHDFS = TableFunctionObjectStorage<IcebergHDFSDefinitio\n #    endif\n using TableFunctionIcebergLocal = TableFunctionObjectStorage<IcebergLocalDefinition, StorageLocalIcebergConfiguration>;\n #endif\n+#if USE_PARQUET && USE_DELTA_KERNEL_RS\n #if USE_AWS_S3\n-#    if USE_PARQUET && USE_DELTA_KERNEL_RS\n using TableFunctionDeltaLake = TableFunctionObjectStorage<DeltaLakeDefinition, StorageS3DeltaLakeConfiguration>;\n-#    endif\n+using TableFunctionDeltaLakeS3 = TableFunctionObjectStorage<DeltaLakeS3Definition, StorageS3DeltaLakeConfiguration>;\n+#endif\n+#if USE_AZURE_BLOB_STORAGE\n+using TableFunctionDeltaLakeAzure = TableFunctionObjectStorage<DeltaLakeAzureDefinition, StorageAzureDeltaLakeConfiguration>;\n+#endif\n+#endif\n+#if USE_AWS_S3\n using TableFunctionHudi = TableFunctionObjectStorage<HudiDefinition, StorageS3HudiConfiguration>;\n #endif\n }\n",
  "test_patch": "diff --git a/tests/integration/test_storage_delta/configs/config.d/named_collections.xml b/tests/integration/test_storage_delta/configs/config.d/named_collections.xml\nindex 527535288fc3..6163f7a52248 100644\n--- a/tests/integration/test_storage_delta/configs/config.d/named_collections.xml\n+++ b/tests/integration/test_storage_delta/configs/config.d/named_collections.xml\n@@ -5,5 +5,9 @@\n         <access_key_id>minio</access_key_id>\n         <secret_access_key>ClickHouse_Minio_P@ssw0rd</secret_access_key>\n     </s3>\n+    <azure>\n+        <account_name>devstoreaccount1</account_name>\n+        <account_key>Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==</account_key>\n+    </azure>\n   </named_collections>\n </clickhouse>\ndiff --git a/tests/integration/test_storage_delta/test.py b/tests/integration/test_storage_delta/test.py\nindex fcc829e0bf6c..87589351830d 100644\n--- a/tests/integration/test_storage_delta/test.py\n+++ b/tests/integration/test_storage_delta/test.py\n@@ -13,6 +13,7 @@\n import pyarrow.parquet as pq\n import pyspark\n import pytest\n+from azure.storage.blob import BlobServiceClient\n from delta import *\n from deltalake.writer import write_deltalake\n from minio.deleteobjects import DeleteObject\n@@ -37,6 +38,8 @@\n from helpers.cluster import ClickHouseCluster\n from helpers.network import PartitionManager\n from helpers.s3_tools import (\n+    AzureUploader,\n+    S3Uploader,\n     get_file_contents,\n     list_s3_objects,\n     prepare_s3_bucket,\n@@ -48,6 +51,7 @@\n from helpers.config_cluster import minio_secret_key\n \n SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n+cluster = ClickHouseCluster(__file__, with_spark=True)\n \n \n def get_spark():\n@@ -72,7 +76,6 @@ def randomize_table_name(table_name, random_suffix_length=10):\n @pytest.fixture(scope=\"module\")\n def started_cluster():\n     try:\n-        cluster = ClickHouseCluster(__file__, with_spark=True)\n         cluster.add_instance(\n             \"node1\",\n             main_configs=[\n@@ -82,6 +85,7 @@ def started_cluster():\n             ],\n             user_configs=[\"configs/users.d/users.xml\"],\n             with_minio=True,\n+            with_azurite=True,\n             stay_alive=True,\n             with_zookeeper=True,\n         )\n@@ -114,7 +118,25 @@ def started_cluster():\n         logging.info(\"Starting cluster...\")\n         cluster.start()\n \n-        prepare_s3_bucket(cluster)\n+        cluster.default_s3_uploader = S3Uploader(\n+            cluster.minio_client, cluster.minio_bucket\n+        )\n+\n+        cluster.minio_restricted_bucket = \"{}-with-auth\".format(cluster.minio_bucket)\n+        if cluster.minio_client.bucket_exists(cluster.minio_restricted_bucket):\n+            cluster.minio_client.remove_bucket(cluster.minio_restricted_bucket)\n+\n+        cluster.minio_client.make_bucket(cluster.minio_restricted_bucket)\n+\n+        cluster.azure_container_name = \"mycontainer\"\n+        cluster.blob_service_client = cluster.blob_service_client\n+        container_client = cluster.blob_service_client.create_container(\n+            cluster.azure_container_name\n+        )\n+        cluster.container_client = container_client\n+        cluster.default_azure_uploader = AzureUploader(\n+            cluster.blob_service_client, cluster.azure_container_name\n+        )\n \n         cluster.spark_session = get_spark()\n \n@@ -165,14 +187,88 @@ def get_delta_metadata(delta_metadata_file):\n     return combined_json\n \n \n-def create_delta_table(node, table_name, bucket=\"root\", use_delta_kernel=False):\n-    node.query(\n-        f\"\"\"\n-        DROP TABLE IF EXISTS {table_name};\n-        CREATE TABLE {table_name}\n-        ENGINE=DeltaLake(s3, filename = '{table_name}/', url = 'http://minio1:9001/{bucket}/')\n-        SETTINGS allow_experimental_delta_kernel_rs={use_delta_kernel}\"\"\"\n-    )\n+def create_delta_table(\n+    instance,\n+    storage_type,\n+    table_name,\n+    cluster,\n+    format=\"Parquet\",\n+    table_function=False,\n+    allow_dynamic_metadata_for_data_lakes=False,\n+    run_on_cluster=False,\n+    use_delta_kernel=False,\n+    **kwargs,\n+):\n+    allow_dynamic_metadata_for_datalakes_suffix = (\n+        \" SETTINGS allow_dynamic_metadata_for_data_lakes = 1\"\n+        if allow_dynamic_metadata_for_data_lakes\n+        else \"\"\n+    )\n+\n+    if storage_type == \"s3\":\n+        if \"bucket\" in kwargs:\n+            bucket = kwargs[\"bucket\"]\n+        else:\n+            bucket = cluster.minio_bucket\n+\n+        if run_on_cluster:\n+            assert table_function\n+            instance.query(f\"deltalakeS3Cluster('cluster_simple', s3, filename = '{table_name}/', format={format}, url = 'http://minio1:9001/{bucket}/')\"\n+                    f\"SETTINGS allow_experimental_delta_kernel_rs={use_delta_kernel}\")\n+        else:\n+            if table_function:\n+                instance.query(f\"deltalakeS3(s3, filename = '{table_name}/', format={format}, url = 'http://minio1:9001/{bucket}/')\"\n+                        f\"SETTINGS allow_experimental_delta_kernel_rs={use_delta_kernel}\")\n+            else:\n+                instance.query(\n+                    f\"\"\"\n+                    DROP TABLE IF EXISTS {table_name};\n+                    CREATE TABLE {table_name}\n+                    ENGINE=DeltaLake(s3, filename = '{table_name}/', format={format}, url = 'http://minio1:9001/{bucket}/')\n+                    SETTINGS allow_experimental_delta_kernel_rs={use_delta_kernel}\"\"\"\n+                    + allow_dynamic_metadata_for_datalakes_suffix\n+                )\n+\n+    elif storage_type == \"azure\":\n+        if run_on_cluster:\n+            assert table_function\n+            instance.query(f\"\"\"\n+                deltalakeAzureCluster('cluster_simple', azure, container = '{cluster.azure_container_name}', storage_account_url = '{cluster.env_variables[\"AZURITE_STORAGE_ACCOUNT_URL\"]}', blob_path = '/{table_name}', format={format})\n+                SETTINGS allow_experimental_delta_kernel_rs={use_delta_kernel}\n+            \"\"\")\n+        else:\n+            if table_function:\n+                instance.query(f\"\"\"\n+                    deltalakeAzure(azure, container = '{cluster.azure_container_name}', storage_account_url = '{cluster.env_variables[\"AZURITE_STORAGE_ACCOUNT_URL\"]}', blob_path = '/{table_name}', format={format})\n+                    SETTINGS allow_experimental_delta_kernel_rs={use_delta_kernel}\n+                \"\"\")\n+            else:\n+                instance.query(\n+                    f\"\"\"\n+                    DROP TABLE IF EXISTS {table_name};\n+                    CREATE TABLE {table_name}\n+                    ENGINE=DeltaLakeAzure(azure, container = {cluster.azure_container_name}, storage_account_url = '{cluster.env_variables[\"AZURITE_STORAGE_ACCOUNT_URL\"]}', blob_path = '/{table_name}', format={format})\n+                    SETTINGS allow_experimental_delta_kernel_rs={use_delta_kernel}\"\"\"\n+                    + allow_dynamic_metadata_for_datalakes_suffix\n+                )\n+    else:\n+        raise Exception(f\"Unknown delta lake storage type: {storage_type}\")\n+\n+\n+def default_upload_directory(\n+    started_cluster, storage_type, local_path, remote_path, **kwargs\n+):\n+    if storage_type == \"s3\":\n+        print(kwargs)\n+        return started_cluster.default_s3_uploader.upload_directory(\n+            local_path, remote_path, **kwargs\n+        )\n+    elif storage_type == \"azure\":\n+        return started_cluster.default_azure_uploader.upload_directory(\n+            local_path, remote_path, **kwargs\n+        )\n+    else:\n+        raise Exception(f\"Unknown delta storage type: {storage_type}\")\n \n \n def create_initial_data_file(\n@@ -194,12 +290,10 @@ def create_initial_data_file(\n     return result_path\n \n \n-@pytest.mark.parametrize(\"use_delta_kernel\", [\"1\", \"0\"])\n-def test_single_log_file(started_cluster, use_delta_kernel):\n+@pytest.mark.parametrize(\"use_delta_kernel, storage_type\", [(\"1\", \"s3\"), (\"0\", \"s3\"), (\"0\", \"azure\")])\n+def test_single_log_file(started_cluster, use_delta_kernel, storage_type):\n     instance = started_cluster.instances[\"node1\"]\n     spark = started_cluster.spark_session\n-    minio_client = started_cluster.minio_client\n-    bucket = started_cluster.minio_bucket\n     TABLE_NAME = randomize_table_name(\"test_single_log_file\")\n \n     inserted_data = \"SELECT number as a, toString(number + 1) as b FROM numbers(100)\"\n@@ -208,23 +302,27 @@ def test_single_log_file(started_cluster, use_delta_kernel):\n     )\n \n     write_delta_from_file(spark, parquet_data_path, f\"/{TABLE_NAME}\")\n-    files = upload_directory(minio_client, bucket, f\"/{TABLE_NAME}\", \"\")\n+\n+    files = default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/{TABLE_NAME}\",\n+        \"\",\n+    )\n+\n     assert len(files) == 2  # 1 metadata files + 1 data file\n \n-    create_delta_table(instance, TABLE_NAME, use_delta_kernel=use_delta_kernel)\n+    create_delta_table(instance, storage_type, TABLE_NAME, started_cluster, use_delta_kernel=use_delta_kernel)\n \n     assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 100\n     assert instance.query(f\"SELECT * FROM {TABLE_NAME}\") == instance.query(\n         inserted_data\n     )\n \n-\n-@pytest.mark.parametrize(\"use_delta_kernel\", [\"1\", \"0\"])\n-def test_partition_by(started_cluster, use_delta_kernel):\n+@pytest.mark.parametrize(\"use_delta_kernel, storage_type\", [(\"1\", \"s3\"), (\"0\", \"s3\"), (\"0\", \"azure\")])\n+def test_partition_by(started_cluster, use_delta_kernel, storage_type):\n     instance = started_cluster.instances[\"node1\"]\n     spark = started_cluster.spark_session\n-    minio_client = started_cluster.minio_client\n-    bucket = started_cluster.minio_bucket\n     TABLE_NAME = randomize_table_name(\"test_partition_by\")\n \n     write_delta_from_df(\n@@ -235,15 +333,21 @@ def test_partition_by(started_cluster, use_delta_kernel):\n         partition_by=\"a\",\n     )\n \n-    files = upload_directory(minio_client, bucket, f\"/{TABLE_NAME}\", \"\")\n+    files = default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/{TABLE_NAME}\",\n+        \"\",\n+    )\n+\n     assert len(files) == 11  # 10 partitions and 1 metadata file\n \n-    create_delta_table(instance, TABLE_NAME, use_delta_kernel=use_delta_kernel)\n+    create_delta_table(instance, storage_type, TABLE_NAME, started_cluster, use_delta_kernel=use_delta_kernel)\n     assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 10\n \n \n-@pytest.mark.parametrize(\"use_delta_kernel\", [\"1\", \"0\"])\n-def test_checkpoint(started_cluster, use_delta_kernel):\n+@pytest.mark.parametrize(\"use_delta_kernel, storage_type\", [(\"1\", \"s3\"), (\"0\", \"s3\"), (\"0\", \"azure\")])\n+def test_checkpoint(started_cluster, use_delta_kernel, storage_type):\n     instance = started_cluster.instances[\"node1\"]\n     spark = started_cluster.spark_session\n     minio_client = started_cluster.minio_client\n@@ -263,7 +367,13 @@ def test_checkpoint(started_cluster, use_delta_kernel):\n             f\"/{TABLE_NAME}\",\n             mode=\"append\",\n         )\n-    files = upload_directory(minio_client, bucket, f\"/{TABLE_NAME}\", \"\")\n+\n+    files = default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/{TABLE_NAME}\",\n+        \"\",\n+    )\n     # 25 data files\n     # 25 metadata files\n     # 1 last_metadata file\n@@ -276,7 +386,7 @@ def test_checkpoint(started_cluster, use_delta_kernel):\n             ok = True\n     assert ok\n \n-    create_delta_table(instance, TABLE_NAME, use_delta_kernel=use_delta_kernel)\n+    create_delta_table(instance, storage_type, TABLE_NAME, started_cluster, use_delta_kernel = use_delta_kernel)\n     assert (\n         int(\n             instance.query(\n@@ -288,7 +398,12 @@ def test_checkpoint(started_cluster, use_delta_kernel):\n \n     table = DeltaTable.forPath(spark, f\"/{TABLE_NAME}\")\n     table.delete(\"a < 10\")\n-    files = upload_directory(minio_client, bucket, f\"/{TABLE_NAME}\", \"\")\n+    files = default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/{TABLE_NAME}\",\n+        \"\",\n+    )\n     assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 15\n \n     for i in range(0, 5):\n@@ -303,7 +418,12 @@ def test_checkpoint(started_cluster, use_delta_kernel):\n     # + 5 metadata files\n     # + 1 checkpoint file\n     # + 1 ?\n-    files = upload_directory(minio_client, bucket, f\"/{TABLE_NAME}\", \"\")\n+    files = default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/{TABLE_NAME}\",\n+        \"\",\n+    )\n     assert len(files) == 53 + 1 + 5 * 2 + 1 + 1\n     assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 20\n \n@@ -337,7 +457,7 @@ def test_multiple_log_files(started_cluster, use_delta_kernel):\n     )\n     assert len(s3_objects) == 1\n \n-    create_delta_table(instance, TABLE_NAME, use_delta_kernel=use_delta_kernel)\n+    create_delta_table(instance, \"s3\", TABLE_NAME, started_cluster, use_delta_kernel = use_delta_kernel)\n     assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 100\n \n     write_delta_from_df(\n@@ -387,7 +507,7 @@ def test_metadata(started_cluster, use_delta_kernel):\n     assert next(iter(stats[\"minValues\"].values())) == 0\n     assert next(iter(stats[\"maxValues\"].values())) == 99\n \n-    create_delta_table(instance, TABLE_NAME, use_delta_kernel=use_delta_kernel)\n+    create_delta_table(instance, \"s3\", TABLE_NAME, started_cluster, use_delta_kernel = use_delta_kernel)\n     assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 100\n \n \n@@ -488,9 +608,9 @@ def test_restart_broken(started_cluster, use_delta_kernel):\n \n     write_delta_from_file(spark, parquet_data_path, f\"/{TABLE_NAME}\")\n     upload_directory(minio_client, bucket, f\"/{TABLE_NAME}\", \"\")\n-    create_delta_table(\n-        instance, TABLE_NAME, bucket=bucket, use_delta_kernel=use_delta_kernel\n-    )\n+\n+    create_delta_table(instance, \"s3\", TABLE_NAME, started_cluster, use_delta_kernel=use_delta_kernel, bucket=bucket)\n+\n     assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 100\n \n     s3_objects = list_s3_objects(minio_client, bucket, prefix=\"\")\n@@ -884,9 +1004,8 @@ def test_complex_types(started_cluster, use_delta_kernel):\n     )\n \n \n-@pytest.mark.parametrize(\"storage_type\", [\"s3\"])\n @pytest.mark.parametrize(\"use_delta_kernel\", [\"1\", \"0\"])\n-def test_filesystem_cache(started_cluster, storage_type, use_delta_kernel):\n+def test_filesystem_cache(started_cluster, use_delta_kernel):\n     instance = started_cluster.instances[\"node1\"]\n     spark = started_cluster.spark_session\n     minio_client = started_cluster.minio_client\n@@ -905,9 +1024,7 @@ def test_filesystem_cache(started_cluster, storage_type, use_delta_kernel):\n \n     write_delta_from_file(spark, parquet_data_path, f\"/{TABLE_NAME}\")\n     upload_directory(minio_client, bucket, f\"/{TABLE_NAME}\", \"\")\n-    create_delta_table(\n-        instance, TABLE_NAME, bucket=bucket, use_delta_kernel=use_delta_kernel\n-    )\n+    create_delta_table(instance,\"s3\", TABLE_NAME, started_cluster, use_delta_kernel = use_delta_kernel)\n \n     query_id = f\"{TABLE_NAME}-{uuid.uuid4()}\"\n     instance.query(\n",
  "problem_statement": "Support DeltaLake table engine for Azure Blob Storage\n**Use case**\r\n\r\nAzure blob storage is similar to s3 and is frequently use in similar data lake scenarios in the Microsoft ecosystem.\r\n\r\n**Describe the solution you'd like**\r\n\r\nI'd like the `DeltaLake` table engine to be expanded to support other storage providers besides s3\r\n\r\nhttps://docs.delta.io/0.8.0/delta-storage.html#azure-data-lake-storage-gen2\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNone\r\n\n",
  "hints_text": "",
  "created_at": "2025-01-13T21:55:52Z",
  "modified_files": [
    "docs/en/sql-reference/table-functions/deltalake.md",
    "src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h",
    "src/Storages/ObjectStorage/registerStorageObjectStorage.cpp",
    "src/Storages/registerStorages.cpp",
    "src/TableFunctions/TableFunctionObjectStorage.cpp",
    "src/TableFunctions/TableFunctionObjectStorage.h"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_delta/configs/config.d/named_collections.xml",
    "tests/integration/test_storage_delta/test.py"
  ]
}