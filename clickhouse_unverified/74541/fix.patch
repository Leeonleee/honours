diff --git a/docs/en/sql-reference/table-functions/deltalake.md b/docs/en/sql-reference/table-functions/deltalake.md
index b2caf7a66e62..4833ad59a855 100644
--- a/docs/en/sql-reference/table-functions/deltalake.md
+++ b/docs/en/sql-reference/table-functions/deltalake.md
@@ -9,25 +9,29 @@ title: 'deltaLake'
 
 # deltaLake Table Function
 
-Provides a read-only table-like interface to the [Delta Lake](https://github.com/delta-io/delta) tables in Amazon S3.
+Provides a read-only table-like interface to [Delta Lake](https://github.com/delta-io/delta) tables in Amazon S3 or Azure Blob Storage.
 
 ## Syntax {#syntax}
 
+`deltaLake` is an alias of `deltaLakeS3`, its supported for compatibility.
+
+
 ```sql
 deltaLake(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])
+
+deltaLakeS3(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])
+
+deltaLakeAzure(connection_string|storage_account_url, container_name, blobpath, [,account_name], [,account_key] [,format] [,compression_method])
 ```
 
 ## Arguments {#arguments}
 
-- `url` — Bucket url with path to existing Delta Lake table in S3.
-- `aws_access_key_id`, `aws_secret_access_key` - Long-term credentials for the [AWS](https://aws.amazon.com/) account user.  You can use these to authenticate your requests. These parameters are optional. If credentials are not specified, they are used from the ClickHouse configuration. For more information see [Using S3 for Data Storage](engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-s3).
-- `format` — The [format](/interfaces/formats) of the file.
-- `structure` — Structure of the table. Format `'column1_name column1_type, column2_name column2_type, ...'`.
-- `compression` — Parameter is optional. Supported values: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. By default, compression will be autodetected by the file extension.
+Description of the arguments coincides with description of arguments in table functions `s3`, `azureBlobStorage`, `HDFS` and `file` correspondingly.
+`format` stands for the format of data files in the Delta lake table.
 
 **Returned value**
 
-A table with the specified structure for reading data in the specified Delta Lake table in S3.
+A table with the specified structure for reading data in the specified Delta Lake table.
 
 **Examples**
 
diff --git a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h
index 7c1d82a2a10b..db46225e1a26 100644
--- a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h
+++ b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h
@@ -278,12 +278,17 @@ using StorageHDFSIcebergConfiguration = DataLakeConfiguration<StorageHDFSConfigu
 using StorageLocalIcebergConfiguration = DataLakeConfiguration<StorageLocalConfiguration, IcebergMetadata>;
 #endif
 
-#if USE_PARQUET && USE_AWS_S3
+#if USE_PARQUET
+#if USE_AWS_S3
 using StorageS3DeltaLakeConfiguration = DataLakeConfiguration<StorageS3Configuration, DeltaLakeMetadata>;
 #endif
 
-#if USE_PARQUET
+#if USE_AZURE_BLOB_STORAGE
+using StorageAzureDeltaLakeConfiguration = DataLakeConfiguration<StorageAzureConfiguration, DeltaLakeMetadata>;
+#endif
+
 using StorageLocalDeltaLakeConfiguration = DataLakeConfiguration<StorageLocalConfiguration, DeltaLakeMetadata>;
+
 #endif
 
 #if USE_AWS_S3
diff --git a/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp b/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp
index f62b9cae37f0..6fdd5fbc91c0 100644
--- a/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp
+++ b/src/Storages/ObjectStorage/registerStorageObjectStorage.cpp
@@ -266,6 +266,34 @@ void registerStorageDeltaLake(StorageFactory & factory)
             .source_access_type = AccessType::S3,
             .has_builtin_setting_fn = StorageObjectStorageSettings::hasBuiltin,
         });
+    factory.registerStorage(
+        "DeltaLakeS3",
+        [&](const StorageFactory::Arguments & args)
+        {
+            auto configuration = std::make_shared<StorageS3DeltaLakeConfiguration>();
+            return createStorageObjectStorage(args, configuration);
+        },
+        {
+            .supports_settings = true,
+            .supports_schema_inference = true,
+            .source_access_type = AccessType::S3,
+            .has_builtin_setting_fn = StorageObjectStorageSettings::hasBuiltin,
+        });
+#    endif
+#    if USE_AZURE_BLOB_STORAGE
+    factory.registerStorage(
+        "DeltaLakeAzure",
+        [&](const StorageFactory::Arguments & args)
+        {
+            auto configuration = std::make_shared<StorageAzureDeltaLakeConfiguration>();
+            return createStorageObjectStorage(args, configuration);
+        },
+        {
+            .supports_settings = true,
+            .supports_schema_inference = true,
+            .source_access_type = AccessType::AZURE,
+            .has_builtin_setting_fn = StorageObjectStorageSettings::hasBuiltin,
+        });
 #    endif
     UNUSED(factory);
 }
diff --git a/src/Storages/registerStorages.cpp b/src/Storages/registerStorages.cpp
index 524aac42e9c6..70f6d44b0422 100644
--- a/src/Storages/registerStorages.cpp
+++ b/src/Storages/registerStorages.cpp
@@ -37,9 +37,10 @@ void registerStorageFuzzJSON(StorageFactory & factory);
 void registerStorageS3(StorageFactory & factory);
 void registerStorageHudi(StorageFactory & factory);
 void registerStorageS3Queue(StorageFactory & factory);
-#  if USE_PARQUET
+#endif
+
+#if USE_PARQUET && USE_DELTA_KERNEL_RS
 void registerStorageDeltaLake(StorageFactory & factory);
-#  endif
 #endif
 
 #if USE_AVRO
@@ -145,11 +146,10 @@ void registerStorages()
 #if USE_AWS_S3
     registerStorageHudi(factory);
     registerStorageS3Queue(factory);
+#endif
 
-#  if USE_PARQUET && USE_DELTA_KERNEL_RS
+#if USE_PARQUET && USE_DELTA_KERNEL_RS
     registerStorageDeltaLake(factory);
-#  endif
-
 #endif
 
 #if USE_HDFS
diff --git a/src/TableFunctions/TableFunctionObjectStorage.cpp b/src/TableFunctions/TableFunctionObjectStorage.cpp
index 27f84dd30142..d70cf1a0835f 100644
--- a/src/TableFunctions/TableFunctionObjectStorage.cpp
+++ b/src/TableFunctions/TableFunctionObjectStorage.cpp
@@ -346,19 +346,38 @@ void registerTableFunctionIceberg(TableFunctionFactory & factory)
 #endif
 
 
-#if USE_AWS_S3
 #if USE_PARQUET && USE_DELTA_KERNEL_RS
 void registerTableFunctionDeltaLake(TableFunctionFactory & factory)
 {
+#if USE_AWS_S3
     factory.registerFunction<TableFunctionDeltaLake>(
         {.documentation
-         = {.description = R"(The table function can be used to read the DeltaLake table stored on object store.)",
+         = {.description = R"(The table function can be used to read the DeltaLake table stored on S3, alias of deltaLakeS3.)",
             .examples{{"deltaLake", "SELECT * FROM deltaLake(url, access_key_id, secret_access_key)", ""}},
             .category{""}},
          .allow_readonly = false});
+
+    factory.registerFunction<TableFunctionDeltaLakeS3>(
+        {.documentation
+         = {.description = R"(The table function can be used to read the DeltaLake table stored on S3.)",
+            .examples{{"deltaLakeS3", "SELECT * FROM deltaLakeS3(url, access_key_id, secret_access_key)", ""}},
+            .category{""}},
+         .allow_readonly = false});
+#endif
+
+#if USE_AZURE_BLOB_STORAGE
+    factory.registerFunction<TableFunctionDeltaLakeAzure>(
+        {.documentation
+         = {.description = R"(The table function can be used to read the DeltaLake table stored on Azure object store.)",
+            .examples{{"deltaLakeAzure", "SELECT * FROM deltaLakeAzure(connection_string|storage_account_url, container_name, blobpath, \"
"
+ "                \"[account_name, account_key, format, compression, structure])", ""}},
+            .category{""}},
+         .allow_readonly = false});
+#endif
 }
 #endif
 
+#if USE_AWS_S3
 void registerTableFunctionHudi(TableFunctionFactory & factory)
 {
     factory.registerFunction<TableFunctionHudi>(
@@ -368,7 +387,6 @@ void registerTableFunctionHudi(TableFunctionFactory & factory)
             .category{""}},
          .allow_readonly = false});
 }
-
 #endif
 
 void registerDataLakeTableFunctions(TableFunctionFactory & factory)
@@ -377,10 +395,11 @@ void registerDataLakeTableFunctions(TableFunctionFactory & factory)
 #if USE_AVRO
     registerTableFunctionIceberg(factory);
 #endif
-#if USE_AWS_S3
+
 #if USE_PARQUET && USE_DELTA_KERNEL_RS
     registerTableFunctionDeltaLake(factory);
 #endif
+#if USE_AWS_S3
     registerTableFunctionHudi(factory);
 #endif
 }
diff --git a/src/TableFunctions/TableFunctionObjectStorage.h b/src/TableFunctions/TableFunctionObjectStorage.h
index 94d3542ad48f..1f1bb7d8d6e0 100644
--- a/src/TableFunctions/TableFunctionObjectStorage.h
+++ b/src/TableFunctions/TableFunctionObjectStorage.h
@@ -101,6 +101,18 @@ struct DeltaLakeDefinition
     static constexpr auto storage_type_name = "S3";
 };
 
+struct DeltaLakeS3Definition
+{
+    static constexpr auto name = "deltaLakeS3";
+    static constexpr auto storage_type_name = "S3";
+};
+
+struct DeltaLakeAzureDefinition
+{
+    static constexpr auto name = "deltaLakeAzure";
+    static constexpr auto storage_type_name = "Azure";
+};
+
 struct HudiDefinition
 {
     static constexpr auto name = "hudi";
@@ -199,10 +211,16 @@ using TableFunctionIcebergHDFS = TableFunctionObjectStorage<IcebergHDFSDefinitio
 #    endif
 using TableFunctionIcebergLocal = TableFunctionObjectStorage<IcebergLocalDefinition, StorageLocalIcebergConfiguration>;
 #endif
+#if USE_PARQUET && USE_DELTA_KERNEL_RS
 #if USE_AWS_S3
-#    if USE_PARQUET && USE_DELTA_KERNEL_RS
 using TableFunctionDeltaLake = TableFunctionObjectStorage<DeltaLakeDefinition, StorageS3DeltaLakeConfiguration>;
-#    endif
+using TableFunctionDeltaLakeS3 = TableFunctionObjectStorage<DeltaLakeS3Definition, StorageS3DeltaLakeConfiguration>;
+#endif
+#if USE_AZURE_BLOB_STORAGE
+using TableFunctionDeltaLakeAzure = TableFunctionObjectStorage<DeltaLakeAzureDefinition, StorageAzureDeltaLakeConfiguration>;
+#endif
+#endif
+#if USE_AWS_S3
 using TableFunctionHudi = TableFunctionObjectStorage<HudiDefinition, StorageS3HudiConfiguration>;
 #endif
 }
