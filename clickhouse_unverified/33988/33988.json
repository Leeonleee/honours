{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 33988,
  "instance_id": "ClickHouse__ClickHouse-33988",
  "issue_numbers": [
    "33973"
  ],
  "base_commit": "fe84167c99ecddcfde8ed24d051418a0dfb97306",
  "patch": "diff --git a/programs/keeper/Keeper.cpp b/programs/keeper/Keeper.cpp\nindex d144b4d332e2..636ce129d63e 100644\n--- a/programs/keeper/Keeper.cpp\n+++ b/programs/keeper/Keeper.cpp\n@@ -330,8 +330,6 @@ int Keeper::main(const std::vector<std::string> & /*args*/)\n \n     DB::ServerUUID::load(path + \"/uuid\", log);\n \n-    const Settings & settings = global_context->getSettingsRef();\n-\n     std::string include_from_path = config().getString(\"include_from\", \"/etc/metrika.xml\");\n \n     GlobalThreadPool::initialize(\n@@ -377,8 +375,8 @@ int Keeper::main(const std::vector<std::string> & /*args*/)\n         {\n             Poco::Net::ServerSocket socket;\n             auto address = socketBindListen(socket, listen_host, port);\n-            socket.setReceiveTimeout(settings.receive_timeout);\n-            socket.setSendTimeout(settings.send_timeout);\n+            socket.setReceiveTimeout(config().getUInt64(\"keeper_server.socket_receive_timeout_sec\", DBMS_DEFAULT_RECEIVE_TIMEOUT_SEC));\n+            socket.setSendTimeout(config().getUInt64(\"keeper_server.socket_send_timeout_sec\", DBMS_DEFAULT_SEND_TIMEOUT_SEC));\n             servers->emplace_back(\n                 listen_host,\n                 port_name,\n@@ -393,8 +391,8 @@ int Keeper::main(const std::vector<std::string> & /*args*/)\n #if USE_SSL\n             Poco::Net::SecureServerSocket socket;\n             auto address = socketBindListen(socket, listen_host, port, /* secure = */ true);\n-            socket.setReceiveTimeout(settings.receive_timeout);\n-            socket.setSendTimeout(settings.send_timeout);\n+            socket.setReceiveTimeout(config().getUInt64(\"keeper_server.socket_receive_timeout_sec\", DBMS_DEFAULT_RECEIVE_TIMEOUT_SEC));\n+            socket.setSendTimeout(config().getUInt64(\"keeper_server.socket_send_timeout_sec\", DBMS_DEFAULT_SEND_TIMEOUT_SEC));\n             servers->emplace_back(\n                 listen_host,\n                 secure_port_name,\ndiff --git a/programs/server/Server.cpp b/programs/server/Server.cpp\nindex a49ccc79b633..98838cb23377 100644\n--- a/programs/server/Server.cpp\n+++ b/programs/server/Server.cpp\n@@ -967,6 +967,83 @@ if (ThreadFuzzer::instance().isEffective())\n         },\n         /* already_loaded = */ false);  /// Reload it right now (initial loading)\n \n+    const auto listen_hosts = getListenHosts(config());\n+    const auto listen_try = getListenTry(config());\n+\n+    if (config().has(\"keeper_server\"))\n+    {\n+#if USE_NURAFT\n+        //// If we don't have configured connection probably someone trying to use clickhouse-server instead\n+        //// of clickhouse-keeper, so start synchronously.\n+        bool can_initialize_keeper_async = false;\n+\n+        if (has_zookeeper) /// We have configured connection to some zookeeper cluster\n+        {\n+            /// If we cannot connect to some other node from our cluster then we have to wait our Keeper start\n+            /// synchronously.\n+            can_initialize_keeper_async = global_context->tryCheckClientConnectionToMyKeeperCluster();\n+        }\n+        /// Initialize keeper RAFT.\n+        global_context->initializeKeeperDispatcher(can_initialize_keeper_async);\n+        FourLetterCommandFactory::registerCommands(*global_context->getKeeperDispatcher());\n+\n+        for (const auto & listen_host : listen_hosts)\n+        {\n+            /// TCP Keeper\n+            const char * port_name = \"keeper_server.tcp_port\";\n+            createServer(\n+                config(), listen_host, port_name, listen_try, /* start_server: */ false,\n+                servers_to_start_before_tables,\n+                [&](UInt16 port) -> ProtocolServerAdapter\n+                {\n+                    Poco::Net::ServerSocket socket;\n+                    auto address = socketBindListen(socket, listen_host, port);\n+                    socket.setReceiveTimeout(config().getUInt64(\"keeper_server.socket_receive_timeout_sec\", DBMS_DEFAULT_RECEIVE_TIMEOUT_SEC));\n+                    socket.setSendTimeout(config().getUInt64(\"keeper_server.socket_send_timeout_sec\", DBMS_DEFAULT_SEND_TIMEOUT_SEC));\n+                    return ProtocolServerAdapter(\n+                        listen_host,\n+                        port_name,\n+                        \"Keeper (tcp): \" + address.toString(),\n+                        std::make_unique<TCPServer>(\n+                            new KeeperTCPHandlerFactory(*this, false), server_pool, socket));\n+                });\n+\n+            const char * secure_port_name = \"keeper_server.tcp_port_secure\";\n+            createServer(\n+                config(), listen_host, secure_port_name, listen_try, /* start_server: */ false,\n+                servers_to_start_before_tables,\n+                [&](UInt16 port) -> ProtocolServerAdapter\n+                {\n+#if USE_SSL\n+                    Poco::Net::SecureServerSocket socket;\n+                    auto address = socketBindListen(socket, listen_host, port, /* secure = */ true);\n+                    socket.setReceiveTimeout(config().getUInt64(\"keeper_server.socket_receive_timeout_sec\", DBMS_DEFAULT_RECEIVE_TIMEOUT_SEC));\n+                    socket.setSendTimeout(config().getUInt64(\"keeper_server.socket_send_timeout_sec\", DBMS_DEFAULT_SEND_TIMEOUT_SEC));\n+                    return ProtocolServerAdapter(\n+                        listen_host,\n+                        secure_port_name,\n+                        \"Keeper with secure protocol (tcp_secure): \" + address.toString(),\n+                        std::make_unique<TCPServer>(\n+                            new KeeperTCPHandlerFactory(*this, true), server_pool, socket));\n+#else\n+                    UNUSED(port);\n+                    throw Exception{\"SSL support for TCP protocol is disabled because Poco library was built without NetSSL support.\",\n+                        ErrorCodes::SUPPORT_IS_DISABLED};\n+#endif\n+                });\n+        }\n+#else\n+        throw Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"ClickHouse server built without NuRaft library. Cannot use internal coordination.\");\n+#endif\n+\n+    }\n+\n+    for (auto & server : servers_to_start_before_tables)\n+    {\n+        server.start();\n+        LOG_INFO(log, \"Listening for {}\", server.getDescription());\n+    }\n+\n     auto & access_control = global_context->getAccessControl();\n     if (config().has(\"custom_settings_prefixes\"))\n         access_control.setCustomSettingsPrefixes(config().getString(\"custom_settings_prefixes\"));\n@@ -1075,83 +1152,6 @@ if (ThreadFuzzer::instance().isEffective())\n     /// try set up encryption. There are some errors in config, error will be printed and server wouldn't start.\n     CompressionCodecEncrypted::Configuration::instance().load(config(), \"encryption_codecs\");\n \n-    const auto listen_hosts = getListenHosts(config());\n-    const auto listen_try = getListenTry(config());\n-\n-    if (config().has(\"keeper_server\"))\n-    {\n-#if USE_NURAFT\n-        //// If we don't have configured connection probably someone trying to use clickhouse-server instead\n-        //// of clickhouse-keeper, so start synchronously.\n-        bool can_initialize_keeper_async = false;\n-\n-        if (has_zookeeper) /// We have configured connection to some zookeeper cluster\n-        {\n-            /// If we cannot connect to some other node from our cluster then we have to wait our Keeper start\n-            /// synchronously.\n-            can_initialize_keeper_async = global_context->tryCheckClientConnectionToMyKeeperCluster();\n-        }\n-        /// Initialize keeper RAFT.\n-        global_context->initializeKeeperDispatcher(can_initialize_keeper_async);\n-        FourLetterCommandFactory::registerCommands(*global_context->getKeeperDispatcher());\n-\n-        for (const auto & listen_host : listen_hosts)\n-        {\n-            /// TCP Keeper\n-            const char * port_name = \"keeper_server.tcp_port\";\n-            createServer(\n-                config(), listen_host, port_name, listen_try, /* start_server: */ false,\n-                servers_to_start_before_tables,\n-                [&](UInt16 port) -> ProtocolServerAdapter\n-                {\n-                    Poco::Net::ServerSocket socket;\n-                    auto address = socketBindListen(socket, listen_host, port);\n-                    socket.setReceiveTimeout(settings.receive_timeout);\n-                    socket.setSendTimeout(settings.send_timeout);\n-                    return ProtocolServerAdapter(\n-                        listen_host,\n-                        port_name,\n-                        \"Keeper (tcp): \" + address.toString(),\n-                        std::make_unique<TCPServer>(\n-                            new KeeperTCPHandlerFactory(*this, false), server_pool, socket));\n-                });\n-\n-            const char * secure_port_name = \"keeper_server.tcp_port_secure\";\n-            createServer(\n-                config(), listen_host, secure_port_name, listen_try, /* start_server: */ false,\n-                servers_to_start_before_tables,\n-                [&](UInt16 port) -> ProtocolServerAdapter\n-                {\n-#if USE_SSL\n-                    Poco::Net::SecureServerSocket socket;\n-                    auto address = socketBindListen(socket, listen_host, port, /* secure = */ true);\n-                    socket.setReceiveTimeout(settings.receive_timeout);\n-                    socket.setSendTimeout(settings.send_timeout);\n-                    return ProtocolServerAdapter(\n-                        listen_host,\n-                        secure_port_name,\n-                        \"Keeper with secure protocol (tcp_secure): \" + address.toString(),\n-                        std::make_unique<TCPServer>(\n-                            new KeeperTCPHandlerFactory(*this, true), server_pool, socket));\n-#else\n-                    UNUSED(port);\n-                    throw Exception{\"SSL support for TCP protocol is disabled because Poco library was built without NetSSL support.\",\n-                        ErrorCodes::SUPPORT_IS_DISABLED};\n-#endif\n-                });\n-        }\n-#else\n-        throw Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"ClickHouse server built without NuRaft library. Cannot use internal coordination.\");\n-#endif\n-\n-    }\n-\n-    for (auto & server : servers_to_start_before_tables)\n-    {\n-        server.start();\n-        LOG_INFO(log, \"Listening for {}\", server.getDescription());\n-    }\n-\n     SCOPE_EXIT({\n         /// Stop reloading of the main config. This must be done before `global_context->shutdown()` because\n         /// otherwise the reloading may pass a changed config to some destroyed parts of ContextSharedPart.\n",
  "test_patch": "diff --git a/tests/integration/test_keeper_and_access_storage/__init__.py b/tests/integration/test_keeper_and_access_storage/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_keeper_and_access_storage/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_keeper_and_access_storage/configs/keeper.xml b/tests/integration/test_keeper_and_access_storage/configs/keeper.xml\nnew file mode 100644\nindex 000000000000..6dd54aebed14\n--- /dev/null\n+++ b/tests/integration/test_keeper_and_access_storage/configs/keeper.xml\n@@ -0,0 +1,36 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>1</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <raft_logs_level>trace</raft_logs_level>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+        </coordination_settings>\n+        <raft_configuration>\n+            <server>\n+                <can_become_leader>true</can_become_leader>\n+                <hostname>node1</hostname>\n+                <id>1</id>\n+                <port>2888</port>\n+                <priority>1</priority>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+\n+    <user_directories>\n+        <replicated>\n+            <zookeeper_path>/clickhouse/access</zookeeper_path>\n+        </replicated>\n+    </user_directories>\n+\n+    <zookeeper>\n+        <node index=\"1\">\n+            <host>node1</host>\n+            <port>9181</port>\n+        </node>\n+    </zookeeper>\n+</clickhouse>\ndiff --git a/tests/integration/test_keeper_and_access_storage/test.py b/tests/integration/test_keeper_and_access_storage/test.py\nnew file mode 100644\nindex 000000000000..3a3c7535a85a\n--- /dev/null\n+++ b/tests/integration/test_keeper_and_access_storage/test.py\n@@ -0,0 +1,21 @@\n+#!/usr/bin/env python3\n+\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', main_configs=['configs/keeper.xml'], stay_alive=True)\n+\n+# test that server is able to start\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+def test_create_replicated(started_cluster):\n+    assert node1.query(\"SELECT 1\") == \"1\\n\"\n",
  "problem_statement": "ClickHouse server fails to start when both Keeper and user replication enabled \n**Describe what's wrong**\r\n\r\nClickHouse server fails to start with the following config:\r\n\r\n```\r\n<keeper_server>\r\n    <tcp_port>2181</tcp_port>\r\n    <server_id>1</server_id>\r\n    <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\r\n    <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\r\n    <coordination_settings>\r\n        <operation_timeout_ms>5000</operation_timeout_ms>\r\n        <raft_logs_level>trace</raft_logs_level>\r\n        <session_timeout_ms>10000</session_timeout_ms>\r\n    </coordination_settings>\r\n    <raft_configuration>\r\n        <server>\r\n            <can_become_leader>true</can_become_leader>\r\n            <hostname>sas-n2pu34sjneaepg49.db.yandex.net</hostname>\r\n            <id>1</id>\r\n            <port>2888</port>\r\n            <priority>1</priority>\r\n            <start_as_follower>false</start_as_follower>\r\n        </server>\r\n    </raft_configuration>\r\n</keeper_server>\r\n\r\n<user_directories>\r\n    <users_xml>\r\n        <path>users.xml</path>\r\n    </users_xml>\r\n   <replicated>\r\n       <zookeeper_path>/clickhouse/access/</zookeeper_path>\r\n   </replicated>\r\n</user_directories>\r\n```\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes, it's reproducible on 22.1.3.7 and earlier versions (e.g. 21.11.10.1).\r\n\r\n**Expected behavior**\r\n\r\nClickHouse server starts and works.\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\n2022.01.25 10:58:06.320203 [ 47212 ] {} <Debug> ConfigReloader: Loading config '/etc/clickhouse-server/config.xml'\r\n2022.01.25 10:58:06.321356 [ 47212 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/config.xml', performing update on configuration\r\n2022.01.25 10:58:06.323213 [ 47212 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/config.xml', performed update on configuration\r\n2022.01.25 10:58:06.326661 [ 47212 ] {} <Debug> ConfigReloader: Loading config '/etc/clickhouse-server/users.xml'\r\n2022.01.25 10:58:06.327609 [ 47212 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performing update on configuration\r\n2022.01.25 10:58:06.328643 [ 47212 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performed update on configuration\r\n2022.01.25 10:58:06.328913 [ 47212 ] {} <Debug> Access(user directories): Added users.xml access storage 'users.xml', path: /etc/clickhouse-server/users.xml\r\n2022.01.25 10:58:06.328935 [ 47212 ] {} <Debug> Access(user directories): Added replicated access storage 'replicated'\r\n2022.01.25 10:58:06.330201 [ 47212 ] {} <Error> Application: Code: 999. Coordination::Exception: All connection tries failed while connecting to ZooKeeper. nodes: [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\nPoco::Exception. Code: 1000, e.code() = 111, Connection refused (version 22.1.3.7 (official build)), [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\nPoco::Exception. Code: 1000, e.code() = 111, Connection refused (version 22.1.3.7 (official build)), [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\nPoco::Exception. Code: 1000, e.code() = 111, Connection refused (version 22.1.3.7 (official build)), [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\n (Connection loss). (KEEPER_EXCEPTION), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa82d07a in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n1. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error, int) @ 0x14c1e7f5 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n2. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error) @ 0x14c1eb36 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n3. Coordination::ZooKeeper::connect(std::__1::vector<Coordination::ZooKeeper::Node, std::__1::allocator<Coordination::ZooKeeper::Node> > const&, Poco::Timespan) @ 0x14c61a2f in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n4. Coordination::ZooKeeper::ZooKeeper(std::__1::vector<Coordination::ZooKeeper::Node, std::__1::allocator<Coordination::ZooKeeper::Node> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Timespan, Poco::Timespan, Poco::Timespan, std::__1::shared_ptr<DB::ZooKeeperLog>) @ 0x14c5fedf in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n5. zkutil::ZooKeeper::init(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x14c21101 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n6. zkutil::ZooKeeper::ZooKeeper(Poco::Util::AbstractConfiguration const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::ZooKeeperLog>) @ 0x14c2380d in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n7. void std::__1::allocator<zkutil::ZooKeeper>::construct<zkutil::ZooKeeper, Poco::Util::AbstractConfiguration const&, char const (&) [10], std::__1::shared_ptr<DB::ZooKeeperLog> >(zkutil::ZooKeeper*, Poco::Util::AbstractConfiguration const&, char const (&) [10], std::__1::shared_ptr<DB::ZooKeeperLog>&&) @ 0x134c769b in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n8. DB::Context::getZooKeeper() const @ 0x134a6ab6 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n9. DB::ReplicatedAccessStorage::initializeZookeeper() @ 0x12f2be52 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n10. DB::ReplicatedAccessStorage::startup() @ 0x12f2bd0f in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n11. DB::AccessControl::addReplicatedStorage(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()> const&) @ 0x12e57aea in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n12. DB::AccessControl::addStoragesFromUserDirectoriesConfig(Poco::Util::AbstractConfiguration const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()> const&) @ 0x12e5a8a5 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n13. DB::AccessControl::addStoragesFromMainConfig(Poco::Util::AbstractConfiguration const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()> const&) @ 0x12e5be61 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n14. DB::Server::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa8b2ce9 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n15. Poco::Util::Application::run() @ 0x174746c6 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n16. DB::Server::run() @ 0xa8a8614 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n17. mainEntryClickHouseServer(int, char**) @ 0xa8a5bc7 in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n18. main @ 0xa82742a in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n19. __libc_start_main @ 0x21bf7 in /lib/x86_64-linux-gnu/libc-2.27.so\r\n20. _start @ 0xa6ae3ee in /usr/lib/debug/.build-id/d1/1bc54a7fe20e44.debug\r\n (version 22.1.3.7 (official build))\r\n2022.01.25 10:58:06.331876 [ 47212 ] {} <Error> Application: Coordination::Exception: All connection tries failed while connecting to ZooKeeper. nodes: [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\nPoco::Exception. Code: 1000, e.code() = 111, Connection refused (version 22.1.3.7 (official build)), [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\nPoco::Exception. Code: 1000, e.code() = 111, Connection refused (version 22.1.3.7 (official build)), [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\nPoco::Exception. Code: 1000, e.code() = 111, Connection refused (version 22.1.3.7 (official build)), [2a02:6b8:c23:168b:0:1589:e034:1c8a]:2181\r\n (Connection loss)\r\n2022.01.25 10:58:06.331966 [ 47212 ] {} <Information> Application: shutting down\r\n2022.01.25 10:58:06.331972 [ 47212 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2022.01.25 10:58:06.332029 [ 47213 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n2022.01.25 10:58:06.362269 [ 47211 ] {} <Information> Application: Child process exited normally with code 70.\r\n```\r\n\n",
  "hints_text": "",
  "created_at": "2022-01-25T13:42:21Z"
}