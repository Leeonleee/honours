{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 15408,
  "instance_id": "ClickHouse__ClickHouse-15408",
  "issue_numbers": [
    "5098",
    "12439"
  ],
  "base_commit": "9dd32ce30f6fae3dff3be687c53c0fcba88cd91a",
  "patch": "diff --git a/src/Formats/ProtobufSerializer.cpp b/src/Formats/ProtobufSerializer.cpp\nindex ac1d5f048e3e..c5781ee6c9ff 100644\n--- a/src/Formats/ProtobufSerializer.cpp\n+++ b/src/Formats/ProtobufSerializer.cpp\n@@ -1423,18 +1423,23 @@ namespace\n     };\n \n \n-    /// Serializes a ColumnVector<UInt32> containing dates to a field of any type except TYPE_MESSAGE, TYPE_GROUP, TYPE_BOOL, TYPE_ENUM.\n+    /// Serializes a ColumnVector<UInt32> containing datetimes to a field of any type except TYPE_MESSAGE, TYPE_GROUP, TYPE_BOOL, TYPE_ENUM.\n     class ProtobufSerializerDateTime : public ProtobufSerializerNumber<UInt32>\n     {\n     public:\n         ProtobufSerializerDateTime(\n-            const FieldDescriptor & field_descriptor_, const ProtobufReaderOrWriter & reader_or_writer_)\n-            : ProtobufSerializerNumber<UInt32>(field_descriptor_, reader_or_writer_)\n+            const DataTypeDateTime & type,\n+            const FieldDescriptor & field_descriptor_,\n+            const ProtobufReaderOrWriter & reader_or_writer_)\n+            : ProtobufSerializerNumber<UInt32>(field_descriptor_, reader_or_writer_),\n+            date_lut(type.getTimeZone())\n         {\n             setFunctions();\n         }\n \n     protected:\n+        const DateLUTImpl & date_lut;\n+\n         void setFunctions()\n         {\n             switch (field_typeid)\n@@ -1458,17 +1463,17 @@ namespace\n                 {\n                     write_function = [this](UInt32 value)\n                     {\n-                        dateTimeToString(value, text_buffer);\n+                        dateTimeToString(value, text_buffer, date_lut);\n                         writeStr(text_buffer);\n                     };\n \n                     read_function = [this]() -> UInt32\n                     {\n                         readStr(text_buffer);\n-                        return stringToDateTime(text_buffer);\n+                        return stringToDateTime(text_buffer, date_lut);\n                     };\n \n-                    default_function = [this]() -> UInt32 { return stringToDateTime(field_descriptor.default_value_string()); };\n+                    default_function = [this]() -> UInt32 { return stringToDateTime(field_descriptor.default_value_string(), date_lut); };\n                     break;\n                 }\n \n@@ -1477,17 +1482,17 @@ namespace\n             }\n         }\n \n-        static void dateTimeToString(time_t tm, String & str)\n+        static void dateTimeToString(time_t tm, String & str, const DateLUTImpl & lut)\n         {\n             WriteBufferFromString buf{str};\n-            writeDateTimeText(tm, buf);\n+            writeDateTimeText(tm, buf, lut);\n         }\n \n-        static time_t stringToDateTime(const String & str)\n+        static time_t stringToDateTime(const String & str, const DateLUTImpl & lut)\n         {\n             ReadBufferFromString buf{str};\n             time_t tm = 0;\n-            readDateTimeText(tm, buf);\n+            readDateTimeText(tm, buf, lut);\n             if (tm < 0)\n                 tm = 0;\n             return tm;\n@@ -2833,7 +2838,7 @@ namespace\n                 case TypeIndex::Float32: return std::make_unique<ProtobufSerializerNumber<Float32>>(field_descriptor, reader_or_writer);\n                 case TypeIndex::Float64: return std::make_unique<ProtobufSerializerNumber<Float64>>(field_descriptor, reader_or_writer);\n                 case TypeIndex::Date: return std::make_unique<ProtobufSerializerDate>(field_descriptor, reader_or_writer);\n-                case TypeIndex::DateTime: return std::make_unique<ProtobufSerializerDateTime>(field_descriptor, reader_or_writer);\n+                case TypeIndex::DateTime: return std::make_unique<ProtobufSerializerDateTime>(assert_cast<const DataTypeDateTime &>(*data_type), field_descriptor, reader_or_writer);\n                 case TypeIndex::DateTime64: return std::make_unique<ProtobufSerializerDateTime64>(assert_cast<const DataTypeDateTime64 &>(*data_type), field_descriptor, reader_or_writer);\n                 case TypeIndex::String: return std::make_unique<ProtobufSerializerString<false>>(field_descriptor, reader_or_writer);\n                 case TypeIndex::FixedString: return std::make_unique<ProtobufSerializerString<true>>(typeid_cast<std::shared_ptr<const DataTypeFixedString>>(data_type), field_descriptor, reader_or_writer);\ndiff --git a/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp b/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp\nindex 01c19deb837a..a921039c8240 100644\n--- a/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp\n+++ b/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp\n@@ -1,7 +1,7 @@\n-#include \"config_formats.h\"\n #include \"ArrowColumnToCHColumn.h\"\n \n #if USE_ARROW || USE_ORC || USE_PARQUET\n+\n #include <DataTypes/DataTypeFactory.h>\n #include <DataTypes/DataTypeNullable.h>\n #include <DataTypes/DataTypesDecimal.h>\n@@ -10,9 +10,11 @@\n #include <DataTypes/DataTypeLowCardinality.h>\n #include <DataTypes/DataTypeTuple.h>\n #include <DataTypes/DataTypeMap.h>\n+#include <DataTypes/DataTypeDate.h>\n #include <common/DateLUTImpl.h>\n #include <common/types.h>\n #include <Core/Block.h>\n+#include <Processors/Chunk.h>\n #include <Columns/ColumnString.h>\n #include <Columns/ColumnNullable.h>\n #include <Columns/ColumnArray.h>\n@@ -22,128 +24,153 @@\n #include <Columns/ColumnMap.h>\n #include <Interpreters/castColumn.h>\n #include <algorithm>\n-#include <fmt/format.h>\n+#include <arrow/array.h>\n+\n+\n+#define FOR_ARROW_NUMERIC_TYPES(M) \\\n+        M(arrow::Type::UINT8, DB::UInt8) \\\n+        M(arrow::Type::INT8, DB::Int8) \\\n+        M(arrow::Type::UINT16, DB::UInt16) \\\n+        M(arrow::Type::INT16, DB::Int16) \\\n+        M(arrow::Type::UINT32, DB::UInt32) \\\n+        M(arrow::Type::INT32, DB::Int32) \\\n+        M(arrow::Type::UINT64, DB::UInt64) \\\n+        M(arrow::Type::INT64, DB::Int64) \\\n+        M(arrow::Type::HALF_FLOAT, DB::Float32) \\\n+        M(arrow::Type::FLOAT, DB::Float32) \\\n+        M(arrow::Type::DOUBLE, DB::Float64)\n+\n+#define FOR_ARROW_INDEXES_TYPES(M) \\\n+        M(arrow::Type::UINT8, DB::UInt8) \\\n+        M(arrow::Type::INT8, DB::UInt8) \\\n+        M(arrow::Type::UINT16, DB::UInt16) \\\n+        M(arrow::Type::INT16, DB::UInt16) \\\n+        M(arrow::Type::UINT32, DB::UInt32) \\\n+        M(arrow::Type::INT32, DB::UInt32) \\\n+        M(arrow::Type::UINT64, DB::UInt64) \\\n+        M(arrow::Type::INT64, DB::UInt64)\n \n \n namespace DB\n {\n-    namespace ErrorCodes\n-    {\n-        extern const int UNKNOWN_TYPE;\n-        extern const int VALUE_IS_OUT_OF_RANGE_OF_DATA_TYPE;\n-        extern const int CANNOT_CONVERT_TYPE;\n-        extern const int CANNOT_INSERT_NULL_IN_ORDINARY_COLUMN;\n-        extern const int THERE_IS_NO_COLUMN;\n-        extern const int BAD_ARGUMENTS;\n-    }\n \n-    static const std::initializer_list<std::pair<arrow::Type::type, const char *>> arrow_type_to_internal_type =\n-    {\n-            {arrow::Type::UINT8, \"UInt8\"},\n-            {arrow::Type::INT8, \"Int8\"},\n-            {arrow::Type::UINT16, \"UInt16\"},\n-            {arrow::Type::INT16, \"Int16\"},\n-            {arrow::Type::UINT32, \"UInt32\"},\n-            {arrow::Type::INT32, \"Int32\"},\n-            {arrow::Type::UINT64, \"UInt64\"},\n-            {arrow::Type::INT64, \"Int64\"},\n-            {arrow::Type::HALF_FLOAT, \"Float32\"},\n-            {arrow::Type::FLOAT, \"Float32\"},\n-            {arrow::Type::DOUBLE, \"Float64\"},\n-\n-            {arrow::Type::BOOL, \"UInt8\"},\n-            {arrow::Type::DATE32, \"Date\"},\n-            {arrow::Type::DATE32, \"Date32\"},\n-            {arrow::Type::DATE64, \"DateTime\"},\n-            {arrow::Type::TIMESTAMP, \"DateTime\"},\n-\n-            {arrow::Type::STRING, \"String\"},\n-            {arrow::Type::BINARY, \"String\"},\n-\n-            // TODO: add other types that are convertible to internal ones:\n-            // 0. ENUM?\n-            // 1. UUID -> String\n-            // 2. JSON -> String\n-            // Full list of types: contrib/arrow/cpp/src/arrow/type.h\n-    };\n+namespace ErrorCodes\n+{\n+    extern const int UNKNOWN_TYPE;\n+    extern const int VALUE_IS_OUT_OF_RANGE_OF_DATA_TYPE;\n+    extern const int CANNOT_CONVERT_TYPE;\n+    extern const int CANNOT_INSERT_NULL_IN_ORDINARY_COLUMN;\n+    extern const int THERE_IS_NO_COLUMN;\n+    extern const int BAD_ARGUMENTS;\n+}\n+\n+static const std::initializer_list<std::pair<arrow::Type::type, const char *>> arrow_type_to_internal_type =\n+{\n+        {arrow::Type::UINT8, \"UInt8\"},\n+        {arrow::Type::INT8, \"Int8\"},\n+        {arrow::Type::UINT16, \"UInt16\"},\n+        {arrow::Type::INT16, \"Int16\"},\n+        {arrow::Type::UINT32, \"UInt32\"},\n+        {arrow::Type::INT32, \"Int32\"},\n+        {arrow::Type::UINT64, \"UInt64\"},\n+        {arrow::Type::INT64, \"Int64\"},\n+        {arrow::Type::HALF_FLOAT, \"Float32\"},\n+        {arrow::Type::FLOAT, \"Float32\"},\n+        {arrow::Type::DOUBLE, \"Float64\"},\n+\n+        {arrow::Type::BOOL, \"UInt8\"},\n+        {arrow::Type::DATE32, \"Date\"},\n+        {arrow::Type::DATE32, \"Date32\"},\n+        {arrow::Type::DATE64, \"DateTime\"},\n+        {arrow::Type::TIMESTAMP, \"DateTime\"},\n+\n+        {arrow::Type::STRING, \"String\"},\n+        {arrow::Type::BINARY, \"String\"},\n+\n+        // TODO: add other types that are convertible to internal ones:\n+        // 0. ENUM?\n+        // 1. UUID -> String\n+        // 2. JSON -> String\n+        // Full list of types: contrib/arrow/cpp/src/arrow/type.h\n+};\n \n /// Inserts numeric data right into internal column data to reduce an overhead\n-    template <typename NumericType, typename VectorType = ColumnVector<NumericType>>\n-    static void fillColumnWithNumericData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n-    {\n-        auto & column_data = static_cast<VectorType &>(internal_column).getData();\n-        column_data.reserve(arrow_column->length());\n+template <typename NumericType, typename VectorType = ColumnVector<NumericType>>\n+static void fillColumnWithNumericData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+{\n+    auto & column_data = static_cast<VectorType &>(internal_column).getData();\n+    column_data.reserve(arrow_column->length());\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n-        {\n-            std::shared_ptr<arrow::Array> chunk = arrow_column->chunk(chunk_i);\n-            /// buffers[0] is a null bitmap and buffers[1] are actual values\n-            std::shared_ptr<arrow::Buffer> buffer = chunk->data()->buffers[1];\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    {\n+        std::shared_ptr<arrow::Array> chunk = arrow_column->chunk(chunk_i);\n+        /// buffers[0] is a null bitmap and buffers[1] are actual values\n+        std::shared_ptr<arrow::Buffer> buffer = chunk->data()->buffers[1];\n \n-            const auto * raw_data = reinterpret_cast<const NumericType *>(buffer->data());\n-            column_data.insert_assume_reserved(raw_data, raw_data + chunk->length());\n-        }\n+        const auto * raw_data = reinterpret_cast<const NumericType *>(buffer->data());\n+        column_data.insert_assume_reserved(raw_data, raw_data + chunk->length());\n     }\n+}\n \n /// Inserts chars and offsets right into internal column data to reduce an overhead.\n /// Internal offsets are shifted by one to the right in comparison with Arrow ones. So the last offset should map to the end of all chars.\n /// Also internal strings are null terminated.\n-    static void fillColumnWithStringData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+static void fillColumnWithStringData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+{\n+    PaddedPODArray<UInt8> & column_chars_t = assert_cast<ColumnString &>(internal_column).getChars();\n+    PaddedPODArray<UInt64> & column_offsets = assert_cast<ColumnString &>(internal_column).getOffsets();\n+\n+    size_t chars_t_size = 0;\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n     {\n-        PaddedPODArray<UInt8> & column_chars_t = assert_cast<ColumnString &>(internal_column).getChars();\n-        PaddedPODArray<UInt64> & column_offsets = assert_cast<ColumnString &>(internal_column).getOffsets();\n+        arrow::BinaryArray & chunk = dynamic_cast<arrow::BinaryArray &>(*(arrow_column->chunk(chunk_i)));\n+        const size_t chunk_length = chunk.length();\n \n-        size_t chars_t_size = 0;\n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+        if (chunk_length > 0)\n         {\n-            arrow::BinaryArray & chunk = static_cast<arrow::BinaryArray &>(*(arrow_column->chunk(chunk_i)));\n-            const size_t chunk_length = chunk.length();\n-\n-            if (chunk_length > 0)\n-            {\n-                chars_t_size += chunk.value_offset(chunk_length - 1) + chunk.value_length(chunk_length - 1);\n-                chars_t_size += chunk_length; /// additional space for null bytes\n-            }\n+            chars_t_size += chunk.value_offset(chunk_length - 1) + chunk.value_length(chunk_length - 1);\n+            chars_t_size += chunk_length; /// additional space for null bytes\n         }\n+    }\n \n-        column_chars_t.reserve(chars_t_size);\n-        column_offsets.reserve(arrow_column->length());\n+    column_chars_t.reserve(chars_t_size);\n+    column_offsets.reserve(arrow_column->length());\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n-        {\n-            arrow::BinaryArray & chunk = static_cast<arrow::BinaryArray &>(*(arrow_column->chunk(chunk_i)));\n-            std::shared_ptr<arrow::Buffer> buffer = chunk.value_data();\n-            const size_t chunk_length = chunk.length();\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    {\n+        arrow::BinaryArray & chunk = dynamic_cast<arrow::BinaryArray &>(*(arrow_column->chunk(chunk_i)));\n+        std::shared_ptr<arrow::Buffer> buffer = chunk.value_data();\n+        const size_t chunk_length = chunk.length();\n \n-            for (size_t offset_i = 0; offset_i != chunk_length; ++offset_i)\n+        for (size_t offset_i = 0; offset_i != chunk_length; ++offset_i)\n+        {\n+            if (!chunk.IsNull(offset_i) && buffer)\n             {\n-                if (!chunk.IsNull(offset_i) && buffer)\n-                {\n-                    const auto * raw_data = buffer->data() + chunk.value_offset(offset_i);\n-                    column_chars_t.insert_assume_reserved(raw_data, raw_data + chunk.value_length(offset_i));\n-                }\n-                column_chars_t.emplace_back('\\0');\n-\n-                column_offsets.emplace_back(column_chars_t.size());\n+                const auto * raw_data = buffer->data() + chunk.value_offset(offset_i);\n+                column_chars_t.insert_assume_reserved(raw_data, raw_data + chunk.value_length(offset_i));\n             }\n+            column_chars_t.emplace_back('\\0');\n+\n+            column_offsets.emplace_back(column_chars_t.size());\n         }\n     }\n+}\n \n-    static void fillColumnWithBooleanData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n-    {\n-        auto & column_data = assert_cast<ColumnVector<UInt8> &>(internal_column).getData();\n-        column_data.reserve(arrow_column->length());\n+static void fillColumnWithBooleanData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+{\n+    auto & column_data = assert_cast<ColumnVector<UInt8> &>(internal_column).getData();\n+    column_data.reserve(arrow_column->length());\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n-        {\n-            arrow::BooleanArray & chunk = static_cast<arrow::BooleanArray &>(*(arrow_column->chunk(chunk_i)));\n-            /// buffers[0] is a null bitmap and buffers[1] are actual values\n-            std::shared_ptr<arrow::Buffer> buffer = chunk.data()->buffers[1];\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    {\n+        arrow::BooleanArray & chunk = dynamic_cast<arrow::BooleanArray &>(*(arrow_column->chunk(chunk_i)));\n+        /// buffers[0] is a null bitmap and buffers[1] are actual values\n+        std::shared_ptr<arrow::Buffer> buffer = chunk.data()->buffers[1];\n \n-            for (size_t bool_i = 0; bool_i != static_cast<size_t>(chunk.length()); ++bool_i)\n-                column_data.emplace_back(chunk.Value(bool_i));\n-        }\n+        for (size_t bool_i = 0; bool_i != static_cast<size_t>(chunk.length()); ++bool_i)\n+            column_data.emplace_back(chunk.Value(bool_i));\n     }\n+}\n \n /// Arrow stores Parquet::DATE in Int32, while ClickHouse stores Date in UInt16. Therefore, it should be checked before saving\n static void fillColumnWithDate32Data(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n@@ -153,487 +180,492 @@ static void fillColumnWithDate32Data(std::shared_ptr<arrow::ChunkedArray> & arro\n \n     for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n     {\n-        arrow::Date32Array & chunk = static_cast<arrow::Date32Array &>(*(arrow_column->chunk(chunk_i)));\n+        arrow::Date32Array & chunk = dynamic_cast<arrow::Date32Array &>(*(arrow_column->chunk(chunk_i)));\n \n         for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n         {\n             UInt32 days_num = static_cast<UInt32>(chunk.Value(value_i));\n+\n             if (days_num > DATE_LUT_MAX_DAY_NUM)\n-            {\n-                // TODO: will it rollback correctly?\n-                throw Exception\n-                    {\n-                        fmt::format(\"Input value {} of a column \\\"{}\\\" is greater than max allowed Date value, which is {}\", days_num, internal_column.getName(), DATE_LUT_MAX_DAY_NUM),\n-                        ErrorCodes::VALUE_IS_OUT_OF_RANGE_OF_DATA_TYPE\n-                    };\n-            }\n+                throw Exception(ErrorCodes::VALUE_IS_OUT_OF_RANGE_OF_DATA_TYPE,\n+                    \"Input value {} of a column '{}' is greater than max allowed Date value, which is {}\",\n+                    days_num, internal_column.getName(), DATE_LUT_MAX_DAY_NUM);\n \n             column_data.emplace_back(days_num);\n         }\n     }\n }\n \n-    static void fillDate32ColumnWithDate32Data(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+static void fillDate32ColumnWithDate32Data(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+{\n+    PaddedPODArray<Int32> & column_data = assert_cast<ColumnVector<Int32> &>(internal_column).getData();\n+    column_data.reserve(arrow_column->length());\n+\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n     {\n-        PaddedPODArray<Int32> & column_data = assert_cast<ColumnVector<Int32> &>(internal_column).getData();\n-        column_data.reserve(arrow_column->length());\n+        arrow::Date32Array & chunk = dynamic_cast<arrow::Date32Array &>(*(arrow_column->chunk(chunk_i)));\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+        for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n         {\n-            arrow::Date32Array & chunk = static_cast<arrow::Date32Array &>(*(arrow_column->chunk(chunk_i)));\n-\n-            for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n-            {\n-                Int32 days_num = static_cast<Int32>(chunk.Value(value_i));\n-                if (days_num > DATE_LUT_MAX_EXTEND_DAY_NUM)\n-                {\n-                    // TODO: will it rollback correctly?\n-                    throw Exception\n-                        {\n-                            fmt::format(\"Input value {} of a column \\\"{}\\\" is greater than max allowed Date value, which is {}\", days_num, internal_column.getName(), DATE_LUT_MAX_DAY_NUM),\n-                            ErrorCodes::VALUE_IS_OUT_OF_RANGE_OF_DATA_TYPE\n-                        };\n-                }\n+            Int32 days_num = static_cast<Int32>(chunk.Value(value_i));\n+            if (days_num > DATE_LUT_MAX_EXTEND_DAY_NUM)\n+                throw Exception(ErrorCodes::VALUE_IS_OUT_OF_RANGE_OF_DATA_TYPE,\n+                    \"Input value {} of a column '{}' is greater than max allowed Date value, which is {}\", days_num, internal_column.getName(), DATE_LUT_MAX_DAY_NUM);\n \n-                column_data.emplace_back(days_num);\n-            }\n+            column_data.emplace_back(days_num);\n         }\n     }\n+}\n \n /// Arrow stores Parquet::DATETIME in Int64, while ClickHouse stores DateTime in UInt32. Therefore, it should be checked before saving\n-    static void fillColumnWithDate64Data(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n-    {\n-        auto & column_data = assert_cast<ColumnVector<UInt32> &>(internal_column).getData();\n-        column_data.reserve(arrow_column->length());\n+static void fillColumnWithDate64Data(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+{\n+    auto & column_data = assert_cast<ColumnVector<UInt32> &>(internal_column).getData();\n+    column_data.reserve(arrow_column->length());\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    {\n+        auto & chunk = dynamic_cast<arrow::Date64Array &>(*(arrow_column->chunk(chunk_i)));\n+        for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n         {\n-            auto & chunk = static_cast<arrow::Date64Array &>(*(arrow_column->chunk(chunk_i)));\n-            for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n-            {\n-                auto timestamp = static_cast<UInt32>(chunk.Value(value_i) / 1000); // Always? in ms\n-                column_data.emplace_back(timestamp);\n-            }\n+            auto timestamp = static_cast<UInt32>(chunk.Value(value_i) / 1000); // Always? in ms\n+            column_data.emplace_back(timestamp);\n         }\n     }\n+}\n+\n+static void fillColumnWithTimestampData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+{\n+    auto & column_data = assert_cast<ColumnVector<UInt32> &>(internal_column).getData();\n+    column_data.reserve(arrow_column->length());\n \n-    static void fillColumnWithTimestampData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n     {\n-        auto & column_data = assert_cast<ColumnVector<UInt32> &>(internal_column).getData();\n-        column_data.reserve(arrow_column->length());\n+        auto & chunk = dynamic_cast<arrow::TimestampArray &>(*(arrow_column->chunk(chunk_i)));\n+        const auto & type = static_cast<const ::arrow::TimestampType &>(*chunk.type());\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+        UInt32 divide = 1;\n+        const auto unit = type.unit();\n+        switch (unit)\n         {\n-            auto & chunk = static_cast<arrow::TimestampArray &>(*(arrow_column->chunk(chunk_i)));\n-            const auto & type = static_cast<const ::arrow::TimestampType &>(*chunk.type());\n-\n-            UInt32 divide = 1;\n-            const auto unit = type.unit();\n-            switch (unit)\n-            {\n-                case arrow::TimeUnit::SECOND:\n-                    divide = 1;\n-                    break;\n-                case arrow::TimeUnit::MILLI:\n-                    divide = 1000;\n-                    break;\n-                case arrow::TimeUnit::MICRO:\n-                    divide = 1000000;\n-                    break;\n-                case arrow::TimeUnit::NANO:\n-                    divide = 1000000000;\n-                    break;\n-            }\n+            case arrow::TimeUnit::SECOND:\n+                divide = 1;\n+                break;\n+            case arrow::TimeUnit::MILLI:\n+                divide = 1000;\n+                break;\n+            case arrow::TimeUnit::MICRO:\n+                divide = 1000000;\n+                break;\n+            case arrow::TimeUnit::NANO:\n+                divide = 1000000000;\n+                break;\n+        }\n \n-            for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n-            {\n-                auto timestamp = static_cast<UInt32>(chunk.Value(value_i) / divide); // ms! TODO: check other 's' 'ns' ...\n-                column_data.emplace_back(timestamp);\n-            }\n+        for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n+        {\n+            auto timestamp = static_cast<UInt32>(chunk.Value(value_i) / divide); // ms! TODO: check other 's' 'ns' ...\n+            column_data.emplace_back(timestamp);\n         }\n     }\n+}\n \n-    template <typename DecimalType, typename DecimalArray>\n-    static void fillColumnWithDecimalData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n-    {\n-        auto & column = assert_cast<ColumnDecimal<DecimalType> &>(internal_column);\n-        auto & column_data = column.getData();\n-        column_data.reserve(arrow_column->length());\n+template <typename DecimalType, typename DecimalArray>\n+static void fillColumnWithDecimalData(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & internal_column)\n+{\n+    auto & column = assert_cast<ColumnDecimal<DecimalType> &>(internal_column);\n+    auto & column_data = column.getData();\n+    column_data.reserve(arrow_column->length());\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    {\n+        auto & chunk = static_cast<DecimalArray &>(*(arrow_column->chunk(chunk_i)));\n+        for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n         {\n-            auto & chunk = static_cast<DecimalArray &>(*(arrow_column->chunk(chunk_i)));\n-            for (size_t value_i = 0, length = static_cast<size_t>(chunk.length()); value_i < length; ++value_i)\n-            {\n-                column_data.emplace_back(chunk.IsNull(value_i) ? DecimalType(0) : *reinterpret_cast<const DecimalType *>(chunk.Value(value_i))); // TODO: copy column\n-            }\n+            column_data.emplace_back(chunk.IsNull(value_i) ? DecimalType(0) : *reinterpret_cast<const DecimalType *>(chunk.Value(value_i))); // TODO: copy column\n         }\n     }\n+}\n \n /// Creates a null bytemap from arrow's null bitmap\n-    static void fillByteMapFromArrowColumn(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & bytemap)\n-    {\n-        PaddedPODArray<UInt8> & bytemap_data = assert_cast<ColumnVector<UInt8> &>(bytemap).getData();\n-        bytemap_data.reserve(arrow_column->length());\n+static void fillByteMapFromArrowColumn(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & bytemap)\n+{\n+    PaddedPODArray<UInt8> & bytemap_data = assert_cast<ColumnVector<UInt8> &>(bytemap).getData();\n+    bytemap_data.reserve(arrow_column->length());\n \n-        for (size_t chunk_i = 0; chunk_i != static_cast<size_t>(arrow_column->num_chunks()); ++chunk_i)\n-        {\n-            std::shared_ptr<arrow::Array> chunk = arrow_column->chunk(chunk_i);\n+    for (size_t chunk_i = 0; chunk_i != static_cast<size_t>(arrow_column->num_chunks()); ++chunk_i)\n+    {\n+        std::shared_ptr<arrow::Array> chunk = arrow_column->chunk(chunk_i);\n \n-            for (size_t value_i = 0; value_i != static_cast<size_t>(chunk->length()); ++value_i)\n-                bytemap_data.emplace_back(chunk->IsNull(value_i));\n-        }\n+        for (size_t value_i = 0; value_i != static_cast<size_t>(chunk->length()); ++value_i)\n+            bytemap_data.emplace_back(chunk->IsNull(value_i));\n     }\n+}\n \n-    static void fillOffsetsFromArrowListColumn(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & offsets)\n-    {\n-        ColumnArray::Offsets & offsets_data = assert_cast<ColumnVector<UInt64> &>(offsets).getData();\n-        offsets_data.reserve(arrow_column->length());\n+static void fillOffsetsFromArrowListColumn(std::shared_ptr<arrow::ChunkedArray> & arrow_column, IColumn & offsets)\n+{\n+    ColumnArray::Offsets & offsets_data = assert_cast<ColumnVector<UInt64> &>(offsets).getData();\n+    offsets_data.reserve(arrow_column->length());\n \n-        for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n-        {\n-            arrow::ListArray & list_chunk = static_cast<arrow::ListArray &>(*(arrow_column->chunk(chunk_i)));\n-            auto arrow_offsets_array = list_chunk.offsets();\n-            auto & arrow_offsets = static_cast<arrow::Int32Array &>(*arrow_offsets_array);\n-            auto start = offsets_data.back();\n-            for (int64_t i = 1; i < arrow_offsets.length(); ++i)\n-                offsets_data.emplace_back(start + arrow_offsets.Value(i));\n-        }\n+    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+    {\n+        arrow::ListArray & list_chunk = dynamic_cast<arrow::ListArray &>(*(arrow_column->chunk(chunk_i)));\n+        auto arrow_offsets_array = list_chunk.offsets();\n+        auto & arrow_offsets = dynamic_cast<arrow::Int32Array &>(*arrow_offsets_array);\n+        auto start = offsets_data.back();\n+        for (int64_t i = 1; i < arrow_offsets.length(); ++i)\n+            offsets_data.emplace_back(start + arrow_offsets.Value(i));\n     }\n-    static ColumnPtr createAndFillColumnWithIndexesData(std::shared_ptr<arrow::ChunkedArray> & arrow_column)\n+}\n+static ColumnPtr createAndFillColumnWithIndexesData(std::shared_ptr<arrow::ChunkedArray> & arrow_column)\n+{\n+    switch (arrow_column->type()->id())\n     {\n-        switch (arrow_column->type()->id())\n-        {\n #    define DISPATCH(ARROW_NUMERIC_TYPE, CPP_NUMERIC_TYPE) \\\n-            case ARROW_NUMERIC_TYPE: \\\n-            { \\\n-                    auto column = DataTypeNumber<CPP_NUMERIC_TYPE>().createColumn(); \\\n-                    fillColumnWithNumericData<CPP_NUMERIC_TYPE>(arrow_column, *column); \\\n-                    return column; \\\n-            }\n-            FOR_ARROW_INDEXES_TYPES(DISPATCH)\n-#    undef DISPATCH\n-            default:\n-                throw Exception(fmt::format(\"Unsupported type for indexes in LowCardinality: {}.\", arrow_column->type()->name()), ErrorCodes::BAD_ARGUMENTS);\n+        case ARROW_NUMERIC_TYPE: \\\n+        { \\\n+                auto column = DataTypeNumber<CPP_NUMERIC_TYPE>().createColumn(); \\\n+                fillColumnWithNumericData<CPP_NUMERIC_TYPE>(arrow_column, *column); \\\n+                return column; \\\n         }\n+        FOR_ARROW_INDEXES_TYPES(DISPATCH)\n+#    undef DISPATCH\n+        default:\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Unsupported type for indexes in LowCardinality: {}.\", arrow_column->type()->name());\n     }\n+}\n \n-    static void readColumnFromArrowColumn(\n-        std::shared_ptr<arrow::ChunkedArray> & arrow_column,\n-        IColumn & internal_column,\n-        const std::string & column_name,\n-        const std::string & format_name,\n-        bool is_nullable,\n-        std::unordered_map<String, ColumnPtr> dictionary_values)\n+static void readColumnFromArrowColumn(\n+    std::shared_ptr<arrow::ChunkedArray> & arrow_column,\n+    IColumn & internal_column,\n+    const std::string & column_name,\n+    const std::string & format_name,\n+    bool is_nullable,\n+    std::unordered_map<String, ColumnPtr> dictionary_values)\n+{\n+    if (internal_column.isNullable())\n     {\n-        if (internal_column.isNullable())\n-        {\n-            ColumnNullable & column_nullable = assert_cast<ColumnNullable &>(internal_column);\n-            readColumnFromArrowColumn(arrow_column, column_nullable.getNestedColumn(), column_name, format_name, true, dictionary_values);\n-            fillByteMapFromArrowColumn(arrow_column, column_nullable.getNullMapColumn());\n-            return;\n-        }\n+        ColumnNullable & column_nullable = assert_cast<ColumnNullable &>(internal_column);\n+        readColumnFromArrowColumn(\n+            arrow_column, column_nullable.getNestedColumn(), column_name, format_name, true, dictionary_values);\n+        fillByteMapFromArrowColumn(arrow_column, column_nullable.getNullMapColumn());\n+        return;\n+    }\n \n-        /// TODO: check if a column is const?\n-        if (!is_nullable && arrow_column->null_count() && arrow_column->type()->id() != arrow::Type::LIST\n-            && arrow_column->type()->id() != arrow::Type::MAP && arrow_column->type()->id() != arrow::Type::STRUCT)\n-        {\n-            throw Exception\n-                {\n-                    fmt::format(\"Can not insert NULL data into non-nullable column \\\"{}\\\".\", column_name),\n-                    ErrorCodes::CANNOT_INSERT_NULL_IN_ORDINARY_COLUMN\n-                };\n-        }\n+    /// TODO: check if a column is const?\n+    if (!is_nullable && arrow_column->null_count() && arrow_column->type()->id() != arrow::Type::LIST\n+        && arrow_column->type()->id() != arrow::Type::MAP && arrow_column->type()->id() != arrow::Type::STRUCT)\n+    {\n+        throw Exception\n+            {\n+                ErrorCodes::CANNOT_INSERT_NULL_IN_ORDINARY_COLUMN,\n+                \"Can not insert NULL data into non-nullable column \\\"{}\\\".\", column_name\n+            };\n+    }\n \n-        switch (arrow_column->type()->id())\n+    switch (arrow_column->type()->id())\n+    {\n+        case arrow::Type::STRING:\n+        case arrow::Type::BINARY:\n+            //case arrow::Type::FIXED_SIZE_BINARY:\n+            fillColumnWithStringData(arrow_column, internal_column);\n+            break;\n+        case arrow::Type::BOOL:\n+            fillColumnWithBooleanData(arrow_column, internal_column);\n+            break;\n+        case arrow::Type::DATE32:\n+            if (WhichDataType(internal_column.getDataType()).isUInt16())\n+            {\n+                fillColumnWithDate32Data(arrow_column, internal_column);\n+            }\n+            else\n+            {\n+                fillDate32ColumnWithDate32Data(arrow_column, internal_column);\n+            }\n+            break;\n+        case arrow::Type::DATE64:\n+            fillColumnWithDate64Data(arrow_column, internal_column);\n+            break;\n+        case arrow::Type::TIMESTAMP:\n+            fillColumnWithTimestampData(arrow_column, internal_column);\n+            break;\n+        case arrow::Type::DECIMAL128:\n+            fillColumnWithDecimalData<Decimal128, arrow::Decimal128Array>(arrow_column, internal_column /*, internal_nested_type*/);\n+            break;\n+        case arrow::Type::DECIMAL256:\n+            fillColumnWithDecimalData<Decimal256, arrow::Decimal256Array>(arrow_column, internal_column /*, internal_nested_type*/);\n+            break;\n+        case arrow::Type::MAP: [[fallthrough]];\n+        case arrow::Type::LIST:\n         {\n-            case arrow::Type::STRING:\n-            case arrow::Type::BINARY:\n-                //case arrow::Type::FIXED_SIZE_BINARY:\n-                fillColumnWithStringData(arrow_column, internal_column);\n-                break;\n-            case arrow::Type::BOOL:\n-                fillColumnWithBooleanData(arrow_column, internal_column);\n-                break;\n-            case arrow::Type::DATE32:\n-                if (WhichDataType(internal_column.getDataType()).isUInt16())\n-                {\n-                    fillColumnWithDate32Data(arrow_column, internal_column);\n-                }\n-                else\n-                {\n-                    fillDate32ColumnWithDate32Data(arrow_column, internal_column);\n-                }\n-                break;\n-            case arrow::Type::DATE64:\n-                fillColumnWithDate64Data(arrow_column, internal_column);\n-                break;\n-            case arrow::Type::TIMESTAMP:\n-                fillColumnWithTimestampData(arrow_column, internal_column);\n-                break;\n-            case arrow::Type::DECIMAL128:\n-                fillColumnWithDecimalData<Decimal128, arrow::Decimal128Array>(arrow_column, internal_column /*, internal_nested_type*/);\n-                break;\n-            case arrow::Type::DECIMAL256:\n-                fillColumnWithDecimalData<Decimal256, arrow::Decimal256Array>(arrow_column, internal_column /*, internal_nested_type*/);\n-                break;\n-            case arrow::Type::MAP: [[fallthrough]];\n-            case arrow::Type::LIST:\n+            arrow::ArrayVector array_vector;\n+            array_vector.reserve(arrow_column->num_chunks());\n+            for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n             {\n-                arrow::ArrayVector array_vector;\n-                array_vector.reserve(arrow_column->num_chunks());\n-                for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n-                {\n-                    arrow::ListArray & list_chunk = static_cast<arrow::ListArray &>(*(arrow_column->chunk(chunk_i)));\n-                    std::shared_ptr<arrow::Array> chunk = list_chunk.values();\n-                    array_vector.emplace_back(std::move(chunk));\n-                }\n-                auto arrow_nested_column = std::make_shared<arrow::ChunkedArray>(array_vector);\n+                arrow::ListArray & list_chunk = dynamic_cast<arrow::ListArray &>(*(arrow_column->chunk(chunk_i)));\n+                std::shared_ptr<arrow::Array> chunk = list_chunk.values();\n+                array_vector.emplace_back(std::move(chunk));\n+            }\n+            auto arrow_nested_column = std::make_shared<arrow::ChunkedArray>(array_vector);\n \n-                ColumnArray & column_array = arrow_column->type()->id() == arrow::Type::MAP\n-                    ? assert_cast<ColumnMap &>(internal_column).getNestedColumn()\n-                    : assert_cast<ColumnArray &>(internal_column);\n+            ColumnArray & column_array = arrow_column->type()->id() == arrow::Type::MAP\n+                ? assert_cast<ColumnMap &>(internal_column).getNestedColumn()\n+                : assert_cast<ColumnArray &>(internal_column);\n \n-                readColumnFromArrowColumn(arrow_nested_column, column_array.getData(), column_name, format_name, false, dictionary_values);\n-                fillOffsetsFromArrowListColumn(arrow_column, column_array.getOffsetsColumn());\n-                break;\n-            }\n-            case arrow::Type::STRUCT:\n-            {\n-                ColumnTuple & column_tuple = assert_cast<ColumnTuple &>(internal_column);\n-                int fields_count = column_tuple.tupleSize();\n-                std::vector<arrow::ArrayVector> nested_arrow_columns(fields_count);\n-                for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n-                {\n-                    arrow::StructArray & struct_chunk = static_cast<arrow::StructArray &>(*(arrow_column->chunk(chunk_i)));\n-                    for (int i = 0; i < fields_count; ++i)\n-                        nested_arrow_columns[i].emplace_back(struct_chunk.field(i));\n-                }\n+            readColumnFromArrowColumn(\n+                arrow_nested_column, column_array.getData(), column_name, format_name, false, dictionary_values);\n \n-                for (int i = 0; i != fields_count; ++i)\n-                {\n-                    auto nested_arrow_column = std::make_shared<arrow::ChunkedArray>(nested_arrow_columns[i]);\n-                    readColumnFromArrowColumn(nested_arrow_column, column_tuple.getColumn(i), column_name, format_name, false, dictionary_values);\n-                }\n-                break;\n+            fillOffsetsFromArrowListColumn(arrow_column, column_array.getOffsetsColumn());\n+            break;\n+        }\n+        case arrow::Type::STRUCT:\n+        {\n+            ColumnTuple & column_tuple = assert_cast<ColumnTuple &>(internal_column);\n+            int fields_count = column_tuple.tupleSize();\n+            std::vector<arrow::ArrayVector> nested_arrow_columns(fields_count);\n+            for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+            {\n+                arrow::StructArray & struct_chunk = dynamic_cast<arrow::StructArray &>(*(arrow_column->chunk(chunk_i)));\n+                for (int i = 0; i < fields_count; ++i)\n+                    nested_arrow_columns[i].emplace_back(struct_chunk.field(i));\n             }\n-            case arrow::Type::DICTIONARY:\n+\n+            for (int i = 0; i != fields_count; ++i)\n             {\n-                ColumnLowCardinality & column_lc = assert_cast<ColumnLowCardinality &>(internal_column);\n-                auto & dict_values = dictionary_values[column_name];\n-                /// Load dictionary values only once and reuse it.\n-                if (!dict_values)\n-                {\n-                    arrow::ArrayVector dict_array;\n-                    for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n-                    {\n-                        arrow::DictionaryArray & dict_chunk = static_cast<arrow::DictionaryArray &>(*(arrow_column->chunk(chunk_i)));\n-                        dict_array.emplace_back(dict_chunk.dictionary());\n-                    }\n-                    auto arrow_dict_column = std::make_shared<arrow::ChunkedArray>(dict_array);\n-\n-                    auto dict_column = IColumn::mutate(column_lc.getDictionaryPtr());\n-                    auto * uniq_column = static_cast<IColumnUnique *>(dict_column.get());\n-                    auto values_column = uniq_column->getNestedColumn()->cloneEmpty();\n-                    readColumnFromArrowColumn(arrow_dict_column, *values_column, column_name, format_name, false, dictionary_values);\n-                    uniq_column->uniqueInsertRangeFrom(*values_column, 0, values_column->size());\n-                    dict_values = std::move(dict_column);\n-                }\n+                auto nested_arrow_column = std::make_shared<arrow::ChunkedArray>(nested_arrow_columns[i]);\n+                readColumnFromArrowColumn(\n+                    nested_arrow_column, column_tuple.getColumn(i), column_name, format_name, false, dictionary_values);\n+            }\n+            break;\n+        }\n+        case arrow::Type::DICTIONARY:\n+        {\n+            ColumnLowCardinality & column_lc = assert_cast<ColumnLowCardinality &>(internal_column);\n+            auto & dict_values = dictionary_values[column_name];\n \n-                arrow::ArrayVector indexes_array;\n+            /// Load dictionary values only once and reuse it.\n+            if (!dict_values)\n+            {\n+                arrow::ArrayVector dict_array;\n                 for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n                 {\n-                    arrow::DictionaryArray & dict_chunk = static_cast<arrow::DictionaryArray &>(*(arrow_column->chunk(chunk_i)));\n-                    indexes_array.emplace_back(dict_chunk.indices());\n+                    arrow::DictionaryArray & dict_chunk = dynamic_cast<arrow::DictionaryArray &>(*(arrow_column->chunk(chunk_i)));\n+                    dict_array.emplace_back(dict_chunk.dictionary());\n                 }\n+                auto arrow_dict_column = std::make_shared<arrow::ChunkedArray>(dict_array);\n+\n+                auto dict_column = IColumn::mutate(column_lc.getDictionaryPtr());\n+                auto * uniq_column = static_cast<IColumnUnique *>(dict_column.get());\n+                auto values_column = uniq_column->getNestedColumn()->cloneEmpty();\n+                readColumnFromArrowColumn(\n+                    arrow_dict_column, *values_column, column_name, format_name, false, dictionary_values);\n+                uniq_column->uniqueInsertRangeFrom(*values_column, 0, values_column->size());\n+                dict_values = std::move(dict_column);\n+            }\n \n-                auto arrow_indexes_column = std::make_shared<arrow::ChunkedArray>(indexes_array);\n-                auto indexes_column = createAndFillColumnWithIndexesData(arrow_indexes_column);\n-\n-                auto new_column_lc = ColumnLowCardinality::create(dict_values, std::move(indexes_column));\n-                column_lc = std::move(*new_column_lc);\n-                break;\n+            arrow::ArrayVector indexes_array;\n+            for (size_t chunk_i = 0, num_chunks = static_cast<size_t>(arrow_column->num_chunks()); chunk_i < num_chunks; ++chunk_i)\n+            {\n+                arrow::DictionaryArray & dict_chunk = dynamic_cast<arrow::DictionaryArray &>(*(arrow_column->chunk(chunk_i)));\n+                indexes_array.emplace_back(dict_chunk.indices());\n             }\n-#    define DISPATCH(ARROW_NUMERIC_TYPE, CPP_NUMERIC_TYPE) \\\n-        case ARROW_NUMERIC_TYPE: \\\n-            fillColumnWithNumericData<CPP_NUMERIC_TYPE>(arrow_column, internal_column); \\\n+\n+            auto arrow_indexes_column = std::make_shared<arrow::ChunkedArray>(indexes_array);\n+            auto indexes_column = createAndFillColumnWithIndexesData(arrow_indexes_column);\n+\n+            auto new_column_lc = ColumnLowCardinality::create(dict_values, std::move(indexes_column));\n+            column_lc = std::move(*new_column_lc);\n             break;\n+        }\n+#    define DISPATCH(ARROW_NUMERIC_TYPE, CPP_NUMERIC_TYPE) \\\n+    case ARROW_NUMERIC_TYPE: \\\n+        fillColumnWithNumericData<CPP_NUMERIC_TYPE>(arrow_column, internal_column); \\\n+        break;\n \n-            FOR_ARROW_NUMERIC_TYPES(DISPATCH)\n+        FOR_ARROW_NUMERIC_TYPES(DISPATCH)\n #    undef DISPATCH\n-                // TODO: support TIMESTAMP_MICROS and TIMESTAMP_MILLIS with truncated micro- and milliseconds?\n-                // TODO: read JSON as a string?\n-                // TODO: read UUID as a string?\n-            default:\n-                throw Exception\n-                    {\n-                        fmt::format(R\"(Unsupported {} type \"{}\" of an input column \"{}\".)\", format_name, arrow_column->type()->name(), column_name),\n-                        ErrorCodes::UNKNOWN_TYPE\n-                    };\n-        }\n+            // TODO: support TIMESTAMP_MICROS and TIMESTAMP_MILLIS with truncated micro- and milliseconds?\n+            // TODO: read JSON as a string?\n+            // TODO: read UUID as a string?\n+        default:\n+            throw Exception(ErrorCodes::UNKNOWN_TYPE,\n+                \"Unsupported {} type '{}' of an input column '{}'.\", format_name, arrow_column->type()->name(), column_name);\n     }\n+}\n \n-    static DataTypePtr getInternalType(std::shared_ptr<arrow::DataType> arrow_type, const DataTypePtr & column_type, const std::string & column_name, const std::string & format_name)\n+static DataTypePtr getInternalType(\n+    std::shared_ptr<arrow::DataType> arrow_type,\n+    const DataTypePtr & column_type,\n+    const std::string & column_name,\n+    const std::string & format_name)\n+{\n+    if (column_type->isNullable())\n     {\n-        if (column_type->isNullable())\n-        {\n-            DataTypePtr nested_type = assert_cast<const DataTypeNullable *>(column_type.get())->getNestedType();\n-            return makeNullable(getInternalType(arrow_type, nested_type, column_name, format_name));\n-        }\n+        DataTypePtr nested_type = assert_cast<const DataTypeNullable *>(column_type.get())->getNestedType();\n+        return makeNullable(getInternalType(arrow_type, nested_type, column_name, format_name));\n+    }\n \n-        if (arrow_type->id() == arrow::Type::DECIMAL128)\n-        {\n-            const auto * decimal_type = static_cast<arrow::DecimalType *>(arrow_type.get());\n-            return std::make_shared<DataTypeDecimal<Decimal128>>(decimal_type->precision(), decimal_type->scale());\n-        }\n+    if (arrow_type->id() == arrow::Type::DECIMAL128)\n+    {\n+        const auto & decimal_type = dynamic_cast<const arrow::DecimalType &>(*arrow_type);\n+        return std::make_shared<DataTypeDecimal<Decimal128>>(decimal_type.precision(), decimal_type.scale());\n+    }\n \n-        if (arrow_type->id() == arrow::Type::DECIMAL256)\n-        {\n-            const auto * decimal_type = static_cast<arrow::DecimalType *>(arrow_type.get());\n-            return std::make_shared<DataTypeDecimal<Decimal256>>(decimal_type->precision(), decimal_type->scale());\n-        }\n+    if (arrow_type->id() == arrow::Type::DECIMAL256)\n+    {\n+        const auto & decimal_type = dynamic_cast<const arrow::DecimalType &>(*arrow_type);\n+        return std::make_shared<DataTypeDecimal<Decimal256>>(decimal_type.precision(), decimal_type.scale());\n+    }\n \n-        if (arrow_type->id() == arrow::Type::LIST)\n-        {\n-            const auto * list_type = static_cast<arrow::ListType *>(arrow_type.get());\n-            auto list_nested_type = list_type->value_type();\n+    if (arrow_type->id() == arrow::Type::LIST)\n+    {\n+        const auto & list_type = dynamic_cast<const arrow::ListType &>(*arrow_type);\n+        auto list_nested_type = list_type.value_type();\n \n-            const DataTypeArray * array_type = typeid_cast<const DataTypeArray *>(column_type.get());\n-            if (!array_type)\n-                throw Exception{fmt::format(\"Cannot convert arrow LIST type to a not Array ClickHouse type {}.\", column_type->getName()), ErrorCodes::CANNOT_CONVERT_TYPE};\n+        const DataTypeArray * array_type = typeid_cast<const DataTypeArray *>(column_type.get());\n+        if (!array_type)\n+            throw Exception{ErrorCodes::CANNOT_CONVERT_TYPE,\n+                \"Cannot convert arrow LIST type to a not Array ClickHouse type {}.\", column_type->getName()};\n \n-            return std::make_shared<DataTypeArray>(getInternalType(list_nested_type, array_type->getNestedType(), column_name, format_name));\n-        }\n+        return std::make_shared<DataTypeArray>(getInternalType(list_nested_type, array_type->getNestedType(), column_name, format_name));\n+    }\n \n-        if (arrow_type->id() == arrow::Type::STRUCT)\n-        {\n-            const auto * struct_type = static_cast<arrow::StructType *>(arrow_type.get());\n-            const DataTypeTuple * tuple_type = typeid_cast<const DataTypeTuple *>(column_type.get());\n-            if (!tuple_type)\n-                throw Exception{fmt::format(\"Cannot convert arrow STRUCT type to a not Tuple ClickHouse type {}.\", column_type->getName()), ErrorCodes::CANNOT_CONVERT_TYPE};\n-\n-            const DataTypes & tuple_nested_types = tuple_type->getElements();\n-            int internal_fields_num = tuple_nested_types.size();\n-            /// If internal column has less elements then arrow struct, we will select only first internal_fields_num columns.\n-            if (internal_fields_num > struct_type->num_fields())\n-                throw Exception\n-                    {\n-                        fmt::format(\n-                            \"Cannot convert arrow STRUCT with {} fields to a ClickHouse Tuple with {} elements: {}.\",\n-                            struct_type->num_fields(),\n-                            internal_fields_num,\n-                            column_type->getName()),\n-                        ErrorCodes::CANNOT_CONVERT_TYPE\n-                    };\n-\n-            DataTypes nested_types;\n-            for (int i = 0; i < internal_fields_num; ++i)\n-                nested_types.push_back(getInternalType(struct_type->field(i)->type(), tuple_nested_types[i], column_name, format_name));\n-\n-            return std::make_shared<DataTypeTuple>(std::move(nested_types));\n-        }\n+    if (arrow_type->id() == arrow::Type::STRUCT)\n+    {\n+        const auto & struct_type = dynamic_cast<const arrow::StructType &>(*arrow_type);\n+        const DataTypeTuple * tuple_type = typeid_cast<const DataTypeTuple *>(column_type.get());\n+        if (!tuple_type)\n+            throw Exception{ErrorCodes::CANNOT_CONVERT_TYPE,\n+                \"Cannot convert arrow STRUCT type to a not Tuple ClickHouse type {}.\", column_type->getName()};\n+\n+        const DataTypes & tuple_nested_types = tuple_type->getElements();\n+        int internal_fields_num = tuple_nested_types.size();\n+        /// If internal column has less elements then arrow struct, we will select only first internal_fields_num columns.\n+        if (internal_fields_num > struct_type.num_fields())\n+            throw Exception(\n+                    ErrorCodes::CANNOT_CONVERT_TYPE,\n+                    \"Cannot convert arrow STRUCT with {} fields to a ClickHouse Tuple with {} elements: {}.\",\n+                    struct_type.num_fields(),\n+                    internal_fields_num,\n+                    column_type->getName());\n+\n+        DataTypes nested_types;\n+        for (int i = 0; i < internal_fields_num; ++i)\n+            nested_types.push_back(getInternalType(struct_type.field(i)->type(), tuple_nested_types[i], column_name, format_name));\n+\n+        return std::make_shared<DataTypeTuple>(std::move(nested_types));\n+    }\n \n-        if (arrow_type->id() == arrow::Type::DICTIONARY)\n-        {\n-            const auto * arrow_dict_type = static_cast<arrow::DictionaryType *>(arrow_type.get());\n-            const auto * lc_type = typeid_cast<const DataTypeLowCardinality *>(column_type.get());\n-            /// We allow to insert arrow dictionary into a non-LowCardinality column.\n-            const auto & dict_type = lc_type ? lc_type->getDictionaryType() : column_type;\n-            return std::make_shared<DataTypeLowCardinality>(getInternalType(arrow_dict_type->value_type(), dict_type, column_name, format_name));\n-        }\n+    if (arrow_type->id() == arrow::Type::DICTIONARY)\n+    {\n+        const auto & arrow_dict_type = dynamic_cast<const arrow::DictionaryType &>(*arrow_type);\n+        const auto * lc_type = typeid_cast<const DataTypeLowCardinality *>(column_type.get());\n+        /// We allow to insert arrow dictionary into a non-LowCardinality column.\n+        const auto & dict_type = lc_type ? lc_type->getDictionaryType() : column_type;\n+        return std::make_shared<DataTypeLowCardinality>(getInternalType(arrow_dict_type.value_type(), dict_type, column_name, format_name));\n+    }\n \n-        if (arrow_type->id() == arrow::Type::MAP)\n-        {\n-            const auto * arrow_map_type = typeid_cast<arrow::MapType *>(arrow_type.get());\n-            const auto * map_type = typeid_cast<const DataTypeMap *>(column_type.get());\n-            if (!map_type)\n-                throw Exception{fmt::format(\"Cannot convert arrow MAP type to a not Map ClickHouse type {}.\", column_type->getName()), ErrorCodes::CANNOT_CONVERT_TYPE};\n-\n-            return std::make_shared<DataTypeMap>(\n-                getInternalType(arrow_map_type->key_type(), map_type->getKeyType(), column_name, format_name),\n-                getInternalType(arrow_map_type->item_type(), map_type->getValueType(), column_name, format_name)\n-                );\n-        }\n+    if (arrow_type->id() == arrow::Type::MAP)\n+    {\n+        const auto & arrow_map_type = typeid_cast<const arrow::MapType &>(*arrow_type);\n+        const auto * map_type = typeid_cast<const DataTypeMap *>(column_type.get());\n+        if (!map_type)\n+            throw Exception{ErrorCodes::CANNOT_CONVERT_TYPE, \"Cannot convert arrow MAP type to a not Map ClickHouse type {}.\", column_type->getName()};\n+\n+        return std::make_shared<DataTypeMap>(\n+            getInternalType(arrow_map_type.key_type(), map_type->getKeyType(), column_name, format_name),\n+            getInternalType(arrow_map_type.item_type(), map_type->getValueType(), column_name, format_name));\n+    }\n \n-        auto filter = [=](auto && elem)\n+    if (arrow_type->id() == arrow::Type::UINT16\n+        && (isDate(column_type) || isDateTime(column_type) || isDate32(column_type) || isDateTime64(column_type)))\n+    {\n+        /// Read UInt16 as Date. It will allow correct conversion to DateTime further.\n+        return std::make_shared<DataTypeDate>();\n+    }\n+\n+    auto filter = [=](auto && elem)\n+    {\n+        auto which = WhichDataType(column_type);\n+        if (arrow_type->id() == arrow::Type::DATE32 && which.isDateOrDate32())\n         {\n-            auto which = WhichDataType(column_type);\n-            if (arrow_type->id() == arrow::Type::DATE32 && which.isDateOrDate32())\n-            {\n-                return (strcmp(elem.second, \"Date\") == 0 && which.isDate()) || (strcmp(elem.second, \"Date32\") == 0 && which.isDate32());\n-            }\n-            else\n-            {\n-                return elem.first == arrow_type->id();\n-            }\n-        };\n-        if (const auto * internal_type_it = std::find_if(arrow_type_to_internal_type.begin(), arrow_type_to_internal_type.end(), filter);\n-            internal_type_it != arrow_type_to_internal_type.end())\n+            return (strcmp(elem.second, \"Date\") == 0 && which.isDate())\n+                || (strcmp(elem.second, \"Date32\") == 0 && which.isDate32());\n+        }\n+        else\n         {\n-            return DataTypeFactory::instance().get(internal_type_it->second);\n+            return elem.first == arrow_type->id();\n         }\n-        throw Exception\n-            {\n-                fmt::format(R\"(The type \"{}\" of an input column \"{}\" is not supported for conversion from a {} data format.)\", arrow_type->name(), column_name, format_name),\n-                ErrorCodes::CANNOT_CONVERT_TYPE\n-            };\n+    };\n+    if (const auto * internal_type_it = std::find_if(arrow_type_to_internal_type.begin(), arrow_type_to_internal_type.end(), filter);\n+        internal_type_it != arrow_type_to_internal_type.end())\n+    {\n+        return DataTypeFactory::instance().get(internal_type_it->second);\n     }\n \n-    ArrowColumnToCHColumn::ArrowColumnToCHColumn(const Block & header_, std::shared_ptr<arrow::Schema> schema_, const std::string & format_name_) : header(header_), format_name(format_name_)\n+    throw Exception(ErrorCodes::CANNOT_CONVERT_TYPE,\n+        \"The type '{}' of an input column '{}' is not supported for conversion from {} data format.\",\n+        arrow_type->name(), column_name, format_name);\n+}\n+\n+ArrowColumnToCHColumn::ArrowColumnToCHColumn(const Block & header_, std::shared_ptr<arrow::Schema> schema_, const std::string & format_name_)\n+    : header(header_), format_name(format_name_)\n+{\n+    for (const auto & field : schema_->fields())\n     {\n-        for (const auto & field : schema_->fields())\n+        if (header.has(field->name()))\n         {\n-            if (header.has(field->name()))\n-            {\n-                const auto column_type = recursiveRemoveLowCardinality(header.getByName(field->name()).type);\n-                name_to_internal_type[field->name()] = getInternalType(field->type(), column_type, field->name(), format_name);\n-            }\n+            const auto column_type = recursiveRemoveLowCardinality(header.getByName(field->name()).type);\n+            name_to_internal_type[field->name()] = getInternalType(field->type(), column_type, field->name(), format_name);\n         }\n     }\n+}\n \n-    void ArrowColumnToCHColumn::arrowTableToCHChunk(Chunk & res, std::shared_ptr<arrow::Table> & table)\n-    {\n-        Columns columns_list;\n-        UInt64 num_rows = 0;\n-\n-        columns_list.reserve(header.rows());\n+void ArrowColumnToCHColumn::arrowTableToCHChunk(Chunk & res, std::shared_ptr<arrow::Table> & table)\n+{\n+    Columns columns_list;\n+    UInt64 num_rows = 0;\n \n-        using NameToColumnPtr = std::unordered_map<std::string, std::shared_ptr<arrow::ChunkedArray>>;\n+    columns_list.reserve(header.rows());\n \n-        NameToColumnPtr name_to_column_ptr;\n-        for (const auto& column_name : table->ColumnNames())\n-        {\n-            std::shared_ptr<arrow::ChunkedArray> arrow_column = table->GetColumnByName(column_name);\n-            name_to_column_ptr[column_name] = arrow_column;\n-        }\n+    using NameToColumnPtr = std::unordered_map<std::string, std::shared_ptr<arrow::ChunkedArray>>;\n \n-        for (size_t column_i = 0, columns = header.columns(); column_i < columns; ++column_i)\n-        {\n-            const ColumnWithTypeAndName & header_column = header.getByPosition(column_i);\n+    NameToColumnPtr name_to_column_ptr;\n+    for (const auto & column_name : table->ColumnNames())\n+    {\n+        std::shared_ptr<arrow::ChunkedArray> arrow_column = table->GetColumnByName(column_name);\n+        name_to_column_ptr[column_name] = arrow_column;\n+    }\n \n-            if (name_to_column_ptr.find(header_column.name) == name_to_column_ptr.end())\n-                // TODO: What if some columns were not presented? Insert NULLs? What if a column is not nullable?\n-                throw Exception{fmt::format(\"Column \\\"{}\\\" is not presented in input data.\", header_column.name),\n-                                ErrorCodes::THERE_IS_NO_COLUMN};\n+    for (size_t column_i = 0, columns = header.columns(); column_i < columns; ++column_i)\n+    {\n+        const ColumnWithTypeAndName & header_column = header.getByPosition(column_i);\n \n-            std::shared_ptr<arrow::ChunkedArray> arrow_column = name_to_column_ptr[header_column.name];\n+        if (name_to_column_ptr.find(header_column.name) == name_to_column_ptr.end())\n+            // TODO: What if some columns were not presented? Insert NULLs? What if a column is not nullable?\n+            throw Exception(ErrorCodes::THERE_IS_NO_COLUMN,\n+                \"Column '{}' is not presented in input data.\", header_column.name);\n \n-            DataTypePtr & internal_type = name_to_internal_type[header_column.name];\n-            MutableColumnPtr read_column = internal_type->createColumn();\n-            readColumnFromArrowColumn(arrow_column, *read_column, header_column.name, format_name, false, dictionary_values);\n+        std::shared_ptr<arrow::ChunkedArray> arrow_column = name_to_column_ptr[header_column.name];\n \n-            ColumnWithTypeAndName column;\n-            column.name = header_column.name;\n-            column.type = internal_type;\n-            column.column = std::move(read_column);\n+        DataTypePtr & internal_type = name_to_internal_type[header_column.name];\n+        MutableColumnPtr read_column = internal_type->createColumn();\n+        readColumnFromArrowColumn(arrow_column, *read_column, header_column.name, format_name, false, dictionary_values);\n \n-            column.column = castColumn(column, header_column.type);\n-            column.type = header_column.type;\n-            num_rows = column.column->size();\n-            columns_list.push_back(std::move(column.column));\n-        }\n+        ColumnWithTypeAndName column;\n+        column.name = header_column.name;\n+        column.type = internal_type;\n+        column.column = std::move(read_column);\n \n-        res.setColumns(columns_list, num_rows);\n+        column.column = castColumn(column, header_column.type);\n+        column.type = header_column.type;\n+        num_rows = column.column->size();\n+        columns_list.push_back(std::move(column.column));\n     }\n+\n+    res.setColumns(columns_list, num_rows);\n }\n+\n+}\n+\n #endif\ndiff --git a/src/Processors/Formats/Impl/ArrowColumnToCHColumn.h b/src/Processors/Formats/Impl/ArrowColumnToCHColumn.h\nindex 7da54a8a02d3..3ce4e42a9bc6 100644\n--- a/src/Processors/Formats/Impl/ArrowColumnToCHColumn.h\n+++ b/src/Processors/Formats/Impl/ArrowColumnToCHColumn.h\n@@ -1,24 +1,19 @@\n #pragma once\n+\n #include \"config_formats.h\"\n \n #if USE_ARROW || USE_ORC || USE_PARQUET\n \n #include <DataTypes/IDataType.h>\n-#include <DataTypes/DataTypesNumber.h>\n-#include <DataTypes/DataTypeDate.h>\n-#include <DataTypes/DataTypeDateTime.h>\n-#include <DataTypes/DataTypeString.h>\n-#include <arrow/type.h>\n-#include <Columns/ColumnVector.h>\n #include <arrow/table.h>\n-#include <arrow/array.h>\n-#include <arrow/buffer.h>\n-#include <Processors/Chunk.h>\n-#include <Core/Block.h>\n+\n \n namespace DB\n {\n \n+class Block;\n+class Chunk;\n+\n class ArrowColumnToCHColumn\n {\n public:\n@@ -27,37 +22,16 @@ class ArrowColumnToCHColumn\n     void arrowTableToCHChunk(Chunk & res, std::shared_ptr<arrow::Table> & table);\n \n private:\n-#define FOR_ARROW_NUMERIC_TYPES(M) \\\n-        M(arrow::Type::UINT8, DB::UInt8) \\\n-        M(arrow::Type::INT8, DB::Int8) \\\n-        M(arrow::Type::UINT16, DB::UInt16) \\\n-        M(arrow::Type::INT16, DB::Int16) \\\n-        M(arrow::Type::UINT32, DB::UInt32) \\\n-        M(arrow::Type::INT32, DB::Int32) \\\n-        M(arrow::Type::UINT64, DB::UInt64) \\\n-        M(arrow::Type::INT64, DB::Int64) \\\n-        M(arrow::Type::HALF_FLOAT, DB::Float32) \\\n-        M(arrow::Type::FLOAT, DB::Float32) \\\n-        M(arrow::Type::DOUBLE, DB::Float64)\n-\n-#define FOR_ARROW_INDEXES_TYPES(M) \\\n-        M(arrow::Type::UINT8, DB::UInt8) \\\n-        M(arrow::Type::INT8, DB::UInt8) \\\n-        M(arrow::Type::UINT16, DB::UInt16) \\\n-        M(arrow::Type::INT16, DB::UInt16) \\\n-        M(arrow::Type::UINT32, DB::UInt32) \\\n-        M(arrow::Type::INT32, DB::UInt32) \\\n-        M(arrow::Type::UINT64, DB::UInt64) \\\n-        M(arrow::Type::INT64, DB::UInt64)\n-\n-\n     const Block & header;\n     std::unordered_map<std::string, DataTypePtr> name_to_internal_type;\n     const std::string format_name;\n+\n     /// Map {column name : dictionary column}.\n     /// To avoid converting dictionary from Arrow Dictionary\n     /// to LowCardinality every chunk we save it and reuse.\n     std::unordered_map<std::string, ColumnPtr> dictionary_values;\n };\n+\n }\n+\n #endif\ndiff --git a/src/Processors/Formats/Impl/CHColumnToArrowColumn.cpp b/src/Processors/Formats/Impl/CHColumnToArrowColumn.cpp\nindex 230b28c657e1..0f502b361621 100644\n--- a/src/Processors/Formats/Impl/CHColumnToArrowColumn.cpp\n+++ b/src/Processors/Formats/Impl/CHColumnToArrowColumn.cpp\n@@ -23,6 +23,30 @@\n #include <arrow/type.h>\n #include <arrow/util/decimal.h>\n \n+#define FOR_INTERNAL_NUMERIC_TYPES(M) \\\n+        M(UInt8, arrow::UInt8Builder) \\\n+        M(Int8, arrow::Int8Builder) \\\n+        M(UInt16, arrow::UInt16Builder) \\\n+        M(Int16, arrow::Int16Builder) \\\n+        M(UInt32, arrow::UInt32Builder) \\\n+        M(Int32, arrow::Int32Builder) \\\n+        M(UInt64, arrow::UInt64Builder) \\\n+        M(Int64, arrow::Int64Builder) \\\n+        M(Float32, arrow::FloatBuilder) \\\n+        M(Float64, arrow::DoubleBuilder)\n+\n+#define FOR_ARROW_TYPES(M) \\\n+        M(UINT8, arrow::UInt8Type) \\\n+        M(INT8, arrow::Int8Type) \\\n+        M(UINT16, arrow::UInt16Type) \\\n+        M(INT16, arrow::Int16Type) \\\n+        M(UINT32, arrow::UInt32Type) \\\n+        M(INT32, arrow::Int32Type) \\\n+        M(UINT64, arrow::UInt64Type) \\\n+        M(INT64, arrow::Int64Type) \\\n+        M(FLOAT, arrow::FloatType) \\\n+        M(DOUBLE, arrow::DoubleType)  \\\n+        M(STRING, arrow::StringType)\n \n namespace DB\n {\n@@ -46,11 +70,8 @@ namespace DB\n         {\"Float32\", arrow::float32()},\n         {\"Float64\", arrow::float64()},\n \n-        //{\"Date\", arrow::date64()},\n-        //{\"Date\", arrow::date32()},\n-        {\"Date\", arrow::uint16()}, // CHECK\n-        //{\"DateTime\", arrow::date64()}, // BUG! saves as date32\n-        {\"DateTime\", arrow::uint32()},\n+        {\"Date\", arrow::uint16()},      /// uint16 is used instead of date32, because Apache Arrow cannot correctly serialize Date32Array.\n+        {\"DateTime\", arrow::uint32()},  /// uint32 is used instead of date64, because we don't need milliseconds.\n \n         {\"String\", arrow::binary()},\n         {\"FixedString\", arrow::binary()},\n@@ -265,11 +286,11 @@ namespace DB\n         auto value_type = assert_cast<arrow::DictionaryType *>(array_builder->type().get())->value_type();\n \n #define DISPATCH(ARROW_TYPE_ID, ARROW_TYPE) \\\n-                if (arrow::Type::ARROW_TYPE_ID == value_type->id()) \\\n-                { \\\n-                    fillArrowArrayWithLowCardinalityColumnDataImpl<ARROW_TYPE>(column_name, column, column_type, null_bytemap, array_builder, format_name, start, end, dictionary_values); \\\n-                    return; \\\n-                }\n+        if (arrow::Type::ARROW_TYPE_ID == value_type->id()) \\\n+        { \\\n+            fillArrowArrayWithLowCardinalityColumnDataImpl<ARROW_TYPE>(column_name, column, column_type, null_bytemap, array_builder, format_name, start, end, dictionary_values); \\\n+            return; \\\n+        }\n \n         FOR_ARROW_TYPES(DISPATCH)\n #undef DISPATCH\n@@ -337,7 +358,6 @@ namespace DB\n         size_t end)\n     {\n         const auto & internal_data = assert_cast<const ColumnVector<UInt32> &>(*write_column).getData();\n-        //arrow::Date64Builder builder;\n         arrow::UInt32Builder & builder = assert_cast<arrow::UInt32Builder &>(*array_builder);\n         arrow::Status status;\n \n@@ -346,8 +366,6 @@ namespace DB\n             if (null_bytemap && (*null_bytemap)[value_i])\n                 status = builder.AppendNull();\n             else\n-                /// Implicitly converts UInt16 to Int32\n-                //status = date_builder.Append(static_cast<int64_t>(internal_data[value_i]) * 1000); // now ms. TODO check other units\n                 status = builder.Append(internal_data[value_i]);\n \n             checkStatus(status, write_column->getName(), format_name);\n@@ -367,7 +385,7 @@ namespace DB\n     {\n         const String column_type_name = column_type->getFamilyName();\n \n-        if (\"Nullable\" == column_type_name)\n+        if (column_type->isNullable())\n         {\n             const ColumnNullable * column_nullable = assert_cast<const ColumnNullable *>(column.get());\n             ColumnPtr nested_column = column_nullable->getNestedColumnPtr();\n@@ -376,35 +394,35 @@ namespace DB\n             const PaddedPODArray<UInt8> & bytemap = assert_cast<const ColumnVector<UInt8> &>(*null_column).getData();\n             fillArrowArray(column_name, nested_column, nested_type, &bytemap, array_builder, format_name, start, end, dictionary_values);\n         }\n-        else if (\"String\" == column_type_name)\n+        else if (isString(column_type))\n         {\n             fillArrowArrayWithStringColumnData<ColumnString>(column, null_bytemap, format_name, array_builder, start, end);\n         }\n-        else if (\"FixedString\" == column_type_name)\n+        else if (isFixedString(column_type))\n         {\n             fillArrowArrayWithStringColumnData<ColumnFixedString>(column, null_bytemap, format_name, array_builder, start, end);\n         }\n-        else if (\"Date\" == column_type_name)\n+        else if (isDate(column_type))\n         {\n             fillArrowArrayWithDateColumnData(column, null_bytemap, format_name, array_builder, start, end);\n         }\n-        else if (\"DateTime\" == column_type_name)\n+        else if (isDateTime(column_type))\n         {\n             fillArrowArrayWithDateTimeColumnData(column, null_bytemap, format_name, array_builder, start, end);\n         }\n-        else if (\"Array\" == column_type_name)\n+        else if (isArray(column_type))\n         {\n             fillArrowArrayWithArrayColumnData<arrow::ListBuilder>(column_name, column, column_type, null_bytemap, array_builder, format_name, start, end, dictionary_values);\n         }\n-        else if (\"Tuple\" == column_type_name)\n+        else if (isTuple(column_type))\n         {\n             fillArrowArrayWithTupleColumnData(column_name, column, column_type, null_bytemap, array_builder, format_name, start, end, dictionary_values);\n         }\n-        else if (\"LowCardinality\" == column_type_name)\n+        else if (column_type->getTypeId() == TypeIndex::LowCardinality)\n         {\n             fillArrowArrayWithLowCardinalityColumnData(column_name, column, column_type, null_bytemap, array_builder, format_name, start, end, dictionary_values);\n         }\n-        else if (\"Map\" == column_type_name)\n+        else if (isMap(column_type))\n         {\n             ColumnPtr column_array = assert_cast<const ColumnMap *>(column.get())->getNestedColumnPtr();\n             DataTypePtr array_type = assert_cast<const DataTypeMap *>(column_type.get())->getNestedType();\n@@ -437,10 +455,10 @@ namespace DB\n                 throw Exception{ErrorCodes::LOGICAL_ERROR, \"Cannot fill arrow array with decimal data with type {}\", column_type_name};\n         }\n     #define DISPATCH(CPP_NUMERIC_TYPE, ARROW_BUILDER_TYPE) \\\n-                else if (#CPP_NUMERIC_TYPE == column_type_name) \\\n-                { \\\n-                    fillArrowArrayWithNumericColumnData<CPP_NUMERIC_TYPE, ARROW_BUILDER_TYPE>(column, null_bytemap, format_name, array_builder, start, end); \\\n-                }\n+        else if (#CPP_NUMERIC_TYPE == column_type_name) \\\n+        { \\\n+            fillArrowArrayWithNumericColumnData<CPP_NUMERIC_TYPE, ARROW_BUILDER_TYPE>(column, null_bytemap, format_name, array_builder, start, end); \\\n+        }\n \n         FOR_INTERNAL_NUMERIC_TYPES(DISPATCH)\n     #undef DISPATCH\n@@ -448,7 +466,7 @@ namespace DB\n         {\n             throw Exception\n                 {\n-                    fmt::format(R\"(Internal type \"{}\" of a column \"{}\" is not supported for conversion into a {} data format.)\", column_type_name, column_name, format_name),\n+                    fmt::format(\"Internal type '{}' of a column '{}' is not supported for conversion into {} data format.\", column_type_name, column_name, format_name),\n                     ErrorCodes::UNKNOWN_TYPE\n                 };\n         }\n@@ -502,14 +520,15 @@ namespace DB\n         }\n     }\n \n-    static std::shared_ptr<arrow::DataType> getArrowType(DataTypePtr column_type, ColumnPtr column, const std::string & column_name, const std::string & format_name, bool * is_column_nullable)\n+    static std::shared_ptr<arrow::DataType> getArrowType(\n+        DataTypePtr column_type, ColumnPtr column, const std::string & column_name, const std::string & format_name, bool * out_is_column_nullable)\n     {\n         if (column_type->isNullable())\n         {\n             DataTypePtr nested_type = assert_cast<const DataTypeNullable *>(column_type.get())->getNestedType();\n             ColumnPtr nested_column = assert_cast<const ColumnNullable *>(column.get())->getNestedColumnPtr();\n-            auto arrow_type = getArrowType(nested_type, nested_column, column_name, format_name, is_column_nullable);\n-            *is_column_nullable = true;\n+            auto arrow_type = getArrowType(nested_type, nested_column, column_name, format_name, out_is_column_nullable);\n+            *out_is_column_nullable = true;\n             return arrow_type;\n         }\n \n@@ -542,7 +561,7 @@ namespace DB\n         {\n             auto nested_type = assert_cast<const DataTypeArray *>(column_type.get())->getNestedType();\n             auto nested_column = assert_cast<const ColumnArray *>(column.get())->getDataPtr();\n-            auto nested_arrow_type = getArrowType(nested_type, nested_column, column_name, format_name, is_column_nullable);\n+            auto nested_arrow_type = getArrowType(nested_type, nested_column, column_name, format_name, out_is_column_nullable);\n             return arrow::list(nested_arrow_type);\n         }\n \n@@ -554,8 +573,8 @@ namespace DB\n             for (size_t i = 0; i != nested_types.size(); ++i)\n             {\n                 String name = column_name + \".\" + std::to_string(i);\n-                auto nested_arrow_type = getArrowType(nested_types[i], tuple_column->getColumnPtr(i), name, format_name, is_column_nullable);\n-                nested_fields.push_back(std::make_shared<arrow::Field>(name, nested_arrow_type, *is_column_nullable));\n+                auto nested_arrow_type = getArrowType(nested_types[i], tuple_column->getColumnPtr(i), name, format_name, out_is_column_nullable);\n+                nested_fields.push_back(std::make_shared<arrow::Field>(name, nested_arrow_type, *out_is_column_nullable));\n             }\n             return arrow::struct_(std::move(nested_fields));\n         }\n@@ -568,7 +587,7 @@ namespace DB\n             const auto & indexes_column = lc_column->getIndexesPtr();\n             return arrow::dictionary(\n                 getArrowTypeForLowCardinalityIndexes(indexes_column),\n-                getArrowType(nested_type, nested_column, column_name, format_name, is_column_nullable));\n+                getArrowType(nested_type, nested_column, column_name, format_name, out_is_column_nullable));\n         }\n \n         if (isMap(column_type))\n@@ -579,9 +598,8 @@ namespace DB\n \n             const auto & columns =  assert_cast<const ColumnMap *>(column.get())->getNestedData().getColumns();\n             return arrow::map(\n-                getArrowType(key_type, columns[0], column_name, format_name, is_column_nullable),\n-                getArrowType(val_type, columns[1], column_name, format_name, is_column_nullable)\n-            );\n+                getArrowType(key_type, columns[0], column_name, format_name, out_is_column_nullable),\n+                getArrowType(val_type, columns[1], column_name, format_name, out_is_column_nullable));\n         }\n \n         const std::string type_name = column_type->getFamilyName();\n@@ -594,8 +612,9 @@ namespace DB\n             return arrow_type_it->second;\n         }\n \n-        throw Exception{fmt::format(R\"(The type \"{}\" of a column \"{}\" is not supported for conversion into a {} data format.)\", column_type->getName(), column_name, format_name),\n-                             ErrorCodes::UNKNOWN_TYPE};\n+        throw Exception(ErrorCodes::UNKNOWN_TYPE,\n+            \"The type '{}' of a column '{}' is not supported for conversion into {} data format.\",\n+            column_type->getName(), column_name, format_name);\n     }\n \n     CHColumnToArrowColumn::CHColumnToArrowColumn(const Block & header, const std::string & format_name_, bool low_cardinality_as_dictionary_)\n@@ -638,7 +657,8 @@ namespace DB\n             arrow::Status status = MakeBuilder(pool, arrow_fields[column_i]->type(), &array_builder);\n             checkStatus(status, column->getName(), format_name);\n \n-            fillArrowArray(header_column.name, column, header_column.type, nullptr, array_builder.get(), format_name, 0, column->size(), dictionary_values);\n+            fillArrowArray(\n+                header_column.name, column, header_column.type, nullptr, array_builder.get(), format_name, 0, column->size(), dictionary_values);\n \n             std::shared_ptr<arrow::Array> arrow_array;\n             status = array_builder->Finish(&arrow_array);\ndiff --git a/src/Processors/Formats/Impl/CHColumnToArrowColumn.h b/src/Processors/Formats/Impl/CHColumnToArrowColumn.h\nindex efe02a0d7d90..1fb2a8af65ef 100644\n--- a/src/Processors/Formats/Impl/CHColumnToArrowColumn.h\n+++ b/src/Processors/Formats/Impl/CHColumnToArrowColumn.h\n@@ -7,42 +7,18 @@\n #include <Processors/Chunk.h>\n #include <arrow/table.h>\n \n+\n namespace DB\n {\n \n class CHColumnToArrowColumn\n {\n public:\n-    CHColumnToArrowColumn(const Block & header, const std::string & format_name_, bool low_cardinality_as_dictionary_ = false);\n+    CHColumnToArrowColumn(const Block & header, const std::string & format_name_, bool low_cardinality_as_dictionary_);\n \n     void chChunkToArrowTable(std::shared_ptr<arrow::Table> & res, const Chunk & chunk, size_t columns_num);\n-private:\n-\n-#define FOR_INTERNAL_NUMERIC_TYPES(M) \\\n-        M(UInt8, arrow::UInt8Builder) \\\n-        M(Int8, arrow::Int8Builder) \\\n-        M(UInt16, arrow::UInt16Builder) \\\n-        M(Int16, arrow::Int16Builder) \\\n-        M(UInt32, arrow::UInt32Builder) \\\n-        M(Int32, arrow::Int32Builder) \\\n-        M(UInt64, arrow::UInt64Builder) \\\n-        M(Int64, arrow::Int64Builder) \\\n-        M(Float32, arrow::FloatBuilder) \\\n-        M(Float64, arrow::DoubleBuilder)\n-\n-#define FOR_ARROW_TYPES(M) \\\n-        M(UINT8, arrow::UInt8Type) \\\n-        M(INT8, arrow::Int8Type) \\\n-        M(UINT16, arrow::UInt16Type) \\\n-        M(INT16, arrow::Int16Type) \\\n-        M(UINT32, arrow::UInt32Type) \\\n-        M(INT32, arrow::Int32Type) \\\n-        M(UINT64, arrow::UInt64Type) \\\n-        M(INT64, arrow::Int64Type) \\\n-        M(FLOAT, arrow::FloatType) \\\n-        M(DOUBLE, arrow::DoubleType)  \\\n-        M(STRING, arrow::StringType)\n \n+private:\n     ColumnsWithTypeAndName header_columns;\n     std::vector<std::shared_ptr<arrow::Field>> arrow_fields;\n     const std::string format_name;\n@@ -52,5 +28,7 @@ class CHColumnToArrowColumn\n     /// Dictionary every chunk we save it and reuse.\n     std::unordered_map<std::string, std::shared_ptr<arrow::Array>> dictionary_values;\n };\n+\n }\n+\n #endif\ndiff --git a/src/Processors/Formats/Impl/ParquetBlockOutputFormat.cpp b/src/Processors/Formats/Impl/ParquetBlockOutputFormat.cpp\nindex 800fd0ff0e89..c3771c7b552d 100644\n--- a/src/Processors/Formats/Impl/ParquetBlockOutputFormat.cpp\n+++ b/src/Processors/Formats/Impl/ParquetBlockOutputFormat.cpp\n@@ -2,16 +2,7 @@\n \n #if USE_PARQUET\n \n-// TODO: clean includes\n-#include <Columns/ColumnString.h>\n-#include <Columns/ColumnVector.h>\n-#include <Common/assert_cast.h>\n-#include <Core/callOnTypeIndex.h>\n-#include <DataStreams/SquashingBlockOutputStream.h>\n #include <Formats/FormatFactory.h>\n-#include <IO/WriteHelpers.h>\n-#include <arrow/api.h>\n-#include <arrow/util/memory.h>\n #include <parquet/arrow/writer.h>\n #include \"ArrowBufferedStreams.h\"\n #include \"CHColumnToArrowColumn.h\"\n@@ -19,6 +10,7 @@\n \n namespace DB\n {\n+\n namespace ErrorCodes\n {\n     extern const int UNKNOWN_EXCEPTION;\n@@ -37,7 +29,7 @@ void ParquetBlockOutputFormat::consume(Chunk chunk)\n     if (!ch_column_to_arrow_column)\n     {\n         const Block & header = getPort(PortKind::Main).getHeader();\n-        ch_column_to_arrow_column = std::make_unique<CHColumnToArrowColumn>(header, \"Parquet\");\n+        ch_column_to_arrow_column = std::make_unique<CHColumnToArrowColumn>(header, \"Parquet\", false);\n     }\n \n     ch_column_to_arrow_column->chChunkToArrowTable(arrow_table, chunk, columns_num);\n@@ -91,11 +83,7 @@ void registerOutputFormatProcessorParquet(FormatFactory & factory)\n            const RowOutputFormatParams &,\n            const FormatSettings & format_settings)\n         {\n-            auto impl = std::make_shared<ParquetBlockOutputFormat>(buf, sample, format_settings);\n-            /// TODO\n-            // auto res = std::make_shared<SquashingBlockOutputStream>(impl, impl->getHeader(), format_settings.parquet.row_group_size, 0);\n-            // res->disableFlush();\n-            return impl;\n+            return std::make_shared<ParquetBlockOutputFormat>(buf, sample, format_settings);\n         });\n }\n \ndiff --git a/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.cpp b/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.cpp\nindex df10fb26d40f..7795292f922a 100644\n--- a/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.cpp\n+++ b/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.cpp\n@@ -2,6 +2,8 @@\n #include <AggregateFunctions/IAggregateFunction.h>\n #include <common/DateLUTImpl.h>\n #include <common/DateLUT.h>\n+#include <DataTypes/DataTypeDateTime.h>\n+\n \n namespace DB\n {\n@@ -16,6 +18,8 @@ static GraphiteRollupSortedAlgorithm::ColumnsDefinition defineColumns(\n     def.value_column_num = header.getPositionByName(params.value_column_name);\n     def.version_column_num = header.getPositionByName(params.version_column_name);\n \n+    def.time_column_type = header.getByPosition(def.time_column_num).type;\n+\n     size_t num_columns = header.columns();\n     for (size_t i = 0; i < num_columns; ++i)\n         if (i != def.time_column_num && i != def.value_column_num && i != def.version_column_num)\n@@ -122,8 +126,8 @@ UInt32 GraphiteRollupSortedAlgorithm::selectPrecision(const Graphite::Retentions\n   * In this case, the date should not change. The date is calculated using the local time zone.\n   *\n   * If the rounding value is less than an hour,\n-  *  then, assuming that time zones that differ from UTC by a non-integer number of hours are not supported,\n-  *  just simply round the unix timestamp down to a multiple of 3600.\n+  *  then, assuming that time zones that differ from UTC by a multiple of 15-minute intervals\n+  *  (that is true for all modern timezones but not true for historical timezones).\n   * And if the rounding value is greater,\n   *  then we will round down the number of seconds from the beginning of the day in the local time zone.\n   *\n@@ -131,7 +135,7 @@ UInt32 GraphiteRollupSortedAlgorithm::selectPrecision(const Graphite::Retentions\n   */\n static time_t roundTimeToPrecision(const DateLUTImpl & date_lut, time_t time, UInt32 precision)\n {\n-    if (precision <= 3600)\n+    if (precision <= 900)\n     {\n         return time / precision * precision;\n     }\n@@ -145,7 +149,10 @@ static time_t roundTimeToPrecision(const DateLUTImpl & date_lut, time_t time, UI\n \n IMergingAlgorithm::Status GraphiteRollupSortedAlgorithm::merge()\n {\n-    const DateLUTImpl & date_lut = DateLUT::instance();\n+    /// Timestamp column can be DateTime or UInt32. If it is DateTime, we can use its timezone for calculations.\n+    const TimezoneMixin * timezone = dynamic_cast<const TimezoneMixin *>(columns_definition.time_column_type.get());\n+\n+    const DateLUTImpl & date_lut = timezone ? timezone->getTimeZone() : DateLUT::instance();\n \n     /// Take rows in needed order and put them into `merged_data` until we get `max_block_size` rows.\n     ///\ndiff --git a/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.h b/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.h\nindex a0e8f1662aa6..0155b73b238e 100644\n--- a/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.h\n+++ b/src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.h\n@@ -35,6 +35,8 @@ class GraphiteRollupSortedAlgorithm final : public IMergingAlgorithmWithSharedCh\n         size_t value_column_num;\n         size_t version_column_num;\n \n+        DataTypePtr time_column_type;\n+\n         /// All columns other than 'time', 'value', 'version'. They are unmodified during rollup.\n         ColumnNumbers unmodified_column_numbers;\n     };\ndiff --git a/src/Storages/MergeTree/registerStorageMergeTree.cpp b/src/Storages/MergeTree/registerStorageMergeTree.cpp\nindex 539f77133203..910492d24674 100644\n--- a/src/Storages/MergeTree/registerStorageMergeTree.cpp\n+++ b/src/Storages/MergeTree/registerStorageMergeTree.cpp\n@@ -480,7 +480,10 @@ static StoragePtr create(const StorageFactory::Arguments & args)\n                     \"No replica name in config\" + getMergeTreeVerboseHelp(is_extended_storage_def), ErrorCodes::NO_REPLICA_NAME_GIVEN);\n             ++arg_num;\n         }\n-        else if (is_extended_storage_def && (arg_cnt == 0 || !engine_args[arg_num]->as<ASTLiteral>() || (arg_cnt == 1 && merging_params.mode == MergeTreeData::MergingParams::Graphite)))\n+        else if (is_extended_storage_def\n+            && (arg_cnt == 0\n+                || !engine_args[arg_num]->as<ASTLiteral>()\n+                || (arg_cnt == 1 && merging_params.mode == MergeTreeData::MergingParams::Graphite)))\n         {\n             /// Try use default values if arguments are not specified.\n             /// Note: {uuid} macro works for ON CLUSTER queries when database engine is Atomic.\n",
  "test_patch": "diff --git a/docker/test/base/Dockerfile b/docker/test/base/Dockerfile\nindex 6877a786b76e..29ac7a925b87 100644\n--- a/docker/test/base/Dockerfile\n+++ b/docker/test/base/Dockerfile\n@@ -61,4 +61,7 @@ ENV TSAN_OPTIONS='halt_on_error=1 history_size=7'\n ENV UBSAN_OPTIONS='print_stacktrace=1'\n ENV MSAN_OPTIONS='abort_on_error=1 poison_in_dtor=1'\n \n+ENV TZ=Europe/Moscow\n+RUN ln -snf \"/usr/share/zoneinfo/$TZ\" /etc/localtime && echo \"$TZ\" > /etc/timezone\n+\n CMD sleep 1\ndiff --git a/docker/test/stateful/run.sh b/docker/test/stateful/run.sh\nindex d8ea2153b367..de058469192a 100755\n--- a/docker/test/stateful/run.sh\n+++ b/docker/test/stateful/run.sh\n@@ -2,6 +2,11 @@\n \n set -e -x\n \n+# Choose random timezone for this test run\n+TZ=\"$(grep -v '#' /usr/share/zoneinfo/zone.tab  | awk '{print $3}' | shuf | head -n1)\"\n+echo \"Choosen random timezone $TZ\"\n+ln -snf \"/usr/share/zoneinfo/$TZ\" /etc/localtime && echo \"$TZ\" > /etc/timezone\n+\n dpkg -i package_folder/clickhouse-common-static_*.deb;\n dpkg -i package_folder/clickhouse-common-static-dbg_*.deb\n dpkg -i package_folder/clickhouse-server_*.deb\ndiff --git a/docker/test/stateless/run.sh b/docker/test/stateless/run.sh\nindex a7fb956bf940..c7f9d2c7079e 100755\n--- a/docker/test/stateless/run.sh\n+++ b/docker/test/stateless/run.sh\n@@ -3,6 +3,11 @@\n # fail on errors, verbose and export all env variables\n set -e -x -a\n \n+# Choose random timezone for this test run.\n+TZ=\"$(grep -v '#' /usr/share/zoneinfo/zone.tab  | awk '{print $3}' | shuf | head -n1)\"\n+echo \"Choosen random timezone $TZ\"\n+ln -snf \"/usr/share/zoneinfo/$TZ\" /etc/localtime && echo \"$TZ\" > /etc/timezone\n+\n dpkg -i package_folder/clickhouse-common-static_*.deb\n dpkg -i package_folder/clickhouse-common-static-dbg_*.deb\n dpkg -i package_folder/clickhouse-server_*.deb\ndiff --git a/docker/test/stateless_unbundled/Dockerfile b/docker/test/stateless_unbundled/Dockerfile\nindex c5463ac447d8..53857a90ac79 100644\n--- a/docker/test/stateless_unbundled/Dockerfile\n+++ b/docker/test/stateless_unbundled/Dockerfile\n@@ -77,9 +77,6 @@ RUN mkdir -p /tmp/clickhouse-odbc-tmp \\\n    && odbcinst -i -s -l -f /tmp/clickhouse-odbc-tmp/share/doc/clickhouse-odbc/config/odbc.ini.sample \\\n    && rm -rf /tmp/clickhouse-odbc-tmp\n \n-ENV TZ=Europe/Moscow\n-RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n-\n COPY run.sh /\n CMD [\"/bin/bash\", \"/run.sh\"]\n \ndiff --git a/docker/test/unit/Dockerfile b/docker/test/unit/Dockerfile\nindex e2f4a6919394..e111611eecd6 100644\n--- a/docker/test/unit/Dockerfile\n+++ b/docker/test/unit/Dockerfile\n@@ -1,8 +1,6 @@\n # docker build -t yandex/clickhouse-unit-test .\n FROM yandex/clickhouse-stateless-test\n \n-ENV TZ=Europe/Moscow\n-RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n RUN apt-get install gdb\n \n COPY run.sh /\ndiff --git a/tests/queries/0_stateless/00538_datediff.sql b/tests/queries/0_stateless/00538_datediff.sql\nindex e40b576902f7..b76ab4ff3f8c 100644\n--- a/tests/queries/0_stateless/00538_datediff.sql\n+++ b/tests/queries/0_stateless/00538_datediff.sql\n@@ -15,27 +15,27 @@ SELECT dateDiff('week', toDate('2017-12-31'), toDate('2018-01-01'));\n SELECT dateDiff('day', toDate('2017-12-31'), toDate('2016-01-01'));\n SELECT dateDiff('day', toDate('2017-12-31'), toDate('2017-01-01'));\n SELECT dateDiff('day', toDate('2017-12-31'), toDate('2018-01-01'));\n-SELECT dateDiff('hour', toDate('2017-12-31'), toDate('2016-01-01'));\n-SELECT dateDiff('hour', toDate('2017-12-31'), toDate('2017-01-01'));\n-SELECT dateDiff('hour', toDate('2017-12-31'), toDate('2018-01-01'));\n-SELECT dateDiff('minute', toDate('2017-12-31'), toDate('2016-01-01'));\n-SELECT dateDiff('minute', toDate('2017-12-31'), toDate('2017-01-01'));\n-SELECT dateDiff('minute', toDate('2017-12-31'), toDate('2018-01-01'));\n-SELECT dateDiff('second', toDate('2017-12-31'), toDate('2016-01-01'));\n-SELECT dateDiff('second', toDate('2017-12-31'), toDate('2017-01-01'));\n-SELECT dateDiff('second', toDate('2017-12-31'), toDate('2018-01-01'));\n+SELECT dateDiff('hour', toDate('2017-12-31'), toDate('2016-01-01'), 'UTC');\n+SELECT dateDiff('hour', toDate('2017-12-31'), toDate('2017-01-01'), 'UTC');\n+SELECT dateDiff('hour', toDate('2017-12-31'), toDate('2018-01-01'), 'UTC');\n+SELECT dateDiff('minute', toDate('2017-12-31'), toDate('2016-01-01'), 'UTC');\n+SELECT dateDiff('minute', toDate('2017-12-31'), toDate('2017-01-01'), 'UTC');\n+SELECT dateDiff('minute', toDate('2017-12-31'), toDate('2018-01-01'), 'UTC');\n+SELECT dateDiff('second', toDate('2017-12-31'), toDate('2016-01-01'), 'UTC');\n+SELECT dateDiff('second', toDate('2017-12-31'), toDate('2017-01-01'), 'UTC');\n+SELECT dateDiff('second', toDate('2017-12-31'), toDate('2018-01-01'), 'UTC');\n \n SELECT 'Date and DateTime arguments';\n \n-SELECT dateDiff('second', toDate('2017-12-31'), toDateTime('2016-01-01 00:00:00'));\n-SELECT dateDiff('second', toDateTime('2017-12-31 00:00:00'), toDate('2017-01-01'));\n-SELECT dateDiff('second', toDateTime('2017-12-31 00:00:00'), toDateTime('2018-01-01 00:00:00'));\n+SELECT dateDiff('second', toDate('2017-12-31'), toDateTime('2016-01-01 00:00:00', 'UTC'), 'UTC');\n+SELECT dateDiff('second', toDateTime('2017-12-31 00:00:00', 'UTC'), toDate('2017-01-01'), 'UTC');\n+SELECT dateDiff('second', toDateTime('2017-12-31 00:00:00', 'UTC'), toDateTime('2018-01-01 00:00:00', 'UTC'));\n \n SELECT 'Constant and non-constant arguments';\n \n-SELECT dateDiff('minute', materialize(toDate('2017-12-31')), toDate('2016-01-01'));\n-SELECT dateDiff('minute', toDate('2017-12-31'), materialize(toDate('2017-01-01')));\n-SELECT dateDiff('minute', materialize(toDate('2017-12-31')), materialize(toDate('2018-01-01')));\n+SELECT dateDiff('minute', materialize(toDate('2017-12-31')), toDate('2016-01-01'), 'UTC');\n+SELECT dateDiff('minute', toDate('2017-12-31'), materialize(toDate('2017-01-01')), 'UTC');\n+SELECT dateDiff('minute', materialize(toDate('2017-12-31')), materialize(toDate('2018-01-01')), 'UTC');\n \n SELECT 'Case insensitive';\n \ndiff --git a/tests/queries/0_stateless/00719_format_datetime_rand.sql b/tests/queries/0_stateless/00719_format_datetime_rand.sql\nindex 0ed71161b4b5..b91c988e1caf 100644\n--- a/tests/queries/0_stateless/00719_format_datetime_rand.sql\n+++ b/tests/queries/0_stateless/00719_format_datetime_rand.sql\n@@ -5,4 +5,7 @@ WITH toDateTime(1 + rand() % 0xFFFFFFFF) AS t SELECT count() FROM numbers(100000\n WITH toDateTime(1 + rand() % 0xFFFFFFFF) AS t SELECT count() FROM numbers(1000000) WHERE formatDateTime(t, '%F %R:%S') != toString(t);\n \n WITH toDate(today() + rand() % 4096) AS t SELECT count() FROM numbers(1000000) WHERE formatDateTime(t, '%F') != toString(t);\n-WITH toDate(today() + rand() % 4096) AS t SELECT count() FROM numbers(1000000) WHERE formatDateTime(t, '%F %T') != toString(toDateTime(t));\n+\n+-- Note: in some other timezones, daylight saving time change happens in midnight, so the first time of day is 01:00:00 instead of 00:00:00.\n+-- Stick to Moscow timezone to avoid this issue.\n+WITH toDate(today() + rand() % 4096) AS t SELECT count() FROM numbers(1000000) WHERE formatDateTime(t, '%F %T', 'Europe/Moscow') != toString(toDateTime(t, 'Europe/Moscow'));\ndiff --git a/tests/queries/0_stateless/00825_protobuf_format_persons.sh b/tests/queries/0_stateless/00825_protobuf_format_persons.sh\nindex 957ed738d99d..fb099a6479e6 100755\n--- a/tests/queries/0_stateless/00825_protobuf_format_persons.sh\n+++ b/tests/queries/0_stateless/00825_protobuf_format_persons.sh\n@@ -27,7 +27,7 @@ CREATE TABLE persons_00825 (uuid UUID,\n                             photo Nullable(String),\n                             phoneNumber Nullable(FixedString(13)),\n                             isOnline UInt8,\n-                            visitTime Nullable(DateTime),\n+                            visitTime Nullable(DateTime('Europe/Moscow')),\n                             age UInt8,\n                             zodiacSign Enum16('aries'=321, 'taurus'=420, 'gemini'=521, 'cancer'=621, 'leo'=723, 'virgo'=823,\n                                               'libra'=923, 'scorpius'=1023, 'sagittarius'=1122, 'capricorn'=1222, 'aquarius'=120,\n@@ -43,12 +43,12 @@ CREATE TABLE persons_00825 (uuid UUID,\n                             randomBigNumber Int64,\n                             measureUnits Nested(unit  String, coef Float32),\n                             nestiness_a_b_c_d Nullable(UInt32),\n-                            \\`nestiness_a_B.c_E\\` Array(UInt32)\n+                            \"nestiness_a_B.c_E\" Array(UInt32)\n                            ) ENGINE = MergeTree ORDER BY tuple();\n \n-INSERT INTO persons_00825 VALUES (toUUID('a7522158-3d41-4b77-ad69-6c598ee55c49'), 'Ivan', 'Petrov', 'male', toDate('1980-12-29'), 'png', '+74951234567', 1, toDateTime('2019-01-05 18:45:00'), 38, 'capricorn', ['Yesterday', 'Flowers'], [255, 0, 0], 'Moscow', [55.753215, 37.622504], 3.14, 214.10, 0.1, 5.8, 17060000000, ['meter', 'centimeter', 'kilometer'], [1, 0.01, 1000], 500, [501, 502]);\n+INSERT INTO persons_00825 VALUES (toUUID('a7522158-3d41-4b77-ad69-6c598ee55c49'), 'Ivan', 'Petrov', 'male', toDate('1980-12-29'), 'png', '+74951234567', 1, toDateTime('2019-01-05 18:45:00', 'Europe/Moscow'), 38, 'capricorn', ['Yesterday', 'Flowers'], [255, 0, 0], 'Moscow', [55.753215, 37.622504], 3.14, 214.10, 0.1, 5.8, 17060000000, ['meter', 'centimeter', 'kilometer'], [1, 0.01, 1000], 500, [501, 502]);\n INSERT INTO persons_00825 VALUES (toUUID('c694ad8a-f714-4ea3-907d-fd54fb25d9b5'), 'Natalia', 'Sokolova', 'female', toDate('1992-03-08'), 'jpg', NULL, 0, NULL, 26, 'pisces', [], [100, 200, 50], 'Plymouth', [50.403724, -4.142123], 3.14159, NULL, 0.007, 5.4, -20000000000000, [], [], NULL, []);\n-INSERT INTO persons_00825 VALUES (toUUID('a7da1aa6-f425-4789-8947-b034786ed374'), 'Vasily', 'Sidorov', 'male', toDate('1995-07-28'), 'bmp', '+442012345678', 1, toDateTime('2018-12-30 00:00:00'), 23, 'leo', ['Sunny'], [250, 244, 10], 'Murmansk', [68.970682, 33.074981], 3.14159265358979, 100000000000, 800, -3.2, 154400000, ['pound'], [16], 503, []);\n+INSERT INTO persons_00825 VALUES (toUUID('a7da1aa6-f425-4789-8947-b034786ed374'), 'Vasily', 'Sidorov', 'male', toDate('1995-07-28'), 'bmp', '+442012345678', 1, toDateTime('2018-12-30 00:00:00', 'Europe/Moscow'), 23, 'leo', ['Sunny'], [250, 244, 10], 'Murmansk', [68.970682, 33.074981], 3.14159265358979, 100000000000, 800, -3.2, 154400000, ['pound'], [16], 503, []);\n \n SELECT * FROM persons_00825 ORDER BY name;\n EOF\ndiff --git a/tests/queries/0_stateless/00900_long_parquet.reference b/tests/queries/0_stateless/00900_long_parquet.reference\nindex bf0f66bb217f..d0cb71338aff 100644\n--- a/tests/queries/0_stateless/00900_long_parquet.reference\n+++ b/tests/queries/0_stateless/00900_long_parquet.reference\n@@ -43,7 +43,7 @@ converted:\n 127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1.032\t-1.064\tstring-2\tfixedstring-2\\0\\0\t2004-06-07\t2004-02-03 04:05:06\n diff:\n dest:\n-79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t1970-01-01 06:29:04\n+79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t2004-05-06 00:00:00\n 80\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr02\tfstr2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2005-03-04\t2006-08-09 10:11:12\n min:\n -128\t0\t0\t0\t0\t0\t0\t0\t-1\t-1\tstring-1\\0\\0\\0\\0\\0\\0\\0\tfixedstring-1\\0\\0\t2003-04-05\t2003-02-03\n@@ -51,10 +51,10 @@ min:\n 79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t2004-05-06\n 127\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\tstring-2\\0\\0\\0\\0\\0\\0\\0\tfixedstring-2\\0\\0\t2004-06-07\t2004-02-03\n max:\n--128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1\t-1\tstring-1\tfixedstring-1\\0\\0\t1970-01-01 06:22:27\t2003-02-03 04:05:06\n--108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1\t-1\tstring-0\tfixedstring\\0\\0\\0\\0\t1970-01-01 06:09:16\t2002-02-03 04:05:06\n+-128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1\t-1\tstring-1\tfixedstring-1\\0\\0\t2003-04-05 00:00:00\t2003-02-03 04:05:06\n+-108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1\t-1\tstring-0\tfixedstring\\0\\0\\0\\0\t2001-02-03 00:00:00\t2002-02-03 04:05:06\n 80\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr02\tfstr2\t2005-03-04 05:06:07\t2006-08-09 10:11:12\n-127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1\t-1\tstring-2\tfixedstring-2\\0\\0\t1970-01-01 06:29:36\t2004-02-03 04:05:06\n+127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1\t-1\tstring-2\tfixedstring-2\\0\\0\t2004-06-07 00:00:00\t2004-02-03 04:05:06\n dest from null:\n -128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1.032\t-1.064\tstring-1\tfixedstring-1\\0\\0\t2003-04-05\t2003-02-03 04:05:06\n -108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1.032\t-1.064\tstring-0\tfixedstring\\0\\0\\0\\0\t2001-02-03\t2002-02-03 04:05:06\ndiff --git a/tests/queries/0_stateless/00900_long_parquet.sh b/tests/queries/0_stateless/00900_long_parquet.sh\nindex c30e1148abe8..e748fbcaf242 100755\n--- a/tests/queries/0_stateless/00900_long_parquet.sh\n+++ b/tests/queries/0_stateless/00900_long_parquet.sh\n@@ -6,15 +6,6 @@ CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n # shellcheck source=../shell_config.sh\n . \"$CUR_DIR\"/../shell_config.sh\n \n-#${CLICKHOUSE_CLIENT} --max_block_size=1 --query=\"SELECT * FROM system.numbers LIMIT 10 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t1.pq\n-#${CLICKHOUSE_CLIENT} --max_block_size=5 --query=\"SELECT * FROM system.numbers LIMIT 10 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t5.pq\n-#${CLICKHOUSE_CLIENT} --max_block_size=15 --query=\"SELECT * FROM system.numbers LIMIT 10 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t15.pq\n-#${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM system.numbers LIMIT 100000 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t100000.pq\n-#${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM system.numbers LIMIT 1000000000 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t1g.pq\n-#${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM system.numbers LIMIT 100000000 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t100m.pq\n-#${CLICKHOUSE_CLIENT} --max_block_size=100000000 --query=\"SELECT * FROM system.numbers LIMIT 100000000 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t100m-100mbs.pq\n-#valgrind --tool=massif  ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM system.numbers LIMIT 1000000 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/t1g.pq\n-\n \n ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS contributors\"\n ${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE contributors (name String) ENGINE = Memory\"\n@@ -35,11 +26,6 @@ ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM system.numbers LIMIT 100000 FORMAT P\n ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_numbers ORDER BY number DESC LIMIT 10\"\n ${CLICKHOUSE_CLIENT} --query=\"TRUNCATE TABLE parquet_numbers\"\n \n-#${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM system.numbers LIMIT 10000000 FORMAT Parquet\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_numbers FORMAT Parquet\"\n-#${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_numbers ORDER BY number DESC LIMIT 10\"\n-#${CLICKHOUSE_CLIENT} --query=\"TRUNCATE TABLE parquet_numbers\"\n-\n-#${CLICKHOUSE_CLIENT} --max_block_size=2 --query=\"SELECT * FROM system.numbers LIMIT 3 FORMAT Parquet\" > ${CLICKHOUSE_TMP}/bs2.pq\n ${CLICKHOUSE_CLIENT} --max_block_size=2 --query=\"SELECT * FROM system.numbers LIMIT 3 FORMAT Parquet\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_numbers FORMAT Parquet\"\n ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_numbers ORDER BY number DESC LIMIT 10\"\n \n@@ -55,16 +41,6 @@ ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM system.events FORMAT Parquet\" | ${CL\n ${CLICKHOUSE_CLIENT} --query=\"SELECT event, description FROM parquet_events WHERE event IN ('ContextLock', 'Query') ORDER BY event\"\n ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE parquet_events\"\n \n-\n-#${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS parquet_types1\"\n-#${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS parquet_types2\"\n-#${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE parquet_types1       (int8 Int8, uint8 UInt8, int16 Int16, uint16 UInt16, int32 Int32,                int64 Int64, uint64 UInt64, float32 Float32, float64 Float64, string String                                                           ) ENGINE = Memory\"\n-#${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE parquet_types2       (int8 Int8, uint8 UInt8, int16 Int16, uint16 UInt16, int32 Int32,                int64 Int64, uint64 UInt64, float32 Float32, float64 Float64, string String                                                           ) ENGINE = Memory\"\n-#${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types1 values (     -108,         108,       -1016,          1116,       -1032,                      -1064,          1164,          -1.032,          -1.064,      'string'                                                    )\"\n-#${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_types1 FORMAT Parquet\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types2 FORMAT Parquet\"\n-\n-\n-\n ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS parquet_types1\"\n ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS parquet_types2\"\n ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS parquet_types3\"\n@@ -84,8 +60,6 @@ ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types1 values (     -128,\n # max\n ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types1 values (      127,         255,       32767,         65535,  2147483647,    4294967295, 9223372036854775807, 9223372036854775807, -1.032,     -1.064,    'string-2',             'fixedstring-2', '2004-06-07', '2004-02-03 04:05:06')\"\n \n-# 'SELECT -127,-128,-129,126,127,128,255,256,257,-32767,-32768,-32769,32766,32767,32768,65535,65536,65537,  -2147483647,-2147483648,-2147483649,2147483646,2147483647,2147483648,4294967295,4294967296,4294967297,   -9223372036854775807,-9223372036854775808,9223372036854775806,9223372036854775807,9223372036854775808,18446744073709551615';\n-\n ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_types1 FORMAT Parquet\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types2 FORMAT Parquet\"\n \n echo original:\n@@ -121,7 +95,6 @@ ${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE parquet_types5       (int8 Nullable(I\n ${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE parquet_types6       (int8 Nullable(Int8), uint8 Nullable(UInt8), int16 Nullable(Int16), uint16 Nullable(UInt16), int32 Nullable(Int32), uint32 Nullable(UInt32), int64 Nullable(Int64), uint64 Nullable(UInt64), float32 Nullable(Float32), float64 Nullable(Float64), string Nullable(String), fixedstring Nullable(FixedString(15)), date Nullable(Date), datetime Nullable(DateTime)) ENGINE = Memory\"\n ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types5 values (               NULL,                  NULL,                  NULL,                    NULL,                  NULL,                    NULL,                  NULL,                    NULL,                      NULL,                      NULL,                    NULL,                                  NULL,                NULL,                        NULL)\"\n ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_types5 ORDER BY int8 FORMAT Parquet\" > \"${CLICKHOUSE_TMP}\"/parquet_all_types_5.parquet\n-#${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_types5 FORMAT Parquet\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types6 FORMAT Parquet\"\n ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_types5 ORDER BY int8 FORMAT Parquet\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types6 FORMAT Parquet\"\n ${CLICKHOUSE_CLIENT} --query=\"SELECT * FROM parquet_types1 ORDER BY int8 FORMAT Parquet\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_types6 FORMAT Parquet\"\n echo dest from null:\n@@ -140,7 +113,7 @@ ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE parquet_types4\"\n \n ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS parquet_arrays\"\n \n-${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE parquet_arrays (id UInt32, a1 Array(Int8), a2 Array(UInt8), a3 Array(Int16), a4 Array(UInt16), a5 Array(Int32), a6 Array(UInt32), a7 Array(Int64), a8 Array(UInt64), a9 Array(String), a10 Array(FixedString(4)), a11 Array(Float32), a12 Array(Float64), a13 Array(Date), a14 Array(Datetime), a15 Array(Decimal(4, 2)), a16 Array(Decimal(10, 2)), a17 Array(Decimal(25, 2))) engine=Memory()\"\n+${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE parquet_arrays (id UInt32, a1 Array(Int8), a2 Array(UInt8), a3 Array(Int16), a4 Array(UInt16), a5 Array(Int32), a6 Array(UInt32), a7 Array(Int64), a8 Array(UInt64), a9 Array(String), a10 Array(FixedString(4)), a11 Array(Float32), a12 Array(Float64), a13 Array(Date), a14 Array(DateTime), a15 Array(Decimal(4, 2)), a16 Array(Decimal(10, 2)), a17 Array(Decimal(25, 2))) engine=Memory()\"\n \n ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO parquet_arrays VALUES (1, [1,-2,3], [1,2,3], [100, -200, 300], [100, 200, 300], [10000000, -20000000, 30000000], [10000000, 2000000, 3000000], [100000000000000, -200000000000, 3000000000000], [100000000000000, 20000000000000, 3000000000000], ['Some string', 'Some string', 'Some string'], ['0000', '1111', '2222'], [42.42, 424.2, 0.4242], [424242.424242, 4242042420.242424, 42], ['2000-01-01', '2001-01-01', '2002-01-01'], ['2000-01-01', '2001-01-01', '2002-01-01'], [0.2, 10.003, 4.002], [4.000000001, 10000.10000, 10000.100001], [1000000000.000000001123, 90.0000000010010101, 0101001.0112341001])\"\n \ndiff --git a/tests/queries/0_stateless/00921_datetime64_compatibility_long.python b/tests/queries/0_stateless/00921_datetime64_compatibility_long.python\nindex c8b9620629dd..03cc088fd875 100644\n--- a/tests/queries/0_stateless/00921_datetime64_compatibility_long.python\n+++ b/tests/queries/0_stateless/00921_datetime64_compatibility_long.python\n@@ -9,69 +9,69 @@ import argparse\n # Create SQL statement to verify dateTime64 is accepted as argument to functions taking DateTime.\n FUNCTIONS=\"\"\"\n toTimeZone(N, 'UTC')\n-toYear(N)\n-toQuarter(N)\n-toMonth(N)\n-toDayOfYear(N)\n-toDayOfMonth(N)\n-toDayOfWeek(N)\n-toHour(N)\n-toMinute(N)\n-toSecond(N)\n+toYear(N, 'Europe/Moscow')\n+toQuarter(N, 'Europe/Moscow')\n+toMonth(N, 'Europe/Moscow')\n+toDayOfYear(N, 'Europe/Moscow')\n+toDayOfMonth(N, 'Europe/Moscow')\n+toDayOfWeek(N, 'Europe/Moscow')\n+toHour(N, 'Europe/Moscow')\n+toMinute(N, 'Europe/Moscow')\n+toSecond(N, 'Europe/Moscow')\n toUnixTimestamp(N)\n-toStartOfYear(N)\n-toStartOfISOYear(N)\n-toStartOfQuarter(N)\n-toStartOfMonth(N)\n-toMonday(N)\n-toStartOfWeek(N)\n-toStartOfDay(N)\n-toStartOfHour(N)\n-toStartOfMinute(N)\n-toStartOfFiveMinute(N)\n-toStartOfTenMinutes(N)\n-toStartOfFifteenMinutes(N)\n-toStartOfInterval(N, INTERVAL 1 year)\n-toStartOfInterval(N, INTERVAL 1 month)\n-toStartOfInterval(N, INTERVAL 1 day)\n-toStartOfInterval(N, INTERVAL 15 minute)\n-date_trunc('year', N)\n-date_trunc('month', N)\n-date_trunc('day', N)\n-date_trunc('minute', N)\n-toTime(N)\n-toRelativeYearNum(N)\n-toRelativeQuarterNum(N)\n-toRelativeMonthNum(N)\n-toRelativeWeekNum(N)\n-toRelativeDayNum(N)\n-toRelativeHourNum(N)\n-toRelativeMinuteNum(N)\n-toRelativeSecondNum(N)\n-toISOYear(N)\n-toISOWeek(N)\n-toWeek(N)\n-toYearWeek(N)\n-timeSlot(N)\n-toYYYYMM(N)\n-toYYYYMMDD(N)\n-toYYYYMMDDhhmmss(N)\n-addYears(N, 1)\n-addMonths(N, 1)\n-addWeeks(N, 1)\n-addDays(N, 1)\n-addHours(N, 1)\n-addMinutes(N, 1)\n-addSeconds(N, 1)\n-addQuarters(N, 1)\n-subtractYears(N, 1)\n-subtractMonths(N, 1)\n-subtractWeeks(N, 1)\n-subtractDays(N, 1)\n-subtractHours(N, 1)\n-subtractMinutes(N, 1)\n-subtractSeconds(N, 1)\n-subtractQuarters(N, 1)\n+toStartOfYear(N, 'Europe/Moscow')\n+toStartOfISOYear(N, 'Europe/Moscow')\n+toStartOfQuarter(N, 'Europe/Moscow')\n+toStartOfMonth(N, 'Europe/Moscow')\n+toMonday(N, 'Europe/Moscow')\n+toStartOfWeek(N, 'Europe/Moscow')\n+toStartOfDay(N, 'Europe/Moscow')\n+toStartOfHour(N, 'Europe/Moscow')\n+toStartOfMinute(N, 'Europe/Moscow')\n+toStartOfFiveMinute(N, 'Europe/Moscow')\n+toStartOfTenMinutes(N, 'Europe/Moscow')\n+toStartOfFifteenMinutes(N, 'Europe/Moscow')\n+toStartOfInterval(N, INTERVAL 1 year, 'Europe/Moscow')\n+toStartOfInterval(N, INTERVAL 1 month, 'Europe/Moscow')\n+toStartOfInterval(N, INTERVAL 1 day, 'Europe/Moscow')\n+toStartOfInterval(N, INTERVAL 15 minute, 'Europe/Moscow')\n+date_trunc('year', N, 'Europe/Moscow')\n+date_trunc('month', N, 'Europe/Moscow')\n+date_trunc('day', N, 'Europe/Moscow')\n+date_trunc('minute', N, 'Europe/Moscow')\n+toTime(N, 'Europe/Moscow')\n+toRelativeYearNum(N, 'Europe/Moscow')\n+toRelativeQuarterNum(N, 'Europe/Moscow')\n+toRelativeMonthNum(N, 'Europe/Moscow')\n+toRelativeWeekNum(N, 'Europe/Moscow')\n+toRelativeDayNum(N, 'Europe/Moscow')\n+toRelativeHourNum(N, 'Europe/Moscow')\n+toRelativeMinuteNum(N, 'Europe/Moscow')\n+toRelativeSecondNum(N, 'Europe/Moscow')\n+toISOYear(N, 'Europe/Moscow')\n+toISOWeek(N, 'Europe/Moscow')\n+toWeek(N, 'Europe/Moscow')\n+toYearWeek(N, 'Europe/Moscow')\n+timeSlot(N, 'Europe/Moscow')\n+toYYYYMM(N, 'Europe/Moscow')\n+toYYYYMMDD(N, 'Europe/Moscow')\n+toYYYYMMDDhhmmss(N, 'Europe/Moscow')\n+addYears(N, 1, 'Europe/Moscow')\n+addMonths(N, 1, 'Europe/Moscow')\n+addWeeks(N, 1, 'Europe/Moscow')\n+addDays(N, 1, 'Europe/Moscow')\n+addHours(N, 1, 'Europe/Moscow')\n+addMinutes(N, 1, 'Europe/Moscow')\n+addSeconds(N, 1, 'Europe/Moscow')\n+addQuarters(N, 1, 'Europe/Moscow')\n+subtractYears(N, 1, 'Europe/Moscow')\n+subtractMonths(N, 1, 'Europe/Moscow')\n+subtractWeeks(N, 1, 'Europe/Moscow')\n+subtractDays(N, 1, 'Europe/Moscow')\n+subtractHours(N, 1, 'Europe/Moscow')\n+subtractMinutes(N, 1, 'Europe/Moscow')\n+subtractSeconds(N, 1, 'Europe/Moscow')\n+subtractQuarters(N, 1, 'Europe/Moscow')\n CAST(N as DateTime('Europe/Minsk'))\n CAST(N as Date)\n CAST(N as UInt64)\n@@ -80,10 +80,10 @@ CAST(N as DateTime64(3, 'Europe/Minsk'))\n CAST(N as DateTime64(6, 'Europe/Minsk'))\n CAST(N as DateTime64(9, 'Europe/Minsk'))\n # Casting our test values to DateTime(12) will cause an overflow and hence will fail the test under UB sanitizer.\n-# CAST(N as DateTime64(12))\n+# CAST(N as DateTime64(12, 'Europe/Moscow'))\n # DateTime64(18) will always fail due to zero precision, but it is Ok to test here:\n-# CAST(N as DateTime64(18))\n-formatDateTime(N, '%C %d %D %e %F %H %I %j %m %M %p %R %S %T %u %V %w %y %Y %%')\n+# CAST(N as DateTime64(18, 'Europe/Moscow'))\n+formatDateTime(N, '%C %d %D %e %F %H %I %j %m %M %p %R %S %T %u %V %w %y %Y %%', 'Europe/Moscow')\n \"\"\".splitlines()\n \n # Expanded later to cartesian product of all arguments, using format string.\ndiff --git a/tests/queries/0_stateless/00921_datetime64_compatibility_long.reference b/tests/queries/0_stateless/00921_datetime64_compatibility_long.reference\nindex 67413512e062..230bfa0c117b 100644\n--- a/tests/queries/0_stateless/00921_datetime64_compatibility_long.reference\n+++ b/tests/queries/0_stateless/00921_datetime64_compatibility_long.reference\n@@ -3,47 +3,47 @@ Code: 43\n \"DateTime('UTC')\",\"2019-09-16 16:20:11\"\n \"DateTime64(3, 'UTC')\",\"2019-09-16 16:20:11.234\"\n ------------------------------------------\n-SELECT toYear(N)\n+SELECT toYear(N, \\'Europe/Moscow\\')\n \"UInt16\",2019\n \"UInt16\",2019\n \"UInt16\",2019\n ------------------------------------------\n-SELECT toQuarter(N)\n+SELECT toQuarter(N, \\'Europe/Moscow\\')\n \"UInt8\",3\n \"UInt8\",3\n \"UInt8\",3\n ------------------------------------------\n-SELECT toMonth(N)\n+SELECT toMonth(N, \\'Europe/Moscow\\')\n \"UInt8\",9\n \"UInt8\",9\n \"UInt8\",9\n ------------------------------------------\n-SELECT toDayOfYear(N)\n+SELECT toDayOfYear(N, \\'Europe/Moscow\\')\n \"UInt16\",259\n \"UInt16\",259\n \"UInt16\",259\n ------------------------------------------\n-SELECT toDayOfMonth(N)\n+SELECT toDayOfMonth(N, \\'Europe/Moscow\\')\n \"UInt8\",16\n \"UInt8\",16\n \"UInt8\",16\n ------------------------------------------\n-SELECT toDayOfWeek(N)\n+SELECT toDayOfWeek(N, \\'Europe/Moscow\\')\n \"UInt8\",1\n \"UInt8\",1\n \"UInt8\",1\n ------------------------------------------\n-SELECT toHour(N)\n+SELECT toHour(N, \\'Europe/Moscow\\')\n Code: 43\n \"UInt8\",19\n \"UInt8\",19\n ------------------------------------------\n-SELECT toMinute(N)\n+SELECT toMinute(N, \\'Europe/Moscow\\')\n Code: 43\n \"UInt8\",20\n \"UInt8\",20\n ------------------------------------------\n-SELECT toSecond(N)\n+SELECT toSecond(N, \\'Europe/Moscow\\')\n Code: 43\n \"UInt8\",11\n \"UInt8\",11\n@@ -53,270 +53,270 @@ Code: 44\n \"UInt32\",1568650811\n \"UInt32\",1568650811\n ------------------------------------------\n-SELECT toStartOfYear(N)\n-\"Date\",\"2019-01-01\"\n+SELECT toStartOfYear(N, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-01-01\"\n \"Date\",\"2019-01-01\"\n ------------------------------------------\n-SELECT toStartOfISOYear(N)\n-\"Date\",\"2018-12-31\"\n+SELECT toStartOfISOYear(N, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2018-12-31\"\n \"Date\",\"2018-12-31\"\n ------------------------------------------\n-SELECT toStartOfQuarter(N)\n-\"Date\",\"2019-07-01\"\n+SELECT toStartOfQuarter(N, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-07-01\"\n \"Date\",\"2019-07-01\"\n ------------------------------------------\n-SELECT toStartOfMonth(N)\n-\"Date\",\"2019-09-01\"\n+SELECT toStartOfMonth(N, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-09-01\"\n \"Date\",\"2019-09-01\"\n ------------------------------------------\n-SELECT toMonday(N)\n-\"Date\",\"2019-09-16\"\n+SELECT toMonday(N, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-09-16\"\n \"Date\",\"2019-09-16\"\n ------------------------------------------\n-SELECT toStartOfWeek(N)\n-\"Date\",\"2019-09-15\"\n-\"Date\",\"2019-09-15\"\n-\"Date\",\"2019-09-15\"\n+SELECT toStartOfWeek(N, \\'Europe/Moscow\\')\n+Code: 43\n+Code: 43\n+Code: 43\n ------------------------------------------\n-SELECT toStartOfDay(N)\n-\"DateTime\",\"2019-09-16 00:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 00:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 00:00:00\"\n+SELECT toStartOfDay(N, \\'Europe/Moscow\\')\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n ------------------------------------------\n-SELECT toStartOfHour(N)\n+SELECT toStartOfHour(N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:00:00\"\n ------------------------------------------\n-SELECT toStartOfMinute(N)\n+SELECT toStartOfMinute(N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n ------------------------------------------\n-SELECT toStartOfFiveMinute(N)\n+SELECT toStartOfFiveMinute(N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n ------------------------------------------\n-SELECT toStartOfTenMinutes(N)\n+SELECT toStartOfTenMinutes(N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n ------------------------------------------\n-SELECT toStartOfFifteenMinutes(N)\n+SELECT toStartOfFifteenMinutes(N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:15:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:15:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:15:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:15:00\"\n ------------------------------------------\n-SELECT toStartOfInterval(N, INTERVAL 1 year)\n-\"Date\",\"2019-01-01\"\n+SELECT toStartOfInterval(N, INTERVAL 1 year, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-01-01\"\n \"Date\",\"2019-01-01\"\n ------------------------------------------\n-SELECT toStartOfInterval(N, INTERVAL 1 month)\n-\"Date\",\"2019-09-01\"\n+SELECT toStartOfInterval(N, INTERVAL 1 month, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-09-01\"\n \"Date\",\"2019-09-01\"\n ------------------------------------------\n-SELECT toStartOfInterval(N, INTERVAL 1 day)\n-\"DateTime\",\"2019-09-16 00:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 00:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 00:00:00\"\n+SELECT toStartOfInterval(N, INTERVAL 1 day, \\'Europe/Moscow\\')\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n ------------------------------------------\n-SELECT toStartOfInterval(N, INTERVAL 15 minute)\n+SELECT toStartOfInterval(N, INTERVAL 15 minute, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:15:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:15:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:15:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:15:00\"\n ------------------------------------------\n-SELECT date_trunc(\\'year\\', N)\n-\"Date\",\"2019-01-01\"\n+SELECT date_trunc(\\'year\\', N, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-01-01\"\n \"Date\",\"2019-01-01\"\n ------------------------------------------\n-SELECT date_trunc(\\'month\\', N)\n-\"Date\",\"2019-09-01\"\n+SELECT date_trunc(\\'month\\', N, \\'Europe/Moscow\\')\n+Code: 43\n \"Date\",\"2019-09-01\"\n \"Date\",\"2019-09-01\"\n ------------------------------------------\n-SELECT date_trunc(\\'day\\', N)\n-\"DateTime\",\"2019-09-16 00:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 00:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 00:00:00\"\n+SELECT date_trunc(\\'day\\', N, \\'Europe/Moscow\\')\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 00:00:00\"\n ------------------------------------------\n-SELECT date_trunc(\\'minute\\', N)\n+SELECT date_trunc(\\'minute\\', N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:00\"\n ------------------------------------------\n-SELECT toTime(N)\n+SELECT toTime(N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"1970-01-02 19:20:11\"\n-\"DateTime('Europe/Minsk')\",\"1970-01-02 19:20:11\"\n+\"DateTime('Europe/Moscow')\",\"1970-01-02 19:20:11\"\n+\"DateTime('Europe/Moscow')\",\"1970-01-02 19:20:11\"\n ------------------------------------------\n-SELECT toRelativeYearNum(N)\n+SELECT toRelativeYearNum(N, \\'Europe/Moscow\\')\n \"UInt16\",2019\n \"UInt16\",2019\n \"UInt16\",2019\n ------------------------------------------\n-SELECT toRelativeQuarterNum(N)\n+SELECT toRelativeQuarterNum(N, \\'Europe/Moscow\\')\n \"UInt32\",8078\n \"UInt32\",8078\n \"UInt32\",8078\n ------------------------------------------\n-SELECT toRelativeMonthNum(N)\n+SELECT toRelativeMonthNum(N, \\'Europe/Moscow\\')\n \"UInt32\",24237\n \"UInt32\",24237\n \"UInt32\",24237\n ------------------------------------------\n-SELECT toRelativeWeekNum(N)\n+SELECT toRelativeWeekNum(N, \\'Europe/Moscow\\')\n \"UInt32\",2594\n \"UInt32\",2594\n \"UInt32\",2594\n ------------------------------------------\n-SELECT toRelativeDayNum(N)\n+SELECT toRelativeDayNum(N, \\'Europe/Moscow\\')\n \"UInt32\",18155\n \"UInt32\",18155\n \"UInt32\",18155\n ------------------------------------------\n-SELECT toRelativeHourNum(N)\n+SELECT toRelativeHourNum(N, \\'Europe/Moscow\\')\n \"UInt32\",435717\n \"UInt32\",435736\n \"UInt32\",435736\n ------------------------------------------\n-SELECT toRelativeMinuteNum(N)\n+SELECT toRelativeMinuteNum(N, \\'Europe/Moscow\\')\n \"UInt32\",26143020\n \"UInt32\",26144180\n \"UInt32\",26144180\n ------------------------------------------\n-SELECT toRelativeSecondNum(N)\n+SELECT toRelativeSecondNum(N, \\'Europe/Moscow\\')\n \"UInt32\",1568581200\n \"UInt32\",1568650811\n \"UInt32\",1568650811\n ------------------------------------------\n-SELECT toISOYear(N)\n+SELECT toISOYear(N, \\'Europe/Moscow\\')\n \"UInt16\",2019\n \"UInt16\",2019\n \"UInt16\",2019\n ------------------------------------------\n-SELECT toISOWeek(N)\n+SELECT toISOWeek(N, \\'Europe/Moscow\\')\n \"UInt8\",38\n \"UInt8\",38\n \"UInt8\",38\n ------------------------------------------\n-SELECT toWeek(N)\n-\"UInt8\",37\n-\"UInt8\",37\n-\"UInt8\",37\n+SELECT toWeek(N, \\'Europe/Moscow\\')\n+Code: 43\n+Code: 43\n+Code: 43\n ------------------------------------------\n-SELECT toYearWeek(N)\n-\"UInt32\",201937\n-\"UInt32\",201937\n-\"UInt32\",201937\n+SELECT toYearWeek(N, \\'Europe/Moscow\\')\n+Code: 43\n+Code: 43\n+Code: 43\n ------------------------------------------\n-SELECT timeSlot(N)\n+SELECT timeSlot(N, \\'Europe/Moscow\\')\n Code: 43\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:00:00\"\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:00:00\"\n ------------------------------------------\n-SELECT toYYYYMM(N)\n+SELECT toYYYYMM(N, \\'Europe/Moscow\\')\n \"UInt32\",201909\n \"UInt32\",201909\n \"UInt32\",201909\n ------------------------------------------\n-SELECT toYYYYMMDD(N)\n+SELECT toYYYYMMDD(N, \\'Europe/Moscow\\')\n \"UInt32\",20190916\n \"UInt32\",20190916\n \"UInt32\",20190916\n ------------------------------------------\n-SELECT toYYYYMMDDhhmmss(N)\n+SELECT toYYYYMMDDhhmmss(N, \\'Europe/Moscow\\')\n \"UInt64\",20190916000000\n \"UInt64\",20190916192011\n \"UInt64\",20190916192011\n ------------------------------------------\n-SELECT addYears(N, 1)\n-\"Date\",\"2020-09-16\"\n-\"DateTime('Europe/Minsk')\",\"2020-09-16 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2020-09-16 19:20:11.234\"\n+SELECT addYears(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2020-09-16 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT addMonths(N, 1)\n-\"Date\",\"2019-10-16\"\n-\"DateTime('Europe/Minsk')\",\"2019-10-16 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-10-16 19:20:11.234\"\n+SELECT addMonths(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-10-16 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT addWeeks(N, 1)\n-\"Date\",\"2019-09-23\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-23 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-23 19:20:11.234\"\n+SELECT addWeeks(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-23 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT addDays(N, 1)\n-\"Date\",\"2019-09-17\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-17 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-17 19:20:11.234\"\n+SELECT addDays(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-17 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT addHours(N, 1)\n-\"DateTime\",\"2019-09-16 01:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 20:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-16 20:20:11.234\"\n+SELECT addHours(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 20:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT addMinutes(N, 1)\n-\"DateTime\",\"2019-09-16 00:01:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:21:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-16 19:21:11.234\"\n+SELECT addMinutes(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:21:11\"\n+Code: 43\n ------------------------------------------\n-SELECT addSeconds(N, 1)\n-\"DateTime\",\"2019-09-16 00:00:01\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:12\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-16 19:20:12.234\"\n+SELECT addSeconds(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:12\"\n+Code: 43\n ------------------------------------------\n-SELECT addQuarters(N, 1)\n-\"Date\",\"2019-12-16\"\n-\"DateTime('Europe/Minsk')\",\"2019-12-16 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-12-16 19:20:11.234\"\n+SELECT addQuarters(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-12-16 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractYears(N, 1)\n-\"Date\",\"2018-09-16\"\n-\"DateTime('Europe/Minsk')\",\"2018-09-16 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2018-09-16 19:20:11.234\"\n+SELECT subtractYears(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2018-09-16 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractMonths(N, 1)\n-\"Date\",\"2019-08-16\"\n-\"DateTime('Europe/Minsk')\",\"2019-08-16 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-08-16 19:20:11.234\"\n+SELECT subtractMonths(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-08-16 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractWeeks(N, 1)\n-\"Date\",\"2019-09-09\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-09 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-09 19:20:11.234\"\n+SELECT subtractWeeks(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-09 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractDays(N, 1)\n-\"Date\",\"2019-09-15\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-15 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-15 19:20:11.234\"\n+SELECT subtractDays(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-15 19:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractHours(N, 1)\n-\"DateTime\",\"2019-09-15 23:00:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 18:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-16 18:20:11.234\"\n+SELECT subtractHours(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 18:20:11\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractMinutes(N, 1)\n-\"DateTime\",\"2019-09-15 23:59:00\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:19:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-16 19:19:11.234\"\n+SELECT subtractMinutes(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:19:11\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractSeconds(N, 1)\n-\"DateTime\",\"2019-09-15 23:59:59\"\n-\"DateTime('Europe/Minsk')\",\"2019-09-16 19:20:10\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-09-16 19:20:10.234\"\n+SELECT subtractSeconds(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-09-16 19:20:10\"\n+Code: 43\n ------------------------------------------\n-SELECT subtractQuarters(N, 1)\n-\"Date\",\"2019-06-16\"\n-\"DateTime('Europe/Minsk')\",\"2019-06-16 19:20:11\"\n-\"DateTime64(3, 'Europe/Minsk')\",\"2019-06-16 19:20:11.234\"\n+SELECT subtractQuarters(N, 1, \\'Europe/Moscow\\')\n+Code: 43\n+\"DateTime('Europe/Moscow')\",\"2019-06-16 19:20:11\"\n+Code: 43\n ------------------------------------------\n SELECT CAST(N as DateTime(\\'Europe/Minsk\\'))\n \"DateTime('Europe/Minsk')\",\"2019-09-16 00:00:00\"\n@@ -353,7 +353,7 @@ SELECT CAST(N as DateTime64(9, \\'Europe/Minsk\\'))\n \"DateTime64(9, 'Europe/Minsk')\",\"2019-09-16 19:20:11.000000000\"\n \"DateTime64(9, 'Europe/Minsk')\",\"2019-09-16 19:20:11.234000000\"\n ------------------------------------------\n-SELECT formatDateTime(N, \\'%C %d %D %e %F %H %I %j %m %M %p %R %S %T %u %V %w %y %Y %%\\')\n+SELECT formatDateTime(N, \\'%C %d %D %e %F %H %I %j %m %M %p %R %S %T %u %V %w %y %Y %%\\', \\'Europe/Moscow\\')\n \"String\",\"20 16 09/16/19 16 2019-09-16 00 12 259 09 00 AM 00:00 00 00:00:00 1 38 1 19 2019 %\"\n \"String\",\"20 16 09/16/19 16 2019-09-16 19 07 259 09 20 PM 19:20 11 19:20:11 1 38 1 19 2019 %\"\n \"String\",\"20 16 09/16/19 16 2019-09-16 19 07 259 09 20 PM 19:20 11 19:20:11 1 38 1 19 2019 %\"\ndiff --git a/tests/queries/0_stateless/00927_asof_join_other_types.sh b/tests/queries/0_stateless/00927_asof_join_other_types.sh\nindex c002d092b409..0c17ca2085f2 100755\n--- a/tests/queries/0_stateless/00927_asof_join_other_types.sh\n+++ b/tests/queries/0_stateless/00927_asof_join_other_types.sh\n@@ -6,7 +6,7 @@ CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n # shellcheck source=../shell_config.sh\n . \"$CURDIR\"/../shell_config.sh\n \n-for typename in \"UInt32\" \"UInt64\" \"Float64\" \"Float32\" \"DateTime\" \"Decimal32(5)\" \"Decimal64(5)\" \"Decimal128(5)\" \"DateTime64(3)\"\n+for typename in \"UInt32\" \"UInt64\" \"Float64\" \"Float32\" \"DateTime('Europe/Moscow')\" \"Decimal32(5)\" \"Decimal64(5)\" \"Decimal128(5)\" \"DateTime64(3, 'Europe/Moscow')\"\n do\n     $CLICKHOUSE_CLIENT -mn <<EOF\n DROP TABLE IF EXISTS A;\n@@ -24,4 +24,4 @@ DROP TABLE A;\n DROP TABLE B;\n EOF\n \n-done\n\\ No newline at end of file\n+done\ndiff --git a/tests/queries/0_stateless/01077_mutations_index_consistency.sh b/tests/queries/0_stateless/01077_mutations_index_consistency.sh\nindex 129ef0b161cc..31086ed67842 100755\n--- a/tests/queries/0_stateless/01077_mutations_index_consistency.sh\n+++ b/tests/queries/0_stateless/01077_mutations_index_consistency.sh\n@@ -9,7 +9,7 @@ $CLICKHOUSE_CLIENT --query \"DROP TABLE IF EXISTS movement\"\n \n $CLICKHOUSE_CLIENT -n --query \"CREATE TABLE movement (date DateTime('Europe/Moscow')) Engine = MergeTree ORDER BY (toStartOfHour(date));\"\n \n-$CLICKHOUSE_CLIENT --query \"insert into movement select toDateTime('2020-01-22 00:00:00') + number%(23*3600) from numbers(1000000);\"\n+$CLICKHOUSE_CLIENT --query \"insert into movement select toDateTime('2020-01-22 00:00:00', 'Europe/Moscow') + number%(23*3600) from numbers(1000000);\"\n \n $CLICKHOUSE_CLIENT --query \"OPTIMIZE TABLE movement FINAL\"\n \n@@ -18,20 +18,20 @@ SELECT\n     count(),\n     toStartOfHour(date) AS Hour\n FROM movement\n-WHERE (date >= toDateTime('2020-01-22T10:00:00')) AND (date <= toDateTime('2020-01-22T23:00:00'))\n+WHERE (date >= toDateTime('2020-01-22T10:00:00', 'Europe/Moscow')) AND (date <= toDateTime('2020-01-22T23:00:00', 'Europe/Moscow'))\n GROUP BY Hour\n ORDER BY Hour DESC\n \" | grep \"16:00:00\" | cut -f1\n \n \n-$CLICKHOUSE_CLIENT --query \"alter table movement delete where date >= toDateTime('2020-01-22T16:00:00')  and date < toDateTime('2020-01-22T17:00:00') SETTINGS mutations_sync = 2\"\n+$CLICKHOUSE_CLIENT --query \"alter table movement delete where date >= toDateTime('2020-01-22T16:00:00', 'Europe/Moscow')  and date < toDateTime('2020-01-22T17:00:00', 'Europe/Moscow') SETTINGS mutations_sync = 2\"\n \n $CLICKHOUSE_CLIENT -n --query \"\n SELECT\n     count(),\n     toStartOfHour(date) AS Hour\n FROM movement\n-WHERE (date >= toDateTime('2020-01-22T10:00:00')) AND (date <= toDateTime('2020-01-22T23:00:00'))\n+WHERE (date >= toDateTime('2020-01-22T10:00:00', 'Europe/Moscow')) AND (date <= toDateTime('2020-01-22T23:00:00', 'Europe/Moscow'))\n GROUP BY Hour\n ORDER BY Hour DESC\n \" | grep \"16:00:00\" | wc -l\n@@ -42,7 +42,7 @@ SELECT\n     count(),\n     toStartOfHour(date) AS Hour\n FROM movement\n-WHERE (date >= toDateTime('2020-01-22T10:00:00')) AND (date <= toDateTime('2020-01-22T23:00:00'))\n+WHERE (date >= toDateTime('2020-01-22T10:00:00', 'Europe/Moscow')) AND (date <= toDateTime('2020-01-22T23:00:00', 'Europe/Moscow'))\n GROUP BY Hour\n ORDER BY Hour DESC\n \" | grep \"22:00:00\" | cut -f1\ndiff --git a/tests/queries/0_stateless/01087_storage_generate.sql b/tests/queries/0_stateless/01087_storage_generate.sql\nindex bc69e8abbacd..a16ad55832c0 100644\n--- a/tests/queries/0_stateless/01087_storage_generate.sql\n+++ b/tests/queries/0_stateless/01087_storage_generate.sql\n@@ -7,7 +7,7 @@ DROP TABLE IF EXISTS test_table;\n SELECT '-';\n \n DROP TABLE IF EXISTS test_table_2;\n-CREATE TABLE test_table_2(a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)) ENGINE=GenerateRandom(10, 5, 3);\n+CREATE TABLE test_table_2(a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3, 'Europe/Moscow'), UUID)) ENGINE=GenerateRandom(10, 5, 3);\n \n SELECT * FROM test_table_2 LIMIT 100;\n \ndiff --git a/tests/queries/0_stateless/01087_table_function_generate.reference b/tests/queries/0_stateless/01087_table_function_generate.reference\nindex c04fa831328b..bf301d34eb36 100644\n--- a/tests/queries/0_stateless/01087_table_function_generate.reference\n+++ b/tests/queries/0_stateless/01087_table_function_generate.reference\n@@ -46,7 +46,7 @@ h\n \\N\n o\n -\n-Date\tDateTime\tDateTime(\\'Europe/Moscow\\')\n+Date\tDateTime(\\'Europe/Moscow\\')\tDateTime(\\'Europe/Moscow\\')\n 2113-06-12\t2050-12-17 02:46:35\t2096-02-16 22:18:22\n 2141-08-09\t2013-10-17 23:35:26\t1976-01-24 12:52:48\n 2039-08-16\t1974-11-17 23:22:46\t1980-03-04 21:02:50\n@@ -58,7 +58,7 @@ Date\tDateTime\tDateTime(\\'Europe/Moscow\\')\n 2008-03-16\t2047-05-16 23:28:36\t2103-02-11 16:44:39\n 2000-07-07\t2105-07-19 19:29:06\t1980-01-02 05:18:22\n -\n-DateTime64(3)\tDateTime64(6)\tDateTime64(6, \\'Europe/Moscow\\')\n+DateTime64(3, \\'Europe/Moscow\\')\tDateTime64(6, \\'Europe/Moscow\\')\tDateTime64(6, \\'Europe/Moscow\\')\n 1978-06-07 23:50:57.320\t2013-08-28 10:21:54.010758\t1991-08-25 16:23:26.140215\n 1978-08-25 17:07:25.427\t2034-05-02 20:49:42.148578\t2015-08-26 15:26:31.783160\n 2037-04-04 10:50:56.898\t2055-05-28 11:12:48.819271\t2068-12-26 09:58:49.635722\ndiff --git a/tests/queries/0_stateless/01087_table_function_generate.sql b/tests/queries/0_stateless/01087_table_function_generate.sql\nindex 05f03a5a4e62..9a0f7db24ecb 100644\n--- a/tests/queries/0_stateless/01087_table_function_generate.sql\n+++ b/tests/queries/0_stateless/01087_table_function_generate.sql\n@@ -42,20 +42,20 @@ LIMIT 10;\n SELECT '-';\n SELECT\n toTypeName(d), toTypeName(dt), toTypeName(dtm)\n-FROM generateRandom('d Date, dt DateTime, dtm DateTime(\\'Europe/Moscow\\')')\n+FROM generateRandom('d Date, dt DateTime(\\'Europe/Moscow\\'), dtm DateTime(\\'Europe/Moscow\\')')\n LIMIT 1;\n SELECT\n d, dt, dtm\n-FROM generateRandom('d Date, dt DateTime, dtm DateTime(\\'Europe/Moscow\\')', 1, 10, 10)\n+FROM generateRandom('d Date, dt DateTime(\\'Europe/Moscow\\'), dtm DateTime(\\'Europe/Moscow\\')', 1, 10, 10)\n LIMIT 10;\n SELECT '-';\n SELECT\n toTypeName(dt64), toTypeName(dts64), toTypeName(dtms64)\n-FROM generateRandom('dt64 DateTime64, dts64 DateTime64(6), dtms64 DateTime64(6 ,\\'Europe/Moscow\\')')\n+FROM generateRandom('dt64 DateTime64(3, \\'Europe/Moscow\\'), dts64 DateTime64(6, \\'Europe/Moscow\\'), dtms64 DateTime64(6 ,\\'Europe/Moscow\\')')\n LIMIT 1;\n SELECT\n dt64, dts64, dtms64\n-FROM generateRandom('dt64 DateTime64, dts64 DateTime64(6), dtms64 DateTime64(6 ,\\'Europe/Moscow\\')', 1, 10, 10)\n+FROM generateRandom('dt64 DateTime64(3, \\'Europe/Moscow\\'), dts64 DateTime64(6, \\'Europe/Moscow\\'), dtms64 DateTime64(6 ,\\'Europe/Moscow\\')', 1, 10, 10)\n LIMIT 10;\n SELECT '-';\n SELECT\n@@ -168,8 +168,8 @@ FROM generateRandom('i String', 1, 10, 10)\n LIMIT 10;\n SELECT '-';\n DROP TABLE IF EXISTS test_table;\n-CREATE TABLE test_table(a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)) ENGINE=Memory;\n-INSERT INTO test_table SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)', 1, 10, 2)\n+CREATE TABLE test_table(a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3, 'Europe/Moscow'), UUID)) ENGINE=Memory;\n+INSERT INTO test_table SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3, \\'Europe/Moscow\\'), UUID)', 1, 10, 2)\n LIMIT 10;\n \n SELECT * FROM test_table ORDER BY a, d, c;\n@@ -179,8 +179,8 @@ DROP TABLE IF EXISTS test_table;\n SELECT '-';\n \n DROP TABLE IF EXISTS test_table_2;\n-CREATE TABLE test_table_2(a Array(Int8), b UInt32, c Nullable(String), d Decimal32(4), e Nullable(Enum16('h' = 1, 'w' = 5 , 'o' = -200)), f Float64, g Tuple(Date, DateTime, DateTime64, UUID), h FixedString(2)) ENGINE=Memory;\n-INSERT INTO test_table_2 SELECT * FROM generateRandom('a Array(Int8), b UInt32, c Nullable(String), d Decimal32(4), e Nullable(Enum16(\\'h\\' = 1, \\'w\\' = 5 , \\'o\\' = -200)), f Float64, g Tuple(Date, DateTime, DateTime64, UUID), h FixedString(2)', 10, 5, 3)\n+CREATE TABLE test_table_2(a Array(Int8), b UInt32, c Nullable(String), d Decimal32(4), e Nullable(Enum16('h' = 1, 'w' = 5 , 'o' = -200)), f Float64, g Tuple(Date, DateTime('Europe/Moscow'), DateTime64(3, 'Europe/Moscow'), UUID), h FixedString(2)) ENGINE=Memory;\n+INSERT INTO test_table_2 SELECT * FROM generateRandom('a Array(Int8), b UInt32, c Nullable(String), d Decimal32(4), e Nullable(Enum16(\\'h\\' = 1, \\'w\\' = 5 , \\'o\\' = -200)), f Float64, g Tuple(Date, DateTime(\\'Europe/Moscow\\'), DateTime64(3, \\'Europe/Moscow\\'), UUID), h FixedString(2)', 10, 5, 3)\n LIMIT 10;\n \n SELECT a, b, c, d, e, f, g, hex(h) FROM test_table_2 ORDER BY a, b, c, d, e, f, g, h;\ndiff --git a/tests/queries/0_stateless/01098_msgpack_format.sh b/tests/queries/0_stateless/01098_msgpack_format.sh\nindex c7a1a0cff427..3bc60b4a9cbb 100755\n--- a/tests/queries/0_stateless/01098_msgpack_format.sh\n+++ b/tests/queries/0_stateless/01098_msgpack_format.sh\n@@ -6,7 +6,7 @@ CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n \n $CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS msgpack\";\n \n-$CLICKHOUSE_CLIENT --query=\"CREATE TABLE msgpack (uint8 UInt8, uint16 UInt16, uint32 UInt32, uint64 UInt64, int8 Int8, int16 Int16, int32 Int32, int64 Int64, float Float32, double Float64, string String, date Date, datetime DateTime, datetime64 DateTime64, array Array(UInt32)) ENGINE = Memory\";\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE msgpack (uint8 UInt8, uint16 UInt16, uint32 UInt32, uint64 UInt64, int8 Int8, int16 Int16, int32 Int32, int64 Int64, float Float32, double Float64, string String, date Date, datetime DateTime('Europe/Moscow'), datetime64 DateTime64(3, 'Europe/Moscow'), array Array(UInt32)) ENGINE = Memory\";\n \n \n $CLICKHOUSE_CLIENT --query=\"INSERT INTO msgpack VALUES (255, 65535, 4294967295, 100000000000, -128, -32768, -2147483648, -100000000000, 2.02, 10000.0000001, 'String', 18980, 1639872000, 1639872000000, [1,2,3,4,5]), (4, 1234, 3244467295, 500000000000, -1, -256, -14741221, -7000000000, 100.1, 14321.032141201, 'Another string', 20000, 1839882000, 1639872891123, [5,4,3,2,1]), (42, 42, 42, 42, 42, 42, 42, 42, 42.42, 42.42, '42', 42, 42, 42, [42])\";\ndiff --git a/tests/queries/0_stateless/01099_operators_date_and_timestamp.sql b/tests/queries/0_stateless/01099_operators_date_and_timestamp.sql\nindex f52d2b774c10..c630e19490d6 100644\n--- a/tests/queries/0_stateless/01099_operators_date_and_timestamp.sql\n+++ b/tests/queries/0_stateless/01099_operators_date_and_timestamp.sql\n@@ -14,7 +14,7 @@ select timestamp '2001-09-28 01:00:00' + interval 23 hour;\n select timestamp '2001-09-28 23:00:00' - interval 23 hour;\n \n -- TODO: return interval\n-select (timestamp '2001-09-29 03:00:00' - timestamp '2001-09-27 12:00:00') x, toTypeName(x); -- interval '1 day 15:00:00'\n+select (timestamp '2001-12-29 03:00:00' - timestamp '2001-12-27 12:00:00') x, toTypeName(x); -- interval '1 day 15:00:00'\n \n -- select -interval 23 hour; -- interval '-23:00:00'\n -- select interval 1 day + interval 1 hour; -- interval '1 day 01:00:00'\ndiff --git a/tests/queries/0_stateless/01186_conversion_to_nullable.sql b/tests/queries/0_stateless/01186_conversion_to_nullable.sql\nindex bf7df6234d22..828d3cac05be 100644\n--- a/tests/queries/0_stateless/01186_conversion_to_nullable.sql\n+++ b/tests/queries/0_stateless/01186_conversion_to_nullable.sql\n@@ -2,9 +2,9 @@ select toUInt8(x) from values('x Nullable(String)', '42', NULL, '0', '', '256');\n select toInt64(x) from values('x Nullable(String)', '42', NULL, '0', '', '256');\n \n select toDate(x) from values('x Nullable(String)', '2020-12-24', NULL, '0000-00-00', '', '9999-01-01');\n-select toDateTime(x) from values('x Nullable(String)', '2020-12-24 01:02:03', NULL, '0000-00-00 00:00:00', '');\n-select toDateTime64(x, 2) from values('x Nullable(String)', '2020-12-24 01:02:03', NULL, '0000-00-00 00:00:00', '');\n-select toUnixTimestamp(x) from values ('x Nullable(String)', '2000-01-01 13:12:12', NULL, '');\n+select toDateTime(x, 'Europe/Moscow') from values('x Nullable(String)', '2020-12-24 01:02:03', NULL, '0000-00-00 00:00:00', '');\n+select toDateTime64(x, 2, 'Europe/Moscow') from values('x Nullable(String)', '2020-12-24 01:02:03', NULL, '0000-00-00 00:00:00', '');\n+select toUnixTimestamp(x, 'Europe/Moscow') from values ('x Nullable(String)', '2000-01-01 13:12:12', NULL, '');\n \n select toDecimal32(x, 2) from values ('x Nullable(String)', '42', NULL, '3.14159');\n select toDecimal64(x, 8) from values ('x Nullable(String)', '42', NULL, '3.14159');\ndiff --git a/tests/queries/0_stateless/01213_alter_rename_with_default_zookeeper.sql b/tests/queries/0_stateless/01213_alter_rename_with_default_zookeeper.sql\nindex d3f83c0cbb0e..e57010777700 100644\n--- a/tests/queries/0_stateless/01213_alter_rename_with_default_zookeeper.sql\n+++ b/tests/queries/0_stateless/01213_alter_rename_with_default_zookeeper.sql\n@@ -12,7 +12,7 @@ ENGINE = MergeTree()\n PARTITION BY date\n ORDER BY key;\n \n-INSERT INTO table_rename_with_default (date, key, value1) SELECT toDate('2019-10-01') + number % 3, number, toString(number)  from numbers(9);\n+INSERT INTO table_rename_with_default (date, key, value1) SELECT toDateTime(toDate('2019-10-01') + number % 3, 'Europe/Moscow'), number, toString(number)  from numbers(9);\n \n SELECT * FROM table_rename_with_default WHERE key = 1 FORMAT TSVWithNames;\n \n@@ -42,7 +42,7 @@ ENGINE = ReplicatedMergeTree('/clickhouse/test_01213/table_rename_with_ttl', '1'\n ORDER BY tuple()\n TTL date2 + INTERVAL 10000 MONTH;\n \n-INSERT INTO table_rename_with_ttl SELECT toDate('2019-10-01') + number % 3, toDate('2018-10-01') + number % 3, toString(number), toString(number) from numbers(9);\n+INSERT INTO table_rename_with_ttl SELECT toDateTime(toDate('2019-10-01') + number % 3, 'Europe/Moscow'), toDateTime(toDate('2018-10-01') + number % 3, 'Europe/Moscow'), toString(number), toString(number) from numbers(9);\n \n SELECT * FROM table_rename_with_ttl WHERE value1 = '1' FORMAT TSVWithNames;\n \ndiff --git a/tests/queries/0_stateless/01236_graphite_mt.sql b/tests/queries/0_stateless/01236_graphite_mt.sql\nindex f3f1905b9016..88d2d0ccb636 100644\n--- a/tests/queries/0_stateless/01236_graphite_mt.sql\n+++ b/tests/queries/0_stateless/01236_graphite_mt.sql\n@@ -1,5 +1,6 @@\n drop table if exists test_graphite;\n-create table test_graphite (key UInt32, Path String, Time DateTime, Value Float64, Version UInt32, col UInt64) engine = GraphiteMergeTree('graphite_rollup') order by key settings index_granularity=10;\n+create table test_graphite (key UInt32, Path String, Time DateTime, Value Float64, Version UInt32, col UInt64)\n+    engine = GraphiteMergeTree('graphite_rollup') order by key settings index_granularity=10;\n \n insert into test_graphite\n select 1, 'sum_1', toDateTime(today()) - number * 60 - 30, number, 1, number from numbers(300) union all\n@@ -21,7 +22,7 @@ select 2, 'max_1', toDateTime(today() - 3) - number * 60 - 30, number, 1, number\n select 1, 'max_2', toDateTime(today() - 3) - number * 60 - 30, number, 1, number from numbers(1200) union all\n select 2, 'max_2', toDateTime(today() - 3) - number * 60 - 30, number, 1, number from numbers(1200);\n \n-optimize table test_graphite;\n+optimize table test_graphite final;\n \n select key, Path, Value, Version, col from test_graphite order by key, Path, Time desc;\n \ndiff --git a/tests/queries/0_stateless/01269_toStartOfSecond.sql b/tests/queries/0_stateless/01269_toStartOfSecond.sql\nindex 5fe6aa9602f8..b74eaabf3512 100644\n--- a/tests/queries/0_stateless/01269_toStartOfSecond.sql\n+++ b/tests/queries/0_stateless/01269_toStartOfSecond.sql\n@@ -4,10 +4,10 @@ SELECT toStartOfSecond(now());  -- {serverError 43}\n SELECT toStartOfSecond();   -- {serverError 42}\n SELECT toStartOfSecond(now64(), 123);   -- {serverError 43}\n \n-WITH toDateTime64('2019-09-16 19:20:11', 3) AS dt64 SELECT toStartOfSecond(dt64, 'UTC') AS res, toTypeName(res);\n+WITH toDateTime64('2019-09-16 19:20:11', 3, 'Europe/Moscow') AS dt64 SELECT toStartOfSecond(dt64, 'UTC') AS res, toTypeName(res);\n WITH toDateTime64('2019-09-16 19:20:11', 0, 'UTC') AS dt64 SELECT toStartOfSecond(dt64) AS res, toTypeName(res);\n WITH toDateTime64('2019-09-16 19:20:11.123', 3, 'UTC') AS dt64 SELECT toStartOfSecond(dt64) AS res, toTypeName(res);\n WITH toDateTime64('2019-09-16 19:20:11.123', 9, 'UTC') AS dt64 SELECT toStartOfSecond(dt64) AS res, toTypeName(res);\n \n SELECT 'non-const column';\n-WITH toDateTime64('2019-09-16 19:20:11.123', 3, 'UTC') AS dt64 SELECT toStartOfSecond(materialize(dt64)) AS res, toTypeName(res);\n\\ No newline at end of file\n+WITH toDateTime64('2019-09-16 19:20:11.123', 3, 'UTC') AS dt64 SELECT toStartOfSecond(materialize(dt64)) AS res, toTypeName(res);\ndiff --git a/tests/queries/0_stateless/01273_arrow.reference b/tests/queries/0_stateless/01273_arrow.reference\nindex 0dc503f65e47..89eca82f8eff 100644\n--- a/tests/queries/0_stateless/01273_arrow.reference\n+++ b/tests/queries/0_stateless/01273_arrow.reference\n@@ -41,7 +41,7 @@ converted:\n 127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1.032\t-1.064\tstring-2\tfixedstring-2\\0\\0\t2004-06-07\t2004-02-03 04:05:06\n diff:\n dest:\n-79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t1970-01-01 06:29:04\n+79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t2004-05-06 00:00:00\n 80\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr02\tfstr2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2005-03-04\t2006-08-09 10:11:12\n min:\n -128\t0\t0\t0\t0\t0\t0\t0\t-1\t-1\tstring-1\\0\\0\\0\\0\\0\\0\\0\tfixedstring-1\\0\\0\t2003-04-05\t2003-02-03\n@@ -49,10 +49,10 @@ min:\n 79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t2004-05-06\n 127\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\tstring-2\\0\\0\\0\\0\\0\\0\\0\tfixedstring-2\\0\\0\t2004-06-07\t2004-02-03\n max:\n--128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1\t-1\tstring-1\tfixedstring-1\\0\\0\t1970-01-01 06:22:27\t2003-02-03 04:05:06\n--108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1\t-1\tstring-0\tfixedstring\\0\\0\\0\\0\t1970-01-01 06:09:16\t2002-02-03 04:05:06\n+-128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1\t-1\tstring-1\tfixedstring-1\\0\\0\t2003-04-05 00:00:00\t2003-02-03 04:05:06\n+-108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1\t-1\tstring-0\tfixedstring\\0\\0\\0\\0\t2001-02-03 00:00:00\t2002-02-03 04:05:06\n 80\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr02\tfstr2\t2005-03-04 05:06:07\t2006-08-09 10:11:12\n-127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1\t-1\tstring-2\tfixedstring-2\\0\\0\t1970-01-01 06:29:36\t2004-02-03 04:05:06\n+127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1\t-1\tstring-2\tfixedstring-2\\0\\0\t2004-06-07 00:00:00\t2004-02-03 04:05:06\n dest from null:\n -128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1.032\t-1.064\tstring-1\tfixedstring-1\\0\\0\t2003-04-05\t2003-02-03 04:05:06\n -108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1.032\t-1.064\tstring-0\tfixedstring\\0\\0\\0\\0\t2001-02-03\t2002-02-03 04:05:06\ndiff --git a/tests/queries/0_stateless/01273_arrow_load.sh b/tests/queries/0_stateless/01273_arrow_load.sh\nindex b2ca0e32af1d..bc5588a905b6 100755\n--- a/tests/queries/0_stateless/01273_arrow_load.sh\n+++ b/tests/queries/0_stateless/01273_arrow_load.sh\n@@ -11,7 +11,7 @@ CB_DIR=$(dirname \"$CLICKHOUSE_CLIENT_BINARY\")\n DATA_FILE=$CUR_DIR/data_arrow/test.arrow\n \n ${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS arrow_load\"\n-${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE arrow_load (bool UInt8, int8 Int8, int16 Int16, int32 Int32, int64 Int64, uint8 UInt8, uint16 UInt16, uint32 UInt32, uint64 UInt64, halffloat Float32, float Float32, double Float64, string String, date32 Date, date64 DateTime, timestamp DateTime) ENGINE = Memory\"\n+${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE arrow_load (bool UInt8, int8 Int8, int16 Int16, int32 Int32, int64 Int64, uint8 UInt8, uint16 UInt16, uint32 UInt32, uint64 UInt64, halffloat Float32, float Float32, double Float64, string String, date32 Date, date64 DateTime('Europe/Moscow'), timestamp DateTime('Europe/Moscow')) ENGINE = Memory\"\n cat \"$DATA_FILE\"  | ${CLICKHOUSE_CLIENT} -q \"insert into arrow_load format Arrow\"\n ${CLICKHOUSE_CLIENT} --query=\"select * from arrow_load\"\n \ndiff --git a/tests/queries/0_stateless/01273_arrow_stream.reference b/tests/queries/0_stateless/01273_arrow_stream.reference\nindex 0dc503f65e47..89eca82f8eff 100644\n--- a/tests/queries/0_stateless/01273_arrow_stream.reference\n+++ b/tests/queries/0_stateless/01273_arrow_stream.reference\n@@ -41,7 +41,7 @@ converted:\n 127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1.032\t-1.064\tstring-2\tfixedstring-2\\0\\0\t2004-06-07\t2004-02-03 04:05:06\n diff:\n dest:\n-79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t1970-01-01 06:29:04\n+79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t2004-05-06 00:00:00\n 80\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr02\tfstr2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2005-03-04\t2006-08-09 10:11:12\n min:\n -128\t0\t0\t0\t0\t0\t0\t0\t-1\t-1\tstring-1\\0\\0\\0\\0\\0\\0\\0\tfixedstring-1\\0\\0\t2003-04-05\t2003-02-03\n@@ -49,10 +49,10 @@ min:\n 79\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr01\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\tfstr1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\t2003-03-04\t2004-05-06\n 127\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\tstring-2\\0\\0\\0\\0\\0\\0\\0\tfixedstring-2\\0\\0\t2004-06-07\t2004-02-03\n max:\n--128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1\t-1\tstring-1\tfixedstring-1\\0\\0\t1970-01-01 06:22:27\t2003-02-03 04:05:06\n--108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1\t-1\tstring-0\tfixedstring\\0\\0\\0\\0\t1970-01-01 06:09:16\t2002-02-03 04:05:06\n+-128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1\t-1\tstring-1\tfixedstring-1\\0\\0\t2003-04-05 00:00:00\t2003-02-03 04:05:06\n+-108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1\t-1\tstring-0\tfixedstring\\0\\0\\0\\0\t2001-02-03 00:00:00\t2002-02-03 04:05:06\n 80\t81\t82\t83\t84\t85\t86\t87\t88\t89\tstr02\tfstr2\t2005-03-04 05:06:07\t2006-08-09 10:11:12\n-127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1\t-1\tstring-2\tfixedstring-2\\0\\0\t1970-01-01 06:29:36\t2004-02-03 04:05:06\n+127\t255\t32767\t65535\t2147483647\t4294967295\t9223372036854775807\t9223372036854775807\t-1\t-1\tstring-2\tfixedstring-2\\0\\0\t2004-06-07 00:00:00\t2004-02-03 04:05:06\n dest from null:\n -128\t0\t-32768\t0\t-2147483648\t0\t-9223372036854775808\t0\t-1.032\t-1.064\tstring-1\tfixedstring-1\\0\\0\t2003-04-05\t2003-02-03 04:05:06\n -108\t108\t-1016\t1116\t-1032\t1132\t-1064\t1164\t-1.032\t-1.064\tstring-0\tfixedstring\\0\\0\\0\\0\t2001-02-03\t2002-02-03 04:05:06\ndiff --git a/tests/queries/0_stateless/01277_toUnixTimestamp64.sql b/tests/queries/0_stateless/01277_toUnixTimestamp64.sql\nindex de2b132a2dc8..eb3e8c612edc 100644\n--- a/tests/queries/0_stateless/01277_toUnixTimestamp64.sql\n+++ b/tests/queries/0_stateless/01277_toUnixTimestamp64.sql\n@@ -12,22 +12,22 @@ SELECT toUnixTimestamp64Micro('abc', 123);  -- {serverError 42}\n SELECT toUnixTimestamp64Nano('abc', 123);  -- {serverError 42}\n \n SELECT 'const column';\n-WITH toDateTime64('2019-09-16 19:20:12.345678910', 3) AS dt64\n+WITH toDateTime64('2019-09-16 19:20:12.345678910', 3, 'Europe/Moscow') AS dt64\n SELECT dt64, toUnixTimestamp64Milli(dt64), toUnixTimestamp64Micro(dt64), toUnixTimestamp64Nano(dt64);\n \n-WITH toDateTime64('2019-09-16 19:20:12.345678910', 6) AS dt64\n+WITH toDateTime64('2019-09-16 19:20:12.345678910', 6, 'Europe/Moscow') AS dt64\n SELECT dt64, toUnixTimestamp64Milli(dt64), toUnixTimestamp64Micro(dt64), toUnixTimestamp64Nano(dt64);\n \n-WITH toDateTime64('2019-09-16 19:20:12.345678910', 9) AS dt64\n+WITH toDateTime64('2019-09-16 19:20:12.345678910', 9, 'Europe/Moscow') AS dt64\n SELECT dt64, toUnixTimestamp64Milli(dt64), toUnixTimestamp64Micro(dt64), toUnixTimestamp64Nano(dt64);\n \n SELECT 'non-const column';\n-WITH toDateTime64('2019-09-16 19:20:12.345678910', 3) AS x\n+WITH toDateTime64('2019-09-16 19:20:12.345678910', 3, 'Europe/Moscow') AS x\n SELECT materialize(x) as dt64, toUnixTimestamp64Milli(dt64), toUnixTimestamp64Micro(dt64), toUnixTimestamp64Nano(dt64);\n \n-WITH toDateTime64('2019-09-16 19:20:12.345678910', 6) AS x\n+WITH toDateTime64('2019-09-16 19:20:12.345678910', 6, 'Europe/Moscow') AS x\n SELECT materialize(x) as dt64, toUnixTimestamp64Milli(dt64), toUnixTimestamp64Micro(dt64), toUnixTimestamp64Nano(dt64);\n \n-WITH toDateTime64('2019-09-16 19:20:12.345678910', 9) AS x\n+WITH toDateTime64('2019-09-16 19:20:12.345678910', 9, 'Europe/Moscow') AS x\n SELECT materialize(x) as dt64, toUnixTimestamp64Milli(dt64), toUnixTimestamp64Micro(dt64), toUnixTimestamp64Nano(dt64);\n \ndiff --git a/tests/queries/0_stateless/01307_orc_output_format.sh b/tests/queries/0_stateless/01307_orc_output_format.sh\nindex 26c7db5ad1b6..b5000bfd3fca 100755\n--- a/tests/queries/0_stateless/01307_orc_output_format.sh\n+++ b/tests/queries/0_stateless/01307_orc_output_format.sh\n@@ -6,7 +6,7 @@ CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n \n $CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS orc\";\n \n-$CLICKHOUSE_CLIENT --query=\"CREATE TABLE orc (uint8 UInt8, uint16 UInt16, uint32 UInt32, uint64 UInt64, int8 Int8, int16 Int16, int32 Int32, int64 Int64, float Float32, double Float64, string String, fixed FixedString(4), date Date, datetime DateTime, decimal32 Decimal32(4), decimal64 Decimal64(10), decimal128 Decimal128(20), nullable Nullable(Int32)) ENGINE = Memory\";\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE orc (uint8 UInt8, uint16 UInt16, uint32 UInt32, uint64 UInt64, int8 Int8, int16 Int16, int32 Int32, int64 Int64, float Float32, double Float64, string String, fixed FixedString(4), date Date, datetime DateTime('Europe/Moscow'), decimal32 Decimal32(4), decimal64 Decimal64(10), decimal128 Decimal128(20), nullable Nullable(Int32)) ENGINE = Memory\";\n \n $CLICKHOUSE_CLIENT --query=\"INSERT INTO orc VALUES (255, 65535, 4294967295, 100000000000, -128, -32768, -2147483648, -100000000000, 2.02, 10000.0000001, 'String', '2020', 18980, 1639872000, 1.0001, 1.00000001, 100000.00000000000001, 1), (4, 1234, 3244467295, 500000000000, -1, -256, -14741221, -7000000000, 100.1, 14321.032141201, 'Another string', '2000', 20000, 1839882000, 34.1234, 123123.123123123, 123123123.123123123123123, NULL), (42, 42, 42, 42, 42, 42, 42, 42, 42.42, 42.42, '42', '4242', 42, 42, 42.42, 42.42424242, 424242.42424242424242, 42)\";\n \ndiff --git a/tests/queries/0_stateless/01379_with_fill_several_columns.sql b/tests/queries/0_stateless/01379_with_fill_several_columns.sql\nindex f98431b61b9a..505b9e0f8e16 100644\n--- a/tests/queries/0_stateless/01379_with_fill_several_columns.sql\n+++ b/tests/queries/0_stateless/01379_with_fill_several_columns.sql\n@@ -1,6 +1,6 @@\n SELECT\n-    toDate((number * 10) * 86400) AS d1, \n-    toDate(number * 86400) AS d2, \n+    toDate(toDateTime((number * 10) * 86400, 'Europe/Moscow')) AS d1,\n+    toDate(toDateTime(number * 86400, 'Europe/Moscow')) AS d2,\n     'original' AS source\n FROM numbers(10)\n WHERE (number % 3) = 1\n@@ -11,11 +11,11 @@ ORDER BY\n SELECT '===============';\n \n SELECT\n-    toDate((number * 10) * 86400) AS d1, \n-    toDate(number * 86400) AS d2, \n+    toDate(toDateTime((number * 10) * 86400, 'Europe/Moscow')) AS d1,\n+    toDate(toDateTime(number * 86400, 'Europe/Moscow')) AS d2,\n     'original' AS source\n FROM numbers(10)\n WHERE (number % 3) = 1\n ORDER BY\n     d1 WITH FILL STEP 5,\n-    d2 WITH FILL;\n\\ No newline at end of file\n+    d2 WITH FILL;\ndiff --git a/tests/queries/0_stateless/01440_to_date_monotonicity.sql b/tests/queries/0_stateless/01440_to_date_monotonicity.sql\nindex e48911e954cf..8843d7ffca68 100644\n--- a/tests/queries/0_stateless/01440_to_date_monotonicity.sql\n+++ b/tests/queries/0_stateless/01440_to_date_monotonicity.sql\n@@ -1,14 +1,16 @@\n DROP TABLE IF EXISTS tdm;\n DROP TABLE IF EXISTS tdm2;\n-CREATE TABLE tdm (x DateTime) ENGINE = MergeTree ORDER BY x SETTINGS write_final_mark = 0;\n+CREATE TABLE tdm (x DateTime('Europe/Moscow')) ENGINE = MergeTree ORDER BY x SETTINGS write_final_mark = 0;\n INSERT INTO tdm VALUES (now());\n-SELECT count(x) FROM tdm WHERE toDate(x) < today() SETTINGS max_rows_to_read = 1;\n+SELECT count(x) FROM tdm WHERE toDate(x) < toDate(now(), 'Europe/Moscow') SETTINGS max_rows_to_read = 1;\n \n-SELECT toDate(-1), toDate(10000000000000), toDate(100), toDate(65536), toDate(65535);\n-SELECT toDateTime(-1), toDateTime(10000000000000), toDateTime(1000);\n+SELECT toDate(-1), toDate(10000000000000, 'Europe/Moscow'), toDate(100), toDate(65536, 'UTC'), toDate(65535, 'Europe/Moscow');\n+SELECT toDateTime(-1, 'Europe/Moscow'), toDateTime(10000000000000, 'Europe/Moscow'), toDateTime(1000, 'Europe/Moscow');\n \n CREATE TABLE tdm2 (timestamp UInt32) ENGINE = MergeTree ORDER BY timestamp SETTINGS index_granularity = 1;\n+\n INSERT INTO tdm2 VALUES (toUnixTimestamp('2000-01-01 13:12:12')), (toUnixTimestamp('2000-01-01 14:12:12')), (toUnixTimestamp('2000-01-01 15:12:12'));\n+\n SET max_rows_to_read = 1;\n SELECT toDateTime(timestamp) FROM tdm2 WHERE toHour(toDateTime(timestamp)) = 13;\n \ndiff --git a/tests/queries/0_stateless/01508_partition_pruning_long.queries b/tests/queries/0_stateless/01508_partition_pruning_long.queries\nindex 3773e907c532..786240145a95 100644\n--- a/tests/queries/0_stateless/01508_partition_pruning_long.queries\n+++ b/tests/queries/0_stateless/01508_partition_pruning_long.queries\n@@ -2,20 +2,20 @@ DROP TABLE IF EXISTS tMM;\n DROP TABLE IF EXISTS tDD;\n DROP TABLE IF EXISTS sDD;\n DROP TABLE IF EXISTS xMM;\n-CREATE TABLE tMM(d DateTime,a Int64) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY tuple() SETTINGS index_granularity = 8192;\n+CREATE TABLE tMM(d DateTime('Europe/Moscow'), a Int64) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY tuple() SETTINGS index_granularity = 8192;\n SYSTEM STOP MERGES tMM;\n-INSERT INTO tMM SELECT toDateTime('2020-08-16 00:00:00') + number*60, number FROM numbers(5000);\n-INSERT INTO tMM SELECT toDateTime('2020-08-16 00:00:00') + number*60, number FROM numbers(5000);\n-INSERT INTO tMM SELECT toDateTime('2020-09-01 00:00:00') + number*60, number FROM numbers(5000);\n-INSERT INTO tMM SELECT toDateTime('2020-09-01 00:00:00') + number*60, number FROM numbers(5000);\n-INSERT INTO tMM SELECT toDateTime('2020-10-01 00:00:00') + number*60, number FROM numbers(5000);\n-INSERT INTO tMM SELECT toDateTime('2020-10-15 00:00:00') + number*60, number FROM numbers(5000);\n-\n-CREATE TABLE tDD(d DateTime,a Int) ENGINE = MergeTree PARTITION BY toYYYYMMDD(d) ORDER BY tuple() SETTINGS index_granularity = 8192;\n+INSERT INTO tMM SELECT toDateTime('2020-08-16 00:00:00', 'Europe/Moscow') + number*60, number FROM numbers(5000);\n+INSERT INTO tMM SELECT toDateTime('2020-08-16 00:00:00', 'Europe/Moscow') + number*60, number FROM numbers(5000);\n+INSERT INTO tMM SELECT toDateTime('2020-09-01 00:00:00', 'Europe/Moscow') + number*60, number FROM numbers(5000);\n+INSERT INTO tMM SELECT toDateTime('2020-09-01 00:00:00', 'Europe/Moscow') + number*60, number FROM numbers(5000);\n+INSERT INTO tMM SELECT toDateTime('2020-10-01 00:00:00', 'Europe/Moscow') + number*60, number FROM numbers(5000);\n+INSERT INTO tMM SELECT toDateTime('2020-10-15 00:00:00', 'Europe/Moscow') + number*60, number FROM numbers(5000);\n+\n+CREATE TABLE tDD(d DateTime('Europe/Moscow'),a Int) ENGINE = MergeTree PARTITION BY toYYYYMMDD(d) ORDER BY tuple() SETTINGS index_granularity = 8192;\n SYSTEM STOP MERGES tDD;\n-insert into tDD select toDateTime(toDate('2020-09-23')), number from numbers(10000) UNION ALL select toDateTime(toDateTime('2020-09-23 11:00:00')), number from numbers(10000) UNION ALL select toDateTime(toDate('2020-09-24')), number from numbers(10000) UNION ALL select toDateTime(toDate('2020-09-25')), number from numbers(10000) UNION ALL select toDateTime(toDate('2020-08-15')), number from numbers(10000);\n+insert into tDD select toDateTime(toDate('2020-09-23'), 'Europe/Moscow'), number from numbers(10000) UNION ALL select toDateTime(toDateTime('2020-09-23 11:00:00', 'Europe/Moscow')), number from numbers(10000) UNION ALL select toDateTime(toDate('2020-09-24'), 'Europe/Moscow'), number from numbers(10000) UNION ALL select toDateTime(toDate('2020-09-25'), 'Europe/Moscow'), number from numbers(10000) UNION ALL select toDateTime(toDate('2020-08-15'), 'Europe/Moscow'), number from numbers(10000);\n \n-CREATE TABLE sDD(d UInt64,a Int) ENGINE = MergeTree PARTITION BY toYYYYMM(toDate(intDiv(d,1000))) ORDER BY tuple() SETTINGS index_granularity = 8192;\n+CREATE TABLE sDD(d UInt64,a Int) ENGINE = MergeTree PARTITION BY toYYYYMM(toDate(intDiv(d,1000), 'Europe/Moscow')) ORDER BY tuple() SETTINGS index_granularity = 8192;\n SYSTEM STOP MERGES sDD;\n insert into sDD select (1597536000+number*60)*1000, number from numbers(5000);\n insert into sDD select (1597536000+number*60)*1000, number from numbers(5000);\n@@ -24,14 +24,14 @@ insert into sDD select (1598918400+number*60)*1000, number from numbers(5000);\n insert into sDD select (1601510400+number*60)*1000, number from numbers(5000);\n insert into sDD select (1602720000+number*60)*1000, number from numbers(5000);\n \n-CREATE TABLE xMM(d DateTime,a Int64, f Int64) ENGINE = MergeTree PARTITION BY (toYYYYMM(d), a) ORDER BY tuple() SETTINGS index_granularity = 8192;\n+CREATE TABLE xMM(d DateTime('Europe/Moscow'),a Int64, f Int64) ENGINE = MergeTree PARTITION BY (toYYYYMM(d), a) ORDER BY tuple() SETTINGS index_granularity = 8192;\n SYSTEM STOP MERGES xMM;\n-INSERT INTO xMM SELECT toDateTime('2020-08-16 00:00:00') + number*60, 1, number FROM numbers(5000);\n-INSERT INTO xMM SELECT toDateTime('2020-08-16 00:00:00') + number*60, 2, number FROM numbers(5000);\n-INSERT INTO xMM SELECT toDateTime('2020-09-01 00:00:00') + number*60, 3, number FROM numbers(5000);\n-INSERT INTO xMM SELECT toDateTime('2020-09-01 00:00:00') + number*60, 2, number FROM numbers(5000);\n-INSERT INTO xMM SELECT toDateTime('2020-10-01 00:00:00') + number*60, 1, number FROM numbers(5000);\n-INSERT INTO xMM SELECT toDateTime('2020-10-15 00:00:00') + number*60, 1, number FROM numbers(5000);\n+INSERT INTO xMM SELECT toDateTime('2020-08-16 00:00:00', 'Europe/Moscow') + number*60, 1, number FROM numbers(5000);\n+INSERT INTO xMM SELECT toDateTime('2020-08-16 00:00:00', 'Europe/Moscow') + number*60, 2, number FROM numbers(5000);\n+INSERT INTO xMM SELECT toDateTime('2020-09-01 00:00:00', 'Europe/Moscow') + number*60, 3, number FROM numbers(5000);\n+INSERT INTO xMM SELECT toDateTime('2020-09-01 00:00:00', 'Europe/Moscow') + number*60, 2, number FROM numbers(5000);\n+INSERT INTO xMM SELECT toDateTime('2020-10-01 00:00:00', 'Europe/Moscow') + number*60, 1, number FROM numbers(5000);\n+INSERT INTO xMM SELECT toDateTime('2020-10-15 00:00:00', 'Europe/Moscow') + number*60, 1, number FROM numbers(5000);\n \n \n SELECT '--------- tMM ----------------------------';\n@@ -44,8 +44,8 @@ select uniqExact(_part), count() from tMM where toYYYYMMDD(d)=20200816;\n select uniqExact(_part), count() from tMM where toYYYYMMDD(d)=20201015;\n select uniqExact(_part), count() from tMM where toDate(d)='2020-10-15';\n select uniqExact(_part), count() from tMM where d >= '2020-09-01 00:00:00' and d<'2020-10-15 00:00:00';\n-select uniqExact(_part), count() from tMM where d >= '2020-01-16 00:00:00' and d < toDateTime('2021-08-17 00:00:00');\n-select uniqExact(_part), count() from tMM where d >= '2020-09-16 00:00:00' and d < toDateTime('2020-10-01 00:00:00');\n+select uniqExact(_part), count() from tMM where d >= '2020-01-16 00:00:00' and d < toDateTime('2021-08-17 00:00:00', 'Europe/Moscow');\n+select uniqExact(_part), count() from tMM where d >= '2020-09-16 00:00:00' and d < toDateTime('2020-10-01 00:00:00', 'Europe/Moscow');\n select uniqExact(_part), count() from tMM where d >= '2020-09-12 00:00:00' and d < '2020-10-16 00:00:00';\n select uniqExact(_part), count() from tMM where toStartOfDay(d) >= '2020-09-12 00:00:00';\n select uniqExact(_part), count() from tMM where toStartOfDay(d) = '2020-09-01 00:00:00';\ndiff --git a/tests/queries/0_stateless/01508_partition_pruning_long.reference b/tests/queries/0_stateless/01508_partition_pruning_long.reference\nindex 334ecb63164a..9cd208a336f6 100644\n--- a/tests/queries/0_stateless/01508_partition_pruning_long.reference\n+++ b/tests/queries/0_stateless/01508_partition_pruning_long.reference\n@@ -35,11 +35,11 @@ select uniqExact(_part), count() from tMM where d >= '2020-09-01 00:00:00' and d\n 3\t15000\n Selected 3/6 parts by partition key, 3 parts by primary key, 3/3 marks by primary key, 3 marks to read from 3 ranges\n \n-select uniqExact(_part), count() from tMM where d >= '2020-01-16 00:00:00' and d < toDateTime('2021-08-17 00:00:00');\n+select uniqExact(_part), count() from tMM where d >= '2020-01-16 00:00:00' and d < toDateTime('2021-08-17 00:00:00', 'Europe/Moscow');\n 6\t30000\n Selected 6/6 parts by partition key, 6 parts by primary key, 6/6 marks by primary key, 6 marks to read from 6 ranges\n \n-select uniqExact(_part), count() from tMM where d >= '2020-09-16 00:00:00' and d < toDateTime('2020-10-01 00:00:00');\n+select uniqExact(_part), count() from tMM where d >= '2020-09-16 00:00:00' and d < toDateTime('2020-10-01 00:00:00', 'Europe/Moscow');\n 0\t0\n Selected 0/6 parts by partition key, 0 parts by primary key, 0/0 marks by primary key, 0 marks to read from 0 ranges\n \ndiff --git a/tests/queries/0_stateless/01508_partition_pruning_long.sh b/tests/queries/0_stateless/01508_partition_pruning_long.sh\nindex 1b3c524ac77b..745d08496a72 100755\n--- a/tests/queries/0_stateless/01508_partition_pruning_long.sh\n+++ b/tests/queries/0_stateless/01508_partition_pruning_long.sh\n@@ -1,22 +1,15 @@\n #!/usr/bin/env bash\n \n-#-------------------------------------------------------------------------------------------\n # Description of test result:\n-#   Test the correctness of the partition\n-#   pruning\n+# Test the correctness of the partition pruning\n #\n-#   Script executes queries from a file 01508_partition_pruning_long.queries  (1 line = 1 query)\n-#   Queries are started with 'select' (but NOT with 'SELECT') are executed with log_level=debug\n-#-------------------------------------------------------------------------------------------\n+# Script executes queries from a file 01508_partition_pruning_long.queries (1 line = 1 query)\n+# Queries are started with 'select' (but NOT with 'SELECT') are executed with log_level=debug\n \n CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n # shellcheck source=../shell_config.sh\n . \"$CURDIR\"/../shell_config.sh\n \n-#export CLICKHOUSE_CLIENT=\"clickhouse-client --send_logs_level=none\"\n-#export CLICKHOUSE_CLIENT_SERVER_LOGS_LEVEL=none\n-#export CURDIR=.\n-\n \n queries=\"${CURDIR}/01508_partition_pruning_long.queries\"\n while IFS= read -r sql\ndiff --git a/tests/queries/0_stateless/01656_sequence_next_node_long.sql b/tests/queries/0_stateless/01656_sequence_next_node_long.sql\nindex d0d01e989b82..9c181f5e491c 100644\n--- a/tests/queries/0_stateless/01656_sequence_next_node_long.sql\n+++ b/tests/queries/0_stateless/01656_sequence_next_node_long.sql\n@@ -4,30 +4,30 @@ DROP TABLE IF EXISTS test_sequenceNextNode_Nullable;\n \n CREATE TABLE IF NOT EXISTS test_sequenceNextNode_Nullable (dt DateTime, id int, action Nullable(String)) ENGINE = MergeTree() PARTITION BY dt ORDER BY id;\n \n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',1,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',1,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:03',1,'C');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:04',1,'D');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',2,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',2,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:03',2,'D');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:04',2,'C');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',3,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',3,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',4,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',4,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:03',4,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:04',4,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:05',4,'C');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',5,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',5,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:03',5,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:04',5,'C');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',6,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',6,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:03',6,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:04',6,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:05',6,'C');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',1,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',1,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:03',1,'C');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:04',1,'D');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',2,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',2,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:03',2,'D');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:04',2,'C');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',3,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',3,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',4,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',4,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:03',4,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:04',4,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:05',4,'C');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',5,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',5,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:03',5,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:04',5,'C');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',6,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',6,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:03',6,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:04',6,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:05',6,'C');\n \n SELECT '(forward, head, A)', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A') AS next_node FROM test_sequenceNextNode_Nullable GROUP BY id ORDER BY id;\n SELECT '(forward, head, B)', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'B') AS next_node FROM test_sequenceNextNode_Nullable GROUP BY id ORDER BY id;\n@@ -50,11 +50,11 @@ SELECT '(forward, head, B->A->A)', id, sequenceNextNode('forward', 'head')(dt, a\n SELECT '(backward, tail, A->A->B)', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'A', action = 'A', action = 'B') AS next_node FROM test_sequenceNextNode_Nullable GROUP BY id ORDER BY id;\n SELECT '(backward, tail, B->A->A)', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'B', action = 'A', action = 'A') AS next_node FROM test_sequenceNextNode_Nullable GROUP BY id ORDER BY id;\n \n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',10,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',10,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:02',10,NULL);\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:03',10,'C');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:04',10,'D');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',10,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',10,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:02',10,NULL);\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:03',10,'C');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:04',10,'D');\n \n SELECT '(forward, head, A) id >= 10', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A') AS next_node FROM test_sequenceNextNode_Nullable WHERE id >= 10 GROUP BY id ORDER BY id;\n SELECT '(forward, head, A) id >= 10', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A', action = 'B') AS next_node FROM test_sequenceNextNode_Nullable WHERE id >= 10 GROUP BY id ORDER BY id;\n@@ -63,10 +63,10 @@ SELECT '(forward, head, A) id >= 10', id, sequenceNextNode('backward', 'tail')(d\n SELECT '(backward, tail, A) id >= 10', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'D', action = 'C') AS next_node FROM test_sequenceNextNode_Nullable WHERE id >= 10 GROUP BY id ORDER BY id;\n SELECT '(backward, tail, A) id >= 10', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'D', action = 'C', action = 'B') AS next_node FROM test_sequenceNextNode_Nullable WHERE id >= 10 GROUP BY id ORDER BY id;\n \n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',11,'A');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',11,'B');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',11,'C');\n-INSERT INTO test_sequenceNextNode_Nullable values ('1970-01-01 09:00:01',11,'D');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',11,'A');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',11,'B');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',11,'C');\n+INSERT INTO test_sequenceNextNode_Nullable values ('2000-01-02 09:00:01',11,'D');\n \n SELECT '(0, A) id = 11', count() FROM (SELECT id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A') AS next_node FROM test_sequenceNextNode_Nullable WHERE id = 11 GROUP BY id HAVING next_node  = 'B');\n SELECT '(0, C) id = 11', count() FROM (SELECT id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'C') AS next_node FROM test_sequenceNextNode_Nullable WHERE id = 11 GROUP BY id HAVING next_node = 'D');\n@@ -100,30 +100,30 @@ DROP TABLE IF EXISTS test_sequenceNextNode;\n \n CREATE TABLE IF NOT EXISTS test_sequenceNextNode (dt DateTime, id int, action String) ENGINE = MergeTree() PARTITION BY dt ORDER BY id;\n \n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',1,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:02',1,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:03',1,'C');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:04',1,'D');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',2,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:02',2,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:03',2,'D');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:04',2,'C');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',3,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:02',3,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',4,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:02',4,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:03',4,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:04',4,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:05',4,'C');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',5,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:02',5,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:03',5,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:04',5,'C');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',6,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:02',6,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:03',6,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:04',6,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:05',6,'C');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',1,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:02',1,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:03',1,'C');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:04',1,'D');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',2,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:02',2,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:03',2,'D');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:04',2,'C');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',3,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:02',3,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',4,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:02',4,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:03',4,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:04',4,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:05',4,'C');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',5,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:02',5,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:03',5,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:04',5,'C');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',6,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:02',6,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:03',6,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:04',6,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:05',6,'C');\n \n SELECT '(forward, head, A)', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A') AS next_node FROM test_sequenceNextNode GROUP BY id ORDER BY id;\n SELECT '(forward, head, B)', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'B') AS next_node FROM test_sequenceNextNode GROUP BY id ORDER BY id;\n@@ -146,10 +146,10 @@ SELECT '(forward, head, B->A->A)', id, sequenceNextNode('forward', 'head')(dt, a\n SELECT '(backward, tail, A->A->B)', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'A', action = 'A', action = 'B') AS next_node FROM test_sequenceNextNode GROUP BY id ORDER BY id;\n SELECT '(backward, tail, B->A->A)', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'B', action = 'A', action = 'A') AS next_node FROM test_sequenceNextNode GROUP BY id ORDER BY id;\n \n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',10,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:02',10,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:03',10,'C');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:04',10,'D');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',10,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:02',10,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:03',10,'C');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:04',10,'D');\n \n SELECT '(forward, head, A) id >= 10', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A') AS next_node FROM test_sequenceNextNode WHERE id >= 10 GROUP BY id ORDER BY id;\n SELECT '(forward, head, A) id >= 10', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A', action = 'B') AS next_node FROM test_sequenceNextNode WHERE id >= 10 GROUP BY id ORDER BY id;\n@@ -158,10 +158,10 @@ SELECT '(forward, head, A) id >= 10', id, sequenceNextNode('backward', 'tail')(d\n SELECT '(backward, tail, A) id >= 10', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'D', action = 'C') AS next_node FROM test_sequenceNextNode WHERE id >= 10 GROUP BY id ORDER BY id;\n SELECT '(backward, tail, A) id >= 10', id, sequenceNextNode('backward', 'tail')(dt, action, 1, action = 'D', action = 'C', action = 'B') AS next_node FROM test_sequenceNextNode WHERE id >= 10 GROUP BY id ORDER BY id;\n \n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',11,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',11,'B');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',11,'C');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',11,'D');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',11,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',11,'B');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',11,'C');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',11,'D');\n \n SELECT '(0, A) id = 11', count() FROM (SELECT id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A') AS next_node FROM test_sequenceNextNode WHERE id = 11 GROUP BY id HAVING next_node  = 'B');\n SELECT '(0, C) id = 11', count() FROM (SELECT id, sequenceNextNode('forward', 'head')(dt, action, 1, action = 'C') AS next_node FROM test_sequenceNextNode WHERE id = 11 GROUP BY id HAVING next_node = 'D');\n@@ -189,8 +189,8 @@ SELECT '(backward, first_match, B->B)', id, sequenceNextNode('backward', 'first_\n \n SELECT '(max_args)', id, sequenceNextNode('forward', 'head')(dt, action, 1, action = '0', action = '1', action = '2', action = '3', action = '4', action = '5', action = '6', action = '7', action = '8', action = '9', action = '10', action = '11', action = '12', action = '13', action = '14', action = '15', action = '16', action = '17', action = '18', action = '19', action = '20', action = '21', action = '22', action = '23', action = '24', action = '25', action = '26', action = '27', action = '28', action = '29', action = '30', action = '31', action = '32', action = '33', action = '34', action = '35', action = '36', action = '37', action = '38', action = '39', action = '40', action = '41', action = '42', action = '43', action = '44', action = '45', action = '46', action = '47', action = '48', action = '49', action = '50', action = '51', action = '52', action = '53', action = '54', action = '55', action = '56', action = '57', action = '58', action = '59', action = '60', action = '61', action = '62', action = '63') from test_sequenceNextNode GROUP BY id ORDER BY id;\n \n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',12,'A');\n-INSERT INTO test_sequenceNextNode values ('1970-01-01 09:00:01',12,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',12,'A');\n+INSERT INTO test_sequenceNextNode values ('2000-01-02 09:00:01',12,'A');\n \n SELECT '(forward, head, A) id = 12', sequenceNextNode('forward', 'head')(dt, action, 1, action = 'A') AS next_node FROM test_sequenceNextNode WHERE id = 12;\n \n@@ -200,18 +200,18 @@ DROP TABLE IF EXISTS test_base_condition;\n \n CREATE TABLE IF NOT EXISTS test_base_condition (dt DateTime, id int, action String, referrer String) ENGINE = MergeTree() PARTITION BY dt ORDER BY id;\n \n-INSERT INTO test_base_condition values ('1970-01-01 09:00:01',1,'A','1');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:02',1,'B','2');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:03',1,'C','3');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:04',1,'D','4');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:01',2,'D','4');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:02',2,'C','3');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:03',2,'B','2');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:04',2,'A','1');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:01',3,'B','10');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:02',3,'B','2');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:03',3,'D','3');\n-INSERT INTO test_base_condition values ('1970-01-01 09:00:04',3,'C','4');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:01',1,'A','1');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:02',1,'B','2');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:03',1,'C','3');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:04',1,'D','4');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:01',2,'D','4');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:02',2,'C','3');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:03',2,'B','2');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:04',2,'A','1');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:01',3,'B','10');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:02',3,'B','2');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:03',3,'D','3');\n+INSERT INTO test_base_condition values ('2000-01-02 09:00:04',3,'C','4');\n \n SELECT '(forward, head, 1)', id, sequenceNextNode('forward', 'head')(dt, action, referrer = '1') AS next_node FROM test_base_condition GROUP BY id ORDER BY id;\n SELECT '(forward, head, 1, A)', id, sequenceNextNode('forward', 'head')(dt, action, referrer = '1', action = 'A') AS next_node FROM test_base_condition GROUP BY id ORDER BY id;\ndiff --git a/tests/queries/0_stateless/01676_reinterpret_as.sql b/tests/queries/0_stateless/01676_reinterpret_as.sql\nindex 5eb94ed0a135..e8c2a0b1373a 100644\n--- a/tests/queries/0_stateless/01676_reinterpret_as.sql\n+++ b/tests/queries/0_stateless/01676_reinterpret_as.sql\n@@ -30,8 +30,8 @@ SELECT reinterpret(a, 'String'), reinterpretAsString(a), reinterpretAsUInt8('11'\n SELECT reinterpret(a, 'String'), reinterpretAsString(a), reinterpretAsUInt16('11') as a;\n SELECT 'Dates';\n SELECT reinterpret(0, 'Date'), reinterpret('', 'Date');\n-SELECT reinterpret(0, 'DateTime'), reinterpret('', 'DateTime');\n-SELECT reinterpret(0, 'DateTime64'), reinterpret('', 'DateTime64');\n+SELECT reinterpret(0, 'DateTime(''Europe/Moscow'')'), reinterpret('', 'DateTime(''Europe/Moscow'')');\n+SELECT reinterpret(0, 'DateTime64(3, ''Europe/Moscow'')'), reinterpret('', 'DateTime64(3, ''Europe/Moscow'')');\n SELECT 'Decimals';\n SELECT reinterpret(toDecimal32(5, 2), 'Decimal32(2)'), reinterpret('1', 'Decimal32(2)');\n SELECT reinterpret(toDecimal64(5, 2), 'Decimal64(2)'), reinterpret('1', 'Decimal64(2)');;\ndiff --git a/tests/queries/0_stateless/01691_DateTime64_clamp.reference b/tests/queries/0_stateless/01691_DateTime64_clamp.reference\nindex 881ab4feff8a..41a8d653a3f6 100644\n--- a/tests/queries/0_stateless/01691_DateTime64_clamp.reference\n+++ b/tests/queries/0_stateless/01691_DateTime64_clamp.reference\n@@ -17,11 +17,11 @@ SELECT toDateTime64(toFloat32(bitShiftLeft(toUInt64(1),33)), 2, 'Europe/Moscow')\n 2106-02-07 09:28:16.00\n SELECT toDateTime64(toFloat64(bitShiftLeft(toUInt64(1),33)), 2, 'Europe/Moscow') FORMAT Null;\n -- These are outsize of extended range and hence clamped\n-SELECT toDateTime64(-1 * bitShiftLeft(toUInt64(1), 35), 2);\n+SELECT toDateTime64(-1 * bitShiftLeft(toUInt64(1), 35), 2, 'Europe/Moscow');\n 1925-01-01 02:00:00.00\n-SELECT CAST(-1 * bitShiftLeft(toUInt64(1), 35) AS DateTime64);\n+SELECT CAST(-1 * bitShiftLeft(toUInt64(1), 35) AS DateTime64(3, 'Europe/Moscow'));\n 1925-01-01 02:00:00.000\n-SELECT CAST(bitShiftLeft(toUInt64(1), 35) AS DateTime64);\n+SELECT CAST(bitShiftLeft(toUInt64(1), 35) AS DateTime64(3, 'Europe/Moscow'));\n 2282-12-31 03:00:00.000\n-SELECT toDateTime64(bitShiftLeft(toUInt64(1), 35), 2);\n+SELECT toDateTime64(bitShiftLeft(toUInt64(1), 35), 2, 'Europe/Moscow');\n 2282-12-31 03:00:00.00\ndiff --git a/tests/queries/0_stateless/01691_DateTime64_clamp.sql b/tests/queries/0_stateless/01691_DateTime64_clamp.sql\nindex c77a66febb3e..2786d9c1c093 100644\n--- a/tests/queries/0_stateless/01691_DateTime64_clamp.sql\n+++ b/tests/queries/0_stateless/01691_DateTime64_clamp.sql\n@@ -11,7 +11,7 @@ SELECT toDateTime64(toFloat32(bitShiftLeft(toUInt64(1),33)), 2, 'Europe/Moscow')\n SELECT toDateTime64(toFloat64(bitShiftLeft(toUInt64(1),33)), 2, 'Europe/Moscow') FORMAT Null;\n \n -- These are outsize of extended range and hence clamped\n-SELECT toDateTime64(-1 * bitShiftLeft(toUInt64(1), 35), 2);\n-SELECT CAST(-1 * bitShiftLeft(toUInt64(1), 35) AS DateTime64);\n-SELECT CAST(bitShiftLeft(toUInt64(1), 35) AS DateTime64);\n-SELECT toDateTime64(bitShiftLeft(toUInt64(1), 35), 2);\n+SELECT toDateTime64(-1 * bitShiftLeft(toUInt64(1), 35), 2, 'Europe/Moscow');\n+SELECT CAST(-1 * bitShiftLeft(toUInt64(1), 35) AS DateTime64(3, 'Europe/Moscow'));\n+SELECT CAST(bitShiftLeft(toUInt64(1), 35) AS DateTime64(3, 'Europe/Moscow'));\n+SELECT toDateTime64(bitShiftLeft(toUInt64(1), 35), 2, 'Europe/Moscow');\ndiff --git a/tests/queries/0_stateless/01692_DateTime64_from_DateTime.reference b/tests/queries/0_stateless/01692_DateTime64_from_DateTime.reference\nindex a0562e400276..3473b027c22b 100644\n--- a/tests/queries/0_stateless/01692_DateTime64_from_DateTime.reference\n+++ b/tests/queries/0_stateless/01692_DateTime64_from_DateTime.reference\n@@ -1,9 +1,5 @@\n--- { echo }\n-select toDateTime64(toDateTime(1), 2);\n 1970-01-01 03:00:01.00\n-select toDateTime64(toDate(1), 2);\n+1970-01-01 03:00:01.00\n 1970-01-02 00:00:00.00\n-select toDateTime64(toDateTime(1), 2, 'GMT');\n 1970-01-01 00:00:01.00\n-select toDateTime64(toDate(1), 2, 'GMT');\n 1970-01-02 00:00:00.00\ndiff --git a/tests/queries/0_stateless/01692_DateTime64_from_DateTime.sql b/tests/queries/0_stateless/01692_DateTime64_from_DateTime.sql\nindex 60f76e9192c0..fac0c3410075 100644\n--- a/tests/queries/0_stateless/01692_DateTime64_from_DateTime.sql\n+++ b/tests/queries/0_stateless/01692_DateTime64_from_DateTime.sql\n@@ -1,5 +1,7 @@\n--- { echo }\n-select toDateTime64(toDateTime(1), 2);\n-select toDateTime64(toDate(1), 2);\n+select toDateTime64(toDateTime(1, 'Europe/Moscow'), 2);\n+select toDateTime64(toDate(1), 2) FORMAT Null; -- Unknown timezone\n+select toDateTime64(toDateTime(1), 2) FORMAT Null; -- Unknown timezone\n+select toDateTime64(toDateTime(1), 2, 'Europe/Moscow');\n+select toDateTime64(toDate(1), 2, 'Europe/Moscow');\n select toDateTime64(toDateTime(1), 2, 'GMT');\n select toDateTime64(toDate(1), 2, 'GMT');\ndiff --git a/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.reference b/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.reference\nindex 7e8307d66a69..c5e86963f222 100644\n--- a/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.reference\n+++ b/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.reference\n@@ -1,9 +1,4 @@\n--- { echo }\n-SELECT toString(toDateTime('-922337203.6854775808', 1));\n 1940-10-09 22:13:17.6\n-SELECT toString(toDateTime('9922337203.6854775808', 1));\n 2283-11-11 23:46:43.6\n-SELECT toDateTime64(CAST('10000000000.1' AS Decimal64(1)), 1);\n 2283-11-11 23:46:40.1\n-SELECT toDateTime64(CAST('-10000000000.1' AS Decimal64(1)), 1);\n 1925-01-01 00:00:00.1\ndiff --git a/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.sql b/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.sql\nindex d1f0416149ab..f51a1bb22801 100644\n--- a/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.sql\n+++ b/tests/queries/0_stateless/01702_toDateTime_from_string_clamping.sql\n@@ -1,5 +1,4 @@\n--- { echo }\n-SELECT toString(toDateTime('-922337203.6854775808', 1));\n-SELECT toString(toDateTime('9922337203.6854775808', 1));\n-SELECT toDateTime64(CAST('10000000000.1' AS Decimal64(1)), 1);\n-SELECT toDateTime64(CAST('-10000000000.1' AS Decimal64(1)), 1);\n+SELECT toString(toDateTime('-922337203.6854775808', 1, 'Europe/Moscow'));\n+SELECT toString(toDateTime('9922337203.6854775808', 1, 'Europe/Moscow'));\n+SELECT toDateTime64(CAST('10000000000.1' AS Decimal64(1)), 1, 'Europe/Moscow');\n+SELECT toDateTime64(CAST('-10000000000.1' AS Decimal64(1)), 1, 'Europe/Moscow');\ndiff --git a/tests/queries/0_stateless/01732_more_consistent_datetime64_parsing.sql b/tests/queries/0_stateless/01732_more_consistent_datetime64_parsing.sql\nindex dcd874f8c45a..88859177a921 100644\n--- a/tests/queries/0_stateless/01732_more_consistent_datetime64_parsing.sql\n+++ b/tests/queries/0_stateless/01732_more_consistent_datetime64_parsing.sql\n@@ -5,7 +5,7 @@ INSERT INTO t VALUES (3, '1111111111222');\n INSERT INTO t VALUES (4, '1111111111.222');\n SELECT * FROM t ORDER BY i;\n \n-SELECT toDateTime64(1111111111.222, 3);\n-SELECT toDateTime64('1111111111.222', 3);\n-SELECT toDateTime64('1111111111222', 3);\n-SELECT ignore(toDateTime64(1111111111222, 3)); -- This gives somewhat correct but unexpected result\n+SELECT toDateTime64(1111111111.222, 3, 'Europe/Moscow');\n+SELECT toDateTime64('1111111111.222', 3, 'Europe/Moscow');\n+SELECT toDateTime64('1111111111222', 3, 'Europe/Moscow');\n+SELECT ignore(toDateTime64(1111111111222, 3, 'Europe/Moscow')); -- This gives somewhat correct but unexpected result\ndiff --git a/tests/queries/0_stateless/01734_datetime64_from_float.reference b/tests/queries/0_stateless/01734_datetime64_from_float.reference\nindex 32e7d2736c6b..eb96016311dc 100644\n--- a/tests/queries/0_stateless/01734_datetime64_from_float.reference\n+++ b/tests/queries/0_stateless/01734_datetime64_from_float.reference\n@@ -1,7 +1,3 @@\n--- { echo }\n-SELECT CAST(1111111111.222 AS DateTime64(3));\n 2005-03-18 04:58:31.222\n-SELECT toDateTime(1111111111.222, 3);\n 2005-03-18 04:58:31.222\n-SELECT toDateTime64(1111111111.222, 3);\n 2005-03-18 04:58:31.222\ndiff --git a/tests/queries/0_stateless/01734_datetime64_from_float.sql b/tests/queries/0_stateless/01734_datetime64_from_float.sql\nindex b6be65cb7c25..416638a4a739 100644\n--- a/tests/queries/0_stateless/01734_datetime64_from_float.sql\n+++ b/tests/queries/0_stateless/01734_datetime64_from_float.sql\n@@ -1,4 +1,3 @@\n--- { echo }\n-SELECT CAST(1111111111.222 AS DateTime64(3));\n-SELECT toDateTime(1111111111.222, 3);\n-SELECT toDateTime64(1111111111.222, 3);\n+SELECT CAST(1111111111.222 AS DateTime64(3, 'Europe/Moscow'));\n+SELECT toDateTime(1111111111.222, 3, 'Europe/Moscow');\n+SELECT toDateTime64(1111111111.222, 3, 'Europe/Moscow');\ndiff --git a/tests/queries/0_stateless/01867_support_datetime64_version_column.sql b/tests/queries/0_stateless/01867_support_datetime64_version_column.sql\nindex f4427be635a3..1aea0fb91f21 100644\n--- a/tests/queries/0_stateless/01867_support_datetime64_version_column.sql\n+++ b/tests/queries/0_stateless/01867_support_datetime64_version_column.sql\n@@ -1,5 +1,5 @@\n drop table if exists replacing;\n-create table replacing(    `A` Int64,    `D` DateTime64(9),    `S` String)    ENGINE = ReplacingMergeTree(D) ORDER BY A;\n+create table replacing(    `A` Int64,    `D` DateTime64(9, 'Europe/Moscow'),    `S` String)    ENGINE = ReplacingMergeTree(D) ORDER BY A;\n \n insert into replacing values (1,'1970-01-01 08:25:46.300800000','a');\n insert into replacing values (2,'1970-01-01 08:25:46.300800002','b');\ndiff --git a/tests/queries/0_stateless/01891_partition_hash.sql b/tests/queries/0_stateless/01891_partition_hash.sql\nindex 6e356e799aba..f401c7c2d071 100644\n--- a/tests/queries/0_stateless/01891_partition_hash.sql\n+++ b/tests/queries/0_stateless/01891_partition_hash.sql\n@@ -1,5 +1,5 @@\n drop table if exists tab;\n-create table tab (i8 Int8, i16 Int16, i32 Int32, i64 Int64, i128 Int128, i256 Int256, u8 UInt8, u16 UInt16, u32 UInt32, u64 UInt64, u128 UInt128, u256 UInt256, id UUID, s String, fs FixedString(33), a Array(UInt8), t Tuple(UInt16, UInt32), d Date, dt DateTime, dt64 DateTime64, dec128 Decimal128(3), dec256 Decimal256(4), lc LowCardinality(String)) engine = MergeTree PARTITION BY (i8, i16, i32, i64, i128, i256, u8, u16, u32, u64, u128, u256, id, s, fs, a, t, d, dt, dt64, dec128, dec256, lc) order by tuple();\n+create table tab (i8 Int8, i16 Int16, i32 Int32, i64 Int64, i128 Int128, i256 Int256, u8 UInt8, u16 UInt16, u32 UInt32, u64 UInt64, u128 UInt128, u256 UInt256, id UUID, s String, fs FixedString(33), a Array(UInt8), t Tuple(UInt16, UInt32), d Date, dt DateTime('Europe/Moscow'), dt64 DateTime64(3, 'Europe/Moscow'), dec128 Decimal128(3), dec256 Decimal256(4), lc LowCardinality(String)) engine = MergeTree PARTITION BY (i8, i16, i32, i64, i128, i256, u8, u16, u32, u64, u128, u256, id, s, fs, a, t, d, dt, dt64, dec128, dec256, lc) order by tuple();\n insert into tab values (-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, '61f0c404-5cb3-11e7-907b-a6006ad3dba0', 'a', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', [1, 2, 3], (-1, -2), '2020-01-01', '2020-01-01 01:01:01', '2020-01-01 01:01:01', '123.456', '78.9101', 'a');\n -- Here we check that partition id did not change.\n -- Different result means Backward Incompatible Change. Old partitions will not be accepted by new server.\ndiff --git a/tests/queries/0_stateless/01891_partition_hash_no_long_int.sql b/tests/queries/0_stateless/01891_partition_hash_no_long_int.sql\nindex bf5c24579239..0751ff2729f5 100644\n--- a/tests/queries/0_stateless/01891_partition_hash_no_long_int.sql\n+++ b/tests/queries/0_stateless/01891_partition_hash_no_long_int.sql\n@@ -1,5 +1,5 @@\n drop table if exists tab;\n-create table tab (i8 Int8, i16 Int16, i32 Int32, i64 Int64, u8 UInt8, u16 UInt16, u32 UInt32, u64 UInt64, id UUID, s String, fs FixedString(33), a Array(UInt8), t Tuple(UInt16, UInt32), d Date, dt DateTime, dt64 DateTime64, dec128 Decimal128(3), lc LowCardinality(String)) engine = MergeTree PARTITION BY (i8, i16, i32, i64, u8, u16, u32, u64, id, s, fs, a, t, d, dt, dt64, dec128, lc) order by tuple();\n+create table tab (i8 Int8, i16 Int16, i32 Int32, i64 Int64, u8 UInt8, u16 UInt16, u32 UInt32, u64 UInt64, id UUID, s String, fs FixedString(33), a Array(UInt8), t Tuple(UInt16, UInt32), d Date, dt DateTime('Europe/Moscow'), dt64 DateTime64(3, 'Europe/Moscow'), dec128 Decimal128(3), lc LowCardinality(String)) engine = MergeTree PARTITION BY (i8, i16, i32, i64, u8, u16, u32, u64, id, s, fs, a, t, d, dt, dt64, dec128, lc) order by tuple();\n insert into tab values (-1, -1, -1, -1, -1, -1, -1, -1, '61f0c404-5cb3-11e7-907b-a6006ad3dba0', 'a', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', [1, 2, 3], (-1, -2), '2020-01-01', '2020-01-01 01:01:01', '2020-01-01 01:01:01', '123.456', 'a');\n -- Here we check that partition id did not change.\n -- Different result means Backward Incompatible Change. Old partitions will not be accepted by new server.\ndiff --git a/tests/queries/0_stateless/01905_to_json_string.sql b/tests/queries/0_stateless/01905_to_json_string.sql\nindex fe8a2407f3dd..80d9c2e26252 100644\n--- a/tests/queries/0_stateless/01905_to_json_string.sql\n+++ b/tests/queries/0_stateless/01905_to_json_string.sql\n@@ -1,10 +1,17 @@\n-drop table if exists t;\n-\n-create table t engine Memory as select * from generateRandom('a Array(Int8), b UInt32, c Nullable(String), d Decimal32(4), e Nullable(Enum16(\\'h\\' = 1, \\'w\\' = 5 , \\'o\\' = -200)), f Float64, g Tuple(Date, DateTime, DateTime64, UUID), h FixedString(2), i Array(Nullable(UUID))', 10, 5, 3) limit 2;\n+create temporary table t engine Memory as select * from generateRandom(\n+$$\n+    a Array(Int8),\n+    b UInt32,\n+    c Nullable(String),\n+    d Decimal32(4),\n+    e Nullable(Enum16('h' = 1, 'w' = 5 , 'o' = -200)),\n+    f Float64,\n+    g Tuple(Date, DateTime('Europe/Moscow'), DateTime64(3, 'Europe/Moscow'), UUID),\n+    h FixedString(2),\n+    i Array(Nullable(UUID))\n+$$, 10, 5, 3) limit 2;\n \n select * apply toJSONString from t;\n \n-drop table t;\n-\n set allow_experimental_map_type = 1;\n select toJSONString(map('1234', '5678'));\ndiff --git a/tests/queries/0_stateless/01926_date_date_time_supertype.sql b/tests/queries/0_stateless/01926_date_date_time_supertype.sql\nindex 559cd465ebb0..cce488a5cffd 100644\n--- a/tests/queries/0_stateless/01926_date_date_time_supertype.sql\n+++ b/tests/queries/0_stateless/01926_date_date_time_supertype.sql\n@@ -15,7 +15,7 @@ WITH toDate('2000-01-01') as a, toDateTime('2000-01-01', 'Europe/Moscow') as b\n SELECT if(value, b, a) as result, toTypeName(result)\n FROM predicate_table;\n \n-WITH toDateTime('2000-01-01') as a, toDateTime64('2000-01-01', 5, 'Europe/Moscow') as b\n+WITH toDateTime('2000-01-01', 'Europe/Moscow') as a, toDateTime64('2000-01-01', 5, 'Europe/Moscow') as b\n SELECT if(value, b, a) as result, toTypeName(result)\n FROM predicate_table;\n \ndiff --git a/tests/queries/0_stateless/data_parquet/alltypes_list.parquet.columns b/tests/queries/0_stateless/data_parquet/alltypes_list.parquet.columns\nindex b633ca86bbf3..794ee47d7577 100644\n--- a/tests/queries/0_stateless/data_parquet/alltypes_list.parquet.columns\n+++ b/tests/queries/0_stateless/data_parquet/alltypes_list.parquet.columns\n@@ -1,1 +1,1 @@\n-`a1` Array(Int8), `a2` Array(UInt8), `a3` Array(Int16), `a4` Array(UInt16), `a5` Array(Int32), `a6` Array(UInt32), `a7` Array(Int64), `a8` Array(UInt64), `a9` Array(String), `a10` Array(FixedString(4)), `a11` Array(Float32), `a12` Array(Float64), `a13` Array(Date), `a14` Array(Datetime), `a15` Array(Decimal(4, 2)), `a16` Array(Decimal(10, 2)), `a17` Array(Decimal(25, 2))\n+`a1` Array(Int8), `a2` Array(UInt8), `a3` Array(Int16), `a4` Array(UInt16), `a5` Array(Int32), `a6` Array(UInt32), `a7` Array(Int64), `a8` Array(UInt64), `a9` Array(String), `a10` Array(FixedString(4)), `a11` Array(Float32), `a12` Array(Float64), `a13` Array(Date), `a14` Array(Datetime('Europe/Moscow')), `a15` Array(Decimal(4, 2)), `a16` Array(Decimal(10, 2)), `a17` Array(Decimal(25, 2))\ndiff --git a/tests/queries/0_stateless/data_parquet/v0.7.1.column-metadata-handling.parquet.columns b/tests/queries/0_stateless/data_parquet/v0.7.1.column-metadata-handling.parquet.columns\nindex 3d08da2522cc..df35127ede80 100644\n--- a/tests/queries/0_stateless/data_parquet/v0.7.1.column-metadata-handling.parquet.columns\n+++ b/tests/queries/0_stateless/data_parquet/v0.7.1.column-metadata-handling.parquet.columns\n@@ -1,1 +1,1 @@\n-`a` Nullable(Int64), `b` Nullable(Float64), `c` Nullable(DateTime), `index` Nullable(String), `__index_level_1__` Nullable(DateTime)\n+`a` Nullable(Int64), `b` Nullable(Float64), `c` Nullable(DateTime('Europe/Moscow')), `index` Nullable(String), `__index_level_1__` Nullable(DateTime('Europe/Moscow'))\ndiff --git a/tests/queries/0_stateless/helpers/00900_parquet_create_table_columns.py b/tests/queries/0_stateless/helpers/00900_parquet_create_table_columns.py\nindex 1a41da8c8b4d..92606c9cb261 100755\n--- a/tests/queries/0_stateless/helpers/00900_parquet_create_table_columns.py\n+++ b/tests/queries/0_stateless/helpers/00900_parquet_create_table_columns.py\n@@ -4,8 +4,8 @@\n import sys\n \n TYPE_PARQUET_CONVERTED_TO_CLICKHOUSE = {\n-    \"TIMESTAMP_MICROS\": \"DateTime\",\n-    \"TIMESTAMP_MILLIS\": \"DateTime\",\n+    \"TIMESTAMP_MICROS\": \"DateTime('Europe/Moscow')\",\n+    \"TIMESTAMP_MILLIS\": \"DateTime('Europe/Moscow')\",\n     \"UTF8\": \"String\",\n }\n \ndiff --git a/tests/queries/1_stateful/00011_sorting.sql b/tests/queries/1_stateful/00011_sorting.sql\nindex 8c6ae4575663..381be7b7dd41 100644\n--- a/tests/queries/1_stateful/00011_sorting.sql\n+++ b/tests/queries/1_stateful/00011_sorting.sql\n@@ -1,1 +1,1 @@\n-SELECT EventTime FROM test.hits ORDER BY EventTime DESC LIMIT 10\n+SELECT EventTime::DateTime('Europe/Moscow') FROM test.hits ORDER BY EventTime DESC LIMIT 10\ndiff --git a/tests/queries/1_stateful/00012_sorting_distributed.sql b/tests/queries/1_stateful/00012_sorting_distributed.sql\nindex 51f249b3db8a..b0cdb8bd8a25 100644\n--- a/tests/queries/1_stateful/00012_sorting_distributed.sql\n+++ b/tests/queries/1_stateful/00012_sorting_distributed.sql\n@@ -1,1 +1,1 @@\n-SELECT EventTime FROM remote('127.0.0.{1,2}', test, hits) ORDER BY EventTime DESC LIMIT 10\n+SELECT EventTime::DateTime('Europe/Moscow') FROM remote('127.0.0.{1,2}', test, hits) ORDER BY EventTime DESC LIMIT 10\ndiff --git a/tests/queries/1_stateful/00066_sorting_distributed_many_replicas.sql b/tests/queries/1_stateful/00066_sorting_distributed_many_replicas.sql\nindex d06361331861..4bc563712c0d 100644\n--- a/tests/queries/1_stateful/00066_sorting_distributed_many_replicas.sql\n+++ b/tests/queries/1_stateful/00066_sorting_distributed_many_replicas.sql\n@@ -1,2 +1,2 @@\n SET max_parallel_replicas = 2;\n-SELECT EventTime FROM remote('127.0.0.{1|2}', test, hits) ORDER BY EventTime DESC LIMIT 10\n+SELECT EventTime::DateTime('Europe/Moscow') FROM remote('127.0.0.{1|2}', test, hits) ORDER BY EventTime DESC LIMIT 10\ndiff --git a/tests/queries/1_stateful/00071_merge_tree_optimize_aio.sql b/tests/queries/1_stateful/00071_merge_tree_optimize_aio.sql\nindex 1891cd635554..241f0f9b13b8 100644\n--- a/tests/queries/1_stateful/00071_merge_tree_optimize_aio.sql\n+++ b/tests/queries/1_stateful/00071_merge_tree_optimize_aio.sql\n@@ -1,6 +1,6 @@\n DROP TABLE IF EXISTS test.hits_snippet;\n \n-CREATE TABLE test.hits_snippet(EventTime DateTime,  EventDate Date,  CounterID UInt32,  UserID UInt64,  URL String,  Referer String) ENGINE = MergeTree(EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192);\n+CREATE TABLE test.hits_snippet(EventTime DateTime('Europe/Moscow'),  EventDate Date,  CounterID UInt32,  UserID UInt64,  URL String,  Referer String) ENGINE = MergeTree(EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192);\n \n SET min_insert_block_size_rows = 0, min_insert_block_size_bytes = 0;\n SET max_block_size = 4096;\ndiff --git a/tests/queries/1_stateful/00072_compare_date_and_string_index.sql b/tests/queries/1_stateful/00072_compare_date_and_string_index.sql\nindex 90f1c875acde..af5d932fecb9 100644\n--- a/tests/queries/1_stateful/00072_compare_date_and_string_index.sql\n+++ b/tests/queries/1_stateful/00072_compare_date_and_string_index.sql\n@@ -15,7 +15,7 @@ SELECT count() FROM test.hits WHERE EventDate IN (toDate('2014-03-18'), toDate('\n SELECT count() FROM test.hits WHERE EventDate = concat('2014-0', '3-18');\n \n DROP TABLE IF EXISTS test.hits_indexed_by_time;\n-CREATE TABLE test.hits_indexed_by_time (EventDate Date, EventTime DateTime) ENGINE = MergeTree(EventDate, EventTime, 8192);\n+CREATE TABLE test.hits_indexed_by_time (EventDate Date, EventTime DateTime('Europe/Moscow')) ENGINE = MergeTree ORDER BY (EventDate, EventTime);\n INSERT INTO test.hits_indexed_by_time SELECT EventDate, EventTime FROM test.hits;\n \n SELECT count() FROM test.hits_indexed_by_time WHERE EventTime = '2014-03-18 01:02:03';\n@@ -25,12 +25,12 @@ SELECT count() FROM test.hits_indexed_by_time WHERE EventTime <= '2014-03-18 01:\n SELECT count() FROM test.hits_indexed_by_time WHERE EventTime >= '2014-03-18 01:02:03';\n SELECT count() FROM test.hits_indexed_by_time WHERE EventTime IN ('2014-03-18 01:02:03', '2014-03-19 04:05:06');\n \n-SELECT count() FROM test.hits_indexed_by_time WHERE EventTime = toDateTime('2014-03-18 01:02:03');\n-SELECT count() FROM test.hits_indexed_by_time WHERE EventTime < toDateTime('2014-03-18 01:02:03');\n-SELECT count() FROM test.hits_indexed_by_time WHERE EventTime > toDateTime('2014-03-18 01:02:03');\n-SELECT count() FROM test.hits_indexed_by_time WHERE EventTime <= toDateTime('2014-03-18 01:02:03');\n-SELECT count() FROM test.hits_indexed_by_time WHERE EventTime >= toDateTime('2014-03-18 01:02:03');\n-SELECT count() FROM test.hits_indexed_by_time WHERE EventTime IN (toDateTime('2014-03-18 01:02:03'), toDateTime('2014-03-19 04:05:06'));\n+SELECT count() FROM test.hits_indexed_by_time WHERE EventTime = toDateTime('2014-03-18 01:02:03', 'Europe/Moscow');\n+SELECT count() FROM test.hits_indexed_by_time WHERE EventTime < toDateTime('2014-03-18 01:02:03', 'Europe/Moscow');\n+SELECT count() FROM test.hits_indexed_by_time WHERE EventTime > toDateTime('2014-03-18 01:02:03', 'Europe/Moscow');\n+SELECT count() FROM test.hits_indexed_by_time WHERE EventTime <= toDateTime('2014-03-18 01:02:03', 'Europe/Moscow');\n+SELECT count() FROM test.hits_indexed_by_time WHERE EventTime >= toDateTime('2014-03-18 01:02:03', 'Europe/Moscow');\n+SELECT count() FROM test.hits_indexed_by_time WHERE EventTime IN (toDateTime('2014-03-18 01:02:03', 'Europe/Moscow'), toDateTime('2014-03-19 04:05:06', 'Europe/Moscow'));\n \n SELECT count() FROM test.hits_indexed_by_time WHERE EventTime = concat('2014-03-18 ', '01:02:03');\n \ndiff --git a/tests/queries/1_stateful/00075_left_array_join.sql b/tests/queries/1_stateful/00075_left_array_join.sql\nindex 424276cf036d..52a48462b9db 100644\n--- a/tests/queries/1_stateful/00075_left_array_join.sql\n+++ b/tests/queries/1_stateful/00075_left_array_join.sql\n@@ -1,2 +1,2 @@\n-SELECT UserID, EventTime, pp.Key1, pp.Key2, ParsedParams.Key1 FROM test.hits ARRAY JOIN ParsedParams AS pp WHERE CounterID = 1704509 ORDER BY UserID, EventTime, pp.Key1, pp.Key2 LIMIT 100;\n-SELECT UserID, EventTime, pp.Key1, pp.Key2, ParsedParams.Key1 FROM test.hits LEFT ARRAY JOIN ParsedParams AS pp WHERE CounterID = 1704509 ORDER BY UserID, EventTime, pp.Key1, pp.Key2 LIMIT 100;\n+SELECT UserID, EventTime::DateTime('Europe/Moscow'), pp.Key1, pp.Key2, ParsedParams.Key1 FROM test.hits ARRAY JOIN ParsedParams AS pp WHERE CounterID = 1704509 ORDER BY UserID, EventTime, pp.Key1, pp.Key2 LIMIT 100;\n+SELECT UserID, EventTime::DateTime('Europe/Moscow'), pp.Key1, pp.Key2, ParsedParams.Key1 FROM test.hits LEFT ARRAY JOIN ParsedParams AS pp WHERE CounterID = 1704509 ORDER BY UserID, EventTime, pp.Key1, pp.Key2 LIMIT 100;\ndiff --git a/tests/queries/1_stateful/00091_prewhere_two_conditions.sql b/tests/queries/1_stateful/00091_prewhere_two_conditions.sql\nindex 201ff7880060..c5952be83b6c 100644\n--- a/tests/queries/1_stateful/00091_prewhere_two_conditions.sql\n+++ b/tests/queries/1_stateful/00091_prewhere_two_conditions.sql\n@@ -2,12 +2,12 @@ SET max_bytes_to_read = 600000000;\n \n SET optimize_move_to_prewhere = 1;\n \n-SELECT uniq(URL) FROM test.hits WHERE EventTime >= '2014-03-20 00:00:00' AND EventTime < '2014-03-21 00:00:00';\n-SELECT uniq(URL) FROM test.hits WHERE EventTime >= '2014-03-20 00:00:00' AND URL != '' AND EventTime < '2014-03-21 00:00:00';\n-SELECT uniq(*) FROM test.hits WHERE EventTime >= '2014-03-20 00:00:00' AND EventTime < '2014-03-21 00:00:00' AND EventDate = '2014-03-21';\n-WITH EventTime AS xyz SELECT uniq(*) FROM test.hits WHERE xyz >= '2014-03-20 00:00:00' AND xyz < '2014-03-21 00:00:00' AND EventDate = '2014-03-21';\n+SELECT uniq(URL) FROM test.hits WHERE toTimeZone(EventTime, 'Europe/Moscow') >= '2014-03-20 00:00:00' AND toTimeZone(EventTime, 'Europe/Moscow') < '2014-03-21 00:00:00';\n+SELECT uniq(URL) FROM test.hits WHERE toTimeZone(EventTime, 'Europe/Moscow') >= '2014-03-20 00:00:00' AND URL != '' AND toTimeZone(EventTime, 'Europe/Moscow') < '2014-03-21 00:00:00';\n+SELECT uniq(*) FROM test.hits WHERE toTimeZone(EventTime, 'Europe/Moscow') >= '2014-03-20 00:00:00' AND toTimeZone(EventTime, 'Europe/Moscow') < '2014-03-21 00:00:00' AND EventDate = '2014-03-21';\n+WITH toTimeZone(EventTime, 'Europe/Moscow') AS xyz SELECT uniq(*) FROM test.hits WHERE xyz >= '2014-03-20 00:00:00' AND xyz < '2014-03-21 00:00:00' AND EventDate = '2014-03-21';\n \n SET optimize_move_to_prewhere = 0;\n \n-SELECT uniq(URL) FROM test.hits WHERE EventTime >= '2014-03-20 00:00:00' AND EventTime < '2014-03-21 00:00:00'; -- { serverError 307 }\n-SELECT uniq(URL) FROM test.hits WHERE EventTime >= '2014-03-20 00:00:00' AND URL != '' AND EventTime < '2014-03-21 00:00:00'; -- { serverError 307 }\n+SELECT uniq(URL) FROM test.hits WHERE toTimeZone(EventTime, 'Europe/Moscow') >= '2014-03-20 00:00:00' AND toTimeZone(EventTime, 'Europe/Moscow') < '2014-03-21 00:00:00'; -- { serverError 307 }\n+SELECT uniq(URL) FROM test.hits WHERE toTimeZone(EventTime, 'Europe/Moscow') >= '2014-03-20 00:00:00' AND URL != '' AND toTimeZone(EventTime, 'Europe/Moscow') < '2014-03-21 00:00:00'; -- { serverError 307 }\ndiff --git a/tests/queries/1_stateful/00159_parallel_formatting_csv_and_friends.sh b/tests/queries/1_stateful/00159_parallel_formatting_csv_and_friends.sh\nindex dc14928afa64..a6b5620812d8 100755\n--- a/tests/queries/1_stateful/00159_parallel_formatting_csv_and_friends.sh\n+++ b/tests/queries/1_stateful/00159_parallel_formatting_csv_and_friends.sh\n@@ -10,10 +10,10 @@ for format in \"${FORMATS[@]}\"\n do\n     echo \"$format, false\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=false -q \\\n-    \"SELECT ClientEventTime as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n+    \"SELECT ClientEventTime::DateTime('Europe/Moscow') as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n \n     echo \"$format, true\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=true -q \\\n-    \"SELECT ClientEventTime as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n+    \"SELECT ClientEventTime::DateTime('Europe/Moscow') as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n done\n \ndiff --git a/tests/queries/1_stateful/00159_parallel_formatting_http.sh b/tests/queries/1_stateful/00159_parallel_formatting_http.sh\nindex a4e68de6a3f4..1dcae50812e5 100755\n--- a/tests/queries/1_stateful/00159_parallel_formatting_http.sh\n+++ b/tests/queries/1_stateful/00159_parallel_formatting_http.sh\n@@ -10,8 +10,8 @@ FORMATS=('TSV' 'CSV' 'JSONCompactEachRow')\n for format in \"${FORMATS[@]}\"\n do\n     echo \"$format, false\";\n-    ${CLICKHOUSE_CURL} -sS \"${CLICKHOUSE_URL}&query=SELECT+ClientEventTime+as+a,MobilePhoneModel+as+b,ClientIP6+as+c+FROM+test.hits+ORDER+BY+a,b,c+LIMIT+1000000+Format+$format&output_format_parallel_formatting=false\" -d' ' | md5sum\n+    ${CLICKHOUSE_CURL} -sS \"${CLICKHOUSE_URL}&query=SELECT+ClientEventTime::DateTime('Europe/Moscow')+as+a,MobilePhoneModel+as+b,ClientIP6+as+c+FROM+test.hits+ORDER+BY+a,b,c+LIMIT+1000000+Format+$format&output_format_parallel_formatting=false\" -d' ' | md5sum\n \n     echo \"$format, true\";\n-    ${CLICKHOUSE_CURL} -sS \"${CLICKHOUSE_URL}&query=SELECT+ClientEventTime+as+a,MobilePhoneModel+as+b,ClientIP6+as+c+FROM+test.hits+ORDER+BY+a,b,c+LIMIT+1000000+Format+$format&output_format_parallel_formatting=true\" -d' ' | md5sum\n+    ${CLICKHOUSE_CURL} -sS \"${CLICKHOUSE_URL}&query=SELECT+ClientEventTime::DateTime('Europe/Moscow')+as+a,MobilePhoneModel+as+b,ClientIP6+as+c+FROM+test.hits+ORDER+BY+a,b,c+LIMIT+1000000+Format+$format&output_format_parallel_formatting=true\" -d' ' | md5sum\n done\ndiff --git a/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.reference b/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.reference\nindex 96353f350ec1..6d663c330570 100644\n--- a/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.reference\n+++ b/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.reference\n@@ -7,6 +7,6 @@ ba1081a754a06ef6563840b2d8d4d327  -\n JSONCompactEachRow, true\n ba1081a754a06ef6563840b2d8d4d327  -\n JSONCompactStringsEachRowWithNamesAndTypes, false\n-902e53f621d5336aa7f702a5d6b64b42  -\n+31ded3cd9971b124450fb5a44a8bce63  -\n JSONCompactStringsEachRowWithNamesAndTypes, true\n-902e53f621d5336aa7f702a5d6b64b42  -\n+31ded3cd9971b124450fb5a44a8bce63  -\ndiff --git a/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.sh b/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.sh\nindex e02515d5c167..9f61b454d561 100755\n--- a/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.sh\n+++ b/tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.sh\n@@ -11,9 +11,9 @@ for format in \"${FORMATS[@]}\"\n do\n     echo \"$format, false\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=false -q \\\n-    \"SELECT ClientEventTime as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n+    \"SELECT ClientEventTime::DateTime('Europe/Moscow') as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n \n     echo \"$format, true\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=true -q \\\n-    \"SELECT ClientEventTime as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n+    \"SELECT ClientEventTime::DateTime('Europe/Moscow') as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n done\ndiff --git a/tests/queries/1_stateful/00159_parallel_formatting_tsv_and_friends.sh b/tests/queries/1_stateful/00159_parallel_formatting_tsv_and_friends.sh\nindex a81dfdc33b4a..02d083c04982 100755\n--- a/tests/queries/1_stateful/00159_parallel_formatting_tsv_and_friends.sh\n+++ b/tests/queries/1_stateful/00159_parallel_formatting_tsv_and_friends.sh\n@@ -11,9 +11,9 @@ for format in \"${FORMATS[@]}\"\n do\n     echo \"$format, false\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=false -q \\\n-    \"SELECT ClientEventTime as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n+    \"SELECT ClientEventTime::DateTime('Europe/Moscow') as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n \n     echo \"$format, true\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=true -q \\\n-    \"SELECT ClientEventTime as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n+    \"SELECT ClientEventTime::DateTime('Europe/Moscow') as a, MobilePhoneModel as b, ClientIP6 as c FROM test.hits ORDER BY a, b, c Format $format\" | md5sum\n done\ndiff --git a/tests/queries/1_stateful/00161_parallel_parsing_with_names.sh b/tests/queries/1_stateful/00161_parallel_parsing_with_names.sh\nindex ca9984900e19..777d95fa0af2 100755\n--- a/tests/queries/1_stateful/00161_parallel_parsing_with_names.sh\n+++ b/tests/queries/1_stateful/00161_parallel_parsing_with_names.sh\n@@ -10,23 +10,23 @@ $CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS parsing_with_names\"\n for format in \"${FORMATS[@]}\"\n do\n     # Columns are permuted\n-    $CLICKHOUSE_CLIENT -q \"CREATE TABLE parsing_with_names(c FixedString(16), a DateTime,  b String) ENGINE=Memory()\"\n-    \n+    $CLICKHOUSE_CLIENT -q \"CREATE TABLE parsing_with_names(c FixedString(16), a DateTime('Europe/Moscow'),  b String) ENGINE=Memory()\"\n+\n     echo \"$format, false\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=false -q \\\n-    \"SELECT URLRegions as d, ClientEventTime as a, MobilePhoneModel as b, ParamPrice as e, ClientIP6 as c FROM test.hits LIMIT 50000 Format $format\" | \\\n+    \"SELECT URLRegions as d, toTimeZone(ClientEventTime, 'Europe/Moscow') as a, MobilePhoneModel as b, ParamPrice as e, ClientIP6 as c FROM test.hits LIMIT 50000 Format $format\" | \\\n     $CLICKHOUSE_CLIENT --input_format_skip_unknown_fields=1 --input_format_parallel_parsing=false -q \"INSERT INTO parsing_with_names FORMAT $format\"\n \n     $CLICKHOUSE_CLIENT -q \"SELECT * FROM parsing_with_names;\" | md5sum\n     $CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS parsing_with_names\"\n \n-    \n-    $CLICKHOUSE_CLIENT -q \"CREATE TABLE parsing_with_names(c FixedString(16), a DateTime,  b String) ENGINE=Memory()\"\n+\n+    $CLICKHOUSE_CLIENT -q \"CREATE TABLE parsing_with_names(c FixedString(16), a DateTime('Europe/Moscow'),  b String) ENGINE=Memory()\"\n     echo \"$format, true\";\n     $CLICKHOUSE_CLIENT --output_format_parallel_formatting=false -q \\\n-    \"SELECT URLRegions as d, ClientEventTime as a, MobilePhoneModel as b, ParamPrice as e, ClientIP6 as c FROM test.hits LIMIT 50000 Format $format\" | \\\n+    \"SELECT URLRegions as d, toTimeZone(ClientEventTime, 'Europe/Moscow') as a, MobilePhoneModel as b, ParamPrice as e, ClientIP6 as c FROM test.hits LIMIT 50000 Format $format\" | \\\n     $CLICKHOUSE_CLIENT --input_format_skip_unknown_fields=1 --input_format_parallel_parsing=true -q \"INSERT INTO parsing_with_names FORMAT $format\"\n \n     $CLICKHOUSE_CLIENT -q \"SELECT * FROM parsing_with_names;\" | md5sum\n     $CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS parsing_with_names\"\n-done\n\\ No newline at end of file\n+done\ndiff --git a/tests/queries/1_stateful/00163_column_oriented_formats.sh b/tests/queries/1_stateful/00163_column_oriented_formats.sh\nindex 1363ccf3c00c..50ad20cbe923 100755\n--- a/tests/queries/1_stateful/00163_column_oriented_formats.sh\n+++ b/tests/queries/1_stateful/00163_column_oriented_formats.sh\n@@ -11,7 +11,7 @@ for format in \"${FORMATS[@]}\"\n do\n     echo $format\n     $CLICKHOUSE_CLIENT -q \"DROP TABLE IF EXISTS 00163_column_oriented SYNC\"\n-    $CLICKHOUSE_CLIENT -q \"CREATE TABLE 00163_column_oriented(ClientEventTime DateTime, MobilePhoneModel String, ClientIP6 FixedString(16)) ENGINE=File($format)\"\n+    $CLICKHOUSE_CLIENT -q \"CREATE TABLE 00163_column_oriented(ClientEventTime DateTime('Europe/Moscow'), MobilePhoneModel String, ClientIP6 FixedString(16)) ENGINE=File($format)\"\n     $CLICKHOUSE_CLIENT -q \"INSERT INTO 00163_column_oriented SELECT ClientEventTime, MobilePhoneModel, ClientIP6 FROM test.hits ORDER BY ClientEventTime, MobilePhoneModel, ClientIP6 LIMIT 100\"\n     $CLICKHOUSE_CLIENT -q \"SELECT ClientEventTime from 00163_column_oriented\" | md5sum\n     $CLICKHOUSE_CLIENT -q \"SELECT MobilePhoneModel from 00163_column_oriented\" | md5sum\ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex 5dc9f9a34c40..d55a198ec5da 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -519,6 +519,7 @@\n         \"01999_grant_with_replace\",\n         \"01902_table_function_merge_db_repr\",\n         \"01946_test_zstd_decompression_with_escape_sequence_at_the_end_of_buffer\",\n-        \"01946_test_wrong_host_name_access\"\n+        \"01946_test_wrong_host_name_access\",\n+        \"01213_alter_rename_with_default_zookeeper\" /// Warning: Removing leftovers from table.\n     ]\n }\n",
  "problem_statement": "GraphiteMergeTree: use time zone of the data type for rounding.\nCurrent behaviour:\r\n\r\nGraphiteMergeTree uses server time zone to perform rounding.\r\nFor example, if the server is run with Moscow time, rounding to midnight will be done in Moscow time.\r\n\r\nDesired behaviour:\r\n\r\nUse time zone specified in DateTime data type `DateTime('UTC')`.\nRun tests in CI with random time zone.\nTo make it less reliable.\r\n\r\nCC @alesapin \n",
  "hints_text": "\n",
  "created_at": "2020-09-28T12:12:41Z",
  "modified_files": [
    "src/Formats/ProtobufSerializer.cpp",
    "src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp",
    "src/Processors/Formats/Impl/ArrowColumnToCHColumn.h",
    "src/Processors/Formats/Impl/CHColumnToArrowColumn.cpp",
    "src/Processors/Formats/Impl/CHColumnToArrowColumn.h",
    "src/Processors/Formats/Impl/ParquetBlockOutputFormat.cpp",
    "src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.cpp",
    "src/Processors/Merges/Algorithms/GraphiteRollupSortedAlgorithm.h",
    "src/Storages/MergeTree/registerStorageMergeTree.cpp"
  ],
  "modified_test_files": [
    "docker/test/base/Dockerfile",
    "docker/test/stateful/run.sh",
    "docker/test/stateless/run.sh",
    "docker/test/stateless_unbundled/Dockerfile",
    "docker/test/unit/Dockerfile",
    "tests/queries/0_stateless/00538_datediff.sql",
    "tests/queries/0_stateless/00719_format_datetime_rand.sql",
    "tests/queries/0_stateless/00825_protobuf_format_persons.sh",
    "tests/queries/0_stateless/00900_long_parquet.reference",
    "tests/queries/0_stateless/00900_long_parquet.sh",
    "tests/queries/0_stateless/00921_datetime64_compatibility_long.python",
    "tests/queries/0_stateless/00921_datetime64_compatibility_long.reference",
    "tests/queries/0_stateless/00927_asof_join_other_types.sh",
    "tests/queries/0_stateless/01077_mutations_index_consistency.sh",
    "tests/queries/0_stateless/01087_storage_generate.sql",
    "tests/queries/0_stateless/01087_table_function_generate.reference",
    "tests/queries/0_stateless/01087_table_function_generate.sql",
    "tests/queries/0_stateless/01098_msgpack_format.sh",
    "tests/queries/0_stateless/01099_operators_date_and_timestamp.sql",
    "tests/queries/0_stateless/01186_conversion_to_nullable.sql",
    "tests/queries/0_stateless/01213_alter_rename_with_default_zookeeper.sql",
    "tests/queries/0_stateless/01236_graphite_mt.sql",
    "tests/queries/0_stateless/01269_toStartOfSecond.sql",
    "tests/queries/0_stateless/01273_arrow.reference",
    "tests/queries/0_stateless/01273_arrow_load.sh",
    "tests/queries/0_stateless/01273_arrow_stream.reference",
    "tests/queries/0_stateless/01277_toUnixTimestamp64.sql",
    "tests/queries/0_stateless/01307_orc_output_format.sh",
    "tests/queries/0_stateless/01379_with_fill_several_columns.sql",
    "tests/queries/0_stateless/01440_to_date_monotonicity.sql",
    "tests/queries/0_stateless/01508_partition_pruning_long.queries",
    "tests/queries/0_stateless/01508_partition_pruning_long.reference",
    "tests/queries/0_stateless/01508_partition_pruning_long.sh",
    "tests/queries/0_stateless/01656_sequence_next_node_long.sql",
    "tests/queries/0_stateless/01676_reinterpret_as.sql",
    "tests/queries/0_stateless/01691_DateTime64_clamp.reference",
    "tests/queries/0_stateless/01691_DateTime64_clamp.sql",
    "tests/queries/0_stateless/01692_DateTime64_from_DateTime.reference",
    "tests/queries/0_stateless/01692_DateTime64_from_DateTime.sql",
    "tests/queries/0_stateless/01702_toDateTime_from_string_clamping.reference",
    "tests/queries/0_stateless/01702_toDateTime_from_string_clamping.sql",
    "tests/queries/0_stateless/01732_more_consistent_datetime64_parsing.sql",
    "tests/queries/0_stateless/01734_datetime64_from_float.reference",
    "tests/queries/0_stateless/01734_datetime64_from_float.sql",
    "tests/queries/0_stateless/01867_support_datetime64_version_column.sql",
    "tests/queries/0_stateless/01891_partition_hash.sql",
    "tests/queries/0_stateless/01891_partition_hash_no_long_int.sql",
    "tests/queries/0_stateless/01905_to_json_string.sql",
    "tests/queries/0_stateless/01926_date_date_time_supertype.sql",
    "tests/queries/0_stateless/data_parquet/alltypes_list.parquet.columns",
    "tests/queries/0_stateless/data_parquet/v0.7.1.column-metadata-handling.parquet.columns",
    "tests/queries/0_stateless/helpers/00900_parquet_create_table_columns.py",
    "tests/queries/1_stateful/00011_sorting.sql",
    "tests/queries/1_stateful/00012_sorting_distributed.sql",
    "tests/queries/1_stateful/00066_sorting_distributed_many_replicas.sql",
    "tests/queries/1_stateful/00071_merge_tree_optimize_aio.sql",
    "tests/queries/1_stateful/00072_compare_date_and_string_index.sql",
    "tests/queries/1_stateful/00075_left_array_join.sql",
    "tests/queries/1_stateful/00091_prewhere_two_conditions.sql",
    "tests/queries/1_stateful/00159_parallel_formatting_csv_and_friends.sh",
    "tests/queries/1_stateful/00159_parallel_formatting_http.sh",
    "tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.reference",
    "tests/queries/1_stateful/00159_parallel_formatting_json_and_friends.sh",
    "tests/queries/1_stateful/00159_parallel_formatting_tsv_and_friends.sh",
    "tests/queries/1_stateful/00161_parallel_parsing_with_names.sh",
    "tests/queries/1_stateful/00163_column_oriented_formats.sh",
    "tests/queries/skip_list.json"
  ]
}