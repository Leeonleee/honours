{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 16390,
  "instance_id": "ClickHouse__ClickHouse-16390",
  "issue_numbers": [
    "520"
  ],
  "base_commit": "2c637d2b375a678f03ef004212e4aefd7de6a85b",
  "patch": "diff --git a/src/Common/CurrentMetrics.cpp b/src/Common/CurrentMetrics.cpp\nindex 3d6a2d6f99c5..c48e76e1d983 100644\n--- a/src/Common/CurrentMetrics.cpp\n+++ b/src/Common/CurrentMetrics.cpp\n@@ -9,7 +9,8 @@\n     M(ReplicatedFetch, \"Number of data parts being fetched from replica\") \\\n     M(ReplicatedSend, \"Number of data parts being sent to replicas\") \\\n     M(ReplicatedChecks, \"Number of data parts checking for consistency\") \\\n-    M(BackgroundPoolTask, \"Number of active tasks in BackgroundProcessingPool (merges, mutations, fetches, or replication queue bookkeeping)\") \\\n+    M(BackgroundPoolTask, \"Number of active tasks in BackgroundProcessingPool (merges, mutations, or replication queue bookkeeping)\") \\\n+    M(BackgroundFetchesPoolTask, \"Number of active tasks in BackgroundFetchesPool\") \\\n     M(BackgroundMovePoolTask, \"Number of active tasks in BackgroundProcessingPool for moves\") \\\n     M(BackgroundSchedulePoolTask, \"Number of active tasks in BackgroundSchedulePool. This pool is used for periodic ReplicatedMergeTree tasks, like cleaning old data parts, altering data parts, replica re-initialization, etc.\") \\\n     M(BackgroundBufferFlushSchedulePoolTask, \"Number of active tasks in BackgroundBufferFlushSchedulePool. This pool is used for periodic Buffer flushes\") \\\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 580756361b1e..e7141677f783 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -71,6 +71,7 @@ class IColumn;\n     M(UInt64, background_buffer_flush_schedule_pool_size, 16, \"Number of threads performing background flush for tables with Buffer engine. Only has meaning at server startup.\", 0) \\\n     M(UInt64, background_pool_size, 16, \"Number of threads performing background work for tables (for example, merging in merge tree). Only has meaning at server startup.\", 0) \\\n     M(UInt64, background_move_pool_size, 8, \"Number of threads performing background moves for tables. Only has meaning at server startup.\", 0) \\\n+    M(UInt64, background_fetches_pool_size, 3, \"Number of threads performing background fetches for replicated tables. Only has meaning at server startup.\", 0) \\\n     M(UInt64, background_schedule_pool_size, 16, \"Number of threads performing background tasks for replicated tables, dns cache updates. Only has meaning at server startup.\", 0) \\\n     M(UInt64, background_message_broker_schedule_pool_size, 16, \"Number of threads performing background tasks for message streaming. Only has meaning at server startup.\", 0) \\\n     M(UInt64, background_distributed_schedule_pool_size, 16, \"Number of threads performing background tasks for distributed sends. Only has meaning at server startup.\", 0) \\\ndiff --git a/src/Storages/MergeTree/BackgroundJobsExecutor.cpp b/src/Storages/MergeTree/BackgroundJobsExecutor.cpp\nindex 5aba208a86e2..a2eede8a0db1 100644\n--- a/src/Storages/MergeTree/BackgroundJobsExecutor.cpp\n+++ b/src/Storages/MergeTree/BackgroundJobsExecutor.cpp\n@@ -9,6 +9,7 @@ namespace CurrentMetrics\n {\n     extern const Metric BackgroundPoolTask;\n     extern const Metric BackgroundMovePoolTask;\n+    extern const Metric BackgroundFetchesPoolTask;\n }\n \n namespace DB\n@@ -183,7 +184,8 @@ BackgroundJobsExecutor::BackgroundJobsExecutor(\n     : IBackgroundJobExecutor(\n         global_context_,\n         global_context_.getBackgroundProcessingTaskSchedulingSettings(),\n-        {PoolConfig{PoolType::MERGE_MUTATE, global_context_.getSettingsRef().background_pool_size, CurrentMetrics::BackgroundPoolTask}})\n+        {PoolConfig{PoolType::MERGE_MUTATE, global_context_.getSettingsRef().background_pool_size, CurrentMetrics::BackgroundPoolTask},\n+         PoolConfig{PoolType::FETCH, global_context_.getSettingsRef().background_fetches_pool_size, CurrentMetrics::BackgroundFetchesPoolTask}})\n     , data(data_)\n {\n }\ndiff --git a/src/Storages/MergeTree/BackgroundJobsExecutor.h b/src/Storages/MergeTree/BackgroundJobsExecutor.h\nindex 537a6064c862..de81472012ec 100644\n--- a/src/Storages/MergeTree/BackgroundJobsExecutor.h\n+++ b/src/Storages/MergeTree/BackgroundJobsExecutor.h\n@@ -33,6 +33,7 @@ enum class PoolType\n {\n     MERGE_MUTATE,\n     MOVE,\n+    FETCH,\n };\n \n /// Result from background job providers. Function which will be executed in pool and pool type.\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\nindex 9263dab638a7..3b1c063df531 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n@@ -1039,6 +1039,16 @@ bool ReplicatedMergeTreeQueue::shouldExecuteLogEntry(\n         }\n     }\n \n+    /// Check that fetches pool is not overloaded\n+    if (entry.type == LogEntry::GET_PART)\n+    {\n+        if (!storage.canExecuteFetch(entry, out_postpone_reason))\n+        {\n+            LOG_TRACE(log, out_postpone_reason);\n+            return false;\n+        }\n+    }\n+\n     if (entry.type == LogEntry::MERGE_PARTS || entry.type == LogEntry::MUTATE_PART)\n     {\n         /** If any of the required parts are now fetched or in merge process, wait for the end of this operation.\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 93febf919c90..3652cd2dea02 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -77,6 +77,10 @@ namespace ProfileEvents\n     extern const Event NotCreatedLogEntryForMutation;\n }\n \n+namespace CurrentMetrics\n+{\n+    extern const Metric BackgroundFetchesPoolTask;\n+}\n \n namespace DB\n {\n@@ -206,6 +210,7 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     , part_check_thread(*this)\n     , restarting_thread(*this)\n     , allow_renaming(allow_renaming_)\n+    , replicated_fetches_pool_size(global_context.getSettingsRef().background_fetches_pool_size)\n {\n     queue_updating_task = global_context.getSchedulePool().createTask(\n         getStorageID().getFullTableName() + \" (StorageReplicatedMergeTree::queueUpdatingTask)\", [this]{ queueUpdatingTask(); });\n@@ -2604,10 +2609,37 @@ std::optional<JobAndPool> StorageReplicatedMergeTree::getDataProcessingJob()\n     if (!selected_entry)\n         return {};\n \n+    PoolType pool_type;\n+\n+    /// Depending on entry type execute in fetches (small) pool or big merge_mutate pool\n+    if (selected_entry->log_entry->type == LogEntry::GET_PART)\n+        pool_type = PoolType::FETCH;\n+    else\n+        pool_type = PoolType::MERGE_MUTATE;\n+\n     return JobAndPool{[this, selected_entry] () mutable\n     {\n         processQueueEntry(selected_entry);\n-    }, PoolType::MERGE_MUTATE};\n+    }, pool_type};\n+}\n+\n+\n+bool StorageReplicatedMergeTree::canExecuteFetch(const ReplicatedMergeTreeLogEntry & entry, String & disable_reason) const\n+{\n+    if (fetcher.blocker.isCancelled())\n+    {\n+        disable_reason = fmt::format(\"Not executing fetch of part {} because replicated fetches are cancelled now.\", entry.new_part_name);\n+        return false;\n+    }\n+\n+    size_t busy_threads_in_pool = CurrentMetrics::values[CurrentMetrics::BackgroundFetchesPoolTask].load(std::memory_order_relaxed);\n+    if (busy_threads_in_pool >= replicated_fetches_pool_size)\n+    {\n+        disable_reason = fmt::format(\"Not executing fetch of part {} because {} fetches already executing, max {}.\", entry.new_part_name, busy_threads_in_pool, replicated_fetches_pool_size);\n+        return false;\n+    }\n+\n+    return true;\n }\n \n bool StorageReplicatedMergeTree::partIsAssignedToBackgroundOperation(const DataPartPtr & part) const\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex e0eaacf5e71d..a8b44da7b689 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -207,8 +207,13 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n      */\n     static void dropReplica(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path, const String & replica, Poco::Logger * logger);\n \n+    /// Get job to execute in background pool (merge, mutate, drop range and so on)\n     std::optional<JobAndPool> getDataProcessingJob() override;\n \n+    /// Checks that fetches are not disabled with action blocker and pool for fetches\n+    /// is not overloaded\n+    bool canExecuteFetch(const ReplicatedMergeTreeLogEntry & entry, String & disable_reason) const;\n+\n private:\n \n     /// Get a sequential consistent view of current parts.\n@@ -319,6 +324,8 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n     /// Do not allow RENAME TABLE if zookeeper_path contains {database} or {table} macro\n     const bool allow_renaming;\n \n+    const size_t replicated_fetches_pool_size;\n+\n     template <class Func>\n     void foreachCommittedParts(const Func & func) const;\n \n",
  "test_patch": "diff --git a/tests/integration/test_limited_replicated_fetches/__init__.py b/tests/integration/test_limited_replicated_fetches/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_limited_replicated_fetches/test.py b/tests/integration/test_limited_replicated_fetches/test.py\nnew file mode 100644\nindex 000000000000..2091c65857eb\n--- /dev/null\n+++ b/tests/integration/test_limited_replicated_fetches/test.py\n@@ -0,0 +1,71 @@\n+#!/usr/bin/env python3\n+\n+import pytest\n+import time\n+from helpers.cluster import ClickHouseCluster\n+from helpers.network import PartitionManager\n+import random\n+import string\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('node1', with_zookeeper=True)\n+node2 = cluster.add_instance('node2', with_zookeeper=True)\n+\n+DEFAULT_MAX_THREADS_FOR_FETCH = 3\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def get_random_string(length):\n+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(length))\n+\n+\n+def test_limited_fetches(started_cluster):\n+    \"\"\"\n+        Test checks that that we utilize all available threads for fetches\n+    \"\"\"\n+    node1.query(\"CREATE TABLE t (key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/test/t', '1') ORDER BY tuple() PARTITION BY key\")\n+    node2.query(\"CREATE TABLE t (key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/test/t', '2') ORDER BY tuple() PARTITION BY key\")\n+\n+    with PartitionManager() as pm:\n+        node2.query(\"SYSTEM STOP FETCHES t\")\n+        node1.query(\"INSERT INTO t SELECT 1, '{}' FROM numbers(5000)\".format(get_random_string(104857)))\n+        node1.query(\"INSERT INTO t SELECT 2, '{}' FROM numbers(5000)\".format(get_random_string(104857)))\n+        node1.query(\"INSERT INTO t SELECT 3, '{}' FROM numbers(5000)\".format(get_random_string(104857)))\n+        node1.query(\"INSERT INTO t SELECT 4, '{}' FROM numbers(5000)\".format(get_random_string(104857)))\n+        node1.query(\"INSERT INTO t SELECT 5, '{}' FROM numbers(5000)\".format(get_random_string(104857)))\n+        node1.query(\"INSERT INTO t SELECT 6, '{}' FROM numbers(5000)\".format(get_random_string(104857)))\n+        pm.add_network_delay(node1, 80)\n+        node2.query(\"SYSTEM START FETCHES t\")\n+        fetches_result = []\n+        background_fetches_metric = []\n+        fetched_parts = set([])\n+        for _ in range(1000):\n+            result = node2.query(\"SELECT result_part_name FROM system.replicated_fetches\").strip().split()\n+            background_fetches_metric.append(int(node2.query(\"select value from system.metrics where metric = 'BackgroundFetchesPoolTask'\").strip()))\n+            if not result:\n+                if len(fetched_parts) == 6:\n+                    break\n+                time.sleep(0.1)\n+            else:\n+                for part in result:\n+                    fetched_parts.add(part)\n+                fetches_result.append(result)\n+                print(fetches_result[-1])\n+                print(background_fetches_metric[-1])\n+                time.sleep(0.1)\n+\n+    for concurrently_fetching_parts in fetches_result:\n+        if len(concurrently_fetching_parts) > DEFAULT_MAX_THREADS_FOR_FETCH:\n+            assert False, \"Found more than {} concurrently fetching parts: {}\".format(DEFAULT_MAX_THREADS_FOR_FETCH, ', '.join(concurrently_fetching_parts))\n+\n+    assert max([len(parts) for parts in fetches_result]) == 3, \"Strange, but we don't utilize max concurrent threads for fetches\"\n+    assert(max(background_fetches_metric)) == 3, \"Just checking metric consistent with table\"\ndiff --git a/tests/queries/0_stateless/01459_manual_write_to_replicas.sh b/tests/queries/0_stateless/01459_manual_write_to_replicas.sh\nindex c402e19c3dc9..1cf0ed56bc5e 100755\n--- a/tests/queries/0_stateless/01459_manual_write_to_replicas.sh\n+++ b/tests/queries/0_stateless/01459_manual_write_to_replicas.sh\n@@ -16,7 +16,9 @@ done\n \n function thread {\n     for x in {0..99}; do\n-        $CLICKHOUSE_CLIENT --query \"INSERT INTO r$1 SELECT $x % $NUM_REPLICAS = $1 ? $x - 1 : $x\"  # Replace some records as duplicates so they will be written by other replicas\n+        # sometimes we can try to commit obsolete part if fetches will be quite fast,\n+        # so supress warning messages like \"Tried to commit obsolete part ... covered by ...\"\n+        $CLICKHOUSE_CLIENT --query \"INSERT INTO r$1 SELECT $x % $NUM_REPLICAS = $1 ? $x - 1 : $x\" 2>/dev/null  # Replace some records as duplicates so they will be written by other replicas\n     done\n }\n \n",
  "problem_statement": "Recovery after data loss doesn't work with more than 2 replicas per shard\nThis is a bit vague report, but we had an issue when one node was put back to production after a disk failure and [couldn't recover from peer replicas][1]. What happened was we put the node back to production, it started receiving parts, but the connection always broke before the part transfer was finished. There was a lot of network and disk i/o but nothing finished successfully, so the disk usage wasn't going up and the instance never recovered. Another strange thing is, when we stopped the faulty instance, the peer replicas continued to read and transmit parts (despite the receiver was down), so I had to restart the peer replicas as well to stop the disk i/o.\r\n\r\nWe did two things to mitigate the issue:\r\n* Switch from xfs to ext4, the xfs nodes experienced strange performance degradation under load where overall load went up 8x and disk i/o almost stalled. It was also impossible to recover from replicas on xfs due to connection breaks, while it progressed from nodes on ext4.\r\n* Change the affected node configuration to only two replicas per shard, so it is forced to recover from only one replica. This works even with xfs nodes, but makes recovery tedious.\r\n\r\nI suspect the core of the issue is that receiving replica cannot moderate senders when it's not able to keep up which leads to connection breaks, there is currently no way to throttle throughput or have any sort of backpressure - the sender replicas just keep sending. I'm not sure how are you dealing with this, but this also affects read query performance while the replication is in progress. The secondary issue is that xfs is not really recommended as a filesystem, we're still investigating what causes this issue.\r\n\r\n[1]: https://groups.google.com/forum/#!topic/clickhouse/uq9891JyV7M\n",
  "hints_text": "> I suspect the core of the issue is that receiving replica cannot moderate senders when it's not able to keep up which leads to connection breaks, there is currently no way to throttle throughput or have any sort of backpressure - the sender replicas just keep sending.\r\n\r\nYes, this is the cause.\r\nWe currently have a task to introduce following limits:\r\n- max number of concurrent fetches (on fetching replica), per table;\r\n- max number of concurrent fetches (on fetching replica), for all server;\r\n- max number of concurrent sends (on source replica), per table;\r\n- max number of concurrent sends (on source replica), for all server;\nWe have experienced the following cases:\r\n\r\n1. When replica is repaired, data is downloaded by too large number of parallel connections (equals to background_pool_size = 16 by default). As even one connection could utilize the network, each connection only downloaded data slower while total throughput is the same. It also leads to more random IO on source replica, raising the possibility of connection loss.\r\n\r\n2. Fetches from source replica during repair could saturate all background_pool_size. Background pool is also used for background merges. So, during repair of one table, other tables on same server could become too unmerged.\r\n\r\n3. One of our users installed a cluster with 10 replicas. When data is inserted to one replica, all 9 another replicas instantly request that data from that single replica, that leads to IO and network overload.\r\n\r\n4. Another user have defined external dictionaries with cache layout and lifetime of 4 hours. Everything was fine, but when we have to repair replica, it could not repair due to connection loss every 4 hours. This was due to high number of created network connections when updating data for external dictionary. Solution was to always specify interval for randomization of lifetime (min and max).\nalready in master:\r\n```\r\n<merge_tree>                                                                          \r\n  <replicated_max_parallel_sends_for_table>2</replicated_max_parallel_sends_for_table>\r\n  <replicated_max_parallel_sends>4</replicated_max_parallel_sends>                    \r\n\r\n  <replicated_max_parallel_fetches_for_table>2</replicated_max_parallel_fetches_for_table>\r\n  <replicated_max_parallel_fetches>4</replicated_max_parallel_fetches>                    \r\n</merge_tree>                                                                             \r\n```\nMarek, please, share your observations if will be any.\nWill do on the next replication, thanks!\r\nIs the new merge tree settings default or should I set it explicitly?\nNew settings are default.\nWe have tested new settings and convinced, that they are still not production ready.\r\nThe settings was disabled by default in master.\nNot sure if it's the right issue to complain, but during lengthy recovery all workers are busy replicating and merge queue never decreases.\r\n\r\n![image](https://user-images.githubusercontent.com/89186/34368341-0e478a0c-ea68-11e7-8098-267e5c000300.png)\r\n\nIt depends... Does the queue disappear after successful recovery? Does the recovery goes slower than expected? Do you see errors in log about failed fetches?\r\n\r\nThis issue is about using excessive number of network connections during replica recovery,\r\nthat can lead to slower recovery and periodic failures in fetching parts during recovery.\nIt definitely disappears at the end, but it takes hours to take to the end. My main issue is that read queries run a lot slower because there's no throttling. We see writes at 1.8GB/s when reads from 2 replicas saturate disks at ~900MB/s.\r\n\r\nShould I file an issue to throttle recovery bandwidth (hopefully with zookeeper override support)?\r\n\r\nWhile I am at it, should I also file an issue for JBOD support in some way? We have 12 x 10TB disks in RAID0 as of now and it will be even more painful to replicate full 100TB datasets even across 50G network (disks will saturate a lot sooner anyway).",
  "created_at": "2020-10-26T11:35:30Z",
  "modified_files": [
    "src/Common/CurrentMetrics.cpp",
    "src/Core/Settings.h",
    "src/Storages/MergeTree/BackgroundJobsExecutor.cpp",
    "src/Storages/MergeTree/BackgroundJobsExecutor.h",
    "src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_limited_replicated_fetches/test.py",
    "tests/queries/0_stateless/01459_manual_write_to_replicas.sh"
  ]
}