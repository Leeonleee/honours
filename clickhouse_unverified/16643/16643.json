{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 16643,
  "instance_id": "ClickHouse__ClickHouse-16643",
  "issue_numbers": [
    "1463"
  ],
  "base_commit": "2fae1c3c315e59a1de91096dd528e4bf577fd0f0",
  "patch": "diff --git a/src/AggregateFunctions/ReservoirSampler.h b/src/AggregateFunctions/ReservoirSampler.h\nindex 2ff5ab863af4..594ff689aa5e 100644\n--- a/src/AggregateFunctions/ReservoirSampler.h\n+++ b/src/AggregateFunctions/ReservoirSampler.h\n@@ -158,12 +158,25 @@ class ReservoirSampler\n         }\n         else\n         {\n-            randomShuffle(samples);\n+            /// Replace every element in our reservoir to the b's reservoir\n+            /// with the probability of b.total_values / (a.total_values + b.total_values)\n+            /// Do it more roughly than true random sampling to save performance.\n+\n             total_values += b.total_values;\n-            for (size_t i = 0; i < sample_count; ++i)\n+\n+            /// Will replace every frequency'th element in a to element from b.\n+            double frequency = static_cast<double>(total_values) / b.total_values;\n+\n+            /// When frequency is too low, replace just one random element with the corresponding probability.\n+            if (frequency * 2 >= sample_count)\n+            {\n+                UInt64 rnd = genRandom(frequency);\n+                if (rnd < sample_count)\n+                    samples[rnd] = b.samples[rnd];\n+            }\n+            else\n             {\n-                UInt64 rnd = genRandom(total_values);\n-                if (rnd < b.total_values)\n+                for (double i = 0; i < sample_count; i += frequency)\n                     samples[i] = b.samples[i];\n             }\n         }\n@@ -222,15 +235,6 @@ class ReservoirSampler\n             return (static_cast<UInt64>(rng()) * (static_cast<UInt64>(rng.max()) + 1ULL) + static_cast<UInt64>(rng())) % lim;\n     }\n \n-    void randomShuffle(Array & v)\n-    {\n-        for (size_t i = 1; i < v.size(); ++i)\n-        {\n-            size_t j = genRandom(i + 1);\n-            std::swap(v[i], v[j]);\n-        }\n-    }\n-\n     void sortIfNeeded()\n     {\n         if (sorted)\n",
  "test_patch": "diff --git a/tests/performance/quantile_merge.xml b/tests/performance/quantile_merge.xml\nnew file mode 100644\nindex 000000000000..7f4d85a254c3\n--- /dev/null\n+++ b/tests/performance/quantile_merge.xml\n@@ -0,0 +1,3 @@\n+<test>\n+    <query>SELECT quantileMerge(arrayJoin(arrayMap(x -> state, range(1000000)))) FROM (SELECT quantileState(rand()) AS state FROM numbers(10000))</query>\n+</test>\n",
  "problem_statement": "Performance of quantilesMerge\nHello.  \r\n\r\nIn investigating ClickHouse performance, I noticed that that `quantilesMerge` function was much slower than the merge of other aggregation types.   On my hardware, It runs at approximately 1.5 seconds per million rows.\r\n\r\nBelow find a scan of 38 million rows with both the `quantilesMerge` function (57 seconds elapsed) and the `avgMerge` function (<1sec elapsed).  A sampling of other merge operators shows `quantilesMerge` is the outlier.\r\n\r\nLooking at the implementation of `ReservoirSampler::merge`, it does do a lot of work, but since scanning over the raw data (just 250 million rows) with `quantile()` takes just 1 second perhaps there's the possibility of optimization here.\r\n\r\nThanks,\r\nDarren.\r\n\r\n```\r\nClickHouse client version 1.1.54023.\r\nConnecting to localhost:9000.\r\nConnected to ClickHouse server version 1.1.54023.\r\n\r\n:) select clientMarketId, quantilesMerge(0.5)(quants) as p50 from test_hr group by clientMarketId;\r\n\r\nSELECT \r\n    clientMarketId, \r\n    quantilesMerge(0.5)(quants) AS p50\r\nFROM test_hr \r\nGROUP BY clientMarketId\r\n\r\n\u2198 Progress: 65.54 thousand rows, 8.97 MB (54.35 thousand rows/s., 7.44 MB/s.) \u258f                                                                                                 0%\u250c\u2500clientMarketId\u2500\u252c\u2500p50\u2500\u2500\u2500\u2500\u2510\r\n\u2502              0 \u2502 [125]  \u2502\r\n\u2502              1 \u2502 [87.5] \u2502\r\n\u2502              2 \u2502 [139]  \u2502\r\n\u2502              3 \u2502 [62]   \u2502\r\n\u2502              4 \u2502 [125]  \u2502\r\n\u2502              5 \u2502 [206]  \u2502\r\n\u2502              6 \u2502 [164]  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u2193 Progress: 38.84 million rows, 5.40 GB (678.21 thousand rows/s., 94.38 MB/s.) 7 rows in set. Elapsed: 57.262 sec. Processed 38.84 million rows, 5.40 GB (678.20 thousand rows/s., 94.38 MB/s.) \r\n\r\n:) select clientMarketId, avgMerge(avg) as p50 from test_hr group by clientMarketId;\r\n\r\nSELECT \r\n    clientMarketId, \r\n    avgMerge(avg) AS av\r\nFROM test_hr \r\nGROUP BY clientMarketId\r\n\r\n\u250c\u2500clientMarketId\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500av\u2500\u2510\r\n\u2502              0 \u2502  733.3191436467522 \u2502\r\n\u2502              1 \u2502 1079.1231858467577 \u2502\r\n\u2502              2 \u2502  890.4164012859483 \u2502\r\n\u2502              3 \u2502  997.8657197406471 \u2502\r\n\u2502              4 \u2502 1048.2662699646294 \u2502\r\n\u2502              5 \u2502  460.5996271142637 \u2502\r\n\u2502              6 \u2502  691.1883294646486 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u2199 Progress: 38.84 million rows, 1.58 GB (579.01 million rows/s., 23.56 GB/s.) \r\n7 rows in set. Elapsed: 0.067 sec. Processed 38.84 million rows, 1.58 GB (578.12 million rows/s., 23.53 GB/s.) \r\n```\n",
  "hints_text": "note that `quantilesTDigestMerge` has great performance and can be a substitute for our use case.\r\n\r\nThank you!\n`quantilesTDigestMerge` is probably fast, but `quantilesTDigest` is not - https://github.com/ClickHouse/ClickHouse/issues/2668\r\n\r\n`quantiles` is fast, but `quantilesMerge` is slow - 2 seconds to merge 30k rows.\r\n\r\n`quantilesTiming` is the only the working solution, but only for values in range [0, 30000].\r\n\r\n:(\nHow to reproduce:\r\n```\r\nSELECT quantileMerge(arrayJoin(arrayMap(x -> state, range(10000)))) FROM (SELECT quantileState(rand()) AS state FROM numbers(10000))\r\n```",
  "created_at": "2020-11-03T20:29:28Z"
}