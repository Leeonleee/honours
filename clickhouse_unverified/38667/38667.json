{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 38667,
  "instance_id": "ClickHouse__ClickHouse-38667",
  "issue_numbers": [
    "37216"
  ],
  "base_commit": "3007507a8b991ae0b8f3712b58e8670b3a136c9a",
  "patch": "diff --git a/src/Common/FST.cpp b/src/Common/FST.cpp\nnew file mode 100644\nindex 000000000000..b409d37919ff\n--- /dev/null\n+++ b/src/Common/FST.cpp\n@@ -0,0 +1,480 @@\n+#include \"FST.h\"\n+#include <algorithm>\n+#include <cassert>\n+#include <iostream>\n+#include <memory>\n+#include <vector>\n+#include <Common/Exception.h>\n+#include <city.h>\n+\n+/// \"paper\" in the comments in this file refers to:\n+/// [Direct Construction of Minimal Acyclic Subsequential Transduers] by Stoyan Mihov and Denis Maurel, University of Tours, France\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int BAD_ARGUMENTS;\n+};\n+\n+namespace FST\n+{\n+\n+UInt64 Arc::serialize(WriteBuffer& write_buffer) const\n+{\n+    UInt64 written_bytes = 0;\n+    bool has_output = output != 0;\n+\n+    /// First UInt64 is target_index << 1 + has_output\n+    assert(target != nullptr);\n+    UInt64 first = ((target->state_index) << 1) + has_output;\n+    writeVarUInt(first, write_buffer);\n+    written_bytes += getLengthOfVarUInt(first);\n+\n+    /// Second UInt64 is output (optional based on whether has_output is not zero)\n+    if (has_output)\n+    {\n+        writeVarUInt(output, write_buffer);\n+        written_bytes += getLengthOfVarUInt(output);\n+    }\n+    return written_bytes;\n+}\n+\n+bool operator==(const Arc & arc1, const Arc & arc2)\n+{\n+    assert(arc1.target != nullptr && arc2.target != nullptr);\n+    return (arc1.output == arc2.output && arc1.target->id == arc2.target->id);\n+}\n+\n+void LabelsAsBitmap::addLabel(char label)\n+{\n+    UInt8 index = label;\n+    UInt256 bit_label = 1;\n+    bit_label <<= index;\n+\n+    data |= bit_label;\n+}\n+\n+UInt64 LabelsAsBitmap::getIndex(char label) const\n+{\n+    UInt64 bit_count = 0;\n+\n+    UInt8 index = label;\n+    int which_int64 = 0;\n+    while (true)\n+    {\n+        if (index < 64)\n+        {\n+            UInt64 mask = index == 63 ? (-1) : (1ULL << (index + 1)) - 1;\n+\n+            bit_count += std::popcount(mask & data.items[which_int64]);\n+            break;\n+        }\n+        index -= 64;\n+        bit_count += std::popcount(data.items[which_int64]);\n+\n+        which_int64++;\n+    }\n+    return bit_count;\n+}\n+\n+UInt64 LabelsAsBitmap::serialize(WriteBuffer& write_buffer)\n+{\n+    writeVarUInt(data.items[0], write_buffer);\n+    writeVarUInt(data.items[1], write_buffer);\n+    writeVarUInt(data.items[2], write_buffer);\n+    writeVarUInt(data.items[3], write_buffer);\n+\n+    return getLengthOfVarUInt(data.items[0])\n+        + getLengthOfVarUInt(data.items[1])\n+        + getLengthOfVarUInt(data.items[2])\n+        + getLengthOfVarUInt(data.items[3]);\n+}\n+\n+bool LabelsAsBitmap::hasLabel(char label) const\n+{\n+    UInt8 index = label;\n+    UInt256 bit_label = 1;\n+    bit_label <<= index;\n+\n+    return ((data & bit_label) != 0);\n+}\n+\n+Arc* State::getArc(char label) const\n+{\n+    auto it = arcs.find(label);\n+    if (it == arcs.cend())\n+        return nullptr;\n+\n+    return const_cast<Arc *>(&it->second);\n+}\n+\n+void State::addArc(char label, Output output, StatePtr target)\n+{\n+    arcs[label] = Arc(output, target);\n+}\n+\n+void State::clear()\n+{\n+    id = 0;\n+    state_index = 0;\n+    flag = 0;\n+\n+    arcs.clear();\n+}\n+\n+UInt64 State::hash() const\n+{\n+    std::vector<char> values;\n+    values.reserve(arcs.size() * (sizeof(Output) + sizeof(UInt64) + 1));\n+    for (const auto & [label, arc] : arcs)\n+    {\n+        values.push_back(label);\n+        const auto * ptr = reinterpret_cast<const char*>(&arc.output);\n+        std::copy(ptr, ptr + sizeof(Output), std::back_inserter(values));\n+\n+        ptr = reinterpret_cast<const char*>(&arc.target->id);\n+        std::copy(ptr, ptr + sizeof(UInt64), std::back_inserter(values));\n+    }\n+\n+    return CityHash_v1_0_2::CityHash64(values.data(), values.size());\n+}\n+\n+bool operator== (const State & state1, const State & state2)\n+{\n+    if (state1.arcs.size() != state2.arcs.size())\n+        return false;\n+\n+    for (const auto & [label, arc] : state1.arcs)\n+    {\n+        const auto it = state2.arcs.find(label);\n+        if (it == state2.arcs.cend())\n+            return false;\n+\n+        if (it->second != arc)\n+            return false;\n+    }\n+    return true;\n+}\n+\n+UInt64 State::serialize(WriteBuffer& write_buffer)\n+{\n+    UInt64 written_bytes = 0;\n+\n+    /// Serialize flag\n+    write_buffer.write(flag);\n+    written_bytes += 1;\n+\n+    if (getEncodingMethod() == EncodingMethod::Sequential)\n+    {\n+        /// Serialize all labels\n+        std::vector<char> labels;\n+        labels.reserve(arcs.size());\n+\n+        for (auto& [label, state] : arcs)\n+        {\n+            labels.push_back(label);\n+        }\n+\n+        UInt8 label_size = labels.size();\n+        write_buffer.write(label_size);\n+        written_bytes += 1;\n+\n+        write_buffer.write(labels.data(), labels.size());\n+        written_bytes += labels.size();\n+\n+        /// Serialize all arcs\n+        for (char label : labels)\n+        {\n+            Arc* arc = getArc(label);\n+            assert(arc != nullptr);\n+            written_bytes += arc->serialize(write_buffer);\n+        }\n+    }\n+    else\n+    {\n+        /// Serialize bitmap\n+        LabelsAsBitmap bmp;\n+        for (auto & [label, state] : arcs)\n+        {\n+            bmp.addLabel(label);\n+        }\n+        written_bytes += bmp.serialize(write_buffer);\n+\n+        /// Serialize all arcs\n+        for (auto & [label, state] : arcs)\n+        {\n+            Arc* arc = getArc(label);\n+            assert(arc != nullptr);\n+            written_bytes += arc->serialize(write_buffer);\n+        }\n+    }\n+\n+    return written_bytes;\n+}\n+\n+FSTBuilder::FSTBuilder(WriteBuffer& write_buffer_) : write_buffer(write_buffer_)\n+{\n+    for (auto & temp_state : temp_states)\n+    {\n+        temp_state = std::make_shared<State>();\n+    }\n+}\n+\n+/// See FindMinimized in the paper pseudo code l11-l21.\n+StatePtr FSTBuilder::findMinimized(const State & state, bool & found)\n+{\n+    found = false;\n+    auto hash = state.hash();\n+\n+    /// MEMBER: in the paper pseudo code l15\n+    auto it = minimized_states.find(hash);\n+\n+    if (it != minimized_states.cend() && *it->second == state)\n+    {\n+        found = true;\n+        return it->second;\n+    }\n+\n+    /// COPY_STATE: in the paper pseudo code l17\n+    StatePtr p = std::make_shared<State>(state);\n+\n+    /// INSERT: in the paper pseudo code l18\n+    minimized_states[hash] = p;\n+    return p;\n+}\n+\n+/// See the paper pseudo code l33-34.\n+size_t FSTBuilder::getCommonPrefixLength(const String & word1, const String & word2)\n+{\n+    size_t i = 0;\n+    while (i < word1.size() && i < word2.size() && word1[i] == word2[i])\n+        i++;\n+    return i;\n+}\n+\n+/// See the paper pseudo code l33-39 and l70-72(when down_to is 0).\n+void FSTBuilder::minimizePreviousWordSuffix(Int64 down_to)\n+{\n+    for (Int64 i = static_cast<Int64>(previous_word.size()); i >= down_to; --i)\n+    {\n+        bool found = false;\n+        auto minimized_state = findMinimized(*temp_states[i], found);\n+\n+        if (i != 0)\n+        {\n+            Output output = 0;\n+            Arc* arc = temp_states[i - 1]->getArc(previous_word[i - 1]);\n+            if (arc)\n+                output = arc->output;\n+\n+            /// SET_TRANSITION\n+            temp_states[i - 1]->addArc(previous_word[i - 1], output, minimized_state);\n+        }\n+        if (minimized_state->id == 0)\n+            minimized_state->id = next_id++;\n+\n+        if (i > 0 && temp_states[i - 1]->id == 0)\n+            temp_states[i - 1]->id = next_id++;\n+\n+        if (!found)\n+        {\n+            minimized_state->state_index = previous_state_index;\n+\n+            previous_written_bytes = minimized_state->serialize(write_buffer);\n+            previous_state_index += previous_written_bytes;\n+        }\n+    }\n+}\n+\n+void FSTBuilder::add(const std::string & current_word, Output current_output)\n+{\n+    /// We assume word size is no greater than MAX_TERM_LENGTH(256).\n+    /// FSTs without word size limitation would be inefficient and easy to cause memory bloat\n+    /// Note that when using \"split\" tokenizer, if a granule has tokens which are longer than\n+    /// MAX_TERM_LENGTH, the granule cannot be dropped and will be fully-scanned. It doesn't affect \"ngram\" tokenizers.\n+    /// Another limitation is that if the query string has tokens which exceed this length\n+    /// it will fallback to default searching when using \"split\" tokenizers.\n+    auto current_word_len = current_word.size();\n+\n+    if (current_word_len > MAX_TERM_LENGTH)\n+        throw DB::Exception(DB::ErrorCodes::BAD_ARGUMENTS, \"Too long term ({}) passed to FST builder.\", current_word_len);\n+\n+    size_t prefix_length_plus1 = getCommonPrefixLength(current_word, previous_word) + 1;\n+\n+    minimizePreviousWordSuffix(prefix_length_plus1);\n+\n+    /// Initialize the tail state, see paper pseudo code l39-43\n+    for (size_t i = prefix_length_plus1; i <= current_word.size(); ++i)\n+    {\n+        /// CLEAR_STATE: l41\n+        temp_states[i]->clear();\n+\n+        /// SET_TRANSITION: l42\n+        temp_states[i - 1]->addArc(current_word[i - 1], 0, temp_states[i]);\n+    }\n+\n+    /// We assume the current word is different with previous word\n+    /// See paper pseudo code l44-47\n+    temp_states[current_word_len]->setFinal(true);\n+\n+    /// Adjust outputs on the arcs\n+    /// See paper pseudo code l48-63\n+    for (size_t i = 1; i <= prefix_length_plus1 - 1; ++i)\n+    {\n+        Arc * arc_ptr = temp_states[i - 1]->getArc(current_word[i - 1]);\n+        assert(arc_ptr != nullptr);\n+\n+        Output common_prefix = std::min(arc_ptr->output, current_output);\n+        Output word_suffix = arc_ptr->output - common_prefix;\n+        arc_ptr->output = common_prefix;\n+\n+        /// For each arc, adjust its output\n+        if (word_suffix != 0)\n+        {\n+            for (auto & [label, arc] : temp_states[i]->arcs)\n+            {\n+                arc.output += word_suffix;\n+            }\n+        }\n+        /// Reduce current_output\n+        current_output -= common_prefix;\n+    }\n+\n+    /// Set last temp state's output\n+    /// paper pseudo code l66-67 (assuming CurrentWord != PreviousWorld)\n+    Arc * arc = temp_states[prefix_length_plus1 - 1]->getArc(current_word[prefix_length_plus1 - 1]);\n+    assert(arc != nullptr);\n+    arc->output = current_output;\n+\n+    previous_word = current_word;\n+}\n+\n+UInt64 FSTBuilder::build()\n+{\n+    minimizePreviousWordSuffix(0);\n+\n+    /// Save initial state index\n+\n+    previous_state_index -= previous_written_bytes;\n+    UInt8 length = getLengthOfVarUInt(previous_state_index);\n+    writeVarUInt(previous_state_index, write_buffer);\n+    write_buffer.write(length);\n+\n+    return previous_state_index + previous_written_bytes + length + 1;\n+}\n+\n+FiniteStateTransducer::FiniteStateTransducer(std::vector<UInt8> data_) : data(std::move(data_))\n+{\n+}\n+\n+void FiniteStateTransducer::clear()\n+{\n+    data.clear();\n+}\n+\n+std::pair<UInt64, bool> FiniteStateTransducer::getOutput(const String & term)\n+{\n+    std::pair<UInt64, bool> result{ 0, false };\n+\n+    /// Read index of initial state\n+    ReadBufferFromMemory read_buffer(data.data(), data.size());\n+    read_buffer.seek(data.size()-1, SEEK_SET);\n+\n+    UInt8 length{ 0 };\n+    read_buffer.readStrict(reinterpret_cast<char&>(length));\n+\n+    /// FST contains no terms\n+    if (length == 0)\n+        return { 0, false };\n+\n+    read_buffer.seek(data.size() - 1 - length, SEEK_SET);\n+    UInt64 state_index{ 0 };\n+    readVarUInt(state_index, read_buffer);\n+\n+    for (size_t i = 0; i <= term.size(); ++i)\n+    {\n+        UInt64 arc_output{ 0 };\n+\n+        /// Read flag\n+        State temp_state;\n+\n+        read_buffer.seek(state_index, SEEK_SET);\n+        temp_state.readFlag(read_buffer);\n+        if (i == term.size())\n+        {\n+            result.second = temp_state.isFinal();\n+            break;\n+        }\n+\n+        UInt8 label = term[i];\n+        if (temp_state.getEncodingMethod() == State::EncodingMethod::Sequential)\n+        {\n+            /// Read number of labels\n+            UInt8 label_num{ 0 };\n+            read_buffer.readStrict(reinterpret_cast<char&>(label_num));\n+\n+            if (label_num == 0)\n+                return { 0, false };\n+\n+            auto labels_position = read_buffer.getPosition();\n+\n+            /// Find the index of the label from \"labels\" bytes\n+            auto begin_it{ data.begin() + labels_position };\n+            auto end_it{ data.begin() + labels_position + label_num };\n+\n+            auto pos = std::find(begin_it, end_it, label);\n+\n+            if (pos == end_it)\n+                return { 0, false };\n+\n+            /// Read the arc for the label\n+            UInt64 arc_index = (pos - begin_it);\n+            auto arcs_start_postion = labels_position + label_num;\n+\n+            read_buffer.seek(arcs_start_postion, SEEK_SET);\n+            for (size_t j = 0; j <= arc_index; j++)\n+            {\n+                state_index = 0;\n+                arc_output = 0;\n+                readVarUInt(state_index, read_buffer);\n+                if (state_index & 0x1) // output is followed\n+                {\n+                    readVarUInt(arc_output, read_buffer);\n+                }\n+                state_index >>= 1;\n+            }\n+        }\n+        else\n+        {\n+            LabelsAsBitmap bmp;\n+\n+            readVarUInt(bmp.data.items[0], read_buffer);\n+            readVarUInt(bmp.data.items[1], read_buffer);\n+            readVarUInt(bmp.data.items[2], read_buffer);\n+            readVarUInt(bmp.data.items[3], read_buffer);\n+\n+            if (!bmp.hasLabel(label))\n+                return { 0, false };\n+\n+            /// Read the arc for the label\n+            size_t arc_index = bmp.getIndex(label);\n+            for (size_t j = 0; j < arc_index; j++)\n+            {\n+                state_index = 0;\n+                arc_output = 0;\n+                readVarUInt(state_index, read_buffer);\n+                if (state_index & 0x1) // output is followed\n+                {\n+                    readVarUInt(arc_output, read_buffer);\n+                }\n+                state_index >>= 1;\n+            }\n+        }\n+        /// Accumulate the output value\n+        result.first += arc_output;\n+    }\n+    return result;\n+}\n+}\n+}\ndiff --git a/src/Common/FST.h b/src/Common/FST.h\nnew file mode 100644\nindex 000000000000..6bb4fdba7e23\n--- /dev/null\n+++ b/src/Common/FST.h\n@@ -0,0 +1,182 @@\n+#pragma once\n+#include <array>\n+#include <map>\n+#include <memory>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <Core/Types.h>\n+#include <IO/ReadHelpers.h>\n+#include <IO/WriteBuffer.h>\n+#include <base/types.h>\n+\n+namespace DB\n+{\n+/// Finite State Transducer is an efficient way to represent term dictionary.\n+/// It can be viewed as a map of <term, output> where output is an integer.\n+/// Detailed explanation can be found in the following paper\n+/// [Direct Construction of Minimal Acyclic Subsequential Transduers] by Stoyan Mihov and Denis Maurel, University of Tours, France\n+namespace FST\n+{\n+using Output = UInt64;\n+\n+class State;\n+using StatePtr = std::shared_ptr<State>;\n+\n+/// Arc represents a transition from one state to another\n+/// It includes the target state to which the arc points and the arc's output.\n+struct Arc\n+{\n+    Arc() = default;\n+\n+    explicit Arc(Output output_, const StatePtr & target_) : output{output_}, target{target_} { }\n+\n+    /// 0 means the arc has no output\n+    Output output = 0;\n+\n+    StatePtr target;\n+\n+    UInt64 serialize(WriteBuffer & write_buffer) const;\n+};\n+\n+bool operator==(const Arc & arc1, const Arc & arc2);\n+\n+/// LabelsAsBitmap implements a 256-bit bitmap for all labels of a state. Each bit represents\n+/// a label's presence and the index value of the bit represents the corresponding label\n+class LabelsAsBitmap\n+{\n+public:\n+    void addLabel(char label);\n+    bool hasLabel(char label) const;\n+\n+    /// computes the rank\n+    UInt64 getIndex(char label) const;\n+\n+    UInt64 serialize(WriteBuffer& write_buffer);\n+private:\n+    friend class State;\n+    friend class FiniteStateTransducer;\n+    /// data holds a 256-bit bitmap for all labels of a state. Its 256 bits correspond to 256\n+    /// possible label values.\n+    UInt256 data{ 0 };\n+};\n+\n+/// State implements the State in Finite State Transducer\n+/// Each state contains all its arcs and a flag indicating if it is final state\n+class State\n+{\n+public:\n+    static constexpr size_t MAX_ARCS_IN_SEQUENTIAL_METHOD = 32;\n+    enum class EncodingMethod\n+    {\n+        /// Serialize arcs sequentially\n+        Sequential = 0,\n+\n+        /// Serialize arcs by using bitmap\n+        /// Note this is NOT enabled for now since it is experimental\n+        Bitmap,\n+    };\n+    State() = default;\n+\n+    State(const State & state) = default;\n+\n+    UInt64 hash() const;\n+\n+    Arc * getArc(char label) const;\n+\n+    void addArc(char label, Output output, StatePtr target);\n+\n+    void clear();\n+\n+    UInt64 serialize(WriteBuffer & write_buffer);\n+\n+    bool isFinal() const\n+    {\n+        return flag_values.is_final == 1;\n+    }\n+    void setFinal(bool value)\n+    {\n+        flag_values.is_final = value;\n+    }\n+    EncodingMethod getEncodingMethod() const\n+    {\n+        return flag_values.encoding_method;\n+    }\n+    void readFlag(ReadBuffer & read_buffer)\n+    {\n+        read_buffer.readStrict(reinterpret_cast<char&>(flag));\n+    }\n+\n+    /// Transient ID of the state which is used for building FST. It won't be serialized\n+    UInt64 id = 0;\n+\n+    /// State index which indicates location of state in FST\n+    UInt64 state_index = 0;\n+\n+    /// Arcs which are started from state, the 'char' is the label on the arc\n+    std::unordered_map<char, Arc> arcs;\n+private:\n+    struct FlagValues\n+    {\n+        unsigned int is_final : 1;\n+        EncodingMethod encoding_method : 3;\n+    };\n+\n+    union\n+    {\n+        FlagValues flag_values;\n+        uint8_t flag = 0;\n+    };\n+};\n+\n+bool operator==(const State & state1, const State & state2);\n+\n+inline constexpr size_t MAX_TERM_LENGTH = 256;\n+\n+/// FSTBuilder is used to build Finite State Transducer by adding words incrementally.\n+/// Note that all the words have to be added in sorted order in order to achieve minimized result.\n+/// In the end, the caller should call build() to serialize minimized FST to WriteBuffer\n+class FSTBuilder\n+{\n+public:\n+    explicit FSTBuilder(WriteBuffer & write_buffer_);\n+\n+    void add(const std::string & word, Output output);\n+    UInt64 build();\n+private:\n+    StatePtr findMinimized(const State & s, bool & found);\n+    void minimizePreviousWordSuffix(Int64 down_to);\n+    static size_t getCommonPrefixLength(const String & word1, const String & word2);\n+\n+    std::array<StatePtr, MAX_TERM_LENGTH + 1> temp_states;\n+    String previous_word;\n+    StatePtr initial_state;\n+\n+    /// map of (state_hash, StatePtr)\n+    std::unordered_map<UInt64, StatePtr> minimized_states;\n+\n+    /// Next available ID of state\n+    UInt64 next_id = 1;\n+\n+    WriteBuffer & write_buffer;\n+    UInt64 previous_written_bytes = 0;\n+    UInt64 previous_state_index = 0;\n+};\n+\n+//FiniteStateTransducer is constructed by using minimized FST blob(which is loaded from index storage)\n+// It is used to retrieve output by given term\n+class FiniteStateTransducer\n+{\n+public:\n+    FiniteStateTransducer() = default;\n+    explicit FiniteStateTransducer(std::vector<UInt8> data_);\n+    std::pair<UInt64, bool> getOutput(const String & term);\n+    void clear();\n+    std::vector<UInt8> & getData() { return data; }\n+\n+private:\n+    std::vector<UInt8> data;\n+};\n+}\n+}\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex f58bd7ebafbf..1ade4ba28682 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -182,6 +182,7 @@ static constexpr UInt64 operator\"\"_GiB(unsigned long long value)\n     M(UInt64, merge_tree_max_rows_to_use_cache, (128 * 8192), \"The maximum number of rows per request, to use the cache of uncompressed data. If the request is large, the cache is not used. (For large queries not to flush out the cache.)\", 0) \\\n     M(UInt64, merge_tree_max_bytes_to_use_cache, (192 * 10 * 1024 * 1024), \"The maximum number of bytes per request, to use the cache of uncompressed data. If the request is large, the cache is not used. (For large queries not to flush out the cache.)\", 0) \\\n     M(Bool, do_not_merge_across_partitions_select_final, false, \"Merge parts only in one partition in select final\", 0) \\\n+    M(Bool, allow_experimental_inverted_index, false, \"If it is set to true, allow to use experimental inverted index.\", 0) \\\n     \\\n     M(UInt64, mysql_max_rows_to_insert, 65536, \"The maximum number of rows in MySQL batch insertion of the MySQL storage engine\", 0) \\\n     \\\ndiff --git a/src/Interpreters/GinFilter.cpp b/src/Interpreters/GinFilter.cpp\nnew file mode 100644\nindex 000000000000..8965d3721d22\n--- /dev/null\n+++ b/src/Interpreters/GinFilter.cpp\n@@ -0,0 +1,185 @@\n+#include <string>\n+#include <algorithm>\n+#include <city.h>\n+#include <Columns/ColumnArray.h>\n+#include <Columns/ColumnNullable.h>\n+#include <Columns/ColumnLowCardinality.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/DataTypeLowCardinality.h>\n+#include <Storages/MergeTree/MergeTreeIndexGin.h>\n+#include <Storages/MergeTree/MergeTreeIndexFullText.h>\n+#include <Disks/DiskLocal.h>\n+#include <Storages/MergeTree/GinIndexStore.h>\n+#include <Interpreters/GinFilter.h>\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int BAD_ARGUMENTS;\n+}\n+GinFilterParameters::GinFilterParameters(size_t ngrams_, Float64 density_)\n+    : ngrams(ngrams_), density(density_)\n+{\n+    if (ngrams > 8)\n+        throw Exception(\"The size of gin filter cannot be greater than 8\", ErrorCodes::BAD_ARGUMENTS);\n+    if (density <= 0 || density > 1)\n+        throw Exception(\"The density of gin filter must be between 0 and 1\", ErrorCodes::BAD_ARGUMENTS);\n+}\n+\n+GinFilter::GinFilter(const GinFilterParameters & params_)\n+    : params(params_)\n+{\n+}\n+\n+void GinFilter::add(const char* data, size_t len, UInt32 rowID, GinIndexStorePtr& store, UInt64 limit) const\n+{\n+    if (len > FST::MAX_TERM_LENGTH)\n+        return;\n+\n+    String term(data, len);\n+    auto it = store->getPostings().find(term);\n+\n+    if (it != store->getPostings().end())\n+    {\n+        if (!it->second->contains(rowID))\n+            it->second->add(rowID);\n+    }\n+    else\n+    {\n+        UInt64 threshold = std::lround(limit * params.density);\n+        GinIndexStore::GinIndexPostingsBuilderPtr builder = std::make_shared<GinIndexPostingsBuilder>(threshold);\n+        builder->add(rowID);\n+\n+        store->setPostingsBuilder(term, builder);\n+    }\n+}\n+\n+/// This method assumes segmentIDs are in increasing order, which is true since rows are\n+/// digested sequentially and segments are created sequentially too.\n+void GinFilter::addRowRangeToGinFilter(UInt32 segmentID, UInt32 rowIDStart, UInt32 rowIDEnd)\n+{\n+    /// check segment ids are monotonic increasing\n+    assert(rowid_ranges.empty() || rowid_ranges.back().segment_id <= segmentID);\n+\n+    if (!rowid_ranges.empty())\n+    {\n+        /// Try to merge the rowID range with the last one in the container\n+        GinSegmentWithRowIDRange & last_rowid_range = rowid_ranges.back();\n+\n+        if (last_rowid_range.segment_id == segmentID &&\n+            last_rowid_range.range_end+1 == rowIDStart)\n+        {\n+            last_rowid_range.range_end = rowIDEnd;\n+            return;\n+        }\n+    }\n+    rowid_ranges.push_back({segmentID, rowIDStart, rowIDEnd});\n+}\n+\n+void GinFilter::clear()\n+{\n+    terms.clear();\n+    rowid_ranges.clear();\n+    query_string.clear();\n+}\n+\n+bool GinFilter::hasEmptyPostingsList(const PostingsCache& postings_cache)\n+{\n+    if (postings_cache.empty())\n+        return true;\n+\n+    for (const auto& term_postings : postings_cache)\n+    {\n+        const SegmentedPostingsListContainer& container = term_postings.second;\n+        if (container.empty())\n+            return true;\n+    }\n+    return false;\n+}\n+\n+bool GinFilter::matchInRange(const PostingsCache& postings_cache, UInt32 segment_id, UInt32 range_start, UInt32 range_end)\n+{\n+    /// Check for each terms\n+    GinIndexPostingsList intersection_result;\n+    bool intersection_result_init = false;\n+\n+    for (const auto& term_postings : postings_cache)\n+    {\n+        /// Check if it is in the same segment by searching for segment_id\n+        const SegmentedPostingsListContainer& container = term_postings.second;\n+        auto container_it = container.find(segment_id);\n+        if (container_it == container.cend())\n+        {\n+            return false;\n+        }\n+        auto min_in_container = container_it->second->minimum();\n+        auto max_in_container = container_it->second->maximum();\n+\n+        //check if the postings list has always match flag\n+        if (container_it->second->cardinality() == 1 && UINT32_MAX == min_in_container)\n+        {\n+            continue; //always match\n+        }\n+\n+        if (range_start > max_in_container ||  min_in_container > range_end)\n+        {\n+            return false;\n+        }\n+\n+        /// Delay initialization as late as possible\n+        if (!intersection_result_init)\n+        {\n+            intersection_result_init = true;\n+            intersection_result.addRange(range_start, range_end+1);\n+        }\n+        intersection_result &= *container_it->second;\n+        if (intersection_result.cardinality() == 0)\n+        {\n+            return false;\n+        }\n+    }\n+    return true;\n+}\n+\n+bool GinFilter::match(const PostingsCache& postings_cache) const\n+{\n+    if (hasEmptyPostingsList(postings_cache))\n+    {\n+        return false;\n+    }\n+\n+    /// Check for each row ID ranges\n+    for (const auto &rowid_range: rowid_ranges)\n+    {\n+        if (matchInRange(postings_cache, rowid_range.segment_id, rowid_range.range_start, rowid_range.range_end))\n+        {\n+            return true;\n+        }\n+    }\n+    return false;\n+}\n+\n+bool GinFilter::contains(const GinFilter & filter, PostingsCacheForStore &cache_store) const\n+{\n+    if (filter.getTerms().empty())\n+        return true;\n+\n+    PostingsCachePtr postings_cache = cache_store.getPostings(filter.getQueryString());\n+    if (postings_cache == nullptr)\n+    {\n+        GinIndexStoreDeserializer reader(cache_store.store);\n+        postings_cache = reader.createPostingsCacheFromTerms(filter.getTerms());\n+        cache_store.cache[filter.getQueryString()] = postings_cache;\n+    }\n+\n+    return match(*postings_cache);\n+}\n+\n+String GinFilter::getName()\n+{\n+    return FilterName;\n+}\n+\n+}\ndiff --git a/src/Interpreters/GinFilter.h b/src/Interpreters/GinFilter.h\nnew file mode 100644\nindex 000000000000..0bcd4156f945\n--- /dev/null\n+++ b/src/Interpreters/GinFilter.h\n@@ -0,0 +1,108 @@\n+#pragma once\n+\n+#include <vector>\n+#include <memory>\n+#include <Storages/MergeTree/GinIndexStore.h>\n+namespace DB\n+{\n+struct GinFilterParameters\n+{\n+    explicit GinFilterParameters(size_t ngrams_, Float64 density_);\n+\n+    size_t ngrams;\n+    Float64 density;\n+};\n+\n+struct GinSegmentWithRowIDRange\n+{\n+    /// Segment ID of the row ID range\n+    UInt32 segment_id;\n+\n+    /// First row ID in the range\n+    UInt32 range_start;\n+\n+    /// Last row ID in the range (inclusive)\n+    UInt32 range_end;\n+};\n+\n+/// GinFilter provides underlying functionalities for building inverted index and also\n+/// it does filtering the unmatched rows according to its query string.\n+/// It also builds and uses skipping index which stores (segmentID, RowIDStart, RowIDEnd) triples.\n+class GinFilter\n+{\n+public:\n+    using GinSegmentWithRowIDRanges = std::vector<GinSegmentWithRowIDRange>;\n+\n+    explicit GinFilter(const GinFilterParameters& params_);\n+\n+    /// Add term(which length is 'len' and located at 'data') and its row ID to\n+    /// the postings list builder for building inverted index for the given store.\n+    void add(const char* data, size_t len, UInt32 rowID, GinIndexStorePtr& store, UInt64 limit) const;\n+\n+    /// Accumulate (segmentID, RowIDStart, RowIDEnd) for building skipping index\n+    void addRowRangeToGinFilter(UInt32 segmentID, UInt32 rowIDStart, UInt32 rowIDEnd);\n+\n+    /// Clear the content\n+    void clear();\n+\n+    /// Check if the filter(built from query string) contains any rows in given filter 'af' by using\n+    /// given postings list cache\n+    bool contains(const GinFilter & filter, PostingsCacheForStore &cache_store) const;\n+\n+    /// Const getter for the row ID ranges\n+    const GinSegmentWithRowIDRanges& getFilter() const { return rowid_ranges; }\n+\n+    /// Mutable getter for the row ID ranges\n+    GinSegmentWithRowIDRanges& getFilter() { return rowid_ranges; }\n+\n+    /// Set the query string of the filter\n+    void setQueryString(const char* data, size_t len)\n+    {\n+        query_string = String(data, len);\n+    }\n+\n+    /// Const getter of the query string\n+    const String &getQueryString() const { return query_string; }\n+\n+    /// Add term which are tokens generated from the query string\n+    void addTerm(const char* data, size_t len)\n+    {\n+        if (len > FST::MAX_TERM_LENGTH)\n+            return;\n+        terms.push_back(String(data, len));\n+    }\n+\n+    /// Const getter of terms(generated from the query string)\n+    const std::vector<String>& getTerms() const { return terms;}\n+\n+    /// Check if the given postings list cache has matched rows by using the filter\n+    bool match(const PostingsCache& postings_cache) const;\n+\n+    /// Get filter name (\"inverted\")\n+    static String getName();\n+\n+    /// Constant of filter name\n+    static constexpr auto FilterName = \"inverted\";\n+private:\n+    /// Filter parameters\n+    const GinFilterParameters& params;\n+\n+    /// Query string of the filter\n+    String query_string;\n+\n+    /// Tokenized terms from query string\n+    std::vector<String> terms;\n+\n+    /// Row ID ranges which are (segmentID, RowIDStart, RowIDEnd)\n+    GinSegmentWithRowIDRanges rowid_ranges;\n+\n+    /// Helper method for checking if postings list cache is empty\n+    static bool hasEmptyPostingsList(const PostingsCache& postings_cache);\n+\n+    /// Helper method to check if the postings list cache has intersection with given row ID range\n+    static bool matchInRange(const PostingsCache& postings_cache, UInt32 segment_id, UInt32 range_start, UInt32 range_end);\n+};\n+\n+using GinFilterPtr = std::shared_ptr<GinFilter>;\n+\n+}\ndiff --git a/src/Interpreters/ITokenExtractor.h b/src/Interpreters/ITokenExtractor.h\nindex afcc8442d583..77de4233b63c 100644\n--- a/src/Interpreters/ITokenExtractor.h\n+++ b/src/Interpreters/ITokenExtractor.h\n@@ -3,7 +3,7 @@\n #include <base/types.h>\n \n #include <Interpreters/BloomFilter.h>\n-\n+#include <Interpreters/GinFilter.h>\n \n namespace DB\n {\n@@ -37,6 +37,15 @@ struct ITokenExtractor\n \n     virtual void stringLikeToBloomFilter(const char * data, size_t length, BloomFilter & bloom_filter) const = 0;\n \n+    virtual void stringToGinFilter(const char * data, size_t length, GinFilter & gin_filter) const = 0;\n+\n+    virtual void stringPaddedToGinFilter(const char * data, size_t length, GinFilter & gin_filter) const\n+    {\n+        return stringToGinFilter(data, length, gin_filter);\n+    }\n+\n+    virtual void stringLikeToGinFilter(const char * data, size_t length, GinFilter & gin_filter) const = 0;\n+\n };\n \n using TokenExtractorPtr = const ITokenExtractor *;\n@@ -71,6 +80,36 @@ class ITokenExtractorHelper : public ITokenExtractor\n         while (cur < length && static_cast<const Derived *>(this)->nextInStringLike(data, length, &cur, token))\n             bloom_filter.add(token.c_str(), token.size());\n     }\n+    void stringToGinFilter(const char * data, size_t length, GinFilter & gin_filter) const override\n+    {\n+        gin_filter.setQueryString(data, length);\n+        size_t cur = 0;\n+        size_t token_start = 0;\n+        size_t token_len = 0;\n+\n+        while (cur < length && static_cast<const Derived *>(this)->nextInString(data, length, &cur, &token_start, &token_len))\n+            gin_filter.addTerm(data + token_start, token_len);\n+    }\n+\n+    void stringPaddedToGinFilter(const char * data, size_t length, GinFilter & gin_filter) const override\n+    {\n+        gin_filter.setQueryString(data, length);\n+        size_t cur = 0;\n+        size_t token_start = 0;\n+        size_t token_len = 0;\n+\n+        while (cur < length && static_cast<const Derived *>(this)->nextInStringPadded(data, length, &cur, &token_start, &token_len))\n+            gin_filter.addTerm(data + token_start, token_len);\n+    }\n+\n+    void stringLikeToGinFilter(const char * data, size_t length, GinFilter & gin_filter) const override\n+    {\n+        gin_filter.setQueryString(data, length);\n+        size_t cur = 0;\n+        String token;\n+        while (cur < length && static_cast<const Derived *>(this)->nextInStringLike(data, length, &cur, token))\n+            gin_filter.addTerm(token.c_str(), token.size());\n+    }\n };\n \n \ndiff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp\nindex b8538a0aff87..bea88885d207 100644\n--- a/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -41,6 +41,7 @@\n #include <Interpreters/InterpreterInsertQuery.h>\n #include <Interpreters/InterpreterRenameQuery.h>\n #include <Interpreters/AddDefaultDatabaseVisitor.h>\n+#include <Interpreters/GinFilter.h>\n \n #include <Access/Common/AccessRightsElement.h>\n \n@@ -100,6 +101,7 @@ namespace ErrorCodes\n     extern const int ENGINE_REQUIRED;\n     extern const int UNKNOWN_STORAGE;\n     extern const int SYNTAX_ERROR;\n+    extern const int SUPPORT_IS_DISABLED;\n }\n \n namespace fs = std::filesystem;\n@@ -677,12 +679,18 @@ InterpreterCreateQuery::TableProperties InterpreterCreateQuery::getTableProperti\n         if (create.columns_list->indices)\n             for (const auto & index : create.columns_list->indices->children)\n             {\n-                properties.indices.push_back(\n-                    IndexDescription::getIndexFromAST(index->clone(), properties.columns, getContext()));\n-                    if (properties.indices.back().type == \"annoy\" && !getContext()->getSettingsRef().allow_experimental_annoy_index)\n-                        throw Exception(\"Annoy index is disabled. Turn on allow_experimental_annoy_index\", ErrorCodes::INCORRECT_QUERY);\n-            }\n+                IndexDescription index_desc = IndexDescription::getIndexFromAST(index->clone(), properties.columns, getContext());\n+                if (index_desc.type == GinFilter::FilterName && getContext()->getSettingsRef().allow_experimental_inverted_index == false)\n+                {\n+                    throw Exception(\n+                            \"Experimental Inverted Index feature is not enabled (the setting 'allow_experimental_inverted_index')\",\n+                            ErrorCodes::SUPPORT_IS_DISABLED);\n+                }\n+                if (index_desc.type == \"annoy\" && !getContext()->getSettingsRef().allow_experimental_annoy_index)\n+                    throw Exception(\"Annoy index is disabled. Turn on allow_experimental_annoy_index\", ErrorCodes::INCORRECT_QUERY);\n \n+                properties.indices.push_back(index_desc);\n+            }\n         if (create.columns_list->projections)\n             for (const auto & projection_ast : create.columns_list->projections->children)\n             {\ndiff --git a/src/Storages/AlterCommands.cpp b/src/Storages/AlterCommands.cpp\nindex f39f830bcc0e..1d4df05c723b 100644\n--- a/src/Storages/AlterCommands.cpp\n+++ b/src/Storages/AlterCommands.cpp\n@@ -13,6 +13,7 @@\n #include <Interpreters/ExpressionAnalyzer.h>\n #include <Interpreters/TreeRewriter.h>\n #include <Interpreters/RenameColumnVisitor.h>\n+#include <Interpreters/GinFilter.h>\n #include <Parsers/ASTAlterQuery.h>\n #include <Parsers/ASTColumnDeclaration.h>\n #include <Parsers/ASTConstraintDeclaration.h>\n@@ -904,7 +905,26 @@ std::optional<MutationCommand> AlterCommand::tryConvertToMutationCommand(Storage\n     return result;\n }\n \n-\n+bool AlterCommands::hasInvertedIndex(const StorageInMemoryMetadata & metadata, ContextPtr context)\n+{\n+    for (const auto & index : metadata.secondary_indices)\n+    {\n+        IndexDescription index_desc;\n+        try\n+        {\n+            index_desc = IndexDescription::getIndexFromAST(index.definition_ast, metadata.columns, context);\n+        }\n+        catch (...)\n+        {\n+            continue;\n+        }\n+        if (index.type == GinFilter::FilterName)\n+        {\n+            return true;\n+        }\n+    }\n+    return false;\n+}\n void AlterCommands::apply(StorageInMemoryMetadata & metadata, ContextPtr context) const\n {\n     if (!prepared)\ndiff --git a/src/Storages/AlterCommands.h b/src/Storages/AlterCommands.h\nindex c91c82e9c7a6..a79827b355d0 100644\n--- a/src/Storages/AlterCommands.h\n+++ b/src/Storages/AlterCommands.h\n@@ -210,6 +210,9 @@ class AlterCommands : public std::vector<AlterCommand>\n     /// empty. If some TTL changes happened than, depending on materialize_ttl\n     /// additional mutation command (MATERIALIZE_TTL) will be returned.\n     MutationCommands getMutationCommands(StorageInMemoryMetadata metadata, bool materialize_ttl, ContextPtr context, bool with_alters=false) const;\n+\n+    /// Check if commands have any inverted index\n+    static bool hasInvertedIndex(const StorageInMemoryMetadata & metadata, ContextPtr context);\n };\n \n }\ndiff --git a/src/Storages/MergeTree/DataPartStorageOnDisk.cpp b/src/Storages/MergeTree/DataPartStorageOnDisk.cpp\nindex 215d6034a531..73e7ae547954 100644\n--- a/src/Storages/MergeTree/DataPartStorageOnDisk.cpp\n+++ b/src/Storages/MergeTree/DataPartStorageOnDisk.cpp\n@@ -741,6 +741,17 @@ std::unique_ptr<WriteBufferFromFileBase> DataPartStorageOnDisk::writeFile(\n     return volume->getDisk()->writeFile(fs::path(root_path) / part_dir / name, buf_size, WriteMode::Rewrite, settings);\n }\n \n+std::unique_ptr<WriteBufferFromFileBase> DataPartStorageOnDisk::writeFile(\n+    const String & name,\n+    size_t buf_size,\n+    WriteMode mode,\n+    const WriteSettings & settings)\n+{\n+    if (transaction)\n+        return transaction->writeFile(fs::path(root_path) / part_dir / name, buf_size, mode, settings, /* autocommit = */ false);\n+    return volume->getDisk()->writeFile(fs::path(root_path) / part_dir / name, buf_size, mode, settings);\n+}\n+\n std::unique_ptr<WriteBufferFromFileBase> DataPartStorageOnDisk::writeTransactionFile(WriteMode mode) const\n {\n     return volume->getDisk()->writeFile(fs::path(root_path) / part_dir / \"txn_version.txt\", 256, mode);\ndiff --git a/src/Storages/MergeTree/DataPartStorageOnDisk.h b/src/Storages/MergeTree/DataPartStorageOnDisk.h\nindex fd408af9cf1b..3e82d44d71e5 100644\n--- a/src/Storages/MergeTree/DataPartStorageOnDisk.h\n+++ b/src/Storages/MergeTree/DataPartStorageOnDisk.h\n@@ -112,6 +112,11 @@ class DataPartStorageOnDisk final : public IDataPartStorage\n     void createFile(const String & name) override;\n     void moveFile(const String & from_name, const String & to_name) override;\n     void replaceFile(const String & from_name, const String & to_name) override;\n+    std::unique_ptr<WriteBufferFromFileBase> writeFile(\n+        const String & name,\n+        size_t buf_size,\n+        DB::WriteMode mode,\n+        const WriteSettings & settings) override;\n \n     void removeFile(const String & name) override;\n     void removeFileIfExists(const String & name) override;\ndiff --git a/src/Storages/MergeTree/GinIndexStore.cpp b/src/Storages/MergeTree/GinIndexStore.cpp\nnew file mode 100644\nindex 000000000000..995f4f9f88c3\n--- /dev/null\n+++ b/src/Storages/MergeTree/GinIndexStore.cpp\n@@ -0,0 +1,487 @@\n+#include <Storages/MergeTree/GinIndexStore.h>\n+#include <vector>\n+#include <unordered_map>\n+#include <iostream>\n+#include <numeric>\n+#include <algorithm>\n+#include <Columns/ColumnString.h>\n+#include <IO/WriteHelpers.h>\n+#include <IO/ReadHelpers.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <IO/ReadBufferFromFile.h>\n+#include <IO/WriteBufferFromFile.h>\n+#include <IO/WriteBufferFromVector.h>\n+#include <Common/FST.h>\n+\n+namespace DB\n+{\n+\n+using TokenPostingsBuilderPair = std::pair<std::string_view, GinIndexStore::GinIndexPostingsBuilderPtr>;\n+using TokenPostingsBuilderPairs = std::vector<TokenPostingsBuilderPair>;\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int UNKNOWN_FORMAT_VERSION;\n+};\n+\n+GinIndexStore::GinIndexStore(const String & name_, DataPartStoragePtr storage_)\n+    : name(name_)\n+    , storage(storage_)\n+{\n+}\n+GinIndexStore::GinIndexStore(const String& name_, DataPartStoragePtr storage_, MutableDataPartStoragePtr data_part_storage_builder_, UInt64 max_digestion_size_)\n+    : name(name_)\n+    , storage(storage_)\n+    , data_part_storage_builder(data_part_storage_builder_)\n+    , max_digestion_size(max_digestion_size_)\n+{\n+}\n+\n+GinIndexPostingsBuilder::GinIndexPostingsBuilder(UInt64 limit) : rowid_lst{}, size_limit(limit)\n+{}\n+\n+bool GinIndexPostingsBuilder::contains(UInt32 row_id) const\n+{\n+    if (useRoaring())\n+        return rowid_bitmap.contains(row_id);\n+\n+    const auto * const it = std::find(rowid_lst.begin(), rowid_lst.begin()+rowid_lst_length, row_id);\n+    return it != rowid_lst.begin() + rowid_lst_length;\n+}\n+\n+void GinIndexPostingsBuilder::add(UInt32 row_id)\n+{\n+    if (containsAllRows())\n+    {\n+        return;\n+    }\n+    if (useRoaring())\n+    {\n+        if (rowid_bitmap.cardinality() == size_limit)\n+        {\n+            //reset the postings list with MATCH ALWAYS;\n+            rowid_lst_length = 1; //makes sure useRoaring() returns false;\n+            rowid_lst[0] = UINT32_MAX; //set CONTAINS ALL flag;\n+        }\n+        else\n+        {\n+            rowid_bitmap.add(row_id);\n+        }\n+        return;\n+    }\n+    assert(rowid_lst_length < MIN_SIZE_FOR_ROARING_ENCODING);\n+    rowid_lst[rowid_lst_length] = row_id;\n+    rowid_lst_length++;\n+\n+    if (rowid_lst_length == MIN_SIZE_FOR_ROARING_ENCODING)\n+    {\n+        for (size_t i = 0; i < rowid_lst_length; i++)\n+            rowid_bitmap.add(rowid_lst[i]);\n+\n+        rowid_lst_length = UsesBitMap;\n+    }\n+}\n+\n+bool GinIndexPostingsBuilder::useRoaring() const\n+{\n+    return rowid_lst_length == UsesBitMap;\n+}\n+\n+bool GinIndexPostingsBuilder::containsAllRows() const\n+{\n+    return rowid_lst[0] == UINT32_MAX;\n+}\n+\n+UInt64 GinIndexPostingsBuilder::serialize(WriteBuffer &buffer) const\n+{\n+    UInt64 written_bytes = 0;\n+    buffer.write(rowid_lst_length);\n+    written_bytes += 1;\n+\n+    if (!useRoaring())\n+    {\n+        for (size_t i = 0; i <  rowid_lst_length; ++i)\n+        {\n+            writeVarUInt(rowid_lst[i], buffer);\n+            written_bytes += getLengthOfVarUInt(rowid_lst[i]);\n+        }\n+    }\n+    else\n+    {\n+        auto size = rowid_bitmap.getSizeInBytes();\n+\n+        writeVarUInt(size, buffer);\n+        written_bytes += getLengthOfVarUInt(size);\n+\n+        auto buf = std::make_unique<char[]>(size);\n+        rowid_bitmap.write(buf.get());\n+        buffer.write(buf.get(), size);\n+        written_bytes += size;\n+    }\n+    return written_bytes;\n+}\n+\n+GinIndexPostingsListPtr GinIndexPostingsBuilder::deserialize(ReadBuffer &buffer)\n+{\n+    UInt8 postings_list_size = 0;\n+    buffer.readStrict(reinterpret_cast<char&>(postings_list_size));\n+\n+    if (postings_list_size != UsesBitMap)\n+    {\n+        assert(postings_list_size < MIN_SIZE_FOR_ROARING_ENCODING);\n+        GinIndexPostingsListPtr postings_list = std::make_shared<GinIndexPostingsList>();\n+        UInt32 row_ids[MIN_SIZE_FOR_ROARING_ENCODING];\n+\n+        for (auto i = 0; i < postings_list_size; ++i)\n+        {\n+            readVarUInt(row_ids[i], buffer);\n+        }\n+        postings_list->addMany(postings_list_size, row_ids);\n+        return postings_list;\n+    }\n+    else\n+    {\n+        size_t size{0};\n+        readVarUInt(size, buffer);\n+        auto buf = std::make_unique<char[]>(size);\n+        buffer.readStrict(reinterpret_cast<char*>(buf.get()), size);\n+\n+        GinIndexPostingsListPtr postings_list = std::make_shared<GinIndexPostingsList>(GinIndexPostingsList::read(buf.get()));\n+\n+        return postings_list;\n+    }\n+}\n+\n+bool GinIndexStore::exists() const\n+{\n+    String id_file_name = getName() + GIN_SEGMENT_ID_FILE_TYPE;\n+    return storage->exists(id_file_name);\n+}\n+\n+UInt32 GinIndexStore::getNextSegmentIDRange(const String& file_name, size_t n)\n+{\n+    std::lock_guard guard(gin_index_store_mutex);\n+\n+    /// When the method is called for the first time, the file doesn't exist yet, need to create it\n+    /// and write segment ID 1.\n+    if (!storage->exists(file_name))\n+    {\n+        /// Create file and write initial segment id = 1\n+        std::unique_ptr<DB::WriteBufferFromFileBase> ostr = this->data_part_storage_builder->writeFile(file_name, DBMS_DEFAULT_BUFFER_SIZE, {});\n+\n+        /// Write version\n+        writeChar(static_cast<char>(CURRENT_GIN_FILE_FORMAT_VERSION), *ostr);\n+\n+        writeVarUInt(1, *ostr);\n+        ostr->sync();\n+    }\n+\n+    /// read id in file\n+    UInt32 result = 0;\n+    {\n+        std::unique_ptr<DB::ReadBufferFromFileBase> istr = this->storage->readFile(file_name, {}, std::nullopt, std::nullopt);\n+\n+        /// Skip version\n+        istr->seek(1, SEEK_SET);\n+\n+        readVarUInt(result, *istr);\n+    }\n+    //save result+n\n+    {\n+        std::unique_ptr<DB::WriteBufferFromFileBase> ostr = this->data_part_storage_builder->writeFile(file_name, DBMS_DEFAULT_BUFFER_SIZE, {});\n+\n+        /// Write version\n+        writeChar(static_cast<char>(CURRENT_GIN_FILE_FORMAT_VERSION), *ostr);\n+\n+        writeVarUInt(result + n, *ostr);\n+        ostr->sync();\n+    }\n+    return result;\n+}\n+\n+UInt32 GinIndexStore::getNextRowIDRange(size_t numIDs)\n+{\n+    UInt32 result =current_segment.next_row_id;\n+    current_segment.next_row_id += numIDs;\n+    return result;\n+}\n+\n+UInt32 GinIndexStore::getNextSegmentID()\n+{\n+    String sid_file_name = getName() + GIN_SEGMENT_ID_FILE_TYPE;\n+    return getNextSegmentIDRange(sid_file_name, 1);\n+}\n+\n+UInt32 GinIndexStore::getNumOfSegments()\n+{\n+    if (cached_segment_num)\n+        return cached_segment_num;\n+\n+    String sid_file_name = getName() + GIN_SEGMENT_ID_FILE_TYPE;\n+    if (!storage->exists(sid_file_name))\n+        return 0;\n+\n+    UInt32 result = 0;\n+    {\n+        std::unique_ptr<DB::ReadBufferFromFileBase> istr = this->storage->readFile(sid_file_name, {}, std::nullopt, std::nullopt);\n+\n+        uint8_t version = 0;\n+        readBinary(version, *istr);\n+\n+        if (version > CURRENT_GIN_FILE_FORMAT_VERSION)\n+            throw Exception(ErrorCodes::UNKNOWN_FORMAT_VERSION, \"Unsupported inverted index version {}\", version);\n+\n+        readVarUInt(result, *istr);\n+    }\n+\n+    cached_segment_num = result - 1;\n+    return cached_segment_num;\n+}\n+\n+bool GinIndexStore::needToWrite() const\n+{\n+    assert(max_digestion_size > 0);\n+    return current_size > max_digestion_size;\n+}\n+\n+void GinIndexStore::finalize()\n+{\n+    if (!current_postings.empty())\n+    {\n+        writeSegment();\n+    }\n+}\n+\n+void GinIndexStore::initFileStreams()\n+{\n+    String segment_file_name = getName() + GIN_SEGMENT_FILE_TYPE;\n+    String term_dict_file_name = getName() + GIN_DICTIONARY_FILE_TYPE;\n+    String postings_file_name = getName() + GIN_POSTINGS_FILE_TYPE;\n+\n+    segment_file_stream = data_part_storage_builder->writeFile(segment_file_name, DBMS_DEFAULT_BUFFER_SIZE, WriteMode::Append, {});\n+    term_dict_file_stream = data_part_storage_builder->writeFile(term_dict_file_name, DBMS_DEFAULT_BUFFER_SIZE, WriteMode::Append, {});\n+    postings_file_stream = data_part_storage_builder->writeFile(postings_file_name, DBMS_DEFAULT_BUFFER_SIZE, WriteMode::Append, {});\n+}\n+\n+void GinIndexStore::writeSegment()\n+{\n+    if (segment_file_stream == nullptr)\n+    {\n+        initFileStreams();\n+    }\n+\n+    /// Write segment\n+    segment_file_stream->write(reinterpret_cast<char*>(&current_segment), sizeof(GinIndexSegment));\n+    TokenPostingsBuilderPairs token_postings_list_pairs;\n+    token_postings_list_pairs.reserve(current_postings.size());\n+\n+    for (const auto& [token, postings_list] : current_postings)\n+    {\n+        token_postings_list_pairs.push_back({token, postings_list});\n+    }\n+\n+    /// Sort token-postings list pairs since all tokens have to be added in FST in sorted order\n+    std::sort(token_postings_list_pairs.begin(), token_postings_list_pairs.end(),\n+                    [](const TokenPostingsBuilderPair& a, const TokenPostingsBuilderPair & b)\n+                    {\n+                        return a.first < b.first;\n+                    });\n+\n+    ///write postings\n+    std::vector<UInt64> posting_list_byte_sizes(current_postings.size(), 0);\n+\n+    for (size_t current_index = 0; const auto& [token, postings_list] : token_postings_list_pairs)\n+    {\n+        auto posting_list_byte_size = postings_list->serialize(*postings_file_stream);\n+\n+        posting_list_byte_sizes[current_index] = posting_list_byte_size;\n+        current_index++;\n+        current_segment.postings_start_offset += posting_list_byte_size;\n+    }\n+    ///write item dictionary\n+    std::vector<UInt8> buffer;\n+    WriteBufferFromVector<std::vector<UInt8>> write_buf(buffer);\n+    FST::FSTBuilder builder(write_buf);\n+\n+    UInt64 offset = 0;\n+    for (size_t current_index = 0; const auto& [token, postings_list] : token_postings_list_pairs)\n+    {\n+        String str_token{token};\n+        builder.add(str_token, offset);\n+        offset += posting_list_byte_sizes[current_index];\n+        current_index++;\n+    }\n+\n+    builder.build();\n+    write_buf.finalize();\n+\n+    /// Write FST size\n+    writeVarUInt(buffer.size(), *term_dict_file_stream);\n+    current_segment.term_dict_start_offset += getLengthOfVarUInt(buffer.size());\n+\n+    /// Write FST content\n+    term_dict_file_stream->write(reinterpret_cast<char*>(buffer.data()), buffer.size());\n+    current_segment.term_dict_start_offset += buffer.size();\n+\n+    current_size = 0;\n+    current_postings.clear();\n+    current_segment.segment_id = getNextSegmentID();\n+\n+    segment_file_stream->sync();\n+    term_dict_file_stream->sync();\n+    postings_file_stream->sync();\n+}\n+\n+GinIndexStoreDeserializer::GinIndexStoreDeserializer(const GinIndexStorePtr & store_)\n+    : store(store_)\n+{\n+    initFileStreams();\n+}\n+\n+void GinIndexStoreDeserializer::initFileStreams()\n+{\n+    String segment_file_name = store->getName() + GinIndexStore::GIN_SEGMENT_FILE_TYPE;\n+    String term_dict_file_name = store->getName() + GinIndexStore::GIN_DICTIONARY_FILE_TYPE;\n+    String postings_file_name = store->getName() + GinIndexStore::GIN_POSTINGS_FILE_TYPE;\n+\n+    segment_file_stream = store->storage->readFile(segment_file_name, {}, std::nullopt, std::nullopt);\n+    term_dict_file_stream = store->storage->readFile(term_dict_file_name, {}, std::nullopt, std::nullopt);\n+    postings_file_stream = store->storage->readFile(postings_file_name, {}, std::nullopt, std::nullopt);\n+}\n+void GinIndexStoreDeserializer::readSegments()\n+{\n+    auto num_segments = store->getNumOfSegments();\n+    if (num_segments == 0)\n+        return;\n+\n+    GinIndexSegments segments (num_segments);\n+\n+    assert(segment_file_stream != nullptr);\n+\n+    segment_file_stream->readStrict(reinterpret_cast<char*>(segments.data()), num_segments * sizeof(GinIndexSegment));\n+    for (size_t i = 0; i < num_segments; ++i)\n+    {\n+        auto seg_id = segments[i].segment_id;\n+        auto term_dict = std::make_shared<SegmentTermDictionary>();\n+        term_dict->postings_start_offset = segments[i].postings_start_offset;\n+        term_dict->term_dict_start_offset = segments[i].term_dict_start_offset;\n+        store->term_dicts[seg_id] = term_dict;\n+    }\n+}\n+\n+void GinIndexStoreDeserializer::readSegmentTermDictionaries()\n+{\n+    for (UInt32 seg_index = 0; seg_index < store->getNumOfSegments(); ++seg_index)\n+    {\n+        readSegmentTermDictionary(seg_index);\n+    }\n+}\n+\n+void GinIndexStoreDeserializer::readSegmentTermDictionary(UInt32 segment_id)\n+{\n+    /// Check validity of segment_id\n+    auto it = store->term_dicts.find(segment_id);\n+    if (it == store->term_dicts.end())\n+    {\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Invalid segment id {}\", segment_id);\n+    }\n+\n+    assert(term_dict_file_stream != nullptr);\n+\n+    /// Set file pointer of term dictionary file\n+    term_dict_file_stream->seek(it->second->term_dict_start_offset, SEEK_SET);\n+\n+    it->second->offsets.getData().clear();\n+    /// Read FST size\n+    size_t fst_size{0};\n+    readVarUInt(fst_size, *term_dict_file_stream);\n+\n+    /// Read FST content\n+    it->second->offsets.getData().resize(fst_size);\n+    term_dict_file_stream->readStrict(reinterpret_cast<char*>(it->second->offsets.getData().data()), fst_size);\n+}\n+\n+SegmentedPostingsListContainer GinIndexStoreDeserializer::readSegmentedPostingsLists(const String& term)\n+{\n+    assert(postings_file_stream != nullptr);\n+\n+    SegmentedPostingsListContainer container;\n+    for (auto const& seg_term_dict : store->term_dicts)\n+    {\n+        auto segment_id = seg_term_dict.first;\n+\n+        auto [offset, found] = seg_term_dict.second->offsets.getOutput(term);\n+        if (!found)\n+            continue;\n+\n+        // Set postings file pointer for reading postings list\n+        postings_file_stream->seek(seg_term_dict.second->postings_start_offset + offset, SEEK_SET);\n+\n+        // Read posting list\n+        auto postings_list = GinIndexPostingsBuilder::deserialize(*postings_file_stream);\n+        container[segment_id] = postings_list;\n+    }\n+    return container;\n+}\n+\n+PostingsCachePtr GinIndexStoreDeserializer::createPostingsCacheFromTerms(const std::vector<String>& terms)\n+{\n+    auto postings_cache = std::make_shared<PostingsCache>();\n+    for (const auto& term : terms)\n+    {\n+        // Make sure don't read for duplicated terms\n+        if (postings_cache->find(term) != postings_cache->end())\n+            continue;\n+\n+        auto container = readSegmentedPostingsLists(term);\n+        (*postings_cache)[term] = container;\n+    }\n+    return postings_cache;\n+}\n+\n+GinIndexStoreFactory& GinIndexStoreFactory::instance()\n+{\n+    static GinIndexStoreFactory instance;\n+    return instance;\n+}\n+\n+GinIndexStorePtr GinIndexStoreFactory::get(const String& name, DataPartStoragePtr storage)\n+{\n+    const String& part_path = storage->getRelativePath();\n+    String key = name + \":\" + part_path;\n+\n+    std::lock_guard lock(stores_mutex);\n+    GinIndexStores::const_iterator it = stores.find(key);\n+\n+    if (it == stores.end())\n+    {\n+        GinIndexStorePtr store = std::make_shared<GinIndexStore>(name, storage);\n+        if (!store->exists())\n+            return nullptr;\n+\n+        GinIndexStoreDeserializer deserializer(store);\n+        deserializer.readSegments();\n+        deserializer.readSegmentTermDictionaries();\n+\n+        stores[key] = store;\n+\n+        return store;\n+    }\n+    return it->second;\n+}\n+\n+void GinIndexStoreFactory::remove(const String& part_path)\n+{\n+    std::lock_guard lock(stores_mutex);\n+    for (auto it = stores.begin(); it != stores.end();)\n+    {\n+        if (it->first.find(part_path) != String::npos)\n+            it = stores.erase(it);\n+        else\n+            ++it;\n+    }\n+}\n+}\ndiff --git a/src/Storages/MergeTree/GinIndexStore.h b/src/Storages/MergeTree/GinIndexStore.h\nnew file mode 100644\nindex 000000000000..c326322191f8\n--- /dev/null\n+++ b/src/Storages/MergeTree/GinIndexStore.h\n@@ -0,0 +1,304 @@\n+#pragma once\n+\n+#include <array>\n+#include <vector>\n+#include <unordered_map>\n+#include <mutex>\n+#include <Core/Block.h>\n+#include <Disks/IDisk.h>\n+#include <IO/ReadBufferFromFileBase.h>\n+#include <IO/WriteBufferFromFileBase.h>\n+#include <roaring.hh>\n+#include <Common/FST.h>\n+#include <Storages/MergeTree/IDataPartStorage.h>\n+\n+/// GinIndexStore manages the inverted index for a data part, and it is made up of one or more immutable\n+/// index segments.\n+///\n+/// There are 4 types of index files in a store:\n+///  1. Segment ID file(.gin_sid): it contains one byte for version followed by the next available segment ID.\n+///  2. Segment Metadata file(.gin_seg): it contains index segment metadata.\n+///     - Its file format is an array of GinIndexSegment as defined in this file.\n+///     - postings_start_offset points to the file(.gin_post) starting position for the segment's postings list.\n+///     - term_dict_start_offset points to the file(.gin_dict) starting position for the segment's term dictionaries.\n+///  3. Term Dictionary file(.gin_dict): it contains term dictionaries.\n+///     - It contains an array of (FST_size, FST_blob) which has size and actual data of FST.\n+///  4. Postings Lists(.gin_post): it contains postings lists data.\n+///     - It contains an array of serialized postings lists.\n+///\n+/// During the searching in the segment, the segment's meta data can be found in .gin_seg file. From the meta data,\n+/// the starting position of its term dictionary is used to locate its FST. Then FST is read into memory.\n+/// By using the term and FST, the offset(\"output\" in FST) of the postings list for the term\n+/// in FST is found. The offset plus the postings_start_offset is the file location in .gin_post file\n+/// for its postings list.\n+\n+namespace DB\n+{\n+enum : uint8_t\n+{\n+    GIN_VERSION_0 = 0,\n+    GIN_VERSION_1 = 1, /// Initial version\n+};\n+\n+static constexpr auto CURRENT_GIN_FILE_FORMAT_VERSION = GIN_VERSION_1;\n+\n+/// GinIndexPostingsList which uses 32-bit Roaring\n+using GinIndexPostingsList = roaring::Roaring;\n+\n+using GinIndexPostingsListPtr = std::shared_ptr<GinIndexPostingsList>;\n+\n+/// Gin Index Postings List Builder.\n+class GinIndexPostingsBuilder\n+{\n+public:\n+    constexpr static int MIN_SIZE_FOR_ROARING_ENCODING = 16;\n+\n+    GinIndexPostingsBuilder(UInt64 limit);\n+\n+    /// Check whether a row_id is already added\n+    bool contains(UInt32 row_id) const;\n+\n+    /// Add a row_id into the builder\n+    void add(UInt32 row_id);\n+\n+    /// Check whether the builder is using roaring bitmap\n+    bool useRoaring() const;\n+\n+    /// Check whether the postings list has been flagged to contain all row ids\n+    bool containsAllRows() const;\n+\n+    /// Serialize the content of builder to given WriteBuffer, returns the bytes of serialized data\n+    UInt64 serialize(WriteBuffer &buffer) const;\n+\n+    /// Deserialize the postings list data from given ReadBuffer, return a pointer to the GinIndexPostingsList created by deserialization\n+    static GinIndexPostingsListPtr deserialize(ReadBuffer &buffer);\n+private:\n+    /// When the list length is no greater than MIN_SIZE_FOR_ROARING_ENCODING, array 'rowid_lst' is used\n+    std::array<UInt32, MIN_SIZE_FOR_ROARING_ENCODING> rowid_lst;\n+\n+    /// When the list length is greater than MIN_SIZE_FOR_ROARING_ENCODING, Roaring bitmap 'rowid_bitmap' is used\n+    roaring::Roaring rowid_bitmap;\n+\n+    /// rowid_lst_length stores the number of row IDs in 'rowid_lst' array, can also be a flag(0xFF) indicating that roaring bitmap is used\n+    UInt8 rowid_lst_length{0};\n+\n+    static constexpr UInt8 UsesBitMap = 0xFF;\n+    /// Clear the postings list and reset it with MATCHALL flags when the size of the postings list is beyond the limit\n+    UInt64 size_limit;\n+};\n+\n+/// Container for postings lists for each segment\n+using SegmentedPostingsListContainer = std::unordered_map<UInt32, GinIndexPostingsListPtr>;\n+\n+/// Postings lists and terms built from query string\n+using PostingsCache = std::unordered_map<std::string, SegmentedPostingsListContainer>;\n+using PostingsCachePtr = std::shared_ptr<PostingsCache>;\n+\n+/// Gin Index Segment information, which contains:\n+struct GinIndexSegment\n+{\n+    ///  Segment ID retrieved from next available ID from file .gin_sid\n+    UInt32 segment_id = 0;\n+\n+    /// Next row ID for this segment\n+    UInt32 next_row_id = 1;\n+\n+    /// .gin_post file offset of this segment's postings lists\n+    UInt64 postings_start_offset = 0;\n+\n+    /// .term_dict file offset of this segment's term dictionaries\n+    UInt64 term_dict_start_offset = 0;\n+};\n+\n+using GinIndexSegments = std::vector<GinIndexSegment>;\n+\n+struct SegmentTermDictionary\n+{\n+    /// .gin_post file offset of this segment's postings lists\n+    UInt64 postings_start_offset;\n+\n+    /// .gin_dict file offset of this segment's term dictionaries\n+    UInt64 term_dict_start_offset;\n+\n+    /// Finite State Transducer, which can be viewed as a map of <term, offset>, where offset is the\n+    /// offset to the term's posting list in postings list file\n+    FST::FiniteStateTransducer offsets;\n+};\n+\n+using SegmentTermDictionaryPtr = std::shared_ptr<SegmentTermDictionary>;\n+\n+/// Term dictionaries indexed by segment ID\n+using SegmentTermDictionaries = std::unordered_map<UInt32, SegmentTermDictionaryPtr>;\n+\n+/// Gin Index Store which has Gin Index meta data for the corresponding Data Part\n+class GinIndexStore\n+{\n+public:\n+    using GinIndexPostingsBuilderPtr = std::shared_ptr<GinIndexPostingsBuilder>;\n+    /// Container for all term's Gin Index Postings List Builder\n+    using GinIndexPostingsBuilderContainer = std::unordered_map<std::string, GinIndexPostingsBuilderPtr>;\n+\n+    explicit GinIndexStore(const String & name_, DataPartStoragePtr storage_);\n+\n+    GinIndexStore(const String& name_, DataPartStoragePtr storage_, MutableDataPartStoragePtr data_part_storage_builder_, UInt64 max_digestion_size_);\n+\n+    /// Check existence by checking the existence of file .gin_sid\n+    bool exists() const;\n+\n+    /// Get a range of next 'numIDs' available row IDs\n+    UInt32 getNextRowIDRange(size_t numIDs);\n+\n+    /// Get next available segment ID by updating file .gin_sid\n+    UInt32 getNextSegmentID();\n+\n+    /// Get total number of segments in the store\n+    UInt32 getNumOfSegments();\n+\n+    /// Get current postings list builder\n+    const GinIndexPostingsBuilderContainer& getPostings() const { return current_postings; }\n+\n+    /// Set postings list builder for given term\n+    void setPostingsBuilder(const String & term, GinIndexPostingsBuilderPtr builder) { current_postings[term] = builder; }\n+    /// Check if we need to write segment to Gin index files\n+    bool needToWrite() const;\n+\n+    /// Accumulate the size of text data which has been digested\n+    void incrementCurrentSizeBy(UInt64 sz) { current_size += sz; }\n+\n+    UInt32 getCurrentSegmentID() const { return current_segment.segment_id;}\n+\n+    /// Do last segment writing\n+    void finalize();\n+\n+    /// method for writing segment data to Gin index files\n+    void writeSegment();\n+\n+    const String & getName() const {return name;}\n+\n+private:\n+    friend class GinIndexStoreDeserializer;\n+\n+    /// Initialize all indexing files for this store\n+    void initFileStreams();\n+\n+    /// Get a range of next available segment IDs by updating file .gin_sid\n+    UInt32 getNextSegmentIDRange(const String &file_name, size_t n);\n+\n+    String name;\n+    DataPartStoragePtr storage;\n+    MutableDataPartStoragePtr data_part_storage_builder;\n+\n+    UInt32 cached_segment_num = 0;\n+\n+    std::mutex gin_index_store_mutex;\n+\n+    /// Terms dictionaries which are loaded from .gin_dict files\n+    SegmentTermDictionaries term_dicts;\n+\n+    /// container for building postings lists during index construction\n+    GinIndexPostingsBuilderContainer current_postings;\n+\n+    /// The following is for segmentation of Gin index\n+    GinIndexSegment current_segment{};\n+    UInt64 current_size = 0;\n+    const UInt64 max_digestion_size = 0;\n+\n+    /// File streams for segment, term dictionaries and postings lists\n+    std::unique_ptr<WriteBufferFromFileBase> segment_file_stream;\n+    std::unique_ptr<WriteBufferFromFileBase> term_dict_file_stream;\n+    std::unique_ptr<WriteBufferFromFileBase> postings_file_stream;\n+\n+    static constexpr auto GIN_SEGMENT_ID_FILE_TYPE = \".gin_sid\";\n+    static constexpr auto GIN_SEGMENT_FILE_TYPE = \".gin_seg\";\n+    static constexpr auto GIN_DICTIONARY_FILE_TYPE = \".gin_dict\";\n+    static constexpr auto GIN_POSTINGS_FILE_TYPE = \".gin_post\";\n+};\n+\n+using GinIndexStorePtr = std::shared_ptr<GinIndexStore>;\n+\n+/// GinIndexStores indexed by part file path\n+using GinIndexStores = std::unordered_map<std::string, GinIndexStorePtr>;\n+\n+/// PostingsCacheForStore contains postings lists from 'store' which are retrieved from Gin index files for the terms in query strings\n+/// PostingsCache is per query string(one query can have multiple query strings): when skipping index(row ID ranges) is used for the part during the\n+/// query, the postings cache is created and associated with the store where postings lists are read\n+/// for the tokenized query string. The postings caches are released automatically when the query is done.\n+struct PostingsCacheForStore\n+{\n+    /// Which store to retrieve postings lists\n+    GinIndexStorePtr store;\n+\n+    /// map of <query, postings lists>\n+    std::unordered_map<String, PostingsCachePtr> cache;\n+\n+    /// Get postings lists for query string, return nullptr if not found\n+    PostingsCachePtr getPostings(const String &query_string) const\n+    {\n+        auto it {cache.find(query_string)};\n+\n+        if (it == cache.cend())\n+        {\n+            return nullptr;\n+        }\n+        return it->second;\n+    }\n+};\n+\n+/// GinIndexStore Factory, which is a singleton for storing GinIndexStores\n+class GinIndexStoreFactory : private boost::noncopyable\n+{\n+public:\n+    /// Get singleton of GinIndexStoreFactory\n+    static GinIndexStoreFactory& instance();\n+\n+    /// Get GinIndexStore by using index name, disk and part_path (which are combined to create key in stores)\n+    GinIndexStorePtr get(const String& name, DataPartStoragePtr storage);\n+\n+    /// Remove all Gin index files which are under the same part_path\n+    void remove(const String& part_path);\n+\n+private:\n+    GinIndexStores stores;\n+    std::mutex stores_mutex;\n+};\n+\n+/// Term dictionary information, which contains:\n+\n+/// Gin Index Store Reader which helps to read segments, term dictionaries and postings list\n+class GinIndexStoreDeserializer : private boost::noncopyable\n+{\n+public:\n+    explicit GinIndexStoreDeserializer(const GinIndexStorePtr & store_);\n+\n+    /// Read all segment information from .gin_seg files\n+    void readSegments();\n+\n+    /// Read all term dictionaries from .gin_dict files\n+    void readSegmentTermDictionaries();\n+\n+    /// Read term dictionary for given segment id\n+    void readSegmentTermDictionary(UInt32 segment_id);\n+\n+    /// Read postings lists for the term\n+    SegmentedPostingsListContainer readSegmentedPostingsLists(const String& term);\n+\n+    /// Read postings lists for terms(which are created by tokenzing query string)\n+    PostingsCachePtr createPostingsCacheFromTerms(const std::vector<String>& terms);\n+\n+private:\n+    /// Initialize Gin index files\n+    void initFileStreams();\n+\n+    /// The store for the reader\n+    GinIndexStorePtr store;\n+\n+    /// File streams for reading Gin Index\n+    std::unique_ptr<ReadBufferFromFileBase> segment_file_stream;\n+    std::unique_ptr<ReadBufferFromFileBase> term_dict_file_stream;\n+    std::unique_ptr<ReadBufferFromFileBase> postings_file_stream;\n+\n+    /// Current segment, used in building index\n+    GinIndexSegment current_segment;\n+};\n+\n+}\ndiff --git a/src/Storages/MergeTree/IDataPartStorage.h b/src/Storages/MergeTree/IDataPartStorage.h\nindex d7c0c9c76e3f..70cc4d3fe702 100644\n--- a/src/Storages/MergeTree/IDataPartStorage.h\n+++ b/src/Storages/MergeTree/IDataPartStorage.h\n@@ -1,5 +1,6 @@\n #pragma once\n #include <IO/ReadSettings.h>\n+#include <IO/WriteSettings.h>\n #include <base/types.h>\n #include <Core/NamesAndTypes.h>\n #include <Interpreters/TransactionVersionMetadata.h>\n@@ -215,6 +216,7 @@ class IDataPartStorage : public boost::noncopyable\n         const String & name,\n         size_t buf_size,\n         const WriteSettings & settings) = 0;\n+    virtual std::unique_ptr<WriteBufferFromFileBase> writeFile(const String & name, size_t buf_size, WriteMode mode, const WriteSettings & settings) = 0;\n \n     /// A special const method to write transaction file.\n     /// It's const, because file with transaction metadata\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 98d0fa3de308..ec2ea4482906 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -1673,6 +1673,8 @@ void IMergeTreeDataPart::remove()\n     metadata_manager->deleteAll(false);\n     metadata_manager->assertAllDeleted(false);\n \n+    GinIndexStoreFactory::instance().remove(getDataPartStoragePtr()->getRelativePath());\n+\n     std::list<IDataPartStorage::ProjectionChecksums> projection_checksums;\n \n     for (const auto & [p_name, projection_part] : projection_parts)\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex b1384dc799fd..9659ec2177cb 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -2774,6 +2774,13 @@ void MergeTreeData::checkAlterIsPossible(const AlterCommands & commands, Context\n         if (!mutation_commands.empty())\n             throw Exception(ErrorCodes::ALTER_OF_COLUMN_IS_FORBIDDEN, \"The following alter commands: '{}' will modify data on disk, but setting `allow_non_metadata_alters` is disabled\", queryToString(mutation_commands.ast()));\n     }\n+\n+    if (commands.hasInvertedIndex(new_metadata, getContext()) && !settings.allow_experimental_inverted_index)\n+    {\n+        throw Exception(\n+                \"Experimental Inverted Index feature is not enabled (the setting 'allow_experimental_inverted_index')\",\n+                ErrorCodes::SUPPORT_IS_DISABLED);\n+    }\n     commands.apply(new_metadata, getContext());\n \n     /// Set of columns that shouldn't be altered.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\nindex fbcf8cb241c1..e9629f83d096 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp\n@@ -1,6 +1,6 @@\n #include <Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h>\n+#include <Storages/MergeTree/MergeTreeIndexGin.h>\n #include <Common/MemoryTrackerBlockerInThread.h>\n-\n #include <utility>\n #include \"IO/WriteBufferFromFileDecorator.h\"\n \n@@ -212,7 +212,14 @@ void MergeTreeDataPartWriterOnDisk::initSkipIndices()\n                         default_codec, settings.max_compress_block_size,\n                         marks_compression_codec, settings.marks_compress_block_size,\n                         settings.query_write_settings));\n-        skip_indices_aggregators.push_back(index_helper->createIndexAggregator());\n+\n+        GinIndexStorePtr store = nullptr;\n+        if (dynamic_cast<const MergeTreeIndexGinFilter *>(&*index_helper) != nullptr)\n+        {\n+            store = std::make_shared<GinIndexStore>(stream_name, data_part->getDataPartStoragePtr(), data_part->getDataPartStoragePtr(), storage.getSettings()->max_digestion_size_per_segment);\n+            gin_index_stores[stream_name] = store;\n+        }\n+        skip_indices_aggregators.push_back(index_helper->createIndexAggregatorForPart(store));\n         skip_index_accumulated_marks.push_back(0);\n     }\n }\n@@ -268,6 +275,18 @@ void MergeTreeDataPartWriterOnDisk::calculateAndSerializeSkipIndices(const Block\n         auto & stream = *skip_indices_streams[i];\n         WriteBuffer & marks_out = stream.compress_marks ? stream.marks_compressed_hashing : stream.marks_hashing;\n \n+        GinIndexStorePtr store = nullptr;\n+        if (dynamic_cast<const MergeTreeIndexGinFilter *>(&*index_helper) != nullptr)\n+        {\n+            String stream_name = index_helper->getFileName();\n+            auto it = gin_index_stores.find(stream_name);\n+            if (it == gin_index_stores.cend())\n+            {\n+                throw Exception(\"Index '\" + stream_name + \"' does not exist\", ErrorCodes::LOGICAL_ERROR);\n+            }\n+            store = it->second;\n+        }\n+\n         for (const auto & granule : granules_to_write)\n         {\n             if (skip_index_accumulated_marks[i] == index_helper->index.granularity)\n@@ -278,7 +297,7 @@ void MergeTreeDataPartWriterOnDisk::calculateAndSerializeSkipIndices(const Block\n \n             if (skip_indices_aggregators[i]->empty() && granule.mark_on_start)\n             {\n-                skip_indices_aggregators[i] = index_helper->createIndexAggregator();\n+                skip_indices_aggregators[i] = index_helper->createIndexAggregatorForPart(store);\n \n                 if (stream.compressed_hashing.offset() >= settings.min_compress_block_size)\n                     stream.compressed_hashing.next();\n@@ -380,7 +399,11 @@ void MergeTreeDataPartWriterOnDisk::finishSkipIndicesSerialization(bool sync)\n         if (sync)\n             stream->sync();\n     }\n-\n+    for (auto & store: gin_index_stores)\n+    {\n+        store.second->finalize();\n+    }\n+    gin_index_stores.clear();\n     skip_indices_streams.clear();\n     skip_indices_aggregators.clear();\n     skip_index_accumulated_marks.clear();\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\nindex ab1adfe7f590..2377a129ac04 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h\n@@ -162,6 +162,7 @@ class MergeTreeDataPartWriterOnDisk : public IMergeTreeDataPartWriter\n     /// Data is already written up to this mark.\n     size_t current_mark = 0;\n \n+    GinIndexStores gin_index_stores;\n private:\n     void initSkipIndices();\n     void initPrimaryIndex();\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex 1ca1779e4b0d..9cae53c71c77 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -9,6 +9,7 @@\n #include <Storages/MergeTree/KeyCondition.h>\n #include <Storages/MergeTree/MergeTreeDataPartUUID.h>\n #include <Storages/MergeTree/StorageFromMergeTreeDataPart.h>\n+#include <Storages/MergeTree/MergeTreeIndexGin.h>\n #include <Storages/ReadInOrderOptimizer.h>\n #include <Storages/VirtualColumnUtils.h>\n #include <Parsers/ASTIdentifier.h>\n@@ -1687,6 +1688,14 @@ MarkRanges MergeTreeDataSelectExecutor::filterMarksUsingIndex(\n     /// this variable is stored to avoid reading the same granule twice.\n     MergeTreeIndexGranulePtr granule = nullptr;\n     size_t last_index_mark = 0;\n+\n+    PostingsCacheForStore cache_in_store;\n+\n+    if (dynamic_cast<const MergeTreeIndexGinFilter *>(&*index_helper) != nullptr)\n+    {\n+        cache_in_store.store = GinIndexStoreFactory::instance().get(index_helper->getFileName(), part->getDataPartStoragePtr());\n+    }\n+\n     for (size_t i = 0; i < ranges.size(); ++i)\n     {\n         const MarkRange & index_range = index_ranges[i];\n@@ -1700,6 +1709,7 @@ MarkRanges MergeTreeDataSelectExecutor::filterMarksUsingIndex(\n         {\n             if (index_mark != index_range.begin || !granule || last_index_mark != index_range.begin)\n                 granule = reader.read();\n+            const auto * gin_filter_condition = dynamic_cast<const MergeTreeConditionGinFilter *>(&*condition);\n             // Cast to Ann condition\n             auto ann_condition = std::dynamic_pointer_cast<ApproximateNearestNeighbour::IMergeTreeIndexConditionAnn>(condition);\n             if (ann_condition != nullptr)\n@@ -1726,7 +1736,13 @@ MarkRanges MergeTreeDataSelectExecutor::filterMarksUsingIndex(\n                 continue;\n             }\n \n-            if (!condition->mayBeTrueOnGranule(granule))\n+            bool result{false};\n+            if (!gin_filter_condition)\n+                result = condition->mayBeTrueOnGranule(granule);\n+            else\n+                result = cache_in_store.store ? gin_filter_condition->mayBeTrueOnGranuleInPart(granule, cache_in_store) : true;\n+\n+            if (!result)\n             {\n                 ++granules_dropped;\n                 continue;\ndiff --git a/src/Storages/MergeTree/MergeTreeIndexFullText.cpp b/src/Storages/MergeTree/MergeTreeIndexFullText.cpp\nindex fd98b0597d49..411141f028be 100644\n--- a/src/Storages/MergeTree/MergeTreeIndexFullText.cpp\n+++ b/src/Storages/MergeTree/MergeTreeIndexFullText.cpp\n@@ -183,6 +183,7 @@ MergeTreeConditionFullText::MergeTreeConditionFullText(\n     rpn = std::move(builder).extractRPN();\n }\n \n+/// Keep in-sync with MergeTreeConditionGinFilter::alwaysUnknownOrTrue\n bool MergeTreeConditionFullText::alwaysUnknownOrTrue() const\n {\n     /// Check like in KeyCondition.\ndiff --git a/src/Storages/MergeTree/MergeTreeIndexGin.cpp b/src/Storages/MergeTree/MergeTreeIndexGin.cpp\nnew file mode 100644\nindex 000000000000..26f3fcb4fb6f\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreeIndexGin.cpp\n@@ -0,0 +1,786 @@\n+\n+#include <algorithm>\n+\n+#include <Columns/ColumnArray.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <IO/WriteHelpers.h>\n+#include <IO/ReadHelpers.h>\n+#include <Interpreters/GinFilter.h>\n+#include <Interpreters/ExpressionActions.h>\n+#include <Interpreters/ExpressionAnalyzer.h>\n+#include <Interpreters/TreeRewriter.h>\n+#include <Interpreters/misc.h>\n+#include <Storages/MergeTree/MergeTreeData.h>\n+#include <Storages/MergeTree/RPNBuilder.h>\n+#include <Storages/MergeTree/MergeTreeIndexGin.h>\n+#include <Storages/MergeTree/MergeTreeIndexUtils.h>\n+#include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Parsers/ASTSubquery.h>\n+#include <Core/Defines.h>\n+\n+#include <Poco/Logger.h>\n+#include <Columns/ColumnNullable.h>\n+#include <Columns/ColumnLowCardinality.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/DataTypeLowCardinality.h>\n+\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int INCORRECT_QUERY;\n+}\n+\n+MergeTreeIndexGranuleGinFilter::MergeTreeIndexGranuleGinFilter(\n+    const String & index_name_,\n+    size_t columns_number,\n+    const GinFilterParameters & params_)\n+    : index_name(index_name_)\n+    , params(params_)\n+    , gin_filters(\n+        columns_number, GinFilter(params))\n+    , has_elems(false)\n+{\n+}\n+\n+void MergeTreeIndexGranuleGinFilter::serializeBinary(WriteBuffer & ostr) const\n+{\n+    if (empty())\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Attempt to write empty fulltext index {}.\", backQuote(index_name));\n+\n+    const auto & size_type = std::make_shared<DataTypeUInt32>();\n+    auto size_serialization = size_type->getDefaultSerialization();\n+\n+    for (const auto & gin_filter : gin_filters)\n+    {\n+        size_t filter_size = gin_filter.getFilter().size();\n+        size_serialization->serializeBinary(filter_size, ostr, {});\n+        ostr.write(reinterpret_cast<const char*>(gin_filter.getFilter().data()), filter_size * sizeof(GinFilter::GinSegmentWithRowIDRanges::value_type));\n+    }\n+}\n+\n+void MergeTreeIndexGranuleGinFilter::deserializeBinary(ReadBuffer & istr, MergeTreeIndexVersion version)\n+{\n+    if (version != 1)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Unknown index version {}.\", version);\n+\n+    Field field_rows;\n+    const auto & size_type = std::make_shared<DataTypeUInt32>();\n+\n+    auto size_serialization = size_type->getDefaultSerialization();\n+    for (auto & gin_filter : gin_filters)\n+    {\n+        size_serialization->deserializeBinary(field_rows, istr, {});\n+        size_t filter_size = field_rows.get<size_t>();\n+\n+        if (filter_size == 0)\n+            continue;\n+\n+        gin_filter.getFilter().assign(filter_size, {});\n+        istr.readStrict(reinterpret_cast<char*>(gin_filter.getFilter().data()), filter_size * sizeof(GinFilter::GinSegmentWithRowIDRanges::value_type));\n+    }\n+    has_elems = true;\n+}\n+\n+\n+MergeTreeIndexAggregatorGinFilter::MergeTreeIndexAggregatorGinFilter(\n+    GinIndexStorePtr store_,\n+    const Names & index_columns_,\n+    const String & index_name_,\n+    const GinFilterParameters & params_,\n+    TokenExtractorPtr token_extractor_)\n+    : store(store_)\n+    , index_columns(index_columns_)\n+    , index_name (index_name_)\n+    , params(params_)\n+    , token_extractor(token_extractor_)\n+    , granule(\n+        std::make_shared<MergeTreeIndexGranuleGinFilter>(\n+            index_name, index_columns.size(), params))\n+{\n+}\n+\n+MergeTreeIndexGranulePtr MergeTreeIndexAggregatorGinFilter::getGranuleAndReset()\n+{\n+    auto new_granule = std::make_shared<MergeTreeIndexGranuleGinFilter>(\n+        index_name, index_columns.size(), params);\n+    new_granule.swap(granule);\n+    return new_granule;\n+}\n+\n+void MergeTreeIndexAggregatorGinFilter::addToGinFilter(UInt32 rowID, const char* data, size_t length, GinFilter& gin_filter, UInt64 limit)\n+{\n+    size_t cur = 0;\n+    size_t token_start = 0;\n+    size_t token_len = 0;\n+\n+    while (cur < length && token_extractor->nextInStringPadded(data, length, &cur, &token_start, &token_len))\n+    {\n+        gin_filter.add(data + token_start, token_len, rowID, store, limit);\n+    }\n+}\n+\n+void MergeTreeIndexAggregatorGinFilter::update(const Block & block, size_t * pos, size_t limit)\n+{\n+    if (*pos >= block.rows())\n+        throw Exception(\n+                \"The provided position is not less than the number of block rows. Position: \"\n+                + toString(*pos) + \", Block rows: \" + toString(block.rows()) + \".\", ErrorCodes::LOGICAL_ERROR);\n+\n+    size_t rows_read = std::min(limit, block.rows() - *pos);\n+    auto row_id = store->getNextRowIDRange(rows_read);\n+    auto start_row_id = row_id;\n+\n+    for (size_t col = 0; col < index_columns.size(); ++col)\n+    {\n+        const auto & column_with_type = block.getByName(index_columns[col]);\n+        const auto & column = column_with_type.column;\n+        size_t current_position = *pos;\n+\n+        bool need_to_write = false;\n+        if (isArray(column_with_type.type))\n+        {\n+            const auto & column_array = assert_cast<const ColumnArray &>(*column);\n+            const auto & column_offsets = column_array.getOffsets();\n+            const auto & column_key = column_array.getData();\n+\n+            for (size_t i = 0; i < rows_read; ++i)\n+            {\n+                size_t element_start_row = column_offsets[current_position - 1];\n+                size_t elements_size = column_offsets[current_position] - element_start_row;\n+\n+                for (size_t row_num = 0; row_num < elements_size; ++row_num)\n+                {\n+                    auto ref = column_key.getDataAt(element_start_row + row_num);\n+                    addToGinFilter(row_id, ref.data, ref.size, granule->gin_filters[col], rows_read);\n+                    store->incrementCurrentSizeBy(ref.size);\n+                }\n+                current_position += 1;\n+                row_id++;\n+\n+                if (store->needToWrite())\n+                    need_to_write = true;\n+            }\n+        }\n+        else\n+        {\n+            for (size_t i = 0; i < rows_read; ++i)\n+            {\n+                auto ref = column->getDataAt(current_position + i);\n+                addToGinFilter(row_id, ref.data, ref.size, granule->gin_filters[col], rows_read);\n+                store->incrementCurrentSizeBy(ref.size);\n+                row_id++;\n+                if (store->needToWrite())\n+                    need_to_write = true;\n+            }\n+        }\n+        granule->gin_filters[col].addRowRangeToGinFilter(store->getCurrentSegmentID(), start_row_id, static_cast<UInt32>(start_row_id + rows_read - 1));\n+        if (need_to_write)\n+        {\n+            store->writeSegment();\n+        }\n+    }\n+\n+    granule->has_elems = true;\n+    *pos += rows_read;\n+}\n+\n+MergeTreeConditionGinFilter::MergeTreeConditionGinFilter(\n+    const SelectQueryInfo & query_info,\n+    ContextPtr context_,\n+    const Block & index_sample_block,\n+    const GinFilterParameters & params_,\n+    TokenExtractorPtr token_extactor_)\n+    :  WithContext(context_), header(index_sample_block)\n+    , params(params_)\n+    , token_extractor(token_extactor_)\n+    , prepared_sets(query_info.prepared_sets)\n+{\n+    if (context_->getSettingsRef().allow_experimental_analyzer)\n+    {\n+        if (!query_info.filter_actions_dag)\n+        {\n+            rpn.push_back(RPNElement::FUNCTION_UNKNOWN);\n+            return;\n+        }\n+        rpn = std::move(\n+                RPNBuilder<RPNElement>(\n+                        query_info.filter_actions_dag->getOutputs().at(0), context_,\n+                        [&](const RPNBuilderTreeNode & node, RPNElement & out)\n+                        {\n+                            return this->traverseAtomAST(node, out);\n+                        }).extractRPN());\n+    }\n+\n+    ASTPtr filter_node = buildFilterNode(query_info.query);\n+\n+    if (!filter_node)\n+    {\n+        rpn.push_back(RPNElement::FUNCTION_UNKNOWN);\n+        return;\n+    }\n+\n+    auto block_with_constants = KeyCondition::getBlockWithConstants(query_info.query, query_info.syntax_analyzer_result, context_);\n+    RPNBuilder<RPNElement> builder(\n+        filter_node,\n+        context_,\n+        std::move(block_with_constants),\n+        query_info.prepared_sets,\n+        [&](const RPNBuilderTreeNode & node, RPNElement & out) { return traverseAtomAST(node, out); });\n+    rpn = std::move(builder).extractRPN();\n+\n+}\n+\n+/// Keep in-sync with MergeTreeConditionFullText::alwaysUnknownOrTrue\n+bool MergeTreeConditionGinFilter::alwaysUnknownOrTrue() const\n+{\n+    /// Check like in KeyCondition.\n+    std::vector<bool> rpn_stack;\n+\n+    for (const auto & element : rpn)\n+    {\n+        if (element.function == RPNElement::FUNCTION_UNKNOWN\n+            || element.function == RPNElement::ALWAYS_TRUE)\n+        {\n+            rpn_stack.push_back(true);\n+        }\n+        else if (element.function == RPNElement::FUNCTION_EQUALS\n+             || element.function == RPNElement::FUNCTION_NOT_EQUALS\n+             || element.function == RPNElement::FUNCTION_HAS\n+             || element.function == RPNElement::FUNCTION_IN\n+             || element.function == RPNElement::FUNCTION_NOT_IN\n+             || element.function == RPNElement::FUNCTION_MULTI_SEARCH\n+             || element.function == RPNElement::ALWAYS_FALSE)\n+        {\n+            rpn_stack.push_back(false);\n+        }\n+        else if (element.function == RPNElement::FUNCTION_NOT)\n+        {\n+            // do nothing\n+        }\n+        else if (element.function == RPNElement::FUNCTION_AND)\n+        {\n+            auto arg1 = rpn_stack.back();\n+            rpn_stack.pop_back();\n+            auto arg2 = rpn_stack.back();\n+            rpn_stack.back() = arg1 && arg2;\n+        }\n+        else if (element.function == RPNElement::FUNCTION_OR)\n+        {\n+            auto arg1 = rpn_stack.back();\n+            rpn_stack.pop_back();\n+            auto arg2 = rpn_stack.back();\n+            rpn_stack.back() = arg1 || arg2;\n+        }\n+        else\n+            throw Exception(\"Unexpected function type in KeyCondition::RPNElement\", ErrorCodes::LOGICAL_ERROR);\n+    }\n+\n+    return rpn_stack[0];\n+}\n+\n+bool MergeTreeConditionGinFilter::mayBeTrueOnGranuleInPart(MergeTreeIndexGranulePtr idx_granule,[[maybe_unused]] PostingsCacheForStore &cache_store) const\n+{\n+    std::shared_ptr<MergeTreeIndexGranuleGinFilter> granule\n+            = std::dynamic_pointer_cast<MergeTreeIndexGranuleGinFilter>(idx_granule);\n+    if (!granule)\n+        throw Exception(\n+                \"GinFilter index condition got a granule with the wrong type.\", ErrorCodes::LOGICAL_ERROR);\n+\n+    /// Check like in KeyCondition.\n+    std::vector<BoolMask> rpn_stack;\n+    for (const auto & element : rpn)\n+    {\n+        if (element.function == RPNElement::FUNCTION_UNKNOWN)\n+        {\n+            rpn_stack.emplace_back(true, true);\n+        }\n+        else if (element.function == RPNElement::FUNCTION_EQUALS\n+             || element.function == RPNElement::FUNCTION_NOT_EQUALS\n+             || element.function == RPNElement::FUNCTION_HAS)\n+        {\n+            rpn_stack.emplace_back(granule->gin_filters[element.key_column].contains(*element.gin_filter, cache_store), true);\n+\n+            if (element.function == RPNElement::FUNCTION_NOT_EQUALS)\n+                rpn_stack.back() = !rpn_stack.back();\n+        }\n+        else if (element.function == RPNElement::FUNCTION_IN\n+             || element.function == RPNElement::FUNCTION_NOT_IN)\n+        {\n+            std::vector<bool> result(element.set_gin_filters.back().size(), true);\n+\n+            for (size_t column = 0; column < element.set_key_position.size(); ++column)\n+            {\n+                const size_t key_idx = element.set_key_position[column];\n+\n+                const auto & gin_filters = element.set_gin_filters[column];\n+                for (size_t row = 0; row < gin_filters.size(); ++row)\n+                    result[row] = result[row] && granule->gin_filters[key_idx].contains(gin_filters[row], cache_store);\n+            }\n+\n+            rpn_stack.emplace_back(\n+                    std::find(std::cbegin(result), std::cend(result), true) != std::end(result), true);\n+            if (element.function == RPNElement::FUNCTION_NOT_IN)\n+                rpn_stack.back() = !rpn_stack.back();\n+        }\n+        else if (element.function == RPNElement::FUNCTION_MULTI_SEARCH)\n+        {\n+            std::vector<bool> result(element.set_gin_filters.back().size(), true);\n+\n+            const auto & gin_filters = element.set_gin_filters[0];\n+\n+            for (size_t row = 0; row < gin_filters.size(); ++row)\n+                result[row] = result[row] && granule->gin_filters[element.key_column].contains(gin_filters[row], cache_store);\n+\n+            rpn_stack.emplace_back(\n+                    std::find(std::cbegin(result), std::cend(result), true) != std::end(result), true);\n+        }\n+        else if (element.function == RPNElement::FUNCTION_NOT)\n+        {\n+            rpn_stack.back() = !rpn_stack.back();\n+        }\n+        else if (element.function == RPNElement::FUNCTION_AND)\n+        {\n+            auto arg1 = rpn_stack.back();\n+            rpn_stack.pop_back();\n+            auto arg2 = rpn_stack.back();\n+            rpn_stack.back() = arg1 & arg2;\n+        }\n+        else if (element.function == RPNElement::FUNCTION_OR)\n+        {\n+            auto arg1 = rpn_stack.back();\n+            rpn_stack.pop_back();\n+            auto arg2 = rpn_stack.back();\n+            rpn_stack.back() = arg1 | arg2;\n+        }\n+        else if (element.function == RPNElement::ALWAYS_FALSE)\n+        {\n+            rpn_stack.emplace_back(false, true);\n+        }\n+        else if (element.function == RPNElement::ALWAYS_TRUE)\n+        {\n+            rpn_stack.emplace_back(true, false);\n+        }\n+        else\n+            throw Exception(\"Unexpected function type in GinFilterCondition::RPNElement\", ErrorCodes::LOGICAL_ERROR);\n+    }\n+\n+    if (rpn_stack.size() != 1)\n+        throw Exception(\"Unexpected stack size in GinFilterCondition::mayBeTrueOnGranule\", ErrorCodes::LOGICAL_ERROR);\n+\n+    return rpn_stack[0].can_be_true;\n+}\n+\n+bool MergeTreeConditionGinFilter::traverseAtomAST(const RPNBuilderTreeNode & node, RPNElement & out)\n+{\n+    {\n+        Field const_value;\n+        DataTypePtr const_type;\n+\n+        if (node.tryGetConstant(const_value, const_type))\n+        {\n+            /// Check constant like in KeyCondition\n+            if (const_value.getType() == Field::Types::UInt64\n+                || const_value.getType() == Field::Types::Int64\n+                || const_value.getType() == Field::Types::Float64)\n+            {\n+                /// Zero in all types is represented in memory the same way as in UInt64.\n+                out.function = const_value.get<UInt64>()\n+                            ? RPNElement::ALWAYS_TRUE\n+                            : RPNElement::ALWAYS_FALSE;\n+\n+                return true;\n+            }\n+        }\n+    }\n+\n+    if (node.isFunction())\n+    {\n+        const auto function = node.toFunctionNode();\n+        // auto arguments_size = function.getArgumentsSize();\n+        auto function_name = function.getFunctionName();\n+\n+        size_t function_arguments_size = function.getArgumentsSize();\n+        if (function_arguments_size != 2)\n+            return false;\n+        auto lhs_argument = function.getArgumentAt(0);\n+        auto rhs_argument = function.getArgumentAt(1);\n+\n+        if (functionIsInOrGlobalInOperator(function_name))\n+        {\n+            if (tryPrepareSetGinFilter(lhs_argument, rhs_argument, out))\n+            {\n+                if (function_name == \"notIn\")\n+                {\n+                    out.function = RPNElement::FUNCTION_NOT_IN;\n+                    return true;\n+                }\n+                else if (function_name == \"in\")\n+                {\n+                    out.function = RPNElement::FUNCTION_IN;\n+                    return true;\n+                }\n+            }\n+        }\n+        else if (function_name == \"equals\" ||\n+                 function_name == \"notEquals\" ||\n+                 function_name == \"has\" ||\n+                 function_name == \"mapContains\" ||\n+                 function_name == \"like\" ||\n+                 function_name == \"notLike\" ||\n+                 function_name == \"hasToken\" ||\n+                 function_name == \"startsWith\" ||\n+                 function_name == \"endsWith\" ||\n+                 function_name == \"multiSearchAny\")\n+        {\n+            Field const_value;\n+            DataTypePtr const_type;\n+            if (rhs_argument.tryGetConstant(const_value, const_type))\n+            {\n+                if (traverseASTEquals(function_name, lhs_argument, const_type, const_value, out))\n+                    return true;\n+            }\n+            else if (lhs_argument.tryGetConstant(const_value, const_type) && (function_name == \"equals\" || function_name == \"notEquals\"))\n+            {\n+                if (traverseASTEquals(function_name, rhs_argument, const_type, const_value, out))\n+                    return true;\n+            }\n+        }\n+    }\n+\n+    return false;\n+}\n+\n+bool MergeTreeConditionGinFilter::traverseASTEquals(\n+    const String & function_name,\n+    const RPNBuilderTreeNode & key_ast,\n+    const DataTypePtr & value_type,\n+    const Field & value_field,\n+    RPNElement & out)\n+{\n+    auto value_data_type = WhichDataType(value_type);\n+    if (!value_data_type.isStringOrFixedString() && !value_data_type.isArray())\n+        return false;\n+\n+    Field const_value = value_field;\n+    size_t key_column_num = 0;\n+    bool key_exists = header.has(key_ast.getColumnName());\n+    bool map_key_exists = header.has(fmt::format(\"mapKeys({})\", key_ast.getColumnName()));\n+\n+    if (key_ast.isFunction())\n+    {\n+        const auto function = key_ast.toFunctionNode();\n+        if (function.getFunctionName() == \"arrayElement\")\n+        {\n+            /** Try to parse arrayElement for mapKeys index.\n+              * It is important to ignore keys like column_map['Key'] = '' because if key does not exists in map\n+              * we return default value for arrayElement.\n+              *\n+              * We cannot skip keys that does not exist in map if comparison is with default type value because\n+              * that way we skip necessary granules where map key does not exists.\n+              */\n+            if (value_field == value_type->getDefault())\n+                return false;\n+\n+            auto first_argument = function.getArgumentAt(0);\n+            const auto map_column_name = first_argument.getColumnName();\n+            auto map_keys_index_column_name = fmt::format(\"mapKeys({})\", map_column_name);\n+            auto map_values_index_column_name = fmt::format(\"mapValues({})\", map_column_name);\n+\n+            if (header.has(map_keys_index_column_name))\n+            {\n+                auto argument = function.getArgumentAt(1);\n+                DataTypePtr const_type;\n+                if (argument.tryGetConstant(const_value, const_type))\n+                {\n+                    key_column_num = header.getPositionByName(map_keys_index_column_name);\n+                    key_exists = true;\n+                }\n+                else\n+                {\n+                    return false;\n+                }\n+            }\n+            else if (header.has(map_values_index_column_name))\n+            {\n+                key_column_num = header.getPositionByName(map_values_index_column_name);\n+                key_exists = true;\n+            }\n+            else\n+            {\n+                return false;\n+            }\n+        }\n+    }\n+\n+    if (!key_exists && !map_key_exists)\n+        return false;\n+\n+    if (map_key_exists && (function_name == \"has\" || function_name == \"mapContains\"))\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_HAS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        auto & value = const_value.get<String>();\n+        token_extractor->stringToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"has\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_HAS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        auto & value = const_value.get<String>();\n+        token_extractor->stringToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+\n+    if (function_name == \"notEquals\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_NOT_EQUALS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        const auto & value = const_value.get<String>();\n+        token_extractor->stringToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"equals\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_EQUALS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        const auto & value = const_value.get<String>();\n+        token_extractor->stringToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"like\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_EQUALS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        const auto & value = const_value.get<String>();\n+        token_extractor->stringLikeToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"notLike\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_NOT_EQUALS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        const auto & value = const_value.get<String>();\n+        token_extractor->stringLikeToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"hasToken\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_EQUALS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        const auto & value = const_value.get<String>();\n+        token_extractor->stringToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"startsWith\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_EQUALS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        const auto & value = const_value.get<String>();\n+        token_extractor->stringToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"endsWith\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_EQUALS;\n+        out.gin_filter = std::make_unique<GinFilter>(params);\n+        const auto & value = const_value.get<String>();\n+        token_extractor->stringToGinFilter(value.data(), value.size(), *out.gin_filter);\n+        return true;\n+    }\n+    else if (function_name == \"multiSearchAny\")\n+    {\n+        out.key_column = key_column_num;\n+        out.function = RPNElement::FUNCTION_MULTI_SEARCH;\n+\n+        /// 2d vector is not needed here but is used because already exists for FUNCTION_IN\n+        std::vector<std::vector<GinFilter>> gin_filters;\n+        gin_filters.emplace_back();\n+        for (const auto & element : const_value.get<Array>())\n+        {\n+            if (element.getType() != Field::Types::String)\n+                return false;\n+\n+            gin_filters.back().emplace_back(params);\n+            const auto & value = element.get<String>();\n+            token_extractor->stringToGinFilter(value.data(), value.size(), gin_filters.back().back());\n+        }\n+        out.set_gin_filters = std::move(gin_filters);\n+        return true;\n+    }\n+\n+    return false;\n+}\n+\n+bool MergeTreeConditionGinFilter::tryPrepareSetGinFilter(\n+    const RPNBuilderTreeNode & lhs,\n+    const RPNBuilderTreeNode & rhs,\n+    RPNElement & out)\n+{\n+    std::vector<KeyTuplePositionMapping> key_tuple_mapping;\n+    DataTypes data_types;\n+\n+    if (lhs.isFunction() && lhs.toFunctionNode().getFunctionName() == \"tuple\")\n+    {\n+        const auto function = lhs.toFunctionNode();\n+        auto arguments_size = function.getArgumentsSize();\n+        for (size_t i = 0; i < arguments_size; ++i)\n+        {\n+            if (header.has(function.getArgumentAt(i).getColumnName()))\n+            {\n+                auto key = header.getPositionByName(function.getArgumentAt(i).getColumnName());\n+                key_tuple_mapping.emplace_back(i, key);\n+                data_types.push_back(header.getByPosition(key).type);\n+            }\n+        }\n+    }\n+    else\n+    {\n+        if (header.has(lhs.getColumnName()))\n+        {\n+            auto key = header.getPositionByName(lhs.getColumnName());\n+            key_tuple_mapping.emplace_back(0, key);\n+            data_types.push_back(header.getByPosition(key).type);\n+        }\n+    }\n+\n+    if (key_tuple_mapping.empty())\n+        return false;\n+\n+    ConstSetPtr prepared_set = rhs.tryGetPreparedSet();\n+    if (!prepared_set && !prepared_set->hasExplicitSetElements())\n+        return false;\n+\n+    for (const auto & data_type : prepared_set->getDataTypes())\n+        if (data_type->getTypeId() != TypeIndex::String && data_type->getTypeId() != TypeIndex::FixedString)\n+            return false;\n+\n+    std::vector<std::vector<GinFilter>> gin_filters;\n+    std::vector<size_t> key_position;\n+\n+    Columns columns = prepared_set->getSetElements();\n+    for (const auto & elem : key_tuple_mapping)\n+    {\n+        gin_filters.emplace_back();\n+        gin_filters.back().reserve(prepared_set->getTotalRowCount());\n+        key_position.push_back(elem.key_index);\n+\n+        size_t tuple_idx = elem.tuple_index;\n+        const auto & column = columns[tuple_idx];\n+        for (size_t row = 0; row < prepared_set->getTotalRowCount(); ++row)\n+        {\n+            gin_filters.back().emplace_back(params);\n+            auto ref = column->getDataAt(row);\n+            token_extractor->stringToGinFilter(ref.data, ref.size, gin_filters.back().back());\n+        }\n+    }\n+\n+    out.set_key_position = std::move(key_position);\n+    out.set_gin_filters = std::move(gin_filters);\n+\n+    return true;\n+}\n+\n+MergeTreeIndexGranulePtr MergeTreeIndexGinFilter::createIndexGranule() const\n+{\n+    return std::make_shared<MergeTreeIndexGranuleGinFilter>(index.name, index.column_names.size(), params);\n+}\n+\n+MergeTreeIndexAggregatorPtr MergeTreeIndexGinFilter::createIndexAggregator() const\n+{\n+    /// should not be called: createIndexAggregatorForPart should be used\n+    assert(false);\n+    return nullptr;\n+}\n+\n+MergeTreeIndexAggregatorPtr MergeTreeIndexGinFilter::createIndexAggregatorForPart(const GinIndexStorePtr &store) const\n+{\n+    return std::make_shared<MergeTreeIndexAggregatorGinFilter>(store, index.column_names, index.name, params, token_extractor.get());\n+}\n+\n+MergeTreeIndexConditionPtr MergeTreeIndexGinFilter::createIndexCondition(\n+        const SelectQueryInfo & query, ContextPtr context) const\n+{\n+    return std::make_shared<MergeTreeConditionGinFilter>(query, context, index.sample_block, params, token_extractor.get());\n+};\n+\n+bool MergeTreeIndexGinFilter::mayBenefitFromIndexForIn(const ASTPtr & node) const\n+{\n+    return std::find(std::cbegin(index.column_names), std::cend(index.column_names), node->getColumnName()) != std::cend(index.column_names);\n+}\n+\n+MergeTreeIndexPtr ginIndexCreator(\n+    const IndexDescription & index)\n+{\n+    size_t n = index.arguments.empty() ? 0 : index.arguments[0].get<size_t>();\n+    Float64 density = index.arguments.size() < 2 ? 1.0f : index.arguments[1].get<Float64>();\n+    GinFilterParameters params(n, density);\n+\n+    /// Use SplitTokenExtractor when n is 0, otherwise use NgramTokenExtractor\n+    if (n > 0)\n+    {\n+        auto tokenizer = std::make_unique<NgramTokenExtractor>(n);\n+        return std::make_shared<MergeTreeIndexGinFilter>(index, params, std::move(tokenizer));\n+    }\n+    else\n+    {\n+        auto tokenizer = std::make_unique<SplitTokenExtractor>();\n+        return std::make_shared<MergeTreeIndexGinFilter>(index, params, std::move(tokenizer));\n+    }\n+}\n+\n+void ginIndexValidator(const IndexDescription & index, bool /*attach*/)\n+{\n+    for (const auto & index_data_type : index.data_types)\n+    {\n+        WhichDataType data_type(index_data_type);\n+\n+        if (data_type.isArray())\n+        {\n+            const auto & gin_type = assert_cast<const DataTypeArray &>(*index_data_type);\n+            data_type = WhichDataType(gin_type.getNestedType());\n+        }\n+        else if (data_type.isLowCarnality())\n+        {\n+            const auto & low_cardinality = assert_cast<const DataTypeLowCardinality &>(*index_data_type);\n+            data_type = WhichDataType(low_cardinality.getDictionaryType());\n+        }\n+\n+        if (!data_type.isString() && !data_type.isFixedString())\n+            throw Exception(\"Inverted index can be used only with `String`, `FixedString`, `LowCardinality(String)`, `LowCardinality(FixedString)` column or Array with `String` or `FixedString` values column.\", ErrorCodes::INCORRECT_QUERY);\n+    }\n+\n+    if (index.type != GinFilter::getName())\n+        throw Exception(\"Unknown index type: \" + backQuote(index.name), ErrorCodes::LOGICAL_ERROR);\n+\n+    if (index.arguments.size() > 2)\n+        throw Exception(\"Inverted index must have less than two arguments.\", ErrorCodes::INCORRECT_QUERY);\n+\n+    if (!index.arguments.empty() && index.arguments[0].getType() != Field::Types::UInt64)\n+        throw Exception(\"The first Inverted index argument must be positive integer.\", ErrorCodes::INCORRECT_QUERY);\n+\n+    if (index.arguments.size() == 2 && (index.arguments[1].getType() != Field::Types::Float64 || index.arguments[1].get<Float64>() <= 0 || index.arguments[1].get<Float64>() > 1))\n+        throw Exception(\"The second Inverted index argument must be a float between 0 and 1.\", ErrorCodes::INCORRECT_QUERY);\n+\n+    size_t ngrams = index.arguments.empty() ? 0 : index.arguments[0].get<size_t>();\n+    Float64 density = index.arguments.size() < 2 ? 1.0f : index.arguments[1].get<Float64>();\n+\n+    /// Just validate\n+    GinFilterParameters params(ngrams, density);\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreeIndexGin.h b/src/Storages/MergeTree/MergeTreeIndexGin.h\nnew file mode 100644\nindex 000000000000..d915d4938103\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreeIndexGin.h\n@@ -0,0 +1,183 @@\n+#pragma once\n+#include <atomic>\n+#include <base/types.h>\n+\n+#include <memory>\n+\n+#include <Storages/MergeTree/MergeTreeData.h>\n+#include <Storages/MergeTree/KeyCondition.h>\n+#include <Interpreters/ITokenExtractor.h>\n+#include <Interpreters/GinFilter.h>\n+\n+namespace DB\n+{\n+struct MergeTreeIndexGranuleGinFilter final : public IMergeTreeIndexGranule\n+{\n+    explicit MergeTreeIndexGranuleGinFilter(\n+        const String & index_name_,\n+        size_t columns_number,\n+        const GinFilterParameters & params_);\n+\n+    ~MergeTreeIndexGranuleGinFilter() override = default;\n+\n+    void serializeBinary(WriteBuffer & ostr) const override;\n+    void deserializeBinary(ReadBuffer & istr, MergeTreeIndexVersion version) override;\n+\n+    bool empty() const override { return !has_elems; }\n+\n+    String index_name;\n+    GinFilterParameters params;\n+\n+    std::vector<GinFilter> gin_filters;\n+    bool has_elems;\n+};\n+\n+using MergeTreeIndexGranuleGinFilterPtr = std::shared_ptr<MergeTreeIndexGranuleGinFilter>;\n+\n+struct MergeTreeIndexAggregatorGinFilter final : IMergeTreeIndexAggregator\n+{\n+    explicit MergeTreeIndexAggregatorGinFilter(\n+        GinIndexStorePtr store_,\n+        const Names & index_columns_,\n+        const String & index_name_,\n+        const GinFilterParameters & params_,\n+        TokenExtractorPtr token_extractor_);\n+\n+    ~MergeTreeIndexAggregatorGinFilter() override = default;\n+\n+    bool empty() const override { return !granule || granule->empty(); }\n+    MergeTreeIndexGranulePtr getGranuleAndReset() override;\n+\n+    void update(const Block & block, size_t * pos, size_t limit) override;\n+\n+    void addToGinFilter(UInt32 rowID, const char* data, size_t length, GinFilter& gin_filter, UInt64 limit);\n+\n+    GinIndexStorePtr store;\n+    Names index_columns;\n+    const String index_name;\n+    const GinFilterParameters params;\n+    TokenExtractorPtr token_extractor;\n+\n+    MergeTreeIndexGranuleGinFilterPtr granule;\n+};\n+\n+\n+class MergeTreeConditionGinFilter final : public IMergeTreeIndexCondition, WithContext\n+{\n+public:\n+    MergeTreeConditionGinFilter(\n+            const SelectQueryInfo & query_info,\n+            ContextPtr context,\n+            const Block & index_sample_block,\n+            const GinFilterParameters & params_,\n+            TokenExtractorPtr token_extactor_);\n+\n+    ~MergeTreeConditionGinFilter() override = default;\n+\n+    bool alwaysUnknownOrTrue() const override;\n+    bool mayBeTrueOnGranule([[maybe_unused]]MergeTreeIndexGranulePtr idx_granule) const override\n+    {\n+        /// should call mayBeTrueOnGranuleInPart instead\n+        assert(false);\n+        return false;\n+    }\n+    bool mayBeTrueOnGranuleInPart(MergeTreeIndexGranulePtr idx_granule, [[maybe_unused]] PostingsCacheForStore& cache_store) const;\n+private:\n+    struct KeyTuplePositionMapping\n+    {\n+        KeyTuplePositionMapping(size_t tuple_index_, size_t key_index_) : tuple_index(tuple_index_), key_index(key_index_) {}\n+\n+        size_t tuple_index;\n+        size_t key_index;\n+    };\n+    /// Uses RPN like KeyCondition\n+    struct RPNElement\n+    {\n+        enum Function\n+        {\n+            /// Atoms of a Boolean expression.\n+            FUNCTION_EQUALS,\n+            FUNCTION_NOT_EQUALS,\n+            FUNCTION_HAS,\n+            FUNCTION_IN,\n+            FUNCTION_NOT_IN,\n+            FUNCTION_MULTI_SEARCH,\n+            FUNCTION_UNKNOWN, /// Can take any value.\n+            /// Operators of the logical expression.\n+            FUNCTION_NOT,\n+            FUNCTION_AND,\n+            FUNCTION_OR,\n+            /// Constants\n+            ALWAYS_FALSE,\n+            ALWAYS_TRUE,\n+        };\n+\n+        RPNElement( /// NOLINT\n+                Function function_ = FUNCTION_UNKNOWN, size_t key_column_ = 0, std::unique_ptr<GinFilter> && const_gin_filter_ = nullptr)\n+                : function(function_), key_column(key_column_), gin_filter(std::move(const_gin_filter_)) {}\n+\n+        Function function = FUNCTION_UNKNOWN;\n+        /// For FUNCTION_EQUALS, FUNCTION_NOT_EQUALS and FUNCTION_MULTI_SEARCH\n+        size_t key_column;\n+\n+        /// For FUNCTION_EQUALS, FUNCTION_NOT_EQUALS\n+        std::unique_ptr<GinFilter> gin_filter;\n+\n+        /// For FUNCTION_IN, FUNCTION_NOT_IN and FUNCTION_MULTI_SEARCH\n+        std::vector<std::vector<GinFilter>> set_gin_filters;\n+\n+        /// For FUNCTION_IN and FUNCTION_NOT_IN\n+        std::vector<size_t> set_key_position;\n+    };\n+\n+    using RPN = std::vector<RPNElement>;\n+\n+    bool traverseAtomAST(const RPNBuilderTreeNode & node, RPNElement & out);\n+\n+    bool traverseASTEquals(\n+        const String & function_name,\n+        const RPNBuilderTreeNode & key_ast,\n+        const DataTypePtr & value_type,\n+        const Field & value_field,\n+        RPNElement & out);\n+\n+    bool tryPrepareSetGinFilter(const RPNBuilderTreeNode & lhs, const RPNBuilderTreeNode & rhs, RPNElement & out);\n+\n+    static bool createFunctionEqualsCondition(\n+        RPNElement & out, const Field & value, const GinFilterParameters & params, TokenExtractorPtr token_extractor);\n+\n+    const Block & header;\n+    GinFilterParameters params;\n+    TokenExtractorPtr token_extractor;\n+    RPN rpn;\n+    /// Sets from syntax analyzer.\n+    PreparedSetsPtr prepared_sets;\n+};\n+\n+class MergeTreeIndexGinFilter final : public IMergeTreeIndex\n+{\n+public:\n+    MergeTreeIndexGinFilter(\n+        const IndexDescription & index_,\n+        const GinFilterParameters & params_,\n+        std::unique_ptr<ITokenExtractor> && token_extractor_)\n+        : IMergeTreeIndex(index_)\n+        , params(params_)\n+        , token_extractor(std::move(token_extractor_)) {}\n+\n+    ~MergeTreeIndexGinFilter() override = default;\n+\n+    MergeTreeIndexGranulePtr createIndexGranule() const override;\n+    MergeTreeIndexAggregatorPtr createIndexAggregator() const override;\n+    MergeTreeIndexAggregatorPtr createIndexAggregatorForPart(const GinIndexStorePtr &store) const override;\n+    MergeTreeIndexConditionPtr createIndexCondition(\n+            const SelectQueryInfo & query, ContextPtr context) const override;\n+\n+    bool mayBenefitFromIndexForIn(const ASTPtr & node) const override;\n+\n+    GinFilterParameters params;\n+    /// Function for selecting next token.\n+    std::unique_ptr<ITokenExtractor> token_extractor;\n+};\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreeIndices.cpp b/src/Storages/MergeTree/MergeTreeIndices.cpp\nindex eeeef27699f7..e5e376e7f69d 100644\n--- a/src/Storages/MergeTree/MergeTreeIndices.cpp\n+++ b/src/Storages/MergeTree/MergeTreeIndices.cpp\n@@ -3,7 +3,6 @@\n #include <Parsers/ParserCreateQuery.h>\n #include <IO/WriteHelpers.h>\n #include <IO/ReadHelpers.h>\n-\n #include <numeric>\n \n #include <boost/algorithm/string.hpp>\n@@ -106,6 +105,9 @@ MergeTreeIndexFactory::MergeTreeIndexFactory()\n     registerCreator(\"annoy\", annoyIndexCreator);\n     registerValidator(\"annoy\", annoyIndexValidator);\n #endif\n+    registerCreator(\"inverted\", ginIndexCreator);\n+    registerValidator(\"inverted\", ginIndexValidator);\n+\n }\n \n MergeTreeIndexFactory & MergeTreeIndexFactory::instance()\ndiff --git a/src/Storages/MergeTree/MergeTreeIndices.h b/src/Storages/MergeTree/MergeTreeIndices.h\nindex 6a671c31944e..52cf8c850b32 100644\n--- a/src/Storages/MergeTree/MergeTreeIndices.h\n+++ b/src/Storages/MergeTree/MergeTreeIndices.h\n@@ -1,10 +1,12 @@\n #pragma once\n \n #include <string>\n+#include <map>\n #include <unordered_map>\n #include <vector>\n #include <memory>\n #include <utility>\n+#include <mutex>\n #include <Core/Block.h>\n #include <Storages/StorageInMemoryMetadata.h>\n #include <Storages/MergeTree/MergeTreeDataPartChecksum.h>\n@@ -14,6 +16,8 @@\n #include <Interpreters/ExpressionActions.h>\n #include <DataTypes/DataTypeLowCardinality.h>\n \n+#include <Storages/MergeTree/GinIndexStore.h>\n+\n constexpr auto INDEX_FILE_PREFIX = \"skp_idx_\";\n \n namespace DB\n@@ -162,6 +166,11 @@ struct IMergeTreeIndex\n \n     virtual MergeTreeIndexAggregatorPtr createIndexAggregator() const = 0;\n \n+    virtual MergeTreeIndexAggregatorPtr createIndexAggregatorForPart([[maybe_unused]]const GinIndexStorePtr &store) const\n+    {\n+        return createIndexAggregator();\n+    }\n+\n     virtual MergeTreeIndexConditionPtr createIndexCondition(\n         const SelectQueryInfo & query_info, ContextPtr context) const = 0;\n \n@@ -228,5 +237,7 @@ void hypothesisIndexValidator(const IndexDescription & index, bool attach);\n MergeTreeIndexPtr annoyIndexCreator(const IndexDescription & index);\n void annoyIndexValidator(const IndexDescription & index, bool attach);\n #endif\n+MergeTreeIndexPtr ginIndexCreator(const IndexDescription& index);\n+void ginIndexValidator(const IndexDescription& index, bool attach);\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 204f7b941d07..ae1bace79e3a 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -25,6 +25,7 @@ struct Settings;\n     M(UInt64, min_compress_block_size, 0, \"When granule is written, compress the data in buffer if the size of pending uncompressed data is larger or equal than the specified threshold. If this setting is not set, the corresponding global setting is used.\", 0) \\\n     M(UInt64, max_compress_block_size, 0, \"Compress the pending uncompressed data in buffer if its size is larger or equal than the specified threshold. Block of data will be compressed even if the current granule is not finished. If this setting is not set, the corresponding global setting is used.\", 0) \\\n     M(UInt64, index_granularity, 8192, \"How many rows correspond to one primary key value.\", 0) \\\n+    M(UInt64, max_digestion_size_per_segment, 1024 * 1024 * 256, \"Max number of bytes to digest per segment to build GIN index.\", 0) \\\n     \\\n     /** Data storing format settings. */ \\\n     M(UInt64, min_bytes_for_wide_part, 10485760, \"Minimal uncompressed size in bytes to create part in wide format instead of compact\", 0) \\\n",
  "test_patch": "diff --git a/src/Common/tests/gtest_fst.cpp b/src/Common/tests/gtest_fst.cpp\nnew file mode 100644\nindex 000000000000..211f98cab74d\n--- /dev/null\n+++ b/src/Common/tests/gtest_fst.cpp\n@@ -0,0 +1,94 @@\n+#include <string>\n+#include <vector>\n+\n+#include <IO/WriteBufferFromVector.h>\n+#include <Common/FST.h>\n+#include <gtest/gtest.h>\n+\n+TEST(FST, SimpleTest)\n+{\n+    std::vector<std::pair<std::string, DB::FST::Output>> indexed_data\n+    {\n+        {\"mop\", 100},\n+        {\"moth\", 91},\n+        {\"pop\", 72},\n+        {\"star\", 83},\n+        {\"stop\", 54},\n+        {\"top\", 55},\n+    };\n+\n+    std::vector<std::pair<std::string, DB::FST::Output>> not_indexed_data\n+    {\n+        {\"mo\", 100},\n+        {\"moth1\", 91},\n+        {\"po\", 72},\n+        {\"star2\", 83},\n+        {\"sto\", 54},\n+        {\"top33\", 55},\n+    };\n+\n+    std::vector<UInt8> buffer;\n+    DB::WriteBufferFromVector<std::vector<UInt8>> wbuf(buffer);\n+    DB::FST::FSTBuilder builder(wbuf);\n+\n+    for (auto& [term, output] : indexed_data)\n+    {\n+        builder.add(term, output);\n+    }\n+    builder.build();\n+    wbuf.finalize();\n+\n+    DB::FST::FiniteStateTransducer fst(buffer);\n+    for (auto& [term, output] : indexed_data)\n+    {\n+        auto [result, found] = fst.getOutput(term);\n+        ASSERT_EQ(found, true);\n+        ASSERT_EQ(result, output);\n+    }\n+\n+    for (auto& [term, output] : not_indexed_data)\n+    {\n+        auto [result, found] = fst.getOutput(term);\n+        ASSERT_EQ(found, false);\n+    }\n+}\n+\n+TEST(FST, TestForLongTerms)\n+{\n+    /// Test long terms within limitation\n+    std::string term1(DB::FST::MAX_TERM_LENGTH - 1, 'A');\n+    std::string term2(DB::FST::MAX_TERM_LENGTH, 'B');\n+\n+    DB::FST::Output output1 = 100;\n+    DB::FST::Output output2 = 200;\n+\n+    std::vector<UInt8> buffer;\n+    DB::WriteBufferFromVector<std::vector<UInt8>> wbuf(buffer);\n+    DB::FST::FSTBuilder builder(wbuf);\n+\n+    builder.add(term1, output1);\n+    builder.add(term2, output2);\n+\n+    builder.build();\n+    wbuf.finalize();\n+\n+    DB::FST::FiniteStateTransducer fst(buffer);\n+\n+    auto [result1, found1] = fst.getOutput(term1);\n+    ASSERT_EQ(found1, true);\n+    ASSERT_EQ(result1, output1);\n+\n+    auto [result2, found2] = fst.getOutput(term2);\n+    ASSERT_EQ(found2, true);\n+    ASSERT_EQ(result2, output2);\n+\n+    /// Test exception case when term length exceeds limitation\n+    std::string term3(DB::FST::MAX_TERM_LENGTH + 1, 'C');\n+    DB::FST::Output output3 = 300;\n+\n+    std::vector<UInt8> buffer3;\n+    DB::WriteBufferFromVector<std::vector<UInt8>> wbuf3(buffer3);\n+    DB::FST::FSTBuilder builder3(wbuf3);\n+\n+    EXPECT_THROW(builder3.add(term3, output3), DB::Exception);\n+}\ndiff --git a/tests/queries/0_stateless/02346_full_text_search.reference b/tests/queries/0_stateless/02346_full_text_search.reference\nnew file mode 100644\nindex 000000000000..e035e93867b9\n--- /dev/null\n+++ b/tests/queries/0_stateless/02346_full_text_search.reference\n@@ -0,0 +1,52 @@\n+af\tinverted\n+101\tAlick a01\n+1\n+101\tAlick a01\n+111\tAlick b01\n+1\n+103\tClick a03\n+108\tClick a08\n+113\tClick b03\n+118\tClick b08\n+1\n+af\tinverted\n+101\tAlick a01\n+106\tAlick a06\n+111\tAlick b01\n+116\tAlick b06\n+101\tAlick a01\n+106\tAlick a06\n+1\n+101\tAlick a01\n+111\tAlick b01\n+1\n+af\tinverted\n+3\t['Click a03','Click b03']\n+1\n+af\tinverted\n+103\t{'Click':'Click a03'}\n+108\t{'Click':'Click a08'}\n+113\t{'Click':'Click b03'}\n+118\t{'Click':'Click b08'}\n+1\n+103\t{'Click':'Click a03'}\n+1\n+af\tinverted\n+101\tAlick a01\n+111\tAlick b01\n+201\trick c01\n+1\n+af\tinverted\n+102\tclickhouse\u4f60\u597d\n+1\n+af\tinverted\n+BC614E,05397FB1,6969696969898240,CF3304\n+1\n+af\tinverted\n+1\n+1\n+af\tinverted\n+1\n+1\n+1\n+1\ndiff --git a/tests/queries/0_stateless/02346_full_text_search.sql b/tests/queries/0_stateless/02346_full_text_search.sql\nnew file mode 100644\nindex 000000000000..a1439f031e21\n--- /dev/null\n+++ b/tests/queries/0_stateless/02346_full_text_search.sql\n@@ -0,0 +1,261 @@\n+SET log_queries = 1;\n+SET allow_experimental_inverted_index = 1;\n+\n+-- create table for inverted(2)\n+DROP TABLE IF EXISTS simple1;\n+CREATE TABLE simple1(k UInt64,s String,INDEX af (s) TYPE inverted(2) GRANULARITY 1) \n+            ENGINE = MergeTree() ORDER BY k\n+            SETTINGS index_granularity = 2;\n+-- insert test data into table\n+INSERT INTO simple1 VALUES (101, 'Alick a01'), (102, 'Blick a02'), (103, 'Click a03'),(104, 'Dlick a04'),(105, 'Elick a05'),(106, 'Alick a06'),(107, 'Blick a07'),(108, 'Click a08'),(109, 'Dlick a09'),(110, 'Elick a10'),(111, 'Alick b01'),(112, 'Blick b02'),(113, 'Click b03'),(114, 'Dlick b04'),(115, 'Elick b05'),(116, 'Alick b06'),(117, 'Blick b07'),(118, 'Click b08'),(119, 'Dlick b09'),(120, 'Elick b10');\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple1') limit 1;\n+\n+-- search inverted index with ==\n+SELECT * FROM simple1 WHERE s == 'Alick a01';\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 1 granules (2 rows total; each granule has 2 rows)\n+SELECT read_rows==2 from system.query_log \n+        where query_kind ='Select'\n+            and current_database = currentDatabase()\n+            and endsWith(trimRight(query), 'SELECT * FROM simple1 WHERE s == \\'Alick a01\\';')\n+            and type='QueryFinish'\n+            and result_rows==1\n+            limit 1;\n+\n+-- search inverted index with LIKE\n+SELECT * FROM simple1 WHERE s LIKE '%01%' ORDER BY k;\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 2 granules (4 rows total; each granule has 2 rows)\n+SELECT read_rows==4 from system.query_log \n+        where query_kind ='Select'\n+            and current_database = currentDatabase()\n+            and endsWith(trimRight(query), 'SELECT * FROM simple1 WHERE s LIKE \\'%01%\\' ORDER BY k;')\n+            and type='QueryFinish'\n+            and result_rows==2\n+            limit 1;\n+\n+-- search inverted index with hasToken\n+SELECT * FROM simple1 WHERE hasToken(s, 'Click') ORDER BY k;\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 4 granules (8 rows total; each granule has 2 rows)\n+SELECT read_rows==8 from system.query_log \n+        where query_kind ='Select'\n+            and current_database = currentDatabase()\n+            and endsWith(trimRight(query), 'SELECT * FROM simple1 WHERE hasToken(s, \\'Click\\') ORDER BY k;')\n+            and type='QueryFinish' \n+            and result_rows==4 limit 1;\n+\n+-- create table for inverted()\n+DROP TABLE IF EXISTS simple2;\n+CREATE TABLE simple2(k UInt64,s String,INDEX af (s) TYPE inverted() GRANULARITY 1) \n+    ENGINE = MergeTree() ORDER BY k\n+    SETTINGS index_granularity = 2;\n+\n+-- insert test data into table\n+INSERT INTO simple2 VALUES (101, 'Alick a01'), (102, 'Blick a02'), (103, 'Click a03'),(104, 'Dlick a04'),(105, 'Elick a05'),(106, 'Alick a06'),(107, 'Blick a07'),(108, 'Click a08'),(109, 'Dlick a09'),(110, 'Elick a10'),(111, 'Alick b01'),(112, 'Blick b02'),(113, 'Click b03'),(114, 'Dlick b04'),(115, 'Elick b05'),(116, 'Alick b06'),(117, 'Blick b07'),(118, 'Click b08'),(119, 'Dlick b09'),(120, 'Elick b10');\n+\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple2') limit 1;\n+\n+-- search inverted index with hasToken\n+SELECT * FROM simple2 WHERE hasToken(s, 'Alick') order by k;\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 4 granules (8 rows total; each granule has 2 rows)\n+SELECT read_rows==8 from system.query_log \n+    where query_kind ='Select'\n+        and current_database = currentDatabase()\n+        and endsWith(trimRight(query), 'SELECT * FROM simple2 WHERE hasToken(s, \\'Alick\\');') \n+        and type='QueryFinish' \n+        and result_rows==4 limit 1;\n+\n+-- search inverted index with IN operator\n+SELECT * FROM simple2 WHERE s IN ('Alick a01', 'Alick a06') ORDER BY k;\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 2 granules (4 rows total; each granule has 2 rows)\n+SELECT read_rows==4 from system.query_log \n+    where query_kind ='Select'\n+        and current_database = currentDatabase()\n+        and endsWith(trimRight(query), 'SELECT * FROM simple2 WHERE s IN (\\'Alick a01\\', \\'Alick a06\\') ORDER BY k;') \n+        and type='QueryFinish' \n+        and result_rows==2 limit 1;\n+\n+-- search inverted index with multiSearch        \n+SELECT * FROM simple2 WHERE multiSearchAny(s, ['a01', 'b01']) ORDER BY k;\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 2 granules (4 rows total; each granule has 2 rows)\n+SELECT read_rows==4 from system.query_log \n+    where query_kind ='Select'\n+        and current_database = currentDatabase()\n+        and endsWith(trimRight(query), 'SELECT * FROM simple2 WHERE multiSearchAny(s, [\\'a01\\', \\'b01\\']) ORDER BY k;') \n+        and type='QueryFinish' \n+        and result_rows==2 limit 1;\n+\n+-- create table with an array column\n+DROP TABLE IF EXISTS simple_array;\n+create table simple_array (k UInt64, s Array(String), INDEX af (s) TYPE inverted(2) GRANULARITY 1)\n+    ENGINE = MergeTree() ORDER BY k\n+    SETTINGS index_granularity = 2;\n+INSERT INTO simple_array SELECT rowNumberInBlock(), groupArray(s) FROM simple2 GROUP BY k%10;\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple_array') limit 1;\n+-- search inverted index with has\n+SELECT * FROM simple_array WHERE has(s, 'Click a03') ORDER BY k;\n+SYSTEM FLUSH LOGS;\n+-- check the query must read all 10 granules (20 rows total; each granule has 2 rows)\n+SELECT read_rows==2 from system.query_log \n+    where query_kind ='Select'\n+        and current_database = currentDatabase()\n+        and endsWith(trimRight(query), 'SELECT * FROM simple_array WHERE has(s, \\'Click a03\\') ORDER BY k;') \n+        and type='QueryFinish' \n+        and result_rows==1 limit 1;\n+\n+-- create table with a map column\n+DROP TABLE IF EXISTS simple_map;\n+CREATE TABLE simple_map (k UInt64, s Map(String,String), INDEX af (mapKeys(s)) TYPE inverted(2) GRANULARITY 1)\n+    ENGINE = MergeTree() ORDER BY k\n+    SETTINGS index_granularity = 2;\n+INSERT INTO simple_map VALUES (101, {'Alick':'Alick a01'}), (102, {'Blick':'Blick a02'}), (103, {'Click':'Click a03'}),(104, {'Dlick':'Dlick a04'}),(105, {'Elick':'Elick a05'}),(106, {'Alick':'Alick a06'}),(107, {'Blick':'Blick a07'}),(108, {'Click':'Click a08'}),(109, {'Dlick':'Dlick a09'}),(110, {'Elick':'Elick a10'}),(111, {'Alick':'Alick b01'}),(112, {'Blick':'Blick b02'}),(113, {'Click':'Click b03'}),(114, {'Dlick':'Dlick b04'}),(115, {'Elick':'Elick b05'}),(116, {'Alick':'Alick b06'}),(117, {'Blick':'Blick b07'}),(118, {'Click':'Click b08'}),(119, {'Dlick':'Dlick b09'}),(120, {'Elick':'Elick b10'});\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple_map') limit 1;\n+-- search inverted index with mapContains\n+SELECT * FROM simple_map WHERE mapContains(s, 'Click') ORDER BY k;\n+SYSTEM FLUSH LOGS;\n+-- check the query must read all 4 granules (8 rows total; each granule has 2 rows)\n+SELECT read_rows==8 from system.query_log \n+    where query_kind ='Select'\n+        and current_database = currentDatabase()\n+        and endsWith(trimRight(query), 'SELECT * FROM simple_map WHERE mapContains(s, \\'Click\\') ORDER BY k;') \n+        and type='QueryFinish' \n+        and result_rows==4 limit 1;\n+\n+-- search inverted index with map key\n+SELECT * FROM simple_map WHERE s['Click'] = 'Click a03';\n+SYSTEM FLUSH LOGS;\n+-- check the query must read all 4 granules (8 rows total; each granule has 2 rows)\n+SELECT read_rows==8 from system.query_log \n+    where query_kind ='Select'\n+        and current_database = currentDatabase()\n+        and endsWith(trimRight(query), 'SELECT * FROM simple_map WHERE s[\\'Click\\'] = \\'Click a03\\';') \n+        and type='QueryFinish' \n+        and result_rows==1 limit 1;\n+\n+-- create table for inverted(2) with two parts\n+DROP TABLE IF EXISTS simple3;\n+CREATE TABLE simple3(k UInt64,s String,INDEX af (s) TYPE inverted(2) GRANULARITY 1)\n+    ENGINE = MergeTree() ORDER BY k\n+    SETTINGS index_granularity = 2;\n+-- insert test data into table\n+INSERT INTO simple3 VALUES (101, 'Alick a01'), (102, 'Blick a02'), (103, 'Click a03'),(104, 'Dlick a04'),(105, 'Elick a05'),(106, 'Alick a06'),(107, 'Blick a07'),(108, 'Click a08'),(109, 'Dlick a09'),(110, 'Elick b10'),(111, 'Alick b01'),(112, 'Blick b02'),(113, 'Click b03'),(114, 'Dlick b04'),(115, 'Elick b05'),(116, 'Alick b06'),(117, 'Blick b07'),(118, 'Click b08'),(119, 'Dlick b09'),(120, 'Elick b10');\n+INSERT INTO simple3 VALUES (201, 'rick c01'), (202, 'mick c02'),(203, 'nick c03');\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple3') limit 1;\n+-- search inverted index\n+SELECT * FROM simple3 WHERE s LIKE '%01%' order by k;\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 3 granules (6 rows total; each granule has 2 rows)\n+SELECT read_rows==6 from system.query_log\n+    where query_kind ='Select' \n+        and current_database = currentDatabase()\n+        and endsWith(trimRight(query), 'SELECT * FROM simple3 WHERE s LIKE \\'%01%\\' order by k;')\n+        and type='QueryFinish' \n+        and result_rows==3 limit 1;\n+\n+-- create table for inverted(2) for utf8 string test\n+DROP TABLE IF EXISTS simple4;\n+CREATE TABLE simple4(k UInt64,s String,INDEX af (s) TYPE inverted(2) GRANULARITY 1) ENGINE = MergeTree() ORDER BY k\n+SETTINGS index_granularity = 2;\n+-- insert test data into table\n+INSERT INTO simple4 VALUES (101, 'Alick \u597d'),(102, 'clickhouse\u4f60\u597d'), (103, 'Click \u4f60'),(104, 'Dlick \u4f60a\u597d'),(105, 'Elick \u597d\u597d\u4f60\u4f60'),(106, 'Alick \u597da\u597da\u4f60a\u4f60');\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple4') limit 1;\n+-- search inverted index\n+SELECT * FROM simple4 WHERE s LIKE '%\u4f60\u597d%' order by k;\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 1 granule (2 rows total; each granule has 2 rows)\n+SELECT read_rows==2 from system.query_log \n+    where query_kind ='Select' \n+        and current_database = currentDatabase()    \n+        and endsWith(trimRight(query), 'SELECT * FROM simple4 WHERE s LIKE \\'%\u4f60\u597d%\\' order by k;') \n+        and type='QueryFinish' \n+        and result_rows==1 limit 1;\n+\n+-- create table for max_digestion_size_per_segment test\n+DROP TABLE IF EXISTS simple5;\n+CREATE TABLE simple5(k UInt64,s String,INDEX af(s) TYPE inverted(0) GRANULARITY 1)\n+                     Engine=MergeTree\n+                          ORDER BY (k)\n+                          SETTINGS max_digestion_size_per_segment = 1024, index_granularity = 256\n+                          AS\n+                          SELECT\n+                          number,\n+                          format('{},{},{},{}', hex(12345678), hex(87654321), hex(number/17 + 5), hex(13579012)) as s\n+                          FROM numbers(10240);\n+\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple5') limit 1;\n+-- search inverted index\n+SELECT s FROM simple5 WHERE hasToken(s, '6969696969898240');\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 1 granule (1 row total; each granule has 256 rows)\n+SELECT read_rows==256 from system.query_log \n+        where query_kind ='Select'\n+            and current_database = currentDatabase()\n+            and endsWith(trimRight(query), 'SELECT s FROM simple5 WHERE hasToken(s, \\'6969696969898240\\');') \n+            and type='QueryFinish' \n+            and result_rows==1 limit 1;\n+\n+DROP TABLE IF EXISTS simple6;\n+-- create inverted index with density==1\n+CREATE TABLE simple6(k UInt64,s String,INDEX af(s) TYPE inverted(0, 1.0) GRANULARITY 1)\n+                     Engine=MergeTree\n+                          ORDER BY (k)\n+                          SETTINGS max_digestion_size_per_segment = 1, index_granularity = 512\n+                          AS\n+                          SELECT number, if(number%2, format('happy {}', hex(number)), format('birthday {}', hex(number)))\n+                          FROM numbers(1024);\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple6') limit 1;\n+-- search inverted index, no row has 'happy birthday'\n+SELECT count()==0 FROM simple6 WHERE s=='happy birthday';\n+SYSTEM FLUSH LOGS;\n+-- check the query only skip all granules (0 row total; each granule has 512 rows)\n+SELECT read_rows==0 from system.query_log \n+        where query_kind ='Select'\n+            and current_database = currentDatabase()\n+            and endsWith(trimRight(query), 'SELECT count()==0 FROM simple6 WHERE s==\\'happy birthday\\';')\n+            and type='QueryFinish' \n+            and result_rows==1 limit 1;\n+\n+DROP TABLE IF EXISTS simple7;\n+-- create inverted index with density==0.1\n+CREATE TABLE simple7(k UInt64,s String,INDEX af(s) TYPE inverted(0, 0.1) GRANULARITY 1)\n+                    Engine=MergeTree\n+                        ORDER BY (k)\n+                        SETTINGS max_digestion_size_per_segment = 1, index_granularity = 512\n+                        AS\n+                        SELECT number, if(number==1023, 'happy new year', if(number%2, format('happy {}', hex(number)), format('birthday {}', hex(number))))\n+                        FROM numbers(1024);\n+-- check inverted index was created\n+SELECT name, type FROM system.data_skipping_indices where (table =='simple7') limit 1;\n+-- search inverted index, no row has 'happy birthday'\n+SELECT count()==0 FROM simple7 WHERE s=='happy birthday';\n+SYSTEM FLUSH LOGS;\n+-- check the query does not skip any of the 2 granules(1024 rows total; each granule has 512 rows)\n+SELECT read_rows==1024 from system.query_log \n+        where query_kind ='Select'\n+            and current_database = currentDatabase()\n+            and endsWith(trimRight(query), 'SELECT count()==0 FROM simple7 WHERE s==\\'happy birthday\\';')\n+            and type='QueryFinish' \n+            and result_rows==1 limit 1;\n+-- search inverted index, no row has 'happy new year'\n+SELECT count()==1 FROM simple7 WHERE s=='happy new year';\n+SYSTEM FLUSH LOGS;\n+-- check the query only read 1 granule because of density (1024 rows total; each granule has 512 rows)\n+SELECT read_rows==512 from system.query_log \n+        where query_kind ='Select'\n+            and current_database = currentDatabase()\n+            and endsWith(trimRight(query), 'SELECT count()==1 FROM simple7 WHERE s==\\'happy new year\\';')\n+            and type='QueryFinish' \n+            and result_rows==1 limit 1;\n+\n",
  "problem_statement": "Inverted Indices (RFC)\nWe already have \"data skipping\" indices in ClickHouse.\r\n\r\nData skipping index is a data structure (aggregation, data sketch, etc) that is created for every data granule (or every n granules) and allows to quickly answer - if some condition can be possibly true or possibly false on this range of data. This way, we can scan through the granules and the corresponding index data and filter out the irrelevant granules by asking every granule of the data skipping index.\r\n\r\nIn comparison, the inverted index is a single data structure created for all the granules and contains a mapping: value -> list of granules (marks) where the value might be.\r\n\r\nWe can have an independent immutable inverted index in every data part (it is the most simple approach compatible with replication, transactions, and MVCC...).\r\n\r\nInverted indices can reuse and extend the interface of `IMergeTreeIndex`. We can imagine it as having only one \"index granule\" per data part.\r\n\r\nInverted indices can be quite heavy (in some cases heavier than the data itself) and work well only when presented in memory. Nevertheless, we can experiment with fully in-memory indices (HashMap + Arena), reusing SSDCache data structure and RocksDB.\r\n\r\nThe main application will be text search. But we still need inverted indices on exact values as a first step in approaching text search. Then we will use tokenization or ngrams, normalization, and ensure that the intersection of granule lists works alright - to implement text search.\n",
  "hints_text": " Note that inverse index more difficult to write (especially if we expect them not to fit in memory). It looks like we need a separate algorithm for it (like, merge two sorted indexes on disk and re-calculate indexes). Just `IMergeTreeIndex` may be not enough.\nMaybe for now it has sense to implement indexes using orderby projections?\r\n\r\nNow you can create ORDERBY projection like `select all_pk_columns order by secondary_col` But such projection is excessive because it stores all_pk_columns instead of the mark (primary index) indexes and it requires to make a subquery in SQL.\n> Note that inverse index more difficult to write (especially if we expect them not to fit in memory).\r\n\r\nA minimal variant is to always keep it in memory but support lazy loading, limitations on its size, and fallback to unused index instead of OOM.\r\n\r\n> like, merge two sorted indexes on disk and re-calculate indexes\r\n\r\nThe first implementation will forget about part's indices during merge and simply build a new index for the new data part.\r\nKeep in mind that data can change during merge, which means that merging indices in a smart way won't always work.\r\n\r\nSo, an inverted index is not much different from a data skipping index with one index granule spanning all the part.\n@den-crane \r\n\r\n> Now you can create ORDERBY projection like select all_pk_columns order by secondary_col But such projection is excessive because it stores all_pk_columns instead of the mark (primary index) indexes and it requires to make a subquery in SQL.\r\n\r\nIt is not possible to have ORDER BY projection only with an index without the data (the values of ORDER BY key) because if we do, then merging of these projections is not possible.\n@larryluogit told me why it is reasonable to use row numbers instead of just granule numbers.\r\n\\- because it will make the intersection of posting lists more efficient, and we will filter more granules.\r\n\r\nImagine the search terms are \"ClickHouse good\" and the granule contains the following rows\r\n\r\n```\r\nGranule 0:\r\nRow 0: \"ClickHouse is awesome\"\r\nRow 1: \"MongoDB is good\"\r\n```\r\n\r\nThen if we have only granule numbers, the granule 0 will be scanned, because it contains both \"ClickHouse\" and \"good\".\r\nBut if we have row numbers, we will intersect two lists: \"ClickHouse\": 0 and \"good\": 1, and get empty intersection and successfully skip this granule.\n@alexey-milovidov \r\nWe implemented an inverted index in ClickHouse from scratch. A prototype solution has been built and will be submitted for review soon.\n> @alexey-milovidov We implemented an inverted index in ClickHouse from scratch. A prototype solution has been built and will be submitted for review soon.\r\n\r\n@larryluogit \r\nIs this solution has been submitted now ?\r\n\nWow, large scale vector approximate search could be implemented based on such kind of index as well.",
  "created_at": "2022-07-01T04:10:49Z"
}